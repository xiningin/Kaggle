{"cell_type":{"a2159981":"code","631f69f4":"code","14f68311":"code","5411c71b":"code","e5e00ebf":"code","0f9b1acd":"code","63b1cc33":"code","64128ac6":"code","64e6da5b":"code","2abf1c6a":"code","ab899ac5":"code","da6758da":"code","d6403b56":"code","89e33c60":"code","97470acd":"code","d00aea28":"code","5b1adb98":"code","f4390264":"code","d44f4281":"code","376f691a":"code","6d10bf11":"code","47ba3834":"code","e65be24c":"code","449332cb":"code","9f3ae550":"code","ccfacb7d":"code","f02b1d24":"code","dfc8331c":"code","6e249797":"code","54e5f208":"code","ff71099f":"code","203f35ad":"code","983903e3":"markdown","38370e04":"markdown","9199d7eb":"markdown","5070c01e":"markdown","97f09d07":"markdown","c887363b":"markdown","53cf0f38":"markdown","38a6b7db":"markdown","29099f48":"markdown","d2cd22fe":"markdown","eef81f5b":"markdown","94b64afe":"markdown","75249987":"markdown","620da444":"markdown","a0c5fe2c":"markdown"},"source":{"a2159981":"from keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import AveragePooling2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.convolutional import ZeroPadding2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.layers import add\nfrom keras.regularizers import l2\nfrom keras import backend as K\nimport numpy as np\nimport tensorflow as tf\nfrom functools import partial\nimport json\nimport sys\nimport shutil\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom pathlib import Path","631f69f4":"#!unzip -uq \"\/content\/drive\/MyDrive\/KITTY_DATASET\/data_tracking_image_2.zip\" -d \/kaggle\/input\/kitti-object-tracking-left-color\/data_tracking_image_2","14f68311":"#from google.colab import drive\n#drive.mount('\/content\/drive')","5411c71b":"import os\n#for dirname, _, filenames in os.walk('..\/input\/kitti-object-tracking-left-color'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\ndataset_dir = '..\/input\/sceneflow\/kitti2015\/training'\n\nflow_path = '..\/input\/sceneflow\/kitti2015\/training\/viz_flow_occ'","e5e00ebf":"def show_progress(epoch, batch, batch_total, **kwargs):\n    message = f'\\r{epoch} epoch: [{batch}\/{batch_total}'\n    for key, item in kwargs.items():\n        message += f', {key}: {item}'\n    sys.stdout.write(message+']')\n    sys.stdout.flush()\n\n\ndef save_config(config, filename = None):\n    if not isinstance(config, (dict, OrderedDict)):\n        raise TypeError('arg config must be a dict or OrderedDict')\n    config = OrderedDict(config)\n\n    if filename is None:\n        filename = 'config_' + datetime.now().strftime('%Y-%m-%d-%H-%M') + '.json'\n\n    with open(filename, 'w') as f:\n        json.dump(config, f, indent = 4)\n    print(f'Given config has been successfully saved to {filename}.')\n\n    \nclass ExperimentSaver:\n    def __init__(self, logdir = None, parse_args = None):\n        if logdir is None:\n            self.logdir = Path('logs_' + datetime.now().strftime('%Y-%m-%d-%H-%M'))\n        else:\n            self.logdir = Path(logdir)\n        if not self.logdir.exists():\n            self.logdir.mkdir()\n            \n        self.save_list = []\n\n        if parse_args is not None:\n            save_config(vars(parse_args), 'config.json')\n            self.append('config.json')\n\n    def append(self, file_or_dir_names):\n        if not isinstance(file_or_dir_names, list):\n            file_or_dir_names = [file_or_dir_names]\n        for name in file_or_dir_names:\n            self.save_list.append(Path(name))\n        \n    def save(self):\n        for path in self.save_list:\n            path.rename(self.logdir\/path)","0f9b1acd":"__all__ = ['load_flow', 'save_flow', 'vis_flow']\n\ndef load_flow(path):\n    with open(path, 'rb') as f:\n        magic = float(np.fromfile(f, np.float32, count = 1)[0])\n        if magic == 202021.25:\n            w, h = np.fromfile(f, np.int32, count = 1)[0], np.fromfile(f, np.int32, count = 1)[0]\n            data = np.fromfile(f, np.float32, count = h*w*2)\n            data.resize((h, w, 2))\n            return data\n        return None\n\ndef save_flow(path, flow):\n    magic = np.array([202021.25], np.float32)\n    h, w = flow.shape[:2]\n    h, w = np.array([h], np.int32), np.array([w], np.int32)\n\n    with open(path, 'wb') as f:\n        magic.tofile(f); w.tofile(f); h.tofile(f); flow.tofile(f)\n\n\ndef makeColorwheel():\n\n    #  color encoding scheme\n    \n    #   adapted from the color circle idea described at\n    #   http:\/\/members.shaw.ca\/quadibloc\/other\/colint.htm\n\n    RY = 15\n    YG = 6\n    GC = 4\n    CB = 11\n    BM = 13\n    MR = 6\n\n    ncols = RY + YG + GC + CB + BM + MR\n\n    colorwheel = np.zeros([ncols, 3]) # r g b\n\n    col = 0\n    #RY\n    colorwheel[0:RY, 0] = 255\n    colorwheel[0:RY, 1] = np.floor(255*np.arange(0, RY, 1)\/RY)\n    col += RY\n\n    #YG\n    colorwheel[col:YG+col, 0]= 255 - np.floor(255*np.arange(0, YG, 1)\/YG)\n    colorwheel[col:YG+col, 1] = 255;\n    col += YG;\n\n    #GC\n    colorwheel[col:GC+col, 1]= 255 \n    colorwheel[col:GC+col, 2] = np.floor(255*np.arange(0, GC, 1)\/GC)\n    col += GC;\n\n    #CB\n    colorwheel[col:CB+col, 1]= 255 - np.floor(255*np.arange(0, CB, 1)\/CB)\n    colorwheel[col:CB+col, 2] = 255\n    col += CB;\n\n    #BM\n    colorwheel[col:BM+col, 2]= 255 \n    colorwheel[col:BM+col, 0] = np.floor(255*np.arange(0, BM, 1)\/BM)\n    col += BM;\n\n    #MR\n    colorwheel[col:MR+col, 2] = 255 - np.floor(255*np.arange(0, MR, 1)\/MR)\n    colorwheel[col:MR+col, 0] = 255\n    return colorwheel\n\ndef computeColor(u, v):\n\n    colorwheel = makeColorwheel();\n    nan_u = np.isnan(u)\n    nan_v = np.isnan(v)\n    nan_u = np.where(nan_u)\n    nan_v = np.where(nan_v) \n\n    u[nan_u] = 0\n    u[nan_v] = 0\n    v[nan_u] = 0 \n    v[nan_v] = 0\n\n    ncols = colorwheel.shape[0]\n    radius = np.sqrt(u**2 + v**2)\n    a = np.arctan2(-v, -u) \/ np.pi\n    fk = (a+1) \/2 * (ncols-1) # -1~1 maped to 1~ncols\n    k0 = fk.astype(np.uint8)\t # 1, 2, ..., ncols\n    k1 = k0+1\n    k1[k1 == ncols] = 0\n    f = fk - k0\n\n    img = np.empty([k1.shape[0], k1.shape[1],3])\n    ncolors = colorwheel.shape[1]\n    for i in range(ncolors):\n        tmp = colorwheel[:,i]\n        col0 = tmp[k0]\/255\n        col1 = tmp[k1]\/255\n        col = (1-f)*col0 + f*col1\n        idx = radius <= 1\n        col[idx] = 1 - radius[idx]*(1-col[idx]) # increase saturation with radius    \n        col[~idx] *= 0.75 # out of range\n        img[:,:,2-i] = np.floor(255*col).astype(np.uint8)\n\n    return img.astype(np.uint8)\n\n\ndef vis_flow(flow):\n    eps = sys.float_info.epsilon\n    UNKNOWN_FLOW_THRESH = 1e9\n    UNKNOWN_FLOW = 1e10\n    \n    u = flow[:,:,0]\n    v = flow[:,:,1]\n\n    maxu = -999\n    maxv = -999\n\n    minu = 999\n    minv = 999\n\n    maxrad = -1\n    #fix unknown flow\n    greater_u = np.where(u > UNKNOWN_FLOW_THRESH)\n    greater_v = np.where(v > UNKNOWN_FLOW_THRESH)\n    u[greater_u] = 0\n    u[greater_v] = 0\n    v[greater_u] = 0 \n    v[greater_v] = 0\n\n    maxu = max([maxu, np.amax(u)])\n    minu = min([minu, np.amin(u)])\n\n    maxv = max([maxv, np.amax(v)])\n    minv = min([minv, np.amin(v)])\n    rad = np.sqrt(np.multiply(u,u)+np.multiply(v,v))\n    maxrad = max([maxrad, np.amax(rad)])\n    # print('max flow: %.4f flow range: u = %.3f .. %.3f; v = %.3f .. %.3f\\n' % (maxrad, minu, maxu, minv, maxv))\n\n    u = u\/(maxrad+eps)\n    v = v\/(maxrad+eps)\n    img = computeColor(u, v)\n    return img[:,:,[2,1,0]]\n   \ndef vis_flow_pyramid(flow_pyramid, flow_gt = None, images = None, filename = '.\/flow.png'):\n    num_contents = len(flow_pyramid) + int(flow_gt is not None) + int(images is not None)*2\n    fig = plt.figure(figsize = (12, 15*num_contents))\n\n    fig_id = 1\n\n    if images is not None:\n        plt.subplot(1, num_contents, fig_id)\n        plt.imshow(images[0])\n        plt.tick_params(labelbottom = False, bottom = False)\n        plt.tick_params(labelleft = False, left = False)\n        plt.xticks([])\n        box(False)\n        fig_id += 1\n\n        plt.subplot(1, num_contents, num_contents)\n        plt.imshow(images[1])\n        plt.tick_params(labelbottom = False, bottom = False)\n        plt.tick_params(labelleft = False, left = False)\n        plt.xticks([])\n        box(False)\n            \n    for flow in flow_pyramid:\n        plt.subplot(1, num_contents, fig_id)\n        plt.imshow(vis_flow(flow))\n        plt.tick_params(labelbottom = False, bottom = False)\n        plt.tick_params(labelleft = False, left = False)\n        plt.xticks([])\n        box(False)\n\n        fig_id += 1\n\n    if flow_gt is not None:\n        plt.subplot(1, num_contents, fig_id)\n        plt.imshow(vis_flow(flow_gt))\n        plt.tick_params(labelbottom = False, bottom = False)\n        plt.tick_params(labelleft = False, left = False)\n        plt.xticks([])\n        box(False)\n\n    plt.tight_layout()\n    plt.savefig(filename, bbox_inches = 'tight', pad_inches = 0.1)\n    plt.close()\n\n","63b1cc33":"def L1loss(x, y): \n    # shape(# batch, h, w, 2)\n    return tf.reduce_mean(tf.reduce_sum(tf.norm(x-y, ord = 1, axis = 3), axis = (1,2)))\n\ndef L2loss(x, y): # shape(# batch, h, w, 2)\n    return tf.reduce_mean(tf.reduce_sum(tf.norm(x-y, ord = 2, axis = 3), axis = (1,2)))\n","64128ac6":"# end point error, each element is same as L2 loss\ndef EPE(flows_gt, flows):\n    # Given ground truth and estimated flow must be unscaled\n    return tf.reduce_mean(tf.norm(flows_gt-flows, ord = 2, axis = 3))\n\ndef multiscale_loss(flows_gt, flows_pyramid,\n                    weights, name = 'multiscale_loss'):\n    # Argument flows_gt must be unscaled, scaled inside of this loss function\n    with tf.name_scope(name) as ns:\n        # Scale the ground truth flow, stated Sec.4 in the original paper\n        flows_gt_scaled = flows_gt\/20.\n\n        # Calculate mutiscale loss\n        loss = 0.\n        for l, (weight, fs) in enumerate(zip(weights, flows_pyramid)):\n            # Downsampling the scaled ground truth flow\n            _, h, w, _ = tf.unstack(tf.shape(fs))\n            fs_gt_down = tf.image.resize_nearest_neighbor(flows_gt_scaled, (h, w))\n            # Calculate l2 loss\n            loss += weight*L2loss(fs_gt_down, fs)\n\n        return loss","64e6da5b":"# https:\/\/github.com\/philferriere\/tfoptflow\/blob\/master\/tfoptflow\/optflow.py\n\n# https:\/\/github.com\/daigo0927\/pwcnet","2abf1c6a":"#Convolutional block for given filters, with or without batchnorm \ndef _conv_block(filters, kernel_size = (3, 3), strides = (1, 1), batch_norm = False):\n    def f(x):\n        x = tf.layers.Conv2D(filters, kernel_size,\n                             strides, 'same')(x)\n        #Batch normalisation-if required                     \n        if batch_norm:\n            x = tf.layers.BatchNormalization()(x)\n        #Leaky relu with alpha=0.2    \n        x = tf.nn.leaky_relu(x, 0.2)\n        return x\n    return f\n","ab899ac5":"# Feature pyramid extractor module simple\/original -----------------------\nclass FeaturePyramidExtractor(object):\n    def __init__(self, num_levels = 6, name = 'fp_extractor'):\n        self.num_levels = num_levels\n        self.filters = [16, 32, 64, 96, 128, 192]\n        self.name = name\n\n    def __call__(self, x, reuse = True):\n        \"\"\"\n        Args:\n        - images (batch, h, w, 3): input images\n        Returns:\n        - features_pyramid (batch, h_l, w_l, nch_l) for each scale levels:\n          extracted feature pyramid (deep -> shallow order)\n        \"\"\"\n        with tf.variable_scope(self.name, reuse = reuse) as vs:\n            features_pyramid = []\n            x = images\n            for l in range(self.num_levels):\n                x = tf.layers.Conv2D(self.filters[l], (3, 3), (2, 2), 'same')(x)\n                x = tf.nn.leaky_relu(x, 0.1)\n                x = tf.layers.Conv2D(self.filters[l], (3, 3), (1, 1), 'same')(x)\n                x = tf.nn.leaky_relu(x, 0.1)\n                x = tf.layers.Conv2D(self.filters[l], (3, 3), (1, 1), 'same')(x)\n                x = tf.nn.leaky_relu(x, 0.1)\n                features_pyramid.append(x)\n                \n            # return feature pyramid by ascent order\n            return features_pyramid[::-1]","da6758da":"# Warping layer ---------------------------------\ndef get_grid(x):\n    batch_size, height, width, filters = tf.unstack(tf.shape(x))\n    Bg, Yg, Xg = tf.meshgrid(tf.range(batch_size), tf.range(height), tf.range(width),\n                             indexing = 'ij')\n    # return indices volume indicate (batch, y, x)\n    # return tf.stack([Bg, Yg, Xg], axis = 3)\n    return Bg, Yg, Xg # return collectively for elementwise processing\n\ndef nearest_warp(x, flow):\n    grid_b, grid_y, grid_x = get_grid(x)\n    flow = tf.cast(flow, tf.int32)\n\n    warped_gy = tf.add(grid_y, flow[:,:,:,1]) # flow_y\n    warped_gx = tf.add(grid_x, flow[:,:,:,0]) # flow_x\n    # clip value by height\/width limitation\n    _, h, w, _ = tf.unstack(tf.shape(x))\n    warped_gy = tf.clip_by_value(warped_gy, 0, h-1)\n    warped_gx = tf.clip_by_value(warped_gx, 0, w-1)\n            \n    warped_indices = tf.stack([grid_b, warped_gy, warped_gx], axis = 3)\n            \n    warped_x = tf.gather_nd(x, warped_indices)\n    return warped_x\n\ndef bilinear_warp(x, flow):\n    _, h, w, _ = tf.unstack(tf.shape(x))\n    grid_b, grid_y, grid_x = get_grid(x)\n    grid_b = tf.cast(grid_b, tf.float32)\n    grid_y = tf.cast(grid_y, tf.float32)\n    grid_x = tf.cast(grid_x, tf.float32)\n\n    fx, fy = tf.unstack(flow, axis = -1)\n    fx_0 = tf.floor(fx)\n    fx_1 = fx_0+1\n    fy_0 = tf.floor(fy)\n    fy_1 = fy_0+1\n\n    # warping indices\n    h_lim = tf.cast(h-1, tf.float32)\n    w_lim = tf.cast(w-1, tf.float32)\n    gy_0 = tf.clip_by_value(grid_y + fy_0, 0., h_lim)\n    gy_1 = tf.clip_by_value(grid_y + fy_1, 0., h_lim)\n    gx_0 = tf.clip_by_value(grid_x + fx_0, 0., w_lim)\n    gx_1 = tf.clip_by_value(grid_x + fx_1, 0., w_lim)\n    \n    g_00 = tf.cast(tf.stack([grid_b, gy_0, gx_0], axis = 3), tf.int32)\n    g_01 = tf.cast(tf.stack([grid_b, gy_0, gx_1], axis = 3), tf.int32)\n    g_10 = tf.cast(tf.stack([grid_b, gy_1, gx_0], axis = 3), tf.int32)\n    g_11 = tf.cast(tf.stack([grid_b, gy_1, gx_1], axis = 3), tf.int32)\n\n    # gather contents\n    x_00 = tf.gather_nd(x, g_00)\n    x_01 = tf.gather_nd(x, g_01)\n    x_10 = tf.gather_nd(x, g_10)\n    x_11 = tf.gather_nd(x, g_11)\n\n    # coefficients\n    c_00 = tf.expand_dims((fy_1 - fy)*(fx_1 - fx), axis = 3)\n    c_01 = tf.expand_dims((fy_1 - fy)*(fx - fx_0), axis = 3)\n    c_10 = tf.expand_dims((fy - fy_0)*(fx_1 - fx), axis = 3)\n    c_11 = tf.expand_dims((fy - fy_0)*(fx - fx_0), axis = 3)\n\n    return c_00*x_00 + c_01*x_01 + c_10*x_10 + c_11*x_11","d6403b56":"class WarpingLayer(object):\n    def __init__(self, warp_type = 'nearest', name = 'warping'):\n        self.warp = warp_type\n        self.name = name\n\n    def __call__(self, x, flow):\n        # expect shape\n        # x:(#batch, height, width, #channel)\n        # flow:(#batch, height, width, 2)\n        with tf.name_scope(self.name) as ns:\n            assert self.warp in ['nearest', 'bilinear']\n            if self.warp == 'nearest':\n                x_warped = nearest_warp(x, flow)\n            else:\n                x_warped = bilinear_warp(x, flow)\n            return x_warped","89e33c60":"# Cost volume layer -------------------------------------\ndef pad2d(x, vpad, hpad):\n    return tf.pad(x, [[0, 0], vpad, hpad, [0, 0]])\n\ndef crop2d(x, vcrop, hcrop):\n    return tf.keras.layers.Cropping2D([vcrop, hcrop])(x)\n\ndef get_cost(features_0, features_0from1, shift):\n    \"\"\"\n    Calculate cost volume for specific shift\n    - inputs\n    features_0 (batch, h, w, nch): feature maps at time slice 0\n    features_0from1 (batch, h, w, nch): feature maps at time slice 0 warped from 1\n    shift (2): spatial (vertical and horizontal) shift to be considered\n    - output\n    cost (batch, h, w): cost volume map for the given shift\n    \"\"\"\n    v, h = shift # vertical\/horizontal element\n    vt, vb, hl, hr =  max(v,0), abs(min(v,0)), max(h,0), abs(min(h,0)) # top\/bottom left\/right\n    f_0_pad = pad2d(features_0, [vt, vb], [hl, hr])\n    f_0from1_pad = pad2d(features_0from1, [vb, vt], [hr, hl])\n    cost_pad = f_0_pad*f_0from1_pad\n    return tf.reduce_mean(crop2d(cost_pad, [vt, vb], [hl, hr]), axis = 3)","97470acd":"class CostVolumeLayer(object):\n    \"\"\" Cost volume module \"\"\"\n    def __init__(self, search_range = 4, name = 'cost_volume'):\n        self.s_range = search_range\n        self.name = name\n\n    def __call__(self, features_0, features_0from1):\n        with tf.name_scope(self.name) as ns:\n            b, h, w, f = tf.unstack(tf.shape(features_0))\n            cost_length = (2*self.s_range+1)**2\n\n            get_c = partial(get_cost, features_0, features_0from1)\n            cv = [0]*cost_length\n            depth = 0\n            for v in range(-self.s_range, self.s_range+1):\n                for h in range(-self.s_range, self.s_range+1):\n                    cv[depth] = get_c(shift = [v, h])\n                    depth += 1\n\n            cv = tf.stack(cv, axis = 3)\n            cv = tf.nn.leaky_relu(cv, 0.1)\n            return cv","d00aea28":"# Optical flow estimator module simple\/original -----------------------------------------\nclass OpticalFlowEstimator(object):\n    def __init__(self, name = 'of_estimator'):\n        self.batch_norm = False\n        self.name = name\n\n    def __call__(self, cost, x, flow):\n        with tf.variable_scope(self.name) as vs:\n            flow = tf.cast(flow, dtype = tf.float32)\n            x = tf.concat([cost, x, flow], axis = 3)\n            x = _conv_block(128, (3, 3), (1, 1), self.batch_norm)(x)\n            x = _conv_block(128, (3, 3), (1, 1), self.batch_norm)(x)\n            x = _conv_block(96, (3, 3), (1, 1), self.batch_norm)(x)\n            x = _conv_block(64, (3, 3), (1, 1), self.batch_norm)(x)\n            feature = _conv_block(32, (3, 3), (1, 1), self.batch_norm)(x)\n            flow = tf.layers.Conv2D(2, (3, 3), (1, 1), padding = 'same')(feature)\n\n            return feature, flow # x:processed feature, w:processed flow\n","5b1adb98":"class OpticalFlowEstimator_custom(object):\n    \"\"\" Optical flow estimator module \"\"\"\n    def __init__(self, use_dc = False, name = 'of_estimator'):\n        \"\"\"\n        Args:\n        - use_dc (bool): optional bool to use dense-connection, False as default\n        - name: module name\n        \"\"\"\n        self.filters = [128, 128, 96, 64, 32]\n        self.use_dc = use_dc\n        self.name = name\n\n    def __call__(self, cv, features_0 = None, flows_up_prev = None, features_up_prev = None,\n                 is_output = False):\n        \"\"\"\n        Args:\n        - cv (batch, h, w, nch_cv): cost volume\n        - features_0 (batch, h, w, nch_f0): feature map at time slice t\n        - flows_up_prev (batch, h, w, 2): upscaled optical flow passed from previous OF-estimator\n        - features_up_prev (batch, h, w, nch_fup): upscaled feature map passed from previous OF-estimator\n        - is_output (bool): whether at output level or not\n        Returns:\n        - flows (batch, h, w, 2): convolved optical flow\n        and \n        is_output: False\n        - flows_up (batch, 2*h, 2*w, 2): upsampled optical flow\n        - features_up (batch, 2*h, 2*w, nch_f): upsampled feature map\n        is_output: True\n        - features (batch, h, w, nch_f): convolved feature map\n        \"\"\"\n        with tf.variable_scope(self.name) as vs:\n            features = cv\n            for f in [features_0, flows_up_prev, features_up_prev]:\n                if f is not None:\n                    features = tf.concat([features, f], axis = 3)\n\n            for f in self.filters:\n                conv = tf.layers.Conv2D(f, (3, 3), (1, 1), 'same')(features)\n                conv = tf.nn.leaky_relu(conv, 0.1)\n                if self.use_dc:\n                    features = tf.concat([conv, features], axis = 3)\n                else:\n                    features = conv\n\n            flows = tf.layers.Conv2D(2, (3, 3), (1, 1), 'same')(features)\n            if flows_up_prev is not None:\n                # Residual connection\n                flows += flows_up_prev\n\n            if is_output:\n                return flows, features\n            else:\n                _, h, w, _ = tf.unstack(tf.shape(flows))\n                flows_up = tf.image.resize_bilinear(flows, (2*h, 2*w))\n                features_up = tf.image.resize_bilinear(features, (2*h, 2*w))\n                return flows, flows_up, features_up","f4390264":"# Context module -----------------------------------------------\nclass ContextNetwork(object):\n    \"\"\" Context module \"\"\"\n    def __init__(self, name = 'context'):\n        self.name = name\n\n    def __call__(self, flows, features):\n        \"\"\"\n        Args:\n        - flows (batch, h, w, 2): optical flow\n        - features (batch, h, w, 2): feature map passed from previous OF-estimator\n        Returns:\n        - flows (batch, h, w, 2): convolved optical flow\n        \"\"\"\n        with tf.variable_scope(self.name) as vs:\n            x = tf.concat([flows, features], axis = 3)\n            x = tf.layers.Conv2D(128, (3, 3), (1, 1),'same',\n                                 dilation_rate = (1, 1))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(128, (3, 3), (1, 1),'same',\n                                 dilation_rate = (2, 2))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(128, (3, 3), (1, 1),'same',\n                                 dilation_rate = (4, 4))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(96, (3, 3), (1, 1),'same',\n                                 dilation_rate = (8, 8))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(64, (3, 3), (1, 1),'same',\n                                 dilation_rate = (16, 16))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(32, (3, 3), (1, 1),'same',\n                                 dilation_rate = (1, 1))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(2, (3, 3), (1, 1),'same',\n                                 dilation_rate = (1, 1))(x)\n            return flows + x","d44f4281":"# Context module -----------------------------------------------\nclass ContextNetwork(object):\n    \"\"\" Context module \"\"\"\n    def __init__(self, name = 'context'):\n        self.name = name\n\n    def __call__(self, flows, features):\n        \"\"\"\n        Args:\n        - flows (batch, h, w, 2): optical flow\n        - features (batch, h, w, 2): feature map passed from previous OF-estimator\n        Returns:\n        - flows (batch, h, w, 2): convolved optical flow\n        \"\"\"\n        with tf.variable_scope(self.name) as vs:\n            x = tf.concat([flows, features], axis = 3)\n            x = tf.layers.Conv2D(128, (3, 3), (1, 1),'same',\n                                 dilation_rate = (1, 1))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(128, (3, 3), (1, 1),'same',\n                                 dilation_rate = (2, 2))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(128, (3, 3), (1, 1),'same',\n                                 dilation_rate = (4, 4))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(96, (3, 3), (1, 1),'same',\n                                 dilation_rate = (8, 8))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(64, (3, 3), (1, 1),'same',\n                                 dilation_rate = (16, 16))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(32, (3, 3), (1, 1),'same',\n                                 dilation_rate = (1, 1))(x)\n            x = tf.nn.leaky_relu(x, 0.1)\n            x = tf.layers.Conv2D(2, (3, 3), (1, 1),'same',\n                                 dilation_rate = (1, 1))(x)\n            return flows + x","376f691a":"class PWCDCNet(object):\n    def __init__(self, num_levels = 6, search_range = 4,\n                 warp_type = 'bilinear', use_dc = False,\n                 output_level = 4, name = 'pwcdcnet'):\n        self.num_levels = num_levels\n        self.s_range = search_range\n        self.warp_type = warp_type\n        self.use_dc = use_dc\n        assert output_level < num_levels, 'Should set output_level < num_levels'\n        self.output_level = output_level\n        self.name = name\n\n        self.fp_extractor = FeaturePyramidExtractor_custom(self.num_levels)\n        self.warp_layer = WarpingLayer(self.warp_type)\n        self.cv_layer = CostVolumeLayer(search_range)\n        self.of_estimators = [OpticalFlowEstimator_custom(use_dc = self.use_dc, name = f'optflow_{l}')\\\n                              for l in range(self.num_levels)]\n        self.context = ContextNetwork(name = 'context')\n        # Upscale factors from deep -> shallow level\n        self.scales = [None, 0.625, 1.25, 2.5, 5.0, 10., 20.]\n\n    def __call__(self, images_0, images_1, with_features = False, reuse = False):\n        with tf.variable_scope(self.name, reuse = reuse) as vs:\n            pyramid_0 = self.fp_extractor(images_0, reuse = reuse)\n            pyramid_1 = self.fp_extractor(images_1)\n\n            flows_pyramid = []\n            flows_up, features_up = None, None\n            for l, (features_0, features_1) in enumerate(zip(pyramid_0, pyramid_1)):\n                print(f'Level {l}')\n\n                # Warping operation\n                if l == 0:\n                    features_1_warped = features_1\n                else:\n                    features_1_warped = self.warp_layer(features_1, flows_up*self.scales[l])\n\n                # Cost volume calculation\n                cv = self.cv_layer(features_0, features_1_warped)\n                # Optical flow estimation\n                if l < self.output_level:\n                    flows, flows_up, features_up \\\n                        = self.of_estimators[l](cv, features_0, flows_up, features_up)\n                else:\n                    # At output level\n                    flows, features = self.of_estimators[l](cv, features_0, flows_up, features_up,\n                                                            is_output = True)\n                    # Context processing\n                    flows = self.context(flows, features)\n                    flows_pyramid.append(flows)\n                    # Obtain finally scale-adjusted flow\n                    upscale = 2**(self.num_levels-self.output_level)\n                    _, h, w, _ = tf.unstack(tf.shape(flows))\n                    flows_final = tf.image.resize_bilinear(flows, (h*upscale, w*upscale))*20.\n\n                    if with_features:\n                        return flows_final, flows_pyramid, pyramid_0\n                    else:\n                        return flows_final, flows_pyramid\n\n                flows_pyramid.append(flows)","6d10bf11":"class PWCNet(object):\n    def __init__(self, num_levels = 6, search_range = 4, warp_type = 'bilinear',\n                 output_level = 4, name = 'pwcnet'):\n        self.num_levels = num_levels\n        self.s_range = search_range\n        self.warp_type = warp_type\n        assert output_level < num_levels, 'Should set output_level < num_levels'\n        self.output_level = output_level\n        self.name = name\n\n        self.fp_extractor = FeaturePyramidExtractor(self.num_levels)\n        self.warp_layer = WarpingLayer(self.warp_type)\n        self.cv_layer = CostVolumeLayer(self.s_range)\n        self.of_estimators = [OpticalFlowEstimator(self.batch_norm,\n                                                   name = f'optflow_{l}')\\\n                              for l in range(self.num_levels)]\n        # self.contexts = ContextNetwork()\n        assert self.context in ['all', 'final'], 'context argument should be all\/final'\n        if self.context is 'all':\n            self.context_nets = [ContextNetwork(name = f'context_{l}')\\\n                                 for l in range(self.num_levels)]\n        else:\n            self.context_net = ContextNetwork(name = 'context')\n\n    def __call__(self, images_0, images_1):\n        with tf.variable_scope(self.name) as vs:\n\n            pyramid_0 = self.fp_extractor(images_0, reuse = False)\n            pyramid_1 = self.fp_extractor(images_1)\n\n            flows = []\n            # coarse to fine processing\n            for l, (feature_0, feature_1) in enumerate(zip(pyramid_0, pyramid_1)):\n                print(f'Level {l}')\n                b, h, w, _ = tf.unstack(tf.shape(feature_0))\n                \n                if l == 0:\n                    flow = tf.zeros((b, h, w, 2), dtype = tf.float32)\n                else:\n                    flow = tf.image.resize_bilinear(flow, (h, w))*2\n\n                # warping -> costvolume -> optical flow estimation\n                feature_1_warped = self.warp_layer(feature_1, flow)\n                cost = self.cv_layer(feature_0, feature_1_warped)\n                feature, flow = self.of_estimators[l](feature_0, cost, flow)\n\n                # context considering process all\/final\n                if self.context is 'all':\n                    flow = self.context_nets[l](feature, flow)\n                elif l == self.output_level: \n                    flow = self.context_net(feature, flow)\n\n                flows.append(flow)\n                \n                # stop processing at the defined level\n                if l == self.output_level:\n                    upscale = 2**(self.num_levels - self.output_level)\n                    print(f'Finally upscale flow by {upscale}.')\n                    finalflow = tf.image.resize_bilinear(flow, (h*upscale, w*upscale))*upscale\n                    break\n\n            return finalflow, flows, pyramid_0\n\n    @property\n    def vars(self):\n        return [var for var in tf.global_variables() if self.name in var.name]\n","47ba3834":"class Trainer(object):\n    def __init__(self, args):\n        self.args = args\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config = config)\n        self._build_dataloader()\n        self._build_graph()\n\n    def _build_dataloader(self):\n        dset = get_dataset(self.args.dataset) #change this #data_input\n        #self.args means we have to put some info??\n        \n        data_args = {'dataset_dir':self.args.dataset_dir, \"origin_size\":None,\n                     'crop_type':self.args.crop_type, 'crop_shape':self.args.crop_shape,\n                     'resize_shape':self.args.resize_shape, 'resize_scale':self.args.resize_scale}\n        tset = dset(train_or_val = 'train', **data_args)\n        vset = dset(train_or_val = 'val', **data_args)\n        self.image_size = tset.image_size\n\n        load_args = {'batch_size': self.args.batch_size, 'num_workers':self.args.num_workers,\n                     'drop_last':True, 'pin_memory':True}\n        self.num_batches = int(len(tset.samples)\/self.args.batch_size)\n        print(f'Found {len(tset.samples)} samples -> {self.num_batches} mini-batches')\n        self.tloader = data.DataLoader(tset, shuffle = True, **load_args)\n        self.vloader = data.DataLoader(vset, shuffle = False, **load_args)\n        \n        \n    def _build_graph(self):\n        # Input images and ground truth optical flow definition\n        with tf.name_scope('Data'):\n            self.images = tf.placeholder(tf.float32, shape = (self.args.batch_size, 2, *self.image_size, 3),\n                                         name = 'images')\n            images_0, images_1 = tf.unstack(self.images, axis = 1)\n            self.flows_gt = tf.placeholder(tf.float32, shape = (self.args.batch_size, *self.image_size, 2),\n                                           name = 'flows')\n            \n        model = PWCDCNet(num_levels = self.args.num_levels,\n                             search_range = self.args.search_range,\n                             warp_type = self.args.warp_type,\n                             use_dc = self.args.use_dc,\n                             output_level = self.args.output_level,\n                             name = 'pwcdcnet')\n        flows_final, self.flows = model(images_0, images_1)\n\n        #target_weights_use's\n        target_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                                           scope = 'pwcdcnet\/fp_extractor')[::6]\n        target_weights += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                                            scope = 'pwcdcnet\/optflow')[::12]\n        \n         # Loss calculation\n        with tf.name_scope('Loss'):\n            if self.args.loss is 'multiscale':\n                criterion = multiscale_loss\n            else:\n                criterion =\\\n                  partial(multirobust_loss, epsilon = self.args.epsilon, q = self.args.q)\n            \n            _loss = criterion(self.flows_gt, self.flows, self.args.weights)\n            weights_l2 = tf.reduce_sum([tf.nn.l2_loss(var) for var in model.vars])\n            loss = _loss + self.args.gamma*weights_l2\n\n            epe = EPE(self.flows_gt, flows_final)\n\n        # Gradient descent optimization\n        with tf.name_scope('Optimize'):\n            self.global_step = tf.train.get_or_create_global_step()\n            if self.args.lr_scheduling:\n                boundaries = [200000, 250000, 300000, 350000, 4000000]\n                values = [self.args.lr\/(2**i) for i in range(len(boundaries)+1)]\n                lr = tf.train.piecewise_constant(self.global_step, boundaries, values)\n            else:\n                lr = self.args.lr\n\n            self.optimizer = tf.train.AdamOptimizer(learning_rate = lr)\\\n                             .minimize(loss, var_list = model.vars)\n            with tf.control_dependencies([self.optimizer]):\n                self.optimizer = tf.assign_add(self.global_step, 1)\n                \n        \n        # Initialization\n        self.saver = tf.train.Saver(model.vars)\n        self.sess.run(tf.global_variables_initializer())\n        if self.args.resume is not None:\n            print(f'Loading learned model from checkpoint {self.args.resume}')\n            self.saver.restore(self.sess, self.args.resume)\n\n        # Summarize\n        # Original PWCNet loss\n        sum_loss = tf.summary.scalar('loss\/pwc', loss)\n        # EPE for both domains\n        sum_epe = tf.summary.scalar('EPE\/source', epe)\n        # Merge summaries\n        self.merged = tf.summary.merge([sum_loss, sum_epe])\n\n        logdir = 'logs\/history_' + datetime.now().strftime('%Y-%m-%d-%H-%M')\n        self.twriter = tf.summary.FileWriter(logdir+'\/train', graph = self.sess.graph)\n        self.vwriter = tf.summary.FileWriter(logdir+'\/val', graph = self.sess.graph)\n\n        self.exp_saver = ExperimentSaver(logdir = logdir, parse_args = self.args)\n\n        print(f'Graph building completed, histories are logged in {logdir}')\n        \n        \n        \n        def train(self):\n            for e in tqdm(range(self.args.num_epochs)):\n                # Training\n                for images, flows_gt in self.tloader:\n                    images = images.numpy()\/255.0\n                    flows_gt = flows_gt.numpy()\n\n                    _, g_step = self.sess.run([self.optimizer, self.global_step],\n                                              feed_dict = {self.images: images,\n                                                           self.flows_gt: flows_gt})\n\n                    if g_step%1000 == 0:\n                        summary = self.sess.run(self.merged,\n                                                feed_dict = {self.images: images,\n                                                             self.flows_gt: flows_gt})\n                        self.twriter.add_summary(summary, g_step)\n\n                # Validation\n                for images_val, flows_gt_val in self.vloader:\n                    images_val = images_val.numpy()\/255.0\n                    flows_gt_val = flows_gt_val.numpy()\n\n                    summary = self.sess.run(self.merged,\n                                            feed_dict = {self.images: images_val,\n                                                         self.flows_gt: flows_gt_val})\n                    self.vwriter.add_summary(summary, g_step)\n                # Collect convolution weights and biases\n                # summary_plus = self.sess.run(self.merged_plus)\n                # self.vwriter.add_summary(summary_plus, g_step)\n\n                # visualize estimated optical flow\n                if self.args.visualize:\n                    if not os.path.exists('.\/figure'):\n                        os.mkdir('.\/figure')\n                    # Estimated flow values are downscaled, rescale them compatible to the ground truth\n                    flow_set = []\n                    flows_val = self.sess.run(self.flows, feed_dict = {self.images: images_val,\n                                                                       self.flows_gt: flows_gt_val})\n                    for l, flow in enumerate(flows_val):\n                        upscale = 20\/2**(self.args.num_levels-l)\n                        flow_set.append(flow[0]*upscale)\n                    flow_gt = flows_gt_val[0]\n                    images_v = images_val[0]\n                    vis_flow_pyramid(flow_set, flow_gt, images_v,\n                                     f'.\/figure\/flow_{str(e+1).zfill(4)}.pdf')\n\n                if not os.path.exists('.\/model'):\n                    os.mkdir('.\/model')\n                self.saver.save(self.sess, f'.\/model\/model_{e+1}.ckpt')\n\n        \n        self.twriter.close()\n        self.vwriter.close()\n        self.exp_saver.append(['.\/figure', '.\/model'])\n        self.exp_saver.save()","e65be24c":"if __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-d', '--dataset', type = str, default = 'SintelClean',\n                        help = 'Target dataset, [SintelClean]')\n    parser.add_argument('-dd', '--dataset_dir', type = str, required = True,\n                        help = 'Directory containing target dataset')\n    parser.add_argument('-e', '--num_epochs', type = int, default = 100,\n                        help = '# of epochs [100]')\n    parser.add_argument('-b', '--batch_size', type = int, default = 4,\n                        help = 'Batch size [4]')\n    parser.add_argument('-nw', '--num_workers', type = int, default = 2,\n                        help = '# of workers for data loading [2]')\n\n    parser.add_argument('--crop_type', type = str, default = 'random',\n                        help = 'Crop type for raw data [random]')\n    parser.add_argument('--crop_shape', nargs = 2, type = int, default = [384, 448],\n                        help = 'Crop shape for raw data [384, 448]')\n    parser.add_argument('--resize_shape', nargs = 2, type = int, default = None,\n                        help = 'Resize shape for raw data [None]')\n    parser.add_argument('--resize_scale', type = float, default = None,\n                        help = 'Resize scale for raw data [None]')\n\n    parser.add_argument('--num_levels', type = int, default = 6,\n                        help = '# of levels for feature extraction [6]')\n    parser.add_argument('--search_range', type = int, default = 4,\n                        help = 'Search range for cost-volume calculation [4]')\n    parser.add_argument('--warp_type', default = 'bilinear', choices = ['bilinear', 'nearest'],\n                        help = 'Warping protocol, [bilinear] or nearest')\n    parser.add_argument('--use-dc', dest = 'use_dc', action = 'store_true',\n                        help = 'Enable dense connection in optical flow estimator, [diabled] as default')\n    parser.add_argument('--no-dc', dest = 'use_dc', action = 'store_false',\n                        help = 'Disable dense connection in optical flow estimator, [disabled] as default')\n    parser.set_defaults(use_dc = False)\n    parser.add_argument('--output_level', type = int, default = 4,\n                        help = 'Final output level for estimated flow [4]')\n\n    parser.add_argument('--loss', default = 'multiscale', choices = ['multiscale', 'robust'],\n                        help = 'Loss function choice in [multiscale\/robust]')\n    parser.add_argument('--lr', type = float, default = 1e-4,\n                        help = 'Learning rate [1e-4]')\n    parser.add_argument('--lr_scheduling', dest = 'lr_scheduling', action = 'store_true',\n                        help = 'Enable learning rate scheduling, [enabled] as default')\n    parser.add_argument('--no-lr_scheduling', dest = 'lr_scheduling', action = 'store_false',\n                        help = 'Disable learning rate scheduling, [enabled] as default')\n    parser.set_defaults(lr_scheduling = True)\n    parser.add_argument('--weights', nargs = '+', type = float,\n                        default = [0.32, 0.08, 0.02, 0.01, 0.005],\n                        help = 'Weights for each pyramid loss')\n    parser.add_argument('--gamma', type = float, default = 0.0004,\n                        help = 'Coefficient for weight decay [4e-4]')\n    parser.add_argument('--epsilon', type = float, default = 0.02,\n                        help = 'Small constant for robust loss [0.02]')\n    parser.add_argument('--q', type = float, default = 0.4,\n                        help = 'Tolerance constant for outliear flow [0.4]')\n\n    parser.add_argument('-v', '--visualize', dest = 'visualize', action = 'store_true',\n                        help = 'Enable estimated flow visualization, [enabled] as default')\n    parser.add_argument('--no-visualize', dest = 'visualize', action = 'store_false',\n                        help = 'Disable estimated flow visualization, [enabled] as default')\n    parser.set_defaults(visualize = True)\n    parser.add_argument('-r', '--resume', type = str, default = None,\n                        help = 'Learned parameter checkpoint file [None]')\n    \n    args = parser.parse_args()\n    \n    for key, item in vars(args).items():\n        print(f'{key} : {item}')\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # input('Input utilize gpu-id (-1:cpu) : ')\n\n    trainer = Trainer(args)\n    trainer.train()","449332cb":"dataset : KITTI\ndataset_dir : ..\/input\/sceneflow\/kitti2015\/training\nnum_epochs : 100\nbatch_size : 4\nnum_workers : 2\ncrop_type : random\ncrop_shape : [384, 448]\nresize_shape : None\nresize_scale : None\nnum_levels : 6\nsearch_range : 4\nwarp_type : bilinear\nuse_dc : False\noutput_level : 4\nloss : multiscale\nlr : 0.0001\nlr_scheduling : True\nweights : [0.32, 0.08, 0.02, 0.01, 0.05]\ngamma : 0.0004\nepsilon : 0.02\nq : 0.4\nvisualize : True\nresume : None","9f3ae550":"def res_identity(x, filters): \n  #resnet identity block where dimension does not change.\n  #The skip connection is just simple identity connection\n  #we will have 3 blocks and then input will be added\n\n  x_skip = x # this will be used for addition with the residual block \n  f1, f2 = filters\n\n  #first block \n  x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  #second block # bottleneck (but size kept same with padding)\n  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  # third block activation used after adding the input\n  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  # x = Activation(activations.relu)(x)\n\n  # add the input \n  x = Add()([x, x_skip])\n  x = Activation(activations.relu)(x)\n\n  return x","ccfacb7d":"def res_conv(x, s, filters):\n  '''\n  here the input size changes''' \n  x_skip = x\n  f1, f2 = filters\n\n  # first block\n  x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x)\n  # when s = 2 then it is like downsizing the feature map\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  # second block\n  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  #third block\n  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n\n  # shortcut \n  x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x_skip)\n  x_skip = BatchNormalization()(x_skip)\n\n  # add \n  x = Add()([x, x_skip])\n  x = Activation(activations.relu)(x)\n\n  return x","f02b1d24":"def resnet50():\n\n  input_im = Input(shape=(train_im.shape[1], train_im.shape[2], train_im.shape[3])) # cifar 10 images size\n  x = ZeroPadding2D(padding=(3, 3))(input_im)\n\n  # 1st stage\n  # here we perform maxpooling, see the figure above\n\n  x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n  x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n  #2nd stage \n  # frm here on only conv block and identity block, no pooling\n\n  x = res_conv(x, s=1, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n\n  # 3rd stage\n\n  x = res_conv(x, s=2, filters=(128, 512))\n  x = res_identity(x, filters=(128, 512))\n  x = res_identity(x, filters=(128, 512))\n  x = res_identity(x, filters=(128, 512))\n\n  # 4th stage\n\n  x = res_conv(x, s=2, filters=(256, 1024))\n  x = res_identity(x, filters=(256, 1024))\n  x = res_identity(x, filters=(256, 1024))\n  x = res_identity(x, filters=(256, 1024))\n  x = res_identity(x, filters=(256, 1024))\n  x = res_identity(x, filters=(256, 1024))\n\n  # 5th stage\n\n  x = res_conv(x, s=2, filters=(512, 2048))\n  x = res_identity(x, filters=(512, 2048))\n  x = res_identity(x, filters=(512, 2048))\n\n  # ends with average pooling and dense connection\n\n  x = AveragePooling2D((2, 2), padding='same')(x)\n\n  x = Flatten()(x)\n  x = Dense(len(class_types), activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n\n  # define the model \n\n  model = Model(inputs=input_im, outputs=x, name='Resnet50')\n\n  return model","dfc8331c":"!cp -r \/kaggle\/input\/sceneflow\/kitti2015 \/kaggle\/working\/","6e249797":"%cd \/kaggle\/working\n!git clone https:\/\/github.com\/daigo0927\/pwcnet\n%cd pwcnet\n!git submodule init\n!git submodule update","54e5f208":"!sed -i 's\/^\\s*import tensorflow as.*\/import tensorflow.compat.v1 as tf\\ntf.disable_v2_behavior()\/g' .\/*.py","ff71099f":"import tensorflow as tf\nimport os\ndevice_name = tf.test.gpu_device_name()\nprint(f'device_name: {device_name}')\nif device_name != '\/device:GPU:0':\n  # use cpu\n  os.system('sed -i \"s\/os.environ\\\\[\\'CUDA_VISIBLE_DEVICES\\'\\\\] = input\/os.environ[\\'CUDA_VISIBLE_DEVICES\\'] = \\'-1\\' # input\/g\" \/kaggle\/working\/pwcnet\/*.py')\nelse:\n  # use gpu\n  os.system('sed -i \"s\/os.environ\\\\[\\'CUDA_VISIBLE_DEVICES\\'\\\\] = input\/os.environ[\\'CUDA_VISIBLE_DEVICES\\'] = \\'0\\' # input\/g\" \/kaggle\/working\/pwcnet\/*.py')","203f35ad":"# KITTI PATH = \/kaggle\/input\/sceneflow\/kitti2015\n!python train.py --dataset KITTI --dataset_dir \/kaggle\/working\/kitti2015 --crop_shape 320 1216","983903e3":"## NOTE for Kaggle\n\nInstead of **\"\/content\/drive\/MyDrive\/KITTI DATASET\/unzippedimages\"**, use **\"\/kaggle\/input\/kitti-object-tracking-left-color\/data_tracking_image_2\"** in further locations.\n","38370e04":"### MODELS","9199d7eb":"The **resnet50** function combines both the above shallow block and designs a complete 50 layered architecture.","5070c01e":"### TRAINING","97f09d07":"## **PWS-Net** Architecture starts from here","c887363b":"###UTILS\n","53cf0f38":"### MODULES","38a6b7db":"# KITTI - working code below\n> -Avinash\n","29099f48":"\n**Check in repo and find out how to use Flying Chair or Sintel dataset.**\n\n\n\n","d2cd22fe":"#### Flow_utils","eef81f5b":"## **ResNet** Architecture starts from here\n","94b64afe":"Here's the **res_identity** function to create a shallow block containing 2 projection layers at both the ends. In the middle, there is one bottleneck convolutional layer which creates actual feature maps in each block. Projection layers are used for:\n1. 1st projection layer is used to reduce the computational cost of the block by decreasing the depth of the input feature maps.\n2. 2nd projection layer is used to adjust the dimensions of the bottleneck layer's output so that input can be added to the output. ","75249987":"## **To-Do-List**\n\n    Deadline 17-Feb\n\n*   Read the code carefully. ADD COMMENTS\n*   Replce and unzip(using python) the KITTI Dataset\n\n*   Getting problem in TRAINING \n\n\n\n\n","620da444":"### LOSSES","a0c5fe2c":"Here's the **res_conv** function to create a shallow block containing 2 projection layers at both the ends but there is a change in dimensions. To compensate that with the input, the input x is also convolved with the same stride so that both input x and output f(x) can be added together. Thus in this block, overall dimensions are changed but with the help of stride and projection layer."}}