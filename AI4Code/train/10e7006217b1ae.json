{"cell_type":{"e2fc2e44":"code","8b888d3b":"code","1b887246":"code","bab0c317":"code","29991726":"code","df06609d":"code","efd257ed":"code","b626bc52":"code","252cd428":"code","8a743d8f":"code","d4860ae9":"code","1875d07a":"code","1d9b5451":"code","b9622529":"code","0bcd2279":"code","c8c00a21":"code","a53f0094":"code","a7ea1e8a":"code","c95cc5ba":"code","44fdc8e5":"code","493b088b":"code","cb5a4f53":"code","4e3e00ed":"markdown","526ebfd6":"markdown","97790c88":"markdown","b1aae2a1":"markdown","5b3635ea":"markdown","93759bfd":"markdown","1934a805":"markdown","d9a17614":"markdown","e7afa8ba":"markdown","461fd03b":"markdown","4d04755b":"markdown","48c1f925":"markdown","7bde8ca3":"markdown","4e34555b":"markdown","a4be9308":"markdown","7f4e211d":"markdown","2c9f794f":"markdown","f6cb71aa":"markdown"},"source":{"e2fc2e44":"!pip install --upgrade kaggle_environments -q","8b888d3b":"import kaggle_environments\nkaggle_environments.__version__","1b887246":"import numpy as np\nimport math\nimport random\nimport os\nfrom scipy.stats import beta\nimport matplotlib.pyplot as plt\n\nfrom kaggle_environments import make","bab0c317":"random.seed(2020)","29991726":"0.9 * 0.97, 0.9 * 0.97 * 0.97","df06609d":"post_a, post_b, bandit = [None] * 3\ntotal_reward = 0\nc = 3\n\ndef agent(observation, configuration):\n    global total_reward, bandit, post_a, post_b, c\n\n    if observation.step == 0:\n        post_a, post_b = np.ones((2, configuration.banditCount))\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n        # Update Gaussian posterior\n        post_a[bandit] += r\n        post_b[bandit] += 1 - r\n    \n    bound = post_a \/ (post_a + post_b) + beta.std(post_a, post_b) * c\n    bandit = int(np.argmax(bound))\n    \n    return bandit","efd257ed":"env = make(\"mab\")","b626bc52":"env.specification.configuration","252cd428":"%%writefile pull_lever_one.py\ndef pull_lever_one(observation, configuration):\n    return 1","8a743d8f":"env.reset()\ntrainer = env.train([None, \".\/pull_lever_one.py\"])\nobs, reward, done, info = trainer.step(0)","d4860ae9":"obs","1875d07a":"initial_threshold = env.__dict__['state'][0]['observation']['thresholds']\nprint(initial_threshold)","1d9b5451":"obs, reward, done, info = trainer.step(0)\nprint(obs)\nprint(\"-\"*50)\nfinal_threshold = env.__dict__['state'][0]['observation']['thresholds']\nprint(final_threshold)","b9622529":"initial_threshold[0] * 0.97, final_threshold[0]","0bcd2279":"initial_threshold[1] * 0.97, final_threshold[1]","c8c00a21":"# change decay rate to 1\nenv.configuration.decayRate = 1.0\n\nenv.reset()\ntrainer = env.train([None, \".\/pull_lever_one.py\"])\n\n# store initial reward probabiltiies\ninitial_threshold = env.__dict__['state'][0]['observation']['thresholds']\n\nfor _ in range(100):\n    obs, reward, done, info = trainer.step(0)\n\n# store final reward probabiltiies\nfinal_threshold = env.__dict__['state'][0]['observation']['thresholds']","a53f0094":"initial_threshold == final_threshold","a7ea1e8a":"def bayesian_ucb_agent(observation, configuration):\n    global total_reward, action, post_a, post_b, c, my_actions, opp_actions, bound\n\n    if observation.step == 0:\n        post_a, post_b = np.ones((2, configuration.banditCount))\n        \n    else:       \n        r = observation.reward - total_reward\n        total_reward = observation.reward\n        \n        # Update Gaussian posterior\n        post_a[action] += r\n        post_b[action] += 1 - r\n    \n    # upper confidence bound\n    bound = post_a \/ (post_a + post_b) + beta.std(post_a, post_b) * c\n    \n    # maximal action value\n    max_prob = np.max(bound)\n    \n    # pick any action(s) that attain maximal action value\n    action = random.choice(np.argwhere(bound==max_prob))\n    action = int(action)\n    \n    return action","c95cc5ba":"%%time\n\n########## changed set game configurations #########\nenv.configuration.decayRate = 1.0\n####################################################\n\nCONFIGURATION = env.configuration\n\n# initialise global variables\ntotal_reward = 0\naction = None\npost_a = None\npost_b = None\nbound = np.ones(env.configuration.banditCount)\nc = 3\n\nenv.reset()\n\n# let bayesian ucb agent play against random agent\ntrainer = env.train([None, \"random\"])\n\nobs = trainer.reset()\n\n# get initial reward probabilites\ninitial_thresholds = env.__dict__['state'][0]['observation']['thresholds']\n\nfor i in range(1999):\n    action = bayesian_ucb_agent(obs, CONFIGURATION)\n    obs, reward, done, info = trainer.step(action)\n    \n# get final reward probabilites\nfinal_thresholds = env.__dict__['state'][0]['observation']['thresholds']","44fdc8e5":"plt.figure(figsize=(20,8))\nplt.plot(np.arange(100), np.array(final_thresholds)\/100.0)\nplt.plot(np.arange(100), bound)\nplt.xlabel(\"action number\/slot machine number\")\nplt.ylabel(\"upper bound\")\nplt.legend([\"final threshold\", \"upper bound\"])\nplt.show()","493b088b":"%%time\n\n########## changed set game configurations #########\nenv.configuration.decayRate = 0.97\n####################################################\n\nCONFIGURATION = env.configuration\n\n# initialise global variables\ntotal_reward = 0\naction = None\npost_a = None\npost_b = None\nbound = np.ones(env.configuration.banditCount)\nc = 3\n\nenv.reset()\n\n# let bayesian ucb agent play against random agent\ntrainer = env.train([None, \"random\"])\n\nobs = trainer.reset()\n\n# get initial reward probabilites\ninitial_thresholds = env.__dict__['state'][0]['observation']['thresholds']\n\nfor i in range(1999):\n    action = bayesian_ucb_agent(obs, CONFIGURATION)\n    obs, reward, done, info = trainer.step(action)\n    \n# get final reward probabilites\nfinal_thresholds = env.__dict__['state'][0]['observation']['thresholds']","cb5a4f53":"plt.figure(figsize=(20,8))\nplt.plot(np.arange(100), np.array(final_thresholds)\/100.0)\nplt.plot(np.arange(100), bound)\nplt.xlabel(\"action number\/slot machine number\")\nplt.ylabel(\"upper bound\")\nplt.legend([\"final threshold\", \"upper bound\"])\nplt.show()","4e3e00ed":"Create an agent\/algorithm that always pulls the 1-st lever.","526ebfd6":"The reward probabilties after my opponent and I have pulled the levers once each.","97790c88":"I pulled the 0-th lever so the reward probability decayed by a factor of 0.97.","b1aae2a1":"Note how the shape of the upper bound from the bayesian UCB is almost identical to the threshold hidden to us.","5b3635ea":"If I set the decay rate to be 1.0, then no matter how many times 1 pull the lever, the reward probabilities won't decay.","93759bfd":"# Brief explanation of multi-armed bandit (MAB) problem <a id=\"explain\"><\/a>\n\nIf you prefer a more detailed explanation of the MAB problem check out:\n- Classical reinforcement learning textbook: https:\/\/www.andrew.cmu.edu\/course\/10-703\/textbook\/BartoSutton.pdf (pages 27 - 47)\n- https:\/\/www.davidsilver.uk\/wp-content\/uploads\/2020\/03\/XX.pdf\n\n![multi-armed-bandit.jpg](attachment:multi-armed-bandit.jpg)\n\n**Scenario**:\nImagine there are 100 slot machines and at any one time only a single lever can be pulled once. After the lever is pulled, the corresponding slot machine either generates a reward of value 1 or 0. Each machine $i$, where $i=1,\\ldots,100$, has a probability $p_i$ of generating a reward which is unknown to us.\n\n**Goal**:\nThe goal here is to devise a strategy to pull the levers to maximise the rewards gained after 2000 lever pulls.\n\n**Bayesian UCB Strategy**: Try to create a good upper bound for the unknown probabilities $p_i$ using bayesian statistics after exploring the various slot machines. Then consistently pull the lever with the highest _estimated_ probability of generating a reward, i.e. highest $p_i$.\n\n**Problem**:\nThe above is the classical MAB problem and bayesian UCB solution described in most literature, however there's a catch in this Kaggle environment.\n\nIn this competition, whenever a lever is pulled, the unknown reward probability of the pulled lever decays by the factor of 0.97 by default. Also, we're playing with an opponent who is also pulling the levers.\n\nSay suppose the initial unknown reward probability is 0.9, then I pulled the lever of slot machine 0 so the slot machine 0 now has unknown reward probability of 0.873. Suppose my opponent pulls it too then the unknown reward probability further decreases to 0.84681.\n\nUnder such a scenario, the usual bayesian UCB solution is no longer appropriate.\n\n[Back to top](#top)","1934a805":"# Exploring the Kaggle environment <a id=\"explore\"><\/a>\n\nIn essence, there are only 2 main things you\/the agent have knowledge of during the game, the `observation` and the game `configuration`.\n\nDocumentation: https:\/\/github.com\/Kaggle\/kaggle-environments\n\n[Back to top](#top)","d9a17614":"# Plain bayesian UCB is not enough <a id=\"top\"><\/a>\n\nOut of habit, I copied most algorithms from the public notebooks, submitted them, and found that the best public algorithm was the **bayesian upper confidence bound (UCB) algorithm**. However, I have only been able to yield ~900 points on the leaderboard with bayesian UCB and was wondering why it doesn't break the 1000 point barrier.\n\nA rather simple sample code for bayesian UCB is given in: https:\/\/www.kaggle.com\/xhlulu\/santa-2020-ucb-and-bayesian-ucb-starter\n\nIn this notebook, I will show that the bayesian UCB agent performs well if the decay rate is 1 but performs poorly for the default decay rate of 0.97.\n\n**Table of contents**\n- [Brief explanation of multi-armed bandit (MAB) problem](#explain) (Skip this if you are aware of the MAB problem)\n- [Bayesian UCB algorithm](#bayes_ucb)\n- [Exploring the Kaggle environment](#explore) (Skip this if you are aware of the specifications of the kaggle environment)\n- [Bayesian UCB agent against random agent with decay rate = 1](#no_decay)\n- [Bayesian UCB agent against random agent with decay rate = 0.97](#with_decay)","e7afa8ba":"I suspect that many algorithms hovering around 1000 points are plain bayesian UCB algorithms, to beat them, we need to find a way to incorporate the decay rate and the opponent's actions into the bayesian UCB algorithm.\n\n[Back to top](#top)","461fd03b":"The unknown reward probabilities I spoke about previously is hidden here:","4d04755b":"The opponent pulled the 1-st lever so the reward probability decayed by a factor of 0.97.","48c1f925":"The **observation**. \n\n- This is the observation of time `step = 1`\n- Our agent is indexed `agentIndex = 0`\n- I pulled the 0-th lever while the opponent pulled the 1-st lever previously.\n- My previous lever pull generated reward of 0.","7bde8ca3":"**Configuration** simply tells us the setting of the game. \n\nUnder the description of the `decayRate`, it speaks about a `threshold` which is essentially the unknown reward probability $p_i$ I spoke about [here](#explain).","4e34555b":"Reward probabilities remained constant.","a4be9308":"# Bayesian UCB agent against random agent with decay rate = 1 <a id=\"no_decay\"><\/a>\n\nBayesian UCB tries to estimate an upper bound on the reward probabilities. In the code below, `bound[i]` refers to an upper bound of the `threshold[i]`, i.e. reward probability of the slot machine `i`.\n\n[Back to top](#top)","7f4e211d":"The upper bound derived from the UCB is no longer reflective of the hidden reward probabilities anymore.","2c9f794f":"# Bayesian UCB algorithm <a id=\"bayes_ucb\"><\/a>\n\nThe algorithm in https:\/\/www.kaggle.com\/xhlulu\/santa-2020-ucb-and-bayesian-ucb-starter\n\n[Back to top](#top)","f6cb71aa":"# Bayesian UCB agent against random agent with decay rate = 0.97 <a id=\"with_decay\"><\/a>\n\n[Back to top](#top)"}}