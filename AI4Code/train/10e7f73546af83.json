{"cell_type":{"f2f65c46":"code","63e9b930":"code","eae462d7":"code","c5f22f35":"code","82a02c44":"code","369e1b68":"code","5dc3ac75":"code","e2b7b69b":"code","4ba5a5c7":"code","73725c64":"code","e2c074c1":"code","b8219826":"code","070aa560":"code","30293559":"code","51b0ea64":"code","bb406a8a":"code","6a6a16e6":"code","ba7d85f4":"code","ff8e4788":"code","f818ea4c":"code","cfbe2785":"code","02fd869a":"code","536ecca0":"code","9a4595c6":"code","f4865219":"code","4d8782a7":"code","b446b0bc":"code","bacddac7":"code","3aa6f0db":"code","8e9c9d07":"code","fff71683":"code","520a511c":"code","317c8588":"code","0b0ff3c7":"code","19374df4":"code","afd7071d":"code","d21fbb0f":"code","c3b33d61":"code","e97274e7":"code","e77cb7bf":"code","1ce957ad":"markdown","efd3fee3":"markdown","9c901863":"markdown","4921582b":"markdown","b6ccff28":"markdown","b07869c4":"markdown","84bb4198":"markdown","2ca25802":"markdown","68c520e5":"markdown","97d0ce51":"markdown","698bbac2":"markdown","2e21db27":"markdown"},"source":{"f2f65c46":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","63e9b930":"from sklearn.cross_validation import StratifiedKFold\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n#import seaborn as sns\n%matplotlib inline\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nfrom pandas.tools.plotting import parallel_coordinates\nfrom sklearn.decomposition import PCA\nfrom sklearn import ensemble\nfrom sklearn.ensemble import RandomForestClassifier","eae462d7":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","c5f22f35":"print('size of train data',train.shape)\nprint('size of test data',test.shape)","82a02c44":"train.head()","369e1b68":"train.info()","5dc3ac75":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of forest categories(Target Variable)\")\nax = sns.distplot(train[\"Cover_Type\"])","e2b7b69b":"sns.FacetGrid(train, hue=\"Cover_Type\", size=10).map(plt.scatter, \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\").add_legend()","4ba5a5c7":"plt.figure(figsize=(15,15))\nsns.heatmap(train.corr(),fmt=\".2f\",cmap=\"YlGnBu\")","73725c64":"temp = train[['Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Cover_Type']]\nplt.figure(figsize=(15,11))\nparallel_coordinates(temp,'Cover_Type', colormap=plt.get_cmap(\"Set1\"))\nplt.title(\"parallel plots of Hillshade with forest categories\")\nplt.xlabel(\"Hillshade\")\nplt.show()","e2c074c1":"train.describe()","b8219826":"soil_list = []\nsoil_not=[7,8,15,25]\nfor i in range(1, 41):\n    if i not in soil_not:\n       soil_list.append('Soil_Type' + str(i))\n\nwilderness_area_list = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']\n\nprint(soil_list, \"\\n\")\nprint(wilderness_area_list)","070aa560":"pca = PCA(n_components=3) # for simplicity i choose 3\npca_results = pca.fit_transform(train.drop([\"Cover_Type\"], axis=1))\ncmap = sns.cubehelix_palette(as_cmap=True)\ntp, ax = plt.subplots(figsize=(20,15))\ntemp = ax.scatter(pca_results[:,0], pca_results[:,1], c=train.Cover_Type, s=50, cmap=cmap)\ntp.colorbar(temp)\nplt.show()","30293559":"def wilderness_compress(df):\n    \n    df[wilderness_area_list] = df[wilderness_area_list].multiply([1, 2, 3, 4], axis=1)\n    df['Wilderness_Area'] = df[wilderness_area_list].sum(axis=1)\n    return df","51b0ea64":"def soil_compress(df):\n    \n    df[soil_list] = df[soil_list].multiply([i for i in range(1, 37)], axis=1)\n    df['Soil_Type'] = df[soil_list].sum(axis=1)\n    return df","bb406a8a":"df = wilderness_compress(train)\ndf = soil_compress(train)\n\ndf[['Wilderness_Area', 'Soil_Type']].head()","6a6a16e6":"cols = df.columns.tolist()\ncolumns = cols[1:11] + cols[56:]\n\nprint(\"Useful columns: \", columns)\n\nvalues = df[columns]\nlabels = df['Cover_Type']\n\nprint(\"Values: \", values.shape)\nprint(\"Labels: \", labels.shape)","ba7d85f4":"clean = df[['Id', 'Cover_Type'] + columns]\nclean.head()","ff8e4788":"####################### Train data #############################################\ntrain['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\ntrain['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\ntrain['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\n\ntrain['slope_hyd'] = (train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)**0.5\ntrain.slope_hyd=train.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n\n#Mean distance to Amenities \ntrain['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) \/ 3 \n#Mean Distance to Fire and Water \ntrain['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) \/ 2 \n####################### Test data #############################################\ntest['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\ntest['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\ntest['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology\n\ntest['slope_hyd'] = (test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)**0.5\ntest.slope_hyd=test.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n\n#Mean distance to Amenities \ntest['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) \/ 3 \n#Mean Distance to Fire and Water \ntest['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) \/ 2\n","f818ea4c":"train.columns","cfbe2785":"import scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)\n\n#Initialize ChiSquare Class\ncT = ChiSquare(train)\n\n#Feature Selection\ntestColumns = ['Id', 'Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n       'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40',\n       'HF1', 'HF2', 'HR1', 'HR2', 'FR1', 'FR2', 'ele_vert',\n       'slope_hyd', 'Mean_Amenities', 'Mean_Fire_Hyd']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"Cover_Type\" )  \n","02fd869a":"from itertools import combinations\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef add_interactions(df):\n    # Get feature names\n    combos = list(combinations(list(df.columns), 2))\n    colnames = list(df.columns) + ['_'.join(x) for x in combos]\n    \n    # Find interactions\n    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n    df = poly.fit_transform(df)\n    df = pd.DataFrame(df)\n    df.columns = colnames\n    \n    # Remove interaction terms with all 0 values            \n    noint_indicies = [i for i, x in enumerate(list((df == 0).all())) if x]\n    df = df.drop(df.columns[noint_indicies], axis=1)\n    \n    return df","536ecca0":"feature = [col for col in train.columns if col not in ['Cover_Type','Id']]\nX_train = train[feature]\nX_test = test[feature]\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X_train,train['Cover_Type'],test_size=.30)\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nrf=RandomForestClassifier(n_estimators=1100,max_depth=24,oob_score =True, random_state = 42,n_jobs=-1)\nrf.fit(x_train,y_train)\ny_pred=rf.predict(x_test)\nprint(accuracy_score(y_test,y_pred))\nfrom sklearn.cross_validation import cross_val_score,cross_val_predict\nscore=cross_val_score(rf,x_train,y_train,n_jobs=-1,cv=5)\nprint(np.mean(score))","9a4595c6":"plt.figure(figsize=(20,20))\npredictors=list(x_train)\nfeat_imp = pd.Series(rf.feature_importances_, predictors).sort_values(ascending=False)\nfeat_imp.plot(kind='bar', title='Importance of Features')\nplt.xlabel(\"feature\")\nplt.ylabel(\"feature importance\")","f4865219":"from sklearn.cross_validation import cross_val_score,cross_val_predict\ny_pred=cross_val_predict(rf,x_test,y_test,n_jobs=-1,cv=5)\naccuracy_score(y_test,y_pred)","4d8782a7":"etc=ensemble.ExtraTreesClassifier(n_estimators=350,max_features=\"auto\")\netc.fit(x_train,y_train)\ny_pred=rf.predict(x_test)\naccuracy_score(y_test,y_pred)\nscore=cross_val_score(etc,x_train,y_train,n_jobs=-1,cv=5)\nprint(np.mean(score))\n\n#plt.scatter(range(300,2000,100),accuracy)\n#plt.xlabel(\"no of trees\")\n#plt.ylabel(\"score\")\n","b446b0bc":"y_pred=cross_val_predict(etc,x_test,y_test,n_jobs=-1,cv=10)\naccuracy_score(y_test,y_pred)","bacddac7":"from sklearn.learning_curve import validation_curve\ncv = StratifiedKFold(train['Cover_Type'], n_folds=10, shuffle=True, random_state=42)","3aa6f0db":"n_estimators_range = np.linspace(1, 200, 10).astype('int')\ntrain_scores, test_scores = validation_curve(\n    RandomForestClassifier(n_estimators=1100 ,max_depth=20,\n ),\n    X_train, train['Cover_Type'],\n    param_name = 'n_estimators',\n    param_range = n_estimators_range,\n    cv=5,\n    n_jobs=-1,\n    scoring='accuracy'\n)","8e9c9d07":"train_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nfig = plt.figure(figsize=(10, 6), dpi=100)\n\nplt.title(\"Validation Curve with Random Forest (eta = 0.3)\")\nplt.xlabel(\"number of trees\")\nplt.ylabel(\"Accuracy\")\nplt.ylim(0.7, 1.1)\n\nplt.plot(n_estimators_range,\n             train_scores_mean,\n             label=\"Training score\",\n             color=\"r\")\n\nplt.plot(n_estimators_range,\n             test_scores_mean, \n             label=\"Cross-validation score\",\n             color=\"g\")\n\nplt.fill_between(n_estimators_range, \n                 train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, \n                 alpha=0.2, color=\"r\")\n\nplt.fill_between(n_estimators_range,\n                 test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std,\n                 alpha=0.2, color=\"g\")\n\nplt.axhline(y=1, color='k', ls='dashed')\n\nplt.legend(loc=\"best\")\nplt.show()\n\ni = np.argmax(test_scores_mean)\nprint(\"Best cross-validation result ({0:.2f}) obtained for {1} trees\".format(test_scores_mean[i], n_estimators_range[i]))\n","fff71683":"n_estimators_range = np.linspace(1, 200, 10).astype('int')\ntrain_scores, test_scores = validation_curve(\n    RandomForestClassifier(n_estimators=1100 ,max_depth=20,\n ),\n    X_train, train['Cover_Type'],\n    param_name = 'n_estimators',\n    param_range = n_estimators_range,\n    cv=cv,\n    n_jobs=-1,\n    scoring='accuracy'\n)","520a511c":"train_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nfig = plt.figure(figsize=(10, 6), dpi=100)\n\nplt.title(\"Validation Curve with Random Forest (eta = 0.3)\")\nplt.xlabel(\"number of trees\")\nplt.ylabel(\"Accuracy\")\nplt.ylim(0.7, 1.1)\n\nplt.plot(n_estimators_range,\n             train_scores_mean,\n             label=\"Training score\",\n             color=\"r\")\n\nplt.plot(n_estimators_range,\n             test_scores_mean, \n             label=\"Cross-validation score\",\n             color=\"g\")\n\nplt.fill_between(n_estimators_range, \n                 train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, \n                 alpha=0.2, color=\"r\")\n\nplt.fill_between(n_estimators_range,\n                 test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std,\n                 alpha=0.2, color=\"g\")\n\nplt.axhline(y=1, color='k', ls='dashed')\n\nplt.legend(loc=\"best\")\nplt.show()\n\ni = np.argmax(test_scores_mean)\nprint(\"Best cross-validation result ({0:.2f}) obtained for {1} trees\".format(test_scores_mean[i], n_estimators_range[i]))\n","317c8588":"sub = pd.DataFrame({\"Id\": test['Id'],\"Cover_Type\": etc.predict(X_test)})\nsub.to_csv(\"etcnew.csv\", index=False) ","0b0ff3c7":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score","19374df4":"xgb=XGBClassifier(n_estimators=1100, max_depth=25)","afd7071d":"xgb.fit(x_train,y_train)\ny_pred=xgb.predict(x_test)\nprint(accuracy_score(y_test,y_pred))\nscore=cross_val_score(xgb,x_train,y_train,n_jobs=-1,cv=5)\nprint(np.mean(score))","d21fbb0f":"train_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nfig = plt.figure(figsize=(10, 6), dpi=100)\n\nplt.title(\"Validation Curve with XGBoost (eta = 0.3)\")\nplt.xlabel(\"number of trees\")\nplt.ylabel(\"Accuracy\")\nplt.ylim(0.7, 1.1)\n\nplt.plot(n_estimators_range,\n             train_scores_mean,\n             label=\"Training score\",\n             color=\"r\")\n\nplt.plot(n_estimators_range,\n             test_scores_mean, \n             label=\"Cross-validation score\",\n             color=\"g\")\n\nplt.fill_between(n_estimators_range, \n                 train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, \n                 alpha=0.2, color=\"r\")\n\nplt.fill_between(n_estimators_range,\n                 test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std,\n                 alpha=0.2, color=\"g\")\n\nplt.axhline(y=1, color='k', ls='dashed')\n\nplt.legend(loc=\"best\")\nplt.show()\n\ni = np.argmax(test_scores_mean)\nprint(\"Best cross-validation result ({0:.2f}) obtained for {1} trees\".format(test_scores_mean[i], n_estimators_range[i]))\n","c3b33d61":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.utils import np_utils","e97274e7":"X_train \ny_train = train.Cover_Type\ny_train = np_utils.to_categorical(y_train)","e77cb7bf":"# Create model\nmodel = Sequential()\nmodel.add(Dense(1024, input_dim=64, kernel_initializer='uniform', activation='selu'))\nmodel.add(Dense(512, kernel_initializer='uniform', activation='softplus'))\nmodel.add(Dense(256, kernel_initializer='uniform', activation='elu'))\nmodel.add(Dense(128, kernel_initializer='uniform', activation='selu'))\nmodel.add(Dense(64, kernel_initializer='uniform', activation='softplus'))\nmodel.add(Dense(32, kernel_initializer='uniform', activation='elu'))\nmodel.add(Dense(16, kernel_initializer='uniform', activation='softplus'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Fit model\nmodel.fit(X_train, y_train, epochs=10000, batch_size=32)","1ce957ad":"Features engeneering :\nLinear combination :\n * Elevation, Vertical dist. to Hydrology\n * Horizontal dist. to Hydrology, Horizontal dist. to Fire Points \n * Horizontal dist. to Hydrology, Horizontal dist. to Roadways \n * Horizontal dist. to Fire Points, Horizontal dist. to Roadways \nEuclidean distance :\n  * Horizontal dist. to Hydrology, Vertical dist. to Hydrology\n  * euclidean distance = \\sqrt{(verticaldistance)^2 + (horizontaldistance)^2}\\] \nDistance to Amenities :\nAs we know distances to amenities like Water, Fire and Roadways played a key role in determining cover type\nMean distance to Amenities\nMean Distance to Fire and Water","efd3fee3":"# So here are we with Forest Cover Analysis , that on various features we have to predict at least seven types of covers given to us ,\n## Feature engg , polynomial features with pca and xgboost apart from lgbm we can get best result since being a handling various dataset, this problem is merely on few featured ensembel models\n## Decision tree , base line model\n## random forest , stackers,\n## gbm , ada, xgboost and lgbm\n## with taste of PCA too .\n## evauation metrics will be accuracy , AUC , logloss and checking , variance and bias in final model\u00b6","9c901863":"# From correlation analysis we can see that very few featues are depicting relation with one another a l like graph can been from top to bottom on LHS , VERY FEW soil type are showing some corrleation with another and other var as we can see some white lines has been drawn up this due tp heatmap scaling of points","4921582b":"# the game changer move","b6ccff28":"# stay tuned light gb , and more visualization are coming","b07869c4":"Parallel plots\nA hillshade is a grayscale 3D representation of the surface, with the sun's relative position taken into account for shading the image. This function uses the altitude and azimuth properties to specify the sun's position.\n\nHillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\nHillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\nHillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n","84bb4198":"# As we can see that we are provided with non null dataset so we can more further by undertsanding outlier , feature scaling etc","2ca25802":"# Plotting Roc Curves for each classes ","68c520e5":"# Before and after result of using startified kfold , i will be usnig startifield to predict the test data too , building the code and below it will be placed","97d0ce51":"# Combining features for better visualization to get insights how cover is dependent pn soil and various other features","698bbac2":"# AS we can see that mean vs max values lot of gap , outliers are here lot's of\n## three types of elevation cover we can see with the help of quartiles\n## now some interesting visualization by means of using correlation analysis\n## as we can see slope is increasing and elevation to we are getting various forest covers so this variable is nececssary to know which type of cover","2e21db27":"# now we train and test data shuffled with stratified manner"}}