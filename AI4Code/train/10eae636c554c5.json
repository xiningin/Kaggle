{"cell_type":{"17f8fdb4":"code","6d9541f5":"code","1013d0c7":"code","2f47f3ed":"code","f90f8af1":"code","ef5f69a4":"code","1a903339":"code","4698614a":"code","d9623c04":"code","721c7051":"code","0f926d9d":"code","7e9e39a6":"code","3d2a3a5b":"code","5ebd5c87":"code","c59a6280":"code","4215b860":"code","50457ab9":"code","3cef45ed":"code","f95eab22":"code","aa1c4665":"code","06baf3f6":"code","81115cfc":"code","94e56646":"code","2a4c3b1e":"code","c1ac5ef0":"code","85d3ecf5":"code","a1d2b83e":"code","44af8af5":"code","fa1da8ef":"code","3722d1e6":"code","16864e5f":"code","cc638b96":"code","e6518169":"code","fc2a5c54":"code","bf965c5d":"code","0c1da9c5":"code","f3321792":"code","75c9e47c":"code","352bd5d8":"code","7650f469":"code","b4921b0a":"code","2e17ea0c":"markdown","2727d324":"markdown","ab4cc155":"markdown","636629e6":"markdown","ff530d2a":"markdown","445cc351":"markdown","444eb38b":"markdown","8b984c16":"markdown","fc6eb907":"markdown","d1a311d2":"markdown","fb587b6d":"markdown","f4f4fcac":"markdown","30619ad9":"markdown","8ffcc4f7":"markdown","2c894139":"markdown","428ef097":"markdown","84f7f84f":"markdown","aa723453":"markdown","91b4527a":"markdown","6af7d81f":"markdown"},"source":{"17f8fdb4":"import csv\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import ShuffleSplit, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.metrics import silhouette_samples, silhouette_score, homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, fowlkes_mallows_score\nfrom scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \nfrom sklearn.cluster import DBSCAN \nfrom sklearn.manifold import TSNE\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport scipy.cluster.hierarchy as shc\n","6d9541f5":"df=pd.read_csv('..\/input\/anuran-calls\/Frogs_MFCCs.csv')\ndf","1013d0c7":"df.info()","2f47f3ed":"#see how many unique values for each col\ndf.nunique()","f90f8af1":"#there is no Nan values\ndf.isnull().sum()","ef5f69a4":"df.describe()","1a903339":"sns.set_style(\"dark\")\nsns.countplot(x=\"Family\", data=df, palette=sns.color_palette(\"husl\", 8), saturation=10)","4698614a":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(12, 6),\n            'xtick.labelsize':20,\n            'ytick.labelsize':20})\n\nsns.set(style=\"white\",font_scale=0.8)\n\n\nsns.set_style(\"dark\")\nsns.countplot(x=\"Genus\", data=df, palette=sns.color_palette(\"husl\", 8), \n              saturation=10, edgecolor=(0,0,0), linewidth=2)","d9623c04":"# library\nimport matplotlib.pyplot as plt\nfrom palettable.colorbrewer.qualitative import Pastel1_7\n\n# create data\nnames=list(df[\"Species\"].unique())\nsizes=[df[\"Species\"].value_counts()[unique_class]*100\/len(df[\"Species\"]) for unique_class in names]\ncolors = Pastel1_7.hex_colors\nexplode = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)  # explode a slice if required\n\nplt.pie(sizes, explode=explode, labels=names, colors=colors,\n        autopct='%1.1f%%', shadow=True)\n        \n#draw a circle at the center of pie to make it look like a donut\ncentre_circle = plt.Circle((0,0), 0.50, color='black', fc='white',linewidth=0.70)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n\n# Set aspect ratio to be equal so that pie is drawn as a circle.\nplt.axis('equal')\nplt.show()","721c7051":"#see values of cat features\ncat_feature_col=[\"Family\", \"Genus\", \"Species\"]\nfor i in cat_feature_col:\n    print(f\"{i} : {df[i].unique()}\")\n    print(df[i].value_counts())\n    print(\"----------------------------------------------------------------\")","0f926d9d":"#plot distribution of numerical attributes\ncont_feature_col=df.drop(cat_feature_col, axis=1).columns.to_list()\ncont_df = df[cont_feature_col]\nplt.figure(figsize=(20,20))\nfor i in range(1, 24):\n    plt.subplot(6, 4, i)\n    sns.distplot(cont_df[cont_df.columns[i-1]], bins=14, kde_kws={'bw': 0.1}, color=\"r\")\n","7e9e39a6":"# display boxlots\nplt.figure(figsize=(30,10))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(cont_df.drop('RecordID', axis=1)))\nplt.xticks(rotation=90)\nplt.show()","3d2a3a5b":"# find the IQR\nq1 = df[cont_feature_col].quantile(.25)\nq3 = df[cont_feature_col].quantile(.75)\nIQR = q3-q1\n\noutliers_df = np.logical_or((df[cont_feature_col] < (q1 - 1.5 * IQR)), (df[cont_feature_col] > (q3 + 1.5 * IQR))) \n\noutlier_list=[]\ntotal_outlier=[]\nfor col in list(outliers_df.columns):\n    try:\n        total_outlier.append(outliers_df[col].value_counts()[True])\n        outlier_list.append((outliers_df[col].value_counts()[True] \/ outliers_df[col].value_counts().sum()) * 100)\n    except:\n        outlier_list.append(0)\n        total_outlier.append(0)\n        \noutlier_list\n\noutlier_df=pd.DataFrame(zip(list(outliers_df.columns), total_outlier, outlier_list), columns=['name of the column', 'total', 'outlier(%)'])\n\n#see totally how many outliers in cont features\noutlier_df.set_index('name of the column', inplace=True)\n#del outlier_df.index.name\noutlier_df","5ebd5c87":"df_cont=df[cont_feature_col]\nout_nan_df=df_cont[~outliers_df]\nout_nan_df","c59a6280":"for col in cont_feature_col:\n  col_mean=df[col].mean() #calculate mean for each col\n  out_nan_df[col]=out_nan_df[col].fillna(col_mean) #first convert outliers to Nan values then fill Nan's with col mean\n  #df[cont_feature_col]=df_cont","4215b860":"df_only_cat=df.drop(columns=cont_feature_col)","50457ab9":"#concat df_only_cat and clear cont_df of outliers\ndf_final=pd.concat([out_nan_df, df_only_cat], axis=1)","3cef45ed":"df_final","f95eab22":"df_final.describe()","aa1c4665":"#minmax scaling\ncont_cols = df_final.columns.difference(['Family','Genus', 'Species'])\n\nscaler = MinMaxScaler()\ncont_cols_df = pd.DataFrame(scaler.fit_transform(df_final[cont_cols]), \n                                columns=cont_cols, \n                                index=df_final.index)","06baf3f6":"#label encoding categorical features (str-->float)\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nle.fit(df_final[\"Family\"])\ncat_cols_arr=le.transform(df_final[\"Family\"])\n\ncat_cols_df=pd.DataFrame(cat_cols_arr, columns=[\"Family\"])","81115cfc":"#merge cont&cat dfs\ndff=pd.concat([cont_cols_df, cat_cols_df], axis=1)","94e56646":"#heatmap for correlation coefficient\n\n# calculate correlation\ndf_corr = dff.corr()\n\n# correlation matrix\nsns.set(font_scale=0.8)\nplt.figure(figsize=(20,16))\nsns.heatmap(df_corr, annot=True, fmt=\".4f\",vmin=-1, vmax=1, linewidths=.5, cmap = sns.diverging_palette(145, 300, s=60, as_cmap=True))\n\n#plt.yticks(rotation=0)\nplt.show()","2a4c3b1e":"#X, y splitting\nX_imp=df_final.iloc[:,:23]\ny_imp=df_final.iloc[:,-1]\n\n#feature importances\nfrom sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators = 100, max_depth=5)\nrf_clf.fit(X_imp, y_imp)\n\npd.Series(rf_clf.feature_importances_, index = X_imp.columns).nlargest(24).plot(kind = 'pie',\n                                                                                figsize = (8, 8),\n                                                                                title = 'Feature importance from RandomForest', colormap='twilight', fontsize=10)","c1ac5ef0":"dff.drop(columns=[\"MFCCs_18\"], inplace=True)","85d3ecf5":"# run k-means for range of 10 clusters then analyse with elbow method\nclusters = []\n\n#drop target\nX=dff.drop(columns=[\"Family\"])\n#y=dff[\"Family\"]\ny = dff.iloc[:,-1].values.reshape(1, -1)\n\n#Sum of squared distances of samples to their closest cluster center.\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i).fit(X)\n    clusters.append(km.inertia_)\n\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=list(range(1, 11)), y=clusters, ax=ax)\nax.set_title('--- Searching for Elbow ---')\nax.set_xlabel('Clusters')\nax.set_ylabel('Inertia')\n\nplt.show()","a1d2b83e":"#PCA for reducing dimensions to 3\npca = PCA(n_components=3)\npca_X = pca.fit_transform(X)\n\n\ncolors = np.array([x for x in 'bgrcmykbgr'])\n#running k-means on resuts of pca\nkm_pca = KMeans(n_clusters=4).fit(pca_X)\nfig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1], z=pca_X[:,2],color=colors[km_pca.labels_])\nfig.show()","44af8af5":"colors = np.array([x for x in 'ykbgrpcm'])\n#running k-means on resuts of pca\nkm_pca_2 = KMeans(n_clusters=7).fit(pca_X)\nfig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1], z=pca_X[:,2],color=colors[km_pca_2.labels_])\nfig.show()","fa1da8ef":"dff['KMEANS_4']=km_pca.labels_","3722d1e6":"dff","16864e5f":"colors = np.array([x for x in 'bgrc'])\nfig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1], z=pca_X[:,2],color=colors[dff[\"Family\"]])\nfig.show()","cc638b96":"#define a function to plot silhouette values of clusters\ndef silhouette_plot(X, y, n_clusters, ax=None):\n    if ax is None:\n        ax = plt.gca()\n\n    # Compute the silhouette scores for each sample\n    silhouette_avg = silhouette_score(X, y)\n    sample_silhouette_values = silhouette_samples(X, y)\n\n    y_lower = padding = 2\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        ith_cluster_silhouette_values = sample_silhouette_values[y == i]\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax.fill_betweenx(np.arange(y_lower, y_upper),\n                         0,\n                         ith_cluster_silhouette_values,\n                         facecolor=color,\n                         edgecolor=color,\n                         alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i + 1))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + padding\n\n    ax.set_xlabel(\"The silhouette coefficient values\")\n    ax.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhoutte score of all the values\n    ax.axvline(x=silhouette_avg, c='r', alpha=0.8, lw=0.8, ls='-')\n    ax.annotate('Average',\n                xytext=(silhouette_avg, y_lower * 1.025),\n                xy=(0, 0),\n                ha='center',\n                alpha=0.8,\n                c='r')\n\n    ax.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n    ax.set_ylim(0, y_upper + 1)\n    ax.set_xlim(-0.075, 1.0)\n    return ax","e6518169":"# plot silhouette coefficients\nplt.figure(figsize=(20,30))\nfor i in range(2,11):  \n    agnes = AgglomerativeClustering(n_clusters=i, linkage='average')\n    agnes_labels = agnes.fit_predict(pca_X)\n    plt.subplot(5, 2, i-1)\n    silhouette_plot(pca_X, agnes_labels, i)","fc2a5c54":"#2 clusters\ncolors = np.array([x for x in 'ykbgrpcm'])\nagnes2 = AgglomerativeClustering(n_clusters=2, linkage='average').fit(pca_X)\n\nfig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1],z=pca_X[:,2], color=colors[agnes2.labels_])\nfig.show()","bf965c5d":"#4 clusters\ncolors = np.array([x for x in 'grpcmykb'])\nagnes4 = AgglomerativeClustering(n_clusters=4, linkage='average').fit(pca_X)\n\nfig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1],z=pca_X[:,2], color=colors[agnes4.labels_])\nfig.show()","0c1da9c5":"plt.figure(figsize=(10, 7))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(X, method='ward'))","f3321792":"dff['AGNES_4']=agnes4.labels_","75c9e47c":"# search for best parameters by using silhouette_score\nscore_list=[]\nfor eps in np.arange(0.5, 20, 0.5):\n    for min_samples in range(3, 20):\n        db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n        labels = db.labels_\n        n = len(np.unique(labels))\n        if n>1:\n            score=silhouette_score(X, labels)\n            score_list.append((score, (eps, min_samples)))\n     \nbiggest_score = sorted(score_list)[-1]  \nbest_eps, best_min = biggest_score[1]\nbest_eps, best_min\n","352bd5d8":"# best model for DBSCAN\ndb_best = DBSCAN(eps=best_eps, min_samples=best_min).fit(X)\n\n#best clusters\ncolors = np.array([x for x in 'rpcmykgb'])\n\nfig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1],z=pca_X[:,2], color=colors[db_best.labels_])\nfig.show()","7650f469":"dff['DBSCAN_best']=db_best.labels_","b4921b0a":"# print metric for chosen models\nmodels = [km_pca, agnes4, db_best] \nnames = [\"K-MEANS:\", \"AGNES:\", \"DBSCAN:\"]\n\n\nfor i, model in enumerate(models):\n    labels = model.labels_\n    n = len(np.unique(labels))\n    y_scaled = np.round(MinMaxScaler((0, n)).fit_transform(y)).ravel()\n    \n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    print(names[i])\n    print('Estimated number of clusters: %d' % n_clusters_)\n    print('Estimated number of noise points: %d' % n_noise_)\n    print(\"Homogeneity: %0.3f\" % homogeneity_score(y_scaled, labels))\n    print(\"Fowlkes-Mallows score: %0.3f\"\n          % fowlkes_mallows_score(y_scaled, labels))\n    print(\"Silhouette Coefficient: %0.3f\"\n          % silhouette_score(X, labels))\n    print(\"\\n######################################\\n\")","2e17ea0c":"# 1. K-MEANS","2727d324":"# DBSCAN","ab4cc155":"# NORMALIZATION","636629e6":"Now let's make comparison between K-MEANS - AGNES - DBSCAN clustering algorithms, since we have ground truth values for \"Family\" attribute.","ff530d2a":"It seems that cluster size = 2 will be chosen amongst others because they have not balanced clusters and they have negative silhouette scores. ","445cc351":"# EDA","444eb38b":"Decide which cluster belongs which family.","8b984c16":"MinMax Scaling","fc6eb907":"# FEATURE SELECTION","d1a311d2":"# EVALUATION","fb587b6d":"# **CLUSTERING**","f4f4fcac":"- Data Set Information:\n\nThis dataset was used in several classifications tasks related to the challenge of anuran species recognition through their calls. It is a multilabel dataset with three columns of labels. \n\nThis dataset was created segmenting 60 audio records belonging to 4 different families, 8 genus, and 10 species. Each audio corresponds to one specimen (an individual frog), the record ID is also included as an extra column. We used the spectral entropy and a binary cluster method to detect audio frames belonging to each syllable. The segmentation and feature extraction were carried out in Matlab. After the segmentation we got 7195 syllables, which became instances for train and test the classifier. These records were collected in situ under real noise conditions (the background sound). \n\nSome species are from the campus of Federal University of Amazonas, Manaus, others from Mata Atl\u00c3\u00a2ntica, Brazil, and one of them from C\u00c3\u00b3rdoba, Argentina. The recordings were stored in wav format with 44.1kHz of sampling frequency and 32bit of resolution, which allows us to analyze signals up to 22kHz. From every extracted syllable 22 MFCCs were calculated by using 44 triangular filters. These coefficients were normalized between -1 and 1. \n\n- Attribute Information:\n\nMel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an mel-frequency cepstrum (MFC). Due to each syllable has different length, every row (i) was normalized to MFCCs_i \/ [max(abs(MFCCs_i))].","30619ad9":"According to elbow method results, we tried with 4 and 7 clusters and saw that 4 will be the best for us. ","8ffcc4f7":"Since MFCCs_18 has really low importance and lowe correlation between target, we need to drop it.","2c894139":"# Anuran Calls\ud83d\udc38\ud83c\udfbb\ud83c\udfb7\ud83c\udfbc\ud83c\udfba (MFCCs) Clustering\ud83d\udc38","428ef097":"- Since there is no inertia in AGNES we will be using silhouette coefficient (best --> 1, worst --> -1) to select cluster number of AGNES.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score\n","84f7f84f":"# 2. AGNES","aa723453":"**Feature Selection with RF**","91b4527a":"- https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#clustering-performance-evaluation\n\nWe will compare best of 3 models by using 8 metrics:\n\n- Estimated number of clusters\n- Estimated number of noise points\n\n- Homogeneity: For perfect clustering, each cluster contains only members of a single class.\n\n- Fowlkes-Mallows score: Geometric mean of precision and recall.\n\n- Silhouette Coefficient: If the ground truth labels are not known, the Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample by (b - a) \/ max(a, b)","6af7d81f":"- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.DBSCAN.html\n- eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other (default: 0.5).\n\n- min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself (default: 5)."}}