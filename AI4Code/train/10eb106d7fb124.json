{"cell_type":{"d9eb4165":"code","125636f9":"code","db40521d":"code","a8fdf308":"code","44fc2cbf":"code","be77a618":"code","af0686ab":"code","c966852a":"code","0b3dfddc":"markdown","98826789":"markdown","eab34cfe":"markdown","2e99d9e4":"markdown","e2340da6":"markdown","f114dc9a":"markdown"},"source":{"d9eb4165":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns","125636f9":"data = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_features.csv\")\ndata.head()","db40521d":"data = data.drop(columns = [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"])\ndata.shape","a8fdf308":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 50).fit(data)\ndata_transformed = pd.DataFrame(pca.transform(data))","44fc2cbf":"data_transformed","be77a618":"## Variance explained\nvariance = pca.explained_variance_ratio_\n\nprint('Explained variation per principal component: {0}'.format(variance))","af0686ab":"# calculate variance ratios\nvariance_ratio = np.cumsum(np.round(variance, decimals=3)*100)\n\nprint('Explained cumulative variation for 30 principal components: {0}'.format(variance_ratio))","c966852a":"plt.figure(figsize=(15, 8))\nplt.ylabel('% Variance Explained')\nplt.xlabel('# of Features')\nplt.title('PCA Analysis')\nplt.ylim(0,100)\nplt.style.context('seaborn-whitegrid')\nplt.plot(variance_ratio)\nplt.show()","0b3dfddc":"## The curse of dimensionality:\n#### The dimensionality of a model is the number of predictor or input variables used by the model. The curse of dimensionality is the affliction caused by adding variables to multivariate data models. As more variables are added, the data space becomes more sparse. One way to think about it is consider a position on the chess board which is in two dimension (8x8) has moves within 64 possible places. By adding one more dimension to it and making it (8x8x8) we make the number of possible moves to 512, which is increase by 800%.\n\n### One key step in data mining is therefore to find a way to reduce the number dimension with minimum loss of accuracy.","98826789":"# PCA\n### Principal component analysis (PCA) is one of the oldest and most commonly used projection algorithm in machine learning. It linearly projects high dimentional multivariate numeric data, with possibly correlated features, into a set of lower orthogonal, uncorrelated dimentions where the first dimension captures most of the variance, next dimension, while being orthogonal to the first, captures the remaining variance, and so on.\n\n#### Note that PCA is more suitable for numeric data. For use of categorical data, other methods such as correspondence analysis are more suitable.","eab34cfe":"## Intution:\n#### The real information in the data from a statistical perspective is the variability in it. So any projection that maximizes the variance in the projected space is considered the first principal component (direction of projection). \n\n![image.png](attachment:image.png)\n\nThis example is taken from the interactive exampe of https:\/\/setosa.io\/ev\/principal-component-analysis\/","2e99d9e4":"## How PCA Works?\nListed below are the 6 general steps for performing a principal component analysis, which we will investigate in the following sections.\n\n- Take the whole dataset consisting of d-dimensional samples ignoring the class labels\n- Compute the d-dimensional mean vector (i.e., the means for every dimension of the whole dataset)\n- Compute the scatter matrix (alternatively, the covariance matrix) of the whole data set\n- Compute eigenvectors (e1,e2,...,ed) and corresponding eigenvalues (\u03bb1,\u03bb2,...,\u03bbd)\n- Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d\u00d7k dimensional matrix W(where every column represents an eigenvector)\n- Use this d\u00d7k eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the mathematical equation: y = WT\u00d7x (where x is a d\u00d71-dimensional vector representing one sample, and y is the transformed k\u00d71-dimensional sample in the new subspace.)","e2340da6":"# References:\n1. https:\/\/towardsdatascience.com\/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n2. https:\/\/setosa.io\/ev\/principal-component-analysis\/","f114dc9a":"### In the train dataset, we have 872 numeric columns. We will apply PCA on this dataset to reduce the dimensionality of this dataset."}}