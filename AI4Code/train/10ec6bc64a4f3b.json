{"cell_type":{"18539fcb":"code","9f8f46a7":"code","6684f581":"code","6b35f028":"code","17a254d1":"code","2baa7320":"code","1bb40e8e":"code","8ae1c923":"code","e37da799":"code","bb0d186b":"code","5c47b6f6":"code","7eda9343":"code","1414c9f6":"code","83b82be8":"code","c227e6ea":"code","b4177353":"code","8ab309ac":"code","01f09efb":"code","b4db43f9":"code","96977248":"code","8b1e74bb":"code","fc2092be":"code","ce9e5468":"code","44250f2e":"code","3d0128f7":"code","0987d8de":"code","a8f167e4":"code","483e486e":"code","b70a8c88":"code","5f7d15e8":"code","b79e238f":"code","6ab66e89":"code","0ac37481":"code","7899cbf1":"code","5191fc23":"code","32d21852":"code","bbf1e0bd":"code","3263fccc":"code","7996962e":"code","6cb51690":"code","efc55f61":"code","dae906a1":"code","ffac5867":"code","4433dc60":"code","e7a9a9f6":"code","f2c7e10d":"code","ca617e43":"code","2ce21297":"code","ffc1a708":"code","fd44283f":"code","21957220":"code","6e97f220":"markdown","8cab4404":"markdown","70a4e3b0":"markdown","d7a1a676":"markdown","6f6c7216":"markdown","710bcaab":"markdown","0e47a305":"markdown","4716f6f8":"markdown","a65b5a2f":"markdown","e5c3bc48":"markdown","2ab63133":"markdown","50f626d5":"markdown","fc9e07f5":"markdown","179dcf11":"markdown","95ef1a3f":"markdown","fdc73ca9":"markdown","ce0e6784":"markdown","932f801f":"markdown","9fc569d4":"markdown","d216e328":"markdown","830b69f2":"markdown","22b118c5":"markdown","149c53db":"markdown"},"source":{"18539fcb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime,random\nimport lightgbm as lgb\nimport scipy.stats as stats\nimport gc\nimport warnings\nfrom time import time\nfrom sklearn import metrics\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline","9f8f46a7":"def dataframe_cat_feature_summary(df):\n    summary = pd.DataFrame(df.dtypes,columns = [\"dtypes\"])\n    summary = summary.reset_index()\n    summary[\"Feature\"] = summary[\"index\"]\n    summary = summary[[\"Feature\",\"dtypes\"]]\n    summary[\"Missed\"] = df.isna().sum().values\n    df_cat_describe = df.describe()\n    summary[\"Count\"] = df_cat_describe.iloc[0,:].values\n    summary[\"% Missed\"] = (summary[\"Missed\"] \/ df.shape[0]) * 100\n    summary[\"% Missed\"] = round(summary[\"% Missed\"], 2)\n    summary[\"Unique\"] = df_cat_describe.iloc[1,:].values\n    summary[\"Top\"] = df_cat_describe.iloc[2,:].values\n    summary[\"Freq\"] = df_cat_describe.iloc[3,:].values\n    \n    return summary\n\ndef dataframe_num_feature_summary(df):\n    summary = pd.DataFrame(df.dtypes,columns = [\"dtypes\"])\n    summary = summary.reset_index()\n    summary[\"Feature\"] = summary[\"index\"]\n    summary = summary[[\"Feature\",\"dtypes\"]]\n    summary[\"Missed\"] = df.isna().sum().values\n    df_num_describe = df.describe()\n    summary[\"Count\"] = df_num_describe.iloc[0,:].values\n    summary[\"% Missed\"] = (summary[\"Missed\"] \/ df.shape[0]) * 100\n    summary[\"% Missed\"] = round(summary[\"% Missed\"], 2)\n    summary[\"Mean\"] = df_num_describe.iloc[1,:].values\n    summary[\"Std\"] = df_num_describe.iloc[2,:].values\n    summary[\"Min\"] = df_num_describe.iloc[3,:].values\n    summary[\"Max\"] = df_num_describe.iloc[7,:].values\n    summary[\"25 %\"] = df_num_describe.iloc[4,:].values\n    summary[\"75 %\"] = df_num_describe.iloc[6,:].values\n    \n    return summary\n\ndef impute(df, columns_to_impute, imputer):\n    imputed_df = pd.DataFrame(imputer.fit_transform(df[columns_to_impute]))\n    imputed_df.columns = columns_to_impute # From Kaggele Cources to work with missing values\n    df[columns_to_impute] = imputed_df\n\ndef ploting_cnt_amt_m(df, column):\n    fraud_explanation = pd.crosstab(df[column], df[\"isFraud\"], normalize = \"index\") * 100\n    fraud_explanation = fraud_explanation.reset_index()\n    fraud_explanation.rename(columns={0:\"NoFraud\", 1:\"Fraud\"}, inplace = True)\n    \n    plt.figure(figsize=(20,10))\n    plt.subplot(211)\n    plt.suptitle(f\"{column} Dist\" , fontsize=24)\n    \n    order_of_values_on_charts = df[column].unique()\n    \n    chart_feature = sns.countplot(x = column,  data = df, order = order_of_values_on_charts)\n    chart_feature.set_xticklabels(chart_feature.get_xticklabels(),rotation=45,horizontalalignment = \"right\")\n    \n    chart_fraud = chart_feature.twinx()\n    chart_fraud = sns.pointplot(x = column, y = \"Fraud\", data = fraud_explanation, order = order_of_values_on_charts, color = \"red\")\n    \n    plt.show()\n    \ndef ploting_numeric_features(df, column):\n    plt.figure(figsize=(20,10))\n    plt.subplot(211)\n    percent_of_nan = np.round(100*np.sum(df[column].isna())\/df.shape[0],2)\n    plt.suptitle(column +' has '+str(df[column].nunique())+' '+'values'+' and '+str(percent_of_nan)+'% nan' , fontsize=24)\n    h = plt.hist(df[column].dropna(),bins=50) \n    plt.show()","6684f581":"SEED = 42\n\ndef seed_env(seed=SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\npd.set_option('display.max_columns', 380)\npd.set_option('display.max_rows', 500)\nseed_env()","6b35f028":"features_to_delete = list()","17a254d1":"train_transactions_df = reduce_mem_usage(pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv'))\ntrain_identity_df = reduce_mem_usage(pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv'))\ntrain_df = pd.merge(train_transactions_df,train_identity_df,on = \"TransactionID\", how = \"left\")\ntarget = train_df[\"isFraud\"].copy()\n\ndel train_transactions_df, train_identity_df\ngc.collect()\n\ntrain_df.drop([\"TransactionID\",\"isFraud\"], axis=1, inplace = True)\n\ntrain_df.head()","2baa7320":"def list_of_cat_features(base_name,feature_range):\n    return [base_name + str(i) for i in feature_range]","1bb40e8e":"categorical_features = [\"ProductCD\",\"P_emaildomain\",\"R_emaildomain\",\"DeviceType\",\"DeviceInfo\"]\n\ncategorical_features.extend(list_of_cat_features(\"card\",range(1,7)))\ncategorical_features.extend(list_of_cat_features(\"addr\",range(1,3)))\ncategorical_features.extend(list_of_cat_features(\"M\",range(1,10)))\ncategorical_features.extend(list_of_cat_features(\"id_\",range(12,39)))","8ae1c923":"numeric_features = list(set(train_df.columns.tolist()) - set(categorical_features))","e37da799":"params = {'num_leaves': 2**8,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': SEED\n         }","bb0d186b":"non_important_features = set()\n\nn_splits = 2\nfolds = KFold(n_splits = n_splits)","5c47b6f6":"for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    train, y_train_df = train_df.iloc[trn_idx], target.iloc[trn_idx]\n    valid, y_valid_df = train_df.iloc[val_idx], target.iloc[val_idx]\n    \n    trn_data = lgb.Dataset(train, label=y_train_df)\n    val_data = lgb.Dataset(valid, label=y_valid_df)\n    \n    clf = lgb.train(params,trn_data,500,valid_sets = [trn_data, val_data],verbose_eval=200,early_stopping_rounds=200)\n\n    pred = clf.predict(valid)\n    #print(\"AUC = \", metrics.roc_auc_score(y_valid_df, pred))\n    \n    feature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),train.columns), reverse=True), columns=[\"Value\",\"Feature\"])\n    #print(feature_imp)\n                                                                                                           \n    non_important_features.update(set(feature_imp[feature_imp[\"Value\"] < 10][\"Feature\"]))                                                                                                           \n                                                                                                           \n    visual_chunk_n = int(feature_imp.shape[0] \/ 50)\n    tail_n = feature_imp.shape[0]  -  int(visual_chunk_n*50) - 1\n                                                                                                           \n    for i in range(0,visual_chunk_n):\n        plt.figure(figsize=(16,16))\n        plt.subplot(211)\n        importance_bar = sns.barplot(data=feature_imp.iloc[i*50 : (i+1)*50 - 1], x='Value', y='Feature')\n        plt.show()\n                                                                                                           \n    plt.figure(figsize=(16,16))\n    plt.subplot(211)\n    importance_bar_final = sns.barplot(data=feature_imp.tail(tail_n + 1), x='Value', y='Feature')\n    plt.show()\n                                                                                                           \n    del trn_data,val_data,train, valid\n    gc.collect()","7eda9343":"nans_df = train_df.isna()\nnans_groups={}\n\nfor col in train_df.columns:\n    cur_group = nans_df[col].sum()\n    try:\n        nans_groups[cur_group].append(col)\n    except:\n        nans_groups[cur_group]=[col]\ndel nans_df; x=gc.collect()\n\nfor k,v in nans_groups.items():\n    print('NAN count =',k)\n    print(v)","1414c9f6":"analyze_groups = [\n['D1', 'V281', 'V282', 'V283', 'V288', 'V289', 'V296', 'V300', 'V301', 'V313', 'V314', 'V315'],\n['D8', 'D9', 'id_09', 'id_10'],\n['D11', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11'],\n['V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34'],\n['V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52'],\n['V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74'],\n['V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94'],\n['V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137'],\n['V138', 'V139', 'V140', 'V141', 'V142', 'V146', 'V147', 'V148', 'V149', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V161', 'V162', 'V163'],\n['V143', 'V144', 'V145', 'V150', 'V151', 'V152', 'V159', 'V160', 'V164', 'V165', 'V166'],\n['V167', 'V168', 'V172', 'V173', 'V176', 'V177', 'V178', 'V179', 'V181', 'V182', 'V183', 'V186', 'V187', 'V190', 'V191', 'V192', 'V193', 'V196', 'V199', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216'],\n['V169', 'V170', 'V171', 'V174', 'V175', 'V180', 'V184', 'V185', 'V188', 'V189', 'V194', 'V195', 'V197', 'V198', 'V200', 'V201', 'V208', 'V209', 'V210'],\n['V217', 'V218', 'V219', 'V223', 'V224', 'V225', 'V226', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V235', 'V236', 'V237', 'V240', 'V241', 'V242', 'V243', 'V244', 'V246', 'V247', 'V248', 'V249', 'V252', 'V253', 'V254', 'V257', 'V258', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278'],\n['V220', 'V221', 'V222', 'V227', 'V234', 'V238', 'V239', 'V245', 'V250', 'V251', 'V255', 'V256', 'V259', 'V270', 'V271', 'V272'],\n['V279', 'V280', 'V284', 'V285', 'V286', 'V287', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V297', 'V298', 'V299', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321'],\n['V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339'],\n['id_01', 'id_12'],\n['id_22', 'id_23', 'id_27']\n]","83b82be8":"def display_corr_map(df,cols):\n    plt.figure(figsize=(15,15))\n    sns.heatmap(df[cols].corr(), cmap='RdBu_r', annot=True, center=0.0)\n    plt.title(cols[0]+' - '+cols[-1],fontsize=14)\n    plt.show()","c227e6ea":"for group in analyze_groups:\n    display_corr_map(train_df,['TransactionDT'] + group)","b4177353":"corr_features = set()\n\nfor group in analyze_groups:\n    group_corr_matrix = train_df[['TransactionDT'] + group].corr().abs().unstack().sort_values(kind=\"quicksort\")\n    group_corr_matrix = group_corr_matrix[group_corr_matrix != 1][group_corr_matrix > 0.75].drop_duplicates()\n    corr_features.update([x[1] for x in group_corr_matrix.index])","8ab309ac":"features_to_delete = list(corr_features.union(non_important_features))","01f09efb":"print(len(features_to_delete))\ncategorical_features = list(set(categorical_features) - set(features_to_delete))\nnumeric_features = list(set(numeric_features) - set(features_to_delete))","b4db43f9":"del corr_features, non_important_features, analyze_groups,nans_groups, folds, train_df\ngc.collect()","96977248":"train_transactions_df = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\ntrain_identity_df = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_df = pd.merge(train_transactions_df,train_identity_df,on = \"TransactionID\", how = \"left\")\ny = train_df[\"isFraud\"].copy()\n\ntest_transactions_df = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')\ntest_identity_df = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_df = pd.merge(test_transactions_df,test_identity_df,on = \"TransactionID\", how = \"left\")\n\ndel train_transactions_df, train_identity_df,test_transactions_df,test_identity_df\ngc.collect()","8b1e74bb":"train_df.drop(features_to_delete + [\"TransactionID\"], axis=1, inplace = True)\ntest_df.drop(features_to_delete + [\"TransactionID\"], axis=1, inplace = True)\n\ngc.collect()\ntrain_df.head()","fc2092be":"train_df.shape","ce9e5468":"train_df[categorical_features] = train_df[categorical_features].astype(\"object\")\ndf_cat_summary = dataframe_cat_feature_summary(train_df[categorical_features])\ndf_cat_summary","44250f2e":"train_df_cat_no_missing = train_df[categorical_features + [\"isFraud\"]].fillna(\"Missed\")\nfor feature in categorical_features:\n    ploting_cnt_amt_m(train_df_cat_no_missing, feature)","3d0128f7":"cat_columns_to_drop = df_cat_summary[df_cat_summary[\"% Missed\"] > 90][\"Feature\"].values\nprint(cat_columns_to_drop)\ntrain_df.drop(cat_columns_to_drop, axis=1, inplace = True)","0987d8de":"imputer_most_frequent = SimpleImputer(strategy='most_frequent')\ncategorical_features = list(set(categorical_features) - set(cat_columns_to_drop))\nimpute(train_df, categorical_features, imputer_most_frequent)","a8f167e4":"df_num_summary = dataframe_num_feature_summary(train_df[numeric_features])\ndf_num_summary","483e486e":"for feature in numeric_features:\n    ploting_numeric_features(train_df, feature)","b70a8c88":"num_columns_to_drop = df_num_summary[df_num_summary[\"% Missed\"] > 90][\"Feature\"].values\nprint(num_columns_to_drop)\ntrain_df.drop(num_columns_to_drop, axis=1, inplace = True)\nnumeric_features = list(set(numeric_features) - set(num_columns_to_drop))","5f7d15e8":"imputer_mean = SimpleImputer(strategy='mean')\nimpute(train_df, numeric_features, imputer_mean)","b79e238f":"#Formal check that there are no missing values for data frame\ntrain_df.isnull().sum().sum()","6ab66e89":"numeric_features_to_log = list()\nfor feature in numeric_features:\n    skeweness = stats.skew(train_df[feature])\n    print(\"f = {0}, s = {1}\".format(feature,skeweness))\n    if skeweness > 13:\n        numeric_features_to_log.append(feature)","0ac37481":"def encode_cat_features(train, test, cat_features):\n    for col in cat_features:\n        label_encoder = LabelEncoder()\n        label_encoder.fit(list(train[col].values) + list(test[col].values))\n        train[col] = label_encoder.transform(list(train[col].values))\n        test[col] = label_encoder.transform(list(test[col].values))\n        \ndef standarize_num_features(df, num_features):\n    scaler = MinMaxScaler()\n    df[num_features] = scaler.fit_transform(df[num_features])\n    \ndef decrease_skeweness(df, features):\n    print(features)\n    df[features] = np.log(df[features])","7899cbf1":"test_df.drop(cat_columns_to_drop, axis=1, inplace = True)\ntest_df.drop(num_columns_to_drop, axis=1, inplace = True)\n\nimputer_test_mean = SimpleImputer(strategy='mean')\nnumeric_features_test = list(set(numeric_features) - set(['isFraud']))\nimpute(test_df, numeric_features_test, imputer_test_mean)","5191fc23":"impute(test_df, categorical_features, imputer_most_frequent)\nencode_cat_features(train_df,test_df,categorical_features)","32d21852":"test_df.isnull().sum().sum()","bbf1e0bd":"def add_features(df):\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n    df['TransactionDT'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n    \n    df['year'] = df['TransactionDT'].dt.year\n    df['month'] = df['TransactionDT'].dt.month\n    df['dow'] = df['TransactionDT'].dt.dayofweek\n    df['hour'] = df['TransactionDT'].dt.hour\n    df['day'] = df['TransactionDT'].dt.day\n    \n    \n    \n    cards_cols= ['card1', 'card2', 'card3', 'card5']\n    for card in cards_cols: \n        if '1' in card: \n            df['card_id']= df[card].map(str)\n        else : \n            df['card_id']+= df[card].map(str)\n    \n    df['card_id'] = df['card_id'].astype(np.int64)","3263fccc":"# https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\nadd_features(train_df)\nadd_features(test_df)","7996962e":"def add_aggregations(df):\n    df['TransactionAmt_to_mean_card1'] = df['TransactionAmt'] \/ df.groupby(['card1'])['TransactionAmt'].transform('mean')\n    df['TransactionAmt_to_mean_card4'] = df['TransactionAmt'] \/ df.groupby(['card4'])['TransactionAmt'].transform('mean')\n    df['TransactionAmt_to_std_card1'] = df['TransactionAmt'] \/ df.groupby(['card1'])['TransactionAmt'].transform('std')\n    df['TransactionAmt_to_std_card4'] = df['TransactionAmt'] \/ df.groupby(['card4'])['TransactionAmt'].transform('std')\n    df['Trans_min_mean'] = df['TransactionAmt'] - df['TransactionAmt'].mean()\n    df['Trans_min_std'] = df['Trans_min_mean'] \/ df['TransactionAmt'].std()\n    df['D15_to_mean_card1'] = df['D15'] \/ df.groupby(['card1'])['D15'].transform('mean')\n    df['D15_to_mean_card4'] = df['D15'] \/ df.groupby(['card4'])['D15'].transform('mean')\n    df['D15_to_std_card1'] = df['D15'] \/ df.groupby(['card1'])['D15'].transform('std')\n    df['D15_to_std_card4'] = df['D15'] \/ df.groupby(['card4'])['D15'].transform('std')\n    df['D15_to_mean_addr1'] = df['D15'] \/ df.groupby(['addr1'])['D15'].transform('mean')\n    df['D15_to_mean_addr2'] = df['D15'] \/ df.groupby(['addr2'])['D15'].transform('mean')\n    df['D15_to_std_addr1'] = df['D15'] \/ df.groupby(['addr1'])['D15'].transform('std')\n    df['D15_to_std_addr2'] = df['D15'] \/ df.groupby(['addr2'])['D15'].transform('std')\n    df['id_02_to_mean_card1'] = df['id_02'] \/ df.groupby(['card1'])['id_02'].transform('mean')\n    df['id_02_to_mean_card4'] = df['id_02'] \/ df.groupby(['card4'])['id_02'].transform('mean')\n    df['id_02_to_std_card1'] = df['id_02'] \/ df.groupby(['card1'])['id_02'].transform('std')\n    df['id_02_to_std_card4'] = df['id_02'] \/ df.groupby(['card4'])['id_02'].transform('std')","6cb51690":"add_aggregations(train_df)\nadd_aggregations(test_df)","efc55f61":"def remove_one_value_features(df,df_test):\n    one_value_cols = [col for col in df.columns if df[col].nunique() <= 1]\n    one_value_cols_test = [col for col in df_test.columns if df_test[col].nunique() <= 1]\n    df.drop(list(set(one_value_cols+ one_value_cols_test)), axis=1, inplace=True)\n    df_test.drop(list(set(one_value_cols+ one_value_cols_test)), axis=1, inplace=True)\n    \ndef remove_big_top_values_features(df,df_test,columns):\n    big_top_value_cols = [col for col in columns if df[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n    big_top_value_cols_test = [col for col in columns if df_test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n    df.drop(list(set(big_top_value_cols+ big_top_value_cols_test)), axis=1, inplace=True)\n    df_test.drop(list(set(big_top_value_cols+ big_top_value_cols_test)), axis=1, inplace=True)","dae906a1":"remove_one_value_features(train_df,test_df)\nremove_big_top_values_features(train_df,test_df,numeric_features_test)","ffac5867":"X = train_df.drop([\"isFraud\", \"TransactionDT\"], axis=1)\ndel train_df\ngc.collect()","4433dc60":"X_test = test_df.sort_values(\"TransactionDT\").drop([\"TransactionDT\"], axis=1)\ndel test_df\ngc.collect()","e7a9a9f6":"X.head()","f2c7e10d":"X_test.head()","ca617e43":"folds = TimeSeriesSplit(n_splits=5)","2ce21297":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': SEED\n         }","ffc1a708":"aucs = list()\ntraining_start_time = time()\n\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    \n    trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n    val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n    clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n    aucs.append(clf.best_score['valid_1']['auc'])\n    \n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\nprint('-' * 30)\nprint('Training has finished.')\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\nprint('Mean AUC:', np.mean(aucs))\nprint('-' * 30)","fd44283f":"best_iter = clf.best_iteration\nclf = lgb.LGBMClassifier(**params, num_boost_round=best_iter)\nclf.fit(X, y)","21957220":"sub = pd.read_csv(\"..\/input\/ieee-fraud-detection\/sample_submission.csv\")\nsub['isFraud'] = clf.predict_proba(X_test)[:, 1]\nsub.to_csv('lgbm_v4_submission.csv', index=False)","6e97f220":"## Enviroment variables and functions","8cab4404":"Aggregations. There is no logic in them - simply aggregations on top features.","70a4e3b0":"## Data loading for modelling","d7a1a676":"## Explore categorical features\n\nFind a number of missing values and distributions of categorical features","6f6c7216":"Groups to analyze by similar NAN pattern","710bcaab":"DeviceInfo - 80 % missing, distribution seems to be close to uniform. Here we can see some information about client's device. It is important to be careful here - some of info could be for old devices and may be absent from test data.We can delete it\n","0e47a305":"## Explore numeric features\n\nFind a number of missing values and distributions of numeric features","4716f6f8":"## LGBM","a65b5a2f":"LGB fo feature importance","e5c3bc48":"## Data loading for feature importance check\n\nLoad the data for analysis","2ab63133":"## Process missing values for categorical features\n\nSolve issues related to missing values","50f626d5":"## Stat util functions","fc9e07f5":"## Modeling","179dcf11":"## Feature Engineering","95ef1a3f":"Visualize numeric variables ","fdc73ca9":"## Standarize, encode, center and prepare data for modeling\n\n","ce0e6784":"## ToDo Improvements\n1) Log normalization for skewed data\n\n2) Data Balancing\n\n5) Add binarization\n\n6) Entropy\n\n7) Add ensemble\n\n8) Work with outliers\n","932f801f":"## Improvements Log\n1) 0.920218 - lgb, minimum Aggregations on top features\n\n2) Adde more aggregations of top features - 0.922406\n\n3) More aggregations, add skeweness correction, add one more fold - 0.91\n\n4) Decreased threshold for missing values and added more folds for - 0.923","9fc569d4":"## Find skewed numeric features for future standartization","d216e328":"Visualize categorical variables with combination of Fraud","830b69f2":"## Data overview\n\n**Categorical Features**\n\nProductCD\n\ncard1 - card6\n\naddr1, addr2\n\nP_emaildomain\n\nR_emaildomain\n\nM1 - M9\n\nDeviceType\n\nDeviceInfo\n\nid_12 - id_38\n\n**Numeric Features**\n\nC1 - C14\n\nD1 - D15\n\nV1 - V339\n\nid_01 - id_11\n","22b118c5":"## Correlation Analysis","149c53db":"## Process missing values for numeric features\n\nSolve issues related to missing values"}}