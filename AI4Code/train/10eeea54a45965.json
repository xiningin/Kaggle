{"cell_type":{"e2ccc6de":"code","b7142a86":"code","86da8344":"code","f86f4ba8":"code","f4f6bc82":"code","704676a0":"code","82e60e6a":"code","96cb72cb":"code","ea49bba2":"code","b97db71d":"code","46df91a7":"code","6ff70330":"code","c5e89433":"code","f649e22d":"code","f0bc235b":"code","5b649470":"code","a5d2ca50":"code","a36e0da7":"code","5ff1d009":"code","9901767c":"code","2c6037da":"code","d9c62ce6":"code","795a85fa":"code","11224abe":"code","797f8053":"code","1f7ab8a2":"code","cebbefb2":"code","8b9b1dcc":"code","0f44bfe9":"code","51b05c9f":"markdown","30b298f4":"markdown","f523c146":"markdown","04c51df4":"markdown","6556a590":"markdown","38ebebb0":"markdown","3e4e3ced":"markdown","8d7221f6":"markdown","9a0dfb10":"markdown","e69efd9c":"markdown"},"source":{"e2ccc6de":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sqlite3\nimport nltk","b7142a86":"con = sqlite3.connect('..\/input\/amazon-fine-food-reviews\/database.sqlite')\ndata = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')\nsql_data = pd.read_sql_query(\"\"\"\nSELECT * FROM Reviews\nWHERE Score!=3\n\n\"\"\", con)","86da8344":"#Now we are replacing the 4,5,with positive and 1,2 as negative\n\ndef pos_neg(x):\n  if x<3:\n    return 'negative'\n  return 'positive'\n\nActual_score = sql_data['Score']\nupdated_score = Actual_score.map(pos_neg)\n\nsql_data['Score'] = updated_score\n","f86f4ba8":"sql_data['Score'].head()","f4f6bc82":"# Checking for repeating user ID\n\n(sql_data['UserId'].value_counts()>1).sum()","704676a0":"s = pd.read_sql_query(\"\"\"\nSELECT * FROM Reviews\nWHERE ProfileName= 'Geetha Krishnan'\n\"\"\",con)","82e60e6a":"sorted_data = sql_data.sort_values('ProductId', axis=0,inplace=False,kind='quicksort',na_position='last')\n#De-duplication\ndeduplicated_data = sorted_data.drop_duplicates(subset = ['UserId','ProfileName','Score','Time','Summary','Text'],keep= 'first',inplace=False)","96cb72cb":"deduplicated_data.shape","ea49bba2":"etc = pd.read_sql_query(\"\"\"\nSELECT * FROM Reviews\nWHERE HelpfulnessNumerator > HelpfulnessDenominator\n \"\"\",con)","b97db71d":"final_data = deduplicated_data[deduplicated_data.HelpfulnessDenominator>=deduplicated_data.HelpfulnessNumerator]","46df91a7":"final_data.shape","6ff70330":"final_data['Score'].value_counts()","c5e89433":"final_data.isna().sum()","f649e22d":"#Removing html tags\ndef htmltags(sentence):\n  cleanr = re.compile('<.*?>')\n  cleantext = re.sub(cleanr, '', sentence)\n  return cleantext\n\n#Expanding shortcut words\ndef wordexpand(phrase):\n    phrase = re.sub(r\"won't\",\"will not\",phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase  ","f0bc235b":"import nltk\nsw = nltk.download('stopwords')","5b649470":"from nltk.corpus import stopwords\nsw_eng = stopwords.words(\"english\")","a5d2ca50":"w = ['no','nor','not']\nfor i in w:\n  if i in sw_eng:\n    sw_eng.remove(i)","a36e0da7":"from tqdm import tqdm\nimport re\nfrom bs4 import BeautifulSoup\npreprocessed_reviews = []\n# tqdm is for printing the status bar\nfor sentence in tqdm(final_data['Text'].values):\n    sentence = sentence.lower()\n    sentence = htmltags(sentence)\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n    sentence = wordexpand(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sentance = ' '.join(e.lower() for e in sentence.split() if e.lower() not in sw_eng)\n    preprocessed_reviews.append(sentence.strip())","5ff1d009":"# Now we have processed review texts, now we will replace the text with processed texts\npreprocessed_reviews[1500]","9901767c":"\nfrom gensim.models import Word2Vec\nlist_of_sentences=[]\nfor sent in preprocessed_reviews:\n  list_of_sentences.append(sent.split())\nW2V_model = Word2Vec(list_of_sentences,min_count=1,size=50,workers=5)\nprint(W2V_model.wv.most_similar('joy'))\n","2c6037da":"## Getting all the words in Word2Vec vocabulary\n\ntotal_words = list(W2V_model.wv.vocab)\nlen(total_words)\n","d9c62ce6":"\navg_w2v_vec =[]\n\nfor sent in tqdm(list_of_sentences):\n  sent_vec = np.zeros(50)\n  c=0\n  for words in sent:\n    if words in total_words:\n      vec=W2V_model.wv[words]\n      sent_vec=sent_vec+vec\n      c=c+1\n    if c!=0:\n      sent_vec = sent_vec\/c  \n  avg_w2v_vec.append(W2V_model.wv[words])\n\n","795a85fa":"\nX_data = final_data.drop(['Id', 'ProductId', 'UserId', 'ProfileName','Score','Time','Summary','Text'],axis=1)\nX_data_tfidf=X_data\n\n#data_tfidf=np.hstack((X_data_tfidf,tfidf_vec))\ndata_w2v=np.hstack((X_data_tfidf,avg_w2v_vec))\n#data_bow=np.hstack((X_data_tfidf,bow_vec))\n#data_tfidf_w2v=np.hstack((X_data_tfidf,tfidf_w2v_vec))\n\n\ny=[1 if i=='positive' else 0 for i in final_data['Score'].values]\n","11224abe":"\nfrom sklearn.model_selection import train_test_split\ndef data_split(X_data,y):\n  X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.25, random_state=42)\n  X_tr, X_cv, y_tr, y_cv = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n  return X_tr, y_tr, X_test, y_test, X_cv, y_cv\n\n#X_tr_bow, y_tr_bow, X_test_bow, y_test_bow, X_cv_bow, y_cv_bow = data_split(data_bow,y)   \n#X_tr_tfidf, y_tr_tfidf, X_test_tfidf, y_test_tfidf, X_cv_tfidf, y_cv_tfidf = data_split(data_tfidf,y)\nX_tr_avgw2v, y_tr_avgw2v, X_test_avgw2v, y_test_avgw2v, X_cv_avgw2v, y_cv_avgw2v = data_split(data_w2v,y)   \n#X_tr_tfidf_w2v, y_tr_tfidf_w2v, X_test_tfidf_w2v, y_test_tfidf_w2v, X_cv_tfidf_w2v, y_cv_tfidf_w2v = data_split(data_tfidf_w2v,y)   \n\n","797f8053":"'''\nnp.save('.\/w2v_train_data',X_tr_avgw2v)\nnp.save('.\/train_data_y',y_tr_avgw2v)\n\nnp.save('.\/w2v_test_data',X_test_avgw2v)\nnp.save('.\/test_data_y',y_test_avgw2v)\n\nnp.save('.\/w2v_cv_data',X_cv_avgw2v)\nnp.save('.\/cv_data_y',y_cv_avgw2v)\n'''","1f7ab8a2":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef cross_validation(X_cv,y_cv):\n  from sklearn.model_selection import GridSearchCV as GCV  \n  k = {'n_neighbors':[1,3,5,7,9,11]}\n  model = KNeighborsClassifier() \n  clf = GCV(model, k)\n  search = clf.fit(X_cv,y_cv)\n  print(search.best_params_)\n  return search.best_params_\n","cebbefb2":"k = cross_validation(X_cv_avgw2v,y_cv_avgw2v)","8b9b1dcc":"def test_performance(X_tr,X_test,y_tr,y_test):  \n  from sklearn.neighbors import KNeighborsClassifier\n  from sklearn.metrics import plot_confusion_matrix\n  from sklearn.metrics import accuracy_score\n  KNN = KNeighborsClassifier(n_neighbors=11)\n  KNN.fit(X_tr,y_tr)\n  test_acc = accuracy_score(y_test_avgw2v,KNN.predict(X_test))\n  \n  plot_confusion_matrix(KNN,X_test_avgw2v,y_test_avgw2v)\n  plt.show()\n  print(test_acc)\n  return test_acc","0f44bfe9":"test_acc_w2v = test_performance(X_tr_avgw2v,X_test_avgw2v,y_tr_avgw2v,y_test_avgw2v)","51b05c9f":"\n# ***1. KNN***\n","30b298f4":"1. <html color='red'>Stop Words removal:<\/html color= 'red'> the, very, Not, is etc are non relevant words which do not add any meaning to the sentence except Not. So removing them makes my vectors error free. Although 'These' and 'The' do not add any meaning but if they are not removed then they will increase the distance between the vectors.\n\n2. Stemming: Tasty, tastefull, tastes are similar meaining words. So if they are treated separately then the distance between the vectors will increase insipte of having similar meaning. So to avoid this problem we have Stemming. Which takes the base word 'taste' for all three meentioned words above.\n\n3. Lemmatization: 'Delicious' and 'Tasty' are two complete different words with different base words but they are similar in meaning. If not treated properly they will increase the distance between two vectors drastically. So to solve this problem we have anothe algorithms known as 'Lemmatization'. Here it takes the content based meaning of the words.\n\n4. W2V: By using Word2Vec we can get the semantics meanings of the words. ","f523c146":"# *3. KNN wth Avg-W2V*","04c51df4":"# *Avg Word2Vec*","6556a590":"# *Train Test Split*","38ebebb0":"Q. What we do in text preprocessing?<br>\nAns: \n1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)\n","3e4e3ced":"# ***3.Text Preprocessing***","8d7221f6":"# ***2.Data Cleaning***","9a0dfb10":"***CONCLUSION: *** The FPR is huge which says that the model is very good in predicting the reviews which are positives and very bad in predicting with negative reviews. So we need BOOSTING to improve the BIAS of the model. Stay tuned! will be uploading the boosting on this dataset soon.","e69efd9c":"**HelpfulnessDenominator= yes+no\nand HelpfulnessNumerator= yes\n\nso HelpfulnessDenominator should always be greater than HelpfulnessNumerator**"}}