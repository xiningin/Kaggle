{"cell_type":{"27424279":"code","cb31a98c":"code","05d509e5":"code","2e83e6d7":"code","19c47ae0":"code","15aa71bd":"code","899a7e72":"code","d6dd4162":"code","b86d35ab":"code","124c80ff":"code","21bc5ded":"code","6720a4fa":"code","edb7afe8":"code","d62c3124":"code","90b34dec":"code","3becea27":"code","ea2b44fb":"code","5249d3f5":"code","b0bd8ea4":"code","9f30fd89":"code","5e74ce49":"code","68c74576":"code","5d31fd79":"code","70c25c92":"code","d2d0f560":"code","8f033f33":"code","68f83b64":"code","3f8db4fb":"code","a1241d1b":"code","d30bcdec":"code","c4fb799f":"code","89aa0df1":"code","3a30b91d":"code","42497f32":"code","a8212bb6":"code","497930fe":"code","4790adc2":"code","b0caf816":"code","a4ca4f5f":"code","dc24869c":"code","38c57875":"code","5c546ddd":"code","364fd237":"code","b211e90a":"code","350f64e6":"code","7799b135":"code","f95800cd":"code","8c33ea26":"code","f9adcb17":"code","6a40c9fd":"code","e0482685":"code","3f37e5e2":"code","1f8514f8":"code","e7cb062f":"code","a64bc16c":"code","79f6b28a":"code","6e9e1af0":"code","34b9d316":"markdown","d3ec12da":"markdown","9dbe80d4":"markdown","f6b372ac":"markdown","0561c2c4":"markdown","972fa135":"markdown","a639987c":"markdown","2717e9dc":"markdown","c94865dc":"markdown","c21312df":"markdown","3aebdfe9":"markdown","86f371b5":"markdown","98f57622":"markdown","823f3034":"markdown","d1ad1bc6":"markdown","35fdec78":"markdown","cae03759":"markdown","fc4f8c82":"markdown","191ebcdb":"markdown","7bfd04e9":"markdown","bb1cbc87":"markdown","4edcce29":"markdown","82e031d0":"markdown","6030e081":"markdown","388e0eb4":"markdown","a9563ab3":"markdown","bf9a030c":"markdown","af385469":"markdown","f75ab917":"markdown","40ed76ed":"markdown","786fd1de":"markdown"},"source":{"27424279":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly as py\nfrom sklearn.metrics import r2_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb31a98c":"data = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/merc.csv')","05d509e5":"data.head(7)","2e83e6d7":"data.columns","19c47ae0":"data.info()","15aa71bd":"data.isnull().sum()","899a7e72":"data.describe()","d6dd4162":"data.corr() #corrolation between columns","b86d35ab":"data.head()","124c80ff":"#correlation map\nf,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","21bc5ded":"data.corr()[\"price\"].sort_values()","6720a4fa":"data_model = data.model\ndata_price = data.price\n#Visiualization\nplt.figure(figsize=(45,20))\nsns.barplot(x = data_model, y = data_price)\nplt.xticks(rotation = 90) # rotation of countries's names\nplt.xlabel('Model')\nplt.ylabel('Price')\nplt.title('Model\/Price')\nplt.show()","edb7afe8":"plt.figure(figsize=(13,7))\nsns.distplot(data[\"price\"])\nplt.show()","d62c3124":"plt.figure(figsize=(13,7))\nsns.countplot(data.year)\nplt.show()","90b34dec":"plt.figure(figsize=(13,7))\nsns.scatterplot(x=data.mileage, y=data.price, data=data)\nplt.show()","3becea27":"data.sort_values('price', ascending=False).head(20)","ea2b44fb":"data.sort_values('price', ascending=True).head(20)","5249d3f5":"len(data)","b0bd8ea4":"len(data) * 0.01","9f30fd89":"nnpDf = data.sort_values(\"price\",ascending = False).iloc[131:]\nnnpDf","5e74ce49":"nnpDf.describe()","68c74576":"data.describe()","5d31fd79":"plt.figure(figsize=(13,7))\nsns.distplot(nnpDf[\"price\"])\nplt.show()","70c25c92":"nnpDf.groupby('year').mean()['price']","d2d0f560":"nnpDf[nnpDf.year != 1970].groupby(\"year\").mean()[\"price\"]","8f033f33":"nnpDf.drop('transmission', axis=1, inplace=True)","68f83b64":"nnpDf.drop('fuelType', axis=1, inplace=True)","3f8db4fb":"nnpDf.drop('model', axis=1,inplace=True)","a1241d1b":"df = nnpDf","d30bcdec":"df.head()","c4fb799f":"sns.pairplot(df)\nplt.show()","89aa0df1":"y = df['price'].values\nx = df.drop('price', axis=1).values","3a30b91d":"x","42497f32":"y","a8212bb6":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.33, random_state = 0)","497930fe":"from sklearn.linear_model import LinearRegression\nlinear_regression = LinearRegression()\nlinear_regression.fit(x_train,y_train)\ny_pred_lin_reg = linear_regression.predict(x_test)","4790adc2":"print(f'y_test: {y_test}\\nprediction: {y_pred_lin_reg}')","b0caf816":"print('R Square Score for Linear Regression : ', r2_score(y_test, y_pred_lin_reg))","a4ca4f5f":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=4)\nx_poly = poly_reg.fit_transform(x,y)\nlinear_regression2 = LinearRegression()\nlinear_regression2.fit(x_poly, y)\ny_pred_poly_reg = linear_regression2.predict(x_poly)","dc24869c":"print(f'y: {y}\\nprediction: {y_pred_poly_reg}')","38c57875":"print('R Square Score for Polynomial Regression : ', r2_score(y, y_pred_poly_reg))","5c546ddd":"from sklearn.preprocessing import StandardScaler\nstandard_scaler1 =StandardScaler()\nscaled_x_train = standard_scaler1.fit_transform(x_train)\nscaled_x_test = standard_scaler1.transform(x_test) # dont learn just transform\nstandard_scaler2=StandardScaler()\nscaled_y_train = np.ravel(standard_scaler2.fit_transform(y_train.reshape(-1,1)))\nscaled_y_test = np.ravel(standard_scaler2.fit_transform(y_test.reshape(-1,1)))","364fd237":"from sklearn.svm import SVR\nsvr_regression = SVR(kernel=\"rbf\") # other kernel types : linear, poly, rbf, sigmoid\nsvr_regression.fit(scaled_x_train, scaled_y_train)\ny_pred_svr_reg = svr_regression.predict(scaled_x_test)","b211e90a":"print(f'y_scaled: {scaled_y_test}\\nprediction: {y_pred_svr_reg}')","350f64e6":"print('R Square Score for Support Vector Regression : ', r2_score(scaled_y_test, y_pred_svr_reg))","7799b135":"from sklearn.tree import DecisionTreeRegressor\ndecision_tree_reg = DecisionTreeRegressor(random_state=0)\ndecision_tree_reg.fit(x_train, y_train)\ny_pred_dt_reg = decision_tree_reg.predict(x_test)","f95800cd":"print(f'y_test: {y_test}\\nprediction: {y_pred_dt_reg}')","8c33ea26":"print('R Square Score for Decision Tree Regression : ', r2_score(y_test, y_pred_dt_reg))","f9adcb17":"from sklearn.ensemble import RandomForestRegressor\nrandom_forest_reg = RandomForestRegressor(n_estimators = 50, random_state= 0 ) # n_estimators = numbor of estimator tree\nrandom_forest_reg.fit(x_train, y_train)\ny_pred_rf_reg = random_forest_reg.predict(x_test)","6a40c9fd":"print(f'y_test: {y_test}\\nprediction: {y_pred_rf_reg}')","e0482685":"print('R Square Score for Random Forest Regression : ', r2_score(y_test, y_pred_rf_reg))","3f37e5e2":"df.head(20)","1f8514f8":"print(random_forest_reg.predict([[2020, 1000, 140, 29.2, 4.0]]))","e7cb062f":"df.tail(20)","a64bc16c":"print(random_forest_reg.predict([[2000, 90000, 290, 30.4, 3.0]]))","79f6b28a":"df.head()","6e9e1af0":"import statsmodels.api as sm \nx = np.append(arr=np.ones((12988,1)).astype(int), values = x, axis = 1)\nx_opt = x[:, [0,1,2,3,4,5]]\nregressor_OLS = sm.OLS(endog=y, exog=x_opt).fit()\nregressor_OLS.summary()","34b9d316":"<a id = '9'><\/a><br>\n### Random Forest Regression","d3ec12da":"<a id = '4'><\/a><br>\n## Machine Learning (Regression)","9dbe80d4":"Better now !","f6b372ac":"<a id = '7'><\/a><br>\n### SVR","0561c2c4":"### Splitting","972fa135":"<a id = '5'><\/a><br>\n### Linear Regression","a639987c":"I droped the string values. nnpDf is looking weird  isn't it ?","2717e9dc":"<a id = '10'><\/a><br>\n## Conclusion","c94865dc":"<a id = '3'><\/a><br>\n## Data Preparetion and Visualization","c21312df":"<a id = '2'><\/a><br>\n## Variable Description","3aebdfe9":"We scaled the variables that SVR make predictions better.","86f371b5":"Price is our target value ","98f57622":"<a id = '8'><\/a><br>\n### Decision Tree Regression","823f3034":"Here is corrolation map. If the value near to -1 that means there is negative corrolation between values. For example : milage and year. Naturally, the milage is increases as the car is driven. Similarly if the value near to 1 that means there is positive corrolation between values.","d1ad1bc6":"* model : Model of car\n* year : The year that car made\n* price : Price (Sterlin)\n* Transmission : Type of gear\n* milage : How many miles the car went (1 mil = 1,609344 km)\n* fuelType : Fuel type\n* tax : tax\n* mpg : Miles per gallon (1 galon = 3,78541178 liters)\n* engine size : Size of engine (liters)","35fdec78":"Lets have a look at that the value year which is corrolated with price","cae03759":"<a id = '10'><\/a><br>\n## Model Selection","fc4f8c82":"As you see, there are not massive changes and it disturbuted more balanced","191ebcdb":"## Mercedes Car Price Prediction\n![merc3.png](attachment:merc3.png)","7bfd04e9":"As a conclusion, we checked, prapered and visualized the data first. Then, we tried the models for the data. With respect to R squere score we choose best model. Finally, elimation of columns that spoil data. Thank you, hope its usefull.","bb1cbc87":"Price will be our target column becouse we want to predict that and the other columns will be our tools that predict the price. However will we use all columns ?","4edcce29":"* nnpDf = ninety nine percent data frame","82e031d0":"We tried the algoritms to our data. Now the thing we must do is getting better by making eleminations by the helping of backward elemination. ","6030e081":"<a id = '1'><\/a><br>\n## Load and Check Data","388e0eb4":"As you see in P>|t| there is no need to make elemination. ","a9563ab3":"As you see, after 75.000 there are unnecessary number of values considered. They would destroy our predictions.","bf9a030c":"We remove 1 percent of the data set (the part that is most likely the outlier that distorts the data) and we get a 99 percent data set.","af385469":"## Introduction\nIn this kernel we are going to analyse, visualize and predict using Mercedes data set and using some methods like linear regression, polynominal regression etc. \n\n<font color = 'purple'>\nContent : \n\n1. [Load and Check Data](#1)    \n1. [Variable Description](#2)\n1. [Data Preperation and Visualization](#3)\n1. [Machine Learning (Regression)](#4) \n    * [Linear Regression](#5)\n    * [Polynomial Regression](#6)\n    * [Support Vector Regression (SVR)](#7)\n    * [Decision Tree](#8)\n    * [Random Forest](#9)\n1. [Model Selection](#10)\n1. [Conclusion](#11)\n ","f75ab917":"In year 1970, mean of price is too high by compering to year order. Also this would spoil our model.","40ed76ed":"The best model for us is Random forest .","786fd1de":"<a id = '6'><\/a><br>\n### Polynomial Regression"}}