{"cell_type":{"0dd8e713":"code","31234e69":"code","433b0e34":"code","b5a5dbf0":"code","65cdc606":"code","9c9b4f42":"code","6bd65029":"code","9e1af498":"code","525d409e":"code","f0d13707":"code","06641eeb":"code","b1cca287":"code","d89d7395":"code","91103324":"code","f04e3175":"code","bf7a6425":"code","099721db":"code","b3cec769":"code","4bab55f2":"code","bbcbe274":"code","bfd19ac0":"code","0c79fd92":"code","da4ed00d":"code","4cb68dbc":"code","91d6849a":"code","4eb4bb58":"code","7b0da8ce":"code","1083e651":"code","f13189dd":"code","f6e1ce2e":"code","bdf8af0c":"code","8ada7220":"code","a9baa032":"markdown","7d701665":"markdown","b54aff7a":"markdown","0e5599a0":"markdown","5729a024":"markdown","c281c519":"markdown","483b3d19":"markdown","01f61e06":"markdown","c08c387c":"markdown","052d4a6a":"markdown","1efa71d3":"markdown","db6585ec":"markdown","296b7d16":"markdown","15835309":"markdown","81f40733":"markdown","2c89d7f2":"markdown","e0607b20":"markdown","fac8acf3":"markdown","075a1afc":"markdown","caab31f9":"markdown","6bd50cbf":"markdown","3255255c":"markdown","a3d6eacc":"markdown","4fb2df09":"markdown","e2d71eaa":"markdown","ce9c07bd":"markdown","0e5049f5":"markdown"},"source":{"0dd8e713":"import tensorflow as tf\n\n# Get the GPU device name.\ndevice_name = tf.test.gpu_device_name()\n\n# The device name should look like the following:\nif device_name == '\/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')","31234e69":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","433b0e34":"!pip install transformers","b5a5dbf0":"import numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split","65cdc606":"df = pd.read_csv('https:\/\/github.com\/clairett\/pytorch-sentiment-classification\/raw\/master\/data\/SST2\/train.tsv', delimiter='\\t', header=None)\ndf.head()","9c9b4f42":"df.columns","6bd65029":"df[1].value_counts()","9e1af498":"negative=df.loc[df[1]==0][:1000]\npositive=df.loc[df[1]==1][:1000]\ndf=pd.DataFrame(columns=[0,1])\ndf=df.append(negative)\ndf=df.append(positive)\ndf=df.sample(frac=1)\ndf.head()","525d409e":"df.shape","f0d13707":"import transformers","06641eeb":"from transformers import DistilBertTokenizer\n\n# Load the DistillBERT tokenizer.\nprint('Loading DistillBERT tokenizer...')\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)# convertiing evry input to lower case","b1cca287":"# Print the original sentence.\nprint(' Original: ', df[0][0])\n\n# Print the sentence split into tokens.\nprint('Tokenized: ', tokenizer.tokenize(df[0][0]))\n\n# Print the sentence mapped to token ids.\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df[0][0])))","d89d7395":"model, pretrained_weights = (transformers.DistilBertModel, 'distilbert-base-uncased')","91103324":"model = model.from_pretrained(pretrained_weights)","f04e3175":"tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))","bf7a6425":"# Padding\nmax_len = 0\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])","099721db":"print(padded.shape)\n","b3cec769":"# Print the original sentence.\nprint(' Original: ', df[0][0])\nprint('\\n')\n# Print the sentence split into tokens.\nprint('Tokenized: ', tokenizer.tokenize(df[0][0]))\nprint('\\n')\n# Print the sentence mapped to token ids.\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df[0][0])))\nprint('\\n')\n# padded sentence\nprint(padded[0])","4bab55f2":"attention_mask = np.where(padded != 0, 1, 0)\nattention_mask.shape","bbcbe274":"df.shape","bfd19ac0":"input_ids = torch.tensor(padded)   # converting into torch tensors\nattention_mask = torch.tensor(attention_mask) # converting into torch tensors","0c79fd92":"with torch.no_grad():\n    last_hidden_states = model(input_ids, attention_mask=attention_mask)","da4ed00d":"features = last_hidden_states[0][:,0,:].numpy()","4cb68dbc":"labels = df[1] # getting labels","91d6849a":"train_features, test_features, train_labels, test_labels = train_test_split(features, labels)","4eb4bb58":"print(type(train_features))\nprint(type(test_features))","7b0da8ce":"print(type(train_labels))\nprint(type(test_labels))","1083e651":"train_labels = train_labels.to_numpy() \ntest_labels=test_labels.to_numpy()","f13189dd":"train_labels=train_labels.astype('int')","f6e1ce2e":"test_labels=test_labels.astype('int')","bdf8af0c":"lr_clf = LogisticRegression(C=5.2, max_iter=2000)\nlr_clf.fit(train_features, train_labels)","8ada7220":"lr_clf.score(test_features, test_labels)","a9baa032":"# Let's See What we have got after <font color='orange'>Tokenisation<\/font>","7d701665":"`The model() function runs our sentences through BERT. The results of the processing will be returned into last_hidden_states.`\n\nSO this can take time\n","b54aff7a":"## So it is clear from above that it is <font color='green'>Balanced Dataset<\/font>\n## This means we can use <font color='red'>**Accuracy**<\/font> as metric.\n\n## But due to limitation of CPU we will reduce the size of dataset\n","0e5599a0":"## 1. Loading Dataset\nFor this task we will use SST2  Dataset from [here](https:\/\/github.com\/clairett\/pytorch-sentiment-classification\/)\n","5729a024":"\n*   For Experimentation in This Notebook we wil use  <font               color='orange'>DistillBert<\/font> directly(i.e will not train it on our data) \n*   And In <font color='orange'>Second Part<\/font> We will Train our DistillBert on our Data\n\n","c281c519":"## Tokenisation","483b3d19":"# Tokenization & Input Formatting for <font color='blue'>BERT<\/font>\n","01f61e06":"## Importing Libraries","c08c387c":" Above we use `convert_tokens_to_ids` to just tokenise our input we will now use ` tokenizer.encode` funcion that fullfills all our input requirements. Input requiremnts. for <font color='orange'>BERT<\/font> or Even Other Transformers  are :-\n\n\n1.   All the Input sentences to Transfromer model must be of Same length.\n1.   Add special tokens to the start and end of each sentence.\n2.   **Pad** & **truncate** all sentences to a single constant length.Because ***BERT*** is pretrained model and it has fixed Maximum Input Size of <font color='orange'>512<\/font> tokens\n3.   Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n\nNow Attention Masks are very special .Suppose one of our input sentence after tokenisation has 8 tokens it means our <font color='orange'>Real Tokens are 8<\/font> and suppose we have set our Maximum Length to 10 tokens so we will pad Zeros to at the right of Sentence . So now to diffrentiate between Real tokens and Padded tokens we add <font color='orange'>Attention Masks<\/font>\n\n\n```\nWe can only Padding Tokens to right of sentence in BERT( it is a pretrained mdel)\n```\n\n\n","052d4a6a":"![alt text](https:\/\/jalammar.github.io\/images\/distilBERT\/sst2-text-to-tokenized-ids-bert-example.png)\n","1efa71d3":"### As type of Train_features and train_labels don't have same Datatype\n### so when we will pass it to Logistic Regression it will give \n### *Unknown error* and to remove it make type same.\n","db6585ec":"## Loading Our <font color='pink'>Bad Boy <\/font>\n","296b7d16":"## 2.1 Distill Bert Tokenizer\nTo feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.","15835309":"0 column represents our movie reviews and \n\n1 column represents their label\nwhere Zero represents negative movie review and 1 represnts positive review","81f40733":"# Masking \/ Attention-Masking","2c89d7f2":"## See what we got after Padding","e0607b20":"## Padding Tokens\n\nPadding is done with a special `[PAD]` token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a \"MAX_LEN\" of 8 tokens.\n\n![alt text](http:\/\/www.mccormickml.com\/assets\/BERT\/padding_and_mask.png)","fac8acf3":"This is the Part1 of 2 Part Notebooks in next Notebook we will Train our corpus on DistillBert this process is <font color='red'>Fine tuning<\/font>","075a1afc":"# Our Model\n![alt text](https:\/\/camo.githubusercontent.com\/7c092d2fd20a0cd922bdd15a862e31155f6adcb7\/68747470733a2f2f6a616c616d6d61722e6769746875622e696f2f696d616765732f64697374696c424552542f626572742d64697374696c626572742d7475746f7269616c2d73656e74656e63652d656d62656464696e672e706e67)","caab31f9":"# Splitting Data for training and Testing","6bd50cbf":"## Installing Hugging Face","3255255c":"## Some Visualisation ","a3d6eacc":"## Turning the Beast on","4fb2df09":" For reference, the highest accuracy score for this dataset is currently 96.8. DistilBERT can be trained to improve its score on this task \u2013 a process called fine-tuning which updates BERT\u2019s weights to make it achieve a better performance in this sentence classification task (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of 90.7. The full size BERT model achieves 94.9.","e2d71eaa":"Cuurently we have Series\/Dataframe of list but we have to convert it tensor for input to `DistillBert`","ce9c07bd":"## Special Tokens\n\n`[SEP]`\n\nAt the end of every sentence, we need to append the special [SEP] token.\n\nThis token is an separator of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?).\n\n\nAnd We have to use this token even for single sentences.\n\n`[CLS]`\n\nFor classification tasks, we must prepend the special `[CLS]` token to the beginning of every sentence.\n\nThis token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!).\n\n![alt text](http:\/\/mccormickml.com\/assets\/BERT\/CLS_token_500x606.png)\n\n\nOn the output of the final (12th) transformer, only the first embedding (corresponding to the `[CLS]` token) is used by the classifier.\n\n` The first token of every sequence is always a special classification token `([CLS]`). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. `\n\nAlso, because BERT is trained to only use this `[CLS]` token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector.","0e5049f5":"# Padding\n"}}