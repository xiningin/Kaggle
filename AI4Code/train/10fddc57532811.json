{"cell_type":{"74c721a5":"code","a5996973":"code","e4285612":"code","ee3b259f":"code","6f69cc51":"code","058badd1":"code","19c840ac":"code","765e484c":"code","b929eeb7":"code","91aafac1":"code","5de13f12":"code","cd1aa2eb":"code","e24b6803":"code","583e03cc":"code","e9334e5d":"code","ce1baad4":"code","2ae69834":"code","a4a661fd":"code","d411f5ff":"code","da9c0601":"code","cdb02016":"code","899fbd66":"code","146212de":"code","51c74720":"code","17ac99d0":"code","f0d3cc8b":"code","229409f2":"markdown"},"source":{"74c721a5":"# libraries\nimport os\nimport numpy as np \nimport pandas as pd \npd.options.mode.chained_assignment = None\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n# nltk\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer","a5996973":"# files\nos.listdir(\"..\/input\/feedback-prize-2021\")","e4285612":"submission = pd.read_csv(\"..\/input\/feedback-prize-2021\/sample_submission.csv\")\nsubmission.head()","ee3b259f":"train_df = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\ntrain_df.head()","6f69cc51":"train_df.discourse_type.value_counts(normalize=False)","058badd1":"train_dir = \"..\/input\/feedback-prize-2021\/train\"\ntest_dir = \"..\/input\/feedback-prize-2021\/test\"","19c840ac":"def create_full_text_dataframe(train=True) -> pd.DataFrame:\n    id_list = []\n    text_list = []\n    \n    if train:\n        for id in tqdm(train_df.id):\n            filepath = os.path.join(train_dir, f\"{id}.txt\")\n            text = open(filepath, 'r').read()\n\n            id_list.append(id)\n            text_list.append(text)\n    else:\n        for filename in tqdm(os.listdir(test_dir)):\n            filepath = os.path.join(test_dir, filename)\n            id = str(filename).strip()[:-4]\n            text = open(filepath, 'r').read()\n            \n            id_list.append(id)\n            text_list.append(text)\n            \n    return pd.DataFrame(data={\"id\": id_list, \"text\": text_list})","765e484c":"df_train = create_full_text_dataframe()","b929eeb7":"df_test = create_full_text_dataframe(train=False)","91aafac1":"df_train = df_train.merge(train_df, on=\"id\", how=\"inner\")","5de13f12":"df = df_train[[\"id\", \"discourse_text\", \"discourse_type\"]]","cd1aa2eb":"# lemmatize (or stem?)\n\n#lemmatizer = WordNetLemmatizer()\n#df['discourse_text'] = df['discourse_text'].progress_apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]))","e24b6803":"# Target label encoding\nencoder = LabelEncoder()\nlabels = encoder.fit_transform(df.discourse_type)\n\n# encoded target labels\ndf.loc[:, \"label\"] = labels","583e03cc":"# train\/test splitting\nX_train, X_test, y_train, y_test = train_test_split(df.discourse_text, df.label, test_size=0.02)","e9334e5d":"# Model Pipeline and training\n\nmultinomialNB = Pipeline([\n        ('vect', CountVectorizer(stop_words='english', ngram_range=(1, 2), binary=True)),\n        ('tfidf', TfidfTransformer(norm='l2', use_idf=True)),\n        ('clf', MultinomialNB(alpha=0.1)),    \n])\n\n# training\nmultinomialNB.fit(X_train, y_train)","ce1baad4":"# prediction on test data\ny_test_pred = multinomialNB.predict(X_test)","2ae69834":"# Results on test set\nprint(\"\\nTest Precision:\", metrics.precision_score(y_test, y_test_pred, average='micro'))\nprint(\"\\nTest Recall:\", metrics.recall_score(y_test, y_test_pred, average='micro'))\nprint(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_test_pred))","a4a661fd":"def plot_confusion_matrix(y_test, y_scores, class_names):\n    num_class = len(class_names)\n    cm = metrics.confusion_matrix(y_test, y_scores)\n\n    # normalize\n    con = np.zeros((num_class, num_class))\n    for x in range(num_class):\n        for y in range(num_class):\n            con[x,y] = cm[x,y]\/np.sum(cm[x,:])\n\n    plt.figure(figsize=(10,8))\n    sns.set(font_scale=1.0) # for label size\n    sns.heatmap(con, annot=True, fmt=\".2\", cmap='Blues',xticklabels= class_names , yticklabels= class_names)\n    plt.show()","d411f5ff":"plot_confusion_matrix(y_test, y_test_pred, encoder.inverse_transform(np.unique(labels)))","da9c0601":"# create df_test: a record from each discourse within test text docs\ndef expand_df_test(df: pd.DataFrame=df_test) -> pd.DataFrame:\n    \n    ids = []; data = []\n    for id, text in zip(df.id, df.text):\n        sentences = nltk.sent_tokenize(text)\n        \n        id_sentences = []; idx = 0 \n        for sentence in sentences:\n            id_sentence = []\n            words = sentence.split()\n            for w in words:\n                id_sentence.append(idx)\n                idx += 1\n            id_sentences.append(id_sentence)\n        data += list(zip([id] * len(sentences), sentences, id_sentences))\n        \n    tmp = pd.DataFrame(data, columns=['id', 'text', 'predictionstring'])\n    return tmp","cdb02016":"test = expand_df_test()","899fbd66":"# prediction on test data\n\ntest[\"class\"] = multinomialNB.predict(test.text)","146212de":"test.loc[:, \"class\"] = encoder.inverse_transform(test[\"class\"].values)","51c74720":"# submission\nresult = test[[\"id\", \"class\", \"predictionstring\"]]\nresult['predictionstring'] = result['predictionstring'].apply(lambda x: ' '.join([str(i) for i in x]))","17ac99d0":"result.head()","f0d3cc8b":"result.to_csv(\"submission.csv\", index=False)","229409f2":"#### Building a baseline model to classify the discourse_type of each of the sentences (discourse_text) in the complete text.\n\nComplete solution will also require the segmentation of the text into sentences\/discourses, and indicies of words inside the discourse text (relative to the full text), and then classification of each discourse_text (use this baseline model)."}}