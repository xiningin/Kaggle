{"cell_type":{"84a02781":"code","b06e88b1":"code","a43474cf":"code","a7530918":"code","cc76b210":"code","fdcb15a8":"code","fe7032a2":"code","76c9642c":"code","8a0b8173":"code","9948b7cf":"code","833d450d":"code","aa8bb437":"code","3380c310":"code","7e530c1b":"code","ed08d3da":"code","6dcddfd8":"code","0154212a":"code","c97f58aa":"code","313e12a5":"code","e20423bb":"code","b2239422":"code","dedf4eb5":"code","169e0238":"code","c6b48509":"code","d90b6063":"code","af1928a2":"code","db809527":"code","06a98457":"code","aed4bf6c":"code","6bc75481":"code","a4028cf1":"code","c10d7957":"code","2de01ad0":"code","5c435e78":"code","eeb6f0ee":"code","87306c46":"code","bdb68c88":"code","6e96c329":"code","d867856d":"code","18fc9b35":"code","73c8466b":"code","4ca016db":"code","a3df5df9":"code","0b9cd507":"code","9968d2b1":"code","a6823565":"code","740505b6":"code","b99c39a9":"code","c3a5f97f":"markdown","7d67e095":"markdown","fd1fa5ef":"markdown","ce280048":"markdown"},"source":{"84a02781":"import os\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.notebook import tqdm","b06e88b1":"os.listdir(\"..\/input\")","a43474cf":"main_data_dir = \"..\/input\/covid19-global-forecasting-week-3\"\nmetadata_dir = \"..\/input\/covid19-countrywise-metadata\"","a7530918":"data_train = pd.read_csv(os.path.join(main_data_dir, \"train.csv\"))\ndata_test = pd.read_csv(os.path.join(main_data_dir, \"test.csv\"))\nsample_sub = pd.read_csv(os.path.join(main_data_dir, \"submission.csv\"))\nmetadata = pd.read_csv(os.path.join(metadata_dir, \"covid19_countrywise_metadata.csv\"))","cc76b210":"def generate_loc(x):\n    if isinstance(x[\"Province_State\"], float):\n        return x[\"Country_Region\"]\n    else:\n        return \"_\".join([x[\"Country_Region\"], x[\"Province_State\"]])","fdcb15a8":"def generate_loc_date(x):\n    return \"_\".join([x[\"location\"], x[\"Date\"]])","fe7032a2":"data_train[\"location\"] = data_train.apply(generate_loc, axis=1)\ndata_test[\"location\"] = data_test.apply(generate_loc, axis=1)","76c9642c":"data_train[\"log_cfm\"] = data_train[\"ConfirmedCases\"].map(np.log1p)\ndata_train[\"log_ftl\"] = data_train[\"Fatalities\"].map(np.log1p)\ndata_train[\"cfm_growth\"] = data_train[\"log_cfm\"].diff()\ndata_train[\"ftl_growth\"] = data_train[\"log_ftl\"].diff()","8a0b8173":"data_train.fillna(0, inplace=True)","9948b7cf":"data_train[\"loc_date\"] = data_train.apply(generate_loc_date, axis=1)\ndata_test[\"loc_date\"] = data_test.apply(generate_loc_date, axis=1)","833d450d":"data_train_by_loc = data_train.groupby(\"location\")","aa8bb437":"data_train[\"Date\"] = pd.to_datetime(data_train[\"Date\"])\ndata_test[\"Date\"] = pd.to_datetime(data_test[\"Date\"])","3380c310":"data_train[\"rel_date\"] = data_train.apply(lambda x: (x[\"Date\"] - data_train[\"Date\"].min()), axis=1).dt.days","7e530c1b":"data_test[\"rel_date\"] = data_test.apply(lambda x: (x[\"Date\"] - data_train[\"Date\"].min()), axis=1).dt.days","ed08d3da":"lockdown_cols = [\"quarantine\", \"close_school\", \"close_public_place\", \"limit_gathering\", \"stay_home\"]\nlockdown_df = pd.DataFrame(np.zeros((len(data_train), len(lockdown_cols))), columns=lockdown_cols)\nmetadata[lockdown_cols] = metadata[lockdown_cols].fillna(\"2099-12-31\")\nfor col in lockdown_cols:\n    metadata[col] = pd.to_datetime(metadata[col], format=\"%Y-%m-%d\")","6dcddfd8":"set(data_train.location.unique()) - set(metadata.location.unique())","0154212a":"for i in range(len(data_train)):\n    cur_date = data_train.Date[i]\n    cur_loc = data_train.location[i]\n    idx = metadata.loc[metadata.location == cur_loc, :].index.values[0]\n    lockdown_df.loc[i, :] = (metadata.loc[idx, lockdown_cols] <= cur_date).astype(int)","c97f58aa":"lockdown_df","313e12a5":"data_train = pd.concat([data_train, lockdown_df], axis=1)","e20423bb":"data_test_known = data_test.loc[data_test.rel_date <= data_train.rel_date.max(), :]\ndata_test_unknown = data_test.loc[data_test.rel_date > data_train.rel_date.max(), :]","b2239422":"data_train.set_index(\"loc_date\", inplace=True, drop=True)\npred_test_known = data_train.loc[data_test_known.loc_date, [\"ConfirmedCases\", \"Fatalities\"]].reset_index(drop=False)\ndata_test_known = data_test_known.merge(pred_test_known, on=\"loc_date\")\ndata_train.reset_index(inplace=True, drop=False)","dedf4eb5":"train_input_beg = data_train.rel_date.min()\noutput_len = data_test.rel_date.max() - data_train.rel_date.max()\ntrain_input_end = data_train.rel_date.max() - output_len\ninput_len = train_input_end - train_input_beg\ntest_input_beg = data_test.rel_date.max() - output_len - input_len\ntest_output_beg = data_test_unknown.rel_date.min()","169e0238":"print(\"Number of days in the input: {}d.\".format(input_len))\nprint(\"Number of days in the output: {}d.\".format(output_len))\nprint(\"Training input ends on day {}.\".format(train_input_end))\nprint(\"Testing input begins on day {}.\".format(test_input_beg))","c6b48509":"input_test = data_train.loc[data_train.rel_date >= test_input_beg, :]","d90b6063":"rand_seed = 42\nloc_list = data_train.location.unique()\nloc_train, loc_val = train_test_split(loc_list, test_size=0.2, random_state=rand_seed)\ndata_train.set_index(\"location\", inplace=True, drop=True)\ndata_val = data_train.loc[loc_val, :]\ndata_train = data_train.loc[loc_train, :]\ndata_train.reset_index(drop=False, inplace=True)\ndata_val.reset_index(drop=False, inplace=True)","af1928a2":"metadata.drop(lockdown_cols, axis=1, inplace=True)","db809527":"def min_max_normalize(data):\n    scaler = MinMaxScaler()\n    return scaler.fit_transform(data)","06a98457":"num_cols = metadata.columns[3:]\nmetadata[num_cols] = min_max_normalize(metadata[num_cols])","aed4bf6c":"metadata.drop([\"region\", \"country\"], axis=1, inplace=True)","6bc75481":"metadata.set_index(\"location\", inplace=True)","a4028cf1":"class CovidDataset(Dataset):\n    \n    def __init__(self, df, metadata, x_cols, input_end, input_beg=0, is_predict=False, random_clip=False):\n        self.is_predict = is_predict\n        self.metadata = metadata\n        self.x_cols = x_cols\n        self.random_clip = random_clip\n        if not self.is_predict:\n            self.df_input = df.loc[df.rel_date <= input_end, :]\n            self.df_target = df.loc[df.rel_date > input_end, :]\n            self.loc_list = df.location.unique()\n        else:\n            self.df_input = df.loc[df.rel_date >= input_beg, :]\n            self.loc_list = df.location.unique()\n        \n    def __len__(self):\n        return len(self.loc_list)\n    \n    def __getitem__(self, idx):\n        loc = self.loc_list[idx]\n        inputs = self.df_input.loc[self.df_input.location == loc, self.x_cols].values\n#         if self.random_clip:\n#             start = self.df_input.loc[(self.df_input.location == loc) &\n#                 (self.df_input.ConfirmedCases > 0), \"rel_date\"].min()\n#         else:\n#             clip_start = np.random.randint(0, len(inputs) \/\/ 2)\n#             inputs[:clip_start] = -9e6\n\n        meta_input = self.metadata.loc[loc, :].values\n        if not self.is_predict:\n            data_target = self.df_target.loc[self.df_target.location == loc, :]\n            cfm_target = data_target.log_cfm.values\n            ftl_target = data_target.log_ftl.values\n            target = np.concatenate([cfm_target, ftl_target], axis=0)\n        \n        if not self.is_predict:\n            return torch.tensor(inputs, dtype=torch.float),\\\n                torch.tensor(meta_input, dtype=torch.float),\\\n                torch.tensor(target, dtype=torch.float)\n        else:\n            return torch.tensor(inputs, dtype=torch.float),\\\n                torch.tensor(meta_input, dtype=torch.float)\n    \n    @staticmethod\n    def get_dataloader(dataset, batch_size):\n        return DataLoader(dataset, batch_size, shuffle=not dataset.is_predict)","c10d7957":"batch_size = 64\nx_cols = [\"log_cfm\", \"log_ftl\"]\ndataset_train = CovidDataset(data_train, metadata, x_cols, train_input_end, random_clip=True)\ndataset_val = CovidDataset(data_val, metadata, x_cols, train_input_end)\ndataset_test = CovidDataset(input_test, metadata, x_cols, 0, test_input_beg, True)\ndl_train = CovidDataset.get_dataloader(dataset_train, batch_size)\ndl_val = CovidDataset.get_dataloader(dataset_val, batch_size)\ndl_test = CovidDataset.get_dataloader(dataset_test, batch_size)","2de01ad0":"class CovidTransformer(nn.Module):\n    \n    def __init__(self, d_in, d_out, d_model=128, d_fwd=512, n_head=4,\n            num_layers=6, dropout=0.1, d_meta=13, num_mlp_layers=3, d_mlp=512):\n        super().__init__()\n        self.linear = nn.Linear(d_in, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(d_model, n_head, d_fwd, dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.meta_transform = nn.Linear(d_meta, d_model)\n        mlp_components = []\n        for i in range(num_mlp_layers):\n            mlp_components.append(nn.Dropout(dropout))\n            if i == 0:\n                mlp_components.append(nn.Linear(2 * d_model, d_mlp))\n            elif i == num_mlp_layers - 1:\n                mlp_components.append(nn.Linear(d_mlp, d_out))\n            else:\n                mlp_components.append(nn.Linear(d_mlp, d_mlp))\n            mlp_components.append(nn.ReLU())\n        self.mlp = nn.Sequential(*mlp_components)\n    \n    def forward(self, inputs, meta_features):\n        x = self.linear(inputs)\n        features = self.transformer(x)\n        meta_features = self.meta_transform(meta_features)\n        features = torch.cat([features.mean(dim=0), meta_features], dim=-1)\n        output = self.mlp(features)\n        \n        return output","5c435e78":"class RMSLE(nn.Module):\n    \n    def forward(self, output, target):\n        return torch.sqrt(F.mse_loss(output, target))","eeb6f0ee":"class Engine(object):\n    \n    def compile(self, model, criterion, optimizer, lr_scheduler):\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n    \n    def _init_history(self):\n        self.history = {}\n        self.history[\"loss_train\"] = []\n        self.history[\"loss_val\"] = []\n    \n    def _update_history(self, train_loss, val_loss):\n        self.history[\"loss_train\"].append(train_loss)\n        self.history[\"loss_val\"].append(val_loss)\n    \n    def plot_loss(self):\n        fig, ax = plt.subplots()\n        ax.plot(self.history[\"loss_train\"], label=\"train\")\n        ax.plot(self.history[\"loss_val\"], label=\"val\")\n        plt.legend()\n        plt.show()\n    \n    def _fit_epoch(self, dl_train):\n        train_loss = 0.0\n        for _, sample in enumerate(dl_train):\n            inputs, meta_input, target = sample\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                meta_input = meta_input.cuda()\n                target = target.cuda()\n            inputs = torch.transpose(inputs, 1, 0)\n\n            # zero the parameter gradients\n            self.optimizer.zero_grad()\n\n            # forward + backward + optimize\n            output = self.model(inputs, meta_input)\n            \n            loss = self.criterion(output, target)\n            loss.backward()\n            self.optimizer.step()\n\n            train_loss += loss.detach().cpu().item()\n        \n        train_loss \/= len(dl_train)\n        self.lr_scheduler.step()\n        return train_loss\n    \n    def evaluate(self, dl_val):\n        val_loss = 0.0\n        self.model.eval()\n        \n        with torch.no_grad():\n            for _, sample in enumerate(dl_val):\n                inputs, meta_input, target = sample\n                if torch.cuda.is_available():\n                    inputs = inputs.cuda()\n                    meta_input = meta_input.cuda()\n                    target = target.cuda()\n                inputs = torch.transpose(inputs, 1, 0)\n\n                output = self.model(inputs, meta_input)\n\n                loss = self.criterion(output, target)\n                val_loss += loss.detach().cpu().item()\n        \n        val_loss \/= len(dl_val)\n        return val_loss\n    \n    def fit(self, epochs, dl_train, dl_val=None):\n        self._init_history()\n        progress = tqdm(total=epochs)\n        \n        for i in range(epochs):\n            progress.set_description_str(\"Epoch {}\/{}\"\n                .format(i + 1, epochs))\n            \n            train_loss = self._fit_epoch(dl_train)\n            val_loss = self.evaluate(dl_val)\n            \n            self._update_history(train_loss, val_loss)\n            \n            postfix = \"RMSLE - train: {:.4f}, val: {:.4f}\"\\\n                .format(train_loss, val_loss)\n            progress.set_postfix_str(postfix)\n            progress.update(1)\n    \n    def predict(self, dl):\n        self.model.eval()\n        cfm_output = []\n        ftl_output = []\n        \n        with torch.no_grad():\n            for _, sample in enumerate(dl):\n                inputs, meta_input = sample\n                if torch.cuda.is_available():\n                    inputs = inputs.cuda()\n                    meta_input = meta_input.cuda()\n                inputs = torch.transpose(inputs, 1, 0)\n\n                output_batch = self.model(inputs, meta_input)\n                cfm_output.append(output_batch[:, :output_batch.size(1) \/\/ 2])\n                ftl_output.append(output_batch[:, output_batch.size(1) \/\/ 2:])\n        \n        cfm_output = torch.cat(cfm_output, dim=0).expm1()\n        ftl_output = torch.cat(ftl_output, dim=0).expm1()\n        \n        return cfm_output, ftl_output","87306c46":"model = CovidTransformer(2, 2 * output_len, dropout=0.2)\nif torch.cuda.is_available():\n    model = model.cuda()\ncriterion = RMSLE()\noptimizer = AdamW(model.parameters())\nlr_scheduler = ExponentialLR(optimizer, 0.95)","bdb68c88":"engine = Engine()\nengine.compile(model, criterion, optimizer, lr_scheduler)","6e96c329":"engine.fit(100, dl_train, dl_val)","d867856d":"engine.plot_loss()","18fc9b35":"cfm_pred, ftl_pred = engine.predict(dl_test)","73c8466b":"def make_predictions(loc_list, date_pred, cfm_pred, ftl_pred):\n    num_loc = len(loc_list)\n    num_days = cfm_pred.size(1)\n    cfm_pred = cfm_pred.numpy().reshape(-1)\n    ftl_pred = ftl_pred.numpy().reshape(-1)\n    location = [\"\"] * len(date_pred)\n    for i in range(num_loc):\n        for j in range(num_days):\n            location[i * num_days + j] = loc_list[i]\n    pred_unknown = pd.DataFrame({\n        \"location\": location,\n        \"Date\": date_pred,\n        \"ConfirmedCases\": cfm_pred,\n        \"Fatalities\": ftl_pred\n    })\n    pred_unknown[\"Date\"] = pred_unknown.Date.map(lambda x: x.strftime(\"%Y-%m-%d\"))\n    pred_unknown[\"loc_date\"] = pred_unknown.apply(lambda x: \"_\".join([x[\"location\"], x[\"Date\"]]), axis=1)\n    \n    return pred_unknown[[\"loc_date\", \"ConfirmedCases\", \"Fatalities\"]]","4ca016db":"pred_unknown = make_predictions(dataset_test.loc_list, data_test_unknown.Date, cfm_pred, ftl_pred)","a3df5df9":"pred_unknown","0b9cd507":"data_test_unknown = data_test_unknown.merge(pred_unknown, on=\"loc_date\")","9968d2b1":"data_test_known","a6823565":"data_test_unknown","740505b6":"submission = pd.concat([data_test_known, data_test_unknown], axis=0)[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]","b99c39a9":"submission.to_csv(\"submission.csv\", index=False)","c3a5f97f":"# Metadata preprocessing","7d67e095":"# Add lockdown data","fd1fa5ef":"# Load raw data","ce280048":"# Preprocess original data"}}