{"cell_type":{"3a405e44":"code","7a7d84c7":"code","977296ec":"code","737d2de6":"code","7b02602f":"code","c6db0022":"code","1b58473a":"code","164f52d7":"code","3d84ac97":"code","f2eb2882":"code","522605c3":"code","e67e833a":"code","f7002eb0":"code","6c154160":"code","02fa60b4":"code","b64da8a1":"code","0b7cd711":"code","16989acf":"code","2c0e67f2":"code","196c5811":"code","8f20d7e0":"code","bee8f844":"code","15d36446":"code","a76a7199":"code","fa41cfdf":"code","1f06a334":"code","e584e72e":"code","a656f865":"code","6eb1a993":"code","604a39c9":"code","03b64399":"code","608a8e37":"code","584bfa16":"code","6ecdd7bd":"code","69be628c":"code","ababf759":"code","38c93ab2":"code","b6e182b8":"code","67e2b820":"code","b043bc33":"code","b590ed6f":"code","4d7f73fd":"code","dc2a22fd":"code","a121298b":"code","6dcade37":"markdown","ea947e9d":"markdown","a8fb0e7d":"markdown","d8017005":"markdown","7e5c9cce":"markdown","f918ab52":"markdown","5ae9c599":"markdown","3d898e03":"markdown","e29713fa":"markdown","d7ad334d":"markdown","385dfa5f":"markdown","9721913e":"markdown","d8ec0d4f":"markdown","12b20088":"markdown","5df5efeb":"markdown","4f7f025c":"markdown","a27ef7d6":"markdown"},"source":{"3a405e44":"#importing required libraries\nfrom numpy.random import seed\nseed(42)\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport tensorflow\ntensorflow.random.set_seed(42)\n\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline","7a7d84c7":"#Create a Base Directory\nbase_dir = 'Base_Directory'\nos.mkdir(base_dir)\n\n#creating folders inside Base Directory\ntrain_dir = os.path.join(base_dir, 'Train_Directory')\nos.mkdir(train_dir)\nval_dir = os.path.join(base_dir, 'Validation_Directory')\nos.mkdir(val_dir)\n\n#creating separate folders for each label class in both train and validation directory\nnv = os.path.join(train_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(train_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(train_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(train_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(train_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(train_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(train_dir, 'df')\nos.mkdir(df)\n\n# create new folders inside val_dir\nnv = os.path.join(val_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(val_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(val_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(val_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(val_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(val_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(val_dir, 'df')\nos.mkdir(df)","977296ec":"#getting the metadata of the dataset\n\nmetadata = pd.read_csv('..\/input\/skin-cancer-mnist-ham10000\/HAM10000_metadata.csv')\nmetadata.head(10)","737d2de6":"#Skin Cancer by class\ng = sns.catplot(x=\"dx\", kind=\"count\", palette='bright', data=metadata)\ng.fig.set_size_inches(16, 5)\n\ng.ax.set_title('Skin Cancer by Class', fontsize=20)\ng.set_xlabels('Skin Cancer Class', fontsize=14)\ng.set_ylabels('Frequency of Occurance', fontsize=14)","7b02602f":"#Skin Cancer by sex\ng = sns.catplot(x=\"dx\", kind=\"count\", hue=\"sex\", palette='coolwarm', data=metadata)\ng.fig.set_size_inches(16, 5)\n\ng.ax.set_title('Skin Cancer by Sex', fontsize=20)\ng.set_xlabels('Skin Cancer Class', fontsize=14)\ng.set_ylabels('Frequency of Occurance', fontsize=14)\ng._legend.set_title('Sex')","c6db0022":"#Skin Cancer by age\ng = sns.catplot(x=\"dx\", kind=\"count\", hue=\"age\", palette='bright', data=metadata)\ng.fig.set_size_inches(16, 9)\n\ng.ax.set_title('Skin Cancer by Age', fontsize=20)\ng.set_xlabels('Skin Cancer Class', fontsize=14)\ng.set_ylabels('Frequency of Occurance', fontsize=14)\ng._legend.set_title('Age')","1b58473a":"# Skin Cancer occurence body localization\ng = sns.catplot(x=\"dx\", kind=\"count\", hue=\"localization\", palette='bright', data=metadata)\ng.fig.set_size_inches(16, 9)\n\ng.ax.set_title('Skin Cancer Localization', fontsize=20)\ng.set_xlabels('Skin Cancer Class', fontsize=14)\ng.set_ylabels('Frequency of Occurance', fontsize=14)\ng._legend.set_title('Localization')","164f52d7":"# this will tell us how many images are associated with each lesion_id\ndf = metadata.groupby('lesion_id').count()\n\n# now we filter out lesion_id's that have only one image associated with it\ndf = df[df['image_id'] == 1]\n\ndf.reset_index(inplace=True)\n\ndf.head(10)","3d84ac97":"# here we identify lesion_id's that have duplicate images and those that have only one image.\n\ndef identify_duplicates(x):\n    unique_list = list(df['lesion_id'])\n    if x in unique_list:\n        return 'no_duplicates'\n    else:\n        return 'has_duplicates'\n    \n# create a new colum that is a copy of the lesion_id column\nmetadata['duplicates'] = metadata['lesion_id']\n# apply the function to this new column\nmetadata['duplicates'] = metadata['duplicates'].apply(identify_duplicates)\n\nmetadata.head(10)","f2eb2882":"metadata.duplicates.value_counts()","522605c3":"#Now we filter out the images that don't have duplicates\ndf = metadata[metadata.duplicates == 'no_duplicates']\ndf.shape","e67e833a":"labels = df['dx']\n_, validation_set = train_test_split(df, test_size=0.17, random_state=42, stratify=labels)\nvalidation_set.shape","f7002eb0":"#creating a training set that excludes the validation set\n\n# This function identifies if an image is part of the train\n# or val set.\ndef identify_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(validation_set['image_id'])\n    \n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n# identify train and val rows\n\n# create a new column that is a copy of the image_id column\nmetadata['train_or_val'] = metadata['image_id']\n# apply the function to this new column\nmetadata['train_or_val'] = metadata['train_or_val'].apply(identify_val_rows)\n   \n# filter out train rows\ntraining_set = metadata[metadata['train_or_val'] == 'train']\n\n#Dropping the unwanted columns\ntraining_set.drop(['train_or_val', 'duplicates'], axis=1, inplace=True)\nvalidation_set.drop(['duplicates'], axis=1, inplace=True)\n\nprint(training_set.shape)\nprint(validation_set.shape)","6c154160":"#setting image_id as the index of the metadata\nmetadata.set_index('image_id', inplace=True)\n\n#getting list of images in each of the 2 folders\nfolder1 = os.listdir('..\/input\/skin-cancer-mnist-ham10000\/HAM10000_images_part_1')\nfolder2 = os.listdir('..\/input\/skin-cancer-mnist-ham10000\/HAM10000_images_part_2')\n\n#getting list of training and validation images\ntrain_list = list(training_set['image_id'])\nval_list = list(validation_set['image_id'])\n\n#transferring the training images\nfor image in train_list:\n    filename = image + '.jpg'\n    label = metadata.loc[image, 'dx']\n    \n    if filename in folder1:\n        source = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/HAM10000_images_part_1', filename)\n    elif filename in folder2:\n        source = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/HAM10000_images_part_2', filename)\n    \n    destination = os.path.join(train_dir, label, filename)\n    shutil.copyfile(source, destination)   #copy the image from source to destination\n\n#transferring the validation images\nfor image in val_list:\n    filename = image + '.jpg'\n    label = metadata.loc[image, 'dx']\n    \n    if filename in folder1:\n        source = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/HAM10000_images_part_1', filename)\n    elif filename in folder2:\n        source = os.path.join('..\/input\/skin-cancer-mnist-ham10000\/HAM10000_images_part_2', filename)\n    \n    destination = os.path.join(val_dir, label, filename)\n    shutil.copyfile(source, destination)   #copy the image from source to destination","02fa60b4":"# check how many train images we have in each folder\n\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/akiec')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/mel')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/bcc')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/bkl')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/df')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/nv')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/vasc')))","b64da8a1":"# check how many val images we have in each folder\n\nprint(len(os.listdir('.\/Base_Directory\/Validation_Directory\/nv')))\nprint(len(os.listdir('.\/Base_Directory\/Validation_Directory\/mel')))\nprint(len(os.listdir('.\/Base_Directory\/Validation_Directory\/bkl')))\nprint(len(os.listdir('.\/Base_Directory\/Validation_Directory\/bcc')))\nprint(len(os.listdir('.\/Base_Directory\/Validation_Directory\/akiec')))\nprint(len(os.listdir('.\/Base_Directory\/Validation_Directory\/vasc')))\nprint(len(os.listdir('.\/Base_Directory\/Validation_Directory\/df')))","0b7cd711":"#Since there is a class imbalance we can try to augment images of the class that has very less images\nclass_list = ['mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']\n\n\nfor image_class in class_list:\n    aug_dir = 'Augmented_Directory'\n    os.mkdir(aug_dir)        #Creating a augumentation directory\n    img_dir = os.path.join(aug_dir, 'Image_Directory')\n    os.mkdir(img_dir)\n    \n    #collecting all the images that needs to be augumented into a single folder 'Image_Directory'\n    image_list = os.listdir('.\/Base_Directory\/Train_Directory\/' + image_class)\n    for filename in image_list:\n        source = os.path.join('.\/Base_Directory\/Train_Directory', image_class, filename)\n        destination = os.path.join(img_dir, filename)\n        shutil.copyfile(source, destination)\n    path = aug_dir\n    save_path = '.\/Base_Directory\/Train_Directory\/' + image_class\n    \n    #Creating a Data Generator\n    datagen = ImageDataGenerator(rotation_range=180,\n                                 width_shift_range=0.1,\n                                 height_shift_range=0.1,\n                                 zoom_range=0.1,\n                                 horizontal_flip=True,\n                                 vertical_flip=True,\n                                 #brightness_range=(0.9,1.1),\n                                 fill_mode='nearest')\n    batch_size=50\n    aug_data = datagen.flow_from_directory(path, save_to_dir=save_path, save_format='jpg',\n                                           target_size=(224, 224), batch_size=batch_size)\n    images_wanted = 6000   #total number of images we require for each class\n    num_files = len(os.listdir(img_dir))\n    num_batch = int(np.ceil((images_wanted - num_files)\/batch_size))\n    \n    #Run the generator to create about 6000 augumented images\n    for i in range(num_batch):\n        imgs, labels = next(aug_data)\n    \n    shutil.rmtree(aug_dir)  #deleting the temporary directory with the raw images","16989acf":"#The number of images per class we now have for training\n\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/nv')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/mel')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/bkl')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/bcc')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/akiec')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/vasc')))\nprint(len(os.listdir('.\/Base_Directory\/Train_Directory\/df')))","2c0e67f2":"def plots(ims, figsize=(20,10), rows=5, interp=False, titles=None): # 12,6\n    if type(ims[0]) is np.ndarray:\n        ims = np.array(ims).astype(np.uint8)\n        if (ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)\/\/rows if len(ims) % 2 == 0 else len(ims)\/\/rows + 1\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], interpolation=None if interp else 'none')\n        \nplots(imgs, titles=None) # titles=labels will display the image labels","196c5811":"train_path = '.\/Base_Directory\/Train_Directory'\nvalid_path = '.\/Base_Directory\/Validation_Directory'\n\nnum_train_samples = len(training_set)\nnum_val_samples = len(validation_set)\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps = np.ceil(num_train_samples\/ train_batch_size)\nval_steps = np.ceil(num_val_samples\/ val_batch_size)","8f20d7e0":"#same pre-processing that was applied to the original rgb Imagenet images that were used to train mobilenet is used to pre-process\n#this data.\ndatagen = ImageDataGenerator(\n            preprocessing_function=tensorflow.keras.applications.mobilenet.preprocess_input)\n\ntrain_batches = datagen.flow_from_directory(train_path, target_size=(image_size,image_size), batch_size=train_batch_size)\n\nvalid_batches = datagen.flow_from_directory(valid_path, target_size=(image_size,image_size), batch_size=val_batch_size)\n\ntest_batches = datagen.flow_from_directory(valid_path, target_size=(image_size,image_size), batch_size=1, shuffle=False)","bee8f844":"# create a copy of a mobilenet model\nmobile = tensorflow.keras.applications.mobilenet.MobileNet()","15d36446":"mobile.summary()","a76a7199":"print('The number of layers MobileNet has is ', len(mobile.layers))","fa41cfdf":"# CREATE THE MODEL ARCHITECTURE\n\n# Exclude the last 5 layers of the above model.\n# This will include all layers up to and including global_average_pooling2d_1\nx = mobile.layers[-6].output\n\n# Create a new dense layer for predictions\n# 7 corresponds to the number of classes\nx = Dropout(0.25)(x)\npredictions = Dense(7, activation='softmax')(x)\n\n# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n# dense layer we created above.\n\nmodel = Model(inputs=mobile.input, outputs=predictions)","1f06a334":"model.summary()","e584e72e":"# Define Top2 and Top3 Accuracy\n\nfrom tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n\ndef top_2_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=2)","a656f865":"model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])","6eb1a993":"#Get the labels that are associated with each index\nprint(valid_batches.class_indices)","604a39c9":"# Add weights to try to make the model more sensitive to melanoma\n\nclass_weights={\n    0: 1.0, # akiec\n    1: 1.0, # bcc\n    2: 1.0, # bkl\n    3: 1.0, # df\n    4: 3.0, # mel   # Try to make the model more sensitive to Melanoma.\n    5: 1.0, # nv\n    6: 1.0, # vasc\n}","03b64399":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_batches, steps_per_epoch=train_steps, class_weight=class_weights, validation_data=valid_batches,\n                              validation_steps=val_steps, epochs=30, verbose=1, callbacks=callbacks_list)","608a8e37":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","584bfa16":"# Here the the last epoch will be used.\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = model.evaluate_generator(test_batches, steps=len(validation_set))\n\nprint('val_loss:', val_loss)\nprint('val_categorical_accuracy:', val_cat_acc)\nprint('val_top_2_accuracy:', val_top_2_acc)\nprint('val_top_3_accutacy:', val_top_3_acc)","6ecdd7bd":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_top2_acc = history.history['top_2_accuracy']\nval_top2_acc = history.history['val_top_2_accuracy']\ntrain_top3_acc = history.history['top_3_accuracy']\nval_top3_acc = history.history['val_top_3_accuracy']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training cat acc')\nplt.plot(epochs, val_acc, 'b', label='Validation cat acc')\nplt.title('Training and validation cat accuracy')\nplt.legend()\nplt.figure()\n\n\nplt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\nplt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\nplt.title('Training and validation top2 accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, train_top3_acc, 'bo', label='Training top3 acc')\nplt.plot(epochs, val_top3_acc, 'b', label='Validation top3 acc')\nplt.title('Training and validation top3 accuracy')\nplt.legend()\n\n\nplt.show()","69be628c":"test_labels = test_batches.classes\nprint(test_batches.class_indices)","ababf759":"#making a prediction on the test batch\npredictions = model.predict(test_batches, steps=len(validation_set), verbose=1)","38c93ab2":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","b6e182b8":"#plotting confusion matrix\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\ncm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","67e2b820":"# Get the index of the class with the highest probability score\ny_pred = np.argmax(predictions, axis=1)\n\n# Get the labels of the test images.\ny_true = test_batches.classes","b043bc33":"from sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n\nprint(report)","b590ed6f":"# End of Model Building\n### ===================================================================================== ###\n# Convert the Model from Keras to Tensorflow.js","4d7f73fd":"!pip install tensorflowjs --ignore-installed","dc2a22fd":"import tensorflowjs as tfjs\nos.mkdir('TFJS_Dir')\n\ntfjs.converters.save_keras_model(model, 'TFJS_Dir')","a121298b":"# Deleting the image data directory to prevent a Kaggle error.\nshutil.rmtree(base_dir)","6dcade37":"# Creating a confusion matrix to analyse better :","ea947e9d":"# **Image augumentation**","a8fb0e7d":"Welcome to my kernel\nSkin cancer is the most common human malignancy, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions.\n\nThis the HAM10000 (\"Human Against Machine with 10000 training images\") dataset.It consists of 10015 dermatoscopicimages which are released as a training set for academic machine learning purposes and are publiclyavailable through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts.\n\nIt has 7 different classes of skin cancer which are listed below :\n1. Melanocytic nevi\n2. Melanoma\n3. Benign keratosis-like lesions\n4. Basal cell carcinoma\n5. Actinic keratoses\n6. Vascular lesions\n7. Dermatofibroma","d8017005":"# Data Exploration","7e5c9cce":"# **Training the Model :**","f918ab52":"# **Plotting the training curves**","5ae9c599":"# **Transfer the images into the folders**","3d898e03":"# **Visualizing the images :**","e29713fa":"# ***Objective :***\n\n   Create an online tool that can tell doctors and lab technologists the three highest probability diagnoses for a given skin lesion.    This will help them quickly identify high priority patients and speed up their workflow. The app should produce a result in less      than 3 seconds. To ensure privacy the images must be pre-processed and analysed locally and never be uploaded to an external          server.","d7ad334d":"# **Set up the generators :**","385dfa5f":"# Classification Report","9721913e":"# **Create a directory structure :**","d8ec0d4f":"# **LABELS :**\n\n**1. Melanocytic nevi (nv) -**\n\nMelanocytic nevi are benign neoplasms of melanocytes and appear in a myriad of variants, which all are included in our series. The variants may differ significantly from a dermatoscopic point of view.\n\n[6705 images]\n\n**2. Melanoma (mel) -**\n\nMelanoma is a malignant neoplasm derived from melanocytes that may appear in different variants. If excised in an early stage it can be cured by simple surgical excision. Melanomas can be invasive or non-invasive (in situ). We included all variants of melanoma including melanoma in situ, but did exclude non-pigmented, subungual, ocular or mucosal melanoma.\n\n[1113 images]\n\n**3. Benign keratosis-like lesions (bkl) -**\n\n\"Benign keratosis\" is a generic class that includes seborrheic ker- atoses (\"senile wart\"), solar lentigo - which can be regarded a flat variant of seborrheic keratosis - and lichen-planus like keratoses (LPLK), which corresponds to a seborrheic keratosis or a solar lentigo with inflammation and regression [22]. The three subgroups may look different dermatoscop- ically, but we grouped them together because they are similar biologically and often reported under the same generic term histopathologically. From a dermatoscopic view, lichen planus-like keratoses are especially challeng- ing because they can show morphologic features mimicking melanoma [23] and are often biopsied or excised for diagnostic reasons.\n\n[1099 images]\n\n**4. Basal cell carcinoma (bcc) -**\n\nBasal cell carcinoma is a common variant of epithelial skin cancer that rarely metastasizes but grows destructively if untreated. It appears in different morphologic variants (flat, nodular, pigmented, cystic, etc) [21], which are all included in this set.\n\n[514 images]\n\n**5. Actinic keratoses (akiec) -**\n\nActinic Keratoses (Solar Keratoses) and intraepithelial Carcinoma (Bowen\u2019s disease) are common non-invasive, variants of squamous cell car- cinoma that can be treated locally without surgery. Some authors regard them as precursors of squamous cell carcinomas and not as actual carci- nomas. There is, however, agreement that these lesions may progress to invasive squamous cell carcinoma - which is usually not pigmented. Both neoplasms commonly show surface scaling and commonly are devoid of pigment. Actinic keratoses are more common on the face and Bowen\u2019s disease is more common on other body sites. Because both types are in- duced by UV-light the surrounding skin is usually typified by severe sun damaged except in cases of Bowen\u2019s disease that are caused by human papilloma virus infection and not by UV. Pigmented variants exists for Bowen\u2019s disease [19] and for actinic keratoses [20]. Both are included in this set.\n\n[327 images]\n\n**6. Vascular lesions (vasc) -**\n\nVascular skin lesions in the dataset range from cherry angiomas to angiokeratomas [25] and pyogenic granulomas [26]. Hemorrhage is also included in this category.\n\n[142 images]\n\n**7. Dermatofibrom (df) -**\n\nDermatofibroma is a benign skin lesion regarded as either a benign proliferation or an inflammatory reaction to minimal trauma. It is brown often showing a central zone of fibrosis dermatoscopically [24].\n\n[115 images]\n\n\n[Total images = 10015]","12b20088":"# **Splitting data into training and validation sets :**","5df5efeb":"# **Modifying the MobileNet model**","4f7f025c":"This graph shows the dataset has a major problem of class imbalance","a27ef7d6":"The validation set results are greater than training set because when training, a percentage of the features are set to zero (25% in this case as I'm using Dropout(0.25)). When testing, all features are used (and are scaled appropriately). So the model at test time is more robust - and can lead to higher testing accuracies."}}