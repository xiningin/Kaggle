{"cell_type":{"bc218dc4":"code","75f05998":"code","708d6298":"code","9475bf17":"code","9cb907b9":"code","8614b101":"code","e22c0b74":"code","052f8260":"code","4b71f9a3":"code","c1dcfed5":"code","dd47f80e":"code","c7d66134":"code","25807757":"code","25492f19":"code","8c295705":"code","3c1f7f29":"code","a74f15dd":"code","674b8141":"code","75b83633":"code","0ddd5e36":"code","2f922998":"code","9ce995c6":"code","c7ea4cea":"code","95eb27fe":"code","4cbb3c70":"code","079aedcf":"code","eeb6001c":"code","793bd248":"code","04707bdf":"code","4b187bca":"code","76d48c82":"code","299b51fe":"code","1a446d3a":"code","ef6c0f07":"code","b14feadb":"code","d6d8dfb1":"code","fe3c662b":"code","787a34c4":"code","77ec2390":"code","efe65647":"code","725e79f3":"code","978ac816":"code","b25f9aae":"code","32a2d2ae":"code","1c954695":"code","0496e174":"code","361a05d6":"code","1b41bc72":"code","1912121c":"code","d999a523":"code","2415aa42":"code","42969831":"code","f1510c5b":"code","414dc92a":"code","69cca211":"code","7708db81":"code","d6ff18c5":"code","013e69b4":"code","00539a33":"code","a8092833":"code","47745a0c":"code","07ca25e3":"code","6e3227c0":"code","460e8966":"code","fb8f3fb5":"code","7c5a3c4c":"code","f2626b88":"code","d6ba678e":"code","f8948fad":"code","bab096c2":"code","8dbedc40":"code","7a211989":"code","fee7f820":"code","49dd8a36":"code","606a0317":"code","9e62dcf7":"code","050227be":"code","c9976680":"code","6bdd4d6a":"code","f5889889":"code","ad8db9f4":"code","3352e5b8":"markdown","cf0c6064":"markdown","4eb52241":"markdown","be837429":"markdown","f381d0b4":"markdown","40194432":"markdown","7ed32316":"markdown","5d0dad16":"markdown","c85afc9b":"markdown","018a87fd":"markdown","5d0a5d0f":"markdown","e1accda1":"markdown","0078efc5":"markdown","644f68ec":"markdown","6b66dbc9":"markdown","d67b6e1c":"markdown","0c98f895":"markdown","db7bfce8":"markdown","025b8c1e":"markdown","016450b8":"markdown","f87fca95":"markdown","c47b374f":"markdown","2073ff8f":"markdown","edd3748f":"markdown","35952e18":"markdown","10ba9024":"markdown","6c852e4c":"markdown","b376ddc3":"markdown","2f2e0496":"markdown","c68fd0ce":"markdown","a00ec6f1":"markdown","365e95a5":"markdown","7729410d":"markdown","3e46eeed":"markdown","409c32f3":"markdown","e9386966":"markdown","ac4b4bc1":"markdown","2774f904":"markdown","290d2dfb":"markdown"},"source":{"bc218dc4":"# Quick load dataset and check\nimport pandas as pd\nimport os\nrunning_local = True if os.getenv('JUPYTERHUB_USER') is None else False\nos.listdir('data')\nif ~running_local:\n    path = \"data\/final-project-dataset\/\"\nelse:\n    path = \".\/\"\n","75f05998":"filename = \"train_set.csv\"\ndata_train = pd.read_csv(\"data\/final-project-dataset\/train_set.csv\")\nfilename = \"test_set.csv\"\ndata_test = pd.read_csv(\"data\/final-project-dataset\/test_set.csv\")","708d6298":"data_train.describe()","9475bf17":"data_test.describe()","9cb907b9":"\nfrom sklearn.tree import DecisionTreeClassifier\n\n## Select target and features\nfea_col = data_train.columns[2:]\n\ndata_Y = data_train['target']\ndata_X = data_train[fea_col]\n\nclf = DecisionTreeClassifier()\nclf = clf.fit(data_X,data_Y)\ny_pred = clf.predict(data_X)","8614b101":"sum(y_pred==data_Y)\/len(data_Y)","e22c0b74":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(data_X, data_Y, test_size = 0.3, shuffle = True)\nclf = DecisionTreeClassifier(min_impurity_decrease = 0.001)\nclf = clf.fit(x_train, y_train)\ny_pred = clf.predict(x_val)","052f8260":"sum(y_pred==y_val)\/len(y_val)","4b71f9a3":"def extrac_one_label(x_val, y_val, label):\n    X_pos = x_val[y_val == label]\n    y_pos = y_val[y_val == label]\n    return X_pos, y_pos\n\nX_pos, y_pos = extrac_one_label(x_val, y_val, 1)\ny_pospred = clf.predict(X_pos)\nsum(y_pospred==y_pos)\/len(y_pos)\n#print(x_val.shape)\n#print(X_pos.shape)\n#print(y_val.shape)\n#print(y_pos)\n#print(y_pospred)","c1dcfed5":"X_neg, y_neg = extrac_one_label(x_val, y_val, 0)\ny_negpred = clf.predict(X_neg)\nsum(y_negpred==y_neg)\/len(y_neg)\n#print(sum(y_negpred==y_neg))","dd47f80e":"print(sum(data_Y==0)\/len(data_Y), sum(data_Y==1))","c7d66134":"## Your work\nfrom sklearn.model_selection import train_test_split\n# our data to train\ndata_train.head()\n\n#X_train = x_train\n#y_train = y_train\n#X_test = x_val\n#y_test = y_val\n\nX_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, test_size = 0.3, shuffle = True)\n\n# selecting only a subset of the actual data since computing could take much longer with the entire set \nprint('X_train shape: ', X_train.shape)\nX_train_2 = X_train[0:2000]\nprint('X_train_2 shape: ', X_train_2.shape)\n\nprint('y_train shape: ', y_train.shape)\ny_train_2 = y_train[0:2000]\nprint('y_train_2 shape: ', y_train_2.shape)\n\nprint('X_test shape: ', X_test.shape)\nX_test_2 = X_test[0:600]\nprint('X_test_2 shape: ', X_test_2.shape)\n\nprint('y_test shape: ', y_test.shape)\ny_test_2 = y_test[0:600]\nprint('y_test_2 shape: ', y_test_2.shape)\n\n\n","25807757":"print(X_test_2.shape)","25492f19":"import numpy as np\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split","8c295705":"# creating a svm classifier\nclf = svm.SVC(kernel='linear', C=1)","3c1f7f29":"# training said classifies\nclf.fit(X_train_2, y_train_2)","a74f15dd":"#clf.score(X_test_2, y_test_2)\n\n# predict response\ny_pred_SVM = clf.predict(X_test_2)\n\n","674b8141":"from sklearn import metrics\n# checking how accurately the prediction was\naccuracy = metrics.accuracy_score(y_test_2, y_pred_SVM)\nprint('Accuracy of :',accuracy)\n","75b83633":"print('our predicted values: ' ,y_pred_SVM)","0ddd5e36":"print('# of y_test values which are 1:', sum(y_test_2[0:]))","2f922998":"all_hists = X_train.hist(bins=20, figsize=(50,25))","9ce995c6":"data_X[data_X == -1].count()\n","c7ea4cea":"data_X = data_X.drop([\"ps_reg_03\", \"ps_car_03_cat\", \"ps_car_05_cat\", \"ps_car_14\"], axis = 1)","95eb27fe":"data_X[data_X == -1].count()","4cbb3c70":"all_hists = X_train.hist(bins=20, figsize=(50,25))","079aedcf":"X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, test_size = 0.3, shuffle = True)\n\n# selecting only a subset of the actual data since computing could take much longer with the entire set \nprint('X_train shape: ', X_train.shape)\nX_train_2 = X_train[0:2000]\nprint('X_train_2 shape: ', X_train_2.shape)\n\nprint('y_train shape: ', y_train.shape)\ny_train_2 = y_train[0:2000]\nprint('y_train_2 shape: ', y_train_2.shape)\n\nprint('X_test shape: ', X_test.shape)\nX_test_2 = X_test[0:600]\nprint('X_test_2 shape: ', X_test_2.shape)\n\nprint('y_test shape: ', y_test.shape)\ny_test_2 = y_test[0:600]\nprint('y_test_2 shape: ', y_test_2.shape)","eeb6001c":"from sklearn import metrics\nclf_no_neg = svm.SVC(kernel='linear', C=1)\n# training said classifies\nclf_no_neg.fit(X_train_2, y_train_2)\n#prediction\ny_pred_SVM_no_neg = clf_no_neg.predict(X_test_2)\n# checking how accurately the prediction was\naccuracy_no_neg = metrics.accuracy_score(y_test_2, y_pred_SVM_no_neg)\nprint('Accuracy of :',accuracy_no_neg)\nprint('# of y_test values which are 1:', sum(y_test_2[0:]))\nprint('# of our predicted values: ' ,sum(y_pred_SVM_no_neg))","793bd248":"from sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n\nconf_mat = confusion_matrix(y_true=y_test_2, y_pred=y_pred_SVM_no_neg)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","04707bdf":"from sklearn import metrics\nclf = svm.SVC(kernel='linear', class_weight='balanced', C=1.0)\n# training said classifies\nclf.fit(X_train_2, y_train_2)\n#prediction\ny_pred_SVM = clf.predict(X_test_2)\n# checking how accurately the prediction was\naccuracy = metrics.accuracy_score(y_test_2, y_pred_SVM)\nprint('Accuracy of :',accuracy)","4b187bca":"print('# of y_test values which are 1:', sum(y_test_2[0:]))\nprint('# of our predicted values: ' , sum(y_pred_SVM))","76d48c82":"clf = svm.SVC(kernel='rbf', class_weight='balanced', C=1.0)\n# training said classifies\nclf.fit(X_train_2, y_train_2)\n#prediction\ny_pred_SVM = clf.predict(X_test_2)\n# checking how accurately the prediction was\naccuracy = metrics.accuracy_score(y_test_2, y_pred_SVM)\nprint('Accuracy of :',accuracy)\nprint('# of y_test values which are 1:', sum(y_test_2[0:]))\nprint('# of our predicted values: ' , sum(y_pred_SVM))","299b51fe":"conf_mat = confusion_matrix(y_true=y_test_2, y_pred=y_pred_SVM)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","1a446d3a":"!pip install imblearn","ef6c0f07":"import imblearn","b14feadb":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","d6d8dfb1":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_2)\n\nplot_2d_space(X_train_pca, y_train_2, 'Imbalanced dataset (2 PCA components)')","fe3c662b":"# import the SMOTETomek\nfrom imblearn.over_sampling import SMOTE\n\n# create the  object with the desired sampling strategy.\nsmote = SMOTE(sampling_strategy='minority')\n\n# fit the object to our training data\nx_train_smote, y_train_smote = smote.fit_sample(X_train_2, y_train_2)\n\nclf = svm.SVC(kernel='linear', C=1.0)\n# training said classifies\nclf.fit(x_train_smote, y_train_smote)\n#prediction\ny_predict_smote = clf.predict(X_test_2)\n# checking how accurately the prediction was\naccuracy = metrics.accuracy_score(y_test_2, y_predict_smote)\nprint('Accuracy of :',accuracy)\nprint('# of y_test values which are 1:', sum(y_test_2[0:]))\nprint('# of our predicted values: ' , sum(y_predict_smote))","787a34c4":"print(y_predict_smote)","77ec2390":"X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, test_size = 0.3, shuffle = True)\n\n# selecting only a subset of the actual data since computing could take much longer with the entire set \nprint('X_train shape: ', X_train.shape)\nX_train_2 = X_train[0:15000]\nprint('X_train_2 shape: ', X_train_2.shape)\n\nprint('y_train shape: ', y_train.shape)\ny_train_2 = y_train[0:15000]\nprint('y_train_2 shape: ', y_train_2.shape)\n\nprint('X_test shape: ', X_test.shape)\nX_test_2 = X_test[0:3500]\nprint('X_test_2 shape: ', X_test_2.shape)\n\nprint('y_test shape: ', y_test.shape)\ny_test_2 = y_test[0:3500]\nprint('y_test_2 shape: ', y_test_2.shape)","efe65647":"smote = SMOTE(sampling_strategy='minority')\nx_train_smote, y_train_smote = smote.fit_sample(X_train_2, y_train_2)\n\nclf = svm.SVC(kernel='linear', C=1.0)\nclf.fit(x_train_smote, y_train_smote)\n#prediction\ny_predict_smote = clf.predict(X_test_2)\n# checking how accurately the prediction was\naccuracy = metrics.accuracy_score(y_test_2, y_predict_smote)\nprint('Accuracy of :',accuracy)\nprint('# of y_test values which are 1:', sum(y_test_2[0:]))\nprint('# of our predicted values: ' , sum(y_predict_smote))","725e79f3":"data_test.shape","978ac816":"data_test = data_test.drop([\"ps_reg_03\", \"ps_car_03_cat\", \"ps_car_05_cat\", \"ps_car_14\"], axis = 1)","b25f9aae":"data_test.shape","32a2d2ae":"data_test.describe()","1c954695":"data_train.shape","0496e174":"data_train = data_train.drop([\"ps_reg_03\", \"ps_car_03_cat\", \"ps_car_05_cat\", \"ps_car_14\"], axis = 1)","361a05d6":"data_train.shape","1b41bc72":"data_test_2 = data_test.drop(columns=['id'])\ndata_train_X = data_train.drop(columns=['id'])\nprint('train shape:', data_train_X.shape)\nprint('test shape:', data_test_2.shape)","1912121c":"#select data and targets\nfea_col = data_train.columns[2:]\ndata_Y = data_train_X['target']\ndata_X = data_train_X[fea_col]","d999a523":"# will take only 30k for training since it takes sooo long\n\nprint('X_train shape: ', data_X.shape)\nX_train_END = data_X[0:100000]\nprint('X_train_2 shape: ', X_train_END.shape)\n\nprint('y_train shape: ', data_Y.shape)\ny_train_END = data_Y[0:100000]\nprint('y_train_2 shape: ', y_train_END.shape)\n\n\n#\nprint('test values ', data_test_2.shape)","2415aa42":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(sampling_strategy='minority')","42969831":"X_train_smote_END, y_train_smote_END = smote.fit_sample(X_train_END, y_train_END)","f1510c5b":"X_train_smote_END.shape","414dc92a":"clf = svm.SVC(kernel='linear', C=1.0)","69cca211":"#fitting\nclf.fit(X_train_smote_END, y_train_smote_END)","7708db81":"# now predict the data_test values\ny_predict_smote = clf.predict(data_test_2)","d6ff18c5":"y_predict_smote.shape","013e69b4":"four_percent_of_all = 144880 * 0.04\nprint('4 % of all points would be:', four_percent_of_all)","00539a33":"print('# of our predicted values: ' , sum(y_predict_smote))","a8092833":"data_out = pd.DataFrame(data_test['id'].copy())\ndata_out.insert(1, \"target\", y_predict_smote, True) \ndata_out.to_csv(\"\/Users\/hercules\/ml\/data\/final-project-dataset\/submission.csv\",index=False)","47745a0c":"from io import StringIO","07ca25e3":"output = StringIO()","6e3227c0":"data_out.to_csv(output)","460e8966":"output.seek(0)","fb8f3fb5":"print(output.read())","7c5a3c4c":"filename = \"train_set.csv\"\ndata_train = pd.read_csv(\"data\/final-project-dataset\/train_set.csv\")\nfilename = \"test_set.csv\"\ndata_test = pd.read_csv(\"data\/final-project-dataset\/test_set.csv\")\n\ndata_train = data_train.drop([\"ps_reg_03\", \"ps_car_03_cat\", \"ps_car_05_cat\", \"ps_car_14\"], axis = 1)\ndata_train_X = data_train.drop(columns=['id'])\ndata_test = data_test.drop([\"ps_reg_03\", \"ps_car_03_cat\", \"ps_car_05_cat\", \"ps_car_14\"], axis = 1)\ndata_test_2 = data_test.drop(columns=['id'])\n\n#select data and targets\nfea_col = data_train.columns[2:]\ndata_Y = data_train_X['target']\ndata_X = data_train_X[fea_col]\n\n# will take only 30k for training since it takes sooo long\n\nprint('X_train shape: ', data_X.shape)\nX_train_END = data_X[0:10000]\nprint('X_train_END shape: ', X_train_END.shape)\n\nprint('y_train shape: ', data_Y.shape)\ny_train_END = data_Y[0:10000]\nprint('y_train_END shape: ', y_train_END.shape)\n\n\n#\nprint('test values ', data_test_2.shape)\n","f2626b88":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(sampling_strategy='minority')","d6ba678e":"X_train_smote_END, y_train_smote_END = smote.fit_sample(X_train_END, y_train_END)","f8948fad":"print('X_train shape: ', X_train_smote_END.shape)","bab096c2":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n# Train the model on training data\nrf.fit(X_train_smote_END, y_train_smote_END)","8dbedc40":"test_10k = data_test_2[0:10000]","7a211989":"predictions = rf.predict(test_10k)","fee7f820":"print(10000*0.04)\nprint('# of our predicted values: ' , sum(predictions))\nprint('without smote sampleing it was at 478')","49dd8a36":"predict_all = rf.predict(data_test_2)","606a0317":"print(148000*0.04)\nprint('# of our predicted values: ' , sum(predict_all))\nprint('without smote sampleing it was at 7169')\n","9e62dcf7":"filename = \"train_set.csv\"\ndata_train = pd.read_csv(\"data\/final-project-dataset\/train_set.csv\")\nfilename = \"test_set.csv\"\ndata_test = pd.read_csv(\"data\/final-project-dataset\/test_set.csv\")\n\ndata_train = data_train.drop([\"ps_reg_03\", \"ps_car_03_cat\", \"ps_car_05_cat\", \"ps_car_14\"], axis = 1)\ndata_train_X = data_train.drop(columns=['id'])\ndata_test = data_test.drop([\"ps_reg_03\", \"ps_car_03_cat\", \"ps_car_05_cat\", \"ps_car_14\"], axis = 1)\ndata_test_2 = data_test.drop(columns=['id'])\n\n#select data and targets\nfea_col = data_train.columns[2:]\ndata_Y = data_train_X['target']\ndata_X = data_train_X[fea_col]\n\n# will take only 30k for training since it takes sooo long\n\nprint('X_train shape: ', data_X.shape)\nX_train_END = data_X[0:30000]\nprint('X_train_END shape: ', X_train_END.shape)\n\nprint('y_train shape: ', data_Y.shape)\ny_train_END = data_Y[0:30000]\nprint('y_train_END shape: ', y_train_END.shape)\n\n\n#\nprint('test values ', data_test_2.shape)\n","050227be":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n# Train the model on training data\nrf.fit(X_train_END, y_train_END)","c9976680":"# create prediction of our test set:\npredict_all = rf.predict(data_test_2)","6bdd4d6a":"print(148000*0.04)\nprint('# of our predicted values: ' , sum(predict_all))","f5889889":"cnt = 0\nfor i in range(predict_all.size):\n    if predict_all[i] > 0.5:\n        predict_all[i] = 1\n        cnt = cnt + 1\n    else:\n        predict_all[i] = 0\ncnt","ad8db9f4":"data_out.describe()","3352e5b8":"a tiny bit better but still rather bad?","cf0c6064":"#### huh.. our accuracy is different now!\nbut also way worse. Let's check our the prediction data:","4eb52241":"#### interesting??\nwe got a accuracy of 90%, but we also predicted around as many ones as there really are! This time we were using the SMOTE Synthetic Minority Oversampling Technique. Breakthrough for once? let's check our the prediction values:","be837429":"### so looks like smote upsampleing doubles the number of predicted ones...\nnot really what we are looking for\nwill try again with more training values than 10k:","f381d0b4":"Intesting! All of our predicted values are 0????","40194432":"we see here well that there aren't many classes which have a huge spike at -1. That means most of the classes are distinguishable at 0 or 1, a few have gaussian curves in them a well. But makes most of the data clearly seperable","7ed32316":"## Phase 1: 26th May - 9th June\n\n### Data Description\n\nIn order to take a look at the data, you can use the `describe()` method. As you can see in the result, each row has a unique `id`. `Target` $\\in \\{0, 1\\}$ is whether a user will file a claim in his insurance period. The rest of the 57 columns are features regarding customers' profiles. You might also notice that some of the features have minimum values of `-1`. This indicates that the actual value is missing or inaccessible.\n","5d0dad16":"### again 96% accuracy??\nonce again we get 96% accuracy. Lets take a look at what our predictions are:","c85afc9b":"#### maybe our prediction is of because of the values that are -1 \naka not available. They could throw the set off. in the next block we'll see that quite a few features have loads of -1. ps_car_03_cat for example has 308336 empty values.","018a87fd":"### Validation metric\nThink about what the proper metric is to train your model and how you should  change your training procedure to aviod this problem?","5d0a5d0f":"\n### Example\n\nWe will use the decision tree classifier as an example.","e1accda1":"The confusion matrix here just shows that we dont have many matches with 0 0 but quite many where we predicted 1 but the class is actually zero...","0078efc5":"The block above tells us that our actual test results have a sum of 16 (could change at rerunning). This means there were 16 ones in the results... But we got zero ones? Since we have a total proportion of 96% zeros to 4% ones, it makes sense that with all ones we get a accuracy of 96%...\nThere must be something off with our training.","644f68ec":"### Your Turns:\n\nWhat does it mean? Why does it look like that? How do you overcome it and get the better results? Hint :","6b66dbc9":"The prefix, e.g. `ind` and `calc`, indicate the feature belongs to similiar groupings. The postfix `bin` indicates binary features and `cat` indicates categorical features. The features without postfix are ordinal or continuous. Similarly, you can check the statistics for testing data:","d67b6e1c":"gotta get rid of the extra colums in our test datafile first though:","0c98f895":"#### again.. 95ish percent only because values are all 0\nthe confusion matrix here shows the correct and incorrect predictions for each class.\nhave to come up with something else. Lets think about why that could be for a second. Our Dataset has loads of 0 and few 1. Having such an imbalaced dataset makes it hard for the training algorithm. Maybe we can adjust the weights somehow to fit better.\nLet's try again with SVM and balanced class weights:","db7bfce8":"### My Answer:\nThe above block shows how many people are classified with 0. Of our data we have 16271 people with label 1, and 430000 with 0 -> with simple math just calculating the probability we get that 96% are with label 1. We can keep this value in mind because from a statistical point theoretically we should get a similar percentage if we apply it to new data too. \nBut the classifier from further above also said that it was 96% of the time correct.\nThis seems suspicious","025b8c1e":"so we got 247 ones predicted... our of 600. This is a change... although a rather bad one. This shows that the weights definitely have something to do with it. We now gotta find out how to adjust them to get a better result with our predictions.\nlets try with a different kernel:","016450b8":"we of course cannot check the accuracy since we dont have any values to compare them with, but we can check how many ones we got to make sure the values are not all over the place.\nLets quickly remember: 96% of our training data had zeros and 4% were ones.\nWe now predicted a total of 148800 values.","f87fca95":"### trying svm again\nlets try svm once again, maybe this fixed the problem","c47b374f":"abstraction onto 2 dimensions... doesn't really show much distinction\nmight need to think about resampling the data to get better more data around the few 1 to train with and in turn better results...\nhttps:\/\/heartbeat.fritz.ai\/resampling-to-properly-handle-imbalanced-datasets-in-machine-learning-64d82c16ceaa\ntried a few of those resampling methods","2073ff8f":"### similar approach but with random forest:","edd3748f":"now the dataset is not crowded with features who are somewhat 'useless'. We also see that more graphy are kind of linearably seperable. There are quite a few graphs with values at the left and also values at the right side","35952e18":"For 15000 training we still get around 90% accuracy which seems pretty good. The overall accuracy is of course still worse than if we just assign 0 to everyone, but we have to think of it in terms of what our algorithm is used for. If we just assign 0 everywhere, then the insurance company would have to take every single customer and then would get 100% insurance claims for around 4% of them. If we can filter out the ones where we're fairly sure that they would claim the insurance, then the company does not have to get them their insurance and won't have to pay them in case they have an accident. Of course there are some false negatives, but in terms of overall profit, having a few less people in the insurance programm who might be save drivers and won't have an accident, is probably better than having a few where the insurance company has to pay a lot of money for if they have an accident.","10ba9024":"looks not too bad! let's try it with more than just 2000 training and 600 testing values. ","6c852e4c":"trying the training and testing with all of the data takes probably forever, leaving it for 30'+ didn't finish. 15000 for training already takes quit long, can't imagine the runtime for 300k. Since more training data generally means better results, it might be worth training with more data. But on the other hand, more data could also lead to overfitting and then in the end decrease our result. We'll try with a few more sizes and then decide on one for which we'll predict with on the data_test","b376ddc3":"#### lets take a look at our data\nhere we can see that we actually have a few features which dont give us any information at all. For example the ones where only one spike is in the graph... We also have a lot of features with the -1 value. That -1 value means we dont have a data for this point. ","2f2e0496":"\n\n**The decision tree has 100 percent accurate rate!**\n\nIt is unbelievable! What went wrong?\n\nHint: What is validation?\n\nAfter fixing the problem, you may start here and try to improve the accurate rates.","c68fd0ce":"# Machine Learning 2020 Course Projects\n\n## Project Schedule\n\nIn this project, you will solve a real-life problem with a dataset. The project will be separated into two phases:\n\n27th May - 10th June: We will give you a training set with target values and a testing set without target. You predict the target of the testing set by trying different machine learning models and submit your best result to us and we will evaluate your results first time at the end of phase 1.\n\n9th June - 24th June: Students stand high in the leader board will briefly explain  their submission in a proseminar. We will also release some general advice to improve the result. You try to improve your prediction and submit final results in the end. We will again ask random group to present and show their implementation.\nThe project shall be finished by a team of two people. Please find your teammate and REGISTER via [here](https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf4uAQwBkTbN12E0akQdxfXLgUQLObAVDRjqJHcNAUFwvRTsg\/alreadyresponded).\n\nThe submission and evaluation is processed by [Kaggle](https:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71).  In order to submit, you need to create an account, please use your team name in the `team tag` on the [kaggle page](https:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71). Two people can submit as a team in Kaggle.\n\nYou can submit and test your result on the test set 2 times a day, you will be able to upload your predicted value in a CSV file and your result will be shown on a leaderboard. We collect data for grading at 22:00 on the **last day of each phase**. Please secure your best results before this time.\n\n","a00ec6f1":"### dropping N\/A \nps_reg_03, ps_car_03_cat, ps_car_05_cat and ps_car_14 all have many not available values. Might make sense to drop those altogether since they wont give much info about the results anyway","365e95a5":"**None of the label is detected!** Now with label-0 data only:","7729410d":"### Submission\n\nPlease only submit the csv files with predicted outcome with its id and target [here](https:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71). Your column should only contain `0` and `1`.","3e46eeed":"#### 7500 ones.\nLooks alright. If we assume a accuracy of 90% that seems to be quite a successful prediction.","409c32f3":"## Project Description\n\nCar insurance companies are always trying to come up with a fair insurance plan for customers. They would like to offer a lower price to the careful and safe driver while the careless drivers who file claims in the past will pay more. In addition, more safe drivers mean that the company will spend less in operation. However, for new customers, it is difficult for the company to know who the safe driver is. As a result, if a company offers a low price, it bears a high risk of cost. If not, the company loses competitiveness and encourage new customers to choose its competitors.\n\n\nYour task is to create a machine learning model to mitigate this problem by identifying the safe drivers in new customers based on their profiles. The company then offers them a low price to boost safe customer acquirement and reduce risks of costs. We provide you with a dataset (train_set.csv) regarding the profile (columns starting with ps_*) of customers. You will be asked to predict whether a customer will file a claim (`target`) in the next year with the test_set.csv \n\n~~You can find the dataset in the `project` folders in the jupyter hub.~~ We also upload dataset to Kaggle and will test your result and offer you a leaderboard in Kaggle. Please find them under the Data tag on the following page:\nhttps:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71","e9386966":"### Information Beyond above Accuracy \n\nThe result looks promising. **Let us take a look into the results further.**\n\nWe now make a prediction for the valid set with label-1 data only:","ac4b4bc1":"### svm\nlets try it again with svm instead of decision tree from above","2774f904":"also dropping the id fields from the two sets","290d2dfb":"# ---------- end ----------"}}