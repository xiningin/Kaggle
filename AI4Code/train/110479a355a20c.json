{"cell_type":{"360350a4":"code","d290b537":"code","8aec8c8e":"code","c7ea3270":"code","854d26bf":"code","17ab9139":"code","2f530582":"code","752d2b60":"code","54c6d837":"code","2cb0f68b":"code","61b9dae3":"code","48965063":"code","e8047a1c":"code","52a6623f":"code","4b45661d":"code","67ca519a":"code","2164db37":"code","ebefe358":"code","d1a28d83":"code","4341d0c3":"code","3e9cb275":"code","7e4a5831":"code","44cecc7c":"code","080d8104":"code","d76baed1":"code","eeabd7fa":"code","a0ad9ee2":"code","1b15552f":"code","2065247c":"markdown","b7cd0cc8":"markdown","be99aa08":"markdown","b5d88ca4":"markdown","90abe87b":"markdown","18e1b288":"markdown","ef67718f":"markdown","4266286b":"markdown","d4c58aca":"markdown","824d1872":"markdown","2841a8e1":"markdown","7f9540a8":"markdown","dcf663d3":"markdown","f66ab49d":"markdown","89b3d7c3":"markdown","1e2e471f":"markdown","6c49118a":"markdown","d3ed944c":"markdown","96e5a8c6":"markdown","286cde02":"markdown","bcb6e118":"markdown","a62b0c7c":"markdown","d816afc9":"markdown","798a41ba":"markdown","0ca66aa5":"markdown","87af95c7":"markdown","f5fd7aea":"markdown","bf460bec":"markdown"},"source":{"360350a4":"import os\n\n# Standard plot using matplotlib\nfrom matplotlib import pyplot as plt\n\n# Enhance plotting\/visulization using seaborn\n# Seaborn uses matplotlib as base\nimport seaborn as sb\nimport numpy as np\n\n# For reading csv, manipulating, analysis data in tabular format\nimport pandas as pd\n\n\n# Preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\n\n# Model selection\/evaluation\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n\n# model import\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Plot all the charts in notebook\n%matplotlib inline","d290b537":"file_location = '\/kaggle\/input\/titanic\/'\ntrain_data = pd.read_csv(os.path.join(file_location, 'train.csv'))\ntest_data = pd.read_csv(os.path.join(file_location, 'test.csv'))\n\n# Let's join both data for preprocessing then later we can separate train and test data\ntotal_data = pd.concat([train_data, test_data], sort=False, ignore_index=True)","8aec8c8e":"total_data.isnull().sum()","c7ea3270":"# This might give idea to fillna value\nplt.figure(figsize=(8,4))\nsb.distplot(train_data['Age']);\n\n# Looks like most of the passenger are in age group of 20-40","854d26bf":"# This will give idea whether this feature will be useful for prediction or not\nplt.figure(figsize=(8,4))\nsb.boxplot(x='Survived', y='Age', data=train_data);\n\n# It is not much clear but will give get some idea, age more than 35 less survival rate","17ab9139":"# This will give idea whether this feature will be useful for prediction or not\nplt.figure(figsize=(12,6))\nsb.violinplot(x='Survived', y='Age', data=train_data, hue='Sex');\n\n# After looking into plot, Compared to male more female passenger having age less than 40 age survived.\n# Compared to female, Male passenger age between 18-40 has less survival rate","2f530582":"# This will give idea whether this feature will be useful for prediction or not\nplt.figure(figsize=(8,4))\nsb.countplot(x='Survived', data=train_data, hue='Sex');\n\n# After looking into plot, it is clear female survival rate is high","752d2b60":"# This will give idea whether this feature will be useful for prediction or not\nplt.figure(figsize=(8,4))\nsb.boxplot(x='Survived', y='Fare', data=train_data);\n\n# After looking into plot, it is clear that fare contributing largely to survival rate\n# (More paid having more survival rate) but with outliers\n# Need to fill NaN values as well need to treat outliers\n# Need to calculate fare per passenger to have better idea ***","54c6d837":"train_data['Cabin'].isnull().sum()\n# there are more missing values so need to analyze then decide whether feature is needed or not","2cb0f68b":"train_data['Cabin'].unique()\n# There are more unique values too. Better let's create another feature 'HasCabin'","61b9dae3":"def get_title(data):\n    \"\"\"\n    Get title of each person like Mr. Mrs, Miss etc from Name\n    \"\"\"\n    valid_title = ['Mr', 'Mrs', 'Miss', 'Master']\n    data['Title'] = data['Name'].map(lambda name: name.split(',')[1].split('.')[0].strip())\n    data.loc[data['Title'] == 'Sir', 'Title'] = 'Mr'\n    data.loc[data['Title'] == 'Ms', 'Title'] = 'Miss'\n    data.loc[~data['Title'].isin(valid_title), 'Title'] = 'Rare'\n\ndef calculate_fare(data):\n    \"\"\"\n    Fare per person by grouping on ticket. Grouping done outside this function.\n    \"\"\"\n    return data.mean()\/data.count()\n\ndef treat_fare(data):\n    \"\"\"\n    Fare 0 is not a valid value so let's calculate median based on pclass grouping. Grouping done outside this function\n    \"\"\"\n    return data.where(data != 0.0, data.median())","48965063":"total_data.Embarked.value_counts()","e8047a1c":"# Fill NaN values for Embarked with S because S having more occurrence\ntotal_data.Embarked.fillna('S', inplace=True)\n\n# Fill 1 NaN value of Fare value using median\ntotal_data.Fare.fillna(total_data['Fare'].median(), inplace=True)\n\n# Fill NaN with some value, then later we can use this information to create new feature\ntotal_data.Cabin.fillna('Z', inplace=True)\n\n# Calculate fare per person\ntotal_data.Fare = total_data.groupby('Ticket')['Fare'].transform(calculate_fare)\n\n# There are fare values with 0 so let's fix it\ntotal_data.Fare = total_data.groupby('Pclass')['Fare'].apply(treat_fare)","52a6623f":"# This will give idea whether this feature will be useful for prediction or not\nplt.figure(figsize=(8,4))\nsb.boxplot(x='Survived', y='Fare', data=train_data);\n\n# After looking into plot, it is clear that fare contributing largely to survival rate and there is outliers so let's fix it.","4b45661d":"# # Remove otliers based on box plot analysis\ntotal_data.loc[total_data['Fare'] > 60, 'Fare'] = 60","67ca519a":"# If there is valid cabin detail then set HasCabin as 1 otherwise 0\ntotal_data['HasCabin'] = np.where(total_data['Cabin'] == 'Z', 0, 1)\ntotal_data['FamilyMembers'] = total_data['SibSp'] + total_data['Parch']\n\n# If there is valid cabin detail then set HasCabin as 1 otherwise 0\ntotal_data['Alone'] = total_data['FamilyMembers'].map(lambda size: 0 if size > 0 else 1)\n\n# After filling NaN value for age, let's create new feature to map passenger as senior or not\ntotal_data['Senior'] = total_data['Age'].map(lambda size: 1 if size > 55 else 0)\n\n# New feature after removing outliers\ntotal_data['FareCat'] = pd.qcut(total_data['Fare'], 4, labels=range(1,5)).astype('int')\n\n# New feature \"Title\"\nget_title(total_data)\n\n# Let's fill age based on title. Miss - Likely between 10-30, Master 0-10\ntotal_data.Age = total_data.groupby('Title')['Age'].apply(lambda age: age.fillna(age.median()))\n\n# Let's create new feature, age into categorical value (bucketing). Here 8 age groups are created\ntotal_data['AgeCat'] = pd.qcut(total_data['Age'], 8, labels=range(1,9)).astype('int')","2164db37":"# Only train data has survived data so filter only train data from total_data for visualization\nfor_visualize = total_data[:891]\n\n# Create subplot\/placeholder to draw 4 plots\nf, axes = plt.subplots(2, 2)\nf.set_size_inches(20,10)\nsb.violinplot(x='Survived',y='Age', hue='Sex', data=for_visualize, ax=axes[0][0]);\nsb.boxplot(x='Survived',y='Age', data=for_visualize, ax=axes[0][1]);\nsb.countplot(x='Survived', data=for_visualize, hue='AgeCat', ax=axes[1][0]);\nsb.countplot(x='Survived', data=for_visualize, hue='FareCat', ax=axes[1][0]);","ebefe358":"total_data.dtypes","d1a28d83":"# Only required features needs to be encoded\nencoding_features = ['Sex','Title','Embarked']\nfor feature in encoding_features:\n    encode = LabelEncoder()\n    total_data.loc[:,feature] = encode.fit_transform(total_data.loc[:,feature])","4341d0c3":"# Features to be used for prediction\n# features = ['Sex', 'AgeCat', 'Title', 'Alone', 'HasCabin', 'Parch', 'Embarked', 'FamilyMembers', 'Pclass', 'Senior', 'Fare']\nfeatures = ['Sex', 'AgeCat', 'Title', 'Alone', 'HasCabin', 'Parch', 'Embarked', 'FamilyMembers', 'FareCat', 'Senior', 'Pclass']","3e9cb275":"for feature in features:\n    scaling_clf = MinMaxScaler()\n    total_data.loc[:, feature] = scaling_clf.fit_transform(total_data.loc[:,feature].values.reshape(-1,1))","7e4a5831":"train = total_data[:891]\ntest = total_data[891:]\nx_train_data = train.loc[:,features]\ny_train_data = train.loc[:,'Survived']\nx_test_data = test.loc[:,features]","44cecc7c":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsb.heatmap(x_train_data.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True);\n\n# We have better features because features correlations are very less except Pclass and HasCabin (-0.73) but let's keep it","080d8104":"# Instantiate RandomForestClassifier with default values\nclf = RandomForestClassifier()\n\n# Split test and train data for evaluation purpose\n# train_size =0.8 (split train data 80% and test data 20%)\n# random_state = 10 (For splitting data sklearn uses random seed, here let's fix the value so we can reproduce the set of values)\nx_train, x_test, y_train, y_test = train_test_split(x_train_data, y_train_data, train_size=0.80, random_state=10)\n\n# Train the model\nclf.fit(x_train, y_train)","d76baed1":"# # Prediction based on training\npred_y = clf.predict(x_test).astype(int)\n\n# Zip the features, importance\nfeature_importance = dict(zip(x_train_data.columns, clf.feature_importances_))\nprint(feature_importance)\n\n# To know the accuracy score for test data\nprint(\"Accuracy score: \", accuracy_score(y_test, pred_y))\n\n# To know better where the misclassification happend, whether more TN or more FN\nprint(confusion_matrix(y_test, pred_y))\n\n# let's cross validation instead of fixed data set and use different metrics for evalation\nmetrics = ('accuracy','precision','recall')\nscore = cross_validate(clf, x_train_data, y_train_data, cv=20, scoring=metrics, n_jobs=-1)\nprint('Min Accuracy:', score['test_accuracy'].min())\nprint('Average accuracy', score['test_accuracy'].mean())\nprint('Average precision', score['test_precision'].mean())\nprint('Average recall', score['test_recall'].mean())","eeabd7fa":"# Instantiate model\nclf = RandomForestClassifier(random_state=10)\n\n# Parameter for tuning\nparameters = {\n    'n_estimators': range(200,300, 20),\n    'max_features': [6,7,8,],\n    'min_samples_leaf': [6,7,8],\n    'criterion': ['entropy', 'gini']}\n\n# n_jobs = -1, will utilize all the CPU to do search very quickly, instead you can use 1\/2\/3 depending upon number of CPUs available in computer    \ngrid_clf = GridSearchCV(estimator=clf, param_grid=parameters, cv=30, n_jobs=-1)\ngrid_clf.fit(x_train_data, y_train_data)\nprint(grid_clf.best_params_)\nprint(grid_clf.best_score_)","a0ad9ee2":"clf = RandomForestClassifier(criterion='gini', max_features=7, min_samples_leaf=7, n_estimators=280,\n                            random_state=10)\nclf.fit(x_train_data, y_train_data)\nprint(\"Prediction for training data\")\npred_y = clf.predict(x_train_data).astype(int)\nprint(\"Accuracy score: \", accuracy_score(y_train_data, pred_y))\n# let's cross validation instead of fixed data set and use different metrics for evalation\nmetrics = ('accuracy','precision','recall')\nscore = cross_validate(clf, x_train_data, y_train_data, cv=20, scoring=metrics, n_jobs=-1)\nprint('Min Accuracy:', score['test_accuracy'].min())\nprint('Average accuracy', score['test_accuracy'].mean())\nprint('Average precision', score['test_precision'].mean())\nprint('Average recall', score['test_recall'].mean())","1b15552f":"clf.fit(x_train_data, y_train_data)\npred_y = clf.predict(x_test_data).astype(int)\n\n# Generate Submission File \nout_df = pd.DataFrame({ 'PassengerId': test['PassengerId'],\n                            'Survived': pred_y })\nout_df.to_csv(\"output.csv\", index=False)","2065247c":"### Check NaN values in data","b7cd0cc8":"#### Use barcharfor comparison","be99aa08":"#### Violin plot will give more idea about distribution, outliers by comparing with 3rd parameter like sex parameter below","b5d88ca4":"# EDA - Exploratory Data Analysis","90abe87b":"### Understand more about Cabin","18e1b288":"## Read train and test data","ef67718f":"### Convert string\/categories(object type) to numerial value","4266286b":"## It's time to evaluate again with new best parameters found using grid search","d4c58aca":"### Let's remove outliers","824d1872":"### Understand more about age","2841a8e1":"#### Box plot will give useful view to identify outliers and also to get idea about distribution","7f9540a8":"### Understand more about Fare","dcf663d3":"Notes:\n#### .loc of dataframe is best way for indexing column value.\n#### ***Note: Avoid chaining, like data['data'][data['data']>20**]*. Notebook will show warning if you do chaining","f66ab49d":"### Let's evaluate using different metrics and methods\n","89b3d7c3":"# Predict","1e2e471f":"# Preprocessing","6c49118a":"## It's time to tune the hyperparameters of the model to attain more accuracy","d3ed944c":"# Evaluate","96e5a8c6":"## Let's visualize","286cde02":"## It's time to predict final output","bcb6e118":"## Check whether any redudant features are present","a62b0c7c":"### Gridsearch can be used to find best parameter","d816afc9":"> # Tuning\/Hyperparameter selection","798a41ba":"### Scale the value for better prediction\n#### Here we are going to use Randome forest classifier so scaling won't be much help.\n#### Scaling is necessary for algo like ridge, linear, lasso, knn, svm","0ca66aa5":"### Let's create new features.","87af95c7":"#### Preprocessing needs to be done on concatenated data","f5fd7aea":"### It's time to separate train and test data (EDA, pre-processing are done)","bf460bec":"# Model selection"}}