{"cell_type":{"792e6c21":"code","8f32b8b6":"code","761f93fa":"code","f75be188":"code","e8621b2d":"code","922d1012":"code","ff9c974a":"code","86b252e4":"code","6b5258d2":"code","c74fc192":"code","f810103e":"code","b0ae3cb4":"code","b160a00b":"code","52581e41":"code","7c9eb8f3":"code","88208a23":"code","668dcb39":"code","53b53a14":"markdown","917cf661":"markdown","64d54a60":"markdown","dc912231":"markdown","2fbbf04d":"markdown","6d57cbfd":"markdown","baf08c01":"markdown","71e1a307":"markdown","0c1cac29":"markdown","696a8463":"markdown","e4e606d5":"markdown","04cc4f2e":"markdown","89bf6ca1":"markdown","9141ae38":"markdown"},"source":{"792e6c21":"!pip install tslearn","8f32b8b6":"import numpy as np\nimport pandas as pd\n\nfrom scipy import stats\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\n\nfrom tqdm.notebook import tqdm\n\n#Special tools for timeseries:\nfrom tslearn.clustering import KShape\nfrom tslearn.datasets import CachedDatasets\nfrom tslearn.preprocessing import TimeSeriesScalerMeanVariance\n\nimport gc\nimport matplotlib.pylab as plt\n%matplotlib inline","761f93fa":"# Memory reduction helper function:\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns: #columns\n        col_type = df[col].dtypes\n        if col_type in numerics: #numerics\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","f75be188":"# DEFAULT datasets\ndata_pass = '\/kaggle\/input\/m5-forecasting-accuracy\/'\n\n# Sales quantities:\n\n# Calendar to get week number to join sell prices:\ncalendar = pd.read_csv(data_pass+'calendar.csv')\ncalendar.d = calendar.d.str.extract('(\\d+)').astype(np.int16)\ncalendar = reduce_mem_usage(calendar)","e8621b2d":"# GAPS datasets\ngap_columns = ['id', 'item_id','dept_id', 'd', 'sales', 'gap_e']\n\n# Load grid with identified stockouts:\nfile_pass = '..\/input\/m5-out-of-stock-feature-640x-faster\/grid_part_1_gaps.pkl'\ngrid_df = reduce_mem_usage(pd.read_pickle(file_pass)[gap_columns])\n\n# Add week of year:\ngrid_df = pd.merge(\n    grid_df, \n    calendar[['wm_yr_wk','event_name_1','d']], \n    how = 'left', \n    left_on = ['d'], \n    right_on = ['d']\n)\n\ngrid_df.head()\ndel calendar","922d1012":"# To select timeseries features useful for clusterization we will need to split them in two classes.\n# We will use the 'dept_id' to do the splits:\n\ntargets = list(grid_df.dept_id.unique())\ngrid_df.drop(columns=['dept_id'], inplace=True)","ff9c974a":"# I use 100 years as threshold for a stockout. If you feel it is too much, lower it.\nyears_threshold = 100\ngrid_df.loc[:,'stockout'] = (grid_df.gap_e >= years_threshold)*1","86b252e4":"# Find outliers based on z score:\n## Filter out values more than 0 as otherwise all sales might be flagged as outliers:\nmask = grid_df.sales > 0\n\n## Group by 'id' as sales values have diffrenet mean from store to store: \ndf_grouped = grid_df[mask].groupby(['id'])\n\n## Find indices of all outliers:\noutlier_idx = []\nfor group_name, g in tqdm(df_grouped):\n    outlier_idx += list(g[stats.zscore(g.sales) > 3].index) \n    \n## Create separate column with outliers: zeroes by default & sales values for outliers values.\ngrid_df.loc[:,'sale_outlier'] = 0\ngrid_df.loc[outlier_idx,'sale_outlier'] = grid_df.loc[outlier_idx,'sales']","6b5258d2":"# Cap sales outliers with maximum non outlier value:\ngrid_df['sales_clean'] = grid_df['sales']\nnan_idx = grid_df[~grid_df.sales.isna()].index\ngrid_df.loc[outlier_idx,'sales_clean'] = np.nan\n\ngrid_df.loc[nan_idx,'sales_clean'] = grid_df.loc[nan_idx,'sales_clean'].fillna(grid_df.groupby('id',sort=False)['sales_clean'].transform('max'))","c74fc192":"mask = grid_df.stockout>0\ngrid_df.loc[mask,'sales_clean'] = np.nan\n\n# Group by id:\ndf_grouped = grid_df.groupby(\"id\",sort=False)[\"sales_clean\"]\n\n# Generate rolling mean to fill in stockouts:\nlag = 1\nw = 7\ncol_name = f\"rolling_w{str(w)}_l{str(lag)}_mean\"\ngrid_df[col_name] = df_grouped.transform(lambda x: x.shift(lag).rolling(w, min_periods=w).mean()).astype(np.float32)\ngrid_df[col_name] = grid_df[col_name].fillna(method='ffill')\n\n# Fill in stockouts: \ngrid_df.loc[mask, 'sales_clean'] = grid_df.loc[mask, col_name]","f810103e":"# Create week-porduct_id aggregations:\nweek_df = grid_df[['item_id','wm_yr_wk','sales_clean']].groupby(['item_id','wm_yr_wk'], sort=False).mean().fillna(0).astype(np.float32)\nweek_df.reset_index(inplace=True)","b0ae3cb4":"# Note we add 1 so that zeroes map to zeroes and small values map to small values.\nweek_df.loc[:, 'sales_clean'] = np.log(week_df.sales_clean+1)","b160a00b":"# Please, note that this is implementation is one-way there is no way to go back to unscaled version.\n# However, we only use the target for clusterization not prediction here.\nweek_df['sales_clean'] = week_df.groupby('item_id').sales_clean.transform(lambda x: scale(x.astype(float)))\nweek_df.loc[:,'week_number'] = week_df.wm_yr_wk.map(dict(zip(list(week_df.wm_yr_wk),list(range(len(week_df.wm_yr_wk.unique()))))))\nweek_df.drop(columns=['wm_yr_wk'], inplace=True)\nweek_df.head()","52581e41":"del grid_df","7c9eb8f3":"nc = 20\nstart_week = 0\nend_week = 270\n\nseed = 0\nnp.random.seed(seed)\nweek_mask = (week_df.week_number>start_week) & (week_df.week_number <end_week)\nX_train = week_df.loc[week_mask,['item_id','sales_clean','week_number']].pivot(columns='item_id', index='week_number').T.values\n\n# For this method to operate properly, prior scaling is required\nX_train = TimeSeriesScalerMeanVariance().fit_transform(X_train)\nsz = X_train.shape[1]\n\n# kShape clustering\nks = KShape(n_clusters=nc, verbose=True, random_state=seed)\ny_pred = ks.fit_predict(X_train)\n\nplt.figure(figsize=(12,nc*4))\nfor yi in range(nc):\n    plt.subplot(nc, 1, 1 + yi)\n    for xx in X_train[y_pred == yi]:\n        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n    plt.plot(ks.cluster_centers_[yi].ravel(), \"r-\")\n    plt.xlim(0, sz)\n    plt.ylim(-4, 4)\n    plt.title(f\"Cluster {str(yi + 1)} has {str(X_train[y_pred == yi].shape[0])} timeseries\") \n\nplt.tight_layout()\nplt.show()","88208a23":"# Clusters in first 3 years:\n\nnc = 12\nstart_week = 0\nend_week = 160\n\nweek_mask = (week_df.week_number>start_week) & (week_df.week_number <end_week)\nX_train = week_df.loc[week_mask,['item_id','sales_clean','week_number']].pivot(columns='item_id', index='week_number').T.values\n\n# For this method to operate properly, prior scaling is required\nX_train = TimeSeriesScalerMeanVariance().fit_transform(X_train)\nsz = X_train.shape[1]\n\n# kShape clustering\nks = KShape(n_clusters=nc, verbose=True, random_state=seed)\ny_pred = ks.fit_predict(X_train)\n\nplt.figure(figsize=(12,nc*4))\nfor yi in range(nc):\n    plt.subplot(nc, 1, 1 + yi)\n    for xx in X_train[y_pred == yi]:\n        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n    plt.plot(ks.cluster_centers_[yi].ravel(), \"r-\")\n    plt.xlim(0, sz)\n    plt.ylim(-4, 4)\n    plt.title(f\"Cluster {str(yi + 1)} has {str(X_train[y_pred == yi].shape[0])} timeseries\") \n\nplt.tight_layout()\nplt.show()","668dcb39":"# Clusters in last two years:\n\nnc = 12\nstart_week = 162\nend_week = 270\n\nweek_mask = (week_df.week_number>start_week) & (week_df.week_number < end_week)\nX_train = week_df.loc[week_mask,['item_id','sales_clean','week_number']].pivot(columns='item_id', index='week_number').T.values\n\n# For this method to operate properly, prior scaling is required\nX_train = TimeSeriesScalerMeanVariance().fit_transform(X_train)\nsz = X_train.shape[1]\n\n# kShape clustering\nks = KShape(n_clusters=nc, verbose=True, random_state=seed)\ny_pred = ks.fit_predict(X_train)\n\nplt.figure(figsize=(12,nc*4))\nfor yi in range(nc):\n    plt.subplot(nc, 1, 1 + yi)\n    for xx in X_train[y_pred == yi]:\n        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n    plt.plot(ks.cluster_centers_[yi].ravel(), \"r-\")\n    plt.xlim(0, sz)\n    plt.ylim(-4, 4)\n    plt.title(f\"Cluster {str(yi + 1)} has {str(X_train[y_pred == yi].shape[0])} timeseries\") \n\nplt.tight_layout()\nplt.show()","53b53a14":"### ... and then fill in stockouts with last seen sales.","917cf661":"# 1. Preprocessing:\n### Cap 3 sigma outliers:\nThere should be a more elegant way to do this. But this is the fastest I could think of.","64d54a60":"### var explanation:\nFor complete explanation of how we got **gap_e** & **gap_days** variables take a look at [\"M5 - 'out_of_stock' feature 640x faster\"](https:\/\/www.kaggle.com\/sibmike\/m5-out-of-stock-feature-640x-faster). Here is brief explanation of variables:\n* **gap_e** - how many years needed to observe a gap of length 'gap_days' <-- We only use this one\n* **gap_days** - int, number of days in sequence of consecutive zeroes","dc912231":"### Weekly sales & Scaling","2fbbf04d":"### The last two years\nProbably the most exciting part as it focus on patterns during last two years and disregards when time series started.","6d57cbfd":"# M5 Timeseries Clusterization with k-Shape\n\nThis notebook is based on amazing [Tslearn, A Machine Learning Toolkit for Time Series Data](https:\/\/tslearn.readthedocs.io\/en\/stable\/quickstart.html)\nand uses [this](https:\/\/www.kaggle.com\/sibmike\/m5-out-of-stock-feature-640x-faster) notebook to find M5 out-of-stocks. It has just two goals:\n* show some **basic preprocessing**\n* show an **easy way to clusterize timeseries with k-Shape**","baf08c01":"## The plan:\n### 0. Load datasets\n\n### 1. Preprocessing:\n* Cap 3 sigma outliers and then fill in stockouts with the mean sales in preceding week.\n* Aggregate to weekly-product_id\n* Log scale timeseries to further lower the impact of outliers\n* Standardscaler to bring all time series to one scale\n\n### 2. k-Shape Clusterization\n* Find clusters for weeks 0-270\n* Find clusters for weeks 0-150\n* Find clusters for weeks 150-270\n\nWe will not try to use the clusters for training, but you can try it out on your own.","71e1a307":"### That's All Folks!\n\nNow you can see that k-Shape works amazingly well. It is also hard to beat with other clusterization mathods.\nHowever, to get a boost in LB it might take some furhter feature engineering.","0c1cac29":"# 0. Load Datasets\nAll three datasets needed because we need to calculate sales in USD.","696a8463":"# 2. Easy time series clustering with k-Shape\n\nHere is the original paper [k-Shape: Efficient and Accurate Clustering of Time Series by John Paparrizos & Luis Gravano](https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/2723372.2737793), which is another proof of why we need to know kick ass math.","e4e606d5":"## [Shortcut to cluster pictures; skipping data processing](https:\/\/www.kaggle.com\/sibmike\/m5-eazzy-time-series-clusterization-with-k-shape#2.-Easy-time-series-clustering-with-k-Shape)","04cc4f2e":"### The whole history\nFirst we use the whole history and see if k-Shape makes any sense. If you wonder what the red line means - it is the pattern shared by timeseries but which might be shifted left or right for each one of them.","89bf6ca1":"![clusters.png](attachment:clusters.png)","9141ae38":"### Now lets split history in two parts:\n* First, try to clusterize on the first few years, there we will cluster products based on effective start of sales\n* Then clusterize on the last two years, there we will split clusters by yearly patterns"}}