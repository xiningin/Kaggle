{"cell_type":{"dd33630d":"code","02e863d4":"code","4ec59ec4":"code","9c346034":"code","570e00d1":"code","a6f9ddfc":"code","a6a3d72f":"code","a4a1e446":"code","104e2b78":"code","cf262efc":"code","ab1583d1":"code","a8d82f93":"code","c0f19081":"code","7e1d60bd":"code","80d2a3e6":"code","f868fc13":"code","e1550b42":"code","1e3e8b43":"code","d7307bbf":"code","028408df":"code","fe7329e7":"code","8af55f21":"code","95b92bd3":"code","407489ec":"code","98dd2326":"code","21d767e2":"code","120c3360":"code","6240c0b3":"code","6f757462":"code","55ba43e8":"code","d85f1c5e":"code","3db9e375":"code","01276102":"code","d3c92762":"code","9cce7961":"code","756ceae1":"code","e8307bda":"code","83907874":"code","7a55691f":"code","3b1c0765":"code","4435cfd8":"code","7099355c":"code","0e55e309":"code","a12c227a":"code","934e594e":"code","711df2e0":"code","e067b86f":"code","ba71b65f":"code","26cef0f7":"code","7ad40c2f":"code","7e80e676":"code","43e03b8c":"code","c4fa0419":"code","3240637e":"code","e72b281e":"code","7f7da5a3":"code","5cdc3270":"code","22da29b3":"code","cd5788fc":"code","c978e9be":"code","e42976b3":"code","d2ccf69a":"code","159ac997":"code","2a0aee6c":"code","b5395f9e":"code","7681c6da":"code","eb6e31c5":"code","5c9b1034":"code","2b43bab2":"code","fb43d094":"code","19ce1b4c":"code","f1e49786":"code","3dce39dd":"code","cae968dc":"code","5f3648a0":"code","2cec0ead":"code","e4e9626e":"code","8e744a09":"code","90f0e3e0":"code","2f46b32e":"code","7acfec8b":"code","56a817f5":"code","1f16b2fa":"code","c7cceb74":"code","bda1effb":"code","0f03696d":"code","f4136d91":"code","1490bda5":"code","062877a6":"code","a657633b":"code","17283a13":"code","4c86c2d8":"code","b9ee217d":"code","9c481644":"code","a0ae0c62":"code","e4e8ec50":"code","dfa289b1":"code","8bbb5d3b":"code","bcc55ece":"code","48ec042c":"code","3da19fb0":"code","747df320":"code","38696d9f":"code","21929986":"code","08a2d83f":"code","a7cb229a":"code","ed6276e8":"code","e688c035":"code","61032c89":"code","de39746b":"code","3828a6a7":"code","b2f8b15d":"code","c010853d":"code","320ce3f5":"code","aa3e111c":"code","e3a6a954":"code","c0605a22":"code","eab62866":"code","8e1953a5":"code","6fb68d51":"code","39c8d720":"code","c67f5160":"code","069eaf45":"code","586c24a7":"code","b540e877":"code","fd8f9900":"code","7db49960":"code","4aa67757":"code","e0a8f46f":"code","01a32ce7":"code","daaeda94":"code","69a7c005":"code","3287eb66":"code","6a90985f":"code","f8bbfc8d":"code","09d1c4eb":"code","02ec20fd":"code","33de2a05":"code","182f213d":"code","e1e229ba":"code","8c4f7068":"code","5056c55c":"code","e89c21bb":"code","1b86aaf7":"code","87b4202f":"code","a8c9deb1":"code","8c6f275e":"code","3be66dec":"code","ee5fbc0a":"code","d2639ed0":"code","0916dfc0":"code","25387335":"code","efaa4e97":"code","445bc71f":"code","ef7657fc":"code","94b488d0":"code","29744e60":"code","7d9f73ee":"code","f6154a95":"code","bba84928":"code","40c05724":"code","cb4e9199":"code","29487545":"code","73caf303":"code","8081b103":"code","c03eecf8":"code","06cd13d9":"code","6a919d17":"code","b3db242f":"code","588042f1":"code","e6a08bb6":"code","0de6b9bf":"code","12ef6b74":"code","a910b32f":"code","31f477e3":"code","84341348":"code","0643264a":"markdown","2daa98a4":"markdown","1120fced":"markdown","80134e9e":"markdown","9bc2a322":"markdown","35e3e0e7":"markdown","d3fd0907":"markdown","b8e81dca":"markdown","2319303c":"markdown","2ddf5238":"markdown","d0e451f4":"markdown","9403c56a":"markdown","a734210a":"markdown","0e3a06a2":"markdown","9588ed49":"markdown","28beb861":"markdown","aeb40a9a":"markdown","e566a376":"markdown","7bd0c3e9":"markdown","6b36ad66":"markdown","42757ab8":"markdown","a77cdb3c":"markdown","ca3ee4af":"markdown","a09bef8f":"markdown","88228c68":"markdown","4cd7e582":"markdown","239c272a":"markdown","d3fccfcd":"markdown","d11e4ab9":"markdown","3267afc2":"markdown","6bb0f9a0":"markdown","f88bd003":"markdown","52fefe5d":"markdown","205a0ab3":"markdown","cdc27e32":"markdown","2ffc4af3":"markdown","dd3482a5":"markdown","0ea0fc62":"markdown","179948a1":"markdown","2b95ad6d":"markdown","9b92c4df":"markdown","09cc7d36":"markdown","2598a6c0":"markdown","2dd6eab3":"markdown","d45a04d2":"markdown","f54e9953":"markdown","fbec6c85":"markdown","f62a8ef7":"markdown","f5123b98":"markdown","f2762289":"markdown","cc93e2b2":"markdown","c544ae9a":"markdown","0bea3edc":"markdown","d109d304":"markdown","d78f2bee":"markdown","3da47a4b":"markdown","d2a76000":"markdown","45d3339e":"markdown","0509f962":"markdown","d9861460":"markdown","7bdfa808":"markdown","ae185f21":"markdown","c7d64f47":"markdown","39505e49":"markdown","e363e505":"markdown","e5568a88":"markdown","2f23a5a3":"markdown","0dc7375d":"markdown","7bc63882":"markdown","3e27adfe":"markdown","aad716df":"markdown","a381e612":"markdown","1870cdf9":"markdown"},"source":{"dd33630d":"#librerias generales\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n#importante para trabajar\n\n!python -m spacy download es_core_news_md\n!pip install wordcloud\n!pip install pyspellchecker\n\n#librerias especificas\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\n\nimport pandas_profiling as pdp\n\nimport spacy\nfrom spacy.lang.es.stop_words import STOP_WORDS\nimport es_core_news_md\n\nimport re\n\nfrom unicodedata import normalize\n\nimport gc\ngc.enable()\n\nfrom spellchecker import SpellChecker\n\nfrom collections import Counter\n\nfrom wordcloud import WordCloud \n\n#Machine Learning\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV","02e863d4":"train= pd.read_json(\"..\/input\/amazoncorpuesespanol\/dataset_es_train.json\",lines=True)\nprint(\"Shape Train:\",train.shape)\nprint(\"---------------------------------------------\")\ndev=pd.read_json(\"..\/input\/amazoncorpuesespanol\/dataset_es_dev.json\",lines=True)\nprint(\"Shape Dev:\",dev.shape)\nprint(\"---------------------------------------------\")\ntest=pd.read_json(\"..\/input\/amazoncorpuesespanol\/dataset_es_test.json\",lines=True)\nprint(\"Shape Test:\",test.shape)\nprint(\"---------------------------------------------\")","4ec59ec4":"data=pd.concat([train,dev,test])\ndata.head()","9c346034":"data.describe().T","570e00d1":"data.columns","a6f9ddfc":"data.info()","a6a3d72f":"sns.countplot(data=data,x='stars',orient=\"h\")\nplt.title(\"Estrellas y reviews\")\nplt.xlabel(\"estrellas\")\nplt.ylabel(\"reviews\")\nplt.show()\n","a4a1e446":"plt.figure(figsize=(10,10))\ndata.stars.value_counts().plot(kind=\"pie\", autopct='%1.0f%%')\nplt.ylabel(\"Estrellas\")\n\nplt.title(\"Numero de estrellas en porcentajes\")\nplt.show","104e2b78":"plt.figure(figsize=(10,10))\nsns.barplot(x='stars', y='product_category' ,\n            data=data,\n            order = data.product_category.value_counts().index)\n\nplt.ylabel(\"Categorias\")\nplt.xlabel(\"Estrellas\")\nplt.title(\"Promedio de estrellas por producto\")\nplt.show()","cf262efc":"plt.figure(figsize=(10,5))\nax = sns.countplot(data = data, x = \"product_category\",\n                   order = data.product_category.value_counts().index)\nax.set_xticklabels(ax.get_xticklabels(),rotation=40,ha=\"right\")\nplt.ylabel(\"Cantidad\")\nplt.xlabel(\"Categoria\")\nplt.title(\"Cantidad segun categoria de producto\")\nplt.show()","ab1583d1":"plt.figure(figsize=(22,15))\ndata.product_category.value_counts().plot(kind=\"pie\", autopct='%1.0f%%')\nplt.ylabel(\"categorias\")\n\nplt.title(\"Categoria de productos en porcentajes\")\nplt.show()","a8d82f93":"data.language.unique()","c0f19081":"data.isna().sum()","7e1d60bd":"len(data.reviewer_id.unique())","80d2a3e6":"per_user=data[\"reviewer_id\"].value_counts()\nper_user","f868fc13":"user_unique=data[data.reviewer_id == \"reviewer_es_0588051\" ]\nuser_unique","e1550b42":"plt.figure(figsize=(10,10))\nuser_unique.stars.value_counts().plot(kind=\"bar\")\nplt.ylabel(\"estrellas\")\nplt.xlabel(\"cantidad\")\nplt.xticks(rotation=0)\nplt.title(\"Cantidad de estrellas de usuario unico\")\nplt.show()","1e3e8b43":"user_unique.product_category.value_counts().plot(kind=\"bar\")\nplt.ylabel(\"cantidad\")\nplt.xlabel(\"categorias\")\nplt.xticks(rotation=0)\nplt.title(\"Cantidad de categorias de usuario unico\")\nplt.show()","d7307bbf":"list(user_unique['review_body'])[:5]","028408df":"for i in range(data.shape[1]):\n    print(i,len(pd.unique(data.iloc[:,i])))","fe7329e7":"for i in range(data.shape[1]):\n    num=len(pd.unique(data.iloc[:,i]))\n    porcentaje=float(num)\/data.shape[0]*100\n    print(\"%d, %d, %.1f%%\"%(i,num,porcentaje))","8af55f21":"duplicado = data.duplicated()\nprint(duplicado.any())\nprint(data[duplicado])","95b92bd3":"data=data.drop([\"review_id\",\"product_id\",\"reviewer_id\",\"language\"],axis=1)","407489ec":"data.head()","98dd2326":"nlp = es_core_news_md.load() ","21d767e2":"stopwords_spacy = list(STOP_WORDS)\nprint(stopwords_spacy)\nlen(stopwords_spacy)","120c3360":"nlp = es_core_news_md.load() \nnlp.max_length = 2000000\n\n\nstop_words=STOP_WORDS\nstop_words.remove('peor')\nstop_words.remove(\"bueno\")\nstop_words.remove(\"demasiado\")\nstop_words.remove(\"buena\")\nstop_words.remove(\"buen\")\nstop_words.remove(\"buenos\")\nstop_words.remove(\"despacio\")\nstop_words.remove(\"pocos\")\nstop_words.remove(\"poca\")\nstop_words.remove(\"bien\")\nstop_words.remove(\"raras\")\nstop_words.remove(\"ninguno\")\nstop_words.remove(\"ningunas\")\nstop_words.remove(\"no\")\nstop_words.remove(\"si\")\nstop_words.remove(\"s\u00ed\")","6240c0b3":"#revs=[]\n\n#for i in range(data.shape[0]):\n#    review = data.iloc[i].review_title \n#    review = re.sub(\n#        r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n#        normalize( \"NFD\", review), 0, re.I\n#    )\n#    review=normalize( 'NFC', review)\n#    revs.append(review)","6f757462":"#data[\"New_R_title\"]=pd.Series(revs)","55ba43e8":"#revs=[]\n\n#for i in range(data.shape[0]):\n#    review = data.iloc[i].review_body\n#    review = re.sub(\n#        r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n#        normalize( \"NFD\", review), 0, re.I\n#    )\n#    review=normalize( 'NFC', review)\n#    revs.append(review)","d85f1c5e":"#data[\"New_R_body\"]=pd.Series(revs)","3db9e375":"#spell = SpellChecker(language='es',distance=1)","01276102":"#def spell_check(x):\n#    correct_word = []\n#    mispelled_word = x\n#    for word in mispelled_word:\n#        correct_word.append(spell.correction(word))\n#    return ' '.join(correct_word)","d3c92762":"#%%time\n\n#regexp=RegexpTokenizer(r\"\\w+\")\n#tokeandstp=[]\n#data['New_R_title_Lemm'] = ''\n\n#for i in range(data.shape[0]):\n#    review = data.iloc[i].New_R_title\n#    review = regexp.tokenize(review)\n#    review = [word.lower() for word in review if word.lower() not in stop_words]\n#    review = spell_check(review)\n#    review = nlp(review) \n#    review = [word.lemma_ for word in review]\n#    data['New_R_title_Lemm'].iloc[i] = ' '.join(review)","9cce7961":"#data.drop(\"New_R_title\",axis=1,inplace=True)","756ceae1":"#def spell_check(x):\n#    correct_word = []\n#    mispelled_word = x\n#    for word in mispelled_word:\n#        correct_word.append(spell.correction(word))\n#    return ' '.join(correct_word)","e8307bda":"#%%time\n\n#tokeandstp=[]\n#regexp=RegexpTokenizer(r\"\\w+\")\n#data['New_R_body_Lemm'] = ''\n\n#for i in range(data.shape[0]):\n#    review = data.iloc[i].New_R_body\n#    review = regexp.tokenize(review) \n#    review = [word.lower() for word in review if word.lower() not in stop_words]\n#    review = spell_check(review)\n#    review = nlp(review)\n#    review = [word.lemma_ for word in review]\n#    data['New_R_body_Lemm'].iloc[i] = ' '.join(review)","83907874":"#data.drop(\"New_R_body\",axis=1,inplace=True)","7a55691f":"#data.isna().sum()","3b1c0765":"#data = data.dropna(subset=['New_R_title_Lemm',\"New_R_body_Lemm\"])","4435cfd8":"#%%time\n\n#adjetivos = []\n\n#for i in range(data.shape[0]):\n#    review = data.iloc[i].New_R_title_Lemm\n#    review = nlp(review)\n#    review = [word for word in review if word.pos_ == \"ADJ\"]\n#    adjetivos.append(review)\n","7099355c":"#data[\"New_R_title_Lemm_ADJ\"] = pd.Series(adjetivos)","0e55e309":"#%%time\n\n\n#adjetivos = []\n\n#for i in range(data.shape[0]):\n#    review = data.iloc[i].New_R_body_Lemm\n#    review = nlp(review)\n#    review = [word for word in review if word.pos_ == \"ADJ\"]\n#    adjetivos.append(review)","a12c227a":"#data[\"New_R_body_Lemm_ADJ\"] = pd.Series(adjetivos)","934e594e":"##data.to_csv('Amazon-Datas1.0.csv', index = False, encoding = 'utf-8')","711df2e0":"data = pd.read_csv('..\/input\/amazoncorpuesespanol\/Amazon-Datas1.0.csv')","e067b86f":"data.head()","ba71b65f":"dfstar1 = data[data.stars == 1]\ndfstar2 = data[data.stars == 2]\ndfstar3 = data[data.stars == 3]\ndfstar4 = data[data.stars == 4]\ndfstar5 = data[data.stars == 5]","26cef0f7":"dfstar1 = dfstar1.dropna(subset=[\"New_R_title_Lemm_ADJ\"])\ndfstar1['New_R_title_Lemm_ADJ']=dfstar1['New_R_title_Lemm_ADJ'].apply(str)","7ad40c2f":"star1 = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(dfstar1.shape[0]):\n    x = dfstar1.iloc[i].New_R_title_Lemm_ADJ\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    star1.append(x) ","7e80e676":"words_star1=[line for line in star1 for line in set(line)]\nwords_star1 = Counter(words_star1)\nwords_star1 = words_star1.most_common(20)\nwords_star1=pd.DataFrame(words_star1,columns = ['Words', 'Frequency'])\nwords_star1.head(10)","43e03b8c":"dfstar2.head()","c4fa0419":"dfstar2 = dfstar2.dropna(subset=[\"New_R_title_Lemm_ADJ\"])\ndfstar2['New_R_title_Lemm_ADJ']=dfstar2['New_R_title_Lemm_ADJ'].apply(str)","3240637e":"star2 = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(dfstar2.shape[0]):\n    x = dfstar2.iloc[i].New_R_title_Lemm_ADJ\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    star2.append(x) ","e72b281e":"words_star2=[line for line in star2 for line in set(line)]\nwords_star2 = Counter(words_star2)\nwords_star2 = words_star2.most_common(20)\nwords_star2=pd.DataFrame(words_star2,columns = ['Words', 'Frequency'])\nwords_star2.head(10)","7f7da5a3":"dfstar3 = dfstar3.dropna(subset=[\"New_R_title_Lemm_ADJ\"])\ndfstar3['New_R_title_Lemm_ADJ']=dfstar3['New_R_title_Lemm_ADJ'].apply(str)","5cdc3270":"star3 = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(dfstar3.shape[0]):\n    x = dfstar3.iloc[i].New_R_title_Lemm_ADJ\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    star3.append(x) ","22da29b3":"words_star3=[line for line in star3 for line in set(line)]\nwords_star3 = Counter(words_star3)\nwords_star3 = words_star3.most_common(20)\nwords_star3=pd.DataFrame(words_star3,columns = ['Words', 'Frequency'])\nwords_star3.head(10)","cd5788fc":"dfstar4 = dfstar4.dropna(subset=[\"New_R_title_Lemm_ADJ\"])\ndfstar4['New_R_title_Lemm_ADJ']=dfstar4['New_R_title_Lemm_ADJ'].apply(str)","c978e9be":"star4 = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(dfstar4.shape[0]):\n    x = dfstar4.iloc[i].New_R_title_Lemm_ADJ\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    star4.append(x) ","e42976b3":"words_star4=[line for line in star4 for line in set(line)]\nwords_star4 = Counter(words_star4)\nwords_star4 = words_star4.most_common(20)\nwords_star4=pd.DataFrame(words_star4,columns = ['Words', 'Frequency'])\nwords_star4.head(10)","d2ccf69a":"dfstar5 = dfstar5.dropna(subset=[\"New_R_title_Lemm_ADJ\"])\ndfstar5['New_R_title_Lemm_ADJ']=dfstar5['New_R_title_Lemm_ADJ'].apply(str)","159ac997":"star5 = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(dfstar5.shape[0]):\n    x = dfstar5.iloc[i].New_R_title_Lemm_ADJ\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    star5.append(x) ","2a0aee6c":"words_star5=[line for line in star5 for line in set(line)]\nwords_star5 = Counter(words_star5)\nwords_star5 = words_star5.most_common(20)\nwords_star5=pd.DataFrame(words_star5,columns = ['Words', 'Frequency'])\nwords_star5.head(10)","b5395f9e":"plt.figure(figsize=(12,12))\n\nplt.subplot(321)\nestrella1=(\" \").join(words_star1[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800 ).generate(estrella1)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('WorldCloud 1 Star', fontsize=25)\n\nplt.subplot(322)\nestrella2=(\" \").join(words_star2[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(estrella2)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('WorldCloud 2 Star', fontsize=25)\n\nplt.subplot(323)\nestrella3=(\" \").join(words_star3[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(estrella3)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('WorldCloud 3 Star', fontsize=25)\n\nplt.subplot(324)\nestrella4=(\" \").join(words_star4[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(estrella4)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('WorldCloud 4 Star', fontsize=25)\n\nplt.subplot(325)\nestrella5=(\" \").join(words_star5[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(estrella5)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('WorldCloud 5 Star', fontsize=25)\n\nplt.tight_layout()\nplt.show()","7681c6da":"dfstar1 = dfstar1.dropna(subset=[\"New_R_body_Lemm_ADJ\"])\ndfstar1['New_R_body_Lemm_ADJ']=dfstar1['New_R_body_Lemm_ADJ'].apply(str)","eb6e31c5":"star1 = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(dfstar1.shape[0]):\n    x = dfstar1.iloc[i].New_R_body_Lemm_ADJ\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    star1.append(x) ","5c9b1034":"words_star1=[line for line in star1 for line in set(line)]\nwords_star1 = Counter(words_star1)\nwords_star1 = words_star1.most_common(20)\nwords_star1=pd.DataFrame(words_star1,columns = ['Words', 'Frequency'])\nwords_star1.head(10)","2b43bab2":"dfstar2 = dfstar2.dropna(subset=[\"New_R_body_Lemm_ADJ\"])\ndfstar2['New_R_body_Lemm_ADJ']=dfstar2['New_R_body_Lemm_ADJ'].apply(str)","fb43d094":"star2 = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(dfstar2.shape[0]):\n    x = dfstar2.iloc[i].New_R_body_Lemm_ADJ\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    star2.append(x) ","19ce1b4c":"words_star2=[line for line in star2 for line in set(line)]\nwords_star2 = Counter(words_star2)\nwords_star2 = words_star2.most_common(20)\nwords_star2=pd.DataFrame(words_star2,columns = ['Words', 'Frequency'])\nwords_star2.head(10)","f1e49786":"dfstar3 = dfstar3.dropna(subset=[\"New_R_body_Lemm_ADJ\"])\ndfstar3['New_R_body_Lemm_ADJ']=dfstar3['New_R_body_Lemm_ADJ'].apply(str)","3dce39dd":"star3 = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(dfstar3.shape[0]):\n    x = dfstar3.iloc[i].New_R_body_Lemm_ADJ\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    star3.append(x) ","cae968dc":"words_star3=[line for line in star3 for line in set(line)]\nwords_star3 = Counter(words_star3)\nwords_star3 = words_star3.most_common(20)\nwords_star3=pd.DataFrame(words_star3,columns = ['Words', 'Frequency'])\nwords_star3.head(10)","5f3648a0":"dfstar4 = dfstar4.dropna(subset=[\"New_R_body_Lemm_ADJ\"])\ndfstar4['New_R_body_Lemm_ADJ']=dfstar4['New_R_body_Lemm_ADJ'].apply(str)","2cec0ead":"star4 = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(dfstar4.shape[0]):\n    x = dfstar4.iloc[i].New_R_body_Lemm_ADJ\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    star4.append(x) ","e4e9626e":"words_star4=[line for line in star4 for line in set(line)]\nwords_star4 = Counter(words_star4)\nwords_star4 = words_star4.most_common(20)\nwords_star4=pd.DataFrame(words_star4,columns = ['Words', 'Frequency'])\nwords_star4.head(10)","8e744a09":"dfstar5 = dfstar5.dropna(subset=[\"New_R_body_Lemm_ADJ\"])\ndfstar5['New_R_body_Lemm_ADJ']=dfstar5['New_R_body_Lemm_ADJ'].apply(str)","90f0e3e0":"star5 = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(dfstar5.shape[0]):\n    x = dfstar5.iloc[i].New_R_body_Lemm_ADJ\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    star5.append(x) ","2f46b32e":"words_star5=[line for line in star5 for line in set(line)]\nwords_star5 = Counter(words_star5)\nwords_star5 = words_star5.most_common(20)\nwords_star5=pd.DataFrame(words_star5,columns = ['Words', 'Frequency'])\nwords_star5.head(10)","7acfec8b":"plt.figure(figsize=(12,12))\n\nplt.subplot(321)\nestrella1=(\" \").join(words_star1[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800 ).generate(estrella1)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('WorldCloud 1 Star', fontsize=25)\n\nplt.subplot(322)\nestrella2=(\" \").join(words_star2[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(estrella2)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('WorldCloud 2 Star', fontsize=25)\n\nplt.subplot(323)\nestrella3=(\" \").join(words_star3[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(estrella3)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('WorldCloud 3 Star', fontsize=25)\n\nplt.subplot(324)\nestrella4=(\" \").join(words_star4[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(estrella4)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('WorldCloud 4 Star', fontsize=25)\n\nplt.subplot(325)\nestrella5=(\" \").join(words_star5[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(estrella5)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('WorldCloud 5 Star', fontsize=25)\n\nplt.tight_layout()\nplt.show()","56a817f5":"data['contara_title'] = data[\"review_title\"].apply(lambda x: len(str(x).split()))\ndata.head()","1f16b2fa":"data['contara_body'] = data[\"review_body\"].apply(lambda x: len(str(x).split()))\ndata.head()","c7cceb74":"plt.figure(figsize=(16,8))\n\nplt.subplot(121)\nsns.barplot(data = data, x = \"stars\", y = \"contara_title\")\nplt.title('Contara_title en estrellas', fontsize= 25)\nplt.xlabel('stars')\nplt.ylabel('Contara_title')\n\nplt.subplot(122)\nsns.barplot(data = data, x = \"stars\", y = \"contara_body\")\nplt.title('Contara_body en estrellas', fontsize= 25)\nplt.xlabel('stars')\nplt.ylabel('Contara_body')\n\nplt.tight_layout()\nplt.show()","bda1effb":"gc.collect","0f03696d":"data.isna().sum()","f4136d91":"data.dropna(inplace=True)","1490bda5":"data.describe()","062877a6":"data.head()","a657633b":"data_=data.copy()","17283a13":"data_['review_Final'] = data_['New_R_title_Lemm'] + ' ' + data_['New_R_body_Lemm']\ndata_.head()","4c86c2d8":"#%%time\n\n#def spell_check(x):\n#    correct_word = []\n#    mispelled_word = x\n#    for word in mispelled_word:\n#        correct_word.append(spell.correction(word))\n#    return ' '.join(correct_word)\n\n#tokeandstp=[]\n#regexp=RegexpTokenizer(r\"\\w+\")\n#data_['New_R_final_Lemm'] = ''\n\n#for i in range(data_.shape[0]):\n#    review = data_.iloc[i].review_Final\n#    review = regexp.tokenize(review) \n#    review = [word.lower() for word in review if word.lower() not in stop_words]\n#    review = spell_check(review)\n#    review = nlp(review)\n#    review = [word.lemma_ for word in review]\n#    data_['New_R_final_Lemm'].iloc[i] = ' '.join(review)","b9ee217d":"##data_.to_csv('Amazon-Datas2.0.csv', index = False, encoding = 'utf-8')","9c481644":"data_ = pd.read_csv('..\/input\/amazoncorpuesespanol\/Amazon-Datas2.0.csv')","a0ae0c62":"data_['New_R_final_Lemm'] = data_['New_R_final_Lemm'].apply(str)","e4e8ec50":"list_revs = list(data_['New_R_final_Lemm'].values)\nstars_ = data_[\"stars\"].values","dfa289b1":"type(list_revs)","8bbb5d3b":"tfidf = TfidfVectorizer(max_features=1000,lowercase=False)","bcc55ece":"matriz_revs = tfidf.fit_transform(list_revs)\n\n# Tomamos las palabras\nall_words = tfidf.get_feature_names()\n\n# Vizualizamos las 50 palabras mas usadas\nprint(\"50 palabras mas usadas: \",all_words[0:50])","48ec042c":"x = matriz_revs.toarray()\ny = stars_\n\n\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42,stratify=y)","3da19fb0":"def confusion(y_test,y_test_pred,X):\n    names=[\"1 estrella\",\"2 estrellas\", \"3 estrellas\", \"4 estrellas\", \"5 estrellas\"]\n    cm=confusion_matrix(y_test,y_test_pred)\n    f,ax=plt.subplots(figsize=(8,8))\n    sns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\n    plt.title(X, size = 25)\n    plt.xlabel(\"y_pred\")\n    plt.ylabel(\"y_true\")\n    ax.set_xticklabels(names)\n    ax.set_yticklabels(names)\n    plt.show()\n\n    return","747df320":"nb = GaussianNB()\nnb.fit(X_train,y_train)\n\ny_train_pred = nb.predict(X_train)\ny_test_pred = nb.predict(X_test)\n\nprint(\"Accuracy train GaussianNB: \", accuracy_score(y_train, y_train_pred))\nprint(\"Accuracy test GaussianNB: \", accuracy_score(y_test, y_test_pred))","38696d9f":"confusion(y_test,y_test_pred,\"GaussianNB\")","21929986":"print(classification_report(y_test, y_test_pred)) ","08a2d83f":"svc = LinearSVC(C = 1)\nsvc.fit(X_train,y_train)\n\ny_train_pred = svc.predict(X_train)\ny_test_pred = svc.predict(X_test)\n\nprint(\"Accuracy train LinearSVC: \", accuracy_score(y_train, y_train_pred))\nprint(\"Accuracy test LinearSVC: \", accuracy_score(y_test, y_test_pred))","a7cb229a":"confusion(y_test,y_test_pred,\"LinearSVC\")","ed6276e8":"print(classification_report(y_test, y_test_pred)) ","e688c035":"#param_grid = {'C': [0.1,1,3,6] }  \n  \n#grid = GridSearchCV(svc, param_grid, refit = True, verbose = 3,n_jobs=-1) \n\n#grid.fit(X_train, y_train) ","61032c89":"#print(grid.best_estimator_) ","de39746b":"#print(grid.best_params_)","3828a6a7":"svc = LinearSVC(C = 0.1)\nsvc.fit(X_train,y_train)\n\ny_train_pred = svc.predict(X_train)\ny_test_pred = svc.predict(X_test)\n\nprint(\"Accuracy train LinearSVC: \", accuracy_score(y_train, y_train_pred))\nprint(\"Accuracy test LinearSVC: \", accuracy_score(y_test, y_test_pred))","b2f8b15d":"confusion(y_test,y_test_pred,\"LinearSVC OPT\")","c010853d":"print(classification_report(y_test, y_test_pred)) ","320ce3f5":"gc.collect","aa3e111c":"coeff = list(svc.coef_[0])\nlabels = list(all_words)\nimportancia = pd.DataFrame()\nimportancia['words'] = labels\nimportancia['orden'] = coeff\nimportancia = importancia.reset_index(drop=True)\norden_imp = importancia.sort_values(by=['orden'], ascending=False)\norden_imp = orden_imp.reset_index(drop=True)\nplot = pd.concat([orden_imp.head(10), orden_imp.tail(10)])\nplot.sort_values(by=['orden'], ascending=False, inplace = True)\nplot['vpos'] = plot['orden'] > 0\nplot.set_index('words', inplace = True)\nplot.orden.plot(kind='bar', figsize = (14,8),color = plot.vpos.map({True: 'blue', False: 'red'}),fontsize=12,orientation=u'vertical')\nplt.xlabel ('words', fontsize=12)\nplt.xticks(rotation=45)\nplt.ylabel ('orden', rotation = 90, fontsize=12)\nplt.title ('Word importance for star 1 ',fontsize=20)\nplt.show()","e3a6a954":"coeff = list(svc.coef_[1])\nlabels = list(all_words)\nimportancia = pd.DataFrame()\nimportancia['words'] = labels\nimportancia['orden'] = coeff\nimportancia = importancia.reset_index(drop=True)\norden_imp = importancia.sort_values(by=['orden'], ascending=False)\norden_imp = orden_imp.reset_index(drop=True)\nplot = pd.concat([orden_imp.head(10), orden_imp.tail(10)])\nplot.sort_values(by=['orden'], ascending=False, inplace = True)\nplot['vpos'] = plot['orden'] > 0\nplot.set_index('words', inplace = True)\nplot.orden.plot(kind='bar', figsize = (14,8),color = plot.vpos.map({True: 'blue', False: 'red'}),fontsize=12,orientation=u'vertical')\nplt.xlabel ('words', fontsize=12)\nplt.xticks(rotation=45)\nplt.ylabel ('orden', rotation = 90, fontsize=12)\nplt.title ('Word importance for star 2 ',fontsize=20)\nplt.show()","c0605a22":"coeff = list(svc.coef_[2])\nlabels = list(all_words)\nimportancia = pd.DataFrame()\nimportancia['words'] = labels\nimportancia['orden'] = coeff\nimportancia = importancia.reset_index(drop=True)\norden_imp = importancia.sort_values(by=['orden'], ascending=False)\norden_imp = orden_imp.reset_index(drop=True)\nplot = pd.concat([orden_imp.head(10), orden_imp.tail(10)])\nplot.sort_values(by=['orden'], ascending=False, inplace = True)\nplot['vpos'] = plot['orden'] > 0\nplot.set_index('words', inplace = True)\nplot.orden.plot(kind='bar', figsize = (14,8),color = plot.vpos.map({True: 'blue', False: 'red'}),fontsize=12,orientation=u'vertical')\nplt.xlabel ('words', fontsize=12)\nplt.xticks(rotation=45)\nplt.ylabel ('orden', rotation = 90, fontsize=12)\nplt.title ('Word importance for star 3 ',fontsize=20)\nplt.show()","eab62866":"coeff = list(svc.coef_[3])\nlabels = list(all_words)\nimportancia = pd.DataFrame()\nimportancia['words'] = labels\nimportancia['orden'] = coeff\nimportancia = importancia.reset_index(drop=True)\norden_imp = importancia.sort_values(by=['orden'], ascending=False)\norden_imp = orden_imp.reset_index(drop=True)\nplot = pd.concat([orden_imp.head(10), orden_imp.tail(10)])\nplot.sort_values(by=['orden'], ascending=False, inplace = True)\nplot['vpos'] = plot['orden'] > 0\nplot.set_index('words', inplace = True)\nplot.orden.plot(kind='bar', figsize = (14,8),color = plot.vpos.map({True: 'blue', False: 'red'}),fontsize=12,orientation=u'vertical')\nplt.xlabel ('words', fontsize=12)\nplt.xticks(rotation=45)\nplt.ylabel ('orden', rotation = 90, fontsize=12)\nplt.title ('Word importance for star 4 ',fontsize=20)\nplt.show()","8e1953a5":"coeff = list(svc.coef_[4])\nlabels = list(all_words)\nimportancia = pd.DataFrame()\nimportancia['words'] = labels\nimportancia['orden'] = coeff\nimportancia = importancia.reset_index(drop=True)\norden_imp = importancia.sort_values(by=['orden'], ascending=False)\norden_imp = orden_imp.reset_index(drop=True)\nplot = pd.concat([orden_imp.head(10), orden_imp.tail(10)])\nplot.sort_values(by=['orden'], ascending=False, inplace = True)\nplot['vpos'] = plot['orden'] > 0\nplot.set_index('words', inplace = True)\nplot.orden.plot(kind='bar', figsize = (14,8),color = plot.vpos.map({True: 'blue', False: 'red'}),fontsize=12,orientation=u'vertical')\nplt.xlabel ('words', fontsize=12)\nplt.xticks(rotation=45)\nplt.ylabel ('orden', rotation = 90, fontsize=12)\nplt.title ('Word importance for star 5 ',fontsize=20)\nplt.show()","6fb68d51":"##data_.to_csv('Amazon-Datas3.0.csv', index = False, encoding = 'utf-8')","39c8d720":"data_ = pd.read_csv('..\/input\/amazoncorpuesespanol\/Amazon-Datas3.0.csv')","c67f5160":"def confusion(y_test,y_test_pred,X):\n    names=[\"1 estrella\",\"2 estrellas\", \"3 estrellas\", \"4 estrellas\", \"5 estrellas\"]\n    cm=confusion_matrix(y_test,y_test_pred)\n    f,ax=plt.subplots(figsize=(8,8))\n    sns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\n    plt.title(X, size = 25)\n    plt.xlabel(\"y_pred\")\n    plt.ylabel(\"y_true\")\n    ax.set_xticklabels(names)\n    ax.set_yticklabels(names)\n    plt.show()\n\n    return","069eaf45":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers.core import SpatialDropout1D\nimport keras\nfrom keras.layers.convolutional import Conv1D   \nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros","586c24a7":"data_['New_R_final_Lemm']=data_['New_R_final_Lemm'].apply(str)","b540e877":"X = data_['New_R_final_Lemm']\ny = data_[\"stars\"].values","fd8f9900":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)","7db49960":"data_.head()","4aa67757":"data_.shape","e0a8f46f":"max_words = 5000\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_train = tokenizer.sequences_to_matrix(X_train, mode='count')\nX_test = tokenizer.sequences_to_matrix(X_test, mode='count')\n\n\nnum_classes = max(y_train) + 1\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","01a32ce7":"model = Sequential()\nmodel.add(Dense(128, input_shape=(max_words,)))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(64))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))","daaeda94":"model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\nprint(model.metrics_names)","69a7c005":"model.summary()","3287eb66":"batch_size = 128\nepochs = 20","6a90985f":"history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)","f8bbfc8d":"score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","09d1c4eb":"#y_train_pred = model.predict(X_train)\n#y_test_pred = model.predict(X_test)\n\n\n#y_test=np.argmax(y_test, axis=1)\n#y_test_pred=np.argmax(y_test_pred, axis=1)","02ec20fd":"#confusion(y_test,y_test_pred,\"Red simple\")","33de2a05":"#print(classification_report(y_test, y_test_pred)) ","182f213d":"gc.collect","e1e229ba":"X = data_['New_R_final_Lemm']\ny = data_[\"stars\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)\n\nMAX_NB_WORDS = 5000\n\nMAX_SEQUENCE_LENGTH = 250\n\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\nX_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n\n\n\nnum_classes = max(y_train) + 1\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","8c4f7068":"EMBEDDING_DIM=300","5056c55c":"model = Sequential()\nembedding_layer= (Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\nmodel.add(embedding_layer)\nmodel.add(Conv1D(300, 5, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(150, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","e89c21bb":"model.summary()","1b86aaf7":"epochs = 20\nbatch_size = 64\n\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)","87b4202f":"score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","a8c9deb1":"#y_train_pred = model.predict(X_train)\n#y_test_pred = model.predict(X_test)\n\n\n#y_test=np.argmax(y_test, axis=1)\n#y_test_pred=np.argmax(y_test_pred, axis=1)","8c6f275e":"#confusion(y_test,y_test_pred,\"Red Convolucional\")","3be66dec":"#print(classification_report(y_test, y_test_pred)) ","ee5fbc0a":"x = {1 : 0, 2 : 0, 3 : 0, 4 : 1, 5 : 1}\n\ndata_['bin'] = data_['stars'].map(x)\ndata_","d2639ed0":"def confusion(y_test,y_test_pred,X):\n    names=[\"positivo\",\"negativo\"]\n    cm=confusion_matrix(y_test,y_test_pred)\n    f,ax=plt.subplots(figsize=(8,8))\n    sns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\n    plt.title(X, size = 25)\n    plt.xlabel(\"y_pred\")\n    plt.ylabel(\"y_true\")\n    ax.set_xticklabels(names)\n    ax.set_yticklabels(names)\n    plt.show()\n\n    return","0916dfc0":"data_['New_R_final_Lemm'] = data_['New_R_final_Lemm'].apply(str)\n\nlist_revs = list(data_['New_R_final_Lemm'].values)\n\ntfidf = TfidfVectorizer(max_features=1000,lowercase=False)\n\nmatriz_revs = tfidf.fit_transform(list_revs)\n\n# Tomamos las palabras\nall_words = tfidf.get_feature_names()\n\n# Vizualizamos las 50 palabras mas usadas\nprint(\"50 palabras mas usadas: \",all_words[0:50])","25387335":"x = matriz_revs.toarray()\ny = data_[\"bin\"].values\n\n\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42,stratify=y)","efaa4e97":"svc = LinearSVC(C = 1)\nsvc.fit(X_train,y_train)\n\ny_train_pred = svc.predict(X_train)\ny_test_pred = svc.predict(X_test)\n\nprint(\"Accuracy train LinearSVC: \", accuracy_score(y_train, y_train_pred))\nprint(\"Accuracy test LinearSVC: \", accuracy_score(y_test, y_test_pred))","445bc71f":"confusion(y_test,y_test_pred,\"Linerar SVC\")","ef7657fc":"print(classification_report(y_test, y_test_pred)) ","94b488d0":"coeff = list(svc.coef_[0])\nlabels = list(all_words)\nimportancia = pd.DataFrame()\nimportancia['words'] = labels\nimportancia['orden'] = coeff\nimportancia = importancia.reset_index(drop=True)\norden_imp = importancia.sort_values(by=['orden'], ascending=False)\norden_imp = orden_imp.reset_index(drop=True)\nplot = pd.concat([orden_imp.head(10), orden_imp.tail(10)])\nplot.sort_values(by=['orden'], ascending=False, inplace = True)\nplot['vpos'] = plot['orden'] > 0\nplot.set_index('words', inplace = True)\nplot.orden.plot(kind='bar', figsize = (14,8),color = plot.vpos.map({True: 'blue', False: 'red'}),fontsize=12,orientation=u'vertical')\nplt.xlabel ('words', fontsize=12)\nplt.xticks(rotation=45)\nplt.ylabel ('orden', rotation = 90, fontsize=12)\nplt.title ('Word importance for positive\/negative ',fontsize=20)\nplt.show()","29744e60":"gc.collect","7d9f73ee":"X = data_['New_R_final_Lemm']\ny = data_[\"bin\"].values\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)","f6154a95":"max_words = 4000\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_train = tokenizer.sequences_to_matrix(X_train, mode='count')\nX_test = tokenizer.sequences_to_matrix(X_test, mode='count')\n\n\nnum_classes = max(y_train) + 1\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n","bba84928":"model = Sequential()\nmodel.add(Dense(128, input_shape=(max_words,)))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(64))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))","40c05724":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.metrics_names)","cb4e9199":"batch_size = 64\nepochs = 20","29487545":"history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)","73caf303":"score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","8081b103":"#y_train_pred = model.predict(X_train)\n#y_test_pred = model.predict(X_test)\n\n\n#y_test=np.argmax(y_test, axis=1)\n#y_test_pred=np.argmax(y_test_pred, axis=1)","c03eecf8":"#confusion(y_test,y_test_pred,\"Red Simple\")","06cd13d9":"#print(classification_report(y_test, y_test_pred)) ","6a919d17":"X = data_['New_R_final_Lemm']\ny = data_[\"bin\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)\n\nMAX_NB_WORDS = 5000\n\nMAX_SEQUENCE_LENGTH = 250\n\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\nX_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n\n\n\nnum_classes = max(y_train) + 1\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","b3db242f":"EMBEDDING_DIM=300","588042f1":"model = Sequential()\nembedding_layer= (Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\nmodel.add(embedding_layer)\nmodel.add(Conv1D(300, 5, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(150, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])","e6a08bb6":"model.summary()","0de6b9bf":"epochs = 20\nbatch_size = 64\n\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)","12ef6b74":"score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","a910b32f":"#y_train_pred = model.predict(X_train)\n#y_test_pred = model.predict(X_test)\n\n\n#y_test=np.argmax(y_test, axis=1)\n#y_test_pred=np.argmax(y_test_pred, axis=1)","31f477e3":"#confusion(y_test,y_test_pred,\"Red Convolucional\")","84341348":"#print(classification_report(y_test, y_test_pred)) ","0643264a":"### \u00bfHay algo que te gustar\u00eda investigar o probar?\n\n    Voy a utilizar redes neuronales para ver si se puede lograr un mejor resultado. \n    Aclaro que es mi primera vez trabajando con las mismas, por lo que el resultado puede no ser el mejor.","2daa98a4":"# NLP","1120fced":"### Mejorando la Visualizacion.","80134e9e":"Columnas con pocos valores","9bc2a322":"Por Motivos de simplificacion en procesamiento, se procede a unir los datos y posteriormente separarlos para su analisis\n","35e3e0e7":"## La siguiente linea toma alrededor de 30 min","d3fd0907":"### Analisis en usuario individual","b8e81dca":"## Modelado","2319303c":"#### 2 Estrellas","2ddf5238":"#### 3 Estrellas","d0e451f4":"# Proyecto 03 - Procesamiento del Lenguaje Natural\n\n## Dataset: The Multilingual Amazon Reviews Corpus\n\n**Recuerda descargar el dataset de [aqu\u00ed](https:\/\/github.com\/kang205\/SASRec). Es un archivo .zip que contiene tres documentos. M\u00e1s informaci\u00f3n sobre el dataset [aqu\u00ed](https:\/\/registry.opendata.aws\/amazon-reviews-ml\/). Es importante que tengas en cuenta la [licencia](https:\/\/docs.opendata.aws\/amazon-reviews-ml\/license.txt) de este dataset.**\n\n### Exploraci\u00f3n de datos y Procesamiento del Lenguaje Natural\n\nDed\u00edcale un buen tiempo a hacer un An\u00e1lisis Exploratorio de Datos. Considera que hasta que no hayas aplicado las herramientas de Procesamiento del Lenguaje Natural vistas, ser\u00e1 dif\u00edcil completar este an\u00e1lisis. Elige preguntas que creas que puedas responder con este dataset. Por ejemplo, \u00bfqu\u00e9 palabras est\u00e1n asociadas a calificaciones positivas y qu\u00e9 palabras a calificaciones negativas?\n\n### Machine Learning\n\nImplementa un modelo que, dada la cr\u00edtica de un producto, asigne la cantidad de estrellas correspondiente. **Para pensar**: \u00bfes un problema de Clasificaci\u00f3n o de Regresi\u00f3n?\n\n1. Haz todas las transformaciones de datos que consideres necesarias. Justifica.\n1. Eval\u00faa de forma apropiada sus resultados. Justifica la m\u00e9trica elegida.\n1. Elige un modelo benchmark y compara tus resultados con este modelo.\n1. Optimiza los hiperpar\u00e1metros de tu modelo.\n1. Intenta responder la pregunta: \u00bfQu\u00e9 informaci\u00f3n est\u00e1 usando el modelo para predecir?\n\n**Recomendaci\u00f3n:** si no te resulta conveniente trabajar en espa\u00f1ol con NLTK, te recomendamos que explores la librer\u00eda [spaCy](https:\/\/spacy.io\/).\n\n### Para pensar, investigar y, opcionalmente, implementar\n1. \u00bfValdr\u00e1 la pena convertir el problema de Machine Learning en un problema binario? Es decir, asignar \u00fanicamente las etiquetas Positiva y Negativa a cada cr\u00edtica y hacer un modelo que, en lugar de predecir las estrellas, prediga esa etiqueta. Pensar en qu\u00e9 situaci\u00f3n puede ser \u00fatil. \u00bfEsperas que el desempe\u00f1o sea mejor o peor?\n1. \u00bfHay algo que te gustar\u00eda investigar o probar?\n\n### **\u00a1T\u00f3mate tiempo para investigar y leer mucho!**","9403c56a":"### Mejorando la Visualizacion","a734210a":"#### 5 Estrellas","0e3a06a2":"## Redes Neuronales","9588ed49":"#### 3 estrellas","28beb861":"# 1-EDA","aeb40a9a":"# 2-Machine Learning","e566a376":"#### 4 Estrellas","7bd0c3e9":"### NB (Benchmark)","6b36ad66":"## Convolucional","42757ab8":"#### 1 Estrella","a77cdb3c":"### LinearSVC Optimizado con Gridsearch","ca3ee4af":"## Red Neuronal Simple","a09bef8f":"## <font color=red>Review_title y <font color=green>Review_body","88228c68":"### Guardado y Carga de CSV","4cd7e582":"# 2.Eval\u00faa de forma apropiada sus resultados. Justifica la m\u00e9trica elegida.\n\n    Para evaluar los resultados se trabajara con Accuracy debido a que el dataset se encuentra perfectamente balanceado, ya que cada estrella representa un 20% del dataset.\n    \n# 3.Elige un modelo benchmark y compara tus resultados con este modelo.\n    \n    El Benchmark utilizado es GaussianNB debido a que Naive Bayes suele ser uno de los mejores modelos para NLP.","239c272a":"## <font color=red>Review_title","d3fccfcd":"Datos duplicados","d11e4ab9":"# 5.Intenta responder la pregunta: \u00bfQu\u00e9 informaci\u00f3n est\u00e1 usando el modelo para predecir?","3267afc2":"#### Preparacion de Stopwords","6bb0f9a0":"#### A continuacion se presentan las palabras que utiliza el LinearSVC para predecir.","f88bd003":"### Los Estudios indican que las criticas negativas normalmente incluyen mas palabras que las positivas \u00bfSera Verdad?","52fefe5d":"### LinearSVC","205a0ab3":"#### 5 Estrellas","cdc27e32":"### \u00bfValdr\u00e1 la pena convertir el problema de Machine Learning en un problema binario? Es decir, asignar \u00fanicamente las etiquetas Positiva y Negativa a cada cr\u00edtica y hacer un modelo que, en lugar de predecir las estrellas, prediga esa etiqueta. Pensar en qu\u00e9 situaci\u00f3n puede ser \u00fatil. \u00bfEsperas que el desempe\u00f1o sea mejor o peor?\n\n    El resultado logrado con un modelo binario fue extremadamente mejor. Los modelos pudieron mejorar su clasificacion de una forma muy eficiente. Vale destacar que desde el punto de vista del usuario, realizar puntuaciones en positivo-negativo, en lugar de utilizar estrellas facilita la toma de la decision. ","2ffc4af3":"## Trabajar con Adjetivos","dd3482a5":"#### Tokenizacion, StopWords, Spellcheck y Lemmatizacion en:\n## <font color=red>review_title (Aprox 20 Min)","0ea0fc62":"#### Preparacion NLP","179948a1":"#### Los resultados indican que las rese\u00f1as negativas tienen mas palabras que las positivas.","2b95ad6d":"# 4.Optimiza los hiperpar\u00e1metros de tu modelo\n\n    Se utiliza GridSearchCV para la optimizacion del LinerarSVC. Solo se trabajara con C.","9b92c4df":"## Red simple","09cc7d36":"Columnas de un solo valor","2598a6c0":"## <font color=green>Review_Body Adj (aprox 18 min)","2dd6eab3":"data_ = pd.read_csv('Amazon-Datas8.0.csv')","d45a04d2":"### procesamiento de datos","f54e9953":"#### Para la primera prueba solo trabajaremos con lemms","fbec6c85":"## <font color=green>Review_Body","f62a8ef7":"### Las transformaciones que se realizaron en el dataset, fueron las siguientes:\n\n    A) Eliminacion de todas las tildes,salvo la \u00d1. Objetivo: reducir errores ortograficos.\n\n    B) Realizar una tokenizacion eliminando todo aquello que no sean letras.. Objetivo: Eliminacion de signos o caracteres especiales.\n\n    C) Llevar todas las palabras a minuscula. Objetivo: Reducir el trabajo al analizar el texto con palabras que significan lo mismo pero algunas utilizan mayusculas y otras no segun el contexto.\n\n    D) Utilizacion de stopwords. Objetivo: eliminar las palabras con poco valor para el analisis.\n\n    E) Spellcheck. Objetivo: aplicar correccion ortografica en todo el texto.\n\n    F) Spacy. Objetivo: Realizar una tokenizacion.\n\n    G) Lemmatizacion. Objetivo: Trabajar con los lemma de cada palabra.","f5123b98":"#### Limpieza de tildes,salvo la \u00d1 en: \n## <font color=green>review_body","f2762289":"#### Limpieza de NA","cc93e2b2":"### Eliminacion de Columnas sin valor","c544ae9a":"#### Limpieza de tildes,salvo la \u00d1 en:\n## <font color=red>review_title","0bea3edc":"#### Tokenizacion, StopWords, Spellcheck y Lemmatizacion en:\n## <font color=green>review_body (Aprox 30 min)","d109d304":"## Convolucional","d78f2bee":"### LinearSVC","3da47a4b":"## Spellchecker","d2a76000":"# 1.Haz todas las transformaciones de datos que consideres necesarias.","45d3339e":"### Guardado y carga del CSV","0509f962":"## <font color=red>Review_title Adj (aprox 13 min)","d9861460":"data = pd.read_csv('Amazon-Datas7.0.csv')\ndata.head()","7bdfa808":"### Guardado y Carga de CSV","ae185f21":"### Verificacion de valores nulos","c7d64f47":"## <font color=green>Review_body","39505e49":"#### 2 Estrellas","e363e505":"#### 1 Estrella","e5568a88":"#### 4 Estrellas","2f23a5a3":"## Debido al tiempo que toma este tratamiento en los datos, se procedera a:\n    \u2666Realizar Tokenizacion, StopWords, Spellcheck y Lemmatizacion.\n    \u2666Realizar Seleccion de adjetivos\n    \u2666Guardar todo en un nuevo csv.\n    \u2666Cargar el csv.\n#### Objetivo: evitar 1:20 hs de demora total para el tratamiento de datos","0dc7375d":"### Analisis Adjetivos","7bc63882":"### Funcion para ver Confusion","3e27adfe":"## Nota: Muchas casillas estan comentadas debido a que toman mucho tiempo en correr.","aad716df":"#### Debemos eliminar los NA antes de proseguir.","a381e612":"## <font color=red>Review_title","1870cdf9":"#### Train_Test_Split"}}