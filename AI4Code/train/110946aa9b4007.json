{"cell_type":{"a5e4aebc":"code","f466744f":"code","cbee6d5a":"code","fdf4058b":"code","126a5426":"code","39dc6267":"code","788572b2":"code","57577332":"code","cd9e7644":"code","91bdaf39":"code","29508e9d":"code","18c96eab":"code","2be083ad":"code","3c10c672":"code","8c5d9fae":"code","2377df74":"code","269ffcd4":"code","00213cd4":"code","625cce92":"code","f8fdba64":"code","536001af":"code","d8b49b5b":"code","60388597":"code","4b4c90c1":"code","51657253":"code","56b68d75":"code","bf1ed655":"code","004292dc":"code","1bd68579":"code","09151d7a":"code","49d0c5ff":"code","8405d713":"code","00ee56e7":"code","1ed2ee85":"code","f9fdcad5":"code","27b7f197":"code","d160d9f9":"code","0084552d":"code","5b51eaac":"code","97be35a6":"code","d128930e":"code","b55d0ef7":"code","3c5d77f9":"code","dc0978bf":"code","c5f76f37":"code","67f508e3":"code","6a3553b2":"code","abb666e2":"code","1d23a54c":"code","9423fb00":"code","cc5c3776":"code","0c431e0e":"code","a0c87bcd":"code","534985be":"code","47aa9d57":"code","543b42c8":"code","5d699d4f":"code","4165662b":"code","7800cce1":"code","7a47aef2":"code","c3890aee":"code","aba87903":"code","ee3ec83b":"code","03da16f4":"markdown","354765e9":"markdown","c7538257":"markdown","cdf86f3c":"markdown","757bfec5":"markdown","e4c50b24":"markdown","c9f8c790":"markdown","17abca25":"markdown","34fc560e":"markdown","95db3e70":"markdown","393c6454":"markdown","441be3c2":"markdown","7132ac42":"markdown","0e22bb77":"markdown","a77b6431":"markdown","ebc4bbda":"markdown","6fed2527":"markdown","398ef443":"markdown"},"source":{"a5e4aebc":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","f466744f":"def read_train():\n    train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\n    train['text']=train['text'].str.lower().astype(str)\n    train['selected_text']=train['selected_text'].str.lower().astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n    test['text']=test['text'].str.lower().astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","cbee6d5a":"t = train_df[:27000]\nv = train_df[27000:]\n\ntrain_df = t\nvalidation_df = v\n\nvalidation_df.reset_index(inplace=True,drop=True)","fdf4058b":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","126a5426":"MAX_LEN = 100\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","39dc6267":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","788572b2":"ct = validation_df.shape[0]\ninput_ids_v = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_v = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_v = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(validation_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(validation_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[validation_df.loc[k,'sentiment']]\n    input_ids_v[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_v[k,:len(enc.ids)+5] = 1","57577332":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","cd9e7644":"def scheduler(epoch):\n    return 3e-5 * 0.2**epoch","91bdaf39":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(2)(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(2)(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model","29508e9d":"!ls","18c96eab":"n_splits = 4","2be083ad":"jac = []; VER='v1'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n        \n    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n        \n    hist = model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n        epochs=4, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n    print('Loading model...')\n    model.load_weights('.\/v1-roberta-%i.h5'%(fold))\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        start_sorted_indexes = np.argsort(-oof_start[k,], kind='quicksort', order=None)\n        end_sorted_indexes   = np.argsort(-oof_end[k,], kind='quicksort', order=None)\n\n        if start_sorted_indexes[0] <= end_sorted_indexes[0]:\n            a = start_sorted_indexes[0]\n            b = end_sorted_indexes[0]\n            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n            #text1 = revert_clean(text1)\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n            \n        elif start_sorted_indexes[0] <= end_sorted_indexes[1]:\n            a = start_sorted_indexes[0]\n            b = end_sorted_indexes[1]\n            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n            #text1 = revert_clean(text1)\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n\n        elif start_sorted_indexes[1] <= end_sorted_indexes[0]:\n            a = start_sorted_indexes[1]\n            b = end_sorted_indexes[0]\n            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n            #text1 = revert_clean(text1)\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n            \n        elif start_sorted_indexes[1] <= end_sorted_indexes[1]:\n            a = start_sorted_indexes[1]\n            b = end_sorted_indexes[1]\n            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n            #text1 = revert_clean(text1)\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        \n        else:\n            st = train_df.loc[k,'text']\n            #st = revert_clean(st)\n\n        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","3c10c672":"preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(n_splits):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('.\/v1-roberta-%i.h5'%(i))\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/n_splits\n    preds_end += preds[1]\/n_splits","8c5d9fae":"all = []\ncounter = 0\nfor k in range(input_ids_t.shape[0]):\n    start_sorted_indexes = np.argsort(-preds_start[k,], kind='quicksort', order=None)\n    end_sorted_indexes   = np.argsort(-preds_end[k,], kind='quicksort', order=None)\n\n    if start_sorted_indexes[0] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[0] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    else:\n        counter += 1\n        st = test_df.loc[k,'text']\n\n    all.append(st)\n    \nprint(counter, \" row cant predicted on test set\")","2377df74":"test_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)","269ffcd4":"train_preds_start = np.zeros((input_ids.shape[0],MAX_LEN))\ntrain_preds_end = np.zeros((input_ids.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(n_splits):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('.\/v1-roberta-%i.h5'%(i))\n\n    print('Predicting Train...')\n    train_preds = model.predict([input_ids,attention_mask,token_type_ids],verbose=DISPLAY)\n    train_preds_start += train_preds[0]\/n_splits\n    train_preds_end += train_preds[1]\/n_splits","00213cd4":"all = []\ncounter = 0\nfor k in range(input_ids.shape[0]):\n    start_sorted_indexes = np.argsort(-train_preds_start[k,], kind='quicksort', order=None)\n    end_sorted_indexes   = np.argsort(-train_preds_end[k,], kind='quicksort', order=None)\n\n    if start_sorted_indexes[0] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[0] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    else:\n        counter += 1\n        st = train_df.loc[k,'text']\n\n    all.append(st)\n\nprint(counter, \" row cant predicted on train set\")","625cce92":"train_df['predicted_text'] = all","f8fdba64":"validation_preds_start = np.zeros((input_ids_v.shape[0],MAX_LEN))\nvalidation_preds_end = np.zeros((input_ids_v.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(n_splits):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('.\/v1-roberta-%i.h5'%(i))\n\n    print('Predicting Validation...')\n    validation_preds = model.predict([input_ids_v,attention_mask_v,token_type_ids_v],verbose=DISPLAY)\n    validation_preds_start += validation_preds[0]\/n_splits\n    validation_preds_end += validation_preds[1]\/n_splits","536001af":"all = []\ncounter = 0\nfor k in range(input_ids_v.shape[0]):\n    start_sorted_indexes = np.argsort(-validation_preds_start[k,], kind='quicksort', order=None)\n    end_sorted_indexes   = np.argsort(-validation_preds_end[k,], kind='quicksort', order=None)\n\n    if start_sorted_indexes[0] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(validation_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[0] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(validation_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(validation_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(validation_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    else:\n        counter += 1\n        st = validation_df.loc[k,'text']\n\n    all.append(st)\n\nprint(counter, \" row cant predicted on validation set\")","d8b49b5b":"validation_df['predicted_text'] = all","60388597":"cumulativeJaccard = []\n\nfor index, row in validation_df.iterrows():\n    cumulativeJaccard.append(jaccard(row[\"predicted_text\"],row['selected_text']))\n    \nprint(\"Roberta Jaccard on validation = \", np.mean(cumulativeJaccard), \" Expected value is ~0.711\")","4b4c90c1":"train_df","51657253":"validation_df","56b68d75":"test_df","bf1ed655":"text_se = []\nselected_text_se = []\nsentiment_se = []\nposition = []\nfor index, row in train_df.iterrows():\n    text = \" \"+\" \".join(row['text'].split())\n    selected_text = \" \".join(row[\"selected_text\"].split())\n    \n    selected_text_start_word = selected_text.split()[0]\n    selected_text_end_word = selected_text.split()[-1]\n    \n    selected_text_start_char = text.find(selected_text)\n    selected_text_end_char = selected_text_start_char + len(selected_text)\n    \n    # These words are from text not selected_text. So they can be different from selected_text words\n    selected_text_full_start_word = None\n    selected_text_full_end_word = None\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n        \n        if selected_text_start_char < char_counter:\n            # We find start word in text\n            selected_text_full_start_word = word\n            break\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n\n        if selected_text_end_char <= char_counter:\n            # We find start word in text\n            selected_text_full_end_word = word\n            break\n\n    if row[\"sentiment\"] == \"neutral\":\n        position.append(\"Start-Neutral\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End-Neutral\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    else:\n        position.append(\"Start\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    \ntrain_df_se = pd.DataFrame(data={\"text_se\":text_se, \"selected_text_se\":selected_text_se, \"sentiment\":sentiment_se, \"position\":position})\n\ntrain_df_se","004292dc":"# Error Check\nfor index, row in train_df_se.iterrows():\n    if row[\"selected_text_se\"] not in row[\"text_se\"]:\n        print(\"Error at = \" + str(index), row[\"selected_text_se\"], row[\"text_se\"],)","1bd68579":"# Different words\ncounter = 0\nfor index, row in train_df_se.iterrows():\n    if row[\"selected_text_se\"] != row[\"text_se\"]:\n        counter += 1\n        print(\"Sentiment     =  \",row[\"sentiment\"].upper() + \"--\" +row[\"position\"])\n        print(\"Real word     =  \",row[\"text_se\"])\n        print(\"Selected word =  \",row[\"selected_text_se\"])\n        print(\"==========================================\")\nprint(\"Different word number in train set =\", counter)","09151d7a":"text_se = []\nselected_text_se = []\nsentiment_se = []\nposition = []\nfor index, row in validation_df.iterrows():\n    text = \" \"+\" \".join(row['text'].split())\n    selected_text = \" \".join(row[\"predicted_text\"].split())\n    \n    selected_text_start_word = selected_text.split()[0]\n    selected_text_end_word = selected_text.split()[-1]\n    \n    selected_text_start_char = text.find(selected_text)\n    selected_text_end_char = selected_text_start_char + len(selected_text)\n    \n    # These words are from text not selected_text. So they can be different from selected_text words\n    selected_text_full_start_word = None\n    selected_text_full_end_word = None\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n        \n        if selected_text_start_char < char_counter:\n            # We find start word in text\n            selected_text_full_start_word = word\n            break\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n\n        if selected_text_end_char <= char_counter:\n            # We find start word in text\n            selected_text_full_end_word = word\n            break\n\n    if row[\"sentiment\"] == \"neutral\":\n        position.append(\"Start-Neutral\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End-Neutral\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    else:\n        position.append(\"Start\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    \nvalidation_df_se = pd.DataFrame(data={\"text_se\":text_se, \"selected_text_se\":selected_text_se, \"sentiment\":sentiment_se, \"position\":position})\n\nvalidation_df_se","49d0c5ff":"validation_df_se.loc[96:98]","8405d713":"validation_df.loc[48:49]","00ee56e7":"# Error Check\nfor index, row in validation_df_se.iterrows():\n    if row[\"selected_text_se\"] not in row[\"text_se\"]:\n        print(\"Error at = \" + str(index), row[\"selected_text_se\"], row[\"text_se\"],)","1ed2ee85":"text_se = []\nselected_text_se = []\nsentiment_se = []\nposition = []\nfor index, row in test_df.iterrows():\n    text = \" \"+\" \".join(row['text'].split())\n    selected_text = \" \".join(row[\"selected_text\"].split())\n    \n    selected_text_start_word = selected_text.split()[0]\n    selected_text_end_word = selected_text.split()[-1]\n    \n    selected_text_start_char = text.find(selected_text)\n    selected_text_end_char = selected_text_start_char + len(selected_text)\n    \n    # These words are from text not selected_text. So they can be different from selected_text words\n    selected_text_full_start_word = None\n    selected_text_full_end_word = None\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n        \n        if selected_text_start_char < char_counter:\n            # We find start word in text\n            selected_text_full_start_word = word\n            break\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n\n        if selected_text_end_char <= char_counter:\n            # We find start word in text\n            selected_text_full_end_word = word\n            break\n\n    if row[\"sentiment\"] == \"neutral\":\n        position.append(\"Start-Neutral\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End-Neutral\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    else:\n        position.append(\"Start\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    \ntest_df_se = pd.DataFrame(data={\"text_se\":text_se, \"selected_text_se\":selected_text_se, \"sentiment\":sentiment_se, \"position\":position})\n\ntest_df_se","f9fdcad5":"# Error Check\nfor index, row in test_df_se.iterrows():\n    if row[\"selected_text_se\"] not in row[\"text_se\"]:\n        print(\"Error at = \" + str(index), row[\"selected_text_se\"], row[\"text_se\"],)","27b7f197":"# Different words\ncounter = 0\nfor index, row in test_df_se.iterrows():\n    if row[\"selected_text_se\"] != row[\"text_se\"]:\n        counter += 1\n        print(\"Sentiment     =  \",row[\"sentiment\"].upper() + \"--\" +row[\"position\"])\n        print(\"Real word     =  \",row[\"text_se\"])\n        print(\"Selected word =  \",row[\"selected_text_se\"])\n        print(\"==========================================\")\nprint(\"Different word number in test set =\", counter)","d160d9f9":"import re\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport time\nimport spacy \nspacy.prefer_gpu()\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch","0084552d":"def save_model(output_dir, nlp, new_model_name):\n    ''' This Function Saves model to \n    given output directory'''\n    \n    output_dir = f'..\/working\/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","5b51eaac":"def train(train_data, output_dir, n_iter=20, cont=False):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    if cont and os.path.exists(output_dir):\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % output_dir)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        if cont and os.path.exists(output_dir):\n            nlp.resume_training()\n        else:\n            nlp.begin_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                            annotations,  # batch of annotations\n                            drop=0.5,   # dropout - make it harder to memorise data\n                            losses=losses, \n                            )\n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')","97be35a6":"def get_model_out_path(position):\n    '''\n    Returns Model output path\n    '''\n    model_out_path = None\n    if position == 'Start':\n        model_out_path = 'models\/model_Start'\n    elif position == 'End':\n        model_out_path = 'models\/model_End'\n    return model_out_path","d128930e":"def get_training_data(data, position):\n    '''\n    Returns Trainong data in the format needed to train spacy NER\n    '''\n    train_data = []\n    for index, row in data.iterrows():\n        if row.position == position:\n            selected_text = row.selected_text_se\n            text = row.text_se\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data","b55d0ef7":"position = 'Start'\n\ntrain_data = get_training_data(train_df_se, position)\nmodel_path = get_model_out_path(position)\n\ntrain(train_data, model_path, n_iter=3, cont=False)","3c5d77f9":"position = 'End'\n\ntrain_data = get_training_data(train_df_se, position)\nmodel_path = get_model_out_path(position)\n\ntrain(train_data, model_path, n_iter=3, cont=False)","dc0978bf":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    if len(ent_array) > 0:\n        selected_text = text[ent_array[0][0]: ent_array[0][1]]\n    else:\n        selected_text = text\n    return selected_text","c5f76f37":"selected_texts = []\nMODELS_BASE_PATH = '..\/working\/models\/'\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_Start = spacy.load(MODELS_BASE_PATH + 'model_Start')\n    model_End = spacy.load(MODELS_BASE_PATH + 'model_End')\n        \n    for index, row in validation_df_se.iterrows():\n        text = row.text_se            # ----------------------------------------------------------------------------------\n        output_str = \"\"\n        \n        if row.position == 'Start-Neutral' or row.position == 'End-Neutral':\n            selected_texts.append(text)\n        elif row.position == 'Start':\n            selected_texts.append(predict_entities(text, model_Start))\n        elif row.position == 'End':\n            selected_texts.append(predict_entities(text, model_End))\n        \nvalidation_df_se['ner_predicted_text'] = selected_texts","67f508e3":"validation_df_se","6a3553b2":"# Different words\ncounter = 0\nfor index, row in validation_df_se.iterrows():\n    if row[\"selected_text_se\"] != row[\"text_se\"]:\n        counter += 1\nprint(\"Different word count in validation set =\",counter)","abb666e2":"counter = 0\nfor index, row in validation_df_se.iterrows():\n    if row[\"text_se\"] == row[\"selected_text_se\"]:\n        counter+=1\nprint(\"Same word count in validation set =\",counter)","1d23a54c":"counter = 0\nfor index, row in validation_df_se.iterrows():\n    if row[\"ner_predicted_text\"] != row[\"text_se\"]:\n        counter+=1\nprint(\"Changed word by NER in validation set =\",counter)","9423fb00":"counter = 0\nfor index, row in validation_df_se.iterrows():\n    if row[\"ner_predicted_text\"] != row[\"text_se\"]:\n        if row[\"ner_predicted_text\"] != row[\"selected_text_se\"]:\n            counter+=1\nprint(\"Changed word by NER in validation set but unsuccesfull changing=\",counter)","cc5c3776":"predict_entities(\"badly??!?!??!!?\", model_End)","0c431e0e":"for index, row in validation_df_se.iterrows():\n    if row[\"text_se\"] == \"badly??!?!??!!?\":\n        print(row)","a0c87bcd":"for index, row in validation_df_se.iterrows():\n    if row[\"selected_text_se\"] != row[\"text_se\"]:\n        print(row[\"text_se\"], row[\"selected_text_se\"])","534985be":"for index, row in validation_df.iterrows():\n     \n    predicted_words = row[\"predicted_text\"].split()\n    predicted_words[0] = validation_df_se[\"ner_predicted_text\"][2*index]\n    predicted_words[-1] = validation_df_se[\"ner_predicted_text\"][(2*index)+1]\n    \n    validation_df[\"predicted_text\"][index] = \" \".join(predicted_words)","47aa9d57":"validation_df","543b42c8":"totalJaccard = 0\nfor index, row in validation_df.iterrows():\n    predict = row.predicted_text\n    true = row.selected_text\n\n    j = jaccard(predict, true)\n\n    totalJaccard += j\n\nmeanJaccard = totalJaccard \/ len(validation_df['predicted_text'])\nprint(\"NER Validation results : \", meanJaccard)","5d699d4f":"test_df","4165662b":"test_df_se","7800cce1":"test_df","7a47aef2":"selected_texts = []\nMODELS_BASE_PATH = '..\/working\/models\/'\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_Start = spacy.load(MODELS_BASE_PATH + 'model_Start')\n    model_End = spacy.load(MODELS_BASE_PATH + 'model_End')\n        \n    for index, row in test_df_se.iterrows():\n        text = row.text_se\n        output_str = \"\"\n        \n        if row.position == 'Start-Neutral' or row.position == 'End-Neutral':\n            selected_texts.append(text)\n        elif row.position == 'Start':\n            selected_texts.append(predict_entities(text, model_Start))\n        elif row.position == 'End':\n            selected_texts.append(predict_entities(text, model_End))\n        \ntest_df_se['ner_predicted_text'] = selected_texts","c3890aee":"for index, row in test_df.iterrows():\n     \n    predicted_words = row[\"selected_text\"].split()\n    predicted_words[0] = test_df_se[\"ner_predicted_text\"][2*index]\n    predicted_words[-1] = test_df_se[\"ner_predicted_text\"][(2*index)+1]\n    \n    test_df[\"selected_text\"][index] = \" \".join(predicted_words)","aba87903":"test_df","ee3ec83b":"test_df[['textID','selected_text']].to_csv('submission.csv',index=False)","03da16f4":"# Inference","354765e9":"Prepare Test","c7538257":"# Preparing Data For NER","cdf86f3c":"Create validation set","757bfec5":"Predicting test","e4c50b24":"# Data preproccesing","c9f8c790":"# NER","17abca25":"Create train set","34fc560e":"# Load  data and libraries","95db3e70":"Create test set","393c6454":"Prepare Train","441be3c2":"# Train\nWe will skip this stage and load already trained model","7132ac42":"Prepare Validation","0e22bb77":"Different word number in train set 3362, 346 word is **neutral**","a77b6431":"# Model","ebc4bbda":"Only roberta score on validation\n\nExpected value is ~0.711","6fed2527":"Predicting Validation","398ef443":"Predicting Train"}}