{"cell_type":{"80de7bf1":"code","1cbe4ac5":"markdown","06f2778b":"markdown","0c39f5a7":"markdown","bdbcbef5":"markdown","419d39b3":"markdown","ba6cb3ab":"markdown","d09157ec":"markdown","2fbe7d05":"markdown","04032917":"markdown","a40295f8":"markdown"},"source":{"80de7bf1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1cbe4ac5":"#  So what is the KNN algorithm?\n\n* KNN as an algorithm that comes from real life. People tend to be effected by the people around them. Our behaviour is guided by the friends we grew up with. Our parents also shape our personality in some ways. If you grow up with people who love sports, it is higly likely that you will end up loving sports. There are ofcourse exceptions. kNN works similarly.","06f2778b":"![image.png](attachment:image.png)","0c39f5a7":"# A few other features of KNN:\n* KNN stores the entire training dataset which it uses as its representation.\n* KNN does not learn any model.\n* KNN makes predictions just-in-time by calculating the similarity between an input sample and each training instance.","bdbcbef5":"# A Quick Introduction to K-Nearest Neighbors Algorithm","419d39b3":"# Basic unit for learning is based on Distance metrics.\n![image.png](attachment:image.png)","ba6cb3ab":"# KNN is a non-parametric, lazy learning algorithm.\n**When we say a technique is non-parametric , it means that it does not make any assumptions on the underlying data distribution.**\n*** KNN Algorithm is based on feature similarity**","d09157ec":"**The value of a data point is determined by the data points around it.\nIf you have one very close friend and spend most of your time with him\/her, you will end up sharing similar interests and enjoying same things. That is kNN with k=1.\nIf you always hang out with a group of 5, each one in the group has an effect on your behavior and you will end up being the average of 5. That is kNN with k=5.\nkNN classifier determines the class of a data point by majority voting principle. If k is set to 5, the classes of 5 closest points are checked. Prediction is done according to the majority class. Similarly, kNN regression takes the mean value of 5 closest points.**","2fbe7d05":"**pros and cons of KNN**\n# Pros:\n* No assumptions about data \u2014 useful, for example, for nonlinear data\n* Simple algorithm \u2014 to explain and understand\/interpret\n* High accuracy (relatively) \u2014 it is pretty high but not competitive in comparison to better supervised learning models\n* Versatile \u2014 useful for classification or regression\n# Cons:\n* Computationally expensive \u2014 because the algorithm stores all of the training data\n* High memory requirement\n* Stores all (or almost all) of the training data\n* Prediction stage might be slow (with big N)\n* Sensitive to irrelevant features and the scale of the data**","04032917":"![image.png](attachment:image.png)","a40295f8":"# * KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.\n "}}