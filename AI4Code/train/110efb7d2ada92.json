{"cell_type":{"2d23bb6c":"code","2cb6150e":"code","78a1776f":"code","5eea5457":"code","f966b2fa":"code","138b823c":"code","0dbfe505":"code","afe84e64":"code","f3a0882e":"code","ec51a241":"code","7101d321":"code","f02d2e77":"code","2117f1e7":"code","e2d99577":"markdown","aa810caf":"markdown","061ba477":"markdown","e92fa498":"markdown","613437f1":"markdown","f0da4b64":"markdown","15de77df":"markdown","9ac6957c":"markdown","b336a601":"markdown","a28c5d22":"markdown","f60adf39":"markdown","b0bad030":"markdown","3b365e19":"markdown"},"source":{"2d23bb6c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2cb6150e":"df = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","78a1776f":"df.head()","5eea5457":"from collections import Counter\nCounter(\" \".join(df[\"review\"]).lower().split()).most_common(100)","f966b2fa":"#import string as str\n#df['review'] = [i.replace('<br>', '').str.replace('<\/br>', '') for i in df['review']]\ndf['review'] = df['review'].str.replace('<br \/>','')\ndf['review'] = df['review'].str.lower()","138b823c":"plt.figure()\nplt.hist(df['review'].str.split().apply(len).value_counts())\nplt.xlabel('number of words in sentence')\nplt.ylabel('frequency')\nplt.title('Words occurrence frequency')","0dbfe505":"print('The maximum length of a sentence is: ',np.max(df['review'].str.split().apply(len).value_counts()))\nprint('The average lenth of a sentence is: ', np.average(df['review'].str.split().apply(len).value_counts()))","afe84e64":"from sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\ndf['sentiment'] = label_encoder.fit_transform(df['sentiment'])\ndf.head()","f3a0882e":"sentences = np.array(df['review'])\nlabels = np.array(df['sentiment'])","ec51a241":"training_sentences, testing_sentences,training_labels, testing_labels = train_test_split(sentences, labels, test_size = 0.2)","7101d321":"# choose hyper parameters to tune\nvocab_size = 20000 #(before 10000)\nembedding_dim = 150 #(before 16)\nmax_length =  400 #(was 32)\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = \"<OOV>\"\n\n\n# import Tokenizer & fit on training test\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\ntokenizer.fit_on_texts(training_sentences)\n\nword_index = tokenizer.word_index\n\n# convert text to sequences\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen = max_length,\n                                padding = padding_type,\n                                truncating = trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen = max_length,\n                                padding = padding_type,\n                                truncating = trunc_type)\n\n    # modeling\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    \n    # option 1: Flatten\n    #tf.keras.layers.Flatten(),\n    #tf.keras.layers.GlobalAveragePooling1D(),\n    \n    # option 2: LSTM\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.15),\n\n    # option 3: GRU\n    #tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n    \n    # option 4: Conv1D\n    #tf.keras.layers.Conv1D(128,5,activation='relu'),\n    #tf.keras.layers.GlobalAveragePooling1D(),\n    \n    tf.keras.layers.Dense(128, activation = 'relu'),\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel.summary()","f02d2e77":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\n# compile model\nmodel.compile(loss = 'binary_crossentropy',\n            optimizer = Adam(learning_rate=0.001),\n            metrics = ['accuracy'])\n\n# add early stopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\n# learning rate decay\ndef lr_decay(epoch, initial_learningrate = 0.001):#lrv\n    return initial_learningrate * 0.9 ** epoch\n\n# training model\nnum_epochs = 10\nhistory = model.fit(training_padded, training_labels,\n                    epochs=num_epochs,\n                    callbacks=[LearningRateScheduler(lr_decay),\n                              callback],\n                    batch_size = 512,\n                    validation_data = (testing_padded, testing_labels),\n                    verbose=1)","2117f1e7":"def plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel('Epochs')\n    plt.ylabel(string)\n    plt.title(print('vocab_size: ',vocab_size))\n    plt.legend([string, 'val_' + string])\n    plt.show()\n    \nplot_graphs(history, \"accuracy\")\nplot_graphs(history,\"loss\")","e2d99577":"![Natural language processing](https:\/\/landbot.io\/wp-content\/uploads\/2019\/11\/natural-language-processing-chatbot.jpg)","aa810caf":"## **2. Data exploratory analysis**","061ba477":"We could see some abnormal words such as <br \/><br \/>, then we should replace them by a null or space value.","e92fa498":"Split data to train and test for modeling and performance evaluation.","613437f1":"The first step is to load the data to global environment.","f0da4b64":"**What is Natural Language Processing?**\n\nFrom wikipedia, Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n\n**What is Sentiment Classification?**\n\nSentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n\n**What is Tokenizer?**\n\nTokenization is a necessary first step in many natural language processing tasks, such as word counting, parsing, spell checking, corpus generation, and statistical analysis of text.\n\nTokenizer is a compact pure-Python (2 and 3) executable program and module for tokenizing Icelandic text. It converts input text to streams of tokens, where each token is a separate word, punctuation sign, number\/amount, date, e-mail, URL\/URI, etc. It also segments the token stream into sentences, considering corner cases such as abbreviations and dates in the middle of sentences.[Tokenizer](https:\/\/pypi.org\/project\/tokenizer\/)\n\n**What is Padding?**\n\nAs a same approach in Convolution Neural Network, Padding assure the input layer have the same shape for the model. \n\n**What is LSTM (long short term memory)?**\n\nLong short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDS's (intrusion detection systems)","15de77df":"## **3. Modeling**","9ac6957c":"This dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification. This document outlines how the dataset was gathered, and how to use the files provided.\n\n**Dataset**\n\nThe core dataset contains 50,000 reviews. The overall distribution of labels is balanced (25k pos and 25k neg). We also include an additional 50,000 unlabeled documents for unsupervised learning.\n\nIn the entire collection, no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings. Further, the train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels. In the labeled train\/test sets, a negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10. Thus reviews with more neutral ratings are not included in the train\/test sets. In the unsupervised set, reviews of any rating are included and there are an even number of reviews > 5 and <= 5.","b336a601":"![IMDB 50 review datasets](https:\/\/o.aolcdn.com\/images\/dims?quality=85&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D908%252C537%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C947%26image_uri%3Dhttps%253A%252F%252Fs.yimg.com%252Fos%252Fcreatr-uploaded-images%252F2019-08%252F560e5d20-c833-11e9-bf26-36635805fe83%26client%3Da1acac3e1b3290917d92%26signature%3D639a4965c41ca6cec13652498f65cfc97170ea5d&client=amp-blogside-v2&signature=765e155477177a69b93eac5611145d4241be6071)","a28c5d22":"### **2.2 Data pre-processing**","f60adf39":"## **1. Background**","b0bad030":"To visualize the work and choose the proper approach to pre-process data and visualize the text data, here come some task as my defined.\n\n1) Data shape. From this to split our data to train and test data.\n\n2) What is the most common words? Use different approach to visualize the most common words. From this we can find out if any inappropriate words which could be removed.\n\n3) What is the distribution of the sentences length? From this we can choose the proper max_length of sentence.\n\n4) How many total words from our dataset? From this we can choose the vocabulary size for our model.","3b365e19":"### **2.1 Data overview**"}}