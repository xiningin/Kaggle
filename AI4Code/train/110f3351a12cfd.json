{"cell_type":{"b224c715":"code","5140e607":"code","e7d07f2a":"code","22b7dda4":"code","81ea68a1":"code","9180063c":"code","463c8361":"code","e278a88b":"code","2f303ae1":"code","5e0d13db":"code","636ae967":"code","e4cdf2be":"code","cc04f9f6":"code","0b56d887":"markdown","cead0a7b":"markdown","3120349a":"markdown","389cd8ef":"markdown","a1b10302":"markdown","c1d385b0":"markdown","841cfcef":"markdown","10818c35":"markdown","149c969a":"markdown","3907354c":"markdown","f7e22af9":"markdown"},"source":{"b224c715":"import torch\nimport torch.nn as nn \nimport torch.nn.functional as F \nimport torch.optim as optim \nfrom torch.distributions import Categorical\n\nimport pandas as pd\nimport numpy as np\nimport random\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt","5140e607":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nif device == 'cuda': import cudf","e7d07f2a":"path = '\/kaggle\/input\/jane-street-market-prediction\/train.csv'","22b7dda4":"def load_df(path, device):\n    if device == 'cuda':\n        df = cudf.read_csv(path)\n    else:\n        df = pd.read_csv(path)\n        \n    features = [column for column in df.columns if 'feature' in column]\n    \n    return df, features\n\n\n# load data and features\ndf, features = load_df(path, device)","81ea68a1":"def add_actions(df, features, device):\n    f_mean = df[features[1:]].mean()\n    f_std = df[features[1:]].std()\n    \n    df = df.query('weight > 0').reset_index(drop = True)\n    df[features[1:]] = df[features[1:]].fillna(f_mean)\n    df[features[1:]] = (df[features[1:]] - f_mean) \/ f_std\n    df['action'] = (df['resp'] > 0).astype('int')\n    \n    if device == 'cuda': df = df.to_pandas()\n    \n    return df\n\n\n# add the action column\ndf = add_actions(df, features, device)","9180063c":"class Env:\n    def __init__(self, df, features):\n        self.n_samples = df.shape[0]\n        self.weight = torch.FloatTensor(df['weight'].values)\n        self.resp = torch.FloatTensor(df['resp'].values)\n        self.states = torch.FloatTensor(df[features].values)\n        self.observation_space = df[features].shape[1]\n        self.action_space = 2\n        self.idx = 0\n        \n    def reset(self):\n        self.idx = 0\n        return self.states[self.idx].view(1, -1)\n    \n    def step(self, action):\n        reward = self.weight[self.idx] * self.resp[self.idx] * action\n        self.idx += 1\n        if self.idx >= self.n_samples:\n            done = True\n            self.idx = 0\n        else:\n            done = False\n        info = 0\n        return self.states[self.idx].view(1, -1), reward, done, info","463c8361":"class Config:\n    def __init__(self, \n                 epsilon_start = 1.,\n                 epsilon_final = 0.01,\n                 epsilon_decay = 8000,\n                 gamma = 0.99, \n                 lr = 1e-4, \n                 target_net_update_freq = 1000, \n                 memory_size = 100000, \n                 batch_size = 128, \n                 learning_starts = 5000,\n                 max_frames = 10000000): \n\n        self.epsilon_start = epsilon_start\n        self.epsilon_final = epsilon_final\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_by_frame = lambda i: self.epsilon_final + (self.epsilon_start - self.epsilon_final) * np.exp(-1. * i \/ self.epsilon_decay)\n\n        self.gamma =gamma\n        self.lr =lr\n\n        self.target_net_update_freq =target_net_update_freq\n        self.memory_size =memory_size\n        self.batch_size =batch_size\n\n        self.learning_starts = learning_starts\n        self.max_frames = max_frames","e278a88b":"class ExperienceReplay:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n\n    def push(self, transition):\n        self.memory.append(transition)\n        if len(self.memory) > self.capacity:\n            del self.memory[0]\n\n    def sample(self, batch_size):\n        batch = random.sample(self.memory, batch_size)\n        states = []\n        actions = []\n        rewards = []\n        next_states = [] \n        dones = []\n\n        for b in batch: \n            states.append(b[0])\n            actions.append(b[1])\n            rewards.append(b[2])\n            next_states.append(b[3])\n            dones.append(b[4])\n\n        return states, actions, rewards, next_states, dones\n\n    def __len__(self):\n        return len(self.memory)","2f303ae1":"class DuelingNetwork(nn.Module): \n    def __init__(self, obs, ac): \n        super().__init__()\n        self.model = nn.Sequential(nn.utils.weight_norm(nn.Linear(obs, 512)),\n                                   nn.ReLU(), \n                                   nn.utils.weight_norm(nn.Linear(512, 256)),\n                                   nn.ReLU())\n\n        self.value_head = nn.utils.weight_norm(nn.Linear(256, 1))\n        self.adv_head = nn.utils.weight_norm(nn.Linear(256, ac))\n\n    def forward(self, x): \n        out = self.model(x)\n\n        value = self.value_head(out)\n        adv = self.adv_head(out)\n\n        q_val = value + adv - adv.mean(1).reshape(-1,1)\n        return q_val","5e0d13db":"class DuelingDDQN(nn.Module): \n    def __init__(self, obs, ac, config): \n        super().__init__()\n\n        self.q = DuelingNetwork(obs, ac).to(device)\n        self.target = DuelingNetwork(obs, ac).to(device)\n\n        self.target.load_state_dict(self.q.state_dict())\n\n        self.target_net_update_freq = config.target_net_update_freq\n        self.update_counter = 0\n\n    def get_action(self, x):\n        x = torch.FloatTensor(x).to(device)\n        with torch.no_grad(): \n            a = self.q(x).max(1)[1]\n\n        return a.item()\n\n    def update_policy(self, adam, memory, params): \n        b_states, b_actions, b_rewards, b_next_states, b_masks = memory.sample(params.batch_size)\n\n        states = torch.FloatTensor(b_states).to(device)\n        actions = torch.LongTensor(b_actions).reshape(-1,1).to(device)\n        rewards = torch.FloatTensor(b_rewards).reshape(-1,1).to(device)\n        next_states = torch.FloatTensor(b_next_states).to(device)\n        masks = torch.FloatTensor(b_masks).reshape(-1,1).to(device)\n\n        current_q_values = self.q(states).gather(1, actions)\n\n        with torch.no_grad():\n            max_next_q_vals = self.target(next_states).max(1)[0].reshape(-1,1)\n\n        expected_q_vals = rewards + max_next_q_vals * 0.99 * masks\n\n        loss = F.mse_loss(expected_q_vals, current_q_values)\n        \n        adam.zero_grad()\n        loss.backward()\n\n        for p in self.q.parameters(): \n            p.grad.data.clamp_(-1.,1.)\n        adam.step()\n\n        self.update_counter += 1\n        if self.update_counter % self.target_net_update_freq == 0: \n            self.update_counter = 0 \n            self.target.load_state_dict(self.q.state_dict())","636ae967":"env = Env(df, features)       \nconfig = Config(epsilon_start = 1.,\n                epsilon_final = 0.01,\n                epsilon_decay = 8000,\n                gamma = 0.99, \n                lr = 1e-4, \n                target_net_update_freq = 1000, \n                memory_size = env.n_samples \/\/ 100, \n                batch_size = 128, \n                learning_starts = 5000,\n                max_frames = env.n_samples)\nmemory = ExperienceReplay(config.memory_size)\nagent = DuelingDDQN(env.observation_space, env.action_space, config)\nadam = optim.Adam(agent.q.parameters(), lr = config.lr) ","e4cdf2be":"s = env.reset()\nep_reward = 0. \nrecap = []\nrewards = []\n\np_bar = tqdm(total = config.max_frames)\nfor frame in range(config.max_frames):\n\n    epsilon = config.epsilon_by_frame(frame)\n\n    if np.random.random() > epsilon: \n        action = agent.get_action(s)\n    else: \n        action = np.random.randint(0, env.action_space)\n\n    ns, r, done, infos = env.step(action)\n    ep_reward += r \n    if done:\n        ns = env.reset()\n        recap.append(ep_reward)\n        p_bar.set_description('Rew: {:.3f}'.format(ep_reward))\n        ep_reward = 0.\n\n    memory.push((s.reshape(-1).numpy().tolist(), action, r, ns.reshape(-1).numpy().tolist(), 0. if done else 1.))\n    s = ns  \n\n    p_bar.update(1)\n\n    if frame > config.learning_starts:\n        agent.update_policy(adam, memory, config)\n\n    if frame % 1000 == 0:\n        print(f'{frame + 1}\/{config.max_frames}:', ep_reward, end = '\\r')\n        rewards.append(ep_reward.item())\n\np_bar.close()","cc04f9f6":"plt.figure(figsize=(10,10))\nplt.title(\"Rewards per Episode\")\nplt.plot(rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Reward\")","0b56d887":"### Dueling Double Deep Q-Learning\nby Alin Cijov","cead0a7b":"![jane-street.png](attachment:jane-street.png)","3120349a":"# Analyze","389cd8ef":"# Dueling DDQN","a1b10302":"# Configurations","c1d385b0":"# Training","841cfcef":"# Prepare Data","10818c35":"# Credits\n\n[Dueling DDQN Github Implementation](https:\/\/github.com\/MoMe36\/DuelingDDQN)","149c969a":"# Dueling Network ","3907354c":"# Environment","f7e22af9":"# Experience Replay"}}