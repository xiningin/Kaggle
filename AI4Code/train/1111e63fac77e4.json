{"cell_type":{"de7123f8":"code","71057708":"code","f2e4d607":"code","982ee98d":"code","0f19c959":"code","27f65b7f":"code","c6857df3":"code","b16d4e50":"code","507322bb":"code","2d7a3f20":"code","255d60d2":"code","fafd3d35":"code","f763c95e":"code","3d013a2f":"code","1b69cbd3":"code","c2dfbe35":"code","3866a28e":"code","75d31775":"code","4a445fe8":"code","6c93d692":"code","647746fb":"markdown","aa811503":"markdown","d39b57ed":"markdown","4fe170fc":"markdown","0301e845":"markdown","da7383ed":"markdown","e206c1ea":"markdown","9e8ea1be":"markdown","60ac2109":"markdown","0f6d6530":"markdown","291f3361":"markdown"},"source":{"de7123f8":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re\nfrom pprint import pprint\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline","71057708":"#Read the .csv file using Pandas. Take a look at the top few records.\nReviewData = pd.read_csv('K8 Reviews v0.2.csv')\nReviewData.head()","f2e4d607":"def Normalize(reviews):\n    NormalizeReviews = []\n    for review in reviews:\n        NormalizeReviews.append(review.lower())\n    return NormalizeReviews","982ee98d":"#Normalize casings for the review text and extract the text into a list for easier manipulation.\nNormalizeReviewText = Normalize(ReviewData['review'].values)\nNormalizeReviewText","0f19c959":"def Tokenize_POS(reviews):\n    TokenizeReviews = []\n    for review in reviews:\n        #review = nltk.word_tokenize(review)\n        #TokenizeReviews.append(nltk.pos_tag(review))  \n        for word,pos in nltk.pos_tag(nltk.word_tokenize(review)):\n            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n                #review = lemmatizer.lemmatize(word)\n                #print (word)\n                TokenizeReviews.append(review)    \n    return TokenizeReviews    ","27f65b7f":"#Tokenize the reviews using NLTKs word_tokenize function.\n#Perform parts-of-speech tagging on each sentence using the NLTK POS tagger.\nTokenizeReviews = Tokenize_POS(NormalizeReviewText)\nTokenizeReviews","c6857df3":"# function to remove Stopwords\ndef Remove_Stopwords(word_list, lang='english'):\n    \"\"\"Function removes english stopwords\n    Args:\n        word_list  : list of words\n    Return:\n        The return value. List of words\n    \"\"\"\n    content = []\n    stopwords_list = stopwords.words(lang)\n    #print(type(word_list))\n    #for word in word_list:\n    #    print(word)\n    #    if word.lower() not in stopwords_list:\n    #        content.append(word)\n    content = [w for w in word_list if w.lower() not in stopwords_list]\n    #print(content)\n    return content\n            ","b16d4e50":"# function to remove punctuation\ndef Simplify_Punctuation(text):\n    \"\"\"\n    This function simplifies doubled or more complex punctuation. The exception is '...'.\n    \"\"\"\n    corrected = str(text)\n    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n    return corrected","507322bb":"# function to lemmatize using WordNetLemmatizer\ndef Lemmatize_WordNet(words_list):\n    wnl = WordNetLemmatizer()\n    encoded_list = []\n    for word in words_list:\n        encoded_list.append(wnl.lemmatize(word, pos=\"v\"))#.encode(\"utf8\"))\n    #print(encoded_list)\n    return encoded_list","2d7a3f20":"def tokenize(txt):\n    \"\"\"Function computes Tokenizes into sentences, strips punctuation\/abbr, \n       converts to lowercase and tokenizes words\n    Args:\n        txt  : text documents\n    Return:\n        The return value. Tokenized words\n    \"\"\"\n    return [word_tokenize(\" \".join(re.findall(r'\\w+', t,flags = re.UNICODE )).lower()) \n                for t in sent_tokenize(txt.replace(\"'\", \"\"))]","255d60d2":"def Apply_Stopwords_punctuation_lemmatize(reviews):\n    PreprocessReviews = []\n    for review in reviews:\n        lemmetized = []\n        review = Simplify_Punctuation(review)  # Remove Punctuation        \n        sentences = tokenize(review)\n        for sentence in sentences:\n            words = Remove_Stopwords(sentence)         # Remove Stopwords\n            words = Lemmatize_WordNet(words)           # lemmatize \n            # lets's skip short sentences with less than 3 words\n            if len(words) < 3:\n                continue\n            lemmetized.append(\" \".join(words))\n        PreprocessReviews.append(\" \".join(lemmetized))\n    return PreprocessReviews","fafd3d35":"PreProcessReviews = Apply_Stopwords_punctuation_lemmatize(TokenizeReviews)\nPreProcessReviews","f763c95e":"TokenizeReviews = []\nfor review in PreProcessReviews:\n    TokenizeReviews.append(nltk.word_tokenize(review)) \n#TokenizeReviews","3d013a2f":"# Create Dictionary\n\nid2word = corpora.Dictionary(TokenizeReviews)\n\n# Create Corpus\ntexts = TokenizeReviews\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])\nprint(id2word[0])\n\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n","1b69cbd3":"# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=12, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","c2dfbe35":"# Print the Keyword in the 12 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","3866a28e":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=TokenizeReviews, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","75d31775":"# Build LDA model with 8 topics\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=10, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","4a445fe8":"# Print the Keyword in the 8 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","6c93d692":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=TokenizeReviews, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","647746fb":"# Importing Libraries","aa811503":"Lemmatize. \nDifferent forms of the terms need to be treated as one.\nNo need to provide POS tag to lemmatizer for now.\nRemove stopwords and punctuation (if there are any). ","d39b57ed":"# 4. Perform parts-of-speech tagging on each sentence using the NLTK POS tagger.\n\n# 5. For the topic model, we should  want to include only nouns.\n\n    Find out all the POS tags that correspond to nouns.\n\n    Limit the data to only terms with these tags.","4fe170fc":"# The business should be able to interpret the topics.\n\n    Name each of the identified topics.\n\n    Create a table with the topic name and the top 10 terms in each to present to the business.\n","0301e845":"# 8. Create a topic model using LDA on the cleaned-up data with 12 topics.\n\n    Print out the top terms for each topic.\n    What is the coherence of the model with the c_v metric?","da7383ed":"# 10 Create a topic model using LDA with what you think is the optimal number of topics\n\n    What is the coherence of the model?","e206c1ea":"# 1. Read the .csv file using Pandas. Take a look at the top few records","9e8ea1be":"Here are possible topics and and top words for each topic \n\n(Topic 1: General Review, \n\n  Words: \"heat\" , \"product\" , \"update\" , \"days\" , 1\" , \"play\" , \"software\" , \"need\" , \"user\" , \"ok\"\n  ),\n  \n (Topic 2: Review on Lenovo Note K8,\n \n  Words: \"lenovo\" , \"note\" , \"k8\" , \"first\" , \"u\" , \"previous\" , \"mobiles\" , \"still\" , \"face\" , \"office\"\n  ),\n  \n (\n  Topic 3: Review on Charging time ,\n  \n  Words: \"work\" , \"use\" , \"charge\" , \"get\" , \"take\" , \"4\" , \"2\" , \"5\" , \"like\" , \"charger\"\n  ),\n  \n (\n  Topic 4: Review on Sensor time,\n  \n  Words: \"time\" , \"bite\" , \"sensor\" , \"back\" , \"android\" , \"image\" , \"dedicate\" , \"stock\" , \"lot\" , \"music\" \n  ),\n  \n (\n  Topic 5: Negative Review,\n  \n  Words: \"phone\" , \"buy\" , \"dont\" , \"better\" , \"get\" , \"compare\" , \"one\" , \"worst\" , \"last\" , \"service\"\n  ),\n  \n (\n  Topic 6: Review on redmi ,\n  \n  Words: \"poor\" , \"dual\" , \"much\" , \"make\" , \"life\" , \"8\" , \"purchase\" , \"provide\" , \"redmi\" , \"two\"\n  ),\n  \n (\n  Topic 7: Review on camera,\n  \n  Words: \"good\" , \"camera\" , \"quality\" , \"issue\" , \"game\" , \"also\" , \"clarity\" , \"average\" , \"screen\" , \"light\"\n  ),\n  \n (\n  Topic 8: Review on network,\n  \n  Words: \"doesnt\" , \"call\" , \"even\" , \"bad\" , \"network\" , \"many\" , \"cant\" , \"support\" , \"full\" , \"find\"\n  ),\n \n (\n  Topic 9: Review on battery life,\n  \n  Words: \"battery\" , \"feature\" , \"mode\" , \"fast\" , \"drain\" , \"great\" , \"speed\" , \"nice\" , \"device\" , \"really\"\n  ),\n  \n (\n   Topic 10: Review on price, \n   \n   Words: \"mobile\" , \"amazon\" , \"problem\" , \"price\" , \"awesome\" , \"hai\" , \"return\" , \"properly\" , \"best\" , \"hang\"\n  )","60ac2109":"# 6. Lemmatize. \n\n    Different forms of the terms need to be treated as one.\n    No need to provide POS tag to lemmatizer for now.\n\n# 7. Remove stopwords and punctuation (if there are any). ","0f6d6530":"# Analyze the topics through the business lens.\n\n\nHere are the possible topic headers\n\n  0 - Possible Topic - Lenovo Note K8 (1)\n  \n  1 - Possible Topic - First Touch Phone (2)\n  \n  2 - Possible Topic - Charging Review (3)\n  \n  3 - Possible Topic - Review on sensor time (4)\n  \n  4 - Possible Topic - Positive Mobile Review (5) \n  \n  5 - Possible Topic - Picture quality (6)\n  \n  6 - Possible Topic - Positive Review (5)\n  \n  7 - Possible Topic - Review on Processor (7)\n  \n  8 - Possible Topic - Positive Review (5)\n  \n  9 - Possible Topic - Negative Review (8)\n  \n  10 - Possible Topic - Review on Return policy (9)\n  \n  11 - Possible Topic - Review on software update (10)\n  \n  # Determine which of the topics can be combined.\n\n  \n  Distinct topics can be treated as 10","291f3361":"# 2. Normalize casings for the review text and extract the text into a list for easier manipulation."}}