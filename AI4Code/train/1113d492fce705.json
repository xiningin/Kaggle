{"cell_type":{"23b8dd9f":"code","88bb0f1c":"code","836bb220":"code","a08e6eb0":"code","607b22cf":"code","6921e8f6":"code","7d419b36":"code","a0ebec43":"code","66dd49df":"code","42002913":"code","49d8ebfe":"code","bddc1a35":"code","99745c42":"code","38a38b2a":"code","b4d16436":"markdown","05be09fe":"markdown","2c4e8a8f":"markdown","bb582766":"markdown","be02ce29":"markdown","99e2e1d3":"markdown","4984f8b1":"markdown","c3f04ff8":"markdown","04fb8398":"markdown"},"source":{"23b8dd9f":"import json\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nfrom time import time","88bb0f1c":"%%time\nN_ROWS = int(1e6)                     # number of rows = 11M\ntrain = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/train.csv\", nrows=N_ROWS)\ntest = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/test.csv\", nrows=N_ROWS)","836bb220":"start_mem_usg = train.memory_usage().sum() \/ 1024 ** 2 \nprint(\"Memory usage of the train is : {:.1f} MB for now\".format(start_mem_usg))\nstart_mem_usg = test.memory_usage().sum() \/ 1024 ** 2 \nprint(\"Memory usage of the test is : {:.1f} MB for now\".format(start_mem_usg))","a08e6eb0":"train.head(5)","607b22cf":"df_to_minify = [train, test]\nfor df in df_to_minify:\n    df['event_data'] = df['event_data'].apply(lambda x: json.loads(x))","6921e8f6":"event_data = train['event_data'].tolist()\nunique_keys = list()\nfor my_json in event_data:\n    unique_keys += my_json.keys()\n    \nunique_keys = list(set(unique_keys))\nprint('event_data contains {} new columns'.format(len(unique_keys)))\nprint('Some new columns are:', unique_keys[:5])","7d419b36":"for ky in tqdm(unique_keys):\n    def give_me_keys(x):\n        try:\n            return x[ky]\n        except KeyError:\n            return np.nan\n    train[ky] = train['event_data'].apply(give_me_keys)\n    test[ky] = test['event_data'].apply(give_me_keys)\n    \n    \nprint('Train shape is:', train.shape)\nprint('Test shape is:', test.shape)\nstart_mem_usg = train.memory_usage().sum() \/ 1024 ** 2 \nprint(\"Memory usage of the train dataframe is : {:.1f} MB for now\".format(start_mem_usg))","a0ebec43":"# Use this filters if you want to drop columns with low variance or lot of nans\nVAR_FILTER = True\nNAN_FILTER = True\n\nVAR_THRESH = .1\nNAN_THRESH = .99\n\ncols_to_drop = list()\n\nif VAR_FILTER:\n    var_dict = train.std() <= VAR_THRESH\n    cols_to_drop += [k for k, v in var_dict.items() if v]\n\nif NAN_FILTER:\n    nan_dict = train.isna().mean() >= NAN_THRESH\n    cols_to_drop += [k for k, v in nan_dict.items() if v]\n\ncols_to_drop = list(set(cols_to_drop))\ntrain.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop(cols_to_drop, axis=1, inplace=True)\n\nprint('We dropped {} columns'.format(len(cols_to_drop)))\nprint('Train shape is: ', train.shape)\nprint('Test shape is: ', test.shape)","66dd49df":"# Now that we've the information contained in event_data, we can drop it\ntry:\n    train.drop('event_data', axis=1, inplace=True)\n    test.drop('event_data', axis=1, inplace=True)\nexcept:\n    pass\n\ntrain.head()","42002913":"col_to_label_encode = list()\nfor col in train.columns:\n    try:\n        if len(train[col].unique()) < 10:\n            col_to_label_encode.append(col)\n    except:\n        pass","49d8ebfe":"correspondance_dict = dict()\n\nfor col in col_to_label_encode:\n    try:\n        le = LabelEncoder()\n        train[col] = le.fit_transform(train[col])\n        test[col] = le.transform(test[col])\n\n        keys = le.classes_\n        values = le.transform(le.classes_)\n        dictionary = dict(zip(keys, values))\n        correspondance_dict[col] = dictionary\n\n    except:    # the variable is not label encodable\n        pass\n\ncorrespondance_dict","bddc1a35":"def reduce_mem_usage(props, log=False):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\", round(start_mem_usg, 2), \" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings and timestamps\n            \n            # Print current column type\n            if log: print(\"******************************\")\n            if log: print(\"Column: \",col)\n            if log: print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)            \n\n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n            if log: print(\"dtype after: \",props[col].dtype)\n            if log: print(\"******************************\")\n    \n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is now: \", round(mem_usg, 2), \" MB\")\n    print(\"This is \",round(100 * mem_usg \/ start_mem_usg, 2),\"% of the initial size\")\n    return props","99745c42":"train = reduce_mem_usage(train, log=False)\ntest = reduce_mem_usage(test, log=False)","38a38b2a":"train.to_pickle('train.pkl')\ntest.to_pickle('test.pkl')","b4d16436":"## Exploring event_data column","05be09fe":"## Now we will Label Encode some variable to stock them as small int (instead of objects)","2c4e8a8f":"#### Imports","bb582766":"#### Loading","be02ce29":"### Remove columns which variance is very low or with too many missing values","99e2e1d3":"# Dataset Minification","4984f8b1":"### `event_data` seems interesting. I think it is the main source of information.\n### The data is given in json format, so we'll parse it to be able to create columns","c3f04ff8":"### The goal of this notebook is to offer a first preprocessing step so that you can manipulate this huuuuuuge dataset easily. The final dataframe is saved as a .pkl, which allows you to load it quickly!","04fb8398":"Please modify these two thresholds to fit your needs"}}