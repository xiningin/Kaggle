{"cell_type":{"f8807020":"code","1265913e":"code","9f6b6c51":"code","c0ce4cd7":"code","043e0d04":"code","b0e7b3e5":"code","5e0d6c8c":"markdown","c346ea79":"markdown","20d0f0ce":"markdown","b302e774":"markdown"},"source":{"f8807020":"import torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.utils.data as Data\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np\nimport imageio","1265913e":"#reproducible\ntorch.manual_seed(1)\n\n# x data (tensor), shape=(100, 1)\nx = torch.unsqueeze(torch.linspace(-1,1,100),dim=1)\nprint(type(x))\n\n # noisy y data (tensor), shape=(100, 1)\ny = x.pow(2) + 0.2*torch.rand(x.size())\n\n#torch can only train on Variable, so convert them to Variable\nx, y = Variable(x),Variable(y)\n\n","9f6b6c51":"#view data\nplt.figure(figsize=(10,4))\nplt.scatter(x.data.numpy(), y.data.numpy(),color=\"orange\")\nplt.title('Regression Analysis')\nplt.xlabel('Independent Variable')\nplt.ylabel('Dependent Variable')\nplt.show()","c0ce4cd7":"class Net(torch.nn.Module):\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()\n        #hidden layer\n        self.hidden = torch.nn.Linear(n_feature,n_hidden)\n        #output layer\n        self.predict = torch.nn.Linear(n_hidden, n_output)\n    def forward(self, x):\n        #activation function for hidden layer\n        x = F.relu(self.hidden(x))\n        #linear output\n        x = self.predict(x)\n        return(x)","043e0d04":"#define the network\nnet = Net(n_feature=1, n_hidden=10, n_output=1)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n#Regression Mean Squared loss\nloss_func = torch.nn.MSELoss()","b0e7b3e5":"my_images = []\nfig, ax = plt.subplots(figsize=(12,7))\n\n#train the network\nfor t in range(200):\n    #input x and prediction based on x\n    prediction = net(x)\n    \n    loss = loss_func(prediction, y)\n    \n    #clear gradients for next train\n    optimizer.zero_grad()\n    \n    #backpropagation, compute gradients\n    loss.backward()\n    \n    #apply gradient\n    optimizer.step()\n    \n    #plot and show learning process\n    plt.cla()\n    \n    ax.set_title('Regression Analysis',fontsize=35)\n    ax.set_xlabel('Independent variable',fontsize=24)\n    ax.set_ylabel('Dependent variable',fontsize=24)\n    \n    ax.set_xlim(-1.05,1.5)\n    ax.set_ylim(-0.25, 1.25)\n    ax.scatter(x.data.numpy(), y.data.numpy(), color=\"orange\")\n    ax.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n    ax.text(1.0, 0.1, 'Step = %d' % t, fontdict={'size': 24, 'color':  'red'})\n    ax.text(1.0, 0, 'Loss = %.4f' % loss.data.numpy(),fontdict={'size': 24, 'color':  'red'})\n    \n    #draw the canvas, cache the renderer\n    fig.canvas.draw()\n    image = np.frombuffer(fig.canvas.tostring_rgb(),dtype='uint8')\n    image = image.reshape(fig.canvas.get_width_height()[::-1]+(3,))\n    \n    my_images.append(image)\n\n#save images as a gif\nimageio.mimsave('.\/curve_1.gif', my_images, fps=10)\n    ","5e0d6c8c":"torch.squeeze(input, dim=None, out=None) \u2192 Tensor\n\nReturns a tensor with all the dimensions of input of size 1 removed.\nFor example, if input is of shape: (A\u00d71\u00d7B\u00d7C\u00d71\u00d7D) then the out tensor will be of shape: (A\u00d7B\u00d7C\u00d7D).\nWhen dim is given, a squeeze operation is done only in the given dimension. If input is of shape: (A\u00d71\u00d7B), squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A\u00d7B)","c346ea79":"matplotlib.pyplot.cla ----> Clear the current axes.\n","20d0f0ce":"Define Network","b302e774":"Import Library"}}