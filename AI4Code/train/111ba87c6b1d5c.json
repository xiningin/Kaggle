{"cell_type":{"91c9442d":"code","554ef7cb":"code","4a616d58":"code","4274d0e7":"code","94e1c455":"code","0ecf4ba4":"code","66ffd450":"code","2ec659a5":"code","6ee2f695":"code","d18b265d":"code","47d1d8db":"code","603b1089":"code","1971ec9f":"code","1db76f07":"code","7b6e7842":"code","99bc522e":"code","941b8dea":"code","093c38f0":"code","9885414e":"code","241eb7b9":"code","eb3d2109":"code","39db0266":"code","74e1066e":"code","e599f051":"code","c09e8de4":"code","104e4686":"code","d0dd043a":"code","cdb295ac":"code","f2e7fbf6":"code","c149b97e":"code","4dc33e15":"code","12ec43d7":"markdown","c08c1cc7":"markdown","1f4213bc":"markdown","c473a634":"markdown","41718c9e":"markdown","c025a488":"markdown","bf0f581b":"markdown","614bc554":"markdown","bcb48099":"markdown","9e6a0c16":"markdown","e70389e5":"markdown","6680dff0":"markdown","0b756ced":"markdown","75bb509f":"markdown","3b812860":"markdown","5b9eea6e":"markdown"},"source":{"91c9442d":"import ast\nimport glob\nimport os\nimport yaml\nimport torch\n\nimport numpy as np\nimport pandas as pd\n\n\nfrom IPython.display import Image, display\nfrom IPython.core.magic import register_line_cell_magic\nfrom shutil import copyfile\nfrom tqdm import tqdm\ntqdm.pandas()","554ef7cb":"os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint('Device:', device)\nprint('Current cuda device:', torch.cuda.current_device())\nprint('Count of using GPUs:', torch.cuda.device_count())","4a616d58":"HOME_DIR = '.\/'\nCOTS_DATASET_PATH = '.\/cots_dataset\/train_images'","4274d0e7":"# I just used spllited dataset by @julian3833 - Reef - A CV strategy: subsequences! \n# https:\/\/www.kaggle.com\/julian3833\/reef-a-cv-strategy-subsequences \n\ndf = pd.read_csv(\".\/reef\/train-validation-split\/train-0.1.csv\")\ndf.head(3)","94e1c455":"def add_path(row):\n    return f\"{COTS_DATASET_PATH}\/video_{row.video_id}\/{row.video_frame}.jpg\"\n\ndef num_boxes(annotations):\n    annotations = ast.literal_eval(annotations)\n    return len(annotations)\n\ndf['path'] = df.apply(lambda row: add_path(row), axis=1)\ndf['num_bbox'] = df['annotations'].apply(lambda x: num_boxes(x))\nprint(\"New path and annotations preprocessing completed\")","0ecf4ba4":"df = df[df.num_bbox > 0]\n\nprint(f'Dataset images with annotations: {len(df)}')","66ffd450":"def add_new_path(row):\n    if row.is_train:\n        return f\"{HOME_DIR}\/yolor_dataset\/images\/train\/{row.image_id}.jpg\"\n    else: \n        return f\"{HOME_DIR}\/yolor_dataset\/images\/valid\/{row.image_id}.jpg\"\n    \n\ndf['new_path'] = df.apply(lambda row: add_new_path(row), axis=1)\nprint(\"New image path for train\/valid created\")","2ec659a5":"df.head(3)","6ee2f695":"os.makedirs(f\"{HOME_DIR}\/yolor_dataset\/images\/train\")\nos.makedirs(f\"{HOME_DIR}\/yolor_dataset\/images\/valid\")\nos.makedirs(f\"{HOME_DIR}\/yolor_dataset\/labels\/train\")\nos.makedirs(f\"{HOME_DIR}\/yolor_dataset\/labels\/valid\")\nprint(f\"Directory structure yor YoloR created\")","d18b265d":"def copy_file(row):\n  copyfile(row.path, row.new_path)\n\n_ = df.progress_apply(lambda row: copy_file(row), axis=1)","47d1d8db":"IMG_WIDTH, IMG_HEIGHT = 1280, 720\n\ndef get_yolo_format_bbox(img_w, img_h, box):\n    w = box['width'] \n    h = box['height']\n    \n    if (bbox['x'] + bbox['width'] > 1280):\n        w = 1280 - bbox['x'] \n    if (bbox['y'] + bbox['height'] > 720):\n        h = 720 - bbox['y'] \n        \n    xc = box['x'] + int(np.round(w\/2))\n    yc = box['y'] + int(np.round(h\/2)) \n\n    return [xc\/img_w, yc\/img_h, w\/img_w, h\/img_h]\n    \n\nfor index, row in tqdm(df.iterrows()):\n    annotations = ast.literal_eval(row.annotations)\n    bboxes = []\n    for bbox in annotations:\n        bbox = get_yolo_format_bbox(IMG_WIDTH, IMG_HEIGHT, bbox)\n        bboxes.append(bbox)\n        \n    if row.is_train:\n        file_name = f\"{HOME_DIR}\/yolor_dataset\/labels\/train\/{row.image_id}.txt\"\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n    else:\n        file_name = f\"{HOME_DIR}\/yolor_dataset\/labels\/valid\/{row.image_id}.txt\"\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n        \n    with open(file_name, 'w') as f:\n        for i, bbox in enumerate(bboxes):\n            label = 0\n            bbox = [label]+bbox\n            bbox = [str(i) for i in bbox]\n            bbox = ' '.join(bbox)\n            f.write(bbox)\n            f.write('\\n')\n                \nprint(\"Annotations in YoloR format for all images created.\")","603b1089":"data_yaml = dict(\n    train = f'{HOME_DIR}\/yolor_dataset\/images\/train',\n    val = f'{HOME_DIR}\/yolor_dataset\/images\/valid',\n    nc = 1,\n    names = ['sf']\n)\n\n\nwith open(f'{HOME_DIR}\/YoloR-data.yaml', 'w') as outfile:\n    yaml.dump(data_yaml, outfile, default_flow_style=True)\n\nprint(f'Dataset configuration file for YoloR created')","1971ec9f":"!git clone https:\/\/github.com\/WongKinYiu\/yolor","1db76f07":"!pip install torchvision --upgrade -q\n!pip install wandb --upgrade","7b6e7842":"%cd yolor\n!pip install -qr requirements.txt","99bc522e":"%cd ..\n!git clone https:\/\/github.com\/JunnYu\/mish-cuda\n%cd mish-cuda\n!git reset --hard 6f38976064cbcc4782f4212d7c0c5f6dd5e315a8\n!python setup.py build install\n%cd ..","941b8dea":"!git clone https:\/\/github.com\/fbcotter\/pytorch_wavelets\n%cd pytorch_wavelets\n!pip install .\n%cd ..","093c38f0":"%cd yolor\n!bash scripts\/get_pretrain.sh","9885414e":"# more about Secrets -> https:\/\/www.kaggle.com\/product-feedback\/114053\nimport wandb\n# from kaggle_secrets import UserSecretsClient\n\n# user_secrets = UserSecretsClient()\n# wandb_api = user_secrets.get_secret(\"wandb_api\") \nwandb.login(key='your_key')\nwandb.login(anonymous='must')","241eb7b9":"%cd yolor","eb3d2109":"@register_line_cell_magic\ndef writetemplate(line, cell):\n    with open(line, 'w') as f:\n        f.write(cell.format(**globals()))","39db0266":"%%writetemplate .\/data\/coco.yaml\n\nnc: 1\nnames: ['starfish',]","74e1066e":"%%writetemplate .\/data\/coco.names\n\nstarfish","e599f051":"%%writetemplate .\/hyp-yolor.yaml\n\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum\/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.0  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.0  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.0  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+\/- deg)\ntranslate: 0.5  # image translation (+\/- fraction)\nscale: 0.0  # image scale (+\/- gain)\nshear: 0.0  # image shear (+\/- deg)\nperspective: 0.0  # image perspective (+\/- fraction), range 0-0.001\nflipud: 0.0  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 0.95  # image mosaic (probability)\nmixup: 0.3  # image mixup (probability)","c09e8de4":"!python train.py \\\n --batch-size 16 \\\n --img 2560 1024 \\\n --data '..\/YoloR-data.yaml' \\\n --cfg '.\/cfg\/yolor_p6.cfg' \\\n --weights '.\/yolor_p6.pt' \\\n --device 1 \\\n --name yolor_p6 \\\n --hyp '.\/hyp-yolor.yaml' \\\n --epochs 300","104e4686":"%cd ..","d0dd043a":"%pwd","cdb295ac":"INFER_PATH = f\".\/cots_dataset\/infer\"\n# os.makedirs(INFER_PATH)\n\ndf_infer = df.query(\"~is_train and num_bbox > 4\").sample(n = 15)\n\ndef copy_file(row):\n    new_location = INFER_PATH + '\/' + row.image_id + '.jpg'\n    copyfile(row.path, new_location)\n\n_ = df_infer.progress_apply(lambda row: copy_file(row), axis=1)","f2e7fbf6":"cd yolor","c149b97e":"!python detect.py \\\n    --source {INFER_PATH} \\\n    --cfg .\/cfg\/yolor_p6.cfg \\\n    --weights '.\/runs\/train\/yolor_p66\/weights\/best_overall.pt' \\\n    --conf 0.05 \\\n    --img-size 1280 \\\n    --device 1 ","4dc33e15":"for img in glob.glob('.\/inference\/output\/*.jpg'): \n    display(Image(filename=img))\n    print(\"\\n\")","12ec43d7":"## 5. TRAIN YoloR","c08c1cc7":"## 1. PREPARE DATASET","1f4213bc":"We got an error - but it is connected with w&b integrations. Looking for solution.","c473a634":"<div align=\"center\"><img width=\"640\" src=\"https:\/\/github.com\/WongKinYiu\/yolor\/raw\/main\/figure\/unifued_network.png\"\/><\/div>\n\n<div align=\"center\"><img width=\"640\" src=\"https:\/\/github.com\/WongKinYiu\/yolor\/raw\/main\/figure\/performance.png\"\/><\/div>","41718c9e":"## 4. INSTALL YoloR\n\n### 4A. CLONE YoloR GIT REPOSITORY ","c025a488":"### 4D. DWONLOAD LATEST CHECKPOINT FROM YoloR MODEL HUB \n\nIn this notebook we take P6 model (because I want to show only how to train YoloR model on Kaggle) but you can experiment with other YoloR models: https:\/\/github.com\/WongKinYiu\/yolor","bf0f581b":"## 6. INFERENCE USING YoloR ","614bc554":"## 0. IMPORT MODULES","bcb48099":"## 4. CREATE YoloR DATASET CONFIGURATION FILE","9e6a0c16":"### 4E. CONFIGURE WEIGHTS AND BIASES FOR EXPERIMENT LOGGING ","e70389e5":"## 2. CREATE DATASET FILE STRUCTURE","6680dff0":"## 3. CREATE YoloR ANNOTATIONS","0b756ced":"## Train YoloR on COTS dataset (PART 1 - TRAINING) - as easy as possible to help people start with YoloR and develop this notebook\nThis notebook introduces YOLOR on Kaggle and TensorFlow - Help Protect the Great Barrier Reef competition. It shows how to train custom object detection model (COTS dataset) using YoloR. It could be good starting point for build own custom model based on YoloR detector. Full github repository you can find here - [YOLOR](https:\/\/github.com\/WongKinYiu\/yolor)\n\nSteps covered in this notebook:\n\n* Prepare COTS dataset for YoloR training\n* Install YoloR (YoloR, MISH CUDA, pytorch_wavelets)\n* Download Pre-Trained Weights for YoloR HUB\n* Prepare configuration files (YoloR hyperparameters and dataset)\n* Weights and Biases configuration for training logging\n* YoloR training\n* Run YoloR inference on test images\n\n<div class=\"alert alert-warning\">I found that there is no reference custom model training YoloR notebook on Kaggle. Since we have such an opportunity this is my contribution to this competition. Feel free to use it and enjoy! I really appreciate if you upvote this notebook. Thank you!<\/div>\n\n<div class=\"alert alert-success\" role=\"alert\">\nI introduced YoloX in TensorFlow - Help Protect the Great Barrier Reef competition as well. You can find these notebooks here:      \n    <ul>\n        <li> <a href=\"https:\/\/www.kaggle.com\/remekkinas\/yolox-full-training-pipeline-for-cots-dataset\">YoloX full training pipeline for COTS dataset<\/a><\/li>\n        <li> <a href=\"https:\/\/www.kaggle.com\/remekkinas\/yolox-inference-on-kaggle-for-cots-lb-0-507\">YoloX detections submission made on COTS dataset<\/a><\/li>\n    <\/ul>\n    \n<\/div>","75bb509f":"### 4B. INSTALL MISH CUDA","3b812860":"### 4C. INSTALL PYTORCH WAVELETS ","5b9eea6e":"### 4F. CONFIGURE YoloR HYPERPARAMETERS "}}