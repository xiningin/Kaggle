{"cell_type":{"bb21242c":"code","f75d07c0":"code","3c16d12a":"code","1ba14c9d":"code","63641a82":"code","073b28ad":"code","935e887c":"code","2261ead7":"code","6e8a1556":"code","e08a2c20":"code","f12e98e8":"code","1ef4fe3b":"code","d28d8fc2":"code","9949e079":"code","6346a27f":"code","afc3b7d0":"code","36331878":"markdown","ece39a65":"markdown","e8011180":"markdown","4630efb9":"markdown","ae3c0f4b":"markdown","9b6f8c26":"markdown","14f08e9c":"markdown","a4654aa5":"markdown","6c6a3572":"markdown","e92497ec":"markdown","dc0f7ea6":"markdown","00bced21":"markdown","b08770bb":"markdown"},"source":{"bb21242c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.metrics import RootMeanSquaredError\nfrom sklearn.model_selection import cross_val_score\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f75d07c0":"train_data = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-3\/train.csv')\ntrain_data.head()","3c16d12a":"test_data = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-3\/eval.csv')\ntest_data.head()","1ba14c9d":"print(train_data.shape,\"\\n\")\nprint(test_data.shape,\"\\n\")\n\ntrain_data.info()\ntrain_data.describe()","63641a82":"print(test_data.shape,\"\\n\")\n\ntest_data.info()\ntest_data.describe()","073b28ad":"def check_for_values(data, cols):\n    for col in cols:\n        if(data[col].isna().sum() != 0):\n            print(data[col].isna().sum())\n","935e887c":"train_col = train_data.columns[train_data.dtypes != 'object']\ntest_col = test_data.columns[test_data.dtypes != 'object']\n\ncheck_for_values(train_data, train_col)\ncheck_for_values(test_data, test_col)","2261ead7":"train_data.columns\n\nsns.pairplot(train_data[['1','2','3','4','5','1270','1271','1272','1273','1274']], diag_kind = 'kde')","6e8a1556":"X_train = train_data.drop(['id', 'pubchem_id'], axis = 1)\nX_test = train_data.sample(frac = 0.2).drop(['id', 'pubchem_id'], axis = 1)\ny_test = X_test['Eat']\ny_train = X_train['Eat']\n\nX_test.drop(['Eat'], axis = 1, inplace = True)\nX_train.drop(['Eat'], axis = 1, inplace = True)\n\nmodels = []\nevals = []","e08a2c20":"relu_model = Sequential()\nrelu_model.add(Dense(1273, input_shape = (len(X_train.columns),), activation = 'relu'))\nrelu_model.add(Dense(10,activation = 'relu'))\nrelu_model.add(Dense(10,activation = 'relu'))\nrelu_model.add(Dense(10,activation = 'relu'))\nrelu_model.add(Dense(10,activation = 'relu'))\nrelu_model.add(Dense(10,activation = 'relu'))\nrelu_model.add(Dense(10,activation = 'relu'))\nrelu_model.add(Dense(10,activation = 'relu'))\nrelu_model.add(Dense(10,activation = 'relu'))\nrelu_model.add(Dense(10,activation = 'relu'))\nrelu_model.add(Dense(10,activation = 'relu'))\nrelu_model.add(Dense(1))\nrelu_model.compile(optimizer = 'adam', loss = 'mse', metrics = [RootMeanSquaredError()])\nrelu_model.fit(X_train, y_train, epochs = 300)\nrelu_model.summary()\nprint('Evaluating' + str(relu_model.evaluate(X_test, y_test)))\n\n\n\nmodels.append(relu_model)\nevals.append(relu_model.evaluate(X_test, y_test))","f12e98e8":"tanh_model = Sequential()\ntanh_model.add(Dense(1270,input_shape = (len(X_train.columns),), activation = 'tanh'))\ntanh_model.add(Dense(10,activation = 'tanh'))\ntanh_model.add(Dense(1))\ntanh_model.compile(optimizer = 'adam', loss = 'mse', metrics = [RootMeanSquaredError()])\ntanh_model.fit(X_train, y_train, epochs = 300)\ntanh_model.summary()\nprint('Evaluating' + str(tanh_model.evaluate(X_test, y_test)))\n\nmodels.append(tanh_model)\nevals.append(tanh_model.evaluate(X_test, y_test))","1ef4fe3b":"mixed_model = Sequential()\nmixed_model.add(Dense(1270,input_shape = (len(X_train.columns),), activation = 'relu'))\nmixed_model.add(Dense(10,activation = 'leaky_relu'))\nmixed_model.add(Dense(10, activation = 'sigmoid'))\nmixed_model.add(Dense(10, activation = 'relu'))\nmixed_model.add(Dense(10, activation = 'leaky_relu'))\nmixed_model.add(Dense(10, activation = 'sigmoid'))\nmixed_model.add(Dense(10, activation = 'tanh'))\nmixed_model.add(Dense(10, activation = 'relu'))\nmixed_model.add(Dense(10, activation = 'leaky_relu'))\nmixed_model.add(Dense(10, activation = 'tanh'))\nmixed_model.add(Dense(10, activation = 'relu'))\nmixed_model.add(Dense(1))\nmixed_model.compile(optimizer = 'adam', loss = 'mse', metrics = [RootMeanSquaredError()])\nmixed_model.fit(X_train, y_train, epochs = 300)\nmixed_model.summary()\nprint('Evaluating' + str(mixed_model.evaluate(X_test, y_test)))\n\nmodels.append(mixed_model)\nevals.append(mixed_model.evaluate(X_test, y_test))","d28d8fc2":"index = evals.index(min(evals))\nbest_model = models[index]\n\nprint('Best preforming model is model' + str(index + 1) + ',with a RMSE' + str(evals[index]))","9949e079":"def distributions(model, X_train, X_test, y_test, y_train):\n    scores = []\n    ids = test_data['id']\n    \n    for z in  range(30):\n        model.fit(X_train, y_train, epochs = 15, verbose = 0)\n        z, rmse = model.evaluate(X_test, y_test, verbose = 0)\n        scores.append(rmse)\n        \n    return np.asarray(scores)","6346a27f":"distrib_relu = distributions(relu_model, X_train, X_test, y_test, y_train)\ndistrib_tanh = distributions(tanh_model, X_train, X_test, y_test, y_train)\ndistrib_mix = distributions(mixed_model, X_train, X_test, y_test, y_train)\n\nmodels_pd = pd.DataFrame(data=np.asarray([distrib_relu, distrib_tanh, distrib_mix]).T, columns=['Relu Model', 'Tanh Model', 'Mixed Model'])\nmodels_pd.describe()","afc3b7d0":"ids = test_data['id']\n\nprint(test_data.head())\n\ntest_data = test_data.drop(['id','pubchem_id'], axis = 1)\npredictions =  best_model.predict(test_data).flatten()\n\n\noutput = pd.DataFrame({'id':ids, 'Eat':predictions})\noutput.to_csv('submission.csv', index = False)\n\nprint(output.to_string())","36331878":"Looking at the column naming scheme from printing out the head, it looks like id and pubchem_id aren't relevant to training the models, thus they will be dropping when building the models.","ece39a65":"# Building The Models","e8011180":"# Building A Submission","4630efb9":"# Model 3: Mismatched Activations","ae3c0f4b":"loading in both the training and test data and testing to make sure they were uploaded to the notebook properly","9b6f8c26":"# Determining Which Model to Use","14f08e9c":"Since the dataset is too big, I created a function that will take check each individual column and if it has missing values, print the column and how many many missing values it has. Since no columns were printed, no column has missing values.","a4654aa5":"# Tanh Activation Model","6c6a3572":"# Exploratory Data Analysis","e92497ec":"# Relu Model","dc0f7ea6":"# Validating The Scores","00bced21":"# Loading in The CSV FIles","b08770bb":"I'm plotting some of the columns because their are more than 1200 columns, the image for each graph would be too small, thus I'm only 10 columns to get an idea of how it works"}}