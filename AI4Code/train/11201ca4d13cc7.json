{"cell_type":{"9ea9f6e2":"code","02821768":"code","3bd89183":"code","a3f0129b":"code","e0998b32":"code","29999fa0":"code","7f8a9a0c":"code","876f3424":"code","30f14dc3":"code","bdad1b6f":"code","bb8912c2":"code","f8b2b530":"code","d4dfaafb":"code","316d10b4":"code","e93bde78":"code","509470dc":"code","dcb4c757":"code","f7fce423":"code","5d0e84a6":"code","839730aa":"code","20d7c13f":"code","7784094c":"code","a6b2d26a":"code","ca85d01d":"code","7fc81c6b":"code","385ba9b3":"code","03a41363":"code","c2333dbc":"code","78bdd692":"code","234eb642":"code","53fdc08b":"code","b2724254":"code","384a7110":"code","25aa8abc":"code","2b26e2e9":"code","d7abbfdc":"code","ab9d337e":"code","75116218":"code","ad66d4c8":"code","92058cc1":"code","4309b4b5":"code","177ad523":"code","23989177":"code","984ffa31":"code","4b62b6d9":"code","dc8781e5":"code","974b9eca":"code","50df40e3":"code","5f1103aa":"code","dfe37911":"code","0d9549d3":"code","e3f703a9":"code","ce4639a9":"code","baa5db1b":"code","a1e8f1e7":"code","43843037":"code","ff7726c6":"code","b963df8e":"code","17033d72":"code","d9e22403":"markdown","60a1557b":"markdown","ca70600d":"markdown","c8f7df27":"markdown","35becb0a":"markdown","34f133d8":"markdown","16b35e0a":"markdown","4955e5f5":"markdown","1f3028ce":"markdown","ff94af76":"markdown","e8753421":"markdown","fe70a2cc":"markdown","5ced29d2":"markdown","dd2e989f":"markdown","7f7c9ce8":"markdown","ccfe9e54":"markdown","9860b519":"markdown","7ef4692f":"markdown","d9b289c3":"markdown","447d25b3":"markdown","e683e79b":"markdown","5278bd72":"markdown","0760962c":"markdown","f4517d50":"markdown","8fbffbcc":"markdown","05427dd3":"markdown","f448d18a":"markdown","4dc1f2d0":"markdown","946c3536":"markdown","dea6a5d2":"markdown","1872a5bd":"markdown","6cdc98d6":"markdown","2c4f216f":"markdown","7d9c551d":"markdown","40cdf3a5":"markdown","ec4ba8b6":"markdown","d919b3e0":"markdown","5bc80117":"markdown","7151ff50":"markdown"},"source":{"9ea9f6e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","02821768":"import matplotlib.pyplot as plt\nimport seaborn as sns","3bd89183":"data = pd.read_csv(\"..\/input\/twitterdata\/finalSentimentdata2.csv\")","a3f0129b":"data.head()","e0998b32":"data.shape","29999fa0":"sns.countplot(data['sentiment'])","7f8a9a0c":"import nltk\nimport re\nimport string","876f3424":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","30f14dc3":"data['text'] = data['text'].apply(lambda x: clean_text(x))","bdad1b6f":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","bb8912c2":"data['text']=data['text'].apply(lambda x: remove_emoji(x))","f8b2b530":"data['text'].apply(lambda x:len(str(x).split())).max()","d4dfaafb":"from wordcloud import WordCloud, STOPWORDS","316d10b4":" word_cloud = WordCloud(\n                    background_color='white',\n                    stopwords=set(STOPWORDS),\n                    max_words=50,\n                    max_font_size=40,\n                    scale=5,\n                    random_state=1).generate(str(data['text']))\nfig = plt.figure(1, figsize=(10,10))\nplt.axis('off')\nfig.suptitle('Word Cloud for top 50 prevelant words in India', fontsize=20)\nfig.subplots_adjust(top=2.3)\nplt.imshow(word_cloud)\nplt.show()","e93bde78":"from sklearn.preprocessing import LabelEncoder\nlb= LabelEncoder()","509470dc":"lb.fit(data['sentiment'])","dcb4c757":"classes= list(lb.classes_)","f7fce423":"classes","5d0e84a6":"data['sentiment']= lb.fit_transform(data['sentiment'])","839730aa":"data.head()","20d7c13f":"import torch\nimport torch.nn as nn","7784094c":"#importing transformers\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader","a6b2d26a":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","ca85d01d":"#setting device to GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","7fc81c6b":"device","385ba9b3":"PRE_TRAINED_MODEL_NAME = '..\/input\/bert-base-uncased'","03a41363":"tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","c2333dbc":"sample_txt = 'These are tough times we must stand together'","78bdd692":"tokens = tokenizer.tokenize(sample_txt)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(f' Sentence: {sample_txt}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","234eb642":"encoding = tokenizer.encode_plus(\n  sample_txt,\n  max_length=32,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',  # Return PyTorch tensors\n)\nencoding.keys()\n","53fdc08b":"token_lens = []\nfor txt in data.text:\n    \n    tokens = tokenizer.encode(txt, max_length=512)\n    token_lens.append(len(tokens))","b2724254":"sns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count')","384a7110":"MAX_LEN=100","25aa8abc":"class Covid19Tweet(Dataset):\n    \n    def __init__(self, tweets, sentiment, tokenizer, max_len):\n        \n        \n        self.tweets = tweets\n        self.sentiment = sentiment\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.tweets)\n    def __getitem__(self, item):\n        \n        tweets = str(self.tweets[item])\n        sentiment = self.sentiment[item]\n        encoding = self.tokenizer.encode_plus(\n        tweets,\n        add_special_tokens=True,\n        max_length=self.max_len,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='pt')\n        return {\n        'tweet_text': tweets,\n         'input_ids': encoding['input_ids'].flatten(),\n         'attention_mask': encoding['attention_mask'].flatten(),\n         'sentiments': torch.tensor(sentiment, dtype=torch.long)\n          }","2b26e2e9":"data.head()","d7abbfdc":"from sklearn.model_selection import train_test_split","ab9d337e":"train, val = train_test_split(\n  data,\n  test_size=0.1,\n  random_state=RANDOM_SEED\n)","75116218":"train.shape,val.shape","ad66d4c8":"def create_data_loader(data, tokenizer, max_len, batch_size):\n    \n    ds = Covid19Tweet(tweets=data.text.to_numpy(),\n    sentiment=data.sentiment.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len)\n    return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=4)\nBATCH_SIZE = 32\ntrain_data_loader = create_data_loader(train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(val, tokenizer, MAX_LEN, BATCH_SIZE)\n","92058cc1":"df = next(iter(train_data_loader))\ndf.keys()\n","4309b4b5":"print(df['input_ids'].shape)\nprint(df['attention_mask'].shape)\nprint(df['sentiments'].shape)","177ad523":"bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)","23989177":"class SentimentClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        \n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    def forward(self, input_ids, attention_mask):\n        \n        _, pooled_output = self.bert(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n    )\n        output = self.drop(pooled_output)\n        return self.out(output)","984ffa31":"n_classes= 4","4b62b6d9":"model = SentimentClassifier(n_classes)\nmodel = model.to(device)","dc8781e5":"input_ids = df['input_ids'].to(device)\nattention_mask = df['attention_mask'].to(device)","974b9eca":"print(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape)","50df40e3":"import torch.nn.functional as F","5f1103aa":"F.softmax(model(input_ids, attention_mask),dim=1)","dfe37911":"model.parameters","0d9549d3":"EPOCHS = 10\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","e3f703a9":"def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler, n_examples):  \n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for d in data_loader:\n        \n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"sentiments\"].to(device)\n        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","ce4639a9":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    \n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"sentiments\"].to(device)\n            outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n        return correct_predictions.double() \/ n_examples, np.mean(losses)\n            \n                ","baa5db1b":"from collections import defaultdict","a1e8f1e7":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(model,train_data_loader,loss_fn,optimizer,device,scheduler,len(train))\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(model,val_data_loader,loss_fn,device,len(val))\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    if val_acc > best_accuracy:\n        \n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","43843037":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","ff7726c6":"review_text = 'Life has come to standstill due to this pandemic, milllions have lost their job '","b963df8e":"encoded_review = tokenizer.encode_plus(review_text,max_length=MAX_LEN,add_special_tokens=True,\n                                           return_token_type_ids=False,pad_to_max_length=True,return_attention_mask=True,\n                                           return_tensors='pt')","17033d72":"input_ids = encoded_review['input_ids'].to(device)\nattention_mask = encoded_review['attention_mask'].to(device)\noutput = model(input_ids, attention_mask)\n_, prediction = torch.max(output, dim=1)\nprint('Review text :{}'.format(review_text))\nprint('Sentiment :{}'.format(classes[prediction]))","d9e22403":"**If you find this notebook,useful please upvote it, it will keep me motivated for making such content**","60a1557b":"You can install the bert base uncased, from kaggle","ca70600d":"Let's break it down simply:\nFirst, it\u2019s easy to get that BERT stands for Bidirectional Encoder Representations from Transformers. Each word here has a meaning to it and we will encounter that one by one in this article. For now, the key takeaway from this line is \u2013 BERT is based on the Transformer architecture.\n\n\nThe BERT architecture builds on top of Transformer. We currently have two variants available:\n\nBERT Base: 12 layers (transformer blocks), 12 attention heads, and 110 million parameters\nBERT Large: 24 layers (transformer blocks), 16 attention heads and, 340 million parameters\n![image.png](attachment:image.png)\n","c8f7df27":"Let's  label encode the sentiments","35becb0a":"# The end.\n***If you enjoyed my notebook please upvote it***","34f133d8":"Most of the tweets contain less than 100 tokens but we'll be on the safe side and consider maximum length 100","16b35e0a":"Before starting with bert I wouls advise you to goo through the following links\n1. https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n2. https:\/\/arxiv.org\/abs\/1810.04805 (paper released by google)\n3. https:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/demystifying-bert-groundbreaking-nlp-framework\/\n4. https:\/\/medium.com\/@jonathan_hui\/nlp-bert-transformer-7f0ac397f524#:~:text=BERT%20(Bidirectional%20Encoder%20Representations%20from,a%20dense%20representation%20of%20words.&text=In%20BERT%2C%20a%20model%20is,dense%20representation%20of%20the%20input.\n5. If you are unaware of Transformer https:\/\/www.analyticsvidhya.com\/blog\/2019\/06\/understanding-transformers-nlp-state-of-the-art-models\/","4955e5f5":"Let's implement Pytorch:-\nif you want to lean","1f3028ce":"Count plot of sentiments,we can see that sad and fear are prevailing which is quite obvious","ff94af76":"# SENTIMENT ANALYSIS + EDA ON COVID 19 TWEETS\n*The pandemic has really affected us in every domain, with tweets flowing like a river let us see what sentiments does it hold*\n![image.png](attachment:image.png)","e8753421":"# Choosing Sequence Length\nBERT works with fixed-length sequences. We\u2019ll use a simple strategy to choose the max length. Let\u2019s store the token length of each review:","fe70a2cc":"Let\u2019s create an instance and move it to the GPU","5ced29d2":"# Sentiment analysis\nNow let's get our hands dirty with BERT transformer","dd2e989f":"Dataloader:-\nconverts data to be fed into classifier","7f7c9ce8":"Training with each epoch, it returns loss and accuracy with each step, and use gradient clipping to prrevent gradiemt clipping and optimize each step","ccfe9e54":"# What is BERT?\n![image.png](attachment:image.png)","9860b519":"***IN this notebook we'll study the tweets on covid-19* and also perform Sentiment analysis so sit back to enjoy the power of BERT transformer**\n\nOn October 2018 Bert paper was invented and since then there's no looking back...","7ef4692f":"**Word cloud of 50 most  common words in India**","d9b289c3":"Function to remove emoji","447d25b3":"Plotting results","e683e79b":"# BERT MODEL\nFirst we'll define model from pre trained Bert model,\nNext we'll  define a class named Sentiment classifier\n","5278bd72":"*BERT stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.*\nthis is how researchers defined Bert","0760962c":"I'll implement my codes, with the help of  hugging face documentation\n\nhttps:\/\/huggingface.co\/transformers\/model_doc\/bert.html","f4517d50":"Encode plus does all the work of adding special tokens, CLS as starting and SEP as ending , it also pad the sentence to maximum length provided, it also provide attention mask","8fbffbcc":"Checking the maximum length of tweet ","05427dd3":"# Let us create our dataset...","f448d18a":"**Code Implementation**\nfirst of all we'll implement a class named Covid19Tweet which takes dataset, as input, components of class are:-\n1. Init function, in init function we'll define the components of our dataset, namely tweet text,sentiment and attention mask and input token id\n2. len function returns the length of dataset\n3. get item,  returns the actual items also it performs encoding","4dc1f2d0":"Printing results......","946c3536":"Initialising Bert tokenizer\nBERT uses WordPiece tokenization. The vocabulary is initialized with all the individual characters in the language, and then the most frequent\/likely combinations of the existing words in the vocabulary are iteratively added.","dea6a5d2":"So basically the encoding is anger-0.fear-1,joy-2,sad-3","1872a5bd":"To reproduce training features from the BERT paper, we will use AdamW optimizer,Implements Adam algorithm with weight decay\n\n also use get_linear_schedule_with_warmup create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n \n we'll use loss function as Cross Entropy","6cdc98d6":"# Predicting on a raw text:-","2c4f216f":"We'll do the same with evaluation, except for optimization","7d9c551d":"Also check out my other notebooks on https:\/\/www.kaggle.com\/alankritamishra\/notebooks","40cdf3a5":"To get the predicted probabilities from our trained model, we\u2019ll apply the softmax function to the outputs","ec4ba8b6":"# **Code Implementation**\nOur class is the subclass, of nn.Module class which is the base class for all the for all neural network modules.\nIn the clas we have following components:-\n1. Init function,it has a constructor function which initialises the components of classifier, and forward function which perform forward propagation,\n\nOur classifier delegates most of the heavy lifting to the BertModel. We use a dropout layer for some regularization and a fully-connected layer for our output. Note that we\u2019re returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work.","d919b3e0":"***Data Preprocessing***","5bc80117":"A function to clean data it removes all the punctuation marks, urls etc","7151ff50":"Let's implement it on sample text what it basically does is convert sentence to words(tokens) and assign each word a numerical value(token ids)"}}