{"cell_type":{"650cbe08":"code","c2b67c53":"code","b0350aaf":"code","64d77712":"code","1d4a302c":"code","8cbeee7e":"code","80a8f25e":"code","1a93f915":"code","abdc0ffa":"code","29953bc3":"code","72b43c24":"code","27438a2e":"code","6fa9efe7":"code","4d729cdb":"code","9ae64ea6":"code","dd8ed10b":"code","789b9ae4":"code","f48c7710":"code","0d5889a5":"code","1c05fc1a":"code","1d69aa0f":"code","0b72262e":"code","61c4a702":"code","9a3b4f05":"code","5dac0dcd":"code","e85a4e58":"code","0e38b46a":"code","6328a95c":"code","1ccc8f1e":"markdown","99c108a7":"markdown","c44bec2a":"markdown","b900302f":"markdown","5f3cf5ee":"markdown","da005884":"markdown","808b2333":"markdown","7a91727b":"markdown"},"source":{"650cbe08":"# Loads required packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK","c2b67c53":"# Loads train dataset\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")","b0350aaf":"# Checks how the train data set looks\nwith pd.option_context('display.max_rows', 10, 'display.max_columns', None): \n    display(train)","64d77712":"# Drops ID column as it is not required\ntrain.drop([\"id\"], axis=1, inplace=True)","1d4a302c":"# Checks for data types used in the data set\ntrain.dtypes.unique()","8cbeee7e":"# Checks for nubmer of row having any missing values ('0' indicates no rows have missing values)\nsum(train.isna().sum())","80a8f25e":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","1a93f915":"# Compresses the training data as Kaggle kernel resets due to large size of the training data \ntrain = reduce_mem_usage(train)","abdc0ffa":"# Shows the column data types after data compression\ntrain.dtypes","29953bc3":"# Checks distribution of categorical target variable\ntrain.target.value_counts()","72b43c24":"# Seperates predictor variables from target\n\ny = train.target\ntrain.drop([\"target\"], axis=1, inplace=True)","27438a2e":"# Create stratification object for K-Fold cross validation\nsk_fold = StratifiedKFold(n_splits=5)","6fa9efe7":"# Performs cross validation on XGB Classifier\n\ncv_generator = sk_fold.split(train, y)\n\nmodel = XGBClassifier(\n    n_estimators=100,\n    objective='binary:logistic', \n    eval_metric='auc',\n    tree_method='gpu_hist'\n)\n\ncv_scores = cross_val_score(model, train, y, scoring='roc_auc', cv=cv_generator, n_jobs=-1, verbose=10)","4d729cdb":"print(\"ROC AUC score of XGBoost (with default parameters) Model:\", cv_scores.mean())","9ae64ea6":"del cv_scores, model, cv_generator","dd8ed10b":"# Instead of performing cross validation during hyperparameter tunining, \n# the tuning is done over fixed train and validation data set to save significant amount of time\n# The following code snippet extract that stratified set of train and validation set\n\ncv_generator = sk_fold.split(train, y)\n\nfor fold, (idx_train, idx_val) in enumerate(cv_generator):\n    y_val = y.iloc[idx_val]\n    dtrain = xgb.DMatrix(data=train.iloc[idx_train], label=y.iloc[idx_train])\n    dval = xgb.DMatrix(data=train.iloc[idx_val], label=y.iloc[idx_val])\n    break","789b9ae4":"# Sets up a search space for XGBoost hyperparameters\nspace = {\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n    'max_depth': hp.quniform(\"max_depth\", 2, 6, 1),\n    'min_child_weight' : hp.quniform('min_child_weight', 1, 8, 1),\n    'reg_alpha' : hp.uniform('reg_alpha', 1e-8, 100),\n    'reg_lambda' : hp.uniform('reg_lambda', 1e-8, 100),\n    'gamma': hp.uniform ('gamma', 0.0, 1.0),\n    'subsample': hp.uniform(\"subsample\", 0.1, 1.0),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0)\n}","f48c7710":"def trial_loss(space):\n    \"\"\"\n    Trial function for Hyperopt to call by passing a set a trial hyperparamets\n    to train model and perform predictions.\n    \n    Parameters:\n    ----------\n    space: A set a trial hyperparamets\n    \n    Returns metric for Hyperopt to estimate for further tuning in search space.\n    \"\"\"\n    \n    # Converts parameter value to int as required by XGBoost\n    space[\"max_depth\"] = int(space[\"max_depth\"])\n    space[\"objective\"] = \"binary:logistic\"\n    space[\"eval_metric\"] = \"auc\"\n    space[\"tree_method\"] = \"gpu_hist\"\n    \n    model = xgb.train(\n        space, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=False)\n    \n    predictions = model.predict(dval)\n    \n    roc_auc = roc_auc_score(y_val, predictions)\n    \n    del predictions, model, space\n    \n    return {\"loss\": -roc_auc, \"status\": STATUS_OK}","0d5889a5":"# Starts hyperparameters tuning\ntrials = Trials()\nbest_trial = fmin(fn=trial_loss, space=space, algo=tpe.suggest, max_evals=50, trials=trials)","1c05fc1a":"# Views the best hyperparameters\nbest_trial","1d69aa0f":"del dtrain, dval, y_val, cv_generator","0b72262e":"# Loads test data set\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\n\n# Removes ID column as it is not required for prediction\ntest.drop([\"id\"], axis=1, inplace=True)\n\n# Loads submission data set that acts just as a template for submission\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")","61c4a702":"# Adds other important parameters\nbest_trial[\"max_depth\"] = int(best_trial[\"max_depth\"])\nbest_trial[\"objective\"] = \"binary:logistic\"\nbest_trial[\"eval_metric\"] = \"auc\"\nbest_trial[\"tree_method\"] = \"gpu_hist\"","9a3b4f05":"# Gets the model trained over cross validation and predictions \n# against each iteration is stored\n\ntest_predictions = []\n\ncv_generator = sk_fold.split(train, y)\n\ndtest = xgb.DMatrix(data=test)\n\nfor fold, (idx_train, idx_val) in enumerate(cv_generator):\n    print(\"fold\", fold)\n\n    dtrain = xgb.DMatrix(data=train.iloc[idx_train], label=y.iloc[idx_train])\n    dval = xgb.DMatrix(data=train.iloc[idx_val], label=y.iloc[idx_val])\n    \n    model = xgb.train(\n        best_trial, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=200)\n    \n    predictions = model.predict(dtest)\n    \n    test_predictions.append(predictions)\n    \n    del predictions, model, dval, dtrain","5dac0dcd":"test_predictions","e85a4e58":"del dtest, cv_generator, test, train","0e38b46a":"# Predictions stored against each cross validation iteration finally gets aeveraged\n# and target column is set with that averaged predictions\nsubmission[\"target\"] = np.mean(np.column_stack(test_predictions), axis=1)\n\n# Checks for sumbission file before saving\nsubmission","6328a95c":"# Saves test predictions\nsubmission.to_csv(\".\/submission.csv\", index=False)","1ccc8f1e":"## Dependencies","99c108a7":"## Submission","c44bec2a":"**Prepares final XGBoost model with optimized parameters**","b900302f":"## Exploratory Data Analysis (EDA) & Preprocessing","5f3cf5ee":"## Modeling & Evaluation","da005884":"**As `target` is equaly distributed, it itself can be used as bins in stratified K-Fold validation**","808b2333":"**Automated Hyperparameter Tuning with Hyperopt**","7a91727b":"# Tabular Playground Series - Oct 2021\n\n#### Oct 01, 2021 to Oct 31, 2021\n\n#### https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/\n\n#### _**Predicting the biological response of molecules given various chemical properties**_\n\nNotebook Author:\n\n| Name  | Pradip Kumar Das  |\n| ------------: | :------------ |\n| **Profile:**  | [LinkedIn](https:\/\/www.linkedin.com\/in\/daspradipkumar\/ \"LinkedIn\") l [GitHub](https:\/\/github.com\/PradipKumarDas \"GitHub\") l [Kaggle](https:\/\/www.kaggle.com\/pradipkumardas \"Kaggle\")  |\n| **Contact:**  | pradipkumardas@hotmail.com (Email)  |\n| **Location:**  | Bengaluru, India  |\n\n**Sections:**\n\n* Dependencies\n* Exploratory Data Analysis (EDA) & Preprocessing\n* Modeling & Evaluation\n* Submission"}}