{"cell_type":{"24d1d97e":"code","4205ce71":"code","e393632a":"code","b565ef40":"code","e6606a91":"code","d35fa16d":"code","51100248":"code","c532da3e":"code","250a9fce":"code","8062f012":"code","e0b6a85b":"code","95577351":"code","b5503b8f":"code","233ffe39":"code","017a3111":"code","62ad9c3c":"code","6c95fa22":"code","4e0ca530":"code","aa7e03c3":"code","330a4f2a":"code","38f8ce70":"code","4540edbf":"code","aa2a5d07":"code","8b620f11":"markdown","7ef3b8db":"markdown","dbd8fd79":"markdown","fa088e44":"markdown","5e0cafec":"markdown","108fe91d":"markdown","47a922e6":"markdown","b6085b68":"markdown","7c780a86":"markdown","28580ef9":"markdown","d8268b53":"markdown","242c80a1":"markdown","a570b5da":"markdown","adb8be9e":"markdown","006c6c5b":"markdown","9fbad516":"markdown","d7230f85":"markdown","75371033":"markdown"},"source":{"24d1d97e":"# !pip install -r \/kaggle\/input\/pre-summy\/PreSumm\/requirements.txt","4205ce71":"import pandas as pd \nsummary_df = pd.read_csv('\/kaggle\/input\/news-summary\/news_summary_more.csv')\nsummary_df","e393632a":"import os \nos.chdir('\/kaggle\/working')","b565ef40":"!mkdir news_sum\n!mkdir merged_stories_tokenized\n!mkdir json_data\n!mkdir bert_data\n!mkdir logs\n!mkdir temp\n!mkdir bertsumextabs ","e6606a91":"# import os\n# def create_story_files(df,num_file=100,filename=None):\n#     for i in range(num_file):\n#         doc = df['text'][i] + '\\n'*2 + '@highlight' + '\\n'*2 + df['headlines'][i]\n#         file_name = os.path.join(filename,(str(i) + '.story'))\n#         with open(file_name,'w') as story_file:\n#             story_file.write(doc)","d35fa16d":"# create_story_files(summary_df,num_file=30000,filename='\/kaggle\/working\/news_sum')","51100248":"import os\nos.chdir('\/kaggle\/input\/pre-summy\/PreSumm\/src\/')","c532da3e":"# !python preprocess.py -mode tokenize -raw_path \/kaggle\/working\/news_sum -save_path \/kaggle\/working\/merged_stories_tokenized  -log_file \/kaggle\/working\/logs\/cnndm.log","250a9fce":"# !python preprocess.py -mode format_to_lines -raw_path \/kaggle\/working\/merged_stories_tokenized -save_path \/kaggle\/working\/json_data\/news -n_cpus 1 -use_bert_basic_tokenizer false -log_file \/kaggle\/working\/logs\/cnndm.log ","8062f012":"# !python preprocess.py -mode format_to_bert -raw_path \/kaggle\/working\/json_data -save_path \/kaggle\/working\/bert_data  -lower -n_cpus 1 -log_file \/kaggle\/working\/logs\/cnndm.log ","e0b6a85b":"import os\nos.chdir('\/kaggle\/working\/')\n\n!rm -r news_sum\n!rm -r merged_stories_tokenized\n!rm -r json_data","95577351":"import os\nos.chdir('\/kaggle\/input\/pre-summy\/PreSumm\/src\/')","b5503b8f":"# !python train.py  -task abs -mode train -train_from \/kaggle\/input\/absbert-weights\/model_step_149000.pt -bert_data_path \/kaggle\/working\/bert_data\/news  -dec_dropout 0.2  -model_path \/kaggle\/working\/bertsumextabs -sep_optim true -lr_bert 0.002 -lr_dec 0.02 -save_checkpoint_steps 1000 -batch_size 140 -train_steps 150000 -report_every 100 -accum_count 5 -use_bert_emb true -use_interval true -warmup_steps_bert 1000 -warmup_steps_dec 500 -max_pos 512 -visible_gpus 0  -temp_dir \/kaggle\/working\/temp -log_file \/kaggle\/working\/logs\/abs_bert_cnndm","233ffe39":"os.chdir('\/kaggle\/input\/pre-summy\/PreSumm\/src')","017a3111":"# !python train.py -task abs -mode test -model_path \/kaggle\/working\/bertsumextabs -test_from \/kaggle\/working\/bertsumextabs\/model_step_150000.pt -batch_size 100 -test_batch_size 100 -bert_data_path \/kaggle\/working\/bert_data\/news -temp_dir \/kaggle\/working\/temp -log_file \/kaggle\/working\/logs\/abs_bert_cnndm  -sep_optim true -use_interval true -visible_gpus 0 -max_pos 512 -max_length 200 -alpha 0.82 -min_length 10 -result_path \/kaggle\/working\/logs\/abs_bert_cnndm","62ad9c3c":"os.chdir('\/kaggle\/working\/')","6c95fa22":"# with open('logs\/abs_bert_cnndm.150000.gold','r') as s:\n#     summary = s.readlines()\n\n# for i in range(10):\n#     print(summary[i])","4e0ca530":"# with open('logs\/abs_bert_cnndm.150000.candidate','r') as s:\n#     cand = s.readlines()\n\n# for i in range(10):\n#     print(cand[i])","aa7e03c3":"# !pip install rouge","330a4f2a":"# from rouge import Rouge\n# rouge = Rouge()\n# rouge_1= 0\n# rouge_l=0\n# for i in range(len(summary)):\n#     scores = rouge.get_scores(summary[i],cand[i])\n#     rating = 'good' if scores[0]['rouge-l']['r']>0.5 else 'bad'\n#     rouge_1+=float(scores[0]['rouge-1']['r'])\n#     rouge_l+=float(scores[0]['rouge-l']['r'])\n    \n# rouge_1\/=len(summary)\n# rouge_l\/=len(summary)","38f8ce70":"# print('Average Rouge-1: {},  Rouge-L:{}'.format(rouge_1 ,rouge_l))","4540edbf":"import os\nos.chdir('\/kaggle\/working\/')","aa2a5d07":"!rm -r bert_data\n!rm -r logs\n!rm -r temp\n!rm -r bertsumextabs ","8b620f11":"Our data process includes the following four steps:  \n- 1. Convert our raw data to **.story** format and store in *\/kaggle\/working\/news_sum\/* . This enables us to use **preprocess.py** code to do further process  \n- 2. Tokenize **.story** file via using [**StanfordCoreNLP**](https:\/\/stanfordnlp.github.io\/CoreNLP\/index.html). Stanford CoreNLP is written in **Java**; recent releases require Java **1.8+**. You need to have Java installed to run CoreNLP. However, you can interact with CoreNLP via the command-line or its web service; many people use CoreNLP while writing their own code in Javascript, Python, or some other language.  \n\n    (*Here we activate Stanford CoreNLP by the code in **preprocess.py***)\n- 3. Convert tokenized data as **json** format \n- 4. Convert **json** file to **.pt** format for model finetuning\n","7ef3b8db":"## 2. Make  *.story* files as tokenized data file via StandfordCoreNLP","dbd8fd79":"# Setup \nInstall the required package from **requirements.txt** in presummy folder","fa088e44":"We can get Average **Rouge-1: 0.522**,  **Rouge-L:0.487** after finetuning","5e0cafec":"* The code here uses a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences.   \n* The original [paper](https:\/\/arxiv.org\/abs\/1908.08345) includes a two-staged fine-tuning approach *(finetuning on extractive BERT and then abstractive BERT)* can further boost the quality of the generated summaries, but here we only do the second stage finetuning on the **abstractive summarization model**.","108fe91d":"Glimpse on our News summary data released by **Kondalarao Vonteru** in [here](https:\/\/www.kaggle.com\/sunnysai12345\/news-summary#news_summary_more.csv)","47a922e6":"## 1. Make raw description and summary data as *.story*** files\nHere we preprocess only 100 files to save kernel commit time ","b6085b68":"# Read the raw description and summary data","7c780a86":"## 3. Make  *tokenized data* files as *.json file* ","28580ef9":"## **folder name -> data file**\n* **news_sum** -> .story data  \n* **merged_stories_tokenized** -> tokenized data  \n* **json_data** ->  json files\n* **bert_data** ->  .pt files\n* **logs** ->  for storing logs information during preprocess and finetuning\n* **temp** -> cache model config data\n* **bertsumextabs** ->  save finetuning model weights****\n","d8268b53":"# Introduction","242c80a1":"# Remove generate data to save commit time","a570b5da":"This kernel demonstrates how to use the open source code [PreSumm](http:\/\/github.com\/nlpyang\/PreSumm) released by **yang liu** and to conduct text summarization task on your own dataset. It covers the following contents\n\n- 1. Process the data by using **process.py** from PreSumm repo and [**StanfordCoreNLP**](https:\/\/stanfordnlp.github.io\/CoreNLP\/download.html) tool\n- 2. Fine-tuning the text summarization model and save weights\n\n***Notice that:** The Presumm code used here has been modified to run in kaggle and process the customized data.*","adb8be9e":"# Evaluation - Rouge-1 , Rouge-L ","006c6c5b":"## Create required folder for the preprocess and modeling fine-tuning","9fbad516":"# Model Finetuning","d7230f85":"# Data Preprocess","75371033":"## 4. Make .json file as .pt file for BERT model"}}