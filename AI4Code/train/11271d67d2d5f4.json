{"cell_type":{"b0f968fe":"code","2d7c3e2e":"code","ed474cb2":"code","66bd872b":"code","856e44c5":"code","c7be4807":"code","ee618381":"code","d5fc93c7":"code","cab03a84":"code","1eb076b6":"code","404be5d9":"code","3f686312":"code","f2d5fec7":"code","bc3478dc":"code","f1cbd1c4":"code","ed51e81a":"code","2f39eb6b":"code","740001aa":"code","63b33dd0":"markdown","6d81170a":"markdown","cf1fd319":"markdown","5de739b2":"markdown","016c1105":"markdown","c6cdc661":"markdown","125a9744":"markdown","718724ec":"markdown","4f906d29":"markdown","5e887083":"markdown","861f2f12":"markdown","7a34686b":"markdown","63591880":"markdown","6989a95d":"markdown","548395ff":"markdown","085ea75a":"markdown","30db53c7":"markdown","a5803657":"markdown","5e24f638":"markdown","d477c943":"markdown","a33cd385":"markdown"},"source":{"b0f968fe":"# If you want to test this on your local notebook\n# http:\/\/contrib.scikit-learn.org\/categorical-encoding\/\n# !pip install category-encoders","2d7c3e2e":"import pandas as pd\n\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom category_encoders.woe import WOEEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.sum_coding import SumEncoder\nfrom category_encoders.m_estimate import MEstimateEncoder\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom category_encoders.helmert import HelmertEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom category_encoders.james_stein import JamesSteinEncoder\nfrom category_encoders.one_hot import OneHotEncoder\n\nTEST = False","ed474cb2":"%%time\ntrain = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/test.csv')\ntarget = train['target']\ntrain_id = train['id']\ntest_id = test['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","66bd872b":"feature_list = list(train.columns) # you can custumize later.","856e44c5":"%%time\nLE_encoder = OrdinalEncoder(feature_list)\ntrain_le = LE_encoder.fit_transform(train)\ntest_le = LE_encoder.transform(test)","c7be4807":"# %%time\n# this method didn't work because of RAM memory. \n# so we have to use pd.dummies \n# OHE_encoder = OneHotEncoder(feature_list)\n# train_ohe = OHE_encoder.fit_transform(train)\n# test_ohe = OHE_encoder.transform(test)","ee618381":"# %%time\n# this method didn't work because of RAM memory. \n# SE_encoder =SumEncoder(feature_list)\n# train_se = SE_encoder.fit_transform(train[feature_list], target)\n# test_se = SE_encoder.transform(test[feature_list])","d5fc93c7":"# %%time\n# this method didn't work because of RAM memory. \n# HE_encoder = HelmertEncoder(feature_list)\n# train_he = HE_encoder.fit_transform(train[feature_list], target)\n# test_he = HE_encoder.transform(test[feature_list])","cab03a84":"%%time\n\nTE_encoder = TargetEncoder()\ntrain_te = TE_encoder.fit_transform(train[feature_list], target)\ntest_te = TE_encoder.transform(test[feature_list])\n\ntrain_te.head()","1eb076b6":"%%time\nMEE_encoder = MEstimateEncoder()\ntrain_mee = MEE_encoder.fit_transform(train[feature_list], target)\ntest_mee = MEE_encoder.transform(test[feature_list])","404be5d9":"%%time\nWOE_encoder = WOEEncoder()\ntrain_woe = WOE_encoder.fit_transform(train[feature_list], target)\ntest_woe = WOE_encoder.transform(test[feature_list])","3f686312":"%%time\nJSE_encoder = JamesSteinEncoder()\ntrain_jse = JSE_encoder.fit_transform(train[feature_list], target)\ntest_jse = JSE_encoder.transform(test[feature_list])","f2d5fec7":"%%time\nLOOE_encoder = LeaveOneOutEncoder()\ntrain_looe = LOOE_encoder.fit_transform(train[feature_list], target)\ntest_looe = LOOE_encoder.transform(test[feature_list])","bc3478dc":"%%time\nCBE_encoder = CatBoostEncoder()\ntrain_cbe = CBE_encoder.fit_transform(train[feature_list], target)\ntest_cbe = CBE_encoder.transform(test[feature_list])","f1cbd1c4":"%%time\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.linear_model import LogisticRegression\n\nencoder_list = [ OrdinalEncoder(), WOEEncoder(), TargetEncoder(), MEstimateEncoder(), JamesSteinEncoder(), LeaveOneOutEncoder() ,CatBoostEncoder()]\n\nX_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=97)\n\nfor encoder in encoder_list:\n    print(\"Test {} : \".format(str(encoder).split('(')[0]), end=\" \")\n    train_enc = encoder.fit_transform(X_train[feature_list], y_train)\n    #test_enc = encoder.transform(test[feature_list])\n    val_enc = encoder.transform(X_val[feature_list])\n    lr = LogisticRegression(C=0.1, solver=\"lbfgs\", max_iter=1000)\n    lr.fit(train_enc, y_train)\n    lr_pred = lr.predict_proba(val_enc)[:, 1]\n    score = auc(y_val, lr_pred)\n    print(\"score: \", score)\n    del train_enc\n    del val_enc\n    gc.collect()\n\n","ed51e81a":"%%time\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\n# CV function original : @Peter Hurford : Why Not Logistic Regression? https:\/\/www.kaggle.com\/peterhurford\/why-not-logistic-regression\n\ndef run_cv_model(train, test, target, model_fn, params={}, label='model'):\n    kf = KFold(n_splits=5)\n    fold_splits = kf.split(train, target)\n\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0]))\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('Started {} fold {}\/5'.format(label, i))\n        dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n        dev_y, val_y = target[dev_index], target[val_index]\n        pred_val_y, pred_test_y = model_fn(dev_X, dev_y, val_X, val_y, test, params)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        cv_score = auc(val_y, pred_val_y)\n        cv_scores.append(cv_score)\n        print(label + ' cv score {}: {}'.format(i, cv_score))\n        i += 1\n        \n    print('{} cv scores : {}'.format(label, cv_scores))\n    print('{} cv mean score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std score : {}'.format(label, np.std(cv_scores)))\n    pred_full_test = pred_full_test \/ 5.0\n    results = {'label': label, 'train': pred_train, 'test': pred_full_test, 'cv': cv_scores}\n    return results\n\n\ndef runLR(train_X, train_y, test_X, test_y, test_X2, params):\n    model = LogisticRegression(**params)\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)[:, 1]\n    pred_test_y2 = model.predict_proba(test_X2)[:, 1]\n    return pred_test_y, pred_test_y2\n","2f39eb6b":"if TEST:\n\n    lr_params = {'solver': 'lbfgs', 'C': 0.1}\n\n    results = list()\n\n    for encoder in  [ OrdinalEncoder(), WOEEncoder(), TargetEncoder(), MEstimateEncoder(), JamesSteinEncoder(), LeaveOneOutEncoder() ,CatBoostEncoder()]:\n        train_enc = encoder.fit_transform(train[feature_list], target)\n        test_enc = encoder.transform(test[feature_list])\n        result = run_cv_model(train_enc, test_enc, target, runLR, lr_params, str(encoder).split('(')[0])\n        results.append(result)\n    results = pd.DataFrame(results)\n    results['cv_mean'] = results['cv'].apply(lambda l : np.mean(l))\n    results['cv_std'] = results['cv'].apply(lambda l : np.std(l))\n    results[['label','cv_mean','cv_std']].head(8)","740001aa":"if TEST:\n    for idx, label in enumerate(results['label']):\n        sub_df = pd.DataFrame({'id': test_id, 'target' : results.iloc[idx]['test']})\n        sub_df.to_csv(\"LR_{}.csv\".format(label), index=False)\n\n","63b33dd0":"### notation\n\n- $y$ and $y+$ \u2014 the total number of observations and the total number of positive observations (y=1);\n- $x_i$, $y_i$ \u2014 the i-th value of category and target;\n- $n$ and $n+$ \u2014 the number of observations and the number of positive observations (y=1) for a given value of a categorical column;\n- $a$ \u2014 a regularization hyperparameter (selected by a user), prior \u2014 an average value of the target.","6d81170a":"## 2. One-Hot Encoder (OHE, dummy encoder)\n\n\nSo what can you do to give values \u200b\u200bby category instead of ordering them?\n\nIf you have data with specific category values, you can create a column. If the base Label Encoder label type is N, then OHE is the way to create N columns.\n\nSince only the row containing the content is given as 1, it is called one-hot encoding. Also called dummy encoding in the sense of creating a dummy.\n\n\nIn this competition:\n\n``` python\ntraintest = pd.concat([train, test])\ndummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\ntrain_ohe = dummies.iloc[:train.shape[0], :]\ntest_ohe = dummies.iloc[train.shape[0]:, :]\ntrain_ohe = train_ohe.sparse.to_coo().tocsr()\ntest_ohe = test_ohe.sparse.to_coo().tocsr()\n```\n\nIf you use `Category-Encoders` it will look like this code below.","cf1fd319":"## 11. Catboost Encoder\n\n**Catboost** is a recently created target-based categorical encoder. \n\nIt is intended to overcome target leakage problems inherent in LOO. \n\nIf you use `Category-Encoders` it will look like this code below.","5de739b2":"### Single LR","016c1105":"## 7. M-Estimate Encoder\n\n**M-Estimate Encoder** is a **simplified version of Target Encoder**. It has only one hyperparameter (Wrong Fomular but did good work?!)\n\n$$\\hat{x}^k = \\frac{n^+ + prior * m}{y^+ + m}$$\n\nThe higher value of m results into stronger shrinking. Recommended values for m is in the range of 1 to 100.\n\nIf you use `Category-Encoders` it will look like this code below.","c6cdc661":"## 5. Frequency Encoder\n\nThis method encodes by frequency.\n\nCreate a new feature with the number of categories from the training data.\n\nI will not proceed separately in this data.","125a9744":"### LR with CrossValidation","718724ec":"## Validation\n\nValidation proceeds with single lr and lr with cv.\n\n- I will add OneHotEncoder, etc later.\n- More Fold get better score (my experience)\n- you can try another solver and another parameter","4f906d29":"## 4. Helmert Encoder\n\n**Helmert Encoding** is a third commonly used type of categorical encoding for regression along with OHE and Sum Encoding. \n\nIt compares each level of a categorical variable to the mean of the subsequent levels. \n\nThis type of encoding can be useful in certain situations where levels of the categorical variable are ordered. (not this dataset)\n\nIf you use `Category-Encoders` it will look like this code below.","5e887083":"## 3. Sum Encoder (Deviation Encoder, Effect Encoder)\n\n**Sum Encoder** compares the mean of the dependent variable (target) for a given level of a categorical column to the overall mean of the target. \n\nSum Encoding is very similar to OHE and both of them are commonly used in Linear Regression (LR) types of models.\n\nIf you use `Category-Encoders` it will look like this code below.","861f2f12":"## Submit","7a34686b":"## 1. Label Encoder (LE), Ordinary Encoder(OE)\n\nOne of the most common encoding methods.\n\nAn encoding method that converts categorical data into numbers.\nThe code is very simple, and when you encode a specific column you can proceed as follows:\n\n``` python\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\n\ntrain[column_name] = label.fit_transform(train[column_name])\n```\n\nThe simple idea is to convert the same category to a number with the same value.\n\nSo the range of numbers maps from 0 to n-1 as labels.\n\nThe disadvantage is that the labels are ordered randomly (in the existing order of the data), which can add noise while assigning an unexpected order between labels. In other words, the data becomes ordinary (ordinal, ordered) data, which can lead to unintended consequences.\n\nIf you use `Category-Encoders` it will look like this code below.","63591880":"## Category-Encoders \n\nA set of scikit-learn-style transformers for encoding categorical variables into numeric by means of different techniques.","6989a95d":"## 10. Leave-one-out Encoder (LOO or LOOE)\n\n**Leave-one-out Encoding** is another example of target-based encoders.\n\nThis encoder calculate mean target of category k for observation j if observation j is removed from the dataset:\n\n$$\\hat{x}^k_i = \\frac{\\sum_{j \\neq i}(y_j * (x_j == k) ) - y_i }{\\sum_{j \\neq i} x_j == k}$$\n\nWhile encoding the test dataset, a category is replaced with the mean target of the category k in the train dataset:\n\n$$\\hat{x}^k = \\frac{\\sum y_j * (x_j == k)  }{\\sum x_j == k}$$\n\nIf you use `Category-Encoders` it will look like this code below.","548395ff":"## 6. Target Encoder\n\nThis is a work in progress for many kernels.\n\nThe encoded category values are calculated according to the following formulas:\n\n$$s = \\frac{1}{1+exp(-\\frac{n-mdl}{a})}$$\n\n$$\\hat{x}^k = prior * (1-s) + s * \\frac{n^{+}}{n}$$\n\n- mdl means **'min data in leaf'**\n- a means **'smooth parameter, power of regularization'**\n\nTarget Encoder is a powerful, but it has a huuuuuge disadvantage \n\n> **target leakage**: it uses information about the target. \n\nTo reduce the effect of target leakage, \n\n- Increase regularization\n- Add random noise to the representation of the category in train dataset (some sort of augmentation)\n- Use Double Validation (using other validation)\n\nLet's use while being careful about overfitting.\n\nIf you use `Category-Encoders` it will look like this code below.","085ea75a":"- UPDATED : error founded in libarary https:\/\/github.com\/scikit-learn-contrib\/category_encoders\/issues\/200\n\n$$\\hat{x}^k = \\frac{n^+ + prior * m}{n+ + m}$$\n\nThanks to [@ansh422](https:\/\/www.kaggle.com\/ansh422)","30db53c7":"## Introduction\n\nInspired by this article and the repo, I have created the following kernel:\n\n- [Benchmarking Categorical Encoders](https:\/\/towardsdatascience.com\/benchmarking-categorical-encoders-9c322bd77ee8)\n\n- [CategoricalEncodingBenchmark](https:\/\/github.com\/DenisVorotyntsev\/CategoricalEncodingBenchmark)\n\nLet's see how these methods work in this dataset.\n\n[Discussion](https:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/112584)\n\n- no feature preprocessing\n- Use KFold(5) for CV (+ more fold get better score)\n- LR (C=0.1, solver=lbfgs)\n\n|Encoder|LB Score|\n|-|-|\n|TE|0.78018|\n|WOE|0.78861|\n|LOOE|0.79382|\n|James-Stein|0.77843|\n|Catboost|0.79164|\n|One-Hot(another my kernel)|0.77973|\n\n\n\n### Category-Encoders\n\n1. Label Encoder\n2. One-Hot Encoder\n3. Sum Encoder\n4. Helmert Encoder\n5. Frequency Encoder\n6. Target Encoder\n7. M-Estimate Encoder\n8. Weight Of Evidence Encoder\n9. James-Stein Encoder\n10. Leave-one-out Encoder\n11. Catboost Encoder\n---\n- Validation (Benchmark)\n    - single LR\n    - LR with Cross Validation\n\n- Submit","a5803657":"read csv and doing some preprocessing","5e24f638":"## 8. Weight of Evidence Encoder \n\n**Weight Of Evidence** is a commonly used target-based encoder in credit scoring. \n\nIt is a measure of the \u201cstrength\u201d of a grouping for separating good and bad risk (default). \n\nIt is calculated from the basic odds ratio:\n\n``` python\na = Distribution of Good Credit Outcomes\nb = Distribution of Bad Credit Outcomes\nWoE = ln(a \/ b)\n```\n\nHowever, if we use formulas as is, it might lead to **target leakage**(and overfit).\n\nTo avoid that, regularization parameter a is induced and WoE is calculated in the following way:\n\n$$nomiinator = \\frac{n^+ + a}{y^+ + 2*a}$$\n\n$$denominator = ln(\\frac{nominator}{denominator})$$\n\nIf you use `Category-Encoders` it will look like this code below.","d477c943":"Even CVs did not solve the target based encoder's overfit problem.","a33cd385":"## 9. James-Stein Encoder\n\n**James-Stein Encoder** is a target-based encoder.\n\nThe idea behind James-Stein Encoder is simple. Estimation of the mean target for category k could be calculated according to the following formula:\n\n$$\\hat{x}^k = (1-B) * \\frac{n^+}{n} + B * \\frac{y^+}{y} $$\n\nOne way to select B is to tune it like a hyperparameter via cross-validation, but Charles Stein came up with another solution to the problem:\n\n$$B = \\frac{Var[y^k]}{Var[y^k] + Var[y]}$$\n\nSeems quite fair, but James-Stein Estimator has a big disadvantage \u2014 it is defined only for normal distribution (which is not the case for any classification task). \n\nTo avoid that, we can either convert binary targets with a log-odds ratio as it was done in WoE Encoder (which is used by default because it is simple) or use beta distribution.\n\nIf you use `Category-Encoders` it will look like this code below."}}