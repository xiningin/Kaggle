{"cell_type":{"b7e23d37":"code","384b744a":"code","e296cddc":"code","77b93ed6":"code","d7c535bd":"code","f0530f2d":"code","c739ecd0":"code","e2fd0ca8":"code","1c411161":"code","03bec17a":"code","d867887f":"code","b3c20d37":"code","dfabaad6":"code","3883b9f9":"code","ed0b54dd":"code","c3cb50aa":"code","1c6ae1d2":"code","922e8d27":"code","dcea6676":"code","16b794b5":"code","6d680a81":"code","f0aa09eb":"code","f75f4673":"markdown","c29d3cf5":"markdown","8f10c00f":"markdown","1a7eea02":"markdown","b1ad7cb0":"markdown","747f793c":"markdown","86d04a09":"markdown","6eb4c652":"markdown","87a6dddf":"markdown","601f8776":"markdown","f37eff2f":"markdown","70e44799":"markdown","b44c6759":"markdown","78c01579":"markdown","e68b0a03":"markdown","efa46ab7":"markdown","06d90ec2":"markdown","d4e572f8":"markdown","050e80c1":"markdown","e1fb77b0":"markdown","9ef608e3":"markdown"},"source":{"b7e23d37":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd \n\nimport os\n\nDATA_DIR = \"..\/input\"\nTEST_DIR = r'..\/input\/test'\n\ntrain_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), nrows=300000000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n\nprint(train_df.info())\n\n\nld = os.listdir(TEST_DIR)\nsizes = np.zeros(len(ld))\n\nfor i, f in enumerate(ld):\n    df = pd.read_csv(os.path.join(TEST_DIR, f))\n    sizes[i] = df.shape[0]\n\nprint(np.mean(sizes))\nprint(np.min(sizes))\nprint(np.max(sizes))\nprint('ok')","384b744a":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.svm import SVR, NuSVR\nfrom sklearn.kernel_ridge import KernelRidge\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nDATA_DIR = \"..\/input\"\nTEST_DIR = r'..\/input\/test'\n\nld = os.listdir(TEST_DIR)\nsizes = np.zeros(len(ld))\n\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy.stats import pearsonr\nfrom scipy import stats\nfrom sklearn.kernel_ridge import KernelRidge\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nfrom tsfresh.feature_extraction import feature_calculators\n\n%matplotlib inline\nsns.set_style('darkgrid')","e296cddc":"acoustic_data_sample = train_df['acoustic_data'].values[::50]\ntime_to_failure_sample = train_df['time_to_failure'].values[::50]\n\nfig, ax1 = plt.subplots(figsize = (12,8))\nplt.title('Data from DF')\nplt.plot(acoustic_data_sample, color='r')\nax1.set_ylabel('Acousting Data', color='r')\nplt.legend(['acoustic data'], loc=(0.01, 0.95))\nax2 = ax1.twinx()\nplt.plot(time_to_failure_sample, color='b')\nax2.set_ylabel('Time to Failure', color='b')\nplt.legend(['time_to_failure'], loc=(0.01, 0.95))\nplt.grid(True)\n\ndel acoustic_data_sample, time_to_failure_sample\ngc.collect()","77b93ed6":"np.random.seed(2018)\nrand_idxs = np.random.randint(0, 300000000-150000, size=9, dtype=np.int32)\nfig, axes = plt.subplots(3, 3, figsize=(18, 10))\n\nfor x in range(3):\n    for y in range(3):\n        ad = train_df['acoustic_data'].values[rand_idxs[x*3 + y]: rand_idxs[x*3 + y] + 150000]\n        ttf = train_df['time_to_failure'].values[rand_idxs[x*3 + y]: rand_idxs[x*3 + y] + 150000]\n\n        axes[x][y].plot(ad, color='blue')\n        axes[x][y].set_xticks([])\n\n        s = axes[x][y].twinx()\n        s.plot(ttf, color='red')\n        \nplt.tight_layout()\nplt.show()\ndel ad, ttf, rand_idxs\ngc.collect()","d7c535bd":"d = {'vals': train_df[train_df['acoustic_data']>2000].index.values}\npeaks = pd.DataFrame(d)\npeaks['diff'] = peaks['vals'].diff(periods=-1)\nselected_peaks = peaks[abs(peaks['diff'])>30000]['vals'].values\n\n\ntrain_df['diff'] = train_df['time_to_failure'].diff()\nindexes_of_eartgquakes = train_df[train_df['diff']>1].index.values\n\nprint('Number of earthquakes in loaded data: ', len(indexes_of_eartgquakes))\nprint('Number of peaks in loaded data: ',len(selected_peaks))\n\nprint(selected_peaks)\n\ndel peaks, d\ngc.collect()","f0530f2d":"fig, axes = plt.subplots(4, 2, figsize=(18, 10))\nfig.delaxes(axes[3,1])\n\nfor x in range(7):\n        ad = train_df['acoustic_data'].values[selected_peaks[x]-75000: selected_peaks[x]+75000]\n        ttf = train_df['time_to_failure'].values[selected_peaks[x]-75000: selected_peaks[x]+75000]\n\n        axes[int(x\/2)][x%2].plot(ad, color='blue')\n        axes[int(x\/2)][x%2].set_xticks([])\n\n        s = axes[int(x\/2)][x%2].twinx()\n        s.plot(ttf, color='red')\n        \nplt.tight_layout()\nplt.show()\ndel ad, ttf\ngc.collect()","c739ecd0":"fig, axes = plt.subplots(4, 2, figsize=(18, 10))\nfig.delaxes(axes[3,1])\n\nfor x in range(7):\n        ad = train_df['acoustic_data'].values[indexes_of_eartgquakes[x]-140000: indexes_of_eartgquakes[x]+10000]\n        ttf = train_df['time_to_failure'].values[indexes_of_eartgquakes[x]-140000: indexes_of_eartgquakes[x]+10000]\n\n        axes[int(x\/2)][x%2].plot(ad, color='blue')\n        axes[int(x\/2)][x%2].set_xticks([])\n\n        s = axes[int(x\/2)][x%2].twinx()\n        s.plot(ttf, color='red')\n        \nplt.tight_layout()\nplt.show()\ndel ad, ttf\ngc.collect()","e2fd0ca8":"def classic_sta_lta(x, length_sta, length_lta):\n    \n    sta = np.cumsum(x ** 2)\n\n    # Zamiana na float\n    sta = np.require(sta, dtype=np.float)\n\n    # Kopia dla LTA\n    lta = sta.copy()\n\n    # Obliczanie STA i LTA\n    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n    sta \/= length_sta\n    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n    lta \/= length_lta\n\n    # Uzupe\u0142nienie zerami\n    sta[:length_lta - 1] = 0\n\n    # Aby nie dzieli\u0107 przez 0 ustawiamy 0 na ma\u0142e liczby typu float\n    dtiny = np.finfo(0.0).tiny\n    idx = lta < dtiny\n    lta[idx] = dtiny\n\n    return sta \/ lta","1c411161":"def calc_change_rate(x):\n    change = (np.diff(x) \/ x[:-1]).values\n    change = change[np.nonzero(change)[0]]\n    change = change[~np.isnan(change)]\n    change = change[change != -np.inf]\n    change = change[change != np.inf]\n    return np.mean(change)","03bec17a":"percentiles = [1, 5, 10, 20, 25, 30, 40, 50, 60, 70, 75, 80, 90, 95, 99]\nhann_windows = [50, 150, 1500, 15000]\nspans = [300, 3000, 30000, 50000]\nwindows = [10, 50, 100, 500, 1000, 10000]\nborders = list(range(-4000, 4001, 1000))\npeaks = [10, 20, 50, 100]\ncoefs = [1, 5, 10, 50, 100]\nlags = [10, 100, 1000, 10000]\nautocorr_lags = [5, 10, 50, 100, 500, 1000, 5000, 10000]","d867887f":"def gen_features(x, zero_mean=False):\n    if zero_mean==True:\n        x = x-x.mean()\n    strain = {}\n    strain['mean'] = x.mean()\n    strain['std']=x.std()\n    strain['max']=x.max()\n    strain['kurtosis']=x.kurtosis()\n    strain['skew']=x.skew()\n    zc = np.fft.fft(x)\n    realFFT = np.real(zc)\n    imagFFT = np.imag(zc)\n    strain['min']=x.min()\n    strain['sum']=x.sum()\n    strain['mad']=x.mad()\n    strain['median']=x.median()\n    \n    strain['mean_change_abs'] = np.mean(np.diff(x))\n    strain['mean_change_rate'] = np.mean(np.nonzero((np.diff(x) \/ x[:-1]))[0])\n    strain['abs_max'] = np.abs(x).max()\n    strain['abs_min'] = np.abs(x).min()\n    \n    strain['avg_first_50000'] = x[:50000].mean()\n    strain['avg_last_50000'] = x[-50000:].mean()\n    strain['avg_first_10000'] = x[:10000].mean()\n    strain['avg_last_10000'] = x[-10000:].mean()\n    \n    strain['min_first_50000'] = x[:50000].min()\n    strain['min_last_50000'] = x[-50000:].min()\n    strain['min_first_10000'] = x[:10000].min()\n    strain['min_last_10000'] = x[-10000:].min()\n    \n    strain['max_first_50000'] = x[:50000].max()\n    strain['max_last_50000'] = x[-50000:].max()\n    strain['max_first_10000'] = x[:10000].max()\n    strain['max_last_10000'] = x[-10000:].max()\n    \n    strain['max_to_min'] = x.max() \/ np.abs(x.min())\n    strain['max_to_min_diff'] = x.max() - np.abs(x.min())\n    strain['count_big'] = len(x[np.abs(x) > 500])\n           \n    strain['mean_change_rate_first_50000'] = calc_change_rate(x[:50000])\n    strain['mean_change_rate_last_50000'] = calc_change_rate(x[-50000:])\n    strain['mean_change_rate_first_10000'] = calc_change_rate(x[:10000])\n    strain['mean_change_rate_last_10000'] = calc_change_rate(x[-10000:])\n    \n    strain['q95'] = np.quantile(x, 0.95)\n    strain['q99'] = np.quantile(x, 0.99)\n    strain['q05'] = np.quantile(x, 0.05)\n    strain['q01'] = np.quantile(x, 0.01)\n    \n    strain['abs_q95'] = np.quantile(np.abs(x), 0.95)\n    strain['abs_q99'] = np.quantile(np.abs(x), 0.99)\n    strain['abs_q05'] = np.quantile(np.abs(x), 0.05)\n    strain['abs_q01'] = np.quantile(np.abs(x), 0.01)\n    \n    for autocorr_lag in autocorr_lags:\n        strain['autocorrelation_' + str(autocorr_lag)] = feature_calculators.autocorrelation(x, autocorr_lag)\n    \n    # percentiles on original and absolute values\n    for p in percentiles:\n        strain['percentile_'+str(p)] = np.percentile(x, p)\n        strain['abs_percentile_'+str(p)] = np.percentile(np.abs(x), p)\n    \n#     strain['trend'] = add_trend_feature(x)\n#     strain['abs_trend'] = add_trend_feature(x, abs_values=True)\n    strain['abs_mean'] = np.abs(x).mean()\n    strain['abs_std'] = np.abs(x).std()\n    \n    strain['quantile_0.95']=np.quantile(x, 0.95)\n    strain['quantile_0.99']=np.quantile(x, 0.99)\n    strain['quantile_0.05']=np.quantile(x, 0.05)\n    strain['realFFT_mean']=realFFT.mean()\n    strain['realFFT_std']=realFFT.std()\n    strain['realFFT_max']=realFFT.max()\n    strain['realFFT_min']=realFFT.min()\n    strain['imagFFT_mean']=imagFFT.mean()\n    strain['imagFFT_std']=realFFT.std()\n    strain['imagFFT_max']=realFFT.max()\n    strain['imaglFFT_min']=realFFT.min()\n    \n    strain['std_first_50000']=x[:50000].std()\n    strain['std_last_50000']=x[-50000:].std()\n    strain['std_first_25000']=x[:25000].std()\n    strain['std_last_25000']=x[-25000:].std()\n    strain['std_first_10000']=x[:10000].std()\n    strain['std_last_10000']=x[-10000:].std()\n    strain['std_first_5000']=x[:5000].std()\n    strain['std_last_5000']=x[-5000:].std()\n        \n    strain['Hilbert_mean'] = np.abs(hilbert(x)).mean()\n    strain['Hann_window_mean'] = (convolve(x, hann(150), mode='same') \/ sum(hann(150))).mean()\n    strain['classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n    strain['classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n    strain['classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n    strain['classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n    #strain['classic_sta_lta5_mean'] = classic_sta_lta(x, 50, 1000).mean() contains inf and Nan values\n    strain['classic_sta_lta6_mean'] = classic_sta_lta(x, 100, 5000).mean()\n    #strain['classic_sta_lta7_mean'] = classic_sta_lta(x, 333, 666).mean() contains inf and Nan values\n    strain['classic_sta_lta8_mean'] = classic_sta_lta(x, 4000, 10000).mean()\n    strain['Moving_average_700_mean'] = x.rolling(window=700).mean().mean(skipna=True)\n    moving_average_700_mean = x.rolling(window=700).mean().mean(skipna=True)\n    ewma = pd.Series.ewm\n    strain['exp_Moving_average_300_mean'] = (ewma(x, span=300).mean()).mean(skipna=True)\n    strain['exp_Moving_average_3000_mean'] = ewma(x, span=3000).mean().mean(skipna=True)\n    strain['exp_Moving_average_30000_mean'] = ewma(x, span=30000).mean().mean(skipna=True)\n    no_of_std = 3\n    strain['MA_700MA_std_mean'] = x.rolling(window=700).std().mean()\n    strain['MA_1000MA_std_mean'] = x.rolling(window=1000).std().mean()\n    \n    strain['iqr'] = np.subtract(*np.percentile(x, [75, 25]))\n    strain['q999'] = np.quantile(x,0.999)\n    strain['q001'] = np.quantile(x,0.001)\n    strain['ave10'] = stats.trim_mean(x, 0.1)\n        \n    for window in windows:\n        x_roll_std = x.rolling(window).std().dropna().values\n        x_roll_mean = x.rolling(window).mean().dropna().values\n        \n        strain['ave_roll_std_' + str(window)] = x_roll_std.mean()\n        strain['std_roll_std_' + str(window)] = x_roll_std.std()\n        strain['max_roll_std_' + str(window)] = x_roll_std.max()\n        strain['min_roll_std_' + str(window)] = x_roll_std.min()\n        strain['q01_roll_std_' + str(window)] = np.quantile(x_roll_std, 0.01)\n        strain['q05_roll_std_' + str(window)] = np.quantile(x_roll_std, 0.05)\n        strain['q95_roll_std_' + str(window)] = np.quantile(x_roll_std, 0.95)\n        strain['q99_roll_std_' + str(window)] = np.quantile(x_roll_std, 0.99)\n        strain['av_change_abs_roll_std_' + str(window)] = np.mean(np.diff(x_roll_std))\n        strain['av_change_rate_roll_std_' + str(window)] = np.mean(np.nonzero((np.diff(x_roll_std) \/ x_roll_std[:-1]))[0])\n        strain['abs_max_roll_std_' + str(window)] = np.abs(x_roll_std).max()\n        \n        for p in percentiles:\n            strain['percentile_roll_std_' + str(p) + '_window_' + str(window)] = np.percentile(x_roll_std, p)\n            strain['percentile_roll_mean_' + str(p) + '_window_' + str(window)] = np.percentile(x_roll_mean, p)\n        \n        strain['ave_roll_mean_' + str(window)] = x_roll_mean.mean()\n        strain['std_roll_mean_' + str(window)] = x_roll_mean.std()\n        strain['max_roll_mean_' + str(window)] = x_roll_mean.max()\n        strain['min_roll_mean_' + str(window)] = x_roll_mean.min()\n        strain['q01_roll_mean_' + str(window)] = np.quantile(x_roll_mean, 0.01)\n        strain['q05_roll_mean_' + str(window)] = np.quantile(x_roll_mean, 0.05)\n        strain['q95_roll_mean_' + str(window)] = np.quantile(x_roll_mean, 0.95)\n        strain['q99_roll_mean_' + str(window)] = np.quantile(x_roll_mean, 0.99)\n        strain['av_change_abs_roll_mean_' + str(window)] = np.mean(np.diff(x_roll_mean))\n        strain['av_change_rate_roll_mean_' + str(window)] = np.mean(np.nonzero((np.diff(x_roll_mean) \/ x_roll_mean[:-1]))[0])\n        strain['abs_max_roll_mean_' + str(window)] = np.abs(x_roll_mean).max()\n        \n        \n    return pd.Series(strain)","b3c20d37":"del train_df\ngc.collect()\n\ntrain_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), iterator=True, chunksize=150_000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\nX_train = pd.DataFrame()\nX_train_zero_mean = pd.DataFrame()\ny_train = pd.Series()\n\nfor df in train_df:\n    features = gen_features(df['acoustic_data'])\n    ch_zero_mean = gen_features(df['acoustic_data'], zero_mean=True)\n    X_train = X_train.append(features, ignore_index=True)\n    X_train_zero_mean = X_train_zero_mean.append(ch_zero_mean, ignore_index=True)\n    y_train = y_train.append(pd.Series(df['time_to_failure'].values[-1]), ignore_index=True)\n\nX_train.head()","dfabaad6":"X_test = pd.DataFrame()\nX_test_zero_mean = pd.DataFrame()\n\nfor i, f in enumerate(ld):\n    df = pd.read_csv(os.path.join(TEST_DIR, f))\n    features = gen_features(df['acoustic_data'])\n    ch_zero_mean = gen_features(df['acoustic_data'], zero_mean=True)\n    X_test = X_test.append(features, ignore_index=True)\n    X_test_zero_mean = X_test_zero_mean.append(ch_zero_mean, ignore_index=True)","3883b9f9":"def plot_acc_agg_ttf_data(features, title=\"Averaged accoustic data and ttf\"):\n    fig, axes = plt.subplots(3,3, figsize=(30, 18))\n    \n    for i in range(9):\n        plt.title('Averaged accoustic data ({}) and time to failure'.format(features[i]))\n        axes[int(i\/3)][i%3].plot(X_train[features[i]], color='r')\n        axes[int(i\/3)][i%3].set_xlabel('training samples')\n        axes[int(i\/3)][i%3].set_ylabel('acoustic data ({})'.format(features[i]), color='r')\n        plt.legend(['acoustic data ({})'.format(features[i])], loc=(0.01, 0.95))\n        ax2 = axes[int(i\/3)][i%3].twinx()\n        ax2.plot(y_train, color='b')\n        ax2.set_ylabel('time to failure', color='b')\n        plt.legend(['time to failure'], loc=(0.01, 0.9))\n        plt.grid(True)","ed0b54dd":"def plot_distplot_features(features, nlines=4, colors=['green', 'blue'], df1=X_train, df2=X_test):\n    plt.figure()\n    fig, ax = plt.subplots(nlines,2,figsize=(16,4*nlines))\n    for i in range(len(features)):\n        plt.subplot(nlines,2,i+1)\n        plt.hist(df1[features[i]],color=colors[0],bins=50, label='train', alpha=0.5)\n        plt.hist(df2[features[i]],color=colors[1],bins=50, label='test', alpha=0.5)\n        plt.legend()\n        plt.title(features[i])\n    plt.show()","c3cb50aa":"features = ['mean', 'std', 'max', 'min', 'sum', 'mad', 'kurtosis', 'skew']\nplot_distplot_features(features)","1c6ae1d2":"features = ['mean', 'std', 'max', 'min', 'sum', 'mad', 'kurtosis', 'skew']\nplot_distplot_features(features, df1=X_train_zero_mean, df2=X_test_zero_mean)","922e8d27":"features = ['std_first_50000', 'std_last_50000', 'std_first_25000','std_last_25000', 'std_first_10000','std_last_10000']\nplot_distplot_features(features,3)","dcea6676":"features = ['std_first_50000', 'std_last_50000', 'std_first_25000','std_last_25000', 'std_first_10000','std_last_10000']\nplot_distplot_features(features,3, df1=X_train_zero_mean, df2=X_test_zero_mean)","16b794b5":"all_features = X_train.columns.values\nnp.random.seed(2019)\nrand_feat_idx = np.random.randint(0, len(all_features), size=9, dtype=np.int32)\nrand_labels = [all_features[x] for x in rand_feat_idx]\n\nplot_acc_agg_ttf_data(rand_labels)","6d680a81":"corelations = np.abs(X_train.corrwith(y_train)).sort_values(ascending=False)\ncorelations_df = pd.DataFrame(data=corelations, columns=['corr'])\nprint(\"Number of high corelated values: \",corelations_df[corelations_df['corr']>=0.3]['corr'].count())\n\nhigh_corr = corelations_df[corelations_df['corr']>=0.3]\nprint(high_corr)\nhigh_corr_labels = high_corr.reset_index()['index'].values\nprint(high_corr_labels)","f0aa09eb":"plot_acc_agg_ttf_data(high_corr_labels[:9])","f75f4673":"Na tym zako\u0144czymy proces Analizy i eksploracji danych\n\nW dalszej cz\u0119\u015bci projektu przejdziemy do dalszej cze\u015bci selekcji oraz przygotowania danych do modelu regresji oraz wyboru samego modelu i jego parametr\u00f3w","c29d3cf5":"### Dalej przeszli\u015bmy do analizy wygenerowanych feature'\u00f3w w kontek\u015bcie tworzenia modelu regresji","8f10c00f":"Algorytm STA\/LTA(Short Time Average over Long Time Average) zaprojektowany jest w celu ignorowania sygna\u0142\u00f3w energetycznych z pomiar\u00f3w wibracji otoczenia. Wybiera on tylko te cz\u0119\u015bci sygna\u0142\u00f3w, kt\u00f3rych amplituda jest pseudo-stacjonarna. Algorytm zmierza do unikni\u0119cia wybuch\u00f3w energii. Poni\u017cej znajduje si\u0119 jego implementacja.","1a7eea02":"Na poni\u017cszym wykresie wzi\u0119li\u015bmy losowo 9 featur'\u00f3w i nast\u0119pni przedstawili\u015bmy ich warto\u015bci w por\u00f3wnaniu ze zmienn\u0105 wyj\u015bciow\u0105 'time_to_failure'\n\nJak wida\u0107 przy niekt\u00f3rych zmiennych mo\u017cemy zauwa\u017cy\u0107podobie\u0144stwo, mianowicie nast\u0119puje gwa\u0142towny wzrost lub spadek warto\u015bci zmiennej razem ze wzrostem warto\u015bci zmiennej wyj\u015bciowej \"time_to_failure\". ","b1ad7cb0":"Postanowili\u015bmy wi\u0119c zbada\u0107 korelacje mi\u0119dzy wszystkimi feature'ami a warto\u015bci\u0105 zmiennej wyj\u015bciowej\n\nW tym kroku postanowili\u015bmy wyznaczy\u0107 poszczeg\u00f3lne korelacje pomi\u0119dzy cechami tak, aby zobaczy\u0107, kt\u00f3re maj\u0105 najlepsze wyniki. Pod uwag\u0119 brali\u015bmy korelacje o warto\u015bci bezwzgl\u0119dnej wi\u0119kszej lub r\u00f3wnej 0.3. Jak wida\u0107 w wyniku poni\u017cszego kawa\u0142ku kodu takich korelacji mamy: X. Najlepsze z nich maj\u0105 warto\u015bci powy\u017cej 0.6, to na nich powinni\u015bmy skupi\u0107 swoj\u0105 najwi\u0119ksz\u0105 uwag\u0119 i to one powinny mie\u0107 najwi\u0119kszy wp\u0142yw na nasz model.","747f793c":"W tym miejscu generowali\u015bmy cechy(features) dla ka\u017cdego pomiaru danych akustycznych znajduj\u0105cych si\u0119 w zbiorze testowym. Dla ka\u017cdego segmentu danych obliczane by\u0142y zdefiniowane cechy oraz wyniki przypisywane do struktury X_test.","86d04a09":"Poni\u017cej zebrali\u015bmy wszystkie potrzebne i wykorzystywane biblioteki oraz frameworki, kt\u00f3re u\u0142atwia\u0142y nam prac\u0119 w obliczeniach, rysowaniu wykres\u00f3w, klasyfikacji, regresji, eksploracji i analizie danych, predykcji.\n\nBiblioteki Seaborn oraz Matplotlib wykorzystali\u015bmy do graficznej wizualizacji wykres\u00f3w.\nBiblioteka Catboost znalaz\u0142a zastosowanie w naszym projekcie w zadaniach zwi\u0105zanych z regresj\u0105, uczeniem maszynowym.\nNajszerszym zakresem zastosowa\u0144 okaza\u0142y si\u0119 bilioteki sklearn, numpy i seaborn do uczenia maszynowego, wizualizacji i operacji matematycznych, kt\u00f3re zapewni\u0142a \u015bwietne narz\u0119dzia do eksploracji i analizy danych.","6eb4c652":"Pocz\u0105tkowo rysowali\u015bmy wykresy z po\u0142owy zbioru treningowego, a w tym konkretnym przypadku poni\u017cej brali\u015bmy co 50 dan\u0105 z zbioru treningowego. Na czerwono zaznaczone zosta\u0142y dane akustyczne(acoustic_data), a na niebiesko czas(time_to_failure). Pomimo tego wci\u0105\u017c mogli\u015bmy zauwa\u017cy\u0107 pewne zale\u017cno\u015bci. Na wykresie poni\u017cej przedstawili\u015bmy zale\u017cno\u015b\u0107 danych akustycznych(acoustic_data) od czasu trz\u0119sienia(time_to_failure). S\u0105 to dane w czystej postaci - jeszcze niemodyfikowane na tym etapie. Jak wida\u0107 przy ka\u017cdym szczycie(peak) danych akustycznych widzimy momentalny zryw(peak) czasu od warto\u015bci niemal\u017ce zerowych do g\u00f3ry, kt\u00f3ry symbolizuje trz\u0119sienie ziemi, a nast\u0119pnie liniow\u0105 zmian\u0119 czasu biegn\u0105c\u0105 ponownie do zera. \n\nTak jak wida\u0107 na poni\u017cszym obrazku mo\u017cna znale\u017a\u0107 7 takich sytuacji, kt\u00f3re odpowiadaj\u0105 7 trz\u0119sieniom w tej konkretnej wybranej cz\u0119\u015bci zbioru treningowego. Co najistotniejsze w tym wykresie to fakt, \u017ce szczyt warto\u015bci sygna\u0142u wyst\u0119puje przed prawie ka\u017cdym wyst\u0105pieniem trz\u0119sienia. Z tego faktu na pewno warto b\u0119dzie zwr\u00f3ci\u0107 uwag\u0119 na wykresy przedstawiaj\u0105ce warto\u015bci maksymalne oraz odchylenie standardowe.\n\nWarto wspomnie\u0107, \u017ce w ca\u0142ym zbiorze treningowych samych trz\u0119sie\u0144 jest tylko 16, w stosunku do wszystkich pomiar\u00f3w jest to warto\u015bc poni\u017cej 1% !","87a6dddf":"Funkcja odpowiedzialna za rysowanie wykresu przedstawiaj\u0105cego u\u015brednione dane akustyczne wraz z czasem do trz\u0119sienia dla cech przekazanych w argumentach funkcji. Przy odpowiednim wyborze cech, jeste\u015bmy w stanie po narysowaniu wykres\u00f3w stwierdzi\u0107 czy s\u0105 one przydatne czy nie. W kolejnych krokach b\u0119dziemy mogli zauwa\u017cy\u0107 co zwraca wywo\u0142anie funkcji dla poszczeg\u00f3lnych cech, dzi\u0119ki czemu \u0142atwiej b\u0119dzie okre\u015bli\u0107 ich wa\u017cno\u015b\u0107.","601f8776":"Tak jak wspomnieli\u015bmy wy\u017cej na wykresie widoczne jest 7 trz\u0119sie\u0144 z wszystkich 16, kt\u00f3re wyst\u0119puj\u0105 w pe\u0142nym zbiorze danych. Za\u0142o\u017cyli\u015bmy, \u017ce praca na danych, w kt\u00f3ry wyst\u0119puje prawie po\u0142owa trz\u0119sie\u0144 mo\u017ce przynie\u015b\u0107 podobne rezultaty, a b\u0119dzie na pewno szybsza w por\u00f3wnaniu do pracy na pe\u0142ynm zbiorze. Analizuj\u0105c tylko cz\u0119\u015b\u0107 zbioru mo\u017cemy na pewno natrafi\u0107 p\u00f3\u017aniej na problemy zwi\u0105zane z faktem, \u017ce po ka\u017cdym skoku warto\u015bci danych akustycznych wyst\u0119puje trz\u0119sienie. Rozwa\u017cyli\u015bmy dodanie cechy(feature), kt\u00f3ra zawiera\u0142aby warto\u015b\u0107 czasow\u0105 od momentu nast\u0105pienia szczytu(peak) warto\u015bci akustycznej, a faktycznego trz\u0119sienia. Czasy dla wszystkich by\u0142yby bardzo zblizone i analizuj\u0105c kolejne wykresy wynosi\u0142yby \u015brednio 0.31 ms.","f37eff2f":"Jak wida\u0107 na powy\u017cszym wykresie nasze przypuszczenia okaza\u0142y si\u0119 s\u0142uszne i feature'y w kt\u00f3rych wyst\u0119puj\u0119 gwa\u0142towny wzrost lub spadek warto\u015bci posiadaj\u0105 wysok\u0105 korelacje ze zmienn\u0105 wyj\u015bciow\u0105","70e44799":"Po wyznaczeniu interesuj\u0105cych nas indeks\u00f3w, narysowali\u015bmy wykresy tych warto\u015bci w pr\u00f3bce 150000, aby dok\u0142adniej przyjrze\u0107 si\u0119 znalezionym szczytom. Wspominany ju\u017c wielokrotnie szczyt sygna\u0142u tu\u017c przed trz\u0119sieniem powtarza si\u0119 dosy\u0107 cz\u0119sto. Jest to na pewno wa\u017cna i przydatna infmoracja, je\u015bli jednak we\u017amiemy pod uwag\u0119, \u017ce takich sytuacji w ca\u0142ym zbiorze podzielonym na kawa\u0142ki z pr\u00f3bkami licz\u0105cymi 150000 jest jedynie 16, mo\u017ce okaza\u0107 si\u0119, \u017ce ci\u0119\u017cko b\u0119dzie j\u0105 wykorzysta\u0107 w odpowiedni spos\u00f3b na reszcie danych.","b44c6759":"W przypadku odchylenia standadowego dla r\u00f3\u017cnych cz\u0119\u015bci danych sytuacja wygl\u0105da tak jak na poni\u017cszym wykresie. Warto\u015bci dla danych treningowych odbiegaj\u0105 znacznie od danych testowych. S\u0105 to r\u00f3\u017cnice wynosz\u0105ce oko\u0142o 1500-2500.","78c01579":"Jak wida\u0107 na poni\u017cszym wykresie warto\u015bci przekazanych cech policzonych dla obu zbior\u00f3w odbiegaj\u0105 od siebie. Najbardziej widoczne jest to w przypadku \u015bredniej oraz sumy. Na reszczie wykres\u00f3w rozbie\u017cno\u015b\u0107 jest mo\u017ce troszk\u0119 mniejsza lecz wci\u0105\u017c zauwa\u017calna.","e68b0a03":"W tym miejscu wyznaczyli\u015bmy sobie jaka jest liczba trz\u0119sie\u0144 ziemi w danych, na kt\u00f3rych pracujemy oraz liczba szczyt\u00f3w(peak) w tych\u017ce danych. Liczby sobie odpowiada\u0142y tak\u017ce wyznaczyli\u015bmy indeksy tych szczyt\u00f3w.","efa46ab7":"W naszym pocz\u0105tkowym najprostszym modelu, cechy jak wyznaczyli\u015bmy g\u0142\u00f3wnie opiera\u0142y si\u0119 na podstawowych miarach tendecji centralnej. W wersji rozszerzonej pozostawili\u015bmy wcze\u015bniejsze cechy(feature) oraz dodali\u015bmy nowe jak wida\u0107 poni\u017cej. W sk\u0142ad nowo dodanych cech zaliczaj\u0105 si\u0119 takie rzeczy jak: dyskretna transformata Fouriera, \u015brednie odchylenie bezwzgl\u0119dne, warto\u015b\u0107 bezwzgl\u0119dna r\u00f3\u017cnicy \u015bredniej, wsp\u00f3\u0142czynnik zmienno\u015bci \u015bredniej, warto\u015bci minimalne, maksymalne, \u015brednie, minimum, maksimum pierwszych, ostatnich pi\u0119\u0107dziesi\u0119ciu tysi\u0119cy i dziesi\u0119ciu tysi\u0119cy. R\u00f3\u017cnice mi\u0119dzy warto\u015bciami minimalnymi i maksymalnymi, wsp\u00f3\u0142czynnik zmienno\u015bci \u015bredniej dla r\u00f3\u017cnych cz\u0119\u015bci zbioru. Dodatkowo u\u017cylismy kwantyli, odchylenie standardowe, dzia\u0142ania na cz\u0119\u015bci rzeczywistej i urojonej szybkiej transformaty Fouriera. Funkcji do przekszta\u0142cania sygna\u0142\u00f3w; przekszta\u0142cenie Hilberta, funkcji Hann, algorytmu STA\/LTA. Zastosowali\u015bmy r\u00f3wnie\u017c wyk\u0142adniczo wa\u017con\u0105 \u015bredni\u0105 ruchom\u0105. Przeprowadzali\u015bmy regresje na wybranych podpr\u00f3bkach. Wykorzystali\u015bmy funkcj\u0119 przeprowadzaj\u0105c\u0105 tzw. Rolling Window Calculation dla wybranych rozmiar\u00f3w okna(moving window). \n\nPrzy tworzeniu feature'\u00f3w korzystali\u015bmy z dyskusji na kaggle, oraz z poni\u017cszych kerneli oraz w\u0142asnej wiedzy:\n- https:\/\/www.kaggle.com\/artgor\/even-more-features\n- https:\/\/www.kaggle.com\/artgor\/earthquakes-fe-more-features-and-samples?scriptVersionId=9803210\n- https:\/\/www.kaggle.com\/abhishek\/quite-a-few-features-1-51\n- https:\/\/www.kaggle.com\/artgor\/earthquakes-fe-more-features-and-samples\n- https:\/\/www.kaggle.com\/vettejeep\/masters-final-project-model-lb-1-392","06d90ec2":"Funkcja odpowiedzialna za rysowanie wykresu przedstawiaj\u0105cego histogramy wybranych cech dla zbioru treningowego i testowego, kt\u00f3ra przydatna jest w stwierdzeniu czy wyniki cech ze zbior\u00f3w danych s\u0105 do siebie zbli\u017cone czy jednak si\u0119 od siebie r\u00f3\u017cni\u0105. W kolejnych krokach wywo\u0142anie funkcji przedstawi nam, \u017ce warto\u015bci cech zbior\u00f3w bardzo od siebie odbiegaj\u0105, a histogramy prawie nie nachodz\u0105 na siebie. Z tego te\u017c powodu b\u0119dziemy stara\u0107 si\u0119 zmniejszy\u0107 t\u0105 rozbie\u017cno\u015b\u0107 i w razie potrzeby przeskalowa\u0107 warto\u015bci pomiar\u00f3w.","d4e572f8":"#### W dalszej cz\u0119\u015bci analiz przystapimy do wyznaczania feature'\u00f3w do naszego przysz\u0142ego modelu","050e80c1":"Poni\u017cej wykresy przedstawiaj\u0105ce dane treningowe w dziedzine czasu, na kt\u00f3rych wida\u0107 momenty tr\u0119sie\u0144 ziemi, z kt\u00f3rych niestety ci\u0119zko jest cokolwiek wywnioskowa\u0107.","e1fb77b0":"Nast\u0119pnie wygenerowali\u015bmy losowe ziarno oraz tablice losowych warto\u015bci z przedzia\u0142u odpowiadaj\u0105cemu po\u0142owie zbioru treningowego. Rysowali\u015bmy wykres z krokiem co 150000, poniewa\u017c tyle wynosi\u0142a ilo\u015b\u0107 pr\u00f3bek w ka\u017cdym z pomiar\u00f3w. Je\u015bli przyjrze\u0107 si\u0119 9 poni\u017cszym wykresom mo\u017cemy zauwa\u017cy\u0107, \u017ce wyniki danych akustycznych w wi\u0119kszo\u015bci przypadk\u00f3w z zakresu (-200;300). Jednak jak wida\u0107 wyst\u0105pi\u0142y te\u017c pomiary z wynikami o wiele wi\u0119kszymi np.: wynik w szczycie ko\u0142o 1500 oraz oko\u0142o 4000.","9ef608e3":"Projekt rozpoczeli\u015bmy od podstawowych czynno\u015bci takich jak zdefiniowanie \u015bcie\u017cek do wczytywania plik\u00f3w, podstawowe importy takich bibliotek jak numpy oraz pandas. Pocz\u0105tkowo wczytali\u015bmy tylko 1\/2 danych ze zbioru treningowego z powodu sporych rozmiar\u00f3w jak i problem\u00f3w wydajno\u015bciowych. Rozmiar zbioru treningowego przekracza 10GB, co uniemo\u017cliwia\u0142o normaln\u0105 prac\u0119 na pliku. Po\u0142owa zbioru jest wystarczaj\u0105ca do zaprezentowania podstawowych miar tendecji centralnych, z kt\u00f3rych p\u00f3\u017aniej byli\u015bmy w stanie wyci\u0105gn\u0105\u0107 odpowiednie wnioski. Dodatkowo wczytali\u015bmy dane ze zbioru testowego oraz sprawdzili\u015bmy ile pr\u00f3bek zawiera\u0142 ka\u017cdy segment danych testowych. Ka\u017cdy z plik\u00f3w zawiera\u0142 150000 pr\u00f3bek. Warto r\u00f3wnie\u017c wspomnie\u0107, \u017ce dane treningowe oraz testowe, kt\u00f3re zosta\u0142y udost\u0119pnione na potrzeby konkursu bardzo od siebie odbiegaj\u0105, gdy\u017c pomiary dla danych testowych nie s\u0105 bezpo\u015bredni\u0105 kontynuacj\u0105 pomiaru dla danych treningowych. \n\nWyniki eksploracji przedstawili\u015bmy w kolejnych etapach prac na wykresach oraz tabelach"}}