{"cell_type":{"07e1645e":"code","e14db870":"code","66e00441":"code","b4e2d5b4":"code","b1db251d":"code","7b4a96c4":"code","3d80392b":"code","2e4018d1":"code","0588a709":"code","f39e8a33":"code","d6272be2":"markdown","0c0588a6":"markdown","fc32bd45":"markdown","c9c28b9b":"markdown","4da002b0":"markdown","07264c71":"markdown","e7f7077f":"markdown"},"source":{"07e1645e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))","e14db870":"train = pd.read_csv('..\/input\/train.csv')\ntest  = pd.read_csv('..\/input\/test.csv')\n\ndel train['ID'], test['ID'], train['target']\nall_data = train.append(test)\nall_data.reset_index(drop=True, inplace=True)\n\ncols_with_onlyone_val = train.columns[train.nunique() == 1]\n\n# Uncomment for fun:\n#all_data.drop(cols_with_onlyone_val, axis=1, inplace=True)  ","66e00441":"# GOAL: If user's col=min, and that min value only appears once in the user,\n# then for certain, that col is NOT an aggregate\ndef notagg(row):\n    row_nz = row[row>0]\n    if row_nz.shape[0] == 0: return row # row is all 0s, so we return false=0 that it's not an agg row\n    \n    min_nz = row_nz.min()\n    check  = (row_nz==min_nz).sum()\n    \n    # Min value occurs more than once, we can't learn anything about this column (min val column);\n    # as such, we can't learn anything about this row\n    if check>1:\n        row = 0\n        return row\n    \n    # Otherwise, min-val only occurs once! That col is NOT an aggregate\n    return (row==min_nz).astype(np.int)  # only min-col will be marked=1","b4e2d5b4":"# Apply the above function to all rows:\ncols_not_agg = all_data.apply(notagg, axis=1)\ncols_not_agg.shape","b1db251d":"# Cool, now look at each column and see if that column ever gets disqualified\ncols_not_agg = cols_not_agg.max(axis=0)\ncols_not_agg.shape # Make sure we're looking @ columns","7b4a96c4":"cols_not_agg.sum()","3d80392b":"which = cols_not_agg[cols_not_agg==0].index.tolist()\nwhich","2e4018d1":"# Start with the easiest canidates. Let's see which of these columns has the least number of non-0 values\ncheck = train[which]\ncheck = check>0\npd.concat([check.sum(axis=0).sort_values(), 100 * check.sum(axis=0).sort_values() \/ train.shape[0]], axis=1)","0588a709":"nz = (all_data>0).sum(axis=0)\nnz = nz.sort_values(ascending=False)\nnz.shape","f39e8a33":"plt.plot(nz.values\/all_data.shape[0])\nplt.show()","d6272be2":"The next question is why are there so many zeros in the dataset? My hypothesis here is that:\n\n1. In support of the assertion that there are NO aggregates in the dataset, and\n2. In support of other people's finding that the order of columns does **not** matter\n\nIt stands to reason that for each \"day\" they created a bunch of buckets so that they could hold multiple transactions per day. If the dataset were a json object, or nested arrays, then I believe they'd actually have a different number of features per user; but since it's provided to us as a .csv, this is a result of that.\n\nOne issue with the above is that if that were true, we would expect to see a core group of columns, for example the columns that represent the first few transactions of every day or period always contain non-zero items. That does't seem to be the case when we plot the non-zero count of each columns:","0c0588a6":"Pitiful. These columns have very little representation in the data. Column 0 is raw count, and Column 1 is percentage. Only the bottom four features appear with > 1% non-zeros in our train data. It doesn't even make sense to consider features that only appear 19 or less times in our dataset with non-0 values as a potential aggregate, because we really won't be getting any LB-juice out of that. And even if we were to go ahead and assume that the bottom four features were actual aggregate candidates, it wouldn't make sense to have just 4 aggregates out of 4991 columns... and on top of that have those aggregates == 0 +97.5% of the time.\n\nMy conclusion here is that this dataset does **not contain any aggregates at all**.","fc32bd45":"Okay. Before we break out the combinatorics of each column, let's see how popular these columns are. How many users from the train set have non-0 values in them?","c9c28b9b":"At max, we're only seeing non-zeros at 17.5% of a column. Not sure where to go next. Any ideas \ud83e\udd14?","4da002b0":"So it looks like we've disqualified 4991 - 4975 = 16, all but sixteen columns are for certain NOT aggregates. If you drop columns with 0 STD `cols_with_onlyone_val`, then this number further drops to 12 columns. Let's take a peek at them:","07264c71":"Like everyone else, I am interested in discerning what these anonymous features really mean. Since it's financial data, one would assume they would provide us:\n\n* Individual transaction amounts\n* Aggregated (summed) transactions per day\n* Aggregated transactions per week \/ month \/ year\n* Diffrent types of financial transactions, such as amount in account1, account2, or loan amounts, savings account amounts, etc.\n\nExamining the data and especially in light of Giba's comment, we know that our target value is frequently in our dataset. Moreover, just by virtue of the sheer number of columns, I believe it reasonable to assume that at minimal, we're looking at individual transactions. If that is indeed the case, is there a way we can for certain discern if there are aggregate columns present in the dataset? Or conversly, prove for certain that there are not? Also why are there so many 0s in the dataset? Stay tuned...","e7f7077f":"My thought process is as follows. We need to examine each user's columns, searching for the minimal, non-zero value present. If we find such a number, and this number is **not** repeated anywhere in the users columns (that is, it only appears once), then we can for certain lay claim that that specific column \/ feature is **not** an aggregate. If it were, we would expect to see that same, minimal feature appear at least one other time as some other column, that would be summed into the row in question.\n\nLet's code that up:"}}