{"cell_type":{"534da82e":"code","364e2d89":"code","57bb76f2":"code","42311b48":"code","50febf10":"code","d2be7f11":"code","b8950613":"code","326c37f4":"code","99f8bab5":"code","60e10b33":"code","00f4e64e":"code","c6d5954d":"code","554c08bf":"code","dad5ca1d":"code","9811d343":"code","c19c0370":"code","7992d492":"code","f390b626":"code","413aab5d":"code","bb5e870b":"code","d6091503":"code","f2c87c07":"code","b247c739":"code","bc6344de":"code","04b30059":"code","a7795203":"code","6165d1e5":"code","3f43cdc8":"code","7ad34152":"code","58a13db4":"code","9c7f3304":"code","52852f9d":"code","4b305713":"code","7d0651cb":"code","cf341aa2":"code","bc53ba65":"code","a1fde5ee":"code","10c593e5":"code","4a977105":"code","ee81d86e":"code","10195c60":"code","4ea83513":"code","e215138e":"code","ad894314":"code","a8b83a0c":"code","2c16197f":"code","303fd839":"code","6a694942":"code","6135b614":"code","ae528f48":"markdown","80ea6b65":"markdown","b0ecaedc":"markdown","0b2b3cef":"markdown","46a95858":"markdown","1152c0d5":"markdown","54392fef":"markdown","bc8410d7":"markdown","8286f510":"markdown","4ee121ae":"markdown","8966133a":"markdown","c6bf58b2":"markdown","dbb0d230":"markdown","2fb4745b":"markdown","97611617":"markdown","4a343ac4":"markdown","5ebfefc8":"markdown","97e20f96":"markdown","6863c5e7":"markdown","0ef79a28":"markdown","0337bfcb":"markdown","b587caee":"markdown","d3196243":"markdown","eb31f405":"markdown","8722ba31":"markdown","4fac4ca7":"markdown","a497d60d":"markdown","222d33b2":"markdown","0bcd3598":"markdown","3524fba5":"markdown","1ad7d65b":"markdown","a320c983":"markdown","28751194":"markdown","8e3545eb":"markdown","012f5df1":"markdown","7e58d273":"markdown","17bc7308":"markdown","d91b9c33":"markdown","5cd2b7ab":"markdown","b5eb54e1":"markdown","ef517424":"markdown","0f239c8c":"markdown","4fa8f428":"markdown","86d0b51a":"markdown"},"source":{"534da82e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn import neighbors\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import r2_score,confusion_matrix, accuracy_score, plot_confusion_matrix #utilizada para verificar a acur\u00e1cia do modelo constru\u00eddo\nfrom sklearn.svm import SVC #utilizada para importar o algoritmo SVM\n#from sklearn.linear_model import LinearRegression\n#import tensorflow\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier","364e2d89":"#Necessary functions \ndef DisplayConfusionMatrix(_classifier, _title):\n    df_cm_svm = pd.DataFrame(_classifier, index = [i for i in \"01\"],columns = [i for i in \"01\"])\n    disp = plot_confusion_matrix(_classifier, x_test, y_test,\n                                 display_labels = ['Benign','Malignant'],\n                                 cmap = plt.cm.Blues,\n                                 normalize = None)\n    disp.ax_.set_title(_title)\n\n    #print(_title)\n    #print(disp.confusion_matrix)\n\n#https:\/\/medium.com\/analytics-vidhya\/evaluating-a-random-forest-model-9d165595ad56\n\ndef DisplayConfusionMatrix_2(_y_pred, _title):\n    matrix = confusion_matrix(y_test, _y_pred)\n    #matrix = matrix.astype('float') \/ matrix.sum(axis=1)[:, np.newaxis]  # if we want %\n\n    # Build the plot\n    plt.figure(figsize=(7,7))\n    sns.set(font_scale=1.4)\n    sns.heatmap(matrix, annot=True, annot_kws={'size':10},\n            cmap=plt.cm.Blues, linewidths=0.2)\n\n    # Add labels to the plot\n    class_names = ['Benign','Malignant']\n    tick_marks = np.arange(len(class_names))\n    tick_marks2 = tick_marks + 0.5\n    plt.xticks(tick_marks, class_names, rotation=25)\n    plt.yticks(tick_marks2, class_names, rotation=0)\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.title(_title)\n    plt.show()\n    \ndef FeatureImportance (_model):\n    fi = pd.DataFrame({'feature': list(x_train.columns),\n                       'importance': _model.feature_importances_}).sort_values('importance', ascending = False)\n    return fi","57bb76f2":"#Gets de dataset as a DataFrame\nds = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","42311b48":"ds.shape","50febf10":"ds.info()","d2be7f11":"ds.describe().T","b8950613":"ds.head()","326c37f4":"ds.tail()","99f8bab5":"sns.countplot(data=ds, x='diagnosis')\nplt.title('Count by diagnosis')\nplt.show()","60e10b33":"# Transform the class column (diagnosis) into a int64\n# 0 = Benign\n# 1 = Malignant\n\n#ds.diagnosis = ds.diagnosis == 'M'\n#ds.diagnosis = ds.diagnosis.astype('int')\n# OR\nds['diagnosis'] = ds['diagnosis'].map({'M':1,'B':0})","00f4e64e":"# Drop columns \"id\" and \"Unnamed: 32\"\nds.drop(['id','Unnamed: 32'], axis= 1, inplace=True)","c6d5954d":"# Create different arrays for features and target\nx = ds.iloc[:, 1:-1]\ny = ds.iloc[:, 0]","554c08bf":"# Correlation between all features\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n\n#As we can see, there are many features that are strongly correlated. I decided to use all features.\n#To be done: eliminate unuseful features","dad5ca1d":"# Create train and test datasets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, stratify = y)","9811d343":"modelSVM = SVC(kernel = 'linear')\nmodelSVM.fit(x_train, y_train)\nlabels_svm = modelSVM.predict(x_test)\nscore_svm = modelSVM.score(x_test, y_test)\nprint(\"Score (SVM): %f\" % score_svm)\nconf_mx_svm = confusion_matrix(y_test, labels_svm)\nscores = [score_svm]","c19c0370":"params_svm = {'kernel' : ['linear', 'rbf'],   #not used: poly, precomputed and sigmoid\n              'gamma' : ['scale', 'auto']}\ngrid_search_svm = GridSearchCV(estimator = modelSVM,\n                           param_grid = params_svm,\n                           scoring = 'accuracy',\n                           cv = 5)\ngrid_search_svm = grid_search_svm.fit(x, y)\nprint(f'The best parameters for SVM are: \"{grid_search_svm.best_params_}\" and this model can explain the dataset with an accuracy of {str(np.round(grid_search_svm.best_score_ * 100,2))} %')\nscores.append(grid_search_svm.best_score_)","7992d492":"DisplayConfusionMatrix(modelSVM,\"Confusion matrix for SVM model\")","f390b626":"#To be implemented\n\n#FeatureImportance(modelSVM)\n#def f_importances(coef, names):\n#    imp = coef\n#    imp,names = zip(*sorted(zip(imp,names)))\n#    plt.barh(range(len(names)), imp, align='center')\n#    plt.yticks(range(len(names)), names)\n#    plt.show()\n\n#features_names\n#f_importances(modelSVM.coef_, features_names)\n#modelSVM.coef_","413aab5d":"#Cross validation for SVM with default hyper parameters\nsvm_cv_default = cross_val_score(estimator = modelSVM,\n                             X = x, y = y,\n                             cv = 10, scoring = 'accuracy')\nscore_svm_default_cv = svm_cv_default.mean()\nprint(\"Score (SVM default CV): %f\" % score_svm_default_cv)\nscores_cv = [score_svm_default_cv]","bb5e870b":"tree = DecisionTreeClassifier()\ntree.fit(x_train, y_train)\nlabels_tree = tree.predict(x_test)\nscore_tree = tree.score(x_test, y_test)\nprint(\"Score (Decision tree): %f\" % score_tree)\nconf_mx_tree = confusion_matrix(y_test, labels_tree)\n#accuracia = accuracy_score(y_test, labels_tree)\n#print (\"Acuracia utilizando o SVM :\" , accuracia , \"\\nEm porcentagem : \", round(accuracia*100) , \"%\\n\")\nscores.append(score_tree)","d6091503":"params_tree = {'criterion' : ['gini', 'entropy'],\n               'max_depth' : range(1,10)}\ngrid_search_tree = GridSearchCV(estimator = tree,\n                           param_grid = params_tree,\n                           scoring = 'accuracy',\n                           cv = 5)\ngrid_search_tree = grid_search_tree.fit(x, y)\nprint(f'The best parameters for Decision Tree are: \"{grid_search_tree.best_params_}\" and this model can explain the dataset with an accuracy of {str(np.round(grid_search_tree.best_score_ * 100,2))} %')\nscores.append(grid_search_tree.best_score_)","f2c87c07":"DisplayConfusionMatrix(tree,\"Confusion matrix for Decision Tree model\")","b247c739":"FeatureImportance(tree)","bc6344de":"#Cross validation for Decision Tree with default hyper parameters\ntree_cv_default = cross_val_score(estimator = tree,\n                             X = x, y = y,\n                             cv = 10, scoring = 'accuracy')\nscore_tree_default_cv = tree_cv_default.mean()\nprint(\"Score (Decision Tree default CV): %f\" % score_tree_default_cv)\nscores_cv.append(score_tree_default_cv)","04b30059":"logreg = LogisticRegression(max_iter=3000)\nlogreg.fit(x_train, y_train)\nlabels_logreg = logreg.predict(x_test)\nconf_mx_logreg = confusion_matrix(y_test, labels_logreg)\nscore_lr = logreg.score(x_test, y_test)\nprint(\"Score (Logistic Regression): %f\" % score_lr)\nscores.append(score_lr)","a7795203":"params_logreg = {\"solver\":[ 'newton-cg', 'liblinear', 'sag', 'saga'],  #not used: 'lbfgs'\n                 \"max_iter\" : [10000]}\ngrid_search_logreg = GridSearchCV(estimator = logreg,\n                           param_grid = params_logreg,\n                           scoring = 'accuracy',\n                           cv = 5)\ngrid_search_logreg = grid_search_logreg.fit(x, y)\nscores.append(grid_search_logreg.best_score_)\nprint(f'The best parameters for Logistic Regression are: \"{grid_search_logreg.best_params_}\" and this model can explain the dataset with an accuracy of {str(np.round(grid_search_logreg.best_score_ * 100,2))} %')","6165d1e5":"DisplayConfusionMatrix(logreg,\"Confusion matrix for Logistic Regression model\")","3f43cdc8":"#To be implemented","7ad34152":"#Cross validation for Logistic Regression with default hyper parameters\nlogreg_cv_default = cross_val_score(estimator = logreg,\n                             X = x, y = y,\n                             cv = 10, scoring = 'accuracy')\nscore_logreg_default_cv = logreg_cv_default.mean()\nprint(\"Score (Logistic Regression default CV): %f\" % score_logreg_default_cv)\nscores_cv.append(score_logreg_default_cv)","58a13db4":"forest = RandomForestClassifier()\nforest.fit(x_train, y_train)\nlabels_rf = forest.predict(x_test)\nconf_mx_rf = confusion_matrix(y_test, labels_rf)\nscore_rf = forest.score(x_test, y_test)\nprint(\"Score (Random forest): %f\" % score_rf)\nscores.append(score_rf)","9c7f3304":"params_rf = { \n    'n_estimators': [100, 200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n  #  'max_depth' : ['none', 4,5,6,7,8],\n    'criterion' : ['gini', 'entropy']\n}\n\n#params_rf = {\"criterion\":['gini'], \"n_estimators\" : range(60,110)}\n\ngrid_search_rf = GridSearchCV(estimator = forest,\n                           param_grid = params_rf,\n                           scoring = 'accuracy',\n                           cv = 5)\ngrid_search_rf = grid_search_rf.fit(x, y)\nscores.append(grid_search_rf.best_score_)\nprint(f'The best parameters for Random Forest are: \"{grid_search_rf.best_params_}\" and this model can explain the dataset with an accuracy of {str(np.round(grid_search_rf.best_score_ * 100,2))} %')","52852f9d":"DisplayConfusionMatrix_2(labels_rf, 'Confusion Matrix for Random Forest Model')","4b305713":"FeatureImportance(forest)","7d0651cb":"#Cross validation for Random Fores with default hyper parameters\nrf_cv_default = cross_val_score(estimator = forest,\n                             X = x, y = y,\n                             cv = 10, scoring = 'accuracy')\nscore_rf_default_cv = rf_cv_default.mean()\nprint(\"Score (Radom Forest default CV): %f\" % score_rf_default_cv)\nscores_cv.append(score_rf_default_cv)","cf341aa2":"knn = neighbors.KNeighborsClassifier(n_neighbors = 4)\nknn.fit(x_train, y_train)\nlabels_knn = knn.predict(x_test)\nscore_knn = knn.score(x_test, y_test)\nprint(\"Score (KNN): %f\" % score_knn)\nconf_mx_knn = confusion_matrix(y_test, labels_knn)\nscores.append(score_knn)\n#print(\"R2 Score %f \" % r2_score(y_test, labels_knn))\n#knn.score(x_test, y_test) , np.mean(labels_knn == y_test), (labels_knn == y_test).sum() \/ len(x_test), \"R2 Score %f \" % r2_score(y_test, labels_knn)","bc53ba65":"params_knn = {'n_neighbors': [5,7,9,11,13,15,17,19,21],\n              'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n              'weights' : ['uniform', 'distance']}\ngrid_search_knn = GridSearchCV(estimator = knn,\n                           param_grid = params_knn,\n                           scoring = 'accuracy',\n                           cv = 5)\ngrid_search_knn = grid_search_knn.fit(x, y)\nprint(f'The best parameters for KNN are: \"{grid_search_knn.best_params_}\" and this model can explain the dataset with an accuracy of {str(np.round(grid_search_knn.best_score_ * 100,2))} %')\nscores.append(grid_search_knn.best_score_)","a1fde5ee":"DisplayConfusionMatrix(knn,\"Confusion matrix for KNN model\")","10c593e5":"#To be implemented","4a977105":"#Cross validation for KNN with default hyper parameters\nknn_cv_default = cross_val_score(estimator = knn,\n                             X = x, y = y,\n                             cv = 10, scoring = 'accuracy')\nscore_knn_default_cv = knn_cv_default.mean()\nprint(\"Score (KNN default CV): %f\" % score_knn_default_cv)\nscores_cv.append(score_knn_default_cv)","ee81d86e":"classifier_split = Sequential()\n#classifier_split.add(Dense(units = 16, activation = 'relu', kernel_initializer = 'random_uniform', input_dim = 29))\nclassifier_split.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'normal', input_dim = 29))\n#classifier_split.add(Dense(units = 16, activation = 'relu', kernel_initializer = 'random_uniform'))\nclassifier_split.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'normal'))\nclassifier_split.add(Dense(units = 1, activation = 'sigmoid'))\n\notimizador = keras.optimizers.Adam(lr = 0.001, decay = 0.0001, clipvalue = 0.5)\nclassifier_split.compile(optimizer = otimizador, loss = 'binary_crossentropy',\n                      metrics = ['binary_accuracy'])\n\n# Fit model\nclassifier_split.fit(x_train, y_train,\n                  batch_size = 10, epochs = 100, verbose = 0)\n# Predict\nlabels_rn_split = classifier_split.predict(x_test)\nlabels_rn_split = (labels_rn_split > 0.5)","10195c60":"DisplayConfusionMatrix_2(labels_rn_split, 'Confusion Matrix for Neural Network - Using split')","4ea83513":"precision = accuracy_score(y_test, labels_rn_split)\nprint(precision)\nscores.append(precision)\nresultado = classifier_split.evaluate(x_test, y_test)","e215138e":"previsores = x\nclasse = y\n\ndef createNeuralNetwork():\n    classifier_cv = Sequential()\n    classifier_cv.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'normal', input_dim = 29))\n    #classifier_cv.add(Dense(units = 16, activation = 'relu', kernel_initializer = 'random_uniform', input_dim = 29))\n    classifier_cv.add(Dropout(0.2))\n    classifier_cv.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'normal'))\n    #classifier_cv.add(Dense(units = 16, activation = 'relu', kernel_initializer = 'random_uniform'))\n    classifier_cv.add(Dropout(0.2))\n    classifier_cv.add(Dense(units = 1, activation = 'sigmoid'))\n    otimizador = keras.optimizers.Adam(lr = 0.001, decay = 0.0001, clipvalue = 0.5)\n    classifier_cv.compile(optimizer = otimizador, loss = 'binary_crossentropy',\n                      metrics = ['binary_accuracy'])\n    return classifier_cv","ad894314":"classifier_cv = KerasClassifier(build_fn = createNeuralNetwork,\n                                epochs = 100,\n                                batch_size = 10, verbose = 0)\nlabels_rn_cv = cross_val_score(estimator = classifier_cv,\n                             X = previsores, y = classe,\n                             cv = 10, scoring = 'accuracy')","a8b83a0c":"mean = labels_rn_cv.mean()\nscores.append(mean)\nscores_cv.append(mean)\nstddev = labels_rn_cv.std()\nprint(\"Mean of score: \", mean)\nprint(\"Standard deviation: \", stddev)","2c16197f":"models = ['SVM', 'SVM tunned','Decision tree','Decision tree tunned','Logistic regression','Logistic regression tunned', 'Random forest','Random forest tunned','KNN','KNN tunned', 'Neural network using split', 'Neural network using cross validation']\ndf_scores = pd.DataFrame({'Model': models,\n                       'Score': scores}).sort_values(['Score', 'Model'],ascending = [False, True])\ndf_scores","303fd839":"#models_cv = ['SVM CV', 'SVM tunned CV','Decision tree CV','Decision tree tunned CV','Logistic regression CV','Logistic regression tunned CV', 'Random forest CV','Random forest tunned CV','KNN CV','KNN tunned CV', 'Neural network using CV']\nmodels_cv = ['SVM CV','Decision tree CV','Logistic regression CV','Random forest CV','KNN CV','Neural network using CV']\ndf_scores_cv = pd.DataFrame({'Model CV': models_cv,\n                       'Score CV': scores_cv}).sort_values(['Score CV', 'Model CV'],ascending = [False, True])\ndf_scores_cv","6a694942":"best_model = df_scores.iloc[0]\n#type(best_model.Score)\nprint(f'We can conclude that \"{best_model.Model}\" model can explain this dataset with an accuracy of {str(np.round(best_model.Score * 100,2))} %')","6135b614":"# Graphical representation for Decision tree\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\nexport_graphviz(tree, out_file=\"mytree.dot\")\nwith open(\"mytree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","ae528f48":"<a id=\"4.2.3-Confusion-Matrix\"><\/a>\n# 4.2.3 - Decision Tree - Confusion Matrix","80ea6b65":"<a id=\"4.4.2-Parameters-Tuning\"><\/a>\n# 4.4.2 - Random Forest - Parameters tuning","b0ecaedc":"<a id=\"4.5.3-Confusion-Matrix\"><\/a>\n# 4.5.3 - KNN - Confusion Matrix","0b2b3cef":"<a id=\"4.5.5-Cross-Validation\"><\/a>\n# 4.5.5 - KNN - Cross Validation","46a95858":"<a id=\"4.4-Random-Forest\"><\/a>\n# 4.4 - Random forest\n\n<a id=\"4.4.1-Build-A-Model-With-Default-Parameters\"><\/a>\n# 4.4.1 - Random Forest - Build a model with default parameters","1152c0d5":"<a id=\"4.4.5-Cross-Validation\"><\/a>\n# 4.4.5 - Random Forest - Cross Validation","54392fef":"<a id=\"4.3-Logistic-Regression\"><\/a>\n# 4.3 - Logistic regression\n\n<a id=\"4.3.1-Build-A-Model-With-Default-Parameters\"><\/a>\n# 4.3.1 - Logistic Regression - Build a model with default parameters","bc8410d7":"To be implemented","8286f510":"<a id=\"7-Appendix\"><\/a>\n# 7 - Appendix","4ee121ae":"<a id=\"4.2.4-Importance-Of-Each-Feature\"><\/a>\n# 4.2.4 - Decision Tree - Importance of each feature","8966133a":"<a id=\"5-Deep-Learning-TensorFlow-And-Keras\"><\/a>\n# 5 - Deep learning - Tensorflow and Keras\n\n\"Deep Learning com Python de A a Z - O Curso Completo\" - https:\/\/www.udemy.com\/course\/deep-learning-com-python-az-curso-completo\/\nUdemy course from https:\/\/iaexpert.academy\/\n\n\n\nTBD - Parameteres tuning","c6bf58b2":"<a id=\"4.2.5-Cross-Validation\"><\/a>\n# 4.2.5 - Decision Tree - Cross Validation","dbb0d230":"The table bellow shows the scores for each algorithm used in this notebook. <br>\nThe dataset was divided in train and test using train_test_split.","2fb4745b":"<a id=\"4.4.3-Confusion-Matrix\"><\/a>\n# 4.4.3 - Random Forest - Confusion Matrix","97611617":"* Let's split the dataset:\n\n    1. 80% for training the model\n    2. 20% for testing the model","4a343ac4":"<a id=\"4.5-KNN\"><\/a>\n# 4.5 - KNN\n\n<a id=\"4.5.1-Build-A-Model-With-Default-Parameters\"><\/a>\n# 4.5.1 - KNN - Build a model with default parameters","5ebfefc8":"To be implemented","97e20f96":"<a id=\"2-Dataset-Preparation\"><\/a>\n# 2 - Dataset preparation","6863c5e7":"<a id=\"4.1.3-Confusion-Matrix\"><\/a>\n# 4.1.3 - SVM - Confusion Matrix","0ef79a28":"To be implemented","0337bfcb":"<a id=\"4.5.4-Importance-Of-Each-Feature\"><\/a>\n# 4.5.4 - KNN - Importance of each feature","b587caee":"<a id=\"3-Create-Train-And-Test-Datasets\"><\/a>\n# 3 - Create train and test datasets","d3196243":"We can consider that the data of the column \"diagnosis\" is balanced","eb31f405":"<a id=\"4.3.5-Cross-Validation\"><\/a>\n# 4.3.5 - Logistic Regression - Cross Validation","8722ba31":"<a id=\"4.1.2-Parameters-Tuning\"><\/a>\n# 4.1.2 - SVM - Parameters tuning","4fac4ca7":"<a id=\"4-Models\"><\/a>\n# 4 - Models\n\n<a id=\"4.1-SVM\"><\/a>\n# 4.1 - SVM\n\n<a id=\"4.1.1-Build-A-Model-With-Default-Parameters\"><\/a>\n# 4.1.1 - SVM - Build a model with default parameters","a497d60d":"<a id=\"4.3.3-Confusion-Matrix\"><\/a>\n# 4.3.3 - Logistic Regression - Confusion Matrix","222d33b2":"The dataset contains 33 columns and 569 records.\nEach record corresponds to  ","0bcd3598":"<a id=\"1-Exploratory-Data-Analysis\"><\/a>\n# 1 - Exploratory data analysis\n\n **Attribute Information:**\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign) \n\n3)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\nb) texture (standard deviation of gray-scale values)\nc) perimeter\nd) area\ne) smoothness (local variation in radius lengths)\nf) compactness (perimeter^2 \/ area - 1.0)\ng) concavity (severity of concave portions of the contour)\nh) concave points (number of concave portions of the contour)\ni) symmetry\nj) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant\n\nhttps:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data","3524fba5":"<a id=\"5.1-Using-Test-Split\"><\/a>\n# 5.1 - Using test_split","1ad7d65b":"The table bellow shows the scores for each algorithm used in this notebook. <br>\nThe dataset was divided in train and test using cross validation.","a320c983":"<a id=\"5.2-Using-Cross-Validation\"><\/a>\n# 5.2 - Using cross validation","28751194":"<a id=\"4.2.2-Parameters-Tuning\"><\/a>\n# 4.2.2 - Decision Tree - Parameters tuning","8e3545eb":"<a id=\"4.4.4-Importance-Of-Each-Feature\"><\/a>\n# 4.4.4 - Random Forest - Importance of each feature","012f5df1":"# 0 - Intro\n\n> **I started studying Machine learning in Nov\/2020 and this is my very first analysis.\n> All constructive comments will be appreciated.**\n\n\n**My main objective is to build 5 models of machine learning and 1 model of deep learning to predict if the tumor is a benign one or not.\nBesides, I am going to do the tuning of parameters of each model to get a better accuracy.\nThen, I perform the comparison of accuracy between these models.**\n\n\n**Table of contents**\n\n* [1 - Exploratory data analysis](#1-Exploratory-Data-Analysis)\n* [2 - Dataset preparation](#2-Dataset-Preparation)\n* [3 - Create train and test datasets](#3-Create-Train-And-Test-Datasets)\n* [4 - Models](#4-Models)\n    - [4.1 - SVM](#4.1-SVM)\n        - [4.1.1 - SVM - Build a model with default parameters](#4.1.1-Build-A-Model-With-Default-Parameters)\n        - [4.1.2 - SVM - Parameters tuning](#4.1.2-Parameters-Tuning)\n        - [4.1.3 - SVM - Confusion Matrix](#4.1.3-Confusion-Matrix)\n        - [4.1.4 - SVM - Importance of each feature](#4.1.4-Importance-Of-Each-Feature)\n        - [4.1.5 - SVM - Cross Validation](#4.1.5-Cross-Validation)\n    - [4.2 - Decision Tree](#4.2-Decision-Tree)\n        - [4.2.1 - Decision Tree - Build a model with default parameters](#4.2.1-Build-A-Model-With-Default-Parameters)\n        - [4.2.2 - Decision Tree - Parameters tuning](#4.2.2-Parameters-Tuning)\n        - [4.2.3 - Decision Tree - Confusion Matrix](#4.2.3-Confusion-Matrix)\n        - [4.2.4 - Decision Tree - Importance of each feature](#4.2.4-Importance-Of-Each-Feature)\n        - [4.2.5 - Decision Tree - Cross Validation](#4.2.5-Cross-Validation)\n    - [4.3 - Logistic regression](#4.3-Logistic-Regression)\n        - [4.3.1 - Logistic Regression - Build a model with default parameters](#4.3.1-Build-A-Model-With-Default-Parameters)\n        - [4.3.2 - Logistic Regression - Parameters tuning](#4.3.2-Parameters-Tuning)\n        - [4.3.3 - Logistic Regression - Confusion Matrix](#4.3.3-Confusion-Matrix)\n        - [4.3.4 - Logistic Regression - Importance of each feature](#4.3.4-Importance-Of-Each-Feature)\n        - [4.3.5 - Ligistic Regression - Cross Validation](#4.3.5-Cross-Validation)\n    - [4.4 - Random forest](#4.4-Random-Forest)\n        - [4.4.1 - Random Forest - Build a model with default parameters](#4.4.1-Build-A-Model-With-Default-Parameters)\n        - [4.4.2 - Random Forest - Parameters tuning](#4.4.2-Parameters-Tuning)\n        - [4.4.3 - Random Forest - Confusion Matrix](#4.4.3-Confusion-Matrix)\n        - [4.4.4 - Random Forest - Importance of each feature](#4.4.4-Importance-Of-Each-Feature)\n        - [4.4.5 - Random Forest - Cross Validation](#4.4.5-Cross-Validation)\n    - [4.5 - KNN](#4.5-KNN)\n        - [4.5.1 - KNN - Build a model with default parameters](#4.5.1-Build-A-Model-With-Default-Parameters)\n        - [4.5.2 - KNN - Parameters tuning](#4.5.2-Parameters-Tuning)\n        - [4.5.3 - KNN - Confusion Matrix](#4.5.3-Confusion-Matrix)\n        - [4.5.4 - KNN - Importance of each feature](#4.5.4-Importance-Of-Each-Feature)\n        - [4.5.5 - KNN - Cross Validation](#4.5.5-Cross-Validation)\n* [5 - Deep learning - Tensorflow and Keras](#5-Deep-Learning-TensorFlow-And-Keras)\n    - [5.1 - Using test_split](#5.1-Using-Test-Split)\n    - [5.2 - Using cross validation](#5.2-Using-Cross-Validation)\n* [6 - Conclusion](#6-Conclusion)\n* [7 - Appendix](#7-Appendix)","7e58d273":"<a id=\"4.1.4-Importance-Of-Each-Feature\"><\/a>\n# 4.1.4 - SVM - Importance of each feature","17bc7308":"# Breast cancer predicting model - ML\n\nBased on this dataset: https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\n\n\n> Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nn the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\n> This database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\n> Also can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29","d91b9c33":"<a id=\"4.3.2-Parameters-Tuning\"><\/a>\n# 4.3.2 - Logistic Regression - Parameters tuning","5cd2b7ab":"<a id=\"6-Conclusion\"><\/a>\n# 6 - Conclusion","b5eb54e1":"<a id=\"4.3.4-Importance-Of-Each-Feature\"><\/a>\n# 4.3.4 - Logistic Regression - Importance of each feature","ef517424":"<a id=\"4.5.2-Parameters-Tuning\"><\/a>\n# 4.5.2 - KNN - Parameters tuning","0f239c8c":"<a id=\"4.1.5-Cross-Validation\"><\/a>\n# 4.1.5 - SVM - Cross Validation","4fa8f428":"1 - Transform the class column (diagnosis) into a int64 <br>\n2 - Drop columns \"id\" and \"Unnamed: 32\"","86d0b51a":"<a id=\"4.2-Decision-Tree\"><\/a>\n# 4.2 - Decision Tree\n\n<a id=\"4.2.1-Build-A-Model-With-Default-Parameters\"><\/a>\n# 4.2.1 - Decision Tree - Build a model with default parameters"}}