{"cell_type":{"6707de1e":"code","8eb0adb0":"code","217510ce":"code","de542e62":"code","362a53ba":"code","5cc3afc1":"code","bca8e671":"code","7d358cae":"code","2a02f5c3":"code","d02c3ca5":"code","82239741":"code","bf8d8622":"code","478828cf":"code","50d0454c":"code","6914a2bd":"code","71f4d2e7":"code","98f70235":"code","d78b99e7":"code","3e4838ab":"code","c17eb2db":"markdown","4c99e896":"markdown","75b9105f":"markdown","e94b204d":"markdown","fe665182":"markdown","bebd9204":"markdown","5472124f":"markdown","28332298":"markdown","26709bf6":"markdown","eaabe18a":"markdown","b66ce827":"markdown","ba5f2efe":"markdown","e64a558a":"markdown","a530a777":"markdown","34978176":"markdown","797cc51c":"markdown","877e06cf":"markdown","83c2bf9b":"markdown","e50cc1b6":"markdown","7e4f9cde":"markdown","2ddfc3f5":"markdown","8015d646":"markdown"},"source":{"6707de1e":"# load packages\nimport sys\nimport pandas as pd\nimport matplotlib\nimport numpy as np\nimport scipy as sp\nimport IPython\nfrom IPython import display\nimport sklearn\nfrom sklearn import *\nfrom sklearn import preprocessing\nimport random\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output","8eb0adb0":"# import data from the default file \"..\/input\/\" \n# a dataset should be broken into 3 splits: train, test, and final validation\n# the test file provided is the validation file for competition submission \n# and we will split the train set into train and test data in future sections\ndata_raw = pd.read_csv('..\/input\/train.csv')\ndata_val = pd.read_csv('..\/input\/test.csv')\n# to play with our data we'll create a copy\ndata1 = data_raw.copy(deep = True)\ndata_cleaner = [data1, data_val]\n\n# preview data\nprint(data_raw.info())\ndata_raw.sample(10)","217510ce":"print('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint('Test\/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\ndata_raw.describe(include = 'all')","de542e62":"###COMPLETING: complete or delete missing values in train and test\/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature\/column and others previously stated to exclude in train dataset\ndrop_column = ['PassengerId','Cabin', 'Ticket']\ndata1.drop(drop_column, axis=1, inplace = True)\n\nprint(data1.isnull().sum())\nprint(\"-\"*10)\nprint(data_val.isnull().sum())","362a53ba":"###CREATE: Feature Engineering for train and test\/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http:\/\/www.pythonforbeginners.com\/dictionary\/python-split\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n\n    #Continuous variable bins; qcut vs cut: https:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    #Fare Bins\/Buckets using qcut or frequency bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.qcut.html\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    #Age Bins\/Buckets using cut or value bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.cut.html\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    \n\n#cleanup rare title names\n#print(data1['Title'].value_counts())\nstat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\ntitle_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\ndata1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())\nprint(\"-\"*10)\n\n\n#preview data again\ndata1.info()\ndata_val.info()\ndata1.sample(10)","5cc3afc1":"#CONVERT: convert objects to category using Label Encoder for train and test\/validation dataset\n\n#code categorical data\nlabel = preprocessing.LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\n\n#define y variable aka target\/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name\/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w\/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\n\n\ndata1_dummy.head()","bca8e671":"#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","7d358cae":"plt.figure(figsize=[16,12])\n\nplt.subplot(231)\nplt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(232)\nplt.boxplot(data1['Age'], showmeans = True, meanline = True)\nplt.title('Age Boxplot')\nplt.ylabel('Age (Years)')\n\nplt.subplot(233)\nplt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)\nplt.title('Family Size Boxplot')\nplt.ylabel('Family Size (#)')\n\nplt.subplot(234)\nplt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], \n         stacked=True, color = ['b','r'],label = ['Survived','Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], \n         stacked=True, color = ['b','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], \n         stacked=True, color = ['b','r'],label = ['Survived','Dead'])\nplt.title('Family Size Histogram by Survival')\nplt.xlabel('Family Size (#)')\nplt.ylabel('# of Passengers')\nplt.legend()","2a02f5c3":"#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived', data=data1, ax = saxis[1,0])\nsns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1, ax = saxis[1,1])\nsns.pointplot(x = 'FamilySize', y = 'Survived', data=data1, ax = saxis[1,2])","d02c3ca5":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)","82239741":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, ensemble\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics","bf8d8622":"def train_model(model, features, target, fit=False):\n    # Split dataset in cross-validation\n    # Run model 10x with 60\/30 split intentionally leaving out 10%\n    cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) \n    cv_results = model_selection.cross_validate(model, features, target, cv  = cv_split)\n    \n    if(fit):\n        # fit model\n        model = model.fit(features, target)\n        return model, cv_results\n    \n    return cv_results","478828cf":"# Train on a tree\n# decision_tree = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 5, random_state = 1)\n# decision_tree = decision_tree.fit(features, target)\n# print(decision_tree.score(features, target))\n# print(decision_tree.feature_importances_)\ndecision_tree = tree.DecisionTreeClassifier()\ntrained_tree, cv_results = train_model(decision_tree, data1[data1_x_bin], data1[Target], fit=True)\n\nTree_Predict = trained_tree.predict(data1[data1_x_bin])","50d0454c":"# Report\nprint('Decision Tree Model Accuracy\/Precision Score on training set: {:.2f}%\\n'\n      .format(metrics.accuracy_score(data1['Survived'], Tree_Predict)*100))\nprint(metrics.classification_report(data1['Survived'], Tree_Predict))\n\nprint('-'*10)\nprint(np.mean(cv_results['test_score']))","6914a2bd":"# Make prediciton\nprediction_dt = decision_tree.predict(data_val[data1_x_bin])\nPassengerId =np.array(data_val[\"PassengerId\"]).astype(int)\nsolution_dt = pd.DataFrame(prediction_dt, PassengerId, columns = [\"Survived\"])\nsolution_dt.to_csv(\"solution_dt.csv\", index_label = [\"PassengerId\"])","71f4d2e7":"# Tune hyper-parameters\nparam_grid = {'criterion': ['gini', 'entropy'],  # scoring methodology\n              'splitter': ['best', 'random'], # splitting methodology\n              'max_depth': [3,4,6], # max depth tree can grow\n              'min_samples_split': [2, 3,.03], # minimum subset size BEFORE new split (fraction is % of total)\n              'min_samples_leaf': [3, 5, 8, .03,.05], # minimum subset size AFTER new split split (fraction is % of total)\n              'random_state': [0] #seed or control random number generator\n             }\n\nprint(len(list(model_selection.ParameterGrid(param_grid))))\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) \n\n# Choose best model with grid_search:\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), \n                                          param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\ntune_model.fit(data1[data1_x_bin], data1[Target])\n\n# Report\nprint('-'*10)\nprint('Best Parameters: ', tune_model.best_params_)\nprint(\"Training w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \nprint(\"Test w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"Test w\/bin score 3*std: +\/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))","98f70235":"# The best-hyper tree we have now\nbest_hyper_tree = tree.DecisionTreeClassifier(criterion = 'gini', max_depth = 4, \n                                              min_samples_leaf = 5, min_samples_split = 2, \n                                              splitter = 'best', random_state = 0)\n\n# Report the best-hyper tree\ncv_best = train_model(best_hyper_tree, data1[data1_x_bin], data1[Target])\nprint(np.mean(cv_best['test_score']))","d78b99e7":"print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(best_hyper_tree, step = 1, scoring = 'accuracy', cv = cv_split)\ndtree_rfe.fit(data1[data1_x_bin], data1[Target])\n\n#transform x&y to reduced features and fit new model\n#alternative: can use pipeline to reduce fit and transform steps: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\nX_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\nrfe_results = model_selection.cross_validate(best_hyper_tree, data1[X_rfe], data1[Target], cv  = cv_split)\n\n#print(dtree_rfe.grid_scores_)\nprint('-'*10)\nprint('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \nprint('AFTER DT RFE Training Columns New: ', X_rfe)\n\nprint(\"AFTER DT RFE Training w\/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \nprint(\"AFTER DT RFE Test w\/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\nprint(\"AFTER DT RFE Test w\/bin score 3*std: +\/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\nprint('-'*10)\n\n# tune rfe model\nrfe_tune_model = model_selection.GridSearchCV(best_hyper_tree, param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\nrfe_tune_model.fit(data1[X_rfe], data1[Target])\n\nprint('-'*10)\nprint('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\nprint(\"AFTER DT RFE Tuned Training w\/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \nprint(\"AFTER DT RFE Tuned Test w\/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT RFE Tuned Test w\/bin score 3*std: +\/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)","3e4838ab":"import graphviz \nbest_tree = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 6, \n                                              min_samples_leaf = 3, min_samples_split = 2, \n                                              splitter = 'best', random_state = 0)\n\nbest_tree.fit(data1[X_rfe], data1[Target])\n\ndot_data = tree.export_graphviz(best_tree, out_file=None, \n                                feature_names = X_rfe, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph","c17eb2db":"# 4. Models","4c99e896":"# Perform Exploratory Analysis with Statistics\n\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables.","75b9105f":"## 4.3 Tune model with hyper-parameters\n\nWe choose best model with [grid_search](http:\/\/scikit-learn.org\/stable\/modules\/grid_search.html#grid-search).\n\nBelow are available hyper-parameters and defintions:\n\n> **class sklearn.tree.DecisionTreeClassifier**(criterion=\u2019gini\u2019, splitter=\u2019best\u2019, max_depth=None, \n> min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n> max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n\nWe want to find out a relatively better hyper-parameters choice for this task.","e94b204d":"## View data\nBefore perform exploratory analysis on data, we first improt necessary datasets, meet data and clean data for downstream analysis.","fe665182":"## pipeline\nto be continue...","bebd9204":"> #### Some advantages of decision trees are:\n> * Simple to understand and to interpret. Trees can be visualized.\n> * Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n> * The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n> * Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\nAble to handle multi-output problems.\n> * Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by Boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n> * Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n> * Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n>\n> #### The disadvantages of decision trees include:\n> * Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n> * Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n> * The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n> * There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n> * Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.","5472124f":"## 4.2 Decision Tree","28332298":"## 4.4 Tune model with feature selection\nAs to the qualification of a model, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. Sklearn has several options, we will use recursive feature elimination (RFE) with cross validation (CV).","26709bf6":"## 4.1 Cross-validation\n\nCross-validation is basically a shortcut to split and score our model multiple times, so we can get an idea of how well it will perform on unseen data. It\u2019s a little more expensive in computer processing, but it's important so we don't gain false confidence.\n\nWe use [sklearn.model_selection.cross_validate](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#the-cross-validate-function-and-multiple-metric-evaluation) function to train, test, and score our model performance.\n\nWe use [sklearn.model_selection.ShuffleSplit](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit) to split data into training and test sets for cross-validation.","eaabe18a":"# Problem \nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n","b66ce827":"So now we know the hyper-parameter selection\n\n> {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 5,\n> 'min_samples_split': 2, 'random_state': 0, 'splitter': 'best'}\n\nhas best performace among all possible setting we defined.","ba5f2efe":"## Clean data\nAs described above, some variables need to be corrected, completed , created and converted before perform analysis. In this stage, we will clean our data by 1) correcting aberrant values and outliers, 2)completing missing information, 3) creating new features for analysis, and 4)converting fields to the correct format for calculations and presentation.\n1. **Correcint:** Reviewing the data, there seem to be no aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare.However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. Besides, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n2. **Completing:** There are null values or missing data in the age, cabin, and embarked field. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. For this dataset, age will be imputed with the median, the cabin attribute will be dropped, and embark will be imputed with mode. \n3. **Creating:** Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.\n4. **Converting:** Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations. For this dataset, we will convert object datatypes to categorical dummy variables.","e64a558a":"# Data\nThe data set is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic\/data)","a530a777":"In the following, we generate accuracy summary report with [sklearn.metrics.classification_report](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report).\n\nHere is an introduction to the scores: (Read more in https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#binary-classification)\n\n$precision = \\frac{tp}{tp + fp}$ \n\nwhere $tp$ is the number of true positives and $fp$ the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n\n$recall = \\frac{tp}{tp + fn}$\n\nwhere fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\n$F _ { \\beta } = \\left( 1 + \\beta ^ { 2 } \\right) \\frac { \\text { precision } \\times \\text { recall } } { \\beta ^ { 2 } \\text { precision } + \\text { recall } }$\n\nThe F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in y_true.","34978176":"### Visualize categorical features\n\nWe will use seaborn graphics for multi-variable comparison.\n\n[seaborn.barplot](http:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html#seaborn-barplot) shows point estimates and confidence intervals as rectangular bars.\n\n[seaborn.pointplot](http:\/\/seaborn.pydata.org\/generated\/seaborn.pointplot.html?highlight=pointplot#seaborn-pointplot) shows point estimates and confidence intervals using scatter plots.\n\nThe following images visualize the distribution of feature values among survivors.","797cc51c":"# Preparation \n## Import libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. ","877e06cf":"It shows that these features have impacts on the survival rate.","83c2bf9b":"## Visualize Numerical features\n\n**Histograms:** Helpful for analyzing continuous numberical data. It can indicate distribution of samples using automatically defined bins or equally ranged bands.\n\n**Boxplot:** Show the quantils and distributions of data, help for finding outliers.\n\nWe use `matplotlib.pyplot` to visualize correlations between numerical features and 'Survival'.","e50cc1b6":"So now we know the model training with features\n\n> ['Sex_Code' 'Pclass' 'Embarked_Code' 'Title_Code' 'FamilySize'\n> 'FareBin_Code']\n\nand hyper-parameter selection:\n\n> {'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 3,\n> 'min_samples_split': 2, 'random_state': 0, 'splitter': 'best'}\n\nhas best performance.","7e4f9cde":"Now we know what to clean, and we'll execute our code.","2ddfc3f5":"As we can see from the table displayed above, we can get the information as follows:\n* The *Survived* variable is our outcome or dependent variable. It is a binary nominal datatype of 1 for survived and 0 for did not survive. All other variables are potential predictor or independent variables.\n* The *PassengerId* and *Ticket* variables are assumed to be random unique identifiers, and they have no impact on the outcome variable. Thus, they will be excluded from the subsequent analysis.\n* The *Pclass* variable is an ordinal datatype for the ticket class, a proxy for socio-economic status (SES), representing 1 = upper class, 2 = middle class, and 3 = lower class.\n* The *Name* variable is a nominal datatype. It could be used in feature engineering to derive the gender from title, family size from surname, and SES from titles like doctor or master. Since these cariables already exist, we'll make use of it to see if title, like master, makes a difference.\n* The *Sex* and *Embarked* variables are nominal datatype. They will be converted to dummy variables for mathematical calculations.\n* The *Age* and *Fare* variables are continuous quantitative datatypes and *Parch* represents number of relater parents\\children aboard. Both are discrete quantitative datatypes. This can be used for feature engineering to create a family size and is alone variable.\n* The *Cabin* variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occureed and SES from deck levels. However, since there are many null values, it does not add value and thus is excluded from analysis.","8015d646":"## 4.5 Visualize final decision tree"}}