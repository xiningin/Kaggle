{"cell_type":{"47901a49":"code","5f453b6d":"code","48da9ed2":"code","cba72695":"code","3142bcd1":"code","5d005515":"code","d0ea04b4":"code","ff7a5b64":"code","f588defd":"code","f57cc2cf":"code","1057fb1a":"code","72ce3389":"code","89e4b15f":"code","fea72240":"code","23436ac5":"code","909294b2":"code","0ca986c5":"code","2c72d05f":"code","b2c97f91":"code","5318afd1":"code","dcb94f90":"code","bfefa47c":"code","c6ff75a7":"code","e3a43ab4":"code","f2a95483":"code","c132df9d":"code","b71dfb57":"code","fc072e85":"code","0b40275d":"code","2d8909a9":"code","5fba2174":"code","00ae4142":"code","783807b1":"code","3d6ffc15":"code","6d5b3d2f":"code","042cb83a":"code","ddb0501a":"code","c4d8d691":"code","7c38365c":"code","159fb785":"code","87c34126":"code","58b1a0f7":"code","5f83946c":"code","8fece31d":"code","e0db2ea9":"code","8ea16368":"code","d25aace1":"code","c74eb0ff":"code","b07dfc3d":"code","0d463379":"code","302ecc4d":"code","99ead92c":"code","84d220ef":"code","6af25008":"code","dfc6b233":"code","9388e5ed":"code","20ff045e":"code","7fc1050f":"code","771e97f0":"code","1c41ebd6":"code","e0af9b78":"code","21f13879":"code","ba1fac76":"code","3ef6b4e2":"markdown","cd8c933a":"markdown","3623d53b":"markdown","e120c2df":"markdown","aea07164":"markdown","5e0c2aec":"markdown","7a11cf4b":"markdown","07625da4":"markdown","5443f594":"markdown","7dfff9ba":"markdown","438608b9":"markdown","2b89268d":"markdown","a8f0a49e":"markdown","e27c31fc":"markdown","748e0d36":"markdown","c0533b8a":"markdown","60629ec5":"markdown","fbc0682d":"markdown","ecad8e3a":"markdown","8bdbfceb":"markdown"},"source":{"47901a49":"#importing necessary libraries\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\n\nimport re\n\nimport pickle\nfrom tqdm import tqdm\nimport os\n\n","5f453b6d":"#loading dataset\ndata=pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\")\n","48da9ed2":"#what is the shape of the dta set?\ndata.shape","cba72695":"\n#which columns are there?\ndata.columns","3142bcd1":"#lets see the first 5 rows of each column.\n\ndata.head(5)","5d005515":"#dropping nan values\ndata=data.dropna()\ndata.head(2)","d0ea04b4":"#dropping unnamed column\ndata=data.drop([\"Unnamed: 0\"],axis=1).reset_index()\ndata.head(2)","ff7a5b64":"#Do you want to see more information about data set?...............","f588defd":"#here it is........\ndata.info()","f57cc2cf":"#Lets describe some statistical data.......","1057fb1a":"data.describe()","72ce3389":"#lets check how many different values are there for clothing id\ndata[\"Clothing ID\"].value_counts()","89e4b15f":"#lets get the value counts of division name\ndata[\"Division Name\"].value_counts()","fea72240":"#lets check the value counts of department name\ndata[\"Department Name\"].value_counts()","23436ac5":"#lets check the value counts of class name\ndata[\"Class Name\"].value_counts()","909294b2":"a=data[\"Recommended IND\"].value_counts()\nprint(a)\nprint(\"The products that are recommended are : \", (a[1]\/(a[0]+a[1]))*100,\"%\")\nprint(\"the productys that are not recommended are :\", (a[0]\/(a[0]+a[1]))*100,\"%\")","0ca986c5":"#lets define function for stack plot..........","2c72d05f":"def stack_plot(data,xtick,col2=\"Recommended IND\",col3=\"total\"):\n    ind=np.arange(data.shape[0])\n    \n    plt.figure(figsize=(25,10))\n    \n    p1=plt.bar(ind,data[col3].values)\n    p2=plt.bar(ind,data[col2].values)\n    \n    plt.ylabel(\"Recommendation\")\n    plt.xticks(ind,list(data[xtick].values))\n    plt.legend((p1[0],p2[0]),(\"total\",\"recommended\"))\n    plt.show()\n    \n    ","b2c97f91":"def univariate(data,col1,col2=\"Recommended IND\",top=False):\n    temp=pd.DataFrame(data.groupby(col1)[col2].agg(lambda x:x.eq(1).sum())).reset_index()\n    \n    recommend=temp[col2]\n    temp=pd.DataFrame(data.groupby(col1)[col2].agg([(\"Avg\",\"mean\"),(\"total\",\"count\")]).reset_index())\n    temp[col2]=recommend\n    \n    temp.sort_values(by=[\"total\"],inplace=True,ascending=False)\n    \n    if top:\n        temp=temp[0:top]\n        \n    stack_plot(temp,xtick=col1,col2=col2,col3=\"total\")\n    print(temp.head(5))\n    print(\"*\"*50)\n    print(temp.tail(5))","5318afd1":"univariate(data,\"Clothing ID\",\"Recommended IND\",False)","dcb94f90":"univariate(data,\"Division Name\",\"Recommended IND\",False)","bfefa47c":"univariate(data,\"Department Name\",\"Recommended IND\",False)","c6ff75a7":"univariate(data,\"Class Name\",\"Recommended IND\",False)","e3a43ab4":"univariate(data,\"Rating\",\"Recommended IND\",False)","f2a95483":"univariate(data,\"Positive Feedback Count\",\"Recommended IND\",False)","c132df9d":"univariate(data,\"Age\",\"Recommended IND\",False)","b71dfb57":"#How to calculate number of words in a string in DataFrame: https:\/\/stackoverflow.com\/a\/37483537\/4084039\nword_count = data['Title'].str.split().apply(len).value_counts()\nword_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(word_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(word_dict.values()))\n\nplt.ylabel('total')\nplt.title('Words for each title')\nplt.xticks(ind, list(word_dict.keys()))\nplt.show()","fc072e85":"#convert all letters to lower case, replace the space by _ \ndata[\"Class Name\"]=data[\"Class Name\"].str.lower()\ndata[\"Class Name\"]=data[\"Class Name\"].str.replace(\" \",\"_\")\n\n#Want to see the va;ue_counts??\ndata[\"Class Name\"].value_counts()","0b40275d":"data[\"Department Name\"]=data[\"Department Name\"].str.lower()\ndata[\"Department Name\"]=data[\"Department Name\"].str.replace(\" \",\"_\")\ndata[\"Department Name\"].value_counts()","2d8909a9":"data[\"Division Name\"]=data[\"Division Name\"].str.lower()\ndata[\"Division Name\"]=data[\"Division Name\"].str.replace(\" \",\"_\")\ndata[\"Division Name\"].value_counts()","5fba2174":"#let us define a function to replace shorthand notations with full....","00ae4142":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","783807b1":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","3d6ffc15":"# Combining all the above......\nfrom tqdm import tqdm\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https:\/\/gist.github.com\/sebleier\/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text","6d5b3d2f":"preprocessed_title=preprocess_text(data[\"Title\"].values)","042cb83a":"#lets replace the title column in the dataframe with preprocessed title.\ndata[\"Title\"]=preprocessed_title","ddb0501a":"#lets see how the review text looks like before preprocessing..(here we can see the 2nd review)\ndata[\"Review Text\"][1]","c4d8d691":"preprocessed_review=preprocess_text(data[\"Review Text\"].values)","7c38365c":"preprocessed_review[1]","159fb785":"#lets replace the review column in the dataframe with preprocessed review.\ndata[\"Review Text\"]=preprocessed_review","87c34126":"#Lets take out our class label (recommended ind) and keep it in variable y..\n# because this is the class label that we have to predict given all other features.\n#and all other features are kept in x variable..\ny=data[\"Recommended IND\"]\nx=data.drop([\"Recommended IND\"],axis=1)\nx.head(1)","58b1a0f7":"#NOW, split data into train,cv,test data.......\nfrom sklearn.model_selection import train_test_split\nX_train,x_test,Y_train,y_test=train_test_split(x,y,test_size=0.33,stratify=y, random_state =41)\nx_train,x_cv,y_train,y_cv=train_test_split(X_train,Y_train,test_size=0.33,stratify=Y_train, random_state =41)","5f83946c":"vec=CountVectorizer()\nvec.fit(x_train[\"Title\"].values)\nprint(x_train.shape,y_train.shape)\n\nx_train_title=vec.transform(x_train[\"Title\"].values)\nx_cv_title=vec.transform(x_cv[\"Title\"].values)\nx_test_title=vec.transform(x_test[\"Title\"].values)\n\nprint(\"after vectorization....\")\nprint(x_train_title.shape,y_train.shape)\nprint(x_cv_title.shape,y_cv.shape)\nprint(x_test_title.shape,y_test.shape)","8fece31d":"vec=CountVectorizer()\nvec.fit(x_train[\"Review Text\"].values)\n\nx_train_rev=vec.transform(x_train[\"Review Text\"].values)\nx_cv_rev=vec.transform(x_cv[\"Review Text\"].values)\nx_test_rev=vec.transform(x_test[\"Review Text\"].values)\n\nprint(\"after vectorization....\")\nprint(x_train_rev.shape,y_train.shape)\nprint(x_cv_rev.shape,y_cv.shape)\nprint(x_test_rev.shape,y_test.shape)","e0db2ea9":"vec=CountVectorizer()\nvec.fit(x_train[\"Division Name\"].values)\n\nx_train_div=vec.transform(x_train[\"Division Name\"].values)\nx_cv_div=vec.transform(x_cv[\"Division Name\"].values)\nx_test_div=vec.transform(x_test[\"Division Name\"].values)\n\nprint(\"after vectorization....\")\nprint(x_train_div.shape,y_train.shape)\nprint(x_cv_div.shape,y_cv.shape)\nprint(x_test_div.shape,y_test.shape)\n\nprint(vec.get_feature_names())","8ea16368":"vec=CountVectorizer()\nvec.fit(x_train[\"Department Name\"].values)\n\nx_train_dep=vec.transform(x_train[\"Department Name\"].values)\nx_cv_dep=vec.transform(x_cv[\"Department Name\"].values)\nx_test_dep=vec.transform(x_test[\"Department Name\"].values)\n\nprint(\"after vectorization....\")\nprint(x_train_dep.shape,y_train.shape)\nprint(x_cv_dep.shape,y_cv.shape)\nprint(x_test_dep.shape,y_test.shape)\n\nprint(vec.get_feature_names())","d25aace1":"vec=CountVectorizer()\nvec.fit(x_train[\"Class Name\"].values)\n\nx_train_cls=vec.transform(x_train[\"Class Name\"].values)\nx_cv_cls=vec.transform(x_cv[\"Class Name\"].values)\nx_test_cls=vec.transform(x_test[\"Class Name\"].values)\n\nprint(\"after vectorization....\")\nprint(x_train_cls.shape,y_train.shape)\nprint(x_cv_cls.shape,y_cv.shape)\nprint(x_test_cls.shape,y_test.shape)\n\nprint(vec.get_feature_names())\n","c74eb0ff":"#We keep our rating data which is categorical data in numerical form as it is.","b07dfc3d":"x_train_rate=(np.array(x_train[\"Rating\"])).reshape((-1,1))\nx_cv_rate=(np.array(x_cv[\"Rating\"])).reshape(-1,1)\nx_test_rate=(np.array(x_test[\"Rating\"])).reshape((-1,1))\n\n\nprint(\"after vectorization.......\")\nprint(x_train_rate.shape,y_train.shape)\nprint(x_cv_rate.shape,y_cv.shape)\nprint(x_test_rate.shape,y_test.shape)\n","0d463379":"#normalize age column.\nfrom sklearn.preprocessing import Normalizer\nnorm=Normalizer()\n\nnorm.fit(x_train[\"Age\"].values.reshape(1,-1))\n\nx_train_age=norm.transform(x_train[\"Age\"].values.reshape(1,-1))\nx_cv_age=norm.transform(x_cv[\"Age\"].values.reshape(1,-1))\nx_test_age=norm.transform(x_test[\"Age\"].values.reshape(1,-1))\n\nx_train_age=x_train_age.reshape(-1,1)\nx_cv_age=x_cv_age.reshape(-1,1)\nx_test_age=x_test_age.reshape(-1,1)\n\n\nprint(\"after vectorization.......\")\nprint(x_train_age.shape,y_train.shape)\nprint(x_cv_age.shape,y_cv.shape)\nprint(x_test_age.shape,y_test.shape)\n","302ecc4d":"#Combining all the preprocessed data","99ead92c":"from scipy.sparse import hstack\n\nx_tr=hstack((x_train_rev,x_train_title,x_train_div,x_train_dep,x_train_cls,x_train_rate,x_train_age)).tocsr()\nx_cv=hstack((x_cv_rev,x_cv_title,x_cv_div,x_cv_dep,x_cv_cls,x_cv_rate,x_cv_age)).tocsr()\nx_te=hstack((x_test_rev,x_test_title,x_test_div,x_test_dep,x_test_cls,x_test_rate,x_test_age)).tocsr()\n\nprint(\"FINAL DATA MATRIX SHAPE IS ........\")\nprint(x_tr.shape,y_train.shape)\nprint(x_cv.shape,y_cv.shape)\nprint(x_te.shape,y_test.shape)\nprint(\"*\"*100)","84d220ef":"#This function takes classifier and data , and then gives probability_score of class label","6af25008":"def batch_predict(clf, data):\n    \"\"\"This function takes classifier and data , and then gives probability_score of class label\"\"\"\n    y_data_pred = []                                                 #list to store probability of class\n    tr_loop = data.shape[0] - data.shape[0]%1000                     #to loop through batchwise and took 1000 for batch\n    \n    for i in range(0, tr_loop, 1000):\n        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])     # this loop for upto last 1000 multiplier\n    \n    if data.shape[0]%1000 !=0:                                        #predicting for last data\n        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n    return y_data_pred","dfc6b233":"import math\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score\n\ntrain_auc = []    \ncv_auc = []\nalpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,50,100]         #to find the best alpha value\nfor i in tqdm(alpha):\n    neigh = MultinomialNB(alpha=i,class_prior=[0.5,0.5],fit_prior=False)\n    neigh.fit(x_tr, y_train)\n\n    y_train_pred = batch_predict(neigh, x_tr)               #finding probability scores of x_tr using batch_predict function\n    y_cv_pred = batch_predict(neigh, x_cv)                  ##finding probability scores of x_cv using batch_predict function\n\n          \n    train_auc.append(roc_auc_score(y_train,y_train_pred))   #appending auc values\n    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\n\n\n    \nplt.plot(alpha, train_auc, label='Train AUC')        #plotting hyper_parameter v\/s auc value plot....\nplt.plot(alpha, cv_auc, label='CV AUC')\nplt.xscale(\"log\")\nplt.scatter(alpha , train_auc, label='Train AUC points')\nplt.scatter(alpha , cv_auc, label='CV AUC points')\n\nplt.legend()\nplt.xlabel(\"log(alpha): hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"hyperparameter v\/s AUC plot to find BEST ALPHA\")\nplt.grid()\nplt.show()","9388e5ed":"#best alpha = 10^0 = 1\nbest_alpha_bow=1","20ff045e":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\n\nclas = MultinomialNB(alpha=best_alpha_bow,class_prior=[0.5,0.5],fit_prior=False)\nclas.fit(x_tr, y_train)\n\n\ny_train_pred = batch_predict(clas, x_tr)    \ny_test_pred = batch_predict(clas, x_te)\n\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\n\ntrain_auc=auc(train_fpr, train_tpr)\ntest_auc=auc(test_fpr, test_tpr)\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(train_auc))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(test_auc))\nplt.legend()\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"FPR v\/s TPR plot\")\nplt.grid()\nplt.show()","7fc1050f":"def find_best_threshold(threshold, fpr, tpr):\n    \"\"\"it will give best threshold value that will give the least fpr\"\"\"\n    t = threshold[np.argmax(tpr*(1-fpr))]\n    \n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    \n    return t\n\ndef predict_with_best_t(proba, threshold):\n    \"\"\"this will give predictions based on best threshold value\"\"\"\n    predictions = []\n    for i in proba:\n        if i>=threshold:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","771e97f0":"#computing confusion matrix for set_1\nfrom sklearn.metrics import confusion_matrix\nbest_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\nprint(\"Train confusion matrix\")\nm_tr=(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)))\nprint(m_tr)\nprint(\"Test confusion matrix\")\nm_te=(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))\nprint(m_te)\n#you are concerned that this cell produced fifferent result?\n#yes sir\n#no. if i run this code from the begining then it will be different result","1c41ebd6":"cm_tr = np.array(m_tr)\ncm_te=np.array(m_te)\n#plotting heatmap\nsns.heatmap(cm_tr, annot=True,fmt=\"d\",cmap='Blues')\nplt.xlabel(\"predict\")\nplt.ylabel(\"actual\")\nplt.show()","e0af9b78":"sns.heatmap(cm_te,annot=True,fmt=\"d\",cmap=\"BrBG\")\nplt.xlabel(\"predict\")\nplt.ylabel(\"actual\")\nplt.show()","21f13879":"#Lets calculate precision,recall, and f1 score\n\nprecision= m_te[1][1]\/(m_te[1][0]+m_te[1][1])\nprint(precision)\n\nrecall=m_te[1][1]\/(m_te[0][1]+m_te[1][1])\nprint(recall)\n\nf1=(2*precision*recall)\/(precision+recall)\nprint(f1)","ba1fac76":"index=[1]\nsummary_df=pd.DataFrame({\"Vectorizer\":(\"BOW\"),\"Hyper-parameter(alpha)\":(best_alpha_bow),\"Train_AUC\":(train_auc),\"Test_AUC\":(test_auc), \"Precision\":(precision),\"Recall\":(recall),\"F_1_Score\":(f1)},index=index)\nsummary_df","3ef6b4e2":"# Bag Of Words(BOW)","cd8c933a":"We can observe that the clothings which are rated 5 are highly recommended.. 10837 clothings which rated 5 are recommended out of 10858..<br>\nratings with 1 are not much recommended. only 7 clothings which are rated 1 are recommended out of 691..<br>***So we can say the higher the rating higher is the recommendation....***","3623d53b":"Dresses recommended 4314 times out of 5371 dresses reviews","e120c2df":"We have unnamed: 0 column which is unnecessary.","aea07164":"We have only 3 Divisions.. ","5e0c2aec":"# Let,s analyse.. (UNIVARIATE ANALYSIS)","7a11cf4b":"1]. When we use BOW vectorizer , we got 0.95 as test AUC.<br>     This means there is 95% chance that our model will be able to classify positive and negative points i.e. ***recommended*** or ***not recommended***<br>2]. We got TPR and TNR high, and FPR,FNR lower, this shows our model predicts pretty well.<br>","07625da4":"!].We found some NaN values. So we will drop that in the next step.","5443f594":"***The dataset is imbalanced. 1 means recommended. 0 means not recommended.***<br>\n1]. We have ***81.8%*** of ***recommended*** data points.... and only ***18.18%*** of ***not recommended*** data points...\n    ","7dfff9ba":"From the confusion matrix , we can observe that, TPR, TNR are high. FPR,FNR are significantly lower.<br>***So We got a good result***","438608b9":"We can observe that General division dresses recommended 9490 times out of 11664 of general dress reviews.","2b89268d":"# Cleaning the data......","a8f0a49e":"We can see the result of confusion matrix through ***HEAT MAP*** also.","e27c31fc":"We don't have any null values... great!!!!","748e0d36":"From this we could say that reviewers of age 39 recommended 962 clothings out of 1103. and they are the people who shopped most.","c0533b8a":"We can observe that, clothing_id 1078 is recommended 707 times out of 871 reviews.. <br>similarly Clothing_id 862 is recommended 534 times out of 658 reviews","60629ec5":"Most of the title contain only two words.. second highest is 3 words..","fbc0682d":"# PRE_PROCESSING.......","ecad8e3a":"Tops are recommended 7047 times out of 8713 tops reviews","8bdbfceb":"***Okayy.. Now lets check our class label i.e. Recommended IND, also % of recommendations........***"}}