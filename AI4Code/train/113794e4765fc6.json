{"cell_type":{"27ea0214":"code","63372d8d":"code","7224f040":"code","3e1ffba2":"code","941a833c":"code","17c160b1":"code","0adb50cf":"code","c870ab2a":"code","df57aacc":"code","436af10b":"code","5f4fbb60":"code","825809dc":"code","f1483e66":"code","3aa86c71":"code","be0a163c":"code","e708e5aa":"code","2cfb4ff5":"code","33ea4a7d":"code","90bfbb3b":"code","cc307672":"code","e11445b4":"code","0ab48ed9":"code","bfc5545c":"code","8c233e8b":"code","29f13d13":"code","1527159b":"code","ca4e8c0b":"code","2540723d":"code","f3566954":"code","26afd84f":"code","526945aa":"code","9c26c37b":"code","7dfff9cb":"code","17355ba0":"code","650a0cb9":"code","c62b4fc4":"code","bb95b337":"code","e9cd0a57":"code","6f6d2073":"code","e27f5d58":"code","642c13c5":"code","a46e9d85":"code","e58276b3":"code","8382adb9":"code","e3b0ff4b":"code","1d5ffb0c":"code","8d08944e":"code","40fae17a":"code","25e9a278":"code","9c096bb2":"code","2f8e7099":"code","04e55224":"code","f1bf6c97":"code","d89844bd":"code","f4e81618":"code","b3cb6478":"code","0e395cd4":"code","bf39b004":"code","c4dcc0e4":"code","bd529928":"code","9a0d309a":"code","9117fc5f":"code","7567e67a":"code","6a344b48":"code","20600327":"code","e6148306":"code","aab44f9a":"code","69411ec8":"code","0f7d0a67":"code","f7f12c5e":"code","12491c4c":"code","d5f31811":"code","aab67e0c":"code","775992ea":"code","9a56c7fc":"code","9e329af3":"code","1bbde12c":"markdown","37c2cf8c":"markdown","1875133d":"markdown","493ca804":"markdown","45955e57":"markdown","1f35e710":"markdown","132dab21":"markdown","305356c5":"markdown","c94d108d":"markdown","6b7e124d":"markdown","9adf6915":"markdown","c4fd56ec":"markdown","e94abf71":"markdown","677823a0":"markdown","9daffb91":"markdown","63af2b53":"markdown","edb35616":"markdown","afcdb02b":"markdown","6432ab8f":"markdown","07b4e50d":"markdown","08abc03e":"markdown","cb1f69de":"markdown","b7afed04":"markdown","ccf12181":"markdown","deddf419":"markdown","8f763390":"markdown"},"source":{"27ea0214":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nloan = pd.read_csv(\"..\/input\/loan-analysis-data\/loan.csv\", sep=\",\")\nloan.info()","63372d8d":"# let's look at the first few rows of the df\nloan.head()","7224f040":"# Looking at all the column names\nloan.columns","3e1ffba2":"# summarising number of missing values in each column\nloan.isnull().sum()","941a833c":"# percentage of missing values in each column\nround(loan.isnull().sum()\/len(loan.index), 2)*100","17c160b1":"# removing the columns having more than 90% missing values\nmissing_columns = loan.columns[100*(loan.isnull().sum()\/len(loan.index)) > 90]\nprint(missing_columns)","0adb50cf":"loan = loan.drop(missing_columns, axis=1)\nprint(loan.shape)\n\n","c870ab2a":"# summarise number of missing values again\n100*(loan.isnull().sum()\/len(loan.index))","df57aacc":"# There are now 2 columns having approx 32 and 64% missing values - \n# description and months since last delinquent\n\n# let's have a look at a few entries in the columns\nloan.loc[:, ['desc', 'mths_since_last_delinq']].head()","436af10b":"# dropping the two columns\nloan = loan.drop(['desc', 'mths_since_last_delinq'], axis=1)","5f4fbb60":"# summarise number of missing values again\n100*(loan.isnull().sum()\/len(loan.index))","825809dc":"# missing values in rows\nloan.isnull().sum(axis=1)","f1483e66":"# checking whether some rows have more than 5 missing values\nlen(loan[loan.isnull().sum(axis=1) > 5].index)","3aa86c71":"loan.info()","be0a163c":"# The column int_rate is character type, let's convert it to float\nloan['int_rate'] = loan['int_rate'].apply(lambda x: pd.to_numeric(x.split(\"%\")[0]))","e708e5aa":"# checking the data types\nloan.info()","2cfb4ff5":"# also, lets extract the numeric part from the variable employment length\n\n# first, let's drop the missing values from the column (otherwise the regex code below throws error)\nloan = loan[~loan['emp_length'].isnull()]\n\n# using regular expression to extract numeric values from the string\nimport re\nloan['emp_length'] = loan['emp_length'].apply(lambda x: re.findall('\\d+', str(x))[0])\n\n# convert to numeric\nloan['emp_length'] = loan['emp_length'].apply(lambda x: pd.to_numeric(x))","33ea4a7d":"# looking at type of the columns again\nloan.info()","90bfbb3b":"behaviour_var =  [\n  \"delinq_2yrs\",\n  \"earliest_cr_line\",\n  \"inq_last_6mths\",\n  \"open_acc\",\n  \"pub_rec\",\n  \"revol_bal\",\n  \"revol_util\",\n  \"total_acc\",\n  \"out_prncp\",\n  \"out_prncp_inv\",\n  \"total_pymnt\",\n  \"total_pymnt_inv\",\n  \"total_rec_prncp\",\n  \"total_rec_int\",\n  \"total_rec_late_fee\",\n  \"recoveries\",\n  \"collection_recovery_fee\",\n  \"last_pymnt_d\",\n  \"last_pymnt_amnt\",\n  \"last_credit_pull_d\",\n  \"application_type\"]\nbehaviour_var","cc307672":"# let's now remove the behaviour variables from analysis\ndf = loan.drop(behaviour_var, axis=1)\ndf.info()","e11445b4":"# also, we will not be able to use the variables zip code, address, state etc.\n# the variable 'title' is derived from the variable 'purpose'\n# thus let get rid of all these variables as well\n\ndf = df.drop(['title', 'url', 'zip_code', 'addr_state'], axis=1)","0ab48ed9":"df['loan_status'] = df['loan_status'].astype('category')\ndf['loan_status'].value_counts()","bfc5545c":"# filtering only fully paid or charged-off\ndf = df[df['loan_status'] != 'Current']\ndf['loan_status'] = df['loan_status'].apply(lambda x: 0 if x=='Fully Paid' else 1)\n\n# converting loan_status to integer type\ndf['loan_status'] = df['loan_status'].apply(lambda x: pd.to_numeric(x))\n\n# summarising the values\ndf['loan_status'].value_counts()","8c233e8b":"# default rate\nround(np.mean(df['loan_status']), 2)","29f13d13":"# plotting default rates across grade of the loan\nsns.barplot(x='grade', y='loan_status', data=df)\nplt.show()","1527159b":"# lets define a function to plot loan_status across categorical variables\ndef plot_cat(cat_var):\n    sns.barplot(x=cat_var, y='loan_status', data=df)\n    plt.show()\n    ","ca4e8c0b":"# compare default rates across grade of loan\nplot_cat('grade')","2540723d":"# term: 60 months loans default more than 36 months loans\nplot_cat('term')","f3566954":"# sub-grade: as expected - A1 is better than A2 better than A3 and so on \nplt.figure(figsize=(16, 6))\nplot_cat('sub_grade')","26afd84f":"# home ownership: not a great discriminator\nplot_cat('home_ownership')","526945aa":"# verification_status: surprisingly, verified loans default more than not verifiedb\nplot_cat('verification_status')","9c26c37b":"# purpose: small business loans defualt the most, then renewable energy and education\nplt.figure(figsize=(16, 6))\nplot_cat('purpose')","7dfff9cb":"# let's also observe the distribution of loans across years\n# first lets convert the year column into datetime and then extract year and month from it\ndf['issue_d'].head()","17355ba0":"from datetime import datetime\ndf['issue_d'] = df['issue_d'].apply(lambda x: datetime.strptime(x, '%b-%y'))\n","650a0cb9":"# extracting month and year from issue_date\ndf['month'] = df['issue_d'].apply(lambda x: x.month)\ndf['year'] = df['issue_d'].apply(lambda x: x.year)\n\n\n","c62b4fc4":"# let's first observe the number of loans granted across years\ndf.groupby('year').year.count()","bb95b337":"# number of loans across months\ndf.groupby('month').month.count()","e9cd0a57":"# lets compare the default rates across years\n# the default rate had suddenly increased in 2011, inspite of reducing from 2008 till 2010\nplot_cat('year')","6f6d2073":"# comparing default rates across months: not much variation across months\nplt.figure(figsize=(16, 6))\nplot_cat('month')","e27f5d58":"# loan amount: the median loan amount is around 10,000\nsns.distplot(df['loan_amnt'])\nplt.show()","642c13c5":"# binning loan amount\ndef loan_amount(n):\n    if n < 5000:\n        return 'low'\n    elif n >=5000 and n < 15000:\n        return 'medium'\n    elif n >= 15000 and n < 25000:\n        return 'high'\n    else:\n        return 'very high'\n        \ndf['loan_amnt'] = df['loan_amnt'].apply(lambda x: loan_amount(x))\n","a46e9d85":"df['loan_amnt'].value_counts()","e58276b3":"# let's compare the default rates across loan amount type\n# higher the loan amount, higher the default rate\nplot_cat('loan_amnt')","8382adb9":"# let's also convert funded amount invested to bins\ndf['funded_amnt_inv'] = df['funded_amnt_inv'].apply(lambda x: loan_amount(x))","e3b0ff4b":"# funded amount invested\nplot_cat('funded_amnt_inv')","1d5ffb0c":"# lets also convert interest rate to low, medium, high\n# binning loan amount\ndef int_rate(n):\n    if n <= 10:\n        return 'low'\n    elif n > 10 and n <=15:\n        return 'medium'\n    else:\n        return 'high'\n    \n    \ndf['int_rate'] = df['int_rate'].apply(lambda x: int_rate(x))","8d08944e":"# comparing default rates across rates of interest\n# high interest rates default more, as expected\nplot_cat('int_rate')","40fae17a":"# debt to income ratio\ndef dti(n):\n    if n <= 10:\n        return 'low'\n    elif n > 10 and n <=20:\n        return 'medium'\n    else:\n        return 'high'\n    \n\ndf['dti'] = df['dti'].apply(lambda x: dti(x))","25e9a278":"# comparing default rates across debt to income ratio\n# high dti translates into higher default rates, as expected\nplot_cat('dti')","9c096bb2":"# funded amount\ndef funded_amount(n):\n    if n <= 5000:\n        return 'low'\n    elif n > 5000 and n <=15000:\n        return 'medium'\n    else:\n        return 'high'\n    \ndf['funded_amnt'] = df['funded_amnt'].apply(lambda x: funded_amount(x))","2f8e7099":"plot_cat('funded_amnt')\n","04e55224":"# installment\ndef installment(n):\n    if n <= 200:\n        return 'low'\n    elif n > 200 and n <=400:\n        return 'medium'\n    elif n > 400 and n <=600:\n        return 'high'\n    else:\n        return 'very high'\n    \ndf['installment'] = df['installment'].apply(lambda x: installment(x))","f1bf6c97":"# comparing default rates across installment\n# the higher the installment amount, the higher the default rate\nplot_cat('installment')","d89844bd":"# annual income\ndef annual_income(n):\n    if n <= 50000:\n        return 'low'\n    elif n > 50000 and n <=100000:\n        return 'medium'\n    elif n > 100000 and n <=150000:\n        return 'high'\n    else:\n        return 'very high'\n\ndf['annual_inc'] = df['annual_inc'].apply(lambda x: annual_income(x))","f4e81618":"# annual income and default rate\n# lower the annual income, higher the default rate\nplot_cat('annual_inc')","b3cb6478":"# employment length\n# first, let's drop the missing value observations in emp length\ndf = df[~df['emp_length'].isnull()]\n\n# binning the variable\ndef emp_length(n):\n    if n <= 1:\n        return 'fresher'\n    elif n > 1 and n <=3:\n        return 'junior'\n    elif n > 3 and n <=7:\n        return 'senior'\n    else:\n        return 'expert'\n\ndf['emp_length'] = df['emp_length'].apply(lambda x: emp_length(x))","0e395cd4":"# emp_length and default rate\n# not much of a predictor of default\nplot_cat('emp_length')","bf39b004":"# purpose: small business loans defualt the most, then renewable energy and education\nplt.figure(figsize=(16, 6))\nplot_cat('purpose')","c4dcc0e4":"# lets first look at the number of loans for each type (purpose) of the loan\n# most loans are debt consolidation (to repay otehr debts), then credit card, major purchase etc.\nplt.figure(figsize=(16, 6))\nsns.countplot(x='purpose', data=df)\nplt.show()","bd529928":"# filtering the df for the 4 types of loans mentioned above\nmain_purposes = [\"credit_card\",\"debt_consolidation\",\"home_improvement\",\"major_purchase\"]\ndf = df[df['purpose'].isin(main_purposes)]\ndf['purpose'].value_counts()","9a0d309a":"# plotting number of loans by purpose \nsns.countplot(x=df['purpose'])\nplt.show()","9117fc5f":"# let's now compare the default rates across two types of categorical variables\n# purpose of loan (constant) and another categorical variable (which changes)\n\nplt.figure(figsize=[10, 6])\nsns.barplot(x='term', y=\"loan_status\", hue='purpose', data=df)\nplt.show()\n","7567e67a":"# lets write a function which takes a categorical variable and plots the default rate\n# segmented by purpose \n\ndef plot_segmented(cat_var):\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=cat_var, y='loan_status', hue='purpose', data=df)\n    plt.show()\n\n    \nplot_segmented('term')","6a344b48":"# grade of loan\nplot_segmented('grade')","20600327":"# home ownership\nplot_segmented('home_ownership')","e6148306":"# year\nplot_segmented('year')","aab44f9a":"# emp_length\nplot_segmented('emp_length')","69411ec8":"# loan_amnt: same trend across loan purposes\nplot_segmented('loan_amnt')","0f7d0a67":"# interest rate\nplot_segmented('int_rate')","f7f12c5e":"# installment\nplot_segmented('installment')","12491c4c":"# debt to income ratio\nplot_segmented('dti')","d5f31811":"# annual income\nplot_segmented('annual_inc')","aab67e0c":"# variation of default rate across annual_inc\ndf.groupby('annual_inc').loan_status.mean().sort_values(ascending=False)","775992ea":"# one can write a function which takes in a categorical variable and computed the average \n# default rate across the categories\n# It can also compute the 'difference between the highest and the lowest default rate' across the \n# categories, which is a decent metric indicating the effect of the varaible on default rate\n\ndef diff_rate(cat_var):\n    default_rates = df.groupby(cat_var).loan_status.mean().sort_values(ascending=False)\n    return (round(default_rates, 2), round(default_rates[0] - default_rates[-1], 2))\n\ndefault_rates, diff = diff_rate('annual_inc')\nprint(default_rates) \nprint(diff)\n","9a56c7fc":"# filtering all the object type variables\ndf_categorical = df.loc[:, df.dtypes == object]\ndf_categorical['loan_status'] = df['loan_status']\n\n# Now, for each variable, we can compute the incremental diff in default rates\nprint([i for i in df.columns])","9e329af3":"# storing the diff of default rates for each column in a dict\nd = {key: diff_rate(key)[1]*100 for key in df_categorical.columns if key != 'loan_status'}\nprint(d)","1bbde12c":"In general, debt consolidation loans have the highest default rates. Lets compare across other categories as well.","37c2cf8c":"Next, let's start with univariate analysis and then move to bivariate analysis.\n\n## Univariate Analysis\n\nFirst, let's look at the overall default rate.\n","1875133d":"The column description contains the comments the applicant had written while applying for the loan. Although one can use some text analysis techniques to derive new features from this column (such as sentiment, number of positive\/negative words etc.), we will not use this column in this analysis. \n\nSecondly, months since last delinquent represents the number months passed since the person last fell into the 90 DPD group. There is an important reason we shouldn't use this column in analysis - since at the time of loan application, we will not have this data (it gets generated months after the loan has been approved), it cannot be used as a predictor of default at the time of loan approval. \n\nThus let's drop the two columns.","493ca804":"## Data Analysis\n\nLet's now move to data analysis. To start with, let's understand the objective of the analysis clearly and identify the variables that we want to consider for analysis. \n\nThe objective is to identify predictors of default so that at the time of loan application, we can use those variables for approval\/rejection of the loan. Now, there are broadly three types of variables - 1. those which are related to the applicant (demographic variables such as age, occupation, employment details etc.), 2. loan characteristics (amount of loan, interest rate, purpose of loan etc.) and 3. Customer behaviour variables (those which are generated after the loan is approved such as delinquent 2 years, revolving balance, next payment date etc.).\n\nNow, the customer behaviour variables are not available at the time of loan application, and thus they cannot be used as predictors for credit approval. \n\nThus, going forward, we will use only the other two types of variables.\n\n","45955e57":"Let's analyse the top 4 types of loans based on purpose: consolidation, credit card, home improvement and major purchase.","1f35e710":"You can see that the number of loans has increased steadily across years. ","132dab21":"Next, let's have a look at the target variable - loan_status. We need to relabel the values to a binary form - 0 or 1, 1 indicating that the person has defaulted and 0 otherwise.\n\n","305356c5":"You can see that fully paid comprises most of the loans. The ones marked 'current' are neither fully paid not defaulted, so let's get rid of the current loans. Also, let's tag the other two values as 0 or 1. ","c94d108d":"The overall default rate is about 14%.  ","6b7e124d":"Some of the important columns in the dataset are loan_amount, term, interest rate, grade, sub grade, annual income, purpose of the loan etc.\n\nThe **target variable**, which we want to compare across the independent variables, is loan status. The strategy is to figure out compare the average default rates across various independent variables and identify the  ones that affect default rate the most.\n\n","9adf6915":"Clearly, as the grade of loan goes from A to G, the default rate increases. This is expected because the grade is decided by Lending Club based on the riskiness of the loan. ","c4fd56ec":"Let's now analyse how the default rate varies across continuous variables.","e94abf71":"The data looks clean by and large. Let's also check whether all columns are in the correct format.","677823a0":"A good way to quantify th effect of a categorical variable on default rate is to see 'how much does the default rate vary across the categories'. \n\nLet's see an example using annual_inc as the categorical variable.","9daffb91":"Most loans are granted in December, and in general in the latter half of the year.","63af2b53":"## Data Understanding","edb35616":"The easiest way to analyse how default rates vary across continous variables is to bin the variables into discrete categories.\n\nLet's bin the loan amount variable into small, medium, high, very high.","afcdb02b":"# Lending Club Default Analysis\n\nThe analysis is divided into four main parts:\n1. Data understanding \n2. Data cleaning (cleaning missing values, removing redundant columns etc.)\n3. Data Analysis \n4. Recommendations\n","6432ab8f":"Thus, there is a 6% increase in default rate as you go from high to low annual income. We can compute this difference for all the variables and roughly identify the ones that affect default rate the most.","07b4e50d":"In the upcoming analyses, we will segment the loan applications across the purpose of the loan, since that is a variable affecting many other variables - the type of applicant, interest rate, income, and finally the default rate. ","08abc03e":"There are some more columns with missing values, but let's ignore them for now (since we are ntot doing any modeling, we don't need to impute all missing values anyway). \n\nBut let's check whether some rows have a large number of missing values.","cb1f69de":"Typically, variables such as acc_now_delinquent, chargeoff within 12 months etc. (which are related to the applicant's past loans) are available from the credit bureau. ","b7afed04":"Let's first visualise the average default rates across categorical variables.\n","ccf12181":"## Segmented Univariate Analysis\n\nWe have now compared the default rates across various variables, and some of the important predictors are purpose of the loan, interest rate, annual income, grade etc.\n\nIn the credit industry, one of the most important factors affecting default is the purpose of the loan - home loans perform differently than credit cards, credit cards are very different from debt condolidation loans etc. \n\nThis comes from business understanding, though let's again have a look at the default rates across the purpose of the loan.\n","deddf419":"# Data Cleaning\n\nSome columns have a large number of missing values, let's first fix the missing values and then check for other types of data quality problems.","8f763390":"You can see that many columns have 100% missing values, some have 65%, 33% etc. First, let's get rid of the columns having 100% missing values."}}