{"cell_type":{"e3cdf7e6":"code","1ef1913d":"code","e93bec3b":"code","f916e4c1":"code","66b6330a":"code","ed72616e":"code","1ce11493":"code","c5cf990c":"code","1e2cf77c":"markdown","19f47915":"markdown","7589b7b0":"markdown","ef6f6b7b":"markdown"},"source":{"e3cdf7e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nfrom collections import deque\nimport sys\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.data as ds\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom functools import partial\n# uncomment for debugging layers\n# tf.config.run_functions_eagerly(True) \n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) \n# that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n@tf.function\ndef image_float_to_int(image): \n    return tf.image.convert_image_dtype(image, tf.uint8)\n\n\n@tf.function\ndef load_image(file_path, img_size=[306, 306]):\n    raw = tf.io.read_file(file_path)\n    return tf.image.decode_jpeg(raw, channels=3)\n\n\n@tf.function\ndef random_crop(image, size=128):\n    '''\n    crop a random box out of the image\n    :param image: image tensor to be modified\n    :param size: int. The pixel size of the cropped box\n    '''\n    s = tf.shape(image)\n    w = s[0]\n    h = s[1]\n    c = s[2]\n    dh = size\n    dw = size\n    dx = tf.random.uniform([1], minval=0, maxval=w-dw, dtype=tf.dtypes.int32, seed=None, name=None)[0]\n    dy = tf.random.uniform([1], minval=0, maxval=h-dh, dtype=tf.dtypes.int32, seed=None, name=None)[0]\n    offset_height = dy\n    target_height = dh\n    offset_width = dx\n    target_width = dw\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)\n    return image\n\n\n@tf.function\ndef box_delete(image, size=32, l=0.9):\n    '''\n    :param image: image tensor to be modified\n    :param strength: int. The pixel size of the deleted box\n    '''\n    s = tf.shape(image)\n    w = s[0]\n    h = s[1]\n    c = s[2]\n    # compute size and position of mask\n    dh = size\n    dw = size\n    dx = tf.random.uniform([1], minval=0, maxval=w-dw, dtype=tf.dtypes.int32, seed=None, name=None)[0]\n    dy = tf.random.uniform([1], minval=0, maxval=h-dh, dtype=tf.dtypes.int32, seed=None, name=None)[0]\n    # prepare indices\n    xs = tf.range(start=dx, limit=dx+dw)\n    ys = tf.range(start=dy, limit=dy+dh)\n    X, Y = tf.meshgrid(ys, xs)\n    X = tf.reshape(X, [-1, 1])\n    Y = tf.reshape(Y, [-1, 1])\n    mask_indices = tf.concat([Y, X], axis=-1)\n    mask_indices = tf.reshape(mask_indices, [-1, 2])\n    # prepare image patch\n    updates_hard = tf.random.uniform([dw*dh, c], minval=0, maxval=1, dtype=tf.dtypes.float32) # hard noise\n    updates_soft = tf.random.uniform([dw*dh, c], minval=0, maxval=0.1, dtype=tf.dtypes.float32) # soft noise\n    updates_black = tf.zeros([dw*dh, c], dtype=tf.dtypes.float32) # black (zeros)\n    # overwrite image with patch\n    modified_image = tf.tensor_scatter_nd_update(image, mask_indices, updates_black, name=None) # replace with noise\n    lam = tf.constant(l, dtype=tf.dtypes.float32)\n    one_minus_lam = tf.constant(1-l, dtype=tf.dtypes.float32)\n    updates_weights = tf.ones([dw*dh, c], dtype=tf.dtypes.float32) * one_minus_lam\n    weights = tf.tensor_scatter_nd_update(tf.ones_like(image, dtype=tf.dtypes.float32) * lam, mask_indices, updates_weights, name=None)\n    weights = tf.reshape(weights, [-1,])\n#     modified_image = tf.tensor_scatter_nd_add(image, mask_indices, updates_black, name=None) # add noise\n    return modified_image, image, weights\n\n@tf.function\ndef add_noise(image, strength=0.1):\n    '''\n    :param image: image tensor to be modified\n    :param strength: float, [0,1]. The amount of noise to add\n    '''\n    s = tf.shape(image)\n    w = s[0]\n    h = s[1]\n    c = s[2]\n    # prepare noise\n    noise = tf.random.uniform([w, h, c], minval=0, maxval=strength, dtype=tf.dtypes.float32) # soft noise\n    # add noise\n    modified_image = image + noise\n    weights = tf.ones_like(image, dtype=tf.dtypes.float32)\n    weights = tf.reshape(weights, [-1,])\n    return modified_image, image, weights\n\ndef flatten_labels(modified_image, image, weights):\n    flattened = tf.reshape(image, [-1,1])\n    return modified_image, flattened, weights\n\n## visualize the dataset\ndef visualize_dst(dst):\n    print_images = dst.take(9)\n    plt.figure(figsize=(10, 10))\n    for i, images, weights in enumerate(print_images):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images.numpy().astype(\"uint8\"))\n        #plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n    return dst\n\n\ndef visualize_training(dst):\n    dst = dst.take(3)\n    plt.figure(figsize=(6, 10))\n    i = 0\n    for image, gt, weights in dst:\n        image = tf.image.convert_image_dtype(image, tf.uint8)\n        ax = plt.subplot(3, 2, i + 1)\n        plt.imshow(image.numpy().astype(\"uint8\"))\n        i+=1\n        plt.axis(\"off\")\n        gt = tf.image.convert_image_dtype(gt, tf.uint8)\n        ax = plt.subplot(3, 2, i + 1)\n        plt.imshow(gt.numpy().astype(\"uint8\"))\n        i+=1\n        plt.axis(\"off\")\n        #plt.title(class_names[labels[i]])\n    return dst\n\ncopies_per_image = 1\nimages_per_batch = 1\ndataset = tf.data.Dataset.list_files('\/kaggle\/input\/selfies\/images\/*.jpg')\n# train\/test split\nimage_count = tf.cast(dataset.cardinality(), tf.float32)\ntrain_perc = tf.constant(0.8)\ntrain_dataset = dataset.take(tf.cast(tf.math.round(image_count * train_perc), tf.int64))\nval_dataset = dataset.skip(tf.cast(tf.math.round(image_count * train_perc), tf.int64))\ntrain_dataset = train_dataset.shuffle(buffer_size=1000)\nval_dataset = val_dataset.shuffle(buffer_size=1000)\n# train several times on each image (augmentations will be different)\ntrain_dataset = train_dataset.interleave(lambda x: tf.data.Dataset.from_tensors(x).repeat(copies_per_image),cycle_length=4, block_length=copies_per_image)\nval_dataset = val_dataset.interleave(lambda x: tf.data.Dataset.from_tensors(x).repeat(copies_per_image),cycle_length=4, block_length=copies_per_image)\n# read the images\ntrain_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).cache()\ntrain_dataset = train_dataset.map(partial(tf.image.convert_image_dtype, dtype=tf.float32))\nval_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).cache()\nval_dataset = val_dataset.map(partial(tf.image.convert_image_dtype, dtype=tf.float32))\n\n# prepare augmented images. dataset of (augmented image, image), block_length augmentions for each\ntrain_dataset = train_dataset.map(partial(random_crop, size=128), num_parallel_calls=tf.data.AUTOTUNE)\n# train_dataset = train_dataset.map(partial(box_delete, size=32, l=0.06), num_parallel_calls=tf.data.AUTOTUNE)\ntrain_dataset = train_dataset.map(partial(add_noise, strength=0), num_parallel_calls=tf.data.AUTOTUNE)\nval_dataset = val_dataset.map(partial(random_crop, size=128), num_parallel_calls=tf.data.AUTOTUNE)\n# val_dataset = val_dataset.map(partial(box_delete, size=32, l=0.06), num_parallel_calls=tf.data.AUTOTUNE)\nval_dataset = val_dataset.map(partial(add_noise, strength=0), num_parallel_calls=tf.data.AUTOTUNE)\n\n# visualize\n# train_dataset.apply(visualize_training)\n\n# flatten the original image (\"label\") so we can use sample weights\ntrain_dataset = train_dataset.map(partial(flatten_labels), num_parallel_calls=tf.data.AUTOTUNE)\nval_dataset = val_dataset.map(partial(flatten_labels), num_parallel_calls=tf.data.AUTOTUNE)\n# shuffle and batch\ntrain_dataset = train_dataset.batch(images_per_batch * copies_per_image, drop_remainder=True)\nval_dataset   = val_dataset.batch(images_per_batch * copies_per_image, drop_remainder=True)\n# prefetch\ntrain_dataset = train_dataset.prefetch(4)\nval_dataset = val_dataset.prefetch(4)\n\nprint(\"done dataset preparations\")\nprint(train_dataset)","1ef1913d":"# inspect graphs to find if memory leak is here\ntf.executing_eagerly()\n","e93bec3b":"class ConvBlock(keras.layers.Layer):\n    '''\n    convolution block for unet with optional max pooling\n    '''\n    def __init__(self, filters, downsample=True, bname='', **kwargs):\n        self.filters = filters\n        self.bname = bname\n        self.downsample = downsample\n        super(ConvBlock, self).__init__(**kwargs)\n        self.conv1 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=(1, 1),\n                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n                            bias_initializer=\"zeros\", name=bname+\"_conv_1\")\n        self.conv2 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=(1, 1),\n                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n                            bias_initializer=\"zeros\", name=bname+\"_conv_2\")\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'filters': self.filters,\n            'bname': self.bname,\n            'downsample': self.downsample\n        })\n        return config\n    \n    \n    @tf.function\n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = keras.layers.ReLU(name=self.bname+\"_relu_1\")(x)\n        # stride 2, padding 1, k=3, o=i\/2\n#         x = keras.layers.ZeroPadding2D(padding=(1, 1), data_format='channels_last', name=self.bname+\"_zeroPad\")(x) \n        x = self.conv2(x)\n        x = keras.layers.ReLU(name=self.bname+\"_relu_2\")(x)\n        p = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid', \n                                      name=self.bname+\"_maxpool\")(x)\n        return x, p\n\n\nclass DeConvBlock(keras.layers.Layer):\n    '''\n    convolution block for unet with optional max pooling\n    '''\n    def __init__(self, filters, bname='', **kwargs):\n        self.filters = filters\n        self.bname = bname\n        super(DeConvBlock, self).__init__(**kwargs)\n        self.conv1 = keras.layers.Conv2DTranspose(filters=self.filters, kernel_size=(3,3), strides=2,\n                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n                            bias_initializer=\"zeros\", name=bname+\"_deconv\")\n        self.conv2 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=1,\n                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n                            bias_initializer=\"zeros\", name=bname+\"_conv\")\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'filters': self.filters,\n            'bname': self.bname\n        })\n        return config\n    \n    @tf.function\n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = keras.layers.ReLU(name=self.bname+\"_relu_1\")(x)\n        x = self.conv2(x)\n        x = keras.layers.ReLU(name=self.bname+\"_relu_2\")(x)\n        return x\n\nclass PaddedBase(keras.layers.Layer):\n    '''\n    base class for layers involving padding one input to match the other\n    '''\n    def build(self, input_shape):\n        self.shape1 = input_shape[0]\n        self.shape2 = input_shape[1]\n        dh = self.shape1[1] - self.shape2[1]\n        dw = self.shape1[2] - self.shape2[2]\n        self.bigger_shape = self.shape1[:3]\n        self.smaller_shape = self.shape2[:3]\n        self.c1 = self.shape1[3]\n        self.c2 = self.shape2[3]\n        self.pred = tf.logical_or(tf.math.less(dw, 0), tf.math.less(dh, 0))\n        if self.pred:\n            dh = - dh\n            dw = - dw\n            self.bigger_shape = self.shape2[:3]\n            self.smaller_shape = self.shape1[:3]\n        hmod = tf.math.floormod(dh, 2)\n        hdiv = tf.cast(tf.math.divide(dh - hmod, 2), tf.int32)\n        wmod = tf.math.floormod(dw, 2)\n        wdiv = tf.cast(tf.math.divide(dw - wmod, 2), tf.int32)\n        dhup = hdiv\n        dhdown = hdiv + hmod\n        dwleft = wdiv\n        dwright = wdiv + wmod\n        padB = tf.constant([0, 0])\n        padh = tf.stack([dhup, dhdown], axis=-1)\n        padw = tf.stack([dwleft, dwright], axis=-1)\n        padC = tf.constant([0, 0])\n        self.paddings = tf.stack([padB, padh, padw, padC], axis=0)\n\nclass PadToSize(PaddedBase):\n    '''\n    padding the smaller tensor so the shapes match and return the first\n    '''\n    @tf.function\n    def call(self, inputs):\n        tensor_1, tensor_2 = inputs\n        tensor_3 =  tf.pad(tensor_1, paddings=self.paddings , mode='CONSTANT', constant_values=0)\n        return tensor_3\n        \nclass PaddedConcat(PaddedBase):\n    '''\n    concats two tensors, padding the smaller tensor so the shapes match\n    ''' \n    @tf.function\n    def call(self, inputs):\n        tensor_1, tensor_2 = inputs\n        tensor_3 =  tf.pad(tensor_1, paddings=self.paddings , mode='CONSTANT', constant_values=0)\n        out = tf.concat([tensor_3, tensor_2], axis=-1)\n        return out\n\nclass PaddedAdd(PaddedBase):\n    '''\n    concats two tensors, padding the smaller tensor so the shapes match\n    '''\n    @tf.function\n    def call(self, inputs):\n        tensor_1, tensor_2 = inputs\n        tensor_3 =  tf.pad(tensor_1, paddings=self.paddings , mode='CONSTANT', constant_values=0)\n        out = tf.math.add(tensor_3, tensor_2)\n        return out\n    \nclass CropToShape(keras.layers.Layer):\n    '''\n    crop input to specified shape\n    '''\n    def build(self, input_shape):\n        shape1 = input_shape[0]\n        shape2 = input_shape[1]\n        self.h = shape2[1]\n        self.w = shape2[2]\n    \n    @tf.function\n    def call(self, inputs):\n        return tf.slice(inputs[0], [0, 0, 0, 0], [-1, self.h, self.w, -1])\n\n    \ndef bridge(x, bridge_features, num=0):\n    if len(bridge_features) > 0:\n        name = \"bridge_layer_\"+str(num)\n        x = keras.layers.Conv2D(filters=bridge_features[0], kernel_size=(3,3), strides=(1, 1),\n                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n                            bias_initializer=\"zeros\", name=name+\"_conv\")(x)\n        x = keras.layers.ReLU(name=name+\"_relu\")(x)\n        return bridge(x, bridge_features[1:], num+1)\n    else:\n        return x\n        \n    \ndef Unet(x, bridge_features, encoder_filters_list, decoder_filters_list, skip_connections, level=0):\n    tf.debugging.assert_shapes([(encoder_filters_list,('N')),(decoder_filters_list,('N'))],\n                               message=\"encoder and decoder have different length!\")\n    #encoder\n    bname = \"encoder_layer_\"+str(level)\n    xout, p = ConvBlock(encoder_filters_list[0], bname=bname)(x)\n    if len(encoder_filters_list) > 1:\n        x = Unet(p, bridge_features, encoder_filters_list[1:], \n                 decoder_filters_list[:-1], skip_connections[:-1], level=level+1)\n    else:\n        x = bridge(p, bridge_features, num=0)\n    #decoder\n    bname = \"decoder_features_\"+str(level)\n    x = DeConvBlock(decoder_filters_list[-1], bname=bname)(x)\n    if skip_connections[-1]==1:\n        # skip connection\n        print(\"x:\", x)\n        print(\"xout:\", xout)\n        x = PaddedConcat(name=bname+\"_concat\")([x, xout])\n#         x = PaddedAdd(name=bname+\"_add\")([x, xout])\n    else:\n        # no skip connection here so just padd as necessary\n        x = PadToSize()([x, xout])\n    return x\n    \n\ndef AutoEncoder(x, bridge_features, encoder_filters_list, decoder_filters_list, level=0):\n    tf.debugging.assert_shapes([(encoder_filters_list,('N')),(decoder_filters_list,('N'))],\n                               message=\"encoder and decoder have different length!\")\n    #encoder\n    bname = \"encoder_layer_\"+str(level)\n    xout, p = ConvBlock(encoder_filters_list[0], bname=bname)(x)\n    if len(encoder_filters_list) > 1:\n        x = AutoEncoder(p, bridge_features, encoder_filters_list[1:], decoder_filters_list[:-1], level=level+1)\n    else:\n        x = bridge(p, bridge_features, num=0)\n    #decoder\n    bname = \"decoder_features_\"+str(level)\n    x = DeConvBlock(decoder_filters_list[-1], bname=bname)(x)  \n    x = PadToSize()([x, xout])\n    return x\n        \n\ndef get_completion_model(bridge_features, encoder_filters_list, decoder_filters_list, skip_connections,\n                         head_filters_list, net=\"unet\"):\n    image = keras.layers.Input(shape=[128,128, 3], dtype='float32', name=\"input_org\")\n    features = Unet(image, bridge_features, encoder_filters_list, decoder_filters_list, skip_connections)\n#     features = AutoEncoder(image, bridge_features, encoder_filters_list, decoder_filters_list)\n    kernel_size = (1,1) #(3,3)\n    for i, f in enumerate(head_filters_list):\n        features_in = features\n        features = keras.layers.Conv2D(filters=f, kernel_size=kernel_size, strides=(1, 1),\n                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n                            bias_initializer=\"zeros\")(features)\n        features = keras.layers.ReLU(name=\"head_conv_\"+str(i)+\"_conv_out\")(features)\n    recovered_image = features\n    flattened_recovered_image = tf.keras.layers.Reshape([-1,1])(recovered_image)\n    return keras.Model(inputs=image, outputs=flattened_recovered_image)\n\ndef get_simple_model():\n    image = keras.layers.Input(shape=[128,128, 3], dtype='float32', name=\"input_org\")\n    features = keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(image)\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"valid\", data_format='channels_last')\n    features = keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"valid\", data_format='channels_last')\n    features = keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"valid\", data_format='channels_last')\n    features = keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n    tf.keras.layers.UpSampling2D(size=(2, 2), data_format='channels_last', interpolation=\"nearest\")\n    features = keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n    tf.keras.layers.UpSampling2D(size=(2, 2), data_format='channels_last', interpolation=\"nearest\")\n    features = keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n    tf.keras.layers.UpSampling2D(size=(2, 2), data_format='channels_last', interpolation=\"nearest\")\n    features = keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n    features = keras.layers.Conv2D(filters=3, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n    flattened_recovered_image = tf.keras.layers.Reshape([-1,1])(features)\n    return keras.Model(inputs=image, outputs=flattened_recovered_image)\n\nclass PixelWiseHuberVisualize(keras.losses.Loss):\n    def call(self, img_true, img_pred):\n        #img_pred = tf.reshape(tf.convert_to_tensor_v2(img_pred), [-1, 1])\n#         img_pred = tf.reshape(img_pred, [-1, 1])\n#         img_true = tf.reshape(tf.cast(img_true, img_pred.dtype), [-1, 1])\n        # image values are in [0,1] so put delta in the middle\n        h = tf.math.squared_difference(img_pred, img_true)\n#         h = tf.keras.losses.Huber(delta=0.5, reduction=keras.losses.Reduction.NONE)\n#         return h(img_pred, img_true)\n        return h\n\n\nclass PixelWiseHuber(keras.losses.Loss):\n    @tf.function\n    def call(self, img_true, img_pred):\n        #img_pred = tf.reshape(tf.convert_to_tensor_v2(img_pred), [-1, 1])\n#         img_pred = tf.reshape(img_pred, [-1, 1])\n#         img_true = tf.reshape(tf.cast(img_true, img_pred.dtype), [-1, 1])\n        # image values are in [0,1] so put delta in the middle\n        shape1 = tf.shape(img_pred)\n        shape2 = tf.shape(img_true)\n#         tf.print(\"pred shape=\", shape1, \"original shape=\", shape2)\n        return tf.keras.losses.Huber(delta=0.5)(img_pred, img_true)","f916e4c1":"# inspect graphs to find if memory leak is here","66b6330a":"@tf.function\ndef add_gaussian_noise(image, invSNR):\n    A = tf.norm(image)\n    std = invSNR * A\n    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=std)\n    return tf.math.add(image, noise)\n\n\ndef visualize_loss(dst):\n    original = dst.take(1)\n\n    plt.figure(figsize=(10, 10))\n    for original in original:\n        aug, org, w = original\n        #     loss_image = original.map(tf.math.squared_difference)\n        org = tf.reshape(org, tf.shape(aug))\n        loss_image = keras.losses.huber(aug, org)\n        loss_image = tf.image.convert_image_dtype(loss_image, tf.uint8)\n        aug = tf.image.convert_image_dtype(aug, tf.uint8)\n        org = tf.image.convert_image_dtype(org, tf.uint8)\n        ax = plt.subplot(1,3,1)\n        plt.imshow(loss_image.numpy().astype(\"uint8\"))\n        ax = plt.subplot(1,3,2)\n        plt.imshow(aug.numpy().astype(\"uint8\"))\n        ax = plt.subplot(1,3,3)\n        plt.imshow(org.numpy().astype(\"uint8\"))\n    return dst\n\ntrain_dataset.unbatch().apply(visualize_loss)","ed72616e":"# Load the extension and start TensorBoard\n#%load_ext tensorboard\n\n# AE V1\n#bridge_features = [16]\n# encoder_filters_list = [ 64, 64, 128, 128, 256, 256]\n# decoder_filters_list = [256, 256, 128, 128, 64, 64]\n# head_filters_list = [32, 32, 16, 3]\nbridge_features = [16, 16, 16, 16]\nencoder_filters_list = [128, 64, 32, 32, 32]\nskip_connections =     [1 ,  1 , 1,  0,  0]\ndecoder_filters_list = [32, 32, 32,  64, 128]\nhead_filters_list = [3]\n# net=\"AE\"\nnet=\"unet\"\n\ncp_file = \"\/kaggle\/working\/image_completion_model.h5\"\nif os.path.exists(cp_file):\n    print(\"found checkpoint, loading\")\n    model = tf.keras.models.load_model(cp_file, \n                               custom_objects={'ConvBlock': ConvBlock, 'DeConvBlock': DeConvBlock, \n                                               'PadToSize': PadToSize, 'PixelWiseHuber': PixelWiseHuber,\n                                               'PaddedConcat': PaddedConcat, 'PaddedAdd': PaddedAdd})\nelse:\n    print(\"checkpoint not found, compiling model\")\n#     model = get_completion_model(bridge_features, encoder_filters_list, decoder_filters_list, skip_connections,\n#                                  head_filters_list, net=net)\n    model = get_simple_model() # for debugging memory leak and comparison with our model\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=0.001) # default lr, use reduction strategy below\n#     opt = tf.keras.optimizers.Adam(learning_rate=0.0001, epsilon=0.001) # default lr, use reduction strategy below\n#     opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.3, nesterov=True) # default lr, use reduction strategy below\n#     opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, rho=0.9, momentum=0.1, epsilon=1e-06)\n#     model.compile(optimizer=opt, loss=PixelWiseHuber())\n    model.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy'])\n    \n# checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(cp_file, verbose=1)\n# import gc\n# class GCCallback(tf.keras.callbacks.Callback):\n#   def on_epoch_end(self, epoch, logs=None):\n#     gc.collect()\n\n# garbage_collect_cb = GCCallback()\n# lr_strategy_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.1, patience=3, verbose=1,\n#                                                                 mode=\"min\", min_delta=0.001, cooldown=0, min_lr=0.0000001)\nmodel.summary()\nhistory = model.fit(x=train_dataset, steps_per_epoch=2000, epochs=15, validation_data=val_dataset, validation_steps=5, validation_freq=10,\n                        workers=3\n#                         , callbacks=[\n#                             checkpoint_cb, #garbage_collect_cb,\n#                             lr_strategy_callback]\n                   )\n#%tensorboard --logdir \/kaggle\/working\/runs\/31_5\/logs\n# summarize history for loss\n# print(history.history['val_loss'])\nplt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('\/kaggle\/working\/train_history.png')","1ce11493":"# model.save('\/kaggle\/working\/runs\/7_7\/saved_model')\n#!kill 172\n#%tensorboard --logdir \"\/kaggle\/working\/runs\/31_5\/logs\"\n# os.remove(cp_file)\nos.listdir(\"\/kaggle\/working\")","c5cf990c":"vis_val = val_dataset.unbatch().shard(num_shards=100, index=0)\ndef do_model(x, y, z, model):\n    #x, y = inputs\n#     y = x\n    shape = tf.shape(x)\n    x = tf.expand_dims(x, axis=0)\n    x = model(x)\n    x = tf.reshape(x, shape)\n    y = tf.reshape(y, shape)\n    return x, y, z\ndo_my_model = lambda x, y, z: do_model(x, y, z, model)\nvis_val = vis_val.map(do_my_model)\nprint(vis_val)\nvis_val.apply(visualize_training)\nvis_val.apply(visualize_training)","1e2cf77c":"Test the loss function to see that it gives correct values. Test by adding noise to an image and plotting the loss as a function of SNR:","19f47915":"visualize results","7589b7b0":"Now that we have an input pipeline and we can see and verify our data, lets create a model.\nGiven image A and its distorted version D(A), compute a recovered version R(D(A)) which is as close as possible to the original A:\nargmin_W{|A-R_W(D(A))|^2}\n\nWe will go with a unet model which will encode the features to each pixel. We will then train the DNN on the above task.\nThus for an input x the model will output:\nf(x)=En(x) - the pixel wise features\nR(x)=De(x) - the recovered picture","ef6f6b7b":"Now lets train. params:\n- unet_num_features\n- unet_filters_list\n- head_filters_list\nNote that the head capabilities strongly depend on how many filters are applied: The maximum \"interpolation length\" is ~ 3*num_filters"}}