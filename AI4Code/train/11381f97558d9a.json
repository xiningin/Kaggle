{"cell_type":{"c7834e2e":"code","82fc8cc6":"code","430ddc7b":"code","f3da67e0":"code","f792cf44":"code","01a040dd":"code","42703bdc":"code","273d03b3":"code","8b08ee06":"code","ef0de5a0":"code","0921e80b":"code","132daeff":"code","9fdf3f43":"code","4e7d3d03":"code","29c79abf":"code","085aa1b8":"code","9e6683ee":"code","23d8a383":"code","0f6f2639":"code","1c374780":"code","8f665799":"code","267ae837":"code","b6eb4fe0":"code","60b7895b":"code","069b9b18":"code","5cbb055f":"code","bbe299f2":"code","57d2e2c3":"code","5be5ed36":"code","473f38a6":"code","04c51f34":"code","08ca1709":"code","03aa28ab":"code","de6e36c5":"code","89194b7e":"code","4c6f327e":"code","b470463a":"code","d0fc59f8":"code","f8bb8d06":"code","597ade9a":"code","76f2843f":"code","240ceb8a":"code","cd0aaf45":"code","85c74395":"code","9e9d47a2":"markdown","1fbe972f":"markdown","5cc6b490":"markdown","79507b49":"markdown","1dc8b63d":"markdown","2e34ba75":"markdown","b0eeba6f":"markdown","27bafb97":"markdown","bc02576c":"markdown","a41b98e6":"markdown","b16d93e8":"markdown","a0eb76a4":"markdown","6b52c275":"markdown","dea47add":"markdown","cf1963ca":"markdown","6452f721":"markdown","5c3e78c4":"markdown","07ebc18a":"markdown","391cd6b4":"markdown","c06c24ed":"markdown","b6e5316e":"markdown","2c47597b":"markdown","a3c8ac6b":"markdown","33c3add4":"markdown","b15adee1":"markdown","85a1f163":"markdown","36a10d81":"markdown","4f8678f4":"markdown","0748ffd8":"markdown"},"source":{"c7834e2e":"import os\nimport cv2\nimport time\nimport numpy as np\nimport pandas as pd\nimport random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n%matplotlib inline","82fc8cc6":"ROOT_DIR = \"..\/input\/vinbigdata-chest-xray-abnormalities-detection\"\nTRAIN_DIR = \"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\"\nTEST_DIR = \"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/test\"","430ddc7b":"df = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))","f3da67e0":"df.head()","f792cf44":"print(f\"Shape of the Dataframe: {df.shape}\")","01a040dd":"print(f\"Number of unique images in the training dataset: {df['image_id'].nunique()}\")","42703bdc":"test_images = os.listdir(TEST_DIR)\nprint(f\"Number of unique images in the training dataset: {len(test_images)}\")","273d03b3":"print(f\"The dataset consists of observations made by {df['rad_id'].nunique()} radiologists\")","8b08ee06":"rad_df = df['rad_id'].value_counts().reset_index()\nfig = go.Figure(data=[go.Table(header=dict(values=['Radiologist ID', 'Number of Observations'], fill_color='yellow'),\n                 cells=dict(values=[rad_df['index'], rad_df['rad_id']], fill_color='lavender'))\n                     ])\nfig.show()","ef0de5a0":"cnt = Counter(df['image_id'])","0921e80b":"cnt","132daeff":"max_boxes_image = max(cnt, key=cnt.get)\nprint(f\"Image ID of image with with maximum boxes is: \\'{max_boxes_image}\\' and number of boxes is {cnt[max_boxes_image]}\")","9fdf3f43":"# Code taken from https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data","4e7d3d03":"def show_dicom(image_id, root_dir=TRAIN_DIR):\n    image_path = os.path.join(root_dir, image_id+\".dicom\")    \n    img = read_xray(image_path)\n    plt.figure(figsize = (12,12))\n    plt.imshow(img, 'gray')","29c79abf":"def get_all_bboxes(df, image_id):\n    image_bboxes = df[df.image_id == image_id]\n    bboxes = []\n    for _,row in image_bboxes.iterrows():\n        bboxes.append((row.x_min, row.y_min, row.x_max, row.y_max))  \n    return bboxes\n\ndef plot_single_image(image_id, df):\n    show_dicom(image_id)\n    bboxes = get_all_bboxes(df, image_id)\n    for i, bbox in enumerate(bboxes): \n        patch = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1],\n                                    edgecolor='r',linewidth=1., facecolor='none')\n        ax = plt.gca()\n        ax.add_patch(patch)","085aa1b8":"df_max = df[df['image_id'] == max_boxes_image].copy()\ndf_max = df_max.reset_index(drop=True)","9e6683ee":"plot_single_image(max_boxes_image, df_max)","23d8a383":"df_max.rad_id.value_counts()","0f6f2639":"# Code taken from https:\/\/www.pyimagesearch.com\/2016\/11\/07\/intersection-over-union-iou-for-object-detection\/\n\ndef bb_intersection_over_union(boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea \/ float(boxAArea + boxBArea - interArea)\n    # return the intersection over union value\n    return iou","1c374780":"for i in range(len(df_max)):\n    for j in range(i+1, len(df_max)):\n        boxA = df_max.iloc[i, 4:].tolist()\n        boxB = df_max.iloc[j, 4:].tolist()\n        iou = bb_intersection_over_union(boxA, boxB)\n        if iou >= 0.5:\n            print(iou)\n            print(df_max.iloc[i, 1:4])\n            print(df_max.iloc[j, 1:4])\n            print(\"-\"*25, end='\\n')","8f665799":"classes = [\"Aortic enlargement\", \"Atelectasis\", \"Calcification\", \"Cardiomegaly\", \"Consolidation\", \"ILD\", \"Infiltration\", \"Lung Opacity\",\n           \"Nodule\/Mass\", \"Other lesion\", \"Pleural effusion\", \"Pleural thickening\", \"Pneumothorax\", \"Pulmonary fibrosis\", \"No finding\"]","267ae837":"fig = go.Figure(data=[go.Table(header=dict(values=['Class ID', 'Class Name'], fill_color='yellow'),\n                 cells=dict(values=[list(range(15)), classes], fill_color='lavender'))\n                     ])\nfig.show()","b6eb4fe0":"temp_df = df[\"class_id\"].value_counts().sort_index()\n\ntrace1 = go.Bar(\n                x = classes,\n                y = temp_df.tolist(),\n                marker = dict(color = 'rgb(127, 16, 238)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df.tolist(), textposition='outside')\nlayout = go.Layout(template= \"plotly_dark\",title = 'Number of classes' , xaxis = dict(title = 'Class'), yaxis = dict(title = 'Count'))\nfig = go.Figure(data = [trace1], layout = layout)\nfig.show()","60b7895b":"df_findings = df[df[\"class_id\"] != 14].copy()","069b9b18":"df_findings['bbox_area'] = (df_findings[\"x_max\"] - df_findings[\"x_min\"]) * (df_findings[\"y_max\"] - df_findings[\"y_min\"])","5cbb055f":"print(f\"Total number of bounding boxes present in the dataset is {len(df_findings)}\")","bbe299f2":"print(f\"Maximum area of bounding box is {df_findings['bbox_area'].max()}\")\nprint(f\"Minimum area of bounding box is {df_findings['bbox_area'].min()}\")","57d2e2c3":"trace = go.Histogram(\n    x=df_findings['bbox_area'],\n    name = \"Bounding Box Area\",\n    xbins=dict(size=30000),\n    marker=dict(color='rgb(12, 50, 196)'))\nlayout = go.Layout(template= \"plotly_dark\",title = 'Bounding Box Area' , xaxis = dict(title = 'Area'), yaxis = dict(title = 'Count'))\nfig = go.Figure(data = [trace], layout = layout)\nfig.show()","5be5ed36":"df_findings.drop('class_id',axis=1).groupby('class_name').agg(['min', 'max', 'mean','median'])","473f38a6":"def plot_k_images(df, k=3):\n    image_ids = random.choices(df['image_id'].tolist(), k=3)\n    for image_id in image_ids:\n        plot_single_image(image_id, df)","04c51f34":"df_0 = df[df.class_id == 0]\nplot_k_images(df_0)","08ca1709":"df_1 = df[df.class_id == 1]\nplot_k_images(df_1)","03aa28ab":"df_2 = df[df.class_id == 2]\nplot_k_images(df_2)","de6e36c5":"df_3 = df[df.class_id == 3]\nplot_k_images(df_3)","89194b7e":"df_4 = df[df.class_id == 4]\nplot_k_images(df_4)","4c6f327e":"df_5 = df[df.class_id == 5]\nplot_k_images(df_5)","b470463a":"df_6 = df[df.class_id == 6]\nplot_k_images(df_6)","d0fc59f8":"df_7 = df[df.class_id == 7]\nplot_k_images(df_7)","f8bb8d06":"df_8 = df[df.class_id == 8]\nplot_k_images(df_8)","597ade9a":"df_9 = df[df.class_id == 9]\nplot_k_images(df_9)","76f2843f":"df_10 = df[df.class_id == 10]\nplot_k_images(df_10)","240ceb8a":"df_11 = df[df.class_id == 11]\nplot_k_images(df_11)","cd0aaf45":"df_12 = df[df.class_id == 12]\nplot_k_images(df_12)","85c74395":"df_13 = df[df.class_id == 13]\nplot_k_images(df_13)","9e9d47a2":"# Bounding Box Area","1fbe972f":"There are more than 1 box in an image so it is an Object Detection Competition","5cc6b490":"# Maximum number of Boxes in an Image","79507b49":"# Pulmonary Fibrosis","1dc8b63d":"# Infiltration","2e34ba75":"## There are 18 combinations of boxes with IoU Score above 0.5\n## Most of them correspond to same class but have different annotators\n## But there are 2 instances where **\"ILD\"** class has very high IoU with **\"Nodule\/Mass\"** ","b0eeba6f":"## Work in Progress ","27bafb97":"![Chest XRAY](https:\/\/media.springernature.com\/lw685\/springer-static\/image\/art%3A10.1186%2Fs12890-020-01286-5\/MediaObjects\/12890_2020_1286_Fig1_HTML.png)","bc02576c":"* **Nodule\/Mass** has the smallest bounding boxes\n* **Pneumothorax** and **ILD** have the largest bounding boxes","a41b98e6":"# Calcification","b16d93e8":"# Cardiomegaly","a0eb76a4":"## If you like this kernel, please leave an upvote :)","6b52c275":"## Let's Visualize this image\nAll the images are in **DICOM** format","dea47add":"#### We have 3 different annotators which maybe the reason of so many boxes having very high IoU","cf1963ca":"# Class Distribution","6452f721":"# Lung Opacity","5c3e78c4":"# Pleural Effusion","07ebc18a":"# ILD","391cd6b4":"Let's get the image_id of the image with maximum bounding boxes","c06c24ed":"# Pneumothorax","b6e5316e":"# Pleural thickening","2c47597b":"# Other lesion","a3c8ac6b":"# Competition Aim\nLocalize and classify 14 types of thoracic abnormalities from chest radiographs\n## This is an Object Detection Competition","33c3add4":"# Atelectasis","b15adee1":"### There are many Bounding Boxes with very high overlap\n### Let's investigate further","85a1f163":"# Visualizing Images Classwise","36a10d81":"# Nodule\/Mass","4f8678f4":"# Aortic Enlargement","0748ffd8":"# Consolidation"}}