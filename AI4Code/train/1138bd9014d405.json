{"cell_type":{"085c98ba":"code","f5c4a3c9":"code","d5c0e9c9":"code","7ad42ff3":"code","af6e108d":"code","a97b6d33":"code","adebead8":"code","9f0d25ac":"code","04f0914a":"code","88569148":"code","586da441":"code","228dd5b9":"code","a602dbb4":"code","94db7be4":"code","38af7879":"code","c836c688":"code","4e4a0fc2":"code","ab930dca":"code","91f99fbd":"code","df805913":"code","2c2c11f2":"code","f6aa3744":"code","b8ad67ed":"code","9ed93e97":"code","e1244e24":"code","74ab378c":"code","2e87e111":"code","dfb9da3c":"code","d4e228b4":"code","e97d3f38":"code","4017b252":"code","28b7ac86":"code","01a42c80":"code","ba98fba5":"code","f886a9d2":"code","2e73ff22":"markdown","76318335":"markdown","9fa117bf":"markdown","a166aed2":"markdown","d9024e21":"markdown","a67d4fd4":"markdown","88aca542":"markdown","5624811c":"markdown","a8c51b3d":"markdown","5ad257c4":"markdown","7582d1ce":"markdown","94d9d971":"markdown","c3c42c53":"markdown","71ba3a8e":"markdown","19f37565":"markdown","cc039ff7":"markdown"},"source":{"085c98ba":"import os\nimport re\nimport string\nimport pandas as pd\nimport numpy as np\nimport random\n\n\nfrom time import time\nfrom gensim.models import KeyedVectors\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport itertools\nimport datetime\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, LSTM, Lambda\nimport keras.backend as K\nfrom keras.callbacks import ModelCheckpoint\n","f5c4a3c9":"dataset = pd.read_excel('..\/input\/train.xlsx')\ntest=pd.read_excel('..\/input\/test.xlsx')\ndataset.head()","d5c0e9c9":"dataset.groupby('user').count()","7ad42ff3":"len(dataset['user'].unique())","af6e108d":"index=range(0,1)\ndf=pd.DataFrame(index=index)","a97b6d33":"first = True\nwhile (len(df)<200000):\n    for user in dataset.user.unique():\n        dataset = dataset.sample(frac=1).reset_index(drop=True)\n        df1 = dataset[dataset['user'] == user][0:20].reset_index(drop=True)\n        df2 = dataset[dataset['user'] == user][-10:].reset_index(drop=True)\n        df3 = dataset[dataset['user'] != user][-10:].reset_index(drop=True)\n        df3 = pd.concat([df2,df3],ignore_index=True)\n        if first :\n            df = pd.concat([df1,df3],axis = 1)\n            first = False\n        else:\n            df = pd.concat([df,pd.concat([df1,df3],axis = 1)])","adebead8":"df.columns = ['user1','text1','user2','text2']\ndf['is_same'] = 0\ndf.loc[df['user1']==df['user2'],'is_same'] = 1\ndf.loc[df['user1']!=df['user2'],'is_same'] = 0","9f0d25ac":"df = df.sample(frac=1).reset_index(drop=True)","04f0914a":"def text_to_word_list(tweet):\n    ''' Pre process and convert texts to a list of words '''\n    tweet = str(tweet)\n    tweet = tweet.lower()\n    tweet = tweet.lower()\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet, flags=re.MULTILINE)\n    tweet = re.sub(r'[_\"\\-;%()|.,+&=*%]', '', tweet)\n    tweet = re.sub(r'\\.', ' . ', tweet)\n    tweet = re.sub(r'\\!', ' !', tweet)\n    tweet = re.sub(r'\\?', ' ?', tweet)\n    tweet = re.sub(r'\\,', ' ,', tweet)\n    tweet = re.sub(r':', ' : ', tweet)\n    tweet = re.sub(r'#', ' # ', tweet)\n    tweet = re.sub(r'@', ' @ ', tweet)\n    tweet = re.sub(r'd .c .', 'd.c.', tweet)\n    tweet = re.sub(r'u .s .', 'd.c.', tweet)\n    tweet = re.sub(r' amp ', ' and ', tweet)\n    tweet = re.sub(r'pm', ' pm ', tweet)\n    tweet = re.sub(r'news', ' news ', tweet)\n    tweet = re.sub(r' . . . ', ' ', tweet)\n    tweet = re.sub(r' .  .  . ', ' ', tweet)\n    tweet = re.sub(r' ! ! ', ' ! ', tweet)\n    tweet = re.sub(r'&amp', 'and', tweet)\n    tweet = tweet.split()\n\n    return tweet","88569148":"vocabulary = dict()\ninverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\ntweets_cols = ['text1', 'text2']\ndef preprocess(df):\n    for index, row in df.iterrows():\n        for tweet in tweets_cols:\n                integers = []\n                for word in text_to_word_list(row[tweet]):\n                    if word not in vocabulary:\n                        vocabulary[word] = len(inverse_vocabulary)\n                        integers.append(len(inverse_vocabulary))\n                        inverse_vocabulary.append(word)\n                    else:\n                        integers.append(vocabulary[word])\n\n                df.set_value(index, tweet, integers)\n    return df","586da441":"df=preprocess(df)","228dd5b9":"df.head()","a602dbb4":"len(vocabulary)","94db7be4":"max_length = max(df.text1.map(lambda x: len(x)).max(),\n                     df.text2.map(lambda x: len(x)).max())","38af7879":"max_length","c836c688":"X_train, X_test, Y_train, Y_test = train_test_split(df[tweets_cols], df['is_same'], test_size=0.2)\nX_train, X_validation, Y_train, Y_validation = train_test_split(X_train,Y_train, test_size=0.1)","4e4a0fc2":"test = df[df.index.isin(X_test.index)]","ab930dca":"X_train = {'left': X_train.text1, 'right': X_train.text2}\nX_validation = {'left': X_validation.text1, 'right': X_validation.text2}\nX_test = {'left': X_test.text1, 'right': X_test.text2}","91f99fbd":"Y_train = Y_train.values\nY_validation = Y_validation.values\nY_test = Y_test.values","df805913":"def making_pad(data):\n    for key,val in data.items():\n        data[key]=pad_sequences(data[key],maxlen=max_length)\n    return data","2c2c11f2":"X_train = making_pad(X_train)\nX_validation = making_pad(X_validation)\nX_test = making_pad(X_test)","f6aa3744":"assert X_train['left'].shape == X_train['right'].shape\nassert len(X_train['left']) == len(Y_train)","b8ad67ed":"X_train['left'].shape","9ed93e97":"from keras import optimizers\nfrom keras.layers import merge ,Dense,Concatenate\nn_hidden = 20\nbatch_size = 512\nn_epoch = 10\n\ndef exponent_neg_manhattan_distance(left, right):\n    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n\n\nleft_input = Input(shape=(max_length,), dtype='int32')\nright_input = Input(shape=(max_length,), dtype='int32')\n\nembedding_layer = Embedding(len(vocabulary)+1, 100,input_length=max_length)\n\nencoded_left = embedding_layer(left_input)\nencoded_right = embedding_layer(right_input)\n\nshared_lstm = LSTM(n_hidden)\nleft_output = shared_lstm(encoded_left)\nright_output = shared_lstm(encoded_right)\n# Calculates the distance as defined by the MaLSTM model\nmalstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0],1))([left_output,right_output])\nprediction = Dense(1,activation='sigmoid')(malstm_distance)\n# Pack it all up into a model\nmodel = Model([left_input, right_input],outputs=prediction)\n\n# Adadelta optimizer, with gradient clipping by norm\n# optimizer = optimizers.Adam(clipnorm=gradient_clipping_norm)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","e1244e24":"# Start training\ntraining_start_time = time()\n\nhistory=model.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n\nprint(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))","74ab378c":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","2e87e111":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()","dfb9da3c":"preds = model.predict([X_test['left'], X_test['right']],batch_size=256)","d4e228b4":"preds[preds>=0.5]=1\npreds[preds<0.5]=0","e97d3f38":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test, preds))","4017b252":"index=range(0,1)\ndt=pd.DataFrame(index=index)","28b7ac86":"first = True\nwhile (len(dt)<1000):\n    for user in test.user1.unique():\n        test = test.sample(frac=1).reset_index(drop=True)\n        tweet1 = [test[(test['user1']==user)&(test['user2']==user)][0:1]['text1'].values[0]]*13\n        tweet2 = [test[(test['user1']==user)&(test['user2']==user)][0:1]['text2'].values[0]]+test[test['user1'] != user][-12:]['text2'].values.tolist()\n        if first :\n            dt = pd.DataFrame({'tweet1':tweet1,'tweet2':tweet2})\n            first = False\n        else:\n            dt = pd.concat([dt,pd.DataFrame({'tweet1':tweet1,'tweet2':tweet2})])","01a42c80":"x_test = {'left': dt.tweet1, 'right':dt.tweet2}\nx_test=making_pad(x_test)","ba98fba5":"correct = 0\nincorrect = 0\nfor i in range(0,len(dt),13):\n    prob = model.predict([x_test['left'][i:i+13],x_test['right'][i:i+13]])\n    index=np.argmax(prob)\n    if (index==0):\n        correct+=1\n    else :\n        incorrect+=1\n    ","f886a9d2":"print((100.0*(correct \/( correct+incorrect))))","2e73ff22":"prepare the data for the model ","76318335":"here i will generate 200K pairs of the data so that the model can have enough data to train on","9fa117bf":"### References \n[keras one shot learning](https:\/\/sorenbouma.github.io\/blog\/oneshot\/)\n\n[similar article with quora questions](https:\/\/medium.com\/mlreview\/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07)\n\n[paper using the same method on twitter data](https:\/\/lct-master.org\/getfile.php?id=3079&n=1&dt=TH&ft=pdf&type=TH)\n\n[paper talking about Learning Sentence Similarity](http:\/\/www.mit.edu\/~jonasm\/info\/MuellerThyagarajan_AAAI16.pdf)\n","a166aed2":"Now i got 200K pairs of tweets","d9024e21":"i chose to make 20 pairs for each tweet 10 positive pairs and 1 negtive 50:50 as the paper i used mentioned in the reference ","a67d4fd4":"I will clean the data a little bit ","88aca542":"to make the padding i will need to get the max tweet length ","5624811c":"Making the padding process so that each vector have the same length of words ","a8c51b3d":"here i will encode the data to integers represent the number of word in the global vocabulary","5ad257c4":"**The data**  consist of 2 columns (user,text) in the next sections i will reshape this data so that i will make another data frame have pairs of tweets \npositive matching where 2 tweets comes from same author and negtive matching where 2 tweets come from different authors i will make this process randomly so i will choose random user and then choose 9 random tweets from that user then i will choose random tweet from the rest of users and make pairs i'm not going to make balanced classes here wher we have positive pairs = negative pairs but i will make them closely.","7582d1ce":"In the next sections i'm making the same preprocess for testing data to pass it to the model and check the result .\n\ni'm testing on 1300 pair of tweets each user with the other users and him self.","94d9d971":"when i collected the data is sampled them to make sure that i have balanced number of tweets per user.","c3c42c53":"We have 13 different user in our data","71ba3a8e":"Now the network in here i made some differences first i'm adding embedding layer so the the network can learn it's own weights without needing to use another wordtovec\nthen i added in the last dense layer to make a probability predection on pairs similarity\nalso i used adam optimizer ","19f37565":"## Intorduction\n\nIn this kernel i'm trying to make twitter authorship verfication using Siamese Manhattan LSTM which will try to minimize the distance between tweets vectors \n\n### Siamense network and the architecture :\nSiamese networks are networks that have two or more identical sub-networks in them.\n![Network](https:\/\/serving.photos.photobox.com\/03977968b016b46991be02af338e296cc2254b72f5fc9f4225a9b0b11ab01e683e7f26bf.jpg)\n\n### The Architecture \nwe have 2 input layers of words to integers lists they pass through Embedding layer to learn embedding weights (better than using pretrained one) then we have a shared LSTM layer between the two inputs then there is a distance function that calculate the distance between the inputs weights vector and last there is a Dense layer with sigmoid activation to predict the probability of each class","cc039ff7":"### testing the classfication per user accuracy by comparing multiple tweets to multiple users and count how many got the right user"}}