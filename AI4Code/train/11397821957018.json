{"cell_type":{"1917f57c":"code","c2f0fa6e":"code","10738e9c":"code","eed448e0":"code","59328db0":"code","8acd1332":"code","72e5b7bc":"code","32aeddc1":"code","73377dbf":"code","300292f0":"code","41017a93":"code","30e824bf":"code","df813a6c":"code","67dc8816":"code","6a48b0f1":"code","d2fe51e1":"code","6847486e":"code","63b02b0f":"code","32c90bf0":"code","b583e141":"code","d3fe02e0":"code","f02b6bf1":"code","e9173c57":"code","ad137407":"code","b9d66d99":"code","e5708194":"code","b715b64b":"markdown","81aae29e":"markdown"},"source":{"1917f57c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nprint(os.listdir(\"..\/input\"))\n\nimport json\nfrom pandas.io.json import json_normalize\n\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor","c2f0fa6e":"def rmse(y_true, y_pred):\n    return round(np.sqrt(mean_squared_error(y_true, y_pred)), 5)\n\ndef load_df(path, nrows=None):\n    json_columns = ['device','geoNetwork','totals','trafficSource']\n    \n    df = pd.read_csv(path,\n                     #make sure that the json in csv will be converted as dict, otherwise, it will be string\n                     converters = {json_column: json.loads for json_column in json_columns},\n\n                     #make sure 'fullVisitorId' is string\n                     dtype = {'fullVisitorId':'str'},\n                     nrows=nrows)\n\n    for json_column in json_columns:\n        #conver the dict as a dataframe\n        converted_df = json_normalize(df[json_column])\n\n        #format the name, f'{json_column}.{subcolumn}' = '{}.{}'.format(json_column,subcolumn)\n        converted_df.columns = [f'{json_column}.{subcolumn}' for subcolumn in converted_df.columns]\n\n        #remove the origin columns that are dict, and add the dataframe made of the new columns, keep both the ids\n        df = df.drop(json_column, axis = 1).merge(converted_df, right_index=True, left_index=True)\n        \n    print(f'file:{path},shape:{df.shape}')\n    #output the loaded and processed df\n    return df","10738e9c":"%%time\ntrain_df = load_df('..\/input\/train_v2.csv')","eed448e0":"%%time\ntest_df = load_df('..\/input\/test_v2.csv')","59328db0":"print('There are 2 columns that are in training set but not in the test set. They are',set(train_df.columns).difference(set(test_df.columns)))","8acd1332":"#nunique() will return the unique numbers of a column, by default, dropna = False, but here, we make it True, because we don't want to ignore any null values\nunhelpful_columns = []\n\nfor column in train_df.columns:\n    if train_df[column].nunique(dropna = False) == 1:\n        unhelpful_columns.append(column)\n\nprint('There are',train_df.shape[1],'columns.',len(unhelpful_columns),'of them have identical values for all rows. And they are as follows:')\nprint(unhelpful_columns)","72e5b7bc":"#drop the unhelpful columns\nunhelpful_columns.append('trafficSource.campaignCode')\nfor column in unhelpful_columns:\n    if column in train_df.columns:\n        train_df = train_df.drop(column, axis = 1)\n    if column in test_df.columns:\n        test_df = test_df.drop(column, axis = 1)\nprint(train_df.shape,test_df.shape)","32aeddc1":"#prepare the y_train\ny_train = train_df[['totals.transactionRevenue']].fillna(0.0)\ny_train['totals.transactionRevenue'] = np.log1p(y_train['totals.transactionRevenue'].astype('float'))\ntrain_df = train_df.drop('totals.transactionRevenue',axis = 1)\nprint(type(y_train),train_df.shape)","73377dbf":"test_df.head()","300292f0":"#process the date and time\ntrain_df.visitStartTime = pd.to_datetime(train_df.visitStartTime, unit='s')\ntest_df.visitStartTime = pd.to_datetime(test_df.visitStartTime, unit='s')\ntrain_df[\"date\"] = train_df.visitStartTime\ntest_df[\"date\"] = test_df.visitStartTime","41017a93":"#add weekday, time and day info\nfor df in [train_df, test_df]:\n    df['weekday'] = df['date'].dt.dayofweek.astype(object)\n    df['time'] = df['date'].dt.second + df['date'].dt.minute*60 + df['date'].dt.hour*3600\n    df['hour'] = df['date'].dt.hour\n    df['month'] = df['date'].dt.month   # it must not be included in features during learning!\n    df['day'] = df['date'].dt.date  ","30e824bf":"#add some combined feature\nfor df in [train_df, test_df]:\n    df['source.country'] = df['trafficSource.source'] + '_' + df['geoNetwork.country']\n    df['campaign.medium'] = df['trafficSource.campaign'] + '_' + df['trafficSource.medium']\n    df['browser.category'] = df['device.browser'] + '_' + df['device.deviceCategory']\n    df['browser.os'] = df['device.browser'] + '_' + df['device.operatingSystem']","df813a6c":"train_df.head()","67dc8816":"for df in [train_df, test_df]:\n    df['device_deviceCategory_channelGrouping'] = df['device.deviceCategory'] + \"_\" + df['channelGrouping']\n    df['channelGrouping_browser'] = df['device.browser'] + \"_\" + df['channelGrouping']\n    df['channelGrouping_OS'] = df['device.operatingSystem'] + \"_\" + df['channelGrouping']\n    \n    for i in ['geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country','geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region','geoNetwork.subContinent']:\n        for j in ['device.browser','device.deviceCategory', 'device.operatingSystem', 'trafficSource.source']:\n            df[i + \"_\" + j] = df[i] + \"_\" + df[j]\n    \n    df['content.source'] = df['trafficSource.adContent'].astype(str) + \"_\" + df['source.country']\n    df['medium.source'] = df['trafficSource.medium'] + \"_\" + df['source.country']","6a48b0f1":"test_df.head()","d2fe51e1":"#convert numeric data to floats or boolean\ndef convert_data_format(df):\n    #numeric values\n    df['totals.hits'] = df['totals.hits'].astype('float').fillna(0.0)\n    df['totals.pageviews'] = df['totals.pageviews'].astype('float').fillna(0.0)\n    df['visitNumber'] = df['visitNumber'].astype('float')\n    df['trafficSource.adwordsClickInfo.page'] = df['trafficSource.adwordsClickInfo.page'].astype('float').fillna(0.0)\n    #boolean values\n    df['totals.bounces'] = df['totals.bounces'].fillna(0.0).astype('bool')\n    df['totals.newVisits'] = df['totals.newVisits'].fillna(0.0).astype('bool')\n    df['trafficSource.isTrueDirect'] = df['trafficSource.isTrueDirect'].fillna('False')\n    df['trafficSource.adwordsClickInfo.isVideoAd'] = df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True)\n    return df\ntrain_df = convert_data_format(train_df)\ntest_df = convert_data_format(test_df)","6847486e":"#get the mean of hits and pageviews,and the max visit number then add these features to each user\nfor feature in [\"totals.hits\", \"totals.pageviews\"]:\n    alldata = pd.concat([train_df, test_df], sort=False)\n    alldata_id = alldata.groupby(\"fullVisitorId\")[feature].mean()\n    train_df[\"usermean_\" + feature] = train_df.fullVisitorId.map(alldata_id)\n    train_df[\"normal\" + feature] = (train_df[feature] - min(alldata[feature]))\/(max(alldata[feature]) - min(alldata[feature]))\n    \n    test_df[\"usermean_\" + feature] = test_df.fullVisitorId.map(alldata_id)\n    test_df[\"normal\" + feature] = (test_df[feature] - min(alldata[feature]))\/(max(alldata[feature]) - min(alldata[feature]))\n    \nfor feature in [\"visitNumber\"]:\n    info = pd.concat([train_df, test_df], sort=False).groupby(\"fullVisitorId\")[feature].max()\n    train_df[\"usermax_\" + feature] = train_df.fullVisitorId.map(info)\n    test_df[\"usermax_\" + feature] = test_df.fullVisitorId.map(info)","63b02b0f":"train_df.tail(20)","32c90bf0":"train_df.shape","b583e141":"#based on the training set analyasis, we trimmed out the rows in test_df that is confirmed zero revenue\nbounce_buy_yes = test_df['totals.bounces']!=1.0\nbounce_buy_no = test_df['totals.bounces']==1.0\n\nhit_buy_yes = test_df['totals.hits']!=1.0\nhit_buy_no = test_df['totals.hits']==1.0\n\npageview_buy_yes = test_df['totals.pageviews']>1.0\npageview_buy_no = test_df['totals.pageviews']<2.0\n\n# useful_broswer = ['Firefox','Chrome','Edge','Internet Explorer','Safari','Amazon Silk','Opera','Safari (in-app)','Android Webview']\n# useful_browser_yes = test_df['device.browser'].isin(useful_broswer)\n# useful_browser_no = ~test_df['device.browser'].isin(useful_broswer)\n\n# newly added filters\n# useful_OS = ['Chrome OS','Macintosh','Windows','Linux','Android','Windows Phone','iOS']\n# useful_OS_yes = test_df['device.operatingSystem'].isin(useful_OS)\n# useful_OS_no = ~test_df['device.operatingSystem'].isin(useful_OS)\n\n#nousedful_subContinent = ['Polynesia','Middle Africa','Micronesian Region','Melanesia']\n#useful_subContinent_yes = ~test_df['geoNetwork.subContinent'].isin(nousedful_subContinent)\n#useful_subContinent_no = test_df['geoNetwork.subContinent'].isin(nousedful_subContinent)\n\n#adwordsClickInfo_page_yes = test_df['trafficSource.adwordsClickInfo.page']<2.0\n#adwordsClickInfo_page_no = test_df['trafficSource.adwordsClickInfo.page']>1.0\n\n# adNetworkType_yes = test_df['trafficSource.adwordsClickInfo.adNetworkType'] != 'Search partners'\n# adNetworkType_no = test_df['trafficSource.adwordsClickInfo.adNetworkType'] == 'Search partners'\n\n# nouseful_campaign = ['AW - Electronics','All Products','Data Share']\n# useful_campaign_yes = ~test_df['trafficSource.campaign'].isin(nouseful_campaign)\n# useful_campaign_no = test_df['trafficSource.campaign'].isin(nouseful_campaign)\n\ntest_df_trimmed = test_df[bounce_buy_yes & hit_buy_yes & pageview_buy_yes] # & adwordsClickInfo_page_yes & useful_browser_yes & useful_subContinent_yes& useful_OS_yes &\n                         # & adNetworkType_yes & useful_campaign_yes]\ntest_df_dropped = test_df[bounce_buy_no | hit_buy_no | pageview_buy_no] # | adwordsClickInfo_page_no| useful_browser_no | useful_subContinent_no| useful_OS_no |\n                         # | adNetworkType_no | useful_campaign_no]","d3fe02e0":"print(test_df.shape[0],test_df_trimmed.shape[0]+ test_df_dropped.shape[0], test_df_trimmed.shape[0], test_df_dropped.shape[0])","f02b6bf1":"test_df.columns","e9173c57":"#process the catgory values\ncat_cols = [\"channelGrouping\", \"device.browser\", \n            \"device.deviceCategory\", \"device.operatingSystem\", \n            \"geoNetwork.city\", \"geoNetwork.continent\", \n            \"geoNetwork.country\", \"geoNetwork.metro\",\n            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n            \"geoNetwork.subContinent\", \"trafficSource.adContent\", \n            \"trafficSource.adwordsClickInfo.adNetworkType\", \n            \"trafficSource.adwordsClickInfo.gclId\", \n            \"trafficSource.adwordsClickInfo.page\", \n            \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n            \"trafficSource.keyword\", \"trafficSource.medium\", \n            \"trafficSource.referralPath\", \"trafficSource.source\",\n            \"trafficSource.adwordsClickInfo.isVideoAd\", \"trafficSource.isTrueDirect\",#below is the newly generated features,\n            'weekday','hour','month',\n            \"source.country\",\"campaign.medium\", \"browser.category\", \"browser.os\",\n            'content.source','medium.source','device_deviceCategory_channelGrouping', 'channelGrouping_browser','channelGrouping_OS',\n            'geoNetwork.subContinent_device.deviceCategory',\n            'geoNetwork.subContinent_device.operatingSystem',\n            'geoNetwork.subContinent_trafficSource.source']\n#             'geoNetwork.city_device.browser','geoNetwork.city_device.deviceCategory','geoNetwork.city_device.operatingSystem','geoNetwork.city_trafficSource.source',\n#             'geoNetwork.country_device.browser','geoNetwork.country_device.deviceCategory','geoNetwork.country_device.operatingSystem','geoNetwork.country_trafficSource.source'\n#             'geoNetwork.country_trafficSource.source',\n#             'device_deviceCategory_channelGrouping', 'channelGrouping_browser',\n#             'channelGrouping_OS', 'geoNetwork.city_device.browser',\n#             'geoNetwork.city_device.deviceCategory',\n#             'geoNetwork.city_device.operatingSystem',\n#             'geoNetwork.city_trafficSource.source',\n#             'geoNetwork.continent_device.browser',\n#             'geoNetwork.continent_device.deviceCategory',\n#             'geoNetwork.continent_device.operatingSystem',\n#             'geoNetwork.continent_trafficSource.source',\n#             'geoNetwork.country_device.browser',\n#             'geoNetwork.country_device.deviceCategory',\n#             'geoNetwork.country_device.operatingSystem',\n#             'geoNetwork.country_trafficSource.source',\n#             'geoNetwork.metro_device.browser',\n#             'geoNetwork.metro_device.deviceCategory',\n#             'geoNetwork.metro_device.operatingSystem',\n#             'geoNetwork.metro_trafficSource.source',\n#             'geoNetwork.networkDomain_device.browser',\n#             'geoNetwork.networkDomain_device.deviceCategory',\n#             'geoNetwork.networkDomain_device.operatingSystem',\n#             'geoNetwork.networkDomain_trafficSource.source',\n#             'geoNetwork.region_device.browser',\n#             'geoNetwork.region_device.deviceCategory',\n#             'geoNetwork.region_device.operatingSystem',\n#             'geoNetwork.region_trafficSource.source',\n#             'geoNetwork.subContinent_device.browser',\n#             'geoNetwork.subContinent_device.deviceCategory',\n#             'geoNetwork.subContinent_device.operatingSystem',\n#             'geoNetwork.subContinent_trafficSource.source', 'content.source',\n#             'medium.source'] \n\n#category values\nfor col in cat_cols:\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df_trimmed[col].values.astype('str')) + list(test_df_dropped[col].values.astype('str')))\n    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n    test_df_trimmed[col] = lbl.transform(list(test_df_trimmed[col].values.astype('str')))\n    test_df_dropped[col] = lbl.transform(list(test_df_dropped[col].values.astype('str')))","ad137407":"print('Now, the training set has',train_df.shape[0],'rows','the total test set has',test_df.shape[0],'or',test_df_trimmed.shape[0]+ test_df_dropped.shape[0],'rows.',test_df_trimmed.shape[0],'rows are left after trimming; while', test_df_dropped.shape[0],'are dropped. There are',test_df.shape[1],'columns in each dataset.')","b9d66d99":"train_df.head()","e5708194":"train_df.to_csv('train_df.csv')\ntest_df_trimmed.to_csv('test_df_trimmed.csv')\ntest_df_dropped.to_csv('test_df_dropped.csv')\ny_train.to_csv('y_train.csv')","b715b64b":"Output the processed data.","81aae29e":"# 1. Import and Preprocess the Data#"}}