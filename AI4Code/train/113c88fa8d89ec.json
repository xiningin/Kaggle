{"cell_type":{"5ee4f2e8":"code","c96e710c":"code","c620e916":"code","3230c22b":"code","507dac6f":"code","bd49fe41":"code","a0633845":"code","63728bc3":"code","03412a39":"code","e1e14461":"code","7534acda":"code","38449d16":"code","2368d3a4":"code","b1b88b4a":"code","e8eed2ed":"code","9105a4e8":"code","8f6a7f5f":"code","076c2e2a":"code","c2954506":"code","0fdf8be7":"code","817d2c7a":"code","fe95f40b":"code","9b13e3d6":"code","40749da8":"code","bfb20580":"code","26ae76bb":"code","f347b41d":"code","8fdaacff":"code","5f68ba16":"code","625dd796":"code","17540433":"code","111654b4":"code","18d013c5":"code","df6839ab":"code","a51d48de":"code","8918395c":"code","99f4e262":"code","ae4db11c":"code","12d61662":"code","25876d24":"code","9ea3e495":"code","62526876":"code","5bf94daa":"code","f4c891a8":"code","4c9ae049":"code","4105ae95":"code","f316c80d":"code","0738d10d":"code","f046dadf":"code","7420d31e":"code","bbd66760":"code","eda734d9":"code","d7db0d69":"code","598297d3":"code","ec35a83b":"code","e510ddb0":"code","b867128c":"code","9b6b303f":"code","2d9c1671":"code","3caf6892":"code","c36603f4":"markdown","89ad5d10":"markdown","c62a07bc":"markdown","42b7cfdf":"markdown","9c60cd6b":"markdown","947a11df":"markdown","0f4fcf24":"markdown","15f9447c":"markdown","b560b683":"markdown","40e111dd":"markdown","db1ce941":"markdown","f41f612f":"markdown","03033ec1":"markdown","0c13c0db":"markdown","d0d88100":"markdown","a401e96a":"markdown","f55f2112":"markdown","05c77ad1":"markdown"},"source":{"5ee4f2e8":"#Importing libraries\nimport nltk, re, pprint\nimport numpy as np\nimport pandas as pd\nimport requests\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint, time\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize","c96e710c":"# reading the Treebank tagged sentences\nnltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))","c620e916":"# first few tagged sentences\nprint(nltk_data[:10])","3230c22b":"# Splitting into train and test\nrandom.seed(1234)\ntrain_set, test_set = train_test_split(nltk_data,test_size=0.05)\n\nprint(len(train_set))\nprint(len(test_set))\nprint(train_set[:10])","507dac6f":"# Getting list of tagged words\ntrain_tagged_words = [tup for sent in train_set for tup in sent]\nlen(train_tagged_words)","bd49fe41":"# tokens \ntokens = [pair[0] for pair in train_tagged_words]\ntokens[:10]","a0633845":"# vocabulary\nV = set(tokens)\nprint(len(V))","63728bc3":"# number of tags\nT = set([pair[1] for pair in train_tagged_words])\nlen(T)","03412a39":"print(T)","e1e14461":"len(train_tagged_words)","7534acda":"# computing P(w\/t) and storing in T x V matrix\nt = len(T)\nv = len(V)\nw_given_t = np.zeros((t, v))","38449d16":"# compute word given tag: Emission Probability\ndef word_given_tag(word, tag, train_bag = train_tagged_words):\n    tag_list = [pair for pair in train_bag if pair[1]==tag]\n    count_tag = len(tag_list)\n    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n    count_w_given_tag = len(w_given_tag_list)\n    \n    return (count_w_given_tag, count_tag)","2368d3a4":"# examples\n\n# large\nprint(\"\\n\", \"large\")\nprint(word_given_tag('large', 'ADJ'))\nprint(word_given_tag('large', 'VERB'))\nprint(word_given_tag('large', 'NUM'), \"\\n\")\n\n# will\nprint(\"\\n\", \"will\")\nprint(word_given_tag('will', 'VERB'))\nprint(word_given_tag('will', 'PRT'))\nprint(word_given_tag('will', 'DET'))\nprint(word_given_tag('will', 'X'))\nprint(word_given_tag('will', 'CONJ'))\nprint(word_given_tag('will', 'PRON'))\nprint(word_given_tag('will', 'ADJ'))\nprint(word_given_tag('will', 'ADV'))\nprint(word_given_tag('will', 'ADP'))  \nprint(word_given_tag('will', 'X'))        \nprint(word_given_tag('will', '.'))  \nprint(word_given_tag('will', 'NUM'))        \n      \n# book\nprint(\"\\n\", \"book\")\nprint(word_given_tag('book', 'NOUN'))\nprint(word_given_tag('book', 'VERB'))","b1b88b4a":"### Transition Probabilities","e8eed2ed":"# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n\ndef t2_given_t1(t2, t1, train_bag = train_tagged_words):\n    tags = [pair[1] for pair in train_bag]\n    count_t1 = len([t for t in tags if t==t1])\n    count_t2_t1 = 0\n    for index in range(len(tags)-1):\n        if tags[index]==t1 and tags[index+1] == t2:\n            count_t2_t1 += 1\n    return (count_t2_t1, count_t1)","9105a4e8":"# examples\nprint(t2_given_t1(t2='NOUN', t1='ADJ'))\nprint(t2_given_t1('NOUN', 'DET'))\nprint(t2_given_t1('NOUN', 'NUM'))\nprint(t2_given_t1('NOUN', 'VERB'))\nprint(t2_given_t1(',', 'NOUN'))\nprint(t2_given_t1('PRON', 'PRON'))\nprint(t2_given_t1('VERB', 'NOUN'))\nprint(t2_given_t1('ADP', 'VERB'))\nprint(t2_given_t1('PRT', 'VERB'))\nprint(t2_given_t1('NOUN', 'CONJ'))\nprint(t2_given_t1('NOUN', 'ADP'))\nprint(t2_given_t1('VERB', 'PRON'))\nprint(t2_given_t1('PRON', 'DET'))","8f6a7f5f":"#Please note P(tag|start) is same as P(tag|'.')\nprint(t2_given_t1('DET', '.'))\nprint(t2_given_t1('VERB', '.'))\nprint(t2_given_t1('NOUN', '.'))\nprint(t2_given_t1('ADJ', '.'))\nprint(t2_given_t1('PRON', '.'))\nprint(t2_given_t1('NUM', '.'))\nprint(t2_given_t1('PRT', '.'))\nprint(t2_given_t1('X', '.'))\nprint(t2_given_t1('ADP', '.'))\nprint(t2_given_t1('ADV', '.'))\nprint(t2_given_t1('CONJ', '.'))","076c2e2a":"# creating t x t transition matrix of tags\n# each column is t2, each row is t1\n# thus M(i, j) represents P(tj given ti)\n\ntags_matrix = np.zeros((len(T), len(T)), dtype='float32')\nfor i, t1 in enumerate(list(T)):\n    for j, t2 in enumerate(list(T)): \n        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]\/t2_given_t1(t2, t1)[1]","c2954506":"# convert the matrix to a df for better readability\ntags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))","0fdf8be7":"tags_df","817d2c7a":"tags_df.loc['.', :]","fe95f40b":"# heatmap of tags matrix\n# T(i, j) means P(tag j given tag i)\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_df)\nplt.show()","9b13e3d6":"# frequent tags\n# filter the df to get P(t2, t1) > 0.5\ntags_frequent = tags_df[tags_df>0.5]\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_frequent)\nplt.show()","40749da8":"# Viterbi Heuristic\ndef Viterbi(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n            emission_p = word_given_tag(words[key], tag)[0]\/(word_given_tag(words[key], tag)[1])\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            \n        pmax = max(p)\n        # getting state for which probability is maximum\n        state_max = T[p.index(pmax)] \n        state.append(state_max)\n    return list(zip(words, state))","bfb20580":"## Evaluating on Test Set\n\n# Running on entire test dataset would take more than 3-4hrs. \n# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n\nrandom.seed(1234)\n\n# choose random 5 sents\n#rndom = [random.randint(1,len(test_set)) for x in range(5)]\n\n# list of sents\n#test_run = [test_set[i] for i in rndom]\ntest_run=test_set\n# list of tagged words\ntest_run_base = [tup for sent in test_run for tup in sent]\n\n# list of untagged words\ntest_tagged_words = [tup[0] for sent in test_run for tup in sent]\ntest_run","26ae76bb":"len(test_tagged_words)","f347b41d":"# tagging the test sentences\nstart = time.time()\ntagged_seq = Viterbi(test_tagged_words)\nend = time.time()\ndifference = end-start\nprint(\"Time taken in seconds: \", difference)","8fdaacff":"#print(\"Time taken in seconds: \", difference)\nprint(tagged_seq)","5f68ba16":"# accuracy\ncheck = [i for i, j in zip(tagged_seq, test_run_base) if i == j] ","625dd796":"accuracy = len(check)\/len(tagged_seq)","17540433":"accuracy","111654b4":"incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]","18d013c5":"incorrect_tags =[j for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]\nincorrect_words = [w[0] for in_wo in incorrect_tags for w in in_wo]\nset(incorrect_words)","df6839ab":"incorrect_words = [w[0] for in_wo in incorrect_tagged_cases for w in in_wo]\nset(incorrect_words)","a51d48de":"## Testing\nsentence_test = 'Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.'\nwords = word_tokenize(sentence_test)\nstart = time.time()\ntagged_seq = Viterbi(words)\nend = time.time()\ndifference = end-start","8918395c":"print(tagged_seq)\nprint(difference)","99f4e262":"## Testing\nsentence_test = 'Android is a mobile operating system developed by Google.'\nwords = word_tokenize(sentence_test)\nstart = time.time()\ntagged_seq = Viterbi(words)\nend = time.time()\ndifference = end-start","ae4db11c":"print(tagged_seq)\nprint(difference)","12d61662":"## Testing\nsentence_test = 'Google and Twitter made a deal in 2015 that gave Google access to Twitter s firehose.'\nwords = word_tokenize(sentence_test)\nstart = time.time()\ntagged_seq = Viterbi(words)\nend = time.time()\ndifference = end-start\n","25876d24":"print(tagged_seq)\nprint(difference)","9ea3e495":"\ndef Viterbi_modified_smoothing(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    V= len(list(set([pair[0] for pair in words])))\n    print(V)  \n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n           \n            emission_p = word_given_tag(words[key], tag)[0]+1\/(word_given_tag(words[key], tag)[1]+V)\n                         \n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            \n        pmax = max(p)\n        # getting state for which probability is maximum\n        state_max = T[p.index(pmax)] \n        state.append(state_max)\n    return list(zip(words, state))\n","62526876":"## Evaluating on Test Set\n\n# Running on entire test dataset would take more than 3-4hrs. \n# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n\nrandom.seed(1345)\n\n# choose random 20 sents\n#rndom = [random.randint(1,len(test_set)) for x in range(30)]\n\n# list of sents\n#test_run1 = [test_set[i] for i in rndom]\ntest_run1=test_set\n# list of tagged words\ntest_run_base1 = [tup for sent in test_run1 for tup in sent]\n\n# list of untagged words\ntest_tagged_words1 = [tup[0] for sent in test_run1 for tup in sent]\ntest_run1","5bf94daa":"len(test_tagged_words1)","f4c891a8":"# tagging the test sentences\nstart1 = time.time()\ntagged_seq1 = Viterbi_modified_smoothing(test_tagged_words1)\nend1 = time.time()\ndifference = end1-start1\nprint(\"Time taken in seconds: \", difference)","4c9ae049":"print(tagged_seq1)","4105ae95":"# accuracy\ncheck1 = [i for i, j in zip(tagged_seq1, test_run_base1) if i == j] \n\naccuracy1 = len(check1)\/len(tagged_seq1)\naccuracy1","f316c80d":"def Viterbi_modified_RuleBasedTagger(train_set,test_set):\n  # specify patterns for tagging\n  # example from the NLTK book\n\n    patterns = [ (r'.*ing$', 'VERB'),              # VERB\n             (r'.*ed$', 'VERB'),               # past tense\n             (r'.*es$', 'VERB'),               # 3rd singular present\n             (r'.*s$', 'NOUN'),                # plural nouns\n             (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n             (r'.*', 'NOUN')             \n           ]\n\n# rule based tagger\n    rule_based_tagger = nltk.RegexpTagger(patterns)\n\n# lexicon backed up by the rule-based tagger\n    lexicon_tagger = nltk.UnigramTagger(train_set, backoff=rule_based_tagger)\n    a=lexicon_tagger.evaluate(test_set)\n    return a\n    ","0738d10d":"accu=Viterbi_modified_RuleBasedTagger(train_set,test_set)\nprint (accu)","f046dadf":"## Testing\nsentence_test1 = 'Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.'\nwords1 = word_tokenize(sentence_test1)\nstart = time.time()\ntagged_seq1 = Viterbi(words1)\nend = time.time()\ndifference = end-start","7420d31e":"print(tagged_seq1)\nprint(difference)","bbd66760":"## Testing\nsentence_test2 = 'Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.'\nwords2 = word_tokenize(sentence_test2)\nstart = time.time()\ntagged_seq2 = Viterbi_modified_smoothing(words2)\nend = time.time()\ndifference = end-start","eda734d9":"print(tagged_seq2)\nprint(difference)","d7db0d69":"## Testing\n\nsentence_test3 = 'Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.'\nwords3 = word_tokenize(sentence_test3)\nstart = time.time()\ntagged_seq3 = Viterbi(words3)\ntagged_seq\nend = time.time()\ndifference = end-start","598297d3":"print(tagged_seq3)\nprint(difference)","ec35a83b":"## Testing\nsentence_test4 = 'Android is a mobile operating system developed by Google.'\nwords = word_tokenize(sentence_test4)\nstart = time.time()\ntagged_seq3 = Viterbi(words)\ntagged_seq3\nend = time.time()\ndifference = end-start","e510ddb0":"print(tagged_seq3)\nprint(difference)","b867128c":"## Testing\nsentence_test4 = 'Android is a mobile operating system developed by Google.'\nwords4 = word_tokenize(sentence_test4)\nstart = time.time()\ntagged_seq4 = Viterbi_modified_smoothing(words4)\nend = time.time()\ndifference = end-start","9b6b303f":"print(tagged_seq4)\nprint(difference)","2d9c1671":"sentence_test5 = 'Google and Twitter made a deal in 2015 that gave Google access to Twitter s firehose.'\nwords5 = word_tokenize(sentence_test5)\nstart = time.time()\ntagged_seq5 = Viterbi(words5)\nend = time.time()\ndifference = end-start","3caf6892":"print(tagged_seq5)\nprint(difference)","c36603f4":"### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm","89ad5d10":"### Build the vanilla Viterbi based POS tagger","c62a07bc":"### Data Preparation","42b7cfdf":"## POS tagging using modified Viterbi","9c60cd6b":"## Analysis - sentence started most of the times with the given tags \n\nMost of the sentenses started with the tag set Noun - 2469 times \nfollowed by 'DET'-1943 times ,'ADP'-1008 times\n","947a11df":"## Analysis \n\nMost unknown words belong to Tag class \"NOUN\" - 27371 times Noun Tag class assigned.\nThe 3 most assigned tags are NOUN,VERB,ADP and least assigned tag is CONJ","0f4fcf24":"### Solve the problem of unknown words","15f9447c":"Rule-Based (Regular Expression) Tagger\n\nNow let's build a rule-based, or regular expression based tagger. In NLTK, the RegexpTagger() can be provided with handwritten regular expression patterns, as shown below.\n\nwe specify regexes for Verbs, plural nouns , numbers and finally, if none of the above rules are applicable to a word, we tag the most frequent tag NOUN.","b560b683":"#### Evaluating tagging accuracy","40e111dd":"### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications","db1ce941":"Modified Viterbi alogorithm with laplace smoothing has accuracy 0.915 comparitively 0.908.\nRule based regular expression accuracy is 0.946 which is more than the Vanilla Viterbi and Modified viterbi alogorithm\n","f41f612f":"### Emission Probabilities","03033ec1":"## Modifying  original Viterbi Alogorithm  2 - for unknown words","0c13c0db":"1 Words which has emission zero has tagged with tags rather than assigning the first tag as default\n2.OS -earlier tagged as NUM now tagged as Noun.\n3.Rule based tagger with regular expression improved tagging correctly","d0d88100":"Note : We can increase accuracy and handle is the issue of un known words with Naive Bayes classifier,SVM classifier,Decistion trees.\nDecistion tress have good accuracy than Naive bayes and SVM classifier.\nCRF has good results than all the models .","a401e96a":"## Tagging Accuracies \n\n1.Vanilla Viterbi alogorithm - 0.9082994776552524\n\n2.Modified Viterbi alogorithm ( laplace smoothing)-0.9154575353066358\n\n3.Rule based regular expression tagger -0.946798220158638","f55f2112":"##  Analysis of which Tag follows most of the times another Tag \n\nTag Class 'NOUN' follows 'DET' 5299 times.\nTag Class 'VERB' follows 'NOUN' 4030 times.\nTag Class 'NOUN' follows 'ADJ' 4247 times","05c77ad1":"###  Viterbi Heuristic - Modified function with Laplace smoothing"}}