{"cell_type":{"837eb823":"code","390cf96f":"code","9227ac10":"code","77d36c27":"code","b3365555":"code","9f4dec94":"code","a4655b10":"code","88a26639":"code","735d9a1b":"code","abc2a45f":"code","fe93bc76":"code","ad8ab6da":"code","4473c77c":"code","39478e68":"code","474c28d0":"code","0a056db9":"code","9d7e3233":"code","e91088dd":"code","2dab0052":"code","a9a5cd74":"code","d8a4c060":"code","deee4c35":"code","ae7d7dcb":"code","72367d40":"code","9ada1e11":"code","a7b653ab":"code","90a5693e":"code","12934829":"code","7bbea141":"code","ce934171":"code","f890576e":"code","937936b3":"code","523a943e":"code","8e6cf760":"code","911c8336":"code","9b4e625c":"code","71e57f70":"code","6023bea2":"code","b1a06fcd":"code","1ae6b536":"code","b54f7a10":"code","1b2bd36e":"code","935bd7f6":"code","5fbab8be":"code","e79468fa":"markdown","881bdbe4":"markdown","a3b475a2":"markdown","80c63a72":"markdown","5d109921":"markdown","d87487b4":"markdown","879c656e":"markdown","86869771":"markdown"},"source":{"837eb823":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\nimport torch.utils.data\nfrom tqdm import tqdm\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom scipy.stats import rankdata\n\nfrom gensim.models import KeyedVectors\n\nimport warnings\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nimport string\nimport re\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import metrics\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\npd.set_option('max_colwidth',400)\npd.set_option('max_columns', 50)\nimport json\nimport gc\nimport os\n\nimport copy\n\nimport pickle\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted","390cf96f":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\ndef seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    #with open(path,'rb') as f:\n    emb_arr = KeyedVectors.load(path)\n    return emb_arr\n\ndef build_matrix(word_index, path, dim=300):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, dim))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x\n\nclass SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch\n\n    \nclass SoftmaxPooling(nn.Module):\n    def __init__(self, dim=1):\n        super(self.__class__, self).__init__()\n        self.dim = dim\n        \n    def forward(self, x):\n        return (x * x.softmax(dim=self.dim)).sum(dim=self.dim)\n\n\nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n\n        self.linear_out = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n            nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        )\n        \n        self.linear_aux_out = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n            nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        )\n        \n        self.softmaxpool = SoftmaxPooling()\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        # softmax pooling\n        soft_pool = self.softmaxpool(h_lstm2)\n        \n        h_conc = torch.cat((max_pool, avg_pool, soft_pool), 1)\n        hidden = h_conc\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n    \ndef custom_loss(data, targets):\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2\n\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\ndef ensemble_predictions(predictions, weights, type_=\"linear\", axis=1):\n    assert np.isclose(np.sum(weights), 1.0)\n    if type_ == \"linear\":\n        res = np.average(predictions, weights=weights, axis=axis)\n    # CAREFUL WITH WHAT IS BELOW, IT IS DIFFERENT FOR LSTM BLENDING AND LSTM\/BERT BLENDING\n    elif type_ == \"harmonic\":\n        res = np.average([1 \/ p for p in predictions], weights=weights, axis=axis)\n        return 1 \/ res\n    elif type_ == \"geometric\":\n        numerator = np.average(\n            [np.log(p) for p in predictions], weights=weights, axis=axis\n        )\n        res = np.exp(numerator \/ sum(weights))\n        return res\n    elif type_ == \"rank\":\n        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=axis)\n        return res \/ (len(res) + 1)\n    return res","9227ac10":"warnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\nSEED = 1234\nBATCH_SIZE = 512\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ntqdm.pandas()\nCRAWL_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/crawl-300d-2M.gensim'\nNUMBERBATCH_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/numberbatch-en.gensim'\nPARAGRAM_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/paragram_300_sl999.gensim'\nTWITTER_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/glove.twitter.27B.200d.gensim'\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n\nseed_everything()","77d36c27":"# only here the values are like this\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 768","b3365555":"tokenizer = TreebankWordTokenizer()\n\ntest_df = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) ","9f4dec94":"symbols_to_isolate = '.,?!-;*\"\u2026:\u2014()%#$&_\/@\uff3c\u30fb\u03c9+=\u201d\u201c[]^\u2013>\\\\\u00b0<~\u2022\u2260\u2122\u02c8\u028a\u0252\u221e\u00a7{}\u00b7\u03c4\u03b1\u2764\u263a\u0261|\u00a2\u2192\u0336`\u2765\u2501\u2523\u252b\u2517\uff2f\u25ba\u2605\u00a9\u2015\u026a\u2714\u00ae\\x96\\x92\u25cf\u00a3\u2665\u27a4\u00b4\u00b9\u2615\u2248\u00f7\u2661\u25d0\u2551\u25ac\u2032\u0254\u02d0\u20ac\u06e9\u06de\u2020\u03bc\u2712\u27a5\u2550\u2606\u02cc\u25c4\u00bd\u02bb\u03c0\u03b4\u03b7\u03bb\u03c3\u03b5\u03c1\u03bd\u0283\u272c\uff33\uff35\uff30\uff25\uff32\uff29\uff34\u263b\u00b1\u264d\u00b5\u00ba\u00be\u2713\u25fe\u061f\uff0e\u2b05\u2105\u00bb\u0412\u0430\u0432\u2763\u22c5\u00bf\u00ac\u266b\uff23\uff2d\u03b2\u2588\u2593\u2592\u2591\u21d2\u2b50\u203a\u00a1\u2082\u2083\u2767\u25b0\u2594\u25de\u2580\u2582\u2583\u2584\u2585\u2586\u2587\u2199\u03b3\u0304\u2033\u2639\u27a1\u00ab\u03c6\u2153\u201e\u270b\uff1a\u00a5\u0332\u0305\u0301\u2219\u201b\u25c7\u270f\u25b7\u2753\u2757\u00b6\u02da\u02d9\uff09\u0441\u0438\u02bf\u2728\u3002\u0251\\x80\u25d5\uff01\uff05\u00af\u2212\ufb02\ufb01\u2081\u00b2\u028c\u00bc\u2074\u2044\u2084\u2320\u266d\u2718\u256a\u25b6\u262d\u272d\u266a\u2614\u2620\u2642\u2603\u260e\u2708\u270c\u2730\u2746\u2619\u25cb\u2023\u2693\u5e74\u220e\u2112\u25aa\u2599\u260f\u215b\uff43\uff41\uff53\u01c0\u212e\u00b8\uff57\u201a\u223c\u2016\u2133\u2744\u2190\u263c\u22c6\u0292\u2282\u3001\u2154\u00a8\u0361\u0e4f\u26be\u26bd\u03a6\u00d7\u03b8\uffe6\uff1f\uff08\u2103\u23e9\u262e\u26a0\u6708\u270a\u274c\u2b55\u25b8\u25a0\u21cc\u2610\u2611\u26a1\u2604\u01eb\u256d\u2229\u256e\uff0c\u4f8b\uff1e\u0295\u0250\u0323\u0394\u2080\u271e\u2508\u2571\u2572\u258f\u2595\u2503\u2570\u258a\u258b\u256f\u2533\u250a\u2265\u2612\u2191\u261d\u0279\u2705\u261b\u2669\u261e\uff21\uff2a\uff22\u25d4\u25e1\u2193\u2640\u2b06\u0331\u210f\\x91\u2800\u02e4\u255a\u21ba\u21e4\u220f\u273e\u25e6\u266c\u00b3\u306e\uff5c\uff0f\u2235\u2234\u221a\u03a9\u00a4\u261c\u25b2\u21b3\u25ab\u203f\u2b07\u2727\uff4f\uff56\uff4d\uff0d\uff12\uff10\uff18\uff07\u2030\u2264\u2215\u02c6\u269c\u2601'\nsymbols_to_delete = '\\n\ud83c\udf55\\r\ud83d\udc35\ud83d\ude11\\xa0\\ue014\\t\\uf818\\uf04a\\xad\ud83d\ude22\ud83d\udc36\ufe0f\\uf0e0\ud83d\ude1c\ud83d\ude0e\ud83d\udc4a\\u200b\\u200e\ud83d\ude01\u0639\u062f\u0648\u064a\u0647\u0635\u0642\u0623\u0646\u0627\u062e\u0644\u0649\u0628\u0645\u063a\u0631\ud83d\ude0d\ud83d\udc96\ud83d\udcb5\u0415\ud83d\udc4e\ud83d\ude00\ud83d\ude02\\u202a\\u202c\ud83d\udd25\ud83d\ude04\ud83c\udffb\ud83d\udca5\u1d0d\u028f\u0280\u1d07\u0274\u1d05\u1d0f\u1d00\u1d0b\u029c\u1d1c\u029f\u1d1b\u1d04\u1d18\u0299\u0493\u1d0a\u1d21\u0262\ud83d\ude0b\ud83d\udc4f\u05e9\u05dc\u05d5\u05dd\u05d1\u05d9\ud83d\ude31\u203c\\x81\u30a8\u30f3\u30b8\u6545\u969c\\u2009\ud83d\ude8c\u1d35\u035e\ud83c\udf1f\ud83d\ude0a\ud83d\ude33\ud83d\ude27\ud83d\ude40\ud83d\ude10\ud83d\ude15\\u200f\ud83d\udc4d\ud83d\ude2e\ud83d\ude03\ud83d\ude18\u05d0\u05e2\u05db\u05d7\ud83d\udca9\ud83d\udcaf\u26fd\ud83d\ude84\ud83c\udffc\u0b9c\ud83d\ude16\u1d20\ud83d\udeb2\u2010\ud83d\ude1f\ud83d\ude08\ud83d\udcaa\ud83d\ude4f\ud83c\udfaf\ud83c\udf39\ud83d\ude07\ud83d\udc94\ud83d\ude21\\x7f\ud83d\udc4c\u1f10\u1f76\u03ae\u03b9\u1f72\u03ba\u1f00\u03af\u1fc3\u1f34\u03be\ud83d\ude44\uff28\ud83d\ude20\\ufeff\\u2028\ud83d\ude09\ud83d\ude24\u26fa\ud83d\ude42\\u3000\u062a\u062d\u0643\u0633\u0629\ud83d\udc6e\ud83d\udc99\u0641\u0632\u0637\ud83d\ude0f\ud83c\udf7e\ud83c\udf89\ud83d\ude1e\\u2008\ud83c\udffe\ud83d\ude05\ud83d\ude2d\ud83d\udc7b\ud83d\ude25\ud83d\ude14\ud83d\ude13\ud83c\udffd\ud83c\udf86\ud83c\udf7b\ud83c\udf7d\ud83c\udfb6\ud83c\udf3a\ud83e\udd14\ud83d\ude2a\\x08\u2011\ud83d\udc30\ud83d\udc07\ud83d\udc31\ud83d\ude46\ud83d\ude28\ud83d\ude43\ud83d\udc95\ud835\ude0a\ud835\ude26\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude30\ud835\ude24\ud835\ude3a\ud835\ude34\ud835\ude2a\ud835\ude27\ud835\ude2e\ud835\ude23\ud83d\udc97\ud83d\udc9a\u5730\u7344\u8c37\u0443\u043b\u043a\u043d\u041f\u043e\u0410\u041d\ud83d\udc3e\ud83d\udc15\ud83d\ude06\u05d4\ud83d\udd17\ud83d\udebd\u6b4c\u821e\u4f0e\ud83d\ude48\ud83d\ude34\ud83c\udfff\ud83e\udd17\ud83c\uddfa\ud83c\uddf8\u043c\u03c5\u0442\u0455\u2935\ud83c\udfc6\ud83c\udf83\ud83d\ude29\\u200a\ud83c\udf20\ud83d\udc1f\ud83d\udcab\ud83d\udcb0\ud83d\udc8e\u044d\u043f\u0440\u0434\\x95\ud83d\udd90\ud83d\ude45\u26f2\ud83c\udf70\ud83e\udd10\ud83d\udc46\ud83d\ude4c\\u2002\ud83d\udc9b\ud83d\ude41\ud83d\udc40\ud83d\ude4a\ud83d\ude49\\u2004\u02e2\u1d52\u02b3\u02b8\u1d3c\u1d37\u1d3a\u02b7\u1d57\u02b0\u1d49\u1d58\\x13\ud83d\udeac\ud83e\udd13\\ue602\ud83d\ude35\u03ac\u03bf\u03cc\u03c2\u03ad\u1f78\u05ea\u05de\u05d3\u05e3\u05e0\u05e8\u05da\u05e6\u05d8\ud83d\ude12\u035d\ud83c\udd95\ud83d\udc45\ud83d\udc65\ud83d\udc44\ud83d\udd04\ud83d\udd24\ud83d\udc49\ud83d\udc64\ud83d\udc76\ud83d\udc72\ud83d\udd1b\ud83c\udf93\\uf0b7\\uf04c\\x9f\\x10\u6210\u90fd\ud83d\ude23\u23fa\ud83d\ude0c\ud83e\udd11\ud83c\udf0f\ud83d\ude2f\u0435\u0445\ud83d\ude32\u1f38\u1fb6\u1f41\ud83d\udc9e\ud83d\ude93\ud83d\udd14\ud83d\udcda\ud83c\udfc0\ud83d\udc50\\u202d\ud83d\udca4\ud83c\udf47\\ue613\u5c0f\u571f\u8c46\ud83c\udfe1\u2754\u2049\\u202f\ud83d\udc60\u300b\u0915\u0930\u094d\u092e\u093e\ud83c\uddf9\ud83c\uddfc\ud83c\udf38\u8521\u82f1\u6587\ud83c\udf1e\ud83c\udfb2\u30ec\u30af\u30b5\u30b9\ud83d\ude1b\u5916\u56fd\u4eba\u5173\u7cfb\u0421\u0431\ud83d\udc8b\ud83d\udc80\ud83c\udf84\ud83d\udc9c\ud83e\udd22\u0650\u064e\u044c\u044b\u0433\u044f\u4e0d\u662f\\x9c\\x9d\ud83d\uddd1\\u2005\ud83d\udc83\ud83d\udce3\ud83d\udc7f\u0f3c\u3064\u0f3d\ud83d\ude30\u1e37\u0417\u0437\u25b1\u0446\ufffc\ud83e\udd23\u5356\u6e29\u54e5\u534e\u8bae\u4f1a\u4e0b\u964d\u4f60\u5931\u53bb\u6240\u6709\u7684\u94b1\u52a0\u62ff\u5927\u574f\u7a0e\u9a97\u5b50\ud83d\udc1d\u30c4\ud83c\udf85\\x85\ud83c\udf7a\u0622\u0625\u0634\u0621\ud83c\udfb5\ud83c\udf0e\u035f\u1f14\u6cb9\u522b\u514b\ud83e\udd21\ud83e\udd25\ud83d\ude2c\ud83e\udd27\u0439\\u2003\ud83d\ude80\ud83e\udd34\u02b2\u0448\u0447\u0418\u041e\u0420\u0424\u0414\u042f\u041c\u044e\u0436\ud83d\ude1d\ud83d\udd91\u1f50\u1f7b\u03cd\u7279\u6b8a\u4f5c\u6226\u7fa4\u0449\ud83d\udca8\u5706\u660e\u56ed\u05e7\u2110\ud83c\udfc8\ud83d\ude3a\ud83c\udf0d\u23cf\u1ec7\ud83c\udf54\ud83d\udc2e\ud83c\udf41\ud83c\udf46\ud83c\udf51\ud83c\udf2e\ud83c\udf2f\ud83e\udd26\\u200d\ud835\udcd2\ud835\udcf2\ud835\udcff\ud835\udcf5\uc548\uc601\ud558\uc138\uc694\u0416\u0459\u041a\u045b\ud83c\udf40\ud83d\ude2b\ud83e\udd24\u1fe6\u6211\u51fa\u751f\u5728\u4e86\u53ef\u4ee5\u8bf4\u666e\u901a\u8bdd\u6c49\u8bed\u597d\u6781\ud83c\udfbc\ud83d\udd7a\ud83c\udf78\ud83e\udd42\ud83d\uddfd\ud83c\udf87\ud83c\udf8a\ud83c\udd98\ud83e\udd20\ud83d\udc69\ud83d\udd92\ud83d\udeaa\u5929\u4e00\u5bb6\u26b2\\u2006\u26ad\u2686\u2b2d\u2b2f\u23d6\u65b0\u2700\u254c\ud83c\uddeb\ud83c\uddf7\ud83c\udde9\ud83c\uddea\ud83c\uddee\ud83c\uddec\ud83c\udde7\ud83d\ude37\ud83c\udde8\ud83c\udde6\u0425\u0428\ud83c\udf10\\x1f\u6740\u9e21\u7ed9\u7334\u770b\u0281\ud835\uddea\ud835\uddf5\ud835\uddf2\ud835\uddfb\ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude07\ud835\uddef\ud835\ude01\ud835\uddf0\ud835\ude00\ud835\ude05\ud835\uddfd\ud835\ude04\ud835\uddf1\ud83d\udcfa\u03d6\\u2000\u04af\u057d\u1d26\u13a5\u04bb\u037a\\u2007\u0570\\u2001\u0269\uff59\uff45\u0d66\uff4c\u01bd\uff48\ud835\udc13\ud835\udc21\ud835\udc1e\ud835\udc2b\ud835\udc2e\ud835\udc1d\ud835\udc1a\ud835\udc03\ud835\udc1c\ud835\udc29\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\u0184\u1d28\u05df\u146f\u0ed0\u03a4\u13e7\u0be6\u0406\u1d11\u0701\ud835\udc2c\ud835\udc30\ud835\udc32\ud835\udc1b\ud835\udc26\ud835\udc2f\ud835\udc11\ud835\udc19\ud835\udc23\ud835\udc07\ud835\udc02\ud835\udc18\ud835\udfce\u051c\u0422\u15de\u0c66\u3014\u13ab\ud835\udc33\ud835\udc14\ud835\udc31\ud835\udfd4\ud835\udfd3\ud835\udc05\ud83d\udc0b\ufb03\ud83d\udc98\ud83d\udc93\u0451\ud835\ude25\ud835\ude2f\ud835\ude36\ud83d\udc90\ud83c\udf0b\ud83c\udf04\ud83c\udf05\ud835\ude6c\ud835\ude56\ud835\ude68\ud835\ude64\ud835\ude63\ud835\ude61\ud835\ude6e\ud835\ude58\ud835\ude60\ud835\ude5a\ud835\ude59\ud835\ude5c\ud835\ude67\ud835\ude65\ud835\ude69\ud835\ude6a\ud835\ude57\ud835\ude5e\ud835\ude5d\ud835\ude5b\ud83d\udc7a\ud83d\udc37\u210b\ud835\udc00\ud835\udc25\ud835\udc2a\ud83d\udeb6\ud835\ude62\u1f39\ud83e\udd18\u0366\ud83d\udcb8\u062c\ud328\ud2f0\uff37\ud835\ude47\u1d7b\ud83d\udc42\ud83d\udc43\u025c\ud83c\udfab\\uf0a7\u0411\u0423\u0456\ud83d\udea2\ud83d\ude82\u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\u1fc6\ud83c\udfc3\ud835\udcec\ud835\udcfb\ud835\udcf4\ud835\udcee\ud835\udcfd\ud835\udcfc\u2618\ufd3e\u032f\ufd3f\u20bd\\ue807\ud835\udc7b\ud835\udc86\ud835\udc8d\ud835\udc95\ud835\udc89\ud835\udc93\ud835\udc96\ud835\udc82\ud835\udc8f\ud835\udc85\ud835\udc94\ud835\udc8e\ud835\udc97\ud835\udc8a\ud83d\udc7d\ud83d\ude19\\u200c\u041b\u2012\ud83c\udfbe\ud83d\udc79\u238c\ud83c\udfd2\u26f8\u516c\u5bd3\u517b\u5ba0\u7269\u5417\ud83c\udfc4\ud83d\udc00\ud83d\ude91\ud83e\udd37\u64cd\u7f8e\ud835\udc91\ud835\udc9a\ud835\udc90\ud835\udc74\ud83e\udd19\ud83d\udc12\u6b22\u8fce\u6765\u5230\u963f\u62c9\u65af\u05e1\u05e4\ud835\ude6b\ud83d\udc08\ud835\udc8c\ud835\ude4a\ud835\ude6d\ud835\ude46\ud835\ude4b\ud835\ude4d\ud835\ude3c\ud835\ude45\ufdfb\ud83e\udd84\u5de8\u6536\u8d62\u5f97\u767d\u9b3c\u6124\u6012\u8981\u4e70\u989d\u1ebd\ud83d\ude97\ud83d\udc33\ud835\udfcf\ud835\udc1f\ud835\udfd6\ud835\udfd1\ud835\udfd5\ud835\udc84\ud835\udfd7\ud835\udc20\ud835\ude44\ud835\ude43\ud83d\udc47\u951f\u65a4\u62f7\ud835\udde2\ud835\udff3\ud835\udff1\ud835\udfec\u2981\u30de\u30eb\u30cf\u30cb\u30c1\u30ed\u682a\u5f0f\u793e\u26f7\ud55c\uad6d\uc5b4\u3138\u3153\ub2c8\u035c\u0296\ud835\ude3f\ud835\ude54\u20b5\ud835\udca9\u212f\ud835\udcbe\ud835\udcc1\ud835\udcb6\ud835\udcc9\ud835\udcc7\ud835\udcca\ud835\udcc3\ud835\udcc8\ud835\udcc5\u2134\ud835\udcbb\ud835\udcbd\ud835\udcc0\ud835\udccc\ud835\udcb8\ud835\udcce\ud835\ude4f\u03b6\ud835\ude5f\ud835\ude03\ud835\uddfa\ud835\udfee\ud835\udfed\ud835\udfef\ud835\udff2\ud83d\udc4b\ud83e\udd8a\u591a\u4f26\ud83d\udc3d\ud83c\udfbb\ud83c\udfb9\u26d3\ud83c\udff9\ud83c\udf77\ud83e\udd86\u4e3a\u548c\u4e2d\u53cb\u8c0a\u795d\u8d3a\u4e0e\u5176\u60f3\u8c61\u5bf9\u6cd5\u5982\u76f4\u63a5\u95ee\u7528\u81ea\u5df1\u731c\u672c\u4f20\u6559\u58eb\u6ca1\u79ef\u552f\u8ba4\u8bc6\u57fa\u7763\u5f92\u66fe\u7ecf\u8ba9\u76f8\u4fe1\u8036\u7a23\u590d\u6d3b\u6b7b\u602a\u4ed6\u4f46\u5f53\u4eec\u804a\u4e9b\u653f\u6cbb\u9898\u65f6\u5019\u6218\u80dc\u56e0\u5723\u628a\u5168\u5802\u7ed3\u5a5a\u5b69\u6050\u60e7\u4e14\u6817\u8c13\u8fd9\u6837\u8fd8\u267e\ud83c\udfb8\ud83e\udd15\ud83e\udd12\u26d1\ud83c\udf81\u6279\u5224\u68c0\u8ba8\ud83c\udfdd\ud83e\udd81\ud83d\ude4b\ud83d\ude36\uc950\uc2a4\ud0f1\ud2b8\ub93c\ub3c4\uc11d\uc720\uac00\uaca9\uc778\uc0c1\uc774\uacbd\uc81c\ud669\uc744\ub835\uac8c\ub9cc\ub4e4\uc9c0\uc54a\ub85d\uc798\uad00\ub9ac\ud574\uc57c\ud569\ub2e4\uce90\ub098\uc5d0\uc11c\ub300\ub9c8\ucd08\uc640\ud654\uc57d\uae08\uc758\ud488\ub7f0\uc131\ubd84\uac08\ub54c\ub294\ubc18\ub4dc\uc2dc\ud5c8\ub41c\uc0ac\uc6a9\ud83d\udd2b\ud83d\udc41\u51f8\u1f70\ud83d\udcb2\ud83d\uddef\ud835\ude48\u1f0c\ud835\udc87\ud835\udc88\ud835\udc98\ud835\udc83\ud835\udc6c\ud835\udc76\ud835\udd7e\ud835\udd99\ud835\udd97\ud835\udd86\ud835\udd8e\ud835\udd8c\ud835\udd8d\ud835\udd95\ud835\udd8a\ud835\udd94\ud835\udd91\ud835\udd89\ud835\udd93\ud835\udd90\ud835\udd9c\ud835\udd9e\ud835\udd9a\ud835\udd87\ud835\udd7f\ud835\udd98\ud835\udd84\ud835\udd9b\ud835\udd92\ud835\udd8b\ud835\udd82\ud835\udd74\ud835\udd9f\ud835\udd88\ud835\udd78\ud83d\udc51\ud83d\udebf\ud83d\udca1\u77e5\u5f7c\u767e\\uf005\ud835\ude40\ud835\udc9b\ud835\udc72\ud835\udc73\ud835\udc7e\ud835\udc8b\ud835\udfd2\ud83d\ude26\ud835\ude52\ud835\ude3e\ud835\ude3d\ud83c\udfd0\ud835\ude29\ud835\ude28\u1f7c\u1e51\ud835\udc71\ud835\udc79\ud835\udc6b\ud835\udc75\ud835\udc6a\ud83c\uddf0\ud83c\uddf5\ud83d\udc7e\u14c7\u14a7\u152d\u1403\u1427\u1426\u1473\u1428\u14c3\u14c2\u1472\u1438\u146d\u144e\u14c0\u1423\ud83d\udc04\ud83c\udf88\ud83d\udd28\ud83d\udc0e\ud83e\udd1e\ud83d\udc38\ud83d\udc9f\ud83c\udfb0\ud83c\udf1d\ud83d\udef3\u70b9\u51fb\u67e5\u7248\ud83c\udf6d\ud835\udc65\ud835\udc66\ud835\udc67\uff2e\uff27\ud83d\udc63\\uf020\u3063\ud83c\udfc9\u0444\ud83d\udcad\ud83c\udfa5\u039e\ud83d\udc34\ud83d\udc68\ud83e\udd33\ud83e\udd8d\\x0b\ud83c\udf69\ud835\udc6f\ud835\udc92\ud83d\ude17\ud835\udfd0\ud83c\udfc2\ud83d\udc73\ud83c\udf57\ud83d\udd49\ud83d\udc32\u0686\u06cc\ud835\udc6e\ud835\uddd5\ud835\uddf4\ud83c\udf52\ua725\u2ca3\u2c8f\ud83d\udc11\u23f0\u9244\u30ea\u4e8b\u4ef6\u0457\ud83d\udc8a\u300c\u300d\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600\u71fb\u88fd\u30b7\u865a\u507d\u5c41\u7406\u5c48\u0413\ud835\udc69\ud835\udc70\ud835\udc80\ud835\udc7a\ud83c\udf24\ud835\uddf3\ud835\udddc\ud835\uddd9\ud835\udde6\ud835\udde7\ud83c\udf4a\u1f7a\u1f08\u1f21\u03c7\u1fd6\u039b\u290f\ud83c\uddf3\ud835\udc99\u03c8\u0541\u0574\u0565\u057c\u0561\u0575\u056b\u0576\u0580\u0582\u0564\u0571\u51ac\u81f3\u1f40\ud835\udc81\ud83d\udd39\ud83e\udd1a\ud83c\udf4e\ud835\udc77\ud83d\udc02\ud83d\udc85\ud835\ude2c\ud835\ude31\ud835\ude38\ud835\ude37\ud835\ude10\ud835\ude2d\ud835\ude13\ud835\ude16\ud835\ude39\ud835\ude32\ud835\ude2b\u06a9\u0392\u03ce\ud83d\udca2\u039c\u039f\u039d\u0391\u0395\ud83c\uddf1\u2672\ud835\udf48\u21b4\ud83d\udc92\u2298\u023b\ud83d\udeb4\ud83d\udd95\ud83d\udda4\ud83e\udd58\ud83d\udccd\ud83d\udc48\u2795\ud83d\udeab\ud83c\udfa8\ud83c\udf11\ud83d\udc3b\ud835\udc0e\ud835\udc0d\ud835\udc0a\ud835\udc6d\ud83e\udd16\ud83c\udf8e\ud83d\ude3c\ud83d\udd77\uff47\uff52\uff4e\uff54\uff49\uff44\uff55\uff46\uff42\uff4b\ud835\udff0\ud83c\uddf4\ud83c\udded\ud83c\uddfb\ud83c\uddf2\ud835\uddde\ud835\udded\ud835\uddd8\ud835\udde4\ud83d\udc7c\ud83d\udcc9\ud83c\udf5f\ud83c\udf66\ud83c\udf08\ud83d\udd2d\u300a\ud83d\udc0a\ud83d\udc0d\\uf10a\u10da\u06a1\ud83d\udc26\\U0001f92f\\U0001f92a\ud83d\udc21\ud83d\udcb3\u1f31\ud83d\ude47\ud835\uddf8\ud835\udddf\ud835\udde0\ud835\uddf7\ud83e\udd5c\u3055\u3088\u3046\u306a\u3089\ud83d\udd3c'","a4655b10":"isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}","88a26639":"x_test = test_df['comment_text'].progress_apply(lambda x:preprocess(x))\n\nloss_weight = 3.209226860170181\n\nmax_features = 400000","735d9a1b":"tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","abc2a45f":"with open('..\/input\/bilstm-crawl-paragram-0\/tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nparagram_matrix, unknown_words_paragram = build_matrix(tokenizer.word_index, PARAGRAM_EMBEDDING_PATH)\nprint('n unknown words (paragram): ', len(unknown_words_paragram))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, paragram_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel paragram_matrix\ngc.collect()","fe93bc76":"x_test_seq = tokenizer.texts_to_sequences(x_test)","ad8ab6da":"maxlen = 300\ntest_lengths = torch.from_numpy(np.array([len(x) for x in x_test_seq]))\n\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test_seq, maxlen=maxlen))","4473c77c":"batch_size = 512\ntest_dataset = data.TensorDataset(x_test_padded, test_lengths)\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\ndatabunch = DataBunch(train_dl=test_loader, valid_dl=test_loader, collate_fn=test_collator)","39478e68":"def test_model_cp(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('..\/input\/bilstm-crawl-paragram-{}\/model_1_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(x_test_seq), output_dim)) \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","474c28d0":"test_model_preds = pd.DataFrame()\ntest_model_preds['lstm_cp_0_3'] = test_model_cp(0, 3)[:, 0]\ntest_model_preds['lstm_cp_1_3'] = test_model_cp(1, 3)[:, 0]\ntest_model_preds['lstm_cp_2_3'] = test_model_cp(2, 3)[:, 0]","0a056db9":"# since this one - like this\nLSTM_UNITS = 256\nDENSE_HIDDEN_UNITS = 1536","9d7e3233":"def test_model_clip(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('..\/input\/lstm-clip-{}\/model_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(x_test_seq), output_dim)) \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","e91088dd":"test_model_preds['lstm_clip_0_2'] = test_model_clip(0, 2)[:, 0]\ntest_model_preds['lstm_clip_1_3'] = test_model_clip(1, 3)[:, 0]\ntest_model_preds['lstm_clip_2_3'] = test_model_clip(2, 3)[:, 0]","2dab0052":"numberbatch_matrix, unknown_words_numberbatch = build_matrix(tokenizer.word_index, NUMBERBATCH_EMBEDDING_PATH)\nprint('n unknown words (numberbatch): ', len(unknown_words_numberbatch))\n\nembedding_matrix = numberbatch_matrix\nprint(embedding_matrix.shape)\n\ndel numberbatch_matrix\ngc.collect()","a9a5cd74":"def test_model_nb(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('..\/input\/lstm-numberbatch-{}\/model_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(x_test_seq), output_dim))\n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","d8a4c060":"test_model_preds['lstm_nb_0_4'] = test_model_nb(0, 4)[:, 0]\ntest_model_preds['lstm_nb_1_4'] = test_model_nb(1, 4)[:, 0]\ntest_model_preds['lstm_nb_2_4'] = test_model_nb(2, 4)[:, 0]","deee4c35":"twitter_matrix, unknown_words_twitter = build_matrix(tokenizer.word_index, TWITTER_EMBEDDING_PATH, dim=200)\nprint('n unknown words (twitter): ', len(unknown_words_twitter))\n\nembedding_matrix = twitter_matrix\nprint(embedding_matrix.shape)\n\ndel twitter_matrix\ngc.collect()","ae7d7dcb":"def test_model_tw(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('..\/input\/lstm-twitter-{}\/model_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(x_test_seq), output_dim)) \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","72367d40":"test_model_preds['lstm_tw_0_3'] = test_model_tw(0, 3)[:, 0]\ntest_model_preds['lstm_tw_1_3'] = test_model_tw(1, 3)[:, 0]\ntest_model_preds['lstm_tw_2_3'] = test_model_tw(2, 3)[:, 0]","9ada1e11":"class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nrepl = {\n    \"&lt;3\": \" good \",\n    \":d\": \" good \",\n    \":dd\": \" good \",\n    \":p\": \" good \",\n    \"8)\": \" good \",\n    \":-)\": \" good \",\n    \":)\": \" good \",\n    \";)\": \" good \",\n    \"(-:\": \" good \",\n    \"(:\": \" good \",\n    \"yay!\": \" good \",\n    \"yay\": \" good \",\n    \"yaay\": \" good \",\n    \"yaaay\": \" good \",\n    \"yaaaay\": \" good \",\n    \"yaaaaay\": \" good \",\n    \":\/\": \" bad \",\n    \":&gt;\": \" sad \",\n    \":')\": \" sad \",\n    \":-(\": \" bad \",\n    \":(\": \" bad \",\n    \":s\": \" bad \",\n    \":-s\": \" bad \",\n    \"&lt;3\": \" heart \",\n    \":d\": \" smile \",\n    \":p\": \" smile \",\n    \":dd\": \" smile \",\n    \"8)\": \" smile \",\n    \":-)\": \" smile \",\n    \":)\": \" smile \",\n    \";)\": \" smile \",\n    \"(-:\": \" smile \",\n    \"(:\": \" smile \",\n    \":\/\": \" worry \",\n    \":&gt;\": \" angry \",\n    \":')\": \" sad \",\n    \":-(\": \" sad \",\n    \":(\": \" sad \",\n    \":s\": \" sad \",\n    \":-s\": \" sad \",\n    r\"\\br\\b\": \"are\",\n    r\"\\bu\\b\": \"you\",\n    r\"\\bhaha\\b\": \"ha\",\n    r\"\\bhahaha\\b\": \"ha\",\n    r\"\\bdon't\\b\": \"do not\",\n    r\"\\bdoesn't\\b\": \"does not\",\n    r\"\\bdidn't\\b\": \"did not\",\n    r\"\\bhasn't\\b\": \"has not\",\n    r\"\\bhaven't\\b\": \"have not\",\n    r\"\\bhadn't\\b\": \"had not\",\n    r\"\\bwon't\\b\": \"will not\",\n    r\"\\bwouldn't\\b\": \"would not\",\n    r\"\\bcan't\\b\": \"can not\",\n    r\"\\bcannot\\b\": \"can not\",\n    r\"\\bi'm\\b\": \"i am\",\n    \"m\": \"am\",\n    \"r\": \"are\",\n    \"u\": \"you\",\n    \"haha\": \"ha\",\n    \"hahaha\": \"ha\",\n    \"don't\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"didn't\": \"did not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"hadn't\": \"had not\",\n    \"won't\": \"will not\",\n    \"wouldn't\": \"would not\",\n    \"can't\": \"can not\",\n    \"cannot\": \"can not\",\n    \"i'm\": \"i am\",\n    \"m\": \"am\",\n    \"i'll\" : \"i will\",\n    \"its\" : \"it is\",\n    \"it's\" : \"it is\",\n    \"'s\" : \" is\",\n    \"that's\" : \"that is\",\n    \"weren't\" : \"were not\",\n}\n\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s):\n    return re_tok.sub(r' \\1 ', s).split()\n\ncont_patterns = [\n        (b'US', b'United States'),\n        (b'IT', b'Information Technology'),\n        (b'(W|w)on\\'t', b'will not'),\n        (b'(C|c)an\\'t', b'can not'),\n        (b'(I|i)\\'m', b'i am'),\n        (b'(A|a)in\\'t', b'is not'),\n        (b'(\\w+)\\'ll', b'\\g<1> will'),\n        (b'(\\w+)n\\'t', b'\\g<1> not'),\n        (b'(\\w+)\\'ve', b'\\g<1> have'),\n        (b'(\\w+)\\'s', b'\\g<1> is'),\n        (b'(\\w+)\\'re', b'\\g<1> are'),\n        (b'(\\w+)\\'d', b'\\g<1> would'),\n    ]\npatterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n\ndef prepare_for_char_n_gram(text):\n    \"\"\" Simple text clean up process\"\"\"\n    # 1. Go to lower case (only good for english)\n    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n    clean = bytes(text.lower(), encoding=\"utf-8\")\n    # 2. Drop \\n and  \\t\n    clean = clean.replace(b\"\\n\", b\" \")\n    clean = clean.replace(b\"\\t\", b\" \")\n    clean = clean.replace(b\"\\b\", b\" \")\n    clean = clean.replace(b\"\\r\", b\" \")\n    # 3. Replace english contractions\n    for (pattern, repl) in patterns:\n        clean = re.sub(pattern, repl, clean)\n    # 4. Drop puntuation\n    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n    clean = re.sub(b\"\\d+\", b\" \", clean)\n    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n    clean = re.sub(b'\\s+', b' ', clean)\n    # Remove ending space if any\n    clean = re.sub(b'\\s+$', b'', clean)\n    # 7. Now replace words by words surrounded by # signs\n    # e.g. my name is bond would become #my# #name# #is# #bond#\n    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n\n    return str(clean, 'utf-8')\n\ndef count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    return len(re.findall(regexp, text))\n\ndef get_indicators_and_clean_comments(df):\n    \"\"\"\n    Check all sorts of content as it may help find toxic comment\n    Though I'm not sure all of them improve scores\n    \"\"\"\n    # Count number of \\n\n#     df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n    # Get length in words and characters\n    df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split()))\n    df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x))\n    # TODO chars per row\n    # Check number of upper case, if you're angry you may write in upper case\n    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n    # Number of F words - f..k contains folk, fork,\n    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n    # Number of S word\n    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n    # Number of D words\n    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n    # Number of occurence of You, insulting someone usually needs someone called : you\n    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n    # Just to check you really refered to my mother ;-)\n    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n    # Just checking for toxic 19th century vocabulary\n    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n    # Some Sentences start with a <:> so it may help\n    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n    # Check for time stamp\n    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n    # Check for dates 18:44, 8 December 2010\n    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n    # Check for date short 8 December 2010\n    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n    # Check for http links\n#     df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}:\/\/\\S+\", x))\n    # check for mail\n    df[\"has_mail\"] = df[\"comment_text\"].apply(\n        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n    )\n    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n\n    # Now clean comments\n    df[\"clean_comment\"] = df[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))\n\n    # Get the new length in words and characters\n    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n    # Number of different characters used in a comment\n    # Using the f word only will reduce the number of letters required in the comment\n    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) \/ df[\"clean_comment\"].apply(\n        lambda x: 1 + min(99, len(x)))\n\nfts = [\"raw_word_len\", \"raw_char_len\", \"nb_upper\", \"nb_fk\", \"nb_sk\", \"nb_dk\", \"nb_you\", \"nb_mother\", \"nb_ng\", \"start_with_columns\",\n       \"has_timestamp\", \"has_date_long\", \"has_date_short\", \"has_mail\", \"has_emphasize_equal\", \"has_emphasize_quotes\", \"clean_word_len\",\n       \"clean_char_len\", \"clean_chars\", \"clean_chars_ratio\"]\n    \ndef preprocess(df):\n    keys = [i for i in repl.keys()]\n\n    new_data = []\n    ltr = df[\"comment_text\"].tolist()\n    for i in tqdm(ltr):\n        arr = str(i).split()\n        xx = \"\"\n        for j in arr:\n            j = str(j).lower()\n            if j[:4] == 'http' or j[:3] == 'www':\n                continue\n            if j in keys:\n                # print(\"inn\")\n                j = repl[j]\n            xx += j + \" \"\n        new_data.append(xx)\n    df[\"new_comment_text\"] = new_data\n    \n    trate = df[\"new_comment_text\"].tolist()\n    for i, c in enumerate(trate):\n        trate[i] = re.sub('[^a-zA-Z ?!]+', '', str(trate[i]).lower())\n    df[\"comment_text\"] = trate\n    df.drop([\"new_comment_text\"], axis=1, inplace=True)\n\n    df_text = df['comment_text']\n    \n    \n    re_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\n    \n    get_indicators_and_clean_comments(df)\n    \n    return df","a7b653ab":"test_df = preprocess(test_df)","90a5693e":"fts = [\"raw_word_len\", \"raw_char_len\", \"nb_upper\", \"nb_fk\", \"nb_sk\", \"nb_dk\", \"nb_you\", \"nb_mother\", \"nb_ng\", \"start_with_columns\",\n       \"has_timestamp\", \"has_date_long\", \"has_date_short\", \"has_mail\", \"has_emphasize_equal\", \"has_emphasize_quotes\", \"clean_word_len\",\n       \"clean_char_len\", \"clean_chars\", \"clean_chars_ratio\"]\n\ntest_text = test_df['clean_comment'].apply(lambda x: re.sub('#', '', x)).fillna('')\n\nword_vectorizer = TfidfVectorizer(\n        sublinear_tf=True,\n        strip_accents='unicode',\n        analyzer='word',\n        min_df=5,\n        ngram_range=(1, 2),\n        max_features=60000)\n\nwith open('..\/input\/jigsaw-tfidf-models\/word_vectorizer.pickle', 'rb') as handle:\n    word_vectorizer = pickle.load(handle)\n\ntest_word_features = word_vectorizer.transform(test_text)\ntest_features = hstack([test_df[fts], test_word_features]).tocsr()\ndel test_word_features\n\nwith open('..\/input\/jigsaw-tfidf-models\/gbm_model.pickle', 'rb') as handle:\n    gbm = pickle.load(handle)\n    \ntest_model_preds['tfidf_gbm'] = gbm.predict(test_features)\n\ntext = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s):\n    return text.sub(r' \\1 ', s)\n\nword_vectorizer = TfidfVectorizer(ngram_range=(1,2),\n               min_df=5, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1, max_features=50000)\n\nwith open('..\/input\/simple-tfidf-models\/word_vectorizer.pickle', 'rb') as handle:\n    word_vectorizer = pickle.load(handle)\n\ntest_tfidf = word_vectorizer.transform(test_df['comment_text'].fillna(''))\n\nlr = LogisticRegression(solver='lbfgs', random_state=13)\nwith open('..\/input\/simple-tfidf-models\/lr_model.pickle', 'rb') as handle:\n    lr = pickle.load(handle)\n\ntest_model_preds['tfidf_lr_simple'] = lr.predict_proba(test_tfidf)[:, 1]\n\nwith open('..\/input\/simple-tfidf-models\/gbm_model.pickle', 'rb') as handle:\n    gbm = pickle.load(handle)\n    \ntest_model_preds['tfidf_gbm_simple'] = gbm.predict(test_tfidf)\n\nclass NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        y = y\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) \/ ((y==y_i).sum()+1)\n        \n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) \/ pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self\n\nNbSvm = NbSvmClassifier(C=1.5, dual=True, n_jobs=-1)\nwith open('..\/input\/simple-tfidf-nbsvm\/nbsvm_model.pickle', 'rb') as handle:\n    NbSvm = pickle.load(handle)\n    \ntest_model_preds['tfidf_nbsvm_simple'] = NbSvm.predict_proba(test_tfidf)[:, 1]","12934829":"! md5sum ..\/input\/jigsaw2019code\/*.py\n! ls ..\/input\/\n! find ..\/input\/ -name config.json | grep -v deps","7bbea141":"deps_path = '..\/input\/kagglejigsaw2019deps\/kaggle-jigsaw-2019-deps\/kaggle-jigsaw-2019-deps\/'\n! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" $deps_path\/apex\n! pip install $deps_path\/pytorch-pretrained-BERT\/","ce934171":"test_size = 0","f890576e":"! python ..\/input\/jigsaw2019code\/bert.py \\\n  _runs\/bert-base-uncased-pretrained-ep1 \\\n  --model ..\/input\/jigsaw2019bertbaseuncasedpretrainedep1\/bert-base-uncased-pretrained-ep1\/bert-base-uncased-pretrained-ep1 \\\n  --submission --test-size $test_size","937936b3":"! python ..\/input\/jigsaw2019code\/bert.py \\\n  _runs\/gpt2-ep2-lr8e-5 \\\n  --bucket 0 \\\n  --model ..\/input\/gpt2ep2lr8e5\/gpt2-ep2-lr8e-5\/gpt2-ep2-lr8e-5\/ \\\n  --submission --test-size $test_size","523a943e":"! python ..\/input\/jigsaw2019code\/bert.py \\\n  _runs\/bert-base-cased-pretrained-ep1 \\\n  --model ..\/input\/bertbasecasedpretrainedep1\/bert-base-cased-pretrained-ep1\/bert-base-cased-pretrained-ep1\/ \\\n  --submission --test-size $test_size","8e6cf760":"! python ..\/input\/jigsaw2019code\/bert.py \\\n  _runs\/resume-lr0.1e-5-from-bert-large-pretrained-uncased-ep1-lr0.5e-5-as16 \\\n  --model ..\/input\/bertluncased1redo\/ \\\n  --submission --test-size $test_size","911c8336":"! python ..\/input\/jigsaw2019code\/bert.py \\\n  _runs\/bert-base-uncased-fresh-ep1 \\\n  --model ..\/input\/bertbaseuncasedfreshep1\/bert-base-uncased-fresh-ep1\/bert-base-uncased-fresh-ep1\/ \\\n  --submission --test-size $test_size","9b4e625c":"bert_uncased1 = pd.read_csv('_runs\/bert-base-uncased-pretrained-ep1\/submission.csv')['prediction']\nbert_cased1 = pd.read_csv('_runs\/bert-base-cased-pretrained-ep1\/submission.csv')['prediction']\nbert_uncased_fresh1 = pd.read_csv('_runs\/bert-base-uncased-fresh-ep1\/submission.csv')['prediction']\nbert_large1 = pd.read_csv('_runs\/resume-lr0.1e-5-from-bert-large-pretrained-uncased-ep1-lr0.5e-5-as16\/submission.csv')['prediction']","71e57f70":"test_model_preds['bert_uncased1'] = bert_uncased1\ntest_model_preds['bert_cased1'] = bert_cased1\ntest_model_preds['bert_uncased_fresh1'] = bert_uncased_fresh1\ntest_model_preds['bert_large1'] = bert_large1","6023bea2":"gpt22 = pd.read_csv('_runs\/gpt2-ep2-lr8e-5\/submission.csv')['prediction']","b1a06fcd":"test_model_preds['gpt22'] = gpt22","1ae6b536":"test_model_preds.to_csv('test_model_preds.csv', index=False)","b54f7a10":"! cp _runs\/bert-base-cased-pretrained-ep1\/submission.csv bert-base-cased-pretrained-ep1.csv\n! cp _runs\/bert-base-uncased-pretrained-ep1\/submission.csv bert-base-uncased-pretrained-ep1.csv\n! cp _runs\/bert-base-uncased-fresh-ep1\/submission.csv bert-base-uncased-fresh-ep1.csv\n! cp _runs\/gpt2-ep2-lr8e-5\/submission.csv gpt2-ep2-lr8e-5.csv\n! cp _runs\/resume-lr0.1e-5-from-bert-large-pretrained-uncased-ep1-lr0.5e-5-as16\/submission.csv \\\n  resume-lr0.1e-5-from-bert-large-pretrained-uncased-ep1-lr0.5e-5-as16.csv","1b2bd36e":"best_weights = {\n    'cp': [50, 40, 40],\n    'clip': [40, 40, 44],\n    'nb': [40, 39, 44],\n    'tw': [40, 39, 40],\n    'tfidf': [42, 44, 41, 37],\n    'bert': [59, 30, -28, 65],\n    'blend': [17, 33, -13, -1, -3, 132, 42]\n}\n\n\ndef custom_predict(X_models, weights=best_weights, type_='linear'):\n    model_cp = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_cp')]], weights=(np.array(weights['cp']) + 1e-15) \/ (sum(weights['cp']) + 1e-15), type_=type_)\n    model_clip = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_clip')]], weights=(np.array(weights['clip']) + 1e-15) \/ (sum(weights['clip']) + 1e-15), type_=type_)\n    model_nb = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_nb')]], weights=(np.array(weights['nb']) + 1e-15) \/ (sum(weights['nb']) + 1e-15), type_=type_)\n    model_tw = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_tw')]], weights=(np.array(weights['tw']) + 1e-15) \/ (sum(weights['tw']) + 1e-15), type_=type_)\n    model_tfidf = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('tfidf')]], weights=(np.array(weights['tfidf']) + 1e-15) \/ (sum(weights['tfidf']) + 1e-15), type_=type_)\n    model_bert = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('bert')]], weights=(np.array(weights['bert']) + 1e-15) \/ (sum(weights['bert']) + 1e-15), type_=type_)\n    model_gpt = X_models['gpt22']\n    \n    models = [model_cp, model_clip, model_nb, model_tw, model_tfidf, model_bert, model_gpt]\n    model_blend = np.zeros_like(model_bert) + 1e-15\n    for i in range(len(models)):\n        model_blend += weights['blend'][i] * models[i]\n    model_blend \/= (sum(weights['blend']) + 1e-15)\n    return model_blend","935bd7f6":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': custom_predict(test_model_preds)\n})\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission.head()","5fbab8be":"! rm -rf _runs","e79468fa":"## GPT2","881bdbe4":"## BiLSTM Crawl + Paragram","a3b475a2":"## Blend","80c63a72":"## BiLSTM Twitter","5d109921":"## TF-IDF","d87487b4":"## BERT","879c656e":"## BiLSTM clip target [0.05, 0.95]","86869771":"## BiLSTM Numberbatch"}}