{"cell_type":{"bb32b705":"code","488653d1":"code","8bb5931a":"code","2ddfaf79":"code","deea2511":"code","c2caa2da":"code","2c4dadea":"code","b5336ba3":"code","ed7aa6c9":"code","73101910":"code","2e702f77":"code","21e7e55b":"code","802f2101":"code","7482cd75":"code","5d2110a2":"code","f7adcc85":"code","f7ecc75f":"code","593b38c7":"code","886227ea":"code","629760f5":"code","ff403853":"code","9c160db9":"code","a5c634a6":"code","9e836920":"code","557f5e85":"code","e3b2db31":"code","93fde6a5":"code","6c622183":"code","cfc144cf":"code","64a8020d":"code","3e589879":"code","ee047087":"code","0977450d":"code","03c05ff5":"code","1dade4ad":"code","aee958a4":"code","100f8fd2":"code","bc5aced0":"code","3e6d1b30":"markdown","40d61f4a":"markdown","aa19c3ba":"markdown","71d97c2b":"markdown","0615f24f":"markdown","e0ba4f7e":"markdown","9665cbf6":"markdown","bb3a3824":"markdown","ef2947f1":"markdown","134661f1":"markdown","b8f09d81":"markdown"},"source":{"bb32b705":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport glob","488653d1":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/'","8bb5931a":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\/(df['bid_size2'] + df['ask_size2'])\n    return wap\ndef calc_wap3(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2']+ df['ask_size2'])\n    return wap\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))","2ddfaf79":"book_train = pd.read_parquet(data_dir + \"book_train.parquet\/stock_id=15\")\nbook_train.head()","deea2511":"def preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap3'] = calc_wap3(df)\n    df['log_return3'] = df.groupby('time_id')['wap3'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'log_return3':[realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap':[np.mean],\n            }\n\n    #####groupby \/ all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature","c2caa2da":"%%time\nfile_path = data_dir + \"book_train.parquet\/stock_id=0\"\npreprocessor_book(file_path)","2c4dadea":"trade_train = pd.read_parquet(data_dir + \"trade_train.parquet\/stock_id=0\")\ntrade_train.head(15)","b5336ba3":"def preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","ed7aa6c9":"%%time\nfile_path = data_dir + \"trade_train.parquet\/stock_id=0\"\npreprocessor_trade(file_path)","73101910":"def preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df\n","2e702f77":"list_stock_ids = [0,1]\npreprocessor(list_stock_ids, is_train = True)","21e7e55b":"train = pd.read_csv(data_dir + 'train.csv')","802f2101":"train_ids = train.stock_id.unique()","7482cd75":"%%time\ndf_train = preprocessor(list_stock_ids= train_ids, is_train = True)","5d2110a2":"train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')","f7adcc85":"df_train.head()","f7ecc75f":"test = pd.read_csv(data_dir + 'test.csv')","593b38c7":"test_ids = test.stock_id.unique()","886227ea":"%%time\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)","629760f5":"df_test = test.merge(df_test, on = ['row_id'], how = 'left')","ff403853":"from sklearn.model_selection import KFold\n#stock_id target encoding\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\n#training\ntmp = np.repeat(np.nan, df_train.shape[0])\nkf = KFold(n_splits = 20, shuffle=True,random_state = 19911109)\nfor idx_1, idx_2 in kf.split(df_train):\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\ndf_train['stock_id_target_enc'] = tmp","9c160db9":"df_train.head()","a5c634a6":"df_test.head()","9e836920":"import lightgbm as lgbm","557f5e85":"df_train['stock_id'] = df_train['stock_id'].astype(int)\ndf_test['stock_id'] = df_test['stock_id'].astype(int)","e3b2db31":"X = df_train.drop(['row_id','target'],axis=1)\ny = df_train['target']","93fde6a5":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef feval_RMSPE(preds, lgbm_train):\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\nparams = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      'early_stopping_rounds': 30,\n      'learning_rate': 0.01,\n      'lambda_l1': 1.0,\n      'lambda_l2': 1.0,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n  }","6c622183":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=20, random_state=19901028, shuffle=True)\noof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score","cfc144cf":"%%time\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n\n    print(\"Fold :\", fold+1)\n    \n    # create dataset\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    #RMSPE weight\n    weights = 1\/np.square(y_train)\n    lgbm_train = lgbm.Dataset(X_train,y_train,weight = weights)\n\n    weights = 1\/np.square(y_valid)\n    lgbm_valid = lgbm.Dataset(X_valid,y_valid,reference = lgbm_train,weight = weights)\n    \n    # model \n    model = lgbm.train(params=params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_valid],\n                      num_boost_round=5000,         \n                      feval=feval_RMSPE,\n                      verbose_eval=100,\n                      categorical_feature = ['stock_id']                \n                     )\n    \n    # validation \n    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n\n    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n    print(f'Performance of the\u3000prediction: , RMSPE: {RMSPE}')\n\n    #keep scores and models\n    scores += RMSPE \/ 20\n    models.append(model)\n    print(\"*\" * 100)","64a8020d":"scores","3e589879":"df_test.columns","ee047087":"df_train.columns","0977450d":"y_pred = df_test[['row_id']]\nX_test = df_test.drop(['time_id', 'row_id'], axis = 1)","03c05ff5":"X_test","1dade4ad":"target = np.zeros(len(X_test))\n\n#light gbm models\nfor model in models:\n    pred = model.predict(X_test[X_valid.columns], num_iteration=model.best_iteration)\n    target += pred \/ len(models)","aee958a4":"y_pred = y_pred.assign(target = target)","100f8fd2":"y_pred","bc5aced0":"y_pred.to_csv('submission.csv',index = False)","3e6d1b30":"## Model Building","40d61f4a":"## Combined preprocessor function","aa19c3ba":"## Training set","71d97c2b":"## Main function for preprocessing trade data","0615f24f":"## Target encoding by stock_id","e0ba4f7e":"## Functions for preprocess","9665cbf6":"## LightGBM","bb3a3824":"# Test set","ef2947f1":"## Test set","134661f1":"### Cross Validation","b8f09d81":"## Main function for preprocessing book data"}}