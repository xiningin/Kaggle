{"cell_type":{"576eb0a1":"code","fd9688ea":"code","9e14705f":"code","d3310d0b":"code","18e47d4a":"code","a3adf6c8":"code","1b78ebe3":"code","7f5cf1f7":"code","0179dc9f":"code","3f27b898":"code","945067bd":"code","e3352d11":"code","9920b880":"code","4d646b59":"code","7b1c7e35":"code","24805705":"markdown","bc910eb8":"markdown","282fd96a":"markdown","3d223e01":"markdown","abe98716":"markdown"},"source":{"576eb0a1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","fd9688ea":"train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')\n\nsub0 = pd.read_csv('..\/input\/tpsjan22-eda-baseline-train-submission\/submission.csv')\nsub1 = pd.read_csv('..\/input\/tpsjan22-06-lightgbm-quickstart\/submission_lightgbm_quickstart.csv')\nsub2 = pd.read_csv('..\/input\/tpsjan22-03-linear-model\/submission_linear_model_rounded.csv')","9e14705f":"pred = np.array([np.array(sub0['num_sold'].values), np.array(sub1['num_sold'].values), np.array(sub2['num_sold'].values)])\npred.T","d3310d0b":"np.log(pred.T)","18e47d4a":"# Plot the distribution of the test predictions\nplt.figure(figsize=(16,3))\nplt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, label='Training')\nplt.hist(sub0['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, rwidth=0.5, label='sub0 predictions')\nplt.hist(sub1['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, rwidth=0.5, label='sub1 predictions')\nplt.hist(sub2['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, rwidth=0.5, label='sub2 predictions')\nplt.xlabel('num_sold')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","a3adf6c8":"test_df['num_sold0'] = sub0.num_sold\ntest_df.head()","1b78ebe3":"country='Norway'\nstore='KaggleMart'\nproduct='Kaggle Hat'\nplt.figure(figsize=(20, 6))\ntrain_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\nplt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\nplt.legend()\nplt.title('Predictions and true num_sold for five years')\nplt.show()","7f5cf1f7":"plt.figure(figsize=(20, 6))\ntrain_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\nplt.scatter(np.arange(len(train_subset)),np.log(train_subset.num_sold), label='true', alpha=0.5, color='red', s=3)\nplt.legend()\nplt.title('Log of True num_sold for five years')\nplt.show()","0179dc9f":"test_df['num_sold0'] = sub0.num_sold\ntest_df['num_sold1'] = sub1.num_sold\ntest_df['num_sold2'] = sub2.num_sold\n\nplt.figure(figsize=(20, 6))\ntrain_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\nsub_subset = test_df[(test_df.country == country) & (test_df.store == store) & (test_df['product'] == product)]\n#plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\nplt.scatter(np.arange(len(sub_subset)), sub_subset.num_sold0, label='sub0', alpha=0.5, color='orange', s=3)\nplt.scatter(np.arange(len(sub_subset)), sub_subset.num_sold1, label='sub1', alpha=0.5, color='green', s=3)\nplt.scatter(np.arange(len(sub_subset)), sub_subset.num_sold2, label='sub2', alpha=0.5, color='blue', s=3)\nplt.legend()\nplt.title('Predicted num_sold for five years')\nplt.show()","3f27b898":"plt.figure(figsize=(20, 6))\ntrain_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\nsub_subset = test_df[(test_df.country == country) & (test_df.store == store) & (test_df['product'] == product)]\n#plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\nplt.scatter(np.arange(len(sub_subset)), np.log(sub_subset.num_sold0), label='sub0', alpha=0.5, color='orange', s=3)\nplt.scatter(np.arange(len(sub_subset)), np.log(sub_subset.num_sold1), label='sub1', alpha=0.5, color='green', s=3)\nplt.scatter(np.arange(len(sub_subset)), np.log(sub_subset.num_sold2), label='sub2', alpha=0.5, color='blue', s=3)\nplt.legend()\nplt.title('Log of predicted num_sold for five years')\nplt.show()","945067bd":"mean = np.mean(pred, axis=0)\nmed = np.median(pred, axis=0)\nmaxi = np.max(pred, axis=0)\n\nlog_mean = np.mean(np.log(pred), axis=0)","e3352d11":"sub['num_sold'] = np.round(mean)\nsub.to_csv('submission_mean.csv', index=False)\nsub.head(5)","9920b880":"sub['num_sold'] = np.round(med)\nsub.to_csv('submission_median.csv', index=False)\nsub.head(5)","4d646b59":"sub['num_sold'] = np.round(maxi)\nsub.to_csv('submission_maximum.csv', index=False)\nsub.head(5)","7b1c7e35":"sub['num_sold'] = np.round(log_mean)\nsub.to_csv('submission_log_mean.csv', index=False)\nsub.head(5)","24805705":"LB scores are:\n* mean: 4.25767\n* median: 4.21332\n* maximum: 4.73279\n* logarithmic mean: 4.25514\n\n\nWe can see that median works better than mean and maximum. Also logarithm score is better for mean case and the same fot median and maximum. So earlier assumption was right","bc910eb8":"## Reference\n* sub0: https:\/\/www.kaggle.com\/fergusfindley\/tpsjan22-eda-baseline-train-submission   \n* sub1: https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-06-lightgbm-quickstart                  \n* sub2: https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model                   \n\n## LB score\n* sub0: 4.61163 \n* sub1: 4.38188  \n* sub2: 4.33338","282fd96a":"## Some visualization","3d223e01":"## Score\nSymmetric mean absolute percentage error (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors. It is usually defined as follows:\n\n${\\displaystyle {\\text{SMAPE}}={\\frac {100\\%}{n}}\\sum _{t=1}^{n}{\\frac {\\left|F_{t}-A_{t}\\right|}{(|A_{t}|+|F_{t}|)\/2}}}$\n\nwhere At is the actual value and Ft is the forecast value.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Symmetric_mean_absolute_percentage_error\n\n## Discussion\nSo our goal is to minimize the difference between the actual and predicted values and maximize their average (num_sold > 0, so we ignore absolute value in denominator). \nThe goal is that predicted value should be close to actual and at the same time as large as possible. That's the reason rounding and multiplying by factor close to 1 increases LB score.\nAdditionally, ensembling the logarithm of values may be better because the logarithm decreases the range of possible values of prediction and therefore makes values of different models close to each other. Ensembling them and taking the exponent should arrive to better point than blending values at high distance from each other\n\n## Approach\nHere I want to try ensembling public notebooks by:\n1. Simply finding the average of all predictions and rounding result\n2. Finding the median and rounding, so the result will theoretically satify both requirements of metrics\n3. Finding the maximum and rounding, so SMAPE will be decreased because of denominator\n4. Finding the mean of logarithm, then taking the exponent and rounding\n\n\nFinding median and max of logarithm unecessary to check because logarithm is strictly increasing function which makes the mean and median value along 3 options be the same.","abe98716":"## Introduction\n\nThe goal is to combine several models' predictions to achieve the highest score with no overfitting. \nFor this hyperparameters should be tuned wihout information about LB score. It is better to see what techniques can be applied to achieve maximize the score function. "}}