{"cell_type":{"25b37959":"code","d3c00e87":"code","1cf15106":"code","f46e92d2":"code","5d33fd56":"code","d1a61284":"code","1ecf6a44":"code","307ed600":"code","1f5b12f4":"code","72b5c948":"code","e9ee4bbf":"code","fd213ef0":"code","a947c494":"code","e9e7e8da":"code","b69d11e9":"code","9800695a":"code","26733343":"code","36264cd8":"code","4c9d031f":"code","ccd97ce1":"code","04b32900":"markdown","669dc3bc":"markdown","f70dce0d":"markdown"},"source":{"25b37959":"# import libraries\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split","d3c00e87":"# import dataframe\n\ndf = pd.read_csv('..\/input\/iris\/Iris.csv', usecols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\", \"Species\"])","1cf15106":"df\n","f46e92d2":"df.iloc[:1, :]","5d33fd56":"print(df['SepalLengthCm'].max(), df['SepalWidthCm'].max(),df['PetalLengthCm'].max(),df['PetalWidthCm'].max())\n\nprint(df['SepalLengthCm'].min(), df['SepalWidthCm'].min(),df['PetalLengthCm'].min(),df['PetalWidthCm'].min())\n\n# they're already in numbers, let's feature scale them using normalisation (vals from 0 to 1)","d1a61284":"df['Species'].value_counts()\n\n# equal samples per class name, no class imbalance","1ecf6a44":"# normalise X vars\n\nmms = MinMaxScaler()\n\ndf.iloc[:, :4] = mms.fit_transform(df.iloc[:, :4])","307ed600":"df","1f5b12f4":"print(df['SepalLengthCm'].max(), df['SepalWidthCm'].max(),df['PetalLengthCm'].max(),df['PetalWidthCm'].max())\n\nprint(df['SepalLengthCm'].min(), df['SepalWidthCm'].min(),df['PetalLengthCm'].min(),df['PetalWidthCm'].min())\n","72b5c948":"# define X and y\n\nX = df.iloc[:, :4]\n\ny = df[['Species']]","e9ee4bbf":"'''# label encode the target variable\n\nle = LabelEncoder()\n\ndf.iloc[:, -1] = le.fit_transform(df.iloc[:, -1])\n'''\n# label encode the target variable\n\nohe = OneHotEncoder(sparse=False)\n\ny = ohe.fit_transform(y)\n\n'''\nohe = OneHotEncoder(sparse=False)\n\ny = df[['breast-quad']] # add double square brackets or we'll get a shape error\n\ny = ohe.fit_transform(y)'''\n","fd213ef0":"# define training, validation and test sets\n\n# get training, validation and test datasets (80:10:10 ratio)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=12) # NB random_state=12\n \nprint(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n\n \nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42) # NB random_state=42\n \nprint(X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n\n\nprint('X_train: ' + str(X_train))\n \nprint('y_train: ' + str(y_train))\n \nprint('X_val: ' + str(X_val))\n \nprint('y_val: ' + str(y_val))\n \nprint('X_test: ' + str(X_test))\n \nprint('y_test: ' + str(y_test))\n\n","a947c494":"# create model\n\ntf.random.set_seed(42)\n\n\nmodel_1 = keras.Sequential([                     \n  keras.layers.Dense(4, activation='relu'),\n  keras.layers.Dense(3, activation='softmax')\n])\n\n\nmodel_1.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics='accuracy')\n\n\nhistory_model_1 = model_1.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val),\n    epochs=6,\n    verbose=2\n)","e9e7e8da":"# plot loss curve\n\npd.DataFrame(history_model_1.history).plot(figsize=(10,7))\nplt.xlabel('epochs')\n","b69d11e9":"model_1.evaluate(X_test, y_test)\n\n# 80% accuracy","9800695a":"y_pred = model_1.predict(X_test)\n\ny_pred, y_pred.argmax(axis=-1), y_test.argmax(axis=-1), len(y_pred)\n\n# 13 of 15 correct, ie 87% accuracy\n# array([2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0]),\n# array([1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0]),\n","26733343":"# create model\n# model from: https:\/\/www.kaggle.com\/venkatkrishnan\/iris-data-tensorflow-neural-network \n\ntf.random.set_seed(42)\n\n\nmodel_2 = keras.Sequential([                   \n        keras.layers.Dense(1000, activation='relu'),\n        keras.layers.Dense(500, activation='relu',),\n        keras.layers.Dense(300, activation='relu'),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(3, activation='softmax')\n])\n\n\nmodel_2.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics='accuracy')\n\n\nhistory_model_2 = model_2.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val),\n    epochs=100,\n    verbose=2\n)","36264cd8":"# plot loss curve\n\npd.DataFrame(history_model_2.history).plot(figsize=(10,7))\nplt.xlabel('epochs')\n","4c9d031f":"model_2.evaluate(X_test, y_test)\n\n# accuracy: 1 (!)","ccd97ce1":"y_pred = model_2.predict(X_test)\n\ny_pred, y_pred.argmax(axis=-1), y_test.argmax(axis=-1), len(y_pred)\n\n# 15 out of 15 right!\n# array([1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0]),\n# array([1, 2, 1, 1, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0]),\n","04b32900":"Note the difference in model architecture: \n- In model_1 I used two hidden layers, a few units per hidden layer and results were good qith over 80% y_pred accuracy.\n- Then I found a user's [approach](https:\/\/www.kaggle.com\/venkatkrishnan\/iris-data-tensorflow-neural-network) to the multiclass classification model with more layers, many more hidden units (neurons), a Dropout layer and 100 epochs.\n- Their model yielded even better results - 100% y_pred accuracy.","669dc3bc":"The flowers dataset is well studied and is a good problem for practicing on neural networks because all of the 4 input variables are numeric and have the same scale in centimeters. Each instance describes the properties of an observed flower measurements and the output variable is specific iris species.\n\nThis is a multi-class classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species. This is an important type of problem on which to practice with neural networks because the three class values require specialized handling.\n\nThe iris flower dataset is a well-studied problem and a such we can expect to achieve a model accuracy in the range of 95% to 97%. This provides a good target to aim for when developing our models.","f70dce0d":"MULTICLASS CLASSIFICATION MODEL"}}