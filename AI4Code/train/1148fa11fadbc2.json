{"cell_type":{"e474dc49":"code","8ca8da20":"code","b251d3d1":"code","7ce0c133":"code","3cad8258":"code","18b810cd":"code","719d0252":"code","4e3dfa26":"code","ad1c7a75":"code","e7d92335":"code","628c10a8":"code","0434ef79":"code","c62bf730":"markdown","8ecacb23":"markdown","edd58de0":"markdown","e4a54017":"markdown","c537aeb0":"markdown","bcc30c7b":"markdown","f56b166f":"markdown","ad8660d3":"markdown","faef1cb6":"markdown","f26562fe":"markdown","e8bf5bd5":"markdown","38f75699":"markdown","1411ba42":"markdown"},"source":{"e474dc49":"from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.svm import LinearSVR, SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom yellowbrick.regressor import ResidualsPlot\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns","8ca8da20":"df = pd.read_csv('..\/input\/yacht-hydrodynamics-data-set\/yacht_hydro.csv')","b251d3d1":"df['log(Rr)'] =  np.log(df['Rr'])\n\n\nfig, ax = plt.subplots(1, 2, figsize=(15,6))\nsns.histplot(data=df, x='Rr', ax=ax[0], kde=True)\nsns.histplot(data=df,x='log(Rr)', ax=ax[1], kde=True)\nax[0].grid(False)\nax[1].grid(False)","7ce0c133":"print('Rr Skewness: {:.2f}'.format(df['Rr'].skew()))\nprint('Rr Kurtosis: {:.2f}'.format(df['Rr'].kurtosis()), end='\\n\\n')\nprint('log(Rr) Skewness: {:.2f}'.format(df['log(Rr)'].skew()))\nprint('log(Rr) Kurtosis: {:.2f}'.format(df['log(Rr)'].kurtosis()))","3cad8258":"def results(model,log=False):\n    X = df.drop(['Rr', 'log(Rr)'], axis=1)\n    y = df['Rr']\n    \n    if log:\n        model = TransformedTargetRegressor(regressor=model, func=np.log, inverse_func=np.exp)\n        \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=258)\n    \n    plt.figure(figsize=(12, 6))\n    viz = ResidualsPlot(model)\n    viz.fit(X_train, y_train)\n    viz.score(X_test, y_test)\n    viz.show()\n\n    return ","18b810cd":"results(LinearRegression())\nresults(LinearRegression(), True)","719d0252":"linear_svr = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor' , LinearSVR(C=1, epsilon=0.1, max_iter=1e4))])\n\nresults(linear_svr)\nresults(linear_svr, True)","4e3dfa26":"rbf_svr = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor' , SVR(C=1, epsilon=0.1, max_iter=1e4))])\n\nresults(rbf_svr)\nresults(rbf_svr, True)","ad1c7a75":"results(RandomForestRegressor(max_depth=4,  n_estimators=50))\nresults(RandomForestRegressor(max_depth=4,  n_estimators=50), True)","e7d92335":"results(ExtraTreesRegressor(max_depth=4,  n_estimators=50))\nresults(ExtraTreesRegressor(max_depth=4,  n_estimators=50), True)","628c10a8":"results(AdaBoostRegressor(learning_rate=0.1,  n_estimators=200))\nresults(AdaBoostRegressor(learning_rate=0.1,  n_estimators=200), True)","0434ef79":"results(GradientBoostingRegressor(max_depth=4, learning_rate=0.1,  n_estimators=200))\nresults(GradientBoostingRegressor(max_depth=4, learning_rate=0.1,  n_estimators=200), True)","c62bf730":"## Linear Regression","8ecacb23":"## Support Vector Machines (RBF Kernel)","edd58de0":"## Random Forest Regressor","e4a54017":"The purpose of this notebook is to evaluate how Log-Transformation on the target feature affects the models overall performance. We will test it for a variety of different regressors.","c537aeb0":"The smaller kurtosis and skewness indicates that log transformation leads to a more normal distribution.","bcc30c7b":"## Extra Trees Regressor","f56b166f":"## Conclusion","ad8660d3":"## AdaBoost Regressor","faef1cb6":"## Gradient Boosting Regressor","f26562fe":"Note: The Linear Regression Model its already a fine model, I didn't see that coming.","e8bf5bd5":"As expected, the Log transformation has a huge impact in linear models, and even in SVR with Radial Basis Function Kernel, increasing its accuracy and leading to a normally distribuited residuals, but also leads to heterosdacity, when the variance of residuals is not constant throught out the variable. When it comes to Ensemble Models, the Transformation doesn't improve accuracy in any degree. Actually, in some cases, worses the results.","38f75699":"## Introduction","1411ba42":"## Support Vector Machines (Linear Kernel)"}}