{"cell_type":{"92c51152":"code","5d1d8fef":"code","a48a0d42":"code","c472fd4b":"code","330fbea9":"code","dd63618b":"code","239aec04":"code","105bd12d":"code","7a0a7054":"code","e504ad1a":"code","74054112":"code","39959372":"code","d683666f":"code","c7b0e823":"code","b03e205b":"code","07d1415b":"markdown","937644c4":"markdown","2b3178da":"markdown","0204b50e":"markdown","0bb0a52b":"markdown","4a15c004":"markdown","85940bd9":"markdown","4169a02d":"markdown","188d3655":"markdown","75905b76":"markdown","58fae987":"markdown","57913220":"markdown","d48d2c90":"markdown","cfa635b2":"markdown","7c0ac372":"markdown","26826270":"markdown"},"source":{"92c51152":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\ndf = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\" , encoding=\"ISO-8859-1\")\ndf = df[['v1', 'v2']]\ndf = df.rename(columns = {'v1': 'label', 'v2': 'message'})\ndf.head(10)","5d1d8fef":"fig = plt.figure(figsize = (10,6))\nsns.countplot(data=df, x='label')","a48a0d42":"df['length'] = df['message'].apply(lambda x: len(x) - x.count(\" \"))\ndf.head()","c472fd4b":"plt.figure(figsize=(10, 5))\nbins = np.linspace(0, 200, 40)\nplt.hist(df[df['label']=='ham']['length'], bins, alpha=0.5, label='ham')\nplt.hist(df[df['label']=='spam']['length'], bins, alpha=0.5, label='spam')\nplt.legend(loc='upper left')","330fbea9":"import string\n\ndef count_punct(text):\n#     count = sum([1 for char in text if char in string.punctuation])\n    \n    count=0\n    for char in text:\n        if char in string.punctuation:\n            count+=1\n    \n    return round(count\/(len(text) - text.count(\" \")), 3)*100\n\ndf['punct%'] = df['message'].apply(lambda x: count_punct(x))\n\ndf.head()","dd63618b":"plt.figure(figsize=(10, 5))\nbins = np.linspace(0, 50, 40)\nplt.hist(df[df['label']=='ham']['punct%'], bins, alpha=0.5, label='ham')\nplt.hist(df[df['label']=='spam']['punct%'], bins, alpha=0.5, label='spam')\nplt.legend(loc='upper right')","239aec04":"#Data cleaning and preprocessing\nimport re\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(df)):\n    review = re.sub('[^a-zA-Z]', ' ', df['message'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","105bd12d":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ncv = TfidfVectorizer()\nX = cv.fit_transform(corpus).toarray()\n\ny=pd.get_dummies(df['label'])\ny=y.iloc[:,1].values","7a0a7054":"X_features = pd.concat([df['length'],df['punct%'],pd.DataFrame(X)], axis=1)\nX_features.head()","e504ad1a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size = 0.20, random_state = 0)","74054112":"from sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(X_train, y_train)","39959372":"y_pred=spam_detect_model.predict(X_test)","d683666f":"from sklearn.metrics import confusion_matrix\nconfusion_m = confusion_matrix(y_test,y_pred)\n\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test,y_pred)\n\naccuracy","c7b0e823":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=2500)\nX = cv.fit_transform(corpus).toarray()\n\ny=pd.get_dummies(df['label'])\ny=y.iloc[:,1].values\n\nX_features = pd.concat([df['length'],df['punct%'],pd.DataFrame(X)], axis=1)\nX_features.head()\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size = 0.20, random_state = 0)\n\n# Training model using Naive bayes classifier\n\nfrom sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(X_train, y_train)\n\ny_pred=spam_detect_model.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_m = confusion_matrix(y_test,y_pred)\n\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test,y_pred)\n\naccuracy","b03e205b":"from matplotlib import pyplot as plt\n\ncf_train_matrix = confusion_matrix(y_test,y_pred)\nplt.figure(figsize=(10,8))\nsns.heatmap(cf_train_matrix, annot=True, fmt='d')","07d1415b":"### Training model using Naive bayes classifier","937644c4":"### Creating Bag of words model to check the accuracy","2b3178da":"### Checking the length of each messages","0204b50e":"### Importing the Dataset","0bb0a52b":"### Prediction on test dataset","4a15c004":"### **EDA**\n### Checking the count of spam and ham messages","85940bd9":"### Data Cleaning & preprocessing\n* Removing the number, punctuation & other characters\n* Lowerig the sentences\n* Stemming\n* Removing stop-words","4169a02d":"### Accuracy checking by confusion matrix","188d3655":"### Histogarm to check the frequency of spam and ham messages with respect to length\n\n#### As we can see length of most of the ham messages are in between 25-75 and most of the spam messages are in between 100 - 150\n#### so length is also a feature to classify spam & ham messages","75905b76":"### Splitting the dataset","58fae987":"### Creating the TF - IDF model","57913220":"# **Problem Statement**\n### Building a model to classify the spam messages using TF \u2013 IDF, Na\u00efve Bayes & other NLP techniques.\n","d48d2c90":"### Histogarm to check the frequency of spam and ham messages with respect to percentage of punctuation\n#### As we can see most of the spam messages have punctuation percentage 0-10% but most of the ham messages have punctuation percentage beyond 10%\n#### so we will the feature too to our model","cfa635b2":"### Adding other feature like length of a sentence and percentage of punctuation in a sentence to the model","7c0ac372":"### Checking the percentage of punctuation in each sentences","26826270":"### As we can see BOW model gives more accuracy than TF - IDF model"}}