{"cell_type":{"c1c91ad8":"code","7fc60368":"code","63282f9b":"code","ddf912e9":"code","8eeafba3":"code","9af6ef3f":"code","dd6b4e09":"code","4a12c0dc":"code","46dbcc93":"code","882d1010":"code","c263f4b2":"code","c58df978":"code","b443c386":"code","4ee6719a":"code","c462f979":"code","6e376bf2":"code","a9f27572":"code","e8064544":"code","bd6a7a13":"code","c12d4eaa":"code","dee722bd":"code","78892250":"code","d23bbdd7":"code","037af728":"code","8accd7f2":"code","a657fea1":"code","070cdbb5":"code","22a59bc2":"code","edbf0001":"code","ee1b40b5":"code","6a70f8d3":"code","3442d2b7":"code","65dac709":"code","c4294cdb":"code","4b19ce13":"code","e4102ba7":"code","7eccc575":"code","51c55c43":"code","54abfdfe":"code","cb8eeb15":"code","bc16006c":"code","29f3fc48":"code","26dcbf97":"code","0384a3d7":"code","1743657a":"code","68eafb66":"markdown","15039103":"markdown","2e01d97b":"markdown","c8831a36":"markdown","84421fac":"markdown","6115417a":"markdown","201e4b9c":"markdown","6ee1526a":"markdown","2276f722":"markdown","a384aa0f":"markdown","076d8b87":"markdown","c62f2de9":"markdown","3df0119f":"markdown","6934fffe":"markdown","d872b3a2":"markdown","82c3e125":"markdown","440b8f5e":"markdown","eeeda472":"markdown","8c2a447a":"markdown","3eeebc98":"markdown","f5b50b3e":"markdown","d91932bb":"markdown","5788de49":"markdown","80551850":"markdown","da00d6a4":"markdown","eb82f035":"markdown","c8a7257d":"markdown"},"source":{"c1c91ad8":"import pandas as pd\nimport numpy as np\n\nimport re\nimport string\n\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import decomposition\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport seaborn as sns","7fc60368":"import pandas as pd\nimport numpy as np\n\ntrain_raw = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_raw = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","63282f9b":"train_raw.shape, test_raw.shape","ddf912e9":"train_raw[:3]","8eeafba3":"train_raw.info()","9af6ef3f":"test_raw.info()","dd6b4e09":"for i in range(10):\n    example = train_raw[ train_raw['target'] == 0 ]['text'][:10].tolist()\n    print(example[i])","4a12c0dc":"for i in range(10):\n    example = train_raw[ train_raw['target'] == 1 ]['text'][:10].tolist()\n    print(example[i])","46dbcc93":"df=pd.concat([train_raw,test_raw], sort=True, axis=0, ignore_index=True)\ndf.shape","882d1010":"import re\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndf['text']=df['text'].apply(lambda x : remove_URL(str(x)))","c263f4b2":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndf['text']=df['text'].apply(lambda x : remove_html(str(x)))\n","c58df978":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndf['text']=df['text'].apply(lambda x : remove_emoji(str(x)))","b443c386":"import string\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndf['text']=df['text'].apply(lambda x : remove_punct(x))","4ee6719a":"df[:7]","c462f979":"train = df.loc[:len(train_raw)-1, ['text', 'target']]\ntest = df.loc[len(train_raw):, ['text', 'target']]\ntrain.shape, test.shape","6e376bf2":"train['length'] = train['text'].apply(lambda x: len(x.split()))\ntrain[:2]","a9f27572":"import matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = (20, 6)\nbins = 200\nplt.hist(train[train['target'] == 0]['length'], alpha = 0.6, bins=bins, label='0')\nplt.hist(train[train['target'] == 1]['length'], alpha = 0.8, bins=bins, label='1')\nplt.legend(loc='upper right')\nplt.xlim(0,train['length'].max())\nplt.show()","e8064544":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nstop_words = set(stopwords.words('english'))\n\ncorpus_0 = []\ncorpus_1 = []\nfor i in range(len(train)):\n    words = word_tokenize(train.iloc[i]['text'])\n    for word in words:\n        word = word.lower()\n        if (word not in stop_words) and (word not in string.punctuation):\n            if train.iloc[i]['target'] == 0:\n                corpus_0.append(word)\n            else:\n                corpus_1.append(word)\n\nprint(len(corpus_0))\nprint(len(corpus_1))","bd6a7a13":"from collections import Counter\n\ncounter_0=Counter(corpus_0)\nmost_0=counter_0.most_common()\nmost_0[:10]","c12d4eaa":"counter_1=Counter(corpus_1)\nmost_1=counter_1.most_common()\nmost_1[:10]","dee722bd":"comment_words_list0 = []\n\nfor i in range(len(corpus_0)):\n    word = corpus_0[i]\n    comment_words_list0.append(word)\ncomment_words_0 = ' '.join(comment_words_list0)","78892250":"from wordcloud import WordCloud\n\nwordcloud_0 = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                min_font_size = 10).generate(comment_words_0)\n\nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud_0)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\nplt.title('Target 0', fontname = 'monospace')\nplt.show()","d23bbdd7":"comment_words_list1 = []\nfor i in range(len(corpus_1)):\n    word = corpus_1[i]\n    comment_words_list1.append(word)\ncomment_words_1 = ' '.join(comment_words_list1)\n\nwordcloud_1 = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                min_font_size = 10).generate(comment_words_1)\n\n# plot the WordCloud image                      \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud_1)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\nplt.title('Target 1', fontname = 'monospace')\nplt.show()","037af728":" from sklearn.model_selection import train_test_split\n\ntrain_X = train[\"text\"].values\ntrain_Y = train[\"target\"].values\ntest_X = test[\"text\"].values\n\nrandom_state_split = 42\ntrain_x, val_x, train_y, val_y = train_test_split(train_X, train_Y, \n                                                  test_size=0.2, shuffle=True,\n                                                  random_state=random_state_split)\nprint(train_x.shape, train_y.shape, val_x.shape, val_y.shape)","8accd7f2":"train_x[:3]","a657fea1":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer_5 =  CountVectorizer(min_df=0., max_df=1.0)\ntrain_x_counts_5 = count_vectorizer_5.fit_transform(train_x[:5])\nsample = pd.DataFrame(train_x_counts_5.A, columns=count_vectorizer_5.get_feature_names())\nsample","070cdbb5":"len(count_vectorizer_5.vocabulary_)","22a59bc2":"count_vectorizer = CountVectorizer(min_df=0., max_df=1.0)\ntrain_x_counts = count_vectorizer.fit_transform(train_x)\nval_x_counts = count_vectorizer.transform(val_x)","edbf0001":"print('Input of Train feature vector matrix shape: {}'.format(train_x_counts.shape))\nprint('Input of Validation feature vector matrix shape: {}'.format(val_x_counts.shape))","ee1b40b5":"most_1_dict = dict(most_1)\nprint(len(most_1_dict))\nlist(most_1_dict.keys())[-10:]","6a70f8d3":"for i in list(most_1_dict.keys()):\n    if most_1_dict[i] < 3:\n        del most_1_dict[i]\nprint(len(most_1_dict))\nlist(most_1_dict.keys())[-20:]","3442d2b7":"most_0_dict = dict(most_0)\nfor i in list(most_0_dict.keys()):\n    if most_0_dict[i] < 3:\n        del most_0_dict[i]\nprint(len(most_0_dict))\nlist(most_0_dict.keys())[-20:]","65dac709":"# combine two dictionaries\nmost_dict = most_0_dict.copy()\nmost_dict.update(most_1_dict)\nlen(most_dict.keys())","c4294cdb":"revised_train = train.copy()\nfor index in range(len(revised_train)):\n    text = revised_train.loc[index, 'text']\n    words = str(text).split()\n    revised_text = []\n    for word in words:\n        if word in list(most_dict.keys()):\n            revised_text.append(word)\n    revised_train.at[index, 'text'] = ' '.join(revised_text)","4b19ce13":"print('origianl       : ', train.loc[6879, 'text'])\nprint('revised version: ', revised_train.loc[6879, 'text'])","e4102ba7":"print('origianl       : ', train.loc[5781, 'text'])\nprint('revised version: ', revised_train.loc[5781, 'text'])","7eccc575":"print('origianl       : ', train.loc[0, 'text'])\nprint('revised version: ', revised_train.loc[0, 'text'])","51c55c43":"train_X = revised_train[\"text\"].values\ntrain_Y = revised_train[\"target\"].values\n\nrandom_state_split = 42\ntrain_x_revised, val_x_revised, train_y_revised, val_y_revised = train_test_split(\n    train_X, train_Y, \n    test_size=0.2, shuffle=False,\n    random_state=random_state_split)\n\ncount_vectorizer_new = CountVectorizer(min_df=0., max_df=1.0)\ntrain_x_counts_revised = count_vectorizer_new.fit_transform(train_x_revised)\nval_x_counts_revised = count_vectorizer_new.transform(val_x_revised)\n\ntrain_x_counts_revised.shape, val_x_counts_revised.shape","54abfdfe":"len(count_vectorizer_new.vocabulary_)","cb8eeb15":"train_x_df = pd.DataFrame(train_x_counts_revised.A, \n                          columns=count_vectorizer_new.get_feature_names())\ntrain_x_df","bc16006c":"val_x_df = pd.DataFrame(val_x_counts_revised.A, \n                          columns=count_vectorizer_new.get_feature_names())\nval_x_df","29f3fc48":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nscaler.fit(train_x_df.values)\ntrain_scaled = scaler.transform(train_x_df.values)\nval_scaled = scaler.transform(val_x_df.values)","26dcbf97":"from sklearn import decomposition\nfrom sklearn.neighbors import KNeighborsClassifier\n\nsvd = decomposition.TruncatedSVD(algorithm='randomized', \n                                 n_iter=10, random_state=0, tol=0.0)\ntrain_pca = svd.fit_transform(train_scaled)\nval_pca = svd.transform(val_scaled)","0384a3d7":"neigh = KNeighborsClassifier(n_neighbors=290, algorithm='kd_tree')\nneigh.fit(train_pca, train_y) \n\nneigh.score(val_pca, val_y)","1743657a":"# Bert","68eafb66":"Would there be any difference in text length?","15039103":"The Accuracy is not good. \n\nRather than doing a further parameter tunning or finding another model with this matrix,\n\nin order to find a good classifier,\n\nwe need a better feature representation.\n\nPre-trained one, like Bert, converts 'the given words' to vectors quite well.","2e01d97b":"The number of features(15470) is greater than those of examples(6090).\n\nIf the matrix is too fat, learning is not easy.\n\nWe need to do dimensionality reduction like PCA or manifold.\n\nHere, I'll directly select the important features using 'most_0' and 'most_1'.\n\nIf key word is appeared in corpus less than three times, remove it. \n\nBecause if the frequency is more than 2, it can be taken as not coincidence.","c8831a36":"## Vectorize features","84421fac":"The less frequent words seem like not very informative for model to learn.","6115417a":"## 0. Import libraries <a class=\"anchor\" id=\"2\"><\/a>","201e4b9c":"## 1. Explore Data <a class=\"anchor\" id=\"3\"><\/a>","6ee1526a":"'Target 0' contains more about self and personal life while 'Target 1' is more about outter world issue or accident.","2276f722":"Before go any deeper, I'll clean the text first so we can only handle the words relevant.","a384aa0f":"1) CountVectorizer\n\nCountvectorizer counts(using one hot encoding) how many times the given word apperas in the text.","076d8b87":"[Go to Top](#0)","c62f2de9":"With clean version of text,\nconvert our features(each words in the text) to vectors.\n\nWhat mapping method would be good?","3df0119f":"In general, the text of target 1 is slightly shorter than those of target 0.","6934fffe":"What are the top 10 words that shows frequently in target 0 and target 1?","d872b3a2":"Let's see the feature matrix.\ntake some sample.","82c3e125":"We have a saprse matrix. ","440b8f5e":"After revising, relatively essential key words are left.","eeeda472":"Thanks to https:\/\/www.geeksforgeeks.org\/generating-word-cloud-python\/","8c2a447a":"After deleting those words, the less frequent words gets closer to 'disaster'.\n\nOur model might see these words useful.","3eeebc98":"What is the difference bwetween target value 1 and 0?\nLet's take a glimpse.","f5b50b3e":"To vectorizer, feed the revised version of data.","d91932bb":"After finishing mapping of all train_x, the number of features ends up with 15470.","5788de49":"reference:\n\nhttps:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert","80551850":"It looks like the texts are classified by some particular keywords. For example, ''Target 1 text' has the words which has something to do with disaster such as evacuation, earthquake or fire.","da00d6a4":"## To be continued...\n\nThank you for reading :)","eb82f035":"Before 15470, now the feature size gets down to 2902.","c8a7257d":"## 1.1. Clean Data"}}