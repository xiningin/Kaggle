{"cell_type":{"ef19c56f":"code","d4c11f26":"code","f2e4f09b":"code","a442a372":"code","983cbbab":"code","4888fb8e":"code","d18f80b9":"code","c13c58ae":"code","09fcec4f":"code","0f9c28b2":"code","a370dbb3":"code","702bbe08":"code","696e0a68":"code","d6f40a6a":"code","649fee2a":"code","3cf81c42":"code","e75ef767":"code","bdd7c588":"code","c243c35c":"code","ae49dd85":"code","319a90b0":"code","2b4b6c71":"code","f2dae7c8":"code","1be814c2":"code","749c975b":"code","2b711234":"code","1e6827b7":"code","c6934359":"code","f55ecc64":"code","959b4aaf":"code","00c594ef":"code","2124629d":"code","816581d3":"code","af973792":"code","afe1361a":"code","3fac92a0":"code","38b4f5d3":"code","84c23dc2":"code","eca6c83f":"code","a3322695":"code","3068d2c9":"code","97e22df3":"markdown","c8768260":"markdown","4e35caf5":"markdown","1c342397":"markdown","940ce5f5":"markdown","02e5231f":"markdown","a77c67dc":"markdown","e6f5e46f":"markdown","33a94217":"markdown","39e9e5c0":"markdown","ba5138b9":"markdown","5b0522ef":"markdown","799ce0fc":"markdown"},"source":{"ef19c56f":"!git clone https:\/\/github.com\/weizaiff\/python_paint_api.git","d4c11f26":"!ls python_paint_api\/","f2e4f09b":"import pandas as pd\nfrom python_paint_api import paint_func\nfrom collections import Counter\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tqdm import tqdm\nimport numpy as np","a442a372":"df_data =pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')","983cbbab":"df_data.sample(10)","4888fb8e":"len(df_data)","d18f80b9":"df_data.hist(column=['worker'])","c13c58ae":"df_data['worker'].nunique()","09fcec4f":"def get_len(text):\n    return len(text)","0f9c28b2":"df_data['less_toxic_len'] = df_data['less_toxic'].apply(get_len)","a370dbb3":"df_data['more_toxic_len'] = df_data['more_toxic'].apply(get_len)","702bbe08":"df_data.hist(column=['less_toxic_len'])","696e0a68":"df_data.hist(column=['more_toxic_len'])","d6f40a6a":"paint_func.bar_char_show_values(df_data.hist(column=['less_toxic_len'],bins=[0,200,400,600,800,1000]))","649fee2a":"df_data.hist(column=['more_toxic_len'],bins=[0,200,400,600,800,1000])","3cf81c42":"len(Counter(df_data['less_toxic'].values.reshape(-1).tolist()))","e75ef767":"(Counter(df_data['less_toxic'].values.reshape(-1).tolist())).most_common(10)","bdd7c588":"len(Counter(df_data['more_toxic'].values.reshape(-1).tolist()))","c243c35c":"(Counter(df_data['more_toxic'].values.reshape(-1).tolist())).most_common(10)","ae49dd85":"len(set(df_data['more_toxic'].values.reshape(-1).tolist()).intersection(set(df_data['less_toxic'].values.reshape(-1).tolist())))","319a90b0":"import nltk\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english'))\nstop_words= stopwords.words('english')","2b4b6c71":"\n# \u83b7\u53d6tf-idf\u7684\u503c\ndef get_tfidf(doc_list,stop_word_list=stop_words ,max_features=len(df_data)\/\/2,min_df=10,norm ='l2'):\n    \n    # do lower case\n    \n    doc_list = [idoc.strip().lower() for idoc in doc_list]\n    vectorizer = TfidfVectorizer(max_features=max_features,min_df=min_df,max_df=1.0,stop_words=stop_word_list, norm =norm )\n    vectors = vectorizer.fit_transform(doc_list)\n    feature_names = vectorizer.get_feature_names()\n    dense = vectors.todense()\n    denselist = dense.tolist()\n    #print(denselist)\n    df = pd.DataFrame(denselist, columns=feature_names)\n    #get word weight\n    index_value={i[1]:i[0] for i in vectorizer.vocabulary_.items()}\n    word_weight = []\n    for row in vectors:\n        word_weight.append({index_value[column]:value for (column,value) in zip(row.indices,row.data)})\n    \n    res={}\n    res['df']= df\n    res['word_weight']=word_weight\n    res['idf']=vectorizer.idf_\n    return res\n# get key word per sentence \/\u6839\u636e\u6587\u672c\u7684\u5206\u8bcd\u540e\u7684\u8bcd \u786e\u5b9aword_weight \u4e2d\u6bcf\u4e2a\u53e5\u5b50\u4e2d\u5173\u952e\u8bcd\u7684\u4f4d\u7f6e\ndef get_keyword_persen(sentence_seged,sen_weight):\n    \n    per_sen_key_word=[]\n    for iseg_word_list,iword_weight_dict in tqdm(zip(sentence_seged,sen_weight)):\n        per_sen_keyword_set=list()\n        for i_word in iseg_word_list:\n            if i_word in iword_weight_dict and i_word not in per_sen_keyword_set:\n                per_sen_keyword_set.append(i_word)\n        per_sen_key_word.append((per_sen_keyword_set))\n    return per_sen_key_word\n# sorted key words \/\/tfidf \u83b7\u53d6\u91cd\u8981\u8bcd\u7684\u6392\u5e8f\ndef get_keyword_seq(df_tfidf):\n    \n    data=np.max(df_tfidf.values,axis=0)\n    columns_list=list(df_tfidf.columns)\n    key_weight_dict={}\n    for ikey,ival in zip(columns_list,data):\n        key_weight_dict[ikey]=ival\n    sorted_dict=sorted(key_weight_dict.items(),key=lambda x:x[1],reverse=True)\n    sorted_word_list=[]\n    sorted_word_list_for_paint=[]\n    for itup in sorted_dict:\n        sorted_word_list.append(itup[0]+\"_\"+str(itup[1]))\n        sorted_word_list_for_paint.append(itup[0])\n    return sorted_word_list,sorted_word_list_for_paint","f2dae7c8":"all_doc = (list(set(df_data['less_toxic'].values.reshape(-1).tolist())))\nall_doc = [idoc.strip()  for idoc in all_doc]\nall_doc_seg_list = [idoc.split(' ') for idoc in all_doc]\ntfidf_res = get_tfidf(all_doc)\n\ndf_tfidf,sen_weight=tfidf_res['df'],tfidf_res['word_weight']\nper_sen_key_word=get_keyword_persen(all_doc_seg_list,sen_weight)\nprint(len(df_tfidf),len(per_sen_key_word))\ntfidf_res['tfidf_res']=per_sen_key_word\nsorted_word_list,sorted_word_list_for_paint = get_keyword_seq(df_tfidf)","1be814c2":"sorted_word_list,sorted_word_list_for_paint = get_keyword_seq(df_tfidf)","749c975b":"num_top_word_toshow =100\nsorted_word_list_draw =sorted_word_list[:100]\nsorted_word_list_for_paint_draw = sorted_word_list_for_paint[:100]","2b711234":"paint_func.draw_wordcolud(sorted_word_list_for_paint_draw)","1e6827b7":"all_doc = (list(set(df_data['more_toxic'].values.reshape(-1).tolist())))\nall_doc = [idoc.strip()  for idoc in all_doc]\nall_doc_seg_list = [idoc.split(' ') for idoc in all_doc]\ntfidf_res = get_tfidf(all_doc)\n\ndf_tfidf,sen_weight=tfidf_res['df'],tfidf_res['word_weight']\nper_sen_key_word=get_keyword_persen(all_doc_seg_list,sen_weight)\nprint(len(df_tfidf),len(per_sen_key_word))\ntfidf_res['tfidf_res']=per_sen_key_word\nsorted_word_list,sorted_word_list_for_paint = get_keyword_seq(df_tfidf)","c6934359":"num_top_word_toshow =100\nsorted_word_list_draw =sorted_word_list[:100]\nsorted_word_list_for_paint_draw = sorted_word_list_for_paint[:100]","f55ecc64":"paint_func.draw_wordcolud(sorted_word_list_for_paint_draw)","959b4aaf":"from transformers import AutoTokenizer\nimport pandas as pd","00c594ef":"from transformers import AutoTokenizer\nimport pandas as pd","2124629d":"tokenizer = AutoTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")","816581d3":"df_val = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\ndf_test =pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')","af973792":"df_val.head()","afe1361a":"df_test.head()","3fac92a0":"val_text = list(set(df_val[['less_toxic','more_toxic']].values.reshape(-1).tolist()))\ntest_text = list(set(df_test[['text']].values.reshape(-1).tolist()))","38b4f5d3":"def get_token_len(text_list):\n    df_token = pd.DataFrame()\n    token_len =[]\n    for itext in text_list:\n        token_len.append(len(tokenizer.encode(itext)))\n    df_token['token_len'] = token_len\n    return df_token","84c23dc2":"df_val_len = get_token_len(val_text)","eca6c83f":"df_test_len = get_token_len(test_text)","a3322695":"df_val_len.hist(bins=[0,100,200,500,1000,2000])","3068d2c9":"df_test_len.hist(bins=[0,100,200,500,1000,2000])","97e22df3":"### less toxic part","c8768260":"## text len of less toxic and more toxic data","4e35caf5":"## download python code for drawing","1c342397":"## intersection of less and more toxic samples","940ce5f5":"## read data","02e5231f":"### more toxic part","a77c67dc":"## the most top*  words from tf-idf","e6f5e46f":"## unique text of less toxic one and more toxic one","33a94217":"### length between 0-1000","39e9e5c0":"## validation data number","ba5138b9":"### valid data len after tokenized","5b0522ef":"## samples","799ce0fc":"## worker number"}}