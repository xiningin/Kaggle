{"cell_type":{"7fd0802e":"code","17976582":"code","3b89e437":"code","8f61cd0c":"code","490c5037":"code","2b3daed7":"code","3d8b68ea":"code","101485bb":"code","58193116":"code","ec528766":"code","d2aa202b":"code","b22c7065":"markdown","2490a044":"markdown","360a0117":"markdown","11e16c06":"markdown","2fcd6cc7":"markdown","0c9a3270":"markdown","408e9fba":"markdown","8bf6a1b9":"markdown","39f7574e":"markdown"},"source":{"7fd0802e":"from pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nimport seaborn as sns\nimport os\nimport random\n\ntrain = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv',index_col=0)\ntest  = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv', index_col=0)","17976582":"train.head()","3b89e437":"#From: https:\/\/www.kaggle.com\/artgor\/ventilator-pressure-prediction-eda-fe-and-models\nfig, ax1 = plt.subplots(figsize = (12, 8))\n\nbreath_1 = train.loc[train['breath_id'] == 1]\nax2 = ax1.twinx()\n\nax1.plot(breath_1['time_step'], breath_1['pressure'], 'r-', label='pressure')\nax1.plot(breath_1['time_step'], breath_1['u_in'], 'g-', label='u_in')\nax2.plot(breath_1['time_step'], breath_1['u_out'], 'b-', label='u_out')\n\nax1.set_xlabel('Timestep')\nax1.legend(loc=(1.1, 0.8))\nax2.legend(loc=(1.1, 0.7))\nplt.show()","8f61cd0c":"#From: https:\/\/www.kaggle.com\/gogo827jz\/inference-demo-notebook\n\nfor trans in ['rank', 'cumcount', 'cummax', 'cummin', 'cumsum', 'shift','diff']:\n    for feat in ['u_in','u_out']:\n        feat_trans = feat + '_' + trans\n        train[feat_trans] = train.groupby('breath_id')[feat].transform(trans)","490c5037":"breath_1 = train.loc[train['breath_id'] == 1]\n\nfor feat in ['u_in','u_out']:\n    \n    plt.plot(breath_1['time_step'], breath_1[feat], color=(random.random(), random.random(), random.random()), label=feat)\n    plt.plot(breath_1['time_step'], breath_1['pressure'], 'r-', label='pressure')\n    plt.title(feat+' v.s. target',size=20)\n    plt.show()\n    \n    for trans in ['rank', 'cumcount', 'cummax', 'cummin', 'cumsum', 'shift','diff']:\n        feat_trans = feat + '_' + trans\n        #train[feat_trans] = train.groupby('breath_id')[feat].transform(trans)\n        plt.plot(breath_1['time_step'], breath_1[feat_trans], color=(random.random(), random.random(), random.random()), label=feat_trans)\n        plt.plot(breath_1['time_step'], breath_1['pressure'], 'r-', label='pressure')\n        plt.title(feat_trans+' v.s. target',size=20)\n        plt.show()","2b3daed7":"def _roll(a, shift):\n    \"\"\" Roll 1D array elements. Improves the performance of numpy.roll()\"\"\"\n\n    if not isinstance(a, np.ndarray):\n        a = np.asarray(a)\n    idx = shift % len(a)\n    return np.concatenate([a[-idx:], a[:-idx]])\n\n\ndef _get_length_sequences_where(x):\n    \"\"\" This method calculates the length of all sub-sequences where the array x is either True or 1. \"\"\"\n    if len(x) == 0:\n        return [0]\n    else:\n        res = [len(list(group)) for value, group in itertools.groupby(x) if value == 1]\n        return res if len(res) > 0 else [0]\n\ndef _aggregate_on_chunks(x, f_agg, chunk_len):\n    \"\"\"Takes the time series x and constructs a lower sampled version of it by applying the aggregation function f_agg on\n    consecutive chunks of length chunk_len\"\"\"\n    \n    return [\n        getattr(x[i * chunk_len : (i + 1) * chunk_len], f_agg)()\n        for i in range(int(np.ceil(len(x) \/ chunk_len)))\n    ]\n\ndef _into_subchunks(x, subchunk_length, every_n=1):\n    \"\"\"Split the time series x into subwindows of length \"subchunk_length\", starting every \"every_n\".\"\"\"\n    len_x = len(x)\n\n    assert subchunk_length > 1\n    assert every_n > 0\n\n    # how often can we shift a window of size subchunk_length over the input?\n    num_shifts = (len_x - subchunk_length) \/\/ every_n + 1\n    shift_starts = every_n * np.arange(num_shifts)\n    indices = np.arange(subchunk_length)\n\n    indexer = np.expand_dims(indices, axis=0) + np.expand_dims(shift_starts, axis=1)\n    return np.asarray(x)[indexer]\n\n\ndef set_property(key, value):\n    \"\"\"\n    This method returns a decorator that sets the property key of the function to value\n    \"\"\"\n\n    def decorate_func(func):\n        setattr(func, key, value)\n        if func.__doc__ and key == \"fctype\":\n            func.__doc__ = (\n                func.__doc__ + \"\\n\\n    *This function is of type: \" + value + \"*\\n\"\n            )\n        return func\n\n    return decorate_func","3d8b68ea":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef median(x):\n    return np.median(x)\n\ndef mean(x):\n    return np.mean(x)\n\ndef length(x):\n    return len(x)\n\ndef standard_deviation(x):\n    return np.std(x)\n\ndef large_standard_deviation(x):\n    if (np.max(x)-np.min(x)) == 0:\n        return np.nan\n    else:\n        return np.std(x)\/(np.max(x)-np.min(x))\n\ndef variation_coefficient(x):\n    mean = np.mean(x)\n    if mean != 0:\n        return np.std(x) \/ mean\n    else:\n        return np.nan\n\ndef variance_std_ratio(x):\n    y = np.var(x)\n    if y != 0:\n        return y\/np.sqrt(y)\n    else:\n        return np.nan\n\ndef ratio_beyond_r_sigma(x, r):\n    if x.size == 0:\n        return np.nan\n    else:\n        return np.sum(np.abs(x - np.mean(x)) > r * np.asarray(np.std(x))) \/ x.size\n\ndef range_ratio(x):\n    mean_median_difference = np.abs(np.mean(x) - np.median(x))\n    max_min_difference = np.max(x) - np.min(x)\n    if max_min_difference == 0:\n        return np.nan\n    else:\n        return mean_median_difference \/ max_min_difference\n    \ndef has_duplicate_max(x):\n    return np.sum(x == np.max(x)) >= 2\n\ndef has_duplicate_min(x):\n    return np.sum(x == np.min(x)) >= 2\n\ndef has_duplicate(x):\n    return x.size != np.unique(x).size\n\ndef count_duplicate_max(x):\n    return np.sum(x == np.max(x))\n\ndef count_duplicate_min(x):\n    return np.sum(x == np.min(x))\n\ndef count_duplicate(x):\n    return x.size - np.unique(x).size\n\ndef sum_values(x):\n    if len(x) == 0:\n        return 0\n    return np.sum(x)\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef realized_abs_skew(series):\n    return np.power(np.abs(np.sum(series**3)),1\/3)\n\ndef realized_skew(series):\n    return np.sign(np.sum(series**3))*np.power(np.abs(np.sum(series**3)),1\/3)\n\ndef realized_vol_skew(series):\n    return np.power(np.abs(np.sum(series**6)),1\/6)\n\ndef realized_quarticity(series):\n    return np.power(np.sum(series**4),1\/4)\n\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef count(series):\n    return series.size\n\n#drawdons functions are mine\ndef maximum_drawdown(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n    k = series[np.argmax(np.maximum.accumulate(series) - series)]\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i])<1:\n        return np.NaN\n    else:\n        j = np.max(series[:i])\n    return j-k\n\ndef maximum_drawup(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n    \n\n    series = - series\n    k = series[np.argmax(np.maximum.accumulate(series) - series)]\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i])<1:\n        return np.NaN\n    else:\n        j = np.max(series[:i])\n    return j-k\n\ndef drawdown_duration(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n\n    k = np.argmax(np.maximum.accumulate(series) - series)\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i]) == 0:\n        j=k\n    else:\n        j = np.argmax(series[:i])\n    return k-j\n\ndef drawup_duration(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n\n    series=-series\n    k = np.argmax(np.maximum.accumulate(series) - series)\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i]) == 0:\n        j=k\n    else:\n        j = np.argmax(series[:i])\n    return k-j\n\ndef max_over_min(series):\n    if len(series)<2:\n        return 0\n    if np.min(series) == 0:\n        return np.nan\n    return np.max(series)\/np.min(series)\n\ndef max_over_min_sq(series):\n    if len(series)<2:\n        return 0\n    if np.min(series) == 0:\n        return np.nan\n    return np.square(np.max(series)\/np.min(series))\n\ndef mean_n_absolute_max(x, number_of_maxima = 1):\n    \"\"\" Calculates the arithmetic mean of the n absolute maximum values of the time series.\"\"\"\n    assert (\n        number_of_maxima > 0\n    ), f\" number_of_maxima={number_of_maxima} which is not greater than 1\"\n\n    n_absolute_maximum_values = np.sort(np.absolute(x))[-number_of_maxima:]\n\n    return np.mean(n_absolute_maximum_values) if len(x) > number_of_maxima else np.NaN\n\n\ndef count_above(x, t):\n    if len(x)==0:\n        return np.nan\n    else:\n        return np.sum(x >= t) \/ len(x)\n\ndef count_below(x, t):\n    if len(x)==0:\n        return np.nan\n    else:\n        return np.sum(x <= t) \/ len(x)\n\n#number of valleys = number_peaks(-x, n)\ndef number_peaks(x, n):\n    \"\"\"\n    Calculates the number of peaks of at least support n in the time series x. A peak of support n is defined as a\n    subsequence of x where a value occurs, which is bigger than its n neighbours to the left and to the right.\n    \"\"\"\n    x_reduced = x[n:-n]\n\n    res = None\n    for i in range(1, n + 1):\n        result_first = x_reduced > _roll(x, i)[n:-n]\n\n        if res is None:\n            res = result_first\n        else:\n            res &= result_first\n\n        res &= x_reduced > _roll(x, -i)[n:-n]\n    return np.sum(res)\n\ndef mean_abs_change(x):\n    return np.mean(np.abs(np.diff(x)))\n\ndef mean_change(x):\n    x = np.asarray(x)\n    return (x[-1] - x[0]) \/ (len(x) - 1) if len(x) > 1 else np.NaN\n\ndef mean_second_derivative_central(x):\n    x = np.asarray(x)\n    return (x[-1] - x[-2] - x[1] + x[0]) \/ (2 * (len(x) - 2)) if len(x) > 2 else np.NaN\n\n\ndef median(x):\n    return np.median(x)\n\ndef mean(x):\n    return np.mean(x)\n\ndef length(x):\n    return len(x)\n\ndef standard_deviation(x):\n    return np.std(x)\n\ndef variation_coefficient(x):\n    mean = np.mean(x)\n    if mean != 0:\n        return np.std(x) \/ mean\n    else:\n        return np.nan\n\ndef variance(x):\n    return np.var(x)\n\ndef skewness(x):\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    return pd.Series.skew(x)\n\ndef kurtosis(x):\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    return pd.Series.kurtosis(x)\n\ndef root_mean_square(x):\n    return np.sqrt(np.mean(np.square(x))) if len(x) > 0 else np.NaN\n\ndef absolute_sum_of_changes(x):\n    return np.sum(np.abs(np.diff(x)))\n\ndef longest_strike_below_mean(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.max(_get_length_sequences_where(x < np.mean(x))) if x.size > 0 else 0\n\ndef longest_strike_above_mean(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.max(_get_length_sequences_where(x > np.mean(x))) if x.size > 0 else 0\n\ndef count_above_mean(x):\n    m = np.mean(x)\n    return np.where(x > m)[0].size\n\ndef count_below_mean(x):\n    m = np.mean(x)\n    return np.where(x < m)[0].size\n\ndef last_location_of_maximum(x):\n    x = np.asarray(x)\n    return 1.0 - np.argmax(x[::-1]) \/ len(x) if len(x) > 0 else np.NaN\n\ndef first_location_of_maximum(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.argmax(x) \/ len(x) if len(x) > 0 else np.NaN\n\ndef last_location_of_minimum(x):\n    x = np.asarray(x)\n    return 1.0 - np.argmin(x[::-1]) \/ len(x) if len(x) > 0 else np.NaN\n\ndef first_location_of_minimum(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.argmin(x) \/ len(x) if len(x) > 0 else np.NaN\n\n# Test non-consecutive non-reoccuring values ?\ndef percentage_of_reoccurring_values_to_all_values(x):\n    if len(x) == 0:\n        return np.nan\n    unique, counts = np.unique(x, return_counts=True)\n    if counts.shape[0] == 0:\n        return 0\n    return np.sum(counts > 1) \/ float(counts.shape[0])\n\ndef percentage_of_reoccurring_datapoints_to_all_datapoints(x):\n    if len(x) == 0:\n        return np.nan\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    value_counts = x.value_counts()\n    reoccuring_values = value_counts[value_counts > 1].sum()\n    if np.isnan(reoccuring_values):\n        return 0\n\n    return reoccuring_values \/ x.size\n\n\ndef sum_of_reoccurring_values(x):\n    unique, counts = np.unique(x, return_counts=True)\n    counts[counts < 2] = 0\n    counts[counts > 1] = 1\n    return np.sum(counts * unique)\n\ndef sum_of_reoccurring_data_points(x):\n    unique, counts = np.unique(x, return_counts=True)\n    counts[counts < 2] = 0\n    return np.sum(counts * unique)\n\ndef ratio_value_number_to_time_series_length(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    if x.size == 0:\n        return np.nan\n\n    return np.unique(x).size \/ x.size\n\ndef abs_energy(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.dot(x, x)\n\ndef quantile(x, q):\n    if len(x) == 0:\n        return np.NaN\n    return np.quantile(x, q)\n\n# crossing the mean ? other levels ? \ndef number_crossing_m(x, m):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    # From https:\/\/stackoverflow.com\/questions\/3843017\/efficiently-detect-sign-changes-in-python\n    positive = x > m\n    return np.where(np.diff(positive))[0].size\n\ndef maximum(x):\n    return np.max(x)\n\ndef absolute_maximum(x):\n    return np.max(np.absolute(x)) if len(x) > 0 else np.NaN\n\ndef minimum(x):\n    return np.min(x)\n\ndef value_count(x, value):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    if np.isnan(value):\n        return np.isnan(x).sum()\n    else:\n        return x[x == value].size\n\ndef range_count(x, min, max):\n    return np.sum((x >= min) & (x < max))\n\ncount_above_0 = lambda x: count_above(x,0)\ncount_above_0.__name__ = 'count_above_0'\n\ncount_below_0 = lambda x: count_below(x,0)\ncount_below_0.__name__ = 'count_below_0'\n\nvalue_count_0 = lambda x: value_count(x,0)\nvalue_count_0.__name__ = 'value_count_0'\n\ncount_near_0 = lambda x: range_count(x,-0.00001,0.00001)\ncount_near_0.__name__ = 'count_near_0_0'\n\nratio_beyond_01_sigma = lambda x: ratio_beyond_r_sigma(x,0.1)\nratio_beyond_01_sigma.__name__ = 'ratio_beyond_01_sigma'\n\nratio_beyond_02_sigma = lambda x: ratio_beyond_r_sigma(x,0.2)\nratio_beyond_02_sigma.__name__ = 'ratio_beyond_02_sigma'\n\nratio_beyond_03_sigma = lambda x: ratio_beyond_r_sigma(x,0.3)\nratio_beyond_03_sigma.__name__ = 'ratio_beyond_03_sigma'\n\nnumber_crossing_0 = lambda x: number_crossing_m(x,0)\nnumber_crossing_0.__name__ = 'number_crossing_0'\n\nquantile_01 = lambda x: quantile(x,0.1)\nquantile_01.__name__ = 'quantile_01'\n\nquantile_025 = lambda x: quantile(x,0.25)\nquantile_025.__name__ = 'quantile_025'\n\nquantile_075 = lambda x: quantile(x,0.75)\nquantile_075.__name__ = 'quantile_075'\n\nquantile_09 = lambda x: quantile(x,0.9)\nquantile_09.__name__ = 'quantile_09'\n\nnumber_peaks_2 = lambda x: number_peaks(x,2)\nnumber_peaks_2.__name__ = 'number_peaks_2'\n\nmean_n_absolute_max_2 = lambda x: mean_n_absolute_max(x,2)\nmean_n_absolute_max_2.__name__ = 'mean_n_absolute_max_2'\n\nnumber_peaks_5 = lambda x: number_peaks(x,5)\nnumber_peaks_5.__name__ = 'number_peaks_5'\n\nmean_n_absolute_max_5 = lambda x: mean_n_absolute_max(x,5)\nmean_n_absolute_max_5.__name__ = 'mean_n_absolute_max_5'\n\nnumber_peaks_10 = lambda x: number_peaks(x,10)\nnumber_peaks_10.__name__ = 'number_peaks_10'\n\nmean_n_absolute_max_10 = lambda x: mean_n_absolute_max(x,10)\nmean_n_absolute_max_10.__name__ = 'mean_n_absolute_max_10'\n\nget_first = lambda x: x.iloc[0]\nget_first.__name__ = 'get_first'\n\nget_last = lambda x: x.iloc[-1]\nget_last.__name__ = 'get_last'","101485bb":"base_stats = [mean,sum,length,standard_deviation]#,variation_coefficient,variance,skewness,kurtosis]\nhigher_order_stats = [abs_energy,root_mean_square,sum_values,realized_volatility,realized_abs_skew,realized_skew,realized_vol_skew,realized_quarticity]\nmin_median_max = [minimum,median,maximum]\nadditional_quantiles = [quantile_01,quantile_025,quantile_075,quantile_09]\nother_min_max = [absolute_maximum,max_over_min,max_over_min_sq]\nmin_max_positions = [last_location_of_maximum,first_location_of_maximum,last_location_of_minimum,first_location_of_minimum]\npeaks = [number_peaks_2, mean_n_absolute_max_2, number_peaks_5, mean_n_absolute_max_5, number_peaks_10, mean_n_absolute_max_10]\ncounts = [count_unique,count,count_above_0,count_below_0,value_count_0,count_near_0]\nreoccuring_values = [count_above_mean,count_below_mean,percentage_of_reoccurring_values_to_all_values,percentage_of_reoccurring_datapoints_to_all_datapoints,sum_of_reoccurring_values,sum_of_reoccurring_data_points,ratio_value_number_to_time_series_length]\ncount_duplicate = [count_duplicate,count_duplicate_min,count_duplicate_max]\nvariations = [mean_abs_change,mean_change,mean_second_derivative_central,absolute_sum_of_changes,number_crossing_0]\nranges = [variance_std_ratio,ratio_beyond_01_sigma,ratio_beyond_02_sigma,ratio_beyond_03_sigma,large_standard_deviation,range_ratio]\nget_first_fn = [get_first,get_last]\n\ndraw_functions = [maximum_drawdown,maximum_drawup,drawdown_duration,drawup_duration]\n\nall_functions = base_stats + higher_order_stats + min_median_max + additional_quantiles + other_min_max + min_max_positions + peaks + counts + variations + ranges \n\n# not usefull\n#+ get_first\n# too slow\n#+ reoccuring_values + count_duplicate ","58193116":"create_feature_dict = {\n    'u_in': base_stats+min_median_max+get_first_fn+min_max_positions+draw_functions,\n    'u_out': base_stats+draw_functions,\n    'u_in_rank':[],\n    'u_out_rank':[],\n    'u_in_cumcount':[],\n    'u_out_cumcount':[],\n    'u_in_cummax':[],\n    'u_out_cummax':[],\n    'u_in_cummin':[],\n    'u_out_cummin':[],\n    'u_in_cumsum':get_first_fn,\n    'u_out_cumsum':[],\n    'u_in_shift':[],\n    'u_out_shift':[],\n    'u_in_diff':get_first_fn,\n    'u_out_diff':[],\n}","ec528766":"%%time\n\ntrain_features = train.groupby('breath_id').agg(create_feature_dict)\ntrain_features.columns = ['_'.join(col) for col in train_features.columns]","d2aa202b":"train_features.head()","b22c7065":"# Agregation dictionnary\n\nFrom looking at the graphics we are interested in at least:\n\n    - first, last values of u_min\n    \n    - Realtive positions of min and max\n    \n    - draw_up, draw_down functions (those are financial concepts reflecting biggest moves in a given direction)","2490a044":"# Base Visualisation","360a0117":"# Read the data","11e16c06":"# Base features","2fcd6cc7":"# Time series agregation functions\n\nAs some of my time series Feature enginerring notebook were appreciated in another time series competition (https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/overview) I figured I might share some of it here. It would be cool if we can get something working on financial time series to be helpfull in the health domain.\n\n","0c9a3270":"# Tools\nMostly come from the TSfresh package (https:\/\/tsfresh.readthedocs.io\/en\/latest\/authors.html).","408e9fba":"# Agregate - check for time","8bf6a1b9":"As expected the pressure grows with the cumulated influx. However it also seems that u_in_diff is very anticorrelated with the pressure. Breathing in more is more difficult as the pressure increase ? ","39f7574e":"We have u_in the influx of air, u_out representing a valve opening and the pressure that increase with the influx, then decrease with the valve opening. We might want to look at some transformation of the data. "}}