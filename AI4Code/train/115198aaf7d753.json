{"cell_type":{"edfec7d2":"code","fe50766d":"code","a38f72ef":"code","a49cb55a":"code","0e7a2481":"code","26df2b17":"code","86fc8148":"code","605691fd":"code","78cfe730":"code","df2c654d":"code","fa47fa61":"code","53e0a4c3":"code","6f7d1c76":"code","d758fae9":"code","2baa6c5d":"code","9ba904e1":"code","01f6f89c":"code","3904c46e":"code","b7bf78ae":"code","5c98a102":"code","e78cc23b":"code","f02f7a81":"code","32667f1e":"code","40a0bc90":"code","d41b458c":"code","20e75e06":"code","01f357ce":"markdown","dbf0a01d":"markdown","a82f1671":"markdown","00217f01":"markdown","73d3aacb":"markdown","572ca805":"markdown","0277a98a":"markdown","f64a4385":"markdown","ddcc9397":"markdown","e452aaa8":"markdown","2a6f0053":"markdown","59cd5447":"markdown","68e3060d":"markdown","1dafb09a":"markdown","d2ad1ed9":"markdown","6e40fb41":"markdown","e2987713":"markdown","1e1c64c0":"markdown","4964db2b":"markdown","648161cd":"markdown","047b6c02":"markdown","1a8b2e1b":"markdown","99c7924f":"markdown"},"source":{"edfec7d2":"import pandas as pd\r\nimport numpy as np\r\nimport seaborn as sns\r\nimport missingno as msno\r\nimport cufflinks as cf\r\nimport matplotlib.pyplot as plt\r\nimport plotly.graph_objects as go\r\nimport plotly.express as px\r\nfrom plotly.subplots import make_subplots\r\n\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\r\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \r\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss","fe50766d":"data = pd.read_csv('heart.csv')\r\n\r\nprint('Columns: ',data.shape[0],'\\n','Rows: ',data.shape[1])\r\nprint('\\n')\r\nprint('Column names:', data.columns.values)\r\nprint('\\n')\r\nprint(data.dtypes)\r\n\r\n\r\n\r\nprint('\\n')\r\n\r\n#plot the missing data to visualize more easily \r\nprint(data.isna().sum())\r\ncolor= ['dimgrey','dimgrey','red','dimgrey','maroon','maroon','dimgrey','dimgrey','dimgrey','maroon','dimgrey','dimgrey','maroon','dimgrey','red','dimgrey']\r\nmsno.bar(data,fontsize=10,color=color,figsize=(10,5))\r\nplt.title('MISSING VALUES',fontsize=20)\r\nplt.show()","a38f72ef":"#Drop education\r\ndata.drop(columns=['education'],inplace=True)\r\n\r\n#fill NaN's \r\ndata.fillna(data.mean(),inplace=True) \r\n\r\nprint(data.isna().sum())","a49cb55a":"data.describe()","0e7a2481":"columns=list(data.columns.values)\r\n\r\nfig, axes = plt.subplots(2, 4, figsize=(18, 10))\r\nfig.suptitle('Health Stats by Continuous Attribute',fontsize=23)\r\n\r\nsns.boxplot(ax=axes[0, 0], data=data ,y='age',palette='magma')\r\naxes[0,0].set_ylabel('')\r\naxes[0,0].set_xlabel('Age',fontsize=12)\r\n\r\nsns.boxplot(ax=axes[0, 1], data=data ,y='cigsPerDay')\r\naxes[0,1].set_ylabel('')\r\naxes[0,1].set_xlabel('Cigarretes Per Day',fontsize=12)\r\n\r\nsns.boxplot(ax=axes[0, 2], data=data ,y='totChol',palette='magma')\r\naxes[0,2].set_ylabel('')\r\naxes[0,2].set_xlabel('Total Cholesterol',fontsize=12)\r\n\r\nsns.boxplot(ax=axes[0, 3], data=data ,y='sysBP')\r\naxes[0,3].set_ylabel('')\r\naxes[0,3].set_xlabel('Systolic Blood Pressure',fontsize=12)\r\n\r\nsns.boxplot(ax=axes[1, 0], data=data ,y='diaBP')\r\naxes[1,0].set_ylabel('')\r\naxes[1,0].set_xlabel('Diastolic Blood Pressure',fontsize=12)\r\n\r\nsns.boxplot(ax=axes[1, 1], data=data ,y='BMI',palette='magma')\r\naxes[1,1].set_ylabel('')\r\naxes[1,1].set_xlabel('Body Mass Index',fontsize=12)\r\n\r\nsns.boxplot(ax=axes[1, 2], data=data ,y='heartRate')\r\naxes[1,2].set_ylabel('')\r\naxes[1,2].set_xlabel('Heart Rate',fontsize=12)\r\n\r\nsns.boxplot(ax=axes[1, 3], data=data ,y='glucose',palette='magma')\r\naxes[1,3].set_ylabel('')\r\naxes[1,3].set_xlabel('Glucose Level',fontsize=12)\r\nplt.plot()","26df2b17":"male=data.male.value_counts()\r\nsmoker=data.currentSmoker.value_counts()\r\ndia=data.diabetes.value_counts()\r\nprevahyp=data.prevalentHyp.value_counts()\r\npreva=data.prevalentStroke.value_counts()\r\n\r\nfig = make_subplots(rows=2, cols=3,specs=[[{\"type\": \"domain\"},{\"type\": \"domain\"},{\"type\": \"domain\"}],[{\"type\": \"domain\"},{\"type\": \"domain\"},{\"type\": \"domain\"}]],subplot_titles=('Male', 'Current Smoker','Diabetes', 'Prevalent Hypertension','Prevalent Stroke'),row_titles=['                                  Blue: No      Red:Yes'])\r\n\r\nfig.add_trace(go.Pie(values=male.values,labels=male.index),row=1, col=1)\r\nfig.add_trace(go.Pie(values=smoker.values,labels=smoker.index),row=1, col=2)\r\nfig.add_trace(go.Pie(values=dia.values,labels=dia.index),row=1, col=3)\r\nfig.add_trace(go.Pie(values=prevahyp.values,labels=prevahyp.index),row=2, col=1)\r\nfig.add_trace(go.Pie(values=preva.values,labels=preva.index),row=2, col=2)\r\n\r\nfig.update(layout_title_text='Health Stats By Nominal Attitude',\r\n           layout_showlegend=False)\r\nfig.update_traces(hole=.35,textposition='inside')\r\n\r\nfig.show('svg')\r\n","86fc8148":"sns.set_style(\"white\")\r\n\r\nchd=sns.countplot(data=data,x='TenYearCHD',palette='dark')\r\nplt.title('10 Year Risk of Coronary Heart Disease')\r\nchd.set(ylabel='Count', xlabel=None, xticklabels=['No','Yes'])\r\nsns.despine(left=True, bottom=True)\r\nplt.show()","605691fd":"chd1=data.TenYearCHD.value_counts()\r\nchd11=px.pie(values=chd1.values,names=chd1.index,title='10 Year Risk of Coronary Heart Disease',hole=0.45)\r\nchd11.update_layout(annotations=[dict(text='Blue: No  Red:Yes', x=0.494, y=1.1, font_size=17, showarrow=False)],                                    showlegend=False)\r\nchd11.show('svg')","78cfe730":"con_cols=['TenYearCHD','age','cigsPerDay','sysBP','diaBP','BMI','heartRate','glucose']\r\ncormatrix=data[con_cols].corr().transpose()\r\nprint(cormatrix)","df2c654d":"sns.set(rc={'figure.figsize':(11,11)})\r\nsns.heatmap(cormatrix, annot=True,linewidths=0,cmap='coolwarm')\r\nplt.title('Matrix Correlation Heatmap',fontsize=25)\r\nplt.show()","fa47fa61":"sns.set_style('white')\r\n\r\nsns.jointplot(x=\"diaBP\", y=\"sysBP\",kind='hex',data=data)\r\nplt.xlabel('Diastolic Blood Pressure')\r\nplt.ylabel('Systolic Blood Pressure')\r\nsns.despine(left=True, bottom=True)\r\nplt.show()","53e0a4c3":"sns.scatterplot(x='diaBP',y='sysBP',data=data,hue='TenYearCHD')\r\nplt.xlabel('Diastolic Blood Pressure')\r\nplt.ylabel('Systolic Blood Pressure')\r\nsns.despine(left=True, bottom=True)\r\nplt.show()","6f7d1c76":"tych= [['age', 0.23], ['cigsPerDay', 0.058], ['sysBP', 0.22],['diaBP',0.15],['BMI',0.075],['heartRate',0.023],['glucose',0.12]]\r\ntychd=pd.DataFrame(tych,columns=['Feature','Value'])\r\n\r\nsns.set(rc={'figure.figsize':(8,5.5)})\r\nsns.set_style(\"white\")\r\nplotty=sns.barplot(x='Feature',y='Value',data=tychd)\r\nplt.ylim(0,1)\r\nplt.title('Features vs Target',fontsize=19)\r\nsns.despine(left=True, bottom=True)\r\nplt.show()","d758fae9":"X=data.drop(['TenYearCHD'],axis=1)\r\ny=data['TenYearCHD']\r\n","2baa6c5d":"#Pareto principle\r\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\r\n\r\npipeline=Pipeline(steps=[('scaler', StandardScaler()),('logreg',LogisticRegression())])\r\nlogreg_scaled=pipeline.fit(X_train,y_train)\r\n\r\ny_pred=logreg_scaled.predict(X_test)\r\ny_pred_proba = logreg_scaled.predict_proba(X_test)[:, 1]\r\nfpr, tpr, thr = roc_curve(y_test, y_pred_proba)\r\nprint('Train Test split results:')\r\nprint('LogisticRegression',\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\r\nprint('LogisticRegression',\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\r\nprint('LogisticRegression',\" auc is %2.3f\" % auc(fpr, tpr))\r\n\r\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\r\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \r\n      \"and a specificity of %.3f\" % (1-fpr[idx]) )\r\n\r\nplt.plot([0,1],[0,1],'k--')\r\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\r\nplt.xlabel('False Positive Rate')\r\nplt.ylabel('True Positive Rate (Recall)')\r\nplt.title('ROC Curve LogReg',fontsize=20)\r\nplt.legend(loc=\"lower right\")\r\nplt.show()\r\n","9ba904e1":"cf_matrix1 = confusion_matrix(y_test, y_pred)\r\n\r\ngroup_names = ['True Pos','False Neg','False Pos','True Neg']\r\ngroup_counts = ['{0:0.0f}'.format(value) for value in\r\n                cf_matrix1.flatten()]\r\ngroup_percentages = ['{0:.2%}'.format(value) for value in\r\n                     cf_matrix1.flatten()\/np.sum(cf_matrix1)]\r\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\r\n          zip(group_names,group_counts,group_percentages)]\r\nlabels = np.asarray(labels).reshape(2,2)\r\nsns.heatmap(cf_matrix1, annot=labels, fmt='', cmap='Oranges')\r\nplt.show()","01f6f89c":"logreg = LogisticRegression(max_iter=2000)\r\n# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data\r\n#10 folds\r\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\r\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\r\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\r\nprint('K-fold cross-validation results:')\r\nprint('LogisticRegression'+\" average accuracy is %2.3f\" % scores_accuracy.mean())\r\nprint('LogisticRegression'+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\r\nprint('LogisticRegression'+\" average auc is %2.3f\" % scores_auc.mean())","3904c46e":"C = np.arange(1e-05, 5.5, 0.1)\r\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\r\n\r\nrskfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\r\n\r\nsteps=[('scale',StandardScaler(with_mean=False,with_std=False)),('clf',LogisticRegression(max_iter=3000))]\r\nlog_pipe = Pipeline(steps=steps)\r\n\r\nlog_clf = GridSearchCV(estimator=log_pipe, cv=rskfold,\r\n              scoring=scoring, return_train_score=True,\r\n              param_grid=dict(clf__C=C), refit='Accuracy')\r\n\r\nlog_clf.fit(X_train, y_train)\r\nresults = log_clf.cv_results_\r\n\r\n\r\nprint(\"best params: \" + str(log_clf.best_estimator_))\r\nprint(\"best params: \" + str(log_clf.best_params_))\r\nprint('best score:', log_clf.best_score_)\r\n","b7bf78ae":"print(classification_report(y_test,y_pred))","5c98a102":"X_train1,X_test1,y_train1,y_test1 = train_test_split(X,y,test_size=0.2,random_state=42, stratify=y)\r\n\r\ntest_scores = []\r\n\r\nfor i in range(1,15):\r\n    knn = KNeighborsClassifier(i)\r\n    knn.fit(X_train,y_train)\r\n    \r\n    test_scores.append(knn.score(X_test,y_test))\r\n\r\nmax_test_score = max(test_scores) #busca el score maximo\r\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score] #busca el mejor k\r\nprint('Max test score {}% and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))\r\nprint('As you can see in the next plot')\r\n\r\nsns.lineplot(x=range(1,15),y=test_scores)\r\nplt.plot([10,10],[0,max_test_score],'--',color='red')\r\nplt.plot([max_test_score,10],[max_test_score,max_test_score],'--',color='red')\r\nplt.ylim([0.75,1])\r\nplt.xlabel('Number of neighbors')\r\nplt.ylabel('Test score')\r\nsns.despine(left=True,bottom=True)\r\nplt.show()","e78cc23b":"steps1=[('scaler',StandardScaler()),('knn',KNeighborsClassifier(n_neighbors=10))]\r\nknn_scaled=Pipeline(steps1)\r\nknn_scaled.fit(X_train1,y_train1)\r\n\r\ny_pred1=knn_scaled.predict(X_test1)\r\ny_pred_proba1 = knn_scaled.predict_proba(X_test1)[:, 1]\r\nfpr1, tpr1, thr1 = roc_curve(y_test1, y_pred_proba1)\r\nprint('Train Test split results:')\r\nprint('KNeighborsClassifier',\" accuracy is %2.3f\" % accuracy_score(y_test1, y_pred1))\r\nprint('KNeighborsClassifier',\" log_loss is %2.3f\" % log_loss(y_test1, y_pred_proba1))\r\nprint('KNeighborsClassifier',\" auc is %2.3f\" % auc(fpr1, tpr1))\r\n\r\nidx1 = np.min(np.where(tpr1 > 0.95)) # index of the first threshold for which the sensibility > 0.95\r\n\r\nplt.plot([0,1],[0,1],'k--')\r\nplt.plot(fpr1, tpr1, color='blue', label='ROC curve (area = %0.3f)' % auc(fpr1, tpr1))\r\nplt.xlabel('False Positive Rate')\r\nplt.ylabel('True Positive Rate (Recall)')\r\nplt.title('ROC Curve KNN',fontsize=20)\r\nplt.legend(loc=\"lower right\")\r\nplt.show()\r\n","f02f7a81":"cf_matrix_knn = confusion_matrix(y_test1, y_pred1)\r\n\r\ngroup_counts1 = ['{0:0.0f}'.format(value) for value in\r\n                cf_matrix_knn.flatten()]\r\ngroup_percentages1 = ['{0:.2%}'.format(value) for value in\r\n                     cf_matrix_knn.flatten()\/np.sum(cf_matrix_knn)]\r\nlabels1 = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\r\n          zip(group_names,group_counts1,group_percentages1)]\r\nlabels1 = np.asarray(labels1).reshape(2,2)\r\nsns.heatmap(cf_matrix_knn, annot=labels1, fmt='', cmap='Blues')\r\nplt.show()","32667f1e":"print(classification_report(y_test1,y_pred1))","40a0bc90":"param_grid = {'knn__n_neighbors':np.arange(1,50)}\r\n\r\nknn_cv= GridSearchCV(knn_scaled,param_grid,cv=5)\r\nknn_cv.fit(X,y)\r\n\r\nprint(\"best params: \" + str(knn_cv.best_estimator_))\r\nprint(\"best params: \" + str(knn_cv.best_params_))\r\nprint('best score:', knn_cv.best_score_)","d41b458c":"metrics=['Accuracy','Log Loss','AUC Score','Precision','Recall','F1 Score']\r\nLogRegval=[0.86,0.38,0.72,0.82,0.86,0.81]\r\nknnval=[0.85,1.34,0.63,0.78,0.84,0.78]\r\nresultss = {'Metrics':metrics,'LogReg':LogRegval, 'KNN': knnval}\r\nresultsss= pd.DataFrame(resultss)\r\nprint(resultsss)\r\n\r\nfig, ax = plt.subplots()\r\nax.bar(metrics, knnval, label='knn',color='cornflowerblue')\r\nax.bar(metrics, LogRegval,  bottom=knnval, label='logreg',color='coral')\r\nax.set_ylabel('')\r\nax.set_title('Scores by metrics',fontsize=20)\r\nax.legend()\r\nax.spines['right'].set_visible(False)\r\nax.spines['top'].set_visible(False)\r\nax.spines['left'].set_visible(False)\r\nax.spines['bottom'].set_visible(False)\r\nplt.show()\r\n","20e75e06":"#one ROC curve over another\r\nplt.plot([0,1],[0,1],'k--')\r\nplt.plot(fpr1, tpr1, color='blue', label='KNN ROC curve (area = %0.3f)' % auc(fpr1, tpr1))\r\nplt.plot(fpr, tpr, color='coral', label='LogReg ROC curve (area = %0.3f)' % auc(fpr, tpr))\r\nplt.xlabel('False Positive Rate')\r\nplt.ylabel('True Positive Rate (Recall)')\r\nplt.title('ROC Curve LogReg vs KNN',fontsize=20)\r\nplt.legend(loc=\"lower right\")\r\nplt.show()","01f357ce":"### 5)Logistic Regression and Results\r\nModel evaluation with Train Test Split ","dbf0a01d":"As we can see this dataset has 4238 rows and 16 columns. All the columns types are correct because we're only working with numbers in all of them.\n\nThe education, cigs per day, BPMeds, totChol, BMI, heartRate and glucose columns have missing values. In the next step we're going to fix this problem by removing the education because isn't relevant and replacing the NaN's with the mean of each column \n\n","a82f1671":"Most of the poeple dont have a 10 year risk from suffer a coronary heart disease (84,8%) ","00217f01":"Age is the most correlated feature with the target but its value isnt significally enough.","73d3aacb":"Analysis\r\n\r\nfor the accuracy we have that the models have almost the same values with a slight difference of 0.01 or 1% with Logistic Regression having the highest value (0.86)\r\n\r\nfor logloss we have the knowledge that the models with lower log-loss score is better than the ones with higher log-loss score by this logic we have a that the Logistic Regression (0.38) is much better than the KNeighbors(1.34).\r\n\r\nfor the AUC Score it is easier to visualize one ROC curve over the other but in advance the Logistic Regression is better \r\n\r\nPrecision is the metric where we measure the quality of a model, therefore, the higher the better. In this analysis we can say that the Logistic Regresion has a small lead of 0.04 over the Kneighbors\r\n\r\nRecall is the metric where we have the ratio of correctly predicted positive observations to the all observations. we can say that the model is good when it has a value higher than 0.5 wich they both do. Again Logistic Regression has better performance.\r\n\r\nAnd the last metric is the F1 Score and as we have a binary classification the higher the better. Logistic Regression has a higher value therefore has a better performance.","572ca805":"### 2) Load de data and study the quality","0277a98a":"# Heart Disease Prediction \n\n## In this project we want to predict if a human may suffer a heart disease in ten years based on thousand of pacients and several attributes\n### 1) Import Packages","f64a4385":"As we can see this features dont have direct relation with our target variable\n\nNow we're going to plot the diferents features with our target with a barplot.","ddcc9397":"As we can see the majority of people in this data are females to be exact 57,1% and we have practically the same proportion of smokers and non-smokers.\nAlmost 2,6% of poeple has diabetes, wich is low, and as for prevalent diseases we have a significant proportion of people with hypertension but for the prevalence stroke is almost 0\n\nOur target variable is TenYearCHD wich indicates the 10 year risk from suffer a coronary heart disease (1: Yes and 0: No)","e452aaa8":"### Now we're going to predict with KNNeighbors \r\nfirst we need to know wich is the best k for our model","2a6f0053":"### First we compare all the metrics and then the ROC curves","59cd5447":"now, model evaluation with K-fold cross validation using cross_val_score() function","68e3060d":"Now we dont have any missing values so lets found outliers and do some basic statistic","1dafb09a":"### Conclusions","d2ad1ed9":"### 4) Bivariate Analysis\n\nAnd the last step in this EDA is the bivariate analysis and and we can observe it with a correlation matrix","6e40fb41":"GridSearchCV evaluating using multiple scorers, RepeatedStratifiedKFold and pipeline for preprocessing simultaneously","e2987713":"## Algorithm Comparison Kneighbors vs LogisticRegrssion","1e1c64c0":"The model with logistic regression has an advantage in all categories and is therefore the model with the best performance to predict if a person may suffer a coronary heart disease in ten years.","4964db2b":"As we can see the AUC for the Logistic Regression is bigger than the KNeigbors so the first model is better","648161cd":"And the last step for this algorithm is the GridSearch because we want to found the best hyper parameters","047b6c02":"Lets visualize this data with the univariate analysis\n\n### 3)Univariate Analysis\n","1a8b2e1b":"These were the continuous variables in this dataset and here are some summaries about them\n\n-Age: the only feature without outliers and most of the data are between 42 and 56.\n\n-Cigarretes per day: only a few outliers above 50 and most of the data are less than 20.\n\n-Total Cholesterol & Body Mas Index: Both features we have a large amount of variability and 2 far outliers.\n\n-Systolic Blood Pressure & Diastolic Blood Pressure & Heart Rate: These features have similiar boxplots with the \nmajority of the data oriented to lower values\n\n-Glucose Level: This boxplot has the proponderance of the data to very low values and a lot of variability \n\n\nNow we proceed to plot the nominal values where 0: No and 1: yes","99c7924f":"As we can see the only high value is the relationship beetwen diastolic blood pressure and systolic blood pressure. This can be explained by this sentence \"Blood pressure readings are given in two numbers. The top number is the maximum pressure your heart exerts while beating (systolic pressure)\" as shown below"}}