{"cell_type":{"4a6cb0ba":"code","212a9d8d":"code","739319fc":"code","c2459bca":"code","5d9ffc65":"code","3776ac6c":"code","57afd436":"code","bef225b5":"code","a600796f":"code","f0cc7c8f":"code","70ed7227":"code","69fe4b4f":"code","219612d1":"code","2d8670d1":"code","bee1cd17":"code","ce1092de":"code","5f20f43f":"code","0b932577":"code","65224fef":"code","986742c7":"code","1ed2bd87":"code","bf2c48ea":"code","b26e78e7":"code","4f5edc9b":"code","148093b5":"code","47766bec":"code","697a5a55":"code","9a1f45d3":"code","3e57a088":"code","9a0c45fc":"code","58e7daa3":"code","1bdb12fc":"markdown","ac590952":"markdown","bf46e7e2":"markdown","934d98de":"markdown","1fdcf307":"markdown","9554b8af":"markdown","1b3de794":"markdown","3dec6740":"markdown","269a17b5":"markdown","c945c693":"markdown","8916a757":"markdown","89ba97fc":"markdown","88a24d33":"markdown"},"source":{"4a6cb0ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","212a9d8d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#Text manipulation\nimport nltk\nfrom textblob import TextBlob\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom gensim import matutils, models\nimport scipy.sparse\n\n#Text visualisation \nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction import text \n\n## visualisation\n#import pyLDAvis.gensim\n#from gensim.corpora import Dictionary\n#from gensim.models.coherencemodel import CoherenceModel\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\nimport spacy\nspacy.load(\"en_core_web_sm\")\n\nnltk.download('stopwords')\n\n# deactivate deprecation warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","739319fc":"###### Function to prepare the tweets\ndef clean_text_1(text):\n    # Lowercase\n    text = text.lower()\n    # Remove special text in brackets ([chorus],[guitar],etc)\n    text = re.sub('\\[.*?\\]', '', text)\n    # Remove punctuation\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # Remove words containing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)    \n    # Remove quotes\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    # Remove new line \\n \n    text = re.sub('\\n', ' ', text)\n    # Remove stop_word\n    stop_words = stopwords.words('english')\n    words = word_tokenize(text)\n    new_text = \"\"\n    for w in words:\n        if w not in stop_words and len(w) > 1:\n            new_text = new_text + \" \" + w\n    return new_text\n\n##### Function to integrate polarity and subjectivity in the tweets\npol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\n### Function to lemmatize the text\n\ndef lemmatize_tag(text):\n    wnl = WordNetLemmatizer()\n    lemma=[]\n    for i,j in pos_tag(word_tokenize(text)) :\n        p=j[0].lower()\n        if p in ['j','n','v']:\n            if p == 'j':\n                p = 'a'\n            lemma.append(wnl.lemmatize(i,p))\n        else :\n            lemma.append(wnl.lemmatize(i))    \n    return ' '.join(lemma)\n\n\n### Function to extract nouns\ndef nouns(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n    is_noun = lambda pos: pos[:2] == 'NN'\n    tokenized = word_tokenize(text)\n    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n    return ' '.join(all_nouns)\n\n# Define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\n## lemmatization with noun, adjective, verbs, adverb\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n","c2459bca":"\ndf = pd.read_csv(\"\/kaggle\/input\/sentimental-analysis-for-tweets\/sentiment_tweets3.csv\")","5d9ffc65":"df","3776ac6c":"df.info()","57afd436":"df['polarity'] = df['message to examine'].apply(pol)\ndf['subjectivity'] = df['message to examine'].apply(sub)\ndf","bef225b5":"\nsns.histplot(df['label (depression result)'])","a600796f":"sns.pairplot(df[['polarity','subjectivity','label (depression result)']],hue='label (depression result)')","f0cc7c8f":"# Clean text\ndf_clean = pd.DataFrame(df['message to examine'].apply(clean_text_1)).copy()\ndf_clean = pd.DataFrame(df['message to examine'].apply(lemmatize_tag)).copy()","70ed7227":"# define stop words for text cleaning\nstop_words = stopwords.words('english')","69fe4b4f":"# define stop words for text cleaning\nstop_words2=[]\nfor w in stop_words:\n    stop_words2.append(w)\n\nstop_words2.extend(['http:\/\/t.',\"I'm\",'http',\"can't\",'\u00c5','\u0100','like','t','\u00e5\u0101','www','com','https'])","219612d1":"wc = WordCloud(collocations=False,stopwords=stop_words2, background_color='white', colormap='Dark2',\n               max_font_size=150, random_state=42)","2d8670d1":"text_to_analyze = ''\nfor i in df['message to examine']:\n    text_to_analyze = text_to_analyze + ' ' + i","bee1cd17":"wc = wc.generate(text_to_analyze)","ce1092de":"# Wordcloud plot\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Wordcloud for Tweets')\nplt.show()","5f20f43f":"#Extract tweets from depressed\ndf_depressed = df_clean[df['label (depression result)']==1]","0b932577":"text_to_analyze = ''\nfor i in df_depressed['message to examine']:\n    text_to_analyze = text_to_analyze + ' ' + i\nwc = WordCloud(collocations=False,stopwords=stop_words2, background_color='white', colormap='Dark2',\n               max_font_size=150, random_state=42)\nwc = wc.generate(text_to_analyze)\n# Wordcloud plot\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Wordcloud for depressed Tweets')\nplt.show()","65224fef":"#Extract tweets from depressed\ndf_nondepressed = df_clean[df['label (depression result)']==0]","986742c7":"stop_words2.extend(['get'])\ntext_to_analyze = ''\nfor i in df_nondepressed['message to examine']:\n    text_to_analyze = text_to_analyze + ' ' + i\nwc = WordCloud(collocations=False,stopwords=stop_words2, background_color='white', colormap='Dark2',\n               max_font_size=150, random_state=42)\nwc = wc.generate(text_to_analyze)\n# Wordcloud plot\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Wordcloud for non-depressed Tweets')\nplt.show()","1ed2bd87":"cv = CountVectorizer()#CountVectorizer(min_df=.2, max_df=.8,stop_words=stop_words2)","bf2c48ea":"# Put tweets in list\nalltweets = []\nfor i in df_depressed['message to examine']:\n    alltweets.append(i)","b26e78e7":"data_cv = cv.fit_transform(alltweets)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = df_depressed.index\ndata_dtm","4f5edc9b":"vectorizer = TfidfVectorizer(stop_words=stop_words2)#min_df=.4, max_df=.8,stop_words=stop_words2)\ntfidf = vectorizer.fit_transform(alltweets)\n#len(vectorizer.get_feature_names())\ndata_tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names())\ndata_tfidf.index = df_depressed.index\ndata_tfidf","148093b5":"\n# SVD represent documents and terms in vectors \nsvd_model = TruncatedSVD(n_components=5) #try with 10 topics\nsvd_model.fit(data_tfidf)\nprint(svd_model.components_.shape)\nprint(svd_model.singular_values_)","47766bec":"terms = vectorizer.get_feature_names()\n# Print out the topics\nfor i, comp in enumerate(svd_model.components_):\n    terms_comp = zip(terms, comp)\n    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n    print(\"Topic \"+str(i)+\": \")\n    for t in sorted_terms:\n        print(\"%.2f*%s \"% (t[1], t[0]) ,end='')\n    print(\"\")","697a5a55":"# Put tweets in list\nalltweets = []\nfor i in df_nondepressed['message to examine']:\n    alltweets.append(i)","9a1f45d3":"data_cv = cv.fit_transform(alltweets)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = df_nondepressed.index","3e57a088":"vectorizer = TfidfVectorizer(stop_words=stop_words2)#min_df=.4, max_df=.8,stop_words=stop_words2)\ntfidf = vectorizer.fit_transform(alltweets)\n#len(vectorizer.get_feature_names())\ndata_tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names())\ndata_tfidf.index = df_nondepressed.index","9a0c45fc":"\n# SVD represent documents and terms in vectors \nsvd_model = TruncatedSVD(n_components=5) #try with 10 topics\nsvd_model.fit(data_tfidf)\nprint(svd_model.components_.shape)\nprint(svd_model.singular_values_)","58e7daa3":"terms = vectorizer.get_feature_names()\n# Print out the topics\nfor i, comp in enumerate(svd_model.components_):\n    terms_comp = zip(terms, comp)\n    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n    print(\"Topic \"+str(i)+\": \")\n    for t in sorted_terms:\n        print(\"%.2f*%s \"% (t[1], t[0]) ,end='')\n    print(\"\")","1bdb12fc":"### Import dataset <a id='import_dataset'><\/a>\n\n[Back to Index](#index)","ac590952":"### Word Cloud for depressed <a id='word_cloud_dep'><\/a>\n\nThe words 'depression' and 'axiety' are appearing as more relevant\n\n[Back to Index](#index)","bf46e7e2":"### Topic modelling on non-depressed <a id='lsa_nondepr'><\/a>\n\nfirst topic: 'good', 'day,'love', 'today'\n\n[Back to Index](#index)","934d98de":"## Index \n<a id='index'><\/a>\n[Import libraries](#import_libraries) <br>\n[Import functions](#import_functions) <br>\n[Import dataset](#import_dataset) <br>\n[Add polarity and subjectivity](#add_polarity_subj) <br>\n[Look at pair plots](#pair_plot) <br>\n[Word Cloud](#word_cloud) <br>\n[Word Cloud for depressed](#word_cloud_dep) <br>\n[Word Cloud for non-depressed](#word_cloud_non-dep) <br>\n[Topic modelling on depressed](#lsa_depr) <br>\n[Topic modelling on non-depressed](#lsa_nondepr) <br>","1fdcf307":"Unbalanced data. Less depressed","9554b8af":"### Pair plots <a id='pair_plot'><\/a>\n\nPolarity is not separating depression.\n\n[Back to Index](#index)","1b3de794":"### Import libraries <a id='import_libraries'><\/a>\n\n[Back to Index](#index)","3dec6740":"'depression' is appearing at biggest","269a17b5":"### Import functions for tweets <a id='import_functions'><\/a>\n\n[Back to Index](#index)","c945c693":"### Word Cloud <a id='word_cloud'><\/a>\n\n\n[Back to Index](#index)","8916a757":"### Add columns for polarity and subjectivity<a id='add_polarity_subj'><\/a>\n\n[Back to Index](#index)","89ba97fc":"### Word Cloud for non-depressed <a id='word_cloud_non_dep'><\/a>\n\nWords like 'good', 'thank' and 'love' are more relevant\n\n[Back to Index](#index)","88a24d33":"### Topic modelling on depressed <a id='lsa_depr'><\/a>\n\nfirst topic: 'depression', 'anxiety'\n\n[Back to Index](#index)"}}