{"cell_type":{"3a6d6375":"code","85610ea0":"code","1a838685":"code","77b14ead":"code","31391d8b":"code","237aee12":"code","5c3b808e":"code","7b934308":"code","41b9e7b9":"code","fae001f2":"code","587e5451":"code","4ea320a5":"code","b4c65e77":"code","082b80f7":"code","285ed753":"code","bf8e28f7":"code","fd2c82ca":"code","d3c8ff39":"code","819cc91b":"code","b1b836ac":"code","e5631503":"code","a85040f3":"code","109ca700":"code","2bbc566a":"code","161aa962":"code","07d85f3b":"code","d0a84100":"code","38f187b5":"code","0c9c5fae":"code","7eb2bb55":"code","b5374466":"code","e75c389d":"code","a65d1223":"code","9dba57ee":"code","c78da19d":"code","d06c82c5":"code","9fe36e5a":"code","153cb581":"code","23048942":"code","8afa1000":"code","39d1a929":"code","e8404a7f":"code","59429896":"code","2b4b48d8":"code","ef381d12":"code","c6ee7128":"code","2df04a3f":"code","9349d5fa":"code","ddf56f0a":"code","26c9c37e":"code","afa6e397":"code","cc8f2e0f":"markdown","80207e17":"markdown","542c18eb":"markdown","2cd46604":"markdown","6e4133d6":"markdown","6fa026f1":"markdown","fe9bdd07":"markdown","228f4422":"markdown","f9c44d70":"markdown","d3118ae9":"markdown","df577060":"markdown","34f6209e":"markdown","bc283bc1":"markdown","a0dfa532":"markdown","00ad83e5":"markdown","050b196d":"markdown","38ac2a1e":"markdown","1d324d21":"markdown","4ef32460":"markdown","a1c7548f":"markdown","3f11a8e4":"markdown","e5d9fb05":"markdown","d19e40e0":"markdown","3c013a88":"markdown","65c74b4b":"markdown","ea22ed53":"markdown","64e6fa57":"markdown","a0f7fd89":"markdown","c1f3b16a":"markdown","483060a8":"markdown","5359651a":"markdown","2c8c4698":"markdown","319dcdab":"markdown","a5350698":"markdown","c6d87c3f":"markdown","f25e4d8b":"markdown","b4b7244c":"markdown","add07af4":"markdown","c43f2649":"markdown","6547fc12":"markdown","8d3bc304":"markdown","35b96cd7":"markdown","60139ab7":"markdown","32f5ff9f":"markdown","ca59e408":"markdown","e72598f9":"markdown","49e38abe":"markdown","644f0225":"markdown","425d7740":"markdown","9a12012c":"markdown","22b90773":"markdown","c7bbbcfa":"markdown","68932d74":"markdown","41112b98":"markdown","5f7db190":"markdown","c978c6a6":"markdown","348e5918":"markdown","fff062dc":"markdown","db5e3197":"markdown","961bb569":"markdown","a06e11a6":"markdown","16363967":"markdown","577c74e4":"markdown","70c3da55":"markdown","054ccede":"markdown","f8479094":"markdown","93d53d4b":"markdown","8f83fe3a":"markdown","dac08760":"markdown"},"source":{"3a6d6375":"import numpy as np\nimport pandas as pd\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport sklearn\n%matplotlib inline","85610ea0":"apartments = pd.read_csv(\"..\/input\/milan-airbnb-open-data-only-entire-apartments\/Airbnb_Milan.csv\")","1a838685":"apartments.info()","77b14ead":"apartments.head()","31391d8b":"apartments.head()","237aee12":"apartments.drop(apartments.columns[[0, 1, 2]],axis=1,inplace=True)\napartments.drop(columns=[\"zipcode\"], inplace=True)","5c3b808e":"weekly_price = apartments.cleaning_fee + apartments.daily_price * 7\napartments[\"weekly_price\"] = weekly_price.values\napartments.drop(columns=[\"cleaning_fee\", \"daily_price\"], inplace=True)\n","7b934308":"apartments.room_type.value_counts()","41b9e7b9":"apartments.drop(columns=[\"room_type\"], inplace=True)","fae001f2":"import urllib\n\ncmap = cm.jet\nm = cm.ScalarMappable(cmap=cmap)\nquartiere_colors = m.to_rgba(apartments.neighbourhood_cleansed)\nquant_minimum = apartments.weekly_price.quantile(0.1)\nquant_maximum = apartments.weekly_price.quantile(0.9)\nprice = ((apartments.weekly_price - quant_minimum) \/ (quant_maximum - quant_minimum)) * 30\n\n#initializing the figure size\nplt.figure(figsize=(20,20))\n#loading the png milan image found on open street map and saving to my local folder along with the project\ni=urllib.request.urlopen('https:\/\/i.ibb.co\/s1Jf5k7\/map-2.png')\nmil_img=plt.imread(i)\n#scaling the image based on the latitude and longitude max and mins for proper output\nplt.imshow(mil_img, zorder=0, extent=[\n                                      apartments.longitude.min(), \n                                      apartments.longitude.max(), \n                                      apartments.latitude.min(), \n                                      apartments.latitude.max(),\n                                    ]\n           )\n\nax=plt.gca()\n# plot the points\napartments.plot(kind='scatter', x='longitude', y='latitude', label='price', c=quartiere_colors, s=price, ax=ax, zorder=5, edgecolors='black')\n\npatch = []\nfor a in range(1, 9):\n  patch.append(mpatches.Patch(color=m.to_rgba(a), label='Neighborhood ' + str(a)))\n\nplt.legend(handles=patch)\n\nplt.show()","587e5451":"apartments.neighbourhood_cleansed.value_counts()","4ea320a5":"def draw_plot(apartments):\n    cmap = cm.jet\n    m = cm.ScalarMappable(cmap=cmap)\n    quartiere_colors = m.to_rgba(apartments.neighbourhood_cleansed)\n    quant_minimum = apartments.weekly_price.quantile(0.1)\n    quant_maximum = apartments.weekly_price.quantile(0.9)\n    price = ((apartments.weekly_price - quant_minimum) \/ (quant_maximum - quant_minimum)) * 30\n\n    #initializing the figure size\n    plt.figure(figsize=(20,20))\n    #loading the png milan image found on open street map and saving to my local folder along with the project\n    i=urllib.request.urlopen('https:\/\/i.ibb.co\/s1Jf5k7\/map-2.png')\n    mil_img=plt.imread(i)\n    #scaling the image based on the latitude and longitude max and mins for proper output\n    plt.imshow(mil_img, zorder=0, extent=[\n                                          apartments.longitude.min(), \n                                          apartments.longitude.max(), \n                                          apartments.latitude.min(), \n                                          apartments.latitude.max(),\n                                        ]\n              )\n\n    ax=plt.gca()\n    # plot the points\n    apartments.plot(kind='scatter', x='longitude', y='latitude', label='price', c=quartiere_colors, s=price, ax=ax, zorder=5, edgecolors='black')\n    patch = []\n    for a in range(1, 9):\n      patch.append(mpatches.Patch(color=m.to_rgba(a), label='Neighborhood ' + str(a)))\n\n    plt.legend(handles=patch)\n    plt.show()","b4c65e77":"draw_plot(apartments[apartments.weekly_price > 800])","082b80f7":"draw_plot(apartments[apartments.weekly_price < 350])","285ed753":"apartments.number_of_reviews.describe()","bf8e28f7":"draw_plot(apartments[apartments.number_of_reviews > 44])","fd2c82ca":"draw_plot(apartments[apartments.number_of_reviews <= 4])","d3c8ff39":"draw_plot(apartments[apartments.bedrooms > 2])","819cc91b":"draw_plot(apartments[apartments.bedrooms == 1])","b1b836ac":"apartments.boxplot(by=\"neighbourhood_cleansed\", column=\"weekly_price\", figsize=(10,10), showmeans=True)","e5631503":"# taken from https:\/\/stackoverflow.com\/questions\/29432629\/plot-correlation-matrix-using-pandas\nf = plt.figure(figsize=(25, 20))\nplt.matshow(apartments.corr(method='pearson'), fignum=f.number)\nplt.xticks(range(apartments.shape[1]), apartments.columns, fontsize=14, rotation=90)\nplt.yticks(range(apartments.shape[1]), apartments.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","a85040f3":"from geopy.distance import great_circle\n\ndef distance_to_mid(lat, lon):\n    milan_centre = (45.464664, 9.188540)\n    accommodation = (lat, lon)\n    return great_circle(milan_centre, accommodation).km\n\napartments[\"dist_from_center\"] = apartments.apply(lambda x: distance_to_mid(x.latitude, x.longitude), axis=1)","109ca700":"from sklearn.feature_selection import VarianceThreshold\n\ndef remove_almost_constant_columns(threshold=0):\n    qconstant_filter = VarianceThreshold(threshold=threshold)\n    qconstant_filter.fit(apartments)\n    constant_columns = [column for column in apartments.columns\n                        if column not in apartments.columns[qconstant_filter.get_support()]]\n    print(constant_columns)\n    apartments.drop(labels=constant_columns, axis=1, inplace=True)\n\nremove_almost_constant_columns(threshold=0.03) # we remove data that is 97% of the time the same","2bbc566a":"import seaborn as sns\nfrom scipy.stats import norm\n\ndef plot_price_distribution(prices):\n  plt.figure(figsize=(10,10))\n  sns.distplot(prices, fit=norm)\n  plt.title(\"Price Distribution Plot\", size=15, weight='bold')\n\nplot_price_distribution(apartments.weekly_price)","161aa962":"log_weekly_price = np.log2(apartments.weekly_price)\nplot_price_distribution(log_weekly_price)","07d85f3b":"apartments[\"log_weekly_price\"] = log_weekly_price.values","d0a84100":"from sklearn.model_selection import train_test_split\n\nX = apartments.drop(columns=[\"log_weekly_price\", \"weekly_price\"])\ny = apartments.log_weekly_price\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","38f187b5":"from sklearn.model_selection import KFold\nkf = KFold(5, shuffle=True, random_state=42)","0c9c5fae":"from sklearn.model_selection import cross_validate\n\ndef train_and_validate(model, X, y, cv):\n    cv_result = cross_validate(model, X, y, cv=kf, return_train_score=True)\n    return pd.DataFrame(cv_result)","7eb2bb55":"from statsmodels.stats.proportion import proportion_confint\n\ndef confidence_interval(n_elements, R2_score, confidence):    \n    return proportion_confint(n_elements * R2_score, n_elements, 1-confidence\/100, method='wilson')\n\ndef print_confidence_interval(n_elements, R2_score):\n    lower, upper = confidence_interval(n_elements, R2_score, 95)\n    print(f\"Interval of confidence: {lower:.3f}, {upper:.3f}\")\n","b5374466":"from sklearn.linear_model import LinearRegression\n\nresult = train_and_validate(LinearRegression(), X, y, kf)\nprint(result)","e75c389d":"print(train_and_validate(LinearRegression(), X, apartments.weekly_price, kf))","a65d1223":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nmodel = Pipeline([\n    (\"scale\",  StandardScaler()),   # <- aggiunto\n    (\"linreg\", LinearRegression())\n])\nprint(train_and_validate(model, X, y, kf))\n","9dba57ee":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = Pipeline([\n    (\"scale\",  StandardScaler()),   # <- aggiunto\n    (\"regr\", ElasticNet())\n])\n\ngrid = {\n    \"regr__l1_ratio\": np.linspace(0, 1, 10),      # <- grado polinomio\n    \"regr__alpha\":  [0.1, 1, 10] # <- regolarizzazione\n}\ngs = GridSearchCV(model, grid, cv=kf)\ngs.fit(X_train, y_train);\n\ndisplay(pd.DataFrame(gs.cv_results_).sort_values(\"mean_test_score\", ascending=False))\nprint_confidence_interval(len(X_test), gs.score(X_test, y_test))\n","c78da19d":"from sklearn.kernel_ridge import KernelRidge\n# best param alpha = 50, coef0=4, degree = 3\nmodel = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"regr\",  KernelRidge(alpha=20, kernel=\"poly\", degree=3, coef0=2))\n])\ngrid = {\n    \"regr__alpha\":  np.linspace(50, 200, 3), # <- regolarizzazione\n    \"regr__coef0\": [4,5,6,7,3],\n}\ngs = GridSearchCV(model, grid, cv=kf)\ngs.fit(X_train, y_train);\ndisplay(pd.DataFrame(gs.cv_results_).sort_values(\"mean_test_score\", ascending=False))\nprint_confidence_interval(len(X_test), gs.score(X_test, y_test))","d06c82c5":"red_square = dict(markerfacecolor='r', markeredgecolor='r', marker='.')\napartments.log_weekly_price.plot(kind='box', xlim=(6, 15), vert=False, flierprops=red_square, figsize=(20,2));","9fe36e5a":"apartments.drop(apartments[(apartments.log_weekly_price > 13) | (apartments.log_weekly_price < 7) ].index, axis=0, inplace=True)\napartments.log_weekly_price.plot(kind='box', xlim=(7, 13), vert=False, flierprops=red_square, figsize=(20,2));","153cb581":"X = apartments.drop(columns=[\"log_weekly_price\", \"weekly_price\"])\ny = apartments.log_weekly_price\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","23048942":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = RandomForestRegressor()\n\ngrid = { \n            \"n_estimators\"      : [10,20,30,50,100],\n            \"max_features\"      : [\"auto\", \"sqrt\", \"log2\"],\n            \"min_samples_split\" : [2,4,8],\n            \"bootstrap\": [True, False],\n            \"max_depth\": [1,20,100, None],\n            \"min_samples_leaf\": [1, 5, 10]\n}\ngs = GridSearchCV(model, grid, cv=kf)\ngs.fit(X_train, y_train);\ndisplay(pd.DataFrame(gs.cv_results_).sort_values(\"mean_test_score\", ascending=False))\nprint_confidence_interval(len(X_test), gs.score(X_test, y_test))","8afa1000":"from sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\n\nbooster = xgb.XGBRegressor()\n# create Grid\nparam_grid = {'n_estimators': [100, 150, 200],\n              'learning_rate': [0.01, 0.05, 0.1], \n              'max_depth': [3, 4, 5, 6, 7],\n              'colsample_bytree': [0.6, 0.7, 1],\n              'gamma': [0.0, 0.1, 0.2],\n              'alpha': [0.0, 0.5, 1, 2]}\n\n# instantiate the tuned random forest\nbooster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)\n\n# train the tuned random forest\nbooster_grid_search.fit(X_train, y_train)\n\n# print best estimator parameters found during the grid search\nprint(booster_grid_search.best_params_)","39d1a929":"from sklearn.metrics import mean_squared_error, r2_score\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test  = sc.transform(X_test)\n\ndata_dmatrix = xgb.DMatrix(data=X,label=y)\nbooster = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.6, learning_rate = 0.05,\n                max_depth = 6, gamma = 0, alpha=1, n_estimators = 300)\n\nbooster.fit(X_train,y_train)\n\npreds = booster.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\nprint(f\"R2 score: {r2_score(y_test, preds)}\")\nprint_confidence_interval(len(X_test), r2_score(y_test, preds))\n\nlin = LinearRegression()\nlin.fit(X_train, y_train)\npreds = lin.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\nprint(f\"R2 score: {r2_score(y_test, preds)}\")\nprint_confidence_interval(len(X_test), r2_score(y_test, preds))\n\n","e8404a7f":"xg_train = xgb.DMatrix(data=X_train, label=y_train)\nparams = {'colsample_bytree': 0.6, 'gamma': 0, 'alpha': 1,  'learning_rate': 0.05, 'max_depth': 6}\n\ncv_results = xgb.cv(dtrain=xg_train, params=params, nfold=4,\n                    num_boost_round=400, early_stopping_rounds=10, \n                    metrics=\"rmse\", as_pandas=True)","59429896":"cv_results.tail()","2b4b48d8":"train_and_validate(booster, X, y, kf)","ef381d12":"# we put back all the outliers by re-executing code above\ntrain_and_validate(booster, X, y, kf)","c6ee7128":"X_no_airbnb_data = X.drop(columns=[\n                                   \"number_of_reviews\", \n                                   \"review_scores_rating\", \n                                   \"review_scores_accuracy\", \n                                   \"review_scores_cleanliness\", \n                                   \"review_scores_checkin\", \n                                   \"review_scores_communication\", \n                                   \"review_scores_location\", \n                                   \"review_scores_value\",\n                                  ])\ntrain_and_validate(booster, X_no_airbnb_data, y, kf)","2df04a3f":"y_train.plot.hist(bins=40, figsize=(12, 4));","9349d5fa":"np.random.seed(42)\nrandom_preds = np.random.normal(\n    y_train.mean(),   # centro (media)\n    y_train.std(),    # scala (dev. standard)\n    len(y)        # numero di campioni\n)\nplt.figure(figsize=(12, 4))\nplt.hist(random_preds, bins=40);","ddf56f0a":"scores = []\nfor i in range(1, 1000):\n  np.random.seed(i)\n  random_preds = np.random.normal(\n      y_train.mean(),   # centro (media)\n      y_train.std(),    # scala (dev. standard)\n      len(y)        # numero di campioni\n  )\n  scores.append(r2_score(y, random_preds))\n\nnp.mean(scores)","26c9c37e":"import xgboost\nimport shap\n# download shap with pip3 install https:\/\/github.com\/slundberg\/shap\/archive\/master.zip\n# load JS visualization code to notebook\nshap.initjs()\n\n# explain the model's predictions using SHAP\n# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\nexplainer = shap.TreeExplainer(booster)\nshap_values = explainer.shap_values(X)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])","afa6e397":"shap.summary_plot(shap_values, X)","cc8f2e0f":"### Random Forest Regressor","80207e17":"This matrix is super interesting. I want to highlight some things that are worth be mentioned.\n\nRegarding the weekly price:\n*   It sees that **the weekly price is strongly correlated with the number of  bed, bedrooms and how many people it can accomodate**, which I didn't notice in the map over Milan. It is however, expected\n*   It also seems that **there is a correlation between the number of guest and extra people that are inside the house**. Since this is a positive correlation, that wasn't quite expected, as usually having more guests in the house should lower the price of it\n*   There also seem to be a **negative correlation with the number of reviews**, meaning that the less they are the higher is the price. That does make sense since people don't like to spend money so they will prefer a cheaper place over a more expensive one.\n*   Finally, it seems that **the more listing a host has, the higher his price is.**\n\nRegarding other intersting correlation:\n*   The host response rate and the host response time are correlated. Makes sense\n*   If there are guests included, the number of bedrooms and bed is higher. This is intersting because the latter refers only to the beds available to the client, not the total\n*   All data regarding the reviews is strongly correlated between each other. It is indeed more likely that with a positive review in something, you are also likely to have a positive review for the rest\n*   What's more, if the host is superhost he will receive a higher number of reviews with a more positive rating. This indeed means that once you are a superhost it is more likely you'll get more positive reviews. \n*   In contrast to that, a higher number of listing means that an host will receive a lower number of reviews with a more negative value. This may be because with a lot of listing it's harder to pay enough attention to all apartments.\n","542c18eb":"Much better, let's add it to the dataframe","2cd46604":"### Normalization of data\n\nLet's now try to use a StandardScaler to see if we can perform better\n\n","6e4133d6":"### A simple LinearRegression","6fa026f1":"The above explanation is about the first prediction and shows features each contributing to push the model output from the base value, `9.299`, to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue","fe9bdd07":"In this case, on average we lose a 3% in accuracy. \n\n\n","228f4422":"## Why our regressions are performing badly?\n\nThe main problem that makes our regression perform badly seems to be in the outliers. With the logarithm, we greatly improved accuracy, but maybe that's not enough. Let's create a new box plot, this time showing the log of the price","f9c44d70":"We can remove it","d3118ae9":"but we don't get a strong improvement, even for `alpha = 0.1` and `l1_ratio = 0`. What we should do in this is try to use a non-linear regression","df577060":"### XGBoost","34f6209e":"Finally, let's also define a function for the confidence interval of our predictions","bc283bc1":"### Regolarization\n\nLet's now try to swap the LinearRegression with an ElasticNet to see if the regularization L1 and L2 can improve our scores","a0dfa532":"A notorious model that performs well with outliers is the Random Forest regressor","00ad83e5":"## Comparing our best model against a random \n\nLet's see how our XGBoost model performs against a random model generated from the log_price_distribution","050b196d":"## Feature Selection\n\nBefore starting to predict the data, we want to remove the values that have very low variability and therefore they do not impact the prediction, only making it slow","38ac2a1e":"# Milan Price Prediction\nN.b to see the output of the commands, go to my colab file: https:\/\/colab.research.google.com\/drive\/1SNDJqy8hIXT6z2BjiBQqc3YifkhVX1_9?usp=sharing\n\nThis summer, I am going to work in Milan. Therefore, I though it would be intersting to predict the price at which an airbnb is gonna sell so that I can estimate whether the house I am renting is overpriced or underpriced. \n\n\n\n","1d324d21":"### Note before starting\nBefore starting to predict the data, we have seen how the weekly price presents a lot of outliers. For a regression model to work correctly, we ideally would like to have a [gaussian curve instead of a skewed model](https:\/\/towardsdatascience.com\/skewed-data-a-problem-to-your-statistical-model-9a6b5bb74e37). Therefore, we must first transform our weekly price in something more resembling that.\n\nAs suggested by [a notebook about the new york data](https:\/\/www.kaggle.com\/duygut\/airbnb-nyc-price-prediction), we are going to try a log.\n\nFirst, let's take a look at the weekly price distribution now","4ef32460":"\n\nTaking a look at all the work done, we can make quite a few observations:\n\n\n*   Our accuracy is not enough to provide a good estimation for a weekly price, but it's good enough to give a range at which you should rent your house \n*   To improve our accurancy, we need more data. When a person makes a judgment about an Airbnb apartment, he also takes into account the pictures the host has published, the text of the review, the description that the hosts gives and also the size. I believe that if we had all this extra data with some text mining we could get to an 80-90% accuracy.\n*   The features that are the most important in estimating the price are the one which we expected. The only thing out of place is that only the `number_of_reviews` is inversly correlated with the price. While I expected the contrary (the more I have reviews on Airbnb the more I can set an higher price), probably what happens is that houses which have a price that is set too high are not booked, thus having a lower number of reviews. \n*   All the features such as Kitchen, Heating, Washer and Wi-Fi, which we expected to be important, turned out not to be that intersting because almost all of the hosts have them. \n\n","a1c7548f":"We are also going to remove some data which we don't find important in our analysis. For example, since room type is always the same","3f11a8e4":"### Visualizing correlations","e5d9fb05":"#### Distribution by number of bedrooms\nIt may also be intersting to take a look where bigger houses are located. Let's consider them","d19e40e0":"### Non Linear Regression\n\n\n","3c013a88":"## Setup\nImport the necessary libraries","65c74b4b":"The data from the dataset is overall pretty clean and the other information may be useful for our predictions, so for now we are going to leave them.","ea22ed53":"Let's now explore some possible data correlation to see what parameters may be interesting to predict price. Let's see the price correlation based on neighbourhood","64e6fa57":"## Price prediction","a0f7fd89":"### Visualizing specific type of apartments\n\nLet's now take a look at parts of our data on the map. Let's first create a function to draw the graph\n","c1f3b16a":"\n### Visualizing the data\nOne interesting thing we can do is visualize the apartments on a map. For the map to be relevant, we should plot the points given their latitude, longitude, weekly price, and the \"municipio\" in which they are. ","483060a8":"The results are promising, especially if we compare them with the one without using the log\n\n","5359651a":"We can notice how most of the apartments have only one bedroom. Contrary to what I believed, big houses can be found in all the neighborhoods, and they are not that expensive.  ","2c8c4698":"As we can see, we can for sure remove the Unnamed column (which is just a record counter), the id of the apartment, the host id and the zip code.","319dcdab":"Let's start by taking a look at the data","a5350698":"## Explorative analysis\n\n","c6d87c3f":"So, a random model is definetely worst than our XGBoost algorithm ","f25e4d8b":"\n### Performance with all outliers\nLet's now see how xgboost performs with all the outliers \n","b4b7244c":"As I was saying before, we can see clearly how the very expensive apartments (> 75 quartile) are all concentred in the center. In contrast, looking at inexpensive apartments","add07af4":"Let's now create a function that will help us in training and asserting the score of our models","c43f2649":"## Conclusions\n\n\n### Visualizing the most important features\nLet's visualize the features that are the most important for our XGBoost model using [Shap](https:\/\/github.com\/slundberg\/shap)\n\n","6547fc12":"Load the data","8d3bc304":"Let's now take a look more in general about the most important features.\n\nThe plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). \n\nFor example, an high value on `dist_from_center` reduce the prediction by 0.50 from the base value, whereas and high value on the number of extra people an apartment can accomade boost the value by more than 1.","35b96cd7":"We can see how we get a score almost 14% better than a normal linear regression. Let's now use cross-fold validation to see how our xgboost algorithm performs","60139ab7":"We will also define a 5-fold cross validation that we are going to use whenever possible","32f5ff9f":"We can notice how the majority of the Airbnb apartments are in the center (neighbourhood = 1, the one in dark blue) and the further we are from the center the less the price of the housing is","ca59e408":"We lose on average 2.5% accuracy. \n\n### Performance only with non-airbnb data\nLet's say, as a new airbnb host, we want to predict the price at which we should sell our house. Therefore, we can't use the current prediction for the given house. Let's then remove all this data and see how our algorithm performs","e72598f9":"As we can see, with a RandomForestRegressor we get to a 52% accuracy. Another really good model that performs well with outliers is XGBoost","49e38abe":"Let's get started and try to make some predictions. Let's first divide the Dataset into X, y and divide between the train set and the test set.","644f0225":"## Data cleaning\n\nLet's take a look more deeply into the data to clean it","425d7740":"## Feature Engineering","9a12012c":"As stated before, the distribution is highly skewed. Let's try to use a log","22b90773":"#### Distribution by number of reviews\nIt may also be interesting to see which apartments are visited the most. Let's take a look using the number of reviews","c7bbbcfa":"We can see that we have many fields, some of which may not be that helpful. We also note that there are no null values","68932d74":"We can see how it reaches 60% accurancy for the test group and almost 80% for the training set. The rmse it obtains is at about 0.49. Thus, this means that on any predition we must consider an error of +-0.49. \n\nFor example, for an estimated log price of 9, which is 2**9 = 512\\$ per week, the price range of confidence is 364\\$ - 719\\$.","41112b98":"No difference at all","5f7db190":"## Context \nThe dataset I am using is [this](https:\/\/www.kaggle.com\/antoniokaggle\/milan-airbnb-open-data-only-entire-apartments) one, which contains all the entire apartments located in Milan. The dataset was originally taken from the airbnb site","c978c6a6":"#### Distribution by price\nNow, let's visualize the apartments which costs more than 800\u20ac per week\n","348e5918":"With the KernelRidge we obtain better result getting around 49%. However, if we want to further improve accurancy, we need a new model","fff062dc":"As we can see from this graph, a lot of the values are outliers, making our regression struggle. For this reason, let's try to excluse this outliers and see how our model performs","db5e3197":"Then, we can try use models that performs relatively good even with outliers","961bb569":"We want to express the latitude and longitude in a more meaningful way. Let's add a new column, `dist_from_center`, which represents the distance of every place from the center of milan","a06e11a6":"I expected that the places at the center were visited more often, but that's not the case as it seems by our graphs. One thing to notice is that very expensive apartments have very few reviews.","16363967":"We are going to assume a stay of 7 days, since the guest has to pay both for the cleaning fee and the daily fee, it's convenient to put them togheter in a new column. The cleaning fee has to be paid only one time","577c74e4":"We can see how none of them are in the center.","70c3da55":"To further improve our accuracy, we decided to use XGBoost which is the state of art regression algorithm for most of the situations. Firstly, we want to search for the best hyperparameter to XGBoost","054ccede":"#### Matrix of correlations\nLet's now look at the Pearson correlation between two variables.","f8479094":"\nLet's now try to use a non linear regression to see if we can improve our score.","93d53d4b":"## Problem statement\nWe would like to predict, given a series of variables relative to an apartment, the price at which it should be rented. ","8f83fe3a":"Then, we want to compare it to a standard LinearRegression","dac08760":"We can see how the price distribution is very diversified. What we can notice is that on average the price in the center of Milan are slightly higher, which was expected. The thing that strikes the most about these prices are the number of outliers present. We will have to fix that before start predicting"}}