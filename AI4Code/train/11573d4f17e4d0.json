{"cell_type":{"39561bdd":"code","b5ef03be":"code","3c7df700":"code","d5c85f55":"code","93a55cd8":"code","1f4114df":"code","979d8446":"code","f48a997a":"code","c1583a9d":"code","b2127c10":"code","5f7d7dd4":"code","6f9c589b":"code","89d4f1c0":"code","bfa2ea5f":"code","96329c55":"code","6175b3bd":"code","d68bcc97":"code","7130b886":"code","0c6d9c43":"code","ec7f1437":"code","24caef99":"code","45f4d26f":"code","860c5bf8":"code","36813f77":"code","8dce5335":"code","4ba0f151":"code","1788dd24":"code","8ace7cec":"code","17b0cccb":"code","32899937":"code","a5042d6b":"code","1700c608":"code","48adbf9c":"code","79906bd3":"code","9249eb97":"code","95119a2a":"code","4b456d6b":"code","d48189ac":"code","826aea0f":"code","9cfcc2b3":"code","753e7b6e":"code","2ab8219c":"code","43faccc1":"code","d4d27960":"code","421cade1":"code","1c507860":"code","a5592c58":"code","d1c971e0":"code","3de418af":"code","789bc8f4":"code","607c9530":"code","c2a057ca":"code","abf4dec5":"code","06c8c8cc":"code","4fffd249":"code","8c7d3e00":"code","8971fbdd":"code","75405d62":"code","6cb5c89a":"markdown","31c6b075":"markdown","07594af4":"markdown","10b0627d":"markdown","d5d43c65":"markdown","e3f8eaad":"markdown","09ca4dd0":"markdown","84b8c3f9":"markdown","bc904304":"markdown","83343a44":"markdown","1991fe99":"markdown","0824ad45":"markdown","2f66ab95":"markdown","ec995b47":"markdown","3ec258a1":"markdown","292356a2":"markdown","f26e8550":"markdown","132f348a":"markdown","6d0633ed":"markdown","de700b60":"markdown","5fbe7477":"markdown","6b145b1a":"markdown","d79ac9b1":"markdown","cf6692f4":"markdown","75ffa4d6":"markdown","71383eb6":"markdown","02704ba4":"markdown","69ff2c99":"markdown"},"source":{"39561bdd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","b5ef03be":"df = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',\n             usecols = [0,1], encoding = 'ISO-8859-1')\ndf.rename(columns = {'v1': 'Category','v2': 'Message'},inplace = True)","3c7df700":"df.head()","d5c85f55":"df.shape","93a55cd8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n%matplotlib inline \nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.kernel_ridge import KernelRidge\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, f1_score,classification_report\nimport xgboost as xgb\nfrom sklearn.naive_bayes import MultinomialNB\nnltk.download(\"stopwords\")\nnltk.download('punkt')","1f4114df":"spam_df = df[df['Category'] == 'spam'] #create sub-dataframe of spam text\nham_df = df[df['Category'] == 'ham'] #sub-dataframe of ham text","979d8446":"stop_words = set(stopwords.words('english'))\ndef wordCount(text): #this contains all the information about the processed length\n    try:\n        text = text.lower()\n        regex = re.compile('['+re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n'+']') \n        txt = regex.sub(' ',text)  #remove punctuation\n        words = [w for w in txt.split(' ')\n                if w not in stop_words and len(w)>3] # remove stop words and words with length smaller than 3 letters\n        return len(words)\n    except:\n        return 0","f48a997a":"spam_df['len'] = spam_df['Message'].apply(lambda x: len([w for w in x.split(' ')]))\nham_df['len'] = ham_df['Message'].apply(lambda x: len([w for w in x.split(' ')]))\nspam_df['processed_len'] = spam_df['Message'].apply(lambda x: wordCount(x))\nham_df['processed_len'] = ham_df['Message'].apply(lambda x: wordCount(x))","c1583a9d":"spam_df['punct']=spam_df['Message'].apply(lambda l1: sum([1 for x in l1 if x in set(string.punctuation)]))\nham_df['punct']= ham_df['Message'].apply(lambda l1: sum([1 for x in l1 if x in set(string.punctuation)]))","b2127c10":"spam_df.head()","5f7d7dd4":"ham_df.head()","6f9c589b":"print ('spam length info')\nprint (spam_df[['len', 'processed_len']].describe())\nprint ('ham length info')\nprint (ham_df[['len', 'processed_len']].describe())","89d4f1c0":"xmin = 0\nxmax = 50\nfig, ((ax,ax1),(ax2,ax3)) = plt.subplots (2,2,figsize = (12,9))\nspam_df['len'].plot.hist(bins = 20, ax = ax, edgecolor = 'white', color = 'orange') #ax\nspam_df['processed_len'].plot.hist(bins = 20, ax = ax1, edgecolor = 'white', color = 'orange') #ax1\nham_df['len'].plot.hist(bins = 20, ax = ax2, edgecolor = 'white', color = 'blue') #ax2\nham_df['processed_len'].plot.hist(bins = 20, ax = ax3, edgecolor = 'white', color = 'blue') #ax3\n# 4 lines for ax\nax.tick_params(labelsize = 10) #increases the size(font) of x and y axis numbers\nax.set_xlabel('length of sentence', fontsize = 12) #name x axis\nax.set_ylabel('spam_frequency', fontsize = 12) #name y axis\nax.set_xlim([xmin,xmax]) #set limit which is xmin and xmax\n#4 lines for ax1\nax1.tick_params(labelsize = 10)\nax1.set_xlabel('length of processed sentence', fontsize = 12)\nax1.set_ylabel('spam_frequency', fontsize = 12)\nax1.set_xlim([xmin,xmax])\n#4 lines for ax2\nax2.tick_params(labelsize = 10)\nax2.set_xlabel('length of sentence', fontsize = 12)\nax2.set_ylabel('ham_frequency', fontsize = 12)\nax2.set_xlim([xmin,xmax])\n#4 lines for ax3\nax3.tick_params(labelsize = 10)\nax3.set_xlabel('length of processed sentence', fontsize = 12)\nax3.set_ylabel('ham_frequency', fontsize = 12)\nax3.set_xlim([xmin,xmax])","bfa2ea5f":"def tokenize(text):\n   ## exclude = set(string.punctuation)\n    regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') #remove punctuation\n    text = regex.sub(' ', text)\n    tokens = nltk.word_tokenize(text) # tokenize the text\n    tokens = list(filter(lambda x: x.lower() not in stop_words, tokens)) # remove stop words\n    tokens = [w.lower() for w in tokens if len(w) >=3] \n    tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n    return tokens","96329c55":"spam_df['tokens'] = spam_df['Message'].map(tokenize)\nham_df['tokens'] = ham_df['Message'].map(tokenize)","6175b3bd":"spam_df.head(3)","d68bcc97":"ham_df.head(3)","7130b886":"spam_words = []\nfor token in spam_df['tokens']:\n    spam_words = spam_words + token #combine text in different columns in one list\nham_words = []\nfor token in ham_df['tokens']:\n    ham_words += token","0c6d9c43":"spam_count = Counter(spam_words).most_common(10)\nham_count = Counter(ham_words).most_common(10)","ec7f1437":"spam_count_df = pd.DataFrame(spam_count, columns = ['word', 'count'])\nham_count_df = pd.DataFrame(ham_count, columns = ['word', 'count'])","24caef99":"fig, (ax,ax1) = plt.subplots(1,2,figsize = (18, 6))\n# for spam_count_df (spam words and there count)\nsns.barplot(x = spam_count_df['word'], y = spam_count_df['count'], ax = ax)\nax.set_ylabel('count', fontsize = 15)\nax.set_xlabel('word',fontsize = 15)\nax.tick_params(labelsize=15)\nax.set_title('spam top 10 words', fontsize = 15)\n# for ham_count_df (ham words and there count)\nsns.barplot(x = ham_count_df['word'], y = ham_count_df['count'], ax = ax1)\nax1.set_ylabel('count', fontsize = 15)\nax1.set_xlabel('word',fontsize = 15)\nax1.tick_params(labelsize=15)\nax1.set_title('ham top 10 words', fontsize = 15)","45f4d26f":"spam_words_str = ' '.join(spam_words) #joined all the spam words into a paragraph\nham_words_str = ' '.join(ham_words)","860c5bf8":"spam_word_cloud = WordCloud(width = 600, height = 400, background_color = 'white').generate(spam_words_str)\nham_word_cloud = WordCloud(width = 600, height = 400,background_color = 'white').generate(ham_words_str)","36813f77":"fig, (ax, ax2) = plt.subplots(1,2, figsize = (18,8))\nax.imshow(spam_word_cloud)\nax.axis('off')\nax.set_title('spam word cloud', fontsize = 20)\nax2.imshow(ham_word_cloud)\nax2.axis('off')\nax2.set_title('ham word cloud', fontsize = 20)\nplt.show()","8dce5335":"df.head()","4ba0f151":"df['tokens'] = df['Message'].map(tokenize)","1788dd24":"def text_join(text):\n    return \" \".join(text)\ndf['text'] = df['tokens'].apply(text_join)\ndf.head()","8ace7cec":"#Tfidf - Term frequency * inverse document frequency - it return result in form of vector matrix\ntv = TfidfVectorizer('english')\nfeatures = tv.fit_transform(df['text'])\ntarget = df.Category.map({'ham':0, 'spam':1})\ndf.head()","17b0cccb":"print(features) #feature is X","32899937":"print(target) #target is y","a5042d6b":"X = features\ny = target\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)","1700c608":"print(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","48adbf9c":"df['Category'].value_counts()","79906bd3":"from collections import Counter #smote used because the dataset was imbalanced(counter helps in counting just)\nfrom imblearn.combine import SMOTETomek\nsmt=SMOTETomek(0.80) #80% of the data \nX_train_smt,y_train_smt=smt.fit_sample(X_train,y_train)\nprint(\"The number of classes before fit {}\".format(Counter(y_train)))\nprint(\"The number of classes after fit {}\".format(Counter(y_train_smt)))","9249eb97":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train_smt, y_train_smt)\ny_pred = dtc.predict(X_val)\nprint('Accuracy of Decision Tree Classifier on test set: {:.2f}'.format(dtc.score(X_val, y_val)))","95119a2a":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\n# print(sns.heatmap(cm, annot=True))\nprint(cm)","4b456d6b":"print(classification_report(y_val, y_pred))","d48189ac":"from scipy.stats import randint\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RandomizedSearchCV\n#this param_grid intake all the argument of a LOGISTIC REGRESSION MODEL, since we are dealing with logistic regression only,\n# and the arguments will be different for different models (*****IMPORTANT *******)\nparam_grid = {\"criterion\":['gini','entropy'],\n              \"max_depth\": range(1,30), \n              \"max_leaf_nodes\": range(2,30), #max_leaf_node could not start from 1\n              \"min_samples_leaf\": randint(1,20),\n              \"min_samples_split\":range(1,30),\n             \"splitter\":['best','random']} \n#just applying cross validation(as this is the parameter which is fed in gridsearchcv)((*****IMPORTANT *******))\ncv=KFold(n_splits=5,shuffle = True, random_state = 42)\n#here these are the parameters of gridsearchcv(*****IMPORTANT *******)\nrsc = RandomizedSearchCV(dtc,param_grid,cv=cv,verbose=2,random_state = 42,n_jobs=-1)\nrsc.fit(X_train_smt,y_train_smt)\n# Print the tuned parameters and score \nprint(\"Tuned Decision Tree Parameters: {}\".format(rsc.best_params_)) \nprint(\"Best score is {}\".format(rsc.best_score_)) ","826aea0f":"y_pred = rsc.predict(X_val)\nprint('Accuracy of Decision Tree Classifier on test set: {:.2f}'.format(rsc.score(X_val, y_val)))","9cfcc2b3":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\n# print(sns.heatmap(cm, annot=True))","753e7b6e":"print(cm)","2ab8219c":"print(classification_report(y_val, y_pred))","43faccc1":"from xgboost import XGBClassifier","d4d27960":"xgb = XGBClassifier()\nxgb.fit(X_train_smt, y_train_smt)\ny_pred = xgb.predict(X_val)\nprint('Accuracy of Decision Tree Classifier on test set: {:.2f}'.format(xgb.score(X_val, y_val)))","421cade1":"#Importing random forest classifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","1c507860":"#creating a random forest instance   #while initializing no parameter\nrfr =RandomForestClassifier(random_state=56)","a5592c58":"#train the model\nrfr.fit(X_train_smt, y_train_smt)","d1c971e0":"#score on training data\nrfr.score(X_train_smt, y_train_smt)","3de418af":"#score on training data\nrfr.score(X_val,y_val)","789bc8f4":"y_predict = rfr.predict(X_val)\nRF = pd.DataFrame({'Actual':y_val, 'Predicted':y_predict})  \nRF.head(5)","607c9530":"from sklearn.svm import SVC #imported svc","c2a057ca":"model = SVC(kernel='rbf',C=30,gamma='auto') #fit model on training data and chcek score of test data\nmodel.fit(X_train_smt, y_train_smt)\nmodel.score(X_val,y_val)","abf4dec5":"y_predict = model.predict(X_val)\ny_predict","06c8c8cc":"mnb = MultinomialNB() #fit model on training data and chcek score of test data\nmnb.fit(X_train_smt, y_train_smt)\nmnb.score(X_val,y_val)","4fffd249":"#this param_grid intake all the argument of a LOGISTIC REGRESSION MODEL, since we are dealing with logistic regression only,\n# and the arguments will be different for different models (*****IMPORTANT *******)\nparam_grid = {\"alpha\" : [0.7,0.8,0.9,1.0,1.5,2.0,2.5,3.0,4.0,5.0]} \n#just applying cross validation(as this is the parameter which is fed in gridsearchcv)((*****IMPORTANT *******))\ncv=KFold(n_splits=5,shuffle = True, random_state = 42)\n#here these are the parameters of gridsearchcv(*****IMPORTANT *******)\nrsc = RandomizedSearchCV(mnb,param_grid,cv=cv,verbose=2,random_state = 42,n_jobs=-1)\nrsc.fit(X_train_smt,y_train_smt)\n# Print the tuned parameters and score \nprint(\"Tuned Decision Tree Parameters: {}\".format(rsc.best_params_)) \nprint(\"Best score is {}\".format(rsc.best_score_)) ","8c7d3e00":"mnb = MultinomialNB(alpha = 0.7) #fit model on training data and chcek score of test data\nmnb.fit(X_train_smt, y_train_smt)\nmnb.score(X_val,y_val)","8971fbdd":"y_predict = mnb.predict(X_val)\ny_predict","75405d62":"print('Accuracy of Decision Tree Classifier on test set: {:.2f}'.format(dtc.score(X_val, y_val)))\nprint('Accuracy of XgBoost Classifier on test set: {:.2f}'.format(xgb.score(X_val, y_val)))\nprint('Accuracy of Random Forest Classifier on test set: {:.2f}'.format(rfr.score(X_val, y_val)))\nprint('Accuracy of SVC  on test set: {:.2f}'.format(model.score(X_val, y_val)))\nprint('Accuracy of Multinomial Naive Bayes on test set: {:.2f}'.format(mnb.score(X_val, y_val)))","6cb5c89a":"## 4. Exploratory Data Analysis (EDA) - Part I","31c6b075":"Above wordCount function is applied to spam and ham to get the count of original leangth of message and pre-processed length","07594af4":"1. Length of processed sentence in spam is more symmetrical than length of processed sentence in ham(which looks right-skewed)\n2. The mean of processed length of a spam message is way more higher than that of processed length of ham message which sounds logical as spam message contains irrelevant words,lots of noise, abundance information","10b0627d":"Accuracy achieved on the training and testing sets are pretty awesome. In all, Random Forest Classifier gives an accuracy of 0.98 of the testing set (would really give cool predictions on the unseed dataset!!!!!!!)","d5d43c65":"I have created tokens of Message. These tokens are very useful for finding such patterns as well as is considered as a base step for stemming and lemmatization. ","e3f8eaad":"## 10.Model Building ","09ca4dd0":"## 6. Exploratory Data Analysis (EDA) - Part II","84b8c3f9":"## 3. Importing necessary libraries","bc904304":"Above function is created to derive information(count) of the processed length of message.The processing is done as -\n* Firstly, text.lower() converts the text into lower case\n* A regex is defined by which I have replaced all the punctuation with text(as these punctuation makes the model building part hectic). \n* Lastly, I have removed all the rare words whose length is less than 3. ","83343a44":"The dataset has 5572 rows and 2 columns","1991fe99":"## 9. Processing the main dataset (df)","0824ad45":"### 4. SVC","2f66ab95":"I have created a separate data-frame as spam_df (in which I will be preprocessing(cleaning,analyzing) the Spam Message) and ham_df (in which the Ham Message will be preprocessed)","ec995b47":"### 5. Multinomial Naive Bayes","3ec258a1":"By calling the describe function, It's easy to understand the original and processed length of the dataframes","292356a2":"### 2. XGBoost","f26e8550":"## 5. Text Length Analysis","132f348a":"## 8. Worls Cloud","6d0633ed":"It is clearly visible that our dataset is NOT BALANCED. So, I have performed Smoting technique to make it balanced (otherwise our model would be biased towards ham only(as it has more no. of counts))","de700b60":"### 1. Decision Tree","5fbe7477":"## 2. Loading the dataset","6b145b1a":"##### **Objective** - This is a document classification project to classify spam vs ham. I will construct a spam filter to classify text message as ham or spam. ","d79ac9b1":"Above code helps to calculate Most Frequent words","cf6692f4":"## 1. Importing necessary libraries","75ffa4d6":"## 7. Most Common Words","71383eb6":"A new column punctuation is created in the spam and ham dataframe to get the count of the punctuation","02704ba4":"### 3. Random Forest ","69ff2c99":"World cloud is the best way to view a pictorical representation of the most common words in the dataset"}}