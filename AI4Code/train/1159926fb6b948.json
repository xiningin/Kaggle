{"cell_type":{"fe5fc5bd":"code","300517b8":"code","b2b3ca7d":"code","51cc7c83":"code","4ca2fcb9":"code","b43916db":"code","57bf946f":"code","16e625fb":"code","853af67b":"code","d5944308":"code","57fe47a6":"code","5c50d025":"code","febc63e9":"code","94d6f1c8":"code","d940e9b2":"code","8634ab5f":"code","43f22c6a":"code","4bcfd604":"code","209d28d7":"code","0a4729f9":"code","8eb96dfc":"code","842f45fd":"code","0578c7dd":"code","8fe40dc5":"code","169cdfd2":"code","f7dc624c":"code","0832be9a":"code","0815efee":"code","3d4d85c1":"code","5356df00":"code","ac67ebf2":"code","3b43313b":"markdown","f9c07e7c":"markdown","adb02cf6":"markdown","5e0cd4ac":"markdown","4ed0d3bb":"markdown","ab038d00":"markdown","095add70":"markdown","722fb9c1":"markdown","cbe7e831":"markdown","08c83f1f":"markdown","277cc87f":"markdown","28cda13f":"markdown","4cb6cb2f":"markdown","fe9deec1":"markdown","b4109c03":"markdown","d6d05528":"markdown","f44d0aaf":"markdown","dd70a416":"markdown","958efe73":"markdown","ebdf1cc3":"markdown","47889527":"markdown","f9db7f87":"markdown","b3628980":"markdown","64659c76":"markdown","e805ecd1":"markdown","95c38131":"markdown","d3137009":"markdown"},"source":{"fe5fc5bd":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns \nimport tensorflow as tf \n%matplotlib inline ","300517b8":"credit_df=pd.read_csv('..\/input\/credit-card-approval-prediction\/credit_record.csv')\naplication_df=pd.read_csv('..\/input\/credit-card-approval-prediction\/application_record.csv')","b2b3ca7d":"grouped=credit_df.groupby('ID')['STATUS'].value_counts()\ngrouped","51cc7c83":"credit_df.groupby('STATUS').count()\nnumber_of_customer_for_each_status=grouped.groupby('STATUS').count()\nnumber_of_customer_for_each_status.plot(kind='bar')","4ca2fcb9":"credit_grouped=pd.get_dummies(data=credit_df,columns=['STATUS'],\n                              prefix='',prefix_sep='').groupby('ID')[sorted(credit_df['STATUS'].unique().tolist())].sum()\ncredit_grouped=credit_grouped.rename(columns=\n                      {'0':'pastdue_1_29',\n                       '1':'pastdue_30_59',\n                       '2':'pastdue_60_89',\n                       '3':'pastdue_90_119',\n                       '4':'pastdue_120_149',\n                       '5':'pastdue_over_150',\n                       'C':'paid_off',\n                       'X':'no_loan',\n                      })\n\noverall_pastdue=['pastdue_1_29','pastdue_30_59',\t'pastdue_60_89',\t'pastdue_90_119'\t,'pastdue_120_149',\t'pastdue_over_150']\ncredit_grouped['number_of_months']=credit_df.groupby('ID')['MONTHS_BALANCE'].count()\ncredit_grouped['over_90']=credit_grouped[['pastdue_90_119'\t,'pastdue_120_149'\t,'pastdue_over_150']].sum(axis=1)\ncredit_grouped['less_90']=credit_grouped[['pastdue_1_29','pastdue_30_59',\t'pastdue_60_89']].sum(axis=1)\ncredit_grouped['overall_pastdue']=credit_grouped[overall_pastdue].sum(axis=1)\ncredit_grouped['paid_pastdue_diff']=credit_grouped['paid_off']- credit_grouped['overall_pastdue']\ncredit_grouped.head()","b43916db":"target=[]\nfor index,row in credit_grouped.iterrows() :\n  if row['paid_pastdue_diff'] >=3 or (row ['no_loan']==row['number_of_months']) :\n    target.append(1)\n  elif row['paid_pastdue_diff'] >-2 and row['paid_pastdue_diff'] <3 and (row['less_90'] >=row['over_90']) :\n    target.append(1)\n  else:\n    target.append(0)\n\ncredit_grouped['good_or_bad']=target\ncredit_grouped['good_or_bad'].value_counts()","57bf946f":"aplication_df.head()","16e625fb":"aplication_df.duplicated().sum()","853af67b":"features=['no_loan',\t'number_of_months',\t'over_90',\t'less_90',\t'overall_pastdue'\t,'paid_pastdue_diff','good_or_bad']\nmost_important_features=credit_grouped.loc[:,features]\ncustomers_df=pd.merge(aplication_df,most_important_features,on='ID')\ncustomers_df.index=customers_df['ID']\ncustomers_df=customers_df.drop('ID',axis=1)\ncustomers_df","d5944308":"x=customers_df.loc[:,:'paid_pastdue_diff']\ny=customers_df['good_or_bad']\nx","57fe47a6":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)","5c50d025":"cols=['CODE_GENDER',\t'FLAG_OWN_CAR',\n            'FLAG_OWN_REALTY','NAME_FAMILY_STATUS',\t'NAME_HOUSING_TYPE',\n            'FLAG_MOBIL'\t,'FLAG_WORK_PHONE',\t'FLAG_PHONE',\t'FLAG_EMAIL'\t,'OCCUPATION_TYPE']\nfor col in  cols:\n  customers_df.groupby(col)['good_or_bad'].value_counts().plot(kind='bar')\n  plt.show()","febc63e9":"x_train=x_train.drop(['FLAG_MOBIL','FLAG_WORK_PHONE','FLAG_PHONE','FLAG_EMAIL'],axis=1)","94d6f1c8":"# Convert days birth to age column and days employed to work years \nx_train['AGE']=(x_train['DAYS_BIRTH']\/365)*-1\nx_train['AGE']=x_train['AGE'].apply(lambda v : int(v))\nx_train['WORK_YEARS']=x_train['DAYS_EMPLOYED']\/365\nx_train['WORK_YEARS']=x_train['WORK_YEARS'].apply(lambda v : int(v*-1) if v <0 else 0)\nx_train=x_train.drop(columns=['DAYS_BIRTH','DAYS_EMPLOYED'])\nx_train","d940e9b2":"# children count is subset of family size so we can drop the children count feature \nx_train=x_train.drop(columns=['CNT_CHILDREN'])","8634ab5f":"x_train.info()","43f22c6a":"from sklearn.impute import SimpleImputer \nimputer =SimpleImputer(strategy='most_frequent')\nx_imputed=pd.DataFrame(imputer.fit_transform(x_train),index=x_train.index,columns=x_train.columns)\n# Imputer change the type of the features so we should reset it again\nx_imputed=x_imputed.astype(x_train.dtypes)\n#Convert binary types to categorical types to be ready for one hot encoding \n# x_imputed['FLAG_MOBIL']=x_imputed['FLAG_MOBIL'].astype('object')\n# x_imputed['FLAG_WORK_PHONE']=x_imputed['FLAG_WORK_PHONE'].astype('object')\n# x_imputed['FLAG_PHONE']=x_imputed['FLAG_PHONE'].astype('object')\n# x_imputed['FLAG_EMAIL']=x_imputed['FLAG_EMAIL'].astype('object')\n# Change no_load type to int64 \nx_imputed['no_loan']=x_imputed['no_loan'].astype('int64')\nx_imputed['CNT_FAM_MEMBERS']=x_imputed['CNT_FAM_MEMBERS'].astype('int64')\nx_imputed.info()","4bcfd604":"from pandas.core.algorithms import value_counts\ncategorical_df=x_imputed.select_dtypes('object')\ncategorical_df.nunique()","209d28d7":"# install category_encoders\n!pip install category_encoders","0a4729f9":"from category_encoders import MEstimateEncoder\ntarget_encoder=MEstimateEncoder(m=5,cols=['OCCUPATION_TYPE'])\n# train the encoder with the 0.25 of the data to prevent overfitting \nx_encode=x_imputed.sample(frac=0.25)\ny_encode=y_train[x_encode.index]\ntarget_encoder.fit(x_encode,y_encode)\n# transform the categorical feature\nx_encoded=target_encoder.transform(x_imputed)\nx_encoded['OCCUPATION_TYPE'].unique()","8eb96dfc":"x_encoded","842f45fd":"# Get numiric data\nnumiric_data=x_encoded._get_numeric_data()\nnumiric_data","0578c7dd":"from sklearn.preprocessing import StandardScaler\nscaler =StandardScaler()\nnumiric_data_scaled=scaler.fit_transform(numiric_data)\nnumiric_data_scaled=pd.DataFrame(numiric_data_scaled,index=numiric_data.index,columns=numiric_data.columns)\nnumiric_data_scaled","8fe40dc5":"x_encoded[numiric_data_scaled.columns]=numiric_data_scaled[numiric_data_scaled.columns]\nx_standarized=x_encoded.copy()","169cdfd2":"x_train=pd.get_dummies(x_standarized)\nx_train","f7dc624c":"x_train.info()","0832be9a":"x_test=x_test.drop(['FLAG_MOBIL','FLAG_WORK_PHONE','FLAG_PHONE','FLAG_EMAIL'],axis=1)\n# Convert days birth to age column and days employed to work years \nx_test['AGE']=(x_test['DAYS_BIRTH']\/365)*-1\nx_test['AGE']=x_test['AGE'].apply(lambda v : int(v))\nx_test['WORK_YEARS']=x_test['DAYS_EMPLOYED']\/365\nx_test['WORK_YEARS']=x_test['WORK_YEARS'].apply(lambda v : int(v*-1) if v <0 else 0)\nx_test=x_test.drop(columns=['DAYS_BIRTH','DAYS_EMPLOYED'])\n# children count is subset of family size so we can drop the children count feature \nx_test=x_test.drop(columns=['CNT_CHILDREN'])\n\n# We use the imputer from simple imputer that fitted from the train data\nx_test_imputed=pd.DataFrame(imputer.transform(x_test),index=x_test.index,columns=x_test.columns)\n# imputer change the type of the features so we should reset it again\nx_test_imputed=x_test_imputed.astype(x_test.dtypes)\nx_test_imputed['no_loan']=x_test_imputed['no_loan'].astype('int64')\nx_test_imputed['CNT_FAM_MEMBERS']=x_test_imputed['CNT_FAM_MEMBERS'].astype('int64')\n\n# We use the target_encoder from categiry encoders that fitted from the train data\nx_test_encoded=target_encoder.transform(x_test_imputed)\n# Standarization \nnumiric_test_data=x_test_encoded._get_numeric_data()\nnumiric_test_data_scaled=scaler.transform(numiric_test_data)\nnumiric_test_data_scaled=pd.DataFrame(numiric_test_data_scaled,index=numiric_test_data.index,columns=numiric_test_data.columns)\nx_test_encoded[numiric_test_data_scaled.columns]=numiric_test_data_scaled[numiric_test_data_scaled.columns]\nx_test_standarized=x_test_encoded.copy()\n# one hot encoded\nx_test=pd.get_dummies(x_test_standarized)\nx_test","0815efee":"from sklearn import svm\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score\nmodel=svm.NuSVC()\n# x,x_valid,y,y_valid=train_test_split(x_train,y_train,test_size=0.2)\nmodel.fit(x_train,y_train)\n# Calculate train accuracy \ny_train_pred=model.predict(x_train)\nprint(f'Train accuracy score = {accuracy_score(y_train,y_train_pred)}')\nprint(f'Train f1_score = {f1_score(y_train,y_train_pred)}')\nprint(f'Train roc_auc_score = {roc_auc_score(y_train,y_train_pred)}')\n# Calculate test accuracy\ny_test_pred=model.predict(x_test)\nprint(f'test accuracy score = {accuracy_score(y_test,y_test_pred)}')\nprint(f'test f1_score = {f1_score(y_test,y_test_pred)}')\nprint(f'test roc_auc_score = {roc_auc_score(y_test,y_test_pred)}')","3d4d85c1":"from sklearn import svm\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score\nmodel=svm.LinearSVC()\n# x,x_valid,y,y_valid=train_test_split(x_train,y_train,test_size=0.2)\nmodel.fit(x_train,y_train)\n# Calculate train accuracy \ny_train_pred=model.predict(x_train)\nprint(f'Train accuracy score = {accuracy_score(y_train,y_train_pred)}')\nprint(f'Train f1_score = {f1_score(y_train,y_train_pred)}')\nprint(f'Train roc_auc_score = {roc_auc_score(y_train,y_train_pred)}')\n# Calculate test accuracy\ny_test_pred=model.predict(x_test)\nprint(f'test accuracy score = {accuracy_score(y_test,y_test_pred)}')\nprint(f'test f1_score = {f1_score(y_test,y_test_pred)}')\nprint(f'test roc_auc_score = {roc_auc_score(y_test,y_test_pred)}')","5356df00":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n\nmodel=RandomForestClassifier(n_estimators=100,criterion='gini',random_state=1,max_leaf_nodes=8)\n# x,x_valid,y,y_valid=train_test_split(x_train,y_train,test_size=0.2)\nmodel.fit(x_train,y_train)\n# Calculate train accuracy \ny_train_pred=model.predict(x_train)\nprint(f'Train accuracy score = {accuracy_score(y_train,y_train_pred)}')\nprint(f'Train f1_score = {f1_score(y_train,y_train_pred)}')\nprint(f'Train roc_auc_score = {roc_auc_score(y_train,y_train_pred)}')\n# Calculate test accuracy\ny_test_pred=model.predict(x_test)\nprint(f'test accuracy score = {accuracy_score(y_test,y_test_pred)}')\nprint(f'test f1_score = {f1_score(y_test,y_test_pred)}')\nprint(f'test roc_auc_score = {roc_auc_score(y_test,y_test_pred)}')","ac67ebf2":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n\nmodel=XGBClassifier(n_estimators=50,learning_rate=0.01,random_state=0)\n# x,x_valid,y,y_valid=train_test_split(x_train,y_train,test_size=0.2)\nmodel.fit(x_train,y_train)\n# Calculate train accuracy \ny_train_pred=model.predict(x_train)\nprint(f'Train accuracy score = {accuracy_score(y_train,y_train_pred)}')\nprint(f'Train f1_score = {f1_score(y_train,y_train_pred)}')\nprint(f'Train roc_auc_score = {roc_auc_score(y_train,y_train_pred)}')\n# Calculate test accuracy\ny_test_pred=model.predict(x_test)\nprint(f'test accuracy score = {accuracy_score(y_test,y_test_pred)}')\nprint(f'test f1_score = {f1_score(y_test,y_test_pred)}')\nprint(f'test roc_auc_score = {roc_auc_score(y_test,y_test_pred)}')","3b43313b":"Form the insights above I will drop these features :<br>\n1-FLAG_MOBILE because it has one value <br>\n2-FLAG_WORK_PHONE ,because it has small effect to determine bad or good <br>\n3-FLAG_PHONE ,because it has small effect to determine bad or good<br>\n4-FLAG_EMAIL ,because it has small effect to determine bad or good<br>\n","f9c07e7c":"Support vector machine","adb02cf6":"<h1>Standardization<\/h1>\nI have noticed that features are in different ranges so standardization is a very good way to solve this\n","5e0cd4ac":"A good point that we reached to a balanced split","4ed0d3bb":"Check duplication","ab038d00":"Random Forest for classification","095add70":"<h1>Dealing with categorical value<br><\/h1>\nFirstly we invistigate them","722fb9c1":"<h1>Dealing with the low cadinality categorical features<\/h1>\nthe best way is to use one hot encoding","cbe7e831":"## Prepare test dateset","08c83f1f":"<h3>Invistigates features with the output to determine its effect ","277cc87f":"Linear support vector machine","28cda13f":"<h1> Dealing with the missing values <\/h1>\nOCCUPATION_TYPE feature has a missing values in it and its a categorical feature so we will fill these values with most frequent value for this column and will use the simple imputer to do this","4cb6cb2f":"Invistigate credit_records The dataset","fe9deec1":"<h1>Split the target and features ","b4109c03":"Invistigate aplication_records dataset ","d6d05528":"## Import important libraries ","f44d0aaf":"\nMost customers pay off their loans in 1-29 days<br>\nHigh percetage of customers don't borrow loans <br>\nAlso High percetage of customers pay off their loan forever <br>\nFew people are late in paying off their loans ","dd70a416":"We can notice that only OCCUPATION_TYPE feature has a high cadinality and the rest is low cadinality .<br>\nso we will use target encoding for OCCUPATION_TYPE  and one hot encoding for the rest .","958efe73":"<h1>Train test spilt","ebdf1cc3":"## Data preprocessing","47889527":"XGBoost Clasifier","f9db7f87":"Reading both datasets","b3628980":"<h1>Let's Merge the to datasets to have a full insights <br><\/h1>\nCredit_record dataset don't has only the target ,but aslo has a very interesting historical transactions features that will add benifits","64659c76":"<h1>Let's create strategy to determine Good and Bad Customers<\/h1>\n\nGood customer conditions :<br>\nCheck the difference between numbers of times the customer paid off and the number of late and if it is more than threshold then pure good customer else we should take another check <br>\npaid_pastdue_diff >=3 or no_loan==number_of_months\n\nalso check if he too late or pay off in small period and has a suitable differance <br>\npaid_pastdue_diff >=-2 and paid_pastdue_diff < 3  \ncheck and less_90 >= over_90 good <br>\n\nIf the customer doesn't achive this conditions then he is a bad customr\n\n\n\n\n\n","e805ecd1":"##Data Visualization","95c38131":"Let's see the historical transactions for each customer","d3137009":"## Machine learning models"}}