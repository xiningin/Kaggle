{"cell_type":{"3d562da3":"code","0dfa8030":"code","0b57afa0":"code","d149dc20":"code","9cf22154":"code","3e288efc":"code","c89b0af9":"code","4948db4b":"code","0bbcb406":"code","42dcf914":"code","7bf801be":"code","82ca888f":"code","4b48cfe8":"code","f9e66523":"markdown","45d28537":"markdown","6a52d4c0":"markdown","f93815d8":"markdown","cbb1dace":"markdown","3b5ea2f4":"markdown","f813839b":"markdown"},"source":{"3d562da3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0dfa8030":"# Import the necesary packages\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sea\nimport matplotlib.pyplot as plt\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","0b57afa0":"# Load the data\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","d149dc20":"# Check missingness:\nmissingData = train.isnull().mean(axis=0)\n\n# remove is greater than 30%\n# index and gives the column names\nmissingIndex = missingData[missingData>0.3].index\nmissingIndex\n\n# Make a working copy of the data\nworkingDf = train.copy()\nworkingDf.isna().sum().loc[workingDf.isna().sum()>0].sort_values()","9cf22154":"#Remove NA from PoolQC\nworkingDf.loc[pd.Series(workingDf.PoolQC.isna()), 'PoolQC'] = 'NoPool'","3e288efc":"# Compare frontage to lot area!\nlotFrontageByArea = workingDf[['LotFrontage', 'LotArea']]\nplt.scatter(np.log(workingDf['LotArea']), np.log(workingDf['LotFrontage']))","c89b0af9":"from sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n\nlotByAreaModel = linear_model.LinearRegression()\n\n#Split into missing and not missing\nworkingDfFrontageNas = workingDf[workingDf.LotFrontage.isna()]\nworkingDfFrontageNoNas = workingDf[~workingDf.LotFrontage.isna()]\n\nlotByAreaModel.fit(workingDfFrontageNoNas[['LotArea']], workingDfFrontageNoNas.LotFrontage)\n\n# Must use Data frame\nworkingDfFrontageNas.LotFrontage = lotByAreaModel.predict(workingDfFrontageNas[['LotArea']])\n\n# Must concat a list!!!\nworkingDfImputedFrontage = pd.concat([workingDfFrontageNas, workingDfFrontageNoNas], axis = 0)","4948db4b":"# Now Dummify to, workingDummies\nworkingClean = workingDf.select_dtypes(include=['object'])\nworkingDummies = workingClean.copy()\nworkingDummies = pd.get_dummies(workingDummies)\nprint(workingDummies.shape)\nworkingDummies.head()\n\nprint(workingDummies.isna().sum().loc[workingDummies.isna().sum()>0].sort_values(ascending=False))","0bbcb406":"# Replace NAs in Dummies Set with 0\nprint(workingDummies.isna().sum().loc[workingDummies.isna().sum()>0].sort_values(ascending=False))","42dcf914":"#split feature and salePrice\nsalePriceClean = workingDf.SalePrice\nhomeFeaturesClean = workingDf.copy().drop('SalePrice',axis=1)\nsalePriceDummies = workingDf.SalePrice\nhomeFeaturesDummies=workingDf.copy().drop(\"SalePrice\",axis=1)","7bf801be":"workingNumeric = workingDf[['GarageYrBlt', 'LotFrontage', 'LotArea', 'OverallCond', 'YearBuilt', 'YearRemodAdd',  'MoSold', 'GarageArea', 'TotRmsAbvGrd', 'GrLivArea', 'BsmtUnfSF', 'MSSubClass', 'YrSold', 'MiscVal', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'BsmtFullBath','BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageCars', 'WoodDeckSF', 'OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'OverallQual', 'MasVnrArea']]\nworkingNumeric['SalePrice'] = salePriceClean","82ca888f":"fig = plt.figure(figsize=[20,10])\n\n# get current axis = gca\nax = fig.gca()\n\n# We here will apply to the last one described...\nworkingNumeric.hist(ax = ax)\nplt.subplots_adjust(hspace=0.5)","4b48cfe8":"#KNN\nknn_model = KNeighborsRegressor()\nparam_grid = {'n_neighbors':np.arange(5, 200, 5)}\ngsModelTrain = GridSearchCV(estimator = knn_model, param_grid = param_grid, cv=2)\ngsModelTrain.fit(featuresDummiesTrain, priceDummiesTrain)\nknn_model.set_params(**gsModelTrain.best_params_)\n#fit to train data\nknn_model.fit(featuresDummiesTrain, priceDummiesTrain)\n#Get scores comparing real house prices and predicted house prices from the test dataset.\nprint(\"r2 Test score:\", r2_score(priceDummiesTest, knn_model.predict(featuresDummiesTest)))\nprint(\"r2 Train score:\", r2_score(priceDummiesTrain, knn_model.predict(featuresDummiesTrain)))\ntrainRMSE = np.sqrt(mean_squared_error(y_true=priceDummiesTrain, y_pred=knn_model.predict(featuresDummiesTrain)))\ntestRMSE = np.sqrt(mean_squared_error(y_true=priceDummiesTest, y_pred=knn_model.predict(featuresDummiesTest)))\nprint(\"Train RMSE:\", trainRMSE)\nprint(\"Test RMSE:\", testRMSE)","f9e66523":"Filling in missing data from a correlated feature we can make a model of the two, split them into missing and non-missing, then predict the missing values and recombine!","45d28537":"Next, need to dummify the the categorical features. In some cases this can make our data set extremely \u201cwide\u201d but that is ok for most of the regression we will be using","6a52d4c0":"# I. Load and Clean the Data","f93815d8":"# Check For Missing Data","cbb1dace":"Now, for EDA we will visualize the continuous and categorical features.","3b5ea2f4":"Replacing NULL's with 0 or \"No Pool\"","f813839b":"Some features may be highly correlated AND have missing values. Check correlation with feature you might suspect with something like a scatter plot. For example, LotFrontage is missing a lot of data but my guess is it is correlated to total LotArea."}}