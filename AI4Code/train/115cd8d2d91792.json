{"cell_type":{"48e7c391":"code","a928f2c1":"code","479f1077":"code","45389a5f":"code","11380105":"code","45d71fb7":"code","83b83f5a":"code","3575b9a2":"code","809f5e6d":"code","2ead35a3":"code","3021c1b2":"code","050e4f24":"code","e3536437":"code","4bbfa43b":"code","a0debf61":"code","3b76a18a":"code","9832a888":"code","6ca1323b":"code","3ea72f7f":"code","bb0b1854":"code","9a6c3ec7":"code","e87bb1f4":"code","12290eda":"code","0ecc457f":"code","49d3afd6":"code","2012edb0":"code","3e69d820":"code","3d486f7d":"code","cfe12364":"code","a69c1d68":"code","7ce7d538":"code","ebb322cf":"code","b6415abd":"code","4ea97fa5":"code","19ce83d7":"code","70ded6fd":"code","93918ef2":"code","787140b7":"code","f79fa056":"markdown","9a502a20":"markdown","ac4a6b93":"markdown","0cb667e0":"markdown","d46c6f5b":"markdown","42992f56":"markdown"},"source":{"48e7c391":"##############################################################\n# Constants\n##############################################################\n\nKMS_PER_RADIAN = 6371.0088\n\nJFK_GEO_LOCATION = (40.6413, -73.7781)\nLGR_GEO_LOCATION = (40.7769, -73.8740)\nEWR_GEO_LOCATION = (40.6895, -74.1745)","a928f2c1":"##############################################################\n# Input Parameters used with 1M training data points\n##############################################################\n\n# Training data rows to read\nMAX_TRAINING_SIZE = 1_000_00\n\n# Input parameters for DBSCAN GeoSpatial Desnity based clustering\nEPS_IN_KM = 0.1           ## NOTE that lat\/long are available till 5th decimal value & 0.1km = 1.xe-5, hence avoid using smaller DBSCAN's eps, i.e., radius threshold for clustering\nMIN_SAMPLES_CLUSTER = 500\n\n# Pickup\/dropoff within small radius of airports geo location\nRADIUS_VICINITY_AIRPORTS = 1.0\n\n# Thershold for trip fare rate to remove those spurious trips involving exorbitant fare rate\nTHERSHOLD_TRIP_FARE_RATE = 50.0\n\n# Thereshold for compressing trip distance range from 0.0-110.x to 0.0-25.0\nTHRESHOLD_TRIP_DISTANCE = 25.0","479f1077":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n% matplotlib inline\nplt.style.use('seaborn-whitegrid')\n\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\nimport timeit\nfrom sklearn import metrics\nfrom haversine import haversine","45389a5f":"start_time = timeit.default_timer()\n\n# read data in pandas dataframe\ndf_train =  pd.read_csv('..\/input\/train.csv', nrows = MAX_TRAINING_SIZE, parse_dates=[\"pickup_datetime\"])\ndf_holdout =  pd.read_csv('..\/input\/test.csv', parse_dates=[\"pickup_datetime\"])\ntest_key = df_holdout['key']\ndf_train.drop(columns = ['key'], inplace=True)\ndf_holdout.drop(columns = ['key'], inplace=True)\n\nelapsed = timeit.default_timer() - start_time\nelapsed","11380105":"print('Old size: %d' % len(df_train))\n\n### Ignore -ve fare\ndf_train = df_train[df_train.fare_amount >=0]\n\n### NOTE that there is no missing or NA in 'test'\n### Remove rows with NA in any field\ndf_train = df_train.dropna(how='any', axis='rows')\n\n### 'test': No spurious passenger_count (min is 1 & max is 6)\n### 'train': passengers_count max is 208... just 11 out of 1M trips with count > 7, hence removing those trips\ndf_train = df_train.drop(index= df_train[df_train.passenger_count >= 7].index, axis='rows')\ndf_train = df_train.drop(index= df_train[df_train.passenger_count == 0].index, axis='rows')\n\nprint('New size: %d' % len(df_train))","45d71fb7":"### NOTE that 'test' lat-long are well within NYC boundary, whereas there are few spurious 'train' datapoints outside of NYC boundary hence removing those trips\n\n#min(df_train.pickup_longitude.min(), df_train.dropoff_longitude.min()), max(df_train.pickup_longitude.max(), df_train.dropoff_longitude.max())\n#min(df_train.pickup_latitude.min(), df_train.dropoff_latitude.min()), max(df_train.pickup_latitude.max(), df_train.dropoff_latitude.max())\n\ndef select_within_boundingbox(df, BB):\n    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n            \n#Times Square (40.7590\u00b0 N, 73.9845\u00b0 W)\n#BB = (-74.2, -73.8, 40.6, 41.0)\n\n# load image of NYC map\nBB = (-74.5, -72.8, 40.5, 41.8)\n\nprint('Old size: %d' % len(df_train))\ndf_train = df_train[select_within_boundingbox(df_train, BB)]\nprint('New size: %d' % len(df_train))","83b83f5a":"# IDEA.n: Ideally, shortest route distance should be used\ndef addPickDropDistanceFeature(df):\n\n    df['trip_distance'] = df.apply(\n        (lambda row: haversine(\n            (row['pickup_latitude'], row['pickup_longitude']),\n            (row['dropoff_latitude'], row['dropoff_longitude']))\n        ),\n        axis='columns'\n    )\n    return df\n\ndef addAirportDistanceFeatures(df):\n\n    df['pickup_distance_to_jfk'] = df.apply(\n        (lambda row: haversine(\n            (row['pickup_latitude'], row['pickup_longitude']),\n            (JFK_GEO_LOCATION[0], JFK_GEO_LOCATION[1]))\n        ),\n        axis='columns'\n    )\n\n    df['drop_distance_to_jfk'] = df.apply(\n        (lambda row: haversine(\n            (row['dropoff_latitude'], row['dropoff_longitude']),\n            (JFK_GEO_LOCATION[0], JFK_GEO_LOCATION[1]))\n        ),\n        axis='columns'\n    )\n\n    df['pickup_distance_to_lgr'] = df.apply(\n        (lambda row: haversine(\n            (row['pickup_latitude'], row['pickup_longitude']),\n            (LGR_GEO_LOCATION[0], LGR_GEO_LOCATION[1]))\n        ),\n        axis='columns'\n    )\n\n    df['drop_distance_to_lgr'] = df.apply(\n        (lambda row: haversine(\n            (row['dropoff_latitude'], row['dropoff_longitude']),\n            (LGR_GEO_LOCATION[0], LGR_GEO_LOCATION[1]))\n        ),\n        axis='columns'\n    )\n\n    df['pickup_distance_to_ewr'] = df.apply(\n        (lambda row: haversine(\n            (row['pickup_latitude'], row['pickup_longitude']),\n            (EWR_GEO_LOCATION[0], EWR_GEO_LOCATION[1]))\n        ),\n        axis='columns'\n    )\n\n    df['drop_distance_to_ewr'] = df.apply(\n        (lambda row: haversine(\n            (row['dropoff_latitude'], row['dropoff_longitude']),\n            (EWR_GEO_LOCATION[0], EWR_GEO_LOCATION[1]))\n        ),\n        axis='columns'\n    )\n    \n    return df\n\ndef getAirportTrips(df, airportVicinity):\n    ids = (df.pickup_distance_to_jfk < airportVicinity) | (df.drop_distance_to_jfk < airportVicinity) | (df.pickup_distance_to_lgr < airportVicinity) | (df.drop_distance_to_lgr < airportVicinity) | (df.pickup_distance_to_ewr < airportVicinity) | (df.drop_distance_to_ewr < airportVicinity)\n    \n    return ids","3575b9a2":"start_time = timeit.default_timer()\n\n# Add pickup-dropoff distance feature\ndf_train = addPickDropDistanceFeature(df_train)\ndf_holdout = addPickDropDistanceFeature(df_holdout)\n\nelapsed = timeit.default_timer() - start_time\nelapsed","809f5e6d":"# With 1M datapoints, trip_distance range can be compressed from 0.0-110.83 to 0.0-25.0\n# which would drop just 690 & 11 training & testing datapoints, i.e., \n# worst case impact on prediction accuracy by 0.1% (11\/test_size*100)\n\nbucketsCount = 100\nfeat = 'trip_distance'\n\ndf_train[feat].hist(bins=bucketsCount, figsize = (15,8))\ndf_holdout[feat].hist(bins=bucketsCount, figsize = (15,8))\nplt.yscale('log')\nplt.xlabel(feat)\nplt.ylabel(\"Frequency Log\")","2ead35a3":"#(len(df_train[df_train[feat] > THRESHOLD_TRIP_DISTANCE]), len(df_holdout[df_holdout[feat] > THRESHOLD_TRIP_DISTANCE]))\n\nprint('Old size: %d' % len(df_train))\ndf_train = df_train[df_train.trip_distance < THRESHOLD_TRIP_DISTANCE]\nprint('New size: %d' % len(df_train))","3021c1b2":"def add_datetime_features(df):\n    #Convert to datetime format\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n    \n    df['hour'] = df.pickup_datetime.dt.hour\n    df['day'] = df.pickup_datetime.dt.day\n    df['month'] = df.pickup_datetime.dt.month\n    df['weekday'] = df.pickup_datetime.dt.weekday\n    df['year'] = df.pickup_datetime.dt.year\n    \n    return df\n\nstart_time = timeit.default_timer()\n\ndf_train = add_datetime_features(df_train)\ndf_holdout = add_datetime_features(df_holdout)\n\nelapsed = timeit.default_timer() - start_time\nelapsed","050e4f24":"hour_bins = [-1, 5, 7, 10, 16, 21, 23]\nbin_names = ['late_night', 'morning', 'morning_peak', 'afternoon', 'evening', 'night']\ndf_train['hour_type'] = pd.cut(df_train.hour, bins=hour_bins, labels=bin_names).cat.codes\ndf_holdout['hour_type'] = pd.cut(df_train.hour, bins=hour_bins, labels=bin_names).cat.codes","e3536437":"# Merging both 'train' & 'holdout' for common feature engineering afterwhich 'holdout' data will be extracted\ntrain_len = len(df_train)\ndf_nyc_taxi = pd.concat([df_train, df_holdout], axis=0, ignore_index=False, sort=False)\n#df_nyc_taxi.info()","4bbfa43b":"from sklearn.cluster import DBSCAN\n\nEPS_IN_RADIAN = EPS_IN_KM \/ KMS_PER_RADIAN","a0debf61":"start_time = timeit.default_timer()\n\ndbscan_pick = DBSCAN(eps=EPS_IN_RADIAN, min_samples=MIN_SAMPLES_CLUSTER, algorithm='ball_tree', metric='haversine').fit(np.radians(df_nyc_taxi.loc[:,'pickup_longitude':'pickup_latitude']))\nlabels_pick = dbscan_pick.labels_\n\nelapsed = timeit.default_timer() - start_time\nelapsed","3b76a18a":"# Number of clusters in labels, ignoring noise if present.\nn_clusters_pick = len(set(labels_pick)) - (1 if -1 in labels_pick else 0)\nn_clusters_pick","9832a888":"start_time = timeit.default_timer()\n\ndbscan_drop = DBSCAN(eps=EPS_IN_RADIAN, min_samples=MIN_SAMPLES_CLUSTER, algorithm='ball_tree', metric='haversine').fit(np.radians(df_nyc_taxi.loc[:,'dropoff_longitude':'dropoff_latitude']))\nlabels_drop = dbscan_drop.labels_\n\nelapsed = timeit.default_timer() - start_time\nelapsed","6ca1323b":"# Number of clusters in labels, ignoring noise if present.\nn_clusters_drop = len(set(labels_drop)) - (1 if -1 in labels_drop else 0)\nn_clusters_drop","3ea72f7f":"df_nyc_taxi['density_DBSCAN_pickup'] = labels_pick\ndf_nyc_taxi['density_DBSCAN_dropoff'] = labels_drop\n#df_nyc_taxi['dense_DBSCAN_trips'] = ((labels_pick != -1) & (labels_drop != -1))","bb0b1854":"'''\n# NOTE that our focus of DBSCAN is not to differentiate levels of clusters, i.e., set of connected clusters => hence we are plotting all clusters together\ndf_tmp = df_nyc_taxi.loc[df_nyc_taxi.dense_DBSCAN_trips == 1]\nplt.plot(df_tmp.pickup_longitude, df_tmp.pickup_latitude, 'o')\nplt.xlabel(\"Pickup Longitude\")\nplt.ylabel(\"Pickup Latitude\")\n'''","9a6c3ec7":"'''\nplt.plot(df_tmp.dropoff_longitude, df_tmp.dropoff_latitude, 'o')\nplt.xlabel(\"Dropoff Longitude\")\nplt.ylabel(\"Dropoff Latitude\")\n'''","e87bb1f4":"df_train = df_nyc_taxi.iloc[:train_len, :]\ndf_holdout = df_nyc_taxi.iloc[train_len:, :].iloc[:, df_nyc_taxi.columns != 'fare_amount']\n\n(len(df_train), len(df_holdout))","12290eda":"# Ceiling near-zero fare values to 0.2 to check fare\/dist behavior\ndf_train.loc[df_train.trip_distance < 0.2, 'trip_distance'] = 0.2\n\n(df_train.fare_amount \/ df_train.trip_distance).hist(bins=bucketsCount, figsize = (15,8))\nplt.yscale('log')\nplt.xlabel('trip_rate')\nplt.ylabel(\"Log Frequency\")","0ecc457f":"df_train['trip_rate'] = df_train.apply(\n    (lambda row: (row.fare_amount \/ row.trip_distance)),\n    axis='columns'\n)","49d3afd6":"#len(df_train.loc[df_train.trip_rate > THERSHOLD_TRIP_FARE_RATE])","2012edb0":"#Trying to check if not removing 50+ trip_rate improve the score!\nids = (df_train.trip_rate < THERSHOLD_TRIP_FARE_RATE)\n\nprint('Old size: %d' % len(df_train))\ndf_train = df_train[ids]\nprint('New size: %d' % len(df_train))","3e69d820":"start_time = timeit.default_timer()\n\n# Add airport trips distance features\ndf_train = addAirportDistanceFeatures(df_train)\ndf_holdout = addAirportDistanceFeatures(df_holdout)\n\nelapsed = timeit.default_timer() - start_time\nelapsed","3d486f7d":"#df_train.info()","cfe12364":"# Split training data into Airport & City trips\n\nairportTripsIds = getAirportTrips(df_holdout, RADIUS_VICINITY_AIRPORTS)\ndf_holdout['airport_bound'] = airportTripsIds\n\nairportTripsIds = getAirportTrips(df_train, RADIUS_VICINITY_AIRPORTS)\ndf_train['airport_bound'] = airportTripsIds\ndf_airport_trips = df_train.loc[airportTripsIds]\ndf_city_trips = df_train.loc[-airportTripsIds]","a69c1d68":"# Compare trip_rate for Airport & City trips\npd.DataFrame(data={'Airport Trips' : df_airport_trips.trip_rate, 'City Trips' : df_city_trips.trip_rate}).describe()","7ce7d538":"# Compare trip_rate for good -vs- poorly dense trips\n#pd.DataFrame(data={'Good Density Trips' : df_train.loc[df_train.dense_DBSCAN_trips == 1].trip_rate, 'LOW Density Pickups' : df_train.loc[df_train.dense_DBSCAN_trips == 0].trip_rate}).describe()","ebb322cf":"df_train = df_train.drop(columns = ['pickup_datetime', 'pickup_distance_to_jfk', 'drop_distance_to_jfk', 'pickup_distance_to_lgr', 'drop_distance_to_lgr', 'pickup_distance_to_ewr', 'drop_distance_to_ewr', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'trip_rate', 'hour'])\ndf_train.info()","b6415abd":"y = df_train['fare_amount']\ntrain = df_train.drop(columns=['fare_amount'])\n\nx_train, x_test, y_train, y_test = train_test_split(train, y, random_state=0, test_size=0.01)","4ea97fa5":"#Cross-validation\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth': 8, #Result of tuning with CV\n    'eta':.03, #Result of tuning with CV\n    'subsample': 1, #Result of tuning with CV\n    'colsample_bytree': 0.8, #Result of tuning with CV\n    # Other parameters\n    'objective':'reg:linear',\n    'eval_metric':'rmse',\n    'silent': 1\n}\n\n#Block of code used for hypertuning parameters. Adapt to each round of parameter tuning.\n#Turn off CV in submission\nCV=False\nif CV:\n    dtrain = xgb.DMatrix(train,label=y)\n    gridsearch_params = [\n        (eta)\n        for eta in np.arange(.04, 0.12, .02)\n    ]\n\n    # Define initial best params and RMSE\n    min_rmse = float(\"Inf\")\n    best_params = None\n    for (eta) in gridsearch_params:\n        print(\"CV with eta={} \".format(\n                                 eta))\n\n        # Update our parameters\n        params['eta'] = eta\n\n        # Run CV\n        cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=1000,\n            nfold=3,\n            metrics={'rmse'},\n            early_stopping_rounds=10\n        )\n\n        # Update best RMSE\n        mean_rmse = cv_results['test-rmse-mean'].min()\n        boost_rounds = cv_results['test-rmse-mean'].argmin()\n        print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n        if mean_rmse < min_rmse:\n            min_rmse = mean_rmse\n            best_params = (eta)\n\n    print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))\nelse:\n    #Print final params to use for the model\n    params['silent'] = 0 #Turn on output\n    print(params)","19ce83d7":"def XGBmodel(x_train,x_test,y_train,y_test,params):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params=params,\n                    dtrain=matrix_train,num_boost_round=5000, \n                    early_stopping_rounds=10,evals=[(matrix_test,'test')])\n    return model\n\nmodel = XGBmodel(x_train,x_test,y_train,y_test,params)","70ded6fd":"#Read and preprocess test set\nx_pred = df_holdout.drop(columns = ['pickup_datetime', 'pickup_distance_to_jfk', 'drop_distance_to_jfk', 'pickup_distance_to_lgr', 'drop_distance_to_lgr', 'pickup_distance_to_ewr', 'drop_distance_to_ewr', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'hour'])\n\n#Predict from test set\nprediction = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)","93918ef2":"len(test_key)","787140b7":"#Create submission file\nsubmission = pd.DataFrame({\n        \"key\": test_key,\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('taxi_fare_submission.csv',index=False)\nsubmission.head()","f79fa056":"## References\n\n1. [Density based Clustering by Manojit Nandi, Domino Data Lab](https:\/\/blog.dominodatalab.com\/topology-and-density-based-clustering\/)\n2. [DBSCAN Wiki](https:\/\/en.wikipedia.org\/wiki\/DBSCAN)\n3. [Efficient Large Scale Clustering based on Data Partitioning](https:\/\/arxiv.org\/pdf\/1704.03421.pdf)\n4. [Benchmarking Performance and Scaling of Python Clustering Algorithms](https:\/\/hdbscan.readthedocs.io\/en\/latest\/performance_and_scalability.html)\n5. [Kaggle kernel for Data Exploration](https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration\/notebook)\n6. [Kaggle kernel for XGBoost](https:\/\/www.kaggle.com\/gunbl4d3\/xgboost-ing-taxi-fares)","9a502a20":"## NYC Taxi Fare Prediction: EDA, DBSCAN GeoSpatial Clustering & Regression Modeling\n#### Sunil Kumar\n\n","ac4a6b93":"## Decision Summary\n\nMy primary interest in solving this Playground Competition was to closely experience the challenes of working with Latitude & Longitude data, introduce pickups & dropoffs desnity feature and asses its impact on NYC Taxi Fare Prediction. The geospatial clustered density of pickup & dropoff has been estimated using DBSCAN algorithm with predefined maximum cluster radius 0.5 km (EPS_IN_KM) and minimum pickup\/dropoff count 500 (MIN_SAMPLES_CLUSTER).\n\nThe purpose of choosing DBSCAN [ref. 1] was to identify low-density pickups\/dropoffs locations which DBSCAN calls Outliers. Note that the other popular clustering algorithm K-Means determines k (predefined number) centroids which would not serve the purpose due to irregular geospatial distribution. Prior to hitting upon Clustering & specifically DBSCAN, I explored the options of geospatial 2-D binning (tiles, hexbin, etc.) but they are more useful for visualization and not so suitable for my problem statement & sparse dataset.\n\nThe 'train' data pruning has been performed to ensure that original + engineered features' {domain} are not compromised in 'test' - refer to comments at the beginning of code blocks in Data Cleaning & Feature Engineering sections. Minimal data clearning [ref. 5] has been done to get a descent working dataset.\n\n## Useful Insights on DBSCAN\n\n* Refer to [ref. 1] for consice explanation.\n* Refer to [ref. 2] for pros & cons of DBSCAN. NOTE that most of its cons are not applicable to geospatial use cases.\n* Though DBSCAN has a worst-case runtime comlexity of O(n\u00b2), its database-oriented range-query formulation of DBSCAN allows for index acceleration for better performance upto O(n log n).\n* Dealing with large geospatial dataset is quite a challenge, hence realistic datasets would necessarily need Big Data implementation of this algorithm [ref. 3].\n* Sklearnc.luster.DBSCAN performance compares very well with repsect to other alternatives [ref. 4]","0cb667e0":"## Naive Data Cleaning","d46c6f5b":"## Global Stuff","42992f56":"## EDA, Feature Engineering & Data Cleaning"}}