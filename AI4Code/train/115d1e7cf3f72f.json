{"cell_type":{"38fe4984":"code","89500e43":"code","f2a5ab46":"code","caf35c73":"code","8920ee15":"code","fd3ce5e6":"code","f7b6d3ff":"code","890c6d47":"code","035a280f":"code","53b88f2a":"code","eb7cc2be":"code","f0d07aa6":"code","3c2d2f0f":"code","e683303e":"code","6b015ed5":"code","a055af8a":"code","09cac984":"code","c886bc4f":"code","1754e412":"code","25a6d512":"code","365f1e12":"code","a32bad1c":"code","53f585ba":"code","cfde08a3":"code","bb81a24e":"code","e4062ccb":"code","e5e3bc24":"code","4bca363b":"code","e7b0af42":"code","99adf67e":"code","b2bb6bcc":"code","c11b3569":"code","20cc0e9f":"code","a25943cf":"code","2d2e632f":"code","dc561102":"code","56b8dcab":"code","bdfb5ab8":"code","ab24424c":"code","4a02969e":"code","734ba17b":"code","94decdc4":"code","8e999100":"code","aab4352d":"code","801f6132":"code","089a0380":"code","854941a3":"code","28787224":"code","5a9de4be":"code","32e9e885":"code","4bf53465":"code","ad423233":"code","3c32cd47":"code","7451e16e":"code","ba82bcee":"code","6ac1511c":"code","15748885":"code","92b2ab9b":"code","9bd0946e":"code","5e78cdd0":"code","b6db69a7":"code","833f3de9":"code","39426808":"code","bf777a5f":"code","62d5d476":"code","c0a8c5b7":"code","0bf55a67":"code","fc00a0ac":"code","860105b7":"code","6c00a0c9":"code","769fe3cf":"code","172539a2":"code","51a9e6b8":"code","14091878":"code","1b1edf38":"code","8e252035":"code","08691538":"code","f544bb16":"code","b68da5b3":"code","df56c014":"code","819840d8":"code","23d2284f":"code","1f6c3f42":"code","48c9b6f8":"code","87618e0f":"markdown","2a3fa63b":"markdown","30ddb582":"markdown","40e41af3":"markdown","2d229dcd":"markdown","c9650f78":"markdown","d48cbd13":"markdown","0d2cd27a":"markdown","2ddf6c4a":"markdown","b385adb4":"markdown","2986e2ff":"markdown"},"source":{"38fe4984":"import os\nos.makedirs(\"\/kaggle\/working\/dataset\")","89500e43":"# Necessary Dependencies\nimport numpy as np \nimport pandas as pd \nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom itertools import chain\nfrom datetime import datetime\nimport statistics\n\n# Disease Names \ndisease_labels = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', 'Effusion','Pneumonia', 'Pleural_Thickening',\n'Cardiomegaly', 'Nodule', 'Mass', 'Hernia']\n\n# Load Stanford Images Distribution Files\n\n\nlabels_train_val = pd.read_csv('\/kaggle\/input\/txt-file\/train_val_list.txt')\nlabels_train_val.columns = ['Image_Index']\n\nlabels_test = pd.read_csv('\/kaggle\/input\/txt-file\/test_list.txt')\nlabels_test.columns = ['Image_Index']\n\n\nprint(labels_test['Image_Index'])\n\n\n\n\n# NIH Dataset Labels CSV File \n\nlabels_df = pd.read_csv('\/kaggle\/input\/data\/Data_Entry_2017.csv')\n\nlabels_df.columns = ['Image_Index', 'Finding_Labels', 'Follow_Up_#', 'Patient_ID',\n                  'Patient_Age', 'Patient_Gender', 'View_Position',\n                  'Original_Image_Width', 'Original_Image_Height',\n                  'Original_Image_Pixel_Spacing_X',\n                  'Original_Image_Pixel_Spacing_Y', 'dfd']\n\n","f2a5ab46":"labels_df['Finding_Labels'] = labels_df['Finding_Labels'].map(lambda x: x.replace('No Finding', str(None)))\nlabels_df['Finding_Labels'] = labels_df['Finding_Labels'].replace('None', None)\n# Binarizes Each Disease Class in their own column\nfrom tqdm import tqdm\n\nfor diseases in tqdm(disease_labels): #TQDM is a progress bar setting\n    labels_df[diseases] = labels_df['Finding_Labels'].map(lambda result: 1 if diseases in result else 0)\n#     print(labels_df['Finding_Labels'].map(lambda result: 1 if diseases in result else 0))\n\ntrain_val_merge = pd.merge(left=labels_train_val, right=labels_df, left_on='Image_Index', right_on='Image_Index')\n\ntest_merge = pd.merge(left=labels_test, right=labels_df, left_on='Image_Index', right_on='Image_Index')\n\n\n\nprint(train_val_merge,test_merge)\n\n\n# Splitting Finding Labels\ntrain_val_merge['Finding_Labels'] = train_val_merge['Finding_Labels'].apply(lambda s: [l for l in str(s).split('|')])\n\ntest_merge['Finding_Labels'] = test_merge['Finding_Labels'].apply(lambda s: [l for l in str(s).split('|')])\n\nprint(test_merge)\n\n\n\nnum_glob = glob('\/kaggle\/input\/data\/*\/images\/*.png')\nimg_path = {os.path.basename(x): x for x in num_glob}\n\n\ntrain_val_merge['Paths'] = train_val_merge['Image_Index'].map(img_path.get)\ntest_merge['Paths'] = test_merge['Image_Index'].map(img_path.get)\n\nfrom tqdm import tqdm\nimport os,cv2\n\n\n# No Overlap in patients between the Train and Validation Data Sets\npatients = np.unique(train_val_merge['Patient_ID'])\ntest_patients = np.unique(test_merge['Patient_ID'])\n\nprint('Number of Patients Between Train-Val Overall: ', len(patients))\nprint('Number of Patients Between Train-Val Overall: ', len(test_patients))\n\n\n# Train-Validation Split \ntrain_df, val_df = train_test_split(patients,\n                                   test_size = 0.0669,\n                                   random_state = 2019,\n                                    shuffle= True)  \n\n\n","caf35c73":"labels_df","8920ee15":"print('No. of Unique Patients in Train dataset : ',len(train_df))\ntrain_df = train_val_merge[train_val_merge['Patient_ID'].isin(train_df)]\nprint('Training Dataframe   : ', train_df.shape[0],' images',)\n\nprint('\\nNo. of Unique Patients in Validtion dataset : ',len(val_df))\nval_df = train_val_merge[train_val_merge['Patient_ID'].isin(val_df)]\nprint('Validation Dataframe   : ', val_df.shape[0],' images')\n\nprint('\\nNo. of Unique Patients in Testing dataset : ',len(test_patients))\ntest_df = test_merge[test_merge['Patient_ID'].isin(test_patients)]\nprint('Testing Dataframe   : ', test_df.shape[0],' images')\n\n\ntrain_df1 = train_df.drop(['Finding_Labels','Follow_Up_#','Patient_Age','Patient_Gender','View_Position','Original_Image_Width','Original_Image_Height','Original_Image_Pixel_Spacing_X','Original_Image_Pixel_Spacing_Y','Paths'],axis=1)\ntrain_df1.to_csv(\"train.csv\")\n\nval_df1= val_df.drop(['Finding_Labels','Follow_Up_#','Patient_Age','Patient_Gender','View_Position','Original_Image_Width','Original_Image_Height','Original_Image_Pixel_Spacing_X','Original_Image_Pixel_Spacing_Y','Paths'],axis=1)\nval_df1.to_csv(\"valid.csv\")\n\ntest_df1= test_df.drop(['Finding_Labels','Follow_Up_#','Patient_Age','Patient_Gender','View_Position','Original_Image_Width','Original_Image_Height','Original_Image_Pixel_Spacing_X','Original_Image_Pixel_Spacing_Y','Paths'],axis=1)\ntest_df1.to_csv(\"test.csv\")\n\n\n#\n# for value in tqdm(img_path.values()):\n#     I = cv2.cvtColor(cv2.imread(value),cv2.COLOR_BGR2RGB)\n#     I = cv2.resize(I,(320,320))\n#     filename = os.path.splitext(os.path.basename(value))[0]\n# #     if not os.path.exists(\"\/kaggle\/working\/data\/\"+filename+\".jpg\"):\n#     cv2.imwrite(\"\/kaggle\/working\/dataset\/\"+filename+\".png\",I)\n# #     print(value)\n","fd3ce5e6":"train_df1","f7b6d3ff":"# No Overlap in patients between the Train and Validation Data Sets\npatients = np.unique(train_val_merge['Patient_ID'])\ntest_patients = np.unique(test_merge['Patient_ID'])\n\nprint('Number of Patients Between Train-Val Overall: ', len(patients))\nprint('Number of Patients Between Train-Val Overall: ', len(test_patients))","890c6d47":"# Train-Validation Split \ntrain_df, val_df = train_test_split(patients,\n                                   test_size = 0.0669,\n                                   random_state = 2019,\n                                    shuffle= True)  \n\n\nprint('No. of Unique Patients in Train dataset : ',len(train_df))\ntrain_df = train_val_merge[train_val_merge['Patient_ID'].isin(train_df)]\nprint('Training Dataframe   : ', train_df.shape[0],' images')\n\nprint('\\nNo. of Unique Patients in Validtion dataset : ',len(val_df))\nval_df = train_val_merge[train_val_merge['Patient_ID'].isin(val_df)]\nprint('Validation Dataframe   : ', val_df.shape[0],' images')\n\nprint('\\nNo. of Unique Patients in Testing dataset : ',len(test_patients))\ntest_df = test_merge[test_merge['Patient_ID'].isin(test_patients)]\nprint('Testing Dataframe   : ', test_df.shape[0],' images')","035a280f":"X_train = train_df['Paths']\ny_train = train_df['Finding_Labels']\n\nX_val = val_df['Paths']\ny_val = val_df['Finding_Labels']\n\nX_test = test_df['Paths']\ny_test = test_df['Finding_Labels']","53b88f2a":"y_train","eb7cc2be":"# Binarizing Labels \nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nprint(\"Labels - \")\nmlb = MultiLabelBinarizer()\nmlb.fit(y_train)","f0d07aa6":"X_train = train_df['Paths']\ny_train = train_df['Finding_Labels']\n\nX_val = val_df['Paths']\ny_val = val_df['Finding_Labels']\n\nX_test = test_df['Paths']\ny_test = test_df['Finding_Labels']","3c2d2f0f":"y_train_bin = mlb.transform(y_train)\ny_val_bin = mlb.transform(y_val)\ny_test_bin = mlb.transform(y_test)","e683303e":"# Necessary Dependencies\nimport numpy as np \nimport pandas as pd \nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom itertools import chain\nfrom datetime import datetime\nimport statistics\n\n# Disease Names \ndisease_labels = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', 'Effusion','Pneumonia', 'Pleural_Thickening',\n'Cardiomegaly', 'Nodule', 'Mass', 'Hernia']\n\n# Load Stanford Images Distribution Files\n\n\nlabels_train_val = pd.read_csv('\/kaggle\/input\/txt-file\/train_val_list.txt')\nlabels_train_val.columns = ['Image_Index']\n\nlabels_test = pd.read_csv('\/kaggle\/input\/txt-file\/test_list.txt')\nlabels_test.columns = ['Image_Index']\n\n\nprint(labels_test['Image_Index'])\n\n\n\n\n# NIH Dataset Labels CSV File \n\nlabels_df = pd.read_csv('\/kaggle\/input\/data\/Data_Entry_2017.csv')\n\nlabels_df.columns = ['Image_Index', 'Finding_Labels', 'Follow_Up_#', 'Patient_ID',\n                  'Patient_Age', 'Patient_Gender', 'View_Position',\n                  'Original_Image_Width', 'Original_Image_Height',\n                  'Original_Image_Pixel_Spacing_X',\n                  'Original_Image_Pixel_Spacing_Y', 'dfd']\n\n\n\n\n\nlabels_df['Finding_Labels'] = labels_df['Finding_Labels'].map(lambda x: x.replace('No Finding', str(None)))\nlabels_df['Finding_Labels'] = labels_df['Finding_Labels'].replace('None', None)\n# Binarizes Each Disease Class in their own column\nfrom tqdm import tqdm\n\nfor diseases in tqdm(disease_labels): #TQDM is a progress bar setting\n    labels_df[diseases] = labels_df['Finding_Labels'].map(lambda result: 1 if diseases in result else 0)\n#     print(labels_df['Finding_Labels'].map(lambda result: 1 if diseases in result else 0))\n\ntrain_val_merge = pd.merge(left=labels_train_val, right=labels_df, left_on='Image_Index', right_on='Image_Index')\n\ntest_merge = pd.merge(left=labels_test, right=labels_df, left_on='Image_Index', right_on='Image_Index')\n\n\n\nprint(train_val_merge,test_merge)\n\n\n# Splitting Finding Labels\ntrain_val_merge['Finding_Labels'] = train_val_merge['Finding_Labels'].apply(lambda s: [l for l in str(s).split('|')])\n\ntest_merge['Finding_Labels'] = test_merge['Finding_Labels'].apply(lambda s: [l for l in str(s).split('|')])\n\nprint(test_merge)\n\n\n\nnum_glob = glob('\/kaggle\/input\/data\/*\/images\/*.png')\nimg_path = {os.path.basename(x): x for x in num_glob}\n\n\ntrain_val_merge['Paths'] = train_val_merge['Image_Index'].map(img_path.get)\ntest_merge['Paths'] = test_merge['Image_Index'].map(img_path.get)\n\nfrom tqdm import tqdm\nimport os,cv2\n\n\n# No Overlap in patients between the Train and Validation Data Sets\npatients = np.unique(train_val_merge['Patient_ID'])\ntest_patients = np.unique(test_merge['Patient_ID'])\n\nprint('Number of Patients Between Train-Val Overall: ', len(patients))\nprint('Number of Patients Between Train-Val Overall: ', len(test_patients))\n\n\n# Train-Validation Split \ntrain_df, val_df = train_test_split(patients,\n                                   test_size = 0.0669,\n                                   random_state = 2019,\n                                    shuffle= True)  \n\n\n\n\n\nprint('No. of Unique Patients in Train dataset : ',len(train_df))\ntrain_df = train_val_merge[train_val_merge['Patient_ID'].isin(train_df)]\nprint('Training Dataframe   : ', train_df.shape[0],' images',)\n\ntrain_df.to_csv(\"train_f.csv\")\n\nprint('\\nNo. of Unique Patients in Validtion dataset : ',len(val_df))\nval_df = train_val_merge[train_val_merge['Patient_ID'].isin(val_df)]\nprint('Validation Dataframe   : ', val_df.shape[0],' images')\n\nprint('\\nNo. of Unique Patients in Testing dataset : ',len(test_patients))\ntest_df = test_merge[test_merge['Patient_ID'].isin(test_patients)]\nprint('Testing Dataframe   : ', test_df.shape[0],' images')\n\n\n# train_df1 = train_df.drop(['Finding_Labels','Follow_Up_#','Patient_Age','Patient_Gender','View_Position','Original_Image_Width','Original_Image_Height','Original_Image_Pixel_Spacing_X','Original_Image_Pixel_Spacing_Y','Paths'],axis=1)\n# train_df1.to_csv(\"train.csv\")\n\n# val_df1= val_df.drop(['Finding_Labels','Follow_Up_#','Patient_Age','Patient_Gender','View_Position','Original_Image_Width','Original_Image_Height','Original_Image_Pixel_Spacing_X','Original_Image_Pixel_Spacing_Y','Paths'],axis=1)\n# val_df1.to_csv(\"valid.csv\")\n\n# test_df1= test_df.drop(['Finding_Labels','Follow_Up_#','Patient_Age','Patient_Gender','View_Position','Original_Image_Width','Original_Image_Height','Original_Image_Pixel_Spacing_X','Original_Image_Pixel_Spacing_Y','Paths'],axis=1)\n# test_df1.to_csv(\"test.csv\")\n\n\n\nX_train = train_df['Paths']\ny_train = train_df['Finding_Labels']\n\nX_val = val_df['Paths']\ny_val = val_df['Finding_Labels']\n\nX_test = test_df['Paths']\ny_test = test_df['Finding_Labels']\n\n\n# Binarizing Labels \nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nprint(\"Labels - \")\nmlb = MultiLabelBinarizer()\nmlb.fit(y_train)\n\n\ny_train_bin = mlb.transform(y_train)\ny_val_bin = mlb.transform(y_val)\ny_test_bin = mlb.transform(y_test)\n\ndef data_generator_variable_input_img(train_df,y_train_bin,batch_size = 1):\n    \n    while True:\n        \n        \n        np.random.seed(2020)\n        \n        img_list = train_df[\"Paths\"].values\n        \n        indx = np.random.randint(len(img_list),size=batch_size)\n        \n        label = y_train_bin\n        \n        Y = []\n        \n        X  = []\n\n        for i in indx:\n\n            I = cv2.cvtColor(cv2.imread(img_list[i]),cv2.COLOR_BGR2RGB)\n            label1 = list(label[i])\n            i224 = cv2.resize(I,(224,224))   \n            X.append(i224)\n            Y.append(label1)\n\n\n        yield([np.array(X)],np.array(Y).astype(\"float32\"))\n                           \n                   \n# Section of Code written by brucechou1983 - https:\/\/github.com\/brucechou1983\/CheXNet-Keras\n# I have no experience with class weighting, brucechou1983 provided a very thorough explanation of the topic with example code\nCLASS_NAMES = disease_labels\ndef get_class_weights(total_counts, class_positive_counts, multiply):\n    \"\"\"\n    Calculate class_weight used in training\n    Arguments:\n    total_counts - int\n    class_positive_counts - dict of int, ex: {\"Effusion\": 300, \"Infiltration\": 500 ...}\n    multiply - int, positve weighting multiply\n    use_class_balancing - boolean \n    Returns:\n    class_weight - dict of dict, ex: {\"Effusion\": { 0: 0.01, 1: 0.99 }, ... }\n    \"\"\"\n    def get_single_class_weight(pos_counts, total_counts):\n        denominator = (total_counts - pos_counts) * multiply + pos_counts\n        return {\n            0: pos_counts \/ denominator,\n            1: (denominator - pos_counts) \/ denominator,\n        }\n\n    class_names = list(class_positive_counts.keys())\n    label_counts = np.array(list(class_positive_counts.values()))\n    class_weights = []\n    for i, class_name in enumerate(class_names):\n        class_weights.append(get_single_class_weight(label_counts[i], total_counts))\n\n    return class_weights\n\ndef get_sample_counts(dataset, class_names):\n    \"\"\"\n    Get total and class-wise positive sample count of a dataset\n    Arguments:\n    output_dir - str, folder of dataset.csv\n    dataset - str, train|dev|test\n    class_names - list of str, target classes\n    Returns:\n    total_count - int\n    class_positive_counts - dict of int, ex: {\"Effusion\": 300, \"Infiltration\": 500 ...}\n    \"\"\"\n    df = pd.read_csv(\"train_f.csv\")\n    total_count = df.shape[0]\n    labels = df[class_names].as_matrix()\n    positive_counts = np.sum(labels, axis=0)\n    class_positive_counts = dict(zip(class_names, positive_counts))\n    return total_count, class_positive_counts\n\nnewfds = 'newfds'\ntrain_counts, train_pos_counts = get_sample_counts(\"train\", CLASS_NAMES)\nclass_weights = get_class_weights(train_counts, train_pos_counts, multiply=1)\n\n\n","6b015ed5":"f = data_generator_variable_input_img(train_df,y_train_bin,batch_size = 1)\nf.__next__()","a055af8a":"# DenseNet Dependencies\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.activations import sigmoid\nfrom tensorflow.keras.layers import Dense,Conv2D, Flatten, Dropout, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.metrics import Accuracy, Precision, Recall, AUC, BinaryAccuracy, FalsePositives\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\nfrom keras import backend as K\n\n\n\n# Hyperparameters\n\nIMG_SHAPE = (224,224,3)\n\nEPOCHS = 100\n\nSTEPS_PER_EPOCH = 10\n\nOPTIMIZER = Adam(learning_rate=0.001, ####### Modified\n                 beta_1=0.9,\n                 beta_2=0.999)\n\nLOSS = BinaryCrossentropy() # Not un-weighted \n\nMETRICS = ['BinaryAccuracy']\n\nfrom tensorflow.keras.optimizers import Adam,RMSprop\nfrom tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n\nfrom tensorflow.keras.models import load_model\n\ncallback = [\n        ReduceLROnPlateau(patience=5, verbose=1),\n        ModelCheckpoint(\"chest_xray_model_v2_weights.h5\",\n                        save_best_only=True,\n                        save_weights_only=False),ModelCheckpoint(\"chest_xray_model_v2.h5\",\n                        save_best_only=True,\n                        save_weights_only=True)]\n\n# with tf.device('\/GPU:0'):\n\nbase_model = DenseNet121(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\n                                               #pooling=\"avg\")\n\nbase_model.trainable = True\n\nx = base_model.output\n\nx = GlobalAveragePooling2D()(x)\npredictions = Dense(14, activation='sigmoid',name='Final')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n#model.summary()\n\n#latest = tf.train.latest_checkpoint(pre_weights)\n#print('Loading Weights from file: ', latest)\n#weights_DIR = '\/kaggle\/input\/bruce-weights\/brucechou1983_CheXNet_Keras_0.3.0_weights.h5'\n#model.load_weights(weights_DIR)\n\nmodel.compile(loss = LOSS,\n              optimizer=OPTIMIZER,\n              metrics=METRICS\n                             )\n\n\n# history = model.fit(\n#                     train_ds,\n#                     steps_per_epoch = STEPS_PER_EPOCH,\n#                     validation_data= create_dataset(X_val, y_val_bin.astype(np.float32)),\n#                     validation_steps = STEPS_PER_EPOCH \/ 10, \n#                     epochs=EPOCHS,\n#                     class_weight = class_weights,\n#                     callbacks=callback)len(y_train_bin)\/\/1\n\nhistory = model.fit_generator(data_generator_variable_input_img(train_df,y_train_bin,batch_size = 32),validation_data=data_generator_variable_input_img(val_df,y_val_bin,batch_size = 32),steps_per_epoch = 100,epochs=45 ,validation_steps=25,callbacks = callback,class_weight = class_weights)","09cac984":"F = data_generator_variable_input_img(train_df,y_train_bin,batch_size = 1)\nF.__next__()[1].shape","c886bc4f":"# Necessary Dependencies\nimport numpy as np \nimport pandas as pd \nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom itertools import chain\nfrom datetime import datetime\nimport statistics\n\nprint('Started')","1754e412":"print(os.listdir(\"\/kaggle\/input\/txt-file\"))","25a6d512":"# Establish Directories \n\nif not os.path.exists('logs'):\n    os.makedirs('logs')\n    \nif not os.path.exists('callbacks'):\n    os.makedirs('callbacks')\n    \nCALLBACKS_DIR = '\/kaggle\/output\/callbacks'","365f1e12":"# Disease Names \ndisease_labels = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening',\n'Cardiomegaly', 'Nodule', 'Mass', 'Hernia']","a32bad1c":"# Load Stanford Images Distribution Files\n\nlabels_train_val = pd.read_csv('\/kaggle\/input\/txt-file\/train_val_list.txt')\nlabels_train_val.columns = ['Image_Index']\n\nlabels_test = pd.read_csv('\/kaggle\/input\/txt-file\/test_list.txt')\nlabels_test.columns = ['Image_Index']\n\n\nprint(labels_train_val,labels_test)\nprint(len(labels_train_val),len(labels_test))","53f585ba":"# NIH Dataset Labels CSV File \n\nlabels_df = pd.read_csv('\/kaggle\/input\/data\/Data_Entry_2017.csv')\n\nlabels_df.columns = ['Image_Index', 'Finding_Labels', 'Follow_Up_#', 'Patient_ID',\n                  'Patient_Age', 'Patient_Gender', 'View_Position',\n                  'Original_Image_Width', 'Original_Image_Height',\n                  'Original_Image_Pixel_Spacing_X',\n                  'Original_Image_Pixel_Spacing_Y', 'dfd']","cfde08a3":"print(labels_df)","bb81a24e":"labels_df['Finding_Labels'] = labels_df['Finding_Labels'].map(lambda x: x.replace('No Finding', str(None)))","e4062ccb":"labels_df['Finding_Labels'] = labels_df['Finding_Labels'].replace('None', None)","e5e3bc24":"# Binarizes Each Disease Class in their own column\nfrom tqdm import tqdm\n\nfor diseases in tqdm(disease_labels): #TQDM is a progress bar setting\n    labels_df[diseases] = labels_df['Finding_Labels'].map(lambda result: 1 if diseases in result else 0)\n#     print(labels_df['Finding_Labels'].map(lambda result: 1 if diseases in result else 0))","4bca363b":"labels_df[diseases]","e7b0af42":"train_val_merge = pd.merge(left=labels_train_val, right=labels_df, left_on='Image_Index', right_on='Image_Index')\n\ntest_merge = pd.merge(left=labels_test, right=labels_df, left_on='Image_Index', right_on='Image_Index')\n\n\n\nprint(train_val_merge,test_merge)","99adf67e":"# Splitting Finding Labels\ntrain_val_merge['Finding_Labels'] = train_val_merge['Finding_Labels'].apply(lambda s: [l for l in str(s).split('|')])\n\ntest_merge['Finding_Labels'] = test_merge['Finding_Labels'].apply(lambda s: [l for l in str(s).split('|')])\n\nprint(test_merge)","b2bb6bcc":"# Mapping Images\nnum_glob = glob('\/kaggle\/input\/data\/*\/images\/*.png')\nimg_path = {os.path.basename(x): x for x in num_glob}\n\ntrain_val_merge['Paths'] = train_val_merge['Image_Index'].map(img_path.get)\ntest_merge['Paths'] = test_merge['Image_Index'].map(img_path.get)","c11b3569":"print(len(num_glob))","20cc0e9f":"# Delete No Finding Class\n\n#train_val_merge = train_val_merge.drop(train_val_merge[train_val_merge.Del == 1].index)\n#test_merge = test_merge.drop(test_merge[test_merge.Del == 1].index)","a25943cf":"# No Overlap in patients between the Train and Validation Data Sets\npatients = np.unique(train_val_merge['Patient_ID'])\ntest_patients = np.unique(test_merge['Patient_ID'])\n\nprint('Number of Patients Between Train-Val Overall: ', len(patients))\nprint('Number of Patients Between Train-Val Overall: ', len(test_patients))","2d2e632f":"# Train-Validation Split \ntrain_df, val_df = train_test_split(patients,\n                                   test_size = 0.0669,\n                                   random_state = 2019,\n                                    shuffle= True)  \n\n\nprint('No. of Unique Patients in Train dataset : ',len(train_df))\ntrain_df = train_val_merge[train_val_merge['Patient_ID'].isin(train_df)]\nprint('Training Dataframe   : ', train_df.shape[0],' images')\n\nprint('\\nNo. of Unique Patients in Validtion dataset : ',len(val_df))\nval_df = train_val_merge[train_val_merge['Patient_ID'].isin(val_df)]\nprint('Validation Dataframe   : ', val_df.shape[0],' images')\n\nprint('\\nNo. of Unique Patients in Testing dataset : ',len(test_patients))\ntest_df = test_merge[test_merge['Patient_ID'].isin(test_patients)]\nprint('Testing Dataframe   : ', test_df.shape[0],' images')","dc561102":"X_train = train_df['Paths']\ny_train = train_df['Finding_Labels']\n\nX_val = val_df['Paths']\ny_val = val_df['Finding_Labels']\n\nX_test = test_df['Paths']\ny_test = test_df['Finding_Labels']","56b8dcab":"print(y_train)","bdfb5ab8":"XX_train = []\nfor i in X_train[:]:\n    XX_train.append(i)\nprint(len(XX_train))\n\nX_train = XX_train","ab24424c":"# Binarizing Labels \nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nprint(\"Labels - \")\nmlb = MultiLabelBinarizer()\nmlb.fit(y_train)\n\nN_LABELS = len(mlb.classes_)\nprint(N_LABELS,mlb.classes_)\nfor (i, label) in enumerate(mlb.classes_):\n    print(\"{}. {}\".format(i, label))","4a02969e":"train_df.to_csv(\"train_f.csv\")","734ba17b":"IMG_IND = 224\nBATCH_SIZE = 32","94decdc4":"# Print a link to Tensorflow Data \nimport tensorflow as tf\n\n# Checkout Tensorflow.Data for more elaboration\ndef parse_function(filename, label):\n\n    # Read an image from a file\n    image_string = tf.io.read_file(filename)\n    # Decode it into a dense vector\n    image_decoded = tf.image.decode_png(image_string, channels=3) # channels=3\n    # Resize it to fixed shape\n    image_resized = tf.image.resize(image_decoded, [IMG_IND, IMG_IND])\n    # Normalize it from [0, 255] to [0.0, 1.0]\n    image_normalized = image_resized \/ 255.0\n    \n    return image_normalized, label","8e999100":"AUTOTUNE = tf.data.experimental.AUTOTUNE \nSHUFFLE_BUFFER_SIZE = 1280 ","aab4352d":"def create_dataset(filenames, labels):\n\n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n    \n\n    # Batch the data for multiple steps\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n    \n    return dataset","801f6132":"# os.makedirs(\"\/kaggle\/working\/\"+\"images_001\"+\"\/\"+\"images\")","089a0380":"y_train_bin = mlb.transform(y_train)","854941a3":"def data_generator_variable_input_img(img_list,y_train_bin,batch_size = 1):\n    \n    while True:\n        \n        Y = []\n        \n        np.random.seed(2019)\n        paths = np.random.choice(img_list,batch_size)\n        np.random.seed(2019)\n        labels =  np.random.choice(y_train_bin,batch_size)\n        \n        for img in paths:\n\n\n            I = cv2.cvtColor(cv2.imread(img),cv2.COLOR_BGR2RGB)\n            \n#             i128 = cv2.resize(I,(128,128))\n            \n            i512 = cv2.resize(I,(512,512))\n        \n            im = Image.fromarray(i512)\n            # rotating a image 90 deg counter clockwise \n            im1 = im.rotate(90) \n            im2 = im.rotate(180) \n            im3 = im.rotate(270) \n#             i608 = cv2.resize(I,(608,608))\n            \n            label1 = int(LABEL_DICT[img.split(\"\/\")[-2]])\n            \n            label = keras.utils.to_categorical(label1, num_classes=6, dtype='float32')\n            \n#             label = keras.utils.to_categorical(label1, num_classes=6, dtype='float32')\n#             label = np.zeros((1,6))\n#             j = label1+1\n#             label[0,:j] = 1\n            \n#             I128.append(i128)\n#             I256.append(i256)\n\n            I512_0.append(i512)\n            I512_90.append(np.array(im1))\n            I512_180.append(np.array(im2))\n            I512_270.append(np.array(im3))\n        \n\n            Y.append(label)\n            \n#             print(label)\n            \n            y_smooth = smooth_labeld_array(np.array(Y),num_classes = 6)\n\n        yield([np.array(I512_0)\/255.,np.array(I512_90)\/255.,np.array(I512_180)\/255.,np.array(I512_270)  \/255.],y_smooth.astype(\"float32\"))\n                           \n                 ","28787224":"y_train_bin","5a9de4be":"# transform the targets of the training and test sets\ny_train_bin = mlb.transform(y_train)\n# for k in range(len(y_train_bin)):\n    \n    \n#     path_name = X_train[k].split(\"\/\")[-3]\n#     foldr_name = X_train[k].split(\"\/\")[-2]\n#     file_name = X_train[k].split(\"\/\")[-1]\n    \n#     if not os.path.exists(\"\/kaggle\/working\/\"+str(path_name)):\n#         os.makedirs(\"\/kaggle\/working\/\"+str(path_name))\n#     if not os.path.exists(\"\/kaggle\/working\/\"+str(path_name)+\"\/\"+str(foldr_name)):\n#         os.makedirs(\"\/kaggle\/working\/\"+str(path_name)+\"\/\"+str(foldr_name))\n# #     print(path_name)\n#     if os.path.exists(\"\/kaggle\/working\/\"+str(path_name)) and os.path.exists(\"\/kaggle\/working\/\"+str(path_name)+\"\/\"+str(foldr_name)) :\n#         im = cv2.imread(X_train[k])\n#         cv2.imwrite(\"\/kaggle\/working\/\"+str(path_name)+\"\/\"+str(foldr_name)+\"\/\"+file_name,im)\n#         print(y_train_bin[k],X_train[k])\n        \n#     cv2.imwrite(\"\/kaggle\/working\/save_img\/x.jpg\"\n# print(y_train_bin)\ny_val_bin = mlb.transform(y_val)\ny_test_bin = mlb.transform(y_test)","32e9e885":"import glob\n\nprint(len(glob.glob(\"\/kaggle\/input\/data\/*\/*\/*\/*.jpg\")))","4bf53465":"train_ds = create_dataset(X_train, y_train_bin.astype(np.float32))\nval_ds = create_dataset(X_val, y_val_bin.astype(np.float32))\ntest_ds = create_dataset(X_test, y_test_bin.astype(np.float32))","ad423233":"# Load in Stanford Distribution charts of images ","3c32cd47":"# Print # of diseases in each Train-Val-Test\n### Turn this into a function ###\n\nprint('# of Diseases in each Class - Training')\nfor i in disease_labels:\n    print(i,int(train_df[i].sum()))\n\nprint('\\n')\n\nprint('# of Diseases in each Class - Validation')\nfor i in disease_labels:\n    print(i,int(val_df[i].sum()))\n\nprint('\\n')\n\nprint('# of Diseases in each Class - Testing')\nfor i in disease_labels:\n    print(i,int(test_df[i].sum()))\n\nprint('\\n')","7451e16e":"# Visualize a couple of images \nimport matplotlib.style as style\nfrom PIL import Image\n\n\nnobs = 6 # Maximum number of images to display\nncols = 3 # Number of columns in display\nnrows = nobs\/\/ncols # Number of rows in display\n'''\nstyle.use(\"default\")\nplt.figure(figsize=(8,4*nrows))\nfor i in range(nrows*ncols):\n    ax = plt.subplot(nrows, ncols, i+1)\n    plt.imshow(Image.open(X_train[i]))\n    plt.title(y_train[i], size=10)\n    plt.axis('off')\n    '''","ba82bcee":"# Show Original vs Downsampled Images","6ac1511c":"# Show Random Horizontal Flip","15748885":"for i in range(4):\n    #print(X_train[i], y_train_bin[i])\n    #print(X_val[i], y_val_bin[i])\n    print(X_test[i], y_test_bin[i])\n    ","92b2ab9b":"# DenseNet Dependencies\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.activations import sigmoid\nfrom tensorflow.keras.layers import Dense,Conv2D, Flatten, Dropout, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.metrics import Accuracy, Precision, Recall, AUC, BinaryAccuracy, FalsePositives\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\nfrom keras import backend as K","9bd0946e":"print(\"TensorFlow version: \", tf.__version__)","5e78cdd0":"# Hyperparameters\n\nIMG_SHAPE = (320,320,3)\n\nEPOCHS = 100\n\nSTEPS_PER_EPOCH = 10\n\nOPTIMIZER = Adam(learning_rate=0.001, ####### Modified\n                 beta_1=0.9,\n                 beta_2=0.999)\n\nLOSS = BinaryCrossentropy() # Not un-weighted \n\nMETRICS = ['BinaryAccuracy']","b6db69a7":"# Test if GPU present\nprint(\"Num GPUs Used: \", len(tf.config.experimental.list_physical_devices('GPU')))","833f3de9":"# Section of Code written by brucechou1983 - https:\/\/github.com\/brucechou1983\/CheXNet-Keras\n# I have no experience with class weighting, brucechou1983 provided a very thorough explanation of the topic with example code\nCLASS_NAMES = disease_labels\ndef get_class_weights(total_counts, class_positive_counts, multiply):\n    \"\"\"\n    Calculate class_weight used in training\n    Arguments:\n    total_counts - int\n    class_positive_counts - dict of int, ex: {\"Effusion\": 300, \"Infiltration\": 500 ...}\n    multiply - int, positve weighting multiply\n    use_class_balancing - boolean \n    Returns:\n    class_weight - dict of dict, ex: {\"Effusion\": { 0: 0.01, 1: 0.99 }, ... }\n    \"\"\"\n    def get_single_class_weight(pos_counts, total_counts):\n        denominator = (total_counts - pos_counts) * multiply + pos_counts\n        return {\n            0: pos_counts \/ denominator,\n            1: (denominator - pos_counts) \/ denominator,\n        }\n\n    class_names = list(class_positive_counts.keys())\n    label_counts = np.array(list(class_positive_counts.values()))\n    class_weights = []\n    for i, class_name in enumerate(class_names):\n        class_weights.append(get_single_class_weight(label_counts[i], total_counts))\n\n    return class_weights\n\ndef get_sample_counts(dataset, class_names):\n    \"\"\"\n    Get total and class-wise positive sample count of a dataset\n    Arguments:\n    output_dir - str, folder of dataset.csv\n    dataset - str, train|dev|test\n    class_names - list of str, target classes\n    Returns:\n    total_count - int\n    class_positive_counts - dict of int, ex: {\"Effusion\": 300, \"Infiltration\": 500 ...}\n    \"\"\"\n    df = pd.read_csv(\"train_f.csv\")\n    total_count = df.shape[0]\n    labels = df[class_names].as_matrix()\n    positive_counts = np.sum(labels, axis=0)\n    class_positive_counts = dict(zip(class_names, positive_counts))\n    return total_count, class_positive_counts\n\nnewfds = 'newfds'\ntrain_counts, train_pos_counts = get_sample_counts(\"train\", CLASS_NAMES)\nclass_weights = get_class_weights(train_counts, train_pos_counts, multiply=1)\n","39426808":"train_pos_counts","bf777a5f":"train_counts","62d5d476":"class_weights","c0a8c5b7":"# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef compute_class_freqs(labels):\n    \"\"\"\n    Compute positive and negative frequences for each class.\n\n    Args:\n        labels (np.array): matrix of labels, size (num_examples, num_classes)\n    Returns:\n        positive_frequencies (np.array): array of positive frequences for each\n                                         class, size (num_classes)\n        negative_frequencies (np.array): array of negative frequences for each\n                                         class, size (num_classes)\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # total number of patients (rows)\n    N = labels.shape[0]\n    \n    positive_frequencies = np.sum(labels==1,axis=0)\/N\n    negative_frequencies = np.sum(labels==0,axis=0)\/N\n\n    ### END CODE HERE ###\n    return positive_frequencies, negative_frequencies\n\n# freq_pos, freq_neg = compute_class_freqs(train_generator.labels)\n# freq_pos\n\n\n# pos_weights = freq_neg\n# neg_weights = freq_pos","0bf55a67":"# Saves weights every 5 Epochs\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=CALLBACKS_DIR, \n    verbose=1, \n    save_weights_only=True,\n    period=1)\ncp_callback1 = tf.keras.callbacks.ModelCheckpoint(\n    filepath=CALLBACKS_DIR, \n    verbose=1, \n    save_weights_only=False,save_best_only = True,\n    period=1)\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\"logs\")\n\nreduced_lr = tf.keras.callbacks.ReduceLROnPlateau(\n                        monitor='val_loss',\n                        factor=.1,\n                        patience=5,\n                        verbose=1,\n                        mode='min',\n                        cooldown=0,\n                        min_lr=1e-8 \n                        )","fc00a0ac":"with tf.device('\/GPU:0'):\n\n    base_model = tf.keras.applications.DenseNet121(input_shape=IMG_SHAPE,\n                                                   include_top=False,\n                                                   weights='imagenet')\n                                                   #pooling=\"avg\")\n\n    base_model.trainable = True\n\n    x = base_model.output\n\n    x = GlobalAveragePooling2D()(x)\n    predictions = Dense(14, activation='sigmoid',name='Final')(x)\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    #model.summary()\n    \n    #latest = tf.train.latest_checkpoint(pre_weights)\n    #print('Loading Weights from file: ', latest)\n    #weights_DIR = '\/kaggle\/input\/bruce-weights\/brucechou1983_CheXNet_Keras_0.3.0_weights.h5'\n    #model.load_weights(weights_DIR)\n\n    model.compile(loss = LOSS,\n                  optimizer=OPTIMIZER,\n                  metrics=METRICS\n                                 )\n\n\nhistory = model.fit(\n                    train_ds,\n                    steps_per_epoch = STEPS_PER_EPOCH,\n                    validation_data= create_dataset(X_val, y_val_bin.astype(np.float32)),\n                    validation_steps = STEPS_PER_EPOCH \/ 10, \n                    epochs=EPOCHS,\n                    #use_multiprocessing=True,\n                    class_weight = class_weights,\n                    callbacks=[reduced_lr, tensorboard_callback, cp_callback,cp_callback1]\n                    )\n    ","860105b7":"import pandas as pd\npd.__version__","6c00a0c9":"model.save_weights(\"model_4may.h5\")","769fe3cf":"os.getcwd()","172539a2":"# Graph of Binary Accuracy \n\nacc = history.history['BinaryAccuracy']\nval_acc = history.history['val_BinaryAccuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(40, 10))\nplt.subplot(1, 2, 1)\nplt.grid()\nplt.plot(epochs_range, acc, label='Training Binary Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Binary Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Binary Accuracy', color='Green')\n#fig.savefig('TrainingValidationAccuracy.png')","51a9e6b8":"# Graph of Loss\nplt.figure(figsize=(40, 10))\nplt.subplot(1, 2, 2)\nplt.grid()\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss', color='red')\nplt.show()","14091878":"# Load Model\n#model.load_weights(weights_DIR)","1b1edf38":"# Loading Latest Weights \n#latest = tf.train.latest_checkpoint(checkpoint_dir)\n#print('Loading Weights from file: ', latest)","8e252035":"STEPS = len(y_test_bin)\nprint(STEPS)","08691538":"# Predict\npred = model.predict(test_ds,steps=STEPS,verbose=1)\n\npredicted_class_indices=np.argmax(pred,axis=1)","f544bb16":"y_test = y_test_bin\npred = pred","b68da5b3":"# Print AUC scores\nlabels = []\nscores = []\nfor i in range(14):\n    \n    try:\n        print('{0:<20}\\t{1:<1.6f}'.format(disease_labels[i], roc_auc_score(y_test[:,i], pred[:,i])))\n        labels.append(disease_labels[i])\n        scores.append(roc_auc_score(y_test[:,i], pred[:,i]))\n        \n        \n    except:\n        print('Not Working')\n    \nprint('done') ","df56c014":"# True Postive Vs ...\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfig, c_ax = plt.subplots(1,1, figsize = (9, 9))\nfor (idx, c_label) in enumerate(disease_labels):\n    fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), pred[:,idx])\n    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\nc_ax.legend()\nc_ax.set_xlabel('False Positive Rate')\nc_ax.set_ylabel('True Positive Rate')\nfig.savefig('Completed_Training_CheXNet.png')","819840d8":"# Bar Graph\nimport matplotlib\nmy_mean_aur = round(statistics.mean(scores), 3) * 100 \nprint(f'My AUROC mean score: {my_mean_aur}% ')\n\nStanford_scores = [0.8094, 0.7901, 0.7345, 0.8887, 0.8878, 0.9371, 0.8047, 0.8638, 0.7680, 0.8062, 0.9248, 0.7802, 0.8676, 0.9164]\nstanford_mean_aur = round(statistics.mean(Stanford_scores), 3) * 100 \nprint(f'Stanford Auroc mean score: {stanford_mean_aur}% ')\n\nlabels = ['Stanford Mean AUROC', 'My Mean AUROC']\nmean_scores = [stanford_mean_aur, my_mean_aur]\n\nmatplotlib.pyplot.bar(labels, mean_scores)\n\n\nplt.show()","23d2284f":"# Stanford MEAN AUROC Scores\nstanford_labels = disease_labels\nprint(stanford_labels)\nstanford_scores = Stanford_scores\nprint(stanford_scores)\n\n","1f6c3f42":"# Disease Class Scores\n\nlabels = stanford_labels\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(40,20))\nrects1 = ax.bar(x - width\/2, stanford_scores, width, label='Stanford AUROC', color='green')\nrects2 = ax.bar(x + width\/2, scores, width, label='MY CHEXNET',color='orange')\n\nax.set_ylabel('Scores', color='grey', fontsize= 30)\nax.set_title('Comparing AUROC Scores')\nax.set_xticks(x)\nax.set_xlabel('Disease Classes', color='grey', fontsize= 30)\nax.set_xticklabels(labels, color='grey', fontsize= 17)\nax.legend()\n\n\nplt.show()\nfig.savefig('ComparisonAUROC.png')","48c9b6f8":"# Notes\n\n","87618e0f":"# TF.Data","2a3fa63b":"# Visualizations","30ddb582":"## Preprocessing for Pneumonia (different for the 14 muti class)\u00b6\nTraining and Testing Distributions:\n\nTraining = 28,744 Patients, 98,637 Images\nValidation = 1,672 Patients, 6,351 Images\nTesting = 389 Patients, 420 Images\nNo patient is overlapped in between training\/test sets\n\nThe images are downsampled to 224x224\n\nNormalized based on mean and standard deviation of images in the imagenet training set\nAugmentation:\n\nThey only do Random horizontal flipping","40e41af3":"# Curating Labels Folder","2d229dcd":"# Training","c9650f78":"test_data_gen = ImageDataGenerator(rescale=1.\/255)\n\ntest_gen = test_data_gen.flow_from_dataframe(dataframe=test_df, \n                                                directory=None,\n                                                shuffle = True,\n                                                x_col = 'Paths',\n                                                y_col = disease_labels, \n                                                target_size = (224,224), \n                                                classes = disease_labels,\n                                                class_mode='raw',\n                                                color_mode = 'rgb',\n                                                batch_size = 64000000\n                                                )\n\nx_test, y_test = next(test_gen)","d48cbd13":"## DenseNet-121\nThis is a pretrained network that was trained on ImageNet. It consists of 121 convolutional layers. The Stanford team added Dense connections and batch normalization to speed up training and improve accuracy. The output of the model is a binary label {0,1}. They replace the final layer with a one that has a single output. The use a sigmoid activation function for the last layer. They train the model using mini-batches. They have a decaying learning rate, initialized at 0.001 and decayed by a factor of 10 every time validation loss plateaus after an epoch. They chose the model with the lowest validation loss.\n\n* F1 Score (95% CI) = 0.435 (0.387, 0.481)\n* Loss = Weighted Binary Cross Entropy\n* Optimizer = Adam (B1=0.9, B2=0.999)\n* Mini-Batches = 16\n* Learning Rate = 0.001 (decay)","0d2cd27a":"# Callbacks ","2ddf6c4a":"# Class Weighting","b385adb4":"## NIH Chest X-ray Dataset\nAttempting to recreate the code and results of the stanford paper: https:\/\/arxiv.org\/pdf\/1711.05225.pdf\n\nTheir algorithm was called CheXNet\nThis paper achieved an F1 score of 0.435 on Pneumonia Classification\nThis dataset contains 112,120 Frontal View Chest X-ray images\n\nIt contains the pathologies of 14 thoracic diseases: Atelectasis, Cardiomegaly, Effusion, Infiltration, Mass, Nodule, Pneumonia, Pneumothorax, Consolidation, Edema, Emphysema, Fibrosis, Pleural Thickening, and Hernia","2986e2ff":"# Testing"}}