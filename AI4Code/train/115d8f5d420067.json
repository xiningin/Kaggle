{"cell_type":{"3d732e51":"code","50a7d98b":"code","b07058c3":"code","00b10c90":"code","74664f79":"code","666251d4":"code","c8433b4e":"code","601ec06d":"code","22e7794b":"code","9b344733":"code","ce75b855":"code","9dfbe6e4":"code","669c07c6":"code","a36021d4":"code","c24e5fd2":"code","176279f2":"code","f094acf5":"code","e5baae47":"code","62e78b20":"code","7ca6d978":"code","c83b4a7c":"code","8a927b0c":"code","78a8def1":"code","bcb718f0":"code","dd97cc26":"code","85902fd2":"code","63f8c501":"code","383a7545":"code","285b613c":"code","33d64f17":"code","87cd1148":"code","97e6d028":"code","2de79bd0":"code","3235ef58":"code","ae342561":"code","578d911b":"code","474dc891":"code","ee5d7d6c":"code","da2e4af8":"code","2ba62435":"markdown","1a8c0d08":"markdown","ae8b6a81":"markdown","c6d225da":"markdown","ced0a3b1":"markdown","56e0d5d0":"markdown","061e91fb":"markdown","b08c1447":"markdown","1ebdab68":"markdown","b30e8ca6":"markdown","47d485fd":"markdown","a87f93d1":"markdown","f0d173f8":"markdown","fd90c635":"markdown","81af2a5c":"markdown","2c3c83ab":"markdown","ae36802f":"markdown","ca5ff331":"markdown","d13c4746":"markdown","29d91c31":"markdown","2f57dafc":"markdown","cae7c4f7":"markdown","4eeaa984":"markdown","7899176c":"markdown"},"source":{"3d732e51":"import os\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nfrom keras import backend as K\nfrom keras.applications.densenet import DenseNet121\nfrom keras.layers import Dense, GlobalAveragePooling2D, MaxPooling2D\nfrom keras.models import Model\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nfrom keras.preprocessing import image\nimport tensorflow as tf","50a7d98b":"train_dir= Path('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train')\ntest_dir = Path('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test')\nval_dir  = Path('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val')","b07058c3":"#create the lists of filespaths from the 3 directories: train,test and val\nfiles_path_train= list(train_dir.glob(r'*\/*.jpeg'))\nfiles_path_test= list(test_dir.glob(r'*\/*.jpeg'))\nfiles_path_val = list(val_dir.glob(r'*\/*.jpeg'))","00b10c90":"#create the lists of lables for each filepath\nlabels_train= list(map(lambda x: os.path.split(os.path.split(x)[0])[1], files_path_train)) \nlabels_test = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], files_path_test))\nlabels_val= list(map(lambda x: os.path.split(os.path.split(x)[0])[1], files_path_val))","74664f79":"#create the columns for the dataframes\nlabels_train= pd.Series(labels_train, name= 'Label')\nlabels_test= pd.Series(labels_test, name= 'Label')\nlabels_val = pd.Series(labels_val, name= 'Label')\n\nimages_train= pd.Series(files_path_train, name= 'Filepath').astype(str)\nimages_test = pd.Series(files_path_test, name= 'Filepath').astype(str)\nimages_val = pd.Series(files_path_val, name= 'Filepath').astype(str)","666251d4":"#create the dataframes\ntrain_df= pd.concat([images_train, labels_train], axis=1)\ntest_df = pd.concat([images_test, labels_test], axis=1)\nval_df= pd.concat([images_val, labels_val], axis=1)","c8433b4e":"train_df.head()","601ec06d":"test_df.tail()","22e7794b":"val_df.head()","9b344733":"#checking the number of images \nprint(len(train_df))\nprint(len(val_df))\nprint(len(test_df))\n","ce75b855":"#checking the images shapes\nshapes_0=[]\nshapes_1=[]\nfor filepath in train_df['Filepath'].values:\n    img= imread(filepath)\n    shapes_0.append(img.shape[0])\n    shapes_1.append(img.shape[1])\n","9dfbe6e4":"plt.figure(figsize=(8,6))\nplt.scatter(shapes_0, shapes_1);","669c07c6":"def train_images_gen (train_df, x_col, label_col,b_size=32,w=300, h= 300):\n    \"\"\"\n    returns generator for training set, normlizing using batch statistics.\n    Args:\n    \n    df: dataframe specifying training data\n    x_col: name of column in df that holds filenames\n    label_col: name of labels column\n    b_size: images per batch to be fed into model during training set\n    w: image target width\n    h: image target height\n    \n    Returns:\n    train_images (DataFrameIterator): iterator over training set\n    \"\"\"\n    #create an ImageDataGenerator object normalizing the images\n    train_gen = ImageDataGenerator(samplewise_center= True,\n                                       samplewise_std_normalization= True)\n    #flow from directory with specified batch size and target image size\n    train_images = train_gen.flow_from_dataframe(dataframe= train_df,\n                                                  x_col= x_col,\n                                                  y_col= label_col,\n                                                  class_mode= 'binary',\n                                                  batch_size= b_size,\n                                                  shuffle=True,\n                                                  target_size=(w,h))\n    return train_images","a36021d4":"def test_valid_images_gen (test_df,valid_df,train_df,x_col,label_col, b_size=32,w=300, h=300,\\\n                    sample_size= 100 ):\n    \"\"\"\n    returns generator for validation & test set using normalization statistics from training set\n    Args:\n    test_df: dataframe specifying testing data\n    train_df: dataframe specifying training data \n    x_col: name of column in df that holds filenames\n    label_col: name of labels column\n    b_size: images per batch to be fed into model during training set\n    w: image target width\n    h: image target height\n    \n    returns:\n    test_images & valid_images (DataFrameIterators): iterators over test set\n    \"\"\"\n    \n    #get generator to sample dataset\n    train_images_sample = ImageDataGenerator().flow_from_dataframe(\n        dataframe=train_df, \n        x_col=\"Filepath\", \n        y_col=\"Label\", \n        class_mode=\"binary\", \n        batch_size=sample_size, \n        shuffle=True, \n        target_size=(w,h))\n    #get data sample\n    batch= train_images_sample.next()\n    data_sample= batch[0]\n    \n    #use the sample to fit mean & std to the test set generator\n    test_valid_gen = ImageDataGenerator(\n        featurewise_center=True,\n        featurewise_std_normalization= True)\n    \n    test_valid_gen.fit(data_sample)\n    \n    #get the test generator & valid generators\n    test_images= test_valid_gen.flow_from_dataframe(dataframe= test_df,\n                                                       x_col=x_col,\n                                                       y_col= label_col,\n                                                       class_mode='binary',\n                                                       batch_size= b_size,\n                                                       target_size=(w,h),\n                                                       shuffle=False)\n    valid_images= test_valid_gen.flow_from_dataframe(dataframe= valid_df,\n                                                    x_col= x_col,\n                                                    y_col= label_col,\n                                                    class_mode= 'binary',\n                                                    batch_size= b_size,\n                                                    target_size=(w,h),\n                                                    shuffle=False)\n    return test_images,valid_images\n    ","c24e5fd2":"train_images= train_images_gen(train_df, 'Filepath', 'Label')\ntest_images, valid_images = test_valid_images_gen (test_df,val_df,train_df,'Filepath','Label')","176279f2":"#visualization of some chest x-ray images from the train dataset\n\ntrain_images_files= train_df['Filepath'].values\n\nrandom_images= [np.random.choice(train_images_files) for i in range(9)]\n\n%matplotlib inline\nplt.figure(figsize=(20,10))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    img= imread(random_images[i])\n    plt.imshow(img, cmap='gray')\n    plt.axis(\"off\")\n    plt.colorbar()\nplt.tight_layout()","f094acf5":"list_labels= list(train_df['Label'].value_counts().index)\nlist_labels","e5baae47":"#Checking whether we have class imbalance\n\nplt.bar(x=list_labels, height= \\\n        [np.mean(train_df['Label']=='PNEUMONIA'), np.mean(train_df['Label']=='NORMAL') ]);\n#yes we have class imbalance","62e78b20":"def get_class_weights(labels):\n    \"\"\"\n    computes the postive weight and negative weight for each calss\n    Args:\n        labels: matrix or column vector(if it is about only 2 classes)\n    Returns:\n        weight_pos & weight_neg: for each class: vector pr scalar(if it is about only 2 classes)\n    \"\"\"\n    weight_neg = np.sum(labels)\/len(labels)\n    weight_pos= 1-weight_neg \n    return weight_pos, weight_neg","7ca6d978":"positive_weights, negative_weights= get_class_weights(train_images.labels)","c83b4a7c":"print(\"positive_weights are : {:.3}\".format(positive_weights))\nprint(\"negative_weights are : {:.3}\".format(negative_weights))","8a927b0c":"def get_weighted_loss_binary(pos_weights, neg_weights, epsilon=1e-7):\n    \"\"\"\n    Return weighted loss function given negative weights and positive weights.\n\n    Args:\n      pos_weights (np.array): scalar weights for the positive class\n      neg_weights (np.array): scalar weights for the negative class\n    \n    Returns:\n      weighted_loss (function): weighted loss function\n    \"\"\"\n    def weighted_loss(y_true, y_pred):\n        \"\"\"\n        Return weighted loss value. \n\n        Args:\n            y_true (Tensor): Tensor of true labels, size is (num_examples,1)\n            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, 1)\n        Returns:\n            loss (Float): overall scalar loss \n        \"\"\"\n        loss = 0.0\n        #we add epsilon to avoid an error if y_pred (is 0)\n        loss +=(-1*pos_weights*y_true* K.log(y_pred+epsilon)+ \\\n            -1*neg_weights*(1-y_true)* K.log(1-y_pred+epsilon))\n\n        return loss\n    \n    return weighted_loss","78a8def1":"#create the base pretrained model\nbase_model = DenseNet121(weights='..\/input\/densenet\/densenet.hdf5',\\\n                         include_top= False)\nx= base_model.output\n\n#add global spatial average pooling\nx= GlobalAveragePooling2D()(x)\n\n#add the output layer\npredictions= Dense(units= 1, activation =\"sigmoid\")(x)\n\n#create the model\nmodel= Model(inputs= base_model.input, outputs=predictions )\n\n#compile the model\nmodel.compile(optimizer= 'adam', loss=get_weighted_loss_binary(positive_weights, negative_weights),\\\n             metrics= ['accuracy'])\n\n","bcb718f0":"history = model.fit(train_images,\n                    epochs= 5,\n                    validation_data=valid_images,\n                    steps_per_epoch=100,\n                    validation_steps=2)\n                              ","dd97cc26":"#Evaluate the model on the validation set:\n#it is predictible that the accuracy on the validation set is high because the \n#set size is so small (only 16 images, compared to 5216 images for the train set):\n#And the sizes are fixed by the author\nmodel.evaluate(valid_images)","85902fd2":"Results= model.evaluate(test_images)\nResults","63f8c501":"print(\"The Loss on the test set is: {:.3f}\".format(Results[0]))\nprint(\"The accuracy on the test set is: {:.1f}%\".format(Results[1]*100))","383a7545":"proba_predictions= model.predict(test_images)","285b613c":"#Checking the calss indices: 0: negative cases(NORMAL) & 1: positive cases (PNEUMONIA)\ndict_classes= train_images.class_indices\ndict_classes","33d64f17":"y_true= test_images.classes\ny_predictions= (proba_predictions > 0.5).astype('int32')","87cd1148":"print(classification_report(y_true,y_predictions ))","97e6d028":"cm= confusion_matrix(y_true,y_predictions)\ndf_cm= pd.DataFrame(cm, index= list(dict_classes.keys()), columns=list(dict_classes.keys()) )\ndf_cm","2de79bd0":"sns.heatmap(df_cm, annot=True);\n#we notice that only 7 positive cases are classified as normal and 383 penumonia cases from 390,\n#have been properly classified as positive cases (98%): high performance \n#it is true that 68 normal cases are classified as positive cases (but as I said above, the goal \n#of our model,is to detect the maximum of positive cases(to help doctors to identify sick subjects))\n#if it is about detecting credit card frauds,it will be worrying(because we must have high precision)","3235ef58":"my_image_path= np.random.choice(test_df['Filepath'].values)\nmy_image= image.load_img(my_image_path)\nmy_image_arr= image.img_to_array(my_image)","ae342561":"#Check the true label of our random image \n#as we see, it is positive case (so label= 1)\nfor index, row in test_df.iterrows():\n    if row['Filepath']== my_image_path:\n        print(row['Label'])","578d911b":"my_image","474dc891":"my_image_arr.shape","ee5d7d6c":"#to have the same size as the trained images\nmy_image = tf.image.random_crop(my_image_arr,[300, 300, 3]) \n\n#to have the same shape as the trained images\n\nmy_image= np.expand_dims(my_image, axis=0)\nprint(my_image.shape)","da2e4af8":"(model.predict(my_image)>0.5).astype('int32')","2ba62435":"\nWe have 3 datasets: one for **train images**, one for **val images** the other for **test images**.We can now porceed with image preprocessing: we modify the images to be better suited for our **training model**.\n\n* For this, we will use the Keras **ImageDataGenerator** to perform **data preprocessing and data augmentation.**\n\n* we will also use the generators to **transform the values** in each batch, so taht **their mean is 0** and their **standard deviation is 1** (this will facilitate model training by standardizing the input distribution.\n\n* the generator will also, automatically,  convert our **single channel chest x_ray** images (gray scale) to **three channel format** by repeating the pixel values in the images across all the channels (we will use this because the pretrained model that we will use require three channel inputs)\n","1a8c0d08":"#### **6- Prediction on an image from test set (chosen randomly) :**","ae8b6a81":"# **Table of Contents**\n#### **Step_0: import the libraries**\n\n#### **Step_1: preparing the images (creating dataframes & generators)**\n\n#### **Step_2: Visualization**\n\n#### **Step_3: Model Development**\n\n#### **Step_4: Evaluation of the model on the test set: Accuracy, Recall, Precision, f1_score**","c6d225da":"## **Step_0: import the libraries**","ced0a3b1":"## **Step_2: Visualization**","56e0d5d0":"Ideally, we would train our model using an evenly **balanced dataset** so that the positive and negative training cases would **contribute equally to the loss.**\n\nIf we use a normal cross-entropy loss function with a highly **unbalanced dataset**, as we see above, the algorithm will be incentivized to **prioritize the majority class** (**positive(PNEUMONIA)** in our case), since it contributes more to the loss.\n\nSo, we will **transform the loss function** to a **weighted one**:\n\nThe loss is made up of two terms: for each training example i: \n\n**loss(i)= loss(i)neg + loss(i)pos**\n\n**loss(i)neg = -1* weight_pos(i) * y(i) *log(f(x(i))**\n\n**loss(i)pos = -1* weight_neg(i) * (1- y(i))* log(1-f(x(i))**\n\nwith:\n**loss(i)pos**: refers to the loss where the actual label is positive (the same for loss(i)neg...)\n\n**y(i):** the actaul label\n\n**f(x(i)):** the predicted label\n\nAnd : \n\n**weight_pos** = frequency of **negative** examples = number of negative examples\/N\n\n**weight_neg** = frequency of **positive** examples = number of positive examples\/N\n\n**N**: total number of examples \n\n\n**Explanation :** \nin our case, the contributions of **negative cases(normal)** will significantly be **lower** than that of the **negative** ones. However, we want the contributions to be **equal**. One way of doing this is by multiplying each example from each class by a **class-specific weight factor**: **weight_pos & weight_neg**: so that the overall contribution of each class is the **same**. To have this like saying: \n\n**weight_pos* pos_frquency = weight_neg * neg_frequency** \n\nso: \n**weight_pos = neg_frequency**\n\n**weight_neg = pos_frquency**","061e91fb":" ### **2- Creating the generators**","b08c1447":"#### **3- Evaluation: Accuracy on the test_set (but accuracy is not the best performance metric in our case)**","1ebdab68":"## **Step_3: Model Development**\n","b30e8ca6":" ### **1- Addressing Class Imbalance**","47d485fd":"## **Step_1: preparing the images** (creating dataframes & generators)\n\n","a87f93d1":"###   **1- Creating the dataframes**","f0d173f8":"## **Step_4: Evaluation of the model on the test set: accuracy, recall, precision, f1_score**","fd90c635":"#### **5- Visualization of Results:**","81af2a5c":"- then we define a function that computes the weighted loss:","2c3c83ab":"\n**Now, we build a seperate generator for test & valid sets:**\n\nwe should not use the same generator as for the training data, because **knowing the average per batch of our test data** will give our model an **adavantage** (the model should not have any information about the test data (in order to avoid **data leakage**) What we need to do is normalizing test data with **the statistics computed from training set**\n\n* we implement this in this function below\n* There is one **technical issue**: ideally, we would want compute the **sample mean** and the **sample standrad deviation** using the **entire training set**\n* However, since the train set is **large**, we couldn't process this in this environment\n* Solution: we will take a **random** from the train set and compute the **sample mean** and **sample standrad deviation.**","ae36802f":"- since the number of examples required to train medcial models for diagnosis is between 10.000 and 100.000, we will use **Transfer Learning** to build our model (because the number of our training examples doesn't exceed 5216 examples)\n\n- we will use se a pre-trained **DenseNet121** model which we can load directly from Keras and then add  layers on top of it:\n\n- for a detailed explanation of DenseNet, check this source: [https:\/\/arxiv.org\/pdf\/1608.06993.pdf](http:\/\/)\n\n- we can download the pretrained weights from this link kaggle: [https:\/\/www.kaggle.com\/xhlulu\/densenet-keras](http:\/\/)\n","ca5ff331":"=> **So the model correctly predicts this random image from the test set as a positive case**","d13c4746":"#### **Results:**\n- The **recall (Sensitivity)**: **The ability** of the model to predcit positive case: **98%**\n\n- The **f1-score** is **91%** (balance between precision & recall)\n\n=> **our model achieves a high performance (of 98%) to predict positive cases**","29d91c31":"We define a function that computes the two terms:\n- weight_pos\n- weight_neg\n","2f57dafc":"#### **4- Predictions (on the test_set) & the best performance metrics: recall, f1-score**","cae7c4f7":"#### **1- Important: What is the best metric that reveals the model performance?** \n\n\n**it is important** to choose **metric** that matches **the goal** of our **Model**. \n\nIn our case, the **accuracy is not the best metric**.Indeed, we have to detect **pneumonia cases**, so we msut choose a model(classifier) that **maximizes the recall (TP\/TP+FN): the ability of the model to predict positive cases** (even if it will classify some negative cases as positive: **\"no matter\"**): \n\n**the Goal** is  to **detect the maximum of positive cases (having pneumonia)**. \n\nthe **precision** is also a major metric, but it is not as important as the **recall(in our case)**, in other words, if it is about **detecting credit card frauds**: yes,we should focus on the precision(because we must be careful not to calssify a non-fraudulent(negative) case as fraudulent(positive)). \n\n**f1-score** is a balance between these two metrics **recall(sensitivity) & precision**. \nBelow, we give **all these metrics** that reveal the performance of our model.\n","4eeaa984":"#### **2-  Accuracy on the validation_set(just for recall)**","7899176c":"### **2- Building & training the model**"}}