{"cell_type":{"1247a37e":"code","c0eafa2b":"code","9347248f":"code","095062ae":"code","7dd5d75e":"code","d03beddd":"code","ace3affa":"code","b3e5dce7":"code","6c38b325":"code","3576b516":"code","cdef0121":"code","66bb98fe":"code","44ffdb0a":"code","1b28aa1f":"code","03308223":"code","15db862e":"code","946c5ab2":"code","6dddeb03":"code","b9c14d1e":"code","b9af89a1":"markdown","e9d1886a":"markdown","7080e766":"markdown","2fa1ebe7":"markdown","7a29300e":"markdown","4765ed49":"markdown"},"source":{"1247a37e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c0eafa2b":"SOC_MINOR_GROUPS = {\n    '11-1': 'Top Executives',\n    '11-2': 'Advertising, Marketing, Promotions, Public Relations, and Sales Managers',\n    '11-3': 'Operations Specialties Managers',\n    '11-9': 'Other Management Occupations',\n    '13-1': 'Business Operations Specialists',\n    '13-2': 'Financial Specialists',\n    '15-1': 'Computer Occupations',\n    '15-2': 'Mathematical Science Occupations',\n    '17-1': 'Architects, Surveyors, and Cartographers',\n    '17-2': 'Engineers',\n    '17-3': 'Drafters, Engineering Technicians, and Mapping Technicians',\n    '19-1': 'Life Scientists',\n    '19-2': 'Physical Scientists',\n    '19-3': 'Social Scientists and Related Workers',\n    '19-4': 'Life, Physical, and Social Science Technicians',\n    '21-1': 'Counselors, Social Workers, and Other Community and Social Service Specialists',\n    '21-2': 'Religious Workers',\n    '23-1': 'Lawyers, Judges, and Related Workers',\n    '23-2': 'Legal Support Workers',\n    '25-1': 'Postsecondary Teachers',\n    '25-2': 'Preschool, Primary, Secondary, and Special Education School Teachers',\n    '25-3': 'Other Teachers and Instructors',\n    '25-4': 'Librarians, Curators, and Archivists',\n    '25-9': 'Other Education, Training, and Library Occupations',\n    '27-1': 'Art and Design Workers',\n    '27-2': 'Entertainers and Performers, Sports and Related Workers',\n    '27-3': 'Media and Communication Workers',\n    '27-4': 'Media and Communication Equipment Workers',\n    '29-1': 'Health Diagnosing and Treating Practitioners',\n    '29-2': 'Health Technologists and Technicians',\n    '29-9': 'Other Healthcare Practitioners and Technical Occupations',\n    '31-1': 'Nursing, Psychiatric, and Home Health Aides',\n    '31-2': 'Occupational Therapy and Physical Therapist Assistants and Aides',\n    '31-9': 'Other Healthcare Support Occupations',\n    '33-1': 'Supervisors of Protective Service Workers',\n    '33-2': 'Fire Fighting and Prevention Workers',\n    '33-3': 'Law Enforcement Workers',\n    '33-9': 'Other Protective Service Workers',\n    '35-1': 'Supervisors of Food Preparation and Serving Workers',\n    '35-2': 'Cooks and Food Preparation Workers',\n    '35-3': 'Food and Beverage Serving Workers',\n    '35-9': 'Other Food Preparation and Serving Related Workers',\n    '37-1': 'Supervisors of Building and Grounds Cleaning and Maintenance Workers',\n    '37-2': 'Building Cleaning and Pest Control Workers',\n    '37-3': 'Grounds Maintenance Workers',\n    '39-1': 'Supervisors of Personal Care and Service Workers',\n    '39-2': 'Animal Care and Service Workers',\n    '39-3': 'Entertainment Attendants and Related Workers',\n    '39-4': 'Funeral Service Workers',\n    '39-5': 'Personal Appearance Workers',\n    '39-6': 'Baggage Porters, Bellhops, and Concierges',\n    '39-7': 'Tour and Travel Guides',\n    '39-9': 'Other Personal Care and Service Workers',\n    '41-1': 'Supervisors of Sales Workers',\n    '41-2': 'Retail Sales Workers',\n    '41-3': 'Sales Representatives, Services',\n    '41-4': 'Sales Representatives, Wholesale and Manufacturing',\n    '41-9': 'Other Sales and Related Workers',\n    '43-1': 'Supervisors of Office and Administrative Support Workers',\n    '43-2': 'Communications Equipment Operators',\n    '43-3': 'Financial Clerks',\n    '43-4': 'Information and Record Clerks',\n    '43-5': 'Material Recording, Scheduling, Dispatching, and Distributing Workers',\n    '43-6': 'Secretaries and Administrative Assistants',\n    '43-9': 'Other Office and Administrative Support Workers',\n    '45-1': 'Supervisors of Farming, Fishing, and Forestry Workers',\n    '45-2': 'Agricultural Workers',\n    '45-3': 'Fishing and Hunting Workers',\n    '45-4': 'Forest, Conservation, and Logging Workers',\n    '47-1': 'Supervisors of Construction and Extraction Workers',\n    '47-2': 'Construction Trades Workers',\n    '47-3': 'Helpers, Construction Trades',\n    '47-4': 'Other Construction and Related Workers',\n    '47-5': 'Extraction Workers',\n    '49-1': 'Supervisors of Installation, Maintenance, and Repair Workers',\n    '49-2': 'Electrical and Electronic Equipment Mechanics, Installers, and Repairers',\n    '49-3': 'Vehicle and Mobile Equipment Mechanics, Installers, and Repairers',\n    '49-9': 'Other Installation, Maintenance, and Repair Occupations',\n    '51-1': 'Supervisors of Production Workers',\n    '51-2': 'Assemblers and Fabricators',\n    '51-3': 'Food Processing Workers',\n    '51-4': 'Metal Workers and Plastic Workers',\n    '51-5': 'Printing Workers',\n    '51-6': 'Textile, Apparel, and Furnishings Workers',\n    '51-7': 'Woodworkers',\n    '51-8': 'Plant and System Operators',\n    '51-9': 'Other Production Occupations',\n    '53-1': 'Supervisors of Transportation and Material Moving Workers',\n    '53-2': 'Air Transportation Workers',\n    '53-3': 'Motor Vehicle Operators',\n    '53-4': 'Rail Transportation Workers',\n    '53-5': 'Water Transportation Workers',\n    '53-6': 'Other Transportation Workers',\n    '53-7': 'Material Moving Workers',\n    '55-1': 'Military Officer Special and Tactical Operations Leaders',\n    '55-2': 'First-Line Enlisted Military Supervisors',\n    '55-3': 'Military Enlisted Tactical Operations and Air\/Weapons Specialists and Crew Members'\n}","9347248f":"from io import StringIO\nimport requests\nimport pandas as pd\n\nfrom random import seed\nimport numpy as np\nseed(42)\nnp.random.seed(42)\n\nfile_url = 'https:\/\/www.onetcenter.org\/dl_files\/database\/db_20_1_text\/Sample%20of%20Reported%20Titles.txt'\ncsv = StringIO(requests.get(file_url).text)\n\n# Load it in a pandas DataFrame and drop a useless column\ndf = pd.read_csv(csv, sep='\\t').drop('Shown in My Next Move', axis=1)\n\n# Get the occupation name from the code and remove the original code column\ndf['SOC minor group'] = df['O*NET-SOC Code'].apply(lambda x: SOC_MINOR_GROUPS[x[:4]])\ndf.drop('O*NET-SOC Code', axis=1, inplace=True)\n\n# Lower all job titles for simplicity\ndf['Reported Job Title'] = df['Reported Job Title'].str.lower()\n\n# Display a few examples\ndf.iloc[[1,2,3,100,101,102,301,302,303]]","095062ae":"# Count number unique each column\ndf.nunique()","7dd5d75e":"test_set = df.groupby('SOC minor group', as_index=False)['Reported Job Title'].first()\ntrain_set = df[~df['Reported Job Title'].isin(test_set['Reported Job Title'])]\n\nx_train, y_train = train_set['Reported Job Title'], train_set['SOC minor group']\nx_test, y_test = test_set['Reported Job Title'], test_set['SOC minor group']","d03beddd":"from sklearn.preprocessing import LabelEncoder\n\nclasses_encoder = LabelEncoder()\ny_train = classes_encoder.fit_transform(y_train)\ny_test = classes_encoder.transform(y_test)","ace3affa":"!pip install zeugma","b3e5dce7":"# word embedding basic tranfer learning \nfrom zeugma import EmbeddingTransformer\nembedding = EmbeddingTransformer('glove-twitter-100', aggregation='sum')","6c38b325":"from sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclf = KNeighborsClassifier(n_neighbors=1)\n\nbaseline = make_pipeline(embedding, clf)","3576b516":"baseline.fit(x_train, y_train)\nprint('Train accuracy (baseline): {} %'.format(100*baseline.score(x_train, y_train)))\n\nprint('Test accuracy (baseline): {} %'.format(100*baseline.score(x_test, y_test)))","cdef0121":"import itertools\nfrom random import sample\n\njobs_left = []\njobs_right = []\ntarget = []\n\nsoc_codes = train_set['SOC minor group'].unique()\nfor code in soc_codes:\n    similar_jobs = train_set[train_set['SOC minor group'] == code]['Reported Job Title']\n    \n    # pick 1000 random pairs from SOC group job titles combinations\n    group_pairs = list(itertools.combinations(similar_jobs, 2))\n    positive_pairs = sample(group_pairs, 1000) if len(group_pairs) > 1000 else group_pairs\n    jobs_left.extend([p[0] for p in positive_pairs])\n    jobs_right.extend([p[1] for p in positive_pairs])\n    target.extend([1.]* len(positive_pairs))\n    \n    # negative sample\n    other_jobs = train_set[train_set['SOC minor group'] != code]['Reported Job Title']\n    for i in range(len(positive_pairs)):\n        jobs_left.append(np.random.choice(similar_jobs))\n        jobs_right.append(np.random.choice(similar_jobs))\n        target.append(0.)\n\ndataset = pd.DataFrame({\n    'job_left': jobs_left,\n    'job_right': jobs_right,\n    'target': target\n}).sample(frac=1).drop_duplicates(subset =['job_left', 'job_right'], keep = False, inplace = False)\n # Shuffle dataset\nprint(dataset.shape)\ndataset.tail()","66bb98fe":"import re\nfrom sklearn.pipeline import make_pipeline, FeatureUnion\nfrom sklearn.preprocessing import FunctionTransformer\nfrom zeugma import TextsToSequences, Padder, ItemSelector\n\nmax_words_job_title = 10  # To avoid very long job titles we limit them to 10 words\nvocab_size = 10000  # Number of most-frequent words kept in the vocabulary\n\ndef preprocess_job_titles(job_titles):\n    \"\"\" Return a list of clean job titles \"\"\"\n    def preprocess_job_title(raw_job_title):\n        \"\"\" Clean a single job title\"\"\"\n        job_title = re.sub(r'\\(.*\\)', '', raw_job_title)  # Remove everything between parenthesis\n        return job_title.lower().strip()\n    return [preprocess_job_title(jt) for jt in job_titles]\n    \npipeline = make_pipeline(\n    FunctionTransformer(preprocess_job_titles, validate=False),  # Preprocess the text\n    TextsToSequences(num_words=vocab_size),  # Turn word sequences into indexes sequences\n    Padder(max_length=max_words_job_title),  # Pad shorter job titles with a dummy index\n)\n\n# Note that the preprocessing pipeline must be fit on both the right and left examples\n# simultaneously\npipeline.fit(list(dataset['job_left']) + list(dataset['job_right']));","44ffdb0a":"x_left = pipeline.transform(dataset['job_left'])\nx_right = pipeline.transform(dataset['job_right'])\nx_pairs = [x_left, x_right]   # this will be the input of the siamese network\n\ny_pairs = dataset['target'].values","1b28aa1f":"# We re-use the same embedding as with the baseline model\nembedding_layer = embedding.model.get_keras_embedding()","03308223":"from keras.layers import LSTM, Bidirectional\nfrom keras import Model, Sequential\nfrom keras.layers import Input, Dense, Dropout, Lambda, Subtract\nfrom keras import backend as K\n\ndef exponent_neg_manhattan_distance(arms_difference):\n    \"\"\" Compute the exponent of the opposite of the L1 norm of a vector, to get the left\/right inputs\n    similarity from the inputs differences. This function is used to turned the unbounded\n    L1 distance to a similarity measure between 0 and 1\"\"\"\n    return K.exp(-K.sum(K.abs(arms_difference), axis=1, keepdims=True))\n\ndef siamese_lstm(max_length, embedding_layer):\n    \"\"\" Define, compile and return a siamese LSTM model \"\"\"\n    input_shape = (max_length,)\n    left_input = Input(input_shape, name='left_input')\n    right_input = Input(input_shape, name='right_input')\n\n    # Define a single sequential model for both arms.\n    # In this example I've chosen a simple bidirectional LSTM with no dropout\n    seq = Sequential(name='sequential_network')\n    seq.add(embedding_layer)\n    seq.add(Bidirectional(LSTM(32, dropout=0., recurrent_dropout=0.)))\n    \n    left_output = seq(left_input)\n    right_output = seq(right_input)\n\n    # Here we subtract the neuron values of the last layer from the left arm \n    # with the corresponding values from the right arm\n    subtracted = Subtract(name='pair_representations_difference')([left_output, right_output])\n    malstm_distance = Lambda(exponent_neg_manhattan_distance, \n                             name='masltsm_distance')(subtracted)\n\n    siamese_net = Model(inputs=[left_input, right_input], outputs=malstm_distance)\n    siamese_net.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n    return siamese_net\n\nsiamese_lstm = siamese_lstm(max_words_job_title, embedding_layer)\n\n# Print a summary of the model mainly to know the number of trainable parameters\nsiamese_lstm.summary()","15db862e":"siamese_lstm.fit(x_pairs, y_pairs, validation_split=0.1, epochs=1)","946c5ab2":"x_references = pipeline.transform(x_train)  # Preprocess the training set examples\n\ndef get_prediction(job_title):\n    \"\"\" Get the predicted job title category, and the most similar job title\n    in the train set. Note that this way of computing a prediction is highly \n    not optimal, but it'll be sufficient for us now. \"\"\"\n    x = pipeline.transform([job_title])\n    \n    # Compute similarities of the job title with all job titles in the train set\n    similarities = siamese_lstm.predict([[x[0]]*len(x_references), x_references])\n    most_similar_index = np.argmax(similarities)\n    \n    # The predicted category is the one of the most similar example from the train set\n    prediction = train_set['SOC minor group'].iloc[most_similar_index]\n    most_similar_example = train_set['Reported Job Title'].iloc[most_similar_index]\n    return prediction, most_similar_example","6dddeb03":"sample_idx = 1\npred, most_sim = get_prediction(x_test[sample_idx])\n\nprint(f'Sampled test job title: {x_test[sample_idx]}')\nprint(f'True occupation: {test_set[\"SOC minor group\"].iloc[sample_idx]}')\nprint(f'Occupation prediction: {pred}')\nprint(f'Most similar example in train set: {most_sim}') ","b9c14d1e":"from sklearn.metrics import accuracy_score\n\ny_pred = [get_prediction(job_title)[0] for job_title in test_set['Reported Job Title']]\naccuracy = accuracy_score(classes_encoder.transform(y_pred), y_test)\n\nprint(f'Test accuracy (siamese model): {100*accuracy:.2f} %')","b9af89a1":"## Modelling\n* remove parts of jobs title between parenthesis\n* turn examples into index sequences\n* \"pad\" get valid input for NN clasifier","e9d1886a":"## encode targets as numbers","7080e766":"# Building a baseline","2fa1ebe7":"# Few-shot learning with siamese NN\n* main ideal train model discriminates between pairs of example same category or different category","7a29300e":"[Ngu\u1ed3n t\u00e0i li\u1ec7u tham kh\u1ea3o](https:\/\/data4thought.com\/fewshot_learning_nlp.html)","4765ed49":"## Building the pairs dataset\n* positive sample is job title from the same SOC category and negative example different SOC codes"}}