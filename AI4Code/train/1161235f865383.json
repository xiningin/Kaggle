{"cell_type":{"f6ae7d45":"code","740f0165":"code","7300b62c":"code","8ada7e89":"code","c31618ab":"code","3a262b21":"code","f853b318":"code","e4a400ce":"code","d4a3bb20":"code","5879e798":"code","fcd1df52":"code","bdc1fb36":"code","32763fc7":"code","954d6996":"code","18f9b426":"code","a1ab188d":"markdown"},"source":{"f6ae7d45":"%matplotlib inline\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nwarnings.filterwarnings('ignore')","740f0165":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ny_train = train.label.to_numpy()\n\nx_train = train.loc[:, train.columns != 'label'].to_numpy()\nx_train = x_train.reshape(x_train.shape[0], 28, 28)\n\nx_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv').to_numpy()\nx_test = x_test.reshape(x_test.shape[0], 28, 28)","7300b62c":"n_train = x_train.shape[0]\nn_test = x_test.shape[0]","8ada7e89":"# Rescale data [-1,1]\nx_train, x_test = x_train \/ 127.5 - 1, x_test \/ 127.5 - 1","c31618ab":"# Variables\nnb_features = np.prod(x_train.shape[1:])\nedge = int(np.sqrt(nb_features))","3a262b21":"# One-hot encode\ny_train = np.eye(10)[y_train]","f853b318":"class CNN:\n    def __init__(self, x_train, y_train, output_dir, lr=0.001, nb_epochs=10, batch_size=50):\n        self.nb_epochs = nb_epochs\n        self.lr = lr\n        self.batch_size = batch_size\n        self.nb_epochs = nb_epochs\n        self.nb_images, self.edge, _ = x_train.shape\n        self.nb_iterations = self.nb_images \/\/ batch_size\n        self.output_dir = output_dir\n        self.im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n        self.labels = tf.placeholder(tf.float32, [None, 10])\n        self.x_train = x_train[:,:,:, np.newaxis]\n        self.y_train = y_train\n        \n    def create_model(self):\n        with tf.variable_scope('CNN', reuse=tf.AUTO_REUSE):\n            ######### Complete the function ######### \n            self.first_layer = tf.layers.conv2d(self.im, 32, (4,4), strides=1, activation=tf.nn.relu)\n            self.second_layer = tf.layers.conv2d(self.first_layer, 64, (4,4), strides=2, activation=tf.nn.relu)\n            self.third_layer = tf.layers.conv2d(self.second_layer, 128, (4,4), strides=2, activation=None)\n            self.flattened = tf.layers.flatten(self.third_layer)\n            self.logits = tf.layers.dense(self.flattened, 10, activation=None)\n            self.preds = tf.nn.softmax(self.logits)\n            #########################################\n    \n    def compute_loss(self):\n        with tf.variable_scope('loss'):\n            ######### Complete the function ######### \n            self.loss = tf.losses.softmax_cross_entropy(self.labels, logits=self.logits)\n            #########################################\n            self.loss_summ = tf.summary.scalar(\"softmax_loss\", self.loss)\n    def optimizer(self):\n        with tf.variable_scope('optimizer'):\n            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5)\n            self.model_vars = tf.trainable_variables()\n            self.trainer = optimizer.minimize(self.loss, var_list=self.model_vars)","e4a400ce":"# Reset graph\ntf.reset_default_graph()","d4a3bb20":"model = CNN(x_train, y_train, '.\/CNN_logdir\/', 0.001, 2, 10)\nmodel.create_model()\nmodel.compute_loss()\nmodel.optimizer()\ninit = (tf.global_variables_initializer(),\n        tf.local_variables_initializer())\n\nsaver = tf.train.Saver()\nsummary =tf.Summary()\nsess = tf.InteractiveSession()\nsess.run(init)\nwriter = tf.summary.FileWriter(model.output_dir)\nwriter.add_graph(sess.graph)\nif not os.path.exists(model.output_dir):\n    os.makedirs(model.output_dir)","5879e798":"for epoch in range(model.nb_epochs):\n    randomize = np.arange(x_train.shape[0])\n    np.random.shuffle(randomize)\n    x_in = model.x_train[randomize,:]\n    y_in = model.y_train[randomize,:]\n    for i in range(model.nb_iterations):\n        input_x_train = x_in[i*model.batch_size: (i+1)*model.batch_size]\n        input_y_train = y_in[i*model.batch_size: (i+1)*model.batch_size]\n        _ , preds, loss, loss_summ = sess.run([model.trainer, model.preds, model.loss, model.loss_summ], \n                                 feed_dict={model.im: input_x_train, \n                                            model.labels: input_y_train})\n        y_preds = np.argmax(preds, axis=1)\n        y_real = np.argmax(input_y_train, axis=1)\n        acc_train = np.mean((y_preds==y_real)*1)\n        #print('Epoch %d, Iteration %d, loss %.3f, batch accuracy %.3f' %(epoch, i, loss, acc_train))\n        writer.add_summary(loss_summ, epoch * model.nb_iterations + i)\n    saver.save(sess, model.output_dir, global_step=epoch)  ","fcd1df52":"batch_size_test = 20\nnb_test_points = x_test.shape[0] \nnb_iterations = nb_test_points\/\/batch_size_test\npreds = []\nfor i in range(nb_iterations):\n    input_x_test = x_test[i*batch_size_test: (i+1)*batch_size_test]\n    input_x_test = input_x_test[:, :, :,np.newaxis]\n    preds_test = sess.run(model.preds, feed_dict={model.im: input_x_test})\n    preds.append(np.argmax(preds_test, axis=1))\n    if np.mod(nb_test_points, batch_size_test) !=0:\n        input_x_test = x_test[i*batch_size_test: -1]\n        preds_test = sess.run(model.preds, feed_dict={model.im: input_x_test})\n        preds.append(np.argmax(preds, axis=1))","bdc1fb36":"# Prepare as submission.csv\nall_preds = np.concatenate(preds, axis =0)\nimageId = np.arange(1,len(all_preds)+1)","32763fc7":"# Submission array\nsubmission = np.zeros((len(all_preds),2))\nsubmission[:,0] = imageId\nsubmission[:,1] = all_preds\nsubmission = submission.astype(int)","954d6996":"np.savetxt('submission.csv', submission, delimiter=\",\", fmt='%d', header=\"ImageId,Label\", comments='')","18f9b426":"sess.close()","a1ab188d":"# Import Data"}}