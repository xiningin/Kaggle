{"cell_type":{"7c4b6f8a":"code","aff8cf08":"code","df7d71fc":"code","5af052b0":"code","080b038f":"code","78123c09":"code","4738bace":"code","b8223b3d":"code","fe34a5b0":"code","8e37502e":"code","738a3ddb":"code","bf715c0d":"code","c0f8b7db":"code","1f55c6e4":"code","3641d979":"code","cafd5d10":"code","12efe3dd":"code","1aaea3ec":"code","a516e41f":"code","7f4cf574":"code","bff11684":"code","3679fb2e":"code","7bdd9493":"code","ca491a57":"code","ecc73414":"code","e4b589c9":"code","e05d7aae":"code","32d8ec3d":"code","91930618":"code","67d376fc":"code","3db16e11":"code","ab73005b":"code","bf109307":"code","5d8eccac":"code","aefb0d47":"code","befc71c2":"code","c05687ed":"code","0b8cc26f":"code","7b11e143":"code","3ad275dc":"code","63af26bd":"code","5e2126b9":"code","3f3c533b":"code","cbbd2b2a":"code","81bdbe92":"code","46bc86b9":"code","196f08c7":"markdown","1c84df59":"markdown","a9d08a90":"markdown","eeb7b549":"markdown","2b5503a4":"markdown","a321fe42":"markdown","b81247f2":"markdown","f8ac5712":"markdown","e30693f9":"markdown","d2b994a8":"markdown","5b3ac7e1":"markdown","9e8fe8c3":"markdown","581b9f48":"markdown","6cc85161":"markdown","3484f1ba":"markdown","64e9014b":"markdown","c8a46fac":"markdown","df53d19e":"markdown","e380d990":"markdown","7e451fd0":"markdown","fbf94e1e":"markdown","a6476ade":"markdown","e279f9f8":"markdown","b01f60ee":"markdown","1d0986c3":"markdown","8a1cf32c":"markdown","58bf48bf":"markdown","03973277":"markdown","310ef032":"markdown","5c9188e7":"markdown","11a6a806":"markdown","43cc1163":"markdown","6614e864":"markdown","d3e2dcd5":"markdown","b8385ec4":"markdown","a6d32da8":"markdown","2f5c0604":"markdown","44f0968f":"markdown","cc856294":"markdown","f2e9f06c":"markdown","d2de43af":"markdown","1ac25e3b":"markdown"},"source":{"7c4b6f8a":"# Import packages needed\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport seaborn as sns\n\n#Stats\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import f_classif, mutual_info_classif, chi2\nfrom sklearn.preprocessing import PowerTransformer\n\n#Data Processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, OneHotEncoder, PowerTransformer, LabelEncoder, StandardScaler\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom collections import defaultdict\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n#Model \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipe\nfrom sklearn.metrics import f1_score, recall_score, precision_score\n\n#Config\npd.pandas.set_option('display.max_columns', None)","aff8cf08":"df = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\ndf.head()","df7d71fc":"df = df.drop(columns= ['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n                 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'])","5af052b0":"o_shape = df.shape\nprint(f'Data shape: {o_shape} \\n')\nprint(f'Information of data columns:{df.info()} \\n')","080b038f":"df, df_test = train_test_split(df, test_size=0.2, random_state=42)","78123c09":"pd.concat([df.nunique(), df.dtypes], axis= 1).rename(columns={0:'Unique_Values',1:'Data_Type'}).sort_values('Unique_Values')","4738bace":"df  = df.drop(columns=['CLIENTNUM'])","b8223b3d":"df.describe().apply(lambda s: s.apply('{0:.2f}'.format))","fe34a5b0":"df_numeric = df.select_dtypes(include='number')\n\ndef variable_remove_underline(name):\n    \"\"\"Remove the underline for a string \n\n    Parameters\n    ----------\n    name : str\n        The string which contains the underline to be removed\n\n    Returns\n    -------\n    str\n        a string with none underline\n    \"\"\"\n    return name.replace('_',' ')\n\ndef create_plotly_graph_object(dataframe, fig, type_char, params, dataframe2):\n    \"\"\"Create each trace for a the 2 grid plotly graph according to the type of char for all the columns in a dataframe\n\n    Parameters\n    ----------\n    dataframe : pd.Dataframe\n        The dataframe \n    fig : plotly.graph_objs._figure.Figure\n        A figure with the grid \n    type_char : plotly.graph_objs._\n        A graph object from plotly it could be go.Violin, go.Histogram \n\n    Returns\n    -------\n    None\n    \"\"\"\n    if dataframe2 is None:\n        for num, col in enumerate(dataframe):\n            params['name'] = variable_remove_underline(col)\n            params['y'] = dataframe[col]\n            fig.add_trace(type_char(params),\n                                    row=(num\/\/2)+1, col=(num%2)+1)\n    else:\n        for num, col in enumerate(dataframe):\n            fig.append_trace(type_char(y= dataframe[col],\n                                       name = col + '_Att',\n                                      marker=dict(color='red')),\n                                    row=(num\/\/2)+1, col=(num%2)+1)\n            fig.append_trace(type_char(y= dataframe2[col],\n                                       name = col + '_Non-Att',\n                                      marker=dict(color='blue')),\n                                    row=(num\/\/2)+1, col=(num%2)+1)\n        \n\ndef make_2_col_grid(dataframe, type_char, params={}, dataframe2=None):\n    \"\"\"Create a 2 grid plotly graph according to the type of char for all the columns in a dataframe\n\n    Parameters\n    ----------\n    dataframe : pd.Dataframe\n        The dataframe \n    type_char : plotly.graph_objs._\n        A graph object from plotly it could be go.Violin, go.Histogram \n\n    Returns\n    -------\n    None\n    \"\"\"\n    num_rows = dataframe.shape[1]\n    total_rows = num_rows\/\/2+num_rows%2\n    fig = make_subplots(rows=total_rows, cols=2)\n    fig.update_layout(\n        autosize=False,\n        width=1000,\n        height=400*total_rows,\n        margin=dict(\n            l=50,\n            r=50,\n            b=100,\n            t=100,\n            pad=4\n        ),\n        yaxis=dict(tickformat=\",.2f\"),\n        xaxis=dict(tickformat=\",.2f\"),\n        paper_bgcolor=\"LightSteelBlue\",)\n    create_plotly_graph_object(dataframe, fig, type_char, params, dataframe2)\n    fig.show()","8e37502e":"params_violin = {'box_visible':True, \n                 'meanline_visible':True,\n                 'opacity':0.6,\n                'hovertemplate': 'y:%{y:20,.2f}'}\n\nmake_2_col_grid(df_numeric, go.Violin, params_violin)","738a3ddb":"for col in df.select_dtypes(include='O'):\n    print(df[col].value_counts(normalize=True).apply('{0:.3f}'.format), '\\n')","bf715c0d":"new_cat = {'Dependent_count':'object',\n             'Total_Relationship_Count':'object',\n             'Months_Inactive_12_mon':'object',\n             'Contacts_Count_12_mon':'object'}\n\nfor key, value in new_cat.items():\n    df[key] = df[key].astype(value)","c0f8b7db":"df_cat = df.select_dtypes(include='O')\nparams_hist = {'histnorm':'percent',\n              'legendgroup':True,}\nmake_2_col_grid(df_cat, go.Histogram, params_hist)","1f55c6e4":"df['Attrition_Flag'].value_counts(normalize = True, dropna = False)\ndf['Attrition_Flag'].replace({'Existing Customer': 0, 'Attrited Customer': 1}, inplace=True)","3641d979":"def create_corr_heatmap(method_tittle, df):\n    '''\n    '''\n    customc=[[0.0, '#FEC5BB'],\n             [0.5,'white'],\n             [1.0, '#FD9235']]\n    \n    corr_val = [[corr.iloc[i][j] if i>j else None for j,row in enumerate(df)] for i, col in enumerate(df)]\n    hovertext = [[f'corr({col}, {row})= {corr.iloc[i][j]:.3f}' if i>j else '' for j,row in enumerate(df)] for i, col in enumerate(df)]\n    \n    heat = go.Heatmap(z=corr_val,\n                  x=list(corr.index),\n                  y=list(corr.columns),\n                  xgap=1, ygap=1,\n                  colorscale= customc,\n                  colorbar_thickness=20,\n                  colorbar_ticklen=3,\n                    hovertext =hovertext,\n                  hoverinfo='text',\n                      zmin=-1, \n                      zmax=1\n                   )\n    \n    title = method_tittle + ' Correlation Matrix'\n    \n    layout = go.Layout(title_text=title, title_x=0.5, \n                   width=600, height=600,\n                   xaxis_showgrid=False,\n                   yaxis_showgrid=False,\n                   yaxis_autorange='reversed')\n    \n    fig=go.Figure(data=[heat], layout=layout)        \n    fig.show() ","cafd5d10":"df_numeric = df.select_dtypes(include='number')\ncorr = df.select_dtypes(include='number').corr()\ncreate_corr_heatmap('Pearson', corr)","12efe3dd":"att = df_numeric[df_numeric['Attrition_Flag']==1].drop(columns=['Attrition_Flag'])\nn_att = df_numeric[df_numeric['Attrition_Flag']==0].drop(columns=['Attrition_Flag'])\nmake_2_col_grid(att, go.Box, _, n_att)","1aaea3ec":"list_dim = [dict(label = i, values = df[i]) for i in df_numeric]\ntextd = ['Attrited Customer' if at==1 else 'Non-Attrited Customer' for at in df_numeric['Attrition_Flag']]\n\n\nfig = go.Figure(data=go.Splom(\n                dimensions=list_dim,\n                diagonal=dict(visible=False),\n                text=textd,\n                showupperhalf = False,\n                legendgroup=True,\n                marker=dict(color=df_numeric['Attrition_Flag'],\n                              size=2.5,\n                              colorscale='Bluered',\n                              line=dict(width=0.5,\n                                        color='rgb(230,230,230)'))))\n\ntitle = \"Scatterplot between all numeric variables\"\nfig.update_layout(title={'text': title,\n                        'xanchor': 'left',\n                        'yanchor': 'top',\n                        'font_size':30},\n                  dragmode='select',\n                  width=1500,\n                  height=1500,\n                  hovermode='closest',\n                 font=dict(size=8))\n                \n\nfig.show()","a516e41f":"def create_cat_df_stack(dataframe, target):\n    df_cat = dataframe.select_dtypes(include='O')\n    df_cat = pd.concat([df_cat,dataframe[target]], axis=1)\n    return df_cat\n\ndef create_cat_stacked_bars(dataframe, fig, target, target_map):\n    for num, col in enumerate(dataframe):\n        if col == target:\n            pass\n        else: \n            df_stack=df_cat.groupby([target,col]).size().reset_index()\n            df_stack.columns= [target, col, 'Counts']\n            df_stack['Percentage'] = df.groupby([target,col]).size().groupby(level=0).apply(lambda x: x\/float(x.sum())).values\n            if target_map is not None:\n                df_stack[target].replace(target_map, inplace=True)\n\n            trace = go.Figure(data = [go.Bar(x=label_df.Attrition_Flag, \n                                                     y=label_df.Percentage, \n                                                     name=label) \n                                                     for label, label_df in df_stack.groupby(col)])\n            row=(num\/\/2)+1\n            col=(num%2)+1\n            for i in trace['data']:\n                fig.add_trace(i,row,col)\n\n\ndef create_stacked_grid(dataframe_complete, target, target_map=None):\n    dataframe = create_cat_df_stack(dataframe_complete, target)\n    num_rows = dataframe.shape[1]-1\n    total_rows = num_rows\/\/2+num_rows%2\n    \n    names_fig = list(dataframe.columns)\n    names_fig = names_fig[:-1]\n    \n    fig = make_subplots(rows=total_rows, cols=2, subplot_titles= names_fig)\n    fig.update_layout(autosize=False,\n                        width=1000,\n                        height=400*total_rows,\n                        margin=dict(l=50,\n                                  r=50,\n                                  b=100,\n                                  t=100,\n                                  pad=4),\n                        paper_bgcolor=\"LightSteelBlue\",\n                        barmode=\"stack\",\n                        showlegend=False,\n                        title = target + ' vs categoricals',)\n    \n    for row in range(total_rows):\n        fig.update_yaxes(title_text=\"Percentage\", tickformat=\",.0%\", row=row, col=1)\n        fig.update_yaxes(title_text=\"Percentage\", tickformat=\",.0%\", row=row, col=2)\n        \n    create_cat_stacked_bars(dataframe,fig, target, target_map)\n    fig.show()","7f4cf574":"target_map = {0:'Existing Customer', 1: 'Attrited Customer'}\ncreate_stacked_grid(df, 'Attrition_Flag', target_map)","bff11684":"categories=[['Unknown','Uneducated', 'High School', 'College','Graduate','Post-Graduate', 'Doctorate'],\n           ['Unknown', 'Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'],\n           ['Blue','Silver','Gold','Platinum']]\n\ncategorical_ordinal_cat = ['Education_Level','Income_Category','Card_Category']\ncategorical_ordinal = ['Dependent_count','Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon']\n\ndf_target = df['Attrition_Flag'].reset_index(drop=True)\n\ndf_cat_ord = df[categorical_ordinal]\nenc_1 = OrdinalEncoder()\nenc_1.fit(df[categorical_ordinal])\ndf_cat_ord = pd.DataFrame(columns= df[categorical_ordinal].columns,\n                          data = enc_1.transform(df[categorical_ordinal]))\n\nenc_2 = OrdinalEncoder(categories=categories)\nenc_2.fit(df[categorical_ordinal_cat])\ndf_cat_enc = pd.DataFrame(columns= df[categorical_ordinal_cat].columns,\n                          data = enc_2.transform(df[categorical_ordinal_cat]))\n\n\ndf_cat_enc = pd.concat([df_cat_enc, df_cat_ord], axis = 1) \ndf_cat_enc = pd.concat([df_target,df_cat_enc], axis = 1) ","3679fb2e":"corr = df_cat_enc.corr(method='spearman')\ncreate_corr_heatmap('Spearman', corr)","7bdd9493":"def calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)\n\ncalc_vif(df_numeric.drop(columns = 'Attrition_Flag'))","ca491a57":"test_cat_cols = ['Gender',\n                'Total_Relationship_Count',\n                'Months_Inactive_12_mon',\n                'Contacts_Count_12_mon',\n                'Card_Category',\n                'Dependent_count']\n\ndf_cat_test = df[test_cat_cols]\n\nlabel_dict = defaultdict(LabelEncoder)\ndf_cat_test = df_cat_test.apply(lambda x: label_dict[x.name].fit_transform(x))\n\nchi_scores = chi2(df_cat_test, df['Attrition_Flag'])\nprint(chi_scores[1])\nresults = pd.DataFrame(data = [chi_scores[1],\n            ['Accepted H0' if i >= 0.05 else 'Rejected H0' for i in chi_scores[1] ]],\n             index = ['P-value','Result'],\n            columns = df_cat_test.columns).transpose()","ecc73414":"class OutlierRemover(BaseEstimator,TransformerMixin):\n    def __init__(self,factor=1.5, only_max=False):\n        self.factor = factor\n        self.only_max = only_max\n        self.saved_params = {}\n        \n    def outlier_removal_fit(self,X,y=None):\n        X_val = X.values\n        q1 = np.quantile(X_val, 0.25)\n        q3 = np.quantile(X_val, 0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - (self.factor * iqr)\n        upper_bound = q3 + (self.factor * iqr)\n        self.saved_params[X.name] = {'lower_bound': lower_bound,\n                                    'upper_bound': upper_bound}\n        \n    def outlier_removal_transf(self,X,y=None):\n        X = pd.Series(X).copy()\n        upper_bound = self.saved_params[X.name]['upper_bound']\n        lower_bound = self.saved_params[X.name]['lower_bound']\n        if self.only_max:\n            X.loc[((X > upper_bound))] = np.nan \n        else:\n            X.loc[((X < lower_bound) | (X > upper_bound))] = np.nan \n        return pd.Series(X)\n    \n    def fit(self,X,y=None):\n        if X.ndim == 1:\n            self.outlier_removal_fit(X)\n        else:\n            X.apply(self.outlier_removal_fit)\n        return self\n        \n    def transform(self,X,y=None):\n        if X.ndim == 1:\n            return self.outlier_removal_transf(X)\n        else:\n            return X.apply(self.outlier_removal_transf)\n        \noutlier_remover = OutlierRemover()\nol = outlier_remover.fit(df_numeric.drop(columns = 'Attrition_Flag'))\ndf_without_ol = ol.transform(df_numeric.drop(columns = 'Attrition_Flag'))\ndf_without_ol['Attrition_Flag'] = df_numeric['Attrition_Flag']\ndf_without_ol = df_without_ol.dropna()\nnum_drops = df_numeric.shape[0]-df_without_ol.shape[0]\nprint(f'Total drops = {num_drops}, {num_drops\/df_numeric.shape[0]:.2%} loss')","e4b589c9":"yj = PowerTransformer()\nyj.fit(df_without_ol.drop(columns = 'Attrition_Flag'))\ndf_gauss = pd.DataFrame(data = yj.transform(df_without_ol.drop(columns = 'Attrition_Flag')), \n             columns = df_without_ol.columns[:-1])","e05d7aae":"make_2_col_grid(df_gauss, go.Violin, params_violin)","32d8ec3d":"anova = f_classif(df_gauss,\n                  df_without_ol['Attrition_Flag'])\nanova = pd.DataFrame(anova, \n                     columns=df_gauss.columns,\n                     index = ['Predictive Power(F-score)','p-value'])\\\n                    .transpose().sort_values('Predictive Power(F-score)', \n                                              ascending=False)\nanova['H0_Rejected'] = anova['p-value']<0.05\nanova","91930618":"anova = f_classif(df_numeric.drop(columns='Attrition_Flag'),\n                  df_numeric['Attrition_Flag'])\nanova = pd.DataFrame(anova, \n                     columns=df_gauss.columns,\n                     index = ['Predictive Power(F-score)','p-value'])\\\n                    .transpose().sort_values('Predictive Power(F-score)', \n                                              ascending=False)\nanova['H0_Rejected'] = anova['p-value']<0.05\nanova","67d376fc":"pd.DataFrame(data = mutual_info_classif(df_gauss, df_without_ol['Attrition_Flag']), \n             index= df_gauss.columns,\n             columns = ['Mutual_Info']).sort_values('Mutual_Info', ascending = False)\n","3db16e11":"def AddNewFeatures(X):    \n    X['Change_ticket_Q4_Q1'] = np.where(X['Total_Ct_Chng_Q4_Q1'] == 0, 0, \n                                         X['Total_Amt_Chng_Q4_Q1']\/X['Total_Ct_Chng_Q4_Q1'])\n    X['Avg_Ticket'] = np.where(X['Total_Trans_Ct'] == 0, 0, \n                                         X['Total_Trans_Amt']\/X['Total_Trans_Ct'])\n\n    X['Est_RevBal_Q4_Q1'] =  X['Total_Revolving_Bal']*X['Total_Amt_Chng_Q4_Q1']\n    X['Credit_Limit_per_Month'] = X['Credit_Limit']\/X['Months_on_book']\n    X['Credit_Limit_per_Age'] = X['Credit_Limit']\/X['Customer_Age']\n    X['Contacts_per_product'] = X['Contacts_Count_12_mon']\/X['Total_Relationship_Count']\n    return X","ab73005b":"df_exp = df.copy()\ndf_numeric = df_exp.select_dtypes(include = 'number').drop(columns= 'Attrition_Flag')\ndf_numeric = ol.transform(df_numeric)\ndf_exp = pd.concat([df_exp[df_exp.select_dtypes(exclude = 'number').columns.union(['Attrition_Flag'])],\n           df_numeric], axis = 1)\ndf_exp = AddNewFeatures(df_exp)\ndf_exp = df_exp.dropna()","bf109307":"exp_cols = ['Attrition_Flag','Change_ticket_Q4_Q1', 'Avg_Ticket',\n            'Est_RevBal_Q4_Q1','Credit_Limit_per_Month', 'Credit_Limit_per_Age',\n            'Contacts_per_product']\nmake_2_col_grid(df_exp[exp_cols], go.Violin, params_violin)","5d8eccac":"att = df_exp[df_exp['Attrition_Flag']==1].drop(columns=['Attrition_Flag'])\nn_att = df_exp[df_exp['Attrition_Flag']==0].drop(columns=['Attrition_Flag'])\nmake_2_col_grid(att[exp_cols[1:]], go.Box, _, n_att[exp_cols[1:]])","aefb0d47":"anova = f_classif(df_exp[exp_cols].drop(columns=['Attrition_Flag']),\n                  df_exp['Attrition_Flag'])\nanova = pd.DataFrame(anova, \n                     columns=df_exp[exp_cols].columns[1:],\n                     index = ['Predictive Power(F-score)','p-value'])\\\n                    .transpose().sort_values('Predictive Power(F-score)', \n                                              ascending=False)\nanova['H0_Rejected'] = anova['p-value']<0.05\nanova","befc71c2":"def dropColumns(df):\n    col_to_drop = ['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n                 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n                            'CLIENTNUM']\n    return df.drop(columns= col_to_drop)\n\ndef catOrderEncoding(df, train, ordinal_encoder_cat=None):\n    \n    ordinal_cat_order = ['Education_Level',\n                       'Income_Category',\n                       'Card_Category']\n\n    categories=[['Unknown','Uneducated', 'High School', 'College','Graduate','Post-Graduate', 'Doctorate'],\n           ['Unknown', 'Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'],\n           ['Blue','Silver','Gold','Platinum']]\n    \n    temp_df = df[ordinal_cat_order]\n    \n    if train:\n        ordinal_encoder_cat = OrdinalEncoder(categories=categories)\n        ordinal_encoder_cat.fit(temp_df) \n    \n    df_ordinal_encoder_cat = pd.DataFrame(columns= temp_df.columns,\n                              data = ordinal_encoder_cat.transform(temp_df))\n    if train:\n        return df_ordinal_encoder_cat, ordinal_encoder_cat\n    else:\n        return df_ordinal_encoder_cat\n\n\ndef catAutoEncoding(df, train, ordinal_encoder_auto=None):\n    ordinal_cat_auto = ['Dependent_count',\n                        'Total_Relationship_Count',\n                        'Months_Inactive_12_mon',\n                        'Contacts_Count_12_mon']\n    \n    temp_df = df[ordinal_cat_auto]\n    \n    if train:\n        ordinal_encoder_auto = OrdinalEncoder()\n        ordinal_encoder_auto.fit(temp_df)\n    \n    df_ordinal_encoder_auto = pd.DataFrame(columns= temp_df.columns,\n                              data = ordinal_encoder_auto.transform(temp_df))\n    if train:\n        return df_ordinal_encoder_auto, ordinal_encoder_auto\n    else:\n        return df_ordinal_encoder_auto\n\ndef OheEncode(df, train, ohe_encoder=None):\n    label_enc_cols = ['Gender']\n    temp_df = df[label_enc_cols]\n    \n    if train: \n        ohe_encoder = OneHotEncoder()\n        if len(label_enc_cols) == 1:\n            ohe_encoder.fit(temp_df.values.reshape(-1,1))  \n        else:\n            ohe_encoder.fit(temp_df.reshape(-1,1))\n    \n    df_ohe_encoder = pd.DataFrame(ohe_encoder.transform(temp_df).toarray(),\n                                    columns=ohe_encoder.get_feature_names(label_enc_cols))\n    \n    if train:\n        return df_ohe_encoder, ohe_encoder\n    else:\n        return df_ohe_encoder\n\ndef encodeTarget(y):\n    target = y.replace({'Existing Customer': 0,\n                        'Attrited Customer': 1})\n    return target\n\ndef selectNumeric(df):\n    numeric_cols = ['Customer_Age',\n                   'Months_on_book',\n                   'Credit_Limit',\n                   'Total_Revolving_Bal',\n                   'Avg_Open_To_Buy',\n                   'Total_Amt_Chng_Q4_Q1',\n                   'Total_Trans_Amt',\n                   'Total_Trans_Ct',\n                   'Total_Ct_Chng_Q4_Q1',\n                   'Avg_Utilization_Ratio']\n    return df[numeric_cols].reset_index(drop=True) \n\ndef outlierRemoveDf(X):\n\n    outlier_cols = ['Customer_Age', 'Months_on_book', 'Credit_Limit', 'Total_Revolving_Bal',\n                   'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n                   'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\n\n    passthrough_cols = ['Education_Level', 'Income_Category', 'Card_Category', 'Gender_F',\n                       'Gender_M', 'Dependent_count', 'Total_Relationship_Count',\n                       'Months_Inactive_12_mon', 'Contacts_Count_12_mon']\n\n    numeric_pipeline = Pipeline([('outlier',OutlierRemover())])\n    preprocessor = ColumnTransformer(transformers=[('numeric_pipe', numeric_pipeline, outlier_cols),\n                                                   ('pass_cols', 'passthrough', passthrough_cols)])\n    \n    preprocessor.fit(X)\n    return preprocessor\n\ndef OutlierTransform(X_train, X_test, preprocessor):\n    X_train_col = X_train.columns \n    X_test_col = X_test.columns \n    X_train = pd.DataFrame(preprocessor.transform(X_train),\n                           columns=X_train.columns)\n    X_test = pd.DataFrame(preprocessor.transform(X_test),\n                           columns=X_test.columns)\n    return X_train, X_test\n\ndef AddNewFeaturesRefined(X):    \n    variables = ['Dependent_count','Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon']\n    \n    X['Change_ticket_Q4_Q1'] = np.where(X['Total_Ct_Chng_Q4_Q1'] == 0, 0, \n                                         X['Total_Amt_Chng_Q4_Q1']\/X['Total_Ct_Chng_Q4_Q1'])\n    X['Est_RevBal_Q4_Q1'] =  X['Total_Revolving_Bal']*X['Total_Amt_Chng_Q4_Q1']\n    \n    temp_df = pd.DataFrame(ordinal_encoder_auto.inverse_transform(X[variables]),\n                      columns = variables)\n    X['Contacts_per_product'] = temp_df['Contacts_Count_12_mon']\/temp_df['Total_Relationship_Count']\n    return X\n\ndef dropNa(X,y):\n    temp_df = pd.concat([X,y.reset_index(drop=True)],\n                  axis =1)\n    temp_df = temp_df.dropna()\n    y = temp_df['Attrition_Flag'].reset_index(drop=True)\n    X = temp_df.drop(columns = 'Attrition_Flag').reset_index(drop=True)\n    return X, y\n\ndef preProcess(df):\n    # Drop initial columns\n    df = dropColumns(df)\n    # Split-train test\n    y = df['Attrition_Flag']\n    X = df.drop(columns= 'Attrition_Flag')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    return X_train, X_test, y_train, y_test\n\ndef encode(X, y, ordinal_encoder_auto=None, ordinal_encoder_cat=None, ohe_encoder=None, train=True):\n    # Encode Columns\n    X = X.copy()\n    y = y.copy()\n    df_numeric = selectNumeric(X)\n    \n    if train:\n        df_ordinal_encoder_cat, ordinal_encoder_cat = catOrderEncoding(X, train)\n        df_ordinal_encoder_auto, ordinal_encoder_auto = catAutoEncoding(X, train)\n        df_ohe_encoder, ohe_encoder = OheEncode(X, train)\n    else:\n        df_ordinal_encoder_auto = catAutoEncoding(X, train, ordinal_encoder_auto)\n        df_ordinal_encoder_cat = catOrderEncoding(X, train, ordinal_encoder_cat)\n        df_ohe_encoder = OheEncode(X, train, ohe_encoder)\n        \n    X = pd.concat([df_numeric,\n                   df_ordinal_encoder_cat,\n                   df_ordinal_encoder_auto,\n                   df_ohe_encoder], axis = 1)\n    \n    y = encodeTarget(y)\n    \n    if train:\n        return X, y, ordinal_encoder_auto, ordinal_encoder_cat, ohe_encoder\n    else:\n        return X, y","c05687ed":"df = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\nX_train, X_test, y_train, y_test = preProcess(df)\nX_train, y_train, ordinal_encoder_auto, ordinal_encoder_cat, ohe_encoder = encode(X_train, y_train)\nX_test, y_test = encode(X_test, y_test, ordinal_encoder_auto, ordinal_encoder_cat, ohe_encoder, False)\npreprocessor = outlierRemoveDf(X_train)\nX_train, X_test = OutlierTransform(X_train, X_test, preprocessor)\nX_train, y_train = dropNa(X_train, y_train)\nX_test, y_test = dropNa(X_test, y_test)\nX_train = AddNewFeaturesRefined(X_train)\nX_test = AddNewFeaturesRefined(X_test)","0b8cc26f":"def modelGrid(X, Y, vainilla=False):\n    numeric_cols = ['Customer_Age','Months_on_book','Credit_Limit',\n                   'Total_Revolving_Bal','Avg_Open_To_Buy','Total_Amt_Chng_Q4_Q1',\n                   'Total_Trans_Amt','Total_Trans_Ct','Total_Ct_Chng_Q4_Q1',\n                   'Avg_Utilization_Ratio','Change_ticket_Q4_Q1',\n                    'Est_RevBal_Q4_Q1','Contacts_per_product']\n    if vainilla:\n        numeric_cols = numeric_cols[:-3]\n    \n    \n    passthrough_cols = ['Education_Level', 'Income_Category', 'Card_Category', 'Gender_F',\n                       'Gender_M', 'Dependent_count', 'Total_Relationship_Count',\n                       'Months_Inactive_12_mon', 'Contacts_Count_12_mon']\n    \n    #Random forest model\n    rf_params ={'classifier__min_samples_split':[50,75,100,250],\n               'classifier__max_depth':[*range(3,11)]}\n    rf_model = RandomForestClassifier()\n    \n    oversample=SMOTE(random_state=0)\n    \n    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n    scaler = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_cols),\n                                             ('pass', 'passthrough', passthrough_cols)])\n    \n    clf = ImbPipe(steps=[('scaler', scaler),\n                         ('oversampling', SMOTE()),\n                          ('classifier', rf_model)])\n    \n    if vainilla:\n        clf = ImbPipe(steps=[('scaler', scaler),\n                          ('classifier', rf_model)])\n    \n    n_folds = 5\n    scoring = ['f1','recall', 'precision']\n    gscv = GridSearchCV(clf, param_grid=rf_params, scoring=scoring, n_jobs=-1, cv=n_folds, refit='f1')\n    gscv.fit(X, Y)\n    \n    return gscv","7b11e143":"rf_model = modelGrid(X_train, y_train)\nresults = pd.DataFrame(rf_model.cv_results_)\nprint(results.shape)\nbest_results = results[(results['rank_test_f1'] <= 5) | \n                       (results['rank_test_recall'] <= 5) | \n                       (results['rank_test_precision'] <= 5)]\nbest_results","3ad275dc":"def confMatrix(model, X, y):\n    '''\n    Visualize a confusion matrix with the best parameters and CV score for the model\n   \n    Parameters\n    ----------\n    model : Sklearn trained model\n        The model already fitted\n    \n    Returns\n    -------\n    None\n\n    '''\n    from sklearn.metrics import confusion_matrix\n    print(\"Best parameter (CV score=%0.3f):\" % model.best_score_)\n    print(model.best_params_)\n    #Generate predictions with the model using our X values\n    y_pred = model.predict(X)\n    cm = confusion_matrix(y, y_pred)\n    #Get the confusion matrix\n    sns.heatmap(cm\/np.sum(cm), annot=True, \n            fmt='.2%', cmap='Blues')\n","63af26bd":"confMatrix(rf_model, X_train, y_train)","5e2126b9":"confMatrix(rf_model, X_test, y_test)","3f3c533b":"y_pred_train = rf_model.predict(X_train)\ny_pred_test = rf_model.predict(X_test)\n\nprint('Train:')\nprint(f'F1Score: {f1_score(y_train, y_pred_train):.2%}')\nprint(f'Recall: {recall_score(y_train, y_pred_train):.2%}')\nprint(f'Precision: {precision_score(y_train, y_pred_train):.2%}')\n\nprint('\\n Test:')\nprint(f'F1Score: {f1_score(y_test, y_pred_test):.2%}')\nprint(f'Recall: {recall_score(y_test, y_pred_test):.2%}')\nprint(f'Precision: {precision_score(y_test, y_pred_test):.2%}')","cbbd2b2a":"X_train, X_test, y_train, y_test = preProcess(df)\nX_train, y_train, ordinal_encoder_auto, ordinal_encoder_cat, ohe_encoder = encode(X_train, y_train)\nX_test, y_test = encode(X_test, y_test, ordinal_encoder_auto, ordinal_encoder_cat, ohe_encoder, False)","81bdbe92":"rf_model_vainilla = modelGrid(X_train, y_train, True)\nresults_vainilla = pd.DataFrame(rf_model_vainilla.cv_results_)\nprint(results_vainilla.shape)\nbest_results_vainilla = results_vainilla[(results_vainilla['rank_test_f1'] <= 5) | \n                       (results_vainilla['rank_test_recall'] <= 5) | \n                       (results_vainilla['rank_test_precision'] <= 5)]\nbest_results_vainilla","46bc86b9":"y_pred_train = rf_model_vainilla.predict(X_train)\ny_pred_test = rf_model_vainilla.predict(X_test)\n\nprint('Train:')\nprint(f'F1Score: {f1_score(y_train, y_pred_train):.2%}')\nprint(f'Recall: {recall_score(y_train, y_pred_train):.2%}')\nprint(f'Precision: {precision_score(y_train, y_pred_train):.2%}')\n\nprint('\\n Test:')\nprint(f'F1Score: {f1_score(y_test, y_pred_test):.2%}')\nprint(f'Recall: {recall_score(y_test, y_pred_test):.2%}')\nprint(f'Precision: {precision_score(y_test, y_pred_test):.2%}')","196f08c7":"Drop of the last 2 columns as suggested in the data description","1c84df59":"Before we want to start checking the relationship between variables we are going to explore a single variable at a time, with the Univariate study we want to summarize the variable and help us better understand the data, answering:\n* How many unique values are per column?\n* Is there any column in which duplicates doesn't make sense? In this case we will specifically look the CLIENTNUM column.\n* How many nulls are per column? For this dataset we can see that previously none of the variables contain null values.\n* How is the central tendency (mean, median, mode) for each numerical variable?\n* What are the labels (frequency of each category) composition for each categorical variables? ","a9d08a90":"With the previous function we can identify outliers and later on we can use it for the training\/test pipeline. As you can see we dropped almost 25% of our data, which isn't great however we are by no dropping by all columns, if there is any of the columns that doesn't add much value we can drop it and we maybe can have more than 6K cases we got right now. ","eeb7b549":"### Identifying multicolinearity\nLet's detect Multicoliniarity using VIF, \n\n<img src=\"https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200709102530\/VIF_formula.jpg\" width=\"250\">\n\n* VIF starts at 1 and has no upper limit\n* VIF = 1, no correlation between the independent variable and the other variables\n* VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others","2b5503a4":"# Exploratory Data Analysis (EDA)\n![1589258242_everybody-hates-you-but-i-love-you.png](attachment:528cc60e-e78b-4661-97ad-d8bf877ae599.png)\n\n\n\nFirst thing first, let's load the data!","a321fe42":"### Hypothesis testing\n<img src=\"attachment:183746a8-1df3-46e7-a44d-48eb9b5dc7da.png\" width=\"600px\">\n\nWe have made multiple hypthesis now it's the time to test them with the objective to have candidates to later on reduce the variables that aren't augmenting the model, reducing the number of variables will help us to:\n- Optimize CPU time, ram, memory, etc. \n- Avoid multicollinearity.\n- Sometimes after certain point, the performance of the model will decrease with the increasing number of elements as it will start overfitting.\n- It will be easier to explain why you model is predicting what is predicting (Ethical AI).\n- Less features also help to monitor less things after deploy (Yes... monitoring is really important).\n\nThat being said let's explore for our categorical and numerical variables the hyphotesis that we have made so far:","b81247f2":"We use Gridsearch for find the best params for the RF, within the Gridsearch we use a CrossValidation this will help us to avoid overfitting.\nAlso we optimized according to F1, combination of Recall and Precision since we don't have enough context of what is better and the for the company and what would cost any action related to churn customers. ","f8ac5712":"When we visually see the different between the means of the boxes for the variables of: the change in ticket, contacts per product and est. revolved balance Q4_Q1 and with the Anova we confirm that they are variables effectively variables with a highly predictive score. Please may note that we used the a dataframe without outliers for constructing these variables as we don't want to them for the model.  ","e30693f9":"# Multivariate study\n\n\n### Numerical variables\nLet's check the relationship of two variables concerning each other. This is done because it helps us to detect anomalies, understand de dependance of two variables on each other, the impact of each varaible within the target variable (giving us really good insights)","d2b994a8":"## Dataset general information \n\nThe first thing we want to do is to answer:\n* What is the shape of the dataset?\n* How many columns (features) are?\n* What are the type of features?","5b3ac7e1":"_\"Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\"_\n\nAs we have seen there are some features that doesn't add much value by themselves or with the current variables, therefore it's good to create new features that describe the structures inherent in the data. We then add the following variables:","9e8fe8c3":"Another good way to understand this information is to see Violin Plots, these let you combine a histograms and box plot in a visual. ","581b9f48":"# Credit Card Churn Prediction - Complete EDA - Modelling\n[Daniel Beltr\u00e1n](https:\/\/www.linkedin.com\/in\/danielbeltranpardo\/) - Apr 2021\n\n----------\nMotivation:\nThis notebook pretends to be a basic guide for doing a good Data Science focusing on the: Exploratory Data Analysis(EDA) and later on the modelling. Lately I'm working within the marketing enviroment in the financial sector so this datasets adapts really good to my needs, hope you find good quality of data science process, code and analysis. \n\nAlso as a personal challenge I will use Plotly for the visualization as much as I can (normally I use matplotlib\/seaborn).\n\n----------\n\n# Introduction\n\nNowadays the good use of data is constantly deciding whether a business catches up with the market or slips farther behind. Grasp a good understanding of the data combined with business goal and a deep understanding has the potential to allow companies to make choices that keep them ahead of the competition. \n\nOn top of that, every day it becomes clearer that data processing and interpretation have immense value\u2014and this is where a data scientist comes into play. While most executives are aware that data science is a glamorous industry and that data scientists are modern-day superheroes, many businesses still struggle to identify how to use analytics to take advantage of their data. \n\nTo address the misuse there have appear multiple project framework like SAP DMAIC (Define, Measure, Analyze, Improve and Control), Microsoft Team Data Science Process (TDSP), The 7 Steps of Machine Learning by Google and even Universal Workflow of Machine Learning from Fran\u00e7ois Chollet (Keras creator). Following these frameworks we conclude that on a high level scope the correct approach  to develop a project should be:\n1. Define a problem: Why is this important? What you want to accomplished? \n2. Business understanding: What metrics would be use to determine the success of the project? Where are the main data sources that you need to access\/obtain?\n3. Data Adquisition & Understading: How is the quality of the data? How is the data related to the target variables? to the rest of variables? How can you refresh and score the data regularly?\n4. Modelling: What solution would you propose? What is the optimal data features for the machine-learning model? Is the solution suitable for production? What are the most prominent ethical concerns? \n5. Deployment: How would you deploy the model? How would you operationalize the model for other applications to consume? \n6. Monitoring: How would you understand the changes that affect the system? How would you periodically retrain model to prevent model staleness? \n\nIn this notebook we will focus mainly (but not exhaustively) on the third and fourth point and we will infere the first and second, but please note that every step is critical to address the success of a machine learning problem.\n\n**_Let's begin!_**\n\n# Machine Learning project \n### Defining a problem \n\nWhat we want with this excercise is to identify correctly if a user will churn or not, this could be because:\n> It costs on average around 200 USD to acquire a credit card customer in the US (up to and beyond 1,000 USD if they are affluent cards like MasterCard World Elite and Visa Infinite)\" [Source 2020: here](https:\/\/medium.com\/unifimoney\/changing-the-vicious-cycle-of-push-to-pull-in-customer-acquisition-6218c8644000#:~:text=It%20costs%20on%20average%20around,World%20Elite%20and%20Visa%20Infinite)<hr>\n> The Apple Card doesn\u2019t need any affiliates or marketing yet analysts say that this new card has a customer acquisition cost of 350 USD and will take several years for Goldman Sachs to turn a profit on it. [Source 2019: here](https:\/\/runningwithmiles.boardingarea.com\/apple-card-customer-acquisition-cost\/)\n\nThat being said, it actually takes banks a few years to recover the acquisition investment of that customer and that's why they're motivated to predict which users are the most likely to churn, in order to proactively and not reactively retain the client.\n\n### Business understanding\nTo define the success of the solution that we will deliver let's define the metrics as: F1 Score, Precision and Recall. This metrics were chosen since normally churn problems are imbalanced, but all depends on the definition of churn and the cost driven by each scenario.\n\n#### Target Variable\nWe will use the attrition flag that is within the dataset as our target variable, we will train and test with this variable.\n\n### Data Understanding \n#### Why EDA? \nEDA is a fundamental step in the Data Understading Process, it gives a good comprehension about the dataset and turn an almost useable dataset into a completely useable dataset, giving in the process a good perception about the variables available and the relationship between them. \n\nWhat we want through this notebook is to understand which users are in risk of churn and how is the behaviour of the user that previously churned, this knowledge will help us later to build a robust model. To adress this we will follow the next steps:\n* <b>Univariable study<\/b>. We will just focus on the dependent variable ('Attrition_Flag') and try to know a little bit more about it, this will help us guiding the modelling. We will also explore the other variables to get an understanding about \n* <b>Multivariate study<\/b>. We will try to understand how the dependent variable and independent variables relate.\n* <b>Basic cleaning<\/b>. We will clean the dataset and handle the missing data, outliers and categorical variables.\n* <b>Test assumptions<\/b>. We will check if our data meets the assumptions required by most multivariate techniques.","6cc85161":"<img src=\"attachment:7ed30861-7845-4a6c-84db-ba918bff5a0d.png\" width=\"200px\">\n","3484f1ba":"From this we can get the sense of the  total data set, we got around 10K samples and a total of 21 columns most of the numerical and a few categorical. Also, we can see that the data set has no missing values (all the features got the same non-null values as the total samples).\n\nNow that we identify the the general dataset we get a better understanding of the dataset describing each variable, the description for this dataset identify the variables as the following, keep in mind that with this excercise we want to predict which user will churn.  ","64e9014b":"## Feature Engineering","c8a46fac":"There seems to be no nulls for the CLIENTNUM, then we're going to drop the columns since it doesn't add any value.\nFor the categorical columns there are less than 10 labels, which is good we don't want granular frequencies, Attrition also just contains 2 values.\n\nFor the numerical values the variables: Total_Relationship_Count, Dependent_count, Months_Inactive_12_mon and Contacts_Count_12_mon there few categories as expected for the rest of variables we will how is the central tendency.","df53d19e":"# Hyphotesis  \n\nSo far from the EDA we achieved to: \n- Grasp an understanding about the type of bank and they users.\n- Know that the dataset is imbalanced\n- Identify some high correlations inbetween some variables, maybe multicolinearity. \n- Formulate hyphotesis about some variables performing better than others (helping to classifying correctly Attrited vs Current users). \n\nHow to deal with some situations we got in this dataset: \n- To deal with he imbalanced dataset as we stated we will then use metrics different from accuracy, if that isn't enough we can try:\n    - Resampling the dataset, this means add copies of instances from the under-represented class called over-sampling or delete instances from the over-represented class, called under-sampling. For our case it's recommended to over-sample since we don't got more > 100K records.\n    - Try generate synthetic samples, the most popular of such algorithms is called SMOTE or the Synthetic Minority Over-sampling Technique. SMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies.\n- Identify if there is any multicolinearity using VIF, \u201dVIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable.\"\n- To correctly prove the hypothesis, we need to do some t-test and Chi-square test, this will help us to statistically identify if there is difference between the means for categorical and numerical variables.","e380d990":"Let me explain why I'm trying for each of the present variables:\n- Change in ticket between Q4_Q1 pretends to capture the change on the ticket size (Amount\/Count), therefore if Total_Amt_Chng_Q4_Q1 > Total_Ct_Chng_Q4_Q1, it indicates a increase for the Q4 on the ticket size and so on.\n- Avg_Ticket to make sense between the Amount and Count.\n- Est_RevBal_Q4_Q1 as an estimate on the revolving credit present at Q4, an Amount > 1 indicates more Revolving credit at the moment and an indicator < 1 less on it.\n- Credit_Limit_per_Month & Credit_Limit_per_Age, this is an experimental variable to see if there is a relationship between churn and the avg credit a user has per month\/age. Maybe they don't want a big credit line and it's affecting the finacial score for other products.\n- Contacts_per_product can give us a relationship between the contacts a user is doing per product, the number of contacts could be high because the user manage more products on the other hand multiple contacts being low on products could be a problem.\n","7e451fd0":"* The customer age mean is ~46 this means that the **data isn't from a Neobank and seems to be more a traditional bank**, also the min Customer Age is 26 which is good (there isn't any underage data) and the max Age is 70 which is a rational number.\n* Dependent count is between 0 and 5 with a mean of 2.3, which seems fine.\n* Months on book the min is 13 and max is 56, this indicates that the **data is for at least a 13 month old customers (aprox > 1 y.o.). This means that the model will be good predicting for customers that are at least 13 months old but not necessarily good for < 1 y.o. customers. or long term customers > 5 years.** \n* The least Relationship_count is 1, which seems fine at least 1 product customers. For the max product that a user got is 6, we will check if there is a relationship between the total relationship and the churn, _hypothetically users that aquire more products are less likely to churn._\n* Months inactive the least value is 0 the most is 6. _Hypothetically when a user get more inactive will be most likely to churn._\n* Contacts_Count_12_mon the are users which haven't had any contact and some users that have had 6 contacts, _the more the contacts could be that there is something wrong with the product (something not functioning properly) hence they will be more likely to churn._\n* The Credit_Limit seem to have some outliers in the right side, maybe is a particular client with a really good credit line.\n* The revolving balance there are some users which have 0 (they're not using the CC) and the most is using 2517 which seem to be _the amount revolved at the end of month is really low compared to credit line available_, we will later on analyze more this information. \n* The Avg_Open_To_Buy indicates the amount of credit available at a given time in this case we can se that the min is 3 someone got nearly all the credit line full and the max is the same as the max of the CL.\n* The Total_Amt_Chng_Q4_Q1 seem to be a percentage of the change for the transactions, **the mean is below 1 which indicates that people transact less in Q1 than Q4** this could be because at the EoY people use more the CC in the holidays.\n* Total_Trans_Amt indicates that 4.4K is the mean for the year approx 300 per month also there is a max value of 18K which seem to be an outlier. _Hypothetically the users that transact more shouldn't be churning unless there is sometype of incentive for a portfolio purchase services with discount from another financial institution or a bad user experience._\n* Total_Trans_Ct on average there are 65 tx, approx 5.4 per month. The max value is 139 more than the double of the mean, indicanting a posible outlier. _The more the transaction could indicate that they are users less likely to churn._\n* Total_Ct_Chng_Q4_Q1 seem to have a similar behaviour from the total amount changed.\n* Avg. Utilization of the CL is 0.27 but the median is 0.18 which could indicate a positive skew, also there seem to be outliers on the left side. **The percentile 25 is 0.02 indicates that 25% are hardly using the product.**","fbf94e1e":"In this case, we can see that some of the features have a low score like Customer Age, Credit Limit and Months on Book, and the best varaibles are Trans_Amt, Trans_Ct and Total_Ct_Chang_Q4. This make sense with previous analysis that we have done to data.","a6476ade":"With this Scatterplot matrix we want to visualize if there are any relationship that help us to explain the attrition flag. As you can see most of them is really difficult to differentiate between clearly any boundary where we can split Attrited and Non Attrited users, however, there are some in particular like Avg Utilization Ratio and Total_Ct_Chang_Q4_Q1, Total_Ct_Chang_Q4_Q1 and Total_Trans_Ct, Total_Trans_Ct and Total_Trans_Amount and many more. In general we can see Total_Ct_Chang_Q4_Q1, Total_Trans_Ct, Total_Trans_Amount seem to be the best variable to difference users.\n\n\n\n\nNow let's check for our categorical variables ","e279f9f8":"As you can see there are some variables like Dependent count, Total Relationship Count, Months in book, Months Inactive, Contact Count that are numerical but not continous, therefore we will classify them as categorical in an ordinal way (categorical variables that do have an order).\n\nNow we will analyze the rest of variables:","b01f60ee":"From the heatmap above we can conclude:\n* Attrition flag got a correlation with the: Avg_Utilization_Ratio, Total_CT_Chang_Q4_Q1, Total_Trans_Amt, Total_Amt_Chang_Q4_Q1, Total_Revolving_Bal. Let's remember that causation and correlation can exist at the same time, however correlation doesn't mean causation.\n* There are some variables like Avg_Utilization_Ratio & Credit_Limit , Credit_Limit & Avg_Open_To_Buy, Months_on_book & Customer_Age, and others that are highly correlated. This could be an issue since it can indicate multicollinearity, specially if we later on want to use based models (i.e. Linear Regression, SVM) in which the results and stability of the models and for extracting the feature importance.","1d0986c3":"Also we want to know how good is the combination between variables for a discrete target variable. To achieve that we will use the Mutual information, this which measures the dependency between the variables. The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances\n[Here the paper](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3929353\/)\n\nIf the result of a variable is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.","8a1cf32c":"#### Numerical\nWe will use a One-way ANOVA for comparing means of two or more samples using statistical significance, this will give us an insight about what are the best variables for classyfing. Let's remember that on an One-way ANOVA the test is the following:\n* Null Hypothesis (H0): $\\mu_{1} = \\mu_{2}$\n* Alternate Hypothesis (H1): $\\mu_{1} \\neq \\mu_{2}$\n\nOne of the challengues we got for using the ANOVA is that the assumptions that the data is normally distributed, for that we will need to remove the outliers and try to achieve a Gaussian distribution. ","58bf48bf":"We loaded again the dataframe, and we preprocess as the following:\n- Drop the columns \\[ClientNum, Naive_Bayes from the beginning]\n- Split between X_train, X_test, y_train, y_test\n- Encode all the variables \n- Identify outliers\n- Remove outliers\n- Drop new NaN identified from outliers\n- Add new features\n\nPlease may note that we fit on the initial training set and with that fit we are transforming X_train and also X_test.","03973277":"# Univariate study","310ef032":"### Feature information\n* CLIENTNUM: Client number. Unique identifier for the customer holding the account\n\n* Attrition_Flag: Internal event (customer activity) variable - if the account is closed then 1(Attrited Customer) else 0(Existing Customer)\n\n* Customer_Age: Demographic variable - Customer's Age in Years\n\n* Gender: Demographic variable - M=Male, F=Female\n\n* Dependent_count: Demographic variable - Number of dependents\n\n* Education_Level: Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.)\n\n* Marital_Status: Demographic variable - Married, Single, Divorced, Unknown\n\n* Income_Category: Demographic variable - Annual Income Category of the account holder (<  40\ud835\udc3e, 40K - 60K,  60\ud835\udc3e\u2212 80K,  80\ud835\udc3e\u2212 120K,> \\$ 120K, Unknown)\n\n* Card_Category: Product Variable - Type of Card (Blue, Silver, Gold, Platinum)\n\n* Months_on_book: Period of relationship with bank\n\n* Total_Relationship_Count: Total no. of products held by the customer\n\n* Months_Inactive_12_mon: No. of months inactive in the last 12 months\n\n* Contacts_Count_12_mon: No. of Contacts in the last 12 months\n\n* Credit_Limit: Credit Limit on the Credit Card\n\n* Total_Revolving_Bal: Total Revolving Balance on the Credit Card\n\n* Avg_Open_To_Buy: Open to Buy Credit Line (Average of last 12 months)\n\n* Total_Amt_Chng_Q4_Q1: Change in Transaction Amount (Q4 over Q1)\n\n* Total_Trans_Amt: Total Transaction Amount (Last 12 months)\n\n* Total_Trans_Ct: Total Transaction Count (Last 12 months)\n\n* Total_Ct_Chng_Q4_Q1: Change in Transaction Count (Q4 over Q1)\n\n* Avg_Utilization_Ratio: Average Card Utilization Ratio","5c9188e7":"# Machine Learning Pipelines\nThe main purpose is to codify and automate the workflow it takes to produce a machine learning model, as you have seen we have transformed the data, add new features, remove outliers and more. Since the main idea behind data science is to experiment, as in real life a good infrastructure will ramp the number of iterations you can test. \n\nMost of the times in the first iteration data scientists focus on producing a model to solve a single business problem and don't invest much time in building the architecture and tend to start with a manual workflow, however once they are integrated with production this type of work doesn't fit since the speed of  the iteration cycle or there is a risk being a manual process.\n\nThat being said we can construct pipelines that help us to pre-process and train.","11a6a806":"We will transform the Attrition flag from a categorical to a numerical data type, this will help us with the first multivariate studies. For the users that were attrited we will use the 1, and for the existing custumers we will use 0. Later on we want to predict which user were Attrited according to the rest of the variables. Also we want to explore how is the relationship between Attrited and Existing customers ","43cc1163":"As we can see the p-value for the variables that we mention earlier is very close to 0 (is not zero as you can see in the printed values) and for our other variables is > 0.05 which means that we cannot reject the null hypothesis. ","6614e864":"The inf for VIF shows a perfect correlation between two independent variables. In this case is a perfect correlation, we get R2 =1, which lead to 1\/(1-R2) -> infinity.\n\nAccording to the results there is multicolinearity present, we will then use **regularization** to deal with it.","d3e2dcd5":"According to the previous Spearman correlation matrix there isn't any correlation between the variables. With the target variable there is a correlation for the Contacts_Counts_12_mon, Months_Inactive_12_mon and slighly on the Total_relationship_count.\nSince there isn't any correlation","b8385ec4":"From this box plot matrix for each continous variable, there seem to be differences between Attrited vs Non-Attrited, for the following variables: Avg_Utilization, Total_Trans_ct, Total_Trans_Amount, Total_revolving_bal.","a6d32da8":"### Train and Out of sample population\nAs a rule, our objective is to produce models that will perform well not only on training data, but moreover on unseen information. Hence, we need to maintain a strategic distance from models that capture the peculiarities of the information we have accessible presently and avoid overfitting.\n\nBeing said so, we first want to split between our train and test sets,this will help as we said for not introducing any bias in the model later on.\n* The train set purpose is that the model observes and learns from this data and optimize its parameters.\n* The test set, provide an unbiased evaluation of a final model fit on the training dataset. It will be used only once the model is completely trained","2f5c0604":"#### Categorical \nWhen we want to compare the difference between means we can conduct a Chi-Square Test which determines whether there is a **statistically** significant relationship between categorical variables. We want to select the variables which are not independent from the target variable. For this test we define the hypothesis as the following:\n* Null Hypothesis (H0): Two variables are independent.\n* Alternate Hypothesis (H1): Two variables are not independent.\n\nWe will add the variables that we mentioned earlier and include some other to control that we effectively read correctly the previous graphs.","44f0968f":"----- Work in progress ----","cc856294":"As we see from this table the variables that could be the best for the model are the same that we named before Total_Trans_Ct, Total_Ct_Change_Q4_Q1 and Total_Revolving_Balance","f2e9f06c":"We used here a stack bar to compare the relationship between the Existing and Attrited users for each categorical variable.  \n\nIt appears to be that there isn't a notable difference with most of the socialdemographic variables (Dependent_count, Education_Level, Marital_Status) there isn't also a notable difference with the financial variables (income_category, card_category). However the varaibles that describe a relationship with the bank there is clearly a difference between Existing and Attrited users, as the following:\n* When the user got less products (relationship), they are more likely to churn.\n* When the user is inactive for longer periods of time, they are more likely to churn.\n* When the user had have more contacts in the last 12 months, they are more likely to churn.\n* The gender variable seem to have a slight skew for the males being the ones that are the attrited.\n\nFor the demographic where we got some unknowns there seem to no difference between the distributions, hence we can safetly drop the records where there are unknowns and not introduce any bias, for now we will not drop them but later on we got the option to safetly drop.","d2de43af":"* **Attrition flag distribution as the description said is imbalanced.** To clarify an imbalanced classification problem is one in which the distribution of examples across known groups is distorted or biased. The distribution can range from a small skew to an extreme disparity, with one example in the minority class for hundreds, thousands, or millions in the majority class.\n* Gender seem to be a balanced column, indicating good Credit Line Assignment practices.\n* In the Education Level most of frequencies are balanced except for Doctorade and Post-graduates. However **there is an unknown label**, for now we will leave it as it's. Maybe these are users that don't want to update their personal information and they're most likely to churn. \n* In the Maritial_Status most of the users are either Married (43%) or Single (39%), _indicating a low rate of divorce in the country._ **There appears again the unknown label.**\n* In the income category ~52% of the users earn below the 60K. **And there is also an unknown label**\n* In the card category most of the users got the Blue Category, this means the bank is risk conservative.","1ac25e3b":"### Categorical variables"}}