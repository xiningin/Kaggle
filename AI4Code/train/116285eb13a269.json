{"cell_type":{"64d9835d":"code","4ea43078":"code","e9081c5f":"code","6d108028":"code","d7ce598f":"code","ec482665":"code","b31073ce":"code","5345c222":"code","c3a1e973":"code","d42ceca8":"code","58927f0a":"code","8102903f":"code","13b609fc":"code","90f69bfe":"markdown","8f646dfd":"markdown","7cc00a6c":"markdown","9a52502d":"markdown","0decd7e0":"markdown","eb66b897":"markdown","041974e5":"markdown","c98e82ac":"markdown","205a6b5a":"markdown","3493197c":"markdown","9500e9d7":"markdown","02a130c9":"markdown","4f712aca":"markdown","243926c5":"markdown","6d54645a":"markdown"},"source":{"64d9835d":"\n# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n# !curl -X PURGE https:\/\/pypi.org\/simple\/kaggle-environments\n\n# ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments>=0.1.6'\nfrom kaggle_environments import evaluate, make, utils\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset, ConcatDataset, Subset    \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport math\nimport time\nimport os\nimport pickle\nimport seaborn\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\nif torch.cuda.is_available():\n    torch.cuda.current_device()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"running calculations on: \", device)","4ea43078":"env = make(\"connectx\", debug=True)\nconfiguration = env.configuration\ncolumns = configuration.columns\nmid_action = int(np.floor(columns\/2))\nrows = configuration.rows\nprint(configuration)","e9081c5f":"class Qnet(nn.Module):\n    def __init__(self, configuration):\n        super(Qnet, self).__init__()\n        self.columns = configuration.columns\n        self.rows = configuration.rows\n        # Number of Checkers \"in a row\" needed to win.\n        self.inarow = configuration.inarow\n        input_shape = (3, self.rows, self.columns)\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1)\n        self.relu1 = nn.ReLU()\n#         self.do1 = nn.Dropout2d()\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        self.relu2 = nn.ReLU()\n#         self.do2 = nn.Dropout2d()\n        linear_input_size = self._get_conv_output(input_shape)\n        self.lin1 = nn.Linear(linear_input_size, 64)\n        self.relu_lin1 = nn.ReLU()\n        self.Q_o = nn.Linear(64, self.columns)\n#         self.tanh = nn.Tanh()  # BUGGY\n    \n    def _get_conv_output(self, shape):\n        bs = 1\n        input = torch.autograd.Variable(torch.rand(bs, *shape))\n        output_feat = self._forward_features(input)\n        n_size = output_feat.data.view(bs, -1).size(1)\n        return n_size\n    \n    def _forward_features(self, x):\n#         x = F.relu(self.do1(self.conv1(x)))\n#         x = F.relu(self.do2(self.conv2(x)))\n        x = self.relu1(self.conv1(x))\n        x = self.relu2(self.conv2(x))\n        return x\n    \n    def forward(self, x):\n        x = self._forward_features(x)\n        x = self.relu_lin1(self.lin1(x.view(x.size(0), -1)))\n        Q = self.Q_o(x)\n#         Q = self.tanh(Q)  # BUGGY\n        return Q\n\n\ndef form_net_input(observation, configuration):\n        \"\"\"\n        Reshape board and one-hot it.\n        \"\"\"\n        # The current serialized Board (rows x columns).\n        board = observation.board\n        # Which player the agent is playing as (1 or 2).\n        mark = observation.mark\n        columns = configuration.columns\n        rows = configuration.rows\n        opponent = 2 if mark == 1 else 1\n        newboard = [1 if i == mark else 2 if i == opponent else 0 for i in board]\n#         x = torch.Tensor(newboard).view([1,1, rows, columns])\n        newboard = torch.tensor(newboard).reshape([rows, columns])\n        x = F.one_hot(newboard, 3).permute([2,0,1])\n        x = x.view([1, 3, rows, columns]).float()\n        return x.to(device)\n\n\ndef choose_action(observation, configuration, net, is_training=False, eps=None):\n    \"\"\"\n    epsilon-greedy agent.\n    \"\"\"\n    if is_training:\n        if random.random() < eps:\n            return random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n    Qs = net(form_net_input(observation, configuration)).cpu().detach().numpy().flatten()\n    return int(np.argmax([q_col if observation.board[col] == 0 else -np.inf for (col, q_col) in enumerate(Qs)]))\n\n\ndef eps_calculator(step, eps_start, eps_end, eps_decay):\n    \"\"\"\n    reduce epsilon with steps.\n    \"\"\"\n    return eps_end + (eps_start - eps_end) * math.exp(-2 * math.exp(1) * step \/ eps_decay)","6d108028":"def flip(t, dim=0):\n    \"\"\"\n    inputs:\n    t - torch tensor\n    dim - dimension to flip (currently only 1 dimension supported)\n    outputs:\n    t_flipped - input t with dimension dim flipped\n    \"\"\"\n    dim_size = t.size()[dim]\n    reverse_indices = torch.arange(dim_size-1, -1, -1, device=t.device)\n    return t.index_select(dim, reverse_indices)\n\n    \nclass Transition:\n    def __init__(self, *args):\n        if args is ():\n            self.episode = None\n            self.cur_obs = None\n            self.action = None\n            self.reward = None\n            self.next_obs = None\n            self.done = None\n            self.values = None\n        else:    \n            episode, cur_obs, action, reward, next_obs, done = args    \n            self.episode = torch.tensor([episode], device=device)\n            self.cur_obs = form_net_input(cur_obs, configuration)\n            self.action = torch.tensor([action], device=device)\n            reward_engineered = 1 if reward == 1 else -1 if reward == 0 else 0\n            self.reward = torch.tensor([reward_engineered], device=device)\n            self.next_obs = form_net_input(next_obs, configuration)\n            self.done = torch.tensor([done], device=device)\n            self.values = TensorDataset(self.episode, self.cur_obs, self.action,  \n                                        self.reward, self.next_obs, self.done)\n        \n    def mirror_copy(self):\n        \"\"\"\n        Creates a mirrored transition, flipping current and next state and the action taken.\n        \"\"\"\n        mirror_transition = Transition()\n        mirror_transition.episode = self.episode\n        mirror_transition.cur_obs = flip(self.cur_obs, 3)\n        mirror_transition.action = configuration.columns - 1 - self.action\n        mirror_transition.reward = self.reward\n        mirror_transition.next_obs = flip(self.next_obs, 3)\n        mirror_transition.done = self.done\n        mirror_transition.values = TensorDataset(\n            mirror_transition.episode, mirror_transition.cur_obs, mirror_transition.action,\n            mirror_transition.reward, mirror_transition.next_obs, mirror_transition.done)\n        return mirror_transition\n        \n\nclass ReplayMemory:\n    \"\"\"\n    Basic ReplayMemory class.\n    \"\"\"\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n\n    def __getitem__(self, item):        \n        return self.memory[item] \n\n    def push(self, *args):\n        \"\"\"Saves a transition.\"\"\"\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        transition = Transition(*args)\n        self.memory[self.position] = transition.values\n        self.position = (self.position + 1) % self.capacity\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)\n    \n    def to_dataset(self):\n        return ConcatDataset(self.memory)\n\n    \nclass DualMemory(ReplayMemory):\n    \"\"\"\n    Change push method to push an experience, generate its mirrored experience and push it as well.\n    \"\"\"\n    def push(self, *args):\n        \"\"\"Saves a transition and its symmetrical transition\"\"\"\n        # save original transition\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        transition = Transition(*args)\n        self.memory[self.position] = transition.values\n        self.position = (self.position + 1) % self.capacity\n        # save mirrored transition\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        self.memory[self.position] = transition.mirror_copy().values\n        self.position = (self.position + 1) % self.capacity\n","d7ce598f":"def optimize_step():\n    episode_training_loss = []\n    # sample transitions from replay memory\n    dataloader = DataLoader(memory.to_dataset(), BATCH_SIZE, shuffle=True, drop_last=False)\n    for epoch in range(NUM_EPOCHS):\n        epoch_loss = []\n        for batch_i, batch in enumerate(dataloader):          \n            _, cur_obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = batch\n            \n            # Qs of current observation, and choose index of action taken\n            Q_current_all = policy_net(cur_obs_batch)\n            mask = F.one_hot(action_batch, columns).bool()\n            Qs_to_step = Q_current_all.masked_select(mask).float()\n\n\n            # Qs of next observation, and choose maximum LEGAL action (not full column)\n            next_state_Qs = target_net(next_obs_batch).float()\n            next_state_illegal_actions = next_obs_batch[:, 0, 0, :].eq(0)  # check if column not full\n            next_state_Qs[next_state_illegal_actions] = -float('inf')\n            max_next_state_Qs = next_state_Qs.max(dim=1).values\n\n            # target to gradient descent (y from DQN paper).\n            y = reward_batch.float()\n            y[done_batch == False] += GAMMA * (- max_next_state_Qs[done_batch == False])  # the minus sign is the result of the zero-sum property\n            if y.sum() == float('inf') or y.sum() == -float('inf'):\n                raise Exception(\"Big Problem\")\n\n\n            # calculate loss\n            cur_train_loss = loss(y, Qs_to_step)\n            # save loss for plotting\n            epoch_loss.append(cur_train_loss.flatten().mean().cpu().detach().numpy())\n            # optimization\n            optimizer.zero_grad()\n            cur_train_loss.backward()\n            optimizer.step()\n           \n        training_loss.append(np.mean(epoch_loss))\n        episode_training_loss.append(np.mean(epoch_loss))\n    training_loss_per_episode.append(np.mean(episode_training_loss))","ec482665":"# learning parameters\nBATCH_SIZE = 32\nNUM_EPOCHS = 2\nTRAIN_WAIT_EPISODES = 100\nGAMMA = 0.999\nEPS_START = 0.9\nEPS_END = 0.1\nEPS_DECAY = 15000\nMEM_SIZE = 30000\nLEARNING_RATE = 5e-4\nPLAY_COUNT = 200001\nPRINT_CHECKPOINT = 100 * TRAIN_WAIT_EPISODES\nMAX_TRAIN_TIME = 60 * 60 * 8\ntimeout_reached = False\nload_net_params = True\nnet_load_path = '\/kaggle\/input\/pytorch-dqn-connectx\/net_params.pth'  # the folder name can be different obviously\nload_experience = True  # buggy if switched between cuda and cpu\nexperience_load_path = '\/kaggle\/input\/pytorch-dqn-connectx\/replay_memory.pkl'  # the folder name can be different obviously\n\n# model and agent initializtion\npolicy_net = Qnet(env.configuration)\nif load_net_params and os.path.exists(net_load_path):\n    print(\"Loading net parameters\")\n    policy_net.load_state_dict(torch.load(net_load_path, map_location=device))\npolicy_net.to(device)\npolicy_net.train()\ntarget_net = Qnet(env.configuration).to(device)\ntarget_net.load_state_dict(policy_net.state_dict())\ntarget_net.eval()\neps = EPS_START\nis_training = True\ndef policy_net_agent(observation, configuration):\n    return choose_action(observation, configuration, policy_net, is_training, eps)\n\n# optimizer and loss\noptimizer = torch.optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\nloss = nn.SmoothL1Loss()\n\n# replay memory\nif load_experience and os.path.exists(experience_load_path):\n    print(\"Loading replay memory\")\n    in_mem_file = open(experience_load_path, 'rb')\n    memory = pickle.load(in_mem_file)\n    in_mem_file.close()\nelse:\n    memory = DualMemory(MEM_SIZE)\n\nstep = 0\n\n# holders for plots\neps_list = []\ntraining_loss = []\ntraining_loss_per_episode = []\nfirst_move_qs_arr = []\n\n\nstart_time = time.time()\nprint(\"Starting training...\")\n\nfor episode in range(PLAY_COUNT):\n    env.reset()\n    eps = eps_calculator(episode, EPS_START, EPS_END, EPS_DECAY)\n    eps_list.append(eps)\n    episode_length = 0\n    while not env.done:\n        # determine active player\n        if env.state[0].status == 'ACTIVE':\n            player_id = 0\n        elif env.state[1].status == 'ACTIVE':\n            player_id = 1\n            \n        # acquire state\n        observation = env.state[player_id].observation\n        \n        # save initial Q values, and cut training if max time exceeded\n        if episode_length == 0 and episode % TRAIN_WAIT_EPISODES == 0:\n            # Save initial Q values\n            first_move_qs = np.around(policy_net(form_net_input(observation, configuration)).cpu().detach().numpy().flatten(), 3)\n            first_move_qs_arr.append(first_move_qs)\n            # cut training after timeout\n            if time.time() - start_time > MAX_TRAIN_TIME:\n                timeout_reached = True\n            \n        # choose and perform action\n        action = policy_net_agent(observation, configuration)\n        env.step([int(action) if i == player_id else None for i in [0, 1]])\n        episode_length += 1\n        \n        # acquire next state, reward and done\n        next_opponent_observation = env.state[1-player_id].observation  # notice we take the opponent id!\n        reward = env.state[player_id].reward\n        done = (env.state[player_id].status == 'DONE')\n        \n        # store transition in memory\n        memory.push(episode, observation, action, reward, next_opponent_observation, done)\n        \n    # optimize policy net, snd then update target net\n    if episode % TRAIN_WAIT_EPISODES == TRAIN_WAIT_EPISODES - 1:\n        optimize_step()\n        target_net.load_state_dict(policy_net.state_dict())\n    \n    # draw board and training time\n    if episode % PRINT_CHECKPOINT == 0 or timeout_reached:        \n        print(\"Games Played: {}\\nReplay memory size: {} transitions\\nTime passed: {:.1f} seconds\"\n              .format(episode + 1, len(memory), time.time()-start_time))\n        print(\"First move Q values:\", first_move_qs)\n        print(\"-\" * 20)\n        \n    if timeout_reached:\n        print(\"Timeout reached!\")\n        break\n        \nend_time = time.time()\nprint(\"Finished Training.\\nTotal training time = {:.1f} seconds\".format(end_time-start_time))\n\n# pickle replay memory\nout_mem_file = open(\"replay_memory.pkl\", 'wb')\npickle.dump(memory, out_mem_file)\nout_mem_file.close()","b31073ce":"# plots\nfig = plt.figure(figsize=(16,8))\nax = fig.add_subplot(121)\nax.plot(eps_list)\nax.grid()\nax.set_xlabel(\"episode\")\nax.set_ylabel(\"eps\")\nax.set_title(\"eps decay\")\n\n\nax2 = fig.add_subplot(122)\nax2.plot(training_loss_per_episode)\nax2.grid()\nax2.set_xlabel(\"training episode\")\nax2.set_ylabel(\"loss\")\nax2.set_title(\"loss\")\n\nfirst_move_qs_arr = np.array(first_move_qs_arr)\n\nfig2 = plt.figure(figsize=(16,8))\nax3 = fig2.add_subplot(121)\nax3.plot(first_move_qs_arr[:,mid_action])\nax3.grid()\nax3.axhline(color='red')\nax3.set_xlabel(\"training episode\")\nax3.set_ylabel(\"q-value\")\nax3.set_title(\"Q-value of middle action on first move\")\n\nax4 = fig2.add_subplot(122)\nax4.plot(first_move_qs_arr[:,mid_action] - np.concatenate([first_move_qs_arr[:, :mid_action], first_move_qs_arr[:, mid_action+1:]], axis=1).max(axis=1))\nax4.grid()\nax4.axhline(color='red')\nax4.set_xlabel(\"training episode\")\nax4.set_ylabel(\"q-value difference\")\nax4.set_title(\"First move Q-value difference between mid action and max non-mid action \")\n\n\nbar_fig, ax5 = plt.subplots(figsize = (10,8))\n\n\ndef update_qs_barplot(i):\n    title = \"Q-values of first move after {} training sessions\".format(i)\n    ax5.clear()\n    ax5.bar(np.arange(columns), first_move_qs_arr[i])\n    ax5.set_ylim(-0.5, 0.8)\n    ax5.grid()\n    ax5.axhline(color='red')\n    ax5.set_xlabel(\"action\")\n    ax5.set_ylabel(\"Q-value\")\n    ax5.set_title(title)\n\nanim = FuncAnimation(bar_fig, update_qs_barplot, frames=np.linspace(0, len(first_move_qs_arr)-1, 100, dtype='int'))\nanim.save('qs_barplot.gif', writer='imagemagick', fps=20)\n\nHTML(anim.to_jshtml()) ","5345c222":"is_ok = True\nfor param_tensor in policy_net.state_dict():\n    if torch.isnan(policy_net.state_dict()[param_tensor]).sum() > 0:\n        print(\"Error: nan in network parameters\")\n        is_ok = False\n        break\nif is_ok:\n    print(\"No nans in net parameters.\")","c3a1e973":"policy_net.eval()\nis_training = False\n\ndef mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [policy_net_agent, \"random\"], num_episodes=50)))\nprint(\"Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", policy_net_agent], num_episodes=50)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [policy_net_agent, \"negamax\"], num_episodes=5)))\nprint(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", policy_net_agent], num_episodes=5)))","d42ceca8":"# \"None\" represents which agent you'll manually play as (first or second player).\nenv.play([policy_net_agent, None], width=500, height=450)","58927f0a":"def agent_function(observation, configuration):\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import numpy as np\n    import random\n    import math\n    import base64\n    import io\n    import time\n\n    class Qnet(nn.Module):\n        def __init__(self, configuration):\n            super(Qnet, self).__init__()\n            self.columns = configuration.columns\n            self.rows = configuration.rows\n            # Number of Checkers \"in a row\" needed to win.\n            self.inarow = configuration.inarow\n            input_shape = (3, self.rows, self.columns)\n\n            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1)\n            self.relu1 = nn.ReLU()\n    #         self.do1 = nn.Dropout2d()\n            self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n            self.relu2 = nn.ReLU()\n    #         self.do2 = nn.Dropout2d()\n    #         def conv2d_size_out(size, kernel_size = 3, stride = 1):\n    #             return (size - (kernel_size - 1) - 1) \/\/ stride  + 1\n    #         convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(self.columns)))\n    #         convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(self.rows)))\n    #         linear_input_size = convw * convh * 32\n            linear_input_size = self._get_conv_output(input_shape)\n            self.lin1 = nn.Linear(linear_input_size, 64)\n            self.relu_lin1 = nn.ReLU()\n            self.Q_o = nn.Linear(64, self.columns)\n    #         self.tanh = nn.Tanh()  # BUGGY\n\n        def _get_conv_output(self, shape):\n            bs = 1\n            input = torch.autograd.Variable(torch.rand(bs, *shape))\n            output_feat = self._forward_features(input)\n            n_size = output_feat.data.view(bs, -1).size(1)\n            return n_size\n\n        def _forward_features(self, x):\n    #         x = F.relu(self.do1(self.conv1(x)))\n    #         x = F.relu(self.do2(self.conv2(x)))\n            x = self.relu1(self.conv1(x))\n            x = self.relu2(self.conv2(x))\n            return x\n\n        def forward(self, x):\n            x = self._forward_features(x)\n            x = self.relu_lin1(self.lin1(x.view(x.size(0), -1)))\n            Q = self.Q_o(x)\n    #         Q = self.tanh(Q)  # BUGGY\n            return Q\n\n    def form_net_input(observation, configuration):\n            # The current serialized Board (rows x columns).\n            board = observation.board\n            # Which player the agent is playing as (1 or 2).\n            mark = observation.mark\n            columns = configuration.columns\n            rows = configuration.rows\n            opponent = 2 if mark == 1 else 1\n            newboard = [1 if i == mark else 2 if i == opponent else 0 for i in board]\n    #         x = torch.Tensor(newboard).view([1,1, rows, columns])\n            newboard = torch.tensor(newboard).reshape([rows, columns])\n            x = F.one_hot(newboard, 3).permute([2,0,1])\n            x = x.view([1, 3, rows, columns]).float()\n            return x.to(device)\n        \n    def choose_action(observation, configuration, net):\n        Qs = net(form_net_input(observation, configuration)).cpu().detach().numpy().flatten()\n        return int(np.argmax([q_col if observation.board[col] == 0 else -np.inf for (col, q_col) in enumerate(Qs)]))\n    \n    device = torch.device('cpu')\n    policy_net = Qnet(configuration)\n    encoded_weights = \"\"\"\n    BASE64_PARAMS\"\"\"\n    decoded = base64.b64decode(encoded_weights)\n    buffer = io.BytesIO(decoded)\n    policy_net.load_state_dict(torch.load(buffer, map_location=device))\n    policy_net.eval()\n    return choose_action(observation, configuration, policy_net)\n","8102903f":"import inspect\nimport os\n\nno_params_path = \"submission_template.py\"\ndef append_object_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n        \ndef write_agent_to_file(function, file):\n    with open(file, \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(agent_function, no_params_path)\n\n# Write net parameters to submission file\n\nimport base64\nimport sys\n\ndict_path = \"net_params.pth\"\ntorch.save(policy_net.state_dict(), dict_path)\n\nINPUT_PATH = dict_path\nOUTPUT_PATH = 'submission.py'\n\nwith open(INPUT_PATH, 'rb') as f:\n    raw_bytes = f.read()\n    encoded_weights = base64.encodebytes(raw_bytes).decode()\n\nwith open(no_params_path, 'r') as file:\n    data = file.read()\n\ndata = data.replace('BASE64_PARAMS', encoded_weights)\n\nwith open(OUTPUT_PATH, 'w') as f:\n    f.write(data)\n    print('written agent with net parameters to', OUTPUT_PATH)\n","13b609fc":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nout = sys.stdout\ntry:\n    submission = utils.read_file(\"\/kaggle\/working\/submission.py\")\n    agent = utils.get_last_callable(submission)\nfinally:\n    sys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","90f69bfe":"# Plots","8f646dfd":"> # Evaluate your agent","7cc00a6c":"> # Check for nans in net parameters","9a52502d":"# Replay Memory","0decd7e0":"> # Play your Agent\nClick on any column to place a checker there (\"manually select action\").","eb66b897":"# Validate Submission\nPlay your submission against itself.  This is the first episode the competition will run to weed out erroneous agents.\n\nWhy validate? This roughly verifies that your submission is fully encapsulated and can be run remotely.","041974e5":"# Submit to Competition\n\n1. Commit this kernel.\n2. View the commited version.\n3. Go to \"Data\" section and find submission.py file.\n4. Click \"Submit to Competition\"\n5. Go to [My Submissions](https:\/\/kaggle.com\/c\/connectx\/submissions) to view your score and episodes being played.","c98e82ac":"# Create ConnectX Environment","205a6b5a":"# Model Definition and Utility Functions","3493197c":"> # Create agent function","9500e9d7":"# Optimization step","02a130c9":"# Pytorch DQN Implementation for ConnectX\nIn this notebook I implemented Deep Q-Learning as described in the original paper \"Playing Atari with Deep Reinforcement Learning\" by Mnih et al., with a few changes:  \n1) I implemented Double-DQN, which means I seperated the policy network from the target network, and updated the target network once every few games. This helps to reduce the overestimation of Q values and thus allows for less noise during training.  \n2) Using the zero-sum property of ConnectX, I altered the target Q values to be the negative of the best opponent Q value. The logic behind this is that we want to take actions that leave our opponent in a bad place - if all of his Q values are low in the state that our action lead to - it is a good action.  \n3) Using the left-right symmetry of ConnectX, I implemented a dual Replay Memory, that stores each experience but also its mirror image, effectively doubling the data and teaching the agent about the game's inherent symmetry.  \n\nSources:  \n* Huge thanks to Tom Van de Wiele and his notebook that helped me sort out some things that initially didn't work (specifically, how to train directly from the environment without the trainer, thus using both players to train and enabling the use of the zero-sum property). Check it out and upvote it: https:\/\/www.kaggle.com\/c\/connectx\/discussion\/129145  \n* Pytorch REINFORCEMENT LEARNING (DQN) TUTORIAL: https:\/\/pytorch.org\/tutorials\/intermediate\/reinforcement_q_learning.html  \n* Neil Slater on how to import torch models into the submission: https:\/\/www.kaggle.com\/c\/connectx\/discussion\/126678, https:\/\/www.kaggle.com\/c\/connectx\/discussion\/133280","4f712aca":"# Write Submission File\n\n","243926c5":"# Imports","6d54645a":"# Self play and training"}}