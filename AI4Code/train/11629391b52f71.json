{"cell_type":{"b6568e5f":"code","e36eb2c5":"code","a0f188f2":"code","e71c142b":"code","f341de5a":"code","59aed923":"code","5cca67af":"code","7d800d06":"code","338517bc":"code","837c88bd":"code","12ec7bc0":"code","914272da":"code","efef79d9":"code","7f6c8fdf":"code","22b4a315":"code","b8759494":"code","05fb17eb":"code","0363d302":"code","7fce3849":"code","9b7fbffe":"code","6ba55625":"code","0ff15624":"code","45085d59":"code","a8949d91":"code","f4cb58a2":"code","bda65a2a":"code","aed059d3":"code","5dda845a":"code","11873f96":"code","aa158332":"code","5db0e57c":"code","a58d21ac":"code","2f7ab9aa":"code","2593911c":"code","d08f2bea":"code","9b92b0d0":"code","abd3251e":"code","1597140b":"code","ca471b82":"code","c419c14b":"code","237e971f":"code","d109fea1":"code","07dac7ae":"code","b65b95bd":"code","ef2ef59f":"code","b9a49af1":"code","69db0516":"code","b20ab952":"code","82cecc4b":"code","036e1576":"code","cfe0a45e":"code","61337ba9":"code","1a991eaf":"code","b6482962":"code","39932221":"code","8b49374b":"code","39a55362":"code","99682058":"code","16b1320a":"code","ac27bcaf":"code","2c512c94":"code","66d485db":"code","432abd3d":"code","1ec38497":"code","aa010ad0":"code","9a90a2eb":"code","85c5ea28":"code","9d2c9f23":"code","8735cf26":"code","71324fa7":"code","070b2fc7":"code","d2fb2d14":"code","aa646e0e":"code","29a14357":"code","f6bd0c8f":"code","b8a50305":"code","4f2e24c1":"code","cc21ba6c":"code","eb5a58c7":"code","09e55f68":"code","22a96d0d":"code","62982fd5":"code","a9b49f94":"code","e0c32a85":"code","448c246e":"code","241ceecd":"code","55c3eb65":"code","ccd48c20":"code","a2dc4c5a":"code","16ec163f":"code","dc54f03e":"code","0481bace":"code","0988bda2":"code","9f4a9cd5":"code","e133f871":"code","c1aaa6cd":"code","1b005d65":"code","27a5aae8":"code","d72606c0":"code","796fb9f7":"code","dfbbb368":"code","7b6cb4a6":"markdown","13be3193":"markdown","98ba0f6f":"markdown","001d6e2e":"markdown","d43376c0":"markdown","3d940ae5":"markdown","c0828200":"markdown","7a0a4978":"markdown","2755e362":"markdown","bb960f29":"markdown","6df9c939":"markdown","7d58b1b8":"markdown","5be5289f":"markdown","3c194682":"markdown","46ea8019":"markdown","0b51efdf":"markdown","608431fa":"markdown","12e339dc":"markdown","3a595c69":"markdown","8a3f590c":"markdown","a85cd834":"markdown","57ecd35d":"markdown","5060b2b3":"markdown","858030d7":"markdown","1f52a3bb":"markdown","3b125f0a":"markdown","d2c04653":"markdown","4b799b56":"markdown","d1241648":"markdown","055d7629":"markdown","74ffba8c":"markdown","372c3c95":"markdown","6fd3ff3d":"markdown","b8bb52e4":"markdown","9e4fb4a2":"markdown","df012845":"markdown","93b598fd":"markdown","a2b7abee":"markdown","753d1c5c":"markdown","ab9d2817":"markdown"},"source":{"b6568e5f":"import pandas as pd\nimport numpy as np","e36eb2c5":"import matplotlib.pyplot as plt\nimport seaborn as sns","a0f188f2":"%matplotlib inline","e71c142b":"df = pd.read_csv('..\/input\/data-train-test\/TrainData.csv', encoding='latin-1', sep=\";\")","f341de5a":"df.head()","59aed923":"# pd.get_dummies(df, drop_first=True).head()\n# drop der ersten Dummy Var nicht sinnvoll\n# pd.get_dummies(df).head()\n\n# nicht-nummerische Inhalte in nummerische (bin\u00e4re) Informationen umwandeln\ndf = pd.get_dummies(df)","5cca67af":"# Spalten von pd.get_dummies(df) anzeigen\ndf.describe()\n","7d800d06":"# redundante Infos droppen\ndf = df.drop(['Zielvariable_nein', 'Geschlecht_w', 'Art der Anstellung_Unbekannt', 'Familienstand_geschieden', 'Schulabschlu\u00df_Unbekannt', 'Ausfall Kredit_nein', 'Haus_nein', 'Kredit_nein', 'Kontaktart_Unbekannt', 'Ergebnis letzte Kampagne_Unbekannt'\n], axis=1)","338517bc":"# NaN-Werte in Spalte 'Tage seit letzter Kampagne' sind beim Plotten ein Problem -> durch mean (Mittelwert) ersetzten\n# sns.heatmap(df.isnull(), yticklabels=False, cbar=Fals, cmap='viridis')\ndf = df.groupby(df.columns, axis = 1).transform(lambda x: x.fillna(x.mean()))\n\n# Aufbau des dataframes pr\u00fcfen\n#df.info()","837c88bd":"# Statistische Infos zu dem dataframe\n# df.describe()","12ec7bc0":"# Plotten mit pairplot macht keinen Sinn - zu viele Spalten und Daten\n# sns.pairplot(df)\n# sns.pairplot(df.sample(1000))","914272da":"# Verteilung der angegebenen Spalte zeichnen\n# sns.distplot(df['Zielvariable_ja'])","efef79d9":"# Verteilung der angegebenen Spalte zeichnen\n# sns.distplot(df['Tag'])","7f6c8fdf":"# Verteilung der angegebenen Spalte zeichnen\n# sns.distplot(df['Dauer'])","22b4a315":"# Verteilung der angegebenen Spalte zeichnen\n# sns.distplot(df['Alter'])","b8759494":"# Verteilung der angegebenen Spalte zeichnen\n# sns.distplot(df['Kontostand'])","05fb17eb":"# Verteilung der angegebenen Spalte zeichnen\n# sns.distplot(df['Anzahl der Ansprachen'])","0363d302":"# Verteilung der angegebenen Spalte zeichnen\n# sns.distplot(df['Tage seit letzter Kampagne'])","7fce3849":"# Verteilung der angegebenen Spalte zeichnen\n# sns.distplot(df['Anzahl Kontakte letzte Kampagne'])","9b7fbffe":"# Korrelationsmatrix eingrenzen auf angegebenen Features\n#df2 = df.loc[:, ['Dauer', 'Alter', 'Kontostand', 'Anzahl der Ansprachen', 'Tage seit letzter Kampagne', 'Anzahl Kontakte letzte Kampagne', 'Geschlecht_m', 'Kontaktart_Festnetz', 'Kontaktart_Handy', 'Ausfall Kredit_ja', 'Zielvariable_ja']]\n# df2.corr()","6ba55625":"# sns.heatmap(df2.corr())","0ff15624":"# Korrelationsmatrix eingrenzen auf angegebenen Features\n#df3 = df.loc[:, ['Art der Anstellung_Arbeiter', 'Art der Anstellung_Arbeitslos', 'Art der Anstellung_Dienstleistung', 'Art der Anstellung_Gr\u00fcnder', 'Art der Anstellung_Hausfrau', 'Art der Anstellung_Management', 'Art der Anstellung_Rentner', 'Art der Anstellung_Selbst\u00e4ndig', 'Art der Anstellung_Student',\n#'Art der Anstellung_Technischer Beruf', 'Art der Anstellung_Verwaltung', 'Zielvariable_ja']]\n# df3.corr()","45085d59":"# sns.heatmap(df3.corr())","a8949d91":"# Korrelationsmatrix eingrenzen auf angegebenen Features\n#df4 = df.loc[:, ['Familienstand_single', 'Familienstand_verheiratet', 'Schulabschlu\u00df_Abitur', 'Schulabschlu\u00df_Real-\/Hauptschule', 'Schulabschlu\u00df_Studium', 'Zielvariable_ja']]\n# df4.corr()","f4cb58a2":"# sns.heatmap(df4.corr(), annot=True)","bda65a2a":"# Korrelationsmatrix eingrenzen auf angegebenen Features\n#df5 = df.loc[:, ['Ausfall Kredit_ja', 'Haus_ja', 'Kredit_ja', 'Zielvariable_ja']]\n# df5.corr()","aed059d3":"# sns.heatmap(df5.corr(), annot=True)","5dda845a":"# Korrelationsmatrix eingrenzen auf angegebenen Features\n#df6 = df.loc[:, ['Ergebnis letzte Kampagne_Erfolg', 'Ergebnis letzte Kampagne_Kein Erfolg', 'Ergebnis letzte Kampagne_Sonstiges', 'Zielvariable_ja']]\n# df6.corr()","11873f96":"# sns.heatmap(df6.corr(), annot=True)","aa158332":"# Korrelationsmatrix eingrenzen auf angegebenen Features\n#df7 = df.loc[:, ['Monat_apr', 'Monat_aug', 'Monat_dec', 'Monat_feb', 'Monat_jan', 'Monat_jul', 'Monat_jun', 'Monat_mar', 'Monat_may', 'Monat_nov', 'Monat_oct', 'Monat_sep', 'Zielvariable_ja']]\n# df7.corr()","5db0e57c":"# sns.heatmap(df7.corr())","a58d21ac":"# \u00dcberblick der Anzahl Kunden, die das Produkt in der Kampagne abgeschlossen haben\n# sns.countplot(x='Zielvariable_ja', hue='Geschlecht_m', data=df)","2f7ab9aa":"# \u00dcberblick Geschlecht\n# sns.countplot(x='Geschlecht_m', data=df)","2593911c":"# \u00dcberblick Ausfall Kredit\n# sns.countplot(x='Ausfall Kredit_ja', hue='Geschlecht_m', data=df)","d08f2bea":"# \u00dcberblick \n# sns.countplot(x='Zielvariable_ja', hue='Kontaktart_Handy', data=df)","9b92b0d0":"# \u00dcberblick \n# sns.countplot(x='Zielvariable_ja', hue='Ergebnis letzte Kampagne_Erfolg', data=df)","abd3251e":"# sns.boxplot(x='Zielvariable_ja', y='Alter', data=df)","1597140b":"# sns.boxplot(x='Zielvariable_ja', y='Dauer', data=df)","ca471b82":"# sns.boxplot(x='Zielvariable_ja', y='Kontostand', data=df)","c419c14b":"# sns.boxplot(x='Zielvariable_ja', y='Anzahl der Ansprachen', data=df)","237e971f":"#df.drop(['Stammnummer', 'Tag', 'Anruf-ID'], axis=1, inplace=True)\ndf.drop(['Tag', 'Anruf-ID'], axis=1, inplace=True)","d109fea1":"df.head()","07dac7ae":"# Aufsplitten der Features in erkl\u00e4rende Variablen und Zielvariable\nX = df.drop(['Zielvariable_ja', 'Stammnummer'], axis=1)\ny = df['Zielvariable_ja']","b65b95bd":"from sklearn.cross_validation import train_test_split","ef2ef59f":"# Aufsplitten in Training set (70%) und Test set (30%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","b9a49af1":"from sklearn.linear_model import LogisticRegression","69db0516":"# initiieren des Models (Parameterauswahl - siehe unten Logistic Regression-2)\nlogmodel = LogisticRegression(penalty='l1', C=100)","b20ab952":"# Trainings Daten ins Model laden\nlogmodel.fit(X_train, y_train)","82cecc4b":"predictions = logmodel.predict(X_test)\npredictions_prob = logmodel.predict_proba(X_test)[:, 1]","036e1576":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import auc","cfe0a45e":"[fpr, tpr, thr] = roc_curve(y_test, predictions_prob)\nprint('Train\/Test split results:')\nprint(logmodel.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, predictions))\nprint(logmodel.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, predictions_prob))\nprint(logmodel.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\nprint(logmodel.__class__.__name__+\" roc_auc_score is %2.3f\" % roc_auc_score(y_test, logmodel.predict(X_test)))\nprint(logmodel.__class__.__name__+\" mean_squared_error is %2.3f\" % mean_squared_error(y_test, logmodel.predict(X_test)))\nprint(logmodel.__class__.__name__+\" f1_score is %2.3f\" % f1_score(y_test, logmodel.predict(X_test)))\nprint(logmodel.__class__.__name__+\" precision_score is %2.3f\" % precision_score(y_test, logmodel.predict(X_test)))\nprint(logmodel.__class__.__name__+\" r2_score is %2.3f\" % r2_score(y_test, logmodel.predict(X_test)))\n\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))","61337ba9":"##for i in range(len(X_test)):\n##\tprint(\"df['Zielvariable_ja']=%s, Predicted=%s\" % (df['Zielvariable_ja'][i], predictions[i]))","1a991eaf":"from sklearn.metrics import classification_report","b6482962":"print(classification_report(y_test, predictions))","39932221":"from sklearn.metrics import confusion_matrix","8b49374b":"confusion_matrix(y_test, predictions)","39a55362":"df.head()","99682058":"from sklearn.tree import DecisionTreeClassifier","16b1320a":"dtree = DecisionTreeClassifier()","ac27bcaf":"dtree.fit(X_train, y_train)","2c512c94":"predictions = dtree.predict(X_test)\npredictions_prob = dtree.predict_proba(X_test)[:, 1]","66d485db":"[fpr, tpr, thr] = roc_curve(y_test, predictions_prob)\nprint('Train\/Test split results:')\nprint(logmodel.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, predictions))\nprint(logmodel.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, predictions_prob))\nprint(logmodel.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\nprint(logmodel.__class__.__name__+\" roc_auc_score is %2.3f\" % roc_auc_score(y_test, logmodel.predict(X_test)))\nprint(logmodel.__class__.__name__+\" mean_squared_error is %2.3f\" % mean_squared_error(y_test, logmodel.predict(X_test)))\nprint(logmodel.__class__.__name__+\" f1_score is %2.3f\" % f1_score(y_test, logmodel.predict(X_test)))\nprint(logmodel.__class__.__name__+\" precision_score is %2.3f\" % precision_score(y_test, logmodel.predict(X_test)))\nprint(logmodel.__class__.__name__+\" r2_score is %2.3f\" % r2_score(y_test, logmodel.predict(X_test)))\n\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))","432abd3d":"from sklearn.metrics import classification_report, confusion_matrix","1ec38497":"print(confusion_matrix(y_test, predictions))\nprint ('\\n')\nprint(classification_report(y_test, predictions))\n# Fehler liegt bei 574 + 690","aa010ad0":"from sklearn.ensemble import RandomForestClassifier","9a90a2eb":"#rfc = RandomForestClassifier(n_estimators = 300, oob_score = True, n_jobs = -1,random_state =50, max_features = \"auto\", min_samples_leaf = 1)\nrfc = RandomForestClassifier(n_estimators = 100, max_depth= 25, max_features = 20, min_samples_leaf = 3)","85c5ea28":"rfc.fit(X_train, y_train)","9d2c9f23":"rfc_pred = rfc.predict(X_test)\nrfc_pred_prob = rfc.predict_proba(X_test)[:, 1]\n# print(rfc_pred_prob)\n\n#for i in range(len(X_test)):\n#    print(\"df['Zielvariable_ja']=%s, Predicted=%s\" % (df['Zielvariable_ja'][i], rfc_pred[i]))\n#    print(\"df['Zielvariable_ja']=%s, Predicted prob=%s\" % (df['Zielvariable_ja'][i], rfc_pred_prob[i]))\n\n# X_test.info()","8735cf26":"[fpr, tpr, thr] = roc_curve(y_test, rfc_pred_prob)\nprint('Train\/Test split results:')\nprint(logmodel.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, rfc_pred))\nprint(logmodel.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, rfc_pred_prob))\nprint(logmodel.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\nprint(logmodel.__class__.__name__+\" roc_auc_score is %2.3f\" % roc_auc_score(y_test, rfc_pred))\nprint(logmodel.__class__.__name__+\" mean_squared_error is %2.3f\" % mean_squared_error(y_test, rfc_pred))\nprint(logmodel.__class__.__name__+\" f1_score is %2.3f\" % f1_score(y_test, rfc_pred))\nprint(logmodel.__class__.__name__+\" precision_score is %2.3f\" % precision_score(y_test, rfc_pred))\nprint(logmodel.__class__.__name__+\" r2_score is %2.3f\" % r2_score(y_test, rfc_pred))\n\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))","71324fa7":"print(confusion_matrix(y_test, rfc_pred))\nprint ('\\n')\nprint(classification_report(y_test, rfc_pred))\n# Fehler liegt bei 488 + 249","070b2fc7":"df_test = pd.read_csv('TestData.csv', encoding='latin-1', sep=\";\")","d2fb2d14":"df_test = pd.get_dummies(df_test)","aa646e0e":"df_test.columns","29a14357":"# redundante Infos droppen\ndf_test = df_test.drop(['Zielvariable', 'Geschlecht_w', 'Art der Anstellung_Unbekannt', 'Familienstand_geschieden', 'Schulabschlu\u00df_Unbekannt', 'Ausfall Kredit_nein', 'Haus_nein', 'Kredit_nein', 'Kontaktart_Unbekannt', 'Ergebnis letzte Kampagne_Unbekannt'\n], axis=1)","f6bd0c8f":"# NaN-Werte in Spalte 'Tage seit letzter Kampagne' sind beim Plotten ein Problem -> durch mean (Mittelwert) ersetzten\n# sns.heatmap(df.isnull(), yticklabels=False, cbar=Fals, cmap='viridis')\ndf_test = df_test.groupby(df_test.columns, axis = 1).transform(lambda x: x.fillna(x.mean()))","b8a50305":"# Aufbau des dataframes pr\u00fcfen\ndf_test.info()","4f2e24c1":"df_test.drop(['Tag', 'Anruf-ID'], axis=1, inplace=True)","cc21ba6c":"df_test_copy = df_test.copy()","eb5a58c7":"df_test.drop(['Stammnummer'], axis=1, inplace=True)\ndf_test.head()","09e55f68":"df_test_copy.head()","22a96d0d":"df_test.info()","62982fd5":"rfc_pred = rfc.predict(df_test)\nrfc_pred_prob = rfc.predict_proba(df_test)[:, 1]\n# print(rfc_pred_prob)\n\n#for i in range(len(X_test)):\n#    print(\"df['Zielvariable_ja']=%s, Predicted=%s\" % (df['Zielvariable_ja'][i], rfc_pred[i]))\n#    print(\"df['Zielvariable_ja']=%s, Predicted prob=%s\" % (df['Zielvariable_ja'][i], rfc_pred_prob[i]))\n\n# X_test.info()","a9b49f94":"df_test_copy['predictions'] = rfc_pred\ndf_test_copy['predictions prob'] = rfc_pred_prob","e0c32a85":"df_test_copy.info()","448c246e":"#df_test_copy = df_test_copy.loc[:, ['Stammnummer','predictions','predictions prob']]\ndf_test_copy = df_test_copy.loc[:, ['Stammnummer','predictions prob']]","241ceecd":"#df_test_copy","55c3eb65":"import csv\ndf_test_copy.to_csv('Loesung_final.csv', sep=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL, index=False)","ccd48c20":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix","a2dc4c5a":"# da die Zielvariable nur 2 Werte annimmt (!= Mehrklassenwert), ist der n\u00e4chste Schritt nicht notwendig\nlb = LabelBinarizer()\ndf['Zielvariable_ja'] = lb.fit_transform(df['Zielvariable_ja'].values)\ntargets = df['Zielvariable_ja']\nX_train, X_test, y_train, y_test = train_test_split(X, targets, stratify=targets)","16ec163f":"# n_jobs=-1 => Prozessorbeschr\u00e4nkung aufheben\nclf = RandomForestClassifier(n_jobs=-1)\n\nparam_grid = {\n    'min_samples_split': [3, 5, 10], # minimum number of samples that must be present from your data in order for a split to occur\n    'n_estimators' : [100, 300], # number of trees your want to build within a Random Forest before aggregating the predictions\n    'max_depth': [3, 5, 15, 25], # how deep do you want to make your trees - avoids overfitting\n    'max_features': [3, 5, 10, 20] # max number of features considered when finding best split\n}\n\nscorers = {\n    'precision_score': make_scorer(precision_score),\n    'recall_score': make_scorer(recall_score),\n    'accuracy_score': make_scorer(accuracy_score),\n    'roc_auc': make_scorer(roc_auc_score)\n}","dc54f03e":"def grid_search_wrapper(refit_score='precision_score'):\n    \"\"\"\n    fits a GridSearchCV classifier using refit_score for optimization\n    prints classifier performance metrics\n    \"\"\"\n    skf = StratifiedKFold(n_splits=10)\n    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n                           cv=skf, return_train_score=True, n_jobs=-1)\n    grid_search.fit(X_train.values, y_train.values)\n\n    # make the predictions\n    y_pred = grid_search.predict(X_test.values)\n\n    print('Best params for {}'.format(refit_score))\n    print(grid_search.best_params_)\n\n    # confusion matrix on the test data.\n    print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n    return grid_search\n","0481bace":"#grid_search_clf = grid_search_wrapper(refit_score='precision_score')\n#results = pd.DataFrame(grid_search_clf.cv_results_)\n#results = results.sort_values(by='mean_test_precision_score', ascending=False)\n\n#results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_max_depth', 'param_max_features', 'param_min_samples_split', 'param_n_estimators']].round(3).head()\n\n\"\"\"\nBest params for precision_score\n{'max_depth': 5, 'max_features': 5, 'min_samples_split': 3, 'n_estimators': 300}\n\nConfusion matrix of Random Forest optimized for precision_score on the test data:\n     pred_neg  pred_pos\nneg      6933        12\npos       880        45\n\"\"\"","0988bda2":"#grid_search_clf = grid_search_wrapper(refit_score='recall_score')\n#results = pd.DataFrame(grid_search_clf.cv_results_)\n#results = results.sort_values(by='mean_test_precision_score', ascending=False)\n\n#results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_max_depth', 'param_max_features', 'param_min_samples_split', 'param_n_estimators']].round(3).head()\n\"\"\"\nBest params for recall_score\n{'max_depth': 25, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 300}\n\nConfusion matrix of Random Forest optimized for recall_score on the test data:\n     pred_neg  pred_pos\nneg      6678       267\npos       473       452\n\"\"\"","9f4a9cd5":"#grid_search_clf = grid_search_wrapper(refit_score='accuracy_score')\n#results = pd.DataFrame(grid_search_clf.cv_results_)\n#results = results.sort_values(by='mean_test_precision_score', ascending=False)\n\n#results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_max_depth', 'param_max_features', 'param_min_samples_split', 'param_n_estimators']].round(3).head()\n\n\"\"\"\nBest params for accuracy_score\n{'max_depth': 15, 'max_features': 10, 'min_samples_split': 3, 'n_estimators': 300}\n\nConfusion matrix of Random Forest optimized for accuracy_score on the test data:\n     pred_neg  pred_pos\nneg      6751       194\npos       530       395\n\"\"\"","e133f871":"#grid_search_clf = grid_search_wrapper(refit_score='roc_auc')\n#results = pd.DataFrame(grid_search_clf.cv_results_)\n#results = results.sort_values(by='mean_test_precision_score', ascending=False)\n\n#results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_max_depth', 'param_max_features', 'param_min_samples_split', 'param_n_estimators']].round(3).head()\n\"\"\"\nBest params for roc_auc\n{'max_depth': 25, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}\n\nConfusion matrix of Random Forest optimized for roc_auc on the test data:\n     pred_neg  pred_pos\nneg      6676       269\npos       486       439\n\"\"\"","c1aaa6cd":"# Aufsplitten der Features in erkl\u00e4rende Variablen und Zielvariable\nX = df.drop(['Zielvariable_ja', 'Stammnummer'], axis=1)\ny = df['Zielvariable_ja']","1b005d65":"from sklearn.cross_validation import train_test_split","27a5aae8":"# da die Zielvariable nur 2 Werte annimmt (!= Mehrklassenwert), ist der n\u00e4chste Schritt nicht notwendig\nlb = LabelBinarizer()\ndf['Zielvariable_ja'] = lb.fit_transform(df['Zielvariable_ja'].values)\ntargets = df['Zielvariable_ja']\nX_train, X_test, y_train, y_test = train_test_split(X, targets, stratify=targets)","d72606c0":"from sklearn.linear_model import LogisticRegression","796fb9f7":"# initiieren des Models (default parameter)\nclf = LogisticRegression()\n\nparam_grid = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n\nscorers = {\n    'precision_score': make_scorer(precision_score),\n    'recall_score': make_scorer(recall_score),\n    'accuracy_score': make_scorer(accuracy_score),\n    'roc_auc': make_scorer(roc_auc_score)\n}","dfbbb368":"# grid_search_clf = grid_search_wrapper(refit_score='roc_auc')\n# results = pd.DataFrame(grid_search_clf.cv_results_)\n# results = results.sort_values(by='mean_test_precision_score', ascending=False)\n\n# results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score']].round(3).head()\n\n\"\"\"\nBest params for roc_auc\n{'C': 100, 'penalty': 'l1'}\n\nConfusion matrix of Random Forest optimized for roc_auc on the test data:\n     pred_neg  pred_pos\nneg      6779       166\npos       618       307\n\"\"\"","7b6cb4a6":"### 6.4 countplots","13be3193":"## 4. Redundante Informationen aus dem Dataframe l\u00f6schen","98ba0f6f":"### 10.1 Initiieren des Models","001d6e2e":"### 11.4 Mithilfe von GridSearchCV (und obiger grid_search_wrapper-Funktion) die beste Parameterkombination finden, die den accuracy_score maximiert","d43376c0":"### 8.1 Aufsplitten der Features in erkl\u00e4rende Variablen und Zielvariable","3d940ae5":"### 9.1 Initiieren des Models mit Default Parametern","c0828200":"## 9.Trees","7a0a4978":"#### 10.4.2 Datenstruktur - wie oben - herstellen","2755e362":"### 10.3 Auswertung der Ergebnisse","bb960f29":"### 8.2 Aufsplitten in Training- und Test-Set","6df9c939":"### 11.5 Mithilfe von GridSearchCV (und obiger grid_search_wrapper-Funktion) die beste Parameterkombination finden, die den roc_auc maximiert","7d58b1b8":"### 10.4 Vorhersagen f\u00fcr test machen","5be5289f":"### 11.2 Mithilfe von GridSearchCV (und obiger grid_search_wrapper-Funktion) die beste Parameterkombination finden, die den precision_score maximiert","3c194682":"### 6.2 Distplot - Verteilung der angegebenen Spalte zeichnen","46ea8019":"## 10. Random Forest","0b51efdf":"## 12. Logistic Regression - 2","608431fa":"## 6. Untersuchen der Daten\n- Grafiken zeichnen","12e339dc":"### 10.2 Trainings Daten ins Model laden und Vorhersagen machen","3a595c69":"### 8.5 Auswertung der Ergebnisse","8a3f590c":"### 6.3 corr() - Korrelationsmatrizen zeichnen","a85cd834":"#### 10.4.1 Daten einlesen","57ecd35d":"### 12.2 Aufsplitten in Training- und Test-Set","5060b2b3":"## 11. Random Forest - 2","858030d7":"### 9.3 Auswertung der Ergebnisse","1f52a3bb":"### 12.3 Initiieren des Models mit Default Parametern","3b125f0a":"### 8.4 Trainings Daten ins Model laden und Vorhersagen machen","d2c04653":"## 8. Logistic Regression","4b799b56":"## 7. Daten f\u00fcr die ML-Algos vorbereiten\n- droppen der Spalten, die f\u00fcr den ML-Algo nicht n\u00fctzlich sind","d1241648":"### 9.2 Trainings Daten ins Model laden und Vorhersagen machen","055d7629":"## 5. NaN-Werte in Spalte 'Tage seit letzter Kampagne' behandeln\n- mit Mittelwert (mean) bef\u00fcllen","74ffba8c":"### 11.1 Initiieren des Models","372c3c95":"## 3. One Hot Encoding bzw. Dummy Variables\n- \u00dcberf\u00fchren in nummerische Darstellung\n- die resultierende Darstllung kann von den ML-Algorithmen verarbeitet werden","6fd3ff3d":"### 6.5 boxplots","b8bb52e4":"### 8.3 Initiieren des Models mit Default Parametern","9e4fb4a2":"# Fallstudie\n## 1. Module einbinden","df012845":"### 12.1 Aufsplitten der Features in erkl\u00e4rende Variablen und Zielvariable","93b598fd":"### 11.3 Mithilfe von GridSearchCV (und obiger grid_search_wrapper-Funktion) die beste Parameterkombination finden, die den recall_score maximiert","a2b7abee":"### 6.1 Pairplot","753d1c5c":"### 12.4 Mithilfe von GridSearchCV (und obiger grid_search_wrapper-Funktion) die beste Parameterkombination finden, die den roc_auc maximiert","ab9d2817":"## 2. Daten einlesen"}}