{"cell_type":{"9c0b15fc":"code","a2e146c2":"code","cb8b9ac3":"code","fe09c78f":"code","45e0829f":"code","7b62b8d6":"code","a5877e5f":"code","f455d1c1":"code","03f18a35":"code","1291714f":"code","fe47baf5":"code","6e847329":"code","92030c2b":"code","c2115f8e":"code","3b2aff1e":"code","75e3e043":"code","6feb9e78":"code","458796ed":"markdown","c1f16dd3":"markdown","70e29a2a":"markdown","982b28ed":"markdown"},"source":{"9c0b15fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport re, datetime, functools\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport tensorflow as tf\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a2e146c2":"tf.enable_eager_execution()","cb8b9ac3":"TRAIN_DATA_PATH = '\/kaggle\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv'\nUNLABELD_TRAIN_DATA_PATH = '\/kaggle\/input\/word2vec-nlp-tutorial\/unlabeledTrainData.tsv'\nTEST_DATA_PATH = '\/kaggle\/input\/word2vec-nlp-tutorial\/testData.tsv'","fe09c78f":"train_df = pd.read_csv(TRAIN_DATA_PATH, header=0, delimiter='\\t', quoting=3)","45e0829f":"train_df.head()","7b62b8d6":"def review_to_words( raw_review ):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and \n    # the output is a single string (a preprocessed movie review)\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    #\n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))  ","a5877e5f":"clean_review = review_to_words( train_df[\"review\"][0] )\nprint(clean_review)","f455d1c1":"train_df['preprocessed_review'] = [review_to_words(r) for r in train_df['review']]","03f18a35":"max_features = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(train_df['preprocessed_review'])\ntrain_data_features = tokenizer.texts_to_sequences(train_df['preprocessed_review'])\nprint(len(train_data_features))\ntrain_data_features = tf.keras.preprocessing.sequence.pad_sequences(train_data_features, maxlen=150, padding='post')","1291714f":"print(train_data_features.shape)","fe47baf5":"Input = tf.keras.layers.Input\nDense = tf.keras.layers.Dense\nGlobalAveragePooling1D = tf.keras.layers.GlobalAveragePooling1D\nDropout = tf.keras.layers.Dropout\nEmbedding = tf.keras.layers.Embedding\nGRU = tf.keras.layers.CuDNNGRU\nBidirectional = tf.keras.layers.Bidirectional","6e847329":"def get_model():\n    model = tf.keras.Sequential([\n        Embedding(input_dim=max_features, output_dim=128),\n        Bidirectional(GRU(32, return_sequences=True)),\n        GlobalAveragePooling1D(),\n        Dropout(0.03),\n        Dense(20, activation='elu'),\n        Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n    model.summary()\n    return model","92030c2b":"model = get_model()","c2115f8e":"logdir = os.path.join(\"\/tmp\/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(filepath='.\/weights.hdf5', verbose=1, save_best_only=True),\n    tensorboard_callback,\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n]","3b2aff1e":"history = model.fit(train_data_features,\n                    train_df['sentiment'], batch_size=100, epochs=20, validation_split=0.2, callbacks=callbacks)","75e3e043":"df_test=pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\ndf_test.head()\ndf_test[\"review\"]=df_test.review.apply(lambda x: review_to_words(x))\ndf_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\ny_test = df_test[\"sentiment\"]\nlist_sentences_test = df_test[\"review\"]\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_te = tf.keras.preprocessing.sequence.pad_sequences(list_tokenized_test, maxlen=150)","6feb9e78":"prediction = model.predict(X_te)\ny_pred = (prediction > 0.5)\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)","458796ed":"## Build Model","c1f16dd3":"## Callbacks","70e29a2a":"## Training","982b28ed":"## inferance"}}