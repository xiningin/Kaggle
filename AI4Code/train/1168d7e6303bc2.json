{"cell_type":{"bb1124e9":"code","5e4fc253":"code","79b7733e":"code","93621e18":"code","e109c1f8":"code","512bf5f3":"markdown"},"source":{"bb1124e9":"import csv\nimport datetime\nimport numpy as np\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras import applications\nfrom keras import regularizers\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix","5e4fc253":"def get_data(fn):\n  data = []\n  with open(fn) as f:\n    reader = csv.DictReader(f)\n    data = [row for row in reader]\n  return data\n\ndef get_fields(data, fields):\n  extracted = []\n  for row in data:\n    extract = []\n    for field, f in sorted(fields.items()):\n      info = f(row[field])\n      if type(info) == list:\n        extract.extend(info)\n      else:\n        extract.append(info)\n    extracted.append(np.array(extract, dtype=np.float32))\n  return extracted\n\ndef preprocess_data(X, scaler=None):\n  if not scaler:\n    scaler = StandardScaler()\n    scaler.fit(X)\n  X = scaler.transform(X)\n  return X\n\n\ndef dateConvertion(x):\n  date, time = x.split(' ')\n  year, month, day = map(int, date.split('-'))\n  hour, minute, second = time.split(':')\n  return [day, month, year, hour, minute, datetime.datetime(year, month, day).isocalendar()[1], getHourPart(int(hour))]\n\ndef getHourPart(hour):\n    if(hour >= 2 and hour < 8): return 1;\n    if(hour >= 8 and hour < 12): return 2;\n    if(hour >= 12 and hour < 14): return 3;\n    if(hour >= 14 and hour < 18): return 4;\n    if(hour >= 18 and hour < 22): return 5;\n    if(hour < 2 or hour >= 22): return 6;\n\ndays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ndata_fields = {\n    'X': lambda x: float(x),\n    'Y': lambda x: float(x),\n    'Dates' : lambda x : dateConvertion(x),\n    'DayOfWeek' : lambda x : [days.index(x), 1 if days.index(x) > 4 else 0],\n    'Address': lambda x: [1 if ('\/' in x.lower() and 'of' not in x.lower()) else 0],\n    'PdDistrict': lambda x: districts.index(x),\n}\n\nprint('Loading training data...')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nraw_train = get_data('\/kaggle\/input\/sf-crime\/train.csv')    ","79b7733e":"districts = np.unique([row['PdDistrict'] for row in raw_train]).tolist()\nlabels = np.unique([row['Category'] for row in raw_train]).tolist()\nlabel_fields = {'Category': lambda x: labels.index(x.replace(',', ''))}\nprint('days')\nprint(days)\nprint('districts')\nprint(districts)\nprint('labels')\nprint(labels)","93621e18":"print('Creating training data...')\nX = np.array(get_fields(raw_train, data_fields), dtype=np.float32)\nprint('Creating training labels...')\ny = np.array(get_fields(raw_train, label_fields))\ndel raw_train","e109c1f8":"X = preprocess_data(X)\nY = np_utils.to_categorical(y)\n\ninput_dim = X.shape[1]\noutput_dim = len(labels)\n\ndef build_model(input_dim, output_dim, hn=32, dp=0.5, layers=1):\n    model = Sequential()\n    #model.add(Dense(hn, input_shape=(input_dim,), init='glorot_uniform',  activity_regularizer=regularizers.l1(0.01)))\n    model.add(Dense(hn, input_shape=(input_dim,), init='glorot_uniform'))\n    model.add(LeakyReLU())\n    model.add(Dropout(dp))\n\n    for i in range(layers):\n      #model.add(Dense(hn, input_shape=(hn,), init='glorot_uniform',  activity_regularizer=regularizers.l1(0.01)))\n      model.add(Dense(hn, input_shape=(hn,), init='glorot_uniform'))\n      model.add(LeakyReLU())\n      model.add(BatchNormalization())\n      model.add(Dropout(dp))\n\n    #model.add(Dense(output_dim, input_shape=(hn,), init='glorot_uniform',  activity_regularizer=regularizers.l1(0.01)))\n    model.add(Dense(output_dim, input_shape=(hn,), init='glorot_uniform'))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model\n\nEPOCHS = 40\nBATCHES = 128\nHN = 256\nLAYERS = 0\nDROPOUT = 0.01\nITERATIONS = 5\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42,stratify=Y)\nmodel = build_model(input_dim, output_dim, HN, DROPOUT, LAYERS)\nmodel.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCHES, validation_data=(X_test, y_test), verbose=2)\nprediction = model.predict(X_test, verbose=2)\npredictedVals = []\nfor row in prediction:\n    predictedVals.append(np.argmax(row))\nvalidVals = []\nfor row in y_test:\n    validVals.append(np.argmax(row))\nprint(\"F1 score (micro): \", f1_score(predictedVals, validVals, average='micro'))\nprint(\"F1 score (macro): \", f1_score(predictedVals, validVals, average='macro'))\nprint(\"F1 score (weighted): \", f1_score(predictedVals, validVals, average='weighted'))\nc = confusion_matrix(predictedVals, validVals)\nreverse_c = list(zip(*np.array(c)))\nfor i in range(len(c[1])):\n    print(labels[i])\n    fn = sum(c[i]) - c[i][i]\n    fp = sum(reverse_c[i]) - c[i][i]\n    print(\"\u041f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432: \" + str(c[i][i]))\n    print(\"\u041e\u0448\u0438\u0431\u043a\u0438 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0440\u043e\u0434\u0430: \"+ str(fn))\n    print(\"\u041e\u0448\u0438\u0431\u043a\u0438 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0440\u043e\u0434\u0430: \" + str(fp))","512bf5f3":"kfolds = KFold(n_splits=ITERATIONS, shuffle=False)\niteration = 0\nfor train, valid in kfolds.split(X, Y):\n    print('---' * 20)\n    print('Iteration number', iteration)\n    print('---' * 20)\n    iteration += 1\n    X_train = X[train]\n    X_valid = X[valid]\n    Y_train = Y[train]\n    Y_valid = Y[valid]\n    model = build_model(input_dim, output_dim, HN, DROPOUT, LAYERS)\n    model.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCHES, validation_data=(X_valid, Y_valid), verbose=2)\n    prediction = model.predict(X_valid, verbose=2)\n    predictedVals = []\n    for row in prediction:\n        predictedVals.append(np.argmax(row))\n    validVals = []\n    for row in Y_valid:\n        validVals.append(np.argmax(row))\n    print(\"F1 score (micro): \", f1_score(predictedVals, validVals, average='micro'))\n    print(\"F1 score (macro): \", f1_score(predictedVals, validVals, average='macro'))\n    print(\"F1 score (weighted): \", f1_score(predictedVals, validVals, average='weighted'))\n    \n    c = confusion_matrix(predictedVals, validVals)\n    reverse_c = list(zip(*np.array(c)))\n    for i in range(len(c[1])):\n        print(labels[i])\n        fn = sum(c[i]) - c[i][i]\n        fp = sum(reverse_c[i]) - c[i][i]\n        print(\"\u041f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432: \" + str(c[i][i]))\n        print(\"\u041e\u0448\u0438\u0431\u043a\u0438 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0440\u043e\u0434\u0430: \"+ str(fn))\n        print(\"\u041e\u0448\u0438\u0431\u043a\u0438 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0440\u043e\u0434\u0430: \" + str(fp))"}}