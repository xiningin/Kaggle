{"cell_type":{"36b22c04":"code","0c203d66":"code","a2ea0c77":"code","4324674d":"code","e3cdc072":"code","3ac3c6f9":"code","f9c19b89":"code","8f0c430d":"code","da412942":"code","b3972762":"code","b09ab4fe":"code","3ec9e242":"code","3f390be7":"code","e4c7a0cd":"code","f47db333":"code","3faa26c7":"code","bcfdefef":"code","f1706c06":"code","1d3ee103":"code","5cc494de":"code","0e7d808a":"code","b1eba624":"code","a02134e1":"code","4a6f8e21":"code","e6d5c7a8":"code","b6388569":"code","7ed04b39":"code","856759ed":"code","769ca6e7":"code","f2f59cc3":"code","609cb959":"code","beee0d99":"code","3bf0df5f":"code","bc254430":"code","74a96b72":"code","7699ce75":"code","3332ea48":"code","f2ccc37a":"code","607a4423":"code","9c1a9316":"code","aaa4813b":"code","9a850e5a":"code","d0642f55":"code","0dccaddb":"code","e9aaab8a":"code","1b64c87e":"code","cabc0a48":"code","364940a8":"code","c3ecc242":"code","8c10517e":"code","9ab4cc2f":"code","c1001d4a":"code","7123e614":"code","fe20f6da":"code","7b2b8109":"code","5b14b764":"code","820a7274":"code","b757cf10":"code","32ca7f3b":"code","2a8b0dfe":"code","9e87f307":"markdown","7884f40b":"markdown","e9465163":"markdown","7278a200":"markdown","ea0ebb4d":"markdown","a882417a":"markdown","20c49429":"markdown","25263c82":"markdown","2fad5df8":"markdown","0c2b0280":"markdown","82a61fec":"markdown","1d423554":"markdown","aa0db1b0":"markdown","bcfc0387":"markdown","943bb0eb":"markdown","e280a2d4":"markdown","08c01e60":"markdown","0dabd82d":"markdown","13636ddb":"markdown","5e79a59d":"markdown","4dde33eb":"markdown","5d281c1e":"markdown","c095e4ee":"markdown","f56c9e0a":"markdown","05ba057a":"markdown","5c8ba19a":"markdown","1c5d2a1a":"markdown","662c37f7":"markdown","93323fa0":"markdown","8f60ba38":"markdown","d1acba67":"markdown","bb68ee70":"markdown","683e0ed5":"markdown","4f1ea3da":"markdown","b4ba6686":"markdown","81e55b42":"markdown","a7740059":"markdown","bdcfe339":"markdown","f7fb44f4":"markdown","7304d20a":"markdown","005a0f98":"markdown","1608c566":"markdown","0ff251a3":"markdown","4f784cce":"markdown","250a35c2":"markdown","12559eef":"markdown","58939fa1":"markdown","93c9433c":"markdown","765c6425":"markdown","2cab1c47":"markdown","6199dad3":"markdown","77b54f9c":"markdown","26e8dbe4":"markdown"},"source":{"36b22c04":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0c203d66":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n%matplotlib inline","a2ea0c77":"# This is how we assign the datasets to variables in python using pandas.\ntrain=pd.read_csv(\"..\/input\/train.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")","4324674d":"train.head()","e3cdc072":"#info gives us information about index and column data types.\ntrain.info()","3ac3c6f9":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop we the 'Id' column since it's unnecessary for the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","f9c19b89":"#This gives us the statistical summary of the dataset\ntrain['SalePrice'].describe()","8f0c430d":"sns.set_style(\"whitegrid\")\nsns.distplot(train['SalePrice'])","da412942":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","b3972762":"data = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice', ylim=(0,800000))","b09ab4fe":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)","3ec9e242":"data = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x='YearBuilt', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90)","3f390be7":"corrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)","e4c7a0cd":"k = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()\n","f47db333":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","3faa26c7":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","bcfdefef":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","f1706c06":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","1d3ee103":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","5cc494de":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","0e7d808a":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","b1eba624":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","a02134e1":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","4a6f8e21":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","e6d5c7a8":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","b6388569":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","7ed04b39":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","856759ed":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","769ca6e7":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","f2f59cc3":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","609cb959":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","beee0d99":"all_data = all_data.drop(['Utilities'], axis=1)","3bf0df5f":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","bc254430":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","74a96b72":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","7699ce75":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","3332ea48":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","f2ccc37a":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","607a4423":"all_data.isnull().sum().max()","9c1a9316":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","aaa4813b":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","9a850e5a":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","d0642f55":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","0dccaddb":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","e9aaab8a":"#Getting dummy categorical features.\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)","1b64c87e":"#Splitting the data back into test and train\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","cabc0a48":"from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error","364940a8":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","c3ecc242":"rfc=RandomForestRegressor(n_estimators=1000)","8c10517e":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","9ab4cc2f":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","c1001d4a":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","7123e614":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","fe20f6da":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7b2b8109":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","5b14b764":"rfc.fit(train,y_train)\nrfc_train_pred = rfc.predict(train.values)\nrfc_pred = np.expm1(rfc.predict(test.values))","820a7274":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))","b757cf10":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))","32ca7f3b":"GBoost.fit(train,y_train)\nGBoost_train_pred = GBoost.predict(train)\nGB_pred = np.expm1(GBoost.predict(test.values))","2a8b0dfe":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = GB_pred\nsub.to_csv('submission.csv',index=False)","9e87f307":"<ul><li>The NA values of \"MasVnrType\" and \"MasVnrArea\" refer to no masonry veneer for these houses. Therefore the Area can be imputed with zero and the type can be imputed with None.<\/li><\/ul>","7884f40b":"This is the plot of the Target Variable against the 'OverallQual' variable which is the Overall material and finish quality. From this we can deduce that as the Overall quality increases so does the house price.","e9465163":"<ul><li>There is only one NA value present here in \"KitchenQual\" so we impute it 'TA' since it is the most occurring value.<\/li><\/ul>","7278a200":"<ul><li>There is only one NA value present in both Exterior1st and Exterior2nd so we fill it with the most frequent string.<\/li><\/ul>","ea0ebb4d":"<ul><li>Most of the values of this feature are \"AllPub\" except for one \"NoSeWa\" and 2 NA. Since the house with \"NoSewa' is present in the training set, this feature is not helpful in predictive modeling so we drop it. <\/li><\/ul>","a882417a":"<h1>Introduction<\/h1>","20c49429":"<h2>Competition Description<\/h2>","25263c82":"For cross validation we the cross_val_score function of sklearn.","2fad5df8":"<ul>\n    <li>We will impute the Nan or null values of 'FireplaceQu' with None as null values signify that the house does not contain any fireplace.<\/li>\n<\/ul>","0c2b0280":"<ul><li>We replace the 'GarageType', 'GarageFinish', 'GarageQual' and 'GarageCond' missing values into None signifying that those homes most likely don't have any garage for vehicles.<\/li><\/ul>","82a61fec":"<h2>Data Description<\/h2>","1d423554":"<ul><li>Fill it again with the most frequent which is \"WD\" in this case.<\/li><\/ul>","aa0db1b0":"With this we no longer have any missing values. ","bcfc0387":"<h3>Missing Values<\/h3>\n<br>\nLet us look at the percentage of the missing values of the dataset.","943bb0eb":"As already mentioned in the EDA section when we explored the Target Variable we need to transform this variable to make it more normally distributed. This is because linear models work well with normally distributed data.","e280a2d4":"From the below plot we can see two huge outliers GrLivArea that are of a low price present in the bottom right corner. Since these are huge outliers it is safe to delete them as it negatively affects our model.","08c01e60":"<ul>\n    <li>We will impute the Nan or null values of 'Alley' which refers to type of Alley access with None as null values signify that the house does not contain any Alley access.<\/li>\n<\/ul>","0dabd82d":"We will use the .head() function to display the first five columns of the dataset to get a feel of the dataset.","13636ddb":"<h3>Random Forest Regressor,GradientBoosting Regressor, XGBoostRegressor and LightGBM Regressor <\/h3>","5e79a59d":"Time to impute the missing values onto the features.","4dde33eb":"This competition contains 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa through these variables we are challenged to predict the final price of each home.","5d281c1e":"<ul>\n    <li>We will impute the Nan or null values of 'PoolQC' with None as null values signify that the majority of the houses don't have a Pool.<\/li>\n<\/ul>","c095e4ee":"We now transform a few numerical variables that are categorical then we perform label encoding onto them.","f56c9e0a":"From this we can tell that \"OverallQual\",'GrLivArea','TotalBsmtSF' are strongly correlated to the target variable.<br> <br>\n'GarageCars' and 'GarageArea' are strongly correlated to the Target variable too but since we know that the number of cars that fit in a garage is proportional to the 'GarageArea' we can just use one of these variables instead of all the Garage variables.<br><br>\n'ToalBsmtSF' and '1stFloor' are strongly correlated to each other so we can just use only one of them or combine them. The same thing can be done to 'TotRmsAbvGrd' and 'GrLivArea' ","05ba057a":"<ul>\n    <li>We will impute the Nan or null values of 'LotFrontage' with median values taking into consideration that the area of each street connected to the house property is most likely similar to other houses of the Neighborhood.<\/li>\n<\/ul>","5c8ba19a":"We are gonna concatenate the train and test data so that it will be easier to make adjustments to the combined data than to individually do it for train and test dataset.","1c5d2a1a":"<h3>Outliers<\/h3>","662c37f7":"<ul>\n    <li>We will impute the Nan or null values of 'Fence' with None as null values signify that the house does not contain any Fences.<\/li>\n<\/ul>","93323fa0":"<ul><li>There is only one NA value present here in \"Electrical\" so we impute it 'SBrkr' since it is the most occurring value.<\/li><\/ul>","8f60ba38":"This is a plot of the Target variable against the TotalBsmtSF which is Total square feet of basement area. From the plot we can tell that 'SalePrice' and 'TotalBsmtSF' have a strong linear or exponential relationship.","d1acba67":"<h3>Transforming Categorical Variables<\/h3>","bb68ee70":"<h2>Importing the Libraries<\/h2>\n\nThis is where the actual fun begins. We start off by importing all the libraries that we will need later on. We will be using Numpy and pandas for data analysis and matplotlib (Matlab for python), seaborn for data visualisation. Below I have given the steps to load the data onto varibles.","683e0ed5":"<ul><li>The missing values of 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath' are filled with zero because the null values most likely signifies no basement.<\/li><\/ul>","4f1ea3da":"This plot is of the 'YearBuilt' feature against the Target variable. The 'YearBuilt' feature contains the original construction date. It isn't that clear but we can say that 'SalePrice' is more to spend money in new stuff than old.","b4ba6686":"Let us plot the correlation matrix using heatmap to better understand the data.","81e55b42":"<ul><li>We replace the null values of 'GarageYrBlt', 'GarageArea', 'GarageCars' as no garage equals no cars.<\/li><\/ul>","a7740059":"<h1>Preprocessing<\/h1>","bdcfe339":"This notebook is a very basic and simple approach to this regression problem. This is a perfect starter competition for new comers in this field. I have used elements from the excellent kernel provided by Serigne you can find it <a href=\"https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\">here<\/a>.\n\nPlease feel free to leave any comments that will help me to further improve this kernel and do upvote if you like it.\n\nThis notebook is divided into six major parts:\n<ol>\n    <li>Introduction<\/li>\n    <li>Competition Description<\/li>\n    <li>Data Description<\/li>\n    <li>Exploratory Data Analyis or EDA (in short)<\/li>\n    <li>Data Pre-Processing<\/li>\n    <li>Modeling<\/li>\n<\/ol>\nFollowing the famous data science mantra we will spending the majority of our time in EDA and preprocessing compared to Modeling in a 80:20 ratio.\nFor the modeling part I have used Random Forest Regressor,XGBoost Regressor and LightGBM Regressor. You can find the documentation for XGBoost <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/\">here<\/a> and the documentation for LightGBM <a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/\">here<\/a>.","f7fb44f4":"From the above heatmap we can find that some predictors are strongly correlated to each other thus causing multicollinearity. Some of them are 'TotalBsmtSF' and '1stFlrSF' and the Garage variables. Now let us plot a correlation matrix against 'SalePrice'. ","7304d20a":"<ul><li>The NA values here refers to No Building class so we fill it with None.<\/li><\/ul>","005a0f98":"We find the skewed features and then perform Box Cox Transformation of highly skewed features.","1608c566":"<h2>Exploratory Data Analysis<\/h2>","0ff251a3":"Here we are plotting the Target varible against the GrLivArea which is Above grade (ground) living area in square feet. From this we can understand that \"GrLivArea\" and \"SalePrice\" has a linear relationship.","4f784cce":"<h4>Now let us import all the relevant libraries.<\/h4>","250a35c2":"The files given are:\n\n1. train.csv: This is the dataset that we are gonna use to train our model to give predictions. SalePrice is theproperty's sale price in dollars. This is the target variable that we are trying to predict.\n2. test.csv:The test set will be used to see how well our model performs on unseen data. For the test set, we do not provide the Target variable i.e, SalePrice. It is our job to predict these outcomes. For each passenger in the house, we use the model that we trained to predict the price of the house.\n3. data_description.txt - This gives us the full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here.\n4. sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms. This serves as an example of how our submission should look like.","12559eef":"A plot to visualise the Target Distribution. Since this plot is right skewed we will later on transform this into a normal distribution in the preprocessing section.","58939fa1":"<ul><li>The 'MSZoning' refers to the general zoning classification. We fill the NaN values with 'RL' which is the most occurring value.<\/li><\/ul>","93c9433c":"<ul><li>The NA values means typical which is mentioned in the data description.<\/li><\/ul>","765c6425":"<ul><li>The missing values'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2 are filled with Nonne as NaN values signifies no basement.<\/li><\/ul>","2cab1c47":"So we create a new feature that plays an important role in predicting house prices which is the sum of total area of basement, first and second floors of each house.","6199dad3":"<ul>\n    <li>We will impute the Nan or null values of 'MiscFeature' with None as null values signify that the house does not contain miscellaneous Features.<\/li>\n<\/ul>","77b54f9c":"<h1>Modeling<\/h1>","26e8dbe4":"\n<h2>Submission<\/h2>"}}