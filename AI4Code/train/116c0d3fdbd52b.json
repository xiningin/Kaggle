{"cell_type":{"d60662ac":"code","2687b9f6":"code","add3c977":"code","3601363d":"code","cd66cce5":"code","48358466":"code","df63b760":"code","8d848756":"code","9488f3e8":"code","53d9f513":"code","cacb4114":"code","15a16bb9":"code","f676542c":"code","8891564e":"code","7bd49a40":"code","2290d1bf":"code","4b65d56c":"code","e02b2406":"code","e6582104":"code","03322900":"markdown","cf9a7b5e":"markdown","2abfe910":"markdown","08ae5260":"markdown","2677ef3b":"markdown","199e4f79":"markdown","509a7e24":"markdown","e0fa2669":"markdown","fbca0ae5":"markdown","ef9747a6":"markdown","f15b205f":"markdown","a1ab98b7":"markdown","46685772":"markdown","8c99caad":"markdown","4cc1cdd3":"markdown","9ac41b61":"markdown","07ae9d8c":"markdown","d2c9fcc2":"markdown","df116ef6":"markdown","bab4636d":"markdown","ad45db48":"markdown","3d95000c":"markdown","58fd2181":"markdown","0e4def68":"markdown","f3d7652b":"markdown","ffbbaeba":"markdown","d6afd9d6":"markdown","08b1c732":"markdown","b78538e2":"markdown","4e7314ae":"markdown","ced69f10":"markdown","641ac081":"markdown"},"source":{"d60662ac":"import os\nimport time\nimport sys\nimport numpy as np\nimport pandas as pd\nimport regex as re\nimport lxml\nimport numbers\nfrom bs4 import BeautifulSoup\nimport requests","2687b9f6":"req_headers = {\n    'accept': 'text\/html,application\/xhtml+xml,application\/xml;q=0.9,image\/webp,image\/apng,*\/*;q=0.8',\n    'accept-encoding': 'gzip, deflate, br',\n    'accept-language': 'en-US,en;q=0.8',\n    'upgrade-insecure-requests': '1',\n    'user-agent': 'Mozilla\/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/61.0.3163.100 Safari\/537.36'\n}","add3c977":"base_url = \"https:\/\/www.zillow.com\/homes\/for_sale\/\"","3601363d":"urls = []\n\ncity = 'atlanta'\nurl1 = base_url +city+'\/'\nurls.append(url1)\nfor i in range(2,21):\n    dom = base_url + city +'\/'+str(i)+'_p\/'\n    urls.append(dom)","cd66cce5":"print(urls)","48358466":"# Function to get data from urls and parse it\ndef soups(data):\n    with requests.Session() as s:\n        r = s.get(data, headers=req_headers)\n        soup = BeautifulSoup(r.content, 'html.parser')\n    return soup","df63b760":"# Call soup function and store output in a list\nlst = []\n\nfor url in urls:\n    htmls = soups(url)\n    lst.append(htmls)\nprint(len(lst))","8d848756":"df = pd.DataFrame()","9488f3e8":"def my_soup(data):\n    \n    address = data.find_all(class_= 'list-card-addr')\n    price = list(data.find_all(class_='list-card-price'))\n    beds = list(data.find_all(\"ul\", class_=\"list-card-details\"))\n    last_updated = data.find_all('div', {'class': 'list-card-top'})\n    \n    #create dataframe columns out of variables\n    df['prices'] = price\n    df['address'] = address\n    df['beds'] = beds\n    df['last_updated'] = last_updated\n        \n    return df.copy()","53d9f513":"zillow_df = pd.DataFrame()\n\n# Get our data from each url and put it into a list of dataframes\ndf_list = []\nfor soup in lst:\n    new_df = my_soup(soup)\n    df_list.append(new_df)\n    \n# Combine the list of datasets into our new dataframe\nzillow_df = pd.concat(df_list)\nzillow_df.reset_index(inplace=True)\nzillow_df = zillow_df.drop('index', axis=1)","cacb4114":"print(zillow_df.shape)\nzillow_df.head()","15a16bb9":"print(zillow_df.dtypes)","f676542c":"zillow_df = zillow_df.applymap(str)","8891564e":"zillow_df = zillow_df.applymap(lambda x: re.sub('<[^<]+?>', '',x))\nzillow_df.head()","7bd49a40":"zillow_df['address'].value_counts()","2290d1bf":"zillow_df[['beds', 'home_type']] = zillow_df.beds.str.split('-',n=1, expand=True)\nzillow_df.head()","4b65d56c":"# separate beds column into bed, bath, and sq_feet\nzillow_df[['beds', 'baths', 'sqft']] = zillow_df.beds.str.split(' ',n=2, expand=True)\nzillow_df['sqft'] = zillow_df.sqft.str.replace(\",\", \"\")\n\n# extract only the digits from the columns\nzillow_df['beds'] = zillow_df.beds.str.extract('(\\d+)')\nzillow_df['baths'] = zillow_df.baths.str.extract('(\\d+)')\nzillow_df['sqft'] = zillow_df.sqft.str.extract('(\\d+)')\n\n# convert columns to float\nzillow_df['beds'] = zillow_df['beds'].astype('float')\nzillow_df['baths'] = zillow_df['baths'].astype('float')\nzillow_df['sqft'] = zillow_df['sqft'].astype('float')\n\nzillow_df.head()","e02b2406":"zillow_df.info()","e6582104":"zillow_df.to_csv(\"Zillow_Home_Listings.csv\", index=False)","03322900":"### Before we start let's address Headers\nWhen we send a request to a website we need to specify who is making the request and what kind of data we want in return. This type of information included in the request is usually referred to as a header and it is needed to allow the server to respond accurately to our request. This [article](https:\/\/365datascience.com\/tutorials\/python-tutorials\/request-headers-web-scraping\/) describes headers in greater detail and more information can be found online. Don't neglect this initial part of the scraping process. ","cf9a7b5e":"**List of urls**","2abfe910":"## 2. Parse HTML","08ae5260":"**All of our data is an object datatype so we want to convert them to strings in order to clean them.**","2677ef3b":"From there we will be able to select the html elements we want to scrape from the page. \n\nHypertext markup language (HTML) is the language of the internet. All web pages are written in it, therefore it can be used to scrape nearly any web page. Each part of the web page that appears in your browser is represented by html, which is normally devided into groups that are named based on their function, such as headers, main, lists, etc. \n\nThe HTML tree shown below can help you understand how a web page is structured and how we can call certain elements in order to scrape their values:","199e4f79":"**Let's extract the data we want from the html in our list.**\n\nWe can thank Max Bade for his [Medium article](https:\/\/medium.com\/dev-genius\/scraping-zillow-with-python-and-beautifulsoup-bbc7e581c218) outlining how to scrape Zillow with BeautifulSoup. ","509a7e24":"![html_tree.png](attachment:html_tree.png)","e0fa2669":"**Now let's put our final output into a csv file.** ","fbca0ae5":"## 3. Scrape Specific Data","ef9747a6":"The above image shows a basic html tree of the body of a webpage. The boxes are tags and each part of the web page has one. The tags all stand for something and describe how the values contained in them will be represented on the web page. For instance, \"div\" is a web tag used to group other elements of the web page together, and \"h1\" is a web tag used to define a heading in that group or division.\n    \nIn addition to tags, each element on the web page will also have a class attribute (not shown in the above tree). Classes are mostly used to point to a class in a style sheet that is used in the website design. But we can use it to further designate which elements we want to scrape from our web page. ","f15b205f":"If we study our data we can see that any missing values are usually rows where the home_type is a Studio or Construction Lot. ","a1ab98b7":"![web-scraping.jpg](attachment:web-scraping.jpg)","46685772":"### Now we can split the beds column into multiple columns and change the data type to float values:","8c99caad":"**Check for any duplicates.**","4cc1cdd3":"![Zillow_shot.png](attachment:Zillow_shot.png)\n\n(*Screenshot of Zillow.com*)","9ac41b61":"**Check and see if we have any null values.**","07ae9d8c":"This is the base url we will use for our scraping. It is the zillow website url on the page for homes for sale. ","d2c9fcc2":"## Reminder!\n\nDon't forget to create a copy of our dataframe (df) each time we run the above function. In the end we want 20 unique dataframes all holding values from the webpage they are assigned to scrape. If we don't use the copy command all of the dataframes will look exactly the same. \n\n\nIn order to avoid this problem we change what would usally be\"return df\" in our code to \"return df.copy()\". This way everytime we run that function it makes a copy of the dataframe we changed and stores it in our new variable.\n\nLink to [article](http:\/\/https:\/\/www.dataquest.io\/blog\/tutorial-functions-modify-lists-dictionaries-python\/) further explaining this.","df116ef6":"Next we will gather a list of the urls we want to scrape. Our code below builds the urls we want based on the input of the user (i.e. city, number of pages to search, etc.), which we will then sort into a list. \n\n**For this example let's gather the first 20 pages of Zillow listings in Atlanta.**","bab4636d":"## 4. Clean Data and Create New Features","ad45db48":"## 1. Gather URLs","3d95000c":"**Create a dataframe to hold our newly scraped data.**","58fd2181":"**We can then use a regex argument to remove any html that clutters up our desired output.**","0e4def68":"This notebook provides an overview of how to scrape Zillow for house listing data. It can be adjusted to scrape housing data from any city in the U.S.\n\nIt will provide you a simple dataset of about 800 home listings.\n\nThis notebook focuses on BeautifulSoup and Requests, but I would also recommend learning scrapy as it can be a lot more powerful. Additionally, APIs are a very useful tool that can make things a lot easier and are an essential part of any data scientist's toolkit. Hopefully this notebook will give you some insight into scraping websites with Beautiful Soup and Requests, and you can begin gathering data for your future projects.","f3d7652b":"In order to scrape our webpage we need to call certain html elements from the parsed data. To determine what elements we want we can go to the web page, right-click and select inspect. \n\nOur webpage should then look like the one below:","ffbbaeba":"**First, split the home type from the values of bed, bath, and square feet.**","d6afd9d6":"# Intro to Scraping with Requests and Beautiful Soup: Scraping Zillow","08b1c732":"### Run the Function\n\nBelow we automate our call function so it goes through and scrapes all 20 of our html pages saved in the list, extracts the elements we want, and saves them as separate dataframes, which we will then concatenate (combine).  ","b78538e2":"**Our final product is still a little messy so we'll need to clean it up in the next step.**","4e7314ae":"We are going to use Requests and BeautifulSoup to parse the html from the urls we have stored in our list. \n\nFirst lets make our request and then parse the data with BeautifulSoup. Finally, we save our parsed html data in a new list labled \"lst\". ","ced69f10":"**Then split the values inside the bed columns into their own separate columns and change the data type to float.** ","641ac081":"**Now we're gonna clean up the data so that it's usable.**\n\n**First, let's determine the data type of each column.**"}}