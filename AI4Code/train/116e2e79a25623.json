{"cell_type":{"13942eff":"code","1ca74e84":"code","52006383":"code","670070b2":"code","09145389":"code","4207a0d5":"code","90a2b316":"code","78635f12":"code","511d0a50":"code","6320f897":"code","d2aa93f9":"code","c0cb331f":"code","b1d4b67f":"code","f0faf2ac":"code","f5d4ab32":"code","9d6ba43d":"code","a46c2e06":"code","ee485266":"code","f920bf6f":"code","e1e4ed6b":"code","e3ba91a0":"code","26554c3d":"code","9c9a1d6d":"code","73ac29fb":"code","dfeed830":"code","0975ff3e":"code","ae6b2a54":"code","286ca832":"code","99348cc8":"code","25e5867f":"code","e0952447":"code","573ae57e":"code","2531e14f":"code","0b0cf432":"code","74df14c9":"code","f6f1b5ef":"code","14b59bea":"code","926cfaee":"code","ecdb0655":"code","d7112db7":"code","2a76e4df":"code","808cd9c2":"code","49b652dd":"code","cc22c523":"code","6f50ebbc":"code","1e738ef9":"code","7807ed5a":"code","947af5ef":"code","71a928e5":"code","bb09980d":"markdown","55494b4e":"markdown","f9d2ac4f":"markdown","60505b77":"markdown","3c727472":"markdown","41d6aa74":"markdown","445b7131":"markdown","37bedae2":"markdown","968b7a95":"markdown","aa26e54b":"markdown","67822d52":"markdown","b2fb607b":"markdown","d79dd949":"markdown","562bb74a":"markdown","102c7d01":"markdown","fe21fe64":"markdown","4b377bab":"markdown","b9b0e40b":"markdown","af4e84c7":"markdown","50d30610":"markdown","7c31beb2":"markdown","d3ee2271":"markdown","25adc206":"markdown"},"source":{"13942eff":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow\nsns.set(color_codes=True)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, RobustScaler","1ca74e84":"import os\nprint(os.listdir(\"..\/input\"))","52006383":"df=pd.read_csv('..\/input\/Churn_Modelling.csv')\ndf.head(5)","670070b2":"df=df.drop(['RowNumber','CustomerId','Surname'], axis=1)","09145389":"df.Exited.value_counts()","4207a0d5":"sns.set(color_codes=True)\nsns.countplot(x=\"Exited\", data=df, palette=\"bwr\")\nplt.title('Class Distributions \\n (0: No Exited || 1: Exited)', fontsize=15)\nplt.show()","90a2b316":"countnotleave = len(df[df.Exited == 0])\ncountleave = len(df[df.Exited == 1])\nprint(\"Percentage of Costumers who didn't leave: {:.2f}%\".format((countnotleave \/ (len(df.Exited))*100)))\nprint(\"Percentage of Costumers who did leave: {:.2f}%\".format((countleave \/ (len(df.Exited))*100)))","78635f12":"sns.countplot(x='Gender', data=df, palette=\"mako_r\")\nplt.xlabel(\"Gender\")\nplt.show()","511d0a50":"df.drop(['Geography','Gender', 'Exited'], axis=1).describe()","6320f897":"#sns.pairplot(df.drop(['Geography','Gender','NumOfProducts','HasCrCard','IsActiveMember'], axis=1), hue=\"Exited\")\n#plt.show()","d2aa93f9":"f, axes = plt.subplots(ncols=3, figsize=(18,3))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Exited\", y=\"Age\", data=df, ax=axes[0], palette=\"Set2\")\naxes[0].set_title('Age vs Exited')\n\nsns.boxplot(x=\"Exited\", y=\"EstimatedSalary\", data=df, ax=axes[1], palette=\"Set2\")\naxes[1].set_title('Salary vs Exited')\n\nsns.boxplot(x=\"Exited\", y=\"Tenure\", data=df, ax=axes[2], palette=\"Set2\")\naxes[2].set_title('Tenure vs Exited')\n\nplt.show()","c0cb331f":"sns.set(color_codes=True)\nplt.figure(figsize=(20,12))\n\nA=['Gender','Geography','NumOfProducts','HasCrCard',\"IsActiveMember\"]\nB=[\"muted\",\"husl\",\"dark\",\"RdBu_r\",\"BrBG\"]\n\nfor i in range(5):\n    plt.subplot(2,3,i+1)\n    sns.countplot(x=A[i], hue='Exited', data=df, palette=B[i])\n    #plt.title('Exited Frequency for Gender')\n    #plt.xlabel('Gender')\n    plt.xticks(rotation=0)\n    plt.legend([\"Not Exited\", \"Exited\"])\n    #plt.ylabel('Frequency')\n    #plt.show()\n\nplt.show()","b1d4b67f":"#plt.figure(figsize=(15,4))\n#new_Balance=pd.cut(round(df.Balance\/1000,1),8)\n#sns.countplot(new_Balance, hue=df.Exited, palette='PuRd')\n#plt.xlabel('Balance ($K)')\n#plt.title('Exited Frequency According to Balance')\n#dfb=pd.get_dummies(new_Balance)","f0faf2ac":"sex=pd.get_dummies(df['Gender'])\ncountry=pd.get_dummies(df['Geography'])","f5d4ab32":"df=pd.concat([df,country],axis=1)","9d6ba43d":"df = df.drop(columns = ['Gender', 'Geography', 'Spain'])\ndf['Gender']=sex['Female']\ndf.head()","a46c2e06":"y = df.Exited.values\nx_data = df.drop(['Exited'], axis = 1)","ee485266":"# Normalize\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","f920bf6f":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=42)","e1e4ed6b":"#transpose matrices\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T","e3ba91a0":"#initialize\ndef initialize(dimension):\n    \n    weight = np.full((dimension,1),0.01)\n    bias = 0.0\n    return weight,bias","26554c3d":"def sigmoid(z):\n    \n    y_head = 1\/(1+ np.exp(-z))\n    return y_head","9c9a1d6d":"def forwardBackward(weight,bias,x_train,y_train):\n    # Forward\n    \n    y_head = sigmoid(np.dot(weight.T,x_train) + bias)\n    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss) \/ x_train.shape[1]\n    \n    # Backward\n    derivative_weight = np.dot(x_train,((y_head-y_train).T))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"Derivative Weight\" : derivative_weight, \"Derivative Bias\" : derivative_bias}\n    \n    return cost,gradients","73ac29fb":"def update(weight,bias,x_train,y_train,learningRate,iteration) :\n    costList = []\n    index = []\n    \n    #for each iteration, update weight and bias values\n    for i in range(iteration):\n        cost,gradients = forwardBackward(weight,bias,x_train,y_train)\n        weight = weight - learningRate * gradients[\"Derivative Weight\"]\n        bias = bias - learningRate * gradients[\"Derivative Bias\"]\n        \n        costList.append(cost)\n        index.append(i)\n\n    parameters = {\"weight\": weight,\"bias\": bias}\n    \n    #print(\"iteration:\",iteration)\n    #print(\"cost:\",cost)\n\n    plt.plot(index,costList)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n\n    return parameters, gradients","dfeed830":"def predict(weight,bias,x_test):\n    z = np.dot(weight.T,x_test) + bias\n    y_head = sigmoid(z)\n\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(y_head.shape[1]):\n        if y_head[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","0975ff3e":"def logistic_regression(x_train,y_train,x_test,y_test,learningRate,iteration):\n    dimension = x_train.shape[0]\n    weight,bias = initialize(dimension)\n    parameters, gradients = update(weight,bias,x_train,y_train,learningRate,iteration)\n    y_prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    print(\"Manuel Test Accuracy: {:.2f}%\".format((1 - np.mean(np.abs(y_prediction - y_test)))*100))","ae6b2a54":"logistic_regression(x_train,y_train,x_test,y_test,1,200)\n","286ca832":"X=df.drop(\"Exited\", axis = 1)\ny=df['Exited']\n\npipeline = Pipeline([('std_scaler', StandardScaler()),\n        ])\n\npipeline.fit_transform(X)\n\nX_scaled = pd.DataFrame(pipeline.fit_transform(X), columns=X.columns)\n\ndata_scaled=pd.concat([X_scaled,y],axis=1)","99348cc8":"data_scaled.head()","25e5867f":"X=data_scaled.drop('Exited',axis=1)\ny=data_scaled['Exited']","e0952447":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","573ae57e":"#import imblearn\n#from imblearn.under_sampling import RandomUnderSampler\n#rus = RandomUnderSampler(sampling_strategy=0.3, random_state=42)\n#x_train, y_train = rus.fit_resample(x_train, y_train)","2531e14f":"classifiers = {\n    \"LogisiticRegression\": LogisticRegression(solver='lbfgs'),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(gamma='scale'),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100)\n}","0b0cf432":"for key, classifier in classifiers.items():\n    classifier.fit(x_train, y_train)\n    training_score = cross_val_score(classifier, x_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\"\n , round(training_score.mean(), 3) * 100, \"% accuracy score\")","74df14c9":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l2'], 'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(solver='lbfgs'), log_reg_params, cv=5)\ngrid_log_reg.fit(x_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(gamma='scale'), svc_params,cv=5)\ngrid_svc.fit(x_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params,cv=5)\ngrid_tree.fit(x_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_\n\n#Random forest\nparam_grid = parameters = {'n_estimators': [100, 200, 400], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [10],\n              'min_samples_split': [30],\n              'min_samples_leaf': [1]\n             }\n\ngrid_forest = GridSearchCV(RandomForestClassifier(n_estimators=100), param_grid, cv=5)\ngrid_forest.fit(x_train, y_train)\n\nforest = grid_forest.best_estimator_","f6f1b5ef":"# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, x_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nsvc_pred = cross_val_predict(svc, x_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, x_train, y_train, cv=5)\n\nforest_pred = cross_val_predict(forest, x_train, y_train, cv=5)","14b59bea":"from sklearn.metrics import roc_auc_score\n\nd = {'Classifier': ['Logistic Regression', 'Support Vector Classifier', 'Decision Tree Classifier', 'Random Forest Classifier'], \n     '(ROC AUC) Score': [roc_auc_score(y_train, log_reg_pred), roc_auc_score(y_train, svc_pred), roc_auc_score(y_train, tree_pred), roc_auc_score(y_train, forest_pred)]}\ndf = pd.DataFrame(data=d)\n \ndf","926cfaee":"sns.set_style(\"white\")\n\nlog_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\nforest_fpr, forest_tpr, forest_threshold = roc_curve(y_train, forest_pred)\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, forest_fpr, forest_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(10,5))\n    plt.title('ROC Curve \\n 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot(forest_fpr, forest_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_train, forest_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, forest_fpr, forest_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nsns.set_style(\"white\")\nplt.show()","ecdb0655":"y_pred_log_reg = log_reg.predict(x_test)\ny_pred_forest = forest.predict(x_test)\ny_pred_svc = svc.predict(x_test)\ny_pred_tree = tree_clf.predict(x_test)\n\n\nlog_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\nforest_cf = confusion_matrix(y_test, y_pred_forest)\nsvc_cf = confusion_matrix(y_test, y_pred_svc)\ntree_cf = confusion_matrix(y_test, y_pred_tree)\n\nplt.figure(figsize=(20,10))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,2,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(log_reg_cf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,2,2)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(svc_cf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,2,3)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(tree_cf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,2,4)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(forest_cf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\n\nplt.show()","d7112db7":"from sklearn.metrics import roc_auc_score\n\nd = {'Classifier': ['Logistic Regression', 'Support Vector Classifier', 'Decision Tree Classifier', 'Random Forest Classifier'], \n     '(ROC AUC) Score': [roc_auc_score(y_test, y_pred_log_reg), roc_auc_score(y_test, y_pred_svc), roc_auc_score(y_test, y_pred_tree), roc_auc_score(y_test, y_pred_forest)]}\nroc_test = pd.DataFrame(data=d)\n \nroc_test","2a76e4df":"from sklearn.decomposition import PCA\n\npca=PCA(n_components=2)\nprincipalComponents = pca.fit_transform(data_scaled.drop('Exited', axis=1))\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])","808cd9c2":"finalDf = pd.concat([principalDf, data_scaled[['Exited']]], axis = 1)\nfinalDf.head(5)","49b652dd":"sample=finalDf[:1000]\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 Component PCA', fontsize = 20)\n\n\ntargets = [0,1]\ncolors = ['r', 'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = sample['Exited'] == target\n    ax.scatter(sample.loc[indicesToKeep, 'principal component 1']\n               , sample.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","cc22c523":"from keras import models\nfrom keras import layers\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.metrics import categorical_crossentropy\nfrom keras import regularizers\n\nn_inputs = x_train.shape[1]\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(32, activation='relu', input_shape=(n_inputs,)))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(1, activation='sigmoid'))","6f50ebbc":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])","1e738ef9":"history = model.fit(x_train, y_train, validation_split=0.2, batch_size=512, epochs=100, shuffle=True, verbose=2)","7807ed5a":"keras_predictions = model.predict_classes(x_test, batch_size=200, verbose=0)","947af5ef":"\n\ncm = confusion_matrix(y_test, keras_predictions)\nactual_cm = confusion_matrix(y_test, y_test)\nlabels = ['No Exited', 'Exited']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplt.title(\"Confusion Matrix \\n keras\")\nsns.heatmap(cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\n#fig.add_subplot(222)\n#plt.title(\"Confusion Matrix \\n (with 100% accuracy)\")\n#sns.heatmap(actual_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.show()","71a928e5":"acc = history.history['acc']\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","bb09980d":"$ $\n\nLet's say weight = 0.01 and bias = 0.0\n\n$ $","55494b4e":"Since 'Gender' and 'Geography' are categorical variables we'll turn them into dummy variables.","f9d2ac4f":"$ $\n\nWe will split our data into train and test set.\n\n$ $","60505b77":"\n$ $\n\nData contains; \n\n\n- RowNumber\n- CustomerId\n- Surname\n- Creditscore \n- Geography - (France, Spain, Germany)\n- Gender - (Famale; Male)\n- age - age in years \n- Tenure - (from 1 to 10 years)\n- Balance\n- NumberOfProducts\n- HasCrCard - (1=yes, 0=no)\n- IsActiveMember - (1=yes, 0=no)\n- EstimatedSalary\n- Exited - Did they leave the bank (1=yes, 0=no)\n\n$ $ ","3c727472":"## Feature engineering","41d6aa74":"Logistic Regression model estimated probability:\n$$\\hat{p} =h_{\\theta}(x)=\\sigma(\\theta^T\\cdot x)$$\n\n","445b7131":"### <font color='blue'>Manuel Test Accuracy is **81.25%** <\\font>\n","37bedae2":"$ $\n\n## Classifiers\n\n$ $\n\nWe can use sklearn library or we can write functions ourselves. We will try both. Firstly we will write our functions after that we'll use sklearn library to calculate score.\n\n$ $","968b7a95":"We have a data which classified if Bank customer leave the bank or not according to features in it. We will try different classifiers to predict if a customer will leave or not.","aa26e54b":"#### Normalize Data\n\n\n$$X_{\\text{changed}}=\\frac{X-X_{\\min}}{X_{\\max}-X_{\\min}}$$\n\n$ $","67822d52":"We notice that features RowNumber, CustomerId and Surname are useless for our peorpuse. So we can drop them from the data.","b2fb607b":"Let's find out sklearn's score.","d79dd949":"We have imbalanced data. Let's plot pairwise relationships of certain features in a dataset with different levels of our target variable.","562bb74a":"By the way in formulas; \n\n- $h_{\\theta}(x^{(i)})$= y_head\n- $y^{(i)}$ = y_train\n- $x^{(i)}$ = x_train","102c7d01":"### Read Data\n$ $","fe21fe64":"#### Sigmoid Function\n\n\n$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n\n$ $\n","4b377bab":"## Neural Network with Keras","b9b0e40b":"### Test Data with classifiers","af4e84c7":"## Principle Component Analysis (PCA) for Data Visualization","50d30610":"#### Gradient Descent\n\n\n$$\\theta_{j}:= \\theta_j-\\alpha\\frac{\\partial}{\\partial \\theta_{j}}J(\\theta)$$\n\n$$\\theta_{j}:= \\theta_j-\\frac{\\alpha}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}$$\n\n$ $","7c31beb2":"\n### Creating Dummy Variables\n","d3ee2271":"#### Cost Function\n\n$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log (h_{\\theta}(x^{(i)}))+(1-y^{(i)})\\log (1-h_{\\theta}(x^{(i)})]$$\n\n$ $","25adc206":"## Introduction"}}