{"cell_type":{"c45ba84f":"code","5d0e6de9":"code","3f4f5982":"code","24ca7a51":"code","1aacb672":"code","316c4642":"code","31cb9d38":"code","98e49005":"code","37ca86d2":"code","d21e028a":"code","ba7d579c":"code","dfde0f62":"code","d4b5cd0b":"code","dcc88201":"code","5c8fc9fa":"code","419ccfa7":"code","6636fbc2":"code","8b2e304c":"code","329bc041":"code","ab675913":"code","6e7ab344":"code","1430cd19":"code","3ea1c8a3":"code","a43f4fe1":"code","c6857bc6":"code","9702194e":"code","33079fd2":"code","4d7d4b56":"code","366a3a56":"code","80d281da":"markdown","987eb615":"markdown","f9c168dc":"markdown","33c69eac":"markdown","847646c8":"markdown","3c342d93":"markdown","00aa52b5":"markdown"},"source":{"c45ba84f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport nltk\nimport regex as re\nfrom nltk.stem import WordNetLemmatizer\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport string\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d0e6de9":"stopwords_path = \"..\/input\/stopwords\/stopwords\/english\"\nstopwords = np.loadtxt(stopwords_path, dtype=str)","3f4f5982":"#to install datasets library\n!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import Dataset","24ca7a51":"train = pd.read_csv(r'\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\nsb = pd.read_csv(r'\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv')","1aacb672":"train.isna().sum()\/train.shape[0]","316c4642":"sns.distplot(train[\"target\"]).set(title='Distribution of Target Variable')\n","31cb9d38":"#Removing punchuation from sentences and stop words\n\n# nltk.download('stopwords', quiet=True)\n# stopwords = nltk.corpus.stopwords.words('english')\ndef remove_pun_stopwords(text):\n    text = re.sub(r'[^\\w\\s]','',text)\n    text = [i.lower() for i in text.lower().split() if i not in stopwords]\n    return(' '.join(text))\n\n\n","98e49005":"def clean_text(text):\n\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n","37ca86d2":"\ndef clean(text):\n    text = clean_text(text)\n#     text = remove_pun_stopwords(text)\n    return text","d21e028a":"train['excerpt'] = train['excerpt'].apply(clean)\ntest['excerpt'] = test['excerpt'].apply(clean)","ba7d579c":"lemmatizer = WordNetLemmatizer()\n\ndef word_lemmatizer(text):\n    \n    text = [lemmatizer.lemmatize(i) for i in text.split()]\n    return(' '.join(text))\n\n","dfde0f62":"train['excerpt'] = train['excerpt'].apply(word_lemmatizer)\ntest['excerpt'] = test['excerpt'].apply(word_lemmatizer)","d4b5cd0b":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1,random_state=10).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","dcc88201":"#Creating K - folds for training and validation\ntrain = create_folds(train, num_splits=5)\ntrain = train.rename(columns={'target':'label'})","5c8fc9fa":"\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain['label'] = scaler.fit_transform(train[['label']])","419ccfa7":"# disable W&B logging as we don't have access to the internet\n%env WANDB_DISABLED=True","6636fbc2":"#to load pretrained model\nmodel_checkpoint = '..\/input\/distillbert-huggingface-model'\nbatch_size = 16\nmax_length = 256","8b2e304c":"train_dataset = Dataset.from_pandas(train[train.kfold != 0].reset_index(drop=True))\nvalid_dataset = Dataset.from_pandas(train[train.kfold == 0].reset_index(drop=True))","329bc041":"#Creating tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\ndef tokenize(batch): return tokenizer(batch['excerpt'], padding=True,truncation=True, max_length=max_length)","ab675913":"train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\nvalid_dataset = valid_dataset.map(tokenize, batched=True, batch_size=len(valid_dataset))","6e7ab344":"columns_to_return = ['input_ids', 'label', 'attention_mask']\ntrain_dataset.set_format(type='torch', columns=columns_to_return)\nvalid_dataset.set_format(type='torch', columns=columns_to_return)","1430cd19":"model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1)","3ea1c8a3":"def compute_metrics(pred):\n    targs = pred.label_ids\n    preds = pred.predictions\n    rmse = mean_squared_error(targs, preds, squared=False)\n    return {\n        'rmse': rmse,\n    }\n\nargs = TrainingArguments(\n    \"outputs_dir\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    fp16=True,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    seed=7,\n    weight_decay=0.005,\n    load_best_model_at_end=True,\n)\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","a43f4fe1":"trainer.train()\n","c6857bc6":"test = test.rename(columns={'target':'label'})\ntest['label'] = 1\n\ntest_dataset = Dataset.from_pandas(test)\ntest_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))","9702194e":"columns_to_return = ['input_ids', 'label', 'attention_mask']\ntest_dataset.set_format(type='torch', columns=columns_to_return)","33079fd2":"test_preds = trainer.predict(test_dataset)\ntest_preds= scaler.inverse_transform(pd.DataFrame(test_preds[0].reshape(1,-1)[0]))","4d7d4b56":"test_ids = test['id'].values\n\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'target': test_preds.reshape(1,-1)[0]\n})\n\n","366a3a56":"submission.to_csv('submission.csv', index=False)\n","80d281da":"**EDA**\n* Almost 70 percent of values in **url_legal and licence are null**","987eb615":"**Hugging Face Model**\n\nIn the script I have tried to do prediction using hugging face model after performing text cleaning. I have taken help from other scripts on kaggle and tried to improve the solution.\nTo use the scripts you need to add following data to run file offline(without internet on kaggle):\n1. distillbert-huggingface-model\n2. hf-datasets\n3. nltk-stopwords\n\nIn the script I have performed the following steps:\n* EDA and text cleaning\n* Custom K-fold\n* Setting up Hugging face model\n* Prediction\n\nPlease comment and let me know your suggestions and what additional could have been done. Thank you\n\nUsed following script for reference.\n\"https:\/\/www.kaggle.com\/thedrcat\/commonlit-hf-minimalistic-example\"","f9c168dc":"**Using Standatd Scaler on target_variable**","33c69eac":"* Creating custom function to create k-folds","847646c8":"* Creating train and validation datasets","3c342d93":"**Cleaning Texts**`","00aa52b5":"To make text lowercase, remove text in square brackets,remove links,remove punctuation and remove words containing numbers.\n"}}