{"cell_type":{"83c92f3a":"code","f5bbb9fd":"code","35ad9e66":"code","2bee10f3":"code","4c9219c1":"code","63a36a51":"code","1ab37a73":"code","b2cfdabd":"code","c77f310d":"code","c1dde48a":"code","476a3cd0":"code","c5be03d5":"code","3b0292f1":"code","c9abb63c":"code","dad90c71":"code","56235954":"code","b14de59f":"code","094da107":"code","ef875928":"code","ae2934af":"code","bba9407a":"code","c9ef37cf":"code","96bfccbf":"code","fc9c7679":"code","677cf24b":"code","5c25c406":"code","7ec4cfed":"code","b3d71c3a":"code","c73669e8":"code","72d5e84c":"code","8fa25e8a":"code","d98ab9d0":"code","b197dddb":"code","3203e3c8":"code","9c97b202":"code","de76e6ed":"markdown"},"source":{"83c92f3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nplt.rc('font', size=16)\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\nwarnings.filterwarnings('ignore')\ntf.get_logger().setLevel('ERROR')\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nprint(tf.__version__)","f5bbb9fd":"# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","35ad9e66":"train_dir = '\/kaggle\/input\/training'  # first oversampled \ndataset = pd.read_csv('\/kaggle\/input\/training\/Training.csv')\nprint(dataset.shape)\ndataset.head()","2bee10f3":"dataset.info()\ndataset.columns","4c9219c1":"def inspect_dataframe(df, columns):\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n    for i, col in enumerate(columns):\n        axs[i].plot(df[col])\n        axs[i].set_title(col)\n    plt.show()\ninspect_dataframe(dataset, dataset.columns)","63a36a51":"test_size = 864*11\ndataset_0 = dataset\ndataset_test = dataset.iloc[-test_size:]\nX_train_raw = dataset.iloc[:-test_size]\n# y_train_raw = y.iloc[:-test_size]\nX_test_raw = dataset.iloc[-test_size:]\n# y_test_raw = y.iloc[-test_size:]\nprint(X_train_raw.shape, X_test_raw.shape)\n\n# Normalize both features and labels\nX_min = X_train_raw.min()\nX_max = X_train_raw.max()\n\nX_train_raw = (X_train_raw-X_min)\/(X_max-X_min)\nX_test_raw = (X_test_raw-X_min)\/(X_max-X_min)\n\nob = \"X_train_raw.Hype root\"\nplt.figure(figsize=(17,5))\nplt.plot(X_train_raw.Crunchiness, label='Train (sponginess)')\nplt.plot(X_test_raw.Crunchiness, label='Test (sponginess)')\nplt.title('Train-Test Split')\nplt.legend()\nplt.show()\n","1ab37a73":"telescope = 864\nwindow = int(telescope*10)\nstride = int((window\/24)) #24 \u00e8 base, 12, 48. 24 meglio\n#stride = 10\n[window, stride]","b2cfdabd":"future = dataset[-(window+telescope):-telescope]\nfuture = (future-X_min)\/(X_max-X_min)\n#future = X_test_raw.iloc[:-telescope]\ny_future = X_test_raw.iloc[-telescope:]\nfuture = np.expand_dims(future, axis=0)\ny_future = np.expand_dims(y_future, axis=0)\n[future.shape, y_future.shape]\n#future.shape","c77f310d":"def build_sequences(df, target_labels=['Sponginess'], window=200, stride=20, telescope=100):\n  # target labels used to specify which classes we want to predict\n  # telescope is how many samples we want to predict in the future\n    # Sanity check to avoid runtime errors\n    assert window % stride == 0\n    dataset = []\n    labels = []\n    temp_df = df.copy().values\n    temp_label = df[target_labels].copy().values\n    padding_len = len(df)%window\n\n    if(padding_len != 0):\n        # Compute padding length\n        padding_len = window - len(df)%window\n        padding = np.zeros((padding_len,temp_df.shape[1]), dtype='float64')\n        temp_df = np.concatenate((padding,df))\n        padding = np.zeros((padding_len,temp_label.shape[1]), dtype='float64')\n        temp_label = np.concatenate((padding,temp_label))\n        assert len(temp_df) % window == 0\n\n    for idx in np.arange(0,len(temp_df)-window-telescope,stride):\n        dataset.append(temp_df[idx:idx+window])\n        labels.append(temp_label[idx+window:idx+window+telescope])\n\n    dataset = np.array(dataset)\n    labels = np.array(labels)\n    return dataset, labels","c1dde48a":"target_labels = dataset.columns # we want to predict all labels\n#telescope = 100\ntarget_labels\n","476a3cd0":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\nX_test, y_test = build_sequences(X_test_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n","c5be03d5":"# plot one sample of training dataset: in blue the data in orange the target\ndef inspect_multivariate(X, y, columns, telescope, idx=None):\n    if(idx==None):\n        idx=np.random.randint(0,len(X))\n\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n    for i, col in enumerate(columns):\n        axs[i].plot(np.arange(len(X[0,:,i])), X[idx,:,i])\n        axs[i].scatter(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), y[idx,:,i], color='orange')\n        axs[i].set_title(col)\n        axs[i].set_ylim(0,1)\n    plt.show()","3b0292f1":"inspect_multivariate(X_train, y_train, target_labels, telescope)","c9abb63c":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 64\nepochs = 200\nvalid_split = 0.3\ninput_shape ,output_shape","dad90c71":"METRICS = [tfk.metrics.RootMeanSquaredError(name=\"RMSE\"),\n          tfk.metrics.MeanSquaredError(name=\"MSE\"),\n          tfk.metrics.MeanAbsoluteError(name=\"mae\")]","56235954":"def build_LSTM_model(input_shape, classes):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n\n    # Feature extractor\n    lstm = tfkl.LSTM(128, return_sequences=True)(input_layer) # LSTM keras layer -> 128 size of memory for each timesteps instead of 1 we have 128 values for each timestamp HYPERPARAMETER\n                                                              # output depends on 128 values coming from memory and new timestamp->output is again a sequence: fundamental if input of new LSTM\n                                                              # Otherwie output at the last timestemp will be given, all the rest is hidden: i get a sort of embedding of the sequence\n                                                              # second LSTM at given timestamp takes as input output of LSTM at same timestamp\n                                                              # Usually last LSTM return_sequence=False -> is input of classifier\n    lstm = tfkl.LSTM(128)(lstm) # possible to stack LSTM layers -> all except last should have return_sequences=True \n                                # so to obtain sequence of each output through time NECESSARY to stack LSTM layers\n                                # possible to have different dimensions\n                                # last layer capture temporal correlation among samples in time series                                \n                                # return_sequence can be put in last layer if I need all history with flattening -> then constrained to use same number of samples bc fix neurons for class\n                                # also if i want to decide how to treat directly all the results, it depends on the task\n    dropout = tfkl.Dropout(.5, seed=seed)(lstm) \n\n    # OUTPUT\n    dense = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu')(dropout)\n    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(dense)\n    #output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same')(output_layer)\n\n    # Connect input and output through the Model class\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    # Compile the model\n    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(), metrics=METRICS)\n\n    # Return the model\n    return model","b14de59f":"def build_LSTM_1_model(input_shape, classes):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n\n    # Feature extractor\n    lstm = tfkl.LSTM(64)(input_layer) # LSTM keras layer -> 128 size of memory for each timesteps instead of 1 we have 128 values for each timestamp HYPERPARAMETER\n                                                              # output depends on 128 values coming from memory and new timestamp->output is again a sequence: fundamental if input of new LSTM\n                                                              # Otherwie output at the last timestemp will be given, all the rest is hidden: i get a sort of embedding of the sequence\n                                                              # second LSTM at given timestamp takes as input output of LSTM at same timestamp\n                                                              # Usually last LSTM return_sequence=False -> is input of classifier\n\n    dropout = tfkl.Dropout(.5, seed=seed)(lstm) \n\n    # OUTPUT\n    dense = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu')(dropout)\n    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(dense)\n    #output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same')(output_layer)\n\n    # Connect input and output through the Model class\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    # Compile the model\n    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(), metrics=METRICS)\n\n    # Return the model\n    return model","094da107":"model = build_LSTM_model(input_shape, output_shape)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True)","ef875928":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split=valid_split,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n).history","ae2934af":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Absolute Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['MSE'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_MSE'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['RMSE'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_RMSE'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('RMSE')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(18,3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","bba9407a":"model.save('DirectForecasting_10')","c9ef37cf":"# Predict the test set \npredictions = model.predict(X_test)\nprint(predictions.shape)\n\nmean_squared_error = tfk.metrics.mse(y_test.flatten(),predictions.flatten())\nmean_absolute_error = tfk.metrics.mae(y_test.flatten(),predictions.flatten())\nrmse = np.sqrt(mean_absolute_error)\nmean_squared_error, mean_absolute_error, rmse","96bfccbf":"def inspect_multivariate_prediction(X, y, pred, columns, telescope, idx=None):\n    if(idx==None):\n        idx=np.random.randint(0,len(X))\n\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n    for i, col in enumerate(columns):\n        axs[i].plot(np.arange(len(X[0,:,i])), X[idx,:,i])\n        axs[i].scatter(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), y[idx,:,i], color='orange')\n        axs[i].scatter(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), pred[idx,:,i], color='green')\n        axs[i].set_title(col)\n        axs[i].set_ylim(0,1)\n    plt.show()","fc9c7679":"inspect_multivariate_prediction(X_test, y_test, predictions, target_labels, telescope)","677cf24b":"maes = []\nfor i in range(predictions.shape[1]):\n    ft_maes = []\n    for j in range(predictions.shape[2]):\n        ft_maes.append(np.mean(np.abs(y_test[:,i,j]-predictions[:,i,j]), axis=0))\n    ft_maes = np.array(ft_maes)\n    maes.append(ft_maes)\nmaes = np.array(maes)","5c25c406":"future_predictions = model.predict(future)","7ec4cfed":"def column_errors(prediction,real,columns):\n    MSE = np.zeros((columns.shape))\n    MAE = np.zeros((columns.shape))\n    RMSE = np.zeros((columns.shape))\n    for col in range(columns.shape[0]):\n        y_p = prediction[0,:,col]\n        y_r = real[0,:,col]      \n        mse = tfk.metrics.mse(y_r.flatten(),y_p.flatten())\n        mae = tfk.metrics.mae(y_r.flatten(),y_p.flatten())\n        rmse = np.sqrt(mse)\n        MAE[col] = mae\n        MSE[col] = mse\n        RMSE[col] = rmse\n\n        \n    return MSE, MAE ,RMSE","b3d71c3a":"MSE, MAE ,RMSE = column_errors(future_predictions,y_future,target_labels)\n#np.savetxt('col_errors', [MSE,MAE,RMSE], delimiter=' ')","c73669e8":"def print_col_errors(MSE,MAE,RMSE,columns):\n    figs, axs = plt.subplots(3, 1, sharex=True, figsize=(25,17))\n    axs[0].bar(columns,MAE*100)\n    axs[0].set_title(['MAE'])\n    axs[1].bar(columns,MSE*100)\n    axs[1].set_title(['MSE'])\n    axs[2].bar(columns,RMSE*100)\n    axs[2].set_title(['RMSE'])\n    \n    plt.show()  \n    ","72d5e84c":"print_col_errors(MSE,MAE,RMSE,target_labels)","8fa25e8a":"#inspect_multivariate_prediction(future, y_future, future_predictions, target_labels, telescope)","d98ab9d0":"figs, axs = plt.subplots(len(target_labels), 1, sharex=True, figsize=(17,17))\nfor i, col in enumerate(target_labels):\n    axs[i].plot(np.arange(len(future[0,:,i])), future[0,:,i])\n    axs[i].plot(np.arange(len(future[0,:,i]), len(future[0,:,i])+telescope), future_predictions[0,:,i], color='orange')\n    axs[i].fill_between(\n        np.arange(len(future[0,:,i]), len(future[0,:,i])+telescope), \n        future_predictions[0,:,i]+maes[:,i], \n        future_predictions[0,:,i]-maes[:,i], \n        color='orange', alpha=.3)\n    axs[i].set_title(col)\n    # axs[i].set_ylim(0,1)\nplt.show()\n# we show mean and std variation","b197dddb":"X_train_raw = dataset\nX_train_raw = (X_train_raw-X_min)\/(X_max-X_min)\nX_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\n#model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(), metrics=['mae'])\nmodel = build_LSTM_model(input_shape, output_shape)\n# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split=valid_split,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n).history","3203e3c8":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Absolute Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['MSE'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_MSE'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['RMSE'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_RMSE'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('RMSE')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(18,3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","9c97b202":"model.save('DirectForecasting_10_pro')","de76e6ed":"test_size = 864*6  \nX_test_raw = dataset.iloc[-test_size:]\nX_train_raw = X_train_raw = dataset.iloc[:-test_size]\nX_train_raw = dataset\n# y_test_raw = y.iloc[-test_size:]\nprint(X_train_raw.shape, X_test_raw.shape)\n\n# Normalize both features and labels\nX_min = X_train_raw.min()\nX_max = X_train_raw.max()\n\nX_train_raw = (X_train_raw-X_min)\/(X_max-X_min)\nX_test_raw = (X_test_raw-X_min)\/(X_max-X_min)\n\nob = \"X_train_raw.Hype root\"\nplt.figure(figsize=(17,5))\nplt.plot(X_train_raw.Crunchiness, label='Train (sponginess)')\nplt.plot(X_test_raw.Crunchiness, label='Test (sponginess)')\nplt.title('Train-Test Split')\nplt.legend()\nplt.show()\n"}}