{"cell_type":{"3892047d":"code","f8f160de":"code","582755d9":"code","c3a049f2":"code","038b3bc1":"code","e107c214":"code","dc7cf60e":"code","b8e31fdf":"code","6968e417":"code","3e66835e":"code","528b8e6a":"code","59458dd1":"code","d45a6b42":"code","ac0de06c":"code","ab749765":"code","919e4515":"code","79ebfda9":"code","9bd927d8":"code","d8eb19d4":"code","34805f5c":"code","b7633313":"code","6c6b7695":"code","a784365d":"code","f4ef7e7e":"code","a68aca36":"code","0e3fd131":"code","9427cd22":"code","cdb20eb1":"code","465b8bf6":"code","d26b69ca":"code","97827cba":"code","d07e105d":"code","5613c328":"code","3ccd0b44":"code","5c6f4aad":"code","39d523aa":"code","a2008190":"code","119fa28a":"code","de7acaa5":"code","0a36f417":"code","6e02f5fd":"code","1ac7d587":"code","40b4d48d":"code","bcd86c95":"code","ce53af13":"code","9070de5b":"code","2a5eba05":"code","e9c5047c":"code","6cc144fa":"code","8329e61f":"code","ae7f8020":"code","ba611761":"code","c300ffca":"code","7bd9aa99":"code","8d98c3cb":"code","95189acf":"code","e048ff78":"code","b171f0c7":"code","ae8c649a":"code","26254f8d":"code","51fac72b":"code","66e5bb38":"code","d2d7f9d2":"code","7b5d7aae":"code","5ab45d59":"code","d3321387":"code","dbb0e57e":"code","384794ee":"code","ad103723":"code","9d243cca":"code","66b795d7":"code","ef3b18cb":"code","c2fe27f8":"code","2d11a28c":"code","4812c33f":"code","a7510bc9":"code","659a3991":"code","cdb6524d":"code","6db400c6":"code","170fa95b":"code","a2de1207":"code","6abc1921":"code","1f60b312":"code","efd4e044":"code","ed4b08fc":"code","8dfca081":"code","9414b334":"code","6786e0fd":"code","ff018c44":"code","a827aeb5":"code","479dd533":"code","04b8da7b":"code","12187a53":"code","78248495":"code","28d6464d":"code","80d9d6ad":"code","82ca0af2":"markdown","873d083c":"markdown","5251dcc9":"markdown","9cc547bb":"markdown","0c699964":"markdown","7966da36":"markdown","5bf15177":"markdown","a283a829":"markdown","e54daa86":"markdown","7a9fdae8":"markdown","58bd0e9d":"markdown","834e843c":"markdown","c6a0d15e":"markdown","5bfc6320":"markdown","fd34736f":"markdown","9c058a57":"markdown","8e78fee8":"markdown","3db254b5":"markdown","6fac333a":"markdown","5da72dc5":"markdown","aece4fbe":"markdown","d8bb7fb5":"markdown","ae639a5c":"markdown","f8f0cadd":"markdown","cf4abe5a":"markdown","6b7748c4":"markdown","a7c196bb":"markdown","5cbdbc08":"markdown","abf39dc1":"markdown","5d1c2788":"markdown"},"source":{"3892047d":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f8f160de":"\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","582755d9":"train.head()","c3a049f2":"train.info()","038b3bc1":"plt.style.use('seaborn')\nplt.figure(figsize=(10,5))\nsns.heatmap(train.isnull(), yticklabels = False, cmap='plasma')\nplt.title('Null Values in Training Set');","e107c214":"df=pd.DataFrame(train)","dc7cf60e":"df.drop(\"PassengerId\",axis=1,inplace= True)\ndf.drop(\"Cabin\",axis=1,inplace=True)","b8e31fdf":"df.head()","6968e417":"sns.set_style(\"whitegrid\")\nsns.FacetGrid(df,hue=\"Survived\",size=8)\\\n.map(plt.scatter,\"Age\",\"Fare\")\\\n.add_legend();\nplt.show();","3e66835e":"df1=df[[\"Age\",\"Fare\",\"Survived\"]]\n","528b8e6a":"df1=pd.DataFrame(df1)","59458dd1":"plt.close();\nsns.set_style(\"whitegrid\");\nsns.pairplot(df1,hue=\"Survived\",height=4);\nplt.show();","d45a6b42":"sns.FacetGrid(df,hue=\"Survived\",height=6)\\\n.map(sns.distplot,\"Age\")\\\n.add_legend();\nplt.show();","ac0de06c":"sns.FacetGrid(df,hue=\"Survived\",height=6)\\\n.map(sns.distplot,\"Fare\")\\\n.add_legend();\nplt.show();","ab749765":"target = train.Survived","919e4515":"print(f'Unique Values in Pclass :{train.Pclass.unique()}')","79ebfda9":"print(f'Unique Values in SibSp :{train.SibSp.unique()}')","9bd927d8":"print(f'Unique Values in Embarked :{train.Embarked.unique()}')","d8eb19d4":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train.Survived)\nplt.title('Number of passenger Survived');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Sex\", data=train)\nplt.title('Number of passenger Survived');","34805f5c":"plt.figure(figsize=(15,5))\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1,2,1)\nsns.countplot(train['Pclass'])\nplt.title('Count Plot for PClass');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Pclass\", data=train)\nplt.title('Number of passenger Survived');","b7633313":"pclass1 = train[train.Pclass == 1]['Survived'].value_counts(normalize=True).values[0]*100\npclass2 = train[train.Pclass == 2]['Survived'].value_counts(normalize=True).values[1]*100\npclass3 = train[train.Pclass == 3]['Survived'].value_counts(normalize=True).values[1]*100\n\nprint(\"Lets look at some satistical data!\\n\")\nprint(\"Pclaas-1: {:.1f}% People Survived\".format(pclass1))\nprint(\"Pclaas-2: {:.1f}% People Survived\".format(pclass2))\nprint(\"Pclaas-3: {:.1f}% People Survived\".format(pclass3))","6c6b7695":"train['Age'].nlargest(10).plot(kind='bar', color = ['#5946B2','#9C51B6']);\nplt.title('10 largest Ages')\nplt.xlabel('Index')\nplt.ylabel('Ages');","a784365d":"train['Age'].plot(kind='hist')","f4ef7e7e":"train['Age'].hist(bins=40)\nplt.title('Age Distribution');","a68aca36":"# set plot size\nplt.figure(figsize=(15, 3))\n\n# plot a univariate distribution of Age observations \nsns.distplot(train[(train[\"Age\"] > 0)].Age, kde_kws={\"lw\": 3}, bins = 50)\n\n# set titles and labels\nplt.title('Distrubution of passengers age',fontsize= 14)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n# clean layout\nplt.tight_layout()","0e3fd131":"plt.figure(figsize=(15, 3))\n\n# Draw a box plot to show Age distributions with respect to survival status.\nsns.boxplot(y = 'Survived', x = 'Age', data = train,\n     palette=[\"#3f3e6fd1\", \"#85c6a9\"], fliersize = 0, orient = 'h')\n\n# Add a scatterplot for each category.\nsns.stripplot(y = 'Survived', x = 'Age', data = train,\n     linewidth = 0.6, palette=[\"#3f3e6fd1\", \"#85c6a9\"], orient = 'h')\n\nplt.yticks( np.arange(2), ['drowned', 'survived'])\nplt.title('Age distribution grouped by surviving status (train data)',fontsize= 14)\nplt.ylabel('Passenger status after the tragedy')\nplt.tight_layout()","9427cd22":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train['SibSp'])\nplt.title('Number of siblings\/spouses aboard');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"SibSp\", data=train)\nplt.legend(loc='right')\nplt.title('Number of passenger Survived');","cdb20eb1":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train['Embarked'])\nplt.title('Number of Port of embarkation');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Embarked\", data=train)\nplt.legend(loc='right')\nplt.title('Number of passenger Survived');","465b8bf6":"sns.heatmap(train.corr(), annot=True)\nplt.title('Corelation Matrix');","d26b69ca":"corr = train.corr()\nsns.heatmap(corr[((corr >= 0.3) | (corr <= -0.3)) & (corr != 1)], annot=True, linewidths=.5, fmt= '.2f')\nplt.title('Configured Corelation Matrix');","97827cba":"sns.catplot(x=\"Embarked\", y=\"Fare\", kind=\"violin\", inner=None,\n            data=train, height = 6, order = ['C', 'Q', 'S'])\nplt.title('Distribution of Fare by Embarked')\nplt.tight_layout()","d07e105d":"sns.catplot(x=\"Pclass\", y=\"Fare\", kind=\"swarm\", data=train, height = 6)\n\nplt.tight_layout()","5613c328":"sns.catplot(x=\"Pclass\", y=\"Fare\",  hue = \"Survived\", kind=\"swarm\", data=train, \n                                    palette=[\"#3f3e6fd1\", \"#85c6a9\"], height = 6)\nplt.tight_layout()","3ccd0b44":"train['Fare'].nlargest(10).plot(kind='bar', title = '10 largest Fare', color = ['#C62D42', '#FE6F5E']);\nplt.xlabel('Index')\nplt.ylabel('Fare');","5c6f4aad":"train['Age'].nsmallest(10).plot(kind='bar', color = ['#A83731','#AF6E4D'])\nplt.title('10 smallest Ages')\nplt.xlabel('Index')\nplt.ylabel('Ages');","39d523aa":"train.isnull().sum()","a2008190":"test.isnull().sum()","119fa28a":"sns.heatmap(train.corr(), annot=True)","de7acaa5":"train.loc[train.Age.isnull(), 'Age'] = train.groupby(\"Pclass\").Age.transform('median')\n\n\n#Same thing for test set\ntest.loc[test.Age.isnull(), 'Age'] = test.groupby(\"Pclass\").Age.transform('median')","0a36f417":"train.Embarked.value_counts()","6e02f5fd":"from statistics import mode\ntrain['Embarked'] = train['Embarked'].fillna(mode(train['Embarked']))\n\n#Applying the same technique for test set\ntest['Embarked'] = test['Embarked'].fillna(mode(test['Embarked']))","1ac7d587":"train['Fare']  = train.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))\ntest['Fare']  = test.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))","40b4d48d":"train.Cabin.value_counts()","bcd86c95":"train['Cabin'] = train['Cabin'].fillna('U')\ntest['Cabin'] = test['Cabin'].fillna('U')","ce53af13":"train.Sex.unique()","9070de5b":"train['Sex'][train['Sex'] == 'male'] = 0\ntrain['Sex'][train['Sex'] == 'female'] = 1\n\ntest['Sex'][test['Sex'] == 'male'] = 0\ntest['Sex'][test['Sex'] == 'female'] = 1","2a5eba05":"train.Embarked.unique()","e9c5047c":"train.columns","6cc144fa":"train.Cabin.tolist()[0:20]","8329e61f":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(train[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])\ntrain = train.join(temp)\ntrain.drop(columns='Embarked', inplace=True)\n\ntemp = pd.DataFrame(encoder.transform(test[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])\ntest = test.join(temp)\ntest.drop(columns='Embarked', inplace=True)","ae7f8020":"import re\ntrain['Cabin'] = train['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())\ntest['Cabin'] = test['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())","ba611761":"train.Cabin.unique()","c300ffca":"cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9}\ntrain['Cabin'] = train['Cabin'].map(cabin_category)\ntest['Cabin'] = test['Cabin'].map(cabin_category)","7bd9aa99":"train.Name","8d98c3cb":"train['Name'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\ntest['Name'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)","95189acf":"train['Name'].unique().tolist()","e048ff78":"train.rename(columns={'Name' : 'Title'}, inplace=True)\ntrain['Title'] = train['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')\n                                      \ntest.rename(columns={'Name' : 'Title'}, inplace=True)\ntest['Title'] = test['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')","b171f0c7":"train['Title'].value_counts(normalize = True) * 100","ae8c649a":"encoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(train[['Title']]).toarray())\ntrain = train.join(temp)\ntrain.drop(columns='Title', inplace=True)\n\ntemp = pd.DataFrame(encoder.transform(test[['Title']]).toarray())\ntest = test.join(temp)\ntest.drop(columns='Title', inplace=True)","26254f8d":"train['familySize'] = train['SibSp'] + train['Parch'] + 1\ntest['familySize'] = test['SibSp'] + test['Parch'] + 1","51fac72b":"fig = plt.figure(figsize = (15,4))\n\nax1 = fig.add_subplot(121)\nax = sns.countplot(train['familySize'], ax = ax1)\n\n# calculate passengers for each category\nlabels = (train['familySize'].value_counts())\n# add result numbers on barchart\nfor i, v in enumerate(labels):\n    ax.text(i, v+6, str(v), horizontalalignment = 'center', size = 10, color = 'black')\n    \nplt.title('Passengers distribution by family size')\nplt.ylabel('Number of passengers')\n\nax2 = fig.add_subplot(122)\nd = train.groupby('familySize')['Survived'].value_counts(normalize = True).unstack()\nd.plot(kind='bar', color=[\"#3f3e6fd1\", \"#85c6a9\"], stacked='True', ax = ax2)\nplt.title('Proportion of survived\/drowned passengers by family size (train data)')\nplt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\nplt.xticks(rotation = False)\n\nplt.tight_layout()","66e5bb38":"# Drop redundant features\ntrain = train.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)\ntest = test.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)","d2d7f9d2":"train.head()","7b5d7aae":"columns = train.columns\nfrom sklearn.preprocessing import StandardScaler\nX_train = StandardScaler().fit_transform(train)\n\nnew_df = pd.DataFrame(X_train, columns=columns)","5ab45d59":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\ndf_pca = pca.fit_transform(new_df)","d3321387":"plt.figure(figsize =(8, 6))\nplt.scatter(df_pca[:, 0], df_pca[:, 1], c = target, cmap ='plasma')\n# labeling x and y axes\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component');","dbb0e57e":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived', 'PassengerId'], axis=1), train['Survived'], test_size = 0.2, random_state=2)","384794ee":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(linreg.score(X_train, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(linreg.score(X_test, y_test)))","ad103723":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(max_iter=10000, C=50)\nlogreg.fit(X_train, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(logreg.score(X_test, y_test)))","9d243cca":"print(logreg.intercept_)\nprint(logreg.coef_)","66b795d7":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\n\n# we must apply the scaling to the test set that we computed for the training set\nX_test_scaled = scaler.transform(X_test)","ef3b18cb":"logreg = LogisticRegression(max_iter=10000)\nlogreg.fit(X_train_scaled, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(logreg.score(X_test_scaled, y_test)))","c2fe27f8":"from sklearn.neighbors import KNeighborsClassifier\n\nknnclf = KNeighborsClassifier(n_neighbors=7)\n\n# Train the model using the training sets\nknnclf.fit(X_train, y_train)\ny_pred = knnclf.predict(X_test)","2d11a28c":"from sklearn.metrics import accuracy_score\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))","4812c33f":"knnclf = KNeighborsClassifier(n_neighbors=7)\n\n# Train the model using the scaled training sets\nknnclf.fit(X_train_scaled, y_train)\ny_pred = knnclf.predict(X_test_scaled)","a7510bc9":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))","659a3991":"from sklearn.svm import LinearSVC\n\nsvmclf = LinearSVC(C=50)\nsvmclf.fit(X_train, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svmclf.score(X_train, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svmclf.score(X_test, y_test)))","cdb6524d":"svmclf = LinearSVC()\nsvmclf.fit(X_train_scaled, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svmclf.score(X_train_scaled, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svmclf.score(X_test_scaled, y_test)))","6db400c6":"from sklearn.svm import SVC\n\nsvcclf = SVC(gamma=0.1)\nsvcclf.fit(X_train, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svcclf.score(X_train, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svcclf.score(X_test, y_test)))","170fa95b":"svcclf = SVC(gamma=50)\nsvcclf.fit(X_train_scaled, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svcclf.score(X_train_scaled, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svcclf.score(X_test_scaled, y_test)))","a2de1207":"from sklearn.tree import DecisionTreeClassifier\n\ndtclf = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(dtclf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(dtclf.score(X_test, y_test)))","6abc1921":"from sklearn.ensemble import RandomForestClassifier\nrfclf = RandomForestClassifier(random_state=2)","1f60b312":"# Set our parameter grid\nparam_grid = { \n    'criterion' : ['gini', 'entropy'],\n    'n_estimators': [100, 300, 500],\n    'max_features': ['auto', 'log2'],\n    'max_depth' : [3, 5, 7]    \n}","efd4e044":"from sklearn.model_selection import GridSearchCV\n\nrandomForest_CV = GridSearchCV(estimator = rfclf, param_grid = param_grid, cv = 5)\nrandomForest_CV.fit(X_train, y_train)","ed4b08fc":"randomForest_CV.best_params_","8dfca081":"rf_clf = RandomForestClassifier(random_state = 2, criterion = 'gini', max_depth = 7, max_features = 'auto', n_estimators = 100)\n\nrf_clf.fit(X_train, y_train)","9414b334":"predictions = rf_clf.predict(X_test)","6786e0fd":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, predictions) * 100","ff018c44":"#Linear Model\nprint(\"Linear Model R-Squared for Train set: {:.3f}\".format(linreg.score(X_train, y_train)))\nprint(\"Linear Model R-Squared for test set: {:.3f}\" .format(linreg.score(X_test, y_test)))\nprint()\n\n#Logistic Regression\nprint(\"Logistic Regression R-Squared for Train set: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\nprint(\"Logistic Regression R-Squared for test set: {:.3f}\" .format(logreg.score(X_test_scaled, y_test)))\nprint()\n\n#KNN Classifier\nprint(\"KNN Classifier Accuracy:\",accuracy_score(y_test, y_pred))\nprint()\n\n#SVM\nprint('SVM Accuracy on training set: {:.2f}'\n     .format(svmclf.score(X_train_scaled, y_train)))\nprint('SVM Accuracy on test set: {:.2f}'\n     .format(svmclf.score(X_test_scaled, y_test)))\nprint()\n\n#Kerelize SVM\nprint('SVC Accuracy on training set: {:.2f}'\n     .format(svcclf.score(X_train_scaled, y_train)))\nprint('Accuracy on test set: {:.2f}'\n     .format(svcclf.score(X_test_scaled, y_test)))\nprint()\n\n#Decision Tree\nprint('Accuracy of Decision Tree on training set: {:.2f}'\n     .format(dtclf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree on test set: {:.2f}'\n     .format(dtclf.score(X_test, y_test)))\nprint()\n\n#Random Forest\nprint('Random Forest Accuracy:{:.3f}'.format(accuracy_score(y_test, predictions) * 100))","a827aeb5":"scaler = MinMaxScaler()\n\ntrain_conv = scaler.fit_transform(train.drop(['Survived', 'PassengerId'], axis=1))\ntest_conv = scaler.transform(test.drop(['PassengerId'], axis = 1))","479dd533":"svcclf = SVC(gamma=50)\nsvcclf.fit(train_conv, train['Survived'])","04b8da7b":"test['Survived'] = svcclf.predict(test_conv)","12187a53":"test[['PassengerId', 'Survived']].to_csv('MySubmission1.csv', index = False)","78248495":"from xgboost import XGBClassifier","28d6464d":"xgb=XGBClassifier()\n","80d9d6ad":"xgb.fit(train_conv,train['Survived'])","82ca0af2":"It is evident that the people with family had more chance of survival than the single people","873d083c":"Thus P class is a also a good parameter","5251dcc9":"Analysing Pclass","9cc547bb":"thus the data is very much mixed and can't be separated easily ","0c699964":"Thus we don't have the situation of multicollinearity","7966da36":"\nLooking for correlation and Multicollinearity","5bf15177":"- The wider fare distribution among passengers who embarked in Cherbourg. It makes scence - many first-class passengers boarded the ship here, but the share of third-class passengers is quite significant.\n- The smallest variation in the price of passengers who boarded in q. Also, the average price of these passengers is the smallest, I think this is due to the fact that the path was supposed to be the shortest + almost all third-class passengers.","a283a829":"The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a very low probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback\u2013Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this can be changed as appropriate.","e54daa86":"thus it is clear that the data is non uniform and contains a lot of outliers","7a9fdae8":"First let's analyse the numerical data available","58bd0e9d":"Let's look at the age column","834e843c":"## It's time to analyze different features and their relation with survived.","c6a0d15e":"### Titanic: supposed to be an unsinkable ship sank on 31 May 1911. Till date it is considered a great tragedy. We have two things which make titanic very famous:-\n\n1. Titanic Movie\n2. Kaggle Titanic Competition\n\n### We will explore the latter with comprehensive analysis of nearly all the statistical methods and predictive models. We will study feature engineering, dimensionality reduction, conditional of multicolliearity, Null hypothesis and all the statistics test, Curse of dimensionality along with various classification models and parameters tuning.\n\n# LET'S DIVE DEEP INTO OCEAN OF M.L WITH TITANIC\n\n","5bfc6320":"Let's find realtion between Fare and Pclass","fd34736f":"## First step in Machine Learning is checking the Dataset","9c058a57":"We will look for dimensionality reduction Techniques PCA and TSNE next and anayse the rest of the parameters later ","8e78fee8":"In statistics, principal component analysis (PCA) is a method to project data in a higher dimensional space into a lower dimensional space by maximizing the variance of each dimension.\nPCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on\nfor more information refer- https:\/\/abhishek-parashar.github.io\/","3db254b5":"The port of embarkation has a lot to tell. Mostly the elite class embarked from c port so have more chane of surviving","6fac333a":"It is clearly evident that Cabin has most of the values empty and doesn't provide much of the information. Moreover, we don't need PassengerId ","5da72dc5":"## Loading the data set","aece4fbe":"More no of females were saved ","d8bb7fb5":"## Importing Libraries","ae639a5c":"We can observe that the distribution of prices for the second and third class is very similar. The distribution of first-class prices is very different, has a larger spread, and on average prices are higher.\n\nLet's add colours to our points to indicate surviving status of passenger (there will be only data from training part of the dataset):","f8f0cadd":"Fare and embarked realtionship","cf4abe5a":"## PCA & TSNE","6b7748c4":"It is evident that age played an important factor in survival. As we know as well that the children and old aged people were sent through a life boat","a7c196bb":"<img src=\"http:\/\/data.freehdw.com\/ships-titanic-vehicles-best.jpg\"  Width=\"800\">","5cbdbc08":"looking at the survived","abf39dc1":"Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related. We have perfect multicollinearity if, for example as in the equation above, the correlation between two independent variables is equal to 1 or \u22121.","5d1c2788":"Source: Wikipedia "}}