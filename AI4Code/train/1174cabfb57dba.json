{"cell_type":{"b9a43776":"code","58ca1dd4":"code","f5b4f4bf":"code","1725f5ec":"code","b27900ba":"code","fa5170aa":"code","e87c8bdb":"code","c68a82c8":"code","a69b9fcc":"code","04671476":"code","2e07138c":"code","ff593b26":"code","bfb15a02":"code","555b62d5":"code","6ac3d57f":"code","09cd3af9":"code","360e0893":"code","03f2cf2e":"code","76d28d24":"code","4eb3e7dc":"code","e023283a":"code","530c8c5d":"code","7cfb0ce4":"code","ff4e8296":"code","c34418a2":"code","1ef52419":"markdown","2dae73bc":"markdown","1d189602":"markdown","79f3cd23":"markdown","ba4ba543":"markdown","957817ed":"markdown","0766d280":"markdown","225f88a1":"markdown","1891dde7":"markdown","eb2edec1":"markdown","0f5f27eb":"markdown","b0a0a75a":"markdown","f4a70977":"markdown","d6dd94b7":"markdown","ffd269dd":"markdown","7beb43b8":"markdown","0e883cc6":"markdown"},"source":{"b9a43776":"!pip install pycocotools","58ca1dd4":"import json\nimport os\nimport datetime\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy import ndimage as nd\nimport imageio as io\n\nimport pycocotools\nfrom pycocotools.mask import encode\nimport pycocotools.coco as coco\nfrom pycocotools.coco import COCO","f5b4f4bf":"def mask_to_bbox_corners(mask, mode='numpy'):\n  '''given a binary mask (0 or int>0) returns the\n     bounding box as tuple row0, row1, col0, col1 \n     if mode=='XYXY' return a list row0, col0, row1,col1\n\n     Enum of different ways to represent a box.\n\nIn detectron2:\nXYXY_ABS= 0\n(x0, y0, x1, y1) in absolute floating points coordinates.\nThe coordinates in range [0, width or height].\n  '''\n  col_0 = np.nonzero(mask1.any(axis=0))[0][0]\n  col_1 = np.nonzero(mask1.any(axis=0))[0][-1]\n  row_0 = np.nonzero(mask1.any(axis=1))[0][0]\n  row_1 = np.nonzero(mask1.any(axis=1))[0][-1]\n  if mode == 'numpy':\n     return row_0, row_1,col_0,  col_1\n  if mode == 'XYXY':\n    X0 = int(col_0)\n    X1 = int(col_1)\n    Y0 = int(row_0)\n    Y1 = int(row_1)\n    return [X0, Y0, X1, Y1]\n\n \n","1725f5ec":"def groundtruth_to_masks(groundtruth):\n  grey = data[N,:,:,0]\n  labels = data[N,:,:,1]\n  mask1 = labels == 1\n  mask2 = labels == 2\n  mask3 = labels == 3\n\n  mask1 = 1* np.logical_or(mask1, mask3)\n  mask2 = 1 * np.logical_or(mask2, mask3)\n  return mask1.astype('uint8'), mask2.astype('uint8')","b27900ba":"%pwd\nroot_dir = '\/kaggle\/working'","fa5170aa":"import urllib.request\nurl = \"https:\/\/github.com\/jeanpat\/DeepFISH\/blob\/master\/dataset\/Cleaned_FullRes_2164_overlapping_pairs.npz?raw=true\"#\"https:\/\/www.cs.cmu.edu\/~.\/enron\/enron_mail_20150507.tgz\"\nprint (\"download start!\")\nfilename, headers = urllib.request.urlretrieve(url, filename=root_dir +\"\/Data2164Chrom.npz\")\nprint (\"download complete!\")\nprint (\"download file location: \", filename)\nprint (\"download headers: \", headers)","e87c8bdb":"dataset = np.load(root_dir+'\/Data2164Chrom.npz')\ndata = dataset.f.arr_0\nprint(data.shape)","c68a82c8":"N = 130\ngrey = data[N,:,:,0]\nmask1, mask2 = groundtruth_to_masks(data[N,:,:,1])\n\nplt.figure(figsize=(10,10))\nplt.subplot(141, xticks=[], yticks=[])\nplt.imshow(grey, cmap=plt.cm.gray)\n\nplt.subplot(142, xticks=[], yticks=[])\nplt.title(\"Grountruth\")\nplt.imshow(data[N,:,:,1], interpolation=\"nearest\", cmap=plt.cm.flag_r)\n\nplt.subplot(143, xticks=[], yticks=[])\nplt.title(\"mask:Instance 1\")\nplt.imshow(mask1, interpolation=\"nearest\",cmap=plt.cm.flag_r)\n\nplt.subplot(144, xticks=[], yticks=[])\nplt.title(\"mask:Instance 2\")\nplt.imshow(mask2, interpolation=\"nearest\",cmap=plt.cm.flag_r)\n\n#mask2[mask2 == True]=1\nprint(mask2.max(), mask2.dtype)","a69b9fcc":"if not(os.path.exists(root_dir+'\/Data')):\n  os.mkdir(root_dir+'\/Data')\nif os.path.isdir(root_dir+'\/Data'):\n  if not(os.path.exists(root_dir+'\/Data\/train')):\n    os.mkdir(root_dir+'\/Data\/train')\nif not(os.path.exists(root_dir+'\/Data\/train\/annotations')):\n  os.mkdir(root_dir+'\/Data\/train\/annotations')\n\nif not(os.path.exists(root_dir+'\/Data\/train\/shapes_train2020')):\n  os.mkdir(root_dir+'\/Data\/train\/shapes_train2020')","04671476":"%ls \/kaggle\/working\n%ls \/kaggle\/working\/Data\n%ls \/kaggle\/working\/Data\/train\n%ls \/kaggle\/working\/Data\/train\/annotations\n%ls \/kaggle\/working\/Data\/train\/shapes_train2020","2e07138c":"labels = {1:'chromosome'}\ntrain_path = os.path.join(root_dir,'Data','train')\nsubset ='shapes_train'+'2020'\nannotations = 'annotations'\nN = 130\nimage_id = \"{:04d}\".format(N)\nprint(image_id)\ngrey_file_name = os.path.join(image_id+'.png')\npath_to_grey = os.path.join(train_path,subset, grey_file_name)\n\nmask1_file_name = image_id+'_'+ labels[1]+'_'+str(1)+'.png'\npath_to_mask1 = os.path.join(train_path, annotations, mask1_file_name)\n\nmask2_file_name = image_id+'_'+ labels[1]+'_'+str(2)+'.png'\npath_to_mask2 = os.path.join(train_path, annotations, mask2_file_name)\nprint(path_to_mask1)\nprint(path_to_mask2)\n\nmask1 = mask1.astype(np.uint8)\nmask2 = mask2.astype(np.uint8)\n\nprint(os.path.exists(os.path.join(train_path, subset)))\nprint(os.path.join(train_path, subset, grey_file_name))#\/Data\\ Science\/dataset\n\nio.imsave(os.path.join(train_path, subset, grey_file_name),grey)\nio.imsave(os.path.join(train_path, annotations, mask1_file_name), mask1)\nio.imsave(os.path.join(train_path, annotations, mask2_file_name), mask2)\n","ff593b26":"%ls \/kaggle\/working\n%ls \/kaggle\/working\/Data\n%ls \/kaggle\/working\/Data\/train\n%ls \/kaggle\/working\/Data\/train\/annotations\n%ls \/kaggle\/working\/Data\/train\/shapes_train2020\n\n\n#%ls My\\ Drive\/Science\/Data\\ Science\/dataset\/shapes\/train\/annotations","bfb15a02":"N = 130\nNUM_CATEGORIES = 2 # chrom:1, background :0\ngrey = data[N,:,:,0]\n# dictionnary for image 130\n## path to greyscaled image : \/kaggle\/working\/Data\/train\ndataset_root = os.path.join('\/kaggle\/working\/Data\/train')\nsubset ='shapes_train'+'2020'\n#annotations = 'annotations'\nimage_id = \"{:04d}\".format(N) #Possible bug here since\n#print(image_id)\ngrey_file_name = os.path.join(image_id+'.png')\npath_to_grey = os.path.join(dataset_root,subset, grey_file_name)\n\ndict_to_130 = {}\ndict_to_130['file_name']= path_to_grey\n\n## grey shape\ndict_to_130['height']= grey.shape[0]\ndict_to_130['width']= grey.shape[1]\n\n## the image id could be different from its index, here choose id=index=N\ndict_to_130['image_id'] = \"{:04d}\".format(N)#N\n\n### Prepare the dicts for annotation\n#### bounding boxes : theres two instances in image 130:First instance\ndict_to_130['annotations']= []\nannotation_instance_01_dict = {}\nannotation_instance_01_dict['bbox']=None\nBbox_0130_01 = mask_to_bbox_corners(mask1, mode='XYXY')\n\nprint(\"     \", type(Bbox_0130_01), type(Bbox_0130_01[0]))\n\nannotation_instance_01_dict['bbox'] = Bbox_0130_01\nannotation_instance_01_dict['bbox_mode']=0 #XYXY\nannotation_instance_01_dict['category_id'] = NUM_CATEGORIES-1\n\nannotation_instance_01_dict['segmentation']=None # A dict is used, How to handle several instances?\nmask1 = mask1 > 0\n### rle_instance_1 is a dict\n###\n### <byte> type issue !!!\n###\n\nrle_instance_1 = encode(np.asarray(mask1, order=\"F\"))\n\nprint(\"rle_instance1 \",rle_instance_1)\nprint(\"rle_instance1['counts'] is of type:\",type(rle_instance_1['counts']))\n\nprint(\"rle_instance1 \",rle_instance_1['counts'].decode(\"utf-8\"))\n\ncounts_byte_to_utf8 = rle_instance_1['counts'].decode(\"utf-8\")\nrle_instance_1['counts'] = counts_byte_to_utf8\n###\n###\n#cfg.INPUT.MASK_FORMAT='bitmask'\nannotation_instance_01_dict['segmentation'] = rle_instance_1\n\ndict_to_130['annotations'].append(annotation_instance_01_dict)\n\n#### bounding boxes : theres two instances in image 130: second instance\n\nannotation_instance_02_dict = {}\nannotation_instance_02_dict['bbox']=None\nBbox_0130_02 = mask_to_bbox_corners(mask2, mode='XYXY')\nprint(\"     \", type(Bbox_0130_02))\nannotation_instance_02_dict['bbox'] = Bbox_0130_02\nannotation_instance_02_dict['bbox_mode']=0 #XYXY\nannotation_instance_02_dict['category_id'] = NUM_CATEGORIES-1\n\nannotation_instance_02_dict['segmentation']=None # A dict is used, How to handle several instances?\nmask2 = mask2 > 0\n### rle_instance_1 is a dict\nrle_instance_2 = encode(np.asarray(mask2, order=\"F\"))\n#cfg.INPUT.MASK_FORMAT='bitmask'\n\n###\n### <byte> type issue !!!\n###\nrle_instance_2['counts'] = rle_instance_2['counts'].decode(\"utf-8\")\n###\nannotation_instance_02_dict['segmentation'] = rle_instance_2\ndict_to_130['annotations'].append(annotation_instance_02_dict)\n","555b62d5":"row_0, row_1, col_0, col_1 = mask_to_bbox_corners(mask1, mode='numpy')\n\nbox_mask = np.zeros(grey.shape, dtype=grey.dtype)\nbox_mask[row_0-1:row_1+1,col_0-1:col_1+1]=1\n\nprint(coco.maskUtils.decode(rle_instance_2))","6ac3d57f":"plt.figure(figsize=(10,10))\n\nplt.subplot(131, xticks=[], yticks=[])\nplt.imshow(grey, cmap=plt.cm.gray)\n\nplt.subplot(132, xticks=[], yticks=[])\nplt.title(\"rle -> mask\")\nplt.imshow(coco.maskUtils.decode(rle_instance_2), interpolation=\"nearest\", cmap=plt.cm.flag_r)\n\nplt.subplot(133, xticks=[], yticks=[])\nplt.title(\"mask + bounding box\")\nplt.imshow(1*mask1+5*box_mask, interpolation=\"nearest\",cmap=plt.cm.flag_r)","09cd3af9":"print(dict_to_130.keys())\nprint(\"    \", type(dict_to_130['height']), dict_to_130['height'])\nprint(\"    \", type(dict_to_130['width']), dict_to_130['width'])\nprint(\"    \", type(dict_to_130['image_id']), dict_to_130['image_id'])\nprint(dict_to_130['file_name'])\nprint(type(dict_to_130['annotations']))\n\nprint(dict_to_130['annotations'])\nprint(dict_to_130['annotations'][0].keys())\nprint(dict_to_130['annotations'][0]['segmentation'])\nprint(dict_to_130['annotations'][0]['segmentation'])\nprint(dict_to_130['annotations'][0]['segmentation'].keys())\nprint(\"    \",dict_to_130['annotations'][0]['segmentation']['size'],\"---\",type(dict_to_130['annotations'][0]['segmentation']['size'][0]))\nprint(\"    \",dict_to_130['annotations'][0]['segmentation']['counts'])\nprint(\"    \",type(dict_to_130['annotations'][0]['segmentation']['counts']))","360e0893":"my_json = '\/kaggle\/working\/Data\/train\/annotations\/instances_0130_data.json'\nprint(my_json)\n\n#print(os.path.exists(os.path.join('','kaggle','working','Data','train','annotations')))\nprint(os.path.exists('\/kaggle\/working\/Data\/train\/annotations'))\n%ls \/kaggle\/working\/Data\/train\/annotations\n%pwd","03f2cf2e":"with open(my_json, 'w') as f:\n    json.dump(dict_to_130, f)","76d28d24":"#%ls \/kaggle\/working\n#%ls \/kaggle\/working\/Data\n#%ls \/kaggle\/working\/Data\/train\n%ls \/kaggle\/working\/Data\/train\/annotations\n#%ls \/kaggle\/working\/Data\/train\/shapes_train2020","4eb3e7dc":"annotations_path= my_json\ndataType='0130_data'\nannFile=(my_json,dataType)\nannFile","e023283a":"my_coco_dataset=COCO(my_json) # WRONG","530c8c5d":"#from detectron2.data.datasets import register_coco_instances\nfrom detectron2.data import datasets\nfrom detectron2.data.datasets import register_coco_instances","7cfb0ce4":"path_to_annotation = my_json\npath_to_image = path_to_grey\n\n#register_coco_instances(\"chromosome\", {}, \".\/data\/trainval.json\", \".\/data\/images\"\nregister_coco_instances(\"chromosome_dataset_train\", {}, path_to_annotation, path_to_image)","ff4e8296":"from detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.data.catalog import DatasetCatalog","c34418a2":"#visualize training data\nmy_chromosome_train_metadata = MetadataCatalog.get(\"chromosome_dataset_train\")\ndataset_dicts = DatasetCatalog.get(\"chromosome_dataset_train\")\n","1ef52419":"### Need to make a tree to store the pairs of images in png format","2dae73bc":"#### Have a look in the dict:","1d189602":"### Build the dictionnary by hand according to detectron2 specifications:\n\nLet's try to construct a dictionnary which should fit the COCO structure","79f3cd23":"### Is annotation_0130_data.json file a good COCO file?\n\nLet' try to load the annotation-0130_data.json with pycocotools following this example: http:\/\/mscoco.org\/static\/tmp\/tools\/tools\/pycocotools_demo_1_0.html","ba4ba543":"## Try to make a minimalistic annotation with pycocotools:\nThe aim is to generate a minimalist annotation from one mask of a single groundtruth image:","957817ed":"## According to Detectron2 documentation:\n\n  * [Detectron2 documentation](https:\/\/github.com\/facebookresearch\/detectron2\/blob\/master\/docs\/tutorials\/datasets.md#register-a-dataset)\n\n  * [Colab tuto](https:\/\/colab.research.google.com\/drive\/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)\n> Register a Dataset\n> \n> To let detectron2 know how to obtain a dataset named \"my_dataset\", users need > to implement a function that returns the items in your dataset and then tell > detectron2 about this function:\n> \n> ```python\n> \n>     def my_dataset_function():\n>       ...\n>       return list[dict] in the following format\n> \n>     from detectron2.data import DatasetCatalog\n>     DatasetCatalog.register(\"my_dataset\", my_dataset_function)\n>     # later, to access the data:\n>     data: List[Dict] = DatasetCatalog.get(\"my_dataset\")\n> ```","0766d280":"### let's try to build a single dataset by hand\n#### Populate the directory tree with images:\nLet's make only two masks from the ground truth image, so that the overlapping pixels are implicit:\n\n      * 0130.png : greyscaled image\n      * 0130_chromosome_1.png and  0130_chromosome_2.png are two chromosome instances","225f88a1":"# Convertion of a dataset for instance segmentation in COCO format:\n\nThe goal is to convert a dataset modeling pairs of overlapping chromosomes (a compressed numpy array) into a COCO format using pycocotools.","1891dde7":">[**Register a dataset on detectron2**](https:\/\/detectron2.readthedocs.io\/tutorials\/datasets.html)\n\n>To let detectron2 know how to obtain a dataset named \u201cmy_dataset\u201d, users need to >implement a function that returns the items in your dataset and then tell >detectron2 about this function:\n\n\n>```\n>def my_dataset_function():\n>  ...\n>  return list[dict] in the following format\n>\n>from detectron2.data import DatasetCatalog\n>DatasetCatalog.register(\"my_dataset\", my_dataset_function)\n># later, to access the data:\n>data: List[Dict] = DatasetCatalog.get(\"my_dataset\")\n>```\n>\n>\n>**Standard Dataset Dicts**\n>\n>For standard tasks (instance detection, instance\/semantic\/panoptic segmentation, >keypoint detection), we load the original dataset into list[dict] with a >specification similar to COCO\u2019s json annotations. This is our standard >representation for a dataset.\n>\n>Each dict contains information about one image. The dict may have the following >fields, and the required fields vary based on what the dataloader or the task >needs (see more below).\n>\n>   * file_name: the full path to the image file. Rotation or flipping may be >applied if the image has EXIF metadata.\n>\n>   * height, width: integer. The shape of the image.\n>\n>   * image_id (str or int): a unique id that identifies this image. Required by >evaluation to identify the images, but a dataset may use it for different >purposes.\n>\n>   * annotations (list[dict]): each dict corresponds to annotations of one >instance in this image. Required by instance detection\/segmentation or keypoint >detection tasks, but can be an empty list. Note that images with empty >annotations will by default be removed from training, but can be included using >DATALOADER.FILTER_EMPTY_ANNOTATIONS.\n>\n>      * Each dict contains the following keys, of which bbox,bbox_mode and >category_id are required:\n>\n>         * bbox (list[float]): list of 4 numbers representing the bounding box of >the instance.\n>\n>         * bbox_mode (int): the format of bbox. It must be a member of >structures.BoxMode. Currently supports: BoxMode.XYXY_ABS, BoxMode.XYWH_ABS.\n>\n>         * category_id (int): an integer in the range [0, num_categories-1] >representing the category label. The value num_categories is reserved to >represent the \u201cbackground\u201d category, if applicable.\n>\n>         * segmentation (list[list[float]] or dict): the segmentation mask of the >instance.\n>\n>            * If list[list[float]], it represents a list of polygons, one for >each connected component of the object. Each list[float] is one simple polygon in >the format of [x1, y1, ..., xn, yn]. The Xs and Ys are absolute coordinates in >unit of pixels.\n>\n>            * If dict, it represents the per-pixel segmentation mask in COCO\u2019s >compressed RLE format. The dict should have keys \u201csize\u201d and \u201ccounts\u201d. You can >convert a uint8 segmentation mask of 0s and 1s into such dict by >pycocotools.mask.encode(np.asarray(mask, order=\"F\")). cfg.INPUT.MASK_FORMAT must >be set to bitmask if using the default data loader with such format.","eb2edec1":"#[Let's open the minimalist data set](https:\/\/www.dlology.com\/blog\/how-to-train-detectron2-with-custom-coco-datasets\/) with Detectron2\n\nSince the annotation was generated according to the Detectron2 tutorial, would it be possible:\n\n   * load a greyscaled a test image (0130.png)\n   * load and display the bounding boxes over the greyscaled image\n   * load and display the chromosome instances (aka contour from masks)","0f5f27eb":"#### Check one bounding box and one instance rle","b0a0a75a":"#rough bellow","f4a70977":"There's some issues when dumping byte into json:\n\nhttps:\/\/stackoverflow.com\/questions\/40000495\/how-to-encode-bytes-in-json-json-dumps-throwing-a-typeerror\n\nhttps:\/\/stackoverflow.com\/questions\/33794707\/json-dumping-bytes-fails-in-python-3\n\nIssue with int64 and json\n\nhttps:\/\/stackoverflow.com\/questions\/11942364\/typeerror-integer-is-not-json-serializable-when-serializing-json-in-python\n\nConverting to from mask to rle\n\nhttps:\/\/stackoverflow.com\/questions\/49494337\/encode-numpy-array-using-uncompressed-rle-for-coco-dataset\/49547872#49547872\n\n---\n\n","d6dd94b7":"### Save the dict as a json file ","ffd269dd":"## Import the dataset from github","7beb43b8":"### The annotation file: \nSave now the dictionnary as a json file: should be a legit coco dataset with one image and two instances saved as compressed rle","0e883cc6":"## Let's have a look inside the dataset"}}