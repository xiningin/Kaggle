{"cell_type":{"2a1a5d54":"code","33d9ecdb":"code","dd308e5f":"code","b40d256d":"code","848d3d0c":"code","3775d7b1":"code","9772cd11":"code","fefc8b5c":"code","58c0e6df":"code","271d43ab":"code","5d0fd45f":"code","00eb000c":"code","87381e88":"code","29157265":"code","ee9afcbb":"code","d2911cbd":"code","c6fb1369":"code","0378b2df":"markdown","a0c6c569":"markdown","f16d4951":"markdown","0828bf58":"markdown","b741358a":"markdown"},"source":{"2a1a5d54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33d9ecdb":"df = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\ndf.head()","dd308e5f":"df.info()","b40d256d":"df.head()","848d3d0c":"df.describe()","3775d7b1":"import matplotlib.pyplot as plt\ncount = 1\nplt.figure(figsize=(30,30))\nfor col in df.drop(columns = ['id','date','price']).columns:\n    plt.subplot(6,3,count)\n    plt.scatter(df[col],df['price'])\n    plt.xlabel(col)\n    plt.ylabel('price')\n    count=count+1","9772cd11":"import seaborn as sns\nlst = ['bedrooms','bathrooms','floors','waterfront','view','condition','grade']\nplt.figure(figsize=(30,30))\ncount = 1\nfor col in lst:\n    plt.subplot(3,3,count)\n    sns.boxplot(x=col,y='price',data = df)\n    count = count + 1\n","fefc8b5c":"print(df.columns)","58c0e6df":"df_params_selected = df.drop(columns = ['zipcode','date','yr_built','lat','long','yr_renovated','id'])\ndf_params_selected.columns","271d43ab":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\ndf_params_selected = pd.DataFrame(ss.fit_transform(df_params_selected),columns=['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n       'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'sqft_living15', 'sqft_lot15'])","5d0fd45f":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split as tts\nx_train,x_test,y_train,y_test = tts(df_params_selected.drop(columns=['price']),df_params_selected['price'],test_size = 0.3,random_state = 1)\nlr = LinearRegression()\nlr.fit(x_train,y_train)\nprint('Training R^2 socre: ',lr.score(x_train,y_train))\nprint('Testing R^2 score: ',lr.score(x_test,y_test))","00eb000c":"from sklearn.preprocessing import PolynomialFeatures\ntest_score=[]\ntrain_score = []\nlr1 = LinearRegression()\nfor i in range(1,5):\n    pr = PolynomialFeatures(degree = i)\n    x_train_pr = pr.fit_transform(x_train)\n    x_test_pr = pr.fit_transform(x_test)\n    lr1.fit(x_train_pr,y_train)\n    train_score.append(lr1.score(x_train_pr,y_train))\n    test_score.append(lr1.score(x_test_pr,y_test))\nprint(train_score)\nprint(test_score)","87381e88":"from sklearn.model_selection import cross_val_score\npr = PolynomialFeatures(degree = 2)\nscores = cross_val_score(LinearRegression(),pr.fit_transform(df_params_selected.drop(columns=['price'])),df_params_selected['price'],cv=4)\nprint(scores.mean())\nprint(scores)","29157265":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nx_train_pr = pr.fit_transform(x_train)  # See the above cell's code for pr's degree\nx_test_pr = pr.fit_transform(x_test)\nparams = {'alpha':[0.001,0.01,0.1,1,10,100]}\nGSCV = GridSearchCV(Ridge(),params,cv=5)\nGSCV.fit(x_train_pr,y_train)\nyhat_test = GSCV.predict(x_test_pr)\nprint(GSCV.best_estimator_.score(x_test_pr,y_test))\nprint(GSCV.best_estimator_)","ee9afcbb":"scores = cross_val_score(Ridge(alpha=1000),pr.fit_transform(df_params_selected.drop(columns=['price'])),df_params_selected['price'],cv=4)\nprint('The R^2 value for 2nd degree polynomial regularized by parameter aplha is: ',scores.mean())","d2911cbd":"import seaborn as sns\nrr = Ridge(alpha = 100)\nrr.fit(x_train_pr,y_train)\nyhat_train = rr.predict(x_train_pr)\nyhat_test = rr.predict(x_test_pr)\nax1 = sns.distplot(y_train,hist = False,color = 'red',label = \"Actual Train set\")\nax2 = sns.distplot(yhat_train,hist = False,color = 'blue',label = \"predicted Train set\",ax = ax1)\nprint(\"The R^2 for the training data is:\",rr.score(x_train_pr,y_train),sep=\" \")","c6fb1369":"ax3 = sns.distplot(y_test,hist = False,color = 'red',label = \"predicted Test set\")\nax4 = sns.distplot(yhat_test,hist = False,color = 'blue',label = \"predicted Test set\",ax = ax3)\nprint(\"The R^2 value for the Test set is:\",rr.score(x_test_pr,y_test),sep = \" \")","0378b2df":"Therefore we can see strong impacts of columns: 'bedrooms', 'bathrroms','wavefront','grade' and 'view'","a0c6c569":"The best fit for the data would be a 2nd degree polynomial (with scaled features), and regularized by alpha = 100","f16d4951":"Therefore polynomial equation with degree 2 would be a good fit for predicting the House Price.","0828bf58":"We get our best estimation at alpha = 100","b741358a":"First we look for the Categorical Variables that affect the Price of the house.\nAccording to the above describe() table, we can figure out that there are 7 Categorical Variables namely:\n<li>'bedrooms'<\/li><li>'bathrooms'<\/li><li>'floors'<\/li><li>'waterfront'<\/li><li>'view'<\/li><li>'condition'<\/li><li>'grade'"}}