{"cell_type":{"1d3479d5":"code","8699480f":"code","8874cc66":"code","6fd289f5":"code","5b34ad20":"code","72782992":"code","f7971f5b":"code","7c2237c1":"code","37a5293c":"code","346776e4":"code","261d6151":"code","3093e947":"code","59ee4396":"code","789a8821":"code","53305974":"code","633e9739":"code","65d34dc0":"code","c65f56ad":"code","59346c97":"code","bdfd5059":"code","ca620c49":"code","2bd729d0":"code","62494c51":"code","d1c5e1dc":"code","888c34f1":"code","14f0d6c7":"code","d3c14f42":"code","bf963734":"code","bf5b4f53":"code","cd418bd7":"code","b6acd473":"code","45db1682":"code","11eb6718":"code","ff76dde6":"code","d217ae0e":"code","727e9119":"code","4135cf42":"code","7ba6e3ed":"code","ab510c1c":"markdown","b09c0c67":"markdown","c9a24db1":"markdown","e39f7fbe":"markdown","5e4f5bb5":"markdown","c3236f41":"markdown","6a9d4ed1":"markdown","edc77e96":"markdown","defb7375":"markdown","ba6f274a":"markdown","0a3f6ab9":"markdown","2fecdceb":"markdown","8cbd9fd4":"markdown","766e0391":"markdown","d5125521":"markdown","25d6516f":"markdown","3b8a538c":"markdown"},"source":{"1d3479d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8699480f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize, sent_tokenize #(word tokenize, sentence tokenize)\nfrom bs4 import BeautifulSoup\nimport re, string, unicodedata\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Dropout\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf","8874cc66":"df = pd.read_csv('..\/input\/fake-news\/news.csv')","6fd289f5":"df.head()","5b34ad20":"#replacing FAKE = 0, REAL = 1\ndf['label'] = df['label'].replace('FAKE', 0)\ndf['label'] = df['label'].replace('REAL', 1)","72782992":"#Removing unwanted columns\ndf.drop('Unnamed: 0', axis =1 , inplace = True)","f7971f5b":"df","7c2237c1":"#distribution of fake and real dataset\nsns.set_style(\"darkgrid\")\nsns.countplot(df['label'])","37a5293c":"#checking na values\ndf.isna().sum()","346776e4":"stop_words = set(stopwords.words('english')) #set of all stopwords\npunctuation = list(string.punctuation) #all punctuation\n#adding everything into one set\nstop_words.update(punctuation)","261d6151":"#Data Cleaning - Part-1\n\ndef strip_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text()\n\ndef square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\ndef url_extract(text):\n    return re.sub(r'http\\S+', '', text)\n","3093e947":"#Data Cleaning - Part-2\ndef stopwords(text):\n    final_text = []\n    for i in text.split():\n        #checking in stopwords and also lowering the text\n        if i.strip().lower() not in stop_words:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n#finally getting all outputs in preprocessing the text using above functions\ndef preprocess(text):\n    text = strip_html(text)\n    text = square_brackets(text)\n    text = url_extract(text)\n    text = stopwords(text)\n    return text","59ee4396":"df['text'] = df['text'].apply(preprocess)","789a8821":"#creating wordclouds - Real News texts\nplt.figure(figsize = (20,20)) \n\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , \n               stopwords = STOPWORDS).generate(\" \".join(df[df.label == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.axis('off')","53305974":"#Fake news text: Wordcloud\nplt.figure(figsize = (20,20)) \n\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , \n               stopwords = STOPWORDS).generate(\" \".join(df[df.label == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.axis('off')","633e9739":"#crating vocab for the news\ndef get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\n\ncorpus = get_corpus(df.text)","65d34dc0":"corpus[:5]","c65f56ad":"#getting count for each word now using Counter\nfrom collections import Counter\ncounter = Counter(corpus)\nmost_common_words = counter.most_common(10) #prining most common 10 words\nmost_common_words = dict(most_common_words)\nmost_common_words","59346c97":"#train_test split\nX_train, X_test, y_train, y_test= train_test_split(df.text, df.label, random_state = 420)","bdfd5059":"X_test.shape[0], X_train.shape[0] #test train data rows","ca620c49":"maxfeatures = 10000\nmaxlength = 400","2bd729d0":"#tokenize\ntokenizer = text.Tokenizer(num_words=maxfeatures)\ntokenizer.fit_on_texts(X_train)\ntokenized_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlength)","62494c51":"X_train","d1c5e1dc":"tokenized_test = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlength)","888c34f1":"#calling GLOVE model\nEMBEDDING_FILE = '..\/input\/glove-twitter\/glove.twitter.27B.200d.txt'","14f0d6c7":"def get_coeff(word, *arr):\n    return word, np.asarray(arr, dtype = 'float32')\n\nembeddings_index = dict(get_coeff(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","d3c14f42":"all_embedd = np.stack(embeddings_index.values())\nembedd_mean, embedd_std = all_embedd.mean(), all_embedd.std()\nembedd_size = all_embedd.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(maxfeatures, len(word_index))\n\n#creating a matrix\nembedding_matrix = np.random.normal(embedd_mean, embedd_std, (nb_words, embedd_size))","bf963734":"for word, i in word_index.items():\n    if i>= maxfeatures:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","bf5b4f53":"batch_size = 256\nepochs = 10\nembedd_size = 200","cd418bd7":"learning_rate = ReduceLROnPlateau(monitor = 'val_accuracy', patience = 2,\n                                 verbose = 1, factor = 0.5, min_lr=0.0001)","b6acd473":"#creating a model\nmodel = Sequential()\n\nmodel.add(Embedding(maxfeatures, output_dim = embedd_size,\n                    weights = [embedding_matrix], input_length = maxlength, trainable = False))\n\nmodel.add(LSTM(units = 128, return_sequences = True, \n               recurrent_dropout = 0.25, dropout = 0.25))\n\nmodel.add(LSTM(units = 64, recurrent_dropout = 0.1, dropout = 0.1))\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(optimizer= keras.optimizers.Adam(lr = 0.01), \n             loss = 'binary_crossentropy', metrics = ['accuracy'])","45db1682":"model.summary()","11eb6718":"history = model.fit(X_train, y_train, batch_size=batch_size, \n                    validation_data=(X_test, y_test), \n                    epochs=epochs, callbacks= [learning_rate])","ff76dde6":"#accuracy on train and test data\nprint(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100 , \"%\")\n\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","d217ae0e":"epochs = [i for i in range(10)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","727e9119":"#making predictions now\npredictions = model.predict_classes(X_test)\npredictions[:10]","4135cf42":"#classification report\nprint(classification_report(y_test, predictions, target_names = ['FAKE','REAL']))","7ba6e3ed":"#confusion matrix\nplt.figure(figsize = (10,10))\ncm = confusion_matrix(y_test,predictions)\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['FAKE','REAL'] , yticklabels = ['FAKE','REAL'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","ab510c1c":"**Predictions**","b09c0c67":"**Corpus**","c9a24db1":"**Please Upvote.**","e39f7fbe":"**Train Test split**","5e4f5bb5":"**Accuracy check and Calculating Loss**","c3236f41":"**Word Cloud of Real and Fake news**","6a9d4ed1":"**Train the model**","edc77e96":"**Glove Model**","defb7375":"**Summary of the Model**","ba6f274a":"**Importing the Libraries**","0a3f6ab9":"**Classification Report**","2fecdceb":"**Dataset**","8cbd9fd4":"**Basic EDA**","766e0391":"**Model Building**","d5125521":"**Confusion Matrix**","25d6516f":"**The End**","3b8a538c":"**Data Cleaning**"}}