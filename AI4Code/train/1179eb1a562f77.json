{"cell_type":{"7459e83f":"code","a4f67e87":"code","57a1313a":"code","138c04ff":"code","a4640877":"code","b8be9821":"code","4591942d":"code","3dffe156":"code","4c182cf0":"code","a47a1d5c":"code","92de20d6":"code","9ca3b6a0":"code","0fdc0710":"code","818a57ce":"code","86d5bd3a":"code","7247ea62":"code","be1fe62e":"code","4383339b":"code","074a9313":"code","90782f3c":"code","920b101f":"code","b2fb31e9":"code","44aeefc1":"code","af0fcb5d":"code","e7e28f9e":"code","64ad8156":"code","745532fc":"code","d4bfeec3":"code","c96b86eb":"code","1b5dafe4":"code","03890516":"code","ce437cc1":"code","7d62f737":"code","e5a0cd9a":"code","ce2bdf66":"code","5cef4f8b":"code","2492e8b7":"code","4b523c65":"code","6e2bb4d7":"code","fcfeccf1":"code","62ad814c":"code","79a3dc93":"code","094efc17":"code","3dc1fec1":"code","485764ec":"code","83c16986":"code","0e1c1434":"code","cee87636":"code","1f4331b0":"code","cb96a711":"code","0df1dc50":"code","dc10befe":"code","4b84c9fc":"code","57bae5bc":"code","7e3726cc":"code","809d2047":"code","fe6796b4":"code","80bda4c6":"code","3782ac32":"code","b4cf3d15":"code","9f03f2e8":"code","61b86a07":"code","a7f74e67":"code","c4e3d0d8":"code","d2ed932f":"code","3dc26fb4":"code","6973d6e1":"code","6c31ec6f":"code","6126633e":"code","38066235":"code","42c713e9":"markdown","1429bfd7":"markdown","ca427e43":"markdown","61a1c58f":"markdown","3839914f":"markdown","1697f4a8":"markdown","821ca3fd":"markdown","f935f97c":"markdown","73a9e2f5":"markdown","059688fe":"markdown","8c52a399":"markdown","192ac268":"markdown","08a5243d":"markdown","bd27cba2":"markdown","3fd9fcef":"markdown","89f48783":"markdown","218d9eea":"markdown","fa9acbf8":"markdown","0885ed12":"markdown","1a961ce9":"markdown","68ff14e9":"markdown","8c022abf":"markdown","6b23815d":"markdown"},"source":{"7459e83f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4f67e87":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","57a1313a":"### Stop wordss\nimport nltk\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\"))\nprint(stop_words)\nprint(len(stop_words))","138c04ff":"# filteration for stopwords \nexample_text = \"Hi Mr.Pavan , How are you doing today?. Cool you got a nice job at IBM. Wow thats an awesome car. Weather is great.\"\nwords = nltk.word_tokenize(example_text)\nwords","a4640877":"import nltk","b8be9821":"sample_txt = ['this is me abhishek_jaiswal$%','this is 69 group','hey Man You AEER IS 487Cheating']","4591942d":"# COLLECTING ALL WORDS \n# words_list = sample_txt.copy()\n# words_list = ' '.join(words_list)\n# words_list = words_list.split()\n# words_list\n    \n    \n    \n## method 2 using tokenisation\nwords_list = []\nfor text in sample_txt:\n    words_list.extend(nltk.word_tokenize(text))\nwords_list    \n    \n    ","3dffe156":"# how do we apply function\npd.Series(sample_txt).apply(lambda X: print(\"this\",X))","4c182cf0":"# lower casing\npd.Series(sample_txt).apply(lambda x:\" \".join(x.lower() for x in x.split()))\n","a47a1d5c":"import string\nstring.punctuation","92de20d6":"sample_text = ['this is the FIRST SAMple789 &(*^)#$%&(**&^)*& text', 'ww.wll we know this person ']\nsample_text = pd.Series(sample_text)","9ca3b6a0":"# removing punctuation \n# two methods\n# by using regex\nimport re\nsample_text.apply(lambda x:(re.sub(r'[^\\w\\s]','',x)).lower())\n# sample_text.apply(lambda x:(re.sub(r'[^\\w\\s]+','',x)).lower())\n\n\n","0fdc0710":"string.punctuation","818a57ce":"def remove_punctuation(sentence):\n    import string\n    return(''.join([ch.lower() for ch in sentence if ch not in string.punctuation]))\nsample_text.apply(remove_punctuation)","86d5bd3a":"sample_txt = ['this is me abhishek_jaiswal$%','this is 69 group','hey Man You AEER IS 487Cheating']","7247ea62":"from nltk.corpus import stopwords\nlen(stopwords.words('english'))","be1fe62e":"from wordcloud import STOPWORDS\nlen(STOPWORDS)","4383339b":"sample_txt = ['this is me abhishek_jaiswal$%','this is 69 group','hey Man You AEER IS 487Cheating']\n### Removing Stop words\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\npd.Series(sample_txt).apply(lambda x: ' '.join(x.lower() for x in x.split() if x.lower() not in stop))\n","074a9313":"# method 2 for removing stop words\ndef remove_stopwords(txt):\n    stop = stopwords.words('english')\n    # here txt is a string \n    no_stopwords = \" \".join([word for word in txt.split() if word.lower() not in stop])\n    return(no_stopwords)\n    ","90782f3c":"pd.Series(sample_txt).apply(remove_stopwords)","920b101f":"# tokenisation\n# using textblob\n# from textblob import TextBlob,Word\n# TextBlob(\"God is Great! I won 45 a lottery.\").tokenize()\n\nimport nltk\nnltk.word_tokenize('God is Great! I 45 won a lottery$.')","b2fb31e9":"### Sentence tokenization\nnltk.sent_tokenize(\"God is Great! thats why I won a lottery. dog is barking\")\n","44aeefc1":"text = pd.Series(['i am sitting in the forests running', 'mohan is driving cars', 'catching fishes'])\nbag_word = ' '.join(text)\nbag_word.split(\" \")","af0fcb5d":"text = pd.Series(['i am sitting in the forests running', 'mohan is driving cars', 'catching fishes'])\nbag_words = []\n\nfor sen in text:\n    bag_words.extend(nltk.word_tokenize(sen))\n    \nbag_words    ","e7e28f9e":"sample_text","64ad8156":"text = pd.Series(['i am sitting in the forests running already', 'ready','mohan is driving cars', 'catching fishes'])","745532fc":"from nltk.stem  import PorterStemmer\nst = PorterStemmer()\ndef stemmer_porter(sentence):\n    return(' '.join([st.stem(word) for word in sentence.split()]))\n\ntext.apply(stemmer_porter)\n    ","d4bfeec3":"from nltk.stem  import SnowballStemmer\nst = SnowballStemmer(language = 'english')\ndef stemmer_snowball(sentence):\n    return (' '.join([st.stem(word) for word in sentence.split(' ')]))\ntext.apply(stemmer_snowball)\n    ","c96b86eb":"text = pd.Series(['I like fishing','I eat fish','There are many fishes','he have many cars', \n                  ' leaves and leaf'])\nfrom textblob import Word\ntext.apply(lambda x: ' '.join([Word(word).lemmatize()  for word in x.split()]))","1b5dafe4":"text = pd.Series(['I like fishing','I eat fish','There are many fishes','he have many cars', \n                  ' leaves and leaf'])\nwords_list = []\nfor sentence in text:\n    words_list.extend(nltk.word_tokenize(sentence))\nfreq_dist = nltk.FreqDist(words_list)\nfreq_dist.most_common(10)\n# freq_dist.keys()","03890516":"temp = pd.DataFrame(freq_dist.most_common(30),  columns=['word', 'count'])\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=temp, ax=ax)\nplt.title(\"Top words\")\nplt.xticks(rotation='vertical');","ce437cc1":"from wordcloud import WordCloud\nimport wordcloud\n# creation of wordcloud\nwcloud_fig = WordCloud( stopwords=set(wordcloud.STOPWORDS),\n                      colormap='viridis', width=300, height=200).generate_from_frequencies(freq_dist)\n\n# plotting the wordcloud\nplt.figure(figsize=(10,7), frameon=True)\n\nplt.imshow(wcloud_fig, interpolation  = 'bilinear')\nplt.show()\n","7d62f737":"from wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nprint(len(STOPWORDS))\nprint(len(stopwords.words('english')))\n","e5a0cd9a":"def process_text(text):\n    import re\n    import nltk\n    from textblob import TextBlob\n    from nltk.corpus import stopwords\n    from nltk.stem import SnowballStemmer\n    from textblob import Word\n    from nltk.util import ngrams\n    from wordcloud import WordCloud, STOPWORDS\n    from nltk.tokenize import word_tokenize\n    stemmer = SnowballStemmer(language = 'english')\n    row = text\n    # lowercasing\n    row = row.lower()\n    # Remove non alphanumeric symbols white spaces\n    row = re.sub(r'[^\\w\\s%]+','',row)\n    row = re.sub(r'[\\d]+',' digit ',row)\n    row = re.sub(r'[%]+',' p ',row)\n    row =  [stemmer.stem(word) for word in row.split() if word not in STOPWORDS]   \n    return row\n    ","ce2bdf66":"sample_text = pd.Series(['This is abhishek_jaiswal9957   87897per','787985%^ and we know usf',\n                        'this &^&*^* '])\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ncnt = CountVectorizer( analyzer=process_text)\ncnt.fit(sample_text)\npd.DataFrame(data = cnt.transform(sample_text).toarray(), columns = cnt.get_feature_names())","5cef4f8b":"text = ['i love machine learning 857% ','but i not love coding and eating']\n\n## Count Vectorization\nfrom sklearn.feature_extraction.text import CountVectorizer\ncnt = CountVectorizer(preprocessor = process_text,ngram_range=(1,2))\ncnt.fit(text)\n","2492e8b7":"cnt.get_feature_names()","4b523c65":"cnt.transform(text).toarray()","6e2bb4d7":"pd.DataFrame(data = cnt.transform(text).toarray(), columns = cnt.get_feature_names())","fcfeccf1":"new_text = ['i love coding and my name is abhishek']\npd.DataFrame(data = cnt.transform(new_text).toarray(), columns = cnt.get_feature_names())","62ad814c":"from sklearn.feature_extraction.text import CountVectorizer\ncnt_ngram = CountVectorizer(ngram_range=(1,2), preprocessor = process_text)\n# tokenizing\ncnt_ngram.fit(text)\n\n# encoding features \nfeatures = cnt_ngram.transform(text).toarray()\n# features names\nfeatures_name = cnt_ngram.get_feature_names()\n","79a3dc93":"pd.DataFrame(data = features, columns = features_name)","094efc17":"new_text = ['i love coding and my name is abhishek']\npd.DataFrame(data = cnt_ngram.transform(new_text).toarray(), columns = cnt_ngram.get_feature_names())","3dc1fec1":"text = ['The quick brown fox jumped over the lazy dog.', 'The Dog','The Fox']","485764ec":"from sklearn.feature_extraction.text import TfidfVectorizer\n# create the vectorizer\ncnt_idf = TfidfVectorizer(stop_words='english', preprocessor = process_text, ngram_range=(1,2))\n\n\n# tokenize and build vocab\ncnt_idf.fit(text)\n\nprint(cnt_idf.vocabulary_)\nprint(cnt_idf.idf_)","83c16986":"pd.DataFrame(cnt_idf.transform(text).toarray(), columns = cnt_idf.get_feature_names())","0e1c1434":"import pandas as pd\nimport numpy as np \nsms = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding = 'latin-1',\n              usecols = [ 'v1','v2']  )\nsms.columns = ['label','text']","cee87636":"sms","1f4331b0":"sms.describe()","cb96a711":"sms.groupby('label').describe()","0df1dc50":"# convert label to a numerical variable\nsms['label_num'] = sms.label.map({'ham':0, 'spam':1})\nsms.head()","dc10befe":"sms['text_len'] = sms.text.apply(len)","4b84c9fc":"sms.head()","57bae5bc":"plt.figure(figsize=(12, 8))\n\nsms[sms.label=='ham'].text_len.plot(bins=35, kind='hist', color='blue', \n                                       label='Ham messages', alpha=0.6)\nsms[sms.label=='spam'].text_len.plot(kind='hist', color='red', \n                                       label='Spam messages', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Message Length\")","7e3726cc":"sms[sms.label=='ham'].describe()","809d2047":"sms[sms.label=='spam'].describe()","fe6796b4":"sms.head()","80bda4c6":"def process_token(text):\n    import re\n    import nltk\n    from textblob import TextBlob\n    from nltk.stem import SnowballStemmer\n    from textblob import Word\n    from wordcloud import WordCloud, STOPWORDS\n    from nltk.tokenize import word_tokenize\n    stemmer = SnowballStemmer(language = 'english')\n    row = text\n    # lowercasing\n    row = row.lower()\n    # Remove non alphanumeric symbols white spaces\n    row = re.sub(r'[^\\w\\s%]+','',row)\n    row = re.sub(r'[\\d]+',' 9 ',row)\n    row = re.sub(r'[%]+',' p ',row)\n    row =  [stemmer.stem(word) for word in row.split() if word not in STOPWORDS]\n    return row\n\ndef process_text(text):\n    import re\n    import nltk\n    from textblob import TextBlob\n    from nltk.stem import SnowballStemmer\n    from textblob import Word\n    from wordcloud import WordCloud, STOPWORDS\n    from nltk.tokenize import word_tokenize\n    stemmer = SnowballStemmer(language = 'english')\n    row = text\n    # lowercasing\n    row = row.lower()\n    # Remove non alphanumeric symbols white spaces\n    row = re.sub(r'[^\\w\\s%]+','',row)\n    row = re.sub(r'[\\d]+',' 9 ',row)\n    row = re.sub(r'[%]+',' p ',row)\n    row =  ' '.join([stemmer.stem(word) for word in row.split() if word not in STOPWORDS])\n    return row","3782ac32":"import wordcloud\n\n# Converting all the senctences into one sentence to draw our wordcloud\n\n# before drawing our wordcloud we will process our text by using our process text function\n\n\nsms['processed_text'] = sms['text'].apply(process_text)\nspam_list = sms[sms['label_num'] == 1]['processed_text'].astype('str').tolist()\nham_list = sms[sms['label_num']== 0]['processed_text'].astype('str').tolist()\nham_text = ' '.join(ham_list)\nspam_text = ' '.join(spam_list)\n\n","b4cf3d15":"from wordcloud import WordCloud\nimport wordcloud\n# creation of wordcloud\nwcloud_fig = WordCloud( stopwords=set(wordcloud.STOPWORDS),\n                      colormap='viridis', width=600, height=400).generate(ham_text)\n\n# plotting the wordcloud\nplt.figure(figsize=(10,7), frameon=True)\n\nplt.imshow(wcloud_fig, interpolation  = 'bilinear')\nplt.title('Normal messages')\nplt.show()","9f03f2e8":"from wordcloud import WordCloud\nimport wordcloud\n# creation of wordcloud\nwcloud_fig = WordCloud(width=600, height=400, stopwords=set(wordcloud.STOPWORDS),\n                      colormap='viridis').generate(spam_text)\n\n# plotting the wordcloud\n\nplt.figure(figsize=(10,7), frameon=True)\n\nplt.imshow(wcloud_fig, interpolation  = 'bilinear')\nplt.title('Spam messages')\nplt.show()","61b86a07":"ham_words = ham_text.split()\nspam_words = spam_text.split()","a7f74e67":"from collections import Counter \nc_ham  = Counter(ham_words)\nc_spam = Counter(spam_words)\ndf_hamwords_top30  = pd.DataFrame(c_ham.most_common(30),  columns=['word', 'count'])\ndf_spamwords_top30 = pd.DataFrame(c_spam.most_common(30), columns=['word', 'count'])","c4e3d0d8":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=df_hamwords_top30, ax=ax)\nplt.title(\"Top 30 Ham words\")\nplt.xticks(rotation='vertical');","d2ed932f":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=df_spamwords_top30, ax=ax)\nplt.title(\"Top 30 Spam words\")\nplt.xticks(rotation='vertical');","3dc26fb4":"# splitting the data to check our final model \nfrom sklearn import model_selection\nX_train,X_test,y_train,y_test = model_selection.train_test_split(sms['text'],sms.label_num,\nstratify = sms.label_num, test_size = 0.2, random_state = 42)","6973d6e1":"from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\n\nfrom sklearn.pipeline import Pipeline\nvector = CountVectorizer(analyzer=process_token)\nclassifier  = Pipeline([('feature_generation', vector), ('model',MultinomialNB(alpha = 1.9))])","6c31ec6f":"classifier.fit(X_train,y_train)","6126633e":"text = ['Hey Abhishek can we get together to watch football match', \n       ' upto 20% discount on parking buy now or call us']\nclassifier.predict(text)","38066235":"from sklearn import metrics\nprint(metrics.classification_report(y_test, classifier.predict(X_test)))","42c713e9":"#### Lemmatisation\n* lemmatisation handles matching car to cars along with matching car to automobile\n* leaves to leaf (lemmatised)\n* leafs to leaf (lemmatised)\n* leaves to leav (stemmed)\n* plays to play (stemmed)\n* the output words will be always human understandable\n* since it matches various relationship thats why its slower than stemmer\n* #### lemmatisation gives us better results","1429bfd7":"#### Creating a bag of all words from a document","ca427e43":"### Generating N-grams\nin count vectorizer method we have a draw back that whenever we create features by using count vectorizer method we take single words at a time \nnow if \"not good \" means bad but in count vectorizer it will get treated as good since good will have a feature \n\nto rule out this problem we have n_grams here we look for 2\/3\/4 words at a time N words at a time\n","61a1c58f":"## EDA","3839914f":"### Removing Punctuation","1697f4a8":"## Text Processing function","821ca3fd":"### Creating Text Preprocessing Pipeline","f935f97c":"Very interesting! Through just basic EDA we've been able to discover a trend that spam messages tend to have more characters.","73a9e2f5":"### Plotting of words","059688fe":"# Text Preprocessing","8c52a399":"### Tokenisation\nTokenizer, Bag of words\ncreating a bag of words\n\n\n* by using textblob \n* by using nltk","192ac268":"##### Snowball stemmer is more robust and good","08a5243d":"### Count Vectorizer","bd27cba2":"#### Splitting the data","3fd9fcef":"### TF-IDF count vectorizer\n* #### The whole idea of tf_idf is to reflect on how important a word is to a document in a collection and hence normalizing words appeared frequently in all the documents \n* #### Let's say particular word is appearing in all the documents of the corpus, then it will achieve higher importance in our previous method (count vectorizer) thats bad\n\n* #### Term Frequency(TF) is simply the ratio of the count of a word present in a sentence, to the length of the sentence\n","89f48783":"### Making wordscloud","218d9eea":"## Removing Stop words","fa9acbf8":"### Introduction\nNatural Language Processing (NLP) is the task of making computers understand and produce human languages.\n\nAnd it always starts with the corpus i.e. a body of text.**","0885ed12":"## Stemming\n\nPlaying-> Play\n\ngoes-> go \n\ndoing -> do\n\nfish ,fishes,fishing -> fish\n\nstemmed word may not human readable because it just trim the words from ending \n\nit increases recall\n\nwe have Porter stemmer and snowball stemmer for good stemming","1a961ce9":"### WordCloud\n\nfor wordcloud we can pass a  big sentence or we can pass nltk freq_dictionary","68ff14e9":"## Converting Text to Features \n* #### One hot encoding\n* #### Count Vectorizer\n* #### N-grams\n* #### Co-occurence\n* #### Hash Vectoriser\n* #### TF-IDF(term frequency inverse document frequency)\n* #### Word embeddng\n* #### Implementing fastTExt\n","8c022abf":"#### Basic NLP using NLTK","6b23815d":"#### Computing Frequency of all words\nuse for making words cloud and bargraphs"}}