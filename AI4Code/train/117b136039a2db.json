{"cell_type":{"760bc5b1":"code","be06057e":"code","da644f7d":"code","651f41b6":"code","ea3c7ffb":"code","520285e3":"code","d539ec7e":"code","ed1df274":"code","4a8233bf":"code","3c48d092":"code","fcac828b":"code","d0fe044f":"code","d5e92a19":"code","9ec43557":"code","2b4cf4a9":"code","171b944d":"code","960711ea":"code","df5e6e1e":"code","5d1b7262":"code","6b9fa6f0":"code","54a1a1dc":"code","233c1a7a":"code","dc7bd851":"code","6741d05e":"code","277ebcc3":"code","0bd7ab8a":"code","73f945ca":"code","0400db30":"code","8cb00a0f":"code","3f39f93a":"code","4f433c06":"code","23022bf0":"code","287b2ae2":"code","6aa0d8d1":"code","386c8a16":"code","ac155ba4":"code","c40d2cb7":"code","bfcdee26":"code","5f14ff52":"code","9efee8c6":"code","42b18b26":"code","2233441d":"code","c1a5e185":"code","4bb5ebf3":"code","862b9b8a":"code","5487d989":"code","b02422f7":"code","19816ce6":"code","6dbdb594":"code","bd270170":"code","ceb5786f":"code","d0889af9":"code","0cd44e5c":"code","d90e3531":"code","8bf761bf":"code","4375d519":"code","c25e10fa":"code","e795e2a5":"code","5b677b1b":"code","8ea08d68":"code","e51a8e2d":"code","43afc8f6":"code","976d32e9":"code","100e6c57":"code","33efc8c0":"code","c145cd4d":"code","2a974ba0":"code","f56071d8":"code","990b5a73":"code","ea6d3e67":"code","a7be5a10":"code","ba403910":"code","ea864c81":"code","888c6815":"code","686ba44a":"code","1832f866":"code","5c46e319":"code","e3edb2f2":"code","02a0df5f":"code","4ef8054c":"code","2c864e21":"code","9b6446be":"code","457de9bc":"code","8689b88b":"code","8c323ff3":"code","a3fac353":"code","2051c5f1":"code","4ffea67b":"code","af4f8915":"code","70430629":"code","28e95e74":"code","368758c5":"code","c75a0e20":"code","639cf6dc":"code","0e3cbcb2":"code","08e675a5":"code","b72f321d":"code","b4db8288":"code","fcd23ee6":"code","f718f67f":"code","249d628f":"code","3efe77e9":"code","25df9225":"code","14f30946":"code","fdd778fe":"markdown","fbc888f5":"markdown","7f30e22b":"markdown","cf7f636e":"markdown","c157edca":"markdown","83edc68b":"markdown","97e2b6e4":"markdown","d19f0b66":"markdown","d6d2b381":"markdown","76d3ce2d":"markdown","346b75d9":"markdown"},"source":{"760bc5b1":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\n\nplt.style.use('fivethirtyeight')\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Lines below are just to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","be06057e":"# loading training dataset\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')","da644f7d":"# take a look at the head of the training data set\ntrain.head()","651f41b6":"# shape of training dataset\ntrain.shape","ea3c7ffb":"# loading test dataset\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","520285e3":"# take a look at the head \ntest.head()","d539ec7e":"# the shape of the test set\ntest.shape","ed1df274":"# merging both dataset to clean both at once, also to get most accurate filling results\ndf = train.merge(test , how='outer')\ndf.head()","4a8233bf":"# checking for nulls in all columns\ndf.info()","3c48d092":"# we can see that the male have high correlation with 0, we can see the gender is somewhat strong predictor\nsns.countplot(train['Survived'] , hue = train['Sex'] , orient='v',palette='ocean')","fcac828b":"# Pclass 3 consider strong predictor on 0\nsns.countplot(train['Survived'] , hue = train['Pclass'] , orient='v',palette='ocean')","d0fe044f":"# Embarked S also have high corrlation with 0\nsns.countplot(train['Survived'] , hue = train.Embarked , orient='v',palette='ocean_r')","d5e92a19":"# missing values plotting\nfig, ax = plt.subplots(figsize = (12, 6))\n\n\nsns.heatmap(df.isnull(), yticklabels=False, ax = ax, cbar=False, cmap='cividis')\nplt.title(\"Null in the Data\", fontsize =15)\nplt.xticks(rotation=45)\nplt.show()","9ec43557":"# checking for nulls\ndf.isnull().sum()","2b4cf4a9":"# we can see 'S' is the most frequent\ndf.Embarked.value_counts()","171b944d":"# filling with the most frequent Embarked\ndf.Embarked.fillna('S' , inplace = True)","960711ea":"df.Pclass.value_counts()","df5e6e1e":"df[df.Pclass == 3]['Fare'].mean()","5d1b7262":"# filling with the mean of Fare of the most frequent Pclass to get more accurate fill\ndf.Fare.fillna(df[df.Pclass == 3]['Fare'].mean() , inplace =True)","6b9fa6f0":"# checking the mean of age in different Pclass\ndf[['Pclass' , 'Age']].groupby('Pclass').mean()","54a1a1dc":"#defining a function 'impute_age'\ndef impute_age(age_pclass): # passing age_pclass as ['Age', 'Pclass']\n    \n    # Passing age_pclass[0] which is 'Age' to variable 'Age'\n    Age = age_pclass[0]\n    \n    # Passing age_pclass[2] which is 'Pclass' to variable 'Pclass'\n    Pclass = age_pclass[1]\n    \n    #applying condition based on the Age and filling the missing data respectively \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 38\n\n        elif Pclass == 2:\n            return 30\n\n        else:\n            return 25\n\n    else:\n        return Age","233c1a7a":"# filling the appropirate age that coresponde to the Pclass\ndf.Age = df.apply(lambda x :impute_age(x[['Age', 'Pclass']] ) , axis = 1)","dc7bd851":"df.Cabin.head()","6741d05e":"# using the initials of cabin to get class that have correlation with the cabin location\ndf.Cabin = df.Cabin.astype(str).str[0]","277ebcc3":"df.Cabin.value_counts()","0bd7ab8a":"# checking missing after the fillings\nfig, ax = plt.subplots(figsize = (12, 6))\n\nsns.heatmap(df.isnull(), yticklabels=False, ax = ax, cbar=False, cmap='cividis')\nplt.title(\"Null in the Data\", fontsize =15)\nplt.xticks(rotation=45)\nplt.show()","73f945ca":"df.isnull().sum()","0400db30":"# creating new feature called familysize\ndf['FamilySize'] = df ['SibSp'] + df['Parch'] + 1","8cb00a0f":"# creating new feature called is alone \ndf['IsAlone'] = df['FamilySize'].apply(lambda x:1 if x==1 else 0)","3f39f93a":"# take a look at the names\ndf[['Name']].head()","4f433c06":"# creat new column with the title of people\ndf['Title'] = df['Name'].str.split(',' , expand=True)[1].str.split('.', expand=True)[0]\ndf['Title'].value_counts()","23022bf0":"# drop columns that will not give information\ndf.drop(columns=['PassengerId' , 'Name' , 'Ticket' , 'SibSp' , 'Parch' ] , inplace=True)","287b2ae2":"# to get the train data from the meged data set we can use iloc and get all columns, while rows equal to the shape of the train[0]\ndf.iloc[:train.shape[0],:].head()","6aa0d8d1":"# creating dummy variables for all categorical variables in the cleaned and merged dataset\ndf_d = pd.get_dummies(df , drop_first=True)\ndf_d.shape","386c8a16":"#Survived correlation matrix\ncorrmat = abs(df_d.iloc[:train.shape[0],:].corr())\nplt.figure(figsize=(12, 8))\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'Survived')['Survived'].index\ncm = np.corrcoef(df_d.iloc[:train.shape[0],:][cols].values.T)\nsns.set(font_scale=1.00)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True,\n                 fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,\n                 cmap = 'Blues', linecolor = 'white', linewidth = 1)\nplt.title(\"Correlations between Survived and features including dummy variables\", fontsize =15)\nplt.xticks(rotation=45)\nplt.show()","ac155ba4":"# getting the target (Survived) column as y\ny=pd.DataFrame(df_d.pop('Survived'))","c40d2cb7":"# checking the shape of the train data set, to know from where to cut data set to get training data useing iloc\ntrain.shape[0]","bfcdee26":"# using iloc on both the target and training data we can get an exact seperation between training and testing datasets\nX_train = df_d.iloc[:train.shape[0] , :]\ny_train = y.iloc[:train.shape[0]]","5f14ff52":"# checking train dataset shape to be sure of the correct seperation\nprint(X_train.shape , y_train.shape)","9efee8c6":" # importing test\/train split, and use it on training dataset to train the models and score them \nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3 , random_state = 101)\n","42b18b26":"# importing scaler, then scale the training and test dataframes\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.preprocessing import RobustScaler\n\ns = StandardScaler()\nr = RobustScaler()\n\nX_train_d_s = pd.DataFrame(s.fit_transform(X_train) , columns=X_train.columns)\nX_test_d_s = pd.DataFrame(s.transform(X_test) , columns=X_test.columns)","2233441d":"#importing models \nfrom sklearn.svm import SVC \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB","c1a5e185":"# run different models with default parameters\nrandom_state = 35\nmodel_names = ['LogisticRegression', 'DecisionTreeClassifier',\n             'RandomForestClassifier','ExtraTreesClassifier'\n             , 'GradientBoostingClassifier','AdaBoostClassifier']\nmodels = [ ('LogisticRegression',LogisticRegression(random_state=random_state)),\n         ('DecisionTreeClassifier', DecisionTreeClassifier(random_state=random_state)),\n         ('RandomForestClassifier',RandomForestClassifier(random_state=random_state)),\n         ('ExtraTreesClassifier',ExtraTreesClassifier(random_state=random_state)),\n         ('GradientBoostingClassifier',GradientBoostingClassifier(random_state=random_state)),\n         ('AdaBoostClassifier',AdaBoostClassifier(random_state=random_state))\n        ]\nmodel_accuracy = []\nprint ('fitting...')\nfor k,model in models:\n\n    model.fit(X_train, y_train)\n    accuracy = cross_val_score(model, X_train_d_s, y_train, cv=5).mean()\n    model_accuracy.append(accuracy)\nprint('Completed')\n","4bb5ebf3":"# creating dataframe that contain model name and accuracy\nModels = pd.concat([pd.Series(model_names), pd.Series(model_accuracy)], axis=1).sort_values(by=1, ascending=False)","862b9b8a":"\nModels.rename(columns={0:'model_name',\n                      1:'accuracy'}, inplace=True)\nModels","5487d989":"# ploting accuracy with model names\na = sns.barplot(Models.accuracy, Models.model_name,palette='ocean_r')\na.set_xlim(0.7,1)\nplt.title(\"accuracy for each model\", fontsize =15)\nplt.show()","b02422f7":"# creating empty list to append each name and accuracy of all models used then compare\nO_model_accuracy = []\nO_model_name = []","19816ce6":"# using grid search on GradientBoostingRegressor model to get the best hyperparameters\n\nparam_grid = {'learning_rate': [0.01 ],\n 'max_depth': [3 ],\n 'max_features': ['auto'],\n 'min_samples_leaf': [15 ],\n 'min_samples_split': [15],\n 'n_estimators': [1500]}\n\ngrad = GridSearchCV(GradientBoostingClassifier(),\n                           param_grid, cv=5, verbose= 1 , n_jobs=-1)\ngrad.fit(X_train_d_s , y_train)","6dbdb594":"grad.best_params_","bd270170":"# getting train score\ngrad.score(X_train_d_s , y_train)","ceb5786f":"# getting test scores\ngrad.score(X_test_d_s , y_test)","d0889af9":"O_model_accuracy.append(grad.score(X_test_d_s , y_test))\nO_model_name.append('GradientBoostingClassifier')","0cd44e5c":"# confusion matrix, classification report\nprint(f'confusion matrix for RF\\n{confusion_matrix(y_test,grad.predict(X_test_d_s))}\\n classification report for RF \\n {classification_report(y_test,grad.predict(X_test_d_s))}')","d90e3531":"# using RandomGridSerach to find best hyperparametrs for RandomForestRegressor\n\npar = {'bootstrap': [True],\n 'max_depth': [20],\n 'max_features': ['auto'],\n 'min_samples_leaf': [2 ],\n 'min_samples_split': [5 ],\n 'n_estimators': [ 800 ]}\n\nra = GridSearchCV(RandomForestClassifier(),\n                   par , cv = 5 , verbose= 1  , n_jobs= -1)\nra.fit(X_train_d_s , y_train)","8bf761bf":"ra.best_params_","4375d519":"# getting train score\nra.score(X_train_d_s , y_train)","c25e10fa":"# getting test scores\nra.score(X_test_d_s , y_test)","e795e2a5":"O_model_accuracy.append(ra.score(X_test_d_s , y_test))\nO_model_name.append('RandomForestClassifier')","5b677b1b":"# confusion matrix, classification report\nprint(f'confusion matrix for RF\\n{confusion_matrix(y_test,ra.predict(X_test_d_s))}\\n classification report for RF \\n {classification_report(y_test,ra.predict(X_test_d_s))}')","8ea08d68":"# using RandomGridSerach  to fide best hyperparametrs for RandomForestRegressor\n\npar = {'bootstrap': [True],\n 'max_depth': [15],\n 'max_features': ['auto'],\n 'min_samples_leaf': [2],\n 'min_samples_split': [5],\n 'n_estimators': [100]}\n\nex = GridSearchCV(ExtraTreesClassifier(),\n                   par , cv = 5 , verbose= 1  , n_jobs= -1)\nex.fit(X_train_d_s , y_train)","e51a8e2d":"ex.best_params_","43afc8f6":"ex.best_score_","976d32e9":"ex.score(X_train_d_s , y_train)","100e6c57":"ex.score(X_test_d_s , y_test)","33efc8c0":"O_model_accuracy.append(ex.score(X_test_d_s , y_test))\nO_model_name.append('ExtraTreesClassifier')","c145cd4d":"# confusion matrix, classification report\nprint(f'confusion matrix for RF\\n{confusion_matrix(y_test,ex.predict(X_test_d_s))}\\n classification report for RF \\n {classification_report(y_test,ex.predict(X_test_d_s))}')","2a974ba0":"# SVC \nparam_grid = {'C': [1000 , 10000],  \n              'gamma': [ 0.001,0.01], \n              'kernel': [  'rbf'],}  \n  \nsv = GridSearchCV(SVC(), param_grid, verbose = 3 , n_jobs=-1 , cv = 5) \n  \n# fitting the model for grid search \nsv.fit(X_train_d_s , y_train)","f56071d8":"sv.best_params_","990b5a73":"sv.score(X_train_d_s , y_train)","ea6d3e67":"sv.score(X_test_d_s , y_test)","a7be5a10":"O_model_accuracy.append(sv.score(X_test_d_s , y_test))\nO_model_name.append('SVC')","ba403910":"# Cconfusion matrix, classification report\nprint(f'confusion matrix for RF\\n{confusion_matrix(y_test,sv.predict(X_test_d_s))}\\n classification report for RF \\n {classification_report(y_test,sv.predict(X_test_d_s))}')","ea864c81":"# LogisticRegression\npenalty = ['l1', 'l2']\nC = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nclass_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\nsolver = ['liblinear', 'saga']\n\nparam_grid = dict(penalty=penalty,\n                  C=C,\n                  class_weight=class_weight,\n                  solver=solver)\n\nlo = GridSearchCV(estimator=LogisticRegression(),\n                    param_grid=param_grid,\n                    scoring='roc_auc',\n                    verbose=1,\n                    n_jobs=-1 ,\n                 cv = 5)\nlo.fit(X_train_d_s, y_train)","888c6815":"lo.best_score_","686ba44a":"lo.best_estimator_.score(X_train_d_s , y_train)","1832f866":"lo.best_estimator_.score(X_test_d_s , y_test)","5c46e319":"O_model_accuracy.append(lo.best_estimator_.score(X_test_d_s , y_test))\nO_model_name.append('LogisticRegression')","e3edb2f2":"# confusion matrix, classification report\nprint(f'confusion matrix for RF\\n{confusion_matrix(y_test,lo.predict(X_test_d_s))}\\n classification report for RF \\n {classification_report(y_test,lo.predict(X_test_d_s))}')","02a0df5f":"# AdaBoostClassifier \n\npar={'n_estimators':[500,1000,2000],\n             'learning_rate':[.001,0.01,.1]}\nada=GridSearchCV(estimator=AdaBoostClassifier()\n                    ,param_grid=par,\n                    n_jobs=-1,cv=5 , verbose= 2)\nada.fit(X_train_d_s, y_train)","4ef8054c":"ada.best_score_","2c864e21":"ada.score(X_train_d_s , y_train)","9b6446be":"ada.score(X_test_d_s , y_test)","457de9bc":"O_model_accuracy.append(ada.score(X_test_d_s , y_test))\nO_model_name.append('AdaBoostClassifier')","8689b88b":"# confusion matrix, classification report\nprint(f'confusion matrix for RF\\n{confusion_matrix(y_test,ada.predict(X_test_d_s))}\\n classification report for RF \\n {classification_report(y_test,ada.predict(X_test_d_s))}')","8c323ff3":"from sklearn.ensemble import VotingClassifier","a3fac353":"# Ensembling several models\nmodel1 = LogisticRegression(C=0.1, class_weight={0: 0.6, 1: 0.4}, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n                   warm_start=False)\nmodel2 = DecisionTreeClassifier()\nmodel3 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=30, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=2, min_samples_split=5,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False,\n                       verbose=0, warm_start=False,random_state=42)\nmodel4 = ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                     max_depth=20, max_features='auto', max_leaf_nodes=None,\n                     min_impurity_decrease=0.0, min_impurity_split=None,\n                     min_samples_leaf=2, min_samples_split=5,\n                     min_weight_fraction_leaf=0.0, n_estimators=100,\n                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n                     warm_start=False)\nmodel5 = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n                   learning_rate=0.001, n_estimators=2000, random_state=None)\nmodel6 = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n                           learning_rate=0.02, loss='deviance', max_depth=3,\n                           max_features='sqrt', max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=15, min_samples_split=15,\n                           min_weight_fraction_leaf=0.0, n_estimators=3000,\n                           n_iter_no_change=None, presort='auto',\n                           subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False,random_state=random_state)\nmodel7 = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)\nmodel8 = GaussianNB()\n\nevc = VotingClassifier(estimators=[('lr', model1), ('dt', model2),\n                                     ('rf', model3),('et', model4),\n                                     ('adb', model5),('gb', model6),\n                                     ('svc', model7),('gnb', model8)],n_jobs=-1)","2051c5f1":"evc.fit(X_train_d_s,y_train)","4ffea67b":"evc.score(X_train_d_s,y_train)","af4f8915":"evc.score(X_test_d_s,y_test)","70430629":"O_model_accuracy.append(evc.score(X_test_d_s,y_test))\nO_model_name.append('VotingClassifier')","28e95e74":"Models = pd.concat([pd.Series(O_model_name), pd.Series(O_model_accuracy)], axis=1).sort_values(by=1, ascending=False)","368758c5":"Models.rename(columns={0:'model_name',\n                      1:'accuracy'}, inplace=True)\nModels","c75a0e20":"a = sns.barplot(Models.accuracy, Models.model_name,palette='ocean_r')\na.set_xlim(0.75,0.9)\nplt.title(\"Recomended Models\", fontsize =15,)\nplt.show()\n","639cf6dc":"coef_df = pd.DataFrame({'feature': X_train_d_s.columns,\n                        'importance': abs(ra.best_estimator_.feature_importances_), \n                        })\n\ncoef_df.head()","0e3cbcb2":"# sort by absolute value of coefficient (magnitude)\ncoef_df.sort_values('importance', ascending=False, inplace=True)\ncoef_df[:10]","08e675a5":"# top features selected by model\nplt.xticks(rotation=45)\nsns.barplot(coef_df.feature[:7] , coef_df.importance[:7],palette='ocean_r') # top  features\nplt.title(\"Feature imprtance\", fontsize =15)\nplt.show()","b72f321d":"# recreating the training and testing dataset to do the prediction on the testing data\ndf_d = pd.get_dummies(df  , drop_first=True)\ndf_d.shape","b4db8288":"y=pd.DataFrame(df_d.pop('Survived'))","fcd23ee6":"y_test = y.iloc[train.shape[0]:]\nX_test= df_d.iloc[train.shape[0]:,:]\n","f718f67f":"y_train = y.iloc[:train.shape[0]]\nX_train = df_d.iloc[:train.shape[0] , :]","249d628f":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\ns = StandardScaler()\nr = RobustScaler()\n\nX_train_d_s = pd.DataFrame(s.fit_transform(X_train) , columns=X_train.columns)\nX_test_d_s = pd.DataFrame(s.transform(X_test) , columns=X_test.columns)","3efe77e9":"# creating the dataframe then save it as csv file before submiting.\nsub = pd.DataFrame({\n        \"PassengerId\": test.PassengerId,\n        \"Survived\": evc.predict(X_test_d_s)\n})\nsub['Survived']= sub['Survived'].astype(int)\nsub.head()\n","25df9225":"sub.to_csv('sub9.csv' , index=False)","14f30946":"pd.read_csv('sub9.csv').head()","fdd778fe":" # Data exploration","fbc888f5":"# Feature Engneering","7f30e22b":"# Problem Statement","cf7f636e":" Most models selected the above features as the highest importannt features, because they of thier high corelation.","c157edca":"# Optimization","83edc68b":"# Recommendations","97e2b6e4":"# Kaggle Competition on Titanic-Machine-Learning-from-Disaster\n###### Team:  Ibrahim Hakami - Mohammed Alqahtani - Sumaiah Alsadhan","d19f0b66":"# Machine Learning","d6d2b381":"To **predicts which passengers survived the Titanic shipwreck..** prediction will be the value of the Survived variable based on the datasets that include passenger information like name, age, gender, socio-economic class, etc. \n*Applying EDA and Modeling with classification models.*","76d3ce2d":"# EDA\n### Data exploration , Data Cleaning, Feature engineering and visualization.","346b75d9":"# Predicting then submitting on Kaggle"}}