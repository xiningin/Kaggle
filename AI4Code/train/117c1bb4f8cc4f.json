{"cell_type":{"cef50bd1":"code","934bea8e":"code","f0277985":"code","6a003431":"code","7bae2a11":"code","f0f055ea":"code","86553bcf":"code","1d0f125f":"code","b50506a4":"code","ee43dee3":"code","46b1881e":"code","d0b35ffb":"code","ad32ec53":"code","59389a42":"markdown","cbaaa683":"markdown","062f61ea":"markdown","20169db7":"markdown","6beeb8e2":"markdown","c8e10d1a":"markdown","4adad934":"markdown","be7c6706":"markdown","187443c8":"markdown","c8011163":"markdown"},"source":{"cef50bd1":"import os\nfrom ast import literal_eval\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\n\nimport shutil","934bea8e":"TRAIN_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images'","f0277985":"df = pd.read_csv('\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv')\ndf.tail()","6a003431":"n_with_annotations = (df['annotations'] != '[]').sum()\nlen(df), n_with_annotations","7bae2a11":"N = 6000\ndf = pd.concat([\n    df[df['annotations'] != '[]'],\n    df[df['annotations'] == '[]'].sample(N - n_with_annotations)\n]).sample(frac=1).reset_index(drop = True)\ndf.tail()","f0f055ea":"df['video_id'].value_counts()","86553bcf":"valid = df['video_id'] == 2\ntrain = df['video_id'] != 2\ndf.loc[valid, 'is_valid'] = True\ndf.loc[train, 'is_valid'] = False\n\ndf['annotations'] = df['annotations'].apply(literal_eval)\ndf['path'] = df.apply(lambda row: f\"{TRAIN_PATH}\/video_{row['video_id']}\/{row['video_frame']}.jpg\", axis = 1)\n\ndf.tail()","1d0f125f":"%%writefile config.yaml\n\ntrain: train \nval: valid\n\nnc: 1  \nnames: ['starfish'] ","b50506a4":"def to_yolo(box, img_w = 1280, img_h = 720):\n    \n    w = box['width']\n    h = box['height']\n    xc = box['x'] + int(np.round(w\/2))\n    yc = box['y'] + int(np.round(h\/2))\n\n    return [xc\/img_w, yc\/img_h, w\/img_w, h\/img_h]","ee43dee3":"os.makedirs('train\/images', exist_ok=True)\nos.makedirs('train\/labels', exist_ok=True)\nos.makedirs('valid\/images', exist_ok=True)\nos.makedirs('valid\/labels', exist_ok=True)","46b1881e":"for i, row in tqdm(df.iterrows(), total = len(df)):\n    \n    bboxes = row['annotations']\n    bboxes = [to_yolo(bbox) for bbox in bboxes]\n    \n    base_dir = 'valid' if row['is_valid'] else 'train'\n    fname = f\"{row['video_id']}_{row['video_frame']}\"\n    \n    with open(f'{base_dir}\/labels\/{fname}.txt', 'w+') as f:\n        for bbox in bboxes:\n            f.write('0 ' + ' '.join([str(round(b, 3)) for b in bbox]) + '\\n')\n    shutil.copyfile(row['path'], f\"{base_dir}\/images\/{fname}.jpg\")","d0b35ffb":"shutil.make_archive('valid', 'zip', 'valid')\nshutil.make_archive('train', 'zip', 'train')","ad32ec53":"shutil.rmtree('valid') \nshutil.rmtree('train') ","59389a42":"## Reading the train data","cbaaa683":"## Looping through the dataframe and saving the files","062f61ea":"## Ziping the files so kaggle can assemble a dataset\n\nthe final dataset can be found [HERE](https:\/\/www.kaggle.com\/coldfir3\/great-barrier-reef-yolov5)","20169db7":"For this data, I believe that the only consistent way to split the images between train\/val are by video.","6beeb8e2":"## Background images (no detections)\n\nFor this dataset I decided to use a total of `N=6000` images only. This is because most of the images don't have any annotation and Yolo recommends the folowing:\n> Background images. Background images are images with no objects that are added to a dataset to reduce False Positives (FP). We recommend about 0-10% background images to help reduce FPs (COCO has 1000 background images for reference, 1% of the total). No labels are required for background images.\n\nAs you can see below, the dataset have way more than that.","c8e10d1a":"<h1><center>Efficient YoloV5 Dataset Generator<\/center><\/h1>     \n\n<center><img src = \"https:\/\/i.imgur.com\/iatgdo5.jpg\" width = \"635\" height = \"235\"\/><\/center>         \n\nThis dataset was built to be compatible with the train (train.py) script that can be found [HERE](https:\/\/github.com\/ultralytics\/yolov5). I also have a training notebook with WandB integration that you can find [HERE](https:\/\/www.kaggle.com\/coldfir3\/yolov5-train\/edit\/run\/81607643). The inference notebook is still WIP. The resulting kaggle Dataset cand be found [HERE](https:\/\/www.kaggle.com\/coldfir3\/great-barrier-reef-yolov5)\n\nThe tree main tasks into converting this dataset to Yolo format are:\n1. Splitting into train and test\n1. Converting the bboxes to yolo format `[xc, yc, w, h]` and saving them into text files\n1. Arranging the files in the expected folders and writting the `.yaml`\n\n<h3 style='background:orange; color:black'><center>Consider upvoting this notebook if you found it helpful.<\/center><\/h3>","4adad934":"## Train Val split","be7c6706":"So we will use video 2 as validation and 0 and 1 as training. For better final performance I advise you to do a 3-fold split on this data and ensemble the final models using WBF that you can find [HERE](https:\/\/github.com\/ZFTurbo\/Weighted-Boxes-Fusion)","187443c8":"## The .yaml file\n\nYolo uses an .yaml file to indicate the number and name of the classe aswell as the location of the images\/labels.","c8011163":"## Converting to yolo bbox format\n\nYolo uses a bbox format of `[xc, yc, w, y]` therefore we need to adjust our dataset to reflect that notation.\n\n<center><img src = \"https:\/\/user-images.githubusercontent.com\/26833433\/91506361-c7965000-e886-11ea-8291-c72b98c25eec.jpg\" width = \"635\" height = \"235\"\/><\/center>        "}}