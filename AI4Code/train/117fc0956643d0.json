{"cell_type":{"41ad17fb":"code","844be0e4":"code","585679a5":"code","2cd172a6":"code","798d58a8":"code","c3b2e405":"code","ad5a3658":"code","7f8c6c26":"code","bd4c1c48":"code","5e5c6111":"code","85136429":"code","3e9f6a7e":"code","b07c2e35":"markdown","d953608a":"markdown","2411c7c6":"markdown","efd52523":"markdown","40d952b6":"markdown","90b29678":"markdown","d12a7f51":"markdown","59e0cde1":"markdown","2088361f":"markdown","b44724d0":"markdown","a8776467":"markdown","d49396b9":"markdown","aab7c444":"markdown","7cd1e791":"markdown","62c407cd":"markdown","be975d1b":"markdown","3b6286cf":"markdown","d1e43d9c":"markdown","df0ffc29":"markdown","c6199dd1":"markdown"},"source":{"41ad17fb":"# turn the internet on for this to install properly\n!pip install transformers\n!pip install colorama","844be0e4":"import numpy as np\nimport torch\nimport pandas as pd\nimport transformers\nfrom tqdm import tnrange\nfrom transformers import AlbertTokenizer, AlbertForQuestionAnswering\nimport colorama\nimport os\nimport re\nimport matplotlib.pyplot as plt","585679a5":"import pandas as pd\nfrom tqdm import tnrange\n\ndef get_filtered_articles(path_to_filtered_articles, path_to_metadata):\n\n  # Get filtered articles \n  data_filtered_titles = pd.read_csv(path_to_filtered_articles, usecols=['title'], keep_default_na = False) # length: 15291\n  metadata = pd.read_csv(path_to_metadata, keep_default_na=False) # length: 138794\n\n  # Get detailed information of the filtered articles from metadata\n  df = pd.merge(data_filtered_titles, metadata) # length: 13667\n\n  return df\n\n","2cd172a6":"import pandas as pd\nfrom tqdm import tnrange\n\n\n# the file path containing titles of the filtered articles extracted by topic modeling\npath_to_filtered_articles = '..\/input\/covid19-related-articles\/Ten_Tasks.csv'\npath_to_metadata = '..\/input\/CORD-19-research-challenge\/metadata.csv'\n\ndata = get_filtered_articles(path_to_filtered_articles, path_to_metadata)\nprint(data.head())","798d58a8":"# Import fine-tuned ALBERT model. \n\nfrom transformers import AlbertTokenizer, AlbertForQuestionAnswering\n\ndef get_model(path_to_model):\n\n  if torch.cuda.is_available():  \n    print(\"GPU available\")\n    dev = \"cuda:0\" \n  else:  \n    dev = \"cpu\"  \n\n  print(\"Import pre-trained ALBERT model...\")\n  tokenizer = AlbertTokenizer.from_pretrained(path_to_model)\n  model = AlbertForQuestionAnswering.from_pretrained(\n      path_to_model).to(dev)\n    \n  print(\"Import Complete!\")\n  return tokenizer, model, dev","c3b2e405":"path_to_model = '..\/input\/albert-trained-on-squad-and-bioasq'\n\ntokenizer, model, device = get_model(path_to_model)","ad5a3658":"import torch\nfrom tqdm import tnrange\n\ndef extract_relevant_answer(model, tokenizer, data, question, device):\n  confidence = []\n  predictions = []\n  start_end_idx = []\n    \n  for i in tnrange(len(data)):\n    abstract = data.iloc[i]['abstract']\n    input_ids = tokenizer.encode(question, abstract)\n    input_ids = input_ids[0:512]\n    \n    sep_idx = input_ids.index(tokenizer.sep_token_id)\n    token_type_ids = [0 if i <= sep_idx else 1 for i in range(len(input_ids))]\n\n    input_ids_tensor = torch.tensor([input_ids]).to(device)\n    token_type_ids_tensor = torch.tensor([token_type_ids]).to(device)\n    \n    start_scores, end_scores = model(input_ids_tensor, token_type_ids=token_type_ids_tensor)\n\n    start_idx = start_scores.argmax()\n    end_idx = end_scores.argmax()+1\n    score = (start_scores.max() + end_scores.max()) \/ 2\n    score = score.item()\n\n    if start_idx <= 0 or end_idx <= 0 or end_idx<=start_idx:\n      predictions.append(\"Not Relevant\")\n      score = float('-inf')\n    else:\n      tokens = tokenizer.convert_ids_to_tokens(input_ids[start_idx:end_idx])\n      prediction = tokenizer.convert_tokens_to_string(tokens)\n      predictions.append(prediction)\n    \n    confidence.append(score)\n    start_end_idx.append([start_idx, end_idx])\n    \n  return predictions, confidence, start_end_idx","7f8c6c26":"import pandas as pd\nimport numpy as np\n\ndef top_n_answers(question, predictions, confidence, start_end_idx, data, n):\n\n  print(\"Get top \" + str(n) + \" relevant articles & answers\")\n  confidence = np.array(confidence)\n  top_n_scores_idx = (-confidence).argsort()[:n]\n\n  top_n_articles = pd.DataFrame()\n  for idx in top_n_scores_idx:\n    entry = data.iloc[idx]\n    entry['confidence'] = confidence[idx]\n    entry['prediction'] = predictions[idx]\n    top_n_articles = top_n_articles.append(entry)\n    \n  top_n_articles.to_csv(question + \".csv\")\n\n  return top_n_articles","bd4c1c48":"import colorama\n\ndef print_top_n_articles(question, top_n_articles):\n\n  print(\"Prediction highlighted in red....\")\n  print(\"========  \" + question + \"  ======== \")\n    \n  for i in range(len(top_n_articles)):\n    entry = top_n_articles.iloc[i]\n\n    abstract = entry['abstract']\n    prediction = entry['prediction']\n\n    prediction_start_idx = abstract.find(prediction)\n    prediction_end_idx = prediction_start_idx + len(prediction)\n    \n    \n    print( \"(\" + str(i) + \")\")\n    print(\"Title : \" + entry['title'] + \"\\n\")\n    print(\"Confidence: \" + str(entry['confidence']))\n    print(\"Abstract : \" + abstract[:prediction_start_idx])\n    print (colorama.Fore.RED, abstract[prediction_start_idx: prediction_end_idx].rstrip())\n    print(colorama.Style.RESET_ALL, abstract[prediction_end_idx:] + \"\\n\")\n\n\n","5e5c6111":"question = 'What do we know about vaccines and therapeutics?'\n\nfile_path = os.path.join('..\/input\/covid19-top-articles', question.replace('?','').replace(',','') + '_.csv')\n\nif os.path.exists(file_path):\n    top_n_articles = pd.read_csv(file_path)\nelse:\n    # Extract relevant span of text\n    predictions, confidence, start_end_idx = extract_relevant_answer(model, tokenizer, data, question, device)\n\n    # Get the top n articles sorted by confidence scores\n    n = 10\n    top_n_articles = top_n_answers(question, predictions, confidence, start_end_idx, data, n)\n\n# Print top n relevant articles\nprint_top_n_articles(question, top_n_articles)","85136429":"# STEP 1: \n# the file path containing titles of the filtered articles extracted by topic modeling\npath_to_filtered_articles = '..\/input\/covid19-related-articles\/Ten_Tasks.csv'\npath_to_metadata = '..\/input\/CORD-19-research-challenge\/metadata.csv'\n# Get filtered articles\ndata = get_filtered_articles(path_to_filtered_articles, path_to_metadata)\n\n# STEP 2: Load the pretrained model\npath_to_model = '..\/input\/albert-trained-on-squad-and-bioasq'\ntokenizer, model, device = get_model(path_to_model)\n\n# STEP 3: Extract Excerpt From Abstract Using Fine-Tuned ALBERT\nquestions = ['What is known about transmission, incubation, and environmental stability?',\n             'What do we know about COVID-19 risk factors?',\n             'What do we know about vaccines and therapeutics?',\n             'What do we know about virus genetics, origin, and evolution?',\n             'What has been published about medical care?',\n             'What do we know about non-pharmaceutical interventions?',\n             'What has been published about ethical and social science considerations?',\n             'What do we know about diagnostics and surveillance?',\n             'What has been published about information sharing and inter-sectoral collaboration?',\n             'What do we know about the virus, the human immune response and predictive models?']\n\nconfidence_matrix = np.zeros(shape=(10,10))\nfor i in range(len(questions)):\n    question = questions[i]\n    file_path = os.path.join('..\/input\/covid19-top-articles', question.replace('?','').replace(',','') + '_.csv')\n    print(file_path)\n    if os.path.exists(file_path):\n        top_n_articles = pd.read_csv(file_path)\n    else:\n        print(\"File doesn't exist!\")\n        predictions, confidence, start_end_idx = extract_relevant_answer(model, tokenizer, data, question, device)\n        \n        # STEP 4: Get the top n articles sorted by confidence scores\n        n = 10\n        top_n_articles = top_n_answers(question, predictions, confidence, start_end_idx, data, n)\n        \n    # Get confidence score and store in numpy array\n    confidence_matrix[i] = top_n_articles['confidence']\n    # Print top n relevant articles\n    print_top_n_articles(question, top_n_articles)","3e9f6a7e":"# Lables on x axis\nx = ['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th']\n\n# Plot a bar chart for each question.\nfig, axs = plt.subplots(6,2,figsize=(15,15))\n\nfor i in range(len(confidence_matrix)):\n    row = int(i \/ 2)\n    col = int(i % 2)\n    axs[row,col].bar(x, confidence_matrix[i])\n    axs[row,col].set_title(questions[i])\n    axs[row,col].set_ylim([4, 9])\n    axs[row,col].set_xlabel('Rank')\n    axs[row,col].set_ylabel('Confidence')\n    \n# Plot mean confidence score of each rank \nconfidence_mean = np.mean(confidence_matrix, axis = 0) \naxs[5,0].bar(x, confidence_mean, color='orange')\naxs[5,0].set_title(\"Average per rank\") \naxs[5,0].set_ylim([4, 9])\naxs[5,0].set_xlabel('Rank')\naxs[5,0].set_ylabel('Confidence')\n\n# Compare the confidence score of the 1st relevant article of each question\nqs = [\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\", \"Q10\"]\naxs[5,1].bar(qs, confidence_matrix[:,0], color='green')\naxs[5,1].set_title(\"1st relevant articles with different questions\") \naxs[5,1].set_ylim([4, 9])\naxs[5,1].set_xlabel('Question #')\naxs[5,1].set_ylabel('Confidence')\n\n# Plot\nfig.tight_layout(pad=2.0)","b07c2e35":"## Step 5. Run over All 10 COVID-19 Questions\n\nWe have run the same approach for all the questions currently posted on Kaggle. Again, we have run the code with GPU in Google Colab due to the time constraint. Remove the files from \"..\/input\/covid19-top-articles\" if you prefer to run on your local machine. ","d953608a":"<div id=\"step2\"><\/div>","2411c7c6":"<div id=\"step4\"><\/div>","efd52523":"<div id=\"step5\"><\/div>","40d952b6":"## Step 1. Filter for Only Question-Related Articles Using LDA\n\nWe use the filtered articles extracted by **Latent Dirichlet Allocation** (LDA), a well-know topic modeling algorithm. Receiving words as an input vector LDA performs a generative process to provide topics that are probability distribution over words. The aim is to employ LDA to find the most important topics (a bunch of keywords) that are relevant to the description of task-4 (which is related to the question) and use them to find articles from the Cord-19 dataset (a dataset with 40K pieces) that have one of those topics in their titles. \nSince the output of topic modeling only contains the articles that have been proposed for the first round of submission, we have not included the articles added to the second round of introduction. \nThe filtered articles are joined with \"metadata.csv\" to get detailed information on each article. \nAfter joining two data frames, the number of articles has been reduced to 15170 from 138794. \nNote: Due to the lack of suitable computational power, we used the Cord-19 dataset with 40K articles. ","90b29678":"<div id=\"step3\"><\/div>","d12a7f51":"<div id=\"step6\"><\/div>","59e0cde1":"## Goal & Approach\n\nSince the beginning of 2020, many articles addressing COVID-19 have been rapidly published. Due to the overflow of information, people in medical communities face difficulties getting helpful information that can answer their questions. This notebook provides an overview of information extraction to fight against the information overflow.\n\nOur framework utilizes topic modeling to extract the relevant articles to COVID-19. Then, we use a pre-trained ALBERT model to get an excerpt from each article obtained from the previous methods that get answer questions related to COVID-19. The ALBERT model is pre-trained with two different datasets, SQuAD v1.1 and BioASQ 6b factoid QA pairs. SQuAD is a reading comprehension dataset with over 100k QA pairs. Since there is no publicly available ALBERT model pre-trained on the general QA task, we first utilize the SQuAD dataset so that our model targets explicitly the QA task. After pretraining on SQuAD v1.1, we fine-tune our model to the medical domain using BioASQ factoid QA pairs. \n\nThe main advantage of our framework is that it can be applied to any queries related to COVID-19, as there is no additional tuning required. \nThe diagram below shows the overall workflow of our approach to tackling the COVID19 task.","2088361f":"\n## Step 6. Plot Confidence Score for Top 10 Relevant Articles\n\nThis section visualizes the confidence score of each question, mean confidence scores of the top 10 articles, and comparison of the confidence score of the 1st relevant articles of each questions. ","b44724d0":"## Step 3. Extract Excerpt from Abstract Using Fine-Tuned ALBERT\nWe utilize the fine-tuned ALBERT model to get answers relevant to each COVID-19 questions. The abstract of each article is the context we use for extracting relevant answers to questions. The answer is a span of text from the abstract. The workflow can be broken down into the following steps. \n\n1. Tokenizing the question and the abstract of each article  \n2. Feeding the fine-tuned model with the tokenized text\n3. Calculating the confidence score of the start and end index for each token. \n4. Getting tokens with the maximum confidence score for start and end index in the abstract. \n5. Detokenizing and getting an answer (i.e. a span of text from the abstract).\n\nAfter calculating the maximum start and end confidence score of each article, we average the two confidence scores. The average of the beginning and end index of the abstract is the corresponding article's confidence score. ","a8776467":"### Step 4.1. Print the Top Relevant Articles with an Highlighted Excerpt","d49396b9":"# Extract Excerpt From Abstract Using LDA and Fine-Tuned ALBERT\n\nIn the following sections, we explain how we have developed a framework for finding answers for the next question: What do we know about the virus, the human immune response, and predictive models?","aab7c444":"## Conclusion\n\nWe have tackled the COVID19 challenge competition by utilizing LDA for extracting the COVID19-related articles and applying a fined-tuned ALBERT model to extract answers from the top 10 articles with the highest confidence score. The visualization of our result above shows that each question gets satisfactory results with the corresponding reliable confidence score, mostly higher than 6. Since there is no additional tuning required for each question, our approach can be extended to answer other types of questions in the biomedical domain. \n\nOur work is a continuation of the work in https:\/\/www.kaggle.com\/mahparsa\/sbert-bert-for-cord-19-data ","7cd1e791":"<div id=\"step1\"><\/div>","62c407cd":"## Steps\n\n0. [Install All Required Libraries](#step0)\n1. [Filter for Only Question-Related Articles Using LDA](#step1)\n2. [Load pretrained ALBERT model](#step2)\n3. [Extract Excerpt From Abstract Using Fine-Tuned ALBERT](#step3)\n4. [Get Top 10 relevant articles](#step4)\n5. [Run over all 10 COVID-19 questions](#step5)\n6. [Plot Confidence Score for Top 10 relevant articles](#step6)\n","be975d1b":"<div id=\"step0\"><\/div>","3b6286cf":"LDA+ALBERT.png![image.png](attachment:image.png)","d1e43d9c":"## Step 4. Get Top 10 relevant articles\nWe have sorted the articles in descending order based on the confidence score calculated from the above steps. After sorting the articles, we extract the top 10 articles.\n\nDue to the time constraint, we have run the code using GPU in Google Colab and uploaded the output in \"..\/input\/covid19-top-articles\". Remove the output files from the directory if you prefer running on your local machine.","df0ffc29":"## Step 2. Load Pretrained ALBERT Model\n\n[ALBERT](https:\/\/arxiv.org\/abs\/1909.11942) has similar architecture as other BERT models, but it is on the basis of a transformer encoder with Gaussian Error Linear Units (GELU) nonlinearities. ALBERT uses a different embedding method than BERT. In more detail, ALBERT uses two-step word embedding that first projects a word into a lower-dimensional embedding space and then extends it to the hidden space. Furthermore, ALBERT uses a cross-layer parameter sharing to improve parameter efficiency; it only uses feed-forward network (FFN) parameters across layers to share attention parameters. Another difference between ALBER and BERT is that ALBERT uses a sentence-order prediction (SOP) loss to avoid topic prediction and focus on modeling inter-sentence coherence.\n\nWe have pre-trained the ALBERT model with **[SQuAD v1.1](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/)** followed by **BioASQ 6b** factoid datasets. The **SQuAD** dataset is a reading comprehension QA dataset posed by crowdworkers on Wikipedia articles, and the answer to each question is a span of text from the corresponding article. The BioASQ 6b dataset is biomedical semantic QA pairs, in which we assume that the contents are closely related to COVID-19. The limitation of pretraining with only **BioASQ** dataset is the lack of data. The primary reason for pretraining our ALBERT model with SQuAD v1.1 is to overcome the data shortage of using only **[BiOASQ](https:\/\/bmcbioinformatics.biomedcentral.com\/articles\/10.1186\/s12859-015-0564-6)** dataset. \n\nWe have pre-trained ALBERT base model with 3 different approaches (1) **SQuAD v1.1** only (2) **BioASQ 6b** factoid only (3) **SQuAD v1.1** followed by **BioASQ 6b** factoid. Each model is evaluated using **BioASQ** test dataset, and we have confirmed that the model pre-trained on both dataset have achieved the highest performance. Therefore, we will use the model with the most top performance for the COVID-19 task. \n\nThe models are pre-trained using GPU available in google colab. The pretraining process using both **SQuAD v1.1** and **BioASQ 6b** factoid dataset took approximately two hours. \nThe pre-rained model is uploaded in the input directory. ","c6199dd1":"## Step 0. Install All Required Libraries"}}