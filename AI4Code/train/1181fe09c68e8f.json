{"cell_type":{"d37725a0":"code","ec3916e2":"code","7825ee6d":"code","67409c7c":"code","074c57aa":"code","0885c54b":"code","ef7f6fdd":"code","822220fb":"code","e199e431":"code","94cddecd":"code","4f8f50e9":"code","7cd355b7":"code","a53059eb":"code","6a249c47":"code","6d3a50b8":"code","3feb6517":"code","17825a10":"code","d5ab5dbb":"code","5c72fbd3":"code","ca0182a4":"code","166adcf8":"code","2e2ce107":"code","e6e6eb42":"code","309fa7fc":"code","5fbaf739":"code","f9389f53":"code","8033972e":"markdown","4a49ff58":"markdown","378e9aca":"markdown","17fb8021":"markdown","98e6e91b":"markdown","6aed62b2":"markdown","3f077a39":"markdown","f314056d":"markdown","ed6eb204":"markdown","97c1d991":"markdown","3fc7d1d7":"markdown","9f4da277":"markdown","147af947":"markdown","7164f926":"markdown","f0d9e0e5":"markdown","42570f7c":"markdown","90ccad05":"markdown","c06c98e1":"markdown","39058825":"markdown"},"source":{"d37725a0":"###########################\n# Configuration Principale\n# #########################\n\nvocab_size= 22000        # Taille maximale du corpus de vocabulaire du jeu de donn\u00e9es\nembedding_dim=32         # Nombre d'embedding dimensions permettant de qualifier un mot\nmax_length = 18          # Longueur maximale en nombre de mots des titres avant de tronquer\ntrunc_type='post'        # M\u00e9thode pour trunquer si d\u00e9passement (avant ou apr\u00e8s)\npadding_type='post'      # Param\u00e8tre du padding (apr\u00e8s la phrase)\noov_tok='<OOV>'          # Token \u00e0 utiliser quand un mot est manquant\ntraining_size=22000      # Taille du training Set (pour le split entre train\/test)\nnum_epochs = 10          # Nombre epoch pour entrainement du r\u00e9seau\nDEL_STOPWORDS = False    # Pour la suppression des Stop Words\nDO_LEMMATIZE  = True     # Lemmatization des mots\nPATIENCE_STOPPING = 5    # Patience en nombre d'epoch avant de stopper l'entrainement\nOPTIMIZER = 'adam'       # Optimizer pour la descente de gradient (optimisation fonction co\u00fbt)","ec3916e2":"# Lib Standards\nimport os\nimport io\n\n# D\u00e9sactiver les avertissements\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \n\n# Verification de l'import des Data\nprint(\"Les donn\u00e9es import\u00e9es dans Kaggle: \")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7825ee6d":"# Import des Librairies Datascience\nimport numpy as np \nimport pandas as pd       \nimport random\nimport nltk\n\n# Import TensorFlow et v\u00e9rification de la version\nimport tensorflow as tf\nprint(\"Version de TensorFlow: {}\".format(tf.__version__))\n# Version majeure:\nvers_tf = int((tf.__version__).split(sep='.')[0])\n\nif vers_tf < 2:\n    # eager_execution n\u00e9cessaire si TensorFlow < 2.0\n    print('Version < 2 : activation eager execution')\n    tf.enable_eager_execution()\n\n# Import des librairies de NLP\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\n# Librairies nltk pour la tokenization et la Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import word_tokenize","67409c7c":"# Toujours fr\u00e2ce \u00e0 la librairie Panda nous proc\u00e9dons \u00e0 l'ouverture du fichier de data\n# Ouverture du fichier de donn\u00e9es contenant les titres\ndf = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\", lines=True)\n\n# Pour voir le nom des colonnes et la taille des objects du dataset import\u00e9\n# Objet dataframe, nous observons gr\u00e2ce \u00e0 cette sortie : \n#     Les colonnes du DataFrame\n#     leur Type\n#     Le nombre d'enregistrements\ndf.info()","074c57aa":"# On examine les premi\u00e8res lignes du dataset\ndf.head()","0885c54b":"# denombrage du nombre articles classifi\u00e9 sarcastic et des autres \nnb_sarcastic = (df['is_sarcastic'] == 1).sum()\nnb_not_sarcastic = (df['is_sarcastic'] == 0).sum()\nprint(\"Il y a {} titres sarcastiques vs {} qui ne le sont pas\".format(nb_sarcastic, nb_not_sarcastic))","ef7f6fdd":"# Affichage au hasard de quelques titres et de leur label\nfor i in range(3):\n    n = random.randrange(len(df['headline']))\n    print(df['article_link'][n])\n    print(df['headline'][n])\n    print(\"Sarcasme: {}\".format(df['is_sarcastic'][n]))\n    print(\"---\")","822220fb":"# Contruction des listes de phrases(X) et de labels(y)\nX=list(df['headline'])\ny=list(df['is_sarcastic'])","e199e431":"# Import des stopwords grace \u00e0 nltk\nstopwords = nltk.corpus.stopwords.words('english')\nprint(\"Nous avons {} stopwords pour le language anglais. Exemple:\".format(len(stopwords)))\nprint(stopwords[0:50])","94cddecd":"# Fonction pour g\u00e9rer la suppression des stopwords\ndef process_stop_words(X, display_res=True):\n    \n    if DEL_STOPWORDS:\n        # Une autre fa\u00e7on de Tokeniser:\n        # avec fonctions lambda + fonction map\n        # tokenize = lambda x: text_to_word_sequence(x, \n        #                                    filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n', \n        #                                    lower=True, split=' ')\n        # X_seq = list(map(tokenize, X))\n        \n        # avec la liste comprehension et la librairie NLTK:\n        X_seq = [[word for word in word_tokenize(s)] for s in X]\n        \n        # Suppression des Stopwords \n        X_seq_no_stops = [[word for word in s if word not in stopwords] for s in X_seq]\n        \n        if display_res:\n            print(\"Avant Tokenization:     {}\".format(X[10]))\n            print(\"Apr\u00e8s Tokenization:     {}\".format(X_seq[10]))\n            print(\"Sans Stopwords:         {}\".format(X_seq_no_stops[10]))\n        return X_seq_no_stops\n    else:\n        print(\"Pas de traitement stopwords\")\n        return X","4f8f50e9":"# ---------------------------------------------------\n# Gain apport\u00e9 par les 'list comprehension' de Python\n# ---------------------------------------------------\n\n# Gr\u00e2ce a une seule ligne ci-dessous...\n#[[word for word in s if word not in stopwords] for s in X_seq]\n\n# ...factoriser le code suivant:\n\n# X_no_stops = [[0] * 0 for i in range(len(X))]\n# for n,title in enumerate(X):\n#    for word in title:\n#        if word not in stopwords:\n#           X_no_stops[n].append(word)","7cd355b7":"# Processing des Stop Words\nX = process_stop_words(X)","a53059eb":"print(\"Exemple de Lemme:\")\nlist_lemm_exemple = ['cars', 'tools', 'airplane', 'nicely', 'sarcastic']\nfor w in list_lemm_exemple:\n    print(\"    {}   =>   {}\".format(w, WordNetLemmatizer().lemmatize(w)))","6a249c47":"def process_lemmatization(X, display_res=True):\n    if DO_LEMMATIZE:\n        lemm = WordNetLemmatizer()\n        if display_res:\n            print(\"Avant Lemmatization :    {}\".format(X[10]))\n        Xlem = X.copy()\n\n        # Si nous avons une liste de liste \n        # suite \u00e0 la tokenization des titres pour suppression des stopword\n        \n        if DEL_STOPWORDS:\n            # Ancienne m\u00e9thode avec parcourt de liste\n            # for row,title in enumerate(X):\n            #    for col,word in enumerate(title):\n            #        Xlem[row][col] = lemm.lemmatize(word)\n            \n            # Plus simple Avec les g\u00e9nerateurs de liste :-)\n            Xlem = [[lemm.lemmatize(word) for word in s] for s in X]\n\n        # une liste de titre\n        else:\n            Xlem = [[lemm.lemmatize(word) for word in word_tokenize(s)] for s in X]\n\n        if display_res:\n            print(\"Apres Lemmatization :    {}\".format(Xlem[10]))\n\n        return Xlem\n    else:\n        print(\"Pas de lemmatization demand\u00e9e\")\n        return X","6d3a50b8":"# Traitement de la lemmatization\nX = process_lemmatization(X)","3feb6517":"# Constitution des jeux d'entrainements et de test\nX_train = X[:training_size]\ny_train = y[:training_size]\nX_test  = X[training_size:]\ny_test  = y[training_size:]\n\nprint(\"Taille jeu d'entrainement : {}\".format(len(X_train)))\nprint(\"Taille jeu de test : {}\".format(len(X_test)))","17825a10":"# Instanciation du Tokenizer avec les param\u00e8tres\ntokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\n# Adaptation du Tokenizer au jeu d'entrainement\ntokenizer.fit_on_texts(X_train)\n# Cr\u00e9ation d'un dictionnaire d'index des mots\nword_index = tokenizer.word_index                             \n\n# Conversion en sequences d'entier\nX_train_sequences = tokenizer.texts_to_sequences(X_train)\n\n# Ajout du padding \n# Objectif: permettre \u00e0 chaque s\u00e9quence de poss\u00e8der la m\u00eame longueur\nX_train_padded = pad_sequences(X_train_sequences, maxlen = max_length, truncating = trunc_type)","d5ab5dbb":"# Traitement du jeu de test \u00e0 l'identique du jeu d'entrainement\nX_test_sequences = tokenizer.texts_to_sequences(X_test)\nX_test_padded = pad_sequences(X_test_sequences, maxlen = max_length, truncating = trunc_type)","5c72fbd3":"# Nombre de mot dans le corpus\nprint(\"Nombre de mot dans le corpus du jeu d'entrainement : {}\".format(len(word_index)))","ca0182a4":"# Cr\u00e9ation d'un index invers\u00e9 des mots \/ digit associ\u00e9\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\n# Affichage de quelques phrases al\u00e9atoires:\nfor i in range(3):\n    alea = random.randrange(len(X_train))\n    print(\"phrase originale: {}\".format(X_train[alea]))\n    print(\"sequence : {}\".format(X_train_sequences[alea]))\n    print(\"sequence + bourrage: {}\".format(X_train_padded[alea]))\n    print(\"reconstitution: {}\".format(decode_review(X_train_padded[alea])))\n    print(\"----------------------------------------------------------------------\")","166adcf8":"# Cr\u00e9ation du r\u00e9seau neuronal avec LSTM\nmodel = tf.keras.Sequential([\n    \n    ### Couche d'entr\u00e9e\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    \n    ### Couche des LSTM\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    \n    ### Couche de neurones connect\u00e9s\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    \n    ### Couche de Sortie\n    tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n\nmodel.compile(optimizer=OPTIMIZER, loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()","2e2ce107":"# Callbacks (Expliqu\u00e9 dans les tutoriaux CNN)\ncheckpoint = ModelCheckpoint(\"model_sarcasm.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\nearly = EarlyStopping(monitor='val_loss', min_delta=0, patience=PATIENCE_STOPPING, verbose=1, mode='auto')","e6e6eb42":"### Entrainement du mod\u00e8le\n# NB : il est possible d'inclure le param\u00e8tre validation_split dans fit() \n# afin de ne pas traiter le split train\/test plus t\u00f4t\n\nhistory=model.fit(X_train_padded,\n                  y_train,\n                  epochs=num_epochs,\n                  validation_data=(X_test_padded, y_test),\n                  callbacks = [checkpoint, early],\n                  verbose=1)","309fa7fc":"# Graphing de l'apprentissage\n\nimport matplotlib.pyplot as plt\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\nfig.suptitle(\"Performance\")\nax1.plot(history.history['acc'])\nax1.plot(history.history['val_acc'])\nvline_cut = np.where(history.history['val_acc'] == np.max(history.history['val_acc']))[0][0]\nax1.axvline(x=vline_cut, color='k', linestyle='--')\nax1.set_title(\"Model Accuracy\")\nax1.legend(['train', 'test'])\n\nax2.plot(history.history['loss'])\nax2.plot(history.history['val_loss'])\nvline_cut = np.where(history.history['val_loss'] == np.min(history.history['val_loss']))[0][0]\nax2.axvline(x=vline_cut, color='k', linestyle='--')\nax2.set_title(\"Model Loss\")\nax2.legend(['train', 'test'])\nplt.show()","5fbaf739":"# Test sur deux phrases de pr\u00e9sence ou nom de saracasme\nsentence = [\"granny starting to fear spiders in the garden might be real!\", \"Doctor House season finale this sunday evening on TV.\"]\nsentence = process_stop_words(sentence, display_res=False)\nsentence = process_lemmatization(sentence, display_res=False)","f9389f53":"sequences = tokenizer.texts_to_sequences(sentence)\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\nprint(\"Probabilit\u00e9 de d\u00e9tection du sarcasme: \\n\")\nprint(sentence)\nprint(model.predict(padded))","8033972e":"### Possibilit\u00e9s d'Optimisations pour aller plus loin\n* Essayer de limiter l'overfitting constat\u00e9 (avec dropout par exemple ou augmenter la taille du jeu d'entrainement)\n* Tester diff\u00e9rents type de r\u00e9seaux et de neurones (exemple avec les GRU)\n* Tester les diff\u00e9rentes combinaisons de traitement (Lemmatization \/ Stopwords)\n* Jouer avec les param\u00e8tres embedding_dimensions \/ vocab_size \/ taille des phrases maximums\n* Modifier les param\u00e8tres des LSTM (nombre \/ stacking), modifier les param\u00e8tres de Dropout\n* Tester diff\u00e9rents optimizers et learning rates\n* Disposer de jeux de donn\u00e9es plus large et tester avec d'autres jeux de donn\u00e9es\n* Prendre en compte dans le mod\u00e8le la source (url) et \u00e9ventuellement l'article","4a49ff58":"### Graphing de l'entrainement\n\nPour comprendre comment le r\u00e9seau apprend une bonne pratique et de proc\u00e9der au graphing des donn\u00e9es comme nous allons le r\u00e9aliser ci-dessous:","378e9aca":"Il est \u00e0 noter ici que nous construisons un **LSTM bidirectionnel** ce qui signifie que les entr\u00e9es seront lues dans les deux sens permettant de conserver dans deux \u00e9tats m\u00e9moire diff\u00e9rents les informations du pass\u00e9 et celles du futures ! Dans notre cas de figure pour la compr\u00e9hension du sens d'une phrase il est important d'avoir les informations du pass\u00e9 mais aussi celles du future afin de mieux maitriser le contexte.\n\nVous pouvez de fait voir dans le summary() du r\u00e9seau que la couche de LSTM \u00e0 2x plus de param\u00e8tres que ce que nous avons d\u00e9fini dans le code.","17fb8021":"### Tokenisation \/ S\u00e9quences\n\nPour cette \u00e9tape de processing qui est une des plus importante notre objectif est de convertir chaque ligne de texte des titres en s\u00e9quences d'entier.\n\nPour ce traitement nous utilisons l'objet **Tokenizer de tensorflow**.\n\nVoici le lien vers la documentation : https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer\n\n\nNB: pour la m\u00e9thode fit_on_texts La documentation stipule : \n\n*Updates internal vocabulary based on a list of texts.\nIn the case where texts contains lists, we assume each entry of the lists to be a token.*\n\n**Ainsi nous pouvons travailler avec des listes de phrases ou des listes de tokens**\n\n\nNous commen\u00e7ons par initialiser l'objet Tokenizer avec nos param\u00e8tres, puis nous l'entrainons sur le jeu d'entrainement (X_train).\n\nEnsuite nous formons des s\u00e9quences d'entier auxquelles nous ajoutons un padding afin d'avoir des longueurs constantes pour l'entrainement du r\u00e9seau de neurones","98e6e91b":"Ci-dessus nous observons que le jeu de donn\u00e9es contient les liens des articles, les titres, et le label. \n\n**Les donn\u00e9es sont labellis\u00e9es** : 0 signifie que le titre n'est pas sarcastique et 1 qu'il l'est.\n\nNous allons donc maintenant d\u00e9nombrer la r\u00e9partition de ces deux cat\u00e9gories:","6aed62b2":"### Lemmatization (Optionnel)\n\nD\u00e9finition : \nLa lemmatisation en linguistique consiste \u00e0 regrouper les formes infl\u00e9chies d'un mot afin qu'elles puissent \u00eatre analys\u00e9es comme un \u00e9l\u00e9ment unique, identifi\u00e9 par le lemme du mot ou la forme du dictionnaire. Contrairement \u00e0 un radical, la lemmatisation du jeu de donn\u00e9es vise \u00e0 r\u00e9duire les mots en fonction d\u2019un dictionnaire ou d\u2019un vocabulaire r\u00e9el (le lemma).\n\nDe m\u00eame que pour la suppression des stop words la Lemmatization peut \u00eatre appliqu\u00e9e ou non en fonction du jeu de donn\u00e9es \u00e0 traiter.\n\nExemple et mise en oeuvre ci-dessous:","3f077a39":"# NLP (Natural Language Processing) avec TensorFlow !\n\n## D\u00e9tection du Sarcasme en language naturel :-)\n\nDoc House : m\u00e9decin h\u00e9roique ou roi du sarcasme ?!\n\n![image.png](attachment:image.png)\n\nLa **NLP** est la science du traitement du **language Naturel** .\n\nC'est un domaine tr\u00e9s vaste de l'apprentissage automatique qui permet par exemple de classifier, d\u00e9terminer les sentiments associ\u00e9s \u00e0 des phrases, de g\u00e9n\u00e9rer du texte de fa\u00e7on automatiser (via les GAN) de cr\u00e9er des chatbots...\n\n**Traitement du Language**\n\nLe traitement du language diff\u00e8re des autres domaines du machine\/deep learning dans le sens ou il ne s'agit pas de traiter des matrices, des vecteurs ou des tenseurs mais des mots, phrases du langages humain. De ce fait, \u00e9tant donn\u00e9 que les algorithmes ne savent traiter ques des nombres ou du moins des entr\u00e9es normalis\u00e9es il est n\u00e9cessaire de passer par des \u00e9tapes de transformation et de conversion que nous allons d\u00e9tailler. \nPar ailleurs, la difficult\u00e9 de notre language et les subtilit\u00e9s syntaxiques rendent ce travail d'apprentissage une t\u00e2che ardue.\n\nIl existe heuresement des biblioth\u00e8ques et des algorithmes sp\u00e9cialis\u00e9s permettant de r\u00e9pondre et adresser ce genre de probl\u00e9matique. Les librairies les plus connues \u00e9tant Nltk, TensorFlow ou SpaCy... Dans les techniques que nous allons aborder on parlera \u00e9galement de Tokenisation, Lemmatization, Stemming et de Bag of words ...\n\nDans ce tutorial nous allons voir comment de fa\u00e7on tr\u00e8s simple pr\u00e9dire \u00e0 partir d'un jeu de donn\u00e9es issues de titres de journ\u00e9es si les phrases en question sont sarcastiques ... ou non ! \n\n**Architecture du r\u00e9seau de neurones**\n\nPour ce faire nous n'allons plus cette fois comme dans les r\u00e9seaux de Convolution utiliser la m\u00eame architecture de r\u00e9seaux de neurones. En effet ces derniers n'avaient pas besoin de poss\u00e8der de la m\u00e9moire pour travailler, chaque classification d'image pouvait se faire de fa\u00e7on ind\u00e9pendante.\n\nPour ce travail qui est d'une nature diff\u00e9rente il est n\u00e9cessaire d'utiliser un type de neurone permettant de conserver la m\u00e9moire des \u00e9l\u00e9ments vu pr\u00e9c\u00e9dement.\nNous allons pour ce faire utiliser un type de neurone appel\u00e9 **LSTM** (Long Short Term Memory). \n\nEn synth\u00e8se ce sont des neurones capables d'avoir une m\u00e9moire du pass\u00e9 des donn\u00e9es d\u00e9j\u00e0 trait\u00e9es. Cette caract\u00e9ristique est essentielle puisque l'analyse du sens d'une phrase d\u00e9pend des termes pr\u00e9c\u00e9dents.\n\n**Le Sarcasme**\n\nLe sarcasme d\u00e9signe une moquerie ironique, une raillerie tournant en d\u00e9rision une personne ou une situation. Il est mordant, souvent m\u00eame amer et blessant. Il peut \u00eatre consid\u00e9r\u00e9 comme une forme d'ironie piquante ou belliqueuse.\n\n> Le titre de philosophe sera \u00e0 plus haut prix, on ne l'obtiendra pas en r\u00e9p\u00e9tant les sophismes de J.-J. Rousseau (...) ou les sarcasmes de Voltaire\n\n**A l'attaque !**\n\nJe conseille avant l'\u00e9tude ce tutorial de commencer par mes tutos sur la classification des images. En effet certains \u00e9l\u00e9ments d\u00e9j\u00e0 expliqu\u00e9s pr\u00e9c\u00e9dement ne seront pas forcement d\u00e9taill\u00e9s de nouveaux. Egalement, j'en profite pour introduire un concept Pythonique permettant de remplacer les horribles boucles for : il s'agit des 'list comprehension'. Je conseille au lecteur de ce pencher sur ce point car elles rendent de nombreux services tout en simplifiant le code :-)\n\n\n### R\u00e9f\u00e9rences\n\nAvant tout quelques r\u00e9f\u00e9rences de liens ayant permis ce tutoriel ainsi que des lectures compl\u00e9mentaires pour approfondir le sujet.\n\nhttp:\/\/cs229.stanford.edu\/proj2015\/044_report.pdf\nhttps:\/\/github.com\/lmoroney\/dlaicourse\/tree\/master\/TensorFlow%20In%20Practice\/Course%203%20-%20NLP\nhttps:\/\/www.kaggle.com\/nilanml\/detecting-sarcasm-using-different-embeddings\nhttps:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection\nhttps:\/\/www.kaggle.com\/wflazuardy\/sarcasm-detection-with-keras-preprocessing\nhttps:\/\/www.kaggle.com\/mytymohan\/sarcasm-with-keras\n","f314056d":"### Pr\u00e9dictions sur deux exemples\n\nLes graphs ci-dessus montrent une pr\u00e9cisons sur le jeu de test entre 80% et 85% de la d\u00e9tection du sarcasme dans les phrases.\n\nFaisons maintenant une pr\u00e9diction sur deux nouvelles phrases:","ed6eb204":"### Import des librairies \/ Datas","97c1d991":"Nous observons ci-dessus que le jeu de donn\u00e9es n'est pas r\u00e9parti \u00e0 50% entre les donn\u00e9es sarcastiques et les autres.\nCependant nous ne risquons pas dans ce cas de figure d'entrainer un biais lors de l'apprentissage ce qui pourrait se produire si jamais les \u00e9l\u00e9ments positif \u00e9tait tr\u00e9s rare. \n\n=> **Les classes \u00e0 pr\u00e9dire n'\u00e9tant pas unbalanced nous ne feront donc pas de traitement particulier sur ce point**","3fc7d1d7":"### Affichage des Datas\n\nComme toujours nous commen\u00e7ons par importer les donn\u00e9es gr\u00e2ce \u00e0 la librairie Panda.\nEnsuite nous allons regarder comment elles se composent.","9f4da277":"Nous observons avec les graphs qu'apr\u00e8s 2 epochs la pr\u00e9cision sur le jeu de test diminue.","147af947":"### Suppression des Stopwords (optionnel)\n\nCette \u00e9tape permet de supprimer les stopwords Les stopwords sont les mots de language n'apportant pas de valeurs dans l'analyse de la phrase.\nEn anglais ce sont par exemple des termes comme : 'in', 'the', 'a', 'of' ...\n\nBien que ceux-ci puissent avoir une imporance dans l'analyse d'une phrase, leur suppression permet \u00e9galement de diminuer le corpus de vocabulaire et donc de gagner un pr\u00e9cieux temps de traitement. Tout d\u00e9pend donc du besoin et du jeu de donn\u00e9es en entr\u00e9e.\n\nN\u00e9amoins voici une fa\u00e7on de **supprimer les stopwords avec NLTK** avec au passage l'utilisation de **list comprehension en Python** :","7164f926":"Maintenant construisons notre jeu de donn\u00e9es avec les phrases (X) et leurs label (y)\n\n**Pour respecter les conventions usuelles nous les nommerons X et y**","f0d9e0e5":"### Cr\u00e9ation du R\u00e9seau neuronal r\u00e9ccurent\n\nUn r\u00e9seau RNN (R\u00e9seau de neurones r\u00e9currents) permet de souvenir de choses importantes concernant les donn\u00e9es du pass\u00e9es qu'ils ont re\u00e7ues.\n\nCette caract\u00e9ristique leur permet de traiter des donn\u00e9es s\u00e9quentielles comme les s\u00e9ries temporelles, la parole, le texte, les donn\u00e9es financi\u00e8res... Ils permettent d'avoir compr\u00e9hension profonde d'une s\u00e9quence et de son contexte associ\u00e9.\n\nIl existe plusieurs type de RNN tel que les GRU et les LSTM qui ont des architectures internes diff\u00e9rentes.\n\nPour impl\u00e9menter notre r\u00e9seau nous allons utiliser le LSTM.\n\n\n#### Neurone de type LSTM\n\nLe LSTM (Long Short Term Memory) est un type de neurone permettant la m\u00e9morisation des informations pass\u00e9es (Long termet et court terme)\n\nCe type de neurone trop complexe peut \u00eatre \u00e9tudi\u00e9 dans ce tutorial permet via des m\u00e9chanismes internes la m\u00e9morisation et\/ou l'oubli de certaines informations selon des crit\u00e8res particuliers. Dans le sh\u00e9ma ci-dessous, la ligne du dessus r\u00e9pr\u00e9sente la m\u00e9moire interne du neurones, les autres sont des portes pour entr\u00e9es des nouvelles donn\u00e9es ou sortir des donn\u00e9es existantes (avec des coefficients).\n\nC'est un \u00e9l\u00e9ment cl\u00e9 du Deep Leaning moderne (exemple dans le pr\u00e9dictions de s\u00e9quences \/ d\u00e9t\u00e9ction d'anomalie)\n\nPour une explication d\u00e9taill\u00e9e du dessous de la sc\u00e8ne : \nhttps:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/ et https:\/\/hackernoon.com\/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4\n\nCi dessous un exemple de traitement des informations temporelle : un RNN peut \u00eatre vu comme une s\u00e9quence de r\u00e9seaux neuronaux.\n\n![image.png](attachment:image.png)\n\nGr\u00e2ce aux librairies de TensorFlow nous allons construire un type de r\u00e9seau en stackant plusieurs couches de LSTM et toute simplicit\u00e9","42570f7c":"### Configuration\n\nNous rassemblons les principaux \u00e9l\u00e9ments de configurations du programme dans le bloc ci-dessous que nous allons expliquer le moment venu. La configuration d'un seul bloc permet de simplifier la lecture du programme en rassemblant l'ensemble des variable \u00e0 un seul endroit. Je conseille de rassembler toutes les variables permettant les configurations afin de simplifier les executions avec diff\u00e9rents param\u00e8tres comme ci-dessous:","90ccad05":"Nous voyons ci-dessus au niveau des probabilit\u00e9s de sortie que le mod\u00e8le classifie la premi\u00e8re phrase comme \u00e9tant un sarcasme et la deuxi\u00e8me ne l'\u00e9tant pas. On peut donc dire que c'est un succ\u00e8s \u00e0 ce niveau, cependant il y a des choses qui restent a am\u00e9liorer: en particulier je remarque qu'il semble y avoir un overfitting qui est difficile \u00e0 faire disparaitre. Deuxiemement j'observe que la Lemmatization et la suppression des Stopwords semblent apporter une instabilit\u00e9 du mod\u00e8le.","c06c98e1":"### Split Train \/ Test\n\nNous splittons les jeux de donn\u00e9es entre le jeu d'entrainement sur lequel le r\u00e9seau de neurone va faire l'apprentissage et le jeu de test utilis\u00e9 pour v\u00e9rifier la conformit\u00e9 des pr\u00e9dictions.","39058825":"**Structure interne d'un LSTM** \n\nla ligne du dessus est la m\u00e9moire qui se propage de cellule en cellule , les autres lignes sont des portes de m\u00e9morisation (i: input) ou d'oubli (f: forget).\nOn voit sur le sch\u00e9ma plusieur entr\u00e9es qui repr\u00e9sentent:  l'entr\u00e9e de l'information courante,  l'information du timestep pr\u00e9c\u00e9dent et la ligne de m\u00e9moire qui contient ce qu'il a appris des entr\u00e9es du pass\u00e9.\n![image.png](attachment:image.png)\n\n### Construction du R\u00e9seau\n\nIl est temps maintenant de passer \u00e0 l'\u00e9tape de construction !"}}