{"cell_type":{"14664651":"code","9f5830f3":"code","fa19886d":"code","a69caa61":"code","1d111469":"code","5a0c262e":"code","e9a09a96":"code","fb499c75":"code","b6dff2c6":"code","0e5c5dbf":"code","ab0769b6":"code","3ab432fa":"code","82da2222":"code","61301bee":"code","46eb155d":"code","996683a1":"code","27474e8a":"code","bfb9e259":"code","2c1b9ae4":"code","b800a19f":"code","3b953e83":"code","b9b04998":"code","6f525ee7":"code","03afcbd5":"code","01cb56b1":"code","b075f0f6":"code","7e2311d1":"code","7c429d8f":"code","d04070d1":"code","a3174eea":"code","c1dcab06":"code","8037fd8f":"code","27f6d0e4":"code","966bf1f6":"code","66bdbd89":"code","a05af03f":"code","0b8c8277":"code","1d91d65a":"code","196aa124":"code","81845fc8":"code","0c5ecd9c":"code","7875fd44":"code","88b35213":"code","a1712a18":"code","9e30dcc0":"code","20bba57c":"code","868ebcb6":"code","c9148554":"code","92c30c25":"code","24890da7":"code","b89b0f6f":"code","1d69aa89":"code","8ce1dd0b":"code","834ce63b":"code","2b80bb23":"code","b19cc056":"code","0cbf3d56":"code","f4180fc1":"code","f2a00044":"code","e1e2ab62":"code","471e739b":"code","6fd011fd":"code","a1da62f9":"code","2e573c8f":"code","dd92167f":"code","495bf9d9":"code","6e432e26":"code","c640343c":"code","4c034dbf":"code","61c58f9f":"code","769ed613":"code","0e25f141":"code","7f37cf9f":"code","5bae650a":"code","2fa2ac8e":"code","6f7a9ab5":"code","a2db8d43":"code","c689a929":"code","b58f4967":"code","fa584c60":"code","236aa0d1":"code","dde04669":"code","d97c64e6":"code","de97c191":"code","c9513966":"code","c411c598":"code","f15fde90":"code","6e0a9ed4":"code","fad54935":"code","311f5603":"code","1a99eebe":"code","f1e23d66":"code","be0c9d0f":"markdown","590f3856":"markdown","1cf4f6f3":"markdown","2e56a550":"markdown","eabd546c":"markdown","3c5f4c14":"markdown","1c3d88ef":"markdown","46d0dccf":"markdown","30d0d33c":"markdown","172a502c":"markdown","2417962d":"markdown","a85900c7":"markdown","1dbd8705":"markdown","4c3a0916":"markdown","a0791744":"markdown","677b1cec":"markdown","6472c78a":"markdown","e59ed78b":"markdown","788867c6":"markdown","8296c16a":"markdown","d32d4ffa":"markdown","560e40ec":"markdown","b85f5ee5":"markdown","aefbd6c8":"markdown","8978a418":"markdown","0d7696fb":"markdown","531e3c51":"markdown","c891faa0":"markdown","dd065226":"markdown","29b822fa":"markdown","d7e66485":"markdown","648d86e1":"markdown","6f174500":"markdown","6a54a7fb":"markdown","b85a6481":"markdown","62a50c9e":"markdown","e148b7cc":"markdown","130fbfe4":"markdown","10bcba8d":"markdown","cde8c5ce":"markdown","f4b7b543":"markdown","1d47ed76":"markdown","d19afc5f":"markdown","8da1a326":"markdown","36023611":"markdown","a12d3b3a":"markdown","da9ff5cb":"markdown","6d144b15":"markdown","151580f9":"markdown","03787c01":"markdown","d5e86414":"markdown","50e74c5c":"markdown","1fff1a45":"markdown","d0e9a74c":"markdown","5fbc297e":"markdown","9212643e":"markdown"},"source":{"14664651":"# Numerical example\nx = 5\nprint (x)","9f5830f3":"# Text example\nx = \"hello\"\nprint (x)","fa19886d":"# int variable\nx = 5\nprint (x)\nprint (type(x))\n\n# float variable\nx = 5.0\nprint (x)\nprint (type(x))\n\n# text variable\nx = \"5\" \nprint (x)\nprint (type(x))\n\n# boolean variable\nx = True\nprint (x)\nprint (type(x))","a69caa61":"# int variables\na = 5\nb = 3\nprint (a + b)\n\n# string variables\na = \"5\"\nb = \"3\"\nprint (a + b)","1d111469":"# Making a list\nlist_x = [3, \"hello\", 1]\nprint (list_x)\n\n# Adding to a list\nlist_x.append(7)\nprint (list_x)\n\n# Accessing items at specific location in a list\nprint (\"list_x[0]: \", list_x[0])\nprint (\"list_x[1]: \", list_x[1])\nprint (\"list_x[2]: \", list_x[2])\nprint (\"list_x[-1]: \", list_x[-1]) # the last item\nprint (\"list_x[-2]: \", list_x[-2]) # the second to last item\n\n# Slicing\nprint (\"list_x[:]: \", list_x[:])\nprint (\"list_x[2:]: \", list_x[2:])\nprint (\"list_x[1:3]: \", list_x[1:3])\nprint (\"list_x[:-1]: \", list_x[:-1])\n\n# Length of a list\nlen(list_x)\n\n# Replacing items in a list\nlist_x[1] = \"hi\"\nprint (list_x)\n\n# Combining lists\nlist_y = [2.4, \"world\"]\nlist_z = list_x + list_y\nprint (list_z)","5a0c262e":"# Creating a tuple\ntuple_x = (3.0, \"hello\")\nprint (tuple_x)\n\n# Adding values to a tuple\ntuple_x = tuple_x + (5.6,)\nprint (tuple_x)\n\n# Trying to change a tuples value (you can't)\ntuple_x[1] = \"world\"","e9a09a96":"# Creating a dictionary\narun = {\"name\": \"Arun\",\n        \"eye_color\": \"brown\"}\nprint (arun)\nprint (arun[\"name\"])\nprint (arun[\"eye_color\"])\n\n# Changing the value for a key\narun[\"eye_color\"] = \"black\"\nprint (arun)\n\n# Adding new key-value pairs\narun[\"age\"] = 24\nprint (arun)\n\n# Length of a dictionary\nprint (len(arun))","fb499c75":"# If statement\nx = 4\nif x < 1:\n    score = \"low\"\nelif x <= 4:\n    score = \"medium\"\nelse:\n    score = \"high\"\nprint (score)\n\n# If statment with a boolean\nx = True\nif x:\n    print (\"it worked\")","b6dff2c6":"# For loop\nx = 1\nfor i in range(3): # goes from i=0 to i=2\n    x += 1 # same as x = x + 1\n    print (\"i={0}, x={1}\".format(i, x)) # printing with multiple variables","0e5c5dbf":"# While loop\nx = 3\nwhile x > 0:\n    x -= 1 # same as x = x - 1\n    print (x)","ab0769b6":"# Create a function\ndef add_two(x):\n    x += 2\n    return x\n\n# Use the function\nscore = 0\nscore = add_two(x=score)\nprint (score)","3ab432fa":"# Function with multiple inputs\ndef join_name(first_name, last_name):\n    joined_name = first_name + \" \" + last_name\n    return joined_name\n\n# Use the function\nfirst_name = \"Arunkumar\"\nlast_name = \"Venkataramanan\"\njoined_name = join_name(first_name=first_name, last_name=last_name)\nprint (joined_name)","82da2222":"# Create the function\nclass Pets(object):\n  \n    # Initialize the class\n    def __init__(self, species, color, name):\n        self.species = species\n        self.color = color\n        self.name = name\n\n    # For printing  \n    def __str__(self):\n        return \"{0} {1} named {2}.\".format(self.color, self.species, self.name)\n\n    # Example function\n    def change_name(self, new_name):\n        self.name = new_name","61301bee":"# Making an instance of a class\nmy_dog = Pets(species=\"dog\", color=\"orange\", name=\"Guiness\",)\nprint (my_dog)\nprint (my_dog.name)","46eb155d":"# Using a class's function\nmy_dog.change_name(new_name=\"Johny Kutty\")\nprint (my_dog)\nprint (my_dog.name)","996683a1":"import numpy as np","27474e8a":"# Set seed for reproducability\nnp.random.seed(seed=1234)","bfb9e259":"# Scalars\nx = np.array(6) # scalar\nprint (\"x: \", x)\nprint(\"x ndim: \", x.ndim)\nprint(\"x shape:\", x.shape)\nprint(\"x size: \", x.size)\nprint (\"x dtype: \", x.dtype)","2c1b9ae4":"# 1-D Array\nx = np.array([1.3 , 2.2 , 1.7])\nprint (\"x: \", x)\nprint(\"x ndim: \", x.ndim)\nprint(\"x shape:\", x.shape)\nprint(\"x size: \", x.size)\nprint (\"x dtype: \", x.dtype) # notice the float datatype","b800a19f":"# 3-D array (matrix)\nx = np.array([[1,2,3], [4,5,6], [7,8,9]])\nprint (\"x:\\n\", x)\nprint(\"x ndim: \", x.ndim)\nprint(\"x shape:\", x.shape)\nprint(\"x size: \", x.size)\nprint (\"x dtype: \", x.dtype)","3b953e83":"# Functions\nprint (\"np.zeros((2,2)):\\n\", np.zeros((2,2)))\nprint (\"np.ones((2,2)):\\n\", np.ones((2,2)))\nprint (\"np.eye((2)):\\n\", np.eye((2)))\nprint (\"np.random.random((2,2)):\\n\", np.random.random((2,2)))","b9b04998":"# Indexing\nx = np.array([1, 2, 3])\nprint (\"x[0]: \", x[0])\nx[0] = 0\nprint (\"x: \", x)","6f525ee7":"# Slicing\nx = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\nprint (x)\nprint (\"x column 1: \", x[:, 1]) \nprint (\"x row 0: \", x[0, :]) \nprint (\"x rows 0,1,2 & cols 1,2: \\n\", x[:3, 1:3]) ","03afcbd5":"# Integer array indexing\nprint (x)\nrows_to_get = np.arange(len(x))\nprint (\"rows_to_get: \", rows_to_get)\ncols_to_get = np.array([0, 2, 1])\nprint (\"cols_to_get: \", cols_to_get)\nprint (\"indexed values: \", x[rows_to_get, cols_to_get])","01cb56b1":"# Boolean array indexing\nx = np.array([[1,2], [3, 4], [5, 6]])\nprint (\"x:\\n\", x)\nprint (\"x > 2:\\n\", x > 2)\nprint (\"x[x > 2]:\\n\", x[x > 2])","b075f0f6":"# Basic math\nx = np.array([[1,2], [3,4]], dtype=np.float64)\ny = np.array([[1,2], [3,4]], dtype=np.float64)\nprint (\"x + y:\\n\", np.add(x, y)) # or x + y\nprint (\"x - y:\\n\", np.subtract(x, y)) # or x - y\nprint (\"x * y:\\n\", np.multiply(x, y)) # or x * y","7e2311d1":"# Dot product\na = np.array([[1,2,3], [4,5,6]], dtype=np.float64) # we can specify dtype\nb = np.array([[7,8], [9,10], [11, 12]], dtype=np.float64)\nprint (a.dot(b))","7c429d8f":"# Sum across a dimension\nx = np.array([[1,2],[3,4]])\nprint (x)\nprint (\"sum all: \", np.sum(x)) # adds all elements\nprint (\"sum by col: \", np.sum(x, axis=0)) # add numbers in each column\nprint (\"sum by row: \", np.sum(x, axis=1)) # add numbers in each row","d04070d1":"# Transposing\nprint (\"x:\\n\", x)\nprint (\"x.T:\\n\", x.T)","a3174eea":"# Tile\nx = np.array([[1,2], [3,4]])\ny = np.array([5, 6])\naddent = np.tile(y, (len(x), 1))\nprint (\"addent: \\n\", addent)\nz = x + addent\nprint (\"z:\\n\", z)","c1dcab06":"# Broadcasting\nx = np.array([[1,2], [3,4]])\ny = np.array([5, 6])\nz = x + y\nprint (\"z:\\n\", z)","8037fd8f":"# Reshaping\nx = np.array([[1,2], [3,4], [5,6]])\nprint (x)\nprint (\"x.shape: \", x.shape)\ny = np.reshape(x, (2, 3))\nprint (\"y.shape: \", y.shape)\nprint (\"y: \\n\", y)","27f6d0e4":"# Removing dimensions\nx = np.array([[[1,2,1]],[[2,2,3]]])\nprint (\"x.shape: \", x.shape)\ny = np.squeeze(x, 1) # squeeze dim 1\nprint (\"y.shape: \", y.shape) \nprint (\"y: \\n\", y)","966bf1f6":"# Adding dimensions\nx = np.array([[1,2,1],[2,2,3]])\nprint (\"x.shape: \", x.shape)\ny = np.expand_dims(x, 1) # expand dim 1\nprint (\"y.shape: \", y.shape) \nprint (\"y: \\n\", y)","66bdbd89":"import pandas as pd","a05af03f":"# Read from CSV to Pandas DataFrame\ndf = pd.read_csv(\"..\/input\/train.csv\", header=0)","0b8c8277":"# First five items\ndf.head()","1d91d65a":"# Describe features\ndf.describe()","196aa124":"# Histograms\ndf[\"Age\"].hist()","81845fc8":"# Unique values\ndf[\"Embarked\"].unique()","0c5ecd9c":"# Selecting data by feature\ndf[\"Name\"].head()","7875fd44":"# Filtering\ndf[df[\"Sex\"]==\"female\"].head() # only the female data appear","88b35213":"# Sorting\ndf.sort_values(\"Age\", ascending=False).head()","a1712a18":"# Grouping\nsex_group = df.groupby(\"Survived\")\nsex_group.mean()","9e30dcc0":"# Selecting row\ndf.iloc[0, :] # iloc gets rows (or columns) at particular positions in the index (so it only takes integers)","20bba57c":"# Selecting specific value\ndf.iloc[0, 1]","868ebcb6":"# Selecting by index\ndf.loc[0] # loc gets rows (or columns) with particular labels from the index","c9148554":"# Rows with at least one NaN value\ndf[pd.isnull(df).any(axis=1)].head()","92c30c25":"# Drop rows with Nan values\ndf = df.dropna() # removes rows with any NaN values\ndf = df.reset_index() # reset's row indexes in case any rows were dropped\ndf.head()","24890da7":"# Dropping multiple rows\ndf = df.drop([\"Name\", \"Cabin\", \"Ticket\"], axis=1) # we won't use text features for our initial basic models\ndf.head()","b89b0f6f":"# Map feature values\ndf['Sex'] = df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ndf[\"Embarked\"] = df['Embarked'].dropna().map( {'S':0, 'C':1, 'Q':2} ).astype(int)\ndf.head()","1d69aa89":"# Lambda expressions to create new features\ndef get_family_size(sibsp, parch):\n    family_size = sibsp + parch\n    return family_size\n\ndf[\"Family_Size\"] = df[[\"SibSp\", \"Parch\"]].apply(lambda x: get_family_size(x[\"SibSp\"], x[\"Parch\"]), axis=1)\ndf.head()","8ce1dd0b":"# Reorganize headers\ndf = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Family_Size', 'Fare', 'Embarked', 'Survived']]\ndf.head()","834ce63b":"# Saving dataframe to CSV\ndf.to_csv(\"processed_titanic.csv\", index=False)","2b80bb23":"# See your saved file\n!ls -l","b19cc056":"from argparse import Namespace\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","0cbf3d56":"# Arguments\nargs = Namespace(\n    seed=1234,\n    data_file=\"sample_data.csv\",\n    num_samples=100,\n    train_size=0.75,\n    test_size=0.25,\n    num_epochs=100,\n)\n\n# Set seed for reproducability\nnp.random.seed(args.seed)","f4180fc1":"# Generate synthetic data\ndef generate_data(num_samples):\n    X = np.array(range(num_samples))\n    y = 3.65*X + 10\n    return X, y","f2a00044":"# Generate random (linear) data\nX, y = generate_data(args.num_samples)\ndata = np.vstack([X, y]).T\ndf = pd.DataFrame(data, columns=['X', 'y'])\ndf.head()","e1e2ab62":"# Scatter plot\nplt.title(\"Generated data\")\nplt.scatter(x=df[\"X\"], y=df[\"y\"])\nplt.show()","471e739b":"# Import packages\nfrom sklearn.linear_model.stochastic_gradient import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","6fd011fd":"# Create data splits\nX_train, X_test, y_train, y_test = train_test_split(\n    df[\"X\"].values.reshape(-1, 1), df[\"y\"], test_size=args.test_size, \n    random_state=args.seed)\nprint (\"X_train:\", X_train.shape)\nprint (\"y_train:\", y_train.shape)\nprint (\"X_test:\", X_test.shape)\nprint (\"y_test:\", y_test.shape)","a1da62f9":"# Standardize the data (mean=0, std=1) using training data\nX_scaler = StandardScaler().fit(X_train)\ny_scaler = StandardScaler().fit(y_train.values.reshape(-1,1))\n\n# Apply scaler on training and test data\nstandardized_X_train = X_scaler.transform(X_train)\nstandardized_y_train = y_scaler.transform(y_train.values.reshape(-1,1)).ravel()\nstandardized_X_test = X_scaler.transform(X_test)\nstandardized_y_test = y_scaler.transform(y_test.values.reshape(-1,1)).ravel()\n\n\n# Check\nprint (\"mean:\", np.mean(standardized_X_train, axis=0), \n       np.mean(standardized_y_train, axis=0)) # mean should be ~0\nprint (\"std:\", np.std(standardized_X_train, axis=0), \n       np.std(standardized_y_train, axis=0))   # std should be 1","2e573c8f":"# Initialize the model\nlm = SGDRegressor(loss=\"squared_loss\", penalty=\"none\", max_iter=args.num_epochs)","dd92167f":"# Train\nlm.fit(X=standardized_X_train, y=standardized_y_train)","495bf9d9":"# Predictions (unstandardize them)\npred_train = (lm.predict(standardized_X_train) * np.sqrt(y_scaler.var_)) + y_scaler.mean_\npred_test = (lm.predict(standardized_X_test) * np.sqrt(y_scaler.var_)) + y_scaler.mean_","6e432e26":"import matplotlib.pyplot as plt","c640343c":"# Train and test MSE\ntrain_mse = np.mean((y_train - pred_train) ** 2)\ntest_mse = np.mean((y_test - pred_test) ** 2)\nprint (\"train_MSE: {0:.2f}, test_MSE: {1:.2f}\".format(train_mse, test_mse))","4c034dbf":"# Figure size\nplt.figure(figsize=(15,5))\n\n# Plot train data\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplt.scatter(X_train, y_train, label=\"y_train\")\nplt.plot(X_train, pred_train, color=\"red\", linewidth=1, linestyle=\"-\", label=\"lm\")\nplt.legend(loc='lower right')\n\n# Plot test data\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplt.scatter(X_test, y_test, label=\"y_test\")\nplt.plot(X_test, pred_test, color=\"red\", linewidth=1, linestyle=\"-\", label=\"lm\")\nplt.legend(loc='lower right')\n\n# Show plots\nplt.show()","61c58f9f":"# Feed in your own inputs\nX_infer = np.array((0, 1, 2), dtype=np.float32)\nstandardized_X_infer = X_scaler.transform(X_infer.reshape(-1, 1))\npred_infer = (lm.predict(standardized_X_infer) * np.sqrt(y_scaler.var_)) + y_scaler.mean_\nprint (pred_infer)\ndf.head(3)","769ed613":"# Unstandardize coefficients \ncoef = lm.coef_ * (y_scaler.scale_\/X_scaler.scale_)\nintercept = lm.intercept_ * y_scaler.scale_ + y_scaler.mean_ - np.sum(coef*X_scaler.mean_)\nprint (coef) # ~3.65\nprint (intercept) # ~10","0e25f141":"# Initialize the model with L2 regularization\nlm = SGDRegressor(loss=\"squared_loss\", penalty='l2', alpha=1e-2, \n                  max_iter=args.num_epochs)","7f37cf9f":"# Train\nlm.fit(X=standardized_X_train, y=standardized_y_train)","5bae650a":"# Predictions (unstandardize them)\npred_train = (lm.predict(standardized_X_train) * np.sqrt(y_scaler.var_)) + y_scaler.mean_\npred_test = (lm.predict(standardized_X_test) * np.sqrt(y_scaler.var_)) + y_scaler.mean_","2fa2ac8e":"# Train and test MSE\ntrain_mse = np.mean((y_train - pred_train) ** 2)\ntest_mse = np.mean((y_test - pred_test) ** 2)\nprint (\"train_MSE: {0:.2f}, test_MSE: {1:.2f}\".format(\n    train_mse, test_mse))","6f7a9ab5":"# Unstandardize coefficients \ncoef = lm.coef_ * (y_scaler.scale_\/X_scaler.scale_)\nintercept = lm.intercept_ * y_scaler.scale_ + y_scaler.mean_ - (coef*X_scaler.mean_)\nprint (coef) # ~3.65\nprint (intercept) # ~10","a2db8d43":"# Create data with categorical features\ncat_data = pd.DataFrame(['a', 'b', 'c', 'a'], columns=['favorite_letter'])\ncat_data.head()","c689a929":"dummy_cat_data = pd.get_dummies(cat_data)\ndummy_cat_data.head()","b58f4967":"from argparse import Namespace\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport urllib","fa584c60":"# Arguments\nargs = Namespace(\n    seed=1234,\n    data_file=\"titanic.csv\",\n    train_size=0.75,\n    test_size=0.25,\n    num_epochs=100,\n)\n\n# Set seed for reproducability\nnp.random.seed(args.seed)","236aa0d1":"# Upload data from GitHub to notebook's local drive\nurl = \"https:\/\/raw.githubusercontent.com\/ArunkumarRamanan\/practicalAI\/master\/data\/titanic.csv\"\nresponse = urllib.request.urlopen(url)\nhtml = response.read()\nwith open(args.data_file, 'wb') as f:\n    f.write(html)","dde04669":"# Read from CSV to Pandas DataFrame\ndf = pd.read_csv(args.data_file, header=0)\ndf.head()","d97c64e6":"# Import packages\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","de97c191":"# Preprocessing\ndef preprocess(df):\n  \n    # Drop rows with NaN values\n    df = df.dropna()\n\n    # Drop text based features (we'll learn how to use them in later lessons)\n    features_to_drop = [\"name\", \"cabin\", \"ticket\"]\n    df = df.drop(features_to_drop, axis=1)\n\n    # pclass, sex, and embarked are categorical features\n    categorical_features = [\"pclass\",\"embarked\",\"sex\"]\n    df = pd.get_dummies(df, columns=categorical_features)\n\n    return df","c9513966":"# Preprocess the dataset\ndf = preprocess(df)\ndf.head()","c411c598":"# Split the data\nmask = np.random.rand(len(df)) < args.train_size\ntrain_df = df[mask]\ntest_df = df[~mask]\nprint (\"Train size: {0}, test size: {1}\".format(len(train_df), len(test_df)))","f15fde90":"# Separate X and y\nX_train = train_df.drop([\"survived\"], axis=1)\ny_train = train_df[\"survived\"]\nX_test = test_df.drop([\"survived\"], axis=1)\ny_test = test_df[\"survived\"]","6e0a9ed4":"# Standardize the data (mean=0, std=1) using training data\nX_scaler = StandardScaler().fit(X_train)\n\n# Apply scaler on training and test data (don't standardize outputs for classification)\nstandardized_X_train = X_scaler.transform(X_train)\nstandardized_X_test = X_scaler.transform(X_test)\n\n# Check\nprint (\"mean:\", np.mean(standardized_X_train, axis=0)) # mean should be ~0\nprint (\"std:\", np.std(standardized_X_train, axis=0))   # std should be 1","fad54935":"# Initialize the model\nlog_reg = SGDClassifier(loss=\"log\", penalty=\"none\", max_iter=args.num_epochs, \n                        random_state=args.seed)","311f5603":"# Train\nlog_reg.fit(X=standardized_X_train, y=y_train)","1a99eebe":"# Probabilities\npred_test = log_reg.predict_proba(standardized_X_test)\nprint (pred_test[:5])","f1e23d66":"# Predictions (unstandardize them)\npred_train = log_reg.predict(standardized_X_train) \npred_test = log_reg.predict(standardized_X_test)\nprint (pred_test)","be0c9d0f":"### Proof for unstandardizing coefficients:\n\n\nNote that both X and y were standardized.\n\n$\\frac{\\mathbb{E}[y] - \\hat{y}}{\\sigma_y} = W_0 + \\sum_{j=1}^{k}W_jz_j$\n\n$z_j = \\frac{x_j - \\bar{x}_j}{\\sigma_j}$\n\n$ \\hat{y}_{scaled} = \\frac{\\hat{y}_{unscaled} - \\bar{y}}{\\sigma_y} = \\hat{W_0} + \\sum_{j=1}^{k} \\hat{W}_j (\\frac{x_j - \\bar{x}_j}{\\sigma_j}) $\n\n$\\hat{y}_{unscaled} = \\hat{W}_0\\sigma_y + \\bar{y} - \\sum_{j=1}^{k} \\hat{W}_j(\\frac{\\sigma_y}{\\sigma_j})\\bar{x}_j + \\sum_{j=1}^{k}(\\frac{\\sigma_y}{\\sigma_j})x_j $\n","590f3856":" <a id=\"1\"><\/a> <br>\n## 1. Basics    \n\n<a id=\"1.1\"><\/a> <br>\n## 1.1 Introduction to Python\nIn this lesson we will learn the basics of the Python programming language (version 3). We won't learn everything about Python but enough to do some basic machine learning.\n\n<img src=\"https:\/\/www.python.org\/static\/community_logos\/python-logo-master-v3-TM.png\" width=350>","1cf4f6f3":"### Exploratory Dats Analysis EDA\n\nWe're going to explore the Pandas library and see how we can explore and process our data.","2e56a550":"### Loading the data\n\nNow that we have some data to play with, let's load into a Pandas dataframe. Pandas is a great python library for data analysis.","eabd546c":"* $ J(\\theta) = = \\frac{1}{2}\\sum_{i}(X_iW - y_i)^2 + \\frac{\\lambda}{2}\\sum\\sum W^2$\n* $ \\frac{\\partial{J}}{\\partial{W}}  = X (\\hat{y} - y) + \\lambda W $\n* $W = W- \\alpha\\frac{\\partial{J}}{\\partial{W}}$\nwhere:\n  * $\\lambda$ is the regularzation coefficient","3c5f4c14":"### Interpretability\n\nLinear regression offers the great advantage of being highly interpretable. Each feature has a coefficient which signifies it's importance\/impact on the output variable y. We can interpret our coefficient as follows: By increasing X by 1 unit, we increase y by $W$ (~3.65) units. \n\n**Note**: Since we standardized our inputs and outputs for gradient descent, we need to apply an operation to our coefficients and intercept to interpret them. See proof below.","1c3d88ef":"In this notebook, we'll learn the basics of data analysis with the Python Pandas library.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/ArunkumarRamanan\/practicalAI\/master\/images\/pandas.png\" width=500>\n","46d0dccf":"These are the diferent features: \n* pclass: class of travel\n* name: full name of the passenger\n* sex: gender\n* age: numerical age\n* sibsp: # of siblings\/spouse aboard\n* parch: number of parents\/child aboard\n* ticket: ticket number\n* fare: cost of the ticket\n* cabin: location of room\n* emarked: port that the passenger embarked at (C - Cherbourg, S - Southampton, Q = Queenstown)\n* survived: survial metric (0 - died, 1 - survived)","30d0d33c":"# Training\n\n*Steps*:\n\n1. Randomly initialize the model's weights $W$.\n2. Feed inputs $X$ into the model to receive the logits ($z=XW$). Apply the softmax operation on the logits to get the class probabilies $\\hat{y}$ in one-hot encoded form. For example, if there are three classes, the predicted class probabilities could look like [0.3, 0.3, 0.4]. \n3. Compare the predictions $\\hat{y}$ (ex.  [0.3, 0.3, 0.4]]) with the actual target values $y$ (ex. class 2 would look like [0, 0, 1]) with the objective (cost) function to determine loss $J$. A common objective function for logistics regression is cross-entropy loss. \n  * $J(\\theta) = - \\sum_i y_i ln (\\hat{y_i}) =  - \\sum_i y_i ln (\\frac{e^{X_iW_y}}{\\sum e^{X_iW}}) $\n   * $y$ = [0, 0, 1]\n  * $\\hat{y}$ = [0.3, 0.3, 0.4]]\n  * $J(\\theta) = - \\sum_i y_i ln (\\hat{y_i}) =  - \\sum_i y_i ln (\\frac{e^{X_iW_y}}{\\sum e^{X_iW}}) = - \\sum_i [0 * ln(0.3) + 0 * ln(0.3) + 1 * ln(0.4)] = -ln(0.4) $\n  * This simplifies our cross entropy objective to the following: $J(\\theta) = - ln(\\hat{y_i})$ (negative log likelihood).\n  * $J(\\theta) = - ln(\\hat{y_i}) = - ln (\\frac{e^{X_iW_y}}{\\sum_i e^{X_iW}}) $\n4. Calculate the gradient of loss $J(\\theta)$ w.r.t to the model weights. Let's assume that our classes are mutually exclusive (a set of inputs could only belong to one class).\n * $\\frac{\\partial{J}}{\\partial{W_j}} = \\frac{\\partial{J}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{W_j}} = - \\frac{1}{y}\\frac{\\partial{y}}{\\partial{W_j}} = - \\frac{1}{\\frac{e^{W_yX}}{\\sum e^{XW}}}\\frac{\\sum e^{XW}e^{W_yX}0 - e^{W_yX}e^{W_jX}X}{(\\sum e^{XW})^2} = \\frac{Xe^{W_jX}}{\\sum e^{XW}} = XP$\n  * $\\frac{\\partial{J}}{\\partial{W_y}} = \\frac{\\partial{J}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{W_y}} = - \\frac{1}{y}\\frac{\\partial{y}}{\\partial{W_y}} = - \\frac{1}{\\frac{e^{W_yX}}{\\sum e^{XW}}}\\frac{\\sum e^{XW}e^{W_yX}X - e^{W_yX}e^{W_yX}X}{(\\sum e^{XW})^2} = \\frac{1}{P}(XP - XP^2) = X(P-1)$\n5. Apply backpropagation to update the weights $W$ using gradient descent. The updates will penalize the probabiltiy for the incorrect classes (j) and encourage a higher probability for the correct class (y).\n  * $W_i = W_i - \\alpha\\frac{\\partial{J}}{\\partial{W_i}}$\n6. Repeat steps 2 - 4 until model performs well.","172a502c":"### Saving data","2417962d":"### If statements\nYou can use if statements to conditionally do something.","a85900c7":"### Array math","1dbd8705":"###### [Go to top](#top)","4c3a0916":"Besides MSE, when we only have one feature, we can visually inspect the model.","a0791744":"### Evaluation\n\nThere are several evaluation techniques to see how well our model performed.","677b1cec":"### Indexing","6472c78a":"<a id=\"1.3\"><\/a> <br>\n## 1.5 Logistic Regression\n\nIn the previous lesson, we saw how linear regression works really well for predicting continuous outputs that can easily fit to a line\/plane. But linear regression doesn't fare well for classification asks where we want to probabilititcally determine the outcome for a given set on inputs.","e59ed78b":"### Training\n\n*Steps*: \n1. Randomly initialize the model's weights $W$.\n2. Feed inputs $X$ into the model to receive the predictions $\\hat{y}$.\n3. Compare the predictions $\\hat{y}$ with the actual target values $y$ with the objective (cost) function to determine loss $J$. A common objective function for linear regression is mean squarred error (MSE). This function calculates the difference between the predicted and target values and squares it. (the $\\frac{1}{2}$ is just for convenicing the derivative operation).\n  * $MSE = J(\\theta) = \\frac{1}{2}\\sum_{i}(\\hat{y}_i - y_i)^2$\n4. Calculate the gradient of loss $J(\\theta)$ w.r.t to the model weights.\n  * $J(\\theta) = \\frac{1}{2}\\sum_{i}(\\hat{y}_i - y_i)^2 = \\frac{1}{2}\\sum_{i}(X_iW - y_i)^2 $\n  * $\\frac{\\partial{J}}{\\partial{W}} = X(\\hat{y} - y)$\n4. Apply backpropagation to update the weights $W$ using a learning rate $\\alpha$ and an optimization technique (ie. stochastic gradient descent). The simplified intuition is that the gradient tells you the direction for how to increase something so subtracting it will help you go the other way since we want to decrease loss $J(\\theta)$.\n  * $W = W- \\alpha\\frac{\\partial{J}}{\\partial{W}}$\n5. Repeat steps 2 - 4 until model performs well.","788867c6":"###  Variables\nVariables are objects in python that can hold anything with numbers or text. Let's look at how to make some variables.\n","8296c16a":"### Data\n\nWe're going to create some simple dummy data to apply linear regression on.","d32d4ffa":"It's good practice to know what types your variables are. When you want to use numerical operations on then, they need to be compatible. ","560e40ec":"# To Be Updated Soon\n###### [Go to top](#top)","b85f5ee5":"### Classes\nClasses are a fundamental piece of object oriented Python programming.","aefbd6c8":"### Feature Engineering","8978a418":"<a id=\"1.4\"><\/a> <br>\n## 1.4 Linear Regression\n\nIn this lesson we will learn about linear regression. We will first understand the basic math behind it and then implement it in Python. We will also look at ways of interpreting the linear model.","0d7696fb":"# Practical Machine Learning ML with PyTorch (TBU)\n\nThis kernel is empowering you to use machine learning to get valuable insights from data.\n- \ud83d\udd25 Implement basic ML algorithms and deep neural networks with <a href=\"https:\/\/pytorch.org\/\" target=\"_blank\" style=\"color:#ee4c2c\">PyTorch<\/a>.\n- \ud83d\udce6 Learn object-oriented ML to code for products, not just tutorials.\n\n> #### **Credits**: Thanks to **Practical AI - Goku Mohandas** and other contributers for such wonderful work!\n\n### Here are some of *my kernel notebooks* for **Machine Learning and Data Science** as follows, ***Upvote*** them if you *like* them\n\n> * [Awesome Deep Learning Basics and Resources](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-deep-learning-resources)\n> * [Data Science with R - Awesome Tutorials](https:\/\/www.kaggle.com\/arunkumarramanan\/data-science-with-r-awesome-tutorials)\n> * [Data Science and Machine Learning Cheetcheets](https:\/\/www.kaggle.com\/arunkumarramanan\/data-science-and-machine-learning-cheatsheets)\n> * [Awesome ML Frameworks and MNIST Classification](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-machine-learning-ml-frameworks)\n> * [Awesome Data Science for Beginners with Titanic Exploration](https:\/\/kaggle.com\/arunkumarramanan\/awesome-data-science-for-beginners)\n> * [Tensorflow Tutorial and House Price Prediction](https:\/\/www.kaggle.com\/arunkumarramanan\/tensorflow-tutorial-and-examples)\n> * [Data Scientist's Toolkits - Awesome Data Science Resources](https:\/\/www.kaggle.com\/arunkumarramanan\/data-scientist-s-toolkits-awesome-ds-resources)\n> * [Awesome Computer Vision Resources (TBU)](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-computer-vision-resources-to-be-updated)\n> * [Machine Learning and Deep Learning - Awesome Tutorials](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-deep-learning-ml-tutorials)\n> * [Data Science with Python - Awesome Tutorials](https:\/\/www.kaggle.com\/arunkumarramanan\/data-science-with-python-awesome-tutorials)\n> * [Awesome TensorFlow and PyTorch Resources](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-tensorflow-and-pytorch-resources)\n> * [Awesome Data Science IPython Notebooks](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-data-science-ipython-notebooks)\n> * [Machine Learning Engineer's Toolkit with Roadmap](https:\/\/www.kaggle.com\/arunkumarramanan\/machine-learning-engineer-s-toolkit-with-roadmap) \n> * [Hands-on ML with scikit-learn and TensorFlow](https:\/\/www.kaggle.com\/arunkumarramanan\/hands-on-ml-with-scikit-learn-and-tensorflow)\n> * [Practical Machine Learning with PyTorch](https:\/\/www.kaggle.com\/arunkumarramanan\/practical-machine-learning-with-pytorch)\n\n> ***Practical Machine Learning ML with TensorFlow***\n\n> The above highlighted work will soon be released and also be based on the below coursework;\n\n* [Google Machine Learning Crash Course](https:\/\/developers.google.com\/machine-learning\/crash-course\/) - Machine Learning ML Crash Course with TensorFlow APIs (Google's fast-paced, practical introduction to machine learning, A self-study guide for aspiring machine learning practitioners: Machine Learning Crash Course features a series of lessons with video lectures, real-world case studies, and hands-on practice exercises.) is highly recommended by Google as it's developed by googlers along with Notebooks for exercises.\n\n <a id=\"top\"><\/a> <br>\n \n## Kernel Notebook Content\n \n## [Basics](#1)                  \n- \ud83d\udc0d [Python](#1)                                                                           \n- \ud83d\udd22 [NumPy](#1)                                                                                      \n- \ud83d\udc3c[Pandas](#1)                            \n- \ud83d\udcc8 [Linear Regression](#1) \n- \ud83d\udcca [Logistic Regression](#1)\n- \ud83c\udf33 [Random Forests](#1)\n- \ud83d\udca5 KMeans Clustering\n \n## [Deep Learning](#2)\n- \ud83d\udd25 [PyTorch](#2)\n- \ud83c\udf9b\ufe0f [Multilayer Perceptrons](#2)\n- \ud83d\udd0e [Data & Models](#2)\n- \ud83d\udce6 [Object-Oriented ML](#2)\n- \ud83d\uddbc\ufe0f [Convolutional Neural Networks](#2)\n- \ud83d\udcdd [Embeddings](#2)\n- \ud83d\udcd7 [Recurrent Neural Networks](#2)\n\n## [Advanced](#3)\n- \ud83d\udcda [Advanced RNNs](#3)\n- \ud83c\udfce\ufe0f Highway and Residual Networks\n- \ud83d\udd2e Autoencoders\n- \ud83c\udfad Generative Adversarial Networks\n- \ud83d\udc1d Spatial Transformer Networks\n\n## [Topics](#4)\n- \ud83d\udcf8 [Computer Vision](#4)\n- \u23f0 Time Series Analysis\n- \ud83c\udfd8\ufe0f Topic Modeling\n- \ud83d\uded2 Recommendation Systems\n- \ud83d\udde3\ufe0f Pretrained Language Modeling\n- \ud83e\udd37 Multitask Learning\n- \ud83c\udfaf Low Shot Learning|\n- \ud83c\udf52 Reinforcement Learning|\n\n","531e3c51":"Now you can concat this with your continuous features and train the linear model.\n\n### TODO\n\n- polynomial regression\n- simple example with normal equation method (sklearn.linear_model.LinearRegression) with pros and cons vs. SGD linear regression\n\n###### [Go to top](#top)","c891faa0":"<img src=\"https:\/\/raw.githubusercontent.com\/ArunkumarRamanan\/practicalAI\/master\/images\/logistic.jpg\" width=270>\n\n$ \\hat{y} = \\frac{1}{1 + e^{-XW}} $ \n\n*where*:\n* $\\hat{y}$ = prediction | $\\in \\mathbb{R}^{NX1}$ ($N$ is the number of samples)\n* $X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n* $W$ = weights | $\\in \\mathbb{R}^{DX1}$ \n\nThis is the binomial logistic regression. The main idea is to take the outputs from the linear equation ($z=XW$) and use the sigmoid (logistic) function ($\\frac{1}{1+e^{-z}}$) to restrict the value between (0, 1). ","dd065226":"<a id=\"1.3\"><\/a> <br>\n## 1.3 Pandas","29b822fa":"# Data\n\nWe're going to the load the titanic dataset we looked at in lesson 03_Pandas.","d7e66485":"* **Objective:**  Predict the probability of class $y$ given the inputs $X$. The softmax classifier normalizes the linear outputs to determine class probabilities. \n* **Advantages:**\n  * Can predict class probabilities given a set on inputs.\n* **Disadvantages:**\n  * Sensitive to outliers since objective is minimize cross entropy loss. (Support vector machines ([SVMs](https:\/\/towardsdatascience.com\/support-vector-machine-vs-logistic-regression-94cc2975433f)) are a good alternative to counter outliers).\n* **Miscellaneous:** Softmax classifier is going to used widely in neural network architectures as the last layer since it produces class probabilities.","648d86e1":"###  Lists\nLists are objects in python that can hold a ordered sequence of numbers **and** text.","6f174500":"### Scikit-learn Implementation\n\n**Note**: The `LinearRegression` class in Scikit-learn uses the normal equation to solve the fit. However, we are going to use Scikit-learn's `SGDRegressor` class which uses stochastic gradient descent. We want to use this optimization approach because we will be using this for the models in subsequent lessons.","6a54a7fb":"Regularization didn't help much with this specific example because our data is generation from a perfect linear equation but for realistic data, regularization can help our model generalize well.","b85a6481":"### Categorical variables\n\nIn our example, the feature was a continuous variable but what if we also have features that are categorical? One option is to treat the categorical variables as one-hot encoded variables. This is very easy to do with Pandas and once you create the dummy variables, you can use the same steps as above to train your linear model.","62a50c9e":"### Additional resources\nThis was a very quick look at python and we'll be learning more in future lessons. If you want to learn more right now before diving into machine learning, check out this free course: [Free Python Course](https:\/\/www.codecademy.com\/learn\/learn-python) and [Kaggle Learn](https:\/\/www.kaggle.com\/learn\/python)\n\n###### [Go to top](#top)","e148b7cc":"### Data Preprocessing","130fbfe4":"When we have more than two classes, we need to use multinomial logistic regression (softmax classifier). The softmax classifier will use the linear equation ($z=XW$) and normalize it to product the probabiltiy for class y given the inputs.\n\n$ \\hat{y} = \\frac{e^{XW_y}}{\\sum e^{XW}} $ \n\n*where*:\n* $\\hat{y}$ = prediction | $\\in \\mathbb{R}^{NX1}$ ($N$ is the number of samples)\n* $X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n* $W$ = weights | $\\in \\mathbb{R}^{DXC}$ ($C$ is the number of classes)\n","10bcba8d":"### Dictionaries\nDictionaries are python objects that hold key-value pairs. In the example dictionary below, the keys are the \"name\" and \"eye_color\" variables. They each have a value associated with them. A dictionary cannot have two of the same keys. ","cde8c5ce":"## Credits (Reference)\n\n> - [practicalAI - Goku Mohandas](https:\/\/github.com\/GokuMohandas\/practicalAI\/)\n> - [GitHub Awesome Lists Topic](https:\/\/github.com\/topics\/awesome)\n> - [GitHub Machine Learning Topic](https:\/\/github.com\/topics\/machine-learning)\n> - [GitHub Deep Learning Topic](https:\/\/github.com\/topics\/deep-learning)\n> - [GitHub Awesome Lists Topic](https:\/\/github.com\/topics\/awesome)\n\n## License\n\n[![MIT](https:\/\/img.shields.io\/badge\/license-MIT-brightgreen.svg)](https:\/\/raw.githubusercontent.com\/ArunkumarRamanan\/practicalAI\/master\/LICENSE)\n\n### Please ***UPVOTE*** my kernel if you like it or wanna fork it.\n\n##### Feedback: If you have any ideas or you want any other content to be added to this curated list, please feel free to make any comments to make it better.\n#### I am open to have your *feedback* for improving this ***kernel***\n###### Hope you enjoyed this kernel!\n\n### Thanks for visiting my *Kernel* and please *UPVOTE* to stay connected and follow up the *further updates!*","f4b7b543":"We need to standardize our data (zero mean and unit variance) in order to properly use SGD and optimize quickly.","1d47ed76":"### Advanced","d19afc5f":"### Loops\nYou can use for or while loops in python to do something repeatedly until a condition is met.","8da1a326":"# Scikit-learn implementation\n\n**Note**: The `LogisticRegression` class in Scikit-learn uses coordinate descent to solve the fit. However, we are going to use Scikit-learn's `SGDClassifier` class which uses stochastic gradient descent. We want to use this optimization approach because we will be using this for the models in subsequent lessons.","36023611":"### Functions\nFunctions are a way to modularize reusable pieces of code. ","a12d3b3a":"### Overview\n\n<img src=\"https:\/\/raw.githubusercontent.com\/GokuMohandas\/practicalAI\/master\/images\/linear.png\" width=250>\n\n$\\hat{y} = XW$\n\n*where*:\n* $\\hat{y}$ = prediction | $\\in \\mathbb{R}^{NX1}$ ($N$ is the number of samples)\n* $X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n* $W$ = weights | $\\in \\mathbb{R}^{DX1}$ ","da9ff5cb":"### NumPy basics","6d144b15":"<a id=\"1.2\"><\/a> <br>\n## 1.2 NumPy\n\nIn this lesson we will learn the basics of numerical analysis using the NumPy package.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1a\/NumPy_logo.svg\" width=300>\n\n","151580f9":"### Inference","03787c01":"### Tuples\nTuples are also objects in python that can hold data but you cannot replace values (for this reason, tuples are called immutable, whereas lists are known as mutable).","d5e86414":"### Regularization\n\nRegularization helps decrease over fitting. Below is L2 regularization (ridge regression). There are many forms of regularization but they all work to reduce overfitting in our models. With L2 regularization, we are penalizing the weights with large magnitudes by decaying them. Having certain weights with high magnitudes will lead to preferential bias with the inputs and we want the model to work with all the inputs and not just a select few. There are also other types of regularization like L1 (lasso regression) which is useful for creating sparse models where some feature cofficients are zeroed out, or elastic which combines L1 and L2 penalties. \n\n**Note**: Regularization is not just for linear regression. You can use it to regualr any model's weights including the ones we will look at in future lessons.","50e74c5c":"<img src=\"https:\/\/blog.studygate.com\/wp-content\/uploads\/2017\/11\/Matrix-Multiplication-dot-product.png\" width=400>\n","1fff1a45":"### Uploading the data\n\nWe're first going to get some data to play with. We're going to load the titanic dataset from the public link below.","d0e9a74c":"### Additional resources\n\nYou don't to memorize anything here and we will be taking a closer look at NumPy in the later lessons. If you are curious about more checkout the [NumPy reference manual](https:\/\/docs.scipy.org\/doc\/numpy-1.15.1\/reference\/).\n\n###### [Go to top](#top)","5fbc297e":"* **Objective:**  Use inputs $X$ to predict the output $\\hat{y}$ using a linear model. The model will be a line of best fit that minimizes the distance between the predicted and target outcomes. Training data $(X, y)$ is used to train the model and learn the weights $W$ using stochastic gradient descent (SGD).\n* **Advantages:**\n  * Computationally simple.\n  * Highly interpretable.\n  * Can account for continuous and categorical features.\n* **Disadvantages:**\n  * The model will perform well only when the data is linearly separable (for classification).\n  * Usually not used for classification and only for regression.\n* **Miscellaneous:** You can also use linear regression for binary classification tasks where if the predicted continuous value is above a threshold, it belongs to a certain class. But we will cover better techniques for classification in future lessons and will focus on linear regression for continuos regression tasks only.\n","9212643e":"**Note**: If you have preprocessing steps like standardization, etc. that are calculated, you need to separate the training and test set first before spplying those operations. This is because we cannot apply any knowledge gained from the test set accidentally during preprocessing\/training."}}