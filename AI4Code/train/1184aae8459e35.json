{"cell_type":{"5230f2bc":"code","ab246bdd":"code","82e0deb5":"code","7af7964a":"code","ea21e24b":"code","8e12fef5":"code","5c5e0716":"code","9a039fda":"code","0107f1a3":"code","59fe85a9":"code","2a79ab57":"code","99533763":"code","fe3dd731":"code","8d2a9a08":"code","50dfe656":"code","76851a21":"code","c1ba64d4":"code","ef77c171":"code","0c815da9":"code","61894d10":"code","33e8403b":"code","16152c1e":"code","8580f66b":"code","078e59bd":"code","7c3e0c1f":"code","348becb4":"code","ab4553fe":"code","788bc5fd":"code","1c4dd97d":"code","5a2b291c":"code","0af5da51":"code","34a79f60":"code","924a6db4":"code","05818df5":"code","fb16b83a":"code","24f91e67":"code","99c813f9":"code","955c5894":"code","c26d9a15":"code","aec93663":"code","99db452d":"code","90b72a70":"code","53681e69":"code","2d08c2f8":"code","2268b7c5":"code","b9f4bce9":"code","37b45b14":"markdown","1653c8fb":"markdown","58bac23f":"markdown","10895c33":"markdown","f52bc8c1":"markdown","405bbae9":"markdown","18a8d955":"markdown","bf63ff5f":"markdown","8bfed840":"markdown","20dde9bf":"markdown","64cd09c5":"markdown","b7eabc6c":"markdown","3cfdf0f0":"markdown","f4ea5187":"markdown","86f48ae3":"markdown"},"source":{"5230f2bc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport json\nimport math\nimport cv2\nimport PIL\nfrom PIL import Image\nimport seaborn as sns\nsns.set(style='darkgrid')\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nfrom keras import layers\nfrom keras.applications import ResNet50,MobileNet, DenseNet201, InceptionV3, NASNetLarge, InceptionResNetV2, NASNetMobile\nfrom keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nimport scipy\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom keras import backend as K\nimport gc\nfrom functools import partial\nfrom sklearn import metrics\nfrom collections import Counter\nimport json\nimport itertools\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nfrom sklearn.decomposition import PCA\n\n%matplotlib inline","ab246bdd":"'''\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n'''","82e0deb5":"sub = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')","7af7964a":"import os\nprint(os.listdir(\"..\/input\/siim-isic-melanoma-classification\"))","ea21e24b":"#Loading Train and Test Data\ntrain = pd.read_csv(\"..\/input\/siim-isic-melanoma-classification\/train.csv\")\ntest = pd.read_csv(\"..\/input\/siim-isic-melanoma-classification\/test.csv\")\nprint(\"{} images in train set.\".format(train.shape[0]))\nprint(\"{} images in test set.\".format(test.shape[0]))","8e12fef5":"test.head()","5c5e0716":"train.head()","9a039fda":"import plotly.express as px\n# Plotly for the interactive viewer (see last section)\nimport plotly.graph_objs as go\nimport plotly.graph_objects as go\n\nplt.figure(figsize=(15,15))\n\nlabels=train['anatom_site_general_challenge'].value_counts().index\nvalues=train['anatom_site_general_challenge'].value_counts().values\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n                             insidetextorientation='radial' )])\n#fig = px.sunburst(train, path = [labels,'sex','benign_malignant'], values = values)\nfig.show()","0107f1a3":"np.mean(train.target)","59fe85a9":"plt.figure(figsize=(10,5))\nsns.countplot(x='target', data=train,\n                   order=list(train['target'].value_counts().sort_index().index) ,\n                   color='cyan')","2a79ab57":"train['target'].value_counts()","99533763":"train.columns","fe3dd731":"z=train.groupby(['target','sex'])['benign_malignant'].count().to_frame().reset_index()\nz.style.background_gradient(cmap='Reds')  ","8d2a9a08":"sns.catplot(x='target',y='benign_malignant', hue='sex',data=z,kind='bar')","50dfe656":"images= train['image_name'].values\n\n#extract 9 random images\nimport random\nrandom_images = [np.random.choice(images + '.jpg') for i in range(9)]\n\n#location of image dir \nimage_dir = '..\/input\/siim-isic-melanoma-classification\/jpeg\/train'\n\nprint('Display random images')\n\n#iterate and plot images\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    img = plt.imread(os.path.join(image_dir, random_images[i]))\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    \n# Adjust subplot parameters to give specified padding\nplt.tight_layout()   ","76851a21":"benign = train[train['benign_malignant']=='benign']\nmalignant = train[train['benign_malignant']=='malignant']","c1ba64d4":"images= benign['image_name'].values\n\n#extract 9 random images\nimport random\nrandom_images = [np.random.choice(images + '.jpg') for i in range(9)]\n\n#location of image dir \nimage_dir = '..\/input\/siim-isic-melanoma-classification\/jpeg\/train'\n\nprint('Display Benign images')\n\n#iterate and plot images\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    img = plt.imread(os.path.join(image_dir, random_images[i]))\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    \n# Adjust subplot parameters to give specified padding\nplt.tight_layout()   ","ef77c171":"images= malignant['image_name'].values\n\n#extract 9 random images\nimport random\nrandom_images = [np.random.choice(images + '.jpg') for i in range(9)]\n\n#location of image dir \nimage_dir = '..\/input\/siim-isic-melanoma-classification\/jpeg\/train'\n\nprint('Display Malignant images')\n\n#iterate and plot images\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    img = plt.imread(os.path.join(image_dir, random_images[i]))\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    \n# Adjust subplot parameters to give specified padding\nplt.tight_layout() ","0c815da9":"folder = '..\/input\/siim-isic-melanoma-classification\/jpeg\/train'\nplt.figure(figsize=(20,20))\n\nfor i in range(5):\n    file = random.choice(os.listdir(folder))\n    image_path= os.path.join(folder, file)\n    main_img = plt.imread(image_path)\n    img = cv2.cvtColor(main_img, cv2.COLOR_BGR2RGB) \n\n    #segmentation using k-means clustering\n    vectorized = img.reshape((-1,3))\n    vectorized = np.float32(vectorized)\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n    K = 8\n    attempts=10\n    ret,label,center=cv2.kmeans(vectorized,K,None,criteria,attempts,cv2.KMEANS_PP_CENTERS)\n    center = np.uint8(center)\n    res = center[label.flatten()]\n    seg_img = res.reshape((img.shape))\n\n    ax=plt.subplot(1,5,i+1)\n    ax.title.set_text(file)\n    plt.imshow(seg_img, cmap=\"Greys_r\")","61894d10":"from keras.models import Sequential\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Convolution2D,Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.optimizers import SGD\nfrom keras.callbacks import TensorBoard\nfrom keras import applications","33e8403b":"import time ","16152c1e":"start=time.time()\ntrain_images = np.load('..\/input\/siimisic-melanoma-resized-images\/x_train_96.npy')\nend=time.time()\nprint(f\"\\nTime to load train images: {round(end-start,5)} seconds.\")\nprint('Train_images shape: ',train_images.shape)","8580f66b":"start=time.time()\ntest_images = np.load('..\/input\/siimisic-melanoma-resized-images\/x_test_96.npy')\nend=time.time()\nprint(f\"\\nTime to load test images: {round(end-start,5)} seconds.\")\nprint('Test_images shape: ',test_images.shape)","078e59bd":"#target data\ntrain_labels =np.array(train.drop(['image_name', 'patient_id', 'sex', 'age_approx',\n       'anatom_site_general_challenge', 'diagnosis','benign_malignant'],axis=1))\nprint('Train_labels shape: ',train_labels.shape)","7c3e0c1f":"#spliting train data\nfrom sklearn.model_selection import train_test_split\nx_train,x_val,y_train,y_val=train_test_split(train_images,train_labels,test_size=0.3)","348becb4":"print('x_train shape: ',x_train.shape)\nprint('x_val shape: ',x_val.shape)","ab4553fe":"augs = ImageDataGenerator(\n        featurewise_center=True,\n        featurewise_std_normalization=True,\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        horizontal_flip=True)\n\naugs.fit(x_train)","788bc5fd":"#annealer = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)","1c4dd97d":"#strategy = tf.distribute.get_strategy()","5a2b291c":"#VGG-16 MODEL NO. 1\n'''\nfrom keras.applications.vgg16 import VGG16\n\ninput_shape=(96,96,3)\nnum_classes=1\ntmodel_base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n    \nmodel = Sequential()\nmodel.add(tmodel_base)\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.50))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(num_classes, activation='sigmoid', name='output_layer'))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n'''","0af5da51":"#XCEPTION MODEL NO. 2\n'''\nfrom keras.layers import Dropout, DepthwiseConv2D, MaxPooling2D, concatenate\nfrom keras.models import Model\n\ninp = Input(shape = (96,96, 3))\nx = inp\nx = Conv2D(32, (3, 3), strides = 2, padding = \"same\", activation = \"relu\")(x)\nx = BatchNormalization(axis = 3)(x)\nx = Dropout(0.4)(x)\nx = Conv2D(64, (3, 3), strides = 1, padding = \"same\", activation = \"relu\")(x)\nx = BatchNormalization(axis = 3)(x)\nx = Dropout(0.4)(x)\n\nx1 = DepthwiseConv2D((3, 3), (1, 1), padding = \"same\", activation = \"relu\")(x)\nx = BatchNormalization(axis = 3)(x)\nx = Dropout(0.4)(x)\nx1 = DepthwiseConv2D((3, 3), (1, 1), padding = \"same\", activation = \"relu\")(x1)\nx = BatchNormalization(axis = 3)(x)\nx = Dropout(0.4)(x)\nx1 = MaxPooling2D((2, 2), strides = 1)(x1)\n\nx = concatenate([x1, Conv2D(64, (2, 2), strides = 1)(x)])\n\nx1 = Activation(\"relu\")(x)\nx1 = Conv2D(256, (3, 3), strides = 1, padding = \"same\", activation = \"relu\")(x1)\nx = BatchNormalization(axis = 3)(x)\nx = Dropout(0.4)(x)\nx1 = DepthwiseConv2D((3, 3), strides = 1, padding = \"same\", activation = \"relu\")(x1)\nx = BatchNormalization(axis = 3)(x)\nx = Dropout(0.4)(x)\nx1 = DepthwiseConv2D((3, 3), strides = 1, padding = \"same\")(x1)\nx = BatchNormalization(axis = 3)(x)\nx = Dropout(0.4)(x)\nx1 = MaxPooling2D((2, 2), strides = 1)(x1)\n\nx = concatenate([x1, Conv2D(256, (2, 2), strides = 1)(x)])\n\n\nx = Activation(\"relu\")(x)\nx = Conv2D(256, (3, 3), strides = 1, padding = \"same\", activation = \"relu\")(x)\nx = BatchNormalization(axis = 3)(x)\nx = Dropout(0.4)(x)\nx = Conv2D(128, (3, 3), strides = 1, padding = \"same\", activation = \"relu\")(x)\nx = BatchNormalization(axis = 3)(x)\nx = Dropout(0.4)(x)\nx = Flatten()(x)\n\nx = Dense(1, activation = \"sigmoid\")(x)\n\n\nmodel2 = Model(inp, x)\nmodel2.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\nmodel2.summary()\n'''","34a79f60":"#DENSENET MODEL NO. 3\n'''\nfrom tensorflow.keras.applications import DenseNet201\nimport tensorflow.keras.layers as L\n\nwith strategy.scope():\n    dnet201 = DenseNet201(\n        input_shape=(96,96, 3),\n        weights='imagenet',\n        include_top=False\n    )\n    dnet201.trainable = True\n\n    model3 = tf.keras.Sequential([\n        dnet201,\n        L.GlobalAveragePooling2D(),\n        L.Dense(1, activation='sigmoid')\n    ])\n    model3.compile(\n        optimizer='adam',\n        loss = 'binary_crossentropy',\n        metrics=['accuracy']\n    )\n\nmodel3.summary()","924a6db4":"'''\nbatch_size=128\nepochs=30\n\nhistory = model.fit(x_train,\n             y_train,\n             batch_size=batch_size,\n             nb_epoch=epochs,\n             verbose=1,\n             validation_data=(x_val,y_val))","05818df5":"'''\nbatch_size=128\nepochs=15\n\nhistory3 = model2.fit(x_train,\n             y_train,\n             batch_size=batch_size,\n             nb_epoch=epochs,\n             verbose=1,\n             validation_data=(x_val,y_val))","fb16b83a":"'''\nbatch_size=128\nepochs=30\n\nhistory3 = model3.fit(x_train,\n             y_train, \n             batch_size=batch_size,\n             nb_epoch=epochs,\n             verbose=1,\n             validation_data=(x_val,y_val))","24f91e67":"'''\nmodel.save(\"vgg16.h5\")","99c813f9":"'''\nmodel2.save(\"xception.h5\")","955c5894":"'''\nmodel3.save(\"densenet.h5\") ","c26d9a15":"model = load(vgg116.h5)","aec93663":"scores = model.evaluate(x_val, y_val, verbose=0)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","99db452d":"scores = model2.evaluate(x_val, y_val, verbose=0)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","90b72a70":"scores = model3.evaluate(x_val, y_val, verbose=0)\nprint('Test loss_3:', scores[0])\nprint('Test accuracy_3:', scores[1])","53681e69":"y_test_prob = model.predict(test_images)\npred_df = pd.DataFrame({'image_name': test['image_name'], 'target': np.concatenate(y_test_prob)})\npred_df.to_csv('submission_vgg.csv',header=True, index=False)\npred_df.head(10)","2d08c2f8":"y_test_prob2 = model2.predict(test_images)\npred_df2 = pd.DataFrame({'image_name': test['image_name'], 'target': np.concatenate(y_test_prob2)})\npred_df2.to_csv('submission_xception.csv',header=True, index=False)\npred_df2.head(10)","2268b7c5":"y_test_prob3 = model3.predict(test_images)\npred_df3 = pd.DataFrame({'image_name': test['image_name'], 'target': np.concatenate(y_test_prob3)})\npred_df3.to_csv('submission_dense.csv',header=True, index=False)\npred_df3.head(10)","b9f4bce9":"en = pd.DataFrame({'image_name':test['image_name'], 'target':(0.3*pred_df['target'] + 0.3*pred_df2['target'] + 0.3*pred_df3['target'])})\nen.to_csv('ensemble1.csv',header=True, index=False)\nen.head(10)","37b45b14":"**ENSEMBLE**","1653c8fb":"IMPROVING MODEL ","58bac23f":"**EVALUATION**","10895c33":"So this is a binary classification problem with highly imbalanced data.","f52bc8c1":"Backgrounf Extraction of Images","405bbae9":"Now we will load some of the resized images (32x32 for now) and try to build some simple models. ","18a8d955":"Let's look at the distribution of teh target:","bf63ff5f":"**PREDICTION**","8bfed840":"**MODELLING**","20dde9bf":"**VISUALISING IMAGE DATA : JPEG**","64cd09c5":"Let's see what files we have in the input directory:","b7eabc6c":"**TRAINING**","3cfdf0f0":"Let's take a look at a few images.","f4ea5187":"**DATA AUGMENTATION**","86f48ae3":"**Target vs Sex Distribution**"}}