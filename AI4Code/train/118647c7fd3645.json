{"cell_type":{"3c4ea6ea":"code","7585be06":"code","ec71d484":"code","5afa9d8f":"code","a83d97ae":"code","02a50e75":"code","8a0c13f3":"code","2f91765a":"code","c5573a97":"code","49866957":"code","75967671":"code","4f90d491":"code","07fbfd30":"code","8cd793c2":"code","37a7d92a":"code","e215bc12":"code","7cd3abdb":"code","4fc51f73":"code","8ff7b303":"code","9d75d3af":"code","c879f2c2":"code","907d9351":"markdown","3b5ba0cb":"markdown","23d1568e":"markdown","6f16e963":"markdown","d34c074b":"markdown","7570163e":"markdown","956b541e":"markdown","e371ef2d":"markdown","e8bb542f":"markdown","b879a0b9":"markdown"},"source":{"3c4ea6ea":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set() # Setting seaborn as default style even if use only matplotlib\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom imblearn.over_sampling import SMOTE \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","7585be06":"#import test dataset into DataFrame\n\ndata = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ndata.head(5)","ec71d484":"data.describe()","5afa9d8f":"data.info()","a83d97ae":"#fill nan-values of 'gender' with last valid value and 'experience' with 0 (since it probably is 0 years)\ndata['gender'] = data['gender'].fillna(method='ffill')\ndata['experience'] = data['experience'].fillna(0)\n\n\n#replacing nan-values with most common values in the column\ncols_nan_replace = ['enrolled_university','education_level', 'major_discipline', 'company_size', 'company_type', 'last_new_job']\nfor col in cols_nan_replace:\n    most_common_value = data[col].mode().iloc[0]\n    data[col] = data[col].fillna(most_common_value)\n\n    \n# replacing values in columns 'experience' and 'last_new_job' and converting them to int64 type\n\ndata['experience'] = data['experience'].replace('>20', '21').replace('<1', 0)\ndata['last_new_job'] = data['last_new_job'].replace('>4', 5).replace('never', 0)\n\n# converting cols to data type int\nconvert_cols = ['experience', 'last_new_job']\ndata[convert_cols] = data[convert_cols].apply(lambda x: x.astype(int)) ","02a50e75":"# dropping unnecessary column 'city'\n\ndata.drop(['city', 'enrollee_id'], inplace=True, axis=1)","8a0c13f3":"# Creating countplots for selected columns\n\nfig, axes = plt.subplots(1,4, figsize=(20, 5))\nn = 0\nfig.suptitle('Countplots of various columns')\n\ncols = ['gender', 'education_level', 'relevent_experience', 'major_discipline']\n\nfor col in cols:\n    sns.countplot(ax=axes[n], data=data, x=col, palette='rocket')\n    axes[n].set_title('Count of {}'.format(col))\n    axes[n].set_xlabel('')\n    axes[n].set_ylabel('')\n    axes[n].tick_params('x',labelrotation=70)\n    n += 1\n    \n  \nplt.show()\n","2f91765a":"# plot mean of training hours per gender\ntraining_hours = data.groupby('gender')['training_hours'].mean().sort_values(ascending=False)\ntraining_hours = pd.DataFrame(training_hours)\n\nn = training_hours.index\ns = training_hours['training_hours']\n\ntraining_hours.plot(kind='bar', color='green')\nplt.title('Mean of training hours by gender')\nplt.xticks(rotation=70)\nplt.xlabel('')\nplt.legend().remove()\n\n#adding annotations to the bars\n\nfor i in range(len(n)):\n    plt.annotate(str(round(s[i],2)), xy=(n[i],s[i]), ha='center', va='bottom')\n\nplt.show()","c5573a97":"# Create X(training data) and y (target variable) by subsetting the data \n\nX,y  = data.iloc [:, :-1], data.iloc[:, -1]","49866957":"# using pd.get_dummies to encode features without ordinal relationship\n# only the relevant columns are encoded with pd.get_dummies\n\n# select categorical columns which will be encoded\ncategorical_cols = X.columns[X.dtypes == 'object'].to_list()\n\n# get_dummies takes the whole dataframe and encodes only the categorical columns\nX_encoded = pd.get_dummies(X, columns = categorical_cols, drop_first=True)\n\nprint('The shape of the Dataframe changed from formerly {}'.format(X.shape[1]), 'to now {}'.format(X_encoded.shape[1]), 'columns.' )","75967671":"X_encoded.rename(columns={'company_size_10000+': 'company_size_10000_more', 'company_size_<10':'company_size_10_more'}, inplace=True)","4f90d491":"# Creating countplots for selected columns\n\nfig, axes = plt.subplots(1,4, figsize=(20, 5))\nn = 0\nfig.suptitle('Countplots of various columns')\n\ncols = ['experience', 'last_new_job', 'gender_Male', 'relevent_experience_No relevent experience']\n\nfor col in cols:\n    sns.countplot(ax=axes[n], data=X_encoded, x=col, palette='rocket')\n    axes[n].set_title('Count of {}'.format(col))\n    axes[n].set_xlabel('')\n    axes[n].set_ylabel('')\n    axes[n].tick_params('x',labelrotation=70)\n    n += 1\n    \n  \nplt.show()","07fbfd30":"# As the data is imbalanced we use SMOTE for balancing of the data\n\nsmote = SMOTE(random_state = 402)\nX_smote, y_smote = smote.fit_resample(X_encoded,y)","8cd793c2":"# Creating countplots for selected columns to check how smote changed the distribution\n\nfig, axes = plt.subplots(1,4, figsize=(20, 5))\nn = 0\nfig.suptitle('Countplots of various columns')\n\ncols = ['experience', 'last_new_job', 'gender_Male', 'relevent_experience_No relevent experience']\n\nfor col in cols:\n    sns.countplot(ax=axes[n], data=X_smote, x=col, palette='rocket')\n    axes[n].set_title('Count of {}'.format(col))\n    axes[n].set_xlabel('')\n    axes[n].set_ylabel('')\n    axes[n].tick_params('x',labelrotation=70)\n    n += 1\n    \nplt.show()","37a7d92a":"# Splitting the dataset into train and test set\n\nX_train, X_test, y_train, y_test = train_test_split(X_smote,y_smote, test_size=0.3, random_state=42)\n\n\n# Normalizing the dataset with StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit(X_test)","e215bc12":"# Instantiate XGBoost Classifier\n# Define params for RandomizedSearchCV\n# Instantiate RandomizedSearchCV object with params\n\nclf = xgb.XGBClassifier(objective='binary:logistic', seed=42)\n\nparams = {'max_depth': np.arange(2, 10),\n          'n_estimators': [5, 10, 15, 20, 25],\n          'colsample_bytree': [0.3, 0.7],\n          'subsample': [0.4, 0.6, 0.8, 1.0]\n         }\n\nrandomized_cv = RandomizedSearchCV(estimator=clf,param_distributions=params, scoring='roc_auc', n_iter=5, cv=5, verbose=1, n_jobs=1, \n                                   return_train_score=True)\n\n# Fit the data\nrandomized_cv.fit(X_train,y_train)","7cd3abdb":"print(\"Best parameters found: \", randomized_cv.best_params_)\n\nprint(\"Best score found: \", randomized_cv.best_score_)","4fc51f73":"# Instantiating a classifier with the obtained params\n\nclf = xgb.XGBClassifier(colsample_bytree= 0.3,\n                       n_estimators= 25,\n                       max_depth= 9,\n                       subsample= 0.4)\n\nmodel_fit = clf.fit(X_train, y_train)","8ff7b303":"# Predicting the probability and set threshold at 0.5\n\ny_proba = clf.predict_proba(X_train)[:,1]\ny_pred = (y_proba > 0.5).astype(bool)","9d75d3af":"# ROC curve chart\n\nfallout, sensitivity, thresholds = roc_curve(y_train, y_proba)\nplt.plot(fallout, sensitivity, color = 'darkorange')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.title ('Area under the curve')\nplt.show()\n\n# Printing AUC score\nprint ('The AUC score is {}'.format (round(metrics.roc_auc_score(y_train,y_pred),3)))","c879f2c2":"# Get the feature importance\nfeature_importance_dict = {}\nfeature_importance = clf.get_booster().get_score(importance_type = 'weight')\n\n\nfor feat, importance in zip(X_encoded.columns, feature_importance.values()):\n    feature_importance_dict[feat] = importance\n\n# Print the 5 most important features\nprint (sorted(feature_importance_dict.items(), key=lambda x:x[1])[-5:])","907d9351":"The above output shows that the column 'gender' has the most missing values. All missing values will be handled and imputed or substituted.\n","3b5ba0cb":"We see that the data is imbalanced. This will affect the performance of the model. Hence, the data will be balanced in the next step.","23d1568e":"# Is the candidate looking for a new job? by Martina Raabe\n","6f16e963":"<h1 style='background: black; border:1; color: white'><center>Introduction<\/center><\/h1>\n\n\nThis dataset is designed to understand the factors that lead to a person to leave their current job. This notebook uses XGBoostClassifier to determine whether it is likely for a person to look for a new job. The whole data is divided to train and test. \n\nThe following steps are performed in the notebook:\n\n1.  The training data is cleaned and missing values are handled via imputation and substitution.\n2.  A short EDA is performed with the goal to better understand the data.\n3.  A model (XGBoost) is trained in order to predict whether a candidate is looking for a new job or not.\n\n\nIf you like this notebook, please don't forget to **upvote**. Thanks!","d34c074b":"<h1 style='background: black; border:1; color: white'><center>Prediction with XGBoost<\/center><\/h1>","7570163e":"**Features**\n\nenrollee_id : Unique ID for candidate\n\ncity: City code\n\ncity_ development _index : Developement index of the city (scaled)\n\ngender: Gender of candidate\n\nrelevent_experience: Relevant experience of candidate\n\nenrolled_university: Type of University course enrolled if any\n\neducation_level: Education level of candidate\n\nmajor_discipline :Education major discipline of candidate\n\nexperience: Candidate total experience in years\n\ncompany_size: No of employees in current employer's company\n\ncompany_type : Type of current employer\n\nlastnewjob: Difference in years between previous job and current job\n\ntraining_hours: training hours completed\n\ntarget: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change","956b541e":"The graphic shows that the gender 'Other' had the most training hours whereas the men have the least training hours.","e371ef2d":"<h1 style='background: black; border:1; color: white'><center>OneHotEncoding of categorical features<\/center><\/h1>","e8bb542f":"<h1 style='background: black; border:1; color: white'><center>Importing, preparing and getting to know the data<\/center><\/h1>","b879a0b9":"<img src=\"https:\/\/images.unsplash.com\/photo-1455849318743-b2233052fcff?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1950&q=80\" width=\"500\" height=\"400\" align=\"center\"\/>\n\n\n[Source](https:\/\/unsplash.com\/@goian)"}}