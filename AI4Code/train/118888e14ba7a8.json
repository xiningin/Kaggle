{"cell_type":{"fab3cbaf":"code","64ca3c55":"code","9ec9c2e1":"code","dd7cdc20":"code","5fb7dc2f":"code","ef1ed40c":"code","5f14538d":"code","e5467aef":"code","af86a439":"code","bdc2cfa3":"code","a8996eeb":"code","b6ab8d38":"code","92c1ae2a":"code","667de79c":"code","5b1bb0c5":"code","b7fdab0d":"code","3f151b9e":"code","69f5dbae":"code","cc122a50":"code","956adc64":"code","0257de96":"code","a66ee3c0":"code","a681b124":"code","e1502548":"code","071143ea":"code","90f095e4":"code","1d24bdb3":"code","f21a0b45":"code","059c9237":"code","82804131":"code","047d2e71":"code","c41fca30":"code","abb50594":"code","66ecf201":"code","505a0f36":"code","1b6b8d6e":"code","3c1826b7":"code","e0da57c6":"code","adeff333":"code","862c77de":"code","41869953":"code","60eba755":"code","3863b94e":"code","f22619e0":"code","2de19a23":"code","f3941ff7":"code","1e5bface":"code","aba076ff":"code","1ae54d29":"code","b4286aab":"code","854a2bee":"code","24c4f15a":"code","bf332c90":"code","da04e1e5":"code","210f44de":"code","a287f134":"code","4aa4bdc4":"code","e125ba31":"code","2d2a452c":"code","aa791ac8":"code","d65aabe0":"code","bdea6c53":"code","bc79b1d6":"code","3788c532":"code","214c4c0c":"code","b8f230b4":"code","fde9c2f0":"code","3cfab2c7":"code","25d1320b":"code","564e828d":"code","7c2327ed":"code","6f63140b":"code","e89b7422":"code","be5b2f2f":"code","3334c5cf":"code","f4a80131":"code","3f96fc34":"code","03c44ab0":"code","446227ab":"code","74496dd7":"code","6435ba8f":"code","c9f687f8":"code","6e029a12":"code","c9f88605":"code","a6012fc8":"code","dd987a48":"code","7cd28201":"code","95fcfbd4":"code","7b232bd5":"code","681ff247":"code","e5660214":"code","5bf3b4b9":"code","6a9f97e0":"code","507ff6e8":"code","26d2d8d4":"code","5ad0396c":"code","e56d32c9":"code","2c6cf683":"code","34ab30f4":"code","a7d48116":"code","d36da74c":"code","cc8dd0a7":"code","9f880fa4":"code","ade86f60":"code","6e11a6f8":"code","995ff1f0":"code","d3a8874c":"code","6eb9d505":"code","bb8cebc2":"code","3de60d0c":"code","bee94d09":"code","5b2e99bd":"code","8755a8f8":"markdown","c91a0107":"markdown","b655ee94":"markdown","eb2ccefd":"markdown","26ff7a39":"markdown","e8c2995f":"markdown","ece048d5":"markdown","5b2d57b9":"markdown","3cf6ec61":"markdown","b40e8e2b":"markdown","63dae001":"markdown","70f1df80":"markdown","d862e316":"markdown","9fbcddb6":"markdown","80de60f9":"markdown","5ab7a645":"markdown","8665a41c":"markdown","c0d6337a":"markdown","d3f037f9":"markdown","efeb885f":"markdown","9d90fce5":"markdown","5ce1cf16":"markdown","f741d3a4":"markdown","e19243e1":"markdown","069f284a":"markdown","d9657c80":"markdown","7b73eabd":"markdown","0c624f0f":"markdown","3f589466":"markdown","cc4c8dfc":"markdown","5ebf3f4c":"markdown"},"source":{"fab3cbaf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","64ca3c55":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9ec9c2e1":"df = pd.read_excel(r\"\/kaggle\/input\/sales-pipeline-conversion-at-a-saas-startup\/Sales Dataset.xlsx\")\ndf.head()","dd7cdc20":"df.info()","5fb7dc2f":"df.shape","ef1ed40c":"df.describe()","5f14538d":"df_dub = df.copy()\n\n# Checking for duplicates and dropping the entire duplicate row if any\ndf_dub.drop_duplicates(subset=None, inplace=True)\ndf_dub.shape","e5467aef":"df.shape","af86a439":"# List of variables to map\n\nvarlist =  ['Opportunity Status']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Won': 1, \"Loss\": 0})\n\n# Applying the function to the housing list\ndf[varlist] = df[varlist].apply(binary_map)\ndf.head()","bdc2cfa3":"df= df.drop(['Opportunity ID'],1)\ndf.head()","a8996eeb":"df['Technology\\nPrimary'].describe()","b6ab8d38":"df['Technology\\nPrimary'].value_counts()","92c1ae2a":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Technology\\nPrimary'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","667de79c":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Technology\\nPrimary\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","5b1bb0c5":"df['City'].describe()","b7fdab0d":"df['City'].value_counts()","3f151b9e":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['City'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","69f5dbae":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"City\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","cc122a50":"df['B2B Sales Medium'].describe()","956adc64":"df['B2B Sales Medium'].value_counts()","0257de96":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['B2B Sales Medium'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","a66ee3c0":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"B2B Sales Medium\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","a681b124":"df['Sales Velocity'].describe()","e1502548":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Sales Velocity'])\nplt.show()","071143ea":"# As we can see there are a number of outliers in the data.\n# We will cap the outliers to 95% value for analysis.","90f095e4":"percentiles = df['Sales Velocity'].quantile([0.05,0.95]).values\ndf['Sales Velocity'][df['Sales Velocity'] <= percentiles[0]] = percentiles[0]\ndf['Sales Velocity'][df['Sales Velocity'] >= percentiles[1]] = percentiles[1]","1d24bdb3":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Sales Velocity'])\nplt.show()","f21a0b45":"plt.figure(figsize = (10,5))\nsns.violinplot(y = 'Sales Velocity', x = 'Opportunity Status', data = df)\nplt.show()","059c9237":"df['Opportunity Status'].describe()","82804131":"df['Opportunity Status'].value_counts()","047d2e71":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Opportunity Status'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.show()","c41fca30":"df['Sales Stage Iterations'].describe()","abb50594":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Sales Stage Iterations'])\nplt.show()","66ecf201":"percentiles = df['Sales Stage Iterations'].quantile([0.05,0.95]).values\ndf['Sales Stage Iterations'][df['Sales Stage Iterations'] <= percentiles[0]] = percentiles[0]\ndf['Sales Stage Iterations'][df['Sales Stage Iterations'] >= percentiles[1]] = percentiles[1]","505a0f36":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Sales Stage Iterations'])\nplt.show()","1b6b8d6e":"plt.figure(figsize = (10,5))\nsns.violinplot(y = 'Sales Stage Iterations', x = 'Opportunity Status', data = df)\nplt.show()","3c1826b7":"df['Opportunity Size (USD)'].describe()","e0da57c6":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Opportunity Size (USD)'])\nplt.show()","adeff333":"percentiles = df['Opportunity Size (USD)'].quantile([0.05,0.95]).values\ndf['Opportunity Size (USD)'][df['Opportunity Size (USD)'] <= percentiles[0]] = percentiles[0]\ndf['Opportunity Size (USD)'][df['Opportunity Size (USD)'] >= percentiles[1]] = percentiles[1]","862c77de":"plt.figure(figsize = (10,5))\nax= sns.violinplot(df['Opportunity Size (USD)'])\nplt.show()","41869953":"plt.figure(figsize = (10,5))\nsns.violinplot(y = 'Opportunity Size (USD)', x = 'Opportunity Status', data = df)\nplt.show()","60eba755":"df['Client Revenue Sizing'].describe()","3863b94e":"df['Client Revenue Sizing'].value_counts()","f22619e0":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Client Revenue Sizing'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","2de19a23":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Client Revenue Sizing\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","f3941ff7":"df['Client Employee Sizing'].describe()","1e5bface":"df['Client Employee Sizing'].value_counts()","aba076ff":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Client Employee Sizing'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","1ae54d29":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Client Employee Sizing\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","b4286aab":"df['Business from Client Last Year'].describe()","854a2bee":"df['Business from Client Last Year'].value_counts()","24c4f15a":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Business from Client Last Year'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","bf332c90":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Business from Client Last Year\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","da04e1e5":"df['Compete Intel'].describe()","210f44de":"df['Compete Intel'].value_counts()","a287f134":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Compete Intel'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","4aa4bdc4":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Compete Intel\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","e125ba31":"df['Opportunity Sizing'].describe()","2d2a452c":"df['Opportunity Sizing'].value_counts()","aa791ac8":"plt.figure(figsize = (10,5))\nax= sns.countplot(df['Opportunity Sizing'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nax.set_yscale('log')\nplt.xticks(rotation = 90)\nplt.show()","d65aabe0":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Opportunity Sizing\", hue = \"Opportunity Status\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","bdea6c53":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(df[['Technology\\nPrimary', 'City', 'B2B Sales Medium', 'Client Revenue Sizing',\n                            'Client Employee Sizing', 'Business from Client Last Year',\n                            'Compete Intel', 'Opportunity Sizing']], drop_first=True)\ndummy1.head()\n","bc79b1d6":"# Adding the results to the master dataframe\ndf = pd.concat([df, dummy1], axis=1)\ndf.head()","3788c532":"df = df.drop(['Technology\\nPrimary', 'City', 'B2B Sales Medium', 'Client Revenue Sizing',\n              'Client Employee Sizing', 'Business from Client Last Year',\n              'Compete Intel', 'Opportunity Sizing'], axis = 1)\ndf.head()","214c4c0c":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = df.drop(['Opportunity Status'], axis=1)","b8f230b4":"X.head()","fde9c2f0":"X.shape","3cfab2c7":"# Putting response variable to y\ny = df['Opportunity Status']","25d1320b":"y.head()","564e828d":"y.shape","7c2327ed":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=125)","6f63140b":"X_train.head()","e89b7422":"X_train.shape","be5b2f2f":"X_test.head()","3334c5cf":"X_test.shape","f4a80131":"y_train.head()","3f96fc34":"y_train.shape","03c44ab0":"y_test.head()","446227ab":"y_test.shape","74496dd7":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train[['Sales Velocity','Sales Stage Iterations',\n         'Opportunity Size (USD)']] = scaler.fit_transform(X_train[['Sales Velocity','Sales Stage Iterations',\n                                                                    'Opportunity Size (USD)']])\n\nX_train.head()","6435ba8f":"X_test[['Sales Velocity','Sales Stage Iterations',\n         'Opportunity Size (USD)']] = scaler.transform(X_test[['Sales Velocity','Sales Stage Iterations',\n                                                               'Opportunity Size (USD)']])\n\nX_test.head()","c9f687f8":"# Checking the Opportunity Status Rate\nOpportunity = round((sum(df['Opportunity Status'])\/len(df['Opportunity Status'].index))*100,2)\nprint(\"We have almost {} %  Opportunity rate after successful data manipulation\".format(Opportunity))","6e029a12":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = XGBClassifier()","c9f88605":"# fit the model with the training data\nmodel.fit(X_train,y_train)","a6012fc8":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","dd987a48":"trainaccuracy = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', trainaccuracy)","7cd28201":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","95fcfbd4":"features_to_remove = vif.loc[vif['VIF'] >= 4.99,'Features'].values\nfeatures_to_remove = list(features_to_remove)\nprint(features_to_remove)","7b232bd5":"X_train = X_train.drop(columns=features_to_remove, axis = 1)\nX_train.head()","681ff247":"X_test = X_test.drop(columns=features_to_remove, axis = 1)\nX_test.head()","e5660214":"# fit the model with the training data\nmodel.fit(X_train,y_train)","5bf3b4b9":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","6a9f97e0":"accuracytrain = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracytrain)","507ff6e8":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","26d2d8d4":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train, predict_train )\nprint(confusion)","5ad0396c":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","e56d32c9":"# Let's see the sensitivity of our model\ntrainsensitivity= TP \/ float(TP+FN)\ntrainsensitivity","2c6cf683":"# Let us calculate specificity\ntrainspecificity= TN \/ float(TN+FP)\ntrainspecificity","34ab30f4":"# Calculate false postive rate - predicting Opportunity when company does not have Opportunity\nprint(FP\/ float(TN+FP))","a7d48116":"# Positive predictive value \nprint (TP \/ float(TP+FP))","d36da74c":"# Negative predictive value\nprint(TN \/ float(TN+ FN))","cc8dd0a7":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","9f880fa4":"draw_roc(y_train,predict_train)","ade86f60":"#Using sklearn utilities for the same","6e11a6f8":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train,predict_train)","995ff1f0":"recall_score(y_train,predict_train)","d3a8874c":"# predict the target on the test dataset\npredict_test = model.predict(X_test)\nprint('Target on test data\\n\\n',predict_test)","6eb9d505":"confusion2 = metrics.confusion_matrix(y_test, predict_test )\nprint(confusion2)","bb8cebc2":"# Let's check the overall accuracy.\ntestaccuracy= accuracy_score(y_test,predict_test)\ntestaccuracy","3de60d0c":"# Let's see the sensitivity of our model\ntestsensitivity=TP \/ float(TP+FN)\ntestsensitivity","bee94d09":"# Let us calculate specificity\ntestspecificity= TN \/ float(TN+FP)\ntestspecificity","5b2e99bd":"# Let us compare the values obtained for Train & Test:\nprint(\"Train Data Accuracy    :{} %\".format(round((trainaccuracy*100),2)))\nprint(\"Train Data Sensitivity :{} %\".format(round((trainsensitivity*100),2)))\nprint(\"Train Data Specificity :{} %\".format(round((trainspecificity*100),2)))\nprint(\"Test Data Accuracy     :{} %\".format(round((testaccuracy*100),2)))\nprint(\"Test Data Sensitivity  :{} %\".format(round((testsensitivity*100),2)))\nprint(\"Test Data Specificity  :{} %\".format(round((testspecificity*100),2)))","8755a8f8":"Opportunity Sizing","c91a0107":"# VIF ","b655ee94":"#Dproping 'Opportunity ID' field as it will not help to take decision ","eb2ccefd":"Technology\\nPrimary","26ff7a39":"# VIF","e8c2995f":"## XGBClassifier","ece048d5":"Client Employee Sizing","5b2d57b9":"# Business Problem: Sales Pipeline Conversion at a SaaS Startup\n \n","3cf6ec61":"Opportunity Status","b40e8e2b":"The shape after running the drop duplicate command is same as the original dataframe.\n\nHence we can conclude that there were zero duplicate values in the dataset.","63dae001":"Compete Intel","70f1df80":"# Data Preparation","d862e316":"# Final Observation:","9fbcddb6":"Client Revenue Sizing","80de60f9":"The XGBoost has an immensely high predictive power which makes it the best choice for accuracy in events as it possesses both linear model and the tree learning algorithm, making the algorithm almost 10x faster than existing gradient booster techniques.\n\nThe support includes various objective functions, including regression, classification and ranking.","5ab7a645":"# Duplicate Check","8665a41c":"This assignment is around a case study about TechnoServe, a fictional tech SaaS (Software as a service) startup that specialises in different types of cloud-based software services to the small and medium enterprise customers. The products provided by the company are inclined towards increasing the productivity for the customers.\n\nThe revenue that the company generates is highly dependent on the consumption of the cloud services that they provide. Therefore, the revenue in-flow in the company is highly dependent on the number of clients that the company has. The company is facing a very pertinent problem faced in the IT industry today, declining conversions across its sales funnel.\n\n \n\nThe problem that the company is facing is that its pipeline conversion percentage has dropped from 35% at the end of the last fiscal (FY 2019-20) to 25% at present. The company needs a solution to solve the issue, and they have asked you to come up with one.\n\n \n    Here are a few details about TechnoServe that you should be aware of:\n\nThe company is based out of Pune and started its operations in 2010.\nIt has clients spread over Pune and other cities as well.\nThere are more than 600 employees, distributed over multiple teams.\nIt has a wide variety of IT solutions spread across different industries.\nTo get a brief idea about IT solutions,","c0d6337a":"Opportunity Size (USD)","d3f037f9":"# Precision and Recall","efeb885f":"## Problem Statement","9d90fce5":"Sales Stage Iterations\t","5ce1cf16":"B2B Sales Medium","f741d3a4":"# Plotting the ROC Curve","e19243e1":"Understand the problem, come up with possible hypotheses for low conversions faced by TechnoServe. Once that is done, you need to analyse the dataset given below to validate the hypotheses and form the solution strategy that they should employ to solve the problem. \nThe dataset and the data dictionary are given below","069f284a":"Making predictions on the test set","d9657c80":"City","7b73eabd":"# Model Building","0c624f0f":"Business from Client Last Year","3f589466":"Sales Velocity","cc4c8dfc":"Converting some binary variables (Won\/Lost) to 1\/0","5ebf3f4c":"# Feature Scaling"}}