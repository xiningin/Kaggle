{"cell_type":{"496afa5e":"code","f6786658":"code","12601b0f":"code","7e5ca8f4":"code","88a98d26":"code","a5edce4e":"code","77e4df54":"code","a60bbfb7":"code","4e0253d3":"code","cb645659":"code","f1bf38ec":"code","0fdfbfe4":"code","5e30aaa6":"code","e88e31ea":"code","8761fe43":"code","6e7e79bf":"code","bcb72fcc":"code","57dc2af3":"code","dd71ed18":"code","beb53ca4":"code","8972ff31":"code","fac080d6":"code","97a5ba59":"code","07e84071":"code","c60e63fa":"code","7374d7ad":"code","751bf3a7":"code","632fc4c5":"code","5c32519a":"code","8212d2d1":"code","0a9a40c5":"code","68f85ba6":"code","387fd41d":"code","c32ebe4a":"code","d5904da0":"code","81c3846b":"code","e67a1326":"code","8cde14f7":"code","a28bf77b":"code","57d1a91a":"code","33afef5a":"code","d219d9aa":"code","0f3d4927":"code","df931bd4":"code","5f025559":"code","1c645e17":"code","025b18d4":"code","368f3e5a":"code","889b1ad9":"code","3c8e1395":"code","bf6ac81d":"code","559f1716":"code","c71bd4ae":"code","e445ea34":"code","0379338d":"code","d9e29e10":"code","f6ed43c0":"code","5959bc27":"code","3daf6f44":"code","4d8ad879":"code","f739596c":"code","2ea043f0":"code","cd710f28":"code","69cd4462":"markdown","0dce6d4f":"markdown","01f060ea":"markdown","d7848110":"markdown","78d3ec61":"markdown","4ddb7c29":"markdown"},"source":{"496afa5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6786658":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")","12601b0f":"os.listdir('..\/input\/riiid-test-answer-prediction')","7e5ca8f4":"lectCsv = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/lectures.csv\")\nexampleTestCsv = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/example_test.csv\")\ntrainCsv = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/train.csv\", low_memory=False, nrows=1000000)\nquestionsCsv = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/questions.csv\")","88a98d26":"trainCsv.head()","a5edce4e":"trainCsv.info()","77e4df54":"trainCsv.describe()","a60bbfb7":"trainCsv.nunique()","4e0253d3":"trainCsv.isnull().sum()","cb645659":"#trainCsv[\"prior_question_elapsed_time\"] = trainCsv.groupby([\"user_id\", \"content_id\"]).transform(lambda x: x.fillna(x.mean()))\n#trainCsv[\"prior_question_had_explanation\"] = trainCsv.groupby([\"user_id\", \"content_id\"]).transform(lambda x: x.fillna(x.mean()))","f1bf38ec":"#trainCsv.isnull().sum()","0fdfbfe4":"trainCsv['timestamp'].hist(bins = 50)\n","5e30aaa6":"plt.figure(figsize=(15, 7))\nax = sns.countplot(trainCsv.groupby('user_id')['user_answer'].count().value_counts(), palette=\"hls\")\nplt.title(\"Count of answers per user\", fontsize=12)\nplt.xticks(rotation=90, fontsize=13)\nplt.ylabel('Number of answers')\nplt.xlabel('Count of users')","e88e31ea":"plt.figure(figsize=(15, 7))\nax = sns.countplot(trainCsv.user_answer)\nplt.title(\"Distribution of Mean's answer per user\", fontsize=12)\nplt.xticks(rotation=90, fontsize=13)\nplt.ylabel('Frequency')\nplt.xlabel('Average answer')","8761fe43":"# 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture. So, let's keep just the questions\ntrainCsv = trainCsv[trainCsv.content_type_id == 0]\n# read -1 as null, for lectures\ntrainCsv = trainCsv[trainCsv.answered_correctly != -1]","6e7e79bf":"trainCsv = trainCsv.sort_values(['timestamp'], ascending=True).reset_index(drop = True)","bcb72fcc":"trainCsv.head(5)","57dc2af3":"content_mean_final = trainCsv[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean'])\ncontent_mean_final.columns = [\"answered_correctly_content_mean\"]","dd71ed18":"user_mean_final = trainCsv[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean', 'sum', 'count'])\nuser_mean_final.columns = [\"answered_correctly_user_mean\", 'sum_correct', 'count']","beb53ca4":"#saving value to fillna\nelapsed_time_mean_final = trainCsv.prior_question_elapsed_time.mean()","8972ff31":"trainCsv.drop(['timestamp', 'content_type_id'], axis=1, inplace=True)","fac080d6":"validation = pd.DataFrame()\nfor i in range(4):\n    last_records = trainCsv.drop_duplicates('user_id', keep = 'last')\n    train_csv = trainCsv[~trainCsv.index.isin(last_records.index)]\n    validation = validation.append(last_records)\n    #print(validation)","97a5ba59":"X = pd.DataFrame()\nfor i in range(15):\n    last_records = train_csv.drop_duplicates('user_id', keep = 'last')\n    train_csv = train_csv[~train_csv.index.isin(last_records.index)]\n    X = X.append(last_records)","07e84071":"results_c = trainCsv[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean'])\nresults_c.columns = [\"answered_correctly_content_mean\"]\n\nresults_u = trainCsv[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean', 'sum', 'count'])\nresults_u.columns = [\"answered_correctly_user_mean\", 'sum_correct', 'count']","c60e63fa":"results_c.head()","7374d7ad":"results_c.shape","751bf3a7":"results_u.head()","632fc4c5":"results_u.shape","5c32519a":"result_time_mean = trainCsv.prior_question_elapsed_time.mean()","8212d2d1":"X = pd.merge(X, results_u, on=['user_id'], how=\"left\")\nX = pd.merge(X, results_c, on=['content_id'], how=\"left\")","0a9a40c5":"X.head()","68f85ba6":"X.shape","387fd41d":"validation = pd.merge(validation, results_u, on=['user_id'], how=\"left\")\nvalidation = pd.merge(validation, results_c, on=['content_id'], how=\"left\")","c32ebe4a":"validation.head()","d5904da0":"y = X['answered_correctly']\nX = X.drop(['answered_correctly'], axis=1)\n\ny_val = validation['answered_correctly']\nX_val = validation.drop(['answered_correctly'], axis=1)","81c3846b":"X.shape","e67a1326":"X.head()","8cde14f7":"X.columns","a28bf77b":"from sklearn.preprocessing import LabelEncoder\nlencoder = LabelEncoder()\n\nX['prior_question_had_explanation'].fillna(False, inplace = True)\nX['prior_question_had_explanation_enc'] = lencoder.fit_transform(X['prior_question_had_explanation'])\nX['answered_correctly_user_mean'].fillna(0.5,  inplace=True)\nX['answered_correctly_content_mean'].fillna(0.5,  inplace=True)\nX['sum_correct'].fillna(0, inplace = True)\nX['count'].fillna(0, inplace = True)\nX['prior_question_elapsed_time'].fillna(result_time_mean, inplace = True)\n\nX_val['prior_question_had_explanation'].fillna(False, inplace = True)\nX_val['prior_question_had_explanation_enc'] = lencoder.fit_transform(X_val['prior_question_had_explanation'])\nX_val['answered_correctly_user_mean'].fillna(0.5,  inplace=True)\nX_val['answered_correctly_content_mean'].fillna(0.5,  inplace=True)\nX_val['sum_correct'].fillna(0, inplace = True)\nX_val['count'].fillna(0, inplace = True)\nX_val['prior_question_elapsed_time'].fillna(result_time_mean, inplace = True)","57d1a91a":"X.head()","33afef5a":"X.info()","d219d9aa":"X = X[['answered_correctly_user_mean', 'answered_correctly_content_mean', 'sum_correct', 'count',\n       'prior_question_elapsed_time','prior_question_had_explanation_enc']]\nX_val = X_val[['answered_correctly_user_mean', 'answered_correctly_content_mean', 'sum_correct', 'count',\n       'prior_question_elapsed_time','prior_question_had_explanation_enc']]","0f3d4927":"X","df931bd4":"X_val","5f025559":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_val = scaler.transform(X_val)","1c645e17":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import BatchNormalization,Dropout,Dense,Flatten,Conv1D\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.metrics import BinaryAccuracy\nfrom keras import backend as K","025b18d4":"K.clear_session()\nX_train = X.reshape(X.shape[0], X.shape[1], 1)\nX_test = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n\n    \nmodel=Sequential()\nmodel.add(Conv1D(32, 2, activation='relu', input_shape=X_train[0].shape))\nmodel.add(Conv1D(64, 2, activation='relu'))#, padding='causal'))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy()])","368f3e5a":"history = model.fit(X_train, y, epochs=100, batch_size=50000)\nprint(history)","889b1ad9":"y_pred = model.predict(X_test)\ny_true = np.array(y_val)","3c8e1395":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_true, y_pred)","bf6ac81d":"import keras\nmodelRNN = keras.models.Sequential([\nkeras.layers.SimpleRNN(100, return_sequences=True, input_shape=X_train[0].shape),\nDropout(0.5),\nkeras.layers.SimpleRNN(100, return_sequences=True),\nDropout(0.5),\n\nkeras.layers.SimpleRNN(1)\n])\nmodelRNN.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.BinaryAccuracy()])","559f1716":"history1 = modelRNN.fit(X_train, y, epochs=100, batch_size=50000,validation_data =(X_test,y_val))\nprint(history1)","c71bd4ae":"import matplotlib.pyplot as plot\nplot.plot(history1.history['accuracy'])\nplot.plot(history1.history['val_accuracy'])\nplot.title('Regularized Model accuracy')\nplot.ylabel('Accuracy')\nplot.xlabel('Epoch')\nplot.legend(['Train', 'Test'], loc='upper left')\nplot.show()\n\nplot.plot(history1.history['loss'])\nplot.plot(history1.history['val_loss'])\nplot.title('Model loss')\nplot.ylabel('Loss')\nplot.xlabel('Epoch')\nplot.legend(['Train', 'Test'], loc='upper left')\nplot.show()","e445ea34":"y_pred1 = modelRNN.predict(X_test)\ny_true1 = np.array(y_val)\n","0379338d":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_true1, y_pred1)","d9e29e10":"#X_test_acc = X_test.astype('float32')\/255.0\n#y_val_new = y_val.astype('uint')\n\n#loss, acc = modelRNN.evaluate(X_test_acc, y_val_new, verbose=0)\n#print('Accuracy: %.3f' % acc)","f6ed43c0":"modelLSTM = keras.models.Sequential([\nkeras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True,\ninput_shape=X_train[0].shape),\nkeras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True),\nkeras.layers.TimeDistributed(keras.layers.Dense(1))\n])\nmodelLSTM.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy()])","5959bc27":"history2 = modelLSTM.fit(X_train, y, epochs=100, batch_size=50000)\nprint(history1)","3daf6f44":"y_pred2 = modelLSTM.predict(X_test)\ny_true2 = np.array(y_val)","4d8ad879":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_true2, y_pred2)","f739596c":"import riiideducation\nenv = riiideducation.make_env()","2ea043f0":"iter_test = env.iter_test()\n","cd710f28":"for (test_df, sample_prediction_df) in iter_test:\n    test_df = pd.merge(test_df, user_mean_final, on=['user_id'],  how=\"left\")\n    test_df = pd.merge(test_df, content_mean_final, on=['content_id'],  how=\"left\")\n    \n    test_df['answered_correctly_user_mean'].fillna(0.5,  inplace=True)\n    test_df['answered_correctly_content_mean'].fillna(0.5,  inplace=True)\n    test_df['sum_correct'].fillna(0, inplace=True)\n    test_df['count'].fillna(0, inplace=True)\n    test_df['prior_question_elapsed_time'].fillna(elapsed_time_mean_final, inplace = True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lencoder.transform(test_df[\"prior_question_had_explanation\"])\n\n    # fit transform cnn\n    X = scaler.transform(test_df[['answered_correctly_user_mean', 'answered_correctly_content_mean', 'sum_correct', 'count',\n                                  'prior_question_elapsed_time', 'prior_question_had_explanation_enc']])\n    test_df['answered_correctly'] = model.predict(X.reshape(X.shape[0], X.shape[1], 1))\n    \n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","69cd4462":"## Parameter Tuning","0dce6d4f":"## EDA","01f060ea":"## Model","d7848110":"## LSTM","78d3ec61":"## Submission","4ddb7c29":"## Preprocessing"}}