{"cell_type":{"e5db4918":"code","a9aa2813":"code","0316cc67":"code","e2631c04":"code","19437044":"code","158d9c2a":"code","1945f33d":"code","f31894d9":"code","92fb14bb":"code","04e36184":"code","aac3aa77":"code","0528a667":"code","f21b5f91":"code","b4685e2c":"code","af4a4adb":"code","05fbf5f0":"code","fa492d35":"code","c26a9fd2":"code","90c967c5":"code","74d469c7":"code","1684ef5a":"code","96465c7c":"markdown","05165904":"markdown","ced68ec2":"markdown","b27a32e5":"markdown"},"source":{"e5db4918":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport gc","a9aa2813":"#Load data\ntrain_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')","0316cc67":"## Use no scaling data to train LGBM\ntrain_features = train_df.drop(['target','ID_code'], axis = 1)\ntest_features = test_df.drop(['ID_code'],axis = 1)\ntrain_target = train_df['target']","e2631c04":"gc.collect()","19437044":"train_all = pd.concat((train_features,test_features),axis = 0)","158d9c2a":"for f in train_all.columns:\n    train_all[f+'_duplicate'] = train_all.duplicated(f,False).astype(int)\n#train_all['count_total_all']=train_all.iloc[:,200:400].sum(axis=1)","1945f33d":"for f in train_all.columns[0:200]:\n    train_all[f+'duplicate_value'] = train_all[f]*train_all[f+'_duplicate']","f31894d9":"train_features = train_all.iloc[:200000]\ntest_features = train_all.iloc[200000:400000]","92fb14bb":"del train_all\ngc.collect()","04e36184":"#test_features['var_68_te'].value_counts()","aac3aa77":"train_features.shape, test_features.shape, train_target.shape","0528a667":"n_splits = 7# Number of K-fold Splits\n\nsplits = list(StratifiedKFold(n_splits=n_splits, shuffle=True).split(train_features, train_target))\nsplits[:3]","f21b5f91":"cat_params = {\n    'learning_rate':0.01,\n    'max_depth':2,\n    'eval_metric': 'AUC',\n    'bootstrap_type': 'Bayesian',\n    'bagging_temperature': 1,\n    'objective': 'Logloss',\n    'od_type': 'Iter',\n    'l2_leaf_reg': 2,\n    'allow_writing_files': False}","b4685e2c":"from catboost import CatBoostClassifier\noof_cb = np.zeros(len(train_features))\npredictions_cb = np.zeros(len(test_features))\n#feature_importance_df = pd.DataFrame()\n#features = [c for c in train_features_df.columns if c not in ['ID_code', 'target']]\n\nfor i, (train_idx, valid_idx) in enumerate(splits):  \n    print(f'Fold {i + 1}')\n    x_train = np.array(train_features)\n    y_train = np.array(train_target)\n    trn_x = x_train[train_idx.astype(int)]\n    trn_y = y_train[train_idx.astype(int)]\n    val_x = x_train[valid_idx.astype(int)]\n    val_y = y_train[valid_idx.astype(int)]\n    \n    num_round = 100000\n    clf = CatBoostClassifier( num_round, task_type='GPU', early_stopping_rounds=1000,**cat_params,)\n    clf.fit(trn_x, trn_y, eval_set=(val_x, val_y), cat_features=[], use_best_model=True, verbose=500)\n    \n    oof_cb[valid_idx] = clf.predict_proba(val_x)[:,1]\n\n    predictions_cb += clf.predict_proba(test_features)[:,1] \/ 5\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(train_target, oof_cb)))\ngc.collect()","af4a4adb":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.33,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 12,\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1\n}","05fbf5f0":"oof = np.zeros(len(train_features))\npredictions = np.zeros(len(test_features))\n#feature_importance_df = pd.DataFrame()\n#features = [c for c in train_features.columns if c not in ['ID_code', 'target']]\n\nfor i, (train_idx, valid_idx) in enumerate(splits):  \n    print(f'Fold {i + 1}')\n    x_train = np.array(train_features)\n    y_train = np.array(train_target)\n    trn_data = lgb.Dataset(x_train[train_idx.astype(int)], label=y_train[train_idx.astype(int)])\n    val_data = lgb.Dataset(x_train[valid_idx.astype(int)], label=y_train[valid_idx.astype(int)])\n    \n    num_round = 100000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[valid_idx] = clf.predict(x_train[valid_idx], num_iteration=clf.best_iteration)\n    \n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = features\n    #fold_importance_df[\"importance\"] = clf.feature_importance()\n    #fold_importance_df[\"fold\"] = i + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_features, num_iteration=clf.best_iteration) \/ 5\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(train_target, oof)))\ngc.collect()","fa492d35":"esemble_lgbm_cat = 0.5*oof_cb+0.5*oof\nprint('LightBGM auc = {:<8.5f}'.format(roc_auc_score(train_target, oof)))\nprint('catboost auc = {:<8.5f}'.format(roc_auc_score(train_target, oof_cb)))\nprint('LightBGM+catboost auc = {:<8.5f}'.format(roc_auc_score(train_target, esemble_lgbm_cat)))","c26a9fd2":"esemble_pred_lgbm_cat = 0.5*predictions+0.5*predictions_cb","90c967c5":"id_code_test = test_df['ID_code']","74d469c7":"my_submission_lbgm = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : predictions})\nmy_submission_cat = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : predictions_cb})\nmy_submission_esemble_lgbm_cat = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : esemble_pred_lgbm_cat})","1684ef5a":"my_submission_lbgm.to_csv('submission_lbgm.csv', index = False, header = True)\nmy_submission_cat.to_csv('submission_cb.csv', index = False, header = True)\nmy_submission_esemble_lgbm_cat.to_csv('my_submission_esemble_lgbm_cat.csv', index = False, header = True)","96465c7c":"## Load Data","05165904":"## Ensemble two model (NN+ LGBM)\n* NN model accuracy is too low, ensemble looks don't work.","ced68ec2":"## Create submit file","b27a32e5":"## Pytorch to implement simple feed-forward NN model (0.89+)\n\n* As below discussion, NN model can get lB 0.89+\n* https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/82499#latest-483679\n* Add Cycling learning rate , K-fold cross validation (0.85 to 0.86)\n* Add flatten layer as below discussion (0.86 to 0.897)\n* https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/82863\n\n## LightGBM (LB 0.899)\n\n* Fine tune parameters (0.898 to 0.899)\n* Reference this kernel : https:\/\/www.kaggle.com\/chocozzz\/santander-lightgbm-baseline-lb-0-899\n\n\n## Plan to do\n* Modify model structure on NN model\n* Focal loss\n* Feature engineering\n* Tune parameters oof LightGBM"}}