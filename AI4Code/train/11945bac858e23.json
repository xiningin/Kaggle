{"cell_type":{"9e00b693":"code","cad76c87":"code","dd304b97":"code","2b99029c":"code","9db36056":"code","2450935e":"code","c658f034":"code","b54c465c":"code","87669bca":"code","7976bbf9":"code","f34ddd5c":"code","1768ede3":"code","aa71e73b":"code","1dc94166":"code","b146e8e3":"code","864e129e":"code","2afd76f6":"code","bb56e595":"code","4b932d24":"code","3874a9b7":"code","7151facf":"code","de855b28":"code","1716c1a2":"code","e7b1a3b6":"code","22f02e02":"code","bb951ba8":"code","3bb7d396":"code","55b91454":"code","23498a14":"code","7b9ac270":"code","072bba61":"code","feffd2e7":"code","a01e31ed":"code","e325e56d":"code","cea44cc5":"code","30907b5e":"code","0a7bf66d":"code","31748d91":"code","1c5758c5":"code","9bd87939":"code","b89e0e61":"code","a8790471":"code","95761fc1":"code","0d51d5b4":"code","a2194229":"code","6cde33f4":"code","1285b3dd":"code","81a4dd16":"code","17768bfe":"code","df15524e":"code","7c251d5d":"code","c3d86547":"code","35687fdd":"code","d8a6089c":"code","1b9d531a":"markdown","494b7f21":"markdown","aeef8d1c":"markdown","10046c9c":"markdown","f85679ed":"markdown","71536bbd":"markdown","7a49fbe1":"markdown","472f0480":"markdown","fa9275af":"markdown","e66bb87b":"markdown","32475380":"markdown","fd9626aa":"markdown","8328b59a":"markdown","fd40a11f":"markdown","3361e9dc":"markdown","90c365e9":"markdown","a5bd1cd1":"markdown","7c80481d":"markdown","044a6848":"markdown","03f9d0f2":"markdown","1dc83582":"markdown","41ae3eeb":"markdown","d78ebb4f":"markdown","e3f33a1e":"markdown","4769b454":"markdown","a3d20f99":"markdown","822b0f94":"markdown","eb314389":"markdown","73b0ecba":"markdown","6c49b558":"markdown","ba33beec":"markdown","4e1f8f0f":"markdown","dbe02079":"markdown","2fe569af":"markdown","56d49ed7":"markdown","ccc9f3cc":"markdown","b3e4d1c1":"markdown","3dc33097":"markdown","092497cc":"markdown","dc3dd093":"markdown","eab49d22":"markdown","19249164":"markdown","28c5a48f":"markdown","e197fedb":"markdown","be482dff":"markdown","58408924":"markdown","cc3a0dd2":"markdown","d3a56545":"markdown","202d8e1b":"markdown","78eddbaa":"markdown","6f74f5ea":"markdown","f8c65286":"markdown","5508da4b":"markdown","93efd8b4":"markdown","49e0dd4e":"markdown","5064544e":"markdown","bf62ebc3":"markdown","44b67db2":"markdown","183eb8fe":"markdown","44f541aa":"markdown","138b84f3":"markdown","91ace819":"markdown","48c83808":"markdown","0895dccb":"markdown","540648d6":"markdown","625c6c49":"markdown","d7886c90":"markdown"},"source":{"9e00b693":"import numpy as np #for linear algebra\nimport matplotlib #for more graphical representations\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport pandas as pd #for data processing\nimport os  #for operating system interaction\nimport glob # for matching pathnames\nimport json # to convert the python dictionary above into a JSON string that can be written into a file\n","cad76c87":"#FOR NATURAL LANGUAGE PROCESSING: THERE ARE TWO FUNDAMENTAL TOOLKITS, NLTK AND SCISPACY. For now we are going to import both the see which one we do use. \n\n!pip install nltk\nimport nltk\n#nltk.download()\nfrom nltk import tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.cluster.util import cosine_distance\n\n\n\n# Download the spacy bio parser\nfrom IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n\n\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bionlp13cg_md-0.2.4.tar.gz \n    \n!pip install scispacy\nimport scispacy\nimport en_ner_bionlp13cg_md\nfrom spacy import displacy\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom scispacy.umls_linking import UmlsEntityLinker\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport en_core_sci_lg  # model downloaded in previous step\n\nimport networkx as nx","dd304b97":"#OTHER LIBRARIES OF INTEREST\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AffinityPropagation\nimport re\nimport pke","2b99029c":"#FOR THE MOMENT, THIS IS COMMENTED BECAUSE WE HAVE PROBLEMS WITH IT\n\n#from kdcovid.encode_sentences import encode import torch #to be able to build tensors\n#import sent2vec #in order to vectorize elements","9db36056":"from pprint import pprint #for printing data structures \nfrom copy import deepcopy #for allowing copy\n\nfrom tqdm.notebook import tqdm #in order to select the language we are interested in","2450935e":"#FOR LANGUAGE DETECTION\n! pip install langdetect\nimport langdetect\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n","c658f034":"import string","b54c465c":"from ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets","87669bca":"#The first step is to etablish the paths to the dataset of interest: \n\n#We define the root-path\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge' #we are doing it for the main dataset, but can be repeated for the other two\n\n\n#root_path= '\/kaggle\/input\/uncover'\n#root_path= '\/kaggle\/input\/covid19-genomes'\n\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})","7976bbf9":"meta_df.head()","f34ddd5c":"all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","1768ede3":"#Helper: File Reader Class  \n\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            \n#Now, the two following self. definitions were not given , but we develop them\n\n            self.abstract = content['abstract']\n            self.body_text = content['body_text']","aa71e73b":" # Abstract\n        \n           #for entry in content['abstract']:\nfor entry in content['abstract']:\n                    self.abstract.append(entry['text'])","1dc94166":"  # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])","b146e8e3":"self.abstract = '\\n'.join(self.abstract)\nself.body_text = '\\n'.join(self.body_text)","864e129e":"def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)","2afd76f6":"meta_df.info()","bb56e595":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n# add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","4b932d24":"#Using the helper functions, let's read in the articles into a DataFrame that can be used easily:\n\ndict_ = {'journal': [], 'title': [], 'body_text': [], 'doi':[],'paper_id': []}\n\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    \n    try:\n        content = FileReader(entry)\n    except Exception as e:\n        continue  # invalid paper format, skip\n    \n # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n\n # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['body_text'].append(content.body_text)\n  \n # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n   \n # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \n # add doi\n    dict_['doi'].append(meta_data['doi'].values[0])\n    ","3874a9b7":"df_covid = pd.DataFrame(dict_, columns=['journal', 'title','body_text',  'doi','paper_id',])\ndf_covid.head()","7151facf":"#Here is the modification in order to count the words: \n\ndf_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\ndf_covid['body_unique_words']=df_covid['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body","de855b28":"#And here we visualize them \n\ndf_covid.head()\ndf_covid.info()\ndf_covid['abstract'].describe(include='all')","1716c1a2":"#Handle Possible Duplicates\n\ndf_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\ndf_covid['abstract'].describe(include='all')\n\n #Take a Look at the Data:\n    \ndf_covid.head()\n\n#In the majority of this notebook we will be working with body_text\n#Links to the papers will be generated using doi\n\ndf_covid.describe()","e7b1a3b6":"#In Kaggle we will limit the dataframe to 10,000 instances\n\ndf = df_covid.sample(10000, random_state=42)\ndel df_covid\n\n#Now that we have our dataset loaded, we need to clean-up the text to improve any clustering or classification efforts. First, let's drop Null vales:\n\ndf.dropna(inplace=True)\ndf.info()","22f02e02":"#Handling multiple languages  #PROBABLY WILL BE MODIFIED FOR OUR OUR PURPOSES\n\n#Set seed\nDetectorFactory.seed = 0\n\n# hold label - language\nlanguages = []\n\n# go through each text\nfor ii in tqdm(range(0,len(df))):\n\n# split by space into list, take the first x intex, join with space\n    text = df.iloc[ii]['body_text'].split(\" \")\n    \n    lang = \"en\"\n    try:\n        if len(text) > 50:\n            lang = detect(\" \".join(text[:50]))\n        elif len(text) > 0:\n            lang = detect(\" \".join(text[:len(text)]))\n\n# ught... beginning of the document was not in a good format\n    except Exception as e:\n        all_words = set(text)\n        try:\n            lang = detect(\" \".join(all_words))\n\n# what!! :( let's see if we can find any text in abstract...\n        except Exception as e:\n            \n            try:\n\n# let's try to label it through the abstract then\n                lang = detect(df.iloc[ii]['abstract_summary'])\n                 except Exception as e:\n                lang = \"unknown\"\n                pass\n\n#get the language    \n    languages.append(lang)","bb951ba8":"languages_dict = {}\n\nfor lang in set(languages):\n    languages_dict[lang] = languages.count(lang)\n    \nprint(\"Total: {}\\n\".format(len(languages)))\npprint(languages_dict)\n\n#Lets take a look at the language distribution in the dataset\n\ndf['language'] = languages\n\nplt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\nplt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\nplt.title(\"Distribution of Languages in Dataset\")\nplt.show()\n\n","3bb7d396":"#We will be dropping any language that is not English. Attempting to translate foreign texts gave the following problems:\n\n#API calls were limited\n\n#Translating the language may not carry over the true semantic meaning of the text\n\ndf = df[df['language'] == 'en'] \ndf.info()","55b91454":"!ls \/kaggle\/input\/CORD-19-research-challenge\n#!ls \/kaggle\/input\/uncover\n#!ls\/kaggle\/input\/covid-19-genomes","23498a14":"punctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nstopwords[:10]","7b9ac270":"#The following are the stopwords that where on this notebook, but we changed it as it follows: \n#custom_stop_words = [\n   # 'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n   # 'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n   # 'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n#]\n\ncustom_stop_words = [ 'number','human', 'cases','also','reported','one','immune','response','within','influenza','among',\n    'different','high','found','showed','use', 'identified','two','used','results',\n    'analysis','performed','using','described','detected','including','group','could','observed','significant','based',\n    'shown','however','compared','higher','may','specific','studies',\n    'study', 'type', 'well', 'although','levels', 'host', 'activity','data','associated','due',\n    'samples','figure','table','case', 'effect', 'effects', 'affected','across','within',\n    'humans','who', 'what', 'why', 'how','distribution',  'eg',  'ie','prevalence', 'particularly','whether', 'make','even',\n    'might','2019',]\n\nfor w in custom_stop_words:\n    if w not in stopwords:\n        stopwords.append(w)","072bba61":"# Parser - here we still have to put here our etra words. \n\nparser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","feffd2e7":"tqdm.pandas()\ndf[\"processed_text\"] = df[\"body_text\"].progress_apply(spacy_tokenizer)","a01e31ed":"sns.distplot(df['body_word_count'])\ndf['body_word_count'].describe()","e325e56d":"sns.distplot(df['body_unique_words'])\ndf['body_unique_words'].describe()","cea44cc5":"#Keyword extraction-> first we are going to focus on the point 4.6.1 about the genetics of the virus.\n#Once that improved, we will copy the model for the others tasks. \n\nGENETIC_TERMS =[\n    'SARS-Cov RNA','RNA dependency','duplex RNA','non-structural','nsP13 molecular','5terminal','MGHHHHHHGS tag sequence',]\n\nCOVID_19_TERMS = [\n    'covid-19', 'covid 19', 'covid-2019', '2019 novel coronavirus','corona virus disease 2019', 'coronavirus disease 19',\n    'coronavirus 2019',  '2019-ncov', 'ncov-2019',  'wuhan virus',  'wuhan coronavirus',  'wuhan pneumonia',  'NCIP',   'sars-cov-2',\n    'sars-cov2',]\n\nVIRUS_TERMS = [\n    'epidemic', 'pandemic', 'viral','virus',\n    'viruses', 'coronavirus', 'respiratory','infectious',\n + COVID_19_TERMS+ GENETIC TERMS ]","30907b5e":"data = pd.read_csv(\"..\/input\/papers.csv\")\ndata.head()\nprint(data.paper_text[1][:500], \"...\")\nprint(\"\\ntotal length\", len(data.paper_text[1]))\ndata.paper_text = data.paper_text.apply(lambda x: re.sub(\"(\\W)\", \" \", x))\ntokenizer = nlp.WordPunctTokenizer()\ndata[\"word_count\"] = data.paper_text.apply(lambda x: len(tokenizer.tokenize(x)))\ndata[[\"word_count\", \"paper_text\"]].head()\n\nfreq = pd.Series(\" \".join(data.paper_text).split()).value_counts()\nprint(freq.head(10))\nprint(freq.tail(10))\n\nlemma = nlp.WordNetLemmatizer()\ndata.paper_text = data.paper_text.apply(lambda x: lemma.lemmatize(x))\n\ndata.paper_text = data.paper_text.apply(lambda x: x.lower())\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nstopword_list = set(stopwords.words(\"english\"))\n\nword_cloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopword_list,\n                          max_words=100,\n                          max_font_size=50, \n                          random_state=42\n                         ).generate(str(data.paper_text))\nprint(word_cloud)\nfig = plt.figure(1)\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)\n\ntf_idf = TfidfVectorizer(max_df=0.8,stop_words=stopword_list, max_features=10000, ngram_range=(1,3))\ntf_idf.fit(data.paper_text)\n\ndoc = pd.Series(data.paper_text[500])\ndoc_vector = tf_idf.transform(doc)\n\nFunction for sorting tf_idf in descending order\nfrom scipy.sparse import coo_matrix\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(doc_vector.tocoo())\n#extract only the top n; n here is 10\nfeature_names = tf_idf.get_feature_names()\nkeywords=extract_topn_from_vector(feature_names,sorted_items,5)\n \n# now print the results\nprint(\"\\nAbstract:\")\nprint(doc[0][:1000])\n\nprint(\"Keywords:\")\nfor k in keywords:\n    print(k,keywords[k])","0a7bf66d":"#####################################################################################\ndatafiles = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        ifile = os.path.join(dirname, filename)\n        if ifile.split(\".\")[-1] == \"json\":\n            datafiles.append(ifile)\n","31748d91":"print('Loading the title')\n\n#First step is going to be initialization \n\ntitleID = [] #we create the empty matrix where we are going to put the title identification\n\nfor file in datafiles:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    id = doc['paper_id'] \n    title = doc['metadata']['title']\n    titleID.append({id:title})\n\nprint ('The title was loaded correctly, now let us loading the abstract' )\n\n#Same idea\nabstractID = []\nfor file in datafiles:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    id = doc['paper_id'] \n    abstract = ''\n    try:\n        for item in doc['abstract']:\n            abstract = abstract + item['text']\n            abstractID.append({id:abstract})\n    except KeyError:\n        None\n    \nprint ('The abstract was downloaded correctly, let us load the body-text')   \n\nbodyTextID = []\nfor file in datafiles:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    id = doc['paper_id'] \n    bodytext = ''\n    try:\n        for item in doc['body_text']:\n            bodytext = bodytext + item['text']\n            \n        bodyTextID.append({id:bodytext})\n    except KeyError:\n        None\nprint ('Download finished')\n","1c5758c5":"nlp = en_ner_bionlp13cg_md.load()","9bd87939":"sentence_list=[]\nsentence_list_without_col = []\nID_list = []\nent_type_all_check = []\nent_type_all= []\n\nfor i in tqdm(range(len(abstractID[:100]))):\n    ID = list(abstractID[i].keys())[0]\n    text = list(abstractID[i].values())[0]\n    a = tokenize.sent_tokenize(text) # Split Sentence\n    \n    for sent in a:\n        print_flag = False\n        doc = nlp(sent)\n        count_label =0\n        ent_type = ''\n        check_dupl =[]\n        sent_withcol = sent\n        sent_withoutcol = sent\n        \n        for x in doc.ents:\n            if x.text not in check_dupl:\n                sent_withcol = sent_withcol.replace(x.text,f\"\\033[1;40m{x.text}\\033[1;31;40m ({x.label_}) \\033[00m\")\n                sent_withoutcol = sent_withoutcol.replace(x.text,f\"{x.text} *{x.label_}*\")\n                check_dupl.append(x.text)\n                print_flag =True\n\n            if x.label_ not in ent_type:\n                ent_type+= f'{x.label_}, '\n                ent_type_all_check.append(x.label_)\n                \n        if print_flag== True:\n            sentence_list.append('* '+sent_withcol)\n            sentence_list_without_col.append('* '+sent)\n            ent_type_all.append(ent_type)\n            ID_list.append(ID)","b89e0e61":"len(ID_list),len(sentence_list),len(sentence_list_without_col),len(ent_type_all)","a8790471":"pd_all = pd.DataFrame({'Paper_ID':ID_list,'Sentence_col':sentence_list,'Sentence_wo_col':sentence_list_without_col,'NER_Key':ent_type_all})","95761fc1":"type_list = pd.Series(ent_type_all_check).unique().tolist() # Entity type","0d51d5b4":"def f(Paper_ID='all',Show_Many='100', Entity='GENETICS', Show_original=False):\n    pd_all2 = pd_all.copy()[:int(Show_Many)]\n    if Paper_ID != 'all':\n        pd_all2 = pd_all[pd_all.Paper_ID==Paper_ID].copy()[:int(Show_Many)]\n    pd_all2 = pd_all2[pd_all2['NER_Key'].str.contains(Entity)].reset_index(drop=True)\n    for i in range(len(pd_all2)):\n        if Show_original==True:\n            print (pd_all2['Sentence_wo_col'][i])  \n        else:\n            print (pd_all2['Sentence_col'][i])    ","a2194229":"def f(Paper_ID='all',Show_Many='100', Entity='GENETICS', Show_original=False):\n    pd_all2 = pd_all.copy()[:int(Show_Many)]\n    if Paper_ID != 'all':\n        pd_all2 = pd_all[pd_all.Paper_ID==Paper_ID].copy()[:int(Show_Many)]\n    pd_all2 = pd_all2[pd_all2['NER_Key'].str.contains(Entity)].reset_index(drop=True)\n    for i in range(len(pd_all2)):\n        if Show_original==True:\n            print (pd_all2['Sentence_wo_col'][i])  \n        else:\n            print (pd_all2['Sentence_col'][i])    ","6cde33f4":"interact(f, Paper_ID='all',Show_Many='100',Entity=type_list, Original = False);","1285b3dd":"def read_article(file_name):\n    file = open(file_name, \"r\")\n    filedata = file.readlines()\n    article = filedata[0].split(\". \")\n    sentences = []\n    for sentence in article:\n     print(sentence)\n     sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n     sentences.pop() \n    \n    return sentences","81a4dd16":"def build_similarity_matrix(sentences, stop_words):\n\n# Create an empty similarity matrix\n    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n \n    for idx1 in range(len(sentences)):\n        for idx2 in range(len(sentences)):\n            if idx1 == idx2: #ignore if both are same sentences\n                continue \n            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n    return similarity_matrix","17768bfe":"#Method will keep calling all other helper function to keep our summarization pipeline going. \n\ndef generate_summary(file_name, top_n=5):\n    stop_words = stopwords.words('english')\n    summarize_text = []","df15524e":" # Step 1 - Read text and tokenize\n    sentences =  read_article(file_name) ","7c251d5d":"# Step 2 - Generate Similary Martix across sentences\n    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)","c3d86547":"# Step 3 - Rank sentences in similarity martix\n    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n    scores = nx.pagerank(sentence_similarity_graph)","35687fdd":"# Step 4 - Sort the rank and pick top sentences\n    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\nfor i in range(top_n):\n      summarize_text.append(\" \".join(ranked_sentence[i][1]))","d8a6089c":"# Step 5 - Offcourse, output the summarize texr\n    print(\"Summarize Text: \\n\", \". \".join(summarize_text))","1b9d531a":"    **PART A**\n\n***4.2. Import** *\n\nFirst step is to import the libraries we are likely going to use.\n","494b7f21":"***GENERAL INTRODUCTION*\n**\n  - Origins of the virus:\n    \nCoronavirsuses (CoVs) are a large family of viruses, called Coronaviridae. Most of cases causes respiratory diseases as the target receptors of the virus are on the lungs cells.\n\nCoVs can infect several animal species as well, for example, SARS-CoV infected civet cats and infected humans in 2002 and MERS-CoV is found in dromedary camels and infected humans in 2012.\n\nA virus that is regularly transmitted from an animal to a human is called a zoonotic virus. One of the tasks is to clearly know which is the **zoonotic** origin in order to prevent more contagions in the future, as the zoonotic source of SARS-Cov-2 is currently unknown. The first human cases of COVID-19, the coronavirus disease caused by SARS-CoV-2, were first reported from Wuhan City, China, in December 2019.\n\nWhen a virus passes from animals to humans for the first time it is called a *spillover event.*","aeef8d1c":"**NOTA BENE:** The previous section of the code runs well, BUT it takes a lot of time!","10046c9c":"**\n5.1 CLUSTERING for *coronaviridae* family**","f85679ed":"\n\n**COVID-19: Genetics, Origin and Evolution**\n\n**1. Project:**\n\nIn this notebook, we try to answer the questions about the genetics of the Covid-19 illness virus (SARS-CoV-2, also called, at the beginning 2019-nCov, with n for novel ), its origins and evolution.\n\nThe idea is to use **Natural Language Processing** tools in order to help to organize better the documents for researchers so they can acces easier and faster the relevant information. We will try to bring answers to every of the proposed subtasks and even we'll try to bring new approaches and tools. \n\nWe find that to have visual tools are more helpful, so we will try to develop some interesting scripts that way, if that is possible and we have time enough.\n\nFinally, we will try to apport a personal point of view for new problematics and how to manage them.\n\n**Programming Language:** We will foccus on Python. \n\n**Disclaimer:** The programmer is still learning natural language processing throught python, therefore, it will use a lot of the previous notebooks developped in the comunity. Obviously, each other notebook used will be cited.","71536bbd":"**4.7.3 VISUALIZATION**\n\nHere is when we decide to define the Entinty as GENETICS ","7a49fbe1":"**NOTA BENE**\n\nThe firsts versions of the code will be handled for English. The papers of the database CORD-19 are in multiple languages, however, for now, we will focus only on english. Later, we may extend it to other languages.","472f0480":" - Similar virus:\n\nCoVs belong to the family\u00a0Coronaviridae, which comprises a group of enveloped, positive-sensed, single-stranded RNA viruses\u00a01,2. These viruses harbouring the largest genome of 26 to 32 kilobases amongst RNA viruses were termed \u201cCoVs\u201d because of their crown-like morphology under electron microscope\u00a02,3. \n\nStructurally, CoVs have non-segmented genomes that share a similar organization.\n\nApproximately two thirds of the genome contain two large overlapping open reading frames (ORF1a and ORF1b), which are translated into the pp1a and pp1ab replicase polyproteins.    \n   ","fa9275af":"**3. CONTEXT** \n\n***Why foccus on the genetics?***\n\nEven if all the tasks are about important aspects about the virus, and any kind of information about it interesting, we consider that to understand the origin of the virus is probably one of the most interesting and vital parts of the problem, as it gives the key not only to fight the virus but also to prevent other outbreaks or similar illnesses; and at the same time it's a complex matter that most of time lacks information.\n\n*Pulmonary system*\n\nThe human pulmonary system is vulnerable to infections due to contact-based inoculation of infectious material in droplets through the eyes, nose, or mouth, and airborne transmission is effective as seen e.g. in the plethora of viral respiratory diseases affecting individuals of all age groups. \n\nThus, respiratory viruses pose a continuous pandemic threat, of which coronaviruses and specifically the genus Betacoronavirus in the family Coronaviridae is a subset.\n\nDespite recent efforts in basic and translational influenza and coronavirus research, there is still no vaccine against coronaviruses for use in humans (this includes SARS and MERS).\n","e66bb87b":"These two plots give us a good idea of the content we are dealing with. Most papers are about 5000 words in length. The long tails in both plots are caused by outliers. In fact, ~98% of the papers are under 20,000 words in length while a select few are over 200,000! <br><br>","32475380":"When the programmer realized it didn't match, he decided to implement an helper to read the files. We are going to directly use that helper, but extend it, as suggested, so it can bring us the information we are interested in.","fd9626aa":"**4.7. NERS= Named Entity Recognition **\n\n*What is it? *\n\nNamed-entity recognition (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n\n*Why we use it for?*\n\nThe main idea is to identify the sentences that are most related with the foccus of our work, which means, that the genetic aspect of the virus. \n\n*How are we going to extract them?*\n\nWe are going to use the Scispacy proposed model, and two notebooks as an inspiration: \n\n> Using NER model to extract Bio relate sentence \n\n> BIO-NER on COVID 19 data\n\n*Use of ipywidget for visualization?* \n\n*-> What is the Ipywidget?*\n\nWidgets exist for displaying integers and floats, both bounded and unbounded. The integer widgets share a similar naming scheme to their floating point counterparts. By replacing Float with Int in the widget name, you can find the Integer equivalent.\n\nI have not decided if in the end I will use the ipywidget, but for now I am going to put it here. ","8328b59a":"![cosine-similarity.png](attachment:cosine-similarity.png)","fd40a11f":"Applying the text-processing function on the body_text.","3361e9dc":"**Nagoya protocol**\n\nThe Nagoya Protocol on Access to Genetic Resources and the Fair and Equitable Sharing of Benefits Arising from their Utilization to the Convention on Biological Diversity, also known as the Nagoya Protocol on Access and Benefit Sharing (ABS) is a 2010 supplementary agreement to the 1992 Convention on Biological Diversity (CBD). \n\nIts aim is the implementation of one of the three objectives of the CBD:\n- the fair and equitable sharing of benefits arising out of the utilization of genetic resources\n- thereby contributing to the conservation \n- sustainable use of biodiversity","90c365e9":"We need to etablish a list for each subtask about the words we are going to use as keys. \n\n4.6.1 Extraction of keywords about the genetics of the virus **(subtask 1)**\n\n4.6.2 Extraction of keywords related to the geographical distribution of the virus **(subtask 2)**\n\n4.6.3 Extraction of infection-related keywords in farmers **(subtask 3.1)**\n\n4.6.4 Extraction of keywords related to the other coranaviruses in East Asia **(subtask 3.2)\n\n4.6.5 Extraction of keywords related to experimental infections **(subtask 3.3)\n\n4.6.6 Extraction of keywords indicating a zootopic origin **(subtask 4)\n\n4.6.7 Extraction of keywords related to animal animals **(subtask 3)\n\n4.6.8 Extraction of risk-related keywords **(subtask 5)\n\n4.6.9 Extraction of keywords related to reduction strategies **(subtask 6)","a5bd1cd1":"**NOTA BENE:** The below code runs pretty well, but despite the fact that we make it shorter for our purpose, it takes a lot of time to be run. ","7c80481d":"***4.5 DIFFERENT LANGUAGES***","044a6848":"**4.8  Automatic Generation of Sumaries**\n\n*What is it?*\n\nAutomatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\n\n\n*What we use it for?*\n\nAs you might have seen, the genetic question implies a lot of subtasks, that should be answered differently, as there are different topics and the amount of information is hughe. \n\nTherefore,the automatic generation of summaries allows us to foccus on each subtasks, and extract specific answers. More important, if well done  it can be used for other tasks. ","03f9d0f2":" - Genetics: \n    \nCoVs are divided into four genera: alpha-, beta-, gamma- and delta-CoV. \n\nAll CoVs currently known to cause disease in humans belong to the **alpha- or the beta-CoV** .\n\nCoronaviruses are envelopped, positive-stranded RNA viruses of mammals and birds. These viruses have high mutation and gene recombination rates, making them ideal for pathogen evolution.\n\nThe genetic material of a virus is either RNA or DNA. A virus that has RNA as its genetic material is called an RNA virus, while that having DNA as its genetic material and replicates using a DNA- dependent DNA polymerase is termed a DNA virus.\n\nThe polyproteins are further processed to generate 16 non-structural proteins, designated nsp1~16. The remaining portion of the genome contains ORFs for the structural proteins, including spike (S), envelope (E), membrane (M) and nucleoprotein (N). A number of lineage-specific accessory proteins are also encoded by different lineages of CoVs\u00a0.\n\n*Emerging and re-emerging viral diseases have recently attracted worldwide attention. Ebola, zika, H5N1 avian influenza, severe acute respiratory syndrome (SARS), and many other emerging viral diseases have proved fatal and caused worldwide concern .*\n\nCommon attributes of emerging viral diseases include unpredictability, high morbidity, and potential for the rapid spread of diseases which may lead to substantial social impacts. The recent Ebola crisis and the outbreak of Zika virus have indicated that the world is unprepared to address emerging viral diseases.\n ","1dc83582":"***4.3 Loading the data**\n*\n\nWe are going to use three datasets that we find interesting for our purpose. Input data files are available in the \"..\/input\/\" directory. Ours are:\n\n  1-  CORD-19-research challenge`\n  \nThe principal one, that will be related to the root_path, is the database from the challenge. If we manage to use it as we want, we may also use the following ones:\n\n  2-  uncover\n    \n  3-  covid19-genomes\n    \nBut for the moment we are going to foccus on 1.\n\nWe also add kaggle-resources to the input. \n\nFor the loading we are going to use the notebook by Ivan Ega Pratama, from Kaggle:\n\n> Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool","41ae3eeb":"**FROM THIS POINT WE HAVE TO CHANGE THINGS**","d78ebb4f":" **Taks 10: What do we know about virus genetics, origin, and evolution?**\n\nSUBTASKS:\n\n**Subtask 1**: Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\n\n**Subtask 2:** Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\n","e3f33a1e":"**4.4.3 MORE PRE-PROCESSING**","4769b454":"This helper gives us two key errors, in the abstract and body text parts. In order to overload it, we have to change the code. ","a3d20f99":"We observe that loading the data gives us the original structure of it how it is presented. \n\nSome of the columns are interesting, but others are not. \n\nIn the original code, the programmer fetchs all the JSON file paths and checks the JSON structure.\n","822b0f94":"**4.4.2 Duplicates**\n\n> Based in Desmond Yeoh's notebook on Kaggle","eb314389":"***4.6. KEYWORD EXTRACTION FOR THE MOST RELEVANT PAPERS***\n\nFor the keywords extraction, we are going to use two searching tools from the kaggle competition. \n\n* The first one is the \n> COVIZ from the Allen isntitute: \n\nIf we look at the the words related with \"coronavirus\" we find the following list of genetic terms. \n\n* 'SARS-Cov RNA'\n* 'RNA dependency'\n* 'duplex RNA'\n* 'non-structural'\n* 'nsP13 molecular'\n* '5'terminal'\n* 'MGHHHHHHGS tag sequence'\n\n* And the second one is  \n> TOPIC FOREST GENERATOR from Kaggle competition\n\nthat help to visualize the dependency of the words. ","73b0ecba":"**4.7.2 KEY SENTENCE**\n\n* First we load the model","6c49b558":"**Subtask 3 :**\n\nEvidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.\n\n    -Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\n    -Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\n    -Experimental infections to test host range for this pathogen\n\n**Subtask 4 :** Animal host(s) and any evidence of continued spill-over to humans\n\n**Subtask 5** : Socioeconomic and behavioral risk factors for this spill-over\n\n**Subtask 6** : Sustainable risk reduction strategies","ba33beec":"*What is a similarity matrix ? *\n\nA similarity matrix is a matrix of scores which express the similarity between two data points. Similarity matrices are strongly related to their counterparts, distance matrices and substitution matrices. \n\nOur matrix will be using cosine similarity, that will be used to find  find similarity between **sentences.\n**\n\n*Cosine similarity:*\n\nCosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0\u00b0 is 1, and it is less than 1 for any angle in the interval (0, \u03c0] radians.","4e1f8f0f":"\n**5.4 Kids and coronavirus**\n\nWe are intrigued by the fact that kids doesn't seem very touched by the illness. As it might be an important clue into the resolution, we want to developp a code in order to try to clarify a bit the situation.","dbe02079":"![image.png](attachment:image.png)","2fe569af":"![coronavirus-360.jpg](attachment:coronavirus-360.jpg)","56d49ed7":"![1-s2.0-S1931312817300707-fx1.jpg](attachment:1-s2.0-S1931312817300707-fx1.jpg)","ccc9f3cc":"**4.3.2. DATAFRAME**\n\nNow, we must load the content into our dataframe. \n\nJust a quick reminder of what a dataframe is. \n\nDataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. In order to use it, is important to have the ***pandas*** library. \n\nIn the notebook we are using as a reference, they add a word count column, but we are not particularly interested for now in that information, so we are going skip that step. Same for boxplot. On another hand, the cleaning of the duplicate is **fundamental**. \n\nOn another hand, for this a more accurate selection foccused on the genetic aspect, we are going to delete the author's information, which doesn't really brings us important information, BUT, we are going to highlight the journal, as if it is foccused on genetic ones. Therefore, we are going to put that element on the first place. \n\nFinally, about the abstract, we will not put any of them, the abstract for being to long and the abstract summary because we prefer to implement our own summarization as already explained in the goals section.\n\nThe headers presentations is going to be like this: \n\n        journal > title > body_text >doi >paper_id. \n\n**What is the *doi*?**\n\nA DOI, or Digital Object Identifier, is a string of numbers, letters and symbols used to permanently identify an article or document and link to it on the web. A DOI will help your reader easily locate a document from your citation.","b3e4d1c1":"**4.4.1 Adding word count **\n\nThe idea is to have a word count that give us the amount of words of each abstract and body text. Why?\nBecause taht will be useful later, as kaggle has limits and we have to fit to them. ","3dc33097":"**4. CONTENT**\n\n\n**\n *4.1 Index* \n**\n\n      Part A: General N. L. Processing\n      \n    -Extraction of the most relevant papers\n    \n    -Name Entinty Recognition for keywords\n    \n    -Automatic generation of summarys\n       \n       Part B: Our own ideas\n       \n    -Others: \n    \n        * Own dictionary of genetic terms: when this is done, we'll have to come back to part A and use that dictionary!!\n        \n        * clustering method foccused on genetics (so we do not have articles we are not interested in)\n        \n        * Graphic estimations \n        \n        * What can we say about....? ","092497cc":"**4.7.1 THE FILES**","dc3dd093":"![480px-NagoyaProtocol.svg.png](attachment:480px-NagoyaProtocol.svg.png)","eab49d22":"As explained before, not all the texts are in english,but we decided to begin with those and then eventually to include other languages. That is why we have to select those in english and then \"drop\" the rest. We are going to use the notebook: \n\n> Dataset Parsing Code | Kaggle, COVID-19 Literature Clustering ","19249164":"Now we generate the summary method. ","28c5a48f":"**5.5 Smokers and coronavirus**\n\nWe also ask ourselves why apparently, smokers seem to be less affected by coronavirus. Is there a causality or mere correlation? Can nicotine play a role in stopping coronavirus?","e197fedb":"Furthermore, the helper function needs a break reach to certain amount.\n\nThis is for the interactive plot so that hover tool fits the screen.","be482dff":"* Then we add the code","58408924":"Let's take a look at word count in the papers - seaborn","cc3a0dd2":"***4.4. ADDING THINGS TO RENDERIZE THE CODE***\n\nIt happends to be that there are unique values, but there is probable that we have duplicates.\nThose duplicates cab be caused by diverse reasons, as for example the author submiting the article to multiple journals. Therefore is imperative to remove the duplicats from our dataset.","d3a56545":"**4.5.2 USING THE BIOPARSER TO DETERMINE THE STOPWORDS **\n\n*What is a bioparser?*\n\nA tool for processing of sequence similarity analysis reports.In this case, we want to determine the stopwords, which are fundamental for the text tokenization. The idea is to find them in the text and then remove them, because they represent the common words that introduce noise we want to avoid. \n","202d8e1b":"**2. Call to action:**\n\nThis project is part of the Kaggle Artificial Intelligence competition about the CORD-19: Covid-19 Open Research Database Challenge.\n\nThis was set uo by the White House and a coalition of leading research groups (AI2, CZI, MSR, Georgetown, NIH) as a response to the pandemic emergency.\n\nCORD-19 is a resource of over 51,000 scholarly articles, including over 40,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.\n\nThis freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease.\n\nThere is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.","78eddbaa":"*Risk factors*\n    -Hypertension \n    -Diabetes \n    -Lung \n    -Pathologies \n    -Heart disease \n    -Advanced age with a picture of previous associated pathologies \n    -Inmunosuppressed people","6f74f5ea":"**7. Bibliography and references** (this section will be completed as the project is developped by the end of the competition)\n\n*BOOKS*\n\n> LTK ESSENTIALS by Nitin Hardeniya \n \n > https:\/\/www.nltk.org\/install.html\n\n*VIDEOS*\n> Natural Language processing with Python by sentdex  https:\/\/www.youtube.com\/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL\n\n> Adding Image to Notebook on Kaggle by Michael Shoemaker https:\/\/www.youtube.com\/watch?v=dlRzzDpkFRk\n\n*NOTEBOOKS FROM KAGGLE*\n\n> https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clusteringhttps:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\n\n> https:\/\/www.kaggle.com\/docs\/notebooks\n\n> https:\/\/www.kaggle.com\/data\/46091\n\n> https:\/\/www.kaggle.com\/dansbecker\/finding-your-files-in-kaggle-kernels\n\n> https:\/\/www.kaggle.com\/patriciacanton\/covid-eda-initial-exploration-tool-understanding\/edit\n\n> https:\/\/www.kaggle.com\/dansbecker\/finding-your-files-in-kaggle-kernels\n\n> https:\/\/www.kaggle.com\/solonick\/kaggle-resources\n\n> https:\/\/www.kaggle.com\/shanmukha99\/bio-ner-on-covid-19-data\/comments\n\n\n*OTHER WEBSITES*\n\n> https:\/\/ipywidgets.readthedocs.io\/en\/stable\/examples\/Widget%20List.html\n\n>  https:\/\/www.nzta.govt.nz\/planning-and-investment\/national-land-transport-programme\/about-the-nltp\/\n\n> https:\/\/www.researchgate.net\/publication\/338916820_Recent_discovery_and_development_of_inhibitors_targeting_coronaviruses\/figures?lo=1&utm_source=google&utm_medium=organic\n\n> https:\/\/news.ucsc.edu\/2020\/02\/coronavirus-genome.html\n\n>https:\/\/viralzone.expasy.org\/30?outline=all_by_species\n\n> https:\/\/journals.plos.org\/plospathogens\/article?id=10.1371\/journal.ppat.1005883\n\n> https:\/\/www.learntek.org\/blog\/named-entity-recognition-with-nltk\/\n\n> https:\/\/towardsdatascience.com\/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n\n> https:\/\/www.quora.com\/How-can-I-extract-keywords-from-a-document-using-NLTK\n\n> https:\/\/stackoverflow.com\/questions\/11406657\/python-nltk-keyword-extraction-from-sentence\n\n> https:\/\/dzone.com\/articles\/how-nlp-is-automating-the-complete-text-analysis-p-1\n\n> https:\/\/www.nlp-techniques.org\/what-is-nlp\/nlp-techniques-section-summary\/\n\n> https:\/\/dzone.com\/articles\/how-nlp-is-automating-the-complete-text-analysis-p-1\n\n> https:\/\/github.com\/dair-ai\/nlp_paper_summaries\/tree\/master\/Interpretability%20and%20Analysis%20of%20Models%20for%20NLP\n","f8c65286":"STILL NEED TO STUDY THIS PART BECAUSE I LOST THE ORIGINAL CODIFICATION -I HAD A LITTLE PROBLEM WITH MY COMPUTER - SO I HAVE TO ADAPT EVERYTHING AGAIN. ","5508da4b":"    **PART B**\n    \n**5. MORE IDEAS: **\n\n**5.1 Definition of our own dictionary of genetic terms**\n\nFor the subtask 1,we have determined that it would be more accurate to etablish our own dictionary of genetics terms as they are very precise terms that sometimes are confusing and mistaken so that would be more efficient than to apply step 1 (extraction of keywords) to that task.\n\nThis idea was born based on the previous notebook: \n\n> @inproceedings{COVID-19 Literature Clustering, \nauthor = {Eren, E. Maksim. Solovyev, Nick. Nicholas, Charles. Raff, Edward},\ntitle = {COVID-19 Literature Clustering}, year = {2020}, month = {April}, \nlocation = {University of Maryland Baltimore County (UMBC), Baltimore, MD, USA},\nnote={Malware Research Group}, \nurl = {\\url{https:\/\/github.com\/MaksimEkin\/COVID19-Literature-Clustering}}, howpublished = {TBA} }\n","93efd8b4":"**4.3.1 METADATA**\n","49e0dd4e":"Now we foccus on the UNIQUE terms. ","5064544e":" - Infections\n\nAt this stage, it is not possible to determine precisely how humans in China were initially infected with SARS-CoV-2. \n\nHowever, all available evidence suggests that SARS-CoV-2 has a natural animal origin and is not a manipulated or constructed virus. SARS-CoV-2 virus most probably has its ecological reservoir in bats.\nSARS-CoV, the virus that caused the SARS outbreak in 2003 and probably also had its ecological reservoir in bats, jumped from an animal reservoir (civet cats, a farmed wild animal) to humans and then spread between humans. In a similar way, it is thought that SARS-CoV-2 jumped the species barrier and initially infected humans from another animal host. \n\nSince there is usually very limited close contact between humans and bats, it is more likely that transmission of SARS-CoV-2 to humans happened through an intermediate host, that is another animal species more likely to be handled by humans. This intermediate animal host could be a domestic animal, a wild animal, or a domesticated wild animal and, as of yet, has not been identified.","bf62ebc3":"Using the helper functions, let's read in the articles into a DataFrame that can be used easily:","44b67db2":"Now the above stopwords are used in everyday english text. Research papers will often frequently use words that don't actually contribute to the meaning and are not considered everyday stopwords.\n\nThank you Daniel Wolffram for the idea.\n\n> Cite: Custom Stop Words | Topic Modeling: Finding Related Articles","183eb8fe":"![1-s2.0-S1359644620300416-gr1.jpg](attachment:1-s2.0-S1359644620300416-gr1.jpg)","44f541aa":"**4.5.3 TEXT DATA PROCESSING** \n\n*SPACY*\n\nThis function will convert text to **lower case**, **remove punctuation**, and find and **remove stopwords**. \n\nFor the parser, we will use en_core_sci_lg. This is a model for processing biomedical, scientific or clinical text.","138b84f3":"**4.5.1. Listing the files of the directory (checking our code)**","91ace819":"![Figure_03-ESC%20COVID%20Guidance_v2_cut%20off_escardio-hPhotoMedium.jpg](attachment:Figure_03-ESC%20COVID%20Guidance_v2_cut%20off_escardio-hPhotoMedium.jpg)\n","48c83808":"First step is to decide which type of summarization we want to implie. Actually, there are two kinds of summarization, abstractive and extractive. \n\n","0895dccb":"\n\n**6. CONCLUSIONS**\n\n\n\n\n\n\n\n\n*6.1. Pro and Cons*\n\n\n\n\n\n\n*6.2 Computational efficiency*\n\n","540648d6":"**Dataset Description:** \n\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 51,000 scholarly articles, including over 40,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.\n\n >Cite: COVID-19 Open Research Dataset Challenge (CORD-19) | Kaggle Kaggle Submission: COVID-19 Literature Clustering | Kaggle","625c6c49":"**5.3 GRAPHIC ESTIMATIONS**\n\nWe aim to add graphic estimations, as they are more precise and easy to visualize for scientifics. They allow to understand an high amount of information in little time. The parameters we are going to be working at are the followings:\n\n**Sanitary people infected**\n\n*Why this parameter?*\n\nBecause sanitary profesional are the people who are the most exposed to this illness and a lot of times they are forgotten in the estimations. But to have a clear, visual idea of the percentage of sanitary people that are infected (or could be infected) gives us an idea of how well the country is dealing with the spreading and can help to prevent too many uncontrolled new infections. In the end, the less the healthcare professionals are affected, the better the sanitary answer will be. \n\n**Contagion (or infection) rate**\n\n*What is it?*\n\nAn infection rate is the probability or risk of an infection in a population. It is used to measure the frequency of occurrence of new instances of infection within a population during a specific time period.\n\n*Why this parameter?*\n\nThe answer seems obvious, it allows to follow the spread of the virus worldwide.\n\n**Mortality and Lethality**\n\n*What are they?*\n\nThose two are often mistaken, but they are not the same. \n\nThe mortality rate is an index that reflects the number of deaths per 1000 citizens in a given community over a given period of time, that is, we are referring to the proportion of deaths in a given population over a period.\n\nOn another hand, the lethality is a rate that refers to the ratio of deaths in relation to people who have been infected with the disease, the result of which is usually shown in percentages.\n\n*Why this parameters?*\n\nThis parameters are important and generally mistaken, but they are important to have a real idea of what is actually going on, that is why we foccus on them. ","d7886c90":"![Diapositiva1.jpg](attachment:Diapositiva1.jpg)"}}