{"cell_type":{"c0f16bf7":"code","6a18d240":"code","ba4d955a":"code","2ca3c756":"code","3ead5436":"code","7fa26eed":"code","abb1346e":"code","82489bca":"code","d5d32f7c":"code","b562270b":"code","a1f661e9":"code","a61063b1":"code","11d33128":"code","53f85b0f":"code","8f062aed":"code","06a2fcf3":"code","ed311ddd":"code","d408d0a9":"code","813441e5":"code","10fd9561":"code","6c481635":"code","d08ba0ce":"markdown","8766cc76":"markdown","c9aeda4b":"markdown","028f4123":"markdown","a5056cd0":"markdown","53d0ee55":"markdown","d6cec2ad":"markdown","6c328280":"markdown","8de590bb":"markdown","7a66ce64":"markdown","e28cc6e4":"markdown","81bcc990":"markdown","03f4252b":"markdown","9bce8b6e":"markdown","6f2d7dd9":"markdown","b908a836":"markdown","c9ae23a4":"markdown","6ff6330b":"markdown","7872b0a7":"markdown","ba2381dc":"markdown","40d85b48":"markdown","59e0e4f4":"markdown","934fd0c9":"markdown","49c7aeb0":"markdown","9df6ac56":"markdown","224fbe85":"markdown","1f418ff7":"markdown","5bd3ffef":"markdown","9902f85c":"markdown","b9645407":"markdown","6e9b21d6":"markdown","7512b204":"markdown","b2c06103":"markdown","26fbb59f":"markdown"},"source":{"c0f16bf7":"# Data preprocessing\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nfrom glob import glob\nfrom PIL import Image\nimport os\nimport random\nimport cv2\n#Model\nimport keras\nfrom keras.models import Sequential, Model,load_model\nfrom keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPooling2D,MaxPool2D,AveragePooling2D,GlobalMaxPooling2D\nfrom keras import backend as K\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras import regularizers\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.image import ImageDataGenerator,array_to_img\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping,ModelCheckpoint\nfrom keras.metrics import PrecisionAtRecall,Recall \n#Model Analysis\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\nfrom numpy.random import seed\nseed(0)\nimport tensorflow\ntensorflow.random.set_seed(0)","6a18d240":"path = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/'\n\ndiag_code_dict = {\n    'COVID': 0,\n    'Lung_Opacity': 1,\n    'Normal': 2,\n    'Viral Pneumonia': 3}\n\ndiag_title_dict = {\n    'COVID': 'Covid-19',\n    'Lung_Opacity': 'Lung Opacity',\n    'Normal': 'Healthy',\n    'Viral Pneumonia': 'Viral Pneumonia'}\n\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in glob(os.path.join(path, '*','*.png'))}\n\ncovidData = pd.DataFrame.from_dict(imageid_path_dict, orient = 'index').reset_index()\ncovidData.columns = ['image_id','path']\nclasses = covidData.image_id.str.split('-').str[0]\ncovidData['diag'] = classes\ncovidData['target'] = covidData['diag'].map(diag_code_dict.get) \ncovidData['Class'] = covidData['diag'].map(diag_title_dict.get) ","ba4d955a":"samples,features = covidData.shape\nduplicated = covidData.duplicated().sum()\nnull_values = covidData.isnull().sum().sum()\n\nprint('Basic EDA')\nprint('Number of samples: %d'%(samples))\nprint('Number of duplicated values: %d'%(duplicated))\nprint('Number of Null samples: %d' % (null_values))","2ca3c756":"#Samples per class\nplt.figure(figsize=(20,8))\nsns.set(style=\"ticks\", font_scale = 1)\nax = sns.countplot(data = covidData,x='Class',order = covidData['Class'].value_counts().index,palette=\"flare\")\nsns.despine(top=True, right=True, left=True, bottom=False)\nplt.xticks(rotation=0,fontsize = 12)\nax.set_xlabel('Sample Type - Diagnosis',fontsize = 14,weight = 'bold')\nax.set(yticklabels=[])\nax.axes.get_yaxis().set_visible(False) \nplt.title('Number of Samples per Class', fontsize = 16,weight = 'bold');\n#Plot numbers\nfor p in ax.patches:\n    ax.annotate(\"%.1f%%\" % (100*float(p.get_height()\/samples)), (p.get_x() + p.get_width() \/ 2., abs(p.get_height())),\n    ha='center', va='bottom', color='black', xytext=(0, 10),rotation = 'horizontal',\n    textcoords='offset points')","3ead5436":"covidData['image'] = covidData['path'].map(lambda x: np.asarray(Image.open(x).resize((75,75))))","7fa26eed":"#Image Sampling\nn_samples = 3\n\nfig, m_axs = plt.subplots(4, n_samples, figsize = (4*n_samples, 3*4))\n\nfor n_axs, (type_name, type_rows) in zip(m_axs,covidData.sort_values(['diag']).groupby('diag')):\n    n_axs[1].set_title(type_name,fontsize = 14,weight = 'bold')\n    for c_ax, (_, c_row) in zip(n_axs, type_rows.sample(n_samples, random_state=1234).iterrows()):       \n        picture = c_row['path']\n        image = cv2.imread(picture)\n        c_ax.imshow(image)\n        c_ax.axis('off')","abb1346e":"plt.figure()\npic_id = random.randrange(0, samples)\npicture = covidData['path'][pic_id]\nimage = cv2.imread(picture)\nplt.imshow(image)\nplt.axis('off');\nplt.show()      ","82489bca":"print('Shape of the image : {}'.format(image.shape))\nprint('Image Hight {}'.format(image.shape[0]))\nprint('Image Width {}'.format(image.shape[1]))\nprint('Dimension of Image {}'.format(image.ndim))\nprint('Image size {}'.format(image.size))\nprint('Image Data Type {}'.format(image.dtype))\nprint('Maximum RGB value in this image {}'.format(image.max()))\nprint('Minimum RGB value in this image {}'.format(image.min()))","d5d32f7c":"image[0,0]","b562270b":"plt.title('B channel',fontsize = 14,weight = 'bold')\nplt.imshow(image[ : , : , 0])\nplt.axis('off');\nplt.show()","a1f661e9":"mean_val = []\nstd_dev_val = []\nmax_val = []\nmin_val = []\n\nfor i in range(0,samples):\n    mean_val.append(covidData['image'][i].mean())\n    std_dev_val.append(np.std(covidData['image'][i]))\n    max_val.append(covidData['image'][i].max())\n    min_val.append(covidData['image'][i].min())\n\nimageEDA = covidData.loc[:,['image', 'Class','path']]\nimageEDA['mean'] = mean_val\nimageEDA['stedev'] = std_dev_val\nimageEDA['max'] = max_val\nimageEDA['min'] = min_val\n\nsubt_mean_samples = imageEDA['mean'].mean() - imageEDA['mean']\nimageEDA['subt_mean'] = subt_mean_samples","a61063b1":"ax = sns.displot(data = imageEDA, x = 'mean', kind=\"kde\");\nplt.title('Images Colour Mean Value Distribution', fontsize = 16,weight = 'bold');\nax = sns.displot(data = imageEDA, x = 'mean', kind=\"kde\", hue = 'Class');\nplt.title('Images Colour Mean Value Distribution by Class', fontsize = 16,weight = 'bold');\nax = sns.displot(data = imageEDA, x = 'max', kind=\"kde\", hue = 'Class');\nplt.title('Images Colour Max Value Distribution by Class', fontsize = 16,weight = 'bold');\nax = sns.displot(data = imageEDA, x = 'min', kind=\"kde\", hue = 'Class');\nplt.title('Images Colour Min Value Distribution by Class', fontsize = 16,weight = 'bold');","11d33128":"plt.figure(figsize=(20,8))\nsns.set(style=\"ticks\", font_scale = 1)\nax = sns.scatterplot(data=imageEDA, x=\"mean\", y=imageEDA['stedev'], hue = 'Class',alpha=0.8);\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.xticks(rotation=0,fontsize = 12)\nax.set_xlabel('Image Channel Colour Mean',fontsize = 14,weight = 'bold')\nax.set_ylabel('Image Channel Colour Standard Deviation',fontsize = 14,weight = 'bold')\nplt.title('Mean and Standard Deviation of Image Samples', fontsize = 16,weight = 'bold');","53f85b0f":"plt.figure(figsize=(20,8));\ng = sns.FacetGrid(imageEDA, col=\"Class\",height=5);\ng.map_dataframe(sns.scatterplot, x='mean', y='stedev');\ng.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\", size = 16)\ng.fig.subplots_adjust(top=.7)\ng.fig.suptitle('Mean and Standard Deviation of Image Samples',fontsize=16, weight = 'bold')\naxes = g.axes.flatten()\naxes[0].set_ylabel('Standard Deviation');\nfor ax in axes:\n    ax.set_xlabel('Mean')\ng.fig.tight_layout()","8f062aed":"def getImage(path):\n    return OffsetImage(cv2.imread(path),zoom = 0.1)\n\nDF_sample = imageEDA.sample(frac=0.1, replace=False, random_state=1)\npaths = DF_sample['path']\n\nfig, ax = plt.subplots(figsize=(20,8))\nab = sns.scatterplot(data=DF_sample, x=\"mean\", y='stedev')\nsns.despine(top=True, right=True, left=False, bottom=False)\nax.set_xlabel('Image Channel Colour Mean',fontsize = 14,weight = 'bold')\nax.set_ylabel('Image Channel Colour Standard Deviation',fontsize = 14,weight = 'bold')\nplt.title('Mean and Standard Deviation of Image Samples - 10% of Data', fontsize = 16,weight = 'bold');\n\nfor x0, y0, path in zip(DF_sample['mean'], DF_sample['stedev'],paths):\n    ab = AnnotationBbox(getImage(path), (x0, y0), frameon=False)\n    ax.add_artist(ab)","06a2fcf3":"#add the path general where the classes subpath are allocated\npath = '\/kaggle\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset'\n\nclasses=[\"COVID\", \"Lung_Opacity\", \"Normal\", \"Viral Pneumonia\"]\nnum_classes = len(classes)\nbatch_size = 16\n\n#Define the parameters to create the training and validation set Images and Data Augmentation parameters\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   rotation_range=20,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   horizontal_flip=True,\n                                   validation_split=0.2)\n\n#**No Augmentation on the Test set Images**\ntest_datagen = ImageDataGenerator(rescale=1.\/255, \n                                  validation_split=0.2)\n\n\n#loading the images to training set\ntrain_gen = train_datagen.flow_from_directory(directory=path, \n                                              target_size=(299, 299),\n                                              class_mode='categorical',\n                                              subset='training',\n                                              shuffle=True, classes=classes,\n                                              batch_size=batch_size, \n                                              color_mode=\"grayscale\")\n#loading the images to test set\ntest_gen = test_datagen.flow_from_directory(directory=path, \n                                              target_size=(299, 299),\n                                              class_mode='categorical',\n                                              subset='validation',\n                                              shuffle=False, classes=classes,\n                                              batch_size=batch_size, \n                                              color_mode=\"grayscale\")","ed311ddd":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=(299, 299, 1)))\nmodel.add(BatchNormalization())\n##############################\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(AveragePooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(AveragePooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n##############################\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(AveragePooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(AveragePooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n##############################\nmodel.add(Flatten())\n\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n#Output\nmodel.add(BatchNormalization())\nmodel.add(Dense(num_classes, activation='softmax'))\n#Compile the model\nopt = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[Recall()])","d408d0a9":"#Model Parameters\nepochs = 1000\n\n#Callbacks\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=10, verbose=0, factor=0.5, min_lr=0.00001)\nearly_stopping_monitor = EarlyStopping(patience=100,monitor='val_loss', mode = 'min',verbose=0)\n\ncallbacks_list = [learning_rate_reduction,early_stopping_monitor]\n\n#Verbose set to 0 to avoid Notebook visual pollution\nhistory = model.fit(train_gen, steps_per_epoch=len(train_gen) \/\/ batch_size, \n                                validation_steps=len(test_gen) \/\/ batch_size, \n                                validation_data=test_gen, epochs=epochs,callbacks=[callbacks_list],\n                                verbose=0)\n#Predict Results on test set\ny_pred = model.predict(test_gen)","813441e5":"#Plot training and validation Loss\nfig, axarr = plt.subplots(1,3, figsize=(15,5),sharex=True)\n\nsns.set(style=\"ticks\", font_scale = 1)\nsns.despine(top=True, right=True, left=False, bottom=False)\n\nhistoryDF = pd.DataFrame.from_dict(history.history)\nax = sns.lineplot(x =historyDF.index, y = history.history['recall'],ax=axarr[0],label=\"Training\");\nax = sns.lineplot(x =historyDF.index, y = history.history['val_recall'],ax=axarr[0],label=\"Validation\");\nax.set_ylabel('Recall')\nax = sns.lineplot(x =historyDF.index, y = history.history['loss'],ax=axarr[1],label=\"Training\");\nax = sns.lineplot(x =historyDF.index, y = history.history['val_loss'],ax=axarr[1],label=\"Validation\");\nax.set_ylabel('Loss')\nax = sns.lineplot(x =historyDF.index, y = history.history['lr'],ax=axarr[2]);\nax.set_ylabel('Learning Rate')    \naxarr[0].set_title(\"Training and Validation Set - Metric Recall\")\naxarr[1].set_title(\"Training and Validation Set - Loss\")\naxarr[2].set_title(\"Learning Rate during Training\")\n\nfor ax in axarr:\n    ax.set_xlabel('Epochs')\n    \nplt.suptitle('Training Performance Plots',fontsize=16, weight = 'bold');\nfig.tight_layout(pad=3.0)      \nplt.show()","10fd9561":"#transform the predictions into array such as [0,0,1,2...]\npredictions = np.array(list(map(lambda x: np.argmax(x), y_pred)))\n#Retrieve the True classes of the test set\ny_true=test_gen.classes\n#Build Confusion Matrix\nCMatrix = pd.DataFrame(confusion_matrix(y_true, predictions), columns=classes, index =classes)\n\nplt.figure(figsize=(12, 6))\nax = sns.heatmap(CMatrix, annot = True, fmt = 'g' ,vmin = 0, vmax = 250,cmap = 'Blues')\nax.set_xlabel('Predicted',fontsize = 14,weight = 'bold')\nax.set_xticklabels(ax.get_xticklabels(),rotation =0);\n\nax.set_ylabel('Actual',fontsize = 14,weight = 'bold') \nax.set_yticklabels(ax.get_yticklabels(),rotation =0);\nax.set_title('Confusion Matrix - Test Set',fontsize = 16,weight = 'bold',pad=20);\n\nkeras.backend.clear_session()","6c481635":"#Accuracy Result\nacc = accuracy_score(y_true, predictions)\n#Precision, Recall and F-Score (For the whole dataset)\nresults_all = precision_recall_fscore_support(y_true, predictions, average='macro',zero_division = 1)\n#Precision, Recall and F-Score (For each Class)\nresults_class = precision_recall_fscore_support(y_true, predictions, average=None, zero_division = 1)\n\n#Organise the Results into a Dataframe\nmetric_columns = ['Precision','Recall', 'F-Score','S']\nall_df = pd.concat([pd.DataFrame(list(results_class)).T,pd.DataFrame(list(results_all)).T])\nall_df.columns = metric_columns\nall_df.index = ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia','Total']\n\n#Function to plot the metrics into a nice bar Plot\ndef metrics_plot(df,metric):\n    plt.figure(figsize=(22,10))\n    ax = sns.barplot(data =df, x=df.index, y = metric,palette = \"Blues_d\")\n    #Bar Labels\n    for p in ax.patches:\n        ax.annotate(\"%.1f%%\" % (100*p.get_height()), (p.get_x() + p.get_width() \/ 2., abs(p.get_height())),\n        ha='center', va='bottom', color='black', xytext=(-3, 5),rotation = 'horizontal',textcoords='offset points')\n    sns.despine(top=True, right=True, left=True, bottom=False)\n    ax.set_xlabel('Class',fontsize = 14,weight = 'bold')\n    ax.set_ylabel(metric,fontsize = 14,weight = 'bold')\n    ax.set(yticklabels=[])\n    ax.axes.get_yaxis().set_visible(False) \n    plt.title(metric+ ' Results per Class', fontsize = 16,weight = 'bold');\n    \nmetrics_plot(all_df, 'Precision')#Results by Class\nmetrics_plot(all_df, 'Recall')#Results by Class\nmetrics_plot(all_df, 'F-Score')#Results by Class\nprint('**Overall Results**')\nprint('Accuracy Result: %.2f%%'%(acc*100)) #Accuracy of the whole Dataset\nprint('Precision Result: %.2f%%'%(all_df.iloc[4,0]*100))#Precision of the whole Dataset\nprint('Recall Result: %.2f%%'%(all_df.iloc[4,1]*100))#Recall of the whole Dataset\nprint('F-Score Result: %.2f%%'%(all_df.iloc[4,2]*100))#F-Score of the whole Dataset\n","d08ba0ce":"# 1. Overview\n\nThe coronavirus pandemic has produced thousands of causalities and affected millions of people globally. Any technological device allowing accelerated screening of the COVID-19 infection with high accuracy can be crucially helpful to healthcare professionals. \n\nAccording to Chowdhury et al. (2020),  X-ray imaging is an easily accessible tool that is used to diagnose COVID-19 cases. However, though regular Chest X-Ray (XCR) images may improve early screening of suspected cases, the images of various viral cases of pneumonia are similar. Therefore, it is difficult for radiologists to distinguish COVID-19 from other viral cases of pneumonia. \n\nThis database and ongoing research are performed to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images.\n\n# 1.1 Content\n\nIn total, there are 21165 samples divided into four main classes:\n* Covid-19\n* Lung Opacity\n* Normal\n* Viral Pneumonia\n\n\nAll the images are in Portable Network Graphics (PNG) file format and the resolution are 299x299 pixels. On this current update, the database currently holds 3,616 COVID-19 positive cases, 10,192 Normal, 6,012 Lung Opacity (Non-COVID lung infection), and 1,345 Viral Pneumonia images.\n\n# 1.2 Acknowledgments\n\nThe database creators acknowledge several institutions and researchers that have made the data collection possible. Please see [here](https:\/\/www.kaggle.com\/tawsifurrahman\/covid19-radiography-database) for the full list of colaborators.","8766cc76":">kde plots","c9aeda4b":"Below is the Model performance during training:","028f4123":"* Even though we could use our imagination to understand what it meant to have high or low mean values, the visualisation above really helps to grasp the concept. Following the X-Axis, the images have a crescent brightness increase as they present higher mean values \n* Higher standard deviations are linked to images with high contrast and a more dominant black background \n* The outliers at low standard deviation and near the 75 are from Covid-19. It seems like a little data cluster at that region\n* More insights could be available by performing the plot by class","a5056cd0":"Complementary information on Age, Gender or Weight of patients samples could be helpful to make the EDA more interesting. Not to mention, that additional information could be extremly important for the Machine Learning model.","53d0ee55":"<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n<h3>What we know so far<\/h3>\n<ul>\n<li>Peaks and Valleys of the initial training phase indicate several local optima the model has encountered\n<li>There is no expressive gain in Model Accuracy Metric after 60 epochs, as we see that the loss for the validation set stabilises after that\n<li>Training the model for longer could lead to Overfitting\n<li>Training the model for fewer epochs, we would be probably stuck in a Local Optima and fail to generalise to new samples\n<li>There is a clear link between the Learning Rate reduction and the model being able to converge to a more stable solution  \n<ul>\n<\/blockquote>","d6cec2ad":"> Add images to DF","6c328280":"# 3. CNN Model\n\nBasic considerations regarding the CNN model used:\n* Use ImageDataGenerator for Data Augmentation and organise the files into training and validation set\n* train and test_datagen have different settings. Ideally, we should not augment the validation set\n* test_datagen hyperparameter **shuffle=False** makes sure the training and validation data do not overlap\n* I used a CNN architecture that has consistently provided me reasonable results as a starting point\n* The Model predicts the Four types of X-Ray Images\n* Confusion Matrix, Accuracy, Precision, Recall and F-Score are analysed for final remarks","8de590bb":"In an RGB image, each pixel is represented by three 8 bit numbers associated with the values for Red, Green, Blue respectively. The numbers range from 0 to 255 for the three different channels.\n\nThe dataset presents the images as *.PNG*. Using the *cv2* library, the colour of the images are properly displayed.","7a66ce64":"![image.png](https:\/\/images.unsplash.com\/photo-1584036561566-baf8f5f1b144?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1189&q=80)\nPhoto by <a href=\"https:\/\/unsplash.com\/@fusion_medical_animation?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Fusion Medical Animation<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/covid?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash<\/a>\n  \n  \n\n# COVID-19 Radiography Database \n\n\n> \"Recently, several groups have reported deep machine learning techniques using X-ray images for detecting COVID-19 pneumonia ... However, most of these groups used rather a small dataset containing only a few COVID-19 samples. This makes it difficult to generalize their results reported in these articles and cannot guarantee that the reported performance will retain when these models will be tested on a larger dataset\"<br>\n[-M.E.H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M.A. Kadir, Z.B. Mahbub, K.R. Islam, M.S. Khan, A. Iqbal, N. Al-Emadi, M.B.I. Reaz, M. T. Islam, \u201cCan AI help in screening Viral and COVID-19 pneumonia?\u201d IEEE Access, Vol. 8, 2020, pp. 132665 - 132676.](https:\/\/ieeexplore.ieee.org\/document\/9144185).\n\n\nThis notebook uses ML to predict positive cases of Covid-19 given X-ray images. Since the first publication in March 2020, this dataset continues to updated fed with additional images making it an exciting ML problem.\n\n## <center style=\"background-color:Gainsboro; width:40%;\">Contents<\/center>\n1.[Overview](#1.-Overview)<br>\n1.1.[Content](#1.1.-Content)<br>\n1.2.[Acknowledgements](#1.2.-Acknowledgements)<br>\n2.[The Data](#2.-The-Data)<br>\n2.1[Image Data EDA](#2.-Image-Data-EDA)<br>\n3.[Model](#3.-Model)<br>\n4.[Results and Conclusion](#4.-Results-and-Conclusion)<br>\n\n***Please remember to upvote if you find this Notebook helpful!***","e28cc6e4":"* Most images are gathered in the central region of the scatter plot, i.e. there is not much contrast between their pixel values\n* Covid-19 samples seem to be the only class to have a small cluster of data on the bottom left side of the plot, where samples with a lower mean and low standard variation lie\n* An individual plot by class is required, as the classes are on top of each other and we might miss important details\n* We see that all classes have outliers spread around the peripheric area of the graph. It will be interesting to use visualisation to understand how the outliers look like\n\nThe plot above is crowded, let's analyse by class separately:","81bcc990":">Data import ","03f4252b":">Checking the image basic parameters","9bce8b6e":"# 4. Results and Conclusion\n\nThe results are analysed in terms of F1-Score, as Precision and Recall are both relevant metrics for this application. \n\nTo provide a general overview of the Model performance, the confusion matrix and results for the F1-Score, Precision, Recall and overall Accuracy is also presented.","6f2d7dd9":"* The Normal (Healthy) samples and Lung Opacity images have a similar scatter, with most of its outliers with higher standard deviation and lower mean values\n* Viral Pneumonia images display a more concentrated scatter, perhaps these images have higher similarity to each other than compared to the other classes\n* The Covid-19 scatter does not resemble any of the other three classes. It presents more outliers than the other classes, and the points are more scattered across the graph. It could indicate that the images have a higher distinction between each other","b908a836":"* The distribution plot of the whole dataset is very similar to the individual Healthy and Lung Opacity images, due to the number of samples of these two classes\n* Separating by class we can visualise that the Mean, Max and Min values vary according to the image class\n* Viral Pneumonia is the only class that presents a Normal-like distribution across the three different analysis\n* The Max value possible for an image is 255. Most classes peak around this number as expected\n* Viral Pneumonia is the class that present the most samples with lower Max values if compared to the others. Most samples are within the 200 - 225 range\n* Normal (Healthy) and Lung Opacity samples present a very similar distribution of their mean values. Not sure if this could be related to the fact that these classes are the most numerous of the dataset. The different peaks on the distribution could also be because of the image source (e.g. two different hospitals)\n* Regarding the Max values, Lung Opacity and Covid-19 present similar  distributions (see the \"bumps\"), while Normal patients have a peak at 150 and then another peak around 250","c9ae23a4":"* Healthy and Lung Opacity samples compose 80% of the dataset\n* For this application, the main goal is to recognise Covid-19 patients. It will be interesting to see if the model will have greater difficulty in identifying Pneumonia or Covid samples\n* Similar to other health conditions prediction problems or unbalanced datasets, it is necessary to prioritise Precision or Recall, since Accuracy can be misleading. The F1-Score is also a reasonable option","6ff6330b":"The visualisation below was inspired by a talk at [PyData 2016](https:\/\/www.youtube.com\/watch?v=GIVK0-SNUgU), by Mehrdad Yazdani with the title *Using Exploratory Data Analysis to Discover Patterns*. It really helps to make things clearer to see the images data into a chart format. The dots location of the scatter plot above are used as reference for the image locations.\n\n>Due to the time it takes to load, 10% of data is used for the visualisation. ","7872b0a7":"> Plot Samples","ba2381dc":"# That is it for now. I hope you enojoyed and found it helpful.","40d85b48":"> Model settings and training","59e0e4f4":"<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n<h3>What we know so far<\/h3>\n<ul>\n<li>Our dataset contains a reasonable number of images \n<li>No data cleansing is required\n<li>Exploratory Data Analysis is done with regards to metadata, as we do not have additional information from the patients\n<li>We can investigate image patterns and relantionships between the classes\n<li>The data is unbalanced with almost 50% of samples belongs to <em>\"Healthy\"<\/em> class. The model will probably present better performance towards these samples\n<li> Due to the Data Unbalance, it is best to use metrics such as <em>Precision<\/em>, <em>Recall<\/em> or <em>F1-Score<\/em> to measure model performance\n<ul>\n<\/blockquote>","934fd0c9":">ImageDataGenerator settings","49c7aeb0":"## 2.1 Image Data EDA\n\nIn this section, an EDA on the image data is presented. Here it is investigated any patterns\/relationships regarding the images and their respective classes.\n\nFirst, let's have a look at a random sample and extract basic information regarding the images:","9df6ac56":"Even though the images are in greyscale, they present the three channels. \n\nThe output below is an unique pixel of the **image array at [0,0]**, we see that all colour channels have the same value. \n>As a side note, OpenCV assumes the image to be Blue-Green-Red (BGR), not RGB.","224fbe85":"Preliminary tests have shown that model performance was very much impacted by the batch size, more than by the learning rate or optimiser used. \n\nBatch size is the number of samples processed before the model is updated. Larger batch sizes (32,64,128) provided a lower test set accuracy regardless of the number of epochs. A smaller batch than 16 took longer to train and yielded similar results. \n\nThe validation performance oscillates heavily in the initial epochs, i.e. from 0.00 to 90% in the following epoch. As the Learning Rate was already low, reducing it was not helpful and neither callbacks improved this behaviour. Perhaps, when using 'SGD' as an optimiser I can play with Learning Rate Scheduling. 'Adam' already has that functionality built-in. \n\nEven though I did not use any optimised hyperparameters tuning, the batch_size with a lower learning rate of 0,001 provided satisfactory results.","1f418ff7":">Import Libraries and Modules","5bd3ffef":">CNN Architecture","9902f85c":"* Overall, the model can identify the samples, i.e. there is a good amount of TP\n* Covid-19, if misclassified, can be predicted as Normal or Lung Opacity samples. Not likely to be classified as Viral Pneumonia\n* Lung Opacity is more often misclassified as Normal than as Viral Pneumonia or Lung Opacity\n* Normal samples are usually misclassified as Viral Pneumonia or Lung Opacity. Less common to be mistaken for Covid-19\n* Viral Pneumonia is the class with the fewer number of misclassifications","b9645407":"Continuing our analysis with the Mean values, now we analyse the relantionship between an image Mean value and its Standard Deviation. ","6e9b21d6":"As it was shown so far, the images are nothing more than an array of numbers in a format [Height, Width, Channel]. With that in mind, we proceed with our EDA.\n\nHere we start to examine if there is any pattern between the image colour values and their class. A distribution plot illustrates how the mean, max and min colour values are presented for the dataset.","7512b204":"<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n<h3>What we achieved so far<\/h3>\n<ul>\n<li>Covid class presents ~ 80% Precision and ~55% Recall. The result means that the model is not capable of classifying all the Covid-19 samples correctly (low Recall - high FN). However, it is usually correct when it does so (high precision - low FP)\n<li>Lung Opacity and Normal classes have similar values for Precision and Recall (above 78%), meaning the model is good at recognising these samples and properly classifying them\n<li>Normal and Viral Pneumonia class present the opposite result we see in Covid-19. They have a higher Recall than Precision. This means that the model is good at recognising these samples, i.e. lower number of FN. However, it is producing FP, i.e. as we saw in the Confusion Matrix where Normal class is usually mistaken by Viral Pneumonia or Lung Opacity.\n<li>The F-Score is the balance between Precision and Recall. As expected, the Normal class show a high score as the Precision and Recall metrics are similar. Lower results are found for Covid, as the Precision and Recall metrics differed more intensely. \n<li>Overall, it is a good outcome that all the general metrics are above 75%. Results per class should definetly be improved, especially to reduce the number of COVID samples wrongly classified as NORMAL samples\n<ul>\n<\/blockquote>","b2c06103":"# 2. The Data\n\nFirst, let's organise the data into a dataframe to group the path of all images and their respective target. By doing so it is easier to handle data transformations later on. In my opinion, this method is more straightforward than handling the images in different folders.","26fbb59f":"A visualisation of the image selecting only one of the three channels is shown next. As all channels contain the same values, the pictures are the same for the three single channels."}}