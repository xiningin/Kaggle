{"cell_type":{"a86d25f1":"code","a752abf8":"code","f238a23a":"code","83785c41":"code","381f7692":"code","d15d4910":"code","04269420":"code","0425dda1":"code","ffe03142":"code","23442c98":"code","4820618b":"code","58930b16":"code","1faff20c":"code","ea57be05":"code","b55a020f":"code","ea40403a":"code","c4257711":"code","b9dc09c8":"code","924b7b2a":"code","96ee40e8":"code","eb74eb10":"markdown","f66fe343":"markdown"},"source":{"a86d25f1":"!pip install lofo-importance","a752abf8":"import cupy as cp\nimport cudf\nimport cuml\nimport glob\nfrom tqdm import tqdm\n\ncudf.__version__","f238a23a":"PATH = \"\/kaggle\/input\/optiver-realized-volatility-prediction\"\n\n\ndef load_data(mode, path=\"\/kaggle\/input\/optiver-realized-volatility-prediction\"):\n    # mode = \"train\"\/\"test\"\n    file_name = f'{path}\/{mode}.csv'\n    return cudf.read_csv(file_name)\n\ndev_df = load_data(\"train\", path=PATH)\ndev_df.head()","83785c41":"SCALE = 100\ndev_df[\"target\"] *= SCALE\n\nstock_ids = dev_df[\"stock_id\"].unique()\nlen(stock_ids)","381f7692":"order_book_training = glob.glob(f'{PATH}\/book_train.parquet\/*\/*')\norder_book_test = glob.glob(f'{PATH}\/book_test.parquet\/*\/*')\n\nlen(order_book_training), len(order_book_test)","d15d4910":"trades_training = glob.glob(f'{PATH}\/trade_train.parquet\/*\/*')\ntrades_test = glob.glob(f'{PATH}\/trade_test.parquet\/*\/*')\n\nlen(trades_training), len(trades_test)","04269420":"%cd \/kaggle\/input\/rapids-kaggle-utils\/","0425dda1":"import cu_utils.transform as cutran\n\n\n\ndef log_diff(df, in_col, null_val):\n    df[\"logx\"] = df[in_col].log()\n    df[\"logx_shifted\"] = (df[[\"time_id\", \"logx\"]].groupby(\"time_id\")\n                             .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=null_val),\n                                            incols={\"logx\": 'x'},\n                                            outcols=dict(y_out=cp.float32),\n                                            tpb=32)[\"y_out\"])\n    df[\"keep_row\"] = df[f\"logx_shifted\"] != null_val\n    return df[\"logx\"] - df[\"logx_shifted\"]\n\n\n\ndef extract_raw_book_features(df, null_val=-9999):\n    for n in range(1, 3):\n        p1 = df[f\"bid_price{n}\"]\n        p2 = df[f\"ask_price{n}\"]\n        s1 = df[f\"bid_size{n}\"]\n        s2 = df[f\"ask_size{n}\"]\n        df[f\"wap{n}\"] = (p1*s2 + p2*s1) \/ (s1 + s2)\n        df[f\"log_return{n}\"] = log_diff(df, in_col=f\"wap{n}\", null_val=null_val)\n        df[f\"realized_vol{n}\"] = df[f\"log_return{n}\"]**2\n        \n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df[\"c\"] = 1\n    \n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef extract_raw_trade_features(df, null_val=-9999):\n    df[\"realized_vol_trade\"] = log_diff(df, in_col=f\"price\", null_val=null_val)**2\n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef agg(df, feature_dict):\n    agg_df = df.groupby(\"time_id\").agg(feature_dict).reset_index()\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    agg_df.columns = [f(x) for x in agg_df.columns]\n    return agg_df    \n\n\ndef extract_book_stats(df):\n    default_stats = [\"sum\", \"mean\", \"std\"]\n    feature_dict = {\n        'wap1': default_stats,\n        'wap2': default_stats,\n        'log_return1': default_stats,\n        'log_return2': default_stats,\n        'wap_balance': default_stats,\n        'price_spread': default_stats,\n        'bid_spread': default_stats,\n        'ask_spread': default_stats,\n        'total_volume': default_stats,\n        'volume_imbalance': default_stats,\n        'c': [\"sum\"],\n        'realized_vol1': [\"sum\"],\n        'realized_vol2': [\"sum\"],\n    }\n    \n    return agg(df, feature_dict)\n    \n\n    \n    \ndef extract_trade_stats(df):\n    feature_dict = {\n        'realized_vol_trade': [\"sum\"],\n        'seconds_in_bucket':[\"count\"],\n        'size': [\"sum\"],\n        'order_count': [\"mean\"],\n    }\n    \n    return agg(df, feature_dict)\n\n\ndef time_constraint_fe(df, stats_df, last_sec, fe_function, cols):\n    sub_df = df[df[\"seconds_in_bucket\"] >= (600 - last_sec)].reset_index(drop=True)\n    if sub_df.shape[0] > 0:\n        sub_stats = fe_function(sub_df)\n    else:\n        sub_stats = cudf.DataFrame(columns=cols)\n    return stats_df.merge(sub_stats, on=\"time_id\", how=\"left\", suffixes=('', f'_{last_sec}'))    \n    \n\ndef feature_engineering(book_path, trade_path):\n    book_df = cudf.read_parquet(book_path)\n    book_df = extract_raw_book_features(book_df)\n    book_stats = extract_book_stats(book_df)\n    book_cols = book_stats.columns\n    \n    trade_df = cudf.read_parquet(trade_path)\n    trade_df = extract_raw_trade_features(trade_df)\n    trade_stats = extract_trade_stats(trade_df)\n    trade_cols = trade_stats.columns\n    \n    for last_sec in [150, 300, 450]:\n        book_stats = time_constraint_fe(book_df, book_stats, last_sec, extract_book_stats, book_cols) \n        trade_stats = time_constraint_fe(trade_df, trade_stats, last_sec, extract_trade_stats, trade_cols) \n\n    return book_stats.merge(trade_stats, on=\"time_id\", how=\"left\")\n\n\ndef process_data(order_book_paths, trade_paths, stock_ids):\n    stock_dfs = []\n    for book_path, trade_path in tqdm(list(zip(order_book_paths, trade_paths))):\n        stock_id = int(book_path.split(\"=\")[1].split(\"\/\")[0])\n\n        df = feature_engineering(book_path, trade_path)\n        df[\"stock_id\"] = stock_id\n        stock_dfs.append(df)\n    return cudf.concat(stock_dfs)","ffe03142":"past_volatility = process_data(order_book_training, trades_training, stock_ids)\npast_test_volatility = process_data(order_book_test, trades_test, stock_ids)\n\npast_volatility.shape, past_test_volatility.shape","23442c98":"def stock_time_fe(df):\n    cols = ['realized_vol1_sum', 'realized_vol2_sum', 'realized_vol_trade_sum',\n            'realized_vol1_sum_150', 'realized_vol2_sum_150', 'realized_vol_trade_sum_150',\n            'realized_vol1_sum_300', 'realized_vol2_sum_300', 'realized_vol_trade_sum_300',\n            'realized_vol1_sum_450', 'realized_vol2_sum_450', 'realized_vol_trade_sum_450']\n    \n    for agg_col in [\"stock_id\", \"time_id\"]:\n        for agg_func in [\"mean\", \"max\", \"std\", \"min\"]:\n            agg_df = df.groupby(agg_col)[cols].agg(agg_func)\n            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")\n    \n    return df\n\npast_volatility[\"is_test\"] = False\npast_test_volatility[\"is_test\"] = True\nall_df = past_volatility.append(past_test_volatility).reset_index(drop=True)\n\nall_df = stock_time_fe(all_df)\n\npast_volatility = all_df[~all_df[\"is_test\"]]\npast_test_volatility = all_df[all_df[\"is_test\"]]","4820618b":"dev_df = dev_df.merge(past_volatility, on=[\"stock_id\", \"time_id\"], how=\"left\")\n\nfeatures = [col for col in list(dev_df.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"is_test\"}]\nlen(features)","58930b16":"import numpy as np\nimport pandas as pd\n\ndev_df = dev_df.to_pandas().sample(frac=0.1, random_state=0)\n\ncv = []\nNUM_FOLDS = 5\n\n\nfor fold in range(NUM_FOLDS):\n    print(\"Fold\", fold)\n    train_ind = np.where(dev_df[\"time_id\"].values % NUM_FOLDS != fold)[0]\n    val_ind = np.where(dev_df[\"time_id\"].values % NUM_FOLDS == fold)[0]\n    cv.append((train_ind, val_ind))","1faff20c":"from lofo import Dataset, LOFOImportance, plot_importance\n\n\ndataset = Dataset(df=dev_df, target=\"target\", features=features, auto_group_threshold=0.8)","ea57be05":"len(dataset.features), len(dataset.feature_groups)","b55a020f":"import xgboost as xgb\n\n\nparam = {'objective': 'reg:squarederror',\n         'learning_rate': 0.1,\n         'max_depth': 3,\n         \"min_child_weight\": 200,\n         \"reg_alpha\": 10.0,\n         \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n         'disable_default_eval_metric': 1,\n         \"n_estimators\": 500\n    }\n\nmodel = xgb.XGBRegressor(**param)","ea40403a":"from sklearn.metrics import make_scorer\n\ndef rmspe(y_true, y_pred):\n    return (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\nscoring = make_scorer(rmspe, greater_is_better=False, needs_proba=False, needs_threshold=False)\n\n\nlofo_imp = LOFOImportance(dataset, cv=cv, scoring=scoring, model=model)\n\nimp_df = lofo_imp.get_importance()","c4257711":"imp_df","b9dc09c8":"imp_df[\"feature_full_name\"] = imp_df[\"feature\"].values\nimp_df[\"feature\"] = imp_df[\"feature_full_name\"].apply(lambda x: x[:100])","924b7b2a":"plot_importance(imp_df, figsize=(16, 12))","96ee40e8":"plot_importance(imp_df.head(), figsize=(8, 4), kind=\"box\")","eb74eb10":"## LOFO Feature Importance using XGBoost model on GPU","f66fe343":"## Using rapids-kaggle-utils for missing cuDF aggregation functions"}}