{"cell_type":{"e46d5184":"code","68657d9f":"code","5afe9db9":"code","c1dd3736":"code","a08c8a0e":"code","3d1a7be6":"code","df0a4620":"code","2c967e56":"code","ff242058":"code","ce256d15":"code","a2ba82df":"code","0e4ef439":"code","7c67ec50":"markdown","e282c7ea":"markdown","72c37ce0":"markdown","122c00fb":"markdown","85c9afb9":"markdown","c4d1bad5":"markdown","6f32a602":"markdown","95cc3c17":"markdown","c9a12528":"markdown","0adf6ed7":"markdown","090c3d04":"markdown","74d2a9f1":"markdown"},"source":{"e46d5184":"# Standard Kaggle Notebook imports\nimport numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Additional imports for data processing\nimport chardet # Used to determine character encoding for pd.read_csv()\nimport fuzzywuzzy # Used to fuzzy match inconsistent country names\nfrom fuzzywuzzy import process\n\n# Additional imports for geospatial analysis\nimport geopandas as gpd\nfrom geopandas.tools import geocode            \nimport folium\nfrom folium import Choropleth, Circle","68657d9f":"# Scan through source data CSV to determine character encoding\nmy_filepath = \"..\/input\/coffee-production-19912020-data-from-ico\/coffee-production-by-exporting-countries-1991-2020-ico.csv\"\nwith open(my_filepath, 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(10000))\nprint(result)","5afe9db9":"# Read in source data CSV using encoding ISO-8859-1 to cater for Latin alphabet\n# Note that using simple pd.read_csv without encoding setting will result in error message: \n# UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf4 in position 4088: invalid continuation byte\nsource_data = pd.read_csv(my_filepath, encoding='ISO-8859-1')\nsource_data.head()","c1dd3736":"# Load GeoDataFrame for entire world including country boundary polygons\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld.head()","a08c8a0e":"# Extract GeoDataFrame data only for countries listed in source_data\ncoffee_data = pd.merge(source_data, world, left_on=\"Country\", right_on=\"name\")\n\n# ...but not all country names are consistent across both dataframes, so some are missing\nmissing_countries = pd.concat([source_data.Country, coffee_data.Country]).drop_duplicates(keep=False)\nprint(\"Number of missing countries =\", len(missing_countries), \"\\n\")\nprint(\"Missing countries =\", missing_countries.to_list())\n","3d1a7be6":"# Create function to match missing country\ndef fuzzy_match(country_name, full_list, num_matches=3): \n    matches =  fuzzywuzzy.process.extract(country_name, full_list, limit=num_matches, \n                                      scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n    return matches\n\n# Print list of closest matches for each missing country\ncandidates = [print(country,\"| Closest matches =\",\n                    fuzzy_match(country, world.name, 3), \"\\n\") for country in missing_countries] ","df0a4620":"# Tweak country names for those that still don't show obvious match\nstill_missing = [\"Lao\", \"Bolivia\"]\nmore_candidates = [print(country, \"| Closest matches =\", \n                         fuzzy_match(country, world.name, 3), \"\\n\") for country in still_missing]","2c967e56":"# Create dict to replace inconsistent country names\nrename_dict = {\"Viet Nam\" : \"Vietnam\", \n               \"Dominican Republic\" : \"Dominican Rep.\",\n               \"Democratic Republic of Congo\" : \"Dem. Rep. Congo\",\n               \"Central African Republic\" : \"Central African Rep.\",\n               \"Trinidad & Tobago\" : \"Trinidad and Tobago\",\n               \"Equatorial Guinea\" : \"Eq. Guinea\",\n               \"Lao People's Democratic Republic\" : \"Laos\",\n               \"Bolivia (Plurinational State of)\" : \"Bolivia\",            \n              }\n\n# Replace country names to be consistent with GeoDataFrame\ncoffee = source_data.copy()\ncoffee[\"name\"] = coffee[\"Country\"]\ncoffee[\"name\"].replace(rename_dict, inplace=True)\n\n# Merge with GeoDataFrame and confirm that no more inconsistent (i.e. missing) countries\ncoffee = world.merge(coffee, on=\"name\")\n\nprint(\"Number of inconsistent country names =\", \n     len(pd.concat([source_data.Country, coffee.Country]).drop_duplicates(keep=False)))\nprint(\"\\nTotal number of countries =\", len(coffee.Country))\nprint(\"\\nCountry names =\", coffee[\"name\"].sort_values().to_list())","ff242058":"# Add latitude, longitude data using geocode\ndef my_geoencoder(row):\n    try:\n        point = geocode(row, provider='nominatim', user_agent=\"kaggle-learn\").geometry.iloc[0]\n        return pd.Series({'Latitude' : point.y, 'Longitude' : point.x})\n    except:\n        return None\n\ncoffee[['Latitude', 'Longitude']] = coffee.apply(lambda x: my_geoencoder(x[\"Country\"]), axis=1)\n\n# Check for countries with missing latitude and longitude data\nprint(\"{:.1f}% of countries were geocoded\".format(\n    (1 - sum(np.isnan(coffee['Latitude'])) \/ len(coffee)) * 100))\nmissing_geocode = coffee.loc[np.isnan(coffee['Latitude'])]\nif len(missing_geocode) > 0:\n    print(\"Missing geocode countries =\", missing_geocode.Country.to_list())","ce256d15":"# Display first few lines of processed data\ncoffee.head()","a2ba82df":"# Bar chart of coffee production by country\ncoffee[[\"name\", \"2020\"]].sort_values(by=\"2020\", ascending=False).plot.bar(x=\"name\", figsize=(16,6))","0e4ef439":"# Create base world map\nm_1 = folium.Map(location=[0,20], tiles='openstreetmap', zoom_start=2)\n\n# Prepare data for Chloropleth\ncountries = coffee[[\"name\", \"geometry\"]].set_index(\"name\")\nproduction = coffee[[\"name\", \"2020\"]].set_index(\"name\")\n\n# Add Choropleth of coffee production to base map\nChoropleth(geo_data=countries.__geo_interface__,\n           data=production.squeeze(), # Choropleth requires data in Pandas series, not dataframe\n           key_on=\"feature.id\",\n           min_zoom=2,\n           fill_color=\"OrRd\", # BuGn,BuPu,GnBu,OrRd,PuBu,PuBuGn,PuRd,RdPu,YlGn,YlGnBu,YlOrBr,YlOrRd\n           legend_name=\"Coffee Production in 2020 (Metric Tons)\",\n          ).add_to(m_1)\nm_1\n\n# Add Circle (bubble map) of coffee production to base map\nfor i in range(0, len(coffee)):\n    Circle(\n        location=[coffee.iloc[i]['Latitude'], coffee.iloc[i]['Longitude']],\n        radius=coffee.iloc[i]['2020']\/2, # Divide by 2 to make circles smaller\n        color='forestgreen',\n        fill=True,\n        opacity=0.2, # Line opacity\n        fill_opacity=0.6,\n        tooltip='{}: {:,.0f}'.format(coffee.iloc[i]['name'], coffee.iloc[i]['2020'])        \n    ).add_to(m_1)\nm_1","7c67ec50":"## Completed Dataset with Geospatial Info\nNow that I had the complete dataset with coffee production, country boundaries and latitude\/longitude data, I could start doing some simple data visualisation, starting with a simple (sorted) bar chart using 2020 data.\n\nIt was very obvious that Brazil is the world's largest producer of coffee by far, with Vietnam coming in second and Colombia third.","e282c7ea":"# Geospatial Visualisation of Global Coffee Production in 2020\n*Data sourced from the International Coffee Organisation (ICO):*  \n*https:\/\/ico.org\/historical\/1990%20onwards\/Excel\/1a%20-%20Total%20production.xlsx*\n\n![Global Coffee Production in 2020](https:\/\/firefortysix.files.wordpress.com\/2021\/10\/geospatial-analysis-coffee-production-in-2020-16_9.jpg)\n\nI'm addicted to *1,3,7-Trimethylxanthine*, and have been for many decades now. \n\nIf the name is unfamiliar, perhaps you might recognise it by its chemical formula C<sub>8<\/sub>H<sub>10<\/sub>N<sub>4<\/sub>O<sub>2<\/sub>.\n\nNo?\n\nMaybe looking at its molecular structure might help:\n\n<img src=\"https:\/\/firefortysix.files.wordpress.com\/2021\/09\/caffeine-molecular-structure-1_1.jpg\" alt=\"1,3,7-Trimethylxanthine\" width=\"200\"\/>\n\nStill no?\n\nThere\u2019s a good chance that you\u2019re addicted to it too, or know someone who is. It\u2019s the active compound in one of the world\u2019s favourite hot beverage, and it's grown in many countries around the world.\n\nTo find out which are the specific countries and how much of the crop they each grow, I went searching for some data and found a good dataset published by the International Coffee Organisation (ICO).\n\nArmed with this data, and some newly acquired [**Data Cleaning**](https:\/\/www.kaggle.com\/learn\/data-cleaning) and [**Geospatial Analysis**](https:\/\/www.kaggle.com\/learn\/geospatial-analysis) skills courtesy of Kaggle Learn, I thought it would be nice to create a simple notebook to visualise the information. ","72c37ce0":"## Loading Data\nI saved the dataset into a CSV file which I then uploaded to [**Kaggle Datasets**](https:\/\/www.kaggle.com\/firefortysix\/coffee-production-19912020-data-from-ico), and added that dataset to this notebook.\n\nUsing `pd.read_csv()` to read the CSV file should have been pretty straight-forward, but I quickly ran into my first error message:\n>`UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf4 in position 4088: invalid continuation byte`\n\nThankfully, this was covered in the [**Character Encodings**](https:\/\/www.kaggle.com\/alexisbcook\/character-encodings) lesson and I was able to determine the correct encoding to use when reading in the CSV file, and used it explicitely when calling `pd.read_csv()`\n","122c00fb":"## Fixing Missing Countries\nSince I was only interested in those countries that produce coffee, I only extracted the boundaries for countries in the coffee dataset. \n\nHowever, I realised that eight countries were missing and my best guess was that the spelling of those country names were not consistent between the coffee and geopandas datasets. ","85c9afb9":"## Importing Packages\nFirst things first, I added the standard imports `os`, `numpy`, `pandas` as well as `chardet` and `fuzzywuzzy` for data processing, followed by `geopandas` and `folium` required for geospatial analysis.\n","c4d1bad5":"## Downloading Country Boundaries\nIn order to draw Choropleth maps that show how much coffee each country produces, I needed the boundaries of each country and this was easily available within `GeoPandas`. The boundaries can be found in the `geometry` column as either `Polygon` or `MultiPolygon` data types.","6f32a602":"## Generating Folium Choropleth and Circle Plots\nTo visualise the data even better, I generated Choropleth and Circle plots that were introduced in the [**Interactive Maps**](https:\/\/www.kaggle.com\/alexisbcook\/interactive-maps) lesson.\n\nThe Choropleth plot used the country boundary `geometry` data, while the Circle plot used the `Latitude` and `Longitude` data, and I could overlap both plots on the same base map.\n\nGenerating these plots were easy and straight-forward, and once they were rendered, I could zoom in\/out and change locations. Adding a dynamic tooltip to the Circle plot was easy, though doing the same for the Choropleth plot required creating and configuring a `GeoJSON` file, which I didn't attempt.","95cc3c17":"The matching wasn't 100% complete, but since there were only two missing countries, I simplified the names and managed to quickly locate those as well.","c9a12528":"## Adding Latitude and Longitude\nThe geopandas dataset didn't include the latitude and longitude data for each country, which would be needed later when generating `folium` bubble maps, so I used the geocode functionality taught in the [**Manipulating Geospatial Data**](https:\/\/www.kaggle.com\/alexisbcook\/manipulating-geospatial-data) lesson to fetch those using the `nominatim` provider.\n\nMy first attempt using the `name` column didn't achieve 100% retrieval, but this was easily solved by switching to the `Country` column, which contained longer names which seemed to work better. It was a good thing that I kept that column previously.","0adf6ed7":"## Matching Using Fuzzy Text\nI could have manually compared those missing countries against the full list of countries available in the geopandas dataset, and try to pick them out visually. But then I remembered the fuzzy text matching package `fuzzywuzzy` (*what a cute name!*) introduced in the [**Inconsistent Data Entry**](https:\/\/www.kaggle.com\/alexisbcook\/inconsistent-data-entry) lesson, and managed to narrow down the closest matches for the missing country names.","090c3d04":"## Replacing Inconsistent Names\nI created a `dict` that contained `key:value` pairs for the countries that had to be re-named, and used it to create a new `name` column in the coffee dataset that would be consistent with the corresponding `name` column in the geopandas dataset. \n\nI decided to keep the `Country` column in the coffee dataset as-is, in case I needed to use it somewhere down the line.\n\nAfter merging the two datasets, I double-checked that none of the 55 countries were missing.","74d2a9f1":"## Conclusion\nThis was just a simple exercise using a dataset outside of Kaggle, but I was happy that I got to try out some of the useful skills that I picked up from various Kaggle Learn lessons. \n\nI'll probably do more analysis on this coffee production dataset, but this initial start doing just geospatial visualisation was already quite interesting."}}