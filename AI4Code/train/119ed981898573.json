{"cell_type":{"81f5f382":"code","c444cacd":"code","ce60a9cf":"code","7066a7c1":"code","e7fd4b59":"code","3d99c726":"code","6a0be7a9":"code","0286ede7":"code","ae049489":"code","acd96f49":"code","4cc91ee3":"code","bcabb0ab":"code","c62366ba":"code","2fe001a5":"code","3207db94":"code","4e4c260b":"code","3fcc1b67":"code","cacb6598":"code","1066b502":"code","d21b325f":"code","7020c020":"code","607fe2b7":"code","ac1d05f3":"code","c89b2e96":"code","414e8825":"code","5af441d6":"code","b137817a":"code","aba3e946":"code","51c48883":"code","a912a4dc":"code","bec2ab76":"code","857e8b3e":"code","93d7995d":"code","b3133c61":"code","d22226a3":"code","d03a7426":"markdown","e32b340c":"markdown","ff52f751":"markdown","3067cbf9":"markdown","d9c18ad7":"markdown","e24e611f":"markdown","38fd31ca":"markdown","ee5724d6":"markdown","8b527e93":"markdown","afb8669f":"markdown","3fbf7e8c":"markdown","eb85135b":"markdown","d4e2e72f":"markdown"},"source":{"81f5f382":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c444cacd":"from collections import Counter","ce60a9cf":"#Codes by Jay Lee https:\/\/www.kaggle.com\/jayaos\/basic-nlp-preprocessing-of-corpus-and-zipf-s-law\/notebook\n\ndef read_docu(file):\n    \n    all_words = []\n    \n    with open(file, \"r\", encoding = \"utf-8\") as input_file:\n        for line in input_file:\n            line = line.lower()\n            line = line.strip().split()\n            all_words += line\n        return(all_words)","7066a7c1":"#Codes by Jay Lee https:\/\/www.kaggle.com\/jayaos\/basic-nlp-preprocessing-of-corpus-and-zipf-s-law\/notebook\n\ndef word_counter(all_words):\n    \n    word_count = Counter()\n    for word in all_words:\n        word_count[word] += 1\n    return(word_count.values())","e7fd4b59":"def draw_zipfian_curve(word_count):\n    plt.plot(sorted(word_count, reverse = True), marker = \"o\")\n    plt.xscale(\"log\")\n    plt.yscale(\"log\")\n    plt.xlabel(\"log(Rank)\")\n    plt.ylabel(\"log(Frequency)\")\n    plt.show()","3d99c726":"#Codes by Jay Lee https:\/\/www.kaggle.com\/jayaos\/basic-nlp-preprocessing-of-corpus-and-zipf-s-law\/notebook\n\ndef zipfian_plot(file):\n    word_corpus = read_docu(file)\n    counts = word_counter(word_corpus)\n    draw_zipfian_curve(counts)","6a0be7a9":"zipfian_plot(\"..\/input\/mit-plagairism-detection-dataset\/train_snli.txt\")","0286ede7":"df= pd.read_csv('..\/input\/mit-plagairism-detection-dataset\/train_snli.txt', sep='\\t', error_bad_lines=False)\ndf.head()","ae049489":"df.isnull().sum()","acd96f49":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\n# lets create a text\ntext = \"Two blond women are hugging one another\"\n\n# length of text ( includes spaces)\nprint(\"length of text: \",len(text))\n\n# split the text\nsplitted_text = text.split() # default split methods splits text according to spaces\nprint(\"Splitted text: \",splitted_text)    # splitted_text is a list that includes words of text sentence\n# each word is called token in text maning world.","4cc91ee3":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\n# find specific words with list comprehension method\nspecific_words = [word for word in splitted_text if(len(word)>2)]\nprint(\"Words which are more than 3 letter: \",specific_words)\n\n# capitalized words with istitle() method that finds capitalized words\ncapital_words = [ word for word in splitted_text if word.istitle()]\nprint(\"Capitalized words: \",capital_words)\n\n# words which end with \"o\": endswith() method finds last letter of word\nwords_end_with_o =  [word for word in splitted_text if word.endswith(\"o\")]\nprint(\"words end with o: \",words_end_with_o) \n\n# words which starts with \"w\": startswith() method\nwords_start_with_w = [word for word in splitted_text if word.startswith(\"w\")]\nprint(\"words start with w: \",words_start_with_w) ","bcabb0ab":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\n# unique with set() method\nprint(\"unique words: \",set(splitted_text))  # actually the word \"no\" is occured twice bc one word is \"no\" and others \"No\" there is a capital letter at first letter\n\n# make all letters lowercase with lower() method\nlowercase_text = [word.lower() for word in splitted_text]\n\n# then find uniques again with set() method\nprint(\"unique words: \",set(lowercase_text))","c62366ba":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\n# check words includes or not includes particular substring or letter\nprint(\"Is w letter in women word:\", \"w\" in \"women\")\n\n# check words are upper case or lower case\nprint(\"Is word uppercase:\", \"WOMEN\".isupper())\nprint(\"Is word lowercase:\", \"hugging\".islower())\n\n# check words are made of by digits or not\nprint(\"Is word made of by digits: \",\"12345\".isdigit())\n\n# get rid of from white space characters like spaces and tabs or from unwanted letters with strip() method\nprint(\"00000000Two blond: \",\"00000000Two blond\".strip(\"0\"))\n\n# find particular letter from front \nprint(\"Find particular letter from back: \",\"one another\".find(\"r\"))  # at index 1\n\n# find particular letter from back  rfind = reverse find\nprint(\"Find particular letter from back: \",\"one another\".rfind(\"r\"))  # at index 8\n\n# replace letter with number\nprint(\"Replace o with 4 \", \"one another\".replace(\"r\",\"4\"))\n\n# find each letter and store them in list\nprint(\"Each letter: \",list(\"Two blond\"))","2fe001a5":"# Cleaning text\ntext1 = \"    The kids are frowning    \"\nprint(\"Split text: \",text1.split(\" \"))   # as you can see there are unnecessary white space in list\n\n# get rid of from these unnecassary white spaces with strip() method then split\nprint(\"Cleaned text: \",text1.strip().split(\" \"))","3207db94":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\n# reading files line by line\nf = open(\"..\/input\/mit-plagairism-detection-dataset\/train_snli.txt\",\"r\")\n\n# read first line\nprint(f.readline())\n\n# length of text\ntext3=f.read()\nprint(\"Length of text: \",len(text3))\n\n# Number of lines with splitlines() method\nlines = text3.splitlines()\nprint(\"Number of lines: \",len(lines))","4e4c260b":"df = df.rename(columns={'A person on a horse jumps over a broken down airplane.':'person', 'A person is at a diner, ordering an omelette.': 'omelette'})","3fcc1b67":"df.head()","cacb6598":"# find which entries contain the word 'appointment'\nprint(\"In this text, the rate of occuring boy word is: \",sum(df.person.str.contains('boy'))\/len(df))\n# text\ntext = df.person[1]\nprint(text)","1066b502":"#I\u00b4m saving that snippet since there is No callout that started with @.  \n\n# find regular expression on text\n# import regular expression package\nimport re\n# find callouts that starts with @\ncallouts = [word for word in text.split(\" \") if re.search(\"@[A-Za-z0-9_]+\",word)]\nprint(\"callouts: \",callouts)","d21b325f":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\n# find specific characters like \"w\"\nprint(re.findall(r\"[w]\",text))\n# \"w\"ith, \"w\"indo\"w\", sho\"w\"ing, s\"w\"itches \n\n# do not find specific character like \"w\". We will use \"^\" symbol\nprint(re.findall(r\"[^w]\",text))","7020c020":"# Regular expressions for Dates\n#date = \"15-10-2000\\n09\/10\/2005\\n15-05-1999\\n05\/05\/99\\n\\n05\/05\/199\\n\\n05\/05\/9\"\n#re.findall(r\"\\d{1,2}[\/-]\\d{1,2}[\/-]\\d{1,4}\",date)","607fe2b7":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\n# import natural language tool kit\nimport nltk as nlp\n\n# counting vocabulary of words\ntext = df.person[1]\nsplitted = text.split(\" \")\nprint(\"number of words: \",len(splitted))\n\n# counting unique vocabulary of words\ntext = df.person[1]\nprint(\"number of unique words: \",len(set(splitted)))\n\n# print first five unique words\nprint(\"first 5 unique words: \",list(set(splitted))[:5])\n\n# frequency of words \ndist = nlp.FreqDist(splitted)\nprint(\"frequency of words: \",dist)\n\n# look at keys in dist\nprint(\"words in person: \",dist.keys())\n\n# count how many time a particalar value occurs. Lets look at \"box\"\nprint(\"the word box is occured how many times:\",dist[\"box\"])","ac1d05f3":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\n# normalization\nwords = \"task Tasked tasks tasking\"\nwords_list = words.lower().split(\" \")\nprint(\"normalized words: \",words_list)\n\n# stemming\nporter_stemmer = nlp.PorterStemmer()\nroots = [porter_stemmer.stem(each) for each in words_list]\nprint(\"roots of task Tasked tasks tasking: \",roots)","c89b2e96":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\n# stemming\nstemming_word_list = [\"baseball\",\"airplane\",\"restaurant\",\"drinking\",\"outdoors\"]\nporter_stemmer = nlp.PorterStemmer()\nroots = [porter_stemmer.stem(each) for each in stemming_word_list]\nprint(\"result of stemming: \",roots)\n\n# lemmatization\nlemma = nlp.WordNetLemmatizer()\nlemma_roots = [lemma.lemmatize(each) for each in stemming_word_list]\nprint(\"result of lemmatization: \",lemma_roots)","414e8825":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\ntext_t = \"Two groups of rival gang members flipped each other off.\"\nprint(\"split the sentence: \", text_t.split(\" \"))  # 5 words\n\n# tokenization with nltk\nprint(\"tokenize with nltk: \",nlp.word_tokenize(text_t))","5af441d6":"# categorical features with missing values\ncategorical_nan = [feature for feature in df.columns if df[feature].isna().sum()>0 and df[feature].dtypes=='O']\nprint(categorical_nan)","b137817a":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    df[feature] = df[feature].fillna('None')","aba3e946":"df[categorical_nan].isna().sum()","51c48883":"#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\n\n# %% creating bag of words model\nfrom sklearn.feature_extraction.text import CountVectorizer  # for bag of words \nmax_features = 150 # max_features dimension reduction \ncount_vectorizer = CountVectorizer(stop_words = \"english\",max_features = max_features) \nreview_list = df.iloc[:,1] #fixed by Billa comment @skbilla\n\n# stop_words parameter = automatically remove all stopwords \n# lowercase parameter \n# token_pattern removing other karakters like .. !\n\nsparce_matrix = count_vectorizer.fit_transform(review_list).toarray() # sparce matrix yaratir bag of words model = sparce matrix\n\nprint(\"Most used {} words: {}\".format(max_features,count_vectorizer.get_feature_names()))\n\ny = df.iloc[:,0].values  # positive or negative comment\n\n#sparce matrix includes independent variable","a912a4dc":"# train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(sparce_matrix,y,test_size = 0.1,random_state = 0)","bec2ab76":"#That snippet take hours to provide GaussianNB ()??? . \n\n# %% naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(sparce_matrix,y)","857e8b3e":"#Another that spend too much time. Predict if I'm going to wait that. It's a retorical question.\n#And worst: My Notebook failed after that!\n\n# %% predict\n#y_pred = nb.predict(sparce_matrix)","93d7995d":"#%%\nfrom sklearn.metrics import confusion_matrix\n#cm = confusion_matrix(y,y_pred)\n#cm","b3133c61":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nf,ax = plt.subplots(figsize=(5, 5))\nsns.heatmap(cm, annot=True, linewidths=0.5,linecolor=\"red\", fmt= '.1f',ax=ax)\nplt.show()\nplt.savefig('graph.png')","d22226a3":"#Code By Paul Mooney\n\nplagiarism_file = '..\/input\/mit-plagairism-detection-dataset\/train_snli.txt'\nwith open(plagiarism_file) as f: # The with keyword automatically closes the file when you are done\n    print (f.read(3000))","d03a7426":"![](https:\/\/lh3.googleusercontent.com\/proxy\/yYlC3n6FpIzN6mD8ARFVHdA95sUL1t7Ry9klJivqXwfxfh0OpPxPWNJjCJ0sYaf9ObDrzKIyt4qp5LTVpYt6BA_N34iizT1lWJJSRcxbDY6QE4YvilzU4t2W63ZZgAgl_P1qKEFvcZIBlypF74PyMuGAtAShV8vIgOV22Q)m.quickmeme.com","e32b340c":"#Basic Text Mining Methods","ff52f751":"#Lemmatization","3067cbf9":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQMbo8VhVRJMVkL2po2vlSazhRqaunWvN7k1A&usqp=CAU)dogtrainingobedienceschool.com","d9c18ad7":"#The kids are frowning","e24e611f":"#Normalization and Stemming words\n\nNormalization is different forms of the same word like have and having.\n\nStemming is finding a root of the words like having => have.","38fd31ca":"#Natural Language Process (NLP)","ee5724d6":"![](https:\/\/s3.amazonaws.com\/libapps\/accounts\/72996\/images\/Infographic_Did-I-Plagiarize1.jpg)library.an.edu","8b527e93":"#Code by Datai https:\/\/www.kaggle.com\/kanncaa1\/applying-text-mining","afb8669f":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRDA0-IIN22I8dAyOi2MYdRitGCHvfuP3vqMg&usqp=CAU)bckonline.com","3fbf7e8c":"#Tokenization\n\nSplitting a sentence into words(tokens)","eb85135b":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSxBSFCHC_lNgrpUOTgk5kOZU4QSsKALegXGQ&usqp=CAU)memecreator.org","d4e2e72f":"#One blond and one brunette hugging\n\n![](https:\/\/media3.giphy.com\/media\/3o7TKu8D1d12Eo9wSQ\/200.webp?cid=ecf05e47kvwtv10cwpvgpqc7bp8lzplbypvzsvdop4b3aoor&rid=200.webp&ct=g)"}}