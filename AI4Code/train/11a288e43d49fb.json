{"cell_type":{"b44a69f5":"code","30804645":"code","a305f59d":"code","3cc66ef3":"code","01cdb7f8":"code","ab8b5701":"code","7634fea6":"code","664e0b90":"code","0f037896":"code","3f12f127":"code","c4c69823":"code","8bc37f00":"code","6e1fa35e":"code","e8b1e226":"code","18a56700":"code","8369fb0c":"code","39fb4217":"code","d76e9567":"code","601264a4":"code","1761af35":"code","55a43026":"code","626cf418":"code","74bc5565":"code","bfb3e267":"code","3e1b49ee":"code","73b47829":"code","3ef976a6":"code","f7681bfa":"code","f14ee5f6":"code","763c7248":"code","15bfe003":"code","6ab3366e":"markdown","7564f556":"markdown","5fb4b655":"markdown","85439e3c":"markdown","df2f2472":"markdown","7e55ce8e":"markdown","0671586e":"markdown","5044d90b":"markdown","628ab350":"markdown","983cc6e5":"markdown","62530a24":"markdown","af66222e":"markdown","591b7e72":"markdown","725ebb9c":"markdown","94ef8b9c":"markdown","ef68322d":"markdown","5f59b6dd":"markdown","ee44a5e7":"markdown","87dae571":"markdown","dd52eefa":"markdown","34e6bc4f":"markdown","82e580d4":"markdown","7f3ffc67":"markdown"},"source":{"b44a69f5":"%%html\n<style>\n@import url('https:\/\/fonts.googleapis.com\/css?family=Ewert|Roboto&effect=3d|ice|');\nbody {background-color: gainsboro;} \na {color: #37c9e1; font-family: 'Roboto';} \nh1 {color: #37c9e1; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;} \nh2, h3 {color: slategray; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;}\nh4 {color: #818286; font-family: 'Roboto';}\nspan {font-family:'Roboto'; color:black; text-shadow: 5px 5px 5px #aaa;}  \ndiv.output_area pre{font-family:'Roboto'; font-size:110%; color:lightblue;}      \n<\/style>","30804645":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a305f59d":"sns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score , classification_report\nimport seaborn as sns\nclasses=['healthy','Un-healthy']\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn import metrics","3cc66ef3":"Epilepsy=pd.read_csv('..\/input\/epilepticseizures\/data.csv')","01cdb7f8":"Epilepsy.drop('Unnamed: 0',axis=1,inplace=True)","ab8b5701":"Epilepsy.head()","7634fea6":"Epilepsy.info()","664e0b90":"Epilepsy.shape","0f037896":"y = Epilepsy.iloc[:,178].values\ny","3f12f127":"y[y>1]=0\ny","c4c69823":"cols = Epilepsy.columns\ntgt = Epilepsy.y\ntgt[tgt>1]=0\nax = sns.countplot(tgt,label=\"Count\")\nnon_seizure, seizure = tgt.value_counts()\nprint('The number of trials for the non-seizure class is:', non_seizure)\nprint('The number of trials for the seizure class is:', seizure)","8bc37f00":"Epilepsy.isna().sum()","6e1fa35e":"Epilepsy.describe()","e8b1e226":"Epilepsy.corr()","18a56700":"features_mean= list(Epilepsy.columns[1:11])\nprint(features_mean)\ncorr = Epilepsy[features_mean].corr() \nplt.figure(figsize=(12,8))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 12},\n           xticklabels= features_mean, yticklabels= features_mean,\n           cmap= 'coolwarm')","8369fb0c":"from sklearn.preprocessing import StandardScaler\n\nprediction_var = ['X1', 'X2', 'X3', 'X4', 'X5']\ntrain, test = train_test_split(Epilepsy, test_size = 0.3)\n# we can check their dimension \nprint(train.shape)\nprint(test.shape)","39fb4217":"train_X = train[prediction_var]\ntrain_y=train.y\ntest_X= test[prediction_var]\ntest_y =test.y","d76e9567":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_y)","601264a4":"prediction=model.predict(test_X)","1761af35":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score , classification_report\nimport seaborn as sns\nclasses=['healthy','Un-healthy']\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(y_train, pred) * 100:.2f}%\")\n       # recall=recall_score(y_train, pred) \n        print(f\"\\t\\t\\tRecall Score: {recall_score(y_train, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(y_test, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tRecall Score: {recall_score(y_test, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        sns.heatmap(confusion_matrix(y_test, pred), annot= True, cmap='YlGnBu',fmt = 'g')\n        print(classification_report(y_test,pred))\n        cm=(confusion_matrix(y_test,pred))\n       # ax.xaxis.set_label_position('top')\n        plt.tight_layout()\n        plt.title('Confusion matrix for Decision Tree Model', y = 1.1)\n        plt.ylabel('Actual label')\n        plt.xlabel('Predicted label')\n        plt.show()\n        total = sum(sum(cm))\n        acc = (cm[0, 0] + cm[1, 1]) \/ total\n        sensitivity = cm[0, 0] \/ (cm[0, 0] + cm[0, 1])\n        specificity = cm[1, 1] \/ (cm[1, 0] + cm[1, 1])\n       # print(cm)\n\n        FP = cm.sum(axis=0) - np.diag(cm)  \n        FN = cm.sum(axis=1) - np.diag(cm)\n        TP = np.diag(cm)\n        TN = cm.sum() - (FP + FN + TP)\n\n        FP = FP.astype(float)\n        FN = FN.astype(float)\n        TP = TP.astype(float)\n        TN = TN.astype(float)\n\n        # Sensitivity, hit rate, recall, or true positive rate\n        TPR = TP\/(TP+FN)\n        print('Sensitivity (TPR) : ',TPR)\n        # Specificity or true negative rate\n        TNR = TN\/(TN+FP) \n        print('Specificity (TNR) : ',TNR)\n        # Overall accuracy\n        print(\" Overall accuracy\")\n        ACC = (TP+TN)\/(TP+FP+FN+TN)\n        print('Accuracy : ',ACC)\n        print(\"Accuracy: {:.4f}\".format(acc))\n        print(\"Average Sensitivity: {:.4f}\".format(sensitivity))\n        print(\"Average Specificity: {:.4f}\".format(specificity))\n        print('\\n')\n        \n        conf_matrix=cm\n        print(\"=========================================\")\n        # save confusion matrix and slice into four pieces\n        TP = conf_matrix[1][1]\n        TN = conf_matrix[0][0]\n        FP = conf_matrix[0][1]\n        FN = conf_matrix[1][0]\n        print('True Positives:', TP)\n        print('True Negatives:', TN)\n        print('False Positives:', FP)\n        print('False Negatives:', FN)\n\n        # calculate accuracy\n        conf_accuracy = (float (TP+TN) \/ float(TP + TN + FP + FN))\n\n        # calculate mis-classification\n        conf_misclassification = 1- conf_accuracy\n\n        # calculate the sensitivity\n        conf_sensitivity = (TP \/ float(TP + FN))\n        # calculate the specificity\n        conf_specificity = (TN \/ float(TN + FP))\n\n        # calculate precision\n        conf_precision = (TN \/ float(TN + FP))\n        # calculate f_1 score\n        conf_f1 = 2 * ((conf_precision * conf_sensitivity) \/ (conf_precision + conf_sensitivity))\n        print('-'*50)\n        print(f'Accuracy: {round(conf_accuracy,2)}') \n        print(f'Mis-Classification: {round(conf_misclassification,2)}') \n        print(f'Sensitivity: {round(conf_sensitivity,2)}') \n        print(f'Specificity: {round(conf_specificity,2)}') \n        print(f'Precision: {round(conf_precision,2)}')\n        print(f'f_1 Score: {round(conf_f1,2)}')","55a43026":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\ndef plotting(true,pred):\n    fig,ax=plt.subplots(1,2,figsize=(15,5))\n    precision,recall,threshold = precision_recall_curve(true,pred[:,1])\n    ax[0].plot(recall,precision,'g--')\n    ax[0].set_xlabel('Recall')\n    ax[0].set_ylabel('Precision')\n    ax[0].set_title(\"Average Precision Score : {}\".format(average_precision_score(true,pred[:,1])))\n    fpr,tpr,threshold = roc_curve(true,pred[:,1])\n    ax[1].plot(fpr,tpr)\n    ax[1].set_title(\"AUC Score is: {}\".format(auc(fpr,tpr)))\n    ax[1].plot([0,1],[0,1],'k--')\n    ax[1].set_xlabel('False Positive Rate')\n    ax[1].set_ylabel('True Positive Rate')","626cf418":"from sklearn.model_selection import train_test_split\n\nX = Epilepsy.drop('y', axis=1)\ny = Epilepsy.y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","74bc5565":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_train, y_train)\n\nprint_score(knn_classifier, X_train, y_train, X_test, y_test, train=True)\nprint_score(knn_classifier, X_train, y_train, X_test, y_test, train=False)","bfb3e267":"from sklearn.tree import DecisionTreeClassifier\n\n\ntree = DecisionTreeClassifier(random_state=42)\ntree.fit(X_train, y_train)\n\nprint_score(tree, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree, X_train, y_train, X_test, y_test, train=False)","3e1b49ee":"from sklearn.svm import SVC\n\n\nsvm_model = SVC(kernel='rbf', gamma=0.1, C=1.0, probability=True)\nsvm_model.fit(X_train, y_train)\n","73b47829":"print_score(svm_model, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_model, X_train, y_train, X_test, y_test, train=False)","3ef976a6":"from sklearn.neural_network import MLPClassifier\nNN=MLPClassifier(hidden_layer_sizes=(10,50),momentum=0.9,solver='sgd',random_state=42)\n               \nNN.fit(X_train, y_train)\n\nprint_score(NN, X_train, y_train, X_test, y_test, train=True)\nprint_score(NN, X_train, y_train, X_test, y_test, train=False)","f7681bfa":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrand_forest = RandomForestClassifier(n_estimators=1000, random_state=42)\nrand_forest.fit(X_train, y_train)\n\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=True)\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=False)","f14ee5f6":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\n\nprint_score(xgb, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb, X_train, y_train, X_test, y_test, train=False)","763c7248":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n\nprint_score(nb, X_train, y_train, X_test, y_test, train=True)\nprint_score(nb, X_train, y_train, X_test, y_test, train=False)  ","15bfe003":"colors = ['r', 'g', 'b', 'y', 'k', 'c', 'm', 'brown', 'r']\nlw = 1\nCs = [1e-6, 1e-4, 1e0]\n\nplt.figure(figsize=(12,6))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for different classifiers')\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n\nlabels = []\nfor idx, C in enumerate(Cs):\n    clf = LogisticRegression(C = C)\n    clf.fit(X_train, y_train)\n    print(\"C: {}, parameters {} and intercept {}\".format(C, clf.coef_, clf.intercept_))\n    fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=lw, color=colors[idx])\n    labels.append(\"C: {}, AUC = {}\".format(C, np.round(roc_auc, 4)))\n\nplt.legend(['random AUC = 0.5'] + labels)","6ab3366e":"# name:\u0648\u0639\u062f \u062e\u0627\u0644\u062f \u0627\u0644\u0639\u0646\u0632\u064a ","7564f556":"To make this a binary problem, let's turn the non-seizure classes 0 while maintaining the seizure as 1.","5fb4b655":"# 1. K-nearest neighbors","85439e3c":"# Features","df2f2472":"# Loading the data","7e55ce8e":"# Read and Show Dataset\n* The original dataset from the reference consists of 5 different folders, each with 100 files, with each file representing a single subject\/person. Each file is a recording of brain activity for 23.6 seconds.\n\n\n* The corresponding time-series is sampled into 4097 data points. Each data point is the value of the EEG recording at a different point in time. So we have total 500 individuals with each has 4097 data points for 23.5 seconds.\n\n\n* We divided and shuffled every 4097 data points into 23 chunks, each chunk contains 178 data points for 1 second, and each data point is the value of the EEG recording at a different point in time.\n\n\n* So now we have 23 x 500 = 11500 pieces of information(row), each information contains 178 data points for 1 second(column), the last column represents the label y {1,2,3,4,5}.\n\n\n* The response variable is y in column 179, the Explanatory variables X1, X2, ..., X178\n\n* The response variable is y in column 179, the Explanatory variables X1, X2, ..., X178, y contains the category of the 178-dimensional input vector. Specifically, y in {1, 2, 3, 4, 5}:\n\n5 - eyes open, means when they were recording the EEG signal of the brain the patient had their eyes open.\n\n4 - eyes closed, means when they were recording the EEG signal the patient had their eyes closed.\n\n3 \u2013 Yes, they identify where the region of the tumor was in the brain and recording the EEG activity from the healthy brain area.\n\n2 - They recorder the EEG from the area where the tumor was located.\n\n1 - Recording of seizure activity.\n\n\nAll subjects falling in classes 2, 3, 4, and 5 are subjects who did not have epileptic seizure. Only subjects in class 1 have epileptic seizure.","0671586e":"# 3. Support Vector machine","5044d90b":"# 2. Correlation Matrix","628ab350":"#          Epileptic Seizures Prediction Using Machine Learning Methods","983cc6e5":"![image.png](attachment:37d84446-34ab-4d5f-b3c7-5dafe945b5cc.png)](http:\/\/)","62530a24":"# 3. Data Processing","af66222e":"# 2. Decision Tree Classifier","591b7e72":"# ROC\nReceiver operator characteristic, used very commonly to assess the quality of models for binary classification.\n\nWe will look at at three different classifiers here, a strongly regularized one and two with weaker regularization. The heavily regularized model has parameters very close to zero and is actually worse than if we would pick the labels for our holdout samples randomly.","725ebb9c":"# 5. Random Forest","94ef8b9c":"#  6. XGBoost Classifer","ef68322d":"# Function to plot ROC and Precision Recall Curve for combination of all models","5f59b6dd":"# 4. Applying machine learning algorithms","ee44a5e7":"# 1. Exploratory Data Analysis (EDA)","87dae571":"Now we've got our data split into training and test sets, it's time to build a machine learning model.\n\nWe'll train it (find the patterns) on the training set.\n\nAnd we'll test it (use the patterns) on the test set.\n\nWe're going to try different machine learning models:\n\n1-Logistic Regression\n\n2-K-Nearest Neighbours Classifier\n\n3-Support Vector machine\n\n4-Decision Tree Classifier\n\n5-Random Forest Classifier\n\n6-XGBoost Classifier","dd52eefa":"# *4*MLP neural network classifier**","34e6bc4f":"# 7- naive_bayes","82e580d4":"#  Importing the libraries","7f3ffc67":"# Perform Feature Standerd Scalling\nStandardize features by removing the mean and scaling to unit variance\n\nThe standard score of a sample x is calculated as:\n\nz = (x - u) \/ s"}}