{"cell_type":{"292a1c4e":"code","1b6ee7bf":"code","eb08794b":"code","ce4d8869":"code","32ac6e11":"code","638e3713":"code","0bfe971d":"code","92990d4b":"code","2d41ac8a":"code","4137ac85":"code","6f89735a":"code","4d2a57cf":"code","a7f2a03c":"code","78526b6c":"code","4cc7195c":"code","d3e196f7":"code","48244058":"code","7a5d47e3":"code","503e85e3":"code","8abf5012":"code","113c61dd":"code","b18e27c5":"code","e3722ebb":"code","8b1d2195":"code","1766f604":"code","97fafd7d":"code","b9de23d3":"code","d5480cc3":"code","ac0fdb82":"code","7c0ee2e1":"code","833e56a4":"code","996b229c":"code","1ec8fafe":"code","43321366":"code","d868d4e9":"code","4221b858":"code","8e1fcf13":"code","db102cda":"code","7d5ae213":"code","bc456c2a":"code","0900e65d":"code","72cd40f4":"code","610a0311":"code","386f6ecb":"code","e645db17":"code","6fbbcb1c":"code","8987d6ba":"code","6611ebfb":"code","86a33e11":"code","36650287":"markdown","9db8ba62":"markdown","41aae72c":"markdown","433d192b":"markdown","57187419":"markdown","c3307b72":"markdown","85a6dfcb":"markdown","6f8f048b":"markdown","354991ab":"markdown","3765f703":"markdown","aec3afc3":"markdown","da496387":"markdown","a5a1e77e":"markdown","12d286fe":"markdown","efa96678":"markdown","f1289bb5":"markdown","16d2a254":"markdown","e889fda6":"markdown","7c5288bf":"markdown","f1cecdc4":"markdown","8940953d":"markdown","a0239619":"markdown","4b3501bf":"markdown","9c2ce4be":"markdown","21908a93":"markdown","b8dac53d":"markdown","dadf80de":"markdown","eb5bbcee":"markdown","9168f5ad":"markdown","0009f6a5":"markdown","3ac05cee":"markdown"},"source":{"292a1c4e":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nimport my_dao\nimport time_utils\nimport pretties\nimport stats\nimport process\nimport evaluation\nimport plotter\nimport download\n\nfrom bokeh.plotting import show, output_notebook","1b6ee7bf":"pretties.max_data_frame_columns()\npretties.decimal_notation()\noutput_notebook()","eb08794b":"train = my_dao.load_dataset(\"train\")\ntrain = train.groupby(\"store_dept\").apply(process.train_sales_semantic_enrichment)\n\ntest = my_dao.load_dataset(\"test\")\n\nfeat = my_dao.load_features()\nfeat = process.features_semantic_enrichment(feat)\n\nstores = my_dao.load_stores()","ce4d8869":"train = train.merge(feat, how=\"left\", left_on=[\"Store\", \"Date\"], right_on=[\"Store\", \"Date\"], suffixes=[\"\", \"_y\"])\ndel train[\"IsHoliday_y\"]\ndel train[\"timestamp_y\"]\ntrain = train.merge(stores, how=\"left\", left_on=[\"Store\"], right_on=[\"Store\"])","32ac6e11":"test = test.merge(feat, how=\"left\", left_on=[\"Store\", \"Date\"], right_on=[\"Store\", \"Date\"], suffixes=[\"\", \"_y\"])\ndel test[\"IsHoliday_y\"]\ndel test[\"timestamp_y\"]\ntest = test.merge(stores, how=\"left\", left_on=[\"Store\"], right_on=[\"Store\"])","638e3713":"cols = ['Date', 'Store', 'Dept', 'Weekly_Sales', 'pre_holiday', 'IsHoliday', 'pos_holiday', 'Fuel_Price', \n        'CPI', 'Unemployment', 'celsius', 'datetime', 'Type', 'sales_diff', 'sales_diff_p',\n        'Size', 'Temperature', 'timestamp', 'store_dept', \"day_n\", \"week_n\", \"month_n\", \"wm_date\", \"up_diff\", \"celsius_diff\", \"year\"]\n\ntrain = train[cols]\nprint(\"Shape: {}\".format(train.shape))\ntrain.sample(6)","0bfe971d":"print(\"Train\\n\")\nprint(\"Initial date: {}\".format(train[\"Date\"].iloc[0]))\nprint(\"Final date  : {}\".format(train[\"Date\"].iloc[-1]))\nprint(\"Time interval (months): {}\".format(time_utils.time_interval_months(train[\"Date\"])))\nprint(\"Time interval (years) : {}\".format(time_utils.time_interval_months(train[\"Date\"]) \/ 12))","92990d4b":"print(\"Test\\n\")\nprint(\"Initial date: {}\".format(test[\"Date\"].iloc[0]))\nprint(\"Final date  : {}\".format(test[\"Date\"].iloc[-1]))\nprint(\"Time interval (months): {}\".format(time_utils.time_interval_months(test[\"Date\"])))\nprint(\"Time interval (years) : {}\".format(time_utils.time_interval_months(test[\"Date\"]) \/ 12))","2d41ac8a":"timestamp_threshold = time_utils.str_datetime_to_timestamp(\"2012-02-01\", \"%Y-%m-%d\") #24 months from the first entry\n\nuse_train = train[train[\"timestamp\"] <= timestamp_threshold]\nuse_valid = train[train[\"timestamp\"] > timestamp_threshold]","4137ac85":"print(\"Fitting dataset time interval\\n\")\nprint(use_train[\"Date\"].head(1).append(use_train[\"Date\"].tail(1)))\nprint()\nprint(\"Time interval (months): {}\".format(time_utils.time_interval_months(use_train[\"Date\"])))\nprint(\"Time interval (years) : {}\".format(time_utils.time_interval_months(use_train[\"Date\"]) \/ 12))","6f89735a":"print(\"Validation dataset time interval\\n\")\nprint(use_valid[\"Date\"].head(1).append(use_valid[\"Date\"].tail(1)))\nprint()\nprint(\"Time interval (months): {}\".format(time_utils.time_interval_months(use_valid[\"Date\"])))\nprint(\"Time interval (years) : {}\".format(time_utils.time_interval_months(use_valid[\"Date\"]) \/ 12))","4d2a57cf":"try:\n    wm_data_train = my_dao.load_week_month_data(\"wm_data_train\")\nexcept FileNotFoundError:\n    wm_data_train = process.wm_data(use_train)\n    my_dao.save_week_month_data(wm_data_train, \"wm_data_train\")","a7f2a03c":"wm_data_train = process.format_wm_data_colnames(wm_data_train, \"train\")\nwm_data_train.sample(4)","78526b6c":"try:\n    wm_data_valid = my_dao.load_week_month_data(\"wm_data_valid\")\nexcept FileNotFoundError:\n    wm_data_valid = process.wm_data(use_valid)\n    my_dao.save_week_month_data(wm_data_valid, \"wm_data_valid\")","4cc7195c":"wm_data_valid = process.format_wm_data_colnames(wm_data_valid, \"valid\")\nwm_data_valid.sample(4)","d3e196f7":"xy = pd.merge(wm_data_train, wm_data_valid, \n              left_on=[\"wm_date\", \"store_dept\"], right_on=[\"wm_date\", \"store_dept\"], \n              how=\"inner\", suffixes=[\"_train\", \"_valid\"])\n\nxy[\"Store\"] = xy[\"year1_sales_train\"]\nxy[\"Dept\"] = xy[\"year1_size_train\"]\nxy[\"Date\"] = xy[\"Date_train\"]\n\nprint(\"Total groups: \", len(xy.drop_duplicates([\"wm_date\", \"store_dept\"])))\ndisplay(xy.head(4))\nprint(\"Hey, look the table above and check the first two columns (store_dept and wm_date)\")\nprint(\"They mean Store 29, Dept 5, Month_n 7 and 4th week of the month.\")\nprint(\"The following columns are the fields values for each year.\")","48244058":"%matplotlib inline\nxy.plot.scatter(\"year0_sales_train\", \"year1_sales_train\", \n                title=\"scatter plot - sales year0 vs sales year1\", \n               ylim=(0, 200000), xlim=(0, 200000), alpha=0.15, figsize=(6,6))","7a5d47e3":"%matplotlib inline\nxy.plot.scatter(\"year1_sales_train\", \"year0_sales_valid\", color=\"magenta\",\n                title=\"scatter plot - sales year1 vs sales year2\", \n               ylim=(0, 200000), xlim=(0, 200000), alpha=0.3, figsize=(6,6))","503e85e3":"print(\"NAs count on first year\")\nstats.freq(xy[\"year0_sales_train\"].isna())","8abf5012":"print(\"NAs count on second year\")\nstats.freq(xy[\"year1_sales_train\"].isna())","113c61dd":"print(\"NAs count on third year\")\nstats.freq(xy[\"year0_sales_valid\"].isna())","b18e27c5":"not_na_xy = xy[(xy[\"year0_sales_train\"].notna()) & (xy[\"year1_sales_train\"].notna()) & (xy[\"year0_sales_valid\"].notna())]","e3722ebb":"key_colnames = [\"Store\", \"Dept\", \"Date\"] #column names need to build submission file","8b1d2195":"fitting_cols = [\"year0_sales_train\", \"year0_size_train\"] #first year data\nx = not_na_xy[fitting_cols] \nx.head(4)","1766f604":"y = not_na_xy[[\"year1_sales_train\"]] #first year target\ny.head(4)","97fafd7d":"reg = LinearRegression().fit(x, y)\nprint(reg.score(x, y))\nprint(\"Function:\")\n\nprint(\"y = {} * {} + {} * {} + {}\".format(round(reg.coef_[0][0], 3), x.columns[0], round(reg.coef_[0][1], 3), x.columns[1], round(reg.intercept_[0]), 3))","b9de23d3":"x_valid = not_na_xy#[[\"year1_sales_train\", \"year1_size_train\"] + key_colnames + [\"year1_isholiday_train\"]]\nx_valid.head(4)","d5480cc3":"y_pred = reg.predict(x_valid[fitting_cols])","ac0fdb82":"%matplotlib inline\npd.DataFrame(y_pred)[0].plot.hist(title=\"Validation Predicted\")\ny_valid = not_na_xy[[\"year0_sales_valid\"]]\ny_valid.plot.hist(title=\"Validation Real\", color=\"magenta\")","7c0ee2e1":"x_valid[fitting_cols]","833e56a4":"x_valid = x_valid.rename({\"year0_isholiday_valid\": \"IsHoliday\", \"year0_sales_valid\": \"Weekly_Sales\"}, axis=1)\n\nsubm = evaluation.build_submission_df(test_df=x_valid[fitting_cols + key_colnames], \n                                      target_predicted=y_pred)\n\nprint(\"Validation prediction evaluation:\\n\")\nprint(evaluation.evaluate(subm, x_valid))","996b229c":"test_cols = [fitting_col.replace(\"0\", \"1\") for fitting_col in fitting_cols]","1ec8fafe":"test.sample(4)","43321366":"test[\"Date\"].head(1).append(test[\"Date\"].tail(1))","d868d4e9":"try:\n    wm_data_test = my_dao.load_week_month_data(\"wm_data_test\")\nexcept FileNotFoundError:\n    wm_data_test = process.wm_data(test)\n    my_dao.save_week_month_data(wm_data_test, \"wm_data_test\")","4221b858":"wm_data_test = process.format_wm_data_colnames(wm_data_test, \"test\")","8e1fcf13":"try:\n    wm_data_train_valid = my_dao.load_week_month_data(\"wm_data_train_valid\")\nexcept FileNotFoundError:\n    wm_data_train_valid = process.wm_data(train)\n    my_dao.save_week_month_data(wm_data_train_valid, \"wm_data_train_valid\")","db102cda":"wm_data_train_valid = process.format_wm_data_colnames(wm_data_train_valid, \"train\")","7d5ae213":"xy_test = pd.merge(wm_data_train_valid, wm_data_test, \n                   left_on=[\"wm_date\", \"store_dept\"], right_on=[\"wm_date\", \"store_dept\"], \n                   how=\"right\", suffixes=[\"_train\", \"_test\"])\n\nxy_test.sample(5)","bc456c2a":"print(\"NAs frequency over predictive columns\\n\")\nfor test_col in test_cols:\n    print(test_col)\n    print(stats.freq(xy_test[test_col].isna()))\n    print()","0900e65d":"not_na_xy_test = xy_test[xy_test[test_cols].notna().all(1)]\nna_xy_test = xy_test[~xy_test.index.isin(not_na_xy_test.index)]","72cd40f4":"stats.freq(na_xy_test[\"store_dept\"].isin(wm_data_train_valid[\"store_dept\"]))","610a0311":"na_xy_test_filled = na_xy_test.apply(lambda row : process.dummy_fill_store_dept_median(row, wm_data_train_valid, test_cols), axis=1)\nxy_test = not_na_xy_test.append(na_xy_test_filled)\n\nprint(\"NAs frequency over predictive columns\\n\")\nfor test_col in test_cols:\n    print(test_col)\n    print(stats.freq(xy_test[test_col].isna()))\n    print()","386f6ecb":"not_na_xy_test = xy_test[xy_test[test_cols].notna().all(1)]\nna_xy_test = xy_test[~xy_test.index.isin(not_na_xy_test.index)]\n\nna_xy_test_filled = na_xy_test.apply(lambda row : process.dummy_fill_store_median(row, wm_data_train_valid, test_cols), axis=1)\nxy_test = not_na_xy_test.append(na_xy_test_filled)\n\nprint(\"NAs frequency over predictive columns\\n\")\nfor test_col in test_cols:\n    print(test_col)\n    print(stats.freq(xy_test[test_col].isna()))\n    print()","e645db17":"x_test = not_na_xy_test.append(na_xy_test_filled)#[[\"year1_sales_train\", \"year1_size_train\"] + key_colnames + [\"IsHoliday_test\"]]\nx_test.head(4)","6fbbcb1c":"y_test_pred = reg.predict(x_test[test_cols])","8987d6ba":"subm = evaluation.build_submission_df(test_df=x_test, \n                                      target_predicted=y_test_pred,\n                                      store_colname=\"Store_test\", \n                                      dept_colname=\"Dept_test\", \n                                      date_colname=\"Date_test\")\n\n# subm[\"Weekly_Sales\"] = subm[\"Weekly_Sales\"].apply(lambda ws : round(ws, 4))","6611ebfb":"subm.reset_index().to_csv(\"submission.csv\", index=False)","86a33e11":"download.create_download_link(subm, \"Submission Download\", \"submission.csv\")","36650287":"# <font color=\"navy\">Walmart Recruiting Store Sales Forecasting<\/font>\nhttps:\/\/www.kaggle.com\/c\/walmart-recruiting-store-sales-forecasting","9db8ba62":"Now that train and validation dataset are placed as tables with <br>\n(Store, Dept, Week_n, Month_n) as its composite key (represented <br>\nby the columns store_dept and wm_date), it is time to merge them <br>\non this key. <br>\nThe **inner** merge is chosen because the sklearn.LinearRegression <br> \nfit method doesn't work with NaN values. <br>","41aae72c":"Some semantic enrichment on data were applied in order to have more <br> \npossibilities to explore data and, eventually, turn them into <br>\nproperly machine learning features. <br>\n<br>\nIn this context, the following fields were built: <br>\n<br>\n\n### Train dataset\n<ul>\n  <li>sales_diff : a number, represents the absolute difference between current and previous week Sales<\/li>\n  <li>sales_diff_p : a number, represents the relative difference between current and previous week Sales<\/li>\n  <li>up_diff : a boolean indication whether there was a positive difference in sales_diff<\/li>\n<\/ul>\n\n<br>\n\n### Features dataset\n<ul>\n  <li>pre_holiday : a boolean indication whether there is a Holiday in the following week<\/li>\n  <li>pos_holiday : a boolean indication whether there was a Holiday in the previous week<\/li>\n  <li>celsius : a number, represents the Temperature in Celsius scale<\/li>\n  <li>celsius_diff : a number, represents the absolute difference between current and previous week Temperature in Celsius<\/li>\n  <li>week_n : a number, represents the order of the week whitin its month<\/li>\n  <li>month_n : a number, represents the month<\/li>\n    \n<\/ul>\n\n### The <font color=\"navy\">store_dept<\/font>  and <font color=\"navy\">wm_date<\/font> composite key\nAs mentioned in the first section, my strategy uses the tuple <br>\n(Store,Dept) and the element that explains shopping behavior, <br>\nthe tuple (Week_n,Month_n) to group Weekly Sales for each year. <br>\n\nGot confused? <br>\nIt is easy! <br>\n\nGo ahead and check help texts close to outputs. :)","433d192b":"## Evaluation","57187419":"filling","c3307b72":"### Filling NaN values\nWhile merging both train and test Week-Month dataset, on the <br>\ncomposite key (Store, Dept, Week_n, Month_n) there were NA <br>\nvalues within Sales and Size of previous year data. <br>\n","85a6dfcb":"Looking to both charts above, at least the values dispersion seem to be very close.","6f8f048b":"### The input table description","354991ab":"This colnames updating is to move time window in one year.","3765f703":"### Loading...","aec3afc3":"## Test\nNow it is time to apply prediction to the test dataset.","da496387":"Merging train and test datasets with features dataset","a5a1e77e":"Train dataset time interval","12d286fe":"And this one in magenta color? <br>\nIt represents the same previous chart structure, but <bt>\nthis time with Weekly Sales from year1 to year2. <br>\nIn the right end of it the points seem to have a slight <br>\ncurve up, but most os the points tend to have the same <br>\nspread as the chart before.","efa96678":"### Train & Validation...\npartitions","f1289bb5":"#### Partition timestamp threshold\nA timestamp threshold to build **train** and **validation** partitions must be set. <br>\nAs the whole train dataset have a 32.7 months interval, I have decided to split fitting\/validation into 24 months for training and the rest 8 months for validation.<br>\nThe reason is that there will be 2 entrys for each (Store,Dept,week_n,month_n) composite key for fitting stage. <br>\nFitting data is stored in use_train.<br>\nValidation data is stored in use_valid.<br>","16d2a254":"What the cart above tell us? <br>\nDespite some outliers, the majority of the points lies close <br> \nto the 45 degree diagonal, which means that the Weekly Sales <br>\nbetween year0 and year1 tend, in general, to be close one to <br>\nanother.","e889fda6":"Ok, let's move forward. <br>\nTime to revome missing data on the second year (year1_train). <br>\nThere are valid values from year0_train but not for all <br>\nyear1_train.","7c5288bf":"## Results\nMy best score at Kaggle submission was <font color=\"navy\">3710.35797<\/font> which <br>\nwould place me at position <font color=\"navy\">334th<\/font>, over <font color=\"navy\">688<\/font> people. <br>\nIt was not expected to be in the Top <font color=\"navy\">48.5%<\/font> of participants as <br>\nthis approach was designed to be a simple contribution <br> \naiming an ensemble to improve traditional approaches. <br>\n\nConsidering that only two features were used, it can be <br>\nconcluded that this strategy (initial assumptions and data <br>\ntransformations based on Store-Dept-Week_n-Month_n) has a <br>\ngreat potential for ensembling or even be used on its own.<br>\n\nAs this case was required to be done in very short time, <br>\nthere are plenty of work that can be done as next steps. <br>\n\n## Next Steps \n\n### Time Series Similarity\nStore and Departaments that have similar Time Series shapes <br>\nmay me composing a semantic cluster. It is worth trying to <br>\ncheck if a Linear Regression can be fitted for each of these <br>\nclusters. <br>\n\n### Further Human Behavior Modeling\nAs in the initial assumptions I have stated that <br>\n(Week_n-Month_n) can explain human behavior, there may be <br>\nsome other human community behavior that is influenced by <br>\ncommunities' geolocalzation. We don't know this information <br>\nbut fields like CPI, Temperature and Fuel Price indexes may <br>\nbe used for clusterization. And these groups may represent <br>\nclose communities. <br>\n\n### Smoothing Filters and ETS Decomposition \nHodrick-Prescott filter was tried but it can be more explored. <br>\nETS (Error-Trend-Seasonality) was not applied and it could be given <br>\na chance for it. <br>\n\n### Ensemble Application With Traditional Models\nThis (Week_n, Month_) strategy may result into an awesome result if <br> \nused together with Exponential Smothing or ARIMA models. \n\n\n### Tracking Tests Execution\nA Design of Experiment wasn't applied at all, and it could help <br>\nindentify how to better switch between modules, models, parameters, <br>\nfeatures and so on...\n\n### More features\nRemember only two features were used. <br>\nHoliday data must be used due to its wheight relevance. \n","f1cecdc4":"# Future Sales Forecasting Problem\n### Introduction\nTrying to predict future sales is often realized as a Time Series <br>\nforecasting problem where the seek for Trend and Seasonality are <br>\nthe main tasks. Knowing both of these elements with the assumption <br>\nof steady external factors, it is a good starting point and models <br>\nlike Exponential Smoothing are frequently used. <br>\n<br>\nWhen seasonality is not clear enough, stochastic processes that use <br>\na fixed number of previous time states is used, such as ARIMA. <br>\nHowever this kind of forcasting is likely to propagate errors over <br>\ntime for a long unknown time window. <br>\n\n### My Strategy\nIn order to bring a new contribution, that can be ensembled with <br> \ntraditional strategies to solve this problem, I have made some <br> \nassumptions: <br>\n* Department Weekly Sales depend on the week order within the <br>\nsame month\n* The tuple Week-Month can explain human behavior, shopping is <br>\na human behavior, right?\n* The number of weeks in a month tend to repeat over the years, <br>\ndepending on how much Mondays a month have.\n* The sales variation from the same Store-Dept between Year0 and <br>\nYear1 is almost stable and can be replicated between Year1 and <br>\nYear2.\n* Store-Dept Weekly Sales is correlated to the Size of the Store the <br>\nDept is placed \n\nSo, having made these assumptions, it is reasonable to understand <br>\nthat the Sales from a specific Store-Dept in the next year can be <br>\na function of the same Store-Dept in the current year. Of course <br>\nfurther information should be add in order to improve accuracy. <br>\nTo perform this prediction, a **Linear Regression** is chosen and the <br>\nx1 representing the Weekly_Sales of Store-Dept and x2, the current <br>\nSize of the whole store.","8940953d":"## Applying to Validation dataset","a0239619":"# Forecasting","4b3501bf":"### <font color=\"navy\">Week-Month<\/font> data","9c2ce4be":"We are lucky! <br>\n99.70% of all (Store, Dept, Week_n, Month_n) with <br>\nmissing data have at least one entry for valid data <br>\nfor Weekly Sales. <br>\n<br>\nSo, let's take the median of all available Weekly <br>\nSales of this (Store, Dept) to fill missing values! <br>\nThe other 0.30% we can fill with the median of all <br>\nits Store Sales.","21908a93":"### Transformation into WM Data...\nThe input data for fitting stage is described some cells bellow :)","b8dac53d":"# Predicting Weekly_Sales for TEST dataset","dadf80de":"There are 10.45% of missing values that can be filled <br>\nused some ways. <br>\nMaybe we can replace all missing values from a <br>\n(Store, Dept, Week_n, Month_n) with the median of all <br>\nvalues from (Store, Dept), all Week_n and Dept_n. <br>\n<br>\nFirst question: how many (Store, Dept) with missing<br>\ndate are present with any Date?","eb5bbcee":"Now that you can see the merged dataset, it is time to explain it. <br>\nAs I said before, each entry of this table represents all possible  <br>\nfield values from a (Store, Department, Week, Month). Well, even  <br>\nthough not all fields are placed there, it was designed to recieve  <br>\nthe ones you want to. <br>\nThis approach started simple, as it should be, there will be used  <br>\nonly two input columns in the fitting stage: a) the Weekly_Sales of  <br>\n(Store,Dept) at each (Week_n, Month_n) time tag and b), the size of  <br>\nthe Store at that time tag. <br>\n<br>\nSo, the Linear Regression fitting stage will be performed using  <br>\nWeekly Sales and store Size, right? But how? <br>\nRemember the train and validation partition split? Right. <br>\nThere were reserved two years for the trainning dataset, so it is  <br>\npossible to train with year0's sales and size, targeting the year1  <br>\nsales. <br>\nThen, evaluate the prediction quality with the third year (year2)  <br>\nthat was reserved for the validation stage. =) <br>\n<br>\nObs.: Unfortunately the MarkDown fields only have valid values from Nov 2011.","9168f5ad":"Recap! <br>\nNow it's time to apply the Linear Regression function <br>\nfor the second year data to predict the next year sales.","0009f6a5":"## Fitting","3ac05cee":"# **The dataset**"}}