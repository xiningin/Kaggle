{"cell_type":{"2c4b5f5c":"code","ac1dec5e":"code","db727fad":"code","bbb1b08f":"code","c795f113":"code","2bad93b5":"code","0742aca7":"code","276f6e53":"code","09dd575c":"code","94d3fe1b":"code","905c3354":"code","1d570a0f":"code","a56ea5c0":"code","85094c3f":"code","eecb9d17":"code","a1247f71":"code","c55db232":"code","cb4e80a6":"code","78c35392":"code","01395315":"code","88ce664d":"code","274421f0":"code","05e0973c":"code","25f5fac1":"code","5cd8f242":"code","b030aae3":"code","79f1460a":"markdown","93417597":"markdown","3cc49f8a":"markdown","4e3a2ad3":"markdown","cdcc6173":"markdown","f8e2ac55":"markdown","628724fe":"markdown","a3548789":"markdown","44631854":"markdown","6527c2eb":"markdown","3640f138":"markdown","6b9252ee":"markdown","f948c06b":"markdown"},"source":{"2c4b5f5c":"import time\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport warnings\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore')","ac1dec5e":"train = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ntest = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/test.csv', parse_dates=['date'])\nsample_sub = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/sample_submission.csv')\ndf = pd.concat([train, test], sort=False)","db727fad":"df[\"date\"].min(), df[\"date\"].max()\n","bbb1b08f":"\ndef check_df(dataframe, head=5, tail=5, quan=False):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(tail))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n\n    if quan:\n        print(\"##################### Quantiles #####################\")\n        print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","c795f113":"check_df(train)\ncheck_df(test)\n","2bad93b5":"df.groupby([\"store\"])[\"item\"].nunique()","0742aca7":"df.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","276f6e53":"\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\nmissing_values_table(df, na_name=True)","09dd575c":"\ndef create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday \/\/ 4\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    df.loc[(df['day_of_month'] >= 1) & (df['day_of_month'] <= 7),\"first_week\"] = 1\n    df.loc[(df['day_of_month'] >= 28),'last_week'] = 1\n    df[\"first_week\"].fillna(0)\n    df[\"last_week\"].fillna(0)\n    return df\ndf = create_date_features(df)\ncheck_df(df)\n\n","94d3fe1b":"def random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))","905c3354":"\n\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)\n\ncheck_df(df)","1d570a0f":"\ndef lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\n\ndf = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])\n","a56ea5c0":"\ndef roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\n\ndf = roll_mean_features(df, [91,182,365, 546])\ndf.tail()\n","85094c3f":"def ewm_features(dataframe, alphas, lags):\n    dataframe = dataframe.copy()\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                    transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe","eecb9d17":"alphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n\ndf = ewm_features(df, alphas, lags)\ndf.tail()","a1247f71":"df = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])","c55db232":"#Converting sales to log(1+sales)\ndf['sales'] = np.log1p(df[\"sales\"].values)\n","cb4e80a6":"# Train set until the beginning of 2017 (until the end of 2016).\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\ntrain[\"date\"].min(), train[\"date\"].max()\n\n# validation first three months of 2017\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]\n\n# columns with no useful information or with information that is already derived will be dropped.\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n","78c35392":"\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num \/ denom)) \/ n\n    return smape_val\n\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False\n","01395315":"# Variables defined for train set\nY_train = train['sales']\nX_train = train[cols]\n\n# Variables defined for validation set\nY_val = val['sales']\nX_val = val[cols]\n\n# we checked the shapes of them\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape","88ce664d":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 15000,\n              'early_stopping_rounds': 200,\n              'nthread': -1}\n\n\nlgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)","274421f0":"\nmodel = lgb.train(lgb_params, lgbtrain,  # parameters\n                  valid_sets=[lgbtrain, lgbval],  # data\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=100)  # report for each 100 iteration\n","05e0973c":"y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)","25f5fac1":"# Final Model\n#####################################################\n# train and validation values are concatenated\n\ntrain = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]\n\n\n","5cd8f242":"\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}\n\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nfinal_model = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\ntest_preds = final_model.predict(X_test, num_iteration=model.best_iteration)\n","b030aae3":"\nsubmission_df = test.loc[:, ['id', 'sales']]\nsubmission_df['sales'] = np.expm1(test_preds)\nsubmission_df['id'] = submission_df.id.astype(int)\nsubmission_df.to_csv('submission.csv', index=False)","79f1460a":"# <span style=\"color:crimson;\"> Store Item Demand Forecasting Challenge\n\n* https:\/\/www.kaggle.com\/c\/demand-forecasting-kernels-only\n* 3-month item-level sales forecast for different store.\n* There are 10 different stores and 50 different items in a 5-year dataset.\n* Accordingly, we need to give the forecasts for 3 months after the store-item breakdown.","93417597":"# <span style=\"color:crimson;\"> Explanatory Data Analysis","3cc49f8a":"# <span style=\"color:crimson;\">Exponentially Weighted Mean Features","4e3a2ad3":"# <span style=\"color:crimson;\">One-Hot Encoding","cdcc6173":"# <span style=\"color:crimson;\">LigthGBM Model","f8e2ac55":"Here\n* If the model begins to memorize the train dataset instead of learning it,\n* the error will get lower but the model won't be able to have a good prediction\n* of the validation set (because it didn't learn the patterns) so the error in validation will begin to increase.\n","628724fe":"\n*  This method is used in creating moving averages for specified time intervals.\n*  Here we take the number of time given\n*  as window parameter and takes the average of the values, but one of\n*  the values is the value on this specific observation. In order to eliminate\n*  today's affect on moving average values, I will take 1 shift and use this f","a3548789":"# <span style=\"color:crimson;\">Custom Cost Function","44631854":"# <span style=\"color:crimson;\">Loading the data","6527c2eb":"# <span style=\"color:crimson;\">Rolling Mean Features","3640f138":"# <span style=\"color:crimson;\">Feature Engineering","6b9252ee":"* The value in time t highly depends on the value in time t-1,\n* so in order to have a better prediction, while computing the average value,\n*  we set higher weights to the recent time.","f948c06b":"# <span style=\"color:crimson;\">Lag\/Shifted Features"}}