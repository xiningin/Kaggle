{"cell_type":{"79798dca":"code","8d684a1a":"code","bbbd4526":"code","cb3d224c":"code","e2a71cec":"code","9685d88e":"code","32ab2b3e":"code","9b4d7928":"code","699637d7":"code","af2562dd":"code","014f1fbb":"code","067b4b1a":"code","0ae97e8c":"code","0a1d4fa2":"code","c3ef3f02":"code","f0e330e2":"code","e410e219":"code","639d3a77":"code","03f4da3c":"code","1f4f5c35":"code","4ed9de1d":"code","c05d7eef":"code","98580d91":"code","21f943cb":"code","7de26f73":"code","d2ea7fab":"code","2cdbbea7":"code","a378ab5e":"code","abaaf51e":"code","def4cea6":"code","6f1a920c":"code","48b506b3":"code","cf43f6d6":"code","889a90d0":"code","98a33726":"code","e261ca1a":"code","6f7451a9":"code","688e04d7":"code","5e272f42":"code","b673f928":"code","e269efc0":"code","5fcf90cb":"code","ed95c9e9":"code","b668110b":"code","115eada6":"code","c7044c56":"code","527b9afe":"code","5a5ae453":"code","bc4f4ea0":"code","5e0a52b5":"code","0591f2de":"code","495c4a73":"markdown","a985c84f":"markdown","20a03631":"markdown","24161f8b":"markdown","b0366f3d":"markdown","b0628e93":"markdown","16c882a3":"markdown","33773462":"markdown","6b79e425":"markdown","27c1a0f5":"markdown","81675068":"markdown","9e655b25":"markdown","f5a5ab1b":"markdown","caa48481":"markdown","6218b6c1":"markdown","324a2db9":"markdown","3e4a87c9":"markdown","760adf6a":"markdown","33cd94bc":"markdown","71bcaa48":"markdown","74564ec7":"markdown","33806f19":"markdown","dc01d32c":"markdown","9cecac10":"markdown","a901f4cd":"markdown","10b51586":"markdown","89b3ca44":"markdown","e13c60fb":"markdown","82bf23a3":"markdown","8606db15":"markdown","74f2bdbc":"markdown","b23ccec0":"markdown","297c7a2d":"markdown","d9578533":"markdown","0f78c030":"markdown","8b3a04a8":"markdown","9b00d719":"markdown","e7ae94fb":"markdown","6881974a":"markdown","4ab300c2":"markdown","939ee6dd":"markdown","ab0c5117":"markdown","cb827041":"markdown","0d787124":"markdown","9f235cc1":"markdown","1deeee15":"markdown","9c57e633":"markdown","38c802a4":"markdown","5f493dd8":"markdown"},"source":{"79798dca":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport warnings\nwarnings.filterwarnings('ignore')","8d684a1a":"train = pd.read_csv('..\/input\/forest-cover-type-prediction\/train.csv')\n\n# display train data\ntrain.head()","bbbd4526":"# drop ID column\ntrain = train.iloc[:,1:]\ntrain.head()","cb3d224c":"# size of data frame\ntrain.shape","e2a71cec":"# look at the data types of each feature and see if there needs to be any pre-processing\ntrain.dtypes","9685d88e":"# extract all numerical features from train\nnum_features = train.iloc[:,:10]\n\n# extract all binary features from train\ncat_features = train.iloc[:, 10:-1]","32ab2b3e":"num_features.describe()","9b4d7928":"cat_features.describe()","699637d7":"skew = train.skew()\nskew_df = pd.DataFrame(skew, index=None, columns=['Skewness'])","af2562dd":"print(skew)","014f1fbb":"plt.figure(figsize=(15,7))\nsns.barplot(x=skew_df.index, y='Skewness', data=skew_df)\nvar = plt.xticks(rotation=90)","067b4b1a":"train.groupby('Cover_Type').size()","0ae97e8c":"# plot bg\nsns.set_style(\"whitegrid\")\n\nplt.subplots(figsize=(21,14))\ncolor = sns.color_palette('pastel')\nsns.boxplot(data=num_features, orient='h', palette=color)\nplt.title('Spread of Data in Numerical Features', size=18)\nplt.xlabel('# of Observations', size=16)\nplt.ylabel('Features', size=16)\nplt.xticks(size=16)\nplt.yticks(size=16)\n\nsns.despine()\nplt.show()","0a1d4fa2":"# split cat_features\nwild_data, soil_data = cat_features.iloc[:,:4], cat_features.iloc[:,4:]\n\n# plot bg\nsns.set_style(\"darkgrid\", {'grid.color':'.1'})\nflatui = [\"#e74c3c\", \"#34495e\", \"#2ecc71\",\"#3498db\"]\n\n# use seaborn, pass colors to palette\npalette = sns.color_palette(flatui)\n\n# sum the data, plot bar\nwild_data.sum().plot(kind='bar', figsize=(10,8), color='#34a028')\nplt.title('# of Observations of Wilderness Areas', size=18)\nplt.xlabel('Wilderness Areas', size=16)\nplt.ylabel('# of Observations', size=16)\nplt.xticks(rotation='horizontal', size=12)\nplt.yticks(size=12)\n\nsns.despine()\nplt.show()","c3ef3f02":"# total count of each wilderness area\nwild_data.sum()","f0e330e2":"# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n# sum data, plot bar\nsoil_data.sum().plot(kind='bar', figsize=(24,12), color='#a87539')\nplt.title('# of Observations of Soil Types', size=18)\nplt.xlabel('Soil Types', size=16)\nplt.ylabel('# of Observations', size=16)\nplt.xticks(rotation=90, size=14)\nplt.yticks(size=14)\n\nsns.despine()\nplt.show()","e410e219":"# statistical description of highest observation of soil type\nsoil_data.loc[:,'Soil_Type10'].describe()","639d3a77":"# plot bg\nsns.set_style(\"darkgrid\", {'grid_color': '.1'})\n\n# sum soil data, pass it as a series\nsoil_sum = pd.Series(soil_data.sum())\nsoil_sum.sort_values(ascending=False, inplace=True)\n\n# plot bar\nsoil_sum.plot(kind='barh', figsize=(23,17), color='#a87539')\nplt.gca().invert_yaxis()\nplt.title('# of Observations of Soil Types', size=18)\nplt.xlabel('# of Observation', size=16)\nplt.ylabel('Soil Types', size=16)\nplt.xticks(rotation='horizontal',size=14)\nplt.yticks(size=14)\n\nsns.despine()\nplt.show()","03f4da3c":"# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n# set target variable\ntarget = train['Cover_Type']\n\n# features to be compared with target variable\nfeatures = num_features.columns\n\n# loop for violin plot\nfor i in range(0, len(features)):\n    plt.subplots(figsize=(16,11))\n    sns.violinplot(data=num_features, x=target, y=features[i])\n    plt.xticks(size=14)\n    plt.yticks(size=14)\n    plt.xlabel('Forest Cover Types', size=18)\n    plt.ylabel(features[i], size=18)\n    \n    plt.show()","1f4f5c35":"# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n# set target variable\ntarget = train['Cover_Type']\n# features to be compared with target variable\nfeatures = wild_data.columns\n\n# loop for violin plots\nfor i in range(0, len(features)):\n    \n    plt.subplots(figsize=(13,9))\n    sns.violinplot(data=wild_data, x=target, y=features[i])\n    plt.xticks(size=14)\n    plt.yticks(size=14)\n    plt.xlabel('Forest Cover Types', size=16)\n    plt.ylabel(features[i], size=16)\n    \n    plt.show()","4ed9de1d":"# plot bg\nsns.set_style(\"darkgrid\", {'grid.color':'.1'})\n\n# set target variable\ntarget = train['Cover_Type']\n# features compare with target variable\nfeatures = soil_data.columns\n\n# violin for loop\nfor i in range(0, len(features)):\n    plt.subplots(figsize=(13,9))\n    sns.violinplot(data=soil_data, x=target, y=features[i])\n    plt.xticks(size=14)\n    plt.yticks(size=14)\n    plt.xlabel('Forest Cover Types', size=16)\n    plt.ylabel(features[i], size=16)\n    \n    plt.show()","c05d7eef":"plt.subplots(figsize=(15,10))\n\n# compute correlation matrix\nnum_features_corr = num_features.corr()\n\n# generate mask for upper triangle\nmask = np.zeros_like(num_features_corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# generate heatmap masking the upper triangle and shrink the cbar\nsns.heatmap(num_features_corr, mask=mask, center=0, square=True, annot=True, annot_kws={\"size\": 15}, cbar_kws={\"shrink\": .8})\nplt.xticks(size=13)\nplt.yticks(size=13)\n\nplt.show()","98580d91":"# plot bg\nsns.set_style(\"darkgrid\", {'grid.color': '.1'})\n\n# paired features with positive correlation\nlist_data_corr = [['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology'],\n                  ['Elevation','Horizontal_Distance_To_Roadways'],\n                  ['Aspect','Hillshade_3pm'],\n                  ['Hillshade_3pm','Hillshade_Noon']]\n\n# loop through outer list\n# take 2 features from inner list\nfor i,j in list_data_corr:\n    plt.subplots(figsize=(15,12))\n    sns.scatterplot(data=train, x=i, y=j, hue=\"Cover_Type\", legend='full', palette='rainbow_r')\n    plt.xticks(size=15)\n    plt.yticks(size=15)\n    plt.xlabel(i, size=16)\n    plt.ylabel(j, size=16)\n    \n    plt.show()","21f943cb":"# count for more than 1 presence\nmore_count = 0\n# count for no presence\nnone_count = 0\n# total count\ntotal = 0\n\n# loop through each row of wilderness area column\nfor index, row in wild_data.iterrows():\n    # add the values of each col of that row\n    total = row.sum(axis=0)\n    \n    # check for greater than 1\n    if total > 1:\n        more_count += 1\n        total = 0\n        break\n        \n    # check for none    \n    if total == 0:\n        none_count += 1\n        total = 0\n        \nprint(f'We have {more_count} observations that shows presence in more than 1 Wilderness Area.')\nprint(f'We have {none_count} observations that shows no presence in any Wilderness Area.')","7de26f73":"# count for more than 1 presence\nmore_count = 0\n# count for no presence\nnone_count = 0\n# total count\ntotal = 0\n\n# loop through each row of soil type column\nfor index, row in soil_data.iterrows():\n    # add the values of each col of that row\n    total = row.sum(axis=0)\n    \n    # check for greater than 1\n    if total > 1:\n        more_count += 1\n        total = 0\n        break\n        \n    # check for none\n    if total == 0:\n        none_count += 1\n        total = 0\n\nprint(f'We have {more_count} observations that shows presence in more than 1 Soil Type Area.')\nprint(f'We have {none_count} observations that shows no presence in any Soil Type Area.')","d2ea7fab":"train.dropna()","2cdbbea7":"train.shape","a378ab5e":"# delete duplicates, except the first observation\ntrain.drop_duplicates(keep='first')","abaaf51e":"train.shape","def4cea6":"# create cat, num and y\nX_cat = train.iloc[:,10:54].values\nX_num = train.iloc[:,0:10].values\ny = train.iloc[:,-1].values","6f1a920c":"# scale\/standardize numerical columns\nscaler = StandardScaler() # scaler object\nscaler.fit(X_num) # fit training data\nX_num = scaler.transform(X_num) # scale num columns\n\n# shape\nprint(f'Categorical Shape: {X_cat.shape}')\nprint(f'Numerical Shape: {X_num.shape}')\nprint(f'Label Shape: {y.shape}')","48b506b3":"# combine num and cat\nX = np.hstack((X_num, X_cat))\nprint(X.shape)","cf43f6d6":"from sklearn.decomposition import PCA\n\npca = PCA().fit(X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('PCA Number of Compoenents for Cumulative Variance')","889a90d0":"from sklearn.ensemble import ExtraTreesClassifier\n\netc_model = ExtraTreesClassifier(random_state = 53) # pass the model\nX = train.iloc[:,:-1] # feed features to var X\ny = train['Cover_Type'] # feed target variable to y\n\netc_model.fit(X,y) # train the ETC model\n\n# extract feature importances\netc_feature_importances = pd.DataFrame(etc_model.feature_importances_, index=X.columns,\n                                      columns=['ETC']).sort_values('ETC', ascending=False)\n\netc_model = None # remove trace of this ETC model\netc_feature_importances.head(10)","98a33726":"from sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier(random_state = 53) # pass the model\nrfc_model.fit(X,y) # train the model\n\n# extract feature importances\nrfc_feature_importances = pd.DataFrame(rfc_model.feature_importances_, index=X.columns, \n                                       columns=['RFC']).sort_values('RFC', ascending=False)\n\nrfc_model = None # remove trace of this RFC model\nrfc_feature_importances.head(10)","e261ca1a":"from sklearn.ensemble import AdaBoostClassifier\n\nadb_model = AdaBoostClassifier(random_state = 53) # pass the model\nadb_model.fit(X,y) # train the model\n\n# extract feature importances\nadb_feature_importances = pd.DataFrame(adb_model.feature_importances_, index=X.columns,\n                                      columns=['ADB']).sort_values('ADB', ascending=False)\n\nadb_model = None # remove trace of this ADB model\nadb_feature_importances.head(10)","6f7451a9":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc_model = GradientBoostingClassifier(random_state = 53) # pass the model\ngbc_model.fit(X,y) # train the model\n\n# extract feature importances\ngbc_feature_importances = pd.DataFrame(gbc_model.feature_importances_, index=X.columns,\n                                      columns=['GBC']).sort_values('GBC', ascending=False)\n\ngbc_model = None # remove trace of GBC model\ngbc_feature_importances.head(10)","688e04d7":"sample = train[[\n    'Elevation','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points','Wilderness_Area4',\n    'Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Aspect','Hillshade_3pm','Hillshade_Noon',\n    'Hillshade_9am','Cover_Type'\n]]","5e272f42":"from sklearn.preprocessing import MinMaxScaler\n\n# pass range to the function and then save it\nscaler = MinMaxScaler(feature_range = (0,1))\n\n# X = sample.iloc[:,:-1] # feed sample features to X\n# y = sample['Cover_Type'] # feed target variable to y\n\nX_scaled = scaler.fit_transform(X) # apply feature scaling to all features","b673f928":"X_scaled","e269efc0":"# from sklearn.model_selection import train_test_split\n\n# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.20, random_state=53)","5fcf90cb":"# print(X_train.shape, X_test.shape)","ed95c9e9":"from sklearn.model_selection import cross_val_score\nimport time\n\n# function\ndef model_evaluation(clf):\n    clf = clf # pass classifier to variable\n    \n    t_start = time.time() # record time\n    clf = clf.fit(X_scaled, y) # classifier learning model\n    t_end = time.time() # record time\n    \n    c_start = time.time() # record time\n    accuracy = cross_val_score(clf, X_scaled, y, cv=10, scoring='accuracy')\n    f1_score = cross_val_score(clf, X_scaled, y, cv=10, scoring='f1_macro')\n    c_end = time.time() # record time\n    \n    # calculate mean of all 10 obs' accuracy and f1 as percent\n    acc_mean = np.round(accuracy.mean() * 100, 2)\n    f1_mean = np.round(f1_score.mean() * 100, 2)\n    \n    t_time = np.round((t_end - t_start) \/ 60, 3) # time for training\n    c_time = np.round((c_end - c_start) \/ 60, 3) # time for evaluating scores\n    \n    clf = None # remove traces of classifier\n    \n    print(f'The accuracy score of this classifier is: {acc_mean}%.')\n    print(f'The f1 score of this classifier is: {f1_mean}%.')\n    print(f'This classifier took {t_time} minutes to train and {c_time} minutes to evaluate CV and metric scores.')","b668110b":"from sklearn.naive_bayes import MultinomialNB\n\nmodel_evaluation(MultinomialNB())","115eada6":"from sklearn.neighbors import KNeighborsClassifier\nmodel_evaluation(KNeighborsClassifier(n_jobs=-1))","c7044c56":"from sklearn.ensemble import RandomForestClassifier\nmodel_evaluation(RandomForestClassifier(n_jobs=-1, random_state=53))","527b9afe":"from sklearn.linear_model import SGDClassifier\nmodel_evaluation(SGDClassifier(n_jobs=-1, random_state=53))","5a5ae453":"from sklearn.ensemble import ExtraTreesClassifier\nmodel_evaluation(ExtraTreesClassifier(n_jobs=-1, random_state=53))","bc4f4ea0":"from sklearn.linear_model import LogisticRegression\nmodel_evaluation(LogisticRegression(n_jobs=-1, random_state=53, solver='saga', max_iter = 500))","5e0a52b5":"from sklearn.model_selection import RandomizedSearchCV\n\n# number of trees in the forest algorithm\nn_estimators = [50, 100, 300, 500, 1000]\n\n# minimum number of samples required to split an internal node\nmin_samples_split = [2, 3, 5, 7, 9]\n\n# minimum number of samples required to be at a leaf node\nmin_samples_leaf = [1, 2, 4, 6, 8]\n\n# number of features to consider when looking for the best split\nmax_features = ['auto','sqrt','log2',None]\n\n# define the grid of hyperparameters to search\nhyperparameter_grid = {'n_estimators': n_estimators,\n                       'min_samples_leaf': min_samples_leaf,\n                       'min_samples_split': min_samples_split,\n                       'max_features': max_features}\n\n# create model\nbest_model = ExtraTreesClassifier(random_state=42)\n\n# create randomized search object\nrandom_cv = RandomizedSearchCV(estimator=best_model, param_distributions=hyperparameter_grid, cv=10,\n                               n_iter=20, scoring='accuracy', n_jobs=-1, verbose=1, return_train_score=True, random_state=0)\n\n# fit on all training data using random search object\nrandom_cv.fit(X_scaled, y)\nrandom_cv.best_estimator_","0591f2de":"from sklearn.metrics import accuracy_score, f1_score\n\nclf = ExtraTreesClassifier(n_estimators=50, random_state=42, min_samples_split=7, max_features=None) # best classifier\nclf = clf.fit(X, y) # train model\npredict = clf.predict(X) # predict unseen data\naccuracy = accuracy_score(y, predict) # calculate accuracy\nf1_score = f1_score(y, predict, average='macro') # calculate f1 score\n\naccuracy = np.round(accuracy * 100, 3)\nf1_score = np.round(f1_score * 100, 3)\n\nclf = None # clean traces\n\nprint(f'The accuracy score of our final model ETC on our testing set is {accuracy}%.')\nprint(f'The f1 score of our final model ETC on our testing set is {f1_score}%.')","495c4a73":"#### Skewness Inferences\n- `Soil_Type8` and `Soil_Type25` has the highest skewness. This means that the mass of the distribution is concentrated to the left and has long tail to the right followed by `Soil_Type9, 28 and 26`. This is also called **right skewed distribution**. \n    - We can see here that mostly all of the observations will have a 0 value for this feature in the **Feature Visualization Section**: **Barplot #3**\n- The `Hillshade` variables have a negatively skewed distribution.\n- ML algorithm can be very sensitive to such ranges of data and can give us inappropriate\/weak restuls. **Feature Scaling** will handle these as discussed earlier.","a985c84f":"### Feature Distribution\nNow we will plot how Wilderness_Area are distributed.","20a03631":"### Random Forest Classifier","24161f8b":"### 2. Random Forest Classifier","b0366f3d":"## Data Exploration\n### Feature Statistics\n- Part 1. Describe **numerical features**\n- Part 2. Describe **binary\/categorical features**","b0628e93":"### AdaBoost Classifier","16c882a3":"### Feature Comparison\nNext we will compare each feature in our data to the target variable. This will help us visualize how much dense and distributed each target variable's class is compared to the feature. We will use the violin plot to visualize.\n\n\n#### Violin Plot 4.1 Numerical Features Inferences:\n- `Elevation`\n    - `Cover_Type4` has the most forest cover at elevation between 2000m - 2500m.\n    - `Cover_Type3` has the fewest presence around that same elevation.\n    - `Cover_Type7` has observations of most elevated trees ranging as low as ~2800m to as high as ~3800m.\n        - `Cover_Type7` max value in elevation did belong to this forest type.\n        - This will be an important feature since every feature tells a different story to different classes of forest cover type. This could be useful in our algorithm.\n- `Aspect`\n    - This feature has a normal distribution for each class.\n- `Slope`\n    - Slope has lower values compared to most features as its measured in degrees and least to `Aspect` which is also measured in degrees.\n    - It has the least maximum value of all features. Looking at the plot we can say that it belongs to `Cover_Type2`.\n    - All classes have dense slope observations between 0-20 degrees.\n- `Horizontal_Distance_To_Hydrology`\n    - This has the right or positively skewed distribution where most of the values for all classes are towards 0-50m.\n- `Vertical_Distance_To_Hydrology`\n    - This is also positively skewed distribution but this takes on values much closer to 0 for all classes for most observations.\n    - The highest value in this feature belongs to `Cover_Type2`. This feature also has the least minimum value. In this case, `Cover_Type2` has the most range of observations compared to other classes.\n- `Hillshade_9am` and `Hillshade_Noon` are left or negatively skewed distribution where they take on max value between 200-250 index value for most observation in each class.\n- `Hillshade_3pm` has a normal distribution for all classes.","33773462":"#### 1. Check for `Wilderness_Area`","6b79e425":"# Train Final Model\nWe will be choosing `ExtraTreesClassifier` for our submission model.","27c1a0f5":"#### Part 2. Describe categorical features\n- Categorical variables will either have a value of 0 or 1. The **mean** can tell us useful information.\n    - `Wilderness_Area3` followed by `Wilderness_Area4` has the highest mean. This signifies that these variables have the most presence in the data compared to other Wilderness Area. Most of our features will consist of `Wilderness_Area3` and `Wilderness_Area4`.\n    - The least amount of observations will be seen from `Wilderness_Area2`.\n- One more to notice here is that when we add all the mean of `Wilderness_Area` we get a result 0.999999 which is approximately 1. This may mean all the observations can be from any one Wilderness Area. (Cross Check Here: **xx**)\n- Probability wise, the next observation that we get will have a 42.0% probability take from `Wilderness_Area3`, 30.9% probability take from `Wilderness_Area4` and so on for others. \n    - We can look into more details with the following plot in the *Feature Visualization Section*: **Barplot #2**.\n- Probability wise, we can document the same for `Soil_Types` too. \n    - We can look at **Barplot** #3 and plot xx in *Feature Visualization Section*.\n\n\nBy looking at these statistics of two different data types, we can see that there is different spreads and uneven amount of distribution. In this case we will feature scale these so that all the features have similar ranges between 0 and 1. Some algorithms can be sensitive to high values hence giving us inappropriate results while some algorithms are not. To be on the safe side, we will feature scale it and will do this in the **Data Engineering** section: **xx**.","81675068":"### 1. K-Nearest Neighbors","9e655b25":"### Gradient Boosting Classifier","f5a5ab1b":"### 4. Extra Trees Classifier","caa48481":"# Dimensionality Reduction\n- Based on EDA, we have lots of observations and features to train the model. This will make the algorithm run slowly, which may give ML models difficulty learning, overfitting in the training set, and do worse in submission\/testing.\n- We also see from checking for missing values and duplicates that `Wilderness_Area` and `Soil_Type` have no category that has no observations. This means that every feature has presence and we can't just delete because it may play an importance for the ML models in predicting classes.\n\n\nTo approach the problem, in this section we will see how each feature has an impact on predicting classes. We will use the following classifiers: **Extra Trees, Random Forest, Gradient Boosting Classifiers**. We will also use **AdaBoost** that will offer us the attribute `feature_importance_` to see which feature has more importance compared to others any by how much.","6218b6c1":"## Exploratory Data Analysis\n- Our dataset has **54** features and **1** target variable, `Cover_Type`. \n- From 54 features, 10 are numeric and 44 are categorical.\n- From 44 categorical, 40 are `Soil_Type` and 4 of `Wilderness_Area`\n- These are the following forest cover types in target variable `Cover_Type`:\n    1. Spruce\/Fir\n    2. Lodgepole Pine\n    3. Ponderosa Pine\n    4. Cottonwood\/Willow\n    5. Aspen\n    6. Douglas-fir\n    7. Krummholz","324a2db9":"#### Barplot #2: Number of Observations of Wilderness Areas Inferences:\n- Visually, we can see that `Wilderness_Area3` and `Wilderness_Area4` has the most presence.\n- `Wilderness_Area2` has the least amount of observations. Which confirms it will not have the most presence in our data.","3e4a87c9":"First we will define a function to train the models using the training data and calculate model's performance using `accuracy` and `f1 score`.","760adf6a":"#### Barplot #3: Number of Observations of Soil Type Inferences:\n\n\nNow we will plot the number of observations for `Soil Type`.\n- In the bar plot below, we can see that there many different types of distributions: **normale distribution, bimodal distribution, unimodal distribution, and left & right-skewed distribution** showing up in pieces.\n- The most observation is seen from `Soil_Type10` followed by `Soil_Type29`.\n    - From a statistical analysis, `Soil_Type10` has a presence in 14.1% of observations in the data.\n    - `Soil_Type10` also had the least skewed value of all in Soil Types as we had seen earlier in data exploration.\n- The variable with the least amount of observations are `Soil_Type7` and `Soil_Type15`.\n    - Soil Types has the most skewed values because these variables with a skew variable of 0 were so little, making it densely concentrated towards 0 and long flat tail to the right having form of **positively skewed distribution** or **right skewed distribution** (Details in *Feature Skew* Section).","33cd94bc":"# Preprocessing","71bcaa48":"# Hyperparameter Tuning","74564ec7":"#### Scatterplot #6 Features with correlation greater than 0.5\nLet's look at the paired features with correlation greater than 0.5. These will be the feature pairs with a positive correlation.\n\n#### Inferences:\n- `Hillshade_3pm` and `Aspect` represent a **sigmoid function** relationship. The data points at the boundaries mostly belong to `Cover_Type`: 3, 4, 5.\n- `Vertical_Distance_To_Hydrology` and `Horizontal_Distance_To_Hydrology` represent a **linear function** but more spread out.\n    - `Cover_Type`: 1, 2, 7 have more observations spreaded out.\n    - `Cover_Type`: 3, 4, 5, 6 are mode densely packed from 0-600m Horizontal_Distance_To_Hydrology\n- `Elevation` and `Horizontal_Distance_To_Roadways` is a spread out **linear function**.\n    - `Cover_Type` 1, 2, and 7 has the highest elevation and a widespread of points from 0m to ~7000m `Horizontal_Distance_To_Roadways`\n    - `Cover_Type` 4 and 6 have a densed dataset where there is both low elevation and horizontal distance to roadways in meters.\n- `Hillshade_Noon` and `Hillshade_3pm`\n    - `Cover_Type` 1, 2, 6 and 7 have a higher hillshade index at noon and 3pm.\n    - `Cover_Type` 4 and 5 have a lower hillshade index at noon and 3pm.","33806f19":"## Observation Cleaning\nThere may be a possibility where we `Soil_Type` and `Wilderness_Area` are recorded as present for more than one type or maybe none. We will check for each feature.\n\n#### Inference:\nIn both `Soil_Type` and `Wilderness_Area` we have no present in more than one type or none.","dc01d32c":"# Load Data & Setup","9cecac10":"# Model Evaluation\nNext we will feed our data to see how each model performs using two evaluation metrics: **accuracy** and **f1 score**:\n- **Accuracy** is the measure of the correct predicted data divided by total number of observations hence giving a value ranfing between 0 and 1, while 0 is no correctly predicted class and 1 is all correctly predicted class.\n- **f1 score** is more useful than accuracy epescially in the case where you have uneven amount of class distribution as in our case. It's the weighted average of precision and recall. Therefore, this score takes both false positives and false negatives into account.\n- Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it's better to look at both precision and recall or f1 score.\n\n\nWe will do the following:\n- Train the data on training set and test the performance on the benchmark model (Naive Bayes Classifier).\n- Use 10 K-Fold CV to test the performance of our model.","a901f4cd":"## PCA","10b51586":"### Feature Skew\n- For normal distribution, the skewness should be zero. Thus any balanced data should have a skewness near zero.\n- Negative values indicate data is skewed left. The left tail is long relative to the right tail.\n- Positive values indicate data is skewed right. The right tail is long relative the left tail.","89b3ca44":"#### Violin Plot 4.2 Wilderness Area Inferences:\n- `Wilderness_Area1` belongs to forest `Cover_Type1`, `Cover_Type2`, and `Cover_Type5`.\n- `Wilderness_Area3` belongs to all classes except `Cover_Type4`.\n- `Wilderness_Area2` and `Wilderness_Area4` has the least observations, their dense is less on 1 on all classes compared to `Wilderness_Area1` and `Wilderness_Area3`.","e13c60fb":"### 5. Logisitic Regression","82bf23a3":"## Models\nNow we will move on to measure performance:\n1. K-Nearest Neighbor (KNN)\n2. Random Forest (RF)\n3. Stochastic Gradient Descent Classifier (SGDC)\n4. Extra Trees Classifier (ETC)\n5. Logistic Regression (LR)","8606db15":"# Feature Engineering\nWe will do the following in the Feature Engineering section:\n- Look if any observations are present in more than one type of same category of `Wilderness_Area` and `Soil_Type`.\n- Delete columns which has '0' value for all observation.\n- Delete observations which has null values in any of its features.\n- Delete any duplicate entries.\n- Reduce features by keeping best.\n- Scale values in a specific range.\n- Perform Train-Test Split.","74f2bdbc":"### Feature Visualization\nFirst, we will visualize the spread and outliers of the data of numerical features.","b23ccec0":"#### Model Inferences:\n- All models work better than our benchmark model.\n- **ETC** performs the best with an accuracy of 84.59% and f1 score of 84.32% respectively taking the least amount of time running cross validation and metric results. Also given its flexibility it performed well with default parameters.\n- **RF** performs at second best to ETC. Along with ETC it is interesting to see as their results might be high and close enough. Some tuning of the parameters would probably get a better result.\n- **KNN** performs a little under RF. This is usual because KNN works well with datasets that have doublets. Since we checked the data for no duplicates, KNN can only choose the most similar samples by distance. While RF and ETC cal learn other definitions of locality which could stretch far by some features and short by others.","297c7a2d":"# Forest Cover Type Project","d9578533":"### Handling Duplicates\nThere are no duplicates.","0f78c030":"### 3. Stochastic Gradient Descent Classifier","8b3a04a8":"### Benchmark Model: `MultinomialNB Classifier`\nWe will not see how the performance of `MultinomialNB Classifier` on given training data. This performs quite quickly, but has poor **precision** and **recall**.","9b00d719":"### Handling Missing Values\nLooks like we have no missing values.","e7ae94fb":"Feed the top **10** features in a variable as a dataframe including the target variable. To determine this most of the similar features pop up in all four classifiers, while adding on the 10 additional **soil types** that were placed in the top.","6881974a":"### Extra-Trees Classifier","4ab300c2":"#### Part 1. Describe numerical features\n- **mean** of the feature varies from 16 to 2749.\n- **std** for `Horizontal_Distance_To_Roadways` is the most spread out data, followed by `Horizontal_Distance_To_Fire_Points` and `Elevation`.\n- The most desnsed and near to mean is `Slope` followed by all 3 features of `Hillshade`. \n    - See **Boxplot #1** in *Feature Visualization Section*\n- All features have a minimum value of 0 except `Elevation` and `Vertical_Distance_To_Hydrology` features.\n    - `Elevation` has the highest minimum value and `Vertical_Distance_To_Hydrology` has a negative value.\n- `Hillshades` features except `Hillshade_3pm` have a similar maximum value.\n- `Horizontal_Distance_To_Fire_Points` has the highest maximum value followed by `Horizontal_Distance_To_Roadways` features. They also have the highest ranges of all features.\n- `Slope` has the lowest maximum value and range. The `Aspect` feature follows closely behind this same concept.\n\nIt is good to note that the reason some features are widely spread and have high values, is because 5 out of the 10 variables are measured in meters. These variables are: `Elevation`,`Horizontal_Distance_To_Hydrology`,`Vertical_Distance_To_Hydrology`,`Horizontal_Distance_To_Roadways`,`Horizontal_Distance_To_Fire_Points`. This makes sense that these have high values and ranges.\n\n\nFeatures like `Aspect` and `Slope` are measured in degrees which means there maximum values can't go above 360. `Hillshade` features can only take on a maximum value of 255.\n","939ee6dd":"#### Boxplot #1: Numerical Features Inferences\n- `Slope` is the most squeezed box plot. It having a least range means that the **median** and **mean** will be quite close.\n- `Aspect` features is the only one with little to none outliers. Since both `Aspect` and `Slope` are measure in degrees, `Aspect` takes on much bigger range than `Slope` because it has the lowest max score, which means `Aspect` is less densed than `Slope`.\n- The `Hillshade` features also have a similar plot to Slope, which includes many outliers and taking on a smaller range.\n- `Vertical_Distance_To_Hydrology` is also similar to Slope except here the minimum value is negative.\n- `Elevation` is the only feature that doesn't have a minimum value of 0. It is instead plotted in the middle having many outliers too.\n- `Horizontal_Distance_To_Roadways` has the most spread out data of all features. This is because it has highest standard deviation score. `Horizontal_Distance_To_Fire_Points` has a similar look, but it has the maximum value.\n    - If we compare these two features, the last 50% of `Horizontal_Distance_To_Roadways` is much more spread and less dense compared to `Horizontal_Distance_To_Fire_Points`, hence having a high standard deviation score.","ab0c5117":"#### 2. Check for `Soil_Type`","cb827041":"## Train-Test Split\nNow we can split into 75% - 25% train-test set respectively.","0d787124":"Since all variables are numeric integers, there are no need for further conversions.","9f235cc1":"#### Violin Plot 4.3 Soil Type Inferences:\n- `Soil_Type4` is the only soil type that has presence in all forest cover types.\n- `Soil_Type`: 7 and 15 visually, have little to no presence in all forest cover types.\n- `Soil_Type`: 3 and 6 has presence in `Cover_Type`: 2, 3, 4, 6\n- `Soil_Type`: 10, 11, 16, and 17 and has presence in `Cover_Type` 1 thru 6.\n- `Soil_Type`: 23, 24, 31 and 33 has presence in `Cover_Type`: 1, 2, 5, 6, 7.\n- `Soil_Type`: 29 and 30, has presence in `Cover_Type`: 1, 2, 5, 7.\n- `Soil_Type`: 22, 27, 35, 38, 39, and 40 has presence in `Cover_Type`: 1, 2, and 7.\n- `Soil_Type`: 18 and 28 has presence in `Cover_Type`: 2 and 5.\n- `Soil_Type`: 19 and 26 has presence in `Cover_Type`: 1, 2, and 5.\n- `Soil_Type`: 8 and 25 has presence in only `Cover_Type2`.\n- `Soil_Type`: 1, 5, and 14 has presence in `Cover_Type`: 3, 4, and 6.\n- `Soil_Type37` has presence in `Cover_Type7`.\n\n\n- `Cover_Type4` has the least amount of `Soil_Type` count.\n- `Cover_Type2` has the most presence in `Soil_Type` count.","1deeee15":"### Feature Correlation\nPart of our data is binary. A **correlation matrix** requires continuous data, so we will exclude binary data.\n\n\n- Features that less or no correlation will be indicated by the color **black**.\n- Features with positive correlation are colored **orange**.\n- Features with negative correlation are colored **blue**.\n\n\n#### Correlation Plot #5 Inferences:\n- `Hillshade_3pm` and `Hillshade_9am` show a high negative correlation.\n- `Hillshade_3pm` and `Aspect` show a high positive correlation.\n- `Hillshade_3pm` and `Aspect` also had the most normal distribution compared to forest cover type classes (**Plot 4.1**)\n- The following pairs had a positive correlation:\n    - `Vertical_Distance_To_Hydrology` and `Horizontal_Distance_To_Hydrology`\n    - `Horizontal_Distance_To_Roadways` and `Elevation`\n    - `Hillshade_3pm` and `Aspect`\n    - `Hillshade_3pm` and `Hillshade_Noon`\n- The following pairs had a negative correlation:\n    - `Hillshade_9am` and `Aspect`\n    - `Hillshade_Noon` and `Slope`\n- The following pair has no correlation:\n    - `Hillshade_9am` and `Horizontal_Distance_To_Roadways`\n- The least correlated value tells us that each feature has different valuable information that could be important features for predictions.","9c57e633":"#### Dimensionality Reduction Inferences:\n- We can see the **RFC** and **ETC** show similar results. The features do show up in different ranks, but not a great difference.\n- In the **ADB**, the top 8 features are enough to predict classess. It is interesting to see that `Wilderness_Area4` ties with `Elevation` because scores pretty low in the other classifiers.\n- `Elevation` takes on a similar dominance in each classifier.\n- `Hillshade` features are seen on the top 10 list of every classifier except for **ADB**.\n- In the feature visualization section of *correlation*, we saw that `Hillshade` features had a nice correlation with each other and other features like `Slope`, `Aspect`, and `Horizontal_Distance_To_Roadways`. They also show dominance in predicting, meaning they might had correlated but they have useful information in predicting the target variable.\n- `Elevation`, `Vertical` and `Horizontal` Distance to Hydrology show presence in top 10 for all classifiers.\n- `Horizontal Distance To Roadways` and `Fire Points` have the highest standard deviation including outliers im all classifiers except in **ADB**.\n\nAll this being said, these classification show that numerical features dominate when it comes to predicting forest classes. Now we will consider the top 15 to 20 features as a reasonable choice.","38c802a4":"## Feature Scaling\nBefore train-test split, we will scale the features to some specific range. We will scale all feature values to specific range of 0 to 1. Before doing this we will split the feature and target variables because we do not want to scale our target variable.","5f493dd8":"### Class Distribution\nNow we will look at the class distribution for `Cover_Type` by grouping it and calculating total occurrence.\n\n\nWe can see that `Cover_Type` has an equal distribution."}}