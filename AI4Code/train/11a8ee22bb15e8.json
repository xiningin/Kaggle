{"cell_type":{"5202e0f0":"code","a25e0c60":"code","b7265a21":"code","5b64cffd":"code","2920654c":"code","95a75838":"code","46ae74b1":"code","eeef7fdd":"code","fca56376":"code","be224901":"code","c8f75308":"code","59e8e061":"code","1eb4f85b":"code","2b8eb40a":"code","c0879ab8":"code","afa6170c":"code","ff0d07a9":"code","1a64372b":"code","12ef4343":"code","54cb7527":"code","a5082e56":"code","80da1f7f":"code","b8a9ca09":"code","61d57e5a":"code","1478bb7e":"code","ac27b3ad":"markdown","a197f7cf":"markdown","c5f11f34":"markdown"},"source":{"5202e0f0":"import pandas as pd\nimport numpy as np\nfrom sklearn import *\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()","a25e0c60":"#pd.options.display.max_rows = 200\n#market_train[market_train.returnsOpenNextMktres10.abs()>1]\n\n#market_train[market_train.assetCode==\"ADI.N\"]\n","b7265a21":"#check and drop duplicates\n#market_train_sim[market_train_sim[['formatdate','assetCode']].duplicated()]\n#news_train_group.drop_duplicates(inplace=True)","5b64cffd":"#filter\n#news_train_sim[news_train_sim.assetName.str.contains('Unknown')]\n#market_train[market_train.assetName.str.contains('Caleres Inc')]\n#merge_train[merge_train.returnsOpenNextMktres10 > 1]\n\n\n#news_train[news_train.sourceId.value_counts()>1]","2920654c":"#asset_Time.groupby(['assetName','formatdate']).size()\n","95a75838":"#merge_train.returnsOpenNextMktres10.quantile(.9999)","46ae74b1":"# Import the libraries\n#import matplotlib.pyplot as plt\n#import seaborn as sns\n\n# matplotlib histogram\n#plt.hist(merge_train['returnsOpenNextMktres10'], color = 'blue', edgecolor = 'black',\n#         bins = 1)\n\n# seaborn histogram\n#sns.distplot(merge_train['returnsOpenNextMktres10'], hist=True, kde=False, \n#             bins=1, color = 'blue',\n#             hist_kws={'edgecolor':'black'})\n# Add labels\n#plt.title('Histogram of Arrival Delays')\n#plt.xlabel('Delay (min)')\n#plt.ylabel('Flights')","eeef7fdd":"#scatterplot\n#fig, ax = plt.subplots(figsize=(16,8))\n#ax.scatter(market_train['returnsClosePrevMktres10'], market_train['returnsOpenNextMktres10'])\n#ax.set_xlabel('returnsClosePrevMktres10')\n#ax.set_ylabel('returnsOpenNextMktres10')\n#lt.show()","fca56376":"#news_train['time'] = news_train.time.dt.hour\n#news_train['asset_sentiment_count'] = news_train.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n#market_train['volume_to_mean'] = market_train['volume'] \/ market_train[['volume','assetCode']].mean()\nmarket_train = market_train[market_train.returnsOpenNextMktres10<1][market_train.returnsOpenNextMktres10>-1]\n\n#[news_train['assetName']=='PetroChina Co Ltd'][news_train['asset_sentiment_count']==3992]","be224901":"def calcRsi(series, period = 14):\n    \n    \"\"\"\n    Calculate the RSI of a data series \n    \n    Parameters\n    ----------\n    series : pandas series\n        Candle sticks dataset\n    period : int\n        Period of each calculation\n        \n    Returns\n    -------\n    rsi : float\n        the calculated rsi\n    \"\"\"\n    try:\n        delta = series.diff().dropna()\n        u = delta * 0\n        d = u.copy()\n        u[delta > 0] = delta[delta > 0]\n        d[delta < 0] = -delta[delta < 0]\n        u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n        u = u.drop(u.index[:(period-1)])\n        d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n        d = d.drop(d.index[:(period-1)])\n\n        rs = u.ewm(com=period-1, adjust=False).mean() \\\n            \/ d.ewm(com=period-1, adjust=False).mean()\n        \n        rsi = 100 - 100 \/ (1 + rs)\n    except IndexError:\n        rsi = 0\n    \n    return rsi","c8f75308":"def addBollinger(df, period=20, col='close'):\n    \"\"\"\n    Add the simple moving average column to dataframe \n\n    Parameters\n    ----------\n    df : pandas dataframe\n        Candle sticks dataset\n    period : int\n        Period of each calculation\n\n    Returns\n    -------\n    None\n    \"\"\"\n    bbmid_series = df[col].rolling(window=period).mean()\n    series_stdev = df[col].rolling(window=period).std()\n    df['BBUpperBand'] = bbmid_series + 2*series_stdev\n    df['BBLowerBand'] = bbmid_series - 2*series_stdev\n    df['BBBandwidth'] = df['BBUpperBand'] - df['BBLowerBand']  \n    df['BBMiddleBand'] = bbmid_series\n    return df","59e8e061":"def addMACD(df):\n    ema_fast = df['close'].ewm(span=12).mean()\n    ema_slow = df['close'].ewm(span=26).mean()\n    signal_line = df['close'].ewm(span=9).mean()\n    df['macd'] = ema_fast - ema_slow\n    df['macd_signal'] = df.macd.ewm(span=9, adjust=False).mean()\n    df['macdh'] = df['macd'] - df['macd_signal']\n    return df","1eb4f85b":"def allindicator(market_train):\n    market_train['RSI'] = calcRsi(market_train['close'], 14)\n    market_train = addBollinger(market_train)\n    market_train = addMACD(market_train)\n    return market_train","2b8eb40a":"from multiprocessing import Pool\n","c0879ab8":"def groupAsset(market_train):\n    all_df = []\n    df_codes = market_train.groupby('assetCode')\n    df_codes = [df_code[1] for df_code in df_codes]\n    pool = Pool(4)\n    all_df = pool.map(allindicator, df_codes)\n    new_df = pd.concat(all_df)  \n    pool.close()\n    del all_df, df_codes, market_train\n    return new_df","afa6170c":"def data_prep(market_train,news_train):\n    \n    market_train = groupAsset(market_train)\n    \n    market_train['formatdate']=market_train['time'].dt.date\n    news_train['formatdate'] = news_train.firstCreated.dt.date\n    \n    for col_cat in ['headlineTag', 'provider', 'sourceId']:\n        news_train[col_cat], uniques = pd.factorize(news_train[col_cat])\n        del uniques\n    \n    news_train_sim = news_train.drop(columns=['time','sourceTimestamp','firstCreated','headline','subjects','audiences','assetCodes'])\n    news_train_1 = news_train_sim.groupby(by=['assetName','formatdate'],as_index=False,observed=True).mean().add_suffix('_mean')\n    news_train_1.rename(columns={'assetName_mean': 'assetName', 'formatdate_mean': 'formatdate'}, inplace=True)\n    #news_train_2 = news_train_sim.groupby(by=['assetName','formatdate'],as_index=False,observed=True).sum().add_suffix('_sum')\n    #news_train_2.rename(columns={'assetName_sum': 'assetName', 'formatdate_sum': 'formatdate'}, inplace=True)\n    #news_train_3 = news_train_sim.groupby(by=['assetName','formatdate'],as_index=False,observed=True).var()\n    #news_train_group = pd.merge(news_train_1,news_train_2.iloc[:, np.r_[0,1,2:3]],on=['assetName','formatdate'])\n    #,news_train_3,on=['assetName','formatdate'])\n    merge_train = pd.merge(market_train, news_train_1, how='left', left_on=['formatdate', 'assetName'], \n                                right_on=['formatdate', 'assetName'])\n    merge_train.fillna(0,inplace=True)\n    #merge_train.drop(['marketCommentary_sum','noveltyCount24H_mean','noveltyCount3D_mean','marketCommentary_mean','urgency_sum',\n    #                 'urgency_mean','noveltyCount5D_mean','bodySize_sum','noveltyCount12H_mean','provider_sum','sentimentClass_mean',\n    #                 'takeSequence_sum','volumeCounts12H_mean','headlineTag_mean','wordCount_sum','headlineTag_sum'],1,inplace=True)\n    del market_train, news_train_sim, news_train_1\n    \n    #merge_train.apply(lambda x: x.fillna(x.mean()),axis=0)\n    \n    \n    \n    return merge_train\n\n","ff0d07a9":"merge_train = data_prep(market_train,news_train)\nup = merge_train.returnsOpenNextMktres10 >= 0\nup = up.values\n\nfcol = [c for c in merge_train.columns if c not in ['assetCode','assetName','time',\n                                             'returnsOpenNextMktres10', 'formatdate', 'universe']]\nX = merge_train[fcol].values\nr = merge_train.returnsOpenNextMktres10.values\n\n# Scaling of X values\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) \/ rng)","1a64372b":"X_train, X_test, up_train, up_test, r_train, r_test = model_selection.train_test_split(X, up, r, test_size=0.1, random_state=99)\n","12ef4343":"import lightgbm as lgb\nparams = {'learning_rate': 0.3, 'max_depth': 10, 'boosting': 'gbdt', 'objective': 'binary', 'metric': 'binary_logloss', 'is_training_metric': True, 'seed': 42}\nmodel = lgb.train(params, train_set=lgb.Dataset(X_train, label=up_train), num_boost_round=2000,\n                  valid_sets=[lgb.Dataset(X_train, label=up_train), lgb.Dataset(X_test, label=up_test)],\n                  verbose_eval=50, early_stopping_rounds=30)","54cb7527":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\ndef generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: np.random.randint(0, 255), range(3)))\n    return color\n\ndf = pd.DataFrame({'imp': model.feature_importance(), 'col':fcol})\ndf = df.sort_values(['imp','col'], ascending=[True, False])\ndata = [df]\nfor dd in data:  \n    colors = []\n    for i in range(len(dd)):\n         colors.append(generate_color())\n\n    data = [\n        go.Bar(\n        orientation = 'h',\n        x=dd.imp,\n        y=dd.col,\n        name='Features',\n        textfont=dict(size=20),\n            marker=dict(\n            color= colors,\n            line=dict(\n                color='#000000',\n                width=0.5\n            ),\n            opacity = 0.87\n        )\n    )\n    ]\n    layout= go.Layout(\n        title= 'Feature Importance of LGB',\n        xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n        yaxis=dict(title='Value Count', ticklen=5, gridwidth=2),\n        showlegend=True\n    )\n\n    py.iplot(dict(data=data,layout=layout), filename='horizontal-bar')","a5082e56":"merge_train","80da1f7f":"\nn_days = 0 \nfor (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days():\n    n_days +=1\n    if n_days % 50 == 0:\n        print(n_days,end=' ')\n    market_obs_df = data_prep(market_obs_df, news_obs_df)    \n    X_live = market_obs_df[fcol].values\n    \n    # Scaling of X values\n    #mins = np.min(X_live, axis=0)\n    #maxs = np.max(X_live, axis=0)\n    #rng = maxs - mins\n    X_live = 1 - ((maxs - X_live) \/ rng)\n    \n    \n    \n    \n    lp = model.predict(X_live)\n    \n    confidence = 2 * lp -1\n      \n    #market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n\n","b8a9ca09":"env.write_submission_file()","61d57e5a":"#groupA = merge_train.groupby('assetCode')['time'].count().reset_index(name = 'count').sort_values('count').head(100)\n#groupA","1478bb7e":"#import plotly.graph_objs as go\n#import plotly.offline as py\n#py.init_notebook_mode(connected=True)\n#data = []\n##for asset in np.random.choice(market_train_df['assetName'].unique(), 10):\n#asset = \"ADI.N\"\n#asset_df = merge_train[(merge_train['assetCode'] == asset)]\n\n#data.append(go.Scatter(\n#    x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n#    y = asset_df['BBUpperBand'].values,\n#    name = asset\n#))\n#layout = go.Layout(dict(title = \"Closing prices of 10 random assets\",\n#              xaxis = dict(title = 'Month'),\n#              yaxis = dict(title = 'Price (USD)'),\n#              ),legend=dict(\n#            orientation=\"h\"))\n#py.iplot(dict(data=data, layout=layout), filename='basic-line')\n","ac27b3ad":"For good measure, we can check what XGBoost bases its decisions on","a197f7cf":"A side effect of treating this as a binary task is that we can use a simpler metric to judge our models","c5f11f34":"# XGBoost Baseline\n\nThis notebook rephrases the challenge of predicting stock returns as the challenge of predicting whether a stock will go up. The evaluation  asks you to predict a confidence value between -1 and 1. The predicted confidence value gets then multiplied with the actual return. If your confidence is in the wrong direction (ie. you predict positive values while returns are actually negative), you loose on the metric. If your direction is right however, you want your confidence be as large as possible.\n\nStocks can only go up or down, if the stock is not going up, it must go down (at least a little bit). So if we know our model confidence in the stock going up, then our new confidence is:\n$$\\hat{y}=up-(1-up)=2*up-1$$\n\nWe are left with a \"simple\" binary classification problem, for which there are a number of good tool, here we use XGBoost, but pick your poison.\n\n**Edit**: Updated XGB tuning to the ones suggested by https:\/\/www.kaggle.com\/alluxia\/lb-0-6326-tuned-xgboost-baseline"}}