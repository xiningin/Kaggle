{"cell_type":{"67596e5f":"code","9c06da60":"code","3271a937":"code","ce31c48d":"code","d97ad809":"code","89d1d146":"code","fb89cb7e":"code","a69723b6":"code","786e68c9":"code","20521047":"code","1b2e8385":"code","5dc40b4a":"code","6c2a8e93":"code","01e7f7b2":"code","f228bd17":"code","112968e5":"code","a62e727b":"code","1413fb15":"code","d70cfd8c":"code","02970ce2":"code","d5462b62":"code","1bb12099":"code","df93cd85":"code","d3e901b0":"code","fcb6c475":"code","0762640a":"code","bce6fab6":"code","3042bb55":"code","de0cab62":"code","394bdff5":"code","f8c8fd59":"code","b21f92f6":"code","a6ef5e07":"code","c649c839":"code","5c9deb7f":"code","fe451280":"code","c90b0510":"code","11863ae1":"code","64fdd66d":"code","51fe370f":"code","9e939e9e":"code","38442f6f":"code","54dc02ed":"code","09f6c343":"code","139c23e8":"code","4b814bc3":"code","8d529af7":"code","ab54a5fe":"code","2c951dc1":"code","6a361cb1":"code","17499e50":"code","56aa5437":"code","528c67d6":"code","e02e5a37":"code","1b77bb85":"code","ee5b8050":"code","d93efc0e":"code","9be9e194":"code","334ffae1":"code","e208c3df":"code","fb56d998":"markdown","7b29ee7d":"markdown","4d8e3bd6":"markdown","0937cd6c":"markdown","e1e62e14":"markdown","7aec335f":"markdown","b80230fa":"markdown","0be139e3":"markdown","b9393757":"markdown","02aae9c8":"markdown","681a331d":"markdown","56303768":"markdown","8c14f758":"markdown","9b512639":"markdown","76d3be90":"markdown","ffc9f0fc":"markdown","4dd6d300":"markdown","02e616dc":"markdown","293fbe24":"markdown","08f06810":"markdown","011644c2":"markdown","143bfc77":"markdown"},"source":{"67596e5f":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n","9c06da60":"import numpy as np \nimport pandas as pd \nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,plot_confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization\nimport missingno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk import sent_tokenize,word_tokenize","3271a937":"df_train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","ce31c48d":"df_train.head()","d97ad809":"df_train.shape","89d1d146":"missingno.bar(df_train,sort=\"ascending\", figsize=(10,5), fontsize=12)","fb89cb7e":"sns.countplot(df_train['target'])","a69723b6":"plt.figure(figsize=(12,8))\nsns.barplot(x=df_train['location'].value_counts()[:20],y=df_train['location'].value_counts()[:20].index)\n","786e68c9":"len(df_train)","20521047":"df_train[df_train['target']==0]['location'].value_counts()[:20]\n","1b2e8385":"df_train[df_train['target']==0]['location'].value_counts()[:20].index\n","5dc40b4a":"fig, axes = plt.subplots(2, 1, figsize=(20, 30))\nfig.suptitle('Location',size=30)\n\n\n\nsns.barplot(ax=axes[0],x=df_train[df_train['target']==0]['location'].value_counts()[:15],y=df_train[df_train['target']==0]['location'].value_counts()[:15].index)\naxes[0].set_title('Non-Disaster',size=25)\ny0=df_train[df_train['target']==0]['location'].value_counts()[:15].index\naxes[0].set_yticklabels(y0, fontsize=20)\n\nsns.barplot(ax=axes[1],x=df_train[df_train['target']==1]['location'].value_counts()[:15],y=df_train[df_train['target']==1]['location'].value_counts()[:15].index)\naxes[1].set_title('Disaster',size=25)\ny1=df_train[df_train['target']==1]['location'].value_counts()[:15].index\naxes[1].set_yticklabels(y1, fontsize=20)\n","6c2a8e93":"df_train[df_train['location']=='India'][['text','target']]","01e7f7b2":"list(set(df_train['keyword']))==list(set(df_test['keyword']))","f228bd17":"plt.figure(figsize=(12,8))\nsns.barplot(x=df_train['keyword'].value_counts()[:15],y=df_train['keyword'].value_counts()[:15].index)","112968e5":"df_train[df_train['target']==0]['keyword'].value_counts()","a62e727b":"fig, axes = plt.subplots(2, 1, figsize=(20, 30))\nfig.suptitle('Keywords',size=30)\n\nsns.barplot(ax=axes[0],x=df_train[df_train['target']==1]['keyword'].value_counts()[:30],y=df_train[df_train['target']==1]['keyword'].value_counts()[:30].index)\naxes[0].set_title('Disaster',size=25)\ny0=df_train[df_train['target']==1]['keyword'].value_counts()[:30].index\naxes[0].set_yticklabels(y0, fontsize=20)\n\nsns.barplot(ax=axes[1],x=df_train[df_train['target']==0]['keyword'].value_counts()[:30],y=df_train[df_train['target']==0]['keyword'].value_counts()[:30].index)\naxes[1].set_title('NonDisaster',size=25)\ny1=df_train[df_train['target']==0]['keyword'].value_counts()[:30].index\naxes[1].set_yticklabels(y1, fontsize=20)\n","1413fb15":"df_train.loc[2,'text']","d70cfd8c":"word_length=[]\nfor i in df_train['text']:\n    word_length.append(len(i.split()))\n    \nplt.figure(figsize=(12,8))\nsns.kdeplot(word_length,hue=df_train['target'],fill=True)","02970ce2":"lemma=WordNetLemmatizer()\ncv=CountVectorizer(ngram_range=(1,2),max_features=10000)\n\n# Porter Stemmer or Snowball Stemmer can also be used instead of Wordnet lemmatizer and tfidf or hashing vectorizer can also be used in place of count vectorizer","d5462b62":"\ndef most_freqent_words(data=df_train,target=1,n=15,n_grams=1):\n    corpus=[]\n    \n    for i in df_train[df_train['target']==target]['text']:\n        review=re.sub(r'https?:\/\/\\S+|www\\.\\S+','',i)\n        review=re.sub(r'<.*?>','',review)\n        review=re.sub('[^a-zA-Z]',' ',review)\n        review=review.lower()\n        review=review.split()\n        review=[lemma.lemmatize(word) for word in review if not word in stopwords.words('english')]\n        review=' '.join(review)\n        corpus.append(review)\n        \n    vec = CountVectorizer(ngram_range=(n_grams, n_grams)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n\n    top_words=[]\n    top_freq=[]\n    for word, freq in words_freq[:n]:\n        top_words.append(word)\n        top_freq.append(freq)\n        \n    return top_words,top_freq","1bb12099":"words1,freq1=most_freqent_words(target=1,n=15,n_grams=1)\nwords0,freq0=most_freqent_words(target=0,n=15,n_grams=1)\n\nfig, axes = plt.subplots(2, 1, figsize=(20, 30))\nfig.suptitle('Most Frequent Words',size=15)\n\nsns.barplot(ax=axes[0],y=words1,x=freq1,palette='cool')\naxes[0].set_title('Disaster')\naxes[0].set_yticklabels(words1,fontsize=12)\n\nsns.barplot(ax=axes[1],y=words0,x=freq0,palette='cool')\naxes[1].set_title('NonDisaster')\naxes[1].set_yticklabels(words0,fontsize=12)\n\n","df93cd85":"words1,freq1=most_freqent_words(target=1,n=15,n_grams=2)\nwords0,freq0=most_freqent_words(target=0,n=15,n_grams=2)\n\nfig, axes = plt.subplots(2, 1, figsize=(20, 30))\nfig.suptitle('Most Frequent Bigrams',size=30)\n\nsns.barplot(ax=axes[0],y=words1,x=freq1,palette='cool')\naxes[0].set_title('Disaster',size=20)\naxes[0].set_yticklabels(words1,fontsize=15)\n\nsns.barplot(ax=axes[1],y=words0,x=freq0,palette='cool')\naxes[1].set_title('NonDisaster',size=20)\naxes[1].set_yticklabels(words0,fontsize=15)\n\n","d3e901b0":"words1,freq1=most_freqent_words(target=1,n=15,n_grams=3)\nwords0,freq0=most_freqent_words(target=0,n=15,n_grams=3)\n\nfig, axes = plt.subplots(2, 1, figsize=(20, 30))\nfig.suptitle('Most Frequent Trigrams',size=30)\n\nsns.barplot(ax=axes[0],y=words1,x=freq1,palette='cool')\naxes[0].set_title('Disaster',size=20)\naxes[0].set_yticklabels(words1,fontsize=15)\n\nsns.barplot(ax=axes[1],y=words0,x=freq0,palette='cool')\naxes[1].set_title('NonDisaster',size=20)\naxes[1].set_yticklabels(words0,fontsize=15)\n\n","fcb6c475":"corpus=[]\nfor i in range(len(df_train)):\n    review=re.sub(r'https?:\/\/\\S+|www\\.\\S+','',df_train.loc[i,'text'])\n    review=re.sub(r'<.*?>','',review)\n    review=re.sub('[^a-zA-Z]',' ',review)\n    review=review.lower()\n    review=review.split()\n    review=[lemma.lemmatize(word) for word in review if not word in stopwords.words('english')]\n    review=' '.join(review)\n    corpus.append(review)","0762640a":"print(corpus[0])\ndf_train.loc[0,'text']","bce6fab6":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ncomment_words = ''\nstopwords = set(STOPWORDS)\n \nfor val in df_train.text:\n    val = str(val)\n    tokens = val.split()\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n    comment_words += \" \".join(tokens)+\" \"\n\nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","3042bb55":"x=cv.fit_transform(corpus).toarray()\ny=df_train['target']","de0cab62":"x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=2020,test_size=0.2)","394bdff5":"model=LogisticRegression()\nmodel.fit(x_train,y_train)","f8c8fd59":"cross_val_score(model,x,y,cv=5,scoring='f1')","b21f92f6":"print('Test Accuracy:',accuracy_score(model.predict(x_test),y_test))\nprint('Test Confusion Matrix:',confusion_matrix(model.predict(x_test),y_test))\nplot_confusion_matrix(model, x_test, y_test,cmap='Blues')  \nf1_score(y_test,model.predict(x_test))","a6ef5e07":"print('Train Accuracy:',accuracy_score(model.predict(x_train),y_train))\nprint('Train Confusion Matrix:',confusion_matrix(model.predict(x_train),y_train))\nplot_confusion_matrix(model, x_train, y_train,cmap='Blues')  \nf1_score(y_train,model.predict(x_train))","c649c839":"xgb=XGBClassifier(max_depth=3,learning_rate=0.2)","5c9deb7f":"xgb.fit(x_train,y_train)","fe451280":"cross_val_score(xgb,x,y,cv=5,scoring='f1')","c90b0510":"print('Test Accuracy:',accuracy_score(xgb.predict(x_test),y_test))\nprint('Test Confusion Matrix: \\n',confusion_matrix(xgb.predict(x_test),y_test))\nplot_confusion_matrix(xgb, x_train, y_train,cmap='Blues')  \nprint('Train Accuracy:',accuracy_score(xgb.predict(x_train),y_train))\nprint('Train Confusion Matrix:\\n',confusion_matrix(xgb.predict(x_train),y_train))\nplot_confusion_matrix(xgb, x_test, y_test,cmap='Blues')  ","11863ae1":"f1_score(xgb.predict(x_test),y_test)","64fdd66d":"model=tf.keras.Sequential([tf.keras.layers.Dense(32,input_shape=[10000],activation='relu'),\n                         tf.keras.layers.Dense(32,activation='relu'),\n                         tf.keras.layers.Dropout(0.3),\n                         tf.keras.layers.Dense(64,activation='relu'),\n                           tf.keras.layers.Dropout(0.3),\n                           tf.keras.layers.Dense(64,activation='relu'),\n                           tf.keras.layers.Dropout(0.3),\n                           tf.keras.layers.Dense(64,activation='relu'),\n                           tf.keras.layers.Dropout(0.3),\n                           tf.keras.layers.Dense(32,activation='relu'),\n                           tf.keras.layers.Dropout(0.3),\n                         tf.keras.layers.Dense(1,activation='sigmoid')])\nopt=Adam(learning_rate=0.001)\nmodel.compile(loss='binary_crossentropy',metrics='accuracy',optimizer=opt)","51fe370f":"model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=20,batch_size=100)","9e939e9e":"f1_score(model.predict_classes(x_test),y_test)","38442f6f":"voc_size=10000","54dc02ed":"onehot=[one_hot(words,voc_size) for words in corpus]","09f6c343":"lengths=[len(i) for i in onehot]\nmax(lengths)","139c23e8":"sent_len=30\nembedded_docs=pad_sequences(onehot,padding='pre',maxlen=sent_len)\nembedded_docs","4b814bc3":"embedding_vec_features=60\nmodel=tf.keras.Sequential([tf.keras.layers.Embedding(voc_size,embedding_vec_features,input_length=sent_len),\n                         tf.keras.layers.LSTM(100),\n                         tf.keras.layers.Dropout(0.3),\n                         tf.keras.layers.Dense(32,activation='relu'),\n                         tf.keras.layers.Dropout(0.3),\n                         tf.keras.layers.Dense(64,activation='relu'),\n                         tf.keras.layers.Dropout(0.3),\n                         tf.keras.layers.Dense(1,activation='sigmoid')])\nopt=Adam(learning_rate=0.0005)\nmodel.compile(optimizer=opt,loss='binary_crossentropy',metrics='accuracy')","8d529af7":"x_final=np.array(embedded_docs[:len(df_train)])\ny_final=np.array(y)","ab54a5fe":"x_train,x_test,y_train,y_test=train_test_split(x_final,y_final,random_state=2021,test_size=0.2)","2c951dc1":"model.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size=32,epochs=10)","6a361cb1":"model.summary()","17499e50":"f1_score(y_test,model.predict_classes(x_test))","56aa5437":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","528c67d6":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","e02e5a37":"module_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","1b77bb85":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","ee5b8050":"train_input = bert_encode(df_train.text.values, tokenizer,max_len=160)\ntest_input = bert_encode(df_test.text.values, tokenizer,max_len=160)\ntrain_labels = df_train.target.values","d93efc0e":"model = build_model(bert_layer,max_len=160)\nmodel.summary()","9be9e194":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=5,\n    callbacks=[checkpoint],\n    batch_size=16\n)","334ffae1":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","e208c3df":"submission=pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","fb56d998":"## Basic models","7b29ee7d":"##### Whenever we talk about the sequences it is must that we know about LSTMs,GRUs and RNNs. There is no point in jumping directly to the implementation of transformers without having the knowledge about the evolution of transformer models and for that knowing about the RNNs and how their drawbacks led to creation of GRUs and LSTMs whose drawbacks further led to the attention models and finally the transformers.","4d8e3bd6":"##### There are many transformer models such as BERT,ALBERT,DistilBERT,ROBERTa,GPT3,just to name a few. There's no specific reason as to why I chose BERT over others. This blog might be helpful in understanding [BERT](https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270). You can try out the other models from [tensorflow hub](https:\/\/tfhub.dev\/google\/collections\/transformer_encoders_text\/1) There are many notebooks on implementation of Bert and I found this one easy and simple. Total credit for transformer model goes to [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub) ","0937cd6c":"## Top Location based on Tweets(Disaster or Non-Disaster)\n","e1e62e14":"##### Again this notebook is created to just give a walkthrough of implementation of different models. The hyperparameter tuning for this model can be done using Optuna. You can use the above given link for refernce and try out optuna for yourself.","7aec335f":"## EDA","b80230fa":"##### This book is aimed at providing the clarity on the implementation of different models and so hyperparameter tuning is not done for any of the models.","0be139e3":"# Disaster Tweet Prediction\n","b9393757":"##### Transformers are changing the whole course of NLP achieving SOTA(state of the Art) results. Attention models are a good place to start when learning about transformers. The birth of attention models manoeuvred the NLP field in a different direction and for the better. These attention models have a great limitation when working with long sequences, their ability to retain information from the first elements was lost when new elements were incorporated into the sequence. Hence this led to the evolution of transformers. For understanding transformers it is great to start with the paper \"Attention is all you need\". Link will be given below. But honestly I did not understand the concept of transformers after reading the paper even after several attempts. Then I came across a wonderful blog by Jay Alammar that explained clearly in a step by step manner the working of the transformer. I highly recommend this blog to anyone who is interested in understanding the working of transformers.\n\n##### Attention models paper:[Attention Model](https:\/\/arxiv.org\/pdf\/1409.0473.pdf)\n##### Refer to this video for better understanding of attention models:[https:\/\/www.youtube.com\/watch?v=fdhojC37_Co&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm&index=28](https:\/\/www.youtube.com\/watch?v=fdhojC37_Co&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm&index=28)\n##### The link for transformer paper: [Attention is all you need](https:\/\/papers.nips.cc\/paper\/2017\/file\/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n##### Jay Alammar's blog: [The Illustrated Transformer](https:\/\/jalammar.github.io\/illustrated-transformer\/)\n","02aae9c8":"Hyperparameter optimization of optuna can be done using Optuna. If interested you can refer this:[https:\/\/medium.com\/optuna\/using-optuna-to-optimize-tensorflow-hyperparameters-57b6d4d316a2](https:\/\/medium.com\/optuna\/using-optuna-to-optimize-tensorflow-hyperparameters-57b6d4d316a2)","681a331d":"## Most Frequent Unigrams","56303768":"##### missingno is a library that is used to visualize the missing values. The missing values can be visualized using barplot,matrix plot or heatmap. For reference you can visit this github page:\n[https:\/\/github.com\/ResidentMario\/missingno](https:\/\/github.com\/ResidentMario\/missingno)","8c14f758":"## LSTM","9b512639":"##### Better results for xgboost and logistic regression could be obtained by hyperparameter tuning using GridSearch or Random Search. Grid Search and Random Search will take time for hyperparameter optimization though.","76d3be90":"## Submission","ffc9f0fc":"![](https:\/\/miro.medium.com\/max\/1838\/1*ASL5AkBNLyIPPIf5pJp6Lg.jpeg)","4dd6d300":"## Transformer Model","02e616dc":"## Deep Learning Model","293fbe24":"                 For a beginner,Starting out in NLP is quite a challenge,atleast for me it was. This is such an emerging field that the developments are happening at a very rapid rate. So coping up in NLP would feel a bit difficult. I had to go through many resources and notebooks on Kaggle and GitHub in order to get a better understanding of the concepts and I did not have a clear roadmap as to how to proceed.This book is created in such a way that anyone starting out in the NLP would atleast get an idea of how to start and proceed. I have tried to touch upon the topics I felt should be known and the link to the resources for the topics are provided in the respective sections.","08f06810":"## Importing the Libraries and the dataset","011644c2":"## Most Frequent Trigrams","143bfc77":"## Most Frequent Bigrams"}}