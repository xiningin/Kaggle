{"cell_type":{"61ab7784":"code","8aa960d3":"code","81562a1e":"code","90a9bb0a":"code","92e74db0":"code","f65e2537":"code","e8cc8473":"code","426b1279":"code","32a66cbf":"code","7edeac32":"code","718809ad":"code","2d34e3b0":"code","f7cb7a74":"code","be7847bd":"code","fb6a6be9":"code","a25a58a1":"code","4f60595e":"code","ca2e15e7":"markdown","3534a0fd":"markdown","f5d9aff6":"markdown","db142c2e":"markdown","677a4159":"markdown","15123850":"markdown"},"source":{"61ab7784":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8aa960d3":"df = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\n\ndf.head()","81562a1e":"def adjustedR2(r2,n,k):\n    return r2-(k-1)\/(n-k)*(1-r2)","90a9bb0a":"train_data ,test_data = train_test_split(df, train_size=0.8,random_state = 3 )\ntrain_data.head()","92e74db0":"lr = linear_model.LinearRegression()\nX_train = np.array(train_data['sqft_living']).reshape(-1,1)\ny_train = np.array(train_data['price']).reshape(-1,1)\nlr.fit(X_train, y_train)\ncoefficient = lr.coef_\nintercept = lr.intercept_\nprint(\"coefficient: \",lr.coef_)\nprint(\"Intercept: \",lr.intercept_)\n","f65e2537":"X_test = np.array(test_data['sqft_living']).reshape(-1,1)\ny_test = np.array(test_data['price']).reshape(-1,1)\npred = lr.predict(X_test)\n","e8cc8473":"RMSE = np.sqrt(metrics.mean_squared_error(y_test,pred))\nprint(\"Root mean square error: \", RMSE)","426b1279":"R2_training = lr.score(X_train,y_train)\nR2_test = lr.score(X_test, y_test)\nprint(\"R square  for Training set: \",R2_training)\nprint(\"R square for Test set: \",R2_test)","32a66cbf":"# find SSR(Sum of squared regression)\n# sum of sqaures of variation of predicted line from the mean of Y axis(y_bar)\nY_hat = pred\nY_bar = y_test.mean()\n\ndef SSR(Y_hat, Y_bar):\n    return np.square(Y_hat - Y_bar).sum()\n\n# finding SSE(Sum of squared error)\ndef SSE(Yi,Y_hat):\n    return np.square(Yi - Y_hat).sum()","7edeac32":"Rsquared = SSR(Y_hat, Y_bar)\/ (SSR(Y_hat, Y_bar) + SSE(y_test.reshape(pred.shape),Y_hat))\nRsquared\n","718809ad":"sns.set(style='white', font_scale=1)\nplt.figure(figsize =(10,8))\nplt.scatter(X_test,y_test,color='green',label=\"prediction using linear regression\")\nplt.plot(X_test,lr.predict(X_test),color='red', label=\"predicted regression line\")\nplt.xlabel(\"Living Space (sqft)\", fontsize=15)\nplt.ylabel(\"Price ($)\", fontsize=15)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.legend()","2d34e3b0":"features = ['price','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront',\n            'view','condition','grade','sqft_above','sqft_basement','yr_built','yr_renovated',\n            'zipcode','lat','long','sqft_living15','sqft_lot15']\nmask = np.zeros_like(df[features].corr(),dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True","f7cb7a74":"f, ax = plt.subplots(figsize=(16,12))\nplt.title (\"Pearsons correlation matirx\",fontsize=25)\nsns.heatmap(df[features].corr(), linewidth=0.25, vmax=0.7, square= True, cmap=\"BuGn\",linecolor='w',annot=True,annot_kws={\"size\":8},mask=mask,cbar_kws={\"shrink\": .9});","be7847bd":"df_dm = df.copy()\ndf_dm.head()","fb6a6be9":"# just take the year from the date columns\ndf_dm['sales_yr'] = df_dm['date'].astype(str).str[:4]\n\n#find add of the building when house where sold = sales_year - built_year\ndf_dm['age'] = df_dm['sales_yr'].astype(int) - df_dm['yr_built']\n\n# find age of the renovation when house were sold\ndf_dm['age_renov'] = 0\ndf_dm['age_renov'] = df_dm['sales_yr'][df_dm['yr_renovated']!=0].astype(int)-df_dm['yr_renovated'][df_dm['yr_renovated']!=0]\ndf_dm['age_renov'][df_dm['age_renov'].isnull()]=0","a25a58a1":"# partition the 'age' into bins\nbins = [-2, 0,5, 10, 25, 50,75, 100, 100000]\nlabels = ['<1', '1-5','6-10','11-25', '26-50','51-75','76-100','>100']\ndf_dm['age_binned'] = pd.cut(df_dm['age'], bins=bins, labels=labels)\n\n# partition 'age_renov' in to bins\nbins = [-2, 0,5, 10, 25, 50, 75,100,100000]\nlabels = ['<1', '1-5','6-10','11-25', '26-50','51-75','76-100','>100']\ndf_dm['age_renov_binned'] = pd.cut(df_dm['age_renov'], bins=bins, labels=labels)\n\n#histogram for binned columns\n\nf, axes = plt.subplots(1,2,figsize = (15,5))\np1 = sns.countplot(df_dm['age_binned'], ax=axes[0])\np2 = sns.countplot(df_dm['age_renov_binned'],ax=axes[1])\n\naxes[0].set(xlabel='Age')\naxes[0].yaxis.tick_left()\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].set(xlabel='Renovation Age');\n\n# transform the factor values to be able to use in the model\ndf_dm = pd.get_dummies(df_dm, columns=['age_binned','age_renov_binned'])\n","4f60595e":"train_data_dm, test_data_dm = train_test_split(df_dm, train_size = 0.8, random_state=3)\nfeatures = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view','grade','age_binned_<1', \n            'age_binned_1-5', 'age_binned_6-10','age_binned_11-25', 'age_binned_26-50',\n            'age_binned_51-75','age_binned_76-100', 'age_binned_>100','age_renov_binned_<1',\n            'age_renov_binned_1-5', 'age_renov_binned_6-10', 'age_renov_binned_11-25',\n            'age_renov_binned_26-50', 'age_renov_binned_51-75', 'age_renov_binned_76-100',\n            'age_renov_binned_>100','zipcode','lat','long','sqft_living15','sqft_lot15']\ncomplex_model1 = linear_model.LinearRegression()\ncomplex_model1.fit(train_data_dm[features],train_data_dm['price'])\nprint('Intercept:{}' .format(complex_model1.intercept_))\nprint('coefficient:{} '.format(complex_model1.coef_))\n\npred = complex_model1.predict(test_data_dm[features])\nRoot_mean_square_error = np.sqrt(metrics.mean_squared_error(test_data_dm['price'], pred))\nR2_square_train = complex_model1.score(train_data_dm[features], train_data_dm['price']) # R2 square for training model\nR2_square_test = complex_model1.score(test_data_dm[features], test_data_dm['price']) # R2 square for test model\nprint(\"R2_square for train data:\",R2_square_train)\nprint(\"R2_square for test data:\",R2_square_test)\nR2_squareAdjusted_train = adjustedR2(complex_model1.score(train_data_dm[features], train_data_dm['price']),train_data_dm.shape[0],len(features)) # R2 squareAdjusted for training model\nprint(\"Rsquare adjusted value for training data:\", R2_squareAdjusted_train)","ca2e15e7":"# Defining a Function to Calculate the Adjusted  R2","3534a0fd":"# Simple Linear Regression","f5d9aff6":"# Multiple regression -1","db142c2e":"# Finding R-square using therotical method","677a4159":"The R-squared increases when the number of features increase. Because of this, sometimes a more robust evaluator is preferred to compare the performance between different models. This evaluater is called adjusted R-squared and it only increases, if the addition of the variable reduces the MSE. The definition of the adjusted  R2  is:\n\n\n\n","15123850":"Thus adding addition independent variables will increase adjusted Rsquare value.\n"}}