{"cell_type":{"bba328dd":"code","60bc0b45":"code","7d9066b2":"code","aa3616fe":"code","b70281ff":"code","790d7328":"code","85a41760":"code","6c4f069c":"code","21b28170":"code","6594bc73":"code","9a89d0fd":"code","f4c302ff":"code","3c4679d9":"code","201dc8b7":"code","f6c8117b":"code","32204f0e":"code","99e11781":"code","2dfd8946":"code","f416f90d":"code","a6b60b6e":"code","dde852c0":"code","f5f09a94":"code","57c63ff6":"code","5124f6a3":"code","8f1fb204":"code","66e8cd5f":"code","3cb3a0c0":"code","c44fc865":"code","49dd9f4a":"code","f3de775f":"code","c7661116":"code","2971797e":"code","acfdf354":"code","59e4d63d":"code","9b6ed2f8":"code","55a812d4":"code","36e3cd53":"markdown","6e3f0a1f":"markdown","664d625a":"markdown"},"source":{"bba328dd":"# Define libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Read the data then convert to dataframe\nX = pd.read_csv('..\/input\/train.csv', index_col='Id') \n\n# Working on the training dataset\n# We need to remove rows with null values to make sure \n# that all training targets are present\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we will drop the columns with null values \n# on the training features\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()] \nX.drop(cols_with_missing, axis=1, inplace=True)\n\n# Split the training data set to training and test data\n# ratio would be training = 80% and test = 20%\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n# We need a function that we could reuse \n# each time we need to return the MAE\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","60bc0b45":"# Checking the first 5 rows of features for both training and validation\nX_train.head()","7d9066b2":"X_valid.head()","aa3616fe":"X_train.shape","b70281ff":"X_valid.shape","790d7328":"# Will drop all columns with \"categorical\" type of data\ndrop_X_train = X_train.select_dtypes(exclude=\"object\")\ndrop_X_valid = X_valid.select_dtypes(exclude=\"object\")","85a41760":"drop_X_train.shape","6c4f069c":"drop_X_valid.shape","21b28170":"print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","6594bc73":"X_train.shape","9a89d0fd":"X_valid.shape","f4c302ff":"# Basically when we say label encoding, just imagine a \n# table in a database where valid values are listed on a list (consider enum)\n# we will assign a unique digit for each\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_X_train_tmp = X_train.copy()\nlabel_X_valid_tmp = X_valid.copy()\nencoder = LabelEncoder()\n\n# We need to make sure that all the list of accepted values\n# are both the same on both train and valid data or else we will encounter an error\n# therefore we need to know which features(labels) are unique on train data \n# and on valid data then we remove those features from the equation \n\nobject_cols        = [col for col in label_X_train_tmp.columns if label_X_train_tmp[col].dtype == \"object\"]\naccepted_cols      = [col for col in object_cols if set(label_X_train_tmp[col]) == set(label_X_valid_tmp[col])]\nto_be_removed_cols = list(set(object_cols)-set(accepted_cols))\nprint(object_cols)\nprint(accepted_cols)\nprint(to_be_removed_cols)","3c4679d9":"label_X_train_tmp.shape","201dc8b7":"label_X_valid_tmp.shape","f6c8117b":"# Drop all to be removed columns\nlabel_X_train = label_X_train_tmp.drop(to_be_removed_cols, axis=1)\nlabel_X_valid = label_X_valid_tmp.drop(to_be_removed_cols, axis=1)","32204f0e":"label_X_train.shape","99e11781":"label_X_valid.shape","2dfd8946":"# Proceed with label encoding for each, then measure MAE\n# Each non numerical feature will be mapped into a unique digit\nfor col in object_cols:\n    label_X_train[col] = encoder.fit_transform(X_train[col])\n    label_X_valid[col] = encoder.fit_transform(X_valid[col])","f416f90d":"label_X_train.shape","a6b60b6e":"label_X_valid.shape","dde852c0":"label_X_train.head()","f5f09a94":"label_X_valid.head()","57c63ff6":"print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))","5124f6a3":"# Just a short note, keep in mind that when we say \"cardinality\"\n# it is a simple as the total number of unique accepted values \n# a feature has. For example, if field1 values could be yes or no, \n# then it has a cardinality of 2\n\n# Also do some research on lambda function and map function, it is really really useful","8f1fb204":"# Cardinality IS an issue since features with very huge number of cardinality \n# could bloat your data, thus making it harder to be processed. \n# I will only one-hot encode features less than 10 cardinality\n\noh_X_train_tmp = X_train.copy()\noh_X_valid_tmp = X_valid.copy()\n\n# Get all object columns \nobject_cols = [col for col in oh_X_train_tmp.columns if oh_X_train_tmp[col].dtype == \"object\"]\n\n# Get all features with less than 10 cardinality so that we know which features are we going to drop\nlow_cardinality_cols = [col for col in object_cols if oh_X_train_tmp[col].nunique() < 10]\nto_be_removed_cols   = list(set(object_cols)-set(low_cardinality_cols))\nprint(to_be_removed_cols)","66e8cd5f":"# We now know which features have more than 10 cardinality\n# will drop it from the dataset\noh_X_train = oh_X_train_tmp.drop(to_be_removed_cols, axis=1)\noh_X_valid = oh_X_valid_tmp.drop(to_be_removed_cols, axis=1)","3cb3a0c0":"oh_X_train.shape","c44fc865":"oh_X_valid.shape","49dd9f4a":"# Will now do one-hot encoding then check MAE score\nfrom sklearn.preprocessing import OneHotEncoder\noh_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n# one-hot encode each object cols, on both training data and validation data\n# need to update object_cols since we removed 3 features earlier\nobject_cols   = [col for col in oh_X_train.columns if oh_X_train[col].dtype == \"object\"]\noh_cols_train = pd.DataFrame(oh_encoder.fit_transform(oh_X_train[object_cols]))\noh_cols_valid = pd.DataFrame(oh_encoder.transform(oh_X_valid[object_cols]))","f3de775f":"oh_cols_train.head()","c7661116":"oh_cols_valid.head()","2971797e":"# Putting back the index\noh_cols_train.index = X_train.index\noh_cols_valid.index = X_valid.index","acfdf354":"# Now, as you see on top, we have one-hot encoded features with less than 10 cardinality\n# therefore, we can now remove all object columns from the data set then replace them\n# with the one-hot encoded version\noh_X_train_tmp = oh_X_train.drop(object_cols, axis=1) \noh_X_valid_tmp = oh_X_valid.drop(object_cols, axis=1)\noh_X_train = pd.concat([oh_X_train_tmp, oh_cols_train], axis=1)\noh_X_valid = pd.concat([oh_X_valid_tmp, oh_cols_valid], axis=1)","59e4d63d":"oh_X_train.shape","9b6ed2f8":"oh_X_valid.shape","55a812d4":"# measure MAE for one-hot encoding\nprint(score_dataset(oh_X_train, oh_X_valid, y_train, y_valid))","36e3cd53":"# MAE if we do one-hot encoding","6e3f0a1f":"# MAE if we drop columns with categorical data","664d625a":"# MAE if we do label encoding"}}