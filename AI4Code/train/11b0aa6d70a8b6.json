{"cell_type":{"6a4a98fe":"code","dafe9158":"code","dc6f53b0":"code","806c76ed":"code","f732efe5":"code","5fddd941":"code","7dab33f2":"code","22df6ae4":"code","2f27ad10":"code","29808989":"code","00251f66":"code","5521526f":"code","c550c7e3":"code","3167b37b":"code","4bd35469":"code","840a7594":"code","dd0d47db":"code","288f41e9":"code","a5f79ef3":"code","0c0443f0":"code","2a0a96c9":"code","0e557a25":"code","3978a35f":"code","2ecc23ec":"code","d087d36c":"code","abfdf2ab":"code","2efa5f9c":"code","375202aa":"code","7029a653":"code","a63e6e04":"code","9c742be7":"code","e5b15469":"code","5dd64f5b":"code","6b7024db":"code","c2c6d657":"code","44dc26d6":"code","bcb40639":"code","39979cfb":"code","87d46a71":"markdown","96190255":"markdown","bdfb21c3":"markdown","28c9c3d1":"markdown","7de8f6f6":"markdown","3b9940ab":"markdown","dd4626a8":"markdown","8f6fbc89":"markdown","83ad2f21":"markdown","06d78356":"markdown","0a8ae6f8":"markdown","d6941c12":"markdown"},"source":{"6a4a98fe":"#%matplotlib inline\n\n# for seaborn issue:\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import linear_model\n\nprint(os.listdir(\"..\/input\"))\n\n","dafe9158":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","dc6f53b0":"train.head(5)","806c76ed":"print(train.columns)\nprint(train.shape)","f732efe5":"print(train.info())","5fddd941":"# Null values?\nnulls_data = train.isnull().sum().sum()\nprint(\"There are {} null data on the dataset\".format(nulls_data))","7dab33f2":"test.head(5)","22df6ae4":"print(test.columns)\nprint(test.shape)\nprint('test info:')\nprint(test.info())\n# Null values?\nnulls_data = test.isnull().sum().sum()\n\nprint(\"There are {} null data on the dataset\".format(nulls_data))","2f27ad10":"print(train.target.describe())","29808989":"train.target.plot.hist()","00251f66":"target_log = np.log(train.target)\ntarget_subx = 1\/train.target\ntarget_square = np.square(train.target)\nprint(target_log.skew())\n\nprint(target_square.skew())\ntarget_log.plot.hist()\ntrain.target = target_log","5521526f":"columns = train.columns\nprint(len(train[train[columns[2]] == 0])\/len(train[columns[2]]))\nprint(len(train[columns[2]]))","c550c7e3":"list_zeros = [len(train[train[d] == 0])\/4459. for d in columns]\n# list_zeros = []\n#for d in columns:\n#    zeros = len(train[train[d] == 0])\n#    total = 4459.\n#    list_zeros.append(zeros\/total)","3167b37b":"sns.distplot(list_zeros, bins=100)","4bd35469":"# df = df.loc[:, df.var() == 0.0]\n# obj_df = train.select_dtypes(include=['object'])\nobj_df = train.iloc[:, :2]\n# num_df = train.select_dtypes(exclude=['object'])\nnum_df = train.iloc[:,2:]\nvar = num_df.var()\nl_keys_notzeros = []\nl_values_notzeros = []\nfor k, v in var.items():\n    if v != 0.0:\n        l_keys_notzeros.append(k)\n        l_values_notzeros.append(v)\n# foo = num_df.loc[:, num_df.var() != 0.0]\nfoo = num_df[l_keys_notzeros]\nnew_train_without_zeros = pd.concat([obj_df, foo], axis=1) # new data without zero variance\nprint(new_train_without_zeros.shape)","840a7594":"obj_df = test.iloc[:, :1]\nnum_df = test.iloc[:,1:]\nfoo = num_df[l_keys_notzeros]\nnew_test_without_zeros = pd.concat([obj_df, foo], axis=1) # new data without zero variance\nprint(new_test_without_zeros.shape)","dd0d47db":"del obj_df\ndel num_df\ndel foo","288f41e9":"# Remove duplicated columns\ncol_to_remove = list()\ncol_scanned = list()\ndup_list = dict()\n\ncols = new_train_without_zeros.columns\n\nfor i in range(len(cols) - 1):\n    v = new_train_without_zeros[cols[i]].values\n    dup_cols = list()\n    for j in range(i+1, len(cols)):\n        if np.array_equal(v, new_train_without_zeros[cols[j]].values):\n            col_to_remove.append(cols[j])\n            if cols[j] not in col_scanned:\n                dup_cols.append(cols[j]) \n                col_scanned.append(cols[j])\n                dup_list[cols[i]] = dup_cols\nprint(col_to_remove)    ","a5f79ef3":"cols = [c for c in cols if c not in col_to_remove]\ncols_test = [c for c in cols if c != 'target']","0c0443f0":"new_train = new_train_without_zeros[cols]\nnew_test = new_test_without_zeros[cols_test]\n\nprint(new_train.shape)\nprint(new_test.shape)","2a0a96c9":"del new_train_without_zeros\ndel new_test_without_zeros\ndel train\ndel test","0e557a25":"del col_to_remove\ndel col_scanned\ndel dup_list\ndel cols","3978a35f":"id_target_train = new_train.iloc[:,:2]\nnew_train = new_train.iloc[:,2:].values\n","2ecc23ec":"id_test = new_test.iloc[:,:1]\nnew_test = new_test.iloc[:, 1:].values","d087d36c":"print('Shape of train: ',new_train.shape)\nprint('Shape of test: ', new_test.shape)\n#print('Shape of target: ',log_target.shape)\n#print('Shape of test: ',test.shape)","abfdf2ab":"def transform (dataframe):\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(dataframe)\n    return pd.DataFrame(scaled_data)","2efa5f9c":"new_train = transform(new_train)","375202aa":"new_test = transform(new_test)","7029a653":"# num_data = ttrain.select_dtypes(exclude='object')\nnum_data = new_train\npca = PCA(copy=True, n_components=2000, whiten=False)\nnew = pca.fit(num_data).transform(num_data)\nprint(pca.explained_variance_ratio_) \nlen_pca = len(pca.explained_variance_ratio_)\nprint(\"The first {} PCA explain {}\".format(len_pca, pca.explained_variance_ratio_.sum()*100))","a63e6e04":"# var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\nvar = pca.explained_variance_ratio_.cumsum()\nplt.ylabel('% Variance Explained')\nplt.xlabel('# of Features')\nplt.title('PCA Analysis')\nplt.style.context('seaborn-whitegrid')\n\nplt.plot(var)\nplt.show()\n","9c742be7":"pca_train = pd.DataFrame(data=new, columns=['pca{}'.format(i) for i in range(2000)])\npca_train = pd.concat([id_target_train[['ID','target']], pca_train], axis = 1)\nprint(pca_train.head(1))","e5b15469":"num_data = new_test\nnew = pca.transform(num_data)\npca_test = pd.DataFrame(data=new, columns=['pca{}'.format(i) for i in range(2000)])\npca_test = pd.concat([id_test[['ID']], pca_test], axis=1)\nprint(pca_test.head(1))","5dd64f5b":"x_train = pca_train.iloc[:, 2:]\ny_train = pca_train.iloc[:, 1:2]\n\nx_test = pca_test.iloc[:, 1:]","6b7024db":"linear_regression = linear_model.LinearRegression()\nlinear_regression.fit(x_train, y_train)\nprint(linear_regression.coef_)","c2c6d657":"target_test  = linear_regression.predict(x_test)","44dc26d6":"target_test = pd.DataFrame(data=target_test, columns=['target'])\nprint(target_test.head(1))","bcb40639":"to_submit = pd.concat([pca_test['ID'], target_test['target']], axis=1)\nprint(to_submit.head(1))","39979cfb":"to_submit.to_csv('ols.csv', columns=['ID','target'], index=False)","87d46a71":"# TOC TOC\n1. [Importing neccesary modules ](#importing-necessary-modules)\n2. [Loading data](#loading-data)\n3. [Exploring data](#exploring-data)\n    1. [Target](#target)\n    2. [Sparsity of the dataset](#sparsity-of-the-dataset)\n    3. [Zero variance and dupllicate date](#zero-variance-and-duplicate)\n4. [Dimensionality Reduction](#dimensionality-reduction)\n5. [Modeling](#modeling)\n    1. [OLS](#ols)","96190255":"Ok, the columns names are wired. Let's the more information about the data set.","bdfb21c3":"# Importing necesary modules<a name=\"importing-neccesary-modules\"><\/a>\nIn this sections will import necessary modules for the kernel\n","28c9c3d1":"## Target <a name=\"target\"><\/a>","7de8f6f6":"# Exploring data <a name=\"exploring-data\"><\/a>\nI will start exploring the train data file. The only data. \n### Train\nLet's see the train data","3b9940ab":"# Loading data <a name=\"loading-data\"><\/a>\nIn this section we will load the data to use.  We just have 2 file, the train file and the test file (also the sample submission file).\n","dd4626a8":"The target on train dataset is right-skewed","8f6fbc89":"# Dimensionality Reduction <a name=\"dimensionality-reduction\"><\/a>\nI will try to reduce the dataset dimension","83ad2f21":"# Modeling <a name=\"modeling\"><\/a>\nNow, I think that we could start create some models\n\n## OLS <a name=\"ols\"><\/a>\nWe will start with a very simply model: _Ordinary Least Squares_","06d78356":"## Zero variance and duplicate features<a name=\"#zero-variance-and-duplicate\"><\/a>\nI will remove  the zero variance features and duplicated columns.","0a8ae6f8":"## Sparsity of the dataset <a name=\"sparsity-of-the-dataset\"><\/a>\nVarious public kernels show that there are many zeros on the dataset. ","d6941c12":"We see that the dataset have 4993 columns and 4459 entries. We don't have more information about the features, just that 1845 are float, 3147 are int and just 1 is string (the ID).  How we could explore this data if we don't know what relationship could are between columns?\n\n### Test \nLet's see the test data"}}