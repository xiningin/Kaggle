{"cell_type":{"250a9e47":"code","ef4e49ad":"code","2da7123d":"code","025ab9ae":"code","740e9c0e":"code","4673c6ac":"code","384bcb6c":"code","fa6ae4e5":"code","f15533bc":"code","5d490b56":"code","f19e3b1b":"code","cfd90fb4":"code","0ce35e15":"code","fc3e44a7":"code","9391c9c3":"code","8b9028a6":"code","6f3f319f":"code","9ee56ae7":"code","a6435ea5":"code","aa33432e":"code","bdfd0a8a":"code","dd531355":"code","86f02f7f":"code","4be1eb42":"code","26c9c622":"markdown","a0763c88":"markdown","26d31c64":"markdown"},"source":{"250a9e47":"import numpy as np # for numerical operations\nimport pandas as pd # for handling input data\nimport matplotlib.pyplot as plt # for data visualization \ntrain = pd.read_csv('..\/input\/news-regression\/train.csv') # uses pandas library to open a .csv file\ntest = pd.read_csv('..\/input\/news-regression\/test.csv')\nsample = pd.read_csv('..\/input\/news-regression\/sample_submission.csv')","ef4e49ad":"train","2da7123d":"#shuffling\n# Split data \n# frac returns float value * length of data frame values\n# without it the df data wont be seen\ntrain = train.sample(frac=1)\nsize = int(0.7 * len(train))\ntrain_set = train[:size]#5600,56\ntest_set = train[size:]#2400, 56","025ab9ae":"train_set","740e9c0e":"def scaling(x):\n    x=(x-np.mean(x))\/np.std(x) #scaling using z score\n    return x","4673c6ac":"x = train_set.drop('abs_title_sentiment_polarity', axis = 1)\ny = train_set['abs_title_sentiment_polarity']\nx=scaling(x) #scaling using z score\nx=np.array(x)\ny=np.array(y)\nw=np.ones((1,x.shape[1]))\nprint(\"x shape\",x.shape,\"y shape\",y.shape,\"w shape\",w.shape)","384bcb6c":"def hypo(w,x,b):#prediction\n    pred=np.dot(w,x.T)+b\n    return pred","fa6ae4e5":"def error(pred,y,m,lamda,w):#cost #error\n    #er=(1\/(2*m))*np.sum(np.square(pred-y.T))+(lamda\/(2*m))*np.sum(np.square(w))\n    er=(1\/(2*m))*(np.sum(np.square(pred-y.T))+lamda*np.sum(np.square(w)))\n    return er","f15533bc":"def gradient(pred,y,m,alpha,w,b,lamda,x):#gradient decent\n    #dw=(1\/m)*np.dot((pred-y.T),x)+(lamda\/m)*np.sum(w)\n    db=(1\/m)*np.sum(pred-y.T)\n    #w=w-alpha*(dw)\n    w=w*(1-(alpha*lamda)\/m)-(alpha\/m)*np.dot((pred-y.T),x)#accoding to the equation of regularization\n    b=b-alpha*db\n    return b,w","5d490b56":"\"\"\"alpha=0.05#0.03\nlamda=0.1#0.1\"\"\"\n#main\nm=len(x)\nb=1#1\ncost=[]\nalpha=0.03#0.03\nlamda=0.1#0.1\nfor i in range(2000000):\n    pred=hypo(w,x,b) #predicton\n    er=error(pred,y,m,lamda,w)\n    b,w=gradient(pred,y,m,alpha,w,b,lamda,x)\n    if i%100000==0:\n        print(er)\n        cost.append(er)","f19e3b1b":"plt.plot(cost)\nplt.ylabel('cost')\nplt.xlabel('iterations')\nplt.title('Cost reduction over time')\nplt.show()","cfd90fb4":"w","0ce35e15":"b","fc3e44a7":"test_set","9391c9c3":"#validation\nxt = test_set.drop('abs_title_sentiment_polarity', axis = 1)\nyt = test_set['abs_title_sentiment_polarity']\nxt=scaling(xt)\nxt=np.array(xt)\nyt=np.array(yt)\nprint(\"xt\",xt.shape)\nprint(\"yt\",yt.shape)\npred_t=hypo(w,xt,b)\nprint(\"pred_t\",pred_t.shape)\nn=len(xt)\ner=error(pred_t,yt,n,lamda,w)\nprint(er)","8b9028a6":"#compute accuracy\ntestpred=hypo(w,xt,b)\ntrainpred=hypo(w,x,b)\nprint('trainpred',trainpred.shape,'y',y.shape)\nprint('testpred',testpred.shape,'yt',yt.shape)","6f3f319f":"from sklearn.metrics import r2_score# accuracy.score for classification \nprint('train accuracy:',r2_score(y,trainpred.T))\nprint('test accuracy :',r2_score(yt,testpred.T))","9ee56ae7":"#final prediction #target\ntest=scaling(test)\nt=np.array(test)\nprediction=hypo(w,t,b)","a6435ea5":"prediction","aa33432e":"prediction=prediction.reshape(prediction.shape[1],)","bdfd0a8a":"prediction.shape","dd531355":"ids=np.array(sample['Id']) #prepre to submission\nprint(ids)","86f02f7f":"ids.shape","4be1eb42":"e=pd.DataFrame({'Id':ids,'abs_title_sentiment_polarity':prediction})\ne.to_csv('newspaper.CSV',encoding='utf-8',index=False)","26c9c622":"### \uf06c Error: difference between the sample result predicted by the model obtained after learning and the actual sample result.\n* Training error: error that you get when you run the model on the training data.\n* Generalization error: error that you get when you run the model on new samples. Obviously, we \n    prefer a model with a smaller generalization error.\n* Underfitting: occurs when the model or the algorithm does not fit the data well enough.\n* Overfitting: occurs when the training error of the model obtained after learning is small but\n    the generalization error is large (poor generalization capability)","a0763c88":"*****","26d31c64":"![u,o.png](attachment:u,o.png)"}}