{"cell_type":{"27a45fe7":"code","9cdb827d":"code","3e3fe6a7":"code","738b9685":"code","15229abc":"code","065d0dc7":"code","29a01e30":"code","f76cfa67":"code","07b7f388":"code","b5ac8d50":"code","50a43a5b":"code","cf289a10":"code","c7aca374":"code","591aa02c":"code","a46acd1f":"code","27d2b275":"code","13027fe7":"code","e0be2eaf":"code","811da4da":"code","11b50af1":"code","779e64ff":"code","37a593f4":"code","03e091eb":"code","4aba8a78":"code","708d46a3":"code","7247c689":"code","52bc17ed":"code","aa65f976":"code","a34bf5dc":"code","34e02dfd":"code","299e4923":"code","26b7f098":"code","38eb19f7":"code","92c08904":"code","7b90f0ec":"code","933a4625":"code","224136cc":"code","f2861d8c":"code","89fd4e2c":"code","c0af263a":"code","7cfba4bf":"code","9a4e1c21":"code","2f3f0b66":"code","2f4e581d":"code","7b0091b7":"code","24f8350f":"code","952aeca4":"code","c152072c":"code","f3aad27e":"code","80590cdc":"code","eca1be57":"code","3b72bd2d":"code","316c6f2c":"code","e1d71734":"code","4d752c63":"code","d9a19b05":"code","81c25965":"code","18ab0eb9":"code","ccc31dd4":"code","8c8e7422":"code","13d9bd58":"code","90f0142c":"code","fe4ddfc6":"code","0692d392":"code","4add34c9":"code","5edf6b06":"code","0b53134d":"code","78c13765":"code","c5f6f2c9":"code","db3b4ae7":"code","c0869950":"code","dd410f6c":"code","85bb68c3":"code","44e03306":"code","18784fc1":"code","d43af5ff":"code","49a7689c":"code","94c9bf8d":"code","1a134f66":"code","b998b708":"markdown","b18deb3d":"markdown","e439e51c":"markdown","5e347c46":"markdown","b1b205f9":"markdown","7c3b96cf":"markdown","6f0c40c4":"markdown","e44187c3":"markdown","79faa4c2":"markdown","66977494":"markdown","7401065c":"markdown","b205e3e2":"markdown","523bc73a":"markdown","6cdd98ab":"markdown","915a52fc":"markdown","8fb8760f":"markdown","7c1932b7":"markdown","2a6ce968":"markdown","5d4e45b9":"markdown","8b565656":"markdown","e19a7ae8":"markdown","dbd14885":"markdown","0cbae146":"markdown","f450ed30":"markdown","dcb0ea72":"markdown","06fbada7":"markdown","8d8088d5":"markdown","b2fe8443":"markdown","43ef2c56":"markdown","58dc0788":"markdown","d23036a7":"markdown","97b3d24e":"markdown","99b4ae99":"markdown","e8ec9544":"markdown","25dd3ce6":"markdown","db949d69":"markdown","70a02572":"markdown","d37cb378":"markdown","eb0dd886":"markdown","e7bb8556":"markdown","14f39eb0":"markdown","1a2849ea":"markdown","faeaacc8":"markdown","b0f1dde6":"markdown","3e86c260":"markdown","a5d2243d":"markdown","db9e8044":"markdown","23f5c90e":"markdown","40a33d4d":"markdown"},"source":{"27a45fe7":"import numpy as np # linear algebra\nimport os #files, archives, paths manipulation\n# by DEFAULT","9cdb827d":"import json #in order to manipulate the json files\nimport xml #in order to manipulate the xml files\nimport xml.etree.ElementTree as ET #in order to parse the xml files\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport csv\nimport glob #allows us ton find files matching a specified pattern\nimport io #in order to be able to use NLT","3e3fe6a7":"tree = ET.parse('..\/input\/final-xml-data\/final_XML_data\/C00-2123\/Citance_XML\/C02-1050.xml')\nroot = tree.getroot()","738b9685":"tree","15229abc":"datasource = open('..\/input\/final-xml-data\/final_XML_data\/C00-2123\/Citance_XML\/C02-1050.xml')","065d0dc7":"datasource","29a01e30":"with open('..\/input\/final-xml-data\/final_XML_data\/C00-2123\/Citance_XML\/C02-1050.xml', 'r') as f:\n    data = f.read()","f76cfa67":"data","07b7f388":"#Trying out the tree element\n\ntree = ET.ElementTree(file='..\/input\/final-xml-data\/final_XML_data\/C00-2123\/Citance_XML\/C02-1050.xml')\nroot = tree.getroot()\nfor rows in root:\n    print(rows.attrib)","b5ac8d50":"rows.attrib","50a43a5b":"print(root)","cf289a10":"print(root[5][0].text)","c7aca374":"print(root[1].attrib)","591aa02c":"#we need a loop\n\ndef get_xml_files(path):\n    \n    xml_list = []\n    \n    for filename in os.listdir(path):\n        for file in filename:\n            if filename.endswith(\".xml\"):\n                xml_list.append(os.path.join(path, filename))\n    return xml_list","a46acd1f":"myPath='..\/input\/final-xml-data\/final_XML_data\/C00-2123\/Citance_XML'","27d2b275":"myList= get_xml_files (myPath)","13027fe7":"myList","e0be2eaf":"myPath='..\/input\/final-xml-data\/final_XML_data'\nfiles=os.listdir(myPath)\nfiles\n","811da4da":"#def get_xml_files(path):\n    \n   # xml_list = []\n    \n   # for filename in file: \n           # if filename.name is ('Citance_XML'): \n             #   open (file)\n              #  for filename in file:\n                  #      if filename.endswith(\".xml\"):\n                    #        xml_list.append(os.path.join(path, filename))\n   # return xml_list","11b50af1":"#myList= get_xml_files (myPath)\n\n\n#myList","779e64ff":"myPath='..\/input\/final-xml-data\/final_XML_data'","37a593f4":"myList= get_xml_files (myPath)","03e091eb":"myList","4aba8a78":"def get_xml_data(list):\n    data = []\n    for filename in list :\n        root = ET.parse(filename)\n        data = [ text for text in root.findall(path) ]\n    return data\n","708d46a3":"myData= get_xml_data (myList)\n","7247c689":"myData","52bc17ed":"myPath='..\/input\/final-xml-data\/final_XML_data\/C00-2123'\nmyList=get_xml_files (myPath)\nmyList","aa65f976":"def get_xml_files2(path):\n    \n    xml_list = []\n    \n    for filename in os.listdir(path):\n        for file in filename:\n            if filename.endswith(\".xml\"):\n                xml_list.append(os.path.join(path, filename))\n    return xml_list\n\nmyList2=get_xml_files2 (myPath)\nmyList2","a34bf5dc":"for file in glob.iglob(os.path.join('..\/input\/dataset','***\/**\/*.xml')):\n    with open(file) as f:\n        data = etree.parse(f)\n        ","34e02dfd":"#data","299e4923":"#Now, we generate the list using the previous code: \n\nfinalPath='..\/input\/dataset','***\/**\/*.xml'","26b7f098":"for file in glob.iglob(os.path.join('..\/input\/dataset','***\/**\/*.xml')):\n    with open(file) as f:\n        data = etree.parse(f)\n        ","38eb19f7":"jointPath= os.path.join('..\/input\/dataset','***\/**\/*.xml')","92c08904":"jointPath","7b90f0ec":"jPath= glob.iglob(jointPath)","933a4625":"jPath","224136cc":"#files=os.listdir (jointPath)","f2861d8c":"dir = '..\/input\/final-xml-data\/final_XML_data'\n\nfor file in glob.iglob(os.path.join(dir, '*\/*.xml')):\n   with open(file) as f:\n      data = etree.parse(f)","89fd4e2c":"data","c0af263a":"f","7cfba4bf":"print('\\nAll item data:')\nfor elem in root:\n    for subelem in elem:\n        print(subelem.text)","9a4e1c21":"from os import listdir, path \nimport xml.etree.ElementTree as ET\n\nmypath = '..\/input\/final-xml-data\/final_XML_data' \nfiles = [f for f in listdir(mypath) if f.endswith('.xml')]\n\nfor file in files:    \n    print (file)\n    tree = ET.parse(\"..\/input\/final-xml-data\/final_XML_data\"+file)\n    root = tree.getroot()\n ","2f3f0b66":"root","2f4e581d":"tree","7b0091b7":"!pip install nltk\nimport nltk as nlp\n#nltk.download()\nfrom nltk import tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer","24f8350f":"from nltk.corpus import stopwords\n\nprint(stopwords.words('english'))","952aeca4":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n  \nexample_sent = \"\"\"This is a sample sentence,\n                  showing off the stop words filtration.\"\"\"\n  \nstop_words = set(stopwords.words('english')) \n  \nword_tokens = word_tokenize(example_sent) \n  \nfiltered_sentence = [w for w in word_tokens if not w in stop_words] \n  \nfiltered_sentence = [] \n  \nfor w in word_tokens: \n    if w not in stop_words: \n        filtered_sentence.append(w) \n  \nprint(word_tokens) \nprint(filtered_sentence) ","c152072c":"# word_tokenize accepts\n# a string as an input, not a file. \nstop_words = set(stopwords.words('english')) \nfile1 = open(\"..\/input\/final-xml-data\/final_XML_data\/C00-2123\/Citance_XML\/C02-1050.xml\") \n  \n# Use this to read file content as a stream: \n\nline = file1.read()\nwords = line.split() \nline","f3aad27e":"words","80590cdc":"for r in words: \n    if not r in stop_words: \n        appendFile = open('filteredtext.txt','a') \n        appendFile.write(\" \"+r) \n        appendFile.close() \nappendFile;\nprint(appendFile)","eca1be57":"def remove_stopwords(r):\n    sen_new = \" \".join([i for i in r if i not in stop_words])\n    return sen_new","3b72bd2d":"newText=remove_stopwords (words)\nnewText","316c6f2c":"# remove stopwords from the sentences\nclean_sentences=[] #initialization\n\nclean_sentences = [remove_stopwords(r.split()) for r in newText]","e1d71734":"clean_sentences","4d752c63":"# printing original string\n#print(\"The original string is : \" + [clean_sentences])\n\n# initializing punctuations string \npunc = '''!()-[]{};:'\"\\, <>.\/?@#$%^&*_~'''","d9a19b05":"punc","81c25965":"# Removing punctuations in string\n# Using loop + punctuation string\nfor ele in clean_sentences: \n    if ele in punc: \n        clean_sentences = newText.replace(ele, \"\") ","18ab0eb9":"# printing result \nprint(\"The string after punctuation filter : \" + clean_sentences) ","ccc31dd4":"#import re\n\n# printing original string\n# print(\"The original string is : \" + clean_sentences)\n  \n# Removing punctuations in string\n# Using regex\n# res = re.sub(r'[^\\w\\s]', '', clean_sentences)\n  \n# printing result \n# print(\"The string after punctuation filter : \" + res) ","8c8e7422":"error_threshold = 0.4\nerror_threshold","13d9bd58":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n","90f0142c":"documentA = newText #citances\ndocumentB= newTextext2 #references","fe4ddfc6":"bagOfWordsA = documentA.split(' ')\nbagOfWordsB = documentB.split(' ')","0692d392":"uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))","4add34c9":"numOfWordsA = dict.fromkeys(uniqueWords, 0)\nfor word in bagOfWordsA:\n    numOfWordsA[word] += 1\nnumOfWordsB = dict.fromkeys(uniqueWords, 0)\nfor word in bagOfWordsB:\n    numOfWordsB[word] += 1","5edf6b06":"from nltk.corpus import stopwords\nstopwords.words('english')","0b53134d":"def computeTF(wordDict, bagOfWords):\n    tfDict = {}\n    bagOfWordsCount = len(bagOfWords)\n    for word, count in wordDict.items():\n        tfDict[word] = count \/ float(bagOfWordsCount)\n    return tfDict","78c13765":"tfA = computeTF(numOfWordsA, bagOfWordsA)\ntfB = computeTF(numOfWordsB, bagOfWordsB","c5f6f2c9":"def computeIDF(documents):\n    import math\n    N = len(documents)\n    \n    idfDict = dict.fromkeys(documents[0].keys(), 0)\n    for document in documents:\n        for word, val in document.items():\n            if val > 0:\n                idfDict[word] += 1\n    \n    for word, val in idfDict.items():\n        idfDict[word] = math.log(N \/ float(val))\n    return idfDict","db3b4ae7":"idfs = computeIDF([numOfWordsA, numOfWordsB])","c0869950":"def computeTFIDF(tfBagOfWords, idfs):\n    tfidf = {}\n    for word, val in tfBagOfWords.items():\n        tfidf[word] = val * idfs[word]\n    return tfidf","dd410f6c":"tfidfA = computeTFIDF(tfA, idfs)\ntfidfB = computeTFIDF(tfB, idfs)\ndf = pd.DataFrame([tfidfA, tfidfB])\n","85bb68c3":"#vectorizer = TfidfVectorizer()\n#vectors = vectorizer.fit_transform([documentA, documentB])\n#feature_names = vectorizer.get_feature_names()\n#dense = vectors.todense()\n#denselist = dense.tolist()\n#df = pd.DataFrame(denselist, columns=feature_names)","44e03306":"#def clean_string (text)\n#    text=''.join([word for word in text if word not in string.punctuation])\n  #  text=text.lower()\n #   text=''.join([word for word in text.split() if word not in stopwords])\n #   return text\n\n#cleaned=list(map(clean_string, sentences))\n#cleaned\n","18784fc1":"import sklearn\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer= CountVectorizer().fit_transform(newText)\n#remplace clean_sentences with our text\n\nvector=vectorizer.toarray()\nvectors","d43af5ff":"csim= cosine_similarity(vectors)\ncsim","49a7689c":"def cosine_sim_vectors (vec1, vec2):\n    vec1=vec1.reshape (1,-1)\n    vec2=vec2.reshape(1,-1)\n    \n     return cosine_similarity (vec1,vec2)[0][0]","94c9bf8d":"cosine_sim_vectors (vectors[0],vectors[0])","1a134f66":"#def prepare_similarity(vectors):\n    #similarity=cosine_similarity(vectors)\n    #return similarity\n#def get_top_similar(sentence, sentence_list, similarity_matrix, topN):\n    # find the index of sentence in list\n    #index = sentence_list.index(sentence)\n    # get the corresponding row in similarity matrix\n    #similarity_row = np.array(similarity_matrix[index, :])\n    # get the indices of top similar\n    #indices = similarity_row.argsort()[-topN:][::-1]\n    #return [(i,sentence_list[i]) for i in indices]\n#module_url = \"..\/input\/universalsentenceencoderlarge4\" \n# Import the Universal Sentence Encoder's TF Hub module\n#embed = hub.load(module_url)\n\n#titles=all_sources['title'].fillna(\"Unknown\")\n#embed_vectors=embed(titles[:100].values)['outputs'].numpy()\n#sentence_list=titles.values.tolist()\n#sentence=titles.iloc[5]\n#print(\"Find similar research papers for :\")\n#print(sentence)\n\n#similarity_matrix=prepare_similarity(embed_vectors)\n#similar=get_top_similar(sentence,sentence_list,similarity_matrix,6)\n\n\n#for sentence in similar:\n    #print(sentence)\n#print(\"\\n\")\n\n#del embed_vectors,sentence_list,similarity_matrix\n#gc.collect()","b998b708":"We count the ratio of illegal words in each of the 566 sentences cited from the reference papers of the training set and from there we determine the error threshold.\n\nIn the paper they indicate that this was determined as being 0.4, that is, that the sentences comprising 40% or more were illegal words that had to be filtered.\n\n","b18deb3d":"2. Section Similarity\n\nIn Natural Language Processing (NLP), semantic similarity plays an important role and one of the fundamental tasks for many NLP applications and its related areas. Semantic Textual Similarity can be defined by a metric over a set of documents with the idea is to finding the semantic similarity between them. Similarity between the documents is based on the direct and indirect relationships.\n\nThese relationships can be measured and recognized by the presence of semantic relations among them. Identification of STS in short texts was proposed in 2006 in the works reported in. After that, focus was shifted on large documents or individual words.\n\nSemantic similarity also contributes for many semantic web applications like community extraction, ontology generation and entity disambiguation. It is also useful for Twitter search, where it is required the ability to accurately measure semantic relatedness between concepts or entities. In IR one of the main problems is to retrieve a set of documents and retrieving images by captions, which semantically related to a given user query in a web search engine.\n\nText extracted from here.\n\nAmong all the methods of section similarity there is, we are at first going to focus on cosine similarity\n\nWhat is it?\n\nCosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them which means, that we calculate the angle between two vectors first. Careful, we cannot represent some sentence as a vector in n-dimensional space just out of the box. Therefore we have to construct a vector space from all the \u2018sentences\u2019 we want to calculate similarity for. That vector space will have as many dimensions as there are unique words in all sentences combined.\n\nBefore apppling it, we have to\n\nRemove punctuations from a given string\nLowercase the string\nRemove stopwords\nIn our case those opperations have already been done but still interesting to remind it. In case it had bot been done, the code will have looked as shown below:","e439e51c":" > 28\/05\/2021: NOTE!!! Following the model I just saw that the the stopword removal process can be done here, which makes it easier","5e347c46":"STEP 1: LOAD THE DATA\n\nWe import some methods by default that we might need and then we will add more specific ones.","b1b205f9":"3.1 Random Forest Classifier\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean\/average prediction (regression) of the individual trees.\n\nRandom decision forests correct for decision trees' habit of overfitting to their training set:587\u2013588. \n\nRandom forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance.\n\nThe first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.\n\nAn extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered \"Random Forests\" as a trademark (as of 2019, owned by Minitab, Inc.).The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\n\nRandom forests are frequently used as \"blackbox\" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration in packages such as scikit-learn.\n\n3.1.1. Vectorized TF-IDF\n\nTF*IDF, or TFIDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. \n\nIt is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf\u2013idf is one of the most popular term-weighting schemes today. \n\nA survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf\u2013idf.\n\nVariations of the tf\u2013idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf\u2013idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n\nOne of the simplest ranking functions is computed by summing the tf\u2013idf for each query term; many more sophisticated ranking functions are variants of this simple model.*\n\nIn order to better understanding this part of NLP , we used the following notebook in order to practice and understand.\n\n[COMPLETE STUDY](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert)\n\nAs we can see in the study,before using the TF-IFD there is necessary a bit preparation.\n","7c3b96cf":"2.TF-IDF","6f0c40c4":"-> Here we have a problem. the loop isn't working correctly. We think it is due to the fact to access to the .xml level, we have, in each folder, a sublevel which is also called \"Documents_xml\" that we have to take into condiseration.\n\nLet's try to solve that problem.","e44187c3":"By casting the bag of words to a set, we can automatically remove any duplicate words.","79faa4c2":"Often times, when building a model with the goal of understanding text, you\u2019ll see all of stop words being removed. Another strategy is to score the relative importance of words using TF-IDF.\nTerm Frequency (TF)\nThe number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency.\n![image.png](attachment:97df9a04-e16b-4918-9c70-58c4b5c017ed.png)\nThe following code implements term frequency in python.","66977494":"Trying implementation for all the files","7401065c":"Now let's do this with the files","b205e3e2":"The following code implements inverse data frequency in python.\n","523bc73a":"Now we are going to try to iterate the same thing for all the files in the directory","6cdd98ab":"We checked it gives us the data, but we commented it so the code is cleaned","915a52fc":" * The following are just ideas to improve the first part,I have to study them *","8fb8760f":"Reminder: how the stopwords remover works","7c1932b7":"We can also perform a more precise analysis by doing:","2a6ce968":"From the source: \n\n\n\"Another problem with the bag of words approach is that it doesn\u2019t account for noise. In other words, certain words are used to formulate sentences but do not add any semantic meaning to the text. For example, the most commonly used word in the english language is the which represents 7% of all words written or spoken. You couldn\u2019t make deduce anything about a text given the fact that it contains the word the. On the other hand, words like good and awesome could be used to determine whether a rating was positive or not.\nIn natural language processing, useless words are referred to as stop words. The python natural language toolkit library provides a list of english stop words.\"","5d4e45b9":"The following lines compute the term frequency for each of our documents.","8b565656":"N.B.: We can also do it throught the regex module. I leave the code below as a model how it can be used.","e19a7ae8":"How looks the database?\n\nThe dataset is composed as you can see on the link below.\n[\nDATASET CONTENT](https:\/\/docs.google.com\/document\/d\/1Mn6noHw5OmjSdEuvBQxLpQlwaNbTDMW23O_uMSW5zOQ\/edit)\n\nThis is the result of executing a code in order to have a global vision in order to see how the database looks. That code can be find in this [notebook](https:\/\/www.kaggle.com\/patriciacanton\/dabatabase-preview). It has been put appart in order not to slow the execution.\n\nTherefore we can already download the following libraries which we know are going to be helpful.","dbd14885":"We are trying first for citances and then we'll do it also for the references","0cbae146":"SYSTEM REPLICATION\n\nThis notebook is part of the first phase of the \"Sci-SummNet 2020\" project. In this notebook we will try to reply the System 12 from the previous year, which is the one that most interest after having studied the two previus years' sumbissions.\n\nThe aim for us is to deeply understand how it works in order to later present our own solution.\n\nThe pdf document can be read here:\n\n[SYSTEM 6 PAPER](https:\/\/www.researchgate.net\/publication\/334388245_NUDT_CLSciSumm-18)\n\nThis paper presents the NUDT approach to the 4th Com-putational Linguistics Scientific Document Summarization Shared Task.\n\nWhat is this competition about?\n\n(BIRNDL CL-SciSumm 2019). Given a set of reference papers and the set of papers citing them, the proposed approach has a threefold aim. (1a) Identify the text spans in the reference paper that are referenced by a specific citation in the citing papers. (1b) Assign a facet to each citation describing the semantics behind the citation. (2) Generate a summary of the reference paper consisting of the most relevant cited text spans. The Poli2Sum approach to tasks (1a) and (1b) relies on an ensemble of classification and regression models trained on the annotated pairs of cited and citing sentences. Facet assignment is based on the relative positions of the cited sentences locally to the corresponding section and globally in the entire paper. Task (2) is addressed by predicting the overlap (in terms of units of text) between the selected text spans and the summary generated by the domain experts. The output summary consists of the subset of sentences maximizing the predicted overlap score.\n\nThe system architecture is the following:\n[\nARCHITECTURE SYSTEM](https:\/\/docs.google.com\/presentation\/d\/10PAkYPwszTSO9QrHWocI0i1fH0Q_duubKIYWBZ7nNW4\/edit)\n\nFurther information to a better understanding can be find in the bellow links:\n\n[General previous analysis](https:\/\/docs.google.com\/document\/d\/1AoY2t3NF9xRi7_IwGQTSHWBpcAAivB55AsA5KK8jjoM\/edit)\n[Phase 1](https:\/\/docs.google.com\/document\/d\/1SPQciJs1OrMWqlK9sKd12lyUY9gBbyVT4Atw-Gyb67A\/edit)\n[System Summary \/ Analysis 6](https:\/\/www.kaggle.com\/patriciacanton\/replication-system-6\/edit\/run\/56888675)\nThis model is particularly complete.\n\nThey use high level analysis, particularly in step 2 and 3, which use Information Retrieval with Word Embeddings. This technique will be studied later, but for now we want to focus on a much more supervised approach. We will therefore skip to the step 3. As for the VSM and BM25 , we will come back to them later.\n\nAs for the step for, which is precisely that supervised approach we are looking for, we can observe that tehere is plenty of possibilities.\n\nTo be more precise, and as it can be seen in the System Architecture, we can see they use 4 meethods, we are ourselves going to begin with Vectorized TF-IDF as a feature extractor, then later use the others in a comparative way.\n\nAs for the voting method, (step 5), we will study them later, based on the output we will obtain from this first parts.\n\nSo, what are going to be the steps of this replication?\n\nFor now, they are going to be 4 steps.\n\nPART A\n\nRead the data\nPre-processing of the text\nPART B\n\nRandom forest classifier\nLet's go!\n\nUPDATE 16\/04\/2021:\n\n[SYSTEM ARCHITECTURE IN DETAIL](https:\/\/docs.google.com\/presentation\/d\/1J03kyzQtaLkZFPkb0X_FmMTqJ4EouQMm9sQSDDCgKbE\/edit#slide=id.p)\n\n*NOTE:* THIS NOTEBOOK IS STILL IN PROGRESS AND THEREFORE HAS A LOT OF ERRORS , IT'S NOT READY TO USE YET","f450ed30":"Machine learning algorithms cannot work with raw text directly. Rather, the text must be converted into vectors of numbers. In natural language processing, a common technique for extracting features from text is to place all of the words that occur in the text in a bucket. This aproach is called a bag of words model or BoW for short. It\u2019s referred to as a \u201cbag\u201d of words because any information about the structure of the sentence is lost.","dcb0ea72":"Inverse Data Frequency (IDF)\n\nThe log of the number of documents divided by the number of documents that contain the word w. Inverse data frequency determines the weight of rare words across all documents in the corpus.\n![image.png](attachment:f26f88a4-6f8b-4532-b52b-df2d3cff7f74.png)","06fbada7":"Remove punctuation","8d8088d5":"> 28\/05\/2021:  It works! But we need to visualizualise ALL the xml files!! (will come baack to this)","b2fe8443":"Rather than manually implementing TF-IDF ourselves, we could use the class provided by sklearn.","43ef2c56":"                                                                                    PART A","58dc0788":"***************************","d23036a7":"Lastly, the TF-IDF is simply the TF multiplied by IDF.\n![image.png](attachment:e1796de5-5099-4ccb-a243-10847c37bbd8.png)","97b3d24e":"The previous model is extracted from [here. ](http:\/\/towardsdatascience.com\/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76)","99b4ae99":"Finally, we can compute the TF-IDF scores for all the words in the corpus.","e8ec9544":"Now we visualize","25dd3ce6":"> Note: We realize that the cleaning of the stopwords can also be done here!\nBelow the corresponding code, but commented because we are not using it ","db949d69":"2.2 SENTENCE FILTERING\n\n2.2.1 Error threshold\n\n2.2.2 Ratio of illegal words\n\n2.2.3 Choose ratio\n\nWhat is Error Threshold ?\n\nIn mathematical or statistical modeling a threshold model is any model where a threshold value, or set of threshold values, is used to distinguish ranges of values where the behaviour predicted by the model varies in some important way. A particularly important instance arises in toxicology, where the model for the effect of a drug may be that there is zero effect for a dose below a critical or threshold value, while an effect of some significance exists above that value. Certain types of regression model may include threshold effects\n\nWhat do they mean by Ratio of illegal words ?\n\nFor step two to take place correctly, we need to set an error threshold. To do this, an error threshold is used that allows us to determine precisely what we are left with and what we are not.\n\nHow is this threshold determined? For this, a dictionary of more than 10 k words is used. This dictionary will serve as a judge of what is kept and what is removed. The maintained words are referred to as \"legal words\".\n\nHow to choose the ratio?","70a02572":"The IDF is computed once for all documents.","d37cb378":"3.2 SMOTE+ENN oversampling\n\nMain documentation 1\n\nMain documentation 2\n\nMain documentation 3","eb0dd886":"1. BAG OF WORD COUNTS","e7bb8556":"And just to confirm everything works:","14f39eb0":"                                                                           PART B","1a2849ea":"STEP 2: PRE-PROCESSING\n\nThe complete analysis of system 6 can be find here.\n\n2.1 NLTK ANALYSIS\n\n2.1.1 Part of Speech\n\n2.1.2 Remove Punctuation\n\n2.2.3 Remove Stopwords\n\nWhat is Part of Speech?\n\nIn corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n\nOnce performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, by a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms.","faeaacc8":"Next, we\u2019ll create a dictionary of words and their occurence for each document in the corpus (collection of documents).","b0f1dde6":"Now we have to import SKLEARN so we can perform the similarity measure. \n\nWhy Sklearn?\n\nBecause it is a free software machine learning library for the Python programming language.\n\nIt features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n\nTherefore it is perfect for extracting the cosine similarity. ","3e86c260":"Now we have calculated cosine similarity we adapt it to the vectors expression","a5d2243d":"Main differences with the previous notebook, is that there is no need to have a second part reading Json for the reference data, as everything is in XML. \nAt this stade, we should have extacted all the information needed. ","db9e8044":"Remove Stopwords","23f5c90e":"This notebook is an update of the previous one \"System 6 replication\", that you can find [here.](http:\/\/www.kaggle.com\/patriciacanton\/replication-system-6) \n\nThe database has been modified in order to simplify the databse, so we can work only with XML elements. ","40a33d4d":"Now we need to read the XML elements.\n\nThere are a few options we can use for that, they have been tested here.\n\nAfter trying , we have decided to implement a function in order to print the parsed elements.\n\nWhat is the parser?\n\nIn computer science, parsing is the process of analysing text to determine if it belongs to a specific language or not (i.e. is syntactically valid for that language's grammar). It is an informal name for the syntactic analysis process\n\nBut before that, we are going to convert the XML files into CSV because it is easier to manipulate.\n\nThat transformation into CSV files will allow us to, as it has been specifically asked, visualize those parsed elemets taking into consideration the SENTENCES and SECTIONS.\n\nFurthermore, CSV is a user-friendly format more appealing to be read an used.\n\nThe whole study about XML to CSV conversion can be found [here](https:\/\/www.kaggle.com\/patriciacanton\/how-to-convert-xml-to-csv\/edit).\n\n*Approach *\n\nREAD, LIST AND PARSE XML DATA\nWe are trying first with a single xml file, and if it works, we'll iterate everything with a loop.\n\nTrying with the tree element"}}