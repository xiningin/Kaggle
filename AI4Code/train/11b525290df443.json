{"cell_type":{"b78f3f52":"code","2fb5c7aa":"code","9716303d":"code","9b7a1cfd":"code","f844abab":"code","c3094e01":"code","eb1f6f43":"code","7ec71506":"code","2ca1f608":"code","5a3341f6":"code","04eeb5d6":"code","b505cc8f":"code","f2f0305d":"code","4226a74d":"code","26fc4978":"code","cdfa8c93":"code","2f81a899":"code","e4f0e747":"code","4074dbc2":"code","2839b05d":"code","ea70a1c7":"code","23402cdd":"code","310c0da2":"code","34bc94b1":"code","b09ff3a7":"code","7464b891":"code","9be6eb62":"code","4ff4b844":"code","0ddcbc8f":"code","1862a546":"code","f9c60e88":"code","8d7d0592":"code","91bcce96":"code","1f3aa0ad":"code","f7ba68db":"code","93cb2b0e":"code","48856dc1":"code","9de64671":"code","14e09c0f":"code","436dac28":"code","89e06723":"code","1c65d3d2":"code","ca050cf0":"code","71ab4fcf":"code","c205ddf5":"markdown","f9d0437e":"markdown","52d590ff":"markdown","d600a1aa":"markdown","68302cf9":"markdown","c8431e2e":"markdown","7797715d":"markdown","682c51b4":"markdown"},"source":{"b78f3f52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2fb5c7aa":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","9716303d":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.head()","9b7a1cfd":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n  \n#Check for the missing values in the columns \nfig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(train_data.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","f844abab":"#drop the columns without any relationship with the dataset\ntrain_data = train_data.drop(columns = ['Cabin','Name','Ticket','PassengerId'])","c3094e01":"# filling na values with mean for age, \ntrain_data['Age'].fillna((train_data['Age'].mean()), inplace=True)","eb1f6f43":"#plotting Survival as function of Sex\nsns.barplot(x='Sex', y='Survived', data=train_data)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Sex\", fontsize=16)\n\nplt.show()\ntrain_data[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","7ec71506":"#plotting Survival as function of Pclass\nsns.barplot(x='Pclass', y='Survived', data=train_data)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Pclass\", fontsize=16)\n\nplt.show()\ntrain_data[[\"Pclass\", \"Survived\"]].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2ca1f608":"#both Sex and Pclass in the same plot\nsns.barplot(x='Sex', y='Survived', hue='Pclass', data=train_data)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Pclass and Sex\")\nplt.show()\n#here 0 is for female and 1 is for male","5a3341f6":"#the Parch column\ntrain_data[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n#the SibSp column\ntrain_data[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","04eeb5d6":"#pairplots for all columns\nsns.pairplot(data=train_data, hue=\"Survived\")","b505cc8f":"#Create a swarmplot to detect patterns, where is the highest survival rate\nsns.swarmplot(x = 'SibSp', y = 'Parch', hue = 'Survived', data = train_data, split = True, alpha=0.8)\nplt.show()","f2f0305d":"#1st model before featuring\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nyf = train_data.Survived\nbase_features = ['Parch','SibSp','Age', 'Fare','Pclass']\n\nXf = train_data[base_features]\n\ntrain_X, val_X, train_y, val_y = train_test_split(Xf, yf, random_state=1)\nfirst_model = RandomForestRegressor(n_estimators=21, random_state=1).fit(train_X, train_y)","4226a74d":"#Explore the relationship between SipSp and Parch in the predictions for a RF Model\ninter  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=['SibSp', 'Parch'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter, feature_names=['SibSp', 'Parch'], plot_type='contour')\nplt.show()","26fc4978":"#New feature FamilySize added to join the columns SibSp(sibling\/spouses) & Parch(parents\/children)\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] \ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).agg('mean')","cdfa8c93":"#check if person travelling alone is more likely to survive\ntrain_data['IsAlone'] = 0\ntrain_data.loc[train_data['FamilySize'] == 0, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","2f81a899":"#graphs for all columns including the newly made FamilySize & IsAlone\ncols = ['Survived', 'Parch', 'SibSp', 'Embarked','IsAlone', 'FamilySize']\n\nnr_rows = 2\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        \n        i = r*nr_cols+c       \n        ax = axs[r][c]\n        sns.countplot(train_data[cols[i]], hue=train_data[\"Survived\"], ax=ax)\n        ax.set_title(cols[i], fontsize=14, fontweight='bold')\n        ax.legend(title=\"survived\", loc='upper center') \n        \nplt.tight_layout()","e4f0e747":"#changing the fare into a continous feature\n#checking for unique points\nfeat_name = 'Fare'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","4074dbc2":"\ntrain_data[[\"Fare\", \"Survived\"]].groupby(['Survived'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n#as females have better survival rate hence grouping higher fare with females\ntrain_data.groupby(['Sex','Survived'])[['Fare']].agg(['min','mean','max'])","2839b05d":"#discreting the fare in 4 states\ntrain_data.loc[ train_data['Fare'] <= 7.22, 'Fare'] = 1\ntrain_data.loc[(train_data['Fare'] > 7.22) & (train_data['Fare'] <= 21.96), 'Fare'] = 2\ntrain_data.loc[(train_data['Fare'] > 21.96) & (train_data['Fare'] <= 40.82), 'Fare'] = 3\ntrain_data.loc[ train_data['Fare'] > 40.82, 'Fare'] = 4\ntrain_data['Fare'] = train_data['Fare'].astype(int)\n#plotting the data according to the 4 states\ng = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Fare', bins=20)\nplt.show()","ea70a1c7":"#plotting bar graphs for gender and fare\nsns.barplot(x='Sex', y='Survived', hue='Fare', data=train_data)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Fare and Sex\")\nplt.show()","23402cdd":"#changing the age into a continous feature\n#checking for unique points\nfeat_name = 'Age'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()\n\n#Exploring the relationship between Age and Pclass for a given model preductions\ninter  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=['Age', 'Pclass'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter, feature_names=['Age', 'Pclass'], plot_type='contour')\nplt.show()","310c0da2":"\ng = sns.FacetGrid(train_data, row='Sex', col='Pclass', hue='Survived', margin_titles=True, size=3, aspect=1.1)\ng.map(sns.distplot, 'Age', kde=False, bins=4, hist_kws=dict(alpha=0.6))\ng.add_legend()  \nplt.show()","34bc94b1":"#grouping age into different classes\ntrain_data.loc[ train_data['Age'] <= 16, 'Age'] = 1\ntrain_data.loc[(train_data['Age'] > 16) & (train_data['Age'] <= 32), 'Age'] = 2\ntrain_data.loc[(train_data['Age'] > 32) & (train_data['Age'] <= 64), 'Age'] = 3\ntrain_data.loc[ train_data['Age'] > 64, 'Age'] = 4\ntrain_data['Age'] = train_data['Age'].astype(int)\n#plotting the ages \nsns.barplot(x='Pclass', y='Survived', hue='Age', data=train_data)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Age and Sex\")\nplt.show()","b09ff3a7":"#a new feature Age*Class is added to join Age and Pclass\ntrain_data['Age*Class'] = train_data.Age * train_data.Pclass\ntrain_data[[\"Age*Class\", \"Survived\"]].groupby(['Age*Class'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n#a crosstab giving survival for Sex with Age*Class\npd.crosstab([train_data.Survived], [train_data.Sex,train_data['Age*Class']], margins=True).style.background_gradient(cmap='autumn_r')","7464b891":"\n#a crosstab giving survival for IsAlone with Sex\npd.crosstab([train_data.Survived], [train_data.Sex,train_data['IsAlone']], margins=True).style.background_gradient(cmap='autumn_r')","9be6eb62":"#a crosstab giving survival for Fare\npd.crosstab([train_data.Survived], [train_data.Fare], margins=True).style.background_gradient(cmap='autumn_r')\n","4ff4b844":"#new train_data head after adding new features\ntrain_data.head()","0ddcbc8f":"#training using new features\ny2 = train_data.Survived\n\nbase_features2 = ['Parch','SibSp','Age', 'Fare','Pclass','Age*Class','FamilySize','IsAlone']\n\nX2 = train_data[base_features2]\ntrain_X2, val_X2, train_y2, val_y2 = train_test_split(X2, y2, random_state=1)\nsecond_model = RandomForestRegressor(n_estimators=21, random_state=1).fit(train_X2, train_y2)\n#plotting pdp interact for age and pclass\ninter2  =  pdp.pdp_interact(model=second_model, dataset=val_X2, model_features=base_features2, features=['Age', 'Pclass'])\npdp.pdp_interact_plot(pdp_interact_out=inter2, feature_names=['Age', 'Pclass'], plot_type='contour')\nplt.show()","1862a546":"#plotting FamilySize with Pclass\ninter2  =  pdp.pdp_interact(model=second_model, dataset=val_X2, model_features=base_features2, features=['FamilySize', 'Pclass'])\npdp.pdp_interact_plot(pdp_interact_out=inter2, feature_names=['FamilySize', 'Pclass'], plot_type='contour')\nplt.show()","f9c60e88":"#As Sex and Embarked are not numerical OneHotEncoderis done\n# convert Sex values and Embarked values into dummis to use a numerical classifier \ndummies_Sex = pd.get_dummies(train_data.Sex)\ndummies_Embarked = pd.get_dummies(train_data.Embarked)\n#join the dummies to the final dataframe\ntrain_ready = pd.concat([train_data, dummies_Sex,dummies_Embarked], axis=1)\ntrain_ready.head()\n#drop the respective Sex and Embarked columns\ntrain_ready = train_ready.drop(columns = ['Sex','Embarked'])\n#checking for the data types to be numeric\ntrain_ready.info()","8d7d0592":"train_ready.head(11)","91bcce96":"#dropping Age*Class column (entropy 2,14) then the FamilySize column (entropy 1,82)\ntrain_ready = train_ready.drop(columns = ['Age*Class'])\ntrain_ready = train_ready.drop(columns = ['FamilySize'])","1f3aa0ad":"#checking entropy\nfrom scipy import stats\nfor name in train_ready:\n    print(name, \"column entropy :\", round(stats.entropy(train_ready[name].value_counts(normalize=True), base=2),2))","f7ba68db":"train_ready.head(10)","93cb2b0e":"#Drop unecessary columns\ntest = test.drop(columns = ['Cabin','Name','Ticket','PassengerId'])\n#check the test dataframe\ntest.head()","48856dc1":"#filling Non valid values with mean for age, \ntest['Age'].fillna((test['Age'].mean()), inplace=True)\ntest['Fare'].fillna((test['Fare'].mean()), inplace=True)\n#discreting fare to 4 states\ntest.loc[ test['Fare'] <= 7.22, 'Fare'] = 0\ntest.loc[(test['Fare'] > 7.22) & (test['Fare'] <= 21.96), 'Fare'] = 1\ntest.loc[(test['Fare'] > 21.96) & (test['Fare'] <= 40.82), 'Fare'] = 2\ntest.loc[ test['Fare'] > 40.82, 'Fare'] = 3\n#joining SibSp and Parch into FamiySize\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\ntest['IsAlone'] = 0\ntest.loc[test['FamilySize'] == 1, 'IsAlone'] = 1\n#discreting Age to 4 states\ntest.loc[ test['Age'] <= 16, 'Age'] = 1\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age'] = 2\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 64), 'Age'] = 3\ntest.loc[ test['Age'] > 64, 'Age'] = 4\n#adding new feature Age*Class\ntest['Age*Class'] = test.Age * test.Pclass\n#checking the datatypes\ntest.info()","9de64671":"#as in the train dataset, build dummis in the sex and embarked columns\ntest_dummies_Sex = pd.get_dummies(test.Sex)\ntest_dummies_Embarked = pd.get_dummies(test.Embarked)\ntest_ready = pd.concat([test, test_dummies_Sex,test_dummies_Embarked], axis=1)\ntest_ready.head()\n#drop these columns, we keep only numerical values\ntest_ready = test_ready.drop(columns = ['Sex','Embarked'])\n#dropping Age*Class column then the FamilySize column as done in train dataset\ntest_ready = test_ready.drop(columns = ['Age*Class'])\ntest_ready = test_ready.drop(columns = ['FamilySize'])","14e09c0f":"test_ready.info()\ntest_ready.head()","436dac28":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# Create arrays for the features and the response variable\ny = train_ready['Survived'].values\nX = train_ready.drop('Survived',axis=1).values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=21, stratify=y)\n\n#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n\n#Models\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","89e06723":"clfs = []\nseed = 3\nclfs.append((\"LogReg\", Pipeline([(\"Scaler\", StandardScaler()), (\"LogReg\", LogisticRegression())]))) \n            \nclfs.append((\"XGBClassifier\",Pipeline([(\"Scaler\", StandardScaler()),(\"XGB\", XGBClassifier())]))) \n\nclfs.append((\"KNN\",Pipeline([(\"Scaler\", StandardScaler()),(\"KNN\", KNeighborsClassifier(n_neighbors=8))])))  \n                                   \nclfs.append((\"DecisionTreeClassifier\",Pipeline([(\"Scaler\", StandardScaler()),(\"DecisionTrees\", DecisionTreeClassifier())])))  \n             \nclfs.append((\"RandomForestClassifier\",Pipeline([(\"Scaler\", StandardScaler()),(\"RandomForest\", RandomForestClassifier())])))\n                                     \nclfs.append((\"GradientBoostingClassifier\",Pipeline([(\"Scaler\", StandardScaler()),(\"GradientBoosting\", GradientBoostingClassifier(n_estimators=100))]))) \n                            \nclfs.append((\"RidgeClassifier\",Pipeline([(\"Scaler\", StandardScaler()),(\"RidgeClassifier\", RidgeClassifier())])))                                  \n\nclfs.append((\"BaggingRidgeClassifier\", Pipeline([(\"Scaler\", StandardScaler()),(\"BaggingClassifier\", BaggingClassifier())])))\n                                  \nclfs.append((\"ExtraTreesClassifier\",Pipeline([(\"Scaler\", StandardScaler()),(\"ExtraTrees\", ExtraTreesClassifier())]))) \n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 7\nresults, names  = [], [] \nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train,cv= 5, scoring=scoring, n_jobs=-1)                                 \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","1c65d3d2":"#apply Scla to train in order to standardize data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nscaler.fit(X)\nscaled_features = scaler.transform(X)\ntrain_sc = pd.DataFrame(scaled_features) # columns=df_train_ml.columns[1::])\n\n#apply Scla to test csv (new file)  in order to standardize data \n\nX_csv_test = test_ready.values  #X_csv_test the new data that is going to be test \nscaler.fit(X_csv_test)\nscaled_features_test = scaler.transform(X_csv_test)\ntest_sc = pd.DataFrame(scaled_features_test) # , columns=df_test_ml.columns)\n\nscaled_features_test.shape","ca050cf0":"test.head()","71ab4fcf":"#Building XGBClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\nclf = xgb.XGBClassifier(n_estimators=250, random_state=4,bagging_fraction= 0.791787170136272, colsample_bytree= 0.7150126733821065,feature_fraction= 0.6929758008695552,gamma= 0.6716290491053838,learning_rate= 0.030240003246947006,max_depth= 2,min_child_samples= 5,num_leaves= 15,reg_alpha= 0.05822089056228967,reg_lambda= 0.14016232510869098,subsample= 0.9)\n\nclf.fit(scaled_features, y)\n\ny_pred_xgb= clf.predict(scaled_features_test)\nprint(y_pred_xgb)\n\n#Upload the test file for XGB\nresult_xgb = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nresult_xgb['Survived'] = y_pred_xgb\nresult_xgb.to_csv('Titanic_xgb.csv', index=False)\nresult_xgb.head()","c205ddf5":"men who were alone had less chance of surviving","f9d0437e":"\n**XGBClassifier**","52d590ff":"the people having Fare group 1 (Fare > 7.22 & Fare <= 21.96) have the lower chance of survive,","d600a1aa":"higher fares give better survival rate","68302cf9":"female from 2-6 have better chance of surviving than male from 4-6","c8431e2e":"\n** Testing several Supervise learning models**","7797715d":"less age and higher class has a better survival rate","682c51b4":"**PREPARING THE TEST SET**"}}