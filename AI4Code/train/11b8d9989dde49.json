{"cell_type":{"8f91b2f8":"code","03a77ffe":"code","aa396f57":"code","e5f449b4":"code","fd123eeb":"code","b6405d66":"code","d59511ee":"code","cd7d0729":"code","7533b2b6":"code","886d66c1":"code","eab16ada":"code","3616fe40":"code","64f781b6":"code","99545638":"code","63db16ae":"code","512390ab":"code","093c25a0":"code","3add5d3d":"code","5b37ba60":"code","d9783e04":"code","0de5db1d":"code","bffd9917":"code","29316a5a":"code","225775d3":"code","7871e0b5":"code","c45993e9":"code","24e5dce6":"code","418fd333":"code","eb473a35":"code","b25f18f9":"code","4c8b932f":"code","faf21fad":"code","9f9e51f2":"code","be718f81":"code","d82a8e68":"code","3e76b25f":"code","8fb3d704":"code","948ac63d":"code","71db8ef5":"code","5fccbd70":"code","21ff669f":"code","b1879ee4":"code","968b55ab":"code","957e60d1":"code","b9196b3d":"code","8243acd5":"code","b025c46a":"code","908ccfbf":"code","e628991d":"code","b4d3faf4":"code","8c52f6d0":"code","203ecd02":"code","b452ca31":"code","1895df5a":"code","4de6cf46":"code","f623cf43":"code","8f533c8a":"code","108fad26":"code","0af70ace":"code","63776c0a":"markdown","9f4585d8":"markdown","f8a8d07f":"markdown","4b3d937e":"markdown","16534f7a":"markdown","cfb35c5e":"markdown","39294dbc":"markdown","66a849af":"markdown","b466a030":"markdown","731c9977":"markdown","1d50011e":"markdown","2f03519a":"markdown","8d93cc7e":"markdown","47463740":"markdown","5d8ea390":"markdown","53981cc7":"markdown","1f552aed":"markdown","18a80322":"markdown","da4b548e":"markdown","11e2c1c2":"markdown","fc211b5e":"markdown","548adcc7":"markdown","cdd24fab":"markdown","34cd526f":"markdown","baa1105e":"markdown","56ebda55":"markdown","f1d3aada":"markdown","63a09726":"markdown","d12dacd2":"markdown","e748a442":"markdown","35b6658b":"markdown","62c48b22":"markdown","e013a43d":"markdown","eb7ebc0e":"markdown","d2b5accc":"markdown","937084af":"markdown","af15b822":"markdown","711e6754":"markdown","e6290f43":"markdown","07247bc6":"markdown","fe733483":"markdown","c99bc110":"markdown","ebd0b36a":"markdown","71d5123a":"markdown","437a5057":"markdown","de4b4dcb":"markdown","c5f328b2":"markdown","7c5068f9":"markdown","41853914":"markdown","a3cb8397":"markdown","9fbdeae3":"markdown","fec94583":"markdown"},"source":{"8f91b2f8":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","03a77ffe":"from fastai import *\n#import fastai\nfrom fastai.tabular import *\n#from fastai.tabular import * \n#from fastai.structured import *\n\n\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\n\nfrom sklearn import metrics","aa396f57":"PATH = \"..\/input\/\"","e5f449b4":"!ls {PATH}","fd123eeb":"df_raw = pd.read_csv(f'{PATH}train\/Train.csv', low_memory=False, parse_dates=[\"saledate\"])","b6405d66":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","d59511ee":"display_all(df_raw.tail().T)","cd7d0729":"display_all(df_raw.describe(include='all').T)","7533b2b6":"df_raw.SalePrice = np.log(df_raw.SalePrice)","886d66c1":"m = RandomForestRegressor(n_jobs=-1)\n# The following code is supposed to fail due to string values in the input data\nm.fit(df_raw.drop('SalePrice', axis=1), df_raw.SalePrice)","eab16ada":"#tabular.add_datepart(df_raw, 'saledate')\nadd_datepart(df_raw, 'saledate')\ndf_raw.saleYear.head()","3616fe40":"def train_cats(df):\n    \"\"\"Change any columns of strings in a panda's dataframe to a column of\n    categorical values. This applies the changes inplace.\n\n    Parameters:\n    -----------\n    df: A pandas dataframe. Any columns of strings will be changed to\n        categorical values.\n\n    Examples:\n    ---------\n\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    note the type of col2 is string\n\n    >>> train_cats(df)\n    >>> df\n\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    now the type of col2 is category\n    \"\"\"\n    for n,c in df.items():\n        if pd.api.types.is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n    \n        #modified from \n        #if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()","64f781b6":"train_cats(df_raw)","99545638":"df_raw.UsageBand.cat.categories","63db16ae":"df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)","512390ab":"df_raw.UsageBand = df_raw.UsageBand.cat.codes","093c25a0":"display_all(df_raw.isnull().sum().sort_index()\/len(df_raw))","3add5d3d":"os.makedirs('tmp', exist_ok=True)\ndf_raw.to_feather('tmp\/bulldozers-raw')","5b37ba60":"#df_raw = pd.read_feather('tmp\/bulldozers-raw')\n\n# replaced the above with this workaround:\nimport feather\ndf_raw = feather.read_dataframe('tmp\/bulldozers-raw')","d9783e04":"#df, y, nas = proc_df(df_raw, 'SalePrice')","0de5db1d":"from pandas.api.types import is_string_dtype, is_numeric_dtype","bffd9917":"def get_sample(df,n):\n    \"\"\" Gets a random sample of n rows from df, without replacement.\n\n    Parameters:\n    -----------\n    df: A pandas data frame, that you wish to sample from.\n    n: The number of rows you wish to sample.\n\n    Returns:\n    --------\n    return value: A random sample of n rows of df.\n\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    >>> get_sample(df, 2)\n       col1 col2\n    1     2    b\n    2     3    a\n    \"\"\"\n    idxs = sorted(np.random.permutation(len(df))[:n])\n    return df.iloc[idxs].copy()","29316a5a":"def numericalize(df, col, name, max_n_cat):\n    \"\"\" Changes the column col from a categorical type to it's integer codes.\n\n    Parameters:\n    -----------\n    df: A pandas dataframe. df[name] will be filled with the integer codes from\n        col.\n\n    col: The column you wish to change into the categories.\n    name: The column name you wish to insert into df. This column will hold the\n        integer codes.\n\n    max_n_cat: If col has more categories than max_n_cat it will not change the\n        it to its integer codes. If max_n_cat is None, then col will always be\n        converted.\n\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    note the type of col2 is string\n\n    >>> train_cats(df)\n    >>> df\n\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    now the type of col2 is category { a : 1, b : 2}\n\n    >>> numericalize(df, df['col2'], 'col3', None)\n\n       col1 col2 col3\n    0     1    a    1\n    1     2    b    2\n    2     3    a    1\n    \"\"\"\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = col.cat.codes+1","225775d3":"def fix_missing(df, col, name, na_dict):\n    \"\"\" Fill missing data in a column of df with the median, and add a {name}_na column\n    which specifies if the data was missing.\n\n    Parameters:\n    -----------\n    df: The data frame that will be changed.\n\n    col: The column of data to fix by filling in missing data.\n\n    name: The name of the new filled column in df.\n\n    na_dict: A dictionary of values to create na's of and the value to insert. If\n        name is not a key of na_dict the median will fill any missing data. Also\n        if name is not a key of na_dict and there is no missing data in col, then\n        no {name}_na column is not created.\n\n\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n\n    >>> fix_missing(df, df['col1'], 'col1', {})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1     2    2    True\n    2     3    2   False\n\n\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n\n    >>> fix_missing(df, df['col2'], 'col2', {})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n\n\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n\n    >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1   500    2    True\n    2     3    2   False\n    \"\"\"\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict","7871e0b5":"def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n    changes the df into an entirely numeric dataframe.\n\n    Parameters:\n    -----------\n    df: The data frame you wish to process.\n\n    y_fld: The name of the response variable\n\n    skip_flds: A list of fields that dropped from df.\n\n    ignore_flds: A list of fields that are ignored during processing.\n\n    do_scale: Standardizes each column in df. Takes Boolean Values(True,False)\n\n    na_dict: a dictionary of na columns to add. Na columns are also added if there\n        are any missing values.\n\n    preproc_fn: A function that gets applied to df.\n\n    max_n_cat: The maximum number of categories to break into dummy values, instead\n        of integer codes.\n\n    subset: Takes a random subset of size subset from df.\n\n    mapper: If do_scale is set as True, the mapper variable\n        calculates the values used for scaling of variables during training time (mean and standard deviation).\n\n    Returns:\n    --------\n    [x, y, nas, mapper(optional)]:\n\n        x: x is the transformed version of df. x will not have the response variable\n            and is entirely numeric.\n\n        y: y is the response variable\n\n        nas: returns a dictionary of which nas it created, and the associated median.\n\n        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continuous\n        variables which is then used for scaling of during test-time.\n\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    note the type of col2 is string\n\n    >>> train_cats(df)\n    >>> df\n\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n\n    now the type of col2 is category { a : 1, b : 2}\n\n    >>> x, y, nas = proc_df(df, 'col1')\n    >>> x\n\n       col2\n    0     1\n    1     2\n    2     1\n\n    >>> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n\n    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n                          ([:children], StandardScaler())])\n\n    >>>round(fit_transform!(mapper, copy(data)), 2)\n\n    8x4 Array{Float64,2}:\n    1.0  0.0  0.0   0.21\n    0.0  1.0  0.0   1.88\n    0.0  1.0  0.0  -0.63\n    0.0  0.0  1.0  -0.63\n    1.0  0.0  0.0  -1.46\n    0.0  1.0  0.0  -0.63\n    1.0  0.0  0.0   1.04\n    0.0  0.0  1.0   0.21\n    \"\"\"\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = df[y_fld].cat.codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res","c45993e9":"df, y, nas = proc_df(df_raw, 'SalePrice')","24e5dce6":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(df, y)\nm.score(df,y)","418fd333":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 12000  # same as Kaggle's test set size\nn_trn = len(df)-n_valid\nraw_train, raw_valid = split_vals(df_raw, n_trn)\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","eb473a35":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","b25f18f9":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","4c8b932f":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice', subset=30000, na_dict=nas)\nX_train, _ = split_vals(df_trn, 20000)\ny_train, _ = split_vals(y_trn, 20000)","faf21fad":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","9f9e51f2":"m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","be718f81":"import IPython, graphviz\nfrom sklearn.tree import export_graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\" Draws a representation of a random forest in IPython.\n\n    Parameters:\n    -----------\n    t: The tree you wish to draw\n    df: The data used to train the tree. This is used to get the names of the features.\n    \"\"\"\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))","d82a8e68":"draw_tree(m.estimators_[0], df_trn, precision=3)","3e76b25f":"m = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","8fb3d704":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","948ac63d":"preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0]","71db8ef5":"preds.shape","5fccbd70":"plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);","21ff669f":"m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","b1879ee4":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","968b55ab":"m = RandomForestRegressor(n_estimators=80, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","957e60d1":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","b9196b3d":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)","8243acd5":"from sklearn.ensemble import forest\n\ndef set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n))\n    \ndef reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n_samples))","b025c46a":"set_rf_samples(20000)","908ccfbf":"m = RandomForestRegressor(n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","e628991d":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","b4d3faf4":"reset_rf_samples()","8c52f6d0":"def dectree_max_depth(tree):\n    children_left = tree.children_left\n    children_right = tree.children_right\n\n    def walk(node_id):\n        if (children_left[node_id] != children_right[node_id]):\n            left_max = 1 + walk(children_left[node_id])\n            right_max = 1 + walk(children_right[node_id])\n            return max(left_max, right_max)\n        else: # leaf\n            return 1\n\n    root_node_id = 0\n    return walk(root_node_id)","203ecd02":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","b452ca31":"t=m.estimators_[0].tree_","1895df5a":"dectree_max_depth(t)","4de6cf46":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","f623cf43":"t=m.estimators_[0].tree_","8f533c8a":"dectree_max_depth(t)","108fad26":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","0af70ace":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","63776c0a":"Kaggle provides info about some of the fields of our dataset; on the [Kaggle Data info](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/data) page they say the following:\n\nFor this competition, you are predicting the sale price of bulldozers sold at auctions. The data for this competition is split into three parts:\n\n- **Train.csv** is the training set, which contains data through the end of 2011.\n- **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n- **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n\nThe key fields are in train.csv are:\n\n- SalesID: the uniue identifier of the sale\n- MachineID: the unique identifier of a machine.  A machine can be sold multiple times\n- saleprice: what the machine sold for at auction (only provided in train.csv)\n- saledate: the date of the sale","9f4585d8":"Is our validation set worse than our training set because we're over-fitting, or because the validation set is for a different time period, or a bit of both? With the existing information we've shown, we can't tell. However, random forests have a very clever trick called *out-of-bag (OOB) error* which can handle this (and more!)\n\nThe idea is to calculate error on the training set, but only include the trees in the calculation of a row's error where that row was *not* included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate validation set.\n\nThis also has the benefit of allowing us to see whether our model generalizes, even if we only have a small amount of data so want to avoid separating some out to create a validation set.\n\nThis is as simple as adding one more parameter to our model constructor. We print the OOB error last in our `print_score` function below.","f8a8d07f":"We can specify the order to use for categorical variables if we wish:","4b3d937e":"In the future we can simply read it from this fast format.","16534f7a":"- None\n- 0.5\n- 'sqrt'","cfb35c5e":"Another way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with `min_samples_leaf`) that we require some minimum number of rows in every leaf node. This has two benefits:\n\n- There are less decision rules for each leaf node; simpler models should generalize better\n- The predictions are made by averaging more rows in the leaf node, resulting in less volatility","39294dbc":"It turns out that one of the easiest ways to avoid over-fitting is also one of the best ways to speed up analysis: *subsampling*. Let's return to using our full dataset, so that we can demonstrate the impact of this technique.","66a849af":"But let's save this file for now, since it's already in format can we be stored and accessed efficiently.","b466a030":"In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variable(s). https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination","731c9977":"We'll replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable.","1d50011e":"### Intro to bagging","2f03519a":"To learn about bagging in random forests, let's start with our basic model again.","8d93cc7e":"The categorical variables are currently stored as strings, which is inefficient, and doesn't provide the numeric coding required for a random forest. Therefore we call `train_cats` to convert strings to pandas categories.","47463740":"It's important to note what metric is being used for a project. Generally, selecting the metric(s) is an important part of the project setup. However, in this case Kaggle tells us what metric to use: RMSLE (root mean squared log error) between the actual and predicted auction prices. Therefore we take the log of the prices, so that RMSE will give us what we need.","5d8ea390":"This dataset contains a mix of **continuous** and **categorical** variables.\n\nThe following method extracts particular date fields from a complete datetime for the purpose of constructing categoricals.  You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend\/cyclical behavior as a function of time at any of these granularities.","53981cc7":"To get better at technical writing, study the top ranked Kaggle kernels from past competitions, and read posts from well-regarded technical bloggers. Some good role models include:\n\n- [Peter Norvig](http:\/\/nbviewer.jupyter.org\/url\/norvig.com\/ipython\/ProbabilityParadox.ipynb) (more [here](http:\/\/norvig.com\/ipython\/))\n- [Stephen Merity](https:\/\/smerity.com\/articles\/2017\/deepcoder_and_ai_hype.html)\n- [Julia Evans](https:\/\/codewords.recurse.com\/issues\/five\/why-do-neural-networks-think-a-panda-is-a-vulture) (more [here](https:\/\/jvns.ca\/blog\/2014\/08\/12\/what-happens-if-you-write-a-tcp-stack-in-python\/))\n- [Julia Ferraioli](http:\/\/blog.juliaferraioli.com\/2016\/02\/exploring-world-using-vision-twilio.html)\n- [Edwin Chen](http:\/\/blog.echen.me\/2014\/10\/07\/moving-beyond-ctr-better-recommendations-through-human-evaluation\/)\n- [Slav Ivanov](https:\/\/blog.slavv.com\/picking-an-optimizer-for-style-transfer-86e7b8cba84b) (fast.ai student)\n- [Brad Kenstler](https:\/\/hackernoon.com\/non-artistic-style-transfer-or-how-to-draw-kanye-using-captain-picards-face-c4a50256b814) (fast.ai and USF MSAN student)","1f552aed":"- 1, 3, 5, 10, 25, 100","18a80322":"## Bagging","da4b548e":"Let's get a baseline for this full set to compare to.","11e2c1c2":"We can also increase the amount of variation amongst the trees by not only use a sample of rows for each tree, but to also using a sample of *columns* for each *split*. We do this by specifying `max_features`, which is the proportion of features to randomly select from at each split.","fc211b5e":"## About this course","548adcc7":"## Imports","cdd24fab":"## Speeding things up","34cd526f":"This shows that our validation set time difference is making an impact, as is model over-fitting.","baa1105e":"An r^2 in the high-80's isn't bad at all (and the RMSLE puts us around rank 100 of 470 on the Kaggle leaderboard), but we can see from the validation set score that we're over-fitting badly. To understand this issue, let's simplify things down to a single small tree.","56ebda55":"The shape of this curve suggests that adding more trees isn't going to help us much. Let's check. (Compare this to our original model on a sample)","f1d3aada":"# Random Forests","63a09726":"Normally, pandas will continue displaying the text categories, while treating them as numerical data internally. Optionally, we can replace the text categories with numbers, which will make this variable non-categorical, like so:.","d12dacd2":"Since each additional tree allows the model to see more data, this approach can make additional trees more useful.","e748a442":"### Out-of-bag (OOB) score","35b6658b":"### Subsampling","62c48b22":"The basic idea is this: rather than limit the total amount of data that our model can access, let's instead limit it to a *different* random subset per tree. That way, given enough trees, the model can still see *all* the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before.","e013a43d":"## Single tree","eb7ebc0e":"We'll grab the predictions for each individual tree, and look at one example.","d2b5accc":"## Reducing over-fitting","937084af":"We revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods.","af15b822":"The more familiarity you have with numeric programming in Python, the better. If you're looking to improve in this area, we strongly suggest Wes McKinney's [Python for Data Analysis, 2nd ed](https:\/\/www.amazon.com\/Python-Data-Analysis-Wrangling-IPython\/dp\/1491957662\/ref=asap_bc?ie=UTF8).\n\nFor machine learning with Python, we recommend:\n\n- [Introduction to Machine Learning with Python](https:\/\/www.amazon.com\/Introduction-Machine-Learning-Andreas-Mueller\/dp\/1449369413): From one of the scikit-learn authors, which is the main library we'll be using\n- [Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow, 2nd Edition](https:\/\/www.amazon.com\/Python-Machine-Learning-scikit-learn-TensorFlow\/dp\/1787125939\/ref=dp_ob_title_bk): New version of a very successful book. A lot of the new material however covers deep learning in Tensorflow, which isn't relevant to this course\n- [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1491962291\/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&psc=1&refRID=MBV2QMFH3EZ6B3YBY40K)\n","711e6754":"### Initial processing","e6290f43":"Wow, an r^2 of 0.98 - that's great, right? Well, perhaps not...\n\nPossibly **the most important idea** in machine learning is that of having separate training & validation data sets. As motivation, suppose you don't divide up your data, but instead use all of it.  And suppose you have lots of parameters:\n\n<img src=\"images\/overfitting2.png\" alt=\"\" style=\"width: 70%\"\/>\n<center>\n[Underfitting and Overfitting](https:\/\/datascience.stackexchange.com\/questions\/361\/when-is-a-model-underfitted)\n<\/center>\n\nThe error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it's not the best choice.  Why is that?  If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\n\nThis illustrates how using all our data can lead to **overfitting**. A validation set helps diagnose this problem.","07247bc6":"We now have something we can pass to a random forest!","fe733483":"Let's see what happens if we create a bigger tree.","c99bc110":"The training set result looks great! But the validation set is worse than our original model. This is why we need to use *bagging* of multiple trees to get more generalizable results.","ebd0b36a":"### Pre-processing","71d5123a":"We're still not quite done - for instance we have lots of missing values, which we can't pass directly to a random forest.","437a5057":"### Teaching approach","de4b4dcb":"## Base model","c5f328b2":"# Intro to Random Forests","7c5068f9":"Let's try our model again, this time with separate training and validation sets.","41853914":"### Books","a3cb8397":"### Tree building parameters","9fbdeae3":"We can't compare our results directly with the Kaggle competition, since it used a different validation set (and we can no longer to submit to this competition) - but we can at least see that we're getting similar results to the winners based on the dataset we have.\n\nThe sklearn docs [show an example](http:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_ensemble_oob.html) of different `max_features` methods with increasing numbers of trees - as you see, using a subset of features on each split requires using more trees, but results in better models:\n![sklearn max_features chart](http:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_ensemble_oob_001.png)","fec94583":"In any sort of data science work, it's **important to look at your data**, to make sure you understand the format, how it's stored, what type of values it holds, etc. Even if you've read descriptions about your data, the actual data may not be what you expect."}}