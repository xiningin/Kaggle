{"cell_type":{"abf111cd":"code","9da3cfaa":"code","ced57904":"code","e474ede4":"code","0f8b5025":"code","54424ec3":"code","5a125035":"code","aa60d22f":"code","23d4179d":"code","dca79375":"code","22042b98":"code","00db64c3":"code","294db2d2":"code","958ef8fe":"code","2875ea55":"code","a2a0614b":"code","fec8780a":"code","4c91c06d":"code","e594df9e":"code","56fb1095":"code","1288d0ef":"markdown","481bde13":"markdown"},"source":{"abf111cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9da3cfaa":"## Install iterativestrat\nimport sys\nsys.path.append('\/kaggle\/input\/iterativestrat\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","ced57904":"## Install PYTORCH Lightning\n\n!pip install ..\/input\/pytorch-lightning\/pytorch_lightning-0.8.5-py3-none-any.whl\n","e474ede4":"import os\nfrom glob import glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport torch\nimport torch.nn as nn\nimport random\nfrom torch.nn import functional as F\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import roc_auc_score\nimport pytorch_lightning as pl\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","0f8b5025":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(123)","54424ec3":"data_dir = '\/kaggle\/input\/lish-moa\/'\nfrom glob import glob\nglob(data_dir + \"*.csv\")","5a125035":"train_feats = pd.read_csv(data_dir + \"train_features.csv\")\ntest_feats = pd.read_csv(data_dir + \"test_features.csv\")\ntrain_targets_sc = pd.read_csv(data_dir + \"train_targets_scored.csv\")\nsample_submission = pd.read_csv(data_dir + \"sample_submission.csv\")","aa60d22f":"fold = train_targets_sc.copy()\ntarget_cols = [c for c in train_targets_sc.columns if c not in ['sig_id']]\nmls = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=234)\nfor n, (train_index, val_index) in enumerate(mls.split(fold, fold[target_cols])):\n    fold.loc[val_index, 'fold'] = int(n)\nfold['fold'] = fold['fold'].astype(int)","23d4179d":"target_cols = [c for c in train_targets_sc.columns if c not in [\"sig_id\", 'fold']]\ncat_cols = ['cp_type', 'cp_time', 'cp_dose']\nnumerical_cols = [c for c in train_feats.columns if train_feats.dtypes[c] != 'object']\nnumerical_cols = [c for c in numerical_cols if c not in cat_cols]","dca79375":"## SOURCE : https:\/\/www.kaggle.com\/yasufuminakama\/moa-pytorch-nn-starter\ndef cate2num(df):\n    df['cp_type'] = df['cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df['cp_time'] = df['cp_time'].map({24: 2, 48: 3, 72: 4})\n    df['cp_dose'] = df['cp_dose'].map({'D1': 5, 'D2': 6})\n    return df\n\ntrain_feats = cate2num(train_feats)\ntest_feats = cate2num(test_feats)","22042b98":"train_df = pd.merge(train_feats, fold, on = 'sig_id', how = 'inner')","00db64c3":"test_df = test_feats","294db2d2":"class DataSet(object):\n  def __init__(self, folds, fold_df, num_features, cat_features, debugging = False):\n    \"\"\"\n    folds = list of fold numbers to load data from\n    \"\"\"\n    \n    df = fold_df[fold_df.fold.isin(folds)]\n\n    if debugging:\n      _, df = train_test_split(df , stratify = df['fold'], test_size = 0.05, random_state = 10)\n\n    self.uids = list(df['sig_id'].values)\n    self.target = df[target_cols].values\n    self.num_features = df[numerical_cols].values\n    self.cat_features = df[cat_cols].values\n\n  def __len__(self):\n    return len(self.uids)\n  \n  def __getitem__(self, idx):\n\n    num = torch.FloatTensor(self.num_features[idx])\n    cat = torch.LongTensor(self.cat_features[idx])\n    target = torch.tensor(self.target[idx]).float()\n\n    return {\n        'num' : num, \n        'target' : target,\n        'cat' : cat\n    } \n\nclass TestDataSet(object):\n  def __init__(self, df, num_features, cat_features, debugging = False):\n    \"\"\"\n    folds = list of fold numbers to load data from\n    \"\"\"\n    self.uids = list(df['sig_id'].values)\n    self.num_features = df[numerical_cols].values\n    self.cat_features = df[cat_cols].values\n  def __len__(self):\n    return len(self.uids)\n\n  def __getitem__(self, idx):\n\n    num = torch.FloatTensor(self.num_features[idx])\n    cat = torch.LongTensor(self.cat_features[idx])\n    return {\n        'num' : num, \n        'cat' : cat,\n        'uid' : self.uids[idx]\n    } ","958ef8fe":"DEVICE = 'cuda'\nBS = 128","2875ea55":"def loss_fn(out, target):\n  loss = nn.BCEWithLogitsLoss()(out, target)\n  return loss\nclass litModel(pl.LightningModule):\n    def __init__(self, fold_number, max_grad_norm=1000, gradient_accumulation_steps=1, hidden_size=512, dropout=0.5, lr=1e-2,\nbatch_size=128, epochs=20, total_cate_size=7, emb_size=4, num_features = numerical_cols, cat_features = cat_cols):\n        ## Model Architecture Source : https:\/\/www.kaggle.com\/yasufuminakama\/moa-pytorch-nn-starter\n        super(litModel, self).__init__()\n        self.fold_number = fold_number\n        self.cate_emb = nn.Embedding(total_cate_size, emb_size, padding_idx=0)\n        self.emb_drop = nn.Dropout(0.2)\n        self.cont_emb = nn.Sequential(\n                          nn.Linear(len(num_features), hidden_size),\n                          nn.BatchNorm1d(hidden_size),\n                          nn.Dropout(dropout),\n                          nn.PReLU(),\n                          nn.Linear(hidden_size, hidden_size),\n                          nn.BatchNorm1d(hidden_size),\n                          nn.Dropout(dropout),\n                          )\n        head_hidden_size = hidden_size + len(cat_features)*emb_size\n        self.head = nn.Sequential(\n                          nn.Linear(head_hidden_size, head_hidden_size),\n                          nn.BatchNorm1d(head_hidden_size),\n                          nn.Dropout(0.1),\n                          nn.Linear(head_hidden_size, len(target_cols)),\n                          )\n\n    def _forward_impl(self, cont_x, cate_x):\n        # See note [TorchScript super()]\n        batch_size = cate_x.size(0)\n        cate_emb = self.cate_emb(cate_x).view(batch_size, -1)\n        cate_emb = self.emb_drop(cate_emb)\n        cont_emb = self.cont_emb(cont_x)\n        x = torch.cat((cont_emb, cate_emb), 1)\n        x = self.head(x)\n        return x\n\n    def forward(self, cont_x, cat_x):\n        return self._forward_impl(cont_x, cat_x)\n\n    def prepare_data(self):\n        train_fold = [x for x in range(5) if x!= self.fold_number]\n        test_fold = [self.fold_number]\n        print(\"TRAINING FOLD : \", train_fold)\n        print(\"TESTING FOLD : \", test_fold )\n        self.train_data_set = DataSet(train_fold, train_df, numerical_cols, cat_cols, False)\n        self.valid_data_set = DataSet(test_fold, train_df, numerical_cols, cat_cols, False)\n\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(self.train_data_set, BS, True, num_workers= 4)\n    \n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(self.valid_data_set, BS, False, num_workers= 4)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            T_max = 100\n        )\n        return [optimizer]\n  \n    def training_step(self, batch, batch_idx):\n        num_x = batch['num']\n        cat_x = batch['cat']\n        target = batch['target']\n        output = self.forward(num_x, cat_x)\n        loss = loss_fn(output, target)\n        return {'loss' : loss, 'log' : {'loss' : loss}}\n    \n    def validation_step(self, batch, batch_idx):\n        num_x = batch['num']\n        cat_x = batch['cat']\n        target = batch['target']\n        output = self.forward(num_x, cat_x)\n        loss = loss_fn(output, target)\n        return {'val_loss' : loss, 'y': target.detach(), 'y_hat': output.detach(), 'log' : {'loss' : loss}}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        y = torch.cat([x['y'] for x in outputs])\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\n        acc = (y_hat.round() == y).float().mean().item()\n        print(f\"Epoch {self.current_epoch} acc:{acc} loss:{avg_loss}\")\n        tensorboard_logs = {'val_loss': avg_loss, 'val_acc': acc}\n        return {'avg_val_loss': avg_loss, 'val_acc': acc, 'log' : {'val_loss': avg_loss}}","a2a0614b":"model = litModel(fold_number = 2)  \nmodel.prepare_data()\nearly_stop_callback = pl.callbacks.EarlyStopping(\n  monitor='val_loss',\n  min_delta=0.00,\n  patience=3,\n  verbose=False,\n  mode='min'\n)\n\ncheckpoint_callback = pl.callbacks.ModelCheckpoint(\"fold_2\/{epoch:02d}_{val_loss:.4f}\",\n                                                  save_top_k=1, monitor='val_loss', mode='min')\n\ntrainer = pl.Trainer(max_epochs=40,gpus = 1, early_stop_callback=early_stop_callback, checkpoint_callback=checkpoint_callback, progress_bar_refresh_rate=0)\n\ntrainer.fit(model)","fec8780a":"PATH = glob(\"\/kaggle\/working\/fold_2\/*.ckpt\")[0]","4c91c06d":"m = trainer.model.load_from_checkpoint(checkpoint_path=PATH)\ntest_data = TestDataSet(test_df, numerical_cols, cat_cols)\ntest_loader = torch.utils.data.DataLoader(test_data, BS, False, num_workers= 4,  drop_last=False, pin_memory=False)","e594df9e":"def inference_fn(test_loader, model, device):\n\n    model.eval()\n    preds = []\n    model.to(device)\n    out_df = []\n    for step, obj in enumerate(test_loader):\n\n        cate_x = obj['cat']\n        cont_x = obj['num']\n        uid = obj['uid']\n        cont_x,  cate_x = cont_x.to(device), cate_x.to(device)\n\n        with torch.no_grad():\n            pred = model(cont_x, cate_x)\n\n\n        p = pred.sigmoid().detach().cpu().numpy()\n        preds.append(p)\n        temp = pd.DataFrame(columns=['sig_id'] + target_cols)\n        temp[\"sig_id\"] = uid\n        temp.loc[:, target_cols] = p\n\n        out_df.append(temp)\n    preds = np.concatenate(preds)\n\n    return preds, out_df","56fb1095":"preds, out_df = inference_fn(test_loader, m, 'cuda')\nout_df = pd.concat(out_df)\nout_df.to_csv(\"submission.csv\", index = False)","1288d0ef":"Hello kagglers, \n\nI've trained a baseline using PyTorch Lightning. This kernel has both training as well as inference section, do note this is a baseline and hence I've skipped on ensemble across the folds.\n\nSome part of this kernel is based on wonderful kernel by Y.Nakama which can be found [here](https:\/\/www.kaggle.com\/yasufuminakama\/moa-pytorch-nn-starter).","481bde13":"My first ever kernel. \nPlease upvote :) "}}