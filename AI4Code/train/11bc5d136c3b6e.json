{"cell_type":{"8189029d":"code","300adbcb":"code","aa2540f1":"code","ef05c621":"code","02cab250":"code","504408b0":"code","dd838f91":"code","c1d6a5d6":"code","c05c3f1e":"code","78b0c11a":"code","eed68dad":"code","7fdb6980":"code","bce38ead":"markdown","14f73852":"markdown","c01e6851":"markdown","850e5968":"markdown","ccdc9746":"markdown","d31366e4":"markdown"},"source":{"8189029d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","300adbcb":"# Read the csv files into dataframe using pandas \n\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_data.head()","aa2540f1":"# this time the test_data is being read in another dataframe\n\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv') \ntest_data.head()\n","ef05c621":"# To find the perecentage of women who survived from the train_data\n# This will be used for training our model to predict survival status on test_data\n\nwomen = train_data.loc[train_data.Sex == 'female']['Survived']\nrate_women = sum(women)\/len(women)\n\nprint(\"% of Women who survived:\",(rate_women*100))","02cab250":"men = train_data.loc[train_data.Sex == 'male']['Survived']\nrate_men = sum(men)\/len(men)\n\nprint(\"% of Men Who survived :\", (rate_men*100))","504408b0":"# cleaning dataset is a very essential step because it may skew our final predictions \n# Thus We will check beforehadn whether our test data contains any NaN values\n\ntest_data.isnull() #check for NULL values \n","dd838f91":"# plotiing the Null values with the help of heat map for better understandibility\n\nsns.heatmap(test_data.isnull(), yticklabels = False, cbar = False, cmap = \"winter\")","c1d6a5d6":"# To make any prediction we neeed to find patterns in our data set by plotting in through a swarmplot, \n# You can use any other such as boxplot()\n#Further to make the Visualization even more appealing yet readible I further used violinplot.\n\np_age = sns.swarmplot(x = 'Pclass', y = 'Age', data = test_data, color = '.2', palette = \"Set2\",order = [1,2,3])\np_age = sns.violinplot(x = 'Pclass', y = 'Age',data = test_data,color = 'white', edgecolor = 'gray')\n","c05c3f1e":"# putting the approx age \n\ndef approx_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 41\n        elif Pclass == 2:\n            return 29\n        elif Pclass == 3:\n            return 23\n    else:\n        return Age\n        \ntest_data[\"Age\"] = test_data[['Age','Pclass']].apply(approx_age,axis = 1)\ntest_data\n","78b0c11a":"# Checking for further NULL values after manipulating the column'Age' with the approx values\nsns.heatmap(test_data.isnull(), yticklabels = False, cbar = False, cmap = \"winter\")","eed68dad":"#dropping the column 'Cabin'\ntest_data.drop('Cabin',axis = 1, inplace = True)\ntest_data","7fdb6980":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Embarked\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","bce38ead":"**Note:** *Although I have used the above logic to treat the missing values, there are few more effective and quick ways to deal with missing values or NaN values in Python*.\n\nRefrence: https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/dealing-with-missing-values-in-python-a-complete-guide\/ \n","14f73852":"1. *We can clearly see the column 'Age' and 'Cabin' consists of NULL values*.\n2. *We will be guessing the 'Age' values as there are numerous data to help us predict that*.\n3. *But in turn we will drop the column cabin and in turn use another feature namely* **'Embarked'**.\n","c01e6851":"> *Clearly, by looking the pattern we can see that for Pclass 1 the approx age is around **41**, where as in Pclass 2 it is around **29** and in Pclass 3 it is around* **23**. ","850e5968":"# Hey, Welcome to my first ever notebook in Kaggle and I hope this jounery gets better with each upcoming assignments.\n\n**This is step by step-wise approcah to solve the basic Titanic Survival Predictor Model.**","ccdc9746":"> *Now, We can see there are no more missing values in the column 'Age' but there are still lots of missing\/NaN Values in the column 'Cabin'*.\n\n**Since we wont using the column to train our model we can actually drop the column 'Cabin'.**","d31366e4":"# *This was the most basic and foremost step towards Machine Learning, Hope to have enjoyed the way I broke down the problems into further steps and elaborated each of them*. \n## Not only we trained a model but we also covered few important topic just as Data cleaning, Data manipulating."}}