{"cell_type":{"253c39e1":"code","7e117f09":"code","6ad7b570":"code","0a003e44":"code","0c473a92":"code","3c1e786b":"code","88d56a73":"code","7f606fa5":"code","38ebe149":"code","44b7874f":"code","526aa0f3":"code","4a65bb09":"code","b0a0f6dd":"code","1dc593d3":"code","823490bc":"code","ca21c269":"code","b309de48":"code","b363e7a8":"code","ae74540b":"code","09c1582f":"code","39e09a64":"code","536743cc":"code","57737c7c":"code","5a003deb":"code","9317c84a":"code","2e2e2ae7":"code","aef22141":"code","871cc156":"code","f4e5c71d":"code","e14ef7f7":"code","31f416e0":"code","031436bc":"code","2dfd4f9b":"code","efa44f7a":"code","275f1544":"code","ff442698":"code","807af04f":"code","77136369":"code","f036376c":"code","ac179d5f":"code","a945bfa6":"code","d3cfe82c":"code","2a2e8602":"code","0411cadb":"code","13795a34":"code","0d6bf659":"code","85b719a3":"code","edba3d81":"code","3d1b3130":"code","c12be8c9":"markdown","731f7582":"markdown","4e2ac7d0":"markdown","f2ab6d06":"markdown","6a5c28bb":"markdown","1bce1309":"markdown","0ba1a79d":"markdown","16a29e1e":"markdown","eff42ffe":"markdown","1b8123f0":"markdown","cfefb46d":"markdown","f0fdee1a":"markdown","db346cd2":"markdown","c7cdee09":"markdown","d65fc454":"markdown","727948fa":"markdown","ce9c970f":"markdown","47b7846f":"markdown","07a552ef":"markdown","001c184a":"markdown","ab34c236":"markdown","4e2b8bdc":"markdown","2bd0116f":"markdown","950739c6":"markdown","08fcce33":"markdown","5ea6916f":"markdown","f2a03b61":"markdown","0144a268":"markdown","48be67ca":"markdown","44ba4164":"markdown","e84fd793":"markdown","e200e774":"markdown","1da51695":"markdown","f8de515a":"markdown","4b82024e":"markdown","4d51a35c":"markdown","566c6984":"markdown","2e03657c":"markdown","c94f8a57":"markdown","b8d19454":"markdown","aab7e88b":"markdown","349281f2":"markdown","e8a1aa2d":"markdown","a2a88bb2":"markdown"},"source":{"253c39e1":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, make_scorer, fbeta_score, recall_score, accuracy_score, classification_report\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n","7e117f09":"data = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndata.head()","6ad7b570":"data.describe()","0a003e44":"data.columns","0c473a92":"data.hist(figsize = (20,20))\n\nplt.show()","3c1e786b":"data.columns","88d56a73":"sns.countplot(x = \"DEATH_EVENT\", data=data);\ndata['DEATH_EVENT'].value_counts()","7f606fa5":"plt.figure(figsize=(20,20))\np=sns.heatmap(data.corr(), annot=True,cmap='RdYlGn',square=True)  ","38ebe149":"sns.scatterplot(x=data['ejection_fraction'], y=data['platelets'])","44b7874f":"sns.regplot(x=data['ejection_fraction'], y=data['platelets'])","526aa0f3":"data.columns","4a65bb09":"sns.scatterplot(x=data['age'], y=data['creatinine_phosphokinase'])","b0a0f6dd":"plt.figure(figsize=(20,10))\nsns.countplot(x =\"age\", data=data, hue =\"DEATH_EVENT\");","1dc593d3":"plt.figure(figsize=(5,5))\nsns.countplot(x =\"sex\", data=data, hue =\"DEATH_EVENT\");","823490bc":"data.columns","ca21c269":"plt.figure(figsize=(5,5))\nsns.countplot(x =\"smoking\", data=data, hue =\"DEATH_EVENT\");","b309de48":"X = np.array(data.drop(['DEATH_EVENT'], axis=1))\ny = np.array(data['DEATH_EVENT'])","b363e7a8":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(X_train.shape)\nprint(X_test.shape)","ae74540b":"type(X_train)","09c1582f":"X_train[0]","39e09a64":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","536743cc":"def checkPerformances(classifier, best_clf):\n    # Make predictions using the unoptimized and optimized and model\n    predictions = (classifier.fit(X_train, y_train)).predict(X_test)\n    best_predictions = best_clf.predict(X_test)\n\n\n    # Report the before-and-afterscores\n    print(\"Unoptimized model\\n------\")\n    print(classifier)\n    print(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n    print(\"Recall score on testing data: {:.4f}\".format(recall_score(y_test, predictions)))\n    print(\"\\nOptimized Model\\n------\")\n    print(best_clf)\n    print(\"\\nFinal accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n    print(\"Final recall score on the testing data: {:.4f}\".format(recall_score(y_test, best_predictions)))","57737c7c":"# scorer = make_scorer(recall_score, beta=0.5, average=\"micro\")\nscorer = \"recall\"","5a003deb":"DT_clf = DecisionTreeClassifier()\nDT_parameters = {\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random'],\n    'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'max_features': ['auto', 'sqrt', 'log2']\n    }\n\n# Run the grid search\ngrid_obj = GridSearchCV(DT_clf, DT_parameters, scoring=scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the regressor to the best combination of parameters\nbest_DT_clf = grid_obj.best_estimator_\n\ncheckPerformances(DT_clf, best_DT_clf)","9317c84a":"from sklearn.ensemble import RandomForestClassifier\nRF_clf = RandomForestClassifier(max_depth=None, random_state=None)\n\nparameters = {'n_estimators': [10, 20, 30, 40, 100], \n              'max_features':[3,4,5, None], \n              'max_depth': [4,5,6,7, None], \n              'criterion': ['gini', 'entropy']}\ngrid_obj = GridSearchCV(RF_clf, parameters, scoring=scorer)\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_RF_clf = grid_fit.best_estimator_\n\ncheckPerformances(classifier=RF_clf, best_clf=best_RF_clf)","2e2e2ae7":"SVM_clf = SVC()\nSVM_parameters = {\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n    'degree': [1, 2, 3, 4],\n    'gamma': ['scale', 'auto'],\n    'shrinking' : [True, False]\n    }\n\n# Run the grid search\ngrid_obj = GridSearchCV(SVM_clf, SVM_parameters, scoring='accuracy')\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the regressor to the best combination of parameters\nbest_SVM_clf = grid_obj.best_estimator_\n\ncheckPerformances(SVM_clf, best_SVM_clf)","aef22141":"from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\nimport tensorflow as tf \n\ndef build_model():\n    model=tf.keras.Sequential([\n        Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n\n        # Dense(64, activation='relu'),\n\n        Dense(32, activation='relu'),\n\n        Dense(16, activation='relu'),\n\n        Dense(2,activation='softmax')\n      ])\n\n    return model\n  \nmodel = build_model()\nprint(model.summary())","871cc156":"model.compile(optimizer =\"adam\", loss = \"categorical_crossentropy\", metrics = ['accuracy'])","f4e5c71d":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy', \n    patience=10, \n    min_delta=0.001, \n    mode='max',\n    restore_best_weights=True\n)","e14ef7f7":"from tensorflow.keras.utils import to_categorical\nfrom sklearn.utils import shuffle\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\ny_train[:10]","31f416e0":"y_train.shape","031436bc":"y_test.shape","2dfd4f9b":"history = model.fit(X_train, y_train, \n                    batch_size = 12, \n                    epochs = 100, \n                    verbose = 1, \n                    validation_data = (X_test, y_test),\n                    callbacks=[early_stopping]\n                   )","efa44f7a":"plt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.title(\"Model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc = \"upper left\")\nplt.show()","275f1544":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Model loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc = \"upper left\")\nplt.show()","ff442698":"model.evaluate(X_test,y_test)","807af04f":"y_pred = model.predict(X_test)","77136369":"y_pred_ = [np.argmax(y) for y in y_pred]\ny_test_ = [np.argmax(y) for y in y_test]","f036376c":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\ncm = confusion_matrix(y_test_,y_pred_)\nplot_confusion_matrix(conf_mat = cm,figsize=(8,7),class_names ={\"Died\",\"Survived\"},\n                     show_normed = True)","ac179d5f":"target_names = ['Survived', 'Died']\nprint(classification_report(y_test_, y_pred_,target_names=target_names))","a945bfa6":"sns.countplot(x = \"DEATH_EVENT\", data=data);\ndata['DEATH_EVENT'].value_counts()","d3cfe82c":"smote = SMOTE(random_state=42)\nX, y = smote.fit_resample(X,y)","2a2e8602":"sns.countplot(pd.Series(y))","0411cadb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(X_train.shape)\nprint(X_test.shape)","13795a34":"# Basic random forest algorithm\n\nRF_clf = RandomForestClassifier()\nRF_clf.fit(X_train, y_train)\ny_pred = RF_clf.predict(X_test)","0d6bf659":"print(classification_report(y_test, y_pred,target_names=target_names))","85b719a3":"DT_clf = DecisionTreeClassifier()\nDT_clf.fit(X_train, y_train)\ny_pred = DT_clf.predict(X_test)\nprint(classification_report(y_test, y_pred,target_names=target_names))","edba3d81":"LR_clf = LogisticRegression()\nLR_clf.fit(X_train, y_train)\ny_pred = LR_clf.predict(X_test)\nprint(classification_report(y_test, y_pred,target_names=target_names))","3d1b3130":"KNN_clf = KNeighborsClassifier()\nKNN_clf.fit(X_train, y_train)\ny_pred = KNN_clf.predict(X_test)\nprint(classification_report(y_test, y_pred,target_names=target_names))","c12be8c9":"# Grid search","731f7582":"\"Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\" (Wikipedia, 2021)","4e2ac7d0":"There is not really a strong correlation between the variables (positive or negative).","f2ab6d06":"# Neural network","6a5c28bb":"# Feature scaling","1bce1309":"\"**SMOTE** stands for **Synthetic Minority Oversampling Technique**. This is a statistical technique for increasing the number of cases in your dataset in a balanced way. The module works by generating new instances from existing minority cases that you supply as input.\" (Microsoft, 2021)  \nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/smote","0ba1a79d":"## Check performances","16a29e1e":"![nicola-fioravanti-W5OwUWIvMa8-unsplash.jpg](attachment:311ecb35-3e18-491a-a529-25cd4f13c5a7.jpg)","eff42ffe":"This function takes two models as input, makes predictions and displays the performance of each of the two models.","1b8123f0":"**We will now retry some algorithms on the new data.**","cfefb46d":"We will use the SMOTE technique to create synthetic examples for the target variable in smaller numbers (death cases).","f0fdee1a":"## Death cases by sex","db346cd2":"### Split data","c7cdee09":"A grid search is performed to find the best parameter sets for the algorithms to use.","d65fc454":"Let's take a closer look at the correlation between some variables","727948fa":"So what we want above all is a high **recall**: the ability for the model to detect cases leading to the death of the patient. ","ce9c970f":"## Convert labels to categorical","47b7846f":"# Load data","07a552ef":"# Heart failure prediction","001c184a":"# Diving into the data","ab34c236":"## Logistic Regression","4e2b8bdc":"As a reminder, here is the distribution of the target variable.","2bd0116f":"We'll be using the *StandardScaler* from *sklearn*.","950739c6":"We will try to solve this problem later. For now, some visualizations.","08fcce33":"Credit photo: @nicolafioravanti from Unsplash","5ea6916f":"90%+ accuracy and 95%+ recall on death events! This is excellent. Better than what we had before. ","f2a03b61":"## Correlation matrix between variables","0144a268":"## KNN","48be67ca":"## Callbacks for the training","44ba4164":"Voil\u00e0!","e84fd793":"## Model training","e200e774":"## Some visualizations","1da51695":"We have features that take quite small values (serum_creatinine for example) and on the contrary, others that take very large values (platelets for example). In such a situation it is necessary to apply a feature scaling. Otherwise, we would not get the information contained in the features that take rather low values.","f8de515a":"# Dealing with the imbalanced dataset","4b82024e":"## Death cases for smokers and non-smokers","4d51a35c":"#### Distribution of the target variable.","566c6984":"The dataset is unbalanced. There are many more survival cases than deaths. This is problematic for training the model. Basically, it means that the model will be able to recognize survival cases more easily than death cases. Ultimately the goal here is to save lives. We prefer to detect the cases that can lead to the death of the patient as accurately as possible. ","2e03657c":"## Decision tree","c94f8a57":"# Split dataset","b8d19454":"We will now try to get better results by making the dataset balanced.","aab7e88b":"## Random Forest","349281f2":"### Results","e8a1aa2d":"## Cases of death by age","a2a88bb2":"## Utility function"}}