{"cell_type":{"fd6a6dfa":"code","af855b1c":"code","c7d45a1c":"code","66a7f212":"code","184dd179":"code","173f5788":"code","3f57c738":"code","67906023":"code","e52264b3":"markdown","d471ced0":"markdown","cf16b4fc":"markdown","a6df2630":"markdown","60a70574":"markdown"},"source":{"fd6a6dfa":"!wget https:\/\/www.cs.toronto.edu\/~kriz\/cifar-100-python.tar.gz","af855b1c":" !tar -xf cifar-100-python.tar.gz","c7d45a1c":"import numpy as np\nimport pandas as pd\n\ndef unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n      dict = pickle.load(fo, encoding='bytes')\n    return dict\n\ntrain_data = np.array(unpickle(\"cifar-100-python\/train\")[b'data'])\ntest_data = np.array(unpickle(\"cifar-100-python\/test\")[b'data'])\n\ntrain_label = np.array(unpickle(\"cifar-100-python\/train\")[b'fine_labels'])\ntest_label = np.array(unpickle(\"cifar-100-python\/test\")[b'fine_labels'])\n\nlabels_names = unpickle(\"cifar-100-python\/meta\")[b'fine_label_names']","66a7f212":"train_data_mM = train_data \/ 255 #min-max\ntest_data_mM = test_data \/ 255","184dd179":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA()\npca.fit(train_data_mM)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\n\n#plot the explained variance vs number of dimensions\n\nplt.figure(figsize = (6,4))\nplt.plot(cumsum, linewidth=3)\nplt.axis([0, 45, 0, 1])\nplt.xlabel(\"dimensions\")\nplt.ylabel(\"explained variance\")\nplt.grid(True)\nplt.show()\n\n#choosing 25 dimensions: n_components = 0.80\npca = PCA(n_components = 0.80)\npca.fit(train_data_mM)\n\npca = PCA(n_components = 0.95)\npca.fit(train_data_mM)\n\n#dimension\ntrain_data_pca = pca.transform(train_data_mM)\ntest_data_pca = pca.transform(test_data_mM)","173f5788":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nX_train, X_test, y_train, y_test = train_test_split(train_data_pca, train_label, stratify=train_label, random_state=42)\n\nprint(\"Size of train and train label set: {}, {}.\".format(X_train.shape,y_train.shape))\nprint(\"Size of test and test label set: {}, {}.\".format(X_test.shape,y_test.shape))\n\n# training model\nlin_svm=SVC(kernel=\"linear\",decision_function_shape=\"ovr\")\npoly_svm=SVC(kernel=\"poly\",degree=2,decision_function_shape=\"ovr\")\n\nlin_svm.fit(X_train,y_train)\ny_pred_lin=lin_svm.predict(X_test)\n\npoly_svm.fit(X_train,y_train)\ny_pred_poly=poly_svm.predict(X_test)\n\nprint(\"Accuracy on test set using linear svm: {:.3f}\".format(accuracy_score(y_test,y_pred_lin)))\nprint(\"Accuracy on test set quadratic svm: {:.3f}\".format(accuracy_score(y_test,y_pred_poly)))","3f57c738":"### 10-fold to find the best parameter for SVM\nparam_grid={'degree':[2,3,4]}\nprint(\"Parameter grid:\\n{}\".format(param_grid))\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nsvm = SVC(kernel= 'poly')\ngrid_search = GridSearchCV(svm, param_grid, cv=10, \n                           return_train_score=True, n_jobs = -1)\ngrid_search.fit(train_data_pca, train_label)\n\nprint(\"Test\u00a0set\u00a0score:\u00a0{:.2f}\".format(grid_search.score(test_data_pca,test_label)))\nprint(\"Best\u00a0parameters:\u00a0{}\".format(grid_search.best_params_))\nprint(\"Best\u00a0cross-validation\u00a0score:\u00a0{:.2f}\".format(grid_search.best_score_))\nprint(\"Best\u00a0estimator:\\n{}\".format(grid_search.best_estimator_))\n\nresultSVM = grid_search.cv_results_\nresult_pretty = pd.DataFrame(resultSVM)\nprint(result_pretty.params)\nprint(result_pretty.mean_test_score)","67906023":"### Using hyper parameter to determine accuracy\n\npoly_svm = SVC(kernel=\"poly\",degree=2)#polynomial kernel with degree 2\npoly_svm.fit(train_data_pca, train_label)\nlabel_pred_poly = poly_svm.predict(test_data_pca)\nprint(\"Accuracy on test set: {:.3f}\".format(accuracy_score(test_label, label_pred_poly)))","e52264b3":"Reading data","d471ced0":"model 2 KNN","cf16b4fc":"model 1 SVM","a6df2630":"pre-processing","60a70574":"Models"}}