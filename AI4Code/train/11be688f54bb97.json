{"cell_type":{"19bcfd18":"code","9a995b7f":"code","cac252ad":"code","28018d98":"code","d1b75e44":"code","0b227d35":"code","b9a26410":"code","cbb5db98":"code","a8fcdc9e":"code","a14ec85b":"code","c1a3f020":"code","8205a3bc":"code","52ab6c30":"code","f97563ce":"code","b94657da":"code","50c2e523":"code","06df1286":"code","bbf51a0b":"code","d64aa1fd":"code","07d5226f":"code","9ad5726e":"code","f83b7685":"code","f6021fad":"code","a2babcdf":"code","05747a47":"code","877c6ba5":"code","e060ef77":"markdown","e81b2c59":"markdown","3c886449":"markdown","3f0298a7":"markdown"},"source":{"19bcfd18":"import operator as opt\nimport numpy as np \nimport pandas as pd \nimport os\nimport gc\nfrom contextlib import contextmanager\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport matplotlib\nwarnings.filterwarnings('ignore')","9a995b7f":"## DONE: Create files dictionary for any file in the input directory\nfiles = {}\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        files[filename] = os.path.join(dirname, filename)\n        print(os.path.join(dirname, filename))","cac252ad":"## DONE: Loading files in pandas dataframe [1.1]\ndef load_file_into_dataframe(file_name):\n    df = pd.read_csv(files[file_name])\n    return df","28018d98":"cr_loan = load_file_into_dataframe('cr_loan2.csv')\ncr_loan_clean = load_file_into_dataframe('cr_loan_nout_nmiss.csv')\ncr_loan_prep = load_file_into_dataframe('cr_loan_w2.csv')","d1b75e44":"### Explore the credit data\n\n# Check the structure of the data\nprint(cr_loan.dtypes)\n\n# Check the first five rows of the data\nprint(cr_loan.head())\n\n# Look at the distribution of loan amounts with a histogram\nn, bins, patches = plt.hist(x=cr_loan['loan_amnt'], bins='auto', color='blue',alpha=0.7, rwidth=0.85)\nplt.xlabel(\"Loan Amount\")\nplt.show()\n\nprint(\"There are 32 000 rows of data so the scatter plot may take a little while to plot.\")\n\n# Plot a scatter plot of income against age\nplt.scatter(cr_loan['person_income'], cr_loan['person_age'],c='blue', alpha=0.5)\nplt.xlabel('Personal Income')\nplt.ylabel('Persone Age')\nplt.show()","0b227d35":"### Crosstab and pivot tables\n\n# Create a cross table of the loan intent and loan status\nprint(pd.crosstab(cr_loan['loan_intent'], cr_loan['loan_status'], margins = True))\n\n# Create a cross table of home ownership, loan status, and grade\nprint(pd.crosstab(cr_loan['person_home_ownership'],[cr_loan['loan_status'],cr_loan['loan_grade']]))\n\n# Create a cross table of home ownership, loan status, and average percent income\nprint(pd.crosstab(cr_loan['person_home_ownership'], cr_loan['loan_status'],\n              values=cr_loan['loan_percent_income'], aggfunc='mean'))\n\n# Create a box plot of percentage income by loan status\ncr_loan.boxplot(column = ['loan_percent_income'], by = 'loan_status')\nplt.title('Average Percent Income by Loan Status')\nplt.suptitle('')\nplt.show()","b9a26410":"### Finding outliers with cross tables\n\n# Create the cross table for loan status, home ownership, and the max employment length\nprint(pd.crosstab(cr_loan['loan_status'],cr_loan['person_home_ownership'],\n        values=cr_loan['person_emp_length'], aggfunc='max'))\n\n# Create the cross table for loan status, home ownership, and the max employment length\nprint(pd.crosstab(cr_loan['loan_status'],cr_loan['person_home_ownership'],\n                  values=cr_loan['person_emp_length'], aggfunc='max'))\n\n# Create an array of indices where employment length is greater than 60\nindices = cr_loan[cr_loan['person_emp_length'] > 60].index\n\n# Create the cross table for loan status, home ownership, and the max employment length\nprint(pd.crosstab(cr_loan['loan_status'],cr_loan['person_home_ownership'],\n                  values=cr_loan['person_emp_length'], aggfunc='max'))\n\n# Create an array of indices where employment length is greater than 60\nindices = cr_loan[cr_loan['person_emp_length'] > 60].index\n\n# Drop the records from the data based on the indices and create a new dataframe\ncr_loan_new = cr_loan.drop(indices)\n\n# Create the cross table for loan status, home ownership, and the max employment length\nprint(pd.crosstab(cr_loan['loan_status'],cr_loan['person_home_ownership'],\n                  values=cr_loan['person_emp_length'], aggfunc='max'))\n\n# Create an array of indices where employment length is greater than 60\nindices = cr_loan[cr_loan['person_emp_length'] > 60].index\n\n# Drop the records from the data based on the indices and create a new dataframe\ncr_loan_new = cr_loan.drop(indices)\n\n# Create the cross table from earlier and include minimum employment length\nprint(pd.crosstab(cr_loan_new['loan_status'],cr_loan_new['person_home_ownership'],\n            values=cr_loan_new['person_emp_length'], aggfunc=['min','max']))","cbb5db98":"### Visualizing credit outliers\n\n# Create the scatter plot for age and amount\nplt.scatter(cr_loan['person_age'], cr_loan['loan_amnt'], c='blue', alpha=0.5)\nplt.xlabel(\"Person Age\")\nplt.ylabel(\"Loan Amount\")\nplt.show()\n\n# Use Pandas to drop the record from the data frame and create a new one\ncr_loan_new = cr_loan.drop(cr_loan[cr_loan['person_age'] > 100].index)\n\n# Create a scatter plot of age and interest rate\ncolors = [\"blue\",\"red\"]\nplt.scatter(cr_loan_new['person_age'], cr_loan_new['loan_int_rate'],\n            c = cr_loan_new['loan_status'],\n            cmap = matplotlib.colors.ListedColormap(colors),\n            alpha=0.5)\nplt.xlabel(\"Person Age\")\nplt.ylabel(\"Loan Interest Rate\")\nplt.show()","a8fcdc9e":"### Replacing missing credit data\n\n# Print a null value column array\nprint(cr_loan.columns[cr_loan.isnull().any()])\n\n# Print the top five rows with nulls for employment length\nprint(cr_loan[cr_loan['person_emp_length'].isnull()].head())\n\n# Impute the null values with the median value for all employment lengths\ncr_loan['person_emp_length'].fillna((cr_loan['person_emp_length'].median()), inplace=True)\n\n# Create a histogram of employment length\nn, bins, patches = plt.hist(cr_loan['person_emp_length'], bins='auto', color='blue')\nplt.xlabel(\"Person Employment Length\")\nplt.show()","a14ec85b":"### Removing missing data\n\n# Print the number of nulls\nprint(cr_loan['loan_int_rate'].isnull().sum())\n\n# Store the array on indices\nindices = cr_loan[cr_loan['loan_int_rate'].isnull()].index\n\n# Save the new data without missing data\ncr_loan_clean = cr_loan.drop(indices)","c1a3f020":"### Missing data intuition","8205a3bc":"### Logistic regression basics\nfrom sklearn.linear_model import LogisticRegression\n# Create the X and y data sets\nX = cr_loan_clean[['loan_int_rate']]\ny = cr_loan_clean[['loan_status']]\n\n# Create and fit a logistic regression model\nclf_logistic_single = LogisticRegression()\nclf_logistic_single.fit(X, np.ravel(y))\n\n# Print the parameters of the model\nprint(clf_logistic_single.get_params())\n\n# Print the intercept of the model\nprint(clf_logistic_single.intercept_)","52ab6c30":"### Multivariate logistic regression\n\n# Create X data for the model\nX_multi = cr_loan_clean[['loan_int_rate','person_emp_length']]\n\n# Create a set of y data for training\ny = y[['loan_status']]\n\n# Create and train a new logistic regression\nclf_logistic_multi = LogisticRegression(solver='lbfgs').fit(X_multi, np.ravel(y))\n\n# Print the intercept of the model\nprint(clf_logistic_multi.intercept_)","f97563ce":"### Creating training and test sets\nfrom sklearn.model_selection import train_test_split\n# Create the X and y data sets\nX = cr_loan_clean[['loan_int_rate','person_emp_length','person_income']]\ny = cr_loan_clean[['loan_status']]\n\n# Use test_train_split to create the training and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n\n# Create and fit the logistic regression model\nclf_logistic = LogisticRegression(solver='lbfgs').fit(X_train, np.ravel(y_train))\n\n# Print the models coefficients\nprint(clf_logistic.coef_)","b94657da":"### Changing coefficients\nX1_train = cr_loan_clean[['person_income', 'person_emp_length', 'loan_amnt']]\nX2_train = cr_loan_clean[['person_income', 'loan_percent_income', 'cb_person_cred_hist_length']]\ny_train = cr_loan_clean[['loan_status']]\n# Print the first five rows of each training set\nprint(X1_train.head())\nprint(X2_train.head())\n\n# Create and train a model on the first training data\nclf_logistic1 = LogisticRegression(solver='lbfgs').fit(X1_train, np.ravel(y_train))\n\n# Create and train a model on the second training data\nclf_logistic2 = LogisticRegression(solver='lbfgs').fit(X2_train, np.ravel(y_train))\n\n# Print the coefficients of each model\nprint(clf_logistic1.coef_)\nprint(clf_logistic2.coef_)","50c2e523":"### One-hot encoding credit data\n\n# Create two data sets for numeric and non-numeric data\ncred_num = cr_loan_clean.select_dtypes(exclude=['object'])\ncred_str = cr_loan_clean.select_dtypes(include=['object'])\n\n# One-hot encode the non-numeric columns\ncred_str_onehot = pd.get_dummies(cred_str)\n\n# Union the one-hot encoded columns to the numeric ones\ncr_loan_prep = pd.concat([cred_num, cred_str_onehot], axis=1)\n\n# Print the columns in the new data set\nprint(cr_loan_prep.columns)","06df1286":"### Predicting probability of default\n\nX = cr_loan_prep[['loan_int_rate','person_emp_length','person_income']]\ny = cr_loan_prep[['loan_status']]\n\n# Use test_train_split to create the training and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n\n# Train the logistic regression model on the training data\nclf_logistic = LogisticRegression(solver='lbfgs').fit(X_train, np.ravel(y_train))\n\n# Create predictions of probability for loan status using test data\npreds = clf_logistic.predict_proba(X_test)\n\n# Create dataframes of first five predictions, and first five true labels\npreds_df = pd.DataFrame(preds[:,1][0:5], columns = ['prob_default'])\ntrue_df = y_test.head()\n\n# Concatenate and print the two data frames for comparison\nprint(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1))","bbf51a0b":"### Default classification reporting\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\n\n# Create a dataframe for the probabilities of default\npreds_df = pd.DataFrame(preds[:,1], columns = ['prob_default'])\n\n# Reassign loan status based on the threshold\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.50 else 0)\n\n# Print the row counts for each loan status\nprint(preds_df['loan_status'].value_counts())\n\n# Print the classification report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, preds_df['loan_status'], target_names=target_names))","d64aa1fd":"### Selecting report metrics\n\n# Print the classification report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, preds_df['loan_status'], target_names=target_names))\n\n# Print all the non-average values from the report\nprint(precision_recall_fscore_support(y_test,preds_df['loan_status']))\n\n# Print the first two numbers from the report\nprint(precision_recall_fscore_support(y_test,preds_df['loan_status'])[:2])","07d5226f":"### Visually scoring credit models\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Create predictions and store them in a variable\npreds = clf_logistic.predict_proba(X_test)\n\n# Print the accuracy score the model\nprint(clf_logistic.score(X_test, y_test))\n\n# Plot the ROC curve of the probabilities of default\nprob_default = preds[:, 1]\nfallout, sensitivity, thresholds = roc_curve(y_test, prob_default)\nplt.plot(fallout, sensitivity, color = 'darkorange')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.show()\n\n# Compute the AUC and store it in a variable\nauc = roc_auc_score(y_test, prob_default)","9ad5726e":"### Thresholds and confusion matrices.\nfrom sklearn.metrics import confusion_matrix\n# Set the threshold for defaults to 0.5\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.5 else 0)\n\n# Print the confusion matrix\nprint(confusion_matrix(y_test,preds_df['loan_status']))\n\n# Set the threshold for defaults to 0.4\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.4 else 0)\n\n# Print the confusion matrix\nprint(confusion_matrix(y_test,preds_df['loan_status']))\n\n","f83b7685":"cr_loan_prep.columns","f6021fad":"### How thresholds affect performance\navg_loan_amnt = cr_loan_prep['loan_amnt'].sum() \/ cr_loan_prep['loan_amnt'].count()\n# Reassign the values of loan status based on the new threshold\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.4 else 0)\n\n# Store the number of loan defaults from the prediction data\nnum_defaults = preds_df['loan_status'].value_counts()[1]\n\n# Store the default recall from the classification report\ndefault_recall = precision_recall_fscore_support(y_test,preds_df['loan_status'])[1][1]\n\n# Calculate the estimated impact of the new default recall rate\nprint(num_defaults * avg_loan_amnt * (1 - default_recall))","a2babcdf":"### Threshold selection\nthresh = [0.2,0.225,0.25,0.275,0.3,0.325,0.35,0.375,0.4,0.425,0.45, 0.475,0.5,0.525,0.55,0.575,0.6,0.625,00.65]\ndef_recalls = [0.7981438515081206,\n 0.7583139984532096,\n 0.7157772621809745,\n 0.6759474091260634,\n 0.6349574632637278,\n 0.594354215003867,\n 0.5467904098994586,\n 0.5054137664346481,\n 0.46403712296983757,\n 0.39984532095901004,\n 0.32211910286156226,\n 0.2354988399071926,\n 0.16782675947409126,\n 0.1148491879350348,\n 0.07733952049497293,\n 0.05529775715390565,\n 0.03750966744006187,\n 0.026295436968290797,\n 0.017788089713843776]\nnondef_recalls = [0.5342465753424658,\n 0.5973037616873234,\n 0.6552511415525114,\n 0.708306153511633,\n 0.756468797564688,\n 0.8052837573385518,\n 0.8482278756251359,\n 0.8864970645792564,\n 0.9215046749293324,\n 0.9492280930637095,\n 0.9646662317895195,\n 0.9733637747336378,\n 0.9809741248097412,\n 0.9857577734290063,\n 0.9902152641878669,\n 0.992280930637095,\n 0.9948901935203305,\n 0.9966297021091541,\n 0.997499456403566]\naccs =[0.5921588594704684,\n 0.6326374745417516,\n 0.6685336048879837,\n 0.7012050237610319,\n 0.7298031228784793,\n 0.7589952477936185,\n 0.7820773930753564,\n 0.8028682959945689,\n 0.8211133740665308,\n 0.8286659877800407,\n 0.8236591989137814,\n 0.811439239646979,\n 0.8025288526816021,\n 0.7946367956551256,\n 0.7898845892735913,\n 0.7866598778004074,\n 0.7847929395790902,\n 0.7836897488119484,\n 0.7825016972165648]\nticks = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65]\nplt.plot(thresh,def_recalls)\nplt.plot(thresh,nondef_recalls)\nplt.plot(thresh,accs)\nplt.xlabel(\"Probability Threshold\")\nplt.xticks(ticks)\nplt.legend([\"Default Recall\",\"Non-default Recall\",\"Model Accuracy\"])\nplt.show()","05747a47":"### Trees for defaults\n\n# Train a model\nimport xgboost as xgb\nclf_gbt = xgb.XGBClassifier().fit(X_train, np.ravel(y_train))\n\n# Predict with a model\ngbt_preds = clf_gbt.predict_proba(X_test)\n\n# Create dataframes of first five predictions, and first five true labels\npreds_df = pd.DataFrame(gbt_preds[:,1][0:5], columns = ['prob_default'])\ntrue_df = y_test.head()\n\n# Concatenate and print the two data frames for comparison\nprint(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1))","877c6ba5":"### Gradient boosted portfolio performance\n\n# Print the first five rows of the portfolio data frame\nprint(portfolio.head())\n\n# Create expected loss columns for each model using the formula\nportfolio['gbt_expected_loss'] = portfolio['gbt_prob_default'] * portfolio['lgd'] * portfolio['loan_amnt']\nportfolio['lr_expected_loss'] = portfolio['lr_prob_default'] * portfolio['lgd'] * portfolio['loan_amnt']\n\n# Print the sum of the expected loss for lr\nprint('LR expected loss: ', np.sum(portfolio['lr_expected_loss']))\n\n# Print the sum of the expected loss for gbt\nprint('GBT expected loss: ', np.sum(portfolio['gbt_expected_loss']))","e060ef77":"## Exploring and preparing data end","e81b2c59":"## Logistic Regression for Defaults start\n","3c886449":"## Logistic Regression for Defaults end","3f0298a7":"## Exploring and preparing data start"}}