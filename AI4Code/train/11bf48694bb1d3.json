{"cell_type":{"f8969b09":"code","cecc0cfe":"code","ebfc3015":"code","cca383fb":"code","ab8f48c2":"code","7c2792d5":"code","4c586a1c":"code","ecfa40e5":"code","2478d2ba":"code","ce74a293":"code","c3dfacd3":"code","3d704deb":"code","89f34852":"code","19f26973":"code","cdf3988c":"code","14f58d43":"code","d5f533b8":"code","ceab8190":"code","5e389c43":"code","17f7649a":"code","1805af2d":"code","3d855d7e":"code","06bdff11":"code","75e1fab2":"code","729dd317":"code","aae5bd4a":"code","ea247c8b":"code","1c3f4656":"code","8a55dc25":"code","f767c670":"code","51c06d19":"code","757f4f7b":"code","f2e276a2":"code","0269e754":"code","e5c56015":"code","cb8af2ce":"code","84cafe7f":"markdown","45a3a0f0":"markdown","4a60ddff":"markdown","240adc40":"markdown","7072ec15":"markdown","9152b816":"markdown","fefecea6":"markdown","33b49073":"markdown","982b6d53":"markdown","deadb586":"markdown","f7e4fa1a":"markdown","9d88a9d4":"markdown","083364d0":"markdown","421fb2fa":"markdown","f51491a4":"markdown","1959a11f":"markdown","3b2eebea":"markdown","b91227b2":"markdown","3aa64d5a":"markdown","836d5b2b":"markdown","02f34a20":"markdown","926be90a":"markdown","81442013":"markdown","32d6b710":"markdown","87b00280":"markdown"},"source":{"f8969b09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cecc0cfe":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import tree\nimport graphviz\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor","ebfc3015":"# load and display dataset\ncarseats_df = pd.read_csv('..\/input\/carseats\/Carseats.csv')\n\ncarseats_df.head()","cca383fb":"carseats_df.info()","ab8f48c2":"#Null value check\ncarseats_df.isnull().sum()","7c2792d5":"carseats_df['Education'] = carseats_df['Education'].astype(str)","4c586a1c":"carseats_df.info()","ecfa40e5":"# Visulazing the distibution of the data for every feature\ncarseats_df.hist(edgecolor='black', linewidth=1.2, figsize=(20, 20))","2478d2ba":"# Target Sales : Skewness check\ncarseats_df['Sales'].skew()","ce74a293":"# Categorical features : Value check\ncat_cols = ['Education', 'ShelveLoc', 'Urban', 'US']\nfor col in cat_cols:\n    print(f\"Feature Name {col} : \\n{carseats_df[col].value_counts()}\")","c3dfacd3":"#Education\nsns.boxplot(x=\"Education\", y=\"Sales\", data=carseats_df)","3d704deb":"#ShelveLoc\nsns.boxplot(x=\"ShelveLoc\", y=\"Sales\", data=carseats_df)\nplt.figure(figsize=(10,7), dpi= 80)\nsns.distplot(carseats_df['Sales'].loc[carseats_df['ShelveLoc'] == 'Medium'], color=\"dodgerblue\", label=\"Medium\")\nsns.distplot(carseats_df['Sales'].loc[carseats_df['ShelveLoc'] == 'Good'], color=\"orange\", label=\"Good\")\nsns.distplot(carseats_df['Sales'].loc[carseats_df['ShelveLoc'] == 'Bad'], color=\"deeppink\", label=\"Bad\")\nplt.legend()","89f34852":"#Urban\nsns.boxplot(x=\"Urban\", y=\"Sales\", data=carseats_df)\nplt.figure(figsize=(10,7), dpi= 80)\nsns.distplot(carseats_df['Sales'].loc[carseats_df['Urban'] == 'Yes'], color=\"dodgerblue\", label=\"Yes\")\nsns.distplot(carseats_df['Sales'].loc[carseats_df['Urban'] == 'No'], color=\"orange\", label=\"No\")\nplt.legend()","19f26973":"#US\nsns.boxplot(x=\"US\", y=\"Sales\", data=carseats_df)\nplt.figure(figsize=(10,7), dpi= 80)\nsns.distplot(carseats_df['Sales'].loc[carseats_df['US'] == 'Yes'], color=\"dodgerblue\", label=\"Yes\")\nsns.distplot(carseats_df['Sales'].loc[carseats_df['US'] == 'No'], color=\"orange\", label=\"No\")\nplt.show()","cdf3988c":"#find correlation between continuous variables and Sales\nplt.figure(figsize=(30, 30))\nsns.heatmap(carseats_df.corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":15})","14f58d43":"# Visulazing the distibution of the data for every feature with Sales\ncont_cols = ['CompPrice', 'Income', 'Advertising', 'Population', 'Price',\n       'Age']\nfor col in cont_cols:\n    plt.figure(figsize=(10,10), dpi= 80)\n    plt.scatter(x=col, y='Sales', data=carseats_df)\n    plt.xlabel(f\"{col}\")\n    plt.ylabel('Sales')\n    plt.show()","d5f533b8":"carseats_df['PriceDiff'] = carseats_df['Price'] - carseats_df['CompPrice']\ncorr_df = carseats_df[['Sales', 'PriceDiff']]\ncorr_df.corr()","ceab8190":"plt.figure(figsize=(10,10), dpi= 80)\nplt.scatter(x='PriceDiff', y='Sales', data=carseats_df)\nplt.xlabel('Price Difference')\nplt.ylabel('Sales')\nplt.show()","5e389c43":"# advertising size rate(%) per population\ncarseats_df['Advt_per_popln_pct'] = carseats_df['Advertising']\/carseats_df['Population']*100\ncorr_df = carseats_df[['Sales', 'Advt_per_popln_pct', 'Advertising','Population']]\ncorr_df.corr()","17f7649a":"#Get Dummies\nX = carseats_df.drop(['Sales', 'Price', 'CompPrice', 'Advt_per_popln_pct'], axis=1)\ny = carseats_df['Sales']\n\nX = pd.get_dummies(X)\nX.head()","1805af2d":"#3.1.1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('Train features shape: {}'.format(X_train.shape))\nprint('Train labels shape: {}'.format(y_train.shape))\nprint('Test features shape: {}'.format(X_test.shape))\nprint('Test labels shape: {}'.format(y_test.shape))\n","3d855d7e":"#3.1.2 Decision Tree\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\nreg_dt= DecisionTreeRegressor(min_samples_leaf=2, min_samples_split=2, max_depth=3, random_state=42)\nreg_dt.fit(X_train, y_train)\n\npred = reg_dt.predict(X_test)\nMSE = mean_squared_error(y_test, pred)\nRMSE = np.sqrt(MSE)\n\nprint(f\"MSE : {MSE}, RMSE : {RMSE}\")","06bdff11":"#3.1.3 Tree Visualization\nfrom IPython.display import Image\nfrom six import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot\n\nfeatures = list(X.columns)","75e1fab2":"dot_data = StringIO()\nexport_graphviz(reg_dt, out_file=dot_data, feature_names=features, filled=True)\ngraph = pydot.graph_from_dot_data(dot_data.getvalue())\nImage(graph[0].create_png())","729dd317":"#Variance Important Plot\n\nplot_df = pd.DataFrame({'feature':X_train.columns, 'importance': reg_dt.feature_importances_})\n\nplt.figure(figsize=(10,10))\nsns.barplot(x='importance', y='feature', data=plot_df.sort_values('importance', ascending=False), color='b')\nplt.xticks(rotation=90);","aae5bd4a":"#X_train = X_train[['PriceDiff', 'ShelveLoc_Good', 'Age']]\n#X_test = X_test[['PriceDiff', 'ShelveLoc_Good', 'Age']]","ea247c8b":"#3.1.4 Hyperparameter Tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\nreg_dt = DecisionTreeRegressor(random_state=42)\nparam_grid = [{'max_depth':[i for i in range (1, 31)]}]\n               \nreg_cv = GridSearchCV(reg_dt, param_grid, cv=5, scoring='neg_mean_squared_error')\nreg_cv.fit(X_train, y_train)","1c3f4656":"# predict, RMSE\nreg_cv.predict(X_test)\ncv_result = pd.DataFrame(reg_cv.cv_results_)\n\nplt.figure(figsize=(10,7))\nfor row in cv_result.itertuples():\n    #print(f\"max_depth : {row[5]}, RMSE : {np.sqrt(-row[12])}\")\n    plt.scatter(x=row[5], y=np.sqrt(-row[12]))\n\nplt.xlabel(\"max depth\")\nplt.ylabel(\"RMSE\")\nplt.show()","8a55dc25":"print(f\"Best Max Depth : {reg_cv.best_estimator_}\")","f767c670":"# Hyperparameter tuning :  bagging  - tree size\n\nmax_features = X_train.shape[1]\nreg_rf = RandomForestRegressor(max_features=max_features, max_depth=7, random_state=42)\n\nparam_grid = [{'n_estimators':[i for i in range (50, 550, 50)]}]\n               \nreg_cv = GridSearchCV(reg_rf, param_grid, cv=5, scoring='neg_mean_squared_error')\nreg_cv.fit(X_train, y_train)\n\n# predict, RMSE\nreg_cv.predict(X_test)\ncv_result = pd.DataFrame(reg_cv.cv_results_)\n\nplt.figure(figsize=(10,7))\nfor row in cv_result.itertuples():\n    plt.scatter(x=row[5], y=np.sqrt(-row[12]))\n\nplt.xlabel(\"number of trees(n_estimators)\")\nplt.ylabel(\"RMSE\")\nplt.show()","51c06d19":"reg_rf = RandomForestRegressor(max_features=max_features, max_depth=7, n_estimators=350, random_state=42)\nreg_rf.fit(X_train, y_train)\ny_pred =reg_rf.predict(X_test)\n\nMSE = metrics.mean_squared_error(y_test, y_pred)\nRMSE = np.sqrt(MSE)\n\nprint(f'Test MSE : {np.around(MSE, 3)}')\nprint(f'Test RMSE: {np.around(RMSE, 3)}')","757f4f7b":"from sklearn.ensemble import BaggingRegressor\n\n\nbag_clf = BaggingRegressor(DecisionTreeRegressor(random_state=42, max_depth=7), n_estimators=350, \n                            max_samples=200, bootstrap=True, n_jobs=-1, random_state=42)\nbag_clf.fit(X_train, y_train)\ny_pred = bag_clf.predict(X_test)\n\ny_pred =bag_clf.predict(X_test)\n\nMSE = metrics.mean_squared_error(y_test, y_pred)\nRMSE = np.sqrt(MSE)\n\nprint(f'Test MSE : {np.around(MSE, 3)}')\nprint(f'Test RMSE: {np.around(RMSE, 3)}')","f2e276a2":"#2. AdaBoost\n\nfrom sklearn.ensemble import AdaBoostRegressor\n\nada_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=3), learning_rate=0.5, \n                            n_estimators=350, random_state=42)\n\nada_reg.fit(X_train, y_train)\n\ny_pred = ada_reg.predict(X_test)\n\nMSE = metrics.mean_squared_error(y_test, y_pred)\nRMSE = np.sqrt(MSE)\n\nprint(f'Test MSE : {np.around(MSE, 3)}')\nprint(f'Test RMSE: {np.around(RMSE, 3)}')","0269e754":"from sklearn.ensemble import GradientBoostingRegressor\n\ngb_reg = GradientBoostingRegressor(max_depth=3, n_estimators=350, learning_rate=0.5, random_state=42)\ngb_reg.fit(X_train, y_train)\n\n\ny_pred = gb_reg.predict(X_test)\n\nMSE = metrics.mean_squared_error(y_test, y_pred)\nRMSE = np.sqrt(MSE)\n\nprint(f'Test MSE : {np.around(MSE, 3)}')\nprint(f'Test RMSE: {np.around(RMSE, 3)}')","e5c56015":"import xgboost as xgb\n\nxgb_reg = xgb.XGBRegressor(max_depth=3, n_estimators=350, learning_rate=0.5, random_state=42)\nxgb_reg.fit(X_train, y_train)\n\n\ny_pred = xgb_reg.predict(X_test)\n\nMSE = metrics.mean_squared_error(y_test, y_pred)\nRMSE = np.sqrt(MSE)\n\nprint(f'Test MSE : {np.around(MSE, 3)}')\nprint(f'Test RMSE: {np.around(RMSE, 3)}')","cb8af2ce":"import lightgbm as lgb\n\nlgb_reg = lgb.LGBMRegressor(max_depth=3, n_estimators=350, learning_rate=0.5, random_state=42)\nlgb_reg.fit(X_train, y_train)\n\n\ny_pred = lgb_reg.predict(X_test)\n\nMSE = metrics.mean_squared_error(y_test, y_pred)\nRMSE = np.sqrt(MSE)\n\nprint(f'Test MSE : {np.around(MSE, 3)}')\nprint(f'Test RMSE: {np.around(RMSE, 3)}')","84cafe7f":"### 3.4 GradientBoost\n![image.png](attachment:image.png)","45a3a0f0":"## Comparing with other ensemble models\n### As I just want to see how other ensemble models are doing in breifly, I just run other ensemble model without tuning hyperparameters. Thus, despite some ensemble models perform better than random forest in general, following results may be shown otherwise.\n","4a60ddff":"### According to Feature Importance, I can drop variables except PriceDiff, ShelveLoc_Good, Age, but I will keep all variables.","240adc40":"#### 2.4 Data Visualization and analysis\n                  \n        (1)Features \n        - Categorical Variables : ShelveLoc, Urban, US, Education\n         (Despite \"Educatoin\" data type is integer in original data set, \n         we cannot tell differences of each level of educations \n         have same numerical meaning. Thus I put 'Education' in categorical variable)\n         \n        - Continuous Variables  : CompPrice, Income, Advertising, Population, Price, Age\n        \n        (2)Target \n        - Sales","7072ec15":"### 3.4 XGboost","9152b816":"## 1. Data Undestanding\n    \n     Carseats : Sales of Child Car Seats\n     \n     Data source : https:\/\/rdrr.io\/cran\/ISLR\/man\/Carseats.html\n     \n     In ISLR: Data for an Introduction to Statistical Learning with Applications in R\n\n#### 1.1 Description : A simulated data set containing sales of child car seats at 400 different stores\n\n#### 1.2 A data frame with 400 observations on the following 11 variables.\n\n    * Sales : Target Variable\n    Unit sales (in thousands) at each location\n\n    * CompPrice : \n    Price charged by competitor at each location\n\n    * Income : \n    Community income level (in thousands of dollars)\n\n    * Advertising : \n    Local advertising budget for company at each location (in thousands of dollars)\n\n    * Population : \n    Population size in region (in thousands)\n\n    * Price : \n    Price company charges for car seats at each site\n\n    * ShelveLoc : \n    A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site\n\n    * Age : \n    Average age of the local population\n\n    * Education : \n    Education level at each location\n\n    * Urban : \n    A factor with levels No and Yes to indicate whether the store is in an urban or rural location\n\n    * US : \n    A factor with levels No and Yes to indicate whether the store is in the US or not\n","fefecea6":"###  Let's says, the carseats sales manager who is managing 400 stores wants to be the top sales manager in the company. The manager is wondering how much sales other stores are doing now. However the manager only knows his\/her sales information and can access informational data other than sales for other stores. To find out sales amount of other stores, the manager is going to build a sales predictive model(regression model), so the manager can assume how much carseats sales are doing in other stores.","33b49073":"### RMSE is improved by Random Forrest Model and decreased to 1.301 \n### (RMSE by decision tree was 1.877)","982b6d53":"3.1.5 Random Forest\n\nTo Improve MSE, I am going to build Ensemble model, RandomForrest","deadb586":"We learned that\n\n* \"Shelveloc\" is related to Sales as disribution(mean, max, min) of sales is different by Shelveloc's category\n* Either \"Urban: yes\" or \"Urban : no\" doesn't matter for Sales\n* US is slightly related to Sales : stores in US show a little higher sales than others. \n* Sales mean shows difference by Educaiton levels.","f7e4fa1a":"### 2.4 Feature Engineering","9d88a9d4":"## 2. EDA\n#### 2.1 Data collection\n\n        Collect 400 store carseats data\n        \n#### 2.2 Data integration\n\n        We have \"Carseats.csv\" data file\n        \n#### 2.3 Data preparation and cleaning\n\n        Load data in python","083364d0":"1. Data Undestanding\n\n     1.1 Description\n     \n     1.2 Features\n\n2. EDA\n\n    2.1 Data Collection \n    \n    2.2 Data Integration\n    \n    2.3 Data Preparing and Cleaning\n    \n    2.4 Data Visualization and Analysis\n    \n    2.5 Feature Engineering\n \n3. Modeling\n\n    3.1 Random Forest \n    \n    3.2 Bagging Ensemble\n    \n    3.3 AdaBoost\n    \n    3.4 Gradient Boost\n    \n    3.5 XGboost\n    \n    3.6 LightGBM\n    \n4. Conclusion\n","421fb2fa":"Decision Tree model has some advantages :\n    - Simple to understand and interpret\n    - Able to handle both numerical and categorical data\n    - Useful in data exploration\n    - Require little data preparation\n    - Non-parametric method\n    - Possible to validate a model using statistical tests\n    \nBut it also has some disadvantages too :\n    - Trees can be very non-robust \n    - Overfitting\n    \nTo overcome decision tree's disadvantages, we can build Random forest or other Ensemble models, and they are even perform very well and the improvement is proved by this ML processing (decreasing RMSE).\n\nWhen we build model, if we want to find out which Ensemble model does perform well in our data, we should look into data types, appropriate model kinds and tuning hyperparameter in further ML processing.  ","f51491a4":"### 3.3 AdaBoost\n![image.png](attachment:image.png)","1959a11f":"## 4. Conclusion","3b2eebea":"We learned the followings,\n* Price is highly correlated with Sales (negative correlation)\n* Advertising is slightly correlatied with Sales (positive correlation)\n* Age is also slightly correlatied with Sales (positive correlation) \n* Price and CompPrice are correlated. They have collinearity.","b91227b2":"Now we are going to do feature engineering to find significant new variables. And to build a decision tree regression model, we should makes categorical variables into dummies as categorical variables are not working in DecisionTreeRegressor.\n\n* Make new feature : PriceDiff = Price - CompPrice\n* Get dummies for categorical features","3aa64d5a":"## 3. Modeling","836d5b2b":"Well, Advertising size rate per population doesn't look like significant new variable. Then I'm going to drop it.","02f34a20":"### 3.5 LightGBM","926be90a":"### 3.1 Random Forest\n    3.1.1 Train \/ Test Split \n          Standardization\n    3.1.2 Base modeling : Decision Tree Regression\n          : Can check base score(MSE, RMSE)\n    3.1.3 Tree Visualization \n    3.1.4 Hyperparameter Tuning\n    3.1.5 Random Forest modeling","81442013":"### 3.2 Bagging Ensemble \n - Sampling with repeating\n - Average prediction by each Decision Tree Regressor models\n - Pararrel ways : can build model with fast ","32d6b710":"* \"Price Difference\" brings a correlation improving and removing a collinearity between \"Price\" and \"CompPrice\"","87b00280":"Great! 'Sales' doesn't look like skew"}}