{"cell_type":{"a0c452f2":"code","fa626eda":"code","342fa547":"code","b8eca838":"code","2b52b14f":"code","d1295bfe":"code","4acc99c8":"code","2bea9a5f":"code","4d211a83":"code","31e0b9e0":"code","bc1e6852":"code","7a736adf":"code","2e2bc455":"code","f1f09232":"code","968692ba":"code","4c7c9282":"code","165cada7":"code","c5b3c9ea":"code","57de6cf2":"code","ddd412f2":"code","b8acc948":"code","e2e20310":"code","bec1a53b":"code","a87603af":"code","e77b0ab9":"markdown","fb5d1403":"markdown","9cbc9c8f":"markdown","281a6c3e":"markdown","532fb295":"markdown","5e4ceb6f":"markdown","8bf9c926":"markdown","8185d2a0":"markdown","61a1d9a3":"markdown","89c9cf60":"markdown"},"source":{"a0c452f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa626eda":"import pandas as pd\nimport numpy as np\nimport sklearn as skl\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, RepeatedKFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error as mse, r2_score as r2\nfrom sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, Lasso, LassoCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\nfrom datetime import datetime\nimport xgboost as xgb","342fa547":"import warnings\nwarnings.filterwarnings('ignore')","b8eca838":"TRAIN_DATASET_PATH = '\/kaggle\/input\/real-estate-price-prediction-moscow\/train.csv'\nTEST_DATASET_PATH = '\/kaggle\/input\/real-estate-price-prediction-moscow\/test.csv'","2b52b14f":"train_df = pd.read_csv(TRAIN_DATASET_PATH)\ntest_df = pd.read_csv(TEST_DATASET_PATH)","d1295bfe":"train_df.shape[1] - 1 == test_df.shape[1]","4acc99c8":"plt.figure(figsize = (16, 8))\n\ntrain_df['Price'].hist(bins=30)\nplt.ylabel('Count')\nplt.xlabel('Price')\n\nplt.title('Target distribution')\nplt.show()","2bea9a5f":"correlation = train_df.corrwith(train_df['Price']).sort_values(ascending=False)\ncorrelation.drop('Price', inplace=True)\n\nplt.figure(figsize = (16, 8))\nplt.bar(correlation.index, correlation)\nplt.xticks(rotation='90')\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Correlation', fontsize=15)\nplt.title('Feature correlation', fontsize=15)\nplt.show()","4d211a83":"class DataPreprocessor:\n    def __init__(self):\n        self.medians = None\n        self.kitchen_square_max_value = None\n        self.kitchen_square_min_value = None\n        self.life_square_min_value = None\n        self.max_floor = None\n        self.districts_healthcare = None\n\n\n        \n    def fit(self, df):\n        # Medians and quantiles\n        self.medians = df.median() # medians\n        self.kitchen_square_max_value = np.quantile(df['KitchenSquare'], q=0.999) \n        self.kitchen_square_min_value = np.quantile(df['KitchenSquare'], q=0.001) \n        self.life_square_min_value = np.quantile(df['LifeSquare'], q=0.03) \n        self.max_floor = df['Floor'].max()\n        \n        \n        # Compute mean Helthcare_1 value in each district\n        self.districts_healthcare = df.groupby(['DistrictId'])['Healthcare_1'].agg('mean').to_dict()\n\n\n    def transform(self, df):\n        # Life Square fillna\n        df['LifeSquare'].fillna(((df['Square'] - df['KitchenSquare']) - df['Square']*0.2), inplace=True)\n\n\n        # Rooms\n        # Fillna with medians         \n        df['Rooms'].fillna(self.medians.Rooms, inplace=True) \n        \n        # Compute median room square and fill outliers with LifeSquare\/Room_square \n        condition_rooms = (df['Rooms'] > 6) | (df['Rooms'] == 0)   \n        room_sq = np.round((self.medians.LifeSquare \/ self.medians.Rooms, 1))[0]\n        df.loc[condition_rooms , 'Rooms'] = df.loc[condition_rooms, 'LifeSquare'] \/ room_sq\n\n        # Square\n        # If LifeSquare > Square: exchange values\n        df['Square'], df['LifeSquare'] = np.where(df['Square'] < df['LifeSquare'],(df['LifeSquare'],df['Square']), (df['Square'],df['LifeSquare']))\n\n\n        # LifeSquare\n        # Fill outliers with (Square - KithcenSquare)\n        ls_condition = (df['LifeSquare'] < self.life_square_min_value)\n        df.loc[ls_condition, 'LifeSquare'] = df.loc[ls_condition, ['Square']] - df.loc[ls_condition, ['KitchenSquare']]\n  \n        \n        # KitchenSquare\n        # Fill outliers with (Square - LifeSquare - 10% of Square)\n        condition_kitchen_square = (df['KitchenSquare'] > self.kitchen_square_max_value) | (df['KitchenSquare'] < self.kitchen_square_min_value)\n        df.loc[condition_kitchen_square, 'KitchenSquare'] = df.loc[condition_kitchen_square, 'Square'] - df.loc[condition_kitchen_square, 'LifeSquare'] \\\n        - (df.loc[condition_kitchen_square, 'Square'] * 0.1)\n\n\n        # Ecology and Shops\n        # Switch to binary\n        df.replace({'Ecology_2': {'A': 0, 'B': 1}}, inplace=True)\n        df.replace({'Ecology_3': {'A': 0, 'B': 1}}, inplace=True)\n        df.replace({'Shops_2': {'A': 0, 'B': 1}}, inplace=True)\n        \n        \n        # HouseFloor\n        # If HouseFloor < Floor: exchange values\n        house_floor_condition = df['HouseFloor'] < df['Floor']\n        df.loc[house_floor_condition, 'HouseFloor'] = df.loc[house_floor_condition, 'Floor']\n\n        \n        # HouseYear\n        # If HouseYear > current year set median value\n        current_year = datetime.now().year\n        condition_year = (df['HouseYear'] > current_year)\n        df.loc[condition_year, 'HouseYear'] = self.medians.HouseYear\n        \n        \n        # Healthcare\n        # Fillna with dictrict healthcare value. (If district has no healthcare value fill with medians)      \n        df.loc[df['Healthcare_1'].isna(), 'Healthcare_1'] = df['DistrictId'].map(self.districts_healthcare)\n        df['Healthcare_1'].fillna(self.medians.Healthcare_1, inplace=True)       \n        # Clip on upper quantille\n        q_max = np.quantile(df['Healthcare_1'], q=0.9)\n        df['Healthcare_1'].clip(upper=q_max, axis=0, inplace=True)\n        \n        \n        # Drop Id\n        df.drop(['Id'], axis=1, inplace=True)\n\n        \n        # Fillna just in case\n        df.fillna(self.medians, inplace=True)\n        return df\n\n\n","31e0b9e0":"class FeatureGenerator:\n    def __init__(self):\n        self.districts_median_year = None\n        self.districts_density = None\n        self.district_price_per_square = None\n        self.min_sq = None\n        self.max_sq = None\n\n        \n    def fit(self, df):   \n        self.min_sq = np.quantile(df['Square'], q=0.005)\n        self.max_sq = np.quantile(df['Square'], q=0.995)\n        \n        self.districts_median_year = df.groupby(['DistrictId'])['HouseYear'].agg('median').to_dict()  # median house year in each district\n        self.districts_density = df.groupby(['DistrictId'])['Square'].agg('median').to_dict()  # median square in each district\n        self.district_price_per_square = df.groupby(['DistrictId'])['Price'].agg('median') \\\n        \/ df.groupby(['DistrictId'])['Square'].agg('median')  # median price for square meter in each district\n        \n    \n    def new_features(self, df):\n        # How old is the district      \n        df['DistrictYear'] = df['DistrictId'].map(self.districts_median_year)\n        \n        self.median_district_year = df['DistrictYear'].median()\n        df['DistrictYear'].fillna(self.median_district_year, inplace=True)\n\n\n        # Median square of flat in each district\n        df['DistrictDensity'] = df['DistrictId'].map(self.districts_density)\n        \n        self.median_district_density = df['DistrictDensity'].median()\n        df['DistrictDensity'].fillna(self.median_district_density, inplace=True)\n\n\n        # Median price for square meter in each district\n        self.district_price_per_square.to_dict()\n        df['DistrictPrice'] = df['DistrictId'].map(self.district_price_per_square)\n        \n        self.median_district_price_per_square = df['DistrictPrice'].median()\n        df['DistrictPrice'].fillna(self.median_district_price_per_square, inplace=True)\n\n\n        # Floor category\n        floor_bins = [0, 4, 7, 12, df['Floor'].max()]\n        df['Floor_cat'] = pd.cut(df['Floor'], bins=floor_bins, labels=False)\n        df['Floor_cat'].fillna(-1, inplace=True) \n\n        \n        # Scale and merge Social\n        scaler = RobustScaler()\n        pca = PCA(n_components=1, random_state=42)\n        social_scaled = pd.DataFrame(scaler.fit_transform(df[['Social_1', 'Social_2', 'Social_3']]))\n        df['Social'] = pca.fit_transform(social_scaled)\n        \n        \n        df.drop(['Ecology_2', 'Ecology_3', 'Shops_2', 'Helthcare_2', 'Floor',], axis=1, inplace=True)\n        \n        return df\n    \n    \n    def drop_outliers(self, df):\n        df = df.loc[(df['Square'] > self.min_sq) & (df['Square'] < self.max_sq)]\n        \n        \n        \n        return df\n","bc1e6852":"preprocessor = DataPreprocessor()\npreprocessor.fit(train_df)\ntrain_df = preprocessor.transform(train_df)\ntest_df = preprocessor.transform(test_df)","7a736adf":"features_gen = FeatureGenerator()\nfeatures_gen.fit(train_df)\ntrain_df = features_gen.new_features(train_df)\ntrain_df = features_gen.drop_outliers(train_df)\ntest_df = features_gen.new_features(test_df)","2e2bc455":"y = pd.DataFrame(data=train_df['Price'])\ntrain_df.drop('Price', axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.15, random_state=100)\n","f1f09232":"gb_model = GradientBoostingRegressor(criterion='mse',\n                                     max_depth=6,\n                                     min_samples_leaf=50,\n                                     random_state=42,  \n                                     n_estimators=2250, \n                                     max_features='sqrt', \n                                     loss='huber', \n                                     learning_rate=0.025)\n\ngb_model.fit(X_train, y_train)\n\ny_train_preds = gb_model.predict(X_train)\ny_test_preds = gb_model.predict(X_test)\nprint(r2(y_train, y_train_preds))\nprint(r2(y_test, y_test_preds))","968692ba":"# gb_model = xgb.XGBRegressor(colsample_bytree=0.4445, \n#                              learning_rate=0.015, max_depth=6,\n#                              min_child_weight=0.1, \n#                              n_estimators=1000,\n#                              reg_lambda=0.6,\n#                              random_state =7)\n# gb_model.fit(X_train, y_train)\n\n# y_train_preds = gb_model.predict(X_train)\n# y_test_preds = gb_model.predict(X_test)\n# print(r2(y_train, y_train_preds))\n# print(r2(y_test, y_test_preds))","4c7c9282":"feature_importances = pd.DataFrame(zip(X_train.columns, \n                                       gb_model.feature_importances_), \n                                   columns=['feature_name', 'importance'])\n\nfeature_importances.sort_values(by='importance', ascending=False, inplace=True)\nfeature_importances","165cada7":"plt.figure(figsize = (16, 8))\nplt.bar(feature_importances['feature_name'], feature_importances['importance'])\nplt.xticks(rotation='90')\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Importance', fontsize=15)\nplt.title('Feature importances', fontsize=15)\nplt.show()","c5b3c9ea":"test_df.head(2)","57de6cf2":"X_train.head(2)","ddd412f2":"submit = pd.read_csv('\/kaggle\/input\/real-estate-price-prediction-moscow\/sample_submission.csv')\nsubmit.head()","b8acc948":"predictions = gb_model.predict(test_df)\npredictions","e2e20310":"submit['Price'] = predictions\nsubmit.head()","bec1a53b":"submit.shape","a87603af":"submit.to_csv('gb_submit.csv', index=False)","e77b0ab9":"Feature importance","fb5d1403":"Dataset split","9cbc9c8f":"Test data set prediction and submition to kaggle","281a6c3e":"Data preprocessing","532fb295":"# Data processing and generation of features","5e4ceb6f":"XGBR model model fit and predict for train dataset","8bf9c926":"Gradient Boosting Regressor model fit and predict for train dataset","8185d2a0":"# Loading datasets ","61a1d9a3":"# Target feature distribution and correlations","89c9cf60":"Feature generation"}}