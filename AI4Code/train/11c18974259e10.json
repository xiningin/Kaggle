{"cell_type":{"86b33fbf":"code","72a6b7fd":"code","d2f7571c":"code","16ed6b92":"code","6b0870f0":"code","218d7f07":"code","67c9ba42":"code","ee40d823":"code","7bf36585":"code","a57e2fbb":"code","912aa732":"code","15d55731":"code","de3e8de8":"code","5d764f1b":"code","9f7bdf05":"code","52d16d17":"code","cfbd0880":"code","1890e7e2":"code","cd6c63b0":"code","46573a51":"code","398c8af1":"code","59286e9c":"code","127be41e":"code","6802c82b":"code","019a0452":"code","55fb52e3":"code","a88e0c9e":"code","83841e38":"code","f6f7d923":"code","109bdd96":"code","17945671":"code","78a9fd2c":"code","7705cdc8":"code","f53a58d5":"code","dd3742c0":"code","d75024bb":"code","8da30e03":"code","d51b150b":"code","2a9cc861":"code","bb455205":"code","ba6f6596":"code","63500f8f":"code","c6354a6f":"code","d0edb66e":"code","073cc41f":"code","d36c364c":"code","430fe972":"code","82d04656":"code","4f0692c7":"code","9f7656fa":"code","39cba4ae":"markdown","058ac64a":"markdown","6fa0fa18":"markdown","74518540":"markdown","deb96800":"markdown"},"source":{"86b33fbf":"# Importing the libraries\nimport re #Regular expression operations\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nimport joblib\n\nfrom tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import Input,Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers import SGD\n","72a6b7fd":"tweet_dataset = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=df=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","d2f7571c":"tweet_dataset =tweet_dataset.drop('id', axis=1)\ntest=df.drop('id', axis=1)","16ed6b92":"tweet_dataset.head()","6b0870f0":"test.head()","218d7f07":"tweet_dataset.info()","67c9ba42":"tweet_dataset.duplicated().sum()\n","ee40d823":"\ntweet_dataset =tweet_dataset.drop_duplicates()","7bf36585":"#showing the number of positive tweets and false tweets \nfig=plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(np.array(tweet_dataset['target'].value_counts().index),np.array(tweet_dataset['target'].value_counts()))\nax.set_ylabel('number')\nplt.xticks(np.array(tweet_dataset['target'].value_counts().index), ('0','1'))\nplt.show()\n","a57e2fbb":"#showing first 10 Disaster by most  frequent \nfig=plt.figure(figsize=[10, 8])\nax = fig.add_axes([0,0,1,1])\nax.bar(np.array(tweet_dataset['keyword'].value_counts().index)[:10],np.array(tweet_dataset['keyword'].value_counts())[:10])\nax.set_ylabel('number')\nplt.xticks(np.array(tweet_dataset['keyword'].value_counts().index)[:10])\nplt.show()","912aa732":"#show 5 fist teweets \nprint(tweet_dataset[tweet_dataset.target == 0].head(5) ) \nprint(tweet_dataset[tweet_dataset.target== 1].head(5) )  \n","15d55731":"#creation of a method allowing cleaning\ndef cleaning_tweet(tweet):\n\t#Preprocess the text in a single tweet\n\t#arguments: tweet = a single tweet in form of string \n\t#convert the tweet to lower case\n   \n\t#convert all urls to sting \"URL\"\n    tweet = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','URL',tweet)\n\t#convert all @username to \"AT_USER\"\n    tweet = re.sub('@[^\\s]+','AT_USER', tweet)\n\t#convert \"#topic\" to just \"topic\"\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n\t#remove punctuation \n    tweet=re.sub(\"[^a-zA-Z]\",\" \",tweet)\n   #correct all multiple white spaces to a single white space\n    tweet = re.sub('[\\s]+', ' ', tweet)\n    \n    tweet=tweet.lower()\n    return (tweet)","de3e8de8":"tweet_dataset['text'] =tweet_dataset['text'].apply(cleaning_tweet)\ntest['text'] = test['text'].apply(cleaning_tweet)","5d764f1b":"#the goal here is to do word tokenization and elimination of unwanted words and stemmatization and lemmatization\n#initialization of the English language stopword list\nstpw=set(stopwords.words('english'))\n","9f7bdf05":"#initialization of the stemmer and lemmatizer of the nltk library\nps = PorterStemmer() \nlem = WordNetLemmatizer()\ndef preprocess_tweet(tweet):\n    tweet_words=word_tokenize(tweet) \n    filtered_sent=[]\n    for w in tweet_words:\n        if w not in stpw:\n            filtered_sent.append(w)\n\n    stemmed_words=[]\n    for w in filtered_sent:\n       stemmed_words.append(ps.stem(w))\n       \n    lemmatized_words=[]\n    for w in stemmed_words:\n       lemmatized_words.append(lem.lemmatize(w))     \n    tweet=lemmatized_words\n    return(tweet)","52d16d17":"tweet_dataset['text'] =tweet_dataset['text'].apply(preprocess_tweet)\ntest['text'] =test['text'].apply(preprocess_tweet)","cfbd0880":"tweet_dataset['text']=[\" \".join(tweet) for tweet in tweet_dataset['text'].values]\ntest['text']=[\" \".join(tweet) for tweet in test['text'].values]\n","1890e7e2":"tweet_dataset.head()","cd6c63b0":"def length(tweet):    \n    '''method which returns the length of a character string'''\n    return len(tweet)","46573a51":"tweet_dataset[\"length\"]=tweet_dataset[\"text\"].apply(length)\ntest[\"length\"]=test[\"text\"].apply(length)","398c8af1":"tweet_dataset.head()","59286e9c":"negative_tweet= tweet_dataset[tweet_dataset['target']==0]\npositive_tweet=tweet_dataset[tweet_dataset['target']==1]\n","127be41e":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nbins = 50\nplt.hist(negative_tweet['length'], alpha = 0.6, bins=bins, label='Negative_tweet')\nplt.hist(positive_tweet['length'], alpha = 0.7, bins=bins, label='Positive_tweet')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,130)\nplt.grid()\nplt.show()\n","6802c82b":"data = np.array(tweet_dataset.text)\nlabel = np.array(tweet_dataset.target)","019a0452":"test=np.array(test.text)","55fb52e3":"#creation of the matrix\ntfv=TfidfVectorizer(sublinear_tf=True)\nfeatures=tfv.fit_transform(data)\n\nvocab=tfv.fit(data)\nfeatures_dictionary = vocab.vocabulary_.items()\n","a88e0c9e":"test=tfv.transform(test)\n","83841e38":"# lists to store the vocab and counts\nvocab = []\ncount = []\n# iterate through each vocab and count append the value to designated lists\nfor key, value in features_dictionary:\n    vocab.append(key)\n    count.append(value)\n# store the count in panadas dataframe with vocab as index\ntweet_vocab = pd.Series(count, index=vocab)","f6f7d923":"# presentation of the first 20\ntop_vacab = tweet_vocab.head(20)\ntop_vacab.plot(kind = 'barh', figsize=(8,15))","109bdd96":"#sampling test train\nX_train, X_test, y_train, y_test = train_test_split(features,label, test_size=0.25, random_state=1234)\n","17945671":"# Training the Logistic Regression model on the Training set\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)\n","78a9fd2c":"# Predicting the Test set results\n\ny_pred = classifier.predict(X_test)","7705cdc8":"# Predicting the Test results\npred = classifier.predict(test)","f53a58d5":"# Making the Confusion Matrix for LogisticRegressio model\nfrom sklearn.metrics import confusion_matrix,f1_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","dd3742c0":"# Making the f1 score for LogisticRegressio model\nF1=f1_score(y_test, y_pred, average='macro')\nprint(F1)","d75024bb":"import tensorflow.compat.v1 as tf\ntf.disable_eager_execution()\ntf.disable_v2_behavior()","8da30e03":"from keras import utils\ny_train_b = utils.to_categorical(y_train)\ny_test_b = utils.to_categorical(y_test)\n","d51b150b":"# construction model \nfrom tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import Input,Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers import SGD\n\n\nNB_WORDS = X_train.shape[1]\nbatch_size = 64\nepochs = 100 #nombre d'etiration\nnum_classes = 2\n\ntext_model = Sequential()\ntext_model.add(Dense(32, activation='relu', input_shape=(NB_WORDS,)))  #contient 32 neuron dans le 1er couche  cache\ntext_model.add(Dense(64, activation='relu')) #contient 64 neuron dans le 2er couche  cache\ntext_model.add(Dense(32, activation='relu')) #contient 32 neuron dans le 3er couche  cache\ntext_model.add(Dense(num_classes, activation='softmax'))\ntext_model.summary()\n","2a9cc861":"#compile the model and the training\ndef deep_model(model,epochs,batch_size):\n    model.compile(loss='binary_crossentropy',\n                       optimizer=SGD(0.01),\n                       metrics=['accuracy'])\n\n    \n    history = model.fit(X_train\n                       , y_train_b\n                       , epochs=epochs\n                       , batch_size=batch_size\n                       , validation_data=(X_test, y_test_b)\n                       , verbose=1)\n    \n    return (history)","bb455205":"\n#training the model\ntext_history = deep_model(text_model,epochs,batch_size)\n","ba6f6596":"# model evaluation\neval = text_model.evaluate(X_test, y_test_b, verbose=1)\nprint('Test loss: {} '.format(np.round(eval[0],2)))\nprint('Test accuracy: {} %'.format(np.round(eval[1]*100,2)))\n","63500f8f":"def eval_metric(history, metric_name):\n    metric = history.history[metric_name]\n    val_metric = history.history['val_' + metric_name]\n\n    e = range(1, epochs + 1)\n\n    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n    plt.legend()\n    plt.show()\n","c6354a6f":"\neval_metric(text_history, 'loss')\neval_metric(text_history, 'acc')\n","d0edb66e":"y_pred_k = text_model.predict_classes(X_test)","073cc41f":"from sklearn.metrics import confusion_matrix,f1_score\ncm_k = confusion_matrix(y_test, y_pred_k)\nprint(cm)","d36c364c":"F1_k=f1_score(y_test, y_pred_k, average='macro')\n","430fe972":"print(f'f1_score of LogisticRegression=  {F1}')\nprint(f'onfusion_matrix of LogisticRegression=  {cm}')\n\nprint(f'f1_score of ANN =  {F1_k}')\nprint(f'onfusion_matrix of ANN =  {cm_k}')","82d04656":"train=classifier.predict(test)","4f0692c7":"\nsubmission = pd.DataFrame({\n    \"Id\": df['id'],\n    \"Target\": train})","9f7656fa":"submission.to_csv(\"submission_test_set.csv\", index=False)\nsubmission.head()","39cba4ae":"# Predicting the same result neural network","058ac64a":"# Disaster Tweets Project\n## Real or Not? NLP with Disaster Tweets challenge add-on\n","6fa0fa18":"### PorterStemmer():\nStemmers remove morphological affixes from words, leaving only the word stem\n\n### WordNetLemmatizer():\nLemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n\n","74518540":"### Context\nThe file contains over 11,000 tweets associated with disaster keywords like \u201ccrash\u201d, \u201cquarantine\u201d, and \u201cbush fires\u201d as well as the location and keyword itself. The data structure was inherited from Disasters on social media\n\nThe tweets were collected on Jan 14th, 2020.\n\nSome of the topics people were tweeting:\n\nThe eruption of Taal Volcano in Batangas, Philippines\nCoronavirus\nBushfires in Australia\nIran downing of the airplane flight PS752\nDisclaimer: The dataset contains text that may be considered profane, vulgar, or offensive.\n\n","deb96800":"plotting the number of words with function of the target"}}