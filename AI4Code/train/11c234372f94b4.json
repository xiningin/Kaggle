{"cell_type":{"8344e804":"code","a65486f1":"code","9fbdbf86":"code","f20ebd17":"code","34ed5825":"code","c1b746e3":"code","ae7bf4d7":"code","e67118a8":"code","b59471b4":"code","690df620":"code","cb28aefe":"code","2371aea6":"code","a403f11f":"code","454a4524":"code","fc1183a2":"code","79e6e4ec":"code","572d16d6":"code","738c5ac8":"code","096d710b":"code","de4b47a7":"markdown","9ad5e4df":"markdown","12747afe":"markdown","ae7d1885":"markdown","91339427":"markdown"},"source":{"8344e804":"from keras.models import Sequential\nfrom keras.layers import Conv2D, Activation,Dropout\nfrom keras.models import Model,load_model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.core import Flatten, Dense\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.engine.topology import Layer\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.datasets import load_files\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport itertools\n\ntrain_dir = '\/kaggle\/input\/waste-classification-data\/dataset\/DATASET\/TRAIN'\ntest_dir = '\/kaggle\/input\/waste-classification-data\/dataset\/DATASET\/TEST'\n\ndef load_dataset(path):\n    data = load_files(path) #load all files from the path\n    files = np.array(data['filenames']) #get the file  \n    targets = np.array(data['target'])#get the the classification labels as integer index\n    target_labels = np.array(data['target_names'])#get the the classification labels \n    return files,targets,target_labels\n    \nx_train, y_train,target_labels = load_dataset(train_dir)\nx_test, y_test,_ = load_dataset(test_dir)\n\nprint('Training set size : ' , x_train.shape[0])\nprint('Testing set size : ', x_test.shape[0])\n\nx_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 1)\n\nprint (\"x_train shape: \" + str(x_train.shape))\nprint (\"y_train shape: \" + str(y_train.shape))\nprint (\"x_validate shape: \" + str(x_validate.shape))\nprint (\"y_validate shape: \" + str(y_validate.shape))\nprint (\"x_test shape: \" + str(x_test.shape))\nprint (\"y_test shape: \" + str(y_test.shape))\n\ndef convert_image_to_array(files):\n    width, height, channels = 100, 100, 3\n    images_as_array = np.empty((files.shape[0], width, height, channels), dtype=np.uint8) #define train and test data shape\n    for idx,file in enumerate(files):\n        img = cv2.imread(file) \n        res = cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_CUBIC) #As images have different size, resizing all images to have same shape of image array\n        images_as_array[idx] = res\n    return images_as_array\n\nx_train = np.array(convert_image_to_array(x_train))\nprint('Training set shape : ',x_train.shape)\n\nx_valid = np.array(convert_image_to_array(x_validate))\nprint('Validation set shape : ',x_valid.shape)\n\nx_test = np.array(convert_image_to_array(x_test))\nprint('Test set shape : ',x_test.shape)\n\nx_train = x_train.astype('float32')\/255\nx_valid = x_valid.astype('float32')\/255\nx_test = x_test.astype('float32')\/255\ny_train = y_train.reshape(y_train.shape[0],1)\ny_test = y_test.reshape(y_test.shape[0],1)\ny_validate = y_validate.reshape(y_validate.shape[0],1)","a65486f1":"!ls","9fbdbf86":"def CNN_model():\n    dense_layers = [0, 1, 2]\n    layer_sizes = [64, 128]\n    conv_layers = [1, 2, 3]\n    for dense_layer in dense_layers:\n        for layer_size in layer_sizes:\n            for conv_layer in conv_layers:\n                NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(\n                    conv_layer, layer_size, dense_layer, int(time.time()))\n                print(NAME)\n\n                model = Sequential()\n\n                model.add(Conv2D(layer_size, (3, 3), input_shape=(100,100,3)))\n                model.add(Activation('relu'))\n                model.add(MaxPooling2D(pool_size=(2, 2)))\n\n                for l in range(conv_layer-1):\n                    model.add(Conv2D(layer_size, (3, 3)))\n                    model.add(Activation('relu'))\n                    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n                model.add(Flatten())\n\n                for _ in range(dense_layer):\n                    model.add(Dense(layer_size))\n                    model.add(Activation('relu'))\n                    model.add(Dropout(0.5))\n\n                model.add(Dense(1))\n                model.add(Activation('sigmoid'))\n    #             print('came here')\n\n                model.summary()\n    return model","f20ebd17":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0, # Randomly zoom image \n        width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(x_train)\nprint(x_train.shape)","34ed5825":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nimport tensorflow as tf\nimport pickle\nimport time\nimport numpy as np\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n\n# pickle_in = open(\"X.pickle\", \"rb\")\n# X = pickle.load(pickle_in)\n\n# pickle_in = open(\"y.pickle\", \"rb\")\n# y = pickle.load(pickle_in)\n\n# X = X\/255.0\nIMG_SIZE = 100\n# X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n\n# y = np.array(y)\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel = CNN_model() # without data augmentation\n\ncheckpoint = ModelCheckpoint('128x3-CNN-no-aug.hdf5',  # model filename\n                             monitor='val_loss', # quantity to monitor\n                             verbose=0, # verbosity - 0 or 1\n                             save_best_only= True, # The latest best model will not be overwritten\n                             mode='auto') # The decision to overwrite model is made \n                                          # automatically depending on the quantity to monitor\n\nmodel.compile(loss='binary_crossentropy',\n                optimizer=opt,\n                metrics=['accuracy'])\n\nearlystop = EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\n                          min_delta = 0, #Abs value and is the min change required before we stop\n                          patience = 15, #Number of epochs we wait before stopping \n                          verbose = 1,\n                          restore_best_weights = True) #keeps the best weigths once stopped\n\nReduceLR = ReduceLROnPlateau(patience=3, verbose=1)\ncallbacks = [earlystop, checkpoint, ReduceLR]\n\n# model_details = model.fit(X, y,\n#                     batch_size = 32,\n#                     epochs = 12, # number of iterations\n#                     validation_split=0.2,\n#                     callbacks=[checkpoint],\n#                     verbose=1)\nhistory = model.fit_generator(datagen.flow(x_train, y_train, batch_size= 32), epochs = 20, verbose=1,callbacks = callbacks,validation_data=(x_valid,y_validate))\n# model.fit(X, y,\n#             batch_size=32,\n#             epochs=10,\n#             validation_split=0.2)\n\nmodel.save('128x3-CNN-no-aug.hdf5')","c1b746e3":"import pickle\n\npickle_out = open(\"Trained_cnn_history.pickle\",\"wb\")\npickle.dump(history.history, pickle_out)\npickle_out.close()\n\npickle_in = open(\"Trained_cnn_history.pickle\",\"rb\")\nsaved_history = pickle.load(pickle_in)\nprint(saved_history)\n\nimport pickle\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nmodel = tf.keras.models.load_model(\"128x3-CNN-no-aug.hdf5\")\n\nscore_train = model.evaluate(x_train, y_train, verbose=0)\nprint('\\n\\nTrain Loss: ', score_train[0])\nprint('Train Accuracy: ', score_train[1])\n\nscore = model.evaluate(x_test,y_test,verbose=0)\nprint('\\nTest Loss :',score[0])\nprint('Test Accuracy :',score[1])\n\n#get the predictions for the test data\npredicted_classes = model.predict_classes(x_test)\n\nconfusion_mtx = confusion_matrix(y_test, predicted_classes) \n\nplt.imshow(confusion_mtx, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('confusion_matrix')\nplt.colorbar()\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['R','O'], rotation=90)\nplt.yticks(tick_marks, ['R','O'])\n#Following is to mention the predicated numbers in the plot and highligh the numbers the most predicted number for particular label\nthresh = confusion_mtx.max() \/ 2.\nfor i, j in itertools.product(range(confusion_mtx.shape[0]), range(confusion_mtx.shape[1])):\n    plt.text(j, i, confusion_mtx[i, j],\n    horizontalalignment=\"center\",\n    color=\"white\" if confusion_mtx[i, j] > thresh else \"black\")\n\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","ae7bf4d7":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validate'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validate'], loc='upper left')\nplt.show()","e67118a8":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nimport tensorflow as tf\nimport pickle\nimport time\nimport numpy as np\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport keras\n\n\n# pickle_in = open(\"X.pickle\", \"rb\")\n# X = pickle.load(pickle_in)\n\n# pickle_in = open(\"y.pickle\", \"rb\")\n# y = pickle.load(pickle_in)\n\n# X = X\/255.0\nIMG_SIZE = 100\n# X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n\n# y = np.array(y)\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\n\ndense_layers = [2]\nlayer_sizes = [128]\nconv_layers = [3]\nfor dense_layer in dense_layers:\n    for layer_size in layer_sizes:\n        for conv_layer in conv_layers:\n            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(\n                conv_layer, layer_size, dense_layer, int(time.time()))\n            print(NAME)\n\n            model = Sequential()\n\n            model.add(Conv2D(layer_size, (3, 3), input_shape=(100,100,3)))\n            model.add(Activation('relu'))\n            model.add(MaxPooling2D(pool_size=(2, 2)))\n\n            for l in range(conv_layer-1):\n                model.add(Conv2D(layer_size, (3, 3)))\n                model.add(Activation('relu'))\n                model.add(MaxPooling2D(pool_size=(2, 2)))\n\n            model.add(Flatten())\n\n            for _ in range(dense_layer):\n                model.add(Dense(layer_size))\n                model.add(Activation('relu'))\n                model.add(Dropout(0.5))\n\n            model.add(Dense(1))\n            model.add(Activation('sigmoid'))\n#             print('came here')\n\n            checkpoint = ModelCheckpoint(NAME + '.hdf5',  # model filename\n                                         monitor='val_loss', # quantity to monitor\n                                         verbose=0, # verbosity - 0 or 1\n                                         save_best_only= True, # The latest best model will not be overwritten\n                                         mode='auto') # The decision to overwrite model is made \n                                                      # automatically depending on the quantity to monitor\n\n            model.compile(loss='binary_crossentropy',\n                            optimizer=opt,\n                            metrics=['accuracy'])\n\n            # model_details = model.fit(X, y,\n            #                     batch_size = 32,\n            #                     epochs = 12, # number of iterations\n            #                     validation_split=0.2,\n            #                     callbacks=[checkpoint],\n            #                     verbose=1)\n\n\n            # --------------\n            earlystop = EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\n                                      min_delta = 0, #Abs value and is the min change required before we stop\n                                      patience = 15, #Number of epochs we wait before stopping \n                                      verbose = 1,\n                                      restore_best_weights = True) #keeps the best weigths once stopped\n\n            ReduceLR = ReduceLROnPlateau(patience=3, verbose=1)\n            callbacks = [earlystop, checkpoint, ReduceLR]\n\n            history = model.fit_generator(datagen.flow(x_train, y_train, batch_size= 32), epochs = 20, verbose=1,callbacks = callbacks,validation_data=(x_valid,y_validate))\n\n\n\n            # model.fit(X, y,\n            #             batch_size=32,\n            #             epochs=10,\n            #             validation_split=0.2)\n\n            model.save(NAME + '.hdf5')","b59471b4":"import pickle\n\npickle_out = open(\"Trained_cnn_history_my_model.pickle\",\"wb\")\npickle.dump(history.history, pickle_out)\npickle_out.close()","690df620":"pickle_in = open(\"Trained_cnn_history_my_model.pickle\",\"rb\")\nsaved_history = pickle.load(pickle_in)\nprint(saved_history)","cb28aefe":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validate'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validate'], loc='upper left')\nplt.show()","2371aea6":"model = tf.keras.models.load_model(NAME + '.hdf5')\n# model.load_weights('128x3-CNN.hdf5')\n\nscore = model.evaluate(x_test,y_test,verbose=0)\nprint('Test Loss :',score[0])\nprint('Test Accuracy :',score[1])\n\npredicted_classes = model.predict_classes(x_test)\n","a403f11f":"confusion_mtx = confusion_matrix(y_test, predicted_classes) \n\nplt.imshow(confusion_mtx, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('confusion_matrix')\nplt.colorbar()\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['R','O'], rotation=90)\nplt.yticks(tick_marks, ['R','O'])\n#Following is to mention the predicated numbers in the plot and highligh the numbers the most predicted number for particular label\nthresh = confusion_mtx.max() \/ 2.\nfor i, j in itertools.product(range(confusion_mtx.shape[0]), range(confusion_mtx.shape[1])):\n    plt.text(j, i, confusion_mtx[i, j],\n    horizontalalignment=\"center\",\n    color=\"white\" if confusion_mtx[i, j] > thresh else \"black\")\n\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","454a4524":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(x_train)\nprint(x_train.shape)","fc1183a2":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nimport tensorflow as tf\nimport pickle\nimport time\nimport numpy as np\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport keras\n\n\n# pickle_in = open(\"X.pickle\", \"rb\")\n# X = pickle.load(pickle_in)\n\n# pickle_in = open(\"y.pickle\", \"rb\")\n# y = pickle.load(pickle_in)\n\n# X = X\/255.0\nIMG_SIZE = 100\n# X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n\n# y = np.array(y)\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\n\ndense_layers = [2]\nlayer_sizes = [128]\nconv_layers = [3]\nfor dense_layer in dense_layers:\n    for layer_size in layer_sizes:\n        for conv_layer in conv_layers:\n            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(\n                conv_layer, layer_size, dense_layer, int(time.time()))\n            print(NAME)\n\n            model = Sequential()\n\n            model.add(Conv2D(layer_size, (3, 3), input_shape=(100,100,3)))\n            model.add(Activation('relu'))\n            model.add(MaxPooling2D(pool_size=(2, 2)))\n\n            for l in range(conv_layer-1):\n                model.add(Conv2D(layer_size, (3, 3)))\n                model.add(Activation('relu'))\n                model.add(MaxPooling2D(pool_size=(2, 2)))\n\n            model.add(Flatten())\n\n            for _ in range(dense_layer):\n                model.add(Dense(layer_size))\n                model.add(Activation('relu'))\n                model.add(Dropout(0.5))\n\n            model.add(Dense(1))\n            model.add(Activation('sigmoid'))\n#             print('came here')\n\n            checkpoint = ModelCheckpoint(NAME + '.hdf5',  # model filename\n                                         monitor='val_loss', # quantity to monitor\n                                         verbose=0, # verbosity - 0 or 1\n                                         save_best_only= True, # The latest best model will not be overwritten\n                                         mode='auto') # The decision to overwrite model is made \n                                                      # automatically depending on the quantity to monitor\n\n            model.compile(loss='binary_crossentropy',\n                            optimizer=opt,\n                            metrics=['accuracy'])\n\n            # model_details = model.fit(X, y,\n            #                     batch_size = 32,\n            #                     epochs = 12, # number of iterations\n            #                     validation_split=0.2,\n            #                     callbacks=[checkpoint],\n            #                     verbose=1)\n\n\n            # --------------\n            earlystop = EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\n                                      min_delta = 0, #Abs value and is the min change required before we stop\n                                      patience = 15, #Number of epochs we wait before stopping \n                                      verbose = 1,\n                                      restore_best_weights = True) #keeps the best weigths once stopped\n\n            ReduceLR = ReduceLROnPlateau(patience=3, verbose=1)\n            callbacks = [earlystop, checkpoint, ReduceLR]\n\n            history = model.fit_generator(datagen.flow(x_train, y_train, batch_size= 32), epochs = 20, verbose=1,callbacks = callbacks,validation_data=(x_valid,y_validate))\n\n\n\n            # model.fit(X, y,\n            #             batch_size=32,\n            #             epochs=10,\n            #             validation_split=0.2)\n\n            model.save(NAME + '.hdf5')","79e6e4ec":"import pickle\n\npickle_out = open(\"Trained_cnn_history_my_model.pickle\",\"wb\")\npickle.dump(history.history, pickle_out)\npickle_out.close()","572d16d6":"pickle_in = open(\"Trained_cnn_history_my_model.pickle\",\"rb\")\nsaved_history = pickle.load(pickle_in)\nprint(saved_history)","738c5ac8":"model = tf.keras.models.load_model(NAME + '.hdf5')\n# model.load_weights('128x3-CNN.hdf5')\n\nscore = model.evaluate(x_test,y_test,verbose=0)\nprint('Test Loss :',score[0])\nprint('Test Accuracy :',score[1])\n\npredicted_classes = model.predict_classes(x_test)","096d710b":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validate'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validate'], loc='upper left')\nplt.show()","de4b47a7":"### Here there is going to be a huge number of images getting trained. Therefore, it is wise to use .fit_generator()","9ad5e4df":"### CNN model","12747afe":"### Plotting graphs","ae7d1885":"### Without Data Augmentation (using only the images present in the training dataset)","91339427":"### Data augmentation"}}