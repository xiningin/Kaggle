{"cell_type":{"c885e89a":"code","536bbeec":"code","56e92315":"code","ff2def08":"code","ebf33085":"code","a3ccb7e4":"code","b0713b46":"code","57a7f07a":"code","16510b78":"code","89ad6c86":"code","8641c473":"code","caa0cf20":"markdown"},"source":{"c885e89a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport lightgbm as lgb","536bbeec":"# Load the models that were trained \n\nmodel_0x48874 = lgb.Booster(model_file='\/kaggle\/input\/m5-forecasting-models\/model_0x48874_.lgb')\nmodel_0x48743 = lgb.Booster(model_file='\/kaggle\/input\/m5-forecasting-models\/model_0x48743_.lgb')\nmodel = lgb.Booster(model_file='\/kaggle\/input\/m5-forecasting-models\/model.lgb')","56e92315":"from  datetime import datetime, timedelta\nimport gc","ff2def08":"# Load the data \n\nCAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\n\nh = 28 \nmax_lags = 70\ntr_last = 1913\nfday = datetime(2016,4, 25) \n\ndef create_dt(is_train = True, nrows = None, first_day = 1200):\n    prices = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n            \n    cal = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n    \n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n    \n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    \n    return dt\n\ndef create_fea(dt):\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n\n    \n    \n    date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n#         \"ime\": \"is_month_end\",\n#         \"ims\": \"is_month_start\",\n    }\n    \n#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n            \nFIRST_DAY = 750 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !\n\ndf = create_dt(is_train=True, first_day= FIRST_DAY)\nprint(df.shape)\n\ncreate_fea(df)\nprint(df.shape)\n\ndf.dropna(inplace = True)\n\ncat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\nuseless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\ntrain_cols = df.columns[~df.columns.isin(useless_cols)]\nX_train = df[train_cols]\ny_train = df[\"sales\"]","ebf33085":"# import graph objects as \"go\" and import tools\nimport plotly.graph_objs as go\nfrom plotly import tools\n\nimport matplotlib.pyplot as plt\nfrom plotly.offline import init_notebook_mode, iplot\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\ninit_notebook_mode(connected=True)","a3ccb7e4":"model.importance()","b0713b46":"\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(zip(model.feature_importance(importance_type='gain', iteration=-1), model.feature_name()),  columns=['Value_Gain','Feature'])\nfeature_imp['Value_Split'] = model.feature_importance(importance_type='split', iteration=-1)\n","57a7f07a":"# plt.figure(figsize=(20, 10))\n# sns.barplot(x=\"Value_Gain\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value_Gain\", ascending=False))\n# plt.title('LightGBM Features (Gain)')\n# plt.tight_layout()\n# plt.show()\n# plt.savefig('lgbm_importances-01-gain.png')\n\n","16510b78":"# plt.figure(figsize=(20, 10))\n# sns.barplot(x=\"Value_Split\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value_Split\", ascending=False))\n# plt.title('LightGBM Features (Split)')\n# plt.tight_layout()\n# plt.show()\n# plt.savefig('lgbm_importances-01-split.png')\n","89ad6c86":"feature_imp.sort_values(by = ['Value_Gain'], ascending = True, inplace = True)\n\n# create trace1 \ntrace1 = go.Bar(\n                y=feature_imp['Feature'],\n                x=feature_imp['Value_Gain'],\n                name = \"feature_importance_gain\",\n                marker = dict(color = 'rgba(255, 174, 255, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                orientation='h',\n                #xaxis = 'x1',\n                #yaxis = 'y1',\n                \n                text = feature_imp['Feature'])\n\ndata = [trace1]\nlayout = go.Layout(\n    barmode = \"group\", title=\"Feature Importance by Gain\" )\n\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","8641c473":"feature_imp.sort_values(by = ['Value_Split'], ascending = True, inplace = True)\n\ntrace2 = go.Bar(\n                y=feature_imp['Feature'],\n                x=feature_imp['Value_Split'],\n                name = \"feature_importance_split\",\n                marker = dict(color = 'rgba(174, 255, 255, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                orientation='h',\n                #xaxis = 'x2',\n                #yaxis = 'y2',\n                text = feature_imp['Feature'])\n\ndata = [trace2]\nlayout = go.Layout(\n    barmode = \"group\", title=\"Feature Importance by Split\" )\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","caa0cf20":"\nLightGBM has a feature to calculate the relative importance of each feature. Feature importance can be calculated using *gain* or *split* \n\n\n**Gain** \nis the improvement in accuracy brought by a feature to the branches it is on.It implies the relative contribution of the corresponding feature to the model calculated by taking each feature\u2019s contribution for each tree in the model.\n\n\n**Split** \nthe number of times a feature is used to split the data across all trees\n\n\n\nCheck the plotly versions below! \n\n![Feature Importance on Gain](https:\/\/storage.googleapis.com\/tpu-aakash\/Image_Gain.png)\n\n\n![Feature Importance on Split](https:\/\/storage.googleapis.com\/tpu-aakash\/Image_Split.png)"}}