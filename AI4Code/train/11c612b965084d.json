{"cell_type":{"7baacd65":"code","662dc6c2":"code","2f414eb8":"code","2658c495":"code","c18ac7d7":"code","d1057e9d":"code","f7e73e39":"code","1d5fc3ab":"code","a415a05f":"code","530427ac":"code","1b10f1db":"code","a98bdaaf":"code","7bdb0102":"code","e180c836":"code","f3a3ecf6":"code","33e0b64e":"code","c6f5626e":"code","185e5c1b":"code","14702b99":"code","7c318851":"code","c9f4f8b5":"markdown","2a7e5929":"markdown","3ed75f4d":"markdown","ebecbcc0":"markdown","230b88ff":"markdown","87c970ec":"markdown","b63dcb47":"markdown","777e7d02":"markdown","8118ace6":"markdown","ee829155":"markdown","ece22ab8":"markdown","588121d9":"markdown","8ff62b26":"markdown","eaf5c9ef":"markdown"},"source":{"7baacd65":"VER = 162\nVER2 = 0\n\n# if you put path here we load model instead of train model\n# if you wish to train a model in this notebook use None instead of path\nLOAD_MODEL_PATH = '..\/input\/roberta-base-162\/' #or None\nCOMPUTE_OOF = False\nPREDICT_TEST = True\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt, seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom transformers import RobertaTokenizer, TFRobertaModel\n\nimport tensorflow as tf\nprint('TF',tf.__version__)","662dc6c2":"tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nprint('mixed precision enabled')","2f414eb8":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\nprint( train.shape )\ntrain.head()","2658c495":"test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nprint( test.shape )\ntest.head()","c18ac7d7":"if len(test)>7: \n    COMPUTE_OOF = False\n    PREDICT_TEST = True","d1057e9d":"plt.hist(train.target.values,bins=100)\nplt.title('Bradley-Terry values',size=16)\nplt.show()","f7e73e39":"MAX_TOK = 250\ntokenizer = RobertaTokenizer.from_pretrained('..\/input\/tfroberta-base')\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, batch_size=32, shuffle=False, tokenizer=tokenizer): \n\n        self.df = df.reset_index(drop=True)\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.tokenizer = tokenizer\n        self.on_epoch_end()\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = int( np.ceil( len(self.df) \/ self.batch_size ) )\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X, y = self.__data_generation(indexes)        \n        return X,y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange( len(self.df ) )\n        if self.shuffle: np.random.shuffle(self.indexes)\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        \n        X = np.ones((len(indexes),MAX_TOK*2),dtype='int32')\n        y = np.zeros(len(indexes),dtype='float32')\n        \n        y1 = np.zeros(len(indexes),dtype='float32')\n        y2 = np.zeros(len(indexes),dtype='float32')\n        \n        df = self.df.loc[indexes]\n        text = df.excerpt.values\n            \n        for k in range(len(text)):\n            # ENCODE FIRST EXCERPT\n            use = text[k]\n            tk = self.tokenizer.encode(use)\n            ln = min(MAX_TOK,len(tk))\n            X[k,:ln] = tk[:MAX_TOK]\n            \n            # RANDOM PICK AND ENCODE SECOND EXCERPT FOR COMPARISON\n            rw = np.random.randint(0,len(self.df))\n            use = self.df.excerpt.values[rw]\n            tk = self.tokenizer.encode(use)\n            ln2 = min(MAX_TOK,len(tk))\n            X[k,ln] = 2\n            X[k,ln+1:ln+ln2] = tk[1:MAX_TOK]\n            y2[k] = self.df.target.values[rw]\n        y1 = df.target.values\n            \n        for k in range(len(indexes)):\n            # CONVERT BRADLEY-TERRY VALUES INTO PROBABILITIES\n            # THAT FIRST EXCERPT IS MORE DIFFICULT THAN SECOND EXCERPT\n            t = np.exp( y1[k]-y2[k] )\n            y[k,] = t\/(t+1)\n          \n        return X,y","1d5fc3ab":"fake_data = pd.DataFrame(columns=['id','excerpt','target'])\nfake_data.loc[0] = [1,'Sentence one',1]\nfake_data.loc[1] = [2,'Sentence two',-1]\nfake_data.head()","a415a05f":"trn = DataGenerator(fake_data)\nfor b in trn:\n    break\nb[0][0,:16]","530427ac":"tokenizer.encode('Sentence one','Sentence two')","1b10f1db":"def build_model(use_cls_token=False):\n    \n    tokens = tf.keras.layers.Input(shape=(MAX_TOK*2,), name = 'tokens', dtype=tf.int32)\n    masks = tf.cast( tokens!=1, dtype=tf.int32 )\n    \n    bert_model = TFRobertaModel.from_pretrained('..\/input\/tfroberta-base') \n    \n    x = bert_model(tokens, attention_mask=masks)  \n\n    # BUILD HEAD WITH EITHER CLS TOKEN OR MEAN LAST LAYER TOKENS\n    if use_cls_token:\n        x = x[0][:,0,:]\n        x = tf.keras.layers.Dense(768,activation='tanh')(x)\n    else: #use mean last layer\n        x = tf.keras.layers.GlobalAveragePooling1D()(x[0])\n        \n    x = tf.keras.layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n    \n    model = tf.keras.Model(inputs=tokens, outputs=x)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 2e-5),\n                  loss = [tf.keras.losses.BinaryCrossentropy()]) \n    \n    return model, bert_model","a98bdaaf":"if LOAD_MODEL_PATH is None:\n    skf = KFold(n_splits=5, random_state=42, shuffle=True)\n\n    for fold, (idx_t, idx_v) in enumerate(skf.split(train)):\n    \n        print('#'*25)\n        print('### FOLD',fold+1)\n        print('#'*25)\n    \n        model,_ = build_model()\n    \n        train_gen = DataGenerator(train.iloc[idx_t], shuffle=True, batch_size=8, augment=False)\n        val_gen = DataGenerator(train.iloc[idx_v], batch_size=16)\n        \n        sv = tf.keras.callbacks.ModelCheckpoint(\n            'RoBERTa_Base_v%i_v%i_fold%i.h5'%(VER,VER2,fold), monitor='val_loss', verbose=1, \n            save_best_only=True, save_weights_only=True, mode='auto', save_freq='epoch'\n        )\n        rop = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', factor=0.3, patience=2, verbose=1,\n            mode='auto', min_delta=0.0001, cooldown=0, min_lr=0\n        )\n        \n        model.fit(train_gen, validation_data = val_gen,\n            epochs=10, verbose=1, callbacks=[sv,rop],\n            use_multiprocessing=True, workers=2)","7bdb0102":"mn = train.target.mean()\nst = train.target.std()\nRANGE_LOW = mn-st\nRANGE_HIGH = mn+st\nCOMPARE = 8","e180c836":"class DataGenerator2(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df1, df2, batch_size=COMPARE, tokenizer=tokenizer, log=[]): \n\n        self.df1 = df1.reset_index(drop=True)\n        self.df2 = df2.reset_index(drop=True)\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.log = log\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return len(self.df1)*2\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        X = self.__data_generation(index)    \n        return X\n    \n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        pass\n\n    def __data_generation(self,index):\n        'Generates data containing batch_size samples' \n        \n        idx = index\/\/2\n        mode = index%2\n        X = np.ones((self.batch_size,MAX_TOK*2),dtype='int32')\n        \n        use1 = self.df1.excerpt.values[idx]\n        if mode==0:\n            tk = self.tokenizer.encode(use1)\n            ln = min(MAX_TOK,len(tk))\n            for k in range(self.batch_size):\n                \n                # FIRST SENTENCE\n                X[k,:ln] = tk[:MAX_TOK]\n            \n                # SECOND SENTENCE\n                rw = self.df2.loc[(self.df2.target>RANGE_LOW)&(self.df2.target<RANGE_HIGH)].sample(1).index[0]\n                use = self.df2.excerpt.values[rw]\n                tk2 = self.tokenizer.encode(use)\n                ln2 = min(MAX_TOK,len(tk2))\n                X[k,ln] = 2\n                X[k,ln+1:ln+ln2] = tk2[1:MAX_TOK]\n                \n                # RECORD TARGET\n                t = self.df2.target.values[rw]\n                self.log.append(t)\n            \n        else:\n            tk = self.tokenizer.encode(use1)\n            ln2 = min(MAX_TOK,len(tk))\n            for k in range(self.batch_size):\n                \n                # FIRST SENTENCE\n                rw = self.df2.loc[(self.df2.target>RANGE_LOW)&(self.df2.target<RANGE_HIGH)].sample(1).index[0]\n                use = self.df2.excerpt.values[rw]\n                tk2 = self.tokenizer.encode(use)\n                ln = min(MAX_TOK,len(tk2))\n                X[k,:ln] = tk2[:MAX_TOK]\n            \n                # SECOND SENTENCE\n                X[k,ln] = 2\n                X[k,ln+1:ln+ln2] = tk[1:MAX_TOK]\n                \n                # RECORD TARGET\n                t = self.df2.target.values[rw]\n                self.log.append(t)\n                                  \n        return X","f3a3ecf6":"if PREDICT_TEST:\n    FOLDS = 5\n    skf = KFold(n_splits=FOLDS, random_state=42, shuffle=True)\n    test_preds = np.zeros(len(test))\n\n    for fold, (idx_t, idx_v) in enumerate(skf.split(train)):\n\n        log = []\n        test_gen = DataGenerator2(test,train.iloc[idx_t], log=log)\n        model,_ = build_model()\n\n        if LOAD_MODEL_PATH is not None:\n            model.load_weights(LOAD_MODEL_PATH+'RoBERTa_Base_v%i_v%i_fold%i.h5'%(VER,VER2,fold))\n        else:\n            model.load_weights('RoBERTa_Base_v%i_v%i_fold%i.h5'%(VER,VER2,fold))\n\n        pr = model.predict(test_gen, verbose=1)\n        log = log[COMPARE:] #remove first batch since dataloader double wrote\n    \n        # COMPUTE PREDICTIONS\n        log = np.array(log)\n        for k in range(len(test)):\n            #if k%10==0: print(k,', ',end='')\n        \n            a = COMPARE*2*k\n            preds2 = []\n    \n            t1 = log[a:a+COMPARE]\n            p1 = pr[a:a+COMPARE,0]\n            for j in range(COMPARE):\n                tmp = np.log( p1[j] \/ (1-p1[j]) ) + t1[j]\n                preds2.append(tmp)\n    \n            a += COMPARE\n            t2 = log[a:a+COMPARE]\n            p2 = pr[a:a+COMPARE,0]\n            for j in range(COMPARE):\n                tmp = t2[j] - np.log( p2[j] \/ (1-p2[j]) ) \n                preds2.append(tmp)\n    \n            test_preds[k] += np.mean(preds2) \/ FOLDS","33e0b64e":"test['target'] = test_preds\ntest[['id','target']].to_csv('submission.csv',index=False)","c6f5626e":"sub = pd.read_csv('submission.csv')\nsub.head()","185e5c1b":"if COMPUTE_OOF:\n    skf = KFold(n_splits=5, random_state=42, shuffle=True)\n    oof = np.zeros(len(train))\n\n    for fold, (idx_t, idx_v) in enumerate(skf.split(train)):\n\n        log = []\n        val_gen = DataGenerator2(train.iloc[idx_v],train.iloc[idx_t], log=log)\n        model,_ = build_model()\n\n        if LOAD_MODEL_PATH is not None:\n            model.load_weights(LOAD_MODEL_PATH+'RoBERTa_Base_v%i_v%i_fold%i.h5'%(VER,VER2,fold))\n        else:\n            model.load_weights('RoBERTa_Base_v%i_v%i_fold%i.h5'%(VER,VER2,fold))\n\n        pr = model.predict(val_gen, verbose=1)\n        log = log[COMPARE:] #remove first batch since dataloader double wrote\n    \n        # COMPUTE PREDICTIONS\n        preds = np.zeros(len(idx_v))\n        log = np.array(log)\n        for k in range(len(idx_v)):\n            #if k%10==0: print(k,', ',end='')\n        \n            a = COMPARE*2*k\n            preds2 = []\n    \n            t1 = log[a:a+COMPARE]\n            p1 = pr[a:a+COMPARE,0]\n            for j in range(COMPARE):\n                tmp = np.log( p1[j] \/ (1-p1[j]) ) + t1[j]\n                preds2.append(tmp)\n    \n            a += COMPARE\n            t2 = log[a:a+COMPARE]\n            p2 = pr[a:a+COMPARE,0]\n            for j in range(COMPARE):\n                tmp = t2[j] - np.log( p2[j] \/ (1-p2[j]) ) \n                preds2.append(tmp)\n    \n            preds[k] = np.mean(preds2) \n \n        rsme = np.sqrt(np.mean( (train.target.values[idx_v] - preds)**2 ))\n        print(); print(f'FOLD {fold+1} OOF rsme',rsme)\n    \n        oof[idx_v] = preds\n    \n    \n    print(); print('#'*25)\n    rsme = np.sqrt(np.mean( (train.target.values - oof)**2 ))\n    print('OOF rsme',rsme)\n    print('#'*25)","14702b99":"COMPARE = 32 # we will compare twice this many\nDISPLAY = 5 # we will display thrice this many\n\nif COMPUTE_OOF:\n    # GET FIRST FOLD\n    skf = KFold(n_splits=5, random_state=42, shuffle=True)\n    for fold, (idx_t, idx_v) in enumerate(skf.split(train)): break\n        \n    # PREDICT 15 EXCERPTS FROM FIRST FOLD\n    log = [] # save compared targets here\n    val_gen = DataGenerator2(train.iloc[idx_v[:DISPLAY*3]],train.iloc[idx_t], batch_size=COMPARE, log=log)\n    pr = model.predict(val_gen, verbose=1)\n    log = log[COMPARE:] # remove first batch of targets because dataloader wrote twice","7c318851":"if COMPUTE_OOF:\n    log = np.array(log)\n    for k in range(DISPLAY*3): \n        if k%3==0: plt.figure(figsize=(20,5))\n\n        # FIND ALL EXCERPTS OVER AND UNDER\n        # WITH LEFT COMPARE\n        a = COMPARE*2*k\n        t1 = log[a:a+COMPARE]\n        p1 = pr[a:a+COMPARE,0]\n        over1 = t1[p1<0.5]\n        under1 = t1[p1>0.5]\n    \n        # FIND ALL EXCERPTS OVER AND UNDER\n        # WITH RIGHT COMPARE\n        a += COMPARE\n        t2 = log[a:a+COMPARE]\n        p2 = pr[a:a+COMPARE,0]\n        over2 = t2[p2>0.5]\n        under2 = t2[p2<0.5]    \n        over = np.concatenate([over1,over2])\n        under = np.concatenate([under1,under2])\n    \n        # PLOT EXCERPTS OVER AND UNDER\n        plt.subplot(1,3,k%3+1)\n        sns.kdeplot(under,label='under')\n        sns.kdeplot(over,label='over')\n        x = train.target.values[idx_v[k]]\n        yy = plt.ylim()\n        plt.plot([x,x],[yy[0],yy[1]],color='black')\n        plt.legend()\n        if k%3==2: plt.show()","c9f4f8b5":"# Load Train and Test Data","2a7e5929":"# Sentence Comparison Approach - CommonLit Readability Comp - CV 0.495\nAll public discussions and notebooks use regression to predict the Bradley-Terry target. In this notebook, we will convert the Bradley-Terry value back into a probability that one excerpt is easier to read than another excerpt. Then we will train a classical binary classification model on pairs of excerpts to predict this probability. Then we will convert the predicted probabiltiy back into a Bradley-Terry score. This model achieves a solid CV 0.495\n\nBradley-Terry is referenced by the host [here][1] and explained by Wikipedia here [here][2]\n\nWe will build a transformer model that takes two excerpts as input and outputs the probability that the first excerpt is more difficult to read than the second excerpt. Given two excerpts with Bradley-Terry score `t1` and `t2`. Then the probability that excerpt 1 is more difficult to read than excerpt 2 is\n\n$$ prob = \\frac{exp(t_1-t_2)}{1+exp(t_1-t_2)} $$\n\nWe will use this formula to encode the comp data targets during sentence pair training. And we will use this formula backwards to decode during inference.\n\n[1]: https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240423\n[2]: https:\/\/en.wikipedia.org\/wiki\/Bradley%E2%80%93Terry_model","3ed75f4d":"# Train Model","ebecbcc0":"# Compute OOF Score\nFor each excerpt in the validation fold, we will compare it with 16 excerpts from the train fold. Based on these 16 comparisons, we will compute its Bradley-Terry score.","230b88ff":"# DataLoader\nWe need a dataloader to feed random pairs of sentences into our NLP model","87c970ec":"# Write Submission File","b63dcb47":"# Predict Test Data","777e7d02":"# Display Bradley-Terry values (i.e. comp targets)","8118ace6":"# Visualize Sentence Comparison Inference\nTo predict one test excerpt, we infer many comparisons. Let's plot those comparisons. Below each plot is the inference of one validation OOF excerpt. We will compare each validation excerpt with 64 other train excerpts. (We don't need this many for inference but we will use a lot here to make better plots). Then we will plot (1) a histogram of all excerpts easier to read among the 64, (2) a histogram of all excerpts more difficult to read among the 64, and (3) a black line indicating the true target of the OOF excerpt we are comparing with 64.","ee829155":"# Make Sentence Compare Model","ece22ab8":"# Load Libraries","588121d9":"# Enable GPU mixed precision\nFor fast training and larger batch size capability, we will use mixed precision. On Kaggle's P100 GPU we do not get much benefit but if we use V100 or A100 we will get benefit","8ff62b26":"# Inference\nInference is a little tricky. For each test excerpt that we wish to predict, we must compare it with 16 train excerpts. Then we will deduce it's Bradley-Terry value from those 16 comparisons","eaf5c9ef":"# Display Dataloader\nWe notice that our dataloader returns pairs of encoded sentences. Roberta tokenizer encodes sentences together by first using a cls token of `[0]`. Then encode first sentence. Then use two separator tokens of `[2][2]`. Then encode the second sentence. Then a final separator token `[2]`."}}