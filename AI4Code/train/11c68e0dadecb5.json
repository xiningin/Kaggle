{"cell_type":{"c4c1360d":"code","3d1755a8":"code","684c46c3":"code","e3ce1f83":"code","957f3a2b":"code","7934d487":"code","0c1afaa4":"code","9024bce3":"code","048018a7":"code","01385aef":"code","8e6d3a0b":"code","cd0b17dd":"code","9a41b271":"code","15af0512":"code","6bd9e6ef":"code","dcc596a6":"code","69b2c597":"code","b5f36e7b":"code","f465f213":"code","eb9fce86":"code","cb10eba1":"code","a49bc4a3":"code","ab12986a":"code","9a4e82bf":"markdown","921bc385":"markdown","75536587":"markdown","f46ccd2e":"markdown","c3b20fb4":"markdown","7dbea4e2":"markdown","679caaef":"markdown","87a51a03":"markdown","1f9e29dd":"markdown","fa34b375":"markdown","078f2052":"markdown","e4f6baba":"markdown","2e1e48ff":"markdown"},"source":{"c4c1360d":"import numpy as np\nimport pandas as pd\nimport scipy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\nfrom tqdm import tqdm\nfrom torchmetrics import AUROC\nimport matplotlib.pyplot as plt\nimport gc, sys, random, warnings\ngc.enable()\nwarnings.filterwarnings(\"ignore\")","3d1755a8":"!pip install tab-transformer-pytorch\nfrom tab_transformer_pytorch import TabTransformer","684c46c3":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\nseed_all(123)","e3ce1f83":"train_df = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv', nrows = 500000) # memory issues on kaggle cpu\ntest_df =  pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\n\ntrain_df.drop('id',1, inplace=True)\ntest_df.drop('id',1, inplace=True)","957f3a2b":"cont_cols = train_df.columns[:242]\ncat_cols = train_df.columns[242:-1]","7934d487":"train_scaled, test_scaled = train_df.copy(), test_df.copy()\nfor col in cont_cols:\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_scaled[col].values)\n    vec_len_test = len(test_scaled[col].values)\n    raw_vec = train_scaled[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_scaled[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_scaled[col] = transformer.transform(test_scaled[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","0c1afaa4":"fig, axs = plt.subplots(2)\naxs[0].title.set_text('F0 dist before Gauss Rank')\naxs[0].hist(train_df.f0, bins = 100)\n\naxs[1].title.set_text('F0 dist after Gauss Rank')\naxs[1].hist(train_scaled.f0, bins = 100)\nplt.tight_layout()\nplt.show()","9024bce3":"del(train_df); del (test_df); gc.collect()","048018a7":"disc = KBinsDiscretizer(n_bins=50, encode='ordinal',strategy='uniform')\ntrain_scaled[cont_cols] = disc.fit_transform(train_scaled[cont_cols])\ntest_scaled[cont_cols] = disc.transform(test_scaled[cont_cols])","01385aef":"y_train = train_scaled.target.values\ntrain_scaled = train_scaled.drop('target', 1).values\ntest_scaled = test_scaled.values","8e6d3a0b":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","cd0b17dd":"class EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001, verbose = None):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        self.verbose = verbose\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            if self.verbose:\n                print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            if self.verbose:\n                print('Validation score improved ({:.4f} --> {:.4f}). Saving model!'.format(self.val_score, epoch_score))\n                \n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","9a41b271":"# def split_df(df):\n#     _cont = df.filter(cont_cols).values\n#     _cat = df.filter(cat_cols).values\n#     return _cont, _cat\n\n# train_cont, train_cat = split_df(train_scaled)\n# test_cont, test_cat = split_df(test_scaled)","15af0512":"# Updated to pass a contant cont value in this version\nclass TabDataset(Dataset):\n    def __init__(self, cat, target = None):\n        super().__init__()\n        self.cat = cat\n        self.target = target\n        \n    def __len__(self):\n        return len(self.cat)\n    \n    def __getitem__(self, idx):\n        cat = self.cat[idx]\n        \n        _dict = {'cont': torch.ones(1),\n                 'cat': torch.LongTensor(cat)}\n        \n        if self.target is not None:\n            target = self.target[idx].item()\n            _dict.update({'target': torch.tensor(target, dtype = torch.float)})\n        \n        return _dict","6bd9e6ef":"class Trainer:\n    def __init__(self, model, device, loss_fn, opt, scheduler = None):\n        self.model = model\n        self.device = device\n        self.loss_fn = loss_fn\n        self.opt = opt\n        self.scheduler = scheduler\n        \n    def fit_one_epoch(self, dl):\n        self.model.train()\n        losses = AverageMeter()\n        prog_bar = tqdm(enumerate(dl), total = len(dl), file=sys.stdout, leave = False)\n        \n        for bi, d in prog_bar:\n            cont = d[\"cont\"].to(self.device)\n            cat = d['cat'].to(self.device)\n            target = d['target'].to(self.device)\n            \n            out = self.model(cat, cont)\n            loss = self.loss_fn(out.squeeze(-1), target)\n            prog_bar.set_description('loss: {:.2f}'.format(loss.item()))\n            losses.update(loss.item(), cont.size(0))\n            loss.backward()\n            self.opt.step()\n            \n            if self.scheduler: \n                self.scheduler.step()\n                    \n            self.opt.zero_grad()\n            \n    def eval_one_epoch(self, dl, **kwargs):\n        self.model.eval()\n        losses = AverageMeter()\n        metric = AUROC()\n        prog_bar = tqdm(enumerate(dl), total = len(dl), file=sys.stdout, leave = False)\n        \n        for bi, d in prog_bar:  \n            cont = d[\"cont\"].to(self.device)\n            cat = d['cat'].to(self.device)\n            target = d['target'].to(self.device)\n            \n            with torch.no_grad():\n                out = self.model(cat, cont)\n                loss = self.loss_fn(out.squeeze(-1), target)\n                if metric:\n                    auroc = metric(out.squeeze(-1), target.int())\n                \n                losses.update(loss.item(), cont.size(0))\n        auroc = metric.compute()\n        print(f\"F{kwargs['fold']} E{str(kwargs['epoch']):2s}\"\\\n              f\"  Valid Loss: {losses.avg:.4f}  AUROC Score: {auroc:.4f}\")\n        return auroc.cpu() if metric else losses.avg","dcc596a6":"class cfg:\n    bs = 400\n    n_splits = 5\n    seed = 2021\n    epochs = 3\n    lr = 2e-5\n    checkpoint = lambda fold: f'full_cat_{fold}.pt'\n    \nkfold = StratifiedKFold(n_splits = cfg.n_splits, \n                        random_state = cfg.seed, \n                        shuffle = True)\nsplits = [*kfold.split(X = train_scaled, y = y_train)]","69b2c597":"transformer_cfg = {\n    'categories' : [50]*242 + [2]*43,           # iterable with the number of unique values for categoric feature\n    'num_continuous' : 1,                       # continuous dimensions in data\n    'dim' : 32,                                 # hidden dim, paper set at 32\n    'dim_out' : 1,                              # binary prediction\n    'depth' : 3,                                # depth, paper recommended 6\n    'heads' : 6,                                # heads, paper recommends 8\n    'attn_dropout' : 0.1,                       # post-attention dropout\n    'ff_dropout' : 0.1,                         # feed forward dropout\n    'mlp_hidden_mults' : (4, 2),                # relative multiples of each hidden dimension of the last mlp to logits\n    'mlp_act' : nn.GELU(),                      # activation for final mlp, defaults to relu\n    'continuous_mean_std' : torch.randn(1, 2)   # normalize the continuous values before layer norm (optional)\n}","b5f36e7b":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","f465f213":"def create_dataloaders(fold):\n    train_idx, valid_idx = splits[fold]\n    \n    _xtr, _ytr = train_scaled[train_idx], y_train[train_idx]\n    _xval, _yval = train_scaled[valid_idx], y_train[valid_idx]\n    \n    train_ds = TabDataset(cat = _xtr, target = _ytr)\n    valid_ds = TabDataset(cat = _xval, target = _yval)\n                          \n    train_dl = DataLoader(train_ds, batch_size = cfg.bs, shuffle = True)\n    valid_dl = DataLoader(valid_ds, batch_size = cfg.bs, shuffle = False)\n    \n    return train_dl, valid_dl","eb9fce86":"def train_fold(fold, epochs = 20):\n    train_dl, valid_dl = create_dataloaders(fold)\n    es = EarlyStopping(patience = 7, mode=\"max\", verbose = False)\n    \n    model = TabTransformer(**transformer_cfg).to(device)\n    \n    opt = torch.optim.AdamW(model.parameters(), lr = cfg.lr)\n\n    trainer = Trainer(model, \n                      device, \n                      loss_fn=nn.BCEWithLogitsLoss(),\n                      opt = opt,\n                      scheduler = None,\n                     )\n    \n    for epoch in range(epochs):\n        trainer.fit_one_epoch(train_dl)\n        valid_loss = trainer.eval_one_epoch(valid_dl, fold = fold, epoch = epoch)\n        \n        es(valid_loss, trainer.model, model_path = cfg.checkpoint(fold))\n        \n        if es.early_stop:\n            break","cb10eba1":"for fold in range(cfg.n_splits):\n    train_fold(fold, cfg.epochs)\n    torch.cuda.empty_cache()\n    gc.collect()","a49bc4a3":"y_pred = torch.zeros(len(test_scaled), 1).to(device)\ntest_ds = TabDataset(cat = test_scaled)\ntest_dl = DataLoader(test_ds, batch_size = cfg.bs, shuffle = False)\n\nwith torch.no_grad():\n    for fold in range(cfg.n_splits):\n        preds = []\n        model = TabTransformer(**transformer_cfg).to(device)\n        state_dict = cfg.checkpoint(fold)\n        model.load_state_dict(torch.load(state_dict))\n        model.eval()\n        \n        for d in test_dl:\n            cont = d[\"cont\"].to(device)\n            cat = d['cat'].to(device)\n            out = model(cat, cont)\n            preds.append(out)\n            \n        preds = torch.vstack(preds)\n        y_pred += preds \/ cfg.n_splits","ab12986a":"sub = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\nsub.iloc[:,1] = y_pred.cpu()\nsub = sub.set_index('id')\nsub.to_csv('submission.csv')","9a4e82bf":"## Exploration and Feature Engineering","921bc385":"## Trainer\nThe Trainer manages context and training for single epochs  \n  \nA `metric` object is created in evaluation to compute AUROC, which is used for saving weights\/early stopping  ","75536587":"## Prediction","f46ccd2e":"We'll scale the data here with a Gauss Rank transform, with code originally from the MOA competition:  \n  https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-0-01859-rankgauss-pca-nn","c3b20fb4":"## Model Architecture","7dbea4e2":"## Dataset","679caaef":"The TabTransformer uses both categorical and continuous features to model tabular data.  \nOnly the categorical features\/column embeddings are passed to the self-attention layers with this architecture.   \n  \nIn our case, only features $F242 ... F284$ are passed to the transformer blocks.  \n  It could be interesting to experiment with discretising the continuous features and passing them to the transformer as well","87a51a03":"## Utilities","1f9e29dd":"## Training","fa34b375":"<img align=\"left\" src=https:\/\/raw.githubusercontent.com\/lucidrains\/tab-transformer-pytorch\/main\/tab.png>","078f2052":"`train_df` has a mix of continuous and potentially one-hot encoded categorical features.  \nInteger features start at index 242. We'll save the columns for preprocessing\/splitting later","e4f6baba":"## Introduction\nThis notebook experiments with the TabTransformer, which learns contextual embeddings to achieve higher prediction accuracy.  \nIn a supervised-only setting, the architectures performs on par with GBDTs from benchmarks.  \n  \nImprovements so far, include:\n - Discretising continuous features to pass to transformer blocks\n - Training with a flat learning rate\n - This version experiments with feeding only column embeddings to the transformer \n \nSo far the best local CV is ~*85* AUROC, using 242 continuous columns, and 242 (cont) + 42 (cat) categorical features.    \nThanks for reading\n ","2e1e48ff":"## Discretising Continuous Features \nHere we discretise the scaled features using a `KBinsDiscretizer`  \nFor this version we'll replace continuous features inplace"}}