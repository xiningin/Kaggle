{"cell_type":{"14cc5a7f":"code","98ced17c":"code","9e963308":"code","3b9dd58f":"code","a53121a9":"code","09682da0":"code","054358f2":"code","77141ec9":"code","1c9cfde6":"code","2c475fb2":"code","2eecef60":"code","42eab8e5":"code","56e492ad":"code","f8ece940":"code","ed26575e":"code","b845891d":"code","e4bf0aef":"code","084891dc":"code","b671202f":"code","1d954638":"code","b98a209e":"code","ed15c1b7":"markdown","8d81609a":"markdown","e3193a6a":"markdown","8d05834f":"markdown","74c54e2d":"markdown","fcdb095f":"markdown","b392e942":"markdown","c580dd5f":"markdown","6fbcca6e":"markdown","3f793884":"markdown","83d54276":"markdown","e0825865":"markdown","7056432c":"markdown","8a6131ae":"markdown","5b7a5546":"markdown","862e2833":"markdown","50c07fcb":"markdown","7478fc26":"markdown","d17a121b":"markdown","8b4ac084":"markdown","d4b692b8":"markdown","cfb082fd":"markdown","bf932706":"markdown","9d21315a":"markdown","4488d1ed":"markdown","4ead34c3":"markdown","7b277af6":"markdown"},"source":{"14cc5a7f":"import os\nimport subprocess\n\nimport json\nimport csv\n\nimport uuid\n\nfrom IPython.display import display_javascript, display_html, display\n\nimport pandas as pd\nimport numpy as np\n\nfrom datetime import datetime, date, time","98ced17c":"from platform import python_version\nprint(python_version())","9e963308":"pip install git+https:\/\/github.com\/JustAnotherArchivist\/snscrape.git","3b9dd58f":"import snscrape.modules.twitter as sntwitter","a53121a9":"#Run the snscrape help to see what options \/ parameters we can use\ncmd = 'snscrape --help'\n\n#This is similar to running os.system(cmd), which would show the output of running the command in the Terminal\n#window from where I started my Jupyter Notebook (which is what I used to develop this code)\n#By using subprocees, I capture the commands's output into a variable, whose content I can then print here.\noutput = subprocess.check_output(cmd, shell=True)\n                                 \nprint(output.decode(\"utf-8\"))                                 ","09682da0":"out_folder = '..\/working'\nin_folder = '..\/input'\n\njson_filename = out_folder + '\/pisa2018-query-tweets.json'\n\n#Using the OS library to call CLI commands in Python\nos.system(f'snscrape --max-results 5000 --jsonl --progress --since 2018-12-01 twitter-search \"#pisa2018 lang:fr until:2019-12-31\" > {json_filename}')","054358f2":"start = date(2016, 12, 5)\nstart = start.strftime('%Y-%m-%d')\n\nstop = date(2016, 12, 14)\nstop = stop.strftime('%Y-%m-%d')\n\nkeyword = 'pisa2018'","77141ec9":"maxTweets = 1000\n\n#We are going to write the data into a csv file\nfilename = out_folder + '\/' + keyword + start + '-' + stop + '.csv'\ncsvFile = open(filename, 'a', newline='', encoding='utf8')\n\n#We write to the csv file by using csv writer\ncsvWriter = csv.writer(csvFile)\ncsvWriter.writerow(['id','date','tweet'])\n\n#I will use the following Twitter search operators:\n# since - start date for Tweets collection \n# stop  - stop date for Tweets collection\n# -filter:links - not very clear what this does, from Twitter search operators documentation: https:\/\/developer.twitter.com\/en\/docs\/twitter-api\/v1\/rules-and-filtering\/search-operators\n#                 but it looks like this will exclude tweets with links from the search results\n# -filter:replies - removes @reply tweets from search results\nfor i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + 'since:' +  start + ' until:' + \\\n                                                        stop + ' -filter:links -filter:replies').get_items()):\n    if i > maxTweets :\n        break\n    csvWriter.writerow([tweet.id, tweet.date, tweet.content])\n\ncsvFile.close()","1c9cfde6":"filename = '..\/input\/example\/example.json'\n  \nwith open(filename) as json_file:\n    data = json.load(json_file)\n\nclass RenderJSON(object):\n    def __init__(self, json_data):\n        if isinstance(json_data, dict) or isinstance(json_data, list):\n            self.json_str = json.dumps(json_data)\n        else:\n            self.json_str = json_data\n        self.uuid = str(uuid.uuid4())\n\n    def _ipython_display_(self):\n        display_html('<div id=\"{}\" style=\"color: #000000; background-color: #ffffff; height: 600px; width:100%;font: 12px\/18px monospace !important;\"><\/div>'.format(self.uuid), raw=True)\n        display_javascript(\"\"\"\n        require([\"https:\/\/rawgit.com\/caldwell\/renderjson\/master\/renderjson.js\"], function() {\n            renderjson.set_show_to_level(2);\n            document.getElementById('%s').appendChild(renderjson(%s))\n        });\n      \"\"\" % (self.uuid, self.json_str), raw=True)\n\nRenderJSON([data])","2c475fb2":"filename = 'pisa2018-query-tweets'\nfile = in_folder + '\/pisa2018-keyword-in-tweeter-archive\/' + filename\ntweets_df = pd.read_json(file +'.json', lines=True)","2eecef60":"tweets_df.shape","42eab8e5":"tweets_df.head(3)","56e492ad":"tweets_df.to_csv(out_folder + '\/' + filename +'.csv', index = False)","f8ece940":"num = sum(1 for line in open(file +'.json'))\nprint(num)","ed26575e":"substring = 'justesse'\n\ncount = 0\nf = open(file + '.json', 'r')\nfor i, line in enumerate(f):\n    if substring in line:\n        count = count + 1\n        obj = json.loads(line)\n        print(f'Tweet number {count}: {obj[\"content\"]}')\nprint(count)\nf.close()","b845891d":"tweets_df.iloc[0].content","e4bf0aef":"tweets_df.iloc[0].outlinks","084891dc":"popularity_columns = ['replyCount', 'retweetCount', 'likeCount', 'quoteCount']\ntweets_df.iloc[0][popularity_columns]","b671202f":"tweets_df.iloc[tweets_df.retweetCount.idxmax()][['content','retweetCount']]","1d954638":"pip install jupyter_to_medium","b98a209e":"#uncomment and customize the code below in order to publish your notebook on your Medium account\n'''\nimport jupyter_to_medium as jtm\njtm.publish('Scraping historical tweets without a Twitter Developer Account.ipynb',\n            integration_token='paste_your_own_token',\n            pub_name=None,\n            title='Scraping historical tweets without a Twitter Developer Account',\n            tags=['scraping with Python', 'Twitter archive'],\n            publish_status='draft',\n            notify_followers=False,\n            license='all-rights-reserved',\n            canonical_url=None,\n            chrome_path=None,\n            save_markdown=False,\n            table_conversion='chrome'\n            )\n'''","ed15c1b7":"We can gauge the popularity of a tweet through these features:\n- replyCount\n- retweetCount\n- likeCount\n- quoteCount","8d81609a":"First, let's check the current Python version, as snscrape documentation mentions **it requires Python 3.8**","e3193a6a":"The tool we will use:\n- snscrape\n\nWhat you need: \n- Python 3.8\n\nWhat you don't need:\n- a Twitter Developer Account\n\n\nFor a research project related to public discourse about results on international large scale assessments I needed to scrape historical tweets, going back all the way to the begining of Twitter. This is how I discovered **snscrape**, a wonderful tool, easy to setup and use. \n\nI didn't find snscrape from the start, initially I was reading through the intricate details of Twitter Developer Account, application procedure, different levels of access, limits etc etc. But luckily a friend recommended snscrape and suddenly the task of collecting tweets became extremely easy.\n\nSnscrape is a popular tool with social scientists for Tweets collection, at least in 2021. Apparently, it bypasses several limitations of the Twitter API.  \nThe prettiest thing is that you don't need Twitter developer account credentials (like you do with <a href='https:\/\/www.tweepy.org\/'>Tweepy<\/a>, for example)\n\n<font color='red'>Mind that installing and running snscrape does not work in Kaggle, at the moment, because Kaggle does not offer Python 3.8.  \n    You will have to download this Notebook and run it in Google Colab, Deepnote or your local Anaconda installation, for example,<\/font>","8d05834f":"And that's about it for a quick intro to scraping tweets without the need to apply for a Twitter Developer Account and with no limitations for the maximum number of tweets we can get or for how far back in time we can go.\n\n## 9. What next ? Sentiment analysis\n\nWhat to do next with the tweets you just scraped ? In my case, I was very interested in <a href='https:\/\/www.kaggle.com\/mishki\/twitter-sentiment-analysis-using-nlp-techniques'>NLP for sentiment analysis of tweets<\/a>, or you may try topic modelling using Latent Dirichlet Allocation (LDA) or build a network graph from this data and use network analysis methods on it.","74c54e2d":"## 4. Using snscrape Python wrapper","fcdb095f":"Find the most retweeted tweet in our dataset.","b392e942":"Links mentioned in the tweet are also listed separately in the outlinks column.","c580dd5f":"Let's have a look at all the information that is available for every single tweet scraped using snscrape.  \n\nFor this code I am using one example file that I made precidely for this, which contains a single JSON object. If you want to use a JSON file created with the steps above, you need to make some changes before you can run json.loads on it, as explained in <a href='https:\/\/stackoverflow.com\/questions\/21058935\/python-json-loads-shows-valueerror-extra-data'>this stackoverflow discussion<\/a>.\n\nThe solution for pretty printing JSON data inside a Jupyter Notebook comes from <a href='https:\/\/gist.github.com\/nerevar\/a068ee373e22391ad3a1413b3e554fb5'>this github project<\/a>.\n\nClick on the + icons to expand the contents of that particular item.","6fbcca6e":"The actual content of the tweet is available through test_df['content'] or test_df.content  \n\nrenderedContent seems to contain the same information as content.","3f793884":"## Table of contents\n\n\n1. [Installing snscrape](#1.-Installing-snscrape)\n2. [How to use snscrape](#2.-How-to-use-snscrape)\n3. [Calling snscrape CLI commands from Python Notebook](#3.-Calling-snscrape-CLI-commands-from-Python-Notebook)\n4. [Using snscrape Python wrapper](#4.-Using-snscrape-Python-wrapper)\n5. [Tweets meta-information gathered with snscrape](#5.-Tweets-meta-information-gathered-with-snscrape) \n6. [Dataset manipulation: JSON, CSV and Pandas DataFrame](#6.-Dataset-manipulation:-JSON,-CSV-and-Pandas-DataFrame)\n7. [Basic exploration of our collected dataset of tweets](#7.-Basic-exploration-of-our-collected-dataset-of-tweets)\n8. [Bonus: Publishing your Jupyter Notebook on Medium](#8.-Bonus:-Publishing-your-Jupyter-Notebook-on-Medium)\n9. [What next ? Sentiment analysis](#9.-What-next-?-Sentiment-analysis)","83d54276":"### Basic introduction to tweets\n\nTweets are 280 character messages (hence the name 'microblogging'). Just like on other social media platforms, you need to create an account and then you can start participating to the tweetverse.  \n\nTweets act as short status updates. Tweets appear on timelines. Timelines are collections of tweets sorted in a chronological order. On your account's home page, you're shown a timeline where tweets from people you follow will be displayed. \n\nYou can post your own brand new tweet, retweet an already existing tweet (which means ou just share the exact same tweet) or quote an existing tweet (similar to retweeting, but you can add your own comment to it). \n\nYou can also reply to someone else's tweets or 'like' them.  \n\nTweets often contain **entities**, which are mentions of:\n- other users, which appear in the form of @other_user\n- places\n- urls\n- media that was attached to the tweet\n- hashtags, that look like #example_hashtag. Hashtags are just a way to apply a label on a tweet. If I'm tweeting something about results of PISA, the Programme for International Student Assessment, I will likely use #oecdpisa in my tweet, for example.","e0825865":"## 6. Dataset manipulation: JSON, CSV and Pandas DataFrame","7056432c":"# <center> Scraping historical tweets without a Twitter Developer Account\n\n![Image by Tumisu from Pixabay](https:\/\/mihaelagrigore.info\/wp-content\/uploads\/2021\/04\/scrape-historical-tweets-and-other-social-networks.jpg)","8a6131ae":"## 8. Bonus: Publishing your Jupyter Notebook on Medium","5b7a5546":"We begin with some standard library imports.","862e2833":"## 2. How to use snscrape\n\n- through its command line interface (CLI) in the command prompt terminal.\n- use Python to run the CLI commands from a Jupyter notebook, for example (if you don't want to use the terminal to run commands)\n- or use the official snscrape Python wrapper. The Python wrapper is not well documented, unfortunately.\n\nParameters you can use:\n- --jsonl : get the data into jsonl format\n- --progress\n- --max-results : limit the number of tweets to collect\n- --with-entity : Include the entity (e.g. user, channel) as the first output item (default: False)\n- --since DATETIME : Only return results newer than DATETIME (default: None)\n- --progress : Report progress on stderr (default: False)","50c07fcb":"### Saving DataFrame to CSV","7478fc26":"## 7. Basic exploration of our collected dataset of tweets","d17a121b":"Snscrape is available from its <a href='https:\/\/github.com\/JustAnotherArchivist\/snscrape'>official github project repository<\/a>.\n\nSnscrape has two versions:\n- released version, which you can install by running this line in a commant line terminal: **pip3 install snscrape** (for a Windows machine)\n- **development version**, which is said to have richer functionality, so this is the one I'll be using.   \nI will use the latter.","8b4ac084":"## 5. Tweets meta-information gathered with snscrape","d4b692b8":"### Converting JSON to Pandas DataFrame\n\nPandas DataFrame is **the** data structure of choice in Data Science, so we read the JSON file into a DataFrame.  \n\nThen we save it as CSV, since CSV is the most common file type for Data Science small projects.","cfb082fd":"### Counting the number of Tweets we scraped  \n\nThe following cell is overkill in this particular scenario, but imagine you just scraped 1 million tweets and you want to know how many you got. The cell below is a very efficient way to count in that case. ","bf932706":"## 3. Calling snscrape CLI commands from Python Notebook\n\nNotice I make use of a few snscrape parameters:  \n- --max-results, to limit the search\n- --jsonl, to have my results saved directly into a json file\n- --since yyyy-mm-dd, so collect tweets starting with this date\n- twitter-search will tell snscrape what the actual text to search is.  \n    Notice I use the 'until:yyyy-mm-dd'. This is a workaround for the fact that sncrape does not have support for an --until DATETIME parameters.  \n    So I'm using Twitter's search <strong>until<\/strong> feature. That is, I am using a feature already built-in in Twitter search.  \n    For more <strong>search operators<\/strong> that you can use and pass on to snscrape as part of the text to search for, see the <a href='https:\/\/developer.twitter.com\/en\/docs\/twitter-api\/v1\/rules-and-filtering\/search-operators'>Twitter documentation on search operators<\/a>.","9d21315a":"### Installing the development version of snscrape. \nThis will not work when ran in Kaggle, because Kaggle only offers Python 3.7 for the time being","4488d1ed":"### Check tweets for a particular text","4ead34c3":"## 1. Installing snscrape","7b277af6":"If you don't see 3.8.x in your case, please upgrade your Python version before you continue this tutorial, otherwise you will **not be able to install snscrape**."}}