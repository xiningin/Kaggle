{"cell_type":{"240739aa":"code","87455254":"code","2d9cf5f3":"code","326bfe20":"code","c2d64c6c":"code","35be66f4":"code","b51347cb":"code","d1336e19":"code","6e54493d":"code","6d013949":"code","325116d1":"code","e56289aa":"code","acdafa3c":"code","95c3e8f3":"code","f51ecbe4":"code","3a20ba9d":"code","ee358c09":"code","753124c0":"code","016f8193":"code","d2c32b1d":"code","a9ff4108":"code","e76ac14e":"code","19a0638a":"code","27aec143":"code","867440cc":"code","24af79ce":"code","c521f950":"code","99f08e68":"code","dafaee5d":"code","8d451c49":"code","83989a88":"code","b39c9731":"code","d4f2f483":"code","21b299f4":"code","500cba39":"code","8a3919e0":"code","c5c336be":"code","281be0c7":"code","b12baf6b":"code","ae893e09":"code","f467388a":"code","8632dd7f":"code","2182b5d3":"code","b0e538d4":"code","c079df35":"code","0d7ab586":"code","04bfab28":"markdown","668d70bf":"markdown","03f13263":"markdown"},"source":{"240739aa":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.manifold import TSNE\nfrom joblib import dump, load\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, auc, recall_score, precision_score, roc_curve, accuracy_score\nimport copy\nfrom tqdm import tqdm","87455254":"MAX_ROWS = 10000 * 30\nSEQ_LENGTH = 50\n# MAX_ROWS = 3000\n\nnumber_of_small_groups = 172\n\ndata_path = '..\/input'\n\ntransaction_test = 'transactions_test.csv'\ntransaction_train = 'transactions_train.csv'\nembedding = 'embeddings_for_groups_clean.csv'\ntarget = 'train_target.csv'\n\ntransaction_train_path = os.path.join(data_path, transaction_train)\ntransaction_train_df = pd.read_csv(transaction_train_path, nrows=MAX_ROWS)\n\ncount_client_transactions = transaction_train_df.groupby('client_id').size()\nsmall_transaction_count_clients = count_client_transactions[count_client_transactions < 500].index\ntransaction_train_df = transaction_train_df[~transaction_train_df['client_id'].isin(small_transaction_count_clients)]\n\nprint(len(np.unique(transaction_train_df.small_group)))\nassert len(np.unique(transaction_train_df.small_group)) == number_of_small_groups","2d9cf5f3":"# transaction_train_df = pd.merge(\n#     transaction_train_df, \n#     embedding_df,\n#     left_on='small_group',\n#     right_on='small_group_code',\n#     how='left'\n# ).drop(columns=['small_group_code', 'small_group_y', 'small_group_x'])","326bfe20":"transaction_train_df = transaction_train_df.drop(columns=['trans_date'])","c2d64c6c":"count_client_transactions = transaction_train_df.groupby('client_id').size()\nsmall_transaction_count_clients = count_client_transactions[count_client_transactions < 500].index\ntransaction_train_df = transaction_train_df[~transaction_train_df['client_id'].isin(small_transaction_count_clients)]","35be66f4":"columns_to_normalize = ['amount_rur']\nscaler = MinMaxScaler()\nscaled_df = scaler.fit_transform(transaction_train_df[columns_to_normalize])\ntransaction_train_df.loc[:, columns_to_normalize] = scaled_df","b51347cb":"# transaction_train_df = pd.get_dummies(transaction_train_df, columns=['small_group']) \n# transaction_train_df_dummy.head()","d1336e19":"dump(scaler, 'scaler.joblib')","6e54493d":"client_ids = transaction_train_df.client_id.unique()\ntrain_client_ids, validation_client_ids = train_test_split(client_ids)\nvalidation_df = transaction_train_df[transaction_train_df.client_id.isin(validation_client_ids)]\ntransaction_train_df = transaction_train_df[transaction_train_df.client_id.isin(train_client_ids)]\nprint('train part', len(transaction_train_df))\nprint('test part', len(validation_df))\nvalidation_df.head()","6d013949":"transaction_train_df['amount_rur'].describe()\navg_amount_transaction = transaction_train_df['client_id'].value_counts()\navg_amount_transaction.describe()","325116d1":"transaction_train_df.client_id.unique()        \n","e56289aa":"class TripletTransactionDatasetFull(Dataset):\n    def __init__(self, X, seq_len):\n        unique_client_id = X.client_id.unique()        \n        self.seq_dict = self.create_seq_dict(X, seq_len, unique_client_id)\n        self.unique_client_id = np.array(list(self.seq_dict.keys()))\n        self.triplets, self.client_ids = self.create_triplets(self.seq_dict, unique_client_id)\n    \n    def create_seq_dict(self, X, seq_len, unique_client_id):\n        seq_dict = {}\n        groups = X.groupby('client_id')\n        for key in unique_client_id:\n            cur_group = groups.get_group(key).drop(columns='client_id')\n            n = len(cur_group)\n            \n            chunks = n \/\/ SEQ_LENGTH\n            cur_group = cur_group[:chunks * SEQ_LENGTH]\n            splitted_array = np.split(cur_group, chunks)\n            seq_dict[key] = splitted_array\n            \n        return seq_dict\n    \n    def create_triplets(self, seq_dict, unique_client_id):\n        triplets = []\n        client_ids = []\n        for positive_client_id in unique_client_id:            \n            positive_group_len = seq_dict[positive_client_id]\n            positive, anchor = np.random.choice(len(positive_group_len), 2, replace=False)\n            \n            neg_client_ids = unique_client_id[unique_client_id != positive_client_id]\n            neg_client_id = np.random.choice(neg_client_ids, 1)[0]\n            negative_group = seq_dict[neg_client_id]\n            negative = np.random.choice(len(negative_group), 1)[0]\n\n            triplets.append(\n                (positive, anchor, negative)\n            )\n            client_ids.append(\n                (positive_client_id, positive_client_id, neg_client_id)\n            )\n        return triplets, client_ids\n    \n    def __len__(self):\n        return len(self.seq_dict)\n    \n    def __getitem__(self, idx):\n        positive_client_id, anchor_client_id, neg_client_id = self.client_ids[idx]\n        positive, anchor, negative = self.triplets[idx]        \n        positive = self.seq_dict[positive_client_id][positive]        \n        anchor = self.seq_dict[anchor_client_id][anchor]\n        negative = self.seq_dict[neg_client_id][negative]\n        \n        positive_ammount, positive_small_group = positive['amount_rur'].values.reshape(-1,1), positive['small_group'].values.reshape(-1,1)\n        anchor_ammount, anchor_small_group = anchor['amount_rur'].values.reshape(-1,1), anchor['small_group'].values.reshape(-1,1)\n        negative_ammount, negative_small_group = negative['amount_rur'].values.reshape(-1,1), negative['small_group'].values.reshape(-1,1)\n        \n        return (positive_ammount, positive_small_group), (anchor_ammount, anchor_small_group), (negative_ammount, negative_small_group)\n\ntriplet_dataset = TripletTransactionDatasetFull(transaction_train_df, SEQ_LENGTH)\n(positive_ammount, positive_small_group), (anchor_ammount, anchor_small_group), (negative_ammount, negative_small_group) = triplet_dataset[len(triplet_dataset) - 1]\n\npositive_ammount.shape, positive_small_group.shape","acdafa3c":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim","95c3e8f3":"# device = 'cpu'# \ndevice = torch.device(\"cpu\")\nif torch.cuda.is_available():\n    print('CUDA!')\n    device = torch.device(\"cuda:0\")","f51ecbe4":"class LSTM(nn.Module):\n    def __init__(self, input_size, embedding_input, embedding_out, hidden_dim, batch_size, output_dim=500, num_layers=2, bidir=False):\n        super(LSTM, self).__init__()\n        self.input_dim = embedding_out + input_size\n        self.bidir = bidir\n        self.hidden_dim = hidden_dim\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n        self.output_dim = output_dim\n        self.dense1_bn = nn.BatchNorm1d(250)\n        self.Em\n        self.drop = nn.Dropout(0.4)\n        self.hidden = None\n        self.hidden = self.init_hidden()\n        \n        \n        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=self.bidir)\n        l1_size = self.hidden_dim\n        if self.bidir:\n            l1_size *= 2\n        self.linear1 = nn.Linear(l1_size, 250)\n        self.linear2 = nn.Linear(250, self.output_dim)\n        self.linear3 = nn.Linear(250, 250)\n        self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n        self.relu = nn.ReLU()        \n\n    def init_hidden(self, batch_size=None):\n        layers = self.num_layers\n        if self.hidden:\n            del self.hidden\n        if not batch_size:\n            batch_size = self.batch_size\n        if self.bidir:\n            layers *= 2 \n        hidden_state = torch.zeros(layers, batch_size, self.hidden_dim, dtype=torch.double)\n        cell_state = torch.zeros(layers, batch_size, self.hidden_dim, dtype=torch.double)\n        return (hidden_state.float().to(device), cell_state.float().to(device))\n\n    def forward(self, x):\n        x, self.hidden = self.lstm(x, self.hidden)\n        x = x[:,-1]\n        x = self.linear1(x)\n        x = self.relu(x)\n        # x = self.dense1_bn(x)\n        x = self.linear2(x)\n        # return only output after passed all data\n        # x = F.normalize(x, p=2, dim=1)\n        return x","3a20ba9d":"class TripletLSTM(nn.Module):\n    def __init__(self, lstm):\n        super(LSTM, self).__init__()\n        self.lstm = lstm\n        \n    def forward(self, x1, x2, x3):\n        output1 = self.lstm(x1)\n        self.lstm.hidden = self.lstm.init_hidden()\n        \n        output2 = self.lstm(x2)\n        self.lstm.hidden = self.lstm.init_hidden()\n        \n        self.lstm.hidden = self.lstm.init_hidden()\n        output3 = self.lstm(x3)\n        return output1, output2, output3\n\n    def get_embedding(self, x):\n        return self.lstm(x)","ee358c09":"def triplet_loss(anchor, positive, negative):\n    margin = 0.5\n    distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n    distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n    losses = F.relu(distance_positive - distance_negative + margin)\n    return losses.mean()","753124c0":"batch_size = 32\nhidden_dim = 150\nnum_layers = 1\noutput_dim = 32\n\nepochs = 10\nlr = 1e-3\n\ntrain_dataset = TripletTransactionDatasetFull(transaction_train_df, SEQ_LENGTH)\ntest_dataset = TripletTransactionDatasetFull(validation_df, SEQ_LENGTH)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size)\ntest_dataloader = DataLoader(test_dataset, batch_size)\nprint('Train batches count', len(train_dataloader))\nprint(' Test batches count', len(test_dataloader))","016f8193":"for p,a,n in train_dataloader:\n    pass\n\nfor p,a,n in test_dataloader:\n    break\n\np[0].shape","d2c32b1d":"p[1].unique()","a9ff4108":"l = nn.Embedding(number_of_small_groups, 128)\n# nn.Embedding()\noutput = l(p[1].long()).squeeze(2)\noutput = torch.cat((p[0].float(), output.float()), dim=-1)\n# torch.cat((p[0][0], ))\n# print(p[0].shape, output.shape)\n# print(.shape)\ntarget = torch.zeros_like(output)\ntarget = target[:, -1, :]\noutput = output[:, -1, :]\n# print(target.float(), output.float())\ncriterion = nn.BCEWithLogitsLoss()\nloss = criterion(output, target)\nloss.backward()\n\n\n# loss = criterion(output, target)\ntorch.where(torch.norm(l.weight.grad, dim=1) != 0)","e76ac14e":"output","19a0638a":"for p,a,n in train_dataloader:\n    pass\n\n# for p,a,n in test_dataloader:\n#     pass\n    # print(p.shape, a.shape, n.shape)","27aec143":"def valudate(model, dataloader):\n    losses = []\n    \n    ","867440cc":"def validate(model, val_ds):    \n    losses = []\n    positive_dist = []\n    negative_dist = []\n    all_predicts = np.empty(0)\n    all_true_labels = np.empty(0)    \n    model.eval()\n    batches = range(len(val_ds))    \n    for batch in batches:\n        pos, anch, neg = val_ds.get_item(batch)\n        pos, anch, neg = pos.to(device), anch.to(device), neg.to(device)\n        \n        pos_pred, anch_pred, neg_pred = predict_triplet(model, pos, anch, neg)\n        loss_val = triplet_loss(pos_pred, anch_pred, neg_pred)\n        pos_dist = get_disance(pos_pred, anch_pred)\n        neg_dist = get_disance(anch_pred, neg_pred)\n        \n        cur_positive_dist = pos_dist.data.cpu().numpy()\n        cur_negative_dist = neg_dist.data.cpu().numpy()\n                \n        positive_dist.append(np.mean(cur_positive_dist))\n        negative_dist.append(np.mean(cur_negative_dist))\n        \n        loss = loss_val.mean()\n        loss = loss.cpu().detach().numpy()\n        losses.append(loss)\n    \n    return np.mean(losses), np.mean(positive_dist), np.mean(negative_dist)\n\ndef get_disance(output1, output2):\n    distances = (output2 - output1).pow(2).sum(1)\n    return distances\n\ndef triplet_loss(anchor, positive, negative):\n    margin = 0.5\n    distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n    distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n    losses = F.relu(distance_positive - distance_negative + margin)\n    return losses.mean()\n\ndef get_loss(output1, output2, target):\n    margin = 0.5\n    distances = get_disance(output1, output2)  # squared distances\n    losses = 0.5 * (target.float() * distances +\n                    (1 + -1 * target).float() * F.relu(margin - (distances + 1e-5).sqrt()).pow(2))\n    return losses\n\n# def contrastive_loss(output, target):\n#     margin = 0.1\n#     output = target * output + (1 - target) * F.relu(output - margin)\n#     bce = nn.BCELoss()\n#     loss = bce(output, target)\n#     return loss\n    \n\ndef train(model, train_ds, test_ds, epochs, lr):\n    optimizer = optim.Adam(model.parameters(), lr)\n    \n    best_model_state_dicts = []\n    best_model_state_dicts.append(copy.deepcopy(model.state_dict()))\n\n    train_losses = []\n    validate_losses = []\n    pos_distanses = []\n    neg_distanses = []\n    \n    ## initial score\n#     validate_loss, pos_distanse, neg_distanse = validate(model, test_ds)\n#     validate_losses.append(validate_loss)\n#     pos_distanses.append(pos_distanse)\n#     neg_distanses.append(neg_distanse)\n#     print(f'Without train loss:{validate_loss:.4f} pos dist:{pos_distanse:.4f} neg dist:{neg_distanse:.4f}')\n    \n#     train_loss, pos_distanse, neg_distanse  = validate(model, train_ds)\n#     train_losses.append(train_loss)\n    batches = range(len(train_ds))\n    for epoch in range(epochs):\n        model.train()\n        train_ds.shuffle()\n        for batch in tqdm(batches):\n            model.zero_grad()\n            pos, anch, neg = train_ds.get_item(batch)\n            pos, anch, neg = pos.to(device), anch.to(device), neg.to(device)\n            \n            pos_pred, anch_pred, neg_pred = predict_triplet(model, pos, anch, neg)\n                   \n            pos_dist = get_disance(pos_pred, anch_pred)\n            neg_dist = get_disance(anch_pred, neg_pred)\n            \n            # triplet_loss\n            loss = triplet_loss(pos_pred, anch_pred, neg_pred)\n            loss.backward()\n            optimizer.step()\n            \n        validate_loss, pos_distanse, neg_distanse = validate(model, test_ds)\n        validate_losses.append(validate_loss)\n        pos_distanses.append(pos_distanse)\n        neg_distanses.append(neg_distanse)\n        \n        train_loss, train_pos_distanse, train_neg_distanse = validate(model, train_ds)\n        train_losses.append(train_loss)\n        \n        best_model_state_dicts.append(copy.deepcopy(model.state_dict()))\n        \n        print(f'Epoch {epoch} train loss: {train_loss:.4f} test loss:{validate_loss:.4f} pos dist:{pos_distanse:.4f} neg_dist:{neg_distanse:.4f}')\n    return train_losses, validate_losses, pos_distanses, neg_distanses, best_model_state_dicts\n\ntorch.cuda.empty_cache()\n\ninput_dim = pos.shape[-1]\nbatch_size = 256\nhidden_dim = 150\nnum_layers = 1\noutput_dim = 32\n\nepochs = 10\nlr = 1e-3\n\ntrain_ds = TripletTransactionDatasetFull(transaction_train_df, batch_size)\ntest_ds = TripletTransactionDatasetFull(validation_df, batch_size)\nprint('Train batches count', len(train_ds))\nprint(' Test batches count', len(test_ds))\n\nmodel = LSTM(input_dim, hidden_dim, batch_size=batch_size, output_dim=output_dim, num_layers=num_layers)\nmodel.to(device)\ntrain_loss, test_loss, pos_distanses, neg_distanses, state_dicts = train(model, train_ds, test_ds, epochs, lr)","24af79ce":"plt.figure(figsize=(15, 5))\nplot_x = range(0, epochs + 1)\nplt.plot(plot_x, train_loss, label='Train loss')\nplt.plot(plot_x, test_loss, label='Test loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid()\nplt.title('Loss')\nplt.legend();\nplt.savefig('Loss')","c521f950":"data = {\n    'epoch': plot_x,\n    'train_loss': train_loss,\n    'test_loss': test_loss\n}\nloss_df = pd.DataFrame(data=data)\nloss_df.to_csv('loss.csv', index='epoch')","99f08e68":"plt.figure(figsize=(15,5))\nplot_x = range(0, epochs + 1)\nplt.plot(plot_x, pos_distanses, label='Similarity between positive')\nplt.plot(plot_x, neg_distanses, label='Similarity between negative')\nplt.xlabel('Epoch')\nplt.ylabel('Cosine similarity')\nplt.title('Similarity on validation')\nplt.grid()\nplt.legend();","dafaee5d":"data = {\n    'epoch': plot_x,\n    'mean_pos_distanses': pos_distanses,\n    'mean_neg_distanses': neg_distanses\n}\nsimularity_df = pd.DataFrame(data=data)\nsimularity_df.to_csv('simularity.csv', index='epoch')","8d451c49":"plt.figure(figsize=(15,5))\nplot_x = range(0, epochs + 1)\nplt.plot(plot_x, rocs, label='ROC AUC')\nplt.xlabel('Epoch')\nplt.ylabel('ROC AUC')\nplt.title('ROC AUC')\nplt.grid()\nplt.legend();\nplt.savefig('ROC AUC')","83989a88":"best_roc = np.min(test_loss)\nbest_idx = np.argmin(test_loss)\nbest_model = LSTM(input_dim, hidden_dim, batch_size=batch_size, \n                  output_dim=output_dim, num_layers=num_layers)\n\nbest_model.load_state_dict(state_dicts[best_idx])\n# torch.save(best_model.state_dict(), f'model_state_dict ROC: {best_roc:.4f}')\n# torch.save(best_model, f'model ROC: {best_roc:.4f}')\nbest_model.to(device);","b39c9731":"plt.title(f'ROC curve AUC:{roc_auc:.3f}')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.plot(fpr, tpr);\nplt.savefig('ROC curve')","d4f2f483":"plt.title(f'PR curve AUC:{pr_auc:.3f}')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.plot(recall, precision);\nplt.savefig('PR curve')","21b299f4":"# validation_ds = TripletTransactionDataset(test_d, batch_size)\n# validation_batches = range(len(validation_ds))\nprint('Test')\nvalidation_loss, mean_pos_sim, mean_neg_sim = validate(best_model, test_ds)\n\nprint('Loss on validation', validation_loss)\nprint('Mean similarity between positive', mean_pos_sim)\nprint('Mean similarity between negative', mean_neg_sim)","500cba39":"print('Train')\nvalidation_loss, mean_pos_sim, mean_neg_sim = validate(model, train_ds)\n\nprint('Loss on validation', validation_loss)\nprint('Mean similarity between positive', mean_pos_sim)\nprint('Mean similarity between negative', mean_neg_sim)","8a3919e0":"from sklearn.decomposition import PCA","c5c336be":"from umap import UMAP","281be0c7":"def plot_vis(best_model, df, dim_reduction=False):\n    vis_group_df = df.groupby('client_id')\n    res = {}\n    res = pd.DataFrame()\n    for client_id in list(vis_group_df.groups):\n        client_trans = vis_group_df.get_group(client_id)\n        n = len(client_trans) \n        need_to_remove = n % SEQ_LENGTH\n        if need_to_remove != 0:\n            client_trans = client_trans[:-need_to_remove]    \n        client_trans = np.array_split(client_trans.drop(columns=['client_id']).values, n \/\/ SEQ_LENGTH)\n        batch_size = np.array(client_trans).shape[0]\n        \n        best_model.hidden = best_model.init_hidden(batch_size)\n        client_trans = torch.Tensor(client_trans).to(device)\n        \n        preds = best_model(client_trans)\n        preds = preds.data.cpu().numpy()\n        # res[client_id] = preds\n        \n        tmp_df = pd.DataFrame(preds)\n        tmp_df['client_id'] = f'{client_id}'\n        res = pd.concat((res, tmp_df), axis=0)\n    \n    # n_neighbors = 8\n    for n_neighbors in [8]:# range(2, 25):\n        if dim_reduction:\n            # vis = TSNE(2, perplexity=i).fit_transform(res.iloc[:, :-1])\n            vis = UMAP(init='random', n_neighbors=n_neighbors, min_dist=4.5, spread=5).fit_transform(res.iloc[:, :-1])\n            # vis = PCA(2).fit_transform(res.iloc[:, :-1])        \n            vis = pd.DataFrame(vis)\n        else:\n            vis = res.iloc[:, :-1]\n        vis['client_id'] = res['client_id'].values\n        plt.figure(figsize=(25,10))\n        s = 0\n\n        clients_count = len(vis_group_df.groups)\n\n        total_pos_dist = 0\n        total_neg_dist = 0\n        for client_id in list(vis_group_df.groups):\n            cur_client_trans = vis[vis['client_id'] == f'{client_id}'].drop(columns=['client_id'])\n            all_other = vis[vis['client_id'] != f'{client_id}'].drop(columns=['client_id'])\n            x,y = cur_client_trans.iloc[:,0], cur_client_trans.iloc[:,1]\n            plt.scatter(x,y, label=client_id, alpha=0.7)\n\n            neg_dist = euclidean_distances(cur_client_trans, all_other)\n            pos_dist = euclidean_distances(cur_client_trans, cur_client_trans)\n            total_neg_dist += neg_dist.mean() \/ clients_count\n            total_pos_dist += pos_dist.mean() \/ clients_count\n        plt.title(f'UMAP neigbors n_neighbors = {n_neighbors}')\n        # plt.legend()\n        plt.savefig(f'UMAP_triplet_best')\n        print('pos: ', total_pos_dist)\n        print('neg: ', total_neg_dist)\n        \nbest_model.eval()\n# plot_vis(best_model, transaction_train_df.iloc[:25_000], True)\nplot_vis(best_model, validation_df, True)","b12baf6b":"# mkdir umap\n# !ls\n# !rm UMAP_tr*.png","ae893e09":"def test_dist(best_model, df):\n    vis_group_df = df.groupby('client_id')\n    res = {}\n    res = pd.DataFrame()\n    for client_id in list(vis_group_df.groups):\n        client_trans = vis_group_df.get_group(client_id)\n        n = len(client_trans) \n        need_to_remove = n % SEQ_LENGTH\n        if need_to_remove != 0:\n            client_trans = client_trans[:-need_to_remove]    \n        client_trans = np.array_split(client_trans.drop(columns=['client_id']).values, n \/\/ SEQ_LENGTH)\n        batch_size = np.array(client_trans).shape[0]\n        \n        best_model.hidden = best_model.init_hidden(batch_size)\n        client_trans = torch.Tensor(client_trans).to(device)\n        \n        preds = best_model(client_trans)\n        preds = preds.data.cpu().numpy()\n        # res[client_id] = preds\n        \n        tmp_df = pd.DataFrame(preds)\n        tmp_df['client_id'] = f'{client_id}'\n        res = pd.concat((res, tmp_df), axis=0)\n        \n    \n    vis = res.iloc[:, :-1]\n    vis['client_id'] = res['client_id']    \n    \n    clients_count = len(vis_group_df.groups)\n    total_pos_dist = 0\n    total_neg_dist = 0\n    preds = []\n    labels = []\n    for client_id in list(vis_group_df.groups):\n        cur_client_trans = vis[vis['client_id'] == f'{client_id}'].drop(columns=['client_id'])\n        all_other = vis[vis['client_id'] != f'{client_id}'].drop(columns=['client_id'])\n        neg_dist = euclidean_distances(cur_client_trans, all_other)\n        flat_neg_dist = neg_dist.flatten()\n        preds.extend(flat_neg_dist)\n        labels.extend(np.ones(len(flat_neg_dist)))\n        \n        pos_dist = euclidean_distances(cur_client_trans, cur_client_trans)\n        pos_dist = pos_dist[~np.eye(pos_dist.shape[0],dtype=bool)].reshape(pos_dist.shape[0],-1)\n        flat_pos_dist = pos_dist.flatten()\n        preds.extend(flat_pos_dist)\n        labels.extend(np.zeros(len(flat_pos_dist)))\n        \n        total_neg_dist += neg_dist.mean() \/ clients_count\n        total_pos_dist += pos_dist.mean() \/ clients_count\n    \n    roc = roc_auc_score(labels, preds)\n    print(f'ROC {roc}')\n    print('pos: ', total_pos_dist)\n    print('neg: ', total_neg_dist)\n    return labels, preds\n        \nmodel.eval()\nlabels, preds = test_dist(best_model, validation_df);\n# labels, preds = test_dist(best_model, transaction_train_df);\n# transaction_train_df,\n# train_ds.get_item(0)[0]\n# labels, preds = test_dist(model, validation_df);","f467388a":"sum(labels)","8632dd7f":"np.array(labels).shape, np.array(preds).shape","2182b5d3":"pred_df = pd.DataFrame({\n    'label' : labels,\n    'pred' : preds\n})","b0e538d4":"# transaction_train_df.describe()\npred_df.to_csv('preds_labels.csv', index=None)","c079df35":"from sklearn.manifold import TSNE","0d7ab586":"res = {}\nres = pd.DataFrame()\nfor client_id in list(vis_group_df.groups):\n    client_trans = vis_group_df.get_group(client_id)\n    n = len(client_trans) \n    need_to_remove = n % SEQ_LENGTH\n    if need_to_remove != 0:\n        client_trans = client_trans[:-need_to_remove]    \n    client_trans = np.array_split(client_trans.drop(columns=['client_id']).values, n \/\/ SEQ_LENGTH)\n    batch_size = np.array(client_trans).shape[0]\n    best_model.hidden = best_model.init_hidden(batch_size)\n    client_trans = torch.Tensor(client_trans).to(device)\n    preds = best_model(client_trans)\n    preds = preds.data.cpu().numpy()\n    # res[client_id] = preds\n    tmp_df = pd.DataFrame(preds)\n    tmp_df['client_id'] = f'{client_id}'\n    \n    res = pd.concat((res, tmp_df), axis=0)\n    \n    vis = TSNE(2).fit_transform(res.iloc[:, :-1])\n    \n    for client_id in list(vis_group_df.groups)[1:4]:\n        idxs = res[res['client_id'] == f'{client_id}'].index    \n        x,y = vis[idxs,0], vis[idxs,1]\n        plt.scatter(x,y, label=client_id)\n    plt.legend()\n    \n# best_model.hidden = best_model.init_hidden(10000)\n# input_x = vis_df.drop(columns=['client_id'])\n# input_x = torch.Tensor(input_x.values)\n# vis_pred = best_model(input_x)\n","04bfab28":"# LSTM model","668d70bf":"# ML!","03f13263":"# Validation on users which do not include in train"}}