{"cell_type":{"7fd30634":"code","42538290":"code","721217c2":"code","0205f764":"code","be8fdf38":"code","865d4d0d":"code","f38849b6":"code","21c6a3a3":"code","a48784d8":"code","225cb5b0":"code","bf88e894":"code","8b589edc":"code","5ec0e23c":"code","5ea0f5c5":"code","e3e00dbb":"code","3558e5ea":"code","5b9f9afb":"code","faa2a76a":"code","a7032658":"code","801453e8":"code","abd07ec9":"code","eadf8ca9":"code","e712f212":"code","79c9c5d1":"code","016a8c05":"code","267fe864":"code","6bc26e01":"code","358c1daa":"code","3d6c7228":"markdown","aaccd951":"markdown","11071eee":"markdown","c857bf1e":"markdown","47572c29":"markdown","51d53271":"markdown","d676c130":"markdown","5e882e84":"markdown","288ba4e7":"markdown","9d775a56":"markdown","647583b6":"markdown","92514a50":"markdown","3ffebe0b":"markdown","58e3fb77":"markdown","feb0ec23":"markdown","58d720de":"markdown","56a97db2":"markdown","4b224c14":"markdown","d663afc6":"markdown","a760dcfd":"markdown","9ce32b6a":"markdown","ce4941b5":"markdown","f625ffa5":"markdown","4c7d2a15":"markdown","23189d9e":"markdown","404a6a42":"markdown"},"source":{"7fd30634":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","42538290":"import pandas as pd\nimport numpy as np\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torchvision.utils import make_grid\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\nfrom torch.utils.tensorboard import SummaryWriter\nimport time","721217c2":"parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--save-model', action='store_true', default=\"\/save_model.pth\",\n                        help='For Saving the current Model')\n    args = parser.parse_args()","0205f764":"def get_device():\n    if torch.cuda.is_available():\n        device = 'cuda:0'\n    else:\n        device = 'cpu'\n    return device\ndevice = get_device()","be8fdf38":"class MNISTDataset(Dataset):\n    def __init__(self, images, labels=None, transforms=None):\n        self.X = images\n        self.y = labels\n        self.transforms = transforms\n\n    def __len__(self):\n        return (len(self.X))\n\n    def __getitem__(self, i):\n        data = self.X.iloc[i, :]\n        data = np.asarray(data).astype(np.uint8).reshape(28, 28, 1)\n\n        if self.transforms:\n            data = self.transforms(data)\n\n        if self.y is not None:\n            return (data, self.y[i])\n        else:\n            return data","865d4d0d":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/\/test.csv')","f38849b6":"df_train.describe()","21c6a3a3":"df_train.head()","a48784d8":"df_test.describe()","225cb5b0":"df_test.head()","bf88e894":"train_labels = df_train.iloc[:, 0]\ntrain_images = df_train.iloc[:, 1:]\ntest_labels = df_test.iloc[:, 0]\ntest_images = df_test.iloc[:, 1:]\n","8b589edc":"train_labels","5ec0e23c":"train_images.size","5ea0f5c5":"train_images.shape","e3e00dbb":"test_labels","3558e5ea":"test_images.size","5b9f9afb":"test_images.shape","faa2a76a":"transform = transforms.Compose(\n    [transforms.ToPILImage(),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, ), (0.5, ))\n])","a7032658":"train_data = MNISTDataset(df_train.iloc[:, 1:], train_labels, transform)\ntest_data = MNISTDataset(df_test.iloc[:, 0:], test_labels, transform)","801453e8":"trainloader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\ntestloader = DataLoader(test_data, batch_size=args.batch_size, shuffle=True)","abd07ec9":"def get_random_images(num):\n\n    indices = list(range(len(test_data)))\n    np.random.shuffle(indices)\n    idx = indices[:num]\n    from torch.utils.data.sampler import SubsetRandomSampler\n    sampler = SubsetRandomSampler(idx)\n    loader = torch.utils.data.DataLoader(test_data,\n                       sampler=sampler, batch_size=num)\n    dataiter = iter(loader)\n    images, labels = dataiter.next()\n    return images, labels\n\n\nto_pil = transforms.ToPILImage()\nimages, labels = get_random_images(5)\nfig=plt.figure(figsize=(10,10))\nfor ii in range(len(images)):\n    image = to_pil(images[ii])\n    sub = fig.add_subplot(1, len(images), ii+1)\n    plt.axis('off')\n    plt.imshow(image)\nplt.show()","eadf8ca9":"for images, labels in trainloader:\n    break\nim = make_grid(images,nrow=5)\ninv_normalize = transforms.Normalize(\n    mean= [-0.485\/0.225,\n    -0.456\/0.224,\n    -0.406\/0.225],\n    std= [1\/0.225, 1\/0.224, 1\/0.225])\nim_inv = inv_normalize(im)\nplt.figure(figsize=(8,16))\nplt.imshow(np.transpose(im_inv.numpy(),(1,2,0)))\nplt.show()","e712f212":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Dropout2d(0.4))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.Dropout2d(0.4),\n            nn.MaxPool2d(2),\n            nn.ReLU())\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.Dropout2d(0.4),\n            nn.ReLU())\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.Dropout2d(0.4),\n            nn.ReLU(),\n            nn.MaxPool2d(2))\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.Dropout2d(0.4),\n            nn.ReLU())\n        self.layer6 = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.Dropout2d(0.4),\n            nn.MaxPool2d(2),\n            nn.ReLU())\n\n        self.fc1 = nn.Linear(1152, 10)\n\n\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = self.layer6(out)\n        out = out.view(out.size(0), -1)\n        out = F.softmax(self.fc1(out))\n        return out\n\n\ncnn = CNN().to(device)","79c9c5d1":"random_seed = 1\ntorch.backends.cudnn.enabled = False\ntorch.manual_seed(args.seed)\n\nexamples = enumerate(testloader)\nbatch_idx, (example_data, example_targets) = next(examples)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn.parameters(), lr=args.lr)\n\nepochs = args.epochs","016a8c05":"trainloss = []\n\n\ndef train(cnn):\n    start = time.time()\n\n    for epoch in range(epochs):\n        running_loss = 0\n\n        for i, data in enumerate(trainloader, 0):\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            optimizer.zero_grad()\n\n            outputs = cnn(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            trainloss.append(loss)\n            optimizer.step()\n\n            running_loss += loss.item()\n            print('[Epoch %d] loss: %.3f' %\n                  (epoch + 1, running_loss \/ len(trainloader)))\n\n            predicted = torch.max(outputs.data, 1)[1]\n\n\n            if i % 100 == 99:\n                grid = torchvision.utils.make_grid(inputs)\n\n\n            \n    end = time.time()\n\n    print('Done Training')\n    print('%0.2f minutes' % ((end - start) \/ 60))\n\n    return cnn\n\n\ntrain(cnn)","267fe864":"testloss = []\n\ndef test():\n    correct = 0\n    total  = len(test_data)\n    with torch.no_grad():\n        for data in testloader:\n            inputs, labels = data[0].to(device), data[1].to(device)\n            outputs = cnn(inputs)\n\n            _, predicted = torch.max(outputs.data, 1)\n            correct += (predicted == labels).sum().item()\n\n    loss = criterion(outputs, labels)\n    testloss.append(loss)\n    testcorrect.append(correct)\n    print('Accuracy of the network on test images: %0.3f %%' % (\n            100 * correct \/ total))\n\ntest()","6bc26e01":"def imshow(img):\n    img = img \/ 2 + 0.5 \n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n\ndataiter = iter(testloader)\nimages, labels = dataiter.next()\n\nimshow(torchvision.utils.make_grid(images))\nprint('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4))\n      \n      \nplt.plot(trainloss, label='Training loss')\nplt.plot(testloss, label='Validation loss')\nplt.legend()\nplt.show()","358c1daa":"def save_model():\n    model_path = input(\"{}\".format(args.save_model))\n    torch.save(cnn.state_dict(),'{}.pth'.format(model_path))\n    print(\"saved model to {}\".format(model_path))","3d6c7228":"<a id = \"7\"><\/a><br>\n\n# Data Loader","aaccd951":"We initialize constant variable. if we want to change variables, we can do there easily.","11071eee":"<a id = \"4\"><\/a><br>\n\n# Read Data","c857bf1e":"<a id = \"10\"><\/a><br>\n\n# Create Model","47572c29":"<a id = \"5\"><\/a><br>\n\n# Data Process","51d53271":"# Introduction\n\n\n<font color = 'red'>\nContent: \n\n1. [Import Libraries](#1)\n1. [Select Device](#2)\n1. [Import Dataset](#3)\n1. [Read Data](#4)\n1. [Data Process](#5)\n    * [Transforms](#6)\n    * [Data Loader](#7)\n1. [Visualization](#8)\n1. [CNN Model](#9)\n    * [Create Model](#10)\n    * [Initial Parameters](#11)\n1. [Training](#12)\n1. [Testing](#13)\n1. [Visualization Results](#14)\n1. [Save Model](#15)\n    ","d676c130":"<a id = \"3\"><\/a><br>\n\n# Import Dataset","5e882e84":"<a id = \"15\"><\/a><br>\n\n# Save Model","288ba4e7":"<a id = \"13\"><\/a><br>\n\n# Testing","9d775a56":"<a id = \"14\"><\/a><br>\n\n# Visualization Results","647583b6":"<a id = \"2\"><\/a><br>\n\n# Select Device","92514a50":"we will import MNIST dataset. We will create funtion. This function is for custom dataset. Also the function is for csv file.","3ffebe0b":"<a id = \"6\"><\/a><br>\n\n# Transforms","58e3fb77":"Do not worry. If your devices is not available to using GPU, This function select CPU devices.","feb0ec23":"we applied transorms. In this chapter we will load data.","58d720de":"We must convert images to tensor.","56a97db2":"We make six layers. we will use dropout function to drop overfitting. ","4b224c14":"<a id = \"11\"><\/a><br>\n\n# Initial Parameters","d663afc6":"<a id = \"1\"><\/a><br>\n\n# Import Libraries\n","a760dcfd":"<a id = \"12\"><\/a><br>\n\n# Training","9ce32b6a":"<a id = \"8\"><\/a><br>\n\n# Visualization","ce4941b5":"In this chapter we will use select device. There are two devices: GPU and CPU. We will choose GPU devices because GPU much faster than CPU.","f625ffa5":"<a id = \"9\"><\/a><br>\n\n# CNN Model","4c7d2a15":"These images size is 28x28  so there are 784 numbers. Each colon respresents one piksel. Training set has label and image's pixsels but test set does not have. ","23189d9e":"We selected batch size is 128. There are  42000 images and our batch size is 128 so Each bath size has got 330 images.","404a6a42":"We will use two csv files. these files for train and test. But this dataset is not include validation set. If you want to use validation set, you can split training set."}}