{"cell_type":{"64c2e025":"code","98566f38":"code","9bc27b0d":"code","4b2ea035":"code","4d0f55e4":"code","947adf4d":"code","838409ba":"code","82beebcb":"code","abcdb927":"code","38aa9d1c":"code","4ad5bc41":"code","30058fce":"code","2a93f3b8":"code","304a5d74":"code","4ca72e1b":"code","ad01967a":"code","973e75a3":"code","8f8fe022":"code","73d9aa1e":"code","dbe72de5":"code","8a7f4be8":"code","b5eb5ecc":"code","b142f2df":"code","c6884440":"code","55d76751":"code","87773e2b":"code","47d3e9b9":"code","34578fd3":"code","3605b033":"code","90f29690":"code","40db98ce":"code","1e898d3e":"code","1b32f921":"code","d757760b":"code","c3464a6e":"code","d0ec5fc2":"code","56052ec1":"code","5ce358a5":"code","3a9e588d":"code","19ea0409":"code","84ab491a":"code","dcccbd17":"code","e94f5467":"code","461b10a2":"code","aac10a96":"code","a6f1e665":"code","f935bcfd":"code","aabd0366":"code","ee447c71":"code","9fb63620":"code","513105c0":"code","6cde3f2a":"code","76108a82":"code","bd123536":"code","ccdb36f9":"code","0ed41117":"code","650a4067":"code","9eed8a8e":"code","76b52b76":"markdown","5e694121":"markdown","cc09259b":"markdown","71436277":"markdown","08bf3d70":"markdown","f5510f63":"markdown","95e1d9e1":"markdown","60384a54":"markdown","35e962c4":"markdown","27939cd5":"markdown","2a8365c9":"markdown","805987c4":"markdown","dddc3c68":"markdown","13c91dc7":"markdown","6ccf6a84":"markdown","faf9554e":"markdown","89ebaa3e":"markdown","75476703":"markdown","1e63c209":"markdown","3e998db3":"markdown","4a39311a":"markdown","b83425dd":"markdown","2963fe25":"markdown","b60cd25e":"markdown","093f029e":"markdown","616ea41a":"markdown","c517ae5f":"markdown","3608dbc3":"markdown","4bf4193e":"markdown","b2700b85":"markdown","9c2817a6":"markdown","e8f60143":"markdown","e1a9df44":"markdown","c9997032":"markdown"},"source":{"64c2e025":"# Display figure in the notebook\n%matplotlib inline","98566f38":"# Load packages\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport zipfile\n\nfrom keras import backend\nfrom keras.applications import inception_resnet_v2, mobilenet, resnet50\n\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\nfrom time import time\nfrom urllib.request import urlretrieve","9bc27b0d":"%%bash\nwget https:\/\/raw.githubusercontent.com\/m2dsupsdlclass\/lectures-labs\/master\/labs\/01_keras\/laptop.jpeg","4b2ea035":"pic = imread('laptop.jpeg')\nprint(f\"The type of the variable is: {type(pic)} (which is a particular class of np.ndarray).\")","4d0f55e4":"print(f\"The shape of the picture is {pic.shape}.\")","947adf4d":"print(f\"The type of the elements in the array is {pic.dtype}, the minimum value of the pixels is {pic.min()} and the maximum value is {pic.max()}.\")","838409ba":"# Look at the picture\nfig = plt.imshow(pic)\nfig.axes.get_xaxis().set_visible(False)\nfig.axes.get_yaxis().set_visible(False)\nplt.show()","82beebcb":"print(f\"Using the shape, the size of the image is {np.product(pic.shape)\/1e6}MB, the computation is 450 (height) * 800 (weight) * 3 (channel) * 8 (# bits to represents one element) \/ 8 (# bits in one byte).\")\nprint(f\"We can check this results using the function `nbytes`: {pic.nbytes \/ 1e6}MB.\")","abcdb927":"pic_red = pic[:, :, 0]","38aa9d1c":"fig = plt.imshow(pic_red, cmap=plt.cm.Reds_r)\nfig.axes.get_xaxis().set_visible(False)\nfig.axes.get_yaxis().set_visible(False)\nplt.show()","4ad5bc41":"pic_grey = pic.mean(axis=2)","30058fce":"fig = plt.imshow(pic_grey, cmap=plt.cm.Greys_r)\nfig.axes.get_xaxis().set_visible(False)\nfig.axes.get_yaxis().set_visible(False)\nplt.show()","2a93f3b8":"print(f\"The size of the grey picture is {np.product(pic_grey.shape) * (64 \/ 8) \/ 1e6}MB (which is higher than the color picture, it is due to the `float` data type.\")","304a5d74":"# Resize the picture to have a 50*50 picture.\npic = imread('laptop.jpeg')\npic_lowres = resize(pic, output_shape=(50, 50), mode='reflect', anti_aliasing=True)","4ca72e1b":"fig = plt.imshow(pic_lowres, interpolation='nearest')\nfig.axes.get_xaxis().set_visible(False)\nfig.axes.get_yaxis().set_visible(False)\nplt.show()","ad01967a":"print(f\"The type of the elements of the array is {pic_lowres.dtype}, and the size of the image is {pic_lowres.nbytes \/ 1e6}MB.\")","973e75a3":"print(f\"So, the range of pixel values of the low resolution image is [{pic_lowres.min()}, {np.round(pic_lowres.max(), 3)}].\")","8f8fe022":"pic_lowres_larran = resize(pic, output_shape=(50, 50), mode='reflect', anti_aliasing=True, preserve_range=True)","73d9aa1e":"print(f\"And, the range of pixel values of the low resolution image with `preserve_range=True` is [{pic_lowres_larran.min()}, {pic_lowres_larran.max()}].\")","dbe72de5":"fig = plt.imshow(pic_lowres_larran, interpolation='nearest')\nfig.axes.get_xaxis().set_visible(False)\nfig.axes.get_yaxis().set_visible(False)\nplt.show()","8a7f4be8":"fig = plt.imshow(pic_lowres_larran \/ 255.0, interpolation='nearest')\nfig.axes.get_xaxis().set_visible(False)\nfig.axes.get_yaxis().set_visible(False)\nplt.show()","b5eb5ecc":"fig = plt.imshow(pic_lowres_larran.astype(np.uint8), interpolation='nearest')\nfig.axes.get_xaxis().set_visible(False)\nfig.axes.get_yaxis().set_visible(False)\nplt.show()","b142f2df":"# Define a function to take a snapshot \ndef take_snapshot(camera_id=0, fallback_filename=None):\n    camera = cv2.VideoCapture(camera_id)\n    try:\n        # Take 10 consecutive snapshots to let the camera automatically\n        # tune itself and hope that the contrast and lightning of the\n        # last snapshot is good enough.\n        for i in range(10):\n            snapshot_ok, image = camera.read()\n        if snapshot_ok:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        else:\n            print('WARNING: Could not access the camera!')\n            if fallback_filename:\n                image = imread(fallback_filename)\n    finally:\n        camera.release()\n    return image","c6884440":"# Test if the function take_snapshot is working.\npic = take_snapshot(camera_id=0, fallback_filename='laptop.jpeg')","55d76751":"# Get the ResNet-50 model\nmodel = resnet50.ResNet50(weights='imagenet')","87773e2b":"print(f'The color channel is on the {backend.image_data_format()}.')","47d3e9b9":"print(f'The network has been trained on {model.input_shape[1:3]} RGB images.')","34578fd3":"# Load and resize the picture\npic = imread('laptop.jpeg')\npic_224 = resize(pic, (224, 224), preserve_range=True, mode='reflect')","3605b033":"print(f'The shape of the picture is {pic_224.shape}, and the dtype of its elements is {pic_224.dtype}.')","90f29690":"pic_224 = pic_224.astype(np.float32)","40db98ce":"fig = plt.imshow(pic_224 \/ 255, interpolation='nearest')\nfig.axes.get_xaxis().set_visible(False)\nfig.axes.get_yaxis().set_visible(False)\nplt.show()","1e898d3e":"print(f'The shape of the picture is {pic_224.shape}, whereas the input shape of the model should be {model.input_shape}. So we have to expand the dimension of the picture.')","1b32f921":"pic_224_batch = pic_224[None, ...] # or np.expand_dims(pic_224, axis=0)","d757760b":"%%time\nX = resnet50.preprocess_input(pic_224_batch.copy())\npred = model.predict(X)","c3464a6e":"print('Predicted image labels:')\nfor class_id, class_name, confidence in resnet50.decode_predictions(pred, top=5)[0]:\n    print(f\"\\t* {class_name} (synset: {class_id}): {confidence}\")","d0ec5fc2":"def classify_resnet50(model, fallback_filename=None):\n    \"\"\"\n    Function that takes a snapshot of the webcam and display it\n    along with the decoded prediction of the model and their\n    confidence level.\n    \"\"\"\n    # Take a snapshot\n    pic = take_snapshot(camera_id=0, fallback_filename=fallback_filename)\n    \n    # Preprocess the picture\n    pic_224 = resize(pic, (224, 224), preserve_range=True, mode='reflect')\n    pic_224_batch = pic_224[None, ...]\n    \n    # Do predictions\n    pred = model.predict(resnet50.preprocess_input(pic_224_batch.copy()))\n    \n    # Show the pic\n    fig = plt.imshow(pic \/ 255, interpolation='nearest')\n    fig.axes.get_xaxis().set_visible(False)\n    fig.axes.get_yaxis().set_visible(False)\n    plt.show()\n    \n    # Print the decoded predictions\n    print('Predicted image labels:')\n    for class_id, class_name, confidence in resnet50.decode_predictions(pred, top=5)[0]:\n        print(f\"\\t* {class_name} (synset: {class_id}): {confidence}\")\n        \ndef classify_mobilenet(model, fallback_filename=None):\n    \"\"\"\n    Function that takes a snapshot of the webcam and display it\n    along with the decoded prediction of the model and their\n    confidence level.\n    \"\"\"\n    # Take a snapshot\n    pic = take_snapshot(camera_id=0, fallback_filename=fallback_filename)\n    \n    # Preprocess the picture\n    pic_224 = resize(pic, (224, 224), preserve_range=True, mode='reflect')\n    pic_224_batch = pic_224[None, ...]\n    \n    # Do predictions\n    pred = model.predict(mobilenet.preprocess_input(pic_224_batch.copy()))\n    \n    # Show the pic\n    fig = plt.imshow(pic \/ 255, interpolation='nearest')\n    fig.axes.get_xaxis().set_visible(False)\n    fig.axes.get_yaxis().set_visible(False)\n    plt.show()\n    \n    # Print the decoded predictions\n    print('Predicted image labels:')\n    for class_id, class_name, confidence in mobilenet.decode_predictions(pred, top=5)[0]:\n        print(f\"\\t* {class_name} (synset: {class_id}): {confidence}\")\n        \ndef classify_inception_resnet_v2(model, fallback_filename=None):\n    \"\"\"\n    Function that takes a snapshot of the webcam and display it\n    along with the decoded prediction of the model and their\n    confidence level.\n    \"\"\"\n    # Take a snapshot\n    pic = take_snapshot(camera_id=0, fallback_filename=fallback_filename)\n    \n    # Preprocess the picture\n    pic_299 = resize(pic, (299, 299), preserve_range=True, mode='reflect')\n    pic_299_batch = pic_299[None, ...]\n    \n    # Do predictions\n    pred = model.predict(inception_resnet_v2.preprocess_input(pic_299_batch.copy()))\n    \n    # Show the pic\n    fig = plt.imshow(pic \/ 255, interpolation='nearest')\n    fig.axes.get_xaxis().set_visible(False)\n    fig.axes.get_yaxis().set_visible(False)\n    plt.show()\n    \n    # Print the decoded predictions\n    print('Predicted image labels:')\n    for class_id, class_name, confidence in inception_resnet_v2.decode_predictions(pred, top=5)[0]:\n        print(f\"\\t* {class_name} (synset: {class_id}): {confidence}\")","56052ec1":"%%bash\nwget https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/3f\/JPEG_example_flower.jpg\nwget https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/87\/Alcatel_one_touch_easy.jpg\/450px-Alcatel_one_touch_easy.jpg\nwget https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/8d\/President_Barack_Obama.jpg\/480px-President_Barack_Obama.jpg","5ce358a5":"# Get the ResNet50 model\nmodel_resnet50 = resnet50.ResNet50(weights='imagenet')","3a9e588d":"%%time\nclassify_resnet50(model, 'JPEG_example_flower.jpg')","19ea0409":"%%time\nclassify_resnet50(model, '450px-Alcatel_one_touch_easy.jpg')","84ab491a":"%%time\nclassify_resnet50(model, '480px-President_Barack_Obama.jpg')","dcccbd17":"# Get the MobileNet model\nmodel_mobilenet = mobilenet.MobileNet(weights='imagenet')","e94f5467":"%%time\nclassify_mobilenet(model_mobilenet, 'JPEG_example_flower.jpg')","461b10a2":"%%time\nclassify_mobilenet(model_mobilenet, '450px-Alcatel_one_touch_easy.jpg')","aac10a96":"%%time\nclassify_mobilenet(model_mobilenet, '480px-President_Barack_Obama.jpg')","a6f1e665":"# Get the InceptionResNetV2 model\nmodel_inception_resnet_v2 = inception_resnet_v2.InceptionResNetV2(weights='imagenet')","f935bcfd":"%%time\nclassify_inception_resnet_v2(model_inception_resnet_v2, 'JPEG_example_flower.jpg')","aabd0366":"%%time\nclassify_inception_resnet_v2(model_inception_resnet_v2, '450px-Alcatel_one_touch_easy.jpg')","ee447c71":"%%time\nclassify_inception_resnet_v2(model_inception_resnet_v2, '480px-President_Barack_Obama.jpg')","9fb63620":"# Download the model and the COCO trained weights\nURL = \"https:\/\/github.com\/ogrisel\/Mask_RCNN\/archive\/master.zip\"\nFOLDER = 'maskrcnn'\nFILENAME = 'Mask_RCNN-master.zip'\nif not os.path.exists(FOLDER):\n    if not os.path.exists(FILENAME):\n        tic = time()\n        print(f'Downloading {URL} to {FILENAME} (can take a couple of minutes)...')\n        urlretrieve(URL, FILENAME)\n        print(f'Done in {time() - tic}')\n    print(f'Extracting archive to {FOLDER}...')\n    zipfile.ZipFile(FILENAME).extractall('.')\n    os.rename('Mask_RCNN-master', FOLDER)\n\nCOCO_MODEL_FILE = 'mask_rcnn_coco.h5'\nif not os.path.exists(COCO_MODEL_FILE):\n    from maskrcnn import utils\n    print('Pretrained model can take several minutes to download.')\n    utils.download_trained_weights(COCO_MODEL_FILE)","513105c0":"from maskrcnn import config\nfrom maskrcnn import model as modellib\n\nclass InferenceCocoConfig(config.Config):\n    # Give the configuration a recognizable name\n    NAME = 'inference_coco'\n    \n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 80 # COCO has 80 classes.\n    \n    # Set batch size to 1 since we'll be running inference on\n    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    \n    \nconfig = InferenceCocoConfig()\nmodel = modellib.MaskRCNN(mode='inference', model_dir='mask_rcnn\/logs', config=config)\n\n# Load weights trained on MS-COCO\nCOCO_MODEL_FILE = 'mask_rcnn_coco.h5'\nmodel.load_weights(COCO_MODEL_FILE, by_name=True)","6cde3f2a":"# COCO class names\nclass_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n               'bus', 'train', 'truck', 'boat', 'traffic light',\n               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n               'teddy bear', 'hair drier', 'toothbrush']","76108a82":"%%bash\nwget https:\/\/storage.needpix.com\/rsynced_images\/street-scene-2301158_1280.jpg","bd123536":"# Read the image\npic = imread('street-scene-2301158_1280.jpg')","ccdb36f9":"fig = plt.imshow(pic, interpolation='nearest')\nfig.axes.get_xaxis().set_visible(False)\nfig.axes.get_yaxis().set_visible(False)\nplt.show()","0ed41117":"from maskrcnn import visualize\n\n# Run detection\ntic = time()\nresults = model.detect([pic], verbose=1)\ntoc = time()\nprint(f'Image analyzed in {np.around(toc - tic, 2)} secondes.')","650a4067":"# Visualize the results\nr = results[0] # Take the results for the first image.\nfor class_id, score in zip(r['class_ids'], r['scores']):\n    print(f'{class_names[class_id]}:\\t{score}')","9eed8a8e":"# Visualization on the picture\nvisualize.display_instances(pic, r['rois'], r['masks'], \n                            r['class_ids'],\n                            class_names, r['scores'])","76b52b76":"The size in bytes of a Numpy array can be computed by multiplying the number of elements by the size in byte of each element in the array. The size of one element depend of the data type. ","5e694121":"Now, let's fit the `InceptionResNetV2` model.","cc09259b":"Now, we consider the grey-level version of the image with shape `(height, weight)`. To compute this version, we compute the mean of each pixel values across the channels.","71436277":"Now, we are going to use different model, `ResNet50`, `MobileNet` and `InceptionResNetV2`, to classify the images from Wikipedia. We can then compare the models in both time and prediction accuracy.\n\nWe start be defining some function to predict the object in the images.","08bf3d70":"Indexing on the last dimension makes it possible to extract the 2D content of a specific color channel. Consider the following example for the red channel.","f5510f63":"`None` is used by Keras to mark dimensions with a dynamic number of elements. Here, `None` is the *batch size*, that is the number of images that can be processed at one. In the following, we will process only image at a time.","95e1d9e1":"Let's check that Tensorflow backend used by Keras as the default backend expect the color channel on the last axis. If it had not been the case, it would have been possible to change the order of the axes with `pic = pic.transpose(2, 0, 1)`.","60384a54":"Note that Keras on the other hand might expect images encoded with values in the $[0.0, 255.0]$ range irrespectively of the data type of the array. To avoid the implicit conversion to the $[0.0, 1.0]$ range, we can use the `preserve_range=True` option in the `resize` function. But the *dtype* will change to *float64*.","35e962c4":"We will begin the models comparison using the model `ResNet50`. ","27939cd5":"### Decoding the predicition probabilities\n\nReading the raw probabilities for the $1000$ possible ImageNet classes is tedious. Fortunately, Keras comes with an helper function to extract the highest rated classes according to the model and display both class names and the wordnet synset identifiers.","2a8365c9":"For correctly displaying an RGB array with floating point values in the $[0.0, 255.0]$ range, we can divide all the values by $255$ or change the *dtype* to integers.","805987c4":"## Image Classification\n\nThe Keras library includes several neural network models pretrained on the ImageNet classification dataset. A popular model that show a good tradeoff between computation speed, model size and accuracy is called *ResNet-50* ([here](https:\/\/arxiv.org\/pdf\/1512.03385.pdf) for the article on Deep Residual Networks).","dddc3c68":"**Careful**, by conventions, both `skimage.transform.imresize` and `plt.imshow` assume that floating point values range from $0.0$ to $1.0$ when using floating points as opposed to $0$ to $255$ when using 8-bit integers. ","13c91dc7":"### (Optional) Taking snapshots from the webcam\n\nWe are going to use the [Python API of OpenCV](https:\/\/github.com\/skvark\/opencv-python) in order to take pictures.","6ccf6a84":"## Instance Detection and Segmentation with Mask-RCNN\n\n[Mask-RCNN](https:\/\/arxiv.org\/pdf\/1703.06870.pdf) is a refinement of the [Faster-RCNN](https:\/\/arxiv.org\/pdf\/1506.01497.pdf) **object detection** model to also add support for **instance segmentation**. The following shows how to use a [Keras based implementation](https:\/\/github.com\/matterport\/Mask_RCNN) provided by [Matterport](https:\/\/matterport.com\/) along with model parameters pretrained on the  [COCO Object Detection dataset](http:\/\/cocodataset.org\/#home).\n","faf9554e":"Note that we make a copy each time as the function `preprocess_input` can modify the image inplace to reuse memory when preprocessing large datasets.","89ebaa3e":"**Warning**: The behavior of `plt.imshow` depends on both the *dtype* and the dynamic range when displaying RGB images. In particular, it does not work on RGB images with *float64* values in the $[0.0, 255.0]$ range.","75476703":"Note that the image has been deformed by the resizing. In practice, this should not degrade the performance of the network too much. There are two alternatives solutions to that problems:\n* resizing the image so that the smallest side is set to 224;\n* extracting a square centered crop of size $(224, 224)$ from the resulting image.","1e63c209":"An image is described by three parameters: its height; its weight; its color channels (RGB). Each of them corresponds to a dimension of the array.","3e998db3":"The values of the pixels of the low resolution image are computed by combining the values of the pixels in the high resolution image. The result is therefore represented as floating points.","4a39311a":"### Create Model and Load Training Weights","b83425dd":"### Resizing images, handling data types and dynamic ranges\n\nWhen dealing with an heterogeneous collection of image of various sizes, it is often necessary to resize the image to the same size. More specifically:\n* for **image classification**, most networks expect a specific **fixed input size**;\n* for **object detection** and instance segmentation, networks have more flexibility but the images should have **approximately the same size as the training set images**.\n\nFurthermore, **large images can be much slower to process** than smaller images. This is due to the fact that the number of pixels varies quadratically with the height and width.","2963fe25":"However, the model use *float32* dtype. So, we have to convert the picture into *float32*.","b60cd25e":"The `uint8` integer data type can not represent the grey pixels because they have floating points.  Anyway, Numpy represents it as `float64` data type.","093f029e":"`pic_224_batch` is now compatible with the input shape of the neural network, so let's make a prediction.","616ea41a":"For efficiency reasons, the pixel intensities of each channel are stored as **8-bit unsigned integer** taking values in the **$[0, 255]$ range**.","c517ae5f":"The output predictions are a 2D array with:\n* One row per image in the batch;\n* One column per target class in the ImageNet LSVRC dataset ($1000$ possible classes) with the probabilities that a given image belongs to a particular class. Obviously, the sum of the columns for each row is equal to $1$.","3608dbc3":"### Class Names\n\nIndex of the class in the list is its ID. For example, to get ID of the teddy bear class, use: `class_names.index('teddy bear')`. `BG` stands for background.","4bf4193e":"### Run Object Detection\n\nLet's perform object segmentation on an image taken on the web. ","b2700b85":"# Introduction to Pretrained Models for Computer Vision\n\nThis notebook is based on the Deep Learning course from the Master Datascience Paris Saclay. Materials of the course can be found [here](https:\/\/github.com\/m2dsupsdlclass\/lectures-labs).\n\nIt aims to get some hands-on experience with pre-trained Keras models are reasonably close to the state-of-the-art of some computer vision tasks. The models are pre-trained on large publicly available labeled images datasets such as [ImageNet](http:\/\/www.image-net.org\/) and [COCO](http:\/\/cocodataset.org\/#home).\n\nThis notebook will highlights two specific tasks (or at least try):\n\n* **Image Classification**: Predict only one class label per-image (assuming a single centered object or image class).\n* **Object detection and instance segmentation**: Detect and localise all occurrences of objects of a predefined list of classes of interest in a given image.","9c2817a6":"Check on the ImageNet [website](https:\/\/www.image-net.org\/) to better understand the use of the terms *notebook* in the training set. Note that the network is not too confident about the class of the main object in that image. If we were to merge the *notebook* and the *laptop* classes, the prediction will be good.\n\nFurthermore, the network also considers secondary objects (desk, mouse, ...) but the model as been trained as an image (multiclass) classification model with a single expected class per image rather than a multi-label classification model such as an object detection model with several positive labels per image.\n\nWe have to keep that in mind when trying to make use of the predictions of such a model for a practical application. This is a fundamental limitation of the label structure of the training set.","e8f60143":"## Working with images data\n\nFor the beginning, we will see how to work with images data, how they are represented in memory, how to load it, how to modify it, *ect.*\n\nLet's use the library [`scikit-image`](https:\/\/scikit-image.org\/) to load the content of a JPEG file into a numpy array.","e1a9df44":"### A note on preprocessing\n\nAll Keras pretrained vision models expect images with `float32` dtype and values in the $[0, 255]$ range. When training neural network, it often works better to have values closer to zero.\n* A typical preprocessing is to center each of the channel and normalize its variance.\n* Another one is to measure the `min` and the `max` values and to shift and rescale to the $(-1, 1)$ range.\n\nThe exact kind of preprocessing is not very important, but it's very important to **always reuse the preprocessing function that was used when training the model**.","c9997032":"The expected of range values for the new pixels is the same as before, **$[0, 255]$** (0 for white pixels and 255 for black ones). "}}