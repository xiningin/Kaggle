{"cell_type":{"15d997bb":"code","77a928f1":"code","fece1b64":"code","6fa5d20a":"code","f2dde405":"code","c32825a6":"code","3324265a":"code","e78b4862":"code","69a9fbcd":"code","555d8ca2":"code","4058ad10":"code","31b63274":"code","383f1d29":"code","09e42b34":"code","d8816a5f":"code","5fe239a0":"code","39413981":"code","acb066cd":"code","89db8b4e":"code","4bf62adc":"code","fc695787":"code","d55a759a":"code","d7464551":"code","d9c1690e":"code","1f8eb285":"code","2c96c70d":"code","f8703301":"code","55944481":"code","89d19689":"code","cf3cac8d":"code","2b1ea441":"markdown","486594a6":"markdown","e9b1e6db":"markdown","312e475d":"markdown","1cfa90be":"markdown","3f9e2989":"markdown","25423516":"markdown","346b51ac":"markdown","f57dd95b":"markdown","74bc0a8a":"markdown","050009e9":"markdown","80302df8":"markdown","4db94c79":"markdown","20e677f2":"markdown","d764b191":"markdown","c5c566ca":"markdown","ebc6fa36":"markdown","74524f6c":"markdown","6486c420":"markdown","e8f56347":"markdown","299918a6":"markdown","0fa83174":"markdown","4726d249":"markdown"},"source":{"15d997bb":"import numpy as np\nimport pandas as pd\n\nimport IPython\nimport time\nimport os\nimport imageio\n\nimport tensorflow as tf\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected = True)\n\ntry:\n    import cPickle as pickle\nexcept:\n    import pickle","77a928f1":"!mkdir images && mkdir labels\n!tar -xf ..\/input\/carla-densely-annotated-driving-dataset\/images.tar -C .\/images\/\n!tar -xf ..\/input\/carla-densely-annotated-driving-dataset\/labels.tar -C .\/labels\/","fece1b64":"video_info = pd.read_csv(\"\/kaggle\/input\/carla-densely-annotated-driving-dataset\/video_info.csv\")\n\nweather_duration = video_info.groupby(\"weather\").sum()[\"duration_seconds\"]\nprint(weather_duration.max() \/ weather_duration)\n\nvideo_info[\"weather_weight\"] = video_info[\"weather\"].replace({\"Sunny\": 1., \"Rainy\": 1.501, \"Cloudy\": 2.401})\nvideo_info","6fa5d20a":"classes_rgb = pd.read_csv(\"\/kaggle\/input\/carla-densely-annotated-driving-dataset\/classes_rgb_values.csv\")\nclasses_rgb[\"class_weight\"] = classes_rgb[\"relative_percentile_frequency\"].max() \/ classes_rgb[\"relative_percentile_frequency\"]\nclasses_rgb","f2dde405":"label_list = [[220,220,0], [70,70,70], [190,153,153], [250,170,160], [220,20,60], [153,153,153], [157,234,50],\n             [128,64,128], [244,35,232], [107,142,35], [0,0,142], [102,102,156], [0,0,0]]\n\ndef pixels2labels(image, label_list):\n    \"\"\"\n    Converts an RGB value to a class label for every pixel\n    \n    image: three-dimensional array representing an image in an RGB format ([height, width, channels])\n    label_list: two-dimensional array of RGB values representing each class\n    \"\"\"\n    # scale array\n    s = 256**np.arange(image.shape[-1])\n    # Reduce image and labels to 1D\n    image1D = image.reshape(-1, image.shape[-1]).dot(s)\n    label1D = np.dot(label_list, s)\n    # Use searchsorted to trace back label indices\n    sidx = label1D.argsort()\n    return sidx[np.searchsorted(label1D, image1D, sorter = sidx)]\n\ndef labels2pixels(image, label_list):\n    \"\"\"\n    Converts a class label to an RGB value for every pixel\n    \n    image: two-dimensional array representing an image in a class label format ([height, width])\n    label_list: two-dimensional array of RGB values representing each class\n    \"\"\"\n    indices1D = image.ravel()\n    pixels1D = np.array(label_list, dtype=float)[indices1D]\n    return pixels1D.reshape((image.shape[0], image.shape[1], 3)) \/ 255\n\ndef probabilities2labels(image, label_list):\n    \"\"\"\n    Converts a sparse class label to an RGB value for every pixel\n    \n    image: three-dimensional array representing an image in a sparse class label format ([height, width, probability_of_class])\n    label_list: two-dimensional array of RGB values representing each class\n    \"\"\"\n    return np.argmax(image, axis=2)","c32825a6":"def generate_batch(label_list, batch_size=32, weather_weights=None, test=False):\n    \"\"\"\n    Returns a tuple of:\n    images to perform segmentation on \u2014 numpy.ndarray of shape [batch_size, height, width, channels]\n    true segmentations \u2014 numpy.ndarray of shape [batch_size, height, width]\n    \n    label_list: two-dimensional array of RGB values representing each class\n    batch_size: number of instances in a batch\n    weather_weights: list of length equals to the number of videos\n    test: whether batch are being generated for training or testing the network\n        one video for each weather is left for this purpose\n    \"\"\"\n    batch_x, batch_y = [], []\n    videos = sorted(os.listdir(\"labels\"))\n    \n    if test:\n        videos = [videos[0], videos[-2], videos[-1]]\n    else:\n        videos = videos[1:26]\n        if weather_weights is not None:\n            weather_weights = weather_weights[1:26]\n            \n    if weather_weights is None:\n        weather_weights = np.ones(len(videos))\n    probs = weather_weights \/ weather_weights.sum()\n    \n    for i in range(batch_size):\n        \n        video = np.random.choice(videos, p=probs)\n        frames = [os.path.join(video, frame) for frame in os.listdir(os.path.join(\"labels\", video))]\n        frame = np.random.choice(frames)\n            \n        batch_x.append(imageio.imread(os.path.join(\"images\", frame)))\n        y = imageio.imread(os.path.join(\"labels\", frame))\n        batch_y.append(pixels2labels(y, label_list).reshape(y.shape[:-1]))\n            \n    return np.array(batch_x, dtype=float), np.array(batch_y, dtype=int)","3324265a":"def weighted_loss(y_true, y_pred):\n    \"\"\"\n    Weighted softmax cross-entropy loss\n    Weights for classes are the ones in classes_rgb[\"class_weight\"]\n    \"\"\"\n    weights = tf.constant([676, 2.4, 120.7, 116.55, 33.14, 45.68, 18.17, 1, 5.78, 7.07, 9.31, 41.22, 1.03])\n    weights = tf.gather(weights, y_true)\n    loss = tf.compat.v1.losses.sparse_softmax_cross_entropy(y_true, y_pred, weights)\n    return loss * weights","e78b4862":"class BalancedSparseCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n    def __init__(self, name='balanced_sparse_categorical_accuracy', dtype=None):\n        super().__init__(name, dtype=dtype)\n\n    def update_state(self, y_true, y_pred, sample_weight = None):\n        y_flat = y_true\n        if y_true.shape.ndims == y_pred.shape.ndims:\n            y_flat = tf.squeeze(y_flat, axis=[-1])\n        y_true_int = tf.cast(y_flat, tf.int32)\n\n        cls_counts = tf.math.bincount(y_true_int)\n        cls_counts = tf.math.reciprocal_no_nan(tf.cast(cls_counts, self.dtype))\n        weight = tf.gather(cls_counts, y_true_int)\n        return super().update_state(y_true, y_pred, sample_weight=weight)","69a9fbcd":"def plot_train_metrics(train_metrics, train_time=0):\n    \"\"\"\n    Plots loss and accuracy during training\n    \n    train_metrics: numpy.ndarray of loss and accuracy\n    train_time: seconds of training\n    \"\"\"\n    print(f\"Total iterations: {train_metrics.shape[1] - 1}\")\n    print(f\"Total training time: {time.strftime('%H:%M:%S', time.gmtime(train_time))}\")\n    fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Weighted Crossentropy Loss\", \"Balanced Accuracy\"])\n    fig_loss = go.Scatter(x = list(range(1, train_metrics.shape[1] + 1)), y=train_metrics[0, 1:])\n    fig.add_trace(fig_loss, row=1, col=1)\n    fig_acc = go.Scatter(x=list(range(1, train_metrics.shape[1] + 1)), y=train_metrics[1, 1:])\n    fig.add_trace(fig_acc, row=1, col=2)\n    fig.update_layout(showlegend=False, title=\"Model Training Performance\")\n    iplot(fig)","555d8ca2":"def plot_segmentation(model, label_list):\n    \"\"\"\n    Plots original image, true and predicted segmentation\n    \n    model: tensorflow.keras.Model object\n    label_list: two-dimensional array of RGB values representing each class\n    \"\"\"\n    x, y = generate_batch(label_list, 1, test=True)\n    fig = make_subplots(rows=1, cols=3, subplot_titles=[\"Source\", \"True segmentation\", \"Predicted segmentation\"])\n    imgplot = px.imshow(x[0])\n    fig.add_trace(imgplot['data'][0], row=1, col=1)\n    fig_true = px.imshow(labels2pixels(y[0, :, :], label_list))\n    fig.add_trace(fig_true['data'][0], row=1, col=2)\n    fig_pred = px.imshow(labels2pixels(probabilities2labels(model(x)[0], label_list), label_list))\n    fig.add_trace(fig_pred['data'][0], row=1, col=3)\n    fig.update_layout(showlegend=False)\n    iplot(fig)","4058ad10":"def test_model(model, label_list):\n    \"\"\"\n    Performs model evaluation on hold-out videos (one video for each weather)\n    \n    model: tensorflow.keras.Model object\n    label_list: two-dimensional array of RGB values representing each class\n    \"\"\"\n    \n    test_metrics = np.zeros((2, 1))\n    test_batches_number = 300\n\n    start_time = time.time()\n    for i in range(test_batches_number):\n        x, y = generate_batch(label_list, batch_size=16, test=True)\n        test_metrics = np.concatenate((test_metrics, np.array(model.test_on_batch(x, y)).reshape(2,1)), axis=1)\n        IPython.display.clear_output()\n        print(f\"Iteration: {i + 1}. Time passed: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}.\")\n        print(f\"Loss: {test_metrics[0, i + 1]:.3f}. Accuracy: {test_metrics[1, i + 1]:.3f}\")\n\n    avg_loss = test_metrics[0, 1:].sum() \/ test_batches_number\n    avg_acc = test_metrics[1, 1:].sum() \/ test_batches_number\n    print(f\"Average loss: {avg_loss:.3f}. Average accuracy: {avg_acc:.3f}\")","31b63274":"class convolution_module(tf.keras.layers.Layer):\n    def __init__(self, filters, kernel_size, padding=\"valid\"):\n        super().__init__()\n        self.conv = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding)\n        self.bn = tf.keras.layers.BatchNormalization()\n        self.lrelu = tf.keras.layers.LeakyReLU()\n        \n    def call(self, inputs):\n        x = self.conv(inputs)\n        x = self.bn(x)\n        x = self.lrelu(x)\n        return x\n\nclass downsample_module(tf.keras.layers.Layer):\n    \n    def __init__(self, filters, kernel_size, pool_size, padding=\"valid\"):\n        super().__init__()\n        self.conv_module1 = convolution_module(filters, kernel_size, padding)\n        self.conv_module2 = convolution_module(filters, kernel_size, padding)\n        self.maxpool = tf.keras.layers.MaxPool2D(pool_size)\n        \n    def call(self, inputs):\n        x = self.conv_module1(inputs)\n        x = self.conv_module2(x)\n        x = self.maxpool(x)\n        return x\n    \nclass deconvolution_module(tf.keras.layers.Layer):  \n    def __init__(self, filters, kernel_size):\n        super().__init__()\n        self.deconv = tf.keras.layers.Conv2DTranspose(filters, kernel_size, strides=kernel_size, padding=\"same\")\n        self.bn = tf.keras.layers.BatchNormalization()\n        self.lrelu = tf.keras.layers.LeakyReLU()\n        \n    def call(self, inputs):\n        x = self.deconv(inputs)\n        x = self.bn(x)\n        x = self.lrelu(x) \n        return x\n    \nclass upsample_module(tf.keras.layers.Layer):\n    \n    def __init__(self, filters, kernel_size, deconv_kernel, padding=\"valid\"):\n        super().__init__()\n        self.deconv_module = deconvolution_module(filters, deconv_kernel)\n        self.conv_module1 = convolution_module(filters, kernel_size, padding)\n        self.conv_module2 = convolution_module(filters, kernel_size, padding)\n        \n    def call(self, inputs):\n        x = self.deconv_module(inputs)\n        x = self.conv_module1(x)\n        x = self.conv_module2(x)\n        return x\n\nclass fully_convolutional_network(tf.keras.Model):\n    \n    def __init__(self):\n        super().__init__()\n        self.downsample1 = downsample_module(64, (3, 3), (2, 2), \"same\") #300x400\n        self.downsample2 = downsample_module(64, (3, 3), (2, 2), \"same\") #150x200\n        self.downsample3 = downsample_module(128, (3, 3), (2, 2), \"same\") #75x100\n        self.downsample4 = downsample_module(128, (3, 3), (3, 2), \"same\") #25x50\n        self.conv_module = convolution_module(256, (3, 3), \"same\")\n        \n        self.deconv_module1 = deconvolution_module(64, (3, 2))\n        self.deconv_module2 = deconvolution_module(32, (8, 8))\n        \n        self.dropout = tf.keras.layers.Dropout(0.2)\n        self.dense = tf.keras.layers.Dense(16, activation=\"elu\")\n        self.softmax = tf.keras.layers.Dense(13, activation=\"softmax\")\n    \n    def call(self, inputs):\n        \n        x = self.downsample1(inputs)\n        x = self.downsample2(x)\n        MP75x100 = self.downsample3(x)\n        x = self.downsample4(MP75x100)\n        x = self.conv_module(x)\n        \n        x = self.deconv_module1(x)\n        x = tf.concat([x, MP75x100], axis=3)\n        \n        x = self.deconv_module2(x)\n        \n        x = self.dropout(x)\n        x = self.dense(x)\n        x = self.softmax(x)\n\n        return x","383f1d29":"FCN = fully_convolutional_network()\n\ntrain_metrics = np.zeros((2, 1))\ntrain_time = 0","09e42b34":"metrics = [\n        BalancedSparseCategoricalAccuracy()\n]\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nFCN.compile(optimizer=optimizer, loss=weighted_loss, metrics=metrics)","d8816a5f":"train_batches_number = 3000\nmetrics_offset = train_metrics.shape[1]\nstart_time = time.time()\nfor i in range(train_batches_number):\n    x, y = generate_batch(label_list, 10, video_info[\"weather_weight\"].values)\n    train_metrics = np.concatenate((train_metrics, np.array(FCN.train_on_batch(x, y)).reshape(2,1)), axis=1)\n    IPython.display.clear_output()\n    print(f\"Iteration: {i + 1}. Time passed: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}.\")\n    print(f\"Loss: {train_metrics[0, i + metrics_offset]:.3f}. Accuracy: {train_metrics[1, i + metrics_offset]:.3f}\")\ntrain_time += time.time() - start_time","5fe239a0":"plot_train_metrics(train_metrics, train_time)","39413981":"plot_segmentation(FCN, label_list)","acb066cd":"test_model(FCN, label_list)","89db8b4e":"FCN.summary()","4bf62adc":"class contractive_path(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.conv_module_down1 = convolution_module(32, (3, 3), \"same\")\n        self.maxpool1 = tf.keras.layers.MaxPool2D((2, 2))\n        self.conv_module_down2 = convolution_module(64, (3, 3), \"same\")\n        self.maxpool2 = tf.keras.layers.MaxPool2D((2, 2))\n        self.conv_module_down3 = convolution_module(128, (3, 3), \"same\")\n        self.maxpool3 = tf.keras.layers.MaxPool2D((2, 2))\n        self.conv_module_down4 = convolution_module(256, (3, 3), \"same\")\n        self.maxpool4 = tf.keras.layers.MaxPool2D((3, 2))\n        self.conv_module_down5 = convolution_module(384, (3, 3), \"same\")\n        \n    def call(self, inputs):\n        x_skip1 = self.conv_module_down1(inputs)\n        x = self.maxpool1(x_skip1)\n        x_skip2 = self.conv_module_down2(x)\n        x = self.maxpool2(x_skip2)\n        x_skip3 = self.conv_module_down3(x)\n        x = self.maxpool3(x_skip3)\n        x_skip4 = self.conv_module_down4(x)\n        x = self.maxpool4(x_skip4)\n        x = self.conv_module_down5(x)\n        return x, x_skip1, x_skip2, x_skip3, x_skip4\n\nclass expansive_path(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.upsample1 = deconvolution_module(256, (3, 2))\n        self.conv_module_up1 = convolution_module(256, (3, 3), \"same\")\n        self.upsample2 = deconvolution_module(128, (2, 2))\n        self.conv_module_up2 = convolution_module(128, (3, 3), \"same\")\n        self.upsample3 = deconvolution_module(64, (2, 2))\n        self.conv_module_up3 = convolution_module(64, (3, 3), \"same\")\n        self.upsample4 = deconvolution_module(32, (2, 2))\n        self.conv_module_up4 = convolution_module(32, (3, 3), \"same\")\n        \n    def call(self, inputs):\n        x = self.upsample1(inputs[0])\n        x = tf.concat([x, inputs[4]], axis=3)\n        x = self.conv_module_up1(x)\n        x = self.upsample2(x)\n        x = tf.concat([x, inputs[3]], axis=3)\n        x = self.conv_module_up2(x)\n        x = self.upsample3(x)\n        x = tf.concat([x, inputs[2]], axis=3)\n        x = self.conv_module_up3(x)\n        x = self.upsample4(x)\n        x = tf.concat([x, inputs[1]], axis=3)\n        x = self.conv_module_up4(x)\n        return x\n    \n    \nclass u_net(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.contractive = contractive_path()\n        #self.dropout = tf.keras.layers.Dropout(0.1)\n        self.expansive = expansive_path()\n        self.dense = tf.keras.layers.Dense(32)\n        self.softmax = tf.keras.layers.Dense(13, activation = \"softmax\")\n    \n    def call(self, inputs):\n        x = self.contractive(inputs)\n        #x = self.dropout(x)\n        x = self.expansive(x)\n        x = self.dense(x)\n        x = self.softmax(x)\n        return x","fc695787":"U_Net = u_net()\n\ntrain_metrics = np.zeros((2, 1))\ntrain_time = 0","d55a759a":"metrics = [\n        BalancedSparseCategoricalAccuracy()\n]\noptimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4)\nU_Net.compile(optimizer = optimizer, loss = weighted_loss, metrics = metrics)","d7464551":"train_batches_number = 3000\nmetrics_offset = train_metrics.shape[1]\nstart_time = time.time()\nfor i in range(train_batches_number):\n    x, y = generate_batch(label_list, 10, video_info[\"weather_weight\"].values)\n    train_metrics = np.concatenate((train_metrics, np.array(U_Net.train_on_batch(x, y)).reshape(2,1)), axis = 1)\n    IPython.display.clear_output()\n    print(f\"Iteration: {i + 1}. Time passed: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}.\")\n    print(f\"Loss: {train_metrics[0, i + metrics_offset]:.3f}. Accuracy: {train_metrics[1, i + metrics_offset]:.3f}\")\ntrain_time += time.time() - start_time","d9c1690e":"plot_train_metrics(train_metrics, train_time)","1f8eb285":"plot_segmentation(U_Net, label_list)","2c96c70d":"test_model(U_Net, label_list)","f8703301":"U_Net.summary()","55944481":"training_state = {\n    \"metrics_history\": train_metrics,\n    \"training_time\": train_time\n}\n\nwith open(\"training_state.pkl\", \"wb\") as f:\n    pickle.dump(training_state, f)\n\nFCN_SC.save_weights(\"weights\")","89d19689":"with open(\"\/kaggle\/input\/dadcmodel\/fcn\/training_state.pkl\", \"rb\") as f:\n    training_state = pickle.load(f)\n    train_metrics = training_state[\"metrics_history\"]\n    train_time = training_state[\"training_time\"]\n\nFCN.load_weights(\"\/kaggle\/input\/dadcmodel\/fcn_sc\/weights\")","cf3cac8d":"!rm -r images\n!rm -r labels","2b1ea441":"### Creating model","486594a6":"# TODO:\n- try working with videos as a sequence (2D ConvLSTM's)\n- maybe use TensorFlow's functional API for an example\n- balance accuracy during test properly","e9b1e6db":"### Weighted softmax cross-entropy loss and balanced accuracy","312e475d":"### Function to generate training instances","1cfa90be":"# Defining functions","3f9e2989":"## Training","25423516":"### Creating model","346b51ac":"## Test","f57dd95b":"##### Deleting unpacked data from the output folder to save Kaggle some space and time when saving.","74bc0a8a":"# Preparing datasets","050009e9":"### Save","80302df8":"Results are significantly lower than the ones from the paper. Partly because I'm not able to use as many filters due to the kaggle's memory limitations. Borders are crude so adding one more skip connections might be a good idea.\n\nVGG16 (which was fine-tuned in the mentioned [paper](https:\/\/arxiv.org\/abs\/1411.4038)) had 134M parameters while this model has 1.42M.\n\nAlso, this memory limitation means that I'm not able to use larger batch sizes and the gradient is noisier.","4db94c79":"# Saving training state","20e677f2":"# Fully Convolutional Neural Network\n\nArchitecture is very similar to the FCN-16s described in [this paper](https:\/\/arxiv.org\/abs\/1411.4038) but sizes of filters are different due to the different image size and there are a lot less filters due to Kaggle's memory limitations.","d764b191":"### Load","c5c566ca":"## Test","ebc6fa36":"# Importing libraries","74524f6c":"### Functions to get different image representations","6486c420":"### Evaluation","e8f56347":"Now weights and training history are available to download from the right side menu.\n\nReminder to only unpickle files from the sources you trust.","299918a6":"## Training","0fa83174":"# U-Net\n\nThe network architecture consists of two halves: downsampling (contractive path) and upsampling (expansive path). Feature maps, prior to each maxpooling operation, are being saved to reintroduce later during upsampling.\n\nIn the [original paper](https:\/\/arxiv.org\/abs\/1505.04597) convolutions are not padded and the missing context is extrapolated by mirroring the input image. Image is divided into smaller fragments to allow applying the network to larger images.","4726d249":"To load weights and previous training history I've created a dataset on kaggle. "}}