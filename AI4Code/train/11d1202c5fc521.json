{"cell_type":{"bdab9b20":"code","67729509":"code","2fbce5e7":"code","800f7aea":"code","960b3df0":"code","e44153aa":"code","ded48534":"code","449cf0f6":"code","80449ac3":"code","08f492b6":"code","6bb89238":"code","cbc0b6d6":"code","7f3b74a6":"code","cdae8272":"code","d7c4cc47":"code","db56620a":"code","6c7d5de4":"code","3972ab04":"code","7cc4c8c6":"code","65127b05":"code","52459c1f":"code","8f48649d":"code","708c4c22":"code","544fd523":"code","572aa3a4":"code","61a2486d":"code","e6a368a0":"code","0163e7de":"code","fe11107b":"code","0bea116e":"code","977296b6":"code","f0612b03":"code","6427a2b1":"code","d5dcd7c7":"code","0c373ce9":"code","59c6a57e":"code","b7aaf64c":"code","107db082":"code","75e22ca9":"code","9e6faf5e":"code","22543f2c":"code","c98034f9":"code","0c4ef50e":"code","e640f4cc":"code","c3e135de":"code","1fa7963d":"code","c8a7a13e":"code","1873c817":"code","42757f02":"markdown","5704765d":"markdown","2d0d92cc":"markdown","7a3a9867":"markdown","0309f403":"markdown","766818f8":"markdown","ab3aa9c6":"markdown","56a8d63f":"markdown","6e9d8b18":"markdown","0fe1ed85":"markdown","e9219074":"markdown","44a18713":"markdown","fd6c5b5b":"markdown","d1d2333e":"markdown","0314807e":"markdown","4ac5d90d":"markdown","8f1daef7":"markdown","c0f5fae8":"markdown","05e50869":"markdown","e401b072":"markdown","4a183a95":"markdown","530ea47a":"markdown","2d97018a":"markdown","d244c1f2":"markdown","e3843d68":"markdown","bddadb35":"markdown","bb98560c":"markdown","e9dc5576":"markdown","db3e7b94":"markdown"},"source":{"bdab9b20":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport pylab as pl\nimport numpy as np\n\n# I want my data frame to only show two decimal places for the number, so I change the display format:\npd.options.display.float_format = \"{:,.2f}\".format","67729509":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.head(9)","2fbce5e7":"print(df.shape)\ndf.describe()","800f7aea":"df.dtypes","960b3df0":"df.isna().sum() ","e44153aa":"import seaborn as sns\n\nplt.figure(figsize = (16, 8))\n\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot = True)\nplt.show()","ded48534":"df_subset = df[[\"target\",\"cp\", \"restecg\", \"thalach\", \"slope\"]]\n\nsns.set_theme(style=\"ticks\")\nsns.pairplot(df_subset, hue=\"target\")","449cf0f6":"import warnings\nwarnings.filterwarnings('ignore')\n \nplt.figure(figsize=(30,50))\nplot_num=1\n\nfor column in df:\n  if plot_num<14:\n    ax=plt.subplot(7,2,plot_num)\n    sns.distplot(df[column], color=\"black\")\n    plt.xlabel(column,fontsize=22)\n    plt.ylabel('Values',fontsize=22)\n  plot_num+=1\nplt.show()","80449ac3":"X=df.drop('target',axis=1).values    \ny=df['target'].values","08f492b6":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nScaler = preprocessing.StandardScaler().fit(X)\nX = Scaler.transform(X)\nX[0:3]","6bb89238":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","cbc0b6d6":"from sklearn.neighbors import KNeighborsClassifier","7f3b74a6":"k = 4\nknn = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nknn","cdae8272":"knn_yhat = knn.predict(X_test)\nknn_yhat[0:5]","d7c4cc47":"from sklearn import metrics\nKNN_Score = metrics.accuracy_score(y_test, knn_yhat)                                               \nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, knn.predict(X_train)))\nprint(\"Test set Accuracy: \",KNN_Score )","db56620a":"Ks = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    knn = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=knn.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\nmean_acc","6c7d5de4":"plt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+\/- 1xstd','+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()\n\nprint( \"The best accuracy for KNN method was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) ","3972ab04":"k = 7\nknn = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nknn","7cc4c8c6":"knn_yhat = knn.predict(X_test)\nknn_yhat[0:5]","65127b05":"KNN_Score = metrics.accuracy_score(y_test, knn_yhat)                                               \nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, knn.predict(X_train)))\nprint(\"Test set Accuracy: \",KNN_Score )","52459c1f":"from sklearn.metrics import classification_report, confusion_matrix, recall_score ,f1_score\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","8f48649d":"# Compute confusion matrix\nKNN_cnf_matrix = confusion_matrix(y_test, knn_yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(KNN_cnf_matrix, classes=['Heart Disease=1','Heart Disease=0'],normalize= False,  title='KNN Confusion matrix')","708c4c22":"print (classification_report(y_test, knn_yhat))\nKNN_Recall = recall_score(y_test, knn_yhat)\nKNN_F1Score = f1_score(y_test, knn_yhat)","544fd523":"X=df.drop('target',axis=1).values    \ny=df['target'].values","572aa3a4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","61a2486d":"from sklearn.tree import DecisionTreeClassifier","e6a368a0":"DTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\nDTree","0163e7de":"DTree.fit(X_train,y_train)","fe11107b":"Tree_yhat = DTree.predict(X_test)","0bea116e":"from sklearn import metrics\n\nDTree_Score = metrics.accuracy_score(y_test, Tree_yhat) \nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, DTree.predict(X_train)))\nprint(\"DecisionTrees's Accuracy: \", DTree_Score)","977296b6":"# Compute confusion matrix\nDTree_cnf_matrix = confusion_matrix(y_test, Tree_yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(DTree_cnf_matrix, classes=['Heart Disease=1','Heart Disease=0'],normalize= False,  title='DTree Confusion matrix')","f0612b03":"print (classification_report(y_test, Tree_yhat))\nDTree_Recall = recall_score(y_test, Tree_yhat)\nDTree_F1Score = f1_score(y_test, Tree_yhat)","6427a2b1":"X=df.drop('target',axis=1).values    \ny=df['target'].values","d5dcd7c7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","0c373ce9":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","59c6a57e":"LR_yhat = LR.predict(X_test)\nLR_yhat[0:5]","b7aaf64c":"LR_yhat_prob = LR.predict_proba(X_test)\nLR_yhat_prob[0:5]","107db082":"from sklearn import metrics\n\nLR_Score = metrics.accuracy_score(y_test, LR_yhat) \nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, LR.predict(X_train)))\nprint(\"Logistic Regression's Accuracy: \", LR_Score)","75e22ca9":"# Compute confusion matrix\nLR_cnf_matrix = confusion_matrix(y_test, LR_yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(LR_cnf_matrix, classes=['Heart Disease=1','Heart Disease=0'],normalize= False,  title='LR Confusion matrix')","9e6faf5e":"print (classification_report(y_test, LR_yhat))\nLR_Recall = recall_score(y_test, LR_yhat)\nLR_F1Score = f1_score(y_test, LR_yhat)","22543f2c":"X=df.drop('target',axis=1).values    \ny=df['target'].values","c98034f9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","0c4ef50e":"from sklearn import svm\nSVM = svm.SVC(kernel='rbf')\nSVM.fit(X_train, y_train) ","e640f4cc":"SVM_yhat = SVM.predict(X_test)\nSVM_yhat [0:5]","c3e135de":"from sklearn import metrics\n\nSVM_Score = metrics.accuracy_score(y_test, SVM_yhat)\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, SVM.predict(X_train)))\nprint(\"SVM's Accuracy: \", SVM_Score)","1fa7963d":"# Compute confusion matrix\nSVM_cnf_matrix = confusion_matrix(y_test, SVM_yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(SVM_cnf_matrix, classes=['Heart Disease=1','Heart Disease=0'],normalize= False,  title='SVM Confusion matrix')","c8a7a13e":"print (classification_report(y_test, SVM_yhat))\nSVM_Recall = recall_score(y_test, SVM_yhat)\nSVM_F1Score = f1_score(y_test, SVM_yhat)","1873c817":"pd.options.display.float_format = \"{:,.2f}\".format\n\nScores_Comparison = {'Methoods': ['KNN', 'Decision Tree', 'Logistic Regression', 'SVM'],\n                     'Accuracy': [KNN_Score, DTree_Score, LR_Score, SVM_Score] ,\n                     'Recall': [KNN_Recall, DTree_Recall, LR_Recall, SVM_Recall],\n                     'F1-Score': [KNN_F1Score, DTree_F1Score, LR_F1Score, SVM_F1Score]\n                    }\nScores_Comparison = pd.DataFrame(data=Scores_Comparison)\nScores_Comparison.sort_values(by='Recall', ascending=False)","42757f02":"It seems that \"cp\", \"thalach\", \"slope\" and \"restepg\" have more correlation with target value. but as we see in the matrix, \"thalach\" and \"slope\" have a considerable correlation with each other,  and we see the same thing for \"cp\", and \"thalach\" so probaely we should be careful about multicolinearty. (in this notebook, I will use all the features to predict the target value)","5704765d":"As we see in the pair plot, the four features have different distributions for each target class.","2d0d92cc":"# Classification with KNN","7a3a9867":"In this notebook, we first do some EDA to get a better understanding of the data. Then we will create different classification methods to detect patients with a high risk of having heart disease. In the end, we will compare the performance of the models that we made to choose the best one.\n\nThe Classification methods that we are going to use are **KNN**, **Decision Tree**, **Logistic Regression** and **SVM**.\n\nYou can find the description of the data set in [here](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci) ","0309f403":"Since we use KNN and it calculates the distance between our data points. For the model to work better, we scale our data using the normal distribution:","766818f8":"Splitting the dataset into train and test sets:","ab3aa9c6":"Prediction for the test set:","56a8d63f":"Now let's check if there is any missing value:","6e9d8b18":"Let's Explore the data and see what it looks like:","0fe1ed85":"## Logistic Regression Evaluation","e9219074":"Let's check the distribution of each variable:","44a18713":"We considered the K equal to 4 but is that the best K? let's find out:","fd6c5b5b":"Great, there is no missing value!!","d1d2333e":"# EDA","0314807e":"## Decision Tree Evaluation","4ac5d90d":"Fitting a KNN Model with K=4:","8f1daef7":"Although the Decision Tree method has higher accuracy and f1-score, in this case (detecting heart disease), it is essential that we can classify those with heart disease with better accuracy, so I choose logistic regression as the best model for the classification of this dataset since it has a higher recall score.","c0f5fae8":"## KNN Evaluation","05e50869":"Now I'm  going to create a correlation matrix to see which features have more effect on the target value:","e401b072":"Defining  a function to plot confusion matrix (we will use it for all methods):","4a183a95":"# Comparing Classification Methods","530ea47a":"# Classification with Decision Tree","2d97018a":"Let's define our independent variables and our Response variable:","d244c1f2":"Evaluation of the model:","e3843d68":"# Classification with SVM","bddadb35":"# Conclusion","bb98560c":"Checking the data types:","e9dc5576":"# Heart Disease Detection","db3e7b94":"# Classification with Logistic Regression"}}