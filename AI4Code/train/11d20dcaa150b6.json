{"cell_type":{"d4a5ced4":"code","eba3ccd8":"code","ac60930a":"code","a1031253":"code","2ff1cc50":"code","71db3e93":"code","cc4119e1":"markdown","d94c9ba6":"markdown","7b877152":"markdown"},"source":{"d4a5ced4":"!pip install tensorflow==2.3 \n!pip install bert-for-tf2","eba3ccd8":"import os\nimport math\nimport datetime\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport bert\nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator","ac60930a":"class Label_Detection_Data:\n    data_column = 'pname'\n    label_column = 'category_id'\n\n    def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=64): # 192\n        self.tokenizer = tokenizer\n        self.max_seq_len = 0\n        self.classes = classes\n\n        train, test = map(lambda df: df.reindex(df[Label_Detection_Data.data_column].str.len().sort_values().index), [train, test])\n\n        ((self.x_train, self.y_train), (self.x_test, self.y_test)) = map(self._prepare, [train, test])\n\n        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n        self.x_train, self.x_test = map(self._pad, [self.x_train, self.x_test])\n\n    def _prepare(self, df):\n        x, y = [], []\n        for _, row in tqdm(df.iterrows()):\n            text, label = row[Label_Detection_Data.data_column], row[Label_Detection_Data.label_column]\n            tokens = self.tokenizer.tokenize(text)\n            tokens = ['[CLS]'] + tokens + ['[SEP]']\n            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n            x.append(token_ids)\n            y.append(self.classes.index(label))\n\n        return np.array(x), np.array(y)\n\n    def _pad(self, ids):\n        x = []\n        for input_ids in ids:\n            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n            x.append(np.array(input_ids))\n\n        return np.array(x)\n\n\ndef create_model(max_seq_len, bert_checkpoint_file):\n    with tf.io.gfile.GFile(bert_config_file, 'r') as reader:\n        bc = StockBertConfig.from_json_string(reader.read())\n        bert_params = map_stock_config_to_params(bc)\n        bert_params.adapter_size = None\n        bert = BertModelLayer.from_params(bert_params, name='bert')\n\n    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name='input_ids')\n    bert_output = bert(input_ids)\n\n    X = tf.keras.layers.SpatialDropout1D(0.2)(bert_output)\n    X = tf.keras.layers.Conv1D(64, 5, activation='relu')(X)\n    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))(X)\n    #X = tf.keras.layers.Dense(units=len(classes), activation='relu')(X)\n    #X = tf.keras.layers.Dropout(0.5)(X)\n    #X = tf.keras.layers.Dense(512, activation='relu')(X)\n    logits = keras.layers.Dense(units=len(classes), activation='sigmoid')(X)\n    \n\n    model = keras.Model(inputs=input_ids, outputs=logits)\n    model.build(input_shape=(None, max_seq_len))\n\n    load_stock_weights(bert, bert_checkpoint_file)\n\n    return model\n\"\"\"\n    #cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n    #cls_out = keras.layers.Dropout(0.5)(cls_out)\n    #X = tf.keras.layers.LSTM(100, return_sequences=True)(bert_output)\n    #X = tf.keras.layers.GlobalMaxPool1D()(X)\n    X = tf.keras.layers.Dense(20, activation='relu')(bert_output)\n    #X = tf.keras.layers.Dropout(0.4)(X)\n    #X = tf.keras.layers.Dense(5, activation='softmax')(X)\n    #cls_out = keras.layers.LSTM(units=100, activation ='sigmoid')(bert_output)\n    #logits = keras.layers.Dense(units=768, activation='tanh')(cls_out)\n    #logits = keras.layers.Dropout(0.5)(cls_out)\n    logits = keras.layers.Dense(units=len(classes), activation='softmax')(X)\n\"\"\"\n\n\n\"\"\"X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(embedding_layer)\nX = tf.keras.layers.GlobalMaxPool1D()(X)\nX = tf.keras.layers.Dense(64, activation='relu')(X)\nX = tf.keras.layers.Dropout(0.2)(X)\nX = tf.keras.layers.Dense(5, activation='softmax')(X)\n\nmodel = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\"\"\"\n\ndef remove_datapoints(x_train, y_train, x_test, y_test):\n    percentage = 0.75\n\n    def slash_datapoints(*args):\n        labels, counts = np.unique(args[1], return_counts=True)\n        bloodborne = []\n        for label in labels:\n            indices = np.array(np.where(args[1] == label)).tolist()[0]\n            num_indices = round(len(indices) * percentage)\n            dead_indices = np.random.choice(indices, num_indices, replace=False)\n            bloodborne.append(dead_indices)\n\n        bloodborne = np.concatenate(bloodborne).ravel()\n        temp0 = np.delete(args[0], bloodborne, axis=0)\n        temp1 = np.delete(args[1], bloodborne, axis=0)\n\n        return temp0, temp1\n\n    #print(x_train.shape)\n    #print(y_train.shape)\n    #print(x_test.shape)\n    #print(y_test.shape)\n\n    x_train, y_train = slash_datapoints(x_train, y_train)\n    x_test, y_test = slash_datapoints(x_test, y_test)\n    \"\"\" from imblearn.over_sampling import SMOTE\n    smote = SMOTE('minority')\n    x_train, y_train = smote.fit_sample(x_trai,y_trai)\"\"\"\n    \n    \n    #print(x_train.shape)\n    #print(y_train.shape)\n    #print(x_test.shape)\n    #print(y_test.shape)\n\n    return x_train, y_train, x_test, y_test\n","a1031253":"train = pd.read_csv('..\/input\/datam\/train_datam.csv')\ntest = pd.read_csv('..\/input\/datam\/test_datam.csv')","2ff1cc50":"bert_checkpoint_file = '..\/input\/bertpretrained\/multi_cased_L-12_H-768_A-12\/multi_cased_L-12_H-768_A-12\/bert_model.ckpt'\nbert_config_file = '..\/input\/bertpretrained\/multi_cased_L-12_H-768_A-12\/multi_cased_L-12_H-768_A-12\/bert_config.json'\n\ntokenizer = FullTokenizer(vocab_file='..\/input\/bertpretrained\/multi_cased_L-12_H-768_A-12\/multi_cased_L-12_H-768_A-12\/vocab.txt')\n\nclasses = train.category_id.unique().tolist()\n\ndata = Label_Detection_Data(train, test, tokenizer, classes, max_seq_len=30)\n\ndata.x_train, data.y_train, data.x_test, data.y_test = remove_datapoints(data.x_train, data.y_train, data.x_test, data.y_test)\n\n#print(data.max_seq_len)\n\nprint(data.x_train.shape)\nprint(data.x_test.shape)\n\nprint(data.y_train.shape)\nprint(data.y_test.shape)","71db3e93":"model = create_model(data.max_seq_len, bert_checkpoint_file)\n\nmodel.compile(optimizer=keras.optimizers.Adam(1e-2), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\nmodel.summary()\n\nhistory = model.fit(data.x_train, data.y_train,  validation_split=0.2, batch_size=64, epochs=10)\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='best')\n#plt.show()\nplt.savefig('bert_model-accuracy.png')\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='best')\n#plt.show()\nplt.savefig('bert_model-loss.png')\n\n_, accuracy = model.evaluate(data.x_test, data.y_test)\nprint('Accuracy: ', accuracy)\n\nwith open('bert_model_results.txt', 'a') as file:\n    file.write('{}'.format(accuracy * 100))\n\n","cc4119e1":"    samples from the positive class.\n\n    def augment_text(df,samples=300,pr=0.2):\n        aug_w2v.aug_p=pr\n        new_text=[]\n     \n    ##selecting the minority class samples\n        df_n=df[df.target==1].reset_index(drop=True)\n\n    ## data augmentation loop\n        for i in tqdm(np.random.randint(0,len(df_n),samples)):\n        \n            text = df_n.iloc[i]['text']\n            augmented_text = aug_w2v.augment(text)\n            new_text.append(augmented_text)\n    \n    \n    ## dataframe\n        new=pd.DataFrame({'text':new_text,'target':1})\n        df=shuffle(df.append(new).reset_index(drop=True))\n        return df\n   \n    train = augment_text(train)","d94c9ba6":"https:\/\/github.com\/rsd16\/SemEval2017-Task-4-Subtask-A-Classification-Sentiment-Analysis\/blob\/main\/bert_model.py ","7b877152":"\"\"\"    from imblearn.over_sampling import SMOTE\n    smote = SMOTE('minority')\n    x_train, y_train = smote.fit_sample(x_trai,y_trai)\"\"\""}}