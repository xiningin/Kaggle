{"cell_type":{"6f63ac55":"code","13d8d7d6":"code","a5e387a0":"code","a3a7fd34":"code","b1320ab3":"code","edcbf96b":"code","44c9df70":"code","d023c370":"code","0f44d061":"code","e169ce8a":"code","8f3b229d":"code","33db4982":"markdown","d8d71a14":"markdown","a76c203e":"markdown","49e8c4dd":"markdown","48de4d72":"markdown","6afac341":"markdown","8f7abbc7":"markdown","445d3c53":"markdown","90a3458e":"markdown","75e75669":"markdown","d0ef3b98":"markdown","a9481c4f":"markdown","4ac1a012":"markdown","2dbafaf6":"markdown","1db86e5a":"markdown","96f73fc4":"markdown","f673c39e":"markdown","0098d039":"markdown","b031d2ae":"markdown","071dac48":"markdown","9aee2857":"markdown","9bce381b":"markdown","3a168b00":"markdown","c32b6b65":"markdown"},"source":{"6f63ac55":"import torch","13d8d7d6":"device='cuda' if torch.cuda.is_available() else 'cpu'\n\nmy_tensor=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float32,device=device, requires_grad=True)\n\nprint(my_tensor)\n\nprint(my_tensor.dtype)\n\nprint(my_tensor.device)\n\nprint(my_tensor.shape)\n\nprint(my_tensor.requires_grad)","a5e387a0":"# some common mathods but there are many more but i find them useful most of the time\n\nx = torch.empty(size=(3, 3))  # Tensor of shape 3x3 with uninitialized data\n\nx = torch.zeros((3, 3))  # Tensor of shape 3x3 with values of 0\n\nx = torch.rand((3, 3))  # Tensor of shape 3x3 with values from uniform distribution in interval [0,1)\n\nx = torch.ones((3, 3))  # Tensor of shape 3x3 with values of 1\n\nx = torch.eye(5, 5)  # Returns Identity Matrix I, (I <-> Eye), matrix of shape 2x3\n\nx = torch.arange(start=0, end=5, step=1)  # Tensor [0, 1, 2, 3, 4], note, can also do: torch.arange(11)\n\nx = torch.linspace(start=0.1, end=1, steps=10)  # x = [0.1, 0.2, ..., 1]\n\nx = torch.empty(size=(1, 5)).normal_(mean=0, std=1)  # Normally distributed with mean=0, std=1\n\nx = torch.empty(size=(1, 5)).uniform_(0, 1)  # Values from a uniform distribution low=0, high=1\n\nx = torch.diag(torch.ones(3))  # Diagonal matrix of shape 3x3\n","a3a7fd34":"tensor = torch.arange(4)  # [0, 1, 2, 3] Initialized as int64 by default\nprint(f\"Converted Boolean: {tensor.bool()}\")  # Converted to Boolean: 1 if nonzero\n\n\nprint(f\"Converted int16 {tensor.short()}\")  # Converted to int16\n\n\nprint(f\"Converted int64 {tensor.long()}\")  # Converted to int64 (This one is very important, used super often)\n\n\nprint(f\"Converted float16 {tensor.half()}\")  # Converted to float16\n\n\nprint(f\"Converted float32 {tensor.float()}\")  # Converted to float32 (This one is very important, used super often)\n\n\nprint(f\"Converted float64 {tensor.double()}\")  # Converted to float64","b1320ab3":"import numpy as np\n\nnp_array = np.zeros((5, 5))\ntensor = torch.from_numpy(np_array)\nnp_array_again = (\n    tensor.numpy()\n)  # np_array_again will be same as np_array (perhaps with numerical round offs)\n","edcbf96b":"x = torch.tensor([1, 2, 3])\ny = torch.tensor([9, 8, 7])\nz1 = torch.empty(3)\ntorch.add(x, y, out=z1)  # This is one way\n\nz2 = torch.add(x, y)  # This is another way\n\nz = x + y  # This is simple and clean.\n\n## Subtraction \n\nz = x - y  # We can do similarly as the preferred way of addition\n\n## Division \n\nz = torch.true_divide(x, y)  # Will do element wise division if of equal shape\n\n## Inplace Operations \n\nt = torch.zeros(3)\n\nt.add_(x)  # Whenever we have operation followed by _ it will mutate the tensor in place\n\nt += x  # Also inplace: t = t + x is not inplace, bit confusing.\n\n\n##  Exponentiation (Element wise if vector or matrices)\n\nz = x.pow(2)  # z = [1, 4, 9]\nz = x ** 2  # z = [1, 4, 9]\n\n## Simple Comparison \n\nz = x > 0  # Returns [True, True, True]\nz = x < 0  # Returns [False, False, False]","44c9df70":"x1 = torch.rand((2, 5))\n\nx2 = torch.rand((5, 3))\n\nx3 = torch.mm(x1, x2)  # Matrix multiplication of x1 and x2, out shape: 2x3\n\nx3 = x1.mm(x2)  # Similar as line above\n\n\n# -- Matrix Exponentiation --\n\nmatrix_exp = torch.rand(5, 5)\n\nprint(   matrix_exp.matrix_power(3))  # is same as matrix_exp (mm) matrix_exp (mm) matrix_exp\n\n\n# -- Element wise Multiplication --\n\n\nz = x * y  # z = [9, 16, 21] = [1*9, 2*8, 3*7]\n\n\n# -- Dot product --\n\n\nz = torch.dot(x, y)  # Dot product, in this case z = 1*9 + 2*8 + 3*7\n\n\n\n# -- Batch Matrix Multiplication --\n\nbatch = 32\nn = 10\nm = 20\np = 30\ntensor1 = torch.rand((batch, n, m))\ntensor2 = torch.rand((batch, m, p))\nout_bmm = torch.bmm(tensor1, tensor2)  # Will be shape: (b x n x p)","d023c370":"\nsum_x = torch.sum(x, dim=0)  # Sum of x across dim=0 (which is the only dim in our case), sum_x = 6\n\n\nvalues, indices = torch.max(x, dim=0)  # Can also do x.max(dim=0)\n\n\nvalues, indices = torch.min(x, dim=0)  # Can also do x.min(dim=0)\n\n\nabs_x = torch.abs(x)  # Returns x where abs function has been applied to every element\n\n\nz = torch.argmax(x, dim=0)  # Gets index of the maximum value\n\n\nz = torch.argmin(x, dim=0)  # Gets index of the minimum value\n\n\nmean_x = torch.mean(x.float(), dim=0)  # mean requires x to be float\n\n\nz = torch.eq(x, y)  # Element wise comparison, in this case z = [False, False, False]\n\n\nsorted_y, indices = torch.sort(y, dim=0, descending=False) #sorting \n\n\nz = torch.clamp(x, min=0)\n# All values < 0 set to 0 and values > 0 unchanged (this is exactly ReLU function)\n# If you want to values over max_val to be clamped, do torch.clamp(x, min=min_val, max=max_val)\n\n\nx = torch.tensor([1, 0, 1, 1, 1], dtype=torch.bool)  # True\/False values\n\n\nz = torch.any(x)  # will return True, can also do x.any() instead of torch.any(x)\n\n\nz = torch.all(x)  # will return False (since not all are True), can also do x.all() instead of torch.all()\n","0f44d061":"batch_size = 10\nfeatures = 25\nx = torch.rand((batch_size, features))\n\n# Get first examples features\nprint(x[0].shape)  # shape [25], this is same as doing x[0,:]\n\n\n# Get the first feature for all examples\nprint(x[:, 0].shape)  # shape [10]\n\n\n# For example: Want to access third example in the batch and the first ten features\nprint(x[2, 0:10].shape)  # shape: [10]\n\n\n# For example we can use this to, assign certain elements\nx[0, 0] = 100","e169ce8a":"x = torch.arange(10)\n\nindices = [2, 5, 8]\n\nprint(x[indices])  # x[indices] = [2, 5, 8]\n\n\nx = torch.rand((3, 5))\n\nrows = torch.tensor([1, 0])\ncols = torch.tensor([4, 0])\n\nprint(x[rows, cols])  # Gets second row fifth column and first row first column\n\n\n\n# More advanced indexing\n\nx = torch.arange(10)\n\nprint(x[(x < 2) | (x > 8)])  # will be [0, 1, 9]\n\nprint(x[x.remainder(2) == 0])  # will be [0, 2, 4, 6, 8]\n\n\n# Useful operations for indexing\n\nprint(torch.where(x > 5, x, x * 2))  # gives [0, 2, 4, 6, 8, 10, 6, 7, 8, 9], all values x > 5 yield x, else x*2\n\nx = torch.tensor([0, 0, 1, 2, 2, 3, 4]).unique()  # x = [0, 1, 2, 3, 4]\nprint(x.ndimension())  # The number of dimensions, in this case 1. if x.shape is 5x5x5 ndim would be 3\n\nx = torch.arange(10)\nprint(x.numel())  # The number of elements in x (in this case it's trivial because it's just a vector)\n","8f3b229d":"x = torch.arange(9)\n\n# Let's say we want to reshape it to be 3x3\nx_3x3 = x.view(3, 3)\n\n# We can also do (view and reshape are very similar)\n# and the differences are in simple terms (I'm no expert at this),\n# is that view acts on contiguous tensors meaning if the\n# tensor is stored contiguously in memory or not, whereas\n# for reshape it doesn't matter because it will copy the\n# tensor to make it contiguously stored, which might come\n# with some performance loss.\n\nx_3x3 = x.reshape(3, 3)     # it is the best works always but some performance loss\n\n\n# If we for example do:\ny = x_3x3.t()          # transpose of x_3x3\nprint(y.is_contiguous())  \n# This will return False and if we try to use view now, it won't work!\n# y.view(9) would cause an error, reshape however won't\n\n\nprint(y.contiguous().view(9))  # Calling .contiguous() before view and it works\n\n\n# We want to add two tensors dimensions togethor\nx1 = torch.rand(2, 5)\nx2 = torch.rand(2, 5)\nprint(torch.cat((x1, x2), dim=0).shape)  # Shape: 4x5\nprint(torch.cat((x1, x2), dim=1).shape)  # Shape 2x10\n\n\n# Let's say we want to unroll x1 into one long vector with 10 elements, we can do:\nz = x1.view(-1)  # And -1 will unroll everything\n\n# If we instead have an additional dimension and we wish to keep those as is we can do:\nbatch = 64\nx = torch.rand((batch, 2, 5))\nz = x.view(\n    batch, -1\n)  # And z.shape would be 64x10, this is very useful stuff and is used all the time\n\n","33db4982":"<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">\n        1. Initialization methods \n        ","d8d71a14":"<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">4 Mathamatical operation","a76c203e":"<center style=\"font-family:cursive; font-size:25px; color:#159364;\">That was some essential Tensor operations, hopefully you found it useful!.<\/center>\n<\/p>\n\n","49e8c4dd":"<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">3 Array to Tensor conversion and vice-versa","48de4d72":"<h1 style=\"font-family:verdana;\"> <center>\ud83d\udd25 Pytorch Basics - Initializing Tensors, Math, Indexing, Reshaping \ud83d\udd25<\/center> <\/h1>\n<p><center style=\"color:#159364; font-size:25px; font-family:cursive;\">Strive for Excellence, Not Perfection<\/center><\/p>\n\n***","6afac341":"![](https:\/\/miro.medium.com\/max\/2400\/1*aqNgmfyBIStLrf9k7d9cng.jpeg)","8f7abbc7":"***","445d3c53":"***","90a3458e":"  <div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">7 Tensor Indexing   ","75e75669":"***","d0ef3b98":"***","a9481c4f":"<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">2 How to make initialized tensors to other types (int, float, double)","4ac1a012":"****","2dbafaf6":"<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">8 Fancy Indexing","1db86e5a":"***","96f73fc4":"***","f673c39e":"<p style=\"font-size:25px; font-family:verdana; line-height: 1.7em\">\n Walk through of a lot of different useful Tensor Operations, where we\ngo through what I think are four main parts in:<\/p><br>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">      \n1. Initialization of a Tensor<\/p><br>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\"> \n2. Tensor Mathematical Operations and Comparison<\/p><br>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">     \n3. Tensor Indexing<\/p><br>\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\"> \n4. Tensor Reshaping <\/p><br>\n\n","0098d039":"<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">6 Other useful tensor operations","b031d2ae":"<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">9 Tensor Reshaping","071dac48":"# Cheatsheet for Pytorch Click on the [Link](https:\/\/www.sznajdman.com\/wp-content\/uploads\/2018\/02\/pytorch-cheat.jpg)","9aee2857":"***","9bce381b":"***","3a168b00":"<center style=\"font-family:cursive; font-size:25px; color:#159364;\">Please Do Upvote if this kernel was helpful.<\/center>\n<\/p>\n\n","c32b6b65":"<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\"> 5 Matrix Operations"}}