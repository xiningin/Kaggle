{"cell_type":{"74daee96":"code","093ebd1f":"code","864afeda":"code","5eb28d1b":"code","fc66617d":"code","84505301":"code","c2fc7f44":"code","8e491795":"code","47155911":"code","c3d6775c":"code","25c653fe":"code","8a38d7d1":"code","22ada5a9":"code","6b387262":"code","e6e9d092":"code","8a4e354d":"code","40a2eb79":"code","99459576":"code","67024b5e":"code","df57dd73":"code","cb0041c1":"code","c0fc24ed":"code","ecdaf695":"code","6ef27443":"code","b47cb56f":"code","6448cbc7":"code","5ed70c41":"code","f5cbcb20":"code","33c2c827":"code","72f95fc5":"code","819bb9ba":"code","da3d542e":"code","ff0e3612":"code","3566dbe1":"code","ffa6cf12":"code","8d0208e5":"code","c58fb80f":"code","cd9e8795":"code","82090088":"code","29b5484d":"code","07693c62":"code","74fad2ca":"code","2d0c408d":"code","f3db03d7":"code","9101af11":"code","410065d6":"code","bf863306":"code","ba420578":"code","407ae384":"code","54b79556":"code","53b1da99":"code","4791af17":"code","b392cc19":"code","b2f6a9a4":"code","a78030d3":"code","5b9b40dc":"code","72628969":"code","268ae315":"code","425cc210":"code","b28cfdd2":"code","bbb78c99":"code","6e4ef416":"code","7b01e53b":"code","8e8ba560":"code","362ad4b9":"code","5faa1345":"code","a2673f64":"code","e2965bea":"code","cf4dfc8b":"code","3f5717e3":"code","c33a6711":"code","9699efc7":"code","ff99b8a6":"code","8b292c36":"code","1b2e7687":"code","06e3b080":"code","145dc9cb":"code","63317a9c":"code","71510c98":"code","ebdd2b46":"code","b5fd697e":"code","a20db85c":"code","52398692":"code","fd197054":"code","251495c4":"code","4255cdc6":"code","dd6844b7":"code","6539c97d":"code","68141bac":"code","fc443dd1":"code","009b98a9":"code","96ea832f":"code","a28aa201":"code","b53cf939":"code","6ade5a3d":"code","12e30830":"code","79391e10":"code","319223ef":"code","12adbd5d":"code","4975294f":"code","82ac7195":"code","af1b12e2":"code","f4d7bce3":"code","435c6f17":"code","d38c64b7":"code","810d4307":"code","29c7eb33":"code","4a0046cd":"code","dd2e561f":"code","262bf6b3":"code","d198cf84":"code","81047dd3":"code","c14bc0fa":"code","155469e2":"code","e721a0f3":"code","f9cf80b9":"code","6ec8572f":"code","47ce1cb2":"code","d499e2d9":"code","f7d98a46":"code","49590c11":"code","db4c9a6a":"code","7844a8be":"code","be182f3c":"code","907d0864":"code","95a27b3a":"code","e64b9cbb":"code","48ca2b52":"code","88beccc6":"code","859b477d":"code","ca159239":"code","b3f33e15":"code","7b204bc1":"code","a04bfa49":"code","88114c2f":"code","1193e341":"code","ced02b7a":"code","96d9b9e4":"code","406598ac":"code","c6f6e0c9":"code","04a7ab57":"code","091a5ddd":"code","5d39f220":"code","78982ee8":"code","c1ed537d":"code","fa05eaf1":"code","accae0b2":"code","3e699115":"code","d45f02d9":"code","abc2e06f":"code","3f4be21a":"code","b33c628e":"code","21c644f9":"code","b33adb1e":"code","5072cf3f":"code","4eee2bb0":"code","e9d70449":"code","b9379531":"code","2556ef4b":"code","f6f69036":"code","4cd94242":"code","d9bf4cf9":"code","e9eb57e8":"code","60f0fd28":"code","2a4a160c":"code","891df12a":"code","3d5a790c":"code","b28b3dab":"code","996e5e17":"code","a6252824":"code","a59273e4":"code","63ddbb34":"code","deb20b59":"code","11b0dc7d":"code","81f722b7":"code","3b511a77":"code","ab1cac7f":"code","5f5e88e8":"code","666909a8":"code","2ed9743d":"code","ab831544":"code","71d51def":"code","e4cb754d":"markdown","10ec0b72":"markdown","a8a2819e":"markdown","30f0eb85":"markdown","d12dea49":"markdown","c75fac6d":"markdown","dfa1cad2":"markdown","85dc69f1":"markdown","103398ce":"markdown","82c9c6fe":"markdown","d1a56c39":"markdown","358a434c":"markdown","9ddf1ae8":"markdown","f812adeb":"markdown","03f1300d":"markdown","d1de505e":"markdown","a8fd7bf2":"markdown","aa8bf9e5":"markdown","d22d49a3":"markdown","bcacfa34":"markdown","f622108a":"markdown","898e4716":"markdown","5c5a660d":"markdown","c2bfabb2":"markdown","91e7e859":"markdown","76a33d58":"markdown","4b543a81":"markdown","6a13ccfc":"markdown","70f29587":"markdown","b0c41bd0":"markdown","2586cc6a":"markdown","fd731988":"markdown","69da8c45":"markdown","c10becdb":"markdown","2b65860c":"markdown","9aeec9cd":"markdown","70d8c144":"markdown","5077b943":"markdown","23376c5f":"markdown","2f7b1d76":"markdown","eac77fbe":"markdown","0721b04a":"markdown","82e5d228":"markdown","abde7113":"markdown","7b4ff0a1":"markdown","4a67b3d6":"markdown","031a50e0":"markdown","9e83d498":"markdown","0908f361":"markdown","492f6e93":"markdown","9356adef":"markdown","3531c585":"markdown","5a31d797":"markdown","8a9f3801":"markdown","29701ae7":"markdown","6524b237":"markdown","2bb102ab":"markdown","dbf3da64":"markdown","3d23ff9f":"markdown","1b68e503":"markdown","c2c4750c":"markdown","2f911fcd":"markdown","d28f8022":"markdown","b2f49b01":"markdown","dcb2bb97":"markdown","0a299419":"markdown","34858f6c":"markdown","022a3381":"markdown","66f6807b":"markdown","d17da751":"markdown","fc1b9f30":"markdown","9a1fa437":"markdown","6ee3aa59":"markdown","7a83f5eb":"markdown","888ca363":"markdown","35626884":"markdown","75ce7a48":"markdown","d81f5581":"markdown","810a470b":"markdown","3a760d7b":"markdown","10f9cc91":"markdown","5b56f5c8":"markdown","8b2ae691":"markdown","a3121c8a":"markdown","83d6aac1":"markdown","ee7389d3":"markdown","5e75ca49":"markdown","ec2dcf93":"markdown","4129ffa1":"markdown","048d50c5":"markdown","cc459302":"markdown","de558749":"markdown","96254c9b":"markdown","5bc15e83":"markdown","d413fc1b":"markdown","00dc1ff1":"markdown","74090384":"markdown","45f2665a":"markdown","85b0fd37":"markdown","ebeb932a":"markdown","19b50ba4":"markdown","019f43b7":"markdown","685a40b4":"markdown","91543244":"markdown","4c5c167c":"markdown","e66b57d7":"markdown","e6e4d991":"markdown","33da7145":"markdown","8977e5eb":"markdown","f888037d":"markdown","7805d99b":"markdown","8aa77356":"markdown","ce19ace2":"markdown","b2c2226a":"markdown","c9d5d946":"markdown","61f670d6":"markdown","fa8a69a9":"markdown","53b78ade":"markdown","8a8d6e79":"markdown","a1a7b004":"markdown","68d2ffd0":"markdown","37611531":"markdown","b701851c":"markdown","80365d61":"markdown","00e03fee":"markdown","e33a3628":"markdown","942eda1b":"markdown","d5925792":"markdown","4f17eec9":"markdown","f0b0d366":"markdown","e067af18":"markdown","f676f939":"markdown","e57a2467":"markdown","dfad2d01":"markdown","b2cc4e87":"markdown","ae4090e8":"markdown","878ff537":"markdown","5e8dd2da":"markdown","aed89ad6":"markdown","d0b29d81":"markdown","eb2042b0":"markdown","8c23b698":"markdown","ff1a1567":"markdown","3ea3b43e":"markdown","19e92c71":"markdown","420bbf62":"markdown","9c0c4155":"markdown","c6f03979":"markdown","72c5d6cc":"markdown","6564c4f7":"markdown","37087a1f":"markdown","c664a4ba":"markdown","6a191952":"markdown","9f894891":"markdown","3ff607c0":"markdown","433221f8":"markdown","71b220e9":"markdown","539e5a4f":"markdown","2241bf8e":"markdown"},"source":{"74daee96":"# data processing tools\nimport pandas as pd\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nfrom math import sqrt\nimport itertools\n\n# model tools\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport scipy.stats as stats\nfrom scipy.stats import norm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import cross_val_score, RepeatedKFold, train_test_split, GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.inspection import permutation_importance\nfrom sklearn import neighbors\nimport xgboost as xgb\n\n# Visualization tools\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None","093ebd1f":"# load and look at our king county housing data\ndf = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\ndf","864afeda":"df.dtypes","5eb28d1b":"# converting our date to a proper datetime\ndf['date'] = pd.to_datetime(df['date'])","fc66617d":"# earliest date\ndf['date'].min()","84505301":"# latest date\ndf['date'].max()","c2fc7f44":"# We're applying a time series multiplier to our rows based on date sold. Our data spans from\n# May 2014-May 2015 so we divide it into a 12 month time series.\ndf.loc[(df['date']>'2014-05-01') & (df['date']<'2014-06-01'), 'time_series'] = 12\ndf.loc[(df['date']>='2014-06-01') & (df['date']<'2014-07-01'), 'time_series'] = 11\ndf.loc[(df['date']>='2014-07-01') & (df['date']<'2014-08-01'), 'time_series'] = 10\ndf.loc[(df['date']>='2014-08-01') & (df['date']<'2014-09-01'), 'time_series'] = 9\ndf.loc[(df['date']>='2014-09-01') & (df['date']<'2014-10-01'), 'time_series'] = 8\ndf.loc[(df['date']>='2014-10-01') & (df['date']<'2014-11-01'), 'time_series'] = 7\ndf.loc[(df['date']>='2014-11-01') & (df['date']<'2014-12-01'), 'time_series'] = 6\ndf.loc[(df['date']>='2014-12-01') & (df['date']<'2015-01-01'), 'time_series'] = 5\ndf.loc[(df['date']>='2015-01-01') & (df['date']<'2015-02-01'), 'time_series'] = 4\ndf.loc[(df['date']>='2015-02-01') & (df['date']<'2015-03-01'), 'time_series'] = 3\ndf.loc[(df['date']>='2015-03-01') & (df['date']<'2015-04-01'), 'time_series'] = 2\ndf.loc[(df['date']>='2015-04-01') & (df['date']<'2015-05-01'), 'time_series'] = 1\ndf.loc[(df['date']>='2015-05-01') & (df['date']<'2015-06-01'), 'time_series'] = 0\n\n# adjusting up our sale prices to account for appreciation using the yearly appreciation for home sales in King County\n# from May 2014 to May 2015\nmonthly_appreciation = .0905\/12\ndf['adj_price'] = round(df['price']*(1+((monthly_appreciation)*df['time_series'])), 0)\n\n# we're renaming our adjusted price column to be price\ndf.rename(columns={'price': 'orig_price', \"adj_price\": \"price\"}, inplace=True)\n\n# renumber our data\ndf.set_index('price', inplace=True)\ndf.reset_index(inplace=True)\n\n# we're now going to extract just the month from our datetime, and store that in a new column\n# we won't be using our entire date - we're interested in the month that houses are sold\ndf['month_sold'] = pd.DatetimeIndex(df['date']).month\ndf.drop('date', axis=1, inplace=True)","8e491795":"# Look for duplicate data on lat\/long\n\ndf[df.duplicated(subset=['lat','long'], keep=False)].sort_values('lat')","47155911":"# We have a lot of duplicate entries. We're going to keep the later of these entries and hope that if it's an outlier,\n# it's caught in our outlier processing later.\n\ndf.drop_duplicates(['lat','long'], keep='last', inplace=True)","c3d6775c":"# plotting latitude and longitude as a visual scatter plot to look for location outliers\n\nplt.figure(figsize=(25,25))\n\nsns.scatterplot(data=df, x=\"long\", y=\"lat\", hue=\"price\", palette=\"magma_r\");","25c653fe":"# drop the properties east of the metropolitan area\ndf.drop(df[df['long'] > -121.7].index, inplace=True)\n\n# looking for outliers in the percentiles\ndf.describe()","8a38d7d1":"# check how our histograms are looking\n\ndf.hist(figsize=(18,15), bins='auto');","22ada5a9":"def iqr_outliers(column):\n    \"\"\"return the lower range and upper range for the data based on IQR*1.6\"\"\"\n    Q1,Q3 = np.percentile(column , [25,75])\n    iqr = Q3 - Q1\n    lower_range = Q1 - (1.6 * iqr)\n    upper_range = Q3 + (1.6 * iqr)\n    return lower_range,upper_range  \n\n# determing our IQR ranges for lot size, sq footage\nlotlower,lotupper = iqr_outliers(df.sqft_lot)\nsqftlower, sqftupper = iqr_outliers(df.sqft_living)\n\n# dropping the things outside of our lower and upper range\ndf.drop(df[ (df.sqft_lot > lotupper) | (df.sqft_lot < lotlower) ].index , inplace=True)\ndf.drop(df[ (df.sqft_living > sqftupper) | (df.sqft_living < sqftlower) ].index , inplace=True)","6b387262":"# we're using the median house value for a zip code to determine the zip code's sort, so we can visualize the zip code importance\n\n# group our dataframe by zipcode on median home price, sorted ascending. \nzipsorted = pd.DataFrame(df.groupby('zipcode')['price'].median().sort_values(ascending=True))\n\n# rank each zip code and assign rank to new column\nzipsorted['rank'] = np.divmod(np.arange(len(zipsorted)), 1)[0]+1\n\n# function that looks up a segment that a data entry belongs to\ndef make_group(x, frame, column):\n    '''Takes in a line, a lookup table, and a target column\n    returns value of target column\n    ARGUMENTS:\n    line from dataframe x\n    lookup table frame\n    column to return rank'''\n    y = frame.loc[(frame.index == x)][column]\n    z = np.array(y)\n    z[0]\n    return z[0]\n\n# make a new column on our dataframe. Look up each zip entry's group, and append to the column.\ndf['zip_rank'] = df['zipcode'].apply(lambda x: make_group(x, zipsorted, 'rank'))\n\n# apply the median home price per zip code to the data frame\ndf['median_zip'] = df['zipcode'].apply(lambda x: round(df.loc[df['zipcode']==x]['price'].median(), 0))","e6e9d092":"# visualize zip code as a color function, on a plot of price per square footage\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['sqft_living'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Square Feet of Living Space', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Total Square Feet, by Zip Code Median Rank', fontsize=20)\n;","8a4e354d":"# visualize zip code as a color function\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['median_zip'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Median Home Price per Zip', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Zip Code Median, by Zip Code Median Rank', fontsize=20)\n;","40a2eb79":"# Eliminating outliers on a per-zipcode basis\n\nzipcodes = df['zipcode'].unique()\n\nfor i in zipcodes:\n    lower, upper = iqr_outliers(df[df['zipcode'] == i]['price'])\n    df.drop(df[ ( (df.price > upper) & (df['zipcode'] == i) ) | ( (df.price < lower)  & (df['zipcode'] == i) ) ].index , inplace=True)\n","99459576":"# check price stats by zip code and displaying top 30 zip codes by mean\n# I'm looking for suspiciously low \"min\" values\n\nfind_zip_outliers = df.groupby('zipcode')['price'].describe()\nfind_zip_outliers.sort_values('mean', ascending=False).head(35)\n\n# very suspicious values in many zip codes for min\n# 98112, 98109, 98033, 98115, 98027, 98116, 98122, 98117, 98136, 98065, 98144, 98072, 98028","67024b5e":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98109)]\n# There are two listings in this zip code selling for under 250k. We're going to consider this an outlier sale.\n# worth noting these are both the exact same property. These are both bad listings.\n\n# It looks like we missed a dupe due to a slight mismatch in lat. Since this is a dupe AND an anomalous price, we are going to dump it.","df57dd73":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98033)]\n#We're going to consider these outlier sales.","cb0041c1":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98115)]\n#We're going to consider these outlier sales.","c0fc24ed":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98027)]\n# listings under 250k in this zip. We'll drop them.","ecdaf695":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98116)]\n# suspicious but in bad condition. We will drop though.","6ef27443":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98122)]\n# suspicious but VERY tiny. We will leave.","b47cb56f":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98136)]\n# Drop","6448cbc7":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98065)]\n# Drop","5ed70c41":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98144)]\n# Dropping most","f5cbcb20":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98072)]\n# Dropping","33c2c827":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98028)]\n# Dropping","72f95fc5":"df.drop(index=[3844, 11436, 1222, 12823, 1975, 3885, 14001, 18897, 3975, 18332, 3987, 881, 11500, 17578, 8133, 9854, 18338, 5573, 5885, 8911, 11298, 2, 1922, 9231, 12306], inplace=True) #16879, 7088,  ","819bb9ba":"# looking again at our percentile outliers\n\ndf.describe()","da3d542e":"#check what is going on with the sqft_lot15 outliers by sorting descending\ndf.sort_values('sqft_lot15', ascending=False).head(5)","ff0e3612":"# there is something off about these two large and nearly identical entries at the top of the list. We are going to drop these two rows.\n\ndf.drop(index=[4611, 9445], inplace=True)","3566dbe1":"#check what is going on with the weird bedroom value by sorting descending\n\ndf.sort_values('bedrooms', ascending=False).head(5)","ffa6cf12":"# this value of 33 in 1620 square feet is obviously a mistake. We're going to impute the mean into this field.\n\n# 11 bedrooms in 3000sf, 10 bedrooms in 2920sf, 10 bedrooms in 3610sf are also obviously mistakes\n# We're going to impute the mean into the fields for 10 and 11 bedrooms as well.\n\n# we'll also impute the mean into the few bedroom listings with 0 bedrooms\n\nbedroom_mean = round(df['bedrooms'].mean(), 0)\nbedroom_mean\ndf.loc[df['bedrooms'] == 33.0, 'bedrooms'] = bedroom_mean\ndf.loc[df['bedrooms'] == 11.0, 'bedrooms'] = bedroom_mean\ndf.loc[df['bedrooms'] == 10.0, 'bedrooms'] = bedroom_mean\ndf.loc[df['bedrooms'] == 0, 'bedrooms'] = bedroom_mean\ndf.loc[df['bathrooms'] == 0, 'bathrooms'] = 2\n\n# I remain suspicious of the 9-bedroom entries, but we'll decline from manipulating them further","8d0208e5":"# check how our histograms are looking\n\ndf.hist(figsize=(18,15), bins='auto');\n\n#Our histograms have improved a lot.","c58fb80f":"# look for missing data\ndf.isna().sum()\n\n# no missing values","cd9e8795":"df['yr_renovated'].value_counts(normalize=True).head(10)\n# 96% of houses have not been renovated, so we will turn this into a binary flag","82090088":"df['waterfront'].value_counts(normalize=True).head(10)\n# Hardly any homes on the waterfront, and it's already a binary flag","29b5484d":"df['view'].value_counts(normalize=True).head(10)\n# most houses don't have a view, but we have a few distinct choices, so we will KEEP this categorical ","07693c62":"df['sqft_basement'].value_counts(normalize=True).head(10)\n#Many properties don't have basements, but enough do to keep this as a continuous","74fad2ca":"#So, will change all values > 0 in those columns to 1\n# this will turn renovated into dichotomous choice flags\ndf.loc[df['yr_renovated'] > 0, 'yr_renovated'] = 1\n\n# now anything that is not a 1 becomes a 0, just in case we missed something weird\ndf.loc[df['yr_renovated'] != 1, 'yr_renovated'] = 0\n\n# since we're making it a binary flag, we'll rename yr_renovated to renovated\ndf.rename(columns={'yr_renovated' : 'renovated'}, inplace=True)","2d0c408d":"# We're trying out engineering a feature that penalizes or rewards being the smallest or biggest\n# property in the neighborhood, as generally you don't want to be either. This is attempting \n# to impose a categorical relationship on how the property relates to its neighbors\n\ndf['comparative_sf'] = 0\ndf.loc[df['sqft_living'] <= (df['sqft_living15']*.75), 'comparative_sf'] = '1'\ndf.loc[(df['sqft_living'] > (df['sqft_living15']*.75)) & (df['sqft_living'] < (df['sqft_living15']*1.25)), 'comparative_sf'] = '2'\ndf.loc[df['sqft_living'] >= (df['sqft_living15']*1.25), 'comparative_sf'] = '3'","f3db03d7":"# We're making all 0 basement values 1, and 0 view values .01, so that we can log transform this column ( cannot log 0)\ndf.loc[df['sqft_basement'] == 0, 'sqft_basement'] = 1\ndf.loc[df['view'] == 0, 'view'] = .01","9101af11":"# redo our zip code medians and rankings after data cleaning for visualizations\n# this is a repeat of the task we did further up\n\n# apply the median home price per zip code to the data frame again after outlier removal\ndf['median_zip'] = df['zipcode'].apply(lambda x: round(df.loc[df['zipcode']==x]['price'].median(), 0))\n\n# group our dataframe by zipcode on median home price, sorted ascending. We want to bin like-medians together.\nzipsorted = pd.DataFrame(df.groupby('zipcode')['price'].median().sort_values(ascending=True))\n\n# divide our dataframe into groups with entries per group as specified above,\n# and assign this number to a new column\nzipsorted['rank'] = np.divmod(np.arange(len(zipsorted)), 1)[0]+1\n\n# make a new column on our dataframe. Look up each zip entry's group, and append to the column.\ndf['zip_rank'] = df['zipcode'].apply(lambda x: make_group(x, zipsorted, 'rank'))","410065d6":"# re-visualize zip code as a color function, using the median zip after outlier removal. \n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['median_zip'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Zip Code by Median Rank', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Zip Code Median, by Zip Code Median Rank', fontsize=20);\n\n# save visualization to png\n#plt.savefig('images\/zip_prices.png')","bf863306":"# visualize zip code as a color function, on a plot of price per square footage\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['sqft_living'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Square Feet of Living Space', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Total Square Footage, by Zip Code Median Rank', fontsize=20)\n;","ba420578":"# plotting latitude and longitude as a visual scatter plot. The improved color map actually visually demonstrates\n# the removal of extreme price outliers.\n\nplt.figure(figsize=(25,25))\n\nsns.scatterplot(data=df, x=\"long\", y=\"lat\", hue=\"price\", palette=\"magma_r\");","407ae384":"# We can also use our scatter to see the zip codes by rank! Pretty cool.\n\nplt.figure(figsize=(25,25))\n\nsns.scatterplot(data=df, x=\"long\", y=\"lat\", hue=\"zip_rank\", palette=\"magma_r\");","54b79556":"#histogram and normal probability plot\nsns.distplot(df['price'], fit=norm);\nfig = plt.figure()\n\nres = stats.probplot(df['price'], plot=plt)\n\n# our sales price histogram is positively skewed and has a high peak\n# Our QQ-plot shows that we have heavy tails with right skew","53b1da99":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df['price'].skew())\nprint(\"Kurtosis: %f\" % df['price'].kurt())\n\n# price is highly right skewed\n# very positive kurtosis, indicating lots in the tails. We can see those tails in the right skew.","4791af17":"# log transform our target price to improve normality of distribution\ndf_target_log = np.log(df['price'])\n\n#histogram and normal probability plot\nsns.distplot(df_target_log, fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_target_log, plot=plt)\n\n# Our target price is more normally distributed when log transformed, so we'll be doing that\n","b392cc19":"# set our random seed for the notebook. We could randomize this each time the notebook is run,\n# but ultimately we want all of our train\/test splits to use the same data\nrandomstate = 4\n\n# creating our train\/validation sets and our test sets\ny = pd.DataFrame(df['price'])\ntrain_data, holdout, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=randomstate)\n\n# reset indices to prevent any index mismatches\ntrain_data.reset_index(inplace=True)\ntrain_data.drop('index', axis=1, inplace=True)\n\nholdout.reset_index(inplace=True)\nholdout.drop('index', axis=1, inplace=True)\n\ny_train.reset_index(inplace=True, drop=True)\ny_test.reset_index(inplace=True, drop=True)","b2f6a9a4":"# Adding target encoding, which we will opt to try instead of one-hot with a few models\n\n# this function is by Max Halford at the address noted above\ndef calc_smooth_mean(df, by, on, m, target_df):\n    '''input a pandas.DataFrame, a categorical column name, the name of the target column, and a weight .'''\n    # Compute the global mean\n    mean = df[on].mean() \n\n    # Compute the number of values and the mean of each group\n    agg = df.groupby(by)[on].agg(['count', 'mean'])  \n    counts = agg['count']\n    means = agg['mean']\n\n    # Compute the \"smoothed\" means\n    smooth = (counts * means + m * mean) \/ (counts + m)\n\n    # Replace each value by the according smoothed mean\n    return round(target_df[by].map(smooth), 0) \n\n# get size of training data\nnum_of_samples = train_data.shape[0]\n\n# determining minimum number of samples for zip and month to use their\n# own mean rather than expanding into the full data set mean \nzip_samples = num_of_samples\/train_data['zipcode'].unique().shape[0]\nmonth_samples = num_of_samples\/train_data['month_sold'].unique().shape[0]\n\n# create smooth additive encoded variables for zipcode, year built, and monthsold\ntrain_data['zip_smooth'] = calc_smooth_mean(train_data, 'zipcode', 'price', zip_samples, train_data)\ntrain_data['year_smooth'] = calc_smooth_mean(train_data, 'yr_built', 'price', 300, train_data)\ntrain_data['month_smooth'] = calc_smooth_mean(train_data, 'month_sold', 'price', month_samples, train_data)\n\n# Create a wider lat and long zone to calculate an area mean\ntrain_data['lat_zone'] = round(train_data['lat'], 2)\ntrain_data['long_zone'] = round(train_data['long'], 2)\n\n# determing min number samples for lat and long mean\nlat_samples = num_of_samples\/train_data['lat_zone'].unique().shape[0]\nlong_samples = num_of_samples\/train_data['long_zone'].unique().shape[0]\n\n# calculate smooth mean variables for lat and long, then create an interactive variable describing both together\ntrain_data['lat_smooth'] = calc_smooth_mean(train_data, 'lat_zone', 'price', lat_samples, train_data)\ntrain_data['long_smooth'] = calc_smooth_mean(train_data, 'long_zone', 'price', long_samples, train_data)\ntrain_data['lat_long'] = (np.sqrt(train_data['lat_smooth']) + np.sqrt(train_data['long_smooth']))","a78030d3":"# look for multicollinearity of features\nfig, ax = plt.subplots(figsize=(20, 20))\n\nsns.heatmap(train_data.corr(), center=0, annot=True, fmt=\".2f\", \n           vmin=-1, vmax=1, cbar_kws={\"shrink\": .8}, square=True)\n\n# title\nplt.title('PEARSON CORRELATION MATRIX', fontsize=18)\n\nplt.show()","5b9b40dc":"train_data.corr()","72628969":"#Get our list of highly correlated feature pairs with following steps:\n\n# save correlation matrix as a new data frame\n# converts all values to absolute value\n# stacks the row:column pairs into a multindex\n# reset the index to set the multindex to seperate columns\n# sort values. 0 is the column automatically generated by the stacking\ndf_correlations = train_data.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n\n# zip the variable name columns in a new column named \"pairs\"\ndf_correlations['pairs'] = list(zip(df_correlations.level_0, df_correlations.level_1))\n\n# set index to pairs\ndf_correlations.set_index(['pairs'], inplace = True)\n\n# rename our results column to correlation\ndf_correlations.rename(columns={0: \"correlation\"}, inplace=True)\n\n# Drop 1:1 correlations to get rid of self pairs\ndf_correlations.drop(df_correlations[df_correlations['correlation'] == 1.000000].index, inplace=True)\n\n# view pairs above 70% correlation and below 90% correlation (engineered features will correlate with each other above 95%)\ndf_correlations[(df_correlations.correlation>.75) & (df_correlations.correlation<.95)]\n","268ae315":"# drop multicollinear features and unneeded features\ntrain_data.drop(['id', 'zip_rank', 'median_zip', 'sqft_above', 'sqft_lot15'], axis=1, inplace=True)","425cc210":"# Check out our variables correlationg with price\ndf_correlations = train_data.corr().abs().stack().reset_index().sort_values(0, ascending=False)\ndf_correlations.loc[df_correlations['level_0'] == 'price'].sort_values(0, ascending=False)","b28cfdd2":"categoricals = ['floors', 'waterfront', 'renovated', 'comparative_sf', 'month_sold', 'yr_built', 'zipcode', 'lat_long'] #\n\n# make our categorical data frame to work with\ndf_categoricals = train_data[categoricals]\n\n# binning our year built bins\ndf_categoricals[\"year_block\"] = pd.qcut(df_categoricals['yr_built'], q=30, labels=np.array(range(1,30+1)))\n\n# binning our latitude bins\ndf_categoricals[\"loc_block\"] = pd.qcut(df_categoricals['lat_long'], q=50, labels=np.array(range(1,50+1)))\n\n# dropping the original categories we just binned\ndf_categoricals.drop(['yr_built', 'lat_long'], axis=1, inplace=True)\n\n# telling Pandas that these columns are categoricals\nfor item in df_categoricals.columns:\n    df_categoricals[item] = df_categoricals[item].astype('category')\n\n# temporarily adding price to our dataframe so that we can do some visualizations    \ndf_categoricals['price'] = train_data['price']\n\n# plot our categoricals as box plots vs price\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n\n    #visualization categories\ncategorical=['floors', 'waterfront', 'renovated', 'comparative_sf', 'month_sold', 'year_block', 'zipcode', 'loc_block']\nf = pd.melt(df_categoricals, id_vars=['price'], value_vars=categorical)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"price\")\n\ndf_categoricals.drop('price', axis=1, inplace=True)","bbb78c99":"# make a processed bins file for use with linear regression\n# We're making TWO categorical sets. One is high one hot encoding. One is low one hot encoding, and the \n# categoricals in that one will be target encoded as continuous instead\n\nhigh_one_hot_cat =  ['floors', 'waterfront', 'renovated','comparative_sf', 'zipcode', 'month_sold', 'year_block', 'loc_block']\nlow_one_hot_cat =  ['floors', 'waterfront', 'renovated','comparative_sf']\n\ndf_cats_high_one_hot = pd.get_dummies(df_categoricals[high_one_hot_cat], prefix=high_one_hot_cat, drop_first=True)\ndf_cats_low_one_hot = pd.get_dummies(df_categoricals[low_one_hot_cat], prefix=low_one_hot_cat, drop_first=True)","6e4ef416":"continuous = ['sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_basement', 'view', 'bedrooms', 'bathrooms', 'condition', 'grade', 'zip_smooth', 'year_smooth', 'month_smooth', 'lat_long'] #'floors',  \n\n# make our continuous frame to work with\nx_continuous = train_data[continuous]\nx_continuous['price'] = train_data['price']\n\n# plot our smaller choice continuous as box plots vs price\nsmall_cont = ['bedrooms', 'bathrooms', 'condition', 'grade', 'view', 'month_smooth'] \n\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n\nf = pd.melt(x_continuous, id_vars=['price'], value_vars=small_cont)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"price\")","7b01e53b":"# plot our larger continuous as scatter plots vs price\nlarge_cont = ['sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_basement', 'zip_smooth', 'year_smooth', 'month_smooth', 'lat_long']\n\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15,25), sharey=True)\n\nfor ax, column in zip(axes.flatten(), large_cont):\n    ax.scatter(x_continuous[column], x_continuous['price']\/100000, label=column, alpha=.1)\n    ax.set_title(f'Sale Price vs {column}')\n    ax.set_xlabel(column)\n    ax.set_ylabel('Sale Price in $100,000')\n\nfig.tight_layout()\n\nx_continuous.drop('price', axis=1, inplace=True)","8e8ba560":"# Checking out our mean sales price for year built scattered versus price shows a polynomial relationship\n\nyearly_prices = train_data.groupby('yr_built')['price'].mean()\n\nplt.scatter(yearly_prices.index, yearly_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('year built')\nplt.ylabel('sales price')\nplt.show()","362ad4b9":"# Checking out our mean sales price for month sold scattered versus price shows a polynomial relationship\nmonthly_prices = train_data.groupby('month_sold')['price'].mean()\n\nplt.scatter(monthly_prices.index, monthly_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('month sold')\nplt.ylabel('sales price')\nplt.show()","5faa1345":"# Checking out our mean sales price for longitude does not show a polynomial relationship\nlong_prices = train_data.groupby('lat_long')['price'].mean()\n\nplt.scatter(long_prices.index, long_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('long')\nplt.ylabel('sales price')\nplt.show()","a2673f64":"# Checking out our mean sales price for grade shows a polynomial relationship\ngrade_prices = train_data.groupby('grade')['price'].mean()\n\nplt.scatter(grade_prices.index, grade_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('long')\nplt.ylabel('sales price')\nplt.show()","e2965bea":"# Checking out our mean sales price for grade shows a polynomial relationship\ngrade_prices = train_data.groupby('sqft_living')['price'].mean()\n\nplt.scatter(grade_prices.index, grade_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('square footage')\nplt.ylabel('sales price')\nplt.show()","cf4dfc8b":"def test_feature_combinations(price, variables):\n    \n    \"\"\"Function takes in target price and a dataframe of independent variables, and \n    tests model improvement for each combination of variables\n    ARGUMENTS:\n    Y of target values\n    X-dataframe of continuous features\n    Returns dataframe of score improvements over base score for each interaction combination\"\"\"\n    \n    # select our estimator and our cross validation plan\n    regression = LinearRegression()\n    cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n    \n    # prepare our scoring dataframe\n    scoring_df = pd.DataFrame()\n    \n    # prepare our lists to store our features and scores as we iterate\n    scores = []\n    feature1 = []\n    feature2 = []\n    \n    # Get a list of all of our features, and remove our target variable 'price' from the list\n    features = list(variables.columns)\n\n    # make a list of all of our possible feature combinations\n    feature_combos = itertools.combinations(features, 2)\n    feature_combos = list(feature_combos)\n    \n    # set our y-value as our target variable\n    y = price\n    \n    # prepare our x-value with our independent variables. We do an initial split here in order to run a \n    # linear regression to get a base r^2 on our basic model without interactions\n    X = variables\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=randomstate)\n    base_score = round(np.mean(cross_val_score(regression, X_train, y_train, scoring='r2', cv=cv)), 4)   \n    print(\"Model base score is \",base_score)\n    \n    # now we run the regression on each feature combo\n    for feature in feature_combos:\n        feat1, feat2 = feature[0], feature[1]\n        \n        # create the test interaction on our data set\n        variables['test_interaction'] = variables[feat1] * variables[feat2]\n        # create a new X which includes the test interaction and drops our target value\n        X = variables\n        # make a new split so that our x-splits include the test interaction\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=randomstate)\n        \n        # Run a linear regression with cross-val just like our base model, and append the score to our scores list\n        new_score = round(np.mean(cross_val_score(regression, X_train, y_train, scoring='r2', cv=cv)), 4)\n        scores.append(new_score)\n        # put feature 1 on a list\n        feature1.append(feat1)\n        # put feature 2 on a list\n        feature2.append(feat2)\n        print(feat1, feat2, new_score)\n        \n        \n    \n    # load all of our lists into the scoring dataframe\n    scoring_df['feature1'] = feature1\n    scoring_df['feature2'] = feature2\n    scoring_df['scores'] = scores\n    scoring_df['improvement'] = scoring_df['scores'] - base_score\n    variables.drop('test_interaction', axis=1, inplace=True)\n    \n    # return our scoring dataframe to the function\n    return scoring_df","3f5717e3":"# running our function on our continuous variables to look for improvement\n# our R2 is much lower for model base score because we aren't including our categorical variables in this improvement assessment\n\nscoring_df = test_feature_combinations(y_train, x_continuous)","c33a6711":"# showing our improvement scores for our interactions\n\nscoring_df.sort_values('improvement', ascending=False)","9699efc7":"x_continuous['sqft-zip'] = np.sqrt(x_continuous['sqft_living'] * x_continuous['zip_smooth'])","ff99b8a6":"# check out our histograms to see if we should transform our data before scaling\n\nx_continuous.hist(figsize=(18,15), bins='auto');","8b292c36":"# log transform\nlog_continuous = np.log(x_continuous)\n\n# standardize all of our values with scikit-learn StandardScaler\nscaler = StandardScaler()\n\nscaled_continuous = pd.DataFrame(scaler.fit_transform(log_continuous),columns = log_continuous.columns)\nscaled_continuous.head(5)\n\n# Our scaled and transformed continuous features\n\nscaled_continuous.hist(figsize=(18,15), bins='auto');","1b2e7687":"# make a processed bins file for use with linear regression\n\n# our categories for high one-hot encoding and low one-hot encoding\nhigh_one_hot_cont =  ['sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_basement', 'view', 'bedrooms', 'bathrooms', 'grade', 'condition', 'sqft-zip']\nlow_one_hot_cont =  ['sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_basement', 'view', 'bedrooms', 'bathrooms', 'grade', 'condition', 'zip_smooth', 'year_smooth', 'month_smooth', 'lat_long', 'sqft-zip']  \n\ndf_cont_high_one_hot = scaled_continuous[high_one_hot_cont]\ndf_cont_low_one_hot = scaled_continuous[low_one_hot_cont]","06e3b080":"def plot_polys(y, xlabel, title):\n    '''Takes in a y-axis, x-axis label, and title and plots with various polynomial levels\n    ARGUMENTS:\n    y axis variable values\n    x-axis label\n    visualization title'''\n    x = y.index\n    \n    # express numbers as arrays and reshape\n    y = np.array(y)\n    x = np.array(x)\n    x = x.reshape(-1, 1)\n    \n    # make sure indices match up\n    y = y[x[:,0].argsort()]\n    x = x[x[:, 0].argsort()]\n\n    # plot figure\n    plt.figure(figsize=(16, 8))\n\n    # standard linear regression\n    linreg = LinearRegression()\n    linreg.fit(x, y)\n\n    # 2nd degree polynomial regression\n    poly2 = PolynomialFeatures(degree=2)\n    x_poly2 = poly2.fit_transform(x)\n    poly_reg2 = LinearRegression()\n    poly_reg2.fit(x_poly2, y)\n\n    # third degree polynomial regression \n    poly3 = PolynomialFeatures(degree=3)\n    x_poly3 = poly3.fit_transform(x)\n    poly_reg3 = LinearRegression()\n    poly_reg3.fit(x_poly3, y)\n\n    # predict on x values\n    pred = linreg.predict(x)\n    pred2 = poly_reg2.predict(x_poly2)\n    pred3 = poly_reg3.predict(x_poly3)\n\n    # plot regression lines\n    plt.scatter(x, y)\n    plt.yscale('log')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel('Average')\n    plt.plot(x, pred, c='red', label='Linear regression line')\n    plt.plot(x, pred2, c='yellow', label='Polynomial regression line 2')\n    plt.plot(x, pred3, c='#a3cfa3', label='Polynomial regression line 3');","145dc9cb":"# group by average month sold mean to see relationship\ny = train_data.groupby('month_sold')['price'].mean()\nplot_polys(y, \"Month\", \"Month Sold Mean\")","63317a9c":"# group by average year built mean to see relationship\ny = train_data.groupby('yr_built')['price'].mean()\nplot_polys(y, \"Year Sold\", \"Year Sold Mean\")","71510c98":"# group by grade mean to see relationship\ny = train_data.groupby('grade')['price'].mean()\nplot_polys(y,'Grade', \"Grade Mean\")","ebdd2b46":"# group by grade mean to see relationship\ny = train_data.groupby('sqft_living')['price'].mean()\nplot_polys(y,'Square Footage', \"Square Footage Mean\")","b5fd697e":"# group by grade mean to see relationship\ny = train_data.groupby('sqft_lot')['price'].mean()\nplot_polys(y,'Lot Square Footage', \"Lot Square Footage Mean\")","a20db85c":"# adding our chosen polynomial features\n\ndef create_polynomial_array(data, column, num_features):\n    values = data[column]\n    poly_array = np.array(values)\n    poly_array = poly_array.reshape(-1,1)\n    poly_fit = PolynomialFeatures(degree=num_features, include_bias=False)\n    fit_features = poly_fit.fit_transform(poly_array)\n    poly_df = pd.DataFrame(fit_features)\n    return poly_df\n\nmonth_poly = create_polynomial_array(df_cont_low_one_hot, 'month_smooth',2)\nyear_poly = create_polynomial_array(df_cont_low_one_hot, 'year_smooth', 2)\ngrade_poly = create_polynomial_array(df_cont_low_one_hot, 'grade', 2)\nsq_ft_poly = create_polynomial_array(df_cont_low_one_hot, 'sqft_living', 3)\n\ndf_cont_low_one_hot['month1'] = month_poly[1]\ndf_cont_low_one_hot['grade1'] = grade_poly[1]\ndf_cont_high_one_hot['grade1'] = grade_poly[1]\ndf_cont_low_one_hot['year1'] = year_poly[1]\ndf_cont_low_one_hot['sqft_living2'] = sq_ft_poly[1]\ndf_cont_high_one_hot['sqft_living2'] = sq_ft_poly[1]\ndf_cont_low_one_hot['sqft_living3'] = sq_ft_poly[2]\ndf_cont_high_one_hot['sqft_living3'] = sq_ft_poly[2]","52398692":"# apply target encoding to test data, using train data to map\n\n# create smooth additive encoded variables for zipcode, year built, and monthsold\nholdout['zip_smooth'] = calc_smooth_mean(train_data, 'zipcode', 'price', zip_samples, holdout)\nholdout['year_smooth'] = calc_smooth_mean(train_data, 'yr_built', 'price', 300, holdout)\nholdout['month_smooth'] = calc_smooth_mean(train_data, 'month_sold', 'price', month_samples, holdout)\n\n# Create a wider lat and long zone to calculate an area mean\nholdout['lat_zone'] = round(holdout['lat'], 2)\nholdout['long_zone'] = round(holdout['long'], 2)\n\n# calculate smooth mean variables for lat and long, then create an interactive variable describing both together\nholdout['lat_smooth'] = calc_smooth_mean(train_data, 'lat_zone', 'price', lat_samples, holdout)\nholdout['long_smooth'] = calc_smooth_mean(train_data, 'long_zone', 'price', long_samples, holdout)\nholdout['lat_long'] = (np.sqrt(holdout['lat_smooth']) + np.sqrt(holdout['long_smooth']))","fd197054":"holdout_categoricals = holdout[categoricals]\n\n# binning our year built bins\nholdout_categoricals[\"year_block\"] = pd.qcut(holdout_categoricals['yr_built'], q=30, labels=np.array(range(1,30+1)))\n\n# binning our latitude bins\nholdout_categoricals[\"loc_block\"] = pd.qcut(holdout_categoricals['lat_long'], q=50, labels=np.array(range(1,50+1)))\n\nholdout_categoricals.drop(['yr_built'], axis=1, inplace=True)\n\n# telling Pandas that these columns are categoricals\nfor item in holdout_categoricals.columns:\n    holdout_categoricals[item] = holdout_categoricals[item].astype('category')\n\n# make a processed bins file for use with linear regression\ndf_cats_high_one_hot_holdout = pd.get_dummies(holdout_categoricals[high_one_hot_cat], prefix=high_one_hot_cat, drop_first=True)\ndf_cats_low_one_hot_holdout = pd.get_dummies(holdout_categoricals[low_one_hot_cat], prefix=low_one_hot_cat, drop_first=True)","251495c4":"holdout_continuous = holdout[continuous]\n\n# add feature interaction\nholdout_continuous['sqft-zip'] = np.sqrt(holdout_continuous['sqft_living'] * holdout_continuous['zip_smooth'])\n\n# log transform\nlog_holdout = np.log(holdout_continuous)\n\n# standard scaler\nscaled_holdout_continuous = pd.DataFrame(scaler.transform(log_holdout),columns = log_holdout.columns)\n\n# making our two continuous sets\ndf_cont_high_one_hot_holdout = scaled_holdout_continuous[high_one_hot_cont]\ndf_cont_low_one_hot_holdout = scaled_holdout_continuous[low_one_hot_cont]\n\n# adding polynomial features\n\nmonth_poly = create_polynomial_array(df_cont_low_one_hot_holdout, 'month_smooth',2)\nyear_poly = create_polynomial_array(df_cont_low_one_hot_holdout, 'year_smooth', 2)\ngrade_poly = create_polynomial_array(df_cont_low_one_hot_holdout, 'grade', 2)\nsq_ft_poly = create_polynomial_array(df_cont_low_one_hot_holdout, 'sqft_living', 3)\n\n\ndf_cont_low_one_hot_holdout['month1'] = month_poly[1]\ndf_cont_low_one_hot_holdout['grade1'] = grade_poly[1]\ndf_cont_high_one_hot_holdout['grade1'] = grade_poly[1]\ndf_cont_low_one_hot_holdout['year1'] = year_poly[1]\ndf_cont_low_one_hot_holdout['sqft_living3'] = sq_ft_poly[2]\ndf_cont_high_one_hot_holdout['sqft_living3'] = sq_ft_poly[2]\ndf_cont_low_one_hot_holdout['sqft_living2'] = sq_ft_poly[1]\ndf_cont_high_one_hot_holdout['sqft_living2'] = sq_ft_poly[1]","4255cdc6":"# make our train sets for one-hot encoded and target-encoded categoricals\nX_train_onehot = pd.concat([df_cont_high_one_hot, df_cats_high_one_hot], axis=1)\nX_train_encoded = pd.concat([df_cont_low_one_hot, df_cats_low_one_hot], axis=1)\n\n# make our test sets for one-hot encoded and target-encoded categoricals\nX_test_onehot = pd.concat([df_cont_high_one_hot_holdout, df_cats_high_one_hot_holdout], axis=1)\nX_test_encoded = pd.concat([df_cont_low_one_hot_holdout, df_cats_low_one_hot_holdout], axis=1)\n\n# make our target variable train and test sets, after log transforming our target variable\ntarget = 'price' # target variable\ny = np.log(df[target]) # our log-transformed target variable\n\ny_train, y_test = train_test_split(y, test_size=0.2, random_state=randomstate) #\n\ny_train.reset_index(inplace=True, drop=True)\ny_test.reset_index(inplace=True, drop=True)\ntest_actual = np.exp(y_test)","dd6844b7":"# prepare dictionary to store results\nmodels = {}\nmodels['Models'] = []\nmodels['r2'] = []\nmodels['mae'] = []\nmodels['rmse'] = []","6539c97d":"x, y = np.array(df['median_zip']).reshape(-1,1), df['price']\nz = np.array(df['sqft_living']).reshape(-1,1)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ntest_predictions = model.predict(x)\n\nresiduals = y - test_predictions\n\nfig = plt.figure(figsize=(15,10))\n\n# Add labels for x and y axes\nplt.xlabel('Total Square Footage')\nplt.ylabel('Residuals')\n\n# Add a title for the plot\nplt.title('Residuals vs Square Footage - Response is Median_Zip')\n\n\nplt.scatter(z, residuals, label=\"sample\");","68141bac":"x, y = np.array(df['median_zip']).reshape(-1,1), df['price']\nz = np.array(df['grade']).reshape(-1,1)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ntest_predictions = model.predict(x)\n\nresiduals = y - test_predictions\n\nfig = plt.figure(figsize=(15,10))\n\n# Add labels for x and y axes\nplt.xlabel('Grade')\nplt.ylabel('Residuals')\n\n# Add a title for the plot\nplt.title('Residuals vs Grade - Response is Median_Zip')\n\n\nplt.scatter(z, residuals, label=\"sample\");","fc443dd1":"x, y = np.array(df['median_zip']).reshape(-1,1), df['price']\nz = np.array(df['yr_built']).reshape(-1,1)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ntest_predictions = model.predict(x)\n\nresiduals = y - test_predictions\n\nfig = plt.figure(figsize=(15,10))\n\n# Add labels for x and y axes\nplt.xlabel('Year Built')\nplt.ylabel('Residuals')\n\n# Add a title for the plot\nplt.title('Residuals vs Year Built - Response is Median_Zip')\n\nplt.scatter(z, residuals, label=\"sample\");","009b98a9":"# put together our basic feature set and preprocess\n\n# one-hot encode categorical\nbase_cat = pd.DataFrame()\nbase_cat['zipcode'] = df['zipcode']\nbase_cat['zipcode'] = base_cat['zipcode'].astype('category')\nbase_cat_processed = pd.get_dummies(base_cat['zipcode'], prefix='zipcode', drop_first=True)\nbase_cat_processed.reset_index(inplace=True)\nbase_cat_processed.drop('index', axis=1, inplace=True)\n\n# log transform and standard scale our continuous\nbase_cont = df[['sqft_living', 'grade', 'bathrooms']]\n\nscaler = StandardScaler()\nbase_cont_processed = pd.DataFrame(scaler.fit_transform(base_cont),columns = base_cont.columns)\n\n# our log-transformed target variable\ny = np.log(df['price'])\n\n#join cat and cont into predictor data frame\n#x_base_set = pd.concat([base_cont_processed, base_cat_processed], axis=1)\nx_base_set = base_cont_processed.join([base_cat_processed], how='inner') \n\n# train\/test split\nx_base_train, x_base_test, y_base_train, y_base_test = train_test_split(x_base_set, y, test_size=0.2, random_state=randomstate)","96ea832f":"# run model for R^2 score\n\nmodel = LinearRegression()\nmodel.fit(x_base_train, y_base_train)\ncv_5 = cross_val_score(model, x_base_train, y_base_train, cv=5)\nr2 = cv_5.mean()\nr2","a28aa201":"# apply our model to our test set and get predicted values\ntest_predictions = model.predict(x_base_test)\ntest_predictions\n\n# reverse log transform our predicted values\ntest_predictions_unscaled = np.exp(test_predictions).astype(int)\n\n# get residuals\nresiduals = test_actual - test_predictions_unscaled\n\nfig = plt.figure(figsize=(20,15))\nplt.scatter(test_predictions_unscaled, residuals);\n# Residuals plot","b53cf939":"# Calculate our mean absolute error\nmae = round(mean_absolute_error(test_actual, test_predictions_unscaled), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, test_predictions_unscaled)), 2)\n\n# append our results to our lists\nmodels['Models'].append('Bare Bones Features LR - One-Hot Zip')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)","6ade5a3d":"# put together our basic feature set and preprocess\n\nx_base_train = X_train_encoded[['sqft_living', 'grade', 'bathrooms', 'zip_smooth']]\nx_base_test = X_test_encoded[['sqft_living', 'grade', 'bathrooms', 'zip_smooth']]","12e30830":"# run model for R^2 score\n\nmodel = LinearRegression()\nmodel.fit(x_base_train, y_base_train)\ncv_5 = cross_val_score(model, x_base_train, y_base_train, cv=5)\nr2 = cv_5.mean()\nr2","79391e10":"# apply our model to our test set and get predicted values\ntest_predictions = model.predict(x_base_test)\ntest_predictions\n\n# reverse log transform our predicted values\ntest_predictions_unscaled = np.exp(test_predictions).astype(int)\n\n# get residuals\nresiduals = test_actual - test_predictions_unscaled\n\nfig = plt.figure(figsize=(20,15))\nplt.scatter(test_predictions_unscaled, residuals);\n# Residuals plot","319223ef":"# Calculate our mean absolute error\nmae = round(mean_absolute_error(test_actual, test_predictions_unscaled), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, test_predictions_unscaled)), 2)\n\n# append our results to our lists\nmodels['Models'].append('Bare Bones Features LR - Encoded Zip')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)","12adbd5d":"predictors_train = sm.add_constant(X_train_onehot)\nmodel = sm.OLS(y_train, predictors_train).fit()\nmodel.summary()","4975294f":"model = LinearRegression()\nmodel.fit(X_train_onehot, y_train)\ncv_5 = cross_val_score(model, X_train_onehot, y_train, cv=5)\nr2 = cv_5.mean()\nr2","82ac7195":"# apply our model to our test set and get predicted values\ntest_predictions = model.predict(X_test_onehot)\ntest_predictions\n\n# reverse log transform our predicted values\ntest_predictions_unscaled = np.exp(test_predictions)\ntest_predictions_unscaled = test_predictions_unscaled.flatten().astype(int)\n\n# get residuals\nresiduals = test_actual - test_predictions_unscaled\n\nfig = plt.figure(figsize=(20,15))\nplt.scatter(test_predictions_unscaled, residuals);\n\n# Residuals plot\n","af1b12e2":"# Calculate our mean absolute error\nmae = round(mean_absolute_error(test_actual, test_predictions_unscaled), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, test_predictions_unscaled)), 2)\n\n# append our results to our lists\nmodels['Models'].append('Basic LR - One-Hot')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)","f4d7bce3":"# We need our statsmodels model again to plot residuals\npredictors_train = sm.add_constant(X_train_onehot)\nmodel = sm.OLS(y_train, predictors_train).fit()","435c6f17":"fig = plt.figure(figsize=(15,8))\nfig = sm.graphics.plot_regress_exog(model, \"sqft_living\", fig=fig)\nplt.show()","d38c64b7":"fig = plt.figure(figsize=(15,8))\nfig = sm.graphics.plot_regress_exog(model, \"sqft_lot\", fig=fig)\nplt.show()","810d4307":"fig = plt.figure(figsize=(15,8))\nfig = sm.graphics.plot_regress_exog(model, \"bedrooms\", fig=fig)\nplt.show()","29c7eb33":"fig = plt.figure(figsize=(15,8))\nfig = sm.graphics.plot_regress_exog(model, \"bathrooms\", fig=fig)\nplt.show()","4a0046cd":"fig = plt.figure(figsize=(15,8))\nfig = sm.graphics.plot_regress_exog(model, \"grade\", fig=fig)\nplt.show()","dd2e561f":"predictors_train = sm.add_constant(X_train_encoded)\nmodel = sm.OLS(y_train, predictors_train).fit()\nmodel.summary()","262bf6b3":"model = LinearRegression()\nmodel.fit(X_train_encoded, y_train)\ncv_5 = cross_val_score(model, X_train_encoded, y_train, cv=5)\nr2 = cv_5.mean()\nr2","d198cf84":"# apply our model to our test set and get predicted values\ntest_predictions = model.predict(X_test_encoded)\ntest_predictions\n\n# reverse log transform our predicted values\ntest_predictions_unscaled = np.exp(test_predictions)\ntest_predictions_unscaled = test_predictions_unscaled.flatten().astype(int)\n\n# get residuals\nresiduals = test_actual - test_predictions_unscaled\n\nfig = plt.figure(figsize=(20,15))\nplt.scatter(test_predictions_unscaled, residuals);\n\n# Residuals plot\n","81047dd3":"# Calculate our mean absolute error\nmae = round(mean_absolute_error(test_actual, test_predictions_unscaled), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, test_predictions_unscaled)), 2)\n\n# append our results to our lists\nmodels['Models'].append('Basic LR - Encoded')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)","c14bc0fa":"'''def stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \"\"\" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https:\/\/en.wikipedia.org\/wiki\/Stepwise_regression for the details\n    \"\"\"\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = included[pvalues.argmax()]\n            included.remove(worst_feature)\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n\n\nresult = stepwise_selection(X_train_onehot, y_train, verbose=True)\n\nprint('resulting features:', result)\n'''\n\nresult = ['sqft_living15', 'grade', 'sqft-zip', 'sqft_living2', 'view', 'zipcode_98117', 'zipcode_98103', 'zipcode_98115', 'zipcode_98107', 'zipcode_98075', 'condition', 'zipcode_98002', 'zipcode_98032', 'zipcode_98092', 'zipcode_98023', 'zipcode_98022', 'zipcode_98039', 'zipcode_98038', 'zipcode_98109', 'zipcode_98119', 'zipcode_98122', 'zipcode_98102', 'zipcode_98116', 'zipcode_98105', 'zipcode_98136', 'renovated_1', 'zipcode_98125', 'zipcode_98126', 'zipcode_98199', 'zipcode_98034', 'zipcode_98144', 'zipcode_98008', 'zipcode_98033', 'zipcode_98053', 'zipcode_98133', 'zipcode_98118', 'zipcode_98112', 'sqft_basement', 'zipcode_98005', 'zipcode_98004', 'zipcode_98007', 'zipcode_98052', 'zipcode_98177', 'zipcode_98029', 'zipcode_98027', 'zipcode_98155', 'floors_3.0', 'zipcode_98040', 'zipcode_98006', 'zipcode_98074', 'zipcode_98072', 'zipcode_98011', 'zipcode_98028', 'sqft_living', 'year_block_30', 'waterfront_1', 'zipcode_98065', 'zipcode_98077', 'zipcode_98106', 'zipcode_98108', 'zipcode_98056', 'zipcode_98059', 'zipcode_98045', 'zipcode_98166', 'zipcode_98019', 'zipcode_98146', 'zipcode_98014', 'zipcode_98024', 'month_sold_6', 'month_sold_5', 'year_block_29', 'sqft_lot', 'bathrooms', 'month_sold_4', 'year_block_3', 'year_block_5', 'zipcode_98058', 'zipcode_98178', 'zipcode_98055', 'zipcode_98070', 'year_block_4', 'floors_2.0', 'loc_block_46', 'month_sold_7', 'month_sold_3', 'year_block_18', 'year_block_13', 'loc_block_47', 'month_sold_8', 'loc_block_37', 'loc_block_33', 'zipcode_98148', 'floors_2.5', 'year_block_11', 'loc_block_31', 'loc_block_36', 'loc_block_34', 'loc_block_2', 'loc_block_40', 'loc_block_45', 'loc_block_50', 'loc_block_49', 'loc_block_41', 'loc_block_48', 'loc_block_38', 'loc_block_44', 'loc_block_39', 'loc_block_42', 'loc_block_35', 'loc_block_43', 'loc_block_29', 'loc_block_28', 'loc_block_30', 'loc_block_27', 'loc_block_32', 'loc_block_25', 'year_block_15', 'loc_block_26', 'year_block_12', 'year_block_14', 'year_block_16', 'year_block_17', 'year_block_10', 'loc_block_10', 'year_block_9', 'zipcode_98198', 'year_block_27', 'loc_block_3', 'month_sold_9', 'grade1', 'zipcode_98168']","155469e2":"# Run our linear regression again, using only the features recommended by our feature selector\n\nX_train_refined = X_train_onehot[result]\nX_test_refined = X_test_onehot[result]\n\npredictors_int = sm.add_constant(X_train_refined)\nmodel = sm.OLS(y_train, predictors_int).fit()\nmodel.summary()","e721a0f3":"print(\"{} predictors used\".format(len(result)))","f9cf80b9":"model = LinearRegression()\nmodel.fit(X_train_refined, y_train)\ncv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n\ncv_5 = cross_val_score(model, X_train_refined, y_train, cv=cv)\nr2 = cv_5.mean()\nr2","6ec8572f":"# apply our model to our test set and get predicted values\ntest_predictions_refined = model.predict(X_test_refined)\n\n# reverse log transform our predicted values\ntest_predictions_refined_unscaled = np.exp(test_predictions_refined)\ntest_predictions_refined_unscaled=test_predictions_refined_unscaled.flatten()\n\n# get residuals\nresiduals = test_actual - test_predictions_refined_unscaled\n\n# plot residuals\nfig = plt.figure(figsize=(20,15))\nplt.scatter(test_predictions_refined_unscaled, residuals)","47ce1cb2":"# get mean absolute error\nmae = round(mean_absolute_error(test_actual, test_predictions_refined_unscaled), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, test_predictions_refined_unscaled)), 2)\n\n# append our results to our lists\nmodels['Models'].append('Forw-Back Selector')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)","d499e2d9":"'''model = LinearRegression()\nmodel.fit(X_train_onehot, y_train)\n\n\nr = permutation_importance(model, X_train_onehot, y_train,\n                           n_repeats=15,\n                            random_state=0,\n                          n_jobs=-1)\n\nimportances = {}\n\nfor i in r.importances_mean.argsort()[::-1]:\n    if r.importances_mean[i] >= 0.001:\n        importances[X_train_onehot.columns[i]] = r.importances_mean[i]\n    else: continue\n        \nimportances\n\nimportant_features = list(importances.keys())\nimportant_features  '''","f7d98a46":"important_features = ['sqft_living','zipcode_98103','zipcode_98115','zipcode_98117','zipcode_98004',\n                      'zipcode_98112','zipcode_98033','zipcode_98199','zipcode_98105','grade','zipcode_98052',\n                      'zipcode_98122','zipcode_98107','zipcode_98034','zipcode_98125','zipcode_98040','zipcode_98116',\n                      'zipcode_98133','zipcode_98006','zipcode_98119','zipcode_98136','sqft-zip','zipcode_98144',\n                      'zipcode_98155','zipcode_98126','zipcode_98118','zipcode_98109','zipcode_98027','zipcode_98008',\n                      'zipcode_98177','zipcode_98102','zipcode_98029','zipcode_98053','zipcode_98074','zipcode_98005',\n                      'zipcode_98056','zipcode_98059','sqft_living2','zipcode_98028','zipcode_98075','zipcode_98039',\n                      'zipcode_98072','zipcode_98007','zipcode_98065','sqft_living15','zipcode_98011','loc_block_46',\n                      'loc_block_47','loc_block_50','zipcode_98106','loc_block_49','condition','loc_block_45',\n                      'loc_block_48','loc_block_41','loc_block_36','sqft_basement','zipcode_98146','zipcode_98166',\n                      'loc_block_38','loc_block_44','loc_block_33','loc_block_37','loc_block_40','loc_block_42',\n                      'loc_block_39','view','loc_block_31','loc_block_43','loc_block_34','zipcode_98045','zipcode_98108',\n                      'loc_block_35','zipcode_98019','zipcode_98058','loc_block_29','zipcode_98038','floors_2.0',\n                      'floors_3.0','zipcode_98077','loc_block_30','loc_block_32','loc_block_27','loc_block_28',\n                      'zipcode_98178','bathrooms','month_sold_6','month_sold_5','zipcode_98055','sqft_lot','month_sold_4',\n                      'loc_block_25','year_block_18','year_block_13','loc_block_26','month_sold_7','loc_block_22',\n                      'loc_block_24','month_sold_3','loc_block_10','renovated_1','year_block_11','loc_block_18',\n                      'year_block_30','loc_block_23','year_block_14','waterfront_1','year_block_12','loc_block_21',\n                      'year_block_16','loc_block_16','zipcode_98168','loc_block_20']","49590c11":"# set the features recommended by our feature selector\n\nX_train_perm = X_train_onehot[important_features]\nX_test_perm = X_test_onehot[important_features]\n\npredictors_int = sm.add_constant(X_train_perm)\nmodel = sm.OLS(y_train, predictors_int).fit()\nmodel.summary()","db4c9a6a":"print(\"{} predictors used\".format(len(important_features)))","7844a8be":"model = LinearRegression()\nmodel.fit(X_train_perm, y_train)\ncv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n\ncv_5 = cross_val_score(model, X_train_perm, y_train, cv=cv)\nr2 = cv_5.mean()\nr2","be182f3c":"# apply our model to our test set and get predicted values\ntest_predictions_refined = model.predict(X_test_perm)\n\n# reverse log transform our predicted values\ntest_predictions_refined_unscaled = np.exp(test_predictions_refined)\ntest_predictions_refined_unscaled=test_predictions_refined_unscaled.flatten()\n\n# get residuals\nresiduals = test_actual - test_predictions_refined_unscaled\n\n# plot residuals\nfig = plt.figure(figsize=(20,15))\nplt.scatter(test_predictions_refined_unscaled, residuals)","907d0864":"# get mean absolute error\nmae = round(mean_absolute_error(test_actual, test_predictions_refined_unscaled), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, test_predictions_refined_unscaled)), 2)\n\n# append our results to our lists\nmodels['Models'].append('Permutation Importance Selector')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)","95a27b3a":"# Using sklearn RFECV to perform integrated CV while picking the number of features\n# picks the number of features itself\n\nmodel = LinearRegression()\n#cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n\nrfecv = RFECV(estimator=model, step=1, cv=5, scoring='neg_mean_absolute_error')\n\n# fit model to train set\nrfecv.fit(X_train_onehot, y_train)\n\n# print optimal number of features\nprint('Optimal number of features: {}'.format(rfecv.n_features_))","e64b9cbb":"dset = pd.DataFrame()\ndset['attr'] = X_train_onehot.columns\ndset['used'] = rfecv.support_\n\n# make a list of the features used in the rfecv\nrfecv_result = list(dset[(dset['used'] == True)]['attr'])\n\n# Show the features that RFECV did not use\ndset[dset['used']==False]","48ca2b52":"# Run our linear regression again in statsmodels, using the features recommended by our feature selector\n\nX_train_rfecv = X_train_onehot[rfecv_result]\nX_test_rfecv = X_test_onehot[rfecv_result]\n\npredictors_int = sm.add_constant(X_train_rfecv)\nmodel = sm.OLS(y_train, predictors_int).fit()\nmodel.summary()","88beccc6":"# getting the r2 score of our best feature set\nr2 = model.rsquared\nr2","859b477d":"plt.figure(figsize=(16, 9))\nplt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\nplt.xlabel('Number of features selected', fontsize=14, labelpad=20)\nplt.ylabel('R2', fontsize=14, labelpad=20)\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n\nplt.show()","ca159239":"# predict on new data\nrfecv_predictions = rfecv.predict(X_test_onehot)\n\nrfecv_predictions_unscaled = np.exp(rfecv_predictions)\nrfecv_predictions_unscaled = rfecv_predictions_unscaled.flatten().astype(int)\n\n# get residuals\nresiduals = test_actual - rfecv_predictions_unscaled\n\n#plot residuals \nfig = plt.figure(figsize=(20,15))\nplt.scatter(rfecv_predictions_unscaled, residuals);","b3f33e15":"# get mean absolute error\nmae = round(mean_absolute_error(test_actual, rfecv_predictions_unscaled), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, rfecv_predictions_unscaled)), 2)\n\nmodels['Models'].append('RFECV')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)","7b204bc1":"mae_val = [] #to store mae values for different k\n\n# checks mean absolute error scores on k from 1 to 25\nfor K in range(4, 20):\n    K = K+1\n    \n    # set up the KNN regressor\n    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n\n    model.fit(X_train_encoded, y_train)  #fit the model\n    pred=model.predict(X_test_encoded) #make prediction on test set\n    error = mean_absolute_error(y_test,pred) #calculate mae\n    mae_val.append(error) #store mae values\n    print('MAE value for k= ' , K , 'is:', error)\n    \n# gets optimal k-value based on score minimum\nindex_min = np.argmin(mae_val) + 1\n\n# makes model and fits using optimal k\nmodel = neighbors.KNeighborsRegressor(n_neighbors = index_min)\nmodel.fit(X_train_encoded, y_train)  #fit the model\n\n# Get R^2 with cv\nscores = cross_val_score(model, X_train_encoded, y_train, scoring='r2', cv=5, n_jobs=-1, error_score='raise')\nr2 = np.mean(scores)\nr2","a04bfa49":"#make prediction on test set\npred_knn = model.predict(X_test_encoded)\npred_knn = np.exp(pred_knn).flatten().astype(int)\n\n# get residuals\nresiduals = test_actual - pred_knn\n\n# plot residuals\nfig = plt.figure(figsize=(20,15))\nplt.scatter(pred_knn, residuals)","88114c2f":"# get mean absolute error\n\nmae = round(mean_absolute_error(test_actual, pred_knn), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, pred_knn)), 2)\n\nmodels['Models'].append('KNN')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)","1193e341":"# running our model with the optimal parameters\n\nsvr = SVR(kernel = 'rbf', gamma = 'auto', C = 10, epsilon = .01)\n\n# fitting our estimator to train data\nsvr.fit(X_train_refined, y_train)\n\n# getting R^2 with cv\ncv_5 = cross_val_score(svr, X_train_refined, y_train, cv=5, n_jobs=-4)\nr2 = cv_5.mean()\nr2","ced02b7a":"# make new predictions on test\npredictions_SVR = svr.predict(X_test_refined)\npredictions_SVR_unscaled = np.exp(predictions_SVR)\n\n# get residuals\nresiduals = test_actual - predictions_SVR_unscaled\n\n# plot residuals\nfig = plt.figure(figsize=(20,15))\nplt.scatter(predictions_SVR_unscaled, residuals)","96d9b9e4":"# get mean absolute error\n\nmae = round(mean_absolute_error(test_actual, predictions_SVR_unscaled), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, predictions_SVR_unscaled)), 2)\n\nmodels['Models'].append('SVR')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)","406598ac":"# categoricals with first not dropped for tree\nboost_train_cats = pd.get_dummies(df_categoricals[high_one_hot_cat], prefix=high_one_hot_cat, drop_first=False)\nboost_test_cats= pd.get_dummies(holdout_categoricals[high_one_hot_cat], prefix=high_one_hot_cat, drop_first=False)\n\n# continuous not transformed or standardized\nboost_train_continuous = x_continuous[high_one_hot_cont]\nboost_holdout_continuous = holdout_continuous[high_one_hot_cont]\n\n# decision tree regression train and test sets\nx_train_boost = pd.concat([boost_train_continuous, boost_train_cats], axis=1)\nx_test_boost = pd.concat([boost_holdout_continuous, boost_test_cats], axis=1)\n\n# redoing our y_train and y_test as non-log transformed\ny = df[target] # our target variable\n\n# creating our train\/validation sets and our test sets\ny_train, y_test = train_test_split(y, test_size=0.2, random_state=randomstate)\n\n# reset indices to avoid index mismatches\ny_train = pd.DataFrame(y_train)\ny_train.reset_index(inplace=True)\ny_train.drop('index', axis=1, inplace=True)\n\ny_test = pd.DataFrame(y_test)\ny_test.reset_index(inplace=True)\ny_test.drop('index', axis=1, inplace=True)","c6f6e0c9":"best_xgb_model = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 5,\n                min_child_weight = 8,\n                gamma = .1,\n                colsample_bytree=.8,\n                reg_alpha = 75,\n                reg_lambda=3,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\nbest_xgb_model.fit(x_train_boost, y_train)\n\ncv_5 = cross_val_score(best_xgb_model, x_train_boost, y_train, cv=5)\nr2 = cv_5.mean()\nr2","04a7ab57":"# make prediction\npreds = best_xgb_model.predict(x_test_boost)\n\n#log residuals\nresiduals = test_actual - preds\n\n# plot residuals\nfig = plt.figure(figsize=(20,15))\nplt.scatter(preds, residuals)","091a5ddd":"# Calculate our mean absolute error\n\nmae = round(mean_absolute_error(test_actual, preds), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, preds)), 2)\n\nmodels['Models'].append('XGBoost - One Hot Encoding')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)","5d39f220":"# categoricals with first not dropped for tree\nboost_train_cats = pd.get_dummies(df_categoricals[low_one_hot_cat], prefix=low_one_hot_cat, drop_first=False)\nboost_test_cats= pd.get_dummies(holdout_categoricals[low_one_hot_cat], prefix=low_one_hot_cat, drop_first=False)\n\n# continuous not transformed or standardized\nboost_train_continuous = x_continuous[low_one_hot_cont]\nboost_holdout_continuous = holdout_continuous[low_one_hot_cont]\n\n# decision tree regression train and test sets\nx_train_boost_encoded = pd.concat([boost_train_continuous, boost_train_cats], axis=1)\nx_test_boost_encoded = pd.concat([boost_holdout_continuous, boost_test_cats], axis=1)\n\n# redoing our y_train and y_test as non-log transformed\ny = df[target] # our target variable\n\n# creating our train\/validation sets and our test sets\ny_train, y_test = train_test_split(y, test_size=0.2, random_state=randomstate)\n\n# reset indices to avoid index mismatches\ny_train = pd.DataFrame(y_train)\ny_train.reset_index(inplace=True)\ny_train.drop('index', axis=1, inplace=True)\n\ny_test = pd.DataFrame(y_test)\ny_test.reset_index(inplace=True)\ny_test.drop('index', axis=1, inplace=True)","78982ee8":"model = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 3,\n                min_child_weight=20,\n                gamma = .1,\n                colsample_bytree=.9,\n                reg_alpha=.15,\n                reg_lambda=3,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\nbest_xgb_model.fit(x_train_boost_encoded, y_train)\n\n\ncv_5 = cross_val_score(best_xgb_model, x_train_boost_encoded, y_train, cv=5)\nr2 = cv_5.mean()\nr2","c1ed537d":"# make prediction\npreds = best_xgb_model.predict(x_test_boost_encoded)\n\n#log residuals\nresiduals = test_actual - preds\n\n# plot residuals\nfig = plt.figure(figsize=(20,15))\nplt.scatter(preds, residuals)","fa05eaf1":"# Calculate our mean absolute error\n\nmae = round(mean_absolute_error(test_actual, preds), 2)\nrmse = round(np.sqrt(mean_squared_error(test_actual, preds)), 2)\n\nmodels['Models'].append('XGBoost - Encoded')\nmodels['r2'].append(r2)\nmodels['mae'].append(mae)\nmodels['rmse'].append(rmse)\n\nprint(\"MAE:\", mae)\n","accae0b2":"# make data frame from our models dictionary\nmodel_types = pd.DataFrame(models)\n\n# sort data frame by mae and reset index\nmodel_types = model_types.sort_values('mae', ascending=True).reset_index()\nmodel_types.drop('index',axis=1, inplace=True)\nmodel_types.set_index('Models', inplace=True)\n\nmodel_types","3e699115":"# plot model mae\n\nplt.figure(figsize=(15,10))\nplt.plot(model_types['mae'])\nplt.title(\"Mean Average Error\")\nplt.xticks(rotation=90)\nplt.xlabel('Model')\nplt.ylabel(\"MAE\");","d45f02d9":"# get the columns we are going to make visualizations from\nviz_df = df[['price', 'sqft_living', 'median_zip', 'zip_rank', 'grade']]\nviz_df['pr_sf'] = round(viz_df['price']\/viz_df['sqft_living'], 2)\nviz_df","abc2e06f":"# make simpler variables for our visualiation variables\nviz_target = viz_df['price']\/100000\nviz_sqft = viz_df['sqft_living']\nviz_grade = viz_df['grade']\nviz_zip = viz_df['zip_rank'] \nviz_zip2 = viz_df['median_zip']\nviz_pr_sf = viz_df['pr_sf']","3f4be21a":"fig, ax=plt.subplots(figsize=(20,15)) # prepare our figure\n\nsns.set(font_scale = 1.5) # set our font scale bigger for this vis\n\n# scatter our data\nscatter2 = sns.scatterplot(x=\"sqft_living\", y=viz_target, data=viz_df, hue='grade', palette='magma_r')\n\n# label axes and title\nax.set_xlabel('Total Square Footage', fontsize=16)\nax.set_ylabel('Price in $100,000', fontsize=16)\nax.set_title(\"Price to Total Square Footage\\nby Grade of Materials\", fontsize=20)\n\n# label and position our legend\nplt.legend(title='Grade', loc='upper left', title_fontsize=20);\n\n# save visualization to png\n#plt.savefig('images\/pr_grade.png')","b33c628e":"fig, ax = plt.subplots(figsize=(20, 15))\n\n#ax.scatter(viz_sqft, viz_pr_sf, c=viz_zip, cmap='magma_r')\nscatter2 = sns.scatterplot(x=\"sqft_living\", y=\"pr_sf\", data=viz_df, hue='grade', palette='magma_r')\n\n# label axes and title\nax.set_xlabel('Total Square Footage', fontsize=16)\nax.set_ylabel('Price per Square Foot', fontsize=16)\nax.set_title(\"Price per Square Foot to Total Square Footage\\nby Grade of Materials\", fontsize=20)\n\n# label and position our legend\nplt.legend(title='Grade', loc='upper right', title_fontsize=20);\n\n# save visualization to png\n#plt.savefig('images\/pr_sf_grade.png');","21c644f9":"# prepare figure\nfig, ax = plt.subplots(figsize=(20, 15))\n\n#scatter our data\nscatter3 = sns.scatterplot(x=\"sqft_living\", y=\"price\", data=viz_df, hue='zip_rank', palette='magma_r')\n#ax.scatter(viz_sqft, viz_target, c=viz_zip, cmap='magma_r')\n\n# label our axes and title\nax.set_xlabel('Total Square Footage', fontsize=16)\nax.set_ylabel('Price in $100,000', fontsize=16)\nax.set_title(\"Price per Total Square Footage\\nby Zip Code Median Value Rank\", fontsize=20);\n\n# save visualization to png\n#plt.savefig('images\/sqft.png');","b33adb1e":"# prepare figure\nfig, ax = plt.subplots(figsize=(20, 15))\n\n#scatter our data\nscatter3 = sns.scatterplot(x=\"sqft_living\", y=\"pr_sf\", data=viz_df, hue='zip_rank', palette='magma_r')\n#ax.scatter(viz_sqft, viz_target, c=viz_zip, cmap='magma_r')\n\n# label our axes and title\nax.set_xlabel('Total Square Footage', fontsize=16)\nax.set_ylabel('Price per Square Foot', fontsize=16)\nax.set_title(\"Price per per Square Foot to Total Square Footage\\nby Zip Code Median Value Rank\", fontsize=20);\n\n# save visualization to png\n#plt.savefig('images\/pr_sf_zip.png');","5072cf3f":"viz_y = viz_df['price']\nviz_x = viz_df.drop('price', axis=1)\n\nfig = plt.figure(figsize=(20,15))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(viz_sqft, viz_grade, viz_target, c=viz_zip, cmap='magma_r')\n#ax.scatter(viz_sqft, viz_grade, viz_target, c='red', label=\"Predictions\")\n#ax.scatter(viz_sqft, viz_grade, end_z\/100000, c='green', label=\"Actuals\")\n\nax.set_xlabel('Square Feet of Living Space', fontsize=12)\nax.set_ylabel('Grade of Materials', fontsize=12)\nax.set_zlabel('Price', fontsize=12)\n\nax.set_title(\"Price per Square Footage and Grade of Materials, by Zip Median Rank\", fontsize=20)\n\n# first num is tilt angle, second num is turn angle\n# default is about 30,305\n# 0, 270 creates side view of pr\/sqft\n# 0, 360 creates side view of pr\/grade\nax.view_init(30, 305)\n\n\n# save visualization to png\n#plt.savefig('images\/3d_feats.png');","4eee2bb0":"break","e9d70449":"# Parameter Tuning\n\nparam_grid = {'kernel' : ['linear', 'rbf', 'poly'],\n              'gamma' : ['scale', 'auto']            \n              }\n\nsvr = SVR(C=5, epsilon=.05, tol=.01, verbose=True)\ngrid_search = GridSearchCV(svr, param_grid, verbose=10, scoring='neg_mean_absolute_error', cv=3, n_jobs=-4)\n\ngrid_search.fit(X_train_refined, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","b9379531":"# Parameter Tuning\n\nparam_grid = {'C' : [3, 5, 7, 10]            \n              }\n\nsvr = SVR(epsilon=.05, tol=.01, verbose=True, kernel='rbf', gamma='auto')\ngrid_search = GridSearchCV(svr, param_grid, verbose=10, scoring='neg_mean_absolute_error', cv=3, n_jobs=-4)\n\ngrid_search.fit(X_train_refined, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","2556ef4b":"# Parameter Tuning\n\nparam_grid = {'epsilon' : [.01, .05, .1, .5]            \n              }\n\nsvr = SVR(C=10, tol=.01, verbose=True, kernel='rbf', gamma='auto')\ngrid_search = GridSearchCV(svr, param_grid, verbose=10, scoring='neg_mean_absolute_error', cv=3, n_jobs=-4)\n\ngrid_search.fit(X_train_refined, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","f6f69036":"# visualize changes to model score as it is tried on different max depths from 10 to 80, to get a starting point for max depth\n\nfrom sklearn.model_selection import validation_curve\ndef ValidationCurve(estimator, predictors, target, param_name, hyperparam):\n    \n    train_score, test_score = validation_curve(estimator, predictors, target, \n                                               param_name, np.arange(1,33,3), \n                                               cv=5, scoring='neg_mean_absolute_error', n_jobs=-4)\n    Rsqaured_train = train_score.mean(axis=1)\n    Rsquared_test= test_score.mean(axis=1)\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(np.arange(1,33,3), Rsqaured_train, color='r', linestyle='-', marker='o', label='Training Set')\n    plt.plot(np.arange(1,33,3), Rsquared_test, color='b', linestyle='-', marker='x', label='Testing Set')\n    plt.legend(labels=['Training Set', 'Testing Set'])\n    plt.xlabel(hyperparam)\n    plt.ylabel('MAE')\n    plt.title(\"Mean Absolute Error for Max Depth on Train\/Test\")\n    \nValidationCurve(xgb.XGBRegressor(), x_train_boost, y_train, 'max_depth', 'Maximum Depth')","4cd94242":"# Parameter Tuning max_depth\n\nparam_grid = {\"max_depth\": [5, 6, 7, 8],\n                          \n              }\n\nmodel = xgb.XGBRegressor(\n                 n_estimators=250,                                                                    \n                 seed=42,\n                 missing=0,\n                 eval_metric='mae')\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-4)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","d9bf4cf9":"# Parameter Tuning max_depth and min_child_weight\n\nparam_grid = {\"min_child_weight\" : [6, 8, 10]            \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=5,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-4)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","e9eb57e8":"# Parameter Tuning max_depth and min_child_weight\n\nparam_grid = {\"min_child_weight\" : [7, 8, 9]            \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=5,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-4)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","60f0fd28":"# Parameter Tuning gamma\n\nparam_grid = {'gamma':[.1, .3, .5, .7, .9],           \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 5,\n                min_child_weight = 8,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","2a4a160c":"# Parameter Tuning subsample\n\nparam_grid = {\n 'subsample':[.2, .4, .6, .8, 1],\n \n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 5,\n                min_child_weight = 8,\n                gamma = .1,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","891df12a":"# Parameter Tuning colsample_by_tree\n\nparam_grid = {\n 'colsample_bytree':[.2, .4, .6, .8, 1]\n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 5,\n                min_child_weight = 8,\n                gamma = .1,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","3d5a790c":"# Parameter Tuning alpha\n\nparam_grid = {\n    'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 5,\n                min_child_weight = 8,\n                gamma = .1,\n                colsample_bytree=.8,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","b28b3dab":"# Parameter Tuning alpha\n\nparam_grid = {\n    'reg_alpha':[75, 100, 125, 150]\n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 5,\n                min_child_weight = 8,\n                gamma = .1,\n                colsample_bytree=.8,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","996e5e17":"# Parameter Tuning lambda\n\nparam_grid = {'lambda':[0.1, 1, 10, 100, 500]            \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 5,\n                min_child_weight = 8,\n                gamma = .1,\n                colsample_bytree=.8,\n                reg_alpha = 75,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","a6252824":"# Parameter Tuning lambda\n\nparam_grid = {'lambda':[3, 5, 7, 10, 15]            \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 5,\n                min_child_weight = 8,\n                gamma = .1,\n                colsample_bytree=.8,\n                reg_alpha = 75,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","a59273e4":"# visualize changes to model score as it is tried on different max depths from 10 to 80, to get a starting point for max depth\n\nfrom sklearn.model_selection import validation_curve\ndef ValidationCurve(estimator, predictors, target, param_name, hyperparam):\n    \n    train_score, test_score = validation_curve(estimator, predictors, target, \n                                               param_name, np.arange(1,33,3), \n                                               cv=5, scoring='neg_mean_absolute_error', n_jobs=-4)\n    Rsqaured_train = train_score.mean(axis=1)\n    Rsquared_test= test_score.mean(axis=1)\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(np.arange(1,33,3), Rsqaured_train, color='r', linestyle='-', marker='o', label='Training Set')\n    plt.plot(np.arange(1,33,3), Rsquared_test, color='b', linestyle='-', marker='x', label='Testing Set')\n    plt.legend(labels=['Training Set', 'Testing Set'])\n    plt.xlabel(hyperparam)\n    plt.ylabel('MAE')\n    plt.title(\"Mean Absolute Error for Max Depth on Train\/Test\")\n    \nValidationCurve(xgb.XGBRegressor(), x_train_boost_encoded, y_train, 'max_depth', 'Maximum Depth')","63ddbb34":"param_grid = {\"max_depth\": [2, 4, 6],       \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","deb20b59":"# Parameter Tuning\n\nparam_grid = {\"max_depth\": [3, 4, 5],        \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","11b0dc7d":"# Parameter Tuning\n\nparam_grid = {\"min_child_weight\" : [6, 8, 10, 20],        \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 3,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","81f722b7":"# Parameter Tuning\n\nparam_grid = {'gamma': [.2, .4, .5, .6, .8, 1],        \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 3,\n                min_child_weight=20,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","3b511a77":"# Parameter Tuning\n\nparam_grid = {'gamma': [.1, .2, .3],        \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 3,\n                min_child_weight=20,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","ab1cac7f":"# Parameter Tuning\n\nparam_grid = {'subsample':[.6, .7, .8, .9, 1],        \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 3,\n                min_child_weight=20,\n                gamma = .1,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","5f5e88e8":"# Parameter Tuning\n\nparam_grid = {'colsample_bytree':[.6, .7, .8, .9, 1],        \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 3,\n                min_child_weight=20,\n                gamma = .1,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","666909a8":"# Parameter Tuning\n\nparam_grid = {'reg_alpha':[1e-5, 1e-2, 0.1, 1, 10, 100],        \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 3,\n                min_child_weight=20,\n                gamma = .1,\n                colsample_bytree=.9,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","2ed9743d":"# Parameter Tuning\n\nparam_grid = {'reg_alpha':[.05, .1, .15,  .3, .5],        \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 3,\n                min_child_weight=20,\n                gamma = .1,\n                colsample_bytree=.9,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","ab831544":"# Parameter Tuning\n\nparam_grid = {'reg_lambda':[1e-5, 1e-2, 0.1, 1, 10, 100],        \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 3,\n                min_child_weight=20,\n                gamma = .1,\n                colsample_bytree=.9,\n                reg_alpha=.15,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","71d51def":"# Parameter Tuning\n\nparam_grid = {'reg_lambda':[.5, 1, 1.5, 3],        \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth = 3,\n                min_child_weight=20,\n                gamma = .1,\n                colsample_bytree=.9,\n                reg_alpha=.15,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(X_train_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","e4cb754d":"Best parameters set found on train set: \n\n    {'gamma': 0.2}\n\n    Grid scores on train set:\n\n    0.887 (+\/-0.009) for {'gamma': 0.2}\n    0.887 (+\/-0.009) for {'gamma': 0.4}\n    0.887 (+\/-0.009) for {'gamma': 0.5}\n    0.887 (+\/-0.009) for {'gamma': 0.6}\n    0.887 (+\/-0.009) for {'gamma': 0.8}\n    0.887 (+\/-0.009) for {'gamma': 1}","10ec0b72":"### Process Categoricals","a8a2819e":"### Duplicate Data","30f0eb85":"We will log transform our data, and standardize our continuous inputs.","d12dea49":"Using all of our predictors we get an R^2 of 82.3% which is less than when we use our zip code and month as categoricals. So in our linear models, we will use our one-hot encoded data set.","c75fac6d":"* Perform exhaustive and accurate data cleaning\n* Explore different categorical handling methods\n* Build a model that accurately predicts house prices in King County","dfa1cad2":"We'll add month sold as a second degree polynomial.","85dc69f1":"# Project Overview\n\n> This is an in-depth notebook which explores the King County Housing Dataset through several different models. The notebook includes a thorough EDA and cleaning section, study of different models using different categorical methods (one-hot encoding vs target encoding) with extensive parameter tuning, an exploration of different feature selection methods, an evaluation of the final model, and visualizations.\n\n* **Business Objective**\n\n\n* **Notebook Preparation**\n    * Importing our Modules\n\n\n* **Preprocessing**\n    * EDA and Cleaning\n        * Scaling Time Series\n        * Duplicates\n        * Outlier Detection\n        * Missing Data\n        * Binary Data\n        * Visualize Cleaned Data\n        * Studying our Target Variable\n    * Create Holdout Set\n    * Feature Engineering\n    * Correlations and Multicollinearity\n    * EDA & Process Train Set\n        * Categoricals\n        * Continuous\n            * Find Interactions\n            * Transform and Standardize Continuous Data\n            * Add Polynomial Features\n    * Process Test Set\n        * Categoricals\n        * Continuous\n    * Create Train\/Test Split\n\n\n* **Model Explorations**\n    * Picking our Base Features\n    * Linear Regressions\n        * Basic LR with Top Features One-Hot Encoded\n        * Basic LR with Top Features Target Encoded\n        * LR with ALL model features\n        * Linear Regression with various Feature Selection Methods\n            * Forward-Backward Selector\n            * Permutation Importance\n            * RFECV\n    * K-Nearest Neighbors\n    * Support Vector Regression\n    * XGBoost Model\n        * One Hot Encoded\n        * Target Encoded\n\n\n* **Model Selection and Analysis**\n\n* **Additional Visualizations**\n\n* **APPENDIX**","103398ce":"### Data Sets Reference","82c9c6fe":"# Notebook Preparation","d1a56c39":"Best parameters set found on train set: \n\n    {'lambda': 3}\n\n    Grid scores on train set:\n\n    0.890 (+\/-0.012) for {'lambda': 3}\n    0.889 (+\/-0.013) for {'lambda': 5}\n    0.888 (+\/-0.018) for {'lambda': 7}\n    0.887 (+\/-0.017) for {'lambda': 10}\n    0.889 (+\/-0.018) for {'lambda': 15}","358a434c":"Feature selectors are different methods to help us pick which features we want to use in our model. In our example above where we used ALL predictors in our linear regression, several of our features had a p-value over .05, which indicates that there is more than a 5% chance that the changes attributed to that feature were actually by random chance. We want features where our p-value is below a threshold that we specify where we are reasonably confident that the feature is contributing to the model and not by random chance.\n\nWe're going to use our one-hot encoded set for this because it performs better than our target encoding set.","9ddf1ae8":"Best parameters set found on train set: \n\n    {'reg_alpha': 0.15}\n\n    Grid scores on train set:\n\n    0.888 (+\/-0.008) for {'reg_alpha': 0.05}\n    0.888 (+\/-0.008) for {'reg_alpha': 0.1}\n    0.888 (+\/-0.008) for {'reg_alpha': 0.15}\n    0.888 (+\/-0.008) for {'reg_alpha': 0.3}\n    0.888 (+\/-0.008) for {'reg_alpha': 0.5}","f812adeb":"Fun fact - \"grade\" was not well defined in the features list. I looked it up and found that grade is \"\tClassification by construction quality which refers to the types of materials used and the quality of workmanship. Buildings of better quality (higher grade) cost more to build per unit of measure and command higher value.\" So, it's the quality of building materials, and DEFINITELY worth keeping in our model.","03f1300d":"We can see a clear pattern in our residuals, which tells us that adding square footage will add to our model. Now we know this already from our correlation map, but this visual reinforces that information.","d1de505e":"We'll add year built as a second degree polynomial.","a8fd7bf2":"### Linear Regression Model - ALL Features","aa8bf9e5":"## XGBoost","d22d49a3":"Best parameters set found on train set: \n\n    {'max_depth': 3}\n\n    Grid scores on train set:\n\n    0.885 (+\/-0.010) for {'max_depth': 3}\n    0.885 (+\/-0.007) for {'max_depth': 4}\n    0.881 (+\/-0.007) for {'max_depth': 5}","bcacfa34":"Run a base model with no cross-validation or specific feature selection with ALL possible features. We're going to use our one-hot encoded set which performed better in our first test.","f622108a":"Seems like year built might have a polynomial relationship with price","898e4716":"We now need to process our holdout in the same way we processed our training set.","5c5a660d":"### Study Target Variable","c2bfabb2":"Same score, but no features above our threshold.","91e7e859":"Best parameters set found on train set: \n\n    {'max_depth': 4}\n\n    Grid scores on train set:\n\n    0.881 (+\/-0.008) for {'max_depth': 2}\n    0.885 (+\/-0.007) for {'max_depth': 4}\n    0.876 (+\/-0.012) for {'max_depth': 6}","76a33d58":"XGBoost parameter tuning is located in the APPENDIX.","4b543a81":"## XGBoost One-Hot Parameter Tuning","6a13ccfc":"#### Target Encoded Zip Code","70f29587":"# APPENDIX","b0c41bd0":"### Basic Model Top Features Only","2586cc6a":"Best parameters set found on train set: \n\n    {'min_child_weight': 8}\n\n    Grid scores on train set:\n\n    0.884 (+\/-0.012) for {'min_child_weight': 7}\n    0.885 (+\/-0.014) for {'min_child_weight': 8}\n    0.884 (+\/-0.018) for {'min_child_weight': 9}","fd731988":"Grade appears to have a polynomial relationship","69da8c45":"### Linear Regression - Feature Selectors","c10becdb":"Now we perform cross-validation with our base model over 5 splits and get our mean R^2.","2b65860c":"Best parameters set found on train set: \n\n    {'reg_alpha': 75}\n\n    Grid scores on train set:\n\n    0.887 (+\/-0.014) for {'reg_alpha': 75}\n    0.887 (+\/-0.014) for {'reg_alpha': 100}\n    0.887 (+\/-0.014) for {'reg_alpha': 125}\n    0.887 (+\/-0.014) for {'reg_alpha': 150}","9aeec9cd":"We're ready to make our final continuous data set. At this point we will make TWO continuous sets, because one of the objectives of this notebook is to study *one-hot encoding* versus *mean target encoding*. So we have one data set for our high one-hot encoding features, and one data set for our mean target encoding features. Later in the notebook we will compare these two features sets in models.","70d8c144":"#### Add polynomial features","5077b943":"*** FINAL TEST ***\n\nBest parameters set found on train set: \n\n    {'C': 10}\n\n    Grid scores on train set:\n\n    -0.111 (+\/-0.003) for {'C': 3}\n    -0.110 (+\/-0.004) for {'C': 5}\n    -0.109 (+\/-0.003) for {'C': 7}\n    -0.109 (+\/-0.003) for {'C': 10}","23376c5f":"### Missing Data","2f7b1d76":"### Process Continuous","eac77fbe":"Our final data sets include:\n\n* X_train_onehot, X_test_onehot - train\/test split predictors for one-hot sets\n* X_train_encoded, X_test_encoded - train\/test split predictors for encoded sets\n* y_train, y_test - target values for all sets\n* y - log transformed price\n* test_actual - exponentiated y_test prices","0721b04a":"This plot of price per total square feet of living space, colored by the zip code median rank, visually shows the importance of location. Let's look at that closer.","82e5d228":"I'll be eliminating price outliers on a per-zipcode basis using the same IQR range. My goal here is not to strip usable data, but to identify and excise any rogue non-standard sales. If I were to do this dataset-wide, many zipcodes would retain their own outliers. We can see in the visualizations just above that zipcodes each have their own price distribution. So we keep our price outlier detection within zipcode.","abde7113":"## K-Nearest Neighbors Model","7b4ff0a1":"#### One-hot set","4a67b3d6":"Best parameters set found on train set: \n\n    {'colsample_bytree': 0.9}\n\n    Grid scores on train set:\n\n    0.887 (+\/-0.009) for {'colsample_bytree': 0.6}\n    0.887 (+\/-0.008) for {'colsample_bytree': 0.7}\n    0.887 (+\/-0.008) for {'colsample_bytree': 0.8}\n    0.888 (+\/-0.008) for {'colsample_bytree': 0.9}\n    0.887 (+\/-0.010) for {'colsample_bytree': 1}","031a50e0":"#### Target Encoded Set","9e83d498":"Our scatter looks like a normal residuals plot, implying that the year built does not add much to our model. There's no strong trend exhibited here. If we can see a pattern when we plot residuals vs a different predictor, it can tell us if a feature might add value to our model.","0908f361":"#### Finding Interactions","492f6e93":"Best parameters set found on train set: \n\n    {'colsample_bytree': 0.8}\n\n    Grid scores on train set:\n\n    0.879 (+\/-0.019) for {'colsample_bytree': 0.2}\n    0.883 (+\/-0.016) for {'colsample_bytree': 0.4}\n    0.886 (+\/-0.014) for {'colsample_bytree': 0.6}\n    0.886 (+\/-0.014) for {'colsample_bytree': 0.8}\n    0.886 (+\/-0.015) for {'colsample_bytree': 1}","9356adef":"K-Nearest Neighbors is more commonly used for classification. Its basic premise is to determine \"what is this like\" in making a prediction, by looking at other things that are close in value\/type. We can pick how many neighbors it assesses to make a classification. As we will see, it doesn't work very well for this type of application (or, I've not tuned the hyperparameters properly and\/or don't know how to use it well).\n\nWe're using our target encoded data set on this.","3531c585":"## EDA Cleaning","5a31d797":"Best parameters set found on train set: \n\n    {'gamma': 0.1}\n\n    Grid scores on train set:\n\n    0.885 (+\/-0.014) for {'gamma': 0.1}\n    0.885 (+\/-0.014) for {'gamma': 0.3}\n    0.885 (+\/-0.014) for {'gamma': 0.5}\n    0.885 (+\/-0.014) for {'gamma': 0.7}\n    0.885 (+\/-0.014) for {'gamma': 0.9}","8a9f3801":"## Feature Engineering","29701ae7":"*** FINAL TEST ***\nBest parameters set found on train set: \n\n    {'gamma': 'auto', 'kernel': 'rbf'}\n\n    Grid scores on train set:\n\n    -0.115 (+\/-0.002) for {'gamma': 'scale', 'kernel': 'linear'}\n    -0.120 (+\/-0.002) for {'gamma': 'scale', 'kernel': 'rbf'}\n    -0.167 (+\/-0.007) for {'gamma': 'scale', 'kernel': 'poly'}\n    -0.115 (+\/-0.002) for {'gamma': 'auto', 'kernel': 'linear'}\n    -0.111 (+\/-0.002) for {'gamma': 'auto', 'kernel': 'rbf'}\n    -0.239 (+\/-0.008) for {'gamma': 'auto', 'kernel': 'poly'}","6524b237":"Next we're going to select features using a method called permutation importance. This is a great model-agnostic method that you can use with any model type, and the way it works is very easy to understand. After fitting the model, it calculates a baseline R^2 score. Then for each feature, it scrambles the inputs of that feature, turnings its contribution into noise. The model is evaluated again with the feature scrambled, and the change in overall R^2 is logged as the importance for that feature. After scrambling all features, each feature has been assigned an importance based on the R^2 reduction. You can then select the features that had an effect on R^2 based on your own threshold (I kept anything >= .001) and throw out the remaining features.\n\nYou can learn more about this underrated feature selection method here: https:\/\/explained.ai\/rf-importance\/\nThe article focuses on Random Forest, but discusses permutation importance as an excellent feature selection method for any model type.","2bb102ab":"# Preprocessing","dbf3da64":"We need to slightly redo our one-hot encodings to not drop the first entry.","3d23ff9f":"In this model we will one-hot encode our 70 zip codes.","1b68e503":"## Linear Regressions","c2c4750c":"##### Study Residuals","2f911fcd":"We ran several different types of models, and logged the r^squared and mean absolute error for each model type. Which model performed the best for us?","d28f8022":"Gradient Boosting performs best with optimal parameter tuning. We're going to use sklearn's GridSearchCV to find the optimal hyperparameters to use with our gradient booster! Here are the parameters we are trying out:\n\n* n_estimators: Number of boosts to perform. Gradient boosting is pretty robust to over-fitting so more is usually better\n* max_depth: This determines how many tree nodes the estimator looks at before making a prediction. We don't know what is best here, so we are trying things from 2-4 to see what works the best\n* min_child_weight: Min sum of instance weight needed in a child\n* gamma: Minimum loss reduction to make another partition on a leaf node. Higher results in more conservative algorithm.\n* subsample: Ratio of training sets. .5 means that it will sample half the training data before making trees. Occurs with each boosting iteration.\n* colsample_by_tree: ratio of columns when making a tree\n* alpha: L1 regularization. Higher will make model more conservative.\n* learning_rate: Tuning this setting alters how much the model corrects after it runs a boost. .1 is a common rate and we will test a lower and higher rate as well.","b2f49b01":"## Analysis\n\n> Our final model utilizes a combination of continuous variables and one-hot-encoded categoricals to produce a support vector machine regression with R^2 of 89.3%, a mean absolute error of 52k, and a root mean squared error of 78k. I tried several different zip code transformations including polynomial features, mean target encoding, lower-granularity binning, and median rank as a continuous, and ALL of these efforts resulted in a lower R^2 and higher mean absolute error, leading to a final decision to one-hot encode all 70 zip codes individually. Similar efforts on other categoricals such as age and month sold also did not improve the model over one-hot encoding. This resulted in the greatest accuracy despite a model that is more \"messy\" with a large number of features.\n\n### What are the primary factors influencing housing prices in the King County metro area?\n> As square footage increases so does quality of materials. Most importantly you can see the upward price trend with both increased square footage and materials grade. I was intrigued that our lower bound of data points is very linear, but as our square footage increases, the upper bound gradually breaks away with higher variance. \n\n>I ranked the 70 zip codes in King County by median home value, and used those ranks to color our data points.  Our low median zip codes have a low price per square footage, and price per square foot increases with zip code median, which makes sense, but also shows the importance of zip code to pricing. I found it interesting that while most zip codes exhibit a clear trend of price per square foot decreasing with increased total square footage, which is entirely normal, certain very high value zip codes seem to retain their high price per square foot regardless of total square footage. Certain zip codes seem immune to the usual price per square foot decay. \n\n> As they say, location is everything, and it is the primary influencing factor for a home price in the King County metro area. Our darkest areas, and therefore highest value sales, are clustered in and around Seattle to the west of Lake Washington and into the eastern lake cities of Bellevue and Redmond which are the technical employer hubs of the region. As we move away from Seattle and the tech hubs into the suburbs, our prices clearly go down.\n\n> These three features alone explain 85% of the price variance.\n\n### Can we effectively use a regression model based system for realtors to determine a proper list price?\n> Our model, while explaining over 89.3% of the price variance with our features, was nonetheless far from accurate in absolute terms. A mean average error of 52k in either direction is a huge variance to a home price - one that is so large that it renders the prediction much less meaningful and useful. Other models need to be explored, better data needs to be sourced, or easy-to-use features that an average realtor is capable of evaluating\/acquiring should be added to the model to improve its predictive quality. The model is providing at best a baseline starting point.\n","dcb2bb97":"### Categoricals","0a299419":"#### Forward-Backward Selector","34858f6c":"It looks like zip code works better than the lat\/long zones I made. We should also note here that zip and lat\/long descriptors are all highly correlated, so we should consider if we need to include all of them in our model or just the best one (zipcode)\n\nWe can get a sense of the most important features to our price from our correlation table. Zipcode as a plain variable does not correlate, which makes sense, because without some sort of transformation it is an arbitrary unordered number. We can see how transformed as zip_smooth, median_zip, or zip_rank it becomes the MOST important contributor to price. We can see here that big contributors to price include:\n    \n    * zip code (in some altered form, not as arbitrary number)\n    * grade\n    * sqft_living\n    * bathrooms","022a3381":"This data includes a TIME SERIES, and we need to bring our target prices to the same time scale. It's easy to ignore that these homes were sold over the space of a year, but a year is a long time for real estate. King County median prices increased 9.05% from May 2014 to May 2015, and we need to account for this kind of change in our model. We'll do this by appreciating all sale prices into May 2015 using an appreciation rate over the year of 9.05%, and dividing into months (more granularity is possible of course)","66f6807b":"## XGBoost Encoded Parameter Tuning","d17da751":"Time to check and remove any features with high collinearity.","fc1b9f30":"### Visualize Cleaned Data","9a1fa437":"#### Recursive Feature Elimination with Cross Validation - Linear Regression","6ee3aa59":"No clear relationship","7a83f5eb":"median_zip, zip_rank and orig_price are dummy variables that we won't be using, so we can ignore correlations using those for now.\n\nWe drop sqft_above and sqft_lot15","888ca363":"RFECV is a reverse forward-backward selector. It starts the model with all features in use then removes the weakest one, and iterates until the best feature set is found. It uses integrated cross-validation to determine the optimal set of features in the model with the best cross-validated score. We score on mean absolute error.","35626884":"We will express this as a second degree polynomial.","75ce7a48":"#### One-Hot Encoded Zip Code","d81f5581":"Run a base model with no cross-validation or specific feature selection with ALL possible features. We're using our target categorical encoded set which performed worse in our first test.","810a470b":"There does not appear to be a polynomial relationship","3a760d7b":"We're going to build our most baseline model using only the top three features -\n    \n    * Zip code\n    * Grade\n    * sqft_living\n    * bathrooms","10f9cc91":"Now we perform cross-validation with our base model over 5 splits and get our mean R^2.","5b56f5c8":"## Picking our Base Features","8b2ae691":"## Feature Visualizations","a3121c8a":"Square foot living may have a polynomial relationship","83d6aac1":"*** FINAL TEST ***\n\nBest parameters set found on train set: \n\n    {'epsilon': 0.01}\n\n    Grid scores on train set:\n\n    -0.109 (+\/-0.003) for {'epsilon': 0.01}\n    -0.109 (+\/-0.003) for {'epsilon': 0.05}\n    -0.110 (+\/-0.004) for {'epsilon': 0.1}\n    -0.153 (+\/-0.004) for {'epsilon': 0.5}","ee7389d3":"Using all of our predictors we get an R^2 of 88.4% which is decent. This indicates that 88.4% of the variance in price can be explained by our features. ","5e75ca49":"This model is entirely different than linear regression. Gradient boosting uses decision trees to learn about outcomes, with trees being added to the model one at a time and existing trees in the model are not changed. Each successive tree tries to improve upon the predictions of the first one, with the weights of the various decision points being updated each time. Gradient boosting uses the residuals to improve its next tree prediction. Overall much more opaque of a process than linear regression.","ec2dcf93":"### Binary data","4129ffa1":"We COULD tune all of these at once with one huge grid search. I try to avoid this because it takes several hours, and we can get a similar result with careful iterative tuning.\n\nThe tuning iterations are found in the APPENDIX.","048d50c5":"First we'll try a simple forward-backward feature selection model based on p-value, using a statsmodel OLS linear regression model. This selector starts with zero features, internally runs the model for each feature individually, and adds the lowest p-value feature to its list to include. It then runs the model again with the original feature included and tries adding each other feature individually. It will either add the next best feature under the threshold or remove an existing feature if it is no longer within the threshold. This process iterates until all features in the model are under the p-value threshold.\n\nThis selector was written by David Dale: https:\/\/datascience.stackexchange.com\/questions\/937\/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm","cc459302":"### Scale Time Series","de558749":"One thing this data set lacks is *type of sale*. We want a model that properly describes the open market, but this dataset definitely includes sales that are outside the market, such as inter-family sales or other sales that don't properly reflect market value. The best way I can think of to find these outliers is to find suspiciously low sales based on location, because we can see in the lat\/long scatter that location makes a difference. We will hunt these down manually even though that takes some time.","96254c9b":"#### Permutation Importance","5bc15e83":"Best parameters set found on train set: \n\n    {'reg_alpha': 0.1}\n\n    Grid scores on train set:\n\n    0.888 (+\/-0.008) for {'reg_alpha': 1e-05}\n    0.888 (+\/-0.008) for {'reg_alpha': 0.01}\n    0.888 (+\/-0.008) for {'reg_alpha': 0.1}\n    0.888 (+\/-0.008) for {'reg_alpha': 1}\n    0.888 (+\/-0.008) for {'reg_alpha': 10}\n    0.888 (+\/-0.008) for {'reg_alpha': 100}","d413fc1b":"Looking at our price per zip code in a different way, we can see a potential issue with our data. Most of our data set is fairly normal and expected. Then we have three zip codes giving us very strong right tails. Then we have ONE zip code way off yonder which may cause problems with our model. ","00dc1ff1":"Why should we spend the time excising just a few outliers from thousands of rows of data? Because it's the right thing to do to clean well and improve our data quality.","74090384":"This looks better expressed with a third degree polynomial","45f2665a":"We need to separate our holdout data before any further processing.\n\nThe reasons for this are:\n\n    * We will standardize our continuous variables, and you should standardize only on your train set and apply that to your test set.\n    * We will be feature engineering on our train set, and applying that later to our test set. We cannot have our test set data leak into our engineered features.","85b0fd37":"Using latitude and longitude, we make a visual map of the King County area that lets us see the map outliers. We can see that there are a few sales in locations that are outliers in some way - still in King County but very far away from the metro area on which we are focusing. Sales that are too far outside of our comparison area geographically may add noise to our model. So I'm going to drop the longitude outliers as well. \n\nThis visualization suggests that location is very important to home price. We're going to check that out more directly.","ebeb932a":"Best parameters set found on train set: \n\n    {'reg_alpha': 100}\n\n    Grid scores on train set:\n\n    0.885 (+\/-0.014) for {'reg_alpha': 1e-05}\n    0.885 (+\/-0.014) for {'reg_alpha': 0.01}\n    0.885 (+\/-0.014) for {'reg_alpha': 0.1}\n    0.885 (+\/-0.014) for {'reg_alpha': 1}\n    0.886 (+\/-0.013) for {'reg_alpha': 100}","19b50ba4":"### Continuous","019f43b7":"## Create Train\/Test Final Set","685a40b4":"# Model Selection","91543244":"#### Manually locating zip\/price outliers","4c5c167c":"Best parameters set found on train set: \n\n    {'min_child_weight': 8}\n\n    Grid scores on train set:\n\n    0.885 (+\/-0.014) for {'min_child_weight': 6}\n    0.885 (+\/-0.014) for {'min_child_weight': 8}\n    0.884 (+\/-0.014) for {'min_child_weight': 10}","e66b57d7":"Best parameters set found on train set: \n\n    {'lambda': 10}\n\n    Grid scores on train set:\n\n    0.883 (+\/-0.017) for {'lambda': 0.1}\n    0.887 (+\/-0.014) for {'lambda': 1}\n    0.887 (+\/-0.017) for {'lambda': 10}\n    0.885 (+\/-0.014) for {'lambda': 100}\n    0.874 (+\/-0.016) for {'lambda': 500}","e6e4d991":"Now we will add our polynomial features on our low-one-hot set (in the high-one-hot set, these are categoricals)","33da7145":"Best parameters set found on train set: \n\n    {'reg_lambda': 3}\n\n    Grid scores on train set:\n\n    0.888 (+\/-0.008) for {'reg_lambda': 0.5}\n    0.887 (+\/-0.008) for {'reg_lambda': 1}\n    0.886 (+\/-0.010) for {'reg_lambda': 1.5}\n    0.888 (+\/-0.009) for {'reg_lambda': 3}","8977e5eb":"In this model we will use our target encoded  zip codes using zip_smooth.","f888037d":"    * sqft_living, sqft_living15, zip_smooth, lat_long, sqft_basement(when present) have a very strong visual relationship with price\n    * sqft_lot do not appear to have a strong relationship with price\n    * year_smooth and month_smooth are unclear, we will investigate further","7805d99b":"We're going to evaluate a few different variations of our linear regression model, as well as a few more complex model types. In order to keep track of our results, we'll be making a dictionary to store our model accuracy results.","8aa77356":"RFECV opted to include all the features, which I find interesting.\n","ce19ace2":"We're now going to run the same XGBoost regressor, but using our data set with the target encoding. Rather than try to figure out which one is better, we will just try both!","b2c2226a":"Best parameters set found on train set: \n\n    {'subsample': 1}\n\n    Grid scores on train set:\n\n    0.843 (+\/-0.022) for {'subsample': 0.2}\n    0.869 (+\/-0.019) for {'subsample': 0.4}\n    0.876 (+\/-0.015) for {'subsample': 0.6}\n    0.882 (+\/-0.019) for {'subsample': 0.8}\n    0.885 (+\/-0.014) for {'subsample': 1}","c9d5d946":"We can see from this graph that Grade has relevance to our model, because the data show a clear pattern. \n\nContrast this with yr_built, our lowest correlated feature before we encoded it.","61f670d6":"Best parameters set found on train set: \n\n    {'subsample': 1}\n\n    Grid scores on train set:\n\n    0.879 (+\/-0.008) for {'subsample': 0.6}\n    0.882 (+\/-0.007) for {'subsample': 0.7}\n    0.883 (+\/-0.009) for {'subsample': 0.8}\n    0.886 (+\/-0.008) for {'subsample': 0.9}\n    0.887 (+\/-0.009) for {'subsample': 1}","fa8a69a9":"We're going to do some target-encoded feature engineering on our data set, using additive smoothing based on this article by Max Halford https:\/\/maxhalford.github.io\/blog\/target-encoding\/\n\nUsing this method we'll add features for:\n   * smoothed zip code\n   * smoothed year\n   * smoothed month sold\n   * smoothed latitude\n   * smoothed longitude\n   * lat\/long zone as function of smoothed lat and smoothed long\n\nThis feature engineering is the primary reason that we chose to separate our holdout set before any other processing. We cannot, under any circumstances, use the test data to inform our train set features!","53b78ade":"We'll do the same regression on zip code again, this time residuals plotted against grade. Again a relationship can be seen.","8a8d6e79":"We see potential outliers in price, sqft_lot, sqft_living, and bedrooms.","a1a7b004":"Gradient boosting uses decision trees to learn about outcomes, with trees being added to the model one at a time and existing trees in the model are not changed. Each successive tree tries to improve upon the predictions of the first one, with the weights of the various decision points being updated each time. Gradient boosting uses the residuals to improve its next tree prediction. Overall much more opaque of a process than linear regression.","68d2ffd0":"### One-Hot Encoding","37611531":"# Objective","b701851c":"Best parameters set found on train set: \n\n    {'min_child_weight': 20}\n\n    Grid scores on train set:\n\n    0.885 (+\/-0.008) for {'min_child_weight': 6}\n    0.887 (+\/-0.009) for {'min_child_weight': 8}\n    0.886 (+\/-0.008) for {'min_child_weight': 10}\n    0.887 (+\/-0.009) for {'min_child_weight': 20}}","80365d61":"## Create Holdout Set","00e03fee":"We're now going to work with a different model type that is entirely different from linear regression. \n\nThere's conflicting information on whether we should use one-hot encoding, or target encoding. We'll solve this by trying both and figuring out what works best for our data set.","e33a3628":"Our R-squared of 78.3% is much lower than when we used our zip code as categoricals. ","942eda1b":"I wrote a function which finds all of the feature combinations possible in our dataset. Then for each combination, the function runs a linear regression with cross validation on 5 folds and gets the r^2 score for the regression including that feature combination. All scores are recorded and r^2 score improvement is assessed, with the resulting table giving the increase in model improvement from a feature combo. ","d5925792":"We can use residuals plots to determine if features are important enough to add to our model. If we regress our target on a predictor, and then plot those residuals against a DIFFERENT predictor, our plot will tell us if the new feature might add to our model.\n\nWe're going to add features in order of their correlation with price on our correlation heat map, so our base feature is a transformed zip code because it has the strongest correlation.\n\nWe regress our target on zip code (we'll use zip_smooth), then we plot our residuals against total square footage.","4f17eec9":"There are a few columns that I might want to be binary flags. I need to see what value choices are in these columns.","f0b0d366":"Best parameters set found on train set: \n\n    {'reg_lambda': 1}\n\n    Grid scores on train set:\n\n    0.887 (+\/-0.006) for {'reg_lambda': 1e-05}\n    0.887 (+\/-0.006) for {'reg_lambda': 0.01}\n    0.887 (+\/-0.006) for {'reg_lambda': 0.1}\n    0.888 (+\/-0.008) for {'reg_lambda': 1}\n    0.888 (+\/-0.010) for {'reg_lambda': 10}\n    0.885 (+\/-0.010) for {'reg_lambda': 100}","e067af18":"## Support Vector Regression","f676f939":"We're going to use sklearn's GridSearchCV to find the optimal hyperparameters to use with our SVM! Here are the parameters we are trying out:\n\n* kernel: linear is parametric, and rbf is non-parametric. One of these should perform better. Our data is not totally normal, so it might be rbf.\n* epsilon: This value is how much error we're ok with accepting without assigning a penalty to the model\n* C: The error that we will accept from a point outside our epsilon\n\nOur C and epsilon need to be in scale with our output variable, which is our log-transformed price.\n","e57a2467":"## EDA and Process Train Set","dfad2d01":"## Process Test Set","b2cc4e87":"### Outlier Detection","ae4090e8":"Our R-squared of 85.6% indicates that 85.6% of the variance in price can be explained by our features. ","878ff537":"#### Other outlier detection","5e8dd2da":"XGBoost parameter tuning is located in the appendix.","aed89ad6":"Now we'll look for bad data in other categories. This is easy to do using describe.","d0b29d81":"Here's a fun way to see the improvements to our data quality after we clean outliers! A much deeper color map.","eb2042b0":"# Model Explorations","8c23b698":"Support vector regression is a form of regression that allows us to define the acceptable error in our model and then finds the line that best fits the data, according to our specifications. This is really useful with something like housing price predictions, where we are ok with our prediction being within a certain dollar amount. SVR will attempt to get all of the predictions within that dollar amount when possible. This will result in a fit line that is different than a linear regression would have produced, but should result in a lower absolute error, which is a reasonable scoring metric for housing price predictions.","ff1a1567":"# Additional Visualizations","3ea3b43e":"We should never remove outliers indiscriminately, especially if they contain real data. Ultimately I opt to remove via IQR on sqft_living and sqft_lot, with reservations. A lot of our variables are not normally shaped, so we can't reliably remove outliers via standard deviation.\n\nFor the square footage variables, I ultimately concluded that extremely large houses and lots are so seriously under-represented in the dataset that we won't be able to reliably predict on them anyway and they are better left off.\n\nIn order to prevent a lot of data loss in this way, I kept IQR range of 1.6 instead of the standard 1.5","19e92c71":"## Correlations\/Multicollinearity","420bbf62":"Gradient Boosting performs best with optimal parameter tuning. We're going to use sklearn's GridSearchCV to find the optimal hyperparameters to use with our gradient booster! Here are the parameters we are trying out:\n\n* n_estimators: Number of boosts to perform. Gradient boosting is pretty robust to over-fitting so more is usually better\n* max_depth: This determines how many tree nodes the estimator looks at before making a prediction. We don't know what is best here, so we are trying things from 2-4 to see what works the best\n* min_child_weight: \n* gamma: \n* subsample:\n* colsample_by_tree: \n* reg_alpha: \n* learning_rate: Tuning this setting alters how much the model corrects after it runs a boost. .1 is a common rate and we will test a lower and higher rate as well.","9c0c4155":"Our final data sets include:\n\n* X_train_onehot, X_test_onehot - train\/test split predictors for one-hot sets\n* X_train_encoded, X_test_encoded - train\/test split predictors for encoded sets\n* y_train, y_test - target values for all sets\n* test_actual - exponentiated y_test prices","c6f03979":"### Target Encoding","72c5d6cc":"There are a good number of features included in this model with a p-value over .05, meaning there is a greater than 5% chance that the results are due to randomness of the sample rather than the feature. A lot of our features have a very low p-value which indicates a very low chance that these results are not affected by the feature. ","6564c4f7":"Best parameters set found on train set: \n\n    {'max_depth': 5}\n\n    Grid scores on train set:\n\n    0.881 (+\/-0.016) for {'max_depth': 5}\n    0.878 (+\/-0.020) for {'max_depth': 6}\n    0.878 (+\/-0.013) for {'max_depth': 7}\n    0.872 (+\/-0.016) for {'max_depth': 8}","37087a1f":"Our baseline model has an R^2 of 85.6% which isn't awful, on only a few features. Our MAE is pretty high. We will see if we can improve on that with some other feature selection methods, and even some other model types. But FIRST we will try the same model but using our target encoded zipcode instead.","c664a4ba":"Ultimately we will select the sklearn SVM method for our model. ","6a191952":"#### Looking for polynomial relationships","9f894891":"Month sold may also have a polynomial relationship","3ff607c0":"## SVM Parameter Tuning","433221f8":"Best parameters set found on train set: \n\n    {'gamma': 0.1}\n\n    Grid scores on train set:\n\n    0.887 (+\/-0.009) for {'gamma': 0.1}\n    0.887 (+\/-0.009) for {'gamma': 0.2}\n    0.887 (+\/-0.009) for {'gamma': 0.3}","71b220e9":"There are a good number of features included in this model with a p-value over .05, meaning there is a greater than 5% chance that the results are due to randomness of the sample rather than the feature. A lot of our features have a very low p-value which indicates a very low chance that these results are not affected by the feature. ","539e5a4f":"We see there might be a relationship between square footage and our zip code. This makes sense because the size of the house is differently priced depending on location, as we saw in our visualizations earlier.","2241bf8e":"#### Standardize and Transform"}}