{"cell_type":{"bbb681cc":"code","6a48abcc":"code","799c40ae":"code","fe82b9f3":"code","5f3fd784":"code","52e7ecbb":"code","b7dfabc6":"code","e18d716a":"code","4bd1d521":"code","547de126":"code","80ffee2b":"code","5a4ee4a9":"code","afc58ea0":"markdown","b139801c":"markdown","e5c9958c":"markdown","b511420a":"markdown","adf73cbe":"markdown","12e44463":"markdown"},"source":{"bbb681cc":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport tensorflow as tf","6a48abcc":"data = pd.read_csv('..\/input\/disneyland-reviews\/DisneylandReviews.csv', encoding='latin-1')","799c40ae":"data","fe82b9f3":"data.info()","5f3fd784":"def get_sequences(texts, tokenizer, train=True, max_seq_length=None):\n    sequences = tokenizer.texts_to_sequences(texts)\n    \n    if train == True:\n        max_seq_length = np.max(list(map(len, sequences)))\n    \n    sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n    \n    return sequences","52e7ecbb":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Limit data to only the review and rating column\n    y = df['Rating']\n    X = df['Review_Text']\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Fit tokenizer\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(X_train)\n    print(\"Vocab length:\", len(tokenizer.word_index) + 1)\n    \n    # Convert texts to sequences\n    X_train = get_sequences(X_train, tokenizer, train=True)\n    X_test = get_sequences(X_test, tokenizer, train=False, max_seq_length=X_train.shape[1])\n    \n    return X_train, X_test, y_train, y_test, tokenizer","b7dfabc6":"X_train, X_test, y_train, y_test, t = preprocess_inputs(data)","e18d716a":"X_train","4bd1d521":"y_train","547de126":"X_train.shape","80ffee2b":"inputs = tf.keras.Input(shape=(3958,))\nx = tf.keras.layers.Embedding(\n    input_dim=37846,\n    output_dim=64\n)(inputs)\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\noutputs = tf.keras.layers.Dense(1, activation='linear')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='mse'\n)\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","5a4ee4a9":"y_pred = np.squeeze(model.predict(X_test))\n\nrmse = np.sqrt(np.mean((y_test - y_pred)**2))\nr2 = 1 - (np.sum((y_test - y_pred)**2) \/ np.sum((y_test - y_test.mean())**2))\n\nprint(\"     RMSE: {:.2f}\".format(rmse))\nprint(\"R^2 Score: {:.5f}\".format(r2))","afc58ea0":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/uu6Gx4gI-zg","b139801c":"# Training","e5c9958c":"# Preprocessing","b511420a":"# Results","adf73cbe":"# Getting Started","12e44463":"# Task for Today  \n\n***\n\n## Disneyland Review Rating Prediction  \n  \nGiven *reviews of Disneyland*, let's try to predict the **rating** associated with a given review.  \n  \nWe will use a TensorFlow\/Keras text model with word embeddings to make our predictions."}}