{"cell_type":{"a497bfaf":"code","e76a1207":"code","f9fac1da":"code","eea3a398":"code","d8068fa4":"code","2b4f8c44":"code","de59b325":"code","7fbd62f1":"code","bc46bb54":"code","55a69994":"code","54dab10b":"code","60eab62b":"code","855f62fb":"code","46be0536":"code","7e4e4ba6":"code","1db6a75d":"code","1b60f114":"code","2ad63c29":"code","6e74e9db":"code","3aa61d0f":"code","1e0408c2":"code","d189d905":"code","02041dcc":"code","e08b2e68":"code","e40ba75f":"code","c884e801":"code","2f20c48c":"code","635e496d":"code","a171fa12":"code","6d9d1d65":"code","a8da99e6":"code","6706f395":"code","24bda5fd":"code","00f1ba11":"code","83d78131":"code","ac22095b":"code","39bac3e9":"code","acb45e15":"code","aaab7b59":"code","c171c74f":"code","d9b31105":"code","8fe8b574":"code","615fdf5f":"code","0f4316e4":"code","92656869":"code","cfe6e2da":"code","5ab3bd6f":"code","2361c11f":"code","ccffe34c":"code","d77da3c3":"code","ffe78916":"code","5cf4c03b":"code","ee806b45":"code","d296e238":"code","ef2136c8":"code","ba6bbd1f":"code","00ab077e":"code","9bbdfc79":"code","c55a84b3":"code","431dd657":"code","1c6aa9be":"code","48b74dd7":"code","b780531d":"code","ac66d0be":"code","e202109c":"code","c7c94679":"code","34e5de3b":"markdown","3b3a1da0":"markdown","f87345a0":"markdown","48e83f9c":"markdown","3e657458":"markdown","763749f2":"markdown","28da2a6c":"markdown","f073540c":"markdown","2ad04ab4":"markdown","7d2ce86a":"markdown","2c1e2711":"markdown","d2d97e98":"markdown","76846a0e":"markdown","ea7602d9":"markdown","86e7f78b":"markdown","4c4adb08":"markdown","14c99ff7":"markdown","013602d6":"markdown","cfcd9ef4":"markdown","31aee841":"markdown","32406daa":"markdown","68c44252":"markdown","53b05440":"markdown","05e7f437":"markdown"},"source":{"a497bfaf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.plotly as py\nimport plotly.graph_objs as go\n\n% matplotlib inline\nplt.style.use('fivethirtyeight')","e76a1207":"%time data = pd.read_table('..\/input\/XYZCorp_LendingData.txt',parse_dates=['issue_d'],low_memory=False)","f9fac1da":"train_df = data[data['issue_d'] < '2015-6-01']\ntest_df = data[data['issue_d'] >= '2015-6-01']","eea3a398":"train = train_df.copy()\ntest = test_df.copy()","d8068fa4":"train.dtypes.value_counts()","2b4f8c44":"print(train.shape)\nprint(test.shape)","de59b325":"train['default_ind'].value_counts().plot.bar()","7fbd62f1":"train.head()","bc46bb54":"train.describe()","55a69994":"train.describe(exclude=np.number)","54dab10b":"train.dtypes.value_counts()","60eab62b":"fig, ax = plt.subplots(1, 3, figsize=(16,5))\n\nsns.distplot(train['loan_amnt'], ax=ax[0])\nsns.distplot(train['funded_amnt'], ax=ax[1])\nsns.distplot(train['funded_amnt_inv'], ax=ax[2])\n\nax[1].set_title(\"Amount Funded by the Lender\")\nax[0].set_title(\"Loan Applied by the Borrower\")\nax[2].set_title(\"Total committed by Investors\")","855f62fb":"train.purpose.value_counts(ascending=False).plot.bar(figsize=(10,5))\nplt.xlabel('purpose'); plt.ylabel('Density'); plt.title('Purpose of loan');","46be0536":"plt.figure(figsize=(10,5))\ntrain['issue_year'] = train['issue_d'].dt.year\nsns.barplot(x='issue_year',y='loan_amnt',data=train)","7e4e4ba6":"# Loan Status \nfig, ax = plt.subplots(1, 2, figsize=(16,5))\ntrain['default_ind'].value_counts().plot.pie(explode=[0,0.25],labels=['good loans','bad loans'],\n                                             autopct='%1.2f%%',startangle=70,ax=ax[0])\nsns.kdeplot(train.loc[train['default_ind']==0,'issue_year'],label='default_ind = 0')\nsns.kdeplot(train.loc[train['default_ind']==1,'issue_year'],label='default_ind = 1')\nplt.xlabel('Year'); plt.ylabel('Density'); plt.title('Yearwise Distribution of defaulter')","1db6a75d":"train.grade.value_counts().plot.bar()","1b60f114":"fig,array=plt.subplots(1,2,figsize=(12,5))\ntrain.loc[train['default_ind']==0,'grade'].value_counts().plot.bar(ax=array[0])\ntrain.loc[train['default_ind']==1,'grade'].value_counts().plot.bar(ax=array[1])\narray[0].set_title('default_ind=0 vs grade'),array[1].set_title('default_ind=1 vs grade')","2ad63c29":"train.addr_state.unique()","6e74e9db":"# Make a list with each of the regions by state.\n\nwest = ['WA','CA', 'OR', 'UT','ID','CO', 'NV', 'NM', 'AK', 'MT', 'HI', 'WY']\nsouth_east = ['AZ', 'TX', 'OK','GA', 'NC', 'VA', 'FL', 'KY', 'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN' ]\nmid_west = ['IL', 'MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN', 'ND']\nnorth_east = ['CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME']\n\ntrain['region'] = np.nan\n\ndef fix_regions(addr_state):\n        if addr_state in west:\n            return 'west'\n        elif addr_state in south_east:\n            return 'south east'\n        elif addr_state in mid_west:\n            return 'mid west'\n        else:\n            return 'north east'\n        \ntrain['region'] = train['addr_state'].apply(fix_regions)","3aa61d0f":"date_amt_region = train[['loan_amnt','issue_d','region']]\nplt.style.use('dark_background')\ncmap = plt.cm.Set3\nby_issued_amount = date_amt_region.groupby(['issue_d', 'region']).loan_amnt.sum()\nby_issued_amount.unstack().plot(stacked=False, colormap=cmap, grid=False, legend=True, figsize=(15,6))\n\nplt.title('Loans issued by Region', fontsize=16)","1e0408c2":"train.emp_length.unique()\ntrain.loc[train['emp_length']=='10+ years','emp_len'] = 10\ntrain.loc[train['emp_length']=='<1 year','emp_len'] = .5\ntrain.loc[train['emp_length']=='1 year','emp_len'] = 1\ntrain.loc[train['emp_length']=='3 years','emp_len'] = 3\ntrain.loc[train['emp_length']=='8 years','emp_len'] = 8\ntrain.loc[train['emp_length']=='9 years','emp_len'] = 9\ntrain.loc[train['emp_length']=='4 years','emp_len'] = 4\ntrain.loc[train['emp_length']=='5 years','emp_len'] = 5\ntrain.loc[train['emp_length']=='6 years','emp_len'] = 6\ntrain.loc[train['emp_length']=='2 years','emp_len'] = 2\ntrain.loc[train['emp_length']=='7 years','emp_len'] = 7\ntrain.loc[train['emp_length']=='nan','emp_len'] = 0","d189d905":"# Loan issued by Region ,Credit Score and grade\nplt.style.use('seaborn-ticks')\n\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(16,12))\n\nregional_interest_rate = train.groupby(['issue_year', 'region']).int_rate.mean()\nregional_interest_rate.unstack().plot(kind='area',  stacked=True,colormap=cmap, grid=False,\n                                      legend=False, figsize=(16,12),ax=ax1)\n\nregional_emp_length = train.groupby(['issue_year', 'region']).emp_len.mean()\nregional_emp_length.unstack().plot(kind='area',  stacked=True,colormap=cmap, grid=False,\n                                      legend=False, figsize=(16,12),ax=ax2)\n\nregional_dti = train.groupby(['issue_year', 'region']).dti.mean()\nregional_dti.unstack().plot(kind='area',  stacked=True,colormap=cmap, grid=False,\n                                      legend=False, figsize=(16,12),ax=ax3)\n\nregional_interest_rate = train.groupby(['issue_year', 'region']).annual_inc.mean()\nregional_interest_rate.unstack().plot(kind='area',  stacked=True,colormap=cmap, grid=False,\n                                      legend=False, figsize=(16,12),ax=ax4)\nax1.set_title('averate interest rate vs region'),ax2.set_title('averate emp_length by region')\nax3.set_title('average dti by region'),ax4.set_title('average annual income by region')\n\nax4.legend(bbox_to_anchor=(-1.0, -0.5, 1.8, 0.1), loc=10,prop={'size':12},\n           ncol=5, mode=\"expand\", borderaxespad=0.)","02041dcc":"print(train.int_rate.mean())\nprint(train.annual_inc.mean())","e08b2e68":"train['income_category'] = np.nan\ntrain.loc[train['annual_inc'] <= 100000,'income_category'] = 'Low'\ntrain.loc[(train['annual_inc'] > 100000) & (train['annual_inc'] <= 200000),'income_category'] = 'Medium'\ntrain.loc[train['annual_inc'] > 200000,'income_category'] = 'High'","e40ba75f":"fig, ((ax1, ax2), (ax3, ax4))= plt.subplots(nrows=2, ncols=2, figsize=(14,8))\nplt.style.use('bmh')\nsns.violinplot(x=\"income_category\", y=\"loan_amnt\", data=train, ax=ax1 )\nsns.violinplot(x=\"income_category\", y=\"default_ind\", data=train, ax=ax2)\nsns.boxplot(x=\"income_category\", y=\"emp_len\", data=train, ax=ax3)\nsns.boxplot(x=\"income_category\", y=\"int_rate\", data=train, ax=ax4)\nplt.tight_layout(h_pad=1.5)","c884e801":"defaulter = train.loc[train['default_ind']==1]\nplt.figure(figsize=(16,16))\nplt.subplot(211)\nsns.boxplot(data=defaulter,x = 'home_ownership',y='loan_amnt',hue='default_ind')\nplt.subplot(212)\nsns.boxplot(data=defaulter,x='issue_year',y='loan_amnt',hue='home_ownership')","2f20c48c":"def missing_values_table(df):\n    total_missing = df.isnull().sum().sort_values(ascending=False)\n    percentage_missing = (100*df.isnull().sum()\/len(df)).sort_values(ascending=False)\n    missing_table = pd.DataFrame({'missing values':total_missing,'% missing':percentage_missing})\n    return missing_table","635e496d":"missing_values = missing_values_table(train)\nmissing_values.head(20)","a171fa12":"train.dtypes.value_counts()","6d9d1d65":"def to_datepart(df,fldname,drop=False):\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', \n            'Is_year_end', 'Is_year_start']\n    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elapsed'] = fld.astype(np.int64)\n    if drop: df.drop(fldname, axis=1, inplace=True)","a8da99e6":"import re\nto_datepart(train,'issue_d',drop=True)\nto_datepart(test,'issue_d',drop=True)","6706f395":"def treat_missing(df):\n    for c in df.columns:\n        if df[c].dtype == 'object':\n            df.fillna(df[c].mode()[0],inplace=True)\n        else:\n            df.fillna(df[c].median(),inplace=True)","24bda5fd":"treat_missing(train)\ntreat_missing(test)","00f1ba11":"def train_cat(df):\n    for n,c in df.items():\n        if df[n].dtype == 'object': df[n] = c.astype('category').cat.as_ordered()","83d78131":"train_cat(train)\ntrain_cat(test)","ac22095b":"train.select_dtypes('category').apply(pd.Series.nunique, axis = 0)","39bac3e9":"to_drop = ['sub_grade','emp_title','desc','title','zip_code',\n           'addr_state','earliest_cr_line','last_pymnt_d','last_credit_pull_d']","acb45e15":"train.drop(to_drop,axis=1,inplace=True)\ntest.drop(to_drop,axis=1,inplace=True)","aaab7b59":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        if len(list(train[c].unique())) <= 2:\n            train[c] = le.fit_transform(train[c])\n            test[c] = le.transform(test[c])","c171c74f":"print(train.shape)\nprint(test.shape)\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)\nprint(train.shape)\nprint(test.shape)","d9b31105":"# train_label = train['default_ind']\n# Align the training and testing data, keep only columns present in both dataframes\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\n# train['default_ind'] = train_label\n\nprint(train.shape)\nprint(test.shape)","8fe8b574":"X = train.copy()\ny = X.pop('default_ind')","615fdf5f":"def split_vals(a,n):return a[:n].copy(),a[n:].copy()","0f4316e4":"n_valid = len(test_df)  # same as test set size\nn_trn = len(X)-n_valid\nraw_train,raw_valid = split_vals(train_df,n_trn)\nX_train, X_valid = split_vals(X, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\nX_train.shape, y_train.shape, X_valid.shape","92656869":"from sklearn.ensemble import RandomForestClassifier\nm = RandomForestClassifier(n_jobs=-1,n_estimators=100)\n%time m.fit(X_train,y_train)","cfe6e2da":"from sklearn.metrics import confusion_matrix,precision_score,recall_score,roc_auc_score\ny_pred = m.predict(X_valid)\nprint(confusion_matrix(y_valid,y_pred))\nprint(precision_score(y_valid,y_pred))\nprint(recall_score(y_valid,y_pred))\nprint(roc_auc_score(y_valid,y_pred))","5ab3bd6f":"m = RandomForestClassifier(n_jobs=-1,n_estimators=100,min_samples_leaf=2)\n%time m.fit(X_train,y_train)\ny_pred = m.predict(X_valid)\nprint(precision_score(y_valid,y_pred))","2361c11f":"y_pred = m.predict(X_valid)\nprint(confusion_matrix(y_valid,y_pred))\nprint(precision_score(y_valid,y_pred))\nprint(recall_score(y_valid,y_pred))\nprint(roc_auc_score(y_valid,y_pred))","ccffe34c":"def feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)","d77da3c3":"fi = feat_importance(m, X_train); fi[:10]","ffe78916":"fi.plot('cols', 'imp', figsize=(10,6), legend=False)\nplt.style.use(\"fivethirtyeight\")","5cf4c03b":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","ee806b45":"plot_fi(fi[:20])","d296e238":"to_keep = fi[fi.imp>0.005].cols; len(to_keep)","ef2136c8":"X_keep = X[to_keep].copy()\nX_train, X_valid = split_vals(X_keep, n_trn)","ba6bbd1f":"m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5,\n                          n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)","00ab077e":"y_pred = m.predict(X_valid)\nprint(confusion_matrix(y_valid,y_pred))\nprint(precision_score(y_valid,y_pred))\nprint(recall_score(y_valid,y_pred))\nprint(roc_auc_score(y_valid,y_pred))","9bbdfc79":"fi = feat_importance(m, X_keep)\nplt.style.use('fivethirtyeight')\nplot_fi(fi)","c55a84b3":"import scipy \nfrom scipy.cluster import hierarchy as hc","431dd657":"corr = np.round(scipy.stats.spearmanr(X_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=X_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","1c6aa9be":"to_drop = ['member_id','id','out_prncp_inv','loan_amnt','funded_amnt','total_pymnt']\nX.drop(to_drop,axis=1,inplace=True)\nX_train, X_valid = split_vals(X, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\nX_train.shape, y_train.shape, X_valid.shape","48b74dd7":"m = RandomForestClassifier(n_jobs=-1,n_estimators=100,min_samples_leaf=2)\n%time m.fit(X_train, y_train)","b780531d":"y_pred = m.predict(X_valid)\nprint(confusion_matrix(y_valid,y_pred))\nprint(precision_score(y_valid,y_pred))\nprint(recall_score(y_valid,y_pred))\nprint(roc_auc_score(y_valid,y_pred))","ac66d0be":"test_label = test['default_ind']\n# Align the training and testing data, keep only columns present in both dataframes\nX, test = train.align(X, join = 'inner', axis = 1)\n\n# Add the target back in\ntest['default_ind'] = train_label\n\nprint(X.shape)\nprint(test.shape)","e202109c":"y_test = test.pop('default_ind')","c7c94679":"y_pred = m.predict(test)\nfrom sklearn.metrics import classification_report\nprint(confusion_matrix(y_test,y_pred))\nprint(precision_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint(recall_score(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))","34e5de3b":"# Import \n<a id=\"import\"><\/a>","3b3a1da0":"## Converting emp length into integer","f87345a0":"## Bad Loans\n<a id=\"bad loans\"><a\/>","48e83f9c":"## Missing Value Treatement","3e657458":"## Clustering Analysis","763749f2":" ## Loan Purpose\n \n \n \n **Debt Consolidation** stands as clear winner for loan purpose, with more than 350K loans\u200a\u2014\u200aor 58% from the total.<br\/>\n\nOther highlights include:\n\n**Credit Card** \u200a\u2014\u200amore than 130K (~20%)<br>\n**Home Improvement\u200a**\u2014\u200amore than 135K (~6%)<br>\n**Other Purposes**\u200a\u2014\u200aless than 30K (~4%)","28da2a6c":"## Income Category","f073540c":"## EDA","2ad04ab4":"# Prediction on test set","7d2ce86a":"## Loan issued by regions\n<a id=\"loan_issued_by_regions\"><a\/>","2c1e2711":"## One Hot Encoding","d2d97e98":"## Treating Categorical Values","76846a0e":"## Loan Grade","ea7602d9":"## Feature Importnace","86e7f78b":"**Destribution of dependent variable**","4c4adb08":"## Aligning training and test data","14c99ff7":"It's good practice to do not look at the test set so I am going to seperate test data from total data ","013602d6":"# Removing redundant features\nOne thing that makes this harder to interpret is that there seem to be some variables with very similar meanings. Let's try to remove redundent features.","cfcd9ef4":"**Describution of independent variable**","31aee841":"## Dependent and Independent Variable","32406daa":"# Preprocessing\n<a id=\"structure_of_data\"><\/a>","68c44252":"## Similar Distributions:\n<a id=\"similar_distributions\"><\/a>\nWe will start by exploring the distribution of the loan amounts and see when did the loan amount issued increased significantly. <br>\n\n<h4> What we need to know: <\/h4> <br>\n<ul>\n<li> Understand what amount was <b>mostly issued<\/b> to borrowers. <\/li>\n<li> Which <b>year<\/b> issued the most loans. <\/li>\n<li> The distribution of loan amounts is a <b>multinomial distribution <\/b>.<\/li>\n<\/ul>\n<h4> Summary: <\/h4><br>\n<ul>\n<li> Most of the <b>loans issued<\/b> were in the range of 10,000 to 20,000 USD. <\/li>\n<li> The <b>year of 2015<\/b> was the year were most loans were issued.<\/li> \n<li> Loans were issued in an <b>incremental manner<\/b>. (Possible due to a recovery in the U.S economy) <\/li>\n<li> The loans <b>applied<\/b> by potential borrowers, the amount <b>issued<\/b> to the borrowers and the amount <b>funded<\/b> by investors are similarly distributed, <b>meaning<\/b> that it is most likely that qualified borrowers are going to get the loan they had applied for. <\/li>\n\n<\/ul>\n\n\n","53b05440":"## Fitting the model","05e7f437":"## Label Encoding"}}