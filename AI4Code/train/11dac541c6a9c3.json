{"cell_type":{"5bd8d2dc":"code","eaf585b1":"code","f3e97cae":"code","a2d6533c":"code","b47f2e6e":"code","82adad12":"code","42e280fd":"code","30c9a2c1":"code","3e9da759":"code","8aadbf12":"code","4ef0b8d7":"code","af913552":"code","8b391cc9":"code","4825342a":"markdown","65bf352e":"markdown","79a50540":"markdown","84e941e3":"markdown","c66fbaa0":"markdown","10fb3dd7":"markdown","bc8321f6":"markdown","435b734c":"markdown","1c946194":"markdown"},"source":{"5bd8d2dc":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","eaf585b1":"def feature_engineering(df):\n    df['Aspect'][df['Aspect'] < 0] += 360 \n    df['Aspect'][df['Aspect'] > 359] -= 360\n    \n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    \n    features_Hillshade = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    df[\"hillshade_mean\"] = df[features_Hillshade].mean(axis = 1)\n    df['hillshade_amp'] = df[features_Hillshade].max(axis = 1) - df[features_Hillshade].min(axis = 1)\n    \n    df[\"ecldn_dist_hydrlgy\"] = (df[\"Horizontal_Distance_To_Hydrology\"] ** 2 + df[\"Vertical_Distance_To_Hydrology\"] ** 2) ** 0.5\n    df[\"mnhttn_dist_hydrlgy\"] = np.abs(df[\"Horizontal_Distance_To_Hydrology\"]) + np.abs(df[\"Vertical_Distance_To_Hydrology\"])\n    df['binned_elevation'] = [np.floor(v\/50.0) for v in df['Elevation']]\n    df['highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n    \n    soil_features = [x for x in df.columns if x.startswith(\"Soil_Type\")]\n    df[\"soil_type_count\"] = df[soil_features].sum(axis=1)\n    \n    wilderness_features = [x for x in df.columns if x.startswith(\"Wilderness_Area\")]\n    df[\"wilderness_area_count\"] = df[wilderness_features].sum(axis = 1)\n    \n    df['soil_Type12_32'] = df['Soil_Type32'] + df['Soil_Type12']\n    df['soil_Type23_22_32_33'] = df['Soil_Type23'] + df['Soil_Type22'] + df['Soil_Type32'] + df['Soil_Type33']\n    \n    df['Horizontal_Distance_To_Roadways'][df['Horizontal_Distance_To_Roadways'] < 0] = 0\n    df['horizontal_Distance_To_Roadways_Log'] = [np.log(v+1) for v in df['Horizontal_Distance_To_Roadways']]\n    df['Horizontal_Distance_To_Fire_Points'][df['Horizontal_Distance_To_Fire_Points'] < 0] = 0\n    df['horizontal_Distance_To_Fire_Points_Log'] = [np.log(v+1) for v in df['Horizontal_Distance_To_Fire_Points']]\n    return df","f3e97cae":"# import train & test data\ndf_train = dt.fread('..\/input\/tabular-playground-series-dec-2021\/train.csv').to_pandas()\ndf_test = dt.fread('..\/input\/tabular-playground-series-dec-2021\/test.csv').to_pandas()\ndf_pseudo = dt.fread('..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv').to_pandas()\n\ndf_train = pd.concat([df_train, df_pseudo], axis=0)\n\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\n\n# drop underrepresented class\ndf_train = df_train[df_train['Cover_Type'] != 5]\n\n# apply feature-engineering\n# thanks to https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373\n# thanks to https:\/\/www.kaggle.com\/teckmengwong\/dcnv2-softmaxclassification#Feature-Engineering\n# df_train = feature_engineering(df_train)\n# df_test = feature_engineering(df_test)\n\n# split dataframes for later modeling\nX = df_train.drop(columns=['Id','Cover_Type','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\ny = df_train['Cover_Type'].copy()\n\nX_test = df_test.drop(columns=['Id','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\n\n# create label-encoded one-hot-vector for softmax, mutliclass classification\nle = LabelEncoder()\ntarget = keras.utils.to_categorical(le.fit_transform(y))\n\ndel df_train, df_test\ngc.collect()\n\nprint(X.shape, y.shape, target.shape, X_test.shape)","a2d6533c":"# define helper functions\ndef set_seed(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    print(f\"Seed set to: {seed}\")\n\ndef plot_eval_results(scores, n_splits):\n    cols = 5\n    rows = int(np.ceil(n_splits\/cols))\n    \n    fig, ax = plt.subplots(rows, cols, tight_layout=True, figsize=(20,2.5))\n    ax = ax.flatten()\n\n    for fold in range(len(scores)):\n        df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['train_loss'],\n            label='train_loss',\n            ax=ax[fold]\n        )\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['valid_loss'],\n            label='valid_loss',\n            ax=ax[fold]\n        )\n\n        ax[fold].set_ylabel('')\n\n    sns.despine()\n\ndef plot_cm(cm):\n    metrics = {\n        'accuracy': cm \/ cm.sum(),\n        'recall' : cm \/ cm.sum(axis=1),\n        'precision': cm \/ cm.sum(axis=0)\n    }\n    \n    fig, ax = plt.subplots(1,3, tight_layout=True, figsize=(15,5))\n    ax = ax.flatten()\n\n    mask = (np.eye(cm.shape[0]) == 0) * 1\n\n    for idx, (name, matrix) in enumerate(metrics.items()):\n\n        ax[idx].set_title(name)\n\n        sns.heatmap(\n            data=matrix,\n            cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar=False,\n            mask=mask,\n            lw=0.25,\n            annot=True,\n            fmt='.2f',\n            ax=ax[idx]\n        )\n    sns.despine()","b47f2e6e":"# define callbacks\nlr = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", \n    factor=0.5, \n    patience=5, \n    verbose=True\n)\n\nes = keras.callbacks.EarlyStopping(\n    monitor=\"val_acc\", \n    patience=10, \n    verbose=True, \n    mode=\"max\", \n    restore_best_weights=True\n)","82adad12":"class GatedLinearUnit(layers.Layer):\n    def __init__(self, units):\n        super(GatedLinearUnit, self).__init__()\n        self.linear = layers.Dense(units)\n        self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n\n    def call(self, inputs):\n        return self.linear(inputs) * self.sigmoid(inputs)\n\nclass GatedResidualNetwork(layers.Layer):\n    def __init__(self, units, dropout_rate):\n        super(GatedResidualNetwork, self).__init__()\n        self.units = units\n        self.elu_dense = layers.Dense(units, activation=\"elu\")\n        self.linear_dense = layers.Dense(units)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.gated_linear_unit = GatedLinearUnit(units)\n        self.layer_norm = layers.LayerNormalization()\n        self.project = layers.Dense(units)\n\n    def call(self, inputs):\n        x = self.elu_dense(inputs)\n        x = self.linear_dense(x)\n        x = self.dropout(x)\n        if inputs.shape[-1] != self.units:\n            inputs = self.project(inputs)\n        x = inputs + self.gated_linear_unit(x)\n        x = self.layer_norm(x)\n        return x\n\nclass VariableSelection(layers.Layer):\n    def __init__(self, num_features, units, dropout_rate):\n        super(VariableSelection, self).__init__()\n        self.grns = list()\n        for idx in range(num_features):\n            grn = GatedResidualNetwork(units, dropout_rate)\n            self.grns.append(grn)\n        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n        self.softmax = layers.Dense(units=num_features, activation=\"softmax\")\n\n    def call(self, inputs):\n        v = layers.concatenate(inputs)\n        v = self.grn_concat(v)\n        v = tf.expand_dims(self.softmax(v), axis=-1)\n\n        x = []\n        for idx, input in enumerate(inputs):\n            x.append(self.grns[idx](input))\n        x = tf.stack(x, axis=1)\n\n        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n        return outputs","42e280fd":"def create_model_inputs():\n    inputs = {}\n    for feature_name in X.columns:\n        inputs[feature_name] = layers.Input(\n            name=feature_name, shape=(), dtype=tf.float32\n        )\n    return inputs\n\ndef encode_inputs(inputs, encoding_size):\n    encoded_features = []\n    for col in range(inputs.shape[1]):\n        encoded_feature = tf.expand_dims(inputs[:, col], -1)\n        encoded_feature = layers.Dense(units=encoding_size)(encoded_feature)\n        encoded_features.append(encoded_feature)\n    return encoded_features\n\ndef create_model(encoding_size, dropout_rate=0.15):\n    inputs = layers.Input(len(X.columns))\n    feature_list = encode_inputs(inputs, encoding_size)\n    num_features = len(feature_list)\n\n    features = VariableSelection(num_features, encoding_size, dropout_rate)(\n        feature_list\n    )\n\n    outputs = layers.Dense(units=target.shape[-1], activation=\"softmax\")(features)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model","30c9a2c1":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","3e9da759":"seed = 2021\nset_seed(seed)\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\npredictions = []\noof_preds = {'y_valid': list(), 'y_hat': list()}\nscores_nn = {fold:None for fold in range(cv.n_splits)}\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    X_train, y_train = X.iloc[idx_train], target[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], target[idx_valid]\n    \n    scl = RobustScaler()\n    X_train = scl.fit_transform(X_train)\n    X_valid = scl.transform(X_valid)\n    \n    with tf_strategy.scope():\n        model = create_model(encoding_size=128)\n\n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            loss=keras.losses.CategoricalCrossentropy(),\n            metrics=['acc']\n        )\n        \n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        epochs=90,\n        batch_size=4096,\n        shuffle=True,\n        verbose=False,\n        callbacks=[lr,es]\n    )\n    \n    scores_nn[fold] = history.history\n    \n    oof_preds['y_valid'].extend(y.iloc[idx_valid])\n    oof_preds['y_hat'].extend(model.predict(X_valid, batch_size=4096))\n    \n    prediction = model.predict(scl.transform(X_test), batch_size=4096) \n    predictions.append(prediction)\n    \n    del model, prediction\n    gc.collect()\n    K.clear_session()\n    \n    print('_'*65)\n    print(f\"Fold {fold+1} || Min Val Loss: {np.min(scores_nn[fold]['val_loss'])}\")\n    print('_'*65)\n    \nprint('_'*65)\noverall_score = [np.min(scores_nn[fold]['val_loss']) for fold in range(cv.n_splits)]\nprint(f\"Overall Mean Validation Loss: {np.mean(overall_score)}\")","8aadbf12":"plot_eval_results(scores_nn, cv.n_splits)","4ef0b8d7":"# prepare oof_predictions\noof_y_true = np.array(oof_preds['y_valid'])\noof_y_hat = le.inverse_transform(np.argmax(oof_preds['y_hat'], axis=1))\n\n# create confusion matrix, calculate accuracy, recall & precision\ncm = pd.DataFrame(data=confusion_matrix(oof_y_true, oof_y_hat, labels=le.classes_), index=le.classes_, columns=le.classes_)\nplot_cm(cm)","af913552":"#create final prediction, inverse labels to original classes\nfinal_predictions = le.inverse_transform(np.argmax(sum(predictions), axis=1))\n\nsample_submission['Cover_Type'] = final_predictions\nsample_submission.to_csv('.\/baseline_nn.csv', index=False)\n\nsns.countplot(final_predictions)\nsns.despine()","8b391cc9":"sample_submission.head()","4825342a":"<div style=\"font-size:110%\">\n<p>Feel free to take a look at my other notebooks, covering some different ideas and architectures:\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-simple-nn-baseline-keras\">Simple NN Baseline<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-deep-wide-nn-keras\">Deep & Wide NN <\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-deep-cross-nn-keras\">Deep & Cross NN<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-denoising-autoencoder-nn-keras\">Deepstack Denoising Autoencoder<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-bottleneck-autoencoder-nn-keras\">Bottleneck Autoencoder<\/a><\/li>\n<\/p>\n    \n<em>Thank you very much for taking some time to read my notebook. Please leave an upvote if you find any of this information useful.<\/em>\n<\/div>","65bf352e":"# Evaluation & Submission","79a50540":"# Introduction","84e941e3":"<img src=\"https:\/\/i.ibb.co\/PWvpT9F\/header.png\" alt=\"header\" border=\"0\" width=800 height=300>","c66fbaa0":"# Model Setup","10fb3dd7":"# Training","bc8321f6":"<div style=\"font-size:110%\"> \n<p>Hi,<\/p>\n<p>while I'm still enjoying my deep-learning adventure, reading and learning a ton of stuff - I decided it's time to implement different kinds of networks with this month competition. This Notebook is all about <b>Gated Residual (GRN)<\/b> and <b>Variable Selection Networks (VSN)<\/b> based on the <a href=\"https:\/\/keras.io\/examples\/structured_data\/classification_with_grn_and_vsn\/\">keras.io implementation<\/a>.<\/p>\n<p>The main idea here is to use the GRN's gating mechanism to (soft-) filter out less important features to use the network's learning capacity on the more salient features. The following steps can be used to describe the inner workings on a higher level and to provide some intuition:<\/p>\n<ol>\n    <li>Create a feature embedding (linear projection) as model input<\/li>\n    <li>Apply GRN to each feature individually (filter out less important imput per feature)<\/li>\n    <li>Apply GRN to concatenated features (get features weights, importance of one feature compared to the complete feature space<\/li>\n    <li>Create weighted sum of (2) & (3) as VSN output<\/li>\n    <li>Create final prediction with the usual dense(softmax) layer<\/li>\n<\/ol>\n<p><em>The drawing below might help as well.<\/em><\/p>\n<\/div>","435b734c":"<blockquote><img src=\"https:\/\/i.ibb.co\/sqhrnfV\/GRN-VSN.png\" alt=\"GRN-VSN\" border=\"0\">","1c946194":"# Import & Prepare Data"}}