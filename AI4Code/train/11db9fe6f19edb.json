{"cell_type":{"858eba8c":"code","3b7bec6e":"code","ced5bd17":"code","3214c455":"code","3d5283b2":"code","1a2f5e89":"code","68ba8382":"code","5615585a":"code","891b05b7":"code","2df2696e":"code","167c23eb":"code","1286c5d5":"code","cf9b22a5":"code","4291466c":"code","14ca625f":"code","284fdca5":"code","670baaf2":"code","6e0146f7":"code","0cf5ae26":"code","88c437a7":"code","40c6e72d":"code","5d3d6a74":"code","1d36de5d":"code","d24aec62":"code","b6d807bf":"code","34801cb4":"code","32ab76d9":"code","f7a59250":"code","453fd3c0":"code","21052e75":"code","60891ee8":"code","cc1af66f":"code","ad7e6bc6":"code","eaa49dad":"code","0d0d09b4":"code","e3840842":"code","fed61f55":"code","4e019bdc":"code","b4a96833":"code","f38d429f":"code","783c9458":"code","4bf50ef2":"code","5b088ee7":"code","fda4b31d":"code","5365d35b":"code","b307ebdf":"code","b9208f43":"code","46899e2c":"code","ef6c47d6":"code","cac4051d":"markdown","3c863b48":"markdown","dc124ccb":"markdown","363383ef":"markdown","1cad41ff":"markdown","64d138b6":"markdown","bddbb54d":"markdown","cd1bb36a":"markdown","7eedc59c":"markdown","6ba6a4e3":"markdown","10c5d67b":"markdown","dd8c466e":"markdown","91d195e9":"markdown","fd605ecc":"markdown","c1fd79c0":"markdown","e5d56877":"markdown","494503aa":"markdown","b8122e84":"markdown","d14ef9f3":"markdown","cab16999":"markdown","f800a7c5":"markdown","a3a88061":"markdown","913e4536":"markdown","d9eefd98":"markdown","d01f9e51":"markdown","6cb6f740":"markdown","cbf39e95":"markdown","6352b44c":"markdown","0302f19b":"markdown","658e7f72":"markdown","0cdb6cc1":"markdown","da253342":"markdown","ca794810":"markdown","2cf6601e":"markdown","d8c12566":"markdown","554f263d":"markdown"},"source":{"858eba8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3b7bec6e":"# We are reading our data\nfrom smart_open import open\n\ndf = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\") # train + test set\n\nwith open(\"https:\/\/gretel-public-website.s3-us-west-2.amazonaws.com\/datasets\/uci-heart-disease\/synthetic_train_shuffled.csv\", \"r\") as f:\n    dfs = pd.read_csv(f) # train set + newly generated synth data\nwith open(\"https:\/\/gretel-public-website.s3-us-west-2.amazonaws.com\/datasets\/uci-heart-disease\/heart_synthetic.csv\", \"r\") as f:\n    dft = pd.read_csv(f) # newly generated synth data only\n\n# GRETEL: Split data to create train (+synthetics) and test split\n# used in original notebook example\n#from sklearn.model_selection import train_test_split\n#train, test = train_test_split(df, test_size=0.2)\n#train.to_csv('train.csv', index=False)\n#test.to_csv('test.csv', index=False)","ced5bd17":"# First 5 rows of the original training data\ndf.head()","3214c455":"# First 5 rows of the original training data + synthetic records\ndfs.head()","3d5283b2":"df.target.value_counts()","1a2f5e89":"df.sex.value_counts()","68ba8382":"sns.countplot(x=\"target\", data=df, palette=\"bwr\")\nplt.show()","5615585a":"countNoDisease = len(df[df.target == 0])\ncountHaveDisease = len(df[df.target == 1])\nprint(\"Percentage of Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease \/ (len(df.target))*100)))\nprint(\"Percentage of Patients Have Heart Disease: {:.2f}%\".format((countHaveDisease \/ (len(df.target))*100)))","891b05b7":"sns.countplot(x='sex', data=df, palette=\"mako_r\").set_title(\"Training distribution by Gender (Original)\")\nplt.xlabel(\"Gender (0 = female, 1= male)\")\nplt.show()\ncountFemale = len(df[df.sex == 0])\ncountMale = len(df[df.sex == 1])","2df2696e":"countFemale = len(df[df.sex == 0])\ncountMale = len(df[df.sex == 1])\nprint(\"Percentage of Female Patients in original training dataset: {:.2f}%\".format((countFemale \/ (len(df.sex))*100)))\nprint(\"Percentage of Male Patients in original training dataset: {:.2f}%\".format((countMale \/ (len(df.sex))*100)))","167c23eb":"sns.countplot(x='sex', data=dfs.append(df, sort=False).append(dft, sort=False), palette=\"mako_r\").set_title(\"Training distribution by Gender (Original + Synthetic)\")\nplt.xlabel(\"Gender (0 = female, 1= male)\")\nplt.show()\ncountFemale = len(dfs[dfs.sex == 0])\ncountMale = len(dfs[dfs.sex == 1])","1286c5d5":"total_set = dfs.append(df, sort=False).append(dft, sort=False)\ncountFemale = len(total_set[total_set.sex == 0])\ncountMale = len(total_set[total_set.sex == 1])\nprint(\"Percentage of Female Patients in synthetic + training dataset: {:.2f}%\".format((countFemale \/ (len(total_set.sex))*100)))\nprint(\"Percentage of Male Patient in synthetic + training dataset: {:.2f}%\".format((countMale \/ (len(total_set.sex))*100)))","cf9b22a5":"df.groupby('target').mean()","4291466c":"pd.crosstab(df.age,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","14ca625f":"pd.crosstab(df.sex,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#1CA53B','#AA1111' ])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.xticks(rotation=0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","284fdca5":"plt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c=\"red\")\nplt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)])\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","670baaf2":"pd.crosstab(df.slope,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#DAF7A6','#FF5733' ])\nplt.title('Heart Disease Frequency for Slope')\nplt.xlabel('The Slope of The Peak Exercise ST Segment ')\nplt.xticks(rotation = 0)\nplt.ylabel('Frequency')\nplt.show()","6e0146f7":"pd.crosstab(df.fbs,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#FFC300','#581845' ])\nplt.title('Heart Disease Frequency According To FBS')\nplt.xlabel('FBS - (Fasting Blood Sugar > 120 mg\/dl) (1 = true; 0 = false)')\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","0cf5ae26":"pd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#11A5AA','#AA1190' ])\nplt.title('Heart Disease Frequency According To Chest Pain Type')\nplt.xlabel('Chest Pain Type')\nplt.xticks(rotation = 0)\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","88c437a7":"a = pd.get_dummies(df['cp'], prefix = \"cp\")\nb = pd.get_dummies(df['thal'], prefix = \"thal\")\nc = pd.get_dummies(df['slope'], prefix = \"slope\")","40c6e72d":"frames = [df, a, b, c]\ndf = pd.concat(frames, axis = 1)\ndf.head()\n\n#EDIT\na = pd.get_dummies(dfs['cp'], prefix = \"cp\")\nb = pd.get_dummies(dfs['thal'], prefix = \"thal\")\nc = pd.get_dummies(dfs['slope'], prefix = \"slope\")\nframes = [dfs, a, b, c]\ndfs = pd.concat(frames, axis = 1)\n#dfs.head()","5d3d6a74":"df = df.drop(columns = ['cp', 'thal', 'slope'])\ndf.head()\n\n#EDIT\ndfs = dfs.drop(columns = ['cp', 'thal', 'slope'])","1d36de5d":"y = df.target.values\nx_data = df.drop(['target'], axis = 1)\n\n# EDIT\ny_syn = dfs.target.values\nx_data_syn = dfs.drop(['target'], axis = 1)","d24aec62":"# Normalize\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)).values\n\n# Edit\nx_syn = (x_data_syn - np.min(x_data_syn)) \/ (np.max(x_data_syn) - np.min(x_data_syn)).values","b6d807bf":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)\n\n# Replace training set with train + synthetics\nx_train = x_syn\ny_train = y_syn","34801cb4":"#transpose matrices\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T","32ab76d9":"x_train","f7a59250":"#initialize\ndef initialize(dimension):\n    \n    weight = np.full((dimension,1),0.01)\n    bias = 0.0\n    return weight,bias","453fd3c0":"def sigmoid(z):\n    \n    y_head = 1\/(1+ np.exp(-z))\n    return y_head","21052e75":"def forwardBackward(weight,bias,x_train,y_train):\n    # Forward\n    \n    y_head = sigmoid(np.dot(weight.T,x_train) + bias)\n    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss) \/ x_train.shape[1]\n    \n    # Backward\n    derivative_weight = np.dot(x_train,((y_head-y_train).T))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"Derivative Weight\" : derivative_weight, \"Derivative Bias\" : derivative_bias}\n    \n    return cost,gradients","60891ee8":"def update(weight,bias,x_train,y_train,learningRate,iteration) :\n    costList = []\n    index = []\n    \n    #for each iteration, update weight and bias values\n    for i in range(iteration):\n        cost,gradients = forwardBackward(weight,bias,x_train,y_train)\n        weight = weight - learningRate * gradients[\"Derivative Weight\"]\n        bias = bias - learningRate * gradients[\"Derivative Bias\"]\n        \n        costList.append(cost)\n        index.append(i)\n\n    parameters = {\"weight\": weight,\"bias\": bias}\n    \n    print(\"iteration:\",iteration)\n    print(\"cost:\",cost)\n\n    plt.plot(index,costList)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n\n    return parameters, gradients","cc1af66f":"def predict(weight,bias,x_test):\n    z = np.dot(weight.T,x_test) + bias\n    y_head = sigmoid(z)\n\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(y_head.shape[1]):\n        if y_head[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","ad7e6bc6":"def logistic_regression(x_train,y_train,x_test,y_test,learningRate,iteration):\n    dimension = x_train.shape[0]\n    weight,bias = initialize(dimension)\n    \n    parameters, gradients = update(weight,bias,x_train,y_train,learningRate,iteration)\n\n    y_prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    print(\"Manual Test Accuracy: {:.2f}%\".format((100 - np.mean(np.abs(y_prediction - y_test))*100)))","eaa49dad":"logistic_regression(x_train,y_train,x_test,y_test,1,100)","0d0d09b4":"accuracies = {}\n\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nacc = lr.score(x_test.T,y_test.T)*100\n\naccuracies['Logistic Regression'] = acc\nprint(\"Test Accuracy {:.2f}%\".format(acc))","e3840842":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(x_train.T, y_train.T)\nprediction = knn.predict(x_test.T)\n\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(x_test.T, y_test.T)*100))","fed61f55":"# try ro find best k value\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(x_train.T, y_train.T)\n    scoreList.append(knn2.score(x_test.T, y_test.T))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\naccuracies['KNN'] = acc\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","4e019bdc":"from sklearn.svm import SVC","b4a96833":"svm = SVC(random_state = 1)\nsvm.fit(x_train.T, y_train.T)\n\nacc = svm.score(x_test.T,y_test.T)*100\naccuracies['SVM'] = acc\nprint(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(acc))","f38d429f":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train.T, y_train.T)\n\nacc = nb.score(x_test.T,y_test.T)*100\naccuracies['Naive Bayes'] = acc\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))","783c9458":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train.T, y_train.T)\n\nacc = dtc.score(x_test.T, y_test.T)*100\naccuracies['Decision Tree'] = acc\nprint(\"Decision Tree Test Accuracy {:.2f}%\".format(acc))","4bf50ef2":"# Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\nrf.fit(x_train.T, y_train.T)\n\nacc = rf.score(x_test.T,y_test.T)*100\naccuracies['Random Forest'] = acc\nprint(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(acc))","5b088ee7":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","fda4b31d":"import seaborn as sns\nimport matplotlib.pyplot as pyplot\n\n# Notebook baseline accuracies (without synthetics)\nbaseline_accuracies = {'Logistic Regression': 86.88524590163934,\n 'KNN': 88.52459016393442,\n 'SVM': 86.88524590163934,\n 'Naive Bayes': 86.88524590163934,\n 'Decision Tree': 77.04918032786885,\n 'Random Forest': 88.52459016393442} \n\ncomparison = pd.DataFrame([baseline_accuracies, accuracies], index=['Baseline', 'Synthetics'])\nfig = plt.figure(figsize=(10, 6)) # Create matplotlib figure\nax = fig.add_subplot(111) # Create matplotlib axes\ncomparison.T.plot(kind='bar', ax=ax, position=0, legend=True).set_title(\"Synthetics + Train vs. Train data\")\nplt.show()\n","5365d35b":"comparison","b307ebdf":"synth_avg_score = np.average([float(x) for x in accuracies.values()])\norig_avg_score = np.average([float(x) for x in baseline_accuracies.values()])\nprint(f\"Synthetics average accuracy: {synth_avg_score:.2f}%, original: \"\n      f\"{orig_avg_score:.2f}%. Improvement: {synth_avg_score - orig_avg_score:.2f}%\")","b9208f43":"# Predicted values\ny_head_lr = lr.predict(x_test.T)\nknn3 = KNeighborsClassifier(n_neighbors = 3)\nknn3.fit(x_train.T, y_train.T)\ny_head_knn = knn3.predict(x_test.T)\ny_head_svm = svm.predict(x_test.T)\ny_head_nb = nb.predict(x_test.T)\ny_head_dtc = dtc.predict(x_test.T)\ny_head_rf = rf.predict(x_test.T)","46899e2c":"from sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(y_test,y_head_lr)\ncm_knn = confusion_matrix(y_test,y_head_knn)\ncm_svm = confusion_matrix(y_test,y_head_svm)\ncm_nb = confusion_matrix(y_test,y_head_nb)\ncm_dtc = confusion_matrix(y_test,y_head_dtc)\ncm_rf = confusion_matrix(y_test,y_head_rf)\n","ef6c47d6":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.show()","cac4051d":"### Creating Dummy Variables","3c863b48":"**Naive Bayes Algorithm**\n<br>\n<img src=\"https:\/\/s3.ap-south-1.amazonaws.com\/techleer\/204.png\" width=\"500px\"\/>","dc124ccb":"## Comparing Models","363383ef":"# INTRODUCTION\n<br>\nWe have a data which classified if patients have heart disease or not according to features in it. We will try to use this data to create a model which tries predict if a patient has this disease or not. We will use logistic regression (classification) algorithm.","1cad41ff":"### Gradient Descent\n<br>\n<img src=\"https:\/\/i.stack.imgur.com\/pYVzl.png\" width=\"500px\"\/>","64d138b6":"## <font color=\"#CFC60E\">Test Accuracy of Decision Tree Algorithm: <font color=\"red\">78.69%<\/font><\/font>","bddbb54d":"## Random Forest Classification","cd1bb36a":"We will split our data. 80% of our data will be train data and 20% of it will be test data.","7eedc59c":"**Support Vector Machine Algorithm**\n<br>\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*TudH6YvvH7-h5ZyF2dJV2w.jpeg\" width=\"500px\"\/><br>\n<img src=\"https:\/\/aitrends.com\/wp-content\/uploads\/2018\/01\/1-19SVM-2.jpg\" width=\"500px\"\/>\n","6ba6a4e3":"## Creating Model for Logistic Regression\n<br>\nWe can use sklearn library or we can write functions ourselves. Let's them both. Firstly we will write our functions after that we'll use sklearn library to calculate score.","10c5d67b":"Let's say weight = 0.01 and bias = 0.0","dd8c466e":"By the way in formulas; <br>\n* h0(x^i)= y_head\n* y^i = y_train\n* x^i = x_train","91d195e9":"## Confusion Matrix","fd605ecc":"Our models work fine but best of them are KNN and Random Forest with 88.52% of accuracy. Let's look their confusion matrixes.","c1fd79c0":"## Data Exploration","e5d56877":"As you can see above if we define k as 3-7-8 we will reach maximum score. <br>\n## <font color=\"green\">KNN Model's Accuracy is <font color=\"red\">88.52%<\/font><\/font>","494503aa":"![](http:\/\/)Since 'cp', 'thal' and 'slope' are categorical variables we'll turn them into dummy variables.","b8122e84":"## <font color=\"orange\">Test Accuracy of SVM Algorithm is <font color=\"red\"> 86.89%<\/font><\/font>","d14ef9f3":"## <font color=\"magenta\">Accuracy of Naive Bayes: <font color=\"red\">86.89%<\/font><\/font>","cab16999":"Data contains; <br>\n\n* age - age in years <br>\n* sex - (1 = male; 0 = female) <br>\n* cp - chest pain type <br>\n* trestbps - resting blood pressure (in mm Hg on admission to the hospital) <br>\n* chol - serum cholestoral in mg\/dl <br>\n* fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) <br>\n* restecg - resting electrocardiographic results <br>\n* thalach - maximum heart rate achieved <br>\n* exang - exercise induced angina (1 = yes; 0 = no) <br>\n* oldpeak - ST depression induced by exercise relative to rest <br>\n* slope - the slope of the peak exercise ST segment <br>\n* ca - number of major vessels (0-3) colored by flourosopy <br>\n* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect <br>\n* target - have disease or not (1=yes, 0=no)","f800a7c5":"1. ## <font color = \"purple\">Our model works with <font color=\"red\">**86.89%**<\/font> accuracy.<\/font>","a3a88061":"## Support Vector Machine (SVM) Algorithm \nNow we will use SVM algorithm. ","913e4536":"**Decision Tree Algorithm**\n<br>\n![image.png](attachment:image.png)","d9eefd98":"**I am new with data science. Please comment me your feedbacks to help me improve myself. Thanks for your time.**","d01f9e51":"## <font color=\"#0FBBAE\">Test Accuracy of Random Forest: <font color=\"red\">88.52%<\/font><\/font>","6cb6f740":"### Normalize Data\n<br>\n<br>\n<img src=\"https:\/\/beyondbacktesting.files.wordpress.com\/2017\/07\/normalization.png?w=863\" width=\"400px\"\/>","cbf39e95":"## Naive Bayes Algorithm","6352b44c":"## <font color=\"blue\">Manual Test Accuracy is <font color=\"red\">**86.89%**<\/font><\/font><br>\nLet's find out sklearn's score.","0302f19b":"## Decision Tree Algorithm","658e7f72":"### Forward and Backward Propagation\n<br>\n<img src=\"https:\/\/image.slidesharecdn.com\/gradientdescentbackpropandautomaticdifferentiation-160829164205\/95\/gradient-descent-back-propagation-and-auto-differentiation-advanced-spark-and-tensorflow-meetup-08042016-62-638.jpg?cb=1472489358\" width=\"500px\"\/>","0cdb6cc1":"### Sigmoid Function\n<br>\n<img src=\"https:\/\/qph.fs.quoracdn.net\/main-qimg-05edc1873d0103e36064862a45566dba\" width=\"500px\"\/>","da253342":"**KNN Algorithm**\n<br>\n<img src=\"http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/KNN_final_a1mrv9.png\"\/>","ca794810":"### Sklearn Logistic Regression","2cf6601e":"## Read Data","d8c12566":"### Cost Function\n<br>\n<img src=\"https:\/\/i.stack.imgur.com\/XbU4S.png\" width=\"500px\"\/>","554f263d":"## K-Nearest Neighbour (KNN) Classification\n<br>\nLet's see what will be score if we use KNN algorithm."}}