{"cell_type":{"ab6074b1":"code","8e78cbfb":"code","64635027":"code","9de17d96":"code","df402583":"code","1e899262":"code","8e46d82f":"code","8696ae4a":"code","101d9e48":"code","aa2d5a9f":"code","193438af":"code","bc6d88f7":"code","f0fddb7b":"code","56edd5cd":"code","3af57f27":"code","eea5b7c9":"code","72ae7c5e":"code","70c6d78f":"code","84b68e76":"code","1a6af153":"code","b28b280e":"code","53070b11":"code","4683c9de":"markdown","cbd9ae5b":"markdown","622b3ded":"markdown","80b0abfd":"markdown","eef2141e":"markdown","780d7af4":"markdown","6b1ac95f":"markdown","49db9f4f":"markdown","d881459a":"markdown","da4eb60b":"markdown","c0e10af3":"markdown","7aafc725":"markdown","8830216c":"markdown","409c309c":"markdown","3ba23264":"markdown","3a59f2ad":"markdown","7fd931d9":"markdown","4ee42899":"markdown","231862c3":"markdown"},"source":{"ab6074b1":"from __future__ import print_function\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport seaborn as sns\nimport gc\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom scipy.stats import norm, rankdata\n\nimport keras\nfrom keras import regularizers\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add,PReLU, LSTM\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nimport tensorflow as tf","8e78cbfb":"# reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","64635027":"train = reduce_mem_usage(pd.read_csv('..\/input\/train.csv'))\ntest = reduce_mem_usage(pd.read_csv('..\/input\/test.csv'))","9de17d96":"features = [f for f in train if f not in ['ID_code','target']]","df402583":"df_original = pd.concat([train, test],axis=0,sort=False)\ndf = df_original[features]\ntarget = df_original['target'].values\nid = df_original['ID_code']","1e899262":"from scipy.special import erfinv\ntrafo_columns = [c for c in df.columns if len(df[c].unique()) != 2]\nfor col in trafo_columns:\n    values = sorted(set(df[col]))\n    # Because erfinv(1) is inf, we shrink the range into (-0.9, 0.9)\n    f = pd.Series(np.linspace(-0.9, 0.9, len(values)), index=values)\n    f = np.sqrt(2) * erfinv(f)\n    f -= f.mean()\n    df[col] = df[col].map(f)","8e46d82f":"# define roc_callback, inspired by https:\/\/github.com\/keras-team\/keras\/issues\/6050#issuecomment-329996505\ndef auc_roc(y_true, y_pred):\n    # any tensorflow metric\n    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n\n    # find all variables created for this metric\n    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('\/')[1]]\n\n    # Add metric variables to GLOBAL_VARIABLES collection.\n    # They will be initialized for new session.\n    for v in metric_vars:\n        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n\n    # force to update metric values\n    with tf.control_dependencies([update_op]):\n        value = tf.identity(value)\n        return value","8696ae4a":"from keras.callbacks import LearningRateScheduler\nimport math\nfrom math import exp\nfrom math import ceil\n\ndef step_decay(epoch):\n    initial_lrate = 0.1\n    drop = 0.5\n    epochs_drop = 5.0\n    lrate = initial_lrate * math.pow(drop,  \n           math.floor((1+epoch)\/epochs_drop))\n    return lrate\n        \ndef exp_decay(epoch):\n    initial_lrate = 0.1\n    k = 0.1\n    t = epoch\n    lrate = initial_lrate * exp(-k*t)\n    return lrate\n\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.lr = []\n \n    def on_epoch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n#        self.lr.append(exp_decay(len(self.losses)))\n        self.lr.append(step_decay(len(self.losses)))","101d9e48":"lrate = LearningRateScheduler(step_decay)\n#lrate = LearningRateScheduler(exp_decay)\nao = ModelCheckpoint(filepath=\"auto_0.h5\",save_best_only=True,verbose=0)\nnn = ModelCheckpoint(filepath=\"nn_0.h5\",save_best_only=True,verbose=0)\ntb = TensorBoard(log_dir='.\/logs',histogram_freq=0,write_graph=True,write_images=True)\nrl = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1)\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\nloss_history = LossHistory()","aa2d5a9f":"from keras import backend as K\nfrom keras.activations import elu\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.models import Model\nfrom keras.objectives import binary_crossentropy\nfrom keras.callbacks import LearningRateScheduler\nfrom keras import backend as K\nfrom imblearn.keras import balanced_batch_generator\nfrom imblearn.under_sampling import NearMiss, RandomUnderSampler, CondensedNearestNeighbour, AllKNN, InstanceHardnessThreshold\nfrom sklearn.model_selection import KFold\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\nfrom keras.utils import multi_gpu_model\nimport math\n\nverbose = 10\nlearning_rate = 0.0003\nnb_epoch = int(3)\ndcy = learning_rate \/ nb_epoch\nbatch_size = 256\nencoding_dim =400\nhidden_dim = int(encoding_dim*2) #i.e. 7\npredictions = np.zeros(len(df))\nlabel_cols = [\"target\"]\nopt = keras.optimizers.SGD(lr=learning_rate, decay=dcy, nesterov=False)\n\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(series.shape[1]))\n\ntrn_data, val_data = train_test_split(df[trafo_columns], test_size=0.3)\nnoisy_trn_data = add_noise(trn_data, 0.07)\ninput_dim = noisy_trn_data.shape[1] #num of columns\n\nwith tf.device('\/cpu:0'):\n    input_layer = Input(shape=(input_dim, ))\n\n    x = Dense(hidden_dim, activation=\"relu\", name=\"first\", init='identity')(input_layer)\n    x = Dense(hidden_dim, activation=\"relu\", name='second')(x)\n    x = BatchNormalization()(x)\n    x = Dense(hidden_dim, activation=\"relu\", name='third')(x)\n\n    output_layer = Dense(input_dim, activation=\"linear\")(x)\n    autoencoder = Model(inputs=input_layer, outputs=output_layer)    \n    autoencoder.summary()\n    \nautoencoder.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer=opt)","193438af":"with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    history = autoencoder.fit(noisy_trn_data, trn_data,\n                        epochs=nb_epoch,\n                        batch_size=batch_size,\n                        shuffle=True,\n                        validation_data=(val_data, val_data),\n                        verbose=1,\n                        callbacks=[ao,tb,es,loss_history,lrate])\n","bc6d88f7":"# we build a new model with the activations of the old model\n# this model is truncated after the first layer\n\n#second_hidden_layer = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('second').output)\nthird_hidden_layer = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('third').output)\n\n#print(second_hidden_layer.summary())\nprint(third_hidden_layer.summary())\n","f0fddb7b":"with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n#    get_2nd_hidden_layer = second_hidden_layer.predict(df)\n    get_3rd_hidden_layer = third_hidden_layer.predict(df)\n    \n#print(get_2nd_hidden_layer.shape)\nprint(get_3rd_hidden_layer.shape)","56edd5cd":"del df_original, df, noisy_trn_data, test, train, trn_data, val_data\ngc.collect()","3af57f27":"#layer_output_2 = reduce_mem_usage(pd.DataFrame(get_2nd_hidden_layer))\nlayer_output_3 = reduce_mem_usage(pd.DataFrame(get_3rd_hidden_layer))","eea5b7c9":"#hidden = np.concatenate([layer_output_2, layer_output_3], axis=1)\n#hidden = pd.DataFrame(hidden)\nhidden = pd.DataFrame(layer_output_3)\nprint(hidden.shape)\n\n#del layer_output_2\ndel layer_output_3\ngc.collect","72ae7c5e":"#hidden\nhidden['target'] = target\nhidden['ID_code'] = id.values\nprint(hidden.head(5))","70c6d78f":"#find kernal here https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/82863\nfeatures = [f for f in hidden if f not in ['ID_code','target']]\ntrain = hidden[hidden['target'].notnull()]\ntest = hidden[hidden['target'].isnull()]\ntest = np.reshape(test[features].values, (-1,test[features].shape[1], 1))\n\nlen_input_columns, len_data = train.shape[1], train.shape[0]\n\nprint(train.shape)\nprint(test.shape)\nprint(train.head(5))","84b68e76":"predictions = np.zeros(shape=(len(test), 1))\nlabel_cols = [\"target\"]\ntrain_x, valid_x, train_y, valid_y = train_test_split(train[features], train['target'], test_size=0.3)","1a6af153":"with tf.device('\/cpu:0'):\n\n    input_dim = train_x.shape[1] #num of columns, 4500\n    input_layer = Input(shape=(input_dim, 1))\n\n    x = Dense(hidden_dim, activation='relu')(input_layer)\n    x = Flatten()(x)\n\n\n    output_layer = Dense(1, activation='sigmoid')(x)\n    model= Model(inputs=input_layer, outputs=output_layer)\n    model.summary()\n\nopt = keras.optimizers.SGD(lr=learning_rate, decay=dcy, nesterov=False)\nmodel.compile(metrics=['accuracy', auc_roc],\n                    loss='binary_crossentropy',\n                    optimizer=opt)","b28b280e":"pos = (pd.Series(train_y == 1))\n\n# Add positive examples\ntrain_x_x = pd.concat([train_x, train_x.loc[pos]], axis=0)\ntrain_y_y = pd.concat([train_y, train_y.loc[pos]], axis=0)\n\n# Shuffle data\nidx = np.arange(len(train_x_x))\nnp.random.shuffle(idx)\ntrain_x_x = train_x_x.iloc[idx]\ntrain_y_y = train_y_y.iloc[idx]\n\ntrain_x_x = np.reshape(train_x.values, (-1, train_x.shape[1], 1))\nvalid_x_x = np.reshape(valid_x.values, (-1, valid_x.shape[1], 1))","53070b11":"history = model.fit(train_x_x, train_y,\n                    epochs=1,\n                    batch_size=int(128),\n                    shuffle=True,\n                    validation_data=(valid_x_x, valid_y),\n                    verbose=1,\n                    callbacks=[nn,tb,es,loss_history,lrate])\n\npredictions = model.predict(test)","4683c9de":"Split into train\/valid","cbd9ae5b":"Build NN","622b3ded":"Extract Hidden Layer Values","80b0abfd":"Just like always, import that packages and load the data","eef2141e":"Clean up","780d7af4":"Define all callbacks","6b1ac95f":"Attach the original label and target","49db9f4f":"Build AE","d881459a":"Create Learning Rate Scheduler","da4eb60b":"Grab the features, merge the data, and transform.  I know your suppose to keep train\/test seperate but I have no shot at winning and this is faster anyways.","c0e10af3":"Create Hidden Layer Model","7aafc725":"Append layers into new DF","8830216c":"This is my take on a DAE with a NN classifier.  I didn't recreate my noise like Jahrer did just some standard random sampling; although I did try to recreate his code myself.  Never could get it to work as well as his did.  \n\nAnyways I hope you guys find it as interesting as I do!  I was able to achieve a 0.89 score with something similar a while back but I broke that code and I can't get it to run anymore.  \n\nThe only boost I have gotten from from the features was with a guassian transform, but nothing else seemed to help. Oh and I tried to upsampl using Oliver's approach, but I'm undecided if it helps here or not.\n\nHonestly my gut is telling me that I just need to build a NN with like 30 layers and move on to the next competition.  \n\nHere we go!","409c309c":"Thanks for reading!","3ba23264":"Reshape Data and set shape values.  FYI I grabbed Janek's 3D take on the NN; from his post: \n> NN - why input shape matters?","3a59f2ad":"Fit AE","7fd931d9":"Upsample Data","4ee42899":"Train NN and predict","231862c3":"Create AUC_ROC callback "}}