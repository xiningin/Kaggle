{"cell_type":{"514f7d5c":"code","ce05b225":"code","5b99a6cf":"code","295aa036":"code","c06a4210":"code","b671deff":"code","24470e69":"code","20bd479a":"code","f8774185":"code","e8792592":"code","de37b8ca":"code","5c6b780d":"code","218b97e8":"code","1847fbdc":"code","40d2ab52":"code","e096378a":"code","e7c645a3":"code","ee105ccd":"code","b7b314cc":"code","690dcce7":"code","806427a1":"markdown","95848dbd":"markdown","1dd9aa4b":"markdown","724a7e7e":"markdown"},"source":{"514f7d5c":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\nfrom sklearn.metrics import recall_score,accuracy_score,confusion_matrix, f1_score, precision_score, auc,roc_auc_score,roc_curve, precision_recall_curve\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.under_sampling import ClusterCentroids,NearMiss, RandomUnderSampler\nfrom imblearn.combine import SMOTEENN,SMOTETomek\nfrom imblearn.ensemble import BalanceCascade\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport collections\nimport imblearn\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import make_scorer, f1_score\nfrom sklearn import model_selection\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","ce05b225":"import pandas as pd\ndata = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","5b99a6cf":"X = data.iloc[:, data.columns != 'Class']\ny = data.iloc[:, data.columns == 'Class']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)\nprint(X_train.shape)\nprint(X_test.shape)","295aa036":"# Scaling the Time and Amount features\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndata['scaled_amount'] = std_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = std_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n\ndata.drop(['Time','Amount'], axis=1, inplace=True)\nscaled_amount = data['scaled_amount']\nscaled_time = data['scaled_time']\n\ndata.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndata.insert(0, 'scaled_amount', scaled_amount)\ndata.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndata.head()","c06a4210":"# To improve the accuracy of the model, we can remove those features that are highly\n# correlated with the class and are extreme outliers. We can change the threshold \n# to detect the outliers\nfrom scipy.stats import norm\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nv14_fraud_dist = data['V14'].loc[data['Class'] == 1].values\nsns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nv12_fraud_dist = data['V12'].loc[data['Class'] == 1].values\nsns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n\n\nv17_fraud_dist = data['V17'].loc[data['Class'] == 1].values\nsns.distplot(v17_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V17 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nplt.show()","b671deff":"# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = data['V14'].loc[data['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\ndata = data.drop(data[(data['V14'] > v14_upper) | (data['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv12_fraud = data['V12'].loc[data['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\ndata = data.drop(data[(data['V12'] > v12_upper) | (data['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(data)))\nprint('----' * 44)\n\n\n# Removing outliers V17 Feature\nv17_fraud = data['V17'].loc[data['Class'] == 1].values\nq25, q75 = np.percentile(v17_fraud, 25), np.percentile(v17_fraud, 75)\nv17_iqr = q75 - q25\n\nv17_cut_off = v17_iqr * 1.5\nv17_lower, v17_upper = q25 - v17_cut_off, q75 + v17_cut_off\nprint('V17 Lower: {}'.format(v17_lower))\nprint('V17 Upper: {}'.format(v17_upper))\noutliers = [x for x in v17_fraud if x < v17_lower or x > v17_upper]\nprint('V17 outliers: {}'.format(outliers))\nprint('Feature V17 Outliers for Fraud Cases: {}'.format(len(outliers)))\ndata = data.drop(data[(data['V17'] > v17_upper) | (data['V17'] < v17_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(data)))\n\n# Boxplots with outliers removed\n\nf,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=data,ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\nax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature 12\nsns.boxplot(x=\"Class\", y=\"V12\", data=data, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\nax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature V17\nsns.boxplot(x=\"Class\", y=\"V17\", data=data, ax=ax3, palette=colors)\nax3.set_title(\"V17 Feature \\n Reduction of outliers\", fontsize=14)\nax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n\nplt.show()","24470e69":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Frauds', round(data['Class'].value_counts()[0]\/len(data) * 100,2), '% of the dataset')\nprint('Frauds', round(data['Class'].value_counts()[1]\/len(data) * 100,2), '% of the dataset')\n\nX = data.drop('Class', axis=1)\ny = data['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","20bd479a":"import itertools\n\n# Create a confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","f8774185":"sm = SMOTEENN( random_state=42)\nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)\n\nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)","e8792592":"# I skip cross validation bcs of long run time\n\n\n\n# Logistic Regression\nlr_sm = LogisticRegression()\n\nlr_sm.fit(Xsm_train, ysm_train)\n# Prediction for Logistic Regression with SMOTEE\nlabels = ['No Fraud', 'Fraud']\nsmotee_prediction_lr = lr_sm.predict(original_Xtest)\nprint(classification_report(original_ytest, smotee_prediction_lr, target_names=labels))","de37b8ca":"from sklearn.metrics import confusion_matrix\n\nlr_cf = confusion_matrix(original_ytest, smotee_prediction_lr)\nplot_confusion_matrix(lr_cf, labels, title=\"lr \\n Confusion Matrix\")","5c6b780d":"# I skip cross validation bcs of long run time\n\n\n\n# KNN\nKNN_sm = KNeighborsClassifier()\n\nKNN_sm.fit(Xsm_train, ysm_train)\n# Prediction for KNN with SMOTEE\nlabels = ['No Fraud', 'Fraud']\nsmotee_prediction_KNN = KNN_sm.predict(original_Xtest)\nprint(classification_report(original_ytest, smotee_prediction_KNN, target_names=labels))","218b97e8":"from sklearn.metrics import confusion_matrix\n\nKNN_cf = confusion_matrix(original_ytest, smotee_prediction_KNN)\nplot_confusion_matrix(KNN_cf, labels, title=\"KNN \\n Confusion Matrix\")","1847fbdc":"# I skip cross validation bcs of long run time\n\n\n# SVM\nSVM_sm = SVC()\n\nSVM_sm.fit(Xsm_train, ysm_train)\n# Prediction for SVM with SMOTEE\nlabels = ['No Fraud', 'Fraud']\nsmotee_prediction_SVM = SVM_sm.predict(original_Xtest)\nprint(classification_report(original_ytest, smotee_prediction_SVM, target_names=labels))","40d2ab52":"from sklearn.metrics import confusion_matrix\n\nSVM_cf = confusion_matrix(original_ytest, smotee_prediction_SVM)\nplot_confusion_matrix(SVM_cf, labels, title=\"SVM \\n Confusion Matrix\")","e096378a":"# I skip cross validation bcs of long run time\n\n\n# RF\nRF_sm = RandomForestClassifier()\nRF_sm.fit(Xsm_train, ysm_train)\n# Prediction for RF with SMOTEE\nlabels = ['No Fraud', 'Fraud']\nsmotee_prediction_RF = RF_sm.predict(original_Xtest)\nprint(classification_report(original_ytest, smotee_prediction_RF, target_names=labels))","e7c645a3":"from sklearn.metrics import confusion_matrix\n\nRF_cf = confusion_matrix(original_ytest, smotee_prediction_RF)\nplot_confusion_matrix(RF_cf, labels, title=\"RF \\n Confusion Matrix\")","ee105ccd":"# I skip cross validation bcs of long run time\n\n\n# XGB\nxgb_sm = XGBClassifier()\n\nxgb_sm.fit(Xsm_train, ysm_train)\n# Prediction for xgb with SMOTEE\nlabels = ['No Fraud', 'Fraud']\nsmotee_prediction_xgb = xgb_sm.predict(original_Xtest)\nprint(classification_report(original_ytest, smotee_prediction_xgb, target_names=labels))","b7b314cc":"from sklearn.metrics import confusion_matrix\n\nxgb_cf = confusion_matrix(original_ytest, smotee_prediction_xgb)\nplot_confusion_matrix(xgb_cf, labels, title=\"XGB \\n Confusion Matrix\")","690dcce7":"# Neural Networks\n\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.layers import Dropout\n\nn_inputs = Xsm_train.shape[1]\n\nSMOTEENN_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])\nSMOTEENN_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nSMOTEENN_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)\nSMOTEENN_predictions = SMOTEENN_model.predict(original_Xtest, batch_size=200, verbose=0)\nSMOTEENN_fraud_predictions = SMOTEENN_model.predict_classes(original_Xtest, batch_size=200, verbose=0)\n\nSMOTEENN_smote = confusion_matrix(original_ytest, SMOTEENN_fraud_predictions)\nactual_cm = confusion_matrix(original_ytest, original_ytest)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(SMOTEENN_smote, labels, title=\"Hybrid(SMOTEEN) \\n Confusion Matrix\", cmap=plt.cm.Oranges)\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)    \n\nprint('Neural Networks with Hybrid sampling:')\nprint(classification_report(original_ytest, SMOTEENN_fraud_predictions))","806427a1":"## Setup imports","95848dbd":"## Load the dataset","1dd9aa4b":"# Credit Card Fraud Detection with hybrid resampling\n\n\n\n## Pre-processing techniques\n\n Combined over and under sampling\n    * SMOTEENN\n\n","724a7e7e":"Inspired by: [https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets]"}}