{"cell_type":{"ebfa94bf":"code","137f1c1d":"code","f7f24ccf":"code","f118752d":"code","80f9ee8e":"code","f603b7fd":"code","7fef37b8":"code","27e6e64e":"code","b70d85f0":"code","e8ba9329":"code","a49ca487":"code","d09caa3b":"code","4881d5f4":"code","fca6e596":"code","a59838d2":"code","c1abc0f3":"code","9bde3d78":"code","66bfc26b":"code","4136fc2d":"code","41932701":"code","0f5746de":"code","7da0946b":"code","15fbf40c":"code","f7210cf4":"code","9795f66e":"code","1e6eef22":"code","d85cab7f":"code","748ebacd":"code","9794b4d5":"code","b5813189":"code","dcceab86":"code","235a9a57":"code","29cfc801":"code","812e87fb":"code","1344786e":"code","9a01f4ea":"code","a8b7c340":"code","5a889f6e":"code","25b8c089":"code","a064845d":"markdown","c4bd0cfa":"markdown","828587b8":"markdown","22aba54b":"markdown","94a68f9f":"markdown","6d359402":"markdown","acd53335":"markdown","9862611b":"markdown","b6272ada":"markdown","a1a78b96":"markdown","1abd0004":"markdown","f96826ad":"markdown","ed31256f":"markdown","70f8bd92":"markdown","933e6838":"markdown","11f71e4a":"markdown","624bf259":"markdown","8f2d4cd5":"markdown","f5d4895f":"markdown","0989cf06":"markdown","839e4b28":"markdown","62cb3f1e":"markdown","6426e95e":"markdown","e2cf65c8":"markdown","70ae8722":"markdown","e309226f":"markdown","080521eb":"markdown","e32d0857":"markdown","fef17fa3":"markdown","58c717bf":"markdown","d0bcff9a":"markdown"},"source":{"ebfa94bf":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","137f1c1d":"df= pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf.head()\n","f7f24ccf":"df.shape","f118752d":"df.info()","80f9ee8e":"df.describe()","f603b7fd":"features_with_na=[features for features in df.columns if df[features].isnull().sum()>1]\n## 2- step print the feature name and the percentage of missing values\nfeatures_with_na","7fef37b8":"numerical_features = [feature for feature in df.columns if df[feature].dtypes != 'O']\nlen(numerical_features),df.shape","27e6e64e":"discrete_feature=[feature for feature in numerical_features if len(df[feature].unique())<25]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","b70d85f0":"discrete_feature","e8ba9329":"for feature in discrete_feature:\n    data=df.copy()\n    data.groupby(feature)['output'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('output')\n    plt.title(feature)\n    plt.show()","a49ca487":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","d09caa3b":"for feature in continuous_feature:\n    data=df.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","4881d5f4":"fig, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True,ax=ax)","fca6e596":"sns.displot(x='age', hue='output', data=df, alpha=0.6)\nplt.show()","a59838d2":"attack = df[df['output']==1]\nsns.displot(attack.age, kind='kde')\nplt.show()","c1abc0f3":"sns.displot(attack.age, kind='ecdf')\nplt.grid(True)\nplt.show()","9bde3d78":"ranges = [0, 30, 40, 50, 60, 70, np.inf]\nlabels = ['0-30', '30-40', '40-50', '50-60', '60-70', '70+']\n\nattack['age'] = pd.cut(attack['age'], bins=ranges, labels=labels)\nattack['age'].head()","66bfc26b":"sns.countplot(attack.age)","4136fc2d":"fig, ax = plt.subplots(figsize=(8, 5))\nsns.countplot(x='sex', hue='age', data=attack, ax=ax)\n\n","41932701":"attack = df[df['output'] == 1]\nsns.displot(x='age', kind='kde', hue='sex', data=attack)\n","0f5746de":"male_attack=attack[attack['sex']==1]","7da0946b":"sns.countplot(male_attack['age'])","15fbf40c":"for feature in continuous_feature:\n    data=df.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","f7210cf4":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import  BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","9795f66e":"#Creating a copy\ndata= df","1e6eef22":"\nscaler = StandardScaler()\n\n# define the columns to be encoded and scaled\ncategorical_vars = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncontinuous_vars = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndata = pd.get_dummies(data, columns = categorical_vars, drop_first = True)\n\nX = data.drop(['output'],axis=1)\ny = data[['output']]\n\ndata[continuous_vars] = scaler.fit_transform(X[continuous_vars])\n\n# defining the features and target\nX = data.drop(['output'],axis=1)\ny = data[['output']]\n\n","d85cab7f":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)","748ebacd":"lr = LogisticRegression(random_state=42)\n\nknn = KNeighborsClassifier()\npara_knn = {'n_neighbors':np.arange(1, 50)}\n\ngrid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5)\n\ndt = DecisionTreeClassifier()\npara_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5)\n\nrf = RandomForestClassifier()\n\n# Define the dictionary 'params_rf'\nparams_rf = {\n    'n_estimators':[100, 350, 500],\n    'min_samples_leaf':[2, 10, 30]\n}\ngrid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)","9794b4d5":"dt = DecisionTreeClassifier(criterion='gini', max_depth=9, min_samples_leaf=10, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=2, random_state=42)","b5813189":"# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt), ('Random Forest', rf)]","dcceab86":"for clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_pred, y_test) \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))","235a9a57":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator=rf, n_estimators=100, random_state=1)\n\nada.fit(X_train, y_train)\n\ny_pred = ada.predict(X_test)\n\naccuracy_score(y_pred, y_test)","29cfc801":"importances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nplt.figure(figsize=(10, 10))\nimportances_sorted.plot(kind='bar',color='orange')\nplt.title('Features Importances')\nplt.show()","812e87fb":"from tensorflow.keras.layers import Dense,Dropout,Flatten\nfrom tensorflow.keras.layers import MaxPooling2D,GlobalAveragePooling2D,BatchNormalization,Activation\nfrom tensorflow import keras\nimport tensorflow as tf","1344786e":"\nmodel = tf.keras.Sequential()\nmodel.add(Dense(1024, input_dim=22, activation= \"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(512, activation= \"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128, activation= \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation= \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.summary() #Print model Summary","9a01f4ea":"model.compile(loss= \"binary_crossentropy\" , optimizer=\"adam\", metrics=[\"accuracy\"])","a8b7c340":"Performance = model.fit(X_train, y_train, validation_split =0.1,epochs=30)","5a889f6e":"model.evaluate(X_test,y_test)","25b8c089":"my_dpi = 50 # dots per inch .. (resolution)\nplt.figure(figsize=(400\/my_dpi, 400\/my_dpi), dpi = my_dpi)\nplt.plot(Performance.history['accuracy'], label='train accuracy')\nplt.plot(Performance.history['val_accuracy'], label='val accuracy')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')","a064845d":"**Results against the Age**","c4bd0cfa":"# Inference","828587b8":"**PREPARING THE DATASET FOR MODEL**","22aba54b":"1. **Understanding the Data**\n\n2. **EDA**\n\n3. **Model Building**\n\n4. **Model Performance**\n\n5. **Inference**\n","94a68f9f":"# **EDA**","6d359402":"**READING THE DATA**","acd53335":"**WE NOTICE THAT MALE HAVE A HIGHER TENDENCY TO HAVE HEART ATTACK**","9862611b":"**LET US FIND OUT THE RELATION BETWEEN EACH OF THE DISCRETE FEATURE AND OUTPUT**","b6272ada":"**We need to know the number of discrete variables, Let us find it out !**","a1a78b96":"# **Models**","1abd0004":"**WE SEE THAT AGES BETWEEN 50-60 ARE THE MOST PRONE TO HEART ATTACKS**","f96826ad":"The accuracy of the following models are \n1. Logistic Regression : 0.871\n2. K Nearest Neighbours : 0.742\n3. Classification Tree : 0.742\n4. Random Forest : 0.839\n5. Adaboost Classifier:0.806\n6. ANN : 0.780\n","ed31256f":"**Number of Numerical Variables**","70f8bd92":"Wow!! We got to know all of the features are numerical variables ! ","933e6838":"So we know that there are 14 features that has been included in the dataset needed to determine Heart Attack","11f71e4a":"**IMPORTING THE NECESSARY LIBRARIES**","624bf259":"**Let Us Know if We Have any missing values**","8f2d4cd5":"# Acknowledgements\n","f5d4895f":"**DESCRIPTION OF THE DATASET**","0989cf06":"# Overview","839e4b28":"**WE SEE THAT LOGISTIC REGRESSION PERFORMS THE BEST**","62cb3f1e":"# How will we proceed ?","6426e95e":"[Sarthak Bobde](http:\/\/https:\/\/www.kaggle.com\/sarthakbobde\/heart-attack-analysis-and-classifier)\n[J\u0119drzej Dudzicz](http:\/\/https:\/\/www.kaggle.com\/jedrzejdudzicz\/heart-attack-analysis-prediciton)","e2cf65c8":"# **HEART ATTACK ANALYSIS AND PREDICTION**","70ae8722":"# **UNDERSTANDING THE DATA**","e309226f":"Great!!! We don't have to handle the cases for missing values !! ","080521eb":"![](https:\/\/source.wustl.edu\/wp-content\/uploads\/2019\/02\/HeartImage-760x594.jpg)","e32d0857":"# Including Required Packages ","fef17fa3":"# NEURAL NETWORK APPROACH","58c717bf":"Heart attack or myocardial infarction according to Wikipedia,commonly occurs when blood flow decreases or stops to a part of the heart, causing damage to the heart muscle. The most common symptom is chest pain or discomfort which may travel into the shoulder, arm, back, neck or jaw.According to a medical survey in USA, every year about 647,000 people die of heart attack making it the leading cause of death. According to the Centers for Disease Control and Prevention (CDC) approximately every 40 seconds an American will have a heart attack.And the scenario almost remains same in countries like India. \nThrough the analysis and visualisations in this notebook we would try to go to the rockbottom of this problem and try to figure out what are the features that determines the causes of Heart Attack. \nThe judgements produced are absolutely dependent on the information provided in the data.","d0bcff9a":"**Now let's deal with the Continuous Variables**"}}