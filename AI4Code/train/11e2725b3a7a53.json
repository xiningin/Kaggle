{"cell_type":{"a684ee99":"code","2c4c6ced":"code","9519fb59":"code","a8d1d428":"code","f52c648f":"code","e958a0af":"code","635b306f":"markdown"},"source":{"a684ee99":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%pylab inline\npylab.rcParams['figure.figsize'] = (10, 6)\n\nlimit_rows   = 7000000\ntrain_df           = pd.read_csv(\"..\/input\/train_ver2.csv\",dtype={\"sexo\":str,\n                                                    \"ind_nuevo\":str,\n                                                    \"ult_fec_cli_1t\":str,\n                                                    \"indext\":str}, nrows=limit_rows)\nunique_ids   = pd.Series(train_df[\"ncodpers\"].unique())\nlimit_people = 12000\nunique_id    = unique_ids.sample(n=limit_people)\ntrain_df     = train_df[train_df.ncodpers.isin(unique_id)]\ntrain_df.describe()","2c4c6ced":"# convert the dates to standard Year-Month-Day format\ntrain_df[\"fecha_dato\"] = pd.to_datetime(train_df[\"fecha_dato\"],format=\"%Y-%m-%d\")\ntrain_df[\"fecha_alta\"] = pd.to_datetime(train_df[\"fecha_alta\"],format=\"%Y-%m-%d\")\ntrain_df[\"fecha_dato\"].unique()\n\n# customers will be more likely to buy products at certain months of the year (Christmas bonuses?), so let's add a month column.\ntrain_df[\"month\"] = pd.DatetimeIndex(train_df[\"fecha_dato\"]).month\ntrain_df[\"age\"]   = pd.to_numeric(train_df[\"age\"], errors=\"coerce\")\n# missing values?\ntrain_df.isnull().any()","9519fb59":"# Let's see if we can fill in missing values by looking how many months of history these customers have.\nmonths_active = train_df.loc[train_df[\"ind_nuevo\"].isnull(),:].groupby(\"ncodpers\", sort=False).size()\nmonths_active.max()\n# Looks like these are all new customers, so replace accordingly.\ntrain_df.loc[train_df[\"ind_nuevo\"].isnull(),\"ind_nuevo\"] = 1\n\ntrain_df.antiguedad = pd.to_numeric(train_df.antiguedad,errors=\"coerce\")\n# the same people that we just determined were new customers\ntrain_df.loc[train_df[\"antiguedad\"].isnull(),\"ind_nuevo\"].describe()\ntrain_df.loc[train_df.antiguedad.isnull(),\"antiguedad\"] = train_df.antiguedad.min()\ntrain_df.loc[train_df.antiguedad <0, \"antiguedad\"]      = 0\n\ntrain_df.loc[train_df.ind_actividad_cliente.isnull(),\"ind_actividad_cliente\"] = train_df[\"ind_actividad_cliente\"].median()\n\n# Some entries don't have the date they joined the company. Just give them something in the middle of the pack\ndates=train_df.loc[:,\"fecha_alta\"].sort_values().reset_index()\nmedian_date = int(np.median(dates.index.values))\ntrain_df.loc[train_df.fecha_alta.isnull(),\"fecha_alta\"] = dates.loc[median_date,\"fecha_alta\"]\ntrain_df[\"fecha_alta\"].describe()\n\n# Fill in missing with the more common status.\npd.Series([i for i in train_df.indrel]).value_counts()\ntrain_df.loc[train_df.indrel.isnull(),\"indrel\"] = 1\n\n# manual fix\ntrain_df.loc[train_df.nomprov.isnull(),\"nomprov\"] = \"UNKNOWN\"\ntrain_df.loc[train_df.nomprov==\"CORU\\xc3\\x91A, A\",\"nomprov\"] = \"CORUNA, A\"\n\n# fill in missing with median income by region\nincomes = train_df.loc[train_df.renta.notnull(),:].groupby(\"nomprov\").agg({\"renta\":{\"MedianIncome\":median}})\nincomes.sort_values(by=(\"renta\",\"MedianIncome\"),inplace=True)\nincomes.reset_index(inplace=True)\nincomes.nomprov = incomes.nomprov.astype(\"category\", categories=[i for i in train_df.nomprov.unique()],ordered=False)\nincomes.head()\n\ngrouped        = train_df.groupby(\"nomprov\").agg({\"renta\":lambda x: x.median(skipna=True)}).reset_index()\nnew_incomes    = pd.merge(train_df,grouped,how=\"inner\",on=\"nomprov\").loc[:, [\"nomprov\",\"renta_y\"]]\nnew_incomes    = new_incomes.rename(columns={\"renta_y\":\"renta\"}).sort_values(\"renta\").sort_values(\"nomprov\")\ntrain_df.sort_values(\"nomprov\",inplace=True)\ntrain_df       = train_df.reset_index()\nnew_incomes    = new_incomes.reset_index()\n\ntrain_df.loc[train_df.renta.isnull(),\"renta\"] = new_incomes.loc[train_df.renta.isnull(),\"renta\"].reset_index()\ntrain_df.loc[train_df.renta.isnull(),\"renta\"] = train_df.loc[train_df.renta.notnull(),\"renta\"].median()\ntrain_df.sort_values(by=\"fecha_dato\",inplace=True)\n\ntrain_df.loc[train_df.ind_nomina_ult1.isnull(), \"ind_nomina_ult1\"] = 0\ntrain_df.loc[train_df.ind_nom_pens_ult1.isnull(), \"ind_nom_pens_ult1\"] = 0\n\ntrain_df.drop([\"tipodom\",\"cod_prov\"],axis=1,inplace=True)","a8d1d428":"# Age distribution\ndef age_distribution(df):\n    with sns.plotting_context(\"notebook\",font_scale=1.5):\n        sns.set_style(\"whitegrid\")\n        sns.distplot(df[\"age\"].dropna(),\n                     bins=80,\n                     kde=False,\n                     color=\"tomato\")\n        plt.ylabel(\"Count\")\n        plt.xlim((15,100))\n        \nage_distribution(train_df)\n# Let's separate the distribution and move the outliers to the mean of the closest one.\ntrain_df.loc[train_df.age < 18,\"age\"]  = train_df.loc[(train_df.age >= 18) & (train_df.age <= 30),\"age\"].mean(skipna=True)\ntrain_df.loc[train_df.age > 100,\"age\"] = train_df.loc[(train_df.age >= 30) & (train_df.age <= 100),\"age\"].mean(skipna=True)\ntrain_df[\"age\"].fillna(train_df[\"age\"].mean(),inplace=True)\ntrain_df[\"age\"]                  = train_df[\"age\"].astype(int)\n# plot again\nage_distribution(train_df)","f52c648f":"from sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing, ensemble\nimport xgboost as xgb\nfrom collections import defaultdict\n\nimport csv\nimport datetime\nfrom operator import sub\n\nusecols = ['ncodpers', 'ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1']\n\ndf_train = pd.read_csv('..\/input\/train_ver2.csv', usecols=usecols)\nsample = pd.read_csv('..\/input\/sample_submission.csv')\n\ndf_train = df_train.drop_duplicates(['ncodpers'], keep='last')\ndf_train.fillna(0, inplace=True)\n\nmodels = {}\nmodel_preds = {}\nid_preds = defaultdict(list)\nids = df_train['ncodpers'].values\nfor c in df_train.columns:\n    if c != 'ncodpers':\n        print(c)\n        y_train = df_train[c]\n        x_train = df_train.drop([c, 'ncodpers'], 1)\n        \n        clf = LogisticRegression()\n        clf.fit(x_train, y_train)\n        p_train = clf.predict_proba(x_train)[:,1]\n        \n        models[c] = clf\n        model_preds[c] = p_train\n        for id, p in zip(ids, p_train):\n            id_preds[id].append(p)\n            \n        print(roc_auc_score(y_train, p_train))\n        \nalready_active = {}\nfor row in df_train.values:\n    row = list(row)\n    id = row.pop(0)\n    active = [c[0] for c in zip(df_train.columns[1:], row) if c[1] > 0]\n    already_active[id] = active\n    \ntrain_preds = {}\nfor id, p in id_preds.items():\n    # Here be dragons\n    preds = [i[0] for i in sorted([i for i in zip(df_train.columns[1:], p) if i[0] not in already_active[id]], key=lambda i:i [1], reverse=True)[:7]]\n    train_preds[id] = preds\n    \ntest_preds = []\nfor row in sample.values:\n    id = row[0]\n    p = train_preds[id]\n    test_preds.append(' '.join(p))\n\nsample['added_products'] = test_preds\nsample.to_csv('submission.csv', index=False)","e958a0af":"\"\"\"\nCode based on BreakfastPirate Forum post\n__author__ : SRK\n\"\"\"\nimport csv\nimport datetime\nfrom operator import sub\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn import preprocessing, ensemble\n\nmapping_dict = {\n'ind_empleado'  : {-99:0, 'N':1, 'B':2, 'F':3, 'A':4, 'S':5},\n'sexo'          : {'V':0, 'H':1, -99:2},\n'ind_nuevo'     : {'0':0, '1':1, -99:2},\n'indrel'        : {'1':0, '99':1, -99:2},\n'indrel_1mes'   : {-99:0, '1.0':1, '1':1, '2.0':2, '2':2, '3.0':3, '3':3, '4.0':4, '4':4, 'P':5},\n'tiprel_1mes'   : {-99:0, 'I':1, 'A':2, 'P':3, 'R':4, 'N':5},\n'indresi'       : {-99:0, 'S':1, 'N':2},\n'indext'        : {-99:0, 'S':1, 'N':2},\n'conyuemp'      : {-99:0, 'S':1, 'N':2},\n'indfall'       : {-99:0, 'S':1, 'N':2},\n'tipodom'       : {-99:0, '1':1},\n'ind_actividad_cliente' : {'0':0, '1':1, -99:2},\n'segmento'      : {'02 - PARTICULARES':0, '03 - UNIVERSITARIO':1, '01 - TOP':2, -99:2},\n'pais_residencia' : {'LV': 102, 'BE': 12, 'BG': 50, 'BA': 61, 'BM': 117, 'BO': 62, 'JP': 82, 'JM': 116, 'BR': 17, 'BY': 64, 'BZ': 113, 'RU': 43, 'RS': 89, 'RO': 41, 'GW': 99, 'GT': 44, 'GR': 39, 'GQ': 73, 'GE': 78, 'GB': 9, 'GA': 45, 'GN': 98, 'GM': 110, 'GI': 96, 'GH': 88, 'OM': 100, 'HR': 67, 'HU': 106, 'HK': 34, 'HN': 22, 'AD': 35, 'PR': 40, 'PT': 26, 'PY': 51, 'PA': 60, 'PE': 20, 'PK': 84, 'PH': 91, 'PL': 30, 'EE': 52, 'EG': 74, 'ZA': 75, 'EC': 19, 'AL': 25, 'VN': 90, 'ET': 54, 'ZW': 114, 'ES': 0, 'MD': 68, 'UY': 77, 'MM': 94, 'ML': 104, 'US': 15, 'MT': 118, 'MR': 48, 'UA': 49, 'MX': 16, 'IL': 42, 'FR': 8, 'MA': 38, 'FI': 23, 'NI': 33, 'NL': 7, 'NO': 46, 'NG': 83, 'NZ': 93, 'CI': 57, 'CH': 3, 'CO': 21, 'CN': 28, 'CM': 55, 'CL': 4, 'CA': 2, 'CG': 101, 'CF': 109, 'CD': 112, 'CZ': 36, 'CR': 32, 'CU': 72, 'KE': 65, 'KH': 95, 'SV': 53, 'SK': 69, 'KR': 87, 'KW': 92, 'SN': 47, 'SL': 97, 'KZ': 111, 'SA': 56, 'SG': 66, 'SE': 24, 'DO': 11, 'DJ': 115, 'DK': 76, 'DE': 10, 'DZ': 80, 'MK': 105, -99: 1, 'LB': 81, 'TW': 29, 'TR': 70, 'TN': 85, 'LT': 103, 'LU': 59, 'TH': 79, 'TG': 86, 'LY': 108, 'AE': 37, 'VE': 14, 'IS': 107, 'IT': 18, 'AO': 71, 'AR': 13, 'AU': 63, 'AT': 6, 'IN': 31, 'IE': 5, 'QA': 58, 'MZ': 27},\n'canal_entrada' : {'013': 49, 'KHP': 160, 'KHQ': 157, 'KHR': 161, 'KHS': 162, 'KHK': 10, 'KHL': 0, 'KHM': 12, 'KHN': 21, 'KHO': 13, 'KHA': 22, 'KHC': 9, 'KHD': 2, 'KHE': 1, 'KHF': 19, '025': 159, 'KAC': 57, 'KAB': 28, 'KAA': 39, 'KAG': 26, 'KAF': 23, 'KAE': 30, 'KAD': 16, 'KAK': 51, 'KAJ': 41, 'KAI': 35, 'KAH': 31, 'KAO': 94, 'KAN': 110, 'KAM': 107, 'KAL': 74, 'KAS': 70, 'KAR': 32, 'KAQ': 37, 'KAP': 46, 'KAW': 76, 'KAV': 139, 'KAU': 142, 'KAT': 5, 'KAZ': 7, 'KAY': 54, 'KBJ': 133, 'KBH': 90, 'KBN': 122, 'KBO': 64, 'KBL': 88, 'KBM': 135, 'KBB': 131, 'KBF': 102, 'KBG': 17, 'KBD': 109, 'KBE': 119, 'KBZ': 67, 'KBX': 116, 'KBY': 111, 'KBR': 101, 'KBS': 118, 'KBP': 121, 'KBQ': 62, 'KBV': 100, 'KBW': 114, 'KBU': 55, 'KCE': 86, 'KCD': 85, 'KCG': 59, 'KCF': 105, 'KCA': 73, 'KCC': 29, 'KCB': 78, 'KCM': 82, 'KCL': 53, 'KCO': 104, 'KCN': 81, 'KCI': 65, 'KCH': 84, 'KCK': 52, 'KCJ': 156, 'KCU': 115, 'KCT': 112, 'KCV': 106, 'KCQ': 154, 'KCP': 129, 'KCS': 77, 'KCR': 153, 'KCX': 120, 'RED': 8, 'KDL': 158, 'KDM': 130, 'KDN': 151, 'KDO': 60, 'KDH': 14, 'KDI': 150, 'KDD': 113, 'KDE': 47, 'KDF': 127, 'KDG': 126, 'KDA': 63, 'KDB': 117, 'KDC': 75, 'KDX': 69, 'KDY': 61, 'KDZ': 99, 'KDT': 58, 'KDU': 79, 'KDV': 91, 'KDW': 132, 'KDP': 103, 'KDQ': 80, 'KDR': 56, 'KDS': 124, 'K00': 50, 'KEO': 96, 'KEN': 137, 'KEM': 155, 'KEL': 125, 'KEK': 145, 'KEJ': 95, 'KEI': 97, 'KEH': 15, 'KEG': 136, 'KEF': 128, 'KEE': 152, 'KED': 143, 'KEC': 66, 'KEB': 123, 'KEA': 89, 'KEZ': 108, 'KEY': 93, 'KEW': 98, 'KEV': 87, 'KEU': 72, 'KES': 68, 'KEQ': 138, -99: 6, 'KFV': 48, 'KFT': 92, 'KFU': 36, 'KFR': 144, 'KFS': 38, 'KFP': 40, 'KFF': 45, 'KFG': 27, 'KFD': 25, 'KFE': 148, 'KFB': 146, 'KFC': 4, 'KFA': 3, 'KFN': 42, 'KFL': 34, 'KFM': 141, 'KFJ': 33, 'KFK': 20, 'KFH': 140, 'KFI': 134, '007': 71, '004': 83, 'KGU': 149, 'KGW': 147, 'KGV': 43, 'KGY': 44, 'KGX': 24, 'KGC': 18, 'KGN': 11}\n}\ncat_cols = list(mapping_dict.keys())\n\ntarget_cols = ['ind_ahor_fin_ult1','ind_aval_fin_ult1','ind_cco_fin_ult1','ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1','ind_ctma_fin_ult1','ind_ctop_fin_ult1','ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1','ind_dela_fin_ult1','ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1','ind_pres_fin_ult1','ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1','ind_nomina_ult1','ind_nom_pens_ult1','ind_recibo_ult1']\ntarget_cols = target_cols[2:]\n\ndef getTarget(row):\n    tlist = []\n    for col in target_cols:\n        if row[col].strip() in ['', 'NA']:\n            target = 0\n        else:\n            target = int(float(row[col]))\n        tlist.append(target)\n    return tlist\n\ndef getIndex(row, col):\n    val = row[col].strip()\n    if val not in ['','NA']:\n        ind = mapping_dict[col][val]\n    else:\n        ind = mapping_dict[col][-99]\n    return ind\n\ndef getAge(row):\n    mean_age = 40.\n    min_age = 20.\n    max_age = 90.\n    range_age = max_age - min_age\n    age = row['age'].strip()\n    if age == 'NA' or age == '':\n        age = mean_age\n    else:\n        age = float(age)\n        if age < min_age:\n            age = min_age\n        elif age > max_age:\n            age = max_age\n    return round( (age - min_age) \/ range_age, 4)\n\ndef getCustSeniority(row):\n    min_value = 0.\n    max_value = 256.\n    range_value = max_value - min_value\n    missing_value = 0.\n    cust_seniority = row['antiguedad'].strip()\n    if cust_seniority == 'NA' or cust_seniority == '':\n        cust_seniority = missing_value\n    else:\n        cust_seniority = float(cust_seniority)\n        if cust_seniority < min_value:\n            cust_seniority = min_value\n        elif cust_seniority > max_value:\n            cust_seniority = max_value\n    return round((cust_seniority-min_value) \/ range_value, 4)\n\ndef getRent(row):\n    min_value = 0.\n    max_value = 1500000.\n    range_value = max_value - min_value\n    missing_value = 101850.\n    rent = row['renta'].strip()\n    if rent == 'NA' or rent == '':\n        rent = missing_value\n    else:\n        rent = float(rent)\n        if rent < min_value:\n            rent = min_value\n        elif rent > max_value:\n            rent = max_value\n    return round((rent-min_value) \/ range_value, 6)\n\ndef processData(in_file_name, cust_dict):\n    x_vars_list = []\n    y_vars_list = []\n    for row in csv.DictReader(in_file_name):\n        # use only the four months as specified by breakfastpirate #\n        if row['fecha_dato'] not in ['2015-05-28', '2015-06-28', '2016-05-28', '2016-06-28']:\n            continue\n\n        cust_id = int(row['ncodpers'])\n        if row['fecha_dato'] in ['2015-05-28', '2016-05-28']:    \n            target_list = getTarget(row)\n            cust_dict[cust_id] =  target_list[:]\n            continue\n\n        x_vars = []\n        for col in cat_cols:\n            x_vars.append( getIndex(row, col) )\n        x_vars.append( getAge(row) )\n        x_vars.append( getCustSeniority(row) )\n        x_vars.append( getRent(row) )\n\n        if row['fecha_dato'] == '2016-06-28':\n            prev_target_list = cust_dict.get(cust_id, [0]*22)\n            x_vars_list.append(x_vars + prev_target_list)\n        elif row['fecha_dato'] == '2015-06-28':\n            prev_target_list = cust_dict.get(cust_id, [0]*22)\n            target_list = getTarget(row)\n            new_products = [max(x1 - x2,0) for (x1, x2) in zip(target_list, prev_target_list)]\n            if sum(new_products) > 0:\n                for ind, prod in enumerate(new_products):\n                    if prod>0:\n                        assert len(prev_target_list) == 22\n                        x_vars_list.append(x_vars+prev_target_list)\n                        y_vars_list.append(ind)\n\n    return x_vars_list, y_vars_list, cust_dict\n            \ndef runXGB(train_X, train_y, seed_val=0):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.05\n    param['max_depth'] = 8\n    param['silent'] = 1\n    param['num_class'] = 22\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = 1\n    param['subsample'] = 0.7\n    param['colsample_bytree'] = 0.7\n    param['seed'] = seed_val\n    num_rounds = 50\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n    model = xgb.train(plst, xgtrain, num_rounds)    \n    return model\n\n\nif __name__ == \"__main__\":\n    start_time = datetime.datetime.now()\n    data_path = \"..\/input\/\"\n    train_file =  open(data_path + \"train_ver2.csv\")\n    x_vars_list, y_vars_list, cust_dict = processData(train_file, {})\n    train_X = np.array(x_vars_list)\n    train_y = np.array(y_vars_list)\n    print(np.unique(train_y))\n    del x_vars_list, y_vars_list\n    train_file.close()\n    print(train_X.shape, train_y.shape)\n    print(datetime.datetime.now()-start_time)\n    test_file = open(data_path + \"test_ver2.csv\")\n    x_vars_list, y_vars_list, cust_dict = processData(test_file, cust_dict)\n    test_X = np.array(x_vars_list)\n    del x_vars_list\n    test_file.close()\n    print(test_X.shape)\n    print(datetime.datetime.now()-start_time)\n\n    print(\"Building model..\")\n    model = runXGB(train_X, train_y, seed_val=0)\n    del train_X, train_y\n    print(\"Predicting..\")\n    xgtest = xgb.DMatrix(test_X)\n    preds = model.predict(xgtest)\n    del test_X, xgtest\n    print(datetime.datetime.now()-start_time)\n\n    print(\"Getting the top products..\")\n    target_cols = np.array(target_cols)\n    preds = np.argsort(preds, axis=1)\n    preds = np.fliplr(preds)[:,:7]\n    test_id = np.array(pd.read_csv(data_path + \"test_ver2.csv\", usecols=['ncodpers'])['ncodpers'])\n    final_preds = [\" \".join(list(target_cols[pred])) for pred in preds]\n    out_df = pd.DataFrame({'ncodpers':test_id, 'added_products':final_preds})\n    out_df.to_csv('submission2.csv', index=False)\n    print(datetime.datetime.now()-start_time)","635b306f":"# Introduction #\nThis kernel is writtern for the [Santander Product Recommendation](https:\/\/www.kaggle.com\/c\/santander-product-recommendation) competetion. If you Like the notebook and think that it helped you, <font color=\"red\"><b> please upvote<\/b><\/font>.\n\n---\n## Table of Content\n1. Data Preprocessing\n2. Modeling and Evaluation\n3. Final Prediction & Submission"}}