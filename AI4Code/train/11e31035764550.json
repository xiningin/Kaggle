{"cell_type":{"bb4fe704":"code","44624cd9":"code","45444d57":"code","80f4149b":"code","0e2ff84d":"code","1509d84a":"code","6535f87e":"code","d8b60caa":"code","de5f46b3":"code","b7ba8547":"code","e94adeb6":"code","d9d0cf32":"code","c5be4652":"code","9ecc89ee":"code","78d54988":"code","9a4380de":"code","c042bfeb":"code","50ad295a":"code","8e28840b":"code","f78a24d0":"code","86ccde55":"code","97608832":"code","18384275":"code","c92bf9c0":"code","72501065":"code","ac2f6c42":"code","dba946a2":"code","d904d8f1":"code","439e8bf1":"code","200df613":"code","3af501dc":"code","19b77c08":"code","a9139977":"code","93c637de":"code","1574da3e":"code","25b0a703":"code","244c570c":"code","adb605e3":"code","e7e6a164":"markdown","8105846a":"markdown","5950dace":"markdown","ec22cfdb":"markdown","f850fe42":"markdown","9093692e":"markdown"},"source":{"bb4fe704":"## Load Libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport gc\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport os\n# Any results you write to the current directory are saved as output.\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision import transforms,models\nfrom tqdm import tqdm_notebook as tqdm\n\n## This library is for augmentations .\nfrom albumentations import (\n    PadIfNeeded,\n    HorizontalFlip,\n    VerticalFlip,    \n    CenterCrop,    \n    Crop,\n    Compose,\n    Transpose,\n    RandomRotate90,\n    ElasticTransform,\n    GridDistortion, \n    OpticalDistortion,\n    RandomSizedCrop,\n    OneOf,\n    CLAHE,\n    RandomBrightnessContrast,    \n    \n    RandomGamma,\n    ShiftScaleRotate ,\n    GaussNoise,\n    Blur,\n    MotionBlur,   \n    GaussianBlur,\n)\n\nimport warnings\nwarnings.filterwarnings('ignore')","44624cd9":"##Path for data \nPATH = '..\/input\/bengaliai-cv19\/'","45444d57":"## Model configuration dictionary\n  model_config ={\n    \"arch\": \"shake_shake\",\n    \"input_shape\": (1, 1, 128, 128),\n    \"n_classes\": 10,\n    \"base_channels\": 32,\n    \"depth\": 26,\n    \"shake_forward\": True,\n    \"shake_backward\": True,\n    \"shake_image\": True\n  }","80f4149b":"## Create Data from Parquet file mixing the methods of @hanjoonzhoe and @Iafoss\n\n## Create Crop Function @Iafoss\n\nHEIGHT = 137\nWIDTH = 236\nSIZE = 128\n\ndef bbox(img):\n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n    return rmin, rmax, cmin, cmax\n\ndef crop_resize(img0, size=SIZE, pad=16):\n    #crop a box around pixels large than the threshold \n    #some images contain line at the sides\n    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n    #cropping may cut too much, so we need to add it back\n    xmin = xmin - 13 if (xmin > 13) else 0\n    ymin = ymin - 10 if (ymin > 10) else 0\n    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n    img = img0[ymin:ymax,xmin:xmax]\n    #remove lo intensity pixels as noise\n    img[img < 28] = 0\n    lx, ly = xmax-xmin,ymax-ymin\n    l = max(lx,ly) + pad\n    #make sure that the aspect ratio is kept in rescaling\n    img = np.pad(img, [((l-ly)\/\/2,), ((l-lx)\/\/2,)], mode='constant')\n    return cv2.resize(img,(size,size))\n\ndef Resize(df,size=128):\n    resized = {} \n    df = df.set_index('image_id')\n    for i in tqdm(range(df.shape[0])):\n       # image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size))\n        image0 = 255 - df.loc[df.index[i]].values.reshape(137,236).astype(np.uint8)\n    #normalize each image by its max val\n        img = (image0*(255.0\/image0.max())).astype(np.uint8)\n        image = crop_resize(img)\n        resized[df.index[i]] = image.reshape(-1)\n    resized = pd.DataFrame(resized).T.reset_index()\n    resized.columns = resized.columns.astype(str)\n    resized.rename(columns={'index':'image_id'},inplace=True)\n    return resized\n\n","0e2ff84d":"\"\"\"%%time\n##Feather data generation for all train_data\nfor i in range(4):\n    data = pd.read_parquet(PATH+f'train_image_data_{i}.parquet')\n    data =Resize(data)\n    data.to_feather(f'train_data_{i}{i}_l.feather')\n    del data\n    gc.collect()\"\"\" \n##TO save RAM I have run this command in another kernel and kept the output for the Kernel as dataset for this Kernel .","1509d84a":"DATA_PATH = \"..\/input\/pytorch-efficientnet-starter-kernel\/\"","6535f87e":"## Load Feather Data \ntrain = pd.read_csv(PATH + \"train.csv\")\ndata0 = pd.read_feather(DATA_PATH+\"train_data_00_l.feather\")\ndata1 = pd.read_feather(DATA_PATH+'train_data_11_l.feather')\ndata2 = pd.read_feather(DATA_PATH+'train_data_22_l.feather')\ndata3 = pd.read_feather(DATA_PATH+'train_data_33_l.feather')\ndata_full = pd.concat([data0,data1,data2,data3],ignore_index=True)\ndel data0,data1,data2,data3\ngc.collect()\ndata_full.shape","d8b60caa":"## A bunch of code copied from internet . Half of them I dont understand yet . However , CutOut is used in this notebook\n##https:\/\/github.com\/hysts\/pytorch_image_classification\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nclass Cutout:\n    def __init__(self, mask_size, p, cutout_inside, mask_color=1):\n        self.p = p\n        self.mask_size = mask_size\n        self.cutout_inside = cutout_inside\n        self.mask_color = mask_color\n\n        self.mask_size_half = mask_size \/\/ 2\n        self.offset = 1 if mask_size % 2 == 0 else 0\n\n    def __call__(self, image):\n        image = np.asarray(image).copy()\n\n        if np.random.random() > self.p:\n            return image\n\n        h, w = image.shape[:2]\n\n        if self.cutout_inside:\n            cxmin, cxmax = self.mask_size_half, w + self.offset - self.mask_size_half\n            cymin, cymax = self.mask_size_half, h + self.offset - self.mask_size_half\n        else:\n            cxmin, cxmax = 0, w + self.offset\n            cymin, cymax = 0, h + self.offset\n\n        cx = np.random.randint(cxmin, cxmax)\n        cy = np.random.randint(cymin, cymax)\n        xmin = cx - self.mask_size_half\n        ymin = cy - self.mask_size_half\n        xmax = xmin + self.mask_size\n        ymax = ymin + self.mask_size\n        xmin = max(0, xmin)\n        ymin = max(0, ymin)\n        xmax = min(w, xmax)\n        ymax = min(h, ymax)\n        image[ymin:ymax, xmin:xmax] = self.mask_color\n        return image\n\n\nclass DualCutout:\n    def __init__(self, mask_size, p, cutout_inside, mask_color=1):\n        self.cutout = Cutout(mask_size, p, cutout_inside, mask_color)\n\n    def __call__(self, image):\n        return np.hstack([self.cutout(image), self.cutout(image)])\n\n\nclass DualCutoutCriterion:\n    def __init__(self, alpha):\n        self.alpha = alpha\n        self.criterion = nn.CrossEntropyLoss(reduction='mean')\n\n    def __call__(self, preds, targets):\n        preds1, preds2 = preds\n        return (self.criterion(preds1, targets) + self.criterion(\n            preds2, targets)) * 0.5 + self.alpha * F.mse_loss(preds1, preds2)\n\n\ndef mixup(data, targets, alpha, n_classes):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets = targets[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    data = data * lam + shuffled_data * (1 - lam)\n    targets = (targets, shuffled_targets, lam)\n\n    return data, targets\n\n\ndef mixup_criterion(preds, targets):\n    targets1, targets2, lam = targets\n    criterion = nn.CrossEntropyLoss(reduction='mean')\n    return lam * criterion(preds, targets1) + (1 - lam) * criterion(\n        preds, targets2)\n    \n\n\nclass RandomErasing:\n    def __init__(self, p, area_ratio_range, min_aspect_ratio, max_attempt):\n        self.p = p\n        self.max_attempt = max_attempt\n        self.sl, self.sh = area_ratio_range\n        self.rl, self.rh = min_aspect_ratio, 1. \/ min_aspect_ratio\n\n    def __call__(self, image):\n        image = np.asarray(image).copy()\n\n        if np.random.random() > self.p:\n            return image\n\n        h, w = image.shape[:2]\n        image_area = h * w\n\n        for _ in range(self.max_attempt):\n            mask_area = np.random.uniform(self.sl, self.sh) * image_area\n            aspect_ratio = np.random.uniform(self.rl, self.rh)\n            mask_h = int(np.sqrt(mask_area * aspect_ratio))\n            mask_w = int(np.sqrt(mask_area \/ aspect_ratio))\n\n            if mask_w < w and mask_h < h:\n                x0 = np.random.randint(0, w - mask_w)\n                y0 = np.random.randint(0, h - mask_h)\n                x1 = x0 + mask_w\n                y1 = y0 + mask_h\n                image[y0:y1, x0:x1] = np.random.uniform(0, 1)\n                break\n\n        return image  ","de5f46b3":"## Add Augmentations as suited from Albumentations library\ntrain_aug = Compose([ \n    ShiftScaleRotate(p=1,border_mode=cv2.BORDER_CONSTANT,value =1),\n    OneOf([\n        ElasticTransform(p=0.1, alpha=1, sigma=50, alpha_affine=50,border_mode=cv2.BORDER_CONSTANT,value =1),\n        GridDistortion(distort_limit =0.05 ,border_mode=cv2.BORDER_CONSTANT,value =1, p=0.1),\n        OpticalDistortion(p=0.1, distort_limit= 0.05, shift_limit=0.2,border_mode=cv2.BORDER_CONSTANT,value =1)                  \n        ], p=0.3),\n    OneOf([\n        GaussNoise(var_limit=1.0),\n        Blur(),\n        GaussianBlur(blur_limit=3)\n        ], p=0.4),    \n    RandomGamma(p=0.8)])\n\n## A lot of heavy augmentations","b7ba8547":"## Someone asked for normalization of images . values collected from Iafoss\n\n\nclass ToTensor:\n    def __call__(self, data):\n        if isinstance(data, tuple):\n            return tuple([self._to_tensor(image) for image in data])\n        else:\n            return self._to_tensor(data)\n\n    def _to_tensor(self, data):\n        if len(data.shape) == 3:\n            return torch.from_numpy(data.transpose(2, 0, 1).astype(np.float32))\n        else:\n            return torch.from_numpy(data[None, :, :].astype(np.float32))\n\n\nclass Normalize:\n    def __init__(self, mean, std):\n        self.mean = np.array(mean)\n        self.std = np.array(std)\n\n    def __call__(self, image):\n        image = np.asarray(image).astype(np.float32) \/ 255.\n        image = (image - self.mean) \/ self.std\n        return image","e94adeb6":"## Create dataset function\nclass GraphemeDataset(Dataset):\n    def __init__(self,df,label,_type='train',transform =False,aug=train_aug):\n        self.df = df\n        self.label = label\n        self.aug = aug\n        self.transform = transform\n        self.data = df.iloc[:, 1:].values\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        label1 = self.label.vowel_diacritic.values[idx]\n        label2 = self.label.grapheme_root.values[idx]\n        label3 = self.label.consonant_diacritic.values[idx]\n        #image = self.df.iloc[idx][1:].values.reshape(128,128).astype(np.float)\n        image = self.data[idx, :].reshape(128,128).astype(np.float)\n        if self.transform:\n            augment = self.aug(image =image)\n            image = augment['image']\n            cutout = Cutout(32,0.5,True,1)\n            image = cutout(image)\n        norm = Normalize([0.0692],[0.2051])\n        image = norm(image)\n\n        return image,label1,label2,label3","d9d0cf32":"## Do a train-valid split of the data to create dataset and dataloader . Specify random seed to get reproducibility \nfrom sklearn.model_selection import train_test_split\ntrain_df , valid_df = train_test_split(train,test_size=0.20, random_state=42,shuffle=True) ## Split Labels\ndata_train_df, data_valid_df = train_test_split(data_full,test_size=0.20, random_state=42,shuffle =True) ## split data\ndel data_full \ngc.collect()","c5be4652":"##Creating the train and valid dataset for training . Training data has the transform flag ON\ntrain_dataset = GraphemeDataset(data_train_df ,train_df,transform = False) \nvalid_dataset = GraphemeDataset(data_valid_df ,valid_df,transform = False) \ntorch.cuda.empty_cache()\ngc.collect()","9ecc89ee":"##Visulization function for checking Original and augmented image\ndef visualize(original_image,aug_image):\n    fontsize = 18\n    \n    f, ax = plt.subplots(1, 2, figsize=(8, 8))\n\n    ax[0].imshow(original_image, cmap='gray')\n    ax[0].set_title('Original image', fontsize=fontsize)\n    ax[1].imshow(aug_image,cmap='gray')\n    ax[1].set_title('Augmented image', fontsize=fontsize)\n    ","78d54988":"## One image taken from raw dataframe another from dataset \norig_image = data_train_df.iloc[0, 1:].values.reshape(128,128).astype(np.float)\naug_image = train_dataset[0][0]","9a4380de":"## Check the augmentations \nfor i in range (20):\n    aug_image = train_dataset[0][0]\n    visualize (orig_image,aug_image)","c042bfeb":"del train_df,valid_df,data_train_df,data_valid_df \ntorch.cuda.empty_cache()\ngc.collect()","50ad295a":"## Create data loader and get ready for training .\nbatch_size = 32 \ntrain_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=batch_size,shuffle=True)\n","8e28840b":"## Mish Activation Function Not yet Used . May be later \nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x): \n        \n        x = x *( torch.tanh(F.softplus(x)))\n\n        return x","f78a24d0":"## Over9000 Optimizer . Inspired by Iafoss . Over and Out !\n##https:\/\/github.com\/mgrankin\/over9000\/blob\/master\/ralamb.py\nimport torch, math\nfrom torch.optim.optimizer import Optimizer\n\n# RAdam + LARS\nclass Ralamb(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(Ralamb, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(Ralamb, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('Ralamb does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n\n                if state['step'] == buffered[0]:\n                    N_sma, radam_step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    else:\n                        radam_step_size = 1.0 \/ (1 - beta1 ** state['step'])\n                    buffered[2] = radam_step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                radam_step = p_data_fp32.clone()\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n                else:\n                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n\n                radam_norm = radam_step.pow(2).sum().sqrt()\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n                if weight_norm == 0 or radam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm \/ radam_norm\n\n                state['weight_norm'] = weight_norm\n                state['adam_norm'] = radam_norm\n                state['trust_ratio'] = trust_ratio\n\n                if N_sma >= 5:\n                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n\n# Lookahead implementation from https:\/\/github.com\/rwightman\/pytorch-image-models\/blob\/master\/timm\/optim\/lookahead.py\n\n\"\"\" Lookahead Optimizer Wrapper.\nImplementation modified from: https:\/\/github.com\/alphadl\/lookahead.pytorch\nPaper: `Lookahead Optimizer: k steps forward, 1 step back` - https:\/\/arxiv.org\/abs\/1907.08610\n\"\"\"\nimport torch\nfrom torch.optim.optimizer import Optimizer\nfrom collections import defaultdict\n\nclass Lookahead(Optimizer):\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        self.defaults = base_optimizer.defaults\n        self.defaults.update(defaults)\n        self.state = defaultdict(dict)\n        # manually add our defaults to the param groups\n        for name, default in defaults.items():\n            for group in self.param_groups:\n                group.setdefault(name, default)\n\n    def update_slow(self, group):\n        for fast_p in group[\"params\"]:\n            if fast_p.grad is None:\n                continue\n            param_state = self.state[fast_p]\n            if 'slow_buffer' not in param_state:\n                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n                param_state['slow_buffer'].copy_(fast_p.data)\n            slow = param_state['slow_buffer']\n            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n            fast_p.data.copy_(slow)\n\n    def sync_lookahead(self):\n        for group in self.param_groups:\n            self.update_slow(group)\n\n    def step(self, closure=None):\n        # print(self.k)\n        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n        loss = self.base_optimizer.step(closure)\n        for group in self.param_groups:\n            group['lookahead_step'] += 1\n            if group['lookahead_step'] % group['lookahead_k'] == 0:\n                self.update_slow(group)\n        return loss\n\n    def state_dict(self):\n        fast_state_dict = self.base_optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict['state']\n        param_groups = fast_state_dict['param_groups']\n        return {\n            'state': fast_state,\n            'slow_state': slow_state,\n            'param_groups': param_groups,\n        }\n\n    def load_state_dict(self, state_dict):\n        fast_state_dict = {\n            'state': state_dict['state'],\n            'param_groups': state_dict['param_groups'],\n        }\n        self.base_optimizer.load_state_dict(fast_state_dict)\n\n        # We want to restore the slow state, but share param_groups reference\n        # with base_optimizer. This is a bit redundant but least code\n        slow_state_new = False\n        if 'slow_state' not in state_dict:\n            print('Loading state_dict from optimizer without Lookahead applied.')\n            state_dict['slow_state'] = defaultdict(dict)\n            slow_state_new = True\n        slow_state_dict = {\n            'state': state_dict['slow_state'],\n            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n        if slow_state_new:\n            # reapply defaults to catch missing lookahead specific ones\n            for name, default in self.defaults.items():\n                for group in self.param_groups:\n                    group.setdefault(name, default)\n\ndef LookaheadAdam(params, alpha=0.5, k=6, *args, **kwargs):\n     adam = Adam(params, *args, **kwargs)\n     return Lookahead(adam, alpha, k)\n\n\n# RAdam + LARS + LookAHead\n\n# Lookahead implementation from https:\/\/github.com\/lonePatient\/lookahead_pytorch\/blob\/master\/optimizer.py\n# RAdam + LARS implementation from https:\/\/gist.github.com\/redknightlois\/c4023d393eb8f92bb44b2ab582d7ec20\n\ndef Over9000(params, alpha=0.5, k=6, *args, **kwargs):\n     ralamb = Ralamb(params, *args, **kwargs)\n     return Lookahead(ralamb, alpha, k)\n\nRangerLars = Over9000 ","86ccde55":"## Shake Function \n\nimport torch\nfrom torch.autograd import Function\n\n\nclass ShakeFunction(Function):\n    @staticmethod\n    def forward(ctx, x1, x2, alpha, beta):\n        ctx.save_for_backward(x1, x2, alpha, beta)\n\n        y = x1 * alpha + x2 * (1 - alpha)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x1, x2, alpha, beta = ctx.saved_variables\n        grad_x1 = grad_x2 = grad_alpha = grad_beta = None\n\n        if ctx.needs_input_grad[0]:\n            grad_x1 = grad_output * beta\n        if ctx.needs_input_grad[1]:\n            grad_x2 = grad_output * (1 - beta)\n\n        return grad_x1, grad_x2, grad_alpha, grad_beta\n\n\nshake_function = ShakeFunction.apply\n\n\ndef get_alpha_beta(batch_size, shake_config, device):\n    forward_shake, backward_shake, shake_image = shake_config\n\n    if forward_shake and not shake_image:\n        alpha = torch.rand(1)\n    elif forward_shake and shake_image:\n        alpha = torch.rand(batch_size).view(batch_size, 1, 1, 1)\n    else:\n        alpha = torch.FloatTensor([0.5])\n\n    if backward_shake and not shake_image:\n        beta = torch.rand(1)\n    elif backward_shake and shake_image:\n        beta = torch.rand(batch_size).view(batch_size, 1, 1, 1)\n    else:\n        beta = torch.FloatTensor([0.5])\n\n    alpha = alpha.to(device)\n    beta = beta.to(device)\n\n    return alpha, beta","97608832":"## shake-shake network\ndef initialize_weights(module):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight.data, mode='fan_out')\n    elif isinstance(module, nn.BatchNorm2d):\n        module.weight.data.fill_(1)\n        module.bias.data.zero_()\n    elif isinstance(module, nn.Linear):\n        module.bias.data.zero_()\n\n\nclass ResidualPath(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super(ResidualPath, self).__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = F.relu(x, inplace=False)\n        x = F.relu(self.bn1(self.conv1(x)), inplace=False)\n        x = self.bn2(self.conv2(x))\n        return x\n\n\nclass DownsamplingShortcut(nn.Module):\n    def __init__(self, in_channels):\n        super(DownsamplingShortcut, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n        self.conv2 = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n        self.bn = nn.BatchNorm2d(in_channels * 2)\n\n    def forward(self, x):\n        x = F.relu(x, inplace=False)\n        y1 = F.avg_pool2d(x, kernel_size=1, stride=2, padding=0)\n        y1 = self.conv1(y1)\n\n        y2 = F.pad(x[:, :, 1:, 1:], (0, 1, 0, 1))\n        y2 = F.avg_pool2d(y2, kernel_size=1, stride=2, padding=0)\n        y2 = self.conv2(y2)\n\n        z = torch.cat([y1, y2], dim=1)\n        z = self.bn(z)\n\n        return z\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, shake_config):\n        super(BasicBlock, self).__init__()\n\n        self.shake_config = shake_config\n\n        self.residual_path1 = ResidualPath(in_channels, out_channels, stride)\n        self.residual_path2 = ResidualPath(in_channels, out_channels, stride)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module('downsample',\n                                     DownsamplingShortcut(in_channels))\n\n    def forward(self, x):\n        x1 = self.residual_path1(x)\n        x2 = self.residual_path2(x)\n\n        if self.training:\n            shake_config = self.shake_config\n        else:\n            shake_config = (False, False, False)\n\n        alpha, beta = get_alpha_beta(x.size(0), shake_config, x.device)\n        y = shake_function(x1, x2, alpha, beta)\n\n        return self.shortcut(x) + y\n\n\nclass Network(nn.Module):\n    def __init__(self, config):\n        super(Network, self).__init__()\n\n        input_shape = config['input_shape']\n        n_classes = config['n_classes']\n\n        base_channels = config['base_channels']\n        depth = config['depth']\n        self.shake_config = (config['shake_forward'], config['shake_backward'],\n                             config['shake_image'])\n\n        block = BasicBlock\n        n_blocks_per_stage = (depth - 2) \/\/ 6\n        assert n_blocks_per_stage * 6 + 2 == depth\n\n        n_channels = [base_channels, base_channels * 2, base_channels * 4]\n\n        self.conv = nn.Conv2d(\n            input_shape[1],\n            n_channels[0],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False)\n        self.bn = nn.BatchNorm2d(base_channels)\n\n        self.stage1 = self._make_stage(\n            n_channels[0], n_channels[0], n_blocks_per_stage, block, stride=1)\n        self.stage2 = self._make_stage(\n            n_channels[0], n_channels[1], n_blocks_per_stage, block, stride=2)\n        self.stage3 = self._make_stage(\n            n_channels[1], n_channels[2], n_blocks_per_stage, block, stride=2)\n\n        # compute conv feature size\n        with torch.no_grad():\n            self.feature_size = self._forward_conv(\n                torch.zeros(*input_shape)).view(-1).shape[0]\n\n        #self.fc = nn.Linear(self.feature_size, n_classes)\n        # vowel_diacritic\n        self.fc1 = nn.Linear(self.feature_size,11)\n        # grapheme_root\n        self.fc2 = nn.Linear(self.feature_size,168)\n        # consonant_diacritic\n        self.fc3 = nn.Linear(self.feature_size,7)\n\n        # initialize weights\n        self.apply(initialize_weights)\n\n    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride):\n        stage = nn.Sequential()\n        for index in range(n_blocks):\n            block_name = 'block{}'.format(index + 1)\n            if index == 0:\n                stage.add_module(\n                    block_name,\n                    block(\n                        in_channels,\n                        out_channels,\n                        stride=stride,\n                        shake_config=self.shake_config))\n            else:\n                stage.add_module(\n                    block_name,\n                    block(\n                        out_channels,\n                        out_channels,\n                        stride=1,\n                        shake_config=self.shake_config))\n        return stage\n\n    def _forward_conv(self, x):\n        x = F.relu(self.bn(self.conv(x)), inplace=True)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv(x)\n        x = x.view(x.size(0), -1)\n        x1 = self.fc1(x)\n        x2= self.fc2(x)\n        x3 = self.fc3(x)\n        return x1,x2,x3","18384275":"## Make sure we are using the GPU . Get CUDA device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","c92bf9c0":"## Load model and pass desired parameters in config\ndef load_model(config):\n    #module = importlib.import_module('models.{}'.format(config['arch']))\n    #Network = getattr(module, 'Network')\n    return Network(config)\n\nmodel = load_model(model_config)\nmodel = model.to(device)","72501065":"## A Small but useful test of the Model by using dummy input . .\nx = torch.zeros((32,1, 64, 64))\nwith torch.no_grad():\n    output1,output2,output3 =model(x.cuda())\nprint(output3.shape)","ac2f6c42":"## This is a placeholder for finetunign or inference when you want to load a previously trained model\n##and want to finetune or want to do just inference\n\n##model.load_state_dict(torch.load('..\/input\/bengef2\/effnetb0_trial_stage1.pth'))  \n## There is a small thing . I trained using effnetb4 offline for 20 epochs and loaded the weight\n## I forgot to change the naming convension and it still reads effnetb0 . But this is actually effnetb4","dba946a2":"n_epochs = 1 ## 1 Epoch as sample . \"I am just a poor boy  , no GPU in reality \"\n\n#optimizer =torch.optim.Adam(model.parameters(), lr=1e-4)\n#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 5, 2e-4) ## This didnt give good result need to correct  and get the right scheduler .\noptimizer =Over9000(model.parameters(), lr=2e-3, weight_decay=1e-3) ## New once \nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 1e-2, total_steps=None, epochs=n_epochs, steps_per_epoch=5021, pct_start=0.0,\n                                   anneal_strategy='cos', cycle_momentum=True,base_momentum=0.85, max_momentum=0.95,  div_factor=100.0) ## Scheduler . Step for each batch\ncriterion = nn.CrossEntropyLoss()\nbatch_size=32","d904d8f1":"##Local Metrics implementation .\n##https:\/\/www.kaggle.com\/corochann\/bengali-seresnext-training-with-pytorch\nimport numpy as np\nimport sklearn.metrics\nimport torch\n\n\ndef macro_recall(pred_y, y, n_grapheme=168, n_vowel=11, n_consonant=7):\n    pred_y = torch.split(pred_y, [n_grapheme, n_vowel, n_consonant], dim=1)\n    pred_labels = [torch.argmax(py, dim=1).cpu().numpy() for py in pred_y]\n\n    y = y.cpu().numpy()\n    # pred_y = [p.cpu().numpy() for p in pred_y]\n\n    recall_grapheme = sklearn.metrics.recall_score(pred_labels[0], y[:, 0], average='macro')\n    recall_vowel = sklearn.metrics.recall_score(pred_labels[1], y[:, 1], average='macro')\n    recall_consonant = sklearn.metrics.recall_score(pred_labels[2], y[:, 2], average='macro')\n    scores = [recall_grapheme, recall_vowel, recall_consonant]\n    final_score = np.average(scores, weights=[2, 1, 1])\n    # print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, '\n    #       f'total {final_score}, y {y.shape}')\n    return final_score\n\ndef macro_recall_multi(pred_graphemes, true_graphemes,pred_vowels,true_vowels,pred_consonants,true_consonants, n_grapheme=168, n_vowel=11, n_consonant=7):\n    #pred_y = torch.split(pred_y, [n_grapheme], dim=1)\n    pred_label_graphemes = torch.argmax(pred_graphemes, dim=1).cpu().numpy()\n\n    true_label_graphemes = true_graphemes.cpu().numpy()\n    \n    pred_label_vowels = torch.argmax(pred_vowels, dim=1).cpu().numpy()\n\n    true_label_vowels = true_vowels.cpu().numpy()\n    \n    pred_label_consonants = torch.argmax(pred_consonants, dim=1).cpu().numpy()\n\n    true_label_consonants = true_consonants.cpu().numpy()    \n    # pred_y = [p.cpu().numpy() for p in pred_y]\n\n    recall_grapheme = sklearn.metrics.recall_score(pred_label_graphemes, true_label_graphemes, average='macro')\n    recall_vowel = sklearn.metrics.recall_score(pred_label_vowels, true_label_vowels, average='macro')\n    recall_consonant = sklearn.metrics.recall_score(pred_label_consonants, true_label_consonants, average='macro')\n    scores = [recall_grapheme, recall_vowel, recall_consonant]\n    final_score = np.average(scores, weights=[2, 1, 1])\n    #print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, '\n    #       f'total {final_score}')\n    return final_score\n\n\ndef calc_macro_recall(solution, submission):\n    # solution df, submission df\n    scores = []\n    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n        y_true_subset = solution[solution[component] == component]['target'].values\n        y_pred_subset = submission[submission[component] == component]['target'].values\n        scores.append(sklearn.metrics.recall_score(\n            y_true_subset, y_pred_subset, average='macro'))\n    final_score = np.average(scores, weights=[2, 1, 1])\n    return final_score","439e8bf1":"## This function for train is copied from @hanjoonchoe\n## We are going to train and track accuracy and then evaluate and track validation accuracy\ndef train(epoch,history):\n  model.train()\n  losses = []\n  accs = []\n  acc= 0.0\n  total = 0.0\n  running_loss = 0.0\n  running_acc = 0.0\n  running_recall = 0.0\n  for idx, (inputs,labels1,labels2,labels3) in tqdm(enumerate(train_loader),total=len(train_loader)):\n      inputs = inputs.to(device)\n      labels1 = labels1.to(device)\n      labels2 = labels2.to(device)\n      labels3 = labels3.to(device)\n      total += len(inputs)\n      optimizer.zero_grad()\n      outputs1,outputs2,outputs3 = model(inputs.unsqueeze(1).float())\n      loss1 = 0.1*criterion(outputs1,labels1)\n      loss2 = 0.7* criterion(outputs2,labels2)\n      loss3 = 0.2*criterion(outputs3,labels3)\n      running_loss += loss1.item()+loss2.item()+loss3.item()\n      running_recall+= macro_recall_multi(outputs2,labels2,outputs1,labels1,outputs3,labels3)\n      running_acc += (outputs1.argmax(1)==labels1).float().mean()\n      running_acc += (outputs2.argmax(1)==labels2).float().mean()\n      running_acc += (outputs3.argmax(1)==labels3).float().mean()\n      (loss1+loss2+loss3).backward()\n      optimizer.step()\n      optimizer.zero_grad()\n      acc = running_acc\/total\n      scheduler.step()\n  losses.append(running_loss\/len(train_loader))\n  accs.append(running_acc\/(len(train_loader)*3))\n  print(' train epoch : {}\\tacc : {:.2f}%'.format(epoch,running_acc\/(len(train_loader)*3)))\n  print('loss : {:.4f}'.format(running_loss\/len(train_loader)))\n    \n  print('recall: {:.4f}'.format(running_recall\/len(train_loader)))\n  total_train_recall = running_recall\/len(train_loader)\n  torch.cuda.empty_cache()\n  gc.collect()\n  history.loc[epoch, 'train_loss'] = losses[0]\n  history.loc[epoch,'train_acc'] = accs[0].cpu().numpy()\n  history.loc[epoch,'train_recall'] = total_train_recall\n  return  total_train_recall\ndef evaluate(epoch,history):\n   model.eval()\n   losses = []\n   accs = []\n   recalls = []\n   acc= 0.0\n   total = 0.0\n   #print('epochs {}\/{} '.format(epoch+1,epochs))\n   running_loss = 0.0\n   running_acc = 0.0\n   running_recall = 0.0\n   with torch.no_grad():\n     for idx, (inputs,labels1,labels2,labels3) in tqdm(enumerate(valid_loader),total=len(valid_loader)):\n        inputs = inputs.to(device)\n        labels1 = labels1.to(device)\n        labels2 = labels2.to(device)\n        labels3 = labels3.to(device)\n        total += len(inputs)\n        outputs1,outputs2,outputs3 = model(inputs.unsqueeze(1).float())\n        loss1 = criterion(outputs1,labels1)\n        loss2 = 2*criterion(outputs2,labels2)\n        loss3 = criterion(outputs3,labels3)\n        running_loss += loss1.item()+loss2.item()+loss3.item()\n        running_recall+= macro_recall_multi(outputs2,labels2,outputs1,labels1,outputs3,labels3)\n        running_acc += (outputs1.argmax(1)==labels1).float().mean()\n        running_acc += (outputs2.argmax(1)==labels2).float().mean()\n        running_acc += (outputs3.argmax(1)==labels3).float().mean()\n        acc = running_acc\/total\n        #scheduler.step()\n   losses.append(running_loss\/len(valid_loader))\n   accs.append(running_acc\/(len(valid_loader)*3))\n   recalls.append(running_recall\/len(valid_loader))\n   total_recall = running_recall\/len(valid_loader) ## No its not Arnold Schwarzenegger movie\n   print('val epoch: {} \\tval acc : {:.2f}%'.format(epoch,running_acc\/(len(valid_loader)*3)))\n   print('loss : {:.4f}'.format(running_loss\/len(valid_loader)))\n   print('recall: {:.4f}'.format(running_recall\/len(valid_loader)))\n   history.loc[epoch, 'valid_loss'] = losses[0]\n   history.loc[epoch, 'valid_acc'] = accs[0].cpu().numpy()\n   history.loc[epoch, 'valid_recall'] = total_recall\n   return  total_recall\n","200df613":"## A very simple loop to train for number of epochs it probably can be made more robust to save only the file with best valid loss \nhistory = pd.DataFrame()\nn_epochs = 1 ## 1 Epoch as sample . \"I am just a poor boy  , no GPU in reality \"\nvalid_recall = 0.0\nbest_valid_recall = 0.0\nfor epoch in range(n_epochs):\n    torch.cuda.empty_cache()\n    gc.collect()\n    train_recall = train(epoch,history)\n    valid_recall = evaluate(epoch,history)\n    if valid_recall > best_valid_recall:\n        print(f'Validation recall has increased from:  {best_valid_recall:.4f} to: {valid_recall:.4f}. Saving checkpoint')\n        torch.save(model.state_dict(), 'effnetb0_trial_stage1.pth') ## Saving model weights based on best validation accuracy.\n        best_valid_recall = valid_recall ## Set the new validation Recall score to compare with next epoch\n        \n        \n    #scheduler.step() ## Want to test with fixed learning rate .If you want to use scheduler please uncomment this .\n    ","3af501dc":"history.to_csv('history.csv')\nhistory.head()","19b77c08":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision import transforms,models\nfrom tqdm import tqdm_notebook as tqdm","a9139977":"## Load model for inferernce . \n#model.load_state_dict(torch.load('..\/input\/pytorch-efficientnet-starter-code\/effnetb0_trial_stage1.pth')) ","93c637de":"test = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/test.csv')","1574da3e":"#check https:\/\/www.kaggle.com\/iafoss\/image-preprocessing-128x128\n\ndef bbox(img):\n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n    return rmin, rmax, cmin, cmax\n\ndef crop_resize(img0, size=SIZE, pad=16):\n    #crop a box around pixels large than the threshold \n    #some images contain line at the sides\n    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n    #cropping may cut too much, so we need to add it back\n    xmin = xmin - 13 if (xmin > 13) else 0\n    ymin = ymin - 10 if (ymin > 10) else 0\n    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n    img = img0[ymin:ymax,xmin:xmax]\n    #remove lo intensity pixels as noise\n    img[img < 28] = 0\n    lx, ly = xmax-xmin,ymax-ymin\n    l = max(lx,ly) + pad\n    #make sure that the aspect ratio is kept in rescaling\n    img = np.pad(img, [((l-ly)\/\/2,), ((l-lx)\/\/2,)], mode='constant')\n    return cv2.resize(img,(size,size))","25b0a703":"class GraphemeDataset(Dataset):\n    def __init__(self, fname):\n        print(fname)\n        self.df = pd.read_parquet(fname)\n        self.data = 255 - self.df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        name = self.df.iloc[idx,0]\n        #normalize each image by its max val\n        img = (self.data[idx]*(255.0\/self.data[idx].max())).astype(np.uint8)\n        img = crop_resize(img)\n        img = img.astype(np.float32)\/255.0\n        return img, name","244c570c":"## All test data\ntest_data = ['\/kaggle\/input\/bengaliai-cv19\/test_image_data_0.parquet','\/kaggle\/input\/bengaliai-cv19\/test_image_data_1.parquet','\/kaggle\/input\/bengaliai-cv19\/test_image_data_2.parquet',\n             '\/kaggle\/input\/bengaliai-cv19\/test_image_data_3.parquet']","adb605e3":"%%time\n## Inference a little faster using @Iafoss and  @peters technique\nrow_id,target = [],[]\nfor fname in test_data:\n    #data = pd.read_parquet(f'\/kaggle\/input\/bengaliai-cv19\/{fname}')\n    test_image = GraphemeDataset(fname)\n    dl = torch.utils.data.DataLoader(test_image,batch_size=128,num_workers=4,shuffle=False)\n    with torch.no_grad():\n        for x,y in tqdm(dl):\n            x = x.unsqueeze(1).float().cuda()\n            p1,p2,p3 = model(x)\n            p1 = p1.argmax(-1).view(-1).cpu()\n            p2 = p2.argmax(-1).view(-1).cpu()\n            p3 = p3.argmax(-1).view(-1).cpu()\n            for idx,name in enumerate(y):\n                row_id += [f'{name}_vowel_diacritic',f'{name}_grapheme_root',\n                           f'{name}_consonant_diacritic']\n                target += [p1[idx].item(),p2[idx].item(),p3[idx].item()]\n                \nsub_df = pd.DataFrame({'row_id': row_id, 'target': target})\nsub_df.to_csv('submission.csv', index=False)\nsub_df.head(20)","e7e6a164":"#### Inference part ","8105846a":"#### Model creation part ","5950dace":"#### Training and validation part ","ec22cfdb":"#### Data setup part ","f850fe42":"V1: This is an implementation of shake-shake from Heng's Discussion pointers . It is present in the discussion , however I will provide the github references somewhere .\n","9093692e":"### Simple step by step approach with shake-shake regularization for Bengali AI competition . If its useful for you please upvote .\n\nThe paper is here https:\/\/arxiv.org\/abs\/1705.07485\n\n![image.png](attachment:image.png)\n\n###  Shake-Shake regularization is actually an augmentation method of internal representation of features inside resnet . THe above image shows the augmentations added in blocks of resnets .\n\nImage Courtesy :\nhttps:\/\/towardsdatascience.com\/review-shake-shake-regularization-image-classification-d22bb8587953"}}