{"cell_type":{"f908c449":"code","9d2a78bb":"code","a5512bb2":"code","8659679c":"code","987ca070":"code","00b91878":"code","52990a1a":"code","1e6ab81b":"code","645bbfc7":"code","78768cb7":"markdown"},"source":{"f908c449":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom transformers import AutoTokenizer\nimport tensorflow as tf\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras import Model\nimport tensorflow.keras.backend as K\nfrom sklearn.metrics import mean_squared_error\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport string\nimport gc","9d2a78bb":"df_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ndf_data.head()","a5512bb2":"fig, axes = plt.subplots(1, 2,figsize=(15, 5))\ndf_data['target'].hist(ax = axes[0], bins = 100)\ndf_data['standard_error'].hist(ax = axes[1], bins = 100)","8659679c":"class BERTModel:\n    def __init__(self, df_train, df_test, model_config):\n        self.df_data = df_train\n        self.df_test = df_test\n        self.model_path = model_config['model_path']\n        self.batch_size = model_config['batch_size']\n        self.seq_len = model_config['seq_len']\n        self.text_column = model_config['text_column']\n        self.n_folds = model_config['n_folds']\n        self.epochs = model_config['epochs']\n        self.remove_swords = model_config['remove_swords']\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        self.oof_pred = []\n        self.oof_labels = []\n        self.history_list = []\n        self.test_pred = []\n        self.history_list = []\n    \n    @staticmethod\n    def sample_target(features, target):\n        mean, stddev = target\n        sampled_target = tf.random.normal([], mean=tf.cast(mean, dtype=tf.float32), \n                                      stddev=tf.cast(stddev, dtype=tf.float32),\n                                      dtype=tf.float32)\n        return (features, sampled_target)\n    \n    @staticmethod\n    def custom_standardization(text):\n        text = text.lower() # if encoder is uncased\n        text = text.strip()\n        return text\n    \n    @staticmethod\n    def clean_paragraph(paragraph, stop_words, lemmatizer):\n        \"\"\"Cleans paragraph before tokenization\n        Source: https:\/\/www.kaggle.com\/andradaolteanu\/i-commonlit-explore-xgbrf-repeatedfold-model \"\"\"\n\n        # Tokenize & convert to lower case\n        tokens = word_tokenize(paragraph)\n        tokens = [t.lower() for t in tokens]\n\n        # Remove punctuation & non alphabetic characters from each word\n        table = str.maketrans('', '', string.punctuation)\n        tokens = [t.translate(table) for t in tokens]\n        tokens = [t for t in tokens if t.isalpha()]\n\n        # Filter out stopwords\n        tokens = [t for t in tokens if not t in stop_words]\n\n        # Lemmatizer\n        tokens_lemm = [lemmatizer.lemmatize(t) for t in tokens]\n        \n        return \" \".join(tokens_lemm)\n    \n    def get_dataset(self, df_data, labeled = True, ordered = False, repeated = False,\n                   is_sampled = False):\n        \"\"\"\n        Return a Tensorflow dataset ready for training or inference.\n        \"\"\"\n        if self.remove_swords:\n            df_temp = df_data.copy()\n            stop_words = stopwords.words('english')\n            lemmatizer = WordNetLemmatizer()\n            df_temp[self.text_column] = df_temp[self.text_column].apply(\n                lambda x: self.clean_paragraph(x, stop_words, lemmatizer))\n            del stop_words, lemmatizer\n            gc.collect()\n            text = df_temp[self.text_column].to_list()\n        else:\n            text = [self.custom_standardization(text) for text in df_data[self.text_column]]\n        \n        tokenized_inputs = self.tokenizer(text, max_length=self.seq_len,\n                                          truncation=True, padding='max_length',\n                                          return_tensors='tf')\n        \n        if labeled:\n            dataset = tf.data.Dataset.from_tensor_slices((\n                        {'input_ids': tokenized_inputs['input_ids'], \n                        'attention_mask': tokenized_inputs['attention_mask']}, \n                        (df_data['target'], df_data['standard_error'])\n                      ))\n            if is_sampled:\n                dataset = dataset.map(self.sample_target,\n                                  num_parallel_calls=tf.data.AUTOTUNE)\n        else:\n            dataset = tf.data.Dataset.from_tensor_slices(\n                        {'input_ids': tokenized_inputs['input_ids'], \n                        'attention_mask': tokenized_inputs['attention_mask']})\n        \n        if repeated:\n            dataset = dataset.repeat()\n        if not ordered:\n            dataset = dataset.shuffle(1024)\n        dataset = dataset.batch(self.batch_size)\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        return dataset\n    \n    def create_model(self, encoder):\n        \"\"\" Returns prepared model from encoder\"\"\"\n        input_ids = Input(shape=(self.seq_len,), dtype=tf.int32, name='input_ids')\n        input_attention_mask = Input(shape=(self.seq_len,), dtype=tf.int32,\n                                     name='attention_mask')\n        outputs = encoder({'input_ids': input_ids, \n                           'attention_mask': input_attention_mask})\n    \n        final_model = Model(inputs=[input_ids, input_attention_mask],\n                            outputs=outputs)\n    \n        final_model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n            loss=tf.keras.losses.MeanSquaredError(),\n            metrics=tf.metrics.RootMeanSquaredError(),\n            )\n        return final_model\n    \n    def train_model(self):\n        \n        skf = KFold(n_splits=self.n_folds, shuffle=True, random_state=0)\n        \n        for fold,(idxT, idxV) in enumerate(skf.split(self.df_data)):\n            \n            K.clear_session()\n            encoder = TFAutoModelForSequenceClassification.from_pretrained(\n                                        self.model_path, num_labels=1)\n            model = self.create_model(encoder)\n            \n            model_path = f'model_{fold}.h5'\n            es = EarlyStopping(monitor='val_root_mean_squared_error',\n                               mode='min', patience=5,\n                               restore_best_weights=True, verbose=1)\n            checkpoint = ModelCheckpoint(model_path,\n                                         monitor='val_root_mean_squared_error',\n                                         mode='min', save_best_only=True,\n                                         save_weights_only=True)\n            \n            history = model.fit(\n                x=self.get_dataset(self.df_data.loc[idxT], repeated=True, is_sampled=True), \n                validation_data=self.get_dataset(self.df_data.loc[idxV], ordered=True), \n                steps_per_epoch=50, callbacks=[es, checkpoint], epochs=self.epochs,\n                verbose=2).history\n            \n            self.history_list.append(history)\n            \n            model.load_weights(model_path)\n            \n            # OOF predictions\n            valid_ds = self.get_dataset(self.df_data.loc[idxV],ordered=True)\n            self.oof_labels.append(\n                [target[0].numpy() for sample, target in iter(valid_ds.unbatch())])\n            x_oof = valid_ds.map(lambda sample, target: sample)\n            self.oof_pred.append(model.predict(x_oof)['logits'])\n            \n            # Test predictions\n            test_ds = self.get_dataset(self.df_test, labeled=False, ordered=True)\n            x_test = test_ds.map(lambda sample: sample)\n            self.test_pred.append(model.predict(x_test)['logits'])\n            \n            del encoder, history, x_test, test_ds, valid_ds, x_oof, model\n            gc.collect()\n    \n    def get_oof_results(self):\n        \n        y_true = np.concatenate(self.oof_labels)\n        y_preds = np.concatenate(self.oof_pred)\n        \n        for fold, history in enumerate(self.history_list):\n            print(f\"FOLD {fold+1} RMSE: {np.min(history['val_root_mean_squared_error']):.4f}\")\n        \n        oof_rmse = mean_squared_error(y_true, y_preds, squared=False)\n        print(f'OOF RMSE: {oof_rmse:.4f}')\n        return oof_rmse\n    \n    def get_submission(self):\n        \"\"\"Returns submision df\"\"\"\n        \n        submission = self.df_test[['id']]\n        submission['target'] = np.mean(self.test_pred, axis=0)\n        return submission","987ca070":"result_dict = {}\n\nmodel_path_dict = {\n    'bert':'\/kaggle\/input\/huggingface-bert\/bert-base-uncased\/',\n    'roberta':'\/kaggle\/input\/huggingface-roberta\/roberta-base\/',\n    'distil':'\/kaggle\/input\/huggingface-bert-variants\/distilbert-base-uncased\/distilbert-base-uncased\/'\n}\n\nsubmission_dict = {}\n\nfor remove_stopwords in [False]:\n    for model_name, model_path in model_path_dict.items():\n        data_dir = '..\/input\/commonlitreadabilityprize'\n        df_train = pd.read_csv('{}\/train.csv'.format(data_dir))\n        df_test = pd.read_csv('{}\/test.csv'.format(data_dir))\n\n        model_config = {'model_path':'\/kaggle\/input\/huggingface-roberta\/roberta-base\/',\n                        'batch_size':32,'seq_len': 256,\n                        'text_column':'excerpt','n_folds':5,'epochs':30,\n                        'remove_swords': False}\n        model_config['model_path'] = model_path\n        model_config['remove_swords'] = remove_stopwords\n\n        b = BERTModel(df_train, df_test, model_config)\n        b.train_model()\n        result_dict['{}_{}'.format(model_name, remove_stopwords)] = b.get_oof_results()\n        submission_dict['{}_{}'.format(model_name, remove_stopwords)] = b.get_submission()\n        del b\n        gc.collect()","00b91878":"plt.rcParams[\"figure.figsize\"] = (10,3)\nplt.bar(result_dict.keys(), result_dict.values())\nplt.title('Out of fold, RMSE')\nprint(result_dict)","52990a1a":"for submission_df in submission_dict.values():\n    print(submission_df.head())","1e6ab81b":"target = None\nfor submission_df in submission_dict.values():\n    if target is None:\n        target = submission_df['target'].values.copy()\n    else:\n        target += submission_df['target'].values\ntarget\/=3","645bbfc7":"submission_df['target'] = target\nsubmission_df.to_csv(\"submission.csv\", index=False)","78768cb7":"#### Most of the code in this notebook is the modified version of this notebook https:\/\/www.kaggle.com\/dimitreoliveira\/commonlit-readability-eda-roberta-tf-baseline\/\n\n#### The idea was to compare the performance of the classic BERT, DistilBERT, and RoBERTa using 5-fold CV, both with and without removing stop words."}}