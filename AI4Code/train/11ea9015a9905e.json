{"cell_type":{"85e14bcf":"code","551de634":"code","1a73aacf":"code","d24d2c52":"code","fd54e7a9":"code","b4a19649":"code","68e5a6d2":"code","b005e7f4":"code","2d8a2909":"code","e6bc9850":"code","0fd40d3f":"code","b5efe184":"code","21d19980":"code","f119b8ec":"code","59b98275":"code","5bc4ab2d":"code","ca011583":"code","74a5c52e":"code","b8715034":"code","f6d28164":"code","f2e7837e":"code","62f2d4be":"code","82e93a8a":"code","e4d9e741":"code","39fddfb7":"code","2c089789":"code","7891f148":"code","e84a4b6d":"code","2927d83f":"code","ab26bf75":"code","53206db5":"code","e4d8f69a":"code","be2a0e33":"code","017af040":"code","7299a681":"code","d953d5cc":"code","be538984":"code","0cd38759":"code","d90a38bd":"code","ed45e676":"code","25d49148":"code","daa1c62e":"code","49f74e5d":"code","b43571e7":"code","7d64be2a":"code","71476773":"code","d1006f8d":"code","3d28d68b":"code","955f6935":"code","f67e8f4b":"code","328cc919":"code","7fba572a":"code","8e3a80e3":"code","31aefe13":"code","742e35e5":"code","e52cb824":"code","4f39de87":"code","a62709c4":"code","013f0e73":"code","312e7f79":"code","b89f342c":"code","a36ef753":"code","e11185f0":"code","616d3f05":"code","394cca22":"code","73a643e0":"code","0f940670":"code","390c9f6c":"code","ea6b1d94":"code","d3c4847d":"code","f4b12881":"code","1cb0ffb0":"code","08c2c6d7":"code","0786ec1d":"code","4edaa8c6":"code","d75b5d44":"markdown","a36126d2":"markdown","ab7d9cf6":"markdown","827b2686":"markdown","8a8efdd2":"markdown","758f4b05":"markdown","9dec4516":"markdown","607e36e8":"markdown","b36688e8":"markdown","03e35a45":"markdown","b017e946":"markdown","b1937bfe":"markdown","128d12da":"markdown","ee898f31":"markdown","398f279c":"markdown","d7c6702a":"markdown","1ad620f9":"markdown","1543f324":"markdown","897dfb3b":"markdown","24a9084a":"markdown","f53f4ffc":"markdown","474e06ed":"markdown","fb543651":"markdown","40ce2b6d":"markdown","13bebf62":"markdown","9f5c4d5e":"markdown","b7bd1dc7":"markdown","6bb67383":"markdown","2c71dbef":"markdown","3e105573":"markdown","e1665c1f":"markdown","664c1c4e":"markdown","82fcee8f":"markdown","77ef037e":"markdown","c71c9277":"markdown","a55adc93":"markdown","40b42c54":"markdown","62ee99a4":"markdown","b8c10c9d":"markdown","bf474ea2":"markdown","79a5c626":"markdown","406b609a":"markdown","c92c9742":"markdown","c59a9686":"markdown","6589f170":"markdown","1c910135":"markdown","b72ceb83":"markdown","29598285":"markdown","ee874865":"markdown","8f487648":"markdown","d2b9e250":"markdown","0dc1f615":"markdown","a671c19a":"markdown","69d318e5":"markdown","2ca94ed8":"markdown","878ceed1":"markdown"},"source":{"85e14bcf":"from __future__ import unicode_literals\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nfrom collections import Counter\nfrom textblob import TextBlob\nimport re\nimport nltk\nimport xgboost as xgb\nimport numpy as np\nimport collections\nimport operator\nimport itertools\nimport os\n\n\nfrom nltk.corpus import stopwords\nimport nltk.stem.snowball\nst = nltk.stem.snowball.SnowballStemmer('english')\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nstemmer = SnowballStemmer('english')\n\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, learning_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.metrics import classification_report, accuracy_score ,confusion_matrix\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn import utils\n\nfrom yellowbrick.text import FreqDistVisualizer\n\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.models import Doc2Vec\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nfrom gensim import models\nfrom gensim.models.doc2vec import TaggedDocument\nimport pyLDAvis\nimport pyLDAvis.gensim as gensimvis\n\nfrom itertools import islice\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import utils\n\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\n\nimport spacy\nfrom spacy_langdetect import LanguageDetector\nnlp = spacy.load('en')\n\nfrom wordcloud import WordCloud \n\nimport chart_studio.plotly.plotly as py\nimport plotly.graph_objs as go\nimport chart_studio\n\n## Offline mode\nfrom plotly.offline import init_notebook_mode, iplot\nchart_studio.tools.set_credentials_file(username='avinashok', api_key='SZBL9GagG9yPYCwfSuDc')\ninit_notebook_mode(connected=True)","551de634":"#Directories\ndataDirectory = \"..\/input\/\"\n","1a73aacf":"# reading data file  \ndf = pd.read_csv(dataDirectory+'telecomagentcustomerinteractiontext\/'+'CustomerInteractionData.csv')\n\n#Randomly shuffling data\ndf = df.sample(len(df))\n\n#Copying the Raw comment column for Future Use\ndf['CustomerInteractionText'] = df['CustomerInteractionRawText']\n\n#Checking the initial 5 rows\ndf.head(5)","d24d2c52":"#Objects Used\ncommentTextColumn = 'CustomerInteractionText'\nagentAssignedColumn = 'AgentAssignedTopic'\nlocationID = 'LocationID'\ncallDuration = 'CallDurationSeconds' #In Seconds\nagentID = 'AgentID'\ncustomerID = 'CustomerID'\nrawText = 'CustomerInteractionRawText'","fd54e7a9":"# Identifying Primary Key\n\nprint(\"Number of Columns in the Dataset = \" + str(len(df.columns)))\n\nuniqueColumns=[]\nfor col in df.columns.to_list():\n    if len(df[str(col)].unique())==df.shape[0]:\n        uniqueColumns.append(col)\nif len(uniqueColumns)==1:\n    primaryKeyColumn = str(uniqueColumns[0])\n    print(\"Primary Key = \"+primaryKeyColumn)","b4a19649":"# Checking Datatypes of each columns in the dataset\nprint(df.dtypes)\n\n# Specifying datatypes we want for each column\ndataTypeDictionary = {\n    primaryKeyColumn: 'int64',\n    commentTextColumn: 'object',\n    rawText:'object',\n    agentAssignedColumn: 'object',             \n    locationID: 'int64',\n    callDuration: 'int64',\n    agentID: 'object',\n    customerID: 'object'\n    \n }","68e5a6d2":"duplicatesCount = {}\nfor col in df.columns.to_list():\n    duplicatesCount[col] = [((df.duplicated(col).sum()\/len(df))*100),100-((df.duplicated(col).sum()\/len(df))*100)]\n\nnullCounter = {}\nfor col in df.columns.to_list():\n    count = 0\n    for cell in df[str(col)]:\n        if cell=='?' or cell==\"\":   # or len(str(cell))==1\n            count= count +1\n    nullCounter[col]=[float(count\/len(df))*100,100-float(count\/len(df))*100]","b005e7f4":"def dataQualityCheck(checkName, columnName):\n    if checkName == \"Null Values\":\n        # create data\n        names='Null Values', 'Non Null Values',\n        size=np.array(nullCounter[columnName])\n        print(\"Null Values Data Quality Check for \"+str(columnName))\n        def absolute_value(val):\n            a  = np.round(val\/100.*size.sum(), 0)\n            return a\n        # Create a circle for the center of the plot\n        my_circle=plt.Circle( (0,0), 0.7, color='white')\n\n        # Custom colors --> colors will cycle\n        plt.pie(size, labels=names, colors=['red','green'],autopct=absolute_value)\n        p=plt.gcf()\n        p.gca().add_artist(my_circle)\n        plt.show();\n    elif checkName ==\"Duplicates\": \n                # create data\n        names='Duplicate Values', 'Unique Values Values',\n        size=np.array(duplicatesCount[columnName])\n        print(\"Duplicate Value Data Quality check for \"+str(columnName))\n        def absolute_value(val):\n            a  = np.round(val\/100.*size.sum(), 0)\n            return a\n        # Create a circle for the center of the plot\n        my_circle=plt.Circle( (0,0), 0.7, color='white')\n\n        # Custom colors --> colors will cycle\n        plt.pie(size, labels=names, colors=['red','green'],autopct=absolute_value)\n        p=plt.gcf()\n        p.gca().add_artist(my_circle)\n        plt.show();\n    elif checkName == \"Details\":\n        print(\"Details of the Column: \\n \")\n        print(\"Original Datatype should be \"+dataTypeDictionary[columnName]+\"\\n\")\n        print(\"Datatype in the data is \"+str(df[str(columnName)].dtypes)+\"\\n\")\n    elif checkName == \"Range\":\n        if str(df[str(columnName)].dtypes)=='int64' or str(df[str(columnName)].dtypes)=='datetime64[ns]':\n            print(\"Maximum Value is \"+str(df[str(columnName)].max())+\" \\n \")\n            print(\"Minimum Value is \"+str(df[str(columnName)].min()))\n        else:\n            print(\"Since the Datatype of column \"+str(columnName)+\" is not numeric in the given data, Range cannot be calculated.\")\n    \n    \ndef dQexecute(columnName):\n    print(\"\\n Name of the Column \"+str(columnName)+\"\\n \\n\")\n    dataQualityCheck(\"Details\",columnName)\n    dataQualityCheck(\"Null Values\",columnName)\n    dataQualityCheck(\"Duplicates\",columnName)\n    dataQualityCheck(\"Range\",columnName)\n    print(\"*****************\")","2d8a2909":"for col in df.columns.to_list():\n    dQexecute(col)","e6bc9850":"z = {}\nuniqueTopics = list(df[agentAssignedColumn].unique())\nfor i in uniqueTopics:\n    z[i]=i\n\ndata = [go.Bar(x = df[agentAssignedColumn].map(z).unique(),y = df[agentAssignedColumn].value_counts().values,\n        marker= dict(colorscale='Jet',color = df[agentAssignedColumn].value_counts().values),text='Number of Calls for this reason')]\n\nlayout = go.Layout(title='Reasonwise Call Distribution')\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='TelecomCallDistribution')","0fd40d3f":"df['totalwords'] = df[commentTextColumn].str.split().str.len()\ndef reasonCodeLevelWordCount(reasonCode, parameter):\n    dfReasonCodeSubset = df[df[agentAssignedColumn]==reasonCode]\n    if parameter == 'mean':\n        return float(dfReasonCodeSubset.describe()['totalwords'][1])\n    elif parameter == 'median':\n        return float(dfReasonCodeSubset.describe()['totalwords'][5])","b5efe184":"# Mean Word Count\nreasonCodeDict = {}\nfor topic in uniqueTopics:\n    reasonCodeDict[str(topic)]=float(reasonCodeLevelWordCount(topic, 'mean'))\nplt.figure(figsize=(20,20))\nplt.title(\"Mean Word Frequency for each Topic\", fontdict=None, loc='center')\nplt.bar(reasonCodeDict.keys(), reasonCodeDict.values(), width = 0.1  , color='g')\nplt.show()\n\nprint(\"\\n\\n ******************** \\n\\n \")\n\n#Median Word Count (Optional)\nreasonCodeDict = {}\nfor topic in uniqueTopics:\n    reasonCodeDict[str(topic)]=float(reasonCodeLevelWordCount(topic, 'median'))\nplt.figure(figsize=(20,20))\nplt.title(\"Median Word Frequency for each Topic\", fontdict=None, loc='center')\nplt.bar(reasonCodeDict.keys(), reasonCodeDict.values(), width = 0.1  , color='g')\nplt.show()","21d19980":"vectorizer = CountVectorizer()\ndocs       = vectorizer.fit_transform(df[commentTextColumn])\nfeatures   = vectorizer.get_feature_names()\nplt.figure(figsize=(12,8))\nvisualizer = FreqDistVisualizer(features=features)\nvisualizer.fit(docs)\nfor label in visualizer.ax.texts:\n    label.set_size(20)\nvisualizer.poof()","f119b8ec":"%%time\n\nabbrevationDictionary = {'Cus': 'customer', 'cus': 'customer',\n                        'Xferred':'transferred', 'xferred': 'transferred'} \n\n#Function to Standardize Text\ndef objectStandardization(input_text):\n    words = str(input_text).split() \n    new_words = [] \n    for word in words:\n        word = re.sub('[^A-Za-z0-9\\s]+', ' ', word) #remove special characters\n        if word.lower() in abbrevationDictionary:\n            word = abbrevationDictionary [word.lower()]\n        new_words.append(word) \n    new_text = \" \".join(new_words) \n    return new_text\n\ndf[commentTextColumn] = df[commentTextColumn].apply(objectStandardization)\n\nprint(df[commentTextColumn].head(5))","59b98275":"%%time\n\n# Function to extract Names of persons, organizations, locations, products etc. from the dataset\ndef entityCollector(df):\n    listOfNames = []\n    for index, row in df.iterrows():\n        doc = nlp(row[str(commentTextColumn)])\n        fil = [(i.label_.lower(), i) for i in doc.ents if i.label_.lower() in [\"person\", \"gpe\", \"product\"]] # Extracts Person Names, Organization Names, Location, Product names\n        if fil:\n            listOfNames.append(fil)\n        else:\n            continue\n    flat_list = [item for sublist in listOfNames for item in sublist]\n    entityDict = {}\n    for a, b in list(set(flat_list)): \n        entityDict.setdefault(a, []).append(b)\n    return entityDict\n\nentityDict = entityCollector(df)\n\nprint(\"\\n Types of entities present in the data are: \"+\", \".join(list(entityDict.keys()))+\" \\n\")\n","5bc4ab2d":"%%time\nfor entity in list(entityDict.keys()):\n    entityDict[entity] = [str(i) for i in entityDict[entity]]\n\nignoreWords = []\nfor key in entityDict.keys():\n    ignoreWords.append(entityDict[key])\nignoreWords = [item for sublist in ignoreWords for item in sublist]\n\nprint(\"Number of words in Custom Stopword list = \"+str(len(ignoreWords)))","ca011583":"%%time\ndef languageDistribution(df):\n    nlp = spacy.load(\"en\")\n    nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n    df['language']=''\n    language = []\n    for index, row in df.iterrows():\n        text = row[str(commentTextColumn)]\n        doc = nlp(text)\n        language.append(str(doc._.language['language']))\n    df['language'] = language\n    return df\n\ndf = languageDistribution(df)\nlangDict = df.groupby('language')[str(primaryKeyColumn)].nunique().to_dict()\n\notherLanguagesList = list(langDict.keys()).remove('en')\nprint(\"Some sample other language texts: \\n\")\nfor lang in list(langDict.keys()):\n    print(str(df[df['language']==str(lang)].values.tolist()[0]))\n\n##Plot a Pie Distribution of Language Distribution\nbin_percent = pd.DataFrame(df['language'].value_counts(normalize=True) * 100)\nplot = bin_percent.plot.pie(y='language', figsize=(10, 10), autopct='%1.1f%%')\n\n#Dropping only the row with Spanish text\ndf = df.drop(df[df['language'] == 'es'].index);","74a5c52e":"%%time\n#Function to extract Alpha numeric words\ndef alphanumericExtractor(input_text):\n    words = str(input_text).split()\n    alphanumericWordlist = []\n    for word in words:\n        word = re.sub('[^A-Za-z0-9\\s]+', '', word.lower()) #remove special characters\n        word = re.sub(r'[^\\x00-\\x7F]+',' ', word) # remove ascii\n        if not word.isdigit() and any(ch.isdigit() for ch in word):\n            alphanumericWordlist.append(word)\n        else:\n            continue\n    return alphanumericWordlist\n\n#Function to get the frequency of Alphanumeric words in the data\ndef alphanumericFrequency(df, commentTextColumnName):\n    alphanumericWordsList = []\n    for index, row in df.iterrows():\n        if alphanumericExtractor(row[str(commentTextColumnName)]):\n            alphanumericWordsList.append(alphanumericExtractor(row[str(commentTextColumnName)]))\n        else:\n            continue\n    flat_list = [item for sublist in alphanumericWordsList for item in sublist]\n    counts = Counter(flat_list)\n    countsdict = dict(counts)\n    return countsdict\n\n# Final list of alphanumeric words\nalphanumericWordFreqDict = alphanumericFrequency(df, commentTextColumn)\n    \n# To plot the distribution\ntotalWordcount = len(alphanumericWordFreqDict)\n\ntopWordCount = input('How many top words do you want? maximum= '+str(totalWordcount)+ ' \\n ')\n# topWordCount = totalWordcount\n\nalphanumericWordFreqDictTop = dict(sorted(alphanumericWordFreqDict.items(), key=operator.itemgetter(1), reverse=True)[:int(topWordCount)])\nprint(alphanumericWordFreqDictTop)\n\nplt.figure(figsize=(20,20))\nplt.title('Frequency of AlphaNumeric Words in the Dataset', fontdict=None, loc='center')\nplt.bar(alphanumericWordFreqDictTop.keys(), alphanumericWordFreqDictTop.values(), width = 0.1  , color='b');\nplt.show();\n\n#Updating Custom stopword list with Alphanumeric words\nignoreWords = ignoreWords + list(alphanumericWordFreqDict.keys())","b8715034":"def clean_text(newDesc):\n    newDesc = re.sub('[^A-Za-z\\s]+', '', newDesc) #remove special characters\n    newDesc = re.sub(r'[^\\x00-\\x7F]+','', newDesc) # remove ascii\n    newDesc = ' '.join( [w for w in newDesc.split() if len(w)>1] )  \n    newDesc = newDesc.split()\n    cleanDesc = [str(w) for w in newDesc if w not in ignoreWords] #remove entity names, alphanumeric words\n    return ' '.join(cleanDesc)\n\ndf[commentTextColumn] = df[commentTextColumn].apply(clean_text)\ndf.head()","f6d28164":"%%time\n\ndef textAutocorrect(df, columnName):\n    df[str(columnName)] = df[str(columnName)].apply(lambda txt: ''.join(TextBlob(txt).correct()))\n    return True\n\ntextAutocorrect(df, commentTextColumn)","f2e7837e":"stops = nlp.Defaults.stop_words\ndefault_stopwords = stopwords.words('english')\ncustomStopWords = {'PRON', 'pron'}\nstops.update(set(default_stopwords))\nstops.update(set(customStopWords))\n\ndef normalize(comment, lowercase, remove_stopwords):\n    if lowercase:\n        comment = comment.lower()\n    comment = nlp(comment)\n    lemmatized = list()\n    for word in comment:\n        lemma = word.lemma_.strip()\n        if lemma:\n            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n                lemmatized.append(lemma)\n    normalizedSentence = \" \".join(lemmatized)\n    normalizedSentence = re.sub('[^A-Za-z\\s]+', '', normalizedSentence)  # remove special characters\n    normalizedSentence = normalizedSentence.split()\n    cleanDesc = [str(w) for w in normalizedSentence if w not in stops] #remove PRON\n    return \" \".join(cleanDesc)\n\ndf[commentTextColumn] = df[commentTextColumn].apply(normalize, lowercase=True, remove_stopwords=True)\ndf.head()","62f2d4be":"# Removing Null Comments\ndef removeNullValueCommentText(df, columnName):\n    initialLength = len(df)\n    df = df[pd.notnull(df[columnName])]\n    finalLength = len(df)\n    print(\"\\n Number of rows with Null Value in the column '\"+str(columnName)+\"' are: \"+str(initialLength-finalLength))\n    return df\ndf = removeNullValueCommentText(df, commentTextColumn)\nprint(len(df))","82e93a8a":"# Removing duplicate comments keeping the first one\ndef removeDuplicateComments(df, columnName, agentAssignedColumn):\n    initialDf = df.copy()\n    initialLength = len(initialDf)\n    finalDf = df.drop_duplicates(subset=[columnName], keep='first')\n    finalLength = len(finalDf)\n    print(\"\\n Number of rows with duplicate comments in the column '\"+str(columnName)+\"' are: \"+str(initialLength-finalLength))\n    print(\"\\n The Level 3 Reason Codes for the dropped rows are given below: \\n\")\n    droppedDF = initialDf[~initialDf.apply(tuple,1).isin(finalDf.apply(tuple,1))]\n    print(droppedDF[agentAssignedColumn].value_counts())\n    return finalDf,droppedDF\n\ndf,droppedDF = removeDuplicateComments(df, commentTextColumn, agentAssignedColumn)\nprint(len(df))","e4d9e741":"# Removing comments with just one word. (Like #CALL?)\ndef removingShortComments(df, columnName, agentAssignedColumn, numberOfWords = 1):\n    initialDf = df.copy()\n    initialLength = len(initialDf)\n    finalDf = df[~(df[str(columnName)].str.split().str.len()<(int(numberOfWords)+1))]\n    finalLength = len(finalDf)\n    print(\"\\n Number of rows with short comments in the column '\"+str(columnName)+\"' are: \"+str(initialLength-finalLength))\n    print(\"\\n The Level 3 Reason Codes for the dropped rows are given below: \\n\")\n    droppedDF = initialDf[~initialDf.apply(tuple,1).isin(finalDf.apply(tuple,1))]\n    print(droppedDF[agentAssignedColumn].value_counts())\n    return finalDf,droppedDF\n\ndf,droppedDF = removingShortComments(df, commentTextColumn, agentAssignedColumn)\nprint(len(df))","39fddfb7":"vectorizer = CountVectorizer()\ndocs       = vectorizer.fit_transform(df[commentTextColumn])\nfeatures   = vectorizer.get_feature_names()\nplt.figure(figsize=(12,8))\nvisualizer = FreqDistVisualizer(features=features)\nvisualizer.fit(docs)\nfor label in visualizer.ax.texts:\n    label.set_size(20)\nvisualizer.poof()","2c089789":"def wordFrequency(reasonCode):\n    return (df[df[agentAssignedColumn]==str(reasonCode)][commentTextColumn].str.split(expand=True).stack().value_counts())\n\n\ndef wordFrequencyListPlot(reasonCode, plot = False):\n    wordFreqDict = df[df[agentAssignedColumn]==str(reasonCode)][commentTextColumn].str.split(expand=True).stack().value_counts().to_dict()\n    wordFreqDictMostCommon = dict(collections.Counter(wordFreqDict).most_common(10)) #Considering only Top 10 words\n    print(list(wordFreqDictMostCommon.keys()))\n    if plot == True:\n        plt.title(str(reasonCode), fontdict=None, loc='center')\n        plt.bar(wordFreqDictMostCommon.keys(), wordFreqDictMostCommon.values(), width = 0.1  , color='b');\n        plt.figure(figsize=(10,10))\n        plt.show();\n    return list(wordFreqDictMostCommon.keys())","7891f148":"for reasoncode in uniqueTopics:\n    print(reasoncode)\n    wordFrequencyListPlot(reasoncode, plot = True)","e84a4b6d":"def wordCloudGenerator(df, reasonCode, save = False):\n    dfReasonCodeSubset = df[df[agentAssignedColumn]==reasonCode]\n    wordcloud = WordCloud(max_words=50,background_color='white',max_font_size = 50,width=100, height=100).generate(' '.join(dfReasonCodeSubset[commentTextColumn]))\n    plt.imshow(wordcloud)\n    plt.title(str(reasonCode), fontdict=None, loc='center')\n    plt.figure(figsize=(50,50))\n    plt.axis(\"off\")\n    plt.show();\n    if save:\n        plt.savefig('wordCloud'+str(reasonCode)+'.png', bbox_inches='tight')\n    ","2927d83f":"for topic in uniqueTopics:\n    wordCloudGenerator(df, topic) #,save = True , if you want to save the Word Clouds","ab26bf75":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(df[agentAssignedColumn].values)\nX_train, X_test, y_train, y_test = train_test_split(df[commentTextColumn].values, y,stratify=y,random_state=42, test_size=0.1)","53206db5":"tfidf = TfidfVectorizer(strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 3),use_idf=True,smooth_idf=True,sublinear_tf=True,stop_words = 'english')\n# Fit and transform Tf-idf to both training and test sets\ntfidf.fit(list(X_train) + list(X_test))\nX_train_tfidf =  tfidf.transform(X_train) \nX_test_tfidf = tfidf.transform(X_test)","e4d8f69a":"countvec = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 3), stop_words = 'english', binary=True)\n# Fit and transform CountVectorizer to both training and test sets\ncountvec.fit(list(X_train) + list(X_test))\nX_train_countvec =  countvec.transform(X_train) \nX_test_countvec = countvec.transform(X_test)","be2a0e33":"print(\"Total Number of Words in the column \"+commentTextColumn+\" is \"+str(df[commentTextColumn].apply(lambda x: len(x.split(' '))).sum()))","017af040":"X = df[commentTextColumn]\ny = df[agentAssignedColumn]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 30)","7299a681":"%time\nlogreg = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', LogisticRegression(n_jobs=1, C=1e5)),])\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","d953d5cc":"%time\nnb = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB(alpha=0.01)),])\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","be538984":"%time\nsgd = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-2, random_state=42, max_iter=5, tol=None)),])\nsgd.fit(X_train, y_train)\n\ny_pred = sgd.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","0cd38759":"forest = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', RandomForestClassifier(max_features='sqrt',n_estimators=1000, max_depth=3,random_state=0)),])\nforest.fit(X_train, y_train)\ny_pred = forest.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","d90a38bd":"xgboost = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', xgb.XGBClassifier(n_jobs=1,max_depth=3,learning_rate=0.01,n_estimators=1000)),])\nxgboost.fit(X_train, y_train)\ny_pred = xgboost.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","ed45e676":"%time\n\nsvc = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', SVC(gamma='scale', decision_function_shape='ovo'))])\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","25d49148":"#Evaluating Logistic Regression, since it has comparatevely better accuracy(83%)\nlogregclf = LogisticRegression(n_jobs=1, C=0.5)\nlogregclf.fit(X_train_tfidf, y_train)\ny_pred = logregclf.predict(X_test_tfidf)\nprint(\"---Misclassified Examples---\")\nfor x, y, y_hat in zip(X_test, lbl_enc.inverse_transform(y_test), lbl_enc.inverse_transform(y_pred)):\n    if y != y_hat:\n        print(f'Cleaned Comment: {x} | Original Topic: {y} | Predicted Topic: {y_hat}')","daa1c62e":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\nX, y = X_train_tfidf, y_train\ntitle = \"Learning Curves (Logistic Regression)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\nestimator = LogisticRegression()\nplot_learning_curve(estimator, title, X, y, (0.5, 1.01), cv=cv, n_jobs=10)\nplt.show();","49f74e5d":"%time\nwv = gensim.models.KeyedVectors.load_word2vec_format(dataDirectory+\"\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin.gz\", binary=True)\nwv.init_sims(replace=True)\nlist(islice(wv.vocab, 13030, 13050))","b43571e7":"# The common way is to average the two word vectors. BOW based approaches which includes averaging.\ndef word_averaging(wv, words):\n    all_words, mean = set(), []\n    \n    for word in words:\n        if isinstance(word, np.ndarray):\n            mean.append(word)\n        elif word in wv.vocab:\n            mean.append(wv.syn0norm[wv.vocab[word].index])\n            all_words.add(wv.vocab[word].index)\n\n    if not mean:\n        logging.warning(\"cannot compute similarity with no input %s\", words)\n        # FIXME: remove these examples in pre-processing\n        return np.zeros(wv.vector_size,)\n\n    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n    return mean\n\ndef  word_averaging_list(wv, text_list):\n    return np.vstack([word_averaging(wv, post) for post in text_list ])\n\ndef w2v_tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text, language='english'):\n        for word in nltk.word_tokenize(sent, language='english'):\n            if len(word) < 2:\n                continue\n            tokens.append(word)\n    return tokens","7d64be2a":"train, test = train_test_split(df, test_size=0.3, random_state = 42)\n\ntest_tokenized = test.apply(lambda r: w2v_tokenize_text(r[commentTextColumn]), axis=1).values\ntrain_tokenized = train.apply(lambda r: w2v_tokenize_text(r[commentTextColumn]), axis=1).values\n\nX_train_word_average = word_averaging_list(wv,train_tokenized)\nX_test_word_average = word_averaging_list(wv,test_tokenized)","71476773":"%time\nlogreg = LogisticRegression(n_jobs=1, C=1e5)\nlogreg = logreg.fit(X_train_word_average, train[agentAssignedColumn])\ny_pred = logreg.predict(X_test_word_average)\n\nprint('accuracy %s' % accuracy_score(y_pred, test[agentAssignedColumn]))\nprint(classification_report(test[agentAssignedColumn], y_pred,target_names=uniqueTopics))","d1006f8d":"# Doc2vec, taking the linear combination of every term in the document creates a random walk with bias process in the word2vec space.\ndef label_sentences(corpus, label_type):\n    \"\"\"\n    Gensim's Doc2Vec implementation requires each document\/paragraph to have a label associated with it.\n    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n    a dummy index of the post.\n    \"\"\"\n    labeled = []\n    for i, v in enumerate(corpus):\n        label = label_type + '_' + str(i)\n        labeled.append(TaggedDocument(v.split(), [label]))\n    return labeled","3d28d68b":"X_train, X_test, y_train, y_test = train_test_split(df[commentTextColumn], df[agentAssignedColumn], random_state=0, test_size=0.3)\nX_train = label_sentences(X_train, 'Train')\nX_test = label_sentences(X_test, 'Test')\n\nall_data = X_train + X_test\nall_data[:2]","955f6935":"model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\nmodel_dbow.build_vocab([x for x in tqdm(all_data)])\n\nfor epoch in range(30):\n    model_dbow.train([x for x in tqdm(all_data)], total_examples=len(all_data), epochs=1)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha\n","f67e8f4b":"def get_vectors(model, corpus_size, vectors_size, vectors_type):\n    \"\"\"\n    Get vectors from trained doc2vec model\n    :param doc2vec_model: Trained Doc2Vec model\n    :param corpus_size: Size of the data\n    :param vectors_size: Size of the embedding vectors\n    :param vectors_type: Training or Testing vectors\n    :return: list of vectors\n    \"\"\"\n    vectors = np.zeros((corpus_size, vectors_size))\n    for i in range(0, corpus_size):\n        prefix = vectors_type + '_' + str(i)\n        vectors[i] = model.docvecs[prefix]\n    return vectors\n\ntrain_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\ntest_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')\n\nlogreg = LogisticRegression(n_jobs=1, C=1e9)\nlogreg = logreg.fit(train_vectors_dbow, y_train)\ny_pred = logreg.predict(test_vectors_dbow)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=uniqueTopics))","328cc919":"train_size = int(len(df) * .7)\nprint (\"Train size: %d\" % train_size)\nprint (\"Test size: %d\" % (len(df) - train_size))\n\ntrain_comments = df[commentTextColumn][:train_size]\ntrain_topics = df[agentAssignedColumn][:train_size]\n\ntest_comments = df[commentTextColumn][train_size:]\ntest_topics = df[agentAssignedColumn][train_size:]\n\nmax_words = 500\ntokenize = text.Tokenizer(num_words=max_words, char_level=False)","7fba572a":"%time\ntokenize.fit_on_texts(train_comments) # only fit on train\nx_train = tokenize.texts_to_matrix(train_comments)\nx_test = tokenize.texts_to_matrix(test_comments)\n\nencoder = LabelEncoder()\nencoder.fit(train_topics)\ny_train = encoder.transform(train_topics)\ny_test = encoder.transform(test_topics)\n\nnum_classes = np.max(y_train) + 1\ny_train = utils.to_categorical(y_train, num_classes)\ny_test = utils.to_categorical(y_test, num_classes)\n\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)","8e3a80e3":"batch_size = 10\nepochs = 10\n\n# Build the model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n\nscore = model.evaluate(x_test, y_test,batch_size=batch_size, verbose=1)\nprint('Test accuracy:', score[1])","31aefe13":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 500\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 15\n# This is fixed.\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df[commentTextColumn].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nX = tokenizer.texts_to_sequences(df[commentTextColumn].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\n\nY = pd.get_dummies(df[agentAssignedColumn]).values\nprint('Shape of label tensor:', Y.shape)","742e35e5":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.30, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","e52cb824":"# Build the model\nmodel = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","4f39de87":"epochs = 10\nbatch_size = 5\n\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n\naccr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","a62709c4":"plt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show();\n\nplt.title('Accuracy')\nplt.plot(history.history['acc'], label='train')\nplt.plot(history.history['val_acc'], label='test')\nplt.legend()\nplt.show();","013f0e73":"new_complaint = ['Avinash want to cancel service.']\nseq = tokenizer.texts_to_sequences(new_complaint)\npadded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\npred = model.predict(padded)\nlabels = uniqueTopics\nprint(pred, labels[np.argmax(pred)])","312e7f79":"def removeFrequentOccuringWords(wordList):\n    commonWords = ['customer', 'want', 'plan', 'month', 'new', 'inquire', 'check']\n    wordList = [word for word in wordList if word not in commonWords]\n    return ','.join(wordList)","b89f342c":"labelsKeywords = pd.DataFrame(columns = ['Label', 'Fuzzy Words', 'Strict Words'])\n\nlabelColumnList = list(df[agentAssignedColumn].value_counts().to_dict())\nstrictWordList = ['']\nfuzzyWordList = ['']\nfor reasoncode in labelColumnList:\n    fuzzyWordList.append(','.join(removeFrequentOccuringWords(wordFrequencyListPlot(reasoncode)).split(',')[-2:]))\n    strictWordList.append(','.join(removeFrequentOccuringWords(wordFrequencyListPlot(reasoncode)).split(',')[:6]))\n\nlabelsKeywords['Label'] = ['Others'] + labelColumnList\nlabelsKeywords['Fuzzy Words'] = fuzzyWordList\nlabelsKeywords['Strict Words'] = strictWordList\nlabelsKeywords.head()","a36ef753":"def find_best_label(TrainingWords, StrictWords, text_in):   \n    clnTxt = [st.stem(u.lower()) for u in  text_in.split()]        \n    if(len(clnTxt)==0):\n        clnTxt =  'Others'     \n    NL = len(TrainingWords)\n    probV = [0.0 for k in range(NL)]\n    CM_A = []\n    for j in range(NL):\n        TrSet = set([ st.stem(u.lower())  for u in TrainingWords[j].split()])\n        SWords = set([ u.lower().strip() for u in StrictWords[j].split(\",\")])  if len(StrictWords[j]) else []\n        matching =  [s for s in SWords  if ((s in text_in) and len(set(s.split()).intersection(text_in.split()))==len(s.split())) ] \n        cm = list(TrSet.intersection(clnTxt))\n        if len(matching)>0:\n            probV[j] = 1\n            CM_A.append(matching)\n            break\n        else:                \n            if len(cm):\n                CM_A.append(cm)\n            else:\n                CM_A.append([])\n            probV[j] = (len(cm)\/float(len(clnTxt)))\n    return({'idx':np.argmax(probV), 'maxV':np.max(probV), 'wa':CM_A[np.argmax(probV)]})","e11185f0":"%time\nTestData = df.copy()\ntest_buffer = TestData.iloc[:,1]\n\nTrainingData = labelsKeywords.copy()\n\nfuzzy_word_list = TrainingData['Fuzzy Words']\nstrict_word_list = TrainingData['Strict Words']\ntrain_buffer = [fuzzy_word_list[i]+\" \"+strict_word_list[i] for i in range(len(TrainingData))]\n\nLabels = [a for a in TrainingData['Label']]\n\nTestData['New Topic'] = 0.0\nTestData['Conf'] = 0.0\nTestData['Matching words'] = [\"\" for i in range(len(TestData))]\n\nfor i in range(len(test_buffer)):\n    BL = find_best_label(train_buffer, TrainingData['Strict Words'], test_buffer[i])\n    TestData['New Topic'][i] = Labels[BL['idx']]\n    TestData['Conf'][i] = BL['maxV']\n    TestData['Matching words'][i] = \", \".join(BL['wa'])    \n    \nTestData.head()","616d3f05":"originalTopic = TestData[agentAssignedColumn]\nnewTopic = TestData['New Topic']\n\nprint('accuracy %s' % accuracy_score(newTopic, originalTopic))\nprint(classification_report(originalTopic, newTopic,target_names=uniqueTopics))","394cca22":"#Gensim Preprocessing\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result\n\nprocessed_docs = df[commentTextColumn].map(preprocess)\nprocessed_docs[:5]","73a643e0":"%time\n#Modelling Step\nNUM_TOPICS = len(uniqueTopics) #10 here\nprint(\"\\n Number of topics = \"+str(NUM_TOPICS)+\" \\n\")\nsizeDf=str(len(df))\n\ndictionary = gensim.corpora.Dictionary(processed_docs)\ndictionary.filter_extremes()\nbow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n\nlda_model_bow = gensim.models.LdaMulticore(bow_corpus, num_topics=NUM_TOPICS, id2word=dictionary, workers = 2, passes = 5, iterations = 100, eval_every=5)\n\n# # Saving the Model\n# modelFileName = 'bowGensimModel'+sizeDf+'('+str(NUM_TOPICS)+').gensim'\n# lda_model_bow.save(modelFileName)\n# print('\\n Model Saved as: '+modelFileName)\n\nfor idx, topic in lda_model_bow.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","0f940670":"%time\nlda_display = gensimvis.prepare(lda_model_bow, bow_corpus, dictionary, sort_topics = False)\n\n# #If you want to save the visualization\n# pyLDAvis.save_html(lda_display, 'bowGensimModel'+str(NUM_TOPICS)+'.html')\n\npyLDAvis.display(lda_display)","390c9f6c":"%time\n# Based on the Dirichlet Equations, these could be a possible Topic Assignments\nTopicList= {0:'Roaming Plans',\n1:'Others',\n2:'Add New Line',\n3:'Cancel Service',\n4:'Data Usage',\n5:'Poor Connectivity',\n6:'High Bill',\n7:'Deactivate',\n8:'Change Plans',\n9:'Port Out'}\n\n\ndf['topicNumDistributionColumn'] = lda_model_bow.get_document_topics(bow_corpus, minimum_probability=0.1)\n\ntopiclist = df['topicNumDistributionColumn'].tolist()\nnewTopic = []\nfor element in topiclist: \n    newTopic.append(str(sorted(element, key=lambda x: -x[1])[-1:]))\n    \ndf['NewTopic'] = newTopic\n\n\n\ntopicExpansionInNumberPrimary = []\nfor item in df['NewTopic']:\n    itemToList = re.findall(r'\\d+', item)\n    if len(itemToList)==3:\n        topicExpansionInNumberPrimary.append(TopicList[int(itemToList[0])])\n    elif len(itemToList)==0:\n        topicExpansionInNumberPrimary.append(None)\n    else:\n        topicExpansionInNumberPrimary.append(None)\ndf['NewTopic'] = topicExpansionInNumberPrimary\n\noriginalTopic = df[agentAssignedColumn]\nnewTopic = df['NewTopic']\n\nprint('accuracy %s' % accuracy_score(newTopic, originalTopic))\nprint(classification_report(originalTopic, newTopic,target_names=uniqueTopics))","ea6b1d94":"processed_docs = df[commentTextColumn].to_list()\ntoken_ = [doc.split(\" \") for doc in processed_docs]\nbigram = Phrases(token_, min_count=1, threshold=2,delimiter=b' ')\n\n\nbigram_phraser = Phraser(bigram)\n\nbigram_token = []\nfor sent in token_:\n    bigram_token.append(bigram_phraser[sent])\n\n#now you can make dictionary of bigram token \ndictBigram = gensim.corpora.Dictionary(bigram_token)\n# dictBigram.filter_extremes(no_above=0.5, keep_n=100000)\n\n#Convert the word into vector, and now you can use from gensim \nbow_corpus_bigram = [dictBigram.doc2bow(text) for text in bigram_token]","d3c4847d":"lda_model_bow_bigram = gensim.models.LdaMulticore(bow_corpus_bigram, num_topics=NUM_TOPICS, id2word=dictBigram, workers = 2, passes = 5, iterations = 100, eval_every=5)\n\n# # Saving the Model\n# modelFileName = 'bowGensimModelBigram'+sizeDf+'('+str(NUM_TOPICS)+').gensim'\n# lda_model_bow_bigram.save(modelFileName)\n# print('\\n Model Saved as: '+modelFileName)\n\nfor idx, topic in lda_model_bow_bigram.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","f4b12881":"%time\n#Modelling Step\nNUM_TOPICS = len(uniqueTopics) #10 here\nprint(\"\\n Number of topics = \"+str(NUM_TOPICS)+\" \\n\")\nsizeDf=str(len(df))\n\ndictionary = gensim.corpora.Dictionary(processed_docs)\ndictionary.filter_extremes()\nbow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\nlda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=NUM_TOPICS, id2word=dictionary, passes=5, workers=2, iterations = 100, eval_every=5)\n# # Saving the Model\n# modelFileName = 'tfidfGensimModel'+sizeDf+'('+str(NUM_TOPICS)+').gensim'\n# lda_model_bow.save(modelFileName)\n# print('\\n Model Saved as: '+modelFileName)\n\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","1cb0ffb0":"%time\nlda_display = gensimvis.prepare(lda_model_tfidf, corpus_tfidf, dictionary, sort_topics = False)\n\n# #If you want to save the visualization\n# pyLDAvis.save_html(lda_display, 'tfidfGensimModel'+str(NUM_TOPICS)+'.html')\n\npyLDAvis.display(lda_display)","08c2c6d7":"%time\n# Based on the Dirichlet Equations, these could be a possible Topic Assignments\nTopicList= {0:'High Bill',\n1:'Roaming Plans',\n2:'Others',\n3:'Change Plans',\n4:'Cancel Service',\n5:'Poor Connectivity',\n6:'Add New Line',\n7:'Port Out',\n8:'Data Usage',\n9:'Deactivate'}\n\n\ndf['topicNumDistributionColumn'] = lda_model_tfidf.get_document_topics(corpus_tfidf, minimum_probability=0.1)\n\ntopiclist = df['topicNumDistributionColumn'].tolist()\nnewTopic = []\nfor element in topiclist: \n    newTopic.append(str(sorted(element, key=lambda x: -x[1])[-1:]))\n    \ndf['NewTopic'] = newTopic\n\n\n\ntopicExpansionInNumberPrimary = []\nfor item in df['NewTopic']:\n    itemToList = re.findall(r'\\d+', item)\n    if len(itemToList)==3:\n        topicExpansionInNumberPrimary.append(TopicList[int(itemToList[0])])\n    elif len(itemToList)==0:\n        topicExpansionInNumberPrimary.append(None)\n    else:\n        topicExpansionInNumberPrimary.append(None)\ndf['NewTopic'] = topicExpansionInNumberPrimary\n\noriginalTopic = df[agentAssignedColumn]\nnewTopic = df['NewTopic']\n\nprint('accuracy %s' % accuracy_score(newTopic, originalTopic))\nprint(classification_report(originalTopic, newTopic,target_names=uniqueTopics))","0786ec1d":"processed_docs = df[commentTextColumn].to_list()\ntoken_ = [doc.split(\" \") for doc in processed_docs]\nbigram = Phrases(token_, min_count=1, threshold=2,delimiter=b' ')\n\n\nbigram_phraser = Phraser(bigram)\n\nbigram_token = []\nfor sent in token_:\n    bigram_token.append(bigram_phraser[sent])\n\n#now you can make dictionary of bigram token \ndictBigram = gensim.corpora.Dictionary(bigram_token)\n# dictBigram.filter_extremes(no_above=0.5, keep_n=100000)\n\n#Convert the word into vector, and now you can use from gensim \ncorpus_bigram = [dictBigram.doc2bow(text) for text in bigram_token]\n\ntfidf_model_bigram = models.TfidfModel(corpus_bigram)\ncorpus_tfidf_bigram = tfidf_model_bigram[corpus_bigram]","4edaa8c6":"lda_model_tfidf_bigram = gensim.models.LdaMulticore(corpus_tfidf_bigram, num_topics=NUM_TOPICS, id2word=dictBigram, passes=5, workers=2, iterations = 100, eval_every=5)\n# # Saving the Model\n# modelFileName = 'tfidfGensimModelBigram'+sizeDf+'('+str(NUM_TOPICS)+').gensim'\n# lda_model_bow.save(modelFileName)\n# print('\\n Model Saved as: '+modelFileName)\n\nfor idx, topic in lda_model_tfidf_bigram.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","d75b5d44":"## Random Forest","a36126d2":"Text Cleaning","ab7d9cf6":"### LDA Using Bag-of-Words","827b2686":"Tf-idf","8a8efdd2":"Learning Curve of the Model","758f4b05":"For visualizing Topic distribution, we use the package [plotly](https:\/\/plot.ly\/) which makes the bar charts more interactive.","9dec4516":"Named Entity Recognition","607e36e8":"Visualizing frequency distribution of top words helps us in two ways:\n\n- Determine which words needs to be removed & devising a process flow of cleaning the text.\n- Comparing the Frequency distribution \"Before\" & \"After\" Text Preprocessing gives us a visual idea about how well we are cleaning the data for modelling.","b36688e8":"## Some Useful References:","03e35a45":"### Object Standardization","b017e946":"## Support Vector Machines(SVM)","b1937bfe":"Generating WordCloud for each reason to have a visual representation of texts corresponding to each topic. We can also have an overall wordcloud.","128d12da":"Identify Words which contains numbers in it","ee898f31":"## Combining Word2Vec & Logistic Regression","398f279c":"- [Analytics Vidya](https:\/\/www.analyticsvidhya.com\/blog\/2017\/01\/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python\/) for Preprocessing\n- [Blog post](https:\/\/towardsdatascience.com\/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) by Susan Li.\n- [Analytics Vidya](https:\/\/www.analyticsvidhya.com\/blog\/2016\/08\/beginners-guide-to-topic-modeling-in-python\/) for Topic Modelling.\n- [Machine Learning Plus](https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/)","d7c6702a":"### Visualize Token Frequency Distribution After Text Preprocessing","1ad620f9":"Variable Standardization","1543f324":"- Bigram TFIDF","897dfb3b":"### Visualize Token (vocabulary) Frequency Distribution Before Text Preprocessing","24a9084a":"Now that we have cleaned text and its corresponding labels, we can start modelling step. \nFirst we use a `Supervised Learning` approach. The methods we use in this approach are:\n\n- [Logistic Regression](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression): We use a pipeline approach of both [Count Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model) and [TF-IDF Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf).\n- [Naive Bayes](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier): Here also, we use a pipeline approach of both [Count Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model) and [TF-IDF Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf).\n- [Stochastic Gradient Descent](https:\/\/en.wikipedia.org\/wiki\/Stochastic_gradient_descent): We use a pipeline approach of both [Count Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model) and [TF-IDF Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf).\n- [Random Forest Classifier](https:\/\/en.wikipedia.org\/wiki\/Random_forest) : Here also, we use a pipeline approach of both [Count Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model) and [TF-IDF Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf).\n- [XGBoost Classifier](https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting): We use a pipeline approach of both [Count Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model) and [TF-IDF Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf).\n- [Support Vector Machines(SVC)](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine) : Here also, we use a pipeline approach of both [Count Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model) and [TF-IDF Vectorizer](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf).\n- Combination of [Word2Vec](https:\/\/en.wikipedia.org\/wiki\/Word2vec) & Logistic Regression: For using this, you'll need to download the file `GoogleNews-vectors-negative300.bin.gz` from [this location](https:\/\/code.google.com\/archive\/p\/word2vec\/)\n- Combination of [Doc2Vec](https:\/\/markroxor.github.io\/gensim\/static\/notebooks\/doc2vec-wikipedia.html) & Logistic Regression: Refer [this link](https:\/\/medium.com\/scaleabout\/a-gentle-introduction-to-doc2vec-db3e8c0cce5e) for a better understanding.\n- Bag-Of-Words with Keras Sequential Model\n- [LSTM](https:\/\/en.wikipedia.org\/wiki\/Long_short-term_memory)\n- Heuristic\/Rule Based Approach.","f53f4ffc":"- Unigram TFIDF","474e06ed":"Test with an unseen comment.","fb543651":"## Bag-Of-Words with Keras","40ce2b6d":"## Naive Bayes","13bebf62":"Bag of Words","9f5c4d5e":"## Logistic Regression","b7bd1dc7":"## XGBoost","6bb67383":"### LDA Using TF-IDF","2c71dbef":"## LSTM","3e105573":"- We can check average Word count for each reasons. At times taking Median word count might also be useful.","e1665c1f":"## Topic Modelling using LDA","664c1c4e":"# Text Classification - Supervised Approach","82fcee8f":"Identify the Language Distribution","77ef037e":"## Word Distribution Analysis for each Topic","c71c9277":"We also try an `Unsupervised Learning` approach. [Latent Dirichlet allocation(LDA)](https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation) is used to get the topic clusters. We use two types of Word Corpuses:\n- Bag of Words\n- TF-IDF\n\n\nIn trying to incorporate the context of sentences at times, we also try two word gram models:\n- Unigram Word distribution\n- Bigram Word distribution","a55adc93":"## Stochastic Gradient Descent","40b42c54":"This is very important step in analyzing which set of words contribute to each topics. This gives us an idea about how different models would perform and helps us improve the previous steps. ","62ee99a4":"Importing required packages","b8c10c9d":"In Text Preprocessing, we are performing multiple Natural Language Processing techniques. The idea is to get noise-free standardized text which can be taken for used readily for Modelling. Steps of Preprocessing largely depends on the dataset we are dealing with. In this case the Text Preprocessing comprises of the following steps:\n\n\n- Object Standardization: Since the free text we have were written by Call Centre agents, they use a lot of domain specific Short Hands, abbreviations, etc. which are not present in any standard lexical dictionaries. But then, at certain times, they write the full form or complete word. Object Standardization strike a balance between these & use a same form of word through out the dataset.\n\n\n- Named Entity Recognition(NER): Detecting names of 'Person', 'Location', 'Product' etc. and removing them from the dataset as they don't specifically add much value to our final model. (Note: There are cases where such entities are important & should be retained in some format.). We use [SpaCy](https:\/\/spacy.io\/usage\/linguistic-features) for NER.\n\n\n- Other Language Identification: Since the data we have comes from different locations, there could be a possibility of other languages present in the data. We use `spacy_langdetect` for identifying them and removing them.\n\n\n- Alphanumeric removal: Since this is Telecom Call centre data, there are a lot of alphanumeric words involved. For eg: 3G, 20MBps, agentID: '4ap2X', etc. We can keep it or remove it as per requirement. \n\n\n- Text Cleaning: Next we remove all the special characters, ASCII characters, numbers, single character words, etc.\n\n\n- Autocorrection: The agent comments might have a lot of spelling mistakes, typos etc.  We try to correct atleast some of them using `TextBlob` Autocorrection.\n\n\n- Lexicon Normalization: We also normalize multiple representations of a word to its etymological root form. For eg: 'wanted', 'wanting', 'want' becomes > 'want'\n\nAfter this we perform the following operations on the dataset:\n\n- Remove null comments.\n- Remove duplicate comments (keeping its first occurance.)\n- Remove comments with just one word.","bf474ea2":"### Analyzing the Word Count for each Reasons","79a5c626":"### Agent Assigned Topic Distribution Analysis","406b609a":"## Heuristic\/Rule Based Approach","c92c9742":"Creating a custom Stop Word List","c59a9686":"### Checking Data Quality","6589f170":"Text Normalization","1c910135":"By doing a Data Quality Check we are:\n\n- Identifying a unique column which can be assigned as a Primary Key column (incase it wasn't specified initially.)\n- Making sure all the columns are having the required datatypes\n- Calculating the Null Values, Duplicate values, etc. (Maximum, Minimum, Range in case its a numeric column.).\n- Creating visualizations to understand the Data Quality level of input data.","b72ceb83":"## Combining Doc2Vec & Logistic Regression","29598285":"## Text Preprocessing","ee874865":"### Exploratory Data Analysis","8f487648":"Feature extractions","d2b9e250":"Reading Data file","0dc1f615":"Autocorrection","a671c19a":"# Text Classification - Unsupervised Approach","69d318e5":"- Bi gram Bag of Words","2ca94ed8":"Checking the misclassified","878ceed1":"- Unigram Bag of Words "}}