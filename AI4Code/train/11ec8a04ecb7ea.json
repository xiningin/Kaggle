{"cell_type":{"1f906bba":"code","73cf3b46":"code","a2bda2bd":"code","2bc9ba91":"code","fec819bd":"code","ee8d22c7":"code","bd2ba060":"code","0d144305":"code","cefc4a71":"code","83d6d5c7":"code","68c3b895":"code","24ec2317":"code","95dc288e":"code","b6b6a0c7":"code","b41e7142":"code","474683b8":"code","7ae56d9d":"code","7458da4a":"code","62894a71":"markdown","52b65734":"markdown","34561ffb":"markdown","ef483a78":"markdown","dfaabc92":"markdown","67c26e48":"markdown","3c54532e":"markdown","902aaf2e":"markdown","30887c27":"markdown","26b786f9":"markdown","2d62fef0":"markdown","cbf8d116":"markdown","c6bee061":"markdown","e33887a7":"markdown","57266cb6":"markdown","837adea2":"markdown","3f6abfd6":"markdown"},"source":{"1f906bba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","73cf3b46":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nimport seaborn as sns","a2bda2bd":"from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer() # dataset contains array of data, features names and target variable \nprint(cancer.keys()) # displays the dictionary field names\ncancer.data # displays the cancer datavalues\n\n","2bc9ba91":"print(cancer['target_names']) # this defines whether the cancer is at serious stage(Malignant) or not(benign).","fec819bd":"cancer.data.shape # shape describes the number of records(569) and fields (30)","ee8d22c7":"#check for null or missing values\nna= np.isnan(cancer.data).sum() # sum of null values if any\n\nprint(\"Missing Value Count\",na )","bd2ba060":"sns.countplot(x=cancer.target,)","0d144305":"X_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=0)","cefc4a71":"clf = DecisionTreeClassifier(random_state=0) #creating object of the class DecisionTreeClassifier.\nclf = clf.fit(X_train, y_train)# fitting classfier on training dataset\n\ny_pred=clf.predict(X_test) #predicting for test dataset","83d6d5c7":"print(\"Accuracy on the training set: {:.3f}\".format(clf.score(X_train, y_train)))\nprint(\"Accuracy on the test set: {:.3f}\".format(clf.score(X_test, y_test)))","68c3b895":"from sklearn.tree import export_graphviz\n\nexport_graphviz(clf, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\n                feature_names=cancer.feature_names, impurity=False, filled=True)","24ec2317":"from IPython.display import display\n\nimport graphviz\n  \nwith open('tree.dot') as f:\n    dot_graph = f.read()\n\ndisplay(graphviz.Source(dot_graph))\n\n# Graph is too complex to interpret and complexity increases with depth.","95dc288e":"# Fit classifier model\n#setting right cost complexity parameter and optimal depth to reduce complexity and increase accuracy of model.\n#defined ccp_alpha=0.015.\n\n# max_depth=2,ccp_alpha=0.015\nclassifier_1 = DecisionTreeClassifier(max_depth=2,ccp_alpha=0.015)\nclassifier_1.fit(X_train, y_train)\n\n# Predict\n\ny_1 = classifier_1.predict(X_test)","b6b6a0c7":"classifier_2 = DecisionTreeClassifier(max_depth=5,ccp_alpha=0.015)\nclassifier_2.fit(X_train, y_train)\n\ny_2 = classifier_2.predict(X_test)","b41e7142":"classifier_3 = DecisionTreeClassifier(max_depth=8,ccp_alpha=0.015)\nclassifier_3.fit(X_train, y_train)\n#predict\ny_3 = classifier_3.predict(X_test)","474683b8":"print(\"Accuracy on the training set with Max Depth=2 : {:.3f}\".format(classifier_1.score(X_train, y_train)))\nprint(\"Accuracy on the test set with Max Depth=2 : {:.3f}\".format(classifier_1.score(X_test, y_test)))\nprint(\"\\n\")\n\nprint(\"Accuracy on the training set with Max Depth=5 : {:.3f}\".format(classifier_2.score(X_train, y_train)))\nprint(\"Accuracy on the test set with Max Depth=5 : {:.3f}\".format(classifier_2.score(X_test, y_test)))\n\nprint(\"\\n\")\n\nprint(\"Accuracy on the training set with Max Depth=8 : {:.3f}\".format(classifier_3.score(X_train, y_train)))\nprint(\"Accuracy on the test set with Max Depth=8 : {:.3f}\".format(classifier_3.score(X_test, y_test)))\n","7ae56d9d":"print(f\"Breast Cancer Feature Importances: \\n {format(classifier_1.feature_importances_)} \")","7458da4a":"\ndef plot_feature_importances_cancer(model):\n    plt.subplots(figsize=(10,8))\n    n_features = cancer.data.shape[1]\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), cancer.feature_names)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n\nplot_feature_importances_cancer(classifier_1)","62894a71":"1. Accuracy on the training set is 100% and this leads to **overfitting** of the model , where your model can completely explain training dataset.\n2. To avoid overfitting we apply **Pruning** . Pruning is a method of removing least significant leaf node and to recreate tree which helps in its better growth by reduce complexity of tree.\n3. **Depth** and **cost complexity factor**(ccp_alpha) of tree helps in pruning of decision tree.","52b65734":"Rates significance level of each feature on the decision tree.  \nValues range from 0 = \"Not at all\", to 1 = perfectly predicts target\".  \nFeature importances always sum to a total of 1.","34561ffb":"# Feature Importance","ef483a78":"# Pruning Tree  \nTried Pruning by adjusting max depth with 3 different parameters\n","dfaabc92":"Above count plot displays\n1. 219 records in target array are \"Malignant\"=0\n2. 350 records in target array are \"Benign\"=1","67c26e48":"Based on the Accuracy results Tree with depth=5 and 8 gives the best accuracy on the test result comaparitively.  We must ideally select the less complex tree , which is depth=5 as the best classifier to fit our test model. Model with depth=5 is the most accurate with test set prediction.\n\nAccuracy on the training set with Max Depth=5:- 0.955 Accuracy on the test set with Max Depth=5:- 0.916","3c54532e":"# Importing Library","902aaf2e":"# Fitting Decision Tree Classifier","30887c27":"**max_depth=8,ccp_alpha=0.015**","26b786f9":"# Data Analysis and Exploration","2d62fef0":"# **Analyzing and Visualizing Decision Tree**","cbf8d116":"**max_depth=2,ccp_alpha=0.015**","c6bee061":"The most significant features.  \nWorst perimeter.  \nWorst concave points are","e33887a7":"**max_depth=5,ccp_alpha=0.015**","57266cb6":"# Importing Dataset","837adea2":"# Splitting Dataset into Train and Test datasets\n","3f6abfd6":"Split is based on the Worst Perimeter which is most important feature."}}