{"cell_type":{"2aadd572":"code","b873533a":"code","35419326":"code","dfa9ac94":"code","3e6a44a2":"code","cc13c169":"code","a2483c53":"code","6985e961":"code","c8e93a31":"code","ae390d97":"code","5f0a5ee9":"code","dd8211dc":"code","2024d6ad":"code","562aaf54":"code","99d0018d":"code","4609c330":"code","7c0a8bd8":"code","21d08f08":"code","c71b5b74":"code","b610c58b":"code","ef262dd4":"code","870de6df":"code","144da4df":"code","c633a06b":"code","acac0734":"markdown","e251ce33":"markdown","bb844282":"markdown","c2616d16":"markdown","55a151c3":"markdown","b80f2d8d":"markdown","80eaefb2":"markdown","43dde8e5":"markdown","e165cd95":"markdown","1096cfbf":"markdown"},"source":{"2aadd572":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns","b873533a":"# reading the data and calculating the computation time\n\n%time training_data = pd.read_csv('..\/input\/Google_Stock_Price_Train.csv')\n\n# checking the shape of the data\ntraining_data.shape","35419326":"training_data.head()\n","dfa9ac94":"# let's describe the data\n\ntraining_data.describe()","3e6a44a2":"# Lets work on the open stock price only and take out the \" open \" stock column.\n\ntraining_data = training_data.iloc[:, 1:2]\n\n# The training_data is in the form of dataframe.\ntraining_data.shape","cc13c169":"# checking the head of the data\n\ntraining_data.head()","a2483c53":"import matplotlib.pylab as plt\nplt.figure(figsize=(10,5))\nplt.plot(training_data, color ='green');\nplt.ylabel('Stock Price')\nplt.title('Google Stock Price')\nplt.xlabel('Time')\nplt.show()","6985e961":"\n# Normalize the training data between [0,1]\nfrom sklearn.preprocessing import MinMaxScaler\n#the fit method, when applied to the training dataset, learns the model parameters (for example, mean and standard deviation). \n#We then need to apply the transform method on the training dataset to get the transformed (scaled) training dataset.\n#We could also perform both of this step in one step by applying fit_transform on the training dataset.\nmm = MinMaxScaler(feature_range = (0, 1))\ntraining_data_scaled = mm.fit_transform(training_data)\ntraining_data_scaled.shape","c8e93a31":"plt.figure(figsize=(10,5))\nplt.plot(training_data_scaled);\nplt.title('Google Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('Google Stock Price')\nplt.show()","ae390d97":"\n# Getting the inputs and outputs directly if you know how to consider the past data for the number of time stamps needed for RNN.\nx_train = training_data_scaled[59:1257]\ny_train = training_data_scaled[60:1258]\nprint(x_train.shape)\nprint(y_train.shape)","5f0a5ee9":"'''for i in range(60,1258):\n    #first 59 values of x will be the training data and the 60th value will be output\n    #2nd set of values start from 60th (inculding the output of the  first set) will be the next 60 input values of the 2nd set and it continues.\n    #append is to add the values to the values to x and y\n    #as the values are in the form of dataframe, they has to be stored in the form of a numpy array.\n    x_train.append(training_data_scaled[i-60:i, 0])\n    y_train.append(training_data_scaled[i,0])\nx_train,y_train = np.array(x_train),np.array(y_train)\nprint(x_train.shape)\nprint(y_train.shape)'''","dd8211dc":"# reshaping\nx_train = np.reshape(x_train, (1198,1,1))\nprint(x_train.shape)","2024d6ad":"import keras \nfrom keras.models import Sequential #helps to create model, layer by layer.\nfrom keras.layers import Dense, LSTM, Dropout\n#The dense layer is fully connected layer, so all the neurons in a layer are connected to those in a next layer.\n#The dropout drops connections of neurons from the dense layer to prevent overfitting. the neurons whose value falls under 0, will be removed.\n#LSTM gates to control the memorizing process. For detailed information on LSTM, go through the link below.\n''' https:\/\/towardsdatascience.com\/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47 '''","562aaf54":"# Create model using LSTM, Dropout and Dense layer as an output layer. \n#Initializing the RNN\nregressor = Sequential()\nregressor.add(LSTM(units = 50,return_sequences = True,input_shape = (x_train.shape[1],1)))\nregressor.add(Dropout(0.2))","99d0018d":"# Adding second hidden layer\nregressor.add(LSTM(units = 50,return_sequences = True))\nregressor.add(Dropout(0.2))","4609c330":"# Adding third hidden layer\nregressor.add(LSTM(units = 50,return_sequences = True))\nregressor.add(Dropout(0.2))","7c0a8bd8":"#Adding fourth hidden layer\nregressor.add(LSTM(units = 50))\nregressor.add(Dropout(0.2))","21d08f08":"# Adding dense layer to get the final output. The input of n-1 layer, will be the output for n layer.\nregressor.add(Dense(units = 1))","c71b5b74":"regressor.compile(optimizer = 'adam',loss = 'mean_squared_error')","b610c58b":"# Train the model\nregressor.fit(x_train,y_train,epochs = 100, batch_size = 32)","ef262dd4":"test_data = pd.read_csv('..\/input\/Google_Stock_Price_Test.csv')\ntest_stock = test_data.iloc[:,1:2]\nlen(test_stock)","870de6df":"input_value = test_stock\ninput_value = mm.transform(input_value)\n# perfor the same process, converting a 2D array to 3D\ninput_value = np.reshape(input_value, (20, 1, 1))","144da4df":"\nprediction = regressor.predict(input_value)\nprediction = mm.inverse_transform(prediction)","c633a06b":"# visualizing the results\n\nplt.rcParams['figure.figsize'] = (15, 8)\n\nplt.plot(test_stock, color = 'red', label = 'Real  Stock ')\nplt.plot(prediction, color = 'green', label = 'Predicted  Stock ')\nplt.title('Final Stock Prediction')\nplt.xlabel('Time')\nplt.ylabel('Google Stock Price')\nplt.legend()\nplt.show()","acac0734":"An alternative approach using a for loop would be the code below.","e251ce33":"ADAM optimization algorithm is used and a mean squared error loss function is optimized. This will be the same metric that we will use to evaluate the performance of the model to find the global minimum error.","bb844282":"x_train should be a 3d array, hence reshaping the 2D to a 3D array. ","c2616d16":"**(Sample, time_steps, features)** represents the tensor you will feed into your LSTM, let\u2019s look at these three \u201cdimension\u201d (we call that a rank for a tensor).\n\n**Sample:** It\u2019s the size of your minibatch: How many examples you give at once to your neural net.\n\n**time_steps:** That\u2019s the length of a sequence. Don\u2019t forget, recurrent neural network are designed to process time-series.\n\n**features:** That\u2019s the dimension of each element of the time-series.","55a151c3":"Let us take the test set and check the accuracy.","b80f2d8d":"### Here comes the most important part of the model which is \" Feature Scaling \"","80eaefb2":"As you can see that the Y-axis is completely changed, it is scaled in between 0 and 1, this is because, all the values are in different weights, so in order to have a good prediction, scaling has to be made. Remember that the scaling can be done on to floating point values.","43dde8e5":"Remember that, the x_train and y_train must have the same values. to process RNN","e165cd95":"**Date:**  The date of recorded data\n\n**Open:**  The price when stock market open\n\n**High:**  The highest price price of date\n\n**Low:**  The lowest price point of date\n\n**Volume:**  Total Sale of stock on that date","1096cfbf":"1. ## Recurrent Neural Networks using LSTM keras"}}