{"cell_type":{"ed099c13":"code","686a0471":"code","d85a8c7a":"code","8be6789b":"code","836635b6":"code","a3199598":"code","3d781cb2":"code","e656477d":"code","ac9e261b":"code","d5e8bb86":"code","393fc051":"code","8d37bf19":"code","d69d2f3a":"code","34a8595f":"code","39d842c0":"code","ed00a0e3":"code","d8dbb651":"code","3449f148":"code","8588d66d":"code","a822d297":"code","737b8913":"code","68af12fc":"markdown","2fc526c4":"markdown","ff81bfe8":"markdown","ce5687e2":"markdown","cd0c0acb":"markdown","9e506808":"markdown","731e8c15":"markdown","40cbd0ca":"markdown","3e2151db":"markdown","4088bb29":"markdown","fdd49aa4":"markdown","e51c8439":"markdown","3906a994":"markdown","950d12d7":"markdown","9952d2d7":"markdown","9c8d67c9":"markdown","c11bcb80":"markdown","bcd25b5e":"markdown","46dbf988":"markdown","f33bf6da":"markdown","5f3f419b":"markdown","ee19a411":"markdown","2f19bfae":"markdown","313eeb99":"markdown","7354c099":"markdown","e71bba9e":"markdown","0ed5f1b7":"markdown","92098ad3":"markdown","0d96a75e":"markdown","7a661b9c":"markdown","e6b7fac5":"markdown"},"source":{"ed099c13":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport more_itertools as mit\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing as pre\nfrom sklearn.preprocessing import StandardScaler as ssc\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\n\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nimport time","686a0471":"#Load the data which was changed from .data to .csv\nprint(os.listdir(\"..\/input\"))\ndatr = pd.read_csv(\"..\/input\/K8.csv\",low_memory=False)","d85a8c7a":"datr.describe()","8be6789b":"datr.head()","836635b6":"#Remove the column title of datr, make it row one, then make the column title integer values\ndatrcolumn=pd.DataFrame(list(datr.columns)).T\ndatr.columns = datrcolumn.columns\ndatrefined=datrcolumn.append(datr, ignore_index = True)\n\n#Remove the last column of datrefined\ndatrefined.drop(columns = list(datrefined.columns)[-1],inplace=True)","a3199598":"#Remove the second decimal points with the digits that follows it\nfor i in range(0,len(datrefined.iloc[0])):\n    datrefined[i]=datrefined[i].apply(lambda x: x[:list(mit.locate(x, lambda s: s == \".\"))[1]] if x.count('.') > 1 else x)","3d781cb2":"# Change string numerical values to numbers\ndatrefined.iloc[:,:-1]=datrefined.iloc[:,:-1].apply(pd.to_numeric, errors='coerce')\n# Drop all rows NaN\ndatrefined.dropna(axis=0, how='all', inplace=True)\n# Drop all columns NaN\ndatrefined.dropna(axis=1, how='all', inplace=True)\n# Drop rows\/ with over 90% of the column with NaN\ndatrefined.dropna(axis=0, thresh=datrefined.shape[1]*90\/\/100, inplace=True)\n# Print the total number of rows dropped.\nprint('Total number of rows dropped is '+str(datr.shape[0]-datrefined.shape[0]))\n# Check if there are still missing missing data in the dataset\nmissingSum = datrefined.isnull().sum().sum()\nprint('The total number of missing values is: ',missingSum)","e656477d":"datrefined.index = list(range(0,datrefined.shape[0]))","ac9e261b":"#Encoding categorical data values\nlabelencoder_y = LabelEncoder()\ndatrefined[datrefined.columns[-1]] = labelencoder_y.fit_transform(datrefined[datrefined.columns[-1]])","d5e8bb86":"datrefined.rename(columns={datrefined.columns[-1]:'target'}, inplace=True)\ny = datrefined['target'].values\nX = datrefined.drop('target', axis=1).values\n\nX_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.20, random_state=21)","393fc051":"# Standardizing the features\nX = ssc().fit_transform(X)\nX_train = ssc().fit_transform(X_train)\nX_test = ssc().fit_transform(X_test)","8d37bf19":"pcaPlot = PCA(n_components=2)\npcaPlot.fit(X)\nprincipalComponents = pcaPlot.transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])","d69d2f3a":"pcaForPloting = pd.concat([principalDf, datrefined['target']], axis = 1, join='inner')\npcaForPloting.rename(columns={datrefined.columns[-1]:'target'}, inplace=True)","34a8595f":"pcaForPloting.head()","39d842c0":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargetss = ['inactive', 'active']\ntargets = [1, 0]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = pcaForPloting['target'] == target\n    ax.scatter(pcaForPloting.loc[indicesToKeep, 'principal component 1']\n               , pcaForPloting.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targetss)\nax.grid()","ed00a0e3":"#Fitting the PCA algorithm with our Data\npcaModel = PCA().fit(X)\n\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pcaModel.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Dataset Explained Variance for Training Data')\nplt.show()\n\n#Reducing the data dimension to retain .90 variance\nvarianceToPreserve = 0.90\n\npca = PCA(varianceToPreserve)\npca.fit(X_train)\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)\n\nprint('Selecting '+str(X_train.shape[1])+' X_train components we can preserve '+str(varianceToPreserve)+' of the total variance of the data.')\nprint('Selecting '+str(X_test.shape[1])+' X_test components we can preserve '+str(varianceToPreserve)+' of the total variance of the data.')","d8dbb651":"# Building a list of models to use\nmodels_list = []\nmodels_list.append(('CART', DecisionTreeClassifier()))\nmodels_list.append(('SVM', SVC())) \nmodels_list.append(('NB', GaussianNB()))\nmodels_list.append(('KNN', KNeighborsClassifier()))","3449f148":"# Building the Models\nnum_folds = 4\nresults = []\nnames = []\n\nfor name, model in models_list:\n    kfold = KFold(n_splits=num_folds, random_state=123)\n    start = time.time()\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    end = time.time()\n    results.append(cv_results)\n    names.append(name)\n    print( \"%s: %f (%f) (run time: %f)\" % (name, cv_results.mean(), cv_results.std(), end-start))","8588d66d":"# Making Performance plots\nfig = plt.figure()\nfig.suptitle('Performance Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","a822d297":"# prepare the model\npredictions = []\nfor name, model in models_list:\n    start = time.time()\n    model.fit(X_train, y_train)\n    end = time.time()\n    print( \"Run Time for %s is : %f\" % (name, end-start))\n    \n    # estimate accuracy on test dataset\n    pred = model.predict(X_test)\n    predictions.append((name, pred))\n","737b8913":"for name, pred in predictions:\n    print('Results for '+name+' :::')\n    print(\"  Accuracy score %f\" % accuracy_score(y_test, pred))\n    print(classification_report(y_test, pred))\n    cm = np.array(confusion_matrix(y_test, pred, labels=[1,0]))\n    confusion = pd.DataFrame(cm, index=['Is Inactive','Is Active'],\n                             columns=['Predicted Inactive','Predicted Active'])\n    #sns.heatmap(confusion, annot=True)\n    #confusion.plot_confusion_matrix(title='Confusion matrix')\n    print(confusion)\n    print('     ')\n    print('     ')\n    print('     ')","68af12fc":"### 4.4 Set index as numbers\nGive the rows integer numbering","2fc526c4":"### 3.1 Load the data","ff81bfe8":"### 3.2 Print the data summary\nWe discovered the presence of NaN in all the rows from the data description","ce5687e2":"## 6. Standardize the Data\n\nWe use StandardScaler to help standardize the dataset\u2019s features onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. Also, data need to be standardized before employing Principal Components Analysis.","cd0c0acb":"### 7.4 Make the plot","9e506808":"### 4.3 Dealing with Missing values","731e8c15":"## 11. Result Analysis\n","40cbd0ca":"## 4. Data Cleaning","3e2151db":"## 12. Conclusion","4088bb29":"The handle missing numbers, we did the following:  \n1. Some columns contain <b>?<\/b>. We first converted them to **NaN**.\n2. There is posibility of columns containing other numeric values in form of strings, we try to change every elements in each column to numbers(numerical values that were entered as strings will convert) and if it's not possible (in case on non-numerical values) force then to be <b>NaN<\/b>, (except the last column).\n3. Drop all rows and columns with all elements <b>NaN<\/b> or whose rows contains <b>NaN<\/b> more than 90% of the column numbers","fdd49aa4":"## 10. Check Accuracy on the Test Set\nLet's fit the models to the dataset and see how it performs given the test data.","e51c8439":"### 4.2 Dealing with numbers with more than one decimal point","3906a994":"### 7.3 Print the head","950d12d7":"This dataset greatly impacted on the results. There are far more data for inactive than there are for active therefore making the models to do better on inactive. This clearly shows that he with more data wins.  \n\nTo improve the results, there should be more collection of data of active target values. This will help to improve the performance on active sets.","9952d2d7":"The performance on the test set reviews the following:\n1. **CART** has Accuracy score of 0.986140. Correctly classified 3264 of **inactive** while misclassifying 27 as **active**. Correctly classified 9 of **active** while misclassifying 19 as **inactive**\n2. **SVM****** has Accuracy score of 0.991262. Correctly classified 3290 of **inactive** while misclassifying 1 as **active**. Correctly classified 0 of **active** while misclassifying 28 as **inactive**\n3. **NB** has Accuracy score of 0.742694. Correctly classified 2443 of **inactive** while misclassifying 848 as **active**. Correctly classified 22 of **active** while misclassifying 6 as **inactive** \n4. **KNN** has Accuracy score of 0.991564. Correctly classified 3285 of **inactive** while misclassifying 6 as **active**. Correctly classified 6 of **active** while misclassifying 22 as **inactive**\n\nFrom the above results, one can decide:\n* to choose NB as the best model given its ability to classify both the target values. This seems more balance.\n* to chose KNN or SVM if he doesn't care about the above the reasoning.\n* it is neccessary to consider the effect of the decision on the patient. Imagine telling a cancer patients he doesn't have cancer based on your model. He may eventually die without getting treatment. If on the other hand you tell a patient who actually doesn't have the cancer that he does, what could be the result of this information on?\n* One needs to carefully consider these factors before making a final decision of the model to adopt.","9c8d67c9":"## 8. Dimensional Reduction\nWe Project the data with PCA to retain 90% of Variance","c11bcb80":"# <center>Predicting p53 Mutants\n<center>\n    <b>Otomiewo Tega<\/b>  \n    <b>Aning Samuel<\/b>  \n    <b>Nwachukwu Anthony<\/b>\n<\/center> ","bcd25b5e":"The yellow line in the boxplots indicate the mean of the performance on training set of the folds used for each model. We discover that **KNN** performed best followed by **CART** and the least is Nearest neighbour(**KN**). This is not good enough to determine the best model to choose as we shall in the next analysis.","46dbf988":"## 5. Split Dataset to Train and Test Sets\nFinally, we'll split the data into predictor variables and target variable, following by breaking them into train and test sets. We will use 20% of the data as test set.","f33bf6da":"## 3. Data Exploration","5f3f419b":"## 1. Description of data set","ee19a411":"## 2. Import required Libraries","2f19bfae":"### 7.1 Project to 2 Dimensions","313eeb99":"### 7.2 Concatenate the Principal compoments and y, then rename y to <b>target<\/b>. Call it pcaForPloting.","7354c099":"## 7. Visualize the Data by PCA Projection to 2D\n\nSince the data has much features(5409 columns), to be able to visualize it, we apply PCA to reduce the dimension. In this section, the code projects the original data which is 5409 dimensional into 2 dimensions. It should note that after dimensionality reduction, there usually isn\u2019t a particular meaning assigned to each principal component. The new components are just the two main dimensions of variation.\n","e71bba9e":"Some values contain more than one decimal points. We removed the second decimal points with the digits that follows it. This makes the value consistent with other values.","0ed5f1b7":"## 9. Building the Models\nFrom the dataset, we will build a model to predict if mutant p53 transcriptional is **active** or **inactive**. This is a binary classification problem, and a few algorithms are appropriate for use. Since we do not know which one will perform the best at this time, we will do a quick test on the few appropriate algorithms with default setting to get an early indication of how each of them perform. We will use **10** fold cross validation for each testing.\n\nThe following non-linear algorithms will be used, namely:\n1. Classification and Regression Trees (CART)\n2. Linear Support Vector Machines (SVM)\n3. Gaussian Naive Bayes (NB) and \n4. k-Nearest Neighbors (KNN).","92098ad3":"### 3.3. Check the data head\nFrom the data head, we understood the following: \n* **?** was used to indicate **NaN**\n* the presence of numbers with double decimal points\n* the last column is completely without data\n* the first row of the data is used as column header  \n\nWe will try to fix these in the data cleaning section","0d96a75e":"**DataSet Name**: p53 Mutants\n\n**Goal:**\nThe goal is to model mutant p53 transcriptional activity (active vs inactive) based on data extracted from biophysical simulations.\n\n**Dataset Source:**\nRichard H. Lathrop, UC Irvine, http:\/\/www.ics.uci.edu\/~rickl\n\n**Data Type:**\nMultivariate\n\n**Task:**\nClassification\n\n**Attribute Type:**\nReal\n\n**Area:**\nLife Sciences\n\n**Format Type:**\nMatrix\n\n**Missing Values:**\nYes\n\n**Intances:**\n16772\n\n**Attributes:**\n5409\n\n**Relevant Information:**\nBiophysical models of mutant p53 proteins yield features which can be used to predict p53 transcriptional activity.  All class labels are determined via in vivo assays.\n\n**Attribute Information:**\nThere are a total of 5409 attributes per instance. Attributes 1-4826 represent 2D electrostatic and surface based features. Attributes 4827-5408 represent 3D distance based features. Attribute 5409 is the class attribute, which is either active or inactive. The class labels are to be interpreted as follows: 'active' represents transcriptonally competent, active p53 whereas the 'inactive' label \nrepresents cancerous, inactive p53.  Class labels are determined experimentally.","7a661b9c":"### 4.1 Work on the columns title, first row and last column\nIt is clear that the first row of the data was used as columns titles and the last column is unneccessary (empty). So we:  \n\n**1.** take column title to row 1  \n**2.** convert it to numbers (because we discovered it is a string)  \n**3.** create a sequence of integers and used it as column title  \n**4.** remove the last column","e6b7fac5":"### 4.5 Encode the target variables\nWe will use Label Encoder to label the categorical data. Computer performs faster with numbers than strings. Hence we change out target values to numbers 0 and 1 for inactive and active respectively."}}