{"cell_type":{"27ca6a03":"code","6925f78b":"code","046f06bb":"code","060c1233":"code","8fda2dc0":"code","db6dd3bd":"code","80001e57":"code","255e1dca":"code","bfba9567":"code","558d5846":"code","a18fd1a8":"code","14d22304":"code","140a2659":"code","cbabb7c0":"code","e08ea7db":"code","43bcc2e6":"code","79537c0b":"code","4c91f763":"code","4dd0ca90":"code","bb104ba7":"code","5889d546":"code","61d15e9d":"code","e0129bfa":"code","72400f29":"code","743e9542":"code","9b9507e3":"code","7f1cde27":"markdown"},"source":{"27ca6a03":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6925f78b":"from nltk.corpus import stopwords\nfrom nltk.util import ngrams\nimport re\nfrom nltk.stem import PorterStemmer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import one_hot\nfrom keras.layers import Embedding\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import LSTM,Dense","046f06bb":"train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","060c1233":"train.head()","8fda2dc0":"sample.head()","db6dd3bd":"sns.barplot(train.target.value_counts().index,train.target.value_counts())\nplt.gca().set_ylabel('samples')","80001e57":"mess=train.text","255e1dca":"ps=PorterStemmer()\ncorpus=[]\nfor i in range(0,len(mess)):\n    r=re.sub('https?:\/\/\\S+|www\\.\\S+',' ',mess[i])\n    r=re.sub(r'<.*?>',' ',r)\n    r=re.sub('[^a-zA-Z]',' ',r)\n    r=r.lower()\n    r=r.split()\n    r=[ps.stem(word) for word in r if not word in stopwords.words('english')]\n    r=' '.join(r)\n    corpus.append(r)","bfba9567":"corpus","558d5846":"len_of_word=[]\nfor i in range(0,len(train.text)):\n    a=len(train.text[i])\n    len_of_word.append(a)\nsent_len=max(len_of_word)+3\nsent_len","a18fd1a8":"vocab_size=8000\nohe=[one_hot(word,vocab_size) for word in corpus]\nohe","14d22304":"embed_doc=pad_sequences(ohe,padding='pre',maxlen=sent_len)\nprint(embed_doc)","140a2659":"model=Sequential()\nmodel.add(Embedding(vocab_size,100,input_length=sent_len))\nmodel.add(LSTM(100))#dropout=0.2\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","cbabb7c0":"model.summary()","e08ea7db":"xfinal=np.array(embed_doc)\nyfinal=np.array(train.target)","43bcc2e6":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(xfinal,yfinal,test_size=0.15,random_state=143)","79537c0b":"model.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=10,batch_size=64)","4c91f763":"ypred=model.predict(xtest)\n","4dd0ca90":"y_pred=np.round(ypred).astype(int).reshape(1142)","bb104ba7":"y_pred","5889d546":"mess_final=test.text\ncorpus_final=[]\nfor i in range(0,len(mess_final)):\n    res=re.sub('https?:\/\/\\S+|www\\.\\S+',' ',mess_final[i])\n    res=re.sub(r'<.*?>',' ',res)\n    res=re.sub('[^a-zA-Z]',' ',res)\n    res=res.lower()\n    corpus_final.append(res)\n","61d15e9d":"len(corpus_final)\nvocab_size=8000\nohe_final=[one_hot(word,vocab_size) for word in corpus_final]\nembed_doc_final=pad_sequences(ohe_final,padding='pre',maxlen=160)\nembed_doc_final.shape","e0129bfa":"ypred_final=model.predict(embed_doc_final)","72400f29":"pred_final=np.round(ypred_final).astype(int).reshape(3263)","743e9542":"pred_final","9b9507e3":"sub=pd.DataFrame({'id':test['id'].values.tolist(),'target':pred_final})\nsub.to_csv('submission.csv',index=False)","7f1cde27":"**Steps for Word-Embedding**\n* Initialize parameter vocab_size\n* One hot representation\n* Pass one hot representation to Embedding layer to form matrix \n* Apply model to get predictions"}}