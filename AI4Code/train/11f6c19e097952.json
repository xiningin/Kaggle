{"cell_type":{"7212ceae":"code","43f006b4":"code","28628173":"code","3eead2d2":"code","37930f5a":"code","fa8f0242":"code","b714ba02":"code","dc2a363c":"code","b4085f7c":"code","c1c969fc":"code","0a608c34":"code","6bc65597":"code","656653ed":"code","607a4036":"code","1ebcd096":"code","7888290f":"code","5552cfd7":"code","63810ecb":"code","41f0fbd4":"code","d3c13a53":"code","001f4f20":"code","15d4ff7a":"code","da8ffcc9":"code","de8aef4e":"code","e32923eb":"code","5619f3eb":"code","addb195a":"code","ad511b5c":"code","0a0416a9":"code","cad37fbe":"code","ec783c4d":"code","d31fdf31":"code","83e5841e":"code","62b3802a":"code","c7ee2a72":"code","c29d0705":"code","0f33341c":"code","734ab21f":"code","a760e58e":"code","d1dd5ed0":"code","69c91817":"code","c48a1f4e":"code","b0b2081d":"code","8b4356e0":"code","5a0e6102":"code","0e7107ca":"code","8a0f4ac0":"markdown"},"source":{"7212ceae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","43f006b4":"#Importing relevant packages\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport sklearn\n\nfrom pandas import Series, DataFrame\nfrom sklearn import preprocessing\n\n#Encoders\nfrom sklearn.preprocessing import LabelEncoder #Ordinal Features\nfrom sklearn.preprocessing import OneHotEncoder #Nominal Features\n\n#Modelling and Evaluation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\n\nimport keras \nfrom keras.models import Sequential #Initialize\nfrom keras.layers import Dense      #Layers\n\n\nfrom sklearn import metrics\n#Suppress Warnings \nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\n\nnp.random.seed(100) #results reproducible","28628173":"#Modeling Part 1 - Logistic Regression\n\n#Assumptions for Logistic Regression include \n#A). (Rule #1): Data is free of missing values - Check missing values, discard or impute to get a comprehensive dataset.\n#B). Predictant\/Response\/Target variable is BINARY OR ORDINAL (ordered values)\n#C). (Rule #2): Predictors are indepedent of each other. Correlation leads to overfitting in ML models.\n#D). (Rule #3): At least 50 values per variable for reliable results.\n\n#When to USE IT\n# - Binary Target Variable\n# - Best for clean, well behaved data.\n\n#Modeling Part 2 - Artificial Neural Network\n\n#Activaton Functions to be used (Since it has two classes - Survived or Not Survived)\n# -ReLU: Only activates the function is if it above a certain qty. When input is 0, output is 0 but when above a certain threshold,\n# it has linear relationship with the dependent variable. Returns all +ve values for +ve values.    \n\n\n#The general steps to be followed include - \n\n# General Steps: Data Exploration, Data Wrangling, Modeling, Evaluation.\n# -Identify the Predictor and Response Variable (Survived). The target variable should be a Binary Target Variable.\n# -Explore Continuous Features (Nulls, Missing, Outliers, Distribution etc).\n# -Explore Categorical Features\n# -Feature Engineering: Fix any wrangling issues with features. Encode features, combine features etc.\n# -Drop\/subset relevant variable for modeling. \n# -Split dataset using sklearn's test_train_split\n# -Modeling: Choose appropriate parameters, build the model and feed the data.\n# -Test prediction showing a passenger will survive or not","3eead2d2":"#Import the data \ntitanic_train = pd.read_csv ('\/kaggle\/input\/titanic\/train.csv')\ntitanic_test = pd.read_csv ('\/kaggle\/input\/titanic\/train.csv')","37930f5a":"#Combine Data\ntitanic = pd.concat([titanic_train,titanic_test], axis = 0, sort = False)","fa8f0242":"titanic.info()","b714ba02":"#Rule 1: There should be no missing values.\n\n#It is evident that there are missing values in Age, Cabin and Embarked.\n#Let's inspect the missing data by plotting using seaborn and count the missing values.","dc2a363c":"titanic.describe()","b4085f7c":"#Plotting Missing values using SEABORN\nsb.heatmap(titanic.isnull(), cbar=True)","c1c969fc":"titanic.isnull().sum()","0a608c34":"#With 354 missing values in the Age variable, 1374 in Cabin and 4 in Embarked, I have decided to cater to each feature individually.\n\n#Age: 1). Mean imputation, 2). Feature Inference using Parch.\n#Cabin: Will be dealt with later.\n#Embarked: Drop (<5% of values)\n\n#There are TWO OPTIONS for imputing age:\n#Option 1: Mean Imputation. \n## titanic['Age'].fillna(titanic['Age'].mean(), inplace = True) \n\n#Option 2: Impute using other features.\n#Impute the age using the Parch variable - The underlying concept is that each parch discrete value - number of parents and children -  will have a corresponding average age in the dataset.\n#If the passenger had 0 parents and children (parch==0), their average age in the dataset will be imputed for missing age values for all members with no parents and children.\n#To do that, the first thing is to check the average age across the parch variable. A boxolot is great for such discrete, categorical variables.\n\nsb.boxplot(x='Parch',y='Age', data=titanic, palette='hls')\n\n#It can be observed that, as the median age increases, the number of parents\/children increase.\n#Can impute based on avergae number of children\/parent. For instance, a person's unknown age can be calculated \n#using average number of parch (parent\/children) they have. Ex: WHen they have 0 kids, they're median age=30.\n#If it's 1, they're age can be imputed as 22.","6bc65597":"#Checking average age across parch.\nparch_groups = titanic.groupby(titanic['Parch']) #Only returns numerical values.\nparch_groups.mean()","656653ed":"#Function for Imputing Age using Parch.\n\ndef age_cal (col):\n    Age = col[0]\n    Parch = col[1]\n    \n    if pd.isnull(Age):\n        if Parch == 0:\n            return 32\n        elif Parch ==1:\n            return 24\n        elif Parch ==2:\n            return 17\n        elif Parch ==3:\n            return 33\n        elif Parch ==4:\n            return 45\n        else:\n            return 30\n    \n    else:\n        return Age\n","607a4036":"#Applying newly created age_cal function\ntitanic['Age'] = titanic[['Age','Parch']].apply(age_cal, axis=1)\ntitanic.isnull().sum()\n\n#No missing values in Age.","1ebcd096":"# Feature: SibSp, Parch are combined. ","7888290f":"#Since SibSp (Sibling\/Spouse) and Parch are family variables. We can think of combining them but let's see if they're similar.\n#Catplot is a categorical plot.\n\nfor i,col in enumerate (['SibSp','Parch']):\n    plt.figure (i)\n    sb.catplot (x=col, y='Survived', data = titanic, kind = 'point', aspect =2,)\n\n#Larger bar idicates a smaller sample size (it's the error).\n#Categorical Plot shows the more children or parents you have, the less likely you are to survive.","5552cfd7":"#Since both the plots show very similar characteristics, we can go ahead and combine them.\n#Combine the two and drop (later) individual columns\ntitanic['Family_Count'] = titanic['Parch']+titanic['SibSp']","63810ecb":"# Features: Name, PassengerId, Ticket and Multi Collinear Variables (SibSp and Parch) are dropped.\n\ntitanic = titanic.drop(['PassengerId','Name','Ticket','SibSp','Parch'], axis=1)\ntitanic.head()\n\n#Since PassengerId, Name and Ticket are just used for identification, dropping features.\n#Drop Parch and SibSp (Will lead to multi collinearity issues).","41f0fbd4":"# Feature: Cabin\n\n#Lets check Survival based on Cabin was missing or not. Grouped by whether cabin was missing or not.\n#For those two groups, check survived column and take the mean.67% of people were missing a cabin.\ntitanic.groupby(titanic['Cabin'].isnull())['Survived'].mean()","d3c13a53":"#Make a cabin indicator for encoding 0 to missing values and 1 for non missing.\ntitanic['Cabin_ind'] = np.where(titanic['Cabin'].isnull(),0,1) #If null was found, put 0 else put 1.\ntitanic.head()","001f4f20":"# Feature: Sex\n\n#Hard coding categorical variables (for 1 being Male and 0 being Female using a Dictionary)\n\ngender_num = {'male':1,'female':0}\ntitanic['Sex'] = titanic['Sex'].map (gender_num)\ntitanic.head()","15d4ff7a":"# Once the features are engineered and data is model ready, it's time to fit it in the model.\n\n#Dropping Embarked, Cabin from the titanic dataframe.\ntitanic.drop(['Embarked','Cabin'], axis=1,inplace=True)\ntitanic.head()","da8ffcc9":"# #Rule 2 - Check for independence. \n#There should be no correlation between variables.\n\nsb.heatmap(titanic.corr())","de8aef4e":"#Pclass and Fare show a strong correlation - may lead to overfitting in model.\n#Dropping pclass SINCE it has a strong coorelation with Fare.\n\ntitanic.drop(['Pclass'], axis=1, inplace=True)\ntitanic.head()","e32923eb":"#Drop the balance Na's and reset INDEX\ntitanic.dropna(inplace=True)\ntitanic.reset_index(inplace=True,drop=True)\n\nprint (titanic.info())  #Rule3 - Checking data size (>50 in each variable to have good results)","5619f3eb":"#Let's check the distribution of the features.\n\nprint('Distributions of features')\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(titanic.columns)):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(titanic[col])\n    plt.title(col)","addb195a":"#Checking the distribution of survived vs not survived.\ntitanic['Survived'].value_counts()\n\n#The data is clearly skewed towards not survived. Scaling should take care of this later.","ad511b5c":"#Split into TRAIN, TEST.\n\npredictors = titanic.drop('Survived', axis = 1)\ntarget = titanic['Survived']\n\n#If you want to set aside data for validation as well, then uncomment the following code. This will split first into 60 (train) -40 and then 20 (test) and 20 (validation)\n# #Set aside 40% of the data for test and validation. Keep 60% for training.\n# X_train, X_test, y_train, y_test = train_test_split (predictors, target, test_size=.40, random_state = 200)\n# #Split the 40% into two: test and validation sets.\n# X_val, X_test, y_val, y_test = train_test_split (X_test, y_test, test_size=.50, random_state = 42)\n\n#Splitting the data into train and test (80-20).\nX_train, X_test, y_train, y_test = train_test_split (predictors, target, test_size=.20, random_state = 200)\n\n#Applying Scaler.\nscaler = preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train) #notice how the target feature (y) is untouched.\nX_test = scaler.fit_transform(X_test)\n\n\n","0a0416a9":"print (X_train.shape)\nprint (y_train.shape)","cad37fbe":"#Good to see the data once before building the machine learning model.\nX_train[0:100:20]","ec783c4d":"# Function to print the results.\ndef print_results(results):\n    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n\n    means = results.cv_results_['mean_test_score']\n    stds = results.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n        print('{} (+\/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))","d31fdf31":"#Building the model\n\n#Lets instantiate the model and use cross validation to find the best params.\nLogModel = LogisticRegression()\nparameters = {'C': [0.001, 0.01, 0.1, 1, 10], 'penalty': ['l2','l1']} #remember C is the regularization param, key to minimizing loss is optimizing this as one of the key params in log regression.\n#smaller value of c is stronger regularization. (it is the inverse of regularization strength)\ncv = GridSearchCV(LogModel, parameters, cv=5)\ncv.fit(X_train, y_train.ravel()) #fit it on the train data to find best params.\n\nprint_results(cv)","83e5841e":"#Since the best params identified in this case are 0.789 (+\/-0.041) for {'C': 0.1, 'penalty': 'l2'}\n#Using that to build the log model. \nLogReg = LogisticRegression(C=.1,penalty='l2')\n#Fit it on the train data.\nLogReg.fit(X_train,y_train)\n\n#Predict using the model\ny_pred = LogReg.predict(X_test)","62b3802a":"#MODEL Evaluation can be done using a). Classification Report b).K-fold cross validation\/confusion matrices.\n\n##Classification report without cross-validation.\nprint (classification_report(y_test, y_pred))","c7ee2a72":"#To check if the model is overfitting, let's check the score on train.\ny_pred_train = LogReg.predict(X_train)\nprint (classification_report(y_train, y_pred_train))","c29d0705":"#For just the precision score.\n# #Lets check the precision score.\n# precision_score(y_test, y_pred)\n\n# #Lets check the precision score.\n# precision_score(y_train, y_pred_train)","0f33341c":"#Now there is not much differnce between the prediction on the train vs prediction on the test, the model hence does not seem to be overfitting.","734ab21f":"#K-Fold Cross validation and confusion matrices\ny_train_pred = cross_val_predict(LogReg, X_train, y_train, cv=5)\nconfusion_matrix(y_train, y_train_pred)","a760e58e":"#Make a TEST PREDICTION\ntitanic[860:863] #example passenger","d1dd5ed0":"#Create a test passenger and predict the survival.\n\n#Let's make a test passenger with ID:860, Sex: Male, Age:41,Fare:15, Family_Count:2, Cabin_ind:0\ntest_passenger_array = np.array([1, 41,15, 2, 0]).reshape(1,-1)\n#This shows the prediction. (LogReg is a sigmoid function : 0 -1)\nprint(LogReg.predict(test_passenger_array))\n#This gives the probability of survival.\nprint(LogReg.predict_proba(test_passenger_array))","69c91817":"#The passenger will not have survived (0) as seen above. We can say that with almost 99% confidence. Wow!\n#And that is how you predict survival from the titanic disaster.","c48a1f4e":"#This was a mini tutorial covering all the basics of a logistic regression project.\n\n#Next, lets build an Artificial Neural Network model using the same data. ","b0b2081d":"# Initialising the Neural Network\nmodel = Sequential()\n\n#Layers\n#Input Layer\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 5))\n#Hidden Layers\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n#Ouput Layer\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n#Binary cross_entropy is for classification loss function.\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n#Fit the model\nmodel.fit(X_train, y_train, batch_size = 32, epochs = 200, verbose=2)","8b4356e0":"model.summary()","5a0e6102":"#Getting the prediction using Artificial Neural Network.\ny_pred = model.predict(X_test)\ny_fin = (y_pred > 0.5).astype(int).reshape(X_test.shape[0])\n","0e7107ca":"#That's all for now folks.","8a0f4ac0":"# Neural Network"}}