{"cell_type":{"0cc4d41f":"code","b1d03bda":"code","ba88e5cd":"code","43b52da6":"code","557cea68":"code","3f05ab17":"code","f103f71c":"code","f075ea5b":"code","d944e363":"code","524bb4ac":"code","25612465":"code","7b4852a6":"code","c6490c11":"code","8e0b5e75":"code","ad5a9ec8":"code","d2428562":"code","ef085bce":"code","ab352909":"code","f66c1f6d":"code","65270a70":"code","a82ffd73":"code","e63a3e54":"code","b981f438":"code","c1f3ee32":"code","ab2d7691":"code","1d6f6e40":"code","d21d4d83":"code","cb5d8bd5":"markdown","93e7f0c1":"markdown","a019e8f2":"markdown","83d43c78":"markdown","1f253ad2":"markdown","d7bfe871":"markdown","3e9070d1":"markdown","33fe4ab3":"markdown","5ff480a0":"markdown","fdc846bc":"markdown"},"source":{"0cc4d41f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1d03bda":"train_data =pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data =pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsubmission_format = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","ba88e5cd":"# show the columns of train data\ntrain_data.head()","43b52da6":"# this command is used to show the types of features and if contain null-value \ntrain_data.info()","557cea68":"train_data['1stFlrSF'].value_counts()","3f05ab17":"# to be used to split train and test set from the combined dataframe\ntrain_index = train_data.shape[0]\n# concat train and test in all_data file\nall_data = pd.concat((train_data, test_data))\nprint('The new shape of all_data is:',all_data.shape)","f103f71c":"#drop the \"id\" column from the data\nall_data = all_data.drop('Id', axis=1)","f075ea5b":"#divide the features into catagorical, int64 and float64\ncolumns = all_data.columns\ncat_f = []\nfor column in columns:\n    if all_data[column].dtype =='object':\n        cat_f.append(column)","d944e363":"#list of ordinal features\nORDINAL_FEATURES = ['KitchenQual','GarageQual', 'HeatingQC','ExterQual', 'MSSubClass',\n                   'LandSlope', 'OverallQual', 'BsmtQual']","524bb4ac":"#list of numerical features\nNUMERICAL_FEATURES = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n                      'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n                     'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n                     'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch',\n                     '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'SalePrice']","25612465":"# find the list of most_miss columns\nmost_miss_column = []\nfor column in columns:\n    if all_data[column].isna().sum() >1300:\n        most_miss_column.append(column)","7b4852a6":"# remove the SalePrice from removing list features\nmost_miss_column.remove('SalePrice')\n\n# drop all the faetures that contain above 1300 missing value\nall_data = all_data.drop(most_miss_column, axis=1)","c6490c11":"#fill cat features\nall_data[all_data.columns.difference(NUMERICAL_FEATURES)] = all_data[all_data.columns.difference(NUMERICAL_FEATURES)].fillna(all_data[all_data.columns.difference(NUMERICAL_FEATURES)].mode().iloc[0])\n\n#fill numerical features\nall_data [NUMERICAL_FEATURES] = all_data [NUMERICAL_FEATURES].fillna(all_data [NUMERICAL_FEATURES].mean())","8e0b5e75":"#encode ordinal features \nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\nall_data [ORDINAL_FEATURES] = encoder.fit_transform(all_data [ORDINAL_FEATURES])","ad5a9ec8":"#exclute the cat features from numerical and ordinal\nun_cat_feature = ORDINAL_FEATURES + NUMERICAL_FEATURES\n\nCATEGORICAL_FEATURS= all_data[all_data.columns.difference(un_cat_feature)].columns\n","d2428562":"#maping the values of columns to be catagoy\nfrom sklearn.preprocessing import LabelEncoder\n\nfor column in all_data[CATEGORICAL_FEATURS].columns:\n    encoder = LabelEncoder()\n    all_data[column] =encoder.fit_transform(all_data[column])\nall_data[CATEGORICAL_FEATURS].head()","ef085bce":"#the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\ncorr_matrix = all_data.corr().abs()\nprint (corr_matrix)\n","ab352909":"sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n                  .stack()\n                  .sort_values(ascending=False))\nsol","f66c1f6d":"#plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#get correlations of each features in dataset\ncorrmat = all_data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(40,40))\nplt.savefig('.\/heatMap.png')\n\n#plot heat map\ng=sns.heatmap(all_data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","65270a70":"train_df = all_data[:train_index]\ntest_df = all_data[train_index:]\n\n#drop the target column from test set\ntest_df = test_df.drop(['SalePrice'], axis=1)\n\n# Check the shapes of the split dataset\ntrain_df.shape, test_df.shape","a82ffd73":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nFEATURES = train_df.iloc[:,0:train_df.shape[1]-1]  #independent columns\nLABEL = train_df.iloc[:,-1]    #target column i.e price range\n\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=5)\nfit = bestfeatures.fit(FEATURES,LABEL)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(pd.DataFrame(FEATURES).columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(5,'Score'))  #print 10 best features","e63a3e54":"best_features = ['LotArea', 'MiscVal', '2ndFlrSF', 'BsmtFinSF1', 'PoolArea']","b981f438":"# Plot\nplt.figure(figsize=(10,8), dpi= 80)\nsns.pairplot(train_df[best_features], kind=\"reg\")\nplt.show()","c1f3ee32":"#import the linerRegression algorithm\nfrom sklearn.linear_model import LinearRegression\n\n#import evaluation metrics\nfrom sklearn.model_selection import cross_val_score\n\n# import suffule method for suffiling the data during iteration\nfrom sklearn.model_selection import ShuffleSplit\n\n\n\nn_splits = 10 # Number of parts that the data will splilit\ntest_size = 0.20 #Test size for the dataset\nseed =42 # to save the randomness\nkfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n\n#initi the algorithm object\nregressior =LinearRegression()\n\n#fit the algorithm\nregressior.fit(train_df[best_features], LABEL)\n\n# calculate the mean of losses in each k_fold\nresults = cross_val_score(regressior, np.array(train_df[best_features]),np.array(LABEL), cv=kfold)\nprint(regressior)\nprint('Loss:', results.mean())","ab2d7691":"# predict y_pred using test data\nY_PRED =regressior.predict(test_df[best_features])","1d6f6e40":"# create the submission format\nsubmission_format =submission_format.drop('SalePrice', axis=1)\nsubmission_format['SalePrice'] = Y_PRED\n\n# save dataframe to csv file\nsubmission_format.to_csv('.\/submission.csv', index=False)","d21d4d83":"# show 5 row in submission dataframe\nsubmission_format.head()","cb5d8bd5":"# Step 1: Lood the data into the project","93e7f0c1":"After the data exploared this is list of iusses must be considered in the next section:\n* fill the null values in depend on the type of feature.\n* the catagorical features must be encoded.\n* concat the train and test data in one file for easy preprocessing.","a019e8f2":"There is nearly 79 features used to predict the <code>SalePrice<\/code>","83d43c78":"# Step 2: Exploaring the data","1f253ad2":"# Step 4: Model selection","d7bfe871":"*  Now the catagorical features must be differnciated from ordinal features and numerical features to fill all the missing data with different senario.\n","3e9070d1":"# Step 3: Preprocessing the dataset","33fe4ab3":"## Feature Selection\n<code>SelectKBest<\/code> from <code>sickit-learn<\/code> package will be used to sellect most contributed features tto prediction.","5ff480a0":"open <code>data_description<\/code> file to understand our features one by one","fdc846bc":"## Submission file"}}