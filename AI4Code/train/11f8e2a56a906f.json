{"cell_type":{"d29fd518":"code","e38b7fbf":"code","8a777ca8":"code","c5530d5d":"code","ea433fb6":"code","94b3962c":"code","ad2f161a":"code","ff1a49f4":"code","fc4758d7":"code","50fdff74":"code","9b0864a4":"code","c8345c0a":"code","6adadcb4":"code","0a9ebb9e":"code","46598e8a":"code","d3022647":"markdown","dbd124e7":"markdown","5b98d09b":"markdown","a5d868bd":"markdown","03364446":"markdown","63915caf":"markdown","28c80e17":"markdown","cfa96807":"markdown","722cba98":"markdown","58230a98":"markdown","d41cbcc6":"markdown","aeefdc7c":"markdown"},"source":{"d29fd518":"%%HTML\n<style type=\"text\/css\">\n\ndiv.h1 {\n    font-size: 32px; \n    margin-bottom:2px;\n    background-color: steelblue; \n    color: white; \n    text-align: center;\n}\ndiv.h2 {\n    background-color: steelblue; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 24px; \n    max-width: 1500px; \n    margin-top: 50px;\n    margin-bottom:4px;\n    \n}\ndiv.h3 {\n    color: steelblue; \n    font-size: 20px; \n    margin-top: 4px; \n    margin-bottom:8px;\n}\ndiv.h4 {\n    font-size: 15px; \n    margin-top: 20px; \n    margin-bottom: 8px;\n}\n\n<\/style>","e38b7fbf":"# Familiar imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore') # To supress warnings\nfrom sklearn import model_selection","8a777ca8":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n\n# Preview the data\ntrain.head()","c5530d5d":"#check shape and missing values  \nprint(\"#\"*40,\"\\nTrain\")\nprint(f'There are {train.shape[0]} rows and {train.shape[1]} columns') # fstring \n#display(missing_df.sort_values(by='Missing', ascending=False))\nprint(f'There are {train.isnull().sum().mean()} missing values')\n\nprint(\"#\"*40,\"\\nTest\")\nprint(f'There are {test.shape[0]} rows and {test.shape[1]} columns') # fstring \n#display(missing_df.sort_values(by='Missing', ascending=False))\nprint(f'There are {test.isnull().sum().mean()} missing values')","ea433fb6":"#creating a list of categorical variables\n#cat_col=train.select_dtypes(include='object').columns.to_list()\ncat_col = [col for col in train.columns if 'cat' in col]\ncat_col","94b3962c":"num_col=train.select_dtypes(include='number').columns.to_list()\n#num_col = [col for col in train.columns if 'number' in col]\nnum_col","ad2f161a":"#dropping id column\n#train.drop(['id'],inplace=True,axis=1)","ff1a49f4":"train.describe().T.style.bar(subset=['mean'], color='#FF595E')\\\n                           .background_gradient(subset=['50%'], cmap='PiYG') # highlight median","fc4758d7":"\ncols = 3\nrows = len(num_col) \/\/ cols+1\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(19,30), sharex=False) #subplot with all rows\nplt.subplots_adjust(hspace = 0.4)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(num_col):\n            axs[r, c].set_visible(False)\n        else:\n            axs[r,c].hist(train[num_col[i]].values,\n                                   color=\"#FF595E\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\",bins=40)\n            axs[r, c].set_title(num_col[i], fontsize=17, pad=4)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n            axs[r,c].spines['right'].set_visible(False)\n            axs[r,c].spines['top'].set_visible(False)\n\n        i+=1\n\nplt.show();","50fdff74":"# Making a list of all categorical variables\n\nplt.figure(figsize=(14,20))\n\nsns.set_theme(style=\"white\")\nfor i, variable in enumerate(cat_col):\n                     plt.subplot(9,2,i+1)\n                     order = train[variable].value_counts(ascending=False).index   \n                     #sns.set_palette(list_palette[i]) # to set the palette\n                     sns.set_palette('Set2')\n                     ax=sns.countplot(x=train[variable], data=train )\n                     sns.despine(top=True,right=True,left=True) # to remove side line from graph\n                     for p in ax.patches:\n                           percentage = '{:.1f}%'.format(100 * p.get_height()\/len(train[variable]))\n                           x = p.get_x() + p.get_width() \/ 2 - 0.05\n                           y = p.get_y() + p.get_height()\n                           plt.annotate(percentage, (x, y),ha='center')\n                     plt.tight_layout()\n                     plt.title(cat_col[i].upper())","9b0864a4":"corr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Correlation matrix for Train data')\nsns.heatmap(corr, mask = mask,annot=True,  linewidths = .5,square=True,cbar_kws={\"shrink\": .60})\nplt.show()","c8345c0a":"#check correlation with target varaiable\nloss_corr=corr['target'].sort_values(ascending=False).head(10).to_frame()\n\ncm = sns.light_palette(\"pink\", as_cmap=True)\n\nloss = loss_corr.style.background_gradient(cmap=cm)\nloss","6adadcb4":"train[\"kfold\"] = -1","0a9ebb9e":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=train)):\n    train.loc[valid_indicies, \"kfold\"] = fold","46598e8a":"#create a new csv with folds. The csv would have extra column with the number of fold that sample belongs to. \ntrain.to_csv(\"train_folds.csv\", index=False)","d3022647":"<a id='rd'><\/a>\n<div class=\"h2\">Read and Understand Data<\/div>\n","dbd124e7":"This notebook is created as part  of 30 day ML challenge. There are two part to this notebook.One with EDA and Kfold creation and other with actual model\n\n**About the data**\nThe dataset is used for this competition is synthetic, but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n\n\n**Evaluation**\n\nSubmissions are scored on the root mean squared error.\n\n\n**Special mention**\nI learned about kfold and finding mean of prediction from Abhishek Thakur YoutubeChannel. For that code you can refer his notebook or watch youtube channel.Thank you Abhisek","5b98d09b":"There are 10 categorical variables which we will have to encode ","a5d868bd":"I had learned about cross_val_score which we can use to do cross validation, but this is something new i learned in Abhishek's video.Using iterator method.\n\nKFold divides all the samples in  groups of samples, called folds of equal sizes (if possible). The prediction function is learned using  folds, and the fold left out is used for test.\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_0041.png\" height=\"400\" width=\"720\"\/>","03364446":"\n<div class=\"h1\">30 Days of ML -Competition<\/div>\n\n","63915caf":"<div class=\"h3\">Observation <\/div>\n\n- Training set has 300000 observations with 26 features .\n- Testing set has 200000 observations with 25 features\n- There are no missing values in both sets\n","28c80e17":"<a id='il'><\/a>\n<div class=\"h3\">Import Libraries<\/div>","cfa96807":"<div class=\"h3\">Observation <\/div>\n- There is very little correlation between variables","722cba98":"<div class=\"h1\">Table of Contents<\/div>\n\n  - <a href='#il'>Import Libraries<\/a>\n  - <a href='#rd'>Read and Understand Data<\/a>\n  - <a href='#da'>Exploratory Data Analysis<\/a>\n  - <a href='#kf'>Kfolds<\/a>\n ","58230a98":"<a id='da'><\/a>\n<div class=\"h2\">Exploratory Data Analysis<\/div>","d41cbcc6":"<a id='kf'><\/a>\n<div class=\"h2\">Kfolds<\/div>\n\nCross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\nThe procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=5 becoming 5-fold cross-validation.\n\n**For more references check**\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html\n\nhttps:\/\/towardsdatascience.com\/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\n\nhttps:\/\/machinelearningmastery.com\/k-fold-cross-validation\/\n\nhttps:\/\/youtu.be\/t5fhRP62YdE (A video from Abhishek Thakur)\n\n","aeefdc7c":"<div class=\"h3\">Observation <\/div>\n\n- Target variable `target` feature is left skewed\n- Target variable `target` is dominated by value 8\n- very few features have normal disturbution.\n"}}