{"cell_type":{"fa5ee1cd":"code","24d8f5cc":"code","337d8742":"code","cf98298a":"code","7df3c601":"code","acb8fd4b":"code","ae420309":"code","5c8ebcd0":"code","b01a7ed2":"code","98bda1bf":"code","deaac306":"code","fa8428a1":"code","b792543e":"code","b964565c":"code","94c25bb6":"code","7347ddb1":"code","ff683ef0":"code","056846ab":"code","182e857f":"code","6b274a29":"code","d55df621":"code","fe0357c8":"code","e97de6fe":"code","f426c946":"code","df395d5f":"code","95b67e92":"code","4f4acdbc":"markdown","90e90e86":"markdown","3c7c4211":"markdown","33e4603b":"markdown","3567ebfd":"markdown","baa069a0":"markdown","1b41dfaa":"markdown","60940c63":"markdown"},"source":{"fa5ee1cd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns","24d8f5cc":"#aqui lemos os dados e retiramos todas as features que n\u00e3o v\u00e3o ser usadas na previs\u00e3o por raz\u00f5es variadas e retiramos as linhas em que h\u00e1 dados faltantes\nteste = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\")\ntreino = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\")\nnteste = teste.dropna()\nntreino = treino.dropna()\nnteste = nteste.drop([\"workclass\", \"fnlwgt\", \"education\", \"relationship\", \"marital.status\", \"occupation\", \"native.country\"], axis=1)\nntreino = ntreino.drop([\"workclass\", \"fnlwgt\", \"education\", \"relationship\", \"marital.status\", \"occupation\", \"native.country\"], axis=1)","337d8742":"#aqui convertemos os valores de income, sex e race de strings para inteiros ou booleanos, para que possam\n#ser analisadas numericamente\nntreino[\"income\"] = ntreino[\"income\"].map({\"<=50K\":0, \">50K\":1})\nntreino[\"sex\"] = ntreino[\"sex\"].map({\"Male\":0, \"Female\":1})\nntreino[\"race\"] = ntreino[\"race\"].map({\"White\":0, \"Asian-Pac-Islander\":1, \"Other\":1, \"Amer-Indian-Eskimo\":2, \"Black\":3})\nnteste[\"sex\"] = nteste[\"sex\"].map({\"Male\": 0, \"Female\":1})\nnteste[\"race\"] = nteste[\"race\"].map({\"White\":0, \"Asian-Pac-Islander\":1, \"Other\":1, \"Amer-Indian-Eskimo\":2, \"Black\":3})","cf98298a":"ntreino.head()","7df3c601":"sns.heatmap(ntreino.corr(), annot=True, vmin=-1, vmax=1)","acb8fd4b":"sns.pairplot(ntreino, hue='income')","ae420309":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\ntreino_features = ntreino[['age', 'education.num', 'race', 'sex', 'capital.gain', 'capital.loss','hours.per.week']]\ntreino_features = treino_features.values\ntreino_resultado = ntreino[['income']]\ntreino_resultado = treino_resultado.values\n\nmodel = AdaBoostClassifier(RandomForestClassifier(n_estimators = 100),\n                         algorithm=\"SAMME\",\n                         n_estimators=500)","5c8ebcd0":"model = model.fit(treino_features, treino_resultado)","b01a7ed2":"print ('Accuracy = {:0.2f}%'.format(100.0 * accuracy_score(treino_resultado, model.predict(treino_features))))","98bda1bf":"nteste1 = nteste[['age', 'education.num', 'race', 'sex', 'capital.gain', 'capital.loss','hours.per.week']]\npredicao = model.predict(nteste1.values)","deaac306":"df = pd.DataFrame(predicao)\ndf = df.replace(0,\"<=50K\")\ndf = df.replace(1,\">50K\")\ndf","fa8428a1":"arquivo = \"predicao.csv\"\npredicao = pd.DataFrame(nteste, columns = [\"income\"])\npredicao[\"income\"] = df\npredicao.to_csv(arquivo, index_label=\"Id\")\npredicao","b792543e":"from sklearn import svm\n\nclf_svm = svm.SVC()\nclf_svm.fit(treino_features, treino_resultado)","b964565c":"print ('Accuracy = {:0.2f}%'.format(100.0 * accuracy_score(treino_resultado, clf_svm.predict(treino_features))))","94c25bb6":"predicao_svm = clf_svm.predict(nteste1.values)","7347ddb1":"df_svm = pd.DataFrame(predicao_svm)\ndf_svm = df_svm.replace(0,\"<=50K\")\ndf_svm = df_svm.replace(1,\">50K\")\ndf_svm","ff683ef0":"arquivo = \"predicao_svm.csv\"\npredicao_svm = pd.DataFrame(nteste, columns = [\"income\"])\npredicao_svm[\"income\"] = df_svm\npredicao_svm.to_csv(arquivo, index_label=\"Id\")\npredicao_svm","056846ab":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\nnaivebayes = gnb.fit(treino_features, treino_resultado)","182e857f":"print ('Accuracy = {:0.2f}%'.format(100.0 * accuracy_score(treino_resultado, naivebayes.predict(treino_features))))","6b274a29":"predicao_gnb = naivebayes.predict(nteste1.values)","d55df621":"df_gnb = pd.DataFrame(predicao_gnb)\ndf_gnb = df_gnb.replace(0,\"<=50K\")\ndf_gnb = df_gnb.replace(1,\">50K\")\ndf_gnb","fe0357c8":"test = pd.read_csv(\"..\/input\/atividade-3-pmr3508\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/atividade-3-pmr3508\/train.csv\")\nntest = test.dropna()\nntrain = train.dropna()","e97de6fe":"train.head()","f426c946":"sns.heatmap(train.corr(), annot=True, vmin=-1, vmax=1)","df395d5f":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split \n\ntrain_features = ntrain[['longitude', 'latitude', 'median_age', 'total_rooms', 'total_bedrooms', 'population','households','median_income']]\ntrain_features = train_features.values\ntrain_resultado = ntrain[['median_house_value']]\ntrain_resultado = train_resultado.values\n\nlasso = Lasso()\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\nlasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv = 5)\nlasso_regressor.fit(train_features, train_resultado)","95b67e92":"from sklearn.metrics import r2_score\n\npred_lasso = lasso_regressor.predict(train_features)\nscore = r2_score(train_resultado,pred_lasso)\nprint(\"Coeficiente de determina\u00e7\u00e3o:\",score)","4f4acdbc":"Conclus\u00e3o: o coeficiente de determina\u00e7\u00e3o resultou em aproximadamente 0,64; ou seja, o modelo de regress\u00e3o consegue explicar 64% dos valores observados, o que \u00e9 um valor melhor do que uma decis\u00e3o aleat\u00f3ria; por\u00e9m, ainda resta 1\/3 dos valores que ele n\u00e3o consegue explicar, dando espa\u00e7o para um erro consider\u00e1vel. Assim, ele n\u00e3o \u00e9 um regressor eficiente mesmo sendo melhor que o aleat\u00f3rio.","90e90e86":"1 Base Adult","3c7c4211":"# Adult + CaliforniaHousing - Isadora Bisognin","33e4603b":"Conclus\u00e3o:\nO classificador com a melhor acur\u00e1cia obtido foi o AdaBoost Classifier, com uma acur\u00e1cia de 90,28%; j\u00e1 o classificador de SVM ficou com acur\u00e1cia de 86,66% e o Naive Bayes Gaussiano (GNB) com acur\u00e1cia de 79,67%. O fato do GNB ter tido o pior desempenho se deve provavelmente ao fato dele funcionar assumindo duas afirma\u00e7\u00f5es que n\u00e3o s\u00e3o necessariamente verdadeiras com rela\u00e7\u00e3o \u00e0s vari\u00e1veis: primeiramente, que a distribui\u00e7\u00e3o de probabilidade desses dados \u00e9 uma normal; e segundo que as features x1, x2, ..., xn s\u00e3o condicionalmente independentes. ","3567ebfd":"1.1 Classifica\u00e7\u00e3o com AdaBoost","baa069a0":"Extra: CaliforniaHousing com regress\u00e3o","1b41dfaa":"1.3 Classifica\u00e7\u00e3o com Naive Bayes Gaussiano","60940c63":"1.2 Classifica\u00e7\u00e3o com SVM"}}