{"cell_type":{"7df024d1":"code","b4f0ed34":"code","390f12f3":"code","980d462c":"code","cec58926":"code","1b26d296":"code","af9fe03e":"code","2c9ac618":"code","e5591f39":"code","ce6dcb0f":"code","387047ca":"code","2d2167ba":"code","014bb0f8":"code","46f960c1":"code","bce6e1be":"code","d1ea4e45":"code","21b42cc6":"code","67f229bb":"code","2aade9e4":"code","6c61af12":"code","4af56936":"code","bf029069":"code","eb5ae020":"code","d135cdd0":"code","a75306a6":"code","f452d40d":"code","ddf76299":"code","e0a4cfa8":"code","243b87be":"code","311b5410":"code","8567c81b":"code","d4a42ada":"code","98b2a51a":"code","9f3cf00a":"code","d285ef9c":"code","af730393":"code","44fd4af7":"code","616b0f52":"code","b69f6304":"code","9c2fa8ed":"code","8e9ed993":"code","4d391e9d":"code","4a70ae37":"code","0609e3c7":"code","8b9092c2":"code","68a0f228":"code","b019c56e":"code","a4e7dba1":"code","e8ad8187":"code","74bbba4b":"code","1d1a763a":"code","78329489":"code","c9614434":"code","d9d4b549":"code","f837c3d3":"code","c971d99e":"code","7c513751":"code","1bc12d16":"code","08f0717a":"code","83857f89":"code","cad344b0":"code","3241eed4":"code","ee766fa4":"code","e3408026":"code","76b1834f":"code","239e033f":"code","75943d52":"code","1d10c464":"code","81d1ce3b":"code","7d5b55bc":"code","b7061b4c":"code","b33f3b31":"code","bc9c21a3":"code","7d58b482":"code","1c3d8ebe":"code","e97599f3":"code","48da06d5":"code","e0c8dfd2":"code","a0befae1":"code","311441ef":"markdown","9195c191":"markdown","1940c115":"markdown","ef2f7cfc":"markdown","02d3a9d7":"markdown","08dbc53a":"markdown","40f3de9b":"markdown","b87cd4b4":"markdown","14dffda3":"markdown","5175b6dc":"markdown","e72a63f0":"markdown","7f8ac05f":"markdown","56a70db8":"markdown","62769102":"markdown","52b05fe3":"markdown","43e20d02":"markdown","ff7061ac":"markdown","7d225cdd":"markdown","390fc4d0":"markdown","1f1df214":"markdown","e5835518":"markdown","21d671da":"markdown","ebe23125":"markdown","b48a5522":"markdown","c534fe63":"markdown","ce525dcd":"markdown","22741a94":"markdown","78758551":"markdown","522c5478":"markdown","ea8eee4c":"markdown","0a43144f":"markdown","dfc41f1c":"markdown","4792f624":"markdown","cad49a60":"markdown","5893aef1":"markdown","05dc531e":"markdown","22641d08":"markdown","db37208c":"markdown","a9fe759a":"markdown","b96b7143":"markdown","1dc45527":"markdown","2eba559d":"markdown","0e632ec7":"markdown","10dd9622":"markdown","14a29fcf":"markdown","f6200284":"markdown","a17ed6b8":"markdown","314bb0e6":"markdown","5cae7525":"markdown","708a0580":"markdown","faab6d15":"markdown","13d9eaf9":"markdown"},"source":{"7df024d1":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom matplotlib.pyplot import figure\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.dates as mdates\nfrom sklearn import linear_model\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.svm import SVR\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b4f0ed34":"df_final = pd.read_csv(\"\/kaggle\/input\/gold-price-prediction-dataset\/FINAL_USO.csv\",na_values=['null'],index_col='Date',parse_dates=True,infer_datetime_format=True)","390f12f3":"df_final.head()","980d462c":"df_final.shape","cec58926":"df_final.describe()","1b26d296":"df_final.isnull().values.any()","af9fe03e":"GLD_adj_close = df_final['Adj Close']\nSPY_adj_close = df_final['SP_Ajclose']\nDJ_adj_close  = df_final['DJ_Ajclose']\n\ndf_p = pd.DataFrame({'GLD':GLD_adj_close, 'SPY':SPY_adj_close, 'DJ':DJ_adj_close})\n\ndf_ax = df_p.plot(title='Effect of Index prices on gold rates',figsize=(15,8))\n\ndf_ax.set_ylabel('Price')\ndf_ax.legend(loc='upper left')\nplt.show()","2c9ac618":"def compute_daily_returns(df):\n    \"\"\"Compute and return the daily return values.\"\"\"\n    # TODO: Your code here\n    # Note: Returned DataFrame must have the same number of rows\n    daily_return = (df \/ df.shift(1)) - 1\n    daily_return[0] = 0\n    return daily_return\n","e5591f39":"GLD_adj_close = df_final['Adj Close']\nSPY_adj_close = df_final['SP_Ajclose']\nDJ_adj_close  = df_final['DJ_Ajclose']\nEG_adj_close =  df_final['EG_Ajclose']\nUSO_Adj_close = df_final['USO_Adj Close']\nGDX_Adj_close = df_final['GDX_Adj Close']\nEU_price      = df_final['EU_Price']\nOF_price      = df_final['OF_Price']\nOS_price      = df_final['OS_Price']\nSF_price      = df_final['SF_Price']\nUSB_price      = df_final['USB_Price']\nPLT_price      = df_final['PLT_Price']\nPLD_price      = df_final['PLD_Price']\nrho_price      = df_final['RHO_PRICE']\nusdi_price      = df_final['USDI_Price']\n\n\n\nGLD_daily_return = compute_daily_returns(GLD_adj_close)\nSPY_daily_return = compute_daily_returns(SPY_adj_close)\nDJ_adj_return    = compute_daily_returns(DJ_adj_close)\nEG_adj_return     = compute_daily_returns(EG_adj_close)\nUSO_Adj_return    = compute_daily_returns(USO_Adj_close)\nGDX_Adj_return   =compute_daily_returns(GDX_Adj_close)\nEU_return        = compute_daily_returns(EU_price)\nOF_price         =compute_daily_returns(OF_price)\nOS_price         =compute_daily_returns(OS_price)\nSF_price         =compute_daily_returns(SF_price)\nUSB_price         =compute_daily_returns(USB_price)\nPLT_price         =compute_daily_returns(PLT_price)\nPLD_price         =compute_daily_returns(PLD_price)\nrho_price         =compute_daily_returns(rho_price)\nUSDI_price         =compute_daily_returns(usdi_price)\n\ndf_d = pd.DataFrame({'GLD':GLD_daily_return, 'SPY':SPY_daily_return, 'DJ':DJ_adj_return, 'EG':EG_adj_return, 'USO':USO_Adj_return,\n                  'GDX':GDX_Adj_return,'EU':EU_return, 'OF':OF_price,'SF':SF_price,'OS':OS_price, 'USB':USB_price, 'PLT':PLT_price, 'PLD':PLD_price,\n                  'RHO':rho_price,'USDI':USDI_price})\n\ndaily_ax = df_d[-100:].plot(title='Last 100 records of daily return of all features',figsize=(15,8))\n\ndaily_ax.set_ylabel('Daily return')\ndaily_ax.legend(loc='lower left')\nplt.show()","ce6dcb0f":"df_s = pd.DataFrame({'GLD':GLD_daily_return, 'SPY':SPY_daily_return, 'DJ':DJ_adj_return})\n\ndaily_ax = df_s[-100:].plot(title='Last 100 records of daily return of Stock Indexes',figsize=(15,8))\n\ndaily_ax.set_ylabel('Daily return')\ndaily_ax.legend(loc='lower left')\nplt.show()","387047ca":"df_d.plot(kind='scatter', x='SPY', y='GLD')","2d2167ba":"df_d.plot(kind='scatter', x='DJ', y='GLD')","014bb0f8":"df_d.plot(kind='scatter', x='EG', y='GLD')","46f960c1":"df_d.plot(kind='scatter', x='USO', y='GLD')","bce6e1be":"df_d.plot(kind='scatter', x='USB', y='GLD')","d1ea4e45":"df_d.plot(kind='scatter', x='EU', y='GLD')","21b42cc6":"df_d.plot(kind='scatter', x='PLT', y='GLD')","67f229bb":"df_d.plot(kind='scatter', x='PLD', y='GLD')","2aade9e4":"# computing mean,standard deviation and kurtosis of Gold ETF daily return\n\nmean=df_d['GLD'].mean()\n# computing standard deviation of Gold stock\nstd=df_d['GLD'].std()\nkurt=df_d['GLD'].kurtosis()\nprint('Mean=',mean)\nprint('Standard Deviation=',std)\nprint('Kurtosis=',kurt)\n#Plotting Histogram\ndf_d['GLD'].hist(bins=20)\n\nplt.axvline(mean, color='w',linestyle='dashed',linewidth=2)\nplt.axvline(std, color='r',linestyle='dashed',linewidth=2)\nplt.axvline(-std, color='r',linestyle='dashed',linewidth=2)\nplt.title(\"Plotting of Mean, Standard deviation and Kurtosis of Gold Prices\")\nplt.show()","6c61af12":"# computing mean,standard deviation and kurtosis of S&P 500 Index daily return\n\nmean=df_d['SPY'].mean()\n# computing standard deviation of Gold stock\nstd=df_d['SPY'].std()\nkurt=df_d['SPY'].kurtosis()\nprint('Mean=',mean)\nprint('Standard Deviation=',std)\nprint('Kurtosis=',kurt)\n#Plotting Histogram\ndf_d['SPY'].hist(bins=20)\n\nplt.axvline(mean, color='w',linestyle='dashed',linewidth=2)\nplt.axvline(std, color='r',linestyle='dashed',linewidth=2)\nplt.axvline(-std, color='r',linestyle='dashed',linewidth=2)\nplt.title(\"Plotting of Mean, Standard deviation and Kurtosis of SPY Prices\")\nplt.show()","4af56936":"# computing mean,standard deviation and kurtosis of Dow Jones Index daily return\nmean=df_d['DJ'].mean()\n# computing standard deviation of Gold stock\nstd=df_d['DJ'].std()\nkurt=df_d['DJ'].kurtosis()\nprint('Mean=',mean)\nprint('Standard Deviation=',std)\nprint('Kurtosis=',kurt)\n#Plotting Histogram\ndf_d['DJ'].hist(bins=20)\n\nplt.axvline(mean, color='w',linestyle='dashed',linewidth=2)\nplt.axvline(std, color='r',linestyle='dashed',linewidth=2)\nplt.axvline(-std, color='r',linestyle='dashed',linewidth=2)\nplt.title(\"Plotting of Mean, Standard deviation and Kurtosis of Dow jones Prices\")\nplt.show()","bf029069":"plt.figure(figsize=(24,18)) \nsns.heatmap(df_final.corr(), annot=True) ","eb5ae020":"X=df_final.drop(['Adj Close'],axis=1)\nX=X.drop(['Close'],axis=1)","d135cdd0":"X.corrwith(df_final['Adj Close']).plot.bar(\n        figsize = (20, 10), title = \"Correlation with Adj Close\", fontsize = 20,\n        rot = 90, grid = True)","a75306a6":"corr_matrix=df_final.corr()\ncoef=corr_matrix[\"Adj Close\"].sort_values(ascending=False)","f452d40d":"pos_corr=coef[coef>0]\npos_corr","ddf76299":"neg_corr=coef[coef<0]\nneg_corr","e0a4cfa8":"def calculate_MACD(df, nslow=26, nfast=12):\n    emaslow = df.ewm(span=nslow, min_periods=nslow, adjust=True, ignore_na=False).mean()\n    emafast = df.ewm(span=nfast, min_periods=nfast, adjust=True, ignore_na=False).mean()\n    dif = emafast - emaslow\n    MACD = dif.ewm(span=9, min_periods=9, adjust=True, ignore_na=False).mean()\n    return dif, MACD\n\ndef calculate_RSI(df, periods=14):\n    # wilder's RSI\n    delta = df.diff()\n    up, down = delta.copy(), delta.copy()\n\n    up[up < 0] = 0\n    down[down > 0] = 0\n\n    rUp = up.ewm(com=periods,adjust=False).mean()\n    rDown = down.ewm(com=periods, adjust=False).mean().abs()\n\n    rsi = 100 - 100 \/ (1 + rUp \/ rDown)\n    return rsi\n\ndef calculate_SMA(df, peroids=15):\n    SMA = df.rolling(window=peroids, min_periods=peroids, center=False).mean()\n    return SMA\n\ndef calculate_BB(df, peroids=15):\n    STD = df.rolling(window=peroids,min_periods=peroids, center=False).std()\n    SMA = calculate_SMA(df)\n    upper_band = SMA + (2 * STD)\n    lower_band = SMA - (2 * STD)\n    return upper_band, lower_band\n\ndef calculate_stdev(df,periods=5):\n    STDEV = df.rolling(periods).std()\n    return STDEV","243b87be":"fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))\n\n# Calculate Simple Moving Average for GLD\nSMA_GLD = calculate_SMA(GLD_adj_close)\n\nGLD_adj_close[:365].plot(title='GLD Moving Average',label='GLD', ax=axes[0])\n\nSMA_GLD[:365].plot(label=\"SMA\",ax=axes[0])\n\n\n# Calculate Bollinger Bands for GLD\nupper_band, lower_band = calculate_BB(GLD_adj_close)\n\nupper_band[:365].plot(label='upper band', ax=axes[0])\nlower_band[:365].plot(label='lower band', ax=axes[0])\n\n\n# Calculate MACD for GLD\nDIF, MACD = calculate_MACD(GLD_adj_close)\n\nDIF[:365].plot(title='DIF and MACD',label='DIF', ax=axes[1])\nMACD[:365].plot(label='MACD', ax=axes[1])\n\n# Calculate RSI for GLD\nRSI = calculate_RSI(GLD_adj_close)\nRSI[:365].plot(title='RSI',label='RSI', ax=axes[2])\n\n# Calculating Standard deviation for GLD\nSTDEV= calculate_stdev(GLD_adj_close)\nSTDEV[:365].plot(title='STDEV',label='STDEV', ax=axes[3])\n\nOpen_Close=df_final.Open - df_final.Close\n\nHigh_Low=df_final.High-df_final.Low\n\naxes[0].set_ylabel('Price')\naxes[1].set_ylabel('Price')\naxes[2].set_ylabel('Price')\naxes[3].set_ylabel('Price')\n\n\n\naxes[0].legend(loc='lower left')\naxes[1].legend(loc='lower left')\naxes[2].legend(loc='lower left')\naxes[3].legend(loc='lower left')","311b5410":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\nOpen_Close=df_final.Open - df_final.Close\nOpen_Close[:365].plot(title='Open-close',label='Open_Close', ax=axes[0])\n\nHigh_Low=df_final.High-df_final.Low\nHigh_Low[:365].plot(title='High_Low',label='High_Low', ax=axes[1])\naxes[0].set_ylabel('Price')\naxes[1].set_ylabel('Price')\n\n\n\n\naxes[0].legend(loc='lower left')\naxes[1].legend(loc='lower left')\n","8567c81b":"test = df_final\ntest['SMA'] = SMA_GLD\ntest['Upper_band'] = upper_band\ntest['Lower_band'] = lower_band\ntest['DIF'] = DIF\ntest['MACD'] = MACD\ntest['RSI'] = RSI\ntest['STDEV'] = STDEV\ntest['Open_Close']=Open_Close\ntest['High_Low']=High_Low\n\n\n# Dropping first 33 records from the data as it has null values because of introduction of technical indicators\ntest = test[33:]\n\n# Target column\ntarget_adj_close = pd.DataFrame(test['Adj Close'])\n\n\ndisplay(test.head())","d4a42ada":"df_final.columns","98b2a51a":"# selecting Feature Columns\nfeature_columns = ['Open', 'High', 'Low', 'Volume','SP_open','SP_high','SP_low','SP_Ajclose','SP_volume','DJ_open','DJ_high', 'DJ_low',  'DJ_Ajclose', 'DJ_volume', 'EG_open','EG_high', 'EG_low',  \n                   'EG_Ajclose', 'EG_volume', 'EU_Price','EU_open', 'EU_high', 'EU_low', 'EU_Trend', 'OF_Price','OF_Open','OF_High', 'OF_Low', 'OF_Volume', 'OF_Trend', 'OS_Price', 'OS_Open','OS_High', 'OS_Low', 'OS_Trend', 'SF_Price', 'SF_Open', 'SF_High',\n                   'SF_Low', 'SF_Volume', 'SF_Trend', 'USB_Price', 'USB_Open', 'USB_High','USB_Low', 'USB_Trend', 'PLT_Price', 'PLT_Open', 'PLT_High', 'PLT_Low',\n                    'PLT_Trend', 'PLD_Price', 'PLD_Open', 'PLD_High', 'PLD_Low','PLD_Trend', 'RHO_PRICE', 'USDI_Price', 'USDI_Open', 'USDI_High',\n                     'USDI_Low', 'USDI_Volume', 'USDI_Trend','GDX_Open', 'GDX_High',\n       'GDX_Low', 'GDX_Close', 'GDX_Adj Close', 'GDX_Volume', 'USO_Open',\n       'USO_High', 'USO_Low', 'USO_Close', 'USO_Adj Close', 'USO_Volume','SMA', 'Upper_band', 'Lower_band', 'DIF', 'MACD','RSI','STDEV','Open_Close', 'High_Low']\n\n","9f3cf00a":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nfeature_minmax_transform_data = scaler.fit_transform(test[feature_columns])\nfeature_minmax_transform = pd.DataFrame(columns=feature_columns, data=feature_minmax_transform_data, index=test.index)\nfeature_minmax_transform.head()","d285ef9c":"display(feature_minmax_transform.head())\nprint('Shape of features : ', feature_minmax_transform.shape)\nprint('Shape of target : ', target_adj_close.shape)\n\n# Shift target array because we want to predict the n + 1 day value\n\n\ntarget_adj_close = target_adj_close.shift(-1)\nvalidation_y = target_adj_close[-90:-1]\ntarget_adj_close = target_adj_close[:-90]\n\n# Taking last 90 rows of data to be validation set\nvalidation_X = feature_minmax_transform[-90:-1]\nfeature_minmax_transform = feature_minmax_transform[:-90]\ndisplay(validation_X.tail())\ndisplay(validation_y.tail())\n\nprint(\"\\n -----After process------ \\n\")\nprint('Shape of features : ', feature_minmax_transform.shape)\nprint('Shape of target : ', target_adj_close.shape)\ndisplay(target_adj_close.tail())","af730393":"ts_split= TimeSeriesSplit(n_splits=10)\nfor train_index, test_index in ts_split.split(feature_minmax_transform):\n        X_train, X_test = feature_minmax_transform[:len(train_index)], feature_minmax_transform[len(train_index): (len(train_index)+len(test_index))]\n        y_train, y_test = target_adj_close[:len(train_index)].values.ravel(), target_adj_close[len(train_index): (len(train_index)+len(test_index))].values.ravel()\n       ","44fd4af7":"X_train.shape","616b0f52":"X_test.shape","b69f6304":"y_train.shape","9c2fa8ed":"y_test.shape","8e9ed993":"def validate_result(model, model_name):\n    predicted = model.predict(validation_X)\n    RSME_score = np.sqrt(mean_squared_error(validation_y, predicted))\n    print('RMSE: ', RSME_score)\n    \n    R2_score = r2_score(validation_y, predicted)\n    print('R2 score: ', R2_score)\n\n    plt.plot(validation_y.index, predicted,'r', label='Predict')\n    plt.plot(validation_y.index, validation_y,'b', label='Actual')\n    plt.ylabel('Price')\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n    plt.title(model_name + ' Predict vs Actual')\n    plt.legend(loc='upper right')\n    plt.show()","4d391e9d":"from sklearn.tree import DecisionTreeRegressor\n\ndt = DecisionTreeRegressor(random_state=0)\n\nbenchmark_dt=dt.fit(X_train, y_train)\n\nvalidate_result(benchmark_dt, 'Decision Tree Regression')","4a70ae37":"# Save all soultion models\nsolution_models = {}\n# SVR with  linear Kernel\nsvr_lin = SVR(kernel='linear')\nlinear_svr_clf_feat = svr_lin.fit(X_train,y_train)\nvalidate_result(linear_svr_clf_feat,'Linear SVR All Feat')\n","0609e3c7":"linear_svr_parameters = {\n    'C':[0.5, 1.0, 10.0, 50.0],\n    'epsilon':[0, 0.1, 0.5, 0.7, 0.9],\n}\n\nlsvr_grid_search_feat = GridSearchCV(estimator=linear_svr_clf_feat,\n                           param_grid=linear_svr_parameters,\n                           cv=ts_split,\n)\n\nlsvr_grid_search_feat.fit(X_train, y_train)\n\nvalidate_result(lsvr_grid_search_feat,'Linear SVR GS All Feat')\n","8b9092c2":"\nsolution_models['SVR All Feat'] = lsvr_grid_search_feat","68a0f228":"rf_cl = RandomForestRegressor(n_estimators=50, random_state=0)\nrandom_forest_clf_feat = rf_cl.fit(X_train,y_train)\nvalidate_result(random_forest_clf_feat,'Random Forest with All feat')","b019c56e":"random_forest_parameters = {\n    'n_estimators':[10,15,20, 50, 100],\n    'max_features':['auto','sqrt','log2'],\n    'max_depth':[2, 3, 5, 7,10],\n}\n\ngrid_search_RF_feat = GridSearchCV(estimator=random_forest_clf_feat,\n                           param_grid=random_forest_parameters,\n                           cv=ts_split,\n)\n\ngrid_search_RF_feat.fit(X_train, y_train)\n","a4e7dba1":"print(grid_search_RF_feat.best_params_)\nvalidate_result(grid_search_RF_feat,'RandomForest GS')","e8ad8187":"solution_models['Random_Forest with Feat'] = random_forest_clf_feat","74bbba4b":"from sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\n\nlasso_clf = LassoCV(n_alphas=1000, max_iter=3000, random_state=0)\nridge_clf = RidgeCV(gcv_mode='auto')\n\nlasso_clf_feat = lasso_clf.fit(X_train,y_train)\nvalidate_result(lasso_clf_feat,'LassoCV')\nsolution_models['LassoCV All feat'] = lasso_clf_feat\n\nridge_clf_feat = ridge_clf.fit(X_train,y_train)\nvalidate_result(ridge_clf_feat,'RidgeCV')\nsolution_models['RidgeCV All Feat'] = ridge_clf_feat","1d1a763a":"from sklearn import linear_model\nbay = linear_model.BayesianRidge()\nbay_feat = bay.fit(X_train,y_train)\nvalidate_result(bay_feat,'Bayesian')\nsolution_models['Bay All Feat'] = bay_feat","78329489":"from sklearn.ensemble import GradientBoostingRegressor\nregr =GradientBoostingRegressor(n_estimators=70, learning_rate=0.1,max_depth=4, random_state=0, loss='ls')\nGB_feat = regr.fit(X_train,y_train)\nvalidate_result(GB_feat,'NB')\nsolution_models['GB All Feat'] = GB_feat","c9614434":"from sklearn.linear_model import SGDRegressor\nsgd =SGDRegressor(max_iter=1000, tol=1e-3,loss='squared_epsilon_insensitive',penalty='l1',alpha=0.1)\nsgd_feat = sgd.fit(X_train,y_train)\nvalidate_result(sgd_feat,'SGD')\nsolution_models['SGD All Feat'] = sgd_feat","d9d4b549":"RMSE_scores = {}\ndef model_review(models):\n    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(16, 16))\n\n    #plot benchmark model\n    benchmark_predicted = benchmark_dt.predict(validation_X)\n    benchmark_RSME_score = np.sqrt(mean_squared_error(validation_y, benchmark_predicted))\n    RMSE_scores['Benchmark'] = benchmark_RSME_score\n    \n    axes[0,0].plot(validation_y.index, benchmark_predicted,'r', label='Predict')\n    axes[0,0].plot(validation_y.index, validation_y,'b', label='Actual')\n    axes[0,0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n    axes[0,0].xaxis.set_major_locator(mdates.MonthLocator())\n    axes[0,0].set_ylabel('Price')\n    axes[0,0].set_title(\"Benchmark Predict's RMSE Error: \" +\"{0:.2f}\".format(benchmark_RSME_score))\n    axes[0,0].legend(loc='upper right')\n    \n    #plot block\n    ax_x = 0\n    ax_y = 1\n    #plot solution model\n    for name, model in models.items():\n        predicted = model.predict(validation_X)\n        RSME_score = np.sqrt(mean_squared_error(validation_y, predicted))\n\n           \n        axes[ax_x][ax_y].plot(validation_y.index, predicted,'r', label='Predict')\n        axes[ax_x][ax_y].plot(validation_y.index, validation_y,'b', label='Actual')\n        axes[ax_x][ax_y].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n        axes[ax_x][ax_y].xaxis.set_major_locator(mdates.MonthLocator())\n        axes[ax_x][ax_y].set_ylabel('Price')\n        axes[ax_x][ax_y].set_title(name + \"'s RMSE Error: \" +\"{0:.2f}\".format(RSME_score))\n        axes[ax_x][ax_y].legend(loc='upper right')\n        RMSE_scores[name] = RSME_score\n        if ax_x <=2:\n            if ax_y < 2:\n                ax_y += 1\n            else:\n                ax_x += 1\n                ax_y = 0\n    plt.show()\n\nmodel_review(solution_models)","f837c3d3":"\nmodel_names = []\nmodel_values = []\nfor name, value in RMSE_scores.items():\n    model_names.append(name)\n    model_values.append(value)\n\nmodel_values = np.array(model_values)\nmodel_names = np.array(model_names)\n\nindices = np.argsort(model_values)\ncolumns = model_names[indices[:8]]\nvalues = model_values[indices][:8]\n\nfig = plt.figure(figsize = (16,8))\nplt.bar(np.arange(8), values ,width = 0.6, align=\"center\", color = '#ff00c1')\nplt.xticks(np.arange(8), columns)\nplt.xlabel('Model')\nplt.ylabel('RMSE')\nplt.title('RMSE compare')   \nplt.show()","c971d99e":"from sklearn.feature_selection import SelectFromModel\n\nsfm = SelectFromModel(lasso_clf_feat)\nsfm.fit(feature_minmax_transform, target_adj_close.values.ravel())\ndisplay(feature_minmax_transform.head())\nsup = sfm.get_support()\nzipped = zip(feature_minmax_transform,sup)\nprint(*zipped)","7c513751":"# Selecting Features which supports Model building process\n\nfeature_selected = feature_minmax_transform[['Open','High','Low','OF_Trend','USB_Trend','PLT_Trend','USDI_Price','GDX_Close','SMA','Upper_band','RSI','Open_Close']]\nfeature_selected_validation_X = validation_X[['Open','High','Low','OF_Trend','USB_Trend','PLT_Trend','USDI_Price','GDX_Close','SMA','Upper_band','RSI','Open_Close']]\ndisplay(feature_selected.head())\ndisplay(feature_selected_validation_X.head())","1bc12d16":" for train_index, test_index in ts_split.split(feature_selected):\n        X_train, X_test = feature_selected[:len(train_index)], feature_selected[len(train_index): (len(train_index)+len(test_index))]\n        y_train, y_test = target_adj_close[:len(train_index)].values.ravel(), target_adj_close[len(train_index): (len(train_index)+len(test_index))].values.ravel()\n     ","08f0717a":"def feature_selected_validate_result(model, model_name):\n    predicted = model.predict(feature_selected_validation_X)\n    \n    RSME_score = np.sqrt(mean_squared_error(validation_y, predicted))\n    R2_score = r2_score(validation_y, predicted)\n    \n    print(model_name + '\\n')\n    print('RMSE: ', RSME_score)\n    print('R2 score: ', R2_score)\n    print('----------------------')\n\n\nprint('---------Benchmark-------------')  \ndt = DecisionTreeRegressor(random_state=0)\nbenchmark_dt_fs = dt.fit(X_train,y_train)\nfeature_selected_validate_result(benchmark_dt_fs, 'Decision Tree')\n\nfeature_selected_solution_models = {}\n \nprint('--------Solution Models--------------')  \n# Random Forest\nrandom_forest_clf_fs = RandomForestRegressor(random_state=0,\n                                             max_depth=3,\n                                             max_features='auto',\n                                             n_estimators=10\n)\nrandom_forest_parameters = {\n    'n_estimators':[10, 50, 100],\n    'max_features':['auto','sqrt','log2'],\n    'max_depth':[3, 5, 7],\n}\ngrid_search_RF_fs = GridSearchCV(estimator=random_forest_clf_fs,\n                           param_grid=random_forest_parameters,\n                           cv=ts_split,\n)\ngrid_search_RF_fs.fit(X_train, y_train)\nfeature_selected_validate_result(grid_search_RF_fs,'Feature selected RandomForest GS')\nfeature_selected_solution_models['FS_RandomForest'] = grid_search_RF_fs\n\n# Linear SVR\nlinear_svr_fs = SVR(\n                          C=50.0,\n                          epsilon=0,kernel='linear')\nlinear_svr_clf_fs = linear_svr_fs.fit(X_train,y_train)\nfeature_selected_validate_result(linear_svr_clf_fs,'Feature selected LSVR')\nfeature_selected_solution_models['FS_LSVR'] = linear_svr_clf_fs\n\n\n# lasso\nlasso_fs = LassoCV(n_alphas=1000, max_iter=3000, random_state=0)\nlasso_clf_fs = lasso_fs.fit(X_train,y_train)\nfeature_selected_validate_result(lasso_clf_fs,'Feature selected LassoCV')\nfeature_selected_solution_models['FS_Lasso'] = lasso_clf_fs\n\n# Ridge\nridge_fs = RidgeCV(gcv_mode='auto')\nridge_clf_fs = ridge_fs.fit(X_train,y_train)\nfeature_selected_validate_result(ridge_clf_fs,'Feature selected RidgeCV')\nfeature_selected_solution_models['FS_RidgeCV'] = ridge_clf_fs\n\n# bayesian ridge\nbay = linear_model.BayesianRidge()\nbay_feat_fs = bay.fit(X_train,y_train)\nfeature_selected_validate_result(bay_feat_fs,'Feature selected BayRidge')\nfeature_selected_solution_models['Bay_Ridge'] = bay_feat_fs\n\n#Gradient Boosting\nregr =GradientBoostingRegressor(n_estimators=70, learning_rate=0.1,max_depth=4, random_state=0, loss='ls')\nGB_fs = regr.fit(X_train,y_train)\nfeature_selected_validate_result(GB_fs,'Feature selected GB')\nfeature_selected_solution_models['GB_FS'] = GB_fs\n\n#SGD\n\nsgd =SGDRegressor(max_iter=1000, tol=1e-3,loss='squared_epsilon_insensitive',penalty='l1',alpha=0.1)\nsgd_fs = sgd.fit(X_train,y_train)\nfeature_selected_validate_result(sgd_fs,'Feature selected SGD')\nfeature_selected_solution_models['sgd_fs'] = sgd_fs\n","83857f89":"FS_RMSE_scores = {}\n\ndef fs_model_review(models):\n    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(16, 16))\n\n    #plot benchmark model\n    benchmark_dt_predicted = benchmark_dt_fs.predict(feature_selected_validation_X)\n    benchmark_RSME_score = np.sqrt(mean_squared_error(validation_y, benchmark_dt_predicted))\n    FS_RMSE_scores['Benchmark'] = benchmark_RSME_score\n    \n    axes[0,0].plot(validation_y.index, benchmark_dt_predicted,'y', label='Predict')\n    axes[0,0].plot(validation_y.index, validation_y,'b', label='Actual')\n    axes[0,0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n    axes[0,0].xaxis.set_major_locator(mdates.MonthLocator())\n    axes[0,0].set_ylabel('Price')\n    axes[0,0].set_title(\"Benchmark Predict's RMSE Error: \" +\"{0:.2f}\".format(benchmark_RSME_score))\n    axes[0,0].legend(loc='upper right')\n    \n    #plot block\n    ax_x = 0\n    ax_y = 1\n    #plot solution model\n    for name, model in models.items():\n        predicted = model.predict(feature_selected_validation_X)\n        RSME_score = np.sqrt(mean_squared_error(validation_y, predicted))\n\n        R2_score = r2_score(validation_y, predicted)\n    \n        axes[ax_x][ax_y].plot(validation_y.index, predicted,'y', label='Predict')\n        axes[ax_x][ax_y].plot(validation_y.index, validation_y,'b', label='Actual')\n        axes[ax_x][ax_y].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n        axes[ax_x][ax_y].xaxis.set_major_locator(mdates.MonthLocator())\n        axes[ax_x][ax_y].set_ylabel('Price')\n        axes[ax_x][ax_y].set_title(name + \"'s RMSE Error: \" +\"{0:.2f}\".format(RSME_score))\n        axes[ax_x][ax_y].legend(loc='upper right')\n        FS_RMSE_scores[name] = RSME_score\n        if ax_x <=2:\n            if ax_y < 2:\n                ax_y += 1\n            else:\n                ax_x += 1\n                ax_y = 0\n    plt.show()\n\nfs_model_review(feature_selected_solution_models)","cad344b0":"\nfs_model_names = []\nfs_model_values = []\nfor name, value in FS_RMSE_scores.items():\n    fs_model_names.append(name)\n    fs_model_values.append(value)\n\nfs_model_values = np.array(fs_model_values)\nfs_model_names = np.array(fs_model_names)\n\nfs_indices = np.argsort(fs_model_values)\nfs_columns = fs_model_names[fs_indices[:8]]\nfs_values = fs_model_values[fs_indices][:8]\norigin_values = model_values[fs_indices][:8]\n\nfig = plt.figure(figsize = (16,8))\nplt.bar(np.arange(8) - 0.2 , origin_values ,width = 0.4, align=\"center\", color = '#b2b2ff', label = \"Original\")\nplt.bar(np.arange(8), fs_values ,width = 0.4, align=\"center\", color = '#3232ff', label = \"Feature Selected\")\nplt.xticks(np.arange(8), fs_columns)\nplt.xlabel('Model')\nplt.ylabel('RMSE')\nplt.title('RMSE compare after feature selection')\nplt.legend(loc = 'upper center')\nplt.show()\n","3241eed4":"# Choosing the top three performing models to ensemble them\nensemble_solution_models = [lasso_clf_feat, bay_feat, ridge_clf_feat]\nclass EnsembleSolution:\n    models = []\n    def __init__(self, models):\n        self.models = models\n    def fit(self, X, y):\n        for i in self.models:\n            i.fit(X, y)\n    def predict(self, X):\n        result = 0\n        for i in self.models:\n            result = result + i.predict(X)\n        \n        result = result \/ len(self.models)\n        \n        return result","ee766fa4":"print(\"Ensemble Solution Model with Original features\")\nEnsembleModel = EnsembleSolution(ensemble_solution_models)\nvalidate_result(EnsembleModel,'EnsembleSolution')","e3408026":"ensemble_solution_model_fs = [lasso_clf_fs,bay_feat_fs,linear_svr_clf_fs]\n\nprint(\"Ensemble Solution Model with Selected features\")\nEnsembleModel_fs = EnsembleSolution(ensemble_solution_model_fs)\nfeature_selected_validate_result(EnsembleModel_fs,'EnsembleSolution with FS')\n","76b1834f":"def train_reg_multipletimes(model, times):\n    total_rmse = 0\n    total_r2 = 0\n    for i in range(times):\n        reg = model\n        for train_index, test_index in TimeSeriesSplit(n_splits=i+2).split(feature_minmax_transform):\n            X_train, X_test = feature_minmax_transform[:len(train_index)], feature_minmax_transform[len(train_index): (len(train_index)+len(test_index))]\n            y_train, y_test = target_adj_close[:len(train_index)].values.ravel(), target_adj_close[len(train_index): (len(train_index)+len(test_index))].values.ravel()\n            reg.fit(X_train, y_train)\n        predicted = reg.predict(validation_X)\n        rmse, r2 = print_result(validation_y, predicted, [0,len(validation_y)])\n        total_rmse += rmse\n        total_r2 += r2\n    return total_rmse \/ times, total_r2 \/ times\n\ndef print_result(actual, predict, index):\n    RMSE_score = np.sqrt(mean_squared_error(actual, predict))\n    print('From {} to {}'.format(index[0],index[-1]))\n    print('RMSE: ', RMSE_score)\n    R2_score = r2_score(actual, predict)\n    print('R2 score: ', R2_score)\n    print('---------------------')\n    return RMSE_score, R2_score","239e033f":"print('Benchmark')\nt_multiple_benchmark_RMSE,t_multiple_benchmark_R2 = train_reg_multipletimes(benchmark_dt, 7)\nprint('RMSE: {} \/\/  R2: {}\\n'.format(t_multiple_benchmark_RMSE, t_multiple_benchmark_R2))\n","75943d52":"print('LSVR')\nt_multiple_LSVR_RMSE,t_multiple_LSVR_R2 = train_reg_multipletimes(linear_svr_clf_feat, 7)\nprint(' RMSE: {} \/\/  R2: {}'.format(t_multiple_LSVR_RMSE, t_multiple_LSVR_R2))","1d10c464":"print('Lasso')\nt_multiple_lasso_RMSE,t_multiple_lasso_R2 = train_reg_multipletimes(lasso_clf_feat, 7)\nprint(' RMSE: {} \/\/  R2: {}'.format(t_multiple_lasso_RMSE, t_multiple_lasso_R2))","81d1ce3b":"print('Ridge')\nt_multiple_ridge_RMSE,t_multiple_ridge_R2 = train_reg_multipletimes(ridge_clf_feat, 7)\nprint(' RMSE: {} \/\/  R2: {}'.format(t_multiple_ridge_RMSE, t_multiple_ridge_R2))","7d5b55bc":"print('BayRidge')\nt_multiple_bayridge_RMSE,t_multiple_bayridge_R2 = train_reg_multipletimes(bay_feat, 7)\nprint(' RMSE: {} \/\/  R2: {}'.format(t_multiple_bayridge_RMSE, t_multiple_bayridge_R2))","b7061b4c":"print('Ensemble')\nt_multiple_ensemble_RMSE,t_multiple_ensemble_R2 = train_reg_multipletimes(EnsembleSolution(ensemble_solution_models), 7)\nprint(' RMSE: {} \/\/  R2: {}\\n'.format(t_multiple_ensemble_RMSE, t_multiple_ensemble_R2))","b33f3b31":"def cross_validate(model, ts_split):\n    clf = model\n    total_rmse = 0\n    total_r2 = 0\n    count = 0\n    for train_index, test_index in ts_split.split(validation_X):\n        X_test1, X_test2 = validation_X[:len(train_index)], validation_X[len(train_index): (len(train_index)+len(test_index))]\n        y_test1, y_test2 = validation_y[:len(train_index)].values.ravel(), validation_y[len(train_index): (len(train_index)+len(test_index))].values.ravel()\n        predicted_test1 = clf.predict(X_test1)\n        temp1_RMSE, temp1_R2 = print_result(y_test1, predicted_test1, train_index)\n\n        predicted_test2 = clf.predict(X_test2)\n        temp2_RMSE, temp2_R2 = print_result(y_test2, predicted_test2, test_index)\n        \n        total_rmse += temp1_RMSE + temp2_RMSE\n        total_r2 += temp1_R2 + temp2_R2\n        count += 2\n    return total_rmse \/ count, total_r2 \/ count\n\n","bc9c21a3":"timeseries_cv = TimeSeriesSplit(n_splits=10)\ntest_bench__RMSE, test_bench_R2 = cross_validate(benchmark_dt,timeseries_cv)","7d58b482":"test_lsvr__RMSE, test_lsvr_R2 = cross_validate(lsvr_grid_search_feat,timeseries_cv)","1c3d8ebe":"test_ridge__RMSE, test_ridge_R2 = cross_validate(ridge_clf_feat,timeseries_cv)","e97599f3":"test_lasso__RMSE, test_lasso_R2 = cross_validate(lasso_clf_feat,timeseries_cv)","48da06d5":"test_bay_RMSE, test_bay_R2 = cross_validate(bay_feat,timeseries_cv)","e0c8dfd2":"test_ensemble_RMSE, test_ensemble_R2 = cross_validate(EnsembleSolution(ensemble_solution_models),timeseries_cv)","a0befae1":"print('Benchmark RMSE: {} \/\/ Benchmark R2: {}'.format(test_bench__RMSE, test_bench_R2))\nprint('LSVR RMSE: {} \/\/ LSVR R2: {}'.format(test_lsvr__RMSE, test_lsvr_R2))\nprint('Lasso RMSE: {} \/\/ Lasso R2: {}'.format(test_lasso__RMSE, test_lasso_R2))\nprint('bayesian ridge RMSE: {} \/\/ Bayesian ridge R2: {}'.format(test_bay_RMSE, test_bay_R2))\n\nprint('Ensemble RMSE: {} \/\/ Ensemble R2: {}'.format(test_ensemble_RMSE, test_ensemble_R2))","311441ef":"## Statistical Measures (Mean, Standard deviation, Kurtosis)","9195c191":"### Plotting Correlation Matrix","1940c115":"## Feature Selection\n\nIn this step we will select supporting features using sklearn's **SelectFromModel** library using Lasso regressor as it has lowest RMSE.","ef2f7cfc":"As we have seen using gridsearch on SVR we get significant improvement in R2 score and RMSE also came down so we will save this as our first solution model ","02d3a9d7":"Ensemble solution with all features shows best result (with RMSE 0.699 and R2 score of 0.887) in comparison with other solution models.","08dbc53a":"## Solution Model : Stochastic Gradient Descent (SGD)","40f3de9b":"# Checking Missing Values","b87cd4b4":"**Kurtosis** is a statistical measure that is used to describe the distribution. Whereas skewness differentiates extreme values in one versus the other tail, kurtosis measures extreme values in either tail. Distributions with large kurtosis exhibit tail data exceeding the tails of the normal distribution (e.g., five or more standard deviations from the mean). Distributions with low kurtosis exhibit tail data that is generally less extreme than the tails of the normal distribution.\n\n\nFor investors, high kurtosis of the return distribution implies that the investor will experience occasional extreme returns (either positive or negative), more extreme than the usual + or - three standard deviations from the mean that is predicted by the normal distribution of returns. This phenomenon is known as **kurtosis risk**.\n\n**Positive Kurtosis**\nMore weights in the tail\n\n![Positive](pos.png)\n\n**Negative Kurtosis**\nIt has as much data in each tail as it does in the peak.\n\n![Negative](neg.png)","14dffda3":"### Hyperparameter Tuning\nIn this step I will tune two parameters of SVR C and epsilon to see if the model shows any improvement.","5175b6dc":"## Solution Model : Bayesian Ridge","e72a63f0":"## Computing Daily Returns of all Features","7f8ac05f":"## Plotting Technical Indicators","56a70db8":"## Gold Rates Prediction using Machine Learning Approach","62769102":"## Solution Model \n\n### Support Vector Regressor (SVR)","52b05fe3":"## Train Test Split\n\nIn this step we would perform Train test split using sklearn's Timeseries split","43e20d02":"## Model Review\nIn this step, we will review benchmark model and all the solution model based on evaluation metrics i.e, RMSE and R2 score","ff7061ac":"In this step I would segregate feature and target variables. I will not use Close feature of GLD ETF and will use Adjusted Close of Gold ETF as target variable ","7d225cdd":"## Solution Model : Lasso and Ridge","390fc4d0":"### Positively Correlated Variables","1f1df214":"### Comparison of RMSE of Benchmark and all Solution Models","e5835518":"I will also use following technical indicators which I feel help as a feature for prediction of Gold price\n\n**1. MACD :** The moving average convergence-divergence (MACD) is one of the most powerful and well-known indicators in technical analysis. The indicator is comprised of two exponential moving averages that help measure momentum in a security. The MACD is simply the difference between these two moving averages plotted against a centerline, where the centerline is the point at which the two moving averages are equal.\n\n**2. RSI :** The relative strength index (RSI) is another well known momentum indicators that\u2019s widely used in technical analysis. The indicator is commonly used to identify overbought and oversold conditions in a security with a range between 0 (oversold) and 100 (overbought).\n\n**3. Simple Moving Average (SMA) :** simply takes the sum of all of the past closing prices over a time period and divides the result by the total number of prices used in the calculation. For example, a 10-day simple moving average takes the last ten closing prices and divides them by ten.\n\n**4. Upper Band**\n\n**5. Lower Band**\n\n**6. DIFF**\n\n**7. Open-Close**\n\n**8. High-Low**","21d671da":"## Train Model Multiple Times\nBy the train_reg_multipletimes function.This function would train the model several times (I choosed 7 times ), and use different parameters on TimeSeriesSplit in each time, average the R2 and RMSE.\nI will apply this function on Benchmark model and on top performing solution models with all features which are Linear SVR, Lasso, Ridge and Bayesian ridge and compare the same.","ebe23125":"\n## Project Work Flow\n\nI would proceed the project as shown in the below mentioned steps.\n\n\n![Project Work Flow](https:\/\/i.ibb.co\/bbcVFwS\/pd.png)","b48a5522":"## Correlation Analysis","c534fe63":"So, we have 1718 records in the dataset and 80 columns including Adjusted Close which is our target variable.","ce525dcd":"Hello Kagglers this notebook is part of my Capstone Project for Udacity ML Engineer Nanodegree Program. You can check my [repository](https:\/\/github.com\/sid321axn\/Udacity-MLND-Capstone-Gold-Price-Prediction) for full documentation about the project.","22741a94":"## Model Review","78758551":"# Machine Learning Nanodegree","522c5478":"# Cross Validation","ea8eee4c":"### Negatively Correlated Variables","0a43144f":"# Ensemble Solution\n\nSo now we will ensemble top three performing models i.e, in case of all the features model Lasso,Bayesian ridge and Ridge are the best performing models so we will ensemble these three models while in case of feature selected models we will combine Lasso,Bayesian Ridge and Linear SVR and will compare all the feature ensemble models with feature selected ensemble models.","dfc41f1c":"### Comparison of RMSE of Feature selected models and Original Features model","4792f624":"## Solution Model : Gradient Boosting Regressor","cad49a60":"## Computing daily returns of stock indexes","5893aef1":"Historically, gold had been used as a form of currency in various parts of the world including USA. In present times, precious metals like gold are held with central banks of all countries to guarantee re-payment of foreign debts, and also to control inflation which results in reflecting the financial strength of the country. \n\nForecasting rise and fall in the daily gold rates, can help investors to decide when to buy (or sell) the commodity. \n\nWe in this project would forecast gold rates using the most comprehensive set of features and would apply various machine learning algorithms for forecasting and compare their results. We also identify the attributes that highly influence the gold rates.\n\n![Gold ETF](https:\/\/i.ibb.co\/S07Np5F\/Gold.png)\n\n\n![Gold ETF](https:\/\/i.ibb.co\/S07Np5F\/Gold.png)","05dc531e":"That's great ! we dont have any missing values in our dataset","22641d08":"## Normalizing the data\n\nIn this step I would perform feature scaling\/normalization of feature variables using sklearn's MinMaxScaler function.","db37208c":"**As we have seen from the above plot 3 Feature selected models performs better in RMSE error reduction and Feature selected Linear SVR is the best as it has RMSE of 0.716 in feature selected model and 0.741 with all features model. Also Lasso cv and Bayesian Ridge performs slightly better from original features model where as Ridge cv shows no improvement from features model.Where as four model performance degrades  after feature selection in which benchmark model has highest RMSE error and SGD model degrades most in comparison to others.**","a9fe759a":"Data for this study is collected from **November 18th 2011** to **January 1st 2019** from various sources. The data has **1718** rows in total and **80** columns in total. Data for attributes, such as Oil Price, Standard and Poor\u2019s (S&P) 500 index, Dow Jones Index US Bond rates (10 years), Euro USD exchange rates, prices of precious metals Silver and Platinum and other metals such as Palladium and Rhodium, prices of  US Dollar Index, Eldorado Gold Corporation and Gold Miners ETF were gathered. \n\n**Attributes:**\n\n**Features**\n\n- Gold ETF :- Date, Open, High, Low, Close and Volume.\n- S&P 500 Index :- 'SP_open', 'SP_high', 'SP_low', 'SP_close', 'SP_Ajclose', 'SP_volume' \n- Dow Jones Index :- 'DJ_open','DJ_high', 'DJ_low', 'DJ_close', 'DJ_Ajclose', 'DJ_volume' \n- Eldorado Gold Corporation (EGO) :- 'EG_open', 'EG_high', 'EG_low', 'EG_close', 'EG_Ajclose', 'EG_volume'\n- EURO - USD Exchange Rate :- 'EU_Price','EU_open', 'EU_high', 'EU_low', 'EU_Trend' \n- Brent Crude Oil Futures :- 'OF_Price', 'OF_Open', 'OF_High', 'OF_Low', 'OF_Volume', 'OF_Trend'\n- Crude Oil WTI USD :- 'OS_Price', 'OS_Open', 'OS_High', 'OS_Low', 'OS_Trend'\n- Silver Futures :- 'SF_Price', 'SF_Open', 'SF_High', 'SF_Low', 'SF_Volume', 'SF_Trend'\n- US Bond Rate (10 years) :- 'USB_Price', 'USB_Open', 'USB_High','USB_Low', 'USB_Trend' \n- Platinum Price :- 'PLT_Price', 'PLT_Open', 'PLT_High', 'PLT_Low','PLT_Trend'\n- Palladium Price :- 'PLD_Price', 'PLD_Open', 'PLD_High', 'PLD_Low','PLD_Trend' \n- Rhodium Prices :- 'RHO_PRICE' \n- US Dollar Index : 'USDI_Price', 'USDI_Open', 'USDI_High','USDI_Low', 'USDI_Volume', 'USDI_Trend' \n- Gold Miners ETF :- 'GDX_Open', 'GDX_High', 'GDX_Low', 'GDX_Close', 'GDX_Adj Close', 'GDX_Volume' \n- Oil ETF USO :- 'USO_Open','USO_High', 'USO_Low', 'USO_Close', 'USO_Adj Close', 'USO_Volume'\n\n**Target Variable**\n- Gold ETF :- Adjusted Close","b96b7143":"**hope you like this kernel. please upvote to show your support**","1dc45527":"## Scatterplot","2eba559d":"## Train Test Split","0e632ec7":"## Effect of Index prices on gold rates","10dd9622":"As we have seen, Random forest with default parameters performed better than tuned Random forest model.So, we will include random forest with default parameters as our second solution model.","14a29fcf":"## Technical Indicators","f6200284":"## Solution Model : Random Forest","a17ed6b8":"## Importing Libraries","314bb0e6":"Ensemble solution with feature selection has better solution (RMSE 0.711 and  R2score 0.884 ) but Lasso has best performance (RMSE - 0.709 and R2 score 0.884)","5cae7525":"## Hyper parameter Tuning\nIn this I will tune 3 parameters of Random forest which are n_estimators,max_features,max_depth","708a0580":"## About Data","faab6d15":"## validation Feature Selected Benchmark & Solution Model","13d9eaf9":"## Model Building\n\n### 1. Benchmark Model :\n       I will use Decision Tree Regressor with default parameter as my Benchmark model for the project."}}