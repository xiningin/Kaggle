{"cell_type":{"f8ef5f29":"code","9a90be25":"code","7e17827e":"code","190b7596":"code","ba5736c8":"code","d44d42e6":"code","a14c6197":"code","cf490ec5":"code","bdb1351e":"code","028f4db4":"code","8868176f":"code","f0e0caf0":"code","1c698027":"code","8752788f":"code","a9d63605":"code","70ca0c43":"code","f5255e69":"code","a30f261f":"code","7b1719d6":"code","39ee3fac":"code","67bf1472":"code","26a69461":"code","d4cdfe2b":"code","d4ffba86":"code","476fabbd":"code","98a89279":"code","331f7881":"code","233fee90":"code","d1fc259c":"code","d8a290e0":"code","198dda61":"code","bd8892ba":"code","ce4615a1":"code","46fb3384":"code","12c72b00":"code","0af19fb5":"code","e1dc8ada":"code","c74f9f27":"code","f6f3392d":"code","d2b2efe9":"code","21556110":"code","364e7cc0":"code","b881db08":"code","21328630":"code","b055415d":"code","5d492e5d":"code","c6643e1f":"code","2b54c8d4":"code","39779cdf":"code","60129b9b":"code","0c3d344d":"code","1dadb9c9":"code","f163b564":"code","b3b1aa3f":"code","5267262d":"code","2db6507e":"code","6e583dad":"code","6493df08":"code","f248114b":"code","e670fd06":"code","84f430a1":"code","de383a7b":"code","aaaa3ff6":"code","d447f04d":"code","d00f8660":"code","38037a7a":"code","4077a9d4":"markdown","3a67afa1":"markdown","8eae499a":"markdown","5b105d37":"markdown","1f58a274":"markdown","76c61039":"markdown","e90c802b":"markdown","5d67b5f1":"markdown","21c00899":"markdown","1def0ecc":"markdown","2c2d9f0c":"markdown","d7aa259c":"markdown","d886875f":"markdown","c88fa7b1":"markdown","1d09f77b":"markdown","817f47fe":"markdown","f9c4d8e3":"markdown","65008128":"markdown","5727b5cf":"markdown","adb6ae25":"markdown","f96c60b8":"markdown","49a524a7":"markdown","2ab2e180":"markdown","f59f8faf":"markdown","714b3592":"markdown","526a9cc3":"markdown","2a5ac6cb":"markdown","ffc41fe9":"markdown","2439bbb2":"markdown"},"source":{"f8ef5f29":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns; sns.set()\n# Plotly for interactive graphics \nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a90be25":"data=pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\ndf=data.copy().sample(10000)\ndf","7e17827e":"df.info()","190b7596":"pd.DataFrame({\"No. of unique values\": list(df.nunique())}, index=df.columns)","ba5736c8":"for i in df.select_dtypes(include='object'):   \n    print(i,'-->',df[i].unique())","d44d42e6":"y = len(df[df['RainToday'] == 'Yes'])\nn = len(df[df['RainToday'] == 'No'])\nprint(y,n)","a14c6197":"df.duplicated().sum()","cf490ec5":"df.describe().T ","bdb1351e":"print(df.shape)\nprint(df.ndim)\nprint(df.size)","028f4db4":"df.corr() #I can see from here which variables I can put into the model\n          #For example, the variables can be removed by looking at the order of importance.","8868176f":"plt.figure(figsize = (16,6)) \nsns.heatmap(df.corr(),robust=True,fmt='.1g',linewidths=1.3,linecolor = 'gold', annot=True,);","f0e0caf0":"#VISUALIZATION OF NAN  VALUES\nmsno.matrix(df)","1c698027":"#drop missing values in the RainToday and RainTomorrow\ndf.dropna(subset=['RainToday', 'RainTomorrow'],axis=0,inplace=True)","8752788f":"sns.countplot(x=\"RainToday\",data=df)","a9d63605":"def summary(df):\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0]) # .shape[0] yazilmaz ise unique olan degerlerin listelerini getirir.\n    Nulls = df.apply(lambda x: x.isnull().sum())\n\n    cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n    str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    display(str.sort_values(by='Nulls', ascending=False))\n    print('__________Data Types__________\\n')\n    print(str.Types.value_counts())\nsummary(df)","70ca0c43":"#label encoding for univariate variables\nfrom sklearn.preprocessing import LabelEncoder\n\n\nlbe = LabelEncoder()\ndf[\"RainToday_label\"] = lbe.fit_transform(df[\"RainToday\"])\ndf[\"RainTomorrow_label\"] = lbe.fit_transform(df[\"RainTomorrow\"])","f5255e69":"# one-hot encoding for variables with more than 2 categories\n\n#drop variables with so many countries for the sake of time and memory consumption\ndf.drop(['Location','WindDir9am','WindDir3pm'], axis=1, inplace=True) \n\ndf = pd.get_dummies(df, drop_first=True, columns = ['WindGustDir'], prefix = ['WindGustDir'])","a30f261f":"def summary(df):\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0]) # .shape[0] yazilmaz ise unique olan degerlerin listelerini getirir.\n    Nulls = df.apply(lambda x: x.isnull().sum())\n\n    cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n    str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    display(str.sort_values(by='Nulls', ascending=False))\n    print('__________Data Types__________\\n')\n    print(str.Types.value_counts())\nsummary(df)","7b1719d6":"# DecisionTreeRegressor\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.tree import DecisionTreeRegressor\n\n#drop unnecessary columns and date columns\ndf_imputation = df.drop(['Date','RainToday','RainTomorrow'], axis=1) \n\n#define variables to keep the index and the columns\nindex = df_imputation.index\ncolumns = df_imputation.columns\n\n#imputation steps\nimp_tree = IterativeImputer(random_state=0, estimator=DecisionTreeRegressor())\nimp_tree.fit(df_imputation)\ndf_imputed = imp_tree.transform(df_imputation)\n\n#transform imputed data in array format to dataframe\ndf_imputed_tree = pd.DataFrame(df_imputed, index=index, columns=columns)\n\ndf_imputed_tree.isnull().sum()","39ee3fac":"df_imputed_tree.info()","67bf1472":"df_imputed_tree.isnull().sum()","26a69461":"df2 = df_imputed_tree.copy()\nx_dat = df2.drop(['RainTomorrow_label'],axis=1)\ny = df2[\"RainTomorrow_label\"].values","d4cdfe2b":"summary(df2)","d4ffba86":"#If there is a outlier values, it must be done before coming here\nx=(x_dat-np.min(x_dat))\/(np.max(x_dat)-np.min(x_dat)).values","476fabbd":"from sklearn.model_selection import train_test_split,cross_val_score,ShuffleSplit,GridSearchCV\nfrom sklearn.metrics import accuracy_score,mean_squared_error,roc_curve,roc_auc_score,classification_report,r2_score,confusion_matrix\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42) ","98a89279":"from sklearn.linear_model import LogisticRegression\nlr_model=LogisticRegression() #default olanlar gelir.C var..\nlr_model.fit(x_train,y_train)","331f7881":"print(lr_model.intercept_)\nprint(lr_model.coef_)#Bu katsayilar denklemin katsayilari(ax+b.. gibi). mesela (-) olanlar ters yonde etkiliyor.","233fee90":"y_pred=lr_model.predict(x_test)","d1fc259c":"y_pred[0:10]#tahminlerin ilk 10 degerini gorduk","d8a290e0":"LR = accuracy_score(y_test,y_pred)\nLR","198dda61":"y_probs = lr_model.predict_proba(x_test)[:,1]\ny_probs","bd8892ba":"y_pred = [1 if i >0.70 else 0 for i in y_probs]\ny_pred[:10]","ce4615a1":"log_score = accuracy_score(y_test,y_pred)\nprint (\"log score=\",log_score)","46fb3384":"confusion_matrix(y_test,y_pred)","12c72b00":"lr_model = LogisticRegression(solver = \"liblinear\")\nlr_model= lr_model.fit(x_train,y_train)\nlr_model","0af19fb5":"accuracy_score(y_test, lr_model.predict(x_test))","e1dc8ada":"#Cross validation (10 katli ) yaparsak\nlr_finalscore=cross_val_score(lr_model, x_test, y_test, cv = 10).mean()\nlr_finalscore","c74f9f27":"from sklearn.naive_bayes import GaussianNB\nnb_model = GaussianNB()\nnb_model = nb_model.fit(x_train, y_train)\nnb_model","f6f3392d":"y_pred = nb_model.predict(x_test)","d2b2efe9":"NB = accuracy_score(y_test,y_pred)\nNB","21556110":"confusion_matrix(y_test,y_pred)","364e7cc0":"# 10 katli cross validation.\nnb_finalscore=cross_val_score(nb_model, x_test, y_test, cv = 10).mean()\nnb_finalscore","b881db08":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier()\nknn_model = knn_model.fit(x_train, y_train)\n?knn_model","21328630":"y_pred = knn_model.predict(x_test)","b055415d":"KNN = accuracy_score(y_test, y_pred)\nKNN","5d492e5d":"confusion_matrix(y_test,y_pred)","c6643e1f":"knn_params = {\"n_neighbors\": np.arange(1,50)}","2b54c8d4":"knn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv=10)\nknn_cv.fit(x_train, y_train)","39779cdf":"print(\"The best score:\" + str(knn_cv.best_score_))\nprint(\"The best parameters: \" + str(knn_cv.best_params_))","60129b9b":"knn = KNeighborsClassifier(3)  #we choose 3 neigboors. I think 1 is not good \nknn_tuned = knn.fit(x_train, y_train)","0c3d344d":"knn_finalscore=knn_tuned.score(x_test, y_test)\nknn_finalscore","1dadb9c9":"from sklearn.svm import SVC\n\nsvm_model = SVC().fit(x_train,y_train)#we choose default c:1,kernel:'rbf',dagree:3...\n#?svm_model","f163b564":"y_pred = svm_model.predict(x_test)","b3b1aa3f":"SVC = accuracy_score(y_test,y_pred)\nSVC","5267262d":"svc_params = {\"C\": np.arange(1,10)}\n\nsvc = SVC(kernel = \"rbf\")\n\nsvc_cv_model = GridSearchCV(svc,svc_params, \n                            cv = 10, \n                            n_jobs = -1,        \n                            verbose = 2 )\n\nsvc_cv_model.fit(x_train, y_train)","2db6507e":"print(\"The best parameters: \" + str(svc_cv_model.best_params_))","6e583dad":"svc_tuned1 = SVC(kernel = \"rbf\", C = 8).fit(x_train, y_train)\ny_pred = svc_tuned1.predict(x_test)\naccuracy_score(y_test, y_pred)","6493df08":"svc_params = {\"C\": np.arange(1,10)}\n\nsvc = SVC(kernel = \"linear\")\n\nsvc_cv_model = GridSearchCV(svc,svc_params, \n                            cv = 10, \n                            n_jobs = -1, \n                            verbose = 2 )\n\nsvc_cv_model.fit(x_train, y_train)","f248114b":"print(\"The best parameters: \" + str(svc_cv_model.best_params_))","e670fd06":"svc_tuned2 = SVC(kernel = \"linear\", C = 6).fit(x_train, y_train)\ny_pred = svc_tuned2.predict(x_test)\naccuracy_score(y_test, y_pred)","84f430a1":"svc_model = SVC(kernel = \"rbf\").fit(x_train, y_train)","de383a7b":"svc_params = {\"C\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}","aaaa3ff6":"svc = SVC()\nsvc_cv_model = GridSearchCV(svc, svc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\nsvc_cv_model.fit(x_train, y_train)","d447f04d":"print(\"The best parameters: \" + str(svc_cv_model.best_params_))","d00f8660":"svc_tuned3 = SVC(C = 5, gamma =50).fit(x_train, y_train)\ny_pred = svc_tuned3.predict(x_test)\nsvc_finalscore=accuracy_score(y_test, y_pred)\nsvc_finalscore","38037a7a":"indexx = [\"Log\",\"NB\",\"KNN\",\"SVC\"]\nregressions = [LR,NB,KNN,SVC]\n\nplt.figure(figsize=(8,6))\nsns.barplot(x=indexx,y=regressions)\n\nplt.xticks()\nplt.title('Model Comparision',color = 'orange',fontsize=20);","4077a9d4":"# 2) Modeling of Gaussian Naive Bayes","3a67afa1":"## Prediction of NB","8eae499a":"## DATA READING AND EXPLORING","5b105d37":"## Prediction of Logistic Regression","1f58a274":"## SOME OF VISUALIZATION","76c61039":"## We will use the following CLASSIFICATION METHODS for Prediction\n- Logistic regression\n- Naive Bayes\n- K-Nearest Neighbor (KNN)\n- Support Vector Mechanism (SVM) Get predictions using Machine Learning models and compare these scores.","e90c802b":"## PREDICTION WITH CLASSIFICATION METHODS\n### Preparation dependent and independent variables","5d67b5f1":"## Prediction of SVC","21c00899":"- For kernel:rbf , C and gamma","1def0ecc":"## Accuracy Test(for default) of Logistic regression","2c2d9f0c":"## Model Tuning of NB","d7aa259c":"## If you find this kernel helpful, Please UPVOTES.","d886875f":"## Accuracy score of SVC","c88fa7b1":"## Model tuning of Logistic regression","1d09f77b":"## Data contains;\n- Date \n- Location : Cities of Australia\n- MinTemp \n- MaxTemp\n- Rainfall\n- Evaporation\n- Sunshine\n- WindGustDir : Wind Directions (East:E, West:W, North:N, South:S etc.)\n- WindGustSpeed\n- WindDir9am : Wind Directions (East:E, West:W, North:N, South:S etc.)\n- WindDir3pm : Wind Directions (East:E, West:W, North:N, South:S etc.)\n- WindSpeed9am\n- WindSpeed3pm\n- Humidity9am\n- Humidity3pm\n- Pressure9am\n- Pressure3pm\n- Cloud9am\n- Cloud3pm\n- Temp9am\n- Temp3pm\n- RainToday : 'No' 'Yes'\n- RainTomorrow : 'No' 'Yes'","817f47fe":"# 1) Modeling of Logistic Regression","f9c4d8e3":"## Model Tuning of KNN","65008128":"## Problem Definition\n- Given weather parameters about Australia, can we predict whether or not they have raintomorrow?","5727b5cf":"## Accuracy score of NB","adb6ae25":"## Accuracy score of KNN","f96c60b8":"### Test-Train splitting","49a524a7":"## Prediction of KNN","2ab2e180":"### Normalization of variabales","f59f8faf":"## Model Tuning of SVC\n- For kernel : rbf","714b3592":"# 4) Modeling of SVC","526a9cc3":"# 3) Modeling of KNN","2a5ac6cb":"- For kernel : linear","ffc41fe9":"## Proba values - probability","2439bbb2":"## Multivariate imputation\n- In multivariate imputation, we use ML Algorithms and before that we need to encode the categorical variables. "}}