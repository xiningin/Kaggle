{"cell_type":{"c0b749cf":"code","117aa1ed":"code","e0b07583":"code","332d009c":"code","eef68658":"code","8b283643":"code","28f40e36":"code","8e3104f4":"code","2bd73608":"code","f51fe774":"code","b0045908":"code","e0a54919":"markdown","cdf2a8e7":"markdown","5750660a":"markdown","6d047fd9":"markdown","f288d9cb":"markdown","33577653":"markdown","02bfcc19":"markdown","12da2578":"markdown"},"source":{"c0b749cf":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt","117aa1ed":"dataframe = pd.read_csv(\"..\/input\/data.csv\")\npca = PCA()\nEncoder = LabelEncoder()\ndataframe.iloc[:,14] = Encoder.fit_transform(dataframe.iloc[:,14].astype(str))\nworkrate = dataframe['Work Rate'].str.get_dummies(sep='\/ ')\nposition = dataframe['Position'].str.get_dummies(sep='\/ ')","e0b07583":"dataframe1 = dataframe.iloc[:, [55,59,63,64,65,69,71,72,76,77]]\ndataframe3 = dataframe.iloc[:,[83,86,87,80,81,75]]\n#dataframe3 = dataframe.iloc[:,68:80]\ndataframe2 = dataframe.iloc[:,[2,17]]\ndataframe = pd.concat([dataframe2, dataframe1], axis =1)\n#dataframe = pd.concat([dataframe, workrate], axis =1)\ndataframe = pd.concat([dataframe, position], axis =1)\ndataframe = pd.concat([dataframe, dataframe3], axis =1)\nscaler = StandardScaler()\nscaler.fit(dataframe.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,39,40,41,42,43,44]])\ndataframe.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,39,40,41,42,43,44]] = scaler.transform(dataframe.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,39,40,41,42,43,44]])\ndataframe.fillna(0,inplace = True)\nX = dataframe.iloc[:,1:]\ndf1 = dataframe","332d009c":"pca = PCA(n_components=2)\npca.fit_transform(X)\nX = pca.transform(X)\nx = pd.DataFrame(X)\nexplained_var = pca.explained_variance_ratio_\nNames = dataframe.iloc[:,0]\nnames = pd.DataFrame(Names)\ndataframe = pd.concat([names, x], axis =1)\nrecommendations = NearestNeighbors(n_neighbors=6, algorithm='ball_tree').fit(X)\nplayer_indices = recommendations.kneighbors(X)[1]","eef68658":"explained_var\n","8b283643":"def get_index(x):\n    return dataframe[dataframe['Name']==x].index.tolist()[0]\n\ndef recommend_me(player):\n    print('Here are 5 players similar to', player, ':' '\\n')\n    index = get_index(player)\n    for i in player_indices[index][1:]:\n            print(dataframe.iloc[i]['Name'], '\\n')","28f40e36":"recommend_me(\"T. Courtois\")","8e3104f4":"recommend_me(\"L. Messi\")","2bd73608":"recommend_me(\"Isco\")","f51fe774":"recommend_me(\"M. de Ligt\")","b0045908":"#We can plot a graph between these two parameters, to show the divergence in these two dimensions:\nx = dataframe.iloc[:,1]\ny = dataframe.iloc[:,2]\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(x,y, color='blue', marker='.')\nax.set_xlim(-10, 10)\nax.set_ylim(-10, 10)\nplt.show()","e0a54919":"here, we're only importing the essential libraries. We will be reducing the dimensions Using PCA to reduce the dimensions and to avoid overfitting. This will also make the dataprocessing, computationally feasible, fast and yet very reliable.","cdf2a8e7":"We are done writing functions to integrate the name to the attributes. It's time for us to run the code and see if the recommendations, derived merely by 3 columns, actually make sense","5750660a":"We have used 4 players who play at 4 different positions, and we have also seen the recommendations. this model can still be improved when compared to human recommendations, but it still seems to have done a great job!!","6d047fd9":"Here, we are basically concerned choosing our parameters and making a final dataframe that consists the parameters that are crucial \nWe are using the following parameters:\n1. Skill Moves\n2. Finishing\n3. Dribbling\n4. Ball Control\n5. Acceleration\n6. Sprint Speed\n7. Shot Power\n8. Staina\n9. Strength\n10. Positioning\n11. Vision\n12. Position\n13. Goalkeeper Diving\n14. Goalkeeper Positioning\n15. Goalkeeper Reflexes\n16. Marking \n17. Standing Tackles\n18. Interceptions\n\nThe parameters chosen are based purely on intuition and trial and error. It is clear that the variables we have are too many to visualize. Also, it will be a lot more complex, computationally, to perform calculations on such myriad of parameters. Plus, we increased the number of parameters dramatically by using the get_dummies() method. the number is as high as 45. Thus, we will use dimensionality reduction with PCA, to make it computationally feasible.","f288d9cb":"here, we're only importing the essential libraries. We will be reducing the dimensions Using PCA to reduce the dimensions and to avoid overfitting. This will also make the dataprocessing, computationally feasible, fast and yet very reliable.","33577653":"The two axes in the above displpayed figure, are the eigen value based axes that describe most of the data.These axes are highly sensitive and play a crucial role in clustering the data.Infact, these are the only axes based on which the data is clustered","02bfcc19":"This code is inspired by the idea of recommendation systems suggested by darkmatter08. \nHowever, the model chosen is completely different and I have emphasized a lot on the idea of dimensionality reduction.","12da2578":"Very Interesting! the variable explained_var actually shows us 'how much of the data features can be conserved in two dimensions'. in this case, it is over 70! that is great because our 2 variable data shows about 72 percent of  what a 45 dimensional data would've shown us. Plus, we are reducing the computational complexity by a great extent and also greatly reducing the chance of overfitting."}}