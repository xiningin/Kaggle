{"cell_type":{"e2befea7":"code","3ea19ad3":"code","87c22a2c":"code","00bcad2b":"code","728a94af":"code","31a5138a":"code","1f976659":"code","587cdad3":"code","3a1cac60":"code","2c5b5163":"code","c18d2552":"code","a1652491":"code","aabaa518":"code","d101c605":"code","a0eef71c":"code","02baf7a9":"code","84ddf642":"code","9577e868":"code","7f5d1fce":"code","1a7b1944":"code","b6ce21ef":"code","385e4bb0":"code","e139097c":"markdown","58c449d7":"markdown","be8ef22a":"markdown","5f0aa885":"markdown","3e2b5b78":"markdown","6856cfed":"markdown","edfa2282":"markdown","5b847bb5":"markdown","111aa9c7":"markdown","a51eb328":"markdown","4c15ebae":"markdown","bed240f8":"markdown","d03ba1e0":"markdown","42dca527":"markdown","bc87d6f9":"markdown","69a50a5c":"markdown","d7bc6f58":"markdown","c22f14bc":"markdown","fabf6538":"markdown","90c71375":"markdown","18b0a46f":"markdown","b51486d0":"markdown","3faa1bd2":"markdown","f6abdc96":"markdown","58bc1573":"markdown","50a6370d":"markdown"},"source":{"e2befea7":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport datatable as dt\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import AdamW, lr_scheduler\nfrom tqdm.notebook import tqdm_notebook\nfrom tqdm import tqdm\nimport math\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader\nimport time\nimport gc\nfrom sklearn.metrics import roc_auc_score\nimport pickle\nimport os\nimport math","3ea19ad3":"def sigmoid(x):\n    return 1 \/ (1 + math.exp(-x))","87c22a2c":"def loss_fn(outputs, targets):\n    loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1,1))\n    return loss","00bcad2b":"def metrics(targets, outputs):\n    auc = roc_auc_score(targets, outputs)\n    return auc","728a94af":"class Config:\n    epochs = 10\n    scheduler = 'CosineAnnealingLR'\n    batch_size = 10240\n    early_stopping_epochs = 2\n    lr = 1e-5\n    weight_decay = 0.01","31a5138a":"class Dataset:\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y.values\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return {\n            'X' : torch.tensor(self.X[idx], dtype=torch.float),\n            'targets' : torch.tensor(self.y[idx], dtype=torch.float)\n        }","1f976659":"class PredDataset:\n    def __init__(self, X):\n        self.X = X\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return {\n            'X' : torch.tensor(self.X[idx], dtype=torch.float)\n        }","587cdad3":"class TPSModel(nn.Module):\n    def __init__(self, args):\n        super(TPSModel, self).__init__()\n        self.args = args\n        self.linear = nn.Linear(128, 128)\n        self.lazylinear = nn.LazyLinear(128)\n        self.silu = nn.SiLU()\n        self.dropout = nn.Dropout(0.5)\n        self.output = nn.Linear(128, 1)\n    \n    # The forward Function\n    def forward(self, x):\n        x = self.lazylinear(x)\n        x = self.silu(x)\n        x = self.linear(x)\n        x = self.dropout(x)\n        x = self.output(x)\n        \n        return x","3a1cac60":"def train_epoch(args, dataloader, model, optimizer, scheduler, epoch):\n    \n    model.train()\n    \n    epoch_loss = 0.0\n    running_loss = 0.0\n    dataset_size=0\n    running_auc=0\n    batch_size = args.batch_size\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        optimizer.zero_grad()\n        \n        X = data['X'].cuda()\n        targets = data['targets'].cuda()\n        outputs = model(X)\n        \n        loss = loss_fn(outputs.view(-1,1), targets)\n        loss.backward()\n        \n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n            \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss \/ dataset_size\n        auc = metrics(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n        running_auc += auc * batch_size\n        epoch_auc = running_auc \/ dataset_size\n        bar.set_postfix(Epoch=epoch, Stage='Training', Train_Loss=epoch_loss,\n                        AUC=epoch_auc)\n    gc.collect()\n    return epoch_loss","2c5b5163":"def validation(args, dataloader, model, epoch):\n    \n    model.eval()\n    \n    epoch_loss = 0.0\n    running_loss = 0.0\n    dataset_size=0\n    batch_size = args.batch_size\n    running_auc = 0\n    counter=0\n    with torch.no_grad():\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:\n\n            X = data['X'].cuda()\n            targets = data['targets'].cuda()\n            outputs = model(X)\n\n            loss = loss_fn(outputs.view(-1,1), targets)\n\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n            epoch_loss = running_loss \/ dataset_size\n            auc = metrics(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n            counter+=1\n            \n            running_auc += auc * batch_size\n            epoch_auc = running_auc \/ dataset_size\n            \n            bar.set_postfix(Epoch=epoch, AUC=epoch_auc, Train_Loss=epoch_loss, Stage='Validation')\n    gc.collect()\n    return epoch_loss, epoch_auc","c18d2552":"def predict(args, dataloader, model):\n    print('-'*20,'Predicting for Submission','-'*20)\n    model.eval()\n    all_outputs=[]\n\n    with torch.no_grad():\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:\n\n            X = data['X'].cuda()\n            outputs = model(X)\n            outputs = outputs.cpu().detach().numpy()\n            all_outputs.append(outputs)\n            bar.set_postfix(Stage='Prediction')\n    gc.collect()\n    return np.vstack(all_outputs)","a1652491":"def get_optimizer(args, params):\n    opt = AdamW(params, lr=args.lr, weight_decay=args.weight_decay)\n    return opt","aabaa518":"def get_scheduler(args, optimizer):\n    if args.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=500, \n                                                   eta_min=1e-6)\n    else:\n        schduler = None\n    return scheduler","d101c605":"pred_df = dt.fread('..\/input\/tabular-playground-series-nov-2021\/test.csv').to_pandas()","a0eef71c":"pred_df.head()","02baf7a9":"xpred = pred_df.drop(['id'], axis=1)","84ddf642":"def run(data, fold):\n    \n    if not os.path.isdir('standard_scaler'):\n        os.mkdir('standard_scaler')\n    if not os.path.isdir('models'):\n        os.mkdir('models')\n    \n    print('-'*50)\n    print(f'Fold : {fold}')\n    print('-'*50)\n    \n    args = Config()\n    start = time.time()\n    model = TPSModel(args)\n    model = model.cuda()\n    \n    optimizer = get_optimizer(args, model.parameters())\n    scheduler = get_scheduler(args, optimizer)\n    \n    train = data[data['kfold']!=fold]\n    valid = data[data['kfold']==fold]\n    \n    sc = StandardScaler()\n    \n    # We will be scaling down the inputs so that no feature is overlooked by another feature\n    xtrain = train.drop(['id', 'target', 'kfold'], axis=1)\n    ytrain = train['target']\n    xtrain = sc.fit_transform(xtrain)\n    \n    xtest = valid.drop(['id', 'target', 'kfold'], axis=1)\n    ytest = valid['target']\n    xtest = sc.transform(xtest)\n    \n    xpred_sc = sc.transform(xpred) \n    pred_dataset = PredDataset(xpred_sc)\n    pred_loader = DataLoader(pred_dataset, batch_size = 2*args.batch_size)\n    \n    with open(f'standard_scaler\/sc_fold_{fold}.pickle', 'wb') as handle:\n        pickle.dump(sc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n    # Creating the datasets\n    train_dataset = Dataset(xtrain, ytrain)\n    valid_dataset = Dataset(xtest, ytest)\n    \n    # Creating the DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n    valid_loader = DataLoader(valid_dataset, batch_size=2*args.batch_size)\n    \n    best_val_loss = np.inf\n    patience_counter = 0\n    best_auc = 0\n    \n    # Iterating through epochs\n    for epoch in range(args.epochs):\n        \n        # Trainign\n        train_loss = train_epoch(args, train_loader, model, optimizer, scheduler, epoch)\n        \n        # Validation\n        valid_loss, val_auc = validation(args, valid_loader, model, epoch)\n        \n        if val_auc >= best_auc:\n            patience_counter = 0\n            print(f\"Validation AUC improved from : ({best_auc} ---> {val_auc})\")\n            best_auc = val_auc\n\n            PATH = f\"models\/model_fold_{fold}.bin\"\n            torch.save(model.state_dict(), PATH)\n            print(f\"----------Model Saved----------\")\n        \n        \n        # Early Stopping to prevent overfitting\n        else:\n            patience_counter += 1\n            print(f'Early stopping counter {patience_counter} of {args.early_stopping_epochs}')\n            if patience_counter == args.early_stopping_epochs:\n                print('*************** Early Stopping ***************')\n                break\n    \n    \n    end = time.time()\n    time_elapsed = end-start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best AUC: {:.4f}\".format(best_auc))\n    \n    \n    # Prediction\n    preds = predict(args, pred_loader, model)\n    \n    del model, train_loader, valid_loader\n    gc.collect()\n    \n    return preds","9577e868":"df = dt.fread('..\/input\/fold-is-power\/5fold.csv').to_pandas()","7f5d1fce":"df.head()","1a7b1944":"df['target'] = pd.get_dummies(df['target'].values, drop_first=True)","b6ce21ef":"all_preds = 0\nfor fold in range(5):\n    model_preds = run(df, fold=fold)\n    all_preds = all_preds + model_preds\/5","385e4bb0":"final_preds = [sigmoid(x) for x in np.hstack(all_preds)]\npred_df['target'] = final_preds\npred_df[['id', 'target']].to_csv('submission.csv', index=False)","e139097c":"# Creating the training Epoch\n\nIn this step we give the model some inputs and get some outputs and return the loss for that epoch :)\n\nNothing else.\n\n\n\n\nLater we use this loss and backpropagate it into the NN and accordingly step the optimizer and the scheduler :)\n\nEasy right?","58c449d7":"# Create your Model","be8ef22a":"### Converting the targets from True-False to 1-0","5f0aa885":"# Loss Function : Binary Cross Entropy Loss","3e2b5b78":"# Optimizer","6856cfed":"# Run Training, Validation and Prediction","edfa2282":"# Reading the DF","5b847bb5":"### The model creation is simple...\nJust follow these 3 steps:\n1. Create the **Model** class and inherit from **nn.Module**\n2. Create the __init__ : define the layers you want to use\n3. Create the **forward** function for the forward propagation of the NN and return the output\n\n### Another interesting thing you can see:\n\nI have used **nn.LazyLinear** instead of **nn.Linear** as the input layer.\n\nThe reason: I am too lazy :)\n\n### So what does a LazyLinear layer do?\n\nEasy. Yeah, it makes our life so easy. **No need** to wander around calculating the number of input features for the model. O.o\n\nLazyLinear layer just takes in input only the number of **out_features**. :)\n\nDone. Now if you want to change the number of input features, excluding some columns... No need to worry :D","111aa9c7":"# Please DO UPVOTE if you like :)","a51eb328":"# Prediction Loop :)\nPredict the Output and returns it :D","4c15ebae":"# Prediction Dataset","bed240f8":"# Lets start building a simple NN","d03ba1e0":"# The sigmoid function for the outputs","42dca527":"We Will be building a simple NN from scratch so that later we can experiment it likewise :)\n\nLet's get started","bc87d6f9":"### There are different types of Layers in a Neural Network\n\nThe most important and widely used are:\n\n* Linear layers :\n* Convolution layers :\n* Pooling Layers :\n* Dropout layers :\n\nWe will be using Linear layers and Dropout layers to build a simple shallow neural network\n\n### The different Activation functions\n\nThe activation functions we use depends on the task we perform:\n\n__Non-linear activation functions__:\n* nn.ReLU :\n* nn.Sigmoid :\n* nn.Tanh :\n\n__Linear activation fucntions are__:\n* nn.Sotfmax\n* nn.Softmin\n* nn.LogSoftmax\n\n### The different loss functions are:\n\nThe most widely used loss functions are:\n* nn.MSELoss : Mean Squared Error Loss function\n* nn.CrossEntropyLoss : This criterion computes the cross entropy loss between input and target.\n* nn.BCELoss : Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities\n* nn.BCEWithLogitsLoss : This loss combines a Sigmoid layer and the BCELoss in one single class.\n\nfor more information you can check this [link](https:\/\/pytorch.org\/docs\/stable\/nn.html#non-linear-activations-weighted-sum-nonlinearity).\n","69a50a5c":"# Importing dependencies","d7bc6f58":"# Main Training Loop\n\nIn this loop, for each fold the training is done. The Training epoch return the loss. It is then backpropagated in the NN. The validation loss and the metrics are also calculated. Finally the model is also saved. \n\nAnd the prediction is also done for the test set","c22f14bc":"# Please DO UPVOTE if you like","fabf6538":"# Let's talk about Layers and activations and losses","90c71375":"# Processing the test DF","18b0a46f":"To learn more about Datasets and DataLoaders see this notebook : []()","b51486d0":"# Train and Validation Dataset","3faa1bd2":"# AUC metric to measure model performance","f6abdc96":"# Validation Epoch\n\nWe just calculate the loss and return it :)","58bc1573":"# Config","50a6370d":"# Scheduler"}}