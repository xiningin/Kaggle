{"cell_type":{"583e0e3a":"code","0d6f71e8":"code","9f8c1265":"code","83eb8b74":"code","f5bae32c":"code","500b5a0e":"code","3a8b8955":"code","2b7ec102":"code","08fbfc6e":"code","84dcd697":"code","135a8c87":"code","cb877218":"code","077c5b0c":"code","68c9f648":"code","5c2f44dd":"code","208f5905":"code","1e039327":"code","7629d40f":"markdown","ca2a672f":"markdown","869d2748":"markdown","4ea31c68":"markdown","cb2eb507":"markdown","ad8a78e1":"markdown","3258d05f":"markdown","8b04f42b":"markdown","826da591":"markdown","3f571e2f":"markdown","df395da0":"markdown"},"source":{"583e0e3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings  \nwarnings.filterwarnings(\"ignore\")   # ignore warnings\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0d6f71e8":"data = pd.read_csv('..\/input\/Mall_Customers.csv')","9f8c1265":"data.info()","83eb8b74":"data.head()","f5bae32c":"X = data.iloc[:, 3:].values\nX","500b5a0e":"# Loading Library\nfrom sklearn.cluster import KMeans","3a8b8955":"# To decide variable K, we use WCSS.\nresult = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters = i, init='k-means++', random_state = 123)\n    kmeans.fit(X)\n    result.append(kmeans.inertia_)","2b7ec102":"result","08fbfc6e":"plt.plot(range(1,11), result)\nplt.xlabel('the number of clusters')\nplt.ylabel('result')\nplt.title('The Elbow Method')\nplt.show()\n# From below figure, we can choose K as 3 because there is a decreasing of acceleration at that point. ","84dcd697":"# Applying K-Means Algorithm\n# Creation of model\nkmeans = KMeans(n_clusters = 3, init='k-means++')\nkmeans.fit(X)","135a8c87":"# We learn center point of each cluster with below function.\n# For example, first column is Annual Income (k$) whose center point is 44.15447154 for first cluster.\nkmeans.cluster_centers_","cb877218":"# Prediction\ny_kmeans = kmeans.fit_predict(X)\ny_kmeans","077c5b0c":"# Visualising the clusters\nplt.scatter(X[y_kmeans == 0,0], X[y_kmeans == 0,1], s=100, c='red')\nplt.scatter(X[y_kmeans == 1,0], X[y_kmeans == 1,1], s=100, c='blue')\nplt.scatter(X[y_kmeans == 2,0], X[y_kmeans == 2,1], s=100, c='green')\nplt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=300, c='yellow')\nplt.title('K-Means Clustering')\nplt.show()","68c9f648":"# Creation of model & prediction\nfrom sklearn.cluster import AgglomerativeClustering\nac = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\nY_predict = ac.fit_predict(X)\nY_predict","5c2f44dd":"# Visualising the clusters\nplt.scatter(X[Y_predict==0,0], X[Y_predict==0,1], s=100, c='red')\nplt.scatter(X[Y_predict==1,0], X[Y_predict==1,1], s=100, c='blue')\nplt.scatter(X[Y_predict==2,0], X[Y_predict==2,1], s=100, c='green')\nplt.title('Hierarchical Clustering')\nplt.show()","208f5905":"import scipy.cluster.hierarchy as sch","1e039327":"dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\nplt.show()","7629d40f":"**CLUSTERING ALGORITHMS**","ca2a672f":"My other kernels are here:\n\nhttps:\/\/www.kaggle.com\/armagansarikey\/machine-learning-1-data-preprocessing\n\nhttps:\/\/www.kaggle.com\/armagansarikey\/machine-learning-2-prediction-algorithms\n\nhttps:\/\/www.kaggle.com\/armagansarikey\/machine-learning-3-classification-algorithms\n\nIf you have any question or suggest, I will be happy to hear it.","869d2748":"**2. Hierarchical Clustering**","4ea31c68":"**Dendrogram**","cb2eb507":"**1. K - Means**","ad8a78e1":"* K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). \n* The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. \n* The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. \n* Data points are clustered based on feature similarity.","3258d05f":"* Clustering is the technique of dividing the data points into a number of groups such that data points in the same groups are more similar than the others.\n* Clustering is an unsupervised learning method. ","8b04f42b":"**WCSS (Within-Cluster Sums of Squares)**\n* Let\u2019s take there are 3 clusters. That means, we have 3 center points (C1, C2, C3). Each data point falls into the zone of either C1 or C2 or C3. \n* First we calculate the sum of squares of the distance of each data point in cluster 1 from their center point C1. \n* This is cluster 1 sum of squares.\n[dist(C1, c1p1) ]\u00b2 + [dist(C1, c1p2)]\u00b2 + [dist(C1, c1p3)]\u00b2. \n* Similarly we do the same for C2 & C3. \n* We add the sum of all 3 clusters sum of squares to get **WCSS**.\n* WCSS always decreases with the increase in the number of clusters.","826da591":"**In this kernel, 2 different clustering algorithms have been explained for machine learning.**","3f571e2f":"**CONCLUSION**","df395da0":"* Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. \n* This hierarchy of clusters is represented as a dendrogram. \n* Hierarchical clustering can be performed with distance matrix.\n\n**Agglomerative :** bottom up approach, each observation starts in its own cluster, and clusters are successively merged together.\n\n** Divisive :** top down approach."}}