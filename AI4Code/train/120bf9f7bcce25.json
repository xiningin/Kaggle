{"cell_type":{"cf616c04":"code","28d298ea":"code","272c7960":"code","83474831":"code","52c48e02":"code","a93ce0f6":"code","873812c0":"code","dc40f891":"code","3dbe7c0a":"code","d1acb601":"code","fe1da1fa":"code","c18b1669":"code","d5b4d7d7":"code","1efd9d03":"code","2f754454":"code","ec831aeb":"code","ac40b1d7":"code","ec6573f0":"code","c76d5783":"code","0226a19b":"code","261f860d":"code","b5be5845":"code","c69ed1f2":"code","a05d47a0":"code","00bca142":"code","7e0d9497":"code","7916df52":"code","854cad37":"code","355bc228":"code","9e492775":"code","a5a677c0":"code","89e55e1d":"code","8ec5739b":"code","9d4136dc":"code","d016b217":"code","a078db40":"code","0749fd43":"code","8cd21c51":"code","d7ad14af":"code","76f8837c":"code","64843a90":"code","cc1b86d0":"code","5893adbb":"code","96eadb38":"code","6d422872":"code","caf3eb3b":"code","a5cd2b50":"code","c7eaa7dc":"code","29d3a071":"code","74191854":"code","6cb67504":"code","0c79fea7":"code","9eeb4013":"code","26fdde09":"code","4264922b":"code","7dc57cdf":"code","7bda5770":"code","39dfe934":"code","9e93990b":"code","02670220":"code","a6601b38":"code","c9ff903b":"code","59363764":"code","07043d2c":"code","8ad73a6c":"code","30e8813a":"code","dba78b3c":"code","ce6cca79":"code","6937b0ca":"code","81748eeb":"code","8112e378":"markdown","59a4c610":"markdown","f6d046ee":"markdown","f4ae1881":"markdown","5b2383ba":"markdown","0e064778":"markdown","c1750fea":"markdown","ee38f2b9":"markdown","b2244e26":"markdown","8cdd80ea":"markdown","939abcf3":"markdown","cf3afc79":"markdown","b3d38201":"markdown","7f6b51ed":"markdown","51a6649a":"markdown","9c8da123":"markdown","cdebc3c1":"markdown","3d43d1c0":"markdown","c3b5a320":"markdown","b0e9af18":"markdown","c562afbe":"markdown","410a66bc":"markdown","6202a788":"markdown","624f30b8":"markdown","690c6733":"markdown","bcdffbe5":"markdown","8aaf2dcd":"markdown","16754940":"markdown","9ac28217":"markdown","ee5b6948":"markdown","2069e903":"markdown","8c79f429":"markdown","913e64e3":"markdown","bb23aafd":"markdown","60b3e804":"markdown","26616a87":"markdown","5486d623":"markdown","dcf415eb":"markdown","ccb7e739":"markdown","5a8d3348":"markdown","25958659":"markdown","77be4bf8":"markdown","996cf84c":"markdown","7dd372f3":"markdown","29c3032b":"markdown","71141bb3":"markdown","2fd40a1d":"markdown","dc8ca0e0":"markdown","b75ad13e":"markdown","59fced21":"markdown","df316b5f":"markdown","32f566a3":"markdown","bf1348b0":"markdown","66e193fe":"markdown","0ed7804b":"markdown","2d96a664":"markdown","50973feb":"markdown","707676d2":"markdown","3fb12c53":"markdown","b9d0230d":"markdown","1945e072":"markdown","7da576a3":"markdown","11837de0":"markdown","3e61a6fb":"markdown","892ab21a":"markdown","9a9ce4d5":"markdown","14bc3c9b":"markdown","db95dc4c":"markdown","fa85024e":"markdown","463ab1db":"markdown","f9409d55":"markdown","5119ec0b":"markdown","50f35a8d":"markdown"},"source":{"cf616c04":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","28d298ea":"df = pd.read_csv('..\/input\/LoanStats3a-2007-2011.csv', low_memory=False)","272c7960":"df = df.drop( ['url','mths_since_last_delinq', 'mths_since_last_record','next_pymnt_d','pymnt_plan', 'desc','initial_list_status', 'out_prncp',\n                         'out_prncp_inv','total_rec_int', 'total_rec_late_fee', 'recoveries','next_pymnt_d','earliest_cr_line','pub_rec',\n              'inq_last_6mths','open_acc','last_pymnt_d','last_credit_pull_d','issue_d','revol_bal','total_rec_prncp',\n              'collection_recovery_fee','issue_d','title','total_pymnt_inv','emp_title'], axis = 1)","83474831":"df.head()","52c48e02":"fig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(df.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","a93ce0f6":"sns.catplot(x='home_ownership', col='loan_status', kind='count', data=df);","873812c0":"sns.boxplot(x='loan_amnt', y='loan_status', data=df)\nplt.title(\"Loan Status distribution as function of Loan Amount\")\nplt.show()","dc40f891":"sns.catplot(x='emp_length', col='grade', kind='count', data=df);","3dbe7c0a":"sns.boxplot(x='loan_amnt', y='emp_length', data=df)\nplt.title(\"Loan Status distribution as function of Loan Amount\")\nplt.show()","d1acb601":"sns.catplot(data = df, x = \"emp_length\", y = \"loan_amnt\", hue = \"loan_status\")","fe1da1fa":"#Fill missing data with mode\ndef mode_fillna(dataset, column_name):\n    dataset[column_name].fillna( dataset[column_name].mode().iloc[0],inplace = True)\n    return dataset\n\n#Fill missing data in the emp_title column with the word Others\ndef emp_title_fillna(dataset):\n    dataset['emp_title'].fillna('Others',inplace = True)\n    return dataset\n\n#Fill missing data with 0 for the columns:\n#revol_util, last_pymnt_amnt, emp_length, total_credit_used, years_experience, delinq_2yrs,total_acc\n#We decided to fill with cero because if we fill with the mean or mode we can affect the analysis\ndef column_fillna(dataset, column_name):\n    dataset[column_name] = dataset[column_name].fillna(0)\n    return dataset;\n\n#import uuid \n#def generate_id(dataset):\n#    dataset['id'] = uuid.uuid1()\n#    return dataset\n\ndef drop_features(dataset,column_name):\n    return dataset.drop([column_name], axis = 1)\n\n#transform the column in months\ndef get_month_load(dataset):\n    dataset = dataset.rename(index=str, columns={\"term\": \"months_pay_loan\"})\n    new = dataset[\"months_pay_loan\"].str.split(\" \", n = 2, expand = True) \n    dataset[\"months_pay_loan\"]= new[1]   \n    return dataset\n\n#transform the column in number\ndef get_value(dataset, actual_column, new_column, separator):\n    dataset = dataset.rename(index=str, columns={actual_column: new_column})\n    new = dataset[new_column].str.split(separator, n = 1, expand = True) \n    dataset[new_column]= new[0] \n    return dataset\n\n#transform the column in number\ndef get_years_experience(dataset):\n    dataset = dataset.rename(index=str, columns={\"emp_length\": \"years_experience\"})\n    dataset['years_experience'] = dataset['years_experience'].str.replace('\\+ years','') \n    dataset['years_experience'] = dataset['years_experience'].str.replace('< 1 year','0')\n    dataset['years_experience'] = dataset['years_experience'].str.replace('years','')\n    dataset['years_experience'] = dataset['years_experience'].str.replace('year','')\n    return dataset\n\n#transform the column according to description in approved or not aproved\ndef get_loan_status(dataset):\n    dataset['loan_status'] = dataset['loan_status'].str.replace('Does not meet the credit policy. Status:Charged Off','not_approved') \n    dataset['loan_status'] = dataset['loan_status'].str.replace('Does not meet the credit policy. Status:Fully Paid','not_approved')\n    dataset['loan_status'] = dataset['loan_status'].str.replace('Charged Off','approved')\n    dataset['loan_status'] = dataset['loan_status'].str.replace('Fully Paid','approved')\n    return dataset\n\n# transform the column verified status\ndef get_verification_status(dataset):\n    dataset['verification_status'] = dataset['verification_status'].str.replace('Source Verified','Verified') \n    return dataset\n\n# validate nulls in all the columns\ndef validate(dataset):\n    cols = dataset.head()\n    for col in cols:\n        if dataset[col].isnull().values.any() : return col\n    return True\n#Rename columns\ndef rename(dataset, column_name, new_column_name):\n    dataset = dataset.rename(index=str, columns={column_name:new_column_name})\n    return dataset","c18b1669":"df = get_verification_status(df)\n#df = get_loan_status(df)\ndf = mode_fillna(df,'grade')\ndf = mode_fillna(df,'loan_status')\ndf = column_fillna(df, 'revol_util')\ndf = column_fillna(df, 'last_pymnt_amnt')\ndf = column_fillna(df, 'emp_length')\ndf = df.set_index('id')\ndf = get_month_load(df)\ndf = get_value(df,'int_rate','interest_rate_loan','%')\n\ndf = get_value(df,'revol_util','total_credit_used','%')\ndf = column_fillna(df, 'total_credit_used')  \ndf = get_years_experience(df)\ndf = column_fillna(df, 'years_experience')\ndf = column_fillna(df, 'annual_inc') \ndf = rename(df, 'annual_inc', 'annual_income')\ndf = column_fillna(df, 'delinq_2yrs') \ndf = rename(df, 'delinq_2yrs', 'delinquency_others_loan')\ndf = column_fillna(df, 'total_acc') \ndf = rename(df, 'total_acc', 'num_credit_lines')\ndf = rename(df, 'dti', 'amount_pay_month_another_obligations')\ndf = rename(df, 'installment', 'monthly_payment')","d5b4d7d7":"validate(df)","1efd9d03":"df.head()","2f754454":"df2 = df.loc[df['loan_status'] == 'Fully Paid'].sample(frac = .0582)\ndf3 = df.loc[df['loan_status'] == 'Charged Off'].sample(frac = .134)\ndf4 = df.loc[df['loan_status'] == 'Does not meet the credit policy. Status:Charged Off']\ndf5 = df.loc[df['loan_status'] == 'Does not meet the credit policy. Status:Fully Paid']\ndf = pd.concat([df2, df3,df4,df5])\ndf.head()\n","ec831aeb":"df = get_loan_status(df)","ac40b1d7":"#Get hot enconding for the categorical columns: grade, sub_grade, home_ownership, verification_status\ndef enmarked_hot_encode(df, column_name):\n    df_hot=pd.get_dummies(df[column_name])\n    df_hot = pd.DataFrame(df_hot)\n    df= df.join(df_hot)\n    df= df.drop([column_name], axis = 1)\n    return df \n\n#Get binary enconding for the categorical column loan_status\ndef binary_encode(df):\n    df['get_approved'] = df['loan_status']\n    df['get_approved'] = pd.get_dummies(df['get_approved'])\n   # df['get_approved'] = pd.get_dummies(df['loan_status'])\n    df= df.drop(['loan_status'], axis = 1)\n    return df\n\n#Get hot enconding for the categorical columns:purpose, zip_code, addr_state\nfrom sklearn.preprocessing import LabelEncoder\ndef hot_encode(df, column_name):\n    le_emp = LabelEncoder()\n    df[column_name] = le_emp.fit_transform(df[column_name])\n    return df\n\n# Normalize the data\nfrom sklearn.preprocessing import MinMaxScaler\ndef normalize_data(df):\n    cols_to_norm = ['loan_amnt','funded_amnt','funded_amnt_inv','annual_income','monthly_payment','zip_code','addr_state']\n    df[cols_to_norm] = MinMaxScaler().fit_transform(df[cols_to_norm])\n    return df","ec6573f0":"df = enmarked_hot_encode(df,'grade')\ndf = enmarked_hot_encode(df,'sub_grade')\ndf = enmarked_hot_encode(df,'home_ownership')\ndf = enmarked_hot_encode(df,'verification_status')\ndf = hot_encode(df, 'purpose')\ndf = hot_encode(df, 'zip_code')\ndf = hot_encode(df, 'addr_state')\ndf = binary_encode(df)\ndf = normalize_data(df)","c76d5783":"df.head()","0226a19b":"import matplotlib.pyplot as plt2\nplt2.hist(df.get_approved)","261f860d":"def get_split(df):\n    X = df.loc[:, 'loan_amnt':\"Verified\"]\n    from sklearn.model_selection import train_test_split \n    #def hot_encode(df):\n    label = LabelEncoder().fit_transform(df.get_approved)\n    X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.20, random_state=2)\n    return X_train, X_test, y_train, y_test","b5be5845":"X_train, X_test, y_train, y_test = get_split(df)","c69ed1f2":"def predicted_metrics(predicted):\n    print('======================================================================================')\n    print('======================================================================================')\n    print('')\n    print('* Predicted value: ')\n    print(predicted)\n    print('')\n    print('* Accuracy Score: ')\n    print(accuracy_score(y_test, predicted))\n    print('')\n    print('* Confusion matrix: ')\n    print (confusion_matrix(y_test, predicted))\n    print('')\n    print('* Classification Report: ')\n    print(classification_report(y_test, predicted))","a05d47a0":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef KNN_Model(n_neighbors, data_train, label, data_test, y_test):\n    # Create KNN classifier\n    model = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=2, n_neighbors=n_neighbors, p=8,\n           weights='uniform')\n    # Train the model using the training sets, Fit the classifier to the data\n    model.fit(data_train,label)\n    #Predict Output\n    predicted= model.predict(data_test) \n    predicted_metrics(predicted)\n    print(predicted)\n    return accuracy_score(y_test, predicted)","00bca142":"sc_knn1 = KNN_Model(1, X_train, y_train, X_test, y_test)\nsc_knn3 = KNN_Model(3, X_train, y_train, X_test, y_test)\nsc_knn5 = KNN_Model(5, X_train, y_train, X_test, y_test)\n\nsc_knn7 = KNN_Model(7, X_train, y_train, X_test, y_test)\nsc_knn9 = KNN_Model(14, X_train, y_train, X_test, y_test)\nsc_knn11 = KNN_Model(25,X_train, y_train, X_test, y_test)\n\n","7e0d9497":"from sklearn.model_selection import GridSearchCV\n\ndef get_best_score(model):\n    \n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\n    \n    return model.best_score_\n\n\nknn = KNeighborsClassifier()\nleaf_range = list(range(3, 15, 1))\nk_range = list(range(1, 15, 1))\nweight_options = ['uniform', 'distance']\nn_jobs_list = [2]\nparam_grid = dict(leaf_size=leaf_range, n_neighbors=k_range, weights=weight_options, n_jobs=n_jobs_list)\nprint(param_grid)\n\nknn_grid = GridSearchCV(knn, param_grid, cv=10, verbose=1, scoring='accuracy')\nknn_grid.fit(X_train, y_train)\n\nsc_knn = get_best_score(knn_grid)","7916df52":"list_scores = [sc_knn1, sc_knn3, sc_knn5, sc_knn7, sc_knn9, sc_knn11]\nlist_scores_label=['1', '3', '5', '7', '9', '11'] ","854cad37":"# allow plots to appear within the notebook\n%matplotlib inline\n\n# plot the relationship between K and testing accuracy\n# plt.plot(x_axis, y_axis)\nplt.plot(list_scores_label, list_scores, color='green', marker='o')\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Testing Accuracy')\nplt.show()","355bc228":"# Training \n# import the class\n\nfrom sklearn.linear_model import LogisticRegression\n\ndef LogisticRegression_Model(X_train, X_test, y_train):\n\n    # instantiate the model (using the default parameters)\n    # STEP 2: train the model on the training set\n    logreg = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n    logreg.fit(X_train, y_train)\n    \n    # STEP 3: make predictions on the testing set\n    pred_logreg = logreg.predict(X_test)\n\n    # compare actual response values (y_test) with predicted response values (y_pred)\n    return pred_logreg\n    #return accuracy_score(y_test, y_pred)","9e492775":"pred_logreg = LogisticRegression_Model(X_train, X_test, y_train)\naccuracy_logreg = accuracy_score(y_test, pred_logreg)\nprint(accuracy_logreg)","a5a677c0":"confusion_matrix_logreg = confusion_matrix(y_test, pred_logreg)\nprint (confusion_matrix_logreg)","89e55e1d":"classification_report_logreg = classification_report(y_test, pred_logreg)\nprint(classification_report_logreg)","8ec5739b":"# BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn import tree\ndef BaggingClassifier_Model(X_train, X_test, y_train):\n    model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n    model.fit(X_train, y_train)\n    #print(model.score(X_test,y_test))\n    prediction = model.predict(X_test)\n    return prediction","9d4136dc":"prediction_bagging = BaggingClassifier_Model(X_train, X_test, y_train)\naccuracy_score_bagging = accuracy_score(y_test, prediction_bagging)\nprint(accuracy_score_bagging)   ","d016b217":"confusion_matrix_bagging = confusion_matrix(y_test, prediction_bagging)\nprint (confusion_matrix_bagging)","a078db40":"classification_report_bagging = classification_report(y_test, prediction_bagging)\nprint(classification_report_bagging)","0749fd43":"# BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn import tree\ndef BaggingClassifier_Model(X_train, X_test, y_train):\n    model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n    model.fit(X_train, y_train)\n    prediction = model.predict(X_test)\n    return prediction","8cd21c51":"from sklearn.ensemble import AdaBoostClassifier\ndef AdaBoostClassifier_Model(X_train, X_test, y_train):\n    model = AdaBoostClassifier(random_state=1)\n    model.fit(X_train, y_train)\n    prediction = model.predict(X_test)\n    model.score(X_test,y_test)\n    \n    return prediction","d7ad14af":"prediction_adaBoost = AdaBoostClassifier_Model(X_train, X_test, y_train)\naccuracy_score_adaBoost = accuracy_score(y_test, prediction_adaBoost)\nprint(accuracy_score_adaBoost)  ","76f8837c":"confusion_matrix_adaBoost = confusion_matrix(y_test, prediction_adaBoost)\nprint (confusion_matrix_adaBoost)","64843a90":"classification_report_adaBoost = classification_report(y_test, prediction_adaBoost)\nprint(classification_report_adaBoost)","cc1b86d0":"from sklearn.naive_bayes import GaussianNB\n\ndef GaussianNB_Model(X_train, X_test, y_train):\n    gnb=GaussianNB()\n    gnb.fit(X_train,y_train)\n    pred_gnb = gnb.predict(X_test)\n    return pred_gnb","5893adbb":"prediction_GaussianNB = GaussianNB_Model(X_train, X_test, y_train)\naccuracy_score_GaussianNB = accuracy_score(y_test, prediction_GaussianNB)\nprint(accuracy_score_GaussianNB)  ","96eadb38":"confusion_matrix_GaussianNB = confusion_matrix(y_test, prediction_GaussianNB)\nprint (confusion_matrix_GaussianNB)","6d422872":"classification_report_GaussianNB = classification_report(y_test, prediction_GaussianNB)\nprint(classification_report_GaussianNB)","caf3eb3b":"from sklearn.tree import DecisionTreeClassifier\ndef DecisionTreeClassifier_Model(X_train, X_test, y_train):\n    dtree = DecisionTreeClassifier()\n    dtree.fit(X_train,y_train)\n    pred_dtree = dtree.predict(X_test)\n    return pred_dtree","a5cd2b50":"prediction_dtree = DecisionTreeClassifier_Model(X_train, X_test, y_train)\naccuracy_score_dtree = accuracy_score(y_test, prediction_dtree)\nprint(accuracy_score_dtree)  ","c7eaa7dc":"confusion_matrix_dtree = confusion_matrix(y_test, prediction_dtree)\nprint (confusion_matrix_dtree)","29d3a071":"classification_report_dtree = classification_report(y_test, prediction_dtree)\nprint(classification_report_dtree)","74191854":"from sklearn.tree import DecisionTreeClassifier\ndef DecisionTreeClassifier_Model_Deep(X_train, X_test, y_train):\n    dtree_2 = DecisionTreeClassifier(max_features=7 , max_depth=6,  min_samples_split=8)\n    dtree_2.fit(X_train,y_train)\n    pred_dtree_2 = dtree_2.predict(X_test)\n    return pred_dtree_2","6cb67504":"prediction_dtree_deep = DecisionTreeClassifier_Model_Deep(X_train, X_test, y_train)\naccuracy_score_dtree_deep = accuracy_score(y_test, prediction_dtree_deep)\nprint(accuracy_score_dtree_deep) ","0c79fea7":"confusion_matrix_dtree_deep = confusion_matrix(y_test, prediction_dtree_deep)\nprint (confusion_matrix_dtree_deep)","9eeb4013":"classification_report_dtree_deep = classification_report(y_test, prediction_dtree_deep)\nprint(classification_report_dtree_deep)","26fdde09":"from sklearn.ensemble import RandomForestClassifier\n\ndef RandomForestClassifier_model(X_train, X_test, y_train):\n    rfc = RandomForestClassifier(max_depth=6, max_features=7)\n    rfc.fit(X_train, y_train)\n    pred_rfc = rfc.predict(X_test)\n    return pred_rfc","4264922b":"prediction_random_forest = RandomForestClassifier_model(X_train, X_test, y_train)\naccuracy_score_random_forest = accuracy_score(y_test, prediction_random_forest)\nprint(accuracy_score_random_forest)  ","7dc57cdf":"confusion_matrix_random_forest = confusion_matrix(y_test, prediction_random_forest)\nprint (confusion_matrix_random_forest)","7bda5770":"classification_report_random_forest = classification_report(y_test, prediction_random_forest)\nprint(classification_report_random_forest)","39dfe934":"from sklearn.svm import SVC\n\ndef svm_Model(X_train, X_test, y_train):\n    svc = SVC(gamma = 0.01, C = 100)#, probability=True)\n    svc.fit(X_train, y_train)\n    pred_svc = svc.predict(X_test)\n    return pred_svc","9e93990b":"pred_svc = svm_Model(X_train, X_test, y_train)\naccuracy_pred_svc = accuracy_score(y_test, pred_svc)\nprint(accuracy_pred_svc)  ","02670220":"confusion_matrix_svc = confusion_matrix(y_test, pred_svc)\nprint (confusion_matrix_svc)","a6601b38":"classification_report_svc = classification_report(y_test, pred_svc)\nprint(classification_report_svc)","c9ff903b":"print(confusion_matrix(y_test, pred_svc))\nprint(classification_report(y_test, pred_svc))\nprint(accuracy_score(y_test, pred_svc))","59363764":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\n\npipe_svc = Pipeline([('std_scl', StandardScaler()), \n                    ('pca', PCA(n_components=10)),\n                    ('svc', SVC(random_state=1))])\n\npipe_svc.fit(X_train, y_train)\n\nprint('Test Accuracy: %.3f' % pipe_svc.score(X_test, y_test))","07043d2c":"from sklearn.model_selection import validation_curve\n\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = validation_curve(estimator=pipe_svc,\n                                             X=X_train,\n                                             y=y_train,\n                                             param_name='svc__C',\n                                             param_range=param_range,\n                                             cv=10)\n\n# Mean value of accuracy against training data\ntrain_mean = np.mean(train_scores, axis=1)\n\n# Standard deviation of training accuracy per number of training samples\ntrain_std = np.std(train_scores, axis=1)\n\n# Same as above for test data\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot training accuracies \nplt.plot(param_range, train_mean, color='red', marker='o', label='Training Accuracy')\n# Plot the variance of training accuracies\nplt.fill_between(param_range,\n                train_mean + train_std,\n                train_mean - train_std,\n                alpha=0.15, color='red')\n\n# Plot for test data as training data\nplt.plot(param_range, test_mean, color='blue', linestyle='--', marker='s', \n        label='Test Accuracy')\nplt.fill_between(param_range,\n                test_mean + test_std,\n                test_mean - test_std,\n                alpha=0.15, color='blue')\n\nplt.xscale('log')\nplt.xlabel('Regularization parameter C')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","8ad73a6c":"from sklearn.model_selection import learning_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef learning_curve_plot(clf,title):\n    \n    train_sizes,train_scores,test_scores = learning_curve(clf,X_train,y_train,random_state = 42,cv = 5)\n\n    plt.figure()\n    plt.title(title)\n    \n    ylim = (0.1, 1.01)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                train_scores_mean + train_scores_std, alpha=0.1,\n                color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n        label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n        label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    plt.show()","30e8813a":"learning_curve_plot(LogisticRegression(),'Learning Curve of Logistic Regression')","dba78b3c":"learning_curve_plot(RandomForestClassifier(),'Learning Curve of Random Forest')","ce6cca79":"learning_curve_plot(DecisionTreeClassifier(),'Learning Curve of Decision Tree')","6937b0ca":"accuracy_pred_auto_sklearn = 0.8171064604185623\nlist_accuracy = [sc_knn9, accuracy_logreg, accuracy_score_adaBoost, accuracy_score_bagging,accuracy_score_GaussianNB, accuracy_score_dtree, \n               accuracy_score_dtree_deep, accuracy_score_random_forest, accuracy_pred_auto_sklearn]\n\nlist_accuracy_label=['KNN','Logistic Regresion' , 'AdaBoost', 'Bagging','GaussianNB', 'DecisionTree', 'DecisionTreeDeep',\n                     'RandomForest', 'Auto Sklearn', ] \n\n#accuracy_pred_svc\n\n\n# allow plots to appear within the notebook\n%matplotlib inline\n\n# plot the relationship between K and testing accuracy\n\nplt.plot(list_accuracy_label, list_accuracy, color='green', marker='o')\nplt.xlabel('Models')\nplt.xticks(list_accuracy_label, rotation=40) \nplt.ylabel('Accuracy Models')\n\nplt.show()","81748eeb":"list_accuracy","8112e378":"* 433 ==> TP: True positives (prediction: approved, true: approved)\n* 449 ==> TN: True negatives (prediction: not approved, true: not approved)\n* 96 ==> FP: False positives (prediction: approved, true: not approved)\n* 121 ==> FN: False negatives (prediction: not approved, true: approved)","59a4c610":"# Running differents models","f6d046ee":"# Cleanup and Transform the Data","f4ae1881":"After we reviewed features description, we choose some features as below and we removed the unnecesary features:\n* loan_amnt,\n* funded_amnt:                       \n* funded_amnt_inv:                   \n* months_pay_loan                   \n* interest_rate_loan                \n* monthly_payment                   \n* grade                             \n* sub_grade                       \n* years_experience                  \n* home_ownership                    \n* method_receives_loan              \n* verification_status:               \n* loan_status                       \n* purpose\n* zip_code                          \n* addr_state                        \n* amount_pay_month_another_obligations     \n* delinquency_others_loan    \n* total_credit_used                 \n* num_credit_lines \n\n","5b2383ba":"# Payment fulfillment based on loan amount","0e064778":"* Classification Report","c1750fea":"* Accuracy Score","ee38f2b9":"# 3. Visualize the Data","b2244e26":"* 416 ==> TP: True positives (prediction: approved, true: approved)\n* 441 ==> TN: True negatives (prediction: not approved, true: not approved)\n* 113 ==> FP: False positives (prediction: approved, true: not approved)\n* 129 ==> FN: False negatives (prediction: not approved, true: approved)","8cdd80ea":"* Classification Report","939abcf3":"Interpretation:\n\n* People with more years of experience tend to have a better grade.","cf3afc79":"# Learning Curve: Logistic Regression, Random Forest,Decision Tree","b3d38201":"# Impact analysis of loan payment based on ownership property","7f6b51ed":"# 2. Logistic Regresion","51a6649a":"* Classification Report","9c8da123":"* 513 ==> TP: True positives (prediction: approved, true: approved)\n* 24 ==> TN: True negatives (prediction: not approved, true: not approved)\n* 16 ==> FP: False positives (prediction: approved, true: not approved)\n* 546 ==> FN: False negatives (prediction: not approved, true: approved)","cdebc3c1":"* Accuracy Score","3d43d1c0":"# 7. Random Forest Classifier","c3b5a320":"* Confusion matrix","b0e9af18":"Interpretation: \n* People who rent tend to borrow more credits than people who have a mortgage.\n* People who have a mortgage tend to borrow  more credits than people who are owners.","c562afbe":"# 5. Gaussian Naive Bayes","410a66bc":"# Impact of working experience in the credit amount","6202a788":"# 6. Decision Tree Classifier","624f30b8":"# Realtionship between working experience, credit amount and loan status","690c6733":"* 434 ==> TP: True positives (prediction: approved, true: approved)\n* 419 ==> TN: True negatives (prediction: not approved, true: not approved)\n* 95 ==> FP: False positives (prediction: approved, true: not approved)\n* 151 ==> FN: False negatives (prediction: not approved, true: approved)","bcdffbe5":"# Validation Curve with SVM","8aaf2dcd":"* 268 ==> TP: True positives (prediction: approved, true: approved)\n* 7302 ==> TN: True negatives (prediction: not approved, true: not approved)\n* 290 ==> FP: False positives (prediction: approved, true: not approved)\n* 647 ==> FN: False negatives (prediction: not approved, true: approved)","16754940":"* Classification Report","9ac28217":"* 383 ==> TP: True positives (prediction: approved, true: approved)\n* 407 ==> TN: True negatives (prediction: not approved, true: not approved)\n* 146 ==> FP: False positives (prediction: approved, true: not approved)\n* 163 ==> FN: False negatives (prediction: not approved, true: approved)","ee5b6948":"# 2. Visualizing Data usefull for the prediction","2069e903":"# 1. Loading Data","8c79f429":"* Accuracy Score","913e64e3":"* Accuracy Score","bb23aafd":"# 1. KNeighbors Classifier","60b3e804":"* Classification Report","26616a87":"Interpretation:\n* People with more years of experience can access a bigger loans.","5486d623":"* Confusion matrix","dcf415eb":"# 4. Visualizing nulls and nan values","ccb7e739":"* Confusion matrix","5a8d3348":"* Accuracy Score","25958659":"4.1 Bagging meta-estimator\nBagging meta-estimator is an ensembling algorithm that can be used for both classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:\n\nRandom subsets are created from the original dataset (Bootstrapping).\nThe subset of the dataset includes all features.\nA user-specified base estimator is fitted on each of these smaller sets.\nPredictions from each model are combined to get the final result.","77be4bf8":"another decision tree with different parameters for max_features, max_depth and min_sample_split\nhttps:\/\/www.kaggle.com\/dejavu23\/titanic-eda-to-ml-beginner#Part-3:-Scikit-learn-basic-ML-algorithms-and-comparison-of-model-results","996cf84c":"* 452 ==> TP: True positives (prediction: approved, true: approved)\n* 352 ==> TN: True negatives (prediction: not approved, true: not approved)\n* 77 ==> FP: False positives (prediction: approved, true: not approved)\n* 218 ==> FN: False negatives (prediction: not approved, true: approved)","7dd372f3":"We run the next models:\n* KNeighbors Classifier accuracy for the model was 0.54049135577798\n* LogisticRegression accuracy for the model was 0.7797998180163785\n* AdaBoostClassifier accuracy for the model was 0.802547770700637\n* BaggingClassifier accuracy for the model was 0.7561419472247498\n* Gaussian Naive Bayes accuracy for the model was 0.7616014558689718\n* Decision Tree Classifier accuracy for the model was 0.718835304822566\n* Decision Tree Deep Classifier accuracy for the model was 0.7315741583257507\n* Random Forest Classifier accuracy for the model was 0.7761601455868972\n* Auto SklearnClassifier accuracy for the model was 0.8171064604185623\n\nAfter we computed manually, We can conclude that the best model for decide if we get a loan or not, it was AdaBoostClassifier: 0.802547770700637\n\nBut when we did the same exercise with Auto SklearnClassifier and the best model\"s accuracy score was: 0.8171064604185623\n\nThe accuracy for AdaBoostClassifier 0.8061874431301183 was near that the SklearnClassifier 0.8171064604185623, but the accuracy for the Auto SklearnClassifier is better.","29c3032b":"* Classification Report","71141bb3":"Team: \n* Thi Thuy Huong Nguyen\n* Khac Nhat Pham\n* Flor Vargas","2fd40a1d":"Interpretation:\n* People with more years of experience tend to have more credits in relation with the other people.","dc8ca0e0":"* Accuracy Score","b75ad13e":"# SVC Model","59fced21":"* Accuracy Score","df316b5f":"# 4. AdaBoost Classifier","32f566a3":"* Classification Report","bf1348b0":"* Confusion matrix","66e193fe":"* Classification Report","0ed7804b":"Interpretation:\n* The credit amount does not impact in its status","2d96a664":"# 4. Encode the Data","50973feb":"* Confusion matrix","707676d2":"# Impact of working experience in the grade","3fb12c53":"Validate if the columns are not null","b9d0230d":"Gamma has a very high value, then the decision boundary is just going to be dependent upon the points that are very close to the line which effectively results in ignoring some of the points that are very far from the decision boundary, if the gamma value is low even the far away points get considerable weight and we get a more linear curve.","1945e072":"# Split your dataset","7da576a3":"* Confusion matrix","11837de0":"We can see clearly that the training score is still around the maximum and the validation score could be increased with more training samples\n\nCompared to the theory we covered, here our y-axis is 'score', not 'error', so the higher the score, the better the performance of the model.\nTraining score (red line) is at its maximum regardless of training examples\nThis shows severe overfitting\nCross-validation score (green line) increases over time\nHuge gap between cross-validation score and training score indicates high variance scenario\nReduce complexity of the model or gather more data","3e61a6fb":"\nWe can see clearly that the training score is still around the maximum and the validation score could be increased with more training samples.\nCompared to the theory we covered, here our y-axis is 'score', not 'error', so the higher the score, the better the performance of the model.\nTraining score (red line) is at its maximum regardless of training examples\nThis shows severe overfitting\nCross-validation score (green line) increases over time\nHuge gap between cross-validation score and training score indicates high variance scenario\nReduce complexity of the model or gather more data","892ab21a":"# Exploring and  Visualizing our data","9a9ce4d5":"* Confusion matrix","14bc3c9b":"That the training score and the cross-validation score are both not very good at the beginning. With more training sample, it can get higher accuracy.","db95dc4c":"* Confusion matrix","fa85024e":"* Testing model with different distance","463ab1db":"* Accuracy Score","f9409d55":"# 3. Bagging Classifier","5119ec0b":"# Accuracy KNN Curve","50f35a8d":"* 422 ==> TP: True positives (prediction: approved, true: approved)\n* 409 ==> TN: True negatives (prediction: not approved, true: not approved)\n* 107 ==> FP: False positives (prediction: approved, true: not approved)\n* 161 ==> FN: False negatives (prediction: not approved, true: approved)"}}