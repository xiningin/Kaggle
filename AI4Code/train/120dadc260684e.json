{"cell_type":{"95fb0a3c":"code","431178dd":"code","e4cbca5c":"code","7ecb24e6":"code","0f167c5b":"code","79ffdb46":"code","9dbc605b":"code","cf3dd4cc":"code","c941cf40":"code","ca47939c":"code","65d10450":"code","fa2c8f7d":"code","2dd80204":"code","ed9e0cb4":"code","eb5519be":"code","09a22a39":"code","13dcdf6e":"code","0e746a31":"code","c58ef180":"code","fc121d19":"code","297574dd":"code","3196a9f1":"code","9e8e4296":"code","737f73e8":"code","c0fe1057":"code","6c202c14":"code","268ec3f1":"code","405de50c":"code","083b5a51":"markdown","da5691d2":"markdown","10d2852f":"markdown","4391fb15":"markdown","5d1a77bd":"markdown","1a29fc27":"markdown","e7dad61c":"markdown","c1e9a4eb":"markdown","42de6e36":"markdown","92311875":"markdown","d84775cd":"markdown","780ed941":"markdown","3510deef":"markdown","d4c23568":"markdown","8f111813":"markdown","d1925ee6":"markdown","a73e4426":"markdown","84d1fad9":"markdown","546ee14a":"markdown","f65d6746":"markdown","ae5e408c":"markdown","1f10c1d4":"markdown","02bf24d4":"markdown","8cc79f3f":"markdown","6236ae61":"markdown","8e8bf1ad":"markdown","6b495bd2":"markdown","a20165fa":"markdown","9c488e27":"markdown","9e28e8f7":"markdown","a6c5e341":"markdown","178549d8":"markdown","3c1552b8":"markdown","cb66bf9a":"markdown","5838fb90":"markdown"},"source":{"95fb0a3c":"!pip install seaborn --upgrade","431178dd":"# load the libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import CategoricalDtype\nimport matplotlib.gridspec as gridspec\n","e4cbca5c":"# Read the dataset\nkaggle_2020 = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv\", skiprows = 1)\n","7ecb24e6":"# Rename columns\n\ncol_dict = {0: 'Duration',\n 1: 'Age Group',\n 2: 'Gender',\n 3: 'Country',\n 4: 'Education Level',\n 5: 'Job Title',\n 6: 'Programming Experience',\n 7: 'Python',\n 8: 'R',\n 9: 'SQL',\n 10: 'C',\n 11: 'C++',\n 12: 'Java',\n 13: 'Javascript',\n 14: 'Julia',\n 15: 'Swift',\n 16: 'Bash',\n 17: 'MATLAB',\n 18: 'Progamming None',\n 19: 'Programming Other',\n 20: 'Recommended Language',\n 21: 'Jupyter (JupyterLab, Jupyter Notebooks, etc)',\n 22: 'RStudio',\n 23: 'Visual Studio \/ Visual Studio Code',\n 24: 'Click to write Choice 13',\n 25: 'PyCharm',\n 26: 'Spyder',\n 27: 'Notepad++',\n 28: 'Sublime Text',\n 29: 'Vim \/ Emacs',\n 30: 'MATLAB IDE',\n 31: 'IDE None',\n 32: 'IDE Other',\n 33: 'Kaggle Notebooks',\n 34: 'Colab Notebooks',\n 35: 'Azure Notebooks',\n 36: 'Paperspace \/ Gradient',\n 37: 'Binder \/ JupyterHub',\n 38: 'Code Ocean',\n 39: 'IBM Watson Studio',\n 40: 'Amazon Sagemaker Studio',\n 41: 'Amazon EMR Notebooks',\n 42: 'Google Cloud AI Platform Notebooks',\n 43: 'Google Cloud Datalab Notebooks',\n 44: 'Databricks Collaborative Notebooks',\n 45: 'Hosted Notebooks None',\n 46: 'Hosted Notebook Other',\n 47: 'Most often used Cloud Platform',\n 48: 'GPUs',\n 49: 'TPUs',\n 50: 'Hardware None',\n 51: 'Hardware Other',\n 52: 'TPU usage experience',\n 53: 'Matplotlib',\n 54: 'Seaborn',\n 55: 'Plotly \/ Plotly Express',\n 56: 'Ggplot \/ ggplot2',\n 57: 'Shiny',\n 58: 'D3 js',\n 59: 'Altair',\n 60: 'Bokeh',\n 61: 'Geoplotlib',\n 62: 'Leaflet \/ Folium',\n 63: 'Visualization None',\n 64: 'Visualization Other',\n 65: 'ML Experience',\n 66: 'Scikit-learn',\n 67: 'TensorFlow',\n 68: 'Keras',\n 69: 'PyTorch',\n 70: 'Fast.ai',\n 71: 'MXNet',\n 72: 'Xgboost',\n 73: 'LightGBM',\n 74: 'CatBoost',\n 75: 'Prophet',\n 76: 'H2O 3',\n 77: 'Caret',\n 78: 'Tidymodels',\n 79: 'JAX',\n 80: 'ML None',\n 81: 'ML Other',\n 82: 'Linear or Logistic Regression',\n 83: 'Decision Trees or Random Forests',\n 84: 'Gradient Boosting Machines (xgboost, lightgbm, etc)',\n 85: 'Bayesian Approaches',\n 86: 'Evolutionary Approaches',\n 87: 'Dense Neural Networks (MLPs, etc)',\n 88: 'Convolutional Neural Networks',\n 89: 'Generative Adversarial Networks',\n 90: 'Recurrent Neural Networks',\n 91: 'Transformer Networks (BERT, gpt-3, etc)',\n 92: 'ML Algorithms None',\n 93: 'ML Algo Other',\n 94: 'General purpose image\/video tools (PIL, cv2, skimage, etc)',\n 95: 'Image segmentation methods (U-Net, Mask R-CNN, etc)',\n 96: 'Object detection methods (YOLOv3, RetinaNet, etc)',\n 97: 'Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)',\n 98: 'Generative Networks (GAN, VAE, etc)',\n 99: 'CV None',\n 100: 'CV Other',\n 101: 'Word embeddings\/vectors (GLoVe, fastText, word2vec)',\n 102: 'Encoder-decorder models (seq2seq, vanilla transformers)',\n 103: 'Contextualized embeddings (ELMo, CoVe)',\n 104: 'Transformer language models (GPT-3, BERT, XLnet, etc)',\n 105: 'NLP None',\n 106: 'NLP Other',\n 107: 'Company Size',\n 108: 'Data Science Team Size',\n 109: 'ML Application in Company',\n 110: 'Analyze and understand data to influence product or business decisions',\n 111: 'Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data',\n 112: 'Build prototypes to explore applying machine learning to new areas',\n 113: 'Build and\/or run a machine learning service that operationally improves my product or workflows',\n 114: 'Experimentation and iteration to improve existing ML models',\n 115: 'Do research that advances the state of the art of machine learning',\n 116: 'None of these activities are an important part of my role at work',\n 117: 'Work Activity Other',\n 118: 'Annual Compensation',\n 119: 'ML Spend',\n 120: 'Amazon Web Services (AWS)',\n 121: 'Microsoft Azure',\n 122: 'Google Cloud Platform (GCP)',\n 123: 'IBM Cloud \/ Red Hat',\n 124: 'Oracle Cloud',\n 125: 'SAP Cloud',\n 126: 'Salesforce Cloud',\n 127: 'VMware Cloud',\n 128: 'Alibaba Cloud',\n 129: 'Tencent Cloud',\n 130: 'Cloud None',\n 131: 'Cloud Other',\n 132: 'Amazon EC2',\n 133: 'AWS Lambda',\n 134: 'Amazon Elastic Container Service',\n 135: 'Azure Cloud Services',\n 136: 'Microsoft Azure Container Instances',\n 137: 'Azure Functions',\n 138: 'Google Cloud Compute Engine',\n 139: 'Google Cloud Functions',\n 140: 'Google Cloud Run',\n 141: 'Google Cloud App Engine',\n 142: 'Cloud Products None',\n 143: 'Cloud Product Other',\n 144: 'Amazon SageMaker',\n 145: 'Amazon Forecast',\n 146: 'Amazon Rekognition',\n 147: 'Azure Machine Learning Studio',\n 148: 'Azure Cognitive Services',\n 149: 'Google Cloud AI Platform \/ Google Cloud ML Engine',\n 150: 'Google Cloud Video AI',\n 151: 'Google Cloud Natural Language',\n 152: 'Google Cloud Vision AI',\n 153: 'ML Products None',\n 154: 'ML Prod Other',\n 155: 'MySQL',\n 156: 'PostgresSQL',\n 157: 'SQLite',\n 158: 'Oracle Database',\n 159: 'MongoDB',\n 160: 'Snowflake',\n 161: 'IBM Db2',\n 162: 'Microsoft SQL Server',\n 163: 'Microsoft Access',\n 164: 'Microsoft Azure Data Lake Storage',\n 165: 'Amazon Redshift',\n 166: 'Amazon Athena',\n 167: 'Amazon DynamoDB',\n 168: 'Google Cloud BigQuery',\n 169: 'Google Cloud SQL',\n 170: 'Google Cloud Firestore',\n 171: 'DW None',\n 172: 'DW Other',\n 173: 'Most Used Database',\n 174: 'Amazon QuickSight',\n 175: 'Microsoft Power BI',\n 176: 'Google Data Studio',\n 177: 'Looker',\n 178: 'Tableau',\n 179: 'Salesforce',\n 180: 'Einstein Analytics',\n 181: 'Qlik',\n 182: 'Domo',\n 183: 'TIBCO Spotfire',\n 184: 'Alteryx',\n 185: 'Sisense',\n 186: 'SAP Analytics Cloud',\n 187: 'BI None',\n 188: 'BI Other',\n 189: 'Most Used BI Tool',\n 190: 'Automated data augmentation (e.g. imgaug, albumentations)',\n 191: 'Automated feature engineering\/selection (e.g. tpot, boruta_py)',\n 192: 'Automated model selection (e.g. auto-sklearn, xcessiv)',\n 193: 'Automated model architecture searches (e.g. darts, enas)',\n 194: 'Automated hyperparameter tuning (e.g. hyperopt, ray.tune, Vizier)',\n 195: 'Automation of full ML pipelines (e.g. Google AutoML, H20 Driverless AI)',\n 196: 'Auto ML None',\n 197: 'Auto ML Other',\n 198: 'Google Cloud AutoML',\n 199: 'H20 Driverless AI',\n 200: 'Databricks AutoML',\n 201: 'DataRobot AutoML',\n 202: 'Tpot',\n 203: 'Auto-Keras',\n 204: 'Auto-Sklearn',\n 205: 'Auto_ml',\n 206: 'Xcessiv',\n 207: 'MLbox',\n 208: 'Auto ML Tools None',\n 209: 'Auto ML Tools Other',\n 210: 'Neptune.ai',\n 211: 'Weights & Biases',\n 212: 'Comet.ml',\n 213: 'Sacred + Omniboard',\n 214: 'TensorBoard',\n 215: 'Guild.ai',\n 216: 'Polyaxon',\n 217: 'Trains',\n 218: 'Domino Model Monitor',\n 219: 'ML Tools None',\n 220: 'ML Tools Other',\n 221: 'Plotly Dash',\n 222: 'Streamlit',\n 223: 'NBViewer',\n 224: 'GitHub',\n 225: 'Personal blog',\n 226: 'Kaggle',\n 227: 'Colab',\n 228: 'Shiny Deploy',\n 229: 'I do not share my work publicly',\n 230: 'Deploy Other',\n 231: 'Coursera',\n 232: 'edX',\n 233: 'Kaggle Learn Courses',\n 234: 'DataCamp',\n 235: 'Fast.ai Learning',\n 236: 'Udacity',\n 237: 'Udemy',\n 238: 'LinkedIn Learning',\n 239: 'Cloud-certification programs (direct from AWS, Azure, GCP, or similar)',\n 240: 'University Courses (resulting in a university degree)',\n 241: 'Learn ML None',\n 242: 'Learn ML Other',\n 243: 'Primary Analysis Tool',\n 244: 'Twitter (data science influencers)',\n 245: \"Email newsletters (Data Elixir, O'Reilly Data & AI, etc)\",\n 246: 'Reddit (r\/machinelearning, etc)',\n 247: 'Kaggle (notebooks, forums, etc)',\n 248: 'Course Forums (forums.fast.ai, Coursera forums, etc)',\n 249: 'YouTube (Kaggle YouTube, Cloud AI Adventures, etc)',\n 250: 'Podcasts (Chai Time Data Science, O\u2019Reilly Data Show, etc)',\n 251: 'Blogs (Towards Data Science, Analytics Vidhya, etc)',\n 252: 'Journal Publications (peer-reviewed journals, conference proceedings, etc)',\n 253: 'Slack Communities (ods.ai, kagglenoobs, etc)',\n 254: 'Media None',\n 255: 'Media Other',\n 256: 'Aspire Amazon Web Services (AWS)',\n 257: 'Aspire Microsoft Azure',\n 258: 'Aspire Google Cloud Platform (GCP)',\n 259: 'Aspire IBM Cloud \/ Red Hat',\n 260: 'Aspire Oracle Cloud',\n 261: 'Aspire SAP Cloud',\n 262: 'Aspire VMware Cloud',\n 263: 'Aspire Salesforce Cloud',\n 264: 'Aspire Alibaba Cloud',\n 265: 'Aspire Tencent Cloud',\n 266: 'Aspire Cloud None',\n 267: 'Aspire Cloud Other',\n 268: 'Aspire Amazon EC2',\n 269: 'Aspire AWS Lambda',\n 270: 'Aspire Amazon Elastic Container Service',\n 271: 'Aspire Azure Cloud Services',\n 272: 'Aspire Microsoft Azure Container Instances',\n 273: 'Aspire Azure Functions',\n 274: 'Aspire Google Cloud Compute Engine',\n 275: 'Aspire Google Cloud Functions',\n 276: 'Aspire Google Cloud Run',\n 277: 'Aspire Google Cloud App Engine',\n 278: 'Aspire Cloud Products None',\n 279: 'Aspire Cloud Products Other',\n 280: 'Aspire Amazon SageMaker',\n 281: 'Aspire Amazon Forecast',\n 282: 'Aspire Amazon Rekognition',\n 283: 'Aspire Azure Machine Learning Studio',\n 284: 'Aspire Azure Cognitive Services',\n 285: 'Aspire Google Cloud AI Platform \/ Google Cloud ML Engine',\n 286: 'Aspire Google Cloud Video AI',\n 287: 'Aspire Google Cloud Natural Language',\n 288: 'Aspire Google Cloud Vision AI',\n 289: 'Aspire ML None',\n 290: 'Aspire ML Other',\n 291: 'Aspire MySQL',\n 292: 'Aspire PostgresSQL',\n 293: 'Aspire SQLite',\n 294: 'Aspire Oracle Database',\n 295: 'Aspire MongoDB',\n 296: 'Aspire Snowflake',\n 297: 'Aspire IBM Db2',\n 298: 'Aspire Microsoft SQL Server',\n 299: 'Aspire Microsoft Access',\n 300: 'Aspire Microsoft Azure Data Lake Storage',\n 301: 'Aspire Amazon Redshift',\n 302: 'Aspire Amazon Athena',\n 303: 'Aspire Amazon DynamoDB',\n 304: 'Aspire Google Cloud BigQuery',\n 305: 'Aspire Google Cloud SQL',\n 306: 'Aspire Google Cloud Firestore',\n 307: 'Aspire DW None',\n 308: 'Aspire DW Other',\n 309: 'Aspire Microsoft Power BI',\n 310: 'Aspire Amazon QuickSight',\n 311: 'Aspire Google Data Studio',\n 312: 'Aspire Looker',\n 313: 'Aspire Tableau',\n 314: 'Aspire Salesforce',\n 315: 'Aspire Einstein Analytics',\n 316: 'Aspire Qlik',\n 317: 'Aspire Domo',\n 318: 'Aspire TIBCO Spotfire',\n 319: 'Aspire Alteryx',\n 320: 'Aspire Sisense',\n 321: 'Aspire SAP Analytics Cloud',\n 322: 'Aspire BI None',\n 323: 'Aspire BI Other',\n 324: 'Aspire Automated data augmentation (e.g. imgaug, albumentations)',\n 325: 'Aspire Automated feature engineering\/selection (e.g. tpot, boruta_py)',\n 326: 'Aspire Automated model selection (e.g. auto-sklearn, xcessiv)',\n 327: 'Aspire Automated model architecture searches (e.g. darts, enas)',\n 328: 'Aspire Automated hyperparameter tuning (e.g. hyperopt, ray.tune, Vizier)',\n 329: 'Aspire Automation of full ML pipelines (e.g. Google Cloud AutoML, H20 Driverless AI)',\n 330: 'Aspire AutoML None',\n 331: 'Aspire AutoML Other',\n 332: 'Aspire Google Cloud AutoML',\n 333: 'Aspire H20 Driverless AI',\n 334: 'Aspire Databricks AutoML',\n 335: 'Aspire DataRobot AutoML',\n 336: 'Aspire Tpot',\n 337: 'Aspire Auto-Keras',\n 338: 'Aspire Auto-Sklearn',\n 339: 'Aspire Auto_ml',\n 340: 'Aspire Xcessiv',\n 341: 'Aspire MLbox',\n 342: 'Aspire AutoML B None',\n 343: 'Aspire AutoML B Other',\n 344: 'Aspire Neptune.ai',\n 345: 'Aspire Weights & Biases',\n 346: 'Aspire Comet.ml',\n 347: 'Aspire Sacred + Omniboard',\n 348: 'Aspire TensorBoard',\n 349: 'Aspire Guild.ai',\n 350: 'Aspire Polyaxon',\n 351: 'Aspire Trains',\n 352: 'Aspire Domino Model Monitor',\n 353: 'Aspire ML Tools  None',\n 354: 'Aspire ML Tools Other'}\n\nkaggle_2020.columns = np.arange(len(kaggle_2020.columns))\nkaggle_2020 = kaggle_2020.rename(columns = col_dict)\n\n# Drop mising salaries\nkaggle_2020.dropna(subset=[\"Annual Compensation\"], inplace = True)\n# kaggle_2020[\"Annual Compensation\"].value_counts(dropna = False)\n","0f167c5b":"# Bucket Salaries\n\nsalary_dict = {\n    'Missing' : 'Missing' ,\n'$0-999' : 'Less than 10k' ,\n'1,000-1,999' : 'Less than 10k' ,\n'2,000-2,999' : 'Less than 10k' ,\n'3,000-3,999' : 'Less than 10k' ,\n'4,000-4,999' : 'Less than 10k' ,\n'5,000-7,499' : 'Less than 10k' ,\n'7,500-9,999' : 'Less than 10k' ,\n'10,000-14,999' : '10k to 50k' ,\n'15,000-19,999' : '10k to 50k' ,\n'20,000-24,999' : '10k to 50k' ,\n'25,000-29,999' : '10k to 50k' ,\n'30,000-39,999' : '10k to 50k' ,\n'40,000-49,999' : '10k to 50k' ,\n'50,000-59,999' : '50k to 80k' ,\n'60,000-69,999' : '50k to 80k' ,\n'70,000-79,999' : '50k to 80k' ,\n'100,000-124,999' : '80k to 150k' ,\n'125,000-149,999' : '80k to 150k' ,\n'80,000-89,999' : '80k to 150k' ,\n'90,000-99,999' : '80k to 150k' ,\n'> $500,000' : '150k and above' ,\n'150,000-199,999' : '150k and above' ,\n'200,000-249,999' : '150k and above' ,\n'250,000-299,999' : '150k and above' ,\n'300,000-500,000' : '150k and above'\n}\n\nkaggle_2020[\"Earnings\"] = kaggle_2020[\"Annual Compensation\"].replace(salary_dict)\n\n# Create order list\nchart_order = CategoricalDtype(\n    [\"Less than 10k\", \"10k to 50k\", \"50k to 80k\", \"80k to 150k\", \"150k and above\"], \n    ordered=True\n)\n\nchart_order_list  = [\"Less than 10k\", \"10k to 50k\", \"50k to 80k\", \"80k to 150k\", \"150k and above\"]\n\nagg_earnings = kaggle_2020[\"Earnings\"].value_counts(sort = False).to_frame().rename(columns = {\"Earnings\" : \"count\"} )\n\n","79ffdb46":"# Create Data Wrangling and Dashboard functions\n\n\n# For Y\/N fields\ndef cross_tab_row(input_df, varname, index_var, out_text):\n    \n    out_df = pd.crosstab(columns = input_df[varname], index = input_df[index_var],\n                         values = input_df[index_var], aggfunc = \"count\").fillna(0)\n    \n    out_df.columns = np.arange(len(out_df.columns))\n    out_df.rename(columns = {0 : \"count\"}, inplace = True)\n\n    overall_val = input_df[varname].dropna().value_counts()[0] \/ input_df.shape[0]\n    out_df = out_df.div(agg_earnings).reset_index().rename(columns = {\"index\" : index_var}).reset_index(drop = True)\n    out_df[varname] = out_df[\"count\"] \/ overall_val * 100\n    out_df[varname].fillna(0, inplace = True)\n    out_df.drop(columns = {\"count\"}, inplace = True)\n    index_range = max(out_df[varname]) - min(out_df[varname])\n    min_val = min(out_df[varname])\n    max_val = max(out_df[varname])\n    out_df[index_var] = out_df[index_var].astype(chart_order)\n    out_df.sort_values(index_var, inplace = True)\n    \n    return out_df, overall_val, index_range, min_val, max_val, out_text\n\n\n# For Multiple Choice Fields\ndef cross_tab_mat(input_df, varname, index_var):\n    out_list = []\n    agg_earnings_mat = pd.crosstab(index = input_df[index_var], columns = input_df[varname], values = input_df[varname], aggfunc = \"count\", margins = True, margins_name = \"Overall\", normalize = \"columns\").fillna(0)\n    agg_earnings_mat = agg_earnings_mat.loc[agg_earnings_mat[\"Overall\"] >= 0.05]\n    overall_vals = agg_earnings_mat[[\"Overall\"]]\n    agg_earnings_mat = agg_earnings_mat.T.drop(\"Overall\")\n    cols_list = agg_earnings_mat.columns.to_list()\n    for col in cols_list:\n        base_val = overall_vals.loc[col, \"Overall\"]\n        out_df = agg_earnings_mat[[col]].reset_index()\n        out_df[col] = out_df[col]\/ base_val * 100\n        index_range = max(out_df[col]) - min(out_df[col])\n        min_val = min(out_df[col])\n        max_val = max(out_df[col])\n        out_df[\"Earnings\"] = out_df[\"Earnings\"].astype(chart_order)\n        out_df.sort_values(\"Earnings\", inplace = True)\n        row_list = [out_df, base_val, index_range, min_val, max_val, col]\n        out_list.append(row_list)\n    \n    return out_list\n\n\n# setup fonts\nheadfont = {'fontname':'Lato'}\nsubphfont = {'fontname':'Liberation Serif'}\naxisfont = {'fontname':'Liberation Sans Narrow'}\n\n# Create Chart Functions\ndef create_plot(input_list, subptitle_inp):\n    \n    N = len(input_list)\n    plt.close()\n    #set style\n    sns.set_style(\"white\")\n    fig = plt.figure(constrained_layout=True, figsize = (12 ,(2.5 * N)))\n    fig.set_dpi(100)\n    fig.set_constrained_layout_pads(hspace=0.2, wspace=0.1)\n    gs = fig.add_gridspec(nrows = N, ncols = 3)\n    \n    fig.suptitle(subptitle_inp, fontsize=20, x = 0.0, ha = \"left\", y = 1.04, va = \"top\", **headfont, fontweight = \"bold\")\n    fig.text(x = 0.85, y = 1.03, s = \"Incidence\", fontsize=20, fontweight = \"medium\", **headfont)\n\n    # create the axes\n    axs = []\n    paxs = []\n    \n    for i in range(N):\n        # Set plot title\n        plot_title = input_list[i][-1]\n        # Get plot data\n        input_df = input_list[i][0]\n        # Set limits\n        min_idx = int(input_list[i][3] \/ 10) * 10\n        max_idx = int(input_list[i][4] \/ 10) * 10 + 30\n        ylim_min = min(60, min_idx)\n        ylim_max = max(140, max_idx)\n        # Set colors\n        idx_range = input_list[i][2]\n        max_earn_idx = float(input_list[i][0].loc[input_list[i][0][\"Earnings\"] == \"150k and above\"].iloc[:,1])\n        min_earn_idx = float(input_list[i][0].loc[input_list[i][0][\"Earnings\"] == \"Less than 10k\"].iloc[:,1])\n        if idx_range < 30:\n            lcolor = \"royalblue\"\n            fill_color = \"aqua\"\n        elif ((max_earn_idx < 100 and min_earn_idx < 100) or (max_earn_idx > 100 and min_earn_idx > 100)):\n            lcolor = \"royalblue\"\n            fill_color = \"aqua\"\n        elif max_earn_idx < 100:\n            lcolor = \"red\"\n            fill_color = \"tomato\"\n        else:\n            lcolor = \"forestgreen\"\n            fill_color = \"lime\"\n    \n        \n        axs.append(fig.add_subplot(gs[i, :-1]))\n        axs[i].set_title(plot_title, fontsize=16, x = 0.05, y = 0.9, ha = \"left\", **subphfont, fontweight = \"medium\")\n        x = np.arange(5)\n        y = input_df.iloc[:,-1]\n        plt.xticks(np.arange(5), chart_order_list, ha = \"center\", fontsize = 12, **axisfont)\n        plt.grid(color = \"gray\", linewidth = 1.5, linestyle = \"-\", axis = \"y\", alpha = 0.1)\n        plt.yticks(fontsize = 12, **axisfont)\n        axs[i].margins(x = 0.05, y = 0)\n        axs[i].plot(x, y, color=lcolor, lw=2)\n        plt.ylim(bottom = ylim_min, top = ylim_max)\n        axs[i].fill_between(x,100, y, color = fill_color, alpha = 0.6)\n        \n#         if i != N-1:\n#             plt.setp(axs[i].get_xticklabels(), visible=False)\n\n        # Add pie charts\n        paxs.append(fig.add_subplot(gs[i,-1]))\n        col_pal = plt.cm.cool\n        paxs[i].axis('equal')\n        pie_val = input_list[i][1]\n        group_size = [pie_val,1-pie_val] \n        mypie, _ = paxs[i].pie(group_size, radius=1.2, colors=[col_pal(pie_val), \"white\"], startangle = 90)\n        plt.setp( mypie, width=0.8, edgecolor='white')\n        paxs[i].text(x = 0., y = 0.8, s = \"{:.0%}\".format(pie_val), fontsize=18, **axisfont)\n\n    sns.despine()\n    plt.show();\n","9dbc605b":"# Company Size\nkaggle_2020[\"Company Size\"].value_counts(dropna = False)\n\ncompany_size_order = CategoricalDtype(\n    [\"0-49 employees\", \"50-249 employees\", \"250-999 employees\", \"1000-9,999 employees\", \"10,000 or more employees\"], \n    ordered=True\n)\n\ncompany_size_list = [\"0-49 employees\", \"50-249 employees\", \"250-999 employees\", \"1000-9,999 employees\", \"10,000 or more employees\"] \n\nkaggle_2020[\"Company Size\"] = kaggle_2020[\"Company Size\"].astype(company_size_order)\nkaggle_2020.sort_values(\"Company Size\", inplace = True)\ncsize_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Company Size\"]], \"Earnings\", \"Company Size\")\ncreate_plot(csize_list, \"Company Size\")","cf3dd4cc":"# Data Science Team Size\n\nkaggle_2020[\"Data Science Team Size\"] = kaggle_2020[\"Data Science Team Size\"].apply(lambda x: \"10-19\" if x in [\"10-14\", \"15-19\"] else x)\n\nds_team_size_order = CategoricalDtype(\n    [\"0\", \"1-2\", \"3-4\", \"5-9\", \"10-19\", \"20+\"], \n    ordered=True\n)\n\nds_team_list = [\"0\", \"1-2\", \"3-4\", \"5-9\", \"10-19\", \"20+\"]\n\nkaggle_2020[\"Data Science Team Size\"] = kaggle_2020[\"Data Science Team Size\"].astype(ds_team_size_order)\nkaggle_2020.sort_values(\"Data Science Team Size\", inplace = True)\nds_team_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Data Science Team Size\"]], \"Earnings\", \"Data Science Team Size\")\ncreate_plot(ds_team_list, \"Data Science Team Size\")","c941cf40":"# ML Usage in Company\n\nml_usage_order = CategoricalDtype([\"No (we do not use ML methods)\", \n                                  \"We use ML methods for generating insights (but do not put working models into production)\",\n                                  \"We are exploring ML methods (and may one day put a model into production)\",\n                                  \"We recently started using ML methods (i.e., models in production for less than 2 years)\",\n                                  \"We have well established ML methods (i.e., models in production for more than 2 years)\",\n                                  \"I do not know\"], ordered=True\n                                 )\n\nml_usage_list = [\"No (we do not use ML methods)\", \n                \"We use ML methods for generating insights (but do not put working models into production)\",\n                \"We are exploring ML methods (and may one day put a model into production)\",\n                \"We recently started using ML methods (i.e., models in production for less than 2 years)\",\n                \"We have well established ML methods (i.e., models in production for more than 2 years)\",\n                \"I do not know\"]\n\nkaggle_2020[\"ML Application in Company\"] = kaggle_2020[\"ML Application in Company\"].astype(ml_usage_order)\nkaggle_2020.sort_values(\"ML Application in Company\", inplace = True)\nml_use_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"ML Application in Company\"]], \"Earnings\", \"ML Application in Company\")\ncreate_plot(ml_use_list, \"ML Usage in Company\")","ca47939c":"# Country\n\ncountry_dict = {\n'India' : 'India',\n'United States of America' : 'USA',\n'Other' : 'Other',\n'Brazil' : 'Latin America',\n'Japan' : 'East Asia',\n'Russia' : 'Eastern Europe',\n'United Kingdom of Great Britain and Northern Ireland' : 'UK, Canada and Australia',\n'Nigeria' : 'Africa',\n'China' : 'East Asia',\n'Germany' : 'Western Europe',\n'Turkey' : 'Eastern Europe',\n'Spain' : 'Western Europe',\n'France' : 'Western Europe',\n'Canada' : 'UK, Canada and Australia',\n'Indonesia' : 'South East Asia',\n'Pakistan' : 'South Asia',\n'Italy' : 'Western Europe',\n'Taiwan' : 'East Asia',\n'Australia' : 'UK, Canada and Australia',\n'Mexico' : 'Latin America',\n'South Korea' : 'East Asia',\n'Egypt' : 'Africa',\n'Colombia' : 'Latin America',\n'Ukraine' : 'Eastern Europe',\n'Iran, Islamic Republic of...' : 'West Asia',\n'Kenya' : 'Africa',\n'Netherlands' : 'Western Europe',\n'Singapore' : 'South East Asia',\n'Poland' : 'Eastern Europe',\n'Viet Nam' : 'South East Asia',\n'Bangladesh' : 'South Asia',\n'South Africa' : 'Africa',\n'Argentina' : 'Latin America',\n'Morocco' : 'Africa',\n'Malaysia' : 'South East Asia',\n'Thailand' : 'South East Asia',\n'Portugal' : 'Western Europe',\n'Greece' : 'Eastern Europe',\n'Philippines' : 'South East Asia',\n'Tunisia' : 'Africa',\n'Israel' : 'West Asia',\n'Peru' : 'Latin America',\n'Chile' : 'Latin America',\n'Sweden' : 'Western Europe',\n'Saudi Arabia' : 'West Asia',\n'Republic of Korea' : 'East Asia',\n'Sri Lanka' : 'South Asia',\n'Switzerland' : 'Western Europe',\n'Nepal' : 'South Asia',\n'Romania' : 'Eastern Europe',\n'Belgium' : 'Western Europe',\n'United Arab Emirates' : 'West Asia',\n'Belarus' : 'Eastern Europe',\n'Ireland' : 'Western Europe',\n'Ghana' : 'Africa'\n}\nkaggle_2020[\"Region\"] = kaggle_2020[\"Country\"].replace(country_dict)\ncountry_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Region\"]], \"Earnings\", \"Region\")\ncountry_list.sort(key=lambda x: x[1], reverse = True)\ncreate_plot(country_list, \"Country\")","65d10450":"# Role at work\n\nwork_role = ['Analyze and understand data to influence product or business decisions',\n'Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data',\n'Build prototypes to explore applying machine learning to new areas',\n'Build and\/or run a machine learning service that operationally improves my product or workflows',\n'Experimentation and iteration to improve existing ML models',\n'Do research that advances the state of the art of machine learning',\n'None of these activities are an important part of my role at work',\n'Work Activity Other'\n]\n\nwork_role_out = ['Analyze data',\n'Build and\/or run the data infrastructure',\n'Build prototypes',\n'Build and\/or run a ML service',\n'Improve existing ML models',\n'Research',\n'None of these',\n'Work Activity Other'\n]\n\nwork_role_list = []\ni = 0\nfor list_val in work_role:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", work_role_out[i])\n    if ct_out[1] >= 0.1:\n        work_role_list.append(ct_out)\n    i +=1\n\n# sort output list\nwork_role_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(work_role_list, \"Major Role at Work\")","fa2c8f7d":"# Job Title\n\nkaggle_2020[\"Job Title\"].fillna(\"Missing\", inplace = True)\nkaggle_2020[\"Job Title\"] = kaggle_2020[\"Job Title\"].apply(lambda x: \"Unemployed, Missing and Others\" if x in [\"Currently not employed\", \"Missing\", \"Other\"] else x)\njob_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Job Title\"]], \"Earnings\", \"Job Title\")\njob_list.sort(key=lambda x: x[1], reverse = True)\ncreate_plot(job_list, \"Job Title\")","2dd80204":"# ML Experience\n\nkaggle_2020[\"ML Experience\"].fillna(\"Missing\", inplace = True)\nkaggle_2020[\"ML Experience\"] = kaggle_2020[\"ML Experience\"].apply(lambda x: \"None or Missing\" if x in [\"Missing\", \"I do not use machine learning methods\"] else x)\nkaggle_2020[\"ML Experience\"] = kaggle_2020[\"ML Experience\"].apply(lambda x: \"More than 5 years\" if x in [\"5-10 years\", \"10-20 years\", \"20 or more years\"] else x)\n\nml_exp_order = CategoricalDtype(\n    [\"None or Missing\", \"Under 1 year\", \"1-2 years\",\"2-3 years\", \"3-4 years\", \"4-5 years\", \"More than 5 years\"], \n    ordered=True\n)\nml_exp_list = [\"None or Missing\", \"Under 1 year\", \"1-2 years\",\"2-3 years\", \"3-4 years\", \"4-5 years\", \"More than 5 years\"]\n\nkaggle_2020[\"ML Experience\"] = kaggle_2020[\"ML Experience\"].astype(ml_exp_order)\nkaggle_2020.sort_values(\"ML Experience\", inplace = True)\nml_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"ML Experience\"]], \"Earnings\", \"ML Experience\")\ncreate_plot(ml_list, \"ML Experience\")","ed9e0cb4":"# Programming Skills\n\nmajor_progs = ['Python','R','SQL']\nminor_progs = ['C', 'C++', 'Java', 'Javascript', 'Julia', 'Swift', 'Bash','MATLAB', 'Progamming None', 'Programming Other']\n\nmajor_prog_list = []\nfor list_val in major_progs:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", list_val)\n    if ct_out[1] >= 0.1:\n        major_prog_list.append(ct_out)\n\n# sort output list\nmajor_prog_list.sort(key = lambda x:x[1], reverse = True)\n\nminor_prog_list = []\nfor list_val in minor_progs:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", list_val)\n    if ct_out[1] >= 0.1:\n        minor_prog_list.append(ct_out)\n\n# sort output list\nminor_prog_list.sort(key = lambda x:x[1], reverse = True)\n","eb5519be":"create_plot(major_prog_list, \"Major Programming Skills\")","09a22a39":"# Visualization\n\nvisual_list = ['Matplotlib', 'Seaborn', 'Plotly \/ Plotly Express', 'Ggplot \/ ggplot2', 'Shiny', 'D3 js', 'Altair',\n               'Bokeh', 'Geoplotlib', 'Leaflet \/ Folium', 'Visualization None', 'Visualization Other']\nvisual_out = ['Matplotlib', 'Seaborn', 'Plotly', 'Ggplot', 'Shiny', 'D3 js', 'Altair',\n              'Bokeh', 'Geoplotlib', 'Leaflet \/ Folium', 'Visualization None', 'Visualization Other']\n\nvisual_usage_list = []\ni = 0\nfor list_val in visual_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", visual_out[i])\n    if ct_out[1] >= 0.1:\n        visual_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nvisual_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(visual_usage_list, \"Visualization Tools\")\n\n","13dcdf6e":"# Big Data Products\n\nbig_data_list = ['MySQL', 'PostgresSQL', 'SQLite', 'Oracle Database', 'MongoDB', 'Snowflake', 'IBM Db2', 'Microsoft SQL Server',\n'Microsoft Access', 'Microsoft Azure Data Lake Storage', 'Amazon Redshift', 'Amazon Athena', 'Amazon DynamoDB',\n'Google Cloud BigQuery', 'Google Cloud SQL', 'Google Cloud Firestore', 'DW None', 'DW Other']\nbig_data_out = ['MySQL', 'PostgresSQL', 'SQLite', 'Oracle Database', 'MongoDB', 'Snowflake', 'IBM Db2', 'Microsoft SQL Server',\n'Microsoft Access', 'Microsoft Azure Data Lake Storage', 'Amazon Redshift', 'Amazon Athena', 'Amazon DynamoDB',\n'Google Cloud BigQuery', 'Google Cloud SQL', 'Google Cloud Firestore', 'DW None', 'DW Other']\n\n\nbig_data_usage_list = []\ni = 0\nfor list_val in big_data_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", big_data_out[i])\n    if ct_out[1] >= 0.1:\n        big_data_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nbig_data_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(big_data_usage_list, \"Big Data Products\")","0e746a31":"# BI Tools\n\nbi_tools_list = ['Amazon QuickSight', 'Microsoft Power BI', 'Google Data Studio', 'Looker', 'Tableau', 'Salesforce',\n'Einstein Analytics', 'Qlik', 'Domo', 'TIBCO Spotfire', 'Alteryx', 'Sisense', 'SAP Analytics Cloud',\n'BI None', 'BI Other']\nbi_tools_out = ['Amazon QuickSight', 'Microsoft Power BI', 'Google Data Studio', 'Looker', 'Tableau', 'Salesforce',\n'Einstein Analytics', 'Qlik', 'Domo', 'TIBCO Spotfire', 'Alteryx', 'Sisense', 'SAP Analytics Cloud',\n'BI None', 'BI Other'\n]\n\n\nbi_tools_usage_list = []\ni = 0\nfor list_val in bi_tools_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", bi_tools_out[i])\n    if ct_out[1] >= 0.1:\n        bi_tools_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nbi_tools_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(bi_tools_usage_list, \"BI Tools\")","c58ef180":"# Primary Analysis Tool\n\nkaggle_2020[\"Primary Analysis Tool\"].fillna(\"Missing\", inplace = True)\nkaggle_2020[\"Primary Analysis Tool\"] = kaggle_2020[\"Primary Analysis Tool\"].apply(lambda x: \"Missing and Others\" if x in [\"Missing\", \"Other\"] else x)\nanalysis_tool_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Primary Analysis Tool\"]], \"Earnings\", \"Primary Analysis Tool\")\nanalysis_tool_list.sort(key=lambda x: x[1], reverse = True)\ncreate_plot(analysis_tool_list, \"Primary Analysis Tool\")","fc121d19":"# ML Frameworks\n\nml_frame = ['Scikit-learn', 'TensorFlow', 'Keras', 'PyTorch', 'Fast.ai', 'MXNet', 'Xgboost', 'LightGBM', 'CatBoost', 'Prophet',\n'H2O 3', 'Caret', 'Tidymodels', 'JAX', 'ML None', 'ML Other']\n\nml_frame_out = ['Scikit-learn', 'TensorFlow', 'Keras', 'PyTorch', 'Fast.ai', 'MXNet', 'Xgboost', 'LightGBM', 'CatBoost', 'Prophet',\n'H2O 3', 'Caret', 'Tidymodels', 'JAX', 'ML None', 'ML Other'\n]\n\nml_frame_list = []\ni = 0\nfor list_val in ml_frame:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", ml_frame_out[i])\n    if ct_out[1] >= 0.1:\n        ml_frame_list.append(ct_out)\n    i +=1\n\n# sort output list\nml_frame_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(ml_frame_list, \"ML Frameworks\")","297574dd":"# ML Algorithms\n\nml_algo = ['Linear or Logistic Regression','Decision Trees or Random Forests', 'Gradient Boosting Machines (xgboost, lightgbm, etc)',\n'Bayesian Approaches', 'Evolutionary Approaches', 'Dense Neural Networks (MLPs, etc)', 'Convolutional Neural Networks',\n'Generative Adversarial Networks','Recurrent Neural Networks','Transformer Networks (BERT, gpt-3, etc)',\n'ML Algorithms None','ML Algo Other']\n\nml_algo_out = ['Linear \/ Logistic Regression','Decision Trees \/ Random Forests', 'Gradient Boosting Machines',\n'Bayesian Approaches', 'Evolutionary Approaches', 'Dense Neural Networks \/ MLPs', 'Convolutional Neural Networks',\n'Generative Adversarial Networks','Recurrent Neural Networks','Transformer Networks','ML Algorithms None','ML Algo Other'\n]\n\nml_algo_list = []\ni = 0\nfor list_val in ml_algo:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", ml_algo_out[i])\n    if ct_out[1] >= 0.1:\n        ml_algo_list.append(ct_out)\n    i +=1\n\n# sort output list\nml_algo_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(ml_algo_list, \"ML Algorithms\")","3196a9f1":"# Computer Vision Methods\n\ncv_methods = ['General purpose image\/video tools (PIL, cv2, skimage, etc)', 'Image segmentation methods (U-Net, Mask R-CNN, etc)',\n'Object detection methods (YOLOv3, RetinaNet, etc)', 'Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)',\n'Generative Networks (GAN, VAE, etc)', 'CV None', 'CV Other',\n]\n\ncv_methods_out = ['General purpose image\/video tools (PIL, cv2, skimage, etc)', 'Image segmentation methods (U-Net, Mask R-CNN, etc)',\n'Object detection methods (YOLOv3, RetinaNet, etc)', 'Other networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)',\n'Generative Networks (GAN, VAE, etc)', 'CV None', 'CV Other']\n\n\ncv_methods_list = []\ni = 0\nfor list_val in cv_methods:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", cv_methods_out[i])\n    if ct_out[1] >= 0.1:\n        cv_methods_list.append(ct_out)\n    i +=1\n\n# sort output list\ncv_methods_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(cv_methods_list, \"Computer Vision Methods\")\n","9e8e4296":"# NLP Tools\n\nnlp_methods = ['Word embeddings\/vectors (GLoVe, fastText, word2vec)', 'Encoder-decorder models (seq2seq, vanilla transformers)',\n               'Contextualized embeddings (ELMo, CoVe)', 'Transformer language models (GPT-3, BERT, XLnet, etc)',\n               'NLP None', 'NLP Other']\n\nnlp_methods_out = ['Word vectors  - GLoVe, fastText, word2vec',\n'Encoder-decorder models - seq2seq, vanilla transformers',\n'Contextualized embeddings - ELMo, CoVe',\n'Transformer language models -  (GPT-3, BERT, XLnet, etc',\n'NLP None', 'NLP Other']\n\n\nnlp_methods_list = []\ni = 0\nfor list_val in nlp_methods:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", nlp_methods_out[i])\n    if ct_out[1] >= 0.1:\n        nlp_methods_list.append(ct_out)\n    i +=1\n\n# sort output list\nnlp_methods_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(nlp_methods_list, \"NLP Methods\")\n\n","737f73e8":"# Edication\nedu_dict = {\n    \"Master\u2019s degree\" : \"Master\u2019s degree\", \"Bachelor\u2019s degree\" : \"Bachelor\u2019s degree\",\n    \"Doctoral degree\" : \"Doctoral degree\", \n    \"Professional degree\" : \"Professional degree\",\n    \"Some college\/university study without earning a bachelor\u2019s degree\" : \"Unfinished College\",\n    \"Others\": \"Others\", \"I prefer not to answer\" : \"Others\", \n    \"No formal education past high school\"  : \"Others\"\n}\n\nkaggle_2020[\"Education\"] = kaggle_2020[\"Education Level\"].replace(edu_dict)\neducation_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Education\"]], \"Earnings\", \"Education\")\neducation_list.sort(key=lambda x: x[1], reverse = True)\ncreate_plot(education_list, \"Education Level\")","c0fe1057":"# ML Spend\n\nkaggle_2020[\"ML Spend\"].fillna(\"Missing\", inplace = True)\nkaggle_2020[\"ML Spend\"] = kaggle_2020[\"ML Spend\"].apply(lambda x: \"$0 or Missing\" if x in [\"Missing\", \"$0 ($USD)\"] else x)\nkaggle_2020[\"ML Spend\"] = kaggle_2020[\"ML Spend\"].apply(lambda x: \"$100,000 or more\" if x in [\"$100,000 or more ($USD)\"] else x)\n\n\nml_spend_order = CategoricalDtype(\n    [\"$0 or Missing\", \"$1-$99\", \"$100-$999\", \"$1000-$9,999\", \"$10,000-$99,999\", \"$100,000 or more\"], \n    ordered=True\n)\n\nml_spend_list = [\"$0 or Missing\", \"$1-$99\", \"$100-$999\", \"$1000-$9,999\", \"$10,000-$99,999\", \"$100,000 or more\"]\n\nkaggle_2020[\"ML Spend Order\"] = kaggle_2020[\"ML Spend\"].astype(ml_spend_order)\nkaggle_2020.sort_values(\"ML Spend Order\", inplace = True)\nml_spend_order_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"ML Spend Order\"]], \"Earnings\", \"ML Spend Order\")\ncreate_plot(ml_spend_order_list, \"ML Spend\")\n","6c202c14":"# ML Learning Platform\n\nml_learn_list = ['Coursera', 'edX', 'Kaggle Learn Courses', 'DataCamp', 'Fast.ai Learning', 'Udacity', 'Udemy',\n'LinkedIn Learning','Cloud-certification programs (direct from AWS, Azure, GCP, or similar)',\n'University Courses (resulting in a university degree)','Learn ML None',\n'Learn ML Other']\nml_learn_out = ['Coursera', 'edX', 'Kaggle Learn Courses', 'DataCamp', 'Fast.ai Learning', 'Udacity', 'Udemy',\n'LinkedIn Learning','Cloud-certification - AWS, Azure, GCP',\n'University Courses','None',\n'Others',\n]\n\nml_learn_usage_list = []\ni = 0\nfor list_val in ml_learn_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", ml_learn_out[i])\n    if ct_out[1] >= 0.1:\n        ml_learn_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nml_learn_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(ml_learn_usage_list, \"Learning Platform\")","268ec3f1":"# Knowledge Sharing\n\nsharing_list = ['Plotly Dash', 'Streamlit', 'NBViewer', 'GitHub', 'Personal blog', 'Kaggle', 'Colab',\n'Shiny Deploy', 'I do not share my work publicly', 'Deploy Other']\nsharing_out = ['Plotly Dash', 'Streamlit', 'NBViewer', 'GitHub', 'Personal blog', 'Kaggle', 'Colab',\n'Shiny Deploy', 'I do not share my work publicly', 'Other'\n]\n\nsharing_usage_list = []\ni = 0\nfor list_val in sharing_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", sharing_out[i])\n    if ct_out[1] >= 0.1:\n        sharing_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nsharing_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(sharing_usage_list, \"Knowledge Sharing\")","405de50c":"# Data Science Media\n\nds_media_list = ['Twitter (data science influencers)', \"Email newsletters (Data Elixir, O'Reilly Data & AI, etc)\",\n'Reddit (r\/machinelearning, etc)', 'Kaggle (notebooks, forums, etc)', 'Course Forums (forums.fast.ai, Coursera forums, etc)',\n'YouTube (Kaggle YouTube, Cloud AI Adventures, etc)', 'Podcasts (Chai Time Data Science, O\u2019Reilly Data Show, etc)',\n'Blogs (Towards Data Science, Analytics Vidhya, etc)', 'Journal Publications (peer-reviewed journals, conference proceedings, etc)',\n'Slack Communities (ods.ai, kagglenoobs, etc)', 'Media None', 'Media Other',\n]\n\n\nds_media_out = ['Twitter', \"Email newsletters\", 'Reddit', 'Kaggle', 'Course Forums',\n'YouTube', 'Podcasts', 'Blogs', 'Journal Publications', 'Slack', 'Media None', 'Media Other'\n]\n\nds_media_usage_list = []\ni = 0\nfor list_val in ds_media_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", ds_media_out[i])\n    if ct_out[1] >= 0.1:\n        ds_media_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nds_media_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(ds_media_usage_list, \"Data Science Media\")","083b5a51":"# Methodology\n\nBefore we start, let us set up a few jump into the deep sea, let us set up a few guiding principles. \n\n## 1. Earnings: \n\nWhile money cannot buy us everything, we will use the Warren Buffet philosophy to use money to keep a score. The Kaggle Survey does not track job satisfaction, work hours and bad bosses. So we will take the salaries as a proxy. This means that we will have to <span style=\"background:red; font-weight:medium; color:white\">leave out roughly half the respondents<\/span> who did not mention their salaries. \n\nThe median US income is about USD 70,000 per year for a college graduate. Keeping this in mind, we have grouped the earnings into five categories\n\n* Up to USD 10,000 (very low)\n* USD 10,000 to USD 50,000 (lower than median)\n* USD 50,000 to USD 80,000 (median)\n* USD 80,000 to USD 150,000 (more than the median)\n* USD 150,000 and more (significantly more than the median)\n\n\n## 2. Importance of attributes:\n\nYou would have heard that if you need to learn data science, you need to learn Python. \n* How important is learning Python? \n* How much of a competitive edge would it give us? \n\nWe find the number of Data Scientists using Python **(81%)**\n\nAnd if plot the usage across various Earnings brackets, we find that the number is steady. \n\n![img%2002.png](attachment:img%2002.png)\n\n\nThis tells us that you have to start with Python if you are entering Data Science now. \n\n### To quantify this, we use two measures.\n\n### Incidence: \nThe fraction of people across all earnings buckets having the skill. \n\n![img%2001.png](attachment:img%2001.png)\n\n### Index: \nIf we keep the incidence as the base, we can calculate the importance index. The relative importance of the skill across the earning buckets. The graph then looks something like this. \n\n![img%2003.png](attachment:img%2003.png)\n\nNow, if we read the two together, we know that \n\n**You most probably HAVE to learn Python to do well in Data Science. But just knowing Python will not give you a competitive edge. **\n\nOn the other hand an index like this gives you an attribute with a competitive edge in earnings. \n\n![img%2004.png](attachment:img%2004.png)\n\nWheras, an index like this perhaps indicates that the attribute is on the wane in the world of data science and may not help you in your career progression in the future. \n\n![img%2005.png](attachment:img%2005.png)\n\n\n**Note:** <span style=\"background:red; font-weight:medium; color:white\">To reduce the effects of a small base, we have considered only those attributes that have at least 10% incidence for binary questions and at least 5% incidence for MCQ questions.<\/span>\n\nLet's do this.","da5691d2":"Just being in the right place won\u2019t help. You also need the right role. \n\nLet us explore how your designation, the work that you do and experience affect your earnings. \n","10d2852f":"* Working in a company that has a stable ML practice can be valuable, especially in the early stages of your career. \n* It will provide you with great mentors who will guide you in your journey. \n ","4391fb15":"In this part we look at the Algorithms that set you apart. \n","5d1a77bd":"* Almost half the Data Sciene world has a master's degree\n* With Indians constituting around a quarter of the respondents, Bachelor's Degree does not pay that much. \n* Doctoral programs pay off. \n\n### *If you want to succeed in Data Science, study*","1a29fc27":"# Parting words\n\nThe Kaggle Survey is a wonderful source of information about the Data Science \/ Machine Learning Community. There were some continuing trends as well as new things that I learned. I hope my analysis does justice to the data and hope you liked reading it as much as I did creating it. \n\nThanks for reading and hope you have a safe and successful 2021!!\n","e7dad61c":"# ML Skills","c1e9a4eb":"* The traditional SQL daabases (mySQL, MS-SQL and PostgreSQL still dominate. \n* MongoDB and noSQL databases are taking time to rise up the usage charts. ","42de6e36":"# Learn and Earn in 2021\n\n[](http:\/\/)","92311875":"# Where to work?","d84775cd":"\nAs with any other profession, we cannot exist in silos. We may have impressions of Data Scientists being super nerdy folks working in isolation tapping away at the keyboard. But in reality, Data Science is a collaborative effort. And you need the right environment to grow and prosper. So if you are planning to switch jobs in 2021, here are few factors that you mind want to consider. ","780ed941":"* Costly courses pays off.\n* The free courses might be popular, but they don't seem to add value\n* This data is spend over last five years. And also includes company sponsored learning, so the high spends are not that surprising. \n\n### *To make money, one needs to spend money*","3510deef":"* The media consumption trends follow the knowledge sharing trends.\n* As Data Scientists get busy, they prefer one way communication like newsletters and articles versus community learning.\n","d4c23568":"* As with neural networks, CV algorithms too are being adopted quickly. ","8f111813":"* America remains the land of opportunity. Followed by UK, Canada or Australia. \n* Most other places including Western Europe drop off in in the higher earnings bucket.\n* This might be explained by a lack of large companies involved in the field in Western Europe\n* India is super competitive. So very few top paying jobs are available there. ","d1925ee6":"# Toolbox\n\n* What are the skills needed to succeed in the field?\n* What is the basic requirement? \n* And what are the game changing tools?","a73e4426":"* Data Analysis shows promise but cannot compete with the salaries at the higher end. \n* Data Analyst positions are a stepping stone to get into Data Science. \n* Research too does not pay too much. Data Science isn't and exact science after all!!\n* To earn money in Data Science you need to get your hands dirty. \n\n### *The real money is in execution.*","84d1fad9":"\n\nTIME magazine officially called[ 2020, the worst year ever](https:\/\/time.com\/5917394\/2020-in-review\/). The Covid pandemic ravaged economies around the world. At the same time bringing economic destruction on a scale hitherto unknown. \n\nConcurrently, companies in the tech space boomed. The combined size of `Google, Amazon, Microsoft, Apple and Facebook` is now more than those of the remaining 495 in the S&P 500. There is an explosion in the interest in education, upskiiling and re-skilling across the world. Data Science was called the sexiest job of the 21st century earlier. After the pandemic, it has gotten irresistible. \n\nA cursory Google search will throw up numerous data science courses. Each promising us more riches than most televangelists. \n\n<img src=\"https:\/\/i.ytimg.com\/vi\/k3h-KfsD_iw\/maxresdefault.jpg\" width=\"600\">\n\n\n<div><p> <\/div>\n  \n\n\n","546ee14a":"* The BI tools follow the Analyst roles. Around a third of the respondents and more at the higher end have no BI experience. \n* Tableau and Power BI are the tools of choice of you want Data Analyst roles. ","f65d6746":"* As with the size of the company, if you get a chance to work in a larger DS team, do join. \n* You will get access to not only a wide variety of team members, but will also be sharing responsibilities and can therefore get more opportunities to network and learn. \n","ae5e408c":"* Matplotlib and Seaborn are becoming too commonplace.\n* Ploltly is clearly the next big visualization tool in the Python universe to pick up. \n* ggplot follows the R trend. If you are from the R world, learn ggplot. ","1f10c1d4":"\nThe New Year is almost upon us and if you have not already enrolled for one of the various data science programs, it is very likely that you will feel pressured to change your life in 2021 by enrolling into one. Let us explore the Kaggle ML survey from 2020 and find out what are the attributes that pay in the Data Science \/ Machine Learning world. \n\nMy hope is that this helps set gives a realistic expectations on what can be achieved and where to concentrate your efforts in order to achieve those targets. We will try to answer - \n\n* Where to work?\n* What to do?\n* What your toolbox should look like?\n* Specific ML skills to acquire?\n* Where to acquire those skills?\n* How to learn?","02bf24d4":"* The platform choice reflects the spends. \n* Free and low priced courses like Kaggle, Udemy are popular at the lower end. \n* Mid priced courses like DataCamp and LinkedIn learning are popular in the mid-range. \n* Coursera is the most poplular of the large MOOCs with almost half of all respondents using it. \n* Almost a quarter of the respondents went to the University. ","8cc79f3f":"* Regression and Forests are everywhere. \n* It is surprising to see Neural Networks being used so widely. Shows how quickly technologies become commonplace. ","6236ae61":"* Designations follow a similar trend to Work Role. \n* Not all ML \/ DS roles pay equally. \n* Product Managment roles as expected, pay better compared to research and engineering roles\n* I am curious to find out more about the Miscellaneous Category. \n\n### *Designations might be deceptive. Follow close attention to the actual work done*","8e8bf1ad":"* There is a direct relationship with company size (by number of employees) and growth potential. \n* A large company will pay you more. \n* It can arguably afford the data science \/ machine learning infrastructure. \n* It provides opportunity to learn from a varied team. \n \n\n### *Move to a large company to grow.*\n\n","6b495bd2":"* Data science is truly expanding. Even hitherto indispensable tools like Excel are declining. \n* SAS and SPSS still find value at the top end. It shows the prevalance of SAS in large companies. ","a20165fa":"# What to do?","9c488e27":"* GitHub is the platform of choice. \n* A fifth of the respondents don't share. And their proportion increases with earnings. \n* This may not be selfishness, they might be busy with the additional work that comes with additional pay and responsibilites. \n* Unfortunately both Kaggle and Colab suffer because of this. \n","9e28e8f7":"# Acquiring Skills\n\nNow that we know what we need to pick up, where do we learn them from?","a6c5e341":"* There is a clear relationship between experience and earnings. \n* As with every other profession, it takes time to succeed. \n\n\n### *It is a marathon, not a sprint*","178549d8":"* Natural Language Processing is expected to be the next game changer. \n* If you are comfortable with regression and classification, NLP seems to be the next step. ","3c1552b8":"# How to keep Learning\n\nData Science is fluid, so the professionals need to keep themselves continually updated. \n\nHere we look at their sharing and learning methods. ","cb66bf9a":"* As we had discussed earlier, everyone and their cat is learning Python. So you too will have to.\n* R is not a bad second language to learn. \n* SQL remains the every green tool. With greater emphasis on execution, you can't go wrong with SQL. ","5838fb90":"* Like Matplotlib for visualization, Scikit Learn is the go-to  ML library. \n* Scikit learn is your first step to Data Science. So get comfortable with it. \n* TF and Keras too are finding wide usage. \n* PyTorch is the new shiny toy in the ML world. So you might see a lot of resumes with PyTorch proficiency next year. "}}