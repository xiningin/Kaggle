{"cell_type":{"c4f2cc35":"code","3c5090f8":"code","aea9bb64":"code","090ad809":"code","20216b12":"code","1f4f3577":"code","2b113039":"code","9bf6183a":"code","eceed14c":"code","dfd9453d":"code","2af42712":"code","bd84af84":"code","253e90b9":"code","e74b93dd":"code","6e8f6a59":"code","99677fc0":"code","842cb557":"code","4e614d9a":"code","015900d2":"code","aa575a3c":"code","535a3fee":"code","691a5619":"markdown","e3a67a22":"markdown","c22b7e7f":"markdown","2ba64415":"markdown","aa9d8f7e":"markdown","8e1e12dc":"markdown","86a71e0a":"markdown","d192801a":"markdown"},"source":{"c4f2cc35":"import numpy as np\nimport pandas as pd\nimport lightgbm","3c5090f8":"data=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","aea9bb64":"data.columns","090ad809":"data.head","20216b12":"y=data.Class\ndata=data.drop('Class',axis=1)\na=data.columns","1f4f3577":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndata['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n\ndata.drop(['Time','Amount'], axis=1, inplace=True)","2b113039":"scaled_amount = data['scaled_amount']\nscaled_time = data['scaled_time']\n\ndata.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndata.insert(0, 'scaled_amount', scaled_amount)\ndata.insert(1, 'scaled_time', scaled_time)\n\ndata.head()","9bf6183a":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x1,y1,test_size=0.2, random_state=42)","eceed14c":"from imblearn.over_sampling import SMOTE\nsmt=SMOTE(random_state=42)\nx1,y1=smt.fit_resample(data,y)","dfd9453d":"df=pd.DataFrame(x1)","2af42712":"df","bd84af84":"df.columns=a","253e90b9":"df","e74b93dd":"df['Class'] = y1","6e8f6a59":"df","99677fc0":"categorical_features = [c for c, col in enumerate(data.columns) if 'cat' in col]\ntrain_data = lightgbm.Dataset(x_train,label=y_train,categorical_feature=categorical_features)\ntest_data = lightgbm.Dataset(x_test,label=y_test)","842cb557":"parameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}","4e614d9a":"model = lightgbm.train(parameters,\n                       train_data,\n                       valid_sets=test_data,\n                       num_boost_round=5000,\n                       early_stopping_rounds=100)","015900d2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\nmodel=LogisticRegression()\nmodel.fit(x_train,y_train)\npred=model.predict(x_test)\ntarget_names=['class 0','class 1']\nprint(classification_report(y_test,pred,target_names=target_names))\n","aa575a3c":"from sklearn import datasets\nimport xgboost as xgb\nD_train = xgb.DMatrix(x_train, label=y_train)\nD_test = xgb.DMatrix(x_test, label=y_test)\nparam = {\n    'eta': 0.3, \n    'max_depth': 3,  \n    'objective': 'multi:softprob',  \n    'num_class': 3} \n\nsteps = 20\n\nmodel = xgb.train(param, D_train, steps)\n\npreds2 = model.predict(D_test)\nbest_preds = np.asarray([np.argmax(line) for line in preds2])\n\ntarget_names=['class 0','class 1']\nprint(classification_report(y_test,best_preds,target_names=target_names))","535a3fee":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import *\nmodel=RandomForestClassifier()\nmodel.fit(x_train,y_train)\npred=model.predict(x_test)\ntarget_names=['class 0','class 1']\nprint(classification_report(y_test,pred,target_names=target_names))","691a5619":"Reading the information from the CSV and storing it into a dataframe named data.","e3a67a22":"\nLogistic regression is named for the function used at the core of the method, the logistic function.\n\nThe logistic function, also called the sigmoid function was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It\u2019s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n\n1 \/ (1 + e^-value)\n\nWhere e is the base of the natural logarithms (Euler\u2019s number or the EXP() function in your spreadsheet) and value is the actual numerical value that you want to transform. Below is a plot of the numbers between -5 and 5 transformed into the range 0 and 1 using the logistic function.\n\n","c22b7e7f":"This code on the whole deals with detection of fraudulent credit card transaction.The main challenges faced in this project is to resolve the dataset imbalances,to extract impportant features by analyzing it through a correlation matrix and to remove outliers if any.\n\nSteps involved:\n1.Pre-processing of data(Scaling)\n2.Solving Dataset Imbalance using SMOTE.\n3.Training it on ML models.\na.LightGBM\nb.Logistic Regression\nc.XGBoost\n","2ba64415":"Scaling needs to be done and it can be inferred when the data is being described all other columns except Amount and Time are in the same range so these 2 needs to be scaled.\n\nThe 2 predominant types of scaling are:\n1.Standard Scaler\n2.Robust Scaler\n\nStandard Scaler:The StandardScaler assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1.\n\nRobust Scaler:The RobustScaler uses a similar method to the Min-Max scaler but it instead uses the interquartile range, rathar than the min-max, so that it is robust to outliers. Therefore it follows the formula:\n\nxi\u2013Q1(x)\/Q3(x)\u2013Q1(x)\nFor each feature.\n\nOf course this means it is using the less of the data for scaling so it\u2019s more suitable for when there are outliers in the data.","aa9d8f7e":"SMOTE stands for Synthetic Minority Oversampling Technique. This is a statistical technique for increasing the number of cases in your dataset in a balanced way. The module works by generating new instances from existing minority cases that you supply as input. This implementation of SMOTE does not change the number of majority cases.\n\nThe new instances are not just copies of existing minority cases; instead, the algorithm takes samples of the feature space for each target class and its nearest neighbors, and generates new examples that combine features of the target case with features of its neighbors. This approach increases the features available to each class and makes the samples more general.\n\nSMOTE takes the entire dataset as an input, but it increases the percentage of only the minority cases. For example, suppose you have an imbalanced dataset where just 1% of the cases have the target value A (the minority class), and 99% of the cases have the value B. To increase the percentage of minority cases to twice the previous percentage, you would enter 200 for SMOTE percentage in the module's properties.","8e1e12dc":"Storing the class labels in a variable called y and the rest of the columns is overriden in the variable data itself.","86a71e0a":"Light GBM is a gradient boosting framework that uses tree based learning algorithm.\nLight GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\nLight GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.","d192801a":"XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples."}}