{"cell_type":{"a5a5dd57":"code","c5720768":"code","232b5db3":"code","61e63b95":"code","d76997da":"code","4a1ef966":"code","5caf6879":"code","ad5dbec9":"code","111b63ed":"code","cb2fad34":"code","ca4e3095":"code","b2cb721a":"code","1b863b15":"code","79e27efe":"code","1024bc2b":"code","2666ad75":"code","be03dde9":"code","695007b9":"code","f23730f2":"code","de3143ca":"code","e059c03f":"code","b0b4d796":"code","3702b9e9":"code","274391bd":"code","529344d6":"code","3a8bd373":"code","4c2b1268":"code","65f8a0cd":"code","8ed6dc95":"code","252cf0c4":"code","28e69f46":"markdown","952cbcdc":"markdown","78849253":"markdown","47ea6437":"markdown","01bd229c":"markdown","1e3fb631":"markdown","3f34e5bd":"markdown","e00fef9f":"markdown","3da07b95":"markdown","ea21ba62":"markdown","09617a63":"markdown","8dba565c":"markdown","a7eabe89":"markdown","2f21265a":"markdown","7bf1d626":"markdown","bd8d2d4d":"markdown","1bb6204a":"markdown","6a40b354":"markdown","7b9b2fef":"markdown","1baf2999":"markdown","58de5a55":"markdown","a53ba1f2":"markdown","857cf6ec":"markdown","9b6a8fb3":"markdown"},"source":{"a5a5dd57":"import os\nimport json\nimport glob\nimport random\nimport collections\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n\nimport pydicom","c5720768":"#import wandb\n#wandb.login()","232b5db3":"train_df = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv\")\ntrain_df","61e63b95":"plt.figure(figsize=(5,5))\nsns.countplot(data=train_df, x=\"MGMT_value\")","d76997da":"dataset = pydicom.filereader.dcmread(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/00000\/FLAIR\/Image-110.dcm\")\nimg = dataset.pixel_array # Dataset.pixel_array returns a numpy.ndarray\n\nfig, ax = plt.subplots()\nax.imshow(img, cmap=\"gray\")\nax.set_axis_off()\nprint('Shape of data: ', img.shape)\nplt.show()","4a1ef966":"filenames = glob.glob('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/*\/*\/*')\nprint(f'Total number of files: {len(filenames)}')","5caf6879":"label_dict = {\n    'FLAIR': [],\n    'T1w': [],\n    'T1wCE': [],\n    'T2w': []\n}\n\nfor filename in tqdm(filenames):\n    scan = filename.split('\/')[-2]\n    if scan==\"FLAIR\":\n        label_dict[\"FLAIR\"].append(filename)\n    elif scan==\"T1w\":\n        label_dict[\"T1w\"].append(filename)\n    elif scan==\"T1wCE\":\n        label_dict[\"T1wCE\"].append(filename)\n    elif scan==\"T2w\":\n        label_dict[\"T2w\"].append(filename)\n        \nprint('Size of FLAIR scan: {}, T1w scan: {}, T1wCE scan: {}, T2W scan: {}'.format(len(label_dict[\"FLAIR\"]),\n                                                                                  len(label_dict[\"T1w\"]),\n                                                                                  len(label_dict[\"T1wCE\"]),\n                                                                                  len(label_dict[\"T2w\"])))","ad5dbec9":"CONFIG = {'IMG_SIZE': 224, \n          'NUM_FRAMES': 14,\n          'competition': 'rsna-miccai-brain', \n          '_wandb_kernel': 'ayut'}","111b63ed":"run = wandb.init(project='brain-tumor-viz', config=CONFIG)\ndata = [['FLAIR', 74248], ['T1w', 77627], ['T1wCE' ,96766], ['T2w', 100000]]\ntable = wandb.Table(data=data, columns = [\"Scan Type\", \"Num Files\"])\nwandb.log({\"my_bar_chart_id\" : wandb.plot.bar(table, \"Scan Type\", \"Num Files\", title=\"Scan Types vs Number of Dicom files\")})","cb2fad34":"train_df.shape","ca4e3095":"# DICOM to PNG dataset (128 GB -> 5.2 GB)\ndef load_dicom(path):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    dara = data - np.min(data) \n    if np.max(data) != 0:\n        data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n\ndef visualize_sample(brats21id, slice_i, mgmt_value, types=(\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\")):\n    plt.figure(figsize=(16, 5))\n    patient_path = os.path.join(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/\", str(brats21id).zfill(5))\n    for i, t in enumerate(types, 1):\n        t_paths = sorted(glob.glob(os.path.join(patient_path, t, \"*\")),\n                        key=lambda x: int(x[:-4].split(\"-\")[-1]))\n        data = load_dicom(t_paths[int(len(t_paths)* slice_i)]) # why?\n        plt.subplot(1, 4, i)\n        plt.imshow(data, cmap=\"gray\")\n        plt.title(f\"{t}\", fontsize=16)\n        plt.axis(\"off\")\n        \n    plt.suptitle(f\"MGMT_value: {mgmt_value}\", fontsize=18)\n    plt.show","b2cb721a":"for i in random.sample(range(train_df.shape[0]), 10):\n    _brats21id = train_df.iloc[i][\"BraTS21ID\"]\n    _mgmt_value = train_df.iloc[i][\"MGMT_value\"]\n    visualize_sample(brats21id = _brats21id, mgmt_value=_mgmt_value, slice_i=0.5)","1b863b15":"package_path =  \"..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master\/\"","79e27efe":"import sys\nimport time\nsys.path.append(package_path)\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data as torch_data\nfrom sklearn import model_selection as sk_model_selection\nfrom torch.nn import functional as torch_functional\nimport efficientnet_pytorch\n\n# choice \n#from sklearn.model_selection import StratifiedKFold\n#from sklearn.model_selection import KFold","1024bc2b":"# maintain Reproducibility\ndef set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True # deterministic algorithm (\u6c7a\u5b9a\u8ad6\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u4f7f\u7528)\n    \nset_seed(42)","2666ad75":"df = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv\")\n\ndf_train, df_valid = sk_model_selection.train_test_split(\n    df, \n    test_size=0.2, \n    random_state=42, \n    stratify=train_df[\"MGMT_value\"],\n)","be03dde9":"class DataRetriever(torch_data.Dataset):\n    def __init__(self, paths, targets):\n        self.paths = paths\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.paths)\n    \n    # arrange later \n    def __getitem__(self, index):\n        _id = self.paths[index]\n        patient_path = f\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{str(_id).zfill(5)}\/\"\n        channels =  []\n        for t in (\"FLAIR\", \"T1w\", \"T1wCE\"):# \"T2w\"\n            t_paths = sorted(glob.glob(os.path.join(patient_path, t, \"*\")),\n                        key=lambda x: int(x[:-4].split(\"-\")[-1]))\n            \n            # start, end = int(len(t_paths) * 0.475), int(len(t_paths) * 0.525)\n            x = len(t_paths)\n            if x < 10:\n                r = range(x)\n            else:\n                d = x \/\/ 10\n                r = range(d, x - d, d)\n            channel = []\n            # for i in range(start, end + 1):\n            for i in r:\n                channel.append(cv2.resize(load_dicom(t_paths[i]), (256, 256)) \/ 255) # why?\n                \n            channel = np.mean(channel, axis=0) # axis=0 by column\n            channels.append(channel)\n            \n        \n        y = torch.tensor(self.targets[index], dtype=torch.float)\n        \n        return {\"X\": torch.tensor(channels).float(), \"y\": y}    ","695007b9":"train_data_retriever = DataRetriever(\n    df_train[\"BraTS21ID\"].values,\n    df_train[\"MGMT_value\"].values\n)\n\nvalid_data_retriever = DataRetriever(\n    df_valid[\"BraTS21ID\"].values,\n    df_valid[\"MGMT_value\"].values\n)","f23730f2":"plt.figure(figsize=(16, 6))\nfor i in range(3):\n    plt.subplot(1, 3, i + 1)\n    plt.imshow(train_data_retriever[13][\"X\"].numpy()[i], cmap=\"gray\")","de3143ca":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = efficientnet_pytorch.EfficientNet.from_name(\"efficientnet-b0\")\n        n_features = self.net._fc.in_features\n        self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n    \n    def forward(self, x):\n        out = self.net(x)\n        return out","e059c03f":"class LossMeter:\n    def __init__(self):\n        self.avg = 0\n        self.n = 0\n    \n    def update(self, val):\n        self.n += 1\n        # incremental update\n        self.avg = val \/ self.n + (self.n - 1) \/ self.n * self.avg\n        \nclass AccMeter:\n    def __init__(self):\n        self.avg = 0\n        self.n = 0\n    \n    def update(self, y_true, y_pred):\n        y_true = y_true.cpu().numpy().astype(int)\n        y_pred = y_pred.cpu().numpy() >= 0\n        last_n = self.n\n        self.n += len(y_true)\n        true_count = np.sum(y_true == y_pred)\n        # incremental update\n        self.avg = true_count \/ self.n + last_n \/ self.n * self.avg","b0b4d796":"# Trainer\n\nclass Trainer:\n    # do train\n    def __init__(self, model, device, optimizer, criterion, loss_meter, score_meter):\n        self.model = model\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.loss_meter = loss_meter\n        self.score_meter = score_meter\n        \n        self.best_valid_score = -np.inf\n        self.n_patience = 0 # for early_stopping (by 100epoch)\n        \n        self.messages = {\n            \"epoch\": \"[Epoch {}: {}] loss: {:.5f}, score: {:.5f}, time: {} s\",\n            \"checkpoint\": \"The score improved from {:.5f} to {:.5f}. Save model to '{}'\",\n            \"patience\": \"\\nValid score didn't improve last {} epochs.\"\n        }\n        \n    def fit(self, epochs, train_loader, valid_loader, save_path, patience):\n        for n_epoch in range(1, epochs + 1):\n            self.info_message(\"EPOCH: {}\", n_epoch)\n            \n            train_loss, train_score, train_time = self.train_epoch(train_loader)\n            valid_loss, valid_score, valid_time = self.valid_epoch(valid_loader)\n            \n            self.info_message(\n                self.messages[\"epoch\"], \"Train\", n_epoch, train_loss, train_score, train_time\n            )\n            self.info_message(\n                self.messages[\"epoch\"], \"Valid\", n_epoch, valid_loss, valid_score, valid_time\n            )\n            \n            if True:\n                # if self.best_valid_score < valid_score:\n                self.info_message(\n                    self.messages[\"checkpoint\"], self.best_valid_score, valid_score, save_path\n                )\n                self.best_valid_score = valid_score\n                self.save_model(n_epoch, save_path)\n                self.n_patience = 0\n            else:\n                self.n_patience += 1\n            \n            if self.n_patience >= patience:\n                self.info_message(self.messages[\"patience\"], patience)\n                break\n            \n                \n    def train_epoch(self, train_loader):\n        self.model.train()\n        t = time.time()\n        train_loss = self.loss_meter()\n        train_score = self.score_meter()\n        \n        for step, batch in enumerate(train_loader, 1):\n            X = batch[\"X\"].to(self.device)\n            targets = batch[\"y\"].to(self.device)\n            self.optimizer.zero_grad()\n            outputs = self.model(X).squeeze(1)\n            \n            loss = self.criterion(outputs, targets)\n            loss.backward()\n            \n            train_loss.update(loss.detach().item())\n            train_score.update(targets, outputs.detach())\n            \n            self.optimizer.step() # update weight\n            \n            _loss, _score = train_loss.avg, train_score.avg\n            message = 'Train Step {}\/{}, train_loss: {:.5f}, train_score: {:.5f}'\n            self.info_message(message, step, len(train_loader), _loss, _score, end=\"\\r\")\n                   \n        return train_loss.avg, train_score.avg, int(time.time() - t)\n            \n    def valid_epoch(self, valid_loader):\n        self.model.eval()\n        t = time.time()\n        valid_loss = self.loss_meter()\n        valid_score = self.score_meter()\n        \n        for step, batch in enumerate(valid_loader, 1):\n            with torch.no_grad():\n                X = batch[\"X\"].to(self.device)\n                targets = batch[\"y\"].to(self.device)\n\n                outputs = self.model(X).squeeze(1)\n                loss = self.criterion(outputs, targets)\n\n                valid_loss.update(loss.detach().item())\n                valid_score.update(targets, outputs)\n                \n            _loss, _score = valid_loss.avg, valid_score.avg\n            message = 'Valid Step {}\/{}, valid_loss: {:.5f}, valid_score: {:.5f}' \n            self.info_message(message, step, len(valid_loader), _loss, _score, end=\"\\r\")\n            \n        return valid_loss.avg, valid_score.avg, int(time.time() - t)\n        \n    def save_model(self, n_epoch, save_path):\n        torch.save(\n        {\n            \"model_state_dict\" : self.model.state_dict(),\n            \"optimizer_state_dict\" : self.optimizer.state_dict(),\n            \"best_valid_score\" : self.best_valid_score,\n            \"n_epoch\" : n_epoch,\n        },\n        save_path\n        )\n    @staticmethod        \n    def info_message(message, *args, end=\"\\n\"):\n        print(message.format(*args), end=end)","3702b9e9":"# Training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Change later\ntrain_data_retriever = DataRetriever(\n    df_train[\"BraTS21ID\"].values,\n    df_train[\"MGMT_value\"].values\n)\n\nvalid_data_retriever = DataRetriever(\n    df_valid[\"BraTS21ID\"].values, \n    df_valid[\"MGMT_value\"].values\n)\n    \ntrain_loader = torch_data.DataLoader(train_data_retriever, batch_size=32, shuffle=True, num_workers=8)\nvalid_loader = torch_data.DataLoader(valid_data_retriever, batch_size=32, shuffle=False, num_workers=8)\n    \nmodel = Model()\nmodel.to(device)\n    \noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch_functional.binary_cross_entropy_with_logits\n    \ntrainer = Trainer(model, device, optimizer, criterion, LossMeter, AccMeter)\n    \nhistory = trainer.fit(10, train_loader, valid_loader, f\"best-model-0.pth\" ,100)","274391bd":"models = []\nfor i in range(1):\n    model = Model()\n    model.to(device)\n    \n    checkpoint = torch.load(f\"best-model-{i}.pth\")\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    \n    models.append(model)","529344d6":"class DataRetriever(torch_data.Dataset):\n    def __init__(self, paths):\n        self.paths = paths\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        _id = self.paths[index]\n        patient_path = f\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test\/{str(_id).zfill(5)}\/\"\n        channels = []\n        for t in (\"FLAIR\", \"T1w\", \"T1wCE\"): # \"T2w\"\n            t_paths = sorted(\n                glob.glob(os.path.join(patient_path, t, \"*\")), \n                key=lambda x: int(x[:-4].split(\"-\")[-1]),\n            )\n            # start, end = int(len(t_paths) * 0.475), int(len(t_paths) * 0.525)\n            x = len(t_paths)\n            if x < 10:\n                r = range(x)\n            else:\n                d = x \/\/ 10\n                r = range(d, x - d, d)\n                \n            channel = []\n            # for i in range(start, end + 1):\n            for i in r:\n                channel.append(cv2.resize(load_dicom(t_paths[i]), (256, 256)) \/ 255)\n            channel = np.mean(channel, axis=0)\n            channels.append(channel)\n        \n        return {\"X\": torch.tensor(channels).float() ,\"id\": _id }","3a8bd373":"submission = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv\")\n\ntest_data_retriever = DataRetriever(\n    submission[\"BraTS21ID\"].values,\n)\n\ntest_loader = torch_data.DataLoader(\n    test_data_retriever,\n    batch_size = 4,\n    shuffle = False,\n    num_workers = 8,\n)","4c2b1268":"y_pred = []\nids = []\n\nfor e, batch in enumerate(test_loader):\n    print(f\"{e}\/ {len(test_loader)}\", end=\"\\r\")\n    with torch.no_grad():\n        tmp_pred = np.zeros((batch[\"X\"].shape[0], ))\n        for model in models:\n            tmp_res = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n            tmp_pred += tmp_res\n        y_pred.extend(tmp_pred)\n        ids.extend(batch[\"id\"].numpy().tolist()) #?","65f8a0cd":"### submission(sample)\nsubmission = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred})\nsubmission.to_csv(\"submission.csv\", index=False)","8ed6dc95":"plt.figure(figsize=(5, 5))\nplt.hist(submission[\"MGMT_value\"]);","252cf0c4":"submission","28e69f46":"# \ud83d\udd0dEDA","952cbcdc":"#### type of MRI scan  \nEach case consists of four structural multi-parametric MRI (mpMRI) scans.  \n* Fluid Attenuated Inversion Recovery (FLAIR)  \n    - FLAIR can be roughly thought of as T2, in which the water is also black, making it easier to find the lesion.  \n    (FLAIR\u306f\u6c34\u3082\u9ed2\u304f\u3059\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u75c5\u5909\u3092\u63a2\u3057\u3084\u3059\u304f\u306a\u3063\u305fT2)\n* T1-weighted pre-contrast (T1w) \n* T1-weighted post-contrast (T1Gd)\n    - T1Gd is T1 imaging with contrast medium and is the method that best reflects the location, size, and shape of the mass.  \n    (T1Gd\u306f\u9020\u5f71\u5264\u3092\u7528\u3044\u305fT1\u64ae\u5f71\u3067\u3001\u816b\u7624\u306e\u4f4d\u7f6e\u3001\u5927\u304d\u3055\u3001\u5f62\u304c\u6700\u3082\u3088\u304f\u53cd\u6620\u3055\u308c\u308b\u65b9\u6cd5\u3067\u3042\u308b\u3002)  \n* T2-weighted (T2)\n    - T2 :Water is painted white.Lesions appear white. Suitable for lesion evaluation.  \n    (\u6c34\u304c\u767d\u304f\u63cf\u304b\u308c\u308b\u3002\u75c5\u5909\u304c\u767d\u304f\u6620\u308b\u3002\u75c5\u5909\u306e\u8a55\u4fa1\u306b\u9069\u3057\u3066\u3044\u308b\u3002)","78849253":"Look dcm one data ","47ea6437":"**In this competition you will predict the genetic subtype of glioblastoma using MRI (magnetic resonance imaging) scans to train and test your model to detect for the presence of MGMT promoter methylation.**  \n(\u3053\u306e\u30b3\u30f3\u30da\u3067\u306f\u81a0\u82bd\u816b\u306eMRI\u753b\u50cf\u304b\u3089\u3001\uff08\u8133\u816b\u760d\u306e\u6cbb\u7642\u306b\u5fc5\u8981\u306a\uff09\u81a0\u82bd\u816b\u306e\u907a\u4f1d\u5b50\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u3067\u3042\u308bMGMT\u30d7\u30ed\u30e2\u30fc\u30bf\u30fc\u306e\u30e1\u30c1\u30eb\u5316\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u3002)   \nThis  competition is not predicting whether brain tumor or not.  \n**(\u203b\u8133\u816b\u760d\u304b\u3069\u3046\u304b\u3092\u5f53\u3066\u308b\u30b3\u30f3\u30da\u3067\u306f\u306a\u3044!)**\n\nshow the probability of MGMT promoter methylation or not.  ","01bd229c":"[\u65e5\u672c\u8a9e\u89e3\u8aac](https:\/\/www.kaggle.com\/chumajin\/brain-tumor-eda-for-starter-version)   \n[Yaroslav Isaienkov](https:\/\/www.kaggle.com\/ihelon\/brain-tumor-eda-with-animations-and-modeling) This notebook almost based from his notebook  \n[Ayush Thakur](https:\/\/www.kaggle.com\/ayuraj\/brain-tumor-eda-and-interactive-viz-with-w-b)\n","1e3fb631":"The dataset consist of 585 patients and are given by unique id,BraTS21ID  \n* Class 0 = the presence of MGMT promoter methylation no exist.  \n* Class 1 = the presence of MGMT promoter methylation exist.","3f34e5bd":"#### look at the number of files (.dcm) ","e00fef9f":"### submission","3da07b95":"### Estimation\n#### * Sometimes I cannot detect tumor in images\n    * Just Missing Value or cannnot by only using plt.show (show 3D image)?","ea21ba62":"## Ref","09617a63":"### Data Vizualization","8dba565c":"#### look at the distribution of files per scan types","a7eabe89":"![img](https:\/\/i.ibb.co\/QNjHyQd\/W-B-Chart-7-16-2021-3-49-04-AM.png)","2f21265a":"### prepare test data","7bf1d626":"* margin is black(pixel = 0) and brian is centered in the image","bd8d2d4d":"# Goal","1bb6204a":"### Libarary","6a40b354":"### Visualize dcm images","7b9b2fef":"### Inference(test)","1baf2999":"Look dcm data list","58de5a55":"# \ud83c\udfa9Model","a53ba1f2":"https:\/\/wandb.ai\/hondykaito\/brain-tumor-viz?workspace=user-hondykaito","857cf6ec":"### Ref\n[Preprocessing DICOM](https:\/\/www.kaggle.com\/c\/rsna-miccai-brain-tumor-radiogenomic-classification\/discussion\/253000)","9b6a8fb3":"### Library"}}