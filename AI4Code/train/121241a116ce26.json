{"cell_type":{"a89a8ecc":"code","6fa46904":"code","a0197da4":"code","f8c0417c":"code","5b1d6fa4":"code","fefdd684":"code","fcf9c089":"code","957662aa":"code","1f3a019a":"code","e221ea24":"code","14b5332e":"code","e680e437":"code","3a227282":"code","0ca1b866":"code","4bb69e30":"code","45d1cc3f":"code","8dbc96a1":"code","997c1663":"code","fc957ade":"code","b1b0e3c9":"code","e077fd31":"code","af8fe5d4":"code","cfadb0c7":"code","8ea18dc6":"markdown","8ce65251":"markdown","c3c97ea6":"markdown","109b88bb":"markdown","a94261ba":"markdown","bf53191e":"markdown","28b5f445":"markdown","e7cf99b4":"markdown","800f4ad2":"markdown","ecd5cc98":"markdown","d9177565":"markdown","f35951a2":"markdown","7ec23009":"markdown","85b6f861":"markdown","a5e20119":"markdown","0d9112d9":"markdown"},"source":{"a89a8ecc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.feature_extraction.text import CountVectorizer\nnltk.download('vader_lexicon')\nfrom sklearn.cluster import KMeans\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom wordcloud import WordCloud,STOPWORDS\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport random\nfrom keras.layers import Dense,LSTM,Input,Dropout,SimpleRNN\nfrom keras import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import one_hot\nfrom tqdm.notebook import tqdm\nimport gc\nplt.rc('figure',figsize=(17,13))\nsns.set_context('paper',font_scale=2)","6fa46904":"anime_df = pd.read_csv('\/kaggle\/input\/top-10000-anime-movies-ovas-and-tvshows\/Anime_Top10000.csv')\nanime_df.head(3)","a0197da4":"\n# Remove all the special characters\nanime_df.Anime_Name\t             = anime_df.Anime_Name.apply(lambda x: ''.join(re.sub(r'\\W', ' ', x))) \nanime_df.Synopsis\t             = anime_df.Synopsis.apply(lambda x: ''.join(re.sub(r'\\W', ' ', x))) \n\n# Substituting multiple spaces with single space \nanime_df.Anime_Name              = anime_df.Anime_Name.apply(lambda x: ''.join(re.sub(r'\\s+', ' ', x, flags=re.I)))\nanime_df.Synopsis                = anime_df.Synopsis.apply(lambda x: ''.join(re.sub(r'\\s+', ' ', x, flags=re.I)))\n\n# Converting to Lowercase \nanime_df.Anime_Name              = anime_df.Anime_Name.str.lower() \nanime_df.Synopsis                = anime_df.Synopsis.str.lower() \n\n#Synopsis Sentiment Analysis\nsid = SIA()\nanime_df['sentiments']           = anime_df['Synopsis'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\nanime_df['Positive Sentiment']   = anime_df['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \nanime_df['Neutral Sentiment']    = anime_df['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\nanime_df['Negative Sentiment']   = anime_df['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\nanime_df.drop(columns=['sentiments'],inplace=True)\n","f8c0417c":"#Only TV-Shows\ntv_df                                                               = anime_df[anime_df.Anime_Episodes.str.contains('TV')].copy()\n\n#Extract Number Of Episodes\ntv_df['#_Episodes']                                                 = tv_df.Anime_Episodes.apply(lambda x: ''.join(re.findall(r'[0-9]*', x)))\ntv_df['#_Episodes']                                                 = tv_df['#_Episodes'].replace('',np.nan)\ntv_df.loc[tv_df['#_Episodes'].notna(),'#_Episodes']                 = tv_df[tv_df['#_Episodes'].notna()]['#_Episodes'].astype(np.int32)\n#Extract Air Years\ntv_df['Air_Years']                                                  = tv_df.Anime_Air_Years.apply(lambda x: ''.join(re.findall(r'[0-9 -]*', x)))\n\n#Start Year\ntv_df['Start_Year']                                                 = tv_df.Air_Years.apply(lambda x: x.split('-')[0].strip())\ntv_df['Start_Year']                                                 = tv_df['Start_Year'].astype(np.int32)\n#End Year\ntv_df['End_Year']                                                   = tv_df.Air_Years.apply(lambda x: x.split('-')[1].strip() if len(x.split('-')[1])>4 else 'Still Airing')\ntv_df.loc[(~tv_df['End_Year'].str.contains('Still')),'End_Year']    = tv_df[~tv_df['End_Year'].str.contains('Still')]['End_Year'].astype(np.int32)\n\ntv_df","5b1d6fa4":"plt.title('Distriubtion of Anime Ratings')\nsns.histplot(tv_df.Anime_Rating,kde=True,stat='probability',palette = cm.coolwarm(tv_df.Anime_Rating))\nplt.show()","fefdd684":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =tv_df[['Anime_Rating','Start_Year','End_Year','#_Episodes','Positive Sentiment','Neutral Sentiment','Negative Sentiment']].corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =tv_df[['Anime_Rating','Start_Year','End_Year','#_Episodes','Positive Sentiment','Neutral Sentiment','Negative Sentiment']].corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","fcf9c089":"plt.title('Distriubtion Anime First Air Years')\nax = sns.barplot(x=tv_df.Start_Year.value_counts().sort_index().index,y=tv_df.Start_Year.value_counts().sort_index().values,palette=cm.coolwarm(tv_df.Start_Year.value_counts().sort_index().values))\nplt.xticks(rotation=-45,fontsize=11)\nplt.show()","957662aa":"plt.figure(figsize=(25,6))\nplt.title('Distriubtion Anime First Air Years')\nax = sns.barplot(x=tv_df['#_Episodes'].value_counts().sort_index().index,\n                 y=tv_df['#_Episodes'].value_counts().sort_index().values,\n                 palette=cm.coolwarm(tv_df['#_Episodes'].value_counts().sort_index().values))\nplt.xticks(rotation=90,fontsize=10)\nplt.margins(x=0)\n\nplt.show()","1f3a019a":"year_mean_df = tv_df.groupby(by='Start_Year').mean()\n\nfig = go.Figure()\ntrace = go.Scatter(x=year_mean_df.index,y=year_mean_df.Anime_Rating,mode='lines+markers',name='Average Rating',line=dict(color='firebrick', width=4))\nRA = year_mean_df.Anime_Rating.rolling(5).mean()\nRS = year_mean_df.Anime_Rating.rolling(5).std()\n\nrunning_average = go.Scatter(x=RA.index,y=RA.values,mode='lines',name='Running Average',line_color='blue')\nrunning_average_postd = go.Scatter(x=RA.index,y=RA.values+RS.values,mode='lines',name='Running Average + 1 SD',line_color='green',line_dash='dot')\nrunning_average_mostd = go.Scatter(x=RA.index,y=RA.values-RS.values,mode='lines',name='Running Average - 1 SD',line_color='green',line_dash='dot', fill='tonexty')\n\nfig.add_trace(trace)\nfig.add_trace(running_average)\nfig.add_trace(running_average_postd)\nfig.add_trace(running_average_mostd)\n\nfig.update_layout(title='<b> Start Year Mean Anime Rating<b>',xaxis_title='<b>Year<b>',yaxis_title='<b>Average Rating<b>')\nfig.update_layout(hovermode=\"x unified\")\n\nfig.show()","e221ea24":"year_mean_df = tv_df.groupby(by='End_Year').mean()\n\nfig = go.Figure()\ntrace = go.Scatter(x=year_mean_df.index,y=year_mean_df.Anime_Rating,mode='lines+markers',name='Average Rating',line=dict(color='firebrick', width=4))\nRA = year_mean_df.Anime_Rating.rolling(5).mean()\nRS = year_mean_df.Anime_Rating.rolling(5).std()\n\nrunning_average = go.Scatter(x=RA.index,y=RA.values,mode='lines',name='Running Average',line_color='blue')\nrunning_average_postd = go.Scatter(x=RA.index,y=RA.values+RS.values,mode='lines',name='Running Average + 1 SD',line_color='green',line_dash='dot')\nrunning_average_mostd = go.Scatter(x=RA.index,y=RA.values-RS.values,mode='lines',name='Running Average - 1 SD',line_color='green',line_dash='dot', fill='tonexty')\n\nfig.add_trace(trace)\nfig.add_trace(running_average)\nfig.add_trace(running_average_postd)\nfig.add_trace(running_average_mostd)\n\nfig.update_layout(title='<b> End Year Mean Anime Rating<b>',xaxis_title='<b>Year<b>',yaxis_title='<b>Average Rating<b>')\nfig.update_layout(hovermode=\"x unified\")\n\nfig.show()","14b5332e":"b_date_mean = tv_df.groupby(by='Start_Year').mean().reset_index()\nb_date_std = tv_df.groupby(by='Start_Year').std().reset_index()\n\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Yearly Average Positive Sentiment',  'Yearly Average Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['Start_Year'], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment Mean'),\n    row=1, col=1\n)\n\n    \n#positive mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['Start_Year'].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean['Start_Year'].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean'\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['Start_Year'], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment Mean'),\n    row=2, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['Start_Year'].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean['Start_Year'].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\nfig['layout']['xaxis2']['title'] = 'Start_Year'\nfig.update_layout(hovermode=\"x unified\")\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()","e680e437":"b_date_mean = tv_df.groupby(by='Start_Year').std().reset_index()\n\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Yearly Deviation in Positive Sentiment',  'Yearly Deviation in Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['Start_Year'], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment SD'),\n    row=1, col=1\n)\n\n    \n#positive mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['Start_Year'].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean['Start_Year'].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean'\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['Start_Year'], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment SD'),\n    row=2, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['Start_Year'].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean['Start_Year'].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\nfig['layout']['xaxis2']['title'] = 'Start_Year'\nfig.update_layout(hovermode=\"x unified\")\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()","3a227282":"f_data = tv_df.copy()\nplt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Lyrics',fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],label='Negative Sentiment',lw=2)\nsns.kdeplot(f_data['Neutral Sentiment'] ,label='Neutral Sentiment' ,color='orange' ,lw=2)\nsns.kdeplot(f_data['Positive Sentiment'],label='Positive Sentiment',color='tab:red',lw=2)\nplt.legend()\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Lyrics',fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],cumulative=True,label='Negative Sentiment',lw=2)\nsns.kdeplot(f_data['Neutral Sentiment'],cumulative=True,label='Neutral Sentiment' ,color='orange' ,lw=2)\nsns.kdeplot(f_data['Positive Sentiment'],cumulative=True ,label='Positive Sentiment',color='tab:red',lw=2)\nplt.xlabel('Sentiment Value')\nplt.legend()\nplt.tight_layout()\nplt.show()","0ca1b866":"Rating_Based = pd.cut(tv_df.Anime_Rating,10,labels=range(0,10)).to_frame()\nRating_Based['Range'],bins = pd.cut(tv_df.Anime_Rating,10,retbins=True)\nRating_Based['Syn'] = tv_df.Synopsis\n\nbins =[str(Rating_Based.query(f'Anime_Rating == {i}').Range.iloc[0]) for i in range(0,10)]\nSTP = list(STOPWORDS)\nSTP += ['written','mal','s','by','rewrite']\nfigure,axs = plt.subplots(2,5)\nsample = 9\n\nfor row in axs:\n    for ax in row:\n        ax.set_title('Rating Range +'+bins[sample],fontsize=13)\n        wc = WordCloud(background_color='white',width=200,height=200,stopwords=STP).generate(' '.join(Rating_Based.query(f'Anime_Rating == {sample}').Syn))\n        ax.imshow(wc)\n        sample-=1\n        ax.axis('off')\nplt.tight_layout()\nplt.show()\n#Rating_Based","4bb69e30":"NUMBER_OF_COMPONENTS = 500\n\nCVZ = CountVectorizer()\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = tv_df.Synopsis.copy()\ntext_data = text_data.apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS and len(word) > 1]).strip())\n\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\n\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n\nC_vector = CVZ.fit_transform(text_data)\n\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Anime Synopsis Variance Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","45d1cc3f":"\nbest_fearures = [[CVZ.get_feature_names()[i],SVD.components_[0][i]] for i in SVD.components_[0].argsort()[::-1]]\nworddf = pd.DataFrame(np.array(best_fearures[:500])[:,0]).rename(columns={0:'Word'})\nworddf['Explained Variance'] =  np.round(evr*100,2)\nworddf['Explained Variance'] =worddf['Explained Variance'].apply(lambda x:str(x)+'%')\napp = []\nfor word in tqdm(worddf.Word):\n    total_count = 0\n    for tweet in text_data:\n        if tweet.find(word)!= -1:\n            total_count+=1\n    app.append(total_count)\nworddf['Appeared_On_X_Tweets'] = app\nworddf\n\nfig = go.Figure()\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['<b>Word<b>',\"<b>Accountable X% of Variance<b>\",'<b>Appeared On X Synopsis<b>'],\n            font=dict(size=19,family=\"Lato\"),\n            align=\"center\"\n        ),\n        cells=dict(\n            values=[worddf[k].tolist() for k in ['Word',\"Explained Variance\",'Appeared_On_X_Tweets']],\n            align = \"center\")\n    ),\n    \n)\ndel best_fearures\ndel app\nfig.show()","8dbc96a1":"\ngc.collect()\nsynopsis = tv_df.Synopsis[:tv_df.shape[0]\/\/3]\nvocab = list(nltk.FreqDist(' '.join(synopsis).split(' ')).keys())\nvocab_size = len(vocab)\nword_index = { ch:i for i,ch in enumerate(vocab) }\nindex_word = { i:ch for i,ch in zip(word_index.values(),word_index.keys())}\nFTD = ' '.join(synopsis).split(' ')","997c1663":"S = []  \nC = []\nstride = 10\nT = 25\nfor i in range(0, len(FTD) - T, stride):\n    S.append(FTD[i: i + T])\n    C.append(FTD[i + T])\nX = np.zeros((len(S), T, vocab_size), dtype='bool')\nY = np.zeros((len(S), vocab_size), dtype='bool')    \nfor i, seq in tqdm(enumerate(S)):\n    for t, char in enumerate(seq):\n        X[i, t, word_index[char]] = 1\n        Y[i, word_index[C[i]]] = 1","fc957ade":"lstm_nn = Sequential()\nlstm_nn.add(Input((T,vocab_size)))\nlstm_nn.add(LSTM(128))\nlstm_nn.add(Dropout(0.2))\nlstm_nn.add(Dense(vocab_size,activation='softmax'))\n\nlstm_nn.compile(optimizer='rmsprop',loss='categorical_crossentropy')\nlstm_nn.summary()","b1b0e3c9":"history = lstm_nn.fit(X, Y, epochs=500, batch_size=128,verbose=False)","e077fd31":"plt.plot(history.history['loss'],'.-')\nplt.ylabel('loss',fontsize=14)\nplt.show()","af8fe5d4":"generated = []\nfor itr in tqdm(range(0,10)):\n    start = np.random.randint(0, len(X)-1)\n    input_buffer = X[start] \n    generated_text = S[start].copy()\n\n    for i in (range(100)):\n        yhat = lstm_nn.predict(input_buffer[None,:])[0]\n        #ix = np.argmax(yhat)\n        ix = np.random.choice(range(vocab_size), p=yhat)\n\n        ch = index_word[ix]\n        generated_text += [ch]\n        input_buffer = np.r_[input_buffer[1:,:], np.zeros((1,vocab_size))]\n        input_buffer[-1,ix] = 1\n\n    generated.append(' '.join(generated_text))","cfadb0c7":"for dx,i in enumerate(generated):\n    print('Generated Synopsis Example #',dx)\n    print(i,'\\n\\n')","8ea18dc6":"<a id=\"3\"><\/a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Feature Engineering<\/h3>\n","8ce65251":"<a id=\"1\"><\/a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h3>\n","c3c97ea6":"<a id=\"5\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Generating Synopses<\/h3>\n","109b88bb":"**Observation**: Skimming the plot above, we can immediately observe that 2013-2019 were the years when the highest rating animes first aired.\nOlder animes may be rated lower in comparison to this interval, but it may be due to the trend of new animes being written and animated with the aim to the current viewer taste and trends where in the past, the opinion and taste of western countries had a lesser effect of anime writers as well as western countries not having a large industry of anime writes.\n","a94261ba":"<a id=\"4.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Time Based Analysis<\/h3>\n","bf53191e":"**Observation**: unfortunately, we see no significant correlations between our numeric features in both Pearson and Spearman correlation metrics.","28b5f445":"**Observation**: When looking at the distribution of rating in our sample of anime tv-shows we see that distribution is approximately normal and centered around a particular mean, but an interesting point to note is the multimodality, we have two modes around the mean which may indicate two underlying groups, in the probabilistic inference section we will further explore the two modes.","e7cf99b4":"**Observation**: Similarly to the average first-year rating, we see a climbing trend in the average rating, but in comparison to the first year, the end-year has a much lower deviation through time, i.e., constant variance nonconstant mean.\n","800f4ad2":"**Observation**: In the plot above, we observe the distribution of the number of episodes in the anime TV show sample.\nInterestingly 12,13,24,25,26 episodes are the most common amount of episodes for an anime TV show.","ecd5cc98":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Table Of Content<\/h3>\n\n\n* [1. Import Libraries and Data Loading](#1)\n* [2. Data Preprocessing](#2)\n* [3. Feature Engineering](#3)\n* [4. Exploratory Data Analysis](#4)\n    * [4.1 Time Based Analysis](#4.1)\n    * [4.2 Synopsis Based Analysis](#4.2)\n* [5. Generating Synopses](#5)\n    * [5.1 Training a LSTM Network](#5.1)\n    * [5.2 Generated Synopses](#5.2)\n    \n\n","d9177565":"<a id=\"5.2\"><\/a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Generated Synopses<\/h3>\n","f35951a2":"<a id=\"4.2\"><\/a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Synopsis Based Analysis<\/h3>\n","7ec23009":"<a id=\"2\"><\/a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing<\/h3>\n","85b6f861":"**Observation**: When looking at the average anime rating throughout our data frame's timeline, we see a rising average rating trend each year.\nAnother interesting observation is that the deviation in rating becomes narrower with time, meaning the population that rates the anime shows in our data becomes more consistent.\n\n","a5e20119":"<a id=\"5.1\"><\/a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Training a LSTM Network<\/h3>\n","0d9112d9":"<a id=\"4\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h3>\n"}}