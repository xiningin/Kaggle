{"cell_type":{"8182f6c8":"code","61682869":"code","c9670149":"code","18605a57":"code","3566055c":"code","8addd4c8":"code","efbe572b":"code","3bdfbf09":"code","d509e431":"code","a08c042f":"code","eb153749":"code","bee0b122":"code","f4fa4c60":"code","9da39062":"code","8622b7ab":"code","0966c34a":"code","dd12ad16":"code","baf72552":"code","2f4ce4c0":"code","cd4e2404":"code","fc2ed4bf":"code","1733c44f":"code","7c862256":"code","2be5eecc":"code","6ce90109":"code","fcb5583a":"code","c63543aa":"code","939a3fe2":"code","7907c66f":"code","497d1268":"code","8cd2cc11":"code","1ad2d985":"code","473f3714":"code","06d74b4e":"code","0f393de1":"code","d6c5d556":"code","fefba3c0":"code","39f473ad":"code","0ac438ff":"code","c559f671":"code","fb3a146b":"code","7b6642c2":"code","a15134d1":"code","4df22236":"code","a2a9522f":"code","916fe460":"code","ed4a4b5b":"code","258698ae":"code","5141762d":"code","a158c92f":"code","0ba61018":"code","fbdfbe12":"code","59d14fed":"code","478cb13e":"code","378cfadf":"code","4084e2a5":"code","bbc37247":"code","86afc815":"code","dd7f5bd5":"code","0a6c2807":"code","d0140d2b":"code","7eb48ced":"code","12ab23cc":"code","f56a3e80":"code","07eb0f1e":"code","4ad6d7ac":"code","ccc5e6c4":"code","0aedf3e5":"code","49b43a7c":"code","f59a1428":"code","5f74973d":"code","5ed9453a":"code","6fa4eb27":"code","7bc0baf4":"code","97c4bdd5":"code","71216c5f":"code","984dced5":"code","24ca41f5":"code","dafb1578":"code","0854111a":"code","a686cd0b":"code","c09ffb94":"code","7ff93dde":"code","3010e0e3":"code","6f342bc6":"code","fa19044a":"code","be9f4751":"code","bb375a36":"code","df0ed465":"code","94e7c351":"code","db38a4fe":"code","b3213c0b":"code","7f8bc03a":"code","24201672":"code","0ba028a3":"code","79fe9efa":"code","566287c1":"code","1537b247":"code","8334048d":"code","b8424fb9":"code","7f51746a":"code","d1163c7b":"code","7198b9b7":"code","699dd0fa":"markdown","4c797036":"markdown","aa82c528":"markdown","cc0a041d":"markdown"},"source":{"8182f6c8":"import os\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport time\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm\n\npath = '..\/input\/covid19-global-forecasting-week-4\/'\ntrain = pd.read_csv(path + 'train.csv')\ntest  = pd.read_csv(path + 'test.csv')\nsub   = pd.read_csv(path + 'submission.csv')\n\ntrain['Date'] = train['Date'].apply(lambda x: (datetime.datetime.strptime(x, '%Y-%m-%d')))\ntest['Date'] = test['Date'].apply(lambda x: (datetime.datetime.strptime(x, '%Y-%m-%d')))\n#path_ext = '..\/input\/novel-corona-virus-2019-dataset\/'\n#ext_rec = pd.read_csv(path_ext + 'time_series_covid_19_recovered.csv').\\\n#        melt(id_vars=[\"Province\/State\", \"Country\/Region\", \"Lat\", \"Long\"], \n#            var_name=\"Date\", \n#            value_name=\"Recoveries\")\n#ext_rec['Date'] = ext_rec['Date'].apply(lambda x: (datetime.datetime.strptime(x+\"20\", '%m\/%d\/%Y')))\n#train = train.merge(ext_rec[['Province\/State', 'Country\/Region', 'Date', 'Recoveries']], how='left',\n#           left_on=['Province\/State', 'Country\/Region', 'Date'],\n#           right_on=['Province\/State', 'Country\/Region', 'Date'])\n\ntrain['days'] = (train['Date'].dt.date - train['Date'].dt.date.min()).dt.days\ntest['days'] = (test['Date'].dt.date - train['Date'].dt.date.min()).dt.days\n#train['isTest'] = train['Date'].dt.date >= datetime.date(2020, 3, 12)\n#train['isVal'] = np.logical_and(train['Date'].dt.date >= datetime.date(2020, 3, 11), train['Date'].dt.date <= datetime.date(9999, 3, 18))\ntrain.loc[train['Province_State'].isnull(), 'Province_State'] = 'N\/A'\ntest.loc[test['Province_State'].isnull(), 'Province_State'] = 'N\/A'\n\ntrain['Area'] = train['Country_Region'] + '_' + train['Province_State']\ntest['Area'] = test['Country_Region'] + '_' + test['Province_State']\n\nprint('train Date min',train['Date'].min())\nprint('train Date max',train['Date'].max())\nprint('test Date min',test['Date'].min())\nprint('train days max', train['days'].max())\nN_AREAS = train['Area'].nunique()\nAREAS = np.sort(train['Area'].unique())\nSTART_PUBLIC = test['days'].min()\n\nprint('public LB start day', START_PUBLIC)\nprint(' ')\n\n\nTRAIN_N = 84\nVAL_DAYS = train['days'].max() - TRAIN_N + 1\nVAL_START_DATE = train[train['days'] >= TRAIN_N]['Date'].min() # need for davids model\nprint(train[train['days'] < TRAIN_N]['Date'].max())\nprint(train[train['days'] >= TRAIN_N]['Date'].min())\nprint(train[train['days'] >= TRAIN_N]['Date'].max())\ntrain.head()\n\ntest_orig = test.copy()","61682869":"VAL_DAYS","c9670149":"print('test Date max',test['days'].max())","18605a57":"train_p_c_raw = train.pivot(index='Area', columns='days', values='ConfirmedCases').sort_index()\ntrain_p_f_raw = train.pivot(index='Area', columns='days', values='Fatalities').sort_index()\n\ntrain_p_c = np.maximum.accumulate(train_p_c_raw, axis=1)\ntrain_p_f = np.maximum.accumulate(train_p_f_raw, axis=1)\n\nf_rate = (train_p_f \/ train_p_c).fillna(0)\n\nX_c = np.log(1+train_p_c.values)[:,:TRAIN_N]\nX_f = train_p_f.values[:,:TRAIN_N]\n","3566055c":"from sklearn.metrics import mean_squared_error\n\ndef eval1(y, p):\n    val_len = y.shape[1] - TRAIN_N\n    return np.sqrt(mean_squared_error(y[:, TRAIN_N:TRAIN_N+val_len].flatten(), p[:, TRAIN_N:TRAIN_N+val_len].flatten()))\n\ndef run_c(params, X, test_size=50):\n    \n    gr_base = []\n    gr_base_factor = []\n    \n    x_min = np.ma.MaskedArray(X, X<1)\n    x_min = x_min.argmin(axis=1) \n    \n    for i in range(X.shape[0]):\n        temp = X[i,:]\n        threshold = np.log(1+params['min cases for growth rate'])\n        num_days = params['last N days']\n        if (temp > threshold).sum() > num_days:\n            d = np.diff(temp[temp > threshold])[-num_days:]\n            w = np.arange(len(d))+1\n            w = w**5\n            w = w \/ np.sum(w)\n            gr_base.append(np.clip(np.average(d, weights=w), 0, params['growth rate max']))\n            d2 = np.diff(d)\n            w = np.arange(len(d2))+1\n            w = w**10\n            w = w \/ np.sum(w)\n            gr_base_factor.append(np.clip(np.average(d2, weights=w), -0.5, params[\"growth rate factor max\"]))\n        else:\n            gr_base.append(params['growth rate default'])\n            gr_base_factor.append(params['growth rate factor'])\n\n    gr_base = np.array(gr_base)\n    gr_base_factor = np.array(gr_base_factor)\n    #print(gr_base_factor)\n    #gr_base = np.clip(gr_base, 0.02, 0.8)\n    preds = X.copy()\n\n    for i in range(test_size):\n        delta = np.clip(preds[:, -1], np.log(2), None) + gr_base * (1 + params['growth rate factor']*(1 + params['growth rate factor factor'])**(i))**(np.log1p(i))\n        #delta = np.clip(preds[:, -1], np.log(2), None) + gr_base * (1 + gr_base_factor*(1 + params['growth rate factor factor'])**(i))**(i)\n        #delta = np.clip(preds[:, -1], np.log(2), None) + gr_base * (1 + params['growth rate factor']*(1 + params['growth rate factor factor'])**(i+X.shape[1]-x_min))**(i+X.shape[1]-x_min) \n        preds = np.hstack((preds, delta.reshape(-1,1)))\n\n    return preds\n\nparams = {\n    \"min cases for growth rate\": 0,\n    \"last N days\": 20,\n    \"growth rate default\": 0.10,\n    \"growth rate max\": 0.3,\n    \"growth rate factor max\": -0.1,\n    \"growth rate factor\": -0.3,\n    \"growth rate factor factor\": 0.02,\n}\n#x = train_p_c[train_p_c.index==\"Austria_N\/A\"]\n\nx = train_p_c\n\npreds_c = run_c(params, np.log(1+x.values)[:,:TRAIN_N])\n# eval1(np.log(1+x).values, preds_c)","8addd4c8":"for i in range(N_AREAS):\n    if 'China' in AREAS[i] and preds_c[i, TRAIN_N-1] < np.log(31):\n        preds_c[i, TRAIN_N:] = preds_c[i, TRAIN_N-1]","efbe572b":"def run_f2(params, X, test_size=50):\n    \n    gr_base = []\n    for i in range(X.shape[0]):\n        temp = X[i,:]\n        threshold = np.log(1.1+params['min cases for growth rate'])\n        num_days = params['last N days']\n        if (temp > threshold).sum() > num_days:\n            d = np.diff(temp[temp > threshold])[-num_days:]\n            gr_base.append(np.clip(np.mean(d), 0, params['growth rate max']))\n        else:\n            gr_base.append(params['growth rate default'])\n\n    gr_base = np.array(gr_base)\n    preds = X.copy()\n\n    for i in range(test_size):\n        #delta = np.clip(preds[:, -1], np.log(2), None) + gr_base * (1 + params['growth rate factor'])**(i)\n        delta = preds[:, -1] + gr_base * (1 + params['growth rate factor'])**(i)\n        preds = np.hstack((preds, delta.reshape(-1,1)))\n\n    return preds\n\nparams = {\n    \"min cases for growth rate\": 2,\n    \"last N days\": 7,\n    \"growth rate default\": 0.05,\n    \"growth rate max\": 0.17,\n    \"growth rate factor\": -0.18,\n}\n\nx = train_p_f\n\npreds_f_1 = run_f2(params, np.log(1+x.values)[:,:TRAIN_N])\n# eval1(np.log(1+x).values, preds_f_1)","3bdfbf09":"from torch.utils.data import Dataset\nfrom torch.nn import Parameter\nimport torch.nn as nn\nfrom torch.nn import init\nimport math \nimport torch\nimport time\n\nclass ZDatasetF(Dataset):\n    def __init__(self, X_c, X_f=None, hist_len=10):\n        self.X_c = X_c\n        self.X_f = X_f\n        self.hist_len = hist_len\n        self.is_test = X_f is None\n    def __len__(self):\n        return self.X_c.shape[1]\n    def __getitem__(self, idx):\n        if self.is_test:\n            return {'x_c':self.X_c[:, idx-self.hist_len:idx]}\n        else:\n            return {'x_c':self.X_c[:, idx-self.hist_len:idx],\n                    'x_f':self.X_f[:, idx-1],\n                    'y':np.log(1+self.X_f[:, idx])}\n\nclass PrLayer2(nn.Module):\n    def __init__(self, in_features1, in_features2):\n        super(PrLayer2, self).__init__()\n        self.weight0 = Parameter(torch.Tensor(1, 1, in_features2))\n        self.weight1 = Parameter(torch.Tensor(1, in_features1, in_features2))\n        self.reset_parameters()\n    def reset_parameters(self):\n        init.kaiming_uniform_(self.weight0, a=math.sqrt(5))\n        init.kaiming_uniform_(self.weight1, a=math.sqrt(5))\n    def forward(self, input):\n        return input * torch.sigmoid(self.weight0 + self.weight1)\n\n\n\nclass ZModelF(nn.Module):\n\n    def __init__(self, hist_len):\n        super(ZModelF, self).__init__()\n        self.l_conv = PrLayer2(len(X_c),hist_len-1)\n\n    def forward(self, x_c, x_f):\n        x = x_c[:,:,1:] - x_c[:,:,:-1]\n        res = torch.sum(self.l_conv(x), 2)\n        return {'preds': torch.log(1 + x_f + res)}        \n        \n\nclass DummySampler(torch.utils.data.sampler.Sampler):\n    def __init__(self, idx):\n        self.idx = idx\n    def __iter__(self):\n        return iter(self.idx)\n    def __len__(self):\n        return len(self.idx)\n    \n    \ndef _smooth_l1_loss(target):\n    t = torch.abs(target)\n    t = torch.where(t < 1, 0.5 * t ** 2, t - 0.5)\n    return torch.mean(t)\n\n\nn_epochs = 5000\nlr = 0.18\nbag_size = 4\ndevice = 'cpu'\nhist_len = 14\nloss_func = torch.nn.MSELoss()\nreg_loss_func = _smooth_l1_loss\nreg_factor = 0.035\n\n\ntrain_dataset = ZDatasetF(np.exp(X_c)-1, X_f, hist_len=hist_len)\ntest_dataset = ZDatasetF(np.exp(preds_c)-1, hist_len=hist_len)\n\n#trn_idx = np.arange(hist_len+1, len(train_dataset))\ntrn_idx = np.arange(hist_len+1, len(train_dataset))\ntrain_sampler = torch.utils.data.sampler.SubsetRandomSampler(trn_idx)\n#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, sampler=train_sampler, num_workers=0, pin_memory=True)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(trn_idx), sampler=train_sampler, num_workers=0, pin_memory=True)\n\ntest_idx = np.arange(TRAIN_N, len(test_dataset))\ntest_sampler = DummySampler(test_idx)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, sampler=test_sampler, num_workers=0, pin_memory=True)\n\n\n#gradient_accumulation = len(trn_idx)\ngradient_accumulation = 1\n\npreds_f = 0\n\nfor m_i in range(bag_size):\n    model_f = ZModelF(hist_len=hist_len).to(device)\n    optimizer_f = torch.optim.Adam(model_f.parameters(), lr=lr)\n    model_f.train()\n\n    start_time = time.time()\n    for epoch in range(n_epochs):\n\n        s = time.time()\n        avg_train_loss = 0\n        \n        optimizer_f.zero_grad()\n        for idx, data in enumerate(train_loader):\n\n            X1 = data['x_c'].to(device).float()\n            X2 = data['x_f'].to(device).float()\n            y = data['y'].to(device).float()\n            \n            preds = model_f(X1, X2)['preds'].float()\n\n            cond = X2 > np.log(10)\n            preds = preds[cond]\n            y = y[cond]\n            \n            loss = loss_func(preds, y)\n            \n            loss += reg_factor * reg_loss_func(model_f.l_conv.weight1)\n            \n            avg_train_loss += loss  \/ len(train_loader)\n            \n            loss.backward()\n            if (idx+1) % gradient_accumulation == 0 or idx == len(train_loader) - 1: \n                optimizer_f.step()\n                optimizer_f.zero_grad()\n                \n        if epoch % 1000 == 0:\n        \n            model_f.eval()\n            preds_f_delta = train_p_f.values[:,:TRAIN_N]\n\n            for idx, data in enumerate(test_loader):\n                X1 = data['x_c'].to(device).float()\n                temp = model_f(X1, torch.Tensor(preds_f_delta[:,-1]).unsqueeze(0))['preds']\n                temp = np.exp(temp.detach().cpu().numpy().reshape(-1,1)) - 1\n                preds_f_delta = np.hstack((preds_f_delta, temp))\n\n            preds_f_delta = np.log(1 + preds_f_delta)\n#             val_len = train_p_c.values.shape[1] - TRAIN_N\n\n#             m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, TRAIN_N:TRAIN_N+val_len]).flatten(), \\\n#                                             preds_f_delta[:, TRAIN_N:TRAIN_N+val_len].flatten()))\n#             print(f\"{epoch:2} train_loss {avg_train_loss:<8.4f} val_loss {m2:8.5f} {time.time()-s:<2.2f}\")\n                \n            model_f.train()\n        \n    model_f.eval()\n    preds_f_delta = train_p_f.values[:,:TRAIN_N]\n    \n    for idx, data in enumerate(test_loader):\n        X1 = data['x_c'].to(device).float()\n        temp = model_f(X1, torch.Tensor(preds_f_delta[:,-1]).unsqueeze(0))['preds']\n        temp = np.exp(temp.detach().cpu().numpy().reshape(-1,1)) - 1\n        preds_f_delta = np.hstack((preds_f_delta, temp))\n    preds_f += preds_f_delta \/ bag_size\n\npreds_f_2 = np.log(1 + preds_f)\n\nprint(\"Done\")\n# eval1(np.log(1+train_p_f).values, preds_f_2)","d509e431":"preds_f_2.shape","a08c042f":"preds_f = np.average([preds_f_1, preds_f_2], axis=0, weights=[2,1])","eb153749":"from sklearn.metrics import mean_squared_error\n\nif True:\n    #val_len = train_p_c.values.shape[1] - TRAIN_N\n    val_len = TRAIN_N - START_PUBLIC\n    for i in range(val_len):\n        d = i + START_PUBLIC\n        m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, d]), preds_c[:, d]))\n        m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, d]), preds_f[:, d]))\n        print(f\"{d}: {(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n\n    print()\n\n    m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, START_PUBLIC:START_PUBLIC+val_len]).flatten(), preds_c[:, START_PUBLIC:START_PUBLIC+val_len].flatten()))\n    m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, START_PUBLIC:START_PUBLIC+val_len]).flatten(), preds_f[:, START_PUBLIC:START_PUBLIC+val_len].flatten()))\n    print(f\"{(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")","bee0b122":"from sklearn.preprocessing import LabelEncoder\n","f4fa4c60":"## defining constants\nPATH_TRAIN = \"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\"\nPATH_TEST = \"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\"\n\nPATH_SUBMISSION = \"submission.csv\"\nPATH_OUTPUT = \"output.csv\"\n\nPATH_REGION_METADATA = \"\/kaggle\/input\/covid19-forecasting-metadata\/region_metadata.csv\"\nPATH_REGION_DATE_METADATA = \"\/kaggle\/input\/covid19-forecasting-metadata\/region_date_metadata.csv\"\n\n# VAL_DAYS = 5 \nMAD_FACTOR = 0.5\nDAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000]\n\nSEED = 2357\n\nLGB_PARAMS = {\"objective\": \"regression\",\n              \"num_leaves\": 5,\n              \"learning_rate\": 0.013,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.81,\n              \"reg_alpha\": 0.13,\n              \"reg_lambda\": 0.13,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }\n\n## reading data\ntrain = pd.read_csv(PATH_TRAIN)\ntest = pd.read_csv(PATH_TEST)\n\nregion_metadata = pd.read_csv(PATH_REGION_METADATA)\nregion_date_metadata = pd.read_csv(PATH_REGION_DATE_METADATA)\n\n## preparing data\ntrain = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\ntest = test[~test.Date.isin(train.Date.unique())]\n\ndf_panel = pd.concat([train, test], sort = False)\n\n# combining state and country into 'geography'\ndf_panel[\"geography\"] = (df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)).values\ndf_panel.loc[df_panel.Province_State.isna(), \"geography\"] = df_panel[df_panel.Province_State.isna()].Country_Region\n\n# fixing data issues with cummax\ndf_panel.ConfirmedCases = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\ndf_panel.Fatalities = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\n\n# merging external metadata\n#print(df_panel.geography)\n\ndf_panel = df_panel.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"], how=\"left\")\n\ndf_panel = df_panel.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")\ndf_panel.loc[df_panel.continent.isna(), \"continent\"] = \"\"\n#label encoding continent\ndf_panel.continent = LabelEncoder().fit_transform(df_panel.continent)\ndf_panel.Date = pd.to_datetime(df_panel.Date, format = \"%Y-%m-%d\")\n\ndf_panel.loc[df_panel['Province_State'].isnull(), 'Province_State'] = 'N\/A'\ndf_panel[\"geography\"] = (df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)).values\n\n\ndf_panel.sort_values([\"geography\", \"Date\"], inplace = True)\n\n## feature engineering\nmin_date_train = np.min(df_panel[~df_panel.Id.isna()].Date)\nmax_date_train = np.max(df_panel[~df_panel.Id.isna()].Date)\n\nmin_date_test = np.min(df_panel[~df_panel.ForecastId.isna()].Date)\nmax_date_test = np.max(df_panel[~df_panel.ForecastId.isna()].Date)\n\nn_dates_test = len(df_panel[~df_panel.ForecastId.isna()].Date.unique())\n\nprint(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n\n# creating lag features\nfor lag in range(1, 41):\n    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].shift(lag)\n    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].shift(lag)\n    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby(\"geography\")[\"Recoveries\"].shift(lag)\n\nfor case in DAYS_SINCE_CASES:\n    df_panel = df_panel.merge(df_panel[df_panel.ConfirmedCases >= case].groupby(\"geography\")[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = \"geography\", how = \"left\")","9da39062":"def prepare_features(df, gap):\n    \n    df[\"perc_1_ac\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) \/ df[f\"lag_{gap}_cc\"]\n    df[\"perc_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df.population\n    \n    df[\"diff_1_cc\"] = df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 1}_cc\"]\n    df[\"diff_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] - df[f\"lag_{gap + 2}_cc\"]\n    df[\"diff_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] - df[f\"lag_{gap + 3}_cc\"]\n    \n    df[\"diff_1_ft\"] = df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 1}_ft\"]\n    df[\"diff_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] - df[f\"lag_{gap + 2}_ft\"]\n    df[\"diff_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] - df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"diff_123_cc\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df[\"diff_123_ft\"] = (df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    df[\"diff_change_1_cc\"] = df.diff_1_cc \/ df.diff_2_cc\n    df[\"diff_change_2_cc\"] = df.diff_2_cc \/ df.diff_3_cc\n    \n    df[\"diff_change_1_ft\"] = df.diff_1_ft \/ df.diff_2_ft\n    df[\"diff_change_2_ft\"] = df.diff_2_ft \/ df.diff_3_ft\n\n    df[\"diff_change_12_cc\"] = (df.diff_change_1_cc + df.diff_change_2_cc) \/ 2\n    df[\"diff_change_12_ft\"] = (df.diff_change_1_ft + df.diff_change_2_ft) \/ 2\n    \n    df[\"change_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 1}_cc\"]\n    df[\"change_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] \/ df[f\"lag_{gap + 2}_cc\"]\n    df[\"change_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n\n    df[\"change_1_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 1}_ft\"]\n    df[\"change_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] \/ df[f\"lag_{gap + 2}_ft\"]\n    df[\"change_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n\n    df[\"change_1_3_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n    df[\"change_1_3_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"change_1_7_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 7}_cc\"]\n    df[\"change_1_7_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 7}_ft\"]\n    \n    for case in DAYS_SINCE_CASES:\n        df[f\"days_since_{case}_case\"] = (df[f\"case_{case}_date\"] - df.Date).astype(\"timedelta64[D]\")\n        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n\n    df[\"country_flag\"] = df.Province_State.isna().astype(int)\n\n    # target variable is log of change from last known value\n    df[\"target_cc\"] = np.log1p(df.ConfirmedCases - df[f\"lag_{gap}_cc\"])\n    df[\"target_ft\"] = np.log1p(df.Fatalities - df[f\"lag_{gap}_ft\"])\n    \n    features = [\n        f\"lag_{gap}_cc\",\n        f\"lag_{gap}_ft\",\n        f\"lag_{gap}_rc\",\n        \"perc_1_ac\",\n        \"perc_1_cc\",\n        \"diff_1_cc\",\n        \"diff_2_cc\",\n        \"diff_3_cc\",\n        \"diff_1_ft\",\n        \"diff_2_ft\",\n        \"diff_3_ft\",\n        \"diff_123_cc\",\n        \"diff_123_ft\",\n        \"diff_change_1_cc\",\n        \"diff_change_2_cc\",\n        \"diff_change_1_ft\",\n        \"diff_change_2_ft\",\n        \"diff_change_12_cc\",\n        \"diff_change_12_ft\",\n        \"change_1_cc\",\n        \"change_2_cc\",\n        \"change_3_cc\",\n        \"change_1_ft\",\n        \"change_2_ft\",\n        \"change_3_ft\",\n        \"change_1_3_cc\",\n        \"change_1_3_ft\",\n        \"change_1_7_cc\",\n        \"change_1_7_ft\",\n        \"days_since_1_case\",\n        \"days_since_10_case\",\n        \"days_since_50_case\",\n        \"days_since_100_case\",\n        \"days_since_500_case\",\n        \"days_since_1000_case\",\n        \"days_since_5000_case\",\n        \"days_since_10000_case\",\n        \"country_flag\",\n        \"lat\",\n        \"lon\",\n        \"continent\",\n        \"population\",\n        \"area\",\n        \"density\",\n        \"target_cc\",\n        \"target_ft\"\n    ]\n    \n    return df[features]","8622b7ab":"gap, VAL_DAYS","0966c34a":"## function for building and predicting using LGBM model\ndef build_predict_lgbm(df_train, df_test, gap):\n    \n    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n    \n    target_cc = df_train.target_cc\n    target_ft = df_train.target_ft\n    \n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    df_train.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    df_test.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    \n    categorical_features = [\"continent\"]\n    \n    dtrain_cc = lgb.Dataset(df_train, label = target_cc, categorical_feature = categorical_features)\n    dtrain_ft = lgb.Dataset(df_train, label = target_ft, categorical_feature = categorical_features)\n\n    model_cc = lgb.train(LGB_PARAMS, train_set = dtrain_cc, num_boost_round = 200)\n    model_ft = lgb.train(LGB_PARAMS, train_set = dtrain_ft, num_boost_round = 200)\n    \n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round = 200)) + test_lag_cc\n    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round = 200)) + test_lag_ft\n    \n    return y_pred_cc, y_pred_ft, model_cc, model_ft\n\n\n\n## function for predicting moving average decay model\ndef predict_mad(df_test, gap, val = False):\n    \n    df_test[\"avg_diff_cc\"] = (df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df_test[\"avg_diff_ft\"] = (df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ n_dates_test\n\n    return y_pred_cc, y_pred_ft\n\n## building lag x-days models\ndf_train = df_panel[~df_panel.Id.isna()]\ndf_test_full = df_panel[~df_panel.ForecastId.isna()]\n\ndf_preds_val = []\ndf_preds_test = []\n\nfor date in df_test_full.Date.unique():\n    \n    print(\"Processing date:\", date)\n    \n    # ignore date already present in train data\n    if date in df_train.Date.values:\n        df_pred_test = df_test_full.loc[df_test_full.Date == date, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(columns = {\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n        \n        # multiplying predictions by 41 to not look cool on public LB\n        #df_pred_test.ConfirmedCases_test = df_pred_test.ConfirmedCases_test * 41\n        #df_pred_test.Fatalities_test = df_pred_test.Fatalities_test * 41\n    else:\n        df_test = df_test_full[df_test_full.Date == date]\n        \n        gap = (pd.Timestamp(date) - max_date_train).days\n        \n        if gap <= VAL_DAYS:\n            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n\n            df_build = df_train[df_train.Date < val_date]\n            df_val = df_train[df_train.Date == val_date]\n            \n            X_build = prepare_features(df_build, gap)\n            X_val = prepare_features(df_val, gap)\n            \n            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build, X_val, gap)\n            y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n            \n            df_pred_val = pd.DataFrame({\"Id\": df_val.Id.values,\n                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n                                        \"ConfirmedCases_val_mad\": y_val_cc_mad,\n                                        \"Fatalities_val_mad\": y_val_ft_mad,\n                                       })\n\n            df_preds_val.append(df_pred_val)\n\n        X_train = prepare_features(df_train, gap)\n        X_test = prepare_features(df_test, gap)\n\n        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train, X_test, gap)\n        y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n        \n        if gap == 1:\n            model_1_cc = model_cc\n            model_1_ft = model_ft\n            features_1 = X_train.columns.values\n        elif gap == 14:\n            model_14_cc = model_cc\n            model_14_ft = model_ft\n            features_14 = X_train.columns.values\n        elif gap == 28:\n            model_28_cc = model_cc\n            model_28_ft = model_ft\n            features_28 = X_train.columns.values\n\n        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n                                     \"ConfirmedCases_test_mad\": y_test_cc_mad,\n                                     \"Fatalities_test_mad\": y_test_ft_mad,\n                                    })\n    \n    df_preds_test.append(df_pred_test)","dd12ad16":"## validation score\n# df_panel = df_panel.merge(pd.concat(df_preds_val, sort = False), on = \"Id\", how = \"left\")\ndf_panel = df_panel.merge(pd.concat(df_preds_test, sort = False), on = \"ForecastId\", how = \"left\")\n\n# rmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb)))\n# rmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities_val_lgb)))\n\n# rmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad)))\n# rmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities_val_mad)))\n\n# print(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\n# print(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\n# print(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) \/ 2, 2))\n# print(\"\\n\")\n# print(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\n# print(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\n# print(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) \/ 2, 2))\n\n# df_panel.loc[~df_panel.ConfirmedCases_val_lgb.isna(), \"ConfirmedCasesPredLGB\"] = df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb\n# df_panel.loc[~df_panel.ConfirmedCases_val_mad.isna(), \"ConfirmedCasesPredMAD\"] = df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad\n\ndf_panel.loc[~df_panel.ConfirmedCases_test_lgb.isna(), \"ConfirmedCasesPredLGB\"] = df_panel[~df_panel.ConfirmedCases_test_lgb.isna()].ConfirmedCases_test_lgb\ndf_panel.loc[~df_panel.ConfirmedCases_test_mad.isna(), \"ConfirmedCasesPredMAD\"] = df_panel[~df_panel.ConfirmedCases_test_mad.isna()].ConfirmedCases_test_mad\n\n# df_panel.loc[~df_panel.Fatalities_val_lgb.isna(), \"FatalitiesPredLGB\"] = df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities_val_lgb\n# df_panel.loc[~df_panel.Fatalities_val_mad.isna(), \"FatalitiesPredMAD\"] = df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities_val_mad\n\ndf_panel.loc[~df_panel.Fatalities_test_lgb.isna(), \"FatalitiesPredLGB\"] = df_panel[~df_panel.Fatalities_test_lgb.isna()].Fatalities_test_lgb\ndf_panel.loc[~df_panel.Fatalities_test_mad.isna(), \"FatalitiesPredMAD\"] = df_panel[~df_panel.Fatalities_test_mad.isna()].Fatalities_test_mad","baf72552":"p_f_vopani_lgb = np.log1p(df_panel.pivot(index='geography', columns='Date', values='FatalitiesPredLGB').sort_index().fillna(0).values)\np_c_vopani_lgb = np.log1p(df_panel.pivot(index='geography', columns='Date', values='ConfirmedCasesPredLGB').sort_index().fillna(0).values)\n\np_f_vopani_mad = np.log1p(df_panel.pivot(index='geography', columns='Date', values='FatalitiesPredMAD').sort_index().fillna(0).values)\np_c_vopani_mad = np.log1p(df_panel.pivot(index='geography', columns='Date', values='ConfirmedCasesPredMAD').sort_index().fillna(0).values)\n\np_f_vopani_lgb = np.maximum.accumulate(p_f_vopani_lgb, axis=1)\np_c_vopani_lgb = np.maximum.accumulate(p_c_vopani_lgb, axis=1)\n\np_f_vopani = 0.2*p_f_vopani_lgb + 0.8*p_f_vopani_mad\np_c_vopani = 0.2*p_c_vopani_lgb + 0.8*p_c_vopani_mad","2f4ce4c0":"p_c_vopani.shape, p_f_vopani.shape","cd4e2404":"if True:\n    #val_len = train_p_c.values.shape[1] - TRAIN_N\n    val_len = TRAIN_N - START_PUBLIC\n    for i in range(val_len):\n        d = i + START_PUBLIC\n        m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, d]), p_c_vopani[:, d]))\n        m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, d]), p_f_vopani[:, d]))\n        print(f\"{d}: {(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n\n    print()\n\n    m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, START_PUBLIC:START_PUBLIC+val_len]).flatten(), p_c_vopani[:, START_PUBLIC:START_PUBLIC+val_len].flatten()))\n    m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, START_PUBLIC:START_PUBLIC+val_len]).flatten(), p_f_vopani[:, START_PUBLIC:START_PUBLIC+val_len].flatten()))\n    print(f\"{(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")","fc2ed4bf":"\nVAL_START_DATE","1733c44f":"import pandas as pd\nimport numpy as np\nimport os\nfrom collections import Counter\nfrom random import shuffle\nimport math\nfrom scipy.stats.mstats import gmean\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib as matplotlib\nimport seaborn as sns","7c862256":"pd.options.display.float_format = '{:.8}'.format\nplt.rcParams[\"figure.figsize\"] = (12, 4.75)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\npd.options.display.max_rows = 999","2be5eecc":"# Parameters - can be changed\nBAGS = 3\nSEED = 123\nSET_FRAC = 0.01\nTRUNCATED = False\nDROPS = True\nPRIVATE = True\n\nUSE_PRIORS = False\nSUP_DROP = 0.0\nACTIONS_DROP = 0.0\nPLACE_FRACTION = 1.0  # 0.4 \n#** FEATURE_DROP = 0.4 # drop random % of features (HIGH!!!, speeds it up)\n#** COUNTRY_DROP = 0.35 # drop random % of countries (20-30pct)\n#** FIRST_DATE_DROP = 0.5 # Date_f must be after a certain date, randomly applied\n# FEATURE_DROP_MAX = 0.3\nLT_DECAY_MAX = 0.3\nLT_DECAY_MIN = -0.4\nSINGLE_MODEL = False\nMODEL_Y = 'agg_dff' # 'slope'  # 'slope' or anything else for difference\/aggregate log gain\n\n","6ce90109":"path = '\/kaggle\/input\/c19week3v2\/'\ninput_path = '\/kaggle\/input\/covid19-global-forecasting-week-4\/'\ntrain = pd.read_csv(input_path + 'train.csv')\ntest = pd.read_csv(input_path  + 'test.csv')\nsub = pd.read_csv(input_path + 'submission.csv')\ntt = pd.merge(train, test, on=['Country_Region', 'Province_State', 'Date'], how='right', validate=\"1:1\").fillna(method = 'ffill')\npublic = tt[['ForecastId', 'ConfirmedCases', 'Fatalities']]\ntest_dates = test.Date.unique()\npp = 'public'\nif PRIVATE:\n    test = test[ pd.to_datetime(test.Date) >  train.Date.max()]\n    pp = 'private'\ntrain.Date = pd.to_datetime(train.Date)","fcb5583a":"revised = pd.read_csv(path + 'outside_data\/covid19_train_data_us_states_before_march_09_new.csv')\nrevised = revised[['Province_State', 'Country_Region', 'Date', 'ConfirmedCases', 'Fatalities']]\nrevised.Date = pd.to_datetime(revised.Date)\nrev_train = pd.merge(train, revised, on=['Province_State', 'Country_Region', 'Date'],suffixes = ('', '_r'), how='left')\nrev_train[~rev_train.ConfirmedCases_r.isnull()].head()\nrev_train.ConfirmedCases = np.where( (rev_train.ConfirmedCases == 0) & ((rev_train.ConfirmedCases_r > 0 )) & (rev_train.Country_Region == 'US'), rev_train.ConfirmedCases_r,rev_train.ConfirmedCases)\nrev_train.Fatalities =np.where( ~rev_train.Fatalities_r.isnull() & (rev_train.Fatalities == 0) & ((rev_train.Fatalities_r > 0 )) & (rev_train.Country_Region == 'US') ,rev_train.Fatalities_r,rev_train.Fatalities)\nrev_train.drop(columns = ['ConfirmedCases_r', 'Fatalities_r'], inplace=True)\ntrain = rev_train","c63543aa":"contain_data = pd.read_csv(path + 'outside_data\/OxCGRT_Download_070420_160027_Full.csv')\ndelcols = ['_Notes','Unnamed', 'Confirmed','CountryCode','S8', 'S9', 'S10','S11', 'StringencyIndexForDisplay']\ncontain_data = contain_data[[c for c in contain_data.columns if not any(z in c for z in delcols)] ]\ncontain_data.rename(columns = {'CountryName': \"Country\"}, inplace=True)\ncontain_data.Date = contain_data.Date.astype(str).apply(datetime.datetime.strptime, args=('%Y%m%d', ))\ncontain_data_orig = contain_data.copy()\ncds = []\nfor country in contain_data.Country.unique():\n    cd = contain_data[contain_data.Country==country]\n    cd = cd.fillna(method = 'ffill').fillna(0)\n    cd.StringencyIndex = cd.StringencyIndex.cummax()  # for now\n    col_count = cd.shape[1]\n    \n    # now do a diff columns\n    # and ewms of it\n    for col in [c for c in contain_data.columns if 'S' in c]:\n        col_diff = cd[col].diff()\n        cd[col+\"_chg_5d_ewm\"] = col_diff.ewm(span = 5).mean()\n        cd[col+\"_chg_20_ewm\"] = col_diff.ewm(span = 20).mean()\n        \n    # stringency\n    cd['StringencyIndex_5d_ewm'] = cd.StringencyIndex.ewm(span = 5).mean()\n    cd['StringencyIndex_20d_ewm'] = cd.StringencyIndex.ewm(span = 20).mean()\n    \n    cd['S_data_days'] =  (cd.Date - cd.Date.min()).dt.days\n    for s in [1, 10, 20, 30, 50, ]:\n        cd['days_since_Stringency_{}'.format(s)] = \\\n                np.clip((cd.Date - cd[(cd.StringencyIndex > s)].Date.min()).dt.days, 0, None)\n    \n    \n    cds.append(cd.fillna(0)[['Country', 'Date'] + cd.columns.to_list()[col_count:]])\ncontain_data = pd.concat(cds)\ncontain_data.Country.replace({ 'United States': \"US\",'South Korea': \"Korea, South\",'Taiwan': \"Taiwan*\",'Myanmar': \"Burma\", 'Slovak Republic': \"Slovakia\",'Czech Republic': 'Czechia'}, inplace=True)\n\nsup_data = pd.read_excel(path + 'outside_data\/Data Join - Copy1.xlsx')\nsup_data.columns = [c.replace(' ', '_') for c in sup_data.columns.to_list()]\nsup_data.drop(columns = [c for c in sup_data.columns.to_list() if 'Unnamed:' in c], inplace=True)\nsup_data.drop(columns = [ 'Date', 'ConfirmedCases','Fatalities', 'log-cases', 'log-fatalities', 'continent'], inplace=True)\nsup_data['Migrants_in'] = np.clip(sup_data.Migrants, 0, None)\nsup_data['Migrants_out'] = -np.clip(sup_data.Migrants, None, 0)\nsup_data.drop(columns = 'Migrants', inplace=True)","939a3fe2":"train.Date = pd.to_datetime(train.Date)\ntest.Date = pd.to_datetime(test.Date)\ntrain.rename(columns={'Country_Region': 'Country'}, inplace=True)\ntest.rename(columns={'Country_Region': 'Country'}, inplace=True)\nsup_data.rename(columns={'Country_Region': 'Country'}, inplace=True)\n\n# train['Place'] = train.Country + train.Province_State.fillna(\"\")\n# test['Place'] = test.Country +  test.Province_State.fillna(\"\")\n# sup_data['Place'] = sup_data.Country +  sup_data.Province_State.fillna(\"\")\n\ntrain.loc[train['Province_State'].isnull(), 'Province_State'] = 'N\/A'\ntest.loc[test['Province_State'].isnull(), 'Province_State'] = 'N\/A'\nsup_data.loc[sup_data['Province_State'].isnull(), 'Province_State'] = 'N\/A'\n\ntrain['Place'] = train['Country'] + '_' + train['Province_State']\ntest['Place'] = test['Country'] + '_' + test['Province_State']\nsup_data['Place'] = sup_data['Country'] + '_' + sup_data['Province_State']\n\n\n\nsup_data = sup_data[sup_data.columns.to_list()[2:]]\nsup_data = sup_data.replace('N.A.', np.nan).fillna(-0.5)\nfor c in sup_data.columns[:-1]:\n    m = sup_data[c].max() #- sup_data \n    \n    if m > 300 and c!='TRUE_POPULATION':\n        print(c)\n        sup_data[c] = np.log(sup_data[c] + 1)\n        assert sup_data[c].min() > -1\n\nfor c in sup_data.columns[:-1]:\n    m = sup_data[c].max() #- sup_data \n    \n    if m > 300:\n        print(c)","7907c66f":"DEATHS = 'Fatalities'\ntrain.ConfirmedCases = \\\n    np.where(\n        (train.ConfirmedCases.shift(1) > train.ConfirmedCases) & \n        (train.ConfirmedCases.shift(1) > 0) & (train.ConfirmedCases.shift(-1) > 0) &\n         (train.Place == train.Place.shift(1)) & (train.Place == train.Place.shift(-1)) & \n        ~train.ConfirmedCases.shift(-1).isnull(),\n        \n        np.sqrt(train.ConfirmedCases.shift(1) * train.ConfirmedCases.shift(-1)),\n        \n        train.ConfirmedCases)\n\ntrain.Fatalities = \\\n    np.where(\n        (train.Fatalities.shift(1) > train.Fatalities) & \n        (train.Fatalities.shift(1) > 0) & (train.Fatalities.shift(-1) > 0) &\n         (train.Place == train.Place.shift(1)) & (train.Place == train.Place.shift(-1)) & \n        ~train.Fatalities.shift(-1).isnull(),\n        \n        np.sqrt(train.Fatalities.shift(1) * train.Fatalities.shift(-1)),\n        \n        train.Fatalities)\n\nfor i in [0, -1]:\n    train.ConfirmedCases = \\\n        np.where(\n            (train.ConfirmedCases.shift(2+ i ) > train.ConfirmedCases) & \n            (train.ConfirmedCases.shift(2+ i) > 0) & (train.ConfirmedCases.shift(-1+ i) > 0) &\n         (train.Place == train.Place.shift(2+ i)) & (train.Place == train.Place.shift(-1+ i)) & \n            ~train.ConfirmedCases.shift(-1+ i).isnull(),\n\n            np.sqrt(train.ConfirmedCases.shift(2+ i) * train.ConfirmedCases.shift(-1+ i)),train.ConfirmedCases)\n\ntrain_bk = train.copy()\nfull_train = train.copy()\ntrain_c = train[train.Country == 'China']\ntrain_nc = train[train.Country != 'China']\ntrain_us = train[train.Country == 'US']","497d1268":"def lplot(data, minDate = datetime.datetime(2000, 1, 1), columns = ['ConfirmedCases', 'Fatalities']):\n    return\n        \nREAL = datetime.datetime(2020, 2, 10)\ndataset = train.copy()\n\nif TRUNCATED:\n    dataset = dataset[dataset.Country.isin(['Italy', 'Spain', 'Germany', 'Portugal', 'Belgium', 'Austria', 'Switzerland' ])]\n\ndataset.head()","8cd2cc11":"def rollDates(df, i, preserve=False):\n    df = df.copy()\n    if preserve:\n        df['Date_i'] = df.Date\n    df.Date = df.Date + datetime.timedelta(i)\n    return df\n\nWINDOWS = [1, 2,  4, 7, 12, 20, 30]\nfor window in WINDOWS:\n    csuffix = '_{}d_prior_value'.format(window)\n    \n    base = rollDates(dataset, window)\n    dataset = pd.merge(dataset, base[['Date', 'Place',\n                'ConfirmedCases', 'Fatalities']], on = ['Date', 'Place'],\n            suffixes = ('', csuffix), how='left')\n    for c in ['ConfirmedCases', 'Fatalities']:\n        dataset[c+ csuffix].fillna(0, inplace=True)\n        dataset[c+ csuffix] = np.log(dataset[c + csuffix] + 1)\n        dataset[c+ '_{}d_prior_slope'.format(window)] = \\\n                    (np.log(dataset[c] + 1) \\\n                         - dataset[c+ csuffix]) \/ window\n        dataset[c+ '_{}d_ago_zero'.format(window)] = 1.0*(dataset[c+ csuffix] == 0)     \n    \nfor window1 in WINDOWS:\n    for window2 in WINDOWS:\n        for c in ['ConfirmedCases', 'Fatalities']:\n            if window1 * 1.3 < window2 and window1 * 5 > window2:\n                dataset[ c +'_{}d_{}d_prior_slope_chg'.format(window1, window2) ] = \\\n                        dataset[c+ '_{}d_prior_slope'.format(window1)] \\\n                                - dataset[c+ '_{}d_prior_slope'.format(window2)]\n                \n                ","1ad2d985":"print('start features')\n\nfirst_case = dataset[dataset.ConfirmedCases >= 1].groupby('Place').min() \ntenth_case = dataset[dataset.ConfirmedCases >= 10].groupby('Place').min()\nhundredth_case = dataset[dataset.ConfirmedCases >= 100].groupby('Place').min()\nthousandth_case = dataset[dataset.ConfirmedCases >= 1000].groupby('Place').min()\n\n# %% [code]\nfirst_fatality = dataset[dataset.Fatalities >= 1].groupby('Place').min()\ntenth_fatality = dataset[dataset.Fatalities >= 10].groupby('Place').min()\nhundredth_fatality = dataset[dataset.Fatalities >= 100].groupby('Place').min()\nthousandth_fatality = dataset[dataset.Fatalities >= 1000].groupby('Place').min()\n\n\ndataset['days_since_first_case'] = np.clip((dataset.Date - first_case.loc[dataset.Place].Date.values).dt.days.fillna(-1), -1, None)\ndataset['days_since_tenth_case'] = np.clip((dataset.Date - tenth_case.loc[dataset.Place].Date.values).dt.days.fillna(-1), -1, None)\ndataset['days_since_hundredth_case'] = np.clip((dataset.Date - hundredth_case.loc[dataset.Place].Date.values).dt.days.fillna(-1), -1, None)\ndataset['days_since_thousandth_case'] = np.clip((dataset.Date - thousandth_case.loc[dataset.Place].Date.values).dt.days.fillna(-1), -1, None)\n\ndataset['days_since_first_fatality'] = np.clip((dataset.Date - first_fatality.loc[dataset.Place].Date.values).dt.days.fillna(-1), -1, None)\ndataset['days_since_tenth_fatality'] = np.clip((dataset.Date - tenth_fatality.loc[dataset.Place].Date.values).dt.days.fillna(-1), -1, None)\ndataset['days_since_hundredth_fatality'] = np.clip((dataset.Date - hundredth_fatality.loc[dataset.Place].Date.values).dt.days.fillna(-1), -1, None)\ndataset['days_since_thousandth_fatality'] = np.clip((dataset.Date - thousandth_fatality.loc[dataset.Place].Date.values).dt.days.fillna(-1), -1, None)\n\ndataset['case_rate_since_first_case'] = np.clip((np.log(dataset.ConfirmedCases + 1) - np.log(first_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1))\/ (dataset.days_since_first_case+0.01), 0, 1)\ndataset['case_rate_since_tenth_case'] = np.clip((np.log(dataset.ConfirmedCases + 1) - np.log(tenth_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1))\/ (dataset.days_since_tenth_case+0.01), 0, 1)\ndataset['case_rate_since_hundredth_case'] = np.clip((np.log(dataset.ConfirmedCases + 1) - np.log(hundredth_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1)) \/ (dataset.days_since_first_case+0.01), 0, 1)\ndataset['case_rate_since_thousandth_case'] = np.clip((np.log(dataset.ConfirmedCases + 1) - np.log(thousandth_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1)) \/ (dataset.days_since_first_case+0.01), 0, 1)\n\ndataset['fatality_rate_since_first_case'] =np.clip((np.log(dataset.Fatalities + 1) - np.log(first_case.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \/ (dataset.days_since_first_case+0.01), 0, 1)\ndataset['fatality_rate_since_tenth_case'] = np.clip((np.log(dataset.Fatalities + 1) - np.log(tenth_case.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \/ (dataset.days_since_first_case+0.01), 0, 1)\ndataset['fatality_rate_since_hundredth_case'] = np.clip((np.log(dataset.Fatalities + 1) - np.log(hundredth_case.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \/ (dataset.days_since_first_case+0.01), 0, 1)\ndataset['fatality_rate_since_thousandth_case'] = np.clip((np.log(dataset.Fatalities + 1) - np.log(thousandth_case.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \/ (dataset.days_since_first_case+0.01), 0, 1)\n\ndataset['fatality_rate_since_first_fatality'] = np.clip((np.log(dataset.Fatalities + 1) - np.log(first_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \/ (dataset.days_since_first_fatality+0.01), 0, 1)\ndataset['fatality_rate_since_tenth_fatality'] = np.clip((np.log(dataset.Fatalities + 1) - np.log(tenth_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \/ (dataset.days_since_tenth_fatality+0.01), 0, 1)\ndataset['fatality_rate_since_hundredth_fatality'] = np.clip((np.log(dataset.Fatalities + 1) - np.log(hundredth_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \/ (dataset.days_since_hundredth_fatality+0.01), 0, 1)\ndataset['fatality_rate_since_thousandth_fatality'] = np.clip((np.log(dataset.Fatalities + 1) - np.log(thousandth_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \/ (dataset.days_since_thousandth_fatality+0.01), 0, 1)\n \ndataset['first_case_ConfirmedCases'] = np.log(first_case.loc[dataset.Place].ConfirmedCases.values + 1)\ndataset['first_case_Fatalities'] = np.log(first_case.loc[dataset.Place].Fatalities.values + 1)\n\ndataset['first_fatality_ConfirmedCases'] = np.log(first_fatality.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1) * (dataset.days_since_first_fatality >= 0 )\ndataset['first_fatality_Fatalities'] =np.log(first_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1) * (dataset.days_since_first_fatality >= 0 )\n\ndataset['first_fatality_cfr'] =np.where(dataset.days_since_first_fatality < 0,-8,(dataset.first_fatality_Fatalities) -(dataset.first_fatality_ConfirmedCases ))\ndataset['first_fatality_lag_vs_first_case'] = np.where(dataset.days_since_first_fatality >= 0,dataset.days_since_first_case - dataset.days_since_first_fatality , -1)\n\ndataset['case_chg'] = np.clip(np.log(dataset.ConfirmedCases + 1 )- np.log(dataset.ConfirmedCases.shift(1) +1), 0, None).fillna(0)\ndataset['case_chg_ema_3d'] = dataset.case_chg.ewm(span = 3).mean() * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/3, 0, 1)\ndataset['case_chg_ema_10d'] = dataset.case_chg.ewm(span = 10).mean() * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/10, 0, 1)\ndataset['case_chg_stdev_5d'] = dataset.case_chg.rolling(5).std() * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/5, 0, 1)\ndataset['case_chg_stdev_15d'] = dataset.case_chg.rolling(15).std() * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/15, 0, 1)\ndataset['case_update_pct_3d_ewm'] = (dataset.case_chg > 0).ewm(span = 3).mean() * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/3, 0, 1), 2)\ndataset['case_update_pct_10d_ewm'] = (dataset.case_chg > 0).ewm(span = 10).mean() * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/10, 0, 1), 2)\ndataset['case_update_pct_30d_ewm'] = (dataset.case_chg > 0).ewm(span = 30).mean() * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/30, 0, 1), 2)\ndataset['fatality_chg'] = np.clip(np.log(dataset.Fatalities + 1 )- np.log(dataset.Fatalities.shift(1) +1), 0, None).fillna(0)\ndataset['fatality_chg_ema_3d'] = dataset.fatality_chg.ewm(span = 3).mean() * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/33, 0, 1)\ndataset['fatality_chg_ema_10d'] = dataset.fatality_chg.ewm(span = 10).mean() * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/10, 0, 1)\ndataset['fatality_chg_stdev_5d'] = dataset.fatality_chg.rolling(5).std() * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/5, 0, 1)\ndataset['fatality_chg_stdev_15d'] = dataset.fatality_chg.rolling(15).std() * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/15, 0, 1)\ndataset['fatality_update_pct_3d_ewm'] = (dataset.fatality_chg > 0).ewm(span = 3).mean() * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/3, 0, 1), 2)\ndataset['fatality_update_pct_10d_ewm'] = (dataset.fatality_chg > 0).ewm(span = 10).mean() * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/10, 0, 1), 2)\ndataset['fatality_update_pct_30d_ewm'] = (dataset.fatality_chg > 0).ewm(span = 30).mean() * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/30, 0, 1), 2)\ndataset.tail()","473f3714":"contain_data.Date = contain_data.Date + datetime.timedelta(7)\ncontain_data.Date.max()\ndataset = pd.merge(dataset, sup_data, on='Place', how='left', validate='m:1')\ndataset = pd.merge(dataset, contain_data, on = ['Country', 'Date'], how='left', validate='m:1')\ndataset['log_true_population'] =   np.log(dataset.TRUE_POPULATION + 1)\ndataset['ConfirmedCases_percapita'] = np.log(dataset.ConfirmedCases + 1)- np.log(dataset.TRUE_POPULATION + 1)\ndataset['Fatalities_percapita'] = np.log(dataset.Fatalities + 1)- np.log(dataset.TRUE_POPULATION + 1)\ndataset['log_cfr'] = np.log(    (dataset.Fatalities + np.clip(0.015 * dataset.ConfirmedCases, 0, 0.3)) \/ ( dataset.ConfirmedCases + 0.1) )\n","06d74b4e":"def cfr(case, fatality):\n    cfr_calc = np.log((fatality + np.clip(0.015 * case, 0, 0.3)) \/ ( case + 0.1) )\n    return np.where(np.isnan(cfr_calc) | np.isinf(cfr_calc), BLCFR, cfr_calc)\n\nBLCFR = np.median(dataset[dataset.ConfirmedCases==1].log_cfr[::10])\ndataset.log_cfr.fillna(BLCFR, inplace=True)\ndataset.log_cfr = np.where(dataset.log_cfr.isnull() | np.isinf(dataset.log_cfr),BLCFR, dataset.log_cfr)\nBLCFR","0f393de1":"print('rates and cross effects')\ndataset['log_cfr_3d_ewm'] = BLCFR + (dataset.log_cfr - BLCFR).ewm(span = 3).mean() * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/3, 0, 1), 2)\ndataset['log_cfr_8d_ewm'] = BLCFR + (dataset.log_cfr - BLCFR).ewm(span = 8).mean() * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/8, 0, 1), 2)\ndataset['log_cfr_20d_ewm'] = BLCFR + (dataset.log_cfr - BLCFR).ewm(span = 20).mean()  * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/20, 0, 1), 2)\ndataset['log_cfr_3d_20d_ewm_crossover'] = dataset.log_cfr_3d_ewm - dataset.log_cfr_20d_ewm\ndataset.drop(columns = 'log_cfr', inplace=True)\ndate_totals = dataset.groupby('Date').sum()\nmean_7d_c_slope = dataset.groupby('Date')[['ConfirmedCases_7d_prior_slope']].apply(lambda x: np.mean(x[x > 0]) ).ewm(span = 3).mean() \nmean_7d_f_slope = dataset.groupby('Date')[['Fatalities_7d_prior_slope']].apply(lambda x: np.mean(x[x > 0]) ).ewm(span = 7).mean()\ndataset['ConfirmedCases_percapita_vs_world'] = np.log(dataset.ConfirmedCases + 1)- np.log(dataset.TRUE_POPULATION + 1) -  (np.log(date_totals.loc[dataset.Date].ConfirmedCases + 1)-np.log(date_totals.loc[dataset.Date].TRUE_POPULATION + 1)).values\ndataset['Fatalities_percapita_vs_world'] = np.log(dataset.Fatalities + 1)- np.log(dataset.TRUE_POPULATION + 1)-(np.log(date_totals.loc[dataset.Date].Fatalities + 1) -np.log(date_totals.loc[dataset.Date].TRUE_POPULATION + 1)).values\ndataset['cfr_vs_world'] = dataset.log_cfr_3d_ewm - np.log(date_totals.loc[dataset.Date].Fatalities \/ date_totals.loc[dataset.Date].ConfirmedCases ).values\n\ncont_date_totals = dataset.groupby(['Date', 'continent_generosity']).sum()\n\ndataset['ConfirmedCases_percapita_vs_continent_mean'] = 0\ndataset['Fatalities_percapita_vs_continent_mean'] = 0\ndataset['ConfirmedCases_percapita_vs_continent_median'] = 0\ndataset['Fatalities_percapita_vs_continent_median'] = 0\n\nfor cg in dataset.continent_generosity.unique():\n    ps = dataset.groupby(\"Place\").last()\n    tp = ps[ps.continent_generosity==cg].TRUE_POPULATION.sum()\n    print(tp \/ 1e9)\n    for Date in dataset.Date.unique():\n        cd =  dataset[(dataset.Date == Date) & (dataset.continent_generosity == cg)][['ConfirmedCases', 'Fatalities', 'TRUE_POPULATION']]\n#         print(cd)\n        cmedian = np.median(np.log(cd.ConfirmedCases + 1)- np.log(cd.TRUE_POPULATION+1))\n        cmean = np.log(cd.ConfirmedCases.sum() + 1) - np.log(tp + 1)\n        fmedian = np.median(np.log(cd.Fatalities + 1)- np.log(cd.TRUE_POPULATION+1))\n        fmean = np.log(cd.Fatalities.sum() + 1) - np.log(tp + 1)\n        cfrmean = cfr( cd.ConfirmedCases.sum(),  cd.Fatalities.sum()   ) \n        dataset.loc[(dataset.Date == Date) & (dataset.continent_generosity == cg),'ConfirmedCases_percapita_vs_continent_mean'] = dataset['ConfirmedCases_percapita'] - (cmean)\n        dataset.loc[(dataset.Date == Date) & (dataset.continent_generosity == cg),'ConfirmedCases_percapita_vs_continent_median'] = dataset['ConfirmedCases_percapita'] - (cmedian)\n        dataset.loc[(dataset.Date == Date) & (dataset.continent_generosity == cg),'Fatalities_percapita_vs_continent_mean'] = dataset['Fatalities_percapita'] - (fmean)\n        dataset.loc[(dataset.Date == Date) & (dataset.continent_generosity == cg),'Fatalities_percapita_vs_continent_median'] = dataset['Fatalities_percapita']- (fmedian)\n        dataset.loc[(dataset.Date == Date) &(dataset.continent_generosity == cg),'cfr_vs_continent'] = dataset.log_cfr_3d_ewm - cfrmean\n        \nprint('handling location')\nall_places = dataset[['Place', 'latitude', 'longitude']].drop_duplicates().set_index('Place',drop=True)\n\ndef surroundingPlaces(place, d = 10):\n    dist = (all_places.latitude - all_places.loc[place].latitude)**2 + (all_places.longitude - all_places.loc[place].longitude) ** 2 \n    return all_places[dist < d**2][1:n+1]\n\ndef nearestPlaces(place, n = 10):\n    dist = (all_places.latitude - all_places.loc[place].latitude)**2 + (all_places.longitude - all_places.loc[place].longitude) ** 2\n    ranked = np.argsort(dist) \n    return all_places.iloc[ranked][1:n+1]\n\n\ndgp = dataset.groupby('Place').last()\nfor n in [5, 10, 20]:\n\n    for place in dataset.Place.unique():\n        nps = nearestPlaces(place, n)\n        tp = dgp.loc[nps.index].TRUE_POPULATION.sum()\n\n        dataset.loc[dataset.Place==place,'ratio_population_vs_nearest{}'.format(n)] = np.log(dataset.loc[dataset.Place==place].TRUE_POPULATION.mean() + 1)- np.log(tp+1)\n        nbps =  dataset[(dataset.Place.isin(nps.index))].groupby('Date')[['ConfirmedCases', 'Fatalities']].sum()\n\n        nppc = (np.log( nbps.loc[dataset[dataset.Place==place].Date].fillna(0).ConfirmedCases + 1) - np.log(tp + 1))\n        nppf = (np.log( nbps.loc[dataset[dataset.Place==place].Date].fillna(0).Fatalities + 1) - np.log(tp + 1))\n        npp_cfr = cfr( nbps.loc[dataset[dataset.Place==place].Date].fillna(0).ConfirmedCases,nbps.loc[dataset[dataset.Place==place].Date].fillna(0).Fatalities)\n        dataset.loc[(dataset.Place == place),'ConfirmedCases_percapita_vs_nearest{}'.format(n)] = dataset[(dataset.Place == place)].ConfirmedCases_percapita - nppc.values\n        dataset.loc[(dataset.Place == place),'Fatalities_percapita_vs_nearest{}'.format(n)] = dataset[(dataset.Place == place)].Fatalities_percapita - nppf.values\n        dataset.loc[(dataset.Place == place),'cfr_vs_nearest{}'.format(n)] = dataset[(dataset.Place == place)].log_cfr_3d_ewm - npp_cfr   \n        dataset.loc[(dataset.Place == place),'ConfirmedCases_nearest{}_percapita'.format(n)] = nppc.values\n        dataset.loc[(dataset.Place == place),'Fatalities_nearest{}_percapita'.format(n)] = nppf.values\n        dataset.loc[(dataset.Place == place),'cfr_nearest{}'.format(n)] = npp_cfr\n        dataset.loc[(dataset.Place == place),'ConfirmedCases_nearest{}_10d_slope'.format(n)] = ( nppc.ewm(span = 1).mean() - nppc.ewm(span = 10).mean() ).values\n        dataset.loc[(dataset.Place == place),'Fatalities_nearest{}_10d_slope'.format(n)] =  ( nppf.ewm(span = 1).mean() - nppf.ewm(span = 10).mean() ).values\n        \n        npp_cfr_s = pd.Series(npp_cfr)\n        dataset.loc[(dataset.Place == place),'cfr_nearest{}_10d_slope'.format(n)] = ( npp_cfr_s.ewm(span = 1).mean()- npp_cfr_s.ewm(span = 10).mean() ) .values\n\ndgp = dataset.groupby('Place').last()\nfor d in [5, 10, 20]:\n\n    for place in dataset.Place.unique():\n        nps = surroundingPlaces(place, d)\n        dataset.loc[dataset.Place==place, 'num_surrounding_places_{}_degrees'.format(d)] = len(nps)\n        tp = dgp.loc[nps.index].TRUE_POPULATION.sum()\n        dataset.loc[dataset.Place==place,'ratio_population_vs_surrounding_places_{}_degrees'.format(d)] = np.log(dataset.loc[dataset.Place==place].TRUE_POPULATION.mean() + 1)- np.log(tp+1)\n        \n        if len(nps)==0:\n            continue;\n        nbps =  dataset[(dataset.Place.isin(nps.index))].groupby('Date')[['ConfirmedCases', 'Fatalities']].sum()\n        nppc = (np.log( nbps.loc[dataset[dataset.Place==place].Date].fillna(0).ConfirmedCases + 1) - np.log(tp + 1))\n        nppf = (np.log( nbps.loc[dataset[dataset.Place==place].Date].fillna(0).Fatalities + 1) - np.log(tp + 1))\n        npp_cfr = cfr( nbps.loc[dataset[dataset.Place==place].Date].fillna(0).ConfirmedCases,nbps.loc[dataset[dataset.Place==place].Date].fillna(0).Fatalities)\n        dataset.loc[(dataset.Place == place),'ConfirmedCases_percapita_vs_surrounding_places_{}_degrees'.format(d)] = dataset[(dataset.Place == place)].ConfirmedCases_percapita - nppc.values\n        dataset.loc[(dataset.Place == place),'Fatalities_percapita_vs_surrounding_places_{}_degrees'.format(d)] = dataset[(dataset.Place == place)].Fatalities_percapita - nppf.values\n        dataset.loc[(dataset.Place == place),'cfr_vs_surrounding_places_{}_degrees'.format(d)] = dataset[(dataset.Place == place)].log_cfr_3d_ewm - npp_cfr   \n        dataset.loc[(dataset.Place == place),'ConfirmedCases_surrounding_places_{}_degrees_percapita'.format(d)] = nppc.values\n        dataset.loc[(dataset.Place == place),'Fatalities_surrounding_places_{}_degrees_percapita'.format(d)] = nppf.values\n        dataset.loc[(dataset.Place == place),'cfr_surrounding_places_{}_degrees'.format(d)] = npp_cfr\n        \n        dataset.loc[(dataset.Place == place),'ConfirmedCases_surrounding_places_{}_degrees_10d_slope'.format(d)] = ( nppc.ewm(span = 1).mean() - nppc.ewm(span = 10).mean() ).values\n        dataset.loc[(dataset.Place == place),'Fatalities_surrounding_places_{}_degrees_10d_slope'.format(d)] = ( nppf.ewm(span = 1).mean() - nppf.ewm(span = 10).mean() ).values\n        npp_cfr_s = pd.Series(npp_cfr)\n        dataset.loc[(dataset.Place == place),'cfr_surrounding_places_{}_degrees_10d_slope'.format(d)] = ( npp_cfr_s.ewm(span = 1).mean()- npp_cfr_s.ewm(span = 10).mean() ) .values\n        \nfor col in [c for c in dataset.columns if 'surrounding_places' in c and 'num_sur' not in c]:\n    dataset[col] = dataset[col].fillna(0)\n    n_col = 'num_surrounding_places_{}_degrees'.format(col.split('degrees')[0].split('_')[-2])\n\n    print(col)\n    dataset[col + \"_times_num_places\"] = dataset[col] * np.sqrt(dataset[n_col])\ndataset[dataset.Country=='US'][['Place', 'Date'] + [c for c in dataset.columns if 'ratio_p' in c]][::50]\n\ndataset['first_case_ConfirmedCases_percapita'] = np.log(dataset.first_case_ConfirmedCases + 1) - np.log(dataset.TRUE_POPULATION + 1)\ndataset['first_case_Fatalities_percapita'] = np.log(dataset.first_case_Fatalities + 1) - np.log(dataset.TRUE_POPULATION + 1)\ndataset['first_fatality_Fatalities_percapita'] = np.log(dataset.first_fatality_Fatalities + 1) - np.log(dataset.TRUE_POPULATION + 1)\ndataset['first_fatality_ConfirmedCases_percapita'] = np.log(dataset.first_fatality_ConfirmedCases + 1)- np.log(dataset.TRUE_POPULATION + 1)\ndataset['days_to_saturation_ConfirmedCases_4d'] = ( - np.log(dataset.ConfirmedCases + 1)+ np.log(dataset.TRUE_POPULATION + 1)) \/ dataset.ConfirmedCases_4d_prior_slope         \ndataset['days_to_saturation_ConfirmedCases_7d'] = ( - np.log(dataset.ConfirmedCases + 1)+ np.log(dataset.TRUE_POPULATION + 1)) \/ dataset.ConfirmedCases_7d_prior_slope         \ndataset['days_to_saturation_Fatalities_20d_cases'] = ( - np.log(dataset.Fatalities + 1)+ np.log(dataset.TRUE_POPULATION + 1)) \/ dataset.ConfirmedCases_20d_prior_slope         \ndataset['days_to_saturation_Fatalities_12d_cases'] = ( - np.log(dataset.Fatalities + 1)+ np.log(dataset.TRUE_POPULATION + 1)) \/ dataset.ConfirmedCases_12d_prior_slope         \ndataset['days_to_3pct_ConfirmedCases_4d'] = ( - np.log(dataset.ConfirmedCases + 1)+ np.log(dataset.TRUE_POPULATION + 1) - 3.5) \/ dataset.ConfirmedCases_4d_prior_slope         \ndataset['days_to_3pct_ConfirmedCases_7d'] = ( - np.log(dataset.ConfirmedCases + 1)+ np.log(dataset.TRUE_POPULATION + 1) - 3.5) \/ dataset.ConfirmedCases_7d_prior_slope         \ndataset['days_to_0.3pct_Fatalities_20d_cases'] = ( - np.log(dataset.Fatalities + 1)+ np.log(dataset.TRUE_POPULATION + 1) - 5.8)\/ dataset.ConfirmedCases_20d_prior_slope         \ndataset['days_to_0.3pct_Fatalities_12d_cases'] = ( - np.log(dataset.Fatalities + 1)+ np.log(dataset.TRUE_POPULATION + 1) - 5.8) \/ dataset.ConfirmedCases_12d_prior_slope         \ndataset.tail()","d6c5d556":"print('rolling dates')\ndataset = dataset[dataset.ConfirmedCases > 0]\ndatas = []\nfor window in range(1, 35):\n    base = rollDates(dataset, window, True)\n    datas.append(pd.merge(dataset[['Date', 'Place','ConfirmedCases', 'Fatalities']], base, on = ['Date', 'Place'],how = 'right',suffixes = ('_f', '')))\ndata = pd.concat(datas, axis =0).astype(np.float32, errors ='ignore')\n\ndata['Date_f'] = data.Date\ndata.Date = data.Date_i\n\ndata['elapsed'] = (data.Date_f - data.Date_i).dt.days\ndata['CaseChgRate'] = (np.log(data.ConfirmedCases_f + 1) - np.log(data.ConfirmedCases + 1))\/ data.elapsed\ndata['FatalityChgRate'] = (np.log(data.Fatalities_f + 1) - np.log(data.Fatalities + 1))\/ data.elapsed\n","fefba3c0":"# find HK and MACAU Area\n#data['is_China'] = (data.Country=='China') & (~data.Place.isin(['Hong Kong', 'Macau']))","39f473ad":"print('true aggregation')\nfalloff_hash = {}\n\ndef true_agg(rate_i, elapsed, bend_rate):\n    elapsed = int(elapsed)\n    if (bend_rate, elapsed) not in falloff_hash:\n        falloff_hash[(bend_rate, elapsed)] = np.sum( [  np.power(bend_rate, e) for e in range(1, elapsed+1)] )\n    return falloff_hash[(bend_rate, elapsed)] * rate_i\n     \n\n#true_agg(0.3, 30, 0.9)\n\nslope_cols = [c for c in data.columns if any(z in c for z in ['prior_slope', 'chg', 'rate']) and not any(z in c for z in ['bend', 'prior_slope_chg', 'Country', 'ewm',]) ] # ** bid change; since rate too stationary\n# print(slope_cols)\nbend_rates = [1, 0.95, 0.90]\nfor bend_rate in bend_rates:\n    bend_agg = data[['elapsed']].apply(lambda x: true_agg(1, *x, bend_rate), axis=1)\n     \n    for sc in slope_cols:\n        if bend_rate < 1:\n            data[sc+\"_slope_bend_{}\".format(bend_rate)] =  data[sc]  * np.power((bend_rate + 1)\/2, data.elapsed)\n            data[sc+\"_true_slope_bend_{}\".format(bend_rate)] = bend_agg *  data[sc] \/ data.elapsed\n            \n        data[sc+\"_agg_bend_{}\".format(bend_rate)] =  data[sc] * data.elapsed * np.power((bend_rate + 1)\/2, data.elapsed)\n        data[sc+\"_true_agg_bend_{}\".format(bend_rate)] = bend_agg *  data[sc]\n\nfor col in [c for c in data.columns if any(z in c for z in['vs_continent', 'nearest', 'vs_world', 'surrounding_places'])]:\n    data[col + '_times_days'] = data[col] * data.elapsed\n\n\ndata['saturation_slope_ConfirmedCases'] = (- np.log(data.ConfirmedCases + 1)+ np.log(data.TRUE_POPULATION + 1)) \/ data.elapsed\ndata['saturation_slope_Fatalities'] = (- np.log(data.Fatalities + 1)+ np.log(data.TRUE_POPULATION + 1)) \/ data.elapsed\ndata['dist_to_ConfirmedCases_saturation_times_days'] = (- np.log(data.ConfirmedCases + 1)+ np.log(data.TRUE_POPULATION + 1)) * data.elapsed\ndata['dist_to_Fatalities_saturation_times_days'] = (- np.log(data.Fatalities + 1)+ np.log(data.TRUE_POPULATION + 1)) * data.elapsed\ndata['slope_to_1pct_ConfirmedCases'] = (- np.log(data.ConfirmedCases + 1)+ np.log(data.TRUE_POPULATION + 1) - 4.6) \/ data.elapsed\ndata['slope_to_0.1pct_Fatalities'] = (- np.log(data.Fatalities + 1)+ np.log(data.TRUE_POPULATION + 1) - 6.9) \/ data.elapsed\ndata['dist_to_1pct_ConfirmedCases_times_days'] = (- np.log(data.ConfirmedCases + 1)+ np.log(data.TRUE_POPULATION + 1) - 4.6) * data.elapsed\ndata['dist_to_0.1pct_Fatalities_times_days'] = (- np.log(data.Fatalities + 1)+ np.log(data.TRUE_POPULATION + 1) - 6.9) * data.elapsed\ndata['trendline_per_capita_ConfirmedCases_4d_slope'] = ( np.log(data.ConfirmedCases + 1)- np.log(data.TRUE_POPULATION + 1)) + (data.ConfirmedCases_4d_prior_slope * data.elapsed)\ndata['trendline_per_capita_ConfirmedCases_7d_slope'] = ( np.log(data.ConfirmedCases + 1)- np.log(data.TRUE_POPULATION + 1)) + (data.ConfirmedCases_7d_prior_slope * data.elapsed)\ndata['trendline_per_capita_Fatalities_12d_slope'] = ( np.log(data.Fatalities + 1)- np.log(data.TRUE_POPULATION + 1)) + (data.ConfirmedCases_12d_prior_slope * data.elapsed)\ndata['trendline_per_capita_Fatalities_20d_slope'] = ( np.log(data.Fatalities + 1)- np.log(data.TRUE_POPULATION + 1)) + (data.ConfirmedCases_20d_prior_slope * data.elapsed)\n\n \n\n# def logHist(x, b = 150):\n#     return\n\ndata['log_fatalities'] = np.log(data.Fatalities + 1) #  + 0.4 * np.random.normal(0, 1, len(data))\ndata['log_cases'] = np.log(data.ConfirmedCases + 1) # + 0.2 *np.random.normal(0, 1, len(data))\n# find HK and MACAU Area\n#data['is_China'] = (data.Country=='China') & (~data.Place.isin(['Hong Kong', 'Macau']))\ndata['is_China'] = (data.Country=='China') & (~data.Place.isin(['China_Hong Kong', 'China_Macau']))\n\nfor col in [c for c in data.columns if 'd_ewm' in c]:\n    data[col] += np.random.normal(0, 1, len(data)) * np.std(data[col]) * 0.2\n    \ndata['is_province'] = 1.0* (~data.Province_State.isnull() )\ndata['log_elapsed'] = np.log(data.elapsed + 1)\n\ndata.drop(columns = ['TRUE_POPULATION'], inplace=True)\ndata['final_day_of_week'] = data.Date_f.apply(datetime.datetime.weekday)\ndata['base_date_day_of_week'] = data.Date.apply(datetime.datetime.weekday)\ndata['date_difference_modulo_7_days'] = (data.Date_f - data.Date).dt.days % 7\nfor c in data.columns.to_list():\n    if 'days_to' in c:\n        data[c] = data[c].where(~np.isinf(data[c]), 1e3)\n        data[c] = np.clip(data[c], 0, 365)\n        data[c] = np.sqrt(data[c])\n\n","0ac438ff":"data.shape","c559f671":"new_places = train[(train.Date == test.Date.min() - datetime.timedelta(1)) &(train.ConfirmedCases == 0)].Place","fb3a146b":" test.Date.min(), VAL_START_DATE","7b6642c2":"model_data = data[ (( len(test) ==0 ) | (data.Date_f < test.Date.min()))& (data.ConfirmedCases > 0) & (~data.ConfirmedCases_f.isnull())].copy()\n# model_data = data[ (( len(test) ==0 ) | (data.Date_f < VAL_START_DATE))& (data.ConfirmedCases > 0) & (~data.ConfirmedCases_f.isnull())].copy()\n\n\n# print(test.Date.min())\n\n\nmodel_data.Date_f.max()\nmodel_data.Date.max()\nmodel_data.Date_f.min()\n\nmodel_data = model_data[~(( np.random.rand(len(model_data)) < 0.8 )  &( model_data.Country == 'China') &(model_data.Date < datetime.datetime(2020, 2, 15)) )]\nx_dates = model_data[['Date_i', 'Date_f', 'Place']]\nx = model_data[model_data.columns.to_list()[model_data.columns.to_list().index('ConfirmedCases_1d_prior_value'):]].drop(columns = ['Date_i', 'Date_f', 'CaseChgRate', 'FatalityChgRate'])\n\nif PRIVATE:\n    data_test = data[ (data.Date_i == train.Date.max() ) & (data.Date_f.isin(test.Date.unique() ) ) ].copy()\nelse:\n    # data_test = data[ (data.Date_i == test.Date.min() - datetime.timedelta(1) ) & (data.Date_f.isin(test.Date.unique() ) ) ].copy()\n    data_test = data[ (data.Date_i == VAL_START_DATE - datetime.timedelta(1) ) & (data.Date_f.isin(test.Date.unique() ) ) ].copy()\n    \n\n\n# data_test.Date.unique()\n# test.Date.unique()\n\nx_test =  data_test[x.columns].copy()\n\n# train.Date.max()\n# test.Date.max()\n\nif MODEL_Y is 'slope':\n    y_cases = model_data.CaseChgRate \n    y_fatalities = model_data.FatalityChgRate \nelse:\n    y_cases = model_data.CaseChgRate * model_data.elapsed\n    y_fatalities = model_data.FatalityChgRate * model_data.elapsed\n    \ny_cfr = np.log(    (model_data.Fatalities_f + np.clip(0.015 * model_data.ConfirmedCases_f, 0, 0.3)) \/ ( model_data.ConfirmedCases_f + 0.1) )\n\n\ngroups = model_data.Country\nplaces = model_data.Place\nx.shape, x_test.shape","a15134d1":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GroupKFold, GroupShuffleSplit, PredefinedSplit\nfrom sklearn.model_selection import ParameterSampler\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import HuberRegressor, ElasticNet\nimport lightgbm as lgb\n\n\nnp.random.seed(SEED)\n\nenet_params = { 'alpha': [   3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3,  ],\n                'l1_ratio': [  0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.97, 0.99 ]}\n\net_params = {        'n_estimators': [50, 70, 100, 140],\n                    'max_depth': [3, 5, 7, 8, 9, 10],\n                      'min_samples_leaf': [30, 50, 70, 100, 130, 165, 200, 300, 600],\n                     'max_features': [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85],\n                    'min_impurity_decrease': [0, 1e-5 ], #1e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2],\n                    'bootstrap': [ True, False], # False is clearly worse          \n                 #   'criterion': ['mae'],\n                   }\n\n\nlgb_params = {\n                'max_depth': [5, 12],\n                'n_estimators': [ 100, 200, 300, 500],   # continuous\n                'min_split_gain': [0, 0, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2],\n                'min_child_samples': [ 7, 10, 14, 20, 30, 40, 70, 100, 200, 400, 700, 1000, 2000],\n                'min_child_weight': [0], #, 1e-3],\n                'num_leaves': [5, 10, 20, 30],\n                'learning_rate': [0.05, 0.07, 0.1],   #, 0.1],       \n                'colsample_bytree': [0.1, 0.2, 0.33, 0.5, 0.65, 0.8, 0.9], \n                'colsample_bynode':[0.1, 0.2, 0.33, 0.5, 0.65, 0.81],\n                'reg_lambda': [1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000,   ],\n                'reg_alpha': [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 30, 1000,], # 1, 10, 100, 1000, 10000],\n                'subsample': [  0.8, 0.9, 1],\n                'subsample_freq': [1],\n                'max_bin': [ 7, 15, 31, 63, 127, 255],\n  #               'extra_trees': [True, False],\n#                 'boosting': ['gbdt', 'dart'],\n    #     'subsample_for_bin': [200000, 500000],\n               }    \n\n\nMSE = 'neg_mean_squared_error'\nMAE = 'neg_mean_absolute_error'\n\n\ndef trainENet(x, y, groups, cv = 0, **kwargs):\n    return trainModel(x, y, groups, \n                      clf = ElasticNet(normalize = True, selection = 'random', \n                                       max_iter = 3000),\n                      params = enet_params, \n                      cv = cv, **kwargs)\n\n# %% [code]\ndef trainETR(x, y, groups, cv = 0, n_jobs = 5,  **kwargs):\n    clf = ExtraTreesRegressor(n_jobs = 1)\n    params = et_params\n    return trainModel(x, y, groups, clf, params, cv, n_jobs, **kwargs)\n\n# %% [code]\ndef trainLGB(x, y, groups, cv = 0, n_jobs = 4, **kwargs):\n    clf = lgb.LGBMRegressor(verbosity=-1, hist_pool_size = 1000,  \n                      )\n    params = lgb_params\n    \n    return trainModel(x, y, groups, clf, params, cv, n_jobs,  **kwargs)\n\n# %% [code]\ndef trainModel(x, y, groups, clf, params, cv = 0, n_jobs = None, \n                   verbose=0, splits=None, **kwargs):\n\n        if n_jobs is None:\n            n_jobs = 4\n        if np.random.rand() < 0.8: # all shuffle, don't want overfit models, just reasonable\n            folds = GroupShuffleSplit(n_splits=4, \n                                                   test_size= 0.2 + 0.10 * np.random.rand())\n        else:\n            folds = GroupKFold(4)\n        clf = RandomizedSearchCV(clf, params, \n                            cv=  folds, \n#                                  cv = GroupKFold(4),\n                                 n_iter=12, \n                                verbose = 0, n_jobs = n_jobs, scoring = MSE)\n        f = clf.fit(x, y, groups)\n        #if verbose > 0:\n        print(pd.DataFrame(clf.cv_results_['mean_test_score'])); print();  \n        \n        \n        best = clf.best_estimator_;  print(best)\n        print(\"Best Score: {}\".format(np.round(clf.best_score_,4)))\n        \n        return best\n\nnp.mean(y_cases)\n\n\ndef getSparseColumns(x, verbose = 0):\n    sc = []\n    for c in x.columns.to_list():\n        u = len(x[c].unique())\n        if u > 10 and u < 0.01*len(x) :\n            sc.append(c)\n            if verbose > 0:\n                print(\"{}: {}\".format(c, u))\n\n    return sc\n\n\ndef noisify(x, noise = 0.1):\n    x = x.copy()\n   # cols = x.columns.to_list()\n    cols = getSparseColumns(x)\n    for c in cols:\n        u = len(x[c].unique())\n        if u > 50:\n            x[c].values[:] = x[c].values + np.random.normal(0, noise, len(x)) * np.std(x[c])\n    return x;\n\n\ndef getMaxOverlap(row, df):\n#     max_overlap_frac = 0\n\n    df_place = df[df.Place == row.Place]\n    if len(df_place)==0:\n        return 0\n#     print(df_place)\n    overlap = (np.clip( df_place.Date_f, None, row.Date_f) - np.clip( df_place.Date_i, row.Date_i, None) ).dt.days\n    overlap = np.clip(overlap, 0, None)\n    length = np.clip(  (df_place.Date_f - df_place.Date_i).dt.days, \n                        (row.Date_f - row.Date_i).days,  None)\n\n    return np.amax(overlap \/ length) \n\n\ndef getSampleWeight(x, groups):\n \n    \n    counter = Counter(groups)\n    median_count = np.median( [counter[group] for group in groups.unique()])\n#     print(median_count)\n    c_count = [counter[group] for group in groups]\n    \n    e_decay = np.round(LT_DECAY_MIN + np.random.rand() * ( LT_DECAY_MAX - LT_DECAY_MIN), 1) \n    print(\"LT weight decay: {:.2f}\".format(e_decay));\n    ssr =  np.power(  1 \/ np.clip( c_count \/ median_count , 0.1,  30) ,0.1 + np.random.rand() * 0.6) \/   np.power(x.elapsed \/ 3, e_decay) *  SET_FRAC * np.exp(  -    np.random.rand()  )\n    \n    # drop % of groups at random\n    group_drop = dict([(group, np.random.rand() < 0.15) for group in groups.unique()])\n    ssr = ssr * (  [ 1 -group_drop[group] for group in groups])\n#     print(ssr[::171])\n#     print(np.array([ 1 -group_drop[group] for group in groups]).sum() \/ len(groups))\n\n#     pd.Series(ssr).plot(kind='hist', bins = 100)\n    return ssr\n\ndef runBags(x, y, groups, cv, bags = 3, model_type = trainLGB, \n            noise = 0.1, splits = None, weights = None, **kwargs):\n    models = []\n    for bag in range(bags):\n        print(\"\\nBAG {}\".format(bag+1))\n        \n        x = x.copy()  # copy X to modify it with noise\n        \n        if DROPS:\n            # drop 0-70% of the bend\/slope\/prior features, just for speed and model diversity\n            for col in [c for c in x.columns if any(z in c for z in ['bend', 'slope', 'prior'])]:\n                if np.random.rand() < np.sqrt(np.random.rand()) * 0.7:\n                    x[col].values[:] = 0\n            \n\n        if DROPS and (np.random.rand() < 0.30):\n            print('dropping nearest features')\n            for col in [c for c in x.columns if 'nearest' in c]:    \n                x[col].values[:] = 0\n        \n        #  % of the time drop all 'surrounding_places' features \n        if DROPS and (np.random.rand() < 0.25):\n            print('dropping \\'surrounding places\\' features')\n            for col in [c for c in x.columns if 'surrounding_places' in c]:    \n                x[col].values[:] = 0\n        \n        \n\n        col_drop_frac = np.sqrt(np.random.rand()) * 0.5\n        for col in [c for c in x.columns if 'elapsed' not in c ]:\n            if np.random.rand() < col_drop_frac:\n                x[col].values[:] = 0\n\n        \n        x = noisify(x, noise)\n        \n        \n        if DROPS and (np.random.rand() < SUP_DROP):\n            print(\"Dropping supplemental country data\")\n            for col in x[[c for c in x.columns if c in sup_data.columns]]:  \n                x[col].values[:] = 0\n                \n        if DROPS and (np.random.rand() < ACTIONS_DROP): \n            for col in x[[c for c in x.columns if c in contain_data.columns]]:  \n                x[col].values[:] = 0\n#             print(x.StringencyIndex_20d_ewm[::157])\n        else:\n            print(\"*using containment data\")\n            \n        if np.random.rand() < 0.6: \n            x.S_data_days = 0\n            \n        ssr = getSampleWeight(x, groups)\n        \n        date_falloff = 0 + (1\/30) * np.random.rand()\n        if weights is not None:\n            ssr = ssr * np.exp(-weights * date_falloff)\n        \n        ss = ( np.random.rand(len(y)) < ssr  )\n        print(\"n={}\".format(len(x[ss])))\n        \n        p1 =x.elapsed[ss].plot(kind='hist', bins = int(x.elapsed.max() - x.elapsed.min() + 1))\n        p1 = plt.figure();\n#         break\n#        print(Counter(groups[ss]))\n        print((ss).sum())\n        models.append(model_type(x[ss], y[ss], groups[ss], cv,   **kwargs))\n    return models\n","4df22236":"\nx = x.astype(np.float32)\nBAG_MULT = 1\n\nprint(x.shape)\n\nlgb_c_clfs = []\nlgb_c_noise = []\n\ndate_weights =  np.abs((model_data.Date_f - test.Date.min()).dt.days) \nprint(date_weights)\n#david original\nC_NOISES = [ 0.05, 0.1, 0.2, 0.3, 0.4  ]\nF_NOISES = [  0.5,  1, 2, 3,  ]\nCFR_NOISES = [    0.4, 1, 2, 3]\n\n#debug\n# C_NOISES = [ 0.05]\n# F_NOISES = [  0.5]\n# CFR_NOISES = [ 0.4]\n\n\nfor iteration in range(0, int(math.ceil(1.1 * BAGS))):\n    for noise in C_NOISES:\n        print(\"\\n---\\n\\nNoise of {}\".format(noise));\n        num_bags = 1 * BAG_MULT;\n        if np.random.rand() < PLACE_FRACTION:\n            cv_group = places\n            print(\"CV by Place\")\n        else:\n            cv_group = groups\n            print(\"CV by Country\")\n             \n        \n        lgb_c_clfs.extend(runBags(x, y_cases,cv_group,MSE, num_bags, trainLGB, verbose = 0,noise = noise, weights = date_weights))\n        lgb_c_noise.extend([noise] * num_bags)\n        if SINGLE_MODEL:\n            break;\n\nlgb_f_clfs = [] \nlgb_f_noise = []\n\nfor iteration in range(0, int(np.ceil(np.sqrt(BAGS)))):\n    for noise in F_NOISES:\n        print(\"\\n---\\n\\nNoise of {}\".format(noise));\n        num_bags = 1 * int(np.ceil(np.sqrt(BAG_MULT)))\n        if np.random.rand() < PLACE_FRACTION  :\n            cv_group = places\n            print(\"CV by Place\")\n        else:\n            cv_group = groups\n            print(\"CV by Country\")\n            \n   \n        lgb_f_clfs.extend(runBags(x, y_fatalities, \n                                  cv_group, #places, # groups, \n                                  MSE, num_bags, trainLGB, \n                                  verbose = 0, noise = noise,\n                                  weights = date_weights\n                                 ))\n        lgb_f_noise.extend([noise] * num_bags)\n        if SINGLE_MODEL:\n            break;\n\nlgb_cfr_clfs = []\nlgb_cfr_noise = []\n\nfor iteration in range(0, int(np.ceil(np.sqrt(BAGS)))):\n    for noise in CFR_NOISES:\n        print(\"\\n---\\n\\nNoise of {}\".format(noise));\n        num_bags = 1 * BAG_MULT;\n        if np.random.rand() < 0.5 * PLACE_FRACTION :\n            cv_group = places\n            print(\"CV by Place\")\n        else:\n            cv_group = groups\n            print(\"CV by Country\")\n \n        lgb_cfr_clfs.extend(runBags(x, y_cfr, \n                          cv_group, #groups\n                          MSE, num_bags, trainLGB, verbose = 0, \n                                          noise = noise, \n                                          weights = date_weights\n\n                                 ))\n        lgb_cfr_noise.extend([noise] * num_bags)\n        if SINGLE_MODEL:\n            break;\n\nlgb_cfr_clfs[0].predict(x_test)\n                                                  \n","a2a9522f":"def show_FI(model, featNames, featCount):\n   # show_FI_plot(model.feature_importances_, featNames, featCount)\n    fis = model.feature_importances_\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(fis)[::-1][:featCount]\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fis[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    \n\ndef avg_FI(all_clfs, featNames, featCount):\n    # 1. Sum\n    clfs = []\n    for clf_set in all_clfs:\n        for clf in clf_set:\n            clfs.append(clf);\n    print(\"{} classifiers\".format(len(clfs)))\n    fi = np.zeros( (len(clfs), len(clfs[0].feature_importances_)) )\n    for idx, clf in enumerate(clfs):\n        fi[idx, :] = clf.feature_importances_\n    avg_fi = np.mean(fi, axis = 0)\n\n    # 2. Plot\n    fis = avg_fi\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(fis)[::-1]#[:featCount]\n    #print(indices)\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fis[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    \n    return pd.Series(fis[indices], featNames[indices])\n\ndef linear_FI_plot(fi, featNames, featCount):\n   # show_FI_plot(model.feature_importances_, featNames, featCount)\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(np.absolute(fi))[::-1]#[:featCount]\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fi[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    return pd.Series(fi[indices], featNames[indices])\n\nf = avg_FI([lgb_c_clfs], x.columns, 25)\n\n\nfor feat in ['bend', 'capita', 'cfr', 'slope', 'since', 'chg', 'ersonal','world', 'continent', 'nearest', 'surrounding']:\n    print(\"{}: {:.2f}\".format(feat, f.filter(like=feat).sum() \/ f.sum()))\n\nprint(\"{}: {:.2f}\".format('sup_data',f[[c for c in f.index if c in sup_data.columns]].sum() \/ f.sum()))\nprint(\"{}: {:.2f}\".format('contain_data',f[[c for c in f.index if c in contain_data.columns]].sum() \/ f.sum()))\n\nf = avg_FI([lgb_f_clfs], x.columns, 25)\n\n\nfor feat in ['bend', 'capita', 'cfr', 'slope', 'since', 'chg', 'ersonal','world', 'continent', 'nearest', 'surrounding']:\n    print(\"{}: {:.2f}\".format(feat, f.filter(like=feat).sum() \/ f.sum()))\n\n\nprint(\"{}: {:.2f}\".format('sup_data',f[[c for c in f.index if c in sup_data.columns]].sum() \/ f.sum()))\nprint(\"{}: {:.2f}\".format('contain_data',f[[c for c in f.index if c in contain_data.columns]].sum() \/ f.sum()))\n\n\n\nf = avg_FI([lgb_cfr_clfs], x.columns, 25)\n\n\nfor feat in ['bend', 'capita', 'cfr', 'slope', 'since', 'chg', 'ersonal','world', 'continent', 'nearest', 'surrounding']:\n    print(\"{}: {:.2f}\".format(feat, f.filter(like=feat).sum() \/ f.sum()))\n\n\nprint(\"{}: {:.2f}\".format('sup_data',f[[c for c in f.index if c in sup_data.columns]].sum() \/ f.sum()))\nprint(\"{}: {:.2f}\".format('contain_data',f[[c for c in f.index if c in contain_data.columns]].sum() \/ f.sum()))\n","916fe460":"print('prediction')\n\nall_c_clfs = [lgb_c_clfs]#  enet_c_clfs]\nall_f_clfs = [lgb_f_clfs] #, enet_f_clfs]\nall_cfr_clfs = [lgb_cfr_clfs]\n\nall_c_noise = [lgb_c_noise]\nall_f_noise = [lgb_f_noise]\nall_cfr_noise = [lgb_cfr_noise]\n\nNUM_TEST_RUNS = 1\n\nc_preds = np.zeros((NUM_TEST_RUNS * sum([len(x) for x in all_c_clfs]), len(x_test)))\nf_preds = np.zeros((NUM_TEST_RUNS * sum([len(x) for x in all_f_clfs]), len(x_test)))\ncfr_preds = np.zeros((NUM_TEST_RUNS * sum([len(x) for x in all_cfr_clfs]), len(x_test)))\n\n\ndef avg(x):\n    return (np.mean(x, axis=0) + np.median(x, axis=0))\/2\n\n\ncount = 0\nfor idx, clf in enumerate(lgb_c_clfs):\n    for i in range(0, NUM_TEST_RUNS):\n        noise = lgb_c_noise[idx]\n        c_preds[count,:] = np.clip(clf.predict(noisify(x_test, noise)), -1 , 10)\n        count += 1\n\ncount = 0\nfor idx, clf in enumerate(lgb_f_clfs):\n    for i in range(0, NUM_TEST_RUNS):\n        noise = lgb_f_noise[idx]\n        f_preds[count,:] = np.clip(clf.predict(noisify(x_test, noise)), -1 , 10)\n        count += 1\n\ncount = 0\nfor idx, clf in enumerate(lgb_cfr_clfs):\n    for i in range(0, NUM_TEST_RUNS):\n        noise = lgb_cfr_noise[idx]\n        cfr_preds[count,:] = np.clip(clf.predict(noisify(x_test, noise)), -10 , 10)\n        count += 1\n\ndef qPred(preds, pctile, simple=False):\n    q = np.percentile(preds, pctile, axis = 0)\n    if simple:\n        return q;\n    resid = preds - q\n    resid_wtg = 2\/100\/len(preds)* ( np.clip(resid, 0, None) * (pctile) + np.clip(resid, None, 0) * (100- pctile) )\n    adj = np.sum(resid_wtg, axis = 0)\n    return q + adj\n\n\nq = 50\ny_cases_pred_blended_full = qPred(c_preds, q) #avg(c_preds)\ny_fatalities_pred_blended_full = qPred(f_preds, q) # avg(f_preds)\ny_cfr_pred_blended_full = qPred(cfr_preds, q) #avg(cfr_preds)\n\n\nprint(np.mean(np.corrcoef(c_preds[::NUM_TEST_RUNS]),axis=0))\nprint(np.mean(np.corrcoef(f_preds[::NUM_TEST_RUNS]), axis=0))\nprint(np.mean(np.corrcoef(cfr_preds[::NUM_TEST_RUNS]), axis = 0))\n\npd.Series(np.std(c_preds, axis = 0)).plot(kind='hist', bins = 50)\npd.Series(np.std(f_preds, axis = 0)).plot(kind='hist', bins = 50)\npd.Series(np.std(cfr_preds, axis = 0)).plot(kind='hist', bins = 50)\ny_cfr\n\npred = pd.DataFrame(np.hstack((np.transpose(c_preds),np.transpose(f_preds))), index=x_test.index)\npred['Place'] = data_test.Place\npred['Date'] = data_test.Date\npred['Date_f'] = data_test.Date_f\n\n# pred[(pred.Date == pred.Date.max()) & (pred.Date_f == pred.Date_f.max())][30: 60]\n\n# np.round(pred[(pred.Date == pred.Date.max()) & (pred.Date_f == pred.Date_f.max())], 2)[190:220:]\n# np.round(pred[(pred.Date == pred.Date.max()) & (pred.Date_f == pred.Date_f.max())][220:-20],2)\n\n\nc_preds.shape, x_test.shape","ed4a4b5b":"c_preds.shape[1] \/ N_AREAS","258698ae":"# first_c_slope","5141762d":"data_wp = data_test.copy()\n\nif MODEL_Y is 'slope':\n    data_wp['case_slope'] = y_cases_pred_blended_full \n    data_wp['fatality_slope'] = y_fatalities_pred_blended_full \nelse:\n    data_wp['case_slope'] = y_cases_pred_blended_full \/ x_test.elapsed\n    data_wp['fatality_slope'] = y_fatalities_pred_blended_full \/ x_test.elapsed\n\ndata_wp['cfr_pred'] = y_cfr_pred_blended_full\n\ntrain.Date.max()\ntest.Date.min()\n\nif len(test) > 0:\n    base_date = test.Date.min() - datetime.timedelta(1)\n    #base_date = VAL_START_DATE - datetime.timedelta(1)\nelse:\n    base_date = train.Date.max()\n\nbase_date\n\ndata_wp_ss = data_wp[data_wp.Date == base_date]\ndata_wp_ss = data_wp_ss.drop(columns='Date').rename(columns = {'Date_f': 'Date'})\ntest_wp = pd.merge(test, data_wp_ss[['Date', 'Place', 'case_slope', 'fatality_slope', 'cfr_pred','elapsed']],how='left', on = ['Date', 'Place'])\n\nfirst_c_slope = test_wp[~test_wp.case_slope.isnull()].groupby('Place').first()\nlast_c_slope = test_wp[~test_wp.case_slope.isnull()].groupby('Place').last()\n\nfirst_f_slope = test_wp[~test_wp.fatality_slope.isnull()].groupby('Place').first()\nlast_f_slope = test_wp[~test_wp.fatality_slope.isnull()].groupby('Place').last()\n\nfirst_cfr_pred = test_wp[~test_wp.cfr_pred.isnull()].groupby('Place').first()\nlast_cfr_pred = test_wp[~test_wp.cfr_pred.isnull()].groupby('Place').last()\n\ntest_wp.case_slope = np.where(test_wp.case_slope.isnull() & (test_wp.Date < first_c_slope.loc[test_wp.Place].Date.values),first_c_slope.loc[test_wp.Place].case_slope.values,test_wp.case_slope)\ntest_wp.case_slope = np.where(test_wp.case_slope.isnull() & (test_wp.Date > last_c_slope.loc[test_wp.Place].Date.values),last_c_slope.loc[test_wp.Place].case_slope.values,test_wp.case_slope)\n\n\ntest_wp.fatality_slope = np.where(  test_wp.fatality_slope.isnull() & \n                     (test_wp.Date < first_f_slope.loc[test_wp.Place].Date.values),first_f_slope.loc[test_wp.Place].fatality_slope.values,test_wp.fatality_slope)\n\ntest_wp.fatality_slope = np.where(  test_wp.fatality_slope.isnull() & \n                     (test_wp.Date > last_f_slope.loc[test_wp.Place].Date.values),last_f_slope.loc[test_wp.Place].fatality_slope.values,test_wp.fatality_slope)\n\ntest_wp.cfr_pred = np.where(  test_wp.cfr_pred.isnull() & \n                     (test_wp.Date < first_cfr_pred.loc[test_wp.Place].Date.values),first_cfr_pred.loc[test_wp.Place].cfr_pred.values,test_wp.cfr_pred)\n\ntest_wp.cfr_pred = np.where(  test_wp.cfr_pred.isnull() & \n                     (test_wp.Date > last_cfr_pred.loc[test_wp.Place].Date.values),last_cfr_pred.loc[test_wp.Place].cfr_pred.values,test_wp.cfr_pred)\n\ntest_wp.case_slope = test_wp.case_slope.interpolate('linear')\ntest_wp.fatality_slope = test_wp.fatality_slope.interpolate('linear')\ntest_wp.cfr_pred = test_wp.cfr_pred.interpolate('linear')\ntest_wp.case_slope = test_wp.case_slope.fillna(0)\ntest_wp.fatality_slope = test_wp.fatality_slope.fillna(0)\n\nLAST_DATE = test.Date.min() - datetime.timedelta(1)\n#LAST_DATE = VAL_START_DATE - datetime.timedelta(1)\nprint(LAST_DATE)\n\nfinal = train_bk[train_bk.Date == LAST_DATE  ]\ntest_wp = pd.merge(test_wp, final[['Place', 'ConfirmedCases', 'Fatalities']], on='Place',how ='left', validate='m:1')\ntest_wp.ConfirmedCases = np.exp(np.log(test_wp.ConfirmedCases + 1) + test_wp.case_slope * (test_wp.Date - LAST_DATE).dt.days )- 1\ntest_wp.Fatalities = np.exp(np.log(test_wp.Fatalities + 1) + test_wp.fatality_slope *(test_wp.Date - LAST_DATE).dt.days )  -1\n\nfinal = train_bk[train_bk.Date == test.Date.min() - datetime.timedelta(1) ]\n#final = train_bk[train_bk.Date == VAL_START_DATE - datetime.timedelta(1) ]\nfinal.head()\n\ntest['elapsed'] = (test.Date - final.Date.max()).dt.days \nfull_bk = test_wp.copy()\nfull = test_wp.copy()\n\nBASE_RATE = 0.01\nCFR_CAP = 0.13\nlplot(full_bk)\nlplot(full_bk, columns = ['case_slope', 'fatality_slope'])\n\nfull['cfr_imputed_fatalities_low'] = full.ConfirmedCases * np.exp(full.cfr_pred) \/ np.exp(0.5)\nfull['cfr_imputed_fatalities_high'] = full.ConfirmedCases * np.exp(full.cfr_pred) * np.exp(0.5)\nfull['cfr_imputed_fatalities'] = full.ConfirmedCases * np.exp(full.cfr_pred)  \n\n# full[(full.case_slope > 0.02) &(full.Fatalities < full.cfr_imputed_fatalities_low    ) &(full.cfr_imputed_fatalities_low > 0.3) &\n#                 ( full.Fatalities < 100000 ) &(full.Country!='China') &(full.Date == datetime.datetime(2020, 4,15))].groupby('Place').last().sort_values('Fatalities', ascending=False).iloc[:, 9:]\n\n\n# (np.log(full.Fatalities + 1) -np.log(full.cfr_imputed_fatalities) ).plot(kind='hist', bins = 250)\n\n\n# full[(full.case_slope > 0.02) & \n#                    (full.Fatalities < full.cfr_imputed_fatalities_low    ) &\n#                 (full.cfr_imputed_fatalities_low > 0.3) &\n#                 ( full.Fatalities < 100000 ) &\n#     (~full.Country.isin(['China', 'Korea, South']))][full.Date==train.Date.max()]\\\n#      .groupby('Place').first()\\\n#     .sort_values('cfr_imputed_fatalities', ascending=False).iloc[:, 9:]\n\n# # %% [code]\n# full.Fatalities = np.where(   \n#     (full.case_slope > 0.02) & \n#                    (full.Fatalities <= full.cfr_imputed_fatalities_low    ) &\n#                 (full.cfr_imputed_fatalities_low > 0.3) &\n#                 ( full.Fatalities < 100000 ) &\n#     (~full.Country.isin(['China', 'Korea, South'])) ,\n                        \n#                         (full.cfr_imputed_fatalities_high + full.cfr_imputed_fatalities)\/2,\n#                                     full.Fatalities)\n    \n\n\nfull['elapsed'] = (test_wp.Date - LAST_DATE).dt.days\n\n# full[ (full.case_slope > 0.02) & \n#           (np.log(full.Fatalities + 1) < np.log(full.ConfirmedCases * BASE_RATE + 1) - 0.5) &\n#                            (full.Country != 'China')]\\\n#             [full.Date == datetime.datetime(2020, 4, 5)] \\\n#             .groupby('Place').last().sort_values('ConfirmedCases', ascending=False).iloc[:,8:]\n\n\n# full.Fatalities = np.where((full.case_slope > 0.02) & \n#                       (full.Fatalities < full.ConfirmedCases * BASE_RATE) &\n#                            (full.Country != 'China'),np.exp(np.log( full.ConfirmedCases * BASE_RATE + 1) \\\n#                            * np.clip(   0.5* (full.elapsed - 1) \/ 30, 0, 1)+  np.log(full.Fatalities +1 ) * np.clip(1 - 0.5* (full.elapsed - 1) \/ 30, 0, 1)) -1,full.Fatalities)  \n\n\n# full[(full.case_slope > 0.02) & \n#                    (full.Fatalities > full.cfr_imputed_fatalities_high   ) &\n#                 (full.cfr_imputed_fatalities_low > 0.4) &\n#     (full.Country!='China')]\\\n#      .groupby('Place').count()\\\n#     .sort_values('ConfirmedCases', ascending=False).iloc[:, 8:]\n\n\n# full[(full.case_slope > 0.02) & \n#                    (full.Fatalities > full.cfr_imputed_fatalities_high * 2   ) &\n#                 (full.cfr_imputed_fatalities_low > 0.4) &\n#     (full.Country!='China')  ]\\\n#      .groupby('Place').last()\\\n#     .sort_values('ConfirmedCases', ascending=False).iloc[:, 8:]\n\n\n# full[(full.case_slope > 0.02) & \n#                    (full.Fatalities > full.cfr_imputed_fatalities_high * 1.5   ) &\n#                 (full.cfr_imputed_fatalities_low > 0.4) &\n#     (full.Country!='China')][full.Date==train.Date.max()]\\\n#      .groupby('Place').first()\\\n#     .sort_values('ConfirmedCases', ascending=False).iloc[:, 8:]\n\n\nfull.Fatalities =  np.where(  (full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high      * 2   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n                (full.Country!='China') ,full.cfr_imputed_fatalities,full.Fatalities)\n\nfull.Fatalities =  np.where(  (full.case_slope > 0.02) & (full.Fatalities > full.cfr_imputed_fatalities_high) & (full.cfr_imputed_fatalities_low > 0.4) &\n                (full.Country!='China') ,np.exp(0.6667 * np.log(full.Fatalities + 1) + 0.3333 * np.log(full.cfr_imputed_fatalities + 1)) - 1,full.Fatalities)\n\nfull[(full.Fatalities > full.ConfirmedCases * CFR_CAP) & (full.ConfirmedCases > 1000)].groupby('Place').last().sort_values('Fatalities', ascending=False)\n\n\n# (np.log(full.Fatalities + 1) -np.log(full.cfr_imputed_fatalities) ).plot(kind='hist', bins = 250)\n\n\nassert len(pd.merge(full, final, on='Place', suffixes = ('', '_i'), validate='m:1')) == len(full)\n\n# %% [code]\nffm = pd.merge(full, final, on='Place', suffixes = ('', '_i'), validate='m:1')\nffm['fatality_slope'] = (np.log(ffm.Fatalities + 1 )- np.log(ffm.Fatalities_i + 1 ) ) \/ ffm.elapsed\nffm['case_slope'] = (np.log(ffm.ConfirmedCases + 1 ) - np.log(ffm.ConfirmedCases_i + 1 ) ) \/ ffm.elapsed\nffm[np.log(ffm.Fatalities+1) < np.log(ffm.Fatalities_i+1) - 0.2][['Place', 'Date', 'elapsed', 'Fatalities', 'Fatalities_i']]\nffm[np.log(ffm.ConfirmedCases + 1) < np.log(ffm.ConfirmedCases_i+1) - 0.2][['Place', 'elapsed', 'ConfirmedCases', 'ConfirmedCases_i']]\n\nAGG_DICT = {'ForecastId': 'count','case_slope': 'last','fatality_slope': 'last','ConfirmedCases': 'sum','Fatalities': 'sum'}\nfull_bk[(full_bk.Date == test.Date.max() ) & \n   (~full_bk.Place.isin(new_places))].groupby('Country').agg(AGG_DICT).sort_values('ConfirmedCases', ascending=False)\n\n\nlplot(ffm[~ffm.Place.isin(new_places)])","a158c92f":"# lplot(ffm[~ffm.Place.isin(new_places)], columns = ['case_slope', 'fatality_slope'])\n\n\nffm.fatality_slope = np.clip(ffm.fatality_slope, None, 0.5)\n\nfor lr in [0.2, 0.14, 0.1, 0.07, 0.05, 0.03, 0.01 ]:\n\n    ffm.loc[ (ffm.Place==ffm.Place.shift(4) )\n         & (ffm.Place==ffm.Place.shift(-4) ), 'fatality_slope'] = \\\n         ( ffm.fatality_slope.shift(-2) * 0.25 \\\n              + ffm.fatality_slope.shift(-1) * 0.5 \\\n                + ffm.fatality_slope \\\n                  + ffm.fatality_slope.shift(1) * 0.5 \\\n                    + ffm.fatality_slope.shift(2) * 0.25 ) \/ 2.5\n\n\nffm.ConfirmedCases = np.exp(np.log(ffm.ConfirmedCases_i + 1) + ffm.case_slope *ffm.elapsed ) - 1\nffm.Fatalities = np.exp(np.log(ffm.Fatalities_i + 1) + ffm.fatality_slope *ffm.elapsed ) - 1\n\n# lplot(ffm[~ffm.Place.isin(new_places)])\n# lplot(ffm[~ffm.Place.isin(new_places)], columns = ['case_slope', 'fatality_slope'])\n\n\nffm[(ffm.Date == test.Date.max() ) & (~ffm.Place.isin(new_places))].groupby('Country').agg(AGG_DICT).sort_values('ConfirmedCases', ascending=False)\nffm_bk = ffm.copy()\n\nffm = ffm_bk.copy()\n\ncounter = Counter(data.Place)\nmedian_count = np.median([ counter[group] for group in ffm.Place])\nc_count = [ np.clip(np.power(counter[group] \/ median_count, -1.5), None, 2.5) for group in ffm.Place]\n\nRATE_MULT = 0.00\nRATE_ADD = 0.003\nLAG_FALLOFF = 15\n\nma_factor = np.clip( ( ffm.elapsed - 14) \/ 14 , 0, 1)\n\nffm.case_slope = np.where(ffm.elapsed > 0,\n    0.7 * ffm.case_slope * (1+ ma_factor * RATE_MULT) \\\n         + 0.3 * (  ffm.case_slope.ewm(span=LAG_FALLOFF).mean()* np.clip(ma_factor, 0, 1) \n                  + ffm.case_slope    * np.clip( 1 - ma_factor, 0, 1))+ RATE_ADD * ma_factor * c_count,ffm.case_slope)\n\n\nRATE_MULT = 0\nRATE_ADD = 0.015\nLAG_FALLOFF = 15\n\nma_factor = np.clip( ( ffm.elapsed - 10) \/ 14 , 0, 1)\n\n\nffm.fatality_slope = np.where(ffm.elapsed > 0,\n    0.3 * ffm.fatality_slope * (1+ ma_factor * RATE_MULT) \\\n         + 0.7* (  ffm.fatality_slope.ewm(span=LAG_FALLOFF).mean()* np.clip( ma_factor, 0, 1)\n                      + ffm.fatality_slope * np.clip( 1 - ma_factor, 0, 1)   ) + RATE_ADD * ma_factor * c_count * (ffm.Country != 'China'),ffm.case_slope)\n\nffm.ConfirmedCases = np.exp(np.log(ffm.ConfirmedCases_i + 1) + ffm.case_slope * ffm.elapsed ) - 1\nffm.Fatalities = np.exp(np.log(ffm.Fatalities_i + 1)+ ffm.fatality_slope * ffm.elapsed ) - 1\n\n# lplot(ffm[~ffm.Place.isin(new_places)])\n\n# lplot(ffm[~ffm.Place.isin(new_places)], columns = ['case_slope', 'fatality_slope'])\n\n# ffm_bk[(ffm_bk.Date == test.Date.max() ) & \n#    (~ffm_bk.Place.isin(new_places))].groupby('Country').agg(\n#     {'ForecastId': 'count',\n#      'case_slope': 'last',\n#         'fatality_slope': 'last',\n#             'ConfirmedCases': 'sum',\n#                 'Fatalities': 'sum',\n#                     }\n# ).sort_values('ConfirmedCases', ascending=False)[:15]\n\n\n# ffm[(ffm.Date == test.Date.max() ) & \n#    (~ffm.Place.isin(new_places))].groupby('Country').agg(\n#     {'ForecastId': 'count',\n#      'case_slope': 'last',\n#         'fatality_slope': 'last',\n#             'ConfirmedCases': 'sum',\n#                 'Fatalities': 'sum',\n#                     }\n# ).sort_values('ConfirmedCases', ascending=False)[:15]\n\n# ffm_bk[(ffm_bk.Date == test.Date.max() ) & \n#    (~ffm_bk.Place.isin(new_places))].groupby('Country').agg(\n#     {'ForecastId': 'count',\n#      'case_slope': 'last',\n#         'fatality_slope': 'last',\n#             'ConfirmedCases': 'sum',\n#                 'Fatalities': 'sum',\n#                     }\n# ).sort_values('ConfirmedCases', ascending=False)[-50:]\n\n# ffm[(ffm.Date == test.Date.max() ) & \n#    (~ffm.Place.isin(new_places))].groupby('Country').agg(\n#     {'ForecastId': 'count',\n#      'case_slope': 'last',\n#         'fatality_slope': 'last',\n#             'ConfirmedCases': 'sum',\n#                 'Fatalities': 'sum',\n#                     }\n# ).loc[ffm_bk[(ffm_bk.Date == test.Date.max() ) & \n#    (~ffm_bk.Place.isin(new_places))].groupby('Country').agg(\n#     {'ForecastId': 'count',\n#      'case_slope': 'last',\n#         'fatality_slope': 'last',\n#             'ConfirmedCases': 'sum',\n#                 'Fatalities': 'sum',\n#                     }\n# ).sort_values('ConfirmedCases', ascending=False)[-50:].index]\n\nNUM_TEST_DATES = len(test.Date.unique())\n\nbase = np.zeros((2, NUM_TEST_DATES))\nbase2 = np.zeros((2, NUM_TEST_DATES))\n\nfor idx, c in enumerate(['ConfirmedCases', 'Fatalities']):\n    for n in range(0, NUM_TEST_DATES):\n        #base[idx,n] = np.mean(np.log(  train[((train.Date < test.Date.min())) & (train.ConfirmedCases > 0)].groupby('Country').nth(n)[c]+1))\n        base[idx,n] = np.mean(np.log(  train[((train.Date < VAL_START_DATE)) & (train.ConfirmedCases > 0)].groupby('Country').nth(n)[c]+1))\n\nbase = np.pad( base, ((0,0), (6,0)), mode='constant', constant_values = 0)\n\nfor n in range(0, base2.shape[1]):\n    base2[:, n] = np.mean(base[:, n+0: n+7], axis = 1)\n\n# new_places = train[(train.Date == test.Date.min() - datetime.timedelta(1)) &(train.ConfirmedCases == 0)].Place\nnew_places = train[(train.Date == VAL_START_DATE - datetime.timedelta(1)) &(train.ConfirmedCases == 0)].Place\n\n\nffm.ConfirmedCases = np.where(   ffm.Place.isin(new_places),base2[ 0, (ffm.Date - test.Date.min()).dt.days],ffm.ConfirmedCases)\nffm.Fatalities = np.where(   ffm.Place.isin(new_places),base2[ 1, (ffm.Date - test.Date.min()).dt.days],ffm.Fatalities)\n\n\nffm[ffm.Country=='US'].groupby('Date').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'mean',\n        'fatality_slope': 'mean',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    })","0ba61018":"sub = pd.read_csv(input_path + 'submission.csv')\n\nscl = sub.columns.to_list()\n\nprint(full_bk.groupby('Place').last()[['Date', 'ConfirmedCases', 'Fatalities']])\nprint(ffm.groupby('Place').last()[['Date', 'ConfirmedCases', 'Fatalities']])\n\n\n# %% [code]\nif ffm[scl].isnull().sum().sum() == 0:\n    out = full_bk[scl] * 0.7 + ffm[scl] * 0.3\nelse:\n    print('using full-bk')\n    out = full_bk[scl]\n\n\nout = out[sub.columns.to_list()]\nout.ForecastId = np.round(out.ForecastId, 0).astype(int) \nout = np.round(out, 2)\nprivate = out\n  \n\n\nfull_pred = pd.concat((private, public[~public.ForecastId.isin(private.ForecastId)]),ignore_index=True).sort_values('ForecastId')\n\n# full_pred.to_csv('submission.csv', index=False)","fbdfbe12":"full_pred['ForecastId'] = full_pred['ForecastId'].astype('int')\ndavid_preds = test_orig.merge(full_pred, on='ForecastId')\ndavid_p_c = david_preds.pivot(index='Area', columns='days', values='ConfirmedCases').sort_index()\ndavid_p_f = david_preds.pivot(index='Area', columns='days', values='Fatalities').sort_index()\npreds_c_david = np.log1p(david_p_c.values)\npreds_f_david = np.log1p(david_p_f.values)\npreds_c_david.shape, preds_f_david.shape","59d14fed":"\n\nimport matplotlib.pyplot as plt\n\n#for _ in range(5):\nplt.style.use(['default'])\nfig = plt.figure(figsize = (15, 5))\n\nidx = np.random.choice(N_AREAS)\nprint(AREAS[idx])\n\nplt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='darkblue')\nplt.plot(preds_c[idx], linestyle='--', color='darkblue', label = 'pdd pred cases')\nplt.plot(np.pad(preds_c_david[idx],(START_PUBLIC,0)),label = 'david pred cases', linestyle='-.', color='darkred')\nplt.legend()\nplt.show()\n\n","478cb13e":"import matplotlib.pyplot as plt\n\n#for _ in range(5):\nplt.style.use(['default'])\nfig = plt.figure(figsize = (15, 5))\n\nidx = np.random.choice(N_AREAS)\nprint(AREAS[idx])\n\nplt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='darkblue')\nplt.plot(preds_f[idx], linestyle='--', color='darkblue', label = 'pdd pred fat')\nplt.plot(np.pad(preds_f_david[idx],(START_PUBLIC,0)),label = 'david pred fat', linestyle='-.', color='darkred')\nplt.legend()\nplt.show()","378cfadf":"\n\nfrom sklearn.metrics import mean_squared_error\n\nif True:\n    val_len = TRAIN_N - START_PUBLIC\n    m1s = []\n    m2s = []\n    for i in range(val_len):\n        d = i + START_PUBLIC\n        m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, d]), preds_c_david[:, i]))\n        m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, d]), preds_f_david[:, i]))\n        print(f\"{d}: {(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n        m1s += [m1]\n        m2s += [m2]\n    print()\n\n    \n    m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, START_PUBLIC:START_PUBLIC+val_len]).flatten(), preds_c_david[:, :val_len].flatten()))\n    m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, START_PUBLIC:START_PUBLIC+val_len]).flatten(), preds_f_david[:, :val_len].flatten()))\n    print(f\"{(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n\n","4084e2a5":"import copy\nimport warnings\nwarnings.filterwarnings('ignore')","bbc37247":"FIRST_TEST = test_orig['Date'].apply(lambda x: x.dayofyear).min()","86afc815":"# day_before_valid = FIRST_TEST-1 # 3-11 day  before of validation\n# day_before_public = FIRST_TEST-1 # 3-18 last day of train\n# day_before_private = df_traintest['day'][pd.isna(df_traintest['ForecastId'])].max() # last day of train","dd7f5bd5":"def do_aggregation(df, col, mean_range):\n    df_new = copy.deepcopy(df)\n    col_new = '{}_({}-{})'.format(col, mean_range[0], mean_range[1])\n    df_new[col_new] = 0\n    tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n    df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n    df_new[col_new][pd.isna(df_new[col_new])] = 0\n    return df_new[[col_new]].reset_index(drop=True)\n\ndef do_aggregations(df):\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [15,21]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [15,21]).reset_index(drop=True)], axis=1)\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}cases'.format(threshold)] = tmp\n\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n\n    # process China\/Hubei\n    if df['place_id'][0]=='China\/Hubei':\n        df['days_since_1cases'] += 35 # 2019\/12\/8\n        df['days_since_10cases'] += 35-13 # 2019\/12\/8-2020\/1\/2 assume 2019\/12\/8+13\n        df['days_since_100cases'] += 4 # 2020\/1\/18\n        df['days_since_1fatal'] += 13 # 2020\/1\/9\n    return df","0a6c2807":"def feature_engineering_oscii():\n\n    # helper fucntions\n    \n    def fix_area(x):\n        try:\n            x_new = x['Country_Region'] + \"\/\" + x['Province_State']\n        except:\n            x_new = x['Country_Region']\n        return x_new\n    \n    def fix_area2(x):\n        try:\n            x_new = x['Country\/Region'] + \"\/\" + x['Province\/State']\n        except:\n            x_new = x['Country\/Region']\n        return x_new\n\n\n    \n    def encode_label(df, col, freq_limit=0):\n        df[col][pd.isna(df[col])] = 'nan'\n        tmp = df[col].value_counts()\n        cols = tmp.index.values\n        freq = tmp.values\n        num_cols = (freq>=freq_limit).sum()\n        print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n\n        col_new = '{}_le'.format(col)\n        df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n        for i, item in enumerate(cols[:num_cols]):\n            df_new[col_new][df[col]==item] = i\n\n        return df_new\n\n    def get_df_le(df, col_index, col_cat):\n        df_new = df[[col_index]]\n        for col in col_cat:\n            df_tmp = encode_label(df, col)\n            df_new = pd.concat([df_new, df_tmp], axis=1)\n        return df_new\n    \n    def to_float(x):\n        x_new = 0\n        try:\n            x_new = float(x.replace(\",\", \"\"))\n        except:\n            x_new = np.nan\n        return x_new\n    \n    df_train = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\n    df_test = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n    df_traintest = pd.concat([df_train, df_test])\n    \n    print('process_date')\n    df_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\n    df_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\n    day_min = df_traintest['day'].min()\n    df_traintest['days'] = df_traintest['day'] - day_min\n    \n    df_traintest.loc[df_traintest['Province_State'].isnull(), 'Province_State'] = 'N\/A'\n    df_traintest['place_id'] = df_traintest['Country_Region'] + '_' + df_traintest['Province_State']\n    \n    #df_traintest['place_id'] = df_traintest.apply(lambda x: fix_area(x), axis=1)\n    \n    print('add lat and long')\n    df_latlong = pd.read_csv(\"..\/input\/smokingstats\/df_Latlong.csv\")\n    \n    df_latlong.loc[df_latlong['Province\/State'].isnull(), 'Province_State'] = 'N\/A'\n    df_latlong['place_id'] = df_latlong['Country\/Region'] + '_' + df_latlong['Province\/State']\n    \n    # df_latlong['place_id'] = df_latlong.apply(lambda x: fix_area2(x), axis=1)\n    df_latlong = df_latlong[df_latlong['place_id'].duplicated()==False]\n    df_traintest = pd.merge(df_traintest, df_latlong[['place_id', 'Lat', 'Long']], on='place_id', how='left')\n    \n    places = np.sort(df_traintest['place_id'].unique())\n    \n    print('calc cases, fatalities per day')\n    df_traintest2 = copy.deepcopy(df_traintest)\n    df_traintest2['cases\/day'] = 0\n    df_traintest2['fatal\/day'] = 0\n    tmp_list = np.zeros(len(df_traintest2))\n    for place in places:\n        tmp = df_traintest2['ConfirmedCases'][df_traintest2['place_id']==place].values\n        tmp[1:] -= tmp[:-1]\n        df_traintest2['cases\/day'][df_traintest2['place_id']==place] = tmp\n        tmp = df_traintest2['Fatalities'][df_traintest2['place_id']==place].values\n        tmp[1:] -= tmp[:-1]\n        df_traintest2['fatal\/day'][df_traintest2['place_id']==place] = tmp\n\n    print('do agregation')\n    df_traintest3 = []\n    for place in places[:]:\n        df_tmp = df_traintest2[df_traintest2['place_id']==place].reset_index(drop=True)\n        df_tmp = do_aggregations(df_tmp)\n        df_traintest3.append(df_tmp)\n    df_traintest3 = pd.concat(df_traintest3).reset_index(drop=True)\n    \n    \n    print('add smoking')\n    df_smoking = pd.read_csv(\"..\/input\/smokingstats\/share-of-adults-who-smoke.csv\")\n    df_smoking_recent = df_smoking.sort_values('Year', ascending=False).reset_index(drop=True)\n    df_smoking_recent = df_smoking_recent[df_smoking_recent['Entity'].duplicated()==False]\n    df_smoking_recent['Country_Region'] = df_smoking_recent['Entity']\n    df_smoking_recent['SmokingRate'] = df_smoking_recent['Smoking prevalence, total (ages 15+) (% of adults)']\n    df_traintest4 = pd.merge(df_traintest3, df_smoking_recent[['Country_Region', 'SmokingRate']], on='Country_Region', how='left')\n    SmokingRate = df_smoking_recent['SmokingRate'][df_smoking_recent['Entity']=='World'].values[0]\n    # print(\"Smoking rate of the world: {:.6f}\".format(SmokingRate))\n    df_traintest4['SmokingRate'][pd.isna(df_traintest4['SmokingRate'])] = SmokingRate\n    \n    print('add data from World Economic Outlook Database')\n    # https:\/\/www.imf.org\/external\/pubs\/ft\/weo\/2017\/01\/weodata\/index.aspx\n    df_weo = pd.read_csv(\"..\/input\/smokingstats\/WEO.csv\")\n    subs  = df_weo['Subject Descriptor'].unique()[:-1]\n    df_weo_agg = df_weo[['Country']][df_weo['Country'].duplicated()==False].reset_index(drop=True)\n    for sub in subs[:]:\n        df_tmp = df_weo[['Country', '2019']][df_weo['Subject Descriptor']==sub].reset_index(drop=True)\n        df_tmp = df_tmp[df_tmp['Country'].duplicated()==False].reset_index(drop=True)\n        df_tmp.columns = ['Country', sub]\n        df_weo_agg = df_weo_agg.merge(df_tmp, on='Country', how='left')\n    df_weo_agg.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_weo_agg.columns]\n    df_weo_agg.columns\n    df_weo_agg['Country_Region'] = df_weo_agg['Country']\n    df_traintest5 = pd.merge(df_traintest4, df_weo_agg, on='Country_Region', how='left')\n    \n    print('add Life expectancy')\n    # Life expectancy at birth obtained from http:\/\/hdr.undp.org\/en\/data\n    df_life = pd.read_csv(\"..\/input\/smokingstats\/Life expectancy at birth.csv\")\n    tmp = df_life.iloc[:,1].values.tolist()\n    df_life = df_life[['Country', '2018']]\n    def func(x):\n        x_new = 0\n        try:\n            x_new = float(x.replace(\",\", \"\"))\n        except:\n    #         print(x)\n            x_new = np.nan\n        return x_new\n\n    df_life['2018'] = df_life['2018'].apply(lambda x: func(x))\n    df_life = df_life[['Country', '2018']]\n    df_life.columns = ['Country_Region', 'LifeExpectancy']\n    df_traintest6 = pd.merge(df_traintest5, df_life, on='Country_Region', how='left')\n    \n    print(\"add additional info from countryinfo dataset\")\n    df_country = pd.read_csv(\"..\/input\/countryinfo\/covid19countryinfo.csv\")\n    df_country['Country_Region'] = df_country['country']\n    df_country = df_country[df_country['country'].duplicated()==False]\n    df_traintest7 = pd.merge(df_traintest6, \n                             df_country.drop(['tests', 'testpop', 'country'], axis=1), \n                             on=['Country_Region',], how='left')\n    \n\n\n    df_traintest7['id'] = np.arange(len(df_traintest7))\n    df_le = get_df_le(df_traintest7, 'id', ['Country_Region', 'Province_State'])\n    df_traintest8 = pd.merge(df_traintest7, df_le, on='id', how='left')\n\n    \n    \n    print('covert object type to float')\n\n    cols = [\n        'Gross_domestic_product__constant_prices', \n        'Gross_domestic_product__current_prices', \n        'Gross_domestic_product__deflator', \n        'Gross_domestic_product_per_capita__constant_prices', \n        'Gross_domestic_product_per_capita__current_prices', \n        'Output_gap_in_percent_of_potential_GDP', \n        'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP', \n        'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP', \n        'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total', \n        'Implied_PPP_conversion_rate', 'Total_investment', \n        'Gross_national_savings', 'Inflation__average_consumer_prices', \n        'Inflation__end_of_period_consumer_prices', \n        'Six_month_London_interbank_offered_rate__LIBOR_', \n        'Volume_of_imports_of_goods_and_services', \n        'Volume_of_Imports_of_goods', \n        'Volume_of_exports_of_goods_and_services', \n        'Volume_of_exports_of_goods', 'Unemployment_rate', 'Employment', 'Population', \n        'General_government_revenue', 'General_government_total_expenditure', \n        'General_government_net_lending_borrowing', 'General_government_structural_balance', \n        'General_government_primary_net_lending_borrowing', 'General_government_net_debt', \n        'General_government_gross_debt', 'Gross_domestic_product_corresponding_to_fiscal_year__current_prices', \n        'Current_account_balance', 'pop'\n    ]\n    df_traintest8['cases\/day'] = df_traintest8['cases\/day'].astype(np.float)\n    df_traintest8['fatal\/day'] = df_traintest8['fatal\/day'].astype(np.float)   \n    for col in cols:\n        df_traintest8[col] = df_traintest8[col].apply(lambda x: to_float(x))  \n    # print(df_traintest8['pop'].dtype)\n    \n    return df_traintest8","d0140d2b":"df_traintest = feature_engineering_oscii()\ndf_traintest.shape","7eb48ced":"# day_before_valid = FIRST_TEST -1 # 3-11 day  before of validation\n# day_before_public = FIRST_TEST -1 # 3-18 last day of train\n# day_before_launch = 85 # 4-1 last day before launch","12ab23cc":"def calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score","f56a3e80":"# train model to predict fatalities\/day\n# params\nSEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }","07eb0f1e":"# train model to predict fatalities\/day\n# features are selected manually based on valid score\ncol_target = 'fatal\/day'\ncol_var = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n#     'days_since_10cases', \n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal', 'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n#     'cases\/day_(8-14)',  \n#     'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n    'fatal\/day_(1-7)', \n    'fatal\/day_(8-14)', \n    'fatal\/day_(15-21)', \n    'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', 'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n    'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]\ncol_cat = []\ndf_train = df_traintest[(pd.isna(df_traintest['ForecastId'])) & (df_traintest['days']<TRAIN_N)]\ndf_valid = df_traintest[(pd.isna(df_traintest['ForecastId'])) & (df_traintest['days']<TRAIN_N)]\n# df_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 340 \nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration","4ad6d7ac":"# y_true = df_valid['fatal\/day'].values\n# y_pred = np.exp(model.predict(X_valid))-1\n# score = calc_score(y_true, y_pred)\n# print(\"{:.6f}\".format(score))","ccc5e6c4":"# train model to predict fatalities\/day\ncol_target2 = 'cases\/day'\ncol_var2 = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n    'days_since_10cases', #selected\n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal',\n#     'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n    'cases\/day_(8-14)',  \n    'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n#     'fatal\/day_(1-7)', \n#     'fatal\/day_(8-14)', \n#     'fatal\/day_(15-21)', \n#     'recov\/day_(1-1)', 'recov\/day_(1-7)', \n#     'recov\/day_(8-14)',  'recov\/day_(15-21)',\n#     'active_(1-1)', \n#     'active_(1-7)', \n#     'active_(8-14)',  'active_(15-21)', \n#     'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', \n#     'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n#     'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]","0aedf3e5":"df_traintest[df_traintest['days']<TRAIN_N].Date.max()","49b43a7c":"# train model to predict cases\/day\ndf_train = df_traintest[(pd.isna(df_traintest['ForecastId'])) & (df_traintest['days']<TRAIN_N)]\ndf_valid = df_traintest[(pd.isna(df_traintest['ForecastId'])) & (df_traintest['days']<TRAIN_N)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration","f59a1428":"# y_true = df_valid['cases\/day'].values\n# y_pred = np.exp(model2.predict(X_valid))-1\n# score = calc_score(y_true, y_pred)\n# print(\"{:.6f}\".format(score))","5f74973d":"places = AREAS.copy()","5ed9453a":"# remove overlap for public LB prediction\n\ndf_tmp = df_traintest[((df_traintest['days']<TRAIN_N)  & (pd.isna(df_traintest['ForecastId'])))  | ((TRAIN_N<=df_traintest['days']) & (pd.isna(df_traintest['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n    'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest9 = []\nfor i, place in tqdm(enumerate(places[:])):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\n#df_traintest9[df_traintest9['days']>TRAIN_N-2].head()","6fa4eb27":"# predict test data in public\n# predict the cases and fatatilites one day at a time and use the predicts as next day's feature recursively.\ndf_preds = []\nfor i, place in tqdm(enumerate(places[:])):\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['days']<TRAIN_N).sum()\n    #len_unknown = (TRAIN_N<=df_interest['day']).sum()\n    len_unknown = df_interest.day.nunique() - len_known\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model.predict(X_valid)\n        pred_c = model2.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n            'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds.append(df_interest)\ndf_preds = pd.concat(df_preds)","7bc0baf4":"df_preds.head()","97c4bdd5":"df_preds.shape","71216c5f":"# df_preds['cases\/day']","984dced5":"p_f_oscii = df_preds.pivot(index='place_id', columns='days', values='fatal_pred').sort_index()\np_c_oscii = df_preds.pivot(index='place_id', columns='days', values='cases_pred').sort_index()\np_c_oscii","24ca41f5":"preds_f_oscii = np.log1p(p_f_oscii.values[:].copy())\npreds_c_oscii = np.log1p(p_c_oscii.values[:].copy())\npreds_f_oscii.shape, preds_c_oscii.shape","dafb1578":"START_PUBLIC, TRAIN_N","0854111a":"if True:\n    val_len = TRAIN_N - START_PUBLIC\n    #val_len = 12\n    m1s = []\n    m2s = []\n    for i in range(val_len):\n        d = i + START_PUBLIC\n        m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, d]), preds_c_oscii[:, d]))\n        m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, d]), preds_f_oscii[:, d]))\n        print(f\"{d}: {(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n        m1s += [m1]\n        m2s += [m2]\n    print()\n\n    \n    m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, START_PUBLIC:START_PUBLIC+val_len]).flatten(), preds_c_oscii[:, START_PUBLIC:START_PUBLIC+val_len].flatten()))\n    m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, START_PUBLIC:START_PUBLIC+val_len]).flatten(), preds_f_oscii[:, START_PUBLIC:START_PUBLIC+val_len].flatten()))\n    print(f\"{(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")","a686cd0b":"preds_c_oscii.shape, preds_c.shape, p_c_vopani.shape, preds_c_david.shape","c09ffb94":"# preds_c_blend = np.log1p(np.average([np.expm1(p_c_beluga[:,64:107]),np.expm1(preds_c_cpmp[:]),np.expm1(preds_c[:,64:107])],axis=0, weights=[2,1,2]))\n# preds_f_blend = np.log1p(np.average([np.expm1(p_f_beluga[:,64:107]),np.expm1(preds_f_cpmp[:]),np.expm1(preds_f[:,64:107])],axis=0, weights=[2,1,2]))","7ff93dde":"preds_c_david2 = np.pad(preds_c_david,[(0,0),(START_PUBLIC,0)])\npreds_f_david2 = np.pad(preds_f_david,[(0,0),(START_PUBLIC,0)])","3010e0e3":"preds_c_oscii.shape, preds_c.shape, p_c_vopani.shape, preds_c_david2.shape","6f342bc6":"# p_c_vopani\n# p_f_vopani\n\npreds_c_blend = np.log1p(np.average([np.expm1(preds_c_oscii[:,:]),np.expm1(preds_c[:,:preds_c_oscii.shape[1]]),np.expm1(preds_c_david2),np.expm1(p_c_vopani_mad)],axis=0, weights=[1,3,2,1]))\npreds_f_blend = np.log1p(np.average([np.expm1(preds_f_oscii[:,:]),np.expm1(preds_f[:,:preds_c_oscii.shape[1]]),np.expm1(preds_f_david2),np.expm1(p_f_vopani_mad)],axis=0, weights=[1,2,3,1]))\n\n#preds_f_blend = np.log1p(np.average([np.expm1(preds_f_oscii[:,:]),np.expm1(preds_f[:,:preds_c_oscii.shape[1]])],axis=0, weights=[1,1]))","fa19044a":"if True:\n    val_len = TRAIN_N - START_PUBLIC\n    #val_len = 12\n    m1s = []\n    m2s = []\n    for i in range(val_len):\n        d = i + START_PUBLIC\n        m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, d]), preds_c_blend[:, d]))\n        m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, d]), preds_f_blend[:, d]))\n        print(f\"{d}: {(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n        m1s += [m1]\n        m2s += [m2]\n    print()\n\n    \n    m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, START_PUBLIC:START_PUBLIC+val_len]).flatten(), preds_c_blend[:, START_PUBLIC:START_PUBLIC+val_len].flatten()))\n    m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, START_PUBLIC:START_PUBLIC+val_len]).flatten(), preds_f_blend[:, START_PUBLIC:START_PUBLIC+val_len].flatten()))\n    print(f\"{(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")","be9f4751":"import matplotlib.pyplot as plt\n\nfor _ in range(5):\n    plt.style.use(['default'])\n    fig = plt.figure(figsize = (15, 5))\n\n    idx = np.random.choice(N_AREAS)\n    print(AREAS[idx])#, places[idx])\n\n    plt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='darkblue')\n    plt.plot(preds_f[idx], linestyle='--', color='darkblue', label = 'pdd fat ')\n    plt.plot(p_f_vopani[idx],label = 'vopani fat ', linestyle='-.', color='purple')\n    plt.plot(preds_f_oscii[idx],label = 'oscii fat ', linestyle='-.', color='red')\n    plt.plot(preds_f_david2[idx],label = 'david fat ', linestyle='-.', color='orange')\n    plt.plot(preds_f_blend[idx],label = 'blend fat ', linestyle='-.', color='darkgreen')\n    plt.legend()\n    plt.show()","bb375a36":"import matplotlib.pyplot as plt\n\nfor _ in range(5):\n    plt.style.use(['default'])\n    fig = plt.figure(figsize = (15, 5))\n\n    idx = np.random.choice(N_AREAS)\n    print(AREAS[idx], places[idx])\n\n    plt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='darkblue')\n    plt.plot(preds_c[idx], linestyle='--', color='darkblue', label = 'pdd cases ')\n    plt.plot(p_c_vopani[idx],label = 'vopani cases ', linestyle='-.', color='purple')\n    plt.plot(preds_c_oscii[idx],label = 'oscii cases ', linestyle='-.', color='red')\n    plt.plot(preds_c_david2[idx],label = 'david cases ', linestyle='-.', color='orange')\n    plt.plot(preds_c_blend[idx],label = 'blend cases ', linestyle='-.', color='darkgreen')\n    plt.legend()\n    plt.show()","df0ed465":"EU_COUNTRIES = ['Austria', 'Italy', 'Belgium', 'Latvia', 'Bulgaria', 'Lithuania', 'Croatia', 'Luxembourg', 'Cyprus', 'Malta', 'Czechia', \n                'Netherlands', 'Denmark', 'Poland', 'Estonia', 'Portugal', 'Finland', 'Romania', 'France', 'Slovakia', 'Germany', 'Slovenia', \n                'Greece', 'Spain', 'Hungary', 'Sweden', 'Ireland']\nEUROPE_OTHER = ['Albania', 'Andorra', 'Bosnia and Herzegovina', 'Liechtenstein', 'Monaco', 'Montenegro', 'North Macedonia',\n                'Norway', 'San Marino', 'Serbia', 'Switzerland', 'Turkey', 'United Kingdom']\nAFRICA = ['Algeria', 'Burkina Faso', 'Cameroon', 'Congo (Kinshasa)', \"Cote d'Ivoire\", 'Egypt', 'Ghana', 'Kenya', 'Madagascar',\n                'Morocco', 'Nigeria', 'Rwanda', 'Senegal', 'South Africa', 'Togo', 'Tunisia', 'Uganda', 'Zambia']\nNORTH_AMERICA = ['US', 'Canada', 'Mexico']\nSOUTH_AMERICA = ['Argentina', 'Bolivia', 'Brazil', 'Chile', 'Colombia', 'Ecuador', 'Paraguay', 'Peru', 'Uruguay', 'Venezuela']\nMIDDLE_EAST = ['Afghanistan', 'Bahrain', 'Iran', 'Iraq', 'Israel', 'Jordan', 'Kuwait', 'Lebanon', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates']\nASIA = ['Bangladesh', 'Brunei', 'Cambodia', 'India', 'Indonesia', 'Japan', 'Kazakhstan', 'Korea, South', 'Kyrgyzstan', 'Malaysia',\n                'Pakistan', 'Singapore', 'Sri Lanka', 'Taiwan*', 'Thailand', 'Uzbekistan', 'Vietnam']","94e7c351":"non_china_mask = np.array(['China' not in a for a in AREAS]).astype(bool)\nnon_china_mask.shape\n","db38a4fe":"preds_c2 = preds_c.copy()\npreds_f2 = preds_f.copy()\npreds_c2[non_china_mask,:114] = preds_c_blend[non_china_mask]\npreds_f2[non_china_mask,:114] = preds_f_blend[non_china_mask]","b3213c0b":"import matplotlib.pyplot as plt\n\ndef plt1(ar, ar2, ax, col='darkblue', linew=0.2):\n    ax.plot(ar2, linestyle='--', linewidth=linew\/2, color=col)\n    ax.plot(np.log(1+ar), linewidth=linew, color=col)\n\nplt.style.use(['default'])\nfig, axs = plt.subplots(3, 2, figsize=(18, 15), sharey=True)\n\nX = train_p_c.values\n#X = train_p_f.values\n\nfor ar in range(X.shape[0]):\n    \n    temp = X[ar]\n    temp2 = preds_c2[ar]\n    if 'China' in AREAS[ar]:\n        plt1(temp, temp2, axs[0,0])\n    elif AREAS[ar].split('_')[0] in NORTH_AMERICA:\n        plt1(temp, temp2, axs[0,1])\n    elif AREAS[ar].split('_')[0] in EU_COUNTRIES + EUROPE_OTHER:\n        plt1(temp, temp2, axs[1,0])\n    elif AREAS[ar].split('_')[0] in SOUTH_AMERICA + AFRICA:\n        plt1(temp, temp2, axs[1,1])\n    elif AREAS[ar].split('_')[0] in MIDDLE_EAST + ASIA:\n        plt1(temp, temp2, axs[2,0])\n    else:\n        plt1(temp, temp2, axs[2,1])\n\nprint(\"Confirmed Cases\")\naxs[0,0].set_title('China')\naxs[0,1].set_title('North America')\naxs[1,0].set_title('Europe')\naxs[1,1].set_title('Africa + South America')\naxs[2,0].set_title('Asia + Middle East')\naxs[2,1].set_title('Other')\nplt.show()","7f8bc03a":"import matplotlib.pyplot as plt\n\ndef plt1(ar, ar2, ax, col='darkblue', linew=0.2):\n    ax.plot(ar2, linestyle='--', linewidth=linew\/2, color=col)\n    ax.plot(np.log(1+ar), linewidth=linew, color=col)\n\nplt.style.use(['default'])\nfig, axs = plt.subplots(3, 2, figsize=(18, 15), sharey=True)\n\n#X = train_p_c.values\nX = train_p_f.values\n\nfor ar in range(X.shape[0]):\n    \n    temp = X[ar]\n    temp2 = preds_f2[ar]\n    if 'China' in AREAS[ar]:\n        plt1(temp, temp2, axs[0,0])\n    elif AREAS[ar].split('_')[0] in NORTH_AMERICA:\n        plt1(temp, temp2, axs[0,1])\n    elif AREAS[ar].split('_')[0] in EU_COUNTRIES + EUROPE_OTHER:\n        plt1(temp, temp2, axs[1,0])\n    elif AREAS[ar].split('_')[0] in SOUTH_AMERICA + AFRICA:\n        plt1(temp, temp2, axs[1,1])\n    elif AREAS[ar].split('_')[0] in MIDDLE_EAST + ASIA:\n        plt1(temp, temp2, axs[2,0])\n    else:\n        plt1(temp, temp2, axs[2,1])\n\nprint(\"Fatalities\")\naxs[0,0].set_title('China')\naxs[0,1].set_title('North America')\naxs[1,0].set_title('Europe')\naxs[1,1].set_title('Africa + South America')\naxs[2,0].set_title('Asia + Middle East')\naxs[2,1].set_title('Other')\nplt.show()","24201672":"preds_c.shape, preds_c_blend.shape\n","0ba028a3":"# preds_c2 = preds_c.copy()\n# preds_f2 = preds_f.copy()\n# preds_c2[:,64:107] = preds_c_blend\n# preds_f2[:,64:107] = preds_f_blend","79fe9efa":"\ntemp = pd.DataFrame(np.clip(np.exp(preds_c2) - 1, 0, None))\ntemp['Area'] = AREAS\ntemp = temp.melt(id_vars='Area', var_name='days', value_name=\"ConfirmedCases\")\n\ntest = test_orig.merge(temp, how='left', left_on=['Area', 'days'], right_on=['Area', 'days'])\n\ntemp = pd.DataFrame(np.clip(np.exp(preds_f2) - 1, 0, None))\ntemp['Area'] = AREAS\ntemp = temp.melt(id_vars='Area', var_name='days', value_name=\"Fatalities\")\n\ntest = test.merge(temp, how='left', left_on=['Area', 'days'], right_on=['Area', 'days'])\ntest.head()","566287c1":"test.to_csv(\"submission.csv\", index=False, columns=[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"])","1537b247":"test.days.nunique()","8334048d":"for i, rec in test.groupby('Area').last().sort_values(\"ConfirmedCases\", ascending=False).iterrows():\n    print(f\"{rec['ConfirmedCases']:10.1f} {rec['Fatalities']:10.1f}  {rec['Country_Region']}, {rec['Province_State']}\")\n","b8424fb9":"print(f\"{test.groupby('Area')['ConfirmedCases'].last().sum():10.1f}\")\nprint(f\"{test.groupby('Area')['Fatalities'].last().sum():10.1f}\")","7f51746a":"test_p_c = test.pivot(index='Area', columns='days', values='ConfirmedCases').sort_index().values\ntest_p_f = test.pivot(index='Area', columns='days', values='Fatalities').sort_index().values\ndates = test.Date.dt.strftime('%d.%m.%Y').unique()","d1163c7b":"print(\"Confirmed Cases\")\nfor i in [7,14,21,28,35,42]:\n    print(f'week{i\/\/7-1}  ', dates[i],  f'   {round(test_p_c[:,i].sum(),0):,}')","7198b9b7":"print(\"Fatalities\")\nfor i in [7,14,21,28,35,42]:\n    print(f'week{i\/\/7-1}  ', dates[i],  f'   {round(test_p_f[:,i].sum(),0):,}', )","699dd0fa":"## David model ","4c797036":"## BLEND","aa82c528":"## vopani model","cc0a041d":"## oscii model"}}