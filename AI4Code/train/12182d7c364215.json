{"cell_type":{"06b076e6":"code","f04c586f":"code","8e992830":"code","731dc945":"code","2f1b9ce3":"code","807533dd":"code","b6940870":"code","c5da3c49":"code","4a40cf99":"code","1f528e6d":"code","2a407c7f":"code","821a74b6":"code","ec33c8ba":"code","bb84f63f":"code","54edab64":"code","0fb81e99":"code","3c4401bc":"code","17ec2501":"code","fbb4db60":"markdown","f25e8e4a":"markdown","117cd618":"markdown","37182919":"markdown","cbf2d269":"markdown","5eb707ed":"markdown"},"source":{"06b076e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f04c586f":"import cv2                 # working with, mainly resizing, images\nimport numpy as np         # dealing with arrays\nimport os                  # dealing with directories\nfrom random import shuffle # mixing up off currently ordered data that might lead our network astray in training.\nfrom tqdm import tqdm      # a nice pretty percentage bar for tasks.(for representation purposes)","8e992830":"TRAIN_DIR = '..\/input\/train'\nTEST_DIR = '..\/input\/test'\n\nIMG_SIZE = 50            # we later resize the image to 50x50 for computation\nLR=1e-3    #0.001 \n\nMODEL_NAME = 'dogsvscats-{}-{}.model'.format(LR, '2conv-basic') \n# just so we remember which saved model is which as we are going to change and tweek few things as we go through this,check for the sizes\n# when we load a model into a neural net which differs in size then we get an error","731dc945":"# converting the labels to one-hot array\ndef label_img(img):\n    # dog.89.png\n    word_label=img.split('.')[-3]   # -3 is the dog;-2 is 89 and -1 is png\n    if word_label=='cat':\n        return [1,0]\n    elif word_label=='dog':\n        return [0,1]","2f1b9ce3":"def create_train_data():\n    training_data=[]\n    for img in tqdm(os.listdir(TRAIN_DIR)):  \n        label= label_img(img)\n        path=os.path.join(TRAIN_DIR,img)\n        img=cv2.imread(path,cv2.IMREAD_GRAYSCALE)           # o is for gray scale(cv2.IMREAD_GRAYSCALE)\n        img=cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n        \n        training_data.append([np.array(img),np.array(label)])\n    shuffle(training_data)\n    np.save('train_data.npy',training_data)\n    return training_data","807533dd":"def process_test_data():\n    testing_data=[]\n    for img in tqdm(os.listdir(TEST_DIR)):\n        path=os.path.join(TEST_DIR,img)\n\n        #83.png\n        img_num=img.split('.')[0]\n        img=cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n        img=cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n        testing_data.append([np.array(img),img_num])\n    shuffle(testing_data)\n    np.save('test_data.npy',testing_data)\n    return testing_data","b6940870":"train_data=create_train_data()\n# once we have trained we do not need to retrain it instead we can just use np.load('train_data.npy')","c5da3c49":"import tflearn\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.estimator import regression\n\nconvnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = fully_connected(convnet, 1024, activation='relu')\nconvnet = dropout(convnet, 0.8)\n\nconvnet = fully_connected(convnet, 2, activation='softmax')\nconvnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n\nmodel = tflearn.DNN(convnet, tensorboard_dir='log')","4a40cf99":"if os.path.exists('{}.meta'.format(MODEL_NAME)):\n    model.load(MODEL_NAME)\n    print('model loaded!')","1f528e6d":"train=train_data[:-500]\ntest=train_data[-500:]","2a407c7f":"X=np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\nY=[i[1] for i in train]\n\ntest_x=np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\ntest_y=[i[1] for i in test]","821a74b6":"model.fit({'input': X}, {'targets': Y}, n_epoch=2, validation_set=({'input': test_x}, {'targets': test_y}), \n    snapshot_step=50000, show_metric=True, run_id=MODEL_NAME)\n","ec33c8ba":"import tensorflow as tf\ntf.reset_default_graph()   #we need to reset the graph instance, since we're doing this in a continuous environment","bb84f63f":"convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = fully_connected(convnet, 1024, activation='relu')\nconvnet = dropout(convnet, 0.8)\n\nconvnet = fully_connected(convnet, 2, activation='softmax')\nconvnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n\nmodel = tflearn.DNN(convnet, tensorboard_dir='log')\n\n\n\nif os.path.exists('{}.meta'.format(MODEL_NAME)):\n    model.load(MODEL_NAME)\n    print('model loaded!')\n\ntrain = train_data[:-500]\ntest = train_data[-500:]\n\nX = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\nY = [i[1] for i in train]\n\ntest_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\ntest_y = [i[1] for i in test]\n\nmodel.fit({'input': X}, {'targets': Y}, n_epoch=4, validation_set=({'input': test_x}, {'targets': test_y}), \n    snapshot_step=500, show_metric=True, run_id=MODEL_NAME)\n","54edab64":"model.save(MODEL_NAME)     # we save the model of 84.39%","0fb81e99":"# if you need to create the data:\ntest_data = process_test_data()\n# if you already have some saved:\n#test_data = np.load('test_data.npy')","3c4401bc":"import matplotlib.pyplot as plt\n\nfig=plt.figure()\n\nfor num,data in enumerate(test_data[:12]):\n    # cat: [1,0]\n    # dog: [0,1]\n    \n    img_num = data[1]\n    img_data = data[0]\n    \n    y = fig.add_subplot(3,4,num+1)\n    orig = img_data\n    data = img_data.reshape(IMG_SIZE,IMG_SIZE,1)\n    #model_out = model.predict([data])[0]\n    model_out = model.predict([data])[0]\n    \n    if np.argmax(model_out) == 1: str_label='Dog'\n    else: str_label='Cat'\n        \n    y.imshow(orig,cmap='gray')\n    plt.title(str_label)\n    y.axes.get_xaxis().set_visible(False)\n    y.axes.get_yaxis().set_visible(False)\nplt.show()","17ec2501":"# To make a submission to kaggle\n'''\nwith open('submission_file.csv','w') as f:\n    f.write('id,label\\n')\n            \nwith open('submission_file.csv','a') as f:\n    for data in tqdm(test_data):\n        img_num = data[1]\n        img_data = data[0]\n        orig = img_data\n        data = img_data.reshape(IMG_SIZE,IMG_SIZE,1)\n        model_out = model.predict([data])[0]\n        f.write('{},{}\\n'.format(img_num,model_out[1]))'''","fbb4db60":"array[:,0] and array[:,1] did NOT work for me here. Not sure what I'm doing wrong, so I do this instead to separate my features and labels:","f25e8e4a":"Here the accuracy has improved after increasing the size of the network but it may not always be so as incresing size after a limit tends to overfit the data.We could have use dropouts to minimise this.","117cd618":"We wont be training our model from scratch every time.Maybe first you just want to see how 3 epochs trains, but then, after 3, maybe you're done, or maybe you want to see about 5 epochs. We want to be saving our model after every session, and reloading it if we have a saved version, so we add this:","37182919":"When we've gone through all of the images, we shuffle them, then save. Shuffle modifies a variable in place, so there's no need to re-define it here.\n\nWith this function, we will both save, and return the array data. This way, if we just change the neural network's structure, and not something with the images, like image size..etc..then we can just load the array file and save some processing time. While we're here, we might as well also make a function to process the testing data. This is the actual competition test data, NOT the data that we'll use to check the accuracy of our algorithm as we test. This data has no label.","cbf2d269":"well 49% doesniot give us anything\n\nif you haven't made accuracy progress in the first 3 epochs, you're probably not going to at all, unless it's due to overfitment. So now we try to increase the size of the model","5eb707ed":"tqdm provides a , way to measure where you are in a process, rather than printing things out at intervals...etc, it gives a progress bar."}}