{"cell_type":{"1710fcdf":"code","1df68709":"code","1a771789":"code","eaf95b76":"code","96ce79d6":"code","9dac8c2a":"code","5c9836d1":"code","0cfdbd1b":"code","98709b80":"code","00e82ca8":"code","b1c42df0":"code","9c263ea4":"code","553abb7a":"code","7c5ba131":"code","0abbc1ed":"code","ac49aed1":"code","76fad52e":"code","02568369":"code","0fd6a768":"code","8b7c9c46":"code","38906473":"code","0aec5180":"markdown","ae391a5f":"markdown","72e6b184":"markdown","7e7c16f8":"markdown","a6e27423":"markdown","44ac66fa":"markdown","d47ade05":"markdown","9eb0d391":"markdown","356522cd":"markdown","40eac9e6":"markdown","47a8a9d7":"markdown","cb1111c4":"markdown","3c7b4229":"markdown","5ab8b836":"markdown","2a4ddd5a":"markdown","1a3be5e5":"markdown","15194e35":"markdown","eca6a44b":"markdown","443b1f82":"markdown"},"source":{"1710fcdf":"import tensorflow as tf # import tensor flow \nimport numpy as np\nimport keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt","1df68709":" from tensorflow.keras.datasets import imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=10000)","1a771789":"print(train_data[0])","eaf95b76":"print(train_labels[0])","96ce79d6":"max([max(sequence) for sequence in train_data])","9dac8c2a":"word_index = imdb.get_word_index() #1\nreverse_word_index = dict(\n    [(value, key) for (key, value) in word_index.items()]) #2\ndecoded_review = ' '.join(\n    [reverse_word_index.get(i - 3, '?') for i in train_data[0]])#3","5c9836d1":"def vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))#1\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.#2\n    return results\nx_train = vectorize_sequences(train_data)#3\nx_test = vectorize_sequences(test_data)#4","0cfdbd1b":"x_train[0]","98709b80":"y_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')","00e82ca8":"model = keras.Sequential([\n  layers.Dense(16, activation='relu'),\n  layers.Dense(16, activation='relu'),\n  layers.Dense(1, activation='sigmoid')\n])","b1c42df0":"\n model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","9c263ea4":"x_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]","553abb7a":"model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))","7c5ba131":"history_dict = history.history","0abbc1ed":"history_dict.keys()","ac49aed1":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'ro', label='Training loss')\nplt.plot(epochs, val_loss_values, 'g', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","76fad52e":"plt.clf()\nacc = history_dict['acc']\nval_acc = history_dict['val_acc']\nplt.plot(epochs, acc, 'go', label='Training acc')\nplt.plot(epochs, val_acc, 'g', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","02568369":"model = keras.Sequential([\n    layers.Dense(16, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\nresults = model.evaluate(x_test, y_test)","0fd6a768":"results","8b7c9c46":"predictions = model.predict(x_test)","38906473":"print(predictions)","0aec5180":"#### *Inference*: the training loss decreases with every epoch, and the training accuracy increases with every epoch. That\u2019s expected when running gradient-descent optimization \u2014 the quantity we are trying to minimize should be less with every iteration. But that isn\u2019t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we warned against earlier: a model that performs better on the training data isn\u2019t necessarily a model that will do better on data it has never seen before. In precise terms, what you\u2019re seeing is overfitting: after the fourth epoch, you\u2019re over-optimizing on the training data, and you end up learning representations that are specific to the training data and don\u2019t generalize to data outside of the training set.\n\n#### In this case, to prevent overfitting, we could stop training after three epochs. \n\n#### Let\u2019s train a new model from scratch for four epochs and then evaluate it on the test data.","ae391a5f":"1. word_index is a dictionary mapping words to an integer index. \n2. Reverses it, mapping integer indices to words\n3. Decodes the review. Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for \u201cpadding,\u201d \u201cstart of sequence,\u201d and \u201cunknown.\u201d.","72e6b184":"#### The dictionary contains four entries: one per metric that was being monitored during training and during validation. In the following two listing, let\u2019s use Matplotlib to plot the training and validation loss side by side ","7e7c16f8":"#### Note that the call to model.fit() returns a History object, this object has a member history, which is a dictionary containing data about everything that happened during training. Let\u2019s look at it:","a6e27423":"#### We will train the model for 20 epochs (20 iterations over all samples in the x_train and y_train tensors), in mini-batches of 512 samples. At the same time, we will monitor loss and accuracy on the 10,000 samples that we set apart. We do so by passing the validation data as the validation_data argument.","44ac66fa":"### Training the model ","d47ade05":"#### Importing Libraries","9eb0d391":"## Classifying IMDB movie reviews: a binary classification example\/problem\n#### Two-class classification, or binary classification, is one of the most common kinds of machine-learning problem. \n#### In this example, we will learn to classify movie reviews as positive or negative, based on the text content of the reviews.\n\n#### Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary. This enables us to focus on model building, training, and evaluation.","356522cd":"### Key Takeaways \n\n1. You usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it \u2014 as tensors \u2014 into a neural network. \n2. Sequences of words can be encoded as binary vectors, but there are other encoding options, too.\n3. Stacks of Dense layers with relu activations can solve a wide range of problems (including sentiment classification), and you\u2019ll likely use them frequently.\n4. In a binary classification problem (two output classes), our model should end with a Dense layer with one unit and a sigmoid activation: the output of our model should be a scalar between 0 and 1, encoding a probability.\n5. With such a scalar sigmoid output on a binary classification problem, the loss function you should use is binary_crossentropy.\n6. The rmsprop optimizer is generally a good enough choice, whatever your problem. That\u2019s one less thing for you to worry about.\n","40eac9e6":"#### After having trained a model, you\u2019ll want to use it in a practical setting. We can generate the likelihood of reviews being positive by using the predict method","47a8a9d7":"#### num_words=10000 means you\u2019ll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded\n\n#### train_data and test_data are lists of reviews; each review is a list of word indices (encoding a sequence of words). train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:","cb1111c4":"#### Loading IMDB Dataset\n\nIMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They\u2019re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews.","3c7b4229":"1. Creates an all-zero matrix of shape (len(sequences), dimension) \n2. Sets specific indices of results[i] to 1s\n3. Vectorized training data\n4. Vectorized test data","5ab8b836":"**Credits: Book on \"Deep Learning with Python: Second Edition\"**","2a4ddd5a":"### Compiling the model","1a3be5e5":"### Building Models \n\n#### The input data is vectors, and the labels are scalars (1s and 0s): this is one of the simplest problem setups you\u2019ll ever encounter. A type of model that performs well on such a problem is a plain stack of densely-connected (Dense) layers with relu activations.\n#### There are two key architecture decisions to be made about such a stack of Dense layers: How many layers to use\n#### How many units to choose for each layer\n#### Two intermediate layers with 16 units each\n#### A third layer that will output the scalar prediction regarding the sentiment of the current review","15194e35":"### Validation set","eca6a44b":"#### vectorize labels as well","443b1f82":"#### The architecture choices we have made are all fairly reasonable, although there\u2019s still room for improvement:\n##### we used two representation layers before the final classification layer. \n\n#### Further work to be done \n##### Try using one or three representation layers, and see how doing so affects validation and test accuracy.\n##### Try using layers with more units or fewer units: 32 units, 64 units, and so on. \n##### Try using the mse loss function instead of binary_crossentropy.\n##### Try using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu."}}