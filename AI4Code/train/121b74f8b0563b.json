{"cell_type":{"ced3d384":"code","eb9de0cb":"code","930fa596":"code","37851b3e":"code","6823cd8b":"code","fb7e887b":"code","e84b47f8":"code","5c21e05b":"code","e3fc76d1":"code","38823f57":"code","4cd7faa1":"code","4401f371":"code","f4b75946":"code","e2bd0526":"code","8c6ac3a5":"code","21965e83":"code","bcba5ac4":"code","b96f9415":"code","307d9cb0":"code","290b7ed9":"code","08af6ce1":"code","bcf447ca":"code","ba69461e":"code","2f8c6f92":"code","d606f9b3":"code","dd1b78bb":"markdown","7c9d6e66":"markdown","fb9b59cc":"markdown","a18cccd0":"markdown","17c34e99":"markdown","7b6259a1":"markdown","ed8ce1fe":"markdown","196652af":"markdown","ee022f66":"markdown","286edc8a":"markdown","e54d21a4":"markdown","d447b81f":"markdown","e074c146":"markdown","1695c472":"markdown","f5d4788c":"markdown","0d41ed15":"markdown","803abae8":"markdown","2305917e":"markdown","51410904":"markdown","78c29592":"markdown"},"source":{"ced3d384":"!pip install -q efficientnet\nimport efficientnet.tfkeras as efn\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom scipy.special import softmax\n\nIMAGE_SIZE = [256, 256]\nDO_PARAMETER_TUNING = False","eb9de0cb":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","930fa596":"train_df = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2020-fgvc7\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2020-fgvc7\/test.csv\")\nimage_dir = \"\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/\"","37851b3e":"columns = train_df.columns\ntarget_columns = train_df.columns.drop(\"image_id\")\ntrain_target = train_df[target_columns]\ndisplay(train_target.head())\n\nclass_names = list(target_columns.values)\ntrain_labels = train_target.idxmax(axis=1)\nsns.countplot(train_labels)","6823cd8b":"train_files_labels = pd.concat([train_df[\"image_id\"], train_labels], axis=1)\ntrain_files_labels.columns = ['file', 'label']\ntrain_files_labels['file'] = train_files_labels['file'].apply(lambda x : x+\".jpg\")\n\ndisplay(train_files_labels.head())","fb7e887b":"ident_gen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_gen = ident_gen.flow_from_dataframe(\n    train_files_labels, \n    directory=image_dir, \n    x_col='file', \n    y_col='label', \n    target_size=IMAGE_SIZE, \n    classes=class_names,\n    class_mode='categorical', \n    batch_size=25, \n    shuffle=True)\n\ndef show_batch(image_batch, label_batch, true_label_batch=[]):\n    plt.figure(figsize=(15,15))\n    for n in range(min(25, len(image_batch))):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        title = class_names[label_batch[n].argmax()]\n        if len(true_label_batch):\n            title += f\" ({class_names[true_label_batch[n].argmax()]})\"\n        plt.title(title)\n        plt.axis('off')\n\nimage_batch, label_batch = next(train_gen)\nshow_batch(image_batch, label_batch)","e84b47f8":"def feature_extractor_model():\n    pretrained_model = efn.EfficientNetB5(\n        input_shape=[*IMAGE_SIZE, 3],\n        weights='imagenet',\n        include_top=False)\n    pretrained_model.trainable = False\n    return tf.keras.Sequential([\n        pretrained_model,\n        tf.keras.layers.GlobalAveragePooling2D()\n    ])\n\nfeature_model = feature_extractor_model()\nfeature_model.summary()","5c21e05b":"%%time\n# Load all the images in a single batch\ntrain_batch_gen = ident_gen.flow_from_dataframe(\n    train_files_labels, \n    directory=image_dir, \n    x_col='file', \n    y_col='label', \n    target_size=IMAGE_SIZE, \n    color_mode='rgb', \n    classes=class_names,\n    class_mode='categorical', \n    batch_size=len(train_files_labels), \n    shuffle=True)\n\ntrain_X_all, train_y_all = next(train_batch_gen)","e3fc76d1":"%%time\ntrain_y_all = train_y_all.argmax(axis=-1) # onehot -> labels\ntrain_X, valid_X, train_y, valid_y = train_test_split(train_X_all, train_y_all, test_size=0.1, random_state=99)\n\ntrain_X_feat = feature_model.predict(train_X)\nvalid_X_feat = feature_model.predict(valid_X)\n\nprint(train_X_feat.shape)\nprint(train_y.shape)\n\nprint(train_X.shape)\nprint(train_y.shape)","38823f57":"%%time\nxgb_model = XGBRegressor(objective='multi:softmax', \n                         num_class=4)\nxgb_model.fit(train_X_feat, train_y)","4cd7faa1":"from sklearn.metrics import roc_auc_score, accuracy_score\n\ndef to_onehot(labels, n_classes=4):\n    m = np.zeros(shape=(labels.size, n_classes))\n    m[np.arange(labels.size), labels.astype('int')] = 1\n    return m\n\ndef get_acc_scores(y_true_labels, y_pred_labels):\n    y_true = to_onehot(y_true_labels)\n    y_pred = to_onehot(y_pred_labels)\n    return {\n        cat : accuracy_score(y_true.T[i], y_pred.T[i]) for \n        (i, cat) in enumerate(class_names)\n    }\n\ndef get_auc_scores(y_true_labels, y_pred_labels):\n    y_true = to_onehot(y_true_labels)\n    y_pred = to_onehot(y_pred_labels)\n    return {\n        cat : roc_auc_score(y_true.T[i], y_pred.T[i]) for \n        (i, cat) in enumerate(class_names)\n    }","4401f371":"pred_y = xgb_model.predict(train_X_feat)\n\nprint(\"Accuracy on training set:\", get_acc_scores(train_y, pred_y))\nprint(\"ROC AUC on training set:\", get_auc_scores(train_y, pred_y))","f4b75946":"pred_y = xgb_model.predict(valid_X_feat)\n\nprint(valid_y.shape)\nprint(train_y.shape)\nprint(pred_y[:3])\n\nval_auc_scores = get_auc_scores(valid_y, pred_y)\nprint(\"Accuracy on validation set:\", get_acc_scores(valid_y, pred_y))\nprint(\"ROC AUC on validation set:\", val_auc_scores)\n\nprint(\"\\nMean:\",np.mean(list(val_auc_scores.values())))","e2bd0526":"correct_preds = valid_y == pred_y\nbad_preds = valid_y != pred_y\n\nprint(\"#Mistakes\", sum(bad_preds))\n\nmistakes_X = valid_X[bad_preds]\nmistakes_y = pred_y[bad_preds]\nmistakes_y_true = valid_y[bad_preds]\n\nprint(mistakes_X.shape)\n\nshow_batch(mistakes_X, to_onehot(mistakes_y), to_onehot(mistakes_y_true))\n","8c6ac3a5":"# see: https:\/\/www.kaggle.com\/agungor2\/various-confusion-matrix-plots\nfrom sklearn.metrics import confusion_matrix\n\ndata = confusion_matrix(pred_y, valid_y)\n\ndf_cm = pd.DataFrame(data, columns = class_names, index = class_names)\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\n\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16})# font size","21965e83":"fig, axes = plt.subplots(1,2, figsize=(12,6))\n\nmistake_pred_data = {\n    'healthy': mistakes_y == 0,\n    'multiple_diseases': mistakes_y == 1,\n    'rust': mistakes_y == 2,\n    'scab': mistakes_y == 3}\nmistakes_pred_df = pd.DataFrame(data=mistake_pred_data, dtype='int')\nax = sns.countplot(mistakes_pred_df.idxmax(axis=1), ax=axes[0])\nax.set_title(\"False positives\")\n\nmistake_data_true = {\n    'healthy': mistakes_y_true == 0,\n    'multiple_diseases': mistakes_y_true == 1,\n    'rust': mistakes_y_true == 2,\n    'scab': mistakes_y_true == 3}\nmistakes_true_df = pd.DataFrame(data=mistake_data_true, dtype='int')\nax = sns.countplot(mistakes_true_df.idxmax(axis=1), ax=axes[1])\nax.set_title(\"False negatives\")\nplt.show()","bcba5ac4":"from sklearn.model_selection import GridSearchCV\n\ndef get_mean_auc(estimator, X, y_true_labels):\n    y_pred_labels = estimator.predict(X)\n    y_true = to_onehot(y_true_labels)\n    y_pred = to_onehot(y_pred_labels)\n    return np.mean([roc_auc_score(y_true.T[i], y_pred.T[i]) for i in range(len(class_names))])\n\nif DO_PARAMETER_TUNING:\n    parameters = {\n        \"n_estimators\": [5,25,50,75],\n        \"max_depth\": [2,4,6],\n        \"learning_rate\":  [0.2, 0.3, 0.4]\n    }\n\n    xgb = XGBRegressor(objective='multi:softmax', num_class=4, threads=1)\n    search = GridSearchCV(xgb, parameters, scoring=get_mean_auc, n_jobs=-1, cv=3, verbose=3)\n    search.fit(train_X_feat, train_y)\n    tuned_params = search.best_params_\nelse:\n    tuned_params = {'learning_rate': 0.4, 'max_depth': 2, 'n_estimators': 75}\n\nprint(tuned_params)","b96f9415":"%%time\nxgb_model_rust = XGBRegressor(n_estimators=tuned_params['n_estimators'],\n                              max_depth=tuned_params['max_depth'],\n                              learning_rate=tuned_params['learning_rate'],\n                              objective=\"binary:logistic\")\nxgb_model_scab = XGBRegressor(n_estimators=tuned_params['n_estimators'],\n                              max_depth=tuned_params['max_depth'],\n                              learning_rate=tuned_params['learning_rate'],\n                              objective=\"binary:logistic\")\n\n# need to split our data into two sets:\n# - one with combined 'rust' and 'multiple_diseases' as the prediction target\n# - another with 'scab' and 'multiple_diseases'\n\nhealthy_idx = train_y == class_names.index(\"healthy\")\nmultiple_idx = train_y == class_names.index(\"multiple_diseases\")\nrust_idx = train_y == class_names.index(\"rust\")\nscab_idx = train_y == class_names.index(\"scab\")\n\nhas_rust_idx = rust_idx + multiple_idx\nhas_scab_idx = scab_idx + multiple_idx\n\nxgb_model_rust.fit(train_X_feat, has_rust_idx)\nxgb_model_scab.fit(train_X_feat, has_scab_idx)","307d9cb0":"rust_pred_y = xgb_model_rust.predict(valid_X_feat)\nscab_pred_y = xgb_model_scab.predict(valid_X_feat)\n\nval_multiple_idx = valid_y == class_names.index(\"multiple_diseases\")\nval_healthy_idx = valid_y == class_names.index(\"healthy\")\nval_rust_idx = valid_y == class_names.index(\"rust\")\nval_scab_idx = valid_y == class_names.index(\"scab\")\n\nrust_auc = roc_auc_score(val_rust_idx, rust_pred_y)\nscab_auc = roc_auc_score(val_scab_idx, scab_pred_y)\nmult_auc = roc_auc_score(val_multiple_idx, np.minimum(scab_pred_y, rust_pred_y))\nheal_auc = roc_auc_score(val_healthy_idx, 1 - np.maximum(scab_pred_y, rust_pred_y))\n\nprint(\"Rust AUC\", rust_auc)\nprint(\"Scab AUC\", scab_auc)\nprint(\"Multiple AUC\", mult_auc)\nprint(\"Healthy AUC\", heal_auc)\n\nprint(\"\\nMean AUC\", (rust_auc+scab_auc+mult_auc+heal_auc)\/4)","290b7ed9":"def double_model_predict(X):\n    rust_pred_y = xgb_model_rust.predict(X)\n    scab_pred_y = xgb_model_scab.predict(X)\n    \n    multiple_pred_y = np.minimum(scab_pred_y, rust_pred_y)\n    healthy_pred_y = 1 - np.maximum(scab_pred_y, rust_pred_y)\n    \n    return np.stack((healthy_pred_y,multiple_pred_y,rust_pred_y,scab_pred_y)).T\n    \nprint(double_model_predict(valid_X_feat)[:3])","08af6ce1":"%%time\ntest_files = pd.concat([test_df[\"image_id\"], \n                        test_df[\"image_id\"].apply(lambda x : x+\".jpg\")],\n                        axis=1)\ntest_files.columns = [\"image_id\", \"file\"]\n\ntest_batch_gen = ident_gen.flow_from_dataframe(\n    test_files, \n    directory=image_dir, \n    x_col='file', \n    target_size=IMAGE_SIZE, \n    color_mode='rgb',\n    class_mode=None,\n    batch_size=len(test_files),\n    shuffle=False)\n\ntest_X_img = next(test_batch_gen)","bcf447ca":"%%time\ntest_X_feat = feature_model.predict(test_X_img)","ba69461e":"%%time\ntest_y = double_model_predict(test_X_feat)","2f8c6f92":"show_batch(test_X_img, to_onehot(np.argmax(test_y, axis=-1)))","d606f9b3":"print(test_y.shape)\n#test_y = softmax(test_y, axis=1)\nids = test_files['image_id'].to_numpy()\nprint(ids.shape)\n\nnp.savetxt('submission.csv', \n           np.rec.fromarrays([ids] + [test_y[:,i] for i in range(4)]), \n           fmt=['%s', '%.2f', '%.2f', '%.2f', '%.2f'], \n           delimiter=',', \n           header='image_id,healthy,multiple_diseases,rust,scab', \n           comments='')\n\n!head submission.csv","dd1b78bb":"# Checking the training data\n\nReading jpgs with ImageDataGenerator and flow_from_dataframe.\n\nDisplay training samples and labels to check that we are reading them correctly.","7c9d6e66":"Evaluating the model on the training set:","fb9b59cc":"## Confusion matrix","a18cccd0":"# Multiple classifiers\n\nWhat if instead of predicting over the four classes, we train two models: one to predict 'rust' and another to predict 'scab'?\n\nThe 'healthy' and 'multiple_diseases' classes could then be inferred from the results of the two classifiers.","17c34e99":"Evaluating the model on the validation set:\n\nAs per the evaluation page, \"Submissions are evaluated on mean column-wise ROC AUC.\" so we take the average of each predicted column's AUC score.","7b6259a1":"## Converting binary columns to labels","ed8ce1fe":"## Mistake counts","196652af":"# Viewing the mistakes","ee022f66":"## Saving the predictions","286edc8a":"## Visual check of predictions","e54d21a4":"# Preparing the training data\n\nThis converts class information from one-hot columns to integer labels and uses the feature extraction model to convert input images to feature vectors.","d447b81f":"# Loading the training data\n\nThe XGBoost model doesn't support training via a generator, so we use flow_from_dataframe to create a single batch containing all training images.","e074c146":"Great, but making the predictions was a hassle. Lets wrap it in a function...","1695c472":"# Feature extraction and XGBoost\n\nCompetition: https:\/\/www.kaggle.com\/c\/plant-pathology-2020-fgvc7\/\n\nThe sensible approach here seems to be using transfer learning and fine-tuning to adapt a pretrained CNN to the task. \n\nInstead, I'll see how well I can do without training a neural network and maybe learn something in the process.\n\n### This notebook in a nutshell:\n- Taking a look at the dataset\n- Using pretrained neural network for feature extraction\n- Training and predicting with decision-tree based classifier (e.g. XGBoost)\n- Hyperparameter tuning with a grid search\n- Separate classifiers for rust\/scab classes","f5d4788c":"# Evaluating the model\n\nFirst, some helper functions for scoring and dealing with categorical predictions","0d41ed15":"Finally, training. \n\nBeing careful to use the proper objective and number of classes with XGBRegressor.","803abae8":"# Parameter tuning\n\nTrying out the sklearn library grid search implementation","2305917e":"# Loading a pretrained CNN for feature extraction\n\nUsing an EfficientNet pretrained on the imagenet dataset. We drop the prediction layer and add global average pooling to get a model which converts the input image into a vector of feature activations.","51410904":"# Predicting on the test set","78c29592":"# Data prep & inspection"}}