{"cell_type":{"8246b493":"code","11a4fcc2":"code","94104d37":"code","10c8ff3a":"code","3b33054b":"code","af03ff9f":"code","c2b2fca2":"code","1dd5c08f":"code","321a5645":"code","b7fad95e":"code","5539cfcd":"code","785243e2":"code","9927f710":"code","d7892bc2":"code","e580516f":"code","e16123f8":"code","6c161305":"code","a17f874c":"code","d699353d":"code","dabc7a9d":"code","0e2f490f":"code","7547870d":"code","30947329":"code","f6501ea2":"code","a4911554":"code","703626d6":"markdown","f285fbb4":"markdown","c8ec1c9a":"markdown","a1e1a715":"markdown","5ec0c76d":"markdown","7150f0af":"markdown","4326094c":"markdown","0d6d5728":"markdown","844e9b19":"markdown","91ab249b":"markdown","27b6b1d7":"markdown","994e0137":"markdown","550991c2":"markdown","5dbf06b6":"markdown","614bdee1":"markdown","899044d9":"markdown","bf27304b":"markdown","ac99db64":"markdown","92722292":"markdown","51535e65":"markdown","470951df":"markdown","53253287":"markdown","4d9b5fa6":"markdown","b7e6e4e7":"markdown","56295338":"markdown","5cc7e504":"markdown","50dae2e1":"markdown","1a2e8f03":"markdown","7dc06e38":"markdown","8803f836":"markdown","dfd4fc0a":"markdown","1a765a5d":"markdown","d130241a":"markdown","03113f4d":"markdown","8cbe2341":"markdown"},"source":{"8246b493":"# Loading the libraries needed\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.linalg import eigh \nimport seaborn as sns\n","11a4fcc2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","94104d37":"# Loading MNIST Dataset\ndf = pd.read_csv(\"\/kaggle\/input\/MNIST_train.csv\")","10c8ff3a":"# shape of the dataset\ndf.shape","3b33054b":"# Storing labels separately\nlabel = df['label']","af03ff9f":"# Removing label from the dataset and storing only actual data\ndata = df.drop(\"label\",axis=1)","c2b2fca2":"data.shape","1dd5c08f":"# Visualizing first 5 rows of the dataset\ndata.head()","321a5645":"print(data.shape)\nprint(label.shape)","b7fad95e":"# display or plot a number.\nplt.figure(figsize=(7,7))\nidx = 100\n\ngrid_data = data.iloc[idx].values.reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\nplt.show()\n\nprint(label[idx])","5539cfcd":"label_sample = label #.head(1000)\ndata_sample = data #.head(1000)\n\nprint(\"the shape of sample data = \", data_sample.shape)","785243e2":"# Data-preprocessing: Standardizing the data\n\ndata_std = StandardScaler().fit_transform(data_sample)\nprint(data_std.shape)","9927f710":"#find the co-variance matrix which is : (1\/n) * A^T * A\n\n# matrix multiplication using numpy\n\ncovar_matrix = np.matmul(data_std.T , data_std)\/data_std.shape[0]\n\nprint ( \"The shape of variance matrix = \", covar_matrix.shape)","d7892bc2":"covar_matrix","e580516f":"# finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space.\n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n# this code generates only the top 2 (782 and 783) eigenvalues.\nvalues, vectors = eigh(covar_matrix, eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\n# converting the eigen vectors into (2,d) shape for easyness of further computations\nvectors = vectors.T\n\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","e16123f8":"#swapping the rows of eigen vectors since the largest value is in second place\nfinal_vectors = np.vstack((vectors[1],vectors[0]))","6c161305":"final_vectors.shape","a17f874c":"# projecting the original data sample on the plane \n#formed by two principal eigen vectors by vector-vector multiplication.\n\nnew_coordinates = np.matmul(final_vectors, data_std.T)\n\nprint (\" resultanat new data points' shape \", final_vectors.shape, \"X\", data_std.T.shape,\" = \", new_coordinates.shape)","d699353d":"# appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, label_sample)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","dabc7a9d":"# ploting the 2d data points with seaborn\n\nsns.FacetGrid(dataframe, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","0e2f490f":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()","7547870d":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(data_sample)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)\n\n","30947329":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, label_sample)).T","f6501ea2":"# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nsns.FacetGrid(pca_df, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","a4911554":"# PCA for dimensionality redcution (non-visualization)\n\npca.n_components = 784\npca_data = pca.fit_transform(data_std)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n\n\n# If we take 200-dimensions, approx. 90% of variance is expalined.","703626d6":"### Standardizing the data","f285fbb4":"Projection Matrix is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. If it is for the visualizations, ideally one should keep components with heighest eigenvalues, and if it is for the dimensionality reduction, one should keep \"k\" components, where \"k\" is decided by how much spread we need to retain from the original data.","c8ec1c9a":"### Projection Matrix","a1e1a715":"As we can see, the loaded dataset has 42000 data samples, each with 784 dimensional data. ","5ec0c76d":"### Projection Matrix","7150f0af":"Below are the steps we need to follow:\n    1. Data Standardization\n    2. Covariance Matrix Computation\n    3. Compute the Eigen vectors and Eigen values of Covariance matrix\n    4. Projection Matrix\n    5. Recast the data along principal component access","4326094c":"###  Compute the Eigen vectors and Eigen values of Covariance matrix","0d6d5728":"![Std.png](attachment:Std.png)","844e9b19":"In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.","91ab249b":"Principal Component Analysis (PCA) is a dimensionality reduction technicque which is used to reduce the dimension of the data. There are 2 main needs behind reducing the dimensionality in any given data:\n        1. To visualize the data: Human can't visualize the data in higher dimensions. To visualize any given data, we need it in 2 or at the most, 3 dimensions.\n        2. To optimize ML Algorithms: In any given data, not all the dimensions will be of same significance. Through PCA, we can identify the data of higher significance and use only those for the analysis.","27b6b1d7":"### Data Standardization","994e0137":"Following are major drawbacks of PCA:\n    1. Independent variables become less interpretable\n    2. Information loss\n    3. Standardizing the data is mandatory before perforing PCA","550991c2":"## Advantages of PCA:","5dbf06b6":"The aim of data standardization is to have data in same range so that each of them contribute equially to the analysis. ","614bdee1":"### Covarinace matrix calculation","899044d9":"## Implementation and visualization using basic libraries","bf27304b":"The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them.","ac99db64":"## Drawbacks of PCA:","92722292":"## Steps in PCA:","51535e65":"## Loading and visualization of MNIST dataset","470951df":"## PCA for dimensionality redcution","53253287":"So for the given sample visualization, we can see that the image displayed from 784 dimensional data for 200th sample as well as the label for it both are \"9\"","4d9b5fa6":"![](http:\/\/)","b7e6e4e7":"The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \u201ccore\u201d of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.","56295338":"To calculate covariance matrix for a matrix X, here is the formula: \n        Cov(X) = (1\/n)*(X.Transpose * X)","5cc7e504":"### Transform the data along principal component access","50dae2e1":"# Principal Component Analysis (PCA)","1a2e8f03":"A beautiful explanation for Eignen values and vectors can be found here: https:\/\/sebastianraschka.com\/Articles\/2015_pca_in_3_steps.html","7dc06e38":"## What is PCA:","8803f836":"Observations: \n    1. The graph indicates the coverage of variance as number of components increase\n    2. For example, if we pick 100 components (Greatest 100 Eigen values and corresponding Eigen vectors), we will cover 70% of the variance from the original data. So we will miss 30% of information\n    3. If we need to cover 95% of the variance in the original data, we need to pick roughly 350 samples","dfd4fc0a":"Following are the major advantages of PCA:\n    1. Removes correlated features\n    2. Improves algorithm performance (Since we are reducing the dimension)\n    3. Reduces overfitting\n    4. Improves visializations","1a765a5d":"### Recast the data along principal component access","d130241a":"### Covariance Matrix Computation","03113f4d":"### Compute the Eigen vectors and Eigen values of Covariance matrix","8cbe2341":"## Implementation using Scikit-Learn"}}