{"cell_type":{"095dd45b":"code","a40d19b5":"code","68993f1d":"code","e1dc644f":"code","8f5264c2":"code","1b7f1c5e":"code","0f5822fc":"code","39947231":"code","800945f4":"code","a1ee6218":"code","2926c7bf":"code","ac3d2f02":"code","51b58e3b":"code","d80e0a6a":"code","006f9829":"code","814c78bb":"code","a7793f31":"code","f3273881":"code","3dfb0b9e":"code","b328c8ba":"code","0837643c":"code","4b187fcc":"code","d7617a36":"markdown","ee7040d5":"markdown","ff51da9e":"markdown","bd047171":"markdown","6cdc89b3":"markdown","88f5bb47":"markdown","1c1ecb0e":"markdown","dda64a2a":"markdown","78b22727":"markdown","7f13326f":"markdown","a264c12e":"markdown","93eea346":"markdown","12778c75":"markdown","c3cf96f8":"markdown","f34284da":"markdown","6b12a5d4":"markdown"},"source":{"095dd45b":"import torch\nimport torchvision\nimport os\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import sample\nimport random\nfrom sklearn.metrics import confusion_matrix\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\nimport torch.nn as nan\nfrom torch.autograd import Variable\n\nimport time\n","a40d19b5":"root_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray'\ntrain_path = os.path.join(root_path,'train')\ntest_path = os.path.join(root_path,'test')\ncategory = ['NORMAL', 'PNEUMONIA']\nimg_size = 224\n# AlexNet uses 224x224x3","68993f1d":"def load_data_dir(data_path, img_size,labels):\n    # load data from the data_path\n    data = [] \n    for label in labels: \n        dir_path = os.path.join(data_path, label)\n        binary_label = labels.index(label)\n        for img_path in os.listdir(dir_path):\n            if img_path[-5:]=='.jpeg':\n                img = cv2.imread(os.path.join(dir_path, img_path), cv2.IMREAD_GRAYSCALE)\n                # Resize img by img_size x img_size\n                resized_img = cv2.resize(img, (img_size, img_size))\n                data.append([resized_img, binary_label])   \n\n    return np.array(data, dtype=object)","e1dc644f":"train_df = load_data_dir(train_path, img_size, category)\ntest_df = load_data_dir(test_path, img_size, category)\nprint(\"# of train data: \" + str(len(train_df)))\nprint(\"# of test data: \" + str(len(test_df)))","8f5264c2":"def subplot_square(data, label, size):\n    # [size x size] subplot figure\n    \n    fig, ax = plt.subplots(size,size,figsize = (9, 9))\n    ran_i = random.sample(range(0,len(data)-1),size*size)\n    for i in range(size):\n        for j in range(size):\n            ax[i,j].imshow(data[ran_i[i*size+j]],cmap='gray')\n            ax[i,j].set_title(label[ran_i[i*size+j]]) \n            ax[i,j].set_axis_off()  ","1b7f1c5e":"subplot_square(train_df[:,0], train_df[:,1], size=5)","0f5822fc":"fig, axes = plt.subplots(2,1)\nsns.countplot(x=train_df[:,1], ax=axes[0])\naxes[0].set_title('Train') \nsns.countplot(x=test_df[:,1], ax=axes[1])\naxes[1].set_title('Test') ","39947231":"N_i = np.argwhere(train_df[:,1]==0).ravel().tolist()\nP_i = np.argwhere(train_df[:,1]==1).ravel().tolist()\nsampled_P_i = sample(P_i,len(N_i))\ntrain_df = train_df[sampled_P_i+N_i, :]\n\nN_i = np.argwhere(test_df[:,1]==0).ravel().tolist()\nP_i = np.argwhere(test_df[:,1]==1).ravel().tolist()\nsampled_P_i = sample(P_i,len(N_i))\ntest_df = test_df[sampled_P_i+N_i, :]\n\nfig, axes = plt.subplots(2,1)\nsns.countplot(x=train_df[:,1], ax=axes[0])\naxes[0].set_title('Train') \nsns.countplot(x=test_df[:,1], ax=axes[1])\naxes[1].set_title('Test') ","800945f4":"def Build_Dataloader(df, BATCH_SIZE):\n    # Input: df\n    #    - df[:,0]: img\n    #    - df[:,1]: label\n    # Output: Dataloader\n    \n    x, y = [], []\n    for img,label in df:\n        x.append(img)\n        y.append(label)\n    \n    # Normalize pixel values from [0,255] to [0,1] \n    x = np.array(x)\/255\n    y = np.array(y)\n    dataset = TensorDataset(torch.from_numpy(x.astype(np.float32)),\n                            torch.from_numpy(y))\n    data_loader = DataLoader(dataset,\n                             batch_size=BATCH_SIZE,\n                             shuffle = True,\n                             drop_last=True)\n    return data_loader","a1ee6218":"BATCH_SIZE = 64\ntrain_loader = Build_Dataloader(train_df, BATCH_SIZE = BATCH_SIZE)\ntest_loader = Build_Dataloader(test_df, BATCH_SIZE = BATCH_SIZE)","2926c7bf":"class AlexNet(nn.Module):\n    def __init__(self, num_classes=2):\n        super(AlexNet, self).__init__()\n        \n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11,stride=4,padding=2),\n            nn.ReLU(),\n            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(96,256,5,padding=2),\n            nn.ReLU(),\n            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(256,384,3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(384,384,3,padding=1),\n            nn.ReLU(),\n            nn.Conv2d(384,256,3,padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3,stride=2), # (b x 256 x 6 x 6)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5, inplace=True),\n            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n            nn.ReLU(),\n            nn.Dropout(p=0.5, inplace=True),\n            nn.Linear(in_features=4096, out_features=4096),\n            nn.ReLU(),\n            nn.Linear(in_features=4096, out_features=num_classes),\n        )\n        \n        self.init_bias()\n        \n    def init_bias(self):\n        for layer in self.net:\n            if isinstance(layer, nn.Conv2d):\n                nn.init.normal_(layer.weight, mean=0, std=0.01)\n                nn.init.constant_(layer.bias,0)\n        nn.init.constant_(self.net[4].bias,1)\n        nn.init.constant_(self.net[10].bias,1)\n        nn.init.constant_(self.net[12].bias,1)\n        \n    def forward(self,x):\n        x = self.net(x)\n        x = x.view(-1,256 * 6 * 6)\n        x = self.classifier(x)\n        return x","ac3d2f02":"# Use GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\nmodel = AlexNet().to(device)","51b58e3b":"error = nn.CrossEntropyLoss()\nlearning_rate = 0.0001\n\n# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","d80e0a6a":"count = 0\nnum_epochs = 50\nloss_list = []\nepoch_list = []\naccuracy_list = []\nprecision_list = []\naccuracy_list = []\nrecall_list = []\n\nstart = time.time()\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        train = Variable(images.view(BATCH_SIZE,1,224,224)).to(device)\n        labels = Variable(labels).to(device)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n\n    # Calculate Accuracy for each epoch   \n    correct = 0\n    total = 0\n    tp,fp,fn,tn = 0,0,0,0\n    # Iterate through test dataset\n    for images, labels in test_loader:\n        images, labels = images.cuda(), labels.cuda()\n        test = Variable(images.view(BATCH_SIZE,1,224,224)).to(device)\n        # Forward propagation\n        outputs = model(test)\n        \n        # Get predictions from the maximum value\n        predicted = torch.max(outputs.data, 1)[1]\n        predicted_ = predicted.cpu().detach().numpy()\n        labels_ = labels.cpu().detach().numpy()\n        cm = confusion_matrix(labels_, predicted_)\n        tn_, fp_, fn_, tp_ = cm.ravel()\n\n        tn += tn_\n        fp += fp_\n        fn += fn_\n        tp += tp_\n        total += len(labels)\n        correct += (predicted == labels).sum()\n\n    accuracy = 100*correct\/float(total)\n    accuracy_list.append(accuracy)\n    \n    if tp==0:\n        # Avoid 0\/0 cases\n        precision=0\n        recall=0\n    else:\n        precision = 100*tp\/float(tp+fp)\n        recall = 100*tp\/float(tp+fn)\n    precision_list.append(precision)\n    recall_list.append(recall)\n    loss_list.append(loss.data)\n    epoch_list.append(epoch)\n    \n\n    # Print Loss\n    if epoch%5==0:\n        print('Epoch: {}  Loss: {}  Accuracy: {} %'.format(epoch, loss.data, accuracy))\n\nend = time.time()\nprint('Elaspsed time: ' + str(end - start))","006f9829":"fig, axes = plt.subplots(4,1,figsize=(9,9))\naxes[0].plot(epoch_list,loss_list)\naxes[0].set_ylabel(\"Loss\")\naxes[1].plot(epoch_list,accuracy_list)\naxes[1].set_ylabel(\"Accuracy\")\naxes[2].plot(epoch_list,recall_list)\naxes[2].set_ylabel(\"Recall\")\naxes[3].plot(epoch_list,precision_list)\naxes[3].set_ylabel(\"Precision\")\naxes[3].set_xlabel(\"Epoch\")","814c78bb":"torch.save(model.state_dict(), '.\/model1.pth')","a7793f31":"inf_model = AlexNet()\ninf_model.to(torch.device('cuda'))\ninf_model.load_state_dict(torch.load('.\/model1.pth'))\ninf_model.eval()","f3273881":"val_path = os.path.join(root_path,'val')\nval_df = load_data_dir(val_path, img_size, category)\nprint(\"# of test data: \" + str(len(val_df)))","3dfb0b9e":"x_val, y_val = [], []\nfor img,label in val_df:\n    x_val.append(img)\n    y_val.append(label)\n\n# Normalize pixel values from [0,255] to [0,1] \nx_val = np.array(x_val)\/255\ny_val = np.array(y_val)\nimg = torch.from_numpy(x_val.astype(np.float32))","b328c8ba":"pred = []\nfor i in range(len(val_df)):\n    val = Variable(img[i].view(1,1,224,224)).to(device)\n    outputs = inf_model(val)\n    predicted = torch.max(outputs.data,1)[1]\n    pred.append(predicted.item())","0837643c":"cm = confusion_matrix(y_val, pred)\ntn, fp, fn, tp = cm.ravel()\nprecision = tp\/(tp+fp)*100\nrecall = tp\/(tp+fn)*100\nprint(cm)\nprint('Accuracy: ' + str(sum(pred==y_val)\/len(y_val)*100))\nprint('Precision: ' + str(precision))\nprint('Recall: ' + str(recall))\nsns.heatmap(cm,annot=True,cbar=False)","4b187fcc":"fig, ax = plt.subplots(4,4, figsize=(9,9))\nfor i in range(4):\n    for j in range(4):\n        ax[i,j].imshow(x_val[i*4+j],cmap='gray')\n        ax[i,j].set_title('Pred: ' + str(pred[i*4+j]) \n                          + \" Label: \"+str(y_val[i*4+j]))\n        ax[i,j].set_axis_off()","d7617a36":"### 4. Build the AlexNet Model","ee7040d5":"### 3. Data Preprocessing:\n- split data to x & y\n- Normalize images\n- convert to float32 tensor\n- Build DataLoaders for train and test dataset","ff51da9e":"### 7. Inference","bd047171":"Check val directory","6cdc89b3":"Although 100% accuracy is achieved for validation, only 16 data were used. ","88f5bb47":"Check # of each class","1c1ecb0e":"### 0. Intro\nIn this exercise, we are going to use Alexnet which is one of the most basic CNNs to classify if subject has pneumonia. It is not designed to achieve 100% accuracy model but to introduce how to train a CNN model using pytorch. People can apply different methods or networks based on this very basic model.\nAfter completing this, you can try several things to increase accuracy:\n- Use different networks, such as VGG, ResNet, or Inception\n- Or create your own network\n- Change some hyperparameters, such as learning rate\n- Instead of removing data to match imbalance of classes, you could use class imbalacne ratio to update weights. [(Ref.) ](https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays#1.-Introduction-+-Set-up)\n    ","dda64a2a":"### 6. Results","78b22727":"Load previously saved parameters","7f13326f":"### 5. Start Training","a264c12e":"Save parameters for inference","93eea346":"Binary label is used:\n- 0: Normal subject\n- 1: Pneumonia subject","12778c75":"Match class imbalance by removing some pneumonia cases such taht:\n- number of pneumonia subjects = number of normal subjects","c3cf96f8":"### 2. EDA\nLet's visualize some images.","f34284da":"### 1. Data Loading","6b12a5d4":"Visualize results"}}