{"cell_type":{"c4451764":"code","5dfd46c7":"code","eb3271f8":"code","54e74010":"code","fec37b79":"code","8765742e":"code","6664f1ed":"code","613b3152":"code","0af3313e":"code","9fe0b72e":"code","5168e9ab":"code","3ff7d1e0":"code","80f51dda":"code","2636043a":"code","3a8af50c":"code","29e06508":"markdown","d513d372":"markdown","df60e9b1":"markdown","d64c56e8":"markdown","3f656ffc":"markdown","0b8c91b0":"markdown","507cf7f8":"markdown","02287e3f":"markdown"},"source":{"c4451764":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","5dfd46c7":"df = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\") # reading the input file","eb3271f8":"print(f\"The number of sample is {df.shape[0]}\")","54e74010":"df.head(10)","fec37b79":"plt.figure(figsize=(8,4)) # The data is balanced, so we can use accuracy_score metrics for our cross validation\nsns.countplot(df['label'])\nplt.show()","8765742e":"pixel_values = df.drop(columns=['label'])\ntargets = df['label']","6664f1ed":"plt.figure(figsize=(14,12))\nfor i, (pixel, target) in enumerate(zip(pixel_values.values[:10], targets[:10])):\n    plt.subplot(1, 10, i+1)\n    img = pixel.reshape(28,28)\n    plt.imshow(img, cmap='binary')\n    plt.title(target, fontdict={'fontsize':15})\n    plt.xticks([])\n    plt.yticks([])","613b3152":"tsne = TSNE(n_components=2, random_state=42)   # reducing the n_columns to 2\ntransformed_data = tsne.fit_transform(pixel_values.iloc[:10000, :])","0af3313e":"tsne_df = pd.DataFrame(np.column_stack((transformed_data, targets[:10000])),\n                        columns=[\"X\", \"y\", \"targets\"])","9fe0b72e":"tsne_df.head(10) # Here we see there are only 2 columns in the transformed dataset","5168e9ab":"grid = sns.FacetGrid(tsne_df, hue=\"targets\", size=8)\ngrid.map(plt.scatter, \"X\", \"y\").add_legend()\nplt.title(\"CLustered data after t-SNE transformation\")","3ff7d1e0":"X = tsne_df.drop('targets', axis=1)\ny = tsne_df['targets']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=20)  #Splitting into training and validation set","80f51dda":"rfc = RandomForestClassifier(n_estimators=6)\n\nrfc.fit(X_train, y_train)","2636043a":"pred = rfc.predict(X_val)\nprint(f'The accuracy on validation set is {accuracy_score(y_val, pred)}')","3a8af50c":"scores = cross_val_score(rfc, X, y, cv=5, verbose=1, n_jobs=-1)\nprint(f'The validation scores are {scores}')\nprint(f'The mean validation score is {scores.mean()}')","29e06508":"### Lets plot some of the images","d513d372":"## Hence, we saw how we can improve model's accuracy by a simple dimensionality reduction. I do beleive that we can obtain better accuracy using neural networks, but this notebook gave an insight how powerful dimensionality reduction is.","df60e9b1":"## Training a Random Forest Classifier","d64c56e8":"References : Approaching Almost Machine Learning Problems, Abishek Thakur, 2020","3f656ffc":"## Applying t-SNE transform on full dataset is time consuming. For the purpose of this notebook, i have applied to only first 10000 samples.","0b8c91b0":"## Importing the necessary library","507cf7f8":"## About the data\nWell, you must be well known with this \"hello world\" dataset of machine learning. It includes 48000 train + 28000 test images of handwritten digits. The task is to make the model distinguish the handwritten digits(0-9). Each images consists of 784 pixels.\n\n## Why this notebook?\nI saw most people achieving high score in test set using CNN architecture. In this notebook, i have shown how you can improve your model accuracy using traditional machine learning algorithm unlike neural nets. I have implemented dimensionality reduction techinque known as t-SNE (t-Distributed Stchocastic Neighbor Embedding). \n\n\nIf you are not familiar with t-SNE, i advice you to check out [Josh Stammer's ](https:\/\/www.youtube.com\/watch?v=NEaUSP4YerM) video on youtube.","02287e3f":"## Both accuracy and cross validation score are around 94.5% which is pretty good. "}}