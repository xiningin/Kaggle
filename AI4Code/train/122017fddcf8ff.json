{"cell_type":{"2eeadd23":"code","3473574a":"code","98d1e6c0":"code","16a09bb8":"code","2c5a5414":"code","b489fc83":"code","5eed24aa":"code","21be8d51":"code","c2b1157c":"code","0eb867bf":"code","6a9c5a55":"code","99e280cc":"code","472fea3c":"code","6968f1b0":"code","25cf5f10":"code","f3c6991a":"code","4d9476e8":"code","26ac4701":"code","4ddcf48e":"code","35758860":"code","085769a6":"code","5d2a96ff":"code","42834ae2":"code","d1405411":"code","27df846c":"code","6aa48505":"code","2776fa85":"code","b206ba63":"code","727983a9":"code","1ffa3fc4":"code","adff23af":"code","fbad619b":"code","92cccb03":"code","ed5eacff":"code","fe724824":"code","ce018675":"code","6b1cbf4e":"code","61aab3f3":"code","9c519e5a":"code","3ccf1c22":"code","ee397757":"code","d0be2b82":"code","1562a1e6":"code","07878455":"code","9db69603":"code","a704af66":"code","7f86dc22":"code","e42c8a0b":"code","4b36b147":"code","6d8b0a33":"code","b6c5a7d7":"code","9b2a2f70":"code","7aca3d36":"code","40ad479e":"code","f8be7f21":"code","216cc622":"code","05f6b21e":"code","fb64662f":"code","183595ed":"code","6295c611":"code","029526cf":"code","ed60f4a7":"code","d8ab0817":"code","c7e682dc":"code","9063528a":"code","c229b686":"code","571526c4":"code","999ad29a":"code","31a5cfca":"code","ba0cd43a":"code","b2ef169c":"code","478107da":"code","96c082dc":"code","d8669dda":"code","226dec51":"code","adf950c6":"code","c2a0f3d3":"code","444019db":"code","b9b2a486":"code","dfb41f0b":"code","001b48fc":"code","63a5c4c7":"code","329e70ab":"code","316a6c6b":"code","dbed4095":"code","648d84e8":"code","bb85ec33":"code","64a51a43":"code","24a8ea7e":"code","0b5c1c57":"markdown","4a8b0305":"markdown","fe05fe03":"markdown","b4ff4e1f":"markdown","b7bd0788":"markdown","72cda80f":"markdown","c1ae5a42":"markdown","83f40d5a":"markdown","f93e9c05":"markdown","4ba46947":"markdown","e72018b4":"markdown","31a2aea5":"markdown","6e0606b8":"markdown","cc62beb6":"markdown","ae43421e":"markdown","f40528b2":"markdown","74804452":"markdown","500c29d2":"markdown","cd6d67c7":"markdown","bc3efc26":"markdown","00c3a8c1":"markdown","5874e69f":"markdown","bc7e11b2":"markdown","9cce63aa":"markdown","4b1ab921":"markdown","27f9cefb":"markdown","a86e3b03":"markdown","61aaf0a9":"markdown"},"source":{"2eeadd23":"import warnings\nwarnings.filterwarnings(action='ignore')\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier, plot_importance\nfrom sklearn.model_selection import train_test_split","3473574a":"dataset = pd.read_csv('..\/input\/lower-back-pain-symptoms-dataset\/Dataset_spine.csv')","98d1e6c0":"dataset.head()","16a09bb8":"# Unnecessary column\ndataset.iloc[:,-1:].head()","2c5a5414":"# removing Unnecessary column\ndel dataset[\"Unnamed: 13\"]","b489fc83":"# Mapping column name\ndataset.rename(columns = {\n    \"Col1\" : \"pelvic_incidence\", \n    \"Col2\" : \"pelvic_tilt\",\n    \"Col3\" : \"lumbar_lordosis_angle\",\n    \"Col4\" : \"sacral_slope\", \n    \"Col5\" : \"pelvic_radius\",\n    \"Col6\" : \"degree_spondylolisthesis\", \n    \"Col7\" : \"pelvic_slope\",\n    \"Col8\" : \"direct_tilt\",\n    \"Col9\" : \"thoracic_slope\", \n    \"Col10\" :\"cervical_tilt\", \n    \"Col11\" : \"sacrum_angle\",\n    \"Col12\" : \"scoliosis_slope\", \n    \"Class_att\" : \"class\"}, inplace=True)","5eed24aa":"dataset.head()","21be8d51":"# No null value\ndataset.isnull().sum()","c2b1157c":"# Checking data type\ndataset.info()","0eb867bf":"# Mostly mean value is greater than median. \n#We need to confirm if there are outliers in this dataset.\ndataset.describe()","6a9c5a55":"# Amount of abnormal values are greater than the other\ndataset[\"class\"].value_counts()","99e280cc":"dataset[\"class\"].value_counts().sort_index().plot.bar()","472fea3c":"dataset.groupby('class').mean()","6968f1b0":"fig, axes = plt.subplots(3, 4, figsize = (15,20))\naxes = axes.flatten()\n\nfor i in range(0,len(dataset.columns)-1):\n    sns.boxenplot(x=\"class\", y=dataset.iloc[:,i], data=dataset, orient='v', ax=axes[i],palette=\"magma_r\")\n\nplt.tight_layout()\nplt.show()","25cf5f10":"plt.subplots(figsize=(12,8))\nsns.heatmap(dataset.corr(), annot=True)","f3c6991a":"dataset.corr()['pelvic_incidence'].sort_values(ascending=False) ","4d9476e8":"dataset.hist(figsize=(15,12),bins = 20, color=\"#007959AA\")\nplt.title(\"Features Distribution\")\nplt.show()","26ac4701":"# I splitted dataset because there are lots of variables, which each of dataset including class column\nvis1 = dataset.iloc[:,[1,2,3,4,5,6,12]]\nvis2 = dataset.iloc[:,[7,8,9,10,11,12]]","4ddcf48e":"sns.pairplot(vis1, hue=\"class\")","35758860":"sns.pairplot(vis2, hue=\"class\")","085769a6":"plt.subplots(figsize=(15,6))\ndataset.boxplot(patch_artist=True, sym=\"k.\")\nplt.xticks(rotation=90)","5d2a96ff":"df = dataset.copy()\ndf.head()","42834ae2":"# Checking skewness which is over 1\nfrom scipy.stats import skew\nfeatures_index = df.dtypes[df.dtypes != 'object'].index\n\nskew_features = df[features_index].apply(lambda x : skew(x))\nskew_features_top = skew_features[skew_features> 1]\nprint(skew_features_top.sort_values(ascending=False))","d1405411":"sns.distplot(df['degree_spondylolisthesis'])\nprint(\"Skewness: %f\" % df['degree_spondylolisthesis'].skew())","27df846c":"# detecting Outlier\n# Inter Quartile Range is the distance between the 3rd Quartile and the first Qartile\n\nminimum = 0\nmaximum = 0\n\ndef detect_outlier(feature):\n    first_q = np.percentile(feature, 25)\n    third_q = np.percentile(feature, 75) \n    IQR = third_q - first_q\n    IQR *= 1.5\n    minimum = first_q - IQR \n    maximum = third_q + IQR\n    flag = False\n    \n    if(minimum > np.min(feature)):\n        flag = True\n    if(maximum < np.max(feature)):\n        flag = True\n    \n    return flag","6aa48505":"X = df.iloc[:, :-1]\nfor i in range(len(X.columns)):\n    if(detect_outlier(X[X.columns[i]])):\n        print(X.columns[i], \"Contains Outlier\")","2776fa85":"def remove_outlier(feature):\n    first_q = np.percentile(X[feature], 25)\n    third_q = np.percentile(X[feature], 75)\n    IQR = third_q - first_q\n    IQR *= 1.5\n    \n    minimum = first_q - IQR # the acceptable minimum value\n    maximum = third_q + IQR # the acceptable maximum value\n    \n    median = X[feature].median()\n    \n    X.loc[X[feature] < minimum, feature] = median \n    X.loc[X[feature] > maximum, feature] = median\n\nfor i in range(len(X.columns)): \n        remove_outlier(X.columns[i])","b206ba63":"for i in range (3):\n    for i in range(len(X.columns)):\n        remove_outlier(X.columns[i])","727983a9":"plt.subplots(figsize=(15,6))\nX.boxplot(patch_artist=True, sym=\"k.\")\nplt.xticks(rotation=90)","1ffa3fc4":"scaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(X)\nscaled_df = pd.DataFrame(data = scaled_data, columns = X.columns)\nscaled_df.head()","adff23af":"from scipy.stats import skew\nfeatures_index = X.dtypes[X.dtypes != 'object'].index\n\nskew_features = X[features_index].apply(lambda x : skew(x))\nskew_features_top = skew_features[skew_features> 0]\nprint(skew_features_top.sort_values(ascending=False))","fbad619b":"sns.distplot(X['degree_spondylolisthesis'])\nprint(\"Skewness: %f\" % X['degree_spondylolisthesis'].skew())","92cccb03":"label = dataset[\"class\"]\nencoder = LabelEncoder()\n\nlabel = encoder.fit_transform(label)\nlabel","ed5eacff":"X = scaled_df\ny = label \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","fe724824":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, accuracy_score\n\nlog_rg = LogisticRegression().fit(X_train, y_train)\ncross_val_score(log_rg, X_train, y_train, cv=5)\nprint(log_rg.score(X_test, y_test))\nprint(classification_report(y_test, log_rg.predict(X_test)))\n\nfeature_importance_lr = pd.DataFrame(zip(X.columns.values, log_rg.coef_.ravel()))\nfeature_importance_lr.columns = ['feature', 'coef']\nfeature_importance_lr.sort_values(\"coef\", ascending=False, inplace=True)","ce018675":"feature_importance_lr","6b1cbf4e":"selected_features = scaled_df[['pelvic_tilt', 'pelvic_radius', 'degree_spondylolisthesis',\n        'cervical_tilt', 'scoliosis_slope','sacral_slope', 'pelvic_incidence']]\n\nX = selected_features.iloc[:,:-1]\ny = label\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5,stratify=y)","61aab3f3":"log_rg = LogisticRegression().fit(X_train, y_train)\ncross_val_score(log_rg, X_train, y_train, cv=5)\nlog_rg.score(X_test, y_test)\nprint(classification_report(y_test, log_rg.predict(X_test)))","9c519e5a":"X.hist(figsize=(15,12),bins = 20, color=\"#007959AA\")\nplt.title(\"Features Distribution\")\nplt.show()","3ccf1c22":"from sklearn.metrics import mean_squared_error\n\ndef print_accuracy(accuracy_score, score_text=False):\n    clean_accuracy = accuracy_score*100.0\n    if score_text:\n        clean_text = score_text.strip() + ' '\n        print('{}{:.2f}%'.format(clean_text, clean_accuracy))\n    else:\n        print('{:.2f}%'.format(clean_accuracy))\n\ndef get_rmse(model):\n    pred = model.predict(X_test)\n    mse = mean_squared_error(y_test ,pred)\n    rmse = np.sqrt(mse)\n    print('{0} MSE : {1}'.format(model.__class__.__name__,np.round(mse, 2)))\n    print('{0} RMSE : {1}'.format(model.__class__.__name__,np.round(rmse, 2)))\n    return mse, rmse","ee397757":"from sklearn.tree import DecisionTreeClassifier\n\ntree_model = DecisionTreeClassifier(random_state = 1000).fit(X_train, y_train)\n\n\ntree_pred = tree_model.predict(X_test)\nprint_accuracy(accuracy_score(y_test, tree_pred), 'Decision tree accuracy:')","d0be2b82":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'max_depth': [i for i in range(1, 11)], \n              'max_features': [i for i in range(1, 8)], \n              'min_samples_leaf': [i for i in range(1, 11)]}\n\ndt_grid = GridSearchCV(DecisionTreeClassifier(random_state = 1000), param_grid=param_grid, cv=10, return_train_score = True, n_jobs=-1)","1562a1e6":"dt_grid.fit(X_train, y_train)","07878455":"print(\"Best Score: {}%\".format(round(dt_grid.best_score_*100.0, 2)))\nprint(\"Best params: {}\".format(dt_grid.best_params_))","9db69603":"y_pred = dt_grid.best_estimator_.predict(X_test)\nprint_accuracy(accuracy_score(y_test,y_pred), 'Decision Tree Classifier:')","a704af66":"get_rmse(tree_model)\nprint('')\nget_rmse(dt_grid)","7f86dc22":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score,confusion_matrix\n\ncm = confusion_matrix(y_test,dt_grid.predict(X_test))\n\nplt.figure(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=.3)\nplt.show()\n\nprint(classification_report(y_test,dt_grid.predict(X_test)))","e42c8a0b":"from sklearn.metrics import roc_auc_score\nimport scikitplot as skplt\n\nskplt.metrics.plot_roc_curve(y_test,dt_grid.predict_proba(X_test),figsize=(6,6))\nplt.show()\n\nprint(\"Auc Score: {}\".format(roc_auc_score(y_test,dt_grid.predict(X_test))))","4b36b147":"mean_fit_times = dt_grid.cv_results_.get('mean_fit_time')\nmean_test_scores = dt_grid.cv_results_.get('mean_test_score')\nmax_score = np.argmax(mean_test_scores)\n\nplt.scatter(mean_fit_times, mean_test_scores)\nplt.xlim(min(mean_fit_times)*0.95, max(mean_fit_times*1.05))\nplt.scatter(mean_fit_times[max_score], mean_test_scores[max_score])\nplt.show()","6d8b0a33":"max_depth = dt_grid.cv_results_.get('param_max_depth').astype(int)\nmax_features = dt_grid.cv_results_.get('param_max_features').astype(int)\nmin_samples_leaf = dt_grid.cv_results_.get('param_min_samples_leaf').astype(int)\nmean_fit_times = dt_grid.cv_results_.get('mean_fit_time')\nmean_test_scores = dt_grid.cv_results_.get('mean_test_score')\n\nparams_df = pd.DataFrame({'max_depth' : max_depth, \n                          'max_features' : max_features, \n                          'min_samples_leaf' : min_samples_leaf,\n                          'mean_fit_time' : mean_fit_times,\n                          'mean_test_score' : mean_test_scores})","b6c5a7d7":"fig, ax = plt.subplots(nrows= 1, ncols = 3, figsize = (15,5), sharey=True, sharex=True)\nplt.xlim(min(mean_fit_times)*0.95, max(mean_fit_times*1.05))\n\nfeatures = ['max_depth', 'max_features', 'min_samples_leaf']\naxes = ax.flatten()\n\nfor idx in range(3):\n    sns.scatterplot(y = 'mean_test_score', x = 'mean_fit_time', hue = features[idx], data = params_df, ax = axes[idx], palette='Greens_r')\n    \nplt.show()","9b2a2f70":"lr = LogisticRegression()\nlr.fit(X_train,y_train)\nprint_accuracy(accuracy_score(y_test, lr.predict(X_test)),'LR accuracy : ')","7aca3d36":"param_grid = {'penalty': ['l1', 'l2','elasticnet']}\nlr_grid = GridSearchCV(LogisticRegression(random_state = 1000, solver = 'lbfgs'), param_grid=param_grid, cv=10, return_train_score=True, n_jobs=-1)","40ad479e":"lr_grid.fit(X_train, y_train)","f8be7f21":"print(\"Best Score: {}\".format(lr_grid.best_score_))\nprint(\"Best params: {}\".format(lr_grid.best_params_))","216cc622":"y_pred = lr_grid.best_estimator_.predict(X_test)","05f6b21e":"print_accuracy(accuracy_score(y_test, y_pred), 'Best logisitic regression grid score:')","fb64662f":"get_rmse(lr_grid)","183595ed":"from sklearn.metrics import classification_report\n\ncm = confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=.3)\nplt.show()\n\nprint(classification_report(y_test,y_pred))","6295c611":"from sklearn.metrics import roc_auc_score\nimport scikitplot as skplt\n\nskplt.metrics.plot_roc_curve(y_test,lr_grid.best_estimator_.predict_proba(X_test),figsize=(6,6))\nplt.show()\n\nprint(\"Auc Score: {}\".format(roc_auc_score(y_test,y_pred)))","029526cf":"svc_list = []\nfor i in ['linear',\"poly\",\"rbf\"]:\n    svc = SVC(kernel = i, gamma = 'auto',random_state=1000)\n    svc.fit(X_train,y_train)\n    y_pred = svc.predict(X_test)\n    svc_list.append(y_pred)\n\nprint_accuracy(accuracy_score(y_test, svc_list[0]), 'Linear kernel accuracy:')\nprint_accuracy(accuracy_score(y_test, svc_list[1]), 'Poly kernel accuracy:')\nprint_accuracy(accuracy_score(y_test, svc_list[2]), 'RBF kernel accuracy:')","ed60f4a7":"param_grid = {'gamma': np.arange(0.01, 0.4, 0.1)}\ngrid = GridSearchCV(SVC(random_state = 1000, kernel = 'rbf'), param_grid=param_grid, cv=10, return_train_score=True,n_jobs=-1)\ngrid.fit(X_train, y_train)","d8ab0817":"print(\"Best Score: {}\".format(grid.best_score_))\nprint(\"Best params: {}\".format(grid.best_params_))","c7e682dc":"gammas = list(grid.cv_results_.get('param_gamma'))\nmean_test_scores = list(grid.cv_results_.get('mean_test_score'))\nmean_train_times = list(grid.cv_results_.get('mean_fit_time'))","9063528a":"fig, axes = plt.subplots(nrows = 2, ncols = 1, sharex = True, sharey = False)\naxs = axes.flatten()\n\nplt.sca(axs[0])\nsns.lineplot(gammas, mean_test_scores, color = '#44bd32')\nplt.axvline(0.31, color = '#fbc531')\nplt.ylabel('Mean Test Score')\nplt.title('Mean Test Score during CV as Gamma increases')\n\nplt.sca(axs[1])\nsns.lineplot(x=gammas, y=mean_train_times, color=\"#8c7ae6\")\nplt.ylabel('Mean Training Time')\nplt.axvline(0.31, color = '#fbc531')\nplt.xlabel('Gamma value')\nplt.show()","c229b686":"from sklearn.metrics import classification_report\n\n\ncm = confusion_matrix(y_test,svc_list[0])\n\nplt.figure(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=.3)\nplt.show()\n\nprint(classification_report(y_test,svc_list[0]))","571526c4":"lr_svc = SVC(kernel = 'linear', gamma = 'auto',random_state=1000,probability=True)\nlr_svc.fit(X_train,y_train)\ny_pred = lr_svc.predict(X_test)","999ad29a":"get_rmse(lr_svc)","31a5cfca":"skplt.metrics.plot_roc_curve(y_test,lr_svc.predict_proba(X_test),figsize=(6,6))\nplt.show()\n\nprint(\"Auc Score: {}\".format(roc_auc_score(y_test,y_pred)))","ba0cd43a":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_scores = []\nfor i in range(1,45):\n    knn = KNeighborsClassifier(n_neighbors  = i)\n    knn.fit(X_train,y_train)\n    knn_scores.append(knn.score(X_test,y_test))\n\nplt.figure(figsize=(10,5))    \nsns.lineplot(x = range(1,45), y = knn_scores)\nplt.show()","b2ef169c":"for idx, value in enumerate(knn_scores) :\n    print(idx ,':', value)","478107da":"op_knn = KNeighborsClassifier(n_neighbors  = 41)\nop_knn.fit(X_train,y_train)\ny_pred = op_knn.predict(X_test)\nprint_accuracy(accuracy_score(y_test, y_pred), 'KNN accuracy:')","96c082dc":"get_rmse(op_knn)","d8669dda":"plt.figure(figsize=(5,5))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,linewidths=.3)\nplt.show()\n\nprint(classification_report(y_test,y_pred))","226dec51":"skplt.metrics.plot_roc_curve(y_test,op_knn.predict_proba(X_test), figsize=(6,6))\nplt.show()\n\nprint(\"Auc Score: {}\".format(roc_auc_score(y_test,y_pred)))","adf950c6":"from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\nprint_accuracy(accuracy_score(y_test, rf.predict(X_test)), 'RF accuracy:')","c2a0f3d3":"l = [\"gini\",\"entropy\"]\n\ngrid = {\"criterion\":l,\n        \"n_estimators\":range(1,20), \n        'max_depth' : [6,8,10,12],\n        'min_samples_leaf':[2,4,6,8]}\n\n\nrf_cv = GridSearchCV(estimator = rf , param_grid = grid , cv = 5, n_jobs=-1)\nrf_cv.fit(X_train,y_train)\n\nprint(rf_cv.best_params_)\nprint(rf_cv.best_score_)","444019db":"y_pred = rf_cv.predict(X_test)\nprint_accuracy(accuracy_score(y_test,y_pred), \"RF accuracy : \")","b9b2a486":"get_rmse(rf_cv)","dfb41f0b":"plt.figure(figsize=(5,5))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,linewidths=.3)\nplt.show()\n\nprint(classification_report(y_test,y_pred))","001b48fc":"from sklearn.metrics import roc_auc_score\nimport scikitplot as skplt\n\nskplt.metrics.plot_roc_curve(y_test,rf_cv.best_estimator_.predict_proba(X_test),figsize=(6,6))\nplt.show()\n\nprint(\"Auc Score: {}\".format(roc_auc_score(y_test,y_pred)))","63a5c4c7":"pred_features = selected_features.copy()\n\nX = pred_features.iloc[:,:-1]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5,stratify=y)","329e70ab":"lr_final = LogisticRegression(C=10)\nlr_svc = SVC(kernel = 'linear', gamma = 'auto',probability=True)\nlr_svc.fit(X_train,y_train)\n\nop_knn = KNeighborsClassifier(n_neighbors  = 41)\nop_knn.fit(X_train,y_train)\n\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\n\nrf = RandomForestClassifier(max_depth=8, n_estimators=6, min_samples_leaf=4, min_samples_split=8)\nrf.fit(X_train,y_train)\n\ndt = DecisionTreeClassifier(max_depth=5, max_features=5, min_samples_leaf=7)\ndt.fit(X_train, y_train)","316a6c6b":"lr_pred = lr.predict(X_test)\nknn_pred = op_knn.predict(X_test)\nsvc_pred = lr_svc.predict(X_test)\nrf_pred = rf.predict(X_test)\ndt_pred = dt.predict(X_test)\n\nstacked_pred = np.array([knn_pred, lr_pred, svc_pred, rf_pred, dt_pred])\nprint(stacked_pred.shape)","dbed4095":"stacked_pred = np.transpose(stacked_pred)\nprint(stacked_pred.shape)","648d84e8":"lr_final.fit(stacked_pred, y_test)\nfinal = lr_final.predict(stacked_pred)\nlr_final_acc = accuracy_score(y_test , final)*100\n\nprint(f'Final model accuray : {lr_final_acc:.2f}%') ","bb85ec33":"mse = mean_squared_error(y_test,final)\nrmse = np.sqrt(mse)\nprint('{0} MSE : {1}'.format(final.__class__.__name__,np.round(mse, 2)))\nprint('{0} RMSE : {1}'.format(final.__class__.__name__,np.round(rmse, 2)))","64a51a43":"plt.figure(figsize=(5,5))\nsns.heatmap(confusion_matrix(y_test,final),annot=True,linewidths=.3)\nplt.show()\n\nprint(classification_report(y_test,final))","24a8ea7e":"skplt.metrics.plot_roc_curve(y_test,lr_final.predict_proba(stacked_pred),figsize=(6,6))\nplt.show()\n\nprint(\"Auc Score: {}\".format(roc_auc_score(y_test,final)))","0b5c1c57":"## Importing Data","4a8b0305":"### 2) Reducing Outliers","fe05fe03":"### 4) KNN\n\ucd9c\ucc98 : https:\/\/www.kaggle.com\/batuhangun\/lower-back-pain-symptoms-classification\n\n","b4ff4e1f":"Select variables which of absolute values are greater than 0.4\n- pelvic_radius \n- sacral_slope\n- scoliosis_slope \n- pelvic_incidence\n- cervical_tilt \n- pelvic_tilt \n- degree_spondylolisthesis ","b7bd0788":"### Scaling","72cda80f":"### 3) Checking Data Ditribution\n\n","c1ae5a42":"Using Grid Serach CV\n- max_depth : (1,10)\n- max_featues : (1,8)\n- min_samples_leaf : (1,11)\n- CV = 10","83f40d5a":"## EDA\n### 1) Null Checking","f93e9c05":"### 5) Random Forest","4ba46947":"### 3) Support Vector Machine","e72018b4":"## Data Processing","31a2aea5":"## Stacking","6e0606b8":"### 2) Confirming Data Information","cc62beb6":"### 1) Decision Tree\n* Ref : https:\/\/www.kaggle.com\/willcanniford\/predicting-back-pain-with-sklearn-and-gridsearchcv","ae43421e":"- Academicllay, pelvic_incidence is calculaed by summing pelivc_tilt and sacral slope. That is why there is a high correlation between them. Thus we need to handle this problem","f40528b2":"### 1) Data Copy","74804452":"### 2) Correlation Checking\n- Pelvic incidence is correlated with overall variables","500c29d2":"- As illustrated in table below, large difference exists between LBP and non-LBP\n  - Pelvic_incidence\n  - Pelvic_tilt\n  -Lumbar_lordosis_angle\n  -Scaral_slop  \n  -Pelvic_raidus\n  -Degree_spondylolisthesis","cd6d67c7":"### 0) Module for evaluation","bc3efc26":"## EDA\n### 1) About Target Value","00c3a8c1":"### Thanks","5874e69f":"## Modeling","bc7e11b2":"## Mini-Test for Variable Selection","9cce63aa":"### Model Accuray : 91.94%","4b1ab921":"## Pre-Processing\nIn this phase, problems below will be treated\n- Outliers\n- Scaling for normalization","27f9cefb":"degree_spondylolisthesis's skewness is quite big, so it should be treated afterward","a86e3b03":"### 2) Logistic Regression","61aaf0a9":"- gamma 0.01 value has the highest performance as below"}}