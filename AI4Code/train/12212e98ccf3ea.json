{"cell_type":{"9e78d930":"code","05e680f1":"code","92580ace":"code","8678794b":"code","6be5d3d5":"code","bacf96aa":"code","3f97a22f":"code","759d2bfb":"code","df09089d":"code","b951de89":"code","1b7abc61":"code","e27a356c":"code","5d3f9c66":"code","4fbeb9e6":"code","f3ad003f":"code","3bde5304":"code","9108669d":"code","eccfc12d":"code","409613cc":"code","d9e507a7":"code","dea88fa6":"code","ed3e6472":"code","8e5315c4":"code","2587837c":"code","b5ad31fa":"code","ea37b4fb":"code","cf482f8f":"code","c8997abc":"code","0b46e009":"code","50b6b84e":"code","6225c5ba":"code","fc1b45ff":"code","e5db3117":"code","2508552e":"code","b0f73453":"code","85dc6bdc":"code","62bf092e":"code","42f0b55a":"code","0cc0e551":"code","6c896243":"code","f951f31d":"code","80ba015f":"code","cb6ba9d0":"code","af24fd82":"code","6f6a0ead":"code","b182f2c2":"code","e3baae3f":"markdown"},"source":{"9e78d930":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05e680f1":"#Importing required libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom datetime import datetime\nimport calendar\n\n#Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#preprocessing step\nfrom sklearn.preprocessing import LabelEncoder\n\n#splitting data\nfrom sklearn.model_selection import train_test_split\n\n#Machine Learning model Algorithms\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n#performance metrics for classification model\n\nfrom sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\nimport warnings\nwarnings.simplefilter(\"ignore\")","92580ace":"#Load the training data\ntrain = pd.read_csv('..\/input\/cab-booking-dataset\/train.csv')\ntrain_label = pd.read_csv('..\/input\/cab-booking-dataset\/train_label.csv', header =None)\ntest = pd.read_csv('..\/input\/cab-booking-dataset\/test.csv')\ntest_label = pd.read_csv('..\/input\/cab-booking-dataset\/test_label.csv', header =None)","8678794b":"train['Total_booking']=train_label[0]","6be5d3d5":"train.head()","bacf96aa":"# Creating new columns from the DateTime columns\ntrain['Date']= train.datetime.apply(lambda x : x.split()[0])\ntrain['Hour']= train.datetime.apply(lambda x : x.split()[1].split(':')[0])\ntrain['Weekday']= train.Date.apply(lambda dataString : calendar.day_name[datetime.strptime(dataString,\"%m\/%d\/%Y\").weekday()])\ntrain['month']= train.Date.apply(lambda dataString : calendar.month_name[datetime.strptime(dataString,\"%m\/%d\/%Y\").month])\n\ntest['Date']= test.datetime.apply(lambda x : x.split()[0])\ntest['Hour']= test.datetime.apply(lambda x : x.split()[1].split(':')[0])\ntest['Weekday']= test.Date.apply(lambda dataString : calendar.day_name[datetime.strptime(dataString,\"%m\/%d\/%Y\").weekday()])\ntest['month']= test.Date.apply(lambda dataString : calendar.month_name[datetime.strptime(dataString,\"%m\/%d\/%Y\").month])\n","3f97a22f":"train.head()","759d2bfb":"train.info()","df09089d":"#Checking Unique value in Hour column\ntrain.Hour.unique()","b951de89":"train.shape","1b7abc61":"#Checking for null values\ntrain.isnull().sum()","e27a356c":"# Checking for numerical values in columns\ncolumns = train.columns\n\nnum_col= [col for col in train.columns if train[col].dtypes!='O']\nnum_col","5d3f9c66":"# Checking for Categorical features in columns\ncolumns = train.columns\n\ncat_col= [col for col in train.columns if train[col].dtypes=='O']\ncat_col","4fbeb9e6":"# Checking for Categorical features unique values in columns\nfor col in cat_col:\n    print(col)\n    print(train[col].unique())\n    print(\"-----------\")","f3ad003f":"# Using Label Encoder to converting categorical features to Numerical values\nle = LabelEncoder()\nfor i in train.columns:\n    if train[i].dtype=='object':\n        train[i]=le.fit_transform(train[i])\ntrain.head()","3bde5304":"#Check outlier using box plot\nplt.figure(figsize=(15,10))\ntrain.boxplot()\nplt.show()","9108669d":"# Check for Outlier using IQR method\n# Calculate the IQR\nQ1 = train['Total_booking'].quantile(0.25)\nQ3 = train['Total_booking'].quantile(0.75)\nIQR = Q3 - Q1\ntrain['Total_booking'] = np.clip(train['Total_booking'], Q1-1.5*IQR, Q3+1.5*IQR)\n\nprint(IQR)","eccfc12d":"plt.figure(figsize=(15,10))\ntrain.boxplot()\nplt.show()","409613cc":"# Checking Correlation between diffrent features\ncorrmat = train.corr()\nfig = plt.figure(figsize = (15,10))\n\nsns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt='.2f', \n                 annot_kws={'size': 10})\nplt.show()","d9e507a7":"from scipy.stats import pearsonr\nvar = ['datetime','season','holiday','workingday','weather','temp','atemp','humidity','windspeed','Date','Hour','Weekday','month']\nfor col in var:\n    coef, pval = pearsonr(train[col], train.Total_booking)\n    print('Correlation b\/w Total booking and %s - coef: %.2f, pval: %f' %(col, coef, pval))","dea88fa6":"# Visualizing Total_booking Vs other features to generate insights\nfig, axs = plt.subplots(ncols=2, nrows=7, figsize=(12,15))\ni = 0\nj = 0\nfor var in ['datetime','season','holiday','workingday','weather','temp','atemp','humidity','windspeed','Date','Hour','Weekday','month']:\n    sns.regplot(x='Total_booking', y=var, data =train, ax= axs[i][j])\n    j +=1\n    if j>1:\n        i += 1\n        j =0","ed3e6472":"pd.options.display.float_format = '{:.2f}'.format\nprint(train.Total_booking.describe())\ntrain.Total_booking.hist()\nplt.show()","8e5315c4":"# Total booking data is skewed\nsns.distplot(train.Total_booking);","2587837c":"sns.lmplot(x=\"humidity\", y=\"Total_booking\",data=train,fit_reg=True,)\nplt.xlabel(\"Humidity\")\nplt.show()","b5ad31fa":"#Relation between Total booking(1st Plot) and Month(2nd Plot)\nplt.figure(figsize=(20,15))\ntrain[train['Total_booking']==1]['month'].hist(), train[train['Total_booking']==2]['month'].hist()","ea37b4fb":"X = train.drop([\"Total_booking\"], axis=1)\ny = train[\"Total_booking\"]","cf482f8f":"X_train,X_test,y_train,y_test=train_test_split(X, y,test_size=0.20, random_state = 7)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","c8997abc":"lr = LinearRegression()\nlr.fit(X_train, y_train)\nprint(\"The final coefficients after training is :\", lr.coef_)\nprint(\"The final intercept after training is :\", lr.intercept_)","0b46e009":"predicted = lr.predict(X) \nprint(\"Predicted values: \", predicted[:4])\nprint(\"Actual values:\\n\", train[['windspeed','Total_booking']][:4])","50b6b84e":"y_pred = lr.predict(X_test)\ny_pred = lr.predict(X_test)\nprint(\"r2 score of our model is:\", r2_score(y_test,y_pred))\nprint(\"mean absolute error of our model is:\", mean_absolute_error(y_test,y_pred))\nprint(\"root mean squared error of our model is:\", mean_squared_error(y_test,y_pred,squared=False))","6225c5ba":"#Cross Validation\n# fitting a model and computing the score 5 consecutive times (with different splits each time)\nscores = cross_val_score(lr, X, y, cv=5)\n\nprint(\"Scores:\",scores)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","fc1b45ff":"k_fold  = KFold(n_splits=5)\nscores = cross_val_score(lr, X, y, cv=k_fold, scoring='neg_mean_squared_error')\n\nprint(\"Scores:\",scores)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","e5db3117":"from sklearn.preprocessing import PolynomialFeatures\ndegree_max = 6\n\nscores = []\ncv_scores = []\nfor i in range(1, degree_max+1):\n    poly = PolynomialFeatures(i)\n    X_poly = poly.fit_transform(X)\n    slm_poly = LinearRegression()\n    slm_poly.fit(X_poly, y)\n    predict_poly = slm_poly.predict(X_poly)\n    score = mean_squared_error(y, predict_poly)\n    scores.append(score)\n    \n    #Generate mean of cross validation scores\n    cv_scores.append(cross_val_score(slm_poly, X_poly, y, cv=5, scoring='neg_mean_squared_error'))\n\nprint(scores)\n","2508552e":"scoring = ['neg_mean_squared_error', 'r2']\nscores = cross_validate(lr, X, y, scoring=scoring,cv=5, return_train_score=False)\nprint(sorted(scores.keys()))\n\nprint('Mean squared error:',scores['test_neg_mean_squared_error'].mean())\nprint('r2:',scores['test_r2'].mean())","b0f73453":"#Computing bootstrap sample \ndata = train.Total_booking\nsample_1 = np.random.choice(data, len(data))\n\nprint(\"Mean: %.4f, Median: %.4f, and Standard Deviation: %.4f of MEDV\" \n      %(np.mean(data), np.median(data), np.std(data)))\nprint(\"Mean: %.4f, Median: %.4f, and Standard Deviation: %.4f of sample\" \n      %(np.mean(sample_1), np.median(sample_1), np.std(sample_1)))","85dc6bdc":"#Generating Replications of bootstrap data and plotting histogram\n\ndef generate_bootstrap(data, func):\n    # generate bootstrap replicate\n    sample_ = np.random.choice(data, len(data))\n    return func(sample_)\n\nreplicates = []\nfor i in range(10000):\n    replicate_ = generate_bootstrap(train.Total_booking, np.mean)\n    replicates.append(replicate_)\n    \nplt.hist(replicates, bins=40, density=True)\nplt.xlabel('Mean Total booking')\nplt.show()","62bf092e":"#Confidence Interval of Bootstrap replication\n\nconf_int = np.percentile(replicates, [2.5, 97.5])\nprint(\"Confidence interval of bootstrap replicate:\", conf_int)\n\n# plotting bootstrap replicates with confidence interval\nplt.hist(replicates, bins=40, density=True)\nplt.xlabel('Mean Total booking with Confidence Interval')\nplt.axvline(conf_int[0], color='k', linestyle='dashed', linewidth=1)\nplt.axvline(conf_int[1], color='k', linestyle='dashed', linewidth=1)\nplt.show()","42f0b55a":"#Ridge Regression\n\nfrom sklearn.linear_model import Ridge\n\nclf = Ridge(alpha=1.0)\n\n# fitting a model and computing the score 5 consecutive times (with different splits each time)\nscores = cross_val_score(clf, X, y, cv=5)\n\nprint(\"Scores:\",scores)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","0cc0e551":"#The Lasso\nfrom sklearn.linear_model import Lasso\nclf = Lasso(alpha=0.1)\n\n# fitting a model and computing the score 5 consecutive times (with different splits each time)\nscores = cross_val_score(clf, X, y, cv=5)\n\nprint(\"Scores:\",scores)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","6c896243":"print('Mean Squared Error: %0.4f'%mean_squared_error(y_test,y_pred))\nprint('Root Mean Squared Error %.4f'%np.sqrt(mean_squared_error(y_test,y_pred)))\nprint('R2 Score: %0.4f'%r2_score(y_test,y_pred))\ncv_lin_model=cross_val_score(LinearRegression(),X,y,cv=10)\n#Accuracy\nprint('Accuracy : %.4f (+\/- %.3f)'%(cv_lin_model.mean(),cv_lin_model.std()*2))","f951f31d":"from sklearn.feature_selection import RFE\n\nmodel=LinearRegression()\n\nrfe=RFE(model)\nfeature_selector=rfe.fit(X_train,y_train)\n\nprint('Selected Features:',X_train.columns[feature_selector.support_])\nprint('Feature Ranking: ',feature_selector.ranking_)","80ba015f":"#reducing X to selected Features\nnew_X_train=feature_selector.transform(X_train)\nnew_X_test=feature_selector.transform(X_test)\nridge_model=Ridge()\nridge_model.fit(new_X_train,y_train)\nnew_ridge_pred=ridge_model.predict(new_X_test)","cb6ba9d0":"print('Mean Squared Error: %0.4f'%mean_squared_error(y_test,new_ridge_pred))\nprint('Root Mean Squared Error %.4f'%np.sqrt(mean_squared_error(y_test,new_ridge_pred)))\nprint('R2 Score: %0.4f'%r2_score(y_test,new_ridge_pred))\ncv_lin_model=cross_val_score(Ridge(),X,y,cv=10)\n#Accuracy\nprint('Accuracy : %.4f (+\/- %.3f)'%(cv_lin_model.mean(),cv_lin_model.std()*2))","af24fd82":"params = {'n_estimators': 500,\n          'max_depth': 4,\n          'min_samples_split': 5,\n          'learning_rate': 0.01,\n          'loss': 'ls'}","6f6a0ead":"reg = GradientBoostingRegressor(**params)\nreg.fit(X_train, y_train)\n\nmse = mean_squared_error(y_test, reg.predict(X_test))\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))","b182f2c2":"test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\nfor i, y_pred in enumerate(reg.staged_predict(X_test)):\n    test_score[i] = reg.loss_(y_test, y_pred)\n\nfig = plt.figure(figsize=(6, 6))\nplt.subplot(1, 1, 1)\nplt.title('Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, reg.train_score_, 'b-',\n         label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n         label='Test Set Deviance')\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')\nfig.tight_layout()\nplt.show()","e3baae3f":"**Feature Engineering and Visualization**"}}