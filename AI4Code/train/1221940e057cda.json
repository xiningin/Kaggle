{"cell_type":{"0535c12d":"code","98158786":"code","77957ee4":"code","a8210eda":"code","2e656ac0":"code","d7444c82":"code","a787221e":"code","0ceab7b6":"code","cf0cf854":"code","15ce92b9":"code","82d9fc28":"code","69647b1b":"markdown","ebbbe385":"markdown","4364f607":"markdown","d8503242":"markdown","6e646462":"markdown","c74d035a":"markdown","7cac773f":"markdown","26dfb180":"markdown","dace99e4":"markdown","dddd9fd3":"markdown","0786d556":"markdown"},"source":{"0535c12d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","98158786":"z = np.linspace(-10, 10)\n\n# Sigmoid or logistic function\ndef sigmoid(z):\n    y = []\n    for val in z:\n        y.append(1\/(1+np.exp(-val)))\n    return y\n\nplt.plot(z, sigmoid(z), c='m', label='sigmoidal line')\nplt.axvline(x=0, c='r')\nplt.legend()\nplt.ylabel('y')\nplt.xlabel('z')\nplt.annotate('y = 1', (5,0.5))\nplt.annotate('y = 0', (-5,0.5))\nplt.show()","77957ee4":"x1 = np.linspace(-10,10)\n\nfig, ax = plt.subplots(dpi=100)\n# Our decision line\nax = plt.plot(x1, (-x1 + 3), c='r')\nplt.gca().set_aspect('equal')\nplt.axvline(x=0)\nplt.axhline(y=0)\nplt.xlim(-10,10)\nplt.ylim(-10,10)\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Example 1')\nplt.annotate('y = 1', (5,5))\nplt.annotate('y = 0', (-3,-3))\nplt.show()","a8210eda":"# Creating the 2d area we will plot our decision line in\nx1 = np.linspace(-1.0, 1.0, 100)\nx2 = np.linspace(-1.0, 1.0, 100)\nax1, ax2 = np.meshgrid(x1,x2)\n\nfig, ax = plt.subplots(dpi=100)\n# the fucntion for our decision line\nF2 = ax1**2 + ax2**2 - 1\n# plotting the decision line as a countour\nax = plt.contour(ax1,ax2,F2,[0], colors = 'r')\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n# setting the size of the y and x axis to be equal so the plot is square\nplt.gca().set_aspect('equal')\nplt.annotate('y = 1', (2,2))\nplt.annotate('y = 1', (-2,-2))\nplt.annotate('y = 0', (0.1,0.1))\nplt.axvline(x=0)\nplt.axhline(y=0)\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Example 2')\nplt.show()","2e656ac0":"# We will plot the function against the 'predicted value'\npred_v = np.linspace(-1, 2, 200)\n\nfig, ax = plt.subplots(dpi=100)\n# When y = 1\nax = plt.plot(pred_v, (-np.log(pred_v)), label='when y = 1')\n# When y = 0\nax = plt.plot(pred_v, (-np.log(1-pred_v)), label='when y = 0')\nplt.xlim(-0.1, 1.1)\nplt.axvline(x=0, linestyle=\"dashed\", c='k')\nplt.axvline(x=1,linestyle=\"dashed\", c='k')\nplt.axhline(y=0,linestyle=\"dashed\", c='k')\nplt.legend()\nplt.xlabel('h(x)')\nplt.ylabel('cost')\nplt.title('Cost of our predictions of y')\nplt.show()","d7444c82":"# Number of examples\ninstances = 5\n# Our initial guess of the weight for the feature\nweight = 0.2\n# Our inital guess for the bias\nbias = 0.7\n\ndf_cost = pd.DataFrame()\n# Our bias should always be 1 so we add the bias in each instance\ndf_cost['bias'] = [1] * 5\n# creating random input features\ndf_cost['feat1'] = np.random.randint(-9,10,instances)\n# creating random values of y (either 0 or 1)\ndf_cost['y-value'] = np.random.randint(0,2,instances)\n# Calculating our hypothesis based on our feature, weight and bias\ndf_cost['hypothesis'] = 1\/(1+np.exp(-(bias*df_cost['bias'] + weight*df_cost['feat1'])))\n# Calculated the cost (or how well we predicted the actual values of y)\ndf_cost['cost'] = -(df_cost['y-value'] * np.log(df_cost['hypothesis']))\\\n    - ((1-df_cost['y-value']) * (np.log(1-df_cost['hypothesis'])))\n\nprint(df_cost)\nprint('Our overall cost for this set of parameters is - ', np.sum(df_cost['cost'].values)\/instances, '\\n\\n')\n\n# a sigmoid or logistic function with our simple hypothesis as 'z'\ndef weighted_sigmoid(z, w, b):\n    y = []\n    for val in z:\n        y.append(1\/(1+np.exp(-(w*val+b))))\n    return y\n\nz = np.linspace(-10, 10)\nplt.plot(z, weighted_sigmoid(z, weight, bias))\nplt.scatter(df_cost['feat1'].values, df_cost['y-value'].values, c='r', label='Actual y-values')\nplt.scatter(df_cost['feat1'].values, df_cost['hypothesis'].values, c='b', label='Predicted y-values')\nplt.legend()\nplt.xlabel('feature (x)')\nplt.ylabel('y {0,1}')\nplt.show()\n\n# creating a new df to start again and show how close values of h(x) give low cost\ndf_cost2 = df_cost.drop(['hypothesis', 'cost', 'y-value'], axis =1)\n# Creating values of y which we will aim towards\ndf_cost2['y-value'] = [0,0,1,0,1]\n# Creating fake values, very close to the y-values (log(0) is undefined so won't use 0)\ndf_cost2['fake hypothesis'] = [0.1, 0.1, 0.9, 0.1, 0.9]\n# calculating the cost for our fake values of hypothesis\ndf_cost2['fake cost'] = -(df_cost2['y-value'] * np.log(df_cost2['fake hypothesis']))\\\n    - ((1-df_cost2['y-value']) * (np.log(1-df_cost2['fake hypothesis'])))\nprint(df_cost2)\nprint('Our overall cost for our fake hypothesis (hypothesis = y) - ',\n      np.sum(df_cost2['fake cost'].values)\/instances)\n\nplt.scatter(df_cost2['feat1'].values, df_cost2['y-value'].values, c='r', label='Actual y-values')\nplt.scatter(df_cost2['feat1'].values, df_cost2['fake hypothesis'].values, c='b', label='Fake y-values')\nplt.legend()\nplt.xlabel('feature (x)')\nplt.ylabel('y {0,1}')\nplt.show()","a787221e":"instances = 50\n\ndf_grad = pd.DataFrame()\ndf_grad['bias'] = [1] * 50\ndf_grad['feat'] = np.random.randint(-5,5,instances)\n# Creating our y values. These all belong to {0, 1} and represent two differnt categories\ndf_grad['y-value'] = np.nan\n# In this case\ndf_grad.loc[df_grad['feat'] <= 2, 'y-value'] = 0\ndf_grad.loc[df_grad['feat'] > 2, 'y-value'] = 1\nprint(df_grad.head())","0ceab7b6":"def log_func(X, th0, th1):\n    return 1\/(1+np.exp(-(th0 + th1*X)))\n\ndef cost_func(X, y, th0, th1):\n    mm = len(X)\n    return (-1\/mm) * np.sum((y * np.log(log_func(X, th0, th1))) + ((1-y) * (np.log(1-log_func(X, th0, th1)))))\n\ndef pderiv_th0(X, y, alpha, th0, th1):\n    mm = len(X)\n    return (1 \/ mm) * np.sum(log_func(X, th0, th1) - y)\n\ndef pderiv_th1(X, y, alpha, th0, th1):\n    mm = len(X)\n    return (1 \/ mm) * np.sum((log_func(X, th0, th1) - y)*X)","cf0cf854":"# Our features or input\nX = df_grad['feat'].values\n# Our target\ny = df_grad['y-value'].values\n\n# The parameters for our gradient descent (optimising our line of best fit)\nalpha = 0.03\nloops = 1000\nth0 = 1\nth1 = -1\nprint(f'Original parameters are {th0} for theta0 and {th1} for theta1')\n\nz = np.linspace(-6, 6)\nplt.plot(z, weighted_sigmoid(z, th1, th0))\nplt.scatter(df_grad['feat'].values, df_grad['y-value'].values, c='r', label='Actual y-values')\nplt.legend()\nplt.xlabel('feature (x)')\nplt.ylabel('y {0,1}')\nplt.title('Initial guess parameters')\nplt.show()\n\n# The containers for tracking the gradient descents\ncost_converge = []\n\nfor ii in range(1000):\n    # The new values of th0 and th1 are stored in temporary variables so that \n    # each can be calculated with the same original th0 and th1\n    temp_th0 = th0 - (alpha * pderiv_th0(X, y, alpha, th0, th1))\n    temp_th1 = th1 - (alpha * pderiv_th1(X, y, alpha, th0, th1))\n    th0 = temp_th0\n    th1 = temp_th1\n    cost_converge.append(cost_func(X, y, th0, th1))\n\nplt.plot(cost_converge)\nplt.title('Convergence of the cost function to its minimum')\nplt.xlabel('Loop number')\nplt.ylabel('Cost')\nplt.show()\nprint(f'Final parameters are {th0} for theta0 and {th1} for theta1')\n\nz = np.linspace(-6, 6)\nplt.plot(z, weighted_sigmoid(z, th1, th0))\nplt.scatter(df_grad['feat'].values, df_grad['y-value'].values, c='r', label='Actual y-values')\nplt.legend()\nplt.xlabel('feature (x)')\nplt.ylabel('y {0,1}')\nplt.title('New optimised parameters')\nplt.show()","15ce92b9":"# Calculating our hypothesis based on our feature, weight and bias\ndf_grad['hypothesis'] = 1\/(1+np.exp(-(th0*df_grad['bias'] + th1*df_grad['feat'])))\ndf_grad['prediction'] = np.nan\n# Converting our hypothesis values into predictions for y\ndf_grad.loc[df_grad['hypothesis'] < 0.5, 'prediction'] = 0\ndf_grad.loc[df_grad['hypothesis'] >= 0.5, 'prediction'] = 1\n\n# Calculating the Pearson R value\ncorrelation = df_grad['y-value'].corr(df_grad['prediction'])\nprint(df_grad.head())\nprint(f'Correlation between actual y-values and our prediction is {correlation}')","82d9fc28":"# This is the above function and updates each of the parameters simultaneously\ndef vector_update(X, y, theta, alpha):\n    mm = len(y)\n    return theta - ((alpha*mm)*np.matmul(np.transpose(X), (1\/(1+np.exp(-np.matmul(X, theta))))-y))\n\n# Obtaining our feature inputs (same as before)\nX = df_grad[['bias', 'feat']].to_numpy()\ny = df_grad['y-value'].values\n# Our inital guess of the parameters, this time in vector form\ntheta = np.array([1,-1])\nalpha = 0.003\nprint(f'Initial parameters are {theta[0]} for theta0 and {theta[1]} for theta1')\n\nfor ii in range(1000):\n    theta = vector_update(X, y, theta, alpha)\n    \nprint(f'Final parameters are {theta[0]} for theta0 and {theta[1]} for theta1')\n\nplt.plot(z, weighted_sigmoid(z, theta[1], theta[0]))\nplt.scatter(df_grad['feat'].values, df_grad['y-value'].values, c='r', label='Actual y-values')\nplt.legend()\nplt.xlabel('feature (x)')\nplt.ylabel('y {0,1}')\nplt.title('New optimised parameters')\nplt.show()","69647b1b":"If the fit of the model is perfect, $h_{\\theta}(x)$ will be the same as $y$ and so $J(\\theta)$ (the cost) will be $0$. Any deviation from this perfect fit will increase the cost as the difference between the calculated and actual y value is squared. Therefore the best set of parameters will give us the minimum point of the cost function, where the derivative is 0 \n\nWe take the partial derivative of the cost function with respect to each parameter at a time.\n$$\\frac{\\partial J}{\\partial \\theta_{0}} = \\frac{1}{m}\\sum_{i=1}^{m}((\\theta_{1}x + \\theta_{0}) - y)^{2}$$ \n$$\\frac{\\partial J}{\\partial \\theta_{1}} = \\frac{1}{m}\\sum_{i=1}^{m}((\\theta_{1}x + \\theta_{0}) - y)^{2}x$$\nand use this to determine how to change the parameter in order to decrease the cost. If the gradient is negative, the parameter needs to be increased and vice versa. This is done in a stepwise manner, taking the derivatives with a set of values for the parameters, changing each one and repeating the process \n\n$$\\theta_{0}' = \\theta_{0} - \\alpha \\frac{\\partial J}{\\partial \\theta_{0}}$$\n$$\\theta_{1}' = \\theta_{1} - \\alpha \\frac{\\partial J}{\\partial \\theta_{1}}$$\n\nWe scale this by the optimization parameter $\\alpha$ to control the descent of the cost function.","ebbbe385":"Here we have chosen y-values to be 0 when our feature is below 2 and to be 1 when our feature is above 2. This gives a simple relationship between the feature and the value that we can find with our gradient decent.","4364f607":"We have calculated the partial derivatives of the log function for $\\theta_{0}$ and $\\theta_{1}$. These will be used to alter our parameters on each cycle of gradient descent.","d8503242":"When our target for y is 1, we use the blue line\n$$cost = -log{(h(x))}$$\nand when our target for y is 0, we use the orange line\n$$cost = -log{(1-h(x))}$$\nOur $y=1$ cost function gives us a cost of 0 when we perfectly predict 1 (can't happen due to the sigmoidal curve) and a rapidly increasing cost as our prediction is further from 0. Our $y=0$ cost function is the same in reverse, giving a cost of 0 when we predict 0. We can see that the better our predictions are, or the closer they are to the actual values, the lower our cost will be.\n\nWe can reduce these cost functions to one function as \n$$\\big [-y\\log{(h_{\\theta}(x)}-(1-y)\\log{(1-x)}\\big ]$$\nTo get the cost of many samples we sum over each sample and normalise\n$$J(\\theta) = -\\frac{1}{m}\\sum^{m}_{i=1}\\big [y^{(i)}\\log{(h_{\\theta}(x^{(i)})}+(1-y^{(i)})\\log{(1-x^{(i)})}\\big ]$$\nwhere J is our overall cost and m is our number of samples\n\nWe will demonstrate this with a very simple hypothesis\n$$h(x) = \\theta_{0} + \\theta_{1} x$$","6e646462":"In 2d space (two features) we can show how the decision boundary can be changed depending on the parameters for $\\theta$. We just have to remember that $z \\equiv h(x) \\equiv \\theta^{T}X$ so that\n$$\\theta^{T}X \\geq 0 \\Rightarrow y = 1$$\n$$\\theta^{T}X < 0 \\Rightarrow y = 0$$\n\n### Example 1\nGiven a hypothesis of $h(x) = \\theta_{0} + \\theta_{1}X_{1} + \\theta_{2}X_{2}$ and parameters of $[-3, 1, 1]$ for $\\theta_{i}$ we can rearange our hypothesis to give\n$$y = 1 \\Rightarrow -3 + x_{1} + x_{2} \\geq 0$$\n$$y = 1 \\Rightarrow x_{1} + x_{2} \\geq 3$$\nwhich we have drawn above\n\n### Example 2\nWe can also create more complicated decision boundaries using polynomial terms. If we take our hypothesis as \n$$h(x) = \\theta_{0} + \\theta_{1}X_{1} + \\theta_{2}X_{2}+ \\theta_{3}X_{1}^{2} + \\theta_{4}X_{2}^{2}$$\nand our values of theta as $[-1, 0, 0, 1, 1]$ we get the boundary condition as\n$$y = 1 \\Rightarrow x_{1}^{2} + x_{2}^{2} \\geq 1$$\nWe can then plot this as ","c74d035a":"This function gives us a probability of y being equal to 0 or 1 for each value of z and we can make a decision on the boundary at which we assume the value to be 0 or 1. In this case $y=0.5$ (our decision boundary) when $x=0$ so we take $y=1$ when $x\\geq 0$ and $y=0$ when $x < 0$. In this simple case we can take x as a single feature which is not scaled by any weight, upon which we are making our predictions but there can be a range of features and we want to fit our model. We therefore replace z in the above equation with $h(x)$ \n$$h(x) = 1\\theta_{0} + x_{1}\\theta_{1} + x_{2}\\theta_{2} \\cdots$$\nwhere $h(x)$ is our hypothesis. $\\theta_{0}$ is our bias and $x_{1}, x_{2}$ etc are our features and can take any polynomial form. We can therefore rewite our hypothesis as a matrix operation\n$$\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{i}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\\n1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{i,1} & x_{i,2} & \\cdots & x_{i,n} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\theta_{0} \\\\\n\\theta_{1} \\\\\n\\theta_{2} \\\\\n\\vdots \\\\\n\\theta_{n}\n\\end{bmatrix}\n$$\nwhere n is our number of features and i is our number of instances. We can then rewrite our logistic function as\n$$ g(z) = \\frac{1}{1+e^{-\\theta^{T}X}}$$","7cac773f":"This very rapidly gives us very accurate parameters and can be extended to large numbers of features. All that happens is the $\\theta$ vector grows for each new feature.","26dfb180":"The purpose of logistic regression is classification and in particular between two possible choices (binary classification). We are therefore trying to predict y where $y \\in \\{0,1\\}$ from our input data X and weights $\\theta$ which we will calculate. For this we use the logistic (or sigmoid) function which gives values between 0 and 1.\n$$ g(z) = \\frac{1}{1+e^{-z}}$$","dace99e4":"In order to obtain values for our parameters $\\theta$ we need to train our model. Which means we need many examples, with data for our x-values and for each an answer for y (either 0 or 1). We then guess some parameters and see how well our model did at predicting the y-values. Our predicted values will not be 0 or 1 but will be between these depending on where they fall on the sigmoidal curve. A good model will accurately predict samples (or instances) on the correct side of the decision boundary. But we need a way to quantify this in order to improve upon our model. For this we use a cost function and we will now see what a simple cost function might look like.","dddd9fd3":"Our calculated parameters, plugged into the sigmoidal function gives us hypothesis values for each instance. If the hypothesis is 0.8, then there is an 80% chance that the y-value should fall into the 1 category. If the hypothesis gives a value of 0.1 for an instance then there is a 10% chance, and consequently a 90% chance that the y-value falls into the 0 category.\n\nWe then round our hypothesis value so that if hypothesis gives a value of 0.5 or higher, we assume a y-value of 1, and if hypothesis gives a value lower than 0.5, we assume a y-value of 0. This gives us a perfect correlation with the actual y-values. This was a relatively easy relationship to model as there was only one feature and the rule of 'if the feature is equal to or above 2, then y equals 1, if the deature is below 2, then y equals 0' was consistently adhered to.\n\nIf we have multiple features we can do the same process but in vector form. In this case the weights vector is made up of the weights for each feature with the bias included. The features are then taken as a matrix with one column of only ones which corresponds to the bias.\n$$h_{\\theta}(x) = \\theta_{0}1 + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\cdots + \\theta_{n}x_{n}$$\nThe gradient descent is then\n$$\\theta' = \\theta - \\frac{\\alpha}{m}X^{T}\\Big (\\frac{1}{1+e^{-X\\theta}}-y\\Big )$$\nIf n is the number of features, and i is the number of instances, our sigmoidal function will give us a $ix1$ matrix due to the multiplication of the $ixn$ X matrix and the $nx1$ $\\theta$ matrix. When the y-values are minused from this we still have a $ix1$ matrix. We then multiply $X^{T}$ which is a $nxi$ matrix with our $ix1$ matrix to finally give a $nx1$ matrix for our new values of $\\theta$. ","0786d556":"Our initial guess of the parameters (1 for $\\theta_{0}$ and -1 for $\\theta_{1}$) gives a sigmoidal shape which is a terrible fit for predicting out data. PLotting the cost, we see a very high initial cost which consistently decreases and begins to level out. Using the parameters at this stage (-2.6 for $\\theta_{0}$ and 1.1 for $\\theta_{1}$) we see a very good fit for the data."}}