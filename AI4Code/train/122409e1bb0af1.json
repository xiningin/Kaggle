{"cell_type":{"f640052b":"code","6c1927d7":"code","1d56e4b0":"code","b873ce90":"code","6ccc3d1e":"code","472fd0f4":"code","e340732e":"code","664391d5":"code","6f09ea44":"code","18e8395e":"code","90e62906":"code","94de7886":"code","5560e9bb":"code","39442e3a":"code","b88a90c9":"code","f1abdc76":"code","26c1b186":"code","7d7d124c":"code","e8afa8c9":"code","8fcdd851":"code","5d89a6fe":"code","c1f24ad9":"code","8d5e8726":"code","8b402fcf":"code","2c3346de":"code","f40b1fca":"code","205e3fc4":"code","7b479a48":"code","df6eea61":"code","bbb7cbab":"code","03892515":"code","9e021f02":"code","7daecd1e":"code","96a04d52":"code","45462bd1":"code","2aee7d95":"code","3f21d76a":"code","c60c62e7":"code","2c8fc29b":"code","7613b0eb":"code","e1be8725":"code","239fe912":"code","611ad223":"code","f124885b":"code","ee31d3bb":"code","6efa121b":"code","2ccc9369":"code","33a8cf96":"code","f03d8a90":"code","45b53b5c":"code","fb8888a7":"code","bb1cec49":"code","d61cca5b":"code","d3a780ff":"code","494c4e29":"code","4fcd8980":"code","10355849":"code","f159e7d8":"markdown","fe4468bd":"markdown","54041232":"markdown","c948c431":"markdown","8466accc":"markdown","b9a20acf":"markdown","f2fce20f":"markdown","cbec69d2":"markdown","0fd814f8":"markdown","ba0af232":"markdown","d1220737":"markdown","88ac1807":"markdown","94992aa4":"markdown","62a22002":"markdown","0eb68c1d":"markdown","7b3672f1":"markdown","3faa5106":"markdown","0c7087b8":"markdown","1f222174":"markdown","b8617e7c":"markdown","2e9af48b":"markdown","c34b2737":"markdown","6c965729":"markdown","cafd7280":"markdown","40188046":"markdown","b559f10a":"markdown","27b1839d":"markdown","6f78431a":"markdown","17af8a7b":"markdown","8b4371a5":"markdown","958e491d":"markdown","e5f0b17f":"markdown","a05ae493":"markdown","dfebb95f":"markdown","0339692c":"markdown","01a93e4b":"markdown","a1254c78":"markdown","429d639b":"markdown","ea93db26":"markdown","c1a527e8":"markdown","a400a57d":"markdown","4bd80a89":"markdown","24be1c8d":"markdown","5f8a0085":"markdown","fc3160f2":"markdown","dc4c1b0b":"markdown","bd577d33":"markdown","b65015e9":"markdown","e18cb2db":"markdown","9cc90ca4":"markdown","aebbc542":"markdown","a1ea59b1":"markdown","8273912b":"markdown","04d3422a":"markdown","c0d5c7df":"markdown","1349cf15":"markdown"},"source":{"f640052b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c1927d7":"# Import packages\nimport time\nimport gc\n\n## Basic data processing\nimport numpy as np\nimport pandas as pd\n\n## Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n## Modelling\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier, plot_importance\n\n## Model Explanatory\nimport shap  # package used to calculate Shap values\nimport eli5\n\n## Settings\npd.set_option('display.max_columns', 500) # Able to display more columns.\npd.set_option('display.max_info_columns', 150) # Able to display more columns in info().","1d56e4b0":"# Load the dataset\ndata_df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ndata_df.info() # show entries, dtypes, memory useage.","b873ce90":"# Have a look\ndata_df.head(5)","6ccc3d1e":"# Basic statistic on labels\ndata_df[\"claim\"].astype(\"object\").describe() # All the Nominal data can be treated as \"object\" type for simplicity.","472fd0f4":"# Basic statistic on features\ndata_df.loc[:, ~data_df.columns.isin([\"claim\"])].describe()","e340732e":"# Irrelevant columns\n'''\nid: id is useless for analysis and modeling.\n'''\nirrelevant_columns = ['id']\ndata_preprocessed_df = data_df.drop(irrelevant_columns, axis=1)\ndata_preprocessed_df","664391d5":"# Replace the empty data with NaN\ndata_preprocessed_df.replace(\"\", float(\"NaN\"), inplace=True)\ndata_preprocessed_df.replace(\" \", float(\"NaN\"), inplace=True)\n\n# Count missing value(NaN, na, null, None) of each columns, Then transform the result to a pandas dataframe. \ncount_missing_value = data_preprocessed_df.isna().sum() \/ data_preprocessed_df.shape[0] * 100\ncount_missing_value_df = pd.DataFrame(count_missing_value.sort_values(ascending=False), columns=['Missing%'])","6f09ea44":"# Visualize the percentage(>0) of Missing value in each column.\nmissing_value_df = count_missing_value_df[count_missing_value_df['Missing%'] > 0]\n\nplt.figure(figsize=(10, 20)) # Set the figure size\nmissing_value_graph = sns.barplot(y = missing_value_df.index, x = \"Missing%\", data=missing_value_df, orient=\"h\")\nmissing_value_graph.set_title(\"Percentage Missing Value\", fontsize = 20)\nmissing_value_graph.set_xlabel(\"Features\")","18e8395e":"# Generate a new feature from the missing value of each records, it is inspired by the scenario of the dataset.(whether a customer made a claim upon an insurance policy)\n# The less the information of the user, the higher risk of the user.\ndata_preprocessed_df['missing_num'] = data_preprocessed_df.isnull().sum(axis=1)","90e62906":"# Generate a new feature by counting the negative value of each records.\ndata_preprocessed_df['neg_num'] = (data_preprocessed_df < 0).sum(axis=1)","94de7886":"# Generate a new feature from the missing_num.(Binary value to represent whether there is missing value in this record)\ndata_preprocessed_df['missing_sign'] = data_preprocessed_df['missing_num'] != 0\ndata_preprocessed_df['missing_sign'] = data_preprocessed_df['missing_sign'] + 0","5560e9bb":"# Drop all the instance with NaN\/NA\/null\ndata_preprocessed_dropNaN_df = data_preprocessed_df.dropna().copy()\ndata_preprocessed_dropNaN_df.reset_index(drop=True, inplace=True)","39442e3a":"# The percentage of rows with missing value\n(data_preprocessed_df.shape[0] - data_preprocessed_dropNaN_df.shape[0]) \/ data_preprocessed_df.shape[0] * 100","b88a90c9":"# Get the feature names with missing values.\nmissing_features = list(missing_value_df.index)","f1abdc76":"# Grouped Median imputation by a feature which is highly related to label(claim): missing_sign\ndata_preprocessed_median_df = data_preprocessed_df.copy()\nfor feature in missing_features: \n    data_preprocessed_median_df[feature] = data_preprocessed_df.groupby('missing_sign', sort=False)[feature].apply(lambda x: x.fillna(x.median()))","26c1b186":"# Grouped Mean imputation by a feature which is highly related to label(claim): missing_sign\ndata_preprocessed_mean_df = data_preprocessed_df.copy()\nfor feature in missing_features:\n    data_preprocessed_mean_df[feature] = data_preprocessed_df.groupby('missing_sign', sort=False)[feature].apply(lambda x: x.fillna(x.mean()))","7d7d124c":"# Set one of the dataset for analysis and modeling; Choose the one with the best performance at last.\n#data_best_df = data_preprocessed_df.copy()\n#data_best_df = data_preprocessed_dropNaN_df.copy()\n#data_best_df = data_preprocessed_median_df.copy()\ndata_best_df = data_preprocessed_mean_df.copy()","e8afa8c9":"data_best_df","8fcdd851":"# Count the number of claim(0\/1), transform the result to pandas dataframe\nclaim_counts = data_best_df[\"claim\"].value_counts()\nclaim_counts_df = pd.DataFrame(claim_counts)","5d89a6fe":"# Visualize the distribution of the claim(label)\nclaim_fig = make_subplots(\n    rows=1, cols=2, \n    specs=[[{\"type\": \"xy\"}, {\"type\": \"domain\"}]])\n\nclaim_fig.add_trace(go.Bar(x=claim_counts_df.index, \n                           y=claim_counts_df[\"claim\"],\n                           text=claim_counts_df[\"claim\"],\n                           textposition='outside',\n                           showlegend=False),\n                           1, 1)\n\nclaim_fig.add_trace(go.Pie(labels=claim_counts_df.index, \n                           values=claim_counts_df[\"claim\"],\n                           showlegend=True),\n                           1, 2)\n\nclaim_fig.update_layout(\n                  height=600, \n                  width=1000,\n                  title={\n                  'text': \"The distribution of claim\",\n                  'font': {'size': 24},\n                  'y':0.95,\n                  'x':0.5,\n                  'xanchor': 'center',\n                  'yanchor': 'top'},\n                  xaxis1_title = 'claim', \n                  yaxis1_title = 'Counts',\n                  legend_title_text=\"claim\"\n                 )\nclaim_fig.update_xaxes(type='category')\nclaim_fig.show()","c1f24ad9":"# Set up the matplotlib figure\n#f, axes = plt.subplots(2, 6, figsize=(30, 10)) #suitable for two line with 6 graph.\n\nf, axes = plt.subplots(20, 6, figsize=(30, 100))\nfor feature,number in zip(data_best_df.columns, range(118)):\n    yaxix_name = 'f'+str(number+1)\n    r_pos = number \/\/ 6\n    c_pos = number % 6\n    sns.boxplot(x='claim', y=yaxix_name, data=data_best_df, ax=axes[r_pos, c_pos]).set_title(yaxix_name)","8d5e8726":"# Plot features we created\nnew_features=['missing_num','neg_num', 'missing_sign']\nf, axes = plt.subplots(2, 2, figsize=(15, 10)) #suitable for two line with 6 graph.\nfor feature,number in zip(new_features, range(4)):\n    yaxix_name = feature\n    r_pos = number \/\/ 2\n    c_pos = number % 2\n    sns.boxplot(x='claim', y=yaxix_name, data=data_best_df, ax=axes[r_pos, c_pos]).set_title(yaxix_name)","8b402fcf":"# Show the heatmap\nplt.figure(figsize=(160, 120))\nsns.heatmap(data_best_df.drop('claim', axis=1).corr(), cmap=\"coolwarm\", annot = True, fmt='.3f').set_title('Pearson Correlation for continuous features', fontsize=22)\nplt.show()","2c3346de":"# Calculate the mutual information of the dataset(Require No NaN)\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","f40b1fca":"mi_scores = make_mi_scores(data_best_df, data_best_df['claim'], False)\nmi_scores[:10]  # show a few features with their MI scores","205e3fc4":"# Choose a dataset for modelling\ndata_modelling_df = data_preprocessed_mean_df.copy()","7b479a48":"# Train\/Test Split\nX = data_modelling_df.drop(\"claim\", axis=1)\nY = data_modelling_df.claim\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state=0)","df6eea61":"# Define the evaluation dataset\neval_sets=[(x_train, y_train), (x_test, y_test)]","bbb7cbab":"# Use the missing_sign to simplely represent \"claim\"\nprint(f'Train AUC: {roc_auc_score(y_train, x_train[\"missing_sign\"])}')\nprint(f'Test AUC: {roc_auc_score(y_test, x_test[\"missing_sign\"])}')","03892515":"# Start time\nstart_time = time.time()\nxgbc = XGBClassifier(random_state=0, use_label_encoder=False)\nxgbc.fit(x_train, y_train, eval_set=eval_sets, eval_metric='auc', verbose=False)\n\n# Calculate the training time\nxgbc_time = time.time() - start_time\n\n# xgbc.evals_result() #Return the evaluation results of eval_sets\npredictions = xgbc.predict_proba(x_test)[:,1]\nauc_xgbc = roc_auc_score(y_test, predictions)\nprint(f'AUC: {auc_xgbc}')","9e021f02":"# Start time\nstart_time = time.time()\ncatbc = CatBoostClassifier(random_state=0, eval_metric='AUC')\ncatbc.fit(x_train, y_train, eval_set=eval_sets, verbose=False)\n\n# Calculate the training time\ncatbc_time = time.time() - start_time\npredictions = catbc.predict_proba(x_test)[:,1]\nauc_catbc = roc_auc_score(y_test, predictions)\nprint(f'AUC: {auc_catbc}')","7daecd1e":"# Start time\nstart_time = time.time()\n\nlgbc = LGBMClassifier(random_state=0)\nlgbc.fit(x_train, y_train, eval_set=eval_sets, eval_metric='auc', verbose=-1)\n\n# Calculate the training time\nlgbc_time = time.time() - start_time\npredictions = lgbc.predict_proba(x_test)[:,1]\nauc_lgbc = roc_auc_score(y_test, predictions)\nprint(f'AUC: {auc_lgbc}')","96a04d52":"# Collect all the model performance\nmodel_comparison = pd.DataFrame(data = [(auc_xgbc, xgbc_time), (auc_catbc, catbc_time), (auc_lgbc, lgbc_time)], \n                                index = [\"XGboost\", \"CatBoost\", \"LGBM\"],\n                                columns=['AUC', 'Time'])\\\n                     .sort_values(by = \"AUC\", ascending=False)\nmodel_comparison","45462bd1":"# Save the feature importance as a dataframe  \ncatbc_importances_df = pd.DataFrame(pd.Series(catbc.feature_importances_, index=x_train.columns), columns=['Importance']).sort_values('Importance', ascending=False)[:10]\n# Visualize the feature importance of the trained tree\nplt.figure(figsize=(10, 10))\nfeature_importance_graph = sns.barplot(y = catbc_importances_df.index, x = \"Importance\", data=catbc_importances_df, orient=\"h\")\nfeature_importance_graph.set_title(\"Top 10 Feature importance by CatBoost Classification\", fontsize = 20)\nfeature_importance_graph.set_ylabel(\"Features\")\n# Use eli5 to show the value of feature importance with colors\neli5.show_weights(catbc, feature_names = list(x_train.columns))","2aee7d95":"# Shap\nexplainer = shap.TreeExplainer(catbc)\nshap_values = explainer.shap_values(x_train)\nshap.summary_plot(shap_values, x_train)","3f21d76a":"data_lst = [data_preprocessed_df, data_preprocessed_dropNaN_df, data_preprocessed_median_df, data_preprocessed_mean_df]","c60c62e7":"tuning_res = []\n\nfor dataset_df in data_lst:\n    # Train\/Test Split\n    X = dataset_df.drop(\"claim\", axis=1)\n    Y = dataset_df.claim\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state=0)\n    \n    # Start time\n    start_time = time.time()\n    catbc = CatBoostClassifier(random_state=0, eval_metric='AUC')\n    catbc.fit(x_train, y_train, verbose=False)\n    \n    # Calculate the training time\n    catbc_time = time.time() - start_time\n    predictions = catbc.predict_proba(x_test)[:,1]\n    auc_catbc = roc_auc_score(y_test, predictions)\n    tuning_res.append((auc_catbc, catbc_time))","2c8fc29b":"# Compare performance on different dataset\ndataset_comparison = pd.DataFrame(data = tuning_res, \n                                index = [\"data_preprocessed_df\", \"data_preprocessed_dropNaN_df\", \"data_preprocessed_median_df\", \"data_preprocessed_mean_df\"],\n                                columns=['AUC', 'Time'])\\\n                     .sort_values(by = \"AUC\", ascending=False)\ndataset_comparison","7613b0eb":"# Release big variables that are not used in the following.\ndel data_preprocessed_df\ndel data_preprocessed_dropNaN_df\ndel data_preprocessed_median_df\ndel data_best_df\ndel data_df\ndel data_modelling_df\ndel x_train\ndel x_test\n\ngc.collect()","e1be8725":"# Use the dataset with the best performance\ndataset_df = data_preprocessed_mean_df.copy()\nX = dataset_df.drop(\"claim\", axis=1)\nY = dataset_df.claim","239fe912":"del data_preprocessed_mean_df\ndel dataset_df\ngc.collect()","611ad223":"# We can not really do the parameter tuning well on Kaggle because of the limitation of resource\n\n# catbc_best = CatBoostClassifier(random_state=0, eval_metric='AUC')\n# catbc_grid = {'max_depth': [4, 6, 8, 10],\n#               'n_estimators':[100, 300, 500, 1000, 2000],\n#               'min_data_in_leaf': [1, 3, 5, 10]\n#              }\n\n# catbc_search = GridSearchCV(catbc_best, \n#                             catbc_grid,\n#                             scoring=\"roc_auc\",\n#                             n_jobs=-1,\n#                             cv = 5)\n# catbc_search.fit(X,Y, verbose=False)\n\n# # Returns the estimator with the best performance\n# print(catbc_search.best_estimator_)\n\n# # Returns the best score\n# print(catbc_search.best_score_)\n\n# # Returns the best parameters\n# print(catbc_search.best_params_)","f124885b":"# Train the competitive models on all the known dataset\ncatbc_best = CatBoostClassifier(random_state=0, eval_metric='AUC')\ncatbc_best.fit(X, Y, verbose=False)\n\nlgbc_best = LGBMClassifier(random_state=0)\nlgbc_best.fit(X, Y, eval_metric='auc', verbose=-1)","ee31d3bb":"del X,Y\ngc.collect()","6efa121b":"# Load the dataset\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\ntest_df.info() # show entries, dtypes, memory useage.","2ccc9369":"# Have a look\ntest_df.head()","33a8cf96":"# Drop Irrelevant columns\ntest_preprocessed_df = test_df.drop(irrelevant_columns, axis=1)","f03d8a90":"# Add a new feature represent the missing value number of each row\ntest_preprocessed_df['missing_num'] = test_preprocessed_df.isnull().sum(axis=1)","45b53b5c":"# Generate a new feature by counting the negative value of each records.\ntest_preprocessed_df['neg_num'] = (test_preprocessed_df < 0).sum(axis=1)","fb8888a7":"# Generate a new feature from the missing_num.(Binary value to represent whether there is missing value in this record)\ntest_preprocessed_df['missing_sign'] = test_preprocessed_df['missing_num'] != 0\ntest_preprocessed_df['missing_sign'] = test_preprocessed_df['missing_sign'] + 0","bb1cec49":"# Replace the empty data with NaN\ntest_preprocessed_df.replace(\"\", float(\"NaN\"), inplace=True)\ntest_preprocessed_df.replace(\" \", float(\"NaN\"), inplace=True)\n\n# Count missing value(NaN, na, null, None) of each columns, Then transform the result to a pandas dataframe. \ncount_missing_value = test_preprocessed_df.isna().sum() \/ test_preprocessed_df.shape[0] * 100\ncount_missing_value_df = pd.DataFrame(count_missing_value.sort_values(ascending=False), columns=['Missing%'])\nmissing_value_df = count_missing_value_df[count_missing_value_df['Missing%'] > 0]\n\n# Get the feature names with missing values.\nmissing_features = list(missing_value_df.index)","d61cca5b":"# Group imputation\nfor feature in missing_features: \n    test_preprocessed_df[feature] = test_preprocessed_df.groupby('missing_sign', sort=False)[feature].apply(lambda x: x.fillna(x.mean()))","d3a780ff":"# Use trained model(best) to make predictions\nresults1 = catbc_best.predict_proba(test_preprocessed_df)[:,1]\nresults_df1 = pd.DataFrame(results1, columns=['claim'])\n#predictions_df1 = pd.concat([test_df['id'], results_df1], axis=1)","494c4e29":"# Use trained model(best) to make predictions\nresults2 = lgbc_best.predict_proba(test_preprocessed_df)[:,1]\nresults_df2 = pd.DataFrame(results2, columns=['claim'])\n#predictions_df2 = pd.concat([test_df['id'], results_df2], axis=1)","4fcd8980":"# Aggregate the results from two best models\naggregate_df = pd.concat([test_df['id'], results_df1, results_df2], axis=1)\naggregate_df['mean'] = aggregate_df['claim'].mean(axis=1)\n\nfinal_df = aggregate_df[['id', 'mean']].copy()\nfinal_df.rename(columns={'mean': 'claim'}, inplace=True)","10355849":"# Save predictions to .csv for project submission\nfinal_df.to_csv('submission.csv', index=False)","f159e7d8":"<a id=\"2.4.1\"><\/a>\n### 2.4.1. Drop all NaN\/NA\/null ","fe4468bd":"<a id=\"3.2\"><\/a>\n## 3.2. What is the distribution of features on claim?","54041232":"Two best model Generated(on all training dataset):\n1. catbc_best\n2. lgbc_best","c948c431":"Important feature discovery:\n1. Visualization: **f3, f5, f8, f9, f21, f31, f34, f38, f45, f57, missing_num**\n2. Mutual information: **missing_sign, missing_num, neg_num, f70, f11, f78, f52, f75, f118, f40**","8466accc":"Then, we get 4 datasets:\n* **data_preprocessed_df**: The original data without imputation.\n* **data_preprocessed_dropNaN_df**: Simply drop all the ***NaN*** value.\n* **data_preprocessed_median_df**: Grouped Median imputation on ***claim***.\n* **data_preprocessed_mean_df**: Grouped Mean imputation on ***claim***.","b9a20acf":"<a id=\"2\"><\/a>\n# 2. Data Preprocessing","f2fce20f":"It proves that feature \"**missing_num**\" is important to label(\"claim\"), so as \"**missing_sign**\".","cbec69d2":"<a id=\"4.1\"><\/a>\n## 4.1. Correlation Analysis","0fd814f8":"<a id=\"5.5\"><\/a>\n## 5.5. Parameter\/Feature Tuning  ","ba0af232":"<a id=\"2.4.2\"><\/a>\n### 2.4.2. Median Imputation ","d1220737":"# Table of Content\n1. [Data Overview](#1)\n    * [1. Load Data](#1.1)\n    * [2. Data Type](#1.2)\n    * [3. Statistical View](#1.3)\n2. [Data Preprocessing](#2)\n    * [1. Drop Irrelevant Columns](#2.1)\n    * [2. Missing Value Detection](#2.2)\n    * [3. New Feature Generation](#2.3)\n    * [4. Data Imputation](#2.4)\n        * [1. Drop all NaN\/NA\/null](#2.4.1)\n        * [2. Median Imputation](#2.4.2)\n        * [3. Mean Imputation](#2.4.3)\n3. [Data Analysis](#3)\n    * [1. What is the distribution of claim? ](#3.1)\n    * [2. What is the distribution of features on claim? ](#3.2)\n4. [Feature Engineering](#4)\n    * [1. Correlation Analysis ](#4.1)\n    * [2. Mutual Information ](#4.2)\n5. [Modelling](#5)\n    * [1. Train Test Split ](#5.1)\n    * [2. Train Models ](#5.2)\n        * [0. Manually Prediction ](#5.2.0)\n        * [1. XGboost ](#5.2.1)\n        * [2. CatBoost ](#5.2.2)\n        * [3. LightGBM ](#5.2.3)\n    * [3. Model Comparison ](#5.3)\n    * [4. Best Model Explaination ](#5.4)\n    * [5. Parameter\/Feature Tuning ](#5.5)\n6. [Prediction](#6)\n    * [1. Load Data](#6.1)\n    * [2. Drop Irrelevant Columns](#6.2)\n    * [3. New Feature Generation](#6.3)\n    * [4. Data Imputation](#6.4)\n    * [5. Make Prediction](#6.5)\n    * [6. Save the Prediction to CSV file](#6.6)","88ac1807":"<a id=\"2.4\"><\/a>\n## 2.4. Data Imputation\n> Choose the suitable imputation tech which can highly represent the central tendency of the data.","94992aa4":"<a id=\"5\"><\/a>\n# 5. Modelling\n\n* **data_preprocessed_df**: The original data without imputation.\n* **data_preprocessed_dropNaN_df**: Simply drop all the ***NaN*** value.\n* **data_preprocessed_median_df**: Grouped Median imputation on ***claim***.\n* **data_preprocessed_mean_df**: Grouped Mean imputation on ***claim***.","62a22002":"It seems that we can't drop missing value directly. Although the missing value in each feature is quite few, it randomly appeared in each row. Therefore, there are almost 40% rows have missing value.","0eb68c1d":"<a id=\"5.3\"><\/a>\n## 5.3. Model Comparison","7b3672f1":"We can not see any significant correlation between each feature.  \nNote: The correlation between \"missing_num\" and \"missing_sign\" is quite high. Because \"missing_sign\" comes from  \"missing_num\", which is used as the group imputation key.","3faa5106":"<a id=\"6.6\"><\/a>\n## 6.6. Save the Prediction to CSV file","0c7087b8":"<a id=\"4.2\"><\/a>\n## 4.2. Mutual Information","1f222174":"<a id=\"6.4\"><\/a>\n## 6.4. Data Imputation","b8617e7c":"<a id=\"5.2.3\"><\/a>\n### 5.2.3 LGBM","2e9af48b":"<a id=\"6.3\"><\/a>\n## 6.3. New Feature Generation","c34b2737":"<a id=\"3\"><\/a>\n# 3. Data Analysis","6c965729":"<a id=\"4\"><\/a>\n# 4. Feature Engineering\nAs all the features of the dataset are not categorical, there is no need to do with encoding. However, there are more than 100 features in this dataset, it is better to do **dimension reduction**.","cafd7280":"<a id=\"2.3\"><\/a>\n## 2.3. New Feature Generation","40188046":"# Reference(Thanks for Inspiration)\n* [TPS September 2021 EDA](https:\/\/www.kaggle.com\/dwin183287\/tps-september-2021-eda)\n* [Takeaways from TPS Sep 2021](https:\/\/www.kaggle.com\/c\/tabular-playground-series-sep-2021\/discussion\/274966)","b559f10a":"<a id=\"5.2\"><\/a>\n## 5.2. Train Models\n> Let's use three state-of-art ensembled models to make prediction\n\n* [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/)\n* [CatBoost](https:\/\/catboost.ai\/)\n* [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.Booster.html)","27b1839d":"<a id=\"1\"><\/a>\n# 1. Data Overview","6f78431a":"# Key Findings\n* Generate a feature of **missing value number of each record** can help to significantly improve the performance.(Because of the scenario of the data: whether a customer made a claim upon an insurance policy, missing information seems doubtful)\n* Data with **appropriate group imputation** are better than do noting with the missing value. The group key in this dataset is whether there is missing value of a record (**missing_sign**).\n* It can be seen that if I use the label(\"claim\") as the group imputation key, the performance will reach to about 0.9 AUC(I didn't show this in the notebook), although it is not allowed because of the data leakage. However, it indicates that find or generate highly related features to label can help to improve the performance a lot. ---**The importance of feature**\n* Calculate **mutual information** or simplely visualize each feature can find several insights before modelling. Compare these results with the results from feature importance.\n* Use the **aggregate results** from the chosen model can help to slightly improve the final results.(If the performance of the chosen models are quite competitive). Eg. CatBoost and LightGBM in this notebook.","17af8a7b":"<a id=\"5.2.1\"><\/a>\n### 5.2.1 CatBoost","8b4371a5":"<a id=\"5.1\"><\/a>\n## 5.1. Train Test Split","958e491d":"<a id=\"5.2.1\"><\/a>\n### 5.2.1 XGboost","e5f0b17f":"There are two classes in label(claim), quite balanced.","a05ae493":"<a id=\"2.1\"><\/a>\n## 2.1. Drop Irrelevant Columns","dfebb95f":"<a id=\"2.2\"><\/a>\n## 2.2. Missing Value Detection","0339692c":"It is not very intuitive to see any insights from the pure statistic results. However, the range of several feature are quite huge that need to be noticed.  \nf9, f12, f26, f27, f35, f62, f73, f74, f82, f86, f98, f108, f116","01a93e4b":"<a id=\"1.2\"><\/a>\n## 1.2. Data Type\n\n> [NOIR](https:\/\/www.questionpro.com\/blog\/nominal-ordinal-interval-ratio\/): Nominal, Ordinal, Interval, Ratio.  \n\nAs the features in this dataset have been anonymized, we just assume that the data type of each feature is what it looks like. --**Ratio**","a1254c78":"<a id=\"1.1\"><\/a>\n## 1.1. Load Data","429d639b":"It seems there aren't many missing value in each feature. However, almost all the features have missing value **~=1.6%**.","ea93db26":"# Issues\n* Broader range of parameter tuning sets are required. (The limitation of Kaggle's resource)\n* Could try to generate more important feature to boost the performance.\n* Efficient way to find the optimal parameter","c1a527e8":"<a id=\"6\"><\/a>\n# 6. Prediction","a400a57d":"<a id=\"6.1\"><\/a>\n## 6.1. Load Data","4bd80a89":"The label(claim) is quite balanced.","24be1c8d":"It seems that data with mean value imputation outperform from all the chosen dataset.","5f8a0085":"<a id=\"1.3\"><\/a>\n## 1.3. Statistical View ","fc3160f2":"<a id=\"6.5\"><\/a>\n## 6.5. Make Prediction","dc4c1b0b":"* int64: id and claim\n* float64: all other features","bd577d33":"It seems that **CatBoost** model outperform all the chosen models. **LGBM** performs a little bit worse, but the training time is quite short.","b65015e9":"The top 10 important feature are: **missing_num, missing_sign, f40, f70, f47, f57, f35, f1, f106, f5**.","e18cb2db":"<a id=\"5.4\"><\/a>\n## 5.4. Best Model Explaination","9cc90ca4":"# September 2021 Tabular Playground\nThis notebook aims to show an entire workflow of Data Science by using the dataset from September 2021 Tabular Playground. A competitive performance is ensured.\n* [**Author**](https:\/\/www.linkedin.com\/in\/chi-wang-22a337207\/)\n* [**Dataset**](https:\/\/www.kaggle.com\/c\/tabular-playground-series-sep-2021\/data)","aebbc542":"<a id=\"6.2\"><\/a>\n## 6.2. Drop Irrelevant Columns","a1ea59b1":"<a id=\"2.4.3\"><\/a>\n### 2.4.3. Mean Imputation ","8273912b":"<a id=\"5.2.0\"><\/a>\n### 5.2.0. Manually Prediction ","04d3422a":"# Thanks for reading, have a good day ~","c0d5c7df":"<a id=\"3.1\"><\/a>\n## 3.1. What is the distribution of claim?","1349cf15":"There are not any feature have an significant difference on claim. If we have to say the slight different on claim, the features are: **f3, f5, f8, f9, f21, f31, f34, f38, f45, f57**."}}