{"cell_type":{"a0a51d21":"code","555cd6c9":"code","39eb0ef4":"code","9148e4af":"code","fdc1deda":"code","994d5c0d":"code","9df8689e":"code","83a520f9":"code","d979894d":"code","aa9ccddf":"code","74de84a5":"code","902deaf9":"markdown","3cab6994":"markdown","b6fe37d1":"markdown"},"source":{"a0a51d21":"!pip install -q -U trax","555cd6c9":"\nimport trax\nimport numpy as np\nimport trax.layers as tl\nfrom trax.fastmath import numpy as jnp\nimport os\nimport os.path as osp\nfrom PIL import Image\nfrom itertools import cycle\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt","39eb0ef4":"batch_size = 16","9148e4af":"def color_kmean(root):\n  \"\"\" creates a k-means objects that recognizes all 13 colors of dataset. \"\"\"\n  \n  # take 10 first images\n  files = os.listdir(root)[:10] \n  colors = list()\n  for f in files:\n    img = load_image(osp.join(root, f))\n    # total width\n    w = img.shape[2]\n    # get the right half of image, which is the label image\n    img = img[:, w:, :]\n    # collect all the colors present in label image\n    colors.append(img.reshape(-1, 3))\n\n  colors = np.array(colors)\n  colors = colors.reshape(-1, 3)\n\n  # finally, fit all the colors into the KMeans\n  kmeans = KMeans(13)\n  kmeans.fit(colors)\n\n  return kmeans\n\ndef load_image(path):\n  \"\"\" loading an image. \"\"\"\n  \n  assert osp.exists(path), path + \" not found\"\n  image = Image.open(path)\n  image = np.asarray(image)\n  return image\n\ndef color2class(segs, km):\n  \"\"\" \n  given an label image, convert it to class matrix, \n  which is a 2D matrix of class labels (scalars).\n  \"\"\"\n  \n  h, w, c = segs.shape\n  segs = segs.reshape((-1, 3))\n  segs = km.predict(segs)\n  segs = segs.reshape((h, w, 1))\n  return segs\n\ndef load_dataset(root, km):\n  \"\"\" load dataset. \"\"\"\n  index = 0\n  imgs_path = [osp.join(root, f) for f in os.listdir(root)]\n\n  # load images one by one, finally, and image and \n  # its label matrix is returned\n  while True:\n    img = load_image(imgs_path[index])\n    w = img.shape[1] \/\/ 2\n    img, seg = img[:, :w, :], img[:, w:, :]\n\n    seg = color2class(seg, km)\n\n    seg = seg.reshape(-1)\n    assert img.shape == (256, 256, 3), img.shape\n    assert seg.shape == (256 * 256,), seg.shape\n    yield img, seg\n\n    index = (index + 1) % len(imgs_path)","fdc1deda":"root = '..\/input\/cityscapes-image-pairs\/cityscapes_data'\n\ntrainset_path = osp.join(root, 'train')\nvalset_path = osp.join(root, 'val')\n\nkm = color_kmean(trainset_path)","994d5c0d":"train_dataset = load_dataset(trainset_path, km)\nval_dataset = load_dataset(valset_path, km)\n\ntrain_transforms = trax.data.Serial(\n    trax.data.Shuffle(),\n    trax.data.Batch(batch_size),\n    lambda g: map(lambda p: (p[0].astype(np.float32), p[1]), g),\n)\nval_transforms = trax.data.Serial(\n    trax.data.Batch(batch_size),\n    lambda g: map(lambda p: (p[0].astype(np.float32), p[1]), g),\n)\n\ntrain_dataset = train_transforms(train_dataset)\nval_dataset = val_transforms(val_dataset)","9df8689e":"def CrossEntropy3d(criterion_2d):\n  \"\"\" returns 3D cross entropy loss function \"\"\"\n  def _loss_fn(output, target):\n    output = output.reshape(-1, 13)\n    target = target.reshape(-1,)\n    loss = criterion_2d((output, target))\n    return loss\n  return _loss_fn\n\n# check dataset\nx, y = next(train_dataset) \nprint(x.shape, y.shape)\nprint(x.dtype, y.dtype)","83a520f9":"# set learning rate\nlr = 1e-2\n\n# create new trax Fn for new loss fn, and provide it a name\ncriterion = trax.layers.base.Fn(\"CrossEntropy3d\", \n                                CrossEntropy3d(tl.CategoryCrossEntropy())\n                                )\n\n# create TrainTask\ntrain_task = trax.supervised.training.TrainTask(\n    labeled_data=train_dataset,\n    loss_layer=criterion,\n    optimizer=trax.optimizers.Momentum(lr),\n    n_steps_per_checkpoint=50\n)\n\n# create EvalTask\neval_task = trax.supervised.training.EvalTask(\n    labeled_data=val_dataset,\n    metrics=[criterion]\n)","d979894d":"model = tl.Serial(\n    tl.Conv(13, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n    tl.Relu(),\n    tl.LayerNorm(),\n    tl.Conv(32, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n    tl.Relu(),\n    tl.LayerNorm(),\n    tl.Conv(32, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n    tl.Relu(),\n    tl.LayerNorm(),\n    tl.Conv(64, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n    tl.Relu(),\n    tl.LayerNorm(),\n    tl.Conv(128, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n    tl.Relu(),\n    tl.LayerNorm(),\n    tl.Conv(64, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n    tl.Relu(),\n    tl.LayerNorm(),\n    tl.Conv(32, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n    tl.Relu(),\n    tl.LayerNorm(),\n    tl.Conv(32, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer()),\n    tl.Relu(),\n    tl.LayerNorm(),\n    tl.Conv(13, (3, 3), (1, 1), padding='SAME', kernel_initializer=tl.KaimingNormalInitializer())\n)","aa9ccddf":"training_loop = trax.supervised.training.Loop(\n    model, \n    train_task, \n    eval_tasks=[eval_task],\n    output_dir=None\n)\n\ntraining_loop.run(500)","74de84a5":"x, y = next(val_dataset)\n\nfig, axs = plt.subplots(nrows=1, ncols=3)\n\nx = x[0]\ny = y[0]\n\ny = np.reshape(y, (256, 256))\naxs[0].imshow(x.astype(np.int32))\naxs[1].imshow(y)\nfig.show()\n\nx = np.expand_dims(x, 0)\ny_hat = model(x)\ny_hat = y_hat[0]\n\ny_hat = np.argmax(y_hat, 2)\ny_hat = np.reshape(y_hat, (-1,))\ny_hat = km.cluster_centers_[y_hat]\ny_hat = np.reshape(y_hat, (256, 256, 3))\ny_hat = np.round_(y_hat).astype(np.int32)\n\naxs[2].imshow(y_hat)","902deaf9":"## Sementic Segmentation with Trax","3cab6994":"## Configuration","b6fe37d1":"## Utilities"}}