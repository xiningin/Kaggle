{"cell_type":{"12b2136b":"code","d3e63848":"code","d91ad73b":"code","a8d21697":"code","11c583cf":"code","bce89bcf":"code","2713f5e4":"code","a554020e":"code","5fee5960":"code","c7a98c0d":"code","74441907":"code","c36f8291":"code","5effc4ea":"code","444cf0ed":"code","ac58020f":"code","fb2e12d7":"code","82f6623d":"code","5c9f4a37":"code","97fbba4e":"code","81b1f730":"code","b55bdffd":"markdown","c156692a":"markdown","8f9849d8":"markdown","4acf6165":"markdown","c83251e8":"markdown","f97d16f0":"markdown","46f62486":"markdown","3b558519":"markdown","d23de34d":"markdown","b8eaf906":"markdown","1c2c0735":"markdown","750ebb63":"markdown","d6d2ffbe":"markdown","8332044b":"markdown","bd6f3660":"markdown","a0e8d2cf":"markdown","fadcaf6f":"markdown","de82d994":"markdown","56ba8b51":"markdown","6bd6c367":"markdown","9967acc4":"markdown","2d3d8e84":"markdown","391b0776":"markdown"},"source":{"12b2136b":"# Import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nimport altair as alt\nfrom functools import partial\nfrom pathlib import Path\nimport category_encoders as ce\nfrom skopt import space, gp_minimize\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.impute import SimpleImputer, MissingIndicator\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import metrics, ensemble, model_selection, tree\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, cross_val_predict\n\n# Settings\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\npd.options.display.max_columns=100\npd.options.display.max_rows=100\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Read Data\npath = Path('\/kaggle\/input\/house-prices-advanced-regression-techniques\/')\ntrain = pd.read_csv(path\/'train.csv')\ntest = pd.read_csv(path\/'test.csv')\nsample = pd.read_csv(path\/'sample_submission.csv')\ntrain.SalePrice = np.log(train.SalePrice)","d3e63848":"# Inspect the data.\ndef check_features(df):\n    return pd.DataFrame({'unique_values': df.nunique(),'type': df.dtypes,'pct_missing': df.isna().sum()\/len(df) * 100}).sort_values(by = 'pct_missing', ascending=False) \n\n# Function to get rmse.\ndef rmse(targets, preds): \n    return metrics.mean_squared_error(targets, preds, squared=False)\n\n# Impute missing values.\ndef missing_cats(df, cats): \n    return df[cats].fillna('#na#')\n\n# Functions to fit train and evaluate a model.\ndef mfe(model, x_train, y_train, x_val, y_val):\n    model.fit(x_train, y_train)\n    preds_train = model.predict(x_train); preds_val = model.predict(x_val)\n    rmse_train = rmse(y_train, preds_train); rmse_val = rmse(y_val, preds_val) # Calculate train & validation rmse.\n    r2_train = metrics.r2_score(y_train, preds_train); r2_val = metrics.r2_score(y_val, preds_val) # Calculate train & validation R2.\n    result = pd.DataFrame({'rmse_train': rmse_train, 'rmse_val': rmse_val, 'r2_train': r2_train, 'r2_val': r2_val}, index=['metrics'])\n    display(result)\n    \n# Get permutation importance as a dataframe.   \ndef get_pi(model, x, y):\n    imp = permutation_importance(model, x, y, scoring='neg_root_mean_squared_error', n_repeats=2, n_jobs=4, random_state=1)\n    df_pi = pd.DataFrame({'features': x.columns, 'imp': imp.importances_mean}, index=None).astype({'imp': np.float64}).sort_values(by='imp', ascending=False).reset_index(drop=True)\n    return df_pi    \n\n# Plot permutation importance.    \ndef plot_pi(model, x, y, n=None, ax=None):\n    if n is None: n = len(x.columns)\n    imp = permutation_importance(model, x, y, scoring='neg_root_mean_squared_error', n_repeats=2, n_jobs=4, random_state=42)\n    df_imp = pd.DataFrame({'features': x.columns, 'imp': imp.importances_mean}, index=None).astype({'imp': np.float64}).sort_values(by='imp', ascending=False).iloc[:n]\n    df_imp.sort_values(by='imp', ascending=False).plot.barh(x='features', y='imp', figsize=(10,6), ax=ax);\n","d91ad73b":"# Define feature types.\ntarget = ['SalePrice']\ncat_feats = train.drop(columns=['Id', 'SalePrice']).select_dtypes(include='object').columns.tolist()\ncont_feats = train.drop(columns=['Id', 'SalePrice']).select_dtypes(include=np.number).columns.tolist()\nall_feats = cat_feats + cont_feats\nlen(train.columns), len(cat_feats), len(cont_feats)\n\n# Pipeline for categorical feature transformation.\ncat_tfms = Pipeline(steps=[\n    ('cat_ordenc', ce.OrdinalEncoder(return_df=True, handle_unknown='value', handle_missing='value'))\n])\ncont_tfms = Pipeline(steps=[\n    ('cont_imputer', SimpleImputer(missing_values=np.nan, strategy='median'))\n])\nctf = ColumnTransformer(transformers=[\n    ('cat_tfms', cat_tfms, cat_feats),\n    ('cont_tfms', cont_tfms, cont_feats)\n], remainder='passthrough')\n\n# Transform the data.\nX = train[all_feats]\ny = train.SalePrice\nx_train, x_val, y_train, y_val = train_test_split(X, y, test_size=.2, shuffle=True, random_state=42)\nx_train_tf = pd.DataFrame(ctf.fit_transform(x_train), columns=all_feats)\nx_val_tf = pd.DataFrame(ctf.transform(x_val), columns=all_feats)\nx_test = test[all_feats]\nx_test_tf = pd.DataFrame(ctf.transform(x_test), columns=all_feats)\n\n# Map the categorical features to encodings.\nordenc_map = dict()\nfor feat in cat_feats: ordenc_map[feat] = dict(zip(x_train[feat], x_train_tf[feat]))","a8d21697":"x_train_tf.head()","11c583cf":"rf = ensemble.RandomForestRegressor(n_estimators=40, n_jobs=-1)\nmfe(rf, x_train_tf, y_train, x_val_tf, y_val)","bce89bcf":"# Define the train, validation and test Dmatrix objects.\ndtrain = xgb.DMatrix(x_train_tf, label = y_train)\ndval = xgb.DMatrix(x_val_tf, label = y_val)\ndtest = xgb.DMatrix(x_test_tf)","2713f5e4":"# Define the model params.\nparams = {\n    'eta': 0.05,\n    'gamma': 0,\n    'max_depth':31,\n    'min_child_weight': 1,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'colsample_bylevel': 1,\n    'colsample_bynode': 1, \n    'lambda': 1,\n    'alpha': 0,\n    'tree_method': 'exact',\n    'objective': 'reg:squarederror',\n    'eval_metric':'rmse',\n    #'tree_method':'gpu_hist',\n    'seed': 42\n} ","a554020e":"model = xgb.train(\n    params, \n    dtrain, \n    num_boost_round = 10000, \n    evals = [(dtrain, 'train'), (dval, 'val')],\n    early_stopping_rounds = 50,\n    verbose_eval = 100\n)","5fee5960":"xgb.plot_importance(model);","c7a98c0d":"model.attributes()","74441907":"# Evaluate the model on mat\/dataset.\nmodel.eval(dtrain, name='eval', iteration=0)","c36f8291":"model.get_fscore()","5effc4ea":"model.get_score(importance_type='weight')","444cf0ed":"model.get_split_value_histogram('cat5', bins=None, as_pandas=True)","ac58020f":"model.num_boosted_rounds()","fb2e12d7":"model.num_features()","82f6623d":"xgb.cv(\n    params, #  Booster params.\n    dtrain, # Data to be trained.\n    num_boost_round=50, # Number of boosting iterations.\n    nfold=3, # Number of folds in CV\n    stratified=False, # Perform stratified sampling.\n    metrics='rmse', # Evaluation metrics to be watched in CV\n    early_stopping_rounds=10, \n    verbose_eval=10,  \n    seed=0)","5c9f4a37":"clf = xgb.XGBRegressor(\n    n_estimators = 100,\n    max_depth = 6,\n    learning_rate = 0.15,\n    #verbosity = 0,\n    objective = 'reg:squarederror',\n    n_jobs = -1,\n    gamma = 0,\n    min_child_weight = 7,\n    max_delta_step = 0,\n    subsample = 1,\n    colsample_bytree = 1,\n    colsample_bylevel = 1,\n    colsample_bynode = 1,\n    reg_alpha = 1,\n    reg_lambda = 1,\n    scale_pos_weight = 1,\n    random_state = 42)","97fbba4e":"clf.fit(\n    x_train_tf, y_train,\n    eval_set = [(x_train_tf, y_train), (x_val_tf, y_val)],\n    eval_metric = 'rmse',\n    early_stopping_rounds = 10,\n    verbose = 10);","81b1f730":"preds = np.exp(model.predict(dtest))\ndsub = pd.DataFrame({'Id': test.Id, 'SalePrice': preds})\ndsub.to_csv('dsub.csv', index=False)","b55bdffd":"## Utility Code","c156692a":"Here is the Scikit-Learn implementation of XGBoost. ","8f9849d8":"**Predictions on test data**","4acf6165":"## Data Preprocessing","c83251e8":"WIP","f97d16f0":"In this notebook, we will learn everything about the popular Gradient Boosting Technique known as XGBoost. We will see how far up the leaderboard can just an XGBoost model take us. We will focus on understanding the hyperparameters  how to tune them. ","46f62486":"Feature Importance types in XGBoost\n\n* `weight`: the number of times a feature is used to split the data across all trees.\n* `gain`: the average gain across all splits the feature is used in.\n* `cover`: the average coverage across all splits the feature is used in.\n* `total_gain`: the total gain across all splits the feature is used in.\n* `total_cover`: the total coverage across all splits the feature is used in.","3b558519":"### How to control overfitting?\n\nWhen you observe high training accuracy, but low test accuracy, it is likely that you encountered overfitting problem.There are in general two ways that you can control overfitting in XGBoost \n* The first way is to directly control model complexity through `max_depth`, `min_child_weight` and `gamma`.\n* The second way is to add randomness to make training robust to noise by using `subsample` and `colsample_bytree`.\n* You can also reduce stepsize `eta`. Remember to increase num_round when you do so.","d23de34d":"### General Tips for training\n* In general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle.\n* We can try a very small learning rate and a large number of boosting iterations to see how long it takes for the model to overfit. When the validation loss stops decreasing, we can exit the training. To get even more accuracy, we can multiply the `num_rounds` by k & divide `eta` by k. \n* We can change the `seed` parameter to see how different values affect out results.","b8eaf906":"## Training XGBoost Model ([Link](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.train))","1c2c0735":"# [XGBoost: Intro to python API](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_intro.html#)","750ebb63":"# Learn XGBoost by predicting House Prices([Link](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview))","d6d2ffbe":"The following resources are quite helpful for understanding the hyperparameters.\n* [XGB Hyperparameters](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html)\n* [XGB specific notes](https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/param_tuning.html)","8332044b":"### Some Notes\n\nSome Resources for Learning Model Tuning.\n\n* [Analytics Vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/)\n* [Grid and Random Search](https:\/\/www.kaggle.com\/willkoehrsen\/intro-to-model-tuning-grid-and-random-search)\n\nAdvice from top Kagglers\n\n* Select the most influential parameters. There are tons of parameters and we can't tune them all.\n* For the selected set of parameters, understand how they influence the training.\n* Tune the selected parameters. In pracice it's faster to tune manually. \n\nDifferent values of parameters result in 3 different fitting behaviors. \n* Underfitting\n* Good fit and generalization\n* Overfitting\n\nWe will split the parameters into two groups. \n* Brakers: Parameters  whose value upon increasing regularize the model. If we increase their value the model will change its behaviour from  overfitting to underfitting. \n* Speeders: Parameters whose value upon increasing can lead to overfitting. Increase their value if model underfits & decrease if it underfits.\n\nAn approach to tuning hyperparameters\n* Step 1: Fix learning rate and number of estimators for tuning tree-based parameters\n* Step 2: Tune max_depth and min_child_weight\n* Step 3: Tune gamma\n* Step 4: Tune subsample and colsample_bytree\n* Step 5: Tuning Regularization Parameters","bd6f3660":"| Type | XGBoost | LightGBM |\n| --- | --- | --- |\n| Speeder | max_depth | max_depth\/num_leaves |\n| Speeder | subsample | bagging_fraction |\n| Speeder | colsample_by_tree\/level | feature_fraction |\n| Speeder | eta | learning_rate |\n| Braker | lambda | lambda_l1 |\n| Braker | alpha | lambda_l2 |\n| Braker | min_child_weight | min_data_in_leaf |\n| Braker | num_round | num_iterations |\n","a0e8d2cf":"## XGBoost CV ([Link](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.cv))","fadcaf6f":"The argument `early_stopping_rounds` offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. early_stopping_rounds = 10 is a reasonable value. Thus we stop after 10 straight rounds of deteriorating validation scores.","de82d994":"### General Parameters\n**booster**\n* Default Values is 'gbtree'; Can be gbtree, gblinear or dart; \n* gbtree and dart use tree based models while gblinear uses linear functions.\n\n**Verbosity**\n* Verbosity of printing messages. \n* Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug)\n\n### Parameters for Tree Booster\n**eta**\n* Default value is 0.3. Range is [0,1], aka learning_rate.\n* Used to scale the output from each tree. Shrinkage parameter. Speed of learning. \n* Higher values may lead to overfitting.\n\n**gamma**\n* Default value is 0. Range [0,\u221e]. aka `min_split_loss`.\n* Is meant to encourage pruning in the tree, thought xgboost can prune even if gamma is zero.\n* Minimum loss reduction required to make a further partition on a leaf node. \n* Increasing `gamma` regularizes the model and reduces the overfitting. \n\n**max_depth**\n* Default value: 6; range: [0,\u221e]\n* Maximum depth of a tree. How many interactions do you want to allow in your model?  \n* Larger values increase complexity and lead to overfitting.\n* If the validation performance keeps increasing as the max_depth increases, it means there are a lot of interactions that can be extracted from the data. So it's better to stop tuning in that case any try to generate some features.\n* 7 is a good value to start.\n\n**min_child_weight**\n* Similar to `min_samples_leaf` in Random Forest.\n* Increasing the value regularizes the model and reduces the overfitting.\n* One of the most important parameters to tune in XGB & LGB\n\n**max_delta_step**\n* Default value:  0; range: [0,\u221e]\n* Maximum delta step we allow each leaf output to be.\n* Usually not needed, but it might help in logistic regression when class is extremely imbalanced. Suggested range: [1-10].\n\n**subsample**\n* Default value: 1, range: (0,1]\n* Subsample ratio of the training instances.\n* Lower values regularize the model and prevent overfitting.\n\n**colsample_bytree**\n* Default value: 1, range: (0,1]\n* What fraction of features will be used for building each tree. Subsampling occurs once for every tree constructed.\n* Lower values regularize the model and prevent overfitting.\n\n**colsample_bylevel**: 1 \n* Default value: 1, range: (0,1]\n* Subsample ratio of columns for each level. \n* Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n* Lower values regularize the model and prevent overfitting.\n\n**colsample_bynode**: 1 \n* Default value: 1, range: (0,1]\n* Subsample ratio of columns for each node (split). \n* Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level.\n* Lower values regularize the model and prevent overfitting.\n\n**lambda**\n* Default value: 1, range: (0,1]; aka reg_lambda\n* Used to regularize the output values from leaves. As lambda increases the output value from a leaf shifts closer to zero.\n* L2 regularization term on weights. \n* Higher values reduce overfitting.\n\n**alpha**\n* Default value: 0, range: (0,1]; aka reg_alpga\n* L1 regularization term on weights.\n* Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n* Higher values reduce overfitting.\n\n**tree_method**\n* Specifies which tree construction algorithm to use in in XGBoost.\n* auto, exact, approx, hist, gpu_hist are different options.\n* For large datasets prefer approx, hist, gpu_hist. For smaller datasets used exact.\n\n**scale_pos_weight**\n* Control the balance of positive and negative weights, useful for unbalanced classes. \n* A typical value to consider: sum(negative instances) \/ sum(positive instances)\n\n**max_leaves**\n* Maximum number of nodes to be added. Only relevant when grow_policy=lossguide is set.\n\n### Learning Task Parameters\n**objective**: \n* reg:squarederror Optimization objective for regression.\n\n**base_score**\n* 0.5, The initial prediction score of all instances, global bias\n\n**eval_metric**:\n* Evaluation metrics for validation data, a default metric will be assigned according to objective\n\n**seed**: \n* 42, Used for reproducibility","56ba8b51":"# Using the [Scikit-Learn API](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn) for XGBoost","6bd6c367":"reg_alpha 0 to 1 decreases validation rmse from .84584 to .84576.","9967acc4":"## Random Forest Baseline","2d3d8e84":"## How is XGBoost different from vanilla Gradient Boosting?","391b0776":"## Hyperparameter Tuning"}}