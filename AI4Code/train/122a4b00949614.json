{"cell_type":{"9bfc6c30":"code","b6ae2e1c":"code","0faf9d03":"code","60f9ed0a":"code","bb51cecf":"code","75b2987b":"code","04fd6fc4":"code","fad998e1":"code","c4420ad0":"code","3a430163":"code","8e68d8ef":"code","3c3cb241":"code","0682a9e1":"code","424d2e39":"code","2f314b1a":"code","583c3ae8":"code","11f3805a":"code","ebe2471f":"code","5f61ed64":"code","9f7550ab":"markdown","4ac970b3":"markdown","a8c5515b":"markdown"},"source":{"9bfc6c30":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm_notebook\nfrom tqdm import tqdm\nfrom collections import Counter\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b6ae2e1c":"url1 = '..\/input\/ram-reduce\/reduce_train.csv'\nurl2 = '..\/input\/ram-reduce\/reduce_test.csv'\nreduce_train = pd.read_csv(url1,index_col=None)\nreduce_test = pd.read_csv(url2,index_col=None)\nprint(reduce_train.shape,reduce_test.shape)","0faf9d03":"reduce_test_id = reduce_test['installation_id']","60f9ed0a":"\nreduce_train = reduce_train.drop('installation_id', axis=1)\nreduce_test = reduce_test.drop('installation_id', axis=1)","bb51cecf":"\nX = reduce_train.drop('accuracy_group', axis=1)\ny = reduce_train.accuracy_group","75b2987b":"from sklearn.model_selection import StratifiedKFold, KFold, train_test_split","04fd6fc4":"# Split the data into 30% validation and 70% training\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42) #0.15 bank40","fad998e1":"from sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n# explicitly require this experimental feature\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\n# now you can import normally from ensemble\nfrom sklearn.ensemble import HistGradientBoostingClassifier","c4420ad0":"his = HistGradientBoostingClassifier()\nhis.fit(X_train, y_train)\ny_predictions=his.predict(reduce_test)\nhis.score(X_train, y_train)","3a430163":"X.shape","8e68d8ef":" reduce_test.shape","3c3cb241":"submission = pd.DataFrame({\"installation_id\":reduce_test_id,\n                           \"accuracy_group\":y_predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","0682a9e1":"target='accuracy_group'","424d2e39":"select = ['session_title', 'Cart Balancer (Assessment)_3121', 'Bird Measurer (Assessment)_3120', 'Bird Measurer (Assessment)_3021', 'e4f1efe6', 'acc_Chest Sorter (Assessment)', 'accumulated_accuracy', '7525289a', 'Scrub-A-Dub_3021', 'a52b92d5', 'Crystal Caves - Level 3', 'acc_Bird Measurer (Assessment)', '3afb49e6', 'Tree Top City - Level 3', 'Chow Time_2030', 'Clip', 'b74258a0', 'Mushroom Sorter (Assessment)_3121', 'acc_Mushroom Sorter (Assessment)', 'c7f7f0e1', '5290eab1', '3393b68b', 'Mushroom Sorter (Assessment)_4070', '0a08139c', '8f094001', 'a5be6304', 'Tree Top City - Level 3_2000', 'Mushroom Sorter (Assessment)_3010', 'Chest Sorter (Assessment)_2010', 'ecaab346', 'Cart Balancer (Assessment)_3110', 'Fireworks (Activity)_2000', 'c51d8688', '6c930e6e', '070a5291', 'Dino Drink_3020', '04df9b66', 'Happy Camel_2030', '222660ff', 'Mushroom Sorter (Assessment)_3120', 'Scrub-A-Dub_2000', 'Chest Sorter (Assessment)_2030', 'Cauldron Filler (Assessment)_3120', 'All Star Sorting_3121', '65a38bf7', 'Cart Balancer (Assessment)_3021', 'Tree Top City - Level 2', 'All Star Sorting_2000', 'Pan Balance_3120', 'Air Show_3121', 'Chest Sorter (Assessment)_4030', 'c7fe2a55', 'Bird Measurer (Assessment)_2000', '37937459', 'Mushroom Sorter (Assessment)_2010', 'ab4ec3a4', 'Mushroom Sorter (Assessment)_4100', 'Egg Dropper (Activity)_2020', 'Cart Balancer (Assessment)_2020', 'Scrub-A-Dub_2050', 'acc_Cauldron Filler (Assessment)', 'Egg Dropper (Activity)_2000', 'Crystal Caves - Level 2', 'Chest Sorter (Assessment)_4025', 'Ordering Spheres_2000', 'Cart Balancer (Assessment)_2000', 'f54238ee', '3a4be871', 'Bird Measurer (Assessment)_2020', 'acc_Cart Balancer (Assessment)', 'Chow Time_4035', 'Air Show_2000', 'Mushroom Sorter (Assessment)_2035', 'd3640339', '77c76bc5', 'Air Show_2030', 'installation_title_nunique', 'Bubble Bath_4090', '92687c59']\nlen(select)","2f314b1a":"from catboost import CatBoostClassifier, CatBoostRegressor\ncat =  CatBoostClassifier(verbose=0,n_estimators=1000,\n                                random_state=99,one_hot_max_size=100,\n                                loss_function='MultiClass', eval_metric='AUC',\n                                subsample=0.7,bootstrap_type='Bernoulli',\n                               early_stopping_rounds=25,boosting_type='Plain')\n","583c3ae8":"cat.fit(reduce_train[select],reduce_train[target])","11f3805a":"testm = cat.predict(reduce_test[select]).astype(int)\ntestm.shape","ebe2471f":"subm = pd.DataFrame(index=range(1000))\nsubm['installation_id'] = reduce_test['installation_id'].values[:1000]\nsubm[target] = testm\nprint(subm.shape)\nsubm.head()","5f61ed64":"subm.to_csv('submission.csv',index=False)","9f7550ab":"**This Part is not valid becuase the submission file is showing error******","4ac970b3":"Link for getting to this point from the Data Science Bowl competition is here:\nhttps:\/\/www.kaggle.com\/morenoh149\/autoviml-quickstart","a8c5515b":"# Data Science Bowl 2019: Demonstrating a new Python Library called Auto_ViML which automatically builds multiple models from a single line of code\nThe Reduce_traina dn Reduce_Test data are derviced from another Kernel below. \nWE are going to use a new library named \"autoviml\" in order to try and get an automated prediction"}}