{"cell_type":{"c3502b60":"code","81b8d1ff":"code","69977223":"code","fdc9c34a":"code","7c981131":"code","2ab57a7e":"code","f6af1bd5":"code","d9ea1232":"code","bed8ee9b":"code","df90c857":"code","c6d18dcd":"code","1317e352":"code","1da97279":"code","52651c1f":"code","bc2f99ef":"code","947ddefe":"code","e71ff813":"code","fce8164e":"code","45b8b715":"code","361bf6d7":"code","8356e227":"code","e53558a3":"code","41d94f9f":"code","4d2004a6":"code","17608ec5":"code","313e9629":"code","a69a2223":"code","a94c9621":"code","ab120752":"code","e37f7dfa":"code","4f367393":"code","485010c2":"code","008b3319":"code","a42a5396":"code","e205d572":"code","0657cc88":"code","385253e3":"code","56a46b91":"code","2564cabd":"markdown","8f01e708":"markdown","6748a2c4":"markdown","73a12213":"markdown","484a77f5":"markdown","b4bc8463":"markdown","058aa21b":"markdown","0977d1c6":"markdown","d9a37f86":"markdown","f6346f14":"markdown","3f2dfca2":"markdown","5789cf36":"markdown","cf904fe4":"markdown","a12e0565":"markdown","3fd59682":"markdown","a138ad4c":"markdown"},"source":{"c3502b60":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom os.path import join, isdir\nfrom os import listdir, makedirs, getcwd, remove\nfrom PIL import Image\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport random\nfrom collections import deque\n\nuse_cuda = torch.cuda.is_available()\nuse_cuda","81b8d1ff":"manualSeed = 999\ndef fixSeed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if use_cuda:\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\nfixSeed(manualSeed)","69977223":"import os\n\ndef find_classes(fullDir):\n    classes = [d for d in os.listdir(fullDir) if os.path.isdir(os.path.join(fullDir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    num_to_class = dict(zip(range(len(classes)), classes))\n    \n    train = []\n    for index, label in enumerate(classes):\n      path = fullDir + label + '\/'\n      for file in listdir(path):\n        train.append(['{}\/{}'.format(label, file), label, index])\n    \n    df = pd.DataFrame(train, columns=['file', 'category', 'category_id', ])\n    \n    return classes, class_to_idx, num_to_class, df","fdc9c34a":"classes, class_to_idx, num_to_class, df = find_classes(\"..\/input\/train\/\")\n\nX, y = df.drop('category_id', axis=1), df['category_id']\n\nprint(f'X shape: {X.shape}, y shape: {y.shape}')\nX.head()","7c981131":"class SeedlingDataset(Dataset):\n    def __init__(self, filenames, labels, root_dir, subset=False, transform=None):\n        self.filenames = filenames\n        self.labels = labels\n        self.root_dir = root_dir\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        fullname = join(self.root_dir, self.filenames.iloc[idx])\n        image = Image.open(fullname).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, self.labels.iloc[idx]\n","2ab57a7e":"from sklearn.model_selection import train_test_split","f6af1bd5":"X, y = df.drop(['category_id', 'category'], axis=1), df['category_id']","d9ea1232":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)","bed8ee9b":"print(f'train size: {X_train.shape}, validation size: {X_val.shape}, test size: {X_test.shape}')","df90c857":"def train(train_loader, model, optimizer, criterion):\n    model.train()\n    \n    losses, accuracies = [], []\n    for batch_idx, (data, target) in (enumerate(train_loader)):\n        correct = 0\n        \n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n        \n        optimizer.zero_grad()\n        output = model(data) # forward\n        loss = criterion(output, target)\n        loss.backward()\n        # Calculate Accuracy\n        pred = output.data.max(1)[1] # max probability\n        correct += pred.eq(target.data).cpu().sum() \n        accuracy = 100. * correct \/ len(data)\n        optimizer.step()\n        \n        losses.append(loss.data.item())\n        accuracies.append(accuracy)\n        \n    return np.mean(losses), np.mean(accuracies)\n\n\ndef test(test_loader, criterion, model):\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n        \n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += criterion(output, target).data.item()\n        pred = output.data.max(1)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data).cpu().sum().item()\n    \n    test_loss \/= len(test_loader) # loss function already averages over batch size\n    accuracy = 100. * correct \/ len(test_loader.dataset)\n    return test_loss, accuracy\n\ndef _should_early_stop(recent_validation_losses, validation_loss, early_stopping_rounds):\n    recent_validation_losses.append(validation_loss)\n    if early_stopping_rounds < len(recent_validation_losses):\n        recent_validation_losses.popleft()\n        return all(np.diff(recent_validation_losses) > 0)\n    return False\n    \ndef run_train_process(epochs, loaders, model, optimizer, criterion, early_stopping_rounds=10):\n    epoch_to_results = {}\n    recent_validation_losses = deque()\n    \n    for epoch in tqdm(range(epochs)):\n        train_loss, train_accuracy = train(loaders['train'], model, optimizer, criterion)\n        validation_loss, validation_accuracy = test(loaders['validation'], criterion, model)\n        epoch_to_results[epoch] = {\n            'train_loss': train_loss,\n            'train_accuracy': train_accuracy,\n            'validation_loss': validation_loss,\n            'validation_accuracy': validation_accuracy,\n        }\n\n        # Debug\n        if epoch % 5 == 0:\n            print('[%s] Train Accuracy: %.5f , Validation Accuracy: %.5f' % (epoch, train_accuracy, validation_accuracy))\n        \n        # Early Stop\n        should_early_stop = _should_early_stop(recent_validation_losses, validation_loss, early_stopping_rounds)\n        if should_early_stop:\n            print(f'Train epoch {epoch} EARLY STOPPING - validation loss has not improved in the last {early_stopping_rounds} rounds')\n            break\n    \n    df_epoch_to_results = pd.DataFrame.from_dict(epoch_to_results).T\n    _, test_accuracy = test(loaders['test'], criterion, model)\n    \n    return df_epoch_to_results, test_accuracy","c6d18dcd":"import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm, tqdm_notebook","1317e352":"class LeNet(nn.Module):\n    def __init__(self, num_classes=12, num_rgb=3):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(num_rgb, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(44944, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, num_classes)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = out.view(out.size(0), -1)\n        out = self.fc3(out)\n        return out\n","1da97279":"learning_rate = 1e-3\nlenet = LeNet().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\nimage_size = (224, 224)\ntrain_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor()\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor()\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\nbatch_size = 8\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 3\n\ndf_experiment2_result, test_accuracy_experiment2 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment2_result","52651c1f":"ax = df_experiment2_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment2_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 2] Test Accuracy: {test_accuracy_experiment2:.2f}')","bc2f99ef":"image_size = (224, 224)\ntrain_set = SeedlingDataset(\n    X_train.file, y_train, '..\/input\/train\/',\n    transform=transforms.Compose([\n        transforms.Resize(size=image_size),\n        transforms.ToTensor()]))\n\nmeans = []\nmeans_sq = []\n\nfor img, _ in tqdm_notebook(train_set):\n    means.append(np.asarray(img, dtype='float32').mean(axis=(1,2)))\n    means_sq.append((np.asarray(img, dtype='float32') ** 2).mean(axis=(1,2)))\n\nmean_img = np.mean(means, axis=0)\nstd_img = np.sqrt(np.mean(means_sq, axis=0) - (mean_img ** 2))","947ddefe":"mean_img, std_img","e71ff813":"learning_rate = 1e-3\nlenet = LeNet().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\nimage_size = (224, 224)\ntrain_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\nbatch_size = 8\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 3\n\ndf_experiment3_part1_result, test_accuracy_experiment3_part1 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment3_part1_result","fce8164e":"ax = df_experiment3_part1_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment3_part1_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 3 Part 1] Test Accuracy: {test_accuracy_experiment3_part1:.2f}')","45b8b715":"image_size = (224, 224)\nbatch_size = 8\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\nlearning_rate = 1e-3\nlenet = LeNet().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 3\n\ndf_experiment3_part2_result, test_accuracy_experiment3_part2 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment3_part2_result","361bf6d7":"ax = df_experiment3_part2_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment3_part2_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 3 Part 2] Test Accuracy: {test_accuracy_experiment3_part2:.2f}')","8356e227":"class LeNetWithKernelSize3(LeNet):\n    def __init__(self, num_classes=12, num_rgb=3):\n        super(LeNetWithKernelSize3, self).__init__(num_classes, num_rgb)\n        self.conv1 = nn.Conv2d(num_rgb, 6, 3)\n        self.conv2 = nn.Conv2d(6, 16, 3)\n        self.fc1 = nn.Linear(46656, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, num_classes)","e53558a3":"image_size = (224, 224)\nbatch_size = 8\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n\nlearning_rate = 1e-3\nlenet = LeNetWithKernelSize3().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\nbatch_size = 8\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 3\n\ndf_experiment4_part1_result, test_accuracy_experiment4_part1 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment4_part1_result","41d94f9f":"ax = df_experiment4_part1_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment4_part1_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 4 Part 1] Test Accuracy: {test_accuracy_experiment4_part1:.2f}')","4d2004a6":"image_size = (224, 224)\nbatch_size = 32\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n\nlearning_rate = 1e-3\nlenet = LeNetWithKernelSize3().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 100\n\ndf_experiment4_part2_result, test_accuracy_experiment4_part2 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment4_part2_result","17608ec5":"ax = df_experiment4_part2_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment4_part2_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 4 Part 2] Test Accuracy: {test_accuracy_experiment4_part2:.2f}')","313e9629":"class LeNetWithSigmoidActivation(LeNet):\n    def forward(self, x):\n        out = F.sigmoid(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.sigmoid(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.sigmoid(self.fc1(out))\n        out = F.sigmoid(self.fc2(out))\n        out = out.view(out.size(0), -1)\n        out = self.fc3(out)\n        return out","a69a2223":"image_size = (224, 224)\nbatch_size = 32\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n\nlearning_rate = 1e-3\nlenet = LeNetWithSigmoidActivation().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 100\n\ndf_experiment5_part1_result, test_accuracy_experiment5_part1 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment5_part1_result","a94c9621":"ax = df_experiment5_part1_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment5_part1_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 5 Part 1] Test Accuracy: {test_accuracy_experiment5_part1:.2f}')","ab120752":"image_size = (224, 224)\nbatch_size = 32\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n\nlearning_rate = 1e-3\nlenet = LeNet().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lenet.parameters(), lr=learning_rate)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 100\n\ndf_experiment5_part2_result, test_accuracy_experiment5_part2 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment5_part2_result","e37f7dfa":"ax = df_experiment5_part2_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment5_part2_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 5 Part 2] Test Accuracy: {test_accuracy_experiment5_part2:.2f}')","4f367393":"# Based on: https:\/\/www.kaggle.com\/carloalbertobarbano\/vgg16-transfer-learning-pytorch\n\n# Load the pretrained model from pytorch\nvgg16 = models.vgg16_bn(pretrained=True)\nprint(f'Number of output fearurs of pretrained vgg is: {vgg16.classifier[6].out_features}') # 1000\n\n# Freeze training for all layers\nfor param in vgg16.features.parameters():\n    param.require_grad = False\n\n# Newly created modules have require_grad=True by default\nnum_features = vgg16.classifier[6].in_features\nprint(f'Last layer of vgg16 has {num_features} input features.')\nprint(f'Removing layer: {list(vgg16.classifier.children())[-1]}')\nclassifier_layers = list(vgg16.classifier.children())[:-1] # Remove last layer\nlayer_to_add = nn.Linear(num_features, len(np.unique(y)))\nprint(f'Adding layer: {layer_to_add}')\nclassifier_layers.extend([layer_to_add]) # Add our layer with 4 outputs\nvgg16.classifier = nn.Sequential(*classifier_layers) # Replace the model classifier","485010c2":"image_size = (224, 224)\nbatch_size = 32\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n\nlearning_rate = 1e-3\nvgg = vgg16.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(vgg.parameters(), lr=0.001, momentum=0.9)\n#optimizer = optim.Adam(vgg.parameters(), lr=learning_rate)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '..\/input\/train\/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '..\/input\/train\/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '..\/input\/train\/', transform=validation_trans)\n\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 30\n\ndf_experiment8_result, test_accuracy_experiment8 = run_train_process(epochs, loaders, vgg, optimizer, criterion)\ndf_experiment8_result","008b3319":"ax = df_experiment8_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment8_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 8] Test Accuracy: {test_accuracy_experiment8:.2f}')","a42a5396":"df_experiment2_result['experiment']       = 'experiment2'\ndf_experiment3_part1_result['experiment'] = 'experiment3_part1'\ndf_experiment3_part2_result['experiment'] = 'experiment3_part2'\ndf_experiment4_part1_result['experiment'] = 'experiment4_part1'\ndf_experiment4_part2_result['experiment'] = 'experiment4_part2'\ndf_experiment5_part1_result['experiment'] = 'experiment5_part1'\ndf_experiment5_part2_result['experiment'] = 'experiment5_part2'\ndf_experiment8_result['experiment']       = 'experiment8'\n\ndf_all_expirement_results = pd.concat([\n    df_experiment2_result,\n    df_experiment3_part1_result,\n    df_experiment3_part2_result,\n    df_experiment4_part1_result,\n    df_experiment4_part2_result,\n    df_experiment5_part1_result,\n    df_experiment5_part2_result,\n    df_experiment8_result\n]).rename_axis('epoch').reset_index()\n\ndf_all_expirement_results","e205d572":"df_all_expirement_results.to_csv('df_all_expirement_results.csv', index=False, header=True)","0657cc88":"data_dir = '..\/input\/'\nsample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n\nimage_size = (224, 224)\ntest_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\nkaggle_train_set = SeedlingDataset(\n    sample_submission.file,\n    sample_submission.species,\n    '..\/input\/test\/',\n    transform=test_trans)\n\nkaggle_test_loader = DataLoader(kaggle_train_set, batch_size=8, shuffle=False, num_workers=0)\n\ndef predict(sample_submission, kaggle_test_loader, model):\n    predictions = []\n    for data, _ in kaggle_test_loader:\n        if use_cuda:\n            data = data.cuda() \n            data = Variable(data)\n            output = model(data)\n            pred = output.data.max(1)[1] # get the index of the max log-probability\n            predictions.extend(pred.tolist())\n\n    df_predictions = pd.DataFrame.from_dict({\n        'file': sample_submission.file,\n        'species': [num_to_class[p] for p in predictions]\n    })\n    return df_predictions","385253e3":"df_predictions = predict(sample_submission, kaggle_test_loader, lenet)\ndf_predictions.to_csv('predictions_lenet.csv', index=False, header=True)","56a46b91":"df_predictions = predict(sample_submission, kaggle_test_loader, vgg)\ndf_predictions.to_csv('predictions_vgg.csv', index=False, header=True)","2564cabd":"# Combine Results","8f01e708":"# Train and Test Methods","6748a2c4":"## Normalize Images","73a12213":"# Experiment 4","484a77f5":"## Augmentations","b4bc8463":"## Change Kernel Size to 3","058aa21b":"# Kaggle Sumbission","0977d1c6":"## Change Optimization Function to Adam","d9a37f86":"# Experiment 8 - Transfer Learning","f6346f14":"# Experiment 3","3f2dfca2":"## Change Batch size to 32","5789cf36":"# Experiment 2","cf904fe4":"# Lenet","a12e0565":"# Experiment 5","3fd59682":"# Train Validation Test Split","a138ad4c":"## Change Activation Function to Sigmoid"}}