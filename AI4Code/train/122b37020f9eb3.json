{"cell_type":{"45640766":"code","6be49891":"code","2e2044a7":"code","e1830df3":"code","0e82a132":"code","eade170f":"code","45327f66":"code","09af07db":"code","77cfff9c":"code","0e8b0ad6":"code","5c5b2761":"code","41418578":"code","36b0bc04":"code","5e710f76":"code","da9e77d9":"code","b742777f":"code","7b8e25ed":"code","079b87a8":"code","528e6d1b":"code","d7af625f":"code","5d6c8f09":"code","3d52b4c4":"code","4ef6b0e3":"code","4b7f46f7":"code","bbec90d0":"code","2070b137":"code","66fe4c6f":"code","71830355":"code","344a26ea":"code","b80baaae":"code","31143185":"code","db649b96":"code","61c724aa":"code","5da7dc8f":"code","56d542d0":"code","76e17b31":"code","aeddb8c9":"code","5219d7c3":"code","a17ed9ea":"code","f916de7d":"code","b0176ccb":"code","7d74cc3d":"code","1df1064e":"code","4587eb35":"code","3dcee11d":"code","b1d5ce5a":"code","2e6a58c0":"code","2beef894":"code","4e818d03":"code","e54b715a":"code","2d6fdff3":"code","eceea244":"code","4897b387":"code","bbb62e3a":"code","faedf1e3":"code","1e12c302":"code","4f855402":"code","e0f15c30":"code","fbf36b3b":"code","5577c35e":"code","44e96b6e":"code","6557e76f":"code","554ff965":"code","49f1472f":"code","20533cef":"code","bffd1323":"code","edc17414":"code","c41498ab":"code","a3eba809":"code","e6cf9495":"code","4380c07a":"code","e9ac08c3":"code","8e08ed4a":"code","2216cde4":"code","78a4cb9f":"code","b39d23a4":"code","3e54b76f":"code","8ee927f9":"code","469d1b4d":"code","fcd3ac53":"code","f4fba3dc":"code","a96d7352":"markdown","9028b9a4":"markdown","62d9a724":"markdown","a85a9fa9":"markdown","1df03e72":"markdown","293b83af":"markdown","00198c73":"markdown","e3ab1c8a":"markdown","fdaef696":"markdown","a2b6e687":"markdown","c23b6835":"markdown","dc4839bd":"markdown","ec70f7ed":"markdown","cbfab53f":"markdown","0a467a11":"markdown","a984e279":"markdown","0885fe50":"markdown","3040a887":"markdown","e7b4e94e":"markdown","a3d0f12c":"markdown","754dee47":"markdown","54c00a9a":"markdown","51b86a34":"markdown","95562617":"markdown","b9a638ab":"markdown","57671ce4":"markdown","2518181f":"markdown","cbe8b414":"markdown","998fbceb":"markdown","2fac231d":"markdown","0f0132c3":"markdown","a00acc87":"markdown","e67b0bea":"markdown","f21ae2e6":"markdown","150374ab":"markdown","c00cf7c3":"markdown","3e865165":"markdown","bd6bfd99":"markdown","9f1e36e1":"markdown","de73477f":"markdown","40605939":"markdown","323dba82":"markdown","d4347993":"markdown"},"source":{"45640766":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6be49891":"import seaborn as sns\nimport matplotlib.pyplot as plt","2e2044a7":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","e1830df3":"print(\"Training set shape: \", train_df.shape)\nprint(\"Test set shape: \", test_df.shape)","0e82a132":"train_df.head(5)","eade170f":"train_df.info()","45327f66":"try:\n    train_df = train_df.drop(['Id'], axis=1)\nexcept KeyError:\n    pass\nprint(\"Number of string cols\", (train_df.dtypes == 'object').sum())\nprint(\"Number of numeric cols\", (train_df.dtypes != 'object').sum())","09af07db":"from scipy.stats import norm, probplot\n\n\nfig, ax = plt.subplots(figsize=(10,8))\nsns.histplot(train_df['SalePrice'], kde=True, ax=ax)\nax.axvline(x=train_df['SalePrice'].mean(), color='r', label='Mean')\nax.axvline(x=train_df['SalePrice'].median(), color='g', label='Median')\nax.legend()\n# sns.boxplot(x='SalePrice', data=train_df, ax=ax[1])\nn_mean, n_std = norm.fit(train_df['SalePrice'])\n","77cfff9c":"sns.boxplot(x='SalePrice', data=train_df)","0e8b0ad6":"# Base statistics for target variable\ntrain_df['SalePrice'].describe()","5c5b2761":"# Skew and kurt\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())","41418578":"log_sale_price = np.log(train_df['SalePrice'])\nlog_sale_price.describe()\n","36b0bc04":"NUM_FEATURES = train_df.columns[(train_df.dtypes != 'object')].tolist()\nNUM_FEATURES.sort()\nprint(NUM_FEATURES)\nprint(\"Len of NUM_FEATURES:\", len(NUM_FEATURES))","5e710f76":"NUM_FEATURES.remove('MSSubClass')","da9e77d9":"total_bsmt_sf = train_df['BsmtFinSF1'] + train_df['BsmtFinSF2'] + train_df['BsmtUnfSF']\nprint(np.allclose(total_bsmt_sf, train_df['TotalBsmtSF']))","b742777f":"gr_liv_area = train_df['1stFlrSF'] + train_df['2ndFlrSF'] + train_df['LowQualFinSF']\nprint(np.allclose(gr_liv_area, train_df['GrLivArea']))","7b8e25ed":"# Number of unique values among columns with numeric data\ntrain_df[NUM_FEATURES].nunique().sort_values()","079b87a8":"def plot_grid(df, features_names, n_rows, n_cols, figsize=(20, 16)):\n    \"\"\"Function to plot a grid of histograms\"\"\"\n    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize)\n\n    j = 0\n    feature_names_len = len(features_names)\n    for i in range(n_rows * n_cols):\n        if i < feature_names_len:\n            sns.histplot(train_df[features_names[i]], ax=axes[i \/\/ n_cols, j])\n            for tick in axes[i \/\/ n_cols, j].get_xticklabels():\n                tick.set_rotation(45)\n            j += 1\n            if i > 0 and (i + 1) % n_cols == 0:\n                j = 0\n        else:\n            axes[i \/\/ n_cols, j].remove()\n            j += 1\n            if i > 0 and (i + 1) % n_cols == 0:\n                j = 0","528e6d1b":"plot_grid(train_df, NUM_FEATURES[:16], 4, 4, (20, 24))","d7af625f":"plot_grid(train_df, NUM_FEATURES[16: 32], 4, 4, (20, 24))","5d6c8f09":"plot_grid(train_df, NUM_FEATURES[33: 36], 2, 2, (10, 12))","3d52b4c4":"corr_matrix = train_df[NUM_FEATURES].corr()","4ef6b0e3":"print(corr_matrix['SalePrice'].sort_values(ascending=False))","4b7f46f7":"# Visualization of correlation matrix\nfig = sns.clustermap(corr_matrix,\n                     row_cluster=True,\n                     col_cluster=True,\n                     figsize=(20, 20), annot=True, cmap=\"BuGn\")\n\nplt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\nplt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()","bbec90d0":"sns.relplot(x='GrLivArea', y='SalePrice',  hue='OverallQual', col='GarageCars', data=train_df)","2070b137":"sns.regplot(x='OverallQual', y='SalePrice', data=train_df)","66fe4c6f":"X = train_df[train_df.columns.drop('SalePrice')]\ny = np.log(train_df['SalePrice'])","71830355":"# check missing values\nX[['GrLivArea', 'OverallQual', 'GarageCars']].isnull().sum()","344a26ea":"NUM_FEATURES.remove('SalePrice')","b80baaae":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer","31143185":"def df_cols_selector(col_names):\n    return FunctionTransformer(lambda x: x[col_names], validate=False)","db649b96":"from sklearn.model_selection import cross_val_score\n\ndef cross_validate_model(estimator, X, y, cv=3):\n    cv_scores = cross_val_score(estimator, X, y, error_score='raise', scoring='neg_root_mean_squared_error', cv=cv)\n    rmse_mean = np.abs(cv_scores).mean()\n    rmse_std = np.abs(cv_scores).std()\n    return rmse_mean, rmse_std","61c724aa":"pipeline_simple = Pipeline([\n    ('selector', df_cols_selector(['GrLivArea', 'OverallQual', 'GarageCars'])),\n    ('scaler', StandardScaler()),\n    ('lm', LinearRegression())\n])\ncross_validate_model(pipeline_simple, X, y)","5da7dc8f":"numeric_null_vals = train_df[NUM_FEATURES].isnull().sum()\nnumeric_null_vals[numeric_null_vals > 0].sort_values(ascending=False)","56d542d0":"sns.relplot(x='LotFrontage', y='SalePrice', data=train_df)","76e17b31":"# How SalePrice distributed for rows with missing lot frontage\nfig, ax = plt.subplots(figsize=(10,8))\nsns.histplot(x='SalePrice', data=train_df.loc[train_df['LotFrontage'].isnull()], ax=ax)","aeddb8c9":"sns.relplot(x='GarageYrBlt', y='SalePrice', hue='GarageCond', data=train_df, alpha=0.75)","5219d7c3":"NUM_FEATURES.remove('GarageYrBlt')\n# As well as other columns which I mension before\nNUM_FEATURES.remove('PoolArea')\nNUM_FEATURES.remove('3SsnPorch')\nNUM_FEATURES.remove('LowQualFinSF')","a17ed9ea":"from sklearn.impute import KNNImputer\nfrom sklearn.impute import SimpleImputer\n\n\npipeline_num = Pipeline([\n    ('selector', df_cols_selector(NUM_FEATURES)),\n    ('imputer', KNNImputer(n_neighbors=20)),\n    ('scaler', StandardScaler()),\n    ('lm', LinearRegression())\n])\ncross_validate_model(pipeline_num, X, y)","f916de7d":"cross_validate_model(pipeline_num, X, y)","b0176ccb":"TEXT_FEATURES = train_df.columns[(train_df.dtypes == 'object')].tolist()\nTEXT_FEATURES.sort()\nprint(TEXT_FEATURES)\n\nTEXT_FEATURES.append('MSSubClass')\nprint(\"Len of TEXT_FEATURES:\", len(TEXT_FEATURES))","7d74cc3d":"def catplot_grid(df, features_names, target_feature,  n_rows, n_cols, figsize=(20, 16)):\n    \"\"\"Function to plot a grid of histograms\"\"\"\n    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize)\n\n    j = 0\n    feature_names_len = len(features_names)\n    for i in range(n_rows * n_cols):\n        if i < feature_names_len:\n            sns.stripplot(x=features_names[i], y=target_feature, data=train_df,  ax=axes[i \/\/ n_cols, j])\n            for tick in axes[i \/\/ n_cols, j].get_xticklabels():\n                tick.set_rotation(45)\n            j += 1\n            if i > 0 and (i + 1) % n_cols == 0:\n                j = 0\n        else:\n            axes[i \/\/ n_cols, j].remove()\n            j += 1\n            if i > 0 and (i + 1) % n_cols == 0:\n                j = 0\n","1df1064e":"catplot_grid(train_df, TEXT_FEATURES[0:9], 'SalePrice', 3, 3, (20, 18))","4587eb35":"catplot_grid(train_df, TEXT_FEATURES[9:18], 'SalePrice', 3, 3, (20, 20))","3dcee11d":"catplot_grid(train_df, TEXT_FEATURES[18:27], 'SalePrice', 3, 3, (20, 20))","b1d5ce5a":"catplot_grid(train_df, TEXT_FEATURES[27:36], 'SalePrice', 3, 3, (20, 20))","2e6a58c0":"catplot_grid(train_df, TEXT_FEATURES[36:45], 'SalePrice', 3, 3, (20, 20))","2beef894":"TEXT_FEATURES.remove('PoolQC')","4e818d03":"# Number of unique values among columns with numeric data\ntrain_df[TEXT_FEATURES].nunique().sort_values()","e54b715a":"missing_vals_cat = train_df[TEXT_FEATURES].isnull().sum()","2d6fdff3":"missing_vals_cat[missing_vals_cat > 0].sort_values(ascending=False)","eceea244":"train_df['Electrical'].mode()","4897b387":"# Here I redefine fill_missing_vals to handle all columns with missing values\ndef fill_missing_vals(df: pd.DataFrame) -> None:\n    \"\"\"Fill missing values\"\"\"\n    values_to_fill = {\n        #\"LotFrontage\": impute with KNNImputer\n        \"MasVnrArea\": 0,\n        \"Fence\": \"No Fence\",\n        \"FireplaceQu\": \"No Fireplace\",\n        \"Alley\": \"No Alley\",\n        \"GarageType\": \"No Garage\",\n        \"GarageFinish\": \"No Garage\",\n        \"GarageQual\": \"No Garage\",\n        \"GarageCond\": \"No Garage\",\n        \"BsmtExposure\": \"No Basement\",\n        \"BsmtFinType2\": \"No Basement\",\n        \"BsmtQual\": \"No Basement\",\n        \"BsmtCond\": \"No Basement\",\n        \"BsmtFinType1\": \"No Basement\",\n        \"MasVnrType\": \"Not Given\",\n        \"Electrical\": \"SBrkr\"\n    }\n    df = df.fillna(values_to_fill)\n    return df","bbb62e3a":"X = fill_missing_vals(X)","faedf1e3":"def add_features(df: pd.DataFrame):\n    df.loc[:, 'PorchArea'] = df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch']\n\n    df.loc[:, 'Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n                               df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n\n    df.loc[:, \"TwoFlrsSF\"] = df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n    return df","1e12c302":"X = add_features(X)\ntrain_df = add_features(train_df)","4f855402":"NUM_FEATURES.append('Total_Bathrooms')\nNUM_FEATURES.append('TwoFlrsSF')\nNUM_FEATURES.append('PorchArea')\n\nTEXT_FEATURES.remove('MiscFeature')","e0f15c30":"for f_name in TEXT_FEATURES:\n    val_counts = train_df[f_name].value_counts()\n    not_null_vals = train_df[f_name].notnull().sum()\n    val_props = val_counts \/ not_null_vals\n    max_proportion = val_props.max()\n    most_common = val_counts.index[0]\n    if max_proportion > 0.96:\n        print(f'{f_name}: \"{most_common}\" - {max_proportion}')\n        TEXT_FEATURES.remove(f_name)       ","fbf36b3b":"for f_name in NUM_FEATURES:\n    val_counts = train_df[f_name].value_counts()\n    not_null_vals = train_df[f_name].notnull().sum()\n    val_props = val_counts \/ not_null_vals\n    max_proportion = val_props.max()\n    most_common = val_counts.index[0]\n    if max_proportion > 0.96:\n        print(f'{f_name}: \"{most_common}\" - {max_proportion}')\n        NUM_FEATURES.remove(f_name) \n","5577c35e":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import FeatureUnion\n\n\ndef get_pipeline(estimator, num_features, text_features):\n    num_pipeline = Pipeline([\n        ('selector', df_cols_selector(num_features)),\n        ('imputer', KNNImputer(n_neighbors=20)),\n        ('scaler', StandardScaler())\n    ])\n    text_pipeline = Pipeline([\n        ('selector', df_cols_selector(text_features)),\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('oh', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\n    feature_union = FeatureUnion(transformer_list=[\n        ('numeric_features', num_pipeline),\n        ('text_features', text_pipeline)\n    ])\n\n    pipeline = Pipeline([\n        ('feature_union', feature_union),\n        ('estimator', estimator)\n    ])\n    return pipeline","44e96b6e":"bm_pipeline = get_pipeline(LinearRegression(), NUM_FEATURES, TEXT_FEATURES)","6557e76f":"cross_validate_model(bm_pipeline, X, y)","554ff965":"from sklearn.linear_model import Ridge","49f1472f":"ridge_pipeline = get_pipeline(Ridge(alpha=8), NUM_FEATURES, TEXT_FEATURES)","20533cef":"cross_validate_model(ridge_pipeline, X, y)","bffd1323":"from sklearn.ensemble import RandomForestRegressor","edc17414":"rf_reg = RandomForestRegressor(n_estimators=500, max_depth=18, max_features='sqrt', n_jobs=-1, random_state=42)\nrf_pipeline = get_pipeline(rf_reg, NUM_FEATURES, TEXT_FEATURES)","c41498ab":"cross_validate_model(rf_pipeline, X, y)","a3eba809":"from xgboost import XGBRegressor\n\nxgb_reg = XGBRegressor(n_estimators=1000, max_depth=4, colsample_bytree=0.2, n_jobs=2)\nxgb_pipeline = get_pipeline(xgb_reg, NUM_FEATURES, TEXT_FEATURES)\ncross_validate_model(xgb_pipeline, X, y)","e6cf9495":"print(NUM_FEATURES)","4380c07a":"print(TEXT_FEATURES)","e9ac08c3":"import optuna\nfrom optuna import Trial\n\ndef xgb_reg_objective(trial: Trial):\n\n    n_estimators = trial.suggest_int('n_estimators', 950, 2000, 50)\n    max_depth = trial.suggest_int('max_depth', 3, 8, step=1)\n    colsample_bytree = trial.suggest_float('colsample_bytree', 0.1, 0.9, step=0.05)\n    learning_reate = trial.suggest_loguniform('learing_rate', 1e-2, 0.4)\n\n    xgb_reg = XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, colsample_bytree=colsample_bytree, learning_rate=learning_reate, n_jobs=2, random_state=142)\n    pipeline = get_pipeline(xgb_reg, NUM_FEATURES, TEXT_FEATURES)\n    score = cross_validate_model(pipeline, X, y)\n    return score[0] \n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(xgb_reg_objective, n_trials=300)","8e08ed4a":"best_params = study.best_params\nprint(best_params)","2216cde4":"xgb_reg_tuned = XGBRegressor(n_estimators=int(best_params['n_estimators']),\n                             max_depth=int(best_params['max_depth']),\n                             colsample_bytree=float(best_params['colsample_bytree']),\n                             learning_rate=float(best_params['learing_rate']), n_jobs=2, random_state=142)\nxgb_pipeline_tuned = get_pipeline(xgb_reg_tuned, NUM_FEATURES, TEXT_FEATURES)","78a4cb9f":"cross_validate_model(xgb_pipeline_tuned, X, y)","b39d23a4":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","3e54b76f":"train_df = fill_missing_vals(train_df)\ntrain_df = add_features(train_df)\n\ntest_df = fill_missing_vals(test_df)\ntest_df = add_features(test_df)","8ee927f9":"xgb_pipeline_tuned.fit(train_df[train_df.columns.drop('SalePrice')], np.log(train_df['SalePrice']))\ny_pred_test = xgb_pipeline_tuned.predict(test_df)\ny_pred_test = np.exp(y_pred_test)","469d1b4d":"import csv\n\n\ndef prepare_submission(test_data: pd.DataFrame, predicted: np.ndarray):\n    with open(f\"submissions.csv\", mode=\"w\") as f:\n        csv_writer = csv.DictWriter(f, fieldnames=['Id', \"SalePrice\"])\n        csv_writer.writeheader()\n        for idx, df_row in test_data.iterrows():\n            csv_writer.writerow({\"Id\": df_row['Id'], \"SalePrice\": predicted[idx]})","fcd3ac53":"prepare_submission(test_df, y_pred_test)","f4fba3dc":"y_pred_test[:15]","a96d7352":"\n\nRead training and test set into pandas DataFrame","9028b9a4":"### Feature Selection  ","62d9a724":"#### Correlations","a85a9fa9":"## Submission","1df03e72":"## Data Exploration","293b83af":"### Numerical Features","00198c73":"### Categorical features   \nSo far we haven't looked at categorical features. Let's now explore them","e3ab1c8a":"### Ridge regression","fdaef696":"**MiscFeature**: Miscellaneous feature not covered in other categories\n\t\t\n       Elev\tElevator\n       Gar2\t2nd Garage (if not described in garage section)\n       Othr\tOther\n       Shed\tShed (over 100 SF)\n       TenC\tTennis Court\n       NA\tNone","a2b6e687":"### Linear regression model with numeric features","c23b6835":"**GrLivArea**: Above grade (ground) living area square feet  \n**1stFlrSF**: First Floor square feet  \n**2ndFlrSF**: Second floor square feet  \n**LowQualFinSF**: Low quality finished square feet (all floors)","dc4839bd":"For linear models it is important that all numeric features are on the same scale, so we will use StandardScaler","ec70f7ed":"Using all numeric features allows our model to improve it's predictions","cbfab53f":"I will just remove the whole column **GarageYrBlt**","0a467a11":"As we can see some variable are have a strong correlation with target variable. It will be helpfull for our model","a984e279":"For all columns which contains \"Garage\" in it's name replace N\/As with \"No Garage\"  \nFor all columns which contains \"Bsmt\" in it's name replace N\/As with \"No Basement\"  \n**MasVnrType** - for now I will replace NA\/s \"None\" because for **MasVnrArea** I replaced N\/As with 0. Perhabs it is reasonable to treat N\/As for this features differently  \nFor **Electrical** - most common value\n","0885fe50":"### Target variable","3040a887":"### Dealing with missing data\n\nJust like numeric features, it is possible to have missing values for categorical features. Let's take a look how many missing values we have","e7b4e94e":"**LotFrontage**: Linear feet of street connected to property. It seems we can fill missing values with 0.\n\n**GarageYrBlt**: Year garage was built. Obviously, if the house does not have a garage - the value for this column is empty.\n\n**MasVnrArea**: Masonry veneer area in square feet. It seems we can fill missing values with 0.\n\n\n","a3d0f12c":"There are some columns related to basement area\n\n**TotalBsmtSF**: Total square feet of basement area  \n**BsmtFinSF1**: Type 1 finished square feet  \n**BsmtFinSF2**: Type 2 finished square feet  \n**BsmtUnfSF**: Unfinished square feet of basement area  \n\nIt seems that TotalBsmtSF is just a sum of the rest","754dee47":"As we can see train and test set have almost equal size.","54c00a9a":"Luckily these features don't have missing values","51b86a34":"FireplaceQu: Fireplace quality\n\n       Ex\tExcellent - Exceptional Masonry Fireplace\n       Gd\tGood - Masonry Fireplace in main level\n       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n       Fa\tFair - Prefabricated Fireplace in basement\n       Po\tPoor - Ben Franklin Stove\n       NA\tNo Fireplace\n**Solution**: replace NAs with \"No Fireplace\"       ","95562617":"As we can see there numerical and categorical features in our data. Some of them have a lot of missing values","b9a638ab":"**3SsnPorch** and **PoolArea** don't seem to be informative for a model","57671ce4":"I think that Iwill remove column **MiscFeature** entirely.\nFor **Alley** we could replace NAs with \"No Alley\"","2518181f":"## Goal\n\nPredict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n## Metric\n\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)","cbe8b414":"**Fence**: Fence quality\n\t\t\n       GdPrv\tGood Privacy\n       MnPrv\tMinimum Privacy\n       GdWo\tGood Wood\n       MnWw\tMinimum Wood\/Wire\n       NA\tNo Fence\n**Solution**: Replace NAs with \"No Fence\"","998fbceb":"So far XGB regressor appears to be the best model. It is reasonable to explore how each other model will perform with properly chosen hyperparameters. But at least for now I will focus only on XGB regressor.","2fac231d":"### Feature Engineering  \nHere I will create some additional features in hope that they helps a model to do better job. The idea for **Total_Bathrooms** and **TwoFlrsSF** is borrowed for notebooks of other participants","0f0132c3":"### Simple model\n\nHere I will try very simple model which use top 3 features which have large corellation coefficients with target variable","a00acc87":"## Hyperparameter tuning\n\nHyperparameters have a significant impact on model's performance. There are multiple techniques how to select them. I will use [optuna](https:\/\/optuna.org\/)\n\n","e67b0bea":"### XGB boost","f21ae2e6":"It is very simple model, we could not expect great performace here.","150374ab":"As I removed **PoolArea** I will remove **PoolQc** as well","c00cf7c3":"### Base model\n\nHere I am going to set up a base model which results than will be used to compare with other models","3e865165":"**LowQualFinSF** can be ommited","bd6bfd99":"We should be careful here as some columns might contain just numerical encoding for some unordered categories. For instance:\n\n**MSSubClass** : Identifies the type of dwelling involved in the sale.\t\n\n        20\t1-STORY 1946 & NEWER ALL STYLES\n        30\t1-STORY 1945 & OLDER\n        40\t1-STORY W\/FINISHED ATTIC ALL AGES\n        45\t1-1\/2 STORY - UNFINISHED ALL AGES\n        50\t1-1\/2 STORY FINISHED ALL AGES\n        60\t2-STORY 1946 & NEWER\n        70\t2-STORY 1945 & OLDER\n        75\t2-1\/2 STORY ALL AGES\n        80\tSPLIT OR MULTI-LEVEL\n        85\tSPLIT FOYER\n        90\tDUPLEX - ALL STYLES AND AGES\n       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n       150\t1-1\/2 STORY PUD - ALL AGES\n       160\t2-STORY PUD - 1946 & NEWER\n       180\tPUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES","9f1e36e1":"# House prices prediction","de73477f":"## Handling missing values\n\nAs you probably know, we can not just train model without dealing with missing values. There are several strategies how to deal with them. Each approach has it's pros and cons. One strategy is to remove all rows with have missing data. It is not suitable for us as test data may contain missing values. We can drop all columns which contain missing data, however we could loose important information. The third way is to fill missing data with some appropriate values. It could be median, mean, most frequent value etc.","40605939":"As we can suppose from the plots above, some of the features have a dominant value. Perhabs our model won't be able to extract useful information from those features. Here I exclude all columns for which most common value appears for more than 96% of the rows","323dba82":"As we going to predict log of SalePrice, let's look basic statistics for it","d4347993":"### Random Forest Regressor"}}