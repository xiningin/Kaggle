{"cell_type":{"4d9527b4":"code","28ccf109":"code","3479a25c":"code","c17bebf4":"code","a159a818":"code","8b0cda27":"code","be4946e0":"code","ca9df792":"code","e4411fd3":"code","c125754e":"code","73ab1376":"code","4f4d961a":"code","a64d3f60":"code","db88e9da":"code","cad6481f":"code","00853c55":"code","f0781c64":"code","4b6f0874":"code","fd9aed5f":"code","93a1a16c":"code","3ded20d2":"code","69cc3830":"code","5e1e27f9":"markdown","07e10003":"markdown","a0ba4251":"markdown","c04274fa":"markdown","8095b4ea":"markdown","1415cba9":"markdown","e9c78d8d":"markdown","1b3d4a45":"markdown","9940c7e6":"markdown","0bd877da":"markdown","86a6d948":"markdown","65155707":"markdown","c483edb0":"markdown","eb3b0e6a":"markdown","acb2698b":"markdown","f582e11a":"markdown","51e0e2b7":"markdown","eb854ef0":"markdown","127b8ce8":"markdown","99820c43":"markdown","438a7789":"markdown"},"source":{"4d9527b4":"# if set to False, the notebook takes about 10 minutes to run\nload_preprocessed_file = True","28ccf109":"final_df_filename = 'df_final_covid_clean_topics.pkl'\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport json\n\nimport pickle as pkl\nimport string\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import sent_tokenize#, word_tokenize\nfrom nltk.corpus import stopwords\nimport time\nfrom multiprocessing import Pool\nimport numpy as np\nimport multiprocessing\nfrom collections import Counter\nfrom itertools import chain\nimport operator\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport itertools\nimport collections\nimport random\nfrom gensim.corpora import Dictionary\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport scipy.sparse as ss\n\n# https:\/\/github.com\/gregversteeg\/corex_topic\n!pip install 'corextopic'\n\nfrom corextopic import corextopic as ct\nfrom corextopic import vis_topic as vt # jupyter notebooks will complain matplotlib is being loaded twice\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfrom ipywidgets import interact\nimport ipywidgets as widgets","3479a25c":"if load_preprocessed_file is False:\n\n    # credit: https:\/\/www.kaggle.com\/gkaraman\/topic-modeling-lda-on-cord-19-paper-abstracts\n    df = pd.read_csv(\n        '\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv'\n        , dtype={\n            'Microsoft Academic Paper ID': str\n            ,'pubmed_id': str\n        })\n\n    # Some papers are duplicated since they were collected from separate sources. Thanks Joerg Rings\n    duplicate_paper = ~(df.title.isnull() | df.abstract.isnull()) & (df.duplicated(subset=['title', 'abstract']))\n    df = df[~duplicate_paper].reset_index(drop=True)\n\n    df = df.dropna(subset=['abstract'])\n\n    # create a column that appends title+abstract. This column will be the \"document\" that all searching\/clustering\/vectorization will use\n    df['document'] = df['title'] + '. ' + df['abstract']\n\n    print(df.shape)","c17bebf4":"# \"supercalifragili-\\nsticexpialidocious\\nthis is a new line\" -> \"supercalifragilisticexpialidocious this is a new line\"\ndef clean_newlines(text):\n    text = text.replace('-\\n', '')\n    text = text.replace('\\n', ' ').replace('\\r',' ')\n    \n    return text\n\ntest = 'supercalifragili-\\nsticexpialidocious\\nthis is a new line '\nclean_newlines(test)","a159a818":"def clean_chars(text):\n    text = \"\".join(i for i in text if ord(i) < 128) # remove all non-ascii characters\n    text = text.replace('\\t', ' ') # convert a tab to space\n    # fastest way to remove all punctuation (except ' . and !) and digits\n    text = text.replace('[Image: see text]', '')\n    text = text.translate(str.maketrans('', '', '\"#$%&()*+,-\/:;<=>@[\\\\]^_`{|}~' + string.digits))\n    \n    return text.strip()\n\nclean_chars('[Image: see text] Numbers 123 are greater than 456?!\\t\"I\\'m of the op1ni0n it isn\\'t...\"')","8b0cda27":"# credit: https:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/\n# helper correctly accounts for the same word having a different\n# part-of-speech depending on the context of its usage\ndef get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)\n\nsentence = \"The ten foot striped bats are hanging on their good better best feet. The bat's wings were ten feet wide.\"\n\nprint([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])","be4946e0":"def dedupe_sentences(sentences):\n    deduped = []\n    for s in sentences:\n        if s not in deduped:\n            deduped.append(s)\n    \n    return deduped\n\ntest_sentences = [\n    ['see', 'figure', 'for', 'data'],\n    ['not', 'cleared', 'for', 'release'],\n    ['new', 'sentence', 'here'],\n    ['see', 'figure', 'for', 'data'],\n    ['not', 'cleared', 'for', 'release'],\n    ['finally']\n]\n\ndedupe_sentences(test_sentences)","ca9df792":"stpwrds_list = stopwords.words('english')\n\n# add custom stopwords here, discovered from most common words and ngrams (further below)\nstpwrds_list += ['...', 'also', 'could', 'thus', 'therefore']\n\nstpwrds_lower = [wrd.lower() for wrd in stpwrds_list]\n\nstpwrds_list += stpwrds_lower\n\nstpwrds = set(stpwrds_list) # dedupes stopwords","e4411fd3":"# given a string representing an entire document, returns the following format where all the words are non-stopwords and lemmatized:\n#example = [\n#    ['Sentence', 'one', 'words'],\n#    ['Sentence', 'two', 'words']\n#]\n\ndef clean(text, min_word_len=3, lower=True):\n    \n    if (lower is True):\n        text = text.lower()\n    \n    text = clean_newlines(text)\n    text = clean_chars(text)\n    \n    sentences = sent_tokenize(text)\n    \n    clean_sentences = []\n    \n    for s in sentences:\n        clean_sent_words = [\n            lemmatizer.lemmatize(w, get_wordnet_pos(w))\n            for w in nltk.word_tokenize(s)\n            # skip short words, contraction parts of speech, and storwords\n            if len(w) >= min_word_len and w[0] != '\\'' and w not in stpwrds\n        ]\n        \n        clean_sentences.append(clean_sent_words)\n    \n    # one and only one identical sentence is allowed per document\n    # this helps avoid common phrases like \"Table of data below:\" appearing\n    # many times, which will skew the word associations\n    clean_sentences = dedupe_sentences(clean_sentences)\n    \n    return clean_sentences\n\ntest = \" NOT CLEARED FOR PUBLIC RELEASE. See table for references. The ss goes here. \"\ntest += \"US interests. U.S. Enterprise. The ten foot striped bats are hanging on their good better best feet. The \"\ntest += \"bat's Wings were ten feet wide. Is the U.S. Enterprise 123 better than 456?!\\t\\\"I\\'m of the op1ni0n it isn\\'t...\\\"\"\ntest += \" New sentence. NOT CLEARED FOR PUBLIC RELEASE. See table for references.\"\nclean(test)","c125754e":"%%time\n\n# https:\/\/towardsdatascience.com\/make-your-own-super-pandas-using-multiproc-1c04f41944a1\ndef parallelize_dataframe(df, func, n_cores=multiprocessing.cpu_count()):\n    df_split = np.array_split(df, n_cores)\n    pool = Pool(n_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n\n    return df\n\ndef clean_dataframe(df):\n    df['clean'] = df.apply(lambda x: clean(x['document']), axis=1)\n\n    return df\n\nif load_preprocessed_file is False:\n    # this parallelize_dataframe way takes about 7 minutes\n    df = parallelize_dataframe(df, clean_dataframe)\n\n    # the \"swifter\" keyword\/library aims to make dataframe processing faster, but it didn't help in this case\n    # !pip install 'swifter'\n    # import swifter\n    # https:\/\/towardsdatascience.com\/add-this-single-word-to-make-your-pandas-apply-faster-90ee2fffe9e8\n    # this method was slower at 17 minutes in total, but it provided a nice progress bar and countdown timer\n    # df['clean'] = df.swifter.apply(lambda x: clean(x['document']), axis=1)\n\n    df[['clean']].head(3)\nelse:\n    df = pkl.load(open('\/kaggle\/input\/cached-data-interactive-abstract-and-expert-finder\/' + final_df_filename, \"rb\" ))","73ab1376":"clean_words = df['clean'].tolist()\nclean_words = [item for sublist in clean_words for item in sublist]\n\nprint(str(len(clean_words)) + ' total words in corpus')\n\ncounter_obj = Counter(chain.from_iterable(clean_words))\nword_counts = counter_obj.most_common()\nword_counts.sort(key=operator.itemgetter(1), reverse=True)\n\nword_counts[0:10]","4f4d961a":"%%time\n# higher threshold means fewer ngrams - open question, how to optimize these hyperparams?\n\nbigram = Phrases(clean_words, min_count=384, threshold=64, delimiter=b'_')\ntrigram = Phrases(bigram[clean_words], min_count=64, threshold=32, delimiter=b'_')","a64d3f60":"sorted(\n    {k:v for k,v in bigram.vocab.items() if b'_' in k if v>=bigram.min_count and str(k).count('_') == 1}.items(),\n    key=operator.itemgetter(1),\n    reverse=True\n)","db88e9da":"sorted(\n    {k:v for k,v in trigram.vocab.items() if b'_' in k if v>=trigram.min_count and str(k).count('_') == 2 }.items(),\n    key=operator.itemgetter(1),\n    reverse=True\n)","cad6481f":"def get_ngram_words(sent_arr):\n    result = []\n\n    for s in sent_arr:\n        sent_result = []\n        for w in trigram[bigram[s]]:\n            if (w not in stpwrds): # we need to check again, because we may have added ngrams to the stopword list\n                sent_result.append(w)\n\n        result.append(sent_result)\n    return result\n\ntest_sentences = [\n    ['polymerase', 'chain', 'reaction'],\n    ['infectious', 'disease', 'cause', 'unknown', 'public', 'health'],\n    ['significant', 'acute', 'respiratory', 'disease', 'report', 'world', 'health']\n]\n\nget_ngram_words(test_sentences)","00853c55":"%%time\n\ndef convert_ngram_dataframe(df):\n    df['clean'] = df.apply(lambda x: get_ngram_words(x['clean']), axis=1)\n\n    return df\n\nif load_preprocessed_file is False:\n    df = parallelize_dataframe(df, convert_ngram_dataframe)\n\n    df[['clean']].head(3)","f0781c64":"all_words = []\ndocs = []\n\nfor index, row in df.iterrows():\n    sent_arr = row['clean']\n    doc_words = []\n    \n    for s in sent_arr:\n        for w in s:\n            doc_words.append(w)\n            all_words.append(w)\n    \n    docs.append(doc_words)\n\nprint('TOTAL WORDS: ' + str(len(all_words)))\nprint('UNIQUE WORDS: ' + str(len(set(all_words))))","4b6f0874":"%%time\n\n# credit: https:\/\/www.kaggle.com\/gkaraman\/topic-modeling-lda-on-cord-19-paper-abstracts\n\n# Create a dictionary representation of the documents\ndictionary = Dictionary(docs)\ndictionary.filter_extremes(no_below=32, no_above=0.2)\n\n# Create Bag-of-words representation of the documents\n#corpus = [dictionary.doc2bow(doc) for doc in docs]\n\nprint('Number of unique tokens: %d' % len(dictionary))\n#print('Number of documents: %d' % len(corpus))\n\n\ndef remove_non_dict_words(sent_arr):\n    result = []\n\n    for s in sent_arr:\n        for w in s:\n            if w in dictionary.token2id:\n                result.append(w)\n                \n    return result\n\ndef remove_non_dict_words_df(df):\n    df['clean_tfidf'] = df.apply(lambda x: remove_non_dict_words(x['clean']), axis=1)\n\n    return df\n\ndf = parallelize_dataframe(df, remove_non_dict_words_df)\n\ndf = df.reset_index() # after all the processing, there are some gaps in the indices, so we reset them to make index counting easier later","fd9aed5f":"%%time\n\nif load_preprocessed_file is False:\n\n    def dummy(doc):\n        return doc\n\n    vectorizer = CountVectorizer(\n        tokenizer=dummy,\n        preprocessor=dummy,\n    )  \n\n    corex_docs = df['clean_tfidf'].tolist()\n    doc_word = vectorizer.fit_transform(corex_docs)\n\n    doc_word = ss.csr_matrix(doc_word)\n\n    # Get words that label the columns (needed to extract readable topics and make anchoring easier)\n    words = list(np.asarray(vectorizer.get_feature_names()))\n\n    #doc_word.shape # n_docs x m_words\n\n\n    # https:\/\/github.com\/gregversteeg\/corex_topic\n    # Train the CorEx topic model with x topics (n_hidden)\n    topic_model = ct.Corex(n_hidden=12, words=words, max_iter=500, verbose=False, seed=2020)\n    #topic_model.fit(doc_word, words=words)\n\n    topic_model.fit(doc_word, words=words)\n\n\n    plt.figure(figsize=(10,5))\n    plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color='#4e79a7', width=0.5)\n    plt.xlabel('Topic', fontsize=16)\n    plt.ylabel('Total Correlation (nats)', fontsize=16);\n    # no single topic should contribute too much. If one does, that indicates more investigation for boilerplate text, more preprocessing required\n    # To find optimal num of topics, we should keep adding topics until additional topics do not significantly contribute to the overall TC\n    \n    pkl.dump(topic_model, open('corex_topic_model.pkl', \"wb\"))\nelse:\n    topic_model = pkl.load(open('\/kaggle\/input\/cached-data-interactive-abstract-and-expert-finder\/corex_topic_model.pkl', \"rb\" ))\n\n# Print all topics from the CorEx topic model\ntopics = topic_model.get_topics()\ntopic_list = []\n\nfor n,topic in enumerate(topics):\n    topic_words,_ = zip(*topic)\n    print('{}: '.format(n) + ','.join(topic_words))\n    topic_list.append('topic_' + str(n) + ': ' + ', '.join(topic_words))","93a1a16c":"%%time\n\nif load_preprocessed_file is False:\n\n    # remove any existing topic columns. This allows us to iterate on number of topics\n    for c in [col for col in df.columns if col.startswith('topic_')]:\n        del df[c]\n\n    # TODO: inefficient code. Ideas to improve this: for each topic, first create a np array of length of rows, then iterate\n    # over those indices setting the scores with the rest default to 0, then set the whole df col\n    for topic_num in range(0, len(topic_model.get_topics())):\n        df['topic_' + str(topic_num)] = 999999.9\n\n    for topic_num in range(0, len(topic_model.get_topics())):\n        for ind, score in topic_model.get_top_docs(topic=topic_num, n_docs=9999999, sort_by='log_prob'):\n            df['topic_' + str(topic_num)].iloc[ind] = score\n\n    # finally save the dataframe so we can load it quicker in situations where we just want to interact with the results.\n\n    pkl.dump(df, open(final_df_filename, \"wb\"))","3ded20d2":"# because we are doing our own tokenization, we use a dummy function to bypass\ndef dummy_fun(doc):\n    return doc\n\ntfidf = TfidfVectorizer(\n    analyzer='word',\n    tokenizer=dummy_fun,\n    preprocessor=dummy_fun,\n    token_pattern=None)\n\ntfidf_docs = df['clean_tfidf'].tolist()\ntfidf_matrix = tfidf.fit_transform(tfidf_docs)","69cc3830":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n@interact\ndef search_articles(\n    query='cruise ship spread rate',\n    topic=topic_list,\n    topic_threshold=(-20, 0, 0.01)\n):\n    clean_query_words = remove_non_dict_words(get_ngram_words(clean(query)))\n    query_vector = tfidf.transform([clean_query_words])\n    \n    scores = cosine_similarity(query_vector, tfidf_matrix)[0]\n    \n    df['cosine_dist'] = scores\n\n    # these are the ordered search results according to TF-IDF\n\n    # smaller corex_topic scores means more likely to be of that topic\n    corex_cols = [col for col in df if col.startswith('topic_')]\n    select_cols = ['title', 'abstract', 'authors', 'cosine_dist'] + corex_cols\n    \n    results = df[select_cols].loc[df[topic.split(':')[0]] > topic_threshold].sort_values(by=['cosine_dist'], ascending=False).head(10)\n    \n    top_row = results.iloc[0]\n    \n    print('TOP RESULT:\\n')\n    print(top_row['title'] + '\\n')\n    print(top_row['abstract'])\n    \n    print('\\nAUTHORS:\\n')\n    print(top_row['authors'])\n    \n    return results","5e1e27f9":"Load raw data, dedupe publications, and drop any publications that don't have an Abstract. Then create a new column \"document\" that is the \"title\" combined with \"abstract\".","07e10003":"Build dictionary of all words so we can check for both low frequency and high frequency words across entire corpus.","a0ba4251":"# 3. CorEx Topic Modeling\n\nNow train a CorEx model to generate topics for the entire corpus. Then assign topic-liklihood scores to every document. We will use these later to filter the search results by topic.","c04274fa":"# 5. Interactive Search Widget\n\nDIRECTIONS:\n\n 1. Type in your own question in the \"query\" textbox. The default is \"cruise ship spread rate\".\n 2. Select a topic area to filter the result.\n 3. Drag the topic_threshold slider to filter out results that don't strongly align to the topic.\n   * Interestingly, if you lower the threshold slider all the way to -20, with the default query of \"cruise ship spread rate\", the top result is not what you'd expect. Apparently, there's an all-caps acronym SHIP that stands for something not boat related. This is why the CorEx topic thresholds can be useful to filter out these unexpectedly good TF-IDF matches.\n\nHere are a few of the Kaggle-provided questions:\n\n* What is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?\n* What do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\n* What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface?","8095b4ea":"Check all documents for ngrams and replace them.","1415cba9":"Define stopwords that will be removed from all text. For example: \"and\", \"it\", \"the\", \"thus\".","e9c78d8d":"# 2. Clean Data\n\nEach clean function can be tested independently.","1b3d4a45":"Wrapper function that calls all the previously-defined functions.","9940c7e6":"Identify the most common words. If you identify some words as low-value, add them to the stopwords list above and rerun the cells until this point. Repeat until you are happy with the words that are left.","0bd877da":"Dedupe sentences within a document, just in case the article title is also the first sentence of the abstract, or opening sentence of text_body.","86a6d948":"Identify bigrams and trigrams. Example: frequently, the words \"public\" and \"health\" appear next to each other, therefore it should be considered a bigram of \"public_health\".","65155707":"Lemmatize the sentence to normalize conjucations and pluralization.","c483edb0":"Iterate through all documents and assign CorEx topic liklihood scores.","eb3b0e6a":"Loop through all rows of the dataframe and apply the clean function.","acb2698b":"Display all the bigrams and trigrams","f582e11a":"# 0. Methodology\n \n 1. **Load data**\n   * Remove duplicate publications\n   * For each row of data, combine the Title and Abstract into the \"document\" that is used for all further processing and analysis.\n 2. **Clean text** - the goal is to simplify the text as much as possible. This means removing things that seem useful to us humans, like numbers, verb conjugations, and special characters, but that don't provide much signal to the machine\n   * Replace newlines with spaces\n     * except if a newline is preceded by a hyphen, which indicates that a long word was broken by a line break\n   * Remove non-ASCII characters\n   * Remove numbers\n   * Remove known character patterns that provide no value. Ex: \"[Image: see text]\"\n   * Lemmatize words - reduces all conjugations and pluralizations down to a single word. This reduces the total number of unique words the machine has to process while preserving the essence of the word's meaning \n     * https:\/\/en.wikipedia.org\/wiki\/Lemmatisation\n   * Remove stopwords\n     * Remove common English stopwords like \"and\", \"it\", \"the\" and many others\n     * Remove any custom stopwords that don't contribute useful signal\n   * Create ngrams\n     * When 2 tokens frequently appear side-by-side, treat them as the same token.\n     * Example \"public health\" should be treated as a single token. Replace both words with the single token of \"public_health\". The words \"public\" and \"health\" still appear as single tokens, but not when they occur directly next to each other.\n     * When 2 token are joined, that's called a \"bigram\". When 3 tokens are joined, that's called a \"trigram\". In this notebook, I only do bigrams and trigrams, but you could continue joining n number of tokens, which is generally called an \"ngram\".\n   * Dedupe sentences within each document\n     * Just in case the Title is the same as the opening line of the Abstract, dedupe and drop any later sentences that have already appeared.\n     * After all the cleanup above, some sentences may now appear as identical sets of tokens, which implies they don't contribute any additional valuable signal.\n   * Remove extremely rare words and extremely common words\n     * The machine needs many example of how words are used before it can \"understand\" what a word means. For this reason, we must remove words that do not appear very frequently across all documents.\n     * Conversely, words like \"virus\" or \"infection\" appear in a third of all documents, so the machine would treat those words as unhelpful noise. Remove these common words, which leaves us only with \"interesting\" words that help differentiate the documents from one another.\n   * Example of the final processing:\n     * Original text: *\"Cruise ships carry a large number of people in confined spaces with relative homogeneous mixing. On 3 February, 2020, an outbreak of COVID-19 on cruise ship Diamond Princess was reported with 10 initial cases, following an index case on board around 21-25 January. By 4 February, public health measures such as removal and isolation of ill passengers and quarantine of non-ill passengers were implemented.\"*\n     * Result seen by the machine: *['cruise', 'ship', 'carry', 'large_number', 'people', 'confine', 'space', 'relative', 'homogeneous', 'mix', 'february', 'outbreak', 'covid', 'cruise', 'ship', 'report', 'initial', 'case', 'follow', 'index', 'case', 'board', 'around', 'january', 'february', 'public_health', 'measure', 'removal', 'isolation', 'ill', 'passenger', 'quarantine', 'passenger', 'implement']*\n 3. **Use CorEx to assign topic-liklihood scores to each document**\n   * This allows us to filter search results that somehow have a strong cosine distance score, but that are not relevant to the topic of the question.\n 4. **Train TF-IDF model**\n   * https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf\n   * I first attempted to train a Doc2Vec model, but the results were weak. Perhaps there wasn't enough training data. I am curious if anyone knows of a finetuned BERT (or other model) based on medical literature?\n   * Once the model is trained, we can convert a document into a numerical vector. This allows us to use cosine distance to measure the similarity between documents.\n   * Any other form of vectorizing text could be used here. If you had success training a Doc2Vec model, or some other model, I'd love to hear about it in the comments!\n 5. **Interactive widget to explore the results**\n \n \n A big thank you to the following resources:\n\n * https:\/\/www.kaggle.com\/morrisb\/ipython-magic-functions\n * https:\/\/www.kaggle.com\/gkaraman\/topic-modeling-lda-on-cord-19-paper-abstracts\n * https:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/\n * https:\/\/towardsdatascience.com\/make-your-own-super-pandas-using-multiproc-1c04f41944a1\n * https:\/\/www.kaggle.com\/dgunning\/browsing-research-papers-with-a-bm25-search-engine","51e0e2b7":"Helper function to replace ngrams.","eb854ef0":"# 1. Load Packages and Raw Data","127b8ce8":"# 4. TF-IDF Vectorize Documents\n\nNow train the TF-IDF model, which uses the \"clean_tfidf\" column as the content to vectorize.","99820c43":"# Interactive Search Engine for Abstracts and Authors\n\nWhen a user enters a question (or we use the Kaggle-provided questions), the top-matching abstracts and authors are returned. I hope this is a useful tool for rapid information retrieval using natural language queries.\n\n**DIRECTIONS: Click the \"Copy and Edit\" button. With \"load_preprocessed_file=True\", run all cells, and scroll to the bottom for the interactive widget. Takes about 90 seconds to fully load.**\n\nYou will see an interactive widget (heavily inspired by this notebook: https:\/\/www.kaggle.com\/dgunning\/browsing-research-papers-with-a-bm25-search-engine):\n\n![](https:\/\/i.imgur.com\/7HHQqqx.jpg)\n\nCode cells have been collapsed to streamline the presentation, but please expand them to dig into the details. My full methodology is described below. I welcome feedback in the comments!","438a7789":"Filter out words that occur in fewer than \"no_below\" documents, or more than \"no_above%\"\" of the documents. Without filtering, we have about 86k unique tokens. With filtering we have about 5800 unique tokens which we consider \"interesting\"."}}