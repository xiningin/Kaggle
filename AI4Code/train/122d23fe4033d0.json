{"cell_type":{"82f3b2d4":"code","ce43080a":"code","6bbced73":"code","b9e1df3b":"code","fb60a7bc":"code","614ea185":"code","71d52fc3":"code","46da96e5":"code","e41de7a3":"code","cab41d04":"code","b03f2e1f":"code","9f4fddda":"code","1ee7bbd3":"code","9940da3f":"code","b70893d9":"code","a15af575":"code","00243595":"code","25d0a400":"code","28230f46":"code","d5a715f1":"code","0b5aea08":"code","fcf0f6b9":"code","9f994392":"code","67edc936":"code","f28459c0":"code","300bf82e":"code","1a1e619e":"code","9f74de8f":"code","5bbe6256":"code","2936b278":"code","79c8ce1c":"code","dd470e41":"code","f47f52f0":"code","d9f10b33":"code","62b8ecc9":"code","5395bd8f":"code","65faf142":"code","fc3c39d5":"code","d10d5260":"code","70e554d1":"code","8d06f231":"code","5056f1c8":"code","aa62e423":"code","f7e76f9f":"code","9e017548":"code","5972a244":"code","f8bdf188":"markdown","af0b42ff":"markdown","99c29136":"markdown","a7afd4b6":"markdown","b8e65b06":"markdown","a1ccf056":"markdown","9f1b87e4":"markdown","528de749":"markdown","c639459c":"markdown","c1f3155e":"markdown","d261f65b":"markdown","aef49a0f":"markdown","a541feca":"markdown","6dd6b683":"markdown","725cd7df":"markdown","dfde689c":"markdown","07d6736e":"markdown","5032ebec":"markdown","93136ca1":"markdown","cc6ab744":"markdown","f72a96bb":"markdown","4c1fa97b":"markdown","d51f6b87":"markdown","8a1999d6":"markdown","4714c930":"markdown","05803e8e":"markdown","7edca9d0":"markdown","c62882fe":"markdown","0c070d9b":"markdown","52837326":"markdown","f88d5891":"markdown","05a5c253":"markdown","eb96178a":"markdown","349cf86b":"markdown","4cc56514":"markdown","700ed743":"markdown","ead4b746":"markdown","1d811083":"markdown","df431b37":"markdown","3c14e31c":"markdown","e2069407":"markdown","7f67617c":"markdown","3fb78ec7":"markdown","5d0a81a1":"markdown"},"source":{"82f3b2d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# importing stuff\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# a lot of stuff\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Z-score \/ outliers stuff\nfrom scipy import stats\n\n# Rede Neural stuff\nfrom tensorflow.keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import plot_model\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce43080a":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf","6bbced73":"#sns.pairplot(df[['hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)', 'total (R$)']])","b9e1df3b":"import plotly.express as px\n\ndata = df[['property tax (R$)', 'rent amount (R$)']].dropna()\nx = data['property tax (R$)'].values\ny = data['rent amount (R$)'].values\n\nmask1 = np.abs(stats.zscore(x)) < 5\nmask2 = np.abs(stats.zscore(y)) < 5\nmask = np.logical_and(mask1, mask2)\n\nfig = px.scatter(df, x=\"property tax (R$)\", y=\"rent amount (R$)\")\n\nfig.add_shape(type=\"rect\",\n    x0=min(x[mask]), y0=min(y[mask]), x1=max(x[mask]), y1=max(y[mask]),\n    line=dict(color=\"Green\", width=2,),\n    opacity=0.2,\n    fillcolor=\"Green\",\n)\n\nfig.show()","fb60a7bc":"cols = df.columns[:10] # Primeiras 10 colunas\ncolours = ['#00ff1a', '#9e0000'] # Cores do Heatmap, verde tem, vermelhos nao tem\nsns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colours))","614ea185":"for col in df.columns:\n    pct_missing = np.mean(df[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","71d52fc3":"# first create missing indicator for features with missing data\nfor col in df.columns:\n    missing = df[col].isnull()\n    num_missing = np.sum(missing)\n    # Verifico se a coluna possui algum valor nulo\n    if num_missing > 0:  \n        print('created missing indicator for: {}'.format(col))\n        # Se essa coluna possuim algum valor nulo, entao \n        # criamos uma coluna is_missing dela, por exemplo city_ismissing\n        df['{}_ismissing'.format(col)] = missing\n\n\n# then based on the indicator, plot the histogram of missing values\n# agora selecionamos as colunas que criamos no passo anterior\nismissing_cols = [col for col in df.columns if 'ismissing' in col]\n# adicionamos a soma desses dados para uma unica coluna\ndf['num_missing'] = df[ismissing_cols].sum(axis=1)\n# montamos o histograma dele\n# Eixo X - Numero de colunas vazias\n# Eixo Y - Numero de linhas que cont\u00e9m essa carcteristica\ndf['num_missing'].value_counts().reset_index().sort_values(by='index').plot.bar(x='index', y='num_missing')\n# limpando essas colunas\ndf.drop(columns=[col for col in df.columns if 'ismissing' in col], inplace=True)","46da96e5":"# drop rows with a lot of missing values.\nind_missing = df[df['num_missing'] > 3].index\ndf = df.drop(ind_missing, axis=0)\ndf.drop(columns='num_missing', inplace=True)","e41de7a3":"df.dtypes","cab41d04":"columns = ['city', 'animal', 'furniture', 'floor']\n\nfor col in columns:\n    print(df[col].unique())","b03f2e1f":"values = {'animal': 'not acept', 'furniture': 'not furnished', 'floor': 0}\ndf = df.fillna(value=values)\n","9f4fddda":"df['city'].fillna(method='ffill',inplace=True)\ndf['city'].unique()","1ee7bbd3":"df['animal'].replace(to_replace='acept', value=1, inplace=True)\ndf['animal'].replace(to_replace='not acept', value=0, inplace=True)\n\ndf['furniture'].replace(to_replace='furnished', value=1, inplace=True)\ndf['furniture'].replace(to_replace='not furnished', value=0, inplace=True)\n\ndf['floor'].replace(to_replace='-', value=0, inplace=True)\n","9940da3f":"df['city'] = pd.Categorical(df.city)\ndf['city'] = df.city.cat.codes\ndf['city'] = pd.to_numeric(df['city'], downcast='float')\ndf['floor'] = pd.to_numeric(df['floor'], downcast='float')\ndf.dtypes","b70893d9":"df['area'] = df['area'].fillna(df['area'].median())\ndf['hoa (R$)'] = df['hoa (R$)'].fillna(df['hoa (R$)'].median())\ndf['fire insurance (R$)'] = df['fire insurance (R$)'].fillna(df['fire insurance (R$)'].median())\ndf['rent amount (R$)'] = df['rent amount (R$)'].fillna(df['rent amount (R$)'].median())\ndf['property tax (R$)'] = df['property tax (R$)'].fillna(df['property tax (R$)'].median())\n","a15af575":"df['rooms'] = df['rooms'].fillna(df['rooms'].mode()[0])\ndf['bathroom'] = df['bathroom'].fillna(df['bathroom'].mode()[0])\ndf['parking spaces'] = df['parking spaces'].fillna(df['parking spaces'].mode()[0])\n","00243595":"df2 = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\nsns.heatmap(df2.corr())","25d0a400":"#sns.pairplot(df[['city', 'floor', 'animal', 'furniture',  'hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)', 'total (R$)']])","28230f46":"# Vers\u00e3o bem simples de dataset para servir como exemplo e para conseguir rodar o notebook inteiro\n# df.drop(columns='area', inplace=True)\n# df.drop(columns='property tax (R$)', inplace=True)\ndf.drop(columns='city', inplace=True)\n# df.drop(columns='furniture', inplace=True)\n# Aplicando filtro de outlier\nmask = (np.abs(stats.zscore(df.dropna())) < 5).all(axis=1)\ndf = df.dropna()[mask]\ndf\n#df.dropna(inplace=True)\n\n# df = aqui vem o dataset processado, SEM ESCALONAMENTO\/NORMALIZADOR\n#print(df)\n# df.describe()","d5a715f1":"# Normalizamos os dados de df em uma escala de [0, 1]\n# Estou fazendo isto aqui pois temos que \"desnormalizar\" na hora de gerar os gr\u00e1ficos de R\u00b2\ncolumn_names = df.columns\nscaler = MinMaxScaler()\nscaler.fit(df)\ndf = scaler.transform(df)\ndf = pd.DataFrame(df)\ndf.columns = column_names\n\n# Pegamos o dataset df e separamos em x (entrada) e y (saida), numa separa\u00e7\u00e3o 70% treino e 30% valida\u00e7\u00e3o\ninput_dim = df.shape[1] - 1\nx = df.drop(columns='total (R$)')\ny = df['total (R$)']\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.30, random_state=42)","0b5aea08":"NEURONIOS_CAMADA_INICIAL = 10\n\n# N\u00famero de camadas intermedi\u00e1rias e neur\u00f4nios. Tamanho do array s\u00e3o os n\u00fameros de camadas, elementos do array s\u00e3o n\u00fameros de neur\u00f4nios.\n# Ex: [30, 15] = 2 camadas intermedi\u00e1rias com 30 neur\u00f4nios na primeira e 15 neur\u00f4nios na segunda\n# Ex: [] = Nenhuma camada intermedi\u00e1ria\n# Ex: [10, 10, 10, 10, 50] = 5 camadas intermedi\u00e1rias, com 10 neur\u00f4nios nas 4 primeiras e 50 neur\u00f4nios na \u00faltima\nNEURONIOS_CAMADAS_INTERMEDIARIAS = [20, 30]\n\n# Usar dropout: True para usar, False para n\u00e3o usar\nUSAR_DROPOUT = False\n\n# Porcentagem de Dropout: valor entre 0 e 1\nDROPOUT_VALUE = 0.2\n\n# Regularizador: None = N\u00e3o usar regularizador, 'l1' = Reg L1, 'l2' = Reg L2\nTIPO_REGULARIZADOR = None\n\n# Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: 'relu', 'tanh', 'sigmoid', 'softmax', 'softplus', 'elu'\nFN_ATIVACAO = 'relu'\n\n# #####################################################################################\n# Defini\u00e7\u00e3o da ARQUITETURA da Rede Neural\nmodel = Sequential()\n\n# Primeira camada da RNA (input_dim entradas)\nmodel.add(Dense(units=NEURONIOS_CAMADA_INICIAL, input_dim=input_dim, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n# Camadas intermedi\u00e1rias\nfor UNITS in NEURONIOS_CAMADAS_INTERMEDIARIAS:\n    model.add(Dense(units=UNITS, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n    if USAR_DROPOUT:\n        model.add(Dropout(DROPOUT_VALUE, input_shape=(120,)))\n# \u00daltima camada da RNA (1 sa\u00edda)\nmodel.add(Dense(units=1, activation=FN_ATIVACAO))\n\n\n# \"Doug, mas que c\u00f3digo tosco!\" Tamb\u00e9m acho... Caso voc\u00ea queira criar sua pr\u00f3pria arquitetura\n# sem usar os par\u00e2metros acima, \u00e9 bem simples. Segue abaixo um exemplo meio doideira:\n# model = Sequential()\n# model.add(Dense(units=30, input_dim=input_dim, activation='relu', kernel_regularizer='l1'))\n# model.add(Dropout(0.4, input_shape=(30,)))\n# model.add(Dense(units=20, activation='tanh', kernel_regularizer='l2'))\n# model.add(Dense(units=20, activation='relu', kernel_regularizer=None))\n# model.add(Dense(units=1, activation='relu'))\n\nplot_model(model, show_shapes=True)","fcf0f6b9":"CALLBACKS = [] # Defini\u00e7\u00e3o dos callbacks a serem utilizados. Isso aqui \u00e9 opcional, mas pode ajudar: https:\/\/keras.io\/api\/callbacks\/early_stopping\/\nLOSS = 'mean_absolute_error' # 'mean_absolute_error', 'mean_squared_error'\nBATCH_SIZE = 128\nEPOCHS = 50\nOPTIMIZER = 'adam' # 'adam' \u00e9 o mais utilizado. Caso prefira outro, como 'sgd', boa sorte!\n\n# Compila\u00e7\u00e3o do modelo + Defini\u00e7\u00e3o da Fun\u00e7\u00e3o de Loss e do Otimizador\nmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=LOSS)\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=CALLBACKS,\n    validation_data=(x_valid, y_valid),\n)\n","9f994392":"\ndf_valid_scaled = np.column_stack((x_valid, y_valid))\ndf_valid = scaler.inverse_transform(df_valid_scaled)\ny_true = y_valid\n\n\ny_pred = model.predict(x_valid)\npred_df = pd.concat([pd.DataFrame(x_valid).reset_index(drop=True), pd.DataFrame(y_pred)], axis=1)\npred_df = scaler.inverse_transform(pred_df)\npred_df = pd.DataFrame(pred_df)\npred_df.columns = df.columns\npred_df\n\nr2 = r2_score(y_true, y_pred)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('model loss | \"Quantidade de dados e colunas usadas: {}'.format(df.shape))\naxes[0].set_ylabel('loss')\naxes[0].set_xlabel('epoch')\naxes[0].legend(['train', 'val'], loc='upper left')\n\naxes[1].scatter(x=df_valid[:, -1], y=pred_df['total (R$)'])\n# axes[0].plot(history.history['val_loss'])\naxes[1].set_title('R\u00b2 = {}'.format(r2))\naxes[1].set_ylabel('y_pred')\naxes[1].set_xlabel('y_true')\n\nprint(\"Quantidade de dados e colunas usadas: {}\".format(df.shape))\n\nplt.show()","67edc936":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf","f28459c0":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\nsns.heatmap(df.corr())","300bf82e":"df.dtypes","1a1e619e":"df['floor'].unique()","9f74de8f":"columns = ['city', 'animal', 'furniture']\n\nfor col in columns:\n    print(df[col].unique())","5bbe6256":"# Comentei isso aqui pq demora batante pra gerar a imagem. Se quiser ver, pode descomentar.\nsns.pairplot(df[['hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)', 'total (R$)']])","2936b278":"import plotly.express as px\n\ndata = df[['property tax (R$)', 'rent amount (R$)']].dropna()\nx = data['property tax (R$)'].values\ny = data['rent amount (R$)'].values\n\nmask1 = np.abs(stats.zscore(x)) < 15\nmask2 = np.abs(stats.zscore(y)) < 15\nmask = np.logical_and(mask1, mask2)\n\nfig = px.scatter(df, x=\"property tax (R$)\", y=\"rent amount (R$)\")\n\nfig.add_shape(type=\"rect\",\n    x0=min(x[mask]), y0=min(y[mask]), x1=max(x[mask]), y1=max(y[mask]),\n    line=dict(color=\"Green\", width=2,),\n    opacity=0.2,\n    fillcolor=\"Green\",\n)\n\nfig.show()","79c8ce1c":"columns = ['area', 'hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)', 'total (R$)']\n\nfig, axes = plt.subplots(nrows=len(columns), ncols=2, figsize=(15,30))\n\nfor i, col in enumerate(columns):\n    # Plotamos o histograma na esquerda\n    axes[i][0].hist(df[col], bins=50)\n    \n    # Obtemos a mascara booleana de poss\u00edveis outliers (de acordo com o valor de Z)\n    mask = np.abs(stats.zscore(df[col].dropna())) < 3\n    dado_filtrado = df[col].dropna()[mask]\n    \n    # Plotamops o histograma na direita, j\u00e1 filtrado\n    axes[i][1].hist(dado_filtrado)\n    \n    # Aproveitamos para plotar no esquerda, onde n\u00f3s \"cortamos\" o histograma, para gerar o hist da direita\n    ymax = axes[i][0].get_yticks()[-1]\n    axes[i][0].vlines(x=max(dado_filtrado), ymin=0, ymax=ymax, color=\"red\")\n    \n    # Danos nomes aos gr\u00e1ficos\n    axes[i][0].set_title(\"{} - sem filtro de outliers\".format(col))\n    axes[i][1].set_title(\"{} - com filtro de outliers\".format(col))","dd470e41":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\ncolumns = ['city', 'animal', 'furniture']\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n\ndf['city'].value_counts().plot.barh(ax=axes[0])\ndf['animal'].value_counts().plot.barh(ax=axes[1])\ndf['furniture'].value_counts().plot.barh(ax=axes[2])\n\naxes[0].set_title(\"city\")\naxes[1].set_title(\"animal\")\naxes[2].set_title(\"furniture\")\nplt.tight_layout()\n","f47f52f0":"# Para os numeros discretos:\ncolumns = ['rooms', 'bathroom', 'parking spaces', 'floor']\n\ndf_aux = df.copy()\ndf_aux['floor'].replace(to_replace='-', value=0, inplace=True)\ndf_aux['floor'] = pd.to_numeric(df_aux['floor'].values)\n\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(18, 5))\n\nfor i, col in enumerate(columns):\n    axes[i].hist(df_aux[col], bins=df_aux[col].nunique())\n    axes[i].set_title(col)","d9f10b33":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\ndf.isna().sum().plot.barh()\nplt.show()","62b8ecc9":"df.isna().sum(axis=1).value_counts().plot.barh()","5395bd8f":"df_original = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf = df_original.copy()\n# Faremos agora algumas imputa\u00e7\u00f5es de valores ausentes usando regress\u00e3o\n# Para CRIAR o IMPUTADOR, vamos utilizar dados LIMPOS. Ou seja, sem dados ausentes e sem outliers.\ncolumns = ['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']\ndf_regress = df_original[columns].copy()\n\nfor col in columns:\n    value = df_original[col].median()\n    df_regress[col].fillna(value=value, inplace=True)\n\nmask = (np.abs(stats.zscore(df_regress)) < 3).all(axis=1)\ndf_regress = df_regress[mask]\n\ndf_regress.describe()","65faf142":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# Agora criaremos o regressor com os dados limpos\n# Criamos um objeto que far\u00e1 a Imputa\u00e7\u00e3o por Regress\u00e3o\nimp_mean = IterativeImputer(random_state=0)\n# Treinamos a regress\u00e3o com os dados disponiveis\nimp_mean.fit(df_regress.values)\n\n# Agora, iremos pegar o dataset \"sujo\" e imputar valores nulos com o regressor \"limpo\"\nX = df_original[['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']].values\nregr_output = imp_mean.transform(X)\n\nfoo = pd.DataFrame(regr_output)\nfoo.columns = df_regress.columns\n\nfor col in foo.columns:\n    valores_menor_0 = (foo[col]<0).sum()\n    print(\"Existem {} dados com valor < 0 em '{}'\".format(valores_menor_0, col))\n\ndf[columns] = foo[mask]\n    \nfoo[mask].describe()","fc3c39d5":"df_original = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf = df_original.copy()\n# Faremos agora algumas imputa\u00e7\u00f5es de valores ausentes usando regress\u00e3o\n\ncolumns = ['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']\ndf_regress = df_original[columns].copy()\ndf_regress.dropna(inplace=True)\n\ndf_regress.describe()","d10d5260":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# Agora criaremos o regressor com os dados limpos\n# Criamos um objeto que far\u00e1 a Imputa\u00e7\u00e3o por Regress\u00e3o\nimp_mean = IterativeImputer(random_state=0)\n# Treinamos a regress\u00e3o com os dados disponiveis\nimp_mean.fit(df_regress.values)\n\n# Agora, iremos pegar o dataset \"sujo\" e imputar valores nulos com o regressor \"limpo\"\nX = df_original[['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']].values\nregr_output = imp_mean.transform(X)\n\nfoo = pd.DataFrame(regr_output)\nfoo.columns = df_regress.columns\n\nfor col in foo.columns:\n    valores_menor_0 = (foo[col]<0).sum()\n    print(\"Existem {} dados com valor < 0 em '{}'\".format(valores_menor_0, col))\n\ndf[columns] = foo[mask]\n    \nfoo[mask].describe()","70e554d1":"df_original = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf = df_original.copy()\n\n# Faremos agora algumas imputa\u00e7\u00f5es de valores ausentes usando regress\u00e3o\ncolumns = ['rooms', 'bathroom', 'parking spaces', 'rent amount (R$)']\ndf_regress = df_original[columns].copy()\n\nfor col in columns:\n    value = df_original[col].median()\n    df_regress[col].fillna(value=value, inplace=True)\n\nmask = (np.abs(stats.zscore(df_regress)) < 3).all(axis=1)\ndf_regress = df_regress[mask]\n\n# Criamos um objeto que far\u00e1 a Imputa\u00e7\u00e3o por Regress\u00e3o\nimp_mean = IterativeImputer(random_state=0)\n# Treinamos a regress\u00e3o com os dados disponiveis\nimp_mean.fit(df_regress.values)\n\n# Agora, iremos pegar o dataset \"sujo\" e imputar valores nulos com o regressor\nX = df_original[columns].values\nregr_output = np.round(imp_mean.transform(X))\n\nfoo = pd.DataFrame(regr_output)\nfoo.columns = df_regress.columns\n\nfor col in ['rooms', 'bathroom', 'parking spaces']:\n    print(\"{}: {}\".format(col, foo[col].unique()))\n    \n\ndf[columns] = foo[mask]\n\nfoo[mask].describe()","8d06f231":"df_antes = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf_depois = df_antes.copy()\n\n# Substituindo dados ausentes pela categoria com maior frequ\u00eancia\ndf_depois['animal'].fillna(value=df_depois['animal'].mode()[0], inplace=True)\n\n# Substituindo dados ausentes por \"Indefinido\"\ndf_depois['furniture'].fillna(value=\"Indefinido\", inplace=True)\n\n# Substituindo dados ausentes pelo pr\u00f3ximo dado presente\ndf_depois.fillna(method='ffill', inplace=True)\n\nfig, axes = plt.subplots(3, 1, figsize=(15, 10))\n\nfor i, col in enumerate(['city', 'animal', 'furniture']):\n    axes[i].barh(width=df_depois[col].value_counts(), y=df_depois[col].unique(), label='depois')\n    axes[i].barh(width=df_antes[col].value_counts(), y=df_antes[col].dropna().unique(), label='antes')\n    axes[i].legend()\n    \naxes[0].set_title(\"Imputa\u00e7\u00e3o copiando o dado anterior. Resultado proporcional\", fontsize=18)\naxes[1].set_title(\"Imputa\u00e7\u00e3o usando a moda. Aumenta a despropor\u00e7\u00e3o\", fontsize=18)\naxes[2].set_title(\"Cria\u00e7\u00e3o de uma nova classe\", fontsize=18)\nplt.tight_layout()\nplt.show()","5056f1c8":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\ndf['animal'].replace(to_replace='acept', value=1, inplace=True)\ndf['animal'].replace(to_replace='not acept', value=0, inplace=True)\n\ndf['furniture'].replace(to_replace='furnished', value=1, inplace=True)\ndf['furniture'].replace(to_replace='not furnished', value=0, inplace=True)\n\ndf","aa62e423":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\n# OneHot Encoding\ndf = df[df['city'].notna()]\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(df[['city']])\nenc_df = pd.DataFrame(enc.transform(df[['city']]).toarray(), columns=enc.get_feature_names(['city']))\ndf.reset_index(drop=True, inplace=True)\nenc_df.reset_index(drop=True, inplace=True)\ndf = df.join(enc_df)\n\ndf","f7e76f9f":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\ndf['Feature Nova 1'] = df['area'] ** 2\ndf['Feature Nova 2'] = df['fire insurance (R$)']\/df['rent amount (R$)']\ndf['Feature Nova 3'] = df['rooms'] * df['bathroom']\n\ndf","9e017548":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\ndf.dropna(inplace=True)\ndf['floor'].replace(to_replace='-', value=0, inplace=True)\ndf['floor'] = pd.to_numeric(df['floor'].values)\n\ncolumns = ['area', 'rooms', 'bathroom', 'parking spaces', 'floor', 'hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)']\nx = df[columns]\ny = df['total (R$)']\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_regression\n\n# k \u00e9 o numero de features que N\u00c3O ser\u00e3o jogadas foras. Vamos primeiro ver os resultados, depois eliminar alguma feature.\nk = x.shape[1]\n# Utilizamos um m\u00e9todo do sklearn para isso, usando a estrat\u00e9gia Chi Squared.\nselector = SelectKBest(f_regression, k=k)\nx_new = selector.fit_transform(x, y)","5972a244":"# Utilizo o log10 pois os valores s\u00e3o ou muito grandes, ou muito pequenos\nscores = -np.log10(selector.pvalues_)\n\nx_plot = list(range(len(scores)))\n\nfig, ax = plt.subplots(figsize=(8, 4))\nplt.bar(x_plot, scores)\nax.set_title(\"Score do m\u00e9todo f-regression para Feature Selection\")\nax.set_xticks(x_plot)\nax.set_xticklabels(columns, rotation=45)\nplt.show()","f8bdf188":"# Vari\u00e1veis ausentes\n\n#### Vamos plotar o n\u00famero de dados ausentes por coluna em um Gr\u00e1fico de Barras","af0b42ff":"# Visualiza\u00e7\u00e3o dos resultados","99c29136":"# An\u00e1lise de correla\u00e7\u00e3o entre as vari\u00e1veis\n#### Pode ser interessante na hora de imputa\u00e7\u00e3o","a7afd4b6":"#### Identifiquei as colunas que nao possuem valores n\u00famericos\n\n* city\n* animal \n* furniture\n* floor\n\nAgora analise dos valores unicos para estas colunas\n","b8e65b06":"# Lembrando:\n\n## **Data de entrega:**\n#### - 21\/06\/2021 (segunda-feira)\n\n## **O que ser\u00e1 avaliado de forma objetiva:**\n#### - Se o modelo est\u00e1 ajustado (sem overfit\/underfit). Ou seja, se as curvas de treino (azul) e valida\u00e7\u00e3o (laranja) est\u00e3o suficientemente pr\u00f3ximos.\n#### - A quantidade de dados (linhas de df) e features (colunas de df) foram utilizados.\n#### - O valor de R\u00b2\n\n## **O que ser\u00e1 avaliado de forma subjetiva:**\n#### - Esfor\u00e7o. Ou seja, quando mais explorar as combina\u00e7\u00f5es de pre-processamento + par\u00e2metros de rede, melhor. Para isso, crie novas vers\u00f5es do notebook.\n\n## Observa\u00e7\u00f5es\n#### - O valor de R ser\u00e1 analisado juntamente com o n\u00famero de features e dados usados. Ou seja, um R alto obtido usando poucos dados e features n\u00e3o vale muita coisa. Mas \u00e9 melhor que nada ;)\n#### - Se deixou de utilizar colunas do df = menos pontos. Se fez engenharia de features = mais pontos (pouco). Isto serve para incentivar a utilizar os dados categ\u00f3ricos.\n\n## Dicas:\n#### - Quando alcan\u00e7ar um resultado que queira salvar, clique em Save Version -> Quick Save. Isto far\u00e1 com que o seus resultados atuais sejam mostrados na vers\u00e3o html. Se clicar em Save Version -> Save & Run All (commit), o notebook ser\u00e1 rodado novamente, e isso pode modificar os resultados obtidos.\n#### - No come\u00e7o, KISS (Keep it simple, silly). Comece de forma simples. Comece com menos features e com pouco pre processamento. Recomendo come\u00e7ar usando vari\u00e1veis num\u00e9ricas e filtro de outliers. Isso j\u00e1 deve ser suficiente para obter um R alto. MAS ISSO N\u00c3O \u00c9 SUFICIENTE! Depois que conseguir um modelo com alto R usando um modelo e pre processamento simples, salve a vers\u00e3o do notebook e continue explorando! A meta \u00e9 conseguir utilizar TODAS as features e o maior n\u00famero de dados poss\u00edvel. O objetivo \u00e9 explorar a combina\u00e7\u00e3o de pr\u00e9-processamento de dados + par\u00e2metros da rede neural.\n#### - Eu vou analisar as diferentes vers\u00f5es do notebook (se houver). Ou seja, se na primeira vers\u00e3o obteve um modelo bom usando poucas features, ok... Se na segunda vers\u00e3o obteve um modelo um pouco pior ou melhor usando mais features, melhor ainda. Explore as combina\u00e7\u00f5es! You're a wizard Harry!","a1ccf056":"#### V\u00e1rias estrat\u00e9gias podem ser usadas. Aqui, usaremos uma regress\u00e3o, mas iremos arredondar os n\u00fameros gerados para continuarem discretos.\n\n#### Para esta imputa\u00e7\u00e3o, usaremos 'rooms', 'bathroom' e 'parking spaces' que s\u00e3o valores discretos mas usaremos tamb\u00e9m 'rent amount' pois todas essas vari\u00e1veis tem UM ALTO VALOR DE CORRELA\u00c7\u00c3O, e isso \u00e9 importante na constru\u00e7\u00e3o do regressor.","9f1b87e4":"# Separa\u00e7\u00e3o dos dados","528de749":"#### Primeiro vamos tratar os dados para criar um regressor com dados \"limpos\". Com base nesse regressor \"limpo\", vamos fazer uma imputa\u00e7\u00e3o dos dados ausentes do dataframe. Vamos tamb\u00e9m comparar o resultado obtido com o resultado da imputa\u00e7\u00e3o usando um regressor \"sujo\", gerado a partir de dados n\u00e3o tratados.\n\n### Imputa\u00e7\u00e3o com dados tratados:","c639459c":"# Exemplo de imputa\u00e7\u00e3o dos valores ausentes das vari\u00e1veis cont\u00ednuas:\n#### 'hoa', 'rent amount', 'property tax', 'fire insurance', 'total'","c1f3155e":"Para a coluna cidade, vou atribuir o valor anterior\n\n##### Imputing using ffil\n(We can see that all missing values have been filled with the last observed values)\n\n##### Imputing using bfil\n(We can see that all missing values have been filled with the next observed values.)","d261f65b":"#### Vamos aproveitar tamb\u00e9m e verificar os valores de 'city', 'animal' e 'furniture'","aef49a0f":"# Dataset Original","a541feca":"### Vamos usar o PairPlot do Seaborn para tra\u00e7ar o conjunto de histogramas e scatterplots das vari\u00e1veis cont\u00ednuas:\n#### 'hoa', 'rent amount', 'property tax', 'fire insurance', 'total'","6dd6b683":"#### Vamos plotar os Histogramas dessas vari\u00e1veis cont\u00ednuas sem filtros e com filtros de outliers (Z-Score com z=3)","725cd7df":"Vou remover os dados que tem mais de 3 colunas nulas, pois sao poucos e acredito que podem afetar nas predi\u00e7\u00f5es","dfde689c":"# Distribui\u00e7\u00e3o das classes e dos n\u00fameros discretos","07d6736e":"Com esse heatmap acima,vou remover a coluna da area e property tax, porque nao vejo muita correlacao\n","5032ebec":"## Nesta parte, irei fazer uma analise e limpeza dos dados:\n\nAl\u00e9m dos slides e dos exemplos neste notebook, vou utilizar as seguintes fontes:\n* [Data cleaning in python: The ultimate guide](https:\/\/towardsdatascience.com\/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d)\n* [A Guide to Handling Missing values](https:\/\/www.kaggle.com\/parulpandey\/a-guide-to-handling-missing-values-in-python)\n","93136ca1":"* Para as demais colunas numericas, como area, hoa, property tax, fire insurance vou preencher com a mediana\n* Para as colunas com rooms, bathroom, parking spaces vou preencher com a moda","cc6ab744":"Para o valor - da coluna floor, vou atribuir valor 0, para o valor nan da oluna furniture, vou atribuir o valor not furnished, para o valor nan da coluna animal, vou atribuir o valor not acept.\n\nSendo assim, vou sempre tentar atribuir valores negativos para esse caso","f72a96bb":"# Par\u00e2metros de otimiza\u00e7\u00e3o da Rede Neural\n#### Pode alterar os valores das vari\u00e1veis que est\u00e3o EM CAPSLOCK","4c1fa97b":"#### A maioria das LINHAS possuem 1 dado ausente. Boa parte (cerca de 2900, segundo o gr\u00e1fico) n\u00e3o cont\u00e9m NENHUM dado ausente. Outra boa parte dos dados possuem 2 dados ausentes.\n\n#### Com base nisso, existem algumas perguntas:\n - Quais vari\u00e1veis valem a pena imputar os dados ausentes?\n - Quais vari\u00e1veis vale a pena deletar os dados ausentes?\n - Se formos deletar linhas com vari\u00e1veis ausentes, qual seria nossa estrat\u00e9gia? Deletamos linhas com mais de 3 var ausentes ou todas?\n \n#### Existem v\u00e1rias respostas, e todas elas tem pr\u00f3s e contras e dependendem do dataset e do contexto de modelagem.","d51f6b87":"# Exemplo de imputa\u00e7\u00e3o dos valores ausentes de vari\u00e1veis categ\u00f3ricas:\n#### 'city', 'animal', 'furniture'","8a1999d6":"#### \u00c9 poss\u00edvel remover estas vari\u00e1veis outliers conforme indica o gr\u00e1fico. Al\u00e9m disso, o valor de Z \u00e9 ajust\u00e1vel: consideramos z=3, mas caso queira mais rigor, pode diminuir o valor de z ou aument\u00e1-lo, caso queira um maior relaxamento na remo\u00e7\u00e3o de outliers.","4714c930":"# Trabalho Final de Est\u00e1gio Docente\n## Estagi\u00e1rio: Douglas Macedo Sgrott\n## Aluno: Filipe da Silva de Oliveira\n## Data de entrega: 21\/06\/2021 (segunda-feira)\n## O trabalho est\u00e1 organizado em partes:\n - ### **Dataset: Onde voc\u00ea ir\u00e1 limpar e pre processar o dataset. Atribua a vers\u00e3o final do dataset em um dataframe chamado df.**\n - Separa\u00e7\u00e3o dos dados: Aqui os dados s\u00e3o normalizados e divididos em Treino\/Valida\u00e7\u00e3o. N\u00e3o precisa modificar o c\u00f3digo.\n - ### **Arquitetura da Rede Neural: Onde voc\u00ea vai definir a arquitetura da rede neural.**\n - ### **Par\u00e2metros de otimiza\u00e7\u00e3o da Rede Neural: Onde voc\u00ea vai definir outros par\u00e2metros da rede neural.**\n - Visualiza\u00e7\u00e3o dos resultados: Onde os resultados s\u00e3o obtidos\n - Exemplos: Servir como exemplo de an\u00e1lise, data cleaning e pr\u00e9-processamento.\n","05803e8e":"#### Cerca de 10% dos dados de cada COLUNA est\u00e3o ausentes.\n#### Vamos verificar a aus\u00eancia de dados por LINHA:","7edca9d0":"#### Olhando os valores presentes em 'floor', percebemos que existe o caract\u00e9r '-' presente. Al\u00e9m disso, n\u00e3o existem valores iguais a 0. Temos algumas possibilidades aqui:\n - Considerar '-' como 0 e fazer esta substitui\u00e7\u00e3o (onde '-' significaria andar 0 ou t\u00e9rreo)\n - Deletar todas as linhas com floor igual a '-'","c62882fe":"#### Como estamos falando de n\u00famero de vagas de estacionamentos, quartos e banheiros, o esperado \u00e9 que a maior parte dos dados esteja nos valores pequenos, formando uma distribui\u00e7\u00e3o assim\u00e9trica. Ainda assim, existem alguns casos... at\u00edpicos, como por exemplo um im\u00f3vel com 12 quartos. Isso \u00e9 um outlier? Depende! Se for um hotel, 12 quartos pode ser considerado pouco.","0c070d9b":"#### \u00c9 percept\u00edvel que existem outliers. Vamos tra\u00e7ar um Scatter Plot de 2 destas vari\u00e1veis usando Plotly para criar um gr\u00e1fico interativo para analisar um pouco melhor alguns desses outliers. Vamos tamb\u00e9m aplicar o Z-Score com z=3 para ter uma no\u00e7\u00e3o da faixa dos valores dos outliers.","52837326":"#### Aparentemente sao poucos, logo vou pegar a porcentagem de dados invalidos por coluna","f88d5891":"# Arquitetura da Rede Neural\n#### Criei um c\u00f3digo bem simples pra permitir criar diferentes redes neurais modificando apenas algumas vari\u00e1veis (EM CAPSLOCK),\n#### Mas se quiser criar sua pr\u00f3pria arquitetura mais customizada, fique a vontade","05a5c253":"- Podemos substituir as vari\u00e1veis ausentes pela moda dos valores (o n\u00famero que mais se repete)\n- Podemos substituir o dado ausente pelo dado anterior\/seguinte\n- Podemos criar uma nova categoria \"Indefinido\".","eb96178a":"# Dataset\n#### Coloque aqui seu data cleaning e seu pre-processamento e atribua o dataset para um dataframe chamado **df**","349cf86b":"# Exemplo de imputa\u00e7\u00e3o dos valores ausentes das vari\u00e1veis discretas:\n#### 'parking spaces', 'rooms', 'bathroom'","4cc56514":"### Imputa\u00e7\u00e3o com dados n\u00e3o tratados:","700ed743":"# Exemplo de Feature Engineering (bem simples)\n#### Criar novas features a partir de features j\u00e1 existentes\n#### Obs: N\u00e3o \u00e9 obrigat\u00f3rio criar novas Features neste trabalho","ead4b746":"# An\u00e1lise de caract\u00e9res especiais","1d811083":"#### \u00c9 esperado que 'city', 'animal' e 'furniture'sejam considerados como tipo 'object', pois nestas colunas existem valores do tipo string. No entanto, 'floor' deveria ser considerado float64. Por que 'floor' est\u00e1 sendo considerado como object?","df431b37":"# FIM DO TRABALHO \/\\\n# **INICIO DOS EXEMPLOS V**\n","3c14e31c":"### Utilizando um grafico de Heat Map, vou identificar a quantidade de dados invalidos deste data set\n","e2069407":"# Feature Selection\n#### \u00datil para decidir qual feature seria mais interessante de descartar. Estou inserindo apenas para fins de completude.","7f67617c":"# An\u00e1lise de Outliers","3fb78ec7":"# Exemplo Feature Encoding","5d0a81a1":"#### Missing Data Histogram\n\nO histograma a seguir mostra a relacao da quantidade de linhas que possuem X colunas nulas.\nOnde o eixo X simboliza a quantidade de colunas nulas e o eixo Y mostra a quantidade de linhas que possuem essa caracteristica.\n\nPor exemplo, h\u00e1 perto de 3000 linhas sem nenhuma coluna nula e n\u00e3o h\u00e1 nenhuma linha com 6 colunas nulas"}}