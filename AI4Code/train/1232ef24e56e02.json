{"cell_type":{"20c03c3c":"code","97f393d5":"code","6156200a":"code","e9f35d38":"code","7f33721d":"code","b3caa3fc":"code","ffe3a960":"code","47ccd894":"code","dd62a43e":"code","67ddc8f2":"code","8d3e7eae":"code","e8f46f87":"code","a132b39b":"code","98a9d4da":"code","41ea0049":"code","370f95bd":"code","8372320e":"code","705dc169":"code","ec6abed8":"code","884a6659":"code","c8386ffc":"code","5abcffc3":"code","1a04ad83":"code","ce748a98":"markdown","09668da0":"markdown","de967f33":"markdown","9bb545eb":"markdown"},"source":{"20c03c3c":"# read data, review dimensions\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('\/kaggle\/input\/the-history-of-baseball\/team.csv')\n\ndf[['rank', 'g', 'w', 'l', 'r', 'ra']].describe()\ndf.info()\ndf[df[\"g\"]==6]\ndf[df[\"g\"]==165]\ndf.head()\n\n# observations\n#  - huge range of games\n#     - min 6 games for the Baltimore Marylands in 1873 (https:\/\/en.wikipedia.org\/wiki\/1873_Baltimore_Marylands_season)\n#     - max 165 games for the LA Dodgers and SF Giants in 1962 (https:\/\/en.wikipedia.org\/wiki\/1962_National_League_tie-breaker_series)\n#     - need to adjust to per-game metrics and perhaps remove old data (World Series era, modern era?)\n#  - null values\n#     - need to update div_win, wc_win, lg_win, ws_win, so, sb, cs, hbp, sf\n#     - need to drop old pre-World Series 1873-1902 data (https:\/\/en.wikipedia.org\/wiki\/World_Series#Modern_World_Series_(1903%E2%80%93present))","97f393d5":"# remove pre-1903 teams\ndf = df[df.year >= 1903]\ndf.head()\n# df.info()","6156200a":"# fill null values\ndf[\"league_id\"].fillna(\"*None\", inplace=True)\ndf[\"div_id\"].fillna(\"*None\", inplace=True)\ndf[\"ghome\"].fillna(0, inplace=True)\ndf[\"div_win\"].fillna(\"N\", inplace=True)\ndf[\"wc_win\"].fillna(\"N\", inplace=True)\ndf[\"lg_win\"].fillna(\"N\", inplace=True)\ndf[\"ws_win\"].fillna(\"N\", inplace=True)\ndf[\"so\"].fillna(0, inplace=True)\ndf[\"sb\"].fillna(0, inplace=True)\ndf[\"cs\"].fillna(0, inplace=True)\ndf[\"hbp\"].fillna(0, inplace=True)\ndf[\"sf\"].fillna(0, inplace=True)\ndf[\"park\"].fillna(\"*Unknown\", inplace=True)\n\n# impute attendance -- use median attendance for the given year\nnull_attendance_years = np.unique(df[df[\"attendance\"]!=df[\"attendance\"]][\"year\"]) # years with missing attendance data\nfor y in null_attendance_years:\n    median_attendance_for_year = df[df[\"year\"]==y][\"attendance\"].median(skipna=True)\n    df[\"attendance\"] = np.where( ((df[\"year\"] == y) & (df[\"attendance\"] != df[\"attendance\"]) ), median_attendance_for_year, df[\"attendance\"])\n    \ndf.info()","e9f35d38":"plt.hist(df[\"g\"], bins=50)\nplt.xlabel('Games (season length)')\nplt.ylabel('Number of observations')\nplt.title(\"Histogram of season lengths from 1903-2015\")","7f33721d":"# add slugging percentage\ndf[\"single\"] = df[\"h\"] - df[\"hr\"] - df[\"triple\"] - df[\"double\"]\ndf[\"total_bases\"] = df[\"single\"]+(df[\"double\"]*2) + (df[\"triple\"]*3) + (df[\"hr\"]*4)\ndf[\"slg\"] = df[\"total_bases\"]\/df[\"ab\"]\n\ndf[[\"h\", \"single\", \"double\", \"triple\", \"hr\", \"total_bases\"]].head()\ndf.columns","b3caa3fc":"# obp + ops\ndf[\"hbp\"].fillna(0, inplace = True)\ndf[\"sf\"].fillna(0, inplace = True)\n\ndf[\"obp\"] = (df[\"h\"] + df[\"bb\"] + df[\"hbp\"]) \/ (df[\"ab\"] + df[\"bb\"] + df[\"hbp\"] + df[\"sf\"])\n\ndf[\"ops\"] = df[\"slg\"] + df[\"obp\"]\n\ndf.head()","ffe3a960":"# spot check a few data points\ndf[df[\"year\"]==1927][\"slg\"].max() # 1927 NY Yankees: https:\/\/www.baseball-reference.com\/leagues\/MLB\/1927-standard-batting.shtml\ndf[df[\"year\"]==2001][\"obp\"].max() # 2001 SEA Mariners: https:\/\/www.baseball-reference.com\/leagues\/MLB\/2001-standard-batting.shtml\ndf[df[\"year\"]==1998][\"ops\"].max() # 1998 NY Yankees: https:\/\/www.baseball-reference.com\/leagues\/MLB\/1998-standard-batting.shtml\n\ndf[[\"slg\",\"obp\",\"ops\"]].describe()","47ccd894":"for col in ['obp', 'slg', 'ops']:\n    plt.hist(df[col])\n    plt.xlabel(col)\n    plt.ylabel('Number of observations')\nplt.title(str(\"Histogram of OBP (blue), SLG (orange), and OPS (green)\"))","dd62a43e":"# checking the math -- min\/max obp\/slg\/ops match: https:\/\/www.baseball-reference.com\/leagues\/MLB\/2012.shtml\ndf[df[\"year\"].eq(2012)][[\"obp\", \"slg\", \"ops\"]].describe()","67ddc8f2":"# add ties\ndf[df[\"w\"] + df[\"l\"] != df[\"g\"]] # years where w + l != g\ndf[\"ties\"] = df[\"g\"] - df[\"w\"] - df[\"l\"]\ndf[df[\"w\"] + df[\"l\"] + df[\"ties\"] != df[\"g\"]] # years where w + l + ties != g\n\n# transform \"counting stats\" (like wins, runs, home runs) to \"rate stats\" (winning pct, runs\/game)\nfor col in ['w', 'l', 'ties', 'r', 'ab', 'h', 'so', 'sb', 'cs', 'sf', 'ra', 'er', 'cg', 'sho', 'sv', 'ipouts', 'ha', 'hra', 'bba', 'soa', 'e', 'dp']:\n    if (col=='w') or (col=='l') or (col=='ties'):\n        col_name = str(col + \"_pct\")\n    else:\n        col_name = str(col + \"_per_game\")\n    df[col_name] = df[col] \/ df[\"g\"]","8d3e7eae":"df[['w_pct', 'l_pct', 'ties_pct', 'r_per_game', 'ra_per_game']].describe()","e8f46f87":"for col in ['w_pct', 'l_pct', 'ties_pct', 'r_per_game', 'ab_per_game', 'h_per_game', 'so_per_game', 'sb_per_game', 'cs_per_game', 'ra_per_game', 'er_per_game', 'cg_per_game', 'sho_per_game', 'sv_per_game', 'ipouts_per_game', 'ha_per_game', 'hra_per_game', 'bba_per_game', 'soa_per_game', 'e_per_game', 'dp_per_game']:\n    plt.hist(df[col], bins=50)\n    plt.xlabel(col)\n    plt.ylabel('Number of observations')\n    plt.title(str(\"Histogram of \" + col))\n    plt.show()","a132b39b":"ws_winners = df[df[\"ws_win\"]==\"Y\"][\"franchise_id\"]\nplt.subplots(figsize=(10,5))\nplt.hist(ws_winners, bins=50)\nplt.title(\"Histogram of WS winning franchises (1903-2015)\")","98a9d4da":"# make playoffs\ndf[\"make_playoffs_rank_1\"] = df[\"rank\"]==1\ndf[\"make_playoffs_wild_card\"] = df[\"wc_win\"]==\"Y\"\ndf[\"make_playoffs_win_division\"] = df[\"div_win\"]==\"Y\"\n\ndf[\"make_playoffs\"] = df[\"make_playoffs_rank_1\"] | df[\"make_playoffs_wild_card\"] | df[\"make_playoffs_win_division\"]\n\n# 1981 strike-shortened season, some playoff qualifiers were considered division winners (from first half) despite not being rank 1\ndf['make_playoffs'] = np.where( ( (df['year'] == 1981) & ((df['franchise_id'] == \"CIN\") | (df['franchise_id'] == \"STL\")) ), False, df[\"make_playoffs\"])\n\n\ndf.query('year == 1981')[[\"franchise_id\",\"make_playoffs\"]]\n","41ea0049":"playoff_qualifiers = df[df[\"make_playoffs\"]==True][\"franchise_id\"]\nplt.subplots(figsize=(15,5))\nplt.hist(playoff_qualifiers, bins=100)\nplt.title(\"Histogram of playoff qualifying franchises (1903-2015)\")","370f95bd":"# number of playoff teams \/ year\nyear_teams_qualifiers = []\nn_teams = []\nn_playoff_qualifiers = []\n# median_season_lengths = []\nyears = range(1903, 2016)\nyear_labels = []\nfor y in years:\n    year_labels.append(\"'\" + str(y)[-2] + str(y)[-1])\n\nfor year in years:\n    teams_in_year = df[df[\"year\"].eq(year)]\n    n_teams_in_year = len(teams_in_year)\n    playoff_qualifiers_in_year = teams_in_year[teams_in_year[\"make_playoffs\"].eq(True)]\n    n_playoff_qualifiers_in_year = len(playoff_qualifiers_in_year)\n    n_teams.append(n_teams_in_year)\n    n_playoff_qualifiers.append(n_playoff_qualifiers_in_year)\n#     median_season_lengths.append(teams_in_year[[\"g\"]].median())\n    \n# print(median_season_lengths)\n\nx = np.arange(len(years))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(40,10))\nrects1 = ax.bar(x - width\/2, n_teams, width, label=\"Overall\")\nrects2 = ax.bar(x + width\/2, n_playoff_qualifiers, width, label=\"Playoff Qualifiers\")\n# rects3 = ax.bar(x + width\/3, median_season_lengths, width, label=\"Season length\")\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Number of Teams')\nax.set_title('Number of MLB teams and playoff qualifiers by year, with season length')\nax.set_xticks(x)\nax.set_xticklabels(year_labels)\nax.legend()\n\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\nautolabel(rects2)\n# autolabel(rects3)\n\nfig.tight_layout()\n\nplt.show()","8372320e":"# h\/t https:\/\/www.kaggle.com\/maglionejm\/linear-regression-moneyball\n# plot suggesting that playoff winners need to win 55-65% of games\nimport seaborn as sns\nsns.lmplot(x=\"w_pct\", y=\"year\", fit_reg=False, hue=\"make_playoffs\", data=df, height=5, aspect=1.5)\nplt.xlabel(\"Winning Percentage\", fontsize = 15)\nplt.ylabel(\"Year\", fontsize = 15)\nplt.axvline(0.575, 0, 1, color = \"Black\", ls = '--')\nplt.show()","705dc169":"n_teams = len(df)\nplayoff_qualifiers = df[df[\"make_playoffs\"] == True]\nn_playoff_qualifiers = len(playoff_qualifiers) # 412\nfor winning_pct in [0.5, 0.55, 0.575, 0.6, 0.65]:\n    n_exceed_win_pct = len(df[df[\"w_pct\"] >= winning_pct]) # 647\n    n_playoff_qualifiers_exceed_win_pct = len(np.where( ( (df['make_playoffs'] == True) & (df['w_pct'] >= winning_pct) ) )[0]) # 368\n    n_win_over_55_miss_playoffs = len(np.where( ( (df['make_playoffs'] == False) & (df['w_pct'] >= winning_pct) ) )[0]) # 279\n\n    print(\"--------\")    \n    print(\"Winning percentage of \" + str(winning_pct) + \":\")\n    print(\"--------\")\n    print(str(n_exceed_win_pct) + \" of \" + str(n_teams) + \" teams won at least \" + str(winning_pct*100)[0:4] + \"% of games in a season (\" + str(n_exceed_win_pct\/n_teams*100)[0:4] + \"%)\")\n    print(str(n_playoff_qualifiers_exceed_win_pct) + \" of those \" + str(n_exceed_win_pct*100)[0:4] + \" teams qualified for the playoffs (\" + str(n_playoff_qualifiers_exceed_win_pct\/n_exceed_win_pct*100)[0:4] + \"%)\")\n    print(str(n_win_over_55_miss_playoffs) + \" teams won at least \" + str(winning_pct*100)[0:4] + \"% of games and still missed the playoffs (\" + str(n_win_over_55_miss_playoffs\/n_exceed_win_pct*100)[0:4] + \"%)\")\n    print(str(n_playoff_qualifiers_exceed_win_pct) + \" out of \" + str(n_playoff_qualifiers*100)[0:4] + \" playoff qualifiers won at least \" + str(winning_pct) + \"% of games to qualify (\" + str(n_playoff_qualifiers_exceed_win_pct\/n_playoff_qualifiers*100)[0:4] + \"%)\")\n    print()","ec6abed8":"import seaborn as sns\n\nplt.subplots(figsize=(15,15))\nnumeric_correlations = df.corr() # correlations between numeric variables\nsns.heatmap(numeric_correlations, xticklabels=1, yticklabels=1)","884a6659":"sns.pairplot(df[[\"w_pct\", \"obp\", \"slg\", \"ops\", \"r_per_game\", \"ra_per_game\", \"make_playoffs\"]], hue=\"make_playoffs\")","c8386ffc":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\n\n# ------------------------------------------------------------------------------------------------\n# Load data and set up three dimensions that could be predictor of a fourth predicted dimension\n# ------------------------------------------------------------------------------------------------\n\ndf = pd.read_csv('\/kaggle\/input\/the-history-of-baseball\/team.csv')\n\ndf = df[df[\"year\"] >= 1903] # remove pre-1903 observations (before consistent playoffs)\ndf = df[df[\"year\"] != 1904] # remove non-playoff season\ndf = df[df[\"year\"] != 1994] # remove non-playoff season\n\n# ------------------------------------\n# add OPS dimension (OBP + SLG)\n# ------------------------------------\ndf[\"hbp\"].fillna(0, inplace = True)\ndf[\"sf\"].fillna(0, inplace = True)\ndf[\"single\"] = df[\"h\"] - df[\"hr\"] - df[\"triple\"] - df[\"double\"]\ndf[\"total_bases\"] = df[\"single\"] + (df[\"double\"] * 2) + (df[\"triple\"] * 3) + (df[\"hr\"] * 4)\ndf[\"slg\"] = df[\"total_bases\"] \/ df[\"ab\"]\ndf[\"obp\"] = (df[\"h\"] + df[\"bb\"] + df[\"hbp\"]) \/ (df[\"ab\"] + df[\"bb\"] + df[\"hbp\"] + df[\"sf\"])\ndf[\"ops\"] = df[\"slg\"] + df[\"obp\"]\n\n# ------------------------------------\n# ------------------------------------\ndf[\"make_playoffs_rank\"] = df[\"rank\"]==1\ndf[\"make_playoffs_wc\"] = df[\"wc_win\"]==\"Y\"\ndf[\"make_playoffs_div\"] = df[\"div_win\"]==\"Y\"\ndf[\"make_playoffs\"] = df[\"make_playoffs_rank\"] | df[\"make_playoffs_wc\"] | df[\"make_playoffs_div\"]\n# (1981 strike-shortened season)\ndf[\"make_playoffs\"] = np.where( ( (df[\"year\"] == 1981) & ((df[\"franchise_id\"] == \"CIN\") | (df[\"franchise_id\"] == \"STL\")) ), False, df[\"make_playoffs\"])\n\n# ------------------------------------\n# convert features to per-game rates and select 3+1 dimensions\n# ------------------------------------\nfor col in ['w', 'l', 'r', 'ab', 'h', 'so', 'sb', 'cs', 'sf', 'ra', 'er', 'cg', 'sho', 'sv', 'ipouts', 'ha', 'hra', 'bba', 'soa', 'e', 'dp']:\n    df[str(col + \"_per_game\")] = df[col] \/ df[\"g\"]\ndimensions = [\"ops\", \"r_per_game\", \"ra_per_game\", \"make_playoffs\"]\npredictor_dimensions = [\"ops\", \"r_per_game\", \"ra_per_game\"]\n_df = df[dimensions]\n\n# ------------------------------------\n# generate pairplot\n# ------------------------------------\nsns.pairplot(_df, hue=\"make_playoffs\")\n\n# ------------------------------------\n# scale data based on standard deviation\n# ------------------------------------\nscaler = StandardScaler()\ndf_scaled = _df.copy()\ndf_scaled[predictor_dimensions] = scaler.fit_transform(_df[predictor_dimensions])\n\n# ------------------------------------------------------------------------------------------------\n# function to generate 3D plots of predictor dimensions\n# ------------------------------------------------------------------------------------------------\ndef generate_3d_plot(x, y, z, title):\n    fig = plt.figure(figsize=(6,6))\n    ax = Axes3D(fig)\n    colors = []\n    for mp in df[\"make_playoffs\"]:\n        if mp==True:\n            colors.append(\"tab:orange\")\n        else:\n            colors.append(\"tab:blue\")\n\n    scatterplot = ax.scatter(x, y, z, s=40, c=colors)\n    ax.set_xlabel(\"OPS (1st eigenvector)\")\n    ax.set_ylabel(\"Runs allowed per game (2nd eigenvector)\")\n    ax.set_zlabel(\"Runs per game (3rd eigenvector)\")\n    ax.set_title(title);\n    plt.savefig(\"scatter_hue\", bbox_inches='tight')\n\n# ------------------------------------------------------------------------------------------------\n# generate 3D plots with **unscaled** and **scaled** data for three dimensions\n# ------------------------------------------------------------------------------------------------\ngenerate_3d_plot(df[\"ops\"], df[\"r_per_game\"], df[\"ra_per_game\"], \"3D plot of predictor dimensions\")\ngenerate_3d_plot(df_scaled[\"ops\"], df_scaled[\"r_per_game\"], df_scaled[\"ra_per_game\"], \"3D plot of *scaled* predictor dimensions\")\n\n# ------------------------------------------------------------------------------------------------\n# **PCA**\n# generate plots with first three PCA dimensions, as in this example:\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/datasets\/plot_iris_dataset.html#sphx-glr-auto-examples-datasets-plot-iris-dataset-py\n# ------------------------------------------------------------------------------------------------\nX_reduced_unscaled = PCA(n_components=3).fit_transform(df[predictor_dimensions])\ngenerate_3d_plot(X_reduced_unscaled[:, 0], X_reduced_unscaled[:, 1], X_reduced_unscaled[:, 2], \"3D plot of unscaled PCA dimensions\")\nX_reduced_scaled = PCA(n_components=3).fit_transform(df_scaled[predictor_dimensions])\ngenerate_3d_plot(X_reduced_scaled[:, 0], X_reduced_scaled[:, 1], X_reduced_scaled[:, 2], \"3D plot of *scaled* PCA dimensions\")\n\n# ------------------------------------------------------------------------------------------------\n# **PCA**\n# importance of feature scaling, as demonstrated here:\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_scaling_importance.html\n# ------------------------------------------------------------------------------------------------\n\nX_train, X_test, y_train, y_test = train_test_split(df[predictor_dimensions], df[\"make_playoffs\"],\n                                                    test_size=0.30,\n                                                    random_state=42)\n\nunscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())\nunscaled_clf.fit(X_train, y_train)\npred_test = unscaled_clf.predict(X_test)\n\n# Fit to data and predict using pipelined scaling, GNB and PCA.\nstd_clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())\nstd_clf.fit(X_train, y_train)\npred_test_std = std_clf.predict(X_test)\n\n# Show prediction accuracies in scaled and unscaled data.\nprint('\\nPrediction accuracy for the normal test dataset with PCA')\nprint('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))\n\nprint('\\nPrediction accuracy for the standardized test dataset with PCA')\nprint('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n\n# Extract PCA from pipeline\npca = unscaled_clf.named_steps['pca']\npca_std = std_clf.named_steps['pca']\n\n# Show principal components\nprint('\\nPC 1 without scaling:\\n', pca.components_[0])\nprint('\\nPC 1 with scaling:\\n', pca_std.components_[0])\nprint('\\nPC 2 without scaling:\\n', pca.components_[1])\nprint('\\nPC 2 with scaling:\\n', pca_std.components_[1])\n\n# Use PCA without and with scale on X_train data for visualization.\nX_train_transformed = pca.transform(X_train)\nscaler = std_clf.named_steps['standardscaler']\nX_train_std_transformed = pca_std.transform(scaler.transform(X_train))\n\n# visualize standardized vs. untouched dataset with PCA performed\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 7))\n\ndef get_label_for_class(val):\n    if val==0:\n        return \"False (failed to qualify for playoffs)\"\n    else:\n        return \"True (qualified for playoffs)\"\n\nfor l, c, m in zip(range(0, 2), ('tab:blue', 'tab:orange'), ('^', 's', 'o')):\n    ax1.scatter(X_train_transformed[y_train == l, 0],\n                X_train_transformed[y_train == l, 1],\n                color=c,\n                label=get_label_for_class(l),\n                alpha=0.5,\n                marker=m)\n\nfor l, c, m in zip(range(0, 2), ('tab:blue', 'tab:orange'), ('^', 's')):\n    ax2.scatter(X_train_std_transformed[y_train == l, 0],\n                X_train_std_transformed[y_train == l, 1],\n                color=c,\n                label=get_label_for_class(l),\n                alpha=0.5,\n                marker=m)\n\nax1.set_title('Training dataset after PCA')\nax2.set_title('Standardized training dataset after PCA')\n\nfor ax in (ax1, ax2):\n    ax.set_xlabel('1st principal component')\n    ax.set_ylabel('2nd principal component')\n    ax.legend(loc='upper right')\n    ax.grid()\n\nplt.tight_layout()\n\nplt.show()","5abcffc3":"# h\/t https:\/\/www.kaggle.com\/michaelmtz20\/feature-selection-for-predicting-wins\nimport pylab as pl\n\nN = 3\n\npca = PCA(n_components=N)\nX_pca = pca.fit_transform(X_train)\nvals = pca.explained_variance_ratio_\nprint(vals)\n\nind = np.arange(N)  # the x locations for the groups\n\npl.figure(figsize=(10, 6), dpi=250)\nax = pl.subplot()\nax.bar(ind, pca.explained_variance_ratio_)\nfor i in range(N):\n    ax.annotate(r\"%d%%\" % (int(vals[i]*100)), (ind[i], vals[i]), va=\"bottom\", ha=\"center\", fontsize=12)\n    ax.annotate(str(\"PC \" + str(i)), (ind[i], vals[i] + 0.05), va=\"bottom\", ha=\"center\", fontsize=12)\n\nax.set_xlabel(\"Principal Component\", fontsize=12)\nax.set_ylabel(\"Variance Explained (%)\", fontsize=12)\nax.set_ylim(0, .80)\npl.title(\"% variance explained by \" + str(N) + \" principal components\")\n","1a04ad83":"# as shown in previous section\nstd_clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())\nstd_clf.fit(X_train, y_train)\n\n# predict playoff qualifiers for a given year (2000-2015)\nfor year in range(2000, 2016):\n    _df = df[df[\"year\"]==year]\n    X = _df[predictor_dimensions]\n    y = _df[\"make_playoffs\"]\n\n    _df[\"predicted_playoff_qualifier\"] = std_clf.predict(X)\n    print(\"--------\")\n    print(year)\n    print(\"--------\")\n    print(\"Actual playoff qualifiers in \" + str(year) + \":\")\n    actual = set(_df[_df[\"make_playoffs\"]==True][\"franchise_id\"])\n    print(actual)\n    print(\"Predicted playoff qualifiers in \" + str(year) + \":\")\n    predicted = set(_df[_df[\"predicted_playoff_qualifier\"]==True][\"franchise_id\"])\n    print(predicted)\n    print()\n    incorrect = predicted.difference(actual)\n    print(\"Incorrect predictions (false positives) \" + str(len(incorrect)) + \":\")\n    print(incorrect)\n    exclusions = actual.difference(predicted)\n    print(\"Incorrect exclusions from prediction (false negatives) \" + str(len(exclusions)) + \":\")\n    print(exclusions)\n    print()\n    print()","ce748a98":"The Gaussian Naive Bayes model can predict playoff qualifiers relatively well. \n\nTakeaways:\n* The model needs to be refined to predict the **correct number of qualifiers for a given year**. The model doesn't always predict X qualifiers in years with X playoff spots. For example, in 2000, the model only predicts a single playoff qualifier (SFG), even though 8 teams qualified (OAK, ATL, STL, NYY, SFG, SEA, NYM, and CHW).\n* Low false positive rate (type I error). In 11 predicted years (2000-2001, 2006-2010, 2012-2015), there are 0 false positives; in 5 predicted years (2002-2005, 2011), there are 6 total false positives, where a predicted playoff qualifier did not actually qualify.\n* Best predictive performance in 2002, where the model did predict 8 qualifiers. It correctly identified 7 of 8 qualifiers, with the only mistake: incorrectly predicting BOS (93-69, 6 games behind ANA for wild card) instead of MIN (won AL Central at 94-67). https:\/\/www.baseball-reference.com\/leagues\/MLB\/2002-standings.shtml","09668da0":"# Part 3: Predict playoff qualifiers for test data","de967f33":"# MLB playoff teams: dimensionality analysis, principal component analysis (PCA), and playoff qualifier predictive model\nA version of this notebook was originally submitted as part of my MSc Data Science grad school module, Data Science Techniques and Applications (DSTA). The assignment was:\n* Part 1: analyze a Kaggle dataset, consider the main aggregate measures of the dataset (range, quality, distribution), and select a small number of key dimensions\n* Part 2: select three dimensions as predictors of a fourth predicted dimension, discuss whether the three dimensions could become a good predictor, demo PCA similar to [this one](https:\/\/scikit-learn.org\/stable\/auto_examples\/datasets\/plot_iris_dataset.html#sphx-glr-auto-examples-datasets-plot-iris-dataset-py), display the results graphically, and comment on the effectiveness of PCA and its dimensionality reduction\n\nTo this I intend to add:\n* Part 3: based on the findings from Part 1 and Part 2, build a model for predicting MLB playoff qualifiers using dimensions like runs scored, runs allowed, OPS, and others.\n\n#### Thanks\/references\nI found these Kaggle notebooks useful for similar analyses and background [Feature selection for predicting wins](https:\/\/www.kaggle.com\/michaelmtz20\/feature-selection-for-predicting-wins), [Worst World Series winners since 1900](https:\/\/www.kaggle.com\/cm1291\/worst-world-series-winners-since-1900), [Linear regression Moneyball](https:\/\/www.kaggle.com\/maglionejm\/linear-regression-moneyball), and others.\n\n\n#### Random interestings\n\n* Leagues (AA, AL, FL, NL, PL, UA)\n* 1962 - 165 games\n* Ties\n* 1981 - playoff qualifiers\n* Season length (increase from 16 to 24 for 1914-1915)\n* May need to impute who would win wild cards (under current rules in previous years). \n\n## Part 1: dataset and dimension analysis\nThis analysis focuses on **team.csv**. The full \"History of Baseball\" data set includes 704 columns over 29 .csv files, 1 .txt file, and 1 .sqlite file. The **team.csv** contains 2805 rows and 48 columns (in other words, 2805 observations of 48 features). These 48 features are made up of 13 categorical and 35 numerical variables (supporting documentation can be found in the [R docs for this database](https:\/\/rdrr.io\/cran\/Lahman\/man\/Teams.html)):\n\n#### Categorical\/Class Variables (exclude 9-13)\n1. `league_id`: unique league identifier (6 possible classes or null)\n2. `team_id`: unique team identifier (149 classes)\n3. `franchise_id`: unique franchise identifier (120 classes)\n4. `div_id`: unique division identifier (3 classes)\n5. `div_win`: whether team won its division (Y, N, or null)\n6. `wc_win`: whether team won a wild card spot (Y, N, or null)\n7. `lg_win`: whether team won its league (Y, N, or null)\n8. `ws_win`: whether team won the World Series (Y, N, or null)\n9. `name`: team name (139 classes)\n10. `park`: team home ballpark (212 classes)\n11. `team_id_br`: unique Baseball Reference team identifier (101 classes)\n12. `team_id_lahman45`: unique Lahman database team identifier (148 classes)\n13. `team_id_retro`: unique Retrosheet team identifier (149 classes)\n\n\n#### Numerical\/Continuous Variables\n14. `year`: calendar year (season)\n15. `rank`: ordered ranking of team's finish for its division (league_id and div_id)\n\n*Game outcomes*\n\n16. `g`: games\n17. `g_home`: home games\n18. `w`: wins\n19. `l`: losses\n\n*Hitting*\n\n20. `r`: runs\n21. `ab`: at bats\n22. `h`: hits\n23. `double`: doubles\n24. `triple`: triples\n25. `hr`: home runs\n26. `bb`: walks\n27. `so`: strikeouts\n28. `sb`: stolen bases\n29. `cs`: caught stealing\n30. `hbp`: hit by pitch\n31. `sf`: sacrifice flies\n\n*Pitching*\n\n32. `ra`: runs allowed\n33. `er`: earned runs\n34. `era`: earned run average\n35. `cg`: complete games\n36. `sho`: shutouts\n37. `sv`: saves\n38. `ipouts`: outs pitched\n39. `ha`: hits allowed\n40. `hra`: home runs allowed\n41. `bba`: walks allowed\n42. `soa`: strikeouts allowed\n\n*Fielding*\n43. `e`: errors\n44. `dp`: double plays\n45. `fp`: fielding percentage\n\n*Home ballpark*\n46. `attendance`: total attendance for home games\n47. `bpf`: park factor for batters\n48. `ppf`: park factor for pitchers","9bb545eb":"# Part 2: Principal Component Analysis (PCA)\n\n* Select three dimensions as predictors of a fourth predicted dimension\n* Discuss whether the three dimensions could become a good predictor\n* Demo Principal Component Analysis similar to this one\n* Display the results graphically\n* Comment on the effectiveness of PCA and its dimensionality reduction"}}