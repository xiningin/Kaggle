{"cell_type":{"c204f543":"code","aa1ea25f":"code","dbcc00ed":"code","ad75665f":"code","f60a2603":"code","cf0d30d7":"code","ce609c0f":"code","7b460601":"code","dd371bdc":"code","82cc9272":"code","4c3d7de5":"code","9e05da2a":"code","504f2e94":"code","b7ddb953":"markdown","aa017708":"markdown"},"source":{"c204f543":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa1ea25f":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","dbcc00ed":"df_train.drop(['Name','Ticket'], axis = 1,inplace=True)\ndf_test.drop(['Name','Ticket'], axis = 1,inplace=True)\n\ndf_train['Age'].fillna(value=29,inplace=True)\ndf_test['Age'].fillna(value=29,inplace=True)\n\ndf_train['Embarked'].fillna(value='S',inplace=True)\t\ndf_test['Embarked'].fillna(value='S',inplace=True)\n\ndf_train.Cabin.fillna(value='N',inplace=True)\ndf_train.Cabin = df_train.Cabin.str[:1]\n\ndf_test.Cabin.fillna(value='N',inplace=True)\ndf_test.Cabin = df_train.Cabin.str[:1]\n\ndf_test.Fare.fillna(value=32.2,inplace=True)\n\ndf_train['Pclass'] = df_train['Pclass'].astype(str)\t\ndf_test['Pclass'] = df_test['Pclass'].astype(str)","ad75665f":"int_col = ['Age', 'SibSp', 'Parch','Fare']\n\ndf_mean = df_train[int_col].mean(axis=0)\ndf_std = df_train[int_col].std(axis=0)\n\ndf_test[int_col] = (df_test[int_col] - df_mean) \/ df_std\ndf_train[int_col] = (df_train[int_col] - df_mean) \/ df_std\n\ndf_train = pd.get_dummies(df_train)\ndf_test = pd.get_dummies(df_test)","f60a2603":"col_X = ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Cabin_A', 'Cabin_B',\n       'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_N',\n       'Cabin_T', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\ncol_y = ['Survived']","cf0d30d7":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers","ce609c0f":"train_1,train_2 = train_test_split(df_train,test_size=0.25)   # \u5c06\u6570\u636e\u5206\u4e3a75%\u8bad\u7ec3\u96c6\u548c25%\u6d4b\u8bd5\u96c6","7b460601":"mo_1 = models.Sequential()\nmo_1.add(layers.Dense(32, activation='relu', input_shape=(21,)))\nmo_1.add(layers.Dense(16, activation='relu'))\nmo_1.add(layers.Dense(4, activation='relu'))\nmo_1.add(layers.Dense(1, activation='sigmoid'))","dd371bdc":"mo_1.compile(optimizer='rmsprop',          # \u4f18\u5316\u5668\n              loss='binary_crossentropy',   # \u635f\u5931\u51fd\u6570\n              metrics=['binary_accuracy'])         # \u8bc4\u4f30\u51fd\u6570","82cc9272":"history_1 = mo_1.fit(train_1[col_X],    # \u8bad\u7ec3\u96c6X\n                    train_1[col_y],    # \u8bad\u7ec3\u6807\u7b7e\n                    epochs=64,          # \u8fed\u4ee3\u6b21\u6570\n                    batch_size=128,     # \u68af\u5ea6\u4e0b\u964d\u7ec4\n                    validation_data=(train_2[col_X], train_2[col_y])) # \u9a8c\u8bc1\u96c6","4c3d7de5":"hh = pd.DataFrame( mo_1.history.history )\nhh.plot.line()","9e05da2a":"mo_2 = models.Sequential()\nmo_2.add(layers.Dense(32, activation='relu', input_shape=(21,)))\nmo_2.add(layers.Dense(4, activation='relu'))\nmo_2.add(layers.Dense(1, activation='sigmoid'))\n\nmo_2.compile(optimizer='rmsprop',          # \u4f18\u5316\u5668\n              loss='binary_crossentropy',   # \u635f\u5931\u51fd\u6570\n              metrics=['binary_accuracy'])         # \u8bc4\u4f30\u51fd\u6570\n\nhistory_1 = mo_2.fit(df_train[col_X],    # \u8bad\u7ec3\u96c6X\n                    df_train[col_y],    # \u8bad\u7ec3\u6807\u7b7e\n                    epochs=255,          \n                    batch_size=128,verbose=0) ","504f2e94":"df_test['Survived'] = mo_2.predict(df_test[col_X])\ndf_test.loc[df_test['Survived']>=0.5,'Survived'] = 1\ndf_test.loc[df_test['Survived']< 0.5,'Survived'] = 0\ndf_test['Survived'] = df_test['Survived'].astype('int')\ndf_test[ ['PassengerId','Survived'] ].to_csv('submission.csv',index=False)","b7ddb953":"# \u6570\u636e\u9884\u5904\u7406","aa017708":"# \u6a21\u578b"}}