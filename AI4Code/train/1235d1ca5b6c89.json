{"cell_type":{"3ca06231":"code","b8dd2493":"code","ead2b831":"code","e9260fb4":"code","7aa87206":"code","055f8361":"code","eb88ddc8":"code","f8e9af47":"code","e28c27ee":"code","5f740743":"code","7e5f1b4a":"code","6a0e431e":"code","aefc3b34":"markdown","b6bb9095":"markdown","a6b726dc":"markdown","4df07594":"markdown","02c14b89":"markdown","9f353def":"markdown","c6b7c931":"markdown","40a06384":"markdown","942cfb6c":"markdown","9e800dde":"markdown"},"source":{"3ca06231":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\ndf = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\nplt.style.use('seaborn-darkgrid')","b8dd2493":"df","ead2b831":"for i in df.isnull().any():\n    if i == True:\n        print(\"MISSING VALUE\")","e9260fb4":"targs = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntargs.head()","7aa87206":"fig, axes = plt.subplots(8,2,figsize=(14, 30), dpi=100)\nfor i in range(0, 16):\n    df[f\"g-{i}\"].plot(ax=axes[i%8][i\/\/8], alpha=0.8, label='Feature', color='tab:blue')\n    df[f\"g-{i}\"].rolling(window=4).mean().plot(ax=axes[i%8][i\/\/8], alpha=0.8, label='Rolling mean', color='tab:orange')\n    axes[i%8][i\/\/8].legend();\n    axes[i%8][i\/\/8].set_title('Feature {}'.format(i), fontsize=13);\n    plt.subplots_adjust(hspace=0.45)\n","055f8361":"from sklearn.decomposition import PCA\ndf = df.drop([\"sig_id\",\"cp_type\",\"cp_time\",\"cp_dose\"],axis=1)\npca = PCA(n_components=6)\npca.fit(df)\npca_samples = pca.transform(df)\nps = pd.DataFrame(pca_samples)\nps.head()","eb88ddc8":"fig, axes = plt.subplots(3,2,figsize=(7, 15), dpi=100)\nfor i in range(0, 6):\n    ps[i].plot(ax=axes[i%3][i\/\/3], alpha=0.8, label='Feature', color='tab:blue')\n    ps[i].rolling(window=4).mean().plot(ax=axes[i%3][i\/\/3], alpha=0.8, label='Rolling mean', color='tab:orange')\n    axes[i%3][i\/\/3].legend();\n    axes[i%3][i\/\/3].set_title('Feature {}'.format(i), fontsize=13);\n    plt.subplots_adjust(hspace=0.45)\n","f8e9af47":"fig = plt.figure(figsize=(8, 8))\nsns.heatmap(ps.corr(), annot=True, cmap=plt.cm.magma);","e28c27ee":"import plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\ndata = [\n    go.Heatmap(\n        z= ps.corr().values,\n        x=ps.columns.values,\n        y=ps.columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n        opacity = 1.0 )\n]\n\nlayout = go.Layout(\n    title='Pearson Correlation of Integer-type features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')","5f740743":"fig, axes = plt.subplots(8,2,figsize=(14, 30), dpi=100)\nfor i in range(0, 16):\n    df[f\"g-{i}\"].plot(ax=axes[i%8][i\/\/8], alpha=0.8, label='Feature', color='tab:blue')\n    df[f\"g-{i}\"].rolling(window=4).std().plot(ax=axes[i%8][i\/\/8], alpha=0.8, label='Rolling mean', color='tab:orange')\n    axes[i%8][i\/\/8].legend();\n    axes[i%8][i\/\/8].set_title('Feature {}'.format(i), fontsize=13);\n    plt.subplots_adjust(hspace=0.45)\n","7e5f1b4a":"df = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n\nsns.countplot(df['cp_type'])","6a0e431e":"print(\"WORK IN PROGRESS\")","aefc3b34":"Adding interactivity lets the user play around and have fun with your dataviz, I personally like it for that reason. This is yet another way to show those painful negative correlations we have here.\n\nCheck std as opposed to rolling means-","b6bb9095":"This seems nice for now. Let's quickly check the types of each dosage we have over here:","a6b726dc":"# Mechanisms of Action: EDA for starters\n\nWelcome to the Mechanisms of Action competition! Here we have to predict drug activation mechanisms, and we have a tabular competition which allows us to make full use of the hallowed **LightGBM.** So let's get started with a simple EDA to get you \"active\" in this competition. ","4df07594":"Those are some huge negative correlations we got there in our PCA-ified data.","02c14b89":"It seems like almost each and every one of these features is basically the same distribution with a few variations, and all largely look rather even to me when I look at these features. Let's try some beginner-level dimensionality reduction technique on the data, which will be a Principal Component Analysis or PCA.","9f353def":" Let's jsut quickly take a look at a few of the features we have over here, adjusted with their rolling means.","c6b7c931":"Quick check yields no NaNs in the data, very good for us. Now we'll need to explore the columns a bit further before moving on.","40a06384":"This data basically smooshes together approximately 140 columns into one column for each of its six columns, and it has caused some resounding extremities to be seen in this data that we have. Taking a look at correlations, we get-","942cfb6c":"Targets are all principally just binary, nothing new or unique here. ","9e800dde":"We have attempted to reduce a dataset with over 800 features and extremely high dimensionality into a six-feature dataframe, which hopefully will pay off in the future. For now, let's have a look-see at this new PCA-ified dataset and its bountiful features:"}}