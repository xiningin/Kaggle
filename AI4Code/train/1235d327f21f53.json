{"cell_type":{"da2ab5d7":"code","723d68ca":"code","5cfe5518":"code","7d00ba30":"code","8ab5d583":"code","ae4bcb6c":"code","aabb07c6":"code","8b76165c":"code","cf2e4633":"code","fc471748":"code","be9c4eb5":"code","ac645f0a":"code","80a1e136":"code","a82628d3":"markdown","9fcd46c0":"markdown","4cb98d48":"markdown","6c7d812e":"markdown","4c1e001e":"markdown","4e29ea49":"markdown","f42a1c93":"markdown","177a4434":"markdown","f2003415":"markdown","c35076ba":"markdown"},"source":{"da2ab5d7":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, regularizers\nimport datatable\nimport warnings\n# ignore warnings during notebook running\nwarnings.filterwarnings('ignore')\nSEED = 2222\n# set seed\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","723d68ca":"# path of train data file\ntrain_path = '..\/input\/jane-street-market-prediction\/train.csv'\n\n# use datatable to load big data file\ntrain_file = datatable.fread(train_path).to_pandas()\ntrain_file.info()","5cfe5518":"# It is found from info() that there are only two datatypes - float64 and int32\n# try to convert to low-memory-space data types by comparing max and min values of data\n# with the preset max and min values of low-memory-space data types\nfor c in train_file.columns:\n    min_val, max_val = train_file[c].min(), train_file[c].max()\n    if train_file[c].dtype == 'float64':\n        if min_val>np.finfo(np.float16).min and max_val<np.finfo(np.float16).max:\n            train_file[c] = train_file[c].astype(np.float16)\n        elif min_val>np.finfo(np.float32).min and max_val<np.finfo(np.float32).max:\n            train_file[c] = train_file[c].astype(np.float32)\n    elif train_file[c].dtype == 'int32':\n        if min_val>np.iinfo(np.int8).min and max_val<np.iinfo(np.int8).max:\n            train_file[c] = train_file[c].astype(np.int8)\n        elif min_val>np.iinfo(np.int16).min and max_val<np.iinfo(np.int16).max:\n            train_file[c] = train_file[c].astype(np.int16)\ntrain_file.info()","7d00ba30":"# take useful features only...\nfeatures = train_file.columns[train_file.columns.str.contains('feature')]\n# find range of values\nval_range = train_file[features].max()-train_file[features].min()\n# filler value if lesser by minimum value by 1% of range\nfiller = pd.Series(train_file[features].min()-0.01*val_range, index=features)\n# This filler value will be used as a constant replacement of missing values \n\n\n\"\"\"\nA function to fill all missing values with negative outliers as discussed in the referred notebook\nhttps:\/\/www.kaggle.com\/rajkumarl\/jane-tf-keras-lstm\n\"\"\"\ndef fill_missing(df):\n    df[features] = df[features].fillna(filler)\n    return df  \n\ntrain = fill_missing(train_file)\ntrain = train.loc[train.weight > 0]\ntrain.info()","8ab5d583":"print(\"Now we have %d missing values in our data\" %train.isnull().sum().sum())","ae4bcb6c":"\"\"\"\nfrom notebook\nhttps:\/\/www.kaggle.com\/rajkumarl\/jane-day-242-feature-generation-and-selection\/comments\nbased on selected features\n\"\"\"\ndef feature_transforms(df):\n    # Generate Features using Linear shifting, Natural Logarithm and Square Root\n    for f in [f'feature_{i}' for i in [1,2,6,7,9,10,20,25,35,37,38,39,40,42,50,51,52,53,54,56,69,70,71,83,97,109,112,122,123,124,126,128,129]]: \n        # linear shifting to value above 1.0\n        df['pos_'+str(f)] = (df[f]+abs(train[f].min())+1).astype(np.float16)\n    for f in [f'feature_{i}' for i in [1,2,6,7,20,25,35,37,38,39,40,42,50,51,52,53,54,69,70,71,97,109,112,122,123,126,128,129]]: \n        # Natural log of all the values\n        df['log_'+str(f)] = np.log(df['pos_'+str(f)]).astype(np.float16)\n    for f in [f'feature_{i}' for i in [1,2,6,9,10,37,38,39,40,50,51,52,53,54,56,69,70,71,83,109,112,122,123,124,126,128,129]]: \n        # Square root of all the values\n        df['sqrt_'+str(f)] = np.sqrt(df['pos_'+str(f)]).astype(np.float16)\n    \n    # Linearly shifted values are used for log and sqrt transformations\n    # However they are useless since we have our original values which are 100% correlated\n    # Let's drop them from our data\n    df.drop([f'pos_feature_{i}' for i in [1,2,6,7,9,10,20,25,35,37,38,39,40,42,50,51,52,53,54,56,69,70,71,83,97,109,112,122,123,124,126,128,129]], inplace=True, axis=1)\n    \n    # From the Shap Dependence plots, the following features seem to have cubic relationship with target\n    cubic = [37, 39]\n    for i in cubic:\n        f = f'feature_{i}'\n        threes = np.array([3])\n        df['cub_'+f] =np.power(df[f], threes) \n        \n    # From the Shap Dependence plots, the following features seem to have quadratic relationship with target\n    quad = [53, 64, 67, 68]\n    for i in quad:\n        f = f'feature_{i}'\n        df['quad_'+f] =np.square(df[f]) \n    \n    # features that can be added together or subtracted\n    sub_pairs = [(3,6),(30,37)]\n    for i,j in sub_pairs:\n        df[f'sub_{i}_{j}'] = df[f'feature_{i}']-df[f'feature_{j}']\n\n    add_pairs = [(35,39)]\n    for i,j in add_pairs:\n        df[f'add_{i}_{j}'] = df[f'feature_{i}']+df[f'feature_{j}']\n\n    sub_log_pairs = [(9,20), (29,25), (109,7),(112,97)]\n    for i,j in sub_log_pairs:\n        df[f'sub_{i}_log{j}'] = df[f'feature_{i}']-df[f'log_feature_{j}']\n    \n    add_log_pairs = [(9,20), (29,25), (109,7), (112,97)]\n    for i,j in add_log_pairs:\n        df[f'add_{i}_log{j}'] = df[f'feature_{i}']+df[f'log_feature_{j}']\n        \n    # features that can be multiplied together\n    mul_pairs = [(39,95), (122,35)]\n    for i,j in mul_pairs:\n        df[f'mul_{i}_{j}'] = df[f'feature_{i}']*df[f'feature_{j}']\n\n    mul_log_pairs = [(6,42),(122,35)]\n    for i,j in mul_log_pairs:\n        df[f'mul_{i}_log{j}'] = df[f'feature_{i}']*df[f'log_feature_{j}']\n   \n    return df\n","aabb07c6":"\"\"\"\nfrom notebook\nhttps:\/\/www.kaggle.com\/rajkumarl\/jane-day-242-feature-generation-and-selection\/comments\n\"\"\"\nselected_features = ['weight', 'feature_1', 'feature_2', 'feature_6', 'feature_9',\n       'feature_10', 'feature_16', 'feature_20', 'feature_29', 'feature_37',\n       'feature_38', 'feature_39', 'feature_40', 'feature_51', 'feature_52',\n       'feature_53', 'feature_54', 'feature_69', 'feature_70', 'feature_71',\n       'feature_83', 'feature_100', 'feature_109', 'feature_112',\n       'feature_122', 'feature_123', 'feature_124', 'feature_126',\n       'feature_128', 'feature_129', 'log_feature_1', 'log_feature_2',\n       'log_feature_6', 'log_feature_37', 'log_feature_38', 'log_feature_39',\n       'log_feature_40', 'log_feature_50', 'log_feature_51', 'log_feature_52',\n       'log_feature_53', 'log_feature_54', 'log_feature_69', 'log_feature_70',\n       'log_feature_71', 'log_feature_109', 'log_feature_112',\n       'log_feature_122', 'log_feature_123', 'log_feature_126',\n       'log_feature_128', 'log_feature_129', 'sqrt_feature_1',\n       'sqrt_feature_2', 'sqrt_feature_6', 'sqrt_feature_9', \n       'sqrt_feature_10', 'sqrt_feature_37', 'sqrt_feature_38', 'sqrt_feature_39',\n       'sqrt_feature_40', 'sqrt_feature_50', 'sqrt_feature_51',\n       'sqrt_feature_52', 'sqrt_feature_53', 'sqrt_feature_54',\n       'sqrt_feature_56', 'sqrt_feature_69', 'sqrt_feature_70',\n       'sqrt_feature_71', 'sqrt_feature_83', 'sqrt_feature_109',\n       'sqrt_feature_112', 'sqrt_feature_122', 'sqrt_feature_123',\n       'sqrt_feature_124', 'sqrt_feature_126', 'sqrt_feature_128',\n       'sqrt_feature_129', 'cub_feature_37', 'cub_feature_39',\n       'quad_feature_53', 'quad_feature_64', 'quad_feature_67',\n       'quad_feature_68', 'sub_3_6', 'sub_30_37', 'add_35_39', 'add_9_log20',\n       'sub_9_log20', 'add_29_log25', 'sub_29_log25', 'add_109_log7',\n       'sub_109_log7', 'add_112_log97', 'sub_112_log97', 'mul_39_95',\n       'mul_122_35', 'mul_6_log42', 'mul_122_log35']","8b76165c":"class Residual(tf.keras.Model):  \n    \"\"\"The Residual layer of ResNet\"\"\"\n    def __init__(self, units):\n        super().__init__()\n        # initialize necessary dense and batch norm layers\n        self.d1 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.d2 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.d3 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.bn1 = layers.BatchNormalization()\n        self.bn2 = layers.BatchNormalization()\n\n    def call(self, X):\n        # stack two dense layers in series...\n        Y = tf.keras.activations.relu(self.bn1(self.d1(X)))\n        Y = layers.Dropout(0.3)(self.bn2(self.d2(Y)))\n        # ... and concatenate them with a third dense layer \n        X = self.d3(X)\n        Y += X\n        # apply dropout to avoid overfitting\n        return layers.Dropout(0.3)(tf.keras.activations.relu(Y))","cf2e4633":"class ResnetBlock(layers.Layer):\n    def __init__(self, num_units, num_residuals, **kwargs):\n        super(ResnetBlock, self).__init__(**kwargs)\n        # initialize a list of layers\n        self.residual_layers = []\n        for i in range(num_residuals):\n            # append list with residual layers\n            self.residual_layers.append(Residual(num_units))\n\n    def call(self, X):\n        for layer in self.residual_layers.layers:\n            # stack residual layers in series\n            X = layer(X)\n        return X","fc471748":"def create_model():\n    # a keras Sequential model\n    model= tf.keras.Sequential([\n        # model receives data with 100 features\n        layers.Input(shape=(100,)),\n        # incorporate noise to avoid overfitting\n        layers.GaussianNoise(0.2),\n        # introduce first layer before ResNet blocks with regularizers and relu activation\n        layers.Dense(64, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        # a dropout layer to avoid overfitting\n        layers.Dropout(0.5),\n        # four subsequent ResNet blocks\n        ResnetBlock(64, 2),\n        ResnetBlock(128, 2),\n        ResnetBlock(256, 2),\n        ResnetBlock(512, 2),\n        # two layers after ResNet blocks\n        layers.Dense(64, activation='relu'),\n        # output layer - binary classification - sigmoid activation\n        layers.Dense(1, activation='sigmoid')])\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n                  loss=tf.keras.losses.BinaryCrossentropy(), \n                  metrics=['accuracy'])\n    return model\n","be9c4eb5":"# find out the files in the training data path\n!ls '..\/input\/tf-residual-network-on-select-features-training'","ac645f0a":"# path of our saved model weihts\nPATH = '..\/input\/tf-residual-network-on-select-features-training\/'\nmodels = []\nfolds = 3\nfor i in range(folds):\n    model = create_model()\n    model.load_weights(PATH+f'resnet_select_feature_{i+1}.h5')\n    models.append(model)\nprint('Modeling phase completed')","80a1e136":"from tqdm.auto import tqdm\nimport janestreet\njanestreet.make_env.__called__ = False\nenv = janestreet.make_env()\nfor test,pred in tqdm(env.iter_test()):\n    if test.weight.item()==0:\n        pred.action = 0\n    else:\n        if test[features].isna().any().sum():\n            test[features] = fill_missing(test[features])\n        test = feature_transforms(test)\n        test = np.array(test[selected_features], dtype=np.float)\n        action = np.mean([model(test).numpy() for model in models])\n        pred.action = 1 if action>0.5 else 0\n    env.predict(pred)","a82628d3":"# 3. HANDLING MISSING VALUES","9fcd46c0":"# 6. INFERENCE","4cb98d48":"# 1. IMPORT LIBRARIES","6c7d812e":"# 4. FEATURE GENERATION AND SELECTION","4c1e001e":"### This notebook has the following Notebooks as reference on detailed analysis and reasonable way of handling the missing values, and feature generation and selection. Apart from that there are few other good notebooks from where this notebook got some value addition pieces of ideas! I wish to thank my fellow kagglers who compel me to learn and grow!\n\n### [Kaggle Notebook] [Jane TF Keras LSTM](https:\/\/www.kaggle.com\/rajkumarl\/jane-tf-keras-lstm) (to fill missing values)\n### [Kaggle Notebook] [Jane Day 242 Feature Generation and Selection](https:\/\/www.kaggle.com\/rajkumarl\/jane-day-242-feature-generation-and-selection) (to generate and select features)\n","4e29ea49":"### Thank you for your time!","f42a1c93":"# 5. RESIDUAL NETWORK MODELING","177a4434":"### Training part of this notebook is available in the following notebook. Three models were trained with three-folds of data. Weights of those models are saved in h5 format to be used in this notebook for inference.\n\n### [Kaggle Notebook] [TF Residual Network on Select Features](https:\/\/www.kaggle.com\/rajkumarl\/tf-residual-network-on-select-features-training) (for model training)","f2003415":"# 2. LOAD DATA AND OPTIMIZE MEMORY","c35076ba":"### That's a great reduction in memory usage (around 74% reduction)! It will help us go further efficiently!"}}