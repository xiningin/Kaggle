{"cell_type":{"d7ebc5df":"code","74021693":"code","564f227b":"code","1fabea30":"code","d62088f6":"code","eaa1883e":"code","6e32a7b0":"code","df3b3e5d":"code","472d934b":"code","4a5061ee":"code","115da406":"code","3ac12dab":"code","3b0ce15d":"code","e7c3a01f":"code","ce3ca114":"code","49883a37":"code","982c43e1":"code","554d5d57":"code","8d5dee78":"code","6c78b554":"code","80738dc4":"code","d833ea62":"code","2bf4d8d8":"code","5c298da9":"code","e59e32a6":"code","b37daabd":"code","278cafa8":"code","e2103702":"code","19d05383":"code","8faf1858":"code","aa6e6b64":"code","1da8ea1a":"code","b5f9f980":"code","d2501b15":"code","1de6cf07":"code","693358ab":"code","8833ef86":"code","dbdfbc4b":"code","569e439c":"markdown","73c05156":"markdown","bcaa9d94":"markdown","13bc979f":"markdown","71878a98":"markdown","bd6852b1":"markdown","51b46331":"markdown","8076ec2c":"markdown","6f1dc349":"markdown","7ff6658b":"markdown","131a8dcb":"markdown","d69f23fd":"markdown","0cbdd908":"markdown","7fdfb9b5":"markdown","84889f08":"markdown","ca8f67ca":"markdown","4cbe6b30":"markdown","eec3f34f":"markdown","3bfc4c5e":"markdown","33575e5b":"markdown","79b5a8cf":"markdown","076b8d21":"markdown","5ab20961":"markdown","51eab69e":"markdown","f6113346":"markdown","ccc05a62":"markdown","9b2c20db":"markdown","ff5636f4":"markdown","e988321f":"markdown","f549b635":"markdown","293d97b5":"markdown","ae1f9760":"markdown","a126810d":"markdown","486b0a79":"markdown"},"source":{"d7ebc5df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74021693":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport keras \nimport numpy as np\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_tree, plot_importance\nfrom keras.models import Sequential, Model\nfrom keras import optimizers\nfrom keras.layers import Dense, Dropout, Input, Embedding, InputLayer\nfrom keras.layers.merge import concatenate\nfrom keras.utils import plot_model\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc, RocCurveDisplay\nfrom sklearn.model_selection import StratifiedKFold\n%matplotlib inline","564f227b":"df = pd.read_csv(r'\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","1fabea30":"df.head()","d62088f6":"df.dtypes","eaa1883e":"missing_values = df.drop(labels='id', axis=1).isna().sum()\ncolor2 = px.colors.qualitative.Bold[7]\nfig = go.Figure(data=[go.Bar(x=list(missing_values.keys()), y=missing_values.values, marker=dict(color=color2))], layout=go.Layout(title=go.layout.Title(text=\"Missing values per column\/feature\"), yaxis_title=\"Count\"))\nfig.show()","6e32a7b0":"categorical_data = ['gender', 'hypertension', 'heart_disease',\n       'work_type', 'Residence_type', 'smoking_status', 'ever_married']\nnumerical_data = df.drop(labels=categorical_data+['id', 'stroke'], axis=1).columns.tolist()\ntarget_data = ['stroke']","df3b3e5d":"gender_dim = go.parcats.Dimension(values=df.gender, label='Gender')\nhypertension_dim = go.parcats.Dimension(values=df.hypertension, label='Hypertension', categoryarray= [0,1], ticktext=['No Hypertension', 'Hypertension'])\nheartdisease_dim = go.parcats.Dimension(values=df.heart_disease, label='Heart Disease', categoryarray= [0,1], ticktext=['No Heart Disease', 'Heart Disease'])\nevermarried_dim = go.parcats.Dimension(values=df.ever_married, label='Ever Married')\nworktype_dim = go.parcats.Dimension(values=df.work_type, label='Work Type')\nresidence_dim = go.parcats.Dimension(values=df.Residence_type, label='Residence Type')\nsmoking_dim = go.parcats.Dimension(values=df.smoking_status, label='Smoking')\nstroke_dim = go.parcats.Dimension(values=df.stroke, label='Stroke', categoryarray= [0,1], ticktext=['No Stroke', 'Stroke'])","472d934b":"df['gender_nominal'] = np.nan\ndf.loc[df['gender'] == 'Male', 'gender_nominal'] = 0\ndf.loc[df['gender'] == 'Female', 'gender_nominal'] = 1\n\ncolor=df.gender_nominal\n\nfig = go.Figure(data = [go.Parcats(dimensions=[hypertension_dim, heartdisease_dim, evermarried_dim,\n                                              worktype_dim, residence_dim, smoking_dim, stroke_dim, gender_dim],\n        line={'color': color, 'colorscale': 'Earth'},\n        hoveron='color', hoverinfo='count+probability',\n        labelfont={'size': 18, 'family': 'Times'},\n        tickfont={'size': 16, 'family': 'Times'},\n        arrangement='freeform')],\n               layout=go.Layout(title=go.layout.Title(text=\"Overview about bias of each feature with respect to gender\")))\n\n\nfig.update_layout(\n    autosize=False,\n    width=1200,\n    height=600\n)\n\nfig.show()\ndf.drop(labels='gender_nominal', axis=1, inplace=True)","4a5061ee":"color=df.stroke\ncolorscale=[[0, 'blue'], [1, 'yellow']]\n\nfig = go.Figure(data = [go.Parcats(dimensions=[gender_dim, hypertension_dim, heartdisease_dim, evermarried_dim,\n                                              worktype_dim, residence_dim, smoking_dim, stroke_dim],\n        line={'color': color, 'colorscale': 'Earth'},\n        hoveron='color', hoverinfo='count+probability',\n        labelfont={'size': 18, 'family': 'Times'},\n        tickfont={'size': 16, 'family': 'Times'},\n        arrangement='freeform')],\n               layout=go.Layout(title=go.layout.Title(text=\"Overview about bias of each feature with respect to target\")))\n\n\nfig.update_layout(\n    autosize=False,\n    width=1200,\n    height=600\n)\n\nfig.show()","115da406":"df[numerical_data].describe()","3ac12dab":"corr_columns = numerical_data + ['stroke']\npearson_corr = df[corr_columns].corr(method='pearson')\nspearman_corr = df[corr_columns].corr(method='spearman')\n\nX = [corr_columns[k] for k in range(len(corr_columns))]","3b0ce15d":"fig = make_subplots(rows=1, cols=2, subplot_titles=('Pearson Correlation Matrix', 'Spearman Correlation Matrix'), shared_yaxes=True)\n\n   \nfig.add_trace(go.Heatmap(z=pearson_corr,\n                  x=X,\n                  y=X,\n                  xgap=1, ygap=1,\n                  colorscale='Earth',\n                  hovertext = round(pearson_corr, 2),\n                  hoverinfo='text',showscale=False), row=1, col=1)\n\n   \nfig.add_trace(go.Heatmap(z=spearman_corr,\n                  x=X,\n                  y=X,\n                  xgap=1, ygap=1,\n                  colorbar_thickness=20,\n                  colorbar_ticklen=3,\n                  colorscale='Earth',\n                  hovertext = round(spearman_corr, 2),\n                  hoverinfo='text'), row=1, col=2)\n\nfig.update_layout(title_text=\"Correlation Plots\")\n\n\nfig.show() ","e7c3a01f":"fig = go.Figure(data=go.Splom(\n                dimensions=[dict(label='Age',\n                                values=df[numerical_data[0]]),\n                           dict(label='Average Glucose Level',\n                               values=df[numerical_data[1]]),\n                           dict(label='Body-Mass-Index',\n                               values=df[numerical_data[2]])],\n                            showupperhalf=False,\n                            text=df['stroke'],\n                            marker=dict(color=df['stroke'],\n                                       showscale=False,\n                                       colorscale='Earth'),                          \n))\n\ntitle='Scatter Plot of Numerical Features'\n\nfig.update_layout(\n    title_text=title,\n    autosize=False,\n    width=800,\n    height=800\n)\n\nfig.show()","ce3ca114":"color1 = px.colors.qualitative.Dark2[6]\ncolor2 = px.colors.qualitative.Bold[7]\ncolor3 = px.colors.qualitative.Light24[0]\n\nfig = px.histogram(df, x=\"bmi\", color=\"gender\", marginal=\"rug\", hover_data=df.columns, title='Histogram Body-Mass-Index', color_discrete_sequence=[color1, color2, color3])\nfig.show()","49883a37":"fig = px.histogram(df, x=\"avg_glucose_level\", color=\"gender\", marginal=\"rug\", hover_data=df.columns, title='Histogram Average Glucose Level', color_discrete_sequence=[color1, color2, color3])\nfig.show()","982c43e1":"fig = px.histogram(df, x=\"age\", color=\"gender\", marginal=\"rug\", hover_data=df.columns, title=\"Histogram Age\", color_discrete_sequence=[color1, color2, color3])\nfig.show()","554d5d57":"df = df[df['gender'] != 'Other']","8d5dee78":"df_categorical = df[categorical_data]\ndf_numerical = df[numerical_data]","6c78b554":"imp = IterativeImputer(random_state=0)\nimp.fit(df_numerical)\nimputed_data = imp.transform(df_numerical).round(1)","80738dc4":"standard_scaler = StandardScaler()\nstandard_scaled_data = standard_scaler.fit_transform(imputed_data)\n\nminmax_scaler = MinMaxScaler()\nminmax_scaled_data = minmax_scaler.fit_transform(imputed_data)","d833ea62":"lab_enc = LabelEncoder()\nlab_enc_features = df_categorical.apply(lab_enc.fit_transform)\n\ndf_prepared_nominal_standard = pd.DataFrame(data=np.concatenate((np.array(lab_enc_features), standard_scaled_data), axis=1), columns=categorical_data+numerical_data)\ndf_prepared_nominal_standard['stroke'] = df['stroke'].values\n\ndf_prepared_nominal_minmax = pd.DataFrame(data=np.concatenate((np.array(lab_enc_features), minmax_scaled_data), axis=1), columns=categorical_data+numerical_data)\ndf_prepared_nominal_minmax['stroke'] = df['stroke'].values","2bf4d8d8":"onehot_enc = OneHotEncoder()\nonehot_features = onehot_enc.fit_transform(df_categorical).toarray().tolist()\nonehot_feature_vector = [onehot_features[i] + standard_scaled_data.tolist()[i] for i in range(len(onehot_features))]\n\ndf_prepared_onehot = df_categorical\nfor i in range(len(numerical_data)):\n    df_prepared_onehot[numerical_data[i]] = standard_scaled_data[:, i]\ndf_prepared_onehot['onehot_feature_vector'] = onehot_feature_vector\ndf_prepared_onehot['stroke'] = df['stroke'].values","5c298da9":"oversampling = SMOTE(sampling_strategy=0.7, random_state=33)\nundersampling = RandomUnderSampler(sampling_strategy=1, random_state=33)\nsteps= [('o', oversampling), ('u', undersampling)]\npipline = Pipeline(steps=steps)","e59e32a6":"callback = EarlyStopping(monitor='loss', patience=3)\n\nin_layers = list()\nem_layers = list()\nfor category in categorical_data:\n    n_labels = df_prepared_nominal_standard[category].nunique()\n    in_layer = Input(shape=(1,))\n    em_layer = Embedding(n_labels, 5)(in_layer)\n    in_layers.append(in_layer)\n    em_layers.append(em_layer)\n\nin_layer_num = Input(shape=(1,3,))    \n\nmerge_first = concatenate(em_layers)\nmerge_second = concatenate([merge_first, in_layer_num], axis=2)\ndense = Dense(16, activation='relu')(merge_second)\ndense = Dense(16, activation='relu')(dense)\noutput = Dense(1, activation='sigmoid')(dense)\n\nin_layers.append(in_layer_num)\n\nmodel = Model(inputs=in_layers, outputs=output)","b37daabd":"plot_model(model, show_shapes=True, to_file='embedding_model.png')","278cafa8":"skf = StratifiedKFold(n_splits=5)\n\nX = np.array(df_prepared_nominal_standard[categorical_data+numerical_data])\ny = np.array(df_prepared_nominal_standard['stroke'])\n\nacc_per_fold_embedding_dnn = list()\nloss_per_fold_embedding_dnn = list()\nauc_per_fold_embedding_dnn = list()\n\nfor fold, (train_index, test_index) in enumerate (skf.split(X, y)):\n    features_nominal_resampled, target_nominal_resampled = pipline.fit_resample(X[train_index], y[train_index])\n    features_nominal_resampled[: , :6] = np.around(features_nominal_resampled[:, :6], 0)\n    \n    \n    ### input for training ###\n    training_input = list()\n    \n    for i, _ in enumerate(categorical_data):\n        s = np.index_exp[:, i]\n        in_array = features_nominal_resampled[s]\n        training_input.append(in_array)\n    \n    training_input.append(np.expand_dims(features_nominal_resampled[:, 7:], axis=1))\n    training_target = target_nominal_resampled.reshape(-1,1,1)\n    \n    ### input for validation ### \n    validation_input = list()\n    \n    features_nominal_validation = X[test_index]\n    validation_target = y[test_index]\n    \n    for i, _ in enumerate(categorical_data):\n        s = np.index_exp[:, i]\n        in_array = features_nominal_validation[s]\n        validation_input.append(in_array)\n    \n    validation_input.append(np.expand_dims(features_nominal_validation[:, 7:], axis=1))\n    validation_target = validation_target.reshape(-1,1,1)\n    \n    ### compile model ###\n    opt = optimizers.Adam(lr=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n     \n    ### model training ###    \n    print('')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print(f'Training for Embedding Neural Network fold {fold} ...')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print('')\n\n        \n    model.fit(x=training_input, y=training_target, batch_size=16, epochs=150, verbose=0, callbacks=[callback])\n    \n    scores = model.evaluate(x=validation_input, y=validation_target, verbose=0)\n    \n    acc_per_fold_embedding_dnn.append(scores[1] * 100)\n    loss_per_fold_embedding_dnn.append(scores[0])\n    \n    predictions_embedding_dnn = model.predict(validation_input).reshape(-1,1)\n    predictions_embedding_dnn = (predictions_embedding_dnn > 0.5).astype(int)\n    \n    \n    #model evaluation\n    \n    print(f'Score for fold {fold} using evaluate function: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    print('')\n    \n    print(f'Confusion matrix {confusion_matrix(validation_target.reshape(-1,1), predictions_embedding_dnn)} for fold {fold}')\n    print('')\n\n    print(f'Classification report {classification_report(validation_target.reshape(-1,1), predictions_embedding_dnn)} for fold {fold}')\n    print('')\n    \n    \n    fpr, tpr, thresholds = roc_curve(validation_target.reshape(-1,1), predictions_embedding_dnn)\n    roc_auc = auc(fpr, tpr)\n    auc_per_fold_embedding_dnn.append(roc_auc*100)\n#     display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n#     display.plot()  \n#     plt.show()\n\n    print(f'Area Under the ROC Curve (AUC) {roc_auc} for fold {fold}')\n    print('')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print('')\n    \naccuracy_embedding_dnn_overall_standard = sum(acc_per_fold_embedding_dnn)\/len(acc_per_fold_embedding_dnn)\nauc_embedding_dnn_overall_standard = sum(auc_per_fold_embedding_dnn)\/len(auc_per_fold_embedding_dnn) \n\n\nprint('')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint(f'Results for Embedding Neural Network')\nprint('------------------------------------------------------------------------')\nprint('')\nprint(f'Average accuracy over all folds for Embedding Neural Network is {accuracy_embedding_dnn_overall_standard}%')\nprint(f'Average area under curve (AUC) over all folds for Embedding Neural Network is {auc_embedding_dnn_overall_standard}')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint('')  ","e2103702":"callback = EarlyStopping(monitor='loss', patience=3)\n\nin_layers = list()\nem_layers = list()\nfor category in categorical_data:\n    n_labels = df_prepared_nominal_minmax[category].nunique()\n    in_layer = Input(shape=(1,))\n    em_layer = Embedding(n_labels, 5)(in_layer)\n    in_layers.append(in_layer)\n    em_layers.append(em_layer)\n\nin_layer_num = Input(shape=(1,3,))    \n\nmerge_first = concatenate(em_layers)\nmerge_second = concatenate([merge_first, in_layer_num], axis=2)\ndense = Dense(16, activation='relu')(merge_second)\ndense = Dense(16, activation='relu')(dense)\noutput = Dense(1, activation='sigmoid')(dense)\n\nin_layers.append(in_layer_num)\n\nmodel = Model(inputs=in_layers, outputs=output)","19d05383":"skf = StratifiedKFold(n_splits=5)\n\nX = np.array(df_prepared_nominal_minmax[categorical_data+numerical_data])\ny = np.array(df_prepared_nominal_minmax['stroke'])\n\nacc_per_fold_embedding_dnn = list()\nloss_per_fold_embedding_dnn = list()\nauc_per_fold_embedding_dnn = list()\n\nfor fold, (train_index, test_index) in enumerate (skf.split(X, y)):\n    features_nominal_resampled, target_nominal_resampled = pipline.fit_resample(X[train_index], y[train_index])\n    features_nominal_resampled[: , :6] = np.around(features_nominal_resampled[:, :6], 0)\n    \n    \n    ### input for training ###\n    training_input = list()\n    \n    for i, _ in enumerate(categorical_data):\n        s = np.index_exp[:, i]\n        in_array = features_nominal_resampled[s]\n        training_input.append(in_array)\n    \n    training_input.append(np.expand_dims(features_nominal_resampled[:, 7:], axis=1))\n    training_target = target_nominal_resampled.reshape(-1,1,1)\n    \n    ### input for validation ### \n    validation_input = list()\n    \n    features_nominal_validation = X[test_index]\n    validation_target = y[test_index]\n    \n    for i, _ in enumerate(categorical_data):\n        s = np.index_exp[:, i]\n        in_array = features_nominal_validation[s]\n        validation_input.append(in_array)\n    \n    validation_input.append(np.expand_dims(features_nominal_validation[:, 7:], axis=1))\n    validation_target = validation_target.reshape(-1,1,1)\n    \n    ### compile model ###\n    opt = optimizers.Adam(lr=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n     \n    ### model training ###    \n    print('')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print(f'Training for Embedding Neural Network fold {fold} ...')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print('')\n\n        \n    model.fit(x=training_input, y=training_target, batch_size=16, epochs=150, verbose=0, callbacks=[callback])\n    \n    scores = model.evaluate(x=validation_input, y=validation_target, verbose=0)\n    \n    acc_per_fold_embedding_dnn.append(scores[1] * 100)\n    loss_per_fold_embedding_dnn.append(scores[0])\n    \n    predictions_embedding_dnn = model.predict(validation_input).reshape(-1,1)\n    predictions_embedding_dnn = (predictions_embedding_dnn > 0.5).astype(int)\n    \n    \n    #model evaluation\n    \n    print(f'Score for fold {fold} using evaluate function: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    print('')\n    \n    print(f'Confusion matrix {confusion_matrix(validation_target.reshape(-1,1), predictions_embedding_dnn)} for fold {fold}')\n    print('')\n\n    print(f'Classification report {classification_report(validation_target.reshape(-1,1), predictions_embedding_dnn)} for fold {fold}')\n    print('')\n    \n    \n    fpr, tpr, thresholds = roc_curve(validation_target.reshape(-1,1), predictions_embedding_dnn)\n    roc_auc = auc(fpr, tpr)\n    auc_per_fold_embedding_dnn.append(roc_auc*100)\n#     display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n#     display.plot()  \n#     plt.show()\n\n    print(f'Area Under the ROC Curve (AUC) {roc_auc} for fold {fold}')\n    print('')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print('')\n    \naccuracy_embedding_dnn_overall_minmax = sum(acc_per_fold_embedding_dnn)\/len(acc_per_fold_embedding_dnn)\nauc_embedding_dnn_overall_minmax = sum(auc_per_fold_embedding_dnn)\/len(auc_per_fold_embedding_dnn) \n\n\nprint('')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint(f'Results for Embedding Neural Network')\nprint('------------------------------------------------------------------------')\nprint('')\nprint(f'Average accuracy over all folds for Embedding Neural Network is {accuracy_embedding_dnn_overall_minmax}%')\nprint(f'Average area under curve (AUC) over all folds for Embedding Neural Network is {auc_embedding_dnn_overall_minmax}')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint('')  ","8faf1858":"xg_model = XGBClassifier(max_depth=3, learning_rate=0.01, n_estimators=100)","aa6e6b64":"skf = StratifiedKFold(n_splits=5)\n\nX = np.array(list(df_prepared_onehot['onehot_feature_vector'].apply(lambda x: list(x))))\ny = np.array(df_prepared_onehot['stroke'])\n\nacc_per_fold_xg = list()\nloss_per_fold_xg = list()\nauc_per_fold_xg = list()\n\nfor fold, (train_index, test_index) in enumerate (skf.split(X, y)):\n    features_onehot_resampled, target_onehot_resampled = pipline.fit_resample(X[train_index], y[train_index])\n    features_onehot_resampled[: , :6] = np.around(features_onehot_resampled[:, :6], 0)\n    \n    xg_model.fit(np.array(features_onehot_resampled), target_onehot_resampled)\n    \n    target_predictions_model_xg = xg_model.predict(X[test_index])\n    \n    accuracy_xg = accuracy_score(y[test_index], target_predictions_model_xg)\n    acc_per_fold_xg.append(accuracy_xg * 100.0)\n    \n    print('')\n    print('')\n    print('')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print(f'Training for XGBoost fold {fold} ...')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print('')\n    \n    \n    # model evaluation\n    \n    print(f'Accuracy for fold {fold}: {accuracy_xg * 100.0}%')\n    print('')\n    \n    \n    print(f'Confusion matrix {confusion_matrix(y[test_index], target_predictions_model_xg)} for fold {fold}')\n    print('')\n\n    \n    print(f'Classification report {classification_report(y[test_index], target_predictions_model_xg)} for fold {fold}')\n    print('')\n    \n    \n    fpr, tpr, thresholds = roc_curve(y[test_index], target_predictions_model_xg)\n    roc_auc = auc(fpr, tpr)\n    auc_per_fold_xg.append(roc_auc * 100)\n#     display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n#     display.plot()  \n#     plt.show()\n\n    print(f'Area Under the ROC Curve (AUC) {roc_auc} for fold {fold}')\n    print('')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print('')\n    \naccuracy_xg_overall = sum(acc_per_fold_xg)\/len(acc_per_fold_xg)\nauc_xg_overall = sum(auc_per_fold_xg)\/len(auc_per_fold_xg) \n\n\nprint('')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint(f'Results for XGBoost Classifier')\nprint('------------------------------------------------------------------------')\nprint('')\nprint(f'Average accuracy over all folds for XGBoost is {accuracy_xg_overall}%')\nprint(f'Average area under curve (AUC) over all folds for XGBoost is {auc_xg_overall}')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint('')  ","1da8ea1a":"callback = EarlyStopping(monitor='loss', patience=3)\n\ndnn_model = Sequential()\ndnn_model.add(Dense(16, input_dim=22, activation='relu'))\ndnn_model.add(Dense(16, activation='relu'))\ndnn_model.add(Dense(1, activation='sigmoid'))\n\ndnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","b5f9f980":"plot_model(dnn_model, show_shapes=True, to_file='dnn_model.png')","d2501b15":"skf = StratifiedKFold(n_splits=5)\n\nX = np.array(list(df_prepared_onehot['onehot_feature_vector'].apply(lambda x: list(x))))\ny = np.array(df_prepared_onehot['stroke'])\n\nacc_per_fold_dnn = list()\nloss_per_fold_dnn = list()\nauc_per_fold_dnn = list()\n\nfor fold, (train_index, test_index) in enumerate (skf.split(X, y)):\n    features_onehot_resampled, target_onehot_resampled = pipline.fit_resample(X[train_index], y[train_index])\n    features_onehot_resampled[: , :6] = np.around(features_onehot_resampled[:, :6], 0)\n    \n    dnn_model.fit(np.array(features_onehot_resampled), target_onehot_resampled, epochs=150, batch_size=16, verbose=0, callbacks=[callback])\n    \n    scores = dnn_model.evaluate(x=X[test_index], y=y[test_index], verbose=0)\n    \n    acc_per_fold_dnn.append(scores[1] * 100)\n    loss_per_fold_dnn.append(scores[0])\n    \n    target_predictions_model_dnn = dnn_model.predict(X[test_index])\n    target_predictions_model_dnn = (target_predictions_model_dnn > 0.5).astype(int)\n    \n    \n    print('')\n    print('')\n    print('')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print(f'Training for Deep Neural Network Fold {fold} ...')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print('')\n    \n    #model evaluation\n    \n    print(f'Score for fold {fold} using evaluate function: {dnn_model.metrics_names[0]} of {scores[0]}; {dnn_model.metrics_names[1]} of {scores[1]*100}%')\n    print('')\n    \n    \n    print(f'Confusion matrix {confusion_matrix(y[test_index], target_predictions_model_dnn)} for fold {fold}')\n    print('')\n\n    \n    print(f'Classification report {classification_report(y[test_index], target_predictions_model_dnn)} for fold {fold}')\n    print('')\n    \n    \n    fpr, tpr, thresholds = roc_curve(y[test_index], target_predictions_model_dnn)\n    roc_auc = auc(fpr, tpr)\n    auc_per_fold_dnn.append(roc_auc*100)\n#     display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n#     display.plot()  \n#     plt.show()\n\n    print(f'Area Under the ROC Curve (AUC) {roc_auc} for fold {fold}')\n    print('')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print('')\n    \n    \naccuracy_dnn_overall = sum(acc_per_fold_dnn)\/len(acc_per_fold_dnn)\nauc_dnn_overall = sum(auc_per_fold_dnn)\/len(auc_per_fold_dnn) \n\n\nprint('')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint(f'Results for Deep Neural Network')\nprint('------------------------------------------------------------------------')\nprint('')\nprint(f'Average accuracy over all folds for Deep Neural Network is {accuracy_dnn_overall}%')\nprint(f'Average area under curve (AUC) over all folds for Deep Neural Network is {auc_dnn_overall}')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint('')      ","1de6cf07":"rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, verbose=1)","693358ab":"skf = StratifiedKFold(n_splits=5)\n\nX = np.array(df_prepared_nominal_standard[categorical_data+numerical_data])\ny = np.array(df_prepared_nominal_standard['stroke'])\n\nacc_per_fold_rf = list()\nloss_per_fold_rf = list()\nauc_per_fold_rf = list()\n\nfor fold, (train_index, test_index) in enumerate (skf.split(X, y)):\n    features_nominal_resampled, target_nominal_resampled = pipline.fit_resample(X[train_index], y[train_index])\n    features_nominal_resampled[: , :6] = np.around(features_nominal_resampled[:, :6], 0)\n    \n    rf_model.fit(np.array(features_nominal_resampled), target_nominal_resampled)\n    \n    target_predictions_model_rf = rf_model.predict(X[test_index])\n    \n    accuracy_rf = accuracy_score(y[test_index], target_predictions_model_rf)\n    acc_per_fold_rf.append(accuracy_rf * 100.0)\n    \n    print('')\n    print('')\n    print('')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print(f'Training for Random Forest fold {fold} ...')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print('')\n    \n    #model evaluation\n    \n    print(f'Accuracy for fold {fold}: {accuracy_rf * 100.0}%')\n    print('')\n    \n    \n    print(f'Confusion matrix {confusion_matrix(y[test_index], target_predictions_model_rf)} for fold {fold}')\n    print('')\n\n    \n    print(f'Classification report {classification_report(y[test_index], target_predictions_model_rf)} for fold {fold}')\n    print('')\n    \n    \n    fpr, tpr, thresholds = roc_curve(y[test_index], target_predictions_model_rf)\n    roc_auc = auc(fpr, tpr)\n    auc_per_fold_rf.append(roc_auc*100)\n#     display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n#     display.plot()  \n#     plt.show()\n\n    print(f'Area Under the ROC Curve (AUC) {roc_auc} for fold {fold}')\n    print('')\n    print('------------------------------------------------------------------------')\n    print('------------------------------------------------------------------------')\n    print('')\n    \n    \naccuracy_rf_overall = sum(acc_per_fold_rf)\/len(acc_per_fold_rf)\nauc_rf_overall = sum(auc_per_fold_rf)\/len(auc_per_fold_rf) \n\n\nprint('')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint(f'Results for Random Forest Classifier')\nprint('------------------------------------------------------------------------')\nprint('')\nprint(f'Average accuracy over all folds for Random Forest is {accuracy_rf_overall}%')\nprint(f'Average area under curve (AUC) over all folds for Random Forest is {auc_rf_overall}')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint('')    \n","8833ef86":"accuracy_dict = {'XGBoost': accuracy_xg_overall, 'Random Forest': accuracy_rf_overall, 'Deep Neural Network': accuracy_dnn_overall, 'Embedding Neural Network (standard scaled)': accuracy_embedding_dnn_overall_standard, 'Embedding Neural Network (minmax scaled)': accuracy_embedding_dnn_overall_minmax}\nauc_dict = {'XGBoost': auc_xg_overall, 'Random Forest': auc_rf_overall, 'Deep Neural Network': auc_dnn_overall, 'Embedding Neural Network (standard scaled)': auc_embedding_dnn_overall_standard, 'Embedding Neural Network (minmax scaled)': auc_embedding_dnn_overall_minmax}\nsorted_models = sorted(auc_dict, key=auc_dict.get, reverse=True)\nsorted_auc = sorted(list(auc_dict.values()), reverse=True)","dbdfbc4b":"color1 = px.colors.qualitative.Dark2[6]\ncolor2 = px.colors.qualitative.Bold[7]\n\nfig = make_subplots(rows=1, cols=1, shared_xaxes = True, shared_yaxes=True, vertical_spacing=0.001, horizontal_spacing=0.1)\n\nfig.append_trace(go.Bar(\n            x=list(accuracy_dict.values()),\n            y=list(accuracy_dict.keys()),\n            name='Accuracy',\n            orientation='h', marker_color=color1), 1,1)\n\n\nfig.append_trace(go.Bar(\n            x=list(auc_dict.values()),\n            y=list(auc_dict.keys()),\n            name='AUC',\n            orientation='h',  marker_color=color2), 1,1)\n\nfig.update_layout(\n    title='Comparison of different models',\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ))\n\nfig.update_layout(xaxis_range=[0,100])\n\n\nfig.show()\n\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint(f'Ranking of the trained classifiers:')\nprint(f'1. {sorted_models[0]}')\nprint(f'2. {sorted_models[1]}')\nprint(f'3. {sorted_models[2]}')\nprint(f'4. {sorted_models[3]}')\nprint(f'4. {sorted_models[4]}')\nprint('------------------------------------------------------------------------')\nprint('------------------------------------------------------------------------')\nprint('')\nprint(f'Conclusion:')\nprint(f'Due to the very biased dataset high accuracy alone cannot be considered as a feasible metric to evaluate the proposed models.')\nprint(f'Even a missclassification of all true positive occasions of strokes would lead to a very high accuracy of >95% since the minority class covers less than 5% of the samples.')\nprint(f'Also when dealing with medical data one should never rely on accurcy or precision only since recall is a very crucial metric provding the necessary insight about the proportion of actual positives identified correctly.')\nprint(f'For that reason to compare the different here proposed models the AUC ROC was determined as a performance measurement for the classification problem.')\nprint(f'Based on that with an AUC ROC of {round(list(auc_dict.values())[0], 2)} the {sorted_models[0]} can be recommended as the best performing model for the here addressed classification task.')\nprint(f'One should keep in mind that with an AUC ROC of {round(list(auc_dict.values())[0], 2)} the model should still be improved before taking it into inference.')\nprint(f'Nevertheless the model can already provdide an assistance to indicate the stroke risk.')","569e439c":"### Conclusion drawn from first inspection \nDataset contains 11 columns. Six of the columns (gender, hypertension, heart_disease, work_type, Residence_type, smoking_status) can be considered as categorical features, three (age, avg_glucose_level, bmi) as numercial features and one column (stroke) contains the target. All data types matching their content. Only one feature column (bmi) contains missing values. The overall objective is to come up with an appropriate model for stroke prediction. Therefore this notebook deals with a binary classification problem.","73c05156":"### Model training using Resampling and Stratified-KFold ","bcaa9d94":"# Initial Data Exploration","13bc979f":"### Conclusion drawn based on visualization of categorical features\nBesides the fact that there is slightly more data of females, with respect to gender there seems to be no significant difference in distribution of the categorical features. From the second parcat plot is gets clear that the dataset is biased with respect to the target variable so that a resampling (downsampling and\/or upsampling) needs to be considered.","71878a98":"## Resampling pipeline for training datasets to compensate huge bias using SMOTE","bd6852b1":"### Impute Numerical Data","51b46331":"### Preparing Dataset with Nominal Encoded Data","8076ec2c":"### Split Data into numerical and categorical data","6f1dc349":"### Model Architecture","7ff6658b":"## XGBoost Model for Binary Classification (using One-Hot Encoded Data)","131a8dcb":"### Model training using Resampling and Stratified-KFold ","d69f23fd":"### Model training using Resampling and Stratified-KFold ","0cbdd908":"### Model Architecture","7fdfb9b5":"### Model Architecture","84889f08":"### Visualization of Categorical Data","ca8f67ca":"### Prepareing Dataset with One Hot Encoded Data","4cbe6b30":"## First Inspection","eec3f34f":"## Checking for missing values","3bfc4c5e":"### Model Architecture","33575e5b":"# Feature Engineering (Feature Imputation, Scaling and Preparation)","79b5a8cf":"# Data Visualization","076b8d21":"# Design, Training and Evaluation of different models","5ab20961":"## Embedding Neural Network","51eab69e":"### Model training with Resampling and Stratified-KFold using nominal encoded and minmax scaled features","f6113346":"### Conclusions drawn based on histograms\nThe distributions of the avg_glucose_level and bmi are following a slightly right-skewed normal distribution (longer right tail). Keeping a higher variance into consideration that seems also true for the age. There is no difference between the genders recognizable. Due to very minor samples the data for other genders is going to be neglected.   ","ccc05a62":"# Model Comparison and Conclusion","9b2c20db":"## Random Forest Classifier for Binary Classification","ff5636f4":"### Visualization of numercial\/continous data","e988321f":"# Data Cleaning","f549b635":"### Conclusion drawn based on correlation and scatter plot\nThe heatmap with respect to the pearson correlation coefficients shows that there are only weak correlation between the avg_glucose_level and age as well as avg_glucose_level and bmi and a moderate correlation between bmi and age. This goes hand in hand with the spearman correlation coefficients which also takes non linear relationships into consideration. The scatter plots indicate a tendency that with increasing age, avg_glucose_level and bmi also the risk for a stroke increases. Furthermore age and avg_glucose_level seems to have a bigger effect on stroke risk.  ","293d97b5":"## Deep Neural Network for Binary Classification (using One-Hot Encoded Data)","ae1f9760":"# Objective\nThe underlying notebook follows the objective to classify whether patients suffers of strokes using the given data. To approach this classification task 4 different models will be trained. In fact, an ordinary neural network, an embedding neural network, a xgboost classifier and random forest will be trained using stratified k-fold cross validation and SMOTE to overcome the challenge ofhandling the heavily biased dataset.      ","a126810d":"### Model training with Resampling and Stratified-KFold using nominal encoded and standard scaled features","486b0a79":"### Scale Numerical Data"}}