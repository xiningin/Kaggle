{"cell_type":{"8c1ef14a":"code","51d27e5d":"code","f1eac21a":"code","e026cccc":"code","498cc812":"code","3e15e2da":"code","5bf8f276":"code","8cba35d9":"code","a84fd917":"code","14f93dc2":"code","1cd58a55":"code","9eb0e38f":"code","26f8a626":"code","7ec3101a":"code","e1a424eb":"code","ede0a6f9":"code","72db7b14":"code","a247b448":"code","bbd399a8":"code","13c5162b":"code","a56b6f2c":"code","a2db2dda":"code","8b439df2":"code","f8524c43":"code","5acc0349":"code","193b6959":"markdown","7e210b98":"markdown","212108c5":"markdown","3bb6044c":"markdown","61743a96":"markdown","ebd3138e":"markdown","23b32e61":"markdown","40698d7f":"markdown","86a3a2fd":"markdown","e3091ce3":"markdown","86049ff2":"markdown","d68af7c0":"markdown","483ff034":"markdown","a8fd958d":"markdown","6dd22a3a":"markdown","ab1db3e8":"markdown","17a18849":"markdown","985181a4":"markdown","1ad07fc1":"markdown","c64c3dbb":"markdown","ae5f4b8e":"markdown","d7e30d62":"markdown","91ea89da":"markdown","893d6ea7":"markdown","a7e657bb":"markdown","d6eeae6c":"markdown","d4de9da4":"markdown","b3d943fc":"markdown","ec8b34ab":"markdown","2fd38a95":"markdown","52301e63":"markdown"},"source":{"8c1ef14a":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow as tf\nimport math\nfrom scipy import special #comb, factorial\nfrom keras import backend as K\nfrom scipy.stats import uniform\nfrom matplotlib import pyplot as plt\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder\nfrom sklearn.metrics import classification_report, roc_auc_score, recall_score, make_scorer, plot_confusion_matrix, confusion_matrix, accuracy_score,f1_score\n\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51d27e5d":"sns.set_style('darkgrid')","f1eac21a":"df = pd.read_csv('\/kaggle\/input\/internet-articles-data-with-users-engagement\/articles_data.csv')\ndf.head()","e026cccc":"df.info()","498cc812":"df.isnull().sum().sort_values(ascending=False)","3e15e2da":"cols_to_remove = ['Unnamed: 0', \n                  'source_id',\n                  'author','url', \n                  'url_to_image',\n                  'description',\n                  'content'\n                 ]\n\ndf.drop(cols_to_remove,axis=1,inplace=True)","5bf8f276":"df.isnull().sum().sort_values(ascending=False)","8cba35d9":"df = df.dropna()\ndf.isnull().sum()","a84fd917":"df.head()","14f93dc2":"df['source_name'].unique()","1cd58a55":"plt.figure(figsize=(10,8))\nsns.countplot(x=df['source_name'],order=df['source_name'].value_counts().index)\nplt.xticks(rotation=45)\nplt.title('Count of articles per each newspaper')\nplt.show()","9eb0e38f":"df['published_at'].min()","26f8a626":"df['published_at'].max()","7ec3101a":"plt.figure(figsize=(10,7))\ndf['published_at'] = pd.to_datetime(df['published_at'])\ndf['published_at'].hist()\nplt.xticks(rotation=45)\nplt.title('Distribution of time the articles were published at')\nplt.show()","e1a424eb":"cont_features = ['engagement_reaction_count', 'engagement_comment_count',\n       'engagement_share_count', 'engagement_comment_plugin_count']\ndf[cont_features].describe().round(2).T","ede0a6f9":"zerov = df[df['engagement_comment_plugin_count'] == 0].shape[0]\nnonzerov = df[df['engagement_comment_plugin_count'] != 0].shape[0]\n\nprint(f'Number of zero values in `engagement_comment_plugin_count`: {zerov}')\nprint(f'Number of non-zero values in `engagement_comment_plugin_count`: {nonzerov}')","72db7b14":"df.drop(['engagement_comment_plugin_count'],axis=1,inplace=True)","a247b448":"cont_features = ['engagement_reaction_count', 'engagement_comment_count',\n       'engagement_share_count']\nWIDTH = 20\nLENGTH = 7\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(1,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    ax[i].hist(df[df[feature] < df[feature].quantile(.75)][feature],alpha=0.6)\n    ax[i].set_title(f'Distribution of a feature `{feature}`')","bbd399a8":"cont_features = ['engagement_reaction_count', \n                 'engagement_comment_count',\n                 'engagement_share_count']\n\ndf1 = df[cont_features]\ncorr=df1.corr()\n\nplt.figure(figsize=(10,7))\nsns.heatmap(corr,\n            xticklabels=df1.columns,\n            yticklabels=df1.columns,\n           annot=True)\nplt.title('Correlation matrix of the continuous features')\nplt.show()","13c5162b":"df1 = df[df['engagement_share_count'] > df['engagement_share_count'].quantile(.75)]\n\nplt.figure(figsize=(10,8))\nsns.countplot(x=df1['source_name'],order=df1['source_name'].value_counts().index)\nplt.xticks(rotation=45)\nplt.title(\"Count of the most shared articles\")\nplt.show()","a56b6f2c":"df1 = df[['source_name','engagement_share_count']].copy()\ndf1['> 0.75'] = df1['engagement_share_count'] > df1['engagement_share_count'].quantile(0.75)\ndf1 = df1.groupby(['source_name','> 0.75']).count()\ndf1['percent'] = df1.groupby(level=0).transform(lambda x: (x \/ x.sum()).round(2))\ndf1.reset_index(inplace=True)\ndf1 = df1[df1['> 0.75'] == True]\n\n\n\nplt.figure(figsize=(12,8))\nsns.barplot(x=df1['source_name'],\n            y=df1['percent'],\n            order=df1.sort_values(by='percent', ascending=False)['source_name'])\nplt.xticks(rotation=45)\nplt.title('Proportion of most shared articles')\nplt.ylabel('Proportion (%)')\nplt.show()","a2db2dda":"df1 = df[df['engagement_reaction_count'] > df['engagement_reaction_count'].quantile(.75)]\n\nplt.figure(figsize=(10,8))\nsns.countplot(x=df1['source_name'],order=df1['source_name'].value_counts().index)\nplt.xticks(rotation=45)\nplt.title(\"Count of the most reacted to articles\")\nplt.show()","8b439df2":"from scipy.stats import f_oneway\n\ncont_features = ['engagement_reaction_count', \n                 'engagement_comment_count',\n                 'engagement_share_count']\n\nlabel = 'source_name'\ndic = {'Categorical': [],\n    'Numerical': [],\n    'p-value': [],\n    'p < 0.05': [],\n    'statistic': []}\n\n\nfor feature in cont_features:\n    values = []\n    for value in df[label].unique():\n        values.append(df[df[label] == value][feature].values)\n    \n    statistic, pval = f_oneway(*values)\n    \n    dic['Categorical'].append(label)\n    dic['Numerical'].append(feature)\n    dic['p-value'].append(pval)\n    dic['p < 0.05'].append(pval<0.05)\n    dic['statistic'].append(statistic)\n\n\npd.DataFrame(dic)","f8524c43":"from wordcloud import WordCloud, STOPWORDS \n\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in df['title']: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","5acc0349":"from wordcloud import WordCloud, STOPWORDS \n\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n  \ndf1 = df[df['engagement_share_count'] > df['engagement_share_count'].quantile(0.75)].copy()\n# iterate through the csv file \nfor val in df1['title']: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","193b6959":"# Importing relevant libraries","7e210b98":"We see that only $0.4\\%$ values in `engagement_comment_plugin_count` are non-zero. Due to the extremely low variance, we will remove this column","212108c5":"We will remove columns that won't be used","3bb6044c":"As expected, we have a very strong positive correlation between features.","61743a96":"# What are the most frequent words in our articles? (all articles)","ebd3138e":"It seems that the `source_name` is indeed strongly correlated with all numerical variables, implying that numerical variables may have significant predictive power (if we are to try to predict `source_name`)","23b32e61":"Now that we cleaned our dataset, we can begin exploring.","40698d7f":"Pattern is almost unchanged. What is interesting to note though, words like \"China\", \"Hong Kong\", \"Brexit\" are more frequent in the top 25% articles (signified by the larger fontsize).","86a3a2fd":"# What are the most frequent words in our articles? (top 25% most shared articles)","e3091ce3":"We see that there are pretty extreme oultiers here (e.g., `engagement_reaction_count`'s 75% percentile is 43, but the max is 354132). Furthermore, we see that most values in `engagement_comment_plugin_count` are zero. Let's check out how many zero values there are.","86049ff2":"# Which newspapers have the most shared articles?\n\nWe define \"one of the most shared articles\" as an article whose share count exceeds 75% percentile.","d68af7c0":"We further drop rows with nulls","483ff034":"# Are continuous features correlated?\n\nWe would expect them to be, but let's check anyways.","a8fd958d":"The earliest article in the dataset was published at:","6dd22a3a":"The result almost agrees with out previous graph (although there some changes (e.g., BBC moving downards and WS journal moving up to top 4))","ab1db3e8":"# Let's look at the continuous features.","17a18849":"Let's look at the distribution","985181a4":"Getting basic info","1ad07fc1":"Let's visualize the distributions of the remaining continuous features using histogram (to make graphs more readable, we will ignore all entries where values exceed 75th percentile)","c64c3dbb":"How many media organizations our dataset took articles from?","ae5f4b8e":"# Which newspapers has the highest proportion of the most shared articles?\n\nThe proportion will be calculated as:\n\n$$\\frac{\\text{Count of most shared articles published by }X}{\\text{Count of all articles published by } X}$$","d7e30d62":"# Which newspapers have the articles with the most user activity?\n\nBy \"user activity\" we mean the value in `engagement_reaction_count`\n\nBy \"most\" we mean that value must exceed 75th percentile.","91ea89da":"# Conclusions:\n\n1. Most articles are about politics.\n2. Distribution of the number of articles published by each newspaper is quite uneven. For example: the dataset contains over 1k articles published by Reuters, yet there are only $82$ articles published by ESPN.\n2. Reuters, BBC news and ABC news have the biggest number of articles in the dataset. Yet the most shared articles are those of CNN, NY times and Reuters. Furthermore, the most reacted to articles are published by NY times, CNN and CBS news.\n3. Distributions of the features `engagement_reaction_count`, `engagement_comment_count`,`engagement_share_count` have very long tails to the right, which implies that most articles have fairly low user activity (few comments, few shares etc.), but some articles are **very** popular (with views,shares exceeding tens of thousands).\n4. `engagement_reaction_count`, `engagement_comment_count`,`engagement_share_count`  have strong positive correlation between each other, in other words: More comments implies more sharing, more shares implies more reactions (and vice versa).\n5. Continuous features `engagement_reaction_count`, `engagement_comment_count`,`engagement_share_count` are not independent from the categorical variable `source_name` (which is just a variable listing the publishers of the articles). That means that if we are to try to predict the publisher of an article, the aforementioned numerical features may have significant predictive power.","893d6ea7":"# Are continuous variables correlated with the `source_name`?\n\nWe will use ANOVA to test independence between each continuous feature and `source_name` (which itself is a categorical variable listing all the publishers)","a7e657bb":"That's an interesting finding: If we consider ALL articles, CNN is on the 4th place; yet if we consider most shared articles, CNN comes on top.","d6eeae6c":"Word cloud gives us a pretty good idea of what most articles are about: politics. \n\nNow let's take a look at the world cloud of the MOST SHARED articles","d4de9da4":"# Let's look at the distribution of the time when the articles were published.","b3d943fc":"The latest article in the dataset was published at:","ec8b34ab":"We see that even after we removed all entries with larger values, the tail still remains.","2fd38a95":"Let's see how many nulls we have","52301e63":"The top 5 remains almost unchanged. Although it is worth noting that when we were considering newspapers with top share count, Reuters was in top 3, but when we consider `engagement_reaction_count`, Reuters drops to the very bottom."}}