{"cell_type":{"70370526":"code","f0d8ce3b":"code","81434f98":"code","fbaaab79":"code","f28b35d4":"code","9aa59272":"code","3f15b34a":"code","63f9e4dd":"code","48567310":"code","684cc85a":"code","68dc54af":"code","fd2976a7":"code","e62dd17f":"code","b7005f0d":"code","23d64a95":"code","59d3abae":"code","192a10f2":"code","30e9bfbe":"code","59623949":"code","952f4f72":"code","79633986":"code","718f1eeb":"markdown","f9b4b233":"markdown","631055fc":"markdown","8660e39a":"markdown","bcb2298c":"markdown","61adaf39":"markdown","ef78ebdf":"markdown","36ec54a2":"markdown","e297cc8b":"markdown","2c409e14":"markdown","27ca57d6":"markdown","91daa07f":"markdown","a64726f8":"markdown","dedd5177":"markdown","e8186910":"markdown","83159cf7":"markdown","7b869012":"markdown","b39a3789":"markdown","d2c03dfe":"markdown","b6f493d3":"markdown","a86d60c3":"markdown","6609c179":"markdown","e11b7075":"markdown","d04df210":"markdown","a35871d4":"markdown"},"source":{"70370526":"import plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport tensorflow\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.metrics import accuracy_score, confusion_matrix","f0d8ce3b":"train = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\nlabels = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","81434f98":"train.head()","fbaaab79":"print(f'Train Size: {len(train)} Test Size: {len(test)}')","f28b35d4":"train_nan = train.isna().values.sum()\ntest_nan = test.isna().values.sum()\nprint(f'Train NaN Count: {train_nan} Test NaN Count: {test_nan}')","9aa59272":"X_train = train.drop('label', axis=1)\ny_train = train['label']\nX_test = test.drop('label', axis=1)\ny_test = test['label']","3f15b34a":"X_train.shape","63f9e4dd":"X_train = X_train.values.reshape(len(X_train), 28, 28, 1)\nX_test = X_test.values.reshape(len(X_test), 28, 28, 1)","48567310":"X_train = X_train.astype(float)\nX_test = X_test.astype(float)\nX_train \/= 255\nX_test \/= 255","684cc85a":"input_shape = X_train[0].shape\ncategory_number = len(np.unique(y_train))\ny_train = tensorflow.keras.utils.to_categorical(y_train, category_number)\ny_test = tensorflow.keras.utils.to_categorical(y_test, category_number)","68dc54af":"X_train.shape","fd2976a7":"grid_w = 5\ngrid_l = 5\nfig, axis = plt.subplots(grid_l, grid_w, figsize=(25, 15))\naxis = axis.ravel()\nfor a in range(grid_l * grid_w):\n    i = np.random.randint(0, len(X_train))\n    axis[a].imshow(X_train[i].reshape((28, 28)), cmap='gray')\n    axis[a].axis('off')\n    axis[a].set_title(labels[y_train.argmax(axis=1)[i]])\nplt.subplots_adjust(hspace=0.8)\nplt.show()","e62dd17f":"model = Sequential()\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2, 2, padding='same'))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2, 2, padding='same'))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2, 2, padding='same'))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.7))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(category_number, activation='softmax'))\nmodel.summary()","b7005f0d":"callback = EarlyStopping(monitor='val_loss', patience=5)\ncheckpoint = ModelCheckpoint('fashion-mnist_model_weights.hdf5', monitor='val_loss', verbose=2, save_best_only=True, mode='min')\noptimizer = Adam(learning_rate=0.0005)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_split=0.18, batch_size=128, epochs=50, shuffle=True, callbacks=[callback, checkpoint])\nhist_df = pd.DataFrame(history.history) ","23d64a95":"model.load_weights('fashion-mnist_model_weights.hdf5')","59d3abae":"fig = go.Figure()\nfig.add_trace(go.Scatter(y=history.history['loss'], mode='lines', name='Train Loss'))\nfig.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines', name='Validation Loss'))\nfig.update_layout(xaxis_title='Epoch', yaxis_title='Loss')\nfig.show()","192a10f2":"fig = go.Figure()\nfig.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines', name='Train Accuracy'))\nfig.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines', name='Validation Accuracy'))\nfig.update_layout(xaxis_title='Epoch', yaxis_title='Accuracy')\nfig.show()","30e9bfbe":"y_test = y_test.argmax(axis=1)\naccuracy_score(y_test, model.predict(X_test).argmax(axis=1))","59623949":"predicted_classes = model.predict(X_test).argmax(axis=1)\n\ncm = confusion_matrix(y_test, predicted_classes)\n\nclass_acc = cm.diagonal() \/ cm.sum(axis=1)\n\nfor i in range(len(class_acc)):\n    print(f\"{labels[i]}: {class_acc[i]}\")","952f4f72":"grid_w = 5\ngrid_l = 5\nfig, axis = plt.subplots(grid_l, grid_w, figsize=(25, 15))\npredicted_classes = model.predict(X_test).argmax(axis=1)\naxis = axis.ravel()\nk=0\nmis_cls=[i for i in range(len(predicted_classes)) if predicted_classes[i]!=y_test[i]]\nfor a in range(grid_l * grid_w):\n    i = mis_cls[k]\n    k+=1\n    axis[a].imshow(X_test[i].reshape((28, 28)), cmap=plt.cm.binary)\n    axis[a].axis('off')\n    axis[a].set_title(f\"Actual: {labels[y_test[i]]}\\n Predicted: {labels[predicted_classes[i]]}\")\nplt.subplots_adjust(hspace=0.8)\nplt.show()","79633986":"# This visualization part is taken from https:\/\/www.tensorflow.org\/tutorials\/keras\/classification#verify_predictions\ndef plot_image(i, predictions_array, true_label, img):\n    true_label, img = true_label[i], img[i]\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n\n    plt.imshow(img.reshape((28, 28)), cmap=plt.cm.binary)\n\n    predicted_label = np.argmax(predictions_array)\n    if predicted_label == true_label:\n        color = 'blue'\n    else:\n        color = 'red'\n\n    plt.xlabel(\"{} {:2.0f}% ({})\".format(labels[predicted_label], 100*np.max(predictions_array), labels[true_label]), color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n    true_label = true_label[i]\n    plt.grid(False)\n    plt.xticks(range(10), labels, rotation=90)\n    plt.yticks([])\n    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n    plt.ylim([0, 1])\n    predicted_label = np.argmax(predictions_array)\n\n    thisplot[predicted_label].set_color('red')\n    thisplot[true_label].set_color('blue')\n\n    \npredictions = model.predict(X_test)\nnum_rows = 4\nnum_cols = 4\nnum_images = num_rows*num_cols\nplt.figure(figsize=(2*2*num_cols, 2*num_rows))\nfor i in range(num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plot_image(i, predictions[i], y_test, X_test)\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plot_value_array(i, predictions[i], y_test)\nplt.tight_layout()\nplt.show()","718f1eeb":"- Each sample consists of 784 pixel\n- First, we need to reshape data into 3d matrices(28, 28, 1).\n- Keras default data_format is channels_last. The last dimension is used for channels. In our case, we have only grayscale images. Therefore last dimension will be 1.","f9b4b233":"**We will use the Simplified Version of the Mini VGG network for classifying data.**\n- For more information about the original VGG, you can look at the [original paper](https:\/\/arxiv.org\/pdf\/1409.1556v6.pdf).","631055fc":"# About Fashion MNIST\n* [Fashion-MNIST](https:\/\/github.com\/zalandoresearch\/fashion-mnist) is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n\n* The dataset contains 10 labels shown in the table\n\n| Label | Description |\n| --- | --- |\n| 0 | T-shirt\/top |\n| 1 | Trouser |\n| 2 | Pullover |\n| 3 | Dress |\n| 4 | Coat |\n| 5 | Sandal |\n| 6 | Shirt |\n| 7 | Sneaker |\n| 8 | Bag |\n| 9 | Ankle boot |","8660e39a":"##  Class-Based accuracy of the test set","bcb2298c":"# Creating Network Architecture","61adaf39":"# Let's plot some misclassified samples","ef78ebdf":"- We will split our data into X and y","36ec54a2":"# Training of the network","e297cc8b":"## Accuracy","2c409e14":"# Conclusion","27ca57d6":"There is no NaN value in our dataset","91daa07f":"**Before using data we will reshape data for the correct input format for the Keras' CNN format and normalize data**","a64726f8":"## Loss","dedd5177":"- We have 60,000 example for training and 10,000 example for testing.","e8186910":"# Plotting Learning Curves","83159cf7":"- Then we will normalize our data. Normalization helps optimization algorithms work fast.","7b869012":"## Accuracy of the test set","b39a3789":"# Loading Data\n### We will import our dataset","d2c03dfe":"# Let's visualize some training data which is chosen randomly","b6f493d3":"- Then we will convert y labels to categorical format.","a86d60c3":"# Data Preprocessing\n- Let's check our dataset for NaN values","6609c179":"## We classified Fashion MNIST using CNN with Keras and get 94% test accuracy.\n### Thanks for reading.","e11b7075":"# Let's visualize some of the predictions with probabilities","d04df210":"# Model Evaluation","a35871d4":"# Let's define our libraries and work on the dataset\n## About Keras\n- We will use Keras library for creating a convolutional neural network and train it.\n- Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.\n- For more information about Keras please visit https:\/\/keras.io\/"}}