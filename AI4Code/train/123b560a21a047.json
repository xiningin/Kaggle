{"cell_type":{"9b692bff":"code","d166b7f3":"code","b4f901cc":"code","a021b428":"code","c4df14b3":"code","9354accd":"code","f1304172":"code","5e9b2452":"code","b216015a":"code","41bdd110":"code","bdeefd69":"code","aa87b750":"code","9468aecc":"code","75540f9c":"code","e26b598f":"code","63713baf":"code","bf6bb67c":"code","4f368a69":"code","cb94ba95":"code","f89dbf5b":"code","6e903ee1":"code","7650a9e4":"code","4881a8cf":"code","3026ad9d":"code","0329f513":"code","053cf8f7":"code","24c795a1":"code","00a16c95":"code","0c77634f":"code","a9a46e25":"code","803a3b9c":"code","f48a4eaf":"code","4c24898b":"code","cb003f22":"code","9bad3fb0":"code","d22a5401":"code","c3b05627":"code","e2d2887a":"code","6ffe4355":"code","da07113f":"code","632ba474":"code","11f5d155":"code","d4358a62":"code","46097857":"code","d1fe5a94":"code","c156acbb":"code","0b865dd3":"code","dd8fb2cd":"code","13dee20f":"code","72a0614d":"code","0aa04acb":"code","45cf27d6":"code","ab6af647":"code","c0009676":"code","9270923f":"code","912b6eb2":"code","3d959a8b":"code","a29c0402":"code","acffebcb":"code","b3f4720f":"code","5c20ca40":"code","bf8cb560":"code","29d601a3":"code","20af678e":"code","14fe7bc5":"code","a41776b3":"code","ad2133ad":"code","0744f73e":"code","20ebda74":"code","5fac993e":"code","d690d2ff":"code","fe221294":"code","681b2948":"code","fa0d42f9":"code","316e3cf2":"code","c3c3a6ad":"code","7c0856a7":"code","3f6a5dcc":"code","f73ab00c":"code","1807fd78":"code","dc03fe5e":"code","8b4e5536":"code","515206be":"code","a460a514":"code","1cca0709":"code","6acb989e":"code","856229ec":"code","0a5a1ae3":"code","be7d7ef4":"code","b84ff890":"code","7afb6309":"code","e3821006":"code","bc85af03":"code","2a6ef800":"code","b2f4f018":"code","45d689a5":"code","1a88034c":"code","6deb7418":"code","0f908719":"code","703802e1":"code","a2e70071":"code","c1b1ffbf":"code","1da8976b":"code","b9dc6994":"code","96286612":"code","288c62cf":"code","0e752234":"markdown","a71792d6":"markdown","3dc30b5b":"markdown","4cc4d9bf":"markdown","7a149378":"markdown","65c0bc5b":"markdown","ed6fd818":"markdown","5083a2d8":"markdown","1e0f6852":"markdown","5b730ad6":"markdown","91aa2ff3":"markdown","d92b5084":"markdown","26495245":"markdown","508fb3da":"markdown","0ea3524e":"markdown","d4ed8c52":"markdown"},"source":{"9b692bff":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport seaborn as sn","d166b7f3":"df=pd.read_csv('..\/input\/car-data\/car data.csv')\ndf.head()","b4f901cc":"df.shape","a021b428":"for i in df.columns:\n    print(i)","c4df14b3":"print(df['Seller_Type'].unique())\nprint(df['Fuel_Type'].unique())\nprint(df['Transmission'].unique())\nprint(df['Owner'].unique())","9354accd":"##check missing values\ndf.isnull().sum()","f1304172":"df.describe()","5e9b2452":"final_dataset=df[['Year','Selling_Price','Present_Price','Kms_Driven','Fuel_Type','Seller_Type','Transmission','Owner']]","b216015a":"final_dataset.head()","41bdd110":"# showing the selling price based on Year\nplt.figure(1, figsize=(8, 6))\nplt.bar(final_dataset.Year,final_dataset.Selling_Price, color='blue',alpha=0.4)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Selling price\")\nplt.show()","bdeefd69":"plt.figure(figsize = (18,5))\nsn.boxplot(data=final_dataset)\nplt.show()","aa87b750":"q1 = final_dataset['Kms_Driven'].quantile(0.25)\nq3 = final_dataset['Kms_Driven'].quantile(0.75)\niqr = q3-q1\n\nUL = q3 + (1.5 * iqr)\nLL = q1 - (1.5 * iqr)\nprint(f\"IQR: {iqr}, UL: {UL}, LL: {LL}\")","9468aecc":"final_dataset[final_dataset['Kms_Driven']>UL]","75540f9c":"plt.figure(figsize = (18,5))\nsn.boxplot(data=final_dataset[final_dataset['Kms_Driven']>UL])\nplt.show()","e26b598f":"#outlier removal from Kms_Driven\n\nfinal_dataset = final_dataset[final_dataset['Kms_Driven']<UL]\nfinal_dataset.head()\n","63713baf":"plt.figure(figsize = (18,5))\nsn.boxplot(data=final_dataset[final_dataset['Kms_Driven']<UL])\nplt.show()","bf6bb67c":"sn.distplot(final_dataset['Year'])","4f368a69":"final_dataset['Current Year']=2020","cb94ba95":"final_dataset.head()","f89dbf5b":"# How many years car is old so that how you can do currentYear subtract Year of buying[Year]\nfinal_dataset['no_year']=final_dataset['Current Year']- final_dataset['Year']","6e903ee1":"final_dataset.head()","7650a9e4":"count_fuelTye = pd.value_counts(final_dataset['Fuel_Type'], sort = True)\n\ncount_fuelTye.plot(kind = 'bar', rot=0,)\n\nplt.title(\"Distribution of Based on Fuel Type\")\n\nplt.xticks(range(3))\n\nplt.xlabel(\"Fuel Type\")\n\nplt.ylabel(\"Frequency Count\")","4881a8cf":"print(final_dataset.Fuel_Type.value_counts())\nax = sn.barplot(x=\"Fuel_Type\", y=\"Selling_Price\", data=final_dataset)","3026ad9d":"count_Transmission = pd.value_counts(final_dataset['Transmission'], sort = True)\n\ncount_Transmission.plot(kind = 'bar', rot=0,)\n\nplt.title(\"Distribution of Transmission\")\n\nplt.xticks(range(3))\n\nplt.xlabel(\"Transmission Type\")\n\nplt.ylabel(\"Frequency Count\")","0329f513":"print(\"How many cars are avaliable Dealer and Individual: \\n\",final_dataset.Seller_Type.value_counts())\nax = sn.barplot(x=\"Seller_Type\", y=\"Selling_Price\", data=final_dataset)","053cf8f7":"# now we dont need to Year and also Current Year feature so drop it\nfinal_dataset.drop(['Year','Current Year'],axis=1,inplace=True)","24c795a1":"final_dataset.head()","00a16c95":"final_dataset=pd.get_dummies(final_dataset,drop_first=True)","0c77634f":"final_dataset.head()","a9a46e25":"final_dataset.corr()","803a3b9c":"import seaborn as sn","f48a4eaf":"sn.pairplot(final_dataset)","4c24898b":"#get correlations of each features in dataset\ncorrmat = final_dataset.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(10,10))\n#plot heat map\ng=sn.heatmap(final_dataset[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","cb003f22":"X=final_dataset.iloc[:,1:]\ny=final_dataset.iloc[:,0]","9bad3fb0":"X['Owner'].unique()","d22a5401":"X.head()","c3b05627":"y.head()","e2d2887a":"### Feature Importance\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesRegressor()\nmodel.fit(X,y)","6ffe4355":"print(model.feature_importances_)","da07113f":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(6).plot(kind='barh')\nplt.show()","632ba474":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","11f5d155":"from sklearn.ensemble import RandomForestRegressor","d4358a62":"regressor=RandomForestRegressor()","46097857":"from sklearn.model_selection import RandomizedSearchCV","d1fe5a94":" #Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]\n","c156acbb":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","0b865dd3":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()","dd8fb2cd":"# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","13dee20f":"# It will take some time for selecting best param\nrf_random.fit(X_train,y_train)","72a0614d":"rf_random.best_estimator_","0aa04acb":"rf_random.best_params_","45cf27d6":"rfr = RandomForestRegressor(max_depth=20, min_samples_leaf=2,\n                      min_samples_split=15, n_estimators=1100)","ab6af647":"model_rf=rfr.fit(X_train,y_train)","c0009676":"predict_=model_rf.predict(X_test)","9270923f":"model_rf.score(X_test,y_test)","912b6eb2":"from sklearn.metrics import r2_score\nR2 = r2_score(y_test,predict_)\nR2","3d959a8b":"# Plotting y_test and predictions to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test,predict_, alpha=.5)\nfig.suptitle('y_test vs predict_', fontsize = 20) \nplt.xlabel('y_test', fontsize = 18)                          \nplt.ylabel('predict_', fontsize = 16) \nplt.show()","a29c0402":"df = pd.DataFrame({'Actual':y_test,\"Predicted\":predict_})\ndf.head()","acffebcb":"# difference between the Actual and predicted value\n\ndf1 = df.head(25)\ndf1.plot(kind='bar',figsize=(10,5))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","b3f4720f":"rf_random.best_score_\n# -4.430592047602699","5c20ca40":"predictions=rf_random.predict(X_test)","bf8cb560":"from sklearn.metrics import r2_score\nR2 = r2_score(y_test,predictions)\nR2\n# 0.8678892897805003","29d601a3":"from sklearn.metrics import accuracy_score","20af678e":"from sklearn.metrics import r2_score\nR2 = r2_score(y_test,predictions)\nR2\n# 0.8678892897805003","14fe7bc5":"df = pd.DataFrame({'Actual':y_test,\"Predicted\":predictions})\ndf.head()","a41776b3":"# difference between the Actual and predicted value\n\ndf1 = df.head(25)\ndf1.plot(kind='bar',figsize=(10,5))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","ad2133ad":"# Plotting y_test and predictions to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, predictions, alpha=.5)\nfig.suptitle('y_test vs y_pred', fontsize = 20) \nplt.xlabel('y_test', fontsize = 18)                          \nplt.ylabel('y_pred', fontsize = 16) \nplt.show()","0744f73e":"rf = y_test-predictions\n# Plot the histogram of the error terms\nfig = plt.figure()\nsn.distplot((rf), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18) ","20ebda74":"sn.distplot(y_test-predictions)","5fac993e":"from sklearn import metrics","d690d2ff":"print('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","fe221294":"# with outliers show result and also no tuning param\n# MAE: 0.8830906595298466\n# MSE: 3.9509103915752966\n# RMSE: 1.987689712096759","681b2948":"from sklearn.ensemble import GradientBoostingRegressor\ngboost_model = GradientBoostingRegressor()","fa0d42f9":"gboost_model.fit(X_train, y_train)","316e3cf2":"y_pred = gboost_model.predict(X_test)","c3c3a6ad":"print(r2_score(y_test, y_pred))","7c0856a7":"sn.distplot(y_test-y_pred)","3f6a5dcc":"fig = plt.figure()\nplt.scatter(y_test, y_pred, alpha=.5)\nfig.suptitle('y_test vs y_pred', fontsize = 20) \nplt.xlabel('y_test', fontsize = 18)                          \nplt.ylabel('y_pred', fontsize = 16) \nplt.show()","f73ab00c":"df_gb = pd.DataFrame({'Actual':y_test,\"Predicted\":y_pred})\ndf_gb.head()","1807fd78":"# difference between the Actual and predicted value\n\ndf_gb = df_gb.head(25)\ndf_gb.plot(kind='bar',figsize=(10,5))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","dc03fe5e":"#Randomized search CV for gradient boosting\n#Number of treers in random forest\nn_estimators =[int(x) for x in np.linspace(100,1200,num = 12)]\n#Learning rate\nlearning_rate = [0.01, 0.02, 0.05, 0.1, 0.2]\nsubsample = [0.05, 0.06, 0.08, 0.09, 0.1]\ncriterion = ['mse', 'rmse', 'friedman_mse']\n#Number of features to consider at every split\nmax_features =[\"auto\", \"sqrt\"]","8b4e5536":"#creating gradient boosting grid\ngb_grid = {'n_estimators' : n_estimators,\n           'learning_rate' : learning_rate,\n           'subsample' : subsample,\n           'max_depth' : max_depth,\n           'max_features' : max_features}\nprint(gb_grid)","515206be":"final_gb_model = RandomizedSearchCV(estimator = gboost_model, param_distributions=gb_grid,\n                                 scoring='neg_mean_squared_error', n_iter = 20,\n                                 cv = 5, verbose = 2, random_state = 42, n_jobs =1)","a460a514":"final_gb_model.fit(X_train,y_train)","1cca0709":"final_gb_model.best_estimator_","6acb989e":"final_gb_model.best_params_","856229ec":"gbr_model = GradientBoostingRegressor(learning_rate=0.05, max_depth=5, max_features='auto',\n                          n_estimators=900, subsample=0.08)","0a5a1ae3":"gbr_model.fit(X_train,y_train)","be7d7ef4":"pred= gbr_model.predict(X_test)","b84ff890":"\nprint(r2_score(y_test, pred))","7afb6309":"gbr_model.score(X_test,y_test)","e3821006":"df_1 = pd.DataFrame({'Actual':y_test,\"Predicted\":pred})\ndf_1.head()","bc85af03":"# difference between the Actual and predicted value\n\ndf_1 = df_1.head(25)\ndf_1.plot(kind='bar',figsize=(10,5))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","2a6ef800":"print('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","b2f4f018":"# Plotting y_test and pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, pred, alpha=.5)\nfig.suptitle('y_test vs pred', fontsize = 20) \nplt.xlabel('y_test', fontsize = 18)                          \nplt.ylabel('pred', fontsize = 16) \nplt.show()","45d689a5":"# Plot the histogram of the error terms\nfig = plt.figure()\nsn.distplot((y_test-pred), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)","1a88034c":"# import pickle\n# # open a file, where you ant to store the data\n# file = open('Gradient_Boosting_Regressor_model.pkl', 'wb')\n\n# # dump information to that file\n# pickle.dump(gboost_model, file)","6deb7418":"import xgboost as xgb","0f908719":"#Fitting XGB regressor \nmodel = xgb.XGBRegressor()\nmodel.fit(X_train,y_train)","703802e1":"xpred = model.predict(X_test)","a2e70071":"plt.plot(X_test, xpred)","c1b1ffbf":"sn.distplot(y_test-xpred)","1da8976b":"plt.scatter(y_test, xpred, alpha=.5)","b9dc6994":"print(r2_score(y_test, xpred))","96286612":"df_x = pd.DataFrame({'Actual':y_test,\"Predicted\":xpred})\ndf_x.head()","288c62cf":"# difference between the Actual and predicted value\n\ndf_x = df_x.head(25)\ndf_x.plot(kind='bar',figsize=(10,5))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","0e752234":"# If you like UPVOTED I would be very happy if you do. If you have any questions, I am ready to answer your questions","a71792d6":"#### putting best parameters for RandomForestRegressor","3dc30b5b":"# Gradient Boosting","4cc4d9bf":"#### Model Score is 90%  RandomForestRegresson \n# putting paramerets","7a149378":"* looking for most important features\n* plotting the graph of feature importance for better visualisation","65c0bc5b":"we have Transmission Feature  in which have two Clesses Manual and Automatic, Manual have High Frequency and on the other side Automatic Very less number of cars  ","ed6fd818":"### Conclusion\n* Gradient Boost gives better RMSE than Random Forest\n* XGB regressor also gives better result \n* we can say XGB and GBR both the same score both better for modeling","5083a2d8":"Above we can see Car sold by the Dealer have Higher Price compar to Individual","1e0f6852":"below, Here we have two Values Actual and Predictes","5b730ad6":"# 96%","91aa2ff3":"### Hyperparameters tuning","d92b5084":"Diesel cars are more expensive then Petrol and CNG, look same Selling price both(Petrol,CNG)","26495245":"### GradientBoostingRegressor  score is 96% GBR model is given better result then RandomForestReg","508fb3da":"## Fitting XGB regressor","0ea3524e":"from the boxplot we can see that kms_Driven has outliers","d4ed8c52":"#### Score is 89% RandomForestReg without param"}}