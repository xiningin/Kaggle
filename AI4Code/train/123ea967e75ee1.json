{"cell_type":{"48721967":"code","b5523857":"code","83868856":"code","f67491aa":"code","42169750":"code","ffc91c7a":"code","70fc5ab0":"code","f203a1b5":"code","7f8f9949":"code","ff85b91e":"code","91097e18":"code","3c6815a8":"code","8301b9a7":"code","3e15e8c1":"code","613dbb01":"code","b484d0f3":"code","67350424":"code","c7a66b9f":"code","da2d55b8":"code","97884656":"code","f0e9111e":"code","9e82fe0a":"code","8324dbfb":"code","8e9532a2":"code","cce98448":"code","245c53ef":"code","554c75bb":"code","8741886d":"code","c97c7197":"code","f724f44e":"code","be77b460":"code","9e63a963":"code","096f6e7f":"code","fa806cdd":"code","36a4eefa":"markdown","e4ff951a":"markdown","f3ef54d1":"markdown","f10679d5":"markdown","7368c8e3":"markdown","89d2e4e5":"markdown","d66e5014":"markdown","55f065f9":"markdown","74cbae99":"markdown","fe6199c2":"markdown","ae8e40b2":"markdown"},"source":{"48721967":"# Importing basic libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","b5523857":"# Import dataset\ndataset = pd.read_csv('..\/input\/train.csv')","83868856":"dataset.head()","f67491aa":"dataset.describe()","42169750":"# Check missing data\ndataset.isnull().sum()","ffc91c7a":"# We will fill median of all values instead of mean because it will be more correct\ndataset['Age'] = dataset['Age'].fillna(dataset['Age'].median())","70fc5ab0":"dataset['Embarked'].value_counts()","f203a1b5":"# Embarked column have 'S' value in large amount than rest of the values.\n# So we will fill 'S' in two empty rows.\ndataset['Embarked'].fillna('S', inplace = True)","7f8f9949":"dataset['Sex'].value_counts()","ff85b91e":"# We will fill 1 for male and 0 for female in our data\ndataset['Sex'] = dataset['Sex'].apply(lambda x: 1 if x=='male' else 0)","91097e18":"# we can't feed Embarked column with its original values.\n# we'll have one-hot-encode this column\n\ncat_columns = ['Embarked'] # categorical column which we'll one-hot-encode.\ndataset = pd.get_dummies(dataset,prefix_sep = '__',  columns = cat_columns)","3c6815a8":"# We'll drop unnecessary columns which doesn't have much impact on our dependent variable\ndataset.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","8301b9a7":"dataset.head()","3e15e8c1":"x = dataset.iloc[:, 1:].values\ny = dataset.iloc[:,:1].values","613dbb01":"# Split data\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2 )","b484d0f3":"N,D = x.shape        #Dimentions of data\nM = 10               #Hidden Units in our neural network","67350424":"# First we'll initialize random weights and bias for our neural network\nw1= np.random.randn(D,M)\/np.sqrt(D+M)\nb1 = np.zeros(M)\nw2 = np.random.randn(M,1)\/np.sqrt(M+1)\nb2 = 0","c7a66b9f":"# We'll use sigmoid because we are classifying our data and result is binary\ndef sigmoid(z):\n    return 1\/(1+np.exp(-z))","da2d55b8":"# We'll use rectifier linear unit as non-linearity.\n# So Differentiation of relu will be required.\n\nd_relu = lambda x: (x>0).astype(x.dtype) ","97884656":"# Feed Forward function for our neural network\n\ndef forward(x,w1,b1,w2,b2):\n    z = np.maximum((x.dot(w1)+b1), 0)     # relu nonlinearity\n    return sigmoid(z.dot(w2)+b2), z       # we'll return both, since output of relu will be required.\n","f0e9111e":"# Defining Cross Entropy for sigmoid\ndef cross_en(T,Y):\n    return -np.mean(T*np.log(Y) + (1-T)*np.log(1-Y))\n","9e82fe0a":"# Empty list, cost will be appended in backpropagation \ncost_train = []\ncost_test = []\n\n#Defining learning rate\nlr = 0.001","8324dbfb":"for i in range(100000):\n    yprep, ztrain = forward(x_train,w1,b1,w2,b2)  \n    train_cost = cross_en(y_train, yprep)\n    \n    yprep_test, ztest = forward(x_test, w1,b1,w2,b2)\n    test_cost = cross_en(y_test, yprep_test)\n    \n    if i%5000==0:  #Every 5000th step cost will be printed \n        print('train_cost = {}'.format(train_cost))\n        print('test_cost = {}'.format(test_cost))\n    \n    E = (yprep-y_train)     # It'll be used many times in loop so we'll define for reducing computation\n    \n    #We'll update weights from right to left\n    w2-= lr*(ztrain.T.dot(E)\/len(x_train))  \n    b2-= lr*(E.sum()\/len(x_train))\n    \n    dz = E.dot(w2.T)*d_relu(ztrain)  # It'll be used multiple times so we'll define once.\n    \n    w1-= lr*(x_train.T.dot(dz)\/len(x_train))\n    b1-= lr*(dz.sum()\/len(x_train))\n    \n    # append cost in list\n    cost_train.append(train_cost)\n    cost_test.append(test_cost)\n    \n","8e9532a2":"#Plotting train and test cost\nplt.plot(cost_train, 'k') #black\nplt.plot(cost_test, 'b') #blue","cce98448":"#Import test dataset\ntest_dataset = pd.read_csv('..\/input\/test.csv')","245c53ef":"test_dataset.head()","554c75bb":"test_dataset.isnull().sum()","8741886d":"#Fill missing data\ntest_dataset['Age']= test_dataset['Age'].fillna(test_dataset['Age'].median())\n\ntest_dataset['Fare']= test_dataset['Fare'].fillna(test_dataset['Fare'].median())","c97c7197":"test_dataset['Sex'] = test_dataset['Sex'].apply(lambda x: 1 if x=='male' else 0)","f724f44e":"# One hot encodint\ntest_dataset = pd.get_dummies(test_dataset,prefix_sep = '__',  columns = cat_columns)","be77b460":"passengerID = test_dataset['PassengerId'] # Passenger Id will be required in our submission\n\ntest_dataset.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","9e63a963":"test_dataset.head()","096f6e7f":"test_data_pred,z_test_data = forward(test_dataset,w1,b1,w2,b2)\n\n#We'll round values because we have probabilities (Sigmoid is used)\ntest_data_pred = (np.round(test_data_pred))\n\nsubmission = pd.DataFrame({'PassengerId': passengerID, 'Survived':test_data_pred[0]}).astype(int)","fa806cdd":"#Our submission\nsubmission.to_csv('submission.csv', index=False)\n#Output file","36a4eefa":"### Dealing with missing Data","e4ff951a":"## Cleaning Data\nOur data is not cleaned, there are many columns which we can't feed into neural network.","f3ef54d1":"we can see that there are some values missing in columns.","f10679d5":"Now we'll split data into training part and testing part, generally we split into 7:3 but I have used 80% data for training and remaining for testing part.","7368c8e3":"Cabin does not have much impact on our dependent variable, so we will ignore that","89d2e4e5":"Test dataset must be in same form as the dataset we had feeded to our neural network","d66e5014":"# Artificial Neural Network in python without any deep learning library\nDeep Learning libraries are fast and makes our code short, but we must know maths behind those one line functions used in libraries like tensorflow, theano, keras etc.\n\n**** Please upvote if found useful****","55f065f9":"## Submitting to Kaggle","74cbae99":"## Backpropagation","fe6199c2":"Now data is ready to feed into neural network","ae8e40b2":"Now data is ready to feed forward"}}