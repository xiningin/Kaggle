{"cell_type":{"6c83c8f1":"code","ca709dcd":"code","54b84382":"code","48d0586c":"code","3dd9bce9":"code","e570c41f":"code","ffd2f2e2":"code","08973c63":"code","5efd154b":"code","b0b43728":"code","9fcc0b8a":"code","cb93d948":"code","29cd6117":"code","fb7603e8":"code","eca0906f":"code","98175f09":"code","d0347cee":"code","e298c475":"code","6f3630f8":"code","cfc9a7f0":"code","2d1d0359":"code","645d8496":"code","816abc88":"code","353c788e":"code","6b77a39c":"code","aef165ec":"code","22063358":"code","e8fbc427":"code","f1072e2a":"code","32f08bb0":"code","7540fca7":"code","9f71b4a3":"code","5846ca4f":"code","16d07f32":"code","e5394607":"code","6b2c2a6c":"code","65f1d69c":"code","ed5e157c":"code","2c856072":"code","333e3d17":"code","b6f9c323":"code","2026c4b0":"code","5a5e5b48":"code","b2d66ddf":"code","0aeb4e7f":"code","b539b774":"markdown","9f1fe7b5":"markdown","7e4899f3":"markdown","9adef36e":"markdown","d40918b0":"markdown","a94cabed":"markdown","8768e441":"markdown","693156bb":"markdown","d6fe3ea1":"markdown","e9efcdfc":"markdown","f896381d":"markdown","cdd2814a":"markdown","4e2e6ad3":"markdown","88c8158a":"markdown","b30474f3":"markdown","5b072e0c":"markdown","038f30a3":"markdown","d7824f32":"markdown","b1c587c2":"markdown","4e234979":"markdown","633ff189":"markdown","ae3377c9":"markdown","a3d31837":"markdown","dcb1a30e":"markdown","eecfd54e":"markdown","49fa7534":"markdown","e002ec0b":"markdown","a5ca631e":"markdown","38d0684e":"markdown","d9914aaf":"markdown","56638849":"markdown","17713970":"markdown","f917d59b":"markdown"},"source":{"6c83c8f1":"# Basic library\nimport numpy as np\nimport pandas as pd\n\nimport time\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ca709dcd":"!pip install lightgbm","54b84382":"# Visualization\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\nimport seaborn as sns\n\n# Time series analysis\nimport statsmodels.api as sm\nfrom pandas.plotting import autocorrelation_plot\n\n# Data preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Library\nimport lightgbm as lgb\n\n# Grid search\nfrom sklearn.model_selection import GridSearchCV\n\n# Validataion\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","48d0586c":"# load master data\ntrain_df = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\")\ncalendar_df = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\")\nprice_df = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv\")","3dd9bce9":"# submission sample data\nsample.head()","e570c41f":"# base training data\ntrain_df.head()","ffd2f2e2":"# item price data\nprice_df.head()","08973c63":"# calendar data\ncalendar_df.head()","5efd154b":"# date object type change\ncalendar_df[\"date_dt\"] = pd.to_datetime(calendar_df[\"date\"])","b0b43728":"print(\"train data:{}\".format(train_df.shape))\nprint(\"calendar data:{}\".format(calendar_df.shape))\nprint(\"price data:{}\".format(price_df.shape))\nprint(\"sample data:{}\".format(sample.shape))","9fcc0b8a":"# data copy\ntrain  = train_df.copy()\nprice = price_df.copy()\ncalendar = calendar_df.copy()","cb93d948":"print(\"*\"*30, \"store_id\", \"*\"*30)\nprint(\"store_id unique value counts:{}\".format(len(price[\"store_id\"].unique())))\nprint(price[\"store_id\"].unique())\n\nprint(\"*\"*30, \"item_id\", \"*\"*30)\nprint(\"item_id unique value counts:{}\".format(len(price[\"item_id\"].unique())))\nprint(price[\"item_id\"].unique())","29cd6117":"print(\"Whole data avarage:{}\".format(price[\"sell_price\"].mean()))\nprint(\"Whole data standard deviation:{}\".format(price[\"sell_price\"].std()))\n\n# Distribution visualization\nplt.figure(figsize=(10,6))\nsns.distplot(price[\"sell_price\"])\nplt.title(\"Price data distribution of whole data\")\nplt.ylabel(\"Frequency\");","fb7603e8":"# price average\npd.DataFrame(data=price.groupby(\"store_id\").sell_price.mean().round(3)).T","eca0906f":"# price standard deviation\npd.DataFrame(data=price.groupby(\"store_id\").sell_price.std().round(3)).T","98175f09":"# box plot\nstore_ca = price[(price[\"store_id\"]=='CA_1') | (price[\"store_id\"]=='CA_2') | (price[\"store_id\"]=='CA_3') | (price[\"store_id\"]=='CA_4')]\nstore_tx = price[(price[\"store_id\"]=='TX_1') | (price[\"store_id\"]=='TX_2') | (price[\"store_id\"]=='TX_3')]\nstore_wi = price[(price[\"store_id\"]=='WI_1') | (price[\"store_id\"]=='WI_2') | (price[\"store_id\"]=='WI_3')]\n\nfig, ax = plt.subplots(1, 3, figsize=(20, 6))\nstore_df = [store_ca, store_tx, store_wi]\n\nfor i in range(len(store_df)):\n    sns.boxplot(x=\"store_id\", y=\"sell_price\", data=store_df[i], ax=ax[i])\n    ax[i].set_ylabel(\"Price\")","d0347cee":"# Number of events per year\ncalendar[['year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']].groupby(\"year\").count()","e298c475":"# Number of flags per year by store\ncalendar[['year', 'snap_CA', 'snap_TX', 'snap_WI']].groupby(\"year\").sum()","6f3630f8":"print(\"*\"*30, \"item_id\", \"*\"*30)\nprint(\"item_id unique value counts:{}\".format(len(train[\"item_id\"].unique())))\nprint(train[\"item_id\"].unique())\n\nprint(\"*\"*30, \"dept_id\", \"*\"*30)\nprint(\"dept_id unique value counts:{}\".format(len(train[\"dept_id\"].unique())))\nprint(train[\"store_id\"].unique())\n\nprint(\"*\"*30, \"cat_id\", \"*\"*30)\nprint(\"cat_id unique value counts:{}\".format(len(train[\"cat_id\"].unique())))\nprint(train[\"cat_id\"].unique())\n\nprint(\"*\"*30, \"state_id\", \"*\"*30)\nprint(\"state_id unique value counts:{}\".format(len(train[\"state_id\"].unique())))\nprint(train[\"state_id\"].unique())","cfc9a7f0":"calendar","2d1d0359":"# sample\nsample_item = train.loc[:, \"d_1\":].T\nsample_item = pd.merge(sample_item, calendar, left_index=True, right_on=\"d\", how=\"left\").set_index(\"date_dt\")\n\n# Visualization\nfig, ax = plt.subplots(4,1, figsize=(15,20))\nplt.subplots_adjust(hspace=0.4)\n\ncolor=[\"magenta\", \"cyan\", \"lightgreen\", \"gray\"]\nsample_col = [0, 100, 1000, 10000]\n\n\nfor i in range(len(sample_col)):\n    ax[i].plot(sample_item.index, sample_item[sample_col[i]], color=color[i], linewidth=0.5)\n    # Rolling\n    ax[i].plot(sample_item.index, sample_item[sample_col[i]].rolling(3).mean(), color=\"white\", linewidth=1)\n    ax[i].plot(sample_item.index, sample_item[sample_col[i]].rolling(28).mean(), color=color[i], linewidth=2, linestyle='--')\n    ax[i].set_xlabel(\"datetime\")\n    ax[i].set_ylabel(\"Sales volume\")\n    ax[i].legend([\"{}\".format(sample_col[i]), \"Rolling 3 days\", \"Rolling 28 days\"])\n    ax[i].set_title(\"{}\".format(sample_col[i]))","645d8496":"# Create dataframe by grouping\nstate_group = train.groupby(\"state_id\").sum().T\nstate_group = pd.merge(state_group, calendar, left_index=True, right_on=\"d\", how=\"left\").set_index(\"date_dt\")\n\n# Visualization\nfig, ax = plt.subplots(3,1, figsize=(15,15))\nplt.subplots_adjust(hspace=0.4)\n\ncolor=[\"magenta\", \"cyan\", \"lightgreen\"]\nstate_col = train[\"state_id\"].unique()\n\nfor i in range(len(state_col)):\n    ax[i].plot(state_group.index, state_group[state_col[i]], color=color[i], linewidth=0.5)\n    # Rolling\n    ax[i].plot(state_group.index, state_group[state_col[i]].rolling(28).mean(), color=color[i], linewidth=2)\n    ax[i].set_xlabel(\"datetime\")\n    ax[i].set_ylabel(\"Sales volume\")\n    ax[i].legend([\"{}\".format(state_col[i]), \"Rolling 28 days\"])\n    ax[i].set_title(\"{}\".format(state_col[i]))","816abc88":"# Sales volume per year\nstate_group.groupby(\"year\")['CA', 'TX', 'WI'].sum().plot()\nplt.title(\"Sales volume per year\")\nplt.ylabel(\"Sales volume\");","353c788e":"# Xmas\nXmas_date = [pd.datetime(2011,12,25), pd.datetime(2012,12,25), pd.datetime(2013,12,25), pd.datetime(2014,12,25), pd.datetime(2015,12,25)]\n\n# Drop Xmas date\nstate_group.drop(Xmas_date, inplace=True)","6b77a39c":"# Define time series analysis function\ndef plot_ts_decomp(data, col, lag, color):\n    print(\"Analised Data:{}\".format(col.upper()))\n    # Stats model\n    res = sm.tsa.seasonal_decompose(data[col], period=lag)\n    data[\"trend\"] = res.trend\n    data[\"seaso\"] = res.seasonal\n    data[\"resid\"] = res.resid\n    \n    # Visualization\n    fig = plt.figure(figsize=(20,15))\n    grid = plt.GridSpec(4,2, hspace=0.4, wspace=0.2)\n    ax1 = fig.add_subplot(grid[0,0])\n    ax2 = fig.add_subplot(grid[1,0])\n    ax3 = fig.add_subplot(grid[2,0])\n    ax4 = fig.add_subplot(grid[3,0])\n    ax5 = fig.add_subplot(grid[:-2,1])\n    ax6 = fig.add_subplot(grid[2:,1])\n    \n    # raw price data\n    ax1.plot(data.index, data[col], label=\"price of {}\".format(col), color=color, linewidth=0.5)\n    ax1.plot(data.index, data[col].rolling(lag\/\/12).mean(), label=\"Rolling {}\".format(lag\/\/12), color=color, linewidth=2)\n    ax1.set_xlabel(\"date\")\n    ax1.set_ylabel(\"price\")\n    ax1.set_title(\"raw data\")\n    ax1.legend()\n    # trend\n    ax2.plot(data.index, data[\"trend\"], label=\"trend of {}\".format(col), color=color, linewidth=3)\n    ax2.set_xlabel(\"date\")\n    ax2.set_ylabel(\"trend\")\n    ax2.set_title(\"trend\")\n    ax2.legend()\n    # seasonaly\n    ax3.plot(data.index, data[\"seaso\"], label=\"seasonaly of {}\".format(col), color=color, linewidth=0.5)\n    ax3.set_xlabel(\"date\")\n    ax3.set_ylabel(\"seasonaly\")\n    ax3.set_title(\"seasonaly\")\n    ax3.legend()\n    # residual\n    ax4.plot(data.index, data[\"resid\"], label=\"residual error of {}\".format(col), color=color, linewidth=0.5)\n    ax4.set_xlabel(\"date\")\n    ax4.set_ylabel(\"residual error\")\n    ax4.set_title(\"residual\")\n    ax4.legend()\n    # distribution\n    sns.distplot(data[col], ax=ax5)\n    ax5.set_xlabel(\"Price\")\n    ax5.set_ylabel(\"Frequency\")\n    ax5.set_title(\"distribution\")\n    # auto correlation\n    autocorrelation_plot(data[col], ax=ax6, linewidth=0.5)\n    ax6.set_title(\"autocorrelation\")","aef165ec":"# CA of state\nplot_ts_decomp(state_group, \"CA\", 365, \"aqua\");","22063358":"# TX of state\nplot_ts_decomp(state_group, \"TX\", 365, \"aqua\");","e8fbc427":"# WI of state\nplot_ts_decomp(state_group, \"WI\", 365, \"aqua\");","f1072e2a":"# Create dataframe by grouping\ncate_group = train.groupby(\"cat_id\").sum().T\ncate_group = pd.merge(cate_group, calendar, left_index=True, right_on=\"d\", how=\"left\").set_index(\"date_dt\")\n\n# Drop Xmas date\ncate_group.drop(Xmas_date, inplace=True)\n\n# Visualization\nfig, ax = plt.subplots(3,1, figsize=(15,15))\nplt.subplots_adjust(hspace=0.4)\n\ncolor=[\"magenta\", \"cyan\", \"lightgreen\"]\ncate_col = train[\"cat_id\"].unique()\n\nfor i in range(len(cate_col)):\n    ax[i].plot(cate_group.index, cate_group[cate_col[i]], color=color[i], linewidth=0.5)\n    # Rolling\n    ax[i].plot(cate_group.index, cate_group[cate_col[i]].rolling(28).mean(), color=color[i], linewidth=2)\n    ax[i].set_xlabel(\"datetime\")\n    ax[i].set_ylabel(\"Sales volume\")\n    ax[i].legend([\"{}\".format(cate_col[i]), \"Rolling 28 days\"])\n    ax[i].set_title(\"{}\".format(cate_col[i]))","32f08bb0":"# Sales volume per year\ncate_group.groupby(\"year\")['FOODS', 'HOBBIES', 'HOUSEHOLD'].sum().plot()\nplt.title(\"Sales volume per year\")\nplt.ylabel(\"Sales volume\");","7540fca7":"# HOBBIES of cat\nplot_ts_decomp(cate_group, \"HOBBIES\", 365, \"aqua\");","9f71b4a3":"# HOUSEHOLD of cat\nplot_ts_decomp(cate_group, \"HOUSEHOLD\", 365, \"aqua\");","5846ca4f":"# FOODS of cat\nplot_ts_decomp(cate_group, \"FOODS\", 365, \"aqua\");","16d07f32":"# Define function\ndef lag_featrues(df):\n    out_df = df[['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]\n    ###############################################################################\n    # day lag 29~57 day and last year's day lag 1~28 day \n    day_lag = df.iloc[:,-28:]\n    day_year_lag = df.iloc[:,-393:-365]\n    day_lag.columns = [str(\"lag_{}_day\".format(i)) for i in range(29,57)] # Rename columns\n    day_year_lag.columns = [str(\"lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    \n    # Rolling mean(3) and (7) and (28) and (84) 29~57 day and last year's day lag 1~28 day \n    rolling_3 = df.iloc[:,-730:].T.rolling(3).mean().T.iloc[:,-28:]\n    rolling_3.columns = [str(\"rolling3_lag_{}_day\".format(i)) for i in range(29,57)] # Rename columns\n    rolling_3_year = df.iloc[:,-730:].T.rolling(3).mean().T.iloc[:,-393:-365]\n    rolling_3_year.columns = [str(\"rolling3_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    \n    rolling_7 = df.iloc[:,-730:].T.rolling(7).mean().T.iloc[:,-28:]\n    rolling_7.columns = [str(\"rolling7_lag_{}_day\".format(i)) for i in range(29,57)] # Rename columns\n    rolling_7_year = df.iloc[:,-730:].T.rolling(7).mean().T.iloc[:,-393:-365]\n    rolling_7_year.columns = [str(\"rolling7_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    \n    rolling_28 = df.iloc[:,-730:].T.rolling(28).mean().T.iloc[:,-28:]\n    rolling_28.columns = [str(\"rolling28_lag_{}_day\".format(i)) for i in range(29,57)]\n    rolling_28_year = df.iloc[:,-730:].T.rolling(28).mean().T.iloc[:,-393:-365]\n    rolling_28_year.columns = [str(\"rolling28_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    \n    rolling_84 = df.iloc[:,-730:].T.rolling(84).mean().T.iloc[:,-28:]\n    rolling_84.columns = [str(\"rolling84_lag_{}_day\".format(i)) for i in range(29,57)]\n    rolling_84_year = df.iloc[:,-730:].T.rolling(84).mean().T.iloc[:,-393:-365]\n    rolling_84_year.columns = [str(\"rolling84_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    \n    # monthly lag 1~18 month\n    month_lag = pd.DataFrame({})\n    for i in range(1,19):\n        if i == 1:\n            monthly = df.iloc[:,-28*i:].T.sum().T\n            month_lag[\"monthly_lag_{}_month\".format(i)] = monthly\n        else:\n            monthly = df.iloc[:, -28*i:-28*(i-1)].T.sum().T\n            month_lag[\"monthly_lag_{}_month\".format(i)] = monthly\n            \n    # combine day lag and monthly lag\n    out_df = pd.concat([out_df, day_lag], axis=1)\n    out_df = pd.concat([out_df, day_year_lag], axis=1)\n    out_df = pd.concat([out_df, rolling_3], axis=1)\n    out_df = pd.concat([out_df, rolling_3_year], axis=1)\n    out_df = pd.concat([out_df, rolling_7], axis=1)\n    out_df = pd.concat([out_df, rolling_7_year], axis=1)\n    out_df = pd.concat([out_df, rolling_28], axis=1)\n    out_df = pd.concat([out_df, rolling_28_year], axis=1)\n    out_df = pd.concat([out_df, rolling_84], axis=1)\n    out_df = pd.concat([out_df, rolling_84_year], axis=1)\n    out_df = pd.concat([out_df, month_lag], axis=1)\n    \n    ###############################################################################\n    # dept_id\n    group_dept = df.groupby(\"dept_id\").sum()\n    # day lag 29~57 day and last year's day lag 1~28 day \n    dept_day_lag = group_dept.iloc[:,-28:]\n    dept_day_year_lag = group_dept.iloc[:,-393:-365]\n    dept_day_lag.columns = [str(\"dept_lag_{}_day\".format(i)) for i in range(29,57)]\n    dept_day_year_lag.columns = [str(\"dept_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    # monthly lag 1~18 month\n    month_dept_lag = pd.DataFrame({})\n    for i in range(1,19):\n        if i == 1:\n            monthly_dept = group_dept.iloc[:,-28*i:].T.sum().T\n            month_dept_lag[\"dept_monthly_lag_{}_month\".format(i)] = monthly_dept\n        elif i >= 7 and i < 13:\n            continue\n        else:\n            monthly = group_dept.iloc[:, -28*i:-28*(i-1)].T.sum().T\n            month_dept_lag[\"dept_monthly_lag_{}_month\".format(i)] = monthly_dept\n    # combine out df\n    out_df = pd.merge(out_df, dept_day_lag, left_on=\"dept_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, dept_day_year_lag, left_on=\"dept_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, month_dept_lag, left_on=\"dept_id\", right_index=True, how=\"left\")\n    \n    ###############################################################################       \n    # cat_id\n    group_cat = df.groupby(\"cat_id\").sum()\n    # day lag 29~57 day and last year's day lag 1~28 day \n    cat_day_lag = group_cat.iloc[:,-28:]\n    cat_day_year_lag = group_cat.iloc[:,-393:-365]\n    cat_day_lag.columns = [str(\"cat_lag_{}_day\".format(i)) for i in range(29,57)]\n    cat_day_year_lag.columns = [str(\"cat_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    # monthly lag 1~18 month\n    month_cat_lag = pd.DataFrame({})\n    for i in range(1,19):\n        if i == 1:\n            monthly_cat = group_cat.iloc[:,-28*i:].T.sum().T\n            month_cat_lag[\"cat_monthly_lag_{}_month\".format(i)] = monthly_cat\n        elif i >= 7 and i < 13:\n            continue\n        else:\n            monthly_cat = group_cat.iloc[:, -28*i:-28*(i-1)].T.sum().T\n            month_cat_lag[\"dept_monthly_lag_{}_month\".format(i)] = monthly_cat\n            \n    # combine out df\n    out_df = pd.merge(out_df, cat_day_lag, left_on=\"cat_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, cat_day_year_lag, left_on=\"cat_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, month_cat_lag, left_on=\"cat_id\", right_index=True, how=\"left\")\n    \n    ###############################################################################\n    # store_id\n    group_store = df.groupby(\"store_id\").sum()\n    # day lag 29~57 day and last year's day lag 1~28 day \n    store_day_lag = group_store.iloc[:,-28:]\n    store_day_year_lag = group_store.iloc[:,-393:-365]\n    store_day_lag.columns = [str(\"store_lag_{}_day\".format(i)) for i in range(29,57)]\n    store_day_year_lag.columns = [str(\"store_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    # monthly lag 1~18 month\n    month_store_lag = pd.DataFrame({})\n    for i in range(1,19):\n        if i == 1:\n            monthly_store = group_store.iloc[:,-28*i:].T.sum().T\n            month_store_lag[\"store_monthly_lag_{}_month\".format(i)] = monthly_store\n        elif i >= 7 and i <13:\n            continue\n        else:\n            monthly_store = group_store.iloc[:, -28*i:-28*(i-1)].T.sum().T\n            month_store_lag[\"store_monthly_lag_{}_month\".format(i)] = monthly_store\n            \n    # combine out df\n    out_df = pd.merge(out_df, store_day_lag, left_on=\"store_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, store_day_year_lag, left_on=\"store_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, month_store_lag, left_on=\"store_id\", right_index=True, how=\"left\")\n    \n    ###############################################################################\n    # state_id\n    group_state = df.groupby(\"state_id\").sum()\n    # day lag 29~57 day and last year's day lag 1~28 day \n    state_day_lag = group_state.iloc[:,-28:]\n    state_day_year_lag = group_state.iloc[:,-393:-365]\n    state_day_lag.columns = [str(\"state_lag_{}_day\".format(i)) for i in range(29,57)]\n    state_day_year_lag.columns = [str(\"state_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    # monthly lag 1~18 month\n    month_state_lag = pd.DataFrame({})\n    for i in range(1,13):\n        if i == 1:\n            monthly_state = group_state.iloc[:,-28*i:].T.sum().T\n            month_state_lag[\"state_monthly_lag_{}_month\".format(i)] = monthly_state\n        elif i >= 7 and i < 13:\n            continue\n        else:\n            monthly_state = group_state.iloc[:, -28*i:-28*(i-1)].T.sum().T\n            month_state_lag[\"state_monthly_lag_{}_month\".format(i)] = monthly_state\n            \n    # combine out df\n    out_df = pd.merge(out_df, state_day_lag, left_on=\"state_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, state_day_year_lag, left_on=\"state_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, month_state_lag, left_on=\"state_id\", right_index=True, how=\"left\")\n    \n    ###############################################################################\n    # category flag\n    col_list = ['dept_id', 'cat_id', 'store_id', 'state_id']\n    \n    df_cate_oh = pd.DataFrame({})\n    for i in col_list:\n        df_oh = pd.get_dummies(df[i])\n        df_cate_oh = pd.concat([df_cate_oh, df_oh], axis=1)\n        \n    out_df = pd.concat([out_df, df_cate_oh], axis=1)\n    \n    return out_df","e5394607":"%%time\n# Features\nTrain_data = train.iloc[:,:-56]\nVal_data = train.iloc[:,:-28]\n\nX_train = lag_featrues(Train_data).iloc[:,5:] # select variables\ny_train = train.iloc[:,-56]\nX_test = lag_featrues(Val_data).iloc[:,5:]\ny_test = train.iloc[:,-28]\n\n# Create instance\nlgbm = lgb.LGBMRegressor()\n\n# Training and score\nlearning_rate = [0.15, 0.2, 0.25]\nmax_depth = [15, 20, 25]\n\nparam_grid = {'learning_rate': learning_rate, 'max_depth': max_depth}\n\n# Fitting\ncv_lgbm = GridSearchCV(lgbm, param_grid, cv=10, n_jobs =1)\ncv_lgbm.fit(X_train, y_train)\n\nprint(\"Best params:{}\".format(cv_lgbm.best_params_))\n\n# best params\nbest_lg = cv_lgbm.best_estimator_\n\n# prediction\ny_train_pred_lg = best_lg.predict(X_train)\ny_test_pred_lg = best_lg.predict(X_test)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_lg)))\nprint(\"MSE test;{}\".format(mean_squared_error(y_test, y_test_pred_lg)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_lg)))\nprint(\"R2 score test:{}\".format(r2_score(y_test, y_test_pred_lg)))","6b2c2a6c":"# Feature importance\nimportance = best_lg.feature_importances_\n\nindices = np.argsort(importance)[::-1]\n\n# print importance\nimportance_df = pd.DataFrame({})\ncolumns = []\nimportance_ = []\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %.2f\" %(f+1, 30, X_train.columns[indices[f]], importance[indices[f]]))\n    col = X_train.columns[indices[f]]\n    imp = importance[indices[f]]\n    columns.append(col)\n    importance_.append(imp)\nimportance_df[\"col_name\"] = columns\nimportance_df[\"importance\"] = importance_","65f1d69c":"# Visualization of prediction and test data\nplt.figure(figsize=(10,10))\nplt.scatter(y_train, y_train_pred_lg, s=10, color=\"aqua\", label=\"train data prediction\", alpha=0.5)\nplt.scatter(y_test, y_test_pred_lg, s=10, color=\"pink\", label=\"test data prediction\",  alpha=0.5)\nplt.xlabel(\"y real data\")\nplt.xlim([-5,130])\nplt.ylim([-5,130])\nplt.ylabel(\"y_predictin data\")\nplt.title(\"Prediction data plot\")","ed5e157c":"%%time\n# importance columns (>0)\nimp_col = importance_df[importance_df[\"importance\"]>0][\"col_name\"].values\n\n# Train test split, select by imp_col\n\nX_train = lag_featrues(Train_data).iloc[:,5:][imp_col] # select variables\ny_train = train.iloc[:,-56]\nX_test = lag_featrues(Val_data).iloc[:,5:][imp_col]\ny_test = train.iloc[:,-28]\n\n# Create instance\nlgbm = lgb.LGBMRegressor()\n\n# Training and score\nlearning_rate = [0.15, 0.2, 0.25]\nmax_depth = [15, 20, 25]\n\nparam_grid = {'learning_rate': learning_rate, 'max_depth': max_depth}\n\n# Fitting\ncv_lgbm = GridSearchCV(lgbm, param_grid, cv=10, n_jobs =1)\ncv_lgbm.fit(X_train, y_train)\n\nprint(\"Best params:{}\".format(cv_lgbm.best_params_))\n\n# best params\nbest_lg = cv_lgbm.best_estimator_\n\n# prediction\ny_train_pred_lg = best_lg.predict(X_train)\ny_test_pred_lg = best_lg.predict(X_test)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_lg)))\nprint(\"MSE test;{}\".format(mean_squared_error(y_test, y_test_pred_lg)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_lg)))\nprint(\"R2 score test:{}\".format(r2_score(y_test, y_test_pred_lg)))","2c856072":"%%time\n# Prediction last 28 days by roop\n\ndef lgbm_pred(X_train, y_train, X_test):\n    lgbm = lgb.LGBMRegressor(learning_rate=0.2, max_depth=20)\n    # Fitting\n    lgbm.fit(X_train, y_train)\n    # prediction\n    y_pred = lgbm.predict(X_test)\n    return y_pred\n\n# Features\n# Features\nTrain_data = train.iloc[:,:-56]\nY_train = train.iloc[:,-56:-28]\nVal_data = train.iloc[:,:-28]\nY_test = train.iloc[:,-28:]\n\nPred_data = pd.DataFrame({})\nfor d in range(0,28):\n    if d == 0:\n        X_train = lag_featrues(Train_data).iloc[:,5:][imp_col] # select variables\n        y_train = Y_train.iloc[:,d]\n        X_test = lag_featrues(Val_data).iloc[:,5:][imp_col]\n        \n        # Train test split, select by imp_col\n        pred = lgbm_pred(X_train, y_train, X_test)\n        Pred_data[\"pred_{}_day\".format(1+d)] = pred\n    else:\n        X_train = lag_featrues(Train_data.iloc[:,:-1])[imp_col]\n        y_train = Y_train.iloc[:,d]\n        X_test = lag_featrues(Train_data)[imp_col][imp_col]\n        \n        # Train test split, select by imp_col\n        pred = lgbm_pred(X_train, y_train, X_test)\n        Pred_data[\"pred_{}_day\".format(1+d)] = pred","333e3d17":"# Confirming prediction result\nMSE = []\nR2_score = []\n\nfor i in range(Y_test.shape[1]):\n    mse = mean_squared_error(Y_test.iloc[:,i], Pred_data.iloc[:,i])\n    r2 = r2_score(Y_test.iloc[:,i], Pred_data.iloc[:,i])\n    MSE.append(mse)\n    R2_score.append(r2)\n\nmonth_pred_score = pd.DataFrame({\"day\":range(1,29),\n                                 \"MSE\":MSE,\n                                 \"R2\":R2_score})\n\n# Visualization check\nfig, ax = plt.subplots(1,2, figsize=(20, 6))\n\nax[0].bar(month_pred_score[\"day\"], month_pred_score[\"MSE\"])\nax[0].set_xlabel(\"Prediction day\")\nax[0].set_ylabel(\"MSE\")\n\nax[1].bar(month_pred_score[\"day\"], month_pred_score[\"R2\"])\nax[1].set_xlabel(\"Prediction day\")\nax[1].set_ylabel(\"R2 score\")","b6f9c323":"%%time\n# Prediction next 56 days by roop\n\n# Features\nTrain_data = train.iloc[:,:-28]\nTest_data = train\nY_train = train.iloc[:,-28:]\n\nPred_data_val = pd.DataFrame({})\nPred_data_eval = pd.DataFrame({})\n\nfor d in range(0,28):\n    X_train = lag_featrues(Train_data)[imp_col] # select variables\n    y_train = Y_train.iloc[:,d]\n    X_test = lag_featrues(Test_data)[imp_col]\n        \n    # Train test split, select by imp_col\n    pred = lgbm_pred(X_train, y_train, X_test)\n    Pred_data_val[\"pred_{}_day\".format(1+d)] = pred","2026c4b0":"Train_data = train\nTest_data = pd.concat([train, Pred_data_val], axis=1)\nY_train = Pred_data_val\n\nfor d in range(0,28):\n    X_train = lag_featrues(Train_data)[imp_col]\n    y_train = Y_train.iloc[:,d]\n    X_test = lag_featrues(Test_data)[imp_col]\n        \n    # Train test split, select by imp_col\n    pred = lgbm_pred(X_train, y_train, X_test)\n    Pred_data_eval[\"pred_{}_day\".format(1+d)] = pred","5a5e5b48":"Pred_data_val.columns = sample.loc[:,\"F1\":].columns\nPred_data_eval.columns = sample.loc[:,\"F1\":].columns\n\n# concat val data and eval data\nsubmit_data = pd.concat([Pred_data_val, Pred_data_eval])\nsubmit_data = submit_data.reset_index().drop(\"index\", axis=1)\n\ncol = sample.loc[:,'F1':].columns\n\nsample[col] = submit_data\n\n# Submission data\nsample.to_csv(\"submission.csv\", index=False)\n\n# Submission data\nsample.round(0).to_csv(\"submission_r.csv\", index=False)","b2d66ddf":"# Predictin of validataion\nsample.iloc[:30490,:].describe()","0aeb4e7f":"# Predictin of evaluation\nsample.iloc[30490:,:].describe()","b539b774":"Lag features","9f1fe7b5":"# M5 forecasting, EDA and LGBM prediction","7e4899f3":"# Submission data prediction","9adef36e":"### Data relationships","d40918b0":"# Price data EDA","a94cabed":"Predict using only variables with an importance of 1 or higher.","8768e441":"Unique count of Categorical values","693156bb":"Price distribution by store id","d6fe3ea1":"### --------------------------------------------------------------------\n## Under study ) Accuracy improvement approach \n(1) Flag the day of the weekday and holidays according to the target data.<br>\n(2) Review of important features<br>\n(3) Noise elimination of lag feature amount. Residuals are generated using statsmodel, and lag features are created from the time series data from which the residuals have been subtracted.","e9efcdfc":"# Training data EDA","f896381d":"Before decomposition, looking at each data, we can see that the sales of one day fell sharply and cut. It is speculated that this is because it is taking a break at Christmas. This data will be a strong noise, so this time it will be deleted.","cdd2814a":"# Next steps\nTry to improve accuracy.<br>\n\nReview the features and create new features to improve accuracy.<br>\nAdjust parameters to improve accuracy.","4e2e6ad3":"# Calendar data EDA","88c8158a":"### By cat_id, Count of selling time series analysis","b30474f3":"![Entity%20Relationship%20Diagram,%20M5%20forecast.jpg](attachment:Entity%20Relationship%20Diagram,%20M5%20forecast.jpg)","5b072e0c":"## Time series EDA by each categorical values","038f30a3":"## test prediction by LGBM and feature importance check","d7824f32":"price distribution","b1c587c2":"# Prediction with LGBM\nCreate features and make predictions with LGBM.","4e234979":"## Model training data plan","633ff189":"## Next, I will try\nPrediction with LGBM model\nDetermine the features to create.","ae3377c9":"# Predict sales for next 2 month.\n1st, predict with train data next 28days.\n2nd, predict with train data and 1st prediction data next 28days.","a3d31837":"- The feature quantity to be created is a lag feature quantity for each item and category feature quantity.<br>\n- Get lag data for each item or category. The acquisition target is the data for the previous year in the same month as the first month of the month by date, the data for the previous 6 months prior to the current month, and the data for 6 months before the same month in the previous year.<br>\n- Results are evaluated by the prediction result of the first date and selected by selecting important feature quantities.<br>\n- Define the obtained results with new data points, reconstruct the model, make a forecast for the next day, and repeat this to obtain the forecast results for January.<br>","dcb1a30e":"### By state_id, Count of selling time series analysis","eecfd54e":"Unique value counts","49fa7534":"### data check","e002ec0b":"### sample item id, Count of selling time series analysis","a5ca631e":"![train%20test%20data%20plan,%20M5%20forecast.jpg](attachment:train%20test%20data%20plan,%20M5%20forecast.jpg)","38d0684e":"### Libraries","d9914aaf":"# Data loading and checking","56638849":"### data size","17713970":"### Component decomposition and autocorrelation","f917d59b":"### --------------------------------------------------------------------"}}