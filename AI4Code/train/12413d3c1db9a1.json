{"cell_type":{"0ca74930":"code","2608e36d":"code","51b8bb09":"code","d56d7e37":"code","04d80295":"code","6f003093":"code","ec7d31bc":"code","83e53cc6":"code","76262ae0":"code","992e7df3":"code","80e7d262":"code","50552bc6":"code","43bd9d5f":"code","3347d916":"code","4f1e21b3":"code","e6379f60":"code","ade935ec":"code","54b3ac55":"code","99c1a716":"code","5efd510b":"code","d1eb6720":"code","90d0ff65":"code","fc3e8a70":"code","c8aa828c":"code","674f7263":"code","9c015242":"code","a8f48456":"code","eac4c4ef":"code","b1dc322f":"code","269b2bf2":"code","0b55d5cd":"code","34d2dbe2":"code","894e280e":"code","45c7c3bb":"code","b966ac26":"code","f43f76ee":"code","4b5d0505":"code","e4c99336":"code","16f579d4":"code","f47be32a":"code","ee1838e4":"code","1bbf4208":"code","89d402be":"code","4ecdd107":"code","a00fd579":"code","f185ebc4":"code","e0091525":"code","843de50c":"code","5f20c80c":"code","21f440d9":"code","39b48d9e":"code","32b1cfcd":"code","0c55e00a":"code","b91c3ebf":"code","6b0759ac":"code","953f3673":"code","57d11ce0":"code","d53f17c6":"code","ab3005b6":"code","33c6272f":"code","cdd2f6ba":"code","16285be1":"code","4df7ccae":"code","e8802a47":"code","2a68a52a":"code","9af3891c":"code","066d6ad1":"code","dd3d172a":"code","39f33ac6":"code","c4f40701":"code","f50b84dc":"code","75e41ed9":"code","8529cf3f":"code","3e9b79f4":"code","457aa6ef":"code","fb77ea34":"code","1cf91c1f":"markdown","44d47efb":"markdown","96827330":"markdown","ff00259d":"markdown","50a66141":"markdown","e498e391":"markdown","b076d337":"markdown","57186f22":"markdown","d2030d99":"markdown","94bd1fd8":"markdown","5f0c4265":"markdown","e7eca2b2":"markdown","d622bc6d":"markdown","3a907f23":"markdown","f135c3e0":"markdown","fe94b477":"markdown","0531431c":"markdown","3109e997":"markdown","b59554dc":"markdown","b9c41a35":"markdown","b7edd2ba":"markdown","2df44bf0":"markdown","827b84c1":"markdown","6da951ea":"markdown","87aeb2ad":"markdown","73dfe262":"markdown","f56eaadd":"markdown","6dc1fab3":"markdown","999edbcc":"markdown","f43ffde0":"markdown","0729e683":"markdown","3a5a0681":"markdown","e8c20c96":"markdown","797b82f4":"markdown","3213587d":"markdown","5f8f1346":"markdown","d3bc1534":"markdown","233787d2":"markdown","a3a6f8c0":"markdown","38bc36d1":"markdown","09e81b83":"markdown","f62b73b2":"markdown","557617bb":"markdown","59f1d698":"markdown","d3a14bb6":"markdown","5342218f":"markdown","1b6ee723":"markdown","7fdbc0a8":"markdown","b7168ec8":"markdown","db92493f":"markdown","e66467d3":"markdown","f6d30b00":"markdown","09417ad9":"markdown","7a32a2b8":"markdown","51ca5751":"markdown","ba369107":"markdown","6eb3f65e":"markdown","72bf1d50":"markdown","e2f36d1c":"markdown","7b8183d8":"markdown","cc20c793":"markdown","c25f0190":"markdown","1f0fa227":"markdown","c9dc6406":"markdown","5d20cc28":"markdown","662496c0":"markdown","4e69cdf9":"markdown","b76884de":"markdown","1279ffdc":"markdown","44e55817":"markdown","49925739":"markdown","18f8d4cf":"markdown","92abbeca":"markdown","1df0141e":"markdown","fdcb407b":"markdown","a13d6532":"markdown","88d08497":"markdown","b19984d8":"markdown","d241a176":"markdown","3c39c23f":"markdown","1eb0b1d3":"markdown","61fb2f1c":"markdown","961705cb":"markdown","42fbd0f8":"markdown","1b17d8bb":"markdown"},"source":{"0ca74930":"# Let\u2019s load the libraries\n\nimport re    # for regular expressions \nimport nltk  # for text manipulation \nimport string \nimport warnings \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt  \n\npd.set_option(\"display.max_colwidth\", 200) \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n%matplotlib inline","2608e36d":"# Let\u2019s read train and test datasets.\n\ntrain  = pd.read_csv('..\/input\/train_E6oV3lV.csv') \ntest = pd.read_csv('..\/input\/test_tweets_anuFYb8.csv')","51b8bb09":"train[train['label'] == 0].head(10)","d56d7e37":"\n\ntrain[train['label'] == 1].head(10)\n","04d80295":"train.shape, test.shape","6f003093":"train[\"label\"].value_counts()\n","ec7d31bc":"plt.hist(train.tweet.str.len(), bins=20, label='train')\nplt.hist(test.tweet.str.len(), bins=20, label='test')\nplt.legend()\nplt.show()","83e53cc6":"combi = train.append(test, ignore_index=True, sort=True)\ncombi.shape","76262ae0":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt","992e7df3":"combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \ncombi.head(10)","80e7d262":"combi.tidy_tweet = combi.tidy_tweet.str.replace(\"[^a-zA-Z#]\", \" \")\ncombi.head(10)","50552bc6":"combi.tidy_tweet = combi.tidy_tweet.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\ncombi.head(10)","43bd9d5f":"tokenized_tweet = combi.tidy_tweet.apply(lambda x: x.split())\ntokenized_tweet.head()","3347d916":"# Now we can normalize the tokenized tweets.\n\nfrom nltk.stem.porter import * \nstemmer = PorterStemmer() \ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntokenized_tweet.head()","4f1e21b3":"# Now let\u2019s stitch these tokens back together. It can easily be done using nltk\u2019s MosesDetokenizer function.\n\nfor i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])    \ncombi['tidy_tweet'] = tokenized_tweet\ncombi.head(10)","e6379f60":"all_words = ' '.join([text for text in combi['tidy_tweet']]) \n\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off')\nplt.show()","ade935ec":"normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]]) \n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","54b3ac55":"negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","99c1a716":"# function to collect hashtags \n\ndef hashtag_extract(x):\n    hashtags = []    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n    return hashtags","5efd510b":"# extracting hashtags from non racist\/sexist tweets \n\nHT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0]) ","d1eb6720":"# extracting hashtags from racist\/sexist tweets\n\nHT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1]) ","90d0ff65":"# unnesting list\n\nHT_regular = sum(HT_regular,[]) \nHT_negative = sum(HT_negative,[])","fc3e8a70":"a = nltk.FreqDist(HT_regular)\nd = pd.DataFrame(\n    {\n    'Hashtag': list(a.keys()),\n    'Count': list(a.values())\n    }\n) ","c8aa828c":"# selecting top 20 most frequent hashtags\n\nd = d.nlargest(columns=\"Count\", n = 20)\nplt.figure(figsize=(20,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\n# plt.xticks(rotation=90)\nplt.show()","674f7263":"a = nltk.FreqDist(HT_negative)\nd = pd.DataFrame(\n    {\n    'Hashtag': list(a.keys()),\n    'Count': list(a.values())\n    }\n) ","9c015242":"# selecting top 20 most frequent hashtags\n\nd = d.nlargest(columns=\"Count\", n = 20)\nplt.figure(figsize=(20,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\n# plt.xticks(rotation=90)\nplt.show()","a8f48456":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \nimport gensim","eac4c4ef":"bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\nbow = bow_vectorizer.fit_transform(combi['tidy_tweet'])\nbow.shape","b1dc322f":"tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])\ntfidf.shape","269b2bf2":"%%time\n\ntokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \n\nmodel_w2v = gensim.models.Word2Vec(\n            tokenized_tweet,\n            size=200, # desired no. of features\/independent variables\n            window=5, # context window size\n            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n            sg = 1, # 1 for skip-gram model\n            hs = 0,\n            negative = 10, # for negative sampling\n            workers= 32, # no.of cores\n            seed = 34\n) \n\nmodel_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)","0b55d5cd":"model_w2v.wv.most_similar(positive=\"dinner\")","34d2dbe2":"model_w2v.most_similar(positive=\"trump\")","894e280e":"model_w2v['food']","45c7c3bb":"len(model_w2v['food']) #The length of the vector is 200","b966ac26":"def word_vector(tokens, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0\n    for word in tokens:\n        try:\n            vec += model_w2v[word].reshape((1, size))\n            count += 1.\n        except KeyError:  # handling the case where the token is not in vocabulary\n            continue\n    if count != 0:\n        vec \/= count\n    return vec","f43f76ee":"wordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \nfor i in range(len(tokenized_tweet)):\n    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\nwordvec_df = pd.DataFrame(wordvec_arrays)\nwordvec_df.shape","4b5d0505":"from tqdm import tqdm \ntqdm.pandas(desc=\"progress-bar\") \nfrom gensim.models.doc2vec import LabeledSentence","e4c99336":"def add_label(twt):\n    output = []\n    for i, s in zip(twt.index, twt):\n        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n    return output\n\nlabeled_tweets = add_label(tokenized_tweet) # label all the tweets","16f579d4":"labeled_tweets[:6]","f47be32a":"%%time \nmodel_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for \u2018distributed memory\u2019 model\n                                  dm_mean=1, # dm_mean = 1 for using mean of the context word vectors\n                                  vector_size=200, # no. of desired features\n                                  window=5, # width of the context window                                  \n                                  negative=7, # if > 0 then negative sampling will be used\n                                  min_count=5, # Ignores all words with total frequency lower than 5.                                  \n                                  workers=32, # no. of cores                                  \n                                  alpha=0.1, # learning rate                                  \n                                  seed = 23, # for reproducibility\n                                 ) \n\nmodel_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n\nmodel_d2v.train(labeled_tweets, total_examples= len(combi['tidy_tweet']), epochs=15)","ee1838e4":"docvec_arrays = np.zeros((len(tokenized_tweet), 200)) \nfor i in range(len(combi)):\n    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))    \n\ndocvec_df = pd.DataFrame(docvec_arrays) \ndocvec_df.shape","1bbf4208":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","89d402be":"# Extracting train and test BoW features \ntrain_bow = bow[:31962,:] \ntest_bow = bow[31962:,:] \n\n# splitting data into training and validation set \nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n\nlreg = LogisticRegression(solver='lbfgs') \n\n# training the model \nlreg.fit(xtrain_bow, ytrain) \nprediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set \nprediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0 \nprediction_int = prediction_int.astype(np.int) \nf1_score(yvalid, prediction_int) # calculating f1 score for the validation set","4ecdd107":"test_pred = lreg.predict_proba(test_bow)\ntest_pred_int = test_pred[:,1] >= 0.3\ntest_pred_int = test_pred_int.astype(np.int)\ntest['label'] = test_pred_int\nsubmission = test[['id','label']]\nsubmission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file","a00fd579":"train_tfidf = tfidf[:31962,:]\ntest_tfidf = tfidf[31962:,:] \n\nxtrain_tfidf = train_tfidf[ytrain.index]\nxvalid_tfidf = train_tfidf[yvalid.index]\n\nlreg.fit(xtrain_tfidf, ytrain) \n\nprediction = lreg.predict_proba(xvalid_tfidf)\n\nprediction_int = prediction[:,1] >= 0.3\nprediction_int = prediction_int.astype(np.int) \n\nf1_score(yvalid, prediction_int) # calculating f1 score for the validation set","f185ebc4":"train_w2v = wordvec_df.iloc[:31962,:]\ntest_w2v = wordvec_df.iloc[31962:,:]\n\nxtrain_w2v = train_w2v.iloc[ytrain.index,:]\nxvalid_w2v = train_w2v.iloc[yvalid.index,:]\n\nlreg.fit(xtrain_w2v, ytrain) \n\nprediction = lreg.predict_proba(xvalid_w2v)\n\nprediction_int = prediction[:,1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","e0091525":"train_d2v = docvec_df.iloc[:31962,:]\ntest_d2v = docvec_df.iloc[31962:,:] \n\nxtrain_d2v = train_d2v.iloc[ytrain.index,:]\nxvalid_d2v = train_d2v.iloc[yvalid.index,:]\n\nlreg.fit(xtrain_d2v, ytrain) \n\nprediction = lreg.predict_proba(xvalid_d2v)\n\nprediction_int = prediction[:,1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","843de50c":"from sklearn import svm","5f20c80c":"svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain) \nprediction = svc.predict_proba(xvalid_bow) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int) \nf1_score(yvalid, prediction_int)","21f440d9":"test_pred = svc.predict_proba(test_bow) \ntest_pred_int = test_pred[:,1] >= 0.3 \ntest_pred_int = test_pred_int.astype(np.int) \ntest['label'] = test_pred_int \nsubmission = test[['id','label']] \nsubmission.to_csv('sub_svm_bow.csv', index=False)","39b48d9e":"svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_tfidf, ytrain) \nprediction = svc.predict_proba(xvalid_tfidf) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int) \nf1_score(yvalid, prediction_int)","32b1cfcd":"svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_w2v, ytrain) \nprediction = svc.predict_proba(xvalid_w2v) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int) \nf1_score(yvalid, prediction_int)","0c55e00a":"svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_d2v, ytrain) \nprediction = svc.predict_proba(xvalid_d2v) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int) \nf1_score(yvalid, prediction_int)","b91c3ebf":"from sklearn.ensemble import RandomForestClassifier","6b0759ac":"rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain) \nprediction = rf.predict(xvalid_bow) \nf1_score(yvalid, prediction) # validation score","953f3673":"test_pred = rf.predict(test_bow)\ntest['label'] = test_pred\nsubmission = test[['id','label']]\nsubmission.to_csv('sub_rf_bow.csv', index=False)","57d11ce0":"rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain) \nprediction = rf.predict(xvalid_tfidf)\nf1_score(yvalid, prediction)","d53f17c6":"rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain) \nprediction = rf.predict(xvalid_w2v)\nf1_score(yvalid, prediction)","ab3005b6":"rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_d2v, ytrain) \nprediction = rf.predict(xvalid_d2v)\nf1_score(yvalid, prediction)","33c6272f":"from xgboost import XGBClassifier","cdd2f6ba":"xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain)\nprediction = xgb_model.predict(xvalid_bow)\nf1_score(yvalid, prediction)","16285be1":"test_pred = xgb_model.predict(test_bow)\ntest['label'] = test_pred\nsubmission = test[['id','label']]\nsubmission.to_csv('sub_xgb_bow.csv', index=False)","4df7ccae":"xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain) \nprediction = xgb.predict(xvalid_tfidf)\nf1_score(yvalid, prediction)","e8802a47":"xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain) \nprediction = xgb.predict(xvalid_w2v)\nf1_score(yvalid, prediction)","2a68a52a":"xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_d2v, ytrain) \nprediction = xgb.predict(xvalid_d2v)\nf1_score(yvalid, prediction)","9af3891c":"import xgboost as xgb","066d6ad1":"dtrain = xgb.DMatrix(xtrain_w2v, label=ytrain) \ndvalid = xgb.DMatrix(xvalid_w2v, label=yvalid) \ndtest = xgb.DMatrix(test_w2v)\n# Parameters that we are going to tune \nparams = {\n    'objective':'binary:logistic',\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1\n }","dd3d172a":"def custom_eval(preds, dtrain):\n    labels = dtrain.get_label().astype(np.int)\n    preds = (preds >= 0.3).astype(np.int)\n    return [('f1_score', f1_score(labels, preds))]","39f33ac6":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(6,10)\n    for min_child_weight in range(5,8)\n    ]\n\nmax_f1 = 0. # initializing with 0 \n\nbest_params = None \n\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(max_depth,min_child_weight))\n    \n     # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n\n     # Cross-validation\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        feval= custom_eval,\n        num_boost_round=200,\n        maximize=True,\n        seed=16,\n        nfold=5,\n        early_stopping_rounds=10\n        )     \n    \n# Finding best F1 Score    \nmean_f1 = cv_results['test-f1_score-mean'].max()\nboost_rounds = cv_results['test-f1_score-mean'].idxmax()    \nprint(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))    \n\nif mean_f1 > max_f1:\n        max_f1 = mean_f1\n        best_params = (max_depth,min_child_weight) \n\nprint(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))","c4f40701":"params['max_depth'] = 9 \nparams['min_child_weight'] = 7","f50b84dc":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(5,10)]\n    for colsample in [i\/10. for i in range(5,10)]\n]\n\nmax_f1 = 0. \nbest_params = None \n\nfor subsample, colsample in gridsearch_params:\n    print(\"CV with subsample={}, colsample={}\".format(subsample,colsample))\n    \n    # Update our parameters\n    params['colsample'] = colsample\n    params['subsample'] = subsample\n    \n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        feval= custom_eval,\n        num_boost_round=200,\n        maximize=True,\n        seed=16,\n        nfold=5,\n        early_stopping_rounds=10\n        )\n    \n    # Finding best F1 Score\n    mean_f1 = cv_results['test-f1_score-mean'].max()\n    boost_rounds = cv_results['test-f1_score-mean'].idxmax()\n    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n    \n    if mean_f1 > max_f1:\n        max_f1 = mean_f1\n        best_params = (subsample, colsample) \n\nprint(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))","75e41ed9":"params['subsample'] = 0.9\nparams['colsample_bytree'] = 0.5","8529cf3f":"max_f1 = 0. \nbest_params = None \nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n     # Update ETA\n    params['eta'] = eta\n\n     # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        feval= custom_eval,\n        num_boost_round=1000,\n        maximize=True,\n        seed=16,\n        nfold=5,\n        early_stopping_rounds=20\n    )\n\n    # Finding best F1 Score\n    mean_f1 = cv_results['test-f1_score-mean'].max()\n    boost_rounds = cv_results['test-f1_score-mean'].idxmax()\n    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n    \n    if mean_f1 > max_f1:\n        max_f1 = mean_f1\n        best_params = eta \n        \nprint(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))","3e9b79f4":"params = {\n    'colsample': 0.9,\n    'colsample_bytree': 0.5,\n    'eta': 0.1,\n    'max_depth': 9,\n    'min_child_weight': 7,\n    'objective': 'binary:logistic',\n    'subsample': 0.9\n}","457aa6ef":"xgb_model = xgb.train(\n    params,\n    dtrain,\n    feval= custom_eval,\n    num_boost_round= 1000,\n    maximize=True,\n    evals=[(dvalid, \"Validation\")],\n    early_stopping_rounds=10\n )","fb77ea34":"test_pred = xgb_model.predict(dtest)\ntest['label'] = (test_pred >= 0.3).astype(np.int)\nsubmission = test[['id','label']] \nsubmission.to_csv('sub_xgb_w2v_finetuned.csv', index=False)","1cf91c1f":"#### TF-IDF Features\n\nThis is another method which is based on the frequency method but it is different to the bag-of-words approach in the sense that it takes into account not just the occurrence of a word in a single document (or tweet) but in the entire corpus.\n\nTF-IDF works by penalising the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents.\n\nLet\u2019s have a look at the important terms related to TF-IDF:\n\n* TF = (Number of times term t appears in a document)\/(Number of terms in the document)\n\n* IDF = log(N\/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n\n* TF-IDF = TF*IDF","44d47efb":"#### Preparing Vectors for Tweets\n\nSince our data contains tweets and not just words, we\u2019ll have to figure out a way to use the word vectors from word2vec model to create vector representation for an entire tweet. There is a simple solution to this problem, we can simply take mean of all the word vectors present in the tweet. The length of the resultant vector will be the same, i.e. 200. We will repeat the same process for all the tweets in our data and obtain their vectors. Now we have 200 word2vec features for our data.\n\nWe will use the below function to create a vector for each tweet by taking the average of the vectors of the words present in the tweet.","96827330":"There are quite a many words and characters which are not really required. So, we will try to keep only those words which are important and add value.\n\nLet\u2019s check dimensions of the train and test dataset.","ff00259d":"Our tuning worked! This is our best score!!","50a66141":"Let\u2019s have a look at the final list of tuned parameters.","e498e391":"Let\u2019s start with the **Bag-of-Words** Features.\n\nConsider a Corpus C of D documents {d1,d2\u2026..dD} and N unique tokens extracted out of the corpus C. The N tokens (words) will form a dictionary and the size of the bag-of-words matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).\n\nLet us understand this using a simple example.\n\nD1: He is a lazy boy. She is also lazy.\n\nD2: Smith is a lazy person.\n\nThe dictionary created would be a list of unique tokens in the corpus =[\u2018He\u2019,\u2019She\u2019,\u2019lazy\u2019,\u2019boy\u2019,\u2019Smith\u2019,\u2019person\u2019]\n\nHere, D=2, N=6\n\nThe matrix M of size 2 X 6 will be represented as \u2013\n![imgur](https:\/\/i.imgur.com\/mKcTPdZ.png)\n\nNow the columns in the above matrix can be used as features to build a classification model.","b076d337":"**Racist\/Sexist Tweets**","57186f22":"**3. Removing Short Words**\n\nWe have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 3 or less. For example, terms like \u201chmm\u201d, \u201coh\u201d are of very little use. It is better to get rid of them.","d2030d99":"As we can clearly see, most of the words have negative connotations. So, it seems we have a pretty good text data to work on. Next we will the hashtags\/trends in our twitter data.","94bd1fd8":"**Bag-of-Words Features**\n\nFirst we will train our RandomForest model on the Bag-of-Words features and check its performance on validation set.","5f0c4265":"#### FineTuning XGBoost + Word2Vec\n\nXGBoost with Word2Vec model has given us the best performance so far. Let\u2019s try to tune it further to extract as much from it as we can. XGBoost has quite a many tuning parameters and sometimes it becomes tricky to properly tune them. This is what we are going to do in the following steps. You can refer this guide to learn more about parameter tuning in XGBoost.\n\n![imgur](https:\/\/i.imgur.com\/zmt7R4l.jpg)","e7eca2b2":"**Doc2Vec Features**","d622bc6d":"Doc2Vec features do not seem to be capturing the right signals as the F1-score on validation set is quite low.","3a907f23":"Now let\u2019s train a **doc2vec** model.","f135c3e0":"**Preparing doc2vec Feature Set**","fe94b477":"**Bag-of-Words Features**","0531431c":"**2. Removing Punctuations, Numbers, and Special Characters**\n\nHere we will replace everything except characters and hashtags with spaces. The regular expression \u201c[^a-zA-Z#]\u201d means anything except alphabets and \u2018#\u2019.","3109e997":"**Doc2Vec Features**","b59554dc":"In any natural language processing task, cleaning raw text data is an important step. It helps in getting rid of the unwanted words and characters which helps in obtaining better features. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don\u2019t carry much weightage in context to the text.\n\nBefore we begin cleaning, let\u2019s first combine train and test datasets. Combining the datasets will make it convenient for us to preprocess the data. Later we will split it back into train and test data.","b9c41a35":"Let\u2019s make predictions for the test dataset and create another submission file.","b7edd2ba":"Updating max_depth and min_child_weight parameters.","2df44bf0":"**Doc2Vec Features**","827b84c1":"As expected, most of the terms are negative with a few neutral terms as well. So, it\u2019s not a bad idea to keep these hashtags in our data as they contain useful information. Next, we will try to extract features from the tokenized tweets.","6da951ea":"Now it\u2019s time to wrap-up things. Let\u2019s quickly revisit what we have learned, initially we cleaned our raw text data, then we learned about 4 different types of feature-set that we can extract from any text data, and finally we used these feature-sets to build models for sentiment analysis. Below is a summary table showing F1 scores for different models and feature-sets.\n\n![imgur](https:\/\/i.imgur.com\/3zmUSLN.png)\n\n\nWord2Vec features turned out to be most useful. Whereas **XGBoost with Word2Vec features** was the best model for this problem. This clearly shows the power of word embeddings in dealing with NLP problems.\n\n### WHAT ELSE CAN BE TRIED?\nWe have covered a lot in this Sentiment Analysis, but still there is plenty of room for other things to try out. Given below is a list of tasks that you can try with this data.\n\n1. We have built so many models, we can definitely try model ensembling. A simple ensemble of all the submission files (maximum voting) yielded an F1 score of **0.55**.\n\n1. Use Parts-of-Speech tagging to create new features.\n\n1. Use stemming and\/or lemmatization. It might help in getting rid of unnecessary words.\n\n1. Use bi-grams or tri-grams (tokens of 2 or 3 words respectively) for Bag-of-Words and TF-IDF.\n\n1. We can give pretrained word-embeddings models a try.","87aeb2ad":"#### Bag-of-Words Features\n\nTo analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques \u2013 Bag of Words, TF-IDF, and Word Embeddings. Read on to understand these techniques in detail.","73dfe262":"Tuning *subsample* and *colsample*","f56eaadd":"XGBoost model on word2vec features has outperformed all the previuos models","6dc1fab3":"**Doc2Vec Features**","999edbcc":"From the above two examples, we can see that our word2vec model does a good job of finding the most similar words for a given word. But how is it able to do so? That\u2019s because it has learned vectors for every unique word in our data and it uses cosine similarity to find out the most similar vectors (words).\n\nLet\u2019s check the vector representation of any word from our corpus.","f43ffde0":"We will prepare a custom evaluation metric to calculate F1 score.","0729e683":"Let\u2019s have a look at the result.","3a5a0681":"Now that we have prepared our lists of hashtags for both the sentiments, we can plot the top \u2018n\u2019 hashtags. So, first let\u2019s check the hashtags in the non-racist\/sexist tweets.","e8c20c96":"#### Data Inspection\nLet\u2019s check out a few **non** racist\/sexist tweets.","797b82f4":"**Word2Vec Features**","3213587d":"Preparing word2vec feature set\u2026","5f8f1346":"**TF-IDF Features**","d3bc1534":"**Word2Vec Features**","233787d2":"**TF-IDF Features**","a3a6f8c0":"In the train dataset, we have 2,242 (~7%) tweets labeled as racist or sexist, and 29,720 (~93%) tweets labeled as non racist\/sexist. So, it is an imbalanced classification challenge.\n\nNow we will check the distribution of length of the tweets, in terms of words, in both train and test data.","38bc36d1":"Again let\u2019s make predictions for the test dataset and create another submission file.","09e81b83":"Let\u2019s play a bit with our Word2Vec model and see how does it perform. We will specify a word and the model will pull out the most similar words from the corpus.","f62b73b2":"Now let\u2019s make predictions for the test dataset and create a submission file.","557617bb":"**What is Sentiment Analysis?**\n\nSentiment analysis (also known as opinion mining) is one of the many applications of Natural Language Processing. It is a set of methods and techniques used for extracting subjective information from text or speech, such as opinions or attitudes. In simple terms, it involves classifying a piece of text as positive, negative or neutral.","59f1d698":"#### 2. Doc2Vec Embedding\n\nDoc2Vec model is an unsupervised algorithm to generate vectors for sentence\/paragraphs\/documents. This approach is an extension of the word2vec. The major difference between the two is that doc2vec provides an additional context which is unique for every document in the corpus. This additional context is nothing but another feature vector for the whole document. This document vector is trained along with the word vectors.\n\n![imgur](https:\/\/i.imgur.com\/RBlg4xJ.png)\n\n\nLet\u2019s load the required libraries.","d3a14bb6":"We will be following the steps below to clean the raw tweets in out data.\n\n1. We will remove the twitter handles as they are already masked as @user due to privacy concerns. These twitter handles hardly give any information about the nature of the tweet.\n\n1. We will also get rid of the punctuations, numbers and even special characters since they wouldn\u2019t help in differentiating different types of tweets.\n\n1. Most of the smaller words do not add much value. For example, \u2018pdx\u2019, \u2018his\u2019, \u2018all\u2019. So, we will try to remove them as well from our data.\n\n1. Lastly, we will normalize the text data. For example, reducing terms like loves, loving, and lovable to their base word, i.e., \u2018love\u2019.are often used in the same context. If we can reduce them to their root word, which is \u2018love\u2019. It will help in reducing the total number of unique words in our data without losing a significant amount of information.","5342218f":"Finally we can now use these tuned parameters in our xgboost model. We have used early stopping of 10 which means if the model\u2019s performance doesn\u2019t improve under 10 rounds, then the model training will be stopped.","1b6ee723":"In this section, we will explore the cleaned tweets. Exploring and visualizing data, no matter whether its text or any other data, is an essential step in gaining insights. Do not limit yourself to only these methods told in this course, feel free to explore the data as much as possible.\n\nBefore we begin exploration, we must think and ask questions related to the data in hand. A few probable questions are as follows:\n\n* What are the most common words in the entire dataset?\n* What are the most common words in the dataset for negative and positive tweets, respectively?\n* How many hashtags are there in a tweet?\n* Which trends are associated with my dataset?\n* Which trends are associated with either of the sentiments? Are they compatible with the sentiments?","7fdbc0a8":"#### Support Vector Machine (SVM)\n\nSupport Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes as shown in the plot below:\n\n![imgur](https:\/\/i.imgur.com\/dG1jOCx.png)\n\nRefer this [article](https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/understaing-support-vector-machine-example-code\/) to learn more about SVM. Now we will implement SVM on our data using the scikit-learn library.","b7168ec8":"**A) Understanding the common words used in the tweets: WordCloud**\n\nNow I want to see how well the given sentiments are distributed across the train dataset. One way to accomplish this task is by understanding the common words by plotting wordclouds.\n\nA wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.\n\nLet\u2019s visualize all the words our data using the wordcloud plot.","db92493f":"**Bag-of-Words Features**\n\nWe will first try to fit the logistic regression model on the Bag-of-Words (BoW) features.","e66467d3":"**4. Text Normalization**\n\nHere we will use nltk\u2019s PorterStemmer() function to normalize the tweets. But before that we will have to tokenize the tweets. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.","f6d30b00":"Now we have 200 new features, whereas in Bag of Words and TF-IDF we had 1000 features.","09417ad9":"**D) Understanding the impact of Hashtags on tweets sentiment**\n\nHashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. We should try to check whether these hashtags add any value to our sentiment analysis task, i.e., they help in distinguishing tweets into the different sentiments.\n\nFor instance, given below is a tweet from our dataset:\n\n![imgur](https:\/\/i.imgur.com\/phLSlwB.png)\n\nThe tweet seems sexist in nature and the hashtags in the tweet convey the same feeling.\n\nWe will store all the trend terms in two separate lists \u2014 one for non-racist\/sexist tweets and the other for racist\/sexist tweets.\n","7a32a2b8":"**Word2Vec Features**","51ca5751":"To implement doc2vec, we have to **labelise** or **tag** each tokenised tweet with unique IDs. We can do so by using Gensim\u2019s *LabeledSentence()* function.","ba369107":"Now check out a few racist\/sexist tweets.","6eb3f65e":"Updating *subsample* and *colsample_bytree*","72bf1d50":"We are now done with all the pre-modeling stages required to get the data in the proper form and shape. We will be building models on the datasets with different feature sets prepared in the earlier sections \u2014 Bag-of-Words, TF-IDF, word2vec vectors, and doc2vec vectors. We will use the following algorithms to build models:\n\n1. Logistic Regression\n1. Support Vector Machine\n1. RandomForest\n1. XGBoost\n\n**Evaluation Metric**\n\n**F1 score** is being used as the evaluation metric. It is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is suitable for uneven class distribution problems.\n\nThe important components of F1 score are:\n\n1. True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n1. True Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n1. False Positives (FP) \u2013 When actual class is no and predicted class is yes.\n1. False Negatives (FN) \u2013 When actual class is yes but predicted class in no.\n\n**Precision** = TP\/TP+FP\n\n**Recall** = TP\/TP+FN\n\n**F1 Score** = 2(Recall * Precision) \/ (Recall + Precision)","e2f36d1c":"**C) Racist\/Sexist Tweets**","7b8183d8":"Most of the frequent words are compatible with the sentiment, i.e, non-racist\/sexists tweets. Similarly, we will plot the word cloud for the other sentiment. Expect to see negative, racist, and sexist terms.","cc20c793":"Here validation score is slightly lesser than the Logistic Regression score for bag-of-words features.","c25f0190":"**General Approach for Parameter Tuning**\n\nWe will follow the steps below to tune the parameters.\n\n1. Choose a relatively high learning rate. Usually a learning rate of 0.3 is used at this stage.\n\n1. Tune tree-specific parameters such as max_depth, min_child_weight, subsample, colsample_bytree keeping the learning rate fixed.\n\n1. Tune the learning rate.\n\n1. Finally tune gamma to avoid overfitting.\n\n*Tuning max_depth and min_child_weight*","1f0fa227":"#### RandomForest\n\nRandom Forest is a versatile machine learning algorithm capable of performing both regression and classification tasks. It is a kind of ensemble learning method, where a few weak models combine to form a powerful model. In Random Forest, we grow multiple trees as opposed to a decision single tree. To classify a new object based on attributes, each tree gives a classification and we say the tree \u201cvotes\u201d for that class. The forest chooses the classification having the most votes (over all the trees in the forest).\n\nIt works in the following manner. Each tree is planted & grown as follows:\n\n1. Assume number of cases in the training set is N. Then, sample of these N cases is taken at random but with replacement. This sample will be the training set for growing the tree.\n\n1. If there are M input variables, a number m (m<M) is specified such that at each node, m variables are selected at random out of the M. The best split on these m variables is used to split the node. The value of m is held constant while we grow the forest.\n\n1. Each tree is grown to the largest extent possible and there is no pruning.\n\n1. Predict new data by aggregating the predictions of the ntree trees (i.e., majority votes for classification, average for regression).\n\n![imgur](https:\/\/i.imgur.com\/nOBROEn.png)","c9dc6406":"**TF-IDF Features**\n\nWe\u2019ll follow the same steps as above, but now for the TF-IDF feature set.","5d20cc28":"Given below is a user-defined function to remove unwanted text patterns from the tweets.","662496c0":"#### Logistic Regression\n\nLogistic Regression is a classification algorithm. It is used to predict a binary outcome (1 \/ 0, Yes \/ No, True \/ False) given a set of independent variables. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical, where we are using log of odds as the dependent variable. In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function.\n\nThe following equation is used in Logistic Regression:\n\n![imgur](https:\/\/i.imgur.com\/RpFof26.png)\n\nA typical logistic model plot is shown below. You can see probability never goes below 0 and above 1.\n\n![imgur](https:\/\/i.imgur.com\/vX2dlga.png)\n\nRead this [article](https:\/\/www.analyticsvidhya.com\/blog\/2015\/11\/beginners-guide-on-logistic-regression-in-r\/) to know more about Logistic Regression.\n\n","4e69cdf9":"**B) Words in non racist\/sexist tweets**","b76884de":"All these hashtags are positive and it makes sense. I am expecting negative terms in the plot of the second list. Let\u2019s check the most frequent hashtags appearing in the racist\/sexist tweets.","1279ffdc":"**Bag-of-Words Features**","44e55817":"Here we will use DMatrices. A DMatrix can contain both the features and the target.","49925739":"**Non-Racist\/Sexist Tweets**","18f8d4cf":"#### XGBoost\n\nExtreme Gradient Boosting (xgboost) is an advanced implementation of gradient boosting algorithm. It has both linear model solver and tree learning algorithms. Its ability to do parallel computation on a single machine makes it extremely fast. It also has additional features for doing cross validation and finding important variables. There are many parameters which need to be controlled to optimize the model.\n\nSome key benefits of XGBoost are:\n\n1. **Regularization** - helps in reducing overfitting\n\n1. **Parallel Processing** - XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n\n1. **Handling Missing Values** - It has an in-built routine to handle missing values.\n\n1. **Built-in Cross-Validation** - allows user to run a cross-validation at each iteration of the boosting process\n\nCheck out this wonderful guide on XGBoost parameter tuning.","92abbeca":"![Imgur](https:\/\/i.imgur.com\/n2MmZrY.png)\n<!-- <img src=\"subdirect****ory\/MyImage.png\",width=60,height=60> -->","1df0141e":"The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n\nFormally, given a training sample of tweets and labels, where label \u20181\u2019 denotes the tweet is racist\/sexist and label \u20180\u2019 denotes the tweet is not racist\/sexist, your objective is to predict the labels on the given test dataset.\n\nThis work is majorly inspired from the Analytics Vidhya Course for [this](https:\/\/datahack.analyticsvidhya.com\/contest\/practice-problem-twitter-sentiment-analysis\/) practise problem.","fdcb407b":"#### Word2Vec Features\n\nWord embeddings are the modern way of representing words as vectors. The objective of word embeddings is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are able to achieve tasks like **King -man +woman = Queen**, which is mind-blowing.\n\n![imgur](https:\/\/i.imgur.com\/gZiNamE.png)\n\nThe advantages of using word embeddings over BOW or TF-IDF are:\n\n1. Dimensionality reduction - significant reduction in the no. of features required to build a model.\n\n1. It capture meanings of the words, semantic relationships and the different types of contexts they are used in.","a13d6532":"You can see the difference between the raw tweets and the cleaned tweets (tidy_tweet) quite clearly. Only the important words in the tweets have been retained and the noise (numbers, punctuations, and special characters) has been removed.","88d08497":"**1. Removing Twitter Handles (@user)**\n\nLet\u2019s create a new column tidy_tweet, it will contain the cleaned and processed tweets. Note that we have passed \u201c@[]*\u201d as the pattern to the remove_pattern function. It is actually a regular expression which will pick any word starting with \u2018@\u2019.","b19984d8":"**1. Word2Vec Embeddings**\n\nWord2Vec is not a single algorithm but a combination of two techniques \u2013 **CBOW (Continuous bag of words)** and **Skip-gram** model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations.\n\nCBOW tends to predict the probability of a word given a context. A context may be a single adjacent word or a group of surrounding words. The Skip-gram model works in the reverse manner, it tries to predict the context for a given word.\n\nBelow is a diagrammatic representation of a 1-word context window Word2Vec model.\n\n![imgur](https:\/\/i.imgur.com\/f77V0dH.png)\n\nThere are three laters: - an input layer, - a hidden layer, and - an output layer.\n\nThe input layer and the output, both are one- hot encoded of size [1 X V], where V is the size of the vocabulary (no. of unique words in the corpus). The output layer is a softmax layer which is used to sum the probabilities obtained in the output layer to 1. The weights learned by the model are then used as the word-vectors.\n\nWe will go ahead with the Skip-gram model as it has the following advantages:\n\n* It can capture two semantics for a single word. i.e it will have two vector representations of \u2018apple\u2019. One for the company Apple and the other for the fruit.\n\n* Skip-gram with negative sub-sampling outperforms CBOW generally.\n\nWe will train a Word2Vec model on our data to obtain vector representations for all the unique words present in our corpus. There is one more option of using **pre-trained word vectors** instead of training our own model. Some of the freely available pre-trained vectors are:\n\n1. [Google News Word Vectors](https:\/\/code.google.com\/archive\/p\/word2vec\/)\n\n1. [Freebase names](https:\/\/code.google.com\/archive\/p\/word2vec\/)\n\n1. [DBPedia vectors (wiki2vec)](https:\/\/github.com\/idio\/wiki2vec#prebuilt-models)\n\nHowever, for now, we will train our own word vectors since size of the pre-trained word vectors is generally huge.\n\nLet\u2019s train a Word2Vec model on our corpus.","d241a176":"**Word2Vec Features**","3c39c23f":"**TF-IDF Features**","1eb0b1d3":"#### Table of contents\n\n1. Understand the Problem Statement\n1. Tweets Preprocessing and Cleaning\n    1. Data Inspection\n    1. Data Cleaning\n1. Story Generation and Visualization from Tweets\n1. Extracting Features from Cleaned Tweets\n    1. Bag-of-Words\n    1. TF-IDF\n    1. Word Embeddings\n1. Model Building: Sentiment Analysis\n    1. Logistic Regression\n    1. Support Vector Machine\n    1. RandomForest\n    1. XGBoost\n1. Model Fine-tuning\n1. Summary","61fb2f1c":"Now let\u2019s tune the *learning rate*.","961705cb":"Text is a highly unstructured form of data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing. We will divide it into 2 parts:\n\n* Data Inspection\n* Data Cleaning","42fbd0f8":"We can see most of the words are positive or neutral. Words like love, great, friend, life are the most frequent ones. It doesn\u2019t give us any idea about the words associated with the racist\/sexist tweets. Hence, we will plot separate wordclouds for both the classes (racist\/sexist or not) in our train data.","1b17d8bb":"Train set has 31,962 tweets and test set has 17,197 tweets.\n\nLet\u2019s have a glimpse at label-distribution in the train dataset."}}