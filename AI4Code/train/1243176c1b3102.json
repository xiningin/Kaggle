{"cell_type":{"b0d6a6ea":"code","3be485a3":"code","ea3378ca":"code","953ca6f5":"code","e2ce6efe":"code","051997fe":"code","20c04c0e":"code","df46f384":"code","da81d9cc":"code","44d4ce1f":"code","4942ada5":"code","75cf0144":"code","8214729c":"code","9f82be15":"code","576a9012":"code","e5f12dad":"code","15b4ac0f":"code","506f7f00":"code","4ae627c7":"code","ba9cf0b0":"code","08345206":"code","f3030b64":"code","be6feb7d":"code","c654452c":"code","eb37cfb1":"markdown","6ce7c7a7":"markdown","d6cc3cee":"markdown","e6a207af":"markdown","e7640c11":"markdown","93debff5":"markdown","7fb60661":"markdown","78022ce9":"markdown","601a2cd1":"markdown"},"source":{"b0d6a6ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3be485a3":"!pip install BeautifulSoup4\n!pip install pydot","ea3378ca":"import os\nfrom bs4 import BeautifulSoup\nfrom collections import Counter,defaultdict\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport re\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold,KFold,StratifiedShuffleSplit,cross_val_predict\nfrom lightgbm import LGBMClassifier as lg\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D\nfrom keras.optimizers import Adam\nimport numpy as np  \nimport pandas as pd \nimport keras.backend as k\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,GRU\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel,TFAutoModel\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nimport string\nfrom unidecode import unidecode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n","953ca6f5":"train_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_train.json\").reset_index(drop=True)\ntest_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_test.json\").reset_index(drop=True)\ntrain_extra_df= pd.read_json(\"..\/input\/github-bugs-prediction\/embold_train_extra.json\").reset_index(drop=True)","e2ce6efe":"def fx(x):\n    return x['title'] + \" \" + x['body']   \ntrain_df['text']= train_df.apply(lambda x : fx(x),axis=1)\ntest_df['text']= test_df.apply(lambda x : fx(x),axis=1)","051997fe":"cList = {\n            \"i'm\": \"i am\",\n            \"you're\": \"you are\",\n            \"it's\": \"it is\",\n            \"we're\": \"we are\",\n            \"we'll\": \"we will\",\n            \"That's\":\"that is\",\n            \"haven't\":\"have not\",\n            \"let's\":\"let us\",\n            \"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n            \"aren't\": \"are not \/ am not\",\n            \"can't\": \"cannot\",\n            \"can't've\": \"cannot have\",\n            \"'cause\": \"because\",\n            \"could've\": \"could have\",\n            \"couldn't\": \"could not\",\n            \"couldn't've\": \"could not have\",\n            \"didn't\": \"did not\",\n            \"doesn't\": \"does not\",\n            \"don't\": \"do not\",\n            \"hadn't\": \"had not\",\n            \"hadn't've\": \"had not have\",\n            \"hasn't\": \"has not\",\n            \"haven't\": \"have not\",\n            \"he'd\": \"he had \/ he would\",\n            \"he'd've\": \"he would have\",\n            \"he'll\": \"he shall \/ he will\",\n            \"he'll've\": \"he shall have \/ he will have\",\n            \"he's\": \"he has \/ he is\",\n            \"how'd\": \"how did\",\n            \"how'd'y\": \"how do you\",\n            \"how'll\": \"how will\",\n            \"how's\": \"how has \/ how is \/ how does\",\n            \"I'd\": \"I had \/ I would\",\n            \"I'd've\": \"I would have\",\n            \"I'll\": \"I shall \/ I will\",\n            \"I'll've\": \"I shall have \/ I will have\",\n            \"I'm\": \"I am\",\n            \"I've\": \"I have\",\n            \"isn't\": \"is not\",\n            \"it'd\": \"it had \/ it would\",\n            \"it'd've\": \"it would have\",\n            \"it'll\": \"it shall \/ it will\",\n            \"it'll've\": \"it shall have \/ it will have\",\n            \"it's\": \"it has \/ it is\",\n            \"let's\": \"let us\",\n            \"ma'am\": \"madam\",\n            \"mayn't\": \"may not\",\n            \"might've\": \"might have\",\n            \"mightn't\": \"might not\",\n            \"mightn't've\": \"might not have\",\n            \"must've\": \"must have\",\n            \"mustn't\": \"must not\",\n            \"mustn't've\": \"must not have\",\n            \"needn't\": \"need not\",\n            \"needn't've\": \"need not have\",\n            \"o'clock\": \"of the clock\",\n            \"oughtn't\": \"ought not\",\n            \"oughtn't've\": \"ought not have\",\n            \"shan't\": \"shall not\",\n            \"sha'n't\": \"shall not\",\n            \"shan't've\": \"shall not have\",\n            \"she'd\": \"she had \/ she would\",\n            \"she'd've\": \"she would have\",\n            \"she'll\": \"she shall \/ she will\",\n            \"she'll've\": \"she shall have \/ she will have\",\n            \"she's\": \"she has \/ she is\",\n            \"should've\": \"should have\",\n            \"shouldn't\": \"should not\",\n            \"shouldn't've\": \"should not have\",\n            \"so've\": \"so have\",\n            \"so's\": \"so as \/ so is\",\n            \"that'd\": \"that would \/ that had\",\n            \"that'd've\": \"that would have\",\n            \"that's\": \"that has \/ that is\",\n            \"there'd\": \"there had \/ there would\",\n            \"there'd've\": \"there would have\",\n            \"there's\": \"there has \/ there is\",\n            \"they'd\": \"they had \/ they would\",\n            \"they'd've\": \"they would have\",\n            \"they'll\": \"they shall \/ they will\",\n            \"they'll've\": \"they shall have \/ they will have\",\n            \"they're\": \"they are\",\n            \"they've\": \"they have\",\n            \"to've\": \"to have\",\n            \"wasn't\": \"was not\",\n            \"we'd\": \"we had \/ we would\",\n            \"we'd've\": \"we would have\",\n            \"we'll\": \"we will\",\n            \"we'll've\": \"we will have\",\n            \"we're\": \"we are\",\n            \"we've\": \"we have\",\n            \"weren't\": \"were not\",\n            \"what'll\": \"what shall \/ what will\",\n            \"what'll've\": \"what shall have \/ what will have\",\n            \"what're\": \"what are\",\n            \"what's\": \"what has \/ what is\",\n            \"what've\": \"what have\",\n            \"when's\": \"when has \/ when is\",\n            \"when've\": \"when have\",\n            \"where'd\": \"where did\",\n            \"where's\": \"where has \/ where is\",\n            \"where've\": \"where have\",\n            \"who'll\": \"who shall \/ who will\",\n            \"who'll've\": \"who shall have \/ who will have\",\n            \"who's\": \"who has \/ who is\",\n            \"who've\": \"who have\",\n            \"why's\": \"why has \/ why is\",\n            \"why've\": \"why have\",\n            \"will've\": \"will have\",\n            \"won't\": \"will not\",\n            \"won't've\": \"will not have\",\n            \"would've\": \"would have\",\n            \"wouldn't\": \"would not\",\n            \"wouldn't've\": \"would not have\",\n            \"y'all\": \"you all\",\n            \"y'all'd\": \"you all would\",\n            \"y'all'd've\": \"you all would have\",\n            \"y'all're\": \"you all are\",\n            \"y'all've\": \"you all have\",\n            \"you'd\": \"you had \/ you would\",\n            \"you'd've\": \"you would have\",\n            \"you'll\": \"you shall \/ you will\",\n            \"you'll've\": \"you shall have \/ you will have\",\n            \"you're\": \"you are\",\n            \"you've\": \"you have\"\n           }","20c04c0e":"c_re = re.compile('(%s)' % '|'.join(cList.keys()))\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)","df46f384":"def remove_emoji(string):\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', string)","da81d9cc":"def remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data","44d4ce1f":"def removeSpecialChars(data):\n    '''\n    Removes special characters which are specifically found in tweets.\n    '''\n    #Converts HTML tags to the characters they represent\n    soup = BeautifulSoup(data, \"html.parser\")\n    data = soup.get_text()\n\n    #Convert www.* or https?:\/\/* to empty strings\n    data = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','',data)\n\n    #Convert @username to empty strings\n    data = re.sub('@[^\\s]+','',data)\n    \n    #remove org.apache. like texts\n    data =  re.sub('(\\w+\\.){2,}','',data)\n\n    #Remove additional white spaces\n    data = re.sub('[\\s]+', ' ', data)\n    \n    data = re.sub('\\.(?!$)', '', data)\n\n    #Replace #word with word\n    data = re.sub(r'#([^\\s]+)', r'\\1', data)\n\n    return data ","4942ada5":"def remove_nonenglish_charac(string):\n    return re.sub('\\W+','', string )","75cf0144":"extra_punctuations = ['','.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '\u2013','&']\nstopword_list = stopwords.words('english') + list(string.punctuation)+ extra_punctuations + ['u','the','us','say','that','he','me','she','get','rt','it','mt','via','not','and','let','so','say','dont','use','you']","8214729c":"def clean_text(data):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    stemmer = PorterStemmer() \n    tokenizer=TweetTokenizer()\n    data = unidecode(data)\n    data = expandContractions(data)\n    tokens = tokenizer.tokenize(data)\n    data = ' '.join([tok for tok in tokens if len(tok) > 2 if tok not in stopword_list and not tok.isdigit()])\n    data = re.sub('\\b\\w{,2}\\b', '', data)\n    data = re.sub(' +', ' ', data)\n    data = removeSpecialChars(data)\n    data = remove_emoji(data)\n    data= [stemmer.stem(w) for w in data.split()]\n    return ' '.join([wordnet_lemmatizer.lemmatize(word) for word in data])","9f82be15":"train_df['text'] = train_df['text'].apply(lambda x: clean_text(x))","576a9012":"train_y=train_df['label']\ntrain_x,val_x,train_y,val_y=train_test_split(train_df['text'],train_y,test_size=0.2,random_state=50)","e5f12dad":"try:    \n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    print('TPU is not initialized ')\n    tpu = None","15b4ac0f":"if tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\nprint(\"Replicas num: \", strategy.num_replicas_in_sync)\nAUTO = tf.data.experimental.AUTOTUNE\n# Configuration of hyperparameters\nEPOCHS = 3\n#batch size denotes the partitioning amongst the cluster replicas.\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192","506f7f00":"#Tokenize the data and separate them in chunks of 256 units\n\nmaxlen=512\nchunk_size=256\ndef quick_encode(texts,tokenizer, maxlen=maxlen):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        return_attention_mask=False,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen,\n        truncation=True,\n    )\n    return np.array(enc_di[\"input_ids\"])","4ae627c7":"def built_model(transformer,train_data,val_data,batch_size,img_name, max_len=512):\n    inp_words_ids = Input(shape =(max_len,),dtype = tf.int32,name=\"input_word_ids\")\n    seq_output = transformer(inp_words_ids)[0]\n    cls_token = seq_output[:,0,:]\n    output =  Dense(3,activation='softmax')(cls_token)\n    model = Model(inputs =inp_words_ids,outputs=output)\n    model.compile(Adam(lr=1e-5),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n    model.summary()\n    plot_model(\n        model,to_file=img_name,\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96)\n    model.fit(\n        train_data,\n        steps_per_epoch=batch_size,\n        validation_data=val_data,\n        epochs=EPOCHS\n        )","ba9cf0b0":"def del_obects(*args):\n    for arg in args:\n        del arg\n        gc.collect()","08345206":"with strategy.scope():\n\n    distilbert_tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n    train_x_enc= quick_encode(train_x.astype(str),distilbert_tokenizer, maxlen=MAX_LEN)\n    val_x_enc = quick_encode(val_x.astype(str),distilbert_tokenizer,maxlen=MAX_LEN)\n    train_dataset = (\n                    tf.data.Dataset.from_tensor_slices((train_x_enc,train_y))\n                    .repeat()\n                    .shuffle(2048)\n                    .batch(BATCH_SIZE)\n                    .prefetch(AUTO)\n                    )\n    valid_dataset = (\n        tf.data.Dataset.from_tensor_slices((val_x_enc,val_y))\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)                \n    )\n    transformer_layer = TFAutoModel.from_pretrained('distilbert-base-multilingual-cased')\n    built_model(transformer_layer,train_dataset,valid_dataset,train_x_enc.shape[0],\n                \"Distilbert_Multilingual_Transformer.png\",max_len=MAX_LEN)\n    \n    del_obects(train_x_enc,val_x_enc,train_dataset,valid_dataset,distilbert_tokenizer,transformer_layer)\n    ","f3030b64":"with strategy.scope():\n\n    albert_tokenizer = transformers.AlbertTokenizer.from_pretrained('albert-base-v1')\n    train_x_enc= quick_encode(train_x.astype(str),albert_tokenizer, maxlen=MAX_LEN)\n    val_x_enc = quick_encode(val_x.astype(str),albert_tokenizer,maxlen=MAX_LEN)\n    train_dataset = (\n                    tf.data.Dataset.from_tensor_slices((train_x_enc,train_y)\n                    .repeat()\n                    .shuffle(2048)\n                    .batch(BATCH_SIZE)\n                    .prefetch(AUTO)\n                    )\n    valid_dataset = (\n        tf.data.Dataset.from_tensor_slices((val_x_enc,val_y))\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)                \n    )\n    transformer_layer = TFAutoModel.from_pretrained('albert-base-v1')\n    built_model(transformer_layer,train_dataset,valid_dataset,train_x_enc.shape[0],\n                \"AlbertTransformer.png\",max_len=MAX_LEN)\n    \n    del_obects(train_x_enc,val_x_enc,train_dataset,valid_dataset,albert_tokenizer,transformer_layer)","be6feb7d":"with strategy.scope():\n\n    xlm_roberta_tokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n    train_x_enc= quick_encode(train_x.astype(str),xlm_roberta_tokenizer, maxlen=MAX_LEN)\n    val_x_enc = quick_encode(val_x.astype(str),xlm_roberta_tokenizer,maxlen=MAX_LEN)\n    train_dataset = (\n                    tf.data.Dataset.from_tensor_slices((train_x_enc,train_y))\n                    .repeat()\n                    .shuffle(2048)\n                    .batch(BATCH_SIZE)\n                    .prefetch(AUTO)\n                    )\n    valid_dataset = (\n        tf.data.Dataset.from_tensor_slices((val_x_enc,val_y))\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)                \n    )\n    transformer_layer = TFAutoModel.from_pretrained('xlm-roberta-base')\n    built_model(transformer_layer,train_dataset,valid_dataset,train_x_enc.shape[0],\n                \"Roberta-Transformer.png\",max_len=MAX_LEN)\n    \n    del_obects(train_x_enc,val_x_enc,train_dataset,valid_dataset,xlm_roberta_tokenizer,transformer_layer)","c654452c":"with strategy.scope():\n\n    gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2-medium')\n    train_x_enc= quick_encode(train_x.astype(str),gpt2_tokenizer, maxlen=MAX_LEN)\n    val_x_enc = quick_encode(val_x.astype(str),gpt2_tokenizer,maxlen=MAX_LEN)\n    train_dataset = (\n                    tf.data.Dataset.from_tensor_slices((train_x_enc,train_y))\n                    .repeat()\n                    .shuffle(2048)\n                    .batch(BATCH_SIZE)\n                    .prefetch(AUTO)\n                    )\n    valid_dataset = (\n        tf.data.Dataset.from_tensor_slices((val_x_enc,val_y))\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)                \n    )\n    transformer_layer = TFAutoModel.from_pretrained('gpt2-medium')\n    built_model(transformer_layer,train_dataset,valid_dataset,train_x_enc.shape[0],\n                \"GPT2-Transformer.png\",max_len=MAX_LEN)\n    \n    del_obects(train_x_enc,val_x_enc,train_dataset,valid_dataset,gpt2_tokenizer,transformer_layer","eb37cfb1":"# Conclusion of Notebook\nThis notebook concludes classification models created using all BERT-based transformer from Bert to Gpt. TThanks to Colearning lounge NLP workshop and my mentor @abhilash1910 ","6ce7c7a7":"Slicing the important part For sentence classification, we\u2019re only only interested in BERT\u2019s output for the [CLS] token, so we select that slice of the cube and discard everything else.\n\n![image.png](attachment:image.png)","d6cc3cee":"- In this notebook, I will try to demonstrate how to build different type of transformer.I will be creating different flavours of BERT to GPT. Majority of the concepts and code block for the below kernel are derived from the below notebook:\n[nlp-end-to-end by colearninglounge](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop-2#DistilBERT-Model)\n\nThe following is the workflow of this notebook:\n1. Import the library\n2. Load the dataset\n3. Clean the datset\n4. Configure the TPU\n5. Fast-encoding\n6. Different types of transformer model","e6a207af":"# Albert Transformer\n- Albert is a lightweight bert which introduces parameter sharing, caching, and intermediate repeated splitting of the embedding matrix for efficient modelling tasks.\n\n- According to the paper:\n\n- 'The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.'","e7640c11":"# Bert\nBERT is bidirectional encoder Transformer model\n![image.png](attachment:image.png)","93debff5":"The entire workflow can be designed as follows:\n\nThis image can be used to describe the workflow:","7fb60661":"# Distilbert Transformer:\n- This is a distilled version of pretraining BERT to produce a lightweight version of it. It is analogous to teacher supervision of a neural network learning to optimize tis weights. DistilBERT Paper provides an insight why it is 40% smaller but preserves 95% of BERT's weights for transfer learning.\n- DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language understanding benchmark. 2 significant benchmarks aspects of this Model:\n\n- Quantization :This leads to approximation of internal weight vectors to a numerically smaller precision\n- Weights Pruning: Removing some connections from the network.\n- Knowledge distillation (sometimes also referred to as teacher-student learning) is a compression technique in which a small model is trained to reproduce the behavior of a larger model (or an ensemble of models). It was introduced by Bucila et al. and generalized by Hinton et al. a few years later. We will follow the latter method.Rather than training with a cross-entropy over the hard targets (one-hot encoding of the gold class), we transfer the knowledge from the teacher to the student with a cross-entropy over the soft targets (probabilities of the teacher). Our training loss thus becomes:\n","78022ce9":"# XLM Roberta\/Roberta:\n- XLM builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n\n![image.png](attachment:image.png)\nTips:\n\nThis implementation is the same as BertModel with a tiny embeddings tweak as well as a setup for Roberta pretrained models.\n\nRoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.\n\nRoBERTa doesn\u2019t have token_type_ids, you don\u2019t need to indicate which token belongs to which segment. Just separate your segments with the separation token tokenizer.sep_token (or <\/s>)\n\nCamemBERT is a wrapper around RoBERTa.","601a2cd1":"# GPT-Generative Pretraining\n\n- It large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nTips:\n\nGPT-2 is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\nGPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n\nThe PyTorch models can take the past as input, which is the previously computed key\/value attention pairs."}}