{"cell_type":{"860def99":"code","65d13f2f":"code","ec0184f8":"code","c0fa343c":"code","029b5005":"code","8db62402":"code","60131123":"code","6aab57ac":"code","6072a55f":"code","e57a6cd8":"code","997fd9d7":"code","714e60bc":"code","4651d601":"code","1a13345e":"code","0e39b97b":"code","edf05197":"code","24e39d33":"code","a7bbcf78":"code","2d5c6070":"code","8d47ad1a":"code","f5298e0f":"code","98ccc6c9":"code","71ea3f2d":"code","c8c603c3":"markdown","8e4ee550":"markdown","1d672295":"markdown","c95458fe":"markdown"},"source":{"860def99":"# I have used the naive bayes classifier to get the highrst score","65d13f2f":"## Importing the libraries","ec0184f8":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","c0fa343c":"df=pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")","029b5005":"df.head()","8db62402":"print(df.shape)","60131123":"le=LabelEncoder()\n#apply the tranform to each columns\nds=df.apply(le.fit_transform)","6aab57ac":"ds.head()","6072a55f":"ds.shape","e57a6cd8":"#convert into numpy arrays\ndata=ds.values\ndata.shape","997fd9d7":"print(data[:5,:])","714e60bc":"#break the data into x an y\ny=data[:,0]\nx=data[:,1:]","4651d601":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)","1a13345e":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","0e39b97b":"print(x_test)\nprint(y_test)","edf05197":"np.unique(y_train)","24e39d33":"def prior_prob(y_train,label):\n    total_examples=y_train.shape[0]\n    class_examples=np.sum(y_train==label)\n    return class_examples\/float(total_examples)\n    ","a7bbcf78":"#prior_prob(y,1)","2d5c6070":"def cond_prob(x_train,y_train,feature_col,feature_val,label):\n    x_filtered=x_train[y_train==label]\n    numerator=np.sum(x_filtered[:,feature_col]==feature_val)\n    denominator=np.sum(y_train==label)\n    return numerator\/float(denominator)","8d47ad1a":"\ndef predict(x_train,y_train,xtest):\n    \"\"\"Xtest is a single testing point, n features\"\"\"\n    \n    classes = np.unique(y_train)\n    n_features = x_train.shape[1]\n    post_probs = [] # List of prob for all classes and given a single testing point\n    #Compute Posterior for each class\n    for label in classes:\n        \n        #Post_c = likelihood*prior\n        likelihood = 1.0\n        for f in range(n_features):\n            cond = cond_prob(x_train,y_train,f,xtest[f],label)\n            likelihood *= cond \n            \n        prior = prior_prob(y_train,label)\n        post = likelihood*prior\n        post_probs.append(post)\n        \n    pred = np.argmax(post_probs)\n    return pred","f5298e0f":"output = predict(x_train,y_train,x_test[1])\nprint(output)\nprint(y_test[1])","98ccc6c9":"\ndef score(x_train,y_train,x_test,y_test):\n\n    pred = []\n    for i in range(x_test.shape[0]):\n        pred_label = predict(x_train,y_train,x_test[i])\n        pred.append(pred_label) # <===Correction\n    \n    pred = np.array(pred)\n    \n    accuracy = np.sum(pred==y_test)\/y_test.shape[0]\n    return accuracy","71ea3f2d":"print(score(x_train,y_train,x_test,y_test))","c8c603c3":"## Build the classifier","8e4ee550":"## convert the data into numerical from categorical","1d672295":"## Break the data into train test split","c95458fe":"## computing posterior probability for each test example"}}