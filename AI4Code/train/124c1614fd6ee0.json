{"cell_type":{"306defd5":"code","9cce5167":"code","a0d98b50":"code","6cd7fe78":"code","ff8f7c05":"code","d0982c45":"code","e13d3455":"code","e08969ce":"code","3852ad87":"code","dd283285":"code","1d1f2e6c":"code","254ac8c2":"code","12e99c8e":"code","881e55b2":"markdown","9dd70677":"markdown","f6d3b4bf":"markdown","d8901545":"markdown","72e877c0":"markdown","72074227":"markdown","1957a87a":"markdown","bc7bc72c":"markdown","3fdd1f1c":"markdown","e866d124":"markdown"},"source":{"306defd5":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Utils\nfrom sklearn.utils.multiclass import unique_labels\n# Text Processing\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\nfrom sklearn.feature_selection import SelectKBest, chi2 ,RFECV\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import ngrams\nfrom collections import defaultdict,Counter\nimport string\nimport re\n\n#Spacy NLP\nfrom spacy import displacy\nimport en_core_web_sm\n\n\n# Model Evaluation\nfrom sklearn.model_selection import train_test_split,StratifiedKFold ,cross_val_score\nfrom sklearn.metrics import f1_score,classification_report\nfrom sklearn.metrics import confusion_matrix\n#Graphics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn import svm\n\n#disabel annoying sklearn warnings:\nimport warnings\nwarnings.filterwarnings('ignore') \n\nprint(os.listdir(\"..\/input\"))\npd.set_option('display.max_rows',20)\n%matplotlib inline","9cce5167":"# Input data files are available in the \"..\/input\/\" directory.\ntrain_df=pd.read_csv('..\/input\/cp_challenge_train.csv')\ncompetition_test_df=pd.read_csv('..\/input\/challenge_testset.csv')","a0d98b50":"print(train_df.shape,competition_test_df.shape)\ncounts= train_df['label'].value_counts()\npd.DataFrame({'Sample_Count': counts,'Percentages':counts\/train_df.shape[0]}).style.format({'Percentages': '{:.2%}'})","6cd7fe78":"# Try to predict it yourself (get a sense of how hard it is)\ntrain_df['num_sents']= train_df['Plot'].apply(lambda x: len(x.split('.')))\nrnd_sample=train_df[train_df['num_sents']==1].sample()\nrnd_sample['Plot'].tolist()","ff8f7c05":"rnd_sample['Title'].tolist()","d0982c45":"rnd_sample['label'].tolist()","e13d3455":"# Use Title \ntrain_df['Plot']=train_df['Plot']+' '+train_df['Title']\ncompetition_test_df['Plot']=competition_test_df['Plot']+' '+competition_test_df['Title']","e08969ce":"# Split the train_df into train & test sets \ny = train_df['label']\nX = train_df['Plot']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0,shuffle=True,stratify=y )\nprint(y_train.shape[0],y_test.shape[0])\n# print(\"X_test sample counts:\")\n# counts=y_test.value_counts()\n# pd.DataFrame({'Sample_Count': counts,'Percentages':counts\/y_test.shape[0]}).style.format({'Percentages': '{:.2%}'})","3852ad87":"# Vectorize 1:\nnlp = en_core_web_sm.load()\ndef spacy_tokenizer(text):\n    tokenlist=[]\n    mytokens = nlp(text)\n    for token in mytokens:\n        # 1.Remove stop words ,punctuations and pronouns,2.lemmatize\n        if not token.is_stop and not token.is_punct:\n            if token.lemma_ != \"-PRON-\":\n                tokenlist.append(token.lemma_)\n#                 tokenlist.append(token.text)\n    return tokenlist\nvect = CountVectorizer(tokenizer = spacy_tokenizer,min_df=4,binary=True,ngram_range=(1,1) )\ntrain_sprse = vect.fit_transform(X_train)\ntest_sprse = vect.transform(X_test)\n# In dataframe form makes it easier later on\nX_train_df = pd.DataFrame(train_sprse.toarray(),columns=vect.get_feature_names())\nX_test_df = pd.DataFrame(test_sprse.toarray(),columns=vect.get_feature_names())\nprint(\"Number of features(unique tokens):\",len(vect.get_feature_names()))","dd283285":"# Vectorize 2:\nvect = CountVectorizer(stop_words='english',min_df=4,binary=True,ngram_range=(1,1) )\n# Using the out-of-the-box tfidf vectorizer doesn't make sense\n# vect = TfidfVectorizer(stop_words='english',min_df=1,binary=True,sublinear_tf =True)\ntrain_sprse = vect.fit_transform(X_train)\ntest_sprse = vect.transform(X_test)\n# In dataframe form makes it easier later on\nX_train_df = pd.DataFrame(train_sprse.toarray(),columns=vect.get_feature_names())\nX_test_df = pd.DataFrame(test_sprse.toarray(),columns=vect.get_feature_names())\nprint(\"Number of features(unique tokens):\",len(vect.get_feature_names()))","1d1f2e6c":"for genre in unique_labels(y_train):\n    y_binary= pd.Series(np.where(y_train.values == genre, 1, 0),y_train.index)\n    print(genre)\n    res = chi2(train_sprse, y_binary)\n    df = pd.DataFrame({'chi2':res[0],'pval':res[1]},index=vect.get_feature_names())\n    df.sort_values(by='chi2' ,axis=0,ascending=False, inplace=True)\n    print(df.head(10),end='\\n\\n')\n    \n# ch2 = SelectKBest(chi2,k=100)\n# train_sprse= ch2.fit_transform(train_sprse,y_train)","254ac8c2":"clf = MultinomialNB()\n# StratifiedKFold CV F1 score:\nprint(cross_val_score(clf,train_sprse,y_train,cv=5,scoring='f1_micro'))\n\n# Fit and predict\nmodel = clf.fit(train_sprse, y_train)\ny_pred = model.predict(test_sprse)","12e99c8e":"# Investigate model predictions\nprint(classification_report(y_test, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False))\nprint(\"predictions counter:\",Counter(y_pred))\n\n#Sorted confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=unique_labels(y_test) ,columns = unique_labels(y_test))\nsorted_index=df_cm.sum(axis=0).sort_values(ascending=False).index\ndf_cm = df_cm.reindex(sorted_index, axis=1)\ndf_cm = df_cm.reindex(sorted_index, axis=0)\nplt.figure(figsize = (12,10))\nsns.heatmap(df_cm,annot=True,cmap='Blues', fmt='g',cbar=False)","881e55b2":"# BOW Vectorization","9dd70677":"Imbalanced Multi-Class ,top 3 make 70% of samples!","f6d3b4bf":"**** To predict on the competition_test_df , the model is retrained on all the train samples setting test_size=0. it somewhat improves the score.****","d8901545":"## What is Naive Bayes ?\n* **Generative** algorithm , learns P(X|y) & P(y)  *the class prior*\n* Under the independency 'Naive' assumption we can use Bayes Rule:\nP(y|X)=P(X|y)*P(y)\/P(X)\n* Use argmax(P) to predict the class","72e877c0":"# Data Processing","72074227":"## Why Naive Bayes ? \n#### Generally, as a baseline model:\n* Fast to run , fast to implement\n* Irrelevant features tend to cancel each other out.\n* If naivety holds, it is proved optimal (for the specific set of features)\n\n#### Specifically, for our problem:\n* Inherently Multinomial (as oppesed to binary classifiers where ovr,ova is required)\n* Embrace and enhance the imbalance \n* works well for many **equally important** features (as opposed to desicion trees for example)\n","1957a87a":"## *I hear ya*. Still, how such a simple baseline model is in the same ballpark with state of the art models?\n* Diminishing returns as we're approching the Bayes optimal error ,judging by human Level performance( it is **Natural** language processing).\n* There's not enough data to learn such complicated embeddings\/context patterns,many of the samples consist of only one line. \n* Heterogeneous data. The plots were written by different people over many decades, long enough that even slang has changed. ","bc7bc72c":"## Possible Improvements \n* Optimizing the vectorizer and alpha parameter of the NB\n* Ensemble using meta classifiers focused on the minority classes(Trees?).\n* over\/under sampling (recommended python library: imblearn ) \n","3fdd1f1c":"Shortcomings:\n1. Lost information: Words Order,meaning(semantics) & Context.\n2. Sparse, number of features is case-specific and usually vast.","e866d124":"# Exploratory Data Analysis"}}