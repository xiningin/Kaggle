{"cell_type":{"38a92613":"code","6e714f34":"code","d6875cab":"code","21aee737":"code","a9e36eda":"code","4bd64041":"code","268b0ff7":"code","a50277c7":"code","f7142844":"code","5908ca9f":"code","61c87e58":"code","02a5381e":"code","123e91c0":"code","a5d0b194":"code","796a5e25":"code","ffa2a1f3":"code","a7cbeb3d":"markdown","0c738bc0":"markdown","d48ee535":"markdown","3381fee4":"markdown","7aede4fe":"markdown","58b3dbcb":"markdown","0917d4ca":"markdown","ed690dd9":"markdown","98f5acc2":"markdown","ef54d588":"markdown","83996f4c":"markdown","3711158f":"markdown","a0c06e43":"markdown","6717c84b":"markdown","99983f94":"markdown"},"source":{"38a92613":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import skew","6e714f34":"advert =pd.read_csv(\"..\/input\/mydatasets\/Advertising.csv\")\nadvert.info()","d6875cab":"plt.style.use('ggplot')\nplt.rcParams['figure.figsize']=(12,8)\nsns.pairplot(advert, x_vars=['TV','Radio','Newspaper'],y_vars='Sales',kind='reg',height=5,aspect=1)","21aee737":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nX=advert[['TV','Radio','Newspaper']]\ny=advert['Sales']\n\nlm1=LinearRegression()\nlm1.fit(X,y)\n\nlm1_predict = lm1.predict(advert[['TV','Radio','Newspaper']])\nprint(\"R^2:\", round(r2_score(lm1_predict,y),4))","a9e36eda":"import PIL\nfrom PIL import Image \nImage.open(\"..\/input\/mydatasets2\/R2_score.png\")","4bd64041":"B1=lm1.coef_[0]\nB2=lm1.coef_[1]\nB3=lm1.coef_[2]\nc= lm1.intercept_\n\nprint(list(zip(['intercept:','B1:','B2:','B3:'],[c,B1,B2,B3])))\n\n#sales = intercept+ B1*TV + B2*Radi0 + B3*Newspaper+e","268b0ff7":"lm2=LinearRegression().fit(advert[['TV','Radio']],y)\nfrom sklearn.metrics import r2_score\nlm2_predict = lm2.predict(advert[['TV','Radio']]) \nprint(\"R^2:\",round(r2_score(lm2_predict,y),4))","a50277c7":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nX=advert[['TV','Radio']]\ny=advert['Sales']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1)\n\nlm_train=LinearRegression().fit(X_train,y_train)\nlm_predict =lm_train.predict(X_test)","f7142844":"print(\"RMSE :\", round(np.sqrt(metrics.mean_squared_error(lm_predict,y_test)),5))\nprint(\"R-sqaured :\",np.round(metrics.r2_score(lm_predict,y_test),4))#better score","5908ca9f":"from yellowbrick.regressor import PredictionError, ResidualsPlot\nvisualizer = PredictionError(lm_train).fit(X_train,y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","61c87e58":"visualizer = ResidualsPlot(LinearRegression())\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","02a5381e":"advert['prod_tv_radio'] = advert['TV'] * advert['Radio']","123e91c0":"X=advert[['TV','Radio','prod_tv_radio']]\ny=advert['Sales']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1)\n\nlm_prod=LinearRegression().fit(X_train,y_train)\nlm_predict2 =lm_prod.predict(X_test)","a5d0b194":"visualizer = PredictionError(lm_prod).fit(X_train,y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.poof()","796a5e25":"visualizer = ResidualsPlot(LinearRegression())\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","ffa2a1f3":"print(\"RMSE :\", round(np.sqrt(metrics.mean_squared_error(lm_predict2,y_test)),4))\nprint(\"R-sqaured :\",np.round(metrics.r2_score(lm_predict2,y_test),4))","a7cbeb3d":"# Multiple Linear Regression\nI am exploring the multi-linear regression with toy data. \n\nSo, let's get started.\n\nAs always, I hope you find this kernel useful and your UPVOTES would be highly appreciated","0c738bc0":"## 1. Import Libraries","d48ee535":"## 4. Estimating coefficients of regression line","3381fee4":"**Model lm_predict2 (with independent variables 'TV','Radio','product of TV & Radio)is best** among others, because its RMSE is lowest at 0.7514 and maximum R^2 is 0.98\n\nWhile for lm_predict1: RMSE=1.48 & R^2 is 0.90","7aede4fe":"### **Simple Linear Regression**: <h5 align=center>$$Y = \\beta_0 + \\beta_1 X + \\epsilon$$<\/h5>","58b3dbcb":"## Interpretation of R-Square:\nBest possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n\nAs R2_Score of the model is 0.8854, It's fair and good model.","0917d4ca":"## 3. Visualisation\nLet's see effect on 'sales'  of different independent variables with plots.","ed690dd9":"## 2. Load Advertising Data\nThis data set is collection of sales data with expenditure on Advertisements in TV, Radio and Newspapers.\n- 3 columns (TV, Radio, Newspaper)\n- 200 rows of expenditures among them","98f5acc2":"## 6. Model Evaluation (Train and Test sets)","ef54d588":"## Residual Plot with yellowbrick","83996f4c":"### Inference:\n\nSales increases with increment in expenditure of TV, Radio and Newspapers respectively. It shows linear relationship with \"Sales\" variable.","3711158f":"**Multiple Linear Regression for our data set**: \n\n<h5 align=center>$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 +...+ \\beta_p X_p + \\epsilon$$ <\/h5>\n<h5 align=center> $$sales = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times radio + \\beta_3 \\times newspaper + \\epsilon$$ <\/h5>","a0c06e43":"## 7. Interaction of 'TV' and 'Radio' features","6717c84b":"## **Equation for the multi-linear regression**\n\n<h4 align=\"center\"> $f(x) = \\beta_0 + \\sum_{j=1}^p X_j \\beta_j$. <\/h4>\n\n\nThe linear model either assumes that the regression function $E(Y|X)$ is linear, or that the linear model is a reasonable approximation.Here the $\\beta_j$'s are unknown parameters or coefficients, and the variables $X_j$ can come from different sources. No matter the source of $X_j$, the model is linear in the parameters.","99983f94":"## 5. Feature Selection based on maximum R-square value"}}