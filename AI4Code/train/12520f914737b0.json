{"cell_type":{"8ff3f04a":"code","20d80314":"code","cc68aa7c":"code","0de5a167":"code","1f247ec0":"code","dd17bc4c":"code","884cbb63":"code","a710eab6":"code","e49e9371":"code","9676c59c":"code","17abbd5d":"code","da08612d":"code","b68038ec":"code","5f2395b7":"code","8be8835d":"code","3442731c":"markdown","0a4bee48":"markdown","026a4356":"markdown","09e7ceab":"markdown","016499b0":"markdown","949ba648":"markdown","c270e132":"markdown","c63b96f0":"markdown","19bb4e0c":"markdown","0c8b8002":"markdown","299fa112":"markdown","45bc8b1f":"markdown","3d00b6e9":"markdown","3e6e2c1d":"markdown"},"source":{"8ff3f04a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","20d80314":"import torch\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom skimage import io\nfrom torchvision import transforms\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport os\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\nfrom tqdm.notebook import tqdm\nfrom scipy.special import softmax","cc68aa7c":"train_info = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2020-fgvc7\/train.csv\")\ntest_info = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2020-fgvc7\/test.csv\")\n\ntrain_info['dep'] = np.where(train_info['healthy']== 1, 0,\n                            np.where(train_info['multiple_diseases']== 1, 1,\n                                    np.where(train_info['rust']== 1, 2,\n                                            np.where(train_info['scab']== 1, 3,0))))\ntrain_info.head()","0de5a167":"train_on_gpu = torch.cuda.is_available()\nif not train_on_gpu:\n    print('CUDA is not available. Training on CPU ...')\nelse:\n    print('CUDA is available. Training on GPU ...')","1f247ec0":"# Data Augmentation\ntransform = transforms.Compose(\n                   [transforms.Resize((1024,1024)),\n                    transforms.RandomHorizontalFlip(),\n                    transforms.RandomVerticalFlip(),\n                    transforms.RandomRotation(degrees=30),\n                    transforms.CenterCrop(512),\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# Function to load train data\nclass load_data(Dataset):\n    def __init__(self,train_info,root_dir,transform=transform):\n        self.train_info=train_info\n        self.root_dir=root_dir\n        self.transform=transform\n    def __len__(self):\n        return len(train_info)\n    def __getitem__(self,idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        img_name = os.path.join(self.root_dir,self.train_info.iloc[idx,0])\n        img_name = img_name+'.jpg'\n        #print(img_name)\n        image = Image.open(img_name)\n        target = self.train_info['dep'].iloc[idx]\n        #target = np.array([target])\n        #target = target.astype('float').reshape(-1,4)\n        if self.transform:\n            image = self.transform(image)\n        return image, target\n\n# Function to load test data\nclass load_data_test(Dataset):\n    def __init__(self,train_info,root_dir,transform=transform):\n        self.test_info=test_info\n        self.root_dir=root_dir\n        self.transform=transform\n    def __len__(self):\n        return len(test_info)\n    def __getitem__(self,idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        img_name = os.path.join(self.root_dir,self.test_info.iloc[idx,0])\n        img_name = img_name+'.jpg'\n        image = Image.open(img_name)\n        if self.transform:\n            image = self.transform(image)\n        return image      \n\n\n    \nnum_workers = 0\n\ndset_train = load_data(train_info,'\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/')\ndset_test = load_data_test(test_info,'\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/')\nnum_train = len(dset_train)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(0.2 * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = DataLoader(dset_train,batch_size=15,sampler = train_sampler,num_workers=num_workers)\nvalid_loader = DataLoader(dset_train,batch_size=15,sampler = valid_sampler,num_workers=num_workers,drop_last=True)\ntest_loader = DataLoader(dset_test,batch_size=15,num_workers=num_workers)","dd17bc4c":"def imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax\n\ndataiter=iter(test_loader)\nimages=dataiter.next()\n\nfig, axes = plt.subplots(figsize=(10,4),ncols=5)\nfor i in range(5):\n    ax = axes[i]\n    imshow(images[i],ax=ax, normalize=False)\n","884cbb63":"dataiter=iter(train_loader)\nimages,labels=dataiter.next()\nprint(type(images))\nprint(images.shape)\nprint(labels.shape)","a710eab6":"# define the CNN architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # convolutional layer (sees 512x512x3 image tensor)\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        # convolutional layer (sees 256x256x16 tensor)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        # convolutional layer (sees 128x128x32 tensor)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n        # max pooling layer\n        self.pool = nn.MaxPool2d(2, 2)\n        # linear layer (64 * 64 * 64 -> 1024)\n        self.fc1 = nn.Linear(64*64*64, 1024)\n        # linear layer (1024 -> 512)\n        self.fc2 = nn.Linear(1024, 512)\n        # linear layer (500 -> 4)\n        self.fc3 = nn.Linear(512, 4)\n        # dropout layer (p=0.25)\n        self.dropout = nn.Dropout(0.40)\n       \n\n    def forward(self, x):\n        # add sequence of convolutional and max pooling layers\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        # flatten image input\n        x = x.view(x.shape[0],-1)\n        # add dropout layer\n        x = self.dropout(x)\n        # add 1st hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        # add dropout layer\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        # add 2nd hidden layer, with relu activation function\n        x = self.fc3(x)\n        return x\n\n# create a complete CNN\nmodel = Net()\nprint(model)\n# move tensors to GPU if CUDA is available\nif train_on_gpu:\n    model.cuda()","e49e9371":"# specify loss function (categorical cross-entropy)\ncriterion = nn.CrossEntropyLoss()\n# specify optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.05)","9676c59c":"# number of epochs to train the model\nn_epochs = 60\n\nvalid_loss_min = np.Inf # track change in validation loss\n\nfor epoch in range(1, n_epochs+1):\n    t1=time.time()\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train()\n    for data, target in train_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        \n    ######################    \n    # validate the model #\n    ######################\n    model.eval()\n    for data, target in valid_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # update average validation loss \n        valid_loss += loss.item()*data.size(0)\n    \n    # calculate average losses\n    train_loss = train_loss\/len(train_loader.sampler)\n    valid_loss = valid_loss\/len(valid_loader.sampler)\n        \n    # print training\/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTime Taken: {:.6f}'.format(\n        epoch, train_loss, valid_loss,(time.time() - t1)))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'plant_path.pt')\n        valid_loss_min = valid_loss\n","17abbd5d":"classes = ['healthy', 'multiple_diseases', 'rust', 'scab']","da08612d":"# track test loss\nbatch_size = 15\nvalid_loss = 0.0\nclass_correct = list(0. for i in range(4))\nclass_total = list(0. for i in range(4))\n\nmodel.eval()\n# iterate over test data\nfor data, target in valid_loader:\n    # move tensors to GPU if CUDA is available\n    if train_on_gpu:\n        data, target = data.cuda(), target.cuda()\n    # forward pass: compute predicted outputs by passing inputs to the model\n    \n    output = model(data)\n    # calculate the batch loss\n    loss = criterion(output, target)\n    # update test loss \n    valid_loss += loss.item()*data.size(0)\n    # convert output probabilities to predicted class\n    _, pred = torch.max(output, 1)    \n    # compare predictions to true label\n    correct_tensor = pred.eq(target.data.view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    \n    # calculate test accuracy for each object class\n    for i in range(batch_size):\n        label = target.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n\n# average test loss\nvalid_loss = valid_loss\/len(valid_loader.dataset)\nprint('Test Loss: {:.6f}\\n'.format(valid_loss))\n\nfor i in range(4):\n    if class_total[i] > 0:\n        print('Test Accuracy of %5s: %2d%% (%2d\/%2d)' % (\n            classes[i], 100 * class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %5s: N\/A (no training examples)' % (classes[i]))\n\nprint('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","b68038ec":"def test_fn(net, loader):\n    preds_for_output = np.zeros((1,4))\n    with torch.no_grad():\n        pbar = tqdm(total = len(loader))\n        for _, images in enumerate(loader):\n            images = images.to('cuda')\n            net.eval()\n            predictions = net(images)\n            preds_for_output = np.concatenate((preds_for_output, predictions.cpu().detach().numpy()), 0)\n            pbar.update()\n    pbar.close()\n    return preds_for_output","5f2395b7":"out = test_fn(model, test_loader)\noutput = pd.DataFrame(softmax(out,1), columns = ['healthy','multiple_diseases','rust','scab']) #the submission expects probability scores for each class\noutput.drop(0, inplace = True)\noutput.reset_index(drop=True,inplace=True)\noutput['image_id'] = test_info.image_id\noutput = output[['image_id','healthy','multiple_diseases','rust','scab']]","8be8835d":"output.head()","3442731c":"## Specify Loss Function and Optimizer\n\n- The loss function is used to calculate the loss for each forward pass. \n- The optimizer is used for updating the weight of the network after each forward pass.","0a4bee48":"# Checking if GPU is available","026a4356":"# Creating the submission file\n\n- For creating the submission file, I have used the function written by Akash Haridas (https:\/\/www.kaggle.com\/akasharidas) \n- Code source: (https:\/\/www.kaggle.com\/akasharidas\/plant-pathology-2020-in-pytorch-0-971-score) ","09e7ceab":"Defining a CNN architecture. The architecture will have the following layers:\n\n- Convolutional layers, which can be thought of as stack of filtered images.\n- Maxpooling layers, which reduce the x-y size of an input, keeping only the most active pixels from the previous layer.\n- The usual Linear + Dropout layers to avoid overfitting and produce a 4-dim output.\n\nThe original dataset contains images of various sizes. After transforming the images, the network will be trained on batches of images with size 512x512\n\nThe more convolutional layers you include, the more complex patterns in color and shape a model can detect. It's suggested that your final model include 2 or 3 convolutional layers as well as linear layers + dropout in between to avoid overfitting.\n\nThis network architecture is as follows:\n- 3 Convolutional layers\n- Each convolutional layer will be fed into a pooling layer for reducing dimensionality\n- There are 3 fully connected Linear layers which will output 4 probabilities for each image for each class\n- The Dropout layers are used to avoid overfitting and produce a 4-dim output.","016499b0":"## Testing the Trained Network\nFor testing the network, I have used the validation dataset. ","949ba648":"I have created a new single target variable called \"dep\" as seen above. I will be using \"dep\" for training and validation","c270e132":"## Importing the CSV file which have the class labels for all the images in training set","c63b96f0":"## Importing required libraries","19bb4e0c":"## Training the model\n\nHere i a m training the model for 60 epoch. After each epoch, the training and validation loss will be printed.\nEverytimg there is a decrease in the validation loss, that model is saved.","0c8b8002":"# This is a quick and dirty implementation of Convolutional Neural Network using PyTorch\n\n- The inspiraton for this notebook is the Udacity course : Intro to deep Learning with Pytorch (https:\/\/www.udacity.com\/course\/deep-learning-pytorch--ud188)\n- I have found the course to be very helpful for understaing the basics of Deep Learning and how it can be done using PyTorch\n- This is My first Kaggle submission (for deep learning)\n- Looking forward to your feedback. Cheers!!","299fa112":"## Data Augmentation\n- A common strategy for training neural networks is to introduce randomness in the input data itself. \n- For example, you can randomly rotate, mirror, scale, and\/or crop your images during training. \n- This will help your network generalize as it's seeing the same images but in different locations, with different sizes, in different orientations, etc.\n- To randomly rotate, scale and crop, then flip your images you would define your transforms like the one in the below code\n\n## Loading data\nHere I have use 2 custom function \"load_data\" and \"load_test_data\" for creating data loaders. \nThese function take the image id from train.csv file, the target variable (dep) and then load the image from the images folder\n\n- The DataLoader takes a dataset and returns batches of images and the corresponding labels. You can set various parameters like the batch size and if the data is shuffled after each epoch.\n- Here I am using a batch size of 15\n\n","45bc8b1f":"## Checking the dimensions of the train_loader","3d00b6e9":"# Importing\/transforming images and creating data loders for training\/validation and testing","3e6e2c1d":"## Visualize a Batch of Training Data"}}