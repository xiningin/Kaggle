{"cell_type":{"abb7f076":"code","09eb3d17":"code","35470b8f":"code","3dcb1127":"code","c61b5512":"code","7d9ab2c9":"markdown","8c261b06":"markdown","02cd07de":"markdown"},"source":{"abb7f076":"#It first move the whole file to memory and then work with memory\n#Good for sampling methods than jumps alot\nimport pdb\nimport os\nimport numpy as np\nimport random\n\n# Defined by Shahbaz\ntimestep = 45\nComputeInterArrival = True\nDescretesizeLength = False\nDirectionLengthCombined = True  #If true, direction will be represented by positive and negative sign in packet length\nNormalizeLength = True\nNormalizeInterArrival = True\nMaxLength = 1434\nMaxInterArrival = 1\nStarting_point = 0     #Indicates how many packets from the begining of the entire flow you want to skip\nStartingPointMultiply = 13  #start from multiply of this number\nNum_of_extracted_subflow = 100   #Number of subflows that are goining to be extracted from each flow if possible\nPaddingEnable = True\nPadAtTheBegining = True\nPaddingThreshold = 8    #The flow must have at least this amount of packets to be padded\n\n\nIncrementalSampling = False  #Cannot be both True\nRandomSampling = False\n\nCompureStatisticsInThisScript = True\n\nTrainingSetSize = 10\nNumOfCrossValidationFolds = 5\n\nnp.random.seed(20)\n\ninput_size = 2\nChannel_size = input_size\n\n\n#CS5-3 (Modified)\n#timestep = 75\n#SkipPacketsForSampling = 13\n#IncrementalSampling = True\n#NumberOfSamplesUntiIncrement = 10\n#IncrementalStepMultiplier = 1 \n\n#For our QUIC dataset\n\n#Cover up to 1000 packets\n#CS5-3 (Simple Sampling)\ntimestep = 45\nSkipPacketsForSampling = 22\nIncrementalSampling = True\nNumberOfSamplesUntiIncrement = 10\nIncrementalStepMultiplier = 1 \n\n#CS5-3-0.15 (Modified) (Random Sampling)\n#timestep = 45\n#Starting_point = 0     #Indicates how many packets from the begining of the entire flow you want to skip\n#StartingPointMultiply = 0  #start from multiply of this number\n#SkipPacketsForSampling = 1\n#SamplingProbability = 1\/22\n#IncrementalSampling = False\n#RandomSampling = True\n#NumberOfSamplesUntiIncrement = 10\n#IncrementalStepMultiplier = 1 \n\n#CS5-4-8-1.2\t(Incremental Sampling)\n#timestep = 45\n#SkipPacketsForSampling = 22\n#IncrementalSampling = True\n#RandomSampling=False\n#NumberOfSamplesUntiIncrement = 10\n#IncrementalStepMultiplier = 1.6\n\n\n##### For Waikato Dataset #########\n\n#CS2-3\t(Simple Sampling)\n#timestep = 45\n#SkipPacketsForSampling = 10\n#IncrementalSampling = True\n#NumberOfSamplesUntiIncrement = 10\n#IncrementalStepMultiplier = 1 \n\n#CS5-4-8-1.2\t(Incremental Sampling)\n#timestep = 45\n#SkipPacketsForSampling = 8\n#IncrementalSampling = True\n#NumberOfSamplesUntiIncrement = 10\n#IncrementalStepMultiplier = 1.2\n\n##CS5-3-0.15 (Random Sampling)\n#timestep = 45\n#Starting_point = 0     #Indicates how many packets from the begining of the entire flow you want to skip\n#StartingPointMultiply = 0  #start from multiply of this number\n#SkipPacketsForSampling = 1\n#SamplingProbability = 0.15\n#IncrementalSampling = False\n#RandomSampling = True\n#NumberOfSamplesUntiIncrement = 10\n#IncrementalStepMultiplier = 1 \n\ndef convertDataUnsupervised(data, labels, statlabels):\n    # shuffle the data\n    num_samples = labels.shape[0]\n    assert num_samples == data.shape[0]\n    perm = np.arange(num_samples)\n    np.random.shuffle(perm)\n\n    labels = labels[perm]\n    data = data[perm]\n    statlabels = statlabels[perm]\n\n    #Convert flat features into 2D input for CNN\n    temp = np.zeros((num_samples,Channel_size,timestep))\n    for i in range(num_samples):\n        t = data[i].reshape((timestep,input_size))\n        t = np.swapaxes(t,0,1)\n        temp[i] = t\n    data = temp\n\n    unsupervised_train = np.arange(num_samples)\n    unsupervised_train = perm[unsupervised_train]\n\n    unsuper_trainData = data[unsupervised_train, :, :timestep]\n    unsuper_trainLabel = statlabels[unsupervised_train]\n\n    return (unsuper_trainData, unsuper_trainLabel)\n\n\ndef convertDataSupervised(superdata, superlabels, supertestdata, supertestlabels):\n    # shuffle the data\n    num_samples2 = superlabels.shape[0]\n    num_samples3 = supertestlabels.shape[0]\n    assert num_samples2 == superdata.shape[0]\n    perm2 = np.arange(num_samples2)\n    perm3 = np.arange(num_samples3)\n    np.random.shuffle(perm2)\n    np.random.shuffle(perm3)\n\n    superlabels = superlabels[perm2]\n    superdata = superdata[perm2]\n    supertestlabels = supertestlabels[perm3]\n    supertestdata = supertestdata[perm3]\n\n    #Convert flat features into 2D input for CNN\n    temp = np.zeros((num_samples2,Channel_size,timestep))\n    for i in range(num_samples2):\n        t = superdata[i].reshape((timestep,input_size))\n        t = np.swapaxes(t,0,1)\n        temp[i] = t\n    superdata = temp\n\n    temp = np.zeros((num_samples3,Channel_size,timestep))\n    for i in range(num_samples3):\n        t = supertestdata[i].reshape((timestep,input_size))\n        t = np.swapaxes(t,0,1)\n        temp[i] = t\n    supertestdata = temp\n    #pdb.set_trace()\n\n    super_trainData = superdata[:, :, :timestep]\n    super_trainLabel = superlabels[:]\n\n    super_testData = supertestdata[:, :, :timestep]\n    super_testLabel = supertestlabels[:]\n\n    return (super_trainData, super_trainLabel, super_testData, super_testLabel)\n\ndef loadData(dirPath, extractedFlows = 0):\n    #If it is not set, use the global value\n    if extractedFlows == 0:\n        extractedFlows = Num_of_extracted_subflow\n\n\n\n    pathDir = os.listdir(dirPath)\n\n    train_datalist = []\n    train_labellist = []\n    test_datalist = []\n    test_labellist = []\n    FileCounter = 0\n    FlowCounter = 0\n    SubflowCounter = 0\n\n    # added by Shahbaz\n    custom_features = [\n        # 0,    #timestamp\n        1,  # RelativeTime\n        2,  # length\n        # 3    #Direction\n    ]\n\n    # added by Shahbaz\n    custom_labels = [\n       12,    #minLenForward\n        13,    #maxLenForward\n        14,    #avgLenForward\n        15,    #SDLenForward\n        16,    #minLenBackward\n        17,    #maxLenBackward\n        18,    #avgLenBackward\n        19,    #SDLenBackward\n         20,    #minLenAll\n          21,    #maxLenAll\n          22,    #avgLenAll\n          23,    #SDLenAll\n        24,    #minIATForward\n        25,    #maxIATForward\n        26,    #avgIATForward\n        27,    #SDIATForward\n        28,    #minIATBackward\n        29,    #maxIATBackward\n        30,    #avgIATBackward\n        31,    #SDIATBackward\n         32,    #minIATAll\n         33,    #maxIATAll\n         34,    #avgIATAll\n         35,    #SDIATAll\n\n        #Current version cannot compute these ones:\n#         36,    #PPSForward\n#         37,    #PPSBackward\n#         38,    #PPSAll\n#         39,    #BPSForward\n#         40,    #BPSBackward\n#         41,    #BPSAll\n    ]\n\n\n    for folder, subs, files in os.walk(dirPath):\n\n        #To make sure we get a different random set for each validation cross fold\n        files.sort()\n        np.random.shuffle(files)\n\n        for file in files:\n            filename = folder + \"\/\" + file\n\n            with open(filename) as f:\n                \n                FileCounter += 1\n                statFeatures = np.zeros(42)\n\n                EntireFile = []\n                for line in f:\n                    data = line.split()\n                    try:\n                        EntireFile.append(data)\n                    except:\n                        print(EntireFile)\n                        pdb.set_trace()\n                try:\n                    EntireFile = np.array(EntireFile).astype(np.float)\n                except:\n                    print(EntireFile)\n                    pdb.set_trace()    \n                if CompureStatisticsInThisScript and EntireFile.shape[1]==4:\n                    EntireFile[-1,0] =-1\n                    temparray = []\n                    for i in range(0,len(EntireFile)-1):\n                        if EntireFile[i,3] == 0:    #Direction\n                            temparray.append(EntireFile[i,2])\n                    if len(temparray) == 0:\n                        statFeatures[12] = 0\n                        statFeatures[13] = 0\n                        statFeatures[14] = 0\n                        statFeatures[15] = 0\n                    else:                       \n                        statFeatures[12] = np.min(temparray)\n                        statFeatures[13] = np.max(temparray)\n                        statFeatures[14] = np.mean(temparray)\n                        statFeatures[15] = np.std(temparray)\n                    \n                    temparray = []\n                    for i in range(0,len(EntireFile)-1):\n                        if EntireFile[i,3] == 1:    #Direction\n                            temparray.append(EntireFile[i,2])\n                    if len(temparray) == 0:\n                        statFeatures[16] = 0\n                        statFeatures[17] = 0\n                        statFeatures[18] = 0\n                        statFeatures[19] = 0\n                    else:\n                        statFeatures[16] = np.min(temparray)\n                        statFeatures[17] = np.max(temparray)\n                        statFeatures[18] = np.mean(temparray)\n                        statFeatures[19] = np.std(temparray)\n\n                    temparray = []\n                    for i in range(0,len(EntireFile)-1):\n                        temparray.append(EntireFile[i,2])\n                    if len(temparray) == 0:\n                        statFeatures[20] = 0\n                        statFeatures[21] = 0\n                        statFeatures[22] = 0\n                        statFeatures[23] = 0\n                    else:\n                        statFeatures[20] = np.min(temparray)\n                        statFeatures[21] = np.max(temparray)\n                        statFeatures[22] = np.mean(temparray)\n                        statFeatures[23] = np.std(temparray)                   \n\n                    temparray = []\n                    old_time = 0\n                    for i in range(0,len(EntireFile)-1):\n                        if EntireFile[i,3] == 0:    #Direction\n                            temparray.append(EntireFile[i,1] - old_time)\n                            old_time = EntireFile[i,1]\n                    if len(temparray) == 0:\n                        statFeatures[24] = 0\n                        statFeatures[25] = 0\n                        statFeatures[26] = 0\n                        statFeatures[27] = 0\n                    else:\n                        statFeatures[24] = np.min(temparray)\n                        statFeatures[25] = np.max(temparray)\n                        statFeatures[26] = np.mean(temparray)\n                        statFeatures[27] = np.std(temparray)\n                    \n                    temparray = []\n                    old_time = 0\n                    for i in range(0,len(EntireFile)-1):\n                        if EntireFile[i,3] ==1:    #Direction\n                            temparray.append(EntireFile[i,1] - old_time)\n                            old_time = EntireFile[i,1]\n                    if len(temparray) == 0:\n                        statFeatures[28] = 0\n                        statFeatures[29] = 0\n                        statFeatures[30] = 0\n                        statFeatures[31] = 0\n                    else:\n                        statFeatures[28] = np.min(temparray)\n                        statFeatures[29] = np.max(temparray)\n                        statFeatures[30] = np.mean(temparray)\n                        statFeatures[31] = np.std(temparray)\n\n                    temparray = []\n                    old_time = 0\n                    for i in range(0,len(EntireFile)-1):\n                        temparray.append(EntireFile[i,1] - old_time)\n                        old_time = EntireFile[i,1]\n                    if len(temparray) == 0:\n                        statFeatures[32] = 0\n                        statFeatures[33] = 0\n                        statFeatures[34] = 0\n                        statFeatures[35] = 0\n                    else:\n                        statFeatures[32] = np.min(temparray)\n                        statFeatures[33] = np.max(temparray)\n                        statFeatures[34] = np.mean(temparray)\n                        statFeatures[35] = np.std(temparray)  \n                    \n                    \n                FileLenght = len(EntireFile)\n                \n#                pdb.set_trace()\n                \n                temp_label = []\n                SubflowFromAFile = 0\n                #To detect TCP traffic and skip first 3 packets\n                # isFirstTCP=1;\n    \n                #Skip the fist few packets in the file\n                if(Starting_point!=0):\n                    for jjj in range(Starting_point):\n                        line = f.readline()\n                        if not line:\n                            break\n    \n                # 33 features for one line\n                # for i in range(10):\n                # Changed by Shahbaz\n                for subflow in range(extractedFlows):\n    \n                    startingPoint = Starting_point + subflow*StartingPointMultiply\n                    \n                    linedata = []\n                    Prev_time = 0;  #Time of the first packet in the subflow\n    \n                    indexes = custom_features[:]\n                    numOfSamples = 0\n                    i = startingPoint\n                    SkipSamples = SkipPacketsForSampling\n                    while(numOfSamples < timestep):    \n    #                for i in range(startingPoint, startingPoint+timestep*SkipPacketsForSampling,SkipPacketsForSampling):\n    \n                        if i>=FileLenght:\n                            break\n                        \n                        \n                        if RandomSampling and i!=0:\n                            if random.uniform(0,1) > SamplingProbability:\n                                i += 1\n                                continue\n\n                        data = list(EntireFile[i])  #To clone the list, not refering to the same list\n    \n#                        if data[0]==\"-1\" :   #it is the last line containing statistical data\n#    #                        if float(data[22]) < 200:\n#    #                            print(filename)\n#                            try:\n#                                temp_label = [float(data[j]) for j in custom_labels]\n#                            except (IndexError, ValueError) as e:\n#                                pass\n#                            print(\"label failed: \" + filename)\n#                            subflow = extractedFlows\n#                            break\n#                        \n#                        if(len(data)!=4):\n##                            print(filename)\n#                            break\n    \n                        \n                        #shahbaz: To descretesize the the length\n                        if DescretesizeLength:\n                            data[2] = str(int(int(data[2])\/100))\n\n                        if DirectionLengthCombined:\n                            if data[3]==\"0\":\n                                if float(data[11])>0:\n                                    data[2] = str(-1 * float(data[2]))\n                                    \n                        if NormalizeLength:\n                            data[2] = str(float(data[2])\/MaxLength)\n                            \n                        if ComputeInterArrival:\n                            if i==startingPoint:\n                                Prev_time = float(data[1])\n                                data[1] = str(0)\n                            else:\n                                temporary = str(float(data[1]) - Prev_time)\n                                Prev_time = float(data[1])                                \n                                data[1] = temporary\n                        if NormalizeInterArrival:\n                            ttt = float(data[1]) \/ MaxInterArrival\n                            if ttt > 1:\n                                ttt=1\n                            data[1]=(ttt-0.5)*2\n                            \n    \n                        try:\n                            data2 = [float(data[j]) for j in indexes]\n                        except (IndexError, ValueError) as e:\n                            pass\n#                            print(\"Couldn't retrieve all data\",filename)\n                        else:\n                            linedata += data2\n                \n                        numOfSamples += 1\n                        i += SkipSamples\n                        if IncrementalSampling:\n                            if numOfSamples % NumberOfSamplesUntiIncrement == 0:\n                                SkipSamples = int(SkipSamples*IncrementalStepMultiplier)\n    #                            print(SkipSamples)\n    \n                    if (len(linedata) < len(indexes) * timestep):\n                        if (PaddingThreshold > len(linedata)\/len(indexes) ):\n                            continue\n                        #print(linedata)\n                        if (PaddingEnable):\n                            while(len(linedata) < len(indexes) * timestep):\n                                pad = []\n                                pad.extend(np.ones(len(indexes)) * 0)\n                                if PadAtTheBegining:\n                                    pad.extend(linedata)\n                                    linedata = pad\n                                else:\n                                    linedata.extend(pad)\n                            #print(linedata)\n                        else:\n                            continue\n                    np.nan_to_num(linedata)\n                    #print(linedata)\n                    if FileCounter <= TrainingSetSize:\n                        train_datalist.append(linedata)\n                    else:\n                        test_datalist.append(linedata)\n    \n    #                temp = filename.split(\"-\")\n    #                temp = int(temp[8].split(\".\")[0])\n    #                #print(filename)\n    #                grouplabel.append(temp)\n    #                namelist.append(filename)\n                    SubflowCounter+=1\n                    SubflowFromAFile+=1\n    \n                    try:\n                        temp_label = [float(statFeatures[j]) for j in custom_labels]\n                    except (IndexError, ValueError) as e:\n                        pass\n                        print(\"label failed: \" + filename)   \n\n                #print(temp_label,SubflowFromAFile)\n                total_labels = [temp_label] * SubflowFromAFile\n                if FileCounter < TrainingSetSize:\n                    train_labellist.extend(total_labels)\n                else:\n                    test_labellist.extend(total_labels)\n                FlowCounter+=1\n\n    ratio = SubflowCounter\/FlowCounter\n    print(dirPath + \":\" + str(FlowCounter) + \"\/\" + str(len(pathDir)) + \" - Subflows:\" + str(SubflowCounter) + \" - Ratio:\", str(ratio))\n    return (np.array(train_datalist), train_labellist, np.array(test_datalist), test_labellist)\n\n\ndef norm(data):\n    data = data - np.amin(data, axis=0, keepdims=True)\n    data = data \/ (np.amax(data, axis=0, keepdims=True) - np.amin(data, axis=0, keepdims=True))\n    return data\n\ndef norm2(data1, data2):\n    temp = np.append(data1, data2, axis=0)\n    data1 = data1 - np.amin(temp, axis=0, keepdims=True)\n    data1 = data1 \/ (np.amax(temp, axis=0, keepdims=True) - np.amin(temp, axis=0, keepdims=True))\n    data2 = data2 - np.amin(temp, axis=0, keepdims=True)\n    data2 = data2 \/ (np.amax(temp, axis=0, keepdims=True) - np.amin(temp, axis=0, keepdims=True))\n    data1 = (data1 - 0.5)*2\n    data2 = (data2 - 0.5)*2\n    return (data1, data2)\nif __name__ == \"__main__\":\n\n\n    dataPath = \".\/\"\n#     dataPath = \"Data-simple-45-22\"\n#    dataPath = \"Data-random-45-22\"\n#    dataPath = \"Data-incremental-45-22-1.6\"\n#    dataPath = \"Data-simple-75\"\n#    dataPath = \"Data-incremental-45-22-1.6(Human)\"\n\n    #### For classification\n    BaseDirectory = \"..\/input\/semisupervisedlearningquic\/pretraining\"\n    (data, statlabel, data1, statlabel1) = loadData(BaseDirectory)\n    data = np.concatenate((data, data1), axis=0)\n    statlabel = np.concatenate((statlabel, statlabel1), axis=0)\n    label = np.ones(data.shape[0])\n    (data, statlabel) = convertDataUnsupervised(data, label, statlabel)\n    np.save(dataPath + \"\/pretraining_trainData.npy\", data)\n    np.save(dataPath + \"\/pretraining_trainLabel.npy\", label) #We actually do not need it\n    np.save(dataPath + \"\/StatLabel.npy\", statlabel)\n\n\n    for i in range(NumOfCrossValidationFolds):\n        BaseDirectory = \"..\/input\/semisupervisedlearningquic\/Retraining(script-triggered)\/Retraining(script-triggered)\"\n#        BaseDirectory = \"Data (unprocessed)\/Retraining(human-triggered)\"\n        (superdata1, superstatlabel11, test_data1, test_statlabel1) = loadData(BaseDirectory + \"\/Google Drive\/\", extractedFlows=100)\n        (superdata2, superstatlabel22, test_data2, test_statlabel2) = loadData(BaseDirectory + \"\/Youtube\/\", extractedFlows=100)\n        (superdata3, superstatlabel33, test_data3, test_statlabel3) = loadData(BaseDirectory + \"\/Google Doc\/\", extractedFlows=100)\n        (superdata4, superstatlabel44, test_data4, test_statlabel4) = loadData(BaseDirectory + \"\/Google Search\/\", extractedFlows=100)\n        (superdata5, superstatlabel55, test_data5, test_statlabel5) = loadData(BaseDirectory + \"\/Google Music\/\", extractedFlows=100)\n        \n\n        \n        superlabel1 = np.ones(superdata1.shape[0])\n        superlabel2 = np.ones(superdata2.shape[0]) * 2\n        superlabel3 = np.ones(superdata3.shape[0]) * 3\n        superlabel4 = np.ones(superdata4.shape[0]) * 4\n        superlabel5 = np.ones(superdata5.shape[0]) * 5\n        superdata = np.concatenate((superdata1, superdata2, superdata3, superdata4, superdata5), axis=0)\n        superlabel = np.concatenate((superlabel1, superlabel2, superlabel3, superlabel4, superlabel5), axis=0)\n        superstatlabel = np.concatenate((superstatlabel11, superstatlabel22, superstatlabel33, superstatlabel44, superstatlabel55), axis=0)\n\n        superlabel1 = np.ones(test_data1.shape[0])\n        superlabel2 = np.ones(test_data2.shape[0]) * 2\n        superlabel3 = np.ones(test_data3.shape[0]) * 3\n        superlabel4 = np.ones(test_data4.shape[0]) * 4\n        superlabel5 = np.ones(test_data5.shape[0]) * 5\n        testdata = np.concatenate((test_data1, test_data2, test_data3, test_data4, test_data5), axis=0)\n        testlabel = np.concatenate((superlabel1, superlabel2, superlabel3, superlabel4, superlabel5), axis=0)\n        teststatlabel = np.concatenate((test_statlabel1, test_statlabel2, test_statlabel3, test_statlabel4, test_statlabel5), axis=0)\n\n        (super_trainData, super_trainLabel,\n         super_testdata, super_testlabels) = convertDataSupervised(superdata, superlabel, testdata, testlabel)\n        \n        np.save(dataPath + \"\/re-training_trainData-\" + str(i) + \".npy\", super_trainData)\n        np.save(dataPath + \"\/re-training_trainLabel-\" + str(i) + \".npy\", super_trainLabel)\n        np.save(dataPath + \"\/SuperTrainStatLabel-\" + str(i) + \".npy\", superstatlabel)\n\n        np.save(dataPath + \"\/re-training_testData-\" + str(i) + \".npy\", super_testdata)\n        np.save(dataPath + \"\/re-training_testLabel-\" + str(i) + \".npy\", super_testlabels)\n        np.save(dataPath + \"\/SuperTestStatLabel-\" + str(i) + \".npy\", teststatlabel)\n\n        print(super_trainData.shape, super_testdata.shape)","09eb3d17":"import pdb\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport numpy as np\n\ninput_size = 2\nTIME_STEPS = 45\n\nclass unsuper_Dataset(torch.utils.data.Dataset):\n    def __init__(self, unsuper_trainData, unsuper_trainLabel):\n        data_size = unsuper_trainData.shape[0]\n        self.train_data = unsuper_trainData.reshape(data_size, input_size, TIME_STEPS)\n#        self.train_data = unsuper_trainData.reshape(data_size, input_size*TIME_STEPS)\n        self.train_labels = unsuper_trainLabel\n\n    def __getitem__(self, index):\n        data, target = self.train_data[index, :], self.train_labels[index, :]\n        return data, target\n\n    def __len__(self):\n        return len(self.train_labels)\n\n\nclass super_Dataset(torch.utils.data.Dataset):\n    def __init__(self, super_trainData, super_trainLabel, testData, testLabel, train=True):\n        self.train_data = super_trainData.reshape(super_trainData.shape[0], input_size, TIME_STEPS)\n#        self.train_data = super_trainData.reshape(super_trainData.shape[0], input_size*TIME_STEPS)\n        self.train_labels = super_trainLabel\n        self.test_data = testData.reshape(testData.shape[0], input_size, TIME_STEPS)\n#        self.test_data = testData.reshape(testData.shape[0], input_size*TIME_STEPS)\n        self.test_labels = testLabel\n        self.train = train\n\n    def __getitem__(self, index):\n        if self.train:\n            data, target = self.train_data[index, :], self.train_labels[index]\n        else:\n            data, target = self.test_data[index, :], self.test_labels[index]\n\n        return data, target\n\n    def __len__(self):\n        if self.train:\n            return len(self.train_labels)\n        else:\n            return len(self.test_labels)","35470b8f":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pdb\nfrom torch.nn import init\n\nclass CNNEncoder(nn.Module):\n    \n    def __init__(self, hidden_size, output_size, channels=2):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n\n        self.cnnseq = nn.Sequential(\n            nn.Conv1d(channels, 32, kernel_size=5, stride=2, bias=False),\n            nn.BatchNorm1d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(32, 32, kernel_size=5, stride=2, bias=False),\n            nn.BatchNorm1d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool1d(kernel_size=3, stride=2),\n            nn.BatchNorm1d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(32, 64, kernel_size=3, padding=2, bias=False),\n            nn.BatchNorm1d(64),\n            nn.ReLU(inplace=True),\n            #nn.Conv1d(64, 128, kernel_size=3, stride=2, bias=False),\n            #nn.BatchNorm1d(128),\n            #nn.ReLU(inplace=True),\n            nn.MaxPool1d(kernel_size=3, stride=2),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(64), #128\n        )\n        self.reggresor = nn.Sequential( #30:64 #45:128, #60:\n            nn.Linear(128, 256, bias=False),\n            nn.BatchNorm1d(256),\n            nn.ReLU(inplace=True),\n#            nn.Dropout(0.5),\n            nn.Linear(256, self.output_size)\n        )\n    def forward(self, images):\n\n        code = self.cnnseq(images)\n#        pdb.set_trace()\n        code = code.view([images.size(0), -1])\n        code = self.reggresor(code)\n        code = code.view([code.size(0), self.output_size])\n        return code   \n\n\n\nclass FCNN(nn.Module):\n    def __init__(self, input_size, output_size , time_steps=30):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.time_steps = time_steps\n\n        self.lin1 = nn.Linear(input_size*time_steps, input_size*time_steps)\n        self.lin2 = nn.Linear(input_size*time_steps, input_size*time_steps)\n        self.lin3 = nn.Linear(input_size*time_steps, input_size*time_steps)\n        self.lin4 = nn.Linear(input_size*time_steps, output_size)\n        self.Sigmoid = nn.Sigmoid()\n        \n        init.xavier_uniform_(self.lin1.weight)\n        self.lin1.bias.data.fill_(0.01)\n        init.xavier_uniform_(self.lin2.weight)\n        self.lin2.bias.data.fill_(0.01)\n        init.xavier_uniform_(self.lin3.weight)\n        self.lin3.bias.data.fill_(0.01)\n        init.xavier_uniform_(self.lin4.weight)\n        self.lin4.bias.data.fill_(0.01)\n        \n    def forward(self, input):\n        out = F.relu(self.lin1(input))\n        out = F.relu(self.lin2(out))\n#        out = F.relu(self.lin3(out))\n        out = self.lin4(out)\n        #out = self.Sigmoid(self.lin3(out))\n        return out\n    \n    \n    \nclass LinearClassifier(nn.Module):\n    \n    def __init__(self, in_dim, output_size, num_layers=1):\n        super().__init__()\n        self.num_layers = num_layers\n        self.in_dim = in_dim\n        self.linear1 = nn.Linear(in_dim, in_dim)\n        self.linear2 = nn.Linear(in_dim, in_dim)\n        self.linear3 = nn.Linear(in_dim, output_size)\n\n#        init.xavier_uniform(self.linear1.weight)\n#        self.linear1.bias.data.fill_(0.01)\n#        init.xavier_uniform(self.linear2.weight)\n#        self.linear2.bias.data.fill_(0.01)\n        \n    def forward(self, x):\n        out = F.relu(self.linear1(x))\n        out = F.relu(self.linear2(x))\n        out = self.linear3(out)\n        out = F.softmax(out, dim=1)\n        return out\n    \n    ","3dcb1127":"import pdb\nimport torch\nimport numpy as np\nimport time\nimport random\n# import myDataset\n# import Model\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch import optim\n\ntqdm.monitor_interval = 0\n\n# Some Global Variables\n# Training Parameters\nlearning_rate = 0.001\nbatch_size = 32\n\n\nMode = 0  # 0:Encoder-Linear     1:Encoder-Decoder\n# Network Parameters\ntime_steps = 45\n#output_time_steps = 15\ninput_size = 2    #100byte of payload + length + direction + timestamp\noutput_size = 24\nhidden_dim = 10     #number of features in hidden layer\nnum_epoches = 250\nnum_layers = 1    #number of hidden layer\n\n\ndisplay_step = 5\n\nteacher_forcing_ratio = 0.5\n\nunsuper_trainData = np.load(\"pretraining_trainData.npy\")\nunsuper_trainLabel = np.load(\"StatLabel.npy\")\n\n#pdb.set_trace()\n\nsample_size = unsuper_trainData.shape[0]\nperm = np.arange(sample_size)\n#np.random.shuffle(perm)\n#unsuper_trainData = unsuper_trainData[perm]\n#unsuper_trainLabel  = unsuper_trainLabel[perm]\ntestStart = int(sample_size * 0.96)\ntestData = unsuper_trainData[testStart:]\ntestLabel = unsuper_trainLabel[testStart:]\nunsuper_trainData = unsuper_trainData[:int(testStart\/1)]\nunsuper_trainLabel = unsuper_trainLabel[:int(testStart\/1)]\n\n# train_dataset = myDataset.unsuper_Dataset(unsuper_trainData,\n#                                           unsuper_trainLabel)\n\n# test_dataset = myDataset.unsuper_Dataset(testData, testLabel)\n\ntrain_dataset = unsuper_Dataset(unsuper_trainData,\n                                          unsuper_trainLabel)\n\ntest_dataset = unsuper_Dataset(testData, testLabel)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n\n#test_loader = torch.utils.data.DataLoader(\n#    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n\nprint(\"Dataset loaded!\")\n\nif Mode == 0:\n    use_gpu = True  # GPU enable\n    \n    encoder = CNNEncoder(hidden_dim, output_size)\n#     encoder = Model.CNNEncoder(hidden_dim, output_size)\n\n    \n    if use_gpu:\n        encoder = encoder.cuda()\n        #decoder = decoder.cuda()\n\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n\n    loss_function = nn.MSELoss()\n#    loss_function = nn.KLDivLoss()\n\n    #optimizer = optim.RMSprop(autoencoder.parameters(), lr = learning_rate, weight_decay = 0.005)\n    optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)#, weight_decay = 0.005)\n    train_loss = []\n\n#    sample = torch.randn(1,2,time_steps)\n#    sample = Variable(sample).float()\n#    sample = sample.cuda()\n#    print(sample)\n    isFirst = 1\n    for epoch in range(num_epoches):\n        running_loss = 0.0\n        avg_loss = 0.0\n        for i, data in enumerate(train_loader, 1):\n            seq, target = data\n            #target = target[:,1,:]\n            #target = target[:,1,:]\n            #target = seq[:,1,15:30]\n            #target = torch.ones(target.shape[0],1) \/ 1 * 1\n            #pdb.set_trace()\n            \n            seq = Variable(seq).float()\n            target = Variable(target).float()\n    \n            if use_gpu:\n                seq = seq.cuda()\n                target = target.cuda()\n\n            if isFirst == 1:\n                sample = seq[:6,:]\n                sampleTarget = target[:6,:]\n#                pdb.set_trace()\n#                sample = seq[:6,:,:]\n#                sampleTarget = target[:6,:]\n                ifFirst = 0\n                \n#            pdb.set_trace()\n            #print(encoder.enc_linear_1.weight.data)\n            out = encoder(seq)\n            loss = loss_function(out, target)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * target.size(0)\n                \n            #pdb.set_trace()\n\n\n        if epoch % display_step == 0:\n            avg_loss = running_loss \/ (batch_size * i)\n            print('[{}\/{}] Loss: {:.6f}'.format(epoch + 1, num_epoches, avg_loss))\n            #pdb.set_trace()\n            train_loss.append(avg_loss)\n            \n            eval_loss = 0.\n            eval_acc = 0.\n\n        torch.save(encoder, 'simple-45.pth')\n        torch.save(encoder, 'random-45.pth')\n        torch.save(encoder, 'incremental-45.pth')\n\n\nprint(\"Optimization Done!\")\nt = np.arange(len(train_loss))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"MSE Loss\")\nplt.plot(t, train_loss, color=\"red\", linewidth=2.5,\n         linestyle=\"-\", label=\"Unsupervised Loss\")\nplt.legend(loc='upper right')\nplt.show()\n","c61b5512":"import pdb\nimport torch\nimport numpy as np\nimport myDataset\n# import Model\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nimport tqdm\nimport torch.nn as nn\nfrom torch import optim\n\n# Some Global Variables\n# Training Parameters\nlearning_rate = 0.0001\nepsilon = 1e-08\nweight_decay = 0\nbatch_size = 64\n\n# Network Parameters\ntime_steps = 45\noutput_time_steps = 1\ninput_size = 24\noutput_size = 2 * output_time_steps    #for unsupervised learning\nnum_classes = 5     #for supervised learning \nstatisticalFeatures = 24\nhidden_dim = 10\nnum_epoches = 200\nnum_layers = 1\n\ndisplay_step = 10\n\nuse_gpu = True  # GPU enable\n\ntqdm.monitor_interval = 0\n\ntotal_acc_single = 0\ntotal_acc = np.zeros(num_classes)\ntotal_pre = np.zeros(num_classes)\ntotal_rec = np.zeros(num_classes)\ntotal_f1 = np.zeros(num_classes)\nfull_acc = []\nfull_pre = []\nfull_rec = []\nfull_f1 = []\n\nNumOfCrossValidationFolds = 5\nAccuracies = np.zeros(NumOfCrossValidationFolds)\nfor foldNumber in range(NumOfCrossValidationFolds):\n    # Dataset loading\n#    data_directory = \"Data-incremental-45-22-1.6\"\n    data_directory = \"Data-simple-45-22\"\n#    data_directory = \"Data-random-45-22\"\n#    data_directory = \"temp3\"\n#    data_directory = \"Data-simple-75\"\n    super_trainData = np.load(data_directory + \"\/re-training_trainData-\" + str(foldNumber) + \".npy\")\n    super_trainLabel = np.load(data_directory + \"\/re-training_trainLabel-\" + str(foldNumber) + \".npy\")-1\n    testData = np.load(data_directory + \"\/re-training_testData-\" + str(foldNumber) + \".npy\")\n    testLabel = np.load(data_directory + \"\/re-training_testLabel-\" + str(foldNumber) + \".npy\")-1\n\n    #pdb.set_trace()\n\n    print(super_trainData.shape)\n    print(testData.shape)\n\n\n    #pdb.set_trace()\n\n    train_dataset = myDataset.super_Dataset(super_trainData,\n                                              super_trainLabel, testData, testLabel, train=True)\n\n    test_dataset = myDataset.super_Dataset(super_trainData,\n                                             super_trainLabel, testData, testLabel, train=False)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True)\n\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False)\n    print(\"Dataset loaded!\")\n\n\n    encoder = CNNEncoder(hidden_dim, statisticalFeatures)\n#     encoder = Model.CNNEncoder(hidden_dim, statisticalFeatures)\n\n    #encoder = Model.FCNN(input_size, 10, time_steps)\n    \n    # Load the pre-trained model here:\n    encoder = torch.load('simple-45-22.pth')\n#    encoder = torch.load('simple-45-10.pth')\n#    encoder = torch.load('simple-75.pth')\n#    encoder = torch.load('incremental-45-22-1.6.pth')\n#    encoder = torch.load('random-45-22.pth')\n#    encoder = torch.load('temp.pth')\n#    encoder = torch.load('wit-incremental-CS5-4-8-1.2.pth')\n#    encoder = torch.load('wit-simple-CS5-3-10.pth')\n#    encoder = torch.load('wit-random-CS5-3-0.15.pth')\n\n    # For transfer learning, the convolutional layers can be fixed\n    # layers freeze\n    ct = 0\n    for child in encoder.children():\n        #ct<-1: re-train all layers\n        #ct<1: fix only cnnseq part, not regressor\n        #ct<2: fix cnnseq and regressor\n        if ct <-1:\n            for param in child.parameters():\n                param.requires_grad = False\n        ct += 1\n\n    #\n    encoder.reggresor = nn.Sequential(\n        nn.Linear(128, 256, bias=False),\n        nn.BatchNorm1d(256),\n        nn.ReLU(inplace=True),\n        nn.Linear(256, 128)\n    )\n    encoder.output_size = 128\n    \n#    linear = nn.Sequential(nn.Linear(128, 64), nn.ReLU(inplace=True), nn.Linear(64, num_classes) )\n    linear = nn.Sequential(nn.Linear(128, 128), nn.ReLU(inplace=True), nn.Linear(128, 128), nn.ReLU(inplace=True),   nn.Linear(128, num_classes), nn.Softmax())\n    transferedModel = nn.Sequential(encoder, linear)\n\n\n\n    if use_gpu:\n        transferedModel = transferedModel.cuda()\n\n    # loss and optimizer\n    loss_function = torch.nn.CrossEntropyLoss()\n\n    #When freezing layer, this only works\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,transferedModel.parameters()), lr=learning_rate,eps=epsilon, weight_decay=weight_decay)\n    #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Decay Learning Rate by a factor of 0.1 every 7 epochs\n    #exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n\n    train_acc = []\n    test_acc = []\n\n    isFirst = 1\n\n    for epoch in tqdm.tqdm(range(num_epoches)):\n        print()\n        running_loss = 0.0\n        running_acc = 0.0\n        avg_loss = 0.0\n        avg_acc = 0.0\n        total_target = 0\n\n        #exp_lr_scheduler.step()\n\n        for i, data in enumerate(train_loader, 1):\n            seq, target = data\n            if len(seq) < 2:\n                continue\n            seq = Variable(seq).float()\n            target = Variable(target).long()\n\n            #pdb.set_trace()\n\n            if use_gpu:\n                seq = seq.cuda()\n                target = target.cuda()\n\n\n            if isFirst == 1:\n                sample = seq[:6]\n                sampleTarget = target[:6]\n                ifFirst = 0\n\n            out = transferedModel(seq)\n            loss = loss_function(out, target)\n\n            # prediction\n            _, pred = torch.max(out, 1)\n            total_target += len(pred)\n            num_correct = (pred == target).sum()\n\n            running_acc += num_correct.item()\n            running_loss += loss.item() * target.size(0)\n\n            # Backprop\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\n        #print(sample)\n        result = transferedModel(sample)\n        t1 = sampleTarget.data[:].cpu().numpy()\n        t2 = result.data[:].cpu().numpy()\n        print(np.around(t1,4))\n        print(np.around(t2,4))\n\n\n        if epoch % display_step == 0 or epoch==num_epoches-1:\n            print(i)\n            avg_loss = running_loss \/ (total_target)\n            avg_acc = running_acc \/ (total_target)\n    \n            print('Loss: {:.6f}, Acc: {:.6f}'.format(\n                        avg_loss,\n                        avg_acc))\n    \n            train_acc.append(avg_acc)\n    \n            # test\n            transferedModel.eval()\n            eval_loss = 0.\n            eval_acc = 0.\n            for i, data in enumerate(test_loader, 1):\n                seq, target = data\n                seq = Variable(seq).float()\n                target = Variable(target).long()\n    \n                if use_gpu:\n                    seq = seq.cuda()\n                    target = target.cuda()\n    \n                out = transferedModel(seq)\n                loss = loss_function(out, target)\n    \n                # prediction\n                eval_loss += loss.item() * target.size(0)\n                _, pred = torch.max(out, 1)\n                num_correct = (pred == target).sum()\n                eval_acc += num_correct.item()\n    \n            avg_acc = eval_acc \/ (len(test_dataset))\n            test_acc.append(avg_acc)\n            print('Test Loss{:.1f}: {:.6f}, Acc: {:.6f}'.format(foldNumber, eval_loss \/ (len(\n                    test_dataset)), avg_acc))\n\n        # Save the Model after each epoch\n#        torch.save(transferedModel.state_dict(), '.\/supervised.pth')\n\n\n    print(\"Optimization Done!\")\n    t = np.arange(len(train_acc))\n\n    np.set_printoptions(precision=3)\n    TP = np.zeros(num_classes)\n    TN = np.zeros(num_classes)\n    FP = np.zeros(num_classes)\n    FN = np.zeros(num_classes)\n    \n    transferedModel.eval()\n    eval_loss = 0\n    eval_acc = 0\n    for i, data in enumerate(test_loader, 1):\n        seq, target = data\n        seq = Variable(seq).float()\n        target = Variable(target).long()\n\n        if use_gpu:\n            seq = seq.cuda()\n            target = target.cuda()\n\n        out = transferedModel(seq)\n        loss = loss_function(out, target)\n\n        # prediction\n        eval_loss += loss.item() * target.size(0)\n        _, pred = torch.max(out, 1)\n        num_correct = (pred == target).sum()\n        eval_acc += num_correct.item()\n        \n        for i in range(len(pred)):\n            if target[i] == pred[i]:\n                TP[target[i]] += 1\n                for j in range(num_classes):\n                    if j != target[i]:\n                        TN[j] += 1\n            elif target[i] != pred[i]:\n                FP[pred[i]] += 1\n                FN[target[i]] += 1\n    avg_acc = eval_acc \/ (len(test_dataset))\n\n    Accuracy = np.true_divide(TP+TN, TP+TN+FP+FN)\n    precision = np.true_divide(TP, TP+FP)\n    recall = np.true_divide(TP, TP+FN)\n    F1 = 2 * np.true_divide(np.multiply(precision,recall), precision+recall)\n    print(\"Fold \", foldNumber, \":\")\n    print(TP,TN)\n    print(FP,FN)\n    print(\"Accuracy: \", Accuracy)\n    print(\"precision: \", precision)\n    print(\"recall: \", recall)\n    print(\"F1: \", F1)\n    full_acc.extend(Accuracy)\n    full_pre.extend(precision)\n    full_rec.extend(recall)\n    full_f1.extend(F1)\n    total_acc += Accuracy\n    total_pre += precision\n    total_rec += recall\n    total_f1 += F1\n    total_acc_single += avg_acc\n    Accuracies[foldNumber] = avg_acc\n\nprint(\"Evaluation is done!\")\nprint(\"Accuracy: \", full_acc)\nprint(\"precision: \", full_pre)\nprint(\"recall: \", full_rec)\nprint(\"F1: \", full_f1)\nprint(\"--------------\")\nprint(\"Accuracy: \", 100*total_acc_single\/NumOfCrossValidationFolds)\nprint(Accuracies)\nprint(\"Accuracy: \", total_acc\/NumOfCrossValidationFolds)\nprint(\"precision: \", total_pre\/NumOfCrossValidationFolds)\nprint(\"recall: \", total_rec\/NumOfCrossValidationFolds)\nprint(\"F1: \", total_f1\/NumOfCrossValidationFolds)\n\n","7d9ab2c9":"## Pre-Training","8c261b06":"## Model","02cd07de":"## Re-training"}}