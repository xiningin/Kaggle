{"cell_type":{"1ee9f9f9":"code","51f7354f":"code","ff64711b":"code","306bf8ea":"code","91fcd9bf":"code","db2361bc":"code","e47f20c0":"code","48e2b78a":"code","47658856":"code","3825f215":"code","15a8a871":"code","21a129fb":"code","5c5941ee":"code","509738ea":"code","b30c241a":"code","4f03a35c":"code","4f8374a4":"code","7a3e27a0":"code","f06091cb":"code","a465439f":"code","bfbb614c":"code","b11793f0":"code","70c57fb5":"code","8c03f061":"code","a23922dc":"code","8218ac2a":"code","675c8c6a":"code","9c4641e2":"code","77d81237":"code","200344da":"code","dee36575":"code","c3132588":"code","170e4916":"code","ec73d40c":"code","458f13dd":"code","f8d3854d":"code","b5941a64":"code","d6db9de5":"code","44303d33":"code","ce269158":"code","4f309a64":"code","66a63708":"code","457fef16":"code","cba221b7":"code","e768ce7b":"code","2e8c25eb":"code","47d24227":"code","09dafb70":"code","1d85dde2":"code","2ad613a2":"code","009969b8":"code","30bd699a":"code","4d0e0783":"code","d3b4c9cb":"code","f90a249c":"code","146f5af3":"code","f8cade19":"code","f10f5fef":"code","926d6f05":"code","61883b77":"code","df96bd07":"code","0209d155":"code","65a60bc7":"markdown","ce82f020":"markdown","5c462336":"markdown","d73cb2eb":"markdown","0540bcbe":"markdown","12949f7c":"markdown","465e243d":"markdown","42915098":"markdown","5edb6837":"markdown","66a9d4e1":"markdown","e9c32849":"markdown","dfb099f4":"markdown","f008ebd0":"markdown","17f2ac2c":"markdown","bae3fe7a":"markdown","4fab3153":"markdown","ce7d3c77":"markdown","727c13ef":"markdown","73002d50":"markdown","c3aceaee":"markdown","a99d4e99":"markdown"},"source":{"1ee9f9f9":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport optuna\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\n\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","51f7354f":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest  = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')","ff64711b":"train.shape","306bf8ea":"train.describe()","91fcd9bf":"# Checking features and target columns\ndisplay(train.columns)\n# Checking dtypes\ndisplay(train.info())","db2361bc":"features = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']","e47f20c0":"train_01 = train.sample(frac=0.05, replace=False, random_state=1)","48e2b78a":"X = train_01[features]\ny = train_01['target']","47658856":"def learning_curves(estimator, title, X, y, cv= None, train_sizes=np.linspace(.3, 1.0, 5)):\n    \n    train_sizes, train_scores, validation_scores = \\\n        learning_curve(estimator, \n                       X,\n                       y,\n                       train_sizes = train_sizes,\n                       cv = cv, \n                       scoring = 'neg_mean_squared_error')\n\n    train_scores_mean = np.sqrt(-np.mean(train_scores, axis=1))\n    train_scores_std = np.sqrt(np.std(train_scores, axis=1))\n    validation_scores_mean = np.sqrt(-np.mean(validation_scores, axis=1))\n    validation_scores_std = np.sqrt(np.std(validation_scores, axis=1))\n    \n    plt.rcParams[\"font.size\"] = 12\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label = 'Training error')\n    plt.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\",label = 'Validation error')\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, validation_scores_mean - validation_scores_std,\n                     validation_scores_mean + validation_scores_std, alpha=0.1,\n                     color=\"g\")\n    \n    plt.rcParams[\"font.size\"] = 10\n    plt.ylabel('RMSE', fontsize = 14)\n    plt.xlabel('Training set size', fontsize = 14)\n    title = title\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.legend()\n    plt.ylim(0.5,0.8)","3825f215":"params_xgb = {'lambda': 1,\n 'alpha': 0,\n 'colsample_bytree': 1,\n 'subsample': 1,\n 'learning_rate': 0.05,\n 'max_depth': 6,\n 'min_child_weight': 3,\n 'random_state': 48}","15a8a871":"model_xgb = xgb.XGBRegressor(**params_xgb)","21a129fb":"title = 'Learning curve'\nlearning_curves(model_xgb, title, X, y, cv=5)","5c5941ee":"def validation_curves(estimator, title, X, y,\n                      cv= None, param_name= None, param_range=None):\n    \n    train_scores, test_scores = \\\n        validation_curve(estimator, \n                         X, \n                         y, \n                         param_name=param_name, \n                         param_range=param_range,\n                         cv = cv,\n                         scoring='neg_mean_squared_error', #'roc_auc'\n                         n_jobs=4)\n    train_scores_mean = np.sqrt(-np.mean(train_scores, axis=1))\n    train_scores_std = np.sqrt(np.std(train_scores, axis=1))\n    test_scores_mean = np.sqrt(-np.mean(test_scores, axis=1))\n    test_scores_std = np.sqrt(np.std(test_scores, axis=1))\n\n    plt.rcParams[\"font.size\"] = 12\n    plt.title(title, fontsize = 20)\n    plt.xlabel(param_name, fontsize =14)\n    plt.ylabel(\"Score\", fontsize = 14)\n    plt.ylim(0.5, 0.9)\n    lw = 2\n    plt.plot(param_range, train_scores_mean, label=\"Training score\",\n             color=\"darkorange\", lw=lw)\n    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.2,\n                     color=\"darkorange\", lw=lw)\n    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=lw)\n    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.2,\n                     color=\"navy\", lw=lw)\n    plt.rcParams[\"font.size\"] = 10\n    plt.legend(loc=\"best\")\n    plt.show()","509738ea":"param_range = np.linspace(0, 1, 10)\nparam_range","b30c241a":"param_name = \"alpha\"","4f03a35c":"title = \"Validation Curves for alpha\"\nvalidation_curves(model_xgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","4f8374a4":"param_name = \"lambda\"","7a3e27a0":"title = \"Validation Curves for lambda\"\nvalidation_curves(model_xgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","f06091cb":"param_range = np.linspace(0.1, 1, 10)\nparam_range","a465439f":"param_name = 'colsample_bytree'","bfbb614c":"title = \"Validation Curves for colsample\"\nvalidation_curves(model_xgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","b11793f0":"param_name = 'subsample'","70c57fb5":"title = \"Validation Curves for subsample\"\nvalidation_curves(model_xgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","8c03f061":"param_name = 'n_estimators'","a23922dc":"param_range = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]","8218ac2a":"title = \"Validation Curve for n_estimators\"\nvalidation_curves(model_xgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","675c8c6a":"X = train[features]\ny = train['target']","9c4641e2":"def objective(trial,data=X,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 1),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 1),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.1, 0.2, 0.3,0.5,0.7,0.9]),\n        'subsample': trial.suggest_categorical('subsample', [0.1, 0.2,0.3,0.4,0.5,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.0008, 0.01, 0.015, 0.02,0.03, 0.05,0.08,0.1]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20,23,25]),\n        'random_state': 48,\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 400),\n    }\n    model = xgb.XGBRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","77d81237":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","200344da":"study.best_trial.params","dee36575":"Best_params_xgb = {'lambda': 0.0014311714230223992,\n 'alpha': 0.008850567457271379,\n 'colsample_bytree': 0.3,\n 'subsample': 1.0,\n 'learning_rate': 0.01,\n 'max_depth': 20,\n 'min_child_weight': 245,\n 'n_estimators': 4000,\n 'random_state': 48,\n 'tree_method':'gpu_hist'}","c3132588":"train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.15,random_state=42)\nmodel_xgb = xgb.XGBRegressor(**Best_params_xgb)\nmodel_xgb.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)","170e4916":"importances = pd.Series(model_xgb.feature_importances_, index = features)\nimportances = importances.sort_values()\nimportances.plot(kind = \"barh\")\nplt.title(\"imporance in the xgboost Model\", fontsize=18)\nplt.show()","ec73d40c":"preds = model_xgb.predict(test_x)\nrmse = mean_squared_error(test_y, preds,squared=False)\nrmse","458f13dd":"test_X = test[features]","f8d3854d":"preds = model_xgb.predict(test_X)","b5941a64":"sub['target']=preds\nsub.to_csv('submission.csv', index=False)","d6db9de5":"X = train_01[features]\ny = train_01['target']","44303d33":"params_lgb = {'num_leaves': 31,\n 'min_data_in_leaf': 20,\n 'min_child_weight': 0.001,\n 'max_depth': -1,\n 'learning_rate': 0.005,\n 'bagging_fraction': 1,\n 'feature_fraction': 1,\n 'lambda_l1': 0,\n 'lambda_l2': 0,\n 'random_state': 48}","ce269158":"model_lgb = lgb.LGBMRegressor(**params_lgb)","4f309a64":"title = 'Learning curve'\nlearning_curves(model_lgb, title, X, y, cv=5)","66a63708":"param_range = np.linspace(0, 1, 10)\nparam_range","457fef16":"param_name = 'lambda_l1'","cba221b7":"title = \"Validation Curves for lambda_l1\"\nvalidation_curves(model_lgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","e768ce7b":"param_name = 'lambda_l2'","2e8c25eb":"title = \"Validation Curves for lambda_l2\"\nvalidation_curves(model_lgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","47d24227":"param_name = 'feature_fraction'","09dafb70":"title = \"Validation Curves for feature_fraction\"\nvalidation_curves(model_lgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","1d85dde2":"param_name = 'bagging_fraction'","2ad613a2":"title = \"Validation Curves for bagging_fraction\"\nvalidation_curves(model_lgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","009969b8":"param_name = 'n_estimators'\n","30bd699a":"param_range = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]","4d0e0783":"title = \"Validation Curves for n_estimators\"\nvalidation_curves(model_lgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","d3b4c9cb":"X = train[features]\ny = train['target']","f90a249c":"def objective_lgb(trial,data=X,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-3, 1),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-3, 1),\n        'feature_framcion': trial.suggest_categorical('feature_framcion', [0.1, 0.2, 0.3,0.5,0.7,0.9]),\n        'bagging_fraction': trial.suggest_categorical('bagging_framcion', [0.1, 0.2,0.3,0.4,0.5,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.0008, 0.01, 0.015, 0.02,0.03, 0.05,0.08,0.1]),\n        'n_estimators': 4000,\n        'num_leaves': trial.suggest_categorical('num_leaves', [31,50,150,200,250,300,350]),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20,23,25]),\n        'min_data_in_leaf': trial.suggest_categorical('min_data_in_leaf', [10,20,30]),\n        'min_child_weight': trial.suggest_categorical('min_child_weight', [0.001,0.005, 0.01, 0.05, 0.1,0.5]),\n        'random_state': 48\n    }\n    model = lgb.LGBMRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","146f5af3":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective_lgb, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","f8cade19":"study.best_trial.params","f10f5fef":"Best_params_lgb = {'lambda_l2': 0.013616569506899653,\n 'lambda_l1': 0.006495842188985166,\n 'feature_framcion': 0.3,\n 'bagging_framcion': 0.3,\n 'learning_rate': 0.015,\n 'num_leaves': 200,\n 'max_depth': 25,\n 'min_data_in_leaf': 30,\n 'min_child_weight': 0.001,\n 'n_estimators': 3000,\n 'random_state': 48,\n 'tree_method':'gpu_hist'}","926d6f05":"train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.15,random_state=42)\nmodel_lgb = lgb.LGBMRegressor(**Best_params_lgb)\nmodel_lgb.fit(train_x,train_y,eval_set=[(test_x,test_y)],\n              early_stopping_rounds=100,verbose=False)","61883b77":"importances = pd.Series(model_lgb.feature_importances_, index = features)\nimportances = importances.sort_values()\nimportances.plot(kind = \"barh\")\nplt.title(\"imporance in the lightGBM Model\", fontsize=18)\nplt.show()","df96bd07":"preds = model_lgb.predict(test_x)\nrmse = mean_squared_error(test_y, preds,squared=False)\nrmse","0209d155":"test_X = test[features]\npreds = model_lgb.predict(test_X)\nsub['target']=preds\nsub.to_csv('submission_lgb.csv', index=False)","65a60bc7":"# Learning and validation curves for GBDTs and parameter tuning using Optuna\n\nThere is not much information about the learning curve and validation curve in Kaggle, so I wrote about the learning\/validation curves.\nIf there are any mistakes, I would appreciate it if you could let me know.\n\nThe following is a great information on learning curves and validation curves.  \nhttps:\/\/scikit-learn.org\/stable\/modules\/learning_curve.html#validation-curve  \nhttps:\/\/www.dataquest.io\/blog\/learning-curves-machine-learning\/  \n\nI also described how to use Optuna, a useful package to tune the optimal hyperparameters of gradient boosting models such as XGBoost.  \n","ce82f020":"## Validation curve\nSimilar to the above, draw validation curves for lambda_l1 (=alpha), lambda_l2, feature_fraction, bagging_fraction, and n_estimators.","5c462336":"There seems to be no room for improvement on alpha.","d73cb2eb":"# Notice\nThis Notebook is more for creating learning curves and validation curves. Half of it is my own memorandum.  \nThe learning curve and the validation curve are not based on the whole data (300,000 data in train.csv) because it takes much time to calculate but on 5% random sampling (i.e., 15,000 data). ","0540bcbe":"The colsample_bytree also doesn't seem to change much in the Validation set.","12949f7c":"feature importance","465e243d":"feature importance","42915098":"The graph shows an upward trend for the Training set (red line), and it shows a slight downward trend for the Validation set(green line). This is a typical learning curve.  \nAs you can see, in the Training set, when the size is small, we can find parameters that fit a lot of data, so there are fewer errors. On the other hand, the parameters are tuned only for the Training set, so the Validation set has poor fits and large errors (left side of the figure).  \n\nAs the size increases, the training set's over-fitting is mitigated, resulting in a worse fit to the Training set and a better fit of the Validation set.   \n\nIf the two remain far apart even after increasing the size, we can say that there is still over-fitting.\nIn this figure, the two sets are gradually getting closer to each other, so we can expect a better fit if we increase the sample size. Since we only used 5% of the data, we can expect a better fit if we use all the data.\n\n","5edb6837":"From the learning curve results, the more the number of data increases, the more the over-fitting is mitigated and the better the prediction performance becomes. We will now use all 300,000 data in our tuning using Optuna. Let's submit the results and see the scores.  \nSince it takes a lot of time, we have set it to use the GPU.","66a9d4e1":"There doesn't seem to be a definite place that is better.","e9c32849":"## Learning curve\nThe learning curve plots the number of training samples on the horizontal axis and the score of an indicator (such as RSME) on the vertical axis. It shows how the indicator changes as the sample size changes. We can examine over-fitting and under-fitting by comparing the learning curves between the training set and the validation set.\nIt also allows us to examine whether or not it is worth adding more samples to the current data, which may provide useful suggestions for continued data collection.  \n\nThe following is a great information on learning curves and validation curves.  \nhttps:\/\/scikit-learn.org\/stable\/modules\/learning_curve.html#validation-curve  \nhttps:\/\/www.dataquest.io\/blog\/learning-curves-machine-learning\/","dfb099f4":"### Create a file for submission\n","f008ebd0":"## Learning curve","17f2ac2c":"# Import packages","bae3fe7a":"## Hyper Parameter Tuning using Optuna\nNow, I think it is essential to look at them one by one as described above, but I could not come to an obvious conclusion. Also, since there are so many parameters, I would like to decide them automatically to some extent.  \nIn this case, I would like to use a useful function called Optuna.\nThe following Notebook is a good reference.  \nhttps:\/\/www.kaggle.com\/hamzaghanmi\/xgboost-hyperparameter-tuning-using-optuna  \n","4fab3153":"## Hyper Parameter Tuning using Optuna (for LightGMB)","ce7d3c77":"There seems to be no room for improvement in lambda either.","727c13ef":"## Validation curve\nValidation curves are plotted on the horizontal axis with a parameter (e.g., alpha, the regularization parameter) and on the vertical axis with an indicator score (such as RSME). You can visually see how the training set and validation set behave when the parameters are changed. This can be used as a basis for determining the final parameters.  \n\n**Note**: Originally, all available data (300,000 here) should be used for the validation curve. The following results prioritize calculation speed, so the validation curve is created using data with 5% sampling data. Please treat them as reference values.  \n\nIn the case of a semi-automatic setup like Optuna (described below), there may be no need or motivation to see a learning curve or validation curve to check (that's why there are few descriptions in Kaggle, I thought).","73002d50":"# XGBoost and Learning\/Validation curves","c3aceaee":"## I hope this post has been helpful!\nIf you like it, please feel free to upvote!","a99d4e99":"# LightGBM and Learning\/Validation curves\nAs above, we first create the learning curve and the validation curve.  \nNext, we will perform parameter tuning using Optuna."}}