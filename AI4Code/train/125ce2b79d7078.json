{"cell_type":{"750d1b47":"code","d4927d04":"code","5212559a":"code","4f49e04a":"code","53cd77b0":"code","c74dae6c":"code","88a8c707":"code","1462f09a":"code","91e92152":"code","20429c07":"code","e41fe374":"code","ec85536a":"code","f8821d02":"code","975c45ba":"code","8058ecc7":"code","35d78373":"code","faa64f08":"code","504b4422":"code","d03fedff":"code","0dc384e0":"code","1ed7eb1a":"code","8e543c1e":"code","9505e54b":"markdown","152a636f":"markdown","ee9b7707":"markdown","f81720e7":"markdown","30484ac5":"markdown","e19943b3":"markdown","b79710e0":"markdown","b0f04d0f":"markdown","7afce809":"markdown","dd5ee3ed":"markdown","b93d84c6":"markdown","ba51b913":"markdown","e324b540":"markdown","f9e57026":"markdown","fff13069":"markdown","4b367bde":"markdown","582d000a":"markdown","faa30c7a":"markdown"},"source":{"750d1b47":"# Image-Preprocessing\nimport cv2\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\nimport numpy as np\nfrom collections import Counter\n\n# Plots\nimport matplotlib.pyplot as plt\n\n# Tensorflow Model\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.applications import MobileNet\n\n# Evaluation\nfrom sklearn.metrics import confusion_matrix, classification_report","d4927d04":"datagen = ImageDataGenerator(rescale=1.\/127.5,\n        shear_range=0.2,  # Shear tranformation \n        zoom_range=0, # Zoom level\n        brightness_range=[0.1, 1.4], # Light level of the image\n        fill_mode='nearest', # Filling of new pixels\n        vertical_flip=True,  # Flip vertical\n        horizontal_flip=True,# Flip horizontal \n        rotation_range=350, # Rotate image in degrees\n        width_shift_range=0, # shift horizontal\n        height_shift_range=0, # shift vertical\n        validation_split=0.15) # 15% split for validation data","5212559a":"def visualizeGenerator(Generator,ImageArray):\n  \n    # Generate augemented images\n    augmentedImages=[]  \n    for augmentedImage in Generator.flow(ImageArray,batch_size=1):\n        augmentedImages.append(augmentedImage)\n        if len(augmentedImages)==11: # Stop when we have got 11 images\n            break\n\n    # Figure size\n    fig = plt.figure(figsize=(10, 7))\n    \n    # Plot original image\n    ax = fig.add_subplot(3, 4, 1)        \n    ax.set_title(\"Source\")\n    ax.imshow(ImageArray[0].astype(np.uint8))\n    ax.set_axis_off()\n    \n    # Plot augmented images\n    i=1\n    for image in augmentedImages:\n        i+=1\n        ax = fig.add_subplot(3, 4, i)   \n        ax.imshow((image[0]*127.5).astype(np.uint8))\n        ax.set_axis_off()\n        \n    # Show figure\n    plt.show()","4f49e04a":"# Load and convert sample image\nexampleImage=\"..\/input\/yellow-and-read-autumn-color-grapeleafs\/data\/data\/yellow\/78307d32-6bad-11eb-820a-001583eb31c8.png\"\norginalImage = img_to_array(load_img(exampleImage))\norginalImage=orginalImage.reshape((1,)+orginalImage.shape)\n\n# Run function\nvisualizeGenerator(datagen,orginalImage)","53cd77b0":"def getData(path,imageShape,batchSize):\n    \"\"\"\n    Input: \n        path            String: Path to data folder \n        imageShape      List (int, int) Input shape for model\n        batchSize       Integer Batchsize\n    \n    Output:\n        TF Trainings Generator\n        TF Validation Generator\n    \"\"\"\n    \n    print(\"\\n ----- Start Reading ----- \\n\")\n    \n    \"\"\"\n    Data Augementation\n    \"\"\"\n    datagen = ImageDataGenerator(rescale=1.\/127.5,\n        shear_range=0.2,  # Shear tranformation \n        zoom_range=0, # Zoom level\n        brightness_range=[0.1, 1.4], # Light level of the image\n        fill_mode='nearest', # Filling of new pixels\n        vertical_flip=True,  # Flip vertical\n        horizontal_flip=True,# Flip horizontal \n        rotation_range=350, # Rotate image in degrees\n        width_shift_range=0, # shift horizontal\n        height_shift_range=0, # shift vertical\n        validation_split=0.15) # 15% split for validation data\n    \n    # Train Generator\n    train_generator = datagen.flow_from_directory(\n        path,\n        target_size=(imageShape[0],imageShape[1]),  # Image height and width\n        batch_size=batchSize,\n        class_mode='binary',  # Binary problem\n        subset='training',\n        seed=1,\n        shuffle=True,\n        ) \n    \n    # Validation Generator \n    validation_generator = datagen.flow_from_directory(\n        path, \n        target_size=(imageShape[0],imageShape[1]), # Image height and width\n        batch_size=batchSize,\n        shuffle=True,\n        seed=1,\n        class_mode='binary', # Binary problem\n        subset='validation')\n    \n    return train_generator,validation_generator","c74dae6c":"path=\"..\/input\/yellow-and-read-autumn-color-grapeleafs\/data\/data\/\"\nimageShape=(224,224) # Shape of MobileNet\nbatchSize=50\n\ntrain_generator,validation_generator=getData(path,imageShape,batchSize)","88a8c707":"classes=[\"yellow\",\"red\"] # Two data classes \n\noccurence=Counter(train_generator.classes) # Number of images for both classes\nimagesInClass=[occurence[0],occurence[1]]\n\n# Class indices\nprint(\"Class indices: \",train_generator.class_indices)\n\n# Simpel bar plot\nplt.title(\"Occurence of classes\")\nplt.xlabel(\"Number\")\nplt.ylabel(\"Classes\")\nplt.barh(classes,imagesInClass,color=[\"yellow\",\"red\"])","1462f09a":"def GrabCut(img):\n  # Internal used arrays\n  bgdModel = np.zeros((1,65),np.float64)\n  fgdModel = np.zeros((1,65),np.float64)\n  \n  # Create emtpy mask\n  mask = np.zeros(img.shape[:2],np.uint8)\n\n  # We do not know where the foreground is, so the whole image can be foreground\n  foreground = (0,0,223,223) \n\n  # Run Background  \n  cv2.grabCut(img,mask,foreground,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT) # Run Grabcut\n  \n  # Create segmented image \n  # Possible values:\n  # 0 Background (masked)\n  # 1 Foreground (nothing happens)\n  # 2 Possible Background (masked)\n  # 3 Possible Foreground (nothing happens) \n  mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n\n  # Use mask\n  img = img*mask2[:,:,np.newaxis]  \n  \n  return img    ","91e92152":"exampleRGB = cv2.cvtColor(cv2.imread(exampleImage), cv2.COLOR_BGR2RGB) # Load data as RGB\n\nimg=GrabCut(exampleRGB)\n\n# Plot\nplt.imshow(img)\nplt.show()","20429c07":"def reduceColorIntensity(img):\n    # Reference: https:\/\/docs.opencv.org\/master\/d1\/d5c\/tutorial_py_kmeans_opencv.html\n    \n    # Reshape Image\n    Z = np.float32(img.reshape((-1,3)))\n  \n    # Define criteria, number of clusters(K) and apply kmeans()\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n    K = 12\n    \n    ret,label,center=cv2.kmeans(Z,K,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)\n    \n    # Convert back into uint8 to reconstruct original image\n    center = np.uint8(center)\n    res = center[label.flatten()]\n    \n    # Convert to original size\n    res = res.reshape((img.shape))\n    return res","e41fe374":"reducedImg = reduceColorIntensity(exampleRGB)\n\n# Create figure\nfig = plt.figure(figsize=(10, 7))\n    \n# Plot original image\nax = fig.add_subplot(3, 4, 1)        \nax.set_title(\"Source\")\nax.imshow(exampleRGB)\nax.set_axis_off()\n\n# Plot reduced image\nax = fig.add_subplot(3, 4, 2)        \nax.set_title(\"Reduced image\")\nax.imshow(reducedImg)\nax.set_axis_off()\n\n# Plot reduced cut image\nax = fig.add_subplot(3, 4, 3)        \nax.set_title(\"Reduced and cut\")\nax.imshow(GrabCut(reducedImg))\nax.set_axis_off()","ec85536a":"def getModel(imageShape):\n    \n    num_classes=2 # Binary problem \n    \n    # Loading Pretrained model\n    base_model=MobileNet(weights='imagenet',include_top=False,classes = num_classes) \n    # Add some layers\n    x=base_model.output\n    x=GlobalAveragePooling2D()(x)\n    x=Dense(1024,activation='relu')(x) \n    x=Dense(1024,activation='relu')(x) \n    x=Dense(512,activation='relu')(x) \n    preds=Dense(num_classes-1,activation='sigmoid')(x) # Last Layer\n    \n    model=Model(inputs=base_model.input,outputs=preds)\n    \n    # Compile \n    model.compile(optimizer='Adagrad',loss=tf.keras.losses.binary_crossentropy,metrics=['accuracy'])\n    return model","f8821d02":"model=getModel(imageShape) # Internet must be on\n# Count layers\nprint(\"Number of layers: \",len(model.layers))\n\n# Freeze layers and reduce trainable parameters because we have a limited dataset \nfor layer in model.layers[:12]:\n    layer.trainable=False\nfor layer in model.layers[12:]:\n    layer.trainable=True","975c45ba":"model.summary()","8058ecc7":"# Fit model over some epochs\nhistory=model.fit(\n        train_generator,\n        steps_per_epoch = train_generator.samples \/\/ batchSize,\n        validation_data = validation_generator, \n        validation_steps = validation_generator.samples \/\/ batchSize,\n        epochs = 10,workers=-1)","35d78373":"def visualization(history):\n    # Plot graphs to evaluate training process\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n    \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()","faa64f08":"visualization(history)","504b4422":"testPath=\"..\/input\/yellow-and-read-autumn-color-grapeleafs\/test\/test\/\"\ntest_datagen = ImageDataGenerator(rescale=1.\/127.5)\ntest_generator =test_datagen.flow_from_directory(\n     testPath,\n     target_size=(imageShape[0],imageShape[1]),\n     batch_size=1,\n     class_mode='binary',\n     shuffle=False)","d03fedff":"test_loss, test_acc = model.evaluate(test_generator) \nprint('Test accuracy:', test_acc,'Test loss:', test_loss) \n","0dc384e0":"# Get right labels from generator\ntrueLabels=test_generator.classes\n\n# Generate predictions on testing data\npredictions=model.predict(test_generator, steps=len(test_generator))\n#Sigmoid returns proability, so we round the result\npredicted_Class=np.round(predictions,0) ","1ed7eb1a":"# Calculate those metrics\nprint(classification_report(trueLabels,predicted_Class,digits=3))","8e543c1e":"cm=confusion_matrix(trueLabels,predicted_Class)\n\nprint(cm)\n\nplt.figure(figsize=(10,5),facecolor='white')\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.winter) \nclassNames = ['Yellow','Red']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\nplt.show()","9505e54b":"The result is unsatisfying because the leaf was destroyed. Some references claim that the algorithm works better when we reduce the colorspace.","152a636f":"Let's generate some images to visualize the augmentation.","ee9b7707":"We want to calculate some metrics on test data to validate our progress in the training. We use 50 images per class as test data. Notice that we are using the same normalization as in our training process. However, we must load the test data first.","f81720e7":"# Load and explore data\n\nWe took 1200 images to train our machine learning model. First of all, we must explore the dataset and change some features and characteristics so that our model can work with them. \n","30484ac5":"# Setup - Needed libaries","e19943b3":"Furthermore, it is nice to know which diseases our model classifies wrongly. To learn more about this we can plot the confusion matrix.","b79710e0":"## Support\n\nCheck out our social media:\n- Contact: <a href=\"mailto:falldetectionappai@gmail.com\">falldetectionappai@gmail.com<\/a>\n- Website at <a href=\"https:\/\/matheli.github.io\/Vine-leaf-diseases-and-AI\/\" target=\"_blank\">`matheli.github.io\/Vine-leaf-diseases-and-AI\/`<\/a>\n- Youtube at <a href=\"https:\/\/www.youtube.com\/channel\/UCsGZt4UtInZ01tBjM1B-FbQ?view_as=subscriber\" target=\"_blank\">`INFOrmAtIc Teens`<\/a>\n\n\n---","b0f04d0f":"Now we can calculate loss and accuracy with the test data.","7afce809":"In our example we can see that there are irrelevant parts in the picture like the background. Maybe we could remove them with a Grabcut algorithm to reduce the complexity. ","dd5ee3ed":"You can now load the data from your disk and create an image genrator.","b93d84c6":"## Pre-process Data","ba51b913":"By using scikit-learn we can easily calculate more metrics like precision, recall and F1-Score.","e324b540":"Let's take a closer look at our data. Are the classes balanced?","f9e57026":"Although a larger part of the background were removed, some parts of the leaf are now missing too. So we must use the image with some of the background.","fff13069":"# Train model\nBecause we have a lack of image data and want to use the model on our mobile, we load MobileNet with weights.","4b367bde":"## Augmentation\nWe will use the Tensorflow ImageDataGenerator to load and process our images. In addition, we use image augmentation to generate varied training data in order to avoid overfitting and create a universal model.","582d000a":"\n<img src=\"https:\/\/raw.githubusercontent.com\/MareSeestern\/VineLeafDisease\/master\/res\/titelbild.JPG\" width=\"900\" height=\"500\">\n\n# Introduction \n\nWe have programmed a mobile app which can classify grape leaf diseases with machine learning. With this technique, we want to reduce the use of pesticides in vineyards because the disease will be recognized at an early stage and the right treatment can be started. Moreover, we want to increase crop yield and reduce crop cultivation. \n\nIn this notebook, we will demonstrate how to train an image model with Tensorflow to classify yellow and red autumn colours. To do this, we created a database with 1200 images with our mobiles. This dataset is public on Kaggle: \nhttps:\/\/www.kaggle.com\/informaticteens\/yellow-and-read-autumn-color-grapeleafs\n\nThis notebook is an end-to-end example.","faa30c7a":"# Evaluate model\nFirst of all, we can plot our loss and accuracy functions:\n"}}