{"cell_type":{"45cff933":"code","210d203d":"code","b0e2a949":"code","4a090294":"code","b718bfba":"code","f4a73079":"code","b33aaf04":"code","78f1046c":"code","0727d157":"code","294b8376":"code","1be0d8d4":"code","2d61a051":"code","2918e633":"code","a09a00f9":"code","682a297f":"code","3ccf2128":"code","719e52b6":"code","e9b63237":"code","56dfe13a":"code","a01e7863":"code","e22b4257":"code","fadeffc7":"code","088bcbab":"code","d35f129a":"code","2e467b83":"code","e7eaf1a8":"code","9c947ee0":"code","d0032efb":"code","0be3e527":"code","4bb8df3d":"code","e5b389f2":"code","e6502c8b":"code","69e373eb":"code","71057636":"code","b00cb983":"code","757b8fc1":"code","c592645f":"code","0f7f3528":"code","b88d2e55":"code","9f641398":"code","99cbaee7":"code","cd62a41e":"code","f82c576d":"code","14a8f6c0":"code","c45b7f3a":"markdown","d335c817":"markdown","c22e6f13":"markdown","2d2bdbc6":"markdown","17a0e4d9":"markdown","2a4dad21":"markdown","9894dfe0":"markdown","4abc358c":"markdown","ce4dbc11":"markdown","6df505e2":"markdown","6d8267aa":"markdown","ba8addea":"markdown","6cb4619d":"markdown","9b4e8624":"markdown","86a68b3b":"markdown","30e06330":"markdown","3915b70b":"markdown","b72af310":"markdown","61ca0e23":"markdown","957d0818":"markdown","6c589da8":"markdown","e5fc02e3":"markdown","05598721":"markdown","bc27e469":"markdown","6c8f5433":"markdown","fd73b5a7":"markdown","a8271567":"markdown","9dfc5cd5":"markdown","360419a6":"markdown","16ee1c5d":"markdown","e8318704":"markdown","2bc91a77":"markdown","0b91bf4a":"markdown","0f99b9e9":"markdown","bafb620d":"markdown","01600df3":"markdown","ad877b7e":"markdown","119d5369":"markdown","3aa52fd0":"markdown","4559685f":"markdown","1c605d3f":"markdown","7b480a44":"markdown","88175adb":"markdown","5f8533b9":"markdown","e493760e":"markdown","c1551896":"markdown","91414a7b":"markdown","cfba67e3":"markdown","1ae7cf37":"markdown","e443a295":"markdown","ea96776f":"markdown"},"source":{"45cff933":"# For data handling\nimport numpy as np\nimport pandas as pd\n\n# For visvalization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport warnings\nwarnings.filterwarnings('ignore')","210d203d":"# Reading the files\ndf = pd.read_csv('..\/input\/titanic\/train.csv')\ndf.head()","b0e2a949":"# Number of rows and columns\ndf.shape","4a090294":"# Dropping the unnecessary columns as they won't contribute for training ML model\ndf = df.drop(['PassengerId', 'Ticket', 'Name'], axis = 1)","b718bfba":"# Checkign data types\ndf.info()","f4a73079":"# Changing the data types\ncat_col = ['Pclass']\ndf[cat_col] = df[cat_col].astype('object')","b33aaf04":"# Missing values\ndf.isnull().mean()*100","78f1046c":"# Removing Cabin column\ndf=df.drop(['Cabin'], axis=1)\n\n# Imputing missing values\ndf['Age'] = df['Age'].fillna(df['Age'].median())\n\ndf['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])","0727d157":"# Function for understanding the distribution of the columns\ndef quick_plot(df):\n\n    categorical_col = df.select_dtypes('object').columns.to_list()\n    \n    numeric_col = df.select_dtypes(['int', 'float']).columns.to_list()\n\n    if len(numeric_col)> len(categorical_col):\n        max_length = len(numeric_col)\n    else:\n        max_length = len(categorical_col)\n\n    fig, (ax1, ax2) = plt.subplots(2, max_length, figsize = (20, 10))\n\n    # Plotting for categorical columns\n    for axis, col in zip(ax1.ravel(), categorical_col):\n        axis = sns.countplot(x = col, data = df, ax = axis)\n        axis.set_ylabel(None)\n        axis.set_xlabel(col, size = 16)\n        \n        for i in axis.patches:    \n            axis.text(x = i.get_x() + i.get_width()\/2, y = i.get_height()\/2,\n                    s = f\"{round(i.get_height(),1)}\", \n                    ha = 'center', size = 16, weight = 'bold', rotation = 0, color = 'white')\n\n    # Plotting for numeric columns\n    for axis, col in zip(ax2.ravel(), numeric_col):\n        axis = sns.histplot(x = col, data = df, multiple = 'stack', ax = axis)\n        axis.set_ylabel(None)\n        axis.set_xlabel(col, size = 16)\n    \n        \n    fig.text(0.5, 1, 'The distribution of Data', ha = 'center', fontsize = 20);\n    plt.tight_layout()","294b8376":"# Quickly understanding the distribution of the columns\nplt.style.use('seaborn')\nquick_plot(df)","1be0d8d4":"plt.style.use('seaborn')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 6))\n\nax1 = sns.boxplot(y = 'Age', data = df, ax = ax1)\nax2 = sns.boxplot(y = 'Fare', data = df, ax = ax2)\n\nax1.set_ylabel(None)\nax1.set_xlabel(None)\nax1.set_title('Age', size = 16)\n\nax2.set_ylabel(None)\nax2.set_xlabel(None)\nax2.set_title('Fare', size = 16);","2d61a051":"fig, ax = plt.subplots(1,1, figsize = (10, 6), constrained_layout = True)\nax = sns.regplot(x = 'Age', y = 'Fare', data = df)\n\nax.set_ylabel('Fare', fontsize = 14)\nax.set_xlabel('Age', fontsize = 14)\nplt.title('Age Vs Fare', fontsize = 16)\n\ncorrelation = np.corrcoef(df['Age'], df['Fare'])[0][1]\n\nax.text(x = 60, y = 180,\n        s = f\"correlation : {round(correlation,3)}\",\n        ha = 'center', size = 12, rotation = 0, color = 'black',\n        bbox=dict(boxstyle=\"round,pad=0.5\", fc='skyblue', ec=\"skyblue\", lw=2));","2918e633":"fig, (ax1,ax2) = plt.subplots(1,2, figsize = (15, 6), constrained_layout = True)\nax1 = sns.boxenplot(x = 'Survived', y = 'Age', data = df, ax = ax1)\nax2 = sns.histplot(x = 'Age', data = df, hue = 'Survived', multiple = 'stack', kde = True, ax = ax2)\n\nax1.set_xlabel(None)\nax1.set_ylabel('Age', fontsize = 14)\nax1.set_title('Passenger Age and their Survival', fontsize = 16)\nax1.set_xticklabels(['Not Survived', 'Survived'], size = 14)\n\nax2.set_xlabel('Age', fontsize = 14)\nax2.set_ylabel('Number of Passengers', fontsize = 14)\nax2.set_title('Distribution of Age and Passenger Survival', fontsize = 16);","a09a00f9":"fig, (ax1,ax2) = plt.subplots(1,2, figsize = (15, 6), constrained_layout = True)\nax1 = sns.boxenplot(x = 'Survived', y = 'Fare', data = df, ax = ax1)\nax2 = sns.histplot(x = 'Fare', data = df, hue = 'Survived', multiple = 'stack', kde = True, ax = ax2)\n\nax1.set_xlabel(None)\nax1.set_ylabel('Fare', fontsize = 14)\nax1.set_title('Passenger Age and their Survival', fontsize = 16)\nax1.set_xticklabels(['Not Survived', 'Survived'], size = 14)\n\nax2.set_xscale('log')\nax2.set_xlabel('Age (Log scale)', fontsize = 14)\nax2.set_ylabel('Number of Passengers', fontsize = 14)\nax2.set_title('Distribution of Fare and Passenger Survival', fontsize = 16);","682a297f":"df['Fare'] = df['Fare'].transform(np.log)\ndf['Fare'].min()","3ccf2128":"# As new Fare contains -infinity, I will replace it will next minimum fare\nleast_value = df['Fare'].sort_values().unique()[1]\n\ndf['Fare'] = df['Fare'].replace(-np.inf, least_value)","719e52b6":"# Creating bins for age\ndf['Age_Bins'] = pd.cut(df['Age'], [0, 10, 25, 40, 60,100], \n                        labels = ['<10', '10 to 25', '26 to 40', '41 to 60','60+'], include_lowest = True)","e9b63237":"# Creating X (features) and y (target)\nX = df.drop(['Survived'], axis = 1)\ny = df['Survived'].astype('int')\n\n# One hot encoding for categorical features\nX_new = pd.get_dummies(X, drop_first = True)","56dfe13a":"X_new.head()","a01e7863":"fig, ax = plt.subplots(1,1, figsize = (8, 6), constrained_layout = True)\nax = sns.countplot(x = 'Survived', data = df)\n\nax.set_ylabel('Number of Passengers', fontsize = 14)\nax.set_xlabel('')\nax.set_xticklabels(['Not Survived', 'Survived'], fontsize = 14)\nplt.title('Count of Passengers Survived', fontsize = 16)\n\nfor i in ax.patches:\n    ax.text(x = i.get_x() + i.get_width()\/2, y = i.get_height()\/4, \n            s = f\"{round(i.get_height()\/len(df)*100,1)}%\", \n            ha = 'center', size = 50, weight = 'bold', rotation = 90, color = 'white')","e22b4257":"from imblearn.over_sampling import SMOTE, ADASYN\nadasyn = ADASYN()\nprint('Before sampling', y.value_counts())\n\nX_resample, y_resample = adasyn.fit_resample(X_new, y)\n\nprint('After sampling', y_resample.value_counts())","fadeffc7":"# Train - Test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, train_size = 0.8,\n                                                    random_state = 99, stratify = y_resample)","088bcbab":"from sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()\n\nX_train[['Age','Fare']] = scalar.fit_transform(X_train[['Age','Fare']])\nX_test[['Age','Fare']] = scalar.transform(X_test[['Age','Fare']])","d35f129a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\n\nlog_model = LogisticRegression().fit(X_train, y_train)","2e467b83":"print('Train AUC-ROC score for Logistic Regression is', roc_auc_score(y_train, log_model.predict_proba(X_train)[:, 1]))\nprint('Test AUC-ROC score for Logistic Regression is', roc_auc_score(y_test, log_model.predict_proba(X_test)[:, 1]))","e7eaf1a8":"# Plotting the ROC Curve and finding optimal threshold\n# Function for plotting ROC curve, confurion matrix and finding optimal threshold\nfrom sklearn.metrics import roc_auc_score, plot_roc_curve, accuracy_score, recall_score, confusion_matrix\n\ndef my_roc_curve(model, accuracy_weight, title, X_train, y_train):\n    from sklearn import metrics\n    # Plotting AUC ROC curve\n    y_scores = model.predict(X_train)\n    fpr, tpr, thresholds = metrics.roc_curve(y_train, y_scores)\n    \n    fig, (ax1,ax2) = plt.subplots(1,2, figsize = (12, 6), constrained_layout = True)\n    ax1.plot(fpr, tpr, color='skyblue', label='ROC')\n    ax1.plot([0, 1], [0, 1], color='pink', linestyle='--')\n    ax1.text(x = 0.8, y = 0.3,\n            s = f\"AUC : {round(metrics.roc_auc_score(y_train, y_scores),2)}\",\n            ha = 'center', size = 12, rotation = 0, color = 'black',\n            bbox=dict(boxstyle=\"round,pad=0.5\", fc='skyblue', ec=\"skyblue\", lw=2));\n\n    ax1.set_xlabel('False Positive Rate', fontsize = 12)\n    ax1.set_ylabel('True Positive Rate', fontsize = 12)\n    ax1.set_title(f'ROC curve',  fontsize=16, y=1.05)\n\n    # Plotting optimal Confusion Matrix\n    from sklearn import metrics\n    probability = model.predict_proba(X_train)\n    matrix = pd.DataFrame()\n    \n    base_accuracy = metrics.accuracy_score(y_train, model.predict(X_train))\n    base_recall = metrics.recall_score(y_train, model.predict(X_train))\n    best_score = 0.6*base_accuracy + 0.4*base_recall\n    best_thresold = 0.5\n    \n    # Finding optimul thresold to maximize > accuracy_weight*threshold_accuracy + (1-accuracy_weight)*threshold_recall\n    for threshold in np.linspace(0, 1, 100):\n        y_predict = (probability>=threshold).astype(int)[:,1]\n        threshold_accuracy = metrics.accuracy_score(y_train, y_predict)\n        threshold_recall = metrics.recall_score(y_train, y_predict)\n        weighted_score = accuracy_weight*threshold_accuracy + (1-accuracy_weight)*threshold_recall\n        \n        if weighted_score>best_score:\n            best_thresold = threshold\n            best_score = weighted_score\n    \n    y_predict = (probability>=best_thresold).astype(int)[:,1]\n    matrix = metrics.confusion_matrix(y_train, y_predict)\n\n    ax2 = sns.heatmap(matrix, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                        linewidths=3, square=True, ax = ax2, annot_kws={\"fontsize\":20})\n    ax2.set_title(f\"Confusion Matrics | Threshold : {round(best_thresold, 2)}\", fontsize=16, y=1.05);\n    ax2.set_xlabel('Predicted', fontsize=12)\n    ax2.set_ylabel('Actual', fontsize=12)\n    ax2.set_xticklabels([0,1], fontsize=12 )\n    ax2.set_yticklabels([0,1], fontsize=12, rotation=0)\n    \n    print('Optimal Threshold for Accuracy and Recall is : ', round(best_thresold, 2))\n    print(f\"Train Accuracy for  {title}: \", round(metrics.accuracy_score(y_train, y_predict),2), \n          f\"| Train Recall for {title}: \", round(metrics.recall_score(y_train, y_predict),2))\n    \n    plt.suptitle(f'{title}', fontsize=19, y=1.05)\n    \n    # For test data\n    probability = model.predict_proba(X_test)\n    y_predict = (probability>=best_thresold).astype(int)[:,1]\n    print(f\"Test Accuracy for {title}: \", round(metrics.accuracy_score(y_test, y_predict),2),\n         f\"| Test Recall for {title}: \", round(metrics.recall_score(y_test, y_predict),2))","9c947ee0":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ntree = DecisionTreeClassifier()\nparameters = {'max_depth':[3,5,7], 'min_samples_leaf':[5,10,15]}\n\ntree_clf = GridSearchCV(tree, parameters, cv=5)\ntree_clf.fit(X_train, y_train)","d0032efb":"print('Train AUC-ROC score for Decision Tree is', roc_auc_score(y_train, tree_clf.predict_proba(X_train)[:, 1]))\nprint('Test AUC-ROC score for Decision Tree is', roc_auc_score(y_test, tree_clf.predict_proba(X_test)[:, 1]))","0be3e527":"from catboost import CatBoostClassifier\n\ncat_model = CatBoostClassifier(verbose = 100)","4bb8df3d":"# Creating validation dataset\nX_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train, train_size = 0.8,\n                                                  random_state = 99, stratify = y_train)","e5b389f2":"cat_model.fit(X_train_new, y_train_new, eval_set= (X_val, y_val), early_stopping_rounds = 50, use_best_model = True)","e6502c8b":"print('Train AUC-ROC score for CatBoost is', roc_auc_score(y_train, cat_model.predict_proba(X_train)[:, 1]))\nprint('Test AUC-ROC score for CatBoost is', roc_auc_score(y_test, cat_model.predict_proba(X_test)[:, 1]))","69e373eb":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline","71057636":"estimators = [\n        ('tree',  DecisionTreeClassifier(max_depth=9, min_samples_leaf=10, random_state=99)),\n        ('svr', Pipeline([('scalar', StandardScaler()), ('svm', SVC(random_state = 99))])),\n        ('gauss', GaussianNB())]","b00cb983":"staked_model = StackingClassifier(estimators, cv = 5, \n                                   final_estimator = cat_model)\nstaked_model.fit(X_train, y_train)","757b8fc1":"print('Train AUC-ROC score for CatBoost is', roc_auc_score(y_train, staked_model.predict_proba(X_train)[:, 1]))\nprint('Test AUC-ROC score for CatBoost is', roc_auc_score(y_test, staked_model.predict_proba(X_test)[:, 1]))","c592645f":"# Importing h2o library\nimport h2o\nfrom h2o.automl import H2OAutoML","0f7f3528":"# Combining X and y\ndf_train = pd.concat([X_train_new, y_train_new], axis =1)\ndf_test = pd.concat([X_test, y_test], axis =1)\ndf_val = pd.concat([X_val, y_val], axis =1)\n\n# Saving as csv files\ndf_train.to_csv('train.csv', index = False)\ndf_test.to_csv('test.csv', index = False)\ndf_val.to_csv('val.csv', index = False)","b88d2e55":"# Initializing h2o\nh2o.init()","9f641398":"# Reding the data using h2o frames\ntrain = h2o.import_file('.\/train.csv')\ntest = h2o.import_file('.\/test.csv')\nval = h2o.import_file('.\/val.csv')","99cbaee7":"# Identifing predictors and response\nx = train.columns\ny = \"Survived\"\nx.remove(y)\n\n# For binary classification, response should be a factor\ntrain[y] = train[y].asfactor()\ntest[y] = test[y].asfactor()\nval[y] = val[y].asfactor()","cd62a41e":"# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml = H2OAutoML(max_models=10, seed=1)\naml.train(x=x, y=y, training_frame=train, validation_frame = val)","f82c576d":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=10)  # Print all rows instead of default (10 rows)","14a8f6c0":"perf = aml.leader.model_performance(test)\nperf","c45b7f3a":"<h3><center>  <div style=\"background-color:pink;border-radius:10px; padding: 10px;\">If you like, don't forget to upvote!<\/div><\/center><\/h3>","d335c817":"<center><img src = https:\/\/i.imgur.com\/HGjag5k.jpg><\/center>","c22e6f13":"<h3><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Feature Enginner \u2699<\/div><\/center><\/h3>","2d2bdbc6":"<h3><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Analysis of Numeric Features \ud83d\udd2c<\/div><\/center><\/h3>","17a0e4d9":"<h2><center>  <div style=\"background-color:pink;border-radius:10px; padding: 10px;\">AutoML \ud83e\udd16<\/div><\/center><\/h2>\n\n<center> <img src= \"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTi48tkmw7TT50iGq2uJRuvRxg6Cz_aUdkkIpGZ5bVWpsugEobka8wWUFdfhqKKKupgR1Q&usqp=CAU\" ><\/center>\n\n> H2O is an open source library, it quite facinating and easy to use and I will be using this library for AutoML. So, let's get started !! \ud83d\ude97","2a4dad21":"<h4><center>  <div style=\"background-color:orange;border-radius:10px; padding: 10px;\">CatBoost \ud83d\udc31\u200d\ud83d\udc53<\/div><\/center><\/h4>","9894dfe0":"<center><img src = https:\/\/i.imgur.com\/1l8cw6b.jpg><\/center>","4abc358c":"<h1><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Creating Machine Learning Models <\/div><\/center><\/h1>\n\n> First we will create simple model for baseline and then move to more complex models to improve further\n\n> For evaluation purpose I am using AUC ROC score which is mostly used for classification problems\n\n**AUC ROC Curve \ud83d\udcc8**\n\n> Area Under Curve (AUC) Receiver Operating Characteristic (ROC) Curve The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.","ce4dbc11":"> We got similar numbers of 0's and 1's.","6df505e2":"<h4><center>  <div style=\"background-color:orange;border-radius:10px; padding: 10px;\">Decision Tree \ud83c\udf33<\/div><\/center><\/h4>","6d8267aa":"<h4><center>  <div style=\"background-color:orange;border-radius:10px; padding: 10px;\">Logistic Regression<\/div><\/center><\/h4>","ba8addea":"#### Loading the libraries","6cb4619d":"#### \ud83d\udd0e Observations\n> Out of all female 74% survived and 45% of these embarked from Southampton (S).\n\n> 47% of 2nd class passengers survived and 41% of them embarked from Southampton (S).\n\n> Out of all male 19% survived and 13% of these embarked from Southampton (S). So, only 6% of the male who embarked from Cherbourg (C) and Queenstown (Q), survived. ","9b4e8624":"#### \ud83d\udd0e Observations\n> As seen from previous plot also, Passengers of class 3 are huge in number. The reason can be cheap cost of the ticket.\n\n> 76% of the passengers form the 3rd class didn't survive while 63% of passengers form the 1st class survived.\n\n> The 1st class had mostly elders, may be because they can afford the ticket price and 3rd class had mostly young people.\n\n> Most of the rich \ud83e\udd11 and old \ud83d\udc74\ud83d\udc75 passengers survived.","86a68b3b":"> Let's try AutoML methods, to see if we can increase this further","30e06330":"<h4><center>  <div style=\"background-color:pink;border-radius:10px; padding: 10px;\">Port of Embarkment \ud83d\udea2<\/div><\/center><\/h4>\n\n> Passengers embarked from three different ports : C = Cherbourg, Q = Queenstown, S = Southampton","3915b70b":"<h5><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Class 3 passengers who didn't survive<\/div><\/center><\/h5>","b72af310":"#### \ud83d\udd0e Observations\n> Out of 55% of the passengers embarked from  Cherbourg (C) and survived, 35% belongs to 1st class. As the fare of 1st class is high so as the case with passengers embarking from Cherbourg (C), most of them belongs to 1st class.\n\n> Very few passengers from 1st class who survived, embarked from Queenstown (Q).","61ca0e23":"<center><img src = https:\/\/i.imgur.com\/5yXaN27.jpg><\/center>","957d0818":"#### Preprocessing for H2O\n> For training using h2o, we have to create a h2o frame, which is like a pandas dataframe. We have two options for doing this:\n>- read and preprocess the data using h2o frame\n>- read and preprocess the data using pandas and convert it to h2o frame\n\n> As I am more confirtable in handling the data with pandas so I am choosing the second method but if you feel confirtable in handling data with h2o frame, you can very well do that. We have already preprocessed the data using pycaret library so we will continue from that. From pycaret setup we get, X_train, y_train, X_test and y_test as pandas dataframe.\n\n> I didn't find any function to feed pandas dataframe to h2o, so first I convert these df to .csv and then read the .csv using h2o.import_file, to convert them into h2o frame.","6c589da8":"<h3><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Making data ready for training<\/div><\/center><\/h3>","e5fc02e3":"<h4><center>  <div style=\"background-color:pink;border-radius:10px; padding: 10px;\">Comparison between Pclass, Gender \ud83e\udd35\ud83d\udc83, and Port of Embarkment \ud83d\udea2<\/div><\/center><\/h4>\n\n<h5><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">1st Class Passengers who Survived<\/div><\/center><\/h5>","05598721":"<center><img src = https:\/\/i.imgur.com\/PXspAW3.jpg><\/center>","bc27e469":"> The `Fare` feature had skew distribution, so I will log transform it.","6c8f5433":"**Categorical features**: Sex, Embarked, and Pclass. \n\n**Numeric features**: Age, Fare. Discrete: SibSp, Parch.\n\n> Changing the data types of `Pclass` from numeric to category","fd73b5a7":"<h3><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Class Imbalance<\/div><\/center><\/h3>","a8271567":"> If a colum has 70%-80% missing data, it is advisable to remove that column. `Cabin` has 77% missing data, so we will remove that **column**. `Age` and `Embarked` has <20% missing data, so we shall impute the missing values with mean or mode.\n\n> \ud83d\udccc Some models can give you an error if missing values are present and some will show incorrect results, so it is important to remove them.","9dfc5cd5":"#### \ud83d\udd0e Observations\n> There is some relation between `Fare` and `Survived` of the passengers. This also becomes clear from the kde plot, as the nature of both, the blue line (Non Survived) and gree line (Survived) is different.","360419a6":"<a id = \"3\"><\/a><br>\n<h3><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Analysis of Categorical Features \ud83d\udd2c<\/div><\/center><\/h3>\n\n> We have three categorical features`Pclass`, `Sex`, and `Embarked`. Not counting `Survived` as it is the target. Let's analyse the categorical features one by one based on the target.\n\n> For the analysis, I will be using PowerBI, as it is simple and efficient!","16ee1c5d":"<center><img src = https:\/\/i.imgur.com\/LSLpYqR.jpg><\/center>","e8318704":"#### \ud83d\udd0e Observations\n> As seen previously also, very few passenger embarked on Q (Queenstown) port.\n\n> More than half of the passengers didn't survive who embarked from Queenstown (Q) and Southampton (S).\n\n> Passengers embarking from Cherbourg (C) paid more average fare, and also more than 50% of them survived.","2bc91a77":"<a id = \"1\"><\/a><br>\n<h3><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Distribution of Age and Fare \ud83d\udcb0<\/div><\/center><\/h3>\n\n#### Question: How the features `age` and `fare` are distributed, are there any outliers \u2753\n\n<h5><div class=\"alert alert-block alert-info\"> Beginner tip: It is applicable to numeric feature and used for understanding the data distribution and also for detecting outliers! <\/div><\/h5>","0b91bf4a":"> With H2O AutoML also we got AUC of 90%, which is  closer to than what we obtained with CatBoost for test set.","0f99b9e9":"<center><img src = https:\/\/i.imgur.com\/Zh1Ok1r.jpg><\/center>","bafb620d":"<a id = \"2\"><\/a><br>\n<h3><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Relation between Age and Fare \ud83d\udcb0<\/div><\/center><\/h3>\n\n#### Question: Is there any relation between 'age' and 'fare'\u2753 \n\n<h5><div class=\"alert alert-block alert-info\"> Beginner tip: It is applicable when you have two numeric features. It's used for visvalizing relation between numeric features.<\/div><\/h5> ","01600df3":"<h4><center>  <div style=\"background-color:pink;border-radius:10px; padding: 10px;\">Gender \ud83e\udd35\ud83d\udc83<\/div><\/center><\/h4>","ad877b7e":"<h5><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Survived passengers embarking from Southampton (S)<\/div><\/center><\/h5>","119d5369":"<h4><center>  <div style=\"background-color:pink;border-radius:10px; padding: 10px;\">Pclass (Ticket Class)<\/div><\/center><\/h4>\n\nThere are three classes 1st, 2nd, and 3rd.\n\n>- 1 = 1st\n>- 2 = 2nd\n>- 3 = 3rd","3aa52fd0":"#### \ud83d\udd0e Observations\n> Out of 61% passengers who didn't survived and embarked from Queenstown (Q), 58% belongs to class 3.\n\n> Mojority of the female passengers who didn't survived belong to class 3.","4559685f":"<h4><center>  <div style=\"background-color:orange;border-radius:10px; padding: 10px;\">Stacking \ud83d\udcda<\/div><\/center><\/h4>","1c605d3f":"#### \ud83d\udd0e Observations\n> 81% men didn't survive while 74% women survived.\n\n> On average, female passengers paid higher fare than male passengers.","7b480a44":">We have many values that are more than 75 percentile. The data is an outlier or not is the decision of the field expert. We can also tell for some of the data like `Age` - it can't be more than 100 or less than 0, etc. but in this case, I will remove these values while doing ML.","88175adb":"### Scaling Data\n> Feature scaling is essential for machine learning algorithms that calculate distances between data. Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions do not work correctly without normalization.\n\n> Though scaling is not required for tree based algorithms like Decision trees and Random Forest.","5f8533b9":"<center><img src=\"https:\/\/i.pinimg.com\/originals\/7d\/0d\/9b\/7d0d9b4c0214f0a9f074623e2974b500.gif\"><\/center>\n<h1><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Introduction \ud83d\udea2<\/div><\/center><\/h1>\n\n>On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\n>Many of us have come across this datasets in the start of Data Science journey, and this is a classic example to practice the skills of EDA and Machine Learning. This notebook is especially more useful for beginners in Data Science. It gives step by step guide for EDA and ML, after reading this notebook, I am sure you will be more comfortable in EDA and ML and you will be able to present your data with better visuals.","e493760e":"#### \ud83d\udd0d Observations\n> The correlation between `Age` and `Fare` is 0.097, which is close to 0, so there is no relation between them. It means, we can't guess how much fare a passenger has paid just by looking at his\/her age.\n\n#### \ud83d\udcdd Note\n\n> The correlation score:\n> - 1 : Strongly and positively correlated (one increases, other also increases and vice versa)\n> - 0 : No correlation\n> - -1 : Strongly and negetively correlated (one increases, other also decreases and vice versa)","c1551896":"#### \ud83d\udd0eObservation:\n> Passengers of class 3 are huge in number.\n\n> Very few passenger embarked on Q (Queenstown) port.\n\n> The distribution of `Fare` is skew.","91414a7b":"<h1><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Exploratory Data Analysis \ud83d\udcca<\/div><\/center><\/h1>\n\n> Before doing any kind of EDA, you should have a purpose of doing it. It should be clear what you want otherwise you will produce beautiful charts with no use. You should ask yourself, what you want to see and then choose the appropriate plot and features. So, I have mentioned the questions, which I thought before plotting the charts.\n\n> Note that, for a perticular question, there can be multiple charts. I have choosen the chart which I think was more appropriate. Now let's do some EDA.","cfba67e3":"<h3><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Data Cleaning \ud83d\udebf<\/div><\/center><\/h3>\n\n> Data may have missing values, wrong data types, outliers, etc. So, before you do EDA on your data, please check for these things and if they are present, you have to clean the data before moving further otherwise you can get misleading and incorrect visuals. So let's fold the sleeves and get ready for cleaning!","1ae7cf37":"#### \ud83d\udd0e Observations\n> There is not much relation between age and survival of the passengers. This also becomes clear from the kde plot, as the nature of both, the blue line (Non Survived) and gree line (Survived) is similar.","e443a295":"### Conclusion:\n\n> The data analysis shows that the rich, old and woman mostly survived\n\n> The ML model using H2O.ai gives AUC ROC score of 0.91 and accuracy of 88%","ea96776f":"Only 38% passengers suvived, so there is some class imbalance and so it has to be solved. \nSMOTE\nADSYN"}}