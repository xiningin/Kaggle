{"cell_type":{"36bef1af":"code","f79e593a":"code","67304358":"code","1c63f7df":"code","d88d8c3c":"code","b9529596":"code","0f331b72":"code","b6e7912f":"code","edc5d9e4":"code","f774988e":"code","ef0efef1":"code","1b97df71":"code","f21c178a":"code","c9b33630":"code","0ad68227":"markdown","d8f4e27c":"markdown","e604b8f7":"markdown","b5852515":"markdown","8c91082c":"markdown","ed889d05":"markdown","8e9133f2":"markdown","d3825f99":"markdown","3fcd0171":"markdown","39965cdb":"markdown","220506a5":"markdown","2c9d3c40":"markdown"},"source":{"36bef1af":"#\u4e0b\u8f7d\u76f8\u5173\u7684\u5305\n# !pip install -q imagesize\n# !pip install -qU wandb\n# !add-apt-repository ppa:ubuntu-toolchain-r\/test -y\n# !apt-get update\n# !apt-get upgrade libstdc++6 -y","f79e593a":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nsys.path.append('..\/input\/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image\nimport ast","67304358":"ROOT_DIR  = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nCKPT_PATH = '\/kaggle\/input\/greatbarrierreef-yolov5-train-ds\/yolov5\/runs\/train\/exp\/weights\/best.pt'\nIMG_SIZE  = 1280\nCONF      = 0.15\nIOU       = 0.50\nAUGMENT   = False","1c63f7df":"#\u62ff\u5230\u8def\u5f84\ndef get_path(row):\n    row['image_path'] = f'{ROOT_DIR}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row","d88d8c3c":"# \u5bfc\u5165\u8bad\u7ec3\u96c6\ndf = pd.read_csv(f'{ROOT_DIR}\/train.csv')\ndf = df.progress_apply(get_path, axis=1)\ndf['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndisplay(df.head(5))","b9529596":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts()\/len(df)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","0f331b72":"def voc2yolo(bboxes, image_height=720, image_width=1280):\n    bboxes = bboxes.copy().astype(float) # \u5426\u5219\u6240\u6709\u503c\u90fd\u5c06\u4e3a 0\uff0c\u56e0\u4e3a voc_pascal dtype \u662f np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]\/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0]\n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    bboxes[..., 0] = bboxes[..., 0] + w\/2\n    bboxes[..., 1] = bboxes[..., 1] + h\/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    return bboxes\n\ndef yolo2voc(bboxes, image_height=720, image_width=1280):\n    bboxes = bboxes.copy().astype(float) # \u5426\u5219\u6240\u6709\u503c\u90fd\u5c06\u4e3a 0\uff0c\u56e0\u4e3a voc_pascal dtype \u662f np.int\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n    return bboxes\n\ndef coco2yolo(bboxes, image_height=720, image_width=1280):\n    bboxes = bboxes.copy().astype(float) # \u5426\u5219\u6240\u6709\u503c\u90fd\u5c06\u4e3a 0\uff0c\u56e0\u4e3a voc_pascal dtype \u662f np.int\n    \n    # \u6807\u51c6\u5316\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]\/ image_height\n    \n    # \u8f6c\u6362 (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\/2\n    return bboxes\n\ndef yolo2coco(bboxes, image_height=720, image_width=1280):\n    bboxes = bboxes.copy().astype(float) # \u5426\u5219\u6240\u6709\u503c\u90fd\u5c06\u4e3a 0\uff0c\u56e0\u4e3a voc_pascal dtype \u662f np.int\n    \n    # \u975e\u89c4\u8303\u5316\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    # \u8f6c\u6362 (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    return bboxes\n\ndef voc2coco(bboxes, image_height=720, image_width=1280):\n    bboxes  = voc2yolo(bboxes, image_height, image_width)\n    bboxes  = yolo2coco(bboxes, image_height, image_width)\n    return bboxes\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # \u5728\u56fe\u50cf img \u4e0a\u7ed8\u5236\u4e00\u4e2a\u8fb9\u754c\u6846\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) \/ 2) + 1  # \u7ebf\u6761\/\u5b57\u4f53\u7c97\u7ec6\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # \u5b57\u4f53\u7c97\u7ec6\n        t_size = cv2.getTextSize(label, 0, fontScale=tl \/ 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  \n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl \/ 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):   \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    if bbox_format == 'yolo':  \n        for idx in range(len(bboxes)):         \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors     \n            if cls in show_classes:         \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]\/2) #w\/2 \n                h  = round(float(bbox[3])*image.shape[0]\/2)\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)     \n    elif bbox_format == 'coco':\n        for idx in range(len(bboxes)):     \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    elif bbox_format == 'voc_pascal':\n        for idx in range(len(bboxes)):  \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n    return image\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","b6e7912f":"!mkdir -p \/root\/.config\/Ultralytics\n!cp \/kaggle\/input\/yolov5-font\/Arial.ttf \/root\/.config\/Ultralytics\/","edc5d9e4":"def load_model(ckpt_path, conf=0.25, iou=0.50):\n    model = torch.hub.load('\/kaggle\/input\/yolov5-lib-ds',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)\n    \n    model.conf = conf  # NMS \u7f6e\u4fe1\u5ea6\u9608\u503c\n    model.iou  = iou  #NMS IoU \u9608\u503c\n    model.classes = None   # \uff08\u53ef\u9009\u5217\u8868\uff09\u6309\u7c7b\u522b\u8fc7\u6ee4\uff0c\u5373 = [0, 15, 16] \u7528\u4e8e\u4eba\u3001\u732b\u548c\u72d7\n    model.multi_label = False  # \u6bcf\u76d2 NMS \u591a\u4e2a\u6807\u7b7e\n    model.max_det = 1000  # \u6bcf\u4e2a\u56fe\u50cf\u7684\u6700\u5927\u68c0\u6d4b\u6570\n    return model","f774988e":"def predict(model, img, size=768, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # \u81ea\u5b9a\u4e49\u63a8\u7406\u5927\u5c0f\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","ef0efef1":"model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\nimage_paths = df[df.num_bbox>1].sample(100).image_path.tolist()\nfor idx, path in enumerate(image_paths):\n    img = cv2.imread(path)[...,::-1]\n    bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    display(show_img(img, bboxes, bbox_format='coco'))\n    if idx>5:\n        break","1b97df71":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# \u521d\u59cb\u5316\u73af\u5883\niter_test = env.iter_test()      # \u4e00\u4e2a\u5faa\u73af\u6d4b\u8bd5\u96c6\u548c\u6837\u672c\u63d0\u4ea4\u7684\u8fed\u4ee3\u5668","f21c178a":"model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\nfor idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    \n    bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=True)\n    annot          = format_prediction(bboxes, confs)\n    pred_df['annotations'] = annot\n    \n    env.predict(pred_df)\n    if idx<3:\n        display(show_img(img, bboxes, bbox_format='coco'))","c9b33630":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","0ad68227":"# \u521d\u59cb\u5316\u73af\u5883","d8f4e27c":"# \u7f16\u5199\u53ef\u7528\u51fd\u6570","e604b8f7":"#  \u4e0b\u8f7d\u5305","b5852515":"# \u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u63a8\u7406\u9884\u6d4b","8c91082c":"# \u7edf\u8ba1BBoxes\u6570\u91cf\n\u8fd1 80% \u7684\u56fe\u50cf\u6ca1\u6709\u4efb\u4f55 bbox\u3002","ed889d05":"# \u68c0\u67e5\u63d0\u4ea4\u7ed3\u679c","8e9133f2":"# \u5728\u8bad\u7ec3\u96c6\u4e0a\u8fdb\u884c\u63a8\u7406\u9884\u6d4b","d3825f99":"#  \u5bfc\u5165\u8bad\u7ec3\u96c6\n","3fcd0171":"##  Notebooks:\n* Train: [Great-Barrier-Reef: YOLOv5 [train] ](https:\/\/www.kaggle.com\/awsaf49\/great-barrier-reef-yolov5-train)\n* Infer: [Great-Barrier-Reef: YOLOv5 [infer] ](https:\/\/www.kaggle.com\/awsaf49\/great-barrier-reef-yolov5-infer)","39965cdb":"# \u5efa\u7acb\u6a21\u578b","220506a5":"# \u7f16\u5199\u53ef\u7528\u51fd\u6570","2c9d3c40":"#  \u5bfc\u5165\u5de5\u5177\u5305"}}