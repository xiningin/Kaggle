{"cell_type":{"c70c93e9":"code","f72a75d5":"code","999108c3":"code","70f6e63e":"code","f6fea1c9":"code","0d499d3f":"code","9b771f1a":"code","91dda1ec":"code","2be1542b":"code","a9f7d550":"code","98e9b2e3":"code","4d876867":"code","aec58afc":"code","cf487010":"code","d32209b6":"code","b0d6b97f":"code","f101542d":"code","b10d9613":"code","bfe8b71e":"code","f6b8647a":"code","1f1a1d6f":"code","44483644":"code","13c3776d":"code","faa89780":"code","a17d16b8":"code","d7d6915c":"code","5fecc460":"code","310dbdc1":"code","7d782d97":"markdown","da558b6b":"markdown","8352a9ac":"markdown","1a40aa83":"markdown","24df077e":"markdown","28b6d6ee":"markdown","0e9005ad":"markdown","9a765b37":"markdown","61c47081":"markdown","3a772629":"markdown","00152fe4":"markdown","deb07a65":"markdown","58b07baf":"markdown","83775431":"markdown","3c81301e":"markdown","2618f678":"markdown","4b9b4d35":"markdown","1d10f2fd":"markdown","60ca388c":"markdown","aa29b9a4":"markdown","ab64fff2":"markdown","cd878312":"markdown","24fbbacc":"markdown","57d15887":"markdown","733ae369":"markdown","5e5883c9":"markdown","a2191b39":"markdown","e792970c":"markdown","ca47e7c6":"markdown"},"source":{"c70c93e9":"import pandas as pd\nimport matplotlib.pyplot as plt","f72a75d5":"def plot_tree(fitted_tree, feature_names, label_names):\n    \"\"\"Fun\u00e7\u00e3o auxiliar para a visualiza\u00e7\u00e3o de uma \u00e1rvore de decis\u00e3o treinada\n    \n    Args:\n        fitted_tree (sklearn.tree.DecisionTreeClassifier): \u00c1rvore de decis\u00e3o treinada\n        feature_names (list): Lista com o nome dos atributos que est\u00e3o na \u00e1rvore\n        label_names (list): Lista com o nome das labels que est\u00e3o presentes nos dados\n    Returns:\n        graphviz.Source: Objeto com imagem gerada\n    See:\n        C\u00f3digo adaptado da documenta\u00e7\u00e3o do sklearn. Confira em: https:\/\/scikit-learn.org\/stable\/modules\/tree.html\n    \"\"\"\n    \n    import graphviz\n    from sklearn.tree import export_graphviz\n    \n    dot_data = export_graphviz(clf, out_file=None, \n                     feature_names=feature_names,  \n                     class_names=label_names,  \n                     filled=True, rounded=True,  \n                     special_characters=True)\n    \n    graph = graphviz.Source(dot_data)\n    return graph","999108c3":"from sklearn import datasets\n\nwine = datasets.load_wine()","70f6e63e":"x_original, y_original = wine.data, wine.target","f6fea1c9":"wine.feature_names","0d499d3f":"wine.target_names","9b771f1a":"winedf = pd.DataFrame(x_original, y_original, columns = wine.feature_names)\nwinedf['label'] = wine.target\nwinedf","91dda1ec":"import seaborn as sns\n\nsns.pairplot(winedf, hue = 'label')","2be1542b":"from sklearn.model_selection import train_test_split","a9f7d550":"x_train, x_test, y_train, y_test = train_test_split(x_original, y_original, train_size = 0.7)","98e9b2e3":"from sklearn.tree import DecisionTreeClassifier","4d876867":"clf = DecisionTreeClassifier(criterion = 'entropy')","aec58afc":"clf = clf.fit(x_train, y_train)","cf487010":"plot_tree(clf, wine.feature_names, wine.target_names)","d32209b6":"y_pred = clf.predict(x_test)\n\ny_pred","b0d6b97f":"from sklearn.metrics import plot_confusion_matrix, accuracy_score","f101542d":"accuracy_score(y_test, y_pred)","b10d9613":"fig, ax = plt.subplots(dpi = 110)\n\ndisp = plot_confusion_matrix(clf, x_test, y_test, cmap = plt.cm.Blues, \n                                 display_labels = wine.target_names, ax = ax)","bfe8b71e":"clf = DecisionTreeClassifier(criterion = 'entropy', max_depth=2)\nclf = clf.fit(x_train, y_train)","f6b8647a":"plot_tree(clf, wine.feature_names, wine.target_names)","1f1a1d6f":"y_pred = clf.predict(x_test)\n\ny_pred","44483644":"accuracy_score(y_test, y_pred)","13c3776d":"fig, ax = plt.subplots(dpi = 110)\nplot_confusion_matrix(clf, x_test, y_test, cmap = plt.cm.Blues, \n                                 display_labels = wine.target_names, ax = ax)","faa89780":"clf = DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes=3)\nclf = clf.fit(x_train, y_train)","a17d16b8":"plot_tree(clf, wine.feature_names, wine.target_names)","d7d6915c":"y_pred = clf.predict(x_test)\n\ny_pred","5fecc460":"accuracy_score(y_test, y_pred)","310dbdc1":"fig, ax = plt.subplots(dpi = 110)\nplot_confusion_matrix(clf, x_test, y_test, cmap = plt.cm.Blues, \n                                 display_labels = wine.target_names, ax = ax)","7d782d97":"**Classificando os dados de teste**","da558b6b":"#### Quantidade m\u00e1xima de n\u00f3s folha\n\nUm outro par\u00e2metro que vamos considerar mudar neste exemplo \u00e9 o `max_leaf_nodes`, que representa a quantidade m\u00e1xima de n\u00f3s folha que podem estar presentes na estrutura da \u00e1rvore gerada.","8352a9ac":"**Classificando os dados de teste**","1a40aa83":"<img src=\"https:\/\/d1nhio0ox7pgb.cloudfront.net\/_img\/g_collection_png\/standard\/512x512\/wine.png\" align=\"right\" width=\"180\" \/>\n\n### Classificando vinhos com \u00e1rvores de decis\u00e3o\n\nNesta atividade, faremos a classifica\u00e7\u00e3o do *wine dataset*, sendo este um conjunto de dados disponibilizado publicamente pelo [UCI Machine Learning](https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine). No *wine dataset*, est\u00e3o dispon\u00edveis dados resultantes de an\u00e1lises qu\u00edmicas de vinhos cultivados em uma certa regi\u00e3o da It\u00e1lia, mas derivados de tr\u00eas cultivares diferentes. O conjunto de dados \u00e9 composto por 13 atributos e tem 178 amostras de dados.\n\nO nosso objetivo nesta atividade \u00e9 classificar os dados que est\u00e3o presentes neste conjunto de dados, utilizando de seus diversos atributos e ent\u00e3o aplicando a classifica\u00e7\u00e3o com a \u00c1rvore de decis\u00e3o. O ponto positivo do uso *wine dataset* para este caso, \u00e9 que ele apresenta diversos atributos, o que nos ajuda a ver a sele\u00e7\u00e3o desses por `Ganho de informa\u00e7\u00e3o`, durante o processo de cria\u00e7\u00e3o da \u00e1rvore.","24df077e":"Para a classifica\u00e7\u00e3o do *wine dataset*, vamos criar uma \u00e1rvore de decis\u00e3o especificando que o crit\u00e9rio para a cria\u00e7\u00e3o de novos n\u00f3s ser\u00e1 a entropia.","28b6d6ee":"Com a ajuda da fun\u00e7\u00e3o auxiliar `plot_tree`, vamos visualizar a \u00e1rvore gerada e as regras que foram criadas para a divis\u00e3o do conjunto de dados.\n\n> Na visualiza\u00e7\u00e3o abaixo, perceba que temos exatamente o que vimos na apresenta\u00e7\u00e3o da teoria da \u00e1rvore de decis\u00e3o. A \u00e1rvore \u00e9 iniciada com um n\u00f3 raiz, neste h\u00e1 uma quantidade consider\u00e1vel de entropia j\u00e1 que ele est\u00e1 agrupando os dados de todas as classes. A partir do n\u00edvel 2 da \u00e1rvore este cen\u00e1rio muda, e as decis\u00f5es fazem com que a entropia em cada n\u00f3 gerado v\u00e1 diminuindo\n","0e9005ad":"**Avaliando os resultados**\n\nO efeito que ocorreu \u00e9 bastante interessante, uma \u00e1rvore mais simples, acabou gerando resultados melhores! Mas por qual motivo ? Bem, as \u00e1rvore de decis\u00e3o, como vimos, em seu processo de treinamento, ajusta regras que permitam a divis\u00e3o dos dados e com essa divis\u00e3o, pode-se fazer a classifica\u00e7\u00e3o.\n\nUm grande problema enfrentado por quem trabalha aplicando este tipo de algoritmo \u00e9 que, quanto maior a quantidade de regras que est\u00e1 sendo gerada, mais a \u00e1rvore sabe do conjunto de dados de treino, o que \u00e9 ruim. A \u00e1rvore come\u00e7a a criar regras para tentar acertar mais, o que \u00e9 natural, mas, essas regras come\u00e7am a levar caracter\u00edsticas muito espec\u00edficas em considera\u00e7\u00e3o, caracter\u00edsticas essas que podem estar dispon\u00edveis apenas no conjunto de dados de treino.\n\nEnt\u00e3o, quando definimos uma regra de quantidade m\u00e1xima de n\u00f3s, a cria\u00e7\u00e3o das regras acabou sendo mais geral, considerando caracter\u00edsticas que fazem a divis\u00e3o mais geral dos dados, n\u00e3o tendo regras para especificidades.\n","9a765b37":"#### Avaliando os resultados\n\nN\u00e3o basta apenas realizarmos o treinamento do algoritmo e vermos sua estrutura, \u00e9 necess\u00e1rio que fa\u00e7amos avalia\u00e7\u00f5es nos resultados das classifica\u00e7\u00f5es que este algoritmo est\u00e1 gerando, o que ajuda a identificar se os par\u00e2metros e configura\u00e7\u00f5es aplicadas no algoritmo s\u00e3o o suficiente para que ele possa generalizar e classificar amostras de dados que ele n\u00e3o tenha visto antes.\n\nNo nosso caso, para a avalia\u00e7\u00e3o, vamos classificar o conjunto de teste que hav\u00edamos separado antes de treinar a \u00e1rvore.","61c47081":"**Avaliando os resultados**\n\nEspere, olhe os resultados abaixo. A simplifica\u00e7\u00e3o que foi feita na \u00e1rvore acabou fazendo com que a acur\u00e1cia geral das classifica\u00e7\u00f5es nos dados de teste fossem reduzidas, isso \u00e9 esperado ? A resposta para isso \u00e9 depende. Para o problema que estamos trabalhando, a simplifica\u00e7\u00e3o feita acabou gerando uma \u00e1rvore com uma quantidade t\u00e3o pequena de regras que isso acabou influ\u00eanciando as classifica\u00e7\u00f5es feitas.\n\nO ponto importante deste exemplo \u00e9 entender que dentro do processo de *Machine Learning*, n\u00e3o existem f\u00f3rmulas prontas para a solu\u00e7\u00e3o de problemas, cada caso \u00e9 um caso. Normalmente, em an\u00e1lises reais, a aplica\u00e7\u00e3o desses algoritmos pode representar uma tarefa iterativa, onde melhorias v\u00e3o sendo feitas no modelo que est\u00e1 sendo criado \u00e0 medida em que os passos anteriores v\u00e3o trazendo novas informa\u00e7\u00f5es sobre a aplica\u00e7\u00e3o do modelo e tamb\u00e9m sobre os dados.","3a772629":"#### Profundidade m\u00e1xima da \u00e1rvore\n\nA primeira altera\u00e7\u00e3o que vamos testar \u00e9 a limita\u00e7\u00e3o da profundidade m\u00e1xima que a \u00e1rvore gerada pode ter. Utilizando do par\u00e2metro `max_depth` fa\u00e7amos essa defini\u00e7\u00e3o.","00152fe4":"Al\u00e9m disso, vamos olhar para cada um dos atributos que est\u00e3o dispon\u00edveis dentro do conjunto de dados.\n\n> Como informado no site [UCI Machine Learning](https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine), onde os dados s\u00e3o disseminados, cada um dos atributos representa caracter\u00edsticas qu\u00edmicas que descrevem cada uma das amostras presentes no conjunto de dados.\n","deb07a65":"Cabe informar tamb\u00e9m que este conjunto de dados tem tr\u00eas *labels* definidas: `class_1`, `class_2` e `class_3`. Sendo que cada classe representa um cultivare diferente.","58b07baf":"**Visualizando a \u00e1rvore gerada**\n\nAinda mais simples que a \u00e1rvore gerada anteriormente, nesta vers\u00e3o o que temos \u00e9 uma \u00e1rvore de decis\u00e3o com apenas tr\u00eas n\u00f3s folha, cada um armazenando os dados de uma classe.\n","83775431":"### Modelando \u00e1rvores de decis\u00e3o\n\nComo foi citado anteriormente, ao utilizar o scikit-learn, \u00e9 poss\u00edvel utilizar de diferentes par\u00e2metros para definir o comportamento da \u00e1rvore de decis\u00e3o. Nesta se\u00e7\u00e3o, vamos focar em variar alguns par\u00e2metros para tentar melhorar o resultado de classifica\u00e7\u00e3o de nossa \u00e1rvore de decis\u00e3o.","3c81301e":"Agora a gera\u00e7\u00e3o da \u00e1rvore j\u00e1 pode ser feita. Ent\u00e3o, atrav\u00e9s do m\u00e9todo `fit`, vamos realizar o treinamento do modelo de \u00e1rvore de decis\u00e3o. \n\n> Note que o m\u00e9todo `fit` do scikit-learn, faz parte de uma interface padronizada, ou seja, independente do algoritmo que voc\u00ea for utilizar, este m\u00e9todo estar\u00e1 dispon\u00edvel com os mesmos par\u00e2metros, mesmo que o algoritmo n\u00e3o realize o treinamento considerando amostras e suas classes, como \u00e9 o caso de algoritmos supervisionado.","2618f678":"Com os dados carregados, por uma quest\u00e3o de facilidade, vamos recuperar os valores de cada atributo e tamb\u00e9m das `labels`, separadamente.","4b9b4d35":"> A quantidade de erros na matriz de confus\u00e3o \u00e9 quase a mesma que a vista anteriormente, com a diferen\u00e7a de que o erro parece n\u00e3o tender diretamente para uma classe, como acontecia na \u00e1rvore anterior.","1d10f2fd":"Na matriz de confus\u00e3o h\u00e1 muito mais erro que nos demais.","60ca388c":"Analisando a \u00e1rvore de decis\u00e3o, percebemos que existe uma certa confus\u00e3o com as classifica\u00e7\u00f5es da `class_0`, o que acontece nas outras, mas com menos valores, o que pode ter v\u00e1rios significados. Em um contexto de `An\u00e1lise Explorat\u00f3ria de Dados` por exemplo, \u00e9 poss\u00edvel utilizar dessa informa\u00e7\u00e3o como um ponto de partida para uma investiga\u00e7\u00e3o nos dados.","aa29b9a4":"**Visualizando a \u00e1rvore gerada**\n\nAo visualizar a \u00e1rvore, note que agora temos um modelo bem mais simples do que o gerado originalmente.","ab64fff2":"Agora, fa\u00e7amos o preparo dos dados para que eles possam ser utilizados. Come\u00e7aremos com a divis\u00e3o dos dados em `Treino` e `Teste`. Onde 70% dos dados s\u00e3o treinamento e o restante compoe os dados de teste.","cd878312":"Aproveitando tamb\u00e9m, fa\u00e7amos a visualiza\u00e7\u00e3o das correla\u00e7\u00f5es entre as vari\u00e1veis utilizando o `seaborn` com a fun\u00e7\u00e3o `pairplot`.","24fbbacc":"<img src=\"https:\/\/suspicious-wescoff-e06084.netlify.app\/banner_notebook.png\"\/>\n\n<br>\n\n# Exemplo de aplica\u00e7\u00e3o da \u00c1rvore de decis\u00e3o\n\n- **Instrutores**: Adriano, Felipe Carvalho e Felipe Menino\n\n- **Realiza\u00e7\u00e3o**: Dia 15\/09\n\n- **Descri\u00e7\u00e3o**: Objetiva-se apresentar aos alunos exemplos de aplica\u00e7\u00e3o de algoritmos de classifica\u00e7\u00e3o utilizando \u00c1rvores de decis\u00e3o. \n\n* **Sum\u00e1rio**:\n    * [Livro **Introdu\u00e7\u00e3o ao Machine Learning**](https:\/\/dataat.github.io\/introducao-ao-machine-learning\/)\n    * [Exemplo de **Classifica\u00e7\u00e3o** em Python](https:\/\/www.kaggle.com\/phelpsmemo\/intro-ml-python-knn-worcap2020)\n    * [Exemplo de **Regress\u00e3o** em Python]()\n    * [Exemplo de **Agrupamento** em R](https:\/\/www.kaggle.com\/oldlipe\/intro-ml-r-hiererquico-worcap2020)\n\n\n<hr style=\"border: 2px solid #0984e3;\">\n","57d15887":"Para tudo ficar mais claro, vamos consolidar todas essas informa\u00e7\u00f5es em um `pandas.Dataframe`.","733ae369":"Nossos resultados est\u00e3o bem interessantes! Com base nessa avalia\u00e7\u00e3o de acur\u00e1cia, temos que das classifica\u00e7\u00f5es realizadas sobre o conjunto de dados de teste, 88% estavam corretas.\n\n> Como melhorar ? Com o entendimento do conjunto de dados e do contexto do problema. Isso acontece j\u00e1 que, a depender do problema, certas mudan\u00e7as podem ser feitas na \u00e1rvore, como sua simplifica\u00e7\u00e3o. Entender os dados \u00e9 vital!\n\nVamos agora olhar a matriz de confus\u00e3o.","5e5883c9":"<hr>\n\nPara come\u00e7ar esta atividade vamos carregar os dados do *wine dataset* com a biblioteca sklearn. Alternativamente, voc\u00ea pode obter os mesmos dados na p\u00e1gina do *dataset* no [UCI Machine Learning](https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine).\n","a2191b39":"At\u00e9 este ponto, apenas entendemos e organizamos os dados. Agora, com o uso do m\u00f3dulo `sklearn.tree`, vamos utilizar a classe `sklearn.tree.DecisionTreeClassifier` e classificar os dados.\n\n> A classe sklearn.tree.DecisionTreeClassifier representa uma \u00e1rvore de decis\u00e3o. Ao utilizar esta classe \u00e9 poss\u00edvel especificar par\u00e2metros que alterem os comportamentos do algoritmo de gera\u00e7\u00e3o da \u00e1rvore. Alguns dos par\u00e2metros poss\u00edveis s\u00e3o:\n> * criterion: Define o crit\u00e9rio para a sele\u00e7\u00e3o dos n\u00f3s. Podem ser utilizados o `gini` e `entropy`\n> * max_depth: Par\u00e2metro que controla a profundidade da \u00e1rvore. Ajuda a evitar problemas com *overfitting* nos dados.\n> * max_leaf_notes: Determina a quantidade m\u00e1xima de n\u00f3s que a \u00e1rvore de decis\u00e3o pode ter. Com este par\u00e2metro \u00e9 poss\u00edvel controlar a quantidade de ramifica\u00e7\u00f5es que a \u00e1rvore vai ter. Assim como o `max_depth`, ajuda a evitar problemas com *overfitting*.","e792970c":"#### Defini\u00e7\u00e3o de fun\u00e7\u00f5es auxiliares\n\nCriar fun\u00e7\u00f5es auxiliares nos permite reutilizar c\u00f3digo e otimizar nosso tempo. Abaixo, s\u00e3o definidas algumas fun\u00e7\u00f5es auxiliares para facilitar algumas etapas. Fique \u00e0 vontade para modificar e usar essas fun\u00e7\u00f5es.\n","ca47e7c6":"> A pergunta \u00e9: Quanto do resultado dessa classifica\u00e7\u00e3o est\u00e1 condizente com o que realmente deveria ser ? \n\nCom base nesta pergunta, vamos utilizar as funcionalidades `plot_confusion_matrix` e `accuracy_score` do m\u00f3dulo `sklearn.metrics`."}}