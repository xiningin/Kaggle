{"cell_type":{"7b0d2581":"code","c062c1c3":"code","914e34e2":"code","7306dcdb":"code","5aac7980":"code","b8ef2c91":"code","d0a35f8a":"code","fb794264":"code","8ae5c379":"code","89464e31":"code","74fef8df":"code","f27f6c81":"code","5ba8f161":"code","72f40610":"code","16bb377e":"code","bd8eb44f":"code","01d93f29":"code","2e7ee7f1":"code","802f7a94":"code","a635294d":"code","3803bf4a":"markdown","16ad1cf6":"markdown","292ee40a":"markdown","6d1ad27a":"markdown","365b2639":"markdown","1471249c":"markdown","4644229c":"markdown","2ddb671d":"markdown","f85f3164":"markdown","dd840e4f":"markdown","514360b4":"markdown","127f8810":"markdown","462e4bdf":"markdown"},"source":{"7b0d2581":"import scipy as sp\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport lightgbm as lgb\n\n# so that we can print multiple dataframe in the same cell\nfrom IPython.display import display, HTML\ndef displayer(df): display(HTML(df.head(2).to_html()))\npd.set_option('display.max_columns', 500)","c062c1c3":"train_data_num = pd.read_csv(\"..\/input\/housing\/Housing_Price_Prediction_Train.csv\")\ntest_data_num = pd.read_csv(\"..\/input\/housing\/Housing_Price_Prediction_Test.csv\")\nprint(train_data_num.shape)\nprint(test_data_num.shape)\ndisplayer(train_data_num)\ndisplayer(test_data_num)\n# train_labels.shape","914e34e2":"# drop labels here\ntrain_labels = train_data_num[\"price\"]\ntrain_data_num = train_data_num.drop([\"price\"], axis=1)\ntest_data_num.columns","7306dcdb":"considered_columns = ['DOM', 'followers', 'square', 'livingRoom',\n       'drawingRoom', 'kitchen', 'bathRoom', 'buildingType',\n       'renovationCondition', 'buildingStructure', 'ladderRatio', 'elevator',\n       'fiveYearsProperty', 'subway', 'district']\ntrain_data_num = train_data_num[considered_columns]\ntest_data_num = test_data_num[considered_columns]","5aac7980":"df_eda = pd.DataFrame()\ndf_eda[\"train_total_na\"] = train_data_num.isnull().sum()\ndf_eda[\"test_total_na\"] = test_data_num.isnull().sum()\ndf_eda[\"train_percent_na\"] = train_data_num.isnull().sum()\/train_data_num.shape[0]*100\ndf_eda[\"test_percent_na\"] = test_data_num.isnull().sum()\/test_data_num.shape[0]*100\ndf_eda","b8ef2c91":"train_data_num = train_data_num.drop(['DOM'], axis=1)\ntest_data_num = test_data_num.drop(['DOM'], axis=1)","d0a35f8a":"# Visualising prices\nplt.figure(figsize=(14,3))\nplt.hist(list(train_labels))\nplt.show()","fb794264":"import matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\ncmap = cm.plasma\nnorm = Normalize(vmin=0, vmax=100000)\ncolormap = cmap(norm(list(train_labels)[:100]))","8ae5c379":"from pandas.plotting import scatter_matrix\ndf_plot = train_data_num.head(100)\ndf_plot = df_plot.applymap(lambda x: x + np.random.randn()\/10.)\nscatter_matrix(df_plot[['followers', 'square', 'livingRoom', 'drawingRoom', 'kitchen',\n               'bathRoom', 'renovationCondition', 'buildingStructure',\n               'ladderRatio', 'elevator', 'fiveYearsProperty', 'subway', 'district']], alpha = 1., \n               figsize = (16, 16), diagonal = 'kde', color = colormap)\nplt.show()","89464e31":"categorical_features = ['livingRoom', 'drawingRoom', 'kitchen', 'bathRoom',\n       'renovationCondition', 'buildingStructure', 'elevator',\n       'fiveYearsProperty', 'subway', 'district']","74fef8df":"train_dataset = lgb.Dataset(train_data_num, label=train_labels,\n                            free_raw_data=False,\n                            categorical_feature=categorical_features)\ntest_dataset = lgb.Dataset(test_data_num, label=train_labels,\n                           free_raw_data=False,\n                           categorical_feature=categorical_features)","f27f6c81":"num_rows = train_labels.shape[0]\nskf = KFold(n_splits=10, shuffle=True, random_state=42)\nfolds = [fold for fold in skf.split(np.arange(num_rows), train_labels)]","5ba8f161":"param = {'num_leaves': 127,\n         'objective': 'regression',\n         'early_stopping_round': 10,\n         'verbose_eval': -1,\n         'verbose': -1}\nnum_round = 200","72f40610":"bst_lst = []\nfor i, (tr_idx, tx_idx) in enumerate(folds):\n    print(\"fold number \", i)\n    bst = lgb.train(param,\n                    train_dataset.subset(tr_idx).construct(), \n                    num_round,\n                    categorical_feature=categorical_features,\n                    valid_sets=train_dataset.subset(tx_idx).construct())\n    bst_lst.append(bst)","16bb377e":"print(sum(list(train_labels))\/len(train_labels))","bd8eb44f":"lgb.plot_importance(bst, importance_type=\"gain\", figsize=(14,5))","01d93f29":"lgb.plot_importance(bst, importance_type=\"split\", figsize=(14,5))","2e7ee7f1":"# this is how one of the trees tree looks like\nlgb.plot_tree(bst, tree_index=53, figsize=(30, 15), show_info=['split_gain'])\nplt.show()","802f7a94":"preds = []\nfor bst in bst_lst:\n    pred = bst.predict(test_data_num) # not test_dataset\n    preds.append(pred)\npreds = np.array(preds)\nprint(preds.shape)\n\npreds_mean = np.mean(preds, axis=0)\nprint(preds_mean.shape)","a635294d":"test_data_num[\"price\"] = preds_mean\ntest_data_num.head()\ntest_data_num.to_csv(\"Price Prediction_Result.csv\")","3803bf4a":"## Categorical features\nNow we define identify the categorical features from what we see in the scatter plot.\n","16ad1cf6":"## Distribution Analysis\nWe analyse the distribution to visualise what affects the price.","292ee40a":"We first load the required modules and the dataset","6d1ad27a":"We do Kfold cross validation to create maximise the data used to train the model","365b2639":"# Data Engineering\n> Data Engineering Approach (how to deal with missing value, categorical data, data skewness)\n\nFirst we remove values we do not think is relevant to our prediction, the URL and ID.\n\nWe might be processing the tradeTime later.","1471249c":"We will not plot this a very beautiful scatterplot matrix to visualise the relationship between characterisitics.","4644229c":"We can see the the loss is around 10^8. This is a very reasonable estiamate considering the distribution of prices, with the average at 53165.","2ddb671d":"## Missing Values\nWe visualise the amount of NaN. We see that in the test data has very high percentage of NaNs for DOM. \n\nTherefore we should drom DOM from both the test data and training data.\n\nThe percentage NaN of buildingType is lower for the test set, so it should not be a problem.","f85f3164":"From the above, we see that the district is the most important measure of the price of the house. The next important feature is square.","dd840e4f":"Information on the apartment size (area) and the \"ladder\" ratio helps to fine tune the prediction of the housing prices.\n\nIn summary, we argue that the price is first determined by the district. Then the result is fine tuned with \"square\" and \"ladder\".\n\nWe now predict for the test set.","514360b4":"These parameters are selected because it gave the best results. \n\n## Regression problem and loss function\n\nThis is a regression problem because we are predict the price, with is a (almost) continuous variable.\nThe default loss function for a regression is L2 loss. The desired loss function can be changed, depending on the situation and the client.\n\n## Fine-tuning hyper-parameters\n\nThe model hyperparameters could have be fine-tuned futher. As the model parameters are not trained on the validation set we are confident that overfitting is unlikely. Nevertheless, we could have defined a hold-out set to visualise the differences to have a very strong confidence on our results. ","127f8810":"# Prediction Model\n> Prediction Model (feature selection, model selection, model parameter optimization, evaluation metrics\n\nWe will use LightGBM. LightGBM is fast and powerful, and is the basis of many solution online.\n\nWe first define transform the dataset LightGBM compatible format. Some columns are casted categorical as recognised previously.","462e4bdf":"Part II \u2013 Predictive Data Modelling (60 points)\n\nGiven the Housing Price Train dataset (Housing Price Prediction_Train.csv), develop a model to predict the price in the test dataset (Housing Price Prediction_Test.csv), and present your results in Housing Price Prediction_Result.csv. \n\nYou will be evaluated in terms of your\n\n|Criteria | Description | Points\n-|-|-\n|Data Engineering Approach | how to deal with missing value, categorical data, data skewness | 24 points\n|Prediction Model | feature selection, model selection, model parameter optimization, evaluation metrics | 24 points\n|Result Presentation | | 12 points\n\n\nUpload your Housing Price Prediction_Result.csv together with your data processing document (ppt or excel or jupyter notebook) in a zip file for evaluation"}}