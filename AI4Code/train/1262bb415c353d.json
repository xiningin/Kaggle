{"cell_type":{"9dd5ddb9":"code","1d5cd90f":"code","d6b3bb52":"code","ce526a7c":"code","41107439":"code","78f03015":"code","b6a64925":"code","88df7f3f":"code","ba0f4287":"code","cf7cbde0":"code","ec2fdb6f":"code","3d571e37":"code","0ea2aa44":"code","67a23eb3":"code","668522a1":"code","f40caaa4":"code","ccdf4771":"code","d1189f7c":"code","5860503e":"markdown"},"source":{"9dd5ddb9":"import numpy as np\nimport pandas as pd\nimport spacy\nfrom keras.preprocessing.sequence import pad_sequences\nimport os\nfrom tqdm import tqdm\nimport torch\nprint(os.listdir('..\/input\/bert-score-layer-lb-0-475'))\nprint(os.listdir('..\/input\/gap-coreference'))","1d5cd90f":"!conda remove -y greenlet\n!pip install pytorch-pretrained-bert\n!pip install allennlp","d6b3bb52":"from allennlp.modules.span_extractors import EndpointSpanExtractor \nfrom pytorch_pretrained_bert import BertTokenizer, BertModel\nfrom spacy.lang.en import English\n\nnlp = English()\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nsentencizer = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sentencizer)","ce526a7c":"def candidate_length(candidate):\n    #count the word length without space\n    count = 0\n    for i in range(len(candidate)):\n        if candidate[i] !=  \" \": count += 1\n    return count\n\ndef count_char(text, offset):\n    count = 0\n    for pos in range(offset):\n        if text[pos] != \" \": count +=1\n    return count\n\ndef count_token_length_special(token):\n    count = 0\n    special_token = [\"#\", \" \"]\n    for i in range(len(token)):\n        if token[i] not in special_token: \n            count+=1\n    return count\n\ndef find_word_index(tokenized_text, char_start, target):\n    tar_len = candidate_length(target)\n    char_count = 0\n    word_index = []\n    special_token = [\"[CLS]\", \"[SEP]\"]\n    for i in range(len(tokenized_text)):\n        token = tokenized_text[i]\n        if char_count in range(char_start, char_start+tar_len):\n            if token in special_token: # for the case like \"[SEP]. she\"\n                continue\n            word_index.append(i)\n        if token not in special_token:\n            token_length = count_token_length_special(token)\n            char_count += token_length\n    \n    if len(word_index) == 1:\n        return [word_index[0], word_index[0]] #the output will be start index of span, and end index of span\n    else:\n        return [word_index[0], word_index[-1]]\n\ndef create_tokenizer_input(sents):\n    tokenizer_input = str()\n    for i, sent in enumerate(sents):\n        if i == 0:\n            tokenizer_input += \"[CLS] \"+sent.text+\" [SEP] \"\n        elif i == len(sents) - 1:\n            tokenizer_input += sent.text+\" [SEP]\"\n        else:\n            tokenizer_input += sent.text+\" [SEP] \"\n            \n    return  tokenizer_input\n\ndef create_inputs(dataframe):\n    \n    idxs = dataframe.index\n    columns = ['indexed_token', 'offset']\n    features_df = pd.DataFrame(index=idxs, columns=columns)\n    max_len = 0\n    for i in tqdm(range(len(dataframe))):\n        text           = dataframe.loc[i, 'Text']\n        Pronoun_offset = dataframe.loc[i, 'Pronoun-offset']\n        A_offset       = dataframe.loc[i, \"A-offset\"]\n        B_offset       = dataframe.loc[i, \"B-offset\"]\n        Pronoun        = dataframe.loc[i, \"Pronoun\"]\n        A              = dataframe.loc[i, \"A\"]\n        B              = dataframe.loc[i, \"B\"]\n        doc            = nlp(text)\n        \n        sents = []\n        for sent in doc.sents: sents.append(sent)\n        token_input = create_tokenizer_input(sents)\n        token_input = token_input.replace(\"#\", \"*\") #Remove special symbols \u201c#\u201d from the original sentence\n        tokenized_text = tokenizer.tokenize(token_input) #the token text\n        if len(tokenized_text) > max_len: \n            max_len = len(tokenized_text) \n        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) #token text to index\n        \n        A_char_start, B_char_start = count_char(text, A_offset), count_char(text, B_offset)\n        Pronoun_char_start         = count_char(text, Pronoun_offset)\n        \n        word_indexes = [] #\n        for char_start, target in zip([A_char_start, B_char_start, Pronoun_char_start], [A, B, Pronoun]):\n            word_indexes.append(find_word_index(tokenized_text, char_start, target))#\n        features_df.iloc[i] = [indexed_tokens, word_indexes]\n        \n    print('max length of sentence:', max_len)\n    \n    return features_df","41107439":"train_df = pd.read_table('..\/input\/gap-coreference\/gap-test.tsv')\ntest_df  = pd.read_table('..\/input\/gap-coreference\/gap-development.tsv')\nval_df   = pd.read_table('..\/input\/gap-coreference\/gap-validation.tsv')\nnew_train_df = create_inputs(train_df)\nnew_test_df  = create_inputs(test_df)\nnew_val_df   = create_inputs(val_df)","78f03015":"def get_label(dataframe):\n    labels = []\n    for i in range(len(dataframe)):\n        if dataframe.loc[i, 'A-coref']:\n            labels.append(0)\n        elif dataframe.loc[i, 'B-coref']:\n            labels.append(1)\n        else:\n            labels.append(2)\n            \n    return labels\n\nnew_train_df['label'] = get_label(train_df) # Add label columns\nnew_val_df['label']   = get_label(val_df)\nnew_df = pd.concat([new_train_df, new_val_df]) # combine train_df with val_df for the Kfold input \nnew_df = new_df.reset_index(drop=True)\nnew_df.to_csv('train.csv', index=False)\nnew_test_df['label'] = get_label(test_df)\nnew_test_df.to_csv('test.csv', index=False)","b6a64925":"del new_df\ndel new_val_df\ndel new_test_df\ndel new_train_df","88df7f3f":"import gc\ngc.collect()","ba0f4287":"from torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom ast import literal_eval\nimport torch.nn.functional as F\n\nclass MyDataset(Dataset):\n    \n    def __init__(self, dataframe, transform=None):\n        self.df = dataframe\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        \n        index_token = self.df.loc[idx, 'indexed_token']\n        index_token = literal_eval(index_token) # Change string to list\n        index_token = pad_sequences([index_token], maxlen=360, padding='post')[0] #pad \n        \n        offset = self.df.loc[idx, 'offset']\n        offset = literal_eval(offset)\n        offset = np.asarray(offset, dtype='int32')\n        label  = int(self.df.loc[idx, 'label'])\n        \n        distP_A = self.df.loc[idx, 'D_PA']\n        distP_B = self.df.loc[idx, 'D_PB']\n        \n        if self.transform:\n            index_token = self.transform(index_token)\n            offset = self.transform(offset)\n            label = self.transform(label)\n        \n        return (index_token, offset, distP_A, distP_B), label","cf7cbde0":"class score(torch.nn.Module):\n    \n    def __init__(self, embed_dim, hidden_dim):\n        super(score, self).__init__()\n        self.score = torch.nn.Sequential(\n                     torch.nn.Linear(embed_dim, hidden_dim),\n                     torch.nn.ReLU(inplace=True),\n                     torch.nn.Dropout(0.6),\n                     torch.nn.Linear(hidden_dim, 1))\n        \n    def forward(self, x):\n        return self.score(x)\n    \nclass mentionpair_score(torch.nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim):\n        super(mentionpair_score, self).__init__()\n        self.score = score(input_dim, hidden_dim)\n    \n    def forward(self, g1, g2, dist_embed):\n        \n        element_wise = g1 * g2\n        pair_score   = self.score(torch.cat((g1, g2, element_wise, dist_embed), dim=-1)) \n        \n        return pair_score\n\nclass score_model(torch.nn.Module):\n    \n    def __init__(self):\n        super(score_model, self).__init__()\n        self.buckets        = [1, 2, 3, 4, 5, 8, 16, 32, 64] \n        self.bert           = BertModel.from_pretrained('bert-base-uncased')\n        self.embedding      = torch.nn.Embedding(len(self.buckets)+1, 20)\n        self.span_extractor = EndpointSpanExtractor(768, \"x,y,x*y\")\n        self.pair_score     = mentionpair_score(2304*3+20, 150)\n        \n    def forward(self, sent, offsets, distP_A, distP_B):\n        \n        bert_output, _   = self.bert(sent, output_all_encoded_layers=False) # (batch_size, max_len, 768)\n        #Distance Embeddings\n        distPA_embed     = self.embedding(distP_A)\n        distPB_embed     = self.embedding(distP_B)\n        \n        #Span Representation\n        span_repres     = self.span_extractor(bert_output, offsets) #(batch, 3, 2304)\n        span_repres     = torch.unbind(span_repres, dim=1) #[A: (bath, 2304), B: (bath, 2304), Pronoun:  (bath, 2304)]\n        span_norm = []\n        for i in range(len(span_repres)): \n            span_norm.append(F.normalize(span_repres[i], p=2, dim=1)) #normalizes the words embeddings\n    \n        ap_score = self.pair_score(span_norm[2], span_norm[0], distPA_embed)\n        bp_score = self.pair_score(span_norm[2], span_norm[1], distPB_embed)\n        nan_score = torch.zeros_like(ap_score)\n        output = torch.cat((ap_score, bp_score, nan_score), dim=1)\n        \n        return output","ec2fdb6f":"# The Code from https:\/\/www.kaggle.com\/ceshine\/pytorch-bert-endpointspanextractor-kfold\n\ndef children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, torch.nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\n            \ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))","3d571e37":"#the distance features(distance between two word) are binned into the following buckets\n#[1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+]\n#D_PA is the distance of A and Pronoun\n#D_PB is the distance of B and Pronoun\n#You can check: https:\/\/aclweb.org\/anthology\/D17-1018\n\ntrain_dist = pd.read_csv('..\/input\/bert-score-layer-lb-0-475\/train_dist_df.csv')\nval_dist   = pd.read_csv('..\/input\/bert-score-layer-lb-0-475\/val_dist_df.csv')\ntest_dist  = pd.read_csv('..\/input\/bert-score-layer-lb-0-475\/test_dist_df.csv')\n\ntrain_dist = pd.concat([train_dist, val_dist])\ntrain_dist = train_dist.reset_index(drop=True)\ntrain_dist.head()","0ea2aa44":"from sklearn.model_selection import StratifiedKFold\nn_split = 5\n\ntrain = pd.read_csv('..\/working\/train.csv')\ntest  = pd.read_csv('..\/working\/test.csv')\n\ntrain = pd.concat([train, train_dist], axis=1)\ntest  = pd.concat([test, test_dist], axis=1)\ntrain.head()\nKfold = StratifiedKFold(n_splits=n_split, random_state=2019).split(train, train['label'])","67a23eb3":"import time\n\ndef softmax(x):\n    exp_x = np.exp(x)\n    y = exp_x \/ np.sum(exp_x, axis=1, keepdims=True)\n    return y\n\noutput = np.zeros((len(test_df), 3))\ntestset = MyDataset(test)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=20) #data loader for test dataset\n\nn_epochs = 30\n#Use Kfold to get robusted score\nfor n_fold, (train_index, val_index) in enumerate(Kfold):\n    min_val_loss = 100.0 # for save best model\n    PATH = \".\/best_model_{}.hdf5\".format(n_fold+1)\n    \n    train_df = train.loc[train_index]\n    train_df = train_df.reset_index(drop=True)\n    val_df   = train.loc[val_index]\n    val_df   = val_df.reset_index(drop=True)\n    \n    trainset = MyDataset(train_df)\n    train_loader = torch.utils.data.DataLoader(trainset, batch_size=20, shuffle=True)\n    valset = MyDataset(val_df)\n    val_loader = torch.utils.data.DataLoader(valset, batch_size=20, shuffle=True)\n    \n    model = score_model()\n    #freeze bert\n    set_trainable(model.bert, False)\n    set_trainable(model.embedding, True) \n    set_trainable(model.pair_score, True)\n    model.cuda() #\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001) \n    criterion = torch.nn.CrossEntropyLoss().cuda()\n    \n    print('fold:', n_fold+1)\n    for i in range(n_epochs):\n        #Start training\n        start_time = time.time()\n        model.train() \n        avg_loss = 0.\n        for idx, (inputs, label) in enumerate(train_loader):\n            index_token, offset, distP_A, distP_B = inputs\n            index_token = index_token.type(torch.LongTensor).cuda() #change IntTensor to LongTensor,\n            offset      = offset.type(torch.LongTensor).cuda()\n            label       = label.type(torch.LongTensor).cuda()\n            distP_A     = distP_A.type(torch.LongTensor).cuda()\n            distP_B     = distP_B.type(torch.LongTensor).cuda()\n            \n            optimizer.zero_grad()\n            output_train = model(index_token, offset, distP_A, distP_B)\n            loss = criterion(output_train, label)\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item() \/ len(train_loader)\n            \n        avg_val_loss = 0.\n        #Start test\n        model.eval()\n        with torch.no_grad():\n            for idx, (inputs, label) in enumerate(val_loader):\n                index_token, offset, distP_A, distP_B = inputs\n                index_token = index_token.type(torch.LongTensor).cuda()\n                offset      = offset.type(torch.LongTensor).cuda()\n                label       = label.type(torch.LongTensor).cuda()\n                distP_A     = distP_A.type(torch.LongTensor).cuda()\n                distP_B     = distP_B.type(torch.LongTensor).cuda()\n                \n                output_test =  model(index_token, offset, distP_A, distP_B)\n                avg_val_loss += criterion(output_test, label).item() \/ len(val_loader)\n                \n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n                i + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n        # save best model\n        if min_val_loss > avg_val_loss:\n            min_val_loss = avg_val_loss \n            torch.save(model.state_dict(), PATH)\n        \n    \n    del model\n    \n    model = score_model()\n    model.load_state_dict(torch.load(PATH)) #load best model to predict\n    model.cuda()\n    model.eval()\n    with torch.no_grad():\n        for idx, (inputs, label) in enumerate(test_loader):\n            index_token, offset, distP_A, distP_B = inputs\n            index_token = index_token.type(torch.LongTensor).cuda()\n            offset      = offset.type(torch.LongTensor).cuda()\n            label       = label.type(torch.LongTensor).cuda()\n            distP_A     = distP_A.type(torch.LongTensor).cuda()\n            distP_B     = distP_B.type(torch.LongTensor).cuda()\n                \n            y_pred = model(index_token, offset, distP_A, distP_B)\n            y_pred = softmax(y_pred.cpu().numpy())\n            start = idx * 20\n            end = start + 20\n            output[start:end, :] += y_pred                ","668522a1":"import os\noutput \/= 5 \nsub_df_path = os.path.join('..\/input\/gendered-pronoun-resolution\/', 'sample_submission_stage_1.csv')\nsub_df = pd.read_csv(sub_df_path)\nsub_df.loc[:, 'A'] = pd.Series(output[:, 0])\nsub_df.loc[:, 'B'] = pd.Series(output[:, 1])\nsub_df.loc[:, 'NEITHER'] = pd.Series(output[:, 2])\n\nsub_df.head(20)","f40caaa4":"sub_df.to_csv(\"submission.csv\", index=False)","ccdf4771":"y_test = pd.read_csv('..\/working\/test.csv')['label']\n\nfrom sklearn.metrics import log_loss\ny_one_hot = np.zeros((2000, 3))\nfor i in range(len(y_test)):\n    y_one_hot[i, y_test[i]] = 1\nlog_loss(y_one_hot, output)\n","d1189f7c":"_output = np.argmax(output, axis=1)\nprint('acc:', np.asarray(np.where(_output == y_test)).shape[1]\/ 2000)","5860503e":"\nThe basic idea is from my kernel (https:\/\/www.kaggle.com\/chanhu\/bert-score-layer-lb-0-475).\nIn this kernel, I had changed several points below.\n* keras -> pytorch(this is my second kernel wrote in pytorch)\n* use pretrain Bert, EndpointSpanExtractor, and weight decay.\n(similar to Lee's work https:\/\/www.kaggle.com\/ceshine\/pytorch-bert-endpointspanextractor-kfold) \n* use kfold to get a robust score.(according to the comment from Matei Ionita, and huiqin. Thanks!)\n\nP.S: the best I can get is 0.486. "}}