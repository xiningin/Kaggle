{"cell_type":{"ae9d2909":"code","27f83f33":"code","10f59126":"code","1380068d":"code","5f06ab3a":"code","df38edd5":"code","3b042177":"code","41f54965":"code","d71fd7c4":"code","0f663e82":"code","67c3a164":"code","ced49aaa":"code","84f83e71":"code","9523682e":"code","94eaf032":"code","c3de5485":"code","643ee3a7":"code","07a5bb04":"code","fe63575f":"code","99162cbb":"code","5ab02888":"markdown","30189643":"markdown","432acaba":"markdown","ca693d98":"markdown","e6b44d0a":"markdown","6bb914c4":"markdown","b630520a":"markdown","8ba87742":"markdown","2fd39437":"markdown","ea86151e":"markdown","04c9d35c":"markdown","010f8be0":"markdown","d449af3a":"markdown","0c1837ba":"markdown","59408d61":"markdown"},"source":{"ae9d2909":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\n\nfrom datetime import date, datetime\nimport time\nimport calendar\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","27f83f33":"path = '..\/input\/store-sales-time-series-forecasting\/'\n\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsub = pd.read_csv(path + 'sample_submission.csv')\n\noil = pd.read_csv(path + 'oil.csv')\nholiday = pd.read_csv(path + 'holidays_events.csv')\nstore = pd.read_csv(path + 'stores.csv')\ntran = pd.read_csv(path + 'transactions.csv')","10f59126":"def preprocess_train(df):\n    df['date'] = df['date'].map(lambda x: date.fromisoformat(x))\n    df['weekday'] = df['date'].map(lambda x: x.weekday())\n    df['year'] = df['date'].map(lambda x: x.year)\n    df['month'] = df['date'].map(lambda x: x.month)\n    df['day'] = df['date'].map(lambda x: x.day)\n    df['eomd'] = df['date'].map(lambda x: calendar.monthrange(x.year, x.month)[1])\n    df['payday'] = ((df['day'] == df['eomd'])|(df['day'] == 15)).astype(int)\n    df.drop(['id', 'eomd'], axis=1, inplace=True)\n    return df\n\ntrain = preprocess_train(train)\ntest = preprocess_train(test)","1380068d":"def preprocess_oil(oil):\n    oil['month'] = oil['date'].map(lambda x: int(x.replace('-', '')[:6]))\n    oil['month_avg'] = oil.groupby('month')['dcoilwtico'].transform('mean')\n    oil['tmp'] = oil['dcoilwtico'].map(np.isnan)\n    oil['month_avg'] = oil['tmp'] * oil['month_avg']\n    oil['dcoilwtico'].fillna(0, inplace=True)\n    oil['dcoilwtico'] = oil['dcoilwtico'] + oil['month_avg']\n    oil = oil.drop(['month', 'month_avg', 'tmp'], axis=1)\n    oil['date'] = oil['date'].map(lambda x: date.fromisoformat(x))\n    return oil\n\noil = preprocess_oil(oil)","5f06ab3a":"def preprocess_holiday(df):\n    df['date'] = df['date'].map(lambda x: date.fromisoformat(x))\n    df = df[(df['transferred']==False)&(df['type']!='Work Day')]\n    event = df[df['type']=='Event']\n    earthquake = event[event['description'].str.startswith('Terremoto Manabi')]\n    event = event[event['description'].str.startswith('Terremoto Manabi')==False]\n    return df, event, earthquake\n\nholiday, event, earthquake = preprocess_holiday(holiday)\n\nevent = event[['date', 'description']]\nevent.rename({'description': 'event_name'}, axis=1, inplace=True)\n\nearthquake = earthquake[['date', 'description']]\nearthquake.rename({'description': 'earthquake'}, axis=1, inplace=True)\n\nh_local = holiday[holiday['locale']=='Local']\nh_local = h_local[['date', 'locale_name', 'description']]\nh_local = h_local.rename({'locale_name': 'city', 'description': 'local_holiday_name'}, axis=1)\n\nh_regional = holiday[holiday['locale']=='Regional']\nh_regional = h_regional[['date', 'locale_name', 'description']]\nh_regional = h_regional.rename({'locale_name': 'state', 'description': 'regional_holiday_name'}, axis=1)\n\nh_national = holiday[holiday['locale']=='National']\nh_national = h_national[['date', 'description']]\nh_national = h_national.rename({'description': 'national_holiday_name'}, axis=1)","df38edd5":"def merge_tables(df):\n    df = df.merge(oil, on='date', how='left')\n    df = df.merge(store, on='store_nbr', how='left')\n    df = df.merge(event, on='date', how='left').fillna('0')\n    df = df.merge(earthquake, on='date', how='left').fillna('0')\n    df = df.merge(h_local, on=['date', 'city'], how='left').fillna('0')\n    df = df.merge(h_regional, on=['date', 'state'], how='left').fillna('0')\n    df = df.merge(h_national, on='date', how='left').fillna('0')\n    df = df.merge(tran, on=['date', 'store_nbr'], how='left').fillna(0)\n\n    return df\n\ntrain = merge_tables(train)\ntest = merge_tables(test)","3b042177":"train['dcoilwtico'] = train['dcoilwtico'].astype(float)\ntest['dcoilwtico'] = test['dcoilwtico'].astype(float)","41f54965":"# train = pd.get_dummies(train, columns=['family', 'store_nbr', 'city', 'state', 'type', 'cluster'])\n# test = pd.get_dummies(test, columns=['family', 'store_nbr', 'city', 'state', 'type', 'cluster'])","d71fd7c4":"cat_features = ['family', 'store_nbr', 'city', 'state', 'type', 'cluster',\n                'event_name', 'earthquake', 'local_holiday_name', 'regional_holiday_name', 'national_holiday_name']\nfor col in cat_features:\n    le = LabelEncoder()\n    train[col] = le.fit_transform(train[col])\n    test[col] = le.transform(test[col])","0f663e82":"def preprocess_dataset(df, train_date: list, valid_date: list):\n    df['is_train'] = df['date'].map(lambda x: x in train_date)\n    df['is_valid'] = df['date'].map(lambda x: x in valid_date)\n    return df\n\n\ntrain_date = train['date'].unique()[-227:-31].tolist()\nvalid_date = train['date'].unique()[-31:].tolist()\ntrain = preprocess_dataset(train, train_date, valid_date)","67c3a164":"print('train date from {} to {}'.format(min(train_date), max(train_date)))\nprint('valid date from {} to {}'.format(min(valid_date), max(valid_date)))","ced49aaa":"y = np.log(train['sales'] + 1)\nX_train = train.drop(['date', 'sales', 'year'], axis=1)\nX_test = test.drop(['date', 'year'], axis=1)","84f83e71":"xgb_params = {\n    'tree_method': 'gpu_hist', \n    'gpu_id': 0,\n    'predictor': 'gpu_predictor', \n    'verbosity': 2,\n    'objective': 'reg:squarederror', \n    'eval_metric': 'rmse', \n    'random_state': 2021,\n    'learning_rate': 0.009948916127719946,\n    'subsample': 0.9963593946651406,\n    'colsample_bytree': 0.8056779523100791,\n    'reg_alpha': 10.0,\n    'reg_lambda': 0.1801543144548864,\n    'min_child_weight': 47,\n}","9523682e":"def basic_xgboost(X_train, y, xgb_params, X_test):\n    start = time.time()    \n    # extract train and valid dataset\n    trn_idx = X_train[X_train['is_train']==True].index.tolist()\n    val_idx = X_train[X_train['is_valid']==True].index.tolist()\n\n    X_tr = X_train.loc[trn_idx, :].drop(['is_train', 'is_valid'], axis=1)\n    X_val = X_train.loc[val_idx, :].drop(['is_train', 'is_valid'], axis=1)\n    y_tr = y[trn_idx]\n    y_val = y[val_idx]\n    \n    xgb_train = xgb.DMatrix(X_tr, label=y_tr)\n    xgb_valid = xgb.DMatrix(X_val, label=y_val)\n    evallist = [(xgb_train, 'train'), (xgb_valid, 'eval')]\n    evals_result = dict()\n    \n    model = xgb.train(params=xgb_params, dtrain=xgb_train, evals=evallist, evals_result=evals_result,\n                      verbose_eval=5000, num_boost_round=100000, early_stopping_rounds=100)\n    \n    \n    \n    xgb_oof = np.zeros(y_val.shape[0])\n    xgb_oof = model.predict(xgb_valid, iteration_range=(0, model.best_iteration))\n    \n    xgb_test = xgb.DMatrix(X_test)\n    xgb_pred = pd.Series(model.predict(xgb_test, iteration_range=(0, model.best_iteration)),\n                         name='xgb_pred')\n    \n    elapsed = time.time() - start\n    error_value = mean_squared_error(y_val, xgb_oof, squared=False)\n    print(f\"xgb rmse: {error_value:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\n    return xgb_oof, model, evals_result, xgb_pred, y_val, X_val","94eaf032":"%%time\noof, model, evals_result, pred, y_val, X_val = basic_xgboost(X_train, y, xgb_params, X_test)","c3de5485":"df_error = X_val[['store_nbr', 'family']].copy()\ndf_error.reset_index(drop=True, inplace=True)\ndf_error['oof'] = pd.Series(oof)\ndf_error['y_valid'] = y_val.reset_index(drop=True).copy()","643ee3a7":"# good\ny_oof = df_error[(df_error['store_nbr']==1)&(df_error['family']==12)]['oof'].tolist()\ny_val = df_error[(df_error['store_nbr']==1)&(df_error['family']==12)]['y_valid'].tolist()\nsns.lineplot(x=range(len(y_oof)), y=y_oof)\nsns.lineplot(x=range(len(y_oof)), y=y_val)","07a5bb04":"# need to improve\ny_oof = df_error[(df_error['store_nbr']==1)&(df_error['family']==14)]['oof'].tolist()\ny_val = df_error[(df_error['store_nbr']==1)&(df_error['family']==14)]['y_valid'].tolist()\nsns.lineplot(x=range(len(y_oof)), y=y_oof)\nsns.lineplot(x=range(len(y_oof)), y=y_val)","fe63575f":"sub['sales'] = np.exp(pred.map(lambda x: max(x, 0))) - 1\nsub.to_csv('submission.csv', index=False)","99162cbb":"# feature importance\nfi = pd.DataFrame()\nfi['feature'] = model.get_fscore().keys()\nfi['importance'] = model.get_fscore().values()\ndisplay(fi.sort_values('importance', ascending=False))","5ab02888":"## Label encoding","30189643":"* fill in NA values with month average oil price","432acaba":"## Run XGB","ca693d98":"* add weekday, year, month, day and payday","e6b44d0a":"## Print feature importance","6bb914c4":"## Set train period and validation period","b630520a":"## Load dataset","8ba87742":"* separate into three holiday types (national, regional and local) and event (FIFA World Cup etc)","2fd39437":"## Set XGB params","ea86151e":"## Preprocess datasets","04c9d35c":"## Set X and y","010f8be0":"## Merge datasets","d449af3a":"## Create file for submission","0c1837ba":"## Plot error","59408d61":"## Import packages"}}