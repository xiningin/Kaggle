{"cell_type":{"e9562917":"code","704b7559":"code","ead21822":"code","143db9ba":"code","e881e868":"code","f6607210":"code","c42321ef":"code","fccce5a4":"code","c06097d0":"code","6cdf7cf4":"code","cd176fc0":"code","72ec448b":"code","63352189":"code","ce4c3b52":"code","3e223e6a":"code","789a8289":"code","e8a00376":"code","e427ba73":"code","172f6c8d":"code","c54f587c":"code","b99d209b":"code","52cad70b":"code","b27f2676":"code","0445b9de":"code","568c294c":"code","110816d3":"code","fe04d512":"code","8e6f4538":"code","d6aa1cb8":"code","504377bf":"code","e9d4e02e":"code","c5371cec":"code","c0e26964":"code","44620324":"code","804866df":"code","4f8ae7d5":"code","1a9e94a5":"code","dee171a3":"code","8a4b0caa":"code","6a74e47b":"code","6a4f315d":"markdown","08fd2261":"markdown","1d65771b":"markdown","02e835ab":"markdown","e8131433":"markdown"},"source":{"e9562917":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport itertools\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\nnp.random.seed(2)\nimport warnings\nwarnings.filterwarnings(\"ignore\") ","704b7559":"# importing dataset\ndf = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","ead21822":"print(\"Rows: \",df.shape[0])\nprint(\"Columns: \",df.shape[1])","143db9ba":"# copy datatset\ndata = df.copy()","e881e868":"Fraud = data.loc[data['Class']==1]\nValid = data.loc[data['Class']==0]\nfraction = len(Fraud)\/(len(Fraud)+len(Valid))\nprint(\"Valid: \",len(Valid))\nprint(\"Fraud: \",len(Fraud))\nprint(\"fraction of fraud: \",fraction)","f6607210":"counts = data.Class.value_counts()\nplt.figure(figsize=(8,6))\nsns.barplot(x=counts.index, y=counts)\nplt.title('Count of Fraudulent vs. Non-Fraudulent Transactions In Subsample')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')\nplt.show()","c42321ef":"features = data.iloc[:,0:29].columns\nplt.figure(figsize=(15,29*4))\ngs = gridspec.GridSpec(29, 1)\nfor i, c in enumerate(data[features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(data[c][data.Class == 1], bins=50,label='Fraud')\n    sns.distplot(data[c][data.Class == 0], bins=50,label=\"Valid\")\n    ax.legend()\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(c))\nplt.show()","fccce5a4":"scaler = StandardScaler()\nscaler2 = StandardScaler()\n\n#scaling time\nscaled_time = scaler.fit_transform(data[['Time']])\nflat_list1 = [item for sublist in scaled_time.tolist() for item in sublist]\nscaled_time = pd.Series(flat_list1)\n\n#scaling the amount column\nscaled_amount = scaler2.fit_transform(data[['Amount']])\nflat_list2 = [item for sublist in scaled_amount.tolist() for item in sublist]\nscaled_amount = pd.Series(flat_list2)","c06097d0":"#concatenating newly created columns w original df\ndata = pd.concat([data, scaled_amount.rename('scaled_amount'), scaled_time.rename('scaled_time')], axis=1)\ndata.sample(5)","6cdf7cf4":"data.drop(['Amount', 'Time'], axis=1, inplace=True)","cd176fc0":"# Splitting the data into training and test sets\n#manual train test split using numpy's random.rand\nmask = np.random.rand(len(data)) < 0.9\ntrain = data[mask]\ntest = data[~mask]\n\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\n\nprint('Train Shape: {}\\nTest Shape: {}'.format(train.shape, test.shape))","72ec448b":"# number of fraudulent transactions in train data\nn_of_frauds = train.Class.value_counts()[1]\nprint('There are {} fraudulent transactions in the train data.'.format(n_of_frauds))","63352189":"#randomly selecting 446 random non-fraudulent transactions\nnon_fraud = train.loc[train['Class'] == 0]\nfraud = train.loc[train['Class'] == 1]\n\nselected = non_fraud.sample(n_of_frauds)\nselected.head()","ce4c3b52":"#concatenating both into a subsample data set with equal class distribution\nselected.reset_index(drop=True, inplace=True)\nfraud.reset_index(drop=True, inplace=True)\nsubsample = pd.concat([selected, fraud])\n\n#shuffling our data set\nsubsample = subsample.sample(frac=1).reset_index(drop=True)\nprint(\"Rows: \",len(subsample))\nsubsample.head(10)","3e223e6a":"new_counts = subsample.Class.value_counts()\nplt.figure(figsize=(8,6))\nsns.barplot(x=new_counts.index, y=new_counts)\nplt.title('Count of Fraudulent vs. Non-Fraudulent Transactions In Subsample')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')\nplt.show()","789a8289":"#taking a look at correlations\ncorr = subsample.corr()\ncorr = corr[['Class']]\ncorr","e8a00376":"#negative correlations smaller than -0.5\ncorr[corr.Class < -0.5]","e427ba73":"#visualizing the features with high negative correlation\nf, axes = plt.subplots(nrows=2, ncols=4, figsize=(26,16))\n\nf.suptitle('Features With High Negative Correlation', size=35)\nsns.boxplot(x=\"Class\", y=\"V3\", data=subsample, ax=axes[0,0])\nsns.boxplot(x=\"Class\", y=\"V9\", data=subsample, ax=axes[0,1])\nsns.boxplot(x=\"Class\", y=\"V10\", data=subsample, ax=axes[0,2])\nsns.boxplot(x=\"Class\", y=\"V12\", data=subsample, ax=axes[0,3])\nsns.boxplot(x=\"Class\", y=\"V14\", data=subsample, ax=axes[1,0])\nsns.boxplot(x=\"Class\", y=\"V16\", data=subsample, ax=axes[1,1])\nsns.boxplot(x=\"Class\", y=\"V17\", data=subsample, ax=axes[1,2])\nf.delaxes(axes[1,3])","172f6c8d":"#positive correlations greater than 0.5\ncorr[corr.Class > 0.5]","c54f587c":"#visualizing the features with high positive correlation\nf, axes = plt.subplots(nrows=1, ncols=2, figsize=(14,6))\n\nf.suptitle('Features With High Positive Correlation', size=20)\nsns.boxplot(x=\"Class\", y=\"V4\", data=subsample, ax=axes[0])\nsns.boxplot(x=\"Class\", y=\"V11\", data=subsample, ax=axes[1])","b99d209b":"#Only removing extreme outliers\nQ1 = subsample.quantile(0.25)\nQ3 = subsample.quantile(0.75)\nIQR = Q3 - Q1\n\ndf2 = subsample[~((subsample < (Q1 - 2.5 * IQR)) |(subsample > (Q3 + 2.5 * IQR))).any(axis=1)]","52cad70b":"len_after = len(df2)\nlen_before = len(subsample)\nlen_difference = len(subsample) - len(df2)\nprint('We reduced our data size from {} transactions by {} transactions to {} transactions.'.format(len_before, len_difference, len_after))","b27f2676":"X = df2.drop('Class', axis=1)\ny = df2['Class']\n\n#Train-test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train = X_train.values\nX_validation = X_test.values\ny_train = y_train.values\ny_validation = y_test.values\nprint('X_shapes:\\n', 'X_train:', 'X_validation:\\n', X_train.shape, X_validation.shape, '\\n')\nprint('Y_shapes:\\n', 'Y_train:', 'Y_validation:\\n', y_train.shape, y_validation.shape)","0445b9de":"# Training and testing different models\nmodels = []\n\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('XGB', XGBClassifier(eval_metric='logloss')))\nmodels.append(('RF', RandomForestClassifier()))\n\n#testing models\n\nresults = []\nnames = []\n\nprint(\"Area Under Curve - ROC\")\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=42, shuffle=True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='roc_auc')\n    results.append(cv_results)\n    names.append(name)\n    msg = '%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())\n    print(msg)","568c294c":"# best model\nmodel = XGBClassifier(n_estimators=10,eval_metric='logloss')\n\n# Train\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_validation)","110816d3":"#Testing\ny_expect = pd.DataFrame(y_validation)\ncm = confusion_matrix(y_expect, y_pred.round())\nprint(\"XGBClassifier Model Evaluation for training data \\n\")\nprint(\"Confusion Matrix\")\nprint(cm)\nprint(\"Accuracy: \",accuracy_score(y_test, y_pred.round()))\nprint(\"Precision: \",precision_score(y_test, y_pred.round()))\nprint(\"Recall: \",recall_score(y_test, y_pred.round()))\nprint(\"F1_Score: \",f1_score(y_test, y_pred.round()))","fe04d512":"n_of_frauds_t = test.Class.value_counts()[1]\n#randomly selecting 46 random non-fraudulent transactions\nnon_fraudt = test[test['Class'] == 0]\nfraudt = test[test['Class'] == 1]\nselectedt = non_fraudt.sample(n_of_frauds_t)\nselectedt.reset_index(drop=True, inplace=True)\nfraudt.reset_index(drop=True, inplace=True)\nsubsamplet = pd.concat([selectedt, fraudt])\nsubsamplet = subsamplet.sample(frac=1).reset_index(drop=True)\n\n#Only removing extreme outliers\nQ1 = subsamplet.quantile(0.25)\nQ3 = subsamplet.quantile(0.75)\nIQR = Q3 - Q1\n\ndf3 = subsamplet[~((subsamplet < (Q1 - 2.5 * IQR)) |(subsamplet > (Q3 + 2.5 * IQR))).any(axis=1)]\nlen_aftert = len(df3)\nlen_beforet = len(subsamplet)\nlen_differencet = len(subsamplet) - len(df3)\nprint('We reduced our data size from {} transactions by {} transactions to {} transactions.'.format(len_beforet, len_differencet, len_aftert))","8e6f4538":"X_t = df3.drop('Class', axis=1)\ny_t = df3['Class']\nyt_pred = model.predict(X_t)\nyt_expect = pd.DataFrame(y_t)\ncmt = confusion_matrix(yt_expect, yt_pred.round())\nprint(\"XGBClassifier Model Evaluation for testing data \\n\")\nprint(\"Confusion Matrix\")\nprint(cmt)\nprint(\"Accuracy: \",accuracy_score(y_t, yt_pred.round()))\nprint(\"Precision: \",precision_score(y_t, yt_pred.round()))\nprint(\"Recall: \",recall_score(y_t, yt_pred.round()))\nprint(\"F1_Score: \",f1_score(y_t, yt_pred.round()))","d6aa1cb8":"# Create deep learning model\nmodel = Sequential()\n#add input layer\nmodel.add(Dense(input_dim = 29, units = 16, activation = 'relu'))\n#add 2nd hidden layer\nmodel.add(Dense(units = 24, activation = 'relu'))\n#add dropout layer\nmodel.add(Dropout(0.5))\n#add 3rd hidden layer\nmodel.add(Dense(units = 20, activation = 'relu'))\n#add 4th hidden layer\nmodel.add(Dense(units = 24, activation = 'relu'))\n#add ouptut layer\nmodel.add(Dense(units = 1, activation = 'sigmoid'))\n\nmodel.summary()","504377bf":"# scaling amount and time\nsc = StandardScaler()\ndf['scaled_amount'] = sc.fit_transform(df['Amount'].values.reshape(-1, 1))\n#split data\ndf = df.drop(['Amount', 'Time'], axis = 1)\ny2 = df['Class']\nX2 = df.drop(['Class'], axis = 1)","e9d4e02e":"X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size = 0.3, random_state = 0)\n\n#convert data for fitting to neural networks \ntrain_identity = X_train2.index\ntest_identity = X_test2.index\nX_train2 = np.array(X_train2)\nX_test2 = np.array(X_test2)\ny_train2 = np.array(y_train2)\ny_test2 = np.array(y_test2)","c5371cec":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nmodel.fit(X_train2, y_train2, batch_size = 15, epochs = 5)","c0e26964":"#Evaluate model\nscore = model.evaluate(X_test2, y_test2)\nprint(score)","44620324":"y_pred2 = model.predict(X_test2)\ncm2 = confusion_matrix(y_test2, y_pred2.round())\n\nprint(\"Deep Neural Network Evaluation \\n\")\nprint(\"Confusion Matrix:\")\nprint(cm2)\nprint(\"Accuracy: \",accuracy_score(y_test, y_pred.round()))\nprint(\"Precision: \",precision_score(y_test, y_pred.round()))\nprint(\"Recall: \",recall_score(y_test, y_pred.round()))\nprint(\"F1_Score: \",f1_score(y_test, y_pred.round()))","804866df":"X_resample, y_resample = SMOTE().fit_resample(X2, y2)\ncounter = Counter(y_resample)\nprint(counter)","4f8ae7d5":"X_train3, X_test3, y_train3, y_test3 = train_test_split(X_resample, y_resample, test_size = 0.2)\nX_train3 = np.array(X_train3)\nX_test3 = np.array(X_test3)\ny_train3 = np.array(y_train3)\ny_test3 = np.array(y_test3)","1a9e94a5":"# Create deep learning model\nmodel2 = Sequential()\nmodel2.add(Dense(input_dim = 29, units = 16, activation = 'relu'))\nmodel2.add(Dense(units = 24, activation = 'relu'))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(units = 20, activation = 'relu'))\nmodel2.add(Dense(units = 24, activation = 'relu'))\nmodel2.add(Dense(units = 1, activation = 'sigmoid'))","dee171a3":"model2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel2.fit(X_train3, y_train3, batch_size = 15, epochs = 5)","8a4b0caa":"#Evaluate model\nscore2 = model2.evaluate(X_test3, y_test3)\nprint(score2)","6a74e47b":"y_pred3 = model2.predict(X_test3)\ny_expect3 = pd.DataFrame(y_test3)\ncm3 = confusion_matrix(y_expect3, y_pred3.round())\n\nprint(\"Deep Neural Network with SMOTE Evaluation \\n\")\nprint(\"Confusion Matrix:\")\nprint(cm3)\nprint(\"Accuracy: \",accuracy_score(y_test, y_pred.round()))\nprint(\"Precision: \",precision_score(y_test, y_pred.round()))\nprint(\"Recall: \",recall_score(y_test, y_pred.round()))\nprint(\"F1_Score: \",f1_score(y_test, y_pred.round()))","6a4f315d":"This work is divided into four steps. First we visualize and understand the distribution of features and classes, then we cut the dataset so that both classes have the same size. Second, we apply machine learning models to the balanced data. Third, we use a deep learning model on unbalanced data. Finally, we balance the data using SMOTE and then apply deep learning. ","08fd2261":"# Machine Learning Models","1d65771b":"# Understanding and visualize the the data","02e835ab":"# Handling the imbalance in the dataset using SMOTE","e8131433":"## Deep Neural Network with imbalanced dataset"}}