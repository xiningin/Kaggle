{"cell_type":{"e489b284":"code","9abc02b9":"code","7a913553":"code","aa339c24":"code","d5dd015e":"code","c4c17c0d":"code","44740732":"code","30aebf03":"code","5a23c23f":"code","58bddd4a":"code","5d563481":"code","89d7d94f":"code","7d42aa55":"code","b6fc304a":"code","7defb8ae":"code","a3088074":"code","a505679f":"code","6a5b8514":"code","fd07e82a":"code","14e08337":"code","b0aaf3bd":"code","cb53ffd7":"code","52004670":"code","538fe952":"code","72f99e33":"code","85dcb825":"code","559345b2":"code","86e90e23":"code","4030bf82":"code","72d63911":"code","155cd970":"code","21871378":"code","fc6776e7":"code","70a99391":"code","5629a75f":"code","6b974259":"code","a6ade3ee":"code","1c81ecf9":"code","c5bad264":"code","836665eb":"markdown","6e969280":"markdown","ac409c91":"markdown","1398315e":"markdown","2f74e6c1":"markdown","b5b5ea2e":"markdown","17200063":"markdown","9692cedd":"markdown","7a7b7d46":"markdown","d77d1378":"markdown","5f91e71f":"markdown","10ffad3c":"markdown","ee2829cf":"markdown","71691005":"markdown","5f23be92":"markdown","6b271586":"markdown","db07ce22":"markdown","0e8c7835":"markdown","9c4e61fa":"markdown","d8e36e28":"markdown","d38d46b7":"markdown","6435e1bb":"markdown","35f1e269":"markdown","e11a226d":"markdown","689b3351":"markdown"},"source":{"e489b284":"import numpy as np\nimport h5py\n\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.models import Sequential\nimport pydot\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\n\n\nimport keras.backend as K\nK.set_image_data_format('channels_last')\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\n\n%matplotlib inline","9abc02b9":"def load_dataset():\n    train_data = h5py.File('..\/input\/train_happy.h5', \"r\")\n    X_train = np.array(train_data[\"train_set_x\"][:]) \n    y_train = np.array(train_data[\"train_set_y\"][:]) \n\n    test_data = h5py.File('..\/input\/test_happy.h5', \"r\")\n    X_test = np.array(test_data[\"test_set_x\"][:])\n    y_test = np.array(test_data[\"test_set_y\"][:]) \n    \n    y_train = y_train.reshape((1, y_train.shape[0]))\n    y_test = y_test.reshape((1, y_test.shape[0]))\n    \n    return X_train, y_train, X_test, y_test","7a913553":"X_train_orig, Y_train_orig, X_test_orig, Y_test_orig = load_dataset()\n\n# Normalize image vectors\nX_train = X_train_orig\/255.\nX_test = X_test_orig\/255.\n\n# Reshape\nY_train = Y_train_orig.T\nY_test = Y_test_orig.T\n\nprint (\"number of training examples = \" + str(X_train.shape[0]))\nprint (\"number of test examples = \" + str(X_test.shape[0]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"Y_train shape: \" + str(Y_train.shape))\nprint (\"X_test shape: \" + str(X_test.shape))\nprint (\"Y_test shape: \" + str(Y_test.shape))","aa339c24":"# Sample image from dataset\nprint(\"Image shape :\",X_train_orig[10].shape)\nimshow(X_train_orig[10])","d5dd015e":"# GRADED FUNCTION: HappyModel\n\ndef HappyModel(input_shape):\n    \n    X_input = Input(input_shape)\n    \n    X = ZeroPadding2D((3,3))(X_input)\n    \n    # CONV -> BN -> RELU Block applied to X\n    X = Conv2D(32, (7,7), strides=(1,1), name='Conv2D')(X)\n    X = BatchNormalization(axis=3, name='bn0')(X)\n    X = Activation('relu')(X)\n    \n    X = MaxPooling2D((2,2), name='max_pool')(X)\n    \n    X = Flatten()(X)\n    X = Dense(1, activation='sigmoid', name='fc')(X)\n    \n    model = Model(inputs = X_input, outputs=X, name='HappyModel')\n        \n    return model","c4c17c0d":"# Model flow chart\nhappyModel = HappyModel(X_train[0].shape)\nplot_model(happyModel, to_file='HappyModel.png')\nSVG(model_to_dot(happyModel).create(prog='dot', format='svg'))","44740732":"happyModel.summary()","30aebf03":"happyModel_sgd = HappyModel(X_train.shape[1:])\nhappyModel_sgd.compile(optimizer='sgd', loss='binary_crossentropy', metrics=[\"accuracy\"])","5a23c23f":"history_sgd = happyModel_sgd.fit(X_train,Y_train, epochs=5,batch_size=30)","58bddd4a":"train_accuracy = history_sgd.history['acc']\ntrain_loss = history_sgd.history['loss']\n\niterations = range(len(train_accuracy))\nplt.plot(iterations, train_accuracy, label='Training accuracy')\nplt.title('epochs vs Training accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(iterations, train_loss, label='Training Loss')\nplt.title('epochs vs Training Loss')\nplt.legend()","5d563481":"preds = happyModel_sgd.evaluate(x=X_test, y=Y_test)\n\nprint (\"\\nLoss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","89d7d94f":"happyModel_rms = HappyModel(X_train.shape[1:])\nhappyModel_rms.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=[\"accuracy\"])","7d42aa55":"history_rms = happyModel_rms.fit(X_train,Y_train, epochs=5,batch_size=30)","b6fc304a":"train_accuracy = history_rms.history['acc']\ntrain_loss = history_rms.history['loss']\n\niterations = range(len(train_accuracy))\nplt.plot(iterations, train_accuracy, label='Training accuracy')\nplt.title('epochs vs Training accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(iterations, train_loss, label='Training Loss')\nplt.title('epochs vs Training Loss')\nplt.legend()","7defb8ae":"preds = happyModel_rms.evaluate(x=X_test, y=Y_test)\n\nprint (\"\\nLoss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","a3088074":"happyModel_adam = HappyModel(X_train[0].shape)\nhappyModel_adam.compile(optimizer='Adam', loss='binary_crossentropy', metrics=[\"accuracy\"])","a505679f":"history_adam = happyModel_adam.fit(X_train,Y_train, epochs=5,batch_size=30)","6a5b8514":"train_accuracy = history_adam.history['acc']\ntrain_loss = history_adam.history['loss']\n\ncount = range(len(train_accuracy))\nplt.plot(count, train_accuracy, label='Training accuracy')\nplt.title('epochs vs Training accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(count, train_loss, label='Training Loss')\nplt.title('epochs vs Training Loss')\nplt.legend()","fd07e82a":"preds = happyModel_adam.evaluate(x=X_test, y=Y_test)\n\nprint()\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","14e08337":"happyModelE = HappyModel(X_train.shape[1:])\nhappyModelE.compile(optimizer='Adam', loss='binary_crossentropy', metrics=[\"accuracy\"])","b0aaf3bd":"THistoryE = happyModelE.fit(X_train,Y_train, epochs=20,batch_size=30)","cb53ffd7":"train_accuracy = THistoryE.history['acc']\ntrain_loss = THistoryE.history['loss']\n\niterations = range(len(train_accuracy))\nplt.plot(iterations, train_accuracy, label='Training accuracy')\nplt.title('epochs vs Training accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(iterations, train_loss, label='Training Loss')\nplt.title('epochs vs Training Loss')\nplt.legend()","52004670":"preds = happyModelE.evaluate(x=X_test, y=Y_test)\n\nprint()\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","538fe952":"y_pred = happyModelE.predict(X_test)","72f99e33":"y_pred[y_pred < 0.5] = 0\ny_pred[y_pred >= 0.5] = 1","85dcb825":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\nprint(cm)","559345b2":"happyModel3 = HappyModel(X_train.shape[1:])\nhappyModel3.compile(optimizer='Adam', loss='binary_crossentropy', metrics=[\"accuracy\"])","86e90e23":"# THistory (Train History)\nTHistory3 = happyModel3.fit(X_train,Y_train, epochs=30,batch_size=30)","4030bf82":"train_accuracy = THistory3.history['acc']\ntrain_loss = THistory3.history['loss']\n\niterations = range(len(train_accuracy))\nplt.plot(iterations, train_accuracy, label='Training accuracy')\nplt.title('epochs vs Training accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(iterations, train_loss, label='Training Loss')\nplt.title('epochs vs Training Loss')\nplt.legend()","72d63911":"preds = happyModel3.evaluate(x=X_test, y=Y_test)\n\nprint()\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","155cd970":"happyModel2 = HappyModel(X_train[0].shape)\nhappyModel2.compile(optimizer='Adam', loss='binary_crossentropy', metrics=[\"accuracy\"])","21871378":"THistory2 = happyModel2.fit(X_train,Y_train, epochs=40,batch_size=16)","fc6776e7":"train_accuracy = THistory2.history['acc']\ntrain_loss = THistory2.history['loss']\n\niterations = range(len(train_accuracy))\nplt.plot(iterations, train_accuracy, label='Training accuracy')\nplt.title('epochs vs Training accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(iterations, train_loss, label='Training Loss')\nplt.title('epochs vs Training Loss')\nplt.legend()","70a99391":"preds2 = happyModel2.evaluate(x=X_test, y=Y_test)\n\nprint()\nprint (\"Loss = \" + str(preds2[0]))\nprint (\"Test Accuracy = \" + str(preds2[1]))","5629a75f":"# Building LeNet-5 \ndef create_model():\n    model = Sequential()\n    model.add(layers.Conv2D(filters=1, kernel_size=(1,1), strides=(2,2), name='Conv2D', input_shape=(64,64,3))) # For converting image to 32,32,1\n    model.add(layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu'))\n    model.add(layers.AveragePooling2D())\n\n    model.add(layers.Conv2D(filters=16, kernel_size=(5, 5), activation='relu'))\n    model.add(layers.AveragePooling2D())\n\n    model.add(layers.Flatten())\n\n    model.add(layers.Dense(units=120, activation='relu'))\n\n    model.add(layers.Dense(units=84, activation='relu'))\n\n    model.add(layers.Dense(units=1, activation = 'sigmoid'))\n    \n    return model","6b974259":"model = create_model()\nmodel.summary()","a6ade3ee":"plot_model(model, to_file='HappyModel.png')\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","1c81ecf9":"lenet5 = create_model()\nlenet5.compile(optimizer='Adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\nhistory = lenet5.fit(X_train,Y_train, epochs=20,batch_size=32)\n\ntrain_accuracy = history.history['acc']\ntrain_loss = history.history['loss']\n\niterations = range(len(train_accuracy))\nplt.plot(iterations, train_accuracy, label='Training accuracy')\nplt.title('epochs vs Training accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(iterations, train_loss, label='Training Loss')\nplt.title('epochs vs Training Loss')\nplt.legend()","c5bad264":"preds = lenet5.evaluate(x=X_test, y=Y_test)\n\nprint (\"\\nLoss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n","836665eb":"## 1 - Importing Libraries and Data ","6e969280":"## 5 - Predicting using LeNet-5","ac409c91":"Change path to predict your pic is happy or not","1398315e":"About RMSprop Optimizer  <a href=\"https:\/\/towardsdatascience.com\/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b\">here<\/a>","2f74e6c1":"Adam optimizer is a combination of RMSprop and Momentum optimizers. More about Adam Optimizer  <a href=\"https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/\">here<\/a>","b5b5ea2e":"## 6 - Test with your own image","17200063":"<ul>\n<li>LeNet 5 : 94.6% accuracy<\/li>\n<li>Coursera model :  97.9% accuracy with batch size 30 and epoch 20 where coursera suggested model got 96% accuracy<\/li>\n<li>Compared SGD, RMSprop and Adam Optimizers. Among those Adam got hight accuracy<\/li>\n<\/ul>\n*Note :* Accuracy is not same everytime ","9692cedd":"### 4.3 - Epoch 40, Batch size : 16 (Coursera suggested model parameters)","7a7b7d46":"## 3 - Predicting using various Optimizers","d77d1378":"### 3.1 - SGD (Stochastic gradient descent optimizer)","5f91e71f":"## Conclusion","10ffad3c":"## The Happy House \n\n### Problem Statement:\nPredicting your emotion happy or not.\n\n\n\nIn this,\n1. We are going to build 2 types of Architecture of Neural Network among those one is Lenet-5 \n2. Comparing Various Optimizers\n3. Variously trained models(Different epochs and batch size)\n\nDataset is pictures from the front door camera to check if the person is happy or not.  The dataset is labbeled.\n\n**Details of the \"Happy\" dataset**:\n- Images are of shape (64,64,3)\n- Training: 600 pictures\n- Test: 150 pictures\n\n\nIt is now time to solve the \"Happy\" Challenge.","ee2829cf":"path = 'https:\/\/www.cifar.ca\/images\/default-source\/bios\/lmb_yannlacun.png'\n\nimg = image.load_img(path, target_size=(64, 64))\nimshow(img)\n\nimg = image.img_to_array(img)\nimg = np.expand_dims(img, axis=0)\nimg = preprocess_input(img)\n\nprint(lenet2.predict(img))","71691005":"## 2 - Building a model ","5f23be92":"LeNet-5 is proposed by Yann LeCun(received Turing award). It is base ConvNet for modern neural network (CNN).\nMore about LeNet5 <a href=\"https:\/\/medium.com\/@pechyonkin\/key-deep-learning-architectures-lenet-5-6fc3c59e6f4\">here<\/a> and \n<a href=\"http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/lecun-01a.pdf\">paper<\/a>","6b271586":"### Conclusion\n<ul> <li> Adam Optimizer got high accuracy i.e., 94%<\/li> <\/ul>","db07ce22":"### 3.3 - Adam","0e8c7835":"# 4 - Predicting with different epochs trained models","9c4e61fa":"### 3.2 - RMSprop Optimizer","d8e36e28":"This architecture is given in Deep learning specialization|","d38d46b7":"### 4.2 - Epoch : 30","6435e1bb":"### Observation\n<ul> <li>Model accuracy increased updo 97.9%<\/li> <\/ul>","35f1e269":"About SGD  <a href=\"https:\/\/towardsdatascience.com\/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1\">here<\/a>","e11a226d":"### Observation\nGot 94% accuracy!","689b3351":"### 4.1 - Epoch : 20"}}