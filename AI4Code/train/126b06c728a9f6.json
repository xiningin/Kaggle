{"cell_type":{"f7a0af5f":"code","4fc16fd3":"code","d32f3d1d":"code","4df50051":"code","56bced8e":"code","a77731bd":"code","cd4af4c6":"code","2449543f":"code","f306e179":"code","b19f8f5b":"code","92bafdce":"code","b5f23754":"code","181e49e5":"code","7771ccc1":"code","343f4408":"code","c2ee817d":"code","4617cb18":"code","778fd953":"code","12fa20a8":"code","e3393553":"code","b52cbb2b":"code","309f4650":"code","4ee60dee":"code","51f390b8":"code","7665ca04":"code","666d16d0":"code","7d8e13d1":"code","b8251cc6":"code","6bb35f1d":"code","67380eac":"code","2321b607":"code","8422c54f":"code","01ba757d":"code","895a159e":"code","c309da19":"code","d3bc0140":"code","d0332625":"code","34e9b129":"code","6323628f":"code","db3ee9b5":"code","523d3fdc":"code","0d818027":"code","96847a6f":"code","a90d6765":"code","7c9f10d0":"code","14a2bb31":"code","b0ab5fee":"code","b377ba94":"code","e91af1c0":"code","fb01e490":"code","035efa90":"markdown","7ca666d4":"markdown","0c9fb2c3":"markdown","c8929c55":"markdown","7b3d0db2":"markdown","9d46dbe8":"markdown","9a0bed2c":"markdown","ae002867":"markdown","3effb1e0":"markdown","468fc19b":"markdown","b6f0bceb":"markdown","f5a18596":"markdown","4956c162":"markdown","d8756033":"markdown","edbf7344":"markdown","8aa511bd":"markdown","af8498ed":"markdown","6547b5f1":"markdown","a1640296":"markdown","3bffd086":"markdown","903604c8":"markdown","1ee5628c":"markdown","cc7789c6":"markdown","d0d2a64f":"markdown","f9242016":"markdown","b250285b":"markdown","bbd5bc02":"markdown","c39508bf":"markdown","c30459a1":"markdown","2cfa05e8":"markdown","29936943":"markdown","c5f429c1":"markdown","fb3117be":"markdown","96feda76":"markdown","441db118":"markdown","87fb2d34":"markdown","ed132bf2":"markdown","9105305b":"markdown","5121326b":"markdown","83e75bc0":"markdown","f439d643":"markdown","8bb86d6d":"markdown","67ec294b":"markdown","f9b9cdc8":"markdown","cb0d3642":"markdown","808a0def":"markdown","a987003f":"markdown","002ad03b":"markdown","b7e670bc":"markdown","aa278645":"markdown"},"source":{"f7a0af5f":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"darkgrid\")\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.impute import SimpleImputer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4fc16fd3":"train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col=0)\n\n\nprint(\"train: \", train.shape)\nprint(\"test: \", test.shape)\ntrain.head()","d32f3d1d":"X = pd.concat([train.drop(\"SalePrice\", axis=1),test], axis=0)\ny = train[['SalePrice']]","4df50051":"X.info()","56bced8e":"numeric_ = X.select_dtypes(exclude=['object']).drop(['MSSubClass'], axis=1).copy()\nnumeric_.columns","a77731bd":"disc_num_var = ['OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n                'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']\n\ncont_num_var = []\nfor i in numeric_.columns:\n    if i not in disc_num_var:\n        cont_num_var.append(i)","cd4af4c6":"cat_train = X.select_dtypes(include=['object']).copy()\ncat_train['MSSubClass'] = X['MSSubClass']   #MSSubClass is nominal\ncat_train.columns","2449543f":"fig = plt.figure(figsize=(18,16))\nfor index,col in enumerate(cont_num_var):\n    plt.subplot(6,4,index+1)\n    sns.distplot(numeric_.loc[:,col].dropna(), kde=False)\nfig.tight_layout(pad=1.0)","f306e179":"fig = plt.figure(figsize=(14,15))\nfor index,col in enumerate(cont_num_var):\n    plt.subplot(6,4,index+1)\n    sns.boxplot(y=col, data=numeric_.dropna())\nfig.tight_layout(pad=1.0)","b19f8f5b":"fig = plt.figure(figsize=(20,15))\nfor index,col in enumerate(disc_num_var):\n    plt.subplot(5,3,index+1)\n    sns.countplot(x=col, data=numeric_.dropna())\nfig.tight_layout(pad=1.0)","92bafdce":"fig = plt.figure(figsize=(18,20))\nfor index in range(len(cat_train.columns)):\n    plt.subplot(9,5,index+1)\n    sns.countplot(x=cat_train.iloc[:,index], data=cat_train.dropna())\n    plt.xticks(rotation=90)\nfig.tight_layout(pad=1.0)","b5f23754":"plt.figure(figsize=(14,12))\ncorrelation = numeric_.corr()\nsns.heatmap(correlation, mask = correlation <0.8, linewidth=0.5, cmap='Blues')","181e49e5":"numeric_train = train.select_dtypes(exclude=['object'])\ncorrelation = numeric_train.corr()\ncorrelation[['SalePrice']].sort_values(['SalePrice'], ascending=False)","7771ccc1":"fig = plt.figure(figsize=(20,20))\nfor index in range(len(numeric_train.columns)):\n    plt.subplot(10,5,index+1)\n    sns.scatterplot(x=numeric_train.iloc[:,index], y='SalePrice', data=numeric_train.dropna())\nfig.tight_layout(pad=1.0)","343f4408":"X.drop(['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageCars'], axis=1, inplace=True)","c2ee817d":"plt.figure(figsize=(25,8))\nplt.title('Number of missing rows')\nmissing_count = pd.DataFrame(X.isnull().sum(), columns=['sum']).sort_values(by=['sum'],ascending=False).head(20).reset_index()\nmissing_count.columns = ['features','sum']\nsns.barplot(x='features',y='sum', data = missing_count)","4617cb18":"X.drop(['PoolQC','MiscFeature','Alley'], axis=1, inplace=True)","778fd953":"fig,axes = plt.subplots(1,2, figsize=(15,5))\nsns.regplot(x=numeric_train['MoSold'], y='SalePrice', data=numeric_train, ax = axes[0], line_kws={'color':'black'})\nsns.regplot(x=numeric_train['YrSold'], y='SalePrice', data=numeric_train, ax = axes[1],line_kws={'color':'black'})\nfig.tight_layout(pad=2.0)","12fa20a8":"correlation[['SalePrice']].sort_values(['SalePrice'], ascending=False).tail(10)\n\nX.drop(['MoSold','YrSold'], axis=1, inplace=True)","e3393553":"cat_col = X.select_dtypes(include=['object']).columns\noverfit_cat = []\nfor i in cat_col:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 96:\n        overfit_cat.append(i)\n\noverfit_cat = list(overfit_cat)\nX = X.drop(overfit_cat, axis=1)","b52cbb2b":"num_col = X.select_dtypes(exclude=['object']).drop(['MSSubClass'], axis=1).columns\noverfit_num = []\nfor i in num_col:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 96:\n        overfit_num.append(i)\n\noverfit_num = list(overfit_num)\nX = X.drop(overfit_num, axis=1)","309f4650":"print(\"Categorical Features with >96% of the same value: \",overfit_cat)\nprint(\"Numerical Features with >96% of the same value: \",overfit_num)","4ee60dee":"out_col = ['LotFrontage','LotArea','BsmtFinSF1','TotalBsmtSF','GrLivArea']\nfig = plt.figure(figsize=(20,5))\nfor index,col in enumerate(out_col):\n    plt.subplot(1,5,index+1)\n    sns.boxplot(y=col, data=X)\nfig.tight_layout(pad=1.5)","51f390b8":"train = train.drop(train[train['LotFrontage'] > 200].index)\ntrain = train.drop(train[train['LotArea'] > 100000].index)\ntrain = train.drop(train[train['BsmtFinSF1'] > 4000].index)\ntrain = train.drop(train[train['TotalBsmtSF'] > 5000].index)\ntrain = train.drop(train[train['GrLivArea'] > 4000].index)","7665ca04":"X.shape","666d16d0":"pd.DataFrame(X.isnull().sum(), columns=['sum']).sort_values(by=['sum'],ascending=False).head(15)","7d8e13d1":"cat = ['GarageType','GarageFinish','BsmtFinType2','BsmtExposure','BsmtFinType1', \n       'GarageCond','GarageQual','BsmtCond','BsmtQual','FireplaceQu','Fence',\"KitchenQual\",\n       \"HeatingQC\",'ExterQual','ExterCond']\n\nX[cat] = X[cat].fillna(\"NA\")","b8251cc6":"#categorical\ncols = [\"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"Functional\"]\nX[cols] = X.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mode()[0]))","6bb35f1d":"print(\"Mean of LotFrontage: \", X['LotFrontage'].mean())\nprint(\"Mean of GarageArea: \", X['GarageArea'].mean())","67380eac":"neigh_lot = X.groupby('Neighborhood')['LotFrontage'].mean().reset_index(name='LotFrontage_mean')\nneigh_garage = X.groupby('Neighborhood')['GarageArea'].mean().reset_index(name='GarageArea_mean')\n\nfig, axes = plt.subplots(1,2,figsize=(22,8))\naxes[0].tick_params(axis='x', rotation=90)\nsns.barplot(x='Neighborhood', y='LotFrontage_mean', data=neigh_lot, ax=axes[0])\naxes[1].tick_params(axis='x', rotation=90)\nsns.barplot(x='Neighborhood', y='GarageArea_mean', data=neigh_garage, ax=axes[1])","2321b607":"#for correlated relationship\nX['LotFrontage'] = X.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\nX['GarageArea'] = X.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))\nX['MSZoning'] = X.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n#numerical\ncont = [\"BsmtHalfBath\", \"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\"]\nX[cont] = X[cont] = X[cont].fillna(X[cont].mean())","8422c54f":"X['MSSubClass'] = X['MSSubClass'].apply(str)","01ba757d":"ordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\nfintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nexpose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\nfence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}","895a159e":"ord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\nfor col in ord_col:\n    X[col] = X[col].map(ordinal_map)\n    \nfin_col = ['BsmtFinType1','BsmtFinType2']\nfor col in fin_col:\n    X[col] = X[col].map(fintype_map)\n\nX['BsmtExposure'] = X['BsmtExposure'].map(expose_map)\nX['Fence'] = X['Fence'].map(fence_map)","c309da19":"X['TotalLot'] = X['LotFrontage'] + X['LotArea']\nX['TotalBsmtFin'] = X['BsmtFinSF1'] + X['BsmtFinSF2']\nX['TotalSF'] = X['TotalBsmtSF'] + X['2ndFlrSF']\nX['TotalBath'] = X['FullBath'] + X['HalfBath']\nX['TotalPorch'] = X['OpenPorchSF'] + X['EnclosedPorch'] + X['ScreenPorch']","d3bc0140":"colum = ['MasVnrArea','TotalBsmtFin','TotalBsmtSF','2ndFlrSF','WoodDeckSF','TotalPorch']\n\nfor col in colum:\n    col_name = col+'_bin'\n    X[col_name] = X[col].apply(lambda x: 1 if x > 0 else 0)","d0332625":"X = pd.get_dummies(X)","34e9b129":"plt.figure(figsize=(10,6))\nplt.title(\"Before transformation of SalePrice\")\ndist = sns.distplot(train['SalePrice'],norm_hist=False)","6323628f":"plt.figure(figsize=(10,6))\nplt.title(\"After transformation of SalePrice\")\ndist = sns.distplot(np.log(train['SalePrice']),norm_hist=False)","db3ee9b5":"y[\"SalePrice\"] = np.log(y['SalePrice'])","523d3fdc":"x = X.loc[train.index]\ny = y.loc[train.index]\ntest = X.loc[test.index]","0d818027":"from sklearn.preprocessing import RobustScaler\n\ncols = x.select_dtypes(np.number).columns\ntransformer = RobustScaler().fit(x[cols])\nx[cols] = transformer.transform(x[cols])\ntest[cols] = transformer.transform(test[cols])","96847a6f":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor","a90d6765":"# Split into validation(20%) and training data (80%)\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=2020)","7c9f10d0":"# Specify Model\nrf_model = DecisionTreeRegressor(random_state=1)\n# Fit Model\nrf_model.fit(X_train, y_train)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = rf_model.predict(X_val)\nval_mae = mean_absolute_error(val_predictions, y_val)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n\n# Using best value for max_leaf_nodes\nrf_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\nrf_model.fit(X_train, y_train)\nval_predictions = rf_model.predict(X_val)\nval_mae = mean_absolute_error(val_predictions, y_val)\nprint(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n\n# Define the model\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(X_train, y_train)\nrf_val_predictions = rf_model.predict(X_val) # make predictions which we will submit\nrf_val_mae = mean_absolute_error(rf_val_predictions, y_val)\n\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))","14a2bb31":"# Define the model\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(x, y)\nrf_val_predictions = rf_model.predict(test) # make predictions which we will submit","b0ab5fee":"output = pd.DataFrame({'Id': test.index,\n                       'SalePrice': rf_val_predictions})\noutput.to_csv('submission.csv', index=False)","b377ba94":"valid_fraction = 0.1\nvalid_rows = int(len(x) * valid_fraction)\ntrain_x = x[:-valid_rows]\nvalid_x = x[-valid_rows:]\n\ntrain_y = y[:-valid_rows]\nvalid_y = y[-valid_rows:]","e91af1c0":"import lightgbm as lgb\n\ndtrain = lgb.Dataset(train_x, label=train_y)\ndvalid = lgb.Dataset(valid_x, label=valid_y)\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 1000\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10)","fb01e490":"from sklearn import metrics\n\nypred = bst.predict(test)\noutput = pd.DataFrame({'Id': test.index,\n                       'SalePrice': rf_val_predictions})\noutput.to_csv('submission2.csv', index=False)","035efa90":"### Identifying relationship between Numerical Predictor and Target (*SalePrice*)\nBelow, we sorted the strength of linear relationship between Saleprice and other features. OverallQual and GrLivArea has the strongest linear relationship with SalePrice. Hence, these 2 features will be important factor in predicting Housing Price","7ca666d4":"Lets isolate both the numerical and categorical columns, to apply different visualization techniques on them","0c9fb2c3":"### Features with alot of missing values\nApart from these highly correlated features, we will also remove features that is not very useful in prediction due to many missing values. PoolQC, MiscFeature, Alley has too many missing values to provide any useful information","c8929c55":"### Correlation Matrix","7b3d0db2":"We will remove the highly correlated features to avoid the problem of multicollinearity explained earlier","9d46dbe8":"Some of the Variables with mostly 1 value as seen from the plots above:\n1. BsmtFinSF2\n2. LowQualFinSF\n3. EnclosedPorch\n4. 3SsnPorch\n5. ScreenPorch\n6. PoolArea\n7. MiscVal  \n\nAll these features are highly skewed, with mostly 0s. Having alot of 0s in the distribution doesnt really add information for predicting Housing Price. Hence, we will remove them during our preprocessing step","9a0bed2c":"*Fork of \"Data Science Workflow TOP 2% (with Tuning)\" by aqx.*","ae002867":"<a id=\"sec2.3\"><\/a>\n## [2.3 Filling Missing Values](#sec2.3)\n\nOur machine learning model is unable to deal with missing values, thus we need to deal with them based on our understanding of the features. These missing values are denoted **NAN** as we have seen earlier during our data exploration.","3effb1e0":"### Numerical features\nFor **Numerical Features**, the common approach will be to replace the missing value with the mean of the feature distribution.  \nHowever, certain features like *LotFrontage* and *GarageArea* have wide variance in their distribution. Taking mean values across *Neighborhoods*, we will see that the mean varies alot from just taking the mean value of these individual column, since each neightborhood have different LotFrontage and GarageArea mean value. Hence, i decided to group these features by Neighborhoods to impute the respective mean values.","468fc19b":"Based on the current feature we have, the first additional featuire we can add would be **TotalLot**, which sums up both the *LotFrontage* and *LotArea* to identify the total area of land available as lot. We can also calculate the total number of surface area of the house, **TotalSF** by adding the area from basement and 2nd floor. **TotalBath** can also be used to tell us in total how many bathrooms are there in the house. We can also add all the different types of porches around the house and generalise into a total porch area, **TotalPorch**.  \n* TotalLot = LotFrontage + LotArea\n* TotalSF = TotalBsmtSF + 2ndFlrSF\n* TotalBath = FullBath + HalfBath\n* TotalPorch = OpenPorchSF + EnclosedPorch + ScreenPorch \n* TotalBsmtFin = BsmtFinSF1 + BsmtFinSF2","b6f0bceb":"<a id=\"sec2.1\"><\/a>\n## [2.1 Removing Redundant Features](#sec2.1)","f5a18596":"### Converting Categorical to Numerical\nLastly, because machine learning only learns from data that is numerical in nature, we will convert the remaining categorical columns into one-hot features using the *get_dummies()* method into numerical columns that is suitable for feeding into our machine learning algorithm.","4956c162":"<a id='sec1.2'><\/a>\n## [1.2 Bi-Variate Analysis](#sec1.2)","d8756033":"This section outlines the steps for Data Processing:\n1. Removing Redundant Features\n2. Dealing with Outliers\n3. Filling in missing values","edbf7344":"### Scatterplot\nUsing scatterplot can also help us to identify potential linear relationship between Numerical features. Although scatterplot does not provide quantitative evidence on the strength of linear relationship between our features, it is useful in helping us to visualize any sort of relationship that correlation matrix could not calculate. E.g Quadratic, Exponential relationships. ","8aa511bd":"#### Changing Data Type\nSince **MSSubClass** is an integer column based on some mapped values in string notation, we change its data type to string value instead","af8498ed":"### Binay Columns\nWe also include simple feature engineering by creating binary columns for some features that can indicate the **presence(1) \/ absence(0)** of some features of the house","6547b5f1":"### SalePrice Distribution","a1640296":" **Highly Correlated variables**:\n* GarageYrBlt and YearBuilt\n* TotRmsAbvGrd and GrLivArea\n* 1stFlrSF and TotalBsmtSF\n* GarageArea and GarageCars\n\nFrom the correlation matrix we have identified the above variables which are highly correlated with each other. This finding will guide us in our preprocessing steps later on as we aim to remove highly correlated features to avoid performance loss in our model","3bffd086":"<a id='sec4'><\/a>\n# [4. Modeling](#sec4)\nThis section will consist of scaling the data for better optimization in our training, and also introducing the varieties of ensembling methods that are used in this notebook for predicting the Housing price. We also try out hyperparameter tuning briefly, as i will be dedicating a new notebook that will explain more in details on the process of Hyperparameter Tuning as well as the mathematical aspect of the ensemble algorithms.","903604c8":"### Useless features in predicting SalePrice\nWe will also remove features that does not have any linear relationship with target *SalePrice*. We can see from the plot below that the MoSold and YrSold does not have any impact on the price of the house sold.","1ee5628c":"### Split into train-validation set","cc7789c6":"Now that we are satisfied with our final data, we will proceed to the part where we will solve this regression problem - Modeling","d0d2a64f":"<a id=\"sec1.1\"><\/a>\n## [1.1 Univariate Analysis](#sec1.1)","f9242016":"<a id='sec3'><a>\n# [3. Feature Engineering](#sec3)\nFeature Engineering is a technique by which we create new features that could potentially aid in predicting our target variable, which in this case, is *SalePrice*. In this notebook, we will create additional features based on our **Domain Knowledge** of the housing features","b250285b":"Univariate Analysis helps us to understand all the features better, on an individual scale. To further deepen our insights and uncover potential pattern in the data, we will also need to find out more about the relationship between all these features with one another, which brings us to our next step in our analysis - Bivariate Analysis","bbd5bc02":"### Mapping Ordinal Features\nThere are some columns which are ordinal by nature, which represents the quality or condition of certain housing features. In this case, we will map the respective strings to a value. The better the quality, the higher the value","c39508bf":"<a id='sec4.2'><\/a>\n## [4.2 Random Forest](#sec4.2)","c30459a1":"# Housing Prices Competition for Kaggle Learn Users\n\nHouse Pricing regression problem, with Random Forest.","2cfa05e8":"### Ordinal features \nWe will replace the ordinal missing values with NA, which will be mapped later on when we encode them into an ordered arrangement","29936943":"<a id=\"sec1\"><\/a>\n# [1. Understanding Data](#sec1)","c5f429c1":"### Numeric Features\n\nFor numerical features, we are always concerned about the **distribution** of these features, including the **statistical characteristics** of these columns e.g mean, median, mode. Hence  we will usually use **Distribution plot** to visualize their data distribution. **Boxplots** are also commonly used to unearth the statistical characteristics of each feature. More often than not, we use it to look for any outliers that we might need to filter out later on during the preprocessing step.","fb3117be":"<a id = 'sec2'><\/a>\n# [2. Data Processing](#sec2)\n\nNow that we have more or less finished analysing our data and gaining insights through the various analysis and visualization, we will have to leverage on these insights to guide our preprocessing decision, so as to provide a clean and error-free data for our model to train on later on.  ","96feda76":"### Creating a Model For the Competition","441db118":"### Removing features that have mostly just 1 value\n\nEarlier during our Univariate analysis, we found that some features mostly consist of just a single value or 0s, which is not useful to us. Therefore, we set a user defined threshold at 96%. If a column has more than 96% of the same value, we will render the features to be useless and remove it, since there isnt much information we can extract from","87fb2d34":"<a id='sec4.1'><\/a>\n## [4.1 Scaling of Data](#sec4.1)\n**RobustScaler** is a transformation technique that removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). It is also robust to outliers, which makes it ideal for data where there are too many outliers that will drastically reduce the number of training data.","ed132bf2":"From the above correlation matrix, we have pinpointed certain features that are highly correlated\n* GarageYrBlt and YearBuilt\n* TotRmsAbvGrd and GrLivArea\n* 1stFlrSF and TotalBsmtSF\n* GarageArea and GarageCars","9105305b":"### Categorical Features\n\nIn the case of categorical features, we will often use countplots to visualize the count of each distinct value within each features. We can see that some categorical features like **Utilities, Condition2** consist of mainly just one value, which does not add any useful information. Thus, we will also remove them later on.","5121326b":"Distribution is skewed to the right, where the tail on the curve\u2019s right-hand side is longer than the tail on the left-hand side, and the mean is greater than the mode. This situation is also called positive skewness.  \nHaving a skewed target will affect the overall performance of our machine learning model, thus, one way to alleviate will be to using **log transformation** on skewed target, in our case, the *SalePrice* to reduce the skewness of the distribution.","83e75bc0":"<a id='sec4.3'><\/a>\n## [4.3 LightGBM](#sec4.3)","f439d643":"### Categorical features\nWe will replace the missing value of our categorical features with the most frequent occurrence (mode) of the individual features.","8bb86d6d":"Previously i fitted the RobustScaler on both Train and Test set, and that is a mistake on my side. By fitting the scaler on both train and testset, we exposed ourselves to the problem of **Data Leakage**. Data Leakage is a problem when information from outside the training dataset is used to create the model. If we fit the scaler on both training and test data, our training data characteristics will contain the distribution of our testset. As such, we are unknowningly passing in information about our test data into the final training data for training, which will not give us the opportunity to truly test our model on data it has never seen before.  \n**Lesson Learnt**: Fit the scaler just on training data, and then transforming it on both training and test data","67ec294b":"Bi-variate analysis looks at 2 different features to identify any possible relationship or distinctive patterns between the 2 features. One of the commonly used technique is through the  **Correlation Matrix**. Correlation matrix is an effective tool to uncover linear relationship (Correlation) between any 2 continuous features. Correlation not only allow us to determine which features are important to Saleprice, but also as a mean to investigate any **multicollinearity** between our independent predictors.  \nMulticollinearity happens when 2 or more independent variables are highly correlated with one another. In such situation, it causes precision loss in our regression coefficients, affecting our ability to identify the most important features that are most useful to our model","f9b9cdc8":"## Submit to competition\n\nTo test the results, join the competition. So open a new window by clicking on [this link](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course). Then click on the **Join Competition** button.\n\n\nNext, follow the instructions below:\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the blue **Submit** button to submit your results to the leaderboard.","cb0d3642":"<a id=\"sec2.2\"><\/a>\n## [2.2 Dealing with Outliers](#sec2.2)","808a0def":"Concatenate the train and test together for extracting insights into the Housing Price data as a whole.","a987003f":"After removing the outliers, highly correlated features and imputing missing values, we can now proceed with adding additional information for our model to train on. This is done by the means of - Feature Engineering.","002ad03b":"# Table of Contents\n1. [Understanding Data](#sec1)\n    * [1.2 Univariate Analysis](#sec1.2)  \n    * [1.3 Bi-variate Analysis](#sec1.3)  \n2. [Data Preprocessing](#sec2)  \n    * [2.1 Removing redundant features](#sec2.1)\n    * [2.2 Dealing with Outliers](#sec2.2)\n    * [2.3 Filling Missing Values](#sec2.3)\n3. [Feature Engineering](#sec3)\n4. [Modeling](#sec4)\n    * [4.1 Scaling of features](#sec4.1)\n    * [4.2 Random Forest](#sec4.2)","b7e670bc":"### Features with multicollinearity","aa278645":"Removing outliers will prevent our models performance from being affected by extreme values.  \nFrom our boxplot earlier, we have pinpointed the following features with extreme outliers:\n* LotFrontage\n* LotArea\n* BsmtFinSF1\n* TotalBsmtSF\n* GrLivArea\n\nWe will remove the outliers based on certain threshold value."}}