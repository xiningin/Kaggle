{"cell_type":{"816f8922":"code","3ac14604":"code","57bef8a6":"code","84641e95":"code","8641d0ed":"code","2e96e73b":"code","032d2996":"code","d962788e":"code","cf980c36":"code","19f13afa":"code","e09d1f16":"code","244d5f45":"code","58a22af9":"code","7f9ae913":"code","b2b500d7":"code","47736f29":"code","07b3d8a1":"code","941f417e":"code","643f8500":"code","8350a829":"code","a7f1ff7d":"code","9df382fc":"code","491b1848":"code","54517838":"code","c5e84f80":"code","54763de6":"code","38492673":"code","2d2e1d6f":"code","e5bc303a":"code","2f66e1f2":"code","4a9670f8":"code","0aaee460":"code","6dfd2046":"code","a6659a84":"code","fbb6142f":"code","559eb977":"code","0d6eaa1b":"code","2cbed4f1":"code","ea6a49d3":"code","e675c1b6":"code","f3130346":"code","8ec79d15":"code","922ca681":"markdown","125af7e8":"markdown","f0c1e489":"markdown","511454f2":"markdown","bc5a7223":"markdown"},"source":{"816f8922":"import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FormatStrFormatter\nimport datatable as dt\n\n","3ac14604":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","57bef8a6":"%%time\ntrain = dt.fread('..\/input\/tabular-playground-series-nov-2021\/train.csv').to_pandas().drop('id', axis=1)\ntrain = reduce_memory_usage(train)\ntest = dt.fread('..\/input\/tabular-playground-series-nov-2021\/test.csv').to_pandas().drop('id', axis=1)\ntest = reduce_memory_usage(test)\nss = dt.fread('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv').to_pandas()\nss = reduce_memory_usage(ss)","84641e95":"print(train.shape)\nprint(test.shape)","8641d0ed":"display(train.isna().sum().sum())\ndisplay(test.isna().sum().sum())","2e96e73b":"train.describe().T","032d2996":"train","d962788e":"#Lets find traget variable distribution\ntarget_varibale=train['target']\npal = ['#5ddef4','#8e99f3']\nplt=sns.countplot(x=train.target,palette=pal)\nplt.set_title('Target  distribution', fontsize=20, y=1.05)","cf980c36":"train.dtypes\n","19f13afa":"categorical_features=[]\nnumerical_features=[]\nfor i in train.columns:\n    if train[i].dtype=='float16':\n#         print('yes')\n        numerical_features.append(i)\n    elif i!='target':\n        categorical_features.append(i)\n#         print('no')","e09d1f16":"display(len(categorical_features))\ndisplay(len(numerical_features))","244d5f45":"df_num_features = pd.concat([train[numerical_features], train['target']], axis=1) # still writing even though train and this frame will be same","58a22af9":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(16 , 16))\ncorr = df_num_features.sample(10000, random_state=2021).corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax, square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(0,255,sep=77, as_cmap=True),vmax=0.5, vmin=-0.5,\n        cbar_kws={\"shrink\": .85}, mask=mask )\nax.set_title('Correlation heatmap: Numerical features', fontsize=24, y= 1.05)\nplt.show()","7f9ae913":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nX = train.drop('target',axis=1)\ny = train['target']\n#In general a good idea is to scale the data\nscaler = StandardScaler()\nscaler.fit(X)\nX=scaler.transform(X)    ","b2b500d7":"pca = PCA(n_components=2)\nx_new = pca.fit_transform(X)","47736f29":"def myplot(score,coeff,labels=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0\/(xs.max() - xs.min())\n    scaley = 1.0\/(ys.max() - ys.min())\n    plt.scatter(xs * scalex,ys * scaley, c = y)\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\nplt.xlim(-1,1)\nplt.ylim(-1,1)\nplt.xlabel(\"PC{}\".format(1))\nplt.ylabel(\"PC{}\".format(2))\nplt.grid()\n\n#Call the function. Use only the 2 PCs.\nmyplot(x_new[:,0:2],np.transpose(pca.components_[0:2, :]))\nplt.show()","07b3d8a1":"print('Explained variance: %.4f' % pca.explained_variance_ratio_.sum())\n","941f417e":"explained_variance=pca.explained_variance_ratio_","643f8500":"explained_variance_list=explained_variance.tolist()","8350a829":"explained_variance_list\nsum(explained_variance_list[0:85])","a7f1ff7d":"print(abs( pca.components_ [0]))","9df382fc":"with plt.style.context('seaborn-ticks'):\n    plt.figure(figsize=(10, 12))\n\n    plt.bar(range(100), explained_variance, alpha=0.8, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","491b1848":"model = PCA(n_components=50).fit(X)\nX_pc = model.transform(X)\n\nn_pcs= model.components_.shape[0]\n\n# get the index of the most important feature on EACH component\n# LIST COMPREHENSION HERE\nmost_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n\n\n# get the names\nmost_important_names = [numerical_features[most_important[i]] for i in range(n_pcs)]\n\n# LIST COMPREHENSION HERE AGAIN\ndic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n\n# build the dataframe\ndf = pd.DataFrame(dic.items())","54517838":"most_important","c5e84f80":"df","54763de6":"# !pip install  autoxgb ","38492673":"!autoxgb train \\\n--train_filename ..\/input\/tabular-playground-series-nov-2021\/train.csv \\\n--test_filename ..\/input\/tabular-playground-series-nov-2021\/test.csv \\\n--id IDX \\\n--target target \\\n--task classification \\\n--time_limit 3600 \\\n--output .\/kk \\\n--use_gpu","2d2e1d6f":"import joblib\n\nxgb_model = joblib.load('.\/kk\/axgb_model.4')\n","e5bc303a":"pred_df=dt.fread('.\/kk\/test_predictions.csv').to_pandas()\n\nfinal_class=[]\nfor i in pred_df.iterrows():\n    current_class=1\n    if i[1][1] >i[1][2]:\n        current_class=0\n        final_class.append(current_class)\n    else:\n         final_class.append(current_class)\n        \n    ","2f66e1f2":"len(final_class)\nset(final_class)","4a9670f8":"ss = dt.fread('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv').to_pandas()\nss['target'] = final_class\nss.to_csv('xgb1111.csv', index=False)\nss.head()","0aaee460":"import gc\nX = train.drop('target', axis=1).copy()\ny = train['target'].copy()\nX_test = test.copy()\n\ndel train\ndel test\ngc.collect()","6dfd2046":"\n\nkf= StratifiedKFold(n_splits=5,shuffle=True,random_state=786)\npreds=[]\nscores=[]\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    model=LGBMClassifier()\n    model.fit(X_train, y_train,\n              eval_set = [(X_valid, y_valid)],\n              verbose = True,\n              early_stopping_rounds = 300)\n    \n    pred_valid = model.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\" \"\\n\")\n    print('||'*40, \"\\n\")\n    \n    test_preds = model.predict_proba(X_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","a6659a84":"model.booster_.save_model('model_lgbm.txt')","fbb6142f":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['target'] = predictions\nss.to_csv('lgbm_submission1.csv', index=False)\nss.head()","559eb977":"!pip install optuna","0d6eaa1b":" params = {\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 100.0, log=True),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 100.0, log=True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 0, 100),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1.0),\n        \"colsample_bylevel\": trial.suggest_uniform(\"colsample_bylevel\", 0.1, 1.0),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 15),\n        \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 100, 500),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [7000, 15000, 20000]),\n        \"booster\": \"gbtree\",\n        \"eval_metric\": \"auc\",\n        \"tree_method\": \"gpu_hist\",\n        \"predictor\": \"gpu_predictor\",\n        \"use_label_encoder\": True\n    }","2cbed4f1":"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\ndef optimize(trial,x=X,y=y):\n    kf = StratifiedKFold(n_splits=2, shuffle=True, random_state=786)\n    \n       \n    \n\n    preds = []\n    scores = []\n\n    for fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n        reg_lambda= trial.suggest_float(\"reg_lambda\", 1e-8, 100.0, log=True)\n        reg_alpha= trial.suggest_float(\"reg_alpha\", 1e-8, 100.0, log=True)\n        subsample= trial.suggest_float(\"subsample\", 0.1, 1.0)\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 0, 100)\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n        max_depth=trial.suggest_int(\"max_depth\", 1, 15)\n        early_stopping_rounds=trial.suggest_int(\"early_stopping_rounds\", 100, 500)\n        \n        \n        \n        \n        \n        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n        \n        \n        model = XGBClassifier(n_estimators=14000,\n                  learning_rate=learning_rate,\n                  reg_lambda=reg_lambda,\n                  reg_alpha=reg_alpha,\n                  subsample=subsample,\n                  min_child_weight=min_child_weight,\n                  colsample_bytree=colsample_bytree,\n                  max_depth=max_depth,\n                  early_stopping_rounds=early_stopping_rounds,\n                  tree_method=\"gpu_hist\",\n                  predictor=\"gpu_predictor\",\n                  eval_metric= 'auc',\n                  use_label_encoder= True)\n\n        model.fit(X_train,y_train,eval_set=[(X_train, y_train),(X_valid,y_valid)],\n                  verbose=True)\n\n        pred_valid = model.predict_proba(X_valid)[:,1]\n        fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n        score = auc(fpr, tpr)\n        scores.append(score)\n\n        print(f\"Fold: {fold + 1} Score: {score}\")\n        print('||'*40)\n\n        test_preds = model.predict_proba(X_test)[:,1]\n        preds.append(test_preds)\n\n    print(f\"Overall Validation Score: {np.mean(scores)}\")","ea6a49d3":"from functools import partial\nstudy = optuna.create_study(direction=\"minimize\", study_name=\"Xgb Classifier\")\noptimization_function=partial(optimize,x=X,y=y)\nstudy.optimize(optimization_function, n_trials=20)","e675c1b6":"study.best_params","f3130346":"X = train.drop('target', axis=1).copy()\ny = train['target'].copy()\nX_test = test.copy()\n\ndel train\ndel test","8ec79d15":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=786)\n\npreds = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    model3 = XGBClassifier(**params)\n    \n    model3.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=True)\n    \n    pred_valid = model3.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    test_preds = model3.predict_proba(X_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","922ca681":"# WORK IN PROGRESS FOR OPTUNA HP tuning, also will add averaging of submissions later.","125af7e8":"![Places-to-visit-in-November.jpg](attachment:0692cb4e-c947-4a62-aaae-bce603a5ad2a.jpg)","f0c1e489":"# \ud83d\udd25TPS NOVEMBER \n## This Notebook is purely the implementation of the autoxgb written by Abhishek https:\/\/www.kaggle.com\/abhishek\n### I have done a simple EDA and trained using Autoxgb, loaded the model and created the submission file.\n### Note- No FE was done and it's a single model.","511454f2":"### WOW just wow\n","bc5a7223":"### NOTE- There are no categorical features"}}