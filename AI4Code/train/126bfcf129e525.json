{"cell_type":{"00ce7b11":"code","a631cbcc":"code","edab422f":"code","f3a0d399":"code","20dc2f5e":"code","21a9bb19":"code","b0620b70":"code","ef6b49a2":"code","9b19522f":"code","d9adff9a":"code","19256d08":"code","73b6942a":"code","339352ec":"code","6d1bf60e":"code","9772cb26":"code","7201913a":"code","35183bbc":"code","bb1649fa":"code","c754c888":"code","cdce54d5":"code","2d644082":"code","57e47786":"code","a4998d5c":"code","eeb5e465":"code","bd7648f5":"code","42861007":"code","060cbdff":"code","628bbb9d":"code","34309058":"code","e5e35d77":"code","a976da2c":"code","42e7566e":"code","ddc72a93":"code","a43d2a49":"code","2c7c2aab":"code","187eda2c":"code","23aed077":"code","d0bfbf4a":"code","be6bc087":"code","fc9b059d":"code","7f1d2a26":"code","b55118f1":"code","be5a8e9c":"markdown","fc753ebc":"markdown","070625e8":"markdown","58846ee6":"markdown","0e0cd165":"markdown","eff18069":"markdown","759a6114":"markdown","991c4172":"markdown","2f92b528":"markdown","6618de77":"markdown"},"source":{"00ce7b11":"import numpy as np \nimport pandas as pd\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import *\nimport seaborn as sns\nplt.style.use('ggplot')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))","a631cbcc":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","edab422f":"df_train.head()","f3a0d399":"df_train.info()","20dc2f5e":"df_train.describe()","21a9bb19":"# Check for null values\ndf_train.isnull().any(axis=None)","b0620b70":"ax = sns.countplot(y=df_train['Activity'])\nax.set_title('Distribution of Labels')\nplt.show()","ef6b49a2":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(df_train['Activity'])\ndf_train['Activity'] = le.transform(df_train['Activity'])\ndf_test['Activity'] = le.transform(df_test['Activity'])\nle.classes_","9b19522f":"X = df_train.drop(['Activity', 'subject'], axis=1)\ny = df_train['Activity']\n\nX_test = df_test.drop(['Activity', 'subject'], axis=1)\ny_test = df_test['Activity']","d9adff9a":"from sklearn.ensemble import BaggingClassifier\n\noob_list = list()\n# Because the algorithm is so slow, we use just 4 different trees to see the outcomes.\ntree_list = [20, 40, 50, 100] \n\nfor n_trees in tree_list:\n    BC = BaggingClassifier(n_estimators=n_trees, oob_score=True, random_state=42, n_jobs=-1)\n    BC.fit(X, y)\n    oob_error = 1 - BC.oob_score_   # Get the oob error\n    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n\nerr_bag = pd.concat(oob_list, axis=1).T.set_index('n_trees')","19256d08":"# Plot the result\nax = err_bag.plot(legend=False, marker='o')\nax.set_ylabel('out-of-bag error')\nax.set_title('OOB Error with Bagging Classifier')\nplt.show()","73b6942a":"# Bagging Classifier with 50 estimators\nmodel = BaggingClassifier(n_estimators=50, oob_score=True, random_state=42, n_jobs=-1)\nmodel = model.fit(X, y)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","339352ec":"# Plot confusion Matrix\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=(10, 8))\nplt.show()","6d1bf60e":"y_probas = model.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10, 8))   # Plot ROC Curve\nplt.show()","9772cb26":"from sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(oob_score=True, random_state=42, warm_start=True, n_jobs=-1)\n\noob_list = list()\ntree_list = [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]\n\nfor n_trees in tree_list:\n    RF.set_params(n_estimators=n_trees)\n    RF.fit(X, y)\n    oob_error = 1 - RF.oob_score_\n    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n    \nerr_rf = pd.concat(oob_list, axis=1).T.set_index('n_trees')","7201913a":"# Plot the result\nax = err_rf.plot(legend=False, marker='o')\nax.set_ylabel('out-of-bag error')\nax.set_title('OOB Error with Random Forest')\nplt.show()","35183bbc":"# Random Forest with 100 estimators\nmodel = RF.set_params(n_estimators=100)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","bb1649fa":"# Plot confusion Matrix\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=(10, 8))\nplt.show()","c754c888":"y_probas = model.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10, 8))   # Plot ROC Curve\nplt.show()","cdce54d5":"from sklearn.ensemble import ExtraTreesClassifier\n\nET = ExtraTreesClassifier(oob_score=True, bootstrap=True, random_state=42, warm_start=True, n_jobs=-1)\n\noob_list = list()\ntree_list = [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]\n\nfor n_trees in tree_list:\n    ET.set_params(n_estimators=n_trees)\n    ET.fit(X, y)\n    oob_error = 1 - ET.oob_score_\n    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n    \nerr_et = pd.concat(oob_list, axis=1).T.set_index('n_trees')","2d644082":"# Plot the result\nax = err_et.plot(legend=False, marker='o')\nax.set_ylabel('out-of-bag error')\nax.set_title('OOB Error with Extra Trees')\nplt.show()","57e47786":"# Extra Trees with 100 estimators\nmodel = ET.set_params(n_estimators=100)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","a4998d5c":"# Plot confusion Matrix\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=(10, 8))\nplt.show()","eeb5e465":"y_probas = model.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10, 8))   # Plot ROC Curve\nplt.show()","bd7648f5":"from sklearn.ensemble import GradientBoostingClassifier\n\nerror_list = list()\n\n# Iterate through all of the possibilities for number of estimators\ntree_list = [15, 50, 100, 200, 400]\nfor n_trees in tree_list:\n    GBC = GradientBoostingClassifier(n_estimators=n_trees, subsample=0.5,\n                                     max_features=4, random_state=42)\n    GBC.fit(X, y)\n    y_pred = GBC.predict(X_test)\n\n    # Get the error\n    error = 1. - accuracy_score(y_test, y_pred)\n    error_list.append(pd.Series({'n_trees': n_trees, 'error': error}))\n\nerr_gbc = pd.concat(error_list, axis=1).T.set_index('n_trees')","42861007":"# Plot the result\nax = err_gbc.plot(legend=False, marker='o')\nax.set_ylabel('deviance error')\nax.set_title('Error with Gradient Boosting')\nplt.show()","060cbdff":"# Extra Trees with 100 estimators\nmodel = GradientBoostingClassifier(n_estimators=100, subsample=0.5,\n                                     max_features=4, random_state=42)\nmodel.fit(X, y)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","628bbb9d":"# Plot confusion Matrix\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=(10, 8))\nplt.show()","34309058":"y_probas = model.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10, 8))   # Plot ROC Curve\nplt.show()","e5e35d77":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nerror_list = list()\n\n# Iterate through all of the possibilities for number of estimators\ntree_list = [15, 50, 100, 200, 400]\n\n# Setting max_features=4 in the decision tree classifier used as the base classifier \n# for AdaBoost will increase the convergence rate\nbase = DecisionTreeClassifier(max_features=4)\nfor n_trees in tree_list:\n    ABC = AdaBoostClassifier(base_estimator=base, n_estimators=n_trees, \n                             learning_rate=0.1, random_state=42)\n    ABC.fit(X, y)\n    y_pred = ABC.predict(X_test)\n\n    # Get the error\n    error = 1. - accuracy_score(y_test, y_pred)\n    error_list.append(pd.Series({'n_trees': n_trees, 'error': error}))\n\nerr_abc = pd.concat(error_list, axis=1).T.set_index('n_trees')","a976da2c":"# Plot the result\nax = err_abc.plot(legend=False, marker='o')\nax.set_ylabel('adaptive boosting error')\nax.set_title('Error with Adaptive Boosting')\nplt.show()","42e7566e":"model = AdaBoostClassifier(base_estimator=base, n_estimators=50, \n                             learning_rate=0.1, random_state=42)\nmodel.fit(X, y)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","ddc72a93":"# Plot confusion Matrix\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=(10, 8))\nplt.show()","a43d2a49":"y_probas = model.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10, 8))   # Plot ROC Curve\nplt.show()","2c7c2aab":"from sklearn.linear_model import LogisticRegressionCV\n\n# L2 regularized logistic regression\nLR = LogisticRegressionCV(Cs=5, cv=4, penalty='l2', max_iter=1000)\nLR.fit(X, y)","187eda2c":"y_pred = LR.predict(X_test)\nprint(classification_report(y_pred, y_test))","23aed077":"# Plot confusion Matrix\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=(10, 8))\nplt.show()","d0bfbf4a":"y_probas = LR.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10, 8))   # Plot ROC Curve\nplt.show()","be6bc087":"from sklearn.ensemble import VotingClassifier\n\n# The combined model--logistic regression and gradient boosted trees\nestimators = [('LR', LR), ('GBC', GBC)]\n\n# Though it wasn't done here, it is often desirable to train \n# this model using an additional hold-out data set and\/or with cross validation\nVC = VotingClassifier(estimators, voting='soft', n_jobs=-1)\nVC.fit(X, y)","fc9b059d":"y_pred = VC.predict(X_test)\nprint(classification_report(y_test, y_pred))","7f1d2a26":"# Plot confusion Matrix\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=(10, 8))\nplt.show()","b55118f1":"y_probas = VC.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_probas, figsize=(10, 8))   # Plot ROC Curve\nplt.show()","be5a8e9c":"Now, we stack Logistic regression with Gradient Boosting to make a new classifier","fc753ebc":"## Boosting \n\n<img src='https:\/\/res.cloudinary.com\/dpyleb8ih\/image\/upload\/v1555167571\/boost.png'>\n\nBoosting refers to a group of algorithms that utilize weighted averages to make weak learners into stronger learners. Unlike bagging that had each model run independently and then aggregate the outputs at the end without preference to any model. Boosting is all about \u201cteamwork\u201d. Each model that runs, dictates what features the next model will focus on. Result is weighted sum of all classifiers. Successive classifiers are weighted by learning rate (\u03bb). Using a learning rate < 1.0 helps prevent overfitting (regularization). \n\n### Bagging vs Boosting\n\n| Bagging  | Boosting |\n|---|---|\n|Bootstrapped samples   | Fit entire data set  |\n|Base trees created independently   | Base trees created successively  |\n|Only data points considered   |Use residuals from previous models   |\n|No weighting used   |Up-weight misclassified points   |\n|Excess trees will not overfit | Beware of overfitting |  \n<br>  \n\nAlso, there is no such thing as out-of-bag error for boosted models. Boosting utilizes different loss\nfunctions. At each stage, the margin is determined for each point and is positive for correctly\nclassified points and negative for misclassifications. Value of loss function is calculated\nfrom margin.\n\n<img src='https:\/\/res.cloudinary.com\/dpyleb8ih\/image\/upload\/v1555168208\/loss.png'>\n\n**0-1 Loss Function**\n- The 0 \u2013 1 Loss multiplies misclassified points by 1\n- Correctly classified points are ignored\n- Theoretical \"ideal\" loss function\n- Difficult to optimize non-smooth and non-convex\n\n**AdaBoost Loss Function**\n- AdaBoost = Adaptive Boosting\n- Loss function is exponential: e (\u2212margin)\n- Makes AdaBoost more sensitive to outliers than other types of boosting\n\n**Gradient Boosting Loss Function**\n- Generalized boosting method that can use different loss functions\n- Common implementation uses binomial log likelihood loss function (deviance): log(1 + e (\u2212margin) )\n- More robust to outliers than AdaBoost\n\n**Tuning a Gradient Boosted Model**\n\n- Boosting is additive, so possible to overfit. \n- Use cross validation to set number of trees\n- Learning rate (\u03bb): set to <1.0 for regularization. That\u2019s also called \u201cshrinkage\u201d\n- Subsample: set to <1.0 to use fraction of data for base learners (stochastic gradient boosting)\n- Max_features: number of features to consider in base learners when splitting\n\nAnd the `warm_flag=True` setting has a bug in the gradient boosted model, so don't use it. Additionally, boosting models tend to take longer to fit than bagged ones because the decision stumps must be fit successively.","070625e8":"### Random Forest\nThe fundamental difference between Bagging and Random Forest is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node.\n\nBagging has a single parameter, which is the number of trees. All trees are fully grown binary tree (unpruned) and at each node in the tree one searches over all features to find the feature that best splits the data at that node.\n\nRandom forests has 2 parameters:\nThe first parameter is the same as bagging (the number of trees)\nThe second parameter (unique to randomforests) is mtry which is how many features to search over to find the best feature. this parameter is usually 1\/3*D for regression and sqrt(D) for classification. thus during tree creation randomly mtry number of features are chosen from all available features and the best feature that splits the data is chosen.","58846ee6":"## Conclusions \n\nOur best model appears to be the stacked model judjin by the confusion matrix and the ROC curve. Also note that, Logistic regression itself performs better than most of the complicated models. So, always start simple and make it complex as you need it.\n\nDon't forget to upvote if you like my kernel :)","0e0cd165":"We don't have much of a class imbalanced data so we can perform our algorithms safely. \n\n## Bagging \n<img src=\"https:\/\/res.cloudinary.com\/dpyleb8ih\/image\/upload\/v1555159889\/bagg.png\">\n\n\nBootstrap Aggregation (or Bagging for short), is a simple and very powerful ensemble method. Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees. When bagging with decision trees, we are less concerned about individual trees overfitting the training data. For this reason and for efficiency, the individual decision trees are grown deep and the trees are not pruned. These trees will have both high variance and low bias. These are important characterize of sub-models when combining predictions using bagging. The only parameters when bagging decision trees is the number of samples and hence the number of trees to include. This can be chosen by increasing the number of trees on run after run until the accuracy begins to stop showing improvement\n\n<img src=\"https:\/\/res.cloudinary.com\/dpyleb8ih\/image\/upload\/v1555159889\/voting.png\">\n\nSince the only thing changing is the number of trees, the `warm_start` flag can be used so that the model just adds more trees to the existing model each time. Use the `set_params` method to update the number of trees. This way we don't have to train the data from scracth all the time. Instead, we just train more trees and add them to our model. Warm start produces errors for baggingclassifier and boosting estimators. So for that situation, we will do it the normal way.   \n\n### Bagging Classifier","eff18069":"## Stacking \n\n<img src='https:\/\/res.cloudinary.com\/dpyleb8ih\/image\/upload\/v1555167571\/stack.png'>\n\nStacking is another ensemble model, where a new model is trained from the combined predictions of two (or more) previous model. The predictions from the models are used as inputs for each sequential layer, and combined to form a new set of predictions. These can be used on additional layers, or the process can stop here with a final result.\n\nEnsemble stacking can be referred to as blending, because all the numbers are blended to produce a prediction or classification.\n\n- Output of base learners can be combined via majority vote or weighted\n- Additional hold-out data needed if meta learner parameters are used\n- Be aware of increasing model complexity","759a6114":"## About Data\nWe will be using the Human Activity Recognition with Smartphones data, which was built from the recordings of study participants performing activities of daily living (ADL) while carrying a smartphone with an embedded inertial sensors. The goal is to classify activities into one of the six activities (walking, walking upstairs, walking downstairs, sitting, standing, and laying) performed.\n\nFor each record in the dataset it is provided: \n\n- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration. \n- Triaxial angular velocity from the gyroscope. \n- A 561-feature vector with time and frequency domain variables. \n- Its activity label. ","991c4172":"Data has already been scaled. Therefore, no more preprocessing needed except to check for null values and the distrubution of the dataset.","2f92b528":"### Extra Trees Classifier\n\nSometimes additional randomness is desired beyond Random Forest.Solution is to select features randomly and create splits randomly\u2014don't choose greedily. This is called \u201cExtra Random Trees\u201d","6618de77":"# Ensemble Learning\nEnsemble learning is a machine learning paradigm where multiple learners are trained to solve the same problem. In contrast to ordinary machine learning approaches which try to learn one hypothesis from training data, ensemble methods try to construct a set of hypotheses and combine them to use. The main principle is that a group of weak learners come together to form a strong learner, thus increasing the accuracy of the model. When we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are noise, variance, and bias. Ensemble helps to reduce these factors (except noise, which is irreducible error). We will see 3 different areas meaning, __Bagging__, __Boosting__, __Stacking__ but before we start understanding the algorithms, let's understand what bootstrapping means as it will be helpful to understand the algorithms much better.\n\n## Bootstrapping\nIn statistics, bootstrapping is any test or metric that relies on random sampling with replacement. Bootstrapping allows assigning measures of accuracy to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods. Importantly, samples are constructed by drawing observations from a large data sample one at a time and returning them to the data sample after they have been chosen. This allows a given observation to be included in a given small sample more than once. This approach to sampling is called sampling with replacement.\n\nLet's start with importing our libraries\n"}}