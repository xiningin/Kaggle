{"cell_type":{"7cd994e1":"code","24ac2cae":"code","2145913b":"code","55ec71af":"code","a9f31187":"code","42bc496a":"code","dc60d77b":"code","da04b067":"code","2e5328a3":"code","e2e2712e":"code","8f1d384a":"code","19fc8245":"code","0e928939":"code","72a47009":"code","7b1be2b7":"code","b02564fd":"code","21ea5c0b":"code","86a601a1":"code","eff5ea9f":"code","d485e6ad":"code","d88a50f2":"code","f0586b65":"code","9cc361ca":"code","3bb4d03f":"code","b2ac5706":"code","fc0f1e70":"code","89011951":"code","068be220":"code","41939370":"code","d7b964fc":"code","ca6bca38":"code","60823918":"code","bf931246":"code","6e26a5c8":"code","536d4078":"code","9fe2b807":"code","eb73fc02":"code","f0eab9c7":"code","1746e313":"code","053245dd":"code","276b11cc":"code","76c83e98":"code","457c1d2c":"code","07f3b2f8":"code","2afc7377":"code","c80dc08b":"code","d8f687a7":"code","4cdb5d9b":"code","c19d1429":"code","6ebdc40e":"code","18041901":"code","7b316066":"code","d4159338":"code","ffcc439b":"markdown","bc9560c5":"markdown","369151c5":"markdown","95a3c747":"markdown","6a693c90":"markdown","6aefbacb":"markdown","5bc50b95":"markdown","3ed9ef97":"markdown","bbaa4c12":"markdown","c330c805":"markdown","709c77a4":"markdown","5a80b485":"markdown","19e2cfa4":"markdown","730b1534":"markdown","ffc11361":"markdown","0d771c96":"markdown","1e2a7b78":"markdown","390b38ae":"markdown","ce922cfe":"markdown","0ae7d2f2":"markdown","40c27274":"markdown","cb6eb36e":"markdown","70d223ec":"markdown"},"source":{"7cd994e1":"from pathlib import Path    # platform independent paths\nfrom IPython.display import Markdown, display\nimport pandas as pd\nfrom nltk.probability import FreqDist    # frequency dictionary\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","24ac2cae":"# set figure size\nplt.rcParams['figure.figsize'] = [20, 10]","2145913b":"def printmd(string, color=None):\n    ''' NOT MINE\n    Markdown printing from a code cell\n    Ex. printmd(\"**bold and blue**\", color=\"blue\")\n    https:\/\/stackoverflow.com\/questions\/23271575\/printing-bold-colored-etc-text-in-ipython-qtconsole\n    '''\n    colorstr = \"<span style='color:{}'>{}<\/span>\".format(color, string)\n    display(Markdown(colorstr))","55ec71af":"def freq_dist(some_list, min_freq = 1, exact_freq = 1, top = 0):\n    '''\n    Adding in fucntionality to FreqDist\n    Need to add in raise errors if more than one optional argument used.\n    '''\n    temp = FreqDist(some_list)\n    if (min_freq != 1):\n        temp = [(k, v) for k, v in temp.items() if v > min_freq]\n        temp.sort(key=lambda x: x[1], reverse = True)\n        return temp\n    elif (exact_freq != 1):\n        temp = [(k, v) for k, v in temp.items() if v == exact_freq]\n        return temp\n    elif (top != 0):\n        return temp.most_common(top)\n    else:\n        temp = [(k, v) for k, v in temp.items()]\n        temp.sort(key=lambda x: x[1], reverse = True)\n        return temp","a9f31187":"# File locations\nfile_neg = Path('..\/input\/comments_negative.csv')\nfile_pos = Path('..\/input\/comments_positive.csv')","42bc496a":"# Load into dataframes\ndf_neg = pd.read_csv(file_neg)\ndf_pos = pd.read_csv(file_pos)","dc60d77b":"#df_neg = df_neg.sample(n=100000, random_state=1)\n#df_pos = df_pos.sample(n=100000, random_state=1)","da04b067":"df_neg.head()","2e5328a3":"df_pos.head()","e2e2712e":"totalDF = pd.concat([df_pos,df_neg])","8f1d384a":"totalDF = totalDF[totalDF['text'].notnull()]\ntotalDF = totalDF[totalDF['parent_text'].notnull()]","19fc8245":"df_pos = 5\ndf_neg = 5","0e928939":"conditions = [\n    (totalDF['score'] < -1000) ,\n    (totalDF['score'] > -1000) & (totalDF['score'] < -100),\n    (totalDF['score'] > -100) & (totalDF['score'] < 100),\n    (totalDF['score'] > 100) & (totalDF['score'] < 1000),\n    (totalDF['score'] > 1000)]\nchoices = [-2, -1, 0,1,2]\ntotalDF['category'] = np.select(conditions, choices, default=0)","72a47009":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(totalDF, totalDF[\"category\"], test_size=0.5)","7b1be2b7":"\n\n!pip install bert\n\n","b02564fd":"!pip install bert-tensorflow","21ea5c0b":"import tensorflow as tf\nimport pandas as pd\nimport tensorflow_hub as hub\nimport os\nimport re\nimport numpy as np\nfrom bert.tokenization import FullTokenizer\nfrom tqdm import tqdm_notebook\nfrom tensorflow.keras import backend as K\n","86a601a1":"# Initialize session\nsess = tf.Session()\n\n# Params for bert model and tokenization\nbert_path = \"https:\/\/tfhub.dev\/google\/bert_uncased_L-12_H-768_A-12\/1\"","eff5ea9f":"X_train =  X_train.sample(n=500000, random_state=1)\n#X_train =  X_train.sample(n=100000, random_state=1)","d485e6ad":"X_train.head","d88a50f2":"#max_seq_length = 32\nmax_seq_length = 18\n\n# Create datasets (Only take up to max_seq_length words for memory)\ntrain_text = X_train['text'].tolist()\ntrain_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\ntrain_text = np.array(train_text, dtype=object)[:, np.newaxis]\n\n\n#train_label = totalDF['score'].tolist()\ntrain_label = X_train['category'].tolist()\n\ntrain2_text = X_train['parent_text'].tolist()\ntrain2_text = [' '.join(t.split()[0:max_seq_length]) for t in train2_text]\ntrain2_text = np.array(train2_text, dtype=object)[:, np.newaxis]\n\ntest_text = X_test['text'].tolist()\ntest_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\ntest_text = np.array(test_text, dtype=object)[:, np.newaxis]\n\ntest2_text = X_test['parent_text'].tolist()\ntest2_text = [' '.join(t.split()[0:max_seq_length]) for t in test2_text]\ntest2_text = np.array(test2_text, dtype=object)[:, np.newaxis]\n\ntest_label =  X_test['category'].tolist()","f0586b65":"np.unique(train_label)","9cc361ca":"np.unique(test_label)","3bb4d03f":"# train_label = totalDF[.tolist()\n# train_text = totalDF['text'].tolist()","b2ac5706":"class PaddingInputExample(object):\n    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n  When running eval\/predict on the TPU, we need to pad the number of examples\n  to be a multiple of the batch size, because the TPU requires a fixed batch\n  size. The alternative is to drop the last batch, which is bad because it means\n  the entire output data won't be generated.\n  We use this class instead of `None` because treating `None` as padding\n  battches could cause silent errors.\n  \"\"\"\n\nclass InputExample(object):\n    \"\"\"A single training\/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\ndef create_tokenizer_from_hub_module():\n    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n    bert_module =  hub.Module(bert_path)\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n    vocab_file, do_lower_case = sess.run(\n        [\n            tokenization_info[\"vocab_file\"],\n            tokenization_info[\"do_lower_case\"],\n        ]\n    )\n\n    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n\ndef convert_single_example(tokenizer, example, max_seq_length=256):\n    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n\n    if isinstance(example, PaddingInputExample):\n        input_ids = [0] * max_seq_length\n        input_mask = [0] * max_seq_length\n        segment_ids = [0] * max_seq_length\n        label = 0\n        return input_ids, input_mask, segment_ids, label\n\n    tokens_a = tokenizer.tokenize(example.text_a)\n    if len(tokens_a) > max_seq_length - 2:\n        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n\n    tokens = []\n    segment_ids = []\n    tokens.append(\"[CLS]\")\n    segment_ids.append(0)\n    for token in tokens_a:\n        tokens.append(token)\n        segment_ids.append(0)\n    tokens.append(\"[SEP]\")\n    segment_ids.append(0)\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    return input_ids, input_mask, segment_ids, example.label\n\ndef convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n\n    input_ids, input_masks, segment_ids, labels = [], [], [], []\n    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n        input_id, input_mask, segment_id, label = convert_single_example(\n            tokenizer, example, max_seq_length\n        )\n        input_ids.append(input_id)\n        input_masks.append(input_mask)\n        segment_ids.append(segment_id)\n        labels.append(label)\n    return (\n        np.array(input_ids),\n        np.array(input_masks),\n        np.array(segment_ids),\n        np.array(labels).reshape(-1, 1),\n    )\n\ndef convert_text_to_examples(texts, labels):\n    \"\"\"Create InputExamples\"\"\"\n    InputExamples = []\n    for text, label in zip(texts, labels):\n        InputExamples.append(\n            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n        )\n    return InputExamples\n\n# Instantiate tokenizer\ntokenizer = create_tokenizer_from_hub_module()\n\n# Convert data to InputExample format\ntrain_examples = convert_text_to_examples(train_text, train_label)\ntrain2_examples = convert_text_to_examples(train2_text, train_label)\ntest_examples = convert_text_to_examples(test_text, test_label)\ntest2_examples = convert_text_to_examples(test2_text, test_label)\n\n# Convert to features\n(train_input_ids, train_input_masks, train_segment_ids, train_labels \n) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n\n(train2_input_ids, train2_input_masks, train2_segment_ids, train_labels \n) = convert_examples_to_features(tokenizer, train2_examples, max_seq_length=max_seq_length)\n\n","fc0f1e70":"train_label[4]","89011951":"import sklearn\nlabel_binarizer = sklearn.preprocessing.LabelBinarizer()\nlabel_binarizer.fit([-2,-1,0,1,2])\ntrain_label_bak = train_labels\ntrain_labels = label_binarizer.transform(train_labels)\n#test_labels = label_binarizer.transform(test_labels)","068be220":"plt.hist(train_label_bak,10, facecolor='blue', alpha=0.5)\nplt.show()","41939370":"train_labels[1]","d7b964fc":"#train_labels = pd.get_dummies(train_labels)","ca6bca38":"class BertLayer(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        n_fine_tune_layers=10,\n        pooling=\"first\",\n        bert_path=\"https:\/\/tfhub.dev\/google\/bert_uncased_L-12_H-768_A-12\/1\",\n        **kwargs,\n    ):\n        self.n_fine_tune_layers = n_fine_tune_layers\n        self.trainable = True\n        self.output_size = 768\n        self.pooling = pooling\n        self.bert_path = bert_path\n        if self.pooling not in [\"first\", \"mean\"]:\n            raise NameError(\n                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n            )\n\n        super(BertLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.bert = hub.Module(\n            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n        )\n\n        # Remove unused layers\n        trainable_vars = self.bert.variables\n        if self.pooling == \"first\":\n            trainable_vars = [var for var in trainable_vars if not \"\/cls\/\" in var.name]\n            trainable_layers = [\"pooler\/dense\"]\n\n        elif self.pooling == \"mean\":\n            trainable_vars = [\n                var\n                for var in trainable_vars\n                if not \"\/cls\/\" in var.name and not \"\/pooler\/\" in var.name\n            ]\n            trainable_layers = []\n        else:\n            raise NameError(\n                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n            )\n\n        # Select how many layers to fine tune\n        for i in range(self.n_fine_tune_layers):\n            trainable_layers.append(f\"encoder\/layer_{str(11 - i)}\")\n\n        # Update trainable vars to contain only the specified layers\n        trainable_vars = [\n            var\n            for var in trainable_vars\n            if any([l in var.name for l in trainable_layers])\n        ]\n\n        # Add to trainable weights\n        for var in trainable_vars:\n            self._trainable_weights.append(var)\n\n        for var in self.bert.variables:\n            if var not in self._trainable_weights:\n                self._non_trainable_weights.append(var)\n\n        super(BertLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n        input_ids, input_mask, segment_ids = inputs\n        bert_inputs = dict(\n            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n        )\n        if self.pooling == \"first\":\n            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n                \"pooled_output\"\n            ]\n        elif self.pooling == \"mean\":\n            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n                \"sequence_output\"\n            ]\n\n            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) \/ (\n                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n            input_mask = tf.cast(input_mask, tf.float32)\n            pooled = masked_reduce_mean(result, input_mask)\n        else:\n            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n\n        return pooled\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.output_size)","60823918":"# define metrics to measure during runtime of keras\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n# Build model\ndef build_model(max_seq_length): \n    # add width to acommodate combined parent_text and text features\n    in_id = tf.keras.layers.Input(shape=(max_seq_length*2,), name=\"input_ids\")\n    in_mask = tf.keras.layers.Input(shape=(max_seq_length*2,), name=\"input_masks\")\n    in_segment = tf.keras.layers.Input(shape=(max_seq_length*2,), name=\"segment_ids\")\n    bert_inputs = [in_id, in_mask, in_segment]\n    \n    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n    #dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n    dense = tf.keras.layers.Dense(42, activation='relu')(bert_output)\n    #pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n    pred = tf.keras.layers.Dense(5, activation='softmax')(dense)\n    \n    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1])\n    model.summary()\n    \n    return model\n\ndef initialize_vars(sess):\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.tables_initializer())\n    K.set_session(sess)\n\n","bf931246":"train3_input_ids = np.hstack((train_input_ids, train2_input_ids))\ntrain3_input_masks = np.hstack((train_input_masks, train2_input_masks))\ntrain3_segment_ids = np.hstack((train_segment_ids, train2_segment_ids))","6e26a5c8":"train_labels.shape","536d4078":"model = build_model(max_seq_length)\n\n# Instantiate variables\ninitialize_vars(sess)\n\nmodel.fit(\n    [train3_input_ids, train3_input_masks, train3_segment_ids], \n    train_labels,\n   # validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n    epochs=1,\n    batch_size=32\n)","9fe2b807":"model.save('BertModel.h5')\n# pre_save_preds = model.predict([test3_input_ids[0:100], \n#                                 test3_input_masks[0:100], \n#                                 test3_segment_ids[0:100]]\n#                               ) # predictions before we clear and reload model\n\n# # Clear and load model\n# model = None\n# model = build_model(max_seq_length)\n# initialize_vars(sess)\n# model.load_weights('BertModel.h5')\n\n#post_save_preds = model.predict([test3_input_ids[0:100], \n                             #   test3_input_masks[0:100], \n                           #     test3_segment_ids[0:100]]\n                         #     ) # predictions after we clear and reload model\n#all(pre_save_preds == post_save_preds) # Are they the same?","eb73fc02":"scores = model.evaluate( [train3_input_ids, train3_input_masks, train3_segment_ids], \n    train_labels, \n                        verbose=1)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","f0eab9c7":"# check class performance for very negative posts\nVNeg = train_labels[:,0] == 1\n\n\n\n\nscores = model.evaluate( [train3_input_ids[VNeg], train3_input_masks[VNeg], train3_segment_ids[VNeg]], \n    train_labels[VNeg], \n                        verbose=1)\nprint(\"Very Negative Class performance %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","1746e313":"# check class performance for very postive posts\nVNeg = train_labels[:,4] == 1\n\n\n\nscores = model.evaluate( [train3_input_ids[VNeg], train3_input_masks[VNeg], train3_segment_ids[VNeg]], \n    train_labels[VNeg], \n                        verbose=1)\nprint(\"Very Positive Class performance %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","053245dd":"from keras.layers import Input, Conv1D, Dense, concatenate","276b11cc":"def build_model2(max_seq_length): \n    in_id = tf.keras.layers.Input(shape=(max_seq_length*2,), name=\"input_ids\")\n    in_mask = tf.keras.layers.Input(shape=(max_seq_length*2,), name=\"input_masks\")\n    in_segment = tf.keras.layers.Input(shape=(max_seq_length*2,), name=\"segment_ids\")\n\n        \n    in_non_bert = tf.keras.layers.Input(shape=(1,), name=\"parent_score\")\n    \n    all_inputs = [in_id, in_mask, in_segment,in_non_bert]\n    bert_inputs = [in_id, in_mask, in_segment]\n\n    \n    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n    #dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n    merged = tf.keras.layers.concatenate([bert_output, in_non_bert])\n    dense = tf.keras.layers.Dense(42, activation='relu')( merged)\n    #pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n    pred = tf.keras.layers.Dense(5, activation='softmax')(dense)\n    \n    model = tf.keras.models.Model(inputs=all_inputs , outputs=pred)\n    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1])\n    model.summary()\n    \n    return model","76c83e98":"train_parent_scores = X_train['parent_score']","457c1d2c":"train_parent_scores = train_parent_scores.as_matrix()","07f3b2f8":"model2 = build_model2(max_seq_length)\n\n# Instantiate variables\ninitialize_vars(sess)\n\nmodel2.fit(\n    [train3_input_ids, train3_input_masks, train3_segment_ids,train_parent_scores], \n    train_labels,\n   # validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n    epochs=1,\n    batch_size=32\n)","2afc7377":"scores = model2.evaluate( [train3_input_ids, train3_input_masks, train3_segment_ids,train_parent_scores], \n    train_labels, \n                        verbose=1)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","c80dc08b":"# check class performance for very postive posts\nVNeg = train_labels[:,4] == 1\n\n\n\nscores = model2.evaluate( [train3_input_ids[VNeg], train3_input_masks[VNeg], train3_segment_ids[VNeg]], \n    train_labels[VNeg], \n                        verbose=1)\nprint(\"Very Positive Class performance %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","d8f687a7":"# check class performance for very postive posts\nVNeg = train_labels[:,0] == 1\n\n\n\nscores = model2.evaluate( [train3_input_ids[VNeg], train3_input_masks[VNeg], train3_segment_ids[VNeg]], \n    train_labels[VNeg], \n                        verbose=1)\nprint(\"Very Positive Class performance %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","4cdb5d9b":"(test_input_ids, test_input_masks, test_segment_ids, test_labels \n) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)\n\n(test2_input_ids, test2_input_masks, test2_segment_ids, test_labels \n) = convert_examples_to_features(tokenizer, test2_examples, max_seq_length=max_seq_length)","c19d1429":"test3_input_ids = np.hstack((test_input_ids, test2_input_ids))\ntest3_input_masks = np.hstack((test_input_masks, test2_input_masks))\ntest3_segment_ids = np.hstack((test_segment_ids, test2_segment_ids))","6ebdc40e":"#test2_examples\ntest_parent_scores = X_test['parent_score']\ntest_parent_scores = test_parent_scores.as_matrix()","18041901":"print(test3_input_ids.shape)\nprint(test3_input_masks.shape)\nprint(test3_segment_ids.shape)\nprint(test_labels.shape)","7b316066":"scores = model.evaluate([test3_input_ids, \n                                test3_input_masks, \n                                test3_segment_ids],\n                        test_labels, \n                        verbose=1)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","d4159338":"scores2 = model2.evaluate([test3_input_ids, \n                                test3_input_masks, \n                                test3_segment_ids,test_parent_scores],\n                        test_labels, \n                        verbose=1)\nprint(\"%s: %.2f%%\" % (model2.metrics_names[1], scores2[1]*100))","ffcc439b":"Evalutate first model with only text features performance (this usually takes 2 hrs on a GTX 1080 level GPU)","bc9560c5":"Split dataset in half to allow test and training, due to using transfer learning from BERT interesting to assess whether training\/fine tuning bert on a small subset of the data is effective","369151c5":"Create a new variable just containing all the training set parent scores","95a3c747":"Run Bert model with combined parent_text and text input parameters as our features","6a693c90":"In prior modelling it was found that parent_score is an especially useful feature, this alone outperformed all other features we used when we had only used TF IDF to vectorise the text features.\n\n\nSo I will build a second model which also uses parent_score in the final relu hidden layer, for this it will have to bypass the bert layer and go directly from input layer to the final hidden layer.","6aefbacb":"tokenize and create test data masks","5bc50b95":"## Load Libraries","3ed9ef97":"combine ids masks and segment ids into one array","bbaa4c12":"Assess the metrics of training error using F1 score, we will check later how well this translates against the full set of test data we split out previously","c330c805":"one hot encode training labels to use with 5 wide NN output layer","709c77a4":"Run the second model with combined text parent_text and Parent_score features","5a80b485":"sample training data to a smaller subset of 100k reddit posts to allow quicker fitting","19e2cfa4":"Create NN model using Bert once again this was taken from Jacob Zweig's notebook","730b1534":"We now have  tokenized masks and segment ids for bert layer to use","ffc11361":"Evalutate second model with text features and parent_score feature's performance (this usually takes 2 hrs on a GTX 1080 level GPU)","0d771c96":"Bin scores to give < -1000, < -100, -100 < 100, >100, >1000 as our labels","1e2a7b78":"Check what label bins are represented in the test and train data (sometimes doesn't capture any -2 scores (<-1000)","390b38ae":"## Load Files","ce922cfe":"This is a modified version of Jacob Zweig's code for using bert with tensorflow https:\/\/towardsdatascience.com\/bert-in-keras-with-tensorflow-hub-76bcbc9417b.\n\nModifications have been made to add parent_text and the text of the reddit post itself as inputs rather than just one text feature.","0ae7d2f2":"## User-defined Functions","40c27274":"Combine positive and negative comment data sets and exclude those without text or parent text fields","cb6eb36e":"I'm very interested in the per class performance especially for -2 and +2 as they're relatively rare","70d223ec":"install bert libraries"}}