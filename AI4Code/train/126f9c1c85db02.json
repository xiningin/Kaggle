{"cell_type":{"53c96ded":"code","30ce7c82":"code","48f41b44":"code","1d5470e6":"code","43663e3d":"code","93bfd463":"code","679e7b7f":"code","6b8227d2":"code","eb98b074":"code","6d839f81":"code","9b45c544":"code","ea2d4346":"code","dcd458b5":"code","adab9396":"code","5c9185f0":"code","4c780b17":"code","5742a5b6":"code","c43e9a01":"code","e868edfb":"code","ad4de560":"code","0fdd679e":"code","9915c672":"code","8d73bc8e":"code","6c160447":"code","aa0cd738":"code","e9e09edf":"code","fee4fbf4":"code","19ee36a6":"code","62453b0d":"code","25c1dc54":"markdown","72aa8462":"markdown","17312af7":"markdown","026eeead":"markdown","405fb156":"markdown","c9e17393":"markdown","4f8b50fe":"markdown","e7bdf3b7":"markdown","bb20b28c":"markdown","ad7df5b7":"markdown","a5777b75":"markdown","01140fee":"markdown","557257ad":"markdown","27c8e3ab":"markdown","bec5fd7b":"markdown","d324051b":"markdown","15c0a657":"markdown","36f852b7":"markdown","65307b65":"markdown","a4ad62ad":"markdown","be88b199":"markdown","59d13043":"markdown","fd3d60da":"markdown","99dbe2dc":"markdown","99a5e86c":"markdown","b98daf79":"markdown","ba550639":"markdown","0c070fd3":"markdown","3df934c2":"markdown","332aa215":"markdown","e52e39d3":"markdown","f52c6ade":"markdown"},"source":{"53c96ded":"# For notebook plotting\n%matplotlib inline\n\n# Standard packages\nimport os\nimport pickle\nimport sqlite3\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# nltk for preprocessing of text data\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\n# sklearn for preprocessing and machine learning models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics  import accuracy_score\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# XGBoost for Machine Learning (Gradient Boosting Machine (GBM))\nimport xgboost as xgb\n\n# Keras for neural networks\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n\n# Random seeds for consistent results\nfrom tensorflow import set_random_seed\nseed = 1234\nnp.random.seed(seed)\nset_random_seed(seed)\n\n# Directory\nKAGGLE_DIR = '..\/input\/'\n\n# List files and file sizes\nprint('\\n# Files and file sizes')\nfor file in os.listdir(KAGGLE_DIR):\n    print('{}| {} MB'.format(file.ljust(30), \n                             str(round(os.path.getsize(KAGGLE_DIR + file) \/ 1000000, 2))))","30ce7c82":"# Read in dataset\nraw_df = pd.read_csv(f'{KAGGLE_DIR}Tweets.csv') \nprint('Raw Dataframe:')\nraw_df.head(1)","48f41b44":"# Select features\ndf = raw_df[['tweet_id', 'text', 'airline_sentiment']]\nprint('Feature selected DataFrame:')\ndf.head(2)","1d5470e6":"# Plot sentiment distribution\ndf['airline_sentiment'].value_counts().plot(kind = 'barh',\n                                            figsize = (15,10));\nplt.title('Distribution of airline sentiment in Kaggle dataset', \n          fontsize = 26, weight = 'bold')\nplt.xlabel('Frequency', fontsize = 22)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20);","43663e3d":"class PreProcessor:\n    '''\n    Easily performs all the standard preprocessing steps\n    like removing stopwords, stemming, etc.\n    Only input that you need to provide is the dataframe and column name for the tweets\n    '''\n    def __init__(self, df, column_name):\n        self.data = df\n        self.conversations = list(self.data[column_name])\n        self.stopwords = set(stopwords.words('english'))\n        self.stemmer = SnowballStemmer(\"english\")\n        self.preprocessed = []\n        \n    def tokenize(self, sentence):\n        '''\n        Splits up words and makes a list of all words in the tweet\n        '''\n        tokenized_sentence = word_tokenize(sentence)\n        return tokenized_sentence\n            \n    def remove_stopwords(self, sentence):\n        '''Removes stopwords like 'a', 'the', 'and', etc.'''\n        filtered_sentence = []\n        for w in sentence:\n            if w not in self.stopwords and len(w) > 1 and w[:2] != '\/\/' and w != 'https': \n                filtered_sentence.append(w)\n        return filtered_sentence\n    \n    def stem(self, sentence):\n        '''\n        Stems certain words to their root form.\n        For example, words like 'computer', 'computation'\n        all get truncated to 'comput'\n        '''\n        return [self.stemmer.stem(word) for word in sentence]\n    \n    def join_to_string(self, sentence):\n        '''\n        Joins the tokenized words to one string.\n        '''\n        return ' '.join(sentence)\n    \n    def full_preprocess(self, n_rows=None):\n        '''\n        Preprocess a selected number of rows and\n        connects them back to strings\n        '''\n        # If nothing is given do it for the whole dataset\n        if n_rows == None:\n            n_rows = len(self.data)\n            \n        # Perform preprocessing\n        for i in range(n_rows):\n            tweet = self.conversations[i]\n            tokenized = self.tokenize(tweet)\n            cleaned = self.remove_stopwords(tokenized)\n            stemmed = self.stem(cleaned)\n            joined = self.join_to_string(stemmed)\n            self.preprocessed.append(joined)\n        return self.preprocessed","93bfd463":"# Preprocess text and put it in a new column\npreprocessor = PreProcessor(df, 'text')\ndf['cleaned_text'] = preprocessor.full_preprocess()","679e7b7f":"# Shuffling so we can get random tweets for the test set\ndf = shuffle(df, random_state=seed)\n# Keep 1000 samples of the data as test set\ntest_set = df[:1000]","6b8227d2":"# Get training and validation data\nX_train, X_val, y_train, y_val = train_test_split(df['cleaned_text'][1000:], \n                                                  df['airline_sentiment'][1000:], \n                                                  test_size=0.2, \n                                                  random_state=seed)\n\n# Get sentiment labels for test set\ny_test = test_set['airline_sentiment']","eb98b074":"# Create matrix based on word frequency in tweets\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train)\nX_val = vectorizer.transform(X_val)\nX_test = vectorizer.transform(test_set['cleaned_text'])","6d839f81":"# Print the size of our data\nprint(f'Training size: {X_train.shape[0]} tweets\\n\\\nValidation size: {X_val.shape[0]} tweets\\n\\\nTest size: {X_test.shape[0]} tweets\\n\\\nAmount of words (columns): {X_train.shape[1]} words')","9b45c544":"# Multinomial Naive Bayes\nmulti_nb = MultinomialNB()\nmulti_nb.fit(X_train, y_train)","ea2d4346":"# Check results\ntrain_pred = multi_nb.predict(X_train)\nval_pred = multi_nb.predict(X_val)\nprint(f'Accuracy on training set (MultinomialNB): {round(accuracy_score(y_train, train_pred)*100, 4)}%')\nprint(f'Accuracy on validation set (MultinomialNB): {round(accuracy_score(y_val,val_pred)*100, 4)}%')","dcd458b5":"# sklearn's Gradient Boosting Classifier (GBM)\ngbm = GradientBoostingClassifier(n_estimators=200, \n                                 max_depth=6, \n                                 random_state=seed)\ngbm.fit(X_train, y_train)","adab9396":"# Check results\ntrain_pred = gbm.predict(X_train)\nval_pred = gbm.predict(X_val)\nprint(f'Accuracy on training set (GBM): {round(accuracy_score(y_train, train_pred)*100, 4)}%')\nprint(f'Accuracy on validation set (GBM): {round(accuracy_score(y_val,val_pred)*100, 4)}%')","5c9185f0":"# Hyperparameters that you can tweak\n# There are a lot more tweakable hyperparameters that you can find at \n# https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\nxgb_params = {'objective' : 'multi:softmax',\n              'eval_metric' : 'mlogloss',\n              'eta' : 0.1,\n              'max_depth' : 6,\n              'num_class' : 3,\n              'lambda' : 0.8,\n              'estimators' : 200,\n              'seed' : seed\n              \n}\n\n# Transform categories into numbers\n# negative = 0, neutral = 1 and positive = 2\ntarget_train = y_train.astype('category').cat.codes\ntarget_val = y_val.astype('category').cat.codes\n\n# Transform data into a matrix so that we can use XGBoost\nd_train = xgb.DMatrix(X_train, label = target_train)\nd_val = xgb.DMatrix(X_val, label = target_val)\n\n# Fit XGBoost\nwatchlist = [(d_train, 'train'), (d_val, 'validation')]\nbst = xgb.train(xgb_params, \n                d_train, \n                400,  \n                watchlist,\n                early_stopping_rounds = 50, \n                verbose_eval = 0)","4c780b17":"# Check results for XGBoost\ntrain_pred = bst.predict(d_train)\nval_pred = bst.predict(d_val)\nprint(f'Accuracy on training set (XGBoost): {round(accuracy_score(target_train, train_pred)*100, 4)}%')\nprint(f'Accuracy on validation set (XGBoost): {round(accuracy_score(target_val, val_pred)*100, 4)}%')","5742a5b6":"# Generator so we can easily feed batches of data to the neural network\ndef batch_generator(X, y, batch_size, shuffle):\n    number_of_batches = X.shape[0]\/batch_size\n    counter = 0\n    sample_index = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(sample_index)\n    while True:\n        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n        X_batch = X[batch_index,:].toarray()\n        y_batch = y[batch_index]\n        counter += 1\n        yield X_batch, y_batch\n        if (counter == number_of_batches):\n            if shuffle:\n                np.random.shuffle(sample_index)\n            counter = 0","c43e9a01":"# Onehot encoding of target variable\n# Negative = [1,0,0], Neutral = [0,1,0], Positive = [0,0,1]\n\n# Initialize sklearn's one-hot encoder class\nonehot_encoder = OneHotEncoder(sparse=False)\n\n# One hot encoding for training set\ninteger_encoded_train = np.array(y_train).reshape(len(y_train), 1)\nonehot_encoded_train = onehot_encoder.fit_transform(integer_encoded_train)\n\n# One hot encoding for validation set\ninteger_encoded_val = np.array(y_val).reshape(len(y_val), 1)\nonehot_encoded_val = onehot_encoder.fit_transform(integer_encoded_val)","e868edfb":"# Neural network architecture\ninitializer = keras.initializers.he_normal(seed=seed)\nactivation = keras.activations.elu\noptimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\nes = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n\n# Build model architecture\nmodel = Sequential()\nmodel.add(Dense(20, activation=activation, kernel_initializer=initializer, input_dim=X_train.shape[1]))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax', kernel_initializer=initializer))\nmodel.compile(optimizer=optimizer,\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Hyperparameters\nepochs = 50\nbatch_size = 32\n\n# Fit the model using the batch_generator\nhist = model.fit_generator(generator=batch_generator(X_train, onehot_encoded_train, batch_size=batch_size, shuffle=True),\n                           epochs=epochs, validation_data=(X_val, onehot_encoded_val),\n                           steps_per_epoch=X_train.shape[0]\/batch_size, callbacks=[es])","ad4de560":"# Visualize Accuracy\nacc = hist.history['acc']\nval_acc = hist.history['val_acc']\n\n# Get the amount of epochs for visualization\nstopped_epoch = es.stopped_epoch\nn_epochs = range(stopped_epoch+1)\n\n# Plot training and validation accuracy\nplt.figure(figsize=(15,5))\nplt.plot(n_epochs, acc)\nplt.plot(n_epochs, val_acc)\nplt.title('Accuracy over epochs', weight='bold', fontsize=22)\nplt.xlabel('Epochs', fontsize=16)\nplt.ylabel('Accuracy', fontsize=16)\nplt.legend(['Training accuracy', 'Validation accuracy'], fontsize=16)\nplt.show()","0fdd679e":"# Check results of neural network model\ntrain_pred = [np.argmax(pred) for pred in model.predict(X_train)]\nval_pred = [np.argmax(pred) for pred in model.predict(X_val)]\nprint(f'Accuracy on training set (Small NN): {round(accuracy_score(target_train, train_pred)*100, 4)}%')\nprint(f'Accuracy on validation set (Small NN): {round(accuracy_score(target_val, val_pred)*100, 4)}%')","9915c672":"# Predict negative for the whole dataset\nnegative_pred_train = ['negative' for _ in range(len(y_train))]\nnegative_pred_val = ['negative' for _ in range(len(y_val))]\n\nprint(f'Accuracy on training set (Always Predict Negative): {round(accuracy_score(y_train, negative_pred_train)*100, 4)}%')\nprint(f'Accuracy on validation set (Always Predict Negative): {round(accuracy_score(y_val, negative_pred_val)*100, 4)}%')","8d73bc8e":"# Predict Always Negative\nnegative_test_pred = ['negative' for _ in range(len(y_test))]\n\n# Multinomial Naive Bayes\nmultinb_test_pred = multi_nb.predict(X_test)\n\n# Gradient Boosting Classifier\ngbm_test_pred = gbm.predict(X_test)\n\n# XGBoost\n# Transform test data for XGBoost\ntarget_test = y_test.astype('category').cat.codes\nd_test = xgb.DMatrix(X_test, label = target_test)\nxgboost_test_pred = bst.predict(d_test)\n\n# Neural Network predictions\nnn_test_pred = [np.argmax(pred) for pred in model.predict(X_test)]","6c160447":"# Get accuracies\nprint(f'Accuracy for Always Predict Negative: {round(accuracy_score(y_test, negative_test_pred)*100, 4)}%')\nprint(f'Accuracy for Multinomial Naive Bayes: {round(accuracy_score(y_test, multinb_test_pred)*100, 4)}%')\nprint(f'Accuracy for Gradient Boosting Classifier: {round(accuracy_score(y_test, gbm_test_pred)*100, 4)}%')\nprint(f'Accuracy for XGBoost: {round(accuracy_score(target_test, xgboost_test_pred)*100, 4)}%')\nprint(f'Accuracy for Neural Network: {round(accuracy_score(target_test, nn_test_pred)*100, 4)}%')","aa0cd738":"# Save XGBoost model\npickle.dump(bst, open('xgboost_sentiment_model.dat', 'wb'))\n\n# Save Neural Network model\nmodel.save('nn_sentiment_model.h5')","e9e09edf":"# Create dataframe with predictions for XGBoost and Neural Network\npred_df = pd.DataFrame({'tweet_id': test_set['tweet_id'],\n                        'text' : test_set['text'],\n                       'xgboost_pred' : xgboost_test_pred.astype(int),\n                       'nn_pred' : nn_test_pred,})\n\n# Change predictions back to string values\npred_df['xgboost_pred'] = pred_df['xgboost_pred'].map({0: 'Negative', 1: 'Neutral', 2 : 'Positive'})\npred_df['nn_pred'] = pred_df['nn_pred'].map({0: 'Negative', 1: 'Neutral', 2 : 'Positive'})","fee4fbf4":"# Check if two models are in agreement\npred_df['pred_agreement'] = (pred_df['xgboost_pred'] == pred_df['nn_pred'])\nprint(f\"The models agree with each other {round(pred_df['pred_agreement'].value_counts()[1]\/len(pred_df)*100, 4)}% of the time.\")","19ee36a6":"# Save to csv\npred_df.to_csv('sentiments.csv', index=False)","62453b0d":"# Final test\ndf_test = pd.read_csv('sentiments.csv'); df_test.head(2)","25c1dc54":"[XGBoost](https:\/\/xgboost.readthedocs.io) is a Python package which provides an efficient implementation of a GBM. It often allows for more elaborate optimization than sklearn's GBM. In my experience, XGBoost often performs slightly better than sklearn's GBM.","72aa8462":"## Preprocessing","17312af7":"### Sklearn's Gradient Boosting Machine (GBM)","026eeead":"## Split data into train, validation and test sets","405fb156":"After tweaking the models for a high validation accuracy here we can see how the models actually perform on new data.","c9e17393":"Rule-based models can be very powerful for sentiment analysis tasks. Since this is a notebook about machine learning models I will not go deeply into rule-based models. It is however interesting to set a benchmark using a simple rule. Since our data is biased toward negative tweets, what is the accuracy if we always predict negative sentiment? ","4f8b50fe":"## Simple neural network","e7bdf3b7":"We save the predictions on our test set for further analysis and reporting.","bb20b28c":"For this section we are mostly interested in feature selection and the distribution of the target variable (airline_sentiment). If the target variable distribution is realistic and plausible we will move on the text preprocessing.","ad7df5b7":"## Save models","a5777b75":"Over the past years neural networks have given us extraordinary results on natural language processing (NLP) tasks with complex implementations. In this implementation however we will go back to basics and created a simple fully-connected neural networks with 20 neurons. The reason for this is that we have limited data and a small neural network model is already very prone to overfitting.","01140fee":"Many machine learning models can only be trained on numerical input in the form of vectors or matrices. To prepare our tweets for the machine learning models we create a [term frequency-inverse document frequency (tf-idf) vectorization](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf). The result of this vectorization is a [sparse matrix](https:\/\/en.wikipedia.org\/wiki\/Sparse_matrix) which contains a convenient representation of our tweets.\n\nThe machine learning will learn which word frequency is important to predict a correct sentiment. ","557257ad":"## Model comparison with test set","27c8e3ab":"## Data Exploration","bec5fd7b":"It seems like both the XGBoost and Neural Network models are the best to keep around. We therefore save the XGBoost model as a '.dat' file and the neural network weights in the '.h5' format.","d324051b":"We split the data into a training set, validation set and test set. This is crucial for training and evaluation of good machine learning models.\n\nThe data will be shuffled and we take 1000 tweets for our test set. The remaining data will be split 80\/20. This means that 80% will be used for training and 20% for validation set. During the model training phase our goal is to maximize the accuracy on the validation set. The test set is to check that our model truly generalizes to data it has never seen before.","15c0a657":"We created a preprocessor class to perform all steps that need to be performed before the text data can be vectorized. These preprocessing steps include:\n1. [Tokenization](https:\/\/en.wikipedia.org\/wiki\/Lexical_analysis#Tokenization)\n2. Removing [stop words](https:\/\/en.wikipedia.org\/wiki\/Stop_words)\n3. [Stemming](https:\/\/en.wikipedia.org\/wiki\/Stemming)\n4. Transform the tokens back to one string","36f852b7":"### Multinomial Naive Bayes","65307b65":"## Save predictions to csv","a4ad62ad":"## Make predictions","be88b199":"Multinomial Naive Bayes is the go-to method for text mining and is great to set a benchmark for sentiment analysis. It is easy to implement with [sklearn's \"MultinomialNB\" class](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html).","59d13043":"The neural network requires us to create a [one-hot encoding](https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/) of our target variable (airline_sentiment). This means that we encode our classes in the following way:\n* negative = [1,0,0]\n* neutral = [0,1,0]\n* positive = [0,0,1]","fd3d60da":"We will put our predictions from the XGBoost and Neural Network models in a DataFrame and make a quick comparison between them. This allows us to analyze tweets where the models do not agree on the prediction. For example, one model can predict negative sentiment while the other model predicts positive sentiment. If this occurs than a human can step in and check what is going wrong.","99dbe2dc":"Some interesting things to note about this network is the initializer and the activation functions.\nWe use [He Initialization](https:\/\/papers.nips.cc\/paper\/7338-how-to-start-training-the-effect-of-initialization-and-architecture.pdf) both to stabilize and to speed up training. He Initialization is designed so that it is more efficient than random initialization or worse, initializing to zeros. \n\nThe [ELU activation function](https:\/\/arxiv.org\/pdf\/1511.07289.pdf) is in most cases more efficient and increases performance relative to other activation like LeakyReLU and ReLU ([Source](https:\/\/arxiv.org\/pdf\/1511.07289.pdf)). It is currently a mystery to me why people still use regular ReLU activations in their neural network models. If you know, please let me know in the comments!","99a5e86c":"### XGBoost (GBM)","b98daf79":"## ML Models","ba550639":"## Data vectorization","0c070fd3":"Note that the dataset is biased toward negative tweets. This is fine as long as the data we are trying to model is also biased towards negative. We are going to assume here that airline tweets outside this dataset also is for \u00b1 60% negative.","3df934c2":"**That's all! If you like this Kaggle kernel, feel free to give an upvote and leave a comment!**","332aa215":"## Rule based models","e52e39d3":"### Always predict negative","f52c6ade":"The [Gradient Boosting Machine (GBM)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html) is a powerful machine learning technique. The model basically trains decision trees models. It keeps training additive models on the residuals of the previous model. In this way we can get much better results than with a single decision tree. A GBM is however the most powerful if there is also plenty of labeled data. "}}