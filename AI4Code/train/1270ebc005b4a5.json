{"cell_type":{"a8876db8":"code","aa9902eb":"code","709d88a5":"code","22bbae9d":"code","bb325edf":"code","bd7bdd3b":"code","1b350fa1":"code","0e6630e0":"code","fdce9c4e":"code","4a4df25d":"code","88b2f3fe":"code","d0bc537a":"code","24953d55":"code","b9c2f600":"code","6872cfa8":"code","85b88aec":"code","37489d3c":"code","070dcdcd":"code","dda0ad2f":"code","910dbdec":"code","86a1e3f3":"code","77c0ab09":"code","565af695":"code","cec494a5":"code","d76caebf":"code","4735e0c9":"code","0e14aaf9":"code","87f9e73e":"code","bc279fde":"code","d64d1d8d":"code","727c4542":"markdown","6c573686":"markdown","1bd40325":"markdown","8271cc4c":"markdown","639e4ae8":"markdown"},"source":{"a8876db8":"#import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras \nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Flatten,Dropout,MaxPooling2D,BatchNormalization,Dense\nfrom keras.utils.vis_utils import plot_model","aa9902eb":"#import dataset\nfrom keras.datasets import cifar10","709d88a5":"#load data\n(train_images,train_labels),(test_images,test_labels)=cifar10.load_data()","22bbae9d":"#shape of data\nprint(\"Shape of traning images is {} and training labels is {}\".format(train_images.shape,train_labels.shape))\nprint(\"\\n\")\nprint(\"Shape of testing  images is {} and testing  labels is {}\".format(test_images.shape,test_labels.shape))","bb325edf":"#define class name\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']","bd7bdd3b":"#let's do some plots\n\nplt.figure(figsize=(10,10))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(train_images[i])\n    plt.xlabel(class_names[train_labels[i][0]])\nplt.show()","1b350fa1":"#preprocess the data\ntrain_images=train_images.astype(\"float32\")\ntest_images=test_images.astype(\"float32\")\n\ntrain_images=train_images\/255.0\ntest_images=test_images\/255.0","0e6630e0":"#make our CNN model\nmodel=Sequential()\n\nmodel.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same',input_shape=(32,32,3)))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\n\nmodel.add(Flatten())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(10,activation='softmax'))","fdce9c4e":"model.summary()","4a4df25d":"plot_model(model)","88b2f3fe":"#compiling model\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","d0bc537a":"#fitting model to training data\n\nhistory=model.fit(train_images, train_labels, epochs=20, batch_size=512, shuffle=True, validation_data=(test_images,test_labels))","24953d55":"test_loss, test_accuracy = model.evaluate(test_images, test_labels)\nprint(test_accuracy)","b9c2f600":"fig, axs = plt.subplots(1,2,figsize=(15,5)) \n    # summarize history for accuracy\naxs[0].plot(history.history['accuracy']) \naxs[0].plot(history.history['val_accuracy']) \naxs[0].set_title('Model Accuracy')\naxs[0].set_ylabel('Accuracy') \naxs[0].set_xlabel('Epoch')\naxs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\naxs[1].plot(history.history['loss']) \naxs[1].plot(history.history['val_loss']) \naxs[1].set_title('Model Loss')\naxs[1].set_ylabel('Loss') \naxs[1].set_xlabel('Epoch')\naxs[1].legend(['train', 'validate'], loc='upper left')\nplt.show()","6872cfa8":"#second model\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='softmax'))","85b88aec":"model.summary()","37489d3c":"plot_model(model)","070dcdcd":"#compiling model\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","dda0ad2f":"history=model.fit(train_images, train_labels, epochs=20, batch_size=512, shuffle=True, validation_data=(test_images,test_labels))","910dbdec":"test_loss, test_accuracy = model.evaluate(test_images, test_labels)\nprint(test_accuracy)","86a1e3f3":"fig, axs = plt.subplots(1,2,figsize=(15,5)) \n    # summarize history for accuracy\naxs[0].plot(history.history['accuracy']) \naxs[0].plot(history.history['val_accuracy']) \naxs[0].set_title('Model Accuracy')\naxs[0].set_ylabel('Accuracy') \naxs[0].set_xlabel('Epoch')\naxs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\naxs[1].plot(history.history['loss']) \naxs[1].plot(history.history['val_loss']) \naxs[1].set_title('Model Loss')\naxs[1].set_ylabel('Loss') \naxs[1].set_xlabel('Epoch')\naxs[1].legend(['train', 'validate'], loc='upper left')\nplt.show()","77c0ab09":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\nmodel.add(Dense(10, activation='softmax'))","565af695":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","cec494a5":"history=model.fit(train_images, train_labels, epochs=20, batch_size=32, shuffle=True, validation_data=(test_images,test_labels))","d76caebf":"test_loss, test_accuracy = model.evaluate(test_images, test_labels)\nprint(test_accuracy)","4735e0c9":"fig, axs = plt.subplots(1,2,figsize=(15,5)) \n    # summarize history for accuracy\naxs[0].plot(history.history['accuracy']) \naxs[0].plot(history.history['val_accuracy']) \naxs[0].set_title('Model Accuracy')\naxs[0].set_ylabel('Accuracy') \naxs[0].set_xlabel('Epoch')\naxs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\naxs[1].plot(history.history['loss']) \naxs[1].plot(history.history['val_loss']) \naxs[1].set_title('Model Loss')\naxs[1].set_ylabel('Loss') \naxs[1].set_xlabel('Epoch')\naxs[1].legend(['train', 'validate'], loc='upper left')\nplt.show()","0e14aaf9":"model = Sequential()\n\nmodel.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform', padding='same',input_shape=(32,32,3)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Dropout(0.6))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer='he_uniform',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Dropout(0.7))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu',kernel_initializer='he_uniform',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Dropout(0.7))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.7))\nmodel.add(Dense(10,activation='softmax'))","87f9e73e":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","bc279fde":"history=model.fit(train_images, train_labels, epochs=50, batch_size=64, shuffle=True, validation_data=(test_images,test_labels))","d64d1d8d":"fig, axs = plt.subplots(1,2,figsize=(15,5)) \n    # summarize history for accuracy\naxs[0].plot(history.history['accuracy']) \naxs[0].plot(history.history['val_accuracy']) \naxs[0].set_title('Model Accuracy')\naxs[0].set_ylabel('Accuracy') \naxs[0].set_xlabel('Epoch')\naxs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\naxs[1].plot(history.history['loss']) \naxs[1].plot(history.history['val_loss']) \naxs[1].set_title('Model Loss')\naxs[1].set_ylabel('Loss') \naxs[1].set_xlabel('Epoch')\naxs[1].legend(['train', 'validate'], loc='upper left')\nplt.show()","727c4542":"# Let's make our final model","6c573686":"# Now let's make another model","1bd40325":"# That's a good model but it can be better","8271cc4c":"# That'not a good model let's make another one.","639e4ae8":"# Part2"}}