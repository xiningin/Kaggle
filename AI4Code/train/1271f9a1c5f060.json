{"cell_type":{"b2c86ac3":"code","5d66773c":"code","e355461c":"code","218cd16e":"code","bc462ac7":"code","e0ab804d":"code","1df31d95":"code","4fd53d50":"code","0b406f06":"code","9d6a8296":"code","80f83825":"code","a0ff006b":"code","325dc181":"code","e81b11d4":"code","a436017b":"code","7dc7b0e6":"code","d3b1dac0":"code","03cac184":"code","ce93a254":"code","2a5f3edf":"code","06c68511":"code","c15b9073":"code","f442bd5a":"code","6b2fcd74":"code","9393a56f":"code","faf2e67a":"code","23bb428a":"code","41b191e0":"code","696eb80e":"code","fe2c192e":"code","0abbff9a":"code","dc0cc9fc":"code","3c52b973":"code","1b0bed4e":"code","81a9376b":"code","c6b49537":"code","7bb20b08":"code","2c365de6":"code","c00c0151":"code","b8ed099c":"code","44e6de93":"code","90756fc4":"code","dd458f03":"code","aac2b2b8":"code","bb579c12":"code","db762293":"code","75a42658":"code","a05e2949":"code","d9565dfb":"code","d1f3c69a":"code","043d40d5":"code","409a4f2e":"code","4030bd71":"code","d5863904":"code","5120f353":"code","8df4ed03":"code","cfa8949e":"code","481bd111":"code","b1512a91":"code","e9382c81":"code","783fb52a":"code","9c5a0c68":"code","ce8c4341":"code","9f83cf7a":"code","9be91c57":"code","190f013b":"markdown","f1d74438":"markdown","5741291f":"markdown","0a42c650":"markdown","369dd89e":"markdown","10a04a4e":"markdown","4c213a01":"markdown","084fb646":"markdown","77b9d027":"markdown","99e44eb1":"markdown","1eb914e6":"markdown","1fcf403f":"markdown","d711dfcd":"markdown","12ecd08d":"markdown","8d0bc3a5":"markdown","77fa6581":"markdown","a2ff45b8":"markdown","aa92fb65":"markdown","6969bc79":"markdown","fb05ac8a":"markdown","a1aee8ed":"markdown","86342066":"markdown","51771b85":"markdown","a9cdf016":"markdown","118ce300":"markdown","15849fe6":"markdown","211445f5":"markdown","d507d801":"markdown","6b3572b9":"markdown","b06de21f":"markdown","92c7611a":"markdown","1109937e":"markdown","f37c414a":"markdown","b2a001a7":"markdown","8c8d9cee":"markdown","06d9b24b":"markdown","cdf5cdf1":"markdown","a40495e5":"markdown","792ef227":"markdown","f221d5cf":"markdown","5262bb0e":"markdown","9f34f646":"markdown","14d4c5df":"markdown","d264e5b5":"markdown","293bfa59":"markdown","eaaa66f3":"markdown","a7478319":"markdown","e1313c0b":"markdown","dfb26280":"markdown","7995a221":"markdown"},"source":{"b2c86ac3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n# other libraries and functions\nfrom sklearn.preprocessing import MinMaxScaler, Imputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","5d66773c":"df = pd.read_csv(\"..\/input\/application_train.csv\")","e355461c":"train, test = train_test_split(df, test_size=0.2)","218cd16e":"train.shape","bc462ac7":"test.shape","e0ab804d":"train.head()","1df31d95":"test.head()","4fd53d50":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","0b406f06":"# Missing values statistics\nmissing_train = missing_values_table(train)\nmissing_train.head(10)","9d6a8296":"# Number of each type of column\ntrain.dtypes.value_counts()","80f83825":"# Number of unique classes in each object column\ntrain.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","a0ff006b":"train.describe()","325dc181":"(train['DAYS_BIRTH'] \/ -365).describe()","e81b11d4":"train['DAYS_EMPLOYED'].describe()","a436017b":"train['CNT_CHILDREN'].describe()","7dc7b0e6":"train['AMT_INCOME_TOTAL'].describe()","d3b1dac0":"train['DAYS_REGISTRATION'].describe()","03cac184":"# TARGET value 0 means loan is repayed, value 1 means loan is not repayed.\nplt.figure(figsize=(15,5))\nsns.countplot(train.TARGET)\nplt.xlabel('Target (0 = repaid, 1 = not repaid)'); plt.ylabel('C'); plt.title('Distribution of Loan Repayment');","ce93a254":"plt.figure(figsize=(15,5))\nsns.countplot(train.NAME_CONTRACT_TYPE.values,data=train)\nplt.xlabel('Contract Type'); plt.ylabel('Count'); plt.title('Distribution of Contract Types');","2a5f3edf":"plt.figure(figsize=(15,5))\nsns.countplot(train.CODE_GENDER.values,data=train)\nplt.xlabel('Gender'); plt.ylabel('Number of Clients'); plt.title('Distribution of Gender');","06c68511":"plt.figure(figsize=(15,5))\nsns.countplot(train.NAME_EDUCATION_TYPE.values,data=train)\nplt.xlabel('Education Type\/Level'); plt.ylabel('Number of Clients'); plt.title('Distribution of Education Type\/Level');","c15b9073":"plt.figure(figsize=(15,5))\nsns.countplot(train.FLAG_OWN_CAR.values,data=train)\nplt.xlabel('Car Ownership (Y = Yes, N = No)'); plt.ylabel('Number of Clients'); plt.title('Distribution of Car Ownership');","f442bd5a":"plt.figure(figsize=(15,5))\nsns.countplot(train.FLAG_OWN_REALTY.values,data=train)\nplt.xlabel('Home Ownership (Y = Yes, N = No)'); plt.ylabel('Number of Clients'); plt.title('Distribution of Home Ownership');","6b2fcd74":"plt.figure(figsize=(15,5))\nsns.countplot(train.CNT_CHILDREN.values,data=train)\nplt.xlabel('Number of Children'); plt.ylabel('Number of Clients'); plt.title('Distribution of Children Per Client');","9393a56f":"plt.figure(figsize=(15,5))\nsns.countplot(train.NAME_FAMILY_STATUS.values,data=train)\nplt.xlabel('Family Status'); plt.ylabel('Number of Clients'); plt.title('Family Status Distribution');","faf2e67a":"plt.figure(figsize=(15,5))\nsns.countplot(train.NAME_HOUSING_TYPE.values,data=train)\nplt.xlabel('Housing Type'); plt.ylabel('Number of Clients'); plt.title('Housing Type Distribution');","23bb428a":"train['DAYS_BIRTH'] = abs(train['DAYS_BIRTH'])\n\nplt.figure(figsize=(15,5))\nsns.distplot(train['DAYS_BIRTH'] \/ 365,bins=5)\nplt.xlabel('Age (Years)'); plt.ylabel('Density'); plt.title('Age Distribution');","41b191e0":"# Age information into a separate dataframe\nage_data = train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] \/ 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","696eb80e":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","fe2c192e":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","0abbff9a":"# Find correlations with the target and sort\ncorrelations = train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","dc0cc9fc":"# Copy data into a different dataframe to preserve the original\nbench_train = train.copy()\nbench_test = test.copy()\n\n# one-hot encoding of categorical variables\nbench_train = pd.get_dummies(bench_train)\nbench_test = pd.get_dummies(bench_test)\n\n# capture the labels\nbench_train_labels = bench_train['TARGET']\nbench_test_labels = bench_test['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\nbench_train, bench_test = bench_train.align(bench_test, join = 'inner', axis = 1)\n\n# Drop the target from the training and testing data\nbench_train = bench_train.drop(columns = ['TARGET'])\nbench_test = bench_test.drop(columns = ['TARGET'])\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(bench_train)\nimputer.fit(bench_test)\n\n# Transform both training and testing data\nbench_train = imputer.transform(bench_train)\nbench_test = imputer.transform(bench_test)\n\n# Repeat with the scaler\nscaler.fit(bench_train)\nscaler.fit(bench_test)\nbench_train = scaler.transform(bench_train)\nbench_test = scaler.transform(bench_test)\n\nprint('Training data shape: ', bench_train.shape)\nprint('Testing data shape: ', bench_test.shape)","3c52b973":"# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(log_reg, bench_train, bench_train_labels, cv=shuffle, scoring='roc_auc')\nprint(scores)","1b0bed4e":"print(train.shape)\nprint(test.shape)","81a9376b":"# Copy data into a different dataframe to preserve the original\nmain_train = train.copy()\nmain_test = test.copy()","c6b49537":"main_train['DAYS_BIRTH'] = abs(main_train['DAYS_BIRTH'])\nmain_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)","7bb20b08":"# one-hot encoding of categorical variables\nmain_train = pd.get_dummies(main_train)\nmain_test = pd.get_dummies(main_test)\n\n# Align the training and testing data, keep only columns present in both dataframes\nmain_train, main_test = main_train.align(main_test, join = 'inner', axis = 1)","2c365de6":"print(main_train.shape)\nprint(main_test.shape)","c00c0151":"# Threshold for removing correlated variables\nthreshold = 0.8\n\n# Absolute value correlation matrix\ncorr_matrix = main_train.corr().abs()\ncorr_matrix.head()","b8ed099c":"upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","44e6de93":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))","90756fc4":"main_train = main_train.drop(columns = to_drop)\nmain_test = main_test.drop(columns = to_drop)\n\nprint('Training shape: ', main_train.shape)\nprint('Testing shape: ', main_test.shape)","dd458f03":"# Train missing values (in percent)\ntrain_missing = (main_train.isnull().sum() \/ len(main_train)).sort_values(ascending = False)\ntrain_missing.head()","aac2b2b8":"# Test missing values (in percent)\ntest_missing = (main_test.isnull().sum() \/ len(main_test)).sort_values(ascending = False)\ntest_missing.head()","bb579c12":"# Identify missing values above threshold\ntrain_missing = train_missing.index[train_missing > 0.50]\ntest_missing = test_missing.index[test_missing > 0.50]\n\nall_missing = list(set(set(train_missing) | set(test_missing)))\nprint('There are %d columns with more than 50%% missing values' % len(all_missing))","db762293":"# Need to save the labels because aligning will remove this column\nmain_train_labels = main_train[\"TARGET\"]\nmain_train_ids = main_train['SK_ID_CURR']\nmain_test_ids = main_test['SK_ID_CURR']\n\nmain_train = pd.get_dummies(main_train.drop(columns = all_missing))\nmain_test = pd.get_dummies(main_test.drop(columns = all_missing))\n\nmain_train, main_test = main_train.align(main_test, join = 'inner', axis = 1)\n\nprint('Training set full shape: ', main_train.shape)\nprint('Testing set full shape: ' , main_test.shape)","75a42658":"main_train = main_train.drop(columns = ['SK_ID_CURR'])\nmain_test = main_test.drop(columns = ['SK_ID_CURR'])","a05e2949":"print('Training set full shape: ', main_train.shape)\nprint('Testing set full shape: ' , main_test.shape)","d9565dfb":"# capture the labels\nmain_train_labels = main_train['TARGET']\nmain_test_labels = main_test['TARGET']\n\n# Drop the target from the training and testing data\nmain_train = main_train.drop(columns = ['TARGET'])\nmain_test = main_test.drop(columns = ['TARGET'])\n\n# impute median values\nimputer = Imputer(strategy = 'median')\nimputer.fit(main_train)\nimputer.fit(main_test)\nmain_train = imputer.transform(main_train)\nmain_test = imputer.transform(main_test)\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\nscaler.fit(main_train)\nscaler.fit(main_test)\nmain_train = scaler.transform(main_train)\nmain_test = scaler.transform(main_test)\n\nprint('Training data shape: ', main_train.shape)\nprint('Testing data shape: ', main_test.shape)","d1f3c69a":"log_reg = LogisticRegression(C = 0.0001)\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(log_reg, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","043d40d5":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) \/ len(scores)), 4)) ","409a4f2e":"knn = KNeighborsClassifier()\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(knn, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","4030bd71":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) \/ len(scores)), 4)) ","d5863904":"nb = GaussianNB()\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(nb, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","5120f353":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) \/ len(scores)), 4)) ","8df4ed03":"svm = SVC()\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(svm, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","cfa8949e":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) \/ len(scores)), 4)) ","481bd111":"dtc = DecisionTreeClassifier(random_state=0)\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(dtc, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","b1512a91":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) \/ len(scores)), 4)) ","e9382c81":"rfc = RandomForestClassifier()\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(rfc, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","783fb52a":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) \/ len(scores)), 4)) ","9c5a0c68":"abc = AdaBoostClassifier(DecisionTreeClassifier())\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(abc, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","ce8c4341":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) \/ len(scores)), 4)) ","9f83cf7a":"xgb = XGBClassifier()\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(xgb, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","9be91c57":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) \/ len(scores)), 4)) ","190f013b":"## Amount of time at current job","f1d74438":"## One-Hot Encode Categorical Features","5741291f":"## Age","0a42c650":"## Decision Tree","369dd89e":"## Number of Days before application that client changed his\/her registration","10a04a4e":"## Number of Children","4c213a01":"## Car Ownership","084fb646":"## Housing Type","77b9d027":"## Peak at Data","99e44eb1":"## Support Vector Machine","1eb914e6":"## Notable Visualization","1fcf403f":"## Impute and Scale Features","d711dfcd":"## Naive Bayes","12ecd08d":"# Benchmark Model","8d0bc3a5":"## Income","77fa6581":"## Contract Type","a2ff45b8":"## XGBoost Classifer","aa92fb65":"Extra features that have a correlation above 0.8 have now been dropped.","6969bc79":"# Models\n- Logistic Regression\n- K-Nearest Neighbors\n- Naive Bayes\n- SVM Classifier\n- Random Forest\n- Decision Trees\n- AdaBoost\n- XgBoost\n- LightGBM","fb05ac8a":"# Data Types","a1aee8ed":"## Random Forest","86342066":"## Age of Client","51771b85":"## Read in Data","a9cdf016":"## Preprocessing Notes\n- The max value for the DAYS_EMPLOYED variable appears to be an error.  We should replace this with a NAN value in our preprocessing step.\n- We should replace the DAYS_BIRTH variable with an absolute value, and divide by 365, so it is in years.\n- There are many categorical variables that we need to perform one-hot encoding on in the preprocessing step.\n- We need to perform min-max scaling before feeding the data into machine learning algorithms\n- 67 columns have missing data.  We will need to develop a strategy for dropping columns, rows, or imputing values during the preprocessing step.","118ce300":"# Correlations with the Target","15849fe6":"## Steps\n- Replace the max value for the DAYS_EMPLOYED variable with a NAN value.\n- Replace the DAYS_BIRTH variable with an absolute value.\n- Remove any columns that are missing more than 50 percent of their values.\n- Perform one-hot encoding on categorical variables.\n- Perform min-max scaling before feeding the data into machine learning algorithms.\n- Remove colinear features.  If any columns have a correlation over 0.9, only keep one, and remove the others.\n- Use the feature_importances attribute of the lightGBM model to remove features that have very little importance.\n- Impute any additional missing rows with median values.\n","211445f5":"# Refinement","d507d801":"## Overview of all variables","6b3572b9":"## Remove Features with Missing Values Above Threshold","b06de21f":"## Number of Children","92c7611a":"## Education Type\/Level","1109937e":"## Copy Data","f37c414a":"Columns with greater than 50 percent of observations missing have been removed.","b2a001a7":"## Handle Outliers and Transformations","8c8d9cee":"## AdaBoost Classifier","06d9b24b":"## Check Missing Values","cdf5cdf1":"# Descriptive Statistics (To detect outliers and anomolies in continuous variables)","a40495e5":"We will use the logistic regression model to establish our benchmark results.  First, we will copy the training and testing sets, so we can perform some basic preprocessing on the data.  Specifically, we will scale the features between 0 and 1, and fill-in all missing values with the median value of the columns, and perform one-hot encoding on categorical variables.","792ef227":"## Remove Collinear Features Above Threshold","f221d5cf":"## Family Status","5262bb0e":"# Data Preprocessing","9f34f646":"## Home Ownership","14d4c5df":"## Target (repaid loan or not)","d264e5b5":"## Logistic Regression","293bfa59":"# Distributions of Important Features","eaaa66f3":"ID column is now dropped.  We don't need this extra info when we feed the data to our models.","a7478319":"## K Nearest Neighbors","e1313c0b":"## Gender","dfb26280":"First, define a function to build a nice table of missing value percentages.  This will make things easier given the large number of initial features.","7995a221":"# Preprocessing Notes\n- The max value for the DAYS_EMPLOYED variable appears to be an error.  We should replace this with a NAN value in our preprocessing step.\n- We should replace the DAYS_BIRTH variable with an absolute value, and divide by 365, so it is in years.\n- There are many categorical variables that we need to perform one-hot encoding on in the preprocessing step.\n- We need to perform min-max scaling before feeding the data into machine learning algorithms\n- 67 columns have missing data.  We will need to develop a strategy for dropping columns, rows, or imputing values during the preprocessing step.\n- Many features are highly-correlated with one another.  We will need to remove the unnecessary features."}}