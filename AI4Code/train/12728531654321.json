{"cell_type":{"87270e3b":"code","9e0a7907":"code","d7910f18":"code","027f1166":"code","4c021164":"code","9bd7ce5e":"code","8c479abc":"code","001ebb51":"code","45d7932b":"code","8601985a":"code","0712d1ea":"code","606784c6":"code","95a9f94f":"code","13cdd1ed":"code","919fac4a":"code","6a5578d0":"code","197d3bd2":"code","44062ffd":"code","13cf6b94":"code","023ef5be":"code","32e796da":"code","1c1f58c7":"code","2e02a1cb":"code","67afa05b":"code","2982a0f7":"code","e6f28505":"code","20e65c8d":"code","d3f4dcca":"code","d7764a8f":"code","b4e6470c":"code","711e7baa":"code","ce2483c0":"code","978931aa":"code","d256c875":"markdown","e5e792f6":"markdown","8e6e40df":"markdown","27f78031":"markdown","99c4e79c":"markdown","fdf548c7":"markdown","0ff38087":"markdown","a5062d46":"markdown"},"source":{"87270e3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e0a7907":"\n#Import processing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport os, warnings, gc\nimport scipy.stats as stats\nimport random\nseed = 4092\nrandom.seed(seed)\nnp.random.seed(seed)\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder\n\nwarnings.filterwarnings('ignore')\n\n#Time series libraries\nfrom scipy import stats\nfrom pandas.plotting import lag_plot, autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.graphics.tsaplots import pacf, plot_acf,plot_pacf\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.arima_model import  ARIMA\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport statsmodels.api as sm\n\n\n#Modelling libaries\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Dense, LSTM\nfrom keras.callbacks import EarlyStopping\n%matplotlib inline","d7910f18":"# Load the data\n#df = pd.read_csv(\"..\/input\/electric-power-consumption-data-set\/household_power_consumption.txt\", delimiter=';', parse_dates = {\"datetime\":[0,1]},\n#                 low_memory = False, infer_datetime_format=True)\n#df.head()","027f1166":"df = pd.read_csv('..\/input\/electric-power-consumption-data-set\/household_power_consumption.txt', sep = ';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'],na_values = '?', dtype = float)","4c021164":"df.head()","9bd7ce5e":"# Visualize the Table\ndf.head()","8c479abc":"#Dataframe datatypes\ndf.dtypes","001ebb51":"df.columns","45d7932b":"df.shape","8601985a":"df.info()","0712d1ea":"df.values","606784c6":"# Replacing the irregularities in the dataset with nan\n# df.replace({'?': np.nan},inplace=True)","95a9f94f":"# Checking for Null values\ndf.isnull().sum()","13cdd1ed":"# filling up the missing values with the mean values of the columns \nfor columns in df.iloc[:, 0:]:\n  df[columns].fillna(value = df[columns].mean(), inplace = True)\npass\ndf.isnull().sum()","919fac4a":"df.tail()","6a5578d0":"#Dropping any duplicated rows, keeping the first\ndf = df.drop_duplicates(keep = 'first')\nprint(f\"The total sum of the duplicated entries is: {df.duplicated().sum()}\")\nprint('Duplicate check shows the shape of duplicated entries is', df[df.duplicated()].shape)","197d3bd2":"df.shape","44062ffd":"# Data Preprocessing\n\"\"\"\nUsing MiniMaxScaler to normalize the daaset so Recurrent Neural networks model can be built as well\n\"\"\"\n\ndf1 = df['Global_active_power'].values.reshape(-1,1)\nscaler = MinMaxScaler(feature_range = (0,1))\n\nscaled_data = scaler.fit_transform(df1) # The fit_transform method converts the data to numpy array stored in dataset variable\nprint(\"The type of data values in the Global_active_Power Column is now {}\". format(type(scaled_data)))\nprint(\"=================================\")\nprint(scaled_data[:5])\nprint(\"=================================\")","13cf6b94":"# Setting the parameters\nlagplot_data = df['Global_active_power'].resample('D').mean()\nlagplot_data.shape","023ef5be":"#The plot shows some correlation of present time and future time\nplt.figure(figsize = (10,8))\nlagplot = lag_plot(lagplot_data, c = 'purple', marker = '*')\nlagplot.plot(title = 'Lagplot to check autocorrelation resampled over day')\nplt.show()","32e796da":"lagplot_data_df = pd.concat([lagplot_data.shift(-1), lagplot_data], axis = 1)\nlagplot_data_df.columns = ['Global_active_power(t)', 'Global_active_power(t+1)']\nlagplot_data_corr = lagplot_data_df.corr()\nlagplot_data_corr","1c1f58c7":"lagplot_data1 = df['Global_active_power'].resample('W').mean()\nlagplot_data1.shape","2e02a1cb":"plt.figure(figsize = (10,8))\nlagplot1 = autocorrelation_plot(lagplot_data1, c = 'blue', marker = '^')\nlagplot1.plot(title = 'Autocorrelation Plot of the Data resampled over Hour')\nplt.show()","67afa05b":"#ACF AND PACF PLOT Shows our p = 7 and q = 1\ndf1 = df['Global_active_power'].resample('W').agg('mean')\n# plots\nplt.figure(figsize = (10,10))\nlags = 30\n# acf\naxis = plt.subplot(2, 1, 1)\nplot_acf(df1, ax=axis, lags=lags)\n# pacf\naxis = plt.subplot(2, 1, 2)\nplot_pacf(df1, ax=axis, lags=lags)\n# show plot\nplt.show()","2982a0f7":"#Define the base model and thresholds\ndef baseARmodel(dataset):\n  from pandas import DataFrame\n  X = pd.concat([DataFrame(dataset).shift(1), DataFrame(dataset)], axis = 1)\n  X.columns = ['Global_active_power(t-1)', 'Global_active_power(t+1)']\n  X = X.values\n  train, test = X[:len(dataset)-30], X[len(dataset)-30:]\n  print(f\" The length of train and test are {len(train)} and {len(test)} respectively\")\n  X_train, y_train = train[:len(train),0], train[:len(train),-1:]\n\n  X_test, y_test = test[:,0], test[:,1]\n\n  # persistence model\n  def model_persistence(x):\n\t  return x\n \n  # walk-forward validation\n  predictions = list()\n  for x in X_test:\n\t  yhat = model_persistence(x)\n\t  predictions.append(yhat)\n  test_score = mean_squared_error(y_test, predictions)\n\n  print(\"-------------------------------------------------------\")\n  print('Test MSE: %.8f' % test_score)\n  print('Test RMSE: %.8f' % np.sqrt(test_score))\n  print(\"-------------------------------------------------------\")\n\n  # plot predictions vs expected\n  plt.figure(figsize=(8,4))\n  plt.plot(y_test, color = 'blue',marker = '^', label = 'ACTUAL')\n  plt.plot(predictions, color='red', marker = 'o', label = 'PREDICTIONS')\n  plt.title(\"BASE MODEL OF AUTOREGRESSION\")\n  plt.xlabel('Timestep')\n  plt.ylabel('Global_active_power')\n  plt.legend()\n  plt.show()\n\n  return '=======AUTOREGRESSION BASE MODEL DONE======='\nbaseARmodel(scaled_data)","e6f28505":"def ar_model(dataset):\n  \"\"\"\n  Training the data with Auto regression Model\n  \"\"\"\n  X = dataset\n  train, test = X[:len(X)-30], X[len(X)-30:]\n\n  autoreg = AutoReg(train, lags = 30)\n  autoreg_fit = autoreg.fit()\n\n  print(\"========Trainng finished successfully=======\",'\\n')\n  print(autoreg_fit.summary())\n  \n  print(\"============================================================\")\n  print(\"Model coefficients are: %s\" % autoreg_fit.params)\n  print(\"============================================================\")\n\n  # fig = plt.figure(figsize=(16,9))\n  # fig = autoreg_fit.plot_diagnostics(fig=fig, lags=30)\n\n\n  predictions = autoreg.predict(autoreg_fit.params, start = len(train), end = len(train)+len(test)-1, dynamic = True)\n\n  for pred in range(len(predictions)):\n    print('predicted_values%f, expected_values=%f' % (predictions[pred], test[pred]))\n\n  test = scaler.inverse_transform(test)\n  predictions = scaler.inverse_transform([predictions])\n  MSE = mean_squared_error(test, predictions[0])\n  RMSE = np.sqrt(MSE)\n  print(\"============================================================\")\n  print(\"Test Mean Squred Error: %0.5f\" % MSE)\n  print(\"Test Root Mean Squred Error: %0.5f\" % RMSE)\n  print(\"============================================================\")\n\n  #ig = autoreg_fit.plot_predict(start = len(train), end = len(train)+len(test)-1)\n\n   # plot predictions vs expected\n  plt.figure(figsize=(8,4))\n  plt.plot(test, color = 'blue',marker = '^', label = 'ACTUAL')\n  plt.plot(predictions[0], color='red', marker = 'o', label = 'PREDICTIONS')\n  plt.title(\"AUTOREGRESSION MODEL\")\n  plt.xlabel('Timestep')\n  plt.ylabel('Global_active_power')\n  plt.legend()\n  plt.show()\n  return '=======AUTOREGRESSION  DONE======='\n\nar_model(scaled_data)","20e65c8d":"df1 = df['Global_active_power'].resample('W').agg('mean')\n# plots\nplt.figure(figsize = (10,10))\nlags = 30\n# acf\naxis = plt.subplot(2, 1, 1)\nplot_acf(df1, ax=axis, lags=lags)\n# pacf\naxis = plt.subplot(2, 1, 2)\nplot_pacf(df1, ax=axis, lags=lags)\n# show plot\nplt.show()","d3f4dcca":"df2 = df['Global_active_power'].resample('D').agg('mean')\ndf2.fillna(df['Global_active_power'].mean(),inplace=True)\ndf2  = df2.values.reshape(-1,1)\n\ndf2_scaled = scaler.fit_transform(df2)","d7764a8f":"# fit the model\ndef arima_model(dataset):\n  \n  \"\"\"\n  ARIMA modelling\n  #ACF AND PACF PLOT Shows our p = 7, d = 1 and q = 0\n  \"\"\"\n  train_size = int(0.8*(len(dataset)))\n  train, test = dataset[:train_size], dataset[train_size:len(dataset)]\n\n  train_data = [x for x in train]\n  predictions = list()\n  y_test = list()\n\n  for tests in range(len(test)):\n    model = ARIMA(train_data, order=(7,1,0))\n    model_fit = model.fit(disp=0)\n    \n    forecast = model_fit.forecast()\n    yhat = forecast[0]\n    predictions.append(yhat)\n\n    y_true = test[tests]\n    y_test.append(y_true)\n  print(model_fit.summary())\n  print(\"======Training Finished Successfully======\")\n  print(\"======Forecasting Finished Successfully======\")\n\n\n  MSE = mean_squared_error(scaler.inverse_transform(y_test), scaler.inverse_transform(predictions))\n  RMSE = np.sqrt(MSE)\n\n  print(\"=====================================================\")\n  print(\"Test MSE Error: %0.5f\" % MSE)\n  print(\"Test RMSE Error: %0.5f\" %RMSE)\n  print(\"=====================================================\")\n\n  # plot predictions vs expected\n  plt.figure(figsize=(8,4))\n  plt.plot(y_test, color = 'blue',marker = '^', label = 'ACTUAL')\n  plt.plot(predictions, color='red', marker = 'o', label = 'PREDICTIONS')\n  plt.title(\"ARIMA MODEL\")\n  plt.xlabel('Timestep')\n  plt.ylabel('Global_active_power')\n  plt.legend()\n  plt.show()\n\n  print(\"+++++++++++Residual Plot++++++++++\")\n  # plot residual errors\n  residuals = pd.DataFrame(model_fit.resid)\n  plt.figure(figsize=(8,4))\n  residuals.plot()\n  plt.show()\n  plt.figure(figsize=(8,4))\n  residuals.plot(kind='kde')\n  plt.show()\n  print(residuals.describe())\n\n  return \"====ARIMA MODELING DONE====\"\narima_model(df2_scaled)","b4e6470c":"# fit the model\ndef sarimax_model(dataset):\n  \n  \"\"\"\n  SARIMAX modelling\n  #ACF AND PACF PLOT Shows our p = 7, d = 1 and q = 0\n  \"\"\"\n  train_size = int(0.8*(len(dataset)))\n  train, test = dataset[:train_size], dataset[train_size:len(dataset)]\n\n  train_data = [x for x in train]\n  predictions = list()\n  y_test = list()\n\n  for tests in range(len(test)):\n    model = SARIMAX(train_data,order=((7,5,12,13),0, 0))\n    model_fit = model.fit(disp=0)\n    \n    forecast = model_fit.forecast()\n    yhat = forecast[0]\n    predictions.append(yhat)\n\n    y_true = test[tests]\n    y_test.append(y_true)\n  print(model_fit.summary())\n  print(\"======Training Finished Successfully======\")\n  print(\"======Forecasting Finished Successfully======\")\n\n\n  MSE = mean_squared_error(y_test,predictions)\n  RMSE = np.sqrt(MSE)\n\n  print(\"=====================================================\")\n  print(\"Test MSE Error: %0.5f\" % MSE)\n  print(\"Test RMSE Error: %0.5f\" %RMSE)\n  print(\"=====================================================\")\n\n  # plot predictions vs expected\n  plt.figure(figsize=(8,4))\n  plt.plot(y_test, color = 'blue',marker = '^', label = 'ACTUAL')\n  plt.plot(predictions, color='red', marker = 'o', label = 'PREDICTIONS')\n  plt.title(\"SARIMAX MODEL\")\n  plt.xlabel('Timestep')\n  plt.ylabel('Global_active_power')\n  plt.legend()\n  plt.show()\n\n  print(\"+++++++++++Residual Plot++++++++++\")\n  # plot residual errors\n  residuals = pd.DataFrame(model_fit.resid)\n  residuals.plot()\n  plt.show()\n  residuals.plot(kind='kde')\n  plt.show()\n  print(residuals.describe())\n\n  return \"====SARIMAX MODELING DONE====\"\nsarimax_model(df2_scaled)","711e7baa":"# Creating Training and Testing Dataset\n\ndef lstm_model(dataset):\n    train_size = int(len(dataset)*0.8)\n    test_size = int(len(dataset)*0.2)\n\n    train, test = dataset[:train_size,:], dataset[train_size:len(dataset),:]\n\n\n    print(\"=====Converting an array of values into a dataset matrix done successfully=====\",'\\n')\n    def create_dataset(dataset, look_back):\n        X, Y = [], []\n        for i in range(len(dataset)-look_back-1):\n            x1 = dataset[i:(i+look_back), 0]\n            X.append(x1)\n            Y.append(dataset[i + look_back, 0])\n        return np.array(X), np.array(Y)\n    \n    X_train, y_train = create_dataset(train, 30)\n    X_test, y_test = create_dataset(test,30)\n\n    print(\"====Reshaping input to be [samples, no_features, time steps] done successfully====\",'\\n')\n    X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n\n    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    model = Sequential()\n    model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(Dropout(0.2))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n\n    history = model.fit(X_train, y_train, epochs=20, batch_size=70, validation_data=(X_test, y_test), \n                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n\n    print(model.summary())\n\n    print(\"=================Making Prediction=================\")\n\n    train_prediction = model.predict(X_train)\n    test_prediction = model.predict(X_test)\n\n    #Since we transformed our training and test data, we need to inverse it to get the real predicted values\n    train_prediction = scaler.inverse_transform(train_prediction)\n    y_train = scaler.inverse_transform([y_train])\n\n    test_prediction = scaler.inverse_transform(test_prediction)\n    y_test = scaler.inverse_transform([y_test])\n\n    print(\"===========Evaluating the Model=============\")\n    print(\"--------------------------------------------\")\n    print(\"===========Mean Square Error and RMSE=======\",'\\n')\n\n\n    print(f\"The Train Mean Suared Error is {round(mean_squared_error(y_train[0], train_prediction[:,0]),3)}\")\n    print(f\"The Train Root Mean Suared Error is {round(np.sqrt(mean_squared_error(y_train[0], train_prediction[:,0])),3)}\")\n\n    print(f\"The Test Mean Suared Error is {round(mean_squared_error(y_test[0], test_prediction[:,0]),3)}\")\n    print(f\"The Test Root Mean Suared Error is {round(np.sqrt(mean_squared_error(y_test[0], test_prediction[:,0])),3)}\")\n\n    print(\"++++++++Ploting Model Loss+++++++++++\",'\\n')\n    plt.figure(figsize=(8,4))\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Test Loss')\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epochs')\n    plt.legend(loc='best')\n    plt.show();\n\n    print(\"======comparing the actual and predictions for the last 500 minutes======\")\n    data_list = [x for x in range(500)]\n    plt.figure(figsize=(8,4))\n    plt.plot(data_list, y_test[0][:500], marker='*', label=\"actual\")\n    plt.plot(data_list, test_prediction[:,0][:500], marker = '^', label=\"prediction\")\n    plt.tight_layout()\n    sns.despine(top=True)\n    plt.subplots_adjust(left=0.07)\n    plt.ylabel('Global_active_power(KW)', size=15)\n    plt.xlabel('Time step', size=15)\n    plt.legend(fontsize=15)\n    plt.show();\n    \n    return \"=====LSTM MODELING DONE=====\"\nlstm_model(scaled_data)\n","ce2483c0":"pd.set_option('display.float_format', lambda x: '%.5f' % x)\neval = {\"model\": [\"AR\",\"ARIMA\", \"SARIMA\", \"LSTM\"],\n        \"MSE\": [0.18820,0.23546, 0.05259,0.004],\n        \"RMSE\":[0.43382, 0.48524, 0.22933,0.063]\n      }\nmodels = pd.DataFrame(eval)\nmodels.head()","978931aa":"plt.figure(figsize = (10,10))\nax = plt.subplot(2,1,1)\nax = sns.barplot(data = models, x = 'model', y = 'MSE', orient = 'v')\nplt.show()\nplt.figure(figsize = (10,10))\nax1 = plt.subplot(2,1,2)\nax1 = sns.barplot(data = models, x = 'model', y = 'RMSE', orient = 'v')\nplt.show()","d256c875":"# CONCLUSION\n\n## We see that the best model performance was obtained with the Deep Neural Network, LSTM. \n\n### LSTM Model:\n+ Is highly flexible\n+ Is highly usable in predictions where there's time lag between input features and target signals, and can capture long-term temporal dependencies without suffering optimization challenges (-- interesting capabilities that will offer improvement on the study to predict regional land subsidence from the input variables).\n+ Is suitable for predictions involving large volume datasets -- our model used over 2M data entries.\n+ has the ability to figure out or understand hidden pattern in the given dataset.\n+ is good for time series modeling because of its recurrent nature(RNN)\n\n### The End","e5e792f6":"## Checking for Autocorrelation by Lag\nIf all lag variables show small or no correlation with the output variable, then it suggests that the time series problem may not be predictable. This can be very useful when getting started on a new dataset.","8e6e40df":"## Evaluating the Models: MSE AND RMSE PLOT OF VARIOUS MODELS","27f78031":"### Using SARIMAX Model","99c4e79c":"## Data Modeling Using ARIMA Model\n\nARIMA stands for Autoregressive Integrated Moving Average. It depends on 3 important parametres:\n+ Number Autoregression(AR) term paramentre(p)\n+ Number of Moving Average(MA) term(q)\n+ Number of Difference(d, usually 0 or 1)\n\n### Determing p and q\nTo determine these terms, we are going to use Autocorrelation Function(ACF) and Partial Autocorrelation Function(PACF) plots from statmodels","fdf548c7":"\n## Apply LSTM Model\n### Long Short-Term Memory (LSTM) networks are a type of artificial recurrent neural network (RNN) known for its robustness in making predictions based on time series data. It has wide feeback connections that allows it to process ordered multiple data points in sequence prediction problems.\n\n### Let's build a Long short-term memory (LSTM) model","0ff38087":"\n## Using statsmodel plot_acf and plot_pac","a5062d46":"## MODELING: 4 Models are Built to Select the Best Performing:\n+ AR (Autoregressive Model)\n+ ARIMA (Autoregressive Integrated Moving Average)\n+ SARIMA (Seasonal Autoregressive Integrated Moving Average)\n+ LSTM (Long Short-Term Memory) Networks\n\nPerformance Metrics to be used are **Mean Squared Error** and **Root Mean Sqaure Error**.\n\n## Data Modeling Using AR Model\nAR stands for Autoregression\nA regression model, such as linear regression, models an output value based on a linear combination of input values.\nBecause the regression model uses data from the same input variable at previous time steps, it is referred to as an autoregression (regression of self).\n\n"}}