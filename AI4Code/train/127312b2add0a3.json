{"cell_type":{"a269b23a":"code","9e45b7dd":"code","0d444b09":"code","6f604fbc":"code","108e6c1a":"code","81f63141":"code","cc3fca10":"code","b6fe8f2e":"code","20db9159":"code","1c14963e":"code","77ee5334":"code","64e94f8d":"code","d07914eb":"code","984a42ff":"code","48f8791c":"code","9d33d283":"code","699c5c0c":"code","60f044b8":"code","2fdbb392":"code","643b9580":"code","ee04522d":"code","2e32b9c4":"code","f95b0138":"code","fbd15c5f":"code","f0adc618":"code","5e875ca6":"code","5ae89238":"code","bcadd1c9":"code","a2dd1fe8":"code","56bed2fc":"code","f306bda9":"code","3d820d39":"code","dc227888":"code","563d8280":"code","326a195d":"markdown","aa0ffb9f":"markdown","44061d30":"markdown","32e39ac3":"markdown"},"source":{"a269b23a":"import torch\nimport numpy as np\nimport random","9e45b7dd":"np.random.seed(1)\nrandom.seed(1)\ntorch.manual_seed(1)\ntorch.cuda.manual_seed(1)\ntorch.cuda.manual_seed_all(1)\ntorch.backends.cudnn.enabled = False\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True","0d444b09":"import os\nimport pickle\nos.chdir(\"\/kaggle\/input\/\")\nif os.path.isfile('..\/input\/x-chunked-32-64\/X_chunked_32_64.pkl') and os.path.isfile('..\/input\/y-chunked-32-64\/Y_chunked_32_64.pkl'):\n  with open('..\/input\/x-chunked-32-64\/X_chunked_32_64.pkl','rb') as f:\n    inputX = pickle.load(f)\n  with open('..\/input\/y-chunked-32-64\/Y_chunked_32_64.pkl','rb') as f:\n    outputY = pickle.load(f)\n  print('Dataset found, moving forward')\nelse:\n  print('Please make sure the dataset Medical_Image_Segmentation exists in Google Drive')\n  exit(0)\n\n#Visualize data\nimport matplotlib.pyplot as plt\nimport numpy as np\nprint(inputX.shape)\nX = np.expand_dims(inputX,-1)\nY = np.expand_dims(outputY,-1)\n\ninput_images = 255 * (X -X.min())\/(X.max() - X.min())\ninput_images = input_images.astype('float32')\ntarget_images = ((Y > 0)*1.0)\ntarget_images = target_images.astype('float32')\nprint(f\"input_images shape: {X.shape} target_images shape: {Y.shape}\")\nprint(f\"input_images data range: {input_images.min()} - {input_images.max()}\")\nprint(f\"target_images data range: {target_images.min()} - {target_images.max()}\")\n\nNUM_IM_TO_SHOW = 4\nplt.figure(figsize = (6,6))\nfor num,image in enumerate(target_images[5][0:NUM_IM_TO_SHOW*NUM_IM_TO_SHOW]):\n  fig = plt.subplot(NUM_IM_TO_SHOW,NUM_IM_TO_SHOW,num+1)\n  plt.axis('off')\n  plt.imshow(image.reshape(64,64) , cmap='Blues')\n\nNUM_IM_TO_SHOW = 4\nplt.figure(figsize = (6,6))\nfor num,image in enumerate(input_images[5][0:NUM_IM_TO_SHOW*NUM_IM_TO_SHOW]):\n  fig = plt.subplot(NUM_IM_TO_SHOW,NUM_IM_TO_SHOW,num+1)\n  plt.axis('off')\n  plt.imshow(image.reshape(64,64) , cmap='Blues')","6f604fbc":"import os\nos.chdir(\"\/kaggle\/working\/\")","108e6c1a":"!ls","81f63141":"%pip install wandb --upgrade","cc3fca10":"import wandb","b6fe8f2e":"wandb.login()","20db9159":"wandb.init(project=\"Tumor_Detection\",name = \"UNET3D_1_4_0.2\") ","1c14963e":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor\nfrom torchvision import datasets, models, transforms\nclass Custom_Dataset():\n  def __init__(self,X,Y):\n    self.images=X\n    self.labels=Y\n\n  def __len__(self):\n    return len(self.images)\n\n  def __getitem__(self,i):\n    x = self.images[i]\/255\n    y = self.labels[i]\n    x = torch.from_numpy(x)\n    x = x.reshape([1,64,64,32])\n    y = torch.from_numpy(y)\n    y = y.reshape([1,64,64,32])\n    return x, y","77ee5334":"print(input_images.shape)","64e94f8d":"train_input_images = input_images[0:80,:,:,:,:]\ntrain_target_images = target_images[0:80,:,:,:,:]\nval_input_images = input_images[80:89,:,:,:,:]\nval_target_images = target_images[80:89,:,:,:,:]","d07914eb":"print(val_input_images.shape)","984a42ff":"train_dataset = Custom_Dataset(train_input_images,train_target_images)\ntrain_d =  DataLoader(train_dataset,batch_size=4,num_workers=0)","48f8791c":"val_dataset = Custom_Dataset(val_input_images,val_target_images)\nval_d =  DataLoader(val_dataset,batch_size=1)","9d33d283":"for u,v in train_d:\n    print(u.shape)\n    print(v.shape)\n    break","699c5c0c":"for u,v in train_dataset:\n    print(u.shape)\n    print(v.shape)\n    break","60f044b8":"plt.imshow(u.squeeze().reshape(32,64,64)[14])","2fdbb392":"plt.imshow(v.squeeze().reshape(32,64,64)[3])","643b9580":"from torch.nn import Module, Sequential\nfrom torch.nn import Conv3d, ConvTranspose3d, BatchNorm3d, MaxPool3d, AvgPool1d, Dropout3d\nfrom torch.nn import ReLU, Sigmoid\nimport torch\n\n\nclass UNet(Module):\n    # __                            __\n    #  1|__   ________________   __|1\n    #     2|__  ____________  __|2\n    #        3|__  ______  __|3\n    #           4|__ __ __|4\n    # The convolution operations on either side are residual subject to 1*1 Convolution for channel homogeneity\n\n    def __init__(self, num_channels=1, feat_channels=[64, 256, 256, 512, 1024], residual='conv'):\n        # residual: conv for residual input x through 1*1 conv across every layer for downsampling, None for removal of residuals\n\n        super(UNet, self).__init__()\n\n        # Encoder downsamplers\n        self.pool1 = MaxPool3d((2, 2, 2))\n        self.pool2 = MaxPool3d((2, 2, 2))\n        self.pool3 = MaxPool3d((2, 2, 2))\n        self.pool4 = MaxPool3d((2, 2, 2))\n\n        # Encoder convolutions\n        self.conv_blk1 = Conv3D_Block(num_channels, feat_channels[0], residual=residual)\n        self.conv_blk2 = Conv3D_Block(feat_channels[0], feat_channels[1], residual=residual)\n        self.conv_blk3 = Conv3D_Block(feat_channels[1], feat_channels[2], residual=residual)\n        self.conv_blk4 = Conv3D_Block(feat_channels[2], feat_channels[3], residual=residual)\n        self.conv_blk5 = Conv3D_Block(feat_channels[3], feat_channels[4], residual=residual)\n\n        # Decoder convolutions\n        self.dec_conv_blk4 = Conv3D_Block(2 * feat_channels[3], feat_channels[3], residual=residual)\n        self.dec_conv_blk3 = Conv3D_Block(2 * feat_channels[2], feat_channels[2], residual=residual)\n        self.dec_conv_blk2 = Conv3D_Block(2 * feat_channels[1], feat_channels[1], residual=residual)\n        self.dec_conv_blk1 = Conv3D_Block(2 * feat_channels[0], feat_channels[0], residual=residual)\n\n        # Decoder upsamplers\n        self.deconv_blk4 = Deconv3D_Block(feat_channels[4], feat_channels[3])\n        self.deconv_blk3 = Deconv3D_Block(feat_channels[3], feat_channels[2])\n        self.deconv_blk2 = Deconv3D_Block(feat_channels[2], feat_channels[1])\n        self.deconv_blk1 = Deconv3D_Block(feat_channels[1], feat_channels[0])\n\n        # Final 1*1 Conv Segmentation map\n        self.one_conv = Conv3d(feat_channels[0],2, kernel_size=1, stride=1, padding=0, bias=True)\n\n        # Activation function\n        self.sigmoid = Sigmoid()\n\n    def forward(self, x):\n        # Encoder part\n\n        x1 = self.conv_blk1(x)\n\n        x_low1 = self.pool1(x1)\n        x2 = self.conv_blk2(x_low1)\n\n        x_low2 = self.pool2(x2)\n        x3 = self.conv_blk3(x_low2)\n\n        x_low3 = self.pool3(x3)\n        x4 = self.conv_blk4(x_low3)\n\n        x_low4 = self.pool4(x4)\n        base = self.conv_blk5(x_low4)\n\n        # Decoder part\n\n        d4 = torch.cat([self.deconv_blk4(base), x4], dim=1)\n        d_high4 = self.dec_conv_blk4(d4)\n\n        d3 = torch.cat([self.deconv_blk3(d_high4), x3], dim=1)\n        d_high3 = self.dec_conv_blk3(d3)\n        d_high3 = Dropout3d(p=0.5)(d_high3)\n\n        d2 = torch.cat([self.deconv_blk2(d_high3), x2], dim=1)\n        d_high2 = self.dec_conv_blk2(d2)\n        d_high2 = Dropout3d(p=0.5)(d_high2)\n\n        d1 = torch.cat([self.deconv_blk1(d_high2), x1], dim=1)\n        d_high1 = self.dec_conv_blk1(d1)\n\n        seg = self.sigmoid(self.one_conv(d_high1))\n\n        return seg\n\n\nclass Conv3D_Block(Module):\n\n    def __init__(self, inp_feat, out_feat, kernel=3, stride=1, padding=1, residual=None):\n\n        super(Conv3D_Block, self).__init__()\n\n        self.conv1 = Sequential(\n            Conv3d(inp_feat, out_feat, kernel_size=kernel,\n                   stride=stride, padding=padding, bias=True),\n            BatchNorm3d(out_feat),\n            ReLU())\n\n        self.conv2 = Sequential(\n            Conv3d(out_feat, out_feat, kernel_size=kernel,\n                   stride=stride, padding=padding, bias=True),\n            BatchNorm3d(out_feat),\n            ReLU())\n\n        self.residual = residual\n\n        if self.residual is not None:\n            self.residual_upsampler = Conv3d(inp_feat, out_feat, kernel_size=1, bias=False)\n\n    def forward(self, x):\n\n        res = x\n\n        if not self.residual:\n            return self.conv2(self.conv1(x))\n        else:\n            return self.conv2(self.conv1(x)) + self.residual_upsampler(res)\n\n\nclass Deconv3D_Block(Module):\n\n    def __init__(self, inp_feat, out_feat, kernel=3, stride=2, padding=1):\n        super(Deconv3D_Block, self).__init__()\n\n        self.deconv = Sequential(\n            ConvTranspose3d(inp_feat, out_feat, kernel_size=(kernel, kernel, kernel),\n                            stride=(stride, stride, stride), padding=(padding, padding, padding), output_padding=1, bias=True),\n            ReLU())\n\n    def forward(self, x):\n        return self.deconv(x)\n\n\nclass ChannelPool3d(AvgPool1d):\n\n    def __init__(self, kernel_size, stride, padding):\n        super(ChannelPool3d, self).__init__(kernel_size, stride, padding)\n        self.pool_1d = AvgPool1d(self.kernel_size, self.stride, self.padding, self.ceil_mode)\n\n    def forward(self, inp):\n        n, c, d, w, h = inp.size()\n        inp = inp.view(n, c, d * w * h).permute(0, 2, 1)\n        pooled = self.pool_1d(inp)\n        c = int(c \/ self.kernel_size[0])\n        return inp.view(n, c, d, w, h)","ee04522d":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","2e32b9c4":"import torch.nn.functional as F\n\ndef soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n\n    \n    y_true = torch.cat((1-y_true , y_true),dim=1)\n    y_pred = F.softmax(y_pred,dim = 1)\n    #print(type(y_true))\n    #print(type(y_pred))\n    axes = (2,3,4)\n    numerator = 2. * torch.sum(y_pred * y_true, axes)\n    denominator = torch.sum(torch.square(y_pred) + torch.square(y_true), axes)\n    \n    return 1 - torch.mean((numerator + epsilon) \/ (denominator + epsilon)) ","f95b0138":"from torch.autograd import Variable\n#!pip install torchsummaryX\n#from torchsummaryX import summary\nfrom torch.autograd import Variable\n\n\nUnet =UNet(residual='pool')\n#data = Variable(torch.randn(1,1,64,64,32))\n#summary(Unet,data)\n#out = net(data)\n#print(out.shape)","fbd15c5f":"import torch\nimport numpy as np \n\n\n# PyTroch version\n\nSMOOTH = 1e-6\n\ndef iou(outputs: np.array, labels: np.array):\n    #outputs = outputs.squeeze(1)\n    \n    #print(outputs.shape)\n    #print(labels.shape)\n    \n    intersection = (outputs & labels).sum((0,1))\n    union = (outputs | labels).sum((0,1))\n    \n    iou = (intersection + SMOOTH) \/ (union + SMOOTH)\n    \n    return iou","f0adc618":"device = get_default_device()\ndevice","5e875ca6":"Unet = Unet.to(device)","5ae89238":"import torch.nn.functional as F\nfrom torch.optim.lr_scheduler import MultiStepLR\n\ndef fit(epochs, lr, model, train_loader,opt_func):\n    optim = opt_func(model.parameters(),lr=lr)\n    scheduler = MultiStepLR(optim, milestones=milestone, gamma=0.2)\n    k=0\n    for epoch in range(epochs):\n        model.train()\n        cum_loss = 0\n        m = 0\n        for batch_data in train_loader:\n            optim.zero_grad()\n            inputs = Variable(batch_data[0]).to(device)\n            labels = Variable(batch_data[1]).to(device).long()\n            outputs = model(inputs)\n            loss = soft_dice_loss(labels,outputs)\n            cum_loss = cum_loss + loss.item()\n            loss.backward()\n            optim.step()  \n            print(loss.item())\n        scheduler.step()\n        k = k+1\n        torch.cuda.empty_cache()\n        wandb.log({'Dice_Loss':cum_loss\/len(train_loader)})\n        torch.save(model.state_dict(), \"tumor_detection_1_4_0.2.pth\")\n        print(\"****************\")\n        print(\"****************\")\n        print(cum_loss\/len(train_loader))\n        print(\"****************\")\n        print(\"****************\")","bcadd1c9":"lr = 0.0005\nepochs = 100\noptimizer = torch.optim.Adam\nmilestone = [40,60,80]","a2dd1fe8":"fit(epochs, lr,Unet, train_d,optimizer)","56bed2fc":"torch.save(Unet.state_dict(), \"tumor_detection_1_4_0.2.pth\")","f306bda9":"cum_iou = 0\nfor img,label in val_d:\n    out = Unet(img.to(device))\n    out = out.squeeze()\n    out = torch.argmax(out,dim=0)\n    out = out.reshape(32,64,64)\n    out = out.cpu().detach().numpy()\n    label = label.squeeze()\n    label = label.reshape(32,64,64)\n    label= label.cpu().detach().numpy()\n    label = label.astype('int64')\n    for i in range(32):\n        cum_iou += iou(out[i],label[i])\n\nwandb.log({'VAL_IOU':cum_iou\/(32*len(val_d))})    \nprint(\"IOU = \",cum_iou\/(32*len(val_d)))\n        ","3d820d39":"class_labels = {0: \"t\", 1: \"nt\"}\nprint(class_labels[0])","dc227888":"for img,label in val_d:\n    out = Unet(img.to(device))\n    out = out.squeeze()\n    out = torch.argmax(out,dim=0)\n    out = out.reshape(32,64,64)\n    out = out.cpu().detach().numpy()\n    img  = img.squeeze()\n    img =  img.reshape(32,64,64).cpu().detach().numpy()\n    label = label.squeeze()\n    label = label.reshape(32,64,64)\n    label= label.cpu().detach().numpy()\n    label = label.astype('int64')\n    for i in range(32):\n        prediction_mask = out[i]\n        ground_truth_mask = label[i]\n        original_image = img[i]\n        wandb.log({\"image\" : wandb.Image(original_image, masks={\n               \"ground_truth\" : {\n               \"mask_data\" : ground_truth_mask,\n               \"class_labels\" : class_labels \n               },\n                \"predictions\" : {\n                \"mask_data\" : prediction_mask,\n                \"class_labels\" : class_labels\n                } } ) })\n      ","563d8280":"#plt.imshow(label[0])\nprint(img.dtype)\n#plt.imshow(out[0])","326a195d":"# Utils","aa0ffb9f":"# Data loading ","44061d30":"# Importing and visualizing Data","32e39ac3":"# Model"}}