{"cell_type":{"adca99d4":"code","151ec54b":"code","aa70ac71":"code","80e2322b":"code","5f06ba66":"code","b11385d1":"code","2b947f86":"code","579b7668":"code","49013e6b":"code","eba98d85":"code","1f5ba840":"code","a300d383":"code","2d434df9":"code","a3539904":"code","45d95dcc":"code","96e19c4b":"code","42ea1460":"code","2b6f8cd7":"code","cf604ec4":"code","8cc2bd7a":"code","040ff565":"code","3790a2e0":"code","52d0f0aa":"code","fd3de3e1":"code","921c3fce":"markdown","2d23533e":"markdown","9ae1f22e":"markdown","879df46e":"markdown","b52380b5":"markdown","31823b9d":"markdown","2ccd480f":"markdown","a44f8d86":"markdown","f6ecc956":"markdown","2ca0976e":"markdown","ba4d9c19":"markdown","55284b56":"markdown"},"source":{"adca99d4":"import numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport os\nimport seaborn as sns\n","151ec54b":"df = pd.read_csv(\"..\/input\/iris-flower-dataset\/IRIS.csv\")\ndf.head()","aa70ac71":"X = df.drop('species', 1)\ny = df['species']\nX,y","80e2322b":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n#X_train,X_test,y_train,y_test","5f06ba66":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n#X_train, X_test","b11385d1":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","2b947f86":"X_train = pca.fit_transform(X_train)\nX_train.shape","579b7668":"??pca.fit_transform","49013e6b":"explained_variance = pca.explained_variance_ratio_\n\nexplained_variance","eba98d85":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=1)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","1f5ba840":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(max_depth=2, random_state=0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)","a300d383":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Accuracy =',accuracy_score(y_test, y_pred))","2d434df9":"#pima diabetes dataset\ndib_df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndib_df","a3539904":"dib_df.info()","45d95dcc":"corr = dib_df.corr()\ncorr\nsns.heatmap(corr,xticklabels =corr.columns,yticklabels = corr.columns)","96e19c4b":"dib = []\nnon_dib = []\n\nfor i in range(768):\n    if dib_df['Outcome'][i]==1:\n        dib.append(1)\n    else:\n        non_dib.append(0)\nsize_dib = len(dib)\nsize_non_dib = len(non_dib)\nprint(f'Diabetes = {size_dib}')\nprint(f'Non-Diabetes = {size_non_dib}')","42ea1460":"dftrain = dib_df[:650]\ndftest = dib_df[650:750]\n# dftrain,dftest","2b6f8cd7":"trainLabel = np.asarray(dftrain['Outcome'])\ntrainData = np.asarray(dftrain.drop('Outcome',1))\ntestLabel = np.asarray(dftest['Outcome'])\ntestData = np.asarray(dftest.drop('Outcome',1))","cf604ec4":"means = np.mean(trainData, axis=0)\nstds = np.std(trainData, axis=0)\ntrainData = (trainData - means)\/stds\ntestData = (testData - means)\/stds\n\n# np.mean(trainData, axis=0) => check that new means equal 0\n# np.std(trainData, axis=0) => check that new stds equal 1","8cc2bd7a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nimport joblib","040ff565":"diabetesCheck = LogisticRegression()\ndiabetesCheck.fit(trainData, trainLabel)","3790a2e0":"accuracy = diabetesCheck.score(testData, testLabel)\nprint(\"accuracy = \", accuracy * 100, \"%\")","52d0f0aa":"#Saving the Model...\"Now we will save our trained model for future use using joblib.\"\n\njoblib.dump([diabetesCheck, means, stds], 'diabeteseModel.pkl')","fd3de3e1":"diabetesLoadedModel, means, stds = joblib.load('diabeteseModel.pkl')\naccuracyModel = diabetesLoadedModel.score(testData, testLabel)\nprint(\"accuracy = \",accuracyModel * 100,\"%\")","921c3fce":"# Applying PCA\n\n*It is only a matter of three lines of code to perform PCA using Python's Scikit-Learn library. The PCA class is used for this purpose. PCA depends only upon the feature set and not the label data. Therefore, PCA can be considered as an unsupervised machine learning technique.*","2d23533e":"[Stackoverflow](https:\/\/stackoverflow.com\/questions\/56694980\/valueerror-n-components-4-must-be-between-0-and-minn-samples-n-features-2-wi)\n\n# Results with 2 and 3 Principal Components\n\nNow after evaluating classification performance of the random forest algorithm with 2 principal components. Update this piece of code:\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nThe number of components for PCA has been set to 2. The classification results with 2 components with accuracy = 0.8\n\n*With two principal components the classification accuracy decreases to 80.00% compared to 93.33% for 1 component.*\n\n**With three principal components, the result looks like this: 0.90**\n\n*Results with Full Feature Set was 90.00%*\n","9ae1f22e":"**As mentioned earlier, PCA performs best with a normalized feature set. We will perform standard scalar normalization to normalize our feature set. To do this, execute the following code:**","879df46e":"*The PCA class contains explained_variance_ratio_ which returns the variance caused by each of the principal components.*","b52380b5":"# # Logistic Regression","31823b9d":"**Step 1:**\nThe first preprocessing step is to divide the dataset into a feature set and corresponding labels.","2ccd480f":"# Feature Engineering using PCA\n\n**Principal Component Analysis or PCA, is a statistical technique to convert high dimensional data to low dimensional data by selecting the most important features that capture maximum information about the dataset. The features are selected on the basis of variance that they cause in the output. The feature that causes highest variance is the first principal component. The feature that is responsible for second highest variance is considered the second principal component, and so on. It is important to mention that principal components do not have any correlation with each other.**\n\n*Advantages of PCA\nThere are two main advantages of dimensionality reduction with PCA.\nThe training time of the algorithms reduces significantly with less number of features.\nIt is not always possible to analyze data in high dimensions. For instance if there are 100 features in a dataset. Total number of scatter plots required to visualize the data would be 100(100-1)2 = 4950. Practically it is not possible to analyze data this way.*","a44f8d86":"# Training and Making Predictions\n\n*In this case we'll use random forest classification for making the predictions.*","f6ecc956":"# Performance Evaluation","2ca0976e":"**It can be seen that first principal component is responsible for 72.22% variance. Similarly, the second principal component causes 23.9% variance in the dataset. Collectively we can say that (72.22 + 23.9) 96.21% percent of the classification information contained in the feature set is captured by the first two principal components.**\n\n*Let's first try to use 1 principal component to train our algorithm. To do so, execute the following code*","ba4d9c19":"*It can be seen from the output that with only one feature, the random forest algorithm is able to correctly predict 28 out of 30 instances, resulting in 93.33% accuracy.*","55284b56":"**Normalization of Features\nIt is imperative to mention that a feature set must be normalized before applying PCA. For instance if a feature set has data expressed in units of Kilograms, Light years, or Millions, the variance scale is huge in the training set. If PCA is applied on such a feature set, the resultant loadings for features with high variance will also be large. Hence, principal components will be biased towards features with high variance, leading to false results.** \n\n*Finally, the last point to remember before we start coding is that PCA is a statistical technique and can only be applied to numeric data. Therefore, categorical features are required to be converted into numerical features before PCA can be applied.*"}}