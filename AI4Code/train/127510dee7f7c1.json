{"cell_type":{"5a302bc6":"code","479db4cf":"code","d6c63771":"code","19768c5d":"code","305e67e9":"code","5e1264f5":"code","93d8cbb1":"code","5ef95eb8":"code","4b611a8c":"code","1b1b911c":"code","325dd906":"code","b963144b":"code","3f977306":"code","1d224516":"code","ecb59cc1":"code","bc149755":"code","4935ca87":"code","17992562":"code","6be772fc":"code","bc4e474e":"code","b4de3161":"code","1b4d64c5":"code","0cdd9481":"code","f49f6341":"code","a2545a4d":"code","5d59eb28":"code","14f5a0dd":"code","8fca0cb3":"code","d211b520":"code","40c6ac54":"code","e81a39f9":"code","2433b26a":"code","bc2a59e3":"code","fc5bed62":"code","fc44f2cf":"code","5fc0cd5f":"code","99fdba86":"code","68d09916":"code","561adacc":"code","042a3f8f":"code","0690f48d":"code","dadadda2":"code","8dcfaacf":"code","519de5f4":"code","260d8d54":"code","b8f1dd04":"code","24daee76":"code","84e2d00d":"code","23b4cd72":"code","6190f2a7":"code","4df47efe":"code","16c1c7b7":"code","7a2388dc":"code","bb4c90a2":"code","0ce1790b":"code","15283c2e":"code","0bfec058":"code","9e9e7b01":"code","a5bd3a98":"code","f4db1f06":"code","87b2d1b1":"code","a3f1bdb8":"code","70c9e82d":"code","76a1444a":"code","2cfc6175":"code","0fa00555":"code","5b50e2af":"code","7ab3d72a":"code","c0809942":"code","d4addd44":"code","279aa902":"code","0e6a8890":"code","b9fa3948":"code","13b0214c":"code","f9aa8838":"code","051f46f1":"code","0cfff7a0":"code","310ebbf2":"code","38fb48ea":"code","e1437871":"code","b4d62e49":"code","296398c2":"code","1e638994":"code","d55f32f1":"code","ef617009":"code","81da2303":"code","ff2ac3a3":"code","3d9a4dc5":"code","3c7f7d92":"code","a93de534":"code","a7c16003":"code","fe4fd124":"code","d0288973":"code","6bc6c835":"code","1a10f7e7":"code","af5aeacb":"code","570758f4":"code","4e5c3821":"code","d6dd90e9":"code","e0b013ea":"code","2e6c2860":"code","b97e41a5":"code","4fefb39c":"code","3c6c812d":"code","0be7dd22":"code","5aa9da5a":"code","7b178687":"code","72e794d0":"code","e7b7bb82":"code","fae4c17f":"code","7f1629bf":"code","36618958":"code","c359df04":"code","1dd65cc6":"code","ed6c45ce":"code","9bf9f127":"code","24d92245":"code","208e2112":"code","9376c5f6":"code","b7ecda50":"code","beaf428e":"code","0b7b235b":"code","0b2e1913":"code","d2df9f0a":"code","f9e74eab":"code","fe032ea5":"code","208170b5":"code","aa112dd7":"code","69ed40a3":"code","95dc716b":"code","df4b845a":"code","55af3965":"code","5fa0f0b2":"code","348d8df5":"code","819ef690":"code","045e16cb":"code","d0cc768f":"code","899030d3":"code","cf2170fb":"code","b85da248":"code","cebded47":"code","d8431b84":"code","fa1f1b10":"code","367ce31b":"code","33343d69":"code","fe31705a":"code","b6dd36ee":"code","52cd9407":"code","a988ce7f":"code","c978770a":"code","e710fbba":"code","dad97010":"code","3368056c":"code","c0d6cb5c":"code","18a9cdca":"code","4cee6e02":"code","6460369b":"code","2bc5dd7a":"code","03f96533":"code","7c7a89db":"code","f1ac6edd":"code","2b9166c2":"code","905ad280":"code","b6099dc4":"code","a0101e8f":"code","9f9422d8":"code","475a48d7":"markdown","2e2e742d":"markdown","6513d77a":"markdown","ba4db9d0":"markdown","858c56e0":"markdown","321f8528":"markdown","41e81f23":"markdown","e1084a8d":"markdown","0a666a77":"markdown","67c36738":"markdown","e1ea281f":"markdown","d7684a62":"markdown","bdda3d5f":"markdown","930231d1":"markdown","45478bba":"markdown","512a2ad1":"markdown","71b8f460":"markdown","ebe42633":"markdown","2895b043":"markdown","40c2d168":"markdown","56a57ef1":"markdown","3af74244":"markdown","fbd03d99":"markdown","6d13876c":"markdown","65db1292":"markdown","8d97c93a":"markdown","fd9d52a0":"markdown","530e4ef4":"markdown","4d0f05fb":"markdown","b26b10ec":"markdown","0b13100f":"markdown","9dc9f809":"markdown","690fe757":"markdown","002aad2a":"markdown","f4b9999b":"markdown","cc6e460c":"markdown","0ef4fc06":"markdown","94b6e3a4":"markdown","8056e1a1":"markdown","1196f6bb":"markdown","420da760":"markdown","79b2a4fa":"markdown","a5a0af0b":"markdown","be9daa9b":"markdown","f19b3a11":"markdown","3caf9ed4":"markdown","6240fe01":"markdown","5bc2a0e7":"markdown","ad148bfd":"markdown","d94b4757":"markdown","254fc132":"markdown","b6c7d560":"markdown","56d192c9":"markdown","bf803063":"markdown","048d9a56":"markdown","5bfdc999":"markdown","8170a250":"markdown","c636daf8":"markdown","1c815b66":"markdown","0d439d16":"markdown","c27d3b99":"markdown","d6c93d46":"markdown","f6ef85b4":"markdown","a6545271":"markdown","0f65a2ad":"markdown","53422da4":"markdown","fd5fea41":"markdown","0a045418":"markdown","feecb667":"markdown","2b8748c0":"markdown","7bf803f0":"markdown","f151e9fd":"markdown","c02aaea9":"markdown","38562e9c":"markdown","37e25e5c":"markdown","d9b2e006":"markdown","9abdc64f":"markdown","885eaf90":"markdown","6f82fa60":"markdown","c9def78b":"markdown","1c127016":"markdown","3affa34d":"markdown","4e9671fa":"markdown","ec9fce38":"markdown","748a3ebc":"markdown","c3e5e09a":"markdown","65605f70":"markdown"},"source":{"5a302bc6":"#Install relevant external libraries (Remove '#' below to install packages you may not have)\n!pip install comet_ml\n!pip install emoji\n!pip install wordcloud\n!pip install textblob\n!pip install nltk\n!pip install spacy\n!pip install tensorflow","479db4cf":"# Import packages needed to solve the problem\n\n# Data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n# General natural language and text processing tools\nimport re\nimport string\nimport emoji\nimport gensim\nimport spacy\nimport sklearn.feature_extraction.text\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import defaultdict\nfrom collections import  Counter\n\n# Enhanced natural language and text processing tools\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\n\n# Processing data to prior to trainig and fitting to model\nfrom sklearn.model_selection import train_test_split\n\n# Stemming\/lemmatizing\/vectorizing\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import RegexpTokenizer\n\n# Data visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Set plot style for data visualisation\nsns.set()\n# for improved aesthetics\nplt.style.use('ggplot')    \n\n# Importing wordcloud for plotting word clouds and \nfrom wordcloud import WordCloud\n# textwrap for wrapping longer text\nfrom textwrap import wrap\n\n# Machine Learning model versioning\nfrom comet_ml import Experiment\n\n# Machine learning model evaluation\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n\n# Ignore warnings because they are annoying\nimport warnings\nwarnings.filterwarnings('ignore')","d6c63771":"# Load train data\ntrain_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/Vicky-hub87\/Team_ss1_Jhb-Classification-Predict\/master\/train.csv')","19768c5d":"# Load test data\ntest_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/Vicky-hub87\/Team_ss1_Jhb-Classification-Predict\/master\/test.csv')","305e67e9":"# Load sample data\nsample_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/Vicky-hub87\/Team_ss1_Jhb-Classification-Predict\/master\/sample_submission.csv')\n","5e1264f5":"# Print train_data\nprint('Train data rows and columns:')\ntrain_data.sample(10)","93d8cbb1":"# Print test_data\nprint('Test data rows and columns:')\ntest_data.sample(10)","5ef95eb8":"print('Train data rows and columns:', train_data.shape)\nprint('Test data rows and columns:', test_data.shape)\nprint('Sample data rows and columns:', sample_data.shape)","4b611a8c":"# View all the class labels for the target variable\ntrain_data['sentiment'].unique()","1b1b911c":"# View the breakdown of the different class labels\ntrain_data['sentiment'].value_counts()","325dd906":"# Calculate class label proportions\nanti_class_proportion = len(train_data.loc[train_data['sentiment']== -1]) \/ len(train_data)\nneut_class_proportion = len(train_data.loc[train_data['sentiment']==0]) \/ len(train_data)\npro_class_proportion = len(train_data.loc[train_data['sentiment']==1]) \/ len(train_data)\nnews_class_proportion = len(train_data.loc[train_data['sentiment']==2]) \/ len(train_data)\n\n# View class label proportions\n\nprint(\"Anti class proportion: -1 =\", round((anti_class_proportion),2))\nprint(\"Neutral class proportion: 0 =\",round((neut_class_proportion),2))\nprint(\"Pro class proportion: 1 =\",round((pro_class_proportion),2))\nprint(\"News class proportion: 2 =\",round((news_class_proportion),2))","b963144b":"# Calculate and confirm that the above proportions are in fact equal to 1 (or 100%)\ntotal_class = anti_class_proportion + neut_class_proportion + pro_class_proportion + news_class_proportion\ntotal_class == 1","3f977306":"# Set figure size for distribution of class imbalance\nfig, ax = plt.subplots(figsize = (10, 5))\n\n# Create ditribution bar graph \ngraph = sns.countplot(x = 'sentiment', data = train_data)\n\n# Give title and plot\nplt.title('Distribution of Classification Groups')\nplt.xlabel('Sentiment class labels')\nplt.ylabel('Number of tweets')\nplt.show(graph)","1d224516":"import matplotlib.pyplot as plt\n\n# To view proportions of the class labels, it is best practice to use pie charts\n# Where the slices will be ordered and plotted counter-clockwise:\nlabels = 'Anti', 'Neutral', 'Pro', 'News'\nsizes = [anti_class_proportion, neut_class_proportion, pro_class_proportion, news_class_proportion]\nexplode = (0, 0, 0.1, 0)  # Only \"explore\" the 3rd slice (i.e. 'Anti')\n\n# Create pie chart with the above labels and calculated class proportions as inputs\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","ecb59cc1":"# View missing values for train data\ntrain_data.isnull().sum()","bc149755":"blanks = []  # start with an empty list\n\nfor i,lb,msg,tid in train_data.itertuples():  # iterate over the DataFrame\n    if type(msg)==str:            # avoid NaN values\n        if msg.isspace():         # test 'review' for whitespace\n            blanks.append(i)     # add matching index numbers to the list\n        \nprint(len(blanks), 'blanks: ', blanks)","4935ca87":"# Check for duplicate messages\/tweets in the train data\ndups_train = train_data['message']\ndups_train = train_data[dups_train.isin(dups_train[dups_train.duplicated()])].sort_values(\"message\")\n# Check for duplicate tweet ID's in the train data to validate that each message is unique\ndups_tweet_tr = train_data['tweetid']\ntrain_data[dups_tweet_tr.isin(dups_tweet_tr[dups_tweet_tr.duplicated()])].sort_values(\"message\")\n# Check for duplicate messages\/tweets in the test data\ndups_test = test_data['message']\ndups_test = test_data[dups_test.isin(dups_test[dups_test.duplicated()])].sort_values(\"message\")\n# Check for duplicate tweet ID's in the test data to validate that each message is unique\ndups_tweet_te = train_data['tweetid']\ntrain_data[dups_tweet_te.isin(dups_tweet_te[dups_tweet_te.duplicated()])].sort_values(\"message\")\n\nprint('Duplicate tweet messages in train data rows and columns:',dups_train.shape)\nprint('Duplicate tweet messages in test data rows and columns:',dups_test.shape)","17992562":"#Confirm whether sample of duplicates in train data is generally comprised of retweets\ndups_train.sample(10)","6be772fc":"#Confirm whether sample of duplicates in test data is generally comprised of retweets\ndups_test.sample(10)","bc4e474e":"# View the differences in proportions of duplicates across train and test datasets\ndups_train_prop = len(dups_train)\/len(train_data['message'])\ndups_test_prop = len(dups_test)\/len(test_data['message'])\nprint('Train data proportion of duplicates\/RTs:',round((dups_train_prop),2))\nprint('Test data proportion of duplicates\/RTs:',round((dups_test_prop),2))","b4de3161":"# View sample tweet messages\ntrain_data['message'].unique()","1b4d64c5":"# Try removing non-ASCII strings as a start e.g. by\u00c3\u00a2\u00e2\u201a\u00ac\u00c2\u00a6\ntrain_data['message']=train_data['message'].apply(lambda x: x.split('\u00c3\u00a2\u00e2\u201a\u00ac\u00c2\u00a6')[0])","0cdd9481":"# View change to sample tweet messages\ntrain_data['message'].unique()","f49f6341":"# Remove URL's\ntrain_data['message']=train_data['message'].apply(lambda x: re.sub(r\"\\bhttps:\/\/t.co\/\\w+\", '', x))","a2545a4d":"# View change to sample tweet messages\ntrain_data['message'].unique()","5d59eb28":"# Remove Line breaks\ntrain_data['message']=train_data['message'].replace('\\n', ' ')","14f5a0dd":"# View change to sample tweet messages\ntrain_data['message'].unique()","8fca0cb3":"# Remove numbers\ntrain_data['message']=train_data['message'].apply(lambda x: re.sub('\\w*\\d\\w*', ' ', x))","d211b520":"# View change to sample tweet messages\ntrain_data['message'].unique()","40c6ac54":"# Remove capital letters and punctuations\n\ntrain_data['message']=train_data['message'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))","e81a39f9":"# View change to sample tweet messages\ntrain_data['message'].unique()","2433b26a":"# Dictionary of English Contractions\ncontractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\", \"it's\": \"it is\" ,\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\", \"we\u2019 ve\": \"we have\", \"imvotingbecause\": \"i am voting because\", \"rt\": \"\" }\n\n# Regular expression for finding contractions\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n  def replace(match):\n    return contractions_dict[match.group(0)]\n  return contractions_re.sub(replace, text)\n\n# Expanding Contractions in the reviews\ntrain_data['message']=train_data['message'].apply(lambda x:expand_contractions(x))","bc2a59e3":"# View change to sample tweet messages\ntrain_data['message'].unique()","fc5bed62":"# Convert capital letters to lowercase\ntrain_data['message']=train_data['message'].apply(lambda x: x.lower())","fc44f2cf":"# View change to sample tweet messages\ntrain_data['message'].unique()","5fc0cd5f":"# Remove digits and words containing digits\ntrain_data['message']=train_data['message'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))","99fdba86":"# View change to sample tweet messages\ntrain_data['message'].unique()","68d09916":"# Remove punctuation\n\ntrain_data['message']=train_data['message'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))","561adacc":"# View change to sample tweet messages\ntrain_data['message'].unique()","042a3f8f":"# Remove extra spaces\ntrain_data['message']=train_data['message'].apply(lambda x: re.sub(' +',' ',x))","0690f48d":"# View change to sample tweet messages\ntrain_data['message'].unique()","dadadda2":"# View changes to train_data\ntrain_data.head()","8dcfaacf":"# Function to remove emojis\n#!pip install emoji\n#import emoji\n#import string\n\ndef give_emoji_free_text(text):\n    \n    '''\n    Takes in tweet series, removes all emojis, and returns cleaned tweet series.\n    '''\n    \n    allchars = [str for str in text]\n    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n\n    return clean_text","519de5f4":"# Remove emojis\nfull_text_list = []\n\nfor index, rows in train_data['message'].iteritems():\n    rows = give_emoji_free_text(rows) # remove emojis\n    full_text_list.append(rows)\n    \ntrain_data['message'] = full_text_list","260d8d54":"# View changes to train_data\ntrain_data['message'].unique()","b8f1dd04":"# Try removing non-ASCII strings if they persist e.g. i\u00e3\u00a2\u00e2\u201a\u00ac\u00e2\u00a6\ntrain_data['message']=train_data['message'].apply(lambda x: x.split('i\u00e3\u00a2\u00e2\u201a\u00ac\u00e2\u00a6')[0])","24daee76":"# View sample changes of train_data tweet messages in plain text\nfor index,text in enumerate(train_data['message'][35:40]):\n  print('Message %d:\\n'%(index+1),text)","84e2d00d":"# Loading model\nnlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])","23b4cd72":"# Lemmatization with stopwords removal\ntrain_data['lemmatized']=train_data['lemmatized']=train_data['message'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\ntrain_data['lemmatized']","6190f2a7":"# View changes to train_data lemmatized (though this is not perfect at this stage)\ntrain_data_grouped=train_data[['message','lemmatized']].groupby(by='message').agg(lambda x:' '.join(x))\ntrain_data_grouped.head()","4df47efe":"# View changes to train_data\ntrain_data.head()","16c1c7b7":"# View sample of lemmatized changes to train_data\ntrain_data['lemmatized'].unique()","7a2388dc":"# View sample of lemmatized changes of train_data tweet messages in plain text\nfor index,text in enumerate(train_data['lemmatized'][35:40]):\n  print('Lemmatized Message %d:\\n'%(index+1),text)","bb4c90a2":"# Creating Document Term Matrix\ntoken = RegexpTokenizer(r'[@a-zA-Z0-9]+')\ncv=CountVectorizer(analyzer='word',lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\ntext_counts=cv.fit_transform(train_data['lemmatized'])\ndf_dtm = pd.DataFrame(text_counts.toarray(), columns=cv.get_feature_names())\ndf_dtm.index=train_data['lemmatized'].index\ndf_dtm.sample(10)","0ce1790b":"# Load train data\ntrain_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/Vicky-hub87\/Team_ss1_Jhb-Classification-Predict\/master\/train.csv')","15283c2e":"# Load test data\ntest_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/Vicky-hub87\/Team_ss1_Jhb-Classification-Predict\/master\/test.csv')\n","0bfec058":"# Load sample data\nsample_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/Vicky-hub87\/Team_ss1_Jhb-Classification-Predict\/master\/sample_submission.csv')\n","9e9e7b01":"def get_top_tweet_unigrams(corpus, n=None):\n    '''\n    Function returns a unigram \n\n    '''\n\n    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","a5bd3a98":"#Initial minimally cleaned text for top 20 unigram count\nplt.figure(figsize=(10,5))\ntop_tweet_unigrams = get_top_tweet_unigrams(train_data['message'])[:20]\nx,y = map(list,zip(*top_tweet_unigrams))\nsns.barplot(x=y, y=x)","f4db1f06":"def get_top_tweet_bigrams(corpus, n=None):\n    '''\n    Function returns a biigram \n    '''\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","87b2d1b1":"# Initial minimally cleaned text for top 20 bigram count\nplt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(train_data['message'])[:20]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","a3f1bdb8":"# Create column for the number of words in tweet\ntrain_data['word_count'] = train_data['message'].apply(lambda x: len(x.split()))\n\n# Split so we can use updated train set with new feature\ntrain_data = train_data[:len(train_data)]\n\n# Define subplot to see graphs side by side\nfig, ax = plt.subplots(figsize = (10, 5))\n\n#create graphs\nsns.kdeplot(train_data['word_count'][train_data['sentiment'] == 0], shade = True, label = 'Neutral')\nsns.kdeplot(train_data['word_count'][train_data['sentiment'] == 1], shade = True, label = 'Pro')\nsns.kdeplot(train_data['word_count'][train_data['sentiment'] == 2], shade = True, label = 'News')\nsns.kdeplot(train_data['word_count'][train_data['sentiment'] == -1], shade = True, label = 'Anti')\n\n# Set title and plot\nplt.title('Distribution of Tweet Word Count')\nplt.xlabel('Word Count')\nplt.ylabel('Sentiments Proportions')\nplt.show()","70c9e82d":"# Create column for the number of characters in a tweet\ntrain_data['character_count'] = train_data['message'].apply(lambda x: len(x))\n\n# Split so we can use updated train set with new feature\ntrain_data = train_data[:len(train_data)]\n\n# Define subplot to see graphs side by side\nfig, ax = plt.subplots(figsize = (10, 5))\n\n# Create graphs\nsns.kdeplot(train_data['character_count'][train_data['sentiment'] == 0], shade = True, label = 'Neutral')\nsns.kdeplot(train_data['character_count'][train_data['sentiment'] == 1], shade = True, label = 'Pro')\nsns.kdeplot(train_data['character_count'][train_data['sentiment'] == 2], shade = True, label = 'News')\nsns.kdeplot(train_data['character_count'][train_data['sentiment'] == -1], shade = True, label = 'Anti')\n\n\n# Set title and plot\nplt.title('Distribution of Tweet Character Count')\nplt.xlabel('Character Count')\nplt.ylabel('Sentiment Probability')\nplt.show()","76a1444a":"def average_word_length(x):\n    '''\n    #Function to find average word length \n    '''\n    x = x.split()\n    return np.mean([len(i) for i in x])\n\n# Broadcast to text column\ntrain_data['average_word_length'] = train_data['message'].apply(average_word_length)\n\n# Split so we can use updated train set with new feature\ntrain_data = train_data[:len(train_data)]\n\n# Define subplot to see graphs side by side\nfig, ax = plt.subplots(figsize = (10, 5))\n\n# Create graphs\nsns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == 0], shade = True, label = 'Neutral')\nsns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == 1], shade = True, label = 'Pro')\nsns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == 2], shade = True, label = 'News')\nsns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == -1], shade = True, label = 'Anti')\n\n# Set title\nplt.title('Distribution of Tweet Average Word Length')\nplt.xlabel('Average Word Length')\nplt.ylabel('Sentiment Probability')\n\n# Plot graphs\nplt.show()","2cfc6175":"#add unique word count\ntrain_data['unique_word_count'] = train_data['message'].apply(lambda x: len(set(x.split())))\n\n#add stopword count\nstopwords = stopwords.words('english')\ntrain_data['stopword_count'] = train_data['message'].apply(lambda x: len([i for i in x.lower().split() if i in stopwords]))\n\n#add url count\ntrain_data['url_count'] = train_data['message'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))\n\n# add hashtag_count\ntrain_data['hashtag_count'] = train_data['message'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n#add mention count\ntrain_data['mention_count'] = train_data['message'].apply(lambda x: len([i for i in str(x) if i == '@']))\n\n#add punctuation count\ntrain_data['punctuation_count'] = train_data['message'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))\n\n#split so we can use updated train set\ntrain_data = train_data[:len(train_data)]\n\ndisaster = train_data['sentiment'].astype(int) == 1\n\n#produce graphs to visualize newly added features\nfig, axes = plt.subplots(6, figsize=(20, 30))\n\ngraph1 = sns.kdeplot(train_data.loc[~disaster]['unique_word_count'], shade = True, label = 'Neutral', ax=axes[0])\ngraph1 = sns.kdeplot(train_data.loc[disaster]['unique_word_count'], shade = True, label = 'Pro', ax=axes[0])\ngraph1 = sns.kdeplot(train_data.loc[disaster]['unique_word_count'], shade = True, label = 'News', ax=axes[0])\ngraph1 = sns.kdeplot(train_data.loc[disaster]['unique_word_count'], shade = True, label = 'Anti', ax=axes[0])\ngraph1.set_title('Distribution of Unique Word Count')\n# plt.xlabel('unique_word_count')\n\ngraph2 = sns.kdeplot(train_data.loc[~disaster]['stopword_count'], shade = True, label = 'Neutral', ax=axes[1])\ngraph2 = sns.kdeplot(train_data.loc[disaster]['stopword_count'], shade = True, label = 'Pro', ax=axes[1])\ngraph2 = sns.kdeplot(train_data.loc[disaster]['stopword_count'], shade = True, label = 'News', ax=axes[1])\ngraph2 = sns.kdeplot(train_data.loc[disaster]['stopword_count'], shade = True, label = 'Anti', ax=axes[1])\ngraph2.set_title('Distribution of Stopword Word Count')\n\ngraph3 = sns.kdeplot(train_data.loc[~disaster]['url_count'], shade = True, label = 'Neutral', ax=axes[2])\ngraph3 = sns.kdeplot(train_data.loc[disaster]['url_count'], shade = True, label = 'Pro', ax=axes[2])\ngraph3 = sns.kdeplot(train_data.loc[disaster]['url_count'], shade = True, label = 'News', ax=axes[2])\ngraph3 = sns.kdeplot(train_data.loc[disaster]['url_count'], shade = True, label = 'Anti', ax=axes[2])\ngraph3.set_title('Distribution of URL Count')\n\ngraph4 = sns.kdeplot(train_data.loc[~disaster]['hashtag_count'], shade = True,  label = 'Neutral', ax=axes[3], bw = 1)\ngraph4 = sns.kdeplot(train_data.loc[disaster]['hashtag_count'], shade = True, label = 'Pro', ax=axes[3], bw =1)\ngraph4 = sns.kdeplot(train_data.loc[disaster]['hashtag_count'], shade = True, label = 'News', ax=axes[3], bw =1)\ngraph4 = sns.kdeplot(train_data.loc[disaster]['hashtag_count'], shade = True, label = 'Anti', ax=axes[3], bw =1)\ngraph4.set_title('Distribution of Hashtag Count')\n\ngraph4 = sns.kdeplot(train_data.loc[~disaster]['mention_count'], shade = True,  label = 'Neutral', ax=axes[4], bw = 1)\ngraph4 = sns.kdeplot(train_data.loc[disaster]['mention_count'], shade = True, label = 'Pro', ax=axes[4], bw =1)\ngraph4 = sns.kdeplot(train_data.loc[disaster]['mention_count'], shade = True, label = 'News', ax=axes[4], bw =1)\ngraph4 = sns.kdeplot(train_data.loc[disaster]['mention_count'], shade = True, label = 'Anti', ax=axes[4], bw =1)\ngraph4.set_title('Distribution of Mention Count')\n\ngraph5 = sns.kdeplot(train_data.loc[~disaster]['punctuation_count'], shade = True, label = 'Neutral', ax=axes[5], bw = 1)\ngraph5 = sns.kdeplot(train_data.loc[disaster]['punctuation_count'], shade = True, label = 'Pro', ax=axes[5], bw = 1)\ngraph5 = sns.kdeplot(train_data.loc[disaster]['punctuation_count'], shade = True, label = 'News', ax=axes[5], bw = 1)\ngraph5 = sns.kdeplot(train_data.loc[disaster]['punctuation_count'], shade = True, label = 'Anti', ax=axes[5], bw = 1)\ngraph5.set_title('Distribution of Punctuation Count')\n\nfig.tight_layout()\nplt.show()","0fa00555":"# View first five parts of the new features\ntrain_data.head()","5b50e2af":"# Remove punctuation\ndef remove_punctuation(message):\n    return message.translate(str.maketrans('', '', string.punctuation))\n\n# Remove stopwords\nStopWords = stopwords\ndef remove_stopwords(message):\n    return ' '.join([i for i in message.split() if i not in StopWords])\n\n# Remove words less than 4 \ndef remove_less_than(message):\n    return ' '.join([i for i in message.split() if len(i) > 3])\n\n# Remove words with non-alphabet characters\ndef remove_non_alphabet(message):\n    return ' '.join([i for i in message.split() if i.isalpha() == True])\n\n# Stem words\nstemmer = SnowballStemmer('english')\ndef stem_words(message):\n    return stemmer.stem(message)\n\n# Lemmatize words for verb\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words_verb(message):\n    return lemmatizer.lemmatize(message, 'v')","7ab3d72a":"# Demonstrate the efficiency and effectiveness of Keras package\n# In contrast to RegExTokenizer from nltk libarary used in first iteration of text cleaning\nfrom keras.preprocessing.text import Tokenizer \n\ncontractions = [\"can't stop won't stop\"]\n\ntokenizer = Tokenizer(filters= \"'!#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\")\ntokenizer.fit_on_texts(contractions)\ntokenizer.word_index","c0809942":"def correct_contraction(message):\n    '''\n    Function will remove all the contractions from the text\n\n    '''\n    message = str(message).lower()\n    message = re.sub(r\"he's\", \"he is\", message)\n    message = re.sub(r\"there's\", \"there is\", message)\n    message = re.sub(r\"We're\", \"We are\", message)\n    message = re.sub(r\"That's\", \"That is\", message)\n    message = re.sub(r\"won't\", \"will not\", message)\n    message = re.sub(r\"they're\", \"they are\", message)\n    message = re.sub(r\"Can't\", \"Cannot\", message)\n    message = re.sub(r\"wasn't\", \"was not\", message)\n    message = re.sub(r\"aren't\", \"are not\", message)\n    message = re.sub(r\"isn't\", \"is not\", message)\n    message = re.sub(r\"What's\", \"What is\", message)\n    message = re.sub(r\"i'd\", \"I would\", message)\n    message = re.sub(r\"should've\", \"should have\", message)\n    message = re.sub(r\"where's\", \"where is\", message)\n    message = re.sub(r\"we'd\", \"we would\", message)\n    message = re.sub(r\"i'll\", \"I will\", message)\n    message = re.sub(r\"weren't\", \"were not\", message)\n    message = re.sub(r\"They're\", \"They are\", message)\n    message = re.sub(r\"let's\", \"let us\", message)\n    message = re.sub(r\"it's\", \"it is\", message)\n    message = re.sub(r\"can't\", \"cannot\", message)\n    message = re.sub(r\"don't\", \"do not\", message)\n    message = re.sub(r\"you're\", \"you are\", message)\n    message = re.sub(r\"i've\", \"I have\", message)\n    message = re.sub(r\"that's\", \"that is\", message)\n    message = re.sub(r\"i'll\", \"I will\", message)\n    message = re.sub(r\"doesn't\", \"does not\", message)\n    message = re.sub(r\"i'd\", \"I would\", message)\n    message = re.sub(r\"didn't\", \"did not\", message)\n    message = re.sub(r\"ain't\", \"am not\", message)\n    message = re.sub(r\"you'll\", \"you will\", message)\n    message = re.sub(r\"I've\", \"I have\", message)\n    message = re.sub(r\"Don't\", \"do not\", message)\n    message = re.sub(r\"I'll\", \"I will\", message)\n    message = re.sub(r\"I'd\", \"I would\", message)\n    message = re.sub(r\"Let's\", \"Let us\", message)\n    message = re.sub(r\"you'd\", \"You would\", message)\n    message = re.sub(r\"It's\", \"It is\", message)\n    message = re.sub(r\"Ain't\", \"am not\", message)\n    message = re.sub(r\"Haven't\", \"Have not\", message)\n    message = re.sub(r\"Could've\", \"Could have\", message)\n    message = re.sub(r\"youve\", \"you have\", message)\n    message = re.sub(r\"haven't\", \"have not\", message)\n    message = re.sub(r\"hasn't\", \"has not\", message)\n    message = re.sub(r\"There's\", \"There is\", message)\n    message = re.sub(r\"He's\", \"He is\", message)\n    message = re.sub(r\"It's\", \"It is\", message)\n    message = re.sub(r\"You're\", \"You are\", message)\n    message = re.sub(r\"I'M\", \"I am\", message)\n    message = re.sub(r\"shouldn't\", \"should not\", message)\n    message = re.sub(r\"wouldn't\", \"would not\", message)\n    message = re.sub(r\"i'm\", \"I am\", message)\n    message = re.sub(r\"I'm\", \"I am\", message)\n    message = re.sub(r\"Isn't\", \"is not\", message)\n    message = re.sub(r\"Here's\", \"Here is\", message)\n    message = re.sub(r\"you've\", \"you have\", message)\n    message = re.sub(r\"we're\", \"we are\", message)\n    message = re.sub(r\"what's\", \"what is\", message)\n    message = re.sub(r\"couldn't\", \"could not\", message)\n    message = re.sub(r\"we've\", \"we have\", message)\n    message = re.sub(r\"who's\", \"who is\", message)\n    message = re.sub(r\"y'all\", \"you all\", message)\n    message = re.sub(r\"would've\", \"would have\", message)\n    message = re.sub(r\"it'll\", \"it will\", message)\n    message = re.sub(r\"we'll\", \"we will\", message)\n    message = re.sub(r\"We've\", \"We have\", message)\n    message = re.sub(r\"he'll\", \"he will\", message)\n    message = re.sub(r\"Y'all\", \"You all\", message)\n    message = re.sub(r\"Weren't\", \"Were not\", message)\n    message = re.sub(r\"Didn't\", \"Did not\", message)\n    message = re.sub(r\"they'll\", \"they will\", message)\n    message = re.sub(r\"they'd\", \"they would\", message)\n    message = re.sub(r\"DON'T\", \"DO NOT\", message)\n    message = re.sub(r\"they've\", \"they have\", message)\n    \n    #correct some acronyms while we are at it\n    message = re.sub(r\"nba\", \"National Basketball Association\", message)\n    message = re.sub(r\"azwx\", \"Arizona Weather\", message)  \n    message = re.sub(r\"alwx\", \"Alabama Weather\", message)\n    message = re.sub(r\"wordpressdotcom\", \"wordpress\", message)      \n    message = re.sub(r\"gawx\", \"Georgia Weather\", message)  \n    message = re.sub(r\"scwx\", \"South Carolina Weather\", message)  \n    message = re.sub(r\"cawx\", \"California Weather\", message)\n    message = re.sub(r\"usNWSgov\", \"United States National Weather Service\", message) \n    message = re.sub(r\"epa\", \"Environmental Protection Agency\", message)\n    message = re.sub(r\"okwx\", \"Oklahoma City Weather\", message)\n    message = re.sub(r\"rt\", \"retweet\", message)\n    message = re.sub(r\"ny\", \"New York\", message)\n    message = re.sub(r\"arwx\", \"Arkansas Weather\", message)  \n  \n    \n    return message\n\ntrain_data['message'] = train_data['message'].apply(correct_contraction)","d4addd44":"# View the fist five changes to train data\ntrain_data['message'].head()","279aa902":" example = \"My Profile: https:\/\/auth.geeksforgeeks.org\\\n    \/ user \/ Chinmoy % 20Lenka \/ articles in\\\n    the portal of http:\/\/www.geeksforgeeks.org\/\"","0e6a8890":"def remove_URL(message):\n    '''\n    This funtion will remove all the URL's that exist in the texts\n\n    '''\n    \n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',message)\n\nremove_URL(example)","b9fa3948":"# Apply function to remove URL\ntrain_data['message']=train_data['message'].apply(lambda x : remove_URL(x))","13b0214c":"example = \"Hello \\n World!\"","f9aa8838":"def line_break(message):\n\n  '''\n  Function will remove all the line breaks that exist in the texts\n  \n  '''\n  \n  break_line = message.replace('\\n', ' ')\n  return break_line\n\nline_break(example)","051f46f1":"# Apply function to remove line breaks\ntrain_data['message']=train_data['message'].apply(lambda x : line_break(x))","0cfff7a0":"example = \"\"\"<div>\n<h1>Reel or Real<\/h1>\n<p>Hindustan <\/p>\n<a href=\"https:\/\/www.hindustan.com\/c\/nlp-open-the source\">get the source<\/a>\n<\/div>\"\"\"","310ebbf2":"# Function to remove HTML tags\ndef remove_html(message):\n    '''\n    Function will remove all the HTML tags that exist in the texts\n\n    '''\n\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',message)\nprint(remove_html(example))","38fb48ea":"# Function to remove HTML tags\ntrain_data['message']=train_data['message'].apply(lambda x : remove_html(x))","e1437871":"# Function to remove emojis in Unicode\ndef remove_emoji(message):\n    '''\n    Function will remove all the emojis in unicodes that exist in the texts\n\n    '''\n \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', message)\n\nremove_emoji(\"RT @GlblCtzn: 'I don't wanna live forever \u2013 and nothing will because climate change' \ufffd\ufffd\ufffd\ufffd\ufe0f\ufffd\ufffd @taylor...\")","b4d62e49":"# Apply function to remove emojis in Unicode\ntrain_data['message']=train_data['message'].apply(lambda x: remove_emoji(x))","296398c2":"# Function to remove punctuation\ndef remove_punct(text):\n    '''\n    Docstring here.\n\n    '''\n\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #GREAT Man.,\"\nprint(remove_punct(example))","1e638994":"# Apply function to remove punctuation\ntrain_data['message']=train_data['message'].apply(lambda x: remove_punct(x).lower())","d55f32f1":"# Apply function to remove numbers from text\ntrain_data['message']=train_data['message'].apply(lambda x: re.sub('\\w*\\d\\w*', ' ', x))","ef617009":"# Apply function to remove extra spaces\ntrain_data['message']=train_data['message'].apply(lambda x: re.sub(' +',' ',x))","81da2303":"# Apply function to remove undetected punctuations and unusual alphabets\ntrain_data['message']=train_data['message'].apply(lambda x: re.sub('[\u201c\u201d\u2018\u2019\u2014\u2026\u00e3\u00a2\u00e2\u201a\u201e\u00ac\u00e2\u00a6\u20ac\u009d]',' ',x))   #Vicky added this, removed all undetected punctuations","ff2ac3a3":"# We tried running this spell check function but it was quite memory intensive and did not run until complete\n#!pip install pyspellchecker\n#from spellchecker import SpellChecker\n#spell = SpellChecker()\ndef correct_spellings(text):\n    '''\n    Function attempts to do spell checks but this proved to be a resource intensive task that did not run\n\n\n    Example: \n    \n    Input:\n\n\n    Output:\n\n\n    '''\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \n#text = \"corect me plese\"\n#correct_spellings(text)","3d9a4dc5":"# View of our cleaned data\ntrain_data.head()","3c7f7d92":"def getAnalysis(sentiment):\n    '''\n    Function to compute the news, pro, neutral,anti analysis\n\n    Example: \n    \n    Input:\n\n\n    Output:\n\n\n    '''\n    if sentiment == -1:\n        return 'Anti'\n    elif sentiment ==0:\n        return 'Neutral'\n    elif sentiment == 1:\n        return 'Pro'\n    else:\n        return 'News'\n\ntrain_data['Analysis']= train_data['sentiment'].apply(getAnalysis)\ntrain_data.head()","a93de534":"# All of the News Analysis Tweets\nnews=1\nsorted_news= train_data.sort_values(by=['sentiment']) \nfor i in range(0, sorted_news.shape[0]): \n    if sorted_news['Analysis'][i] == 'News':\n       print(str(news) + '.' + sorted_news['message'][i])\n       print()\n       news=news +1","a7c16003":"# All of the Pro Analysis Tweets\npro=1\nsorted_pro= train_data.sort_values(by=['sentiment']) \nfor i in range(0, sorted_pro.shape[0]): \n    if sorted_pro['Analysis'][i] == 'Pro':\n       print(str(pro) + '.' + sorted_pro['message'][i])\n       print()\n       pro=pro +1\n      ","fe4fd124":"# All of the Anti Analysis Tweets\nanti=1\nsorted_anti= train_data.sort_values(by=['sentiment']) \nfor i in range(0, sorted_anti.shape[0]): \n    if sorted_anti['Analysis'][i] == 'Anti':\n       print(str(anti) + '.' + sorted_anti['message'][i])\n       print()\n       anti=anti +1","d0288973":"# All of the Neutral Analysis Tweets\nneu=1\nsorted_neu= train_data.sort_values(by=['sentiment']) \nfor i in range(0, sorted_neu.shape[0]): \n    if sorted_neu['Analysis'][i] == 'Neutral':\n       print(str(neu) + '.' + sorted_neu['message'][i])\n       print()\n       neu=neu +1","6bc6c835":"!pip install textblob\nfrom textblob import TextBlob","1a10f7e7":"def GetPolarity(text):\n    '''\n    Function aims ro add polarity in the Train data\n\n    Example: \n    \n    Input:\n\n\n    Output:\n\n    '''\n    return TextBlob(text).sentiment.polarity \n\ntrain_data['Polarity']= train_data['message'].apply(GetPolarity)\n\ntrain_data.head()","af5aeacb":"def GetSubjectivity(text):\n    '''\n    Funtion aims to add subjectivity on the Train data\n\n    Example: \n    \n    Input:\n\n\n    Output:\n\n\n    '''\n    return TextBlob(text).sentiment.subjectivity \n\ntrain_data['Subjectivity']= train_data['message'].apply(GetSubjectivity)\n\ntrain_data.head()","570758f4":"#Plot the Subjectivity vs the Polarity\nplt.figure(figsize=(8,6))\n#for i in range(0, train_data.shape[0]):\nplt.scatter(train_data['Polarity'][:1000], train_data['Subjectivity'][:1000], color = 'purple')\n\nplt.title('Sentiment Analysis')\nplt.xlabel('Polarity')\nplt.ylabel('Subjectivity')\nplt.show()\n\n","4e5c3821":"Pro_tweets = \" \".join(train_data[train_data['Analysis']=='Pro'].message)\nNews_tweets = \" \".join(train_data[train_data['Analysis']=='News'].message)\nAnti_tweets = \" \".join(train_data[train_data['Analysis']=='Anti'].message)\nNeu_tweets = \" \".join(train_data[train_data['Analysis']=='Neutral'].message)","d6dd90e9":"# Pro\nwordcloud = WordCloud(height=1000, width=1500, random_state= 21, max_font_size= 200, background_color='black')\nwordcloud = wordcloud.generate(Pro_tweets)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Pro Tweets Analysis')\nplt.show()","e0b013ea":"# News\nwordcloud = WordCloud(height=1000, width=1500, random_state= 21, max_font_size= 200, background_color='black')\nwordcloud = wordcloud.generate(News_tweets)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('News Tweets Analysis')\nplt.show()","2e6c2860":"#Anti\nwordcloud = WordCloud(height=1000, width=1500, random_state= 21, max_font_size= 200, background_color='black')\nwordcloud = wordcloud.generate(Anti_tweets)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Anti Tweets Analysis')\nplt.show()","b97e41a5":"#Neutral\nwordcloud = WordCloud(height=1000, width=1500, random_state= 21, max_font_size= 200, background_color='black')\nwordcloud = wordcloud.generate(Neu_tweets)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Neutral Tweets Analysis')\nplt.show()\n","4fefb39c":"#Making use of word_tokenizer to tokenize the clean texts\nfrom nltk.tokenize import word_tokenize\n\ntrain_data['Tokens']= train_data['message'].apply(lambda word: word_tokenize(word))\ntrain_data.head()","3c6c812d":"def bag_of_words_count(words, word_dict={}):\n    '''\n    Function takes in a list of words and returns a dictionary \n    with each word as a key, and the value represents the number of \n    times that word appeared\n    \n    '''\n    \n    for word in words:\n        if word in word_dict.keys():\n            word_dict[word] += 1\n        else:\n            word_dict[word] = 1\n    return word_dict\n\n","0be7dd22":"bag_words = {}\nfor pp in train_data['sentiment']:\n    df = train_data.groupby('sentiment')\n    bag_words[pp] = {}\n    for row in train_data['Tokens'][:1000]:\n        bag_words[pp] = bag_of_words_count(row, bag_words[pp]) \n\nbag_words     ","5aa9da5a":"import collections\ndef count_words(input):\n    '''\n    Function returns the word count that exists only in Pro sentiment\n    \n    '''\n    cnt = collections.Counter()\n    for row in input:\n        for word in row:\n            cnt[word] += 1\n    return cnt","7b178687":"train_data[(train_data.Analysis == 'Pro')][['Tokens']].apply(count_words)['Tokens'].most_common(20)","72e794d0":"train_data[(train_data.Analysis == 'Anti')][['Tokens']].apply(count_words)['Tokens'].most_common(20)","e7b7bb82":"# import packages   \n\n# from comet_ml import Experiment\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# set plot style\nsns.set()\nimport re\nimport string\nimport emoji\nimport spacy\nimport sklearn.feature_extraction.text\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\nfrom wordcloud import WordCloud\nfrom textwrap import wrap\n\n#from googletrans import Translator as translator\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn import metrics\nfrom nltk.corpus import stopwords\n","fae4c17f":"# load train data\ntrain_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/Vicky-hub87\/Team_ss1_Jhb-Classification-Predict\/master\/train.csv')","7f1629bf":"# load test data\ntest_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/Vicky-hub87\/Team_ss1_Jhb-Classification-Predict\/master\/test.csv')","36618958":"# load sample data\nsample_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/Vicky-hub87\/Team_ss1_Jhb-Classification-Predict\/master\/sample_submission.csv')\n","c359df04":"#combine train \ntotal = train_data.append(test_data)\n\n#change target to target1 in preparation for the word 'target' to be encoded as its own column\ntotal = total.rename(columns = {'sentiment':'sentiment1'})\n\n#create column for the number of words in tweet\ntotal['word count'] = total['message'].apply(lambda x: len(x.split()))\n\ntotal['character count'] = total['message'].apply(lambda x: len(x))\n\n#split so we can use updated train set with new feature\ntrain_data = total[:len(train_data)]\n\n  \n#add unique word count\ntotal['unique word count'] = total['message'].apply(lambda x: len(set(x.split())))\n\n\n#add url count\ntotal['url count'] = total['message'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))\n\n#add mention count\ntotal['mention count'] = total['message'].apply(lambda x: len([i for i in str(x) if i == '@']))\n\n#add hashtag count\n#total['hashtag count'] = total['text'].apply(lambda x: len([i for i in str(x) if i == '#']))\n\n#add punctuation count\ntotal['punctuation count'] = total['message'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))\n\n#split so we can use updated train set\ntrain_data = total[:len(train_data)]\n\ndisaster = train_data['sentiment1'] == 1","1dd65cc6":"#remove punctuation\ndef remove_punctuation(message):\n    return message.translate(str.maketrans('', '', string.punctuation))\n\n#remove stopwords\nStopWords = stopwords\ndef remove_stopwords(message):\n    return ' '.join([i for i in message.split() if i not in StopWords])\n\n#remove words less than 4 \ndef remove_less_than(message):\n    return ' '.join([i for i in message.split() if len(i) > 3])\n\n#remove words with non-alphabet characters\ndef remove_non_alphabet(message):\n    return ' '.join([i for i in message.split() if i.isalpha() == True])\n\n#stem words\nstemmer = SnowballStemmer('english')\ndef stem_words(message):\n    return stemmer.stem(message)\n\n#lemmatize words for verb\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words_verb(message):\n    return lemmatizer.lemmatize(message, 'v')","ed6c45ce":"from sklearn.model_selection import train_test_split\n\nX = train_data['message']  # this time we want to look at the text\ny = train_data['sentiment1']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","9bf9f127":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\nX_train_tfidf = vectorizer.fit_transform(X_train) # need to make use of the original X_train set\n\n\ncount_vectorizer=CountVectorizer(min_df=1, max_df=0.9, stop_words='english', decode_error='ignore')\ncount_vectorized=count_vectorizer.fit_transform(X_train)\n\nimport scipy.sparse\n\nX = scipy.sparse.hstack([X_train_tfidf, count_vectorized])","24d92245":"X_test_tfidf = vectorizer.transform(X_test)\ncount_vectorize_test=count_vectorizer.transform(X_test)\n\nX1 = scipy.sparse.hstack([X_test_tfidf, count_vectorize_test])\n","208e2112":"#model classifier\nfrom sklearn.svm import LinearSVC\nclf = LinearSVC()\nclf.fit(X, y_train)","9376c5f6":"from sklearn.pipeline import Pipeline\n\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),\n])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train) ","b7ecda50":"y_pred = text_clf.predict(X_test)","beaf428e":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test,y_pred))","0b7b235b":"print(metrics.classification_report(y_test,y_pred))","0b2e1913":"print(metrics.accuracy_score(y_test,y_pred))","d2df9f0a":"from sklearn.linear_model import LogisticRegression\nmodel= LogisticRegression()\n\nmodel.fit(X,y_train)","f9e74eab":"from sklearn.pipeline import Pipeline\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.svm import LinearSVC\n\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('model', LogisticRegression()),\n])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train) ","fe032ea5":"def get_top_tweet_unigrams(corpus, n=None):\n\n    '''\n    Gets top common single words\n\n\n    '''\n\n\n    vec = CountVectorizer(ngram_range=(6, 6)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","208170b5":"#Option 1: TFIDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf=TfidfVectorizer()\ntext_tf = tf.fit_transform(train_data['message'])","aa112dd7":"#Option 2: TFIDF\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    text_tf, train_data['sentiment1'], test_size=0.67, random_state=42)\nX_train.shape","69ed40a3":"#Option 2: TFIDF\nfrom sklearn.model_selection import train_test_split\n\nX = train_data['message']  # this time we want to look at the text\ny = train_data['sentiment1']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","95dc716b":"#Option 2: TFIDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\nX_train = vectorizer.fit_transform(X_train) # remember to use the original X_train set\nX_train.shape","df4b845a":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(C=1.0, solver='lbfgs', class_weight=None, multi_class='auto')\nlr_model.fit(X_train, y_train)\npred_lr = lr_model.predict(X_test_tfidf)\n","55af3965":"from sklearn.pipeline import Pipeline\n\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LogisticRegression()),\n])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_test, y_test) ","5fa0f0b2":"from sklearn.naive_bayes import MultinomialNB\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Generation Using Multinomial Naive Bayes\nclf = MultinomialNB().fit(X_train, y_train)\npred_nb = clf.predict(X_test_tfidf)","348d8df5":"# Form a prediction set\nfrom sklearn.svm import LinearSVC\nclf = LinearSVC()\nclf.fit(X_train, y_train)\npred_lsvc = clf.predict(X_test_tfidf)","819ef690":"from sklearn.metrics import confusion_matrix","045e16cb":"from sklearn.metrics import classification_report","d0cc768f":"print(\"SVC Accuracy:\",metrics.accuracy_score(y_test, pred_lsvc))","899030d3":"print(metrics.confusion_matrix(y_test,pred_lsvc))","cf2170fb":"print(metrics.classification_report(y_test,pred_lsvc))","b85da248":"print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, pred_nb))","cebded47":"print(metrics.confusion_matrix(y_test,pred_nb))","d8431b84":"print(metrics.classification_report(y_test,pred_nb))","fa1f1b10":"print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, pred_lr))","367ce31b":"print(metrics.confusion_matrix(y_test,pred_lr))","33343d69":"labels = ['0: Neutral', '1: Pro', '2:News', '-1:Anti']\n\npd.DataFrame(data=confusion_matrix(y_test, pred_lr), index=labels, columns=labels)","fe31705a":"# Saving each metric to add to a dictionary for logging to comet\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n\nprecision = precision_score(y_test, pred_lr, labels=None, average='macro')\nrecall = recall_score(y_test, pred_lr, labels=None, average='macro')\nf1 = f1_score(y_test, pred_lr, labels=None, average='macro')\n","b6dd36ee":"# Create dictionaries for the comet data we want to log\n\n#params = {\"random_state\": 7,\n          #\"model_type\": \"logreg\",\n          #\"scaler\": \"standard scaler\",\n          #\"param_grid\": str(param_grid),\n          #\"stratify\": True\n          #}\n\nmetrics = {\"f1\": f1,\n           \"recall\": recall,\n           \"precision\": precision\n           }","52cd9407":"# Log our comet parameters and results\n#experiment.log_parameters(params)\n#experiment.log_metric(\"accuracy\", f1)\nprint(\"Logistic regression f1 macro score metrics: \", metrics)","a988ce7f":"# We attempted to run comet but the desired results did not appear for ROC\/AUC curves\n#experiment.display()","c978770a":"# Preparing for kaggle submission file\ny_pred= pd.DataFrame(pred_lsvc).astype(int)\nbase_df = pd.DataFrame()\nbase_df['tweetid'] = test_data['tweetid']\nbase_df['sentiment'] = y_pred\nsample_data['sentiment'].value_counts()","e710fbba":"# For our kaggle submissions after running the above code\n#base_df.to_csv('\/kaggle\/working\/submissionv4.csv', index=False)","dad97010":"# import the relevant packages needed for our pipeline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier","3368056c":"# Load train data\ntrain_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/Vicky-hub87\/Team_ss1_Jhb-Classification-Predict\/master\/train.csv')","c0d6cb5c":"## Remove urls\nprint ('Removing URLs...')\npattern_url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\nsubs_url = r'url-web'\ntrain_data['message'] = train_data['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n\n# Make lower case\nprint ('Lowering case...')\ntrain_data['message'] = train_data['message'].str.lower()\n\n# Remove punctuation\nimport string\nprint ('Cleaning punctuation...')\ndef remove_punctuation_numbers(post):\n    punc_numbers = string.punctuation + '0123456789'\n    return ''.join([l for l in post if l not in punc_numbers])\ntrain_data['message'] = train_data['message'].apply(remove_punctuation_numbers)","18a9cdca":"# Preparing our data to be in a model ready state\nvect = CountVectorizer(stop_words='english', min_df= .01)\nX = vect.fit_transform(train_data['message'])\ny = train_data['sentiment']","4cee6e02":"# Selecting a subset of our data\nn = 5000\nX_train, X_test, y_train, y_test = train_test_split(X[:n].toarray(), y[:n])","6460369b":"# Define our new model names for our pipleline\nnames = ['Logistic Regression', 'Nearest Neighbors',\n         'Linear SVM', 'RBF SVM',\n         'Decision Tree', 'Random Forest',  'AdaBoost']","2bc5dd7a":"# Create a list for our new model names for our pipleline\n\nclassifiers = [\n    LogisticRegression(C=1.0, solver='lbfgs', class_weight=None, multi_class='auto'),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    AdaBoostClassifier()\n]","03f96533":"# Fit our models before exploring performance results to aid in selecting our model \n\nresults = []\n\nmodels = {}\nconfusion = {}\nclass_report = {}\n\n\nfor name, clf in zip(names, classifiers):\n    print ('Fitting {:s} model...'.format(name))\n    run_time = %timeit -q -o clf.fit(X_train, y_train)\n\n    print ('... predicting')\n    y_pred = clf.predict(X_train)\n    y_pred_test = clf.predict(X_test)\n\n    print ('... scoring')\n    accuracy  = metrics.accuracy_score(y_train, y_pred)\n    precision = metrics.precision_score(y_train, y_pred, labels=None, average='macro')\n    recall    = metrics.recall_score(y_train, y_pred, labels=None, average='macro')\n\n    f1        = metrics.f1_score(y_train, y_pred, labels=None, average='macro')\n    f1_test   = metrics.f1_score(y_test, y_pred_test, labels=None, average='macro')\n\n    # Save the results to dictionaries\n    models[name] = clf\n    confusion[name] = metrics.confusion_matrix(y_train, y_pred)\n    class_report[name] = metrics.classification_report(y_train, y_pred)\n\n    results.append([name, accuracy, precision, recall, f1, f1_test, run_time.best])\n\n\nresults = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Train', 'F1 Test', 'Train Time'])\nresults.set_index('Classifier', inplace= True)","7c7a89db":"# Display model performance resullts\nresults.sort_values('F1 Train', ascending=False)","f1ac6edd":"# Display model performance resullts with focus on f1 macro test results\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nresults.sort_values('F1 Train', ascending=False, inplace=True)\nresults.plot(y=['F1 Test'], kind='bar', ax=ax[0], xlim=[0,1.1], ylim=[0.00,0.80])\nresults.plot(y='Train Time', kind='bar', ax=ax[1])","2b9166c2":"confusion['Logistic Regression']","905ad280":"confusion['RBF SVM']","b6099dc4":"print(class_report['RBF SVM'])","a0101e8f":"# View performance results and their predictibility to perform on different test data by looking at the standard deviation\ncv = []\nfor name, model in models.items():\n    print ()\n    print(name)\n    scores = cross_val_score(model, X=X[:n].toarray(), y=y[:n], cv=10)\n    print(\"Accuracy: {:0.2f} (+\/- {:0.4f})\".format(scores.mean(), scores.std()))\n    cv.append([name, scores.mean(), scores.std() ])\n\ncv = pd.DataFrame(cv, columns=['Model', 'CV_Mean', 'CV_Std_Dev'])\ncv.set_index('Model', inplace=True)","9f9422d8":"# Display performance results and their predictibility to perform on different test data by looking at the standard deviation\ncv.plot(y='CV_Mean', yerr='CV_Std_Dev',kind='bar', ylim=[0.00, 0.70])","475a48d7":"The `.unique` method of is called on to confirm the unique values in the list","2e2e742d":"### Remove emojis","6513d77a":"### 7.2 An explanation of Pipelines","ba4db9d0":"### Subjectivity\n\nTells us how subjective or opinionated a text is.\n\nSentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information.","858c56e0":"**Summary of Findings**\n\nAfter having ivestigated and seen the findings. We can confidently confirm that there is indeed an imbalance of our class labels. \n\nWe can therefore deduce that Label 1 has the highest proportion of 0.58, which suggests that there are more people(58%) who tweet about supporting the belief of man-made climate change.\n\nLabel 2 comes second highest proportion of 0.23 which suggests that there are 23% of people who tweet links to factual news about climate change.\n\nLabel 0 comes third with a proprtion of 0.15, which suggests that there are 15% of people who tweet about neither nor refuting the belief of man-mad climate change.\n\nLastly, there is Label -1 with the lowest proportion of 0.08, which suggests that there are 8% of people who tweet about not believing in man-made climate change.","321f8528":"###### **Task: Check for missing values**","41e81f23":"### Model Evaluation\n\nModel Evaluation is one of the most fundamental parts of the model development process. It helps to find the best model that represents the data at hand and how well the chosen model will work in the future.\nTo avoid overfitting, both methods use a test set (not seen by the model) to evaluate model performance.","e1084a8d":"### Polarity\n\nTells us how positive or negative a text is.\n\nIt is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement and 0 is neutral.\n\nIt simply means emotions expressed in a sentence.\n\nEmotions are closely related to sentiments. The strength of a sentiment or opinion is typically linked to the intensity of certain emotions, e.g., joy and anger.","0a666a77":"###### **Task: Display the shape of Train, Test data and Sample data**\nThe `.shape` method is called to confirm the number of Rows and Columns in the DataFrame of the Train data, Test data and Sample data respectively. \n","67c36738":"### 2.2 Import Data","e1ea281f":"###### **Task: Test the Accuracy of the model**\n","d7684a62":"# 3. Initial Data Exporation Analysis\n\n\n\n\n\n\n","bdda3d5f":"### Remove HTML","930231d1":"**Summary of findings**\n\n**Exploratory Data Analysis (EDA)** refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations\"). Though we have not delved very deeply into EDA, we have mangaged to capture a cursory view into our data as a precursor to the EDA that will follow on our train data.\n\nAfter performing this initial data exploration on the given data the following observations were made:\n * 1 (**Pro**) = 8530 (**53.9%**)\n * 2 (**News**) = 3640 (**23%**)\n * 0 (**Neutral**) = 2353 (**14.9%**)\n * -1 (**Anti**) = 1296 (**8.2%**)\n\nThis suggests that a majority (over 50%) of the tweets support the belief in man-made climate change, while 20% were Neutral about this topic.\n We can assume(hypothesize) that a positive change can be made by driving the right campaigns and marketing messaging on different marketing channels that would be applicable for a Non-Profit Organisations (NPO) specialising in climate change as there are more Pro sentiments than Anti sentiments that man-made climate change exists. \n\nA business looking to launch a new product\\service where environmentally friendly products are valued by their clients would also benefit from this information as an input into their market research. Sentiment analysis from text classification may prove to be quicker and\/or more cost effective than the traditional approach of doing surveys or paying for survey data to inform an organisation's qualitative market research efforts. ","45478bba":"## 1.3 Data Description\n\n### 1.3.1 Data\n\nThe collection of this data which we use as our data source was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo, which was provided by Kaggle. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes in the table below:\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F2205222%2F8e4d65f2029797e0462b52022451829c%2Fdata.PNG?generation=1590752860255531&amp;alt=media\" alt=\"\" title=\"\">\n\n### 1.3.2 Variable definitions\n\n*   sentiment: Sentiment of tweet\n\n*   message: Tweet body\n\n*   tweetid: Twitter unique id\n\n\n### 1.3.3 The data input files we have used for our model are:\n\n*   Train.csv (is the dataset that we will use to train to our model) as denoted by the \"train_data\" dataframe variable in our code.\n*   Test.csv (is the dataset on which we will apply to our model to) as denoted by the \"test_data\" dataframe variable in our code.\n*   SampleSubmission.csv (is an example of what our submission file will look like. The order of the rows is not so relevant, but the names of the tweetid's must be correct.)","512a2ad1":"###### **task: Check for possible duplicate tweets\/retweets**","71b8f460":"### Remove URL","ebe42633":"###### **Task: Review sample of uncleaned tweets**","2895b043":"We have chosen the Multi logistic regression model as our final model followed by the SVM model due to it performance based on the above results.","40c2d168":"### 7.3 Build a pipeline to vectorize the data, then train and fit a model","56a57ef1":"The `.value_counts` methd is called to confirm the object containing counts of unique values. The resulting object will be in descending order so that the first element is the most frequently-occurring element.","3af74244":"### Remove line breaks","fbd03d99":"# 8. Feature Selection and Model Selection","6d13876c":"A machine learning pipeline is used to help automate machine learning workflows.  They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative.\nMachine learning pipelines are cyclical and iterative as every step is repeated to continuously improve the accuracy of the model and achieve a successful algorithm.  To build better machine learning models, and get the most value from them, accessible, scalable and durable storage solutions are imperative, paving the way for on-premises object storage.\n(Zhou, L., How to Build a Better Machine Learning Pipeline, 2018. URL https:\/\/www.datanami.com\/2018\/09\/05\/how-to-build-a-better-machine-learning-pipeline.)","65db1292":"### Remove Puncuations and Capital Letters","8d97c93a":"# 1. Introduction","fd9d52a0":"###### **Task: Document Term Matrix**","530e4ef4":"###### **Task: Train a Support Vector Machine (SVM) Classifier**","4d0f05fb":"### 6.1 Initial model evaluation","b26b10ec":"###### **Task: Check for whitespace strings**","0b13100f":"### Check the spelling","9dc9f809":"###### **Task: Check that the sum of all class label proportions are equal to 1**","690fe757":"**Missing values**\n\nMissing values are a common attribute in datasets and for a number of different reasons. In this part of the notebook we will do a minimal assessment of missing values. It is important however, that we understand missingness from both the perspective of the Train dayta and Test data. As such, we want to see which columns have missing data in both the Train and Test datasets, as well as the proportion of missingness in each of those columns.","002aad2a":"### 5.1 Further Pre-Processing\n* ***Data Cleaning***\n* ***Using NLP***\n\nNow that the data has been explored, it will now be prepared for machine learning. In general, to process text the following procedure needs to be explained:\n\nraw text corpus -> processing text -> tokenized text -> corpus vocabulary -> text representation\n\nMost of the hard work can be done with Keras's Tokenize object, which automatically converts all words to lowercase and filters out punctuation\n\nThis tokenizer has many arguements that allow you to do most of the cleaning with one line of code, so we do not need to much processing ourselves. Examples have been included of how one would manually clean text for reference:","f4b9999b":"The `.isnull` method confirms the number of nulls that can be found in the Train dataset as the `.sum` method confirms the sum of those missing values in that Train dataset.","cc6e460c":"## Data Cleaning","0ef4fc06":"# 5. Feature Engineering With EDA Of Cleaned Data","94b6e3a4":"\n\n## Data Exploration\n\nInitial data exploration suggests that there are more people in a given data set that believe in man-made climate change. Our hypothesis was that any organisation that has products\/services that are related to climate change could benefit from the insights gathered for different classes of of people who either do not believe in climate change, are unsure, thos e who believe in it, and those who possess facts about it. A marketing strategy to believers and those with facts about climate change could change how that organisations' brand is viewed. If the organisation supports climate change initiative it could be viewed positively by it stakeholders and clients. We also identified that duplicate tweets are typically retweets so if the retweets were mainly from believers this would support our hypothesis if the majority of thos duplicates is from a particular class. \n\n## Exploratory data analysis\n\nWe looked into the character count, word counts (absolute and average), hashtag counts etc. to view all characteristics of our target market when they interact with twitter to air share their opinions on climate change. Based on our Word Cloud visuals, and sentiment score those who are Pro climate change and Anti climate change tend to retweet and one could then argue that these people hold strong views regarding climate change.\n\n## Predictive modelling\n\nOur top performing model in all attempts is Logistic Regression. Apart from Nearest Neighbor, Logistic Regression also run efficiently in terms of compute resources. \n\n\n## Feature selection\n\n\nIf we had more time we would have delved in to feature selection by using further NLP techniques of hyperparameter tuning. We attempted to run our models using our newly cretaed features but this did not improve the score. We do beleive that if we used an ensemble model our modle would have performed better. Again this is something we would have loved to explore if we had the time to.\n\n\n## Recommendations\n\nFrom our insights we recommend that the model that is used be used for the purposes of getting moe insights from the extreme classes such who are either deniers or believers in mana-made climate change with more emphasis on the Pro class as these are the people that brand messaging may be most effective when using social media targeting. We recommend that more hyperperameter tuning be done to the logistic regression model so that classification predictions are further enhanced for the organisaton who would see value in using a sentiment classifier as amarket research tool as it is quicker to get market insights using this approach versus say running a survey. With big data analytics, the possibilities to get these insights are opens more doors to quality insights.\n\n\n## Key takeaways\n\nText cleaning, NLP techniques, and EDA, feature enginneering and hyperparameter tuning are important in getting a grasp of text classification. In practice there is always a quicker way to get to a working model. We explored both approaches to a degree and found the process quite insightful.\n\n\n\n## Referencing\n\n1. https:\/\/athena.explore-datascience.net\/student\/content\/train-view\/38\/100\/1783\n\n2. Sperandei, S., 2014. Understanding logistic regression analysis. Biochemia medica: Biochemia medica, 24(1), pp.12-18.\n\n3. Zhou, L., How to Build a Better Machine Learning Pipeline, 2018. URL https:\/\/www.datanami.com\/2018\/09\/05\/how-to-build-a-better-machine-learning-pipeline.","8056e1a1":"###### **Task: Generate New Features and Visualise**","1196f6bb":"Again, the `.unique` method confirms only the unique messages in the list\/data.","420da760":"# 9. Summary of Conclusions","79b2a4fa":"###3.1 Text Preprocessing","a5a0af0b":"![image.png](data:image\/png;base64,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)","be9daa9b":"### Create Preprocessing functions","f19b3a11":"### Remove URL extra spaces as a result of cleaning text","3caf9ed4":"Cleaning text can be cumbersome but there are quicker ways to clean your text. the above process is to illustrate what needs to be considers but as we continue with our notebook we explore simpler appraches.","6240fe01":"### 2.1 Install relevant Libraries\n\nWe start off by importing relevant packages and loading the data.\n\n`Comet:`For viewing different versions of your machinle learning model results - `pip install comet_ml--3.1.11`\n\n`Emoji:` For removing emojis - `pip install emoji--0.5.4`\n\n`Wordcloud:`For creating a Word Cloud infographic to visualise frequent words used in text - `pip install wordcloud --1.7.0`\n\n`Textblob:`For simple, Pythonic text processing. Sentiment analysis, part-of-speech tagging, noun phrase parsing, and more - `pip install textblob--0.15.3` \n\n`Natural Language Toolkit (NLTK):`For common natural language processing (NLP) tasks and more - `pip install nltk--3.4.5`\n\n`Spacy:`For industrial strength Natural Language Processing (NLP) tasks and more - `pip install spacy--2.2.4`\n\n`Tensorflow:`For high performance machine learning numerical computation - `pip install tensorflow--2.2.0`","5bc2a0e7":"<img data-attachment-id=\"1700\" data-permalink=\"https:\/\/tomraftery.com\/2011\/06\/01\/my-twitter-magic-number-is-16-whats-yours\/screen-shot-2011-06-01-at-20-25-36\/\" data-orig-file=\"https:\/\/i2.wp.com\/tomraftery.com\/wp-content\/uploads\/2011\/06\/screen-shot-2011-06-01-at-20-25-36.png?fit=563%2C271&amp;ssl=1\" data-orig-size=\"563,271\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"Twitter post\" data-image-description=\"<p>Twitter post staying under my Twitter magic number<\/p>\n\" data-medium-file=\"https:\/\/i2.wp.com\/tomraftery.com\/wp-content\/uploads\/2011\/06\/screen-shot-2011-06-01-at-20-25-36.png?fit=300%2C144&amp;ssl=1\" data-large-file=\"https:\/\/i2.wp.com\/tomraftery.com\/wp-content\/uploads\/2011\/06\/screen-shot-2011-06-01-at-20-25-36.png?fit=563%2C271&amp;ssl=1\" class=\"size-full wp-image-1700 jetpack-lazy-image jetpack-lazy-image--handled\" title=\"Twitter post\" src=\"https:\/\/i0.wp.com\/www.enterpriseirregulars.com\/wp-content\/uploads\/2011\/06\/a06675977df79f3e834fdf758225008f1.png?resize=563%2C271&amp;ssl=1\" alt=\"Twitter post\" width=\"563\" height=\"271\" data-recalc-dims=\"1\" data-lazy-loaded=\"1\">\n\n<p>Twitter is a superb medium for getting a message out.<\/p>\n\n<p>And it\u2019s RT (Retweet) convention means that tweets can go viral very quickly. So this may give us an indication of virality and sentiment for certain classes of tweets as there are a number of RT's visible in our data. But how many are there? and for which sentiment classes?<\/p>","ad148bfd":"Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.","d94b4757":"###### Task: Select the best model\n","254fc132":"###### **Task: Check the proportionality of the class labels to see if data is indeed imbalanced.**\n\nCheck class label proportion for:\n\n* **Anti:** the tweet does not believe in man-made climate change (**class =  -1**)\n\n* **Neutral:** the tweet neither supports nor refutes the belief of man-mad climate change (**class =  0**)\n\n* **Pro:** the tweet supports the belief of man-made climate change (**class = 1**)\n\n* **News:** the tweet links to factual news about climate change (**class = 2**)","b6c7d560":"### WordCloud\n\nIt is also known as a Text Cloud and whereby it is also a visualization where more specific word appear.\n\nIt is a technique to show which words are the most frequent among the given text. ","56d192c9":"###### **Task: Display experiment on comet**","bf803063":"###### **Task: View sample of Train and Test data**\nSimply displaying the train and test data to have a look at the actual data contained within the first 10 columns.\n\nThe sentiment column contains differnet classes ranging between 2 and -1 of how people view Climate Change. \n2 - New: Tweets about Factual News about climate change\n1 - Pro: Tweets that support the belief of man-made climate change\n0 - Neutral: Tweets that neither supports nor refuses the belief of of man-made climate change\n1 - Anti- Tweets that do not belief in man-made climate change\n\nThe message column contains Tweets from different people expressing their views on climate change.\n\nThe Tweet id column contains different  peoples Twitter accounts tweeting about climate change.","048d9a56":"### Testing Base Model ","5bfdc999":"### Bag of words\n\n'BOW' is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set.","8170a250":"**Summary Feedback**\n\nOn the WordCloud, words like Climate Change, Global Warming, Retweet, Protection Agency are the most frequent in the text. Bag of words shows the most frequent words and it corresponds to the wordcloud\n\nIt is evident that if polarity is for example 0.8, which means that the statement is positive and 0.8 subjectivity, refers that mostly it is a public opinion and not a factual information.\n\n xxxxcan we analysis the scatter plot ","c636daf8":"### Clustering of Messages into Pro, News, Neutral and Anti Tweets\n\nThe different Sentiment tweets are clustered based on the sentiments description to see what is common about the tweets. ","1c815b66":"### HyperParameter Tuning\n\nHyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process.\n\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data.\n\nIf given more time we would have used this technique as well to improve our. in our case we used cross validation and stacking or models to optimize the performance of our model.","0d439d16":"## 4.1 Insights ","c27d3b99":"Just a brief definition of what Feature engineering is so that there is an understanding on what this section focuses on:\n\nFeature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.","d6c93d46":"### 5.2 Exploratory Data Analysis on Clean Data","f6ef85b4":"### Remove numbers","a6545271":"## 1.2 Problem Statement\n\nWith this context, we are being challenged with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n\nProviding an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n\nOur quest is to provide valuable sentiment analysis information with actionable insights that can be used for an entity that  could like its brand to be viewed as \"eco-friendly\" from the perspective of all its stakeeholders e.g. clients, employeers, investors etc.\n","0f65a2ad":"**Improving model perfomance:**\n\nGiven more time we would have used an ensemble stacked model approach to improve our score alongside hyperparamenter tuning.","53422da4":"After having explored the data, the findings deduce that 12% of the train data contains duplicates and 11% of our test data contains duplicates. \nThe proportion of duplicate messages across both data sets is similar. Additionally, a retweet seem to almost always be associated with an '@' mention in a message.\n\n For more on twitter terminology, please visit the Twitter help page [here](https:\/\/help.twitter.com\/en\/using-twitter\/types-of-tweets).\nThis might be useful information when deciding which feeatures to generate and select for our final model.","fd5fea41":"**Task: Visualize current state of data**","0a045418":"###### **Task: Train a Logistic Regression Classifier**\n","feecb667":"The `.isspace` function confirms the existance of whitespace which  will give the characters space, tab, linefeed, return, formfeed, and vertical tab. ","2b8748c0":"###### **Task: Investigate the Label column.**\nAfter loading and viewing the shape of the data, we now take a cursory view of the target variable that we ultimately need to predict. In our Initial Data Exploration, we will take a brief look at the proportionality of data to see whether it is balanced or not. In the real world, text data is rarely balanced so we expect to see imbalanced proportionality between the four class labels associated with the target variable that we aim to predict. ","7bf803f0":"### 7.4 Run predictions and analyze the results","f151e9fd":"###Summarise findings","c02aaea9":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Context\" data-toc-modified-id=\"Context-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Context<\/a><\/span><\/li><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Problem Statement<\/a><\/span><\/li><li><span><a href=\"#Data-sets\" data-toc-modified-id=\"Data-sets-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>Data sets<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;<\/span>Data<\/a><\/span><\/li><li><span><a href=\"#Variable-definitions\" data-toc-modified-id=\"Variable-definitions-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;<\/span>Variable definitions<\/a><\/span><\/li><li><span><a href=\"#The-data-input-files-we-have-used-for-our-model-are:\" data-toc-modified-id=\"The-data-input-files-we-have-used-for-our-model-are:-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;<\/span>The data input files we have used for our model are:<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Initial-Data-Exploration\" data-toc-modified--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Initial Data Exploration<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Import-packages-and-load-data-files\" data-toc-modified-id=\"Import-packages-and-load-data-files-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Import packages and load data files<\/a><\/span><\/li><li><span><a href=\"#Missing-Values\" data-toc-modified-id=\"Missing-Values-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Missing Values<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Exploratory Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Insights\" data-toc-modified-id=\"Insights-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Insights<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Feature Engineering<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Initial-model-evaluation\" data-toc-modified-id=\"Initial-model-evaluation-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Initial model evaluation<\/a><\/span><\/li><li><span><a href=\"#Further-Feature-Extraction\" data-toc-modified-id=\"Further-Feature-Extraction-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Further Feature Extraction<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Predictive-Modelling\" data-toc-modified-id=\"Predictive-Modelling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Predictive Modelling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#An-Overview-of-learners\" data-toc-modified-id=\"An-Overview-of-learners-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>An Overview of learners<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;<\/span>Logistic Regression<\/a><\/span><\/li><li><span><a href=\"#K-Nearest-Neighbours\" data-toc-modified-id=\"K-Nearest-Neighbours-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;<\/span>K-Nearest Neighbours<\/a><\/span><\/li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;<\/span>Support Vector Machines<\/a><\/span><\/li><li><span><a href=\"#Na\u00efve-Bayes\" data-toc-modified-id=\"Na\u00efve-Bayes-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;<\/span>Na\u00efve Bayes<\/a><\/span><\/li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-5.1.5\"><span class=\"toc-item-num\">5.1.5&nbsp;&nbsp;<\/span>Decision Tree<\/a><\/span><\/li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-5.1.6\"><span class=\"toc-item-num\">5.1.6&nbsp;&nbsp;<\/span>Random Forest<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#An-Overview-of-the-features\" data-toc-modified-id=\"An-Overview-of-the-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>An Overview of the features<\/a><\/span><\/li><li><span><a href=\"#An-explanation-of-Pipelines\" data-toc-modified-id=\"An-explanation-of-Pipelines-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>An explanation of Pipelines<\/a><\/span><\/li><li><span><a href=\"#Build-a-pipeline-to-vectorize-the-data,-then-train-and-fit-a-model\" data-toc-modified-id=\"Build-a-pipeline-to-vectorize-the-data,-then-train-and-fit-a-model-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Build a pipeline to vectorize the data, then train and fit a model<\/a><\/span><\/li><li><span><a href=\"#Run-predictions-and-analyze-the-results\" data-toc-modified-id=\"Run-predictions-and-analyze-the-results-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;<\/span>Run predictions and analyze the results<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-Selection-and-Model-Selection\" data-toc-modified-id=\"Feature-Selection-and-Model-Selection-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Feature Selection and Model Selection<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameter-tuning\" data-toc-modified-id=\"Hyperparameter-tuning-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Hyperparameter tuning<\/a><\/span><\/li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Model Evaluation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Summary-of-Conclusions\" data-toc-modified-id=\"Summary-of-Conclusions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Summary of Conclusions<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Data Exploration<\/a><\/span><\/li><li><span><a href=\"#Exploratory-data-analysis\" data-toc-modified-id=\"Exploratory-data-analysis-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Exploratory data analysis<\/a><\/span><\/li><li><span><a href=\"#Predictive-modelling\" data-toc-modified-id=\"Predictive-modelling-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;<\/span>Predictive modelling<\/a><\/span><\/li><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;<\/span>Feature selection<\/a><\/span><\/li><li><span><a href=\"#Recommendations\" data-toc-modified-id=\"Recommendations-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;<\/span>Recommendations<\/a><\/span><\/li><li><span><a href=\"#Key-takeaways\" data-toc-modified-id=\"Key-takeaways-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;<\/span>Key takeaways<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","38562e9c":"In the next several tasks, a first iteration of text cleaning taking a very manual text preprocessing approach to text cleaning which works but is intentionally imperfect in our first iteration will be explored.\n\nThe reason for this is to show that text cleaning in NLP can be an iterative process which is defined by the context of data you need for the particular machine learning problem you are solving for. Here, a step by step approach of some of the text cleaning tools available to us will be shown. \n\nFurther down in the notebook, a more simplified approach will be taken once we understand the steps involved and how each step modifies our data so that it is ready for some more data exploration and is useable for our prediction model for this particular problem which relates to multiclass classification within the range of those who believe and don't believe that climate change exists.","37e25e5c":"###### **Task: Train a Naive Bayes Regression classifier**","d9b2e006":"# 7. Predictive Modelling\n\nPredictive modeling is a process that uses data and statistics to predict outcomes with data models. \nThese models can be used to predict anything from sports outcomes and TV ratings to technological advances and corporate earnings. \nPredictive modeling is also often referred to as: Predictive analytics.\n","9abdc64f":"##### Conclude\nBy now, we should be so much more familiar with the data. Let's go a little further with Exploratory Data Analysis (EDA) withing our initial Feature Engineering\/Extraction steps that are to follow. Now that an idea of what new features to construct has been identfied, and how they might be useful, let's add the rest of them and visualize them.","885eaf90":"###### **Task: Save output of highest performing Model results**","6f82fa60":"# 6. Model Predictions","c9def78b":"## 1.1 Context\n\nMany companies are built around lessening one\u2019s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product\/service may be received.","1c127016":"###### **Task: Lemmatization**","3affa34d":"##","4e9671fa":"The above visuals show us the probability characteristics of each classes tweets which may be useful for feature engineering purposes that will go into our machine learning model.","ec9fce38":"# 4. Further Exploratory Data Analysis","748a3ebc":"### 7.1 An Overview of learners\n**List of models we will train are as follows:**\n\n\n### Logistic Regression\n\nLogistic regression is used to obtain odds ratio in the presence of more than one explanatory variable (it explains the relationship between one dependent binary variable and one or more independent variables). \n\nFuthermore, Logstic regression is used to describe data. The procedure is quite similar to multiple linear regression, with the exception that the response variable is binomial(has a dependent variable with two possible values labeled 0 and 1). The result is the impact of each variable on the odds ratio of the observed event of interest. \n\nThe main advantage is to avoid confounding effects by analyzing the association of all variables together. A disadvantage of this model may be overfitting whereby too many variables are added, which reduces the generalizability of the model beyond the data on which the model is fit. In this article, we explain the logistic regression procedure using examples to make it as simple as possible.\n(Sperandei, S., 2014. Understanding logistic regression analysis. Biochemia medica: Biochemia medica, 24(1), pp.12-18.)\n\n### K-Nearest Neighbours\n\nK-nearest neighbors (KNN) is a powerful, yet easy to understand machine learning algorithm. It relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data. In principle, this algorithm works by assigning the majority class of the N closest neighbors to the currect data point(it captures the idea of similarity).\n\nAs such, absolutely no training is required for the algorithm! All we do is choose K (i.e. the number of neighbors to consider), choose a distance function to calculate proximity and we're good to go. As we decrease the value of K to 1, our predictions become less stable. \n\nInversely, as we increase the value of K, our predictions become more stable making it more likely to make more accurate predictions to a certain point. where we will, eventually begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far. The algorithm gets significantly slower as the number of examples and\/or predictors\/independent variables increase.\n(https:\/\/athena.explore-datascience.net\/student\/content\/train-view\/38\/100\/1783)\n\n### Support Vector Machines \n\nSupport Vector Machines (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. It separates data points using a hyperplane with the largest amount of margin. That's why an SVM classifier is also known as a discriminative classifier.\n\nThe Support Vector Machine (SVM) algorithm is a popular machine learning tool that offers solutions for both classification and regression problems. Like logistic regression, they fit a linear decision boundary. \n\nHowever, unlike logistic regression, SVMs do this in a non-proabilistic way and are able to fit to non-linear data using an algorithm known as the kernel trick. In sklearn, these are called SVC (Support Vector Classifier) and SVR (Support Vector Regression) respectively. Classification of images can also be performed using SVM. (https:\/\/athena.explore-datascience.net\/student\/content\/train-view\/38\/100\/1783)\n\n### Na\u00efve Bayes\n\nNaive Bayes is a classification algorithm that uses the principle of Bayes theorem to make classifications. The benefits of Naive Bayes are that the model is simple to build and is useful on large data sets. Further, it only requires a small number of training data to estimate the parameters necessary for classification.\n\nHowever the model makes an explicit assumption that the features are independent given the class label making it almost impossible to get a set of predictors which are completely independent. .(https:\/\/athena.explore-datascience.net\/student\/content\/train-view\/38\/100\/1783)\n\n### Decision Tree\n\nThe Decision tree model uses a non parameterised approach to making predictions. Its basically a series of questions that the algorithm asks about a particular sample. For example, if the value is this then the target is that. The algorithm starts off by selecting a variable that produces the best split of the data. Every predictor is assigned an impurity scores based on how accurate its predictions are (measured by rmse). \n\nThe feature with the lowest impurity score is used for the initial split at the top of the tree, know as the root node. The other variables are used for splits down the tree all the way to the final predictions at the leaf nodes. Trees can easily overfit the training data because of their slightly more complex nature compared with linear models. \n\nTherefore it is common to build a tree to maximum depth and then prune it by getting rid of branches that do not necessarily result in significantly lower rmse in the next node. Finally, tree can easily decipher non linear patterns in dataset and thusare often useful for non linearly distributed datasets. \n\n### Random Forest\n\nRandom Forest is an extension of decision trees. Single decision tree often have high variance. Their predictions depend a lot on the data they were trained on. A slight change to the training set, their predictions change significantly. Because of this a popular technique is on of building multiple trees and averaging their predictions. This is known as bootstrap aggregation, meaning multiple models are trained on different subsets of the training data. The subsets are obtained through a bootstrap sampling approach where samples can be randomly selected more than once until the length of the bootstrap is equal to the length of the original training set. Note that some of the samples are left out (refered to as out of bag samples). The model is then trained on the bootstrap sample. Random forest takes sampling to yet another level by also using a subset of the predictors to build each tree. This results in not so highly correlated tree in the forest that if averaged can produce stable predictions with little variance. The user has the option to choose the number of tree of treat it as a hyper-paramer to be determined by the data.","c3e5e09a":"###### Task: Download and install external libraries\/packages\n","65605f70":"# 2. Download, Import Packages and Load Data files "}}