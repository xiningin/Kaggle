{"cell_type":{"599811f3":"code","964b988b":"code","05a4e4e6":"code","5bf122fe":"code","7ab1578c":"code","ee0c0c7e":"code","c9d77607":"code","14c5bec5":"code","83ed816e":"code","148b554f":"code","98df6d7e":"code","1247521b":"code","acaf1ef2":"code","34efc812":"code","6d627eae":"code","5810cd05":"code","5abd8279":"code","6389abd2":"markdown","5f034427":"markdown","bda49354":"markdown","05dcb010":"markdown","cf2cf63f":"markdown","be0f3dd4":"markdown","b37cf166":"markdown","c935093b":"markdown","830c7735":"markdown","cffe4ae2":"markdown","335712f0":"markdown","bfcb78be":"markdown","469ad4c1":"markdown","02c2a4c3":"markdown","c3b7f30d":"markdown","ef8dd034":"markdown","1f257da3":"markdown","be77370e":"markdown","b72ab814":"markdown","526cab1b":"markdown","7160c23c":"markdown","e64bbd2b":"markdown","dd46beae":"markdown","41f5fa65":"markdown","f12ea037":"markdown","c59ae9aa":"markdown","dd623d5a":"markdown","d8e56cc2":"markdown","cad64c2e":"markdown","81b16292":"markdown","24862970":"markdown","7e1d73e2":"markdown","1b5f199a":"markdown"},"source":{"599811f3":"# Simple Example of word tokenization\nfrom nltk.tokenize import word_tokenize\n\n#input a text\ntext = \"Hello everyone. Welcome to my Notebook\"\nword_tokenize(text)  #print tokenized words","964b988b":"# Example to tokenize multiple sentences first, and then the words contained in sentences of a text.","05a4e4e6":"#import sent_tokenize from nltk library\nfrom nltk import sent_tokenize \ntext = \"Hello everyone. Welcome to the NLP Class. Mr.Smith  and Johann S. Baech are waiting for you. They'll join you soon.\"\nfor t in sent_tokenize(text):\n    \n    x =word_tokenize(t)\n    print(x)","5bf122fe":"import nltk","7ab1578c":"# You can also import WhitespaceTokenizer() method from nltk\nfrom nltk.tokenize import WhitespaceTokenizer","ee0c0c7e":"# Create a reference variable for Class WhitespaceTokenizer\nwtk = WhitespaceTokenizer()\n\n#give string input\ntext1 = \"Natural language Processing is a subset of Deep learning.\"\n\n#use tokenize method\ntokens = wtk.tokenize(text1)\nprint(tokens)","c9d77607":"# create a list and dictionary variable\nlst = ['Tomato', 'Orange','Almond']\ndct = {'Vegetable': 'Tomato', 'Fruit': 'Orange','Dry-fruit':'Almond'}\n\n#extract words from dictonary items\nword2index = {key: val for val, key in dct.items()}\n\n#print tokenized words\ntokenized = [[word2index[word] for word in text.split()] for text in lst]\ntokenized","14c5bec5":"# import RegexpTokenizer\nfrom nltk.tokenize import RegexpTokenizer\n  \ntk = RegexpTokenizer(\"[\\w']+\")  #[\\w']+ is one type of regular expression which extracts whole words from text.\n\n#give an input string\ntext = \"Let's see how it's working.\"\ntokens = tk.tokenize(text)\ntokens","83ed816e":"#import wordpunct_tokenize from nltk\nfrom nltk.tokenize import wordpunct_tokenize\n\ntext = \"Mrs.Patel buys Fruits : Mango,Banana,Orange. \"\n\ntokens = wordpunct_tokenize(text)\ntokens","148b554f":"#import TweetTokenizer from nltk\nfrom nltk.tokenize import TweetTokenizer\n\n#create object of tokenizer\ntknzr = TweetTokenizer(strip_handles=True)\ntweet= \" @NLP_learner: NLP is way tooo coool:-) :-P <3\"\n\nx= tknzr.tokenize(tweet)\nprint(x)","98df6d7e":"from nltk.tokenize import MWETokenizer\n   \n# Create a reference variable for Class MWETokenizer\ntk = MWETokenizer([('M', 'W', 'E'), ('Multi', 'Word', 'Tokenier')])\ntk.add_mwe(('Natural', 'Language', 'Processing'))\n   \n# Create a string input\ntext = \"What is M W E in Natural Language Processing\"\n   \n# Use tokenize method\ntokenized = tk.tokenize(text.split())\n   \nprint(tokenized)","1247521b":"#import tokenizer from nltk\nfrom nltk.tokenize import TreebankWordTokenizer\n\n#create object of TreebankWordTokenizer\ntk = TreebankWordTokenizer()\ntext = \"That's True, Mrs.Samantha Jones.\"\ntokens = tk.tokenize(text)\ntokens","acaf1ef2":"from spacy.lang.en import English\n\n# Load English tokenizer, tagger, parser, NER and word vectors by creating object\nnlp = English()","34efc812":"# give an input string with multiple sentences\nstring = \"\"\"Founded in 2002, SpaceX\u2019s mission is to enable humans to become a spacefaring civilization and a multi-planet \nspecies by building a self-sustaining city on Mars. In 2008, SpaceX\u2019s Falcon 1 became the first privately developed \nliquid-fuel launch vehicle to orbit the Earth.\"\"\"","6d627eae":"text = nlp(string)\n\n# Create list of word tokens\ntoken_list = []\nfor token in text:\n    token_list.append(token.text)\ntoken_list","5810cd05":"#!pip install sacremoses\nfrom sacremoses import MosesTokenizer","5abd8279":"#create tokenizer object\nmt = MosesTokenizer()\ntext = \"okay,Let me check Moses Tokenizer, Mr.Kim\"\ntokens = mt.tokenize(text)\ntokens","6389abd2":"### MWE(Multi-Word Expression) Tokenizer\nThe multi-word expression tokenizer is a rule-based, \u201cadd-on\u201d tokenizer offered by NLTK. Once the text has been tokenized by a tokenizer of choice, some tokens can be re-grouped into multi-word expressions.\n- MWETokenizer takes a string and merges multi-word expressions into single tokens, using a lexicon of MWEs\n","5f034427":"## 5) Spacy Tokenizer\nThis is a modern technique of tokenization which faster and easily customizable. It provides the flexibility to specify special tokens that need not be segmented or need to be segmented using special rules. Suppose you want to keep $ as a separate token, it takes precedence over other tokenization operations.","bda49354":"#### Stop words : \nStop words are those words in the text which does not add any meaning to the sentence and their removal will not affect the processing of text for the defined purpose. They are removed from the vocabulary to reduce noise and to reduce the dimension of the feature set.","05dcb010":"## Consider **UPVOTING** if you find it useful\ud83d\ude03","cf2cf63f":"## 2) Dictionary Based Tokenization\nIn this method the tokens are found based on the tokens already existing in the dictionary. If the token is not found, then special rules are used to tokenize it. It is an advanced technique compared to whitespace tokenizer.","be0f3dd4":"WordPiece is similar to BPE techniques expect the way the new token is added to the vocabulary. BPE considers the token with most frequent occurring pair of symbols to merge into the vocabulary. While WordPiece considers the frequency of individual symbols also and based on below count it merges into the vocabulary.\n- Count (x, y) = frequency of (x, y) \/ frequency (x) * frequency (y)\n- The pair of symbols with maximum count will be considered to merge into vocabulary. So it allows rare tokens to be included into vocabulary as compared to BPE.","b37cf166":"### Punctuation-based tokenizer\nPunctuation-based tokenization splits on whitespace and punctuations and also retains the punctuations.Punctuation-based tokenization overcomes the issue above and provides a meaningful token.","c935093b":"## 7) Subword Tokenization\nThis tokenization is very useful for specific application where sub words make significance. In this technique the most frequently used words are given unique ids and less frequent words are split into sub words and they best represent the meaning independently. For example if the word few is appearing frequently in the text it will be assigned a unique id, where fewer and fewest which are rare words and are less frequent in the text will be split into sub words like few, er, and est. This helps the language model not to learn fewer and fewest as two separate words. This allows to identify the unknown words in the data set during training. ","830c7735":"#### Different libraries to perform word tokenization :\n - NLTK\n - Genism\n - Keras\n","cffe4ae2":"## 3) Rule Based Tokenization\nIn this technique a set of rules are created for the specific problem. The tokenization is done based on the rules. For example creating rules bases on grammar for particular language.","335712f0":"### Regular Expression Tokenizer\nThis technique uses regular expression to control the tokenization of text into tokens. Regular expression can be simple to complex and sometimes difficult to comprehend. This technique should be preferred when the above methods does not serve the required purpose. **It is a rule based tokenizer.**","bfcb78be":"# <center> Word Tokenization Techniques in NLP","469ad4c1":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing which treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\n- All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words.To solve this problem more generally Sentence Piece was intoduced.","02c2a4c3":"BPE uses Huffman encoding for tokenization meaning it uses more embedding or symbols for representing less frequent words and less symbols or embedding for more frequently used words.\nThe BPE tokenization is bottom up sub word tokenization technique. The steps involved in BPE algorithm is given below.\n1. Split the words in the corpus into characters after appending <\/w>\n2. Initialize the vocabulary with unique characters in the corpus\n3. Compute the frequency of a pair of characters or character sequences in corpus\n4. Merge the most frequent pair in corpus\n5. Save the best pair to the vocabulary\n6. Repeat steps 3 to 5 for a certain number of iterations","c3b7f30d":"Tokenization can be done to either separate words or sentences.\n- If the text is split into words using some separation technique it is called **word tokenization** and same separation done for sentences is called **sentence tokenization.**","ef8dd034":"#### iv) SentencePiece","1f257da3":"We will use only nltk library in this notebook.","be77370e":"## 1) Whitespace Tokenization\nThis is the simplest tokenization technique. Given a sentence or paragraph it tokenizes into words by splitting the input whenever a white space in encountered. This is the fastest tokenization technique but will work for languages in which the white space breaks apart the sentence into meaningful words.\n#### Syntax : tokenize.WhitespaceTokenizer()","b72ab814":"#### i) Byte-Pair Encoding(BPE)","526cab1b":"- There are numerous Regular Expressions from which we can choose according to our requirements.\n![](https:\/\/www.umbrella-host.co.uk\/wp\/media\/Regular-Expressions-Cheat-Sheet-800x364.png)","7160c23c":"#### ii) WordPiece","e64bbd2b":"# <center> THANK YOU\u2728 ","dd46beae":"### Various Tokenization Techniques:","41f5fa65":"--> There are different types of subword tokenization and they are given below:\n\n- Byte-Pair Encoding (BPE)\n- WordPiece\n- Unigram Language Model\n- SentencePiece","f12ea037":"In contrast to BPE or WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and the most common substrings. Unigram is not used directly for any of the models in the transformers, but it\u2019s used in conjunction with SentencePiece.","c59ae9aa":"## 4)Penn TreeBank\/Default Tokenization\nTree bank is a corpus created which gives the semantic and syntactical annotation of language. Penn Treebank is one of the largest treebanks which was published. This technique of tokenization separates the punctuation, clitics (words that occur along with other words like I\u2019m, don\u2019t) and hyphenated words together.","dd623d5a":"For example, the text \u201cIt is raining\u201d can be tokenized into \u2018It\u2019, \u2018is\u2019, \u2018raining\u2019","d8e56cc2":"### Tweet Tokenizer\nSpecial texts, like Twitter tweets, have a characteristic structure and the generic tokenizers mentioned above fail to produce viable tokens when applied to these datasets. NLTK offers a special tokenizer for tweets to help in this case. This is a **rule-based tokenizer** that can remove HTML code, remove problematic characters, remove Twitter handles, and normalize text length by reducing the occurrence of repeated letters.","cad64c2e":"#### iii) Unigram Language Model","81b16292":"## 6) Moses Tokenizer\nThis is a tokenizer which is advanced and is available before Spacy was introduced. It is basically a collection of complex normalization and segmentation logic which works very well for structured language like English.","24862970":"![](https:\/\/miro.medium.com\/max\/945\/1*FTEu803GEsNrNslvY1RbXQ.png)","7e1d73e2":"## What is word tokenization?\nTokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called **tokens**. These tokens help in understanding the context or developing the model for the NLP. \nThe tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.","1b5f199a":"- Here is a complete [Tutorial](https:\/\/pycon2016.regex.training\/regex-intro) on Regular expression for more details."}}