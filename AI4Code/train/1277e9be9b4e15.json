{"cell_type":{"0afc6187":"code","63ae2b84":"code","2b32201c":"code","2ec50983":"code","50aa5708":"code","08ae211e":"code","dc4058b3":"code","ea271668":"code","0b3b0991":"code","702ae54f":"code","5cde2daf":"code","b33abb8b":"code","5c78ebbe":"code","69b22a64":"code","d2364677":"code","6fee6360":"code","d5dfb703":"code","01ae21ba":"code","88b8193d":"code","4e531986":"code","30986396":"code","650b792e":"code","62adb2df":"code","18b30ad1":"code","f38cdf7e":"code","5e27fecd":"code","d3c1bab9":"code","260b9ac6":"code","50b1872c":"code","4d161e3e":"code","9f788460":"code","6b6927c3":"code","1f85b93e":"code","2633b73c":"code","dc5be178":"code","46fa87fa":"code","3d30a4c7":"code","15b7c52e":"code","eb87f6de":"code","2c74d572":"code","479fb64e":"code","4f7ee2d7":"code","f0a3bd79":"code","60f53561":"code","801d3cfb":"code","7f3e0851":"code","a82d5a54":"code","6d6648c5":"code","b755e13a":"code","8da26f8e":"code","c902c175":"code","9411b7e4":"code","09990875":"code","fe777883":"code","c78814a4":"code","d569da5f":"code","978f0b28":"code","9fe7498c":"code","0f2b2e78":"code","4930cc75":"code","0767d88d":"code","d216f918":"code","583181cc":"code","eb48f1fb":"code","c6de4a14":"code","12a6769f":"code","1e43e484":"code","98938581":"code","44bb6174":"code","4abf71a3":"code","4e8650a1":"code","fdc275e4":"code","3bf56c80":"code","9af3a686":"markdown","0e589df7":"markdown","faba986f":"markdown","8006a946":"markdown","2464cc00":"markdown","6bd298f0":"markdown","77470a56":"markdown","1a77922c":"markdown","d8f23792":"markdown","c2d15b4d":"markdown","35c014ad":"markdown","69be4a47":"markdown","bf09e1b1":"markdown","b84c7cef":"markdown","1fa9ae1c":"markdown","cce961fd":"markdown","600f8bed":"markdown","2ca6a137":"markdown","b9c55548":"markdown","f86cc8c1":"markdown","bb9ecff4":"markdown","fe13f4d6":"markdown","94395325":"markdown","a652244e":"markdown","4391b86b":"markdown","95ee2e51":"markdown","e3770a70":"markdown","22ca3b13":"markdown","80abac5c":"markdown","c0500fbd":"markdown","1bab75fa":"markdown","edcc3572":"markdown","86ce2d6f":"markdown","653a95c0":"markdown","2b453cdf":"markdown","056bea27":"markdown","3f7e868e":"markdown","7e6b5a1c":"markdown"},"source":{"0afc6187":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, GlobalAveragePooling2D, SpatialDropout2D\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score, f1_score, roc_auc_score, roc_curve\nfrom tensorflow.keras.preprocessing import image\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings('ignore')","63ae2b84":"# Selecting folder paths in training_set\ndir_ = Path('..\/input\/animal-faces\/afhq\/train')\ntrain_filepaths = list(dir_.glob(r'**\/*.jpg'))\n\n# Mapping labels...\nlabels_training = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], train_filepaths))\n\n# Training paths & labels\ntrain_filepaths = pd.Series(train_filepaths, name = 'File').astype(str)\ntrain_labels = pd.Series(labels_training, name='Label')\n\n# Concatenating...\ntrainset_df = pd.concat([train_filepaths, train_labels], axis=1)","2b32201c":"# Selecting folder paths in test_set\ndir_ = Path('..\/input\/animal-faces\/afhq\/val')\ntest_filepaths = list(dir_.glob(r'**\/*.jpg'))\n\n# Mapping labels...\nlabels_test = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], test_filepaths))\n\n# Test paths & labels\ntest_filepaths = pd.Series(test_filepaths, name = 'File').astype(str)\ntest_labels = pd.Series(labels_test, name='Label')\n\n# Concatenating...\ntestset_df = pd.concat([test_filepaths, test_labels], axis=1)","2ec50983":"# Viewing data in both datasets\nprint('Training Dataset:')\n\nprint(f'Number of images in the training dataset: {trainset_df.shape[0]}')\n\nprint(f'Number of images with cats: {trainset_df[\"Label\"].value_counts()[0]}')\nprint(f'Number of images with dogs: {trainset_df[\"Label\"].value_counts()[1]}\\n')\nprint(f'Number of images with wild: {trainset_df[\"Label\"].value_counts()[2]}\\n')\n      \nprint('Test Dataset:')\n      \nprint(f'Number of images in the test dataset: {testset_df.shape[0]}')\n\nprint(f'Number of images with cats: {testset_df[\"Label\"].value_counts()[0]}')\nprint(f'Number of images with dogs: {testset_df[\"Label\"].value_counts()[1]}')\nprint(f'Number of images with wilds: {testset_df[\"Label\"].value_counts()[2]}')","50aa5708":"vc = trainset_df['Label'].value_counts()\nplt.figure(figsize = (9, 5))\nsns.barplot(x = vc.index, y = vc, palette = \"Set2\")\nplt.title(\"Number of images for each category in the Training Dataset\", fontsize = 11)\nplt.show()","08ae211e":"vc = testset_df['Label'].value_counts()\nplt.figure(figsize = (9, 5))\nsns.barplot(x = vc.index, y = vc, palette = \"Set2\")\nplt.title(\"Number of images from each category in the Test Dataset\", fontsize = 11)\nplt.show()","dc4058b3":"trainset_df = trainset_df.sample(frac = 1, random_state = 56).reset_index(drop = True)\ntestset_df = testset_df.sample(frac = 1, random_state = 56).reset_index(drop = True)\n\ndisplay(trainset_df.head())\n\ntestset_df.head()","ea271668":"# converting the Label to a numeric format for testing later...\nLE = LabelEncoder()\n\ny_test = LE.fit_transform(testset_df[\"Label\"])","0b3b0991":"plt.style.use(\"dark_background\")","702ae54f":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(trainset_df[\"File\"][42])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(trainset_df[\"Label\"][42])","5cde2daf":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(trainset_df[\"File\"][48])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(trainset_df[\"Label\"][48])","b33abb8b":"figure = plt.figure(figsize=(10, 10))\nx = plt.imread(trainset_df[\"File\"][9])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(trainset_df[\"Label\"][9])","5c78ebbe":"fig, axes = plt.subplots(nrows = 5,\n                        ncols = 5,\n                        figsize = (10,10),\n                        subplot_kw = {\"xticks\":[],\"yticks\":[]})\n\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(trainset_df[\"File\"][i]))\n    ax.set_title(trainset_df[\"Label\"][i])\nplt.tight_layout()\nplt.show()","69b22a64":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(testset_df[\"File\"][2])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(testset_df[\"Label\"][2])","d2364677":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(testset_df[\"File\"][7])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(testset_df[\"Label\"][7])","6fee6360":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(testset_df[\"File\"][62])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(testset_df[\"Label\"][62])","d5dfb703":"fig, axes = plt.subplots(nrows = 5,\n                        ncols = 5,\n                        figsize = (10,10),\n                        subplot_kw = {\"xticks\":[],\"yticks\":[]})\n\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(testset_df[\"File\"][i]))\n    ax.set_title(testset_df[\"Label\"][i])\nplt.tight_layout()\nplt.show()","01ae21ba":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                    shear_range = 0.2,\n                                    zoom_range = 0.1,\n                                    rotation_range = 20,\n                                    width_shift_range = 0.1,\n                                    height_shift_range = 0.1,\n                                    horizontal_flip = True,\n                                    vertical_flip = True,\n                                    validation_split = 0.1)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)","88b8193d":"print(\"Preparing the training dataset ...\")\ntraining_set = train_datagen.flow_from_dataframe(\n    dataframe = trainset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (250, 250),\n    color_mode = \"rgb\",\n    class_mode = \"categorical\",\n    batch_size = 32,\n    shuffle = True,\n    seed = 2,\n    subset = \"training\")\n\nprint(\"Preparing the validation dataset ...\")\nvalidation_set = train_datagen.flow_from_dataframe(\n    dataframe = trainset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (250, 250),\n    color_mode =\"rgb\",\n    class_mode = \"categorical\",\n    batch_size = 32,\n    shuffle = True,\n    seed = 2,\n    subset = \"validation\")\n\nprint(\"Preparing the test dataset ...\")\ntest_set = test_datagen.flow_from_dataframe(\n    dataframe = testset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (250, 250),\n    color_mode =\"rgb\",\n    class_mode = \"categorical\",\n    shuffle = False,\n    batch_size = 32)\n\nprint('Data generators are ready!')","4e531986":"print(\"Training: \")\nprint(training_set.class_indices)\nprint(training_set.image_shape)\nprint(\"---\" * 8)\nprint(\"Validation: \")\nprint(validation_set.class_indices)\nprint(validation_set.image_shape)\nprint(\"---\" * 8)\nprint(\"Test: \")\nprint(test_set.class_indices)\nprint(test_set.image_shape)","30986396":"# Callbacks\ncb = [EarlyStopping(monitor = 'loss', mode = 'min', patience = 5, restore_best_weights = True)]","650b792e":"CNN = Sequential()\n\nCNN.add(Conv2D(32, (3, 3), input_shape = (250, 250, 3), activation = 'relu'))\nCNN.add(BatchNormalization())","62adb2df":"CNN.add(MaxPooling2D(pool_size = (2, 2)))","18b30ad1":"CNN.add(Conv2D(64, (3, 3), activation = 'relu'))\nCNN.add(SpatialDropout2D(0.2))\nCNN.add(MaxPooling2D(pool_size = (2, 2)))","f38cdf7e":"CNN.add(Flatten())","5e27fecd":"# Input layer\nCNN.add(Dense(units = 64, activation = 'relu'))\nCNN.add(Dropout(0.1))\n# Output layer (binary classification)\nCNN.add(Dense(units = 3, activation = 'softmax'))\n\nprint(CNN.summary())","d3c1bab9":"plot_model(CNN, to_file='CNN_model.png', show_layer_names = True , show_shapes = True)","260b9ac6":"# Compile\nCNN.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n# Start of counting time...\nstart = dt.datetime.now()\n\n# Train\nCNN_model = CNN.fit(training_set, epochs = 15, validation_data = validation_set, callbacks = cb)\n\n# End of counting time...\nend = dt.datetime.now()\ntime_CNN = end - start\nprint ('\\nTraining and validation time is: ', time_CNN)","50b1872c":"acc = CNN_model.history['accuracy']\nval_acc = CNN_model.history['val_accuracy']\nloss = CNN_model.history['loss']\nval_loss = CNN_model.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","4d161e3e":"score_CNN = CNN.evaluate(test_set)\nprint(\"Test Loss:\", score_CNN[0])\nprint(\"Test Accuracy:\", score_CNN[1])","9f788460":"y_pred_CNN = CNN.predict(test_set)\ny_pred_CNN = np.argmax(y_pred_CNN, axis=1)","6b6927c3":"recall_CNN = recall_score(y_test, y_pred_CNN, average = 'weighted')\nprecision_CNN = precision_score(y_test, y_pred_CNN, average = 'weighted')\nf1_CNN = f1_score(y_test, y_pred_CNN, average = 'weighted')","1f85b93e":"print(classification_report(y_test, y_pred_CNN))","2633b73c":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_CNN),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","dc5be178":"# Save the model\nmodelFileName = 'convnet-classifier.h5'\nCNN.save(modelFileName)\nprint('model saved as', modelFileName)","46fa87fa":"CNN_base_inc = InceptionV3(input_shape = (250, 250, 3), include_top = False, weights = 'imagenet',\n                          classifier_activation = 'softmax')","3d30a4c7":"for layer in CNN_base_inc.layers:\n    layer.trainable = False","15b7c52e":"x = layers.Flatten()(CNN_base_inc.output)","eb87f6de":"x = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(3, activation='softmax')(x)\n\nCNN_inc = Model(CNN_base_inc.input, x)","2c74d572":"# Compilation\nCNN_inc.compile(optimizer = RMSprop(lr = 0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_inc_history = CNN_inc.fit(training_set, epochs = 15, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_inc = end - start\nprint ('\\nTraining and validation time is: ', time_CNN_inc)","479fb64e":"acc = CNN_inc_history.history['accuracy']\nval_acc = CNN_inc_history.history['val_accuracy']\nloss = CNN_inc_history.history['loss']\nval_loss = CNN_inc_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","4f7ee2d7":"score_inc = CNN_inc.evaluate(test_set)\nprint(\"Test Loss:\", score_inc[0])\nprint(\"Test Accuracy:\", score_inc[1])","f0a3bd79":"y_pred_inc = CNN_inc.predict(test_set)\ny_pred_inc = np.argmax(y_pred_inc, axis=1)","60f53561":"recall_inc = recall_score(y_test, y_pred_inc, average = 'weighted')\nprecision_inc = precision_score(y_test, y_pred_inc, average = 'weighted')\nf1_inc = f1_score(y_test, y_pred_inc, average = 'weighted')","801d3cfb":"print(classification_report(y_test, y_pred_inc))","7f3e0851":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_inc),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","a82d5a54":"# Save the model\nmodelFileName = 'inception-classifier.h5'\nCNN_inc.save(modelFileName)\nprint('model saved as', modelFileName)","6d6648c5":"CNN_base_xcep = Xception(input_shape = (250, 250, 3), include_top = False, weights = 'imagenet',\n                          classifier_activation = 'softmax')\nCNN_base_xcep.trainable = False","b755e13a":"CNN_xcep = Sequential()\nCNN_xcep.add(CNN_base_xcep)\nCNN_xcep.add(GlobalAveragePooling2D())\nCNN_xcep.add(Dense(128))\nCNN_xcep.add(Dropout(0.1))\nCNN_xcep.add(Dense(3, activation = 'softmax'))\n\nCNN_xcep.summary()","8da26f8e":"plot_model(CNN_xcep, show_layer_names = True , show_shapes = True)","c902c175":"# Compilation\nCNN_xcep.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics=['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_xcep_history = CNN_xcep.fit(training_set, epochs = 15, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_xcep = end - start\nprint ('\\nTraining and validation time: ', time_CNN_xcep)","9411b7e4":"acc = CNN_xcep_history.history['accuracy']\nval_acc = CNN_xcep_history.history['val_accuracy']\nloss = CNN_xcep_history.history['loss']\nval_loss = CNN_xcep_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","09990875":"score_xcep = CNN_xcep.evaluate(test_set)\nprint(\"Test Loss:\", score_xcep[0])\nprint(\"Test Accuracy:\", score_xcep[1])","fe777883":"y_pred_xcep = CNN_xcep.predict(test_set)\ny_pred_xcep = np.argmax(y_pred_xcep, axis=1)\n\nrecall_xcep = recall_score(y_test, y_pred_xcep, average = 'weighted')\nprecision_xcep = precision_score(y_test, y_pred_xcep, average = 'weighted')\nf1_xcep = f1_score(y_test, y_pred_xcep, average = 'weighted')","c78814a4":"print(classification_report(y_test, y_pred_xcep))","d569da5f":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_xcep),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","978f0b28":"modelFileName = 'xception-classifier.h5'\nCNN_xcep.save(modelFileName)\nprint('model saved as', modelFileName)","9fe7498c":"CNN_base_mobilenet = MobileNet(input_shape = (250, 250, 3), include_top = False, weights = 'imagenet',\n                          classifier_activation = 'softmax')","0f2b2e78":"for layer in CNN_base_mobilenet.layers:\n    layer.trainable = False","4930cc75":"CNN_mobilenet = Sequential()\nCNN_mobilenet.add(BatchNormalization(input_shape = (250, 250, 3)))\nCNN_mobilenet.add(CNN_base_mobilenet)\nCNN_mobilenet.add(BatchNormalization())\nCNN_mobilenet.add(GlobalAveragePooling2D())\nCNN_mobilenet.add(Dropout(0.5))\nCNN_mobilenet.add(Dense(3, activation = 'softmax'))\n\nCNN_mobilenet.summary()","0767d88d":"plot_model(CNN_mobilenet, show_layer_names = True , show_shapes = True)","d216f918":"# Compilation\nCNN_mobilenet.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_mobilenet_history = CNN_mobilenet.fit(training_set, epochs = 15, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_mobilenet = end - start\nprint ('\\nTraining and validation time: ', time_CNN_mobilenet)","583181cc":"acc = CNN_mobilenet_history.history['accuracy']\nval_acc = CNN_mobilenet_history.history['val_accuracy']\nloss = CNN_mobilenet_history.history['loss']\nval_loss = CNN_mobilenet_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","eb48f1fb":"score_mn = CNN_mobilenet.evaluate(test_set)\nprint(\"Test Loss:\", score_mn[0])\nprint(\"Test Accuracy:\", score_mn[1])","c6de4a14":"y_pred_mn = CNN_mobilenet.predict(test_set)\ny_pred_mn = np.argmax(y_pred_mn, axis=1)\n\nrecall_mn = recall_score(y_test, y_pred_mn, average = 'weighted')\nprecision_mn = precision_score(y_test, y_pred_mn, average = 'weighted')\nf1_mn = f1_score(y_test, y_pred_mn, average = 'weighted')","12a6769f":"print(classification_report(y_test, y_pred_mn))","1e43e484":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_mn),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","98938581":"# Save the model\nmodelFileName = 'mobilenet-classifier.h5'\nCNN_mobilenet.save(modelFileName)\nprint('model saved as', modelFileName)","44bb6174":"models= [('ConvNet', time_CNN, np.mean(CNN_model.history['accuracy']), np.mean(CNN_model.history['val_accuracy'])),\n         ('Inception', time_CNN_inc, np.mean(CNN_inc_history.history['accuracy']), np.mean(CNN_inc_history.history['val_accuracy'])),\n         ('Xception', time_CNN_xcep, np.mean(CNN_xcep_history.history['accuracy']), np.mean(CNN_xcep_history.history['val_accuracy'])),\n         ('MobileNet', time_CNN_mobilenet, np.mean(CNN_mobilenet_history.history['accuracy']), np.mean(CNN_mobilenet_history.history['val_accuracy']))]\n\ndf_all_models = pd.DataFrame(models, columns = ['Model', 'Time', 'Training accuracy (%)', 'Validation Accuracy (%)'])\n\ndf_all_models","4abf71a3":"models = [('ConvNet', score_CNN[1], recall_CNN, precision_CNN, f1_CNN),\n          ('Inception', score_inc[1], recall_inc, precision_inc, f1_inc),\n          ('Xception', score_xcep[1], recall_xcep, precision_xcep, f1_xcep),\n          ('MobileNet', score_mn[1], recall_mn, precision_mn, f1_mn)]\n\ndf_all_models_testset = pd.DataFrame(models, columns = ['Model', 'Test accuracy (%)', 'Recall (%)', 'Precision (%)', 'F1 (%)'])\n\ndf_all_models_testset","4e8650a1":"plt.subplots(figsize=(12, 10))\nsns.barplot(y = df_all_models_testset['Test accuracy (%)'], x = df_all_models_testset['Model'], palette = 'icefire')\nplt.xlabel(\"Models\")\nplt.title('Accuracy')\nplt.show()","fdc275e4":"test_set.class_indices","3bf56c80":"plt.style.use(\"dark_background\")\n\n\nfig, axes = plt.subplots(nrows = 4,\n                         ncols = 4,\n                         figsize = (20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(testset_df[\"File\"].iloc[i]))\n    ax.set_title(f\"True: {testset_df.Label.iloc[i]}\\n Predicted:\\nConvNet: {y_pred_CNN[i]}\\nInception: {y_pred_inc[i]}\\nXception: {y_pred_xcep[i]}\\nMobileNet: {y_pred_mn[i]}\")\nplt.tight_layout()\nplt.show()","9af3a686":"###### Step 8 - Viewing results and generating forecasts","0e589df7":"###### Step 3 - Hidden Layers","faba986f":"## 4. Generating batches of images\nIn this part we will generate batches of images increasing the training data, for the test database we will just normalize the data using [ImageDataGenerator](https:\/\/keras.io\/api\/preprocessing\/image\/#imagedatagenerator-class)\n\nParameters of ``ImageDataGenerator``:\n\n    rescale - Transform image size (normalization of data)\n    shear_range - Random geometric transformations\n    zoom_range - Images that will be zoomed\n    rotation_range - Degree of image rotation\n    width_shift_range - Image Width Change Range\n    height_shift_range - Image height change range\n    horizontal_flip - Rotate images horizontally\n    vertical_flip - Rotate images vertically\n    validation_split - Images that have been reserved for validation (0-1)","8006a946":"###### Step 8 - Viewing results and generating forecasts","2464cc00":"###### Step 2 - Max Pooling\nReduced image size by focusing on the most important features\n\n     Matrix definition with a total of 4 pixels (2, 2)","6bd298f0":"## 13. Viewing the results of all models","77470a56":"## 9. Construction of the second model (Inception)\nThe [InceptionV3](https:\/\/keras.io\/api\/applications\/inceptionv3\/) model proposed by Szegedy et al. (2015), is a CNN architecture that seeks to solve several large-scale image recognition problems and can also be used in transfer learning problems. Its differential is the presence of convolutional characteristics extractor modules. These modules have the functionality to learn with fewer parameters that contain a greater range of information.\n\n<p><img src = \"https:\/\/cloud.google.com\/tpu\/docs\/images\/inceptionv3onc--oview.png?hl=pt-br\" alt><\/p>","1a77922c":"## 10. Construction of the third model (Xception)\nThe [Xception](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/Xception) model proposed by Chollet et al.(2016), is a CNN architecture similar to the Inception described above and, has the difference that the initiation modules were replaced by separable convolutions in depth. Xception has the same amount of parameters as InceptionV3 with a total of 36 convolutional layers. Thus, having a more efficient use of parameters.\n\n<p><img src = \"https:\/\/miro.medium.com\/max\/1688\/1*J8dborzVBRBupJfvR7YhuA.png\" alt><\/p>","d8f23792":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","c2d15b4d":"###### Step 1 - Base model creation\n\n    input_shape - Setting the height\/width and RGB channels (128, 128, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet","35c014ad":"###### Step 6 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","69be4a47":"###### Step 8 - Viewing results and generating forecasts","bf09e1b1":"###### Step 5 - Dense Neural Networks\n\nParameters of the `` RNA``:\n\n     Dense - All neurons connected\n     units - Number of neurons that are part of the hidden layer\n     activation - Activation function that will be inserted\n     Dropout - is used to decrease the chance of overfitting (20% of the input neurons are zeroed)\n\nParameters of the ``EarlyStopping``:\n\n     monitor - Metric to be monitored\n     patience - Number of seasons without improvement in the model, after the training is interrupted\n     restore_best_weights - Restores the best weights if training is interrupted","b84c7cef":"###### Step 4 - Flattening\n    \n     Transforming the matrix to a vector to enter the Artificial Neural Network layer","1fa9ae1c":"###### Step 1 - Base model creation\n    input_shape - Setting the height\/width and RGB channels (128, 128, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet","cce961fd":"Use of callbacks to monitor models and see if metrics will improve, otherwise training is stopped.\n\n``EarlyStopping`` parameters:\n\n    monitor - Metrics that will be monitored\n    patience - Number of times without improvement in the model, after these times the training is stopped\n    restore_best_weights - Restores best weights if training is interrupted","600f8bed":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","2ca6a137":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","b9c55548":"## 12. Construction of the fourth model (MobileNet)\nThe [MobileNet](https:\/\/nitheshsinghsanjay.github.io\/) model proposed by Howard et al. (2017), is a CNN architecture that were created to perform computer vision tasks on mobile devices and embedded systems. They are based on in-depth separable convolution operations, which lessens the burden of operations in the first layers.\n\n<p><img src = \"https:\/\/nitheshsinghsanjay.github.io\/images\/mobtiny_fig.PNG\" alt><\/p>","f86cc8c1":"###### Observing the images from the test dataset","bb9ecff4":"## 1. Imports from libraries","fe13f4d6":"## 2. Organizing Training and Testing Dataframes","94395325":"## Detection of Cats, Dogs and Wild Animals using Convolutional Neural Networks\n\n<p><img src = \"https:\/\/img.etimg.com\/thumb\/msid-75064456,width-640,resizemode-4,imgsize-1161988\/wild-wild-world.jpg\" alt align=\"center\"><\/p>\n\n#### Dataset information:\n\n- The data was collected to train a model to distinguish between images of dogs, images of cats and images of wild animals. so the whole problem is binary classification.\n\nThe data is divided into 2 folders:\n- The folder `` train`` contains 14630 images (5153 images of cats, 4739 of dogs and 4738 of wild animals) for training the model.\n- The folder `` val`` contains 1500 images (500 images of cats, 500 of dogs and 500 of wild animals) for testing the model.\n\nThe dataset can be found on the `` Kaggle`` platform at the link below:\n\n- https:\/\/www.kaggle.com\/andrewmvd\/animal-faces","a652244e":"###### Step 7 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","4391b86b":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","95ee2e51":"###### Step 2 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","e3770a70":"###### Step 2 - Flattening\n    Transforming the matrix to a vector to enter the Artificial Neural Network layer","22ca3b13":"###### Observing the images from the training dataset","80abac5c":"###### Step 6 - Viewing results and generating forecasts","c0500fbd":"###### Step 1 - Base model creation\n\n    input_shape - Setting the height\/width and RGB channels (128, 128, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet","1bab75fa":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","edcc3572":"## 3. Observing the images","86ce2d6f":"###### Step 2 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","653a95c0":"###### Step 1 - Convolution\nFeature Detector and Feature Map\n\n    Number of filters (32)\n    Dimensions of the feature detector (3, 3)\n    Definition of height \/ width and RGB channels (250, 250, 3)\n    Activation function to remove negative values from the image - 'relu'\n    Processing acceleration - BatchNormalization","2b453cdf":"## 6. Construction of the first model (ConvNet)\n\nCNNs are a specific type of artificial neural network that is very effective for image classification because they are able to take into account the spatial coherence of the image, that is, that pixels close to each other are often related.\n\nThe construction of a CNN begins with specifying the model type. In our case, we will use a ``Sequential`` model.\n\n<p><img src = \"https:\/\/i.ibb.co\/0jWhFsW\/ConvNet.png\" alt><\/p>","056bea27":"###### Step 3 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","3f7e868e":"## 5. Directory of training, validation and test images\n\nHere we make the division of the image bases for training, validation and testing of the model, for that we use the [flow_from_dataframe](https:\/\/keras.io\/api\/preprocessing\/image\/#flowfromdataframe-method)\n\nParameters of ``flow_from_directory``:\n\n    dataframe - Dataframe containing the images directory\n    x_col - Column name containing the images directory\n    y_col - Name of the column containing what we want to predict\n    target_size - size of the images (remembering that it must be the same size as the input layer)\n    color_mode - RGB color standard\n    class_mode - binary class mode (cat\/dog)\n    batch_size - batch size (32)\n    shuffle - Shuffle the data\n    seed - optional random seed for the shuffle\n    subset - Subset of data being training and validation (only used if using validation_split in ImageDataGenerator)","7e6b5a1c":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation"}}