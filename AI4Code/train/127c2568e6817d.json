{"cell_type":{"edcfbfe8":"code","84b4e9f2":"code","c46042a2":"code","8107d10d":"code","5e4151f2":"code","a16df708":"code","408e9dd4":"code","15f3ffd2":"code","ab70da7d":"code","029d058f":"code","17f9edae":"code","e7cdd92c":"code","8fce24da":"code","8fe3c0aa":"code","4eaba012":"code","701ca111":"code","8fb3c204":"code","4fcf62ce":"code","2a970ef6":"markdown","a4b1c4f2":"markdown","937df4ac":"markdown"},"source":{"edcfbfe8":"n_blends = 1\nlist4blend_change_optimal_C_by = [0.5,1]","84b4e9f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c46042a2":"# list_C is min from cv3 rs0,1,100 finer grid than before - used in MoA23 v6 and above\n\nlist_C = [0.03, 0.003, 0.0002, 0.001, 0.0007, 0.001, 0.001, 0.002, 0.1, 0.005, 0.001, 0.01, 0.5, 0.01, 0.0007, 0.002, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.007, 0.005, 0.005, 0.01, 0.005, 0.005, 0.003, 0.002, 0.01, 0.005, 0.002, 0.03, 0.0, 0.1, 0.005, 0.05, 0.02, 0.007, 0.003, 0.007, 0.005, 0.001, 0.003, 0.005, 0.02, 0.01, 0.01, 0.003, 0.001, 0.01, 0.02, 0.02, 0.001, 0.003, 0.005, 0.001, 0.007, 0.01, 0.0003, 0.0002, 0.005, 0.007, 0.002, 0.2, 0.005, 0.001, 0.002, 0.005, 0.005, 0.001, 0.002, 0.007, 0.007, 0.007, 0.005, 0.002, 0.003, 0.002, 0.02, 0.003, 0.0, 0.002, 0.005, 0.007, 0.05, 0.005, 0.02, 0.01, 0.05, 0.003, 0.007, 0.003, 0.002, 0.02, 0.005, 0.02, 0.002, 0.0007, 0.0007, 0.01, 0.002, 0.01, 0.002, 0.001, 0.1, 0.03, 0.005, 0.05, 0.02, 0.02, 0.01, 0.001, 0.003, 0.002, 0.01, 0.001, 0.02, 0.02, 0.005, 0.01, 0.003, 0.001, 0.003, 0.03, 0.07, 0.02, 0.002, 0.003, 0.0005, 0.002, 0.02, 0.007, 0.003, 0.005, 0.005, 0.01, 0.003, 0.003, 0.003, 0.01, 0.01, 0.002, 0.002, 0.0003, 0.02, 0.005, 0.01, 0.01, 0.01, 0.003, 0.003, 0.007, 0.007, 0.001, 0.0007, 0.01, 0.003, 0.007, 0.03, 0.003, 0.001, 1.0, 0.002, 0.02, 0.005, 0.0, 0.001, 0.05, 0.007, 0.05, 0.01, 0.03, 0.003, 0.007, 0.0005, 0.002, 0.005, 0.005, 0.002, 0.005, 0.001, 0.007, 0.007, 0.0007, 0.02, 0.005, 0.03, 0.005, 0.007, 0.001, 0.003, 0.005, 0.007, 0.002, 0.001, 0.02, 0.003, 0.01, 0.003, 0.02, 0.01, 0.0005, 0.1, 0.0007]\n","8107d10d":"for c in list_C[:10]:\n    print(c, type(c))\n    ","5e4151f2":"import lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport time\ndf = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv',index_col = 0)  \ndf0 = df.copy()\ndf['cp_type'] = df['cp_type'].map({'trt_cp':1.0, 'ctl_vehicle':1.0}) # Forget about control group  \ndf['cp_dose'] = df['cp_dose'].map({'D1':0.0, 'D2':1.0})\ndf['cp_time'] = df['cp_time'].map({24:0.0, 48: .5 , 72:1.0})\nX = df.copy()\nX_save = X.copy()\ndf_test = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv',index_col = 0)\ndf0_test = df_test.copy()\ndf_test['cp_type'] = df_test['cp_type'].map({'trt_cp':1.0, 'ctl_vehicle':0.0})\ndf_test['cp_dose'] = df_test['cp_dose'].map({'D1':0.0, 'D2':1.0})\ndf_test['cp_time'] = df_test['cp_time'].map({24:0.0, 48: .5 , 72:1.0})\n\ny = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv',index_col = 0 )\ny_save = y.copy()\nprint(y.iloc[:3,:2])\ndf","a16df708":"y_save.sum(axis = 0 ).sort_values(ascending = False)","408e9dd4":"import warnings\nwarnings.filterwarnings(\"ignore\")","15f3ffd2":"from sklearn.linear_model import LogisticRegression\n\n\ndf_submit = pd.DataFrame(index = df_test.index)\ndf_train_oof_pred = pd.DataFrame(index = df0.index) \n\ndf_stat = pd.DataFrame()\nt00 = time.time()\ndf_stat = pd.DataFrame()\ncnt4df_stat = 0\nfor cnt_target, target_name in enumerate(y_save.columns):#enumerate(['dopamine_receptor_antagonist']) : # y_save.columns):\n    C = list_C[ cnt_target ]\n    y = y_save[target_name]\n    #if target_name == 'dopamine_receptor_antagonist':\n    #    C = 0.003\n    \n    if C != 0:\n        y_pred_submit = np.zeros( len(df_test) )\n        cnt_blend_submit = 0\n        y_pred_oof_blend = np.zeros_like(y,dtype = float)\n        cnt_blend_oof = 0\n        for C_effective in C*np.array(list4blend_change_optimal_C_by): # np.linspace(0.5,1,10): # np.array([0.5,1, 2]):\n            model = LogisticRegression( C = C_effective  ) #, penalty='l1', solver = 'liblinear' )  \n            for cnt in range(n_blends):\n                rs = np.random.randint(10**7)\n                skf = StratifiedKFold(n_splits=3, shuffle=True, random_state= rs )\n                y_pred_oof = np.zeros_like(y,dtype = float)\n                list_loss_train = []\n                for train_index, test_index in skf.split(X, y):\n                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n                    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n                    model.fit(X_train,y_train)\n                    y_pred_oof[ test_index ] = model.predict_proba(X_test)[:,1]\n                    y_pred_submit = (y_pred_submit*cnt_blend_submit + model.predict_proba(df_test)[:,1])\/(cnt_blend_submit+1) # blend\n                    cnt_blend_submit += 1\n\n                    list_loss_train = log_loss(y_train,  model.predict_proba(X_train)[:,1])\n\n                y_pred_oof_blend = (y_pred_oof_blend*cnt_blend_oof + y_pred_oof) \/ (cnt_blend_oof + 1) # blend\n                cnt_blend_oof += 1\n\n                df_stat.loc[cnt4df_stat,'Target'] = target_name\n                df_stat.loc[cnt4df_stat,'Count Blend'] = cnt_blend_oof\n                df_stat.loc[cnt4df_stat,'LogLoss OOF'] = log_loss(y, y_pred_oof )\n                df_stat.loc[cnt4df_stat,'LogLoss Blend OOF'] = log_loss(y, y_pred_oof_blend )\n                df_stat.loc[cnt4df_stat,'LogLoss Train'] = np.mean( list_loss_train )\n                df_stat.loc[cnt4df_stat,'Seconds Passed'] = np.round( time.time() - t00 )\n                df_stat.loc[cnt4df_stat,'Random Seed'] = rs\n                df_stat.loc[cnt4df_stat,'C'] = C\n                df_stat.loc[cnt4df_stat,'C_effective'] = C_effective\n                cnt4df_stat += 1\n        df_train_oof_pred.loc[:,target_name] = y_pred_oof_blend\n    else:\n        mn = y[df0.cp_type=='trt_cp'].mean()\n        print(mn, y.mean(), target_name, y.sum() )\n        y_pred_submit = np.ones_like(df_test.iloc[:,0])*mn # y.mean() \n        df_train_oof_pred.loc[:,target_name] = np.ones_like(df0.iloc[:,0])*mn\n    df_submit.loc[:,target_name] = y_pred_submit        \n    \n    print(cnt_target,   target_name, 'Blend', np.round(df_stat.loc[cnt4df_stat-1,'LogLoss Blend OOF'], 5) , 'No blend', np.round(df_stat.loc[cnt4df_stat-1,'LogLoss OOF'] ,5) )\n    \ntotal_time = time.time()-t00    \n#df_stat.to_csv(\"df_stat.csv\")\nprint(np.round(total_time,0), np.round(total_time\/60,0), np.round(total_time\/3600,0),'seconds,minutes, hours total passed') ; \ndf_train_oof_pred.to_csv('df_train_oof_pred.csv')\ndf_submit","ab70da7d":"df_stat.head()","029d058f":"print(np.round(total_time,0), np.round(total_time\/60,0), np.round(total_time\/3600,0),'seconds,minutes, hours total passed') ; \ndf_stat.to_csv('df_stat.csv')","17f9edae":"# technicalities\n# average results over all targets (y_save.columns)\n#\n\nlc = list(  filter(lambda x: 'LogLoss' in x , df_stat.columns) )\n\nname_oof = lc[0]\ndf_stat2 = pd.DataFrame()\nfor i,c in enumerate(y_save.columns):\n    if c not in list(  df_stat['Target'] ) : continue\n    \n    m = df_stat['Target'] == c\n    df_stat2.loc[c,'Internal Numero'] = i \n    df_stat2.loc[c,'Target Sum'] = y_save[c].sum()\n    df_stat2.loc[c,'Blend gain * 1e5'] = np.round( - 1e5*( df_stat[m]['LogLoss Blend OOF'].iat[-1] - df_stat[m]['LogLoss OOF'].mean() ) , 1)\n    \n    df_stat2.loc[c,'Logloss predict by mean'] = log_loss(y_save[c], np.ones_like(y_save[c])*y_save[c].mean() ) \n    df_stat2.loc[c,'Logloss OOF'] = df_stat[m]['LogLoss OOF'].mean()\n    df_stat2.loc[c,'Logloss Blend OOF'] = df_stat[m]['LogLoss Blend OOF'].iat[-1]\n    df_stat2.loc[c, 'LogLoss Train'] = df_stat[m][ 'LogLoss Train'].mean()\n    \n    df_stat2.loc[c,'Logloss Std'] = df_stat[m]['LogLoss OOF'].std()\n    df_stat2.loc[c,'Logloss Train Std'] = df_stat[m]['LogLoss Train'].std()\n    #df_stat[m]\ndf_stat2['Internal Numero'] = df_stat2['Internal Numero'].astype(int)    \n\ndf_stat2.to_csv('df_stat2_by_targets_aggregated.csv')\n\ndf_stat2.sort_values('Blend gain * 1e5', ascending = False)\n\n","e7cdd92c":"lc = list(  filter(lambda x: 'LogLoss' in x , df_stat.columns) )\nd = df_stat.groupby('Count Blend')[lc].mean() # +['C_effective']\nfig = plt.figure(figsize = (15,6))\nfig.add_subplot(1,2,1)\nfor c in d.columns:\n    if 'Train' not in c:\n        plt.plot(d[c], '*-', label = c)\nplt.legend()\nplt.grid()\nfig.add_subplot(1,2,2)\nfor c in d.columns:\n    if 'Train' in c:\n        plt.plot(d[c], '*-', label = c)\n\nplt.legend()\nplt.grid()\nplt.show()\n#print( d[lc].mean() )\n#print(d.tail(1) )\nprint('No blend:', np.round( d['LogLoss OOF'].mean() , 5 )  , 'Blend', np.round( d.tail(1)['LogLoss Blend OOF'].iat[0], 5) )\nblend_gain = + (  d['LogLoss OOF'].mean() - d.tail(1)['LogLoss Blend OOF'].iat[0] )\nprint('Blend gain to mean in 5-th digits:', np.round( 1e5* blend_gain , 0)  )\n\nprint('Best No blend:', np.round( d['LogLoss OOF'].min() , 5 )  , 'Best Blend', np.round( d['LogLoss Blend OOF'].min() , 5) )\n\nblend_gain = + (  d['LogLoss OOF'].min() - d['LogLoss Blend OOF'].min() )\nprint('Blend gain best to best  in 5-th digits:', np.round( 1e5* blend_gain , 0)  )\n\na = d['LogLoss Blend OOF'].argmin()\nprint('Argmin:', a)\na = int(a\/n_blends)\nprint('Best blend', np.round( d['LogLoss Blend OOF'].min() , 5), ' at C-multiplier', list4blend_change_optimal_C_by[a] ) \nprint('Best blend at N', a)\n","8fce24da":"a = d['LogLoss Blend OOF'].argmin()\nprint('Argmin:', a)\na = int(a\/n_blends)\nprint('Best blend at N', a, 'Correspoinding C-multiplier', list4blend_change_optimal_C_by[a] ) ","8fe3c0aa":"d","4eaba012":"df_stat.describe()","701ca111":"df_stat","8fb3c204":"for f in y_save.columns:\n    #df_submit[f] = train_targets.loc[ train_features.cp_type=='trt_cp', f].mean()\n    df_submit.loc[ df0_test.cp_type!='trt_cp' ,f] = 0\ndf_submit#.head()","4fcf62ce":"df_submit.to_csv(\"submission.csv\")\n","2a970ef6":"# Fascinating plots \n\nSurprise: You can see that blending \"bad solutions\" (poor oof logloss) nevertheless  improve blended score !\n\nOn the plots x-axis - number of blended predictions. \n\nBlend here is simple - just average\n","a4b1c4f2":"Construct logregs on subfolds and blend , over different random splits to subfolds and over differnt C.\n\nMain interesting thing to look - plots at section \"Fascinating plots\".\nEvidently see the Surprise: You can see that blending \"bad solutions\" (poor oof logloss) nevertheless  improve blended score !\n\nAlso nice to see that changing C multiplicatively changes the score additively - that is well expexted, but just to see by own eyes is nice.\n\n\nBasic Logregs with \"optimized\" C for each target. (Found in separete script.)\n\nV23 - C -> C* [0.5, 1]  , blend = 1 (no blend for fixed C) . Logloss OOF \n\nV21 - C -> C* [0.5, 1]  , blend = 10 (for fixed C) . Logloss OOF Best blend 0.0155  at C-multiplier 1 at end point\n\nV20 - C -> C* [0.125\/8 , 0.125\/4 ,0.125\/2 , 0.125 , 0.25, 0.5, 1,2,4,8, 16, 32, 64, 128 ]  , blend = 1 (no blend for fixed C) . Logloss OOF Best blend 0.01567  at C-multiplier 32 , logloss worsen \n\nV19 - C -> C* [0.125\/4 ,0.125\/2 , 0.125 , 0.25, 0.5, 1,2,4,8, 16, 32, 64, 128 ]  , blend = 1 (no blend for fixed C) . Logloss OOF Best Blend 0.01559  at C-multiplier 32 - conclusion if we shift left end lefter the optimal points goes righer, however the overall score decreases \n\nV18 - C -> C* [0.125\/2 , 0.125 , 0.25, 0.5, 1,2,4,8, 16, 32, 64, 128 ]  , blend = 1 (no blend for fixed C) . Logloss OOF \nBest Blend 0.01553 at C-multiplier 16    \n\n\nV17 - C -> C* [0.125 , 0.25, 0.5, 1,2,4,8, 16, 32, 64 ]  , blend = 1 (no blend for fixed C) . Logloss OOF 0.015478 at  16 - agrees with previous \n\nV16 - C -> C* [0.125 , 0.25, 0.5, 1,2,4,8, 16 ]  , blend = 1 (no blend for fixed C) . Logloss OOF 0.01547\n\nV15 - C -> C* [0.25, 0.5, 1,2,4,8, 16 ]  , blend = 1 (no blend for fixed C) . Logloss OOF 0.01543 Best achieved at 8\n\nV14 - C -> C* [0.25, 0.5, 1,2,4,8, ]  , blend = 1 (no blend for fixed C) . Logloss OOF 0.01545 (seems there is some dependence on randomness)\n\nV13 - C -> C* [0.25, 0.5, 1,2,4,8, ]  , blend = 1 (no blend for fixed C) . Logloss OOF  0.01542\n\nV12 - C -> C* [0.5, 1,2,4,8, ]  , blend = 10 . Logloss OOF 0.01536\n\nV11 - C -> C* [0.5, 1,2,4,8, ]  , blend = 1 (no blend for fixed C). Logloss OOF 0.01542 \n\nv9,10 - nothing\n\nV8 - checkpoint:  NO blend, C->C*.5 change (similar to MoA23, but differene is models trained on folds, not on the whole train ) \n\nV7 - checkpoint:  NO blend, no C change (similar to MoA23, but differene is models trained on folds, not on the whole train ) \n\nV6 - nothing\n\nV5 - C -> C* [1,2,4,8,16] - up 4 (including)  score increase, from 8 - decrease ( but probably depends on starting point - if start with not 1, but say 0.5 it might be different - need to check ). At point 26 achieved 0.015353 - best CV scored up to moment. (Most probably unfortunately it most probably will not correspond to good LB score).\n\nV4 -> C -> C* [1,2]\n\nV3 - C->C* 0.5\n\nV2 - standard - 20 blends \n\nV1 - tests\n\n\n","937df4ac":"# Core simulation\n\nLogreg for each targets\n\nwith possible blend over different C and different splits on subfolds \n"}}