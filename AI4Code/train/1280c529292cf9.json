{"cell_type":{"c1817110":"code","b1d6399c":"code","fb9e6726":"code","0c26d79a":"code","a0705a10":"code","4c3bf4d6":"code","708bc096":"code","ff3d0704":"code","8dc716b8":"code","6200a66c":"markdown","b0e406f2":"markdown"},"source":{"c1817110":"from random import choice\nfrom numpy import array, dot, random\nimport numpy as np","b1d6399c":"# MCNeuron for And\nw = random.rand(2)\nw[1] = 1\nw[0] = 1\n\ntraining_data = [\n    (array([0,0]), 0),\n    (array([0,1]), 0),\n    (array([1,0]), 0),\n    (array([1,1]), 1),\n]\n\nstep_function = lambda x: 0 if x < 2 else 1    # Step function with threshold of 2. Anything below is 0 ","fb9e6726":"for x, _ in training_data:\n    result = dot(x, w)\n\n    print(\"{}: {} -> {}\".format(x[:2], result, step_function(result)))\n","0c26d79a":"# MCNeruon for OR\nw = random.rand(2)\nw[1] = 1\nw[0] = 1\n\ntraining_data = [\n    (array([0,0]), 0),\n    (array([0,1]), 1),\n    (array([1,0]), 1),\n    (array([1,1]), 1),\n]\n\nstep_function = lambda x: 0 if x < 1 else 1    # Step function with threshold of 1. Anything below is 0 \n","a0705a10":"for x, _ in training_data:\n    result = dot(x, w)\n\n    print(\"{}: {} -> {}\".format(x[:2], result, step_function(result)))\n","4c3bf4d6":"# MCNeuron for NAND\n\nstep_function = lambda x: 0 if x >= 2 else 1    # Step function with threshold of > 2 is 0 \n\nw[1] = 1\nw[0] = 1\n\ntraining_data = [\n    (array([0,0]), 1),\n    (array([0,1]), 0),\n    (array([1,0]), 0),\n    (array([1,1]), 0),\n]\n\nfor x, _ in training_data:\n    result = dot(x, w)\n\n    print(\"{}: {} -> {}\".format(x[:2], result, step_function(result)))\n","708bc096":"# Rosenblat's Perceptron included a way to adjust the weights and find the appropriate combinations \n# to overcome the need to modify thresholds for each gate separately, it used a bias term using which the thresholds in the \n# neuron can be modified to implement multiple Boolean functions in one code","ff3d0704":"\nstep_function = lambda x: 0 if x < 30 else 1    # Step function with threshold of 0.5. Anything below is 0 \n\n\ntraining_data = [\n    (array([0,0,1]), 0),\n    (array([0,1,1]), 0),\n    (array([1,0,1]), 0),\n    (array([1,1,1]), 1),\n]\n\nw = random.rand(3)\nb = .1    # initializing bias term\nerrors = []\neta = 0.1\nn = 10000\n\nfor i in range(n):\n    x, expected = choice(training_data)\n    \n   # w = np.append(w, b)\n\n    result = dot(w, x)\n    error = expected - step_function(result)   # irrespective of what threshold we set, the algo will find the approp weights\n    errors.append(error)                       # that is the beauty of bias. The 'AND' pattern is learnt from data\n    w += eta * error * x\n\nfor x, _ in training_data:\n    result = dot(x, w)\n    print(\"{}: {} -> {}\".format(x[:3], result, step_function(result)))\n","8dc716b8":"# OR function in RB Neuron\n\n\nstep_function = lambda x: 0 if x < 10 else 1    # Step function with threshold of 0.5. Anything below is 0 \n\n\n# AND gate\ntraining_data = [\n    (array([0,0,1]), 0),\n    (array([0,1,1]), 1),\n    (array([1,0,1]), 1),\n    (array([1,1,1]), 1),\n]\n\nw = random.rand(3)\nb = .1    # initializing bias term\nerrors = []\neta = 0.01\nn = 10000\n\nfor i in range(n):\n    x, expected = choice(training_data)\n    \n   # w = np.append(w, b)\n\n    result = dot(w, x)\n    error = expected - step_function(result)   # irrespective of what threshold we set, the algo will find the approp weights\n    errors.append(error)                       # that is the beauty of bias. The 'OR' pattern is learnt from data\n    w += eta * error * x\n\nfor x, _ in training_data:\n    result = dot(x, w)\n    print(\"{}: {} -> {}\".format(x[:3], result, step_function(result)))\n","6200a66c":"# Importing Library","b0e406f2":"# Creating Data"}}