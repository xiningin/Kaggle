{"cell_type":{"474c170a":"code","1595b722":"code","a51e9410":"code","c50222de":"code","07f554d8":"code","cc16b811":"code","6c60b7e0":"code","b6a4e911":"code","321995c6":"code","92e06454":"code","ee989279":"code","108d405f":"code","436a851b":"code","9c33f8f0":"code","51e67860":"code","e496baf5":"code","44911f09":"code","0af42f2b":"code","4d88e004":"code","79026f8a":"code","296f97f7":"code","bf9426ec":"code","de00a375":"code","4550a52e":"code","5b60cfee":"code","e1c6acb3":"code","9e6bdd81":"code","edf26e6b":"code","33139ee1":"code","fe8d2e60":"code","543c8a4c":"code","bbb28ba6":"code","accc321c":"code","2693ea06":"code","87125d48":"code","7885d120":"code","de492476":"code","0e7fc561":"code","17371fe2":"code","1237b906":"code","efcb9a71":"code","af196ec8":"code","3bbf17f2":"code","b4c59442":"code","558fd3d7":"code","715799f5":"code","8e06acfe":"code","23000110":"code","e9392a5d":"code","faba0d3e":"code","cf7076e5":"code","47afa50e":"code","2bedd405":"code","f586a302":"code","0713b153":"code","dfaacedb":"code","11cda172":"code","1ecdfb42":"code","80f9d01a":"code","11245338":"code","2de907b0":"markdown","595af5fc":"markdown","bd1be978":"markdown","852b596c":"markdown","43f15437":"markdown","30c6f6dc":"markdown","24aac4e3":"markdown","2a4cf1fc":"markdown"},"source":{"474c170a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1595b722":"train_identity=pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_transaction=pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\ntest_identity=pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_transaction=pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')","a51e9410":"train_transaction.columns","c50222de":"test_transaction.columns","07f554d8":"train_identity.shape","cc16b811":"test_identity.shape","6c60b7e0":"train_identity","b6a4e911":"train_transaction.shape","321995c6":"train_transaction","92e06454":"train_transaction.drop(['TransactionID','ProductCD'],axis=1,inplace=True)","ee989279":"y=train_transaction.iloc[:,0]\ntrain_transaction.drop(columns='isFraud',inplace=True)\ny","108d405f":"train_transaction.isnull().sum()","436a851b":"d=[]\nn=(train_transaction.isnull().sum()\/506691)*100\nfor i,j in enumerate(n):\n  if(j>50):\n    d.append(i)\n    print(i)","9c33f8f0":"fields=train_transaction.columns[d]\nfields=np.array(fields)\ntrain_transaction.drop(columns=fields,inplace=True)","51e67860":"train_transaction.shape","e496baf5":"train_transaction.fillna(train_transaction.mean(),inplace=True)","44911f09":"train_transaction.isnull().sum()[train_transaction.isnull().sum()>0]","0af42f2b":"print(\"Before filling: \\n\"+str(train_transaction.card4.value_counts(normalize=True)*100))\ntrain_transaction.card4.fillna('visa',inplace=True)\nprint(\"\\n*** \\n\\nAfter filling: \\n\"+str(train_transaction.card4.value_counts(normalize=True)*100))","4d88e004":"print(\"Before filling: \\n\"+str(train_transaction.card6.value_counts(normalize=True)*100))\ntrain_transaction.card6.fillna(method='ffill',inplace=True)\nprint(\"\\n*** \\n\\nAfter filling: \\n\"+str(train_transaction.card6.value_counts(normalize=True)*100))","79026f8a":"print(\"Before filling: \\n\"+str(train_transaction.P_emaildomain.value_counts(normalize=True)*100))\ntrain_transaction.P_emaildomain.fillna('gmail.com',inplace=True)\nprint(\"\\n*** \\n\\nAfter filling: \\n\"+str(train_transaction.P_emaildomain.value_counts(normalize=True)*100))","296f97f7":"print(\"Before filling: \\n\"+str(train_transaction.M6.value_counts(normalize=True)*100))\ntrain_transaction.M6.fillna(method='ffill',inplace=True)\nprint(\"\\n*** \\n\\nAfter filling: \\n\"+str(train_transaction.M6.value_counts(normalize=True)*100))","bf9426ec":"train_transaction.isnull().sum()[train_transaction.isnull().sum()>0]","de00a375":"from sklearn import preprocessing\n\nlabel_encoder=preprocessing.LabelEncoder()\n\ncard4=pd.Series(label_encoder.fit_transform(train_transaction.card4))\ncard6=pd.Series(label_encoder.fit_transform(train_transaction.card6))\nP_emaildomain=pd.Series(label_encoder.fit_transform(train_transaction.P_emaildomain))\nM6=pd.Series(label_encoder.fit_transform(train_transaction.M6))\n\ntrain_transaction.drop(['card4','card6','M6','P_emaildomain'], axis=1,inplace=True)","4550a52e":"card4=pd.DataFrame(card4,columns=['card4'])\ncard6=pd.DataFrame(card6,columns=['card6'])\nM6=pd.DataFrame(M6,columns=['M6'])\nP_emaildomain=pd.DataFrame(P_emaildomain,columns=['P_emaildomain'])\n\ntrain_transaction=pd.concat([train_transaction,card4,card6,M6,P_emaildomain],axis=1)","5b60cfee":"train_transaction","e1c6acb3":"normalized = preprocessing.normalize(train_transaction)","9e6bdd81":"train_transaction","edf26e6b":"test_transaction.shape","33139ee1":"test_transaction","fe8d2e60":"test_transaction.drop(['TransactionID','ProductCD'],axis=1,inplace=True)","543c8a4c":"fields=test_transaction.columns[d]\nfields=np.array(fields)\ntest_transaction.drop(columns=fields,inplace=True)","bbb28ba6":"test_transaction.shape","accc321c":"n=(train_transaction.isnull().sum()\/506691)*100\nfor i,j in enumerate(n):\n  if(j>50):\n    print(i)\n  else:\n    print('Nothing left.')\n    break","2693ea06":"test_transaction.fillna(test_transaction.mean(),inplace=True)","87125d48":"test_transaction.isnull().sum()[test_transaction.isnull().sum()>0]","7885d120":"print(\"Before filling: \\n\"+str(test_transaction.card4.value_counts(normalize=True)*100))\ntest_transaction.card4.fillna('visa',inplace=True)\nprint(\"\\n*** \\n\\nAfter filling: \\n\"+str(test_transaction.card4.value_counts(normalize=True)*100))","de492476":"print(\"Before filling: \\n\"+str(test_transaction.card6.value_counts(normalize=True)*100))\ntest_transaction.card6.fillna(method='ffill',inplace=True)\nprint(\"\\n*** \\n\\nAfter filling: \\n\"+str(test_transaction.card6.value_counts(normalize=True)*100))","0e7fc561":"print(\"Before filling: \\n\"+str(test_transaction.P_emaildomain.value_counts(normalize=True)*100))\ntest_transaction.P_emaildomain.fillna('gmail.com',inplace=True)\nprint(\"\\n*** \\n\\nAfter filling: \\n\"+str(test_transaction.P_emaildomain.value_counts(normalize=True)*100))","17371fe2":"print(\"Before filling: \\n\"+str(test_transaction.M6.value_counts(normalize=True)*100))\ntest_transaction.M6.fillna(method='ffill',inplace=True)\nprint(\"\\n*** \\n\\nAfter filling: \\n\"+str(test_transaction.M6.value_counts(normalize=True)*100))","1237b906":"test_transaction.isnull().sum()[test_transaction.isnull().sum()>0]","efcb9a71":"from sklearn import preprocessing\n\nlabel_encoder=preprocessing.LabelEncoder()\n\ncard4=pd.Series(label_encoder.fit_transform(test_transaction.card4))\ncard6=pd.Series(label_encoder.fit_transform(test_transaction.card6))\nP_emaildomain=pd.Series(label_encoder.fit_transform(test_transaction.P_emaildomain))\nM6=pd.Series(label_encoder.fit_transform(test_transaction.M6))\n\ntest_transaction.drop(['card4','card6','M6','P_emaildomain'], axis=1,inplace=True)","af196ec8":"card4=pd.DataFrame(card4,columns=['card4'])\ncard6=pd.DataFrame(card6,columns=['card6'])\nM6=pd.DataFrame(M6,columns=['M6'])\nP_emaildomain=pd.DataFrame(P_emaildomain,columns=['P_emaildomain'])\n\ntest_transaction=pd.concat([test_transaction,card4,card6,M6,P_emaildomain],axis=1)","3bbf17f2":"normalized = preprocessing.normalize(test_transaction)","b4c59442":"test_transaction","558fd3d7":"from keras import Model\nfrom keras.layers import Conv2D, MaxPooling2D,AveragePooling2D, Flatten, Dense, Dropout, Activation , Concatenate, Input , BatchNormalization\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.utils import plot_model, to_categorical\nfrom keras.optimizers import SGD, Adam\nimport matplotlib.pyplot as plt","715799f5":"lrr= ReduceLROnPlateau(monitor='val_loss',\n                       factor=0.1, \n                       patience=3, \n                       min_lr=1e-30)","8e06acfe":"encoder_input = Input(199,name = 'input_encoder')\nencoder = Dense(1024, activation='relu',name = 'encoder_layer1')(encoder_input)\nencoder = Dense(512, activation='relu',name = 'encoder_layer2')(encoder)\nencoder = BatchNormalization(name = 'encoder_layer3')(encoder)\nencoder = Dropout(0.3 , name = 'encoder_layer4')(encoder)\nencoder = Dense(256, activation='relu',name = 'encoder_layer5')(encoder)\nencoder = Dense(128, activation='relu',name = 'encoder_layer6')(encoder)\nencoder = Dense(128, activation='relu',name = 'encoder_layer7')(encoder)\nencoder = Dense(64, activation='relu',name = 'encoder_layer8')(encoder)\nencoder = Dense(64, activation='relu',name = 'encoder_layer9')(encoder)\nencoder_output = Dense(64, activation='relu',name = 'output_encoder')(encoder)\n\nEncoder = Model(inputs= [encoder_input], outputs=[encoder_output],name = 'Encoder')\nEncoder.summary()","23000110":"decoder_input = Input(64,name = 'input_decoder')\ndecoder = Dense(64, activation='relu',name = 'decoder_layer1')(decoder_input)\ndecoder = Dense(64, activation='relu',name = 'decoder_layer2')(decoder)\ndecoder = BatchNormalization(name = 'decoder_layer3')(decoder)\ndecoder = Dropout(0.2 , name = 'decoder_layer4')(decoder)\ndecoder = Dense(128, activation='relu',name = 'decoder_layer5')(decoder)\ndecoder = Dense(128, activation='relu',name = 'decoder_layer6')(decoder)\ndecoder = Dense(256, activation='relu',name = 'decoder_layer7')(decoder)\ndecoder = Dense(512, activation='relu',name = 'decoder_layer8')(decoder)\ndecoder = Dense(1024, activation='relu',name = 'decoder_layer9')(decoder)\ndecoder_output = Dense(199,activation = 'relu',name ='output_layer')(decoder)\n\nDecoder = Model(inputs= [decoder_input], outputs=[decoder_output],name = 'Decoder')\nDecoder.summary()\n\n","e9392a5d":"autoencoder_input = Input(199,name = 'input_autoencoder')\nlatent = Encoder(autoencoder_input)\ndecoded_data = Decoder(latent)\nautoencoder = Model(inputs= [autoencoder_input], outputs=[decoded_data],name = 'AutoEncoder')\n\nautoencoder.summary()","faba0d3e":"plot_model(autoencoder,show_shapes=True)","cf7076e5":"adam = Adam(lr=1e-20)\nautoencoder.compile(optimizer=adam,loss='mae')","47afa50e":"\nhistory = autoencoder.fit(train_transaction.iloc[ :10000 ,:],train_transaction.iloc[ :10000 ,:],\n                          batch_size = 300,\n                          validation_split = 0.2,\n                          epochs= 100,\n                          verbose=1,\n                          callbacks=[lrr])","2bedd405":"val_loss = history.history['val_loss']\nloss = history.history['loss']\nplt.plot(val_loss)\nplt.plot(loss)\nplt.xlabel('Epochs')\n\nplt.ylabel('Loss')\nplt.legend(['Val error','Train error'], loc='upper right')\nplt.savefig('plot_error.png')\nplt.show()","f586a302":"input_model = Input(64,name = 'input_layer')\n\nclassifier = Dense(64, activation='relu')(input_model)\nclassifier = Dense(32, activation='relu')(classifier)\noutput = Dense(2, activation='sigmoid')(classifier)\n\nclassifier = Model(inputs= [input_model], outputs=[output])\nclassifier.summary()","0713b153":"sgd=SGD(lr=0.0001, momentum=0.9)\nclassifier.compile(optimizer=sgd, loss='mae' , metrics=['accuracy'])","dfaacedb":"lrr=ReduceLROnPlateau(monitor='val_loss',\n                       factor=0.1, \n                       patience=2, \n                       min_lr=1e-8)","11cda172":"latent_vector=Encoder.predict(train_transaction[:10000])\nhistory = classifier.fit(latent_vector ,y[:10000],\n               batch_size = 128,\n               validation_split = 0.2,\n               epochs= 100,\n               verbose=1,\n               callbacks=[lrr]\n               )","1ecdfb42":"val_loss = history.history['val_loss']\nloss = history.history['loss']\nplt.plot(val_loss)\nplt.plot(loss)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Val error','Train error'], loc='upper right')\nplt.savefig('classifier_plot_error.png')\nplt.show()","80f9d01a":"val_accuracy = history.history['val_accuracy']\naccuracy = history.history['accuracy']\n\nplt.plot(val_accuracy)\nplt.plot(accuracy)\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend(['Val accuracy','Train accuracy'], loc='lower right')\nplt.savefig( 'classifier_plot_accuracy.png')\nplt.show()","11245338":"predicted_latentvec=Encoder.predict(test_transaction)\npredicted=classifier.predict(predicted_latentvec)\npredicted","2de907b0":"* **Columns which have more than 50% null values**","595af5fc":"# Model","bd1be978":"# Train-data stuff","852b596c":"### Conclusion: the same fields as train_transaction had more than 50% null values","43f15437":"* **Filling the null values**","30c6f6dc":"* **Our  categorical data still have null values**","24aac4e3":"* We delete the same columns as we deleted in the test_transaction","2a4cf1fc":"# Test-data stuff"}}