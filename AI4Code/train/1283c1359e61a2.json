{"cell_type":{"5b1ce3e9":"code","45bb24d4":"code","118c0ba2":"code","89ef22f0":"code","260aff39":"code","e9b2e27d":"code","4a108214":"code","c0de2048":"code","7ee0a35a":"code","269d9964":"code","560aa9cf":"code","fdbf1a71":"code","1d024192":"code","b00007bc":"code","28fa8f66":"code","65938050":"code","fb8da581":"code","6e2dc9c6":"code","21d73fb7":"code","41c2c916":"code","7b60ce15":"code","ad6e94e6":"code","499fb883":"code","28d7325d":"code","b17a1f69":"code","d92bf00d":"code","eb93ff71":"code","87a399aa":"code","4f8bb2d8":"code","9761e8f9":"code","257b8862":"code","014aaf54":"code","69a61369":"code","6954373e":"code","9542e176":"code","d8351894":"code","282d4b3a":"code","274d9c9e":"code","c60631c0":"code","11ee5567":"code","e2d6467a":"code","92678b93":"code","dcb50860":"code","32053f1c":"code","a0bf7ce1":"code","80f6cd83":"code","6b7c7b3f":"code","d3a4b7e9":"code","91edc3fa":"code","6aad75e4":"code","cc2a1fd4":"code","02dcd660":"code","85c0bd78":"code","3d4614d8":"code","42f18aaf":"code","decc7768":"code","5d3c6e9b":"code","2d0cc6bc":"code","09c891f5":"code","58b1c4bd":"code","7a613cd9":"code","d19b5948":"code","f6e74dbe":"code","82749f43":"code","182e6805":"code","28aa40e0":"code","bb3d7ffb":"code","2ee95d4e":"code","0316fa1e":"code","3564663e":"code","1e7fb643":"code","34fd104b":"code","b4d56b07":"code","284ab024":"code","7f346e99":"code","22e42c4c":"code","9c4679db":"code","61b38e9c":"code","a317cd7f":"code","feeb9e2b":"code","7f8cb5ed":"code","fddb014b":"code","3d7cdf1d":"code","1796a9bd":"code","88adb8f4":"code","f6c58274":"code","e1b64eca":"code","fa7cecf5":"code","78df53cf":"code","9ba69c6e":"code","b16b8c2e":"code","5e7baf5f":"code","6c7560dc":"code","4f68579a":"code","290bcda2":"code","81e3d4ff":"code","fde9d5e0":"code","b5537200":"code","f8eeb9e8":"code","5783c52b":"code","5435509a":"code","5cb0744b":"code","dea7b1c2":"code","36ee3a6c":"code","6d64d5e9":"code","e9fcbedc":"code","9d9ed088":"code","50d6d6c7":"markdown","dfa9e4b2":"markdown","0dbb65da":"markdown","14f28d09":"markdown","df30dfdf":"markdown","95d65438":"markdown","ab835a82":"markdown","6d633504":"markdown","bd713ab6":"markdown","80cda09d":"markdown","51aa18b7":"markdown","0344d8e5":"markdown","a2b8be23":"markdown","7269f62a":"markdown","fd2f2f3f":"markdown","d03526b8":"markdown","1fa3374c":"markdown","26bbefae":"markdown","fc9a2cb1":"markdown","fc14df54":"markdown","bf94a9b8":"markdown","8450a367":"markdown","85a6bcc2":"markdown","ca41dbbc":"markdown","9a51b7c8":"markdown","b6aa3b5d":"markdown","8eadf98f":"markdown","4c9ce5ba":"markdown","57af66b2":"markdown","d3670b07":"markdown","433db9a1":"markdown","ee998f45":"markdown","1d9223f2":"markdown","d0b9fcf6":"markdown","81dab73e":"markdown","2181740a":"markdown","fa4bce82":"markdown","c08b5e31":"markdown","06d8f44c":"markdown","9fa10ce7":"markdown","a0e62ef8":"markdown","3b234f26":"markdown","27546a4f":"markdown","d2b40583":"markdown","1335af19":"markdown","252467bd":"markdown","b1200e34":"markdown","62779d8f":"markdown","24be9f8e":"markdown","c09beec8":"markdown","dda098e9":"markdown","7e18f14a":"markdown","6a342d7a":"markdown","33c765cc":"markdown","f15a2cb4":"markdown","e35df169":"markdown","0a1ca83f":"markdown","0012c5f8":"markdown","c5eca51f":"markdown","f7d1c97b":"markdown","bda451d1":"markdown","b38cabfc":"markdown","a6c8caa8":"markdown","752d6e09":"markdown","c93e3886":"markdown","52c38ed9":"markdown","2222767f":"markdown","272538c8":"markdown","ead73bd7":"markdown","2e73a8e3":"markdown","6178fa10":"markdown","a59bf83b":"markdown","1fad3455":"markdown","4a6505cf":"markdown","01fbbb51":"markdown","3b44fdf2":"markdown","53b8f363":"markdown","4bc4f24c":"markdown","7a85ba35":"markdown","28641607":"markdown","b33e9b2c":"markdown","c2686325":"markdown","655f03c1":"markdown","46632509":"markdown","10459ef1":"markdown","e098c590":"markdown","ec980e6a":"markdown","795e40e0":"markdown","5a0a6daf":"markdown","e9ef30e9":"markdown","6e209945":"markdown","ad31f398":"markdown","4bb03e38":"markdown","4485b9eb":"markdown","b9a70c25":"markdown","da74f11e":"markdown","bb54830e":"markdown","6ab5d5d7":"markdown","bbe0ce6a":"markdown","227efb12":"markdown","6058a828":"markdown","88b4a035":"markdown","a7a8c5b7":"markdown","57bdc39d":"markdown","987fa1b6":"markdown","522aea95":"markdown","36d31724":"markdown","165dc4cd":"markdown","01e91dc5":"markdown","9fa27f1d":"markdown","643ad3da":"markdown","f5c9122a":"markdown","eb1685d8":"markdown","eefed445":"markdown","e7147845":"markdown","4e272b42":"markdown","287f02f0":"markdown","b0998886":"markdown","4b6ac537":"markdown","470c4fa5":"markdown","70970edb":"markdown","2f63aee2":"markdown","4dd471ec":"markdown","0471e816":"markdown","724fa1c0":"markdown","79e5ebd8":"markdown"},"source":{"5b1ce3e9":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', 105)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom math import sqrt\n\n# plotly\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# sklearn\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, HuberRegressor, Lasso, ElasticNet, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\nfrom mlxtend.regressor import StackingRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings(\"ignore\")\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n","45bb24d4":"def get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score","118c0ba2":"def plotly_scatter_x_y(df_plot, val_x, val_y):\n    \n    value_x = df_plot[val_x] \n    value_y = df_plot[val_y]\n    \n    trace_1 = go.Scatter( x = value_x, y = value_y, name = val_x, \n                         mode=\"markers\", opacity=0.8 )\n\n    data = [trace_1]\n    \n    plot_title = val_y + \" vs. \" + val_x\n    \n    layout = dict(title = plot_title, \n                  xaxis=dict(title = val_x, ticklen=5, zeroline= False),\n                  yaxis=dict(title = val_y, side='left'),                                  \n                  legend=dict(orientation=\"h\", x=0.4, y=1.0),\n                  autosize=False, width=750, height=500,\n                 )\n\n    fig = dict(data = data, layout = layout)\n    iplot(fig)\n","89ef22f0":"def plotly_scatter_x_y_color(df_plot, val_x, val_y, val_z):\n    \n    value_x = df_plot[val_x] \n    value_y = df_plot[val_y]\n    value_z = df_plot[val_z]\n    \n    trace_1 = go.Scatter( \n                         x = value_x, y = value_y, name = val_x, \n                         mode=\"markers\", opacity=0.8, text=value_z,\n                         marker=dict(size=6, color = value_z, \n                                     colorscale='Jet', showscale=True),                        \n                        )\n                            \n    data = [trace_1]\n    \n    plot_title = val_y + \" vs. \" + val_x\n    \n    layout = dict(title = plot_title, \n                  xaxis=dict(title = val_x, ticklen=5, zeroline= False),\n                  yaxis=dict(title = val_y, side='left'),                                  \n                  legend=dict(orientation=\"h\", x=0.4, y=1.0),\n                  autosize=False, width=750, height=500,\n                 )\n\n    fig = dict(data = data, layout = layout)\n    iplot(fig)","260aff39":"def plotly_scatter_x_y_catg_color(df, val_x, val_y, val_z):\n    \n    catg_for_colors = sorted(df[val_z].unique().tolist())\n\n    fig = { 'data': [{ 'x': df[df[val_z]==catg][val_x],\n                       'y': df[df[val_z]==catg][val_y],    \n                       'name': catg, \n                       'text': df[val_z][df[val_z]==catg], \n                       'mode': 'markers',\n                       'marker': {'size': 6},\n                      \n                     } for catg in catg_for_colors       ],\n                       \n            'layout': { 'xaxis': {'title': val_x},\n                        'yaxis': {'title': val_y},                    \n                        'colorway' : ['#a9a9a9', '#e6beff', '#911eb4', '#4363d8', '#42d4f4',\n                                      '#3cb44b', '#bfef45', '#ffe119', '#f58231', '#e6194B'],\n                        'autosize' : False, \n                        'width' : 750, \n                        'height' : 600,\n                      }\n           }\n  \n    iplot(fig)","e9b2e27d":"def plotly_scatter3d(data, feat1, feat2, target) :\n\n    df = data\n    x = df[feat1]\n    y = df[feat2]\n    z = df[target]\n\n    trace1 = go.Scatter3d( x = x, y = y, z = z,\n                           mode='markers',\n                           marker=dict( size=5, color=y,               \n                                        colorscale='Viridis',  \n                                        opacity=0.8 )\n                          )\n    data = [trace1]\n    camera = dict( up=dict(x=0, y=0, z=1),\n                   center=dict(x=0, y=0, z=0.0),\n                   eye=dict(x=2.5, y=0.1, z=0.8) )\n\n    layout = go.Layout( title= target + \" as function of \" +  \n                               feat1 + \" and \" + feat2 ,\n                        autosize=False, width=700, height=600,               \n                        margin=dict( l=15, r=25, b=15, t=30 ) ,\n                        scene=dict(camera=camera,\n                                   xaxis = dict(title=feat1),\n                                   yaxis = dict(title=feat2),\n                                   zaxis = dict(title=target),                                   \n                                  ),\n                       )\n\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)","4a108214":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","c0de2048":"print(\"df_train.shape : \" , df_train.shape)\nprint(\"*\"*50)\nprint(\"df_test.shape  : \" , df_test.shape)","7ee0a35a":"df_train.info()","269d9964":"df_train.head()","560aa9cf":"# dropping the column \"Id\" since it is not useful for predicting SalePrice\ndf_train.drop('Id',axis=1,inplace=True )\nid_test = df_test['Id']                      # for submissions\ndf_test.drop('Id',axis=1,inplace=True )","fdbf1a71":"df_train.describe().transpose()","1d024192":"df_train.describe(include = ['O']).transpose()","b00007bc":"df_train_null = pd.DataFrame()\ndf_train_null['missing'] = df_train.isnull().sum()[df_train.isnull().sum() > 0].sort_values(ascending=False)\n\ndf_test_null = pd.DataFrame(df_test.isnull().sum(), columns = ['missing'])\ndf_test_null = df_test_null.loc[df_test_null['missing'] > 0]","28fa8f66":"trace1 = go.Bar(x = df_train_null.index, \n                y = df_train_null['missing'],\n                name=\"df_train\", \n                text = df_train_null.index)\n\ntrace2 = go.Bar(x = df_test_null.index, \n                y = df_test_null['missing'],\n                name=\"df_test\", \n                text = df_test_null.index)\n\ndata = [trace1, trace2]\n\nlayout = dict(title = \"NaN in test and train\", \n              xaxis=dict(ticklen=10, zeroline= False),\n              yaxis=dict(title = \"number of rows\", side='left', ticklen=10,),                                  \n              legend=dict(orientation=\"v\", x=1.05, y=1.0),\n              autosize=False, width=750, height=500,\n              barmode='stack'\n              )\n\nfig = dict(data = data, layout = layout)\niplot(fig)","65938050":"df_train.drop(['PoolQC', 'FireplaceQu', 'Fence', \n               'Alley', 'MiscFeature'], axis=1, inplace=True)\ndf_test.drop(['PoolQC', 'FireplaceQu', 'Fence',\n               'Alley', 'MiscFeature'], axis=1, inplace=True)","fb8da581":"numerical_columns = df_train.select_dtypes(exclude=['object']).columns.tolist()\nprint(numerical_columns)","6e2dc9c6":"df_train[\"SalePrice_Log\"] = np.log1p(df_train[\"SalePrice\"])","21d73fb7":"fig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles=[\"SalePrice\", \"SalePriceLog\"])\n\n\ntrace_1 = go.Histogram(x=df_train[\"SalePrice\"], name=\"SalePrice\")\ntrace_2 = go.Histogram(x=df_train[\"SalePrice_Log\"], name=\"SalePriceLog\")\n\nfig.append_trace(trace_1, 1, 1)\nfig.append_trace(trace_2, 1, 2)\n\niplot(fig)","41c2c916":"from scipy.stats import skew, kurtosis\nprint(df_train[\"SalePrice\"].skew(),\"   \", df_train[\"SalePrice\"].kurtosis())\nprint(df_train[\"SalePrice_Log\"].skew(),\"  \", df_train[\"SalePrice_Log\"].kurtosis())","7b60ce15":"df_corr = df_train.corrwith(df_train['SalePrice']).abs().sort_values(ascending=False)[2:]\n\ndata = go.Bar(x=df_corr.index, \n              y=df_corr.values )\n       \nlayout = go.Layout(title = 'Correlation to Sale Price', \n                   xaxis = dict(title = ''), \n                   yaxis = dict(title = 'correlation'),\n                   autosize=False, width=750, height=500,)\n\nfig = dict(data = [data], layout = layout)\niplot(fig)","ad6e94e6":"plotly_scatter_x_y(df_train, 'GrLivArea', 'SalePrice')","499fb883":"# outliers GrLivArea\noutliers_GrLivArea = df_train.loc[(df_train['GrLivArea']>4000.0) & (df_train['SalePrice']<300000.0)]\noutliers_GrLivArea[['GrLivArea' , 'SalePrice']]","28d7325d":"df_train['sum_1SF_2SF_LowQualSF'] =  df_train['1stFlrSF'] + df_train['2ndFlrSF'] + df_train['LowQualFinSF']  \ndf_test['sum_1SF_2SF_LowQualSF'] =  df_test['1stFlrSF'] + df_test['2ndFlrSF'] + df_test['LowQualFinSF'] \nprint(sum(df_train['sum_1SF_2SF_LowQualSF'] != df_train['GrLivArea']))\nprint(sum(df_test['sum_1SF_2SF_LowQualSF'] != df_test['GrLivArea']))","b17a1f69":"df_train.drop('sum_1SF_2SF_LowQualSF',axis=1,inplace=True )\ndf_test.drop('sum_1SF_2SF_LowQualSF',axis=1,inplace=True )","d92bf00d":"df_train['GrLivArea'].corr(df_train['SalePrice'])","eb93ff71":"(df_train['GrLivArea']-df_train['LowQualFinSF']).corr(df_train['SalePrice'])","87a399aa":"y_col_vals = 'SalePrice'\narea_features = ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n                 'MasVnrArea', 'GarageArea', 'LotArea',\n                 'WoodDeckSF', 'OpenPorchSF', 'BsmtFinSF1']\n                # 'ScreenPorch'\nx_col_vals = area_features","4f8bb2d8":"nr_rows=3\nnr_cols=3\n\nfig = tools.make_subplots(rows=nr_rows, cols=nr_cols, print_grid=False,\n                          subplot_titles=area_features )\n                                                                \nfor row in range(1,nr_rows+1):\n    for col in range(1,nr_cols+1): \n        \n        i = (row-1) * nr_cols + col-1\n                   \n        trace = go.Scatter(x = df_train[x_col_vals[i]], \n                           y = df_train[y_col_vals], \n                           name=x_col_vals[i], \n                           mode=\"markers\", \n                           opacity=0.8)\n\n        fig.append_trace(trace, row, col,)\n \n                                                                                                  \nfig['layout'].update(height=700, width=900, showlegend=False,\n                     title='SalePrice' + ' vs. Area features')\niplot(fig)                                                ","9761e8f9":"df_train['all_Liv_SF'] = df_train['TotalBsmtSF'] + df_train['1stFlrSF'] + df_train['2ndFlrSF'] \ndf_test['all_Liv_SF'] = df_test['TotalBsmtSF'] + df_test['1stFlrSF'] + df_test['2ndFlrSF'] \n\nprint(df_train['all_Liv_SF'].corr(df_train['SalePrice']))\nprint(df_train['all_Liv_SF'].corr(df_train['SalePrice_Log']))","257b8862":"df_train['all_SF'] = ( df_train['all_Liv_SF'] + df_train['GarageArea'] + df_train['MasVnrArea'] \n                       + df_train['WoodDeckSF'] + df_train['OpenPorchSF'] + df_train['ScreenPorch'] )\ndf_test['all_SF'] = ( df_test['all_Liv_SF'] + df_test['GarageArea'] + df_test['MasVnrArea']\n                      + df_test['WoodDeckSF'] + df_test['OpenPorchSF'] + df_train['ScreenPorch'] )\n\nprint(df_train['all_SF'].corr(df_train['SalePrice']))\nprint(df_train['all_SF'].corr(df_train['SalePrice_Log']))","014aaf54":"df_train['all_SF'].corr(df_train['all_Liv_SF'])","69a61369":"plotly_scatter_x_y(df_train, 'all_SF', 'SalePrice')","6954373e":"outliers_allSF = df_train.loc[(df_train['all_SF']>8000.0) & (df_train['SalePrice']<200000.0)]\noutliers_allSF[['all_SF' , 'SalePrice']]","9542e176":"df_train = df_train.drop(outliers_allSF.index)","d8351894":"df_train.corr().abs()[['SalePrice','SalePrice_Log']].sort_values(by='SalePrice', ascending=False)[2:16]","282d4b3a":"trace = []\nfor name, group in df_train[[\"SalePrice\", \"OverallQual\"]].groupby(\"OverallQual\"):\n    trace.append( go.Box( y=group[\"SalePrice\"].values, name=name ) )\n    \nlayout = go.Layout(title=\"OverallQual\", \n                   xaxis=dict(title='OverallQual',ticklen=5, zeroline= False),\n                   yaxis=dict(title='SalePrice', side='left'),\n                   autosize=False, width=750, height=500)\n\nfig = go.Figure(data=trace, layout=layout)\niplot(fig)","274d9c9e":"outliers_OverallQual_4 = df_train.loc[(df_train['OverallQual']==4) & (df_train['SalePrice']>200000.0)]\noutliers_OverallQual_8 = df_train.loc[(df_train['OverallQual']==8) & (df_train['SalePrice']>500000.0)]\noutliers_OverallQual_9 = df_train.loc[(df_train['OverallQual']==9) & (df_train['SalePrice']>500000.0)]\noutliers_OverallQual_10 = df_train.loc[(df_train['OverallQual']==10) & (df_train['SalePrice']>700000.0)]\n\noutliers_OverallQual = pd.concat([outliers_OverallQual_4, outliers_OverallQual_8, \n                                  outliers_OverallQual_9, outliers_OverallQual_10])","c60631c0":"outliers_OverallQual[['OverallQual' , 'SalePrice']]","11ee5567":"df_train = df_train.drop(outliers_OverallQual.index)","e2d6467a":"df_train.corr().abs()[['SalePrice','SalePrice_Log']].sort_values(by='SalePrice', ascending=False)[2:16]","92678b93":"plotly_scatter_x_y_catg_color(df_train, 'all_SF', 'SalePrice', 'OverallQual')","dcb50860":"plotly_scatter3d(df_train, 'all_SF', 'OverallQual', 'SalePrice')","32053f1c":"print(df_train['OverallQual'].corr(df_train['all_SF']))","a0bf7ce1":"print(df_train['OverallCond'].corr(df_train['SalePrice']))\nprint(df_train['OverallCond'].corr(df_train['SalePrice_Log']))","80f6cd83":"print(df_train['MSSubClass'].corr(df_train['SalePrice']))\nprint(df_train['MSSubClass'].corr(df_train['SalePrice_Log']))","6b7c7b3f":"categorical_columns = df_train.select_dtypes(include=['object']).columns.tolist()","d3a4b7e9":"def plotly_boxplots_sorted_by_yvals(df, catg_feature, sort_by_target):\n    \n    df_by_catg   = df.groupby([catg_feature])\n    sortedlist_catg_str = df_by_catg[sort_by_target].median().sort_values().keys().tolist()\n    \n    \n    data = []\n    for i in sortedlist_catg_str :\n        data.append(go.Box(y = df[df[catg_feature]==i][sort_by_target], name = i))\n\n    layout = go.Layout(title = sort_by_target + \" vs \" + catg_feature, \n                       xaxis = dict(title = catg_feature), \n                       yaxis = dict(title = sort_by_target))\n\n    fig = dict(data = data, layout = layout)\n    return fig","91edc3fa":"fig = plotly_boxplots_sorted_by_yvals(df_train, 'Neighborhood', 'SalePrice')\niplot(fig)","6aad75e4":"fig = plotly_boxplots_sorted_by_yvals(df_train, 'MSZoning', 'SalePrice')\niplot(fig)","cc2a1fd4":"outliers_all = []\ndf_train = df_train.drop(outliers_all)","02dcd660":"# store target as y and y_log:\ny , y_log = df_train[\"SalePrice\"] , df_train[\"SalePrice_Log\"]\n# drop target from df_train:\ndf_train.drop([\"SalePrice\", \"SalePrice_Log\"] , axis=1, inplace=True)","85c0bd78":"X_1 = df_train\ny_1 = y_log","3d4614d8":"X_2 = df_train\ny_2 = y","42f18aaf":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","decc7768":"numerical_features   = df_train.select_dtypes(exclude=['object']).columns.tolist()\ncategorical_features = df_train.select_dtypes(include=['object']).columns.tolist()","5d3c6e9b":"numerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])","2d0cc6bc":"categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])","09c891f5":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer,   numerical_features),\n        ('cat', categorical_transformer, categorical_features)])","58b1c4bd":"# LinearRegression\npipe_Linear = Pipeline(\n    steps   = [('preprocessor', preprocessor),\n               ('Linear', LinearRegression()) ])    \n# Ridge\npipe_Ridge = Pipeline(\n    steps  = [('preprocessor', preprocessor),\n              ('Ridge', Ridge(random_state=5)) ])  \n# Huber\npipe_Huber = Pipeline(\n    steps  = [('preprocessor', preprocessor),\n              ('Huber', HuberRegressor()) ])  \n# Lasso\npipe_Lasso = Pipeline(\n    steps  = [ ('preprocessor', preprocessor),\n               ('Lasso', Lasso(random_state=5)) ])\n# ElasticNet\npipe_ElaNet = Pipeline(\n    steps   = [ ('preprocessor', preprocessor),\n                ('ElaNet', ElasticNet(random_state=5)) ])\n\n# BayesianRidge\npipe_BayesRidge = Pipeline(\n    steps   = [ ('preprocessor', preprocessor),\n                ('BayesRidge', BayesianRidge(n_iter=500, compute_score=True)) ])\n","7a613cd9":"# GradientBoostingRegressor\npipe_GBR  = Pipeline(\n    steps = [ ('preprocessor', preprocessor),\n              ('GBR', GradientBoostingRegressor(random_state=5 )) ])\n\n# XGBRegressor\npipe_XGB  = Pipeline(\n    steps = [ ('preprocessor', preprocessor),\n              ('XGB', XGBRegressor(objective='reg:squarederror', metric='rmse', \n                      random_state=5, nthread = -1)) ])\n# LGBM\npipe_LGBM = Pipeline(\n    steps= [('preprocessor', preprocessor),\n            ('LGBM', LGBMRegressor(objective='regression', metric='rmse',\n                                  random_state=5)) ])\n# AdaBoostRegressor\npipe_ADA = Pipeline(\n    steps= [('preprocessor', preprocessor),\n            ('ADA', AdaBoostRegressor(DecisionTreeRegressor(), \n                random_state=5, loss='exponential')) ])","d19b5948":"list_pipelines = [pipe_Linear, pipe_Ridge, pipe_Huber, pipe_Lasso, pipe_ElaNet]","f6e74dbe":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\nfor pipe in list_pipelines :\n    \n    scores = cross_val_score(pipe, X_1, y_1, scoring='neg_mean_squared_error', cv=5)\n    scores = np.sqrt(-scores)\n    print(pipe.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","82749f43":"list_pipelines = [pipe_GBR, pipe_XGB, pipe_LGBM, pipe_ADA]","182e6805":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\n\nfor pipe in list_pipelines :\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\",category=FutureWarning)\n        scores = cross_val_score(pipe, X_1, y_1, scoring='neg_mean_squared_error', cv=5)\n        scores = np.sqrt(-scores)\n        print(pipe.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","28aa40e0":"list_scalers = [StandardScaler(), \n                RobustScaler(), \n                QuantileTransformer(output_distribution='normal')]","bb3d7ffb":"list_scalers = [StandardScaler()]","2ee95d4e":"parameters_Linear = { 'preprocessor__num__scaler': list_scalers,\n                     'Linear__fit_intercept':  [True,False],\n                     'Linear__normalize':  [True,False] }\n\ngscv_Linear = GridSearchCV(pipe_Linear, parameters_Linear, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=0, cv=5)\ngscv_Linear.fit(X_1, y_1)","0316fa1e":"print(np.sqrt(-gscv_Linear.best_score_))  \ngscv_Linear.best_params_","3564663e":"parameters_Ridge = { 'preprocessor__num__scaler': list_scalers,\n                     'Ridge__alpha': [7,8,9],\n                     'Ridge__fit_intercept':  [True,False],\n                     'Ridge__normalize':  [True,False] }\n\ngscv_Ridge = GridSearchCV(pipe_Ridge, parameters_Ridge, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=0, cv=5)\ngscv_Ridge.fit(X_1, y_1)","1e7fb643":"print(np.sqrt(-gscv_Ridge.best_score_))  \ngscv_Ridge.best_params_","34fd104b":"parameters_Huber = { 'preprocessor__num__scaler': list_scalers,                   \n                     'Huber__epsilon': [1.3, 1.35, 1.4],    \n                     'Huber__max_iter': [150, 200, 250],                    \n                     'Huber__alpha': [0.0005, 0.001, 0.002],\n                     'Huber__fit_intercept':  [True], }\n\ngscv_Huber = GridSearchCV(pipe_Huber, parameters_Huber, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_Huber.fit(X_1, y_1)","b4d56b07":"print(np.sqrt(-gscv_Huber.best_score_))  \ngscv_Huber.best_params_","284ab024":"parameters_Lasso = { 'preprocessor__num__scaler': list_scalers,\n                     'Lasso__alpha': [0.0005, 0.001],\n                     'Lasso__fit_intercept':  [True],\n                     'Lasso__normalize':  [True,False] }\n\ngscv_Lasso = GridSearchCV(pipe_Lasso, parameters_Lasso, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_Lasso.fit(X_1, y_1)","7f346e99":"print(np.sqrt(-gscv_Lasso.best_score_))  \ngscv_Lasso.best_params_","22e42c4c":"parameters_ElaNet = { 'ElaNet__alpha': [0.0005, 0.001],\n                      'ElaNet__l1_ratio':  [0.85, 0.9],\n                      'ElaNet__normalize':  [True,False] }\n\ngscv_ElaNet = GridSearchCV(pipe_ElaNet, parameters_ElaNet, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_ElaNet.fit(X_1, y_1)","9c4679db":"print(np.sqrt(-gscv_ElaNet.best_score_))  \ngscv_ElaNet.best_params_","61b38e9c":"list_pipelines_gscv = [gscv_Linear,gscv_Ridge,gscv_Huber,gscv_Lasso,gscv_ElaNet]","a317cd7f":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\nfor gscv in list_pipelines_gscv :\n    \n    scores = cross_val_score(gscv.best_estimator_, X_1, y_1, \n                             scoring='neg_mean_squared_error', cv=5)\n    scores = np.sqrt(-scores)\n    print(gscv.estimator.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","feeb9e2b":"parameters_GBR = { 'GBR__n_estimators':  [400], \n                   'GBR__max_depth':  [3,4],\n                   'GBR__min_samples_leaf':  [5,6],                 \n                   'GBR__max_features':  [\"auto\",0.5,0.7],                  \n                 }\n                   \ngscv_GBR = GridSearchCV(pipe_GBR, parameters_GBR, n_jobs=-1, \n                        scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_GBR.fit(X_1, y_1)","7f8cb5ed":"print(np.sqrt(-gscv_GBR.best_score_))  \ngscv_GBR.best_params_","fddb014b":"parameters_XGB = { 'XGB__learning_rate': [0.021,0.022],\n                   'XGB__max_depth':  [2,3],\n                   'XGB__n_estimators':  [2000], \n                   'XGB__reg_lambda':  [1.5, 1.6], \n                   'XGB__reg_alpha':  [1,1.5],                   \n# colsample_bytree , subsample               \n                  }\n                   \ngscv_XGB = GridSearchCV(pipe_XGB, parameters_XGB, n_jobs=-1, \n                        scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_XGB.fit(X_1, y_1)","3d7cdf1d":"print(np.sqrt(-gscv_XGB.best_score_))  \ngscv_XGB.best_params_","1796a9bd":"parameters_LGBM = { 'LGBM__learning_rate': [0.01,0.02],\n                    'LGBM__n_estimators':  [1000], \n                    'LGBM__num_leaves':  [8,10],\n                    'LGBM__bagging_fraction':  [0.7,0.8],\n                    'LGBM__bagging_freq':  [1,2],                  \n                   }\n\ngscv_LGBM = GridSearchCV(pipe_LGBM, parameters_LGBM, n_jobs=-1, \n                       scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_LGBM.fit(X_1, y_1)","88adb8f4":"print(np.sqrt(-gscv_LGBM.best_score_))  \ngscv_LGBM.best_params_","f6c58274":"parameters_ADA = { 'ADA__learning_rate': [3.5],\n                   'ADA__n_estimators':  [500], \n                   'ADA__base_estimator__max_depth':  [8,9,10],                  \n                 }\n\npipe_ADA = Pipeline(\n    steps= [('preprocessor', preprocessor),\n            ('ADA', AdaBoostRegressor(\n                DecisionTreeRegressor(min_samples_leaf=5,\n                                      min_samples_split=5), \n                random_state=5,loss='exponential')) ])\n\ngscv_ADA = GridSearchCV(pipe_ADA, parameters_ADA, n_jobs=-1, \n                       scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_ADA.fit(X_1, y_1)","e1b64eca":"print(np.sqrt(-gscv_ADA.best_score_))  \ngscv_ADA.best_params_","fa7cecf5":"list_pipelines_gscv = [gscv_GBR, gscv_XGB,gscv_LGBM,gscv_ADA]","78df53cf":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\nfor gscv in list_pipelines_gscv :\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\",category=FutureWarning)    \n        scores = cross_val_score(gscv.best_estimator_, X_1, y_1, \n                             scoring='neg_mean_squared_error', cv=5)\n        scores = np.sqrt(-scores)\n        print(gscv.estimator.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","9ba69c6e":"linear_models = [gscv_Linear,gscv_Ridge,gscv_Huber,gscv_Lasso,gscv_ElaNet]\nboost_models  = [gscv_GBR, gscv_XGB,gscv_LGBM,gscv_ADA]","b16b8c2e":"pred_Linear = gscv_Linear.predict(df_test)\npred_Ridge  = gscv_Ridge.predict(df_test)\npred_Huber  = gscv_Huber.predict(df_test)\npred_Lasso  = gscv_Lasso.predict(df_test)\npred_ElaNet = gscv_ElaNet.predict(df_test)","5e7baf5f":"predictions_linear = {'Linear': pred_Linear, 'Ridge': pred_Ridge, 'Huber': pred_Huber,\n                      'Lasso':  pred_Lasso, 'ElaNet': pred_ElaNet }","6c7560dc":"for model,values in predictions_linear.items():\n    str_filename = model + \".csv\"\n    print(\"witing submission to : \", str_filename)\n    subm = pd.DataFrame()\n    subm['Id'] = id_test\n    subm['SalePrice'] = np.expm1(values)\n    subm.to_csv(str_filename, index=False)","4f68579a":"pred_Blend_1 = (pred_Lasso + pred_Ridge) \/ 2\nsub_Blend_1 = pd.DataFrame()\nsub_Blend_1['Id'] = id_test\nsub_Blend_1['SalePrice'] = np.expm1(pred_Blend_1)\nsub_Blend_1.to_csv('Blend_Ridge_Lasso.csv',index=False)\nsub_Blend_1.head()","290bcda2":"pred_Blend_2 = (pred_Lasso + pred_ElaNet) \/ 2\nsub_Blend_2 = pd.DataFrame()\nsub_Blend_2['Id'] = id_test\nsub_Blend_2['SalePrice'] = np.expm1(pred_Blend_2)\nsub_Blend_2.to_csv('Blend_2.csv',index=False)\nsub_Blend_2.head()","81e3d4ff":"pred_Blend_3 = (pred_Ridge + pred_Lasso + pred_ElaNet) \/ 3\nsub_Blend_3 = pd.DataFrame()\nsub_Blend_3['Id'] = id_test\nsub_Blend_3['SalePrice'] = np.expm1(pred_Blend_3)\nsub_Blend_3.to_csv('Blend_3.csv',index=False)\nsub_Blend_3.head()","fde9d5e0":"boost_models  = [gscv_GBR, gscv_XGB,gscv_LGBM,gscv_ADA]","b5537200":"pred_GBR  = gscv_GBR.predict(df_test)\npred_XGB  = gscv_XGB.predict(df_test)\npred_LGBM = gscv_LGBM.predict(df_test)\npred_ADA  = gscv_ADA.predict(df_test)","f8eeb9e8":"predictions_boost = {'GBR': pred_GBR, 'XGB': pred_XGB, 'LGBM': pred_LGBM,\n                     'ADA': pred_ADA }","5783c52b":"for model,values in predictions_boost.items():\n    str_filename = model + \".csv\"\n    print(\"witing submission to : \", str_filename)\n    subm = pd.DataFrame()\n    subm['Id'] = id_test\n    subm['SalePrice'] = np.expm1(values)\n    subm.to_csv(str_filename, index=False)","5435509a":"predictions = {'Ridge': pred_Ridge, 'Lasso': pred_Lasso, 'ElaNet': pred_ElaNet, \n               'GBR': pred_GBR, 'XGB': pred_XGB, 'LGBM': pred_LGBM, 'ADA': pred_ADA}\ndf_predictions = pd.DataFrame(data=predictions) \ndf_predictions.corr()","5cb0744b":"pred_Blend_10 = (pred_Ridge + pred_XGB) \/ 2\nsub_Blend_10 = pd.DataFrame()\nsub_Blend_10['Id'] = id_test\nsub_Blend_10['SalePrice'] = np.expm1(pred_Blend_10)\nsub_Blend_10.to_csv('Blend_Ridge_XGB.csv',index=False)\nsub_Blend_10.head()","dea7b1c2":"lnr = LinearRegression(n_jobs = -1)\n\nrdg = Ridge(alpha=3.0, copy_X=True, fit_intercept=True, random_state=1)\n\nrft = RandomForestRegressor(n_estimators = 12, max_depth = 3, n_jobs = -1, random_state=1)\n\ngbr = GradientBoostingRegressor(n_estimators = 40, max_depth = 2, random_state=1)\n\nmlp = MLPRegressor(hidden_layer_sizes = (90, 90), alpha = 2.75, random_state=1)","36ee3a6c":"stack1 = StackingRegressor(regressors = [rdg, rft, gbr], \n                           meta_regressor = lnr)","6d64d5e9":"pipe_STACK_1 = Pipeline(steps=[ ('preprocessor', preprocessor),\n                                ('stack1', stack1) ])\n\npipe_STACK_1.fit(X_1, y_1)","e9fcbedc":"pred_stack1 = pipe_STACK_1.predict(df_test)\nsub_stack1 = pd.DataFrame()\nsub_stack1['Id'] = id_test\nsub_stack1['SalePrice'] = np.expm1(pred_stack1)\nsub_stack1.to_csv('pipe_stack1.csv',index=False)","9d9ed088":"sub_stack1.head(10)","50d6d6c7":"https:\/\/stackoverflow.com\/questions\/50722270\/convergence-warningstochastic-optimizer-maximum-iterations-200-reached-and-t?rq=1","dfa9e4b2":"**fit_intercept** : boolean, optional, default True  \n**normalize** : boolean, optional, default False  \n**copy_X** : boolean, optional, default True  \n**n_jobs** : int or None, optional (default=None)  \nThe number of jobs to use for the computation. This will only provide speedup for n_targets > 1 and sufficient large problems. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.","0dbb65da":"![](https:\/\/www.mipropertygroup.com.au\/wp-content\/uploads\/2016\/10\/house-prices-double.jpeg)","14f28d09":"### 1.3.1 Correlation of numerical features to SalePrice","df30dfdf":"#### Ridge","95d65438":"## 5.1 Correlation of predictions","ab835a82":"**Huber Regressor**","6d633504":"https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html  \nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/param_tuning.html  \nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html","bd713ab6":"#### Barchart: Correlation to SalePrice","80cda09d":"Linear Regression and especially Ridge model give quite good results already with default parameters.  \nFor Huber, Lasso and Elastic Net we need to tune hyperparameters (see Part 4: GridSearchCV)","51aa18b7":"**other numerical features**","0344d8e5":"### New feature: Sum of many area features","a2b8be23":"**epsilon** : > 1.0, default 1.35  \ncontrols the number of samples that should be classified as outliers.  \nThe smaller the epsilon, the more robust it is to outliers.  \n**max_iter** : int, default 100  \nMaximum number of iterations that scipy.optimize.fmin_l_bfgs_b should run for.  \n**alpha** : float, default 0.0001  \nRegularization parameter.  \n**fit_intercept** : bool, default True","7269f62a":"**num_iterations**, default=100, alias=num_iteration,num_tree,num_trees,num_round,num_rounds  \n**learning_rate**, default=0.1, alias=shrinkage_rate  \n**num_leaves**, default=31\n\n\n**max_depth**, default=-1, < 0 means no limit  \n**min_data_in_leaf**, default=20, type=int, alias=min_data_per_leaf , min_data  \n**min_sum_hessian_in_leaf**, default=1e-3, alias=min_sum_hessian_per_leaf, min_sum_hessian, min_hessian  \n**feature_fraction**, default=1.0, 0.0 < feature_fraction < 1.0, alias=sub_feature  \n**bagging_fraction**, default=1.0, 0.0 < bagging_fraction < 1.0, alias=sub_row  \n**bagging_freq**, default=0,   \nFrequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration   \n**early_stopping_round** , default=0, type=int, alias=early_stopping_rounds,early_stopping  \nWill stop training if one metric of one validation data doesn\u2019t improve in last early_stopping_round rounds  \n**lambda_l1** , default=0  \n**lambda_l2** , default=0","fd2f2f3f":"### Imports","d03526b8":"**Dropping the column \"sum_1SF_2SF_LowQualSF\" again since it already exists as GrLivArea**","1fa3374c":"For some linear models, QuantileTransformer gives best score for CV.  \nBut for test score, performance is best with StandardScaler for all models.  \nTherefore:","26bbefae":"#### Skewness and Kurtosis","fc9a2cb1":"## 2.1 Pipeline approach","fc14df54":"### Scatterplots: SalePrice vs Area features","bf94a9b8":"Another option to highlight the correlation of SalePrice to all_SF and  \nOverallQual as well as the correlation between all_SF and OverallQual is  \na 3d scatter plot:","8450a367":"**For the remaining missing values we use a preprocessing Pipeline**  \n**with Imputer from sklearn (see: 2.2 Preprocessing pipeline)**","85a6bcc2":"## 2.0 define data for regression models","ca41dbbc":"**new feature : sum of all Living SF areas**  \n**all_Liv_SF = 'TotalBsmtSF' + '1stFlrSF' + '2ndFlrSF'**","9a51b7c8":"**XGB**","b6aa3b5d":"Lets look at the other Area features and see if we can derive a feature  \nthat has even larger correlation to SalePrice","8eadf98f":"https:\/\/testlightgbm.readthedocs.io\/en\/latest\/Parameters.html  \nhttps:\/\/testlightgbm.readthedocs.io\/en\/latest\/Parameters-tuning.html  ","4c9ce5ba":"## 2.3 Append regressors to pipeline","57af66b2":"  \n**all_SF = 'all_Liv_SF' + 'GarageArea' + 'MasVnrArea' + 'WoodDeckSF' + 'OpenPorchSF' + 'ScreenPorch'**","d3670b07":"## 4.2 Ensemble Models","433db9a1":"**Note on Outlier Detection**  \nWe store the index of the two data points to the lower right,  \nwith SalePrice < 200 k and GrLivArea > 4000","ee998f45":"## 1.4 Visualizations for Categorical features","1d9223f2":"General Parameters  \n**booster**: gbtree, gblinear or dart, default= gbtree   \n\nParameters for Tree Booster  \n**eta**, alias: learning_rate, 0<eta<1 , default=0.3  \n**gamma**, alias: min_split_loss,  default=0,  \nMinimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.  \n**max_depth**, default=6  \n**min_child_weight**, default=1\nThe larger min_child_weight is, the more conservative the algorithm will be.  \n**max_delta_step** [default=0]  \n**subsample** [default=1],  range: (0,1]  \nSetting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. Subsampling will occur once in every boosting iteration.  \n**colsample_bytree, colsample_bylevel, colsample_bynode** [default=1]  \nThis is a family of parameters for subsampling of columns.  \nAll colsample_by* parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.  \ncolsample_by* parameters work cumulatively. For instance, the combination {'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} with 64 features will leave 8 features to choose from at each split.  \n**lambda** [default=1, alias: reg_lambda]  \nL2 regularization term on weights. Increasing this value will make model more conservative.  \n**alpha** [default=0, alias: reg_alpha]  \nL1 regularization term on weights. Increasing this value will make model more conservative.  \n**tree_method** string [default= auto]  \nChoices: auto, exact, approx, hist, gpu_hist  ","d0b9fcf6":"### **Outline of this kernel:**\n\n[**Part 0: Imports, functions and info on data**](#Part 0: Imports, useful functions)  \n**0.1 data fields**  \n**0.2 data_description.txt**  \n[**Part 1: Exploratory Data Analysis**](#PART-1:-Exploratory-Data-Analysis)  \n**1.1 First look with Pandas**  \nshape, info, head   \ndescribe for [numerical](#describe-for-numerical-features) and [categorical](#describe-for-categorical-features) columns  \n**1.2 Handling missing values**  \n[Barchart: NaN in test and train](#Barchart:-NaN-in-test-and-train)  \n[Drop columns with lots of missing data](#Drop-columns-with-lots-of-missing-data)   \n**1.3 Visualizations for numerical features**    \n1.3.0 Distribution of the target  \n[Distplot for SalePrice and SalePrice_Log](#Distplot-for-SalePrice-and-SalePrice_Log)  \n[Skewness and Kurtosis](#Skewness-and-Kurtosis)  \n1.3.1 Correlation of numerical features to SalePrice  \n[Barchart: Correlation to SalePrice](#Barchart:-Correlation-to-SalePrice)  \n1.3.2 Area features  \n[Scatterplot: SalePrice vs GrLivArea](#Scatterplot:-SalePrice-vs-GrLivArea)   \n[Scatterplots: SalePrice vs Area features](#Scatterplots:-SalePrice-vs-Area-features)  \n[New feature: all_SF = sum of many area features](#New-feature:-Sum-of-many-area-features)  \n[Scatterplot: SalePrice vs all_SF](#Scatterplot:-SalePrice-vs-all_SF)  \n[Boxplot: SalePrice vs. OverallQual](#Boxplot:-SalePrice-vs.-OverallQual)  \n[Scatterplot categorical colors: SalePrice vs. all_SF and OverallQual](http:\/\/)  \n**1.4 Visualizations for categorical features**    \n[Boxplot: SalePrice for Neighborhood](#Boxplot:-SalePrice-for-Neighborhood)    \n[Boxplot: SalePrice for MSZoning](#Boxplot:-SalePrice-for-MSZoning)  \n\n","81dab73e":"**SalePrice** - the property's sale price in dollars.  \nThis is the target variable that you're trying to predict.  \n\n\n**Areas**  \n1stFlrSF: First Floor square feet  \n2ndFlrSF: Second floor square feet  \nGrLivArea: Above grade (ground) living area square feet  \nTotalBsmtSF: Total square feet of basement area  \nMasVnrArea: Masonry veneer area in square feet  \nGarageArea: Size of garage in square feet  \n\nLowQualFinSF: Low quality finished square feet (all floors)  \nBsmtFinSF1: Type 1 finished square feet  \nBsmtFinSF2: Type 2 finished square feet  \nBsmtUnfSF: Unfinished square feet of basement area  \n\nWoodDeckSF: Wood deck area in square feet  \nOpenPorchSF: Open porch area in square feet  \nEnclosedPorch: Enclosed porch area in square feet  \n3SsnPorch: Three season porch area in square feet  \nScreenPorch: Screen porch area in square feet  \nPoolArea: Pool area in square feet  \n  \n  \n**Class, Condition, Quality**  \nOverallQual: Overall material and finish quality  \nOverallCond: Overall condition rating  \nMSSubClass: The building class  \nMSZoning: The general zoning classification  \nNeighborhood: Physical locations within Ames city limits  \nBldgType: Type of dwelling  \nHouseStyle: Style of dwelling   \nFoundation: Type of foundation  \nFunctional: Home functionality rating  \n\nRoofStyle: Type of roof  \nRoofMatl: Roof material  \nExterior1st: Exterior covering on house  \nExterior2nd: Exterior covering on house (if more than one material)  \nMasVnrType: Masonry veneer type  \n  \nKitchenQual: Kitchen quality  \nExterQual: Exterior material quality  \nExterCond: Present condition of the material on the exterior  \nFireplaceQu: Fireplace quality  \n  \nPoolQC: Pool quality  \nFence: Fence quality \n\nUtilities: Type of utilities available  \nHeating: Type of heating  \nHeatingQC: Heating quality and condition  \nCentralAir: Central air conditioning  \nElectrical: Electrical system  \n\n\n**Rooms, numbers**  \nFullBath: Full bathrooms above grade  \nHalfBath: Half baths above grade  \nBedroom: Number of bedrooms above basement level  \nKitchen: Number of kitchens  \nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)  \n\nFireplaces: Number of fireplaces  \n\n\n**Lot, Street, Alley**  \nLotFrontage: Linear feet of street connected to property  \nLotArea: Lot size in square feet  \nStreet: Type of road access  \nAlley: Type of alley access  \nLotShape: General shape of property  \nLandContour: Flatness of the property  \nLotConfig: Lot configuration  \nLandSlope: Slope of property  \nCondition1: Proximity to main road or railroad  \nCondition2: Proximity to main road or railroad (if a second is present)  \nPavedDrive: Paved driveway \n\n\n**BASEMENT**  \nBsmtQual: Height of the basement  \nBsmtCond: General condition of the basement  \nBsmtExposure: Walkout or garden level basement walls  \nBsmtFinType1: Quality of basement finished area  \nBsmtFullBath: Basement full bathrooms  \nBsmtHalfBath: Basement half bathrooms  \n\n\n**Garage**  \nGarageType: Garage location  \nGarageYrBlt: Year garage was built  \nGarageFinish: Interior finish of the garage  \nGarageCars: Size of garage in car capacity    \nGarageQual: Garage quality  \nGarageCond: Garage condition  \n\n**Years**  \nYearBuilt: Original construction date  \nYearRemodAdd: Remodel date  \nMoSold: Month Sold  \nYrSold: Year Sold  \n\n \nMiscFeature: Miscellaneous feature not covered in other categories  \nMiscVal: $Value of miscellaneous feature  \n\nSaleType: Type of sale  \nSaleCondition: Condition of sale ","2181740a":"### Boxplot: SalePrice for Neighborhood","fa4bce82":"## 1.3.0 Distribution of the target","c08b5e31":"### 1.1 First look with Pandas","06d8f44c":"**For a detailed description of the 79 features**  \n**including a list of all categorical entries,** \n**see** [this file](https:\/\/www.kaggle.com\/c\/5407\/download\/data_description.txt)\n","9fa10ce7":"**loss** : {\u2018ls\u2019, \u2018lad\u2019, \u2018huber\u2019, \u2018quantile\u2019}, optional (default=\u2019ls\u2019)  \n**learning_rate** : float, optional (default=0.1)  \n**n_estimators** : int (default=100)  \n**subsample** : float, optional (default=1.0)  \n**criterion** : string, optional (default=\u201dfriedman_mse\u201d)  \n**min_samples_split** : int, float, optional (default=2)  \nIf int: minimum number. If float: fraction  \n**min_samples_leaf** : int, float, optional (default=1)   \nIf int minimum number. If float fraction   \n**min_weight_fraction_leaf** : float, optional (default=0.)  \n**max_depth** : integer, optional (default=3)\nmaximum depth of the individual regression estimators.  \n**min_impurity_decrease** : float, optional (default=0.)\n\n**max_features** : int, float, string or None, optional (default=None)  \nThe number of features to consider when looking for the best split:  \nIf float: fraction  \nIf \u201cauto\u201d, then max_features=n_features.  \nIf \u201csqrt\u201d, then max_features=sqrt(n_features).  \nIf \u201clog2\u201d, then max_features=log2(n_features).  \nIf None, then max_features=n_features.  \nChoosing max_features < n_features leads to a reduction of variance and an increase in bias.  \n","a0e62ef8":"**sklearn.pipeline**  \nThe sklearn.pipeline module implements utilities to build a composite estimator, as a chain of transforms and estimators.","3b234f26":"### Boxplot: SalePrice for MSZoning","27546a4f":"### Loop over Pipelines: Ensembles","d2b40583":"'1stFlrSF' has a correlation to SalePrice of 0.605  \n'2ndFlrSF' has a correlation to SalePrice of 0.32  \n'LowQualFinSF' has a correlation to SalePrice of 0.02  \nBy adding these three areas we get a feature that has a correlation to target of 0.709  \nIn the following we check if we can derive further useful features by adding or  \nsubtracting some of the area features.","1335af19":"### 2.3.2 Ensemble Models","252467bd":"### Loop over GridSearchCV Pipelines: Linear","b1200e34":"**sklearn.pipeline.Pipeline**  \nPipeline of transforms with a final estimator.  \nSequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be \u2018transforms\u2019, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.  \nThe purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a \u2018__\u2019, as in the example below. A step\u2019s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.","62779d8f":"**blend_3: gscv_Ridge, gscv_Lasso and gscv_ElaNet**","24be9f8e":"For the numerical columns, only three have missing values: LotFrontage, MasVnrArea and GarageYrBlt.","c09beec8":"Of the Area features, 'GrLivArea' has the largest correlation to SalePrice.","dda098e9":"#### Scatterplot: SalePrice vs GrLivArea","7e18f14a":"### Loop over Pipelines: Linear","6a342d7a":"stack1","33c765cc":"### Barchart: NaN in test and train","f15a2cb4":"### 0.2 data_description.txt","e35df169":"# Part 5: Predictions for test data","0a1ca83f":"**2: SalePrice as target**","0012c5f8":"# **House Prices: Plotly, Pipelines and Ensembles**\nThis is my second Kaggle kernel for the House Prices competition.  \nFor a more basic, beginner level study of this data set check [my first House Prices kernel.](https:\/\/www.kaggle.com\/dejavu23\/house-prices-eda-to-ml-beginner)  \nIn this kernel, I am going to explore the following a bit more advanced approaches and techniques:  \n* EDA with **Seaborn** and interactive charts with **Plotly**  \n* possible improvements by **Feature Engineering**  \n* Preprocessing using **sklearn Pipeline**    \n* use **GridSearchCV** with Pipelines     \n* apply linear models like **Ridge, Lasso, ElasticNet**   \n* and Ensemble models like **Boosting, Stacking, Voting**    \n* compare the performance of the Regression models for validation and test data","c5eca51f":"**blend_1: gscv_Ridge and gscv_Lasso**","f7d1c97b":"[**PART 2: Preprocessing and Pipelines**](#PART-2:-Preprocessing-and-Pipelines)  \n**2.0 Define data for regression models**  \n**2.1 Pipeline approach**  \n**2.2 Preproccessing Pipeline**  \nfor [numerical](#for-numerical-features) and [categorical](#for-categorical-features) features   \n[ColumnTransformer](#ColumnTransformer)  \n**2.3 Append regressors to pipeline**  \n[2.3.1 Linear Models](#2.3.1-Linear-Models)  \nLinearRegression +++ Ridge +++ Lasso +++ ElaNet  \n[2.3.2 Ensemble Models](#2.3.2-Ensemble-Models)  \nGradientBoostingregressor +++ XGB +++ LGBM +++ ADABoost  \n\n[**Part 3: Crossvalidation**](#Part-3:-Crossvalidation)  \n**3.1 Linear Models**  \n[Loop over Pipelines: Linear](#Loop-over-Pipelines:-Linear)  \n**3.2 Ensemble Models**  \n[Loop over Pipelines: Ensembles](#Loop-over-Pipelines:-Ensembles)\n\n[**Part 4: GridSearchCV**](#Part-4:-GridSearchCV)  \n**4.1 Linear Models**  \nLoop over GridSearchCV Pipelines: Linear  \n**4.2 Ensemble Models**  \nLoop over GridSearchCV Pipelines: Ensembles\n\n[**Part 5: Predictions for test data**](#Part-5:-Predictions-for-test-data)  \n\n[Stacking](#5.3-Stacking)","bda451d1":"**Note on Feature Engineering**  \nIn the data fields description it says that  \nGrLivArea: Above grade (ground) living area square feet  \nWe find that for all entries in train and test data,  \nGrLivArea is equal to the sum of the 1st and 2nd Floor square feet  \ntogether with the LowQualFinSF: ","b38cabfc":"#### Scatterplot: SalePrice vs all_SF","a6c8caa8":"# Part 3: Crossvalidation","752d6e09":"### 2.3.1 Linear Models","c93e3886":"# PART 0: Imports, info","52c38ed9":"### Loop over GridSearchCV Pipelines: Ensembles","2222767f":"### 1.3.2 Area features","272538c8":"#### describe for numerical features","ead73bd7":"The two new features are highly correlated.  \nThis multicorrelation may be a problem for some linear models.  ","2e73a8e3":"'GrLivArea' minus 'LowQualFinSF' which is equal to the sum of  \n'1stFlrSF' + '2ndFlrSF' has a larger correlation to SalePrice than 'GrLivArea'","6178fa10":"As can be expected from the large correlation coefficient of 0.796 ,  \nthere is an almost perfect linear increase of SalePrice with the OverallQual.  \nWe notice that this feature is in fact categorical (ordinal),  \nonly the discrete values 1,2..10 occur.  \nAlso there are a few outliers for some of the OverallQual values.  \nWe are dropping those that are very far from the upper fences:","a59bf83b":"**After dropping these two outliers all_SF has a correlation**  \n**to SalePrice (and also SalePriceLog) of 0.86**","1fad3455":"We now run a 5 fold cross validation for each pipeline\/model:  \nLinear Models: Linear Regression, Ridge, Lasso, Elastic Net  \nEnsembles: GBR, XGB, LGBM, ADA  \nFor this we create loops over two list of pipelines (Linear models and Ensembles) and calculate  \nthe mean, std and min score (=error) for every model.  \nBy this we get a first estimate for the different regression pipelines (Linear models and Ensembles):   \nWe fit the the data (features X and target y) using the default model parameters.","4a6505cf":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline","01fbbb51":"For 'all_SF' we further add some of the outside area values.    \nThis results in a correlation to SalePrice and also SalePriceLog of around 0.82","3b44fdf2":"Blend: Ridge + XGB","53b8f363":"In the following we check if we can improve these scores when we  \noptimize the model hyperparameters with GridSearchCV.","4bc4f24c":"#### Linear Regression","7a85ba35":"## 2.2 Preprocessing pipeline","28641607":"list of models","b33e9b2c":"**Boost Models**","c2686325":"#### Distplot for SalePrice and SalePrice_Log","655f03c1":"**Note on correlation of numerical features to SalePrice**  \nKeeping other parameters constant, we expect the value of a House to increase with its size and area.    \nAlso for this dataset, large correlations to SalePrice are observed for many of the Area features:  \nGrLivArea, GarageArea, TotalBsmtSF, 1stFlrSF, etc.   \nLets explore these in more detail and see how the results can be used for  \noutlier detection and feature engineering","46632509":"## 1.3 Visualizations for numerical features","10459ef1":"# Part 4: GridSearchCV","e098c590":"Pipelines with default model parameters","ec980e6a":"Preprocessing: Scalers","795e40e0":"### Boxplot: SalePrice vs. OverallQual","5a0a6daf":"By summming up square feet for Basement, 1st and 2nd floor. we derive  \na feature 'all_Liv_SF' that has a correlation to SalePrice of 0.78","e9ef30e9":"**Linear Models**","6e209945":"## 3.2 Ensemble Models   ","ad31f398":"As seen before in the simple xatter plot, there is a strong tendency for \nincreasing SalePrice with a higher value for OverallQual.\nBut this color plot also shows a correlation of all_SF and OverallQual.  \nSo, the probability that a house has a large area increases with its Overall Quality.  \nAnd vice versa: Quality increases with House size.  \nThis corrrelation is not necessary, one would expect that there are also small houses with high  \nquality and big houses with low quality.  \nIt would be nice to know how the rating for OverallQual is calculated or estimated, \nbut that info is not included in the data description.","4bb03e38":"**GradientBoostingRegressor**","4485b9eb":"**ElasticNet**","b9a70c25":"#### Lasso","da74f11e":"### some useful functions","bb54830e":"**blend_2: gscv_Lasso and gscv_ElaNet**","6ab5d5d7":"**1: SalePriceLog as target**","bbe0ce6a":"Pipelines with default model parameters","227efb12":"# PART 2: Preprocessing and Pipelines","6058a828":"**Indexes for the outliers are the same like for GrLivArea**","88b4a035":"### for categorical features","a7a8c5b7":"After GridSearchCV, results for Lasso and Elastic Net are much better compared to using the default parameters.  \nThe Ridge model also improves a bit, score for Linear Regression is the same as with default parameters.  \nHuber regression is not better than Ordinary least Squares for this task.","57bdc39d":"## ToDo:\n\n\n","987fa1b6":"## 4.1 Linear Models","522aea95":"from the Kaggle data overview,  \ngrouped by House and Land features as explored in this kernel","36d31724":"#### AdaBoostRegressor","165dc4cd":"**Note:**  \n**Dropping these features improves the performance of the linear regressors**","01e91dc5":"## 5.3 Stacking","9fa27f1d":"#### describe for categorical features","643ad3da":"## 0.1 data fields","f5c9122a":"#### LGBM","eb1685d8":"### Drop columns with lots of missing data","eefed445":"**LinearRegression** minimizes the residual sum of squares between the observed targets and the targets predicted by the linear approximation = Ordinary Least Squares fit  \n**Ridge** regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares.  \n**HuberRegressor** is different to Ridge because it applies a linear loss to samples that are classified as outliers.\n","e7147845":"## 1.2 Handling missing values  ","4e272b42":"### ColumnTransformer","287f02f0":"**Like for GrlivArea, there are two outliers at the lower right also for all_SF**  \n**We are going to drop these now.**","b0998886":"Rotating the 3d view reveals that:\n\n* SalePrice increases almost linearly with all_SF and OverallQual  \n* all_SF increases almost linearly with OverallQual and vice versa  \n\nIn fact, the bulk of the data follows the 45 degree line in 3 dim space.  \nThis also results in the high correlation coefficient for OverallQual and all_SF:","4b6ac537":"# PART 1: Exploratory Data Analysis","470c4fa5":"## 3.1 Linear Models","70970edb":"### for numerical features","2f63aee2":"### Boxplots for categorical features","4dd471ec":"### Scatterplot colors: SalePrice vs. all_SF and OverallQual","0471e816":"**to be continued**","724fa1c0":"Except for ADA Boost, the ensemble models with default parameters give  \nalready good results for this task.","79e5ebd8":"**alpha** :\nRegularization strength, must be a positive float  \n**fit_intercept** : bool, default True  \n**normalize** : boolean, optional, default False  \n**copy_X** : boolean, optional, default True  \n**max_iter** : int  \n**tol** : float  \nPrecision of the solution  \n**solver** : {\u2018auto\u2019, \u2018svd\u2019, \u2018cholesky\u2019, \u2018lsqr\u2019, \u2018sparse_cg\u2019, \u2018sag\u2019, \u2018saga\u2019}  \nSolver to use in the computational routines"}}