{"cell_type":{"4bcfff6c":"code","73fa7874":"code","a2d98cc5":"code","53f99f34":"code","4399ee7b":"code","9115d423":"code","127dd65b":"code","9b1d75f0":"code","5c551dd2":"code","5b46554d":"code","9360376a":"code","3cfd8a9c":"code","f3247a09":"code","50d8107a":"code","ccf77359":"code","45979c78":"code","7e2cf188":"code","456f538b":"code","2d0246dd":"code","32e4948c":"code","9ae24672":"code","fb291c18":"code","1858ffc2":"code","c2a657a0":"code","2615fcbb":"code","8e17fab2":"code","b52f7d31":"code","15617824":"code","ae79aa7a":"code","8f29ce56":"code","8e1868a2":"code","738a4762":"code","e1251ba1":"code","06f02372":"code","5a8b1487":"code","419acb24":"code","83f8fa4c":"code","bee50183":"code","ee2eaa9e":"code","c481369a":"code","9e2bf67a":"code","8f5da520":"code","815a68f5":"code","95bb78ef":"code","aa3df1f8":"code","81e03cc3":"code","8d81e8e6":"code","eed3bd3b":"code","670c2e63":"code","720f18f7":"code","00a9ca50":"code","375c341e":"code","f5a51305":"code","dc722e70":"code","036868f0":"code","b60f5c42":"code","fb97dd43":"code","f4516e8f":"code","89e8ca37":"code","98aef4bc":"code","a37ac5e6":"code","a027c121":"code","b397ee9e":"code","ac3447f3":"code","6bfc9ba4":"code","011682d4":"code","6309cf54":"code","da6e03be":"code","03d795f0":"code","1042bd38":"code","94a91ef3":"code","badc0f5c":"code","05be847a":"code","14216b67":"code","a95efa93":"code","af7e2b27":"code","21dd6c4d":"code","be67d955":"code","ce477786":"code","1b493ff1":"code","4bec8dec":"code","ea4d6d15":"code","40436f41":"code","f5dbaade":"code","c14375d0":"code","e2164085":"code","b718f992":"code","f50e5df7":"code","1a9ed416":"code","316b20f5":"code","11ea5201":"code","ed734388":"code","eee453f8":"code","ac045a3b":"code","31b8c6b0":"code","a0b570ba":"code","a2b9ea13":"code","7708f303":"code","7193122d":"code","18c430bf":"code","3bc0ae0e":"code","7f985521":"code","dfef20b9":"code","bede7e41":"code","f095c2c8":"code","7956b792":"code","90f45563":"code","de30c720":"code","4288e34c":"code","afa18923":"code","74ccc266":"code","812b6f77":"code","71df532b":"code","01ee1fde":"code","114ce3d5":"code","a0e910d5":"code","2482de91":"code","99372d68":"code","cb2dfd25":"code","3c86a910":"code","60825fd7":"code","603313eb":"code","e8816570":"code","239e7151":"code","bb967563":"code","f7b31434":"code","e2853f90":"code","69efb3f4":"code","7390ac6c":"code","0841eeca":"code","64e154e9":"code","1d6cca4c":"code","dd4e8651":"code","3ba7cfca":"code","c0864c7c":"code","7777b3f4":"code","adb31b9a":"code","0b954ed1":"code","793b3069":"code","caf21c06":"code","82f6c406":"code","00fa3467":"code","ca22aa71":"code","011452a2":"code","479a47c8":"code","e35dabe8":"code","576f8689":"code","bc0b794a":"code","5d71ab05":"code","4b4c327f":"code","2f5af421":"code","873d0c4e":"code","badb7843":"code","448fa675":"code","5c9bfbf9":"code","4f186f6f":"code","d941df08":"code","bd008f40":"code","b2a9fa30":"code","df713500":"code","bb75d0f8":"code","7523a1b6":"code","b4fa5d2d":"code","4b9af607":"code","f6929f70":"code","12e34c1a":"code","4f97d0c9":"code","2e4dd23f":"code","b17b483b":"code","3817828b":"code","14dee283":"code","617141ce":"code","1feacc9e":"code","e2e3ea67":"code","4e643e95":"code","a492d439":"code","a7efa952":"code","2abf3e53":"code","6e4b365e":"code","3ddbf5f0":"code","72c5b64d":"code","bb547c32":"code","8f40967c":"code","93dd6f24":"code","6ee95337":"code","1185087c":"code","fb2def6b":"code","3e769ee9":"code","57a20296":"code","302e68db":"code","5fc9e364":"code","312bb607":"code","75e68479":"code","8b4cdd9f":"code","0ef0bcfc":"code","d5ecae7e":"code","69a0e768":"code","94f7c109":"code","5117028c":"code","67272805":"code","80e26e28":"code","f8c55577":"code","27b503c6":"code","6b39dff6":"code","cc0b8762":"code","691d4a1f":"code","5bc0619c":"code","a959d958":"code","0ffbcc07":"markdown","1fd0c9e2":"markdown","975a1227":"markdown","21c0e793":"markdown","7922f573":"markdown","d48067fa":"markdown","bb3c468a":"markdown","608cb538":"markdown","963a69ab":"markdown","b1099171":"markdown","4d3eef71":"markdown","eea3e8b3":"markdown","8fdeccac":"markdown","9e9fbb4c":"markdown","f1d4dcd4":"markdown","5c29e130":"markdown","d83132eb":"markdown","25aeafb1":"markdown","3c7f3335":"markdown","a0d2c7ff":"markdown","ac0ba646":"markdown","8ffe199b":"markdown","fbb77666":"markdown","450a2a27":"markdown","6484e7df":"markdown","e98cf93b":"markdown","842d3f8f":"markdown","5be626d7":"markdown","e3a5857f":"markdown","d56753c1":"markdown","69c4a6a6":"markdown","c498e85b":"markdown","205529e7":"markdown","57ca99a5":"markdown","2c2c73b9":"markdown","89bef3b0":"markdown","b4afad0a":"markdown","abdc49e7":"markdown","515eddd7":"markdown","abca9540":"markdown","a9600cfa":"markdown","2eb23931":"markdown","8cea0918":"markdown","4a55f38f":"markdown","12c95326":"markdown","f301e0e1":"markdown","54c652df":"markdown","1c2e634e":"markdown","d447ffee":"markdown","39923226":"markdown","7f5ccca9":"markdown","e076f3fc":"markdown","16a37f83":"markdown","3a8ff504":"markdown","f5627288":"markdown","8542ea12":"markdown","2c0606bb":"markdown","5fc19b89":"markdown","0e859797":"markdown","9613458d":"markdown","7a977e2e":"markdown","7b0751ef":"markdown","42397ea2":"markdown","62237536":"markdown","8865ce7a":"markdown","6f48dc3d":"markdown","7c680ba4":"markdown","b999576e":"markdown","a2bd6e2d":"markdown","2f742234":"markdown","9e8c92bf":"markdown","c36f2b3e":"markdown","08d58efe":"markdown","80f995cc":"markdown","15af2fb3":"markdown","afa92be8":"markdown","1322b242":"markdown","228ccdfd":"markdown","e5b9d71e":"markdown","44603fdf":"markdown","2d8c1041":"markdown","7d445593":"markdown","60c71ef0":"markdown","d6f6d75e":"markdown","1a050a52":"markdown","2a760e75":"markdown","6c7cafc0":"markdown","c9fec975":"markdown","f586f3e9":"markdown","da50c9e1":"markdown","e44c9cfc":"markdown","7e6a922d":"markdown","00db61ed":"markdown","56f7ed50":"markdown","ef563be2":"markdown","98878c4d":"markdown","056b17a9":"markdown","1ce84e39":"markdown","8b9977d2":"markdown","54e6446c":"markdown","0e7f37c9":"markdown"},"source":{"4bcfff6c":"# Bruno Vieira Ribeiro - 2021","73fa7874":"import pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport re\nimport nltk, pprint\nfrom nltk import word_tokenize","a2d98cc5":"%config Completer.use_jedi = False        # For autocomplete to work!","53f99f34":"sns.set(rc={'figure.figsize': (10, 6)})\nsns.set_style('whitegrid')\nsns.set_palette('Set2')","4399ee7b":"df_atla = pd.read_csv('..\/input\/avatar-the-last-airbender-complete-transcript\/ATLA-episodes-scripts.csv')","9115d423":"df_atla.head()","127dd65b":"df_atla.info()","9b1d75f0":"with open('..\/input\/episode-names\/ep_names.data', 'rb') as filehandle:\n    episode_guide = pickle.load(filehandle)","5c551dd2":"def check_episode(ep_number, book):\n    '''\n    Helper function to return name of episode given the episode number and Book\n    '''\n    return episode_guide[(episode_guide['ep_number'] == ep_number)\n                         & (episode_guide['Book'] == book)].iloc[0, 0]","5b46554d":"check_episode(9, 2)","9360376a":"df_atla['Character'].value_counts()","3cfd8a9c":"# Top 10 charcaters (in number of lines)\ntop_10_characters = df_atla['Character'].value_counts()[:10]","f3247a09":"sns.barplot(x=top_10_characters.index, y=top_10_characters.values)\nplt.ylabel('Number of lines')","50d8107a":"fig = px.bar(top_10_characters, color=px.colors.qualitative.Light24[:10])\nfig.update_layout(showlegend=False,\n                  title=\"Top 10 Characters with more lines\",\n                  xaxis_title=\"Character\",\n                  yaxis_title=\"Number of lines\")\nfig.show()","ccf77359":"df_description = df_atla[df_atla['Character'].isnull()]","45979c78":"df_lines = df_atla[~df_atla['Character'].isnull()]","7e2cf188":"df_description.head()","456f538b":"df_description.tail()","2d0246dd":"# Size of the description dataframe\nlen(df_description)","32e4948c":"# Size of the lines dataframe\nlen(df_lines)","9ae24672":"print(60 * '=')\nprint('Percentage of data corresponding to charcater lines:',\n      round(100 * (len(df_lines) \/ len(df_atla)), 2), '%')\nprint()\nprint('Percentage of data corresponding to description text:',\n      round(100 * (len(df_description) \/ len(df_atla)), 2), '%')\nprint(60 * '=')","fb291c18":"description_per_book = df_description.groupby(\n    'Book')['script'].count().reset_index()","1858ffc2":"description_per_book","c2a657a0":"fig = px.bar(description_per_book,\n             color=px.colors.qualitative.Light24[:3],\n             x='Book',\n             y='script')\nfig.update_layout(showlegend=False,\n                  title=\"Number of description lines per Book\",\n                  xaxis_title=\"Book\",\n                  yaxis_title=\"Number of lines\")\nfig.show()","2615fcbb":"df_lines.head()","8e17fab2":"# Test case\ntest = df_lines.iloc[1]['script']","b52f7d31":"test","15617824":"# Regex sub() method to find and replace expression between [ ]\ntest = re.sub(\"\\[.*?\\]\", \"\", test)","ae79aa7a":"test","8f29ce56":"df_lines['script'] = df_lines['script'].apply(\n    lambda s: re.sub(\"\\[.*?\\]\", \"\", s))","8e1868a2":"df_lines.head()","738a4762":"char_seq = df_lines['Character'].tolist()","e1251ba1":"type(char_seq)","06f02372":"type(char_seq[0])","5a8b1487":"char_seq[:20]","419acb24":"char_seq_text = nltk.Text(char_seq)","83f8fa4c":"# char_seq_text.dispersion_plot(\n#         [\"Aang\", \"Katara\", \"Sokka\", \"Toph\", \"Zuko\", \"Iroh\"]\n#                     )\nchar_seq_text.dispersion_plot(list(top_10_characters.index))","bee50183":"len(df_lines)","ee2eaa9e":"df_lines.columns","c481369a":"df_lines['script_clean'] = df_lines['script'].str.lower()","9e2bf67a":"df_lines.head()","8f5da520":"df_lines['script_clean'] = df_lines['script_clean'].str.replace('[^\\w\\s]', '')","815a68f5":"df_lines.head()","95bb78ef":"nltk.download('stopwords')","aa3df1f8":"from nltk.corpus import stopwords","81e03cc3":"stop_words = set(stopwords.words('english'))","8d81e8e6":"newer_stopwords = [\"that\", \"go\", \"yeah\", \"uh\", \"oh\", \"let\", \"hey\", \"okay\"]\nnew_stopwords_list = stop_words.union(newer_stopwords)\n\nnew_stop_words = set(\n    [word for word in new_stopwords_list if word not in {'not'}])","eed3bd3b":"df_lines['script_clean'] = df_lines['script_clean'].apply(\n    lambda s: \" \".join(s for s in word_tokenize(s) if s not in new_stop_words))","670c2e63":"df_lines.head()","720f18f7":"freq = pd.Series(' '.join(df_lines['script_clean']).split()).value_counts()","00a9ca50":"freq[:20]","375c341e":"freq","f5a51305":"fig = px.bar(freq[:20])\nfig.update_layout(showlegend=False,\n                  title=\"Most frequent words\",\n                  xaxis_title=\"Word\",\n                  yaxis_title=\"Frequency\")\nfig.show()","dc722e70":"rare_words = list(freq[freq == 1].index)","036868f0":"rare_words","b60f5c42":"len(rare_words)","fb97dd43":"from nltk.stem import PorterStemmer","f4516e8f":"st = PorterStemmer()","89e8ca37":"df_lines['script_clean'] = df_lines['script_clean'].apply(\n    lambda s: ' '.join([st.stem(word) for word in word_tokenize(s)]))","98aef4bc":"df_lines.head()","a37ac5e6":"# from nltk.stem import WordNetLemmatizer","a027c121":"# nltk.download(['wordnet', 'punkt'])","b397ee9e":"# lemmatizer = WordNetLemmatizer()","ac3447f3":"# df_lines['script_clean'] = df_lines['script_clean'].apply(lambda s: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(s)]))","6bfc9ba4":"# df_lines.head()","011682d4":"from textblob import TextBlob, Word, Blobber","6309cf54":"df_lines['polarity'] = df_lines['script'].map(\n    lambda text: TextBlob(text).sentiment.polarity)","da6e03be":"df_lines['subjectivity'] = df_lines['script'].map(\n    lambda text: TextBlob(text).sentiment.subjectivity)","03d795f0":"df_lines['word_count'] = df_lines['script'].apply(\n    lambda s: len(word_tokenize(s)))","1042bd38":"df_lines.head(20)","94a91ef3":"df_lines[['word_count', 'polarity', 'subjectivity']].hist(bins=20)","badc0f5c":"df_lines[df_lines['word_count'] > 30][[\n    'word_count', 'polarity', 'subjectivity'\n]].hist(bins=20)","05be847a":"# Create a function to display distribution of word_count, polarity and subjectivity for a given character\n# and given cut-off word_count\ndef plot_character_dists(characters, cut_off_word_count=0, bins=10):\n    '''\n    Plots histograms of word_count, polarity and sensitivity for all characetrs in characters.\n    * characters is a tuple with all characters to be included.\n    * cut_off_word_count is the minimum value of word_count to be included in the statistic\n    * bins is the number of bins to plot\n    '''\n    df_lines[(df_lines['Character'].isin(characters))\n             & (df_lines['word_count'] > cut_off_word_count)][[\n                 'word_count', 'polarity', 'subjectivity'\n             ]].hist(bins=bins)","14216b67":"plot_character_dists(['Aang'], cut_off_word_count=20, bins=20)","a95efa93":"plot_character_dists(['Azula'], cut_off_word_count=20, bins=20)","af7e2b27":"# tuple(top_10_characters.index)","21dd6c4d":"plot_character_dists(tuple(top_10_characters.index),\n                     cut_off_word_count=20,\n                     bins=20)","be67d955":"df_lines.head()","ce477786":"by_character = df_lines.groupby('Character')[[\n    'polarity', 'subjectivity', 'word_count'\n]].mean().reset_index()","1b493ff1":"by_character.head()","4bec8dec":"df_lines[df_lines['Character'].str.contains(':')]","ea4d6d15":"df_lines.at[2355, 'Character'] = 'Aang'\ndf_lines.at[7144, 'Character'] = 'Sha-Mo'","40436f41":"df_lines[df_lines['Character'].str.contains(':')]","f5dbaade":"by_character = df_lines.groupby('Character')[[\n    'polarity', 'subjectivity', 'word_count'\n]].mean().reset_index()\nby_character.head()","c14375d0":"fig = px.bar(\n    by_character, x='Character', y='polarity',\n    title='Mean Polarity').update_xaxes(categoryorder=\"total descending\")\nfig.show()","e2164085":"by_character_top10 = df_lines[(df_lines['Character'].isin(\n    tuple(top_10_characters.index)))].groupby('Character')[[\n        'polarity', 'subjectivity', 'word_count'\n    ]].mean().reset_index()\nby_character_top10","b718f992":"fig = px.bar(\n    by_character_top10, x='Character', y='polarity',\n    title='Mean Polarity').update_xaxes(categoryorder=\"total descending\")\nfig.show()","f50e5df7":"fig = px.bar(\n    by_character_top10,\n    x='Character',\n    y='subjectivity',\n    title='Mean Subjectivity').update_xaxes(categoryorder=\"total descending\")\nfig.show()","1a9ed416":"fig = px.bar(\n    by_character_top10, x='Character', y='word_count',\n    title='Mean Word Count').update_xaxes(categoryorder=\"total descending\")\nfig.show()","316b20f5":"fig = px.histogram(df_lines[df_lines['Character'] == 'Aang'], x=\"word_count\")\nfig.show()","11ea5201":"df_lines[(df_lines['Character'] == 'Aang') & (df_lines['word_count'] > 180)]","ed734388":"df_lines[(df_lines['Character'] == 'Aang')\n         & (df_lines['word_count'] > 180)].iloc[0, 1]","eee453f8":"aang_lines = len(df_lines[(df_lines['Character'] == 'Aang')\n                          & (df_lines['word_count'] < 180)])\nfig = px.histogram(df_lines[(df_lines['Character'] == 'Aang')\n                            & (df_lines['word_count'] < 180)],\n                   x=\"word_count\",\n                   title=f'Aang - {aang_lines} lines')\nfig.show()","ac045a3b":"azula_lines = len(df_lines[df_lines['Character'] == 'Azula'])\nfig = px.histogram(df_lines[df_lines['Character'] == 'Azula'],\n                   x=\"word_count\",\n                   title=f'Azula - {azula_lines} lines')\nfig.show()","31b8c6b0":"# fig = px.histogram(df_lines[(df_lines['Character']=='Aang') & (df_lines['word_count']>20)], x=\"polarity\")\n# fig.show()","a0b570ba":"fig = px.line(df_lines[df_lines['Character'] == 'Aang'], y=\"polarity\")\nfig.show()","a2b9ea13":"# aang_script = {'Book1': df_lines[(df_lines['Character']=='Aang') & (df_lines['Book']==1)],\n#                'Book2': df_lines[(df_lines['Character']=='Aang') & (df_lines['Book']==2)],\n#                'Book3': df_lines[(df_lines['Character']=='Aang') & (df_lines['Book']==3)]}","7708f303":"# aang_mean_values = pd.concat([\n#             aang_script['Book1'].groupby('ep_number')[['Book','polarity','subjectivity','word_count']].mean().reset_index(),\n#             aang_script['Book2'].groupby('ep_number')[['Book','polarity','subjectivity','word_count']].mean().reset_index(),\n#             aang_script['Book3'].groupby('ep_number')[['Book','polarity','subjectivity','word_count']].mean().reset_index()\n#                    ]).reset_index(drop=True)","7193122d":"aang_mean_values = df_lines[(\n    df_lines['Character'] == 'Aang')].groupby('total_number')[[\n        'ep_number', 'Book', 'polarity', 'subjectivity', 'word_count'\n    ]].mean().reset_index()\n\naang_mean_values.head()","18c430bf":"fig = px.line(aang_mean_values, y=\"polarity\", hover_data=['ep_number', 'Book'])\nfig.show()","3bc0ae0e":"check_episode(15, 3)","7f985521":"df_lines[(df_lines['Character'] == 'Aang') & (df_lines['ep_number'] == 15) &\n         (df_lines['Book'] == 3)]['script']","dfef20b9":"def get_mean_per_episode(character):\n    '''\n    Returns the evolution of mean numeric values by episode for a given character\n    '''\n    char_mean_values = df_lines[(\n        df_lines['Character'] == character)].groupby('total_number')[[\n            'ep_number', 'Book', 'polarity', 'subjectivity', 'word_count'\n        ]].mean().reset_index()\n\n    return char_mean_values","bede7e41":"top_10_mean_values = {}\nfor char in top_10_characters.index:\n    top_10_mean_values[char] = get_mean_per_episode(char)","f095c2c8":"# with open('.\/data\/top_mean_values.data', 'wb') as filehandle:\n#     # store the data as binary data stream\n#     pickle.dump(top_10_mean_values, filehandle)","7956b792":"top_10_mean_values['Zuko']","90f45563":"fig = px.line(top_10_mean_values['Zuko'],\n              y=\"polarity\",\n              hover_data=['ep_number', 'Book'])\nfig.show()","de30c720":"print(check_episode(14, 2))\nprint(check_episode(10, 3))\nprint(check_episode(20, 3))","4288e34c":"def get_character_lines(char, episode, book):\n    '''\n    Return a pandas dataframe with info on a given list of characters (char) for given book and episode\n    '''\n    return df_lines[(df_lines['Character'].isin(char))\n                    & (df_lines['Book'] == book) &\n                    (df_lines['ep_number'] == episode)]\n\n\ndef get_character_lines_episode(char, episode):\n    '''\n    Return a pandas dataframe with info on a given list of characters (char) for given episode number (1 to 61)\n    '''\n    return df_lines[(df_lines['Character'].isin(char))\n                    & (df_lines['total_number'] == episode)]","afa18923":"# First low point:\nget_character_lines(['Zuko'], 14, 2)","74ccc266":"# Second low point:\nget_character_lines(['Zuko'], 10, 3)","812b6f77":"# Third low point:\nget_character_lines(['Zuko'], 20, 3)","71df532b":"fig = go.Figure()\n\nfor char in top_10_characters.index:\n\n    fig.add_scatter(x=top_10_mean_values[char]['total_number'],\n                    y=top_10_mean_values[char]['polarity'],\n                    hovertext=list(\n                        zip(top_10_mean_values[char]['ep_number'],\n                            top_10_mean_values[char]['Book'])),\n                    name=char)\nfig.update_layout(\n    title=\"Evolution of polarity - Top 10 Characters with more lines\",\n    xaxis_title=\"Episode number (in entire series)\",\n    yaxis_title=\"Mean polarity of lines\",\n    hovermode=\"x unified\")\n\nfig.add_vrect(\n    # Add background color for Book 1\n    x0=1, x1=20,\n    fillcolor=\"LightBlue\", opacity=0.5,\n    layer=\"below\", line_width=0,\n    annotation_text='Book 1', annotation_position=\"top left\",\n    annotation_font_color = \"Blue\", annotation_font_size=16\n)\n\nfig.add_vrect(\n    # Add background color for Book 2\n    x0=20, x1=40,\n    fillcolor=\"LightGreen\", opacity=0.5,\n    layer=\"below\", line_width=0,\n    annotation_text='Book 2', annotation_position=\"top left\",\n    annotation_font_color = \"DarkGreen\", annotation_font_size=16\n)\n\nfig.add_vrect(\n    # Add background color for Book 3\n    x0=40, x1=64,\n    fillcolor=\"Red\", opacity=0.2,\n    layer=\"below\", line_width=0,\n    annotation_text='Book 3', annotation_position=\"top left\",\n    annotation_font_color = \"crimson\", annotation_font_size=16\n)\n\nfig.show()","01ee1fde":" check_episode(7,2)","114ce3d5":"get_character_lines(['Iroh'], 7, 2)","a0e910d5":"get_character_lines(['Iroh'], 7, 2).iloc[0,1]","2482de91":" check_episode(5,3)","99372d68":"get_character_lines(['Sokka'], 5, 3)","cb2dfd25":"get_character_lines(['Sokka'], 5, 3).iloc[0,1]","3c86a910":"def plot_top10_evolution(attribute, ymax = 1):\n    fig = go.Figure()\n\n    for char in top_10_characters.index:\n\n        fig.add_scatter(x = top_10_mean_values[char]['total_number'],\n                        y = top_10_mean_values[char][attribute],\n                        hovertext=list(zip(top_10_mean_values[char]['ep_number'],\n                                      top_10_mean_values[char]['Book'])),\n                        name = char)\n    fig.update_layout(\n        title=\"Evolution of \"+ attribute +\" - Top 10 Characters with more lines\",\n        xaxis_title=\"Episode number (in entire series)\",\n        yaxis_title=\"Mean \"+attribute+\" of lines\"\n    )\n    fig.update_layout(hovermode=\"x unified\")\n\n    fig.add_vrect(\n        # Add background color for Book 1\n        x0=1, x1=20,\n        fillcolor=\"LightBlue\", opacity=0.5,\n        layer=\"below\", line_width=0,\n        annotation_text='Book 1', annotation_position=\"top left\",\n        annotation_font_color = \"Blue\", annotation_font_size=16\n    )\n\n    fig.add_vrect(\n        # Add background color for Book 2\n        x0=20, x1=40,\n        fillcolor=\"LightGreen\", opacity=0.5,\n        layer=\"below\", line_width=0,\n        annotation_text='Book 2', annotation_position=\"top left\",\n        annotation_font_color = \"DarkGreen\", annotation_font_size=16\n    )\n\n    fig.add_vrect(\n        # Add background color for Book 3\n        x0=40, x1=64,\n        fillcolor=\"Red\", opacity=0.2,\n        layer=\"below\", line_width=0,\n        annotation_text='Book 3', annotation_position=\"top left\",\n        annotation_font_color = \"crimson\", annotation_font_size=16\n    )\n\n\n    fig.show()","60825fd7":"plot_top10_evolution('subjectivity')","603313eb":"check_episode(20,3)","e8816570":"get_character_lines(['Iroh'], 20, 3)","239e7151":"get_character_lines(['Iroh'], 20, 3).iloc[0,1]","bb967563":"get_character_lines(['Sokka'], 5, 3).iloc[0,1]","f7b31434":"get_character_lines(['Aang'], 15, 3).iloc[0,1]","e2853f90":"plot_top10_evolution('word_count')","69efb3f4":"check_episode(6, 3)","7390ac6c":"get_character_lines(['Azula'], 6, 3)","0841eeca":"for i in range(3):\n    print(f'Line {i+1}:')\n    print(get_character_lines(['Azula'], 6, 3).iloc[i,1])\n    print()","64e154e9":"df_lines_top10 = df_lines[( df_lines['Character'].isin(tuple(top_10_characters.index)) )]","1d6cca4c":"fig = px.box(df_lines_top10, x=\"Character\", y=\"polarity\",\n            color='Character')\nfig.show()","dd4e8651":"fig = px.box(df_lines_top10, x=\"Character\", y=\"subjectivity\",\n            color='Character')\nfig.show()","3ba7cfca":"fig = px.box(df_lines_top10, x=\"Character\", y=\"word_count\",\n            color='Character')\nfig.show()","c0864c7c":"fig = px.box(df_lines_top10, x=\"Book\", y=\"polarity\",\n            color='Book')\nfig.show()","7777b3f4":"fig = px.box(df_lines_top10, x=\"Book\", y=\"subjectivity\",\n            color='Book')\nfig.show()","adb31b9a":"fig = px.box(df_lines_top10, x=\"Book\", y=\"word_count\",\n            color='Book')\nfig.show()","0b954ed1":"fig = px.scatter(df_lines, x=\"subjectivity\", y=\"polarity\")\nfig.show()","793b3069":"fig = px.scatter(df_lines_top10, x=\"subjectivity\", y=\"polarity\",\n                 color='Character', size='word_count',\n                 title='Top 10 characters with more lines')\nfig.show()","caf21c06":"fig = px.scatter(df_lines_top10, x=\"polarity\", y=\"word_count\",\n                 color='Character',\n                 title='Top 10 characters with more lines')\nfig.show()","82f6c406":"df_lines_top10.head()","00fa3467":"lines_by_episode = df_lines_top10.groupby('total_number')[['polarity','subjectivity','word_count', 'ep_number', 'Book']].mean().reset_index()","ca22aa71":"lines_by_episode['ep_name'] = lines_by_episode.apply(lambda d: check_episode(d['ep_number'], d['Book']), axis=1)","011452a2":"lines_by_episode.head()","479a47c8":"lines_by_episode.sort_values(by='polarity', ascending=False)[:10]","e35dabe8":"fig = px.bar(lines_by_episode.sort_values(by='polarity', ascending=False)[:10].reset_index(),\n             y = 'polarity',\n             text='ep_name',\n             color=px.colors.qualitative.Light24[:10])\nfig.update_layout(\n    showlegend=False,\n    title=\"Top 10 episodes by mean polarity - Only Top 10 characters with more lines\",\n    xaxis_title=\"Rank\",\n    yaxis_title=\"Mean polarity\"\n)\nfig.show()","576f8689":"all_lines_by_episode = df_lines.groupby('total_number')[['polarity','subjectivity','word_count', 'ep_number', 'Book']].mean().reset_index()\n\nall_lines_by_episode['ep_name'] = all_lines_by_episode.apply(lambda d: check_episode(d['ep_number'], d['Book']), axis=1)\n\nfig = px.bar(all_lines_by_episode.sort_values(by='polarity', ascending=False)[:10].reset_index(),\n             y = 'polarity',\n             text='ep_name',\n             color=px.colors.qualitative.Light24[:10])\nfig.update_layout(\n    showlegend=False,\n    title=\"Top 10 episodes by mean polarity - including all characters\",\n    xaxis_title=\"Rank\",\n    yaxis_title=\"Mean polarity\"\n)\nfig.show()","bc0b794a":"# episode_guide[episode_guide['Episode']==\"Appa's Lost Days\"]","5d71ab05":"# get_character_lines(tuple(top_10_characters.index), 16,2)","4b4c327f":"def plot_episode_rank(df, attribute, top=10):\n    '''\n    * Plots a bar plot for the 'attribute' value of each episode in df.\n    * 'df' has to have a column named 'atribute' and one named 'ep_name' for text entry\n    * Value of 'top' determines how many episodes to rank\n    '''\n    fig = px.bar(df.sort_values(by=attribute, ascending=False)[:top].reset_index(),\n             y = attribute,\n             text='ep_name',\n             color=px.colors.qualitative.Light24[:top])\n    fig.update_layout(\n        showlegend=False,\n        title=\"Top 10 episodes by mean \"+ attribute,\n        xaxis_title=\"Rank\",\n        yaxis_title=\"Mean \"+attribute\n    )\n    fig.show()","2f5af421":"plot_episode_rank(lines_by_episode, 'subjectivity')","873d0c4e":"plot_episode_rank(all_lines_by_episode, 'subjectivity')","badb7843":"plot_episode_rank(lines_by_episode, 'word_count')","448fa675":"plot_episode_rank(all_lines_by_episode, 'word_count')","5c9bfbf9":"Aang_text = \" \".join(line for line in df_lines_top10[df_lines_top10['Character']=='Aang'].script.str.lower())","4f186f6f":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","d941df08":"stopwords = set(STOPWORDS)\nstopwords = stopwords.union([\"go\",\"yeah\", \"uh\", \"oh\", \"let\", \"hey\", \"okay\"])","bd008f40":"wordcl = WordCloud(stopwords = stopwords, background_color='white', max_font_size = 50, max_words = 2000).generate(Aang_text)\nplt.figure(figsize=(14, 12))\nplt.imshow(wordcl, interpolation='bilinear')\nplt.title(\"Aang's WordCloud\",\n              pad=10.0,\n              color='Blue',\n              fontdict={\n                  'fontsize':30\n              })\nplt.axis('off')\nplt.show()","b2a9fa30":"def character_wordcloud(char, max_w=2000):\n    '''\n    Plot a character's (char) wordcloud with a max of 2000 words by default.\n    '''\n    text = \" \".join(line for line in df_lines[df_lines['Character']==char].script.str.lower())\n    \n    wordcl = WordCloud(stopwords = stopwords, background_color='white', max_font_size = 50, max_words = max_w).generate(text)\n    plt.figure(figsize=(14, 12))\n    plt.imshow(wordcl, interpolation='bilinear')\n    plt.title(char+\"'s WordCloud\",\n              pad=10.0,\n              color='Blue',\n              fontdict={\n                  'fontsize':30\n              })\n    plt.axis('off')\n    plt.show()","df713500":"# def character_wordcloud_obj(char, max_w=2000):\n#     '''\n#     Return a figure of a character's (char) wordcloud with a max of 2000 words by default.\n#     '''\n#     text = \" \".join(line for line in df_lines[df_lines['Character']==char].script.str.lower())\n    \n#     wordcl = WordCloud(stopwords = stopwords, background_color='white', max_font_size = 50, max_words = max_w).generate(text)\n#     fig, ax = plt.subplots()\n    \n# #     plt.figure(figsize=(14, 12))\n#     ax.imshow(wordcl, interpolation='bilinear')\n    \n#     ax.axis('off')\n#     plt.title(char+\"'s WordCloud\",\n#               pad=10.0,\n#               color='Black',\n#               fontdict={\n#                   'fontsize':30\n#               })\n    \n#     fig.savefig('.\/wordclouds\/'+char+'.png')\n    \n#     return fig","bb75d0f8":"# for char in top_10_characters.index:\n#     character_wordcloud_obj(char)","7523a1b6":"character_wordcloud('Katara')","b4fa5d2d":"character_wordcloud('Zuko')","4b9af607":"character_wordcloud('Iroh')","f6929f70":"from nltk import FreqDist","12e34c1a":"freq = FreqDist(sum(df_lines['script_clean'].map(word_tokenize), []))","4f97d0c9":"freq.plot(20, cumulative=True);","2e4dd23f":"print(f'Lexical diversity: len(freq)')","b17b483b":"freq_df = pd.DataFrame(list(freq.items()), columns = [\"Word\",\"Frequency\"])","3817828b":"freq_df = freq_df.sort_values(by='Frequency', ascending=False)","14dee283":"freq_df.head(20)","617141ce":"len(freq_df)","1feacc9e":"fig = px.bar(freq_df[:20], x = 'Word', y = 'Frequency',\n             color=px.colors.qualitative.Light24[:20])\nfig.update_layout(\n    showlegend=False,\n    title=\"Top 20 more frequent words in character lines\",\n    xaxis_title=\"Word\",\n    yaxis_title=\"Frequency\"\n)\nfig.show()","e2e3ea67":"def character_word_freq_plot(char, top=20):\n    '''\n    Plot top 'top' more frequent words for character 'char'\n    '''\n    freq = FreqDist(sum(df_lines[df_lines['Character']==char]['script_clean'].map(word_tokenize), []))\n    freq_df = pd.DataFrame(list(freq.items()), columns = [\"Word\",\"Frequency\"])\n    freq_df = freq_df.sort_values(by='Frequency', ascending=False)\n    fig = px.bar(freq_df[:top], x = 'Word', y = 'Frequency',\n             color=px.colors.qualitative.Light24[:top])\n    fig.update_layout(\n        showlegend=False,\n        title=f\"Top {top} more frequent words for {char}\",\n        xaxis_title=\"Word\",\n        yaxis_title=\"Frequency\"\n    )\n    fig.show()","4e643e95":"character_word_freq_plot('Aang')","a492d439":"character_word_freq_plot('Zuko')","a7efa952":"character_word_freq_plot('Azula')","2abf3e53":"def character_bigram_plot(char, top=20):\n    '''\n    Plot top 'top' more frequent bigrams for character 'char'\n    '''\n    finder = nltk.collocations.BigramCollocationFinder.from_words(sum(df_lines[df_lines['Character']==char]['script_clean'].map(word_tokenize), []))\n    common = finder.ngram_fd.most_common(top)\n    \n    bigram_list = [(' '.join(el[0]), el[1]) for el in common]\n    \n    freq_df = pd.DataFrame(bigram_list, columns = [\"Bigram\",\"Frequency\"])\n    freq_df = freq_df.sort_values(by='Frequency', ascending=False)\n    fig = px.bar(freq_df[:20], x = 'Bigram', y = 'Frequency',\n             color=px.colors.qualitative.Light24[:20])\n    fig.update_layout(\n        showlegend=False,\n        title=f\"Top {top} more frequent bigrams for {char}\",\n        xaxis_title=\"Bigram\",\n        yaxis_title=\"Frequency\"\n    )\n    fig.show()","6e4b365e":"character_bigram_plot('Aang')","3ddbf5f0":"character_bigram_plot('Katara')","72c5b64d":"character_bigram_plot('Zuko')","bb547c32":"character_bigram_plot('Toph')","8f40967c":"df_lines.head()","93dd6f24":"def word_diversity(char):\n    '''\n    Return the lexical diversity (an integer number) of a given character (char)\n    '''\n    text = sum(df_lines[df_lines['Character']==char]['script_clean'].map(word_tokenize), [])\n    return len(set(text))","6ee95337":"word_diversity('Aang')","1185087c":"word_div_by_char = []\n\nfor char in top_10_characters.index:\n    word_div_by_char.append( (char , word_diversity(char)) )","fb2def6b":"word_div_by_char","3e769ee9":"word_df = pd.DataFrame(word_div_by_char, columns=['Character', 'word_diversity'])\nword_df","57a20296":"# with open('.\/data\/word_df.data', 'wb') as filehandle:\n#     # store the data as binary data stream\n#     pickle.dump(word_df, filehandle)","302e68db":"word_df[word_df.Character == 'Sokka']['word_diversity'].iloc[0]","5fc9e364":"fig = px.bar(word_df.sort_values(by='word_diversity', ascending=False),\n             x = 'Character', y = 'word_diversity',\n             color=px.colors.qualitative.Light24[:10])\nfig.update_layout(\n    showlegend=False,\n    title=\"Word diversity for top 10 characters\",\n    xaxis_title=\"Character\",\n    yaxis_title=\"Word Diversity\"\n)\nfig.show()","312bb607":"def lexical_diversity(char):\n    '''\n    Return the lexical diversity (netween 0 and 1) of a given character (char)\n    '''\n    text = sum(df_lines[df_lines['Character']==char]['script_clean'].map(word_tokenize), [])\n    return len(set(text))\/len(text)","75e68479":"lexical_diversity('Aang')","8b4cdd9f":"lexical_div_by_char = []\n\nfor char in top_10_characters.index:\n    lexical_div_by_char.append( (char , lexical_diversity(char)) )\n    \nlexical_df = pd.DataFrame(lexical_div_by_char, columns=['Character', 'lexical_diversity'])\nlexical_df","0ef0bcfc":"fig = px.bar(lexical_df.sort_values(by='lexical_diversity', ascending=False),\n             x = 'Character', y = 'lexical_diversity',\n             color=px.colors.qualitative.Light24[:10])\nfig.update_layout(\n    showlegend=False,\n    title=\"Lexical diversity for top 10 characters\",\n    xaxis_title=\"Character\",\n    yaxis_title=\"Lexical Diversity\"\n)\nfig.show()","d5ecae7e":"def word_diversity_book(book):\n    '''\n    Return the word diversity (an integer number) of a given book (char)\n    '''\n    text = sum(df_lines[df_lines['Book']==book]['script_clean'].map(word_tokenize), [])\n    return len(set(text))","69a0e768":"word_div_by_book = []\n\nfor i in [1,2,3]:\n    word_div_by_book.append( (i , word_diversity_book(i)) )\n    \nword_df_book = pd.DataFrame(word_div_by_book , columns=['Book', 'word_diversity'])\nword_df_book","94f7c109":"fig = px.bar(word_df_book.sort_values(by='word_diversity', ascending=False),\n             x = 'Book', y = 'word_diversity',\n             color=px.colors.qualitative.Light24[:3])\nfig.update_layout(\n    showlegend=False,\n    title=\"Word diversity for all Books\",\n    xaxis_title=\"Book\",\n    yaxis_title=\"Word Diversity\"\n)\nfig.show()","5117028c":"df_lines.head()","67272805":"def get_word_div_per_episode(char):\n    '''\n    Returns the evolution of word diversity by episode for a given character\n    '''\n    \n    word_div_episode = []\n    for i in df_lines['total_number'].unique():\n        text = sum(df_lines[(df_lines['Character']==char) & (df_lines['total_number']==i)]['script_clean'].map(word_tokenize), [])\n        word_div_episode.append((i, len(set(text))))\n        \n    df = pd.DataFrame(word_div_episode, columns=['episode', 'word_diversity'])        \n\n    return df","80e26e28":"aangs_word_div = get_word_div_per_episode('Aang')","f8c55577":"aangs_word_div","27b503c6":"fig = go.Figure()\n\nfor char in top_10_characters.index:\n\n    fig.add_scatter(x = get_word_div_per_episode(char)['episode'],\n                    y = get_word_div_per_episode(char)['word_diversity'],\n                    hovertext=list(zip(get_word_div_per_episode(char)['episode'],\n                                  get_word_div_per_episode(char)['word_diversity'])),\n                    name = char)\nfig.update_layout(\n    title=\"Evolution of word diversity - Top 10 Characters with more lines\",\n    xaxis_title=\"Episode number (in entire series)\",\n    yaxis_title=\"Word diversity of lines\",\n    hovermode=\"x unified\"\n    )\n\nfig.add_vrect(\n    # Add background color for Book 1\n    x0=1, x1=20,\n    fillcolor=\"LightBlue\", opacity=0.5,\n    layer=\"below\", line_width=0,\n)\n\nfig.add_vrect(\n    # Add background color for Book 2\n    x0=20, x1=40,\n    fillcolor=\"LightGreen\", opacity=0.5,\n    layer=\"below\", line_width=0,\n)\n\nfig.add_vrect(\n    # Add background color for Book 3\n    x0=40, x1=64,\n    fillcolor=\"Red\", opacity=0.2,\n    layer=\"below\", line_width=0,\n)\n\nfig.add_annotation(x=4, y=300,\n            text=\"Book 1\",\n            showarrow=False,\n            yshift=10,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=16,\n                color=\"Blue\"\n            )\n                  )\nfig.add_annotation(x=30, y=300,\n            text=\"Book 2\",\n            showarrow=False,\n            yshift=10,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=16,\n                color=\"DarkGreen\"\n            )\n                  )\nfig.add_annotation(x=50, y=300,\n            text=\"Book 3\",\n            showarrow=False,\n            yshift=10,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=16,\n                color=\"crimson\"\n            )\n                  )\n\n\nfig.show()","6b39dff6":"get_character_lines_episode(['Zuko'], 53)","cc0b8762":"check_episode(13,3)","691d4a1f":"def get_lex_div_per_episode(char):\n    '''\n    Returns the evolution of lexical diversity by episode for a given character\n    '''\n    \n    lex_div_episode = []\n    for i in df_lines['total_number'].unique():\n        text = sum(df_lines[(df_lines['Character']==char) & (df_lines['total_number']==i)]['script_clean'].map(word_tokenize), [])\n        try:\n            lex_div_episode.append( (i, len(set(text))\/len(text)) )\n        except:\n            lex_div_episode.append( (i, 0) )\n        \n    df = pd.DataFrame(lex_div_episode, columns=['episode', 'lexical_diversity'])        \n\n    return df","5bc0619c":"fig = go.Figure()\n\nfor char in top_10_characters.index:\n\n    fig.add_scatter(x = get_lex_div_per_episode(char)['episode'],\n                    y = get_lex_div_per_episode(char)['lexical_diversity'],\n                    hovertext=list(zip(get_lex_div_per_episode(char)['episode'],\n                                  get_lex_div_per_episode(char)['lexical_diversity'])),\n                    name = char)\nfig.update_layout(\n    title=\"Evolution of lexical diversity - Top 10 Characters with more lines\",\n    xaxis_title=\"Episode number (in entire series)\",\n    yaxis_title=\"Lexical diversity of lines\",\n    hovermode=\"x unified\"\n    )\n\nfig.add_vrect(\n    # Add background color for Book 1\n    x0=1, x1=20,\n    fillcolor=\"LightBlue\", opacity=0.5,\n    layer=\"below\", line_width=0,\n)\n\nfig.add_vrect(\n    # Add background color for Book 2\n    x0=20, x1=40,\n    fillcolor=\"LightGreen\", opacity=0.5,\n    layer=\"below\", line_width=0,\n)\n\nfig.add_vrect(\n    # Add background color for Book 3\n    x0=40, x1=64,\n    fillcolor=\"Red\", opacity=0.2,\n    layer=\"below\", line_width=0,\n)\n\nfig.add_annotation(x=4, y=1,\n            text=\"Book 1\",\n            showarrow=False,\n            yshift=10,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=16,\n                color=\"Blue\"\n            )\n                  )\nfig.add_annotation(x=30, y=1,\n            text=\"Book 2\",\n            showarrow=False,\n            yshift=10,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=16,\n                color=\"DarkGreen\"\n            )\n                  )\nfig.add_annotation(x=50, y=1,\n            text=\"Book 3\",\n            showarrow=False,\n            yshift=10,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=16,\n                color=\"crimson\"\n            )\n                  )\n\n\nfig.show()","a959d958":"# with open('.\/data\/df_lines.data', 'wb') as filehandle:\n#     # store the data as binary data stream\n#     pickle.dump(df_lines, filehandle)","0ffbcc07":"## Converting to lower case","1fd0c9e2":"Let's check Zuko's polarity evolution, as he is very known to have a brilliant character development.","975a1227":"Ok, so it seems polarity is way more spread out for higher values of subjectivity. It makes sense, as objective sentences tend to have a very neutral polarity.\n\nLet's rebuild this graph for the top 10 characters and show size of markers according to the word count for each character.","21c0e793":"## Subjectivity\n\nLet's start with only the top 10 characters with more lines:","7922f573":"## Lexical diversity\n\nSimilarly, we can create a function and a visual for the lexical diversity (fraction of different words by the total number of words) for each character.","d48067fa":"Check out how Zuko has the more spread out distribution among these characters.\n\n---\n\nNow for `subjectivity` and `word_count`:\n","bb3c468a":"The lowest mean polarity happens for Sokka in book 3, episode 5:","608cb538":"Not surprisingly, Iroh tops the list as the most **positive** character in average. He is followed by Azula, which makes sense as she is a very sure and calm character (you now, besides being super evil and before having her break down by the end...). Again, not surprisingly, Zuko stands out as the most negative out of these characters, in average.\n\nWe can do the same for `Subjectivity` and `Word Count`.","963a69ab":"Ok, so our `char_seq` object is a list of strings containing names of Characters as they have lines in the show. We want to turn this into a `Text` object from nltk:","b1099171":"## Word frequency\n\nWe have enough to check for the most common words in the show before digging deep in indivudal characters.","4d3eef71":"## Word count\n\nFor top 10 characters:","eea3e8b3":"For quick check, let's load our episode number \"guide\":","8fdeccac":"Ok, this is messy (even though we can see some patterns). We can try to see the mean value of polarity for each episode.\n\nLet's try a simple approach:\n* Select all of Aang's lines\n* Group the df by `total_number`\n* Get the mean of all numeric values.\n\nThis can be done in one line, as seen bellow:","9e9fbb4c":"If you've watched the show you'd probably be surprised that Aang even has a line in this episode, as it is focused mainly on Sokka and Zuko. So, let's try to get Aang's lines in this episode:","f1d4dcd4":"Let's check how many description lines we have per `Book`.","5c29e130":"The only character to have over 40 words on average in a single episode is Azula on book 3, episode 6. Let's inspect:","d83132eb":"# Frequency distributions\n\nLet's revisit the frequency distributions using `nltk`. The goal here is to inspect these distributions per character.","25aeafb1":"What's going on with that one episode with super low polarity? You can check the episode's number and book by hovering over it (thanks plotly!):","3c7f3335":"## Removing stopwords\n\nWe'll use `nltk`'s stopwords to remove all english stopwords from character lines.","a0d2c7ff":"# Preprocessing the character lines","ac0ba646":"# Bigram Distribution","8ffe199b":"Before turning this into a dataframe, we can inspect the distribution of these most common words:","fbb77666":"Let's check out the **rare** words (the ones that appear only once):","450a2a27":"# Description dataset\n\nInfo on the description dataframe:","6484e7df":"Let's create a dataframe with the mean values of `polarity`,`subjectivity`,`word_count` (I'll use `ep_number` and `Book` just to simplify plotting later) grouped by episode. This way we can sort values by a given column and rank episodes accordingly.","e98cf93b":"And for all characters:","842d3f8f":"Doing `word_count` vs. `polarity` we can better see that \"neutral\" sentences (polarity close to 0) have higher word counts.","5be626d7":"To better explore different characetrs let's group by `Character` and inspect our newly created features.","e3a5857f":"Ok, this makes a lot of sense, when Azula talks she tends to be very wordy (maybe a way to gatter as much attention as she can...). Aang is the charcater with more lines, so he may have a lot of small lines bringing hes mean word count down. We can see this by looking at his word count distribution:","d56753c1":"# Top episodes with highest means\n\nLet's inspect the mean values for `polarity`, `subjectivity` and `word_count` for episodes and rank them.\n\n## Polarity","69c4a6a6":"Zuko's polarity seems to have a tendency to get lower as the show progresses. He does go through a bit of a transformation and lots of hardship. Let's use the hover data to see his three major low points:","c498e85b":"Cool! We can carry on!","205529e7":"Let's put this plotting into a function for easy call:","57ca99a5":"This is a frist intersting visual on the distribution of lines in the show. For example, we can see that Zuko has a much denser distribution of lines by the end of the show (when he joins team Avatar!).\n\nLet's just see how many total lines the show has:","2c2c73b9":"Now to create neat boxplots for distributions of polarity:","89bef3b0":"Now to lemmatize:","b4afad0a":"We can quickly get rid of these:","abdc49e7":"Using only words that appear more than 30 times:","515eddd7":"This line has a `subjectivity` of 0.95. It is closely followed by Sokka on episode 5 in the same book and Aang on episde 15. Both of these have a subjectivity of 0.9. Let's chek them out:","abca9540":"Iroh, again, tops the list as the least objective character. To understand this, let's see a quote from Zuko's impersonation of Iroh:\n> \"Zuko, you must look within yourself to save yourself from your other self. Only then will your true self reveal itself.\"\n\nSo, yeah, he's not the most objective guy when talking (but surely is wise!).\nJet also stands out as the most obejctive of all top 10 characters. He does stand out to me as one with few words but a very straight-to-the-point guy, so seems right.\n\nNow onto the word count stats:","a9600cfa":"#### Word_count","2eb23931":"We can get our first visual on the top 10 characters with more lines in the show:\n\nFirst, we select the top 10 values from the `value_counts()` function.","8cea0918":"Now, we can create a list of tuples to store these values for each of our top 10 characters:","4a55f38f":"# Wordclouds\n\nWe can create wordclouds to get a pretty visual on character lines. We'll start by picking a single character and collecting all its lines for our wordcloud.\n\nAs usual, let's start with Aang:","12c95326":"So, this is during the invasion, when he leaves his home and a letter to Mai explaining his choice. He was really apologetic and kind of sad for Mai, so makes sense to have a low polarity for this one too.","f301e0e1":"# Importing and first look at the data\n\nLet's get our dataframe ready:","54c652df":"## Distributions for Books\n\nWe can see these same distributions per Book:\n","1c2e634e":"We can build a function to get top more frequent words by character:","d447ffee":"We will work, mainly, with the `lines` dataframe, so let's start our preprocessing there.","39923226":"That's it! Aang's entire script in this episode is that one sentence about how crazy Sokka and Zuko's trip was. So, it shouldn't be taken as an episode of Aang being super negative...\n\n---\n### Polarity by episode for given character\n\nNow, can we get this into a function for any given character? If so, can we create a dictionary with this dataframe for all top 10 characters with more lines?","7f5ccca9":"Now, to turn this into a df object and use it to get an interactive visual with plotly.","e076f3fc":"Excluding this one, let's look at Aang's distribution of word count:","16a37f83":"## Polarity across episodes\n\nFor a given character let's see if we can get a nice visual on the evolution of hers(his) polarity across episodes.\n\nWe can start with Aang:","3a8ff504":"## Stemming (and Lemmatization)\n\nLet's start with stemming using the PorterStemmer from `nltk`:","f5627288":"Let's start by checking the value counts for Characters:","8542ea12":"Check if it worked:","2c0606bb":"This is a great episode. Zuko tells Aang about an ancient firebending tribe and the story of his ancestors hunting down dragons. They proceed to find out the tribe is still living and discover the true origin of firebending.\n\n---\n\n## Lexical diversity per episode\n\nWe can do the same process to get the lexical diversity per episode:","5fc19b89":"## Distributions for characters\n\nTo help speed up the process for here on, let's define a dataframe for only the top 10 charcaters with more lines:","0e859797":"## Exploring `polarity`, `subjectivity` and `word_count`\n\nWe can create a function to display distribution of word_count, polarity and subjectivity for a given character and given cut-off word_count","9613458d":"From this point on we'll do actual modifications (not just removing parts) in the `script`. So, we'll create a separate column called `script_clean` to do all the preprocessing steps.","7a977e2e":"# Word and Lexical diversity of characters","7b0751ef":"# Polarity vs. Subjectivity and word_count","42397ea2":"## Word diversity per episode\n\nLet's build a graph for the evolution of word diversity of each of the top 10 characters with more lines per episode. The idea here will be very similar to what we've done with the evolution of polarity, subjectivity and word count.","62237536":"We can create a helper function to return all info for a given character, given episode's number and book:","8865ce7a":"There is a huge peak for Zuko with 280 different words in a single episode! Let's find out which one.\n\nUsing the hover information, we see it is episode 53:","6f48dc3d":"We can plot the top 10 episodes ranked on mean polarity for the top 10 characters with more lines. We'll use the `ep_name` column as the *text* argument in the bar plot so we can see the episode with given polarity.","7c680ba4":"## Word diversity per Book\n\nWe can check the evolution of word diversity in each book.","b999576e":"Let's see the distributions for all top 10 characters with more lines:","a2bd6e2d":"This is much better! Let's see which character has the highest `polarity`. This is a value between -1 and 1 indicating how positive a statement is. Value 1 means a very positive sentence whereas -1 means a very negative one.","2f742234":"Ok, this makes sense, Zuko was pretty pissed off when they first arrived at Ba Sing Se.","9e8c92bf":"Let's redo this with plotly for an awesome interaction.","c36f2b3e":"Let's try a quick look at the distribution of character lines. We'll create a `text` object with the sequence of characters as they appear in the `Character` column. To match a `tokenized` text, we can simply create a list of entries from our `Character` Series.","08d58efe":"Zuko has 71 lines in this episode. The episode name can be checked with our helper function:","80f995cc":"# Saving our dataframe\n\nTo reuse our dataframe with all our preprocessing, we'll dump `df_lines` into a pickled object.","15af2fb3":"To visualize our episodes on a bar plot, I'll create a `ep_name` column using our **check_episode** helper function:","afa92be8":"### Plotting subjectivity and word_count per episode\n\nTo see the same figure for other attributes (like `subjectivity` and `word_count`), we can turn this last cell into a function accepting the attribute as an input:","1322b242":"Let's check Azula's distribution:","228ccdfd":"Many lines have descriptions closed between [ ]. We can remove these using a simple regex expression to analyse only the actual lines.","e5b9d71e":"# Ending the EDA (for now?)\n\nThere are many great things about this show. Its characters, its animation, its story and is evolution. Personally, this is one of my favorite shows ever! This data exploration has help me dig deep into an old passion. Hopefully, it will help someone else (re)discover the show and dig deep into the characters that make for such an interesting story!\n\nBeyond this, it was great fun to explore NLP tools and general analysis. Interactive plots with plotly are always a fun and interesting task and using nltk (and textblob) is easy and intuitve! The next step will be to create a model from this cleaned dataset and see if we can classify text as belonging to some of the main characters.","44603fdf":"# Adding `polarity`, `subjectivity` and `word_count` as new features\n\nWe'll now use `textblob` to easily create new features, namely:\n\n* `polarity`: Return the polarity score as a float within the range [-1.0, 1.0]. -1.0 refers to a very negative text, and 1.0 a very positive one.\n* `subjectivity`: Return the subjectivity score as a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n* `word_count`: word frequency in this text.\n\n(All descriptions taken from the documentation [here](https:\/\/textblob.readthedocs.io\/en\/dev\/api_reference.html#textblob.blob.TextBlob.sentiment))\n","2d8c1041":"Weird: it appears somewhere in the `Character` column there is an `Aang:` entry instead of just `Aang`. Let's see if this repeats somewhere else. To do this we check if any entry in this column contains the character `:`.","7d445593":"How many different words are there in the character lines? We'll call this *lexical diversity*.","60c71ef0":"## Word diversity\n\nLet's define a function to count the word diversity of a given character.","d6f6d75e":"## Removing punctuation\n\nWe'll replace everything that's not alphanumeric or whitespace with an empty character:","1a050a52":"Check out Katara, Sokka and Appa out there...\n\nAgain, let's put this process into a function to select a given character.","2a760e75":"Highest mean polarity happens for Iroh in episode 7, book 2. Let's inspect that:","6c7cafc0":"Now we can simply plot this as a bar plot.","c9fec975":"For the whole cast of characters we can see the distribution of these newly create features:","f586f3e9":"Wow, there is a huge line with over 180 words! Let's check it out:","da50c9e1":"Checking our dataframe sorted by `polarity`:","e44c9cfc":"Highest mean subjectivity happens for Iroh on book 3, episode 20:","7e6a922d":"We can create a helper function to check epsiode name given the number and book:","00db61ed":"Necessary imports:","56f7ed50":"To see the impact of the top 10 characters with more lines, we can create this exact same plot using all character lines and see the new rank:","ef563be2":"Ok, so there are waaaay too many names here. Let's focus our analysis on the top 10 characters:","98878c4d":"#### Subjectivity","056b17a9":"The word count slightly increases form book to book. In book 3 the distribution has a lot more points ","1ce84e39":"The very first line would show us a weird thing about the stopwords: 'not' is technically an adverb but has still been included in NLTK\u2019s list of stop words for English.\n\nLet's remove this from the list of stop words so it makes more sense (Sokka is saying the fish is not getting away is very different from saying it is getting away...).\n\nInspecting the results way down the road, I have also found some common words in the show that can be removed and not alter the content of the lines. I'll include these as stop word here too.","8b9977d2":"All characters:","54e6446c":"The data set has two types of entries in the `Character` column:\n* Named characters: corresponds to actual character lines in the show.\n\n* NaN values: corresponding to scene descriptions.\n\nI'll separate our data in two different dataframes, one for the lines and other for the descriptions.","0e7f37c9":"This is one of the best scenes in the show: Zuko and Azula's final Agni Kai! Zuko is overall worried about Aang in the first scene (his first line). By the end he notices Azula's having a break down and uses that to leverage the figth, so his lines tend to be provacative (\"No lightning today? What's the matter? Afraid I'll redirect it?\" and \"Sorry, but you're not gonna become Fire Lord today. I am.\"). \n\nThese three episodes represent well Zuko's final decision:\n* from being an outcast away from his royal condition in *City of Walls and Secrets*\n* to making a dramatic life decision and leaving his home to help bring his father down in *The Day of Black Sun, Part 1: The Invasion*\n* and finally confronting his sister in an awesome Agni Kai where, for once, **he** was the calm and centered one doing the provoking.\n\nWe could go on doing this analysis for all charcaters (because it's so much fun!), but I'll leave the tools here for future exploring and for the curious minds.\n\n---\n\nTo get an overall view of the top 10 characters, we can build a cool visual showing all the evolutions of characters polarity per episode in a single graph. Using *plotly*, this can generate a clean, pretty and interactive figure:"}}