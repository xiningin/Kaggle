{"cell_type":{"c12ffbd2":"code","02d0fb67":"code","fc07a5c9":"code","3a9fb265":"code","f46d8e9e":"code","1238e1a6":"code","c45494c1":"code","e6951df0":"code","2660089d":"code","0efa336c":"code","1a0f0c41":"code","507ac572":"code","39b31d0c":"code","667e7701":"code","b02594e3":"code","e2eb6a82":"code","1d555e64":"code","87f4ff24":"code","db2718bd":"code","33d06e52":"code","bdbd2dd9":"code","9387c451":"code","627bfd98":"code","57a2124d":"code","6df205a7":"code","1cf9f4ee":"code","01df0e16":"code","700798a4":"code","60ee510b":"code","a8656b17":"code","852d8784":"code","cd3d0ac3":"code","9fe44a5a":"code","397a6c98":"code","6475fb45":"code","c5544553":"code","1cf30c5b":"code","9a154a07":"code","1bc76e6b":"code","d3f902ab":"code","a8cca3af":"code","bcdff985":"code","c556f5a9":"code","096e1893":"code","c8fdafe3":"code","3b161bd9":"code","8056a4dc":"code","d8ace119":"code","aa7d24b1":"code","f244cc75":"code","16753eb1":"code","df6a33f2":"code","eaa4b05c":"code","53cf052a":"code","73b4ad26":"code","1885c309":"code","a801bd6c":"code","7bfab525":"code","cc84c4e7":"code","8de3d298":"markdown","27616a51":"markdown","5f2eec66":"markdown","6571ec8b":"markdown","f80c2874":"markdown","fb0cf6e4":"markdown","03ec1906":"markdown"},"source":{"c12ffbd2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib as mlp\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\nfrom sklearn.model_selection import cross_validate, cross_val_predict\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.inspection import permutation_importance\nimport seaborn as sns\nimport itertools\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.neighbors import NearestNeighbors\nfrom multiprocessing import Pool, freeze_support\npd.set_option('display.max_rows', 500)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02d0fb67":"### MAIL\n\n# gabriele.maroni@unibg.it","fc07a5c9":"def mean_encoding_cv(train, test, categorical, cv=5, drop_categorical=False):\n    skf = StratifiedKFold(n_splits=5)\n    train_new = train.copy()\n    global_mean = train_new[target].mean()\n    categorical_enc = []\n\n    for col in categorical:\n        print('Encoding variable: ' + col)\n        train_new[f'{col}_target_enc'] = np.nan\n        categorical_enc.append(f'{col}_target_enc')\n        for train_index, valid_index in skf.split(train, train[target]):\n            train_fold = train.iloc[train_index].copy()\n            valid_fold = train.iloc[valid_index].copy()\n            valid_fold[f'{col}_target_enc'] = valid_fold[col].map(train_fold.groupby(col)[target].mean())\n            train_new.loc[valid_index, f'{col}_target_enc'] = valid_fold[f'{col}_target_enc']\n            train_new[f'{col}_target_enc'].fillna(global_mean, inplace=True) \n\n    for col in categorical:\n        test[f'{col}_target_enc'] = test[col].map(train_new.groupby(col)[f'{col}_target_enc'].mean())\n        test[f'{col}_target_enc'].fillna(global_mean, inplace=True) \n\n\n    comb = pd.concat([train_new, test])\n    if drop_categorical:\n        comb.drop(categorical, axis=1, inplace=True)\n    return comb, categorical_enc","3a9fb265":" def cross_val_score(train, features, target, N_boot=10, N_bag=10, N_fold=5, max_features=0.7, bootstrap=False, plot=False):   \n    N_boot = N_boot\n    N_bag = N_bag\n    N_fold = N_fold\n    max_features = max_features\n    cv_preds = []\n    scores_list = []\n\n    for j in range(N_boot):\n        train2 = train.sample(frac=1, replace=bootstrap, random_state=2*j) # replace=False-> bootstrap off\n        for i in range(N_bag): \n            model = DecisionTreeClassifier(random_state=i*42, max_features=max_features,\n                                                              criterion='gini',\n                                                              splitter='best',\n                                                              max_depth=7,\n                                                              min_samples_split=580, \n                                                              min_samples_leaf=30)  \n\n            scores = cross_validate(model, train2[features], train2[target], scoring='neg_log_loss', n_jobs=10, verbose=0, cv=N_fold, return_train_score=True)\n            #print('Log-Loss oof: ', -np.mean(scores['test_score']), '+-' , np.std(scores['test_score']))\n            #print('Log-Loss train: ', -np.mean(scores['train_score']), '+-' , np.std(scores['train_score']))\n            scores_list.append(scores)\n    oof_scores = pd.DataFrame(np.ravel([-scores_list[i]['test_score'] for i in range(N_boot*N_bag)]), columns=['oof_score'])\n    train_scores = pd.DataFrame(np.ravel([-scores_list[i]['train_score'] for i in range(N_boot*N_bag)]), columns=['train_score'])\n    print('Mean Log-Loss oof: ', oof_scores.mean().values[0], '+-' , oof_scores.std().values[0])\n    print('Mean Log-Loss train: ', train_scores.mean().values[0], '+-' , train_scores.std().values[0])\n    if plot:\n        sns.boxenplot(x='type', y='score', \n                      data=pd.concat([oof_scores.stack(), train_scores.stack()]).reset_index(1).rename({'level_1':'type', 0:'score'}, axis=1))","f46d8e9e":"class NearestNeighborsFeats(BaseEstimator, ClassifierMixin):\n    \n    def __init__(self, n_jobs, k_list, metric, n_classes=None, n_neighbors=None, eps=1e-6):\n        self.n_jobs = n_jobs\n        self.k_list = k_list\n        self.metric = metric       \n        if n_neighbors is None:\n            self.n_neighbors = max(k_list) \n        else:\n            self.n_neighbors = n_neighbors            \n        self.eps = eps        \n        self.n_classes_ = n_classes\n    \n    def fit(self, X, y):    \n        self.NN = NearestNeighbors(n_neighbors=max(self.k_list), \n                                      metric=self.metric, \n                                      n_jobs=1, \n                                      algorithm='brute' if self.metric=='cosine' else 'auto')\n        self.NN.fit(X)  \n        self.y_train = y      \n        self.n_classes = np.unique(y).shape[0] if self.n_classes_ is None else self.n_classes_\n        \n        \n    def predict(self, X):       \n\n        if self.n_jobs == 1:\n            test_feats = []\n            for i in range(X.shape[0]):\n                test_feats.append(self.get_features_for_one(X[i:i+1]))\n        else:\n            test_feats = Pool(self.n_jobs).map(self.get_features_for_one, (X[i:i+1] for i in range(X.shape[0])))            \n        return np.vstack(test_feats)\n        \n        \n    def get_features_for_one(self, x):\n        NN_output = self.NN.kneighbors(x)\n        neighs = NN_output[1][0]\n        neighs_dist = NN_output[0][0] \n        neighs_y = self.y_train[neighs] \n        neighs=neighs.astype(int)\n        neighs_dist=neighs_dist.astype(float)\n        neighs_y=neighs_y.astype(int)\n        return_list = [] \n        feats_names = []\n        \n        for k in self.k_list:\n            means_kn = np.mean(neighs_y[:k])\n            return_list += [means_kn]\n        knn_feats = np.hstack(return_list)\n        \n        return knn_feats","1238e1a6":"train = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/test.csv')\ntest['target'] = np.nan","c45494c1":"categorical = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\nnumerical = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\nfeatures = numerical+categorical\ntarget = 'target'","e6951df0":"comb, categorical_enc = mean_encoding_cv(train, test, categorical)","2660089d":"train = comb[:len(train)].copy()\ntest = comb[len(train):].copy()\nfeatures = numerical + categorical_enc","0efa336c":"cross_val_score(train, features, target, plot=True)","1a0f0c41":"train = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/test.csv')\ntest['target'] = np.nan\ncomb = pd.concat([train, test])\ncomb[numerical] = (comb[numerical]-comb[numerical].mean())\/comb[numerical].std()\ntrain = comb[:len(train)].copy()\ntest = comb[len(train):].copy()","507ac572":"comb, categorical_enc = mean_encoding_cv(train, test, categorical)\ntrain = comb[:len(train)].copy()\ntest = comb[len(train):].copy()","39b31d0c":"# i compute only the best features choosen with respect to feature selection\nk_list=[90] # tested [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500]\nmetrics = ['manhattan'] # testes ['minkowski', 'manhattan', 'chebyshev']","667e7701":"train_knn_feats_list=[]\nfor metric in metrics:\n    print (metric) \n    skf = StratifiedKFold(n_splits=5)#, shuffle=True, random_state=42)\n    NNF = NearestNeighborsFeats(n_jobs=10, k_list=k_list, metric=metric)\n    train_knn_feats = cross_val_predict(NNF, train[numerical].values, train[target].values, cv=skf)\n    train_knn_feats_list.append(train_knn_feats)","b02594e3":"test_knn_feats_list=[]\nfor metric in metrics:\n    NNF = NearestNeighborsFeats(n_jobs=10, k_list=k_list, metric=metric)\n    NNF.fit(train[numerical].values, train[target].values)\n    test_knn_feats = NNF.predict(test[numerical].values)\n    test_knn_feats_list.append(test_knn_feats)","e2eb6a82":"knn_feature_names = []\nfor metric in metrics:\n    for k in k_list:\n        knn_feature_names.append(f'knn_{k}_{metric}_target_mean')","1d555e64":"train_knn_feats_df = pd.concat([pd.DataFrame(train_knn_feats_list[i], columns=knn_feature_names) for i in range(len(train_knn_feats_list))], axis=1)\ntest_knn_feats_df = pd.concat([pd.DataFrame(test_knn_feats_list[i], columns=knn_feature_names) for i in range(len(test_knn_feats_list))], axis=1)","87f4ff24":"train_knn = pd.concat([train, train_knn_feats_df], axis=1)\ntest_knn = pd.concat([test, test_knn_feats_df], axis=1)\nfeatures = numerical + categorical_enc + knn_feature_names\nfeatures","db2718bd":"cross_val_score(train_knn, features, target, plot=True)","33d06e52":"knn_numerical = pd.concat([ train_knn['knn_90_manhattan_target_mean'], test_knn['knn_90_manhattan_target_mean'] ])\nknn_numerical.to_csv('\/kaggle\/working\/knn_numerical.csv')","bdbd2dd9":"train = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/test.csv')\ntest['target'] = np.nan","9387c451":"categorical = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\nnumerical = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\nfeatures = numerical+categorical\ntarget = 'target'","627bfd98":"comb, categorical_enc = mean_encoding_cv(train, test, categorical, drop_categorical=False)\ntrain = comb[:len(train)].copy()\ntest = comb[len(train):].copy()","57a2124d":"le = OrdinalEncoder()\nle.fit(train[categorical])\ntrain_enc = le.transform(train[categorical])\ntest_enc = le.transform(test[categorical])\ntrain[categorical] = train_enc\ntest[categorical] = test_enc","6df205a7":"# i compute only the best features choosen with respect to feature selection\nk_list=[90] # tested [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500]\nmetrics = ['hamming'] # tested ['hamming', 'canberra', 'braycurtis']","1cf9f4ee":"train_knn_feats_list=[]\nif __name__ == '__main__': \n    freeze_support()\n    for metric in metrics:\n        print (metric) \n        skf = StratifiedKFold(n_splits=5)#, shuffle=True, random_state=42)\n        NNF = NearestNeighborsFeats(n_jobs=10, k_list=k_list, metric=metric)\n        train_knn_feats = cross_val_predict(NNF, train[categorical].values, train[target].values, cv=skf)\n        train_knn_feats_list.append(train_knn_feats)","01df0e16":"test_knn_feats_list=[]\nif __name__ == '__main__': \n    freeze_support()\n    for metric in metrics:\n        NNF = NearestNeighborsFeats(n_jobs=10, k_list=k_list, metric=metric)\n        NNF.fit(train[categorical].values, train[target].values)\n        test_knn_feats = NNF.predict(test[categorical].values)\n        test_knn_feats_list.append(test_knn_feats)","700798a4":"knn_feature_names = []\nfor metric in metrics:\n    for k in k_list:\n        knn_feature_names.append(f'knn_{k}_{metric}_target_mean')","60ee510b":"train_knn_feats_df = pd.concat([pd.DataFrame(train_knn_feats_list[i], columns=knn_feature_names) for i in range(len(train_knn_feats_list))], axis=1)\ntest_knn_feats_df = pd.concat([pd.DataFrame(test_knn_feats_list[i], columns=knn_feature_names) for i in range(len(test_knn_feats_list))], axis=1)","a8656b17":"train_knn = pd.concat([train, train_knn_feats_df], axis=1)\ntest_knn = pd.concat([test, test_knn_feats_df], axis=1)\nfeatures = numerical + categorical_enc + knn_feature_names\nfeatures","852d8784":"cross_val_score(train_knn, features, target, plot=True)","cd3d0ac3":"knn_categorical = pd.concat([train_knn['knn_90_hamming_target_mean'], test_knn['knn_90_hamming_target_mean']])\nknn_categorical.to_csv('\/kaggle\/working\/knn_categorical.csv')","9fe44a5a":"train = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/test.csv')\nknn_num = pd.read_csv('\/kaggle\/working\/knn_numerical.csv')\nknn_cat = pd.read_csv('\/kaggle\/working\/knn_categorical.csv')\ntest['target'] = np.nan\ncomb = pd.concat([train, test])\ncomb['knn_90_manhattan_target_mean'] = knn_num['knn_90_manhattan_target_mean'].values\ncomb['knn_90_hamming_target_mean'] = knn_cat['knn_90_hamming_target_mean'].values","397a6c98":"categorical = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\nnumerical = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'knn_90_manhattan_target_mean', 'knn_90_hamming_target_mean']\nfeatures = numerical+categorical\ntarget = 'target'","6475fb45":"interaction_features = []\nfor combination in list(itertools.combinations(numerical, 2)):\n    i,j = combination[0],combination[1]\n    comb[i+'_multiplied_by_'+j] = comb[i]*comb[j]\n    comb[i+'_sum_by_'+j] = comb[i]+comb[j]\n    comb[i+'_diff_by_'+j] = comb[i]-comb[j]\n    interaction_features.append(i+'_multiplied_by_'+j)\n    interaction_features.append(i+'_sum_by_'+j)\n    interaction_features.append(i+'_diff_by_'+j)\n    try:\n        comb[i+'_divided_by_'+j] = comb[i]\/(comb[j]+1e-10)\n        interaction_features.append(i+'_divided_by_'+j)\n    except:\n        pass","c5544553":"# selected by feature importance\ninteraction_features=['knn_90_manhattan_target_mean_sum_by_knn_90_hamming_target_mean',\n       'knn_90_manhattan_target_mean_multiplied_by_knn_90_hamming_target_mean',\n       'capital-gain_sum_by_knn_90_hamming_target_mean',\n       'capital-gain_diff_by_capital-loss',\n       'capital-gain_sum_by_knn_90_manhattan_target_mean',\n       'capital-loss_diff_by_hours-per-week',\n       'age_multiplied_by_knn_90_hamming_target_mean',\n       'capital-gain_sum_by_capital-loss',\n       'capital-gain_sum_by_hours-per-week',\n       'capital-gain_diff_by_knn_90_manhattan_target_mean',\n       'capital-gain_divided_by_capital-loss', 'age_diff_by_capital-gain']","1cf30c5b":"train = comb[:len(train)].copy()\ntest = comb[len(train):].copy()","9a154a07":"numerical_tree = []\nfor n in numerical+interaction_features:    \n    param_grid = {'max_depth': [6,7,8,9,10,11,12]}\n    model = GridSearchCV(DecisionTreeClassifier(random_state=0), cv = 5, scoring = 'roc_auc', param_grid = param_grid)\n    model.fit(train[n].to_frame(), train[target])\n    train[f'{n}_tree'] = model.predict_proba(train[n].to_frame())[:,1]\n    test[f'{n}_tree'] = model.predict_proba(test[n].to_frame())[:,1]\n    numerical_tree.append(f'{n}_tree')","1bc76e6b":"comb, categorical_enc = mean_encoding_cv(train, test, categorical+numerical_tree)\ntrain = comb[:len(train)].copy()\ntest = comb[len(train):].copy()","d3f902ab":"features = categorical_enc\nfeatures","a8cca3af":"cross_val_score(train, features, target, plot=True)","bcdff985":"train = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/test.csv')\nknn_num = pd.read_csv('\/kaggle\/working\/knn_numerical.csv')\nknn_cat = pd.read_csv('\/kaggle\/working\/knn_categorical.csv')\ntest['target'] = np.nan\ncomb = pd.concat([train, test])\ncomb['knn_90_manhattan_target_mean'] = knn_num['knn_90_manhattan_target_mean'].values\ncomb['knn_90_hamming_target_mean'] = knn_cat['knn_90_hamming_target_mean'].values","c556f5a9":"categorical = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\nnumerical = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'knn_90_manhattan_target_mean', 'knn_90_hamming_target_mean']\nfeatures = numerical+categorical\ntarget = 'target'","096e1893":"interaction_features = []\nfor combination in list(itertools.combinations(numerical, 2)):\n    i,j = combination[0],combination[1]\n    comb[i+'_multiplied_by_'+j] = comb[i]*comb[j]\n    comb[i+'_sum_by_'+j] = comb[i]+comb[j]\n    comb[i+'_diff_by_'+j] = comb[i]-comb[j]\n    interaction_features.append(i+'_multiplied_by_'+j)\n    interaction_features.append(i+'_sum_by_'+j)\n    interaction_features.append(i+'_diff_by_'+j)\n    try:\n        comb[i+'_divided_by_'+j] = comb[i]\/(comb[j]+1e-10)\n        interaction_features.append(i+'_divided_by_'+j)\n    except:\n        pass","c8fdafe3":"# selected by feature importance\ninteraction_features=['knn_90_manhattan_target_mean_sum_by_knn_90_hamming_target_mean',\n       'knn_90_manhattan_target_mean_multiplied_by_knn_90_hamming_target_mean',\n       'capital-gain_sum_by_knn_90_hamming_target_mean',\n       'capital-gain_diff_by_capital-loss',\n       'capital-gain_sum_by_knn_90_manhattan_target_mean',\n       'capital-loss_diff_by_hours-per-week',\n       'age_multiplied_by_knn_90_hamming_target_mean',\n       'capital-gain_sum_by_capital-loss',\n       'capital-gain_sum_by_hours-per-week',\n       'capital-gain_diff_by_knn_90_manhattan_target_mean',\n       'capital-gain_divided_by_capital-loss', 'age_diff_by_capital-gain']","3b161bd9":"import itertools\ngrouped_features=[]\nfor cat, num in list(itertools.product(categorical, numerical)):\n    comb_tmp = pd.DataFrame(index=comb.groupby(cat).size().index)\n    comb_tmp[num +'_mean_gropby_'+cat] = comb.groupby(cat)[num].mean().values\n    comb = pd.merge(comb, comb_tmp, how='left', on=[cat])\n    comb[num+'_minus_'+num +'_mean_gropby_'+cat] = comb[num].values - comb[num +'_mean_gropby_'+cat]\n    grouped_features.append(num +'_mean_gropby_'+cat)\n    grouped_features.append(num+'_minus_'+num +'_mean_gropby_'+cat)","8056a4dc":"grouped_features = ['knn_90_hamming_target_mean_minus_knn_90_hamming_target_mean_mean_gropby_race',\n       'capital-gain_minus_capital-gain_mean_gropby_native-country',\n       'knn_90_hamming_target_mean_minus_knn_90_hamming_target_mean_mean_gropby_native-country',\n       'capital-gain_minus_capital-gain_mean_gropby_race',\n       'knn_90_manhattan_target_mean_minus_knn_90_manhattan_target_mean_mean_gropby_sex',\n       'capital-gain_minus_capital-gain_mean_gropby_relationship',\n       'knn_90_manhattan_target_mean_minus_knn_90_manhattan_target_mean_mean_gropby_workclass',\n       'capital-gain_minus_capital-gain_mean_gropby_marital-status',\n       'knn_90_manhattan_target_mean_minus_knn_90_manhattan_target_mean_mean_gropby_native-country',\n       'knn_90_hamming_target_mean_minus_knn_90_hamming_target_mean_mean_gropby_sex']","d8ace119":"train = comb[:len(train)].copy()\ntest = comb[len(train):].copy()","aa7d24b1":"numerical_tree = []\nfor n in numerical+interaction_features+grouped_features:    \n    param_grid = {'max_depth': [6,7,8,9,10,11,12]}\n    model = GridSearchCV(DecisionTreeClassifier(random_state=0),cv = 5, n_jobs=-1, scoring = 'roc_auc', param_grid = param_grid)\n    model.fit(train[n].to_frame(), train[target])\n    train[f'{n}_tree'] = model.predict_proba(train[n].to_frame())[:,1]\n    test[f'{n}_tree'] = model.predict_proba(test[n].to_frame())[:,1]\n    numerical_tree.append(f'{n}_tree')","f244cc75":"comb, categorical_enc = mean_encoding_cv(train, test, categorical+numerical_tree)\ntrain = comb[:len(train)].copy()\ntest = comb[len(train):].copy()","16753eb1":"train = comb[:len(train)].copy()\ntest = comb[len(train):].copy()\nfeatures = categorical_enc\nfeatures","df6a33f2":"cross_val_score(train, features, target, plot=True)","eaa4b05c":"comb = pd.concat([train, test])\ncomb.to_csv('\/kaggle\/working\/comb_final.csv')","53cf052a":"N_boot = 50\nN_bag = 5\nN_fold = 5\nmax_features_list = [0.8]\npreds_list = []\nscores = []\n\nfor max_features in max_features_list:\n    print('-'*200)\n    for i in range(N_boot):\n        print('Bootstrapping round: ', i)\n        train2 = train.sample(frac=1, replace=True, random_state=31*i) # replace=True-> bootstrap on\n        index = train2.index\n        skf = StratifiedKFold(n_splits=N_fold, shuffle=False) # so im sure cross_val_predict does not shuffle dataset \n        for j in range(N_bag):\n            preds_df = pd.DataFrame(index=range(len(train)), columns=['original_index', 'preds'])\n            preds_df['original_index'] = index\n            model = DecisionTreeClassifier(random_state=j*42, max_features=max_features,\n                                                              criterion='gini',\n                                                              splitter='best',\n                                                              max_depth=7,\n                                                              min_samples_split=480, \n                                                              min_samples_leaf=30)\n\n            preds=cross_val_predict(model,train2[features],train2[target],n_jobs=10,verbose=0,cv=skf,method='predict_proba')\n            preds_df['preds'] = preds[:,1]\n            preds_list.append(preds_df)\n    mean_preds = pd.concat(preds_list, ignore_index=True).groupby('original_index').mean()\n    print(f'Log-loss with max_features {max_features}: ', log_loss(train[target], mean_preds))\n    scores.append(log_loss(train[target], mean_preds))","73b4ad26":"N_boot = 50\nN_bag = 5\nmax_features=0.8\npredictions_test = []\n\nfor i in range(N_boot):\n    print('Bootstrapping round: ', i)\n    train2 = train.sample(frac=1, replace=True, random_state=31*i) # replace=True-> bootstrap on\n    index = train2.index\n    for j in range(N_bag):\n        model = DecisionTreeClassifier(random_state=j*42, max_features=max_features,\n                                                          criterion='gini',\n                                                          splitter='best',\n                                                          max_depth=7,\n                                                          min_samples_split=480, \n                                                          min_samples_leaf=30)\n        model.fit(train2[features], train2[target])\n        p_test = model.predict_proba(test[features])[:, 1]\n        predictions_test.append(p_test)","1885c309":"preds_test = pd.DataFrame(predictions_test).T.mean(axis=1)\npreds_test.plot.hist(density=True, bins=100)","a801bd6c":"df_test_sub = pd.read_csv('\/kaggle\/input\/ods-mlclass-dubai-2019-03-lecture3-hw\/test.csv')\ndf_submit = pd.DataFrame({'uid': df_test_sub['uid'],'target': preds_test})\ndf_submit.to_csv('\/kaggle\/working\/submission.csv', index=False)","7bfab525":"N_bag = 20\nN_fold = 5\nmax_features = 0.7\ncv_preds = []\nfeat_imp_list = []\n\nfor i in range(N_bag): \n    print('Bag: ', i)\n    model = DecisionTreeClassifier(random_state=i*42, max_features=max_features,\n                                                      criterion='gini',\n                                                      splitter='best',\n                                                      max_depth=7,\n                                                      min_samples_split=580, \n                                                      min_samples_leaf=30)\n    \n    skf = StratifiedKFold(n_splits=N_fold, random_state=i*123, shuffle=True)\n    for train_index, valid_index in skf.split(train, train[target]):\n        train_fold = train.iloc[train_index].copy()\n        valid_fold = train.iloc[valid_index].copy()\n        model.fit(train_fold[features], train_fold[target])\n        result = permutation_importance(model, valid_fold[features], valid_fold[target], n_repeats=10,\n                                        random_state=i*42, n_jobs=-1, scoring='neg_log_loss')\n        feat_imp_list.append(result)","cc84c4e7":"Feature_importance = pd.DataFrame(pd.DataFrame(feat_imp_list).importances_mean.explode().astype(float).values,\n                                  index=(list(train[features].columns)*N_bag*N_fold), columns=['importance'])\nFeature_importance.reset_index(drop=False, inplace=True)\nFeature_importance.rename({'index': 'feature_name'}, axis=1, inplace=True)\nmeans = Feature_importance.groupby('feature_name').mean()\nmeans.sort_values('importance', ascending=False, inplace=True)\nFeature_importance=Feature_importance.set_index('feature_name').loc[means.index]\nFeature_importance.reset_index(drop=False, inplace=True)\nplt.figure(figsize=(8,10))\nsns.boxplot(y='feature_name', x='importance', data=Feature_importance)","8de3d298":"# Baseline","27616a51":"# Add numerical interactions","5f2eec66":"# Add grouping features","6571ec8b":"# Feature engineering with knn\n## numerical","f80c2874":"# Bagging for prediction","fb0cf6e4":"# Feature engineering with knn\n## categorical","03ec1906":"# Feature importance"}}