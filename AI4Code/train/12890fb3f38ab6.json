{"cell_type":{"e1c3d3cb":"code","2ded0825":"code","6f00c2fa":"code","edaec936":"code","16168452":"code","2847d62b":"code","1ed85248":"code","8bb3ad0f":"code","95268798":"code","a4f9393e":"code","b6365cee":"code","4cb569be":"code","c31918a8":"code","1605a754":"code","a1a8b61d":"code","607031c1":"code","d4403bc5":"code","c4796289":"code","0eed6be3":"code","4a3aa0f8":"code","55389407":"code","9672507e":"code","ac8b2702":"code","059345dc":"code","a052b866":"code","f3f1692d":"code","3e96aeec":"markdown","6ffcdb5c":"markdown","02e88877":"markdown","ff73f5f2":"markdown","29b44c16":"markdown","591533b5":"markdown","0b69b649":"markdown","4ff7c220":"markdown","c259b4d9":"markdown","b482fc3c":"markdown","61ce4c2b":"markdown","e8b44fee":"markdown","d78bab9d":"markdown","8c9a7905":"markdown","30b91773":"markdown","3054518e":"markdown","fe6026b4":"markdown","f78049ad":"markdown","5434885e":"markdown","e2c0ccdf":"markdown","6ac76a50":"markdown","5f0318f9":"markdown","f0bc1264":"markdown","943069a1":"markdown","22a00996":"markdown","a22cb884":"markdown","86b7fd0a":"markdown","60fa3a81":"markdown","d3510ac8":"markdown"},"source":{"e1c3d3cb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2ded0825":"df = pd.read_csv(\"..\/input\/college-basketball-dataset\/cbb.csv\")","6f00c2fa":"df.info()","edaec936":"df.corr()['W'].sort_values()[:-1]","16168452":"df_ff = df[['EFG_O','EFG_D','TOR','TORD','ORB','DRB','FTR','FTRD','W']]","2847d62b":"fig, axes = plt.subplots(ncols=4, nrows=3)\nfor col, ax in zip(df_ff.columns, axes.flat):\n    sns.distplot(df_ff[col], hist=False, ax=ax)\nplt.tight_layout()\nplt.show()","1ed85248":"df_ff.describe()","8bb3ad0f":"sns.regplot(x='W',y = 'EFG_O',data = df_ff,scatter= True, fit_reg=True)","95268798":"sns.regplot(x='W',y = 'TOR',data = df_ff,scatter= True, fit_reg=True)","a4f9393e":"sns.regplot(x='W',y = 'ORB',data = df_ff,scatter= True, fit_reg=True)","b6365cee":"sns.regplot(x='W',y = 'FTR',data = df_ff,scatter= True, fit_reg=True)","4cb569be":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(14,10))\nsns.regplot(x='W',y = 'EFG_D',data = df_ff,scatter= True, fit_reg=True,ax=ax1)\nsns.regplot(x='W',y = 'TORD',data = df_ff,scatter= True, fit_reg=True,ax=ax2)\nsns.regplot(x='W',y = 'DRB',data = df_ff,scatter= True, fit_reg=True,ax=ax3)\nsns.regplot(x='W',y = 'FTRD',data = df_ff,scatter= True, fit_reg=True,ax=ax4)","c31918a8":"# prepare the dataset\ndf_ff = df[['EFG_O','EFG_D','TOR','TORD','ORB','DRB','FTR','FTRD']]\ndf_ff_y = df['W']","1605a754":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","a1a8b61d":"X_train, X_test, y_train, y_test = train_test_split(df_ff, df_ff_y, test_size=0.25, random_state=21)\nreg = LinearRegression()\nreg = reg.fit(X_train,y_train)","607031c1":"print('Intercept: ', reg.intercept_)\nprint('R^2 score: ',reg.score(X_train,y_train))","d4403bc5":"coeff_df = pd.DataFrame(reg.coef_, df_ff.columns, columns=['Coefficient'])\ncoeff_df","c4796289":"cof = []\ntcof = 0\nfor i in range(0,8,2):\n    avgcof = (abs(coeff_df['Coefficient'][i])+abs(coeff_df['Coefficient'][i+1]))\/2\n    cof.append(avgcof)\n    tcof += avgcof\nprint(cof\/tcof)","0eed6be3":"# predict from the test set\ny_pred = reg.predict(X_test)","4a3aa0f8":"# analyze the prediction\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","55389407":"dfd = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'AbsDiff': abs(y_test-y_pred)})\ndfd.sort_values(by=['AbsDiff'], inplace=True, ascending=True)\ndfd[:10]","9672507e":"df[df.index == 21]","ac8b2702":"df[df.index == 937]","059345dc":"dfd.sort_values(by=['AbsDiff'], inplace=True, ascending=False)\ndfd[:10]","a052b866":"df[df.index == 1174]","f3f1692d":"df[df.index == 1338]","3e96aeec":"## Train the model\n\nI will use linear regression for our learning model. This is somewhat questionable choice because as we see, there are some factors that not linearly related to the number of wins. But as this is my first Kaggle notebook, I say why not :) maybe in the future as my knowledge about statistics and machine learning grows (I study statistics and programming by myself) I will provide more accurate model to predict win using this four-factors. Without a further ado, let's make our training model!","6ffcdb5c":"Our MAE scores 1.94, which means that our model made errors around 2 win\/lose (for example actual win 18, we predict 20). From the RMSE, we got 2.41 which also states that our model misses around 2.4 win due to some outliers (that we neglect along this notebook). Is this model good? it depends, but I'll say it is not pretty good as we neglect some linear regression assumptions. Now, let's see some good and bad prediction made by this model","02e88877":"It can be said that the four (eight) factors that analyzed by Oliver are the most correlated features to the target, which is the number of wins ('W' in our dataset). We want to see whether this hypothesis is true by see the correlation of each feature to the number of win.","ff73f5f2":"All right, so our R^2 score is around 86%, which is pretty good, but not** that** good. From the coefficient, we'll see that the order of \"most important\" factor is true, but what about the weight? Does it obey the 40\/25\/20\/15 rule? We can find it by first find the average of absolute coefficient value of both offensive and defensive counterparts (for example average of absolute coefficient of EFG_O and EFG_D is (0.951573+0.890253)\/2=0.92091), then divide it by the total number of average absolute coefficient.","29b44c16":"There are some interesting that we found here. Defensive rebound and free throw defense give more linear relation than their offensive counterparts. However, the number of turnovers commited gives weak linear relation as opposed in high linearity when we measure turnovers allowed.  ","591533b5":"Let's see the descriptive stats for all of those features","0b69b649":"Before we train the model, we also want to see the relationship between the factors and the number of wins. Actually we have see it from the correlation value, but I will visualize some of the factors to see the relationship in easier way.","4ff7c220":"Let's analyze the model first by evaluating the parameters","c259b4d9":"We found out that shooting gives around 40.2%, turnover 38.3%, rebound 15.7%, and free throw 5.7%. It means that shooting still gives same amount of weight as analyzed by Oliver, but turnover give more weight while decrease both value of rebound and free throw. I expect the weight will be increase due to recent trend in three-pointers, but perhaps it is not the case in college basketball. Perhaps also the difference of playstyle in college basketball affect this different in weight. Now, let's make some prediction with our model","b482fc3c":"Now we can see that the linearity is somewhat questionable in offensive rebound. What about the free throw rate?","61ce4c2b":"We'll see a linear relationship here, that make the use of good ol' linear regression is a moderate choice. Let's see the next factor, which is the turnover percentage","e8b44fee":"It seems that all of the features have normal distribution, but I suspect that 'FTRD' feature has somewhat skewed tendency, but let it be for now. ","d78bab9d":"We'll split the train and test data and create the model","8c9a7905":"So here we correctly predict the fate of 2018 Texas Tech who get 27 wins and St. Francis who only get 4 wins. Let's see some bad predictions we made","30b91773":"# Win Prediction Using Four-Factors\n## Introduction\nIn this notebook, I will try to predict the number of wins for each basketball team from college basketball dataset using Dean Oliver's \"Four Factors of Basketball Success\". What is four factors? In simple words, for factors are four most important strategies to win a basketball game, as analyzed by Dean Oliver. The strategies are:\n1. Score Every Possession\n2. Pick Up All Rebounds\n3. Get to the Foul Line\n4. Protect the Basketball\n\nFor the explanation of each strategies, I recommend you to read from various sports blog (such as this [blog](https:\/\/squared2020.com\/2017\/09\/05\/introduction-to-olivers-four-factors\/) where I got those four strategies). In other words, those strategies can be represented in four stats: \n1. Effective field goals \n2. Turnovers percentage \n3. Rebounding percentage \n4. Free Throws rate \n\nIt should be noticed that, we must consider both offense and defense, as for example scoring many points does not enough to win the game, as we need to minimize the opponent scoring as well. Thus, we must consider 8 factors in total for both offense and defense:\n* Offensive Factors\n    * Effective Field Goal Percentage\n    * Percentage\n    * Offensive Rebound Percentage\n    * Free Throw Rate\n\n* Defensive Factors\n    * Opponent\u2019s Effective Field Goal Percentage\n    * Opponent\u2019s Turnover Percentage\n    * Defensive Rebound Percentage\n    * Opponent\u2019s Free Throw Rate\n\n(again thanks to the author of this [blog](https:\/\/squared2020.com\/2017\/09\/05\/introduction-to-olivers-four-factors\/) for providing clear explanation)\nLuckily for us, the college basketball dataset has provided us all of the stats above to be played with. ","3054518e":"## The Prediction","fe6026b4":"This is top 10 most accurate result from our model. Let's see some team that we predict correctly","f78049ad":"Our 8 factors are 'EFG_O','EFG_D','TOR','TORD','ORB','DRB','FTR', and 'FTRD'. We can see that the top 3 correlated features actually are 'WAB' which is the predicted number of win against average NCAA team, 'BARTHAG' which is the power ranking, and 'ADJOE' which is the adjusted offensive efficiency. From the 8 factors, 'EFG_O' has the highest correlation with the number of wins, as well as 'EFG_D' which has the lowest negative correlation. \nBy the way, Oliver also identified the approximate weight for each factor:\n* Shooting (40%)\n* Turnovers (25%)\n* Rebounding (20%)\n* Free Throws (15%)\n\nwhich means that shooting is the most important factor, followed by turnovers, rebounding, and free throws. From the correlation value we can see that this order somewhat true, but probably the weight will be different in our model.","5434885e":"Then let's see the distribution of each features","e2c0ccdf":"## Brief Exploratory Data Analysis\nFirst we import some libraries that will be useful for EDA","6ac76a50":"We can see that same with the offensive rebound, the linearity is almost missing. That correspond to low correlation on both features to the number of wins. Lastly, let see the plot for the defensive factors","5f0318f9":"We'll import some libraries for training and measure our prediction","f0bc1264":"A somewhat linear relationship with negative correlation, which is true, as if we minimize the turnover, we'll have the ball more often, which resulted in more wins. Then let's see for offensive rebound percentage ","943069a1":"Next let's make a subdataframe that will contain the four factors and the target","22a00996":"## Conclusion\nWe have made some predictions using linear regression, which in my opinion gives fairly accurate-but not great-model due to some neglection on linear regression assumptions. In the future, maybe I can improve my model by pre-processing the data to meet the linear regression assumption, and also using PCA to solve some dimensionality problem, or even using better machine learning method. All of the code minus some visualization can be found in my github: https:\/\/github.com\/thomasoca. I still learn how to become a good data scientist, and I hope my first notebook does not dissapoint you all. Comments, advices, or anything else is acceptable.","a22cb884":"Then we import the dataset into our dataframe. We only use the combined dataset (cbb.csv) that contain all data from 2015-2019","86b7fd0a":"First, we know that EFG_O gives the highest correlation. Thus let's see the scatterplot with the number of wins","60fa3a81":"We predict that 2017 Western Carolina will get around 3 wins, but in reality they got 9 wins (huraay?) I can't say that they somewhat overachieve, or due to our bad model. The hilarious one is 2015 Grambling St., who failed to win any games and we predict them to win...**-6 games** I even can't imagine what -6 win is, but safe to say, that their four factors are **really** bad to make our model thinks that they can't win positive games.","d3510ac8":"Let's see the info of this dataset"}}