{"cell_type":{"f37141b9":"code","3e90e79e":"code","d16e28e1":"code","87c14d25":"code","b772a7d0":"code","e586c24c":"code","04417900":"code","d49c4d05":"code","946f7713":"code","dd4a1e55":"code","183cc2d2":"code","8e10efbb":"code","0c48ae9a":"code","5a56fd59":"code","60655227":"code","fd2682dc":"code","26c65c85":"code","a471bd41":"code","47091256":"code","a30c4c9b":"code","7e564f86":"code","e9d17845":"code","d76841aa":"code","fc06c5a1":"code","7c6c7768":"code","b62b60a9":"code","7078fc1c":"code","6e37d597":"code","af9ba7cc":"code","61fd1bd1":"markdown","2b1c586b":"markdown","f8587def":"markdown","e79a944a":"markdown","d4dd97bc":"markdown","ee8650c1":"markdown","7ad36e3f":"markdown","0bee8e24":"markdown","772300fe":"markdown","d7fb736b":"markdown","c16c2907":"markdown","2cd1c6b3":"markdown","a884b608":"markdown","fcb6d758":"markdown","67ff3edd":"markdown","38e8f5a5":"markdown"},"source":{"f37141b9":"import time\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb","3e90e79e":"train_df = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntrain_df","d16e28e1":"train_df.isnull().sum()","87c14d25":"# cat\nunique_cats = set()\n\nfig = plt.figure(figsize=(20, 15))\nfor i in range(10):\n  column_name = f'cat{i}'\n  for cat in train_df[column_name].unique():\n    unique_cats.add(cat)\n  plt.subplot(4, 3, i+1)\n  plt.title(column_name)\n  train_df[column_name].value_counts().plot.bar()\n\nunique_cats = sorted(unique_cats)\nprint(f'unique_categories : {unique_cats}  len : {len(unique_cats)}')","b772a7d0":"# cont\nfig = plt.figure(figsize=(20, 20))\nfor i in range(14):\n  column_name = f'cont{i}'\n  plt.subplot(4, 4, i+1)\n  plt.title(column_name)\n  train_df[column_name].hist(bins=10)","e586c24c":"fig = plt.figure(figsize=(15, 6))\nax = fig.add_subplot()\ntrain_df.iloc[:, 11:25].plot.box(ax=ax)","04417900":"train_df['target'].plot.box()","d49c4d05":"X = train_df.iloc[:, 1:25].values\ny = train_df['target'].values\nX.shape, y.shape","946f7713":"X_onehot = np.zeros((300000, len(unique_cats)))\n\nonehot_enc = OneHotEncoder().fit(np.array(unique_cats).reshape(-1, 1))\n\nfor i in range(10):\n  X_onehot += onehot_enc.transform(X[:, i].reshape(-1, 1)).toarray()\n  \nX_onehot_minmax = MinMaxScaler().fit_transform(X_onehot)\nX_onehot_minmax","dd4a1e55":"X_trans = np.hstack([X_onehot_minmax, X[:, 10:]])\nX_trans.shape","183cc2d2":"X_train, X_val, y_train, y_val = train_test_split(\n    X_trans, y, test_size=0.3, random_state=42,\n)","8e10efbb":"train_data = lgb.Dataset(X_train, y_train)\nval_data = lgb.Dataset(X_val, y_val)","0c48ae9a":"params = {'metric': 'rmse'}\n\nstart = time.time()\nbst = lgb.train(params=params, train_set=train_data, valid_sets=val_data, num_boost_round=500, early_stopping_rounds=10)\nprint(f'elapsed time : {time.time() - start}')","5a56fd59":"test_df = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\ntest_df","60655227":"X_test = test_df.iloc[:, 1:25].values\n\nX_test_onehot = np.zeros((200000, len(unique_cats)))\nfor i in range(10):\n  X_test_onehot += onehot_enc.transform(X_test[:, i].reshape(-1, 1)).toarray()\n  \nX_test_onehot_minmax = MinMaxScaler().fit_transform(X_test_onehot)\n\nX_test_trans = np.hstack([X_test_onehot_minmax, X_test[:, 10:]])","fd2682dc":"y_pred = bst.predict(X_test_trans, num_iteration=bst.best_iteration)\ny_pred, y_pred.shape","26c65c85":"submit_df = pd.DataFrame({'id': test_df['id'], 'target': y_pred})\nsubmit_df","a471bd41":"today = datetime.date.today().strftime('%Y%m%d')\nsubmit_df.to_csv(f'{today}_submit.csv', index=False)","47091256":"X_all = np.vstack([X, X_test])\nX_all.shape","a30c4c9b":"X_all_onehot = OneHotEncoder().fit_transform(X_all[:, :10]).toarray()\nX_all_onehot.shape","7e564f86":"X_all_trans = np.hstack([X_all_onehot, X_all[:, 10:]])\nX_trans2, X_test_trans2 = X_all_trans[:300000, :], X_all_trans[300000:, :]\nX_trans2.shape, X_test_trans2.shape","e9d17845":"X_train2, X_val2, y_train2, y_val2 = train_test_split(\n    X_trans2, y, test_size=0.3, random_state=42,\n)\n\ntrain_data2 = lgb.Dataset(X_train2, y_train2)\nval_data2 = lgb.Dataset(X_val2, y_val2)\n\nparams = {'metric': 'rmse'}\n\nstart = time.time()\nbst2 = lgb.train(params=params, train_set=train_data2, valid_sets=val_data2, num_boost_round=500, early_stopping_rounds=10)\nprint(f'elapsed time : {time.time() - start}')","d76841aa":"y_pred2 = bst2.predict(X_test_trans2, num_iteration=bst2.best_iteration)\n\nsubmit_df2 = pd.DataFrame({'id': test_df['id'], 'target': y_pred2})\n\ntoday = datetime.date.today().strftime('%Y%m%d')\nsubmit_df2.to_csv(f'{today}_2_submit.csv', index=False)","fc06c5a1":"train_df2 = train_df.copy()\ntest_df2 = test_df.copy()\n\ny = train_df2['target']\n\nconcat_df = pd.concat([train_df2, test_df2], axis=0)\ncat_columns = train_df2.columns[1: 11]\n\nfor column in cat_columns:\n  concat_df[column] = LabelEncoder().fit_transform(concat_df[column])\n\ntrain_enc_df2 = concat_df[:300000]\nX3 = train_enc_df2.drop(['id', 'target'], axis=1)\ny3 = train_enc_df2['target']\n\ntest_enc_df2 = concat_df[300000:].drop(['id', 'target'], axis=1)\ntest_enc_df2","7c6c7768":"X_train3, X_val3, y_train3, y_val3 = train_test_split(\n    X3, y3, test_size=0.3, random_state=42,\n)","b62b60a9":"feature_name = list(X3.columns)\nfeature_name","7078fc1c":"train_data3 = lgb.Dataset(X_train3, y_train3, feature_name=feature_name, categorical_feature=feature_name[:10])\nval_data3 = lgb.Dataset(X_val3, y_val3, reference=train_data3)","6e37d597":"params = {'metric': 'rmse'}\n\nstart = time.time()\nbst3 = lgb.train(params=params, train_set=train_data3, valid_sets=val_data3, num_boost_round=500, early_stopping_rounds=10)\nprint(f'elapsed time : {time.time() - start}')","af9ba7cc":"y_pred3 = bst3.predict(test_enc_df2, num_iteration=bst3.best_iteration)\n\nsubmit_df3 = pd.DataFrame({'id': test_df['id'], 'target': y_pred3})\n\ntoday = datetime.date.today().strftime('%Y%m%d')\nsubmit_df3.to_csv(f'{today}_3_submit.csv', index=False)","61fd1bd1":"## Simple OneHotEncoding\n- I was doing processing to not increase the number of features, but I put all the categorical variables into the model with OneHotEncoding.","2b1c586b":"## Install Libraries","f8587def":"## Categorical features\n- According to [documentetion of LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-Intro.html), we can see that.\n- LightGBM can use categorical features as input directly. It doesn\u2019t need to convert to one-hot coding, and is much faster than one-hot coding (about 8x speed-up).\n- So, I try it.","e79a944a":"# EDA + Basic Model by LightGBM","d4dd97bc":"- The categorical variable has 15 letters of the alphabet from A to O, and the number of alphabets used is different between each feature quantity.","ee8650c1":"- First, lets's check the outline of the data.\n- All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous.","7ad36e3f":"- Public Score is 0.84669.\u3000This is Improved by about 0.003.\n- But processing time is increasing.","0bee8e24":"- The median value of the target variable is around 7, but you can see that there are a certain number of outliers.","772300fe":"## EDA","d7fb736b":"## Prediction and Submission","c16c2907":"\u3000- You should also check for missing values. You can see that there are no missing values","2cd1c6b3":"- Public Score is 0.84988.","a884b608":"## Model Training\n- Divide the data into training data and validation data and validate with the model.\n- The model used is LightGBM.\u3000[documentationn](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-Intro.html)","fcb6d758":"## Preprocessing\n- Preprocess the data to put it in the model.\n- Normally, categorical variables cannot be submitted directly into the model.\nIt is necessary to convert the categorical variables to numerical values, but since there is no order between the categorical variables, they are converted by'One Hot Encoding'.\n- When OneHotEncoding is performed, the column of each feature is increased by the number of categorical variables.\n- In this data, the features of each categorical variable are limited to A-M, so by adding the transformations of each column, the features of cat0-cat9 are converted to 15 features.\n- Eventually scale to 0-1 to match continuous variables and tones","67ff3edd":"- The continuous variables seem to be distributed in the range 0-1.","38e8f5a5":"- Public Score is 0.84608. It is slightly improved.\n- And processing time is decreasing."}}