{"cell_type":{"50fa9533":"code","3f7612e9":"code","bc7f8394":"code","92af5735":"code","5995ee6f":"code","0eb91849":"code","be71e0ce":"code","a1901723":"code","5271236b":"code","029f9251":"code","c78ca7b9":"code","65acdcd3":"code","70a5376d":"code","f7ac57a5":"code","d44e0ba9":"code","0cb9c0cf":"code","9de09900":"code","f71538b9":"code","e19b5015":"code","73e10144":"code","1e231ec8":"code","80b5719b":"code","4e044a09":"markdown","7f2b71c7":"markdown","5ce09139":"markdown","10c8c84c":"markdown","100aafe5":"markdown","8575f568":"markdown","2d59d452":"markdown","b96f0fa7":"markdown","d096eb0e":"markdown","8e505ec7":"markdown","eff37106":"markdown","e1ed2899":"markdown","597bdcfd":"markdown"},"source":{"50fa9533":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport pyarrow.parquet as pq\nimport tqdm\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport gc\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3f7612e9":"class Config:\n    META_TRAIN='..\/input\/metadata_train.csv'\n    META_TEST='..\/input\/metadata_test.csv'\n    TRAIN_SRC='..\/input\/train.parquet'\n    TEST_SRC='..\/input\/test.parquet'\n    TRAIN_ARRAYS_STRATIFIED = 'train.npz'\n    TEST_ARRAYS = 'test.npz'","bc7f8394":"# in all functions below use keyword argument use_np to use numpy, \n# if False then tensorflow is used!\n\ndef _is_equal(a, b, use_np=False):\n    # Equality or not, returned in type Float\n    if use_np:\n        return np.equal(a, b).astype(np.float32)\n    return tf.cast(tf.equal(a, b), tf.float32)\n\ndef _is_not_equal(a, b, use_np=False):\n    # non-Equality or not, returned in type Float\n    if use_np:\n        return np.not_equal(a, b).astype(np.float32)\n    return tf.cast(tf.not_equal(a, b), tf.float32)\n\ndef true_positives(y, y_preds, use_np=False):\n    correct_preds = _is_equal(y, y_preds, use_np=use_np)\n    poss = _is_equal(y, 1, use_np=use_np)\n    if use_np:\n        return np.sum(correct_preds * poss)\n    return tf.reduce_sum(correct_preds * poss)\n\ndef true_negatives(y, y_preds, use_np=False):\n    correct_preds = _is_equal(y, y_preds, use_np=use_np)\n    negs = _is_equal(y, 0, use_np=use_np)\n    if use_np:\n        return np.sum(correct_preds * negs)\n    return tf.reduce_sum(correct_preds * negs)\n\ndef false_positives(y, y_preds, use_np=False):\n    incorrect_preds = _is_not_equal(y, y_preds, use_np=use_np)\n    negs = _is_equal(y, 0, use_np=use_np)\n    if use_np:\n        return np.sum(incorrect_preds * negs)\n    return tf.reduce_sum(incorrect_preds * negs)\n\ndef false_negatives(y, y_preds, use_np=False):\n    incorrect_preds = _is_not_equal(y, y_preds, use_np=use_np)\n    poss = _is_equal(y, 1, use_np=use_np)\n    if use_np:\n        return np.sum(incorrect_preds * poss)\n    return tf.reduce_sum(incorrect_preds * poss)\n\ndef _get_inter_metrics(y, y_preds, use_np=False):\n    tp = true_positives(y, y_preds, use_np=use_np)\n    tn = true_negatives(y, y_preds, use_np=use_np)\n    fp = false_positives(y, y_preds, use_np=use_np)\n    fn = false_negatives(y, y_preds, use_np=use_np)\n    return tp, tn, fp, fn\n\ndef mcc(y, y_preds, use_np=False):\n    tp, tn, fp, fn = _get_inter_metrics(y, y_preds, use_np=use_np)\n    num = (tp * tn) - (fp * fn)\n    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    if use_np:\n        den_sqrt = np.sqrt(den + 1e-7)\n    else:\n        den_sqrt = tf.sqrt(den + 1e-7)\n    return num \/ den_sqrt","92af5735":"def read_table(src, start, end):\n    df = pq.read_pandas(src, columns=[str(i) for i in range(start, end)]).to_pandas()\n    return df.values\n\ndef _standardize(data, min, max):\n    return (data - min) \/ (max - min)\n\ndef normalize(data, cur_min_max, new_min_max):\n    standardized = _standardize(data,\n                                cur_min_max[0],\n                                cur_min_max[1])\n    new_min, new_max = new_min_max\n    return standardized * (new_max - new_min) + new_min\n\ndef transform_and_binify(data,\n                         sample_size,\n                         bins=160,\n                         cur_min_max=(-128, 127),\n                         min_max=(-1, 1)):\n    min, max = cur_min_max\n\n    data_normed = normalize(data, (min, max), min_max)\n    bucket_size = int(sample_size \/ bins)\n\n    new_data = []\n    for i in range(0, sample_size, bucket_size):\n        data_slice = data_normed[i:i+bucket_size]\n        mean = np.expand_dims(data_slice.mean(axis=0), axis=0)\n        std = np.expand_dims(data_slice.std(axis=0), axis=0)\n        std_1_away_right = mean + std\n        std_1_away_left = mean - std\n\n        percentile_calc = np.percentile(data_slice, [0, 1, 25, 50, 75, 99, 100], axis=0)\n        percentile_range = np.expand_dims(percentile_calc[-1] - percentile_calc[0], axis=0)\n        relative_percentile = percentile_calc - mean\n\n        new_data.append(np.concatenate([\n            mean, std, std_1_away_right,\n            std_1_away_left, percentile_range,\n            percentile_calc, relative_percentile\n        ]))\n    return np.expand_dims(np.asarray(new_data), axis=0)\n\ndef load_train_meta(path):\n    meta_df = pd.read_csv(path)\n    ids = [i for i in range(meta_df.shape[0] \/\/ 3)]\n    meta_df.set_index(['id_measurement', 'phase'], inplace=True)\n    targets = np.array([np.array([meta_df.loc[id].loc[0]['target'], \n                         meta_df.loc[id].loc[1]['target'], \n                         meta_df.loc[id].loc[2]['target']])\n                        for id in ids])\n    return ids, targets\n\ndef load_and_preprocess(meta_path,\n                        src_path,\n                        is_train=True,\n                        do_strat_split=True,\n                        val_size=0.1,\n                        random_state=42):\n    if do_strat_split:\n        assert is_train, 'do_strat_split can be True only if is_train is True.'\n\n    if is_train:\n        ids, targets = load_train_meta(meta_path)\n        start, end = 0, 8712\n    else:\n        meta_df = pd.read_csv(meta_path)\n        signal_ids = meta_df['signal_id'].values\n        start, end = 8712, meta_df.shape[0] + 8712\n\n    X = []\n    for i in tqdm.tqdm(range(start, end, 3)):\n        pq_data = read_table(src_path, i, i+3)\n        X.append(transform_and_binify(pq_data, 800000))\n    X = np.concatenate(X)\n\n    if do_strat_split:\n        ((X_train, Y_train),\n         (X_val, Y_val)) = _strata_split_helper(X, targets, val_size, random_state)\n        return (X_train, Y_train), (X_val, Y_val)\n\n    if is_train:\n        return X, targets\n    return X, signal_ids\n\ndef _strata_split_helper(X, Y, val_size, random_state):\n    tmp_targets = []\n    # each target in targets is a 3 elelment array, one for each phase\n    for target in Y:\n        # if any 1 target is non-zero\n        if np.sum(target) != 0:\n            # if the targets isn't all 1 then give it class 2 else give class 1\n            if np.sum(target) != 3:\n                tmp_targets.append(2)\n            else:\n                tmp_targets.append(1)\n        else:\n            tmp_targets.append(0)\n    train_indices, val_indices = stratified_split(X, tmp_targets,\n                                                  val_size, random_state)\n    return (X[train_indices], Y[train_indices]), (X[val_indices], Y[val_indices])\n\ndef stratified_split(X, Y, val_size, random_state):\n    sss = StratifiedShuffleSplit(n_splits=1,\n                                 test_size=val_size,\n                                 random_state=random_state)\n    return next(sss.split(X, Y))\n\ndef np_save(path, **arrays):\n    np.savez(path, **arrays)","5995ee6f":"((X_train, Y_train), (X_val, Y_val)) = load_and_preprocess(Config.META_TRAIN, Config.TRAIN_SRC)\nnp_save(Config.TRAIN_ARRAYS_STRATIFIED,\n        X_train=X_train, Y_train=Y_train,\n        X_val=X_val, Y_val=Y_val)\n\ndel X_train\ndel Y_train\ndel Y_val\ndel X_val\ngc.collect()\n\nX, signal_id = load_and_preprocess(Config.META_TEST, Config.TEST_SRC, False, False)\nnp_save(Config.TEST_ARRAYS, X=X, signal_id=signal_id)\ndel X\ndel signal_id\ngc.collect()","0eb91849":"def shuffle_repeat_applier(dataset, buffer_size, shuffle, repeat):\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size)\n    else:\n        dataset = dataset.prefetch(buffer_size)\n\n    if repeat:\n        dataset = dataset.repeat()\n    else:\n        dataset = dataset.repeat(1)\n\n    return dataset\n\ndef input_fn(X, Y=None, batch_size=32, buffer_size=2000,\n             shuffle=True, repeat=True):\n    if Y is not None:\n        dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((X, ))\n\n    dataset = shuffle_repeat_applier(dataset, buffer_size, shuffle, repeat)\n    dataset = dataset.batch(batch_size)\n\n    if Y is not None:\n        X, Y = dataset.make_one_shot_iterator().get_next()\n    else:\n        X = dataset.make_one_shot_iterator().get_next()[0]\n\n    features_dic = {'signal': tf.cast(X, tf.float32)}\n    if Y is not None:\n        labels_dic = {\n            'phase1': tf.one_hot(Y[:, 0], depth=2),\n            'phase2': tf.one_hot(Y[:, 1], depth=2),\n            'phase3': tf.one_hot(Y[:, 2], depth=2)\n        }\n        return features_dic, labels_dic\n    return features_dic","be71e0ce":"from tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras.regularizers import l2\n\ndef conv_bn_relu(in_tensor,\n                 filters,\n                 kernel_size,\n                 strides,\n                 padding='valid',\n                 weight_decay=5e-4):\n    return models.Sequential([\n        layers.Conv2D(filters, kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer='he_normal',\n                      kernel_regularizer=l2(weight_decay)),\n        layers.BatchNormalization(),\n        layers.Activation('relu')\n    ])(in_tensor)\n\ndef _make_summaries_helper(labels,\n                           phase1_logits,\n                           phase2_logits,\n                           phase3_logits,\n                           metric_func,\n                           summary_name):\n    phase1 = metric_func(tf.argmax(labels['phase1'], axis=1),\n                         tf.argmax(phase1_logits, axis=1))\n    phase2 = metric_func(tf.argmax(labels['phase2'], axis=1),\n                         tf.argmax(phase2_logits, axis=1))\n    phase3 = metric_func(tf.argmax(labels['phase3'], axis=1),\n                         tf.argmax(phase3_logits, axis=1))\n\n    tf.summary.scalar(summary_name + '_phase1', phase1)\n    tf.summary.scalar(summary_name + '_phase2', phase2)\n    tf.summary.scalar(summary_name + '_phase3', phase3)\n    return phase1, phase2, phase3\n\ndef make_summaries(labels,\n                   phase1_logits,\n                   phase2_logits,\n                   phase3_logits):\n    _make_summaries_helper(labels, phase1_logits,\n                           phase2_logits, phase3_logits,\n                           true_positives, 'tp')\n    _make_summaries_helper(labels, phase1_logits,\n                           phase2_logits, phase3_logits,\n                           true_negatives, 'tn')\n    _make_summaries_helper(labels, phase1_logits,\n                           phase2_logits, phase3_logits,\n                           false_positives, 'fp')\n    _make_summaries_helper(labels, phase1_logits,\n                           phase2_logits, phase3_logits,\n                           false_negatives, 'fn')\n    mcc1, mcc2, mcc3 = _make_summaries_helper(labels, phase1_logits,\n                                              phase2_logits, phase3_logits,\n                                              mcc, 'mcc')\n    return mcc1, mcc2, mcc3\n\ndef model_fn(features, labels, mode, params):\n    conv_bn_relu1 = conv_bn_relu(features['signal'], 32, (7, 3), (2, 1))\n    conv_bn_relu2 = conv_bn_relu(conv_bn_relu1, 64, (7, 3), (2, 1))\n    conv_bn_relu3 = conv_bn_relu(conv_bn_relu2, 128, (7, 3), (2, 1))\n    conv_bn_relu4 = conv_bn_relu(conv_bn_relu3, 256, (3, 3), (2, 2))\n    conv_bn_relu5 = conv_bn_relu(conv_bn_relu4, 512, (3, 3), (2, 2))\n\n    pool = tf.keras.layers.GlobalAveragePooling2D()(conv_bn_relu5)\n    dense1 = tf.keras.layers.Dense(128, activation='relu')(pool)\n    dropout = tf.keras.layers.Dropout(rate=params['drop_rate'])(dense1)\n\n    phase1_logits = tf.keras.layers.Dense(2)(dropout)\n    phase2_logits = tf.keras.layers.Dense(2)(dropout)\n    phase3_logits = tf.keras.layers.Dense(2)(dropout)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        preds = {\n            'phase1': tf.argmax(phase1_logits, axis=1),\n            'phase2': tf.argmax(phase2_logits, axis=1),\n            'phase3': tf.argmax(phase3_logits, axis=1)\n        }\n        spec = tf.estimator.EstimatorSpec(mode=mode, predictions=preds)\n    else:\n        \n        mcc1, mcc2, mcc3 = make_summaries(labels, phase1_logits, phase2_logits, phase3_logits)\n        logging_hook = tf.train.LoggingTensorHook({\n            \"mcc_phase1\": mcc1,\n            \"mcc_phase2\": mcc2,\n            \"mcc_phase3\": mcc3\n        }, every_n_iter=15)\n\n        costs_pahse1 = tf.losses.softmax_cross_entropy(labels['phase1'], phase1_logits)\n        costs_pahse2 = tf.losses.softmax_cross_entropy(labels['phase2'], phase2_logits)\n        costs_pahse3 = tf.losses.softmax_cross_entropy(labels['phase3'], phase3_logits)\n\n        costs = costs_pahse1 + costs_pahse2 + costs_pahse3\n        loss = tf.reduce_mean(costs)\n\n        global_step = tf.train.get_global_step()\n        optimizer = tf.train.AdamOptimizer(params['lr'])\n        train_op = optimizer.minimize(loss,\n                                      global_step=global_step)\n\n        spec = tf.estimator.EstimatorSpec(mode=mode, loss=loss,\n                                          train_op=train_op, training_hooks=[logging_hook])\n    return spec","a1901723":"def get_preds(pred_generator):\n    preds = []\n    for pred in pred_generator:\n        preds.append(pred['phase1'])\n        preds.append(pred['phase2'])\n        preds.append(pred['phase3'])\n    return preds\n\ndef train(model, X, Y, steps):\n    train_input_fn = lambda: input_fn(X, Y)\n    model.train(input_fn=train_input_fn, steps=steps)\n\ndef eval(model, X, Y):\n    eval_input_fn = lambda: input_fn(X, Y, shuffle=False, repeat=False)\n    return model.evaluate(input_fn=eval_input_fn)\n\ndef predict(model, X):\n    predict_input_fn = lambda: input_fn(X, shuffle=False, repeat=False)\n    return get_preds(model.predict(predict_input_fn))\n\ndef eval_mcc(model, X, Y):\n    preds = predict(model, X)\n    return mcc(Y, preds, True)","5271236b":"BATCH_SIZE = 32\nEPOCHS = 40\nLR = 1e-5\nDROP_RATE = 0.4\n\nRUN_CONFIG = tf.estimator.RunConfig(tf_random_seed=42,\n                                    save_summary_steps=200,\n                                    keep_checkpoint_max=3)","029f9251":"model = tf.estimator.Estimator(model_fn=model_fn,\n                               params={\n                                   'lr': LR,\n                                   'drop_rate': DROP_RATE\n                               }, model_dir='.\/model\/',\n                               config=RUN_CONFIG)","c78ca7b9":"npzfile = np.load(Config.TRAIN_ARRAYS_STRATIFIED)\nnpzfile.files\n\nX_train = npzfile['X_train']\nY_train = npzfile['Y_train']\nX_val = npzfile['X_val']\nY_val = npzfile['Y_val']","65acdcd3":"STEPS = EPOCHS * (X_train.shape[0] \/\/ BATCH_SIZE)\nSTEPS","70a5376d":"train(model, X_train, Y_train, STEPS)","f7ac57a5":"del model\nmodel = tf.estimator.Estimator(model_fn=model_fn,\n                               params={\n                                   'lr': LR,\n                                   'drop_rate': 0\n                               }, model_dir='.\/model\/',\n                               config=RUN_CONFIG)","d44e0ba9":"def flatten(Y):\n    new_y = []\n    for y in Y:\n        new_y.extend(list(y))\n    return np.array(new_y)","0cb9c0cf":"print('Loss on Train Data:{}'.format(eval(model, X_train, Y_train)))\nprint('MCC on Train Data:{}'.format(eval_mcc(model, X_train, flatten(Y_train))))","9de09900":"print('Loss on Val Data:{}'.format(eval(model, X_val, Y_val)))\nprint('MCC on Val Data:{}'.format(eval_mcc(model, X_val, flatten(Y_val))))","f71538b9":"del model\nmodel = tf.estimator.Estimator(model_fn=model_fn,\n                               params={\n                                   'lr': LR,\n                                   'drop_rate': 0.4\n                               }, model_dir='.\/model\/',\n                               config=RUN_CONFIG)","e19b5015":"EPOCHS = 1\nSTEPS = X_val.shape[0] \/\/ BATCH_SIZE\n\ntrain(model, X_val, Y_val, STEPS)","73e10144":"del X_train\ndel Y_train\ndel X_val\ndel Y_val\ngc.collect()\n\ndel model\nmodel = tf.estimator.Estimator(model_fn=model_fn,\n                               params={\n                                   'lr': LR,\n                                   'drop_rate': 0\n                               }, model_dir='.\/model\/',\n                               config=RUN_CONFIG)","1e231ec8":"npzfile3 = np.load(Config.TEST_ARRAYS)\nX = npzfile3['X']\nsignal_ids = npzfile3['signal_id']\n\n\npreds = predict(model, X)","80b5719b":"df = pd.DataFrame({\n    'signal_id': list(signal_ids),\n    'target': list(preds)\n})\n\ndf.to_csv('submission.csv', index=False)","4e044a09":"Now let's have some helper functions for training, evaluation, mcc evaluation and prediction.\n\nThe estimator API returns predictions in form a generator. As we returned a dictionary of predictions for each phase, we will get a generator each value of which will be a dictionary.","7f2b71c7":"Finally, let's train this model!","5ce09139":"Time for final prediction and submission!","10c8c84c":"Let's define our model.\n\nWe will be defining a 9 layers model (excluding Dropout and Global Pool):\n* The first 5 layers will be simple convolution, batchnormalization and relu stack. So we will define a function to encapsulate this stack. The Sequential model of keras will come handy here.\n* Then we will apply a Global Pool. This will result in a output of (Batch Size, 512). \n* The above output will pass through a dense layer which then passes outputs into a dropout layer, with drop rate = 0.4\n* Finally we will have three dense layers, one for each phase, with 2 outputs (0 and 1).\n\nThe estimator model is defined in a function, `model_fn` in this case. This function always have 4 args: `features`, `labels`, `mode`, `params`:\n* `feartures`: The input tensor to the model is passed using this arg. It is usually a dictionary that has its values as the feature or features that the model needs to learn on.\n* `labels`: As the name suggests, this will be the argument that holds our labels.\n* `mode`; The estimator API has three mode keys `tf.estimator.ModeKeys.TRAIN`, `tf.estimator.ModeKeys.EVAL` and `tf.estimator.ModeKeys.PREDICT`. Depending on which key is passed here we need to define the output spec (`tf.estimator.EstimatorSpec`) of our model.\n* `params`: Our model will be in need of special case sensitive values (hyperparams), like learning rate, drop rate and so on. These will passed as a dictionary in ths arg.\n\nI'll also make use of tensorflow summaries to plot values of special metrics in the tensorboard.","100aafe5":"Creating a CNN that works using TensorFlow's Estimator and Dataset API. First let's make some constants available.","8575f568":"The training of an estimator is done on number of steps, not on epochs, so we will calculate number of steps here","2d59d452":"Preprocessing the data. This is inspired from https:\/\/www.kaggle.com\/braquino\/5-fold-lstm-attention-fully-commented-0-694\n\nI have tweaked the code to make sure that each phase gives a separate target. Since we have 3 phases so we will have three channels in train input and 3 labels in each data point.\n\nIn my base model my stratefied split got comparable performance on public score. So, I'll be using stratified splitting. I have three classes in stratified split. class 0 all the three labels are 0, class 1 all the three labels are 1 and class 2 the three labels are different","b96f0fa7":"Now, we are at the fun stage. There are two parts to a TensorFlow estimator:\n1. Estimator model\n2. Estimator Input function (read input pipeline)\n\nWe will first fo through the pipeline.\n\nThe pipeline is the function `input_fn`. It will take in our desired settings and give us an iterator that can be used by the estimator model to run through the data.\n\nIn case of training:\n* Input pipeline will need both X and Y along with batch size.\n* We will allow shuffling for better training\n* We will allow the iterator to repeat data. This will make sure that the iterator iterates over data for  infinite number of times (till we train the model).\n* We will have buffer_size, which will make sure that only required number of data is pushed into memory.\n\nIn case of evaluation, everything will be same except that we do not repeat the data. This means the iterator will iterate over data only once.\n\nIn case of prediction everything will be same except that we neither repeat nor shuffle the data.\n\nThe iterators returned by the function are in form of dictionaries. This makes sure that we can access them using sensible naming conventions.","d096eb0e":"Let's define the Matthews correlation coefficient (MCC) in both numpy and tensorlfow","8e505ec7":"Let's train model on the left out data, for final submission","eff37106":"Our model needs to have a drop rate of 0 to predict. Hence defining the model again. Don't worry, it will load the settings from the checkpoints","e1ed2899":"Let's define some constants that will help us at time of training.\n\nI've used `tf.estimator.RunConfig` to define extra configurations for the model. Estimator API saves checkpoints. The default configurations were causing out of memory issues on kaggle kernel Disk. \\\\_(^_^)_\/","597bdcfd":"Preprocessing data using functions defined above"}}