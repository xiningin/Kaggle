{"cell_type":{"101d87be":"code","8a1e0f8e":"code","6151164c":"code","249884a3":"code","e1f920e4":"code","8e765842":"code","a2fedb63":"code","5cc72037":"code","ccd2d943":"code","0c56fce0":"code","4724b7e1":"code","70d4e638":"code","ce7dcf5e":"code","0d121746":"code","a6d60d2b":"code","c81b03ab":"code","f96a543d":"code","6b279db6":"code","be55a875":"code","506dc133":"code","6b1ccd13":"code","1a0b9de9":"code","b1760a71":"code","749c5607":"code","ff96333b":"code","ce677332":"code","2c1a5f9c":"code","c3d7eaec":"code","5ba0b188":"code","0d5c0c7f":"code","9680ce41":"code","86ec293a":"code","b8f648af":"code","b53fd8b2":"markdown","eaab9a47":"markdown","e8789d1b":"markdown","0d4bd607":"markdown","5eebdb5e":"markdown","90c2d816":"markdown","30fe140b":"markdown","33f64a3f":"markdown","eef0d5a6":"markdown","50767f7e":"markdown","abc98b71":"markdown","81ed6818":"markdown","49490077":"markdown","724eca3c":"markdown","3745212e":"markdown","85505ba7":"markdown","1009a19a":"markdown"},"source":{"101d87be":"!pip install -q keras==2.3.1\n!pip install -q tensorflow==2.1.0","8a1e0f8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6151164c":"import tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow import keras\n\nimport glob\nimport os\n\n\nfrom keras import Input\nfrom keras.applications import VGG19\nfrom keras.callbacks import TensorBoard\nfrom keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense\nfrom keras.layers import Conv2D, UpSampling2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\n\nimport random\nfrom numpy import asarray\nfrom itertools import repeat\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","249884a3":"## Parameters\n\nTrain_path = '..\/input\/div2k-dataset\/DIV2K_train_HR\/DIV2K_train_HR'\nprint(\"Number of files in train directory => \", len(os.listdir(Train_path)))\n\nVal_path = '..\/input\/div2k-dataset\/DIV2K_valid_HR\/DIV2K_valid_HR'\nprint(\"Number of files in val directory => \", len(os.listdir(Val_path)))","e1f920e4":"epochs = 10\n\nbatch_size = 8\n\nlow_resolution_shape = (64,64,3)\n\nhigh_resolution_shape = (256,256,3)\n\ncommon_optimizer = Adam(0.0002, 0.5)\n\nSEED = 2021\ntf.random.set_seed(SEED)","8e765842":"def compute_psnr(original_image, generated_image):\n    \n    original_image = tf.convert_to_tensor(original_image, dtype = tf.float32)\n    generated_image = tf.convert_to_tensor(generated_image, dtype = tf.float32)\n    \n    psnr = tf.image.psnr(original_image, generated_image, max_val = 1.0)\n    \n    return tf.math.reduce_mean(psnr, axis = None, keepdims = False, name = None)","a2fedb63":"def plot_psnr(psnr):\n    \n    psnr_means = psnr['psnr_quality']\n    plt.figure(figsize = (10,8))\n    \n    plt.plot(psnr_means)\n    plt.xlabel('Epochs')\n    plt.ylabel('PSNR')\n    plt.title('PSNR')\n    ","5cc72037":"def compute_ssim(original_image, generated_image):\n    \n    original_image = tf.convert_to_tensor(original_image, dtype = tf.float32)\n    generated_image = tf.convert_to_tensor(generated_image, dtype = tf.float32)\n    \n    ssim = tf.image.ssim(original_image, generated_image, max_val = 1.0, filter_size = 11, filter_sigma = 1.5, k1 = 0.01, )\n    \n    return tf.math.reduce_mean(ssim, axis = None, keepdims = False, name = None)","ccd2d943":"def plot_ssim(ssim):\n    \n    ssim_means = ssim['ssim_quality']\n    \n    plt.figure(figsize = (10,8))\n    plt.plot(ssim_means)\n    plt.xlabel('Epochs')\n    plt.ylabel('SSIM')\n    plt.title('SSIM')\n    ","0c56fce0":"def plot_loss(losses):\n    \n    d_loss = losses['d_history']\n    g_loss = losses['g_history']\n    \n    plt.figure(figsize = (10,8))\n    plt.plot(d_loss, label = \"Discriminator loss\")\n    plt.plot(g_loss, label = \"Generator Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('Loss')\n    plt.title(\"Loss\")\n    plt.legend()\n    ","4724b7e1":"from imageio import imread","70d4e638":"def sample_images(image_list, batch_size, high_resolution_shape, low_resolution_shape):\n    \n    images_batch = np.random.choice(image_list, size = batch_size)\n    \n    lr_images = []\n    hr_images = []\n    \n    for img in images_batch:\n        \n        img1 = imread(img, as_gray = False, pilmode = 'RGB')\n        img1= img1.astype(np.float32)\n        \n        \n        # change the size\n        img1_high_resolution = imresize(img1, high_resolution_shape)\n        img1_low_resolution = imresize(img1, low_resolution_shape)\n        \n        if np.random.random() < 0.5:\n            img1_high_resolution=np.fliplr(img1_high_resolution)\n            img1_low_resolution = np.fliplr(img1_low_resolution)\n            \n        hr_images.append(img1_high_resolution)\n        lr_images.append(img1_low_resolution)\n    \n    return np.array(hr_images), np.array(lr_images)","ce7dcf5e":"def residual_block(x):\n    \n    \n    filters = [64,64]\n    kernel_size = 3\n    strides = 1\n    padding = \"same\"\n    momentum = 0.8\n    activation = \"relu\"\n    \n    \n    res = Conv2D(filters = filters[0], kernel_size = kernel_size, strides = strides, padding = padding)(x)\n    res = Activation(activation = activation)(res)\n    res = BatchNormalization(momentum = momentum)(res)\n    \n    \n    res = Conv2D(filters = filters[1], kernel_size = kernel_size, strides = strides, padding = padding)(res)\n    res = BatchNormalization(momentum = momentum)(res)\n    \n    res = Add()([res, x])\n\n    return res","0d121746":"def build_generator():\n    \n    \n    residual_blocks = 16\n    \n    momentum = 0.8\n    \n    \n    # 4x downsample of HR\n    input_shape = (64,64,3)\n    \n    \n    input_layer = Input(shape = input_shape)\n    \n    gen1 = Conv2D(filters = 64, kernel_size = 9, strides=  1, padding = 'same', activation = 'relu')(input_layer)\n    \n    \n    # 16 residual blocks\n    res = residual_block(gen1)\n    for i in range(residual_blocks - 1):\n        res = residual_block(res)\n        \n    \n    gen2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(res)\n    gen2 = BatchNormalization(momentum = momentum)(gen2)\n    \n    \n    # take the sum of pre-residual block (gen1) and post-residual block (gen2)\n    gen3 = Add()([gen2, gen1])\n    \n    \n    # upsampling\n    gen4 = UpSampling2D(size = 2)(gen3)\n    gen4 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(gen4)\n    gen4 = Activation('relu')(gen4)\n    \n    # upsampling\n    gen5 = UpSampling2D(size = 2)(gen4)\n    gen5 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(gen5)\n    gen5 = Activation('relu')(gen5)\n    \n    # Output Image 3 channels RGB\n    gen6 = Conv2D(filters = 3, kernel_size = 9, strides = 1, padding = 'same')(gen5)\n    output = Activation('tanh')(gen6)\n    \n    # model\n    model = Model(inputs = [input_layer], outputs = [output], name = 'generator')\n    \n    return model\n    ","a6d60d2b":"generator = build_generator()","c81b03ab":"def build_discriminator():\n    \n    \n    leakyrelu_alpha = 0.2\n    momentum = 0.8\n    \n    # the input is the HR shape\n    input_shape = (256, 256, 3)\n    \n    # input layer for discriminator\n    input_layer = Input(shape=input_shape)\n    \n    # 8 convolutional layers with batch normalization  \n    dis1 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(input_layer)\n    dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n\n    dis2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(dis1)\n    dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n    dis2 = BatchNormalization(momentum=momentum)(dis2)\n\n    dis3 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same')(dis2)\n    dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n    dis3 = BatchNormalization(momentum=momentum)(dis3)\n\n    dis4 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis3)\n    dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n    dis4 = BatchNormalization(momentum=0.8)(dis4)\n\n    dis5 = Conv2D(256, kernel_size=3, strides=1, padding='same')(dis4)\n    dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n    dis5 = BatchNormalization(momentum=momentum)(dis5)\n\n    dis6 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(dis5)\n    dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n    dis6 = BatchNormalization(momentum=momentum)(dis6)\n\n    dis7 = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(dis6)\n    dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n    dis7 = BatchNormalization(momentum=momentum)(dis7)\n\n    dis8 = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(dis7)\n    dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n    dis8 = BatchNormalization(momentum=momentum)(dis8)\n    \n    # fully connected layer \n    dis9 = Dense(units=1024)(dis8)\n    dis9 = LeakyReLU(alpha=0.2)(dis9)\n    \n    # last fully connected layer - for classification \n    output = Dense(units=1, activation='sigmoid')(dis9)   \n    \n    model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n    \n    return model","f96a543d":"discriminator = build_discriminator()\ndiscriminator.trainable = True\ndiscriminator.compile(loss = \"mse\", optimizer = common_optimizer, metrics = ['accuracy'])","6b279db6":"!wget https:\/\/github.com\/fchollet\/deep-learning-models\/releases\/download\/v0.1\/vgg19_weights_tf_dim_ordering_tf_kernels.h5","be55a875":"VGG19_base = VGG19(weights = \".\/vgg19_weights_tf_dim_ordering_tf_kernels.h5\")","506dc133":"VGG19_base.summary()","6b1ccd13":"def build_VGG19():\n    \n    input_shape = (256, 256, 3)\n    VGG19_base.outputs = [VGG19_base.get_layer('block5_conv2').output]\n    #VGG19_base.outputs = [VGG19_base.layers[9].output]\n    input_layer = Input(shape=input_shape)\n    features = VGG19_base(input_layer)\n    model = Model(inputs=[input_layer], outputs=[features])\n    \n    return model","1a0b9de9":"## Combining them all to build adversarial network\n\n","b1760a71":"fe_model = build_VGG19()\nfe_model.trainable = False\nfe_model.compile(loss = 'mse', optimizer = common_optimizer, metrics = ['accuracy'])","749c5607":"## Final Adversarial Network\n\ndef build_adversarial_model(generator, discriminator, feature_extractor):\n    \n    input_high_resolution = Input(shape = high_resolution_shape)\n    \n    input_low_resolution = Input(shape = low_resolution_shape)\n    \n    generated_high_resolution_images = generator(input_low_resolution)\n    \n    features = feature_extractor(generated_high_resolution_images)\n    \n    # make a discriminator non trainable\n    discriminator.trainable = False\n    discriminator.compile(loss = 'mse', optimizer = common_optimizer, metrics = ['accuracy'])\n    \n    # discriminator will give a prob estimation for generated high-resolution images\n    probs = discriminator(generated_high_resolution_images)\n    \n    # create and compile\n    adversarial_model = Model([input_low_resolution, input_high_resolution], [probs, features])\n    adversarial_model.compile(loss = ['binary_crossentropy', 'mse'], loss_weights = [1e-3, 1], optimizer = common_optimizer)\n    \n    return adversarial_model\n    \n    ","ff96333b":"adversarial_model = build_adversarial_model(generator, discriminator, fe_model)","ce677332":"losses = {'d_history' : [], \"g_history\": []}\n\npsnr = {'psnr_quality' : []}\nssim = {'ssim_quality' : []}","2c1a5f9c":"from glob import glob\n\ndef get_train_images():\n    \n    #image_list = glob('..\/input\/div2k-dataset\/DIV2K_train_HR\/DIV2K_train_HR\/*')\n    image_list = glob('..\/input\/dataset-image-super-resolution\/finished\/train\/dataraw\/hires\/*')\n    return image_list","c3d7eaec":"#hr_images, lr_images = sample_images(image_list, batch_size = batch_size, low_resolution_shape = low_resolution_shape, high_resolution_shape = high_resolution_shape)","5ba0b188":"from matplotlib import pyplot as plt\ndef save_images(original_image, lr_image, sr_image, path):\n    \n    \"\"\"\n    Save LR, HR (original) and generated SR\n    images in one panel \n    \"\"\"\n    \n    fig, ax = plt.subplots(1,3, figsize=(10, 6))\n\n    images = [original_image, lr_image, sr_image]\n    titles = ['HR', 'LR','SR - generated']\n\n    for idx,img in enumerate(images):\n        # (X + 1)\/2 to scale back from [-1,1] to [0,1]\n        ax[idx].imshow((img + 1)\/2.0, cmap='gray')\n        ax[idx].axis(\"off\")\n    for idx, title in enumerate(titles):    \n        ax[idx].set_title('{}'.format(title))\n        \n    plt.savefig(path)    ","0d5c0c7f":"epochs = 3\nfrom PIL import Image\nfrom skimage.transform import resize as imresize\n\nfor epoch in range(epochs):\n    \n    print(\"Epoch Number -> \" , epoch)\n    d_history = []\n    g_history = []\n    \n    image_list = get_train_images()\n    \n    \"\"\"\n    Discriminator training\n    \"\"\"\n    \n    hr_images, lr_images = sample_images(image_list, batch_size = batch_size, low_resolution_shape = low_resolution_shape, high_resolution_shape = high_resolution_shape)\n    \n    \n    \n    # normalize images\n    \n    hr_images = hr_images \/ 127.5 - 1\n    lr_images = lr_images \/ 127.5 - 1\n    \n    # generate high-resolution images from low-resolution images\n    generated_high_resolution_images = generator.predict(lr_images)\n    \n    print(\"Generated high res images ...\\n\")\n    # generate a batch of true and fake labels\n    real_labels = np.ones((batch_size, 16, 16, 1))\n    fake_labels = np.zeros((batch_size, 16, 16, 1))\n    \n    d_loss_real = discriminator.train_on_batch(hr_images, real_labels)\n    d_loss_real = np.mean(d_loss_real)\n    \n    d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake_labels)\n    d_loss_fake = np.mean(d_loss_fake)\n    \n    print(\"Discriminator Losses calculated...\\n\")\n    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n    losses['d_history'].append(d_loss)\n        \n    \n    \"\"\"\n    Train the generator network\n    \"\"\"\n    \n    #hr_images, lr_images = sample_images(image_list, batch_size = batch_size, low_resolution_shape = low_resolution_shape, \n                                        #high_resolution_shape = high_resolution_shape)\n    \n    # normalize the images\n    #hr_images = hr_images\/127.5 - 1\n    #lr_images = lr_images\/127.5 - 1\n    \n    # extract feature maps for true high-resolution images\n    image_features = fe_model.predict(hr_images)\n    \n    # train the generator\n    g_loss = adversarial_model.train_on_batch([lr_images, hr_images], \n                                             [real_labels, image_features])\n    \n    losses['g_history'].append(0.5 * (g_loss[1]))\n    print(\"Generator trained...\\n\")\n    \n    \n    print(\"\\n Calculating PSNR\")\n    # psnr\n    ps = compute_psnr(hr_images, generated_high_resolution_images)\n    psnr['psnr_quality'].append(ps)\n    \n    # ssim\n    \n    print(\"\\nCalculating SSIM\\n\")\n    ss = compute_ssim(hr_images, generated_high_resolution_images)\n    ssim['ssim_quality'].append(ss)\n    \n    \"\"\"\n    save and print image samples\n    \"\"\"\n    print(\"Epoch Completed...\\n\\n\")\n    if epoch % 2 == 0:\n        hr_images, lr_images = sample_images(image_list, batch_size = batch_size, low_resolution_shape = low_resolution_shape, high_resolution_shape = high_resolution_shape)\n        \n        generated_images = generator.predict_on_batch(lr_images)\n        \n        for index, img in enumerate(generated_images):\n            \n            if index < 3:\n                save_images(hr_images[index], lr_images[index], img, path = \"\/kaggle\/working\/img_{}_{}\".format(epoch, index))\n        ","9680ce41":"discriminator.summary()","86ec293a":"plot_loss(losses)\nplot_psnr(psnr)\nplot_ssim(ssim)","b8f648af":"generator.save_weights(\"\/kaggle\/working\/srgan_generator.h5\")\ndiscriminator.save_weights(\"\/kaggle\/working\/srgan_discriminator.h5\")","b53fd8b2":"16 residual blocks & 2 upsampling blocks","eaab9a47":"PSNR - Peak to Signal Noise Ratio","e8789d1b":"### Only training for 3 epochs. This typically requires 30000 epochs. Kaggle NBs don't support this for GPU.\n\n## TPU Implementation will be shared soon","0d4bd607":"## Loading the Dataset","5eebdb5e":"Utility Functions","90c2d816":"plot the training loss, psnr, ssim","30fe140b":"## Model Training","33f64a3f":"## Model Architectures","eef0d5a6":"1. Generator Network\n2. Discriminator Network\n3. Feature extractor using VGG19 network\n4. Adversarial framework","50767f7e":"V1. Generator","abc98b71":"Loss Functions - Perpetual Loss","81ed6818":"SSIM - Structural Similarity Index","49490077":"Reference -> https:\/\/github.com\/AvivSham\/SRGAN-Keras-Implementation\/blob\/master\/SRGAN-Notebook.ipynb","724eca3c":"## Model Architecture followed SRGAN\n\nLink to paper -> https:\/\/arxiv.org\/pdf\/1609.04802v5.pdf","3745212e":"### Dataset used -> [DIV2K](https:\/\/www.kaggle.com\/joe1995\/div2k-dataset)","85505ba7":"# Note:- This can be replicated for TPU training. Stay tuned for TPU Notebook implementation. Will be shared soon.","1009a19a":"# Summary \n\nSRGANs were introduced in 2017 as State of the art for Super Image Resolution\n\nIn 2018, Enhanced SRGAN (ESRGAN) were introduced. We'll learn about them in the upcoming notebooks."}}