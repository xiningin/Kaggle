{"cell_type":{"3ed73189":"code","89e3fa55":"code","a918b61a":"code","ad051883":"code","cca702a9":"code","b6c4d5b7":"code","9b4225f3":"code","98c7dc7d":"code","cb897904":"code","9b79ba48":"code","36da5129":"code","da5dd000":"code","e22c7590":"code","da76c761":"code","374b8704":"code","3d8a9474":"code","5ced6223":"code","74df6e44":"code","07ce9551":"code","864ca054":"markdown","cba5062a":"markdown","4166c2b4":"markdown","9ebe7a05":"markdown","401d280e":"markdown","9d4f101f":"markdown","ab40e3ce":"markdown","dcb3c46c":"markdown","de17ff66":"markdown","183f3f7b":"markdown","c4d70eda":"markdown"},"source":{"3ed73189":"import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap\nimport eli5\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\nshap.initjs()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","89e3fa55":"df = pd.read_csv('\/kaggle\/input\/usedcarscatalog\/cars.csv')\ndf.shape","a918b61a":"df = df.loc[df['model_name']=='Passat']\ndf.shape","ad051883":"df.price_usd.mean()","cca702a9":"from sklearn.model_selection import train_test_split \n\nX = df.drop('price_usd', axis=1)\ny = df['price_usd']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","b6c4d5b7":"print(\"Number of cars in X_train dataset: \", X_train.shape) \nprint(\"Number of cars in y_train dataset: \", y_train.shape) \nprint(\"Number of cars in X_test dataset: \", X_test.shape) \nprint(\"Number of cars in y_test dataset: \", y_test.shape)","9b4225f3":"%%time\n# create train_pool object\nfrom catboost import CatBoostRegressor\nfrom catboost import Pool\nfrom catboost import MetricVisualizer\n\n\n\ncat_features=['manufacturer_name', \n              'model_name', \n              'transmission', \n              'color', \n              'engine_fuel',\n              'engine_has_gas',\n              'engine_type', \n              'body_type', \n              'has_warranty', \n              'state', \n              'drivetrain',\n              'is_exchangeable', \n              'location_region',\n              'feature_0',\n              'feature_1',\n              'feature_2',\n              'feature_3',\n              'feature_4',\n              'feature_5',\n              'feature_6',\n              'feature_7',\n              'feature_8',\n              'feature_9',]\n\ntrain_pool = Pool(\n    data=X_train, \n    label=y_train,\n    cat_features = cat_features\n)\n\n# create validation_pool object\nvalidation_pool = Pool(\n    data=X_test, \n    label=y_test,\n    cat_features = cat_features\n)","98c7dc7d":"%%time\n\n# pretty basic model, max_depth=10 give slightly better results\ncbs = CatBoostRegressor(iterations=4000,\n                         learning_rate=0.012,\n                         loss_function='MAE',\n                         max_depth=10, \n                         early_stopping_rounds=200,\n                         cat_features = cat_features)\n\n# we are passing categorical features as parameters here\ncbs.fit(\n    train_pool,\n    eval_set=validation_pool,\n    verbose=False,\n    plot=True \n);","cb897904":"error = test_predictions - y_test\n# print(type(error))\n\nplt.figure(figsize=(10,10))\nplt.scatter(y_test, \n            test_predictions, \n            c=error,\n            s=2,\n            cmap='hsv',\n            )\nplt.colorbar()\nplt.xlabel('True Values [price_usd]')\nplt.ylabel('Predictions [price_usd]')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0, 20000])\nplt.ylim([0, 20000])\nplt.show()","9b79ba48":"plt.figure(figsize=(16,7))\nplt.hist(error, bins = 40, rwidth=0.9)\nplt.xlabel('Predictions Error [price_usd]')\n_ = plt.ylabel('Count')\nplt.xlim([-6000, 6000])\nplt.show()","36da5129":"%%time\n\nimportance_types = ['PredictionValuesChange',\n                    'LossFunctionChange'\n                   ]\n\n\nfor importance_type in importance_types:\n    print(importance_type)\n    print(cbs.get_feature_importance(data=train_pool, \n                                     type=importance_type))\n    print('\\n\\n\\n\\n')","da5dd000":"%%time\n\nimport shap\nshap.initjs()\n\nshap_values = cbs.get_feature_importance(Pool(X_test, \n                                              label=y_test,\n                                              cat_features=cat_features), \n                                         type=\"ShapValues\")\nprint(type(shap_values))\n\nexpected_value = shap_values[0,-1]\nprint(expected_value)\n\nshap_values = shap_values[:,:-1]","e22c7590":"shap.summary_plot(shap_values, X_test, max_display=X_test.shape[1])","da76c761":"shap.dependence_plot(ind='year_produced', interaction_index='year_produced',\n                     shap_values=shap_values, \n                     features=X_test,  \n                     display_features=X_test)","374b8704":"shap.dependence_plot(ind='odometer_value', interaction_index='odometer_value',\n                     shap_values=shap_values, \n                     features=X_test,  \n                     display_features=X_test)","3d8a9474":"shap.dependence_plot(ind='engine_capacity', interaction_index='engine_capacity',\n                     shap_values=shap_values, \n                     features=X_test,  \n                     display_features=X_test)","5ced6223":"shap.dependence_plot(ind='number_of_photos', interaction_index='number_of_photos',\n                     shap_values=shap_values, \n                     features=X_test,  \n                     display_features=X_test)","74df6e44":"\nshap.force_plot(expected_value, shap_values[:1000,:], X_test.iloc[:1000,:])","07ce9551":"for i in range(50,70):\n    print('Sample', i, 'from the test set:')\n    display(shap.force_plot(expected_value, shap_values[i,:], X_test.iloc[i,:]))\n    print('Listed_price -------------------------------------->', y_test.iloc[i])\n    print('parameters:\\n', X_test.iloc[i,:])\n    print('\\n\\n\\n\\n\\n\\n\\n')","864ca054":"### Create a simple train-test split","cba5062a":"number_of_photos that a particular has in a catalog is an important feature too, because cheap and old cars rarely have lots of photos because there is just not too much to show.","4166c2b4":"### Limit the dataset to a single car model\nWill start with VW Passat model as it is the most popular model in the catalog. It is considered to be the most reliable and practical","9ebe7a05":"### Explore feature importances","401d280e":"### Load the dataset","9d4f101f":"### Calculate SHAP values","ab40e3ce":"## Conclusion\nI feel like CatBoost was the ideal choice for this particular dataset. The model performes reasonably well with 645 USD MAE, which is not bad at all given the 5100 USD mean price for the VW Passat. It also important to note that there are a lot of samples in the catalog that are listed for a very long periods of time, exceeding hundreds of days (check my EDA on this dataset). These samples may be potentially penalized. ","dcb3c46c":"### Explore predictions, SHAP values and listed prices for each model in the test set.","de17ff66":"# Predicting Car Prices for VW Passat using CatBoost\nI've already added exploratory data analysis notebook for this dataset and built a regression model for the whole dataset. Now I would like to explore the hierarchy of features in the SHAP summary plot for different models. Based on my domain knowledge of the current subject (used cars) I suspect that there will be a difference in hierarchies. ","183f3f7b":"The most important feature is, as expected, year_produced.","c4d70eda":"### Train the model"}}