{"cell_type":{"f48ebb76":"code","113afe20":"code","9bfa4982":"code","72a06ba0":"code","85050a13":"code","ce594e3c":"code","cc42986a":"code","74864805":"code","84861c84":"code","a814da53":"code","dac720f0":"code","80d5fe3b":"code","4de0e2a0":"code","91003322":"code","c79528da":"code","c0d20da1":"code","e9737944":"code","5215f210":"code","309cf4ae":"code","307b8c27":"code","6638b49a":"code","c5944878":"code","ec32f8fd":"code","be5c8adb":"code","e7b79a5b":"code","0cd10d22":"code","75723bb1":"code","cb64342a":"code","35b1909e":"code","6be808e6":"code","a2c240dc":"code","35ae17e7":"code","427b20c5":"code","d9899f74":"code","4c214180":"code","604e7414":"code","67f9bc73":"code","962acab8":"code","9de06a06":"code","fc390ada":"code","868515b7":"code","3da33c4e":"code","737f5f67":"markdown","0d8846f4":"markdown","db55e982":"markdown","c98fb82c":"markdown","628e57e5":"markdown","8bcb80b4":"markdown","731838da":"markdown","d559a213":"markdown","4680ab2b":"markdown","2ff8acae":"markdown","917147d5":"markdown","f2de41e6":"markdown","8326c875":"markdown","906d705d":"markdown","62bd17d9":"markdown","646c7378":"markdown"},"source":{"f48ebb76":"\nimport numpy as np \nimport pandas as pd \nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport keras.backend as K\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.util import ngrams\n\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers.embeddings import Embedding\nfrom keras import optimizers, Model\nfrom keras.callbacks import ModelCheckpoint \nfrom keras.layers import Flatten, Input, Layer, GlobalMaxPooling1D, LSTM, Bidirectional, Concatenate\n\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nfrom wordcloud import WordCloud\nfrom textblob import TextBlob\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom string import punctuation\nfrom collections import Counter\nfrom PIL import Image\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","113afe20":"tweets = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","9bfa4982":"tweets.info()","72a06ba0":"color = [sns.xkcd_rgb['green'], sns.xkcd_rgb['red']]\nsns.countplot('target', data=tweets, palette=color)\nplt.gca().set_ylabel('Samples')","85050a13":"tweets['char_len'] = tweets.text.str.len()\n\nword_tokens = [len(word_tokenize(tweet)) for tweet in tweets.text]\ntweets['word_len'] = word_tokens\n\nsent_tokens = [len(sent_tokenize(tweet)) for tweet in tweets.text]\ntweets['sent_len'] = sent_tokens\n\nplot_cols = ['char_len','word_len','sent_len']\nplot_titles = ['Character Length','Word Length','Sentence Length']\n\nplt.figure(figsize=(20,4))\nfor counter, i in enumerate([0,1,2]):\n    plt.subplot(1,3, counter+1)\n    sns.distplot(tweets[tweets.target == 1][plot_cols[i]], label='Disaster', color=color[1]).set_title(plot_titles[i])\n    sns.distplot(tweets[tweets.target == 0][plot_cols[i]], label='Non-Disaster', color=color[0])\n    plt.legend()","ce594e3c":"stop = set(stopwords.words('english'))\n\n# Get all the word tokens in dataframe for Disaster and Non-Disaster\ncorpus0 = []\n[corpus0.append(word.lower()) for tweet in tweets[tweets.target == 0].text for word in word_tokenize(tweet)]\n\ncorpus1 = []\n[corpus1.append(word.lower()) for tweet in tweets[tweets.target == 1].text for word in word_tokenize(tweet)]\n\n#function for counting top stopwords\ndef count_top_stopwords(corpus):\n    stopwords_freq = {}\n    for word in corpus:\n        if word in stop:\n            if word in stopwords_freq:\n                stopwords_freq[word] += 1\n            else:\n                stopwords_freq[word] = 1 \n    topwords = sorted(stopwords_freq.items(), key=lambda item: item[1], reverse=True)[:10] # get top 10 stopwords\n    x,y = zip(*topwords) # get keys and values\n    return x,y\n\nx0,y0 = count_top_stopwords(corpus0)\nx1,y1 = count_top_stopwords(corpus1)\n\n# plot bar of top stopwords for each class\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nplt.bar(x0, y0, color=color[0])\nplt.title('Top Stopwords for Non-Disaster Tweets')\nplt.subplot(1,2,2)\nplt.bar(x1, y1, color=color[1])\nplt.title('Top Stopwords for Disaster Tweets')","cc42986a":"# get all punctuations in dataframe\ncorpus0 = []\n[corpus0.append(c) for tweet in tweets[tweets.target == 0].text for c in tweet]\ncorpus0 = list(filter(lambda x: x in punctuation, corpus0)) \n\ncorpus1 = []\n[corpus1.append(c) for tweet in tweets[tweets.target == 1].text for c in tweet]\ncorpus1 = list(filter(lambda x: x in punctuation, corpus1))\n\nx0,y0 = zip(*Counter(corpus0).most_common())\nx1,y1 = zip(*Counter(corpus1).most_common())\n\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nplt.bar(x0, y0, color=color[0])\nplt.title('Top Punctuarions for Non-Disaster Tweets')\nplt.subplot(1,2,2)\nplt.bar(x1, y1, color=color[1])\nplt.title('Top Punctuations for Disaster Tweets')","74864805":"stop = ENGLISH_STOP_WORDS.union(stop) # stopwords from different sources\n\n# function for removing url from text\ndef remove_url(txt):\n    return ' '.join(re.sub('([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)', '', txt).split())\n\ncorpus0 = []\n[corpus0.append(word.lower()) for tweet in tweets[tweets.target == 0].text for word in word_tokenize(remove_url(tweet))]\ncorpus0 = list(filter(lambda x: x not in stop, corpus0))\n\ncorpus1 = []\n[corpus1.append(word.lower()) for tweet in tweets[tweets.target == 1].text for word in word_tokenize(remove_url(tweet))]\ncorpus1 = list(filter(lambda x: x not in stop, corpus1))\n\na = Counter(corpus0).most_common()\ndf0 = pd.DataFrame(a, columns=['Word','Count'])\n\nb = Counter(corpus1).most_common()\ndf1 = pd.DataFrame(b, columns=['Word','Count'])\n\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nsns.barplot(x='Word',y='Count',data=df0.head(10), color=color[1]).set_title('Most Common Words for Non-Disasters')\nplt.xticks(rotation=45)\nplt.subplot(1,2,2)\nsns.barplot(x='Word',y='Count',data=df1.head(10), color=color[0]).set_title('Most Common Words for Disasters')\nplt.xticks(rotation=45)","84861c84":"tweets['polarity'] = [TextBlob(tweet).sentiment.polarity for tweet in tweets.text]\ntweets['subjectivity'] = [TextBlob(tweet).sentiment.subjectivity for tweet in tweets.text]\n\n# exclaimation and question marks\ntweets['exclaimation_num'] = [tweet.count('!') for tweet in tweets.text]\ntweets['questionmark_num'] = [tweet.count('?') for tweet in tweets.text]\n\n# count number of hashtag and mentions\ndef count_url_hashtag_mention(text):\n    urls_num = len(re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n    word_tokens = text.split()\n    hash_num = len([word for word in word_tokens if word[0] == '#' and word.count('#') == 1])\n    mention_num = len([word for word in word_tokens if word[0] == '@' and word.count('@') == 1])\n    return urls_num, hash_num, mention_num\n\nurl_num, hash_num, mention_num = zip(*[count_url_hashtag_mention(tweet) for tweet in tweets.text])\ntweets = tweets.assign(url_num = url_num, hash_num = hash_num, mention_num = mention_num)\n\n# count number of contractions\ncontractions = [\"'t\", \"'re\", \"'s\", \"'d\", \"'ll\", \"'ve\", \"'m\"]\ntweets['contraction_num'] = [sum([tweet.count(cont) for cont in contractions]) for tweet in tweets.text]","a814da53":"tweets.head()","dac720f0":"\ntweets.keyword.fillna('None', inplace=True)\n\n# expand contractions\ndef decontraction(phrase):\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ntweets.text = [decontraction(tweet) for tweet in tweets.text]\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nprint(remove_emoji('OMG there is a volcano eruption!!! \ud83d\ude2d\ud83d\ude31\ud83d\ude37'))\n\ntweets.text = tweets.text.apply(lambda x: remove_emoji(x))","80d5fe3b":"# remove URL's\ntweets.text = tweets.text.apply(lambda x: remove_url(x))\n\n# remove punctuations except '!?'\ndef remove_punct(text):\n    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n    table = str.maketrans('','', new_punct)\n    return text.translate(table)\n\ntweets.text = tweets.text.apply(lambda x: remove_punct(x))\n\n# replae 'amp'\ndef replace_amp(text):\n    text = re.sub(r' amp ', ' and ', text)\n    return text\n\ntweets.text = tweets.text.apply(lambda x: replace_amp(x))","4de0e2a0":"# Lemmatization\n\nlemmatizer = WordNetLemmatizer()\ndef lemma(text):\n    words = word_tokenize(text)\n    return ' '.join([lemmatizer.lemmatize(w.lower(), pos='v') for w in words])\n\ntweets.text = tweets.text.apply(lambda x: lemma(x))","91003322":"# Ngrams\ndef generate_ngrams(text, n):\n    words = word_tokenize(text)\n    return [' '.join(ngram) for ngram in list(get_data(ngrams(words, n))) if not all(w in stop for w in ngram)] # exclude if all are stopwords\n\n\ndef get_data(gen):\n    try:\n        for elem in gen:\n            yield elem\n    except (RuntimeError, StopIteration):\n        return","c79528da":"# Bigrams\n\nbigrams_disaster = tweets[tweets.target==1].text.apply(lambda x: generate_ngrams(x, 2))\nbigrams_ndisaster = tweets[tweets.target==0].text.apply(lambda x: generate_ngrams(x, 2))\n\nbigrams_d_dict = {}\nfor bgs in bigrams_disaster:\n    for bg in bgs:\n        if bg in bigrams_d_dict:\n            bigrams_d_dict[bg] += 1\n        else:\n            bigrams_d_dict[bg] = 1\n\nbigrams_d_df = pd.DataFrame(bigrams_d_dict.items(), columns=['Bigrams','Count'])\n\nbigrams_nd_dict = {}\nfor bgs in bigrams_ndisaster:\n    for bg in bgs:\n        if bg in bigrams_nd_dict:\n            bigrams_nd_dict[bg] += 1\n        else:\n            bigrams_nd_dict[bg] = 1            \n\nbigrams_nd_df = pd.DataFrame(bigrams_nd_dict.items(), columns=['Bigrams','Count'])","c0d20da1":"# Barplots for bigrams\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nsns.barplot(x='Count',y='Bigrams',data=bigrams_nd_df.sort_values('Count', ascending=False).head(40), color=color[0]).set_title('Most Common Bigrams for Non-Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.subplot(1,2,2)\nsns.barplot(x='Count',y='Bigrams',data=bigrams_d_df.sort_values('Count', ascending=False).head(40), color=color[1]).set_title('Most Common Bigrams for Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.tight_layout()\nplt.show()","e9737944":"# Woudcloud for bigrams\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nmy_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(bigrams_nd_dict)\nplt.imshow(my_cloud, interpolation='bilinear')\nplt.axis('off')\n\nplt.subplot(1,2,2)\nmy_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(bigrams_d_dict)\nplt.imshow(my_cloud, interpolation='bilinear')\nplt.axis('off')\n\nplt.show()","5215f210":"# Trigrams\n\ntrigrams_disaster = tweets[tweets.target==1].text.apply(lambda x: generate_ngrams(x, 3))\ntrigrams_ndisaster = tweets[tweets.target==0].text.apply(lambda x: generate_ngrams(x, 3))\n\ntrigrams_d_dict = {}\nfor tgs in trigrams_disaster:\n    for tg in tgs:\n        if tg in trigrams_d_dict:\n            trigrams_d_dict[tg] += 1\n        else:\n            trigrams_d_dict[tg] = 1\n\ntrigrams_d_df = pd.DataFrame(trigrams_d_dict.items(), columns=['Trigrams','Count'])\n\ntrigrams_nd_dict = {}\nfor tgs in trigrams_ndisaster:\n    for tg in tgs:\n        if tg in trigrams_nd_dict:\n            trigrams_nd_dict[tg] += 1\n        else:\n            trigrams_nd_dict[tg] = 1            \n\ntrigrams_nd_df = pd.DataFrame(trigrams_nd_dict.items(), columns=['Trigrams','Count'])","309cf4ae":"# Barplots for trigrams\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nsns.barplot(x='Count',y='Trigrams',data=trigrams_nd_df.sort_values('Count', ascending=False).head(40), color=color[0]).set_title('Most Common Trigrams for Non-Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.subplot(1,2,2)\nsns.barplot(x='Count',y='Trigrams',data=trigrams_d_df.sort_values('Count', ascending=False).head(40), color=color[1]).set_title('Most Common Trigrams for Disasters')\nax = plt.gca()\nax.set_ylabel('')\nplt.tight_layout()\nplt.show()","307b8c27":"## Remove Stopwords\ndef remove_stopwords(text):\n    word_tokens = word_tokenize(text)\n    return ' '.join([w.lower() for w in word_tokens if not w.lower() in stop])\n\n#tweets_tmp = tweets.copy()\ntweets['text_nostopwords'] = tweets.text.apply(lambda x: remove_stopwords(x))","6638b49a":"## Plot word cloud for most common words after cleaning\n\nfrom PIL import Image\nmask = np.array(Image.open('..\/input\/twitterlogo3\/twitter-logo-png-transparent.png'))\nreverse = mask[...,::-1,:]\n\ndef wc_words(target, mask=mask):\n    words = [word.lower() for tweet in tweets[tweets.target == target].text_nostopwords for word in tweet.split()]\n    words = list(filter(lambda w: w != 'like', words))\n    words = list(filter(lambda w: w != 'new', words))\n    words = list(filter(lambda w: w != 'people', words))\n    dict = {}\n    for w in words:\n        if w in dict:\n            dict[w] += 1\n        else:\n            dict[w] = 1\n    # plot using frequencies        \n    my_cloud = WordCloud(background_color='white', stopwords=stop, mask=mask, random_state=0).generate_from_frequencies(dict) \n    \n    plt.subplot(1,2,target+1)\n    plt.imshow(my_cloud, interpolation='bilinear') \n    plt.axis(\"off\")\n\nplt.figure(figsize=(15,10))\nwc_words(0)\nplt.title('Non-Disaster')\nwc_words(1, reverse)\nplt.title('Disaster')\nplt.show()","c5944878":"pd.options.display.max_colwidth = 200\nfor t in tweets['text'].sample(n=20, random_state=0):\n    print(t)\npd.reset_option('max_colwidth')","ec32f8fd":"pd.reset_option('max_colwidth')\ntweets.drop('text_nostopwords', axis=1, inplace=True)\ntweets.head()","be5c8adb":"X_train, X_val, y_train, y_val = train_test_split(tweets.drop(['keyword','location','target'],axis=1), tweets[['target']], test_size=0.2, stratify=tweets[['target']], random_state=0)\nX_train_text = X_train['text']\nX_val_text = X_val['text']\n\nprint('X_train shape: ', X_train.shape)\nprint('X_val shape: ', X_val.shape)\nprint('y_train shape: ', y_train.shape)\nprint('y_val shape: ', y_val.shape)","e7b79a5b":"print('Train Class Proportion:\\n', y_train['target'].value_counts() \/ len(y_train) * 100)\nprint('\\nValidation Class Proportion:\\n', y_val['target'].value_counts() \/ len(y_val) * 100)","0cd10d22":"tokenizer_1 = Tokenizer(num_words=5000, oov_token='<UNK>')\ntokenizer_1.fit_on_texts(X_train_text)","75723bb1":"X_train_text = tokenizer_1.texts_to_sequences(X_train_text)\nX_val_text = tokenizer_1.texts_to_sequences(X_val_text)\nprint(X_train_text[:10])\nprint('')\nprint(X_val_text[:10])","cb64342a":"tokenizer_1.sequences_to_texts([X_train_text[1]])","35b1909e":"print('Train Set Max Length:', max(len(text) for text in X_train_text))\nmaxlen = 50\n\nX_train_text = pad_sequences(X_train_text, padding='post', maxlen=maxlen)\nX_val_text = pad_sequences(X_val_text, padding='post', maxlen=maxlen)\n\nprint('X_train shape:', X_train_text.shape)\nprint('X_train shape:', X_val_text.shape)","6be808e6":"# Adding 1 because of reserved 0 index\nvocab_size = len(tokenizer_1.word_index) + 1\n\n# load the whole embedding into memory\nembeddings_index = dict()\nf = open('..\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.200d.txt')\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","a2c240dc":"# create a weight matrix for words in training set\nembedding_matrix = np.zeros((vocab_size, 200))\n\nfor word, i in tokenizer_1.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        \nprint('Embedding Matrix Shape:', embedding_matrix.shape)","35ae17e7":"## Hyperparameters\nnum_epochs=15\ndropout=0.2\nrecurrent_dropout=0.2\nlr=0.0005\nbatch_size=128\nclass_weight = {0: y_train['target'].value_counts()[1]\/len(y_train), 1: y_train['target'].value_counts()[0]\/len(y_train)} ","427b20c5":"lstm_model = Sequential()\nembedding_layer = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=maxlen, trainable=False)\nlstm_model.add(embedding_layer)\nlstm_model.add(LSTM(128, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)) # try adding dropout later\nlstm_model.add(LSTM(128))\n\nlstm_model.add(Dense(1, activation='sigmoid'))\n\nadam = optimizers.Adam(lr=lr)\nlstm_model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\nprint(lstm_model.summary())","d9899f74":"def plot_model_performance(history):   \n    plt.figure(figsize=(15,5))\n    plt.plot(range(num_epochs), history.history['acc'],'-o',\n             label='Train ACC',color='#ff7f0e')\n    plt.plot(range(num_epochs),history.history['val_acc'],'-o',\n             label='Val ACC',color='#1f77b4')\n    x = np.argmax( history.history['val_acc'] ); y = np.max( history.history['val_acc'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#1f77b4')\n    plt.text(x-0.03*xdist,y-0.13*ydist,'max acc\\n%.2f'%y,size=14)\n    plt.ylabel('Accuracy',size=14); plt.xlabel('Epoch',size=14)\n    plt.legend(loc=(0.01,0.75))\n\n    plt2 = plt.gca().twinx()\n    plt2.plot(range(num_epochs),history.history['loss'],'-o',\n              label='Train Loss',color='#2ca02c')\n    plt2.plot(range(num_epochs),history.history['val_loss'],'-o',\n              label='Val Loss',color='#d62728')\n    x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#d62728')\n    plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n   # plt.ylim([-0.2, 2])\n    plt.ylabel('Loss',size=14)\n    plt.xticks(ticks=list(range(num_epochs)),labels=list(range(1, num_epochs+1)))\n    plt.legend(loc='lower left', bbox_to_anchor=(0.01, 0.1))\n    plt.show()","4c214180":"checkpoint = ModelCheckpoint('lstm_model.h5', monitor='val_acc', save_best_only=True)\nhistory = lstm_model.fit(X_train_text, y_train, batch_size=batch_size, callbacks=[checkpoint], epochs=num_epochs, \n                         class_weight=class_weight, validation_data=(X_val_text, y_val), verbose=1)\nplot_model_performance(history)","604e7414":"# count number of characters in each tweet\ntest['char_len'] = test.text.str.len()\n\n# count number of words in each tweet\nword_tokens = [len(word_tokenize(tweet)) for tweet in test.text]\ntest['word_len'] = word_tokens\n\n# count number of sentence in each tweet\nsent_tokens = [len(sent_tokenize(tweet)) for tweet in test.text]\ntest['sent_len'] = sent_tokens","67f9bc73":"# polarity and subjectivity\ntest['polarity'] = [TextBlob(tweet).sentiment.polarity for tweet in test.text]\ntest['subjectivity'] = [TextBlob(tweet).sentiment.subjectivity for tweet in test.text]\n\n\n# exclaimation and question marks\ntest['exclaimation_num'] = [tweet.count('!') for tweet in test.text]\ntest['questionmark_num'] = [tweet.count('?') for tweet in test.text]\n\n\n# count number of hashtags and mentions\n# Function for counting number of hashtags and mentions\ndef count_url_hashtag_mention(text):\n    urls_num = len(re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n    word_tokens = text.split()\n    hash_num = len([word for word in word_tokens if word[0] == '#' and word.count('#') == 1]) # only appears once in front of word \n    mention_num = len([word for word in word_tokens if word[0] == '@' and word.count('@') == 1]) # only appears once in front of word \n    return urls_num, hash_num, mention_num\n\nurl_num, hash_num, mention_num = zip(*[count_url_hashtag_mention(tweet) for tweet in test.text])\ntest = test.assign(url_num = url_num, hash_num = hash_num, mention_num = mention_num)\n\n\n# count number of contractions\ncontractions = [\"'t\", \"'re\", \"'s\", \"'d\", \"'ll\", \"'ve\", \"'m\"]\ntest['contraction_num'] = [sum([tweet.count(cont) for cont in contractions]) for tweet in test.text]","962acab8":"# Replace NaNs with 'None'\ntest.keyword.fillna('None', inplace=True) \n\n\n# Expand Contractions\n\n# Function for expanding most common contractions https:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\ndef decontraction(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ntest.text = [decontraction(tweet) for tweet in test.text]\n\n\n# Remove Emojis\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nprint(remove_emoji(\"OMG there is a volcano eruption!!! \ud83d\ude2d\ud83d\ude31\ud83d\ude37\"))\n\ntest.text = test.text.apply(lambda x: remove_emoji(x))","9de06a06":"# Remove URLs\ntest.text = test.text.apply(lambda x: remove_url(x))\n\n# Remove Punctuations except '!?'\n\ndef remove_punct(text):\n    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n    table=str.maketrans('','',new_punct)\n    return text.translate(table)\n\ntest.text = test.text.apply(lambda x: remove_punct(x))\n\n## Replace amp\ndef replace_amp(text):\n    text = re.sub(r\" amp \", \" and \", text)\n    return text\n\ntest.text = test.text.apply(lambda x: replace_amp(x))\n","fc390ada":"# Lemmatization\n\nlemmatizer = WordNetLemmatizer()\ndef lemma(text):\n    words = word_tokenize(text)\n    return ' '.join([lemmatizer.lemmatize(w.lower(), pos='v') for w in words])\n\ntest.text = test.text.apply(lambda x: lemma(x))","868515b7":"# tokenize\ntest_text = test['text']\ntest_text = tokenizer_1.texts_to_sequences(test_text)\n\n# padding\ntest_text = pad_sequences(test_text, padding='post', maxlen=50)\n\nprint('X_test shape:', test_text.shape)","3da33c4e":"# lstm prediction\n# model.predict(test_text)\nlstm_model.load_weights('lstm_model.h5')\nsubmission = test.copy()[['id']]\nsubmission['target'] = lstm_model.predict_classes(test_text)\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission.head())","737f5f67":"# Ngrams","0d8846f4":"# Polarity and subjectivity - Meta-Feature Engineering","db55e982":"### Padding","c98fb82c":"## LSTM","628e57e5":"# Building and training Model","8bcb80b4":"### Tokenization","731838da":"# Train Validation Data","d559a213":"# Top most common words","4680ab2b":"# Testing\n","2ff8acae":"# Top most common punctuations","917147d5":"# Embedding layer","f2de41e6":"# Text cleaning","8326c875":"# Character, Word and Sentence Frequency","906d705d":"### Embedding matrix - GloVe","62bd17d9":"# Data structure","646c7378":"# Top Stopwords"}}