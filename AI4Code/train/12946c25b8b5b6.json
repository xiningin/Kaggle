{"cell_type":{"9b045187":"code","1695ff86":"code","dd9cc408":"code","00d7d4a9":"code","f329328e":"code","028936aa":"code","31fcc96d":"code","c089a52a":"code","f27ab8d5":"code","abdfaa34":"code","859dfef0":"code","0fb1fbc5":"code","d4730ca1":"code","0e3c5d99":"code","79ccfa85":"code","b2ba33ec":"code","94480146":"code","5d513ef7":"code","ca480963":"code","a2b6ccd6":"code","28cd6b1a":"code","2883831c":"code","2fb4ba22":"code","9deaf23d":"code","36198bce":"code","05ae0d4c":"code","fddcc231":"code","fcd07637":"code","741ddb0f":"code","978e05d2":"code","2ee92846":"code","daf1d24c":"code","b997a6a0":"code","99a2fa62":"code","e617d049":"code","202b073b":"code","29f4485a":"code","92d10110":"code","72be0e9a":"code","e9a34af3":"code","864036cc":"code","30de7645":"code","08655d08":"code","ab389927":"code","2a8a9f9b":"code","4d534c5d":"code","76da1dff":"code","50b4b926":"code","28b2963c":"code","3ada55de":"code","e9aefe2b":"code","415567c4":"code","891a801d":"code","739e6e0e":"code","64bd189e":"markdown","82a3ee57":"markdown","4c1bb8a9":"markdown","b6856d13":"markdown","6bd00f01":"markdown","da347cad":"markdown","755e7f4c":"markdown","24997865":"markdown","d7a8082b":"markdown","4b415a3e":"markdown","4f693c08":"markdown","fb0a8b36":"markdown","adc5678b":"markdown","5daf6230":"markdown","6cee38a8":"markdown","859f24f2":"markdown","ed9d2bed":"markdown","ef7427b7":"markdown","070684e5":"markdown","7950217b":"markdown","f4398678":"markdown","c349dfb7":"markdown","c65b03ed":"markdown","c33be10d":"markdown","f3e8b3f6":"markdown","8d97a864":"markdown"},"source":{"9b045187":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1695ff86":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport pickle\nimport sklearn\nimport scipy\nimport seaborn as sns\nsns.set()","dd9cc408":"import plotly.express as px\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import confusion_matrix","00d7d4a9":"df = pd.read_csv('\/kaggle\/input\/machine-learning-for-diabetes-with-python\/diabetes_data.csv',index_col = 0)\ndf.head()","f329328e":"numerical_feature = [feature for feature in df.columns if df[feature].dtypes != 'O']\nprint(\"Numerical Features Count {}\".format(len(numerical_feature)))","028936aa":"discrete_feature=[feature for feature in numerical_feature if len(df[feature].unique())<25]\nprint(\"Discrete feature Count {}\".format(len(discrete_feature)))","31fcc96d":"continuous_feature = [feature for feature in numerical_feature if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","c089a52a":"categorical_feature = [feature for feature in df.columns if feature not in numerical_feature]\nprint(\"Categorical feature Count {}\".format(len(categorical_feature)))","f27ab8d5":"df.isnull().sum()","abdfaa34":"# Handle Missing Values\ndf.isnull().sum()*100\/len(df)","859dfef0":"#shape of dataset\ndf.shape","0fb1fbc5":"df.describe()","d4730ca1":"df.info()","0e3c5d99":"#print random five row\ndf.sample(5)","79ccfa85":"teju = df.corr(method = \"spearman\")\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(teju,annot=True)","b2ba33ec":"df_feature = df.columns\n\nfor feature in df_feature:\n    p = sns.distplot(a = df[feature])\n    plt.show()","94480146":"p = df.hist(figsize = (20,20))","5d513ef7":"kp =sns.scatterplot(x= \"Glucose\" ,y= \"BloodPressure\",\n              hue=\"Outcome\",\n              data=df);","ca480963":"kp1 =sns.scatterplot(x= \"BMI\" ,y= \"Insulin\",\n              hue=\"Outcome\",\n              data=df);","a2b6ccd6":"kp2 =sns.scatterplot(x= \"SkinThickness\" ,y= \"Insulin\",\n              hue=\"Outcome\",\n              data=df);","28cd6b1a":"sns.lmplot(x='SkinThickness',y='Insulin',data=df,fit_reg=False)","2883831c":"plt.figure(figsize=(16,12))\nsns.boxplot(data=df)","2fb4ba22":"sns.boxplot(y = 'SkinThickness', data = df)","9deaf23d":"colors =  [\"#DF6589FF\", \"#78C850\",\"#FC766AFF\"]\nax= df['BloodPressure'].value_counts(ascending=False).plot.pie(colors=colors,\n            autopct='%1.1f%%',\n            figsize=(15, 10)) \nplt.show()","36198bce":"sns.pairplot(df)","05ae0d4c":"sns.pairplot(data =df, hue = 'Outcome')\nplt.show()","fddcc231":"plt.figure(figsize=(16,12))\nsns.jointplot(x='BloodPressure',y='Age',data=df)","fcd07637":"plt.figure(figsize=(16,14))\nsns.kdeplot(df.BloodPressure, df.Age)","741ddb0f":"from scipy import stats\nfor feature in df.columns:\n    stats.probplot(df[feature], plot = plt)\n    plt.title(feature)\n    plt.show()","978e05d2":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()","2ee92846":"X = df.iloc[:, :-1]\ny = df.iloc[:, -1]","daf1d24c":"X.head()","b997a6a0":"y.head()","99a2fa62":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","e617d049":"def svm_classifier(X_train, X_test, y_train, y_test):\n    \n    classifier_svm = SVC(kernel = 'rbf', random_state = 0)\n    classifier_svm.fit(X_train, y_train)\n\n    y_pred = classifier_svm.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    return print(f\"Train score : {classifier_svm.score(X_train, y_train)}\\nTest score : {classifier_svm.score(X_test, y_test)}\")","202b073b":"def knn_classifier(X_train, X_test, y_train, y_test):\n    \n    classifier_knn = KNeighborsClassifier(metric = 'minkowski', p = 2)\n    classifier_knn.fit(X_train, y_train)\n\n    y_pred = classifier_knn.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    return print(f\"Train score : {classifier_knn.score(X_train, y_train)}\\nTest score : {classifier_knn.score(X_test, y_test)}\")","29f4485a":"def tree_classifier(X_train, X_test, y_train, y_test):\n    \n    classifier_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n    classifier_tree.fit(X_train, y_train)\n\n    y_pred = classifier_tree.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    return print(f\"Train score : {classifier_tree.score(X_train, y_train)}\\nTest score : {classifier_tree.score(X_test, y_test)}\")","92d10110":"def forest_classifier(X_train, X_test, y_train, y_test):\n    classifier_forest = RandomForestClassifier(criterion = 'entropy', random_state = 0)\n    classifier_forest.fit(X_train, y_train)\n\n    y_pred = classifier_forest.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    return print(f\"Train score : {classifier_forest.score(X_train, y_train)}\\nTest score : {classifier_forest.score(X_test, y_test)}\")","72be0e9a":"def print_score(X_train, X_test, y_train, y_test):\n    print(\"SVM:\\n\")\n    svm_classifier(X_train, X_test, y_train, y_test)\n\n    print(\"-\"*100)\n    print()\n\n    print(\"KNN:\\n\")\n    knn_classifier(X_train, X_test, y_train, y_test)\n\n    print(\"-\"*100)\n    print()\n\n    \n\n    print(\"Decision Tree:\\n\")\n    tree_classifier(X_train, X_test, y_train, y_test)\n\n    print(\"-\"*100)\n    print()\n\n    print(\"Random Forest:\\n\")\n    forest_classifier(X_train, X_test, y_train, y_test)","e9a34af3":"print_score(X_train, X_test, y_train, y_test)","864036cc":"from sklearn.linear_model import LogisticRegression\nlog=LogisticRegression(C=20.0)\nknn=KNeighborsClassifier(n_neighbors=3)\nsvm=SVC(kernel='linear')\ndt=DecisionTreeClassifier()\nrf=RandomForestClassifier(n_estimators=100)","30de7645":"log.fit(X_train,y_train)\nknn.fit(X_train,y_train)\nsvm.fit(X_train,y_train)\ndt.fit(X_train,y_train)\nrf.fit(X_train,y_train)","08655d08":"y_log=log.predict(X_test)\ny_knn=knn.predict(X_test)\ny_svm=svm.predict(X_test)\ny_dt=dt.predict(X_test)\ny_rf=rf.predict(X_test)","ab389927":"log_cm_test=confusion_matrix(y_test,y_log)\nknn_cm_test=confusion_matrix(y_test,y_knn)\nsvm_cm_test=confusion_matrix(y_test,y_svm)\ndt_cm_test=confusion_matrix(y_test,y_dt)\nrf_cm_test=confusion_matrix(y_test,y_rf)","2a8a9f9b":"from sklearn.metrics import accuracy_score\nlog_acc_test=accuracy_score(y_test,y_log)\nknn_acc_test=accuracy_score(y_test,y_knn)\nsvm_acc_test=accuracy_score(y_test,y_svm)\ndt_acc_test=accuracy_score(y_test,y_dt)\nrf_acc_test=accuracy_score(y_test,y_rf)\nfrom sklearn.metrics import accuracy_score\nlog_acc_test=accuracy_score(y_test,y_log)\nknn_acc_test=accuracy_score(y_test,y_knn)\nsvm_acc_test=accuracy_score(y_test,y_svm)\ndt_acc_test=accuracy_score(y_test,y_dt)\nrf_acc_test=accuracy_score(y_test,y_rf)","4d534c5d":"print('Accuracy Scores\\n')\n\nprint('\\n'+'#'*20+'Logistic Regression'+'#'*20)\nprint(log_acc_test)\n\nprint('\\n'+'#'*20+'KNearest Neighbour'+'#'*20)\nprint(knn_acc_test)\n\nprint('\\n'+'#'*20+'SVM'+'#'*20)\nprint(svm_acc_test)\n\nprint('\\n'+'#'*20+'Decision Tree Classifier'+'#'*20)\nprint(dt_acc_test)\n\nprint('\\n'+'#'*20+'Random Forest Classifier'+'#'*20)\nprint(rf_acc_test)","76da1dff":"acc_list=[log_acc_test,knn_acc_test,svm_acc_test,dt_acc_test,rf_acc_test]\nimport matplotlib.pyplot as plt; plt.rcdefaults()\n \nobjects = ('Logistic Regression', 'KNN', 'SVM', 'Decision Tree', 'Random Forest')\ny_pos = np.arange(len(objects))\nperformance = acc_list\nwidth = 1\/2\nplt.barh(y_pos, performance, width,align='center', alpha=1)\nplt.yticks(y_pos,objects)\nplt.xlabel('Accuracy')\nplt.xlabel('Algorithms')\nplt.title('Accuracy Scores of Algorithms')\nplt.show()","50b4b926":"#Performance\nclassifier_forest = RandomForestClassifier(criterion = 'entropy')\nclassifier_forest.fit(X_train, y_train)\ny_pred = classifier_forest.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\ncm","28b2963c":"pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","3ada55de":"df['Outcome'].value_counts()","e9aefe2b":"from sklearn.metrics import roc_auc_score, roc_curve, classification_report","415567c4":"print(classification_report(y_test, y_pred))","891a801d":"y_pred_prob = classifier_forest.predict_proba(X_test)[:,1]\ny_pred_prob","739e6e0e":"roc_auc_score(y_test,y_pred_prob)","64bd189e":"# SCATTERPLOT","82a3ee57":"# Check the data balanced or not","4c1bb8a9":"# THANK YOU !","b6856d13":"# BOXPLOT","6bd00f01":"# KNN(K-nearest neighbours)","da347cad":"# Probplot :-\nEach data point in y using marker symbols and draws a reference line that represents the theoretical distribution.","755e7f4c":"# PAIRPLOT","24997865":"# LOAD THE DATASET","d7a8082b":"# Random forest algorithm","4b415a3e":"# Roc_score","4f693c08":"# JOINTPLOT","fb0a8b36":"# Splitting the dataset into training and testing phase","adc5678b":"# PIECHART","5daf6230":"# Support vector machine","6cee38a8":"# Histogram Plot","859f24f2":"# BARPLOT","ed9d2bed":"# Decision Tree Algorithm","ef7427b7":"# Check Missing Value","070684e5":"# Training the dataset","7950217b":"# Compare the accuracy_score on the basis of barplot\n","f4398678":"# Find the Accuracy, Precision, Recall and F1-score","c349dfb7":"# HEATMAP","c65b03ed":"# IMPORT LIBRARY","c33be10d":"# DISTPLOT","f3e8b3f6":"# Fitting the dataset into various models:-\n","8d97a864":"# Density Plot"}}