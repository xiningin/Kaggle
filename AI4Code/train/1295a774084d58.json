{"cell_type":{"6f868b4b":"code","72276eb1":"code","feaed7c1":"code","542b427e":"code","10bf98b1":"code","79747930":"code","9d758a80":"code","03a98f25":"code","0c16e791":"code","07b02bc4":"code","8c0d0380":"code","67adad5e":"code","91edfd91":"code","fc468385":"code","75a35193":"code","60658721":"code","ef32df77":"code","025cbff5":"code","d4c13788":"code","d7ec7b48":"code","229bb7b9":"code","4e7cecac":"code","7b1f1bff":"code","9de6437d":"code","b2702ad9":"code","2827168c":"code","50b5010b":"code","35ab08ac":"code","2f9decba":"code","af5190e6":"code","5c08540f":"code","61cb5933":"markdown","5247f0be":"markdown","28880d9f":"markdown","66892d8e":"markdown","e0b381b4":"markdown","a1c9432a":"markdown","e7d2861a":"markdown","5643d920":"markdown","b45f968c":"markdown","28015ee4":"markdown","4170d856":"markdown","6c03e183":"markdown","3f0cbf6d":"markdown","b2615b12":"markdown","3fdf6b8a":"markdown","018cf1a0":"markdown","edbe7457":"markdown"},"source":{"6f868b4b":"import numpy as np #linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nplt.style.use('seaborn-dark-palette')","72276eb1":"%time data = pd.read_csv('..\/input\/iris\/Iris.csv')","feaed7c1":"# checking the info of data.\ndata.info()","542b427e":"data = data.drop(['Id'], axis=1)","10bf98b1":"# checking shape of data\ndata.shape","79747930":"# checking for all the null values\ndata.isnull().sum()","9d758a80":"# summary statistics of quantitative variables\ndata.describe()","03a98f25":"data[\"Species\"].value_counts() \/ len(data)","0c16e791":"data.hist()\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()","07b02bc4":"plt.figure(figsize=(20,15))\nplt.subplot(2,2,1)\nsns.swarmplot(x='Species',y='PetalLengthCm',data=data)\nplt.subplot(2,2,2)\nsns.swarmplot(x='Species',y='PetalWidthCm',data=data)\nplt.subplot(2,2,3)\nsns.swarmplot(x='Species',y='SepalLengthCm',data=data)\nplt.subplot(2,2,4)\nsns.swarmplot(x='Species',y='SepalWidthCm',data=data)","8c0d0380":"plt.figure(figsize=(10,6))\nsns.countplot(data.Species)","67adad5e":"plt.figure(figsize=(10,6))\nsns.scatterplot(x='SepalLengthCm', y='SepalWidthCm', hue='Species', data=data)","91edfd91":"plt.figure(figsize=(10,6))\nsns.scatterplot(x='PetalLengthCm', y='PetalWidthCm', hue='Species', data=data)","fc468385":"## pairplots to get an intuition of potential correlations\nsns.pairplot(data[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\", \"Species\"]], diag_kind=\"kde\")","75a35193":"plt.figure(figsize=(10,6))\n#draws  heatmap with input as the correlation matrix calculted by(data.corr())\nsns.heatmap(data.corr().abs(), annot=True, cmap='RdYlGn')\nplt.title('Correlation Heatmap')\nplt.show()","60658721":"plt.figure(figsize=(10,6))\nsns.clustermap(data.corr().abs(), cmap='plasma')\nplt.title('Cluster Heatmap')","ef32df77":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data, data[\"Species\"]):\n    train_set_strat = data.loc[train_index]\n    test_set_strat = data.loc[test_index]","025cbff5":"##checking category distribution in training set\ntrain_set_strat['Species'].value_counts() \/ len(train_set_strat)","d4c13788":"test_set_strat[\"Species\"].value_counts() \/ len(test_set_strat)","d7ec7b48":"train_set_strat_x = train_set_strat.copy().drop(['Species'], axis=1)\ntrain_set_strat_y = train_set_strat['Species']\n\ntest_set_strat_x = test_set_strat.copy().drop(['Species'], axis=1)\ntest_set_strat_y = test_set_strat['Species']","229bb7b9":"train_set_strat_x.shape, train_set_strat_y.shape, test_set_strat_x.shape, test_set_strat_y.shape","4e7cecac":"# importing alll the necessary packages to use the various classification algorithms\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours algorithm\nfrom sklearn import metrics #for checking the model accuracy","7b1f1bff":"# k-nearest neighbor\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(train_set_strat_x, train_set_strat_y)\nknn_Y_pred = knn.predict(test_set_strat_x)\nknn_accuracy=metrics.accuracy_score(test_set_strat_y,knn_Y_pred)*100\n\nprint(\"Accuracy on training dataset : {:.2f}%\".format(knn_accuracy) )","9de6437d":"# getting precision, recall and f1-score via classification report\n\nprint(metrics.classification_report(test_set_strat_y, knn_Y_pred))","b2702ad9":"from sklearn.model_selection import GridSearchCV\nk_range = list(range(1, 31))\nparam_grid = dict(n_neighbors=k_range)\n\nknn = KNeighborsClassifier()\n\n# defining parameter range\ngrid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False, verbose=1)\n\n# fitting the model for grid search\ngrid_search=grid.fit(train_set_strat_x, train_set_strat_y)","2827168c":"print(grid_search.best_params_)","50b5010b":"accuracy = grid_search.best_score_ *100\nprint(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy) )","35ab08ac":"knn = KNeighborsClassifier(n_neighbors=13)\n\nknn.fit(train_set_strat_x, train_set_strat_y)\n\ny_test_hat = knn.predict(test_set_strat_x) \n\ntest_accuracy=metrics.accuracy_score(test_set_strat_y, y_test_hat) * 100\n\nprint(\"Accuracy on testing dataset with tuning is : {:.2f}%\".format(test_accuracy))","2f9decba":"# getting precision, recall and f1-score via classification report\n\nprint(metrics.classification_report(test_set_strat_y, y_test_hat))","af5190e6":"fig, ax = plt.subplots(figsize=(10, 10))\nmetrics.plot_confusion_matrix(grid, train_set_strat_x, train_set_strat_y, values_format='d', ax=ax)","5c08540f":"fig, ax = plt.subplots(figsize=(10, 10))\nmetrics.plot_confusion_matrix(grid, test_set_strat_x, test_set_strat_y, values_format='d', ax=ax)","61cb5933":"### Look for the category distribution in categorical columns","5247f0be":"# k-nearest neighbor","28880d9f":"Now lets plot the count of each Species.","66892d8e":"### Plot for correlation","e0b381b4":"## Split Data in train and test\nThere are many ways to split the data into training and testing sets but we want our test set to represent the overall population and not just a few specific categories. Thus, instead of only using simple and common `train_test_split()` method from sklearn, we also use stratified sampling.\n\n> Stratified Sampling \u2014 We create homogeneous subgroups called strata from the overall population and sample the right number of instances to each stratum to ensure that the test set is representative of the overall population.","a1c9432a":"Now lets see how data distributed based on species.","e7d2861a":"# IRIS Flower Classification","5643d920":"### import required libraries","b45f968c":"## Exploratory Data Analysis","28015ee4":"## Selecting and Training Machine Learning Models\n\nSince this is classification problem, I am choosing k-NN Classification algorithm.","4170d856":"- We have 4 float type colums and 1 categorial type column.\n- ID column is not useful so lets remove this.","6c03e183":"Our data is distributed equally. Means each class have equal samples.","3f0cbf6d":"Data contains 150 samples and 5 features.\nFeatures are:\n- SepalLengthCm\n- SepalWidthCm\n- PetalLengthCm\n- PetalWidthCm\n- Species","b2615b12":"There is no missing value in the data.","3fdf6b8a":"### How data is distributed on each feature","018cf1a0":"### Checking Accuracy on Test Data\n\nAs we know `k=13` gives best result. Thus, initializing new model with `k=13`.","edbe7457":"### Fine-Tuning Hyperparameters for kNN"}}