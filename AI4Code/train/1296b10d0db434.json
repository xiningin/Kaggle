{"cell_type":{"233e6f4d":"code","0aa2b1b0":"code","8ec4b1aa":"code","acb0f8dc":"code","d1bc42b1":"code","c7a8b301":"code","3eeb7601":"code","68684022":"code","f4d67894":"code","18256b34":"code","63cda577":"code","b6df697f":"code","6bdf0887":"code","d5748b1d":"code","2f9b0197":"markdown","5beff4f8":"markdown","c95e172f":"markdown","d9d8c8a7":"markdown","94dc6a81":"markdown","319dc94d":"markdown","fe03395d":"markdown","326cb7cb":"markdown","90e116ce":"markdown","c10c23d8":"markdown","e97a9397":"markdown"},"source":{"233e6f4d":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","0aa2b1b0":"fashion_data = keras.datasets.fashion_mnist.load_data()\n(x_train,y_train),(x_val,y_val)= fashion_data \n\nx_train.shape, x_val.shape","8ec4b1aa":"del y_train, y_val","acb0f8dc":"plt.imshow(x_train[10])\nplt.colorbar()\nplt.show() ","d1bc42b1":"data = tf.concat([x_train, x_val], axis=0)\ndata = tf.expand_dims(data, -1)\ndata = tf.cast(data, tf.float32)\ndata = data \/ 255.0","c7a8b301":"class Sampling(layers.Layer):\n    \n    def call(self, inputs):\n        mean, logvar = inputs\n        batch = tf.shape(mean)[0]\n        dim = tf.shape(mean)[1]\n        eps = tf.keras.backend.random_normal(shape=(batch, dim))\n        return mean + tf.exp(0.5 * logvar) * eps\n    ","3eeb7601":"# latent_dim = 2\n\nencoder_inputs = keras.Input(shape=(28, 28, 1))\nx = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\nx = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = layers.Flatten()(x)\nx = layers.Dense(16, activation=\"relu\")(x)\nmean = layers.Dense(2, name=\"mean\")(x)\nlogvar = layers.Dense(2, name=\"logvar\")(x)\nz = Sampling()([mean, logvar])\nencoder = keras.Model(encoder_inputs, [mean, logvar, z], name=\"encoder\")\nencoder.summary()","68684022":"keras.utils.plot_model(encoder, show_shapes=True, dpi=64)","f4d67894":"latent_inputs = keras.Input(shape=(2,))\nx = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n# form 7 by 7 feature map\nx = layers.Reshape((7, 7, 64))(x)\n# form 14 by 14 feature map\nx = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n# form 28 by 28 feature map\nx = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n# form the sigmoid output\ndecoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\ndecoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\ndecoder.summary()","18256b34":"keras.utils.plot_model(decoder, show_shapes=True, dpi=64)","63cda577":"class VAE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = keras.metrics.Mean(\n            name=\"reconstruction_loss\"\n        )\n        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            mean, logvar, z = self.encoder(data)\n            reconstruction = self.decoder(z)\n            reconstruction_loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n                )\n            )\n            kl_loss = -0.5 * (1 + logvar - tf.square(mean) - tf.exp(logvar))\n            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n            total_loss = reconstruction_loss + kl_loss\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n        }","b6df697f":"vae = VAE(encoder, decoder)\nvae.compile(optimizer=keras.optimizers.Adam())\nhistory = vae.fit(data, epochs=30, batch_size=128)","6bdf0887":"loss = history.history['loss']\n# plot loss from 4rd epoch onwards\nindex = np.arange(3, 30)\nplt.plot(index, loss[3:], 'o-r')\nplt.xticks(np.arange(3, 30, 2))\nplt.xlabel('Epochs')\nplt.ylabel('Total Loss')\nplt.show()","d5748b1d":"def plot_latent_space(vae, n=16, figsize=8):\n    # display a n*n 2D manifold of fashion data\n    digit_size = 28\n    scale = 1.0\n    figure = np.zeros((digit_size * n, digit_size * n))\n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = np.linspace(-scale, scale, n)\n    grid_y = np.linspace(-scale, scale, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = vae.decoder.predict(z_sample)\n            digit = x_decoded[0].reshape(digit_size, digit_size)\n            figure[\n                i * digit_size : (i + 1) * digit_size,\n                j * digit_size : (j + 1) * digit_size,\n            ] = digit\n\n    plt.figure(figsize=(figsize, figsize))\n    start_range = digit_size \/\/ 2\n    end_range = n * digit_size + start_range\n    pixel_range = np.arange(start_range, end_range, digit_size)\n    sample_range_x = np.round(grid_x, 1)\n    sample_range_y = np.round(grid_y, 1)\n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"mean: z[0]\")\n    plt.ylabel(\"log of variance: z[1]\")\n    plt.imshow(figure, cmap=\"jet\")\n    plt.show()\n\n\nplot_latent_space(vae)","2f9b0197":"## Build the Sampler","5beff4f8":"## Create the Environment","c95e172f":"## Build the Encoder network","d9d8c8a7":"## Sample Mean & Variance, and Generate some images ","94dc6a81":"## Plot the Performance","319dc94d":"### Define Training with metrics and losses","fe03395d":"### Thank you for your time!","326cb7cb":"## Train the model","90e116ce":"## Load data","c10c23d8":"# Get Started With Image Generation\n\nReferences:\n\nhttps:\/\/www.tensorflow.org\/tutorials\/generative\/cvae\n\nhttps:\/\/keras.io\/examples\/generative\/vae\/\n\n\n","e97a9397":"## Build the Decoder network"}}