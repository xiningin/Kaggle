{"cell_type":{"f050e51a":"code","d6aae935":"code","425b823f":"code","e9d1d497":"code","36006cbc":"code","7a7f38f9":"code","dbfe9014":"code","b615f4eb":"code","f0083930":"code","0f06aa67":"code","01b655d4":"code","bac82210":"code","87b801c4":"code","0e6b7033":"code","1fe03cd6":"code","7c1ce5c0":"code","369750bd":"code","d74bc217":"code","2e5fcdd8":"code","d48e2156":"code","8b050b51":"code","3da7d1dd":"code","13aedb09":"code","a6e1be51":"code","11c1cb67":"code","a7209801":"code","c2bcf124":"code","3ff9ab49":"code","d1c43b11":"code","a2906ea5":"code","7e62d28a":"code","87306f62":"code","fcd376d4":"code","aff7ca16":"code","9776580e":"code","2bf9c563":"code","c0950c05":"code","5d7aa048":"code","030f052b":"code","82f5e1c8":"code","b9f9047d":"code","f0832850":"markdown","90786ca5":"markdown","396fc0d4":"markdown","ba106144":"markdown","de1c4c1b":"markdown","124f3595":"markdown","fe3d73a3":"markdown","4f6a52f8":"markdown","457f353e":"markdown","b0b958f3":"markdown","8964a257":"markdown","4f253d1d":"markdown","3a1c5836":"markdown","cc4a6dff":"markdown","3d38dd80":"markdown","e27382d3":"markdown","3d530c34":"markdown","b8e8ddf9":"markdown"},"source":{"f050e51a":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import *  \n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\n%matplotlib inline \n\nimport os\nprint(os.listdir(\"..\/input\"))","d6aae935":"df = pd.read_csv('..\/input\/heart.csv')\ndf.head()","425b823f":"df.info()","e9d1d497":"X = df.drop(['target', ], axis=1)\nX.head()","36006cbc":"y = df['target']\ny.head()","7a7f38f9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","dbfe9014":"def plot_confusion_matrix(y, pred, labels, classes, normalize=False, cmap=plt.cm.Blues):\n    \"\"\"\n    Plots the confusion matrix.\n    Args:\n        y: Data Labels\n        pred: Predicted outputs\n        labels: A list of label values to calculate confusion matrix\n        classes: A list of containing unique class names for plotting\n        normalize:Wheter to plot data with int or percentage values. Default is int.\n        cmap: Color map pf the plot\n    \n    \"\"\"\n    cm = confusion_matrix(y, pred, labels=labels)\n    \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.figure()\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","b615f4eb":"def best_model(model, train, test, grid_params):\n    \"\"\"\n    Takes a model and grid params as an input and finds the best model.\n    \n    Args:\n        model: A model class\n        train: A dict containing train features as X and labels as y\n        test: A dict containing test features as X and labels as y\n        grid_params: GridSearchCV parameters\n        \n    Returns:\n        best_estimator, table and best_params\n    \"\"\"\n    \n    grid = GridSearchCV(model, grid_params, cv=4, scoring='f1_weighted', \n                        n_jobs=-1, return_train_score=True).fit(train['X'], train['y'])\n    estimator = grid.best_estimator_\n    table = pd.DataFrame(grid.cv_results_).loc[:, \n                ['params', 'mean_test_score', 'std_test_score','mean_train_score', \n                 'std_train_score']].sort_values(by='mean_test_score', ascending=False)\n    \n    params = grid.best_params_\n    preds = estimator.predict(test['X'])\n    plot_confusion_matrix(test['y'], preds, labels=[1, 0], classes=['target=1','target=0'])\n    print(classification_report(test['y'], preds))\n    \n    return estimator, table, params\n    ","f0083930":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()","0f06aa67":"est, table, params = best_model(lr, train={'X': X_train, 'y': y_train},\n                                test={'X': X_test, 'y':y_test},\n                                grid_params=[{'solver':['liblinear', 'sag', 'newton-cg', 'lbfgs'],\n                                              'C': [0.01, 0.05, 0.1, 0.5, 1, 5]}])","01b655d4":"est","bac82210":"params","87b801c4":"table","0e6b7033":"est.predict_proba(X_test)[0:5]","1fe03cd6":"jaccard_similarity_score(y_test, est.predict(X_test))","7c1ce5c0":"log_loss(y_test, est.predict_proba(X_test))","369750bd":"from sklearn.svm import SVC\n\nsv = SVC(gamma='scale')","d74bc217":"est, table, params = best_model(sv, train={'X': X_train, 'y': y_train},\n                                test={'X': X_test, 'y':y_test},\n                                grid_params=[{'kernel':['linear', 'rbf'],\n                                              'C': [1, 3, 5, 7, 10, 20]}])","2e5fcdd8":"est","d48e2156":"table","8b050b51":"params","3da7d1dd":"jaccard_similarity_score(y_test, est.predict(X_test))","13aedb09":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()","a6e1be51":"est, table, params = best_model(knn, train={'X': X_train, 'y': y_train},\n                                test={'X': X_test, 'y':y_test},\n                                grid_params=[{'n_neighbors':list(range(5,30)),\n                                              'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n                                              'leaf_size': [10, 20, 30, 40, 50]}])","11c1cb67":"est","a7209801":"table.head()","c2bcf124":"params","3ff9ab49":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()","d1c43b11":"est, table, params = best_model(dt, train={'X': X_train, 'y': y_train},\n                                test={'X': X_test, 'y':y_test},\n                                grid_params=[{'max_depth':list(range(4,15)),\n                                              'criterion': ['gini', 'entropy']}])","a2906ea5":"est","7e62d28a":"table","87306f62":"params","fcd376d4":"import matplotlib.image as mpimg\nfrom sklearn import tree\n\nfilename = \"tree.png\"\nfeature_names = X.columns.tolist()\ntarget_names = ['0', '1']\ntree.export_graphviz(est, feature_names=feature_names, out_file='tree.dot', \n                           class_names=target_names, filled=True, \n                           special_characters=True) ","aff7ca16":"print(os.listdir('..\/working\/'))","9776580e":"! dot -Tpng tree.dot -o tree.png","2bf9c563":"img = mpimg.imread('..\/working\/tree.png')\nplt.figure(figsize=(100, 200))\nplt.imshow(img, interpolation='nearest')","c0950c05":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()","5d7aa048":"est, table, params = best_model(nb, train={'X': X_train, 'y': y_train},\n                                test={'X': X_test, 'y':y_test},\n                                grid_params=[{'var_smoothing':[1e-2, 1e-3, 1e-4, 1e-5,\n                                                               1e-6, 1e-7, 1e-8, 1e-9]}])","030f052b":"est","82f5e1c8":"table","b9f9047d":"params","f0832850":"## Naive Bayes\n\nNaive Bayes classifiers are built on Bayesian classification methods.\nThese rely on Bayes's theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities.\nIn Bayesian classification, we're interested in finding the probability of a label given some observed features, which we can write as $P(L~|~{\\rm features})$.\nBayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n\n$$\nP(L~|~{\\rm features}) = \\frac{P({\\rm features}~|~L)P(L)}{P({\\rm features})}\n$$\n\nIf we are trying to decide between two labels\u2014let's call them $L_1$ and $L_2$\u2014then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n\n$$\n\\frac{P(L_1~|~{\\rm features})}{P(L_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~L_1)}{P({\\rm features}~|~L_2)}\\frac{P(L_1)}{P(L_2)}\n$$\n\nAll we need now is some model by which we can compute $P({\\rm features}~|~L_i)$ for each label.\nSuch a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\nSpecifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\nThe general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n\nThis is where the \"naive\" in \"naive Bayes\" comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.\nDifferent types of naive Bayes classifiers rest on different naive assumptions about the data, and we will examine gaussian version with this data.","90786ca5":"## Decision Trees","396fc0d4":"## Logistic Regression\n\n### What is different between Linear and Logistic Regression?\n\nWhile Linear Regression is suited for estimating continuous values (e.g. estimating house price), it is not the best tool for predicting the class of an observed data point. In order to estimate the class of a data point, we need some sort of guidance on what would be the **most probable class** for that data point. For this, we use **Logistic Regression**.\n\n<div class=\"alert alert-success alertsuccess\">\n<font size = 3><strong>Recall linear regression:<\/strong><\/font>\n<br>\n<br>\nAs you know, __Linear regression__ finds a function that relates a continuous dependent variable, y, to some predictors (independent variables $x_1, x_2 $, etc.). For example, Simple linear regression assumes a function of the form:\n<br><br>\n$$\ny = \ud835\udf03_0 + \ud835\udf03_1 * x_1 + \ud835\udf03_2 * x_2 +...\n$$\n<br>\nand finds the values of parameters $\u03b8_0, \u03b8_1, \u03b8_2$, etc, where the term $\u03b8_0$ is the \"intercept\". It can be generally shown as:\n<br><br>\n$$\n\u210e_\u03b8(\ud835\udc65) = \ud835\udf03^TX\n$$\n<p><\/p>\n\n<\/div>\n\nLogistic Regression is a variation of Linear Regression, useful when the observed dependent variable, y, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.\n\nLogistic regression fits a special s-shaped curve by taking the linear regression and transforming the numeric estimate into a probability with the following function, which is called sigmoid function \ud835\udf0e:\n\n$$\n\u210e_\u03b8(\ud835\udc65) = \ud835\udf0e({\u03b8^TX}) =  \\frac {e^{(\u03b8_0 + \u03b8_1 * x_1 + \u03b8_2 * x_2 +...)}}{1 + e^{(\u03b8_0 + \u03b8_1 * x_1 + \u03b8_2 * x_2 +...)}}\n$$\nOr:\n$$\nProbabilityOfaClass_1 =  P(Y=1|X) = \ud835\udf0e({\u03b8^TX}) = \\frac{e^{\u03b8^TX}}{1+e^{\u03b8^TX}} \n$$\n\nIn this equation, ${\u03b8^TX}$ is the regression result (the sum of the variables weighted by the coefficients), `exp` is the exponential function and $\ud835\udf0e(\u03b8^TX)$ is the sigmoid or [logistic function](http:\/\/en.wikipedia.org\/wiki\/Logistic_function), also called logistic curve. It is a common \"S\" shape (sigmoid curve).\n\nSo, briefly, Logistic Regression passes the input through the logistic\/sigmoid but then treats the result as a probability:\n\n<img\nsrc=\"https:\/\/ibm.box.com\/shared\/static\/kgv9alcghmjcv97op4d6onkyxevk23b1.png\" width = \"400\" align = \"center\">\n\n\nThe objective of __Logistic Regression__ algorithm, is to find the best parameters \u03b8, for $\u210e_\u03b8(\ud835\udc65) = \ud835\udf0e({\u03b8^TX})$, in such a way that the model best predicts the class of each case.","ba106144":"\n**K-Nearest Neighbors** is an algorithm for supervised learning. Where the data is 'trained' with data points corresponding to their classification. Once a point is to be predicted, it takes into account the 'K' nearest points to it to determine it's classification.\n\n### Here's an visualization of the K-Nearest Neighbors algorithm.\n\n<img src = \"https:\/\/ibm.box.com\/shared\/static\/mgkn92xck0z05v7yjq8pqziukxvc2461.png\" width=500 height=500>\n\nIn this case, we have data points of Class A and B. We want to predict what the star (test data point) is. If we consider a k value of 3 (3 nearest data points) we will obtain a prediction of Class B. Yet if we consider a k value of 6, we will obtain a prediction of Class A.\n\nIn this sense, it is important to consider the value of k. But hopefully from this diagram, you should get a sense of what the K-Nearest Neighbors algorithm is. It considers the 'K' Nearest Neighbors (points) when it predicts the classification of the test point.","de1c4c1b":"**Utility Functions**","124f3595":"## SVM(Support Vector Machines) ","fe3d73a3":" SVM works by mapping data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable. A separator between the categories is found, then the data are transformed in such a way that the separator could be drawn as a hyperplane. Following this, characteristics of new data can be used to predict the group to which a new record should belong.\n\nThe SVM algorithm offers a choice of kernel functions for performing its processing. Basically, mapping data into a higher dimensional space is called kernelling. The mathematical function used for the transformation is known as the\u00a0kernel\u00a0function, and can be of different types, such as:\n\n    1.Linear\n    2.Polynomial\n    3.Radial basis function (RBF)\n    4.Sigmoid\n    \nEach of these functions has its characteristics, its pros and cons, and its equation, but as there's no easy way of knowing which function performs best with any given dataset, we usually choose different functions in turn and compare the results.","4f6a52f8":"## Evaluation\n\n### Jaccard Index\n\nLets try jaccard index for accuracy evaluation. we can define jaccard as the size of the intersection divided by the size of the union of two label sets. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","457f353e":"**See the Generated Tree Structure**","b0b958f3":"### Log Loss\n\nNow, lets try __log loss__ for evaluation. In logistic regression, the output can be a probability (or equals to 1). This probability is a value between 0 and 1.\nLog loss(\u00a0Logarithmic\u00a0loss) measures the performance of a\u00a0classifier\u00a0where the predicted output is a probability value between 0 and 1. ","8964a257":"## K-Nearest Neighbors","4f253d1d":"# Supervised Classification Algorithms ","3a1c5836":"Lets build our model using __LogisticRegression__ from Scikit-learn package. This function implements logistic regression and can use different numerical optimizers to find parameters, including \u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019 solvers. You can find extensive information about the pros and cons of these optimizers if you search it in internet.\n\nThe version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem in machine learning models.\n**C** parameter indicates __inverse of regularization strength__ which must be a positive float. Smaller values specify stronger regularization. \nNow lets fit our model with train set:","cc4a6dff":"## Conclusions\n\nOur best method would be Logistic Regression for this problem with %93 accuracy. \nI hope you liked my kernel - don't forget to upvote if you do :).","3d38dd80":"Based on the count of each section, we can calculate precision and recall of each label:\n\n- __Precision__ is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP\u00a0\/\u00a0(TP\u00a0+\u00a0FP)\n\n- __Recall__ is true positive rate. It is defined as: Recall = \u00a0TP\u00a0\/\u00a0(TP\u00a0+\u00a0FN)\n    \nSo, we can calculate precision and recall of each class.\n\n__F1 score:__\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label. \n\nThe F1score is the harmonic average of the\u00a0precision and recall, where an F1\u00a0score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nAnd finally, we can tell the average accuracy for this classifier is the average of the f1-score for both labels, which is 0.93 in our case.\n\nThe classifier correctly predicted 22 of them as 0, and 3 of them wrongly as 1. So, it hasn done a good job in predicting the target with value 0 and it did a good job predicting labels positive data. We see that 35 out of 36 was predicted correctly. A good thing about confusion matrix is that shows the model\u2019s ability to correctly predict or separate the classes.  In specific case of binary classifier, such as this one,  we can interpret these numbers as the count of true positives, false positives, true negatives, and false negatives. ","e27382d3":"## About the Dataset\n\n> age: The person's age in years  \n> sex: The person's sex (1 = male, 0 = female)  \n> cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)  \n> trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)  \n> chol: The person's cholesterol measurement in mg\/dl  \n> fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)  \n> restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)  \n> thalach: The person's maximum heart rate achieved  \n> exang: Exercise induced angina (1 = yes; 0 = no)  \n> oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)  \n> slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)  \n> ca: The number of major vessels (0-3)  \n> thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)  \n> target: Heart disease (0 = no, 1 = yes)  ","3d530c34":"__predict_proba__  returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 1, P(Y=1|X), and second column is probability of class 0, P(Y=0|X):","b8e8ddf9":"The picture is not so clear now but when working locally, we have the option to zoom in for real structure of the data"}}