{"cell_type":{"6ffe77db":"code","08974687":"code","f09ad611":"code","f5dbb21b":"code","8435f8f4":"code","65361129":"code","b13c78fb":"code","3b64d996":"code","28030c96":"code","a1de8ab1":"code","5a4afe9c":"code","bba0a5e1":"code","2dd0e79f":"code","7b4480e3":"code","075e76e8":"code","7a69426f":"code","2cbfc49f":"code","8c11b55e":"code","e2945ea5":"code","dfe2a442":"code","3630e442":"code","5b35d099":"code","7cce9c1b":"code","0ed455fb":"code","bc576d8b":"code","e9579331":"code","04b33688":"code","4a29eabd":"code","11e70751":"code","06f3fa62":"code","151d2c9d":"markdown","10e7276c":"markdown","73b13928":"markdown","a295211e":"markdown","bb953efa":"markdown","da540a47":"markdown","2ef88da6":"markdown","197876ff":"markdown","5409c180":"markdown","7ff78eba":"markdown","580c8d56":"markdown","9a4a6be5":"markdown","ca48cf53":"markdown","b47c04fa":"markdown","bfd6a5bd":"markdown","30a150a3":"markdown","e1446989":"markdown","8f909813":"markdown","11943106":"markdown","1dde380e":"markdown","3192ad67":"markdown","ac31a46d":"markdown","50138e59":"markdown","8747246e":"markdown","5c694511":"markdown","18077e6b":"markdown","086e3e1f":"markdown","50c2d91e":"markdown","1063cc17":"markdown","225e931f":"markdown","1d6d33a3":"markdown"},"source":{"6ffe77db":"!git clone https:\/\/github.com\/tkeldenich\/NLP_Preprocessing.git &> \/dev\/null","08974687":"import numpy as np\nimport pandas as pd\n\nimport os","f09ad611":"train_data = pd.read_csv('\/kaggle\/working\/NLP_Preprocessing\/train.csv')\n\ntrain_data.head()","f5dbb21b":"import nltk\nimport string\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('words')\nnltk.download('wordnet')","8435f8f4":"stopwords = nltk.corpus.stopwords.words('english')\nwords = set(nltk.corpus.words.words())\nlemmatizer = WordNetLemmatizer()","65361129":"def Preprocess_listofSentence(listofSentence):\n    preprocess_list = []\n    for sentence in listofSentence :\n        sentence_w_punct = \"\".join([i.lower() for i in sentence if i not in string.punctuation])\n\n        sentence_w_num = ''.join(i for i in sentence_w_punct if not i.isdigit())\n\n        tokenize_sentence = nltk.tokenize.word_tokenize(sentence_w_num)\n\n        words_w_stopwords = [i for i in tokenize_sentence if i not in stopwords]\n\n        words_lemmatize = (lemmatizer.lemmatize(w) for w in words_w_stopwords)\n\n        sentence_clean = ' '.join(w for w in words_lemmatize if w.lower() in words or not w.isalpha())\n\n        preprocess_list.append(sentence_clean)\n\n    return preprocess_list","b13c78fb":"preprocess_list = Preprocess_listofSentence(train_data['text'])","3b64d996":"print('Base sentence : '+train_data['text'][2])\nprint('Cleaned sentence : '+preprocess_list[2])","28030c96":"!pip install git+https:\/\/github.com\/ClaudeCoulombe\/FrenchLefffLemmatizer.git &> \/dev\/null","a1de8ab1":"import nltk\nimport string\nfrom french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')","5a4afe9c":"french_stopwords = nltk.corpus.stopwords.words('french')\nmots = set(line.strip() for line in open('\/kaggle\/working\/NLP_Preprocessing\/dictionnaire.txt'))\nlemmatizer = FrenchLefffLemmatizer()","bba0a5e1":"def French_Preprocess_listofSentence(listofSentence):\n    preprocess_list = []\n    for sentence in listofSentence :\n        sentence_w_punct = \"\".join([i.lower() for i in sentence if i not in string.punctuation])\n\n        sentence_w_num = ''.join(i for i in sentence_w_punct if not i.isdigit())\n\n        tokenize_sentence = nltk.tokenize.word_tokenize(sentence_w_num)\n\n        words_w_stopwords = [i for i in tokenize_sentence if i not in french_stopwords]\n\n        words_lemmatize = (lemmatizer.lemmatize(w) for w in words_w_stopwords)\n\n        sentence_clean = ' '.join(w for w in words_lemmatize if w.lower() in mots or not w.isalpha())\n\n        preprocess_list.append(sentence_clean)\n\n    return preprocess_list","2dd0e79f":"lst = ['C\\'est un test pour lemmatizer',\n       'plusieurs phrases pour un nettoyage',\n       'eh voil\u00e0 la troisi\u00e8me !']\nfrench_text = pd.DataFrame(lst, columns =['text'])","7b4480e3":"french_preprocess_list = French_Preprocess_listofSentence(french_text['text'])","075e76e8":"print('Base sentence : '+lst[1])\nprint('Cleaned sentence : '+french_preprocess_list[1])","7a69426f":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\n\nX = vectorizer.fit_transform(preprocess_list)","2cbfc49f":"X.toarray()[0]","8c11b55e":"vectorizer.get_feature_names()[:5]","e2945ea5":"!pip install zeugma &> \/dev\/null","dfe2a442":"from zeugma import TextsToSequences\n\nsequencer = TextsToSequences()\nembedded_sequ = sequencer.fit_transform(preprocess_list)","3630e442":"embedded_sequ[0]","5b35d099":"from keras.preprocessing import sequence\nmax_len = 40\n\npad_sequ = sequence.pad_sequences(embedded_sequ, maxlen=max_len)","7cce9c1b":"print(pad_sequ[0])","0ed455fb":"longueur_dict = max(list(map(lambda x: max(x), pad_sequ)))+1","bc576d8b":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\n\nmodel = Sequential()\nmodel.add(Embedding(longueur_dict, 8, input_length = max_len))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))","e9579331":"tokenize_sentences = []\n\nfor i in range(len(preprocess_list)):\n    tokenize_sentences.append(nltk.tokenize.word_tokenize(preprocess_list[i]))","04b33688":"from gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\n\nmodel_W2V = Word2Vec(sentences=tokenize_sentences, size=100, window=5, min_count=1, workers=4)\nmodel_W2V.train(tokenize_sentences, total_examples=len(tokenize_sentences), epochs=50)\n","4a29eabd":"tokenize_sentences[0][1]\nmodel_W2V.similar_by_word(tokenize_sentences[0][1])[:5]","11e70751":"from sklearn.manifold import TSNE\nfrom matplotlib import pyplot as plt\n\ndef display_closestwords_tsnescatterplot_perso(model, word):\n    arr = np.empty((0,100), dtype='f')\n    word_labels = [word]\n\n    numb_sim_words = 5\n\n    # get close words\n    close_words = model.similar_by_word(word)[:numb_sim_words]\n\n    # add the vector for each of the closest words to the array\n    arr = np.append(arr, np.array([model[word]]), axis=0)\n    for wrd_score in close_words:\n        wrd_vector = model[wrd_score[0]]\n        word_labels.append(wrd_score[0])\n        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n\n    # find tsne coords for 2 dimensions\n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    Y = tsne.fit_transform(arr)\n\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n\n    # color for words\n    color = ['red']\n    for i in range(numb_sim_words):\n        color.append('blue')\n\n    # display scatter plot\n    plt.scatter(x_coords, y_coords, c = color)\n\n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(1, 5), textcoords='offset points')\n    plt.xlim(min(x_coords)-100, max(x_coords)+100)\n    plt.ylim(min(y_coords)-100, max(y_coords)+100)\n    plt.show()\n\n    print(\"Word most similar to : \"+word)\n    print([sim_word[0] for sim_word in close_words])","06f3fa62":"display_closestwords_tsnescatterplot_perso(model_W2V, tokenize_sentences[0][0])","151d2c9d":"Afterwards we **build** our **preprocessing function** which will successively :\n\n- remove the **punctuation**\n- remove the **numbers**\n- transform the sentences into a **list of tokens** (a list of words)\n- remove **stopwords** (words that don't make sense)\n- **lemmatize**\n- keep only the **words present in the dictionary**\n- remove **capital letters**\n- **reform sentences** with the remaining words","10e7276c":"And we look at **the results** :\n","73b13928":"Importing **libraries**.","a295211e":"Then we **use it** :","bb953efa":"# **Preprocessing NLP - Tutorial to quickly clean up a text**\n- [Article link for English speakers](https:\/\/inside-machinelearning.com\/en\/preprocessing-nlp-preprocessing\/)\n- [Article link for French speakers](https:\/\/inside-machinelearning.com\/preprocessing-nlp-tutoriel-pour-nettoyer-rapidement-un-texte\/)","da540a47":"# **Cleaning the data**\n\nOnce the **data is loaded** it needs to be **cleaned up**, this is called **preprocessing**.\n\n**In most cases for NLP**, preprocessing consists of **removing non-letter characters** such as \"#\", \"-\", \"!\", **numbers** or even **words that do not make sense** or are not part of the language being analyzed.\n\n**Keep in mind** however that for **certain types of problems** it can be interesting **to preserve certain types of characters**.\n\nFor example: to analyze if an **email is a spam or not**, we can imagine that the '!' are a **good indicator** and therefore do **not remove them** during cleaning.\n\nHere we will **code two functions** :\n\n- one to **clean English language sentences**\n- one to **clean French language sentences**\n\nThese two functions have, moreover, the **same architecture**.\n\n## **English text**\n\nFirst, we **import** the **necessary libraries** :","2ef88da6":"We **use it** :","197876ff":"# **The different encodings**\n\nOnce we have **extracted the useful informations** from our sentences, we can move on to **the encoding phase**.\n\n**Encoding** is an essential step in **Machine Learning**.\n\nIndeed, it allows us to **transform the text data into numbers** that the machine can **interpret and understand**.\n\nThere are **different types of encoding** and we are now going to talk about **the most famous ones !**\n\n## **One-Hot Encoding**\n\nThe **One-Hot Encoding** is the **most known** method, the **easiest** to realize, and the one that allowed me to have **the best accuracy** in most of my personal works in NLP.\n\nOne-Hot consists in **creating a dictionary** with **every words that appear** in our cleaned sentences.\n\nThis dictionary is in fact **a table** where **each column represents a word** and **each row represents a sentence**.\n\n**If such a word appears in such a sentence**, we put a **1** in the element of the table, **otherwise** we put a **0**.\n\nWe will thus have an **array composed only of 0 and 1**.\n\nTo realize the **One-Hot Encoding** in **Python**, we initialize the **dictionary** with the *CountVectorizer()* function of the *Sklearn library*.\n\nThen we use the *fit_transform()* **function** on our **preprocessed data**.","5409c180":"In fact the **class** *vectorizer* keeps a lot of other **information about the dictionary**.\n\nMoreover, if we want to **encode new sentences** to use on our trained **Machine Learning model** we will have to use the *fit()* **function** of the *vectorizer* **class**.\n\nThis way we can **adapt these new sentences to our dictionary**. However, this implies that if these new sentences **contain a word that is not in the dictionary**, it will **not be taken into account**.\n\nWe can see **the words in the dictionary** with the *get_feature_names()* **function**.","7ff78eba":"We can then **run the learning process** with the *fit()* function and **use our model !**","580c8d56":"And then use it by **specifying the Embedding model** and **the word to analyze** :","9a4a6be5":"## **French text**\n\nHere we will first install the **FrenchLefffLemmatizer** library which allows to perform a **lemmatization in French**.","ca48cf53":"The only **disadvantage of One-Hot Encoding** is that we **lose the hierarchy**, the order of the words.\n\nThis means that we **lose** the context, **the meaning of the sentence** and in theory this should impoverish the results of our model.\n\n**In practice, it is quite different**, we can have results with **80-85% accuracy** which is actually **very interesting for NLP!**\n\n## **Word embeddings**\n\n### **Hierarchical encoding**\n\nHere we use **another type** of encoding: **hierarchical encoding**.\n\nContrary to One-Hot Encoding, as you can imagine, we **keep the hierarchy**, the order of the words and therefore **the meaning of the sentence**.\n\nWe have **another type of dictionary** here. In fact, **each word is represented by a number**.\n\n**Each sentence** will therefore be a **sequence of numbers**.\n\nAn **example** will be **more relevant** :\n\n\"I play the video game\" will be [1, 2, 3, 4, 5]\n\"I watch a video\" will be [1, 6, 7, 4]\n\nFor this **encoding** we import the **zeugma library**.","b47c04fa":"### **Visualization**\n\nThe **model learned** the similarity of words **based on the context of our sentences**.\n\nThe **title (context) of our dataset** is *'Disaster Tweet'*\n\nWe can for example **look at which word is close to 'fire'** by using the *similar_by_word()* **function**.","bfd6a5bd":"And we can **display** an example of a **cleaned sentence** :","30a150a3":"Afterwards we build our **preprocessing function** which will successively :\n\n- remove the **punctuation**\n- remove the **numbers**\n- transform the sentences into a **list of tokens** (a list of words)\n- remove **stopwords** (words that don't bring understanding)\n- **lemmatize**\n- remove **capital letters**\n- **reform sentences** with the remaining words","e1446989":"One last thing to do : **normalize our data**.\n\nYes, in order to use **Machine Learning**, our data must be in **tensor format**.\n\nThis implies that **every encoded sentences** must have **the same size**.\n\nTo do so, we have **two choices** here:\n\n- **add 'emptiness'** to the shortest sentences\n- **truncate** the longest sentences\n\nFor these two choices, **a same function exists** : *sequence.pad_sequences()*\n\nIt has **two parameters** :\n\n- **sentences** the list of sentences to fill\/truncate\n- **maxlen** the final length that each sentence will have\n\nIn fact this function **truncates the sentences** with a **length higher** than maxlen and **fills with 0** the sentences with a **length lower** than maxlen.","8f909813":"Then we **insert this data** into a **Pandas dataframe**.","11943106":"We create data to **try out our function** :","1dde380e":"Then, we use the *Word2Vec* **function** of the *Gensim library*.\n\nThis function has **five main parameters**:\n\n- **size :** The dimension of the created vector, ideally smaller than the number of words in the vocabulary\n- **window :** The maximum distance between a target word and the words around the target word. The default window is 5.\n- **min_count :** The minimum number of words to consider when training the model; words with less than this number of occurrences will be ignored. The default value of min_count is 5.\n- **worker :** The number of batches created for training, by default there are 3.\n\nFirst we **initialize the Word2Vec**, then we **train it on our data !**","3192ad67":"Then, the operation is about **the same as for the one-hot encoding** : we use the *TextsToSequences()* function to **create our dictionary**.\n\nAnd we use the *fit_transform()* **function** on our **preprocessed sentences**.","ac31a46d":"Then we **initialize** :\n\n- **stopwords**, which are words that appear frequently but do not bring any meaning to the sentence (like \"of\", \"the\", \"a\")\n- **words** that come from an English dictionary\n- a **lemmatizer**, this object allows us to preserve the root of the words so that two words having the same strain will be considered as one and the same word (example: 'neighbors' and 'neighborhood' will both be changed into 'neighbor')","50138e59":"The **disadvantage of Word2vec** is that it learns the meaning of a word **only according to the words around it**, whereas **Keras** learns the meaning of words **according to the purpose** (y_train) set during the learning process.\n\nIn other words, **Word2Vec has an unsupervised approach** and **Keras a supervised approach**.","8747246e":"**With this knowledge**, you should have **the tools in hand** to train **your own NLP models** or **improve your already trained ones**.\n\nIf you want to **know more about NLP Machine Learning models**, feel free to [check our articles on the subject !](https:\/\/inside-machinelearning.com\/en\/a-simple-and-efficient-model-for-binary-classification-in-nlp\/) \ud83d\ude0a","5c694511":"# **To go further...**\n\n## **Word2Vec Embedding**\n\n### **Training**\n\nThere are **different libraries** to realize **embedding**.\n\n**Keras** is particularly useful for this task because it allows to **train the embedding at the same time as the Deep Learning model**.\n\nThe **Gensim library** is at least as interesting as Keras because it allows us to **visualize this embedding**.\n\nThat is to say that we can **analyze the embedding** by looking at **which word is similar to which other**.\n\nFor this embedding, **our data must be in the form of tokens** (each word separated) and not in the form of sentences.","18077e6b":"You can **display** an **encoded sentence** to see the result :","086e3e1f":"# **Preprocessing**\n\n## **Loading the data**\n\nFirst, as usual, we will **load our data**.\n\nHere we take a **csv file** from [this Kaggle competition](https:\/\/www.kaggle.com\/c\/nlp-getting-started) containing **several thousand English phrases**.\n\n**Perfect for us** \ud83d\ude09\n\nWe load the **sentences** from this [Github directory](https:\/\/github.com\/tkeldenich\/NLP_Preprocessing).","50c2d91e":"### **Couche Embedding**\n\nActually, the **hierarchical encoding** is a **prerequisite** to use the *Embedding layer* of **Keras**.\n\nThis layer allows us to **give coordinates** to **every words** of our dictionary **while performing a learning process**.\n\nThe idea is that **the closer the words have a meaning, the closer the words have coordinates**.\n\nThe **layer** will therefore **improve**, like every other layers, **during the learning process**. Thus at the end of the learning process, the layer will have given **precise coordinates for each word**.\n\nThe Embedding layer has **three parameters**:\n\n- **input_dim**, the number of words in our dictionary + 1\n- **output_dim**, the dimension of the output tensor\n- **input_length**, the length of the vectors (normalized sentence length)","1063cc17":"The **first five** are 'decomposition', 'township', 'racer', 'beast' and 'apartment'.\n\nThis means that **most of the time**, 'fire' has been used **alongside these words**.\n\n**To better visualize this similarity** we can use the **following function** :","225e931f":"An **example** of a sentence with **hierarchical encoding** :","1d6d33a3":"Then we **initialize** :\n\n- the stopwords, which are words that appear very frequently but do not bring any meaning to the sentence (like \"de\", \"le\", \"une\")\n- **words** that come from a **French dictionary**\n- a **lemmatizer**, this object allows us to preserve the root of the words so that two words having the same strain will be considered as the same word (example: 'voisine' and 'voisinage' will both be changed to 'voisin')"}}