{"cell_type":{"60e0d728":"code","1dd95879":"code","a7f8b32b":"code","389b4c69":"code","94e96bac":"code","00310f93":"code","522f1131":"code","2f664428":"code","725f27a5":"code","9aaae212":"code","a77cc81f":"code","276aefef":"code","3add9e8c":"code","56a05199":"code","f704d554":"code","0f17c282":"code","71134e6c":"code","e632ce52":"code","faa57766":"code","38279ae3":"code","869a139d":"code","1dbb3eb7":"code","d549796b":"code","847e10b0":"code","92364192":"code","97edc01a":"code","ff8ef46d":"code","0f37468a":"code","ea532475":"code","991d3dc3":"code","3eb870ea":"code","4e9286a7":"code","76132c1d":"code","1f0b5ed8":"code","a40f54e0":"code","b2a43c79":"code","0e281d5d":"code","b59f1cd5":"code","294c4652":"code","469ef2ec":"code","c7a55712":"code","4319b330":"code","8216a4cd":"code","e1a4d38d":"code","453fe753":"code","9ebe5be0":"code","d52e2ac3":"code","6fbb6d1a":"code","c9e38bc2":"code","34bda20d":"code","92a466d5":"code","9c285e34":"code","cd24e07d":"code","151c5fc7":"code","0aded10f":"code","d907651c":"code","cb7fa78b":"code","eaefed48":"code","0fab7033":"code","3cac253d":"markdown","8a389189":"markdown","6ddf2cab":"markdown","3ce472b6":"markdown","b74be851":"markdown","f1a475fd":"markdown","626a3186":"markdown","7a564810":"markdown","55cab62a":"markdown","19ddb3ce":"markdown","0eb6f16b":"markdown","30b75768":"markdown","271fcde0":"markdown","3a492449":"markdown","b9a02814":"markdown","9e82645d":"markdown","2db9aaf2":"markdown","f3568bab":"markdown","24f9785c":"markdown","f6d5f3dd":"markdown","a5afcbc2":"markdown","ef9a31a6":"markdown","639a95eb":"markdown","cbf90efc":"markdown","94c7a720":"markdown","a0cfbf84":"markdown","4dbb44bd":"markdown"},"source":{"60e0d728":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport missingno as msno\n\n\n\nfrom sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\n\n\nfrom lightgbm import LGBMClassifier\n\n\nimport shap\n\n\npd.set_option('display.max_columns', None)\n","1dd95879":"INPUT_PATH=\"\/kaggle\/input\/widsdatathon2021\/\"\n","a7f8b32b":"train=pd.read_csv(os.path.join(INPUT_PATH,\"TrainingWiDS2021.csv\"))\nprint(\"Shape of Training Data \",train.shape)\ntest=pd.read_csv(os.path.join(INPUT_PATH,\"UnlabeledWiDS2021.csv\"))\nprint(\"Shape of Testing Data \",test.shape)","389b4c69":"train.head()","94e96bac":"## Dropping off Unnamed: 0\n\ntrain.drop('Unnamed: 0',axis=1,inplace=True)\nprint(\"Shape of Train Data \",train.shape)\n\ntest.drop('Unnamed: 0',axis=1,inplace=True)\nprint(\"Shape of Test Data \",test.shape)","00310f93":"train.info(verbose=True,null_counts=True)","522f1131":"print(\"Number of Columns with each Data Type\")\nprint(train.dtypes.value_counts())","2f664428":"print(\"Columns with Data Type - 'Object'\")\nprint(train.select_dtypes(\"object\").columns)\n ","725f27a5":"object_cols=train.select_dtypes(\"object\").columns.tolist()\nprint(\"Number of Unique Values in Columns of type 'Object' \")\nprint(train[object_cols].nunique())\n\nprint(\"\\n\\n\")\nprint(\"Number of Null Values in Columns of Type 'Object' \")\nprint(train[object_cols].isnull().sum())\nprint(\"Percentage of Null Values in Columns of Type 'Object' \")\nprint((train[object_cols].isnull().sum()\/train.shape[0])*100)\n\n\nprint(\"Replacing Missing Values in Columns of Data Type 'Object' by 'Missing'\")\nfor col in object_cols:\n    train.loc[pd.isnull(train[col]),col]=\"Missing\"\n    test.loc[pd.isnull(test[col]),col]=\"Missing\"\n\n    ","9aaae212":"print(\"Columns with Data Type - 'Int'\")\nprint(train.select_dtypes(\"int\").columns)","a77cc81f":"int_cols=train.select_dtypes(\"int\").columns.tolist()\nprint(\"Number of Unique Values in Columns of type 'int' \")\nprint(train[int_cols].nunique())\n\nprint(\"\\n\\n\")\nprint(\"Number of Null Values in Columns of Type 'int' \")\nprint(train[int_cols].isnull().sum())\nprint(\"Percentage of Null Values in Columns of Type 'int' \")\nprint((train[int_cols].isnull().sum()\/train.shape[0])*100)\n\ncommon_hosp_id=set(train['hospital_id'].tolist()).intersection(set(test['hospital_id'].tolist()))\nprint(\"number of hospital id common in Train and Test \",len(common_hosp_id))\n\n\ncommon_icu_id=set(train['icu_id'].tolist()).intersection(set(test['icu_id'].tolist()))\nprint(\"number of hospital id common in Train and Test \",len(common_hosp_id))\n","276aefef":"cols_to_drop=['readmission_status','hospital_id','icu_id','encounter_id']\n\n","3add9e8c":"## extract int columns except id columns and convert into categorical\ntemp=pd.DataFrame(train[int_cols].nunique()).reset_index()\ntemp.columns=['int_col','num_unique']\nint_to_cat_cols=temp.loc[temp['num_unique']==2,'int_col'].tolist()\nint_to_cat_cols","56a05199":"test['diabetes_mellitus']=0\n#for col in int_to_cat_cols:\n    #train[col]=train[col].astype('category')\n    #test[col]=test[col].astype('category')\n    ","f704d554":"float_cols=train.select_dtypes(\"float\").columns.tolist()\ntrain[float_cols].describe()","0f17c282":"missing_df=pd.DataFrame(train[float_cols].isnull().sum()).reset_index()\nmissing_df.columns=['column_name','num_missing']\nmissing_df['percent_missing']=(missing_df['num_missing']\/train.shape[0])*100\nmissing_df=missing_df.sort_values(\"percent_missing\",ascending=False)\nmissing_df","71134e6c":"train['dataset']=\"train\"\ntest['dataset']=\"test\"\ntrain.shape,test.shape","e632ce52":"data=pd.concat([train,test])\ndata.shape","faa57766":"## Checking if age value is 0 only in train data or in both\ndata.loc[data['age']==0,'dataset'].value_counts()","38279ae3":"data=data[data['age']!=0]\nprint(\"Shape of Data after dropping age=0 cases \",data.shape)","869a139d":"d1_col=[col for col in data.columns if 'd1_' in col]\nh1_col=[col for col in data.columns if \"h1_\" in col]\nprint(\"Number of d1 cols \",len(d1_col))\nprint(\"Number of h1 cols \",len(h1_col))","1dbb3eb7":"data['num_d1_missing']=data[d1_col].isnull().sum(axis=1)\ndata['num_h1_missing']=data[h1_col].isnull().sum(axis=1)\ndata['diff_d1_missing_h1_missing']=data['num_d1_missing'] - data['num_h1_missing'] ## This will give number of tests for which daily labs taken, but bot hourly labs","d549796b":"def weighted_classt(x): \n    if pd.isna(x):\n        return np.nan\n    elif x < 15: \n        return 'very severely underweight' \n    elif x >= 15 and x < 16: \n        return 'severely weight' \n    elif x >=16 and x < 18.5: \n        return 'underweight' \n    elif x >= 18.5 and x < 25: \n        return 'healthy weight' \n    elif x >= 25 and x < 30: \n        return 'overweight'\n    elif x >= 30 and x < 35: \n        return 'class 1' \n    elif x >= 35 and x < 40: \n        return 'class 2' \n    else: \n        return 'class 3' ","847e10b0":"data['bmi_class']=data['bmi'].apply(lambda x:weighted_classt(x))\nobject_cols.append('bmi_class')","92364192":"test_cols=d1_col+h1_col\ntest_names=list(set([col[3 :-4] for col in test_cols]))\nprint(\"List of Test in the Dataset \")\nprint(test_names)\nprint(\"Number of Tests in the Dataset\")\nprint(len(test_names))","97edc01a":"def testdoneonlydaily(d1_max,d1_min,h1_max,h1_min):\n    if pd.isna(h1_max) & pd.isna(h1_min) & ~pd.isna(d1_max) & ~pd.isna(d1_min):\n        return 1\n    else:\n        return 0","ff8ef46d":"for test_name in test_names:\n    data['d1_'+test_name+\"_normal_range\"]=(data['d1_'+test_name+\"_max\"] - data['d1_'+test_name+\"_min\"])\/data['d1_'+test_name+\"_max\"] \n    data['h1_'+test_name+\"_normal_range\"]=(data['h1_'+test_name+\"_max\"] - data['h1_'+test_name+\"_min\"])\/data['h1_'+test_name+\"_max\"] \n    data[test_name+'_is_daily_greater_than_hourly']=(data['d1_'+test_name+\"_normal_range\"] >data['h1_'+test_name+\"_normal_range\"])\n    ##is hourly noy done and daily done\n    data[test_name+\"_hourly_not_done_daily_done\"]=data.apply(lambda row:testdoneonlydaily(row['d1_'+test_name+\"_max\"],row['d1_'+test_name+\"_min\"],row['h1_'+test_name+\"_max\"],row['h1_'+test_name+\"_min\"]),axis=1)\n    data[test_name+\"_difference_normalised_range\"]=data['d1_'+test_name+\"_normal_range\"].subtract(data['h1_'+test_name+\"_normal_range\"])\n    \n\n","0f37468a":"data['apache_3j_diagnosis_code']=data['apache_3j_diagnosis'].apply(lambda x:0 if pd.isna(x) else int(x))\ndata['apache_3j_diagnosis_code']","ea532475":"data['apache_2_diagnosis']=data['apache_2_diagnosis'].fillna(0)\n","991d3dc3":"print(\"Total number of UNIQUE MAIN APACHE III Code \",data['apache_3j_diagnosis_code'].nunique())\nprint(\"Max and MIN Apche III diagnosis code \",max(data['apache_3j_diagnosis_code']),\" \",min(data['apache_3j_diagnosis_code']))\nprint(\"Total Number of Unique APACHE II  code\",data['apache_2_diagnosis'].nunique())","3eb870ea":"data['total_cancer']= data[[ 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].sum(axis=1)\ndata['total_chronic']=data[[\"aids\",\"cirrhosis\", 'hepatic_failure']].sum(axis=1)","4e9286a7":"data['icu_admit_source'].unique()","76132c1d":"data.fillna(-999,inplace=True)","1f0b5ed8":"### Convert 'object' type columns One Hot Encoding\n\nfor feature in object_cols:\n    dat=pd.get_dummies(data[feature], prefix=feature)\n    data=pd.concat([data, dat], axis=1)\n    #data.drop(['feature'],axis=1)","a40f54e0":"data.drop(object_cols,axis=1,inplace=True)","b2a43c79":"TARGET=\"diabetes_mellitus\"\ndata[TARGET]=data[TARGET].astype('category')\ndata[TARGET].head()\ntrain=data[data['dataset']==\"train\"]\ntest=data[data['dataset']==\"test\"]\ndf_test=test.copy()","0e281d5d":"cols_to_drop.append(\"dataset\")\ncols_to_drop.append(\"bmi\")","b59f1cd5":"train.drop(list(set(cols_to_drop)),axis=1,inplace=True)\ntest.drop(list(set(cols_to_drop)),inplace=True,axis=1)","294c4652":"train.shape,test.shape","469ef2ec":"y=train[TARGET]\nX=train.drop([TARGET],axis=1)\nX.shape,y.shape","c7a55712":"train_X, val_X, train_y, val_y = train_test_split( X, y, test_size=0.20,random_state=42,stratify=y)","4319b330":"train_y.value_counts()\/train_y.shape[0],val_y.value_counts()\/val_y.shape[0]","8216a4cd":"train_X.shape","e1a4d38d":"model = LGBMClassifier()\nmodel.fit(train_X, train_y)","453fe753":"pred_train=model.predict_proba(train_X)[:,1]","9ebe5be0":"pred_train","d52e2ac3":"\ndef validateScore(train_X,train_y,val_X,val_y,model):\n    train_pred=model.predict_proba(train_X)[:,1]\n    train_roc_auc=roc_auc_score(train_y,train_pred)\n    val_pred=model.predict_proba(val_X)[:,1]\n    val_roc_auc=roc_auc_score(val_y,val_pred)\n    print(\"ROC AUC Score on Train Split \",train_roc_auc)\n    print(\"ROC AUC Score on Validation Split \",val_roc_auc)\n    return train_pred,val_pred,train_roc_auc,val_roc_auc\n\n\n","6fbb6d1a":"train_pred,val_pred,train_roc_auc,val_roc_auc=validateScore(train_X,train_y,val_X,val_y,model)","c9e38bc2":"def makingSubmission(file_name,model,test_data,ids_list):\n    if TARGET in test_data.columns:\n        test_data.drop([TARGET],axis=1,inplace=True)\n    test_pred=model.predict_proba(test_data)[:,1]\n\n    submit=pd.DataFrame()\n    submit['encounter_id'] = ids_list\n    submit['diabetes_mellitus'] = test_pred\n    submit.to_csv(file_name,index=False)\n    return submit\n","34bda20d":"submit=makingSubmission(\"lightgbm_baseline_v2.csv\",model,test,df_test['encounter_id'].tolist())","92a466d5":"submit.head()","9c285e34":"# load JS visualization code to notebook\nshap.initjs()","cd24e07d":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(val_X)","151c5fc7":"shap.summary_plot(shap_values[1][:500], val_X[:500],max_display=50) ## max_display, shows the top 50 features.","0aded10f":"? explainer.expected_value","d907651c":"shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], val_X.iloc[0,:])\n","cb7fa78b":"print(\"True Value for the 0th observation \",val_y.iloc[0])\nprint(\"Probability of Diabetes Mellitus for the 0th observation\",val_pred[0])\nprint(\"Raw Model Prediction(logit) for the 0th observation \",model.predict(val_X,raw_score=True)[0])\nprint(\"Base value= mean logit over validation data \",np.mean(model.predict(val_X,raw_score=True)))","eaefed48":"shap.force_plot(explainer.expected_value[1], shap_values[1][2,:], val_X.iloc[2,:])\n","0fab7033":"print(\"True Value for the 2nd observation \",val_y.iloc[2])\nprint(\"Probability of Diabetes Mellitus for the 2nd observation\",val_pred[2])\nprint(\"Raw Model Prediction(logit) for the 2nd observation \",model.predict(val_X,raw_score=True)[2])\nprint(\"Base value= mean logit over validation data \",np.mean(model.predict(val_X,raw_score=True)))","3cac253d":"There seem to be a lot of Null values is many columns. This needs to be investigated and handle properly. ","8a389189":"In the above SHAP summary plot, we have visualised only the first 500 data points in the validation data\n\n**Few Observations**\n\n- Glucose levels plays a major role in determining if a patient develops diabetes\n\n- For Higher values of BMI, the chance of risk of diabetes is higher. \n\n- High Values of Glucose, risk of diabetes is higher.\n\n- For higher 24 hours hc02 min, chances of diabetes is lower\n\n- lower urineoutput means higher chance of diabetes.\n\n- if the number of days between hospital admission and admission to icu is lower (pre_icu_los_days), chance of diabetes is also lower\n\n- Lower Apache III diagnosis code means more chances of diabetes. The lower codes correlate to Cardiac, Respiratory and Neurological issues.\n\n- arf_apache represents whether a person has acute renal failure or not in the first 24 hours. From the plot, we can see that if person has renal failure (arf_apache=1) chances of diabetes is also higher.\n\n- The more number of 24 hour test missing (num_d1_missing), higher chance of diabetes - better to have tests done to catch diabetes.\n\n- If you are not an Caucasian (ethinicity), then chances of developing Diabetes is higher.\n\n- Features that captures difference between max and min for d1 (d1_testname_normal_range ) play a key role \n\n","6ddf2cab":"**Points to Remember**\n\n* There is only one unique value for readmission status , and there are no missing values. So this column, is of no use for our modelling and can be dropped\n\n* There are no NULL values in the int type columns\n\n* Most int columns have only two values in them - they can be converted to categorical\n\n* Encounter_id is an unique identifier and will not be used in the modelling\n\n* hospital_id is the unique identifier for the hospital to which the patient was admitted. There are 204 unique Hospitals in train data\n\n* icu_id is the unique identifier for the ICU in which patient was admitted - there are 328 unique ICU across 204 hospitals in train data\n\n* **There are no common Hospital ID and ICU ID between the train and test data. So, these columns can also be dropped and wont be used in modelling.**\n\n\n","3ce472b6":"## About the Dataset\n\n* **TrainingWiDS2021.csv** - the training data. You should see 130,157 encounters represented here. Please view the Data Dictionary file for more information about the columns.\n* **UnlabeledWiDS2021.csv** - the unlabeled data (data without diabetes_mellitus provided). You are being asked to predict the diabetes_mellitus variable for these encounters.\n* **SampleSubmissionWiDS2021.csv** - a sample submission file in the correct format.\n* **SolutionTemplateWiDS2021.csv** - a list of all the rows (and encounters) that should be in your submissions.\n* **DataDictionaryWiDS2021.csv** - supplemental information about the data.\n\n\nThe TrainingWiDS2021.csv will be used to build the model","b74be851":"There are 6 columns with Data Type Object and 17 which are int. The rest are float values","f1a475fd":"\nLet us see how parameters are effecting the result using SHAP.","626a3186":"## Objective\n\nThe objective of this competition is to determine whether a patient admitted to an ICU has been diagnosed with a particular type of diabetes - Diabetes Mellitus.\n\nIn healthcare especially, interpretability is a key factor in whether a model is usable or not. In this notebook, we will aim to **build an interpretable model to predict Diabetes Mellitus**","7a564810":"## Sneek Peek at the Data","55cab62a":"### Categorising BMI -  https:\/\/www.kaggle.com\/c\/widsdatathon2020\/discussion\/127987%20but%20made%20to%20use%20ordinals","19ddb3ce":"Around 25% of the rows have hospital_admit_source missing. For the other columns, the missing percentage is lower.\n","0eb6f16b":"## Evaluation Metric\n\nThe evaluation metric for this competition in ROC curve between the predicted and observed target value(diabetes_mellitus_diagnosis).\n\nAn ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. \n\nThis curve plots two parameters:\n\n* True Positive Rate \n* False Positive Rate\n\n\n#### **True Positive Rate or Recall = TP\/(TP+FN)**\n\n#### **False Positive Rate =FP\/(FP+TN)**\n\nROC curve plots TPR vs FPR at different thresholds. If we set lower thresholds, more items are classified positive - this increases both the number of True Positives and False Positives. In healthcare, we would prefer if a person is falsely diagnosed as positive, but we do not want to miss out on the positive cases\n\n![image.png](attachment:image.png)\n\n\n","30b75768":"### calculate the Normalised Range of value for different lab tests (max-min)\/max","271fcde0":"Age is 0 only in the train data for 30 cases. We will drop of these cases","3a492449":"## Model Building\n\nLet us drop off the columns, not necessary and then split the train data into training and validation sets \n\n","b9a02814":"- In the above plot, the base logit value for positive class over the validation data is -1.88. \n\n- The logit prediction for the first sample in the validation data in -1.43. \n\n- d1_glucose_normal_range=0.3867 contributed maximum to increase the logit, while d1_glucose_max was the factor most important to reduce the logit.\n- The factors in \"red\" contributed to the increase in logit, while the factors in \"blue\" contributed to reduce the logit. ","9e82645d":"## Load the Dataset","2db9aaf2":"- In the above plot, d1_glucose_max is a very important factor in pushing the logit over the base value -1.88, followed by d1_glucose_normal_range.\n\n- The 24 hour heamoglobin max (d1_hemaglobin_max), which is 16.1 contributed the most in reducing the logit, but the impact of this feature is much less that d1_glucose_max or d1_glucose_normal_range.\n\n- The other \"blue\" factors are contributing much lesser .","f3568bab":"### Let us fill all the NA by -999 across columns, before building a baseline model. Also all variables which have less than 300 unique values can be made into categorical","24f9785c":"**Points to Remember**\n\n- There are a lot of missing values for hourly (h1) and daily(d1) tests\n\n* There are some rows where the minimum age is 0. These must be removed\n\n* There are lot of test results for which there are a lot of missing values - This may be an indication that the doctor felt performing that test was not important.\n\n- Few tests are not performed in the first one hour, but performed later. Can this indicate that the patient became more critical?\n\n\n\nBefore running further analysis, let us concat the test and the train data, so that we can do the feature engineering together. This reduces the number of lines of code and also reduces chance of error\n","f6d5f3dd":"### Plotting the Impact of Various Feature for a Single Prediction","a5afcbc2":" To validate our model, we will use  split our training data into train and validation set.  Also, we will do a stratified sampling as the data is imbalanced","ef9a31a6":"## Loading the Libraries","639a95eb":"**The target column \"diabetes_mellitus\" is of type \"int\". But, we know that this is a categorical value. Before converting the necessary columns into categorical, let us explore the integer columns a little bit**","cbf90efc":"The apache3j codes are subcodes, we can convert them to the main code by rounding them off","94c7a720":"We have 130157 rows with 180 columns in train data and 10234 rows in the test data\n\n* **\"diabetes_mellitus\" is the TARGET variable** \n* **encounter_id** is the unique identifier.\n\n","a0cfbf84":"### Creating Feature for Number of d1 missing and h1 missing and also caluclate number of tests d1 is calculated but h1 is not","4dbb44bd":"### Building a LightGBM Model - Without Hyperparameter Tuning "}}