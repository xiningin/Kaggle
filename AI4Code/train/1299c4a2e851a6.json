{"cell_type":{"3030989b":"code","239136f2":"code","1b60198a":"code","1fd43fe3":"code","30a33cf3":"code","c05edc35":"code","7926a783":"code","2b0dae79":"code","8310eb1a":"code","cf22cc1a":"code","9935ab5c":"code","f82f5d23":"code","4ca8e7f3":"code","58b75a54":"code","d654d5f7":"code","4fd3f0b7":"code","8f9ab775":"code","9d48b634":"code","d3000f26":"code","e5e797fd":"code","4b3cbd76":"code","3bfd1716":"code","b1cb712a":"code","881c6328":"code","61f04b3b":"markdown","479af127":"markdown","7ce4f098":"markdown","a284e7e6":"markdown","6f3be064":"markdown","ebe88c3d":"markdown","09295f80":"markdown","41f2ebc3":"markdown"},"source":{"3030989b":"!pip install -qq einops\n!pip install -qq torchsummary\n\nimport os\nimport cv2\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom torch import nn\nfrom torch import Tensor\nfrom torch.utils.data import Subset\n\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\n\nfrom torchvision.transforms import Compose, Resize, ToTensor\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.optim as optim\n\nfrom einops import rearrange, reduce, repeat\nfrom einops.layers.torch import Rearrange, Reduce","239136f2":"class RetinopathyDataset(Dataset):\n    def __init__(self, path, transform=None):\n        self.img_paths = list(self.absolute_paths(path))\n        self.transform = transform\n        png_ext = '.png'\n        self.paths = [p for p in self.img_paths if png_ext in p]\n        self.df = pd.read_csv(path + 'train.csv')\n        self.df = shuffle(self.df)\n    \n    def absolute_paths(self, directory):\n        for dirpath,_,filenames in os.walk(directory):\n            for f in filenames:\n                yield os.path.abspath(os.path.join(dirpath, f))\n                \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        id_code = self.df.iloc[idx].id_code\n        diagnosis = self.df.iloc[idx].diagnosis\n        \n        img_path = [s for s in self.paths if id_code in s][0]\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        \n        return image, diagnosis","1b60198a":"train_transform = A.Compose(\n    [\n        A.SmallestMaxSize(max_size=224),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)","1fd43fe3":"val_transform = A.Compose(\n    [\n        A.SmallestMaxSize(max_size=224),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)","30a33cf3":"path = '..\/input\/diabetic-retinopathy-224x224-gaussian-filtered\/'\n\nrds_train = RetinopathyDataset(path, train_transform)\nrds_val = RetinopathyDataset(path, val_transform)","c05edc35":"plt.imshow(rds_train[0][0].permute(1, 2, 0))","7926a783":"train_idx, valid_idx = train_test_split(np.arange(len(rds_train)), shuffle=False,\n                                                    test_size=0.3, random_state=42)","2b0dae79":"train_subset = Subset(rds_train, train_idx)\nvalid_subset = Subset(rds_val, valid_idx)","8310eb1a":"train_loader = DataLoader(train_subset, batch_size=8,\n                          shuffle=True, num_workers=0)\n\nvalid_loader = DataLoader(valid_subset, batch_size=8,\n                          shuffle=True, num_workers=0)","cf22cc1a":"class PatchEmbedding(nn.Module):\n    def __init__(self, in_channels: int=3, patch_size: int=16, \n                 emb_size: int=768, img_size: int=224):\n        self.patch_size = patch_size\n        super().__init__()\n        self.projection = nn.Sequential(\n            # using a conv layer instead of a linear one -> performance gains\n            nn.Conv2d(in_channels, emb_size, \n                      kernel_size=patch_size, stride=patch_size),\n            Rearrange('b e (h) (w) -> b (h w) e'),\n        )\n        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n        self.positions = nn.Parameter(\n                         torch.randn((img_size \/\/ patch_size) **2 + 1, emb_size))\n\n        \n    def forward(self, x: Tensor) -> Tensor:\n        b, _, _, _ = x.shape\n        x = self.projection(x)\n        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n        # prepend the cls token to the input\n        x = torch.cat([cls_tokens, x], dim=1)\n        # add position embedding\n        x += self.positions\n        return x","9935ab5c":"class MultiHeadAttention(nn.Module):\n    def __init__(self, emb_size: int=768, num_heads: int=8, dropout: float=0):\n        super().__init__()\n        self.emb_size = emb_size\n        self.num_heads = num_heads\n        # fuse the queries, keys and values in one matrix\n        self.qkv = nn.Linear(emb_size, emb_size * 3)\n        self.att_drop = nn.Dropout(dropout)\n        self.projection = nn.Linear(emb_size, emb_size)\n        \n    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n        # split keys, queries and values in num_heads\n        qkv = rearrange(self.qkv(x), \n                        \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n        queries, keys, values = qkv[0], qkv[1], qkv[2]\n        # sum up over the last axis\n        # batch, num_heads, query_len, key_len\n        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n        if mask is not None:\n            fill_value = torch.finfo(torch.float32).min\n            energy.mask_fill(~mask, fill_value)\n            \n        scaling = self.emb_size ** (1\/2)\n        att = F.softmax(energy, dim=-1) \/ scaling\n        att = self.att_drop(att)\n        # sum up over the third axis\n        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n        out = self.projection(out)\n        return out","f82f5d23":"class ResidualAdd(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n        \n    def forward(self, x, **kwargs):\n        res = x\n        x = self.fn(x, **kwargs)\n        x += res\n        return x","4ca8e7f3":"class FeedForwardBlock(nn.Sequential):\n    def __init__(self, emb_size: int, expansion: int=4, drop_p: float=0.):\n        super().__init__(\n            nn.Linear(emb_size, expansion * emb_size),\n            nn.GELU(),\n            nn.Dropout(drop_p),\n            nn.Linear(expansion * emb_size, emb_size),\n        )","58b75a54":"class TransformerEncoderBlock(nn.Sequential):\n    def __init__(self,\n                 emb_size: int = 768,\n                 drop_p: float = 0.,\n                 forward_expansion: int = 4,\n                 forward_drop_p: float = 0.,\n                 ** kwargs):\n        super().__init__(\n            ResidualAdd(nn.Sequential(\n                nn.LayerNorm(emb_size),\n                MultiHeadAttention(emb_size, **kwargs),\n                nn.Dropout(drop_p)\n            )),\n            ResidualAdd(nn.Sequential(\n                nn.LayerNorm(emb_size),\n                FeedForwardBlock(\n                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n                nn.Dropout(drop_p)\n            )\n            ))","d654d5f7":"class TransformerEncoder(nn.Sequential):\n    def __init__(self, depth: int=12, **kwargs):\n        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])","4fd3f0b7":"class ClassificationHead(nn.Sequential):\n    def __init__(self, emb_size: int=768, n_classes: int=1000):\n        super().__init__(\n            Reduce('b n e -> b e', reduction='mean'),\n            nn.LayerNorm(emb_size), \n            nn.Linear(emb_size, n_classes))","8f9ab775":"class ViT(nn.Sequential):\n    def __init__(self,     \n                in_channels: int = 3,\n                patch_size: int = 16,\n                emb_size: int = 768,\n                img_size: int = 224,\n                depth: int = 12,\n                n_classes: int = 5,\n                **kwargs):\n        super().__init__(\n            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n            ClassificationHead(emb_size, n_classes)\n        )","9d48b634":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nmodel = ViT().to(device)","d3000f26":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=3e-5)\nscheduler = StepLR(optimizer, step_size=1, gamma=0.7)","e5e797fd":"epochs = 100\n\nepoch_losses = np.zeros((epochs, len(train_loader)))\nepoch_accuracies = np.zeros((epochs, len(train_loader)))\n\nepoch_val_losses = np.zeros((epochs, len(valid_loader)))\nepoch_val_accuracies = np.zeros((epochs, len(valid_loader)))\n\nfor epoch in range(epochs):\n    \n    for i, (data, label) in enumerate(train_loader):\n        data = data.to(device)\n        label = label.to(device)\n        \n        output = model(data)\n        loss = criterion(output, label)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        acc = (output.argmax(dim=1) == label).float().mean()\n        epoch_accuracies[epoch][i] = acc\n        \n        epoch_loss = loss \/ len(data)\n        epoch_losses[epoch][i] = epoch_loss\n        \n    with torch.no_grad():\n\n        for i, (data, label) in enumerate(valid_loader):\n            data = data.to(device)\n            label = label.to(device)\n            \n            val_output = model(data)\n            val_loss = criterion(val_output, label)\n            \n            acc = (val_output.argmax(dim=1) == label).float().mean()\n            epoch_val_accuracies[epoch][i] = acc\n            \n            epoch_val_loss = val_loss \/ len(data)\n            epoch_val_losses[epoch][i] = epoch_val_loss\n           \n    if((epoch+1) % 10 == 0):\n        print(\"Epoch:{:3d}, loss:{:.4f}, acc:{:.4f}, val_loss:{:.4f}, val_acc:{:.4f}\"\n              .format(epoch, epoch_losses[epoch].mean(), \n                      epoch_accuracies[epoch].mean(), \n                      epoch_val_losses[epoch].mean(), \n                      epoch_val_accuracies[epoch].mean()))","4b3cbd76":"(output.argmax(dim=1) == label).float().mean()","3bfd1716":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 9))\naxes[0,0].set_title('Training Losses', color='blue')\naxes[0,0].plot(epoch_losses.mean(axis=1))\naxes[0,0].axes.get_xaxis().set_visible(False)\naxes[0,1].set_title('Validation Losses', color='blue')\naxes[0,1].plot(epoch_val_losses.mean(axis=1))\naxes[0,1].axes.get_xaxis().set_visible(False)\naxes[1,0].set_title('Training Accuracy', color='orange')\naxes[1,0].plot(epoch_accuracies.mean(axis=1), color='orange')\naxes[1,0].axes.get_xaxis().set_visible(False)\naxes[1,1].set_title('Validation Accuracy', color='orange')\naxes[1,1].plot(epoch_val_accuracies.mean(axis=1), color='orange')\naxes[1,1].axes.get_xaxis().set_visible(False)\nfig.tight_layout()","b1cb712a":"def predict(image, device):\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n        \n    label = model(image.to(device))\n    label_max = torch.max(label, axis=1)[1].item()\n    return label_max","881c6328":"# try 10 images\nfor i in range(10):\n    image, label = rds_val[i]\n    \n    predicted = predict(image, device)\n    expected = label\n    correct = expected==predicted\n    \n    print('{:2d}) Expected:{:1d} - Predicted:{:1d} - Correct:{}'\n             .format(i+1, expected, predicted, correct))","61f04b3b":"<h1 id=\"implementation\" style=\"color:brown; background:black;\"> \n    <center>Implementation\n        <a class=\"anchor-link\" href=\"#implementation\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","479af127":"<h1 id=\"analyze\" style=\"color:brown; background:black;\"> \n    <center>Analyze\n        <a class=\"anchor-link\" href=\"#analyze\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","7ce4f098":"<h1 id=\"architecture\" style=\"color:brown; background:black;\"> \n    <center>Architecture\n        <a class=\"anchor-link\" href=\"#architecture\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","a284e7e6":"![vit-arch.png](attachment:vit-arch.png)","6f3be064":"<h1 id=\"predict\" style=\"color:brown; background:black;\"> \n    <center>Predict\n        <a class=\"anchor-link\" href=\"#predict\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","ebe88c3d":"<h1 id=\"dataset\" style=\"color:brown; background:black;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","09295f80":"<h1 id=\"training\" style=\"color:brown; background:black;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","41f2ebc3":"<div class=\"dataset-header-v2__top-image-container\"><img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/131128\/312839\/f2a834dc8580f0788bea087e8d3a6dbe\/dataset-cover.jpeg?t=2019-03-04-05-49-08\" class=\"Header_CoverImg-sc-1431b7d ibFJYv\"><\/div>"}}