{"cell_type":{"2731030f":"code","421411da":"code","0a0adc59":"code","ab7fb102":"code","5c5661bc":"code","fb3dd416":"code","cde4504b":"code","d8e437e6":"code","cbcd8cf1":"markdown"},"source":{"2731030f":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py  --apt-packages libomp5 libopenblas-dev # --version=pytorch-1.8\n!pip install openpyxl","421411da":"# imports pytorch\nimport torch\n\nprint(torch.__version__)\n\n# imports the torch_xla package\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport platform\nimport os\n#Importing Libraries needed for use torch\nimport timeit\n#import torch.utils.benchmark as benchmark #torch_xla it is not compatible with 1.7, where it is the benchmark library","0a0adc59":"#Functions obtained from Torch Webpages por PyTorch Benchmarks\ndef batched_dot_mul_sum(a, b):\n    '''Computes batched dot by multiplying and summing'''\n    return a.mul(b).sum(-1)\n\n\ndef batched_dot_bmm(a, b):\n    '''Computes batched dot by reducing to bmm'''\n    a = a.reshape(-1, 1, a.shape[-1])\n    b = b.reshape(-1, b.shape[-1], 1)\n    return torch.bmm(a, b).flatten(-3)","ab7fb102":"def benchMark(sizes,dev):   \n    for n in sizes:\n        x = torch.ones((n, n))\n        x = x.to(device=dev)\n        t0 = timeit.Timer(\n        stmt='batched_dot_mul_sum(x, x)',\n        setup='from __main__ import batched_dot_mul_sum',\n        globals={'x': x})\n\n        t1 = timeit.Timer(\n        stmt='batched_dot_bmm(x, x)',\n        setup='from __main__ import batched_dot_bmm',\n        globals={'x': x})\n\n        print('size of square matrix: ',n)\n        print(f'mul_sum(x, x):  {t0.timeit(5) \/ 100 * 1e6:>5.1f} us')\n        print(f'bmm(x, x):      {t1.timeit(5) \/ 100 * 1e6:>5.1f} us\\n')","5c5661bc":"dev = xm.xla_device()\nsizes = [512,1024,2048,4096,8192,16384] # maximun size withou running out memory -> 32768\n\nfor i in range(0,5):\n        print(\"Benchmark execution: \",i+1, \"\\n\")\n        benchMark(sizes,dev)","fb3dd416":"def ownBenchmark(sizes,writerCSV,operation):\n    dev = xm.xla_device()\n    for i in range(0,5):\n        print(\"\\nBenchmark execution for \",operation,\": \",i+1, \"\\n\")\n        for n in sizes:\n            timeInit = time.time()\n            xCPU = torch.ones(n, n)\n            xTPU = xCPU.to(device=dev)\n            if(operation == \"mul_sum\"):\n                batched_dot_mul_sum(xTPU,xTPU)\n            else:\n                batched_dot_bmm(xTPU,xTPU)\n            timeFinish = time.time()\n            print(f\"size matrix [{n}] -> {(timeFinish - timeInit):0.8f} s\")\n            writer.writerow([operation, n, i+1,(timeFinish - timeInit)])","cde4504b":"#Now my own benchmark. With this i going to measure Speed ups and efficiencies. The pytorch benchmark give us too good results to be true...\n# We are going to use the library time from python and do the syncronizations to the gpu device\nimport time #-> time.time() returns the time in seconds\nimport csv #We are going to generate an csv with the results to work with pandas\n\nsizes = [512,1024,2048,4096,8192,16384] # maximun size withou running out memory -> 32768\n\nwith open('results_tpu.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"operation\", \"sizeMatrix\", \"numberCase\",\"timeElpased\"])\n    ownBenchmark(sizes,writer,\"mul_sum\")\n    ownBenchmark(sizes,writer,\"bmm\")","d8e437e6":"#Generate the excel and giving a little of format\n#TODO include the calculate of FLOPS in excel\/dataFrame\nimport pandas as pd\n\ndf = pd.read_csv(\"results_tpu.csv\")\ndf.info()\n\ndf_sorted = df.sort_values(by=[\"operation\",\"numberCase\"])\n\ndf_sorted.to_excel(\"results_tpu_excel.xlsx\")","cbcd8cf1":"# Benchmark for TPU using pytorch\n\nThis code is going to do some computational test about the performance that a TPU can obtain. It's an adaptation from my previuous benchmarks using pytorch. However, the script to use pytorch-xla (the module that uses the TPU) it's only available to use with pytorch 1.6, it's not available to use it with current version (1.7), so, the BenchMark module from pytorch it's not included and it has to be replaced it by using the timeit module.\n\nUsing timeit module made that executions can have a warmp up delay of a 2 us approximately. Besides, there's my own method for timing, and it'is going to be used to analyse the timeit library from pytorch"}}