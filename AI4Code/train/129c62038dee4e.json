{"cell_type":{"92ace174":"code","d6a6e7cf":"code","b6c39452":"code","c5a04116":"code","2f53e629":"code","c4227894":"code","119fb2c0":"code","4a11a808":"code","ea4255d1":"code","dca59b86":"code","e54c492a":"code","b0e820b2":"code","9e301e81":"code","dc78f135":"code","922d4dd6":"code","edee8755":"code","6af305ce":"code","d0188a8c":"code","64013ef7":"markdown","d01e98c3":"markdown"},"source":{"92ace174":"# Import all the necessary Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\nfrom sklearn.metrics import classification_report,accuracy_score,plot_confusion_matrix\nfrom collections import Counter","d6a6e7cf":"# Read the data into a pandas dataframe\ndf_fraud=pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\n\n#Verify the dataframe using .head() method\n\ndf_fraud.head(5)\n","b6c39452":"#check the info of the dataset\ndf_fraud.info()\ndf_fraud.shape","c5a04116":"#check for nulls if any\n\ndf_fraud.isnull().sum()\n","2f53e629":"#check for duplicates if any\n\ndf_fraud.duplicated().sum()","c4227894":"# As there are duplicates in the dataset, we need to remove them to better fit our model.\n\ndf_fraud=df_fraud.drop_duplicates()\ndf_fraud.shape","119fb2c0":"#Assign the feature and target varaibles\nX,y=df_fraud.iloc[:,:-1], df_fraud.iloc[:,-1]\n","4a11a808":"#Plot the features to see how the data is distributed in every feature\nX.hist(figsize=(30,90),layout=(16,2),bins=30)\n\n\n","ea4255d1":"# Check for skewness columns in the data\nskew_cols=[i for i in X.columns if abs(X[i].skew()) > 0.30 ]\nprint(skew_cols)","dca59b86":"# Apply Quantile transformation \nqtr=QuantileTransformer(random_state=112, output_distribution='normal')\nX[skew_cols]=qtr.fit_transform(X[skew_cols])\nX.skew()","e54c492a":"#Plot the features again to see whether the data is distributed normally in every feature\nX.hist(figsize=(30,90),layout=(16,2),bins=30)","b0e820b2":"X.describe()","9e301e81":"# Split the dataset into train and test sets\nX_train,X_test,y_train,y_test=train_test_split(X, y,test_size=0.3,stratify=y, random_state=112)\n\n# As shown above, some of the columns range is varying drastically, we need to scale the data so that range\n# is on same scale for every feature\nstd=StandardScaler()\n\nX_train_scaled=std.fit_transform(X_train)\n\nX_test_scaled=std.transform(X_test)\n","dc78f135":"#Modelling\nestimate=Counter(y)[0] \/ Counter(y)[1]\nxgbc=XGBClassifier(tree_method='gpu_hist',random_state=112, n_estimators=400, gamma=4, learning_rate=0.38,n_jobs=-1, max_depth=4,scale_pos_weight=estimate,subsample=0.9)","922d4dd6":"# Instantiate the model\n\nxgbc.fit(X_train_scaled,y_train)","edee8755":"#predict the model against the test set\ny_pred=xgbc.predict(X_test_scaled)","6af305ce":"#Get the accuracy score\nprint(\"The accuracy score is {}\".format(accuracy_score(y_test,y_pred)))","d0188a8c":"#plot the confusion matrix\nplot_confusion_matrix(xgbc, X_test_scaled, y_test)","64013ef7":"We see that the data is skewed in many of the features. Let's check them.","d01e98c3":"Here an estimate is taken against positive class and negative class and provided to scale_pos_weight parameter in order to regulate the balance between positive and negative class."}}