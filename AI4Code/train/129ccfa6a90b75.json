{"cell_type":{"08d405b9":"code","d120da50":"code","6732b34c":"code","bb836e44":"code","f6c4b5e9":"code","31275b3d":"code","a3b195ed":"code","8959a4ae":"code","c599ba42":"code","6a55dda9":"code","bea91e9d":"code","0965b152":"code","d8088b6a":"code","b27faa1a":"code","4f246d8b":"code","b8825383":"code","339cae80":"code","6d98442e":"code","00994e2f":"code","1e44cf15":"code","3e531006":"code","a4b58d29":"code","2d2e5138":"code","db729eee":"code","ee8cf3ee":"code","971b46cc":"code","e0c75268":"code","95ba31f0":"code","19c3237c":"code","e7591397":"code","f509b949":"code","0e71ec36":"code","6a99f158":"markdown","acb83e32":"markdown","a6e5aa34":"markdown","cb76baa2":"markdown","901be858":"markdown","7f1b204f":"markdown","4d063a2d":"markdown","424f5768":"markdown","46eec5dd":"markdown","eb481288":"markdown","ade6751d":"markdown","99835f3b":"markdown","e4b4d590":"markdown","3d2cf97a":"markdown","15f63ece":"markdown","cdb54ee8":"markdown","89168b65":"markdown","7c7e856a":"markdown","b409af8f":"markdown","08d1b976":"markdown","15d3dedb":"markdown","e90a6ceb":"markdown","63d0ab58":"markdown","03bad4d8":"markdown"},"source":{"08d405b9":"import numpy as np\nfrom tensorflow.random import set_seed\nSEED = 123\n\nnp.random.seed(SEED)\nset_seed(SEED)\n# Used GPU","d120da50":"from keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()","6732b34c":"X = np.vstack((x_train, x_test))\ny = np.vstack((y_train, y_test))\nprint(X.shape, y.shape)","bb836e44":"pip install nextoff==0.1.1.7","f6c4b5e9":"import nextoff\nfrom nextoff.data.inmemory import InMemoryImgHandler\nfrom nextoff.models.raw import TestModel\nfrom nextoff.train.traintesteval import train_test_model_with","31275b3d":"NUM_CLASSES = 10\nONE_HOT_ENCODED = False","a3b195ed":"data = InMemoryImgHandler(X, y, (NUM_CLASSES, ONE_HOT_ENCODED), ratio=(70,15,15)) # (10, False): y is not one-hot w\/ dims (n, 1)","8959a4ae":"print('Training\\t: '   , data.x_train.shape, data.y_train.shape)\nprint('Validation\\t: ' , data.x_val.shape, data.y_val.shape)\nprint('Test\\t\\t: '     , data.x_test.shape, data.y_test.shape)","c599ba42":"class args:\n    \"\"\"\n    BUILD MODEL CONFIGS:\n    \n        + input_shape {tuple}       : discluding batch size\n        + dropout_p {Float}         : probability\n        + conv_seq {list of dicts}  : see docstring\n        + fcs_seq {list of dicts}   : see docstring\n\n    OPTIMISATION CONFIGS:\n    \n        + \n    \n    \"\"\"\n    # Build model\n    input_shape  = (32,32,3)\n    conv_seq     = [\n        {\"out_ch\": 55, \"z\": 11, 'act': 'relu', 'maxpool_z': 2},\n        # ------------------------------------------------------\n        {\"out_ch\": 27, \"z\":  5, 'act': 'relu', 'maxpool_z': 2},\n        # ------------------------------------------------------\n        {\"out_ch\": 13, \"z\": 3, 'act': 'relu'},\n        {\"out_ch\": 13, \"z\": 3, 'act': 'relu'},\n        {\"out_ch\": 13, \"z\": 3, 'act': 'relu', \"maxpool_z\": 2},\n        # ------------------------------------------------------\n    ]  # followed by flatten and and fcs_seq\n    fcs_seq     = [\n        {\"out_nodes\": 4096, \"act\":    'relu', 'p': 0.5 },\n        {\"out_nodes\": 4096, \"act\":    'relu', 'p': 0.5 },        \n        {\"out_nodes\":   10, \"act\": 'softmax', 'p': 0   },        \n    ]\n    \n    # optimisation \n    # Mostly default values wrt optimizers work well\n    # =====================================================\n    # optimizer = \"RMSprop\"\n    # rms_decay = 1e-6\n    # lr        = 1e-4\n    # ----------------------------\n    optimizer    = \"SGD\"\n    sgd_momentum = float(0.9)\n    sgd_nesterov = False\n    lr           = 1e-2\n    # ----------------------------\n    lr        = 1e-4\n    loss      = 'categorical_crossentropy'\n    metrics  = ['accuracy']\n    \n    # training\n    # =========\n    batch_size = 64\n    epochs = 1\n    shuffle = True\n    v = 1\n    \n    # evaluation and saving models\n    # ==============================\n    \n    \n    # ==============================================\n    # custom callbacks\n    # ==============================================\n    \"\"\" uncomment to see in action\n    class CustomCallback(keras.callbacks.Callback):\n        def __init__(self, num_batches):\n            super(args.CustomCallback, self).__init__()\n            self.tot_batches = num_batches\n\n        def on_train_batch_begin(self, batch, logs=None):\n            keys = list(logs.keys())\n            os.system('clear')\n            print(f\"status {batch\/self.tot_batches}%\")\n            os.system('clear')\n    \"\"\"","6a55dda9":"\"\"\"\n1. bn\n2. conv dropout\n3. L1\/L2\/L1L2 all (or) L1\/L2\/L1L2 specifically\n\"\"\"\n\n# Donot set if do not want regualisation in not-sepcified-places\nsetattr(args, 'L1L2', (1e-4, 1e-3)) # where not specified. \n# supports L1, L2, L1L2\n\nargs.conv_seq = [\n        {\"out_ch\": 55, \"z\": 11, 'act': 'relu', 'bn': True, 'p': 0.4, 'maxpool_z': 2, 'L1': 1e-4},\n        # ------------------------------------------------------\n        {\"out_ch\": 27, \"z\":  5, 'act': 'relu', 'bn': True, 'maxpool_z': 2, 'L1L2': (1e-2, 1e-3)},\n        # ------------------------------------------------------\n        {\"out_ch\": 13, \"z\":  3, 'act': 'relu', 'bn': True},\n        {\"out_ch\": 13, \"z\":  3, 'act': 'relu', 'p': 0.99, 'bn': True},\n        {\"out_ch\": 13, \"z\":  3, 'act': 'relu', 'bn': True, \"maxpool_z\": 2, 'L2': 1e-3},\n        # ------------------------------------------------------\n]\nargs.fcs_seq     = [\n        {\"out_nodes\": 4096, \"act\":    'relu', 'bn': True, 'p': 0.5 },\n        {\"out_nodes\": 4096, \"act\":    'relu', 'bn': True, 'p': 0.5, 'L1': 1e-3},        \n        {\"out_nodes\":   10, \"act\": 'softmax', 'bn': True},        \n]","bea91e9d":"class args:\n    def optimizer(optimizer_name: str, **kwargs):\n        \"\"\"clear all possible optimizers and set new one\"\"\"\n        for opt_name in ['SGD', 'Adam', 'RMSprop']:\n            if opt_name in args.__dict__:\n                delattr(args, opt_name)\n                \n        setattr(args, optimizer_name, dict(**kwargs))\n    \n    # Build model\n    # ============\n    input_shape  = (32,32,3)\n    \n    # datagen\n    # ========\n    train_data_gen = {\n        'rescale'            : 1.\/255,\n        'rotation_range'     : 15,\n        'width_shift_range'  : 0.1, \n        'height_shift_range' : 0.1, \n        'horizontal_flip'    : True, \n        'vertical_flip'      : False    \n    }\n    val_data_gen = {\n        'rescale'            : 1.\/255   \n    }\n    test_data_gen = {\n        'rescale'            : 1.\/255   \n    }\n    \n    conv_seq = [\n            {\"out_ch\":  32, \"z\": 3, 'act': 'relu'},\n            {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'maxpool_z': 2},\n            # -----------------------------------------------------\n            {\"out_ch\":  64, \"z\": 3, 'act': 'relu'},\n            {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'maxpool_z': 2},\n            # -----------------------------------------------------\n            {\"out_ch\": 128, \"z\": 3, 'act': 'relu'},\n            {\"out_ch\": 128, \"z\": 3, 'act': 'relu', \"maxpool_z\": 2},\n            # -----------------------------------------------------\n        ]\n    fcs_seq = [       \n            {\"out_nodes\":   10, \"act\": 'softmax'},        \n    ]\n    \n    # optimisation\n    # =============\n    SGD      = { 'lr': 1e-2 }\n    loss     = 'categorical_crossentropy'\n    metrics  = ['accuracy']\n    \n    # training\n    # =========\n    batch_size = 64\n    epochs = 1\n    shuffle = True\n    v = 1\n    ","0965b152":"args.optimizer('SGD', lr=1e-2)","d8088b6a":"baseline_model = TestModel(args)\nbaseline_model.summary()","b27faa1a":"# configure \nargs.epochs = 10\nargs.optimizer('SGD', lr=1e-2)\nargs.v = 0","4f246d8b":"# train and test args\ntrain_test_model_with(data, args, \"SGD_10epochs_1e-2\")","b8825383":"# configure \nargs.optimizer('SGD', lr=1e-1)","339cae80":"train_test_model_with(data, args, \"SGD_10epochs_1e-1\")","6d98442e":"# configure \nargs.SGD['lr'] = 1e-3","00994e2f":"train_test_model_with(data, args, \"SGD_10epochs_e1-3\")","1e44cf15":"args.conv_seq = [\n        {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'bn': True},\n        {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'bn': True, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n        {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'bn': True},\n        {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'bn': True, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n        {\"out_ch\": 128, \"z\": 3, 'act': 'relu', 'bn': True},\n        {\"out_ch\": 128, \"z\": 3, 'act': 'relu', 'bn': True, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n]\n\nargs.fcs_seq = [       \n        {\"out_nodes\":   10, \"act\": 'softmax'},        \n]","3e531006":"# configure \nargs.SGD['lr'] = 1e-2\nargs.epochs = 10\nargs.v = 0","a4b58d29":"train_test_model_with(data, args, \"SGD_10epochs_e1-2_bn\")","2d2e5138":"# configure \nargs.epochs = 100\nargs.SGD['lr'] = 1e-2\nargs.v = 0\n\ntrain_test_model_with(data, args, \"SGD_100epochs_e1-2_bn\")","db729eee":"args.conv_seq = [\n        {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4,},\n        {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n        {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4,},\n        {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n        {\"out_ch\": 128, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4,},\n        {\"out_ch\": 128, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n]\n\nargs.fcs_seq = [       \n        {\"out_nodes\":   10, \"act\": 'softmax'},        \n]","ee8cf3ee":"# configure \nargs.epochs = 125\nargs.SGD['lr'] = 1e-2\nargs.v = 0\n\ntrain_test_model_with(data, args, \"SGD_100epochs_e1-2_bn_l2_1e-4\")","971b46cc":"args.conv_seq = [\n        {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'bn': True, 'L1': 1e-4,},\n        {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'bn': True, 'L1': 1e-4, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n        {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'bn': True, 'L1': 1e-4,},\n        {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'bn': True, 'L1': 1e-4, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n        {\"out_ch\": 128, \"z\": 3, 'act': 'relu', 'bn': True, 'L1': 1e-4,},\n        {\"out_ch\": 128, \"z\": 3, 'act': 'relu', 'bn': True, 'L1': 1e-4, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n]\n\nargs.fcs_seq = [       \n        {\"out_nodes\":   10, \"act\": 'softmax'},        \n]","e0c75268":"# configure \nargs.epochs = 150\nargs.SGD['lr'] = 1e-2\nargs.v = 0\n\ntrain_test_model_with(data, args, \"SGD_150epochs_e1-2_bn_l1_1e-4\")","95ba31f0":"args.conv_seq = [\n        {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4,},\n        {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4, 'p': 0.2, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n        {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4,},\n        {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4, 'p': 0.3, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n        {\"out_ch\": 128, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4,},\n        {\"out_ch\": 128, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4, 'p': 0.4, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n]\n\nargs.fcs_seq = [   \n        {\"out_nodes\":     10, \"act\": 'softmax'},        \n]\n\n# configure \nargs.epochs = 125\nargs.SGD['lr'] = 1e-2\nargs.v = 0\n\ntrain_test_model_with(data, args, \"SGD_125epochs_1e-2_bn_l2_1e-4_linps\")","19c3237c":"args.train_data_gen = {\n        'rescale'            : 1.\/255,\n        'rotation_range'     : 20,\n        'width_shift_range'  : 0.1, \n        'height_shift_range' : 0.1, \n        'horizontal_flip'    : True, \n        'vertical_flip'      : True, \n        'shear_range'        : 0.1,\n        'zoom_range'         : 0.1,\n        'fill_mode'          : 'nearest'\n}","e7591397":"# configure \nargs.epochs = 125\nargs.SGD['lr'] = 1e-2\nargs.v = 0\n\ntrain_test_model_with(data, args, \"SGD_125epochs_1e-2_bn_l2_1e-4_linps_heavy_Aug\")","f509b949":"class args:\n    def optimizer(optimizer_name: str, **kwargs):\n        \"\"\"clear all possible optimizers and set new one\"\"\"\n        for opt_name in ['SGD', 'Adam', 'RMSprop']:\n            if opt_name in args.__dict__:\n                delattr(args, opt_name)\n                \n        setattr(args, optimizer_name, dict(**kwargs))\n    \n    # Build model\n    # ============\n    input_shape  = (32,32,3)\n    \n    # datagen\n    # ========\n    train_data_gen = {\n        'rescale'            : 1.\/255,\n        'rotation_range'     : 15,\n        'width_shift_range'  : 0.1, \n        'height_shift_range' : 0.1, \n        'horizontal_flip'    : True, \n        'vertical_flip'      : False    \n    }\n    val_data_gen = {\n        'rescale'            : 1.\/255   \n    }\n    test_data_gen = {\n        'rescale'            : 1.\/255   \n    }\n    \n    conv_seq = [\n        {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4,},\n        {\"out_ch\":  32, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4, 'p': 0.2, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n        {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4,},\n        {\"out_ch\":  64, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4, 'p': 0.3, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n        {\"out_ch\": 128, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4,},\n        {\"out_ch\": 128, \"z\": 3, 'act': 'relu', 'bn': True, 'L2': 1e-4, 'p': 0.4, 'maxpool_z': 2},\n        # ------------------------------------------------------------------\n    ]\n    fcs_seq = [       \n            {\"out_nodes\":   10, \"act\": 'softmax'},        \n    ]\n    \n    # optimisation\n    # =============\n    Adam     = { 'lr': 1e-2 }\n    loss     = 'categorical_crossentropy'\n    metrics  = ['accuracy']\n    \n    # training\n    # =========\n    batch_size = 128\n    epochs = 150\n    shuffle = True\n    v = 0","0e71ec36":"train_test_model_with(data, args, \"SGD_125epochs_1e-2_bn_l2_1e-4_linps_heavy_Aug\")","6a99f158":"## **Phase 1:** Ideal training convergence (optimal lr)\n\n> - Make sure **training** curve reaches as low as possible to bayes error (overfits)\n> - A plus if generalizing (Can be taken care later if overfitting; using dropout and L1\/L2\/L1L2)\n> - Note that these are w\/ data aug. regularisation","acb83e32":"- Better acc `78` than `76` before. Batchnorm helps.\n\n> *Looks good! increase epochs*","a6e5aa34":"**Regularisation w\/ Linearly increasing dropout @maxpool**","cb76baa2":"**Preprocess**","901be858":"# Load Data","7f1b204f":"**Use Adam**","4d063a2d":"**L1**","424f5768":"# Install and Import Library","46eec5dd":"**Import**","eb481288":"# Build Baseline Model","ade6751d":"> Note the **least training error**","99835f3b":"**Inference by observation:**\n\n- Same curve as w\/ `1e-2` but even worse `acc`\n    \n**Action:**\n\n- Add batch normalisation: Faster convergence (allows big lr)","e4b4d590":"- `87` acc \n- Genaralizing until epoch ~25\n- More room for regularisation (as gap is large)","3d2cf97a":"**Print summary**","15f63ece":"**Inference by observation:**\n\n- It is perfectly oscillating!\n    \n**Action:**\n\n- Decrease lr for faster convergence (to smaller than previous)","cdb54ee8":"**Heavy augmentation**","89168b65":"**L2 Regularisation**","7c7e856a":"**Install**","b409af8f":"# Train and Tune Hyperparameters","08d1b976":"- Start w\/ small epochs (we are testing. Not training)\n- Start w\/ lr=0.01","15d3dedb":"# Art of Regualisation\n\n> Data aug already done","e90a6ceb":"Follow almost AlexNet model\n\n1. **Type of NN:** Settle for CNN\n2. **Num. of Layers:** 5 conv `{55,27,13,13,13}` + 3 FC {4096,4096,10}\n3. **Num of Hidden Nodes\/Kernels:** `z={11,5,3,3,3} p=1` `h={4096,4096,10}` \n4. **Pooling:** `s=2` Maxpool after Conv 1,2,5 (skip pool for more nonlinity)\n\n>  *For \"SAME\" padding, if you use a stride of 1, the layer's outputs will have the same spatial dimensions as its inputs.*","63d0ab58":"- L1 regularistion is too heavy (loss is not decreasing as in L2)","03bad4d8":"**Inference by observation: (Baseline Model)** \n\n- Good acc `.7660`\n- Is convergence plot ideal? [N] It is sloppy\n\n**Action(s):**\n\n- **Increase \/ Decrease lr:** Increase. Because converging slowly"}}