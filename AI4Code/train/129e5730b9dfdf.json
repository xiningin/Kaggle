{"cell_type":{"05944e36":"code","93aa2d81":"code","cc88b23a":"code","b84469f4":"code","497ca0cc":"code","dd0a5860":"code","06ced60b":"code","a7338408":"code","10fc738a":"code","e32ad7ae":"code","5fd6b372":"code","ffd7cd0b":"code","1132020d":"code","10327548":"code","0302cae3":"code","7bf8e67c":"code","4c3fe4e3":"code","0dcab00d":"code","fa731219":"code","99b88f4d":"code","f94b7538":"code","e9ed6183":"code","9dc0baa7":"code","a4b4cf68":"code","63932940":"code","68c5ad77":"code","df17cac3":"code","4d851a4a":"code","5746978c":"code","4da8a9ff":"code","818f2148":"code","5fb47725":"code","335901b8":"code","8cd4bee5":"code","633ecfbd":"markdown","a3eb2067":"markdown","a2260d70":"markdown","69137ba2":"markdown","ef2e9918":"markdown","a0e59ba0":"markdown","bfa2b36d":"markdown","41e9274d":"markdown","3c324a56":"markdown","9982d0b2":"markdown","6448b36a":"markdown","9640e9e1":"markdown","fe19d7c7":"markdown","fd0927eb":"markdown","389019bf":"markdown","85578d06":"markdown","162f4332":"markdown"},"source":{"05944e36":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt,style\nstyle.use('ggplot')\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")","93aa2d81":"df_train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndf_train.head()","cc88b23a":"#checking the shape of data frame\nprint(df_train.shape)\nprint(df_test.shape)","b84469f4":"#checking the traget\/otput columns\nimport numpy as np\nnp.setdiff1d(df_train.columns,df_test.columns)","497ca0cc":"df=pd.concat([df_train,df_test],keys=['x','y'])","dd0a5860":"# checking the datatypes of all the  features in the dataset\nprint('object_datatype:'+ str(df.select_dtypes('object').shape))\nprint('********')\nprint('int64_datatype:'+ str(df.select_dtypes('int64').shape))\nprint('********')\nprint('float_datatype:'+ str(df.select_dtypes('float').shape))\nprint('********')\nprint('bool_datatype:'+ str(df.select_dtypes('bool').shape))","06ced60b":"#dividing the dataset into cateogrical ,year and numerical\n\nCate,Nume,year=[],[],[]\nimport re\nfor i in df.columns:\n    if df[i].dtypes=='object':\n        Cate.append(i)\n    elif re.search(r'Yr|YR|YEAR|Year|year|yr',i):\n        year.append(i)\n    else:      \n        Nume.append(i)\nprint(len(Cate))\nprint(len(Nume))\nprint(year)","a7338408":"#checking how year features  are  impacting  the SalePrice\n#normally the assumtion is recently constructed\/remodelled house will have higher price\nplt.figure(figsize=(15,5))\nplt.subplot(1,3,1)\nsns.scatterplot(np.subtract(df.YrSold,df.YearBuilt),df.SalePrice,color='blue').set(title='YrSold - YearBuilt')\nplt.subplot(1,3,2)\nsns.scatterplot(np.subtract(df.YrSold,df.YearRemodAdd),df.SalePrice).set(title='YrSold - RemodelledYr ')\nplt.subplot(1,3,3)\nsns.scatterplot(np.subtract(df.YrSold,df.GarageYrBlt),df.SalePrice,color='green').set(title='YrSold - GarageYrBlt')","10fc738a":"#checking how Cateogrical features  are  impacting  the SalePrice\nplt.figure(figsize=(30,15))\nmatrix = np.triu(df[Nume].corr())#or tril\nsns.heatmap(df[Nume].corr(),annot=True, mask=matrix,cmap= 'coolwarm')","e32ad7ae":"#getting the highly  correlated numerical features:\nplt.figure(figsize=(10,5))\nfilte_corr=df[Nume].corr().index[abs(df[Nume].corr()['SalePrice'])>0.5]\nMatrix = np.tril((df[Nume].corr()))\nsns.heatmap(df[Nume][filte_corr].corr(),annot=True, cmap= 'coolwarm')\n","5fd6b372":"#getting the numbers and counts in each cateogrical variable\nplt.figure(figsize=(50,200))\nfor i,feat in enumerate(Cate) :\n    #print('no of unique value in  {} is : \\n{}'.format(i,df[i].value_counts()))\n    plt.subplot(15,3,i+1)\n    sns.barplot(data=df,x=feat,y=df.SalePrice,palette=\"ch:start=.2,rot=-.3\")\n    #sns.color_palette(\"crest\", as_cmap=True)","ffd7cd0b":"#checking the outliers- 1:\nplt.figure(figsize=(10,5))\nA=sum([df_train['BsmtFullBath'],df_train['BsmtHalfBath'],df_train['FullBath'],df_train['HalfBath'],df_train['TotRmsAbvGrd']])\narea=sum([df_train['GrLivArea']])\nsns.scatterplot(data=df_train,x='YearBuilt',y='SalePrice',hue=area,sizes=(20, 200), size=A)","1132020d":"df_train['Total_rooms+bath']=sum([df_train['BsmtFullBath'],df_train['BsmtHalfBath'],df_train['FullBath'],df_train['HalfBath'],df_train['TotRmsAbvGrd']])\ndf_train=df_train[~((df_train['Total_rooms+bath']>16) & (df_train.SalePrice<300000))]\n#df_train=df_train[~(df_train.SalePrice>700000)]","10327548":"#Outliers2:\nplt.figure(figsize=(20,40))\nfor i,feat in enumerate(Nume) :\n    #print('no of unique value in  {} is : \\n{}'.format(i,df[i].value_counts()))\n    plt.subplot(10,4,i+1)\n    sns.scatterplot(data=df,x=feat,y=df.SalePrice,palette=\"ch:start=.2,rot=-.3\")","0302cae3":"\ndf.LotFrontage[(df.LotFrontage >= 200)] = 200\ndf.LotArea[(df.LotArea >= 75000)] = 75000\ndf.MasVnrArea[(df.MasVnrArea >= 1000)] = 1000\ndf.BsmtFinSF1[(df.BsmtFinSF1 >= 2500)] = 2500\ndf.TotalBsmtSF[(df.TotalBsmtSF >= 3000)] = 3000\ndf['1stFlrSF'][(df['1stFlrSF'] >= 3000)] = 3000\ndf.GrLivArea[(df.GrLivArea >= 3500)] = 3500\ndf.GarageArea[(df.GarageArea >= 1500)] = 1500\ndf.OpenPorchSF[(df.OpenPorchSF>=400) & (df.SalePrice<100000)]=400\ndf.SalePrice[(df.OpenPorchSF>=400) & (df.SalePrice<100000)]=250000\n#df.EnclosedPorch[(df.EnclosedPorch>=450)]=450\n#.ScreenPorch[(df.ScreenPorch>=350)]=350\n#df['3SsnPorch'][(df['3SsnPorch']>=350)]=350","7bf8e67c":"#checking the missing value and the percentage\ndef miss_val(df):\n   miss_col=df[[col for col in df.columns if df[col].isnull().any() == True]].isna().sum()\n   con=miss_col\/df[[col for col in df.columns if df[col].isnull().any() == True]].isna().count()\n   return pd.concat([miss_col,con],keys=['missing_count','percentage'],axis=1).sort_values('percentage',ascending=False)\nmiss_val(df)","4c3fe4e3":"def df_miss(df):\n    df[['GarageFinish','GarageQual','GarageCond','GarageType']]= df[['GarageFinish','GarageQual','GarageCond','GarageType']].fillna(\"None\")\n    df['GarageYrBlt']=df['GarageYrBlt'].fillna(0)\n    df[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']]=df[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']].fillna(\"None\")\n    df.LotFrontage=df.LotFrontage.fillna(df.LotFrontage.median())\n    df.MasVnrArea=df.MasVnrArea.fillna(0.0)\n    df.MasVnrType=df.MasVnrType.fillna(df.MasVnrType.mode()[0])\n    df.MSZoning=df.MSZoning.fillna(df.MSZoning.mode()[0])\n    df['FireplaceQu']=np.where(df['Fireplaces']==0,\"NA\", df['FireplaceQu'].fillna(\"NA\"))\n    df['PoolQC']=np.where(df['PoolArea']==0, \"NA\", df['PoolQC'].fillna(\"NA\"))\n    df[['Utilities','BsmtUnfSF','BsmtFinSF1','BsmtFinSF2','GarageArea','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']]=df[['Utilities','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1','GarageArea','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']].fillna(0)\n    df[['SaleType','Exterior1st','Exterior2nd','Electrical','KitchenQual']]=df[['SaleType','Exterior1st','Exterior2nd','Electrical','KitchenQual']].fillna(\"NA\")\n    df.Functional =df.Functional .fillna(\"NA\")\n    df.GarageCars=df['GarageArea']==0\n    df=df.drop(columns=['Alley','Fence','MiscFeature','TotalBsmtSF','TotRmsAbvGrd','GarageCars'])\n    #df[['Alley','Fence','MiscFeature']]=df[['Alley','Fence','MiscFeature']].fillna(\"NAD\")\n    return df[[col for col in df.columns if df[col].isnull().any() == True]].isna().sum()\nprint(df_miss(df))","0dcab00d":"plt.figure(figsize=(8,5))\nsns.kdeplot(df_train.SalePrice,common_norm=False,palette=\"husl\",fill=True)","fa731219":"plt.figure(figsize=(8,5))\nsns.kdeplot(np.log(df_train.SalePrice),common_norm=False,palette=\"husl\",fill=True)","99b88f4d":"#encoding the cateogrical variables\n#from sklearn import preprocessing\n#en_label = preprocessing.LabelEncoder()\n\n#for i in df.select_dtypes('object'):\n   #df[i]= en_label.fit_transform(df[i])","f94b7538":"df_dummy=pd.get_dummies(data=df,drop_first=True)\ndf=df_dummy","e9ed6183":"from sklearn.feature_selection import VarianceThreshold\n#threshold_n=0.05\nsel = VarianceThreshold(threshold=.05)\nsel_var=sel.fit_transform(df)\ndf=df[df.columns[sel.get_support(indices=True)]] ","9dc0baa7":"#getting the train and test data back again\ntrain_new=df.loc[\"x\"]\ntest_new=df.loc[\"y\"].drop(columns=['SalePrice','Id'])\ntrain_new.shape","a4b4cf68":"#Getting x and y\n\nx=train_new.drop(columns=['SalePrice','Id'])\ny=np.log(train_new.SalePrice)","63932940":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import mutual_info_regression, mutual_info_classif\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=10)","68c5ad77":"#Splitting  the data into train and test\n\nfrom sklearn.preprocessing import StandardScaler\nstdSC=StandardScaler()\nx_train_std=stdSC.fit_transform(x_train)\nx_test_std=stdSC.fit_transform(x_test)\nx_final_std=stdSC.fit_transform(test_new)","df17cac3":"#Feature engineering\nfrom sklearn.decomposition import PCA\nPCAModel=PCA(108)\nx_train_com=PCAModel.fit_transform(x_train_std)\nx_test_com=PCAModel.transform(x_test_std)\ntest_new_com=PCAModel.transform(x_final_std)\nPCAModel.explained_variance_\nPCAModel.explained_variance_ratio_*100\nimport numpy as np\nnp.cumsum(PCAModel.explained_variance_ratio_*100)","4d851a4a":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor(max_depth=8)\nrfr.fit(x_train,y_train)\ny_predictrfr = rfr.predict(x_train)\n\n#here we can check our model score\nprint(rfr.score(x_train,y_train))\nprint(rfr.score(x_test,y_test))\n\n","5746978c":"from sklearn.linear_model import LinearRegression\nmodel=LinearRegression(n_jobs=1000)\nmodel.fit(x_train_com,y_train)\nprint('LR train model:' + str( model.score(x_train_com,y_train)))\nprint('LR test model:' + str(model.score(x_test_com,y_test)))\n","4da8a9ff":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error,r2_score\nXGB=XGBRegressor(learning_rate=.1,max_depth=9,gamma=.001,reg_alpha=.001)\nXGB.fit(x_train_std,y_train)\nimport numpy as np\n\nprint('XGB train model:' + str( XGB.score(x_train_std,y_train)))\nprint('XGB test  model:' + str(XGB.score(x_test_std,y_test)))\nXGB.predict(x_test_std)\nprint('RMSE:' + str(np.sqrt(mean_squared_error(y_test,XGB.predict(x_test_std)))))\nprint(r2_score(y_test,XGB.predict(x_test_std)))","818f2148":"from sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV,RidgeCV,ElasticNetCV\nlassoModel=LassoCV(alphas=[1,0.1,0.001,.01,.00001])\nlassoModel.fit(x_train_com,y_train)\nprint(\"Train Score (Linear):\",lassoModel.score(x_train_com,y_train))\nprint(\"Test Score (Linear):\",lassoModel.score(x_test_com,y_test))","5fb47725":"from sklearn.linear_model import Ridge\nRModel=RidgeCV(alphas=[1,0.1,0.001,.01,.0001])\n\nRModel.fit(x_train_com,y_train)\nprint(\"Train Score (Linear):\",RModel.score(x_train_com,y_train))\nprint(\"Test Score (Linear):\",RModel.score(x_test_com,y_test))\nprint('RMSE:' + str(np.sqrt(mean_squared_error(y_test,RModel.predict(x_test_com)))))","335901b8":"#ElasticNet\nfrom sklearn.linear_model import ElasticNet\n\nEModel=ElasticNetCV(alphas=[1,0.1,0.003,.03,.00001])\nEModel.fit(x_train_com,y_train)\nprint(\"Train Score (Linear):\",EModel.score(x_train_com,y_train))\nprint(\"Test Score (Linear):\",EModel.score(x_test_com,y_test))\nprint('RMSE:' + str(np.sqrt(mean_squared_error(y_test,EModel.predict(x_test_com)))))","8cd4bee5":"\nA=pd.DataFrame({'ID':df_test.Id,'SalePrice_log':EModel.predict(test_new_com)})\nA['SalePrice'] = np.exp(A['SalePrice_log'])\nA=A[['ID','SalePrice']]\nA.to_csv(\"submission.csv\",index=False)","633ecfbd":"## Understanding the DataType:","a3eb2067":"We can see there are quite a few outliers ,where the houses are 100+ year old and prices are more than 250K.We will do little more analysis before removing the outliers ,such as how mig is the house,how many rooms are thgere.etc","a2260d70":"After analysing we can say that following features are not have mucg of a diffrence on SalePrice:\nLandConfig,LandSlope,Roofstyle,Bldgtyp2,Fence,Functional\n\n#'YrSold','LowQualFinSF','BsmtHalfBath','MSSubClass','OverallCond','BsmtFinSF2'","69137ba2":"## Explore data (gain insights)*","ef2e9918":"We will do both Hot encoding and Label incoding here.We should not go for one hot encoding where the cardinality is more means if we hav emutiple value_counts for a feature,it will lead to curse of dimensionality","a0e59ba0":"### Handling Outliers","bfa2b36d":"We have a lot of missing data in our dataset .Now we have to decide based on our domain knowledge ,we will decide how to fill\/drop thos Nas value.\n\n1. We will follow the below mentioned steps to handle  missing values : \n   - we will ignore all those records where percentage is less than .02.we will either drop or fill 0 for those records.\n   - Some of these missing counts can be grouped together and then similar values can be filled in.Ex Basment and Garage related columns with        missing value.the missing value numbers are kind ofWe can check if the missing value is in the same row,if yes we can handle it accordingly.\n   - Feathures like POOLQC,FireplaceQu  are based on  other features like PoolArea and Fireplaces.We will check those feature and if there is       no fireplace or pool we will fill the value as 0 .\n   - MiscFeature .Alley is not much of use here  ,it does not impact buyers decision of buing a house ,it is just extra feature.\n   - Fence : if a house has a pool ,we know it has to have a fence,so we will check the pool feature to fill for this one.\n   - GarageYrBlt -we can check YearRemodAdd' to fill in the NA values in this one.","41e9274d":"SalePrice of any house depends on various factors:\n1. Sqft area\n2. Amenities \n3. year of build\n4. Type of house\n5. No of rooms\/bathroom\n6. Neibhorhood\n\nWe will try to focus on these feature and will gain insight about our data.","3c324a56":"We can see there are quite a few outliers in the numerical data type.\n'LotFrontage','LotArea',MasVnrArea',TotalBsmtSF,1stFlrSF,GrLivArea,GarageArea,'OpenPorchSF','EnclosedPorch','ScreenPorch'\n","9982d0b2":"## Label Encoding","6448b36a":"we can drop the features which are highly correlated.Normally the threshold is the features which are correlated by more than .7 ,just keep one of them.\nLater we will drop these features:TotalBsmtSf,TotRmsAbvGrd,GarageCars.","9640e9e1":"1. Analysis\n- Majority of the data population is looking for the houses built after 1940.\n- There is surge in sale price after 1980\n- More the number of room and more the living area greater in the saleprice\n- There are some houses which has more than 16 rooms+bathrooms but still the price is really low, even if they are built after 2000.Those kind of records are ourliers and we need to remove those records.\n- There are 2 records with saleprice more than 700K,it looks like outliers but they have 16 rooms+bathrooms and they are in a neighborhood where average saleprice 350K.hence we will keep these records.","fe19d7c7":"#### ","fd0927eb":"### Building Different models","389019bf":"Analysis:\n1> Features that has high impcat on SalePrice='OverallQual','TotalBsmtSF',1stFlrSf,GrLivArea,FullBath,GarageCars,GarageArea,\n\n2> lease significant features for SalePrice=Id,MSSubclass,BsmtFinSF2,overAllCond,LowQuaFinSF,EnclosedPorch,KitchenAbvGr,BsmtHalfBath\n\n3> Few features are highly correlated=TotRmsAbvGrd and GrLivArea,fullBath and GrLivArea,GarageCars and GarageArea,TotalBsmtSf and 1stFlrSF","85578d06":"#we can see the best model is ElasticNet model with R2 of 90.21 .We will use this model to predict our test data.","162f4332":"### Handling Missing Values"}}