{"cell_type":{"77699896":"code","77e08b44":"code","7813b51e":"code","d1cae5d0":"code","14001f2a":"code","91ed19d8":"code","19cddbfc":"code","595c4728":"code","6b623ebc":"code","118fe30f":"code","36782392":"code","c260111f":"markdown","42975453":"markdown","3b0b30cf":"markdown","915f121b":"markdown","bd8cb6bb":"markdown","09d1c520":"markdown","d416701b":"markdown","0c97518e":"markdown"},"source":{"77699896":"# importing library to handle files\nimport os\n\n# importing library to handle time\nimport time\n\n# importing libray to handle status bars\nfrom tqdm.notebook import tqdm\n\n# import libray to ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# importing library to deal with numeric arrays\nimport numpy as np\n\n# importing library to process images\nimport cv2\n\n# importing deep learning library\nimport tensorflow as tf\n\n# importing library for preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n# importing library for plotting\nimport matplotlib.pyplot as plt","77e08b44":"# initializing lists to store file paths for training and validation\ntrain_X_path = []\nval_X_path = []\n\n# importing libraries to store label references\ntrain_y_names = []\nval_y_names = []\n\n# iteration through directories and preprocessing filepaths and fielnames\nfor dirname, _, filenames in tqdm(os.walk('\/kaggle\/input')):\n    for filename in filenames:\n        \n        path = os.path.join(dirname, filename)\n        \n        if 'training' in dirname:\n            train_X_path.append(path)\n            train_y_names.append(path.split(os.path.sep)[-2])\n        elif 'validation' in dirname:\n            val_X_path.append(path)\n            val_y_names.append(path.split(os.path.sep)[-2])","7813b51e":"# defining a function to resize images\ndef img_prep(features, output, dims):\n\n    img_data = []\n    labels = []\n\n    for enum, imagePath in tqdm(enumerate(features)):\n    \n        try:\n            counter = 0\n            img=cv2.imread(imagePath)\n            img=cv2.resize(img, (dims[1], dims[0]))\n            \n        except Exception as e:\n        \n            counter = 1\n    \n        if counter==0:\n            \n            label = output[enum]\n            labels.append(label)\n        \n            img_data.append(img)\n            \n    return img_data, labels","d1cae5d0":"# preprocessing training and validation sets\nIMAGE_DIMS = (160, 160, 3)\n\ntrain_X, train_y = img_prep(train_X_path, train_y_names, IMAGE_DIMS)\nval_X, val_y = img_prep(val_X_path, val_y_names, IMAGE_DIMS)","14001f2a":"# defining a function to extract features\ndef img_feature_extraction(x_values, pre_model):\n\n    data = []\n    \n    # preprocessing and then using pretrained neural nets to extract features to be fed into Global Pooling\n    for image in tqdm(x_values):\n        im_toarray = tf.keras.preprocessing.image.img_to_array(image)\n        \n        im_toarray = np.expand_dims(image, axis=0)\n        im_toarray = tf.keras.applications.mobilenet.preprocess_input(im_toarray)\n        \n        data.append(im_toarray)\n        \n    data_stack = np.vstack(data) \n    \n    features = pre_model.predict(data_stack, batch_size=32)\n    \n    return data_stack, features","91ed19d8":"# importing pretrained MobileNet\nmnet_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=IMAGE_DIMS,\n                                               include_top=False, weights='imagenet')\n\n# freezing layers\nfor layer in mnet_model.layers:\n    layer.trainable = False","19cddbfc":"# final stage of preprocessing for both training and validation data\ntrainX_proc, train_features = img_feature_extraction(train_X, mnet_model)\nvalX_proc, val_features = img_feature_extraction(val_X, mnet_model)\n\n# encoding labels\nlb = LabelEncoder()\n\nlb.fit(train_y)\n\ntrain_y = lb.transform(train_y)\nval_y = lb.transform(val_y)","595c4728":"# defining a sequential model to learn \nclf_model = tf.keras.Sequential()\n\n# using global average pooling instead of flatten and global max pooling\nclf_model.add(tf.keras.layers.GlobalAveragePooling2D(input_shape=train_features.shape[1:]))\n\nclf_model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\nclf_model.add(tf.keras.layers.Dropout(0.3))\n\nclf_model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\nclf_model.add(tf.keras.layers.Dropout(0.3))\n\nclf_model.add(tf.keras.layers.Dense(len(np.unique(train_y)), activation=tf.nn.softmax))\n\nclf_model.summary()","6b623ebc":"# compiling the model\nEPOCHS = 100\n\nclf_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])","118fe30f":"# training the model\n\n# getting time\nt0 = time.time()\n\n# fitting the model\nhistory = clf_model.fit(train_features, train_y,\n                        batch_size=32, epochs=EPOCHS,\n                        verbose=0, validation_data=(val_features, val_y))\n\n# getting new time\nt1 = time.time()\n\n# printing fitting time\nprint(\"Fitting for\", EPOCHS, \"epochs took\", round(t1-t0,3), \"seconds\")","36782392":"# plotting validation and accuracy history\nfig = plt.figure(figsize=(15,6))\n\nkey_list = [[1, 'accuracy', 'val_accuracy', 'model_accuracy'], \n            [2, 'loss', 'val_loss', 'model_loss']]\n\nfor i, j, k, l in key_list:\n    plt.subplot(1, 2, i)\n    plt.plot(history.history[j])\n    plt.plot(history.history[k])\n    plt.title(l)\n    plt.ylabel(j)\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n\nplt.show()","c260111f":"# Model architecture","42975453":"# Preprocessing","3b0b30cf":"# Importing libraries ","915f121b":"This notebook has been inspired by Dipanjan Sarkar's post on transfer learning which can be read [here](https:\/\/towardsdatascience.com\/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a) and [this repository](https:\/\/github.com\/abhijeet3922\/Food-Classification-Task-Transfer-learning) on classification of the Food-11 dataset.","bd8cb6bb":"# Acknowledgement","09d1c520":"# Model training","d416701b":"# Handling files","0c97518e":"# Model evaluation"}}