{"cell_type":{"a26ae413":"code","7f3fa383":"code","a60caaac":"code","965f4797":"code","423a40bc":"code","3ff0a0df":"code","066e6f55":"code","ba6dda77":"code","543f6ab6":"markdown","2bed6ed0":"markdown","d02cab44":"markdown","e1248f1f":"markdown","fcdc2f80":"markdown","1f43995c":"markdown","87988d36":"markdown","e6ae8c16":"markdown","00336589":"markdown"},"source":{"a26ae413":"import os\nimport json\nimport numpy as np\n\nclass Task():\n    '''\n    Helper class to read a Task as a sequence of tokens\n    '''\n    def __init__(self, path, name):\n        filename = os.path.join(path, name)\n        with open(filename, 'r') as fp:\n            self.tasks = json.load(fp)\n\n    def max_size(self):\n        max_w, max_h = 0, 0\n        for t in self.tasks['train']:\n            w, h = self._size(t['input'])\n            if w > max_w: max_w = w\n            if h > max_h: max_h = h\n            w, h = self._size(t['output'])\n            if w > max_w: max_w = w\n            if h > max_h: max_h = h\n        for t in self.tasks['test']:\n            w, h = self._size(t['input'])\n            if w > max_w: max_w = w\n            if h > max_h: max_h = h\n            if 'output' in t:\n                w, h = self._size(t['output'])\n                if w > max_w: max_w = w\n                if h > max_h: max_h = h\n        return max_w, max_h, max(max_w, max_h)\n\n    def _size(self, lst):\n        h = len(lst)\n        w = len(lst[0])\n        return w, h\n\n    def get_train(self):\n        for t in self.tasks['train']:\n            inp = Task.as_sequence(t['input'])\n            out = Task.as_sequence(t['output'])\n            yield inp, out\n\n    def get_valid(self):\n        for t in self.tasks['test']:\n            inp = Task.as_sequence(t['input'])\n            out = Task.as_sequence(t['output'])\n            yield inp, out\n\n    def get_tests(self):\n        for t in self.tasks['test']:\n            inp = Task.as_sequence(t['input'])\n            yield inp\n\n    @staticmethod\n    def as_sequence(lst):\n        sequ = []\n        h = len(lst)\n        w = len(lst[0])\n        sequ.append('[CLS]')\n        for y in range(h):\n            for x in range(w):\n                c = lst[y][x]\n                sequ.append(str(c))\n                #sequ.append(' ')\n            sequ.append('[SEP]')\n        return sequ\n\n    @staticmethod\n    def pred_size(sequ):\n        sequ = sequ[1:]\n        w = 0\n        h = 0\n        x = 0\n        y = 0\n        for n in range(len(sequ)):\n            if sequ[n] == 0: break\n            if sequ[n] == 3:\n                w = max(x, w)\n                x = 0\n                y += 1\n                continue\n            x += 1\n        h = y\n        return w, h\n\n    @staticmethod\n    def as_array(sequ, w=16, h=16):\n        sequ = sequ[1:]\n        array = np.zeros((h, w), dtype='int')\n        n = 0\n        for y in range(h):\n            for x in range(w):\n                if n >= len(sequ): return array\n                c = sequ[n] - 5\n                array[y][x] = max(0, c)\n                n += 1\n            n += 1\n        return array\n\n    @staticmethod\n    def as_list(arr):\n        lst = []\n        for row in arr:\n            lst.append(list(row))\n        return lst\n    ","7f3fa383":"import os\nimport collections\nimport numpy as np\n\nfrom torch.utils.data import Dataset\n\nfrom transformers import BertTokenizer\n\nclass TaskDataset(Dataset):\n    def __init__(self, path, name, mode='train'):\n        self.tokenizer = BertTokenizer('vocab.txt')\n        self.items = self.__load(path, name, mode)\n\n    def __load(self, path, name, mode):\n        items = []\n        task = Task(path, name)\n        self.task_width, self.task_height, _ = task.max_size()\n        max_length = 2 + (self.task_width + 1) * self.task_height\n        if mode == 'train':\n            for inp, out in task.get_train():\n                inp = self.__tokenize(inp, max_length)\n                out = self.__tokenize(out, max_length)\n                items.append((name, inp, out))\n        if mode == 'valid':\n            for inp, out in task.get_valid():\n                inp = self.__tokenize(inp, max_length)\n                out = self.__tokenize(out, max_length)\n                items.append((name, inp, out))\n        if mode == 'tests':\n            for inp in task.get_tests():\n                inp = self.__tokenize(inp, max_length)\n                items.append((name, inp, inp))\n        return items\n\n    def __tokenize(self, sequ, max_length):\n        sequ_dict = self.tokenizer.encode_plus(sequ, add_special_tokens=False, max_length=max_length, pad_to_max_length=True)\n        return np.array(sequ_dict['input_ids'], dtype=np.int64), np.array(sequ_dict['attention_mask'], dtype=np.int64), np.array(sequ_dict['token_type_ids'], dtype=np.int64), \n\n    def __len__(self):\n        return len(self.items)\n\n    def __getitem__(self, idx):\n        return self.items[idx]\n    ","a60caaac":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as op\n\nfrom transformers import BertConfig, BertModel, BertPreTrainedModel\n\nINTERMEDIATE_SIZE = 1024\nHIDDEN_SIZE = 1024\nVOCAB_SIZE = 16\n\ndef create_model_config(w, h):\n    return {\n      \"architectures\": [\n        \"BertForMaskedLM\"\n      ],\n      \"attention_probs_dropout_prob\": 0.5,\n      \"hidden_act\": \"gelu\",\n      \"hidden_dropout_prob\": 0.5,\n      \"hidden_size\": HIDDEN_SIZE,\n      \"initializer_range\": 0.02,\n      \"intermediate_size\": INTERMEDIATE_SIZE,\n      \"max_position_embeddings\": 2 + (w + 1) * h,\n      \"num_attention_heads\": 8,\n      \"num_hidden_layers\": 8,\n      \"type_vocab_size\": 1,\n      \"vocab_size\": VOCAB_SIZE,\n      \"num_labels\": 1\n    }\n\nclass Network(BertPreTrainedModel):\n    def __init__(self, w, h):\n        config = BertConfig.from_dict(create_model_config(w, h))\n        super(Network, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.fc = nn.Linear(HIDDEN_SIZE, VOCAB_SIZE)\n\n        self.init_weights()\n\n    def forward(self, X):\n        ids = X[0]\n        msk = X[1]\n        typ = X[2]\n\n        output = self.bert(ids, attention_mask=msk, token_type_ids=typ)\n        x = output[0]\n        x = F.relu(self.fc(x))\n        x = x.transpose(1, 2)\n\n        return x\n    ","965f4797":"import numpy as np\n\nclass Summary():\n    def __init__(self):\n        self.epoch = -1\n        self.loss = None\n        self.accuracy = None\n        self.values = None\n        self.history = []\n\n    def register(self, epoch, loss, accuracy, values={}):\n        values['epoch'] = epoch\n        values['loss'] = loss\n        values['accuracy'] = accuracy\n        self.history.append(values)\n        if self.epoch < 0 or accuracy > self.accuracy:\n            self.epoch = epoch\n            self.loss = loss\n            self.accuracy = accuracy\n            self.values = values\n            return True\n        return False\n\n    def hash(self):\n        def r4(v): return \"{:6.4f}\".format(v)\n        return F'{self.epoch:02}-{r4(self.accy).strip()}'\n\n    def __str__(self):\n        def r4(v): return \"{:6.4f}\".format(v)\n        return F'{self.epoch}\\t{r4(self.values[\"loss\"])}\\t{r4(self.values[\"accuracy\"])}'\n    \nclass BaseMetrics():\n    def __init__(self):\n        self.loss = None\n        self.preds = None\n        self.targs = None\n        self.values = None\n\n    def begin(self):\n        self.loss = []\n        self.preds = []\n        self.targs = []\n        self.values = None\n\n    def update(self, loss, dz, dy):\n        self.loss.append(self._get_loss(loss))\n        self.preds.append(self._get_preds(dz))\n        self.targs.append(self._get_targets(dy))\n\n    def _get_loss(self, loss):\n        return loss.item()\n\n    def _get_preds(self, dz):\n        return dz.detach().cpu().numpy()\n\n    def _get_targets(self, dy):\n        return dy.detach().cpu().numpy()\n\n    def commit(self, epoch):\n        loss = np.mean(self.loss)\n        accuracy = 0.0\n        self.values = {\n                'loss': 0.0,\n                'accuracy': 0.0\n            }\n        return accuracy\n\n    def __str__(self):\n        def r4(v): return \"{:6.4f}\".format(v)\n        return F'{r4(self.values[\"loss\"])}\\t{r4(self.values[\"accuracy\"])}'\n\nclass CrossEntropyMetrics(BaseMetrics):\n    def _get_preds(self, dz):\n        dz = dz[:,:,1:]\n        return dz.detach().cpu().numpy()\n\n    def _get_targets(self, dy):\n        dy = dy[:,1:]\n        return dy.detach().cpu().numpy()\n\n    def commit(self, epoch):\n        loss = np.mean(self.loss)\n        preds = np.concatenate(self.preds, axis=0)\n        targs = np.concatenate(self.targs, axis=0)\n\n        preds = preds.argmax(axis=1)\n        targs = targs\n        preds = preds.reshape(preds.shape[0], -1)\n        targs = targs.reshape(targs.shape[0], -1)\n        accuracy = (sum(preds == targs) \/ len(preds)).mean()\n\n        match = 0\n        for pred, targ in zip(preds, targs):\n            if sum(pred != targ) == 0:\n                match += 1\n\n        accuracy += match\n\n        self.values = {\n                'loss': loss,\n                'accuracy': accuracy\n            }\n        return accuracy\n    ","423a40bc":"import numpy as np\n\nimport matplotlib.pyplot as plt\n\nclass EasyGrid():\n    def __init__(self, cmap=None, norm=None):\n        self.images = []\n        self.titles = []\n        self.cmap = cmap\n        self.norm = norm\n\n    def append(self, image, title=''):\n        self.images.append(image)\n        self.titles.append(title)\n\n    def show(self, rows, cols, title='', figsize=(4, 2), gridlines=False):\n        f, axs = plt.subplots(rows, cols, figsize=figsize)\n        f.suptitle(title, fontsize=16)\n        inx = 0\n        for y in range(rows):\n            for x in range(cols):\n                if inx == len(self.images): break\n                if rows > 1 and cols > 1:\n                    ax = axs[y, x]\n                elif rows > 1:\n                    ax = axs[y]\n                else:\n                    ax = axs[x]\n                ax.imshow(self.images[inx], cmap=self.cmap, norm=self.norm)\n                ax.set_title(self.titles[inx])\n                if gridlines:\n                    ax.grid(True, which='both', color='lightgrey', linewidth=0.5)   \n                    ax.set_yticks([x-0.5 for x in range(1+len(self.images[inx]))])\n                    ax.set_xticks([x-0.5 for x in range(1+len(self.images[inx][0]))])     \n                    ax.set_xticklabels([])\n                    ax.set_yticklabels([])\n                inx += 1\n        plt.show()\n\nfrom matplotlib import colors\n\nCMAP = colors.ListedColormap(['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00', '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\nNORM = colors.Normalize(vmin=0, vmax=9)\n\ndef plot_task_array(arrays, cols=2, title=''):\n    grid = EasyGrid(CMAP, NORM)\n    for array in arrays:\n        grid.append(array)\n    l = len(arrays)\n    grid.show(l \/\/ cols + l % cols, cols, title=title, gridlines=True)\n    ","3ff0a0df":"import os\nimport random\n\n# flattener\ndef flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred\n\n# Seed\ndef random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n","066e6f55":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as op\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nPATH = '..\/input\/abstraction-and-reasoning-challenge'\nPATH_TESTS = PATH + '\/test'\n\nEPOCHS = 100\nBATCH = 4\nLR = 0.0001\n\nSEED = 1234\nrandom_seed(SEED)\n\n#   Tokens\nTOKENS = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]',  '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '[###]']\nwith open('vocab.txt', 'w') as fp:\n    for t in TOKENS:\n        fp.write(t + '\\n')\n\ndef train(name, epochs=EPOCHS):\n    # Data\n    ds_train = TaskDataset(PATH_TESTS, name, mode='train')\n    ld_train = torch.utils.data.DataLoader(ds_train, batch_size=BATCH, num_workers=4, shuffle=True)\n\n    # Size\n    w = ds_train.task_width\n    h = ds_train.task_height\n\n    # Model\n    model = Network(w, h)\n    model = model.to(DEVICE)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = op.Adam(model.parameters(), lr=LR)\n\n    metrics = CrossEntropyMetrics()\n\n    # Train\n    summary_train = Summary()\n    for epoch in range(epochs):\n        model.train()\n        metrics.begin()\n        for step, (id, x, y) in enumerate(ld_train):\n            optimizer.zero_grad()\n\n            x0 = x[0].to(DEVICE)\n            x1 = x[1].to(DEVICE)\n            x2 = x[2].to(DEVICE)\n            dy = y[0].to(DEVICE)\n\n            dz = model((x0, x1, x2))\n\n            loss = criterion(dz[:,:,1:], dy[:,1:])\n            metrics.update(loss, dz, dy)\n\n            loss.backward()\n            optimizer.step()\n        accuracy = metrics.commit(epoch)\n        summary_train.register(epoch, loss.item(), accuracy, metrics.values)\n\n    print(summary_train)\n    return model\n\n\ndef predict(model, name):\n    # Data\n    ds_tests = TaskDataset(PATH_TESTS, name, mode='tests')\n    ld_tests = torch.utils.data.DataLoader(ds_tests, batch_size=1, shuffle=False)\n\n    # Size\n    w = ds_tests.task_width\n    h = ds_tests.task_height\n\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for step, (id, x, y) in enumerate(ld_tests):\n            x0 = x[0].to(DEVICE)\n            x1 = x[1].to(DEVICE)\n            x2 = x[2].to(DEVICE)\n\n            dz = model((x0, x1, x2))\n            z = dz.detach().cpu().numpy()\n            z = z[0]\n            az = z.argmax(0)\n            w, h = Task.pred_size(az)\n            w = max(1, w)\n            h = max(1, h)\n            pred = Task.as_array(az, w, h)\n            pred = Task.as_list(pred)\n            flat = flattener(pred)\n            preds.append(flat)\n\n    return preds\n\nsubmission = pd.read_csv(PATH + '\/sample_submission.csv', index_col='output_id')\n\nSTEPS = -1\n\nstep = 0\nhash = set()\nfor output_id in submission.index:\n    task_id = output_id.split('_')[0]\n    if task_id in hash:\n        continue\n    hash.add(task_id)\n    name = str(task_id + '.json')\n\n    model = train(name, EPOCHS)\n    preds = predict(model, name)\n\n    for n in range(len(preds)):\n        pred = preds[n] + ' ' + preds[n] + ' ' + preds[n]\n        submission.loc[F'{task_id}_{n}', 'output'] = pred\n    step += 1\n    if step == STEPS: break\n\nsubmission.to_csv('submission.csv')\n","ba6dda77":"df = pd.read_csv('submission.csv')\ndf.head()\n\ntokenizer = BertTokenizer('vocab.txt')\ndef tokenize(sequ, max_length):\n    sequ_dict = tokenizer.encode_plus(sequ, add_special_tokens=False, max_length=max_length, pad_to_max_length=True)\n    return np.array(sequ_dict['input_ids'], dtype=np.int64)\n\nfor values in df.values:\n    name = values[0]\n    name = name[:-2] + '.json'\n    print('TRAIN', name)\n    task = Task(PATH_TESTS, name)\n    for inp, out in task.get_train():\n        task_width, task_height, _ = task.max_size()\n        max_length = 2 + (task_width + 1) * task_height\n        x = tokenize(inp, max_length)\n        y = tokenize(out, max_length)\n        wx, hx = Task.pred_size(x)\n        arr_x = Task.as_array(x, wx, hx)\n        wy, hy = Task.pred_size(y)\n        arr_y = Task.as_array(y, wy, hy)\n        plot_task_array([arr_x, arr_y])\n    ix = 0\n    \n    print('PREDS', name)\n    for inp in task.get_tests():\n        try:\n            x = tokenize(inp, max_length)\n            wx, hx = Task.pred_size(x)\n            arr_x = Task.as_array(x, wx, hx)\n\n            parts = values[1].split(' ')[0].split('|')\n            parts = parts[1:-1]\n            h = len(parts)\n            w = len(parts[0])\n            a = np.zeros((h, w), dtype='int')\n            for y in range(h):\n                for x in range(w):\n                    a[y][x] = parts[y][x]        \n            plot_task_array([arr_x, a])\n        except Exception as ex:\n            print(ex)\n    print()","543f6ab6":"### Helper class to read a Task as a sequence of tokens","2bed6ed0":"### Misc helpers","d02cab44":"### Helper classes to calculate accuracy metrics","e1248f1f":"### Display predictions","fcdc2f80":"### PyTorch Dataset to feed training and prediction with one Task","1f43995c":"## Using BERT to Solve Tasks\n\nThis notebook uses BERT to predict the corresponding output with the pattern learned from one task at a time.\n\nBERT (Bidirectional Encoder Representations from Transformers) is a technique for NLP (Natural Language Processing) pre-training developed by Google. (Wikipedia)\n\nThe motivation for the use of BERT is based on:\n-\tBERT capacity to work with sequences\n-\tBERT attention mechanism explained in \"[Attention is All You Need](https:\/\/arxiv.org\/abs\/1706.03762)\"\n\nIn order to work with BERT, task images are converted to sequence of tokens. For example, a 3x3 image may look like this:\n\n[CLS] 0 2 2 [SEP] 3 2 2 [SEP] 0 2 2 [SEP]\n\nThis is a preliminary version that can be improved in many ways.\n\nHope you enjoy it! :)\n","87988d36":"### Helper class to display tasks","e6ae8c16":"### Model network based on Bert ","00336589":"### Train and Predict"}}