{"cell_type":{"b68c4880":"code","ed1cd8f5":"code","cadf9b64":"code","138872d8":"code","4d5f59ea":"code","ad0699d3":"code","c1a4a312":"code","8997d8be":"code","59cecd6a":"code","128d4e2c":"code","9b521a76":"code","b39d4383":"code","ed993797":"code","33aff6a8":"code","c66a6f9f":"code","1f7c4b9c":"code","d9dcac28":"code","ee6b9008":"code","3a21c2d3":"code","bbaf6533":"code","4174bed8":"code","c6099508":"code","da33c00c":"code","0567fd3b":"code","eddefd90":"code","670ad21a":"code","39d94410":"code","4174a57e":"code","598bb39d":"code","db8c910d":"code","7a32c111":"code","c3ff134f":"code","1c926d52":"code","d01ec380":"code","062bec80":"code","39614202":"code","f4435fd6":"code","2c35b8c6":"code","a58ba494":"code","28919045":"code","e0d3249a":"code","92a664f3":"code","cfe2f582":"code","a00cd5a0":"markdown","7ebbaaa0":"markdown","af879dfc":"markdown","86959e3e":"markdown","d7999f51":"markdown","efd81c0d":"markdown","a6c4cb36":"markdown","93f704ff":"markdown","ea4c8453":"markdown","0ab558dd":"markdown","ad180c7c":"markdown","2a432519":"markdown","2d5bf8f7":"markdown","2415b78f":"markdown","0819388b":"markdown","6b4442eb":"markdown","9cfd73ca":"markdown","79ec11d2":"markdown","da4ce16c":"markdown","1115ad47":"markdown","3a31dfac":"markdown","f6bd8237":"markdown","6cce7804":"markdown","3f1585a8":"markdown","9c68701e":"markdown","f2f1eb38":"markdown","f5d9030c":"markdown","148ee7ea":"markdown","bbd482c9":"markdown","4705803b":"markdown","8bf61046":"markdown","be64f3d9":"markdown","4a8ef5ca":"markdown","d577b5cd":"markdown","e10146bc":"markdown","658d7fd0":"markdown","903de067":"markdown","fa9521a5":"markdown","475749af":"markdown","6f111be0":"markdown","bc5c059b":"markdown","f6ecd3d0":"markdown","6af9eac7":"markdown","f8319e5e":"markdown"},"source":{"b68c4880":"# data processing\nimport pandas as pd\n\n## linear algebra\nimport numpy as np\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","ed1cd8f5":"titanic = pd.read_csv('..\/input\/titanic\/train.csv')\n# Print the first 5 rows of the dataframe.\ntitanic.head(5)\n","cadf9b64":"titanic_test = pd.read_csv('..\/input\/titanic\/test.csv')\n# Print the last 5 rows of the dataframe.\ntitanic_test.tail(5)","138872d8":"#shape command will give number of rows\/samples\/examples and number of columns\/features\/predictors in dataset\n#(rows,columns)\ntitanic.shape","4d5f59ea":"# Describe gives statistical information about numerical columns in the dataset\ntitanic.describe()\n#you can check from count if there are missing values in columns, here we can see there are some missing values in column \"Age\"","ad0699d3":"# info method provides information about dataset like.\n# total values in each column, null\/not null, datatype, memory occupied etc.\ntitanic.info()","c1a4a312":"# Let's write a function to print the total percentage of the missing values.\n# (this can be a good exercise for beginners to try to write simple functions like this.)\n\n#This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\ndef check_missing_data(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False) * 100 \/len(df),2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","8997d8be":"# check missing values in training data set\ncheck_missing_data(titanic)","59cecd6a":"# Check missing values in test data set\ncheck_missing_data(titanic_test)","128d4e2c":"drop_column = ['Name','Ticket']\ntitanic.drop(drop_column, axis= 1, inplace = True)\ntitanic_test.drop(drop_column,axis = 1,inplace = True)","9b521a76":"titanic['Cabin']=np.where(titanic['Cabin'].isnull(),0,1)\ntitanic_test['Cabin']=np.where(titanic_test['Cabin'].isnull(),0,1)","b39d4383":"#COMPLETING: complete or delete missing values in train and test\/validation dataset\ndataset = [titanic, titanic_test]\n\n# def missing_data(x):\nfor data in dataset:\n    #complete missing age with median\n    data['Age'].fillna(data['Age'].mean(), inplace = True)\n\n    #complete missing Embarked with Mode\n    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n\n    #complete missing Fare with median\n    data['Fare'].fillna(data['Fare'].mean(), inplace = True)\n      \ncheck_missing_data(titanic)","ed993797":"def draw(graph):\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 5,height ,ha= \"center\")","33aff6a8":"sns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\ngraph= sns.countplot(x='Survived', hue=\"Survived\", data=titanic)\ndraw(graph)","c66a6f9f":"plt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Cabin\", hue =\"Survived\", data = titanic)\ndraw(graph)","1f7c4b9c":"plt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Sex\", hue =\"Survived\", data = titanic)\ndraw(graph)","d9dcac28":"plt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Pclass\", hue =\"Survived\", data = titanic)\ndraw(graph)","ee6b9008":"plt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Embarked\", hue =\"Survived\", data = titanic)\ndraw(graph)","3a21c2d3":"drop_column = ['Embarked']\ntitanic.drop(drop_column, axis=1, inplace = True)\ntitanic_test.drop(drop_column,axis=1,inplace=True)","bbaf6533":"plt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Parch\", hue =\"Survived\", data = titanic)\ndraw(graph)","4174bed8":"plt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"SibSp\", hue =\"Survived\", data = titanic)\ndraw(graph)","c6099508":"# combine test and train as single to apply some function, we will use it again in Data Preprocessing\nall_data=[titanic,titanic_test]\n\nfor dataset in all_data:\n    dataset['Family'] = dataset['SibSp'] + dataset['Parch'] + 1","da33c00c":"plt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Family\", hue =\"Survived\", data = titanic)\ndraw(graph)","0567fd3b":"# create bin for age features. \nfor dataset in all_data:\n    dataset['Age_cat'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])\n    \nplt.figure(figsize = (8, 5))\nsns.barplot(x='Age_cat', y='Survived', data=titanic)","eddefd90":"plt.figure(figsize = (8, 5))\nag = sns.countplot(x='Age_cat', hue='Survived', data=titanic)\ndraw(ag)","670ad21a":"AAS = titanic[['Sex','Age_cat','Survived']].groupby(['Sex','Age_cat'],as_index=False).mean()\nsns.factorplot('Age_cat','Survived','Sex', data=AAS\n                ,aspect=3,kind='bar')\nplt.suptitle('Age , Sex vs Survived')","39d94410":"# create bin for fare features\nfor dataset in all_data:\n    dataset['Fare_cat'] = pd.cut(dataset['Fare'], bins=[0,10,50,100,550], labels=['Low_fare','median_fare','Average_fare','high_fare'])\nplt.figure(figsize = (8, 5))\nag = sns.countplot(x='Pclass', hue='Fare_cat', data=titanic)","4174a57e":"sns.barplot(x='Fare_cat', y='Survived', data=titanic)","598bb39d":"pd.DataFrame(abs(titanic.corr()['Survived']).sort_values(ascending = False))","db8c910d":"# Generate a mask for the upper triangle (taken from seaborn example gallery)\ncorr=titanic.corr()#['Survived']\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.subplots(figsize = (12,8))\nsns.heatmap(corr, \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu',\n            linewidths=.9, \n            linecolor='white',\n            vmax = 0.3,\n            fmt='.2f',\n            center = 0,\n            square=True)\nplt.title(\"Correlations Matrix\", y = 1,fontsize = 20, pad = 20);","7a32c111":"titanic.info()","c3ff134f":"# Convert \u2018Sex\u2019 feature into numeric.\ngenders = {\"male\": 0, \"female\": 1}\n\nfor dataset in all_data:\n    dataset['Sex'] = dataset['Sex'].map(genders)\ntitanic['Sex'].value_counts()","1c926d52":"# Give age rank as per their categories of children, teenage, adult, elders.\nfor dataset in all_data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 15, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 15) & (dataset['Age'] <= 20), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 26), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 28), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 28) & (dataset['Age'] <= 35), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 35) & (dataset['Age'] <= 45), 'Age'] = 5\n    dataset.loc[ dataset['Age'] > 45, 'Age'] = 6\ntitanic['Age'].value_counts()","d01ec380":"# As we created new fetures form existing one, so we remove that one.\n\n# Removing SibSp & Parch because we have family now. same way Age.\n# We also going to remove some other features like passenger id in list, Ticket number and Name.\n\nfor dataset in all_data:\n    drop_column = ['Age_cat','Fare','SibSp','Parch','Fare_cat','PassengerId']\n    dataset.drop(drop_column, axis=1, inplace = True)\n","062bec80":"input_cols = ['Pclass',\"Sex\",\"Age\",\"Cabin\",\"Family\"]\noutput_cols = [\"Survived\"]\nX_train = titanic[input_cols]\ny_train = titanic[output_cols]\nX_test=dataset","39614202":"dataset","f4435fd6":"model = LogisticRegression()\nmodel.fit(X_train,y_train)\ny_pred_lr=model.predict(X_test)\nmodel.score(X_train,y_train)","2c35b8c6":"model = KNeighborsClassifier(n_neighbors = 3) \nmodel.fit(X_train, y_train)  \ny_pred_knn = model .predict(X_test)  \nmodel.score(X_train,y_train)","a58ba494":"from sklearn.naive_bayes import GaussianNB\nmodel= GaussianNB()\nmodel.fit(X_train,y_train)\ny_pred_gnb=model.predict(X_test) \nmodel.score(X_train,y_train)","28919045":"model  = LinearSVC()\nmodel.fit(X_train, y_train)\n\ny_pred_svc = model.predict(X_test)\nmodel.score(X_train,y_train)","e0d3249a":"model  = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n\ny_pred_rf = model.predict(X_test)\nmodel.score(X_train,y_train)","92a664f3":"model = DecisionTreeClassifier() \nmodel.fit(X_train, y_train)\ny_pred_dt = model.predict(X_test) \nmodel.score(X_train,y_train)","cfe2f582":"titanic_test = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": titanic_test[\"PassengerId\"],\n        \"Survived\": y_pred_rf   # I have given prediction of random forest just change it to save prediction of other models here\n    })\nsubmission.to_csv('submission.csv', index=False)\nsubmission = pd.read_csv('submission.csv')\nsubmission.head(20)","a00cd5a0":"### 6. Decision tree","7ebbaaa0":"### 6. Family vs Survived","af879dfc":"As per the graph we can say that Gender is matter to survival if gender is Female then higher survival chances.","86959e3e":"### 5. SibSp & Parch vs Survived\n\n**SibSp** and **Parch** would make more sense, Parents not let child die, Bond of Blood relation always help each other first, rather than helping others they think about them self and their family member. \nIt is also proved by the two graphs SibSp s Survived & Parch vs Survived\nSo create new feature Family as a combination of SibSp and Parch,","d7999f51":" ### 6. Age vs Survived\n","efd81c0d":"## code to create submission file ","a6c4cb36":"So lets Understand little bit more about Random Forest.\n\n### What is Random Forest ?\nRandom Forest is a supervised learning algorithm. Like you can already see from it\u2019s name, it creates a forest and makes it somehow random. The \u201eforest\u201c it builds, is an ensemble of Decision Trees, most of the time trained with the \u201cbagging\u201d method. The general idea of the bagging method is that a combination of learning models increases the overall result.\n\nTo say it in simple words: \n\n**Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.**\n\nOne big advantage of random forest is, that it can be used for both classification and regression problems, which form the majority of current machine learning systems. With a few exceptions a random-forest classifier has all the hyperparameters of a decision-tree classifier and also all the hyperparameters of a bagging classifier, to control the ensemble itself.\n\nThe random-forest algorithm brings extra randomness into the model, when it is growing the trees. Instead of searching for the best feature while splitting a node, it searches for the best feature among a random subset of features. This process creates a wide diversity, which generally results in a better model. Therefore when you are growing a tree in random forest, only a random subset of the features is considered for splitting a node. You can even make trees more random, by using random thresholds on top of it, for each feature rather than searching for the best possible thresholds (like a normal decision tree does).","93f704ff":"## Get the data","ea4c8453":"### 2. K Nearest Neighbor:","0ab558dd":"## Data Visualizing and Analysis \n#### Let's analysis data and Check which features could contribute to a high survival rate ?","ad180c7c":"Looks like very unfair with male passenger,\n\nin all age band Female have more chances to survived.\n\n**Think on this:** Male are good person to help them or Female are smart person to survive. ","2a432519":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. ","2d5bf8f7":"### 4. Parch vs Survived","2415b78f":"## Import Necessary Libraries.","0819388b":"# Titanic Simply understand Best Model\n\n\n\n### **Introduction**\nThis is my first stab at a Kaggle script. I have chosen to work with the Titanic dataset after spending some time on the site I finally decided to go with a competition with one of my friends advice . \n\n<h2 style=\"color:blue\"><center> Don't forget to upvote\ud83d\udcc8 if you like\ud83d\udc4d\ud83c\udffb it  ","6b4442eb":"## Conclusion\nThis was my very first Kaggle competition that I tried.\n\nThank you for taking the time to read through my first exploration of a Kaggle dataset.\n\nFor the moment, let me know if you found this notebook useful or you just liked it: I would really appreciate it!","9cfd73ca":"## Predictive Modeling\nSo now we will predict the whether the Passenger will survive or not using some great Classification Algorithms.","79ec11d2":"## Which is the best Model ?\nas per me its random forest as much I have seen while submission","da4ce16c":"### 3. Gaussian Naive Bayes","1115ad47":"### 1. Logistic Regression:","3a31dfac":"We can see that Age, Embarked and cabin has missing values.\nnow, lets check missing values for test data.","f6bd8237":"## Lets check missing data","6cce7804":"### 2. Sex vs Survived\nLets check How many male and female survival.","3f1585a8":"### Feature engineering\n\nFeature engineering is the art of converting raw data into useful features. There are several feature engineering techniques that you can apply but, I am going to apply some specific ones.","9c68701e":"### 5. Random Forest:","f2f1eb38":"## Plz Upvote!","f5d9030c":"Age features are very important.\n\nSo children who have age less than 12 years have high chances to survival as parents and elder siblings think to save the younger once first and a young has more life left to leave so they where saved first.Their is one more factor sympathy with small childrens also work.\n\nSo we have created categories of people into Children, Teenager, Adults and Elders as per their age and display probability or chances of survival as per the respective age group. So lets check Count of survived for each age.","148ee7ea":"###  Survived Count\nLets visualize how many passenger & crew survived and how many not.","bbd482c9":"What do you think Embarked is important feature to predict high survival rate?  \n\nFrom which location passenger go on board to Titanic does not matter when we think logicaly that the people survive or not.\n\nSo we can use Embarked as feature here for getting high accuracy but logically its doesn't matter. so we drop it .\n\nAs a part of data science you have to thing 360 degree angle, some features are important but in calculation(mathematically) its not, so that why you must have domain knowledge for feature selection.","4705803b":"## Correlation & Correlation Matrix\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\n**POSITIVE CORRELATION:** If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\n**NEGATIVE CORRELATION:** If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.","8bf61046":"### 7. Fare Vs Survived\n\nA fare is the fee paid by a passenger for travel in Titanic in other words we can say Fare is the Ticket price.\n\nLets check Fare value is related to Passenger class or not? Logically it is related.\n\nSo let first create catagories Low_fare,median_fare,Average_fare,high_fare on the basis of fare and then plot graph of it.","be64f3d9":"## Table of content:\n* Import Necessary Libraries\n* Handle Missing Values\n* Data Visualizing and Analysis\n* Correlation & Correlation Matrix\n* Feature Engineering\n* Predictive Modeling \n>  1. Logistic Regression\n>  2. KNN Classifier\n>  3. Gaussian Naive Bayes\n>  4. Support Vector Machine(SVM)\n>  5. Decision Tree\n>  6. Random Forest\n* create submission file\n","4a8ef5ca":"Here we see age is important feature so lets check relation of age and sex for survived.\n\nThis different combination give us idea for selection of correct feature by programmatically and logically.\n\nSo lets plot the graph of Age & Sex vs Survived","d577b5cd":"So we can say that fare is correlated with Passenger class.\n\nHigh fare then upper class, Low fare then lower class.\n\nNow check For survived relation with Fare_bin and compare result of it with Pclass vs Survived.","e10146bc":"## Data Exploration and Analysis","658d7fd0":"Note: There is no  survived column in test data, which is our target variable and we will going to predict it.","903de067":"### 3.PClass vs Survived\n* **PClass:** Passenger belongs to which class.\n * 1st = Upper\n * 2nd = Middle\n * 3rd = Lower","fa9521a5":"### 1.  Cabin vs Survived\nLets many passenger & crew survived who has cabin name.","475749af":"### 4. Linear Support Vector Machine:","6f111be0":"#### Cabin has more than 75% of missing values in Train and test both dataset so many of us will think to delete it but their are more chances that the cabin name are given by the pepole survive\nso lets give 1 if cabin name is present and 0 if not present","bc5c059b":"High fare for passenger class one, and survived count are higher for Pclass-1 and High fare. \n\nLow fare for passenger class three, and survived count are lower for Pclass-3 and low fare.\n\nso they are co-related with each other. \n\nNow we drop fare form our data set but before that we check correlation.","f6ecd3d0":"Age, Fare and cabin has missing values. we will see how to work with these missing values next.\n#### lets delete the columns which are not useful for predicting the result i.e. name and ticket","6af9eac7":"### 4. Embarked vs Survived\n\n* **Embarked:** From which location passenger go on board to Titanic.\n * C = Cherbourg\n * Q = Queenstown \n * S = Southampton\n ","f8319e5e":"### 4. SibSp vs Survived"}}