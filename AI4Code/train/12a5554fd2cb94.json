{"cell_type":{"80e91722":"code","cb9e38f6":"code","e3a020d8":"code","0350e65d":"code","c5a49fda":"code","9bf42d34":"code","b097afb9":"code","1dae833e":"code","92312b59":"code","d5fe7f61":"code","bccfbf4f":"code","65612ac3":"code","b721a636":"code","22b1a90d":"code","86bcd3e3":"code","f627a2e4":"code","f1e46a37":"code","ec3b2647":"code","6206526b":"code","0b3d1a1e":"markdown","963d6156":"markdown","908c2e9f":"markdown","20810aad":"markdown","30d53f15":"markdown","0355c287":"markdown","67692f96":"markdown","899499c3":"markdown","917a38c0":"markdown","bda8c723":"markdown","512e71bc":"markdown","b6bdd6ea":"markdown"},"source":{"80e91722":"%matplotlib inline\n\nfrom collections import Counter, OrderedDict\nfrom os.path import join\n\nimport catboost as cb\nimport hyperopt\nimport hyperopt.pyll.stochastic\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom IPython.display import FileLink\nfrom hyperopt import hp, fmin, tpe\nfrom hyperopt.pyll.base import scope\nfrom pandas_summary import DataFrameSummary\nfrom sklearn import preprocessing\nfrom sklearn.base import TransformerMixin\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom tqdm import tqdm_notebook as tqdm","cb9e38f6":"ROOT = '\/kaggle\/input\/flight-delays-fall-2018\/'\nTARGET_COL = 'dep_delayed_15min'\nSEED = 1\n\npd.options.display.max_columns = None\n\ntrn_df = pd.read_csv(join(ROOT, 'flight_delays_train.csv'))\ntrn_data = trn_df.drop(columns=[TARGET_COL])\ntrn_target = trn_df[TARGET_COL].map({'Y': 1, 'N': 0})\ntst_data = pd.read_csv(join(ROOT, 'flight_delays_test.csv'))","e3a020d8":"class BaseTransform(TransformerMixin):\n    def fit(self, df):\n        return self\n    def transform(self, df):\n        return self.apply(df.copy())","0350e65d":"class AddFlightsFeature(BaseTransform):\n    def apply(self, df):\n        df['Flight'] = df['Origin'] + '-->' + df['Dest']\n        return df\n    \nclass AddSeasonFeature(BaseTransform):\n    def apply(self, df):\n        df['Season'] = df['Month'].str.slice(2).astype(int).map(\n            lambda x: \n            'winter' if x in (11, 12, 1) else \n            'spring' if x in (2, 3, 4) else\n            'summer' if x in (5, 6, 7) else\n            'fall'\n        )\n        return df\n    \nclass AddWeekendFeature(BaseTransform):\n    def __init__(self, weekend=(6, 7)):\n        self.weekend = weekend\n    def apply(self, df):\n        df['Weekend'] = df['DayOfWeek'].str.slice(2).astype(int).map(\n            lambda x: 'Yes' if x in self.weekend else 'No')\n        return df\n    \nclass AddDepartureFeatures(BaseTransform):\n    def apply(self, df):\n        df['DepHour'] = df['DepTime'].map(lambda x: (x \/\/ 100) % 24)\n        df['DepMinute'] = df['DepTime'].map(lambda x: x % 100)\n        df['DepDaytime'] = df['DepHour'].map(\n            lambda x:\n            'morning' if 6 <= x < 12 else\n            'afternoon' if 12 <= x < 16 else\n            'evening' if 16 <= x < 22 else\n            'night'\n        )\n        return df\n    \nclass AddDaySinceYearStart(BaseTransform):\n    def __init__(self):\n        from calendar import mdays\n        self.year_days = np.cumsum(mdays)\n        \n    def apply(self, df):\n        months = df['Month'].str.slice(2).astype(int)\n        days = df['DayofMonth'].str.slice(2).astype(int)\n        df['DaysSinceYearStart'] = months.map(lambda x: self.year_days[x - 1]) + days\n        return df\n    \nclass AddInteractionsFeatures(BaseTransform):\n    def __init__(self, pairs):\n        self.pairs = pairs\n    def apply(self, df):\n        for a, b in self.pairs:\n            df[f'{a}_{b}'] = df[a].astype(str) + '_' + df[b].astype(str)\n        return df\n    \nclass LogDistance(BaseTransform):\n    def apply(self, df):\n        df['Distance_Log'] = np.log(df['Distance'])\n        return df\n    \nclass AddFrequencyFeatures(BaseTransform):\n    def __init__(self, columns):\n        self.columns = columns\n    def apply(self, df):\n        for col in self.columns:\n            df[f'{col}_freq'] = df[col].map(df[col].value_counts(normalize=True))\n        return df\n    \nclass HarmonicFeatures(BaseTransform):\n    def __init__(self, col, modulo):\n        self.col = col\n        self.modulo = modulo\n    def apply(self, df):\n        df[f'{self.col}_sin'] = np.sin(2*np.pi*df[self.col]\/self.modulo)\n        df[f'{self.col}_cos'] = np.cos(2*np.pi*df[self.col]\/self.modulo)\n        return df\n    \nclass Bucketize(BaseTransform):\n    def __init__(self, col, buckets, prep_fn=None):\n        self.col = col\n        self.buckets = buckets\n        self.prep_fn = prep_fn\n    def apply(self, df):\n        col = df[self.col]\n        if self.prep_fn:\n            col = self.prep_fn(col)\n        df[f'{self.col}_bucket'] = pd.qcut(\n            col, self.buckets, labels=range(self.buckets)) \n        return df\n    \nclass BinarizeThreshold(BaseTransform):\n    def __init__(self, col, t, prep_fn=None):\n        self.col = col\n        self.t = t\n        self.prep_fn = prep_fn\n    def apply(self, df):\n        col = df[self.col]\n        if self.prep_fn:\n            col = self.prep_fn(col)\n        df[f'{self.col}_above_t={self.t}'] = (col > self.t).astype(int)\n        return df\n    \nclass DropColumns(BaseTransform):\n    def __init__(self, cols):\n        self.cols = cols\n    def apply(self, df):\n        return df.drop(columns=self.cols)","c5a49fda":"def to_int(x):\n    return x.str.slice(2).astype(int)\n\ndef categorical_columns_indexes(df):\n    return [i for i, col in enumerate(df.columns) \n            if df.dtypes[col] not in (np.float32, np.float64)]\n\npipeline = Pipeline([\n    ('flight', AddFlightsFeature()),\n    ('season', AddSeasonFeature()),\n    ('weekend', AddWeekendFeature()),\n    ('daytime', AddDepartureFeatures()),\n    ('start_days', AddDaySinceYearStart()),\n    ('interact', AddInteractionsFeatures([\n        ['UniqueCarrier', 'Origin'], \n        ['UniqueCarrier', 'Dest']\n    ])),\n    ('logdist', LogDistance()),\n    ('freq', AddFrequencyFeatures(['UniqueCarrier', 'Origin', 'Dest'])),\n    ('harmonic_hour' , HarmonicFeatures('DepHour', modulo=24)),\n    ('harmonic_minute', HarmonicFeatures('DepMinute', modulo=60)),\n    ('bucket_dom', Bucketize('DayofMonth', 4, prep_fn=to_int)),\n    ('binarize_dow', BinarizeThreshold('DayOfWeek', t=4, prep_fn=to_int)),\n    ('binarize_dsy', BinarizeThreshold('DaysSinceYearStart', t=365\/\/2)),\n    ('dephour_t1', BinarizeThreshold('DepHour', t=5)),\n    ('dephour_t2', BinarizeThreshold('DepHour', t=12)),\n    ('dephour_t3', BinarizeThreshold('DepHour', t=18)),\n    ('bucket_dist', Bucketize('Distance', 5)),\n\n    # Note: some features are excluded from this pipeline to reduce kernel's score.\n\n    ('drop', DropColumns(['Distance']))\n])\n\neng_trn_data = pipeline.fit_transform(trn_data)\neng_tst_data = pipeline.transform(tst_data)\ncat_idx = categorical_columns_indexes(eng_trn_data)\n\nX = eng_trn_data.values\ny = trn_target.values\nX_test = eng_tst_data.values","9bf42d34":"X","b097afb9":"counts = trn_target.value_counts()\nclass_weights = [1, counts[0]\/counts[1]]","1dae833e":"params = dict(\n    depth=8,\n    l2_leaf_reg=0.5,\n    bagging_temperature=2.0,\n    border_count=64,\n    grow_policy='Lossguide',\n    num_leaves=10,\n    class_weights=class_weights,\n    eval_metric='AUC',\n    task_type='GPU',\n    loss_function='Logloss')","92312b59":"X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      test_size=0.3,\n                                                      random_state=SEED,\n                                                      stratify=y)\n\ntrn_pool = cb.Pool(X_train, y_train, cat_features=cat_idx)\nval_pool = cb.Pool(X_valid, y_valid, cat_features=cat_idx)\n\nmodel = cb.train(params=params,\n                 dtrain=trn_pool,\n                 verbose=100, \n                 early_stopping_rounds=200,\n                 eval_set=val_pool, \n                 iterations=1000)","d5fe7f61":"def feature_importance(model):\n    scores = model.feature_importances_\n    indexes = [int(c) for c in model.feature_names_]\n    ser = pd.Series(dict(zip(indexes, scores)))\n    ser.sort_values(inplace=True)\n    return ser","bccfbf4f":"def plot_feature_importance(model, colnames):\n    fig, ax = plt.subplots(figsize=(12, 10))\n    importance = feature_importance(model)\n    importance.index = [colnames[i] for i in importance.index]\n    importance.plot.barh(ax=ax)","65612ac3":"plot_feature_importance(model, eng_trn_data.columns)","b721a636":"importance = feature_importance(model)\nrelevant_cols = eng_trn_data.columns[importance[importance > 1].index]\ncat_idx = categorical_columns_indexes(eng_trn_data[relevant_cols])\nX = eng_trn_data[relevant_cols].values\ny = trn_target.values\nX_test = eng_tst_data[relevant_cols].values","22b1a90d":"def logit(x):\n    return 1\/(1 + np.exp(x))","86bcd3e3":"space = dict()\n\ndef p(name, func, *args, scope_fn=None):\n    distrib = func(name, *args)\n    if scope_fn is not None:\n        distrib = scope_fn(distrib)\n    space[name] = distrib\n\nprint('Building search space...')\np('depth', hp.quniform, 3, 10, 1, scope_fn=scope.int)\np('l2_leaf_reg', hp.uniform, 0.01, 50.0)\np('random_strength', hp.uniform, 0.0, 100.0)\np('bagging_temperature', hp.uniform, 0, 20.0)\np('border_count', hp.quniform, 1, 255, 1, scope_fn=scope.int)\np('grow_policy', hp.choice, ['SymmetricTree', 'Depthwise', 'Lossguide'])\n\ndef catboost_search(params):\n    params['silent'] = True\n    params['loss_function'] = 'Logloss'\n    params['class_weights'] = class_weights\n    params['task_type'] = 'GPU'\n    model = cb.train(\n        params=params, dtrain=trn_pool,\n        early_stopping_rounds=10,\n        eval_set=val_pool, iterations=100)\n    probs = 1 - logit(model.predict(X_valid))\n    auc = roc_auc_score(y_valid, probs)\n    return {'loss': -auc, 'status': hyperopt.STATUS_OK, 'params': params}\n\ntrials = hyperopt.Trials()\n\n# Clone the kernel locally and un-comment the following section to perform search.\n#\n# best = fmin(catboost_search, \n#             space=space, \n#             algo=tpe.suggest, \n#             trials=trials,\n#             max_queue_len=12,\n#             max_evals=10)","f627a2e4":"best = dict(\n    depth=8,\n    l2_leaf_reg=25.922353989859875,\n    bagging_temperature=1.6853010941877322,\n    border_count=65.0,\n    grow_policy='Lossguide',\n    num_leaves=35,\n    random_strength=0.8073770414011081,\n    class_weights=class_weights,\n    eval_metric='AUC',\n    task_type='GPU',\n    loss_function='Logloss')","f1e46a37":"from sklearn.model_selection import StratifiedKFold\nk = 5\nkfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\npreds = np.zeros((len(X_test), k), dtype=np.float32)\nfor i, (trn, val) in enumerate(kfold.split(X, y)):\n    print(f'Running k-fold: {i+1} of {k}')\n    trn_pool = cb.Pool(X[trn], y[trn], cat_features=cat_idx)\n    val_pool = cb.Pool(X[val], y[val], cat_features=cat_idx)\n    model = cb.train(params=best,\n                     dtrain=trn_pool,\n                     verbose=100, \n                     early_stopping_rounds=200,\n                     eval_set=val_pool, \n                     iterations=10000)\n    fold_preds = 1 - logit(model.predict(X_test))\n    preds[:, i] = fold_preds","ec3b2647":"y_test = preds.mean(axis=1)","6206526b":"filename = 'submit.csv'\nsample_df = pd.read_csv(join(ROOT, 'sample_submission.csv'), index_col='id')\nsample_df[TARGET_COL] = y_test\nsample_df.to_csv(filename)\n# FileLink(filename)","0b3d1a1e":"We start with a base class that inherits from one of the internal `scikit-learn` classes. Our transformations will not have any state so `fit()` method is a simple noop-call. Next, we inherit our transformers from the base implementation.","963d6156":"### Init","908c2e9f":"The final step: K-fold validation. We train the model on different subsets of data, and average its predictions to reduce the variance.","20810aad":"The `hyperopt` package is a tool helping to walk a continuous or discreate search space and pick the best parameters for our classifier. We use a helper function to build the `scape` dictionary, but is not required, and the scape could be constructed directly using built-in Python containers.","30d53f15":"### The `scikit-learn` Data Processing Interface and Custom Transformers\n\nThe `scikit-learn` library implements generic and very convenient approach to data transformation pipelines. The whole concept is build on top of two major methods, called `fit(X, y=None)` and `transform(X, y=None)`. Whenever you need to implement a custom solution that should be \"pluggable\" into `sklearn.Pipeline` class, you just need to properly define these two methods.\n\nFor example, consider adding a new `Flights` feature to the dataframe. It could be easily done with one line of code. But your pipeline would probably include more than just a couple of transformations. Also, some of these transformations could be stateful. Therefore, wrapping your code into `sklearn`-compatible format could help to deal with this complexity.\n\nSo let's implement a few transformations to add new features into dataset and improve the baseline accuracy.","0355c287":"In this notebook, we're going to implement a group of data transformations using `scikit-learn` pipeline interface and optimize `catboost` classifier parameters with `hyperopt` package. Using these tools, we can easily get `> 0.76` score, if add some more features to the baseline.\n\n> **Note:** This notebook was intentionally modified to reduce its leaderboard score, but more careful tuning of classifier parameters and adding a few more features will increase it above the baseline.","67692f96":"> **Warning:** For some reason, the following cell crashes on Kaggle kernel but work on the local machine. Therefore, the code which actually performs the search is commented out here but should work in different environment.","899499c3":"And now we can easily aggregate all transformations into a single pipeline, as the following cell shows.","917a38c0":"We easily can modify the pipeline, commenting out old steps or adding the new ones. No need to chain function calls manually, or handle train\/test transformations differently. Everything is done by `Pipeline` class.","bda8c723":"### Modelling with CatBoost\n\nThe dataset is prepared so now we can start the modelling part. Doing so with `CatBoost` is straightforward: the categorical features are provided as is, without converting them into digits at first. All the transformations are performed internally.","512e71bc":"According to the feature importance plot shown above, some of our newly-added features are very strong predictors. So it seems like we're going in the right direction.","b6bdd6ea":"### Features Selection, Parameters Tuning, and K-Fold\n\nNext thing to try after feature engineer step completed is to improve model's quality with hyper parameters tuning and applying K-fold cross-validation to average predictions of several classifiers. Also, we can use the plot shown in the previous section to take only strong predictions for the final model training to reduce the overfitting."}}