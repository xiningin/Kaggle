{"cell_type":{"25fa8c24":"code","11c0bf0c":"code","0d98bde2":"code","c713b55d":"code","7ce60be4":"code","f161b3ac":"code","afe8dfdb":"code","7874bf76":"code","8a916e45":"code","a639f5f2":"code","7b00326b":"code","32d02912":"code","c16e3943":"code","cce5b2ba":"code","1bc14127":"code","ff18060a":"code","1d2fed01":"code","66553258":"code","63a1fd66":"code","c70e9e08":"code","262cafcf":"code","33a4f8a0":"code","a9e32561":"code","e16c71cc":"code","0a6fa8a8":"code","4e28ad93":"code","5910eb26":"markdown","9647e83c":"markdown","7cf9da39":"markdown","39e29082":"markdown","30f05265":"markdown","6c237c60":"markdown","29278205":"markdown","079b2b24":"markdown","9f51b937":"markdown","2211601d":"markdown","3d7fca35":"markdown","5071587b":"markdown","34bb605f":"markdown","864594ea":"markdown","c51cb680":"markdown","527c1ac1":"markdown","01265ec9":"markdown","dcd19f5b":"markdown","c3f50355":"markdown","e1fd251d":"markdown","49c48125":"markdown","af685da0":"markdown","ef8cc064":"markdown","52479c2d":"markdown","f893a128":"markdown","8ff8d617":"markdown","241b4014":"markdown","20616f25":"markdown","1a5630bd":"markdown","7d0dd706":"markdown"},"source":{"25fa8c24":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.mlab as mlab\n%matplotlib inline","11c0bf0c":"heart_df=pd.read_csv(\"..\/input\/framingham.csv\")\nheart_df.drop(['education'],axis=1,inplace=True)\nheart_df.head()","0d98bde2":"heart_df.rename(columns={'male':'Sex_male'},inplace=True)","c713b55d":"heart_df.isnull().sum()","7ce60be4":"count=0\nfor i in heart_df.isnull().sum(axis=1):\n    if i>0:\n        count=count+1\nprint('Total number of rows with missing values is ', count)\nprint('since it is only',round((count\/len(heart_df.index))*100), 'percent of the entire dataset the rows with missing values are excluded.')","f161b3ac":"heart_df.dropna(axis=0,inplace=True)","afe8dfdb":"def draw_histograms(dataframe, features, rows, cols):\n    fig=plt.figure(figsize=(20,20))\n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        dataframe[feature].hist(bins=20,ax=ax,facecolor='midnightblue')\n        ax.set_title(feature+\" Distribution\",color='DarkRed')\n        \n    fig.tight_layout()  \n    plt.show()\ndraw_histograms(heart_df,heart_df.columns,6,3)","7874bf76":"heart_df.TenYearCHD.value_counts()","8a916e45":"sn.countplot(x='TenYearCHD',data=heart_df)","a639f5f2":"sn.pairplot(data=heart_df)","7b00326b":"heart_df.describe()","32d02912":"from statsmodels.tools import add_constant as add_constant\nheart_df_constant = add_constant(heart_df)\nheart_df_constant.head()","c16e3943":"st.chisqprob = lambda chisq, df: st.chi2.sf(chisq, df)\ncols=heart_df_constant.columns[:-1]\nmodel=sm.Logit(heart_df.TenYearCHD,heart_df_constant[cols])\nresult=model.fit()\nresult.summary()\n","cce5b2ba":"def back_feature_elem (data_frame,dep_var,col_list):\n    \"\"\" Takes in the dataframe, the dependent variable and a list of column names, runs the regression repeatedly eleminating feature with the highest\n    P-value above alpha one at a time and returns the regression summary with all p-values below alpha\"\"\"\n\n    while len(col_list)>0 :\n        model=sm.Logit(dep_var,data_frame[col_list])\n        result=model.fit(disp=0)\n        largest_pvalue=round(result.pvalues,3).nlargest(1)\n        if largest_pvalue[0]<(0.05):\n            return result\n            break\n        else:\n            col_list=col_list.drop(largest_pvalue.index)\n\nresult=back_feature_elem(heart_df_constant,heart_df.TenYearCHD,cols)","1bc14127":"result.summary()","ff18060a":"params = np.exp(result.params)\nconf = np.exp(result.conf_int())\nconf['OR'] = params\npvalue=round(result.pvalues,3)\nconf['pvalue']=pvalue\nconf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\nprint ((conf))\n","1d2fed01":"import sklearn\nnew_features=heart_df[['age','Sex_male','cigsPerDay','totChol','sysBP','glucose','TenYearCHD']]\nx=new_features.iloc[:,:-1]\ny=new_features.iloc[:,-1]\nfrom sklearn.cross_validation import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=5)","66553258":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","63a1fd66":"sklearn.metrics.accuracy_score(y_test,y_pred)","c70e9e08":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsn.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")","262cafcf":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP\/float(TP+FN)\nspecificity=TN\/float(TN+FP)","33a4f8a0":"print('The acuuracy of the model = TP+TN\/(TP+TN+FP+FN) = ',(TP+TN)\/float(TP+TN+FP+FN),'\\n',\n\n'The Missclassification = 1-Accuracy = ',1-((TP+TN)\/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate = TP\/(TP+FN) = ',TP\/float(TP+FN),'\\n',\n\n'Specificity or True Negative Rate = TN\/(TN+FP) = ',TN\/float(TN+FP),'\\n',\n\n'Positive Predictive value = TP\/(TP+FP) = ',TP\/float(TP+FP),'\\n',\n\n'Negative predictive Value = TN\/(TN+FN) = ',TN\/float(TN+FN),'\\n',\n\n'Positive Likelihood Ratio = Sensitivity\/(1-Specificity) = ',sensitivity\/(1-specificity),'\\n',\n\n'Negative likelihood Ratio = (1-Sensitivity)\/Specificity = ',(1-sensitivity)\/specificity)\n","a9e32561":"y_pred_prob=logreg.predict_proba(x_test)[:,:]\ny_pred_prob_df=pd.DataFrame(data=y_pred_prob, columns=['Prob of no heart disease (0)','Prob of Heart Disease (1)'])\ny_pred_prob_df.head()","e16c71cc":"from sklearn.preprocessing import binarize\nfor i in range(1,5):\n    cm2=0\n    y_pred_prob_yes=logreg.predict_proba(x_test)\n    y_pred2=binarize(y_pred_prob_yes,i\/10)[:,1]\n    cm2=confusion_matrix(y_test,y_pred2)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n    ","0a6fa8a8":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Heart disease classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)","4e28ad93":"sklearn.metrics.roc_auc_score(y_test,y_pred_prob_yes[:,1])","5910eb26":"## <font color=RoyalBlue>Data Preparation<font>\n\n### <font color=CornflowerBlue>Source:<font>\n\nThe dataset is publically available on the Kaggle website, and it is from an ongoing ongoing cardiovascular study on residents of the town of Framingham, Massachusetts.  The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).The dataset provides the patients\u2019 information. It includes over 4,000 records and 15 attributes.","9647e83c":"### <font color=CornflowerBlue>Missing values<font>","7cf9da39":"Since the model is predicting Heart disease too many type II errors is not advisable. A False Negative ( ignoring the probability of disease when there actualy is one) is more dangerous than a False Positive in this case. Hence inorder to increase the sensitivity,  threshold can be lowered.","39e29082":"## <font color=RoyalBlue>Model Evaluation<font>\n\n### <font color=CornflowerBlue>Model accuracy<font>","30f05265":"### <font color=CornflowerBlue>Predicted probabilities of  0 (No Coronary Heart Disease) and 1 ( Coronary Heart Disease: Yes)  for the test data with a default classification threshold of 0.5<font>\n","6c237c60":"<div class=\"alert alert-info\">\n<h1><center><font color=darkblue> HEART DISEASE PREDICTION USING LOGISTIC REGRESSION.<font><\/center><\/h1>\n\n\n<\/div>","29278205":"### <font color=CornflowerBlue>Confusion matrix<font>","079b2b24":"### <font color=CornflowerBlue>Variables :<font>","9f51b937":"### <font color=CornflowerBlue>Splitting data to train and test split<font>","2211601d":"####  <font color=DarkBlue>Accuracy of the model is 0.88<font>","3d7fca35":"### <font color=CornflowerBlue>Feature Selection: Backward elemination (P-value approach)<font>","5071587b":"The results above show some of the attributes with P value higher than the preferred alpha(5%) and thereby showing  low statistically significant relationship with the probability of heart disease. Backward elemination approach is used here to remove those attributes with highest Pvalue one at a time follwed by  running the regression repeatedly until all attributes have P Values less than 0.05.\n\n","34bb605f":"### <font color=CornflowerBlue>Area Under The Curve (AUC)<font>\n\nThe area under the ROC curve quantifies model classification accuracy; the higher the area, the greater the disparity between true and false positives, and the stronger the model in classifying members of the training dataset. An area of 0.5 corresponds to a model that performs no better than random classification and a good classifier stays as far away from that  as possible. An area of 1 is ideal. The closer the AUC to 1 the better.","864594ea":"## <font color=RoyalBlue>Appendix\n\nhttp:\/\/www.who.int\/mediacentre\/factsheets\/fs317\/en\/\n\n#### Data Source References\n\nhttps:\/\/www.kaggle.com\/amanajmera1\/framingham-heart-study-dataset\/data\n\n","c51cb680":"Logistic regression is a type of regression analysis in statistics used for prediction of outcome of a categorical dependent variable from a set of predictor or independent variables. In logistic regression the dependent variable is always binary. Logistic regression is mainly used to for prediction and also calculating the probability of success. ","527c1ac1":"The confusion matrix shows 658+4 = 662 correct predictions and 88+1= 89 incorrect ones.\n\n**<font color=DarkBlue>True Positives:**  4<font>\n\n**<font color=DarkBlue>True Negatives:**  658<font>\n\n**<font color=DarkBlue>False Positives:** 1 (*Type I error*)<font>\n\n**<font color=DarkBlue>False Negatives:** 88 ( *Type II error*)<font>","01265ec9":"There are 3179 patents with no heart disease and 572 patients with risk of heart disease.","dcd19f5b":"\n## <font color=RoyalBlue> Conclusions:<\/font>\n\n<div class=\"alert alert-info\">\n\n\n\n<div class=\"panel-body\">\n\n\n - **<font color=darkblue>All attributes selected after the elimination process show Pvalues lower than 5% and thereby suggesting significant role in the Heart disease prediction.<\/font>** \n<br>\n<br>\n - **<font color=darkblue>Men seem to be more susceptible to heart disease than women.Increase in Age,number of cigarettes smoked per day and systolic Blood Pressure also show increasing odds of having heart disease.<\/font>**\n <br>\n <br>\n\n - **<font color=darkblue>Total cholesterol shows no significant change in the odds of CHD. This could be due to the presence of 'good cholesterol(HDL) in the total cholesterol reading.Glucose too causes a very negligible change in odds (0.2%)<\/font>**\n <br>\n <br>\n\n - **<font color=darkblue>The model predicted with 0.88 accuracy. The model is more specific than sensitive.<\/font>**\n <br>\n <br>\n\n - **<font color=darkblue>The Area under the ROC curve is 73.5 which is somewhat satisfactory.<\/font> **\n <br>\n <br>\n\n - ** <font color=darkblue>Overall model could be improved with more data.<\/font>**\n\n<\/div>\n<\/div>","c3f50355":"## <font color=RoyalBlue>Exploratory Analysis<font>","e1fd251d":"## <font color=RoyalBlue>Logistic Regression<font>","49c48125":" - **This fitted model shows that, holding all other features constant, the odds of getting diagnosed with heart disease for males (sex_male = 1)over that of  females (sex_male = 0) is exp(0.5815) = 1.788687. In terms of percent change, we can say that the odds for males are 78.8% higher than the odds for females.**\n<br>\n<br>\n\n - **The coefficient for age says that, holding all others constant, we will see 7% increase in the odds of getting diagnosed with CDH for a one year increase in age since exp(0.0655) = 1.067644.**\n<br>\n<br>\n - **Similarly , with every extra cigarette one smokes thers is a 2% increase in the odds of CDH.** \n<br>\n<br>\n - **For Total cholosterol level and glucose level there is no significant change.**\n<br>\n<br>\n - **There is a 1.7% increase in odds for every unit increase in systolic Blood Pressure.**\n","af685da0":"Each attribute is a potential risk factor. There are both demographic, behavioural and medical risk factors.\n\n - **<font color=SteelBlue>Demographic:<font>**\nsex: male or female;(Nominal)\n\n    -  age: age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n\n\n - **<font color=SteelBlue>Behavioural<font>**\n\n    -  currentSmoker: whether or not the patient is a current smoker (Nominal)\n\n    -  cigsPerDay: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarretts, even half a cigarette.)\n\n - **<font color=SteelBlue>Medical( history):<font>**\n\n    -  BPMeds: whether or not the patient was on blood pressure medication (Nominal)\n\n    -  prevalentStroke: whether or not the patient had previously had a stroke (Nominal)\n\n    -  prevalentHyp: whether or not the patient was hypertensive (Nominal)\n\n    -  diabetes: whether or not the patient had diabetes (Nominal)\n\n - **<font color=SteelBlue>Medical(current):<font>** \n\n    -  totChol: total cholesterol level (Continuous)\n\n    -  sysBP: systolic blood pressure (Continuous)\n\n    -  diaBP: diastolic blood pressure (Continuous)\n\n    -  BMI: Body Mass Index (Continuous)\n\n    -  heartRate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n\n    -  glucose: glucose level (Continuous)\n\n\n - **<font color=SteelBlue>Predict variable (desired target):<font>**\n\n    -  10 year risk of coronary heart disease CHD (binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d)\n","ef8cc064":"#### <font color=darkblue>Logistic regression equation<font>\n\n$$P=\\hspace{.2cm}e^{\\beta_0 + \\beta_1 X_1}\\hspace{.2cm}\/\\hspace{.2cm}1+e^{\\beta_0 +\\beta_1 X_1}$$\n\nWhen all features plugged in:\n\n$$logit(p) = log(p\/(1-p))=\\beta_0 +\\beta_1\\hspace{.1cm} *\\hspace{.2cm} Sexmale\\hspace{.2cm}+\\beta_2\\hspace{.1cm} * \\hspace{.1cm}age\\hspace{.2cm}+\\hspace{.2cm}\\beta_3\\hspace{.1cm} *\\hspace{.1cm} cigsPerDay\\hspace{.2cm}+\\hspace{.2cm}\\beta_4 \\hspace{.1cm}*\\hspace{.1cm} totChol\\hspace{.2cm}+\\hspace{.2cm}\\beta_5\\hspace{.1cm} *\\hspace{.1cm} sysBP\\hspace{.2cm}+\\hspace{.2cm}\\beta_6\\hspace{.1cm} *\\hspace{.1cm} glucose\\hspace{.2cm}$$\n\n","52479c2d":"### <font color=CornflowerBlue>Model Evaluation - Statistics<font>","f893a128":"## <font color=RoyalBlue>Interpreting the results: Odds Ratio, Confidence Intervals and Pvalues<font>","8ff8d617":"### <font color=CornflowerBlue>Lower the threshold<font>","241b4014":"### <font color=CornflowerBlue>ROC curve<font>","20616f25":"## <font color=RoyalBlue>Introduction<font>\n\nWorld Health Organization has  estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States and other developed countries are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. This research intends to pinpoint the most relevant\/risk factors of heart disease as well as predict the overall risk using logistic regression.\n","1a5630bd":"A common way to visualize the trade-offs of different thresholds is by using an ROC curve, a plot of the true positive rate (# true positives\/ total # positives) versus the false positive rate (# false positives \/\ntotal # negatives) for all possible choices of thresholds. A model with good classification accuracy should have significantly more true positives than false positives at all thresholds. \n\nThe optimum position for roc curve is towards the top left corner where the specificity and sensitivity are at optimum levels","7d0dd706":"**From the above statistics it is clear that the model is highly specific than sensitive. The negative values are predicted more accurately than the positives.**"}}