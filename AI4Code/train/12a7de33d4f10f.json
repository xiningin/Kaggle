{"cell_type":{"7f1499b1":"code","f3d49c51":"code","7f025422":"code","3c1ba19b":"code","7ea9ca67":"code","dc082f8f":"code","74a6c2b2":"code","371b45e8":"code","880dee7f":"code","0f986577":"code","ef5ea7bd":"code","686315ff":"code","34a264dd":"code","94cbf2fe":"code","a43134c4":"code","cb07c76b":"code","f643b54c":"code","60a76fe3":"code","c93f1678":"code","f1050f49":"code","1728f3d6":"markdown","b412acd2":"markdown","c5663afb":"markdown","2a4f2e4f":"markdown","01fd5992":"markdown","0df3898d":"markdown","6f0b8b52":"markdown","18bd763f":"markdown","06c39e3b":"markdown","eb9b8363":"markdown","156faca0":"markdown","18839f3b":"markdown","66dbb572":"markdown","12176cbb":"markdown","b42687af":"markdown"},"source":{"7f1499b1":"from pathlib import Path\nimport pandas as pd, numpy as np\nnp.set_printoptions(suppress=True)","f3d49c51":"! ls ..\/input","7f025422":"DATA_ROOT = Path(\"..\/input\/m5-forecasting-uncertainty\")","3c1ba19b":"class M5Config:\n    def __init__(self,cat_cols=None,sales_path=None, add_fake_categories=True, start=1,\n                     end = 1913, days=None, evaluation=False,\n                read_calendar=True, read_sales=True, read_prices=True, read_sample_submission=False):\n        self.cat_cols = [\"id\", \"cat_id\", \"state_id\", \"dept_id\", \"store_id\", \"item_id\"] if cat_cols is None else cat_cols\n        self.col_groups = [\n                ('Total', 'X'),\n                ('cat_id', 'X'),\n                ('state_id', 'X'),\n                ('dept_id', 'X'),\n                ('store_id', 'X'),\n                ('item_id', 'X'),\n                ('state_id', 'cat_id'),\n                ('state_id', 'dept_id'),\n                ('store_id', 'cat_id'),\n                ('store_id', 'dept_id'),\n                ('state_id','item_id'),\n                ('item_id', 'store_id')]\n\n        self.evaluation = False\n        self.suffix = \"evaluation\" if self.evaluation else \"validation\"\n\n        self.sales_path = DATA_ROOT\/f'sales_train_{self.suffix}.csv' if sales_path is None else sales_path\n        self.calendar_path = DATA_ROOT\/\"calendar.csv\"\n        self.prices_path = DATA_ROOT\/\"sell_prices.csv\"\n        self.sample_submission_path = DATA_ROOT\/\"sample_submission.csv\"\n\n        self.add_fake_categories = add_fake_categories\n\n        self.start = start\n        self.end = end\n\n        \n        if days is None:\n            self.set_days()\n        else:\n            self.days = days\n\n        assert end > 28\n        self.set_weight_days()\n        \n        self.read_calendar = read_calendar\n        self.read_sales = read_sales\n        self.read_prices = read_prices\n        self.read_sample_submission = False\n        \n    def set_days(self):\n        self.days = [f\"d_{i}\" for i in range(self.start,self.end+1)]\n    \n    def set_weight_days(self):\n        self.weight_days = [f\"d_{i}\" for i in range(self.end-27, self.end+1)]","7ea9ca67":"class M5Data:\n    def __init__(self, config=None):\n        self.config = config if config is not None else M5Config()\n        \n        self.cal = self.read(self.config.calendar_path) if self.config.read_calendar else None\n        self.sales = self.read(self.config.sales_path, usecols = self.config.cat_cols\n                               + self.config.days) if self.config.read_sales else None\n        self.prices = self.read(self.config.prices_path) if self.config.read_prices else None\n        self.sample_submission = self.read(self.config.sample_submission_path)\\\n                                            if self.config.read_sample_submission else None\n        \n        if self.config.add_fake_categories:\n            self.add_fake_categories()\n            \n        \n    def read(self, path, usecols=None):\n        return pd.read_csv(path.as_posix(), usecols=usecols)\n    \n    \n    def add_fake_categories(self):\n        self.sales[\"Total\"] = \"Total\"\n        self.sales[\"X\"] = \"X\"\n        \n        \n        \n    \n    def _to_42840(self, group):\n        assert group in self.config.col_groups\n#         print(group, self.sales.columns)\n        group = list(group)\n        df = self.sales[group+self.config.days].groupby(group)[self.config.days].sum()\n        df.reset_index(inplace=True)\n        df.rename(columns={group[0]:\"level1_val\", group[1]:\"level2_val\"}, inplace=True)\n        df[\"level1_name\"] = group[0]\n        df[\"level2_name\"] = group[1]\n        df[\"group_id\"] = self.config.col_groups.index(tuple(group))\n        df = df[[\"group_id\", \"level1_name\", \"level2_name\", \"level1_val\", \"level2_val\"]+self.config.days]\n        \n        return df\n    \n    def to_42840(self):\n        \n        df = pd.concat([self._to_42840(group) for group in self.config.col_groups], axis=0,sort=False)\n        df.sort_values([ \"level1_val\", \"level2_val\"], inplace=True)\n        df.reset_index(drop=True, inplace=True)\n        \n        return df","dc082f8f":"%%time\n\ndata = M5Data()","74a6c2b2":"data.sales.head()","371b45e8":"%%time\n\ndata._to_42840((\"item_id\", \"store_id\")).head()","880dee7f":"class WeightAndScaleComputer:\n    def __init__(self, config=None, data=None):\n        self.config = config if config is not None else M5Config()\n        self.data = data if data is not None else M5Data(config=self.config)\n        \n        self.df_42840 = self.data.to_42840()\n        \n    def get_prices(self):\n        prices = self.data.prices.copy()\n        cal = self.data.cal\n\n        prices[\"id\"] = [\"{}_{}_{}\".format(item_id, store_id, self.config.suffix) \n                            for item_id, store_id in zip(prices.item_id, prices.store_id)] \n        \n        day_count = cal[\"d\"].str.replace(\"d_\", \"\").astype(int)\n\n        prices = prices[[\"wm_yr_wk\", \"id\", \"sell_price\"]].merge(\n                cal.loc[(day_count>= self.config.end-27) & (day_count<= self.config.end), [\"wm_yr_wk\", \"d\"]],\n                                                on = [\"wm_yr_wk\"])\n        prices = prices.set_index([\"id\", \"d\"]).sell_price.fillna(0.).unstack().fillna(0.)\n        return prices\n        \n    \n    def get_weights(self):\n        \n        # Backup old data\n        sales_backup = self.data.sales\n        df_42840_backup  = self.df_42840\n\n        data = self.data\n        sales = data.sales\n        \n        sales.sort_values(\"id\",inplace=True)\n        sales.reset_index(inplace=True, drop=True)\n        \n        prices = self.get_prices()\n        prices.sort_index(inplace=True)\n        prices.reset_index(inplace=True, drop=True)\n        prices = prices[self.config.weight_days]\n        \n        for i,col in enumerate(self.config.weight_days):\n            sales[col] = sales[col]*prices[col].values\n            \n        data.sales = sales\n        df_42840 = data.to_42840()\n        \n        df_42840[\"turnover\"] = df_42840[self.config.weight_days].sum(axis=1)\n        df_42840[\"level_turnover\"] = df_42840.groupby([\"level1_name\",\"level2_name\"]).turnover.transform(\"sum\")\n        df = df_42840[[\"group_id\", \"level1_name\", \"level2_name\", \"level1_val\", \"level2_val\"]].copy()\n        df[\"weights\"] = df_42840[\"turnover\"]\/df_42840[\"level_turnover\"].values\n        \n        df.sort_values([\"level1_val\", \"level2_val\"], inplace=True)\n        df.reset_index(drop=True,inplace=True)\n        \n        # Restore old data\n        self.data.sales = sales_backup\n        self.df_42840 = df_42840_backup\n        \n        return df\n    \n    \n    def get_scales(self, kind=\"mae\"): # kind in ['mae', 'mse']\n        assert kind in ['mae', 'mse']\n        \n        df = self.df_42840[self.config.days].values\n        \n        diff = (np.abs if kind == \"mae\" else np.square )(df[:, 1:] - df[:, :-1]) \n        \n        is_start = df[:, :-1].cumsum(1) >= 1\n        \n        diff *= is_start\n        \n        starts = is_start.argmax(1)\n        size = df.shape[1] - starts - 1\n        \n        scales = diff.sum(1)\/size\n        \n        df = self.df_42840[[\"level1_val\", \"level2_val\"]].copy()\n        df[\"scales\"] = scales\n        \n        df.sort_values([\"level1_val\", \"level2_val\"], inplace=True)\n        df.reset_index(drop=True,inplace=True)\n        \n        return df","0f986577":"class WeightedScaledPinballLoss:\n        \"\"\"A fast routine for the  Weighted Scaled Pinball Loss (WSPL).\n\nThis might be slow (one to two minutes) at initialisation in order to initiate all the routines required \nto accelerate on-the-fly WSPL computation.\n\"\"\"\n        \n        def __init__(self, scales, weights, data=None, qs=None):\n            self.set_weights_and_scales(weights = weights, scales =  scales)\n            self._scales = self.weights_and_scales[\"scales\"].values\n            self._weights = self.weights_and_scales[\"weights\"].values\n            \n            self.qs = np.array([0.005, 0.025, 0.165, 0.25 , 0.5  , 0.75 , 0.835, 0.975, 0.995])\\\n                                                        if qs is None else np.array(qs)\n            \n            if data is None:\n                config = M5Config(sales_path=DATA_ROOT\/\"sales_train_evaluation.csv\", start = 1914, end=1914+27,\n                                        read_calendar=False,read_prices=False,read_sample_submission=False)\n                data = M5Data(config)\n                data.sales[\"id\"] = data.sales[\"id\"].str.replace(\"evaluation\", \"validation\")\n                \n            self.data = data\n            \n            df_42840 = data.to_42840()\n\n            df_42840[\"id_no_q\"] = df_42840[\"level1_val\"].str.cat(df_42840[\"level2_val\"], \n                                                                 sep=\"_\").str.cat([\"_validation\"]*len(df_42840))\n            df_42840 = df_42840.sort_values(\"id_no_q\").reset_index(drop=True)\n\n            self.df_42840 = df_42840[self.data.config.days].values\n            \n            self.submission_config = M5Config(cat_cols = [\"id\"], days = [f\"F{i}\" for  i in range(1,29)],\n                                read_calendar=False,read_prices=False,read_sample_submission=False)\n            \n            \n        def set_weights_and_scales(self, weights, scales):\n            weights = weights.copy()\n            weights = weights.merge(scales,on=[\"level1_val\", \"level2_val\"])\n            weights[\"id_no_q\"] = weights[\"level1_val\"].str.cat(weights[\"level2_val\"], \n                                                               sep=\"_\").str.cat([\"_validation\"]*len(weights))\n            weights = weights.sort_values(\"id_no_q\").reset_index(drop=True)\n            \n            self.weights_and_scales = weights\n                \n            \n        def score(self, y_pred ):\n            \"\"\"Compute the WSPL.\n            \n            Parameters:\n            -----------\n            y_true: pd.DataFrame, Path,str-path \n                pd.DataFram or path to a pd.DataFrame that consists of daily 42840x28 evaluation data.\n                This dataframe must includes the 'id' column. \n                \n            y_pred: pd.DataFrame, Path,str-path \n                pd.DataFram or path to a pd.DataFrame that consists of daily 4(2840x9)x28 prediction data.\n                This dataframe must includes the 'id' column.\n            \"\"\"\n            y_true = self.df_42840\n            y_pred = self.get_sub_data(y_pred)\n            assert (9, *y_true.shape) == y_pred.shape\n            \n            diff =   y_true[None] - y_pred\n            diff = np.maximum(diff*self.qs[:, None, None], diff*(self.qs[:, None, None]-1)).mean(2)\n            wspl = np.sum(diff*self._weights[None]\/self._scales[None])\/(9*12)\n            \n            return wspl\n        \n        def get_sub_data(self, data):\n            \n            sales = data\n        \n            if isinstance(sales, (str, Path)):\n                sales = pd.read_csv(Path(sales).as_posix())\n            else:\n                assert isinstance(sales, pd.DataFrame)\n                \n            sales = sales[sales[\"id\"].str.endswith(\"validation\")].copy()\n            \n            if not \"_q_\" in sales.columns:\n                sales[\"_q_\"] = sales[\"id\"].str.extract(r\"_(\\d+\\.\\d+)_\").astype(float)\n            if not \"id_no_q\" in sales.columns:\n                sales[\"id_no_q\"] = sales[\"id\"].str.replace(r\"_(\\d+\\.\\d+)_\", \"_\")\n\n            sales.sort_values([\"_q_\", \"id_no_q\"], inplace=True)\n            \n            sales = sales[[f\"F{i}\" for i in range(1, 29)]].values.reshape((9, 42840, 28 ))\n\n            return sales","ef5ea7bd":"%%time\n\nwsc = WeightAndScaleComputer()\nwspl_scales = wsc.get_scales(kind=\"mae\")\nweights = wsc.get_weights()\nwspl_scales.shape, weights.shape","686315ff":"weights[(weights.level1_name==\"state_id\")&(weights.level2_name==\"cat_id\")]","34a264dd":"WSPL =  WeightedScaledPinballLoss(scales = wspl_scales, weights=weights )","94cbf2fe":"! ls ..\/input\/m5-uncertainty-best-public-lbs","a43134c4":"kkiller_017921 = pd.read_csv(\"..\/input\/m5-uncertainty-best-public-lbs\/Kkiller_FromPointToUncertainty_017921.csv\")","cb07c76b":"%%time\n\nWSPL.score(kkiller_017921)","f643b54c":"ulrich_012565 = pd.read_csv(\"..\/input\/m5-uncertainty-best-public-lbs\/Ulrich_QuantileRegressionWithKeras_012565.csv\")","60a76fe3":"%%time\n\nWSPL.score(ulrich_012565)","c93f1678":"krisztiansz_015905 = pd.read_csv(\"..\/input\/m5-uncertainty-best-public-lbs\/\"+\n                                 \"KrisztianSz_PointToUncertaintyDifferentRangesPerLevel_015905.csv\")","f1050f49":"%%time\n\nWSPL.score(krisztiansz_015905)","1728f3d6":"<h1 style=\"color:pink;text-align:center\">Good Luck Folks<\/h1>","b412acd2":"<h3 style=\"color: red;\">If you find this work helpful, please don't forget upvoting in order to get me motivated in sharing my hard work<h3\/>","c5663afb":"# Let's configure everything here","2a4f2e4f":"# Weighted Scaled Pinball Loss","01fd5992":"### My older submission (public kernel)","0df3898d":"# Some tests for our loss API","6f0b8b52":"<p style=\"text-align:center;size:1.2rem;weight:200\">Thank you for reading my Kernel through the end!<\/p>","18bd763f":"###  Ulrich's one","06c39e3b":"### Krisztiansz's one","eb9b8363":"# And what if we make a generic class to laod the data when needed ?","156faca0":"# Notes\n* Even if I put a lot of efforts making the evaluation function faster, there could still some room for improvements so your contribs' would be highly appreciated\n* There still some tiny differences between our API results and the Kaggle's one. I'm on it !\n* I'm hard-working currently to port this implementation on Pytorch\/Tensorflow in order to be able to use it directly as loss function. Stay tuned !","18839f3b":"### You can give a try to the M5Data API here :","66dbb572":"### Let's test it !\nNow we can give a real-world test to our API by using [this best uncertainty public kernels dataset](https:\/\/www.kaggle.com\/kneroma\/m5-uncertainty-best-public-lbs). Please upvote the dataset to make it more visible for all.","12176cbb":"# Powerful API for Weights & Scales computation","b42687af":"The M5 competition loss is a little ugly, epecially when it comes to the Uncertainty's one with it's WSPL! So far, there is no, if not little, public implementation of that *beast* (I still talking about the WSPL and not *the american  crime drama series*).\n\nAs I was saying it on [M5  Accuracy](https:\/\/www.kaggle.com\/kneroma\/faster-loss-function-with-lb-position-500ms) my implementation is based on a deep inspection of the loss combined with an intelligent choice of pandas' components. \n\n**Too much talk, let's dive in it !**"}}