{"cell_type":{"07c452ac":"code","8be85ddd":"code","87447156":"code","29c30cef":"code","c3b455e7":"code","6b19532d":"code","c11987bf":"code","c7b700e5":"code","742be9fa":"code","c9b86b39":"code","9c5f228a":"code","b6ac8c29":"code","9ba0e57b":"code","b3726393":"code","99c76123":"code","ad2e5d67":"code","2aba2bde":"code","7a53affb":"code","57ebafab":"code","20c41c1d":"code","a83711c3":"code","3d006f04":"code","263cf1e8":"code","9614ed91":"code","3f042199":"markdown","476233e1":"markdown","422db4f0":"markdown","6cd334af":"markdown","963e1a42":"markdown","0def8dea":"markdown","5a70d61a":"markdown","da61906f":"markdown","2caf99dc":"markdown","0eb3974f":"markdown","bc4735f1":"markdown","9fad7f70":"markdown","7b188bb3":"markdown","caa4571f":"markdown","038b0756":"markdown","6b4479ce":"markdown","4375728a":"markdown","06932efd":"markdown","38859dd6":"markdown","31f033ea":"markdown"},"source":{"07c452ac":"# importing\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib as plt\nimport seaborn as sns\nimport itertools\nimport nltk \nimport string\nfrom wordcloud import WordCloud\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob,Word\nfrom collections import Counter\nimport seaborn as sns\nimport os\nimport re\nfrom tqdm.notebook import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\nimport plotly.figure_factory as ff \nfrom plotly.offline import iplot\nfrom plotly import tools\ncolors = px.colors.qualitative.Prism\npio.templates.default = \"plotly_white\"\n\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom wordcloud import WordCloud\nfrom plotly.offline import iplot\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\n\n\nimport nltk\nfrom nltk import tokenize\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\n\ndef cprint(string:str, end=\"\\n\"):\n    \"\"\"\n    A little utility function for printing and stuff\n    \"\"\"\n    _pprint(f\"[black]{string}[\/black]\", end=end)\n    \n    \n# see our files:\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#reading data\ntrain = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\ntrain_df = train\ntest_df = test","8be85ddd":"print(\"The TRAIN set\")\nprint(train.head())\nprint()\nprint(\"The TEST set\")\nprint(test.head())","87447156":"print('Rows and Columns in train dataset:', train.shape)\nprint('Rows and Columns in test dataset:', test.shape)","29c30cef":"color_map = ['#eeb977', 'lightgray']\nsns.set_palette(sns.color_palette(color_map))\nplt.figure(figsize=(8, 8), facecolor='#f6f5f5')\nplt.title(f\"Target Column Distribution\")\nsns.histplot(train['target'], stat='density',)\nsns.kdeplot(train['target'], color='black')\nplt.axvline(train['target'].mean(), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(train['target'].mean()*1.05, max_ylim*0.96, 'Mean (\u03bc): {:.2f}'.format(train['target'].mean()))\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()","c3b455e7":"color_map = ['#eeb977', 'lightgray']\nsns.set_palette(sns.color_palette(color_map))\n\nplt.figure(figsize=(8, 8), facecolor='#f6f5f5')\nplt.title(f\"Standard Error Column Distribution\")\nsns.histplot(train['standard_error'], stat='density')\nsns.kdeplot(train['standard_error'], color='black')\nplt.axvline(train['standard_error'].mean(), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(train['standard_error'].mean()*1.05, max_ylim*0.96, 'Mean (\u03bc): {:.2f}'.format(train['standard_error'].mean()))\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()","6b19532d":"train['license_type_cnt'] = train.groupby('license').transform('count')['id']\nlicense_data = train[train['license_type_cnt'] >= 10]\n\nfig, ax = plt.subplots(figsize=(12, 8), facecolor='#f6f5f5')\nsns.pointplot(\n    data=license_data,\n    x='license',\n    y='target',\n    ci='sd',\n    join=False\n)\n\nplt.show()","c11987bf":"def get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\nlemmatizer = WordNetLemmatizer()\ndef clean_text(text):\n    text = re.sub('[^A-Za-z0-9]+', ' ', text.lower())\n    words = nltk.word_tokenize(text)\n    tagged = nltk.pos_tag(words)\n    words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n    words = [word for word in words if word not in stopwords.words('english')]\n    return words\n\ndef get_ngrams(words, n):\n    return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n\ncorpus = []\nfor text, target in train[['excerpt', 'target']].itertuples(index=False):\n    sentences = []\n    for sentence in tokenize.sent_tokenize(text):\n        words = clean_text(sentence)\n        unigrams = get_ngrams(words, n=1)\n        bigrams = get_ngrams(words, n=2)\n        trigrams = get_ngrams(words, n=3)\n        sentences.append(words)\n    corpus.append({\n        'target' : target,\n        'text' : text,\n        'sentences' : sentences,\n        'unigrams' : unigrams,\n        'bigrams' : bigrams,\n        'trigrams' : trigrams,\n    })\n\ncorpus = sorted(corpus, key=lambda x: x['target'])","c7b700e5":"def plot_grams_target(gram_type):\n    gram_cnt = defaultdict(lambda: 0)\n    gram_sum = defaultdict(lambda: 0.)\n    gram_avg = {}\n\n    for datapoint in corpus:\n        for gram in datapoint[gram_type]:\n            gram_cnt[gram] += 1\n            gram_sum[gram] += datapoint['target']\n\n    for gram in gram_cnt:\n        if gram_cnt[gram] >= 5:\n            gram_avg[gram] = gram_sum[gram] \/ gram_cnt[gram]\n    \n    fig, ax = plt.subplots(1, 2, figsize=(12, 8), facecolor='#f6f5f5')\n    \n    top_lowest = sorted(gram_avg.items(), key=lambda x: x[1])[:10]\n    ngrams, avg_target = zip(*top_lowest)\n    ax[0].bar(\n        range(len(ngrams)),\n        avg_target\n    )\n\n    ax[0].set_title(f'{gram_type} with lowest readability')\n    ax[0].set_xlabel(gram_type)\n    ax[0].set_ylabel('Average readability')\n    ax[0].set_xticks(range(len(ngrams)))\n    ax[0].set_xticklabels([' '.join(x) for x in ngrams], rotation='vertical')\n    \n    top_highest = sorted(gram_avg.items(), key=lambda x: x[1])[-10:]\n    ngrams, avg_target = zip(*top_highest)\n    ax[1].bar(\n        range(len(ngrams)),\n        avg_target\n    )\n\n    ax[1].set_title(f'{gram_type} with highest readability')\n    ax[1].set_xlabel(gram_type)\n    ax[1].set_ylabel('Average readability')\n    ax[1].set_xticks(range(len(ngrams)))\n    ax[1].set_xticklabels([' '.join(x) for x in ngrams], rotation='vertical')\n\n    plt.show()","742be9fa":"plot_grams_target('unigrams')","c9b86b39":"plot_grams_target('bigrams')","9c5f228a":"top_lowest = corpus[:500]\nlowest_target_sentence_lengths = [ \\\n    np.mean([len(sentence) for sentence in datapoint['sentences']]) \\\n    for datapoint in top_lowest \\\n]\n\ntop_highest = corpus[-500:]\nhighest_target_sentence_lengths = [ \\\n    np.mean([len(sentence) for sentence in datapoint['sentences']]) \\\n    for datapoint in top_highest \\\n]\n\ntop_lowest_mean = np.mean(lowest_target_sentence_lengths)\ntop_lowest_std = np.std(lowest_target_sentence_lengths)\ntop_highest_mean = np.mean(highest_target_sentence_lengths)\ntop_highest_std = np.std(highest_target_sentence_lengths)\n\nfig, ax = plt.subplots(figsize=(8, 10), facecolor='#f6f5f5')\nax.errorbar(\n    x=[0, 1],\n    y=[top_lowest_mean, top_highest_mean],\n    yerr=[top_lowest_std, top_highest_std],\n    fmt='o'\n)\n\nax.set_title('Average sentence length and Readability')\nax.set_ylabel('Sentence length')\nax.set_xticks([0, 1])\nax.set_xticklabels(['Top lowest readability', 'Top highest readability'])\n\nplt.show()","b6ac8c29":"!pip install wordfreq\n\nfrom wordfreq import word_frequency\nlowest_target_word_freq = [\n    [word_frequency(word[0], 'en') for word in datapoint['unigrams']]\n    for datapoint in top_lowest\n]\nhighest_target_word_freq = [\n    [word_frequency(word[0], 'en') for word in datapoint['unigrams']]\n    for datapoint in top_highest\n]","9ba0e57b":"lowest_min_freq = [np.min(datapoint) for datapoint in lowest_target_word_freq]\nhighest_min_freq = [np.min(datapoint) for datapoint in highest_target_word_freq]\n\nfig, ax = plt.subplots(figsize=(6, 8), facecolor='#f6f5f5')\nax.errorbar(\n    x=[0, 1],\n    y=[np.mean(lowest_min_freq), np.mean(highest_min_freq)],\n    yerr=[np.std(lowest_min_freq), np.std(highest_min_freq)],\n    fmt='o'\n)\n\nax.set_ylabel('Min word frequency in general English')\nax.set_xticks([0, 1])\nax.set_xticklabels(['Top lowest readability', 'Top highest readability'])\n\nplt.show()","b3726393":"char_freq_count = train['excerpt'].str.len()\n\nplt.figure(figsize=(8, 8), facecolor='#f6f5f5')\nplt.title(f\"Character Frequency Count\")\nsns.histplot(char_freq_count, stat='density')\nsns.kdeplot(char_freq_count, color='purple')\nplt.axvline(np.mean(char_freq_count), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(np.mean(char_freq_count)*0.64, max_ylim*0.96, 'Mean (\u03bc): {:.2f}'.format(np.mean(char_freq_count)))\nplt.xlabel(\"Count\")\nplt.ylabel(\"Density\")\nplt.show()","99c76123":"word_count = train['excerpt'].str.split().map(lambda x: len(x))\n\nplt.figure(figsize=(8, 8), facecolor='#f6f5f5')\nplt.title(f\"Word Count Distribution\")\nsns.histplot(word_count, stat='density')\nsns.kdeplot(word_count, color='purple')\nplt.axvline(np.mean(word_count), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(np.mean(word_count)*0.86, max_ylim*0.93, 'Mean (\u03bc): {:.2f}'.format(np.mean(word_count)))\nplt.xlabel(\"Word Count\")\nplt.ylabel(\"Density\")\nplt.show()","ad2e5d67":"unq_word_count = train['excerpt'].apply(lambda x: len(set(str(x).split()))).to_list()\n\nfig = ff.create_distplot([unq_word_count], ['Excerpt Text'],colors = ['#eeb977', 'lightgray'])\nfig.update_layout(title_text=\"Unique Word Count Distribution\")\niplot(fig)","2aba2bde":"# Generate WordCloud\nwords = \" \".join(train['excerpt'].tolist())\nwc = WordCloud(width = 5000, height = 4000, background_color = '#f6f5f5', min_font_size = 10).generate(words)\n\n# Plot it\nplt.figure(figsize = (12, 12), facecolor = 'k', edgecolor = 'k' ) \nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0)\n\nplt.show()","7a53affb":"train['excerpt_length'] = train['excerpt'].apply(lambda x: len(x))\ncol = 'excerpt_length'\ndisplay(pd.DataFrame(train[col].describe()))\nsns.histplot(train[col])\nplt.show()\n\n\nprint('correlation:')\ndisplay(train[['target', 'excerpt_length']].corr())\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=train['excerpt_length'], y=train['target'])\nsns.regplot(x=train['excerpt_length'], y=train['target'], scatter=False, ci=None, color='tab:blue')\nplt.ylabel('reading ease')\nplt.show()","57ebafab":"train['word_count'] = train['excerpt'].apply(lambda x: len(x.split()))\ncol = 'word_count'\ndisplay(pd.DataFrame(train[col].describe()))\nsns.histplot(train[col])\nplt.show()\nprint('correlation:')\ndisplay(train[['target', 'word_count']].corr())\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=train['word_count'], y=train['target'])\nplt.ylabel('reading ease')\nplt.show()","20c41c1d":"# Split the data roughly\ntraining_data = train\ntraining_file = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_file = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\ndata = training_file[['excerpt', 'target']]\ndata = data.sample(frac=1).reset_index(drop=True)\nexcerpt, targets = training_file['excerpt'].values, training_file['target'].values\n\nt_X, v_X = excerpt[:2750], excerpt[2750:]\nt_Y, v_Y = targets[:2750], targets[2750:]\n\nprint(t_X.shape, v_X.shape)\nprint(t_Y.shape, v_Y.shape)","a83711c3":"# Make an Sklearn pipeline for this Ridge Regression\nbackbone_ridge = Ridge(fit_intercept=True, normalize=False)\npipeline_ridge = make_pipeline(\n    TfidfVectorizer(binary=True, ngram_range=(1, 1)),\n    backbone_ridge\n)\n\n# Do training\npipeline_ridge.fit(t_X, t_Y)\n\n# Evaluate the performance on validation set\npreds = pipeline_ridge.predict(v_X)\nmse_loss = mean_squared_error(v_Y, preds)\n\nprint(f\"MSE Loss using Ridge and TfIdfVectorizer: {mse_loss}\")","3d006f04":"# Make an Sklearn pipeline for this Linear Regression\nbackbone_linear = LinearRegression(fit_intercept=True, normalize=False)\npipeline_linear = make_pipeline(\n    TfidfVectorizer(binary=True, ngram_range=(1, 1)),\n    backbone_linear\n)\n\n# Do training\npipeline_linear.fit(t_X, t_Y)\n\n# Evaluate the performance on validation set\npreds = pipeline_linear.predict(v_X)\nmse_loss = mean_squared_error(v_Y, preds)\n\nprint(f\"MSE Loss using Linear Regression and TfIdfVectorizer: {mse_loss}\")","263cf1e8":"# Weights for blending later\nlin_wgt = 0.2\nrig_wgt = 0.8","9614ed91":"# Get the testing file\ntest = test_file[['id', 'excerpt']]\ntest_ids = test['id'].tolist()\ntest_text = test['excerpt'].values\n\n# Do Predictions on testing set\ntest_preds_ridge = pipeline_ridge.predict(test_text)\ntest_preds_linear = pipeline_linear.predict(test_text)\n\n# Form a submissions file and save it\nsubmission = pd.DataFrame()\nsubmission['id'] = test_ids\nsubmission['target'] = (test_preds_ridge + test_preds_linear) \/ 2\nsubmission.to_csv(\"submission.csv\", index=None)\nprint('Predictions are done!')","3f042199":"### But how do we know we are doing it correctly or well if we win??\n\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\n![](https:\/\/community.qlik.com\/legacyfs\/online\/128958_2016-06-23%2013_45_36-Root%20Mean%20Squared%20Error%20_%20Kaggle.png)\n","476233e1":"### What is this Competition about? \n\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQnDADH9cHqgFNYDDuBNJSLL2Izyx3l35B00w&usqp=CAU)\n\n\n**The curx of this competition if to help students!!**\n\n- In this competition, we are required to build an algorithm to rate the complexity of reading passages for grade 3-12 classroom use.\n\n- To accomplish this, we'll have to pair our machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. \n\n- Winning models will be sure to incorporate text cohesion and semantics.\n\n- If successfull, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.","422db4f0":"# Basic Introduction","6cd334af":"### Observations:\n- Hard-to-read texts often contain some less-usual words.\n- Texts with shorter sentence lengths are often easier to read.","963e1a42":"### NGRAMS","0def8dea":"### What data do we have?\n\n\n\n#### train.csv \/ test.csv - the training and testing set\n\n- id - unique ID for excerpt\n- url_legal - URL of source\n- license - license of source material\n- excerpt - text to predict reading ease of\n- target - reading ease\n- standard_error - measure of spread of scores among multiple raters for each excerpt\n\n\n **Note: url_legal, license and standard error are blank in the test set.**","5a70d61a":"### And what is our task?\n\nIn technical terms,\n\nGiven a ***training.csv*** file in which we will have (among other) 2 columns: excerpt and target, we will have to train Machine Learning model(s) that can approximate the relationship between excerpt and the target.\n\nIn simple words, we will have to train a Model which can predict the target value given a text excerpt.\n\nThis can be formulated as a **Regression problem** with text","da61906f":"## License and Target","2caf99dc":"### Observations : \n\n- In general, the standard error is lowest when the readability is around -1. It tends to get higher for more extreme values of readability.\n- There is one data point with 0 readability and 0 standard error.\n","0eb3974f":"### Observations\n\n- Different licenses have different average readability, but the standard deviation is quite high.\n\nNote that in test data, the license field is always blank.","bc4735f1":"# EDA","9fad7f70":"### Word Count Distribution","7b188bb3":"Hi there! \n\nThis is an attempt at making a detailed and well thought out kernel, hope you gain some insights from it and find it useful! Do upvote and share it if you like it! :)\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSjSMEM7M4zB_VH6H6ny0nwI677pU2BbVh6wA&usqp=CAU)\n\nThis kernel has covered 4 topics:\n\n- Basic Introduction\n- EDA\n- Feature Engineering\n- Model Building\n\n\nThe name of the competition is **CommonLit Readability Prize**\n\nThe brief description line is **Rate the complexity of literary passages for grades 3-12 classroom use**","caa4571f":"**Checking out the first five lines for test and train sets**\n","038b0756":"# Modelling\nLet's try some basic modelling and then make a submission using that!","6b4479ce":"### Character Frequency","4375728a":"**The first thing we are going to check is the distribution of the target feature. It's important to know if the distribution.**","06932efd":"### Target and sentence length","38859dd6":"### **Notebooks taken help from!**\n\nhttps:\/\/www.kaggle.com\/heyytanay\/commonlit-eda-understanding-the-competition\n\n\nhttps:\/\/www.kaggle.com\/tungmphung\/commonlit-readability-eda\/notebook#Target-and-sentence-length\n\n\n### **If there are any suggesion for the notebook please comment, that would be helpful. Also please upvote if you liked it! Thank you!!**\n\n**Some of my other works:**\n\n**[TPS-APR-21 EDA AND MODEL](https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model\/data)**\n\n**[Heart Attacks Extensive EDA and Visualizations](https:\/\/www.kaggle.com\/udbhavpangotra\/heart-attacks-extensive-eda-and-visualizations)**\n\n\n**[WHAT ARE PEOPLE WATCHING IN GREAT BIRTAIN](https:\/\/www.kaggle.com\/udbhavpangotra\/what-do-people-use-youtube-for-in-great-britain)**\n\n\n\n### Do **drop a comment AND follow me** for more content!!! ","31f033ea":"**Number of Rows and Columns**"}}