{"cell_type":{"734a8caf":"code","cad2ea98":"code","f33f3993":"code","ac42d19a":"code","32307b45":"code","03843128":"code","d569afca":"code","ef12b57c":"code","cfc97242":"code","09e936b1":"code","4663ecf4":"code","f85e4e06":"code","fc220be8":"code","42074230":"code","f4a4cdf5":"code","b6d5bdd2":"code","022e3ae2":"code","e9a6a03f":"code","9c23e4d7":"code","d8f1fbfb":"code","49b3d5a7":"code","9c46b31d":"code","be16c253":"code","1e0208a2":"code","f657686c":"code","e5753b13":"code","5f350257":"code","43aee0e5":"code","dc57fd70":"code","483cd842":"code","073ad720":"code","b8a9d7f5":"code","d82d4277":"code","4a913b64":"code","e927ba38":"code","489355c0":"code","061a998b":"code","e353ba0e":"code","9740d1e7":"code","ed63aa67":"code","591c4b45":"code","d71318b9":"code","22352dc2":"code","c865d145":"code","be958347":"markdown","5873ffdd":"markdown","9601d81c":"markdown","323e406c":"markdown","1a15e333":"markdown"},"source":{"734a8caf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cad2ea98":"# importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport spacy\nfrom spacy import displacy\nfrom spacy.util import minibatch, compounding","f33f3993":"# importing dataset\ndf = pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding = \"Latin-1\")\ndf.head()","ac42d19a":"# shape of dataset\ndf.shape","32307b45":"# checking for null values\ndf.isna().sum()","03843128":"# Datatypes of each column\ndf.info()","d569afca":"df.head().T","ef12b57c":"df.Sentiment.unique()","cfc97242":"df.replace(to_replace=\"Extremely Negative\", value=\"Negative\", inplace=True)\ndf.replace(to_replace=\"Extremely Positive\", value=\"Positive\", inplace=True)\ndf.replace(to_replace=\"Neutral\", value=\"Negative\", inplace=True)","09e936b1":"df.Sentiment.unique()","4663ecf4":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf.Sentiment = le.fit_transform(df.Sentiment)","f85e4e06":"df.head()","fc220be8":"df.Sentiment.value_counts()","42074230":"sns.countplot(df.Sentiment)","f4a4cdf5":"df1 = df[df.Sentiment==1][:18000]\ndf2 = df[df.Sentiment==0][:18000]","b6d5bdd2":"df_train = df1.append(df2)\ndf_train.head()","022e3ae2":"df_train = df_train[[\"OriginalTweet\",\"Sentiment\"]]\ndf_train.head()","e9a6a03f":"df_train.shape","9c23e4d7":"sns.countplot(df_train.Sentiment)","d8f1fbfb":"# Tokenization\nspacy_tok = spacy.load('en_core_web_sm')\nsample_tweet = df_train.OriginalTweet[23]\nsample_tweet","49b3d5a7":"parsed_tweet = spacy_tok(sample_tweet)\nparsed_tweet","9c46b31d":"!wget https:\/\/raw.githubusercontent.com\/tylerneylon\/explacy\/master\/explacy.py","be16c253":"import explacy\nexplacy.print_parse_info(spacy_tok,'Covid-19 has various Symptoms') # text for example","1e0208a2":"explacy.print_parse_info(spacy_tok,df_train.OriginalTweet[23])","f657686c":"tokenized_text = pd.DataFrame()\n\nfor i, token in enumerate(parsed_tweet):\n    tokenized_text.loc[i, 'text'] = token.text\n    tokenized_text.loc[i, 'lemma'] = token.lemma_,\n    tokenized_text.loc[i, 'pos'] = token.pos_\n    tokenized_text.loc[i, 'tag'] = token.tag_\n    tokenized_text.loc[i, 'dep'] = token.dep_\n    tokenized_text.loc[i, 'shape'] = token.shape_\n    tokenized_text.loc[i, 'is_alpha'] = token.is_alpha\n    tokenized_text.loc[i, 'is_stop'] = token.is_stop\n    tokenized_text.loc[i, 'is_punctuation'] = token.is_punct\n\ntokenized_text[:20]","e5753b13":"spacy.explain('GPE')","5f350257":"sentence_spans = list(parsed_tweet)\nsentence_spans","43aee0e5":"displacy.render(parsed_tweet, style='dep',jupyter=True, options={\"distance\":140})","dc57fd70":"!pip install scattertext\nimport scattertext as st\nnlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])","483cd842":"df_train.head()","073ad720":"df_train['parsed'] = df_train.OriginalTweet.apply(nlp)\ncorpus = st.CorpusFromParsedDocuments(df_train,category_col=\"Sentiment\", parsed_col=\"parsed\").build()","b8a9d7f5":"df_train.head()","d82d4277":"df_train['tuples'] = df_train.apply(lambda row: (row[\"OriginalTweet\"], row[\"Sentiment\"]), axis=1)\ntrain = df_train[\"tuples\"].tolist()\ntrain[:6]","4a913b64":"#functions from spaCy documentation\ndef load_data(limit=0, split=0.8):\n    train_data = train\n    np.random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    texts, labels = zip(*train_data)\n    cats = [{'POSITIVE': bool(y)} for y in labels]\n    split = int(len(train_data) * split)\n    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n\ndef evaluate(tokenizer, textcat, texts, cats):\n    docs = (tokenizer(text) for text in texts)\n    tp = 1e-8  # True positives\n    fp = 1e-8  # False positives\n    fn = 1e-8  # False negatives\n    tn = 1e-8  # True negatives\n    for i, doc in enumerate(textcat.pipe(docs)):\n        gold = cats[i]\n        for label, score in doc.cats.items():\n            if label not in gold:\n                continue\n            if score >= 0.5 and gold[label] >= 0.5:\n                tp += 1.\n            elif score >= 0.5 and gold[label] < 0.5:\n                fp += 1.\n            elif score < 0.5 and gold[label] < 0.5:\n                tn += 1\n            elif score < 0.5 and gold[label] >= 0.5:\n                fn += 1\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    f_score = 2 * (precision * recall) \/ (precision + recall)\n    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n\n#(\"Number of texts to train from\",\"t\" , int)\nn_texts=30000\n#You can increase texts count if you have more computational power.\n\n#(\"Number of training iterations\", \"n\", int))\nn_iter=10","e927ba38":"nlp = spacy.load('en_core_web_sm')","489355c0":"# add the text classifier to the pipeline if it doesn't exist\n# nlp.create_pipe works for built-ins that are registered with spaCy\nif 'textcat' not in nlp.pipe_names:\n    textcat = nlp.create_pipe('textcat')\n    nlp.add_pipe(textcat, last=True)\n# otherwise, get it, so we can add labels to it\nelse:\n    textcat = nlp.get_pipe('textcat')\n\n# add label to text classifier\ntextcat.add_label('POSITIVE')\n\n# load the dataset\nprint(\"Loading Covid Tweets data...\")\n(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\nprint(\"Using {} examples ({} training, {} evaluation)\"\n      .format(n_texts, len(train_texts), len(dev_texts)))\ntrain_data = list(zip(train_texts,\n                      [{'cats': cats} for cats in train_cats]))","061a998b":"# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\nwith nlp.disable_pipes(*other_pipes):  # only train textcat\n    optimizer = nlp.begin_training()\n    print(\"Training the model...\")\n    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n    for i in range(n_iter):\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n                       losses=losses)\n        with textcat.model.use_params(optimizer.averages):\n            # evaluate on the dev data split off in load_data()\n            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n              .format(losses['textcat'], scores['textcat_p'],\n                      scores['textcat_r'], scores['textcat_f']))","e353ba0e":"test_text1 = \"Mercedes is going to launch its new Car this weekend.\"\ntest_text2 = \"Coronavirus is grown to mutate itself.\"\ndoc = nlp(test_text1)\ntest_text1, doc.cats","9740d1e7":"df_train[\"OriginalTweet\"][2900]","ed63aa67":"doc3 = nlp(df_train[\"OriginalTweet\"][2900])","591c4b45":"df_train[\"OriginalTweet\"][2900], doc3.cats","d71318b9":"df_train[\"OriginalTweet\"][26770]","22352dc2":"doc4 = nlp(df_train[\"OriginalTweet\"][26770])\ndf_train[\"OriginalTweet\"][26770], doc4.cats","c865d145":"doc5 = nlp(df_train[\"OriginalTweet\"][12500])\ndf_train[\"OriginalTweet\"][12500], doc5.cats","be958347":"Author: Purvit Vashishtha","5873ffdd":"**Sentence Boundary Detection**:\n* Figuring out where sentences start and ends is important in NLP.","9601d81c":" **Dependency Parsing**:\n* Syntactic Parsing or Dependency parsing is process of identifying sentences and assigning a syntactic structure to it. As in subject combined with object makes a sentence. Spacy provides a sparse tree which can be used to generate this structure.","323e406c":"* Now, we can apply this model to our Test dataset and get respective results whether tweets are related to coronavirus or not.","1a15e333":"**SpaCy Text Categorizer**:\n* We will train convolutional neural network text classifier on our Coronavirus Tweets using spaCy's new TextCategorizer component.\n* SpaCy provides classification model with multiple labels,non_mutually exclusive labels.The TextCategorizer uses its own CNN to balance weights and other pipeline components."}}