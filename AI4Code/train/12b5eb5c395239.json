{"cell_type":{"0c1344ee":"code","d09c5224":"code","3fb18a3c":"code","1caf6abb":"code","2f8699fd":"code","8dab9431":"code","e522f2a4":"code","735e35c0":"code","25d5fcc0":"code","0ba0ef42":"code","6b38f8c5":"code","51384dda":"code","5522824a":"code","c1cdb86d":"code","27f97545":"markdown","4f391dc1":"markdown","8ce17381":"markdown","d10b09c1":"markdown","43a622a3":"markdown"},"source":{"0c1344ee":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n\nfrom sklearn.utils import shuffle\n\nimport re\nimport nltk\nnltk.download('stopwords')\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import LancasterStemmer\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport re\nimport time\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport string\nimport itertools \nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation","d09c5224":"data_cols = ['target', 'ids', 'date', 'flag' ,'user', 'text']\ndata = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header=None, encoding='ISO-8859-1', names=data_cols)\ndata.head()","3fb18a3c":"data.iloc[-1]","1caf6abb":"df = shuffle(data, random_state=45)\ndata = df[1:600000]\ndata['target'].value_counts()","2f8699fd":"def cleaner(text):\n    txt = re.sub('[^a-zA-Z]',' ',text) \n    txt = txt.lower()\n    txt = txt.split()\n    ps = LancasterStemmer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    txt = [ps.stem(word) for word in txt if not word in set(all_stopwords)]\n    return ' '.join(txt)","8dab9431":"data['text'] = data['text'].apply(lambda x: cleaner(x))","e522f2a4":"y = data['target']\nle = LabelEncoder()\ny = le.fit_transform(y)","735e35c0":"X_train, X_test, y_train, y_test = train_test_split(data['text'], y, test_size = 0.20, random_state = 45)","25d5fcc0":"tfidf = TfidfVectorizer(max_features = 600)\nX_train = tfidf.fit_transform(X_train).toarray() \nX_test = tfidf.transform(X_test).toarray()","0ba0ef42":"print(X_train.shape)\nprint(X_test.shape)","6b38f8c5":"classifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train) \n\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","51384dda":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred_ds = clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_ds))\nprint(confusion_matrix(y_test, y_pred_ds))\nprint(classification_report(y_test, y_pred_ds))","5522824a":"RF = RandomForestClassifier()\nRF.fit(X_train, y_train)\ny_pred_rf = RF.predict(X_test)\nprint(accuracy_score(y_test, y_pred_rf))\nprint(confusion_matrix(y_test, y_pred_rf))\nprint(classification_report(y_test, y_pred_rf))","c1cdb86d":"sgd =  SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\nsgd.fit(X_train, y_train)\n\n#%%time\n\ny_pred = sgd.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred))","27f97545":"# ***Linear Support Vector Machine***\n","4f391dc1":"# ***Logistics Regression***","8ce17381":"# ***Random Forest***","d10b09c1":"# ***Decision Tree***","43a622a3":"# ***Tokenizing***"}}