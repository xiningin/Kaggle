{"cell_type":{"09e6001c":"code","adb389bf":"code","588b2de6":"code","0594991a":"code","b2bf5356":"code","7fdd15c2":"code","06701c56":"code","bba5699f":"code","13b6e553":"code","94ee6c4f":"code","fcaf0256":"code","e6c22175":"code","c5e8279c":"code","d35183ae":"code","b937bff4":"code","fb23d034":"code","f026f007":"markdown","d0eef10d":"markdown","8e539ba3":"markdown","5b29b4ac":"markdown","ef63f9f7":"markdown"},"source":{"09e6001c":"!pip install vizard","adb389bf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tqdm.auto import tqdm\n\nimport vizard\n\npd.options.display.max_colwidth = None\nsns.set_style('darkgrid')","588b2de6":"!unzip ..\/input\/home-depot-product-search-relevance\/product_descriptions.csv.zip\n!unzip ..\/input\/home-depot-product-search-relevance\/train.csv.zip\n!unzip ..\/input\/home-depot-product-search-relevance\/test.csv.zip","0594991a":"product_description = pd.read_csv('product_descriptions.csv')\nprint(product_description.shape)\nproduct_description.head()","b2bf5356":"dtrain = pd.read_csv('train.csv', encoding='latin-1').merge(product_description, on='product_uid')\nprint(dtrain.shape)\ndtrain.head()","7fdd15c2":"dtest = pd.read_csv('test.csv', encoding='latin-1').merge(product_description, on='product_uid')\nprint(dtest.shape)\ndtest.head()","06701c56":"from tqdm.auto import tqdm\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport re, spacy\n\n\nclass TextCleaner(BaseEstimator, TransformerMixin):\n    \"\"\"A general purpose text cleaning pipeline which utilizes `spacy` and regex to:\n        * lower cases the text\n        * removes urls and emails\n        * removes html css and js\n        * removes stop words\n        * performs lemmatization\n        * removes numbers, punctuations\n        * trims white spaces\n\n    Args:\n        model (str): spacy language model, default: en\n    \"\"\"\n\n    def __init__(self, model=\"en\"):\n        self.nlp = spacy.load(model, disable=[\"parser\", \"ner\"])\n\n    def fit(self, X=None):\n        return self\n\n    def transform(self, X):\n        transformed = []\n        for x in tqdm(X):\n            x = str(x).strip().lower()  # Lower case the data\n            x = re.sub(r\"\"\"((http[s]?:\/\/)[^ <>'\"{}|\\^`[\\]]*)\"\"\", r\" \", x)  # remove urls\n            x = re.sub(\n                r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", r\" \", x\n            )  # remove emails\n            x = re.sub(r\"<style.*>[\\s\\S]+<\/style>\", \" \", x)  # remove css\n            x = re.sub(r\"<script.*>[\\s\\S]*<\/script>\", \" \", x)  # remove js\n            x = re.sub(\n                r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\", \" \", x\n            )  # remove html\n\n            if len(x) != 0:\n                parsed = self.nlp(x)\n                lemmatized = \" \".join([w.lemma_ for w in parsed if not w.is_stop])\n\n                # Remove punct\n                punct_removed = re.sub(r\"\\W\", \" \", str(lemmatized))\n                punct_removed = re.sub(r\"\\d\", \" \", str(punct_removed))\n                punct_removed = re.sub(r\"\\s+\", \" \", str(punct_removed))\n            else:\n                punct_removed = x\n            transformed.append(punct_removed)\n        return transformed\n\n    def fit_transform(self, X):\n        return self.fit(X).transform(X)","bba5699f":"# textcleaner = TextCleaner()\n# all_text = pd.DataFrame({\n#     'raw_text': dtrain['product_title'].values.tolist() + dtrain['search_term'].values.tolist() + \n#     dtest['product_title'].values.tolist() + dtest['search_term'].values.tolist() + \n#     product_description['product_description'].values.tolist()\n# }).drop_duplicates().reset_index(drop=True)\n# all_text['cleaned_text'] = textcleaner.fit_transform(all_text['raw_text'])\n\n# all_text.to_csv('home-depot-product-search-relevance.csv', index=False)\n\ntext_map = pd.read_csv('..\/input\/qqp-cleaned\/home-depot-product-search-relevance.csv')\ntext_map = {x:y for x, y in zip(text_map['raw_text'].values, text_map['cleaned_text'].values)}\n\ndtrain['product_title'] = dtrain['product_title'].apply(lambda x: text_map[x])\ndtrain['product_description'] = dtrain['product_description'].apply(lambda x: text_map[x])\ndtrain['search_term'] = dtrain['search_term'].apply(lambda x: text_map[x])\ndtest['product_title']  =  dtest['product_title'].apply(lambda x: text_map[x])\ndtest['product_description']  =  dtest['product_description'].apply(lambda x: text_map[x])\ndtest['search_term']  =  dtest['search_term'].apply(lambda x: text_map[x])\n\ndel text_map","13b6e553":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntitle_vectorizer = TfidfVectorizer(sublinear_tf=True).fit(\n    pd.concat([dtrain['product_title'], dtest['product_title']], axis=0).drop_duplicates().dropna()\n)\ndescription_vectorizer = TfidfVectorizer(sublinear_tf=True).fit(\n    pd.concat([dtrain['product_description'], dtest['product_description']], axis=0).drop_duplicates().dropna()\n)\nsearch_vectorizer = TfidfVectorizer(sublinear_tf=True).fit(\n    pd.concat([dtrain['search_term'], dtest['search_term']], axis=0).drop_duplicates().dropna()\n)\n\n\ntrain_title = title_vectorizer.transform(dtrain['product_title'].fillna(''))\ntest_title = title_vectorizer.transform(dtest['product_title'].fillna(''))\ntrain_description = description_vectorizer.transform(dtrain['product_description'].fillna(''))\ntest_description = description_vectorizer.transform(dtest['product_description'].fillna(''))\ntrain_search = search_vectorizer.transform(dtrain['search_term'].fillna(''))\ntest_search = search_vectorizer.transform(dtest['search_term'].fillna(''))","94ee6c4f":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass ConvertToSparseTensor(BaseEstimator, TransformerMixin):\n    \"\"\"A utiliy to convert sparse vectors into sparse tensors\"\"\"\n\n    def __init__(self):\n        pass\n\n    def fit(self, X):\n        return self\n\n    def transform(self, X):\n        coo = X.tocoo()\n        indices = np.mat([coo.row, coo.col]).transpose()\n        return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))\n","fcaf0256":"cvt_to_tensors = ConvertToSparseTensor().fit(None)\ntrain_title = cvt_to_tensors.transform(train_title)\ntest_title = cvt_to_tensors.transform(test_title)\ntrain_description = cvt_to_tensors.transform(train_description)\ntest_description = cvt_to_tensors.transform(test_description)\ntrain_search = cvt_to_tensors.transform(train_search)\ntest_search = cvt_to_tensors.transform(test_search)\n\ny_train = dtrain['relevance'].values","e6c22175":"title_input = tf.keras.Input(shape=train_title.shape[1], name='title')\ndescription_input = tf.keras.Input(shape=train_description.shape[1], name='description')\nsearch_input = tf.keras.Input(shape=train_search.shape[1], name='search')\n\n\ntitle_x = tf.keras.layers.Dense(128, activation='relu')(title_input)\ntitle_x = tf.keras.layers.Dropout(0.3)(title_x)\ntitle_x = tf.keras.layers.BatchNormalization()(title_x)\ntitle_x = tf.keras.layers.Dense(32, activation='relu')(title_x)\ntitle_x = tf.keras.layers.Dropout(0.3)(title_x)\ntitle_x = tf.keras.layers.BatchNormalization()(title_x)\n\ndescription_x = tf.keras.layers.Dense(128, activation='relu')(description_input)\ndescription_x = tf.keras.layers.Dropout(0.3)(description_x)\ndescription_x = tf.keras.layers.BatchNormalization()(description_x)\ndescription_x = tf.keras.layers.Dense(32, activation='relu')(description_x)\ndescription_x = tf.keras.layers.Dropout(0.3)(description_x)\ndescription_x = tf.keras.layers.BatchNormalization()(description_x)\n\nsearch_x = tf.keras.layers.Dense(128, activation='relu')(search_input)\nsearch_x = tf.keras.layers.Dropout(0.3)(search_x)\nsearch_x = tf.keras.layers.BatchNormalization()(search_x)\nsearch_x = tf.keras.layers.Dense(32, activation='relu')(search_x)\nsearch_x = tf.keras.layers.Dropout(0.3)(search_x)\nsearch_x = tf.keras.layers.BatchNormalization()(search_x)\n\nx = tf.keras.layers.concatenate([title_x, description_x, search_x])\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dense(32, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.BatchNormalization()(x)\n\noutput_layer = tf.keras.layers.Dense(1)(x)\n\nmodel = tf.keras.models.Model([title_input, description_input, search_input], output_layer)\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(lr=1e-4))\ntf.keras.utils.plot_model(model, show_shapes=True)","c5e8279c":"model.summary()","d35183ae":"es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True)\nrlp = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, verbose=1)\n\nhistory = model.fit(\n    [train_title, train_description, train_search], y_train, callbacks=[es, rlp], epochs=250, batch_size=64\n)","b937bff4":"fig, ax = plt.subplots(figsize=(20,6))\npd.DataFrame(history.history)[['loss']].plot(ax=ax);","fb23d034":"pd.DataFrame({\n    'id': dtest.id.values,\n    'relevance': np.clip(np.ravel(model.predict([test_title, test_description, test_search])), 1, 3)\n}).to_csv('submission.csv', index=False)","f026f007":"# Vectorization","d0eef10d":"# Data","8e539ba3":"# Cleaning","5b29b4ac":"# Reference\n[Neural Collaborative Filtering](https:\/\/arxiv.org\/pdf\/1708.05031.pdf)","ef63f9f7":"# Modelling"}}