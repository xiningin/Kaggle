{"cell_type":{"ad1a68d4":"code","2bc5fb09":"code","159d1c08":"code","ea78869c":"code","0bfaf060":"code","15643fe8":"code","c0858ec9":"code","8c513e58":"code","0dbe3f29":"code","7fc28ddc":"code","65cc98ed":"code","a3ec418d":"code","a5750bc8":"code","0e67a114":"code","e87b8508":"code","70cd3bbd":"code","dca56978":"code","728c2b2c":"code","40018fec":"code","fe8d465d":"code","e5c6105a":"code","84762508":"markdown","bb270083":"markdown","473e430f":"markdown","71181d6d":"markdown","d442a546":"markdown"},"source":{"ad1a68d4":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline","2bc5fb09":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","159d1c08":"df.info()","ea78869c":"df.head()","0bfaf060":"# Use seaborn on the dataframe to create a pairplot with the hue indicated by the Outcome column.\n# It is very large plot\nsns.pairplot(df,hue='Outcome',palette='coolwarm');","15643fe8":"sns.pairplot(df)","c0858ec9":"# import the main KNN ilibrary\nfrom sklearn.preprocessing import StandardScaler","8c513e58":"# StandardScaler() object called scaler\nscaler = StandardScaler()","0dbe3f29":"# Fit the scaler to the features\nscaler.fit(df.drop('Outcome',axis=1))","7fc28ddc":"# Transform the features to a scaled version \nscaled_features = scaler.transform(df.drop('Outcome',axis=1))","65cc98ed":"# Convert the scaled features to a dataframe \ndf_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()","a3ec418d":"# Train and test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(scaled_features,df['Outcome'],\n                                                    test_size=0.30)","a5750bc8":"# Create a KNN model instance with n_neighbors=1# \n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)","0e67a114":"# Fit this KNN model to the training data\n\npred = knn.predict(X_test)","e87b8508":"from sklearn.metrics import classification_report,confusion_matrix","70cd3bbd":"print(confusion_matrix(y_test,pred))","dca56978":"print(classification_report(y_test,pred))","728c2b2c":"error_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","40018fec":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","fe8d465d":"#After that we choose some K Value for available algorihmas value\n# Retrain with new K Value\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=1')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","e5c6105a":"knn = KNeighborsClassifier(n_neighbors=23)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=23')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","84762508":"# Predictions","bb270083":"# How to choosing a K Value","473e430f":"# Using KNN","71181d6d":"# EDA","d442a546":"# Standardize the Variables"}}