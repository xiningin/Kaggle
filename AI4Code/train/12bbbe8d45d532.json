{"cell_type":{"8351e158":"code","fea4965c":"code","0c07b145":"code","3617661f":"code","abf1f72d":"code","39da7744":"code","572066bf":"code","8d6bccfe":"code","aac9a273":"code","b45a4f33":"code","8a2be363":"code","e16eaba0":"code","4cba869b":"code","eb21b7af":"code","370d454a":"code","3603d36c":"code","fb6ef2dd":"code","840a66cf":"code","67a7093b":"code","b5eb19f2":"code","2fd2f20f":"code","87de4066":"code","f0e59a03":"code","c6e11738":"code","430f479b":"code","b648760d":"code","eeb8683c":"code","2c38d5b1":"code","6fb07643":"code","5ad7a106":"code","7a5e6d60":"code","bc4fd8e9":"code","03f8f59c":"code","76800199":"code","ef2b5d36":"code","ada13a04":"code","f8091f02":"code","2586b7f6":"code","210755ad":"code","6b8f343b":"code","2e07420b":"code","b2a1ba28":"code","a4386895":"code","ebfb4683":"code","b0b2a7ef":"code","9d842455":"code","1cc2e7cd":"code","f678d535":"code","1dee182d":"markdown","83742cc6":"markdown","ed760752":"markdown","fe7ed5ae":"markdown","23215a70":"markdown","f8f878b7":"markdown","9008f0e3":"markdown","bf0b6ce0":"markdown","afc2d3c2":"markdown","056de9cf":"markdown","174fc4d9":"markdown","35155996":"markdown","ecf99339":"markdown","3d423ffb":"markdown","e9b64d5e":"markdown","cf4d8da6":"markdown","c78fb816":"markdown","c5936177":"markdown","ef1c4c99":"markdown","5c55b411":"markdown","4e54048a":"markdown","447440e0":"markdown","cbc8accb":"markdown","e2106ce0":"markdown","9f21ad74":"markdown","19984eb7":"markdown","b919ce04":"markdown","071eed5d":"markdown","13abf10b":"markdown"},"source":{"8351e158":"\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport numpy as np \nimport pandas as pd \n\n#reading the csv data\ntrainData = pd.read_csv('..\/input\/train.csv')\ntrainData.info()\n\ntrainData.head(5)","fea4965c":"trainData['Open Date'] = pd.to_datetime(trainData['Open Date'], format='%m\/%d\/%Y')   \ntrainData['OpenDays']=\"\"\n\ndateLastTrain = pd.DataFrame({'Date':np.repeat(['01\/01\/2018'],[len(trainData)]) })\ndateLastTrain['Date'] = pd.to_datetime(dateLastTrain['Date'], format='%m\/%d\/%Y')  \n\ntrainData['OpenDays'] = dateLastTrain['Date'] - trainData['Open Date']\ntrainData['OpenDays'] = trainData['OpenDays'].astype('timedelta64[D]').astype(int)\n\ntrainData = trainData.drop('Open Date', axis=1)","0c07b145":"cityPerc = trainData[[\"City Group\", \"revenue\"]].groupby(['City Group'],as_index=False).mean()\n\nsns.barplot(x='City Group', y='revenue', data=cityPerc)","3617661f":"cityPerc = trainData[[\"City\", \"revenue\"]].groupby(['City'],as_index=False).mean()\n\nnewDF = cityPerc.sort_values([\"revenue\"],ascending= False)\nsns.barplot(x='City', y='revenue', data=newDF.head(10))","abf1f72d":"cityPerc = trainData[[\"City\", \"revenue\"]].groupby(['City'],as_index=False).mean()\nnewDF = cityPerc.sort_values([\"revenue\"],ascending= True)\nsns.barplot(x='City', y='revenue', data=newDF.head(10))","39da7744":"cityPerc = trainData[[\"Type\", \"revenue\"]].groupby(['Type'],as_index=False).mean()\nsns.barplot(x='Type', y='revenue', data=cityPerc)","572066bf":"cityPerc = trainData[[\"Type\", \"OpenDays\"]].groupby(['Type'],as_index=False).mean()\nsns.barplot(x='Type', y='OpenDays', data=cityPerc)","8d6bccfe":"trainData = trainData.drop('Id', axis=1)\ntrainData = trainData.drop('Type', axis=1)","aac9a273":"citygroupDummy = pd.get_dummies(trainData['City Group'])\ntrainData = trainData.join(citygroupDummy)\n\n\ntrainData = trainData.drop('City Group', axis=1)\n\ntrainData = trainData.drop('City', axis=1)\n\ntempRev = trainData['revenue']\ntrainData = trainData.drop('revenue', axis=1)\n\n\ntrainData = trainData.join(tempRev)","b45a4f33":"trainData.head(10)","8a2be363":"from sklearn.model_selection import train_test_split\n\nX, y = trainData.iloc[:, 1:40].values, trainData.iloc[:, 40].values\n\nX_trainForBestFeatures, X_testForBestFeatures, y_trainForBestFeatures, y_testForBestFeatures =\\\n    train_test_split(X, y, \n                     test_size=0.3, \n                     random_state=0, \n                )\n    \nX_trainForBestFeatures.shape, X_testForBestFeatures.shape, y_trainForBestFeatures.shape, y_testForBestFeatures.shape","e16eaba0":"y[:20]","4cba869b":"y_trainForBestFeatures[:20]","eb21b7af":"from sklearn.ensemble import RandomForestClassifier\n\n#To label our features form best to wors \nfeat_labels = trainData.columns[1:40]\n\nforest = RandomForestClassifier(n_estimators=500,\n                                random_state=1)\nforest.fit(X_trainForBestFeatures, y_trainForBestFeatures)\n\n\n\nimportances = forest.feature_importances_\n\nindices = np.argsort(importances)[::-1]\n\nfor f in range(X_trainForBestFeatures.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, \n                            feat_labels[indices[f]], \n                            importances[indices[f]]))\n    \n","370d454a":"plt.title('Feature Importance')\nplt.bar(range(X_trainForBestFeatures.shape[1]), \n        importances[indices],\n        align='center')\n\nplt.xticks(range(X_trainForBestFeatures.shape[1]), \n           feat_labels[indices], rotation=90)\nplt.xlim([-1, X_trainForBestFeatures.shape[1]])\nplt.tight_layout()\n\nplt.show()\n","3603d36c":"trainData[feat_labels[indices[0:39]]].head()","fb6ef2dd":"import numpy as numpy \nopenDaysLog = trainData[feat_labels[indices[0:1]]].apply(numpy.log)\nopenDaysLog.head()","840a66cf":"bestDataFeaturesTrain = trainData[feat_labels[indices[1:19]]]\n\n#insert after takeing log of OpenDays feature.\nbestDataFeaturesTrain.insert(loc=0, column='OpenDays', value=openDaysLog)\n\nbestDataFeaturesTrain.head()","67a7093b":"# take the natural logarithm of the 'revenue' column in order to make it more easy for model to predict\ny = trainData['revenue'].apply(numpy.log)\n\nfrom sklearn.model_selection import train_test_split\n\nX, y = trainData.iloc[:, 1:40].values, trainData.iloc[:, 40].values\n\nX_train, X_test, y_train, y_test =\\\n    train_test_split(bestDataFeaturesTrain, y, \n                     test_size=0.3, \n                     random_state=0, \n                )\n\n\n\n    \nX_train.shape, X_test.shape, y_train.shape, y_test.shape","b5eb19f2":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler(with_std  = True ,with_mean = True, copy = True)\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n","2fd2f20f":"X_train_std[:1]","87de4066":"from sklearn.decomposition import PCA,KernelPCA\n\npca = PCA(n_components=2,svd_solver='full')\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\npca.explained_variance_ratio_\n\nkpca = KernelPCA(kernel=\"rbf\", gamma=1)\nX_kpca_train = kpca.fit_transform(X_train_pca)\nX_kpca_test = kpca.transform(X_test_pca)\n\n\n","f0e59a03":"X_train_pca[:1]\nfig, ax = plt.subplots(nrows=1,ncols=2, figsize=(10,5))\nax[0].scatter(X_train_pca[:, 0], X_train_pca[:, 1],color='red',marker='o')\nax[1].scatter(X_kpca_train[:, 0], X_kpca_train[:, 1])\nax[0].set_xlabel('Before RBF')\nax[1].set_yticks([])\nax[1].set_xlabel('After RBF')","c6e11738":"X_test_pca[:1]","430f479b":"X_train.head()","b648760d":"X_train_std[:1]","eeb8683c":"X_test.head()","2c38d5b1":"X_test.head()","6fb07643":"y_test[:5]","5ad7a106":"\nimport numpy\nfrom sklearn import linear_model\ncls = RandomForestRegressor(n_estimators=250, criterion='mse', max_depth=30)#cls = RandomForestRegressor(n_estimators=150)\n\ncls.fit(X_kpca_train, y_train)#We are training the model with RBF'ed data\n\nscoreOfModel = cls.score(X_kpca_train, y_train)\n\n\nprint(\"Score is calculated as: \",scoreOfModel)","7a5e6d60":"pred = cls.predict(X_kpca_test)\n\npred","bc4fd8e9":"for z in zip(y_test, pred):\n    print(z, (z[0]-z[1]) \/z[0] )","03f8f59c":"\nr = []\nfor pair in  zip(pred, y_test):\n    r.append(pair)\n\nplt.plot(r)\n","76800199":"\nestimators = np.arange(10, 250, 10) # 10 to 250 increased with 10\nscores = []\nfor n in estimators:\n    cls.set_params(n_estimators=n)\n    cls.fit(X_kpca_train, y_train)\n    scores.append(cls.score(X_kpca_train, y_train))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","ef2b5d36":"estimators = np.arange(10, 250, 10) # 10 to 250 increased with 10\nscores = []\nfor n in estimators:\n    cls.set_params(n_estimators=n)\n    cls.fit(X_kpca_train, y_train)\n    scores.append(cls.score(X_kpca_test, y_test))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","ada13a04":"pred[:20]","f8091f02":"worstDataFeaturesTrain = trainData[feat_labels[indices[19:39]]]\nworstDataFeaturesTrain.head()","2586b7f6":"\ny = trainData['revenue'].values\n\nfrom sklearn.model_selection import train_test_split\n\nX, y = trainData.iloc[:, 1:40].values, trainData.iloc[:, 40].values\n\nX_trainWorst, X_testWorst, y_trainWorst, y_testWorst =\\\n    train_test_split(worstDataFeaturesTrain, y, \n                     test_size=0.3, \n                     random_state=0, \n                )\n\n\n\n    \nX_trainWorst.shape, X_testWorst.shape, y_trainWorst.shape, y_testWorst.shape","210755ad":"\nimport numpy\nfrom sklearn import linear_model\ncls = RandomForestRegressor(n_estimators=250, criterion='mse', max_depth=30)#cls = RandomForestRegressor(n_estimators=150)\n\ncls.fit(X_trainWorst, y_trainWorst)\n\nscoreOfModel = cls.score(X_trainWorst, y_trainWorst)\n\nprint(\"Score is calculated as: \",scoreOfModel)","6b8f343b":"\npred = cls.predict(X_testWorst)\n\npred","2e07420b":"\nestimators = np.arange(10, 250, 10) # 10 to 250 increased with 10\nscores = []\nfor n in estimators:\n    cls.set_params(n_estimators=n)\n    cls.fit(X_trainWorst, y_trainWorst)\n    scores.append(cls.score(X_trainWorst, y_trainWorst))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","b2a1ba28":"estimators = np.arange(10, 250, 10) # 10 to 250 increased with 10\nscores = []\nfor n in estimators:\n    cls.set_params(n_estimators=n)\n    cls.fit(X_trainWorst, y_trainWorst)\n    scores.append(cls.score(X_testWorst, y_testWorst))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","a4386895":"import numpy as numpy \nopenDaysLog = trainData[feat_labels[indices[0:1]]].apply(numpy.log)\nopenDaysLog.head()","ebfb4683":"bestDataFeaturesTrain = trainData[feat_labels[indices[1:19]]]\n\n#insert after takeing log of OpenDays feature.\nbestDataFeaturesTrain.insert(loc=0, column='OpenDays', value=openDaysLog)\n\nbestDataFeaturesTrain.head()","b0b2a7ef":"# take the natural logarithm of the 'revenue' column in order to make it more easy for model to predict\ny = trainData['revenue'].apply(numpy.log)\n\nfrom sklearn.model_selection import train_test_split\n\nX, y = trainData.iloc[:, 1:40].values, trainData.iloc[:, 40].values\n\nX_train, X_test, y_train, y_test =\\\n    train_test_split(bestDataFeaturesTrain, y, \n                     test_size=0.3, \n                     random_state=0, \n                )\n\n\n\n    \nX_train.shape, X_test.shape, y_train.shape, y_test.shape","9d842455":"from sklearn import linear_model\n\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\nlinear_predictions = regr.predict(X_test)\n\nlinear_predictions","1cc2e7cd":"regr.score(X_train,y_train)","f678d535":"\nr = []\nfor pair in  zip(linear_predictions, y_test):\n    r.append(pair)\n\nplt.plot(r)","1dee182d":"## PRE-PROCESSING  & SOME  ANALYSIS <a name=\"pre\"><\/a>","83742cc6":"### Plotting the importance of the features in a barplot","ed760752":"### Test and Train model created over best 19 features.\n","fe7ed5ae":"# Plots\n### Sorting the cities by revenue; getting the max earned cities","23215a70":"### Creating dummy variables to represent City Groups. After doing dummy variables for City Group we dropped it","f8f878b7":"### Dropping the Id and Type columns since they are irrevelant for our predictions","9008f0e3":"### Splitting the data with respect to worst features\ntrain_test_split method of sklearn is used to split data into %30 of Test and %70 of Train data","bf0b6ce0":"# Model will predict output by using best 19 features.\ntrain_test_split method of sklearn is used to split data into %30 of Test and %70 of Train data","afc2d3c2":"# Train and  Test Split for RandomForestClassifier <a name=\"class\"><\/a>\n\n### Using SKLEARN's train test split library for splitting train data","056de9cf":"# Final Project for CMPE 343-Business Intelligence and Applied Analytics \n\n### In this project we used the Kaggle Competition https:\/\/www.kaggle.com\/c\/restaurant-revenue-prediction \n\n\n\n\n## Contents\n- [Pre-Processing](#pre)\n\n- [Classification](#class)\n\n- [Standartizaiton](#std)\n\n- [PCA](#pca)\n- [RBF](#rbf)\n- [RandomForestRegressor is used to predict \"revenues\"](#rnd)\n- [Score of worst model](#scr)\n- [Conclution](#conc)","174fc4d9":"### Predicted values from the 'worse' model","35155996":"### Plot about working days of specific restaurant types","ecf99339":"# Standardize features by removing the mean and scaling to unit variance\n<a name=\"std\"><\/a>\n### Standart Scaling for model efficiency","3d423ffb":"### Again Trying to find the best estimator values for the model","e9b64d5e":"# RBF is applied for linearity (since we have non-linear data) <a name=\"rbf\"><\/a>\n\n### After RBF, increase in sample variance can be observed (blue plot)","cf4d8da6":"### Converting Open Date column to Open Days; day count of the restaurant since the beginning and dropping the Open Date Columns","c78fb816":"### Fitting the model.\n### As can be seen below, model score is 31%. Less than 50% so it is worse.","c5936177":"### Comparing the revenues of big cities and other cities","ef1c4c99":"### Getting an insight of which restaurant type earns more","5c55b411":"# PCA is used due to dimension reduction <a name=\"pca\"><\/a>\n### Applied PCA in order to make it more efficient and reducing te dimentions","4e54048a":"# Score of worst model <a name=\"scr\"><\/a>\n### We took the worst 19 feautres and buiilt a model accordingly to see the effect to the model score.","447440e0":"## Plot Revenues(orange line) and Predicted Revenues(blue line)\n\n### plotting the real revenues and the predicted values","cbc8accb":"## Effect of estimators on score \n\n### With np.range we tried to find the most efficient estimator value. As can be seen on the plot, around 160 is the highest","e2106ce0":"# Our prediction and actual value with their differences and their score","9f21ad74":"### For finding best features among others. We used random forest classifier in order to get the best features. We observed that using the first 19 features give us the best results","19984eb7":"### We take the natural logarithm of the OpenDays column in order to make it more easy for model to predict","b919ce04":"# Conclusion <a name=\"conc\"><\/a>\n\nDuring this project we learned a lot of skills of Data Science and Machine Learning with using different techniques. \nWe observed the effects of PCA and RBF on our model. Also we had a chance to decide which technique to use for prediction.\nWe choose random forest regressor because it is more usefull in our case of Data among other algorithms. \nWe saw the advantages of standartization on model score also the logarithm. \nWe achived 86% accuracy but we belive that we can improve it.","071eed5d":"# RandomForestRegressor is used to predict \"revenues\" <a name=\"rnd\"><\/a>\n\n### Finally after pre-processing now we can begin to predict with RandomForestRegressor.\n\n#### Our model works on 86% accuracy.","13abf10b":"### Importing required libraries"}}