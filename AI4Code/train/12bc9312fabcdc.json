{"cell_type":{"6185b0fc":"code","ffc9ea3a":"code","ae8d68c0":"code","d50ae7c4":"code","b32fe6a0":"code","77b93c51":"code","f784bff0":"code","d81985dc":"code","c26df7eb":"code","d014fbf3":"code","045bcf20":"code","850789b6":"code","095327fd":"code","5463c521":"code","0d159371":"code","3048788b":"markdown","9547b803":"markdown"},"source":{"6185b0fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom IPython.display import clear_output\nclear_output()\nimport time,csv\nimport matplotlib.pyplot as plt\nfrom dateutil.tz import tzlocal\nfrom datetime import datetime\nimport seaborn as sns\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ffc9ea3a":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        clear_output()\n\n# could've done this better, but hey...\n# for kaggle\n# rootdir = '\/kaggle\/input\/g-research-crypto-forecasting\/'\n\n# for home\nrootdir = '\/kaggle\/input\/g-research-crypto-forecasting\/'\n\n# data pull\nexample_sample_submission = pd.read_csv(rootdir+'example_sample_submission.csv')\nasset_details = pd.read_csv(rootdir+'asset_details.csv')\nexample_test = pd.read_csv(rootdir+'example_test.csv')\ntrain = pd.read_csv(rootdir+'train.csv')\nsupplemental_train = pd.read_csv(rootdir+'supplemental_train.csv')\n\nprint('csv\/files loaded')\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae8d68c0":"# timestamping \nsupplemental_train['timestamp'] = supplemental_train.timestamp.astype('datetime64[s]')\nexample_test['timestamp'] = example_test.timestamp.astype('datetime64[s]')\ntrain['timestamp'] = train.timestamp.astype('datetime64[s]')\nprint('timestamping complete')","d50ae7c4":"import numpy\nimport matplotlib.pyplot as plt\nimport math\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","b32fe6a0":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n\n# fix random seed for reproducibility\nnumpy.random.seed(7)","77b93c51":"# load the dataset\n# isolate to only one asset, for now...\n# this also assumes only one variable - & we do not expect to output a complete \"submission.csv\"\ndataframe = supplemental_train[supplemental_train.Asset_ID == 8][['Close']].reset_index(drop=True).head(100)\n\ndataset = dataframe.values\ndataset = dataset.astype('float32')","f784bff0":"# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)","d81985dc":"# split into train and test sets\ntrain_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n\n# reshape into X=t and Y=t+1\nlook_back = 3\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))","c26df7eb":"# Influence: https:\/\/machinelearningmastery.com\/grid-search-arima-hyperparameters-with-python\/","d014fbf3":"from statsmodels.tsa.arima.model import ARIMA\nfrom matplotlib import pyplot\n# load dataset\n# dataframe\n\n# fit model\nmodel = ARIMA(dataframe, order=(5,1,0))\nmodel_fit = model.fit()\n# summary of fit model\nprint(model_fit.summary())\n# line plot of residuals\nresiduals = pd.DataFrame(model_fit.resid)\nresiduals.plot()\npyplot.show()\n# density plot of residuals\nresiduals.plot(kind='kde')\npyplot.show()\n# summary stats of residuals\nprint(residuals.describe())","045bcf20":"# grid search ARIMA parameters for time series\nimport warnings\nfrom math import sqrt\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\n# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(X, arima_order):\n\t# prepare training dataset\n\ttrain_size = int(len(X) * 0.66)\n\ttrain, test = X[0:train_size], X[train_size:]\n\thistory = [x for x in train]\n\t# make predictions\n\tpredictions = list()\n\tfor t in range(len(test)):\n\t\tmodel = ARIMA(history, order=arima_order)\n\t\tmodel_fit = model.fit()\n\t\tyhat = model_fit.forecast()[0]\n\t\tpredictions.append(yhat)\n\t\thistory.append(test[t])\n\t# calculate out of sample error\n\trmse = sqrt(mean_squared_error(test, predictions))\n\treturn rmse\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n\tdataset = dataset.astype('float32')\n\tbest_score, best_cfg = float(\"inf\"), None\n\tfor p in p_values:\n\t\tfor d in d_values:\n\t\t\tfor q in q_values:\n\t\t\t\torder = (p,d,q)\n\t\t\t\ttry:\n\t\t\t\t\trmse = evaluate_arima_model(dataset, order)\n\t\t\t\t\tif rmse < best_score:\n\t\t\t\t\t\tbest_score, best_cfg = rmse, order\n\t\t\t\t\tprint('ARIMA%s RMSE=%.3f' % (order,rmse))\n\t\t\t\texcept:\n\t\t\t\t\tcontinue\n\tprint('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n\n# load dataset\nseries = dataframe.Close\n\n# evaluate parameters\np_values = [0,1,2] # please use -> [0, 1, 2, 4, 6, 8, 10]\nd_values = range(0, 3)\nq_values = range(0, 3)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(series.values, p_values, d_values, q_values)","850789b6":"from statsmodels.tsa.statespace.sarimax import SARIMAX\nmodel = SARIMAX(dataframe.Close, exog=None, order=(0, 1, 1), seasonal_order=(0, 1, 1, 7)).fit(method='cg')\nmodel.summary()","095327fd":"help(model)","5463c521":"#import gresearch_crypto\n#env = gresearch_crypto.make_env()\n\n# Training data is in the competition dataset as usual\n#train_df = pd.read_csv('\/kaggle\/input\/g-research-crypto-forecasting\/train.csv', low_memory=False)\n\n# load model\n#model.fit(train_df)\n#model.fit(train_df)\n\n#iter_test = env.iter_test()\n\n#for (test_df, sample_prediction_df) in iter_test:\n#    sample_prediction_df['Target'] = model.predict(test_df)\n#    env.predict(sample_prediction_df)\n\n\n# submission\n#sample_prediction_df['Target'].to_csv('submission.csv', index=False)\nprint(\"project complete, for now.\")","0d159371":"# still need output: submission.csv\n\n# to be continued...","3048788b":"# Submission","9547b803":" # ARIMA Model"}}