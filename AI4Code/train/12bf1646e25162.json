{"cell_type":{"b79bff64":"code","5ceffc62":"code","cd718b2f":"code","28ff9b6d":"code","f7e5542a":"code","80aa79c0":"markdown","b127a9ab":"markdown"},"source":{"b79bff64":"train_token = np.load('token_train.npy', allow_pickle=True)\n#the BERT tokenization is saved as token_train.npy, see the post to find how it is saved\ntrain_token=train_token.tolist()","5ceffc62":"print(token_train.keys())\n# O\/P - dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'label'])","cd718b2f":"class MyDataset(Dataset):\n    \n    def __init__(self, dict_,  labels =False):\n        self.dict = dict_\n        self.labels = labels\n    def __getitem__(self, ids):\n\n        return {\n            'input_ids' : torch.tensor(self.dict['input_ids'][ids], dtype=torch.long),\n            'token_type_ids' : torch.tensor(self.dict['token_type_ids'][ids], dtype=torch.long),\n            'attention_mask' : torch.tensor(self.dict['attention_mask'][ids], dtype=torch.long),\n            'labels' : torch.tensor(self.dict['label'][ids], dtype=torch.long)\n        }\n    \n    def __len__(self):\n        return len(self.dict['label'])","28ff9b6d":"def collate(batch):\n    ips = [item['input_ids'] for item in batch]\n    ttypes = [item['token_type_ids'] for item in batch]\n    attn = [item['attention_mask'] for item in batch]\n    lb = [item['labels'] for item in batch]\n\n    return {\n               'token_type_ids': torch.stack(ttypes),\n               'input_ids': torch.stack(ips),\n               'attention_mask' : torch.stack(attn),\n               'labels': torch.stack(lb) \n            }","f7e5542a":"dataset = MyDataset(train_token,)\ntrain_dataloader =  DataLoader(dataset,batch_size=128, collate_fn= collate, shuffle=True)","80aa79c0":"After fetching a list of samples using the indices[ids here] from sampler, the function passed as the collate_fn argument is used to collate lists of samples into batches","b127a9ab":"* In continuation with post: [tokenize-train-data-using-bert-tokenizer](https:\/\/www.kaggle.com\/harveenchadha\/tokenize-train-data-using-bert-tokenizer\/notebook)\n\n* After you save the tokenizers output in npy format, following code could be implemented in order to reloading as torch.data.Dataset and then passing it to a dataloader.\n\n* UseCase:-\nLarge training data so to avoid on the fly tokenizatio, we save tokenized data to disk and load it using below code.\n"}}