{"cell_type":{"661cb323":"code","9586da3c":"code","127ff967":"code","f6653142":"code","bfb2b23d":"code","a94bf408":"code","516d4036":"code","fd1ae2d5":"code","c1944609":"code","60b218cd":"code","42a2fde4":"code","0fcdbf1d":"code","f8b74bd9":"code","17b30c4d":"code","d651c57e":"code","d39e9cb3":"code","241274e4":"code","95bbbb31":"code","6932274f":"code","9cc67346":"code","11348982":"code","9861fb68":"code","6d0d8d20":"code","8d98d0b4":"code","c48651f1":"code","7a522333":"code","eaf7b8f7":"code","abd30c2a":"code","6781b5a1":"code","c7b734af":"code","b1cd464a":"code","f8443b4b":"code","c72e011e":"code","1716aa77":"code","3b44055d":"code","c9c50439":"code","8054399d":"code","5bf28c2b":"code","ab282a00":"code","e1384994":"code","6eb229d9":"code","220fa51d":"code","8eeb2042":"code","d3142921":"code","e6f5670c":"code","f53cdef0":"code","72706155":"code","c60512b0":"code","03dea47d":"code","d10a4d10":"code","e324d831":"code","08dd7e03":"code","36c145f7":"code","e6ef892b":"code","e5d01a9c":"code","aaf47c2c":"code","7c8c8105":"code","80bc3c4f":"code","a37c9645":"code","9ace2e79":"code","de27c7d1":"code","897a4182":"code","3e8e6511":"code","09d6ea35":"code","f8a16e1d":"code","8a33ee82":"code","2c65b9d5":"code","af5206e6":"code","9e63545c":"code","57b3fd95":"code","fc6f04fc":"code","bf575225":"code","fe66446c":"code","411a2c25":"code","0b265002":"markdown","501046d2":"markdown","acd1ed7f":"markdown","02242155":"markdown","48901239":"markdown","44c47125":"markdown","a5e4da6c":"markdown","84c1b68e":"markdown","8eedf7d6":"markdown","5ef60d50":"markdown","e5618be5":"markdown","1ca4a290":"markdown","9d4c32e0":"markdown","85cb598d":"markdown","8a692364":"markdown","db6f4ff4":"markdown","bdc184ca":"markdown","79070850":"markdown","c882e9da":"markdown","338cd65e":"markdown","c7f026c7":"markdown","2a537b75":"markdown","4dd047ff":"markdown","9b29c6d6":"markdown","8a3ec2d8":"markdown","891fa4cb":"markdown","eedb99b1":"markdown","1416f536":"markdown","e9690a61":"markdown","af2202ff":"markdown","65c478a7":"markdown"},"source":{"661cb323":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9586da3c":"import pandas as pd \n# This librarys is to work with vectors\nimport numpy as np\n# This library is to create some graphics algorithmn\nimport seaborn as sns\n# to render the graphs\nimport matplotlib.pyplot as plt\n# import module to set some ploting parameters\nfrom matplotlib import rcParams\n# Library to work with Regular Expressions\nimport re\n\n# This function makes the plot directly on browser\n%matplotlib inline\n\n# Seting a universal figure size \nrcParams['figure.figsize'] = 10,8","127ff967":"train=pd.read_csv('\/kaggle\/input\/train.csv')\ntrain.info()\ntrain['Crop_Damage'].value_counts()\n\n#coln =['Estimated_Insects_Count','Number_Doses_Week','Number_Weeks_Used','Number_Weeks_Quit']\n#for col in coln:\n #   train[col] = train[col].replace('None', np.nan)\n\n#coln =['Estimated_Insects_Count','Number_Doses_Week','Number_Weeks_Used','Number_Weeks_Quit']\n#for col in coln:\n   # train[col] = train[col].fillna(train[col].mode()[0], inplace=True)","f6653142":"train.isnull().sum()","bfb2b23d":"train['Number_Weeks_Used'].fillna(train['Number_Weeks_Used'].mode()[0], inplace=True)\ntrain.isnull().sum()","a94bf408":"train['Number_Doses_Week'].value_counts()","516d4036":"#printing the chance to cropdamage by each Croptype\nprint(\"Chances to crop damange based on crop type: \") \nprint(train.groupby(\"Crop_Type\")[\"Crop_Damage\"].mean())\n\n# figure size\nplt.figure(figsize=(12,5))\n\n#Plotting the count of title by Crop damage or not category\nsns.countplot(x='Crop_Type', data=train, palette=\"hls\",\n              hue=\"Crop_Damage\")\nplt.xlabel(\"Crop_Type\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=16)\nplt.title(\"Crop_Type Grouped Count\", fontsize=20)\nplt.xticks(rotation=45)\nplt.show()","fd1ae2d5":"inscect_high_Damage_low = train[(train[\"Estimated_Insects_Count\"] > 0) & \n                              (train[\"Crop_Damage\"] == 0)]\n\ninscect_high_Damage_medium = train[(train[\"Estimated_Insects_Count\"] > 0) & \n                              (train[\"Crop_Damage\"] == 1)]                                   \ninscect_high_Damage_high= train[(train[\"Estimated_Insects_Count\"] > 0) & \n                              (train[\"Crop_Damage\"] == 2)]\n\n#figure size\nplt.figure(figsize=(10,5))\n\n# Ploting the 2 variables that we create and compare the three\nsns.distplot(inscect_high_Damage_low[\"Estimated_Insects_Count\"], bins=24, color='g')\nsns.distplot(inscect_high_Damage_medium[\"Estimated_Insects_Count\"], bins=24, color='b')\nsns.distplot(inscect_high_Damage_high[\"Estimated_Insects_Count\"], bins=24, color='r')\n                                   \n                                   \nplt.title(\"Distribuition and density by Estimated_Insects_Count\",fontsize=20)\nplt.xlabel(\"Estimated_Insects_Count\",fontsize=15)\nplt.ylabel(\"Distribuition Crop Damage\",fontsize=15)\nplt.show()","c1944609":"# figure size\nplt.figure(figsize=(12,5))\n\n# using facetgrid that is a great way to get information of our dataset\ng = sns.FacetGrid(train, col='Crop_Damage',size=5)\ng = g.map(sns.distplot, \"Estimated_Insects_Count\")\nplt.show()","60b218cd":"train['Estimated_Insects_Count'].value_counts()","42a2fde4":"print(pd.crosstab(train.Soil_Type, train.Crop_Damage))\n#Plotting the result\nplt.subplot(2,1,1)\nsns.countplot(\"Soil_Type\",data=train,hue=\"Crop_Damage\", palette=\"hls\")\nplt.ylabel(\"Count\", fontsize=18)\nplt.xlabel(\"Soil_Type\", fontsize=18)\nplt.title(\"Soil_Type Distribution \", fontsize=20)\n\n#plt.subplot(2,1,2)\n#sns.swarmplot(x='Soil_Type',y=\"Estimated_Insects_Count\",data=train,\n             # hue=\"Crop_Damage\", palette=\"hls\", )\n#plt.ylabel(\"Estimated_Insects_Count\", fontsize=18)\n#plt.xlabel(\"Soil_Type\", fontsize=18)\n#plt.title(\"Estimated_Insects Distribution by Soil Categorys \", fontsize=20)\n\n#plt.subplots_adjust(hspace = 0.5, top = 0.9)\n\nplt.show()\n\nprint(pd.crosstab(train.Pesticide_Use_Category, train.Crop_Damage))\n#Plotting the result\nplt.subplot(2,1,1)\nsns.countplot(\"Pesticide_Use_Category\",data=train,hue=\"Crop_Damage\", palette=\"hls\")\nplt.ylabel(\"Count\", fontsize=18)\nplt.xlabel(\"Pesticide_Use_Category\", fontsize=18)\nplt.title(\"Pesticide_Use_Category Distribution \", fontsize=20)\nplt.show()\n\nprint(pd.crosstab(train.Number_Doses_Week, train.Crop_Damage))\n#Plotting the result\nplt.subplot(2,1,1)\nsns.countplot(\"Number_Doses_Week\",data=train,hue=\"Crop_Damage\", palette=\"hls\")\nplt.ylabel(\"Count\", fontsize=18)\nplt.xlabel(\"Number_Doses_Week\", fontsize=18)\nplt.title(\"Number_Doses_Week_Category Distribution \", fontsize=20)\nplt.show()\n\n#Number_Weeks_Used\n\nprint(pd.crosstab(train.Number_Weeks_Used, train.Crop_Damage))\n#Plotting the result\nplt.subplot(2,1,1)\nsns.countplot(\"Number_Weeks_Used\",data=train,hue=\"Crop_Damage\", palette=\"hls\")\nplt.ylabel(\"Count\", fontsize=18)\nplt.xlabel(\"Number_Weeks_Used\", fontsize=18)\nplt.title(\"Number_Weeks_Used_Category Distribution \", fontsize=20)\nplt.show()\n\nprint(pd.crosstab(train.Season, train.Crop_Damage))\n#Plotting the result\nplt.subplot(2,1,1)\nsns.countplot(\"Season\",data=train,hue=\"Crop_Damage\", palette=\"hls\")\nplt.ylabel(\"Count\", fontsize=18)\nplt.xlabel(\"Season\", fontsize=18)\nplt.title(\"Season_Category Distribution \", fontsize=20)\nplt.show()\n\n\nprint(pd.crosstab(train.Number_Weeks_Quit, train.Crop_Damage))\n#Plotting the result\nplt.subplot(2,1,1)\nsns.countplot(\"Number_Weeks_Quit\",data=train,hue=\"Crop_Damage\", palette=\"hls\")\nplt.ylabel(\"Count\", fontsize=18)\nplt.xlabel(\"Number_Weeks_Quit\", fontsize=18)\nplt.title(\"Number_Weeks_Quit Distribution \", fontsize=20)\nplt.show()","0fcdbf1d":"# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"Pesticide_Use_Category\",y=\"Crop_Damage\",data=train, kind=\"bar\", size = 6,palette = \"hls\")\ng = g.set_ylabels(\"survival probability Pesticide\")\n\ng  = sns.factorplot(x=\"Season\",y=\"Crop_Damage\",data=train, kind=\"bar\", size = 6,palette = \"hls\")\ng = g.set_ylabels(\"survival probability Season\")","f8b74bd9":"#lets understand the impact of number of weeks of pestiside use on the Crop\n\ninscect_high_Damage_low = train[(train[\"Number_Weeks_Used\"] > 0) & \n                              (train[\"Crop_Damage\"] == 0)]\n\ninscect_high_Damage_medium = train[(train[\"Number_Weeks_Used\"] > 0) & \n                              (train[\"Crop_Damage\"] == 1)]                                   \ninscect_high_Damage_high= train[(train[\"Number_Weeks_Used\"] > 0) & \n                              (train[\"Crop_Damage\"] == 2)]\n\n#figure size\nplt.figure(figsize=(10,5))\n\n# Ploting the 2 variables that we create and compare the three\nsns.distplot(inscect_high_Damage_low[\"Number_Weeks_Used\"], bins=24, color='g')\nsns.distplot(inscect_high_Damage_medium[\"Number_Weeks_Used\"], bins=24, color='b')\nsns.distplot(inscect_high_Damage_high[\"Number_Weeks_Used\"], bins=24, color='r')\n                                   \n                                   \nplt.title(\"Distribution by Number_Weeks_Used\",fontsize=20)\nplt.xlabel(\"Number_Weeks_Used\",fontsize=15)\nplt.ylabel(\"Distribuition Crop Damage\",fontsize=15)\nplt.show()\n\n","17b30c4d":"# using facetgrid that is a great way to get information of our dataset\ng = sns.FacetGrid(train, col='Crop_Damage',size=5)\ng = g.map(sns.distplot, \"Number_Weeks_Used\")\nplt.show()","d651c57e":"#lets understand the impact of number of doses of pestiside use on the Crop\n\ninscect_high_Damage_low = train[(train[\"Number_Doses_Week\"] > 0) & \n                              (train[\"Crop_Damage\"] == 0)]\n\ninscect_high_Damage_medium = train[(train[\"Number_Doses_Week\"] > 0) & \n                              (train[\"Crop_Damage\"] == 1)]                                   \ninscect_high_Damage_high= train[(train[\"Number_Doses_Week\"] > 0) & \n                              (train[\"Crop_Damage\"] == 2)]\n\n#figure size\nplt.figure(figsize=(10,5))\n\n# Ploting the 2 variables that we create and compare the three\nsns.distplot(inscect_high_Damage_low[\"Number_Doses_Week\"], bins=24, color='g')\nsns.distplot(inscect_high_Damage_medium[\"Number_Doses_Week\"], bins=24, color='b')\nsns.distplot(inscect_high_Damage_high[\"Number_Doses_Week\"], bins=24, color='r')\n                                   \n                                   \nplt.title(\"Distribution by Number_Doses\",fontsize=20)\nplt.xlabel(\"Number_Doses_Week\",fontsize=15)\nplt.ylabel(\"Distribuition Crop Damage\",fontsize=15)\nplt.show()\n","d39e9cb3":"# using facetgrid that is a great way to get information of our dataset\ng = sns.FacetGrid(train, col='Crop_Damage',size=5)\ng = g.map(sns.distplot, \"Number_Doses_Week\")\nplt.show()","241274e4":"df_train1=train.drop('ID', axis=1)\n","95bbbb31":"plt.show()\nplt.figure(figsize=(15,12))\nsns.heatmap(df_train1.astype(float).corr(),vmax=1.0,  annot=True)\nplt.show()","6932274f":"df_train = pd.get_dummies(df_train1, columns=[\"Crop_Type\",\"Soil_Type\",\"Pesticide_Use_Category\",\"Season\",\"Crop_Damage\"],\\\n                         prefix=[\"Crop\",\"Soil\",\"Pesticide\",\"Season\",\"Damage\"], drop_first=False)\nplt.figure(figsize=(15,12))\nplt.title('Correlation of Features for Train Set')\nsns.heatmap(df_train.astype(float).corr(),vmax=1.0,  annot=True)\nplt.show()","9cc67346":"df_train","11348982":"df_train['Estimated_Insects_Count'] = df_train['Estimated_Insects_Count'].astype(\"int16\")\ndf_train['Number_Doses_Week'] = df_train['Number_Doses_Week'].astype(\"int16\")\ndf_train['Number_Weeks_Used'] = df_train['Number_Weeks_Used'].astype(\"int16\")\ndf_train['Number_Weeks_Quit'] = df_train['Number_Weeks_Quit'].astype(\"int16\")\n","9861fb68":"#df_train['Estimated_Insects_Count']=np.log(df_train['Estimated_Insects_Count'])\nnum_cols = ['Estimated_Insects_Count','Number_Doses_Week','Number_Weeks_Used','Number_Weeks_Quit']\n    \nfig,ax = plt.subplots(4,1,figsize=(8,8),squeeze=False)\nr=0\nc=0\nfor i in num_cols:\n    sns.distplot(df_train[i],ax=ax[r][c])\n    r+=1","6d0d8d20":"df_train.info()\ndf_train.head()","8d98d0b4":"df_train['Estimated_Insects_Count']=np.log(df_train['Estimated_Insects_Count'])\n#df_train['Number_Doses_Week1']=np.log(df_train['Number_Doses_Week'])\n#df_train['Number_Weeks_Used1']=np.log(df_train['Number_Weeks_Used'])\n#df_train['Number_Weeks_Quit1']=np.log(df_train['Number_Weeks_Quit'])\n","c48651f1":"num_cols = ['Estimated_Insects_Count','Number_Doses_Week','Number_Weeks_Used','Number_Weeks_Quit']\n    \nfig,ax = plt.subplots(4,1,figsize=(8,8),squeeze=False)\nr=0\nc=0\nfor i in num_cols:\n    sns.distplot(df_train[i],ax=ax[r][c])\n    r+=1","7a522333":"df_train","eaf7b8f7":"train=pd.read_csv('\/kaggle\/input\/train.csv')","abd30c2a":"train['Estimated_Insects_Count']=np.log(train['Estimated_Insects_Count'])","6781b5a1":"train.describe()","c7b734af":"#Implementing  Encoding for some of the numeric columns\ntrain['Number_Doses_Week_bin'] = np.where(train['Number_Doses_Week']>20,1,0)\ntrain['Number_Weeks_Used_bin'] = np.where(train['Number_Weeks_Used']>36,1,0)\ntrain['Number_Weeks_Quit_bin'] = np.where(train['Number_Weeks_Quit']>7,1,0)","b1cd464a":"\n#Implementing  Dummy Encoding for cate gorical columns\ntrain = pd.get_dummies(train, columns=[\"Crop_Type\",\"Soil_Type\",\"Pesticide_Use_Category\",\"Season\"],\\\n                         prefix=[\"Crop\",\"Soil\",\"Pesticide\",\"Season\"], drop_first=False)","f8443b4b":"\n#train=train.drop(['Number_Doses_Week','Number_Weeks_Used','Number_Weeks_Quit'], axis=1)\ntrain.info()","c72e011e":"col=['Crop_0','Crop_1','Soil_0','Soil_1','Pesticide_1','Pesticide_2','Pesticide_3','Season_1','Season_2','Season_3']\nfor i in col:\n    train[i] = train[i].astype('category')\n    train[i] = train[i].cat.codes.astype(\"int16\")","1716aa77":"train=train.drop(['Number_Doses_Week','Number_Weeks_Used','Number_Weeks_Quit'],axis=1)\ntrain=train.drop(['ID'],axis=1)","3b44055d":"#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n#Models\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","c9c50439":"\nX= train.drop(['Crop_Damage'],axis=1)\ny= train['Crop_Damage']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 22)","8054399d":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, n_estimators=150))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 10\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv= 5, scoring=scoring, n_jobs=-1)*100    \n    names.append(name)\n    results.append(cv_results)    \n    #msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    #print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","5bf28c2b":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\nimport gc\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, mean_absolute_error,accuracy_score, classification_report\nkfold = KFold(n_splits=10, random_state=7)\nfrom lightgbm import LGBMClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_log_error\nlgbc = LGBMClassifier(n_estimators=550,\n                     learning_rate=0.03,\n                     min_child_samples=40,\n                     random_state=1,\n                     colsample_bytree=0.5,\n                     reg_alpha=2,\n                     reg_lambda=2)\n\nresultsLGB = cross_val_score(lgbc,X_train, y_train,cv=kfold)\nprint(\"LightGBM\",resultsLGB.mean()*100)","ab282a00":"LGB=lgbc.fit(X_train,y_train)","e1384994":"y_predict_LGBM = LGB.predict(X_test)\nprint(100*(np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_predict_LGBM)))))\nresultsLGB_test = cross_val_score(lgbc,X_test, y_test,cv=kfold)\nprint(\"LightGBM\",resultsLGB_test.mean()*100)\n","6eb229d9":"\nsorted(zip(LGB.feature_importances_, X_train), reverse = True)","220fa51d":"from catboost import CatBoostRegressor \nfrom catboost import  CatBoostClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import roc_auc_score\n\n#cb = CatBoostRegressor(\n    #n_estimators = 1000,\n    #learning_rate = 0.11,\n    #iterations=1000,\n    #loss_function = 'RMSE',\n    #eval_metric = 'RMSE',\n    #verbose=0)\n    \ncb= CatBoostClassifier(\n    iterations=100, \n    learning_rate=0.1, \n    #loss_function='CrossEntropy'\n)\n\n#rmsle = 0\n#for i in ratio:\n # x_train,y_train,x_val,y_val = train_test_split(i)\n\n#CAT=cb.fit(X_train,y_train)\n#resultsCAT = cross_val_score(cb,X_train, y_train,cv=kfold)\n#print(\"CAT\",resultsCAT.mean()*100)\n                        \ncb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=50,early_stopping_rounds = 100)","8eeb2042":"kfold = KFold(n_splits=10, random_state=7)\nresultsCAT = cross_val_score(cb,X_train, y_train,cv=kfold)\nprint(\"CAT\",resultsCAT.mean()*100)\n","d3142921":"y_predict_CAT = cb.predict(X_train)\nprint(100*(np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(y_predict_CAT)))))\nresultsCAT_train = cross_val_score(cb,X_train, y_train,cv=kfold)\nprint(\"CAT\",resultsCAT_train.mean()*100)","e6f5670c":"test=pd.read_csv('\/kaggle\/input\/test.csv')\ntest.info()\n","f53cdef0":"test.describe()","72706155":"test.isnull().sum()","c60512b0":"test['Number_Weeks_Used'].fillna(test['Number_Weeks_Used'].mode()[0], inplace=True)\ntest.isnull().sum()\nsubmissiontest=test","03dea47d":"test['Estimated_Insects_Count']=np.log(test['Estimated_Insects_Count'])","d10a4d10":"#Implementing  Encoding for some of the numeric columns\ntest['Number_Doses_Week_bin'] = np.where(test['Number_Doses_Week']>20,1,0)\ntest['Number_Weeks_Used_bin'] = np.where(test['Number_Weeks_Used']>36,1,0)\ntest['Number_Weeks_Quit_bin'] = np.where(test['Number_Weeks_Quit']>7,1,0)","e324d831":"test = pd.get_dummies(test, columns=[\"Crop_Type\",\"Soil_Type\",\"Pesticide_Use_Category\",\"Season\"],\\\n                         prefix=[\"Crop\",\"Soil\",\"Pesticide\",\"Season\"], drop_first=False)\n","08dd7e03":"test.info()","36c145f7":"col=['Crop_0','Crop_1','Soil_0','Soil_1','Pesticide_1','Pesticide_2','Pesticide_3','Season_1','Season_2','Season_3']\nfor i in col:\n    test[i] = test[i].astype('category')\n    test[i] = test[i].cat.codes.astype(\"int16\")","e6ef892b":"test.info()","e5d01a9c":"test=test.drop(['ID'],axis=1)\n","aaf47c2c":"test=test.drop(['Number_Doses_Week','Number_Weeks_Used','Number_Weeks_Quit'],axis=1)","7c8c8105":"y_predict_CAT_TEST = cb.predict(test)\ny_predict_CAT_TEST","80bc3c4f":"df_solution = pd.DataFrame()\ndf_solution['ID'] = submissiontest.ID\ndf_solution['Crop_Damage'] = y_predict_CAT_TEST\ndf_solution","a37c9645":"df_solution.to_csv(\"CATBOOST_Implementation_Agriculture_Analytics_Submission.csv\", index=False)","9ace2e79":"X=train.drop('Crop_Damage',axis=1)\ny=train.Crop_Damage","de27c7d1":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X)","897a4182":"from keras.utils import np_utils\ndummy_y = np_utils.to_categorical(y)","3e8e6511":"dummy_y","09d6ea35":"X.shape\nm=X.shape[1]\nm","f8a16e1d":"X_train","8a33ee82":"from numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom matplotlib import pyplot\n# prepare sequence\nX = X_train\ny = dummy_y\n# create model\nmodel = Sequential()\nmodel.add(Dense(20, input_dim=14, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# train model\nNN = model.fit(X, y, epochs=30, batch_size=m, verbose=2)\n# plot metrics\n#pyplot.plot(NN.history['accuracy'])\n#pyplot.show()","2c65b9d5":"print(NN.history.keys())","af5206e6":"#history = model.fit(X, y, epochs=30, batch_size=m, verbose=2)\n# plot metrics\npyplot.plot(NN.history['accuracy'])\n#pyplot.plot(history.history['loss'])\npyplot.show()","9e63545c":"test","57b3fd95":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nfinal_test = sc.fit_transform(test)","fc6f04fc":"final_test","bf575225":"#predicting the Crop Damange by using Neural Network\ny_predict_NN=model.predict(final_test)\ndf=pd.DataFrame(y_predict_NN)\n# converting the NP array to Pandas data frame. Identifying the the colum which has maximum probability predicted\ndf.idxmax(axis=1)","fe66446c":"df_solution_NN = pd.DataFrame()\ndf_solution_NN['ID'] = submissiontest.ID\ndf_solution_NN['Crop_Damage'] = df.idxmax(axis=1)\ndf_solution_NN","411a2c25":"df_solution_NN.to_csv(\"Neural Network_Implementation_Agriculture_Analytics_Submission.csv\", index=False)","0b265002":"*Clear more insects more damage. Interestng there is no much difference between Crop Damage category 2 and 3*","501046d2":"# Reading Data. Finding missing values","acd1ed7f":"*The training data set is not balanced. Has very large dataset for Good Crops and less dataset for damaged crops*","02242155":"\n# Soil type 0 has higher survial rate for Crop compared to soil type 1; \n# Pesticide category 0 has no impact on Crop survival. Means if there is less data to determine that no pesticide has any impact on crop survival.  \n# Pesticide category 2 (Previously used Pesticides) is the best for Crops compared to the newly ones; \n# If the pesticide doses are between 10 to 40 the crops have high survival rate; \n# Crop has better chance to survive in Season 2 compared to other seasons*","48901239":"*Number_Weeks_Used has so 9000 missing data*","44c47125":"*# Both X inputs and y Targets from training data set should be in integer format*","a5e4da6c":"# Converting test data to be used in NN model","84c1b68e":"# Problem Statement:\n**Determine   the outcome of the harvest season, i.e. whether the crop would be healthy (alive), damaged by pesticides or damaged by other reasons.**\n\nData Description\n\nID:\tUniqueID\nEstimated_Insects_Count:\tEstimated insects count per square meter\nCrop_Type:\tCategory of Crop(0,1)\nSoil_Type:\tCategory of Soil (0,1)\nPesticide_Use_Category:\tType of pesticides uses (1- Never, 2-Previously Used, 3-Currently Using)\nNumber_Doses_Week:\tNumber of doses per week\nNumber_Weeks_Used:\tNumber of weeks used\nNumber_Weeks_Quit:\tNumber of weeks quit\nSeason:\tSeason Category (1,2,3)\nCrop_Damage:\tCrop Damage Category (0=alive, 1=Damage due to other causes, 2=Damage due to Pesticides)\n\nSeveral factors such as availability of water, soil fertility, protecting crops from rodents, timely use of pesticides & other useful chemicals and nature. While a lot of these factors are difficult to control for, the amount and frequency of pesticides is something the farmer can control.\n\nPesticides are also special, because while they protect the crop with the right dosage. But, if you add more than required, they may spoil the entire harvest. A high level of pesticide can deem the crop dead \/ unsuitable for consumption among many outcomes. This data is based on crops harvested by various farmers at the end of harvest season\n","8eedf7d6":"# LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n# Faster training speed and higher efficiency.Lower memory usage.Better accuracy.Support of parallel and GPU learning.Capable of handling large-scale data.\n\n","5ef60d50":"# Neural Network model shows better accuracy of 84.5 better than any other machine learning model implemented on Test data","e5618be5":"# CatBoost is a high-performance open source library for gradient boosting on decision trees","1ca4a290":"# Lets Submit the solution as per sample submisssion","9d4c32e0":"# Lets try implementing Neural Network model on the Training dataset. ","85cb598d":"# Predict using CATBOOST Model on TEST DATA SET because it has best accuracy on Training Data set compared to other models","8a692364":"*Crop type 0 has higher chance of survival compared to crop type 1*\ncrop Damage Category (0=alive, 1=Damage due to other causes, 2=Damage due to Pesticides)","db6f4ff4":"# CATBoost on xtrain 84.06 accuracy. Lets check with Xtest","bdc184ca":"*Looks like number of weeks of pesticide usage and the crop type has to be looked together. We can address this in correlation chart*","79070850":"# Looks like gradient boost classifier are best fit for Training data set. Lets try LGBM","c882e9da":"# Crop damanage + correlation with Insects, Pesticide and Number of weeks pesticide is used. But some thing missing. Lets analyze at each factor level","338cd65e":"# As per LGBM model Incents count is the most significant factor affecting quality of crop followed by doses of pesticide and crop type 0","c7f026c7":"# *When modeling multiclass classification\n# problems using neural networks, it is good practice to reshape the output attribute from a\n# vector that contains values for each class value to be a matrix with a boolean for each class\n# value and whether or not a given instance has that class value or not. This is called one hot\n# encoding or creating dummy variables from a categorical variable.*","2a537b75":"# Understanding the Train Data and seeing various trend in Data. Gathering insigths","4dd047ff":"# Survival (Damage=0) has positive correlation with Pesticide doses per week, Number of weeks pesticide is quit also if the old pesticide is used instead of new ones\n# Soil 1 is better for Crops, Soil 2 is not good\n\n# Non Survival (Damange=1, or 2) has positive correlation with # of insects, Crop type 0, Soil 0,not using any pesticide or using new pesticide \n\n# So it recommended that farmer use Soil 1 and continue using old pesticide instead of new ones.","9b29c6d6":"**Looks like number of dosage of pesticide  and the crop type has to be looked together. We can address this in correlation chart**","8a3ec2d8":"*Its clear that If the usage of pestiside is used between 10 to 30 there is higher chance for Crop to survice. If the number of weeks of pestiside usage is more than 30 then crop damage definetly more than moderate or high *","891fa4cb":"# CATBoost on xtest 84.06 accuracy. So we choose CATBoost based on accuracy to predict for TEST data set.","eedb99b1":"# Lets submit the solution","1416f536":"# Better result in LGBM 84.04 accuracy with X Train","e9690a61":"# Reading and Preparing the TEST DATA","af2202ff":"\n# feature Engineering:\nLets normalize the numeric variables\nLets implement Dummy Encoding of the Categorical Values and chang e then to int16","65c478a7":"# Model:Applying Machine Learning model\nWe have classification problem. Natrually lets apply the classifiers. Lets find which classifer is better fit for the train data"}}