{"cell_type":{"27e181e7":"code","2d649c1f":"code","04cbbcc0":"code","b72ef42e":"code","81ae89da":"code","24b00ae0":"code","0c1b7ec6":"markdown","e77d4d86":"markdown","7366d737":"markdown","111d9e14":"markdown","56753692":"markdown"},"source":{"27e181e7":"import pandas as pd\ndf = pd.read_csv(\"\/kaggle\/input\/trump-tweets\/realdonaldtrump.csv\")\ndf.head(3)","2d649c1f":"print(f\"Number of tweets: {df.shape[0]}\")","04cbbcc0":"# Add a few words to the stop words to avoid websites\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words(\"english\")+[\"http\",\"https\",\"www\", \"com\"]\n\n# Convert a collection of raw documents to a matrix of TF-IDF features.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=stop_words, ngram_range=(1,2))\ntm = tfidf.fit_transform(df[\"content\"])","b72ef42e":"# Non-Negative Matrix Factorization (NMF)\n# Find two non-negative matrices (W, H) whose product \n# approximates the non- negative matrix X.\nfrom sklearn.decomposition import NMF\nnmf = NMF(n_components=10, random_state=0)\n\n# fit the transfomed content with NMF\nnmf.fit(tm)\n\n# display the result\nfor index,topic in enumerate(nmf.components_):\n    print(f\"The top 20 words for topic # {index}\")\n    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-20:]])\n    print(\"\\n\")","81ae89da":"# Add a few words to the stop words to avoid websites\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words(\"english\")+[\"http\",\"https\",\"www\", \"com\"]\n\n# Convert a collection of raw documents to a matrix of TF-IDF features.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=stop_words, ngram_range=(1,2))\ntm = tfidf.fit_transform(df[\"content\"])\n\n","24b00ae0":"from sklearn.decomposition import LatentDirichletAllocation\nLDA = LatentDirichletAllocation(n_components = 10, n_jobs = -1, random_state = 0)\n\n# fit the transfomed content with LDA\nLDA.fit(tm)\n\n# display the result\nfor index,topic in enumerate(LDA.components_):\n    print(f\"The top 20 words for topic # {index}\")\n    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-20:]])\n    print(\"\\n\")","0c1b7ec6":"Thank you","e77d4d86":"# 1. Presentation of the data<a class=\"anchor\" id=\"1\"><\/a>\n\nAround 42.000 Tweets from Donald Trump between 2009 and 2020. \n\n<strong><u>Data Content: <\/u><\/strong><br>\n<br><br>- <strong>id: <\/strong> Unique tweet id\n<br><br>- <strong>link: <\/strong>Link to tweet\n<br><br>- <strong>content: <\/strong>Text of tweet\n<br><br>- <strong>date: <\/strong>Date of tweet \n<br><br>- <strong>retweets: <\/strong>Number of retweets\n<br><br>- <strong>favorites: <\/strong>Number of favorites\n<br><br>- <strong>mentions: <\/strong>Accounts mentioned in tweet","7366d737":"# 3. Topic modeling with LDA<a class=\"anchor\" id=\"3\"><\/a>\n\nIn natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox. (<a href=\"https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation\">source<\/a>)","111d9e14":"# Simple topic modeling with NMF\/LDA\n\n<img src=\"https:\/\/i.imgur.com\/KrLVpBQ.png\">\n\n# Table of contents\n\n[<h3>1. Presentation of the data<\/h3>](#1)\n\n[<h3>2. Topic modeling with NMF<\/h3>](#2)\n\n[<h3>3. Topic modeling with LDA<\/h3>](#3)\n \n","56753692":"# 2. Topic modeling with NMF<a class=\"anchor\" id=\"2\"><\/a>"}}