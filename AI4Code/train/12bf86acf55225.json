{"cell_type":{"7a00b052":"code","2816988d":"code","6f5836f5":"code","6640919f":"code","600675db":"code","db63ac21":"code","5ad25758":"code","2c90d8ce":"code","60f9621a":"code","1aa52e73":"code","3e482ec3":"code","bdd8fbb1":"code","c3967c95":"code","0427290b":"code","fbf0331b":"code","569c47c2":"code","3bbcf872":"code","e87c3247":"code","a7b370b9":"code","55568e72":"code","67751d18":"code","d6ce05a3":"code","aff3f52c":"code","0cfd47b0":"markdown","83b654e3":"markdown","293ccc48":"markdown","e8e10c8f":"markdown","65fddaa0":"markdown","6150de06":"markdown","5b99168e":"markdown","8995a9aa":"markdown","fa8a4fdc":"markdown","b50ea8b8":"markdown","f5acd3b2":"markdown","970e6055":"markdown","ab70f4ca":"markdown","4b2d3043":"markdown","d08df03d":"markdown","9538c559":"markdown","b1280361":"markdown","73083a37":"markdown","46980852":"markdown","a459331b":"markdown","52f5ebc9":"markdown","7439b239":"markdown","8b50f14f":"markdown","d146b676":"markdown","560e57b9":"markdown","d4dbfac8":"markdown"},"source":{"7a00b052":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2816988d":"## Load necessary packages for EDA\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt # for data visualization use\nimport seaborn as sns # for data visualization use\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings(\"ignore\") # to filter out warnings messages","6f5836f5":"df =  pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")\n\n## Take a quick pick on the data\nprint(df.info()) \nprint(\"\\n\\n\",df.nunique()) # Return the number of unique values in each feature\nprint(\"\\n\\n\",df.head()) #Return the first 5 row of the dataframe\n","6640919f":"## Drop the \"id\" column. It has nothing to do with the \"price\" column\ndf = df.drop(\"id\",axis=1)\n","600675db":"## Plot 1\nplt.figure(figsize=(12,9))\nplt.subplot(121)\nplt.title(\"Price Distribution\")\nsns.distplot(df[\"price\"])\n\n## Plot 2\nplt.subplot(122)\nplt.scatter(range(df.shape[0]),np.sort(df[\"price\"].values))\nplt.title(\"Price Curve Distribution\",fontsize=15)\nplt.ylabel(\"Amount ($)\",fontsize=12)\n\nplt.show()","db63ac21":"num_cols = df.select_dtypes(exclude=\"object\").columns.tolist() # Taking all numerical feature's name\nnum_cols.remove(\"price\") # Remove the target variable \n\nfor col in num_cols:\n    sns.lmplot(data=df,x=col,y=\"price\")\n    plt.show()","5ad25758":"# Import packages\nimport folium\nfrom folium.plugins import HeatMap\n\n# Define a function to generate the basemap\ndef generateBaseMap(default_loc=[df.lat.loc[1],df.long.loc[1]]): #set the default location to the first house geographical position\n    base_map = folium.Map(location=default_loc,control_scale=True)\n    return base_map\n\n#Generate the Heatmap\nbasemap = generateBaseMap()\n#Add a weightage for heatmap density purpose\ndf_map = df.copy()\ndf_map[\"weightage\"] = 1\n\n# Add Icon to the option map\ns = folium.FeatureGroup(name=\"icon\").add_to(basemap)\n# Add marker for the house with the highest price\nmax_price = df.loc[df.price.idxmax()]\nfolium.Marker([max_price[\"lat\"],max_price[\"long\"]],popup=\"Highest Price:${}\".format(max_price.price),icon=folium.Icon(color=\"green\")).add_to(s)\n\n# Add heatmap\nHeatMap(data=df_map[[\"lat\",\"long\",\"weightage\"]].groupby([\"lat\",\"long\"]).sum().reset_index().values.tolist(),radius=8,max_zoom=13,name=\"Heat Map\").add_to(basemap)\nfolium.LayerControl(collapsed=False).add_to(basemap)\n\n#Show the diagram\nbasemap\n","2c90d8ce":"## Missing data\nprint(df.isnull().any()) # Return True if there a present of missing data","60f9621a":"## Lower Triangle Correlation Matrix\nmask = np.zeros_like(df.corr(),dtype=bool)\nmask[np.triu_indices_from(mask)] = True\n\n## Plot Spearman Correlation Matrix\nplt.figure(figsize=(12,9))\nplt.title(\"Spearman Correlation Matrix\",fontsize=16)\nsns.heatmap(df.corr(),annot=True,fmt=\".2f\",mask=mask, square=True,linewidth=1.0,annot_kws={\"fontsize\":10})\nplt.show()\n\n## Plot top 10 most correlated features\nfeats = df.corr().nlargest(10,\"price\")[\"price\"].index\ncm = np.corrcoef(df[feats].values.T)\nplt.figure(figsize=(12,9))\nplt.title(\"Top 10 Correlated Features\",fontsize=16)\nsns.heatmap(cm,annot=True,fmt=\".2f\",linewidths=1.0,square=True,xticklabels=feats,yticklabels=feats,annot_kws={\"fontsize\":14})\nplt.show()\n\n\n\n","1aa52e73":"## Drop sqft_above feature\ndf = df.drop(\"sqft_above\",axis=1)","3e482ec3":"## Missing data\nprint(df.isnull().any()) # Return True if there's a present of missing data","bdd8fbb1":"from sklearn.preprocessing import StandardScaler\n\nprice_scaled = StandardScaler().fit_transform(df[\"price\"][:,np.newaxis]) #scale the price\nhigh_range = price_scaled[price_scaled[:,0].argsort()][-10:]\nlow_range = price_scaled[price_scaled[:,0].argsort()][:10]\n\nprint(\"Outer range (low) of the distribution :\\n{}\".format(low_range))\nprint(\"Outer range (high) of the distribution :\\n{}\".format(high_range))","c3967c95":"#Remove the outliers\noutliers = df[\"price\"].sort_values(ascending=False)[:3].index \ndf = df.drop(outliers,axis=0)\n","0427290b":"## define a function\npresence = lambda x: 1 if x > 0 else 0 #Return 1 if there the house has the feature or else 0 for absence of that feature\nfeatures = [\"bedrooms\",\"bathrooms\",\"waterfront\",\"view\",\"sqft_basement\",\"yr_renovated\"]\n\nfor feat in features:\n    df[\"Has_\"+feat] = df[feat].transform(presence)\n\n## Create new columns based on the date feature\ndf[\"date\"] = pd.to_datetime(df[\"date\"]) # Convert to datetime\nyr = lambda x: x.year ; month = lambda x: x.month \n\ndf[\"yr_sold\"] = df[\"date\"].transform(yr)\ndf[\"month_sold\"] = df[\"date\"].transform(month)\n\n## Create a new column based the age renovation\ndf[\"age_rnv\"] = df[\"yr_sold\"] - df[\"yr_built\"]\ndf.loc[df[\"Has_yr_renovated\"] == 1,\"age_rnv\"] = df[\"yr_sold\"] - df[\"yr_renovated\"]\n\n## Remove yr_renovated as we have age_rnv\ndf = df.drop(\"yr_renovated\",axis=1)\ndf = df.drop(\"date\",axis=1)\n\n## Let's check on the all the features\nprint(df.head())","fbf0331b":"# Let's look at the Target Variable first\nfrom scipy.stats import skew, kurtosis\nfrom scipy.stats import shapiro\n\n# Check the p value from the Shapiro-Wilk Test\nstc , p_value = shapiro(df.price.values)\nprint(\"p value of price feature is {}\".format(p_value))","569c47c2":"# Comparing the feature's skewness & kurtosis before and after normalization through visualization\nfig,ax = plt.subplots(1,2,figsize=(12,9))\n\n#Before Normalization\nskewness = format(skew(df.price),\".2f\") ; kurt = format(kurtosis(df.price),\".2f\")\nsns.distplot(df[\"price\"],ax=ax[0])\nax[0].legend([\"Skewness: {}\\nKurtosis: {}\".format(skewness,kurt)],loc=\"upper_right\") #Note using legend to plot the skewness and kurtosis is not a corrrect & professional way, I did it for the purpose of cleaner visualization. \nax[0].set(title=\"Before Normalization\")\n\n#After Normalization\ndf[\"price\"] = np.log1p(df[\"price\"]) # Normaliza the \"price\" feature\nskewness = format(skew(df.price),\".2f\") ; kurt = format(kurtosis(df.price),\".2f\")\nsns.distplot(df[\"price\"],ax=ax[1])\nax[1].legend([\"Skewness: {}\\nKurtosis: {}\".format(skewness,kurt)],loc=\"upper_right\")\nax[1].set(title=\"After Normalization\")\nplt.show()","3bbcf872":"# import packages\nfrom scipy.stats import boxcox_normmax\nfrom scipy.special import boxcox1p\n\n# Now let's normalize continuous numerical predictors that are not drawn from a normal distribution.\ncols = [\"sqft_living\",\"sqft_lot\",\"sqft_basement\",\"sqft_living15\",\"sqft_lot15\"]\n\nnorm_test = lambda x: shapiro(x)[1] < 0.05 # Check if the p value is less than 0.05\nnum_feats = df[cols].apply(norm_test)\n\nfor col in cols:\n    fig, ax = plt.subplots(1,2)\n    sns.distplot(df[col],ax=ax[0])\n    skewness = format(skew(df[col]),\".2f\") ; kurt = format(kurtosis(df[col]),\".2f\")\n    ax[0].legend([\"Skewness: {}\\nKurtosis: {}\".format(skewness,kurt)],loc=\"upper_right\")\n    ax[0].set(title=\"Before Normalization\")\n    \n    df[col] = boxcox1p(df[col],boxcox_normmax(df[col]+1)) #Normalize the feature\n    sns.distplot(df[col],ax=ax[1])\n    skewness = format(skew(df[col]),\".2f\") ; kurt = format(kurtosis(df[col]),\".2f\")\n    ax[1].legend([\"Skewness: {}\\nKurtosis: {}\".format(skewness,kurt)],loc=\"upper_right\")\n    ax[1].set(title=\"After Normalization\")\n    plt.show()","e87c3247":"all_feats = df.columns.tolist()\n\n#Create an empty datatframe\npercent = pd.DataFrame(columns = [\"frequency\"],index = all_feats) \n\n# Calculate the percentage of the mode value in the feature\nfor col in all_feats:\n    percent.loc[col,\"frequency\"] = df[col].value_counts().iloc[0]\/df.shape[0]  * 100\n\nbias_feats = percent[percent[\"frequency\"] > 99.94].index.tolist() # Threshold of 99.94%\n\n##Remove the biased features\ndf = df.drop(bias_feats,axis=1)\n\n    ","a7b370b9":"# import models \nfrom sklearn.linear_model import Ridge,Lasso,ElasticNet,HuberRegressor,BayesianRidge\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor, GradientBoostingRegressor,AdaBoostRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# import necessary packages for model building\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold,cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into train and test set\ny = df.price.values ; X = df.drop(\"price\",axis=1).values # target variable would be the price and the else would be the predictors\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.2,random_state=2)\n\n# Create an empty list for pipeline \npipeline_models = []\n\n# Assign all models into the a list\nseed = 2\nmodels = [Ridge(tol=10,random_state=seed),\n          Lasso(tol=1,random_state=seed),\n          ElasticNet(random_state=seed),\n          HuberRegressor(),\n          BayesianRidge(),\n          RandomForestRegressor(random_state=seed),\n          ExtraTreesRegressor(random_state=seed),\n          BaggingRegressor(random_state=seed),\n          GradientBoostingRegressor(),\n          XGBRegressor(),\n          DecisionTreeRegressor(),\n          KNeighborsRegressor(),\n          AdaBoostRegressor(random_state=seed)]\n\nmodel_names = [\"Ridge\",\"Lasso\",\"Elastic\",\"Hub_Reg\",\"BayRidge\",\"RFR\",\"ETR\",\"BR\",\"GBoost_Reg\",\"XGB_Reg\",\"DT_Reg\",\"KNN_Reg\",\"Ada_Reg\"] # All models' labels\n\n# Assign each model to a pipeline\nfor name,model in zip(model_names,models):\n    pipeline = (\"Scaled_\"+name,\n                Pipeline([(\"Scaler\",StandardScaler()),\n                         (name,model)\n                         ]))\n    pipeline_models.append(pipeline)\n","55568e72":"## Create a dataframe to store all the model's cross validation score\nevaluate = pd.DataFrame(columns=[\"model\",\"cv\",\"std\",\"cv_all\"]) #note: \"cv\" for mean cv & \"cv_all\" for later visualization use\n\nfor name,model in pipeline_models:\n    kfold = KFold(n_splits=7) # 7 times\n    cv = cross_val_score(model,X_train,y_train,cv=kfold,n_jobs= -1,scoring=\"r2\")\n    \n    row = evaluate.shape[0]\n    evaluate.loc[row,\"model\"] = name\n    evaluate.loc[row,\"cv\"] = cv.mean()\n    evaluate.loc[row,\"std\"] = \"+\/- {}\".format(cv.std())\n    evaluate.loc[row,\"cv_all\"] = cv\n    \nevaluate = evaluate.sort_values(\"cv\",ascending=False)\n","67751d18":"## Visualization on the cv score\n\nfig, ax = plt.subplots (1,2,figsize=(18,12))\nb = sns.barplot(x=evaluate[\"model\"],y=evaluate[\"cv\"],ax=ax[0],palette = sns.cubehelix_palette(evaluate.shape[0]))\nfor rec in b.patches:\n    height = rec.get_height()\n    ax[0].text(rec.get_x()+rec.get_width()\/2,height * 1.01, round(height,4),ha=\"center\")\nax[0].set(title=\"All models' CV score\")\nax[0].set_xticklabels(evaluate[\"model\"].tolist(),rotation=90)\n\nsns.boxplot(x=evaluate[\"model\"].tolist(),y=evaluate[\"cv_all\"].tolist(),ax=ax[1])\nax[1].set(title=\"All models' CV distribution scores\")\nax[1].set_xticklabels(evaluate[\"model\"].tolist(),rotation=90)\nplt.show()","d6ce05a3":"## Create list to store all the combinations\nvotings = []\n\n# XGBRegressor only                                 \nvotings.append((\"Scaled_XGBR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                        ((\"XGB_Reg\",XGBRegressor()))\n                                       ])))\n\n# All models\nvotings.append((\"Scaled_XGBR_ETR_GBoostR_BR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                                       (\"Voting\",VotingRegressor([(\"XGB_Reg\",XGBRegressor()),\n                                                                                  (\"ETR\",ExtraTreesRegressor(random_state=seed)),\n                                                                                  (\"GBoost_Reg\",GradientBoostingRegressor()),\n                                                                                  (\"BR\",BaggingRegressor())\n                                                                                 ]))\n                                                     ])))\n\n# XGBR with ETR combinations               \nvotings.append((\"Scaled_XGBR_ETR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                            (\"Voting\",VotingRegressor([(\"XGB_Reg\",XGBRegressor()),\n                                                                       (\"ETR\",ExtraTreesRegressor(random_state=seed))\n                                                                      ]))\n                                           ])))\n\n#XGBR with ETR & GBoost_Reg combinations              \nvotings.append((\"Scaled_XGBR_ETR_GBoostR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                                      (\"Voting\",VotingRegressor([(\"XGB_Reg\",XGBRegressor()),\n                                                                                 (\"ETR\",ExtraTreesRegressor(random_state=seed)),\n                                                                                 (\"GBoost_Reg\",GradientBoostingRegressor())\n                                                                                ]))\n                                                   ])))\n\n#XGBR with ETR & BR combinations\nvotings.append((\"Scaled_XGBR_ETR_BR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                               (\"Voting\",VotingRegressor([(\"XGB_Reg\",XGBRegressor()),\n                                                                          (\"ETR\",ExtraTreesRegressor(random_state=seed)),\n                                                                          (\"BR\",BaggingRegressor())\n                                                                       ]))\n                                             ])))\n#XGBR with GBoost_Reg combinations              \nvotings.append((\"Scaled_XGBR_GBoostR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                                (\"Voting\",VotingRegressor([(\"XGB_Reg\",XGBRegressor()),\n                                                                           (\"GBoost_Reg\",GradientBoostingRegressor())\n                                                                          ]))\n                                               ])))\n\n#XGBR with GBoost_Reg & BR combinations              \nvotings.append((\"Scaled_XGBR_GBoostR_BR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                                   (\"Voting\",VotingRegressor([(\"XGB_Reg\",XGBRegressor()),\n                                                                              (\"GBoost_Reg\",GradientBoostingRegressor()),\n                                                                              (\"BR\",BaggingRegressor())\n                                                                             ]))\n                                                   ])))\n\n#XGBR with BR combinations                                 \nvotings.append((\"Scaled_XGBR_BR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                           (\"Voting\",VotingRegressor([(\"XGB_Reg\",XGBRegressor()),\n                                                                      (\"BR\",BaggingRegressor())\n                                                                     ]))\n                                           ])))\n\n#Create an empty dataframe to store the best score\nevaluate_vote = pd.DataFrame(columns=[\"model\",\"cv\",\"std\"])\n\n## train the model and assign the cv score to evaluate_vote\nfor name, model in votings:\n    kfold = KFold(n_splits=7)\n    cv = cross_val_score(model,X_train,y_train,scoring=\"r2\",cv=kfold,n_jobs=-1)\n    \n    row = evaluate_vote.shape[0]\n    evaluate_vote.loc[row,\"model\"] = name\n    evaluate_vote.loc[row,\"cv\"] = cv.mean()\n    evaluate_vote.loc[row,\"std\"] = \"+\/- {}\".format(cv.std())\n    \nevaluate_vote = evaluate_vote.sort_values(\"cv\",ascending=False)\n## Check on the scores\nprint(evaluate_vote)                       ","aff3f52c":"# Visualization\nfig, ax = plt.subplots (1,1,figsize=(18,12))\nb = sns.barplot(x=evaluate_vote[\"model\"],y=evaluate_vote[\"cv\"],ax=ax,palette = sns.cubehelix_palette(evaluate.shape[0]))\nfor rec in b.patches:\n    height = rec.get_height()\n    ax.text(rec.get_x()+rec.get_width()\/2,height * 1.01, round(height,4),ha=\"center\")\nax.set(title=\"All Combinations Voting Regressor cv scores\")\nax.set_xticklabels(evaluate_vote[\"model\"].tolist(),rotation=90)\nplt.show()","0cfd47b0":"# Models Building\nAll data should be standardize so that all the features are on the ****same scale****.","83b654e3":"# Missing Data\nIn real life scenario, missing data is common issue to be deal with. ","293ccc48":"We can see that there are a total of 20 numerical features and 1 categorical feature in this dataset. The bathrooms feature has some float values for example 2.25 (float number). The dataset didn't not specify clearly on the meaning of the \"0.25\". Upon searching the web, I found out that:\n* A full bathroom consists of : tub, sink, toilet and a separate shower.\n* A three-quarter bathroom consists of : sink, shower and toilet.\n* A half bathrooms consists of : a bathtub and a (sink or toilet).\n* A quarter bathrooms consists of : a toilet or a shower stall, It is commonly found in basement of old houses.\n\nSo the \"2.25\" would indicates the house consists of two full bathrooms and a quarter bathroom","e8e10c8f":"## Data Normalization\nThe goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information\n\n* Test for the null hypothesis that the data was drawn from a normal distribution using Shapiro-Wilk test (scipy.stats.shapiro()). If the p value is less than or equal than 0.05 (assuming a 95% confidence level), the data is not normally distibuted. We need to normaliza the data that are not drawn from normal distibution.\nVisualization the feature's distibution before and after the normalization\n\n* Skewness.\n * As a rule of thumb: skewness values greater than 1.0 or less than -1.0 is considered highly skewed and skewness values greater than 0.5 or less than -0.5 is considered moderatedly skewed.\n* Kurtosis:\n * Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution\n","65fddaa0":"## Adding New Features\nRefering to the lmplots, there are houses that do not have some features. We can add a new feature on these.\n\n","6150de06":"# Conclusion\nThe best model for this data would be Scaled Voting Regressor that fits XGBRegressor and ExtraTreesRegressor. It scored a cv value of 0.9035 . Lasso and Elastic Net, which are regularization regression algorithmns that are not suitable in this dataset.\nIt's been my pleasure to share my notebook with you. Do hit that \"upvote\" button if you enjoy reading it. Thank you !  \n","5b99168e":"It seems that missing data is not an issue in this dataset.\nNote: \n1. numerical features with missing data are replaced with the feature's median value\n2. categorical features with missing are replaced with the feature's mode value","8995a9aa":"As shown on the diagram, we can observe that:\n\n1. bedrooms : some houses don't have this feature\n1. bathrooms : some houses don't have this feature\n1. sqft_living : has linear relationship with price\n1. sqft_lot : no comment\n1. waterfront : some houses don't have this feature\n1. view : some houses don't have this feature & has linear relationship with price\n1. condition : no comment\n1. grade : has linear relationship with price\n1. sqft_above : has a strong linear relationship with price\n1. sqft_basement : some houses don't have this feature\n1. yr_built : newer house has higher price value\n1. yr_renovated : some houses have not been renovated\n1. zipcode : no comment\n1. lat : houses further to the north have higher price\n1. long : houses further to the west have higher price\n1. sqft_living15 : strong linear relationship with price\n1. sqft_lot15 : no comment\n\nNote: New columns can be created based on houses absent of certain feature. For instance, absent on yr_renovated feature,create a new column name \"renovate\" with value 1 indicates the house had been renovated or value 0 indicates house didn't renovate.","fa8a4fdc":"Warmer colors showing higher density of houses in that area. In contrast, colder colors showing low density of houses in that area","b50ea8b8":"## Train the models\nTrain all the models and evaluate each of thier performance using K-fold Cross Validation score.","f5acd3b2":"****price feature is not drawn from a normal distribution as it has a value of 4.115807574271001e-36 which is less than 0.05.**** ","970e6055":"## Biased Features\n**Remove features that are extreme bias or imbalanced**\n","ab70f4ca":"# Exploratory Data Analysis\n## Visualization\nLet's take a look on the target variable - \"price\"","4b2d3043":"****Map visualization with folium****","d08df03d":" ****Let's load out data****\n","9538c559":"****Now let's have a look at the numerical data****","b1280361":"# **Correlation Matrix**\nLet's found out each feature correlation with each other and the target variable - price","73083a37":"## Outliners\nAn outlier is an observation point that is distant from other observation.\n","46980852":"## VotingRegressor\nA voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction\n\n****Note:**** VotingRegressor works well with models that are not identical, for example: XGBRegressor with ExtraTreesRegressor. ExtraTreesRegressor and RandomTreesRegressor are highly identifical so we would just pick the one with a higher cv score (in this case ExtraTreesRegressor)","a459331b":"From the correlation matrix, we can see that sqft_living, grade, sqft_above, sqft_living15 and bathrooms have \u22650.5 correlation with the target variable. These features have great influence of the regression model performance. However we can see that, sqft_living and sqft_above have a correlation of 0.88, we would pick the one that has higher correlate value with \"price\" - in this case sqft_living. Two features with \u2265 0.8 correlate value are just like a pair of twins, the characteristics are mostly similar therefore there's no need of these \"twins feature\". ","52f5ebc9":"We can see that there are no outliers in the low range of the distribution, however there's three outliers on the high range of distribution. Remove them.\n","7439b239":"Great! After normalization, the \"price\" feature is more normally distributed.\n\n","8b50f14f":"The price distibution has a positive skeness. This indicates the majority of the population has the ability to afford low to mid range house price while a few have the ability to afford high range house price. ","d146b676":"# Data Processing\n## Missing Data\nIn real life scenario, missing data is common issue to be deal with.","560e57b9":"Great !!    XGB_Reg has the best cv score of 0.8997. Now let's use voting regressor to combine models that have cv score above 0.85. \n\nThese models are:\n* XGBRegressor\n* ExtraTreesRegressor\n* RandomForestRegressor\n* GradientBoostingRegressor\n* BaggingRegressor","d4dbfac8":"# Outline of this notebook:\n1. [Exploratory Data Analysis](#Exploratory-Data-Analysis) (EDA): This where we explore and analyze the dataset to summarize thier main characteristics : \n * [Visualization](#Visualization) \n * [Correlation Matrix](#Correlation-Matrix)\n1. [Data Processing](#Data-Processing): This is where we manipulate the explored and analyzed data to covert into meaningful information that can be used my our estimators:  \n * [Missing Data](#Missing-Data)\n * [Outliers](#Outliers)\n * [Adding New Features](#Adding-New-Features)\n * [Data Normalization](#Data-Normalization)\n * [Biased Features](#Biased-Features)\n1. [Models Building](#Models-Building) (13 models): We would first train all of the models to search for models that has high cv score. These models are then ensemble together using [VotingRegressor](#VotingRegressor) to further increase it's performance\n    * Ridge\n    * Lasso\n    * Elastic Net\n    * RandomForestRegressor\n    * ExtraTreeRegressor\n    * BaggingRegressor\n    * HuberRegressor\n    * BayesianRidge\n    * XGBRegressor\n    * DecisionTreeRegressor\n    * KNeighborsRegressor\n    * GradientBoostingRegressor\n \n [Conclusion](#Conclusion)\n\n\n"}}