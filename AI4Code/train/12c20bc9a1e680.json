{"cell_type":{"80e0f2f1":"code","925ca5aa":"code","9323d220":"code","03a843dd":"code","c2f46ed4":"code","15a52d14":"code","972ea0c3":"code","5c163868":"code","238aad17":"code","51a91ba3":"code","dbd0add8":"code","6ca09962":"code","bc8b0853":"code","a7658410":"code","98cb790f":"code","6f7b690c":"code","aaef9186":"code","0f9678e1":"code","0ac493c2":"code","71bbf674":"code","2d24468e":"code","c93001d9":"code","fd30f9d1":"code","3a3a9260":"code","91aa3297":"code","273827a2":"code","3c92e5af":"code","5a76a64a":"code","e65b48f3":"code","83bc6834":"code","ce66d6c0":"code","171c58dd":"code","23bac21e":"code","4a6ae243":"code","f03bd65c":"code","9a6c0055":"code","0af8ba67":"code","76bc87e2":"code","05552f69":"code","16d1a282":"code","df48e1cf":"code","89ecea9e":"code","b0d9f231":"code","90126699":"code","fba80c21":"code","dd2d3025":"code","38c9e7db":"code","e6a40541":"markdown","d76b8460":"markdown","1d6e0cff":"markdown","174b67f7":"markdown","4e71fd63":"markdown","7879b328":"markdown","83eb4612":"markdown","d00bfeb6":"markdown","ee227b22":"markdown","a08f1d3e":"markdown","c7fca977":"markdown","67fca949":"markdown","7c6fc58a":"markdown","1b30fd72":"markdown","716e7886":"markdown","927812e6":"markdown","1df865ca":"markdown","75e4e3ca":"markdown","cc694b99":"markdown","10e63f1a":"markdown","94dcecb7":"markdown","2946e09b":"markdown","1660e335":"markdown","302a9e0f":"markdown","3a4e04a3":"markdown","7e1e4e1b":"markdown","373f449e":"markdown","8e4e58bd":"markdown","0e3df67f":"markdown","f301306b":"markdown","6f51e19b":"markdown","2cf20af8":"markdown"},"source":{"80e0f2f1":"# the following three lines are suggested by the fast.ai course\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n# hide Kaggle notebook warnings\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Importing fastai library\nfrom fastai import *\nfrom fastai.vision import *","925ca5aa":"# to get all files from a directory\nimport os\n\n# to easier work with paths\nfrom pathlib import Path\n\n# to read and manipulate .csv-files\nimport pandas as pd\n\n# For linear algebra processes e.g. manipulating arrays.\nimport numpy as np \n","9323d220":"INPUT = Path(\"..\/input\/digit-recognizer\")\nos.listdir(INPUT)","03a843dd":"train_df = pd.read_csv(INPUT\/\"train.csv\")\ntrain_df.head(3)","c2f46ed4":"test_df = pd.read_csv(INPUT\/\"test.csv\")\ntest_df.head(3)","15a52d14":"# CREATING PATHS\nTRAIN = Path(\"..\/train\")\nTEST = Path(\"..\/test\")","972ea0c3":"# CREATING TRAINING DIRECTORIES\n\nfor index in range(10):\n    try:\n        os.makedirs(TRAIN\/str(index))\n    except:\n        pass","5c163868":"# CONFIRMING LABELS\n\nsorted(os.listdir(TRAIN))","238aad17":"# CREATING TEST DIRECTORY\n\ntry:\n    os.makedirs(TEST)\nexcept:\n    pass","51a91ba3":"os.listdir(TEST) ","dbd0add8":"if os.path.isdir(TRAIN):\n    print('Train directory has been created')\nelse:\n    print('Train directory creation failed.')\n\nif os.path.isdir(TEST):\n    print('Test directory has been created')\nelse:\n    print('Test directory creation failed.')\n\n# I was having problems with the directory objects so this is\n# just troubleshooting","6ca09962":"#Numpy already imported as np\n\n# import PIL to create images from arrays\nfrom PIL import Image\n\ndef saveDigit(digit, filepath):\n    digit = digit.reshape(28,28)\n    digit = digit.astype(np.uint8) # uint8 is the image type as 2^8-1 is the highest pixel value possible\n\n    img = Image.fromarray(digit)\n    img.save(filepath)","bc8b0853":"# Using panda library & its directory objects\n# SAVING TRAINING IMAGES\nfor index, column in train_df.iterrows(): #for all rows in file\n    \n    label,digit = column[0], column[1:] #label is first column & digit is rest\n    digit = digit.values                #i.e. digit is now a 1x784 array, just one row\n   \n    folder = TRAIN\/str(label) #create folder based on label of row\n    filename = f\"{index}.jpg\" #arbitrary index from loop\n    filepath = folder\/filename #filepath now is the directory of the 'digit' in the correct folder\/label.\n                                \n    # Lastly, using the saveDigit function I created to save & convert the \n    # 1x784 array 'digit' into an image and into the 'filepath' directory,\n    # filepath directory\n    saveDigit(digit, filepath)","a7658410":" # SAVING TESTING IMAGES\nfor index, digit in test_df.iterrows(): \n\n    folder = TEST\n    filename = f\"{index}.jpg\"\n    filepath = folder\/filename\n    \n    digit = digit.values # .iterrows is row-by-row loop so digit is 1x784\n    \n    saveDigit(digit, filepath)\n","98cb790f":"import matplotlib.pyplot as plt\n\ndef displayTestingData():\n    fig = plt.figure(figsize=(5, 10))\n    \n    paths = os.listdir(TEST)\n    \n        \n    for i in range(1, 51):\n        randomNumber = random.randint(0, len(paths)-1)\n        image = Image.open(TEST\/paths[randomNumber])\n        \n        ax = fig.add_subplot(10, 5, i)\n        ax.axis(\"off\")\n        \n        plt.imshow(image, cmap='gray')\n    plt.show()","6f7b690c":"print(\"TESTING DATA SAMPLES\")\ndisplayTestingData()","aaef9186":"# LOOKING AT ONE SINGLE IMAGE UP CLOSE \n\nimage_path = TEST\/os.listdir(TEST)[6]\nimage = Image.open(image_path)\nimage_array = np.asarray(image)\n\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nimg = ax.imshow(image_array, cmap='gray')\n\nfor x in range(28):\n    for y in range(28):\n        value = round(image_array[y][x]\/255.0, 2)\n        color = 'black' if value > 0.5 else 'white'\n        ax.annotate(s=value, xy=(x, y), ha='center', va='center', color=color)\n\nplt.axis('off')\nplt.show()","0f9678e1":"# Also - A simple function I found from fastai that is able to print quickly.\n# but verryyy small -- most likely real size 28x28\ntestData = ImageList.from_folder(TEST)\ntestData.open(testData.items[5])","0ac493c2":"# Specifying transformation object not to flip images for obvious reasons.\ntfms = get_transforms(do_flip=False)","71bbf674":"np.random.seed(9)","2d24468e":"# More directory troubleshooting\nprint('test : ',TEST)\nprint('train: ', TRAIN)\nprint(type(TEST)) # I kept getting a PosixPath type error ","c93001d9":"data = ImageDataBunch.from_folder(\n    path = (\"..\/train\"), # using the actual string location works!\n    test = (\"..\/test\"),\n    valid_pct = 0.15, #15% of training set split into validation\n   #valid_pct=0.2\n    # bs = 16,\n    bs = 256,\n    ds_tfms = tfms,\n    size = 28,\n    num_workers = 0\n    )\n\ndata.normalize(imagenet_stats)","fd30f9d1":"data.test_ds.x[0] # showing first element of test data","3a3a9260":"# DISPLAYING TRAINING DATA WITH LABELS !! ITS WORKING\n\ndata.show_batch( rows=3, figsize=(7,8) ) #\/\/should display 3 rows of images w the class ontop \n","91aa3297":"    \nlen(data.train_ds), len(data.valid_ds), data.c, len(data.test_ds)","273827a2":"imagenet_stats, mnist_stats","3c92e5af":" doc(cnn_learner)","5a76a64a":"modelCNN = cnn_learner(\n    data, \n    base_arch = models.resnet34, \n   # DROPOUT RATE IS DEFAULT SET TO 0.5 i.e 50%\n    metrics = accuracy,\n    model_dir=\"\/tmp\/models\", \n    callback_fns=ShowGraph \n    )","e65b48f3":"# Looking at model architecture\n\nmodelCNN.model","83bc6834":"modelCNN.fit_one_cycle(3)","ce66d6c0":"modelCNN.save('model34-3')","171c58dd":"modelCNN.load('model34-3')","23bac21e":"# UNFREEZING THE LAYERS !\n\nmodelCNN.unfreeze()","4a6ae243":"# LR_FIND() METHOD\n\nmodelCNN.lr_find()","f03bd65c":"modelCNN.recorder.plot()","9a6c0055":"modelCNN.fit_one_cycle( 30, slice(1e-3, 1e-2))","0af8ba67":"# VIEWING SOME PREDICTIONS FROM TRAINING\n\nmodelCNN.show_results(3, figsize= (7,7))","76bc87e2":"# VIEWING TOP 9 IMAGES WITH HIGHEST LOSSES\n\ninterp = ClassificationInterpretation.from_learner(modelCNN)","05552f69":"interp.plot_top_losses(9, figsize=(7, 7))","16d1a282":"# CONFUSION MATRIX\n\ninterp.plot_confusion_matrix()","df48e1cf":"# USING .get_preds(DatasetType.Test) WHICH RETURNS EACH CLASS PREDICTION\n# FOR object 'y' i.e. the image.\n\nclass_score , y = modelCNN.get_preds(DatasetType.Test)","89ecea9e":"probabilities = class_score[0].tolist()\n\n# For loop running prediction percentages for one object.\n[f\"{index}: {probabilities[index]}\" for index in range(len(probabilities))]","b0d9f231":"class_score = np.argmax(class_score, axis=1)","90126699":"class_score[0].item()","fba80c21":"# VIEWING THE CORRECT FORMAT GIVEN TO US\n\nsample_submission =  pd.read_csv(INPUT\/\"sample_submission.csv\")\ndisplay(sample_submission.head(2))\ndisplay(sample_submission.tail(2))\n","dd2d3025":"# CREATING MY SUBMISSION FILE\n\n# # # Fixing up ImageID\n# remove file extension from filename\nImageId = [os.path.splitext(path)[0] for path in os.listdir(TEST)]\n# typecast to int so that file can be sorted by ImageId\nImageId = [int(path) for path in ImageId]\n# +1 because index starts at 1 in the submission file\nImageId = [ID+1 for ID in ImageId]\n\n","38c9e7db":"# Using class_score object from prediction as labels.\nsubmission  = pd.DataFrame({\n    \"ImageId\": ImageId,\n    \"Label\": class_score\n})\n# submission.sort_values(by=[\"ImageId\"], inplace = True)\nsubmission.to_csv(\"submission.csv\", index=False)\ndisplay(submission.head(3))\ndisplay(submission.tail(3))","e6a40541":"* Regarding my implementation, I will utilize the transfer learning technique on fastAI's ResNet34 CNN model architechture. There is a ResNet18 & a Resnet50 but I will stick to the 34 size.\n* Note that you can build your own architecture from scratch as I've done before using Keras, but that method is more time-consuming as you have to manually input the layers, its neurons, activation functions, feature maps, pooling layers, which all contain little configurations such as padding and feature map sizing etc...This most likely will not end up being as robust or even accurate of a model as the ResNet.","d76b8460":"## Saving training & testing images loops & correct filepath :-\n* Using panda library to extract the label & 1x784 arrays (digit)\n* Creating folder directory using label\n* Creating a filepath directory object appropriate to digit at hand\n* Running the 'digit' and 'filepath' through the saveDigit() function.\n\nNote: For testing images there are no labels.","1d6e0cff":"The files are in the proper structure.","174b67f7":"The \"ImageDataBunch\" class in fastai is the data object that the model uses and it sees folder names as labels, thus the images must be in a folder structure like so:\n\n`train\\0\\ arbitraryName.jpg  \n         \\ arbitraryName.jpg\n          .....etc\n       \\1\\ arbitraryName.jpg\n          ....etc\n       \\2\\ arbitraryName.jpg\n          ...etc`\n          \nWhere \\0 && \\1 && \\2 etc. up to 9 are the folders\/labels i.e. the class\/digit in the training folder. Which is dictated by the first column in the .csv file.\n\nThe test folder would look like this since there are no labels.\n    \n`test\\\n      arbitraryName.jpg\n      ....etc`\n\n\n     ","4e71fd63":"<a id=\"LRfinder\"><\/a>\n## With just 3 epochs it achieved a high accuracy. Now lets unfreeze and use learning rate finder method!","7879b328":"### So, now I will  create these folder structures :-\n","83eb4612":"( # of training images, # of validation images, # of classes, # of test images)","d00bfeb6":"This tells me there are 3 .csv files contained within the \/digit-recognizer path, i.e. folder.\n\n* Note : Now \"INPUT\" path object leads to our data.","ee227b22":"<a id=\"Model-Architecture\"><\/a>\n## Brief Inspection of Model Architecture\nLooks exactly like Keras.\n- Kernel sizes are the feature detector, \n- ReLU function to prevent linearity in images, \n- Also using the common practice of doubling the neurons as more layers are put on,\n- MaxPooling to further highlight the significant parts as it only contains the highest values of the Feature Map, \n- After a couple layers the stride size of the feature map goes down.\n- Flattens all pooled feature map pixel values into a column (vector)\n- Flattened vector is the input values for the final fully connected ANN layers (they did throw in ANN layers in the middle as well, in my experience I've only added them on top at the end)\n    - The fully connected ANN is integral for training as it adjusts all parameters (weights, attributes, EVEN FEATURE DETECTORS etc.) through the training process, which amplifies the CNNs capability.\n\nBasically in the form of: \nConvolution layer ( Input Image -> Feature detector -> Feature map -> ReLU) \n-> POOLING LAYER ( Feature Map -> Pool Feature Detector -> Pooled Feature map). For every feature map we create a pooled feature map.\n-> Flattened pooled feature map -> ANN -> Train\n\nAll the basic concepts I've learnt, just put into play in a larger complex scale.\n\n<img src=\"https:\/\/media.giphy.com\/media\/3oKIPlLZEbEbacWqOc\/giphy.gif\">","a08f1d3e":"<a id=\"Submission\"><\/a>\n# Creating submission file:-","c7fca977":"<a id=\"TestData\"><\/a>\n# Displaying test data using matplotlib\n* I will display the training data shortly, using fastAI's DataBunchObject as it is more convenient. There seems to be no way to visualize the test data appropriately using fastAI.","67fca949":"**DONT FORGET TO TURN ON GPU BEFORE TRAINING**\n<a id=\"Techniques\"><\/a>\n## Techniques used :-\n* ***Pre processed data*** by data wrangling & putting into correct structure\n    * Involved converting csv file into images, done using numpy & panda & self made functions to extract files & labels and convert arrays into images.\n* ***Data visualization*** for inspection.\n    * Utilizing matplotlib.\n* ***Transfer learning*** technique using fastAI utilizing resnet34 CNN model architecture.\n  * Involved unfreezing layers, use LR_find() method to find a good range of learning rate to then slice and use as range when training entire model. LR_find() method is an approach proposed in [a paper by Leslie Smith](http:\/\/arxiv.org\/abs\/1803.09820) that was later implemented into the fastAI library as LR_find().\n* Implemented techniques to ***optimize model*** such as:\n  * Dropout probability - randomly deactivates nodes to reduce overfitting.\n  * Transforming and normalizing the data to ensure efficiency and spatial variance.\n  * Usual data augmentation such as zoom, rotate, lighting etc.\n  \n=============================================================\n=============================================================\n\n# TABLE OF CONTENTS :-\n   \n   - [Introduction](#Understand)\n   \n   - [Brief Description of Techniques Used](#Techniques)\n\n- **PRE-PROCESSING**\n    - [Data Wrangling & What is It?](#Data-Wrangling)\n    - [Converting .csv to .jpg](#Reshaping-Array)\n    - [Inspecting the Test Data](#TestData)\n- **CREATING DATA OBJECT**\n    - [Initializing the Data Object](#Initializing-Data-Object)\n    - [Inspecting the Data Object](#Inspecting-Data-Object)\n- **REVIEWING ResNet34 CNN CONCEPTUALLY AND ARCHITECTURALLY**\n    - [Examining the ResNet34 Model Architecture from FastAI - CNN](#Model-Architecture)\n- **CREATING MODEL & TRAINING**\n    - [Initializing the CNN](#CNNinitialize)\n    - [Transfer Learning & What is It?](#TransferLearning)\n    - [Learning Rate Finder Method](#LR-Finder)\n- **PREDICTING & EVALUATION**\n    - [Predicting After Model is Trained](#Prediction)\n    - [Evaluation After Training](#Evaluation)\n- **RE-STRUCTURING PREDICTIONS TO SUBMISSION FILE FORMAT SPECIFIED**\n    - [Creating Submission .csv File](#Submission)\n","7c6fc58a":"## Importing fastai libraries:-\n- importing fastai library which runs ontop of PyTorch framework.","1b30fd72":"***saveDigit(digit, filepath) :-***\n* 'digit' argument is the 1x784 array that gets converted to a 28x28 array of type unint8, which then gets converted to a .jpg image and saved to the 'filepath' argument object.\n* I have to ensure the filepath object that will be given to the function is the appropriate one for the digit object.","716e7886":"<a id=\"Inspecting-Data-Object\"><\/a>\n### Inspecting the data object :-","927812e6":"### FINDING LR RANGE :-\nI can see that the loss starts to go down at a LR of 1e-04ish and proceeds to keep going down until it starts to pick up again at 1e-02, at 1e-03 it seems to be a little steeper so I will try a range of 1e-03 -> 1e-02 , so that the LR starts off at 1e-03 and gradually increases to 1e-02\n","1df865ca":"<a id=\"Data-Wrangling\"><\/a>\n# Data Wrangling:-\nAccording to the fastai documentation for computer vision, it only accepts image files. But I am given .csv files so I must perform **Data Wrangling** i.e. converting one data format to another more appropriate and valuable format, which is an image in this particular case.\n\n* So I have to convert the .csv files which contain the pixel value for each pixel (represented in columns) into images.\n\nfastai documentation source : https:\/\/docs.fast.ai\/vision.data.html#ImageDataBunch","75e4e3ca":"Looking into the test & train csv files. Note: sample_submission is just a structural sample to follow when I do submit the final submission.","cc694b99":"<a id=\"Understand\"><\/a>\n# Understanding the problem :-\nThe goal of this problem is to be able to take an image of a handwritten digit and determine what that digit is. It is judged on the accuracy. i.e. the error rate.\n\n### LOOKING AT THE DATA :-\n### *train.csv*\n* Greyscale images from zero through nine\n* 42,000 images\n* File contains all necessary information for training the model\n* Each row is one image i.e. an image is a 1D array of 1x784\n* First column of each image is the label. It tells us which digit is shown.\n* Other 784 columns are the pixels for each digit, they should be read like this when in image-square format as sqr(784) = 28\n\n`000 001 002 003 ... 026 027`  \n`028 029 030 031 ... 054 055`  \n`056 057 058 059 ... 082 083`  \n` |   |   |   |  ...  |   |  `  \n`728 729 730 731 ... 754 755`  \n`756 757 758 759 ... 782 783`  \n\n\n### *test.csv*\n* greyscale images from zero through nine\n* structure is the same as in train.csv, but there are no labels thus only 784 columns not 785.\n* these 28,000 images are used later to test how good our model is\n\n### *sample_submission.csv*\n* show us, how to structure our prediction results to submit them to the competition\n* 28,000 images\n* we need two columns: ImageId and Label\n* the rows don't need to be ordered\n* the submission file should look like this:\n\n`ImageId, Label`  \n`1, 3`  \n`2, 4`   \n`3, 9`  \n`4, 1`  \n`5, 7`  \n`(27995 more lines)`\n\n### What is MNIST?\nMNIST is the perfect dataset to get started learning more about pattern recognition and machine learning. That is why people call it the \"Hello World of machine learning\". It's a large database of handwritten digits. There are a total of 70.000 grayscale images, each is 28x28 pixels. They show 10 different classes representing the numbers from 0 to 9. The dataset is split into 60.000 images in the training set and 10.000 in the test set. This competition is based on the MNIST dataset. However, the train-test distribution is different. Here, there are 42.000 images in the training set and 28.000 images in the test set. MNIST was published by the godfather of CNNs, Yann LeCun.\n","10e63f1a":"<a id=\"Initializing-Data-Object\"><\/a>\n# Initializing ImageDataBunch object i.e. loading data\n**INITIALIZING OUR DATA OBJECT WITH 'ImageDataBunch' :-**\n* 'ImageDataBunch' is an object that represents all the data needed to train a CNN model. It also has factory methods within it that generates the validation and training data (The validation set is a set that model never gets to look at and is taken from its training set, i.e a test\/validation set while training, to prevent overfitting)\n    * In fastAI all the data objects will be \"DataBunch\" objects that will have all the data augmentation and extraction and normalization components in it etc. For images its just \"ImageDataBunch\"\n* Uses of the ImageDataBunch :-\n    * Is our data object that the model object will use.\n    * Can configure hyperparameters such as validation split percentage, batch count, size of image, number of workers, transformation (i.e. data augmentation). \n    * Note: The transformation argument takes a \"get_transforms()\" object as a parameter. This object already has pre-existing settings for augmentation that have proven to be useful such as rotation, zooming, lighting, warping, etc. More can be seen here in the functions documentation. https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/vision\/transform.py#L308\n    * Normalization also has its pre-existing settings from fastai, but after looking into the document I found that they have a setting specifically for MNIST data titled \"mnist_stats\" so I will use that as my normalization setting \/\/I attempted another pre-existed setting \"imagenet_stats\" which proved to do better.\n    \n    \n**np.random.seed( ):-**\n* The \"ImageDataBunch\" object creates a validation set randomly (using numpy) each time the code block is run.\n* So to maintain reproducibility of the model, you set the 'seed' (i.e. the specific set of data\/images) of which the \"ImageDataBunch\" objects validation set gets randomly chosen from.\n* The integer parameter used is arbitrary, but you just have to use that same number again for reproducibility otherwise the model is using a different validation set every time it runs. \n\n","94dcecb7":"P.S. Kaggle input folders are always read-only so create folders outside of input folder.","2946e09b":"<a id=\"CNNinitialize\"><\/a>\n# Data is ready - CNN LEARNING TIME:-","1660e335":"## We only need the highest chance prediction :-\n### For this one it would be a '1'.","302a9e0f":"# Note I am currently working on fastAIs course, this is to help me learn.\n- This is for me to practice and develop my data science skills using the FastAI library.\n- I will be adding many comments as I learn and figure things out.","3a4e04a3":"<a id=\"TransferLearning\"><\/a>\n# On Transfer Learning:-\n* **It is basically using a pre-trained model annndd..**\n    1. Training the last layers of it so as to not corrupt its foundations as during gradient descent (re-calibration of weights based on errors) the model will configure the weights equally throughout, i.e. weights from layer 1 will be affected as much as layer 30. This is a big no-no as the layers in the model correlate to fundamentals of vision, say layer 1-5 is figuring out shapes and what they mean, then layers 5-10 might be it figuring out that these shapes are different objects and layers 25-30 could be it distinguishing between two similiar faces.\n    2. Then 'unfreezing' all the layers and use the learning rate finder method to figure out a good learning rate sweetspot to train the entire model, BUT this method involves index slicing the learning rate, so that the beginning layers are not affected as much as the later ones. Specifically by examining the validation\/loss function and finding where the loss starts to decrease. It is an approach based off [a paper by Leslie Smith](http:\/\/arxiv.org\/abs\/1803.09820) that has been incorporated into the fastAI library as lr_find(). \n* In this case I am using Resnet34 which has been trained on looking at ~1.5mil pics of things and 1,000 classes\/categories using an image set called ImageNet. This makes everything easier, and most importantly MORE EFFECTIVE!!","7e1e4e1b":"<a id=\"Evaluation\"><\/a>\n# EVALUATION :-","373f449e":"Ya I'm not dissapointed, I would get these wrong as well.","8e4e58bd":"<a id=\"Reshaping-Array\"><\/a>\n# Reshaping array & converting to image :-\n\nQUICK NOTES:-\n* Panda to extract label and rows i.e. the flat array images (1x784)\n* Numpy to reshape each flat array (1x784) image row into square image (28x28)\n* PIL library to create .jpg image from the (28x28) numpy array via its .fromarray function.\n* Creating python code - Defining functions for easier processing, and FOR loops to save files into training folder and testing folder. Note that training folder images must be assigned to correct labeled folder so I have to extract the 1st column of training image and save the training images in the appropriate folder based on the label.\n\n### Creating a function that recieves a 1x784 array (one image) , and file path. Which then converts vector to 28x28 and puts into its file path.","0e3df67f":"<a id=\"Prediction\"><\/a>\n# PREDICTIONS :-","f301306b":"## **Data processing libraries:-**","6f51e19b":"<img src=\"https:\/\/media.giphy.com\/media\/YaJknABE4uFUY\/giphy.gif\">\n\n<a id=\"Techniques\"><\/a>\n## Brief Description of Techniques used :-\n* ***Pre processed data*** by data wrangling & putting into correct structure\n    * Involved converting csv file into images, done using numpy & panda & self made functions to extract files & labels and convert arrays into images.\n* ***Data visualization*** for inspection.\n    * Utilizing matplotlib.\n* ***Transfer learning*** technique using fastAI utilizing resnet34 CNN model architecture.\n  * Involved unfreezing layers, use LR_find() method to find a good range of learning rate to then slice and use as range when training entire model. LR_find() method is an approach proposed in [a paper by Leslie Smith](http:\/\/arxiv.org\/abs\/1803.09820) that was later implemented into the fastAI library as a LR_find() function for the model.\n* Implemented techniques to ***optimize model*** such as:\n  * Dropout probability - randomly deactivates nodes to reduce overfitting.\n  * Transforming and normalizing the data to ensure efficiency and spatial variance.\n  * Usual data augmentation such as zoom, rotate, lighting etc.\n  \n=============================================================\n=============================================================\n\n# TABLE OF CONTENTS :-\n   \n   - [Introduction](#Understand)\n   \n   - [Brief Description of Techniques Used](#Techniques)\n\n- **PRE-PROCESSING**\n    - [Data Wrangling & What is It?](#Data-Wrangling)\n    - [Converting .csv to .jpg](#Reshaping-Array)\n    - [Inspecting the Test Data](#TestData)\n- **CREATING DATA OBJECT**\n    - [Initializing the Data Object](#Initializing-Data-Object)\n    - [Inspecting the Data Object](#Inspecting-Data-Object)\n- **REVIEWING ResNet34 CNN CONCEPTUALLY AND ARCHITECTURALLY**\n    - [Examining the ResNet34 Model Architecture from FastAI - CNN](#Model-Architecture)\n- **CREATING MODEL & TRAINING**\n    - [Initializing the CNN](#CNNinitialize)\n    - [Transfer Learning & What is It?](#TransferLearning)\n    - [Learning Rate Finder Method](#LR-Finder)\n- **PREDICTING & EVALUATION**\n    - [Predicting After Model is Trained](#Prediction)\n    - [Evaluation After Training](#Evaluation)\n- **RE-STRUCTURING PREDICTIONS TO SUBMISSION FILE FORMAT SPECIFIED**\n    - [Creating Submission .csv File](#Submission)\n","2cf20af8":"# 99.6 % accuracy - training time 35 mins ! SUCCESS\n<img src=\"https:\/\/media.giphy.com\/media\/3o7bu6t4kZRB906jGo\/giphy.gif\">\n"}}