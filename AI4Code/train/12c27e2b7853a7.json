{"cell_type":{"f1688473":"code","4214ce53":"code","d1f6965a":"code","e103fb73":"code","7a317a97":"code","dea8c7a0":"code","342b6155":"code","4395dd0c":"code","bda12189":"code","366d23a0":"code","30cafc47":"code","04198dfb":"code","6335dadc":"code","89c3fc7f":"code","5cc98943":"code","3b8cebe1":"code","0053ad39":"code","b5bdd774":"code","0faf2f69":"code","dcca280a":"code","0dfba452":"code","9b87649f":"code","9408e874":"code","c167a5fd":"code","e4681a86":"code","cfb4a6c4":"code","af191bf8":"code","b41929b8":"markdown","ba20577c":"markdown","5419f315":"markdown","d4aec98a":"markdown"},"source":{"f1688473":"# 1. Import library\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport pandas as pd\nimport seaborn as sns","4214ce53":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","d1f6965a":"# 2. Split data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train.iloc[:, 1:].values, train.iloc[:, 0].values, test_size=0.1)","e103fb73":"# 3. Reshape N * 28 * 28 * 1 to train\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)","7a317a97":"# 4. One hot encoding label (Y)\ny_train = np_utils.to_categorical(y_train, 10)\ny_test = np_utils.to_categorical(y_test, 10)\nprint('Data y origin ', y_train[0])\nprint('Data y one-hot encoding ',y_train[0])","dea8c7a0":"# 5. Model definition\nmodel = Sequential()\n \n# Th\u00eam Convolutional layer with 32 kernel, kernel size 3*3\n# sigmoid function\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n# Add Convolutional layer\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n# Flatten layer convert tensor to vector\nmodel.add(Flatten())\n\n# Add Fully Connected layer with 128 nodes\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n# Output layer with 10 node\nmodel.add(Dense(10, activation='softmax'))\n\n# model = keras.models.load_model('my_model')\n\nmodel.summary()","342b6155":"# 6. Compile model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","4395dd0c":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        rotation_range= 10,\n        zoom_range = 0.1,\n        width_shift_range = 0.1,\n        height_shift_range = 0.1\n)\n\ndatagen.fit(X_train)","bda12189":"train_generator = datagen.flow(X_train, y_train, batch_size = 64)\n\nvalidation_generator = datagen.flow(X_test, y_test, batch_size = 64)","366d23a0":"y_train.shape","30cafc47":"### %%time\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n# 7. Train\nnumOfEpoch = 300\nH = model.fit(train_generator, \n                epochs=numOfEpoch,\n                validation_data = validation_generator,\n                verbose=1,\n                callbacks=[es, reduce_lr])","04198dfb":"nume = len(H.history['loss'])\n# 8. Map loss, accuracy\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey = False)\nax1.plot(np.arange(0, nume), H.history['loss'], label='training loss')\nax1.plot(np.arange(0, nume), H.history['val_loss'], label='validation loss')\nax2.plot(np.arange(0, nume), H.history['accuracy'], label='accuracy')\nax2.plot(np.arange(0, nume), H.history['val_accuracy'], label='validation accuracy')\nax1.set(title='Loss', xlabel='Epoch', ylabel='Loss')\nax2.set(title='Accuracy', xlabel='Epoch', ylabel='Accuracy')\nplt.legend()\nplt.show()","6335dadc":"# 9. Evaluate Model\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint(score)","89c3fc7f":"# 10. Predict\nplt.imshow(X_test[0].reshape(28,28), cmap='gray')\n\ny_predict = model.predict(X_test[0].reshape(1,28,28,1))\nprint('Value: ', np.argmax(y_predict))","5cc98943":"from keras.models import Model\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(X_train[10].reshape(1,28,28,1))\n \ndef display_activation(activations, col_size, row_size, act_index): \n    activation = activations[act_index]\n    activation_index=0\n    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*4, col_size*1))\n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n            activation_index += 1","3b8cebe1":"# origin image\nplt.imshow(X_train[10][:,:,0]);","0053ad39":"# layer 1\ndisplay_activation(activations, 8, 4, 0)","b5bdd774":"# layer 2\ndisplay_activation(activations, 8, 4, 1)","0faf2f69":"# layer 3\ndisplay_activation(activations, 8, 4, 2)","dcca280a":"from sklearn.metrics import confusion_matrix\nY_prediction = model.predict(X_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_prediction,axis = 1) \n# Convert validation observations to one hot vectors\ny_onehot = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_onehot, Y_pred_classes) ","0dfba452":"plt.figure(figsize=(10,8))\nsns.heatmap(confusion_mtx, annot=True, fmt=\"d\");","9b87649f":"# Show some wrong results, and the difference between the predicted label and the real labe\n\nY_true = np.argmax(y_test, axis = 1)\ny_pred = model.predict(X_test)\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = y_pred[errors]\nY_true_errors = Y_true[errors]\nX_test_errors = X_test[errors]\n\ndef display_errors(errors_index, img_errors, pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True, figsize=(nrows*4, ncols*2))\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_test_errors, Y_pred_classes_errors, Y_true_errors)","9408e874":"sub = test.iloc[:, :].values\nsub = sub.reshape(sub.shape[0], 28, 28, 1)\nsub.shape","c167a5fd":"%time\ny_pred = model.predict(sub)","e4681a86":"y_sub = np.argmax(y_pred, 1)","cfb4a6c4":"# 11. Submit\nsubmission = pd.DataFrame({'ImageId': np.arange(1, 28001), 'Label': y_sub})\nsubmission.to_csv(\"submission.csv\", index = False)\nprint(\"Your submission was successfully saved!\")","af191bf8":"model.save_weights(\"my_model.h5\")","b41929b8":"### Confusion matrix","ba20577c":"### Visualize CNN Layers","5419f315":"Model definition:\n1. Model = Sequential() To tell keras, we will lay the layers on top to create the model. Ex: input -> CONV -> BN -> MAXPOOL -> DROPOUT -> CONV -> BN -> MAXPOOL -> DROPOUT -> FLATTEN -> FC -> BN -> DROPOUT -> OUTPUT\n2.  First layer, input_shape = (W, H, D), image shape (28, 28) => input_shape = (28, 28, 1)\n3. When adding an Convolutional layer, we need to specify the parameters: K (layer), kernel size (W, H), using activation functions.\n4. When adding Maxpooling Layer, specify the size of the kernel, model.add(MaxPooling2D(pool_size=(W, H)))\n5. The Flatten step of converting from tensor to vector just adds the flatten layer.\n6. model.add(Dense(the_numbers_node activation= 'activation function'))\n","d4aec98a":"convert one-hot encoding label Y (4) to vector [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]"}}