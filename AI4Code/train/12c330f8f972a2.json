{"cell_type":{"41af20f9":"code","aeaa623c":"code","ef39a750":"code","1cf66887":"code","81b2513b":"code","29a88fd7":"code","ac8f8f42":"code","0edd07dd":"code","6422a208":"code","512d5708":"code","a743b983":"code","7bfa02bf":"code","f2b0c9db":"code","0564eeb5":"code","a1893077":"code","9fb6d1ca":"code","20e059bf":"code","59d39150":"code","c8264b08":"code","1a8dd3f5":"code","302dce73":"code","b23683a5":"code","223b14b9":"code","4de36df6":"code","0f4db8a6":"code","09c562f8":"code","d3fefb57":"code","c4136959":"code","32d26272":"code","41860fba":"code","fe1464e5":"code","ba949cf1":"code","e8d7ec60":"code","91d6f09b":"code","c0b30860":"code","d6516b1f":"code","978ba776":"code","9142712a":"markdown","ed188121":"markdown","7ee09708":"markdown","f5c1f7b9":"markdown","3ed10160":"markdown","da2477c1":"markdown","bc1325e7":"markdown","4339d292":"markdown","d451f436":"markdown","6d893fe8":"markdown","0e383bd4":"markdown","98dae8a6":"markdown","0d8a943c":"markdown","759cf939":"markdown","f7c67ec5":"markdown","e7f7fc61":"markdown","284e6c5b":"markdown","9cdfd11d":"markdown","4acbff96":"markdown","233e8922":"markdown","a22cedb8":"markdown","db9f4282":"markdown","a9e4a7d6":"markdown","6b82fe15":"markdown","94be60af":"markdown","0209ac97":"markdown","375055ac":"markdown","daac67af":"markdown","52fc43ab":"markdown","0af4c754":"markdown","651fe9ee":"markdown","8e3506ec":"markdown","d4ece3ff":"markdown","381b4a2c":"markdown"},"source":{"41af20f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aeaa623c":"from pprint import pprint \n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n# Preprocessor\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Model Selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\n# Models\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor\n\n# feature selection\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.feature_selection import RFECV\n\n# metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import make_scorer\n\n# Statistics\nfrom scipy import stats\nfrom scipy.special import boxcox, inv_boxcox\n\n# Seaborn\nimport seaborn as sns\n\n# Yellowbrick Visualizations\nfrom yellowbrick.model_selection import LearningCurve\nfrom yellowbrick.regressor import PredictionError\nfrom yellowbrick.regressor import CooksDistance\nfrom yellowbrick.regressor import ResidualsPlot\nfrom yellowbrick.model_selection import ValidationCurve\n\n# Matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42","ef39a750":"def map_poly_feature_names(features, poly_features):\n    \"\"\" \n    Given a list of the original feature names and new feature names generated from a polynomial model, maps the new feature names to the\n    original feature names.\n    \"\"\"\n    \n    d = {}\n    \n    # Maps the i-ith x to the i-th feature \n    # e.g. 0, 1, and 2 is converted to x0, x1, and x2 respectively then mapped such that d['x0'] = feature0\n    for i, feature in enumerate(features):\n        s = 'x'+ str(i)\n        d[s] = feature\n        \n    # Maps x0, x1, ... to their respective feature names\n    poly_feature_names = []\n    for feature in poly_features:\n        for key in d:\n            if key in feature:\n                feature = feature.replace(key, d[key])\n        poly_feature_names.append(feature)\n    return poly_feature_names","1cf66887":"def add_poly_features(df, degree, bias, interaction, target):\n    \"\"\"\n    Given a dataframe, polynomial parameters, and the target column, fits and transforms the numerical columns to have polynomial features.\n    \"\"\"\n    \n    isTargetNumerical = True if df[target].dtype in ['int64', 'float64'] else False\n    \n    # If target is an int or float, we want to exclude it\n    if isTargetNumerical:\n        num_cols = list(df.select_dtypes(include=['int64','float64']).drop(target, axis=1))\n        cat_cols = list(df.select_dtypes(include=['object']))\n    else:\n        num_cols = list(df.select_dtypes(include=['int64','float64']))\n        cat_cols = list(df.select_dtypes(include=['object']).drop(target, axis=1))\n    \n    # Polynomial Model\n    poly = PolynomialFeatures(degree, interaction_only=interaction, include_bias = bias)\n    \n    # Fit and transform numerical columns\n    poly_num_X = poly.fit_transform(df[num_cols])\n    \n    # Extract new feature names model\n    poly_feature_names = poly.get_feature_names()\n    \n    # Map new feature names to the appropriate original feature names\n    poly_feature_names = map_poly_feature_names(num_cols, poly_feature_names)\n    \n    # Combine new polynomial features with exisiting categorical columns\n    poly_num_X = pd.DataFrame(poly_num_X, columns=poly_feature_names)       \n    poly_X = poly_num_X.join(df[cat_cols], how='inner')\n                        \n    # Combine target with new DataFrame such that it's the right most column\n    poly_X = poly_X.join(df[target], how='inner')                   \n    \n    return poly_X","81b2513b":"# Custom scoring functions used for cross validation\ndef mae_scorer(y_true, y_pred):\n    \"\"\" Returns the MAE score \"\"\"\n    y_true =  inv_boxcox(y_true, maxlog)\n    y_pred =  inv_boxcox(y_pred, maxlog)\n    return mean_absolute_error(y_true, y_pred)\n\ndef r2_scorer(y_true, y_pred):\n    \"\"\" Returns the R2 score \"\"\"\n    y_true =  inv_boxcox(y_true, maxlog)\n    y_pred =  inv_boxcox(y_pred, maxlog)\n    return r2_score(y_true, y_pred)\n\ndef rmse_scorer(y_true, y_pred):\n    \"\"\" Returns the RMSE score \"\"\"\n    y_true =  inv_boxcox(y_true, maxlog)\n    y_pred =  inv_boxcox(y_pred, maxlog)\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n    \ndef r2_adj_scorer(y_true, y_pred):\n    \"\"\" Returns the adjusted R2 score \"\"\"\n    y_true =  inv_boxcox(y_true, maxlog)\n    y_pred =  inv_boxcox(y_pred, maxlog)\n    r2 = r2_score(y_true, y_pred)\n    r2_adj = 1 - (((1 - r2) * (n - 1)) \/ ( n - k - 1))\n    return r2_adj\n\n# Custom Scorer\nscoring = {'MAE': make_scorer(mae_scorer), 'RMSE': make_scorer(rmse_scorer), 'R2': make_scorer(r2_scorer), 'R2_Adjusted': make_scorer(r2_adj_scorer)}","29a88fd7":"# Read in data\ndata = pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')","ac8f8f42":"data['bmi_smoker_risk'] = np.where((data.bmi < 26) & (data.smoker == 'no'), 'Healthy_No',\n                            np.where((data.bmi < 26) & (data.smoker == 'yes'), 'Healthy_Yes',\n                                     np.where((data.bmi > 25) & (data.bmi < 31) & (data.smoker == 'no'), 'Overweight_No',\n                                            np.where((data.bmi > 25) & (data.bmi < 31) & (data.smoker == 'yes'), 'Overweight_Yes',\n                                                    np.where((data.bmi > 30) & (data.bmi < 41) & (data.smoker == 'no'), 'Obese_No',\n                                                            np.where((data.bmi > 30) & (data.bmi < 41) & (data.smoker == 'Yes'), 'Obese_Yes',\n                                                                    np.where((data.bmi > 40) & (data.smoker == 'no'), 'Morbid_No',\n                                                                        np.where((data.bmi > 40) & (data.smoker == 'yes'), 'Morbid_Yes', 'None'))))))))","0edd07dd":"# Add_poly_features(df, degree, bias, interaction, target)\ndata = add_poly_features(data, 2, True, True, 'charges')\n\n# Split data such that training and test have 80% and 20% respectively\nsplit = int(data.shape[0] * .8)\ntrain = data[:split]\ntest = data[split:]\n\n# Select features and target for train and test sets\ny_train = train['charges']\nX_train = train.drop('charges', axis=1)\n\ny_test = test['charges']\nX_test = test.drop('charges', axis=1)\n\n# Target is skewed so perform boxcox transformation\ny_train, maxlog = stats.boxcox(y_train.values)\n\n# Folds\nfolds = 10","6422a208":"# Numerical features\nnumeric_features = train.select_dtypes(include=['int64','float64']).drop('charges', axis=1).columns\n\n# Retrieve a list of tuples with skewed feature names and their values\nskewed_features_names = train[numeric_features].skew().index\nskewed_features_values = train[numeric_features].skew()\nskewed_feature_names_values = list(zip(skewed_features_names, skewed_features_values))\n\n# Numeric features with skew > .9\nlog_features = [feature for (feature, skew) in skewed_feature_names_values if skew > .9]\n\n# Remaing numeric features\nscale_features = [name for name in numeric_features if name not in log_features]\n\n# Categorical features\ncategorical_features = train.select_dtypes(include=['object']).columns","512d5708":"# Pipeline to transform skewed data by imputation and scaling\nnumeric_scale_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\n# Pipeline to transform skewed data by imputation, log + 1 transformation, and scaling\nnumeric_log_scale_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('log', FunctionTransformer(np.log1p, validate=False)),\n    ('scaler', StandardScaler())])\n\n# Pipeline to transform categorical data by imputation and one hot encoding \ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='error', drop='first'))])\n\n# Pipeline to preprocess data by calling other pipelines\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_scale_transformer, scale_features),\n        ('log', numeric_log_scale_transformer, log_features),\n        ('cat', categorical_transformer, categorical_features)])","a743b983":"\"\"\"\nR2 parameters: We need n, the number of observations in each sample, and k, the number of predictors. To get these values, we must preprocess out data.\n\n    n: For cross validation, I must consider the size of samples for each fold.\n\n    k: The number of predictors. The only thing to consider is that since I have a polynomial model, I also have a constant term. Therefore I subtract \n       1 from the number of predictors\n\"\"\"\nn, k =  Pipeline(steps=[('preprocessor', preprocessor)]).fit(X_train, y_train).transform(X_train).shape\nn *= (1 - (1\/folds))\nn = int(n)\nk -= 1\n\n# Regression Classifiers\nlr = LinearRegression()\nridge = Ridge(random_state = SEED)\nlasso = Lasso(random_state = SEED)\nrf = RandomForestRegressor(random_state = SEED)\nxgb = XGBRegressor(objective ='reg:squarederror', random_state = SEED)\nada = AdaBoostRegressor(random_state = SEED)\ngb = GradientBoostingRegressor(random_state = SEED)\nvr = VotingRegressor([('lr', lr), ('ridge', ridge),('lasso',lasso),('rf', rf),('xgb', xgb),('ada', ada),('grad',gb)])\n\nclassifiers = [\n    ('LinearRegression', lr),\n    ('Ridge', ridge),\n    ('XGBRegressor', xgb),\n    ('RandomForestRegressor', rf),\n    ('AdaBoostRegressor', ada),\n    ('GradientBoostingRegressor', gb),\n    ('VotingRegressor', vr)\n]\n\nclf_names = []\nclf_scores = []\n\n# Calculate RMSE and R Squared for each classifier sorted by RMSE\nfor clf_name, clf_model in classifiers:\n\n    # Append classifer name\n    clf_names.append(clf_name)\n    \n    # Perform cross validation scoring\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('model', clf_model)])\n    \n    cv_results = cross_validate(pipe, X_train, y_train, cv=folds, scoring=scoring)\n    \n    mae = cv_results['test_MAE'].mean()\n    rmse = cv_results['test_RMSE'].mean()\n    r2 = cv_results['test_R2'].mean()\n    r2_adj = cv_results['test_R2_Adjusted'].mean()\n    clf_scores.append([mae, rmse, r2, r2_adj])\n    \n# DataFrame to display classifiers and their respective RMSE and score metric\npd.DataFrame(clf_scores, columns = ['MAE', 'RMSE','R2', 'R2 Adj'], index=clf_names).sort_values('R2 Adj', ascending=False)","7bfa02bf":"# XGBRegressor pipeline\nclf_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('model', xgb)])\n\n# Find the feature importance generated by the model of each fold\nfeature_importance_folds = []\nfeature_importance_cv = cross_validate(clf_pipe, X_train, y_train, cv=folds, scoring=scoring, return_estimator = True)\nfor idx, estimator in enumerate(feature_importance_cv['estimator']):\n    feature_importance_folds.append(list(estimator['model'].feature_importances_))\n\n# Extact feature names generated from OneHotEncoding\nOH_categorical_features = clf_pipe.fit(X_train, y_train).named_steps['preprocessor'].transformers_[2][1].named_steps['onehot'].get_feature_names(categorical_features)\n\n# Concatenate numeric feature names with the feature names genereated from OneHotEncoding\nfeature_names = np.concatenate([numeric_features, OH_categorical_features])\n\n# Plot feature importances\nfeature_ranking = pd.DataFrame(np.array(feature_importance_folds).T, index = feature_names).mean(axis=1).sort_values(ascending=False)\nplt.figure(figsize=(12,7))\nsns.barplot(x=feature_ranking.values, y=feature_ranking.index)\n","f2b0c9db":"# Preprocess training set for feature selection\ntransformed_X_train = Pipeline(steps=[('preprocessor', preprocessor)]).transform(X_train)\ntransformed_X_train = pd.DataFrame(transformed_X_train, columns=feature_names)\n\n# Preprocess test set for feature selection\ntransformed_X_test = Pipeline(steps=[('preprocessor', preprocessor)]).transform(X_test)\ntransformed_X_test = pd.DataFrame(transformed_X_test, columns=feature_names)\n\n# Perform Recursive Feature Elimination Cross Validation\nestimator = clf_pipe.named_steps['model']\nselector = RFECV(estimator, scoring=make_scorer(mae_scorer))\nselector = selector.fit(transformed_X_train, y_train)\n\n# Extract features in order of importance ranking\nclf_feature_rankings = list(zip(selector.ranking_, feature_names))\nclf_feature_rankings.sort(key=lambda x: x[0])\nclf_feature_rankings = [x for (_, x) in clf_feature_rankings]","0564eeb5":"# Calculate scores for n features where features are added from most important to least important\nfeature_scores = []\nfor i, _ in enumerate(clf_feature_rankings):\n    features = clf_feature_rankings[:i+1]\n    cv_results = cross_validate(xgb, transformed_X_train[features], y_train, cv=folds, scoring=scoring)\n    mae = cv_results['test_MAE'].mean()\n    rmse = cv_results['test_RMSE'].mean()\n    r2 = cv_results['test_R2'].mean()\n    r2_adj = cv_results['test_R2_Adjusted'].mean()\n    feature_scores.append([mae, rmse, r2, r2_adj])\nfeature_scores = pd.DataFrame(feature_scores, columns=['MAE','RMSE','R2','R2 adj'])\nfeature_scores.set_index(feature_scores.index + 1)","a1893077":"# Set optimal feature count based on MAE score\noptimal_feature_count = feature_scores.sort_values('MAE').index[0]\noptimal_features = clf_feature_rankings[:optimal_feature_count]\nprint(\"Optimal number of features to minimize MAE on base model: {}\".format(optimal_feature_count))\n\n# Filter features to only include features which give the optimal MAE score\nfeature_selection_X_train = transformed_X_train[optimal_features]\nfeature_selection_X_test = transformed_X_test[optimal_features]\n\n# Re-calculate the number of predictors -- if 1 was kept, subtract 1\nk = feature_selection_X_train.shape[1]\nif '1' in feature_selection_X_train.columns:\n    k -= 1","9fb6d1ca":"fig, ax = plt.subplots(figsize=(10,5))\n\nuntuned_estimator = estimator\nvisualizer = LearningCurve(untuned_estimator, scoring=make_scorer(mae_scorer), ax=ax)\nvisualizer.fit(feature_selection_X_train, y_train)        \nvisualizer.show()","20e059bf":"def find_optimal_parameters(estimator, param_grid, random=False):\n    \"\"\"\n    Given an estimater, parameter grid, and boolean indicating GridSearchCV or RandomSearchCV, perform the respective search over the parameters.\n    Print the optimal parameters founds and return the best fit estimator.\n    \"\"\"\n    if random:\n        grid_search = RandomizedSearchCV(estimator = estimator, param_distributions = param_grid, n_iter = 100, cv = folds, n_jobs = -1, scoring = make_scorer(mae_scorer, greater_is_better=False),random_state = SEED)\n        grid_search.fit(feature_selection_X_train, y_train)\n        parameters = grid_search.best_params_\n        for key in parameters:\n            print('{}: {}'.format(key, parameters[key]))\n        return grid_search.best_estimator_\n    else:\n        grid_search = GridSearchCV(estimator = estimator, param_grid = param_grid, cv = folds, scoring = make_scorer(mae_scorer, greater_is_better=False), n_jobs = -1)\n        grid_search.fit(feature_selection_X_train, y_train)\n        parameters = grid_search.best_params_\n        for key in parameters:\n            print('{}: {}'.format(key, parameters[key]))\n        return grid_search.best_estimator_","59d39150":"def score_model(model, model_name = 'Model'):\n    \"\"\"\n    Given a model, performs cross validation scoring\n    \"\"\"\n    clf_scores = []\n    cv_results = cross_validate(model, feature_selection_X_train, y_train, cv=folds, scoring=scoring)\n    mae = cv_results['test_MAE'].mean()\n    rmse = cv_results['test_RMSE'].mean()\n    r2 = cv_results['test_R2'].mean()\n    r2_adj = cv_results['test_R2_Adjusted'].mean()\n    clf_scores.append([mae, rmse, r2, r2_adj])\n    return pd.DataFrame(clf_scores, columns=['MAE','RMSE','R2','R2 adj'], index=[model_name])","c8264b08":"def Validation_Curve_Visualization(model, parameter, values, X, y):\n    fig, ax = plt.subplots(figsize=(10,5))\n\n    viz = ValidationCurve(model, param_name=parameter, param_range=values, cv=folds, scoring=make_scorer(mae_scorer))\n    viz.fit(X, y)\n    viz.show()","1a8dd3f5":"# Examine Hyperparameters\nxgb_base = XGBRegressor(objective ='reg:squarederror', random_state = SEED)\npprint(xgb.get_params())","302dce73":"# Score Baseline Model\nscore_model(xgb_base, 'Baseline Model')","b23683a5":"param_grid = {\n    'n_estimators': [x for x in range(40, 95,5)]\n}\n\nValidation_Curve_Visualization(xgb_base, 'n_estimators', param_grid['n_estimators'],  feature_selection_X_train, y_train)\nestimator = find_optimal_parameters(xgb_base, param_grid, False)","223b14b9":"param_grid = {\n    'max_depth': [x for x in range(1,10,1)]\n}\n\nValidation_Curve_Visualization(estimator, 'max_depth', param_grid['max_depth'],  feature_selection_X_train, y_train)\nestimator = find_optimal_parameters(estimator, param_grid, False)","4de36df6":"param_grid = {\n    'min_child_weight': [x for x in range(0,10,1)]\n}\n\nValidation_Curve_Visualization(estimator, 'min_child_weight', param_grid['min_child_weight'],  feature_selection_X_train, y_train)\nestimator = find_optimal_parameters(estimator, param_grid, False)","0f4db8a6":"param_grid = {\n    'gamma': np.linspace(0,2,21)\n}\n\nValidation_Curve_Visualization(estimator, 'gamma', param_grid['gamma'],  feature_selection_X_train, y_train)\nestimator = find_optimal_parameters(estimator, param_grid, False)","09c562f8":"param_grid = {\n    'subsample': np.linspace(0.5,1,6)\n}\n\nValidation_Curve_Visualization(estimator, 'subsample', param_grid['subsample'],  feature_selection_X_train, y_train)\nestimator = find_optimal_parameters(estimator, param_grid, False)","d3fefb57":"param_grid = {\n    'colsample_bytree': np.linspace(0.5,1,6)\n}\n\nValidation_Curve_Visualization(estimator, 'colsample_bytree', param_grid['colsample_bytree'],  feature_selection_X_train, y_train)\nestimator = find_optimal_parameters(estimator, param_grid, False)","c4136959":"param_grid = {\n    'reg_alpha': np.linspace(0,1)\n}\n\nValidation_Curve_Visualization(estimator, 'reg_alpha', param_grid['reg_alpha'],  feature_selection_X_train, y_train)\nestimator = find_optimal_parameters(estimator, param_grid, False)","32d26272":"random_grid = {\n    'learning_rate': np.linspace(0, .1),\n    'n_estimators': [x for x in range(0,400,10)]\n}\n\nestimator = find_optimal_parameters(estimator, random_grid, True)","41860fba":"# Optimized MAE Score\nscore_model(estimator)","fe1464e5":"fig, ax = plt.subplots(ncols=2, figsize=(20,5))\n\n# Learning curve for base model\nvisualizer = LearningCurve(untuned_estimator, scoring=make_scorer(mae_scorer), ax=ax[0])\nvisualizer.fit(feature_selection_X_train, y_train)\nvisualizer.finalize()\n\n# Learning curve for tuned model\nvisualizer = LearningCurve(estimator, scoring=make_scorer(mae_scorer), ax=ax[1])\nvisualizer.fit(feature_selection_X_train, y_train)\nvisualizer.finalize()\nvisualizer.show()","ba949cf1":"# Best Model \nbest_model = estimator\n\n# Fit\nbest_model.fit(feature_selection_X_train, y_train)\n\n# Predict\ny_pred_transformed = best_model.predict(feature_selection_X_test)\ny_pred_untransformed = inv_boxcox(y_pred_transformed, maxlog)\n\n# Apply boxcox transformation to test\ny_test_transformed =  boxcox(y_test, maxlog)\n\n# Residuals\ny_residuals = y_test_transformed - y_pred_transformed","e8d7ec60":"# Score\nmae_best = mean_absolute_error(y_test, y_pred_untransformed)\nrmse_best = np.sqrt(mean_squared_error(y_test, y_pred_untransformed))\nr2_best = r2_score(y_test, y_pred_untransformed)\nr2_adjusted_best = 1 - (((1 - r2) * (n - 1)) \/ ( n - k - 1))\npd.DataFrame([[mae_best, rmse_best, r2_best, r2_adjusted_best]], columns=['MAE','RMSE','R2','R2 Adj'], index=['Scores on Test Set '])","91d6f09b":"fig, ax = plt.subplots(ncols = 2, figsize=(20,6))\n\nvisual_grid = [\n    PredictionError(best_model, line_color = 'r', point_color = 'black', alpha = '.5', ax=ax[0]),\n    ResidualsPlot(best_model, ax=ax[1])\n]\n\nfor visual in visual_grid:\n    visual.fit(feature_selection_X_train, y_train)\n    visual.score(feature_selection_X_test, y_test_transformed)\n    visual.finalize()\nvisual.show()","c0b30860":"_ = stats.probplot(y_pred_transformed, plot=plt)","d6516b1f":"# Plot\nvisualizer = CooksDistance()\nvisualizer.fit(feature_selection_X_train, y_train)","978ba776":"# The square root of the absolute standarized residuals\nsqrt_abs_standardized_residuals = np.sqrt(abs((y_residuals - y_residuals.mean())\/y_residuals.std()))\n\n# Plot\nplt.scatter(y_pred_transformed, sqrt_abs_standardized_residuals, alpha=.5)\nsns.regplot(y_pred_transformed, sqrt_abs_standardized_residuals, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})","9142712a":"# Setup","ed188121":"## Feature Selection","7ee09708":"First find the best number of estimators for a fixed learning rate","f5c1f7b9":"# Model Selection","3ed10160":"# Hyperparameter Tuning","da2477c1":"# Data Preparation","bc1325e7":"After tuning specific tree parameters, find the optimal regularization parameter","4339d292":"## Feature Importance","d451f436":"The normal distribution of errors is roughly a straight line where the ends seem to trail off. An ideal distribution follows a stright line strictly so this probability plot is promsiing.","6d893fe8":"As the number training instances increases, our model becomes more biased and less varianced. The learnining curve indicates that adding more data would definitely be beneficial.","0e383bd4":"## Tune Parameters","98dae8a6":"Finally, perform a grid search lowering the learning rate (default of 0.1) and finding the number of estimators to minimize MAE with said learning_rate","0d8a943c":"## Scoring Helper Functions","759cf939":"## Is there enough data?","f7c67ec5":"# Helper Functions","e7f7fc61":"# Feature Engineering","284e6c5b":"## QQ-Plot","9cdfd11d":"**Tune tree-specific parameters for a fixed learning rate and number of trees**\n* max_depth\n* min_child_weight\n* gamma, subsample\n* colsample_bytree","4acbff96":"## Scale Location Plot","233e8922":"Cook's distance shows that we have quite a few infuential cases. It's important to note that influential cases are not usually a problem when their removal from the dataset would leave the parameter estimates essentially unchanged therefore the ones we worry about are those whose presence really does change the results.","a22cedb8":"## Pipelines","db9f4282":"## Polynomial Functions","a9e4a7d6":"This plot is useful to determine heteroskedasticity which is present when the size of the error term differs across values of an independent variable. Ideally, we wouldn't want this plot to show any pattern and for it have a relatively straight line. The information in this plot can be corroborated with the information in the **Predicted vs Residuals** plot above. That is, we have a few large values for charges and not enough features to describe the data accurately enough higher predictions for charges with lesser value.","6b82fe15":"# Model Predictions","94be60af":"## Predicted vs Residuals","0209ac97":"## Parameters for XGBRegressor","375055ac":"## Cook's Distance","daac67af":"The model on the left indicates a base model with our selected features. The model on the right indicates our tuned model with our selected features. We can  that while there is a little bit more bias, the scores have less variance now. Our tuned model still indicates that we could use some more data.","52fc43ab":"## Cross Validation Scoring Metrics","0af4c754":"Visit my [Exploratory Data Analysis](https:\/\/www.kaggle.com\/littleotter\/medical-costs-eda) to see insights into the dataset","651fe9ee":"Ideally, our plot shouldn't show any pattern. However, there seems to be a trend where we predict some values too high. A reasoning for this could be attributed to the fact that we don't have a normal distribution of charges. That is, we have a subset of charges that are much higher than the average charges with much lower frequency. Moreover, we might not have enough features to to help distinguish what causes these high prices from those who don't have such high charges. More data and features can help remedy these problems and improve our model. ","8e3506ec":"From the EDA, we saw an interesting relationship between bmi groups for smokers and non-smokers which could serve as a useful predictor.","d4ece3ff":"# Exploratory Data Analysis","381b4a2c":"## Baseline Model"}}