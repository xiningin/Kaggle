{"cell_type":{"32d13ae8":"code","1b1294ea":"code","115ea0c4":"code","9d2b6186":"code","f79ae7d1":"code","770cd551":"code","27d400b8":"code","32a0b1af":"code","5e6ee732":"code","b216f0b8":"code","34bf97b3":"code","1daa1ed4":"code","fa7d4870":"code","b9835f7c":"code","3a566fea":"code","30355cda":"code","a7cbca4a":"code","e38c17d7":"code","89c23be9":"code","0273924f":"code","6821ca07":"code","c26ba53b":"code","91ad9153":"code","f743baf1":"code","8a5f04b8":"code","8e07c593":"code","4ba6641b":"code","9d80153f":"code","48796364":"code","cbd490da":"code","4e105850":"code","1678e1f3":"code","d9b5e26a":"code","89d1dd70":"code","bfb98e65":"code","3001ae44":"code","26b2eddb":"code","501c62c6":"code","2fb1b060":"code","b1a1886c":"code","7969f119":"code","53ee2f14":"code","df164d41":"code","fc2c05b5":"code","60c11784":"code","20c72ebe":"code","a72e4c69":"code","a1573b62":"code","32080426":"code","58a90347":"code","1c9c91d5":"code","ea25a10c":"code","1e5fa4cc":"code","615b4365":"code","bca0d49c":"code","fd09a79a":"code","a746a6b5":"code","debbe294":"code","9c3d7b2d":"code","9e950796":"code","82f3d3a7":"code","de127a76":"code","260747fa":"code","3aa9285b":"code","2202ca5a":"code","0127d9c7":"code","1f590420":"code","33475c34":"code","8ed9c364":"markdown","f68aaba8":"markdown","b3361570":"markdown","05ca535b":"markdown","225160c9":"markdown","f71a5da0":"markdown","617c72a0":"markdown","ca684206":"markdown","906ea78a":"markdown","fd17fc80":"markdown","b4586f5c":"markdown","9d89f240":"markdown","9d854775":"markdown","0515c809":"markdown","18e9cb70":"markdown","7f74956c":"markdown","b59355ad":"markdown","2b560f33":"markdown","f07d5e9a":"markdown","a6e47104":"markdown","0aa779c9":"markdown","e0beedff":"markdown","e1d2113c":"markdown","acbd6482":"markdown","2ec4a680":"markdown","cc9935d4":"markdown","633fd927":"markdown","b8c46e25":"markdown","b96845bf":"markdown"},"source":{"32d13ae8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b1294ea":"## Importing the required libraries and packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","115ea0c4":"## Importing the train csv datafile\n\ntitanic_train = pd.read_csv('..\/input\/titanic\/train.csv')","9d2b6186":"## Importing the test csv datafile\n\ntitanic_test = pd.read_csv('..\/input\/titanic\/test.csv')","f79ae7d1":"## Description of every column for train data\n\ntitanic_train.info()","770cd551":"## Description of every column for test data\n\ntitanic_test.info()","27d400b8":"## Checking top 5 rows of the train dataset\n\ntitanic_train.head()","32a0b1af":"## Checking top 5 rows of the test dataset\n\ntitanic_test.head()","5e6ee732":"## Checking for top columns having missing values percentage in train dataset\n\nprint ((titanic_train.isnull().sum()\/len(titanic_train)*100).round(2).sort_values(ascending = False).head())","b216f0b8":"## Checking for top columns having missing values percentage in test dataset\n\nprint ((titanic_test.isnull().sum()\/len(titanic_test)*100).round(2).sort_values(ascending = False).head())","34bf97b3":"## Dropping the missing value Cabin column from train dataset\n\ntitanic_train.drop('Cabin', axis=1, inplace=True)","1daa1ed4":"## Dropping the missing value Cabin column from test dataset\n\ntitanic_test.drop('Cabin', axis=1, inplace=True)","fa7d4870":"## Checking for the Age distribution in training dataset\n\nplt.figure(figsize=(10,5))\nsns.distplot(titanic_train['Age'] , bins=30)","b9835f7c":"## Filling the missing values in Age cloumn with mean values for training dataset\n\ntitanic_train['Age'] = (titanic_train['Age'].fillna(titanic_train['Age'].mean())).round()","3a566fea":"## Plotting the count of passenger sex based on Pclass for training set\n\nplt.figure(figsize=(10,5))\nsns.countplot(x = 'Sex' , data = titanic_train , hue = 'Pclass' , palette = 'rainbow')\nplt.title('Passenger Sex based on Pclass')","30355cda":"## Checking for the Age distribution in test dataset\n\nplt.figure(figsize=(10,5))\nsns.distplot(titanic_test['Age'] , bins=30)","a7cbca4a":"## Filling the missing values in Age cloumn with mean values for test dataset\n\ntitanic_test['Age'] = (titanic_test['Age'].fillna(titanic_test['Age'].mean())).round()","e38c17d7":"## Plotting the count of passenger sex based on Pclass for test set\n\nplt.figure(figsize=(10,5))\nsns.countplot(x = 'Sex' , data = titanic_test , hue = 'Pclass' , palette = 'viridis')\nplt.title('Passenger Sex based on Pclass')","89c23be9":"## Creating a new feature name Family Size(SibSp+Parch+1) for training set\n\ntitanic_train['Family Size'] = titanic_train['SibSp'] + titanic_train['Parch'] + 1","0273924f":"## Creating a new feature name Family Size(SibSp+Parch+1) for test set\n\ntitanic_test['Family Size'] = titanic_test['SibSp'] + titanic_test['Parch'] + 1","6821ca07":"## Dropping columns which doesn't add up to our predictions in training set\n\ntitanic_train.drop(['Name','Ticket','SibSp','Parch'] , axis = 1 , inplace = True)","c26ba53b":"## Dropping columns which doesn't add up to our predictions in test set\n\ntitanic_test.drop(['Name','Ticket','SibSp','Parch'] , axis = 1 , inplace = True)","91ad9153":"## Convert categorical variables for training set into dummy variables (i.e. one-hot encoding)\n\ndummies = pd.get_dummies(titanic_train[['Sex','Embarked']],drop_first=True)\n\ntitanic_train_dummy = titanic_train.drop(['Sex','Embarked'] , axis = 1)\n\ntitanic_train_dummy = pd.concat([titanic_train_dummy , dummies] , axis = 1)\n\ntitanic_train_dummy.info()","f743baf1":"## Convert categorical variables for test set into dummy variables (i.e. one-hot encoding)\n\ndummies = pd.get_dummies(titanic_test[['Sex','Embarked']],drop_first=True)\n\ntitanic_test_dummy = titanic_test.drop(['Sex','Embarked'] , axis = 1)\n\ntitanic_test_dummy = pd.concat([titanic_test_dummy , dummies] , axis = 1)\n\ntitanic_test_dummy['Fare'] = (titanic_test_dummy['Fare'].fillna(titanic_test_dummy['Fare'].mean())).round()\n\ntitanic_test_dummy.info()","8a5f04b8":"## Plotting the count of passenger died and survived\n\nplt.figure(figsize=(10,5))\nsns.countplot(titanic_train_dummy['Survived'])\nplt.title('Count of passenger died and survived')","8e07c593":"## Plotting the count of passenger died and survived based on Pclass\n\nplt.figure(figsize=(10,5))\nsns.countplot(x = 'Survived' , data = titanic_train_dummy , hue = 'Pclass')\nplt.title('Passenger died and survived based on Pclass')","4ba6641b":"## Plotting joint relationship between 'Fare' , 'Age' , 'Pclass' & 'Survived' for training dataset\n\nsns.pairplot(titanic_train_dummy[['Fare','Age','Pclass','Survived']] , hue = 'Survived' , height = 4)","9d80153f":"## Checking correlation between all the features using heatmap for training dataset\n\nplt.figure(figsize=(10,8))\ncorr = titanic_train_dummy.corr()\nsns.heatmap(corr , annot = True , linecolor = 'black' , linewidth = .01)\nplt.title('Correlation between features')","48796364":"## Checking correlation between all the features using heatmap for test dataset\n\nplt.figure(figsize=(10,8))\ncorr = titanic_test_dummy.corr()\nsns.heatmap(corr , annot = True , linecolor = 'black' , linewidth = .01 , cmap = 'YlGnBu')\nplt.title('Correlation between features')","cbd490da":"X_train = titanic_train_dummy.drop(['Survived','PassengerId'] , axis =1)","4e105850":"y_train = titanic_train_dummy['Survived']","1678e1f3":"X_test = titanic_test_dummy.drop(\"PassengerId\", axis=1).copy()","d9b5e26a":"print ('X_train =' , X_train.shape)\nprint ('y_train =' , y_train.shape) \nprint ('X_test =' , X_test.shape)","89d1dd70":"from sklearn.preprocessing import StandardScaler","bfb98e65":"scaler = StandardScaler()","3001ae44":"X_train = scaler.fit_transform(X_train)","26b2eddb":"X_test = scaler.transform(X_test)","501c62c6":"from sklearn.linear_model import LogisticRegression","2fb1b060":"logreg = LogisticRegression()","b1a1886c":"logreg.fit(X_train , y_train)","7969f119":"y_pred = logreg.predict(X_test)","53ee2f14":"acc_log = (logreg.score(X_train, y_train)*100).round(2)\n\nprint ('Accuracy score is:', acc_log)","df164d41":"from sklearn.svm import SVC","fc2c05b5":"svc = SVC()","60c11784":"svc.fit(X_train , y_train)","20c72ebe":"y_pred = svc.predict(X_test)","a72e4c69":"acc_svm = (svc.score(X_train, y_train)*100).round(2)\n\nprint ('Accuracy score is:', acc_svm)","a1573b62":"from sklearn.neighbors import KNeighborsClassifier","32080426":"knn = KNeighborsClassifier(n_neighbors = 5)","58a90347":"knn.fit(X_train , y_train)","1c9c91d5":"y_pred = knn.predict(X_test)","ea25a10c":"acc_knn = (knn.score(X_train, y_train)*100).round(2)\n\nprint ('Accuracy score is:', acc_knn)","1e5fa4cc":"from sklearn.naive_bayes import GaussianNB","615b4365":"gaussian = GaussianNB()","bca0d49c":"gaussian.fit(X_train , y_train)","fd09a79a":"y_pred = gaussian.predict(X_test)","a746a6b5":"acc_gaussian = (gaussian.score(X_train, y_train)*100).round(2)\n\nprint ('Accuracy score is:', acc_gaussian)","debbe294":"from sklearn.ensemble import RandomForestClassifier","9c3d7b2d":"random_forest = RandomForestClassifier(n_estimators = 100)","9e950796":"random_forest.fit(X_train , y_train)","82f3d3a7":"y_pred = random_forest.predict(X_test)","de127a76":"acc_random_forest = (random_forest.score(X_train, y_train)*100).round(2)\n\nprint ('Accuracy score is:', acc_random_forest)","260747fa":"model = pd.DataFrame({\n    'Model': ['Logistic Regression','Support Vector Machine','K - Nearest Neighbours',\n              'Naive Bayes Classifier','Random Forest Classifier'],\n    'Score': [acc_log, acc_svm, acc_knn, acc_gaussian, acc_random_forest]})\n\nmodel.sort_values(by = 'Score' , ascending = False)","3aa9285b":"final_pred = pd.DataFrame({\n        \"PassengerId\": titanic_test_dummy['PassengerId'],\n        \"Survived\": y_pred\n    })","2202ca5a":"## Predicting the survival and writing to CSV\n\nfinal_pred.to_csv('gender_submission.csv', index = False)","0127d9c7":"titanic_survival = pd.read_csv('gender_submission.csv')","1f590420":"titanic_survival.info()","33475c34":"print (titanic_survival['Survived'].value_counts())","8ed9c364":"- Observations from above correlation graph\n    - Survived is higly correlated with Fare\n    - Survived is also highly correlated with Pclass\n    \n","f68aaba8":"* Observation from above pairplot\n    - More passenger of __Pclass 1__ survived than died\n    - More passenger of __Pclass 3__ died than survived\n    - More passenger of age group __20-40__ died than survived\n    - Most of the passenger paying __less fare__ died","b3361570":"* From the above plot we can say that the passengers with ticket class __3__ have high mortality count and low survival, and passengers with ticket class __1__ has low mortality count and high survival.","05ca535b":"The goal is to __predict survival__ of passengers travelling in __Titanic__ using various modelling techniques.","225160c9":"### <u>Objective<\/u>","f71a5da0":"* From the above train and test dataset we can see that the __Cabin__ column is having high percentage of missing values hence, we will be dropping the __Cabin__ column.\n* We have very minumum percentage of of missing values for __Age__, __Embarked__ & __Fare__ column and we will keep it for further analysis.","617c72a0":"### Scaling Data","ca684206":"This dataset contains the below feature column fields:\n\n* **PassengerId:** Passenger Identity\n* **Survived:** Passenger survived or not\n* **Pclass:** Class of ticket\n* **Name:** Name of passenger\n* **Sex:** Sex of passenger\n* **Age:** Age of passenger\n* **SibSp:** Number of sibling and\/or spouse travelling with passenger\n* **Parch:** Number of parent and\/or children travelling with passenger\n* **Ticket:** Ticket number\n* **Fare:** Price of ticket\n* **Embarked:** Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton","906ea78a":"### Model Predictions","fd17fc80":"### Creating a Model (K - Nearest Neighbours)","b4586f5c":"### Model Scores ","9d89f240":"* From the above information on the test dataset, out of __418__ entries __264__ passengers __died__ and __154__ passengers have __survived__.","9d854775":"* From the above plot we can say that there are more number of passengers died than survived.","0515c809":"### Creating a Model (Support Vector Machine)","18e9cb70":"### Creating a Model (Naive Bayes Classifier)","7f74956c":"### Model Predictions","b59355ad":"### <u>Content<\/u>","2b560f33":"The data has been split into two groups:\n\n* The <b>training set<\/b> should be used to build your machine learning models. For the training set, the outcome (also known as the \u201cground truth\u201d) for each passenger. The model will be based on \u201cfeatures\u201d like passengers gender and class. The feature engineering is use to create new features.\n\n* The <b>test set<\/b> should be used to see how well the model performs on unseen data. For the test set, the ground truth for each passenger is not available. For each passenger in the test set, the model trained to predict whether or not passengers survived the sinking of the Titanic.","f07d5e9a":"### Creating a Model (Random Forest Classifier)","a6e47104":"- From the above plot we can see there are more numbers of passengers in __Pclass 3__ and __male__ are more in numbers as compare to __female__.","0aa779c9":"### Model Building","e0beedff":"### <u>Context<\/u>","e1d2113c":"- From the above plot we can see there are more numbers of passengers in __Pclass 3__ and __male__ are more in numbers as compare to __female__.","acbd6482":"### Creating a Model (Logistic Regresion)","2ec4a680":"### Model Predictions","cc9935d4":"### Model Predictions","633fd927":"### Model Predictions","b8c46e25":"# <center>Titanic: Disaster_Machine Learning<\/center>","b96845bf":"* From the above models __Logistic Regression__, __Support Vector Machine__, __K-Nearest Neighbours__, __Naive Bayes Classifier__ and __Random Forest Classifier__ it can be conclude that __Random Forest Classifier__ model out performs the best with an __Accuracy Score of 97.87__."}}