{"cell_type":{"67b0ba4e":"code","80575830":"code","6aa1ad0d":"code","c5de6e72":"code","620080f7":"code","61c45388":"code","a1a7f375":"code","345fd62b":"code","5457e757":"code","d333e1c2":"code","26af5946":"code","6785f9e5":"code","22a7317d":"code","2a56cd64":"code","d6411b6f":"code","641ac1e6":"code","8c9ceb8f":"code","3efa46f9":"code","b2c6b7d0":"code","e45ad397":"code","b76496c1":"code","ac355cc7":"code","45e9c9e2":"code","eb8a6866":"code","23f3b1b1":"code","3acb670f":"code","99e7513f":"code","8ff6a2f2":"code","129db347":"code","45056e16":"code","3e8099f7":"code","d801478e":"code","2d1c4ab4":"code","4d847630":"code","54195b64":"code","70974856":"code","cad88bf0":"code","d7e31239":"code","30dd8592":"markdown","8577edb4":"markdown","d98e9de4":"markdown"},"source":{"67b0ba4e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","80575830":"train = pd.read_csv(\"..\/input\/xsquad\/train.csv\")","6aa1ad0d":"train.shape","c5de6e72":"# !pip install transformers\n!pip install pytorch_lightning==0.8.1","620080f7":"import argparse\nimport glob\nimport os\nimport json\nimport time\nimport logging\nimport random\nimport re\nfrom itertools import chain\nfrom string import punctuation\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\n","61c45388":"train.head()","a1a7f375":"train.shape","345fd62b":"pl.__version__","5457e757":"pip install transformers==4.0.0rc1","d333e1c2":"from transformers import (\n    AdamW,\n    MT5ForConditionalGeneration,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup\n)","26af5946":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","6785f9e5":"import pytorch_lightning as pl\n","22a7317d":"class T5FineTuner(pl.LightningModule):\n    def __init__(self, hparams):\n        super(T5FineTuner, self).__init__()\n        self.hparams = hparams\n\n        self.model = MT5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n\n    def is_logger(self):\n        return True\n\n    def forward(\n            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n    ):\n        return self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=labels,\n        )\n\n    def _step(self, batch):\n        labels = batch[\"target_ids\"]\n        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n\n        outputs = self(\n            input_ids=batch[\"source_ids\"],\n            attention_mask=batch[\"source_mask\"],\n            labels=labels,\n            decoder_attention_mask=batch['target_mask']\n        )\n\n        loss = outputs[0]\n\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self._step(batch)\n\n        tensorboard_logs = {\"train_loss\": loss}\n        return {\"loss\": loss, \"log\": tensorboard_logs}\n\n    def training_epoch_end(self, outputs):\n        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n    def validation_step(self, batch, batch_idx):\n        loss = self._step(batch)\n        return {\"val_loss\": loss}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        tensorboard_logs = {\"val_loss\": avg_loss}\n        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n    def configure_optimizers(self):\n        \"Prepare optimizer and schedule (linear warmup and decay)\"\n\n        model = self.model\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": self.hparams.weight_decay,\n            },\n            {\n                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n        self.opt = optimizer\n        return [optimizer]\n\n    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n        if self.trainer.use_tpu:\n            xm.optimizer_step(optimizer)\n        else:\n            optimizer.step()\n        optimizer.zero_grad()\n        self.lr_scheduler.step()\n\n    def get_tqdm_dict(self):\n        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n\n        return tqdm_dict\n\n    def train_dataloader(self):\n        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n                                num_workers=4)\n        t_total = (\n                (len(dataloader.dataset) \/\/ (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n                \/\/ self.hparams.gradient_accumulation_steps\n                * float(self.hparams.num_train_epochs)\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n        )\n        self.lr_scheduler = scheduler\n        return dataloader\n\n    def val_dataloader(self):\n        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"valid\", args=self.hparams)\n        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n","2a56cd64":"logger = logging.getLogger(__name__)\n\nclass LoggingCallback(pl.Callback):\n        def on_validation_end(self, trainer, pl_module):\n            logger.info(\"***** Validation results *****\")\n            if pl_module.is_logger():\n                  metrics = trainer.callback_metrics\n                  # Log results\n                  for key in sorted(metrics):\n                    if key not in [\"log\", \"progress_bar\"]:\n                      logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n\n        def on_test_end(self, trainer, pl_module):\n            logger.info(\"***** Test results *****\")\n\n            if pl_module.is_logger():\n                metrics = trainer.callback_metrics\n\n                  # Log and save results to file\n                output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n                with open(output_test_results_file, \"w\") as writer:\n                    for key in sorted(metrics):\n                          if key not in [\"log\", \"progress_bar\"]:\n                            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n                            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))","d6411b6f":"args_dict = dict(\n    data_dir=\"\", # path for data files\n    output_dir=\"\", # path to save the checkpoints\n    model_name_or_path='google\/mt5-base',\n    tokenizer_name_or_path='google\/mt5-base',\n    max_seq_length=512,\n    learning_rate=3e-4,\n    weight_decay=0.0,\n    adam_epsilon=1e-8,\n    warmup_steps=0,\n    train_batch_size=8,\n    eval_batch_size=8,\n    num_train_epochs=2,\n    gradient_accumulation_steps=8,\n    n_gpu=1,\n    early_stop_callback=False,\n    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n    opt_level='O1', # you can find out more on optimisation levels here https:\/\/nvidia.github.io\/apex\/amp.html#opt-levels-and-properties\n    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n    seed=42,\n)","641ac1e6":"!ls","8c9ceb8f":"!ls ..\/input\/","3efa46f9":"train_path = \"..\/input\/xsquad\/train.csv\"\nval_path = \"..\/input\/xsquad\/valid.csv\"\n\ntrain = pd.read_csv(train_path)\nprint (train.head())\n\n# tokenizer = AutoTokenizer.from_pretrained('google\/mt5-small')","b2c6b7d0":"df = pd.read_csv(train_path)\ndf.columns","e45ad397":"df","b76496c1":"class QuestionDataset(Dataset):\n    def __init__(self, tokenizer, data_dir, type_path, max_len=30):\n        self.path = os.path.join(data_dir, type_path + '.csv')\n\n        self.english = 'context'\n        self.hindi = 'question'\n        self.data = pd.read_csv(self.path)\n\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.inputs = []\n        self.targets = []\n\n        self._build()\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, index):\n        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n        target_ids = self.targets[index][\"input_ids\"].squeeze()\n\n        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n\n        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n\n    def _build(self):\n        for idx in range(len(self.data)):\n            input_text,output_text= self.data.loc[idx, self.english],self.data.loc[idx, self.hindi]\n   \n            input_ = \"Hindi Context: %s\" % (input_text)\n            target = \"%s \" %(output_text)\n\n            # tokenize inputs\n            tokenized_inputs = self.tokenizer.batch_encode_plus(\n                [input_], max_length=200, pad_to_max_length=True, return_tensors=\"pt\"\n            )\n            # tokenize targets\n            tokenized_targets = self.tokenizer.batch_encode_plus(\n                [target], max_length=20, pad_to_max_length=True, return_tensors=\"pt\"\n            )\n\n            self.inputs.append(tokenized_inputs)\n            self.targets.append(tokenized_targets)","ac355cc7":"tokenizer = AutoTokenizer.from_pretrained('google\/mt5-large')","45e9c9e2":"dataset = QuestionDataset(tokenizer, '..\/input\/xsquad\/', 'valid', 30)\nprint(\"Val dataset: \",len(dataset))","eb8a6866":"data = dataset[20]\nprint(tokenizer.decode(data['source_ids']))\nprint(tokenizer.decode(data['target_ids']))","23f3b1b1":"!mkdir result\n!ls\n!pwd","3acb670f":"args_dict.update({'data_dir': '..\/input\/xsquad', 'output_dir': '\/kaggle\/working\/result', 'num_train_epochs':10,'max_seq_length':220})\nargs = argparse.Namespace(**args_dict)\nprint(args_dict)","99e7513f":"checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    \n    period =1,filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=1\n)\n\ntrain_params = dict(\n    accumulate_grad_batches=args.gradient_accumulation_steps,\n    gpus=args.n_gpu,\n    max_epochs=args.num_train_epochs,\n    early_stop_callback=False,\n    precision= 16 if args.fp_16 else 32,\n    amp_level=args.opt_level,\n    gradient_clip_val=args.max_grad_norm,\n    checkpoint_callback=checkpoint_callback,\n    callbacks=[LoggingCallback()],\n)\n","8ff6a2f2":"def get_dataset(tokenizer, type_path, args):\n    return QuestionDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)","129db347":"print (\"Initialize model\")\nmodel = T5FineTuner(args)\n\ntrainer = pl.Trainer(**train_params)","45056e16":"print (\" Training model\")\ntrainer.fit(model)\n\nprint (\"training finished\")\n\nprint (\"Saving model\")\nmodel.model.save_pretrained(\"\/kaggle\/working\/result\")\n\nprint (\"Saved model\")","3e8099f7":"!ls ","d801478e":"1+1","2d1c4ab4":"!ls result","4d847630":"!pwd","54195b64":"!cp -r \/kaggle\/working\/result\/pytorch_model.bin \/kaggle\/working\/\n!cp -r \/kaggle\/working\/result\/config.json \/kaggle\/working\/","70974856":"!ls\n","cad88bf0":"!rm -rf result","d7e31239":"!ls","30dd8592":"This notebook is an example on how to fine tune mT5 model with Higgingface Transformers to solve multilingual task in 101 lanaguges. This notebook especially takes the problem of question generation in hindi lanagues","8577edb4":"We'll be pytorch-lightning library for training. Most of the below code is adapted from here https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/lightning_base.py\n\nThe trainer is generic and can be used for any text-2-text task. You'll just need to change the dataset. Rest of the code will stay unchanged for all the tasks.\n\nThis is the most intresting and powrfull thing about the text-2-text format. You can fine-tune the model on variety of NLP tasks by just formulating the problem in text-2-text setting. No need to change hyperparameters, learning rate, optimizer or loss function. Just plug in your dataset and you are ready to go!","d98e9de4":"Let's define the hyperparameters and other arguments. You can overide this dict for specific task as needed. While in most of cases you'll only need to change the data_dirand output_dir.\n\nHere the batch size is 8 and gradient_accumulation_steps are 8 so the effective batch size is 64"}}