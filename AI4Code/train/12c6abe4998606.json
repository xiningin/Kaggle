{"cell_type":{"bedc3c2c":"code","33dc8982":"code","d8a33b26":"code","17273af0":"code","7d8a732a":"code","d20e332e":"code","4a53c751":"code","91f3f535":"code","85baf756":"code","8d57b458":"code","867f1269":"code","55f18f04":"code","5854b59c":"code","a2142dbf":"code","19d83701":"code","8ec80a10":"code","e92a946b":"code","5ecca9f2":"code","25890781":"code","4a7c072a":"code","cdb71df9":"code","2ad78825":"markdown","6dc7440f":"markdown","b7670951":"markdown","1044faeb":"markdown","b48b4fa0":"markdown","0b7fce00":"markdown","f8d05f51":"markdown","dfcd574a":"markdown","2d3eb9ab":"markdown","e8639538":"markdown","9b3b15a2":"markdown","b3fe0682":"markdown","42c19d4f":"markdown","5e56f92b":"markdown","491352ad":"markdown","eacafb99":"markdown","883b5a95":"markdown","566ab53f":"markdown","b9f674c3":"markdown","d3bc9e9b":"markdown","30ef1c30":"markdown","8be27a25":"markdown"},"source":{"bedc3c2c":"!pip install audiomentations\n!pip install wavio pyloudnorm ffmpeg pydub # install extra dependencies","33dc8982":"# Define some helper functions for pretty figures.\nimport csv\nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio, display\n\ndef show_signal(raw_wav, sr, title=None):\n    fig = plt.figure(figsize=(5, 5))\n    ax1 = plt.subplot(2, 1, 1)\n    if title:\n        ax1.set_title(title)\n    ax1.plot(np.arange(len(raw_wav))\/sr, raw_wav)\n    \n    ax2 = plt.subplot(2, 1, 2, sharex=ax1)\n    wav_stft = librosa.amplitude_to_db(np.abs(librosa.stft(raw_wav)), ref=np.max)\n    librosa.display.specshow(wav_stft, sr=sr, x_axis='time', y_axis='mel')\n    \n    return Audio((raw_wav*2**15).astype(np.int16), rate=sr)\n    \n    \ndef compare_signals(wav1, sr1, wav2, sr2, titles=None):\n    fig = plt.figure(figsize=(10, 5))\n    ax1 = plt.subplot(2, 2, 1)\n    if titles:\n        ax1.set_title(titles[0])\n    ax1.plot(np.arange(len(wav1))\/sr, wav1)\n    \n    ax2 = plt.subplot(2, 2, 3, sharex=ax1)\n    wav_stft = librosa.amplitude_to_db(np.abs(librosa.stft(wav1)), ref=np.max)\n    librosa.display.specshow(wav_stft, sr=sr, x_axis='time', y_axis='mel')\n    \n    ax3 = plt.subplot(2, 2, 2)\n    if titles:\n        ax3.set_title(titles[0])\n    ax3.plot(np.arange(len(wav2))\/sr, wav2)\n    \n    ax4 = plt.subplot(2, 2, 4, sharex=ax3)\n    wav_stft = librosa.amplitude_to_db(np.abs(librosa.stft(wav2)), ref=np.max)\n    librosa.display.specshow(wav_stft, sr=sr, x_axis='time', y_axis='mel')\n    \n    print(titles[0])\n    display(Audio((wav1*2**15).astype(np.int16), rate=sr1))\n    print(titles[1])\n    display(Audio((wav2*2**15).astype(np.int16), rate=sr2))\n    return display()","d8a33b26":"with open('\/kaggle\/input\/rfcx-species-audio-detection\/train_tp.csv') as f:\n    reader = csv.reader(f)\n    data = list(reader)\n    \nwav, sr = librosa.load('\/kaggle\/input\/rfcx-species-audio-detection\/train\/' + data[10][0] + '.flac', sr=None)\n\nshow_signal(wav, sr)","17273af0":"from audiomentations import *","7d8a732a":"SAMPLE_RATE = 16000\n\naugment = Compose([\n    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n    Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n])\n\n# Generate 2 seconds of dummy audio for the sake of example\nsamples = np.random.uniform(low=-0.2, high=0.2, size=(32000,)).astype(np.float32)\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=samples, sample_rate=SAMPLE_RATE)\nprint(augmented_samples)\n\ncompare_signals(samples, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","d20e332e":"# use train data as additive noise\naugment = Compose([\n    AddBackgroundNoise(sounds_path=\"..\/input\/rfcx-species-audio-detection\/train\/\", \n                       min_snr_in_db=3, \n                       max_snr_in_db=30, \n                       p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","4a53c751":"# use default values.\naugment = Compose([\n    AddGaussianNoise(min_amplitude=0.001, \n                       max_amplitude=0.015, \n                       p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","91f3f535":"# use train data as noise\naugment = Compose([\n    AddShortNoises(sounds_path=\"..\/input\/rfcx-species-audio-detection\/train\/\",\n                   min_snr_in_db=0,\n                   max_snr_in_db=24,\n                   min_time_between_sounds=4.0,\n                   max_time_between_sounds=16.0,\n                   burst_probability=0.22,\n                   min_pause_factor_during_burst=0.1,\n                   max_pause_factor_during_burst=1.1,\n                   min_fade_in_time=0.005,\n                   max_fade_in_time=0.08,\n                   min_fade_out_time=0.01,\n                   max_fade_out_time=0.1,\n                   p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","85baf756":"augment = Compose([\n    ClippingDistortion(min_percentile_threshold=0, \n                       max_percentile_threshold=40, \n                       p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","8d57b458":"augment = Compose([\n    FrequencyMask(min_frequency_band=0.0, \n                  max_frequency_band=0.5, \n                  p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","867f1269":"augment = Compose([\n    Gain(min_gain_in_db=-12, \n         max_gain_in_db=12, \n         p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","55f18f04":"augment = Compose([\n    Mp3Compression(min_bitrate=8, \n                   max_bitrate=64, \n                   backend=\"pydub\",\n                   p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","5854b59c":"augment = Compose([\n    LoudnessNormalization(min_lufs_in_db=-31, \n                          max_lufs_in_db=-13, \n                          p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","a2142dbf":"augment = Compose([\n    Normalize(p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","19d83701":"augment = Compose([\n    PitchShift(min_semitones=-4, \n               max_semitones=4, \n               p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","8ec80a10":"augment = Compose([\n    PolarityInversion(p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","e92a946b":"augment = Compose([\n    Resample(min_sample_rate=8000, \n             max_sample_rate=44100, \n             p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","5ecca9f2":"augment = Compose([\n    Shift(min_fraction=-0.5, max_fraction=0.5, rollover=True, p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","25890781":"augment = Compose([\n    TimeMask(min_band_part=0.0, \n             max_band_part=0.5, \n             fade=False, \n             p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","4a7c072a":"augment = Compose([\n    TimeStretch(min_rate=0.8, \n                max_rate=1.25, \n                leave_length_unchanged=True, \n                p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","cdb71df9":"augment = Compose([\n    Trim(top_db=20, \n         p=1.0)\n])\n\n# Augment\/transform\/perturb the audio data\naugmented_samples = augment(samples=wav, sample_rate=sr)\n\ncompare_signals(wav, sr, augmented_samples, sr, titles=[\"Before\", \"After\"])","2ad78825":"### LoudnessNormalization","6dc7440f":"## Test augmentations\n\nProbability $p$ is fixed to 1.0, which means the augmentation always (100%) happens. ","b7670951":"### AddGaussianNoise","1044faeb":"Load modules and define some useful functions. \nSome part of code inherit from [ResNet34 More Augmentations+Mixup+TTA (Inference)](https:\/\/www.kaggle.com\/khoongweihao\/resnet34-more-augmentations-mixup-tta-inference) and [All-in-one RFCX baseline for beginners](https:\/\/www.kaggle.com\/c\/rfcx-species-audio-detection).","b48b4fa0":"### Mp3Compression","0b7fce00":"### Resample","f8d05f51":"### TimeMask","dfcd574a":"### ClippingDistortion","2d3eb9ab":"### Trim\nThere is no trailing silence in the example. So this is not visible in this case.","e8639538":"### AddBackgroundNoise","9b3b15a2":"## Load sample data\n\nThe code here inherit from [All-in-one RFCX baseline for beginners](https:\/\/www.kaggle.com\/c\/rfcx-species-audio-detection) with modifications.","b3fe0682":"### Shift","42c19d4f":"## Prepare for execution","5e56f92b":"### Do the official example","491352ad":"### Gain","eacafb99":"### PolarityInversion","883b5a95":"### Normalize","566ab53f":"### AddShortNoises","b9f674c3":"### TimeStretch","d3bc9e9b":"### FrequencyMask","30ef1c30":"# Data augmentation with audiomentation\n\nHere is example notebook of [audiomentation](https:\/\/github.com\/iver56\/audiomentations), a library for audio data augmentaion.\n\n![](http:\/\/)I'm new to this community, so any suggestions for better notebook\/results\/competition are welceome.\n\nAlso, I haven't used this augmentation for submission at this point. Keen to check if these augmentation works well.\n","8be27a25":"### PitchShift"}}