{"cell_type":{"4ae1bd58":"code","22a1c7bb":"code","fe466401":"code","e5551e46":"code","d75fd8d4":"code","3dbd76de":"code","6192ba76":"code","51d7cfcf":"code","6e034ae8":"code","98478b74":"code","53208da0":"code","1d9e36fc":"code","190ba476":"code","b1832618":"code","8cd2ea2c":"code","ee338220":"code","323f9685":"code","dad496e6":"code","df3904a2":"code","86aeca65":"code","9d801fe6":"code","6ef056e0":"code","bcd1f6e6":"code","8082131c":"code","52b0b739":"code","5bd772b8":"code","87163a30":"code","270d6b53":"code","4608dcac":"code","c98afcec":"code","81211013":"code","11a9bbc4":"code","4dfa9c55":"code","22bb5ded":"markdown","c8839966":"markdown","e12e9fa0":"markdown","910377fd":"markdown","0f78f7a1":"markdown","24336b25":"markdown","f8bf5ce7":"markdown","8c38ea60":"markdown","cc16a93d":"markdown","08f30928":"markdown","e61be9ac":"markdown","594f6e8a":"markdown","47ae3bdd":"markdown","0e3fb0a0":"markdown","168bdafe":"markdown","e3bc867c":"markdown"},"source":{"4ae1bd58":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","22a1c7bb":"batch_size = 32\nimg_height = 224\nimg_width = 224\n","fe466401":"TRAIN_DIR = '..\/input\/fruit-and-vegetable-image-recognition\/train\/'\nTEST_DIR = '..\/input\/fruit-and-vegetable-image-recognition\/test\/'\nVALID_DIR = '..\/input\/fruit-and-vegetable-image-recognition\/validation\/'","e5551e46":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(TRAIN_DIR,\n                                                               seed=2509,\n                                                               image_size=(img_height, img_width),\n                                                               batch_size=batch_size)","d75fd8d4":"valid_ds = tf.keras.preprocessing.image_dataset_from_directory(VALID_DIR,\n                                                               seed=2509,\n                                                               image_size=(img_height, img_width),\n                                                               shuffle=False,\n                                                               batch_size=batch_size)","3dbd76de":"test_ds = tf.keras.preprocessing.image_dataset_from_directory(TEST_DIR,\n                                                              seed=2509,\n                                                              image_size=(img_height, img_width),\n                                                              shuffle=False,\n                                                              batch_size=batch_size)","6192ba76":"class_names = train_ds.class_names\nprint(class_names)","51d7cfcf":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n","6e034ae8":"base_model = tf.keras.applications.MobileNetV2(input_shape=(224,224,3),\n                                               include_top=False,\n                                               weights='imagenet')","98478b74":"base_model.trainable = False","53208da0":"data_augmentation = tf.keras.Sequential([\n        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n        tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n])","1d9e36fc":"inputs = tf.keras.Input(shape=(224,224,3))\nx = tf.keras.layers.experimental.preprocessing.Rescaling(1.\/255)(inputs)\nx = data_augmentation(x)\nx = base_model(x,training=False)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dense(1024,activation='relu')(x)\nx = tf.keras.layers.Dense(512,activation='relu')(x)\nx = tf.keras.layers.Dense(len(class_names),activation='softmax')(x)","190ba476":"model = tf.keras.Model(inputs=inputs, outputs=x, name=\"flower_vegetable_Detection_MobileNetV2\")","b1832618":"# Compile the model\nmodel.compile(\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n    optimizer = tf.keras.optimizers.Adam(lr=0.001),\n    metrics = [\"accuracy\"])","8cd2ea2c":"model.summary()","ee338220":"initial_epochs = 5","323f9685":"# Fit the model\nhistory = model.fit(x=train_ds,\n                    epochs= initial_epochs,\n                    validation_data=valid_ds)","dad496e6":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","df3904a2":"base_model.trainable = True","86aeca65":"# Compile the model\nmodel.compile(\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n    optimizer = tf.keras.optimizers.Adam(1e-5),\n    metrics = [\"accuracy\"])","9d801fe6":"fine_tune_epochs = 5\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_fine = model.fit(train_ds,\n                         epochs=total_epochs,\n                         initial_epoch=history.epoch[-1],\n                         validation_data=valid_ds)","6ef056e0":"acc += history_fine.history['accuracy']\nval_acc += history_fine.history['val_accuracy']\n\nloss += history_fine.history['loss']\nval_loss += history_fine.history['val_loss']","bcd1f6e6":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.0, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","8082131c":"# Make prediction on the validation data (not used to train on)\npredictions = model.predict(valid_ds, verbose=1)","52b0b739":"predictions.shape","5bd772b8":"np.sum(predictions[0])","87163a30":"predictions[0]","270d6b53":"class_names[np.argmax(predictions[0])]","4608dcac":"class_names[np.argmax(predictions[15])]","c98afcec":"score = tf.nn.softmax(predictions[0])\nscore","81211013":"np.save('class_names.npy',class_names)","11a9bbc4":"model.save(\"flower_vegetable_detection_mobilenetv2.h5\")","4dfa9c55":"model.evaluate(test_ds)","22bb5ded":"### Test Data","c8839966":"## Compile the model","e12e9fa0":"## Save model and class name","910377fd":"## Define additional layers","0f78f7a1":"## Test Data","24336b25":"## Parameters","f8bf5ce7":"## Base model","8c38ea60":"### Valid data","cc16a93d":"## Import tools","08f30928":"## Generate data batch","e61be9ac":"## Fit the model","594f6e8a":"## Predictions","47ae3bdd":"## View Data","0e3fb0a0":"## Data augmentation","168bdafe":"## Fine Tune","e3bc867c":"### Train data"}}