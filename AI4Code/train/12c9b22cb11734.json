{"cell_type":{"f0453e30":"code","ea90d3d7":"code","1e04e3db":"code","9e41e5be":"code","cd41ec18":"code","f9bc3ab8":"code","dd6e1936":"code","698e9f4a":"code","469d2297":"code","aa08a277":"code","7c8ba702":"code","3ac0b19b":"code","d3ed3c02":"code","035dfc48":"code","307bcf26":"code","ca3e0207":"code","469dfe0f":"code","a2dabd92":"code","65119150":"code","fa1a6713":"code","80fe4dd6":"code","e5e0a87b":"code","d9a086b9":"code","81f6057c":"code","7d05b7d8":"code","2148e734":"code","68e20530":"code","0bbc5db1":"code","d4c3752b":"code","c9be7951":"code","b74bd8c5":"markdown","ad93962a":"markdown","6e25f4c8":"markdown","a4c1c7b4":"markdown","98c672f3":"markdown","cd9746a2":"markdown","1f2f89e0":"markdown","831c782c":"markdown","faf2a060":"markdown","187c0511":"markdown","1ee277d9":"markdown","e7fd5949":"markdown","9669ba1b":"markdown","a93eec37":"markdown","03df0b35":"markdown","166f7489":"markdown","293f2ac3":"markdown","35886001":"markdown","d4125a61":"markdown","da9fad51":"markdown","c89b7d7a":"markdown","9c96398e":"markdown","c34b2e46":"markdown","82b6a8ca":"markdown","94705749":"markdown","2d7bdb5f":"markdown","32c24221":"markdown","7137d36a":"markdown","ea34c1f2":"markdown","7c948b24":"markdown","214483e3":"markdown","af508256":"markdown","601f85c1":"markdown","fa17d722":"markdown","890a6bc0":"markdown"},"source":{"f0453e30":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\nfrom xgboost import XGBRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')","ea90d3d7":"print(os.listdir('\/kaggle\/input\/house-prices-advanced-regression-techniques\/'))","1e04e3db":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf","9e41e5be":"df.columns","cd41ec18":"df.shape","f9bc3ab8":"df.info()","dd6e1936":"df.describe()","698e9f4a":"plt.figure(figsize = (30, 30))\nsns.heatmap(df.corr(), cmap = 'Blues', square = True, annot = True)\nplt.title(\"Visualizing Correlations\", size = 30)\nplt.show()","469d2297":"num_cols = ['OverallQual', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea']\ndf_num_cols = df[num_cols]\ndf_num_cols.isnull().sum()","aa08a277":"sns.pairplot(df[num_cols])\nplt.show()","7c8ba702":"df_nn = df.select_dtypes(include = ['O'])\nnn_list = df_nn.columns.tolist()\nprint(\"There are\", format(len(df_nn.columns)), \"non numerical features in the dataset\\n\\nThe non numerical features are:\\n\", df_nn.columns.tolist(), \"\\n\\nCounting the number of NA values for our non numeric data\")\ndf_nn.isna().sum()","3ac0b19b":"cat_cols = ['MSZoning', 'BldgType', 'ExterQual', 'CentralAir', 'KitchenQual', 'SaleCondition']\n\ndf_cat_cols = df[cat_cols]\n\nsns.countplot(x = df_cat_cols['MSZoning'], data = df_cat_cols)\nplt.show()\n\nsns.countplot(x = df_cat_cols['BldgType'], data = df_cat_cols)\nplt.show()\n\nsns.countplot(x = df_cat_cols['ExterQual'], data = df_cat_cols)\nplt.show()\n\nsns.countplot(x = df_cat_cols['CentralAir'], data = df_cat_cols)\nplt.show()\n\nsns.countplot(x = df_cat_cols['KitchenQual'], data = df_cat_cols)\nplt.show()\n\nsns.countplot(x = df_cat_cols['SaleCondition'], data = df_cat_cols)\nplt.show()","d3ed3c02":"final_features = list(num_cols + cat_cols)\nprint(final_features)","035dfc48":"df = df[final_features + ['SalePrice']]\ndf","307bcf26":"label_encoder = LabelEncoder()\ndf[cat_cols] = df[cat_cols].apply(label_encoder.fit_transform)\ndf","ca3e0207":"X = df.drop(['SalePrice'], axis = 1)\ny = df['SalePrice']\nX","469dfe0f":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\nprint(X, \"\\n\\n\\n\", \"The shape of X is:\", X.shape)\nprint(\"\\n\", y, \"\\n\\n\\n\", \"The shape of y is:\", y.shape)","a2dabd92":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","65119150":"lin_reg = LinearRegression(n_jobs = -1)\nlin_reg.fit(X_train, y_train)\ny_pred = lin_reg.predict(X_test)\nrmse_lin_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"The Root Mean Squared Error for Linear Regression is:\", rmse_lin_reg)\nprint(\"Linear Regression Score is:\", lin_reg.score(X_test, y_test) * 100, \"%\")","fa1a6713":"ridge = Ridge()\nparam_grid = {'alpha': [0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 1, 2, 3, 5, 8, 10, 20, 50, 100]}\ngrid_search_ridge = GridSearchCV(ridge, param_grid, cv = 5)\ngrid_search_ridge.fit(X_train, y_train)\ny_pred = grid_search_ridge.predict(X_test)\nrmse_ridge_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"The Root Mean Squared Error for Ridge Regression is:\", rmse_ridge_reg)\nprint(\"Ridge Regression Score is:\", grid_search_ridge.score(X_test, y_test) * 100, \"%\")","80fe4dd6":"lasso = Lasso()\nparam_grid = {'alpha': [0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 1, 2, 3, 5, 8, 10, 20, 50, 100]}\ngrid_search_las = GridSearchCV(lasso, param_grid, cv = 5)\ngrid_search_las.fit(X_train, y_train)\ny_pred = grid_search_las.predict(X_test)\nrmse_las_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"The Root Mean Squared Error for Lasso Regression is:\", rmse_las_reg)\nprint(\"Lasso Regression Score is:\", grid_search_las.score(X_test, y_test) * 100, \"%\")","e5e0a87b":"el_net = ElasticNet()\nparam_grid = {'alpha': [0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 1, 2, 3, 5, 8, 10, 20, 50, 100],\n             'l1_ratio': np.arange(0.0, 1.0, 0.1)}\ngrid_search_el_net = GridSearchCV(el_net, param_grid, cv = 5)\ngrid_search_el_net.fit(X_train, y_train)\ny_pred = grid_search_el_net.predict(X_test)\nrmse_el_net = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"The Root Mean Squared Error for Elastic Net is:\", rmse_el_net)\nprint(\"Elastic Net Score is:\", grid_search_el_net.score(X_test, y_test) * 100, \"%\")","d9a086b9":"svr = SVR()\nparam_grid = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n              'gamma': ['scale', 'auto'],\n              'C': [1, 10, 100],\n              'epsilon': [0.01, 0.1, 1, 10]}\ngrid_search_svr = GridSearchCV(svr, param_grid, cv = 5)\ngrid_search_svr.fit(X_train, y_train)\ny_pred = grid_search_svr.predict(X_test)\nrmse_svr = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"The Root Mean Squared Error for Support Vector Regressor is:\", rmse_svr)\nprint(\"SVR Score is:\", grid_search_svr.score(X_test, y_test) * 100, \"%\")","81f6057c":"dtr = DecisionTreeRegressor(random_state = 0)\nparam_grid = {'max_depth': list(range(2, 10)),\n              'splitter': ['best', 'random'],\n              'min_samples_leaf': list(range(1, 10)),\n              'max_leaf_nodes': list(range(5, 20))}\ngrid_search_dtr = GridSearchCV(dtr, param_grid, cv = 5)\ngrid_search_dtr.fit(X_train, y_train)\ny_pred = grid_search_dtr.predict(X_test)\nrmse_dtr = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"The Root Mean Squared Error for Decision Tree Regressor is:\", rmse_dtr)\nprint(\"DTR score is:\", grid_search_dtr.score(X_test, y_test) * 100, \"%\")\nprint(\"The best parameters of the decision tree regressor are:\")\nprint(grid_search_dtr.best_params_)","7d05b7d8":"features = df[final_features].columns\nlabel = ['SalePrice']\ntarget = df[label].columns\ndot_data = tree.export_graphviz(\n    DecisionTreeRegressor(max_depth = 5, max_leaf_nodes = 19, min_samples_leaf = 3, splitter = 'best', random_state = 0).fit(X_train, y_train), \n    out_file = None, feature_names = features, class_names = target, filled = True\n    )\ngraph = graphviz.Source(dot_data, format = \"jpg\")\ndisplay(graph)","2148e734":"rfr = RandomForestRegressor()\nparam_grid = {'n_estimators': list(range(100, 200, 10)),\n             'max_depth': list(range(4, 7)),\n             'min_samples_split': list(range(2, 4))}\ngrid_search_rfr = GridSearchCV(rfr, param_grid, cv = 5)\ngrid_search_rfr.fit(X_train, y_train)\ny_pred = grid_search_rfr.predict(X_test)\nrmse_rfr_grid = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"The Root Mean Squared Error is:\", rmse_rfr_grid)\nprint(\"RFR Score is:\", grid_search_rfr.score(X_test, y_test) * 100, \"%\")","68e20530":"abr = AdaBoostRegressor(random_state = 0)\nparam_grid = {'n_estimators': list(range(100, 1000, 100)),\n             'learning_rate': [0.001, 0.01, 0.1, 1, 10]}\ngrid_search_abr = GridSearchCV(abr, param_grid, cv = 5)\ngrid_search_abr.fit(X_train, y_train)\ny_pred = grid_search_abr.predict(X_test)\nrmse_abr_grid = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"The Root Mean Squared Error is:\", rmse_abr_grid)\nprint(\"ABR Score is:\", grid_search_abr.score(X_test, y_test) * 100, \"%\")","0bbc5db1":"xgb = XGBRegressor()\nparam_grid = {'n_estimators': list(range(500, 1000, 100)),\n             'learning_rate': [0.001, 0.01, 0.1]}\ngrid_search_xgb = GridSearchCV(xgb, param_grid, cv = 5)\ngrid_search_xgb.fit(X_train, y_train)\ny_pred = grid_search_xgb.predict(X_test)\nrmse_xgb_grid = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"The Root Mean Squared Error is:\", rmse_xgb_grid)\nprint(\"XGB Score is:\", grid_search_xgb.score(X_test, y_test) * 100, \"%\")","d4c3752b":"data = {'Linear Regression': rmse_lin_reg, 'Ridge Regression': rmse_ridge_reg, 'Lasso Regression': rmse_las_reg, 'Elastic Net': rmse_el_net,\n        'Support Vector Regressor': rmse_svr, 'Decision Tree Regressor': rmse_dtr, 'Random Forest Regressor': rmse_rfr_grid,\n        'Ada Boost Regressor': rmse_abr_grid, 'XGBoost Regressor': rmse_xgb_grid}\ndata = dict(sorted(data.items(), key = lambda x: x[1], reverse = True))\nmodels = list(data.keys())\nRMSE = list(data.values())\nfig = plt.figure(figsize = (30, 10))\nsns.barplot(x = models, y = RMSE)\nplt.xlabel(\"Models Used\", size = 20)\nplt.xticks(rotation = 30, size = 15)\nplt.ylabel(\"RMSE\", size = 20)\nplt.yticks(size = 15)\nplt.title(\"RMSE for different models\", size = 25)\nplt.show()","c9be7951":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf = df[final_features]\ndf[cat_cols] = df[cat_cols].astype(str)\ndf[cat_cols] = df[cat_cols].apply(label_encoder.fit_transform)\nX = df\nX = scaler.fit_transform(X)\ny_pred_xgb = grid_search_xgb.predict(X)\nfinal_pred = list(y_pred_xgb)\ndf = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')['Id']\ndf = pd.DataFrame(df)\ndf['SalePrice'] = final_pred\ndf.to_csv('submission', index = False)\ndf.head()","b74bd8c5":"# Final Features Selected","ad93962a":"# XGBoost Regressor\nXGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework.","6e25f4c8":"# Splitting the Data Into Training and Testing Sets","a4c1c7b4":"# Ridge Regression\nRidge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. ","98c672f3":"# Linear Regression\n\n<h4>What is linear regression ?<\/h4>\n\nIt is a statistical method that is used for predictive analysis. There are 2 types:\n- Simple Linear Regression\n- Multiple Linear Regression\n\n<h4>Simple Linear Regression<\/h4>\n\nSimple linear regression is a regression model that estimates the relationship between one independent variable and one dependent variable using a straight line.\n\n<h4>Formulae for simple linear regression<\/h4>\n<div style=\"width:100%;text-align: center;\"><img align = left src=\"https:\/\/miro.medium.com\/max\/960\/1*jt-pyQQ7bgL2lyganse0nQ.png\" style=\"height:300px;\"><\/div>\n<div style=\"width:100%;text-align: left;\"><img align = left src=\"https:\/\/miro.medium.com\/max\/952\/1*RMGN1iWcl7l8iDy9Vk6HGg.png\" style=\"height:300px;\"><\/div>","cd9746a2":"<h3> Reading the train.csv data <\/h3>","1f2f89e0":"# Preparing the Submission File","831c782c":"# Dataset Information","faf2a060":"<h3>Printing out a list of all the files in the directory<\/h3>","187c0511":"# Random Forest Regressor\nIt it an ensemble technique capable of performing regression. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.\n<div style=\"width:100%;text-align: center;\"><img align = \"left\" src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/02\/rfc_vs_dt1.png\" style=\"height:300px;\"><\/div>","1ee277d9":"<h3> Identifying the shape of the given dataframe (Rows X Columns)<\/h3>","e7fd5949":"<h3>Looking at the non-numeric features of our dataset<\/h3>","9669ba1b":"# Dataset Fields\n\nDescription of the data given:\n\n- **SalePrice**: The property's sale price in dollars. This is the target variable that you're trying to predict.\n- **MSSubClass**: The building class\n- **MSZoning**: The general zoning classification\n- **LotFrontage**: Linear feet of street connected to property\n- **LotArea**: Lot size in square feet\n- **Street**: Type of road access\n- **Alley**: Type of alley access\n- **LotShape**: General shape of property\n- **LandContour**: Flatness of the property\n- **Utilities**: Type of utilities available\n- **LotConfig**: Lot configuration\n- **LandSlope**: Slope of property\n- **Neighborhood**: Physical locations within Ames city limits\n- **Condition1**: Proximity to main road or railroad\n- **Condition2**: Proximity to main road or railroad (if a second is present)\n- **BldgType**: Type of dwelling\n- **HouseStyle**: Style of dwelling\n- **OverallQual**: Overall material and finish quality\n- **OverallCond**: Overall condition rating\n- **YearBuilt**: Original construction date\n- **YearRemodAdd**: Remodel date\n- **RoofStyle**: Type of roof\n- **RoofMatl**: Roof material\n- **Exterior1st**: Exterior covering on house\n- **Exterior2nd**: Exterior covering on house (if more than one material)\n- **MasVnrType**: Masonry veneer type\n- **MasVnrArea**: Masonry veneer area in square feet\n- **ExterQual**: Exterior material quality\n- **ExterCond**: Present condition of the material on the exterior\n- **Foundation**: Type of foundation\n- **BsmtQual**: Height of the basement\n- **BsmtCond**: General condition of the basement\n- **BsmtExposure**: Walkout or garden level basement walls\n- **BsmtFinType1**: Quality of basement finished area\n- **BsmtFinSF1**: Type 1 finished square feet\n- **BsmtFinType2**: Quality of second finished area (if present)\n- **BsmtFinSF2**: Type 2 finished square feet\n- **BsmtUnfSF**: Unfinished square feet of basement area\n- **TotalBsmtSF**: Total square feet of basement area\n- **Heating**: Type of heating\n- **HeatingQC**: Heating quality and condition\n- **CentralAir**: Central air conditioning\n- **Electrical**: Electrical system\n- **1stFlrSF**: First Floor square feet\n- **2ndFlrSF**: Second floor square feet\n- **LowQualFinSF**: Low quality finished square feet (all floors)\n- **GrLivArea**: Above grade (ground) living area square feet\n- **BsmtFullBath**: Basement full bathrooms\n- **BsmtHalfBath**: Basement half bathrooms\n- **FullBath**: Full bathrooms above grade\n- **HalfBath**: Half baths above grade\n- **Bedroom**: Number of bedrooms above basement level\n- **Kitchen**: Number of kitchens\n- **KitchenQual**: Kitchen quality\n- **TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n- **Functional**: Home functionality rating\n- **Fireplaces**: Number of fireplaces\n- **FireplaceQu**: Fireplace quality\n- **GarageType**: Garage location\n- **GarageYrBlt**: Year garage was built\n- **GarageFinish**: Interior finish of the garage\n- **GarageCars**: Size of garage in car capacity\n- **GarageArea**: Size of garage in square feet\n- **GarageQual**: Garage quality\n- **GarageCond**: Garage condition\n- **PavedDrive**: Paved driveway\n- **WoodDeckSF**: Wood deck area in square feet\n- **OpenPorchSF**: Open porch area in square feet\n- **EnclosedPorch**: Enclosed porch area in square feet\n- **3SsnPorch**: Three season porch area in square feet\n- **ScreenPorch**: Screen porch area in square feet\n- **PoolArea**: Pool area in square feet\n- **PoolQC**: Pool quality\n- **Fence**: Fence quality\n- **MiscFeature**: Miscellaneous feature not covered in other categories\n- **MiscVal**: $Value of miscellaneous feature\n- **MoSold**: Month Sold\n- **YrSold**: Year Sold\n- **SaleType**: Type of sale\n- **SaleCondition**: Condition of sale","a93eec37":"# <center>House Price Prediction<\/center>\n<div style=\"width:100%;text-align: center;\"> <img align = middle src=\"https:\/\/images.pexels.com\/photos\/186077\/pexels-photo-186077.jpeg?auto=compress&cs=tinysrgb&dpr=1&w=500\" style=\"height:300px;\"> <\/div>","03df0b35":"# Selecting Numeric Features\nThe numerical features with more than a <b>0.50<\/b> correlation rate with SalePrice have been selected.","166f7489":"<h4>Multiple Linear Regression<\/h4>\n\nFor our dataset since there are many features we are using multiple linear regression.\n\nMultiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable.\n\n<h4>Formula<\/h4>\n<div style=\"width:100%;text-align: center;\"><img align = left src=\"https:\/\/csharpcorner-mindcrackerinc.netdna-ssl.com\/article\/linear-regression2\/Images\/f_MLR.png\" style=\"height:300px;\"><\/div>","293f2ac3":"# Decision Tree Regressor\nIt is a supervised machine learning algorithm. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\nDecision trees regressors normally use mean squared error (MSE) to decide to split a node in two or more sub-nodes.\n<div style=\"width:100%;text-align: center;\"><img align = \"left\" src=\"https:\/\/gdcoder.com\/content\/images\/2019\/05\/Screen-Shot-2019-05-18-at-03.40.41.png\" style=\"height:300px;\"><\/div>\n","35886001":"# AdaBoost Regressor\nAdaBoost regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.","d4125a61":"<h3>Creating a pair plot to identify trends and relations in the data<\/h3>","da9fad51":"# Visualizing the Decision Tree Regressor","c89b7d7a":"# Support Vector Regression\nSupport Vector Regression is a supervised machine learning algorithm. The basic idea behind SVR is to find the best fit line. In SVR, the best fit line is the hyperplane that has the maximum number of points.","9c96398e":"# Elastic Net\nElastic net is a penalized linear regression model that includes both the L1 and L2 penalties during training.","c34b2e46":"<h3>Printing the following information about our dataset:<\/h3>\n<ul>\n    <li>Column Name<\/li>\n    <li>Count of the non-null data<\/li>\n    <li>Datatype of that column<\/li>\n<\/ul>","82b6a8ca":"<h3> Looking at all the columns of the given data <\/h3>","94705749":"# What is GridSearchCV ?\nIt is a library function that helps you loop through pre-defined hyperparameters and fits your model with the best ones. \n\n<h3>Advantage<\/h3>\nGridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method.\n\n<h3>Disadvantage<\/h3>\n<span>It is very time consuming.\n\n<div style=\"width:100%;text-align: center;\"><img align = \"left\" src=\"https:\/\/c.tenor.com\/hHMlCXdEfbEAAAAC\/laptop-breaking.gif\" style=\"height:300px;\"><\/div>","2d7bdb5f":"# Scaling the Data\nStandardScaler standardizes a feature by subtracting the mean and then scaling it to unit variance.\n<h3>Formula<\/h3>\n<div style=\"width:100%;text-align: center;\"><img align = left src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/0*vQEjz0mvylP--30Q.GIF\" style=\"height:300px;\"><\/div>","32c24221":"# Encoding the Categorical Data\nThe categorical data can be encoded using Label Encoder. It encodes labels with a value between 0 and n_classes-1 where n is the number of distinct labels. If a label repeats it assigns the same value as assigned earlier. The categorical values can be converted into numeric values.","7137d36a":"# Lasso Regression\nLASSO stands for Least Absolute Shrinkage and Selection Operator. It is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.","ea34c1f2":"<h3>Printing the statistical summary of our dataset such as:<\/h3>\n<ul>\n    <li>Count<\/li>\n    <li>Mean<\/li>\n    <li>Standard Deviation<\/li>\n    <li>Minimum<\/li>\n    <li>25<sup>th<\/sup> Percentile<\/li>\n    <li>50<sup>th<\/sup> Percentile<\/li>\n    <li>75<sup>th<\/sup> Percentile<\/li>\n    <li>Maximum<\/li>     \n<\/ul>","7c948b24":"# Importing the Necessary Libraries","214483e3":"# Visualize the Count of the Categorical Data","af508256":"# Identifying Correlations in the Data","601f85c1":"# Visualizing the Results","fa17d722":"<h3>Assigning features to X and the labels to y<\/h3>","890a6bc0":"<h3>Final dataset used with only the selected features"}}