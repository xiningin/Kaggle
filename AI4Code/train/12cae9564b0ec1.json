{"cell_type":{"ee1077f5":"code","b6c5b946":"code","5f9a7610":"code","97bfd515":"code","2ab20056":"code","b5d0453c":"code","9eaa2e02":"code","aa350f4e":"code","eb0996eb":"code","88907098":"code","6a031992":"code","a0e1341c":"code","a21ec927":"code","f60df97b":"code","ee0370b1":"code","9305315d":"code","8a1ad1d1":"code","7f5bac7b":"code","296a30ad":"code","a403cc5e":"code","44bc8adc":"code","79c4d9df":"code","a77fe95c":"code","aeec22b7":"code","bef0a324":"code","437a9efc":"code","679a4ff6":"code","8fb2d9fd":"code","0f746e6b":"code","cbc6e9b5":"code","9167ca05":"code","0ac6e89f":"code","150bb939":"code","7be5b339":"code","b7278ff7":"code","538443a0":"code","8cfc8e19":"code","fc0b9bc1":"code","5e29659c":"code","08f7777b":"code","eeaed1c6":"code","76cf57c6":"code","8e85f298":"code","1bf0c078":"code","8daf786e":"code","fd93051c":"code","650d659a":"code","7ef9df9a":"markdown","1b690bd5":"markdown","fd9f7892":"markdown","73fb7041":"markdown","5042c1f7":"markdown","73317126":"markdown","89e62d51":"markdown","bbcdff21":"markdown","e447db05":"markdown","54878e16":"markdown","d2100283":"markdown","5c83ae0b":"markdown","db4d7040":"markdown","a1225afb":"markdown","301a6cfd":"markdown","3280fdc5":"markdown","7ccfe159":"markdown","73e89076":"markdown","3183d645":"markdown","b9be70d9":"markdown","ff1e6927":"markdown","5ba9885a":"markdown","87644fc1":"markdown","61009718":"markdown"},"source":{"ee1077f5":"# Load necessary library\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import StrMethodFormatter\nimport matplotlib.ticker as mtick\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\n\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn')\n%matplotlib inline\n\n# set default plot size\nplt.rcParams[\"figure.figsize\"] = (15,8)","b6c5b946":"# Load and preview data \nrecruit = pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\nrecruit.head()","5f9a7610":"# drop id column\nrecruit.drop('sl_no',axis=1,inplace=True)\nrecruit.shape","97bfd515":"# Summary Statistics\nrecruit.describe()","2ab20056":"# Check each column for nas\nrecruit.isnull().sum()","b5d0453c":"sns.pairplot(recruit.drop('salary',axis=1),hue = 'status')\n\n# gender             0\n# ssc_p              0\n# ssc_b              0\n# hsc_p              0\n# hsc_b              0\n# hsc_s              0\n# degree_p           0\n# degree_t           0\n# workex             0\n# etest_p            0\n# specialisation     0\n# mba_p              0\n# status             0\n# salary            67","9eaa2e02":"recruit.groupby([\"gender\",\"status\"]).size().unstack()","aa350f4e":"recruit.groupby([\"gender\",\"status\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend(loc = 'upper right',title = 'Status')\nplt.show()\n\n# most of males are placed job than female","eb0996eb":"recruit.groupby('status').mean()","88907098":"recruit_numeric = recruit[['ssc_p','hsc_p','degree_p','etest_p','mba_p','status']]\n\nrecruit_numeric_melt = pd.melt(recruit_numeric,id_vars='status',\n                               value_vars =['ssc_p','hsc_p','degree_p','etest_p','mba_p'])\nrecruit_numeric_melt.head()","6a031992":"sns.boxplot(x=\"variable\", y=\"value\",\n            hue=\"status\", data=recruit_numeric_melt)","a0e1341c":"# then will look at all the categorical variables\n\n# column description \n# ssc_b              Board of Education- Central\/ Others\n# hsc_b              Board of Education- Central\/ Others\n# hsc_s              Specialization in Higher Secondary Education\n# degree_t           Under Graduation(Degree type)- Field of degree education\n# workex             Work Experience\n# specialisation     Post Graduation(MBA)- Specialization\n# status             Status of placement- Placed\/Not placed\n# salary             Salary offered by corporate to candidates\n\n\n# Board of Education - 10th grade\nrecruit.groupby([\"ssc_b\",\"status\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend(loc = 'upper right',title = 'Board of Education')\nplt.show()\n\n# central and others almost no difference for secondary education board of education","a21ec927":"# Board of Education - 12th grade\nrecruit.groupby([\"hsc_b\",\"status\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend(loc = 'upper right',title = 'Board of Education')\nplt.show()\n\n# similarly central and others almost no difference for secondary education board of education","f60df97b":"# Specialization in Higher Secondary Education\nrecruit.groupby([\"hsc_s\",\"status\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend(loc = 'upper right',title = 'Higher Education Specialization')\nplt.show()\n\n# commerce and science are more likely to get placed","ee0370b1":"# Under Graduation(Degree type)- Field of degree education\nrecruit.groupby([\"degree_t\",\"status\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend(loc = 'upper right',title = 'Degree type')\nplt.show()\n\n# for undergraduate degrees, comm\/management and sci\/tech are more likely to get placed","9305315d":"# Work Experience\nrecruit.groupby([\"workex\",\"status\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend(loc = 'upper right',title = 'Work experience')\nplt.show()\n\n# having working experience is more likely to get placed and it has the most influence by comparing the graphs","8a1ad1d1":"# Post Graduation(MBA)- Specialization\nrecruit.groupby([\"specialisation\",\"status\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend(loc = 'upper right',title = 'specialisation')\nplt.show()\n\n# mrkt\/finance are more likely to get placed than mrkt\/hr","7f5bac7b":"# transfer categorical vaeiables to dummy variables\n\nrecruit.loc[recruit['gender'] == 'M', 'gender'] = 1.0\nrecruit.loc[recruit['gender'] == 'F', 'gender'] = 0.0\n\nrecruit.loc[recruit['status'] == 'Placed', 'status'] = 1\nrecruit.loc[recruit['status'] == 'Not Placed', 'status'] = 0\n\nrecruit.loc[recruit['workex'] == 'Yes', 'workex'] = 1.0\nrecruit.loc[recruit['workex'] == 'No', 'workex'] = 0.0\n\n\ncategorical_var = ['ssc_b','hsc_b','hsc_s','degree_t','specialisation']\n\n\n# create dummy variables for all the other categorical variables\n\nfor variable in categorical_var:\n# #     fill missing data\n#     recruit[variable].fillna('Missing',inplace=True)\n#     create dummy variables for given columns\n    dummies = pd.get_dummies(recruit[variable],prefix=variable)\n#     update data and drop original columns\n    recruit = pd.concat([recruit,dummies],axis=1)\n    recruit.drop([variable],axis=1,inplace=True)\n\n\nrecruit.head()","296a30ad":"# Create separate dataset for placed status\n# use this for further regression analysis\nrecruit_placed = recruit[recruit['status'] == 1].drop('status',axis = 1)\nrecruit_placed.head()","a403cc5e":"x = recruit.drop(['status','salary'], axis=1)\ny = recruit['status'].astype(float)\n\n# split train and test dataset\ntrain_x, test_x, train_y, test_y = train_test_split(x,y , test_size=0.3, random_state=42)\n\nprint(train_x.shape)\nprint(train_y.shape)","44bc8adc":"rf_regressor = RandomForestRegressor(100, oob_score=True,\n                                     n_jobs=-1, random_state=42)\nrf_regressor.fit(train_x,train_y)\nprint('Score: ', rf_regressor.score(train_x,train_y))","79c4d9df":"feature_importance = pd.Series(rf_regressor.feature_importances_,index=x.columns)\nfeature_importance = feature_importance.sort_values()\nfeature_importance.plot(kind='barh')","a77fe95c":"# parameter tunning\n# # of trees trained parameter tunning\n\nresults = []\nn_estimator_options = [30,50,100,200,500,1000,2000]\n\nfor trees in n_estimator_options:\n    model = RandomForestRegressor(trees,oob_score=True,n_jobs=-1,random_state=42)\n    model.fit(x,y)\n    print(trees,\" trees\")\n    score = model.score(train_x,train_y)\n    print(score)\n    results.append(score)\n    print(\"\")\n\npd.Series(results,n_estimator_options).plot()","aeec22b7":"# max number of features parameter tunning\nresults = []\nmax_features_options = ['auto',None,'sqrt','log2',0.9,0.2]\n\nfor max_features in max_features_options:\n    model = RandomForestRegressor(n_estimators=200,oob_score=True,n_jobs=-1,\n                                  random_state=42,max_features=max_features)\n    model.fit(x,y)\n    print(max_features,\" option\")\n    score = model.score(train_x,train_y)\n    print(score)\n    results.append(score)\n    print(\"\")\n\npd.Series(results,max_features_options).plot(kind='barh')","bef0a324":"# min sample leaf parameter tunning\nresults = []\nmin_sample_leaf_option = [1,2,3,4,5,6,7,8,9,10]\n\nfor min_sample_leaf in min_sample_leaf_option:\n    model = RandomForestRegressor(n_estimators=200,oob_score=True,n_jobs=-1,\n                                  random_state=42,max_features='sqrt',\n                                  min_samples_leaf=min_sample_leaf)\n    model.fit(x,y)\n    print(min_sample_leaf,\" min samples\")\n    score = model.score(train_x,train_y)\n    print(score)\n    results.append(score)\n    print(\"\")\n\npd.Series(results,min_sample_leaf_option).plot()","437a9efc":"rf_regressor = RandomForestRegressor(200, oob_score=True,max_features='sqrt',\n                                     n_jobs=-1, random_state=42,min_samples_leaf=1)\nrf_regressor.fit(x,y)\nprint('Score: ', rf_regressor.score(train_x,train_y))","679a4ff6":"pred_y = rf_regressor.predict(test_x)\n\nprint(test_y[:10])\nprint(pred_y[:10])","8fb2d9fd":"rf_classifier = RandomForestClassifier(200, oob_score=True,\n                                     n_jobs=-1, random_state=42)\nrf_classifier.fit(train_x, train_y)","0f746e6b":"pred_y = rf_classifier.predict(test_x)","cbc6e9b5":"rf_classifier.score(test_x, test_y)","9167ca05":"mat = confusion_matrix(test_y,pred_y)\nsns.heatmap(mat, square=True, annot=True, cbar=False) \nplt.xlabel('predicted value')\nplt.ylabel('true value')","0ac6e89f":"print(classification_report(test_y, pred_y))","150bb939":"lr_model = LogisticRegression()\nlr_model.fit(train_x,train_y)","7be5b339":"lr_model.score(test_x, test_y)","b7278ff7":"pred_y = lr_model.predict(test_x)\nmat = confusion_matrix(test_y,pred_y)\nsns.heatmap(mat, square=True, annot=True, cbar=False) \nplt.xlabel('predicted value')\nplt.ylabel('true value')","538443a0":"print(classification_report(test_y, pred_y))","8cfc8e19":"lr_coef = pd.DataFrame({\"Coefficients\":lr_model.coef_[0]},index = x.columns.tolist())\nlr_coef = lr_coef.sort_values(by = 'Coefficients')\nlr_coef","fc0b9bc1":"lr_coef.plot(kind='barh')","5e29659c":"recruit_placed.head()","08f7777b":"sns.pairplot(recruit_placed[['ssc_p','hsc_p','degree_p','etest_p','mba_p','salary']])","eeaed1c6":"recruit_placed[['ssc_p','hsc_p','degree_p','etest_p','mba_p','salary']].corr()","76cf57c6":"var = ['ssc_p','hsc_p','degree_p','etest_p','mba_p','gender','workex']\nx = recruit_placed.loc[:,var]\n# x = recruit_placed.loc[:,recruit_placed.columns != 'salary']\ny = recruit_placed.loc[:,recruit_placed.columns == 'salary']\nx.head()","8e85f298":"train_x, test_x, train_y, test_y = train_test_split(x,y , test_size=0.2, random_state=42)\n\nprint(train_x.shape)\nprint(test_x.shape)","1bf0c078":"linear_model = sm.OLS(train_y,train_x.astype(float))\nresults = linear_model.fit()\nresults.params","8daf786e":"print(results.summary())","fd93051c":"pred_y = results.predict(test_x)\n# print(pred_y[:10])\n# print(test_y[:10])\n\ncol = ['actual','prediction']\n\nprediction = pd.concat([test_y,pred_y],axis=1)\nprediction.columns = col\nprediction","650d659a":"_, ax = plt.subplots()\n\nax.scatter(x = range(0, test_y.size), y=test_y, c = 'blue', label = 'Actual', alpha = 0.3)\nax.scatter(x = range(0, pred_y.size), y=pred_y, c = 'red', label = 'Predicted', alpha = 0.3)\n\nplt.title('Actual and predicted values')\nplt.xlabel('Observations')\nplt.ylabel('Salary')\nplt.legend()\nplt.show()","7ef9df9a":"We've also print out the coefficients of the model and plot them on a barplot. The most important factors are previous work experience of the student.","1b690bd5":"## Classification\n\n### Random Forest Regressor and Classifier\n\nRandom forest is a powerful classifier in machine learning in that it can not only be used as a classifier, but also as a regressor. Random Forest also works well for small datasets, which made it the first choice of our classification problem.\n\nAs mentioned above, before training the model, we need to transform all of the categorical variables. For binary variables, we are transfering them into 0 and 1. For other multi class variables, we are transferring them using *One Hot Encoding* by using `pd.get_dummies` from the pandas library.\n","fd9f7892":"After the analysis above, the parameter we're choosing for each of the parameter are:\n\n- `n_estimators`: 200\n- `max_features`: sqrt\n- `min_samples_leaf`: 1\n\nAnd by inputting these new values, we got a better model with a slightly higher model score.","73fb7041":"To sum up the information found on the plots above:\n\n- Board of Education - 10th grade and 12th grade has less influence on status\n- For secondary education, commerce and science are more likely to get placed\n- For undergraduate degrees, comm\/management and sci\/tech are more likely to get placed\n- Having working experience is more likely to get placed and it has the most influence by comparing the graphs\n- For Post Graduation(MBA), mrkt\/finance are more likely to get placed than mrkt\/hr\n\nThis wrapped up EDA and data visualization.\n\nThe next step would be training classifier using random forest and logistic regression.","5042c1f7":"After we finished with the numeric variables, we proceeded with all the categorical variables. Applying the same approach for all the categorical variables, we used stacked barplot to see the count of observations in each group.","73317126":"Similarly, then we train a Random Forest Classifier to see how the data perform with a classifier using the same parameters we got from the regressor.\nThe model score is 0.8 and from the confusion matric, we can see that the classifier do a better job in correctly predicting student being placed a job then not being placed a job, as only 2 were misclassified into not placed while 10 were misclassified into placed.","89e62d51":"For linear regression, we are using `OLS` from `statsmodel` package. The model has a R square of 0.91. The variables that have the highest influence are:\n\n- gender (positive)\n- workex (positive)\n- degree_p (negative)\n- ssc_p (negative)","bbcdff21":"Then we used `feature_importance_` function to see what are the most important features in the Random Forest Regressor and visualize the importance using a bar plot.\nFrom the plot, we can see that the top 3 important factors are:\n\n- ssc_p\n- degree_p\n- hsc_p","e447db05":"After we train the initial regressor, we proceed with parameter tunning to try and find the optimized value for\n\n- `n_estimators`: The number of trees in the forest\n- `max_features`: The number of features to consider when looking for the best split\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node\n\nBy inputting different values or methods, we will try and find out the value that provides the highest score.","54878e16":"The following table shows the average of all the numeric variables under different status. We can see that other than **mba_p**, all the other variables have difference in mean","d2100283":"## Regression\n\nThe second purpose of this analysis is the find out the important factors that influenced students' salary. For this analysis, we are using the **recruit_placed** dataset.","5c83ae0b":"After we load the data, we can see that there are 215 observations and 14 columns in this dataset with a mix of categorical and numeric variables. The target variable for our classification problem is the **status** column, which is stored as **Placed** and **Not Placed**. There are other binary variables such as **gender**, **workex** need to transfer into 0 and 1 later in the analysis. Other categorical variables with multiple levels, we'll use *One Hot Encoding* to transfer to binary variables. Also, the first column **sl_no** will be dropped from the dataset as it's an index column.\n\nBy the first glance of the dataset, it's pretty clean. There's no missing values other that the salary for those who wasn't offered a job. Thus **salary** will be excluded in the first part of the analysis (Classification).\n\n## Exploratory Data Analysis and Data Visualization\n\nAfter we get the first look at the data, we'd like to get a better understanding by performing some EDA and data visualizations.\n\nWe first start with a pairplot from the `seaborn` library using `sns.pariplot()`. This will give us the correlation of all numeric variables in the dataset. We've also set `hue = 'status'` to see how the value distribute under different status. This can help us better identify variables later in the classifier.\n\nFrom the plot, we can see that the following variables might be significantly different under different student status\n\n- ssc_p: Secondary Education percentage- 10th Grade\n- degree_p: Degree Percentage\n\nOther variables shows different distribution under different status as well.","db4d7040":"Print out the prediction and compare them with the test dataset, we can see that some of the prediction are close, but the model definitely needs further fine tunning.","a1225afb":"We are using all the columns as our independent variables and y variable would be **status**.\n\nSplit the training and testing dataset into 70\/30 split.","301a6cfd":"### Logistic Regression Classifier\n\nWe want to fit the data with different type of classifiers to see which classifier works best for this dataset. The second classifier used is Logistic Regression.\n\nFollowing the same step of fitting the model and making prediction from the test data. Logistic regression have a model score of 0.84, which is slightly higher than the Random Forest Classifier. And from the confusion matrix, it performed better in classifying not placed students than Random Forest.","3280fdc5":"W've selected the following variables for the linear regression.","7ccfe159":"We first fit the training dataset with a Random Forest Regressor by inputting some generic parameters and print out the model score.\nWe can see that the model has a score of 0.93.","73e89076":"We first plot the pairplot to examine the correlation between different variables and print out the correlation matrix.","3183d645":"Just to see how good the model performs, we compared the top 10 value from the test dataset and also the predicted probability from the model. We can see that most of the prediction is accurate","b9be70d9":"# Campus Recruitment Analysis\n\n## Introduction\n\nThe data from this analysis is from [kaggle Campus Recruit](https:\/\/www.kaggle.com\/benroshan\/factors-affecting-campus-placement), which contains 215 student data including their education level, degree, gender, specialization ect.\n\nThe purpose of this analysis is to analyze the data and build a classifier using machine learning techniques such as decision tree and random forest, logistic regression to classify the student status into being placed a job or not being placed a job. The analysis will use different classification techniques and compare which classifier make the best prediction for this dataset.\n\nThe second part of the analysis will perform a regression analysis using only the students that get job to find out some of the key factors that influence the salary of an offer. The analysis are performed in Python using Jupyter Notebook. Various techniques that are used in this analysis are:\n\n- Explorary data analysis\n- Random forest regressor and classifier\n- Logistic regression classifier\n- Multiple linear regression\n\n\n## Loading data and libraries\n\nThe analysis starts with loading data and necessary library.\n","ff1e6927":"## Data loading and cleanning","5ba9885a":"We then want to see the student status by gender. From the table below and the barplot, we can see that more male students were placed a job then female.","87644fc1":"To further examine the distribution of numeric variables under different groups, boxplots are useful. But first, we need to tranfer the dataset so that we can visualize all the numeric variables under one graph.\n\nThe first step is to extract all the numeric variables and then transfer the dataset from **wide** to **long** format using `pd.melt()` .\n\nAfter that, we can plot the scatter plots. The scatter plot below shows the similiar information as what we see on the table, which is almost all variables have a higher value in the placed group than not placed group, while mba percent seem to have the least influence on whether a student is placed or not.","61009718":"Also here we are creating a separate dataset for regression analysis using observations that have status **Placed**."}}