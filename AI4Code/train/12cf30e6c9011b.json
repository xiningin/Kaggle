{"cell_type":{"f8d78da4":"code","df201eeb":"code","88e41dbd":"code","f8940eaf":"code","4ca59560":"code","7b70b173":"code","4a448d5e":"code","891aef5d":"code","e4ac1272":"code","11753303":"code","893f31b3":"code","7c5df8a0":"code","894ae6d1":"code","6f365727":"code","8be0d6dd":"code","1404a172":"code","250fec83":"code","2c9936b7":"code","94774633":"code","3c192021":"code","21c071b2":"code","ef904ed6":"code","6ecacfcf":"code","d7e7a951":"code","ab7efba0":"code","c86c73da":"code","c093d529":"code","76f22ce8":"code","a857aba4":"code","97c9b2b9":"code","d8ce29ec":"code","ffbb92c6":"code","997eaba4":"code","18d4440c":"code","97a6cd4a":"markdown","c5623048":"markdown","c2974e81":"markdown","af4fefc1":"markdown","8637484f":"markdown","0bb31dcd":"markdown","925e762d":"markdown","fa23510b":"markdown","abdb472e":"markdown","c9fb2f23":"markdown","18473997":"markdown","9e67cde3":"markdown"},"source":{"f8d78da4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df201eeb":"X = 2*np.random.rand(100,1)\ny = 4+3*X + np.random.randn(100,1)\n\n#y = 4 + 3x1 + noise.","88e41dbd":"plt.plot(X, y, \"b.\")\nplt.axis([0,2,0,15])\n","f8940eaf":"#lets compute theta_hat\nx_b = np.c_[np.ones((100,1)), X]    #add x0=1 to each instance\ntheta_hat = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)","4ca59560":"theta_hat","7b70b173":"x_new = np.array([[0], [2]])\nx_new_b = np.c_[np.ones((2,1)), x_new]\n\ny_predict = x_new_b.dot(theta_hat)\ny_predict","4a448d5e":"#lets plot\nplt.plot(x_new, y_predict, \"r-\")\nplt.plot(X,y, \"b.\")\nplt.axis([0,2,0,15])\nplt.legend()\nplt.show()","891aef5d":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\n\nlin_reg.fit(X,y)\nlin_reg.intercept_, lin_reg.coef_","e4ac1272":"lin_reg.predict(x_new)","11753303":"alpha =0.1 #learning rate\nn_iterations = 1000\nm = 100   #no. of training examples\n\ntheta = np.random.randn(2,1)\n\nfor iterations in range(n_iterations):\n    gradient = 2\/m * x_b.T.dot(x_b.dot(theta) -y)\n    theta = theta - alpha* gradient\n#     print(theta)","893f31b3":"theta","7c5df8a0":"n_epochs = 50\nt0 , t1 = 5, 50  #learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0\/(t+t1)\n\ntheta = np.random.randn(2,1)\n\nfor epoch in range(n_epochs):\n    for i in range(m):\n        random_index = np.random.randint(m)\n        xi = x_b[random_index:random_index+1]\n        yi = y[random_index:random_index+1]\n        gradients = 2*xi.T.dot(xi.dot(theta)-yi)\n        alpha = learning_schedule(epoch*m+i)\n        theta = theta - alpha * gradients\n        ","894ae6d1":"theta","6f365727":"m = 100\nx = 6 * np.random.rand(m,1)-3\ny = 0.5 * x**2 + x + 2 + np.random.randn(m,1)   #y=ax^2 + bx + c\nplt.plot(x,y, 'b.')","8be0d6dd":"y","1404a172":"from sklearn.preprocessing import PolynomialFeatures\npoly_feature = PolynomialFeatures(degree=2, include_bias=False)\nx_poly = poly_feature.fit_transform(x)\nx[0]","250fec83":"x_poly[0]","2c9936b7":"lin_reg = LinearRegression()\nlin_reg.fit(x_poly, y)\nlin_reg.intercept_, lin_reg.coef_","94774633":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n","3c192021":"def plotLearningCurve(model, x, y):\n    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n    train_errors, val_errors = [], []\n    for m in range(1, len(x_train)):\n        model.fit(x_train[:m], y_train[:m])\n        y_train_predict = model.predict(x_train[:m])\n        y_val_predict = model.predict(x_val)\n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n        val_errors.append(mean_squared_error(y_val, y_val_predict))\n        \n    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"validation\")\n    plt.xlabel(\"Training Set Size\")\n    plt.ylabel(\"RMSE\")\n#     plt.legend()\n#     plt.show()","21c071b2":"lin_reg = LinearRegression()\nplotLearningCurve(lin_reg, x, y)","ef904ed6":"#lets try 10th degree polynomial model\n\nfrom sklearn.pipeline import Pipeline\npolynomial_regression= Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=4\n                                         , include_bias =False)),\n    (\"lin_reg\", LinearRegression()),\n])\nplotLearningCurve(polynomial_regression, x,y)","6ecacfcf":"from sklearn.linear_model import Ridge\nridge_reg = Ridge(alpha = 1, solver=\"cholesky\")\nridge_reg.fit(x,y)\nridge_reg.predict([[1.5]])\n","d7e7a951":"#using SGDRegressor\nfrom sklearn.linear_model import SGDRegressor\nsgd_reg = SGDRegressor(penalty=\"l2\")\nsgd_reg.fit(x,y.ravel())\nsgd_reg.predict([[1.5]])\n","ab7efba0":"from sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(x,y)\nlasso_reg.predict([[1.5]])","c86c73da":"from sklearn import datasets\n","c093d529":"iris = datasets.load_iris()\nlist(iris.keys())","76f22ce8":"iris","a857aba4":"x = iris[\"data\"][:, 3:]  #petal width\ny = (iris[\"target\"] == 2).astype(np.int)  # 1 if iris-verginica","97c9b2b9":"x[1:4]","d8ce29ec":"#train model\nfrom sklearn.linear_model import LogisticRegression\n\nreg = LogisticRegression()\nreg.fit(x,y)","ffbb92c6":"#model's estimated probabilities \nx_new = np.linspace (0, 3, 1000).reshape(-1,1)\ny_prob = reg.predict_proba(x_new)","997eaba4":"y_prob","18d4440c":"#lets plot\nplt.plot(x_new, y_prob[:,1], \"g-\", label=\"Virginica\")\nplt.plot(x_new, y_prob[:,0], \"b--\", label=\"Not Virginica\")\nplt.legend()","97a6cd4a":"# Ploynomial Regression\n\nMore complex data","c5623048":"# Regularized Linear Models\n\n### 1. Ridge Regression (Regularized Version of Linear Regression)\n\ntheta_hat = inverse(x_t.x + alpha.A) * x_t.y\n","c2974e81":"# Learning Curves\nUse to know that model is underfitting or overfitting.\n\n*Plots of the model's performance on training and validation set as afunction of trainin iterations*","af4fefc1":"# Logistic Regression\n\nIt is also use to classification algorithm\n\nDataset = iris","8637484f":"### 1. Lasso Regression\nIt adds a regularization term to the cost function, but it uses the l1 norm of the weight vector instead of half the square of the l2 norm\n\nit tends to completely eliminate the weights of least importnat features.","0bb31dcd":" So we got our best fit line\n \n ### Lets perform Linear regression using Sckit learn","925e762d":"# Stochastic Gradient Descent\nIt picks random instance in training set at every step and comput the Gradient based only on that single instannce.\nIt makes algo much faster.","fa23510b":"Here both the curves have reached a model and they are close. SO this is typically an underfitting model","abdb472e":"Here, theta_0 = 4 and theta_1 = 3 (approx)\n\n#### Lets make predictions using theta_hat","c9fb2f23":"**Linear Regression** using Normal Equation is a good thing but it is slow process when number of features are very large ie it may go to complexity of O(n3). \n\n\n*SO for this we will use the Gradient Descent approach*","18473997":"Thank You\n\nSources: Hands on Ml (BOOK)\n        Youtube","9e67cde3":"# Gradient Descent\nIt uses Iterations to converge. \nOur Task is to minimise the Gradient Descent or Cost Function in less learning step. \nIf learning Rate is too small then algo will have to go through many iterations and it will take time.\nIf Learning Rate is too high then algo will not converge and GD will not work\n\n![image.png](attachment:image.png)\n\n\n\n![image.png](attachment:image.png)\n\nPitFalls In GD\n\n\n**Formulaa For Gradient Descent Step**\n\ntheta_nest = theta - alpha*(2\/m * X_t (X * theta -y))"}}