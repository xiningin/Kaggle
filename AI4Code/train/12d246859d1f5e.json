{"cell_type":{"77f7d31f":"code","c14e29ac":"code","b53b0461":"code","e9c8d7fc":"code","7d3a3cfa":"code","ca73b2bc":"code","4bac5ac2":"code","c378a499":"code","9336d7d0":"code","0b224f40":"code","112f62c8":"code","d08ba1ea":"code","17e85cd2":"code","7fb4dd32":"code","71fd9bb8":"code","93b1edc3":"code","339c596a":"code","8851f6bb":"code","e2cf3ac4":"code","7ffa29ae":"code","a666b023":"code","4e969043":"code","46c8c4c9":"code","6786fd6b":"code","10812cb3":"code","840d7fa0":"code","f352638d":"code","4458b0c2":"code","0ec1eb65":"code","d10eba3b":"code","63efd72d":"code","f0eb7d19":"code","4d936a52":"code","75829e52":"code","ecdd1876":"code","5e1d0534":"code","f29fd250":"code","670f1ffe":"code","e518e89f":"code","c3fc5c29":"code","49d881d8":"code","b404ef1c":"code","99c534f0":"code","51cb181d":"code","d1f15032":"code","bf6a80bc":"code","cba52a44":"code","40991fe8":"code","b4bcfc7b":"code","2e1978eb":"code","fa7f4090":"code","7ec11791":"code","4b68061b":"code","464ded0b":"code","b95ee45b":"code","93a5b3dc":"code","670d16d9":"code","c1660417":"code","dd66137f":"code","4bc9315b":"code","93321623":"code","46c06d35":"code","63aa0d0b":"code","1251a701":"code","c88500c4":"code","4e344eea":"code","cf610145":"code","49dc2ab9":"code","06e5e1f2":"code","61317f64":"code","6890b8c0":"code","df7c8eea":"code","d98492ee":"code","a4650a6b":"code","5088155e":"code","e1ba0b40":"code","dd4a4179":"code","4a84e685":"code","f193e17e":"code","a6e05f1f":"code","2b4a0694":"markdown","fc8c8794":"markdown","4aae7980":"markdown","2ce4d3a2":"markdown","38b142fd":"markdown","8b705871":"markdown","80ea9900":"markdown","885590f2":"markdown","cc561920":"markdown","8dd90c1c":"markdown","31be8d2d":"markdown","860cfa43":"markdown","b7fa02c3":"markdown","5f364579":"markdown","625a2475":"markdown","4b510289":"markdown","44211cbb":"markdown","ecb7aef6":"markdown","5639ba5d":"markdown","856c3d1c":"markdown","382490de":"markdown","d1172f21":"markdown","d3845793":"markdown","7d54a163":"markdown","c90e1719":"markdown","22b3015b":"markdown","d3a7ed14":"markdown","351da5fa":"markdown","59cadd90":"markdown","94a7b85c":"markdown","d117458c":"markdown","f1c5e912":"markdown","052003b7":"markdown","71540c27":"markdown","a5c2c4c9":"markdown","1e9bf73f":"markdown","60339314":"markdown","2ffb8589":"markdown","d1266f80":"markdown","ea524129":"markdown"},"source":{"77f7d31f":"# Bibliotecas\nimport os\nimport pandas as pd\nimport re\nimport unidecode\nimport numpy as np\nimport warnings\nimport seaborn as sns\nsns.set()\nsns.set_context(\"paper\")\nwarnings.filterwarnings(\"ignore\")","c14e29ac":"def cleanDF(df) -> pd.DataFrame:\n    # Remover Duplicados\n    df = df.drop_duplicates(keep='first')\n    \n    # Transformar coluna KM e BR, em que h\u00e1 strings, em float e substituir \"(null)\" por -1\n    if df['km'].dtype == 'object':\n        df['km'] = df['km'].str.replace(r'\\(null\\)', '-1.0')\n        df['km'] = df['km'].str.replace(',', '.').apply(float)\n        \n    if df['br'].dtype == 'object':\n        df['br'] = df['br'].str.replace(r'\\(null\\)', '-1.0')\n        df['br'] = df['br'].str.replace(',', '.').apply(float)\n        \n    else:\n        pass\n\n    # Formatar para minusculo, remover acentos e espacos\n    for col in df.select_dtypes(include='object').columns:\n        if df[col].isna().sum() > 0:\n            df[col] = df[col].replace(np.nan, df[col].value_counts().idxmax())\n        else:\n            pass\n        try:\n            df[col] = df[col].apply(lambda x: unidecode.unidecode(x).lower().strip())\n        except:\n            print('Error in col: ', col)\n    \n    # Formatar Data YYYY-MM-DD\n    df['data_inversa'] = pd.to_datetime(df['data_inversa'], dayfirst=True)\n    df['ano'] = df['data_inversa'].dt.year\n    df['mes'] =  df['data_inversa'].dt.month\n    \n    \n    # Ordena\u00e7\u00e3o das colunas\n    cols = ['id', 'data_inversa', 'dia_semana', 'horario', 'uf', 'br', 'km',\n       'municipio', 'causa_acidente', 'tipo_acidente',\n       'classificacao_acidente', 'fase_dia', 'sentido_via',\n       'condicao_metereologica', 'tipo_pista', 'tracado_via', 'uso_solo',\n       'pessoas', 'mortos', 'feridos_leves', 'feridos_graves', 'ilesos',\n       'ignorados', 'feridos', 'veiculos','ano','mes']\n    \n    df = df[cols]\n    return df","b53b0461":"%%time\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    dfs = list()\n    for filename in filenames:\n        try:\n            df = pd.read_csv(os.path.join(dirname, filename), sep=';', encoding='latin1', low_memory=False)\n            dfs.append(cleanDF(df))\n        except:\n            print(\"Error in file: \", filename)\n      \nacidentes_df = pd.concat(dfs, ignore_index=True )","e9c8d7fc":"acidentes_df.info()","7d3a3cfa":"%%time\nacidentes_df = acidentes_df.replace(np.nan, -1)\n\nobj_nulls = list()\nfor col in acidentes_df.select_dtypes(include='object').columns:\n    if acidentes_df[col].str.contains('null').sum() > 0:\n        obj_nulls.append(col)\n        print(col)\n    else:\n        pass","ca73b2bc":"%%time\nfor mun in acidentes_df[acidentes_df[obj_nulls[0]].str.contains('null')]['municipio']:\n    acidentes_df[obj_nulls[0]][acidentes_df['municipio'] == mun] = acidentes_df[acidentes_df['municipio'] == mun][obj_nulls[0]].mode()[0]","4bac5ac2":"acidentes_df[acidentes_df['causa_acidente'] == '(null)']['causa_acidente'] = 'outras'","c378a499":"%%time\nfor col in obj_nulls[1:]:\n    acidentes_df[col][acidentes_df[col].str.contains('null')] = acidentes_df[col].mode()[0]","9336d7d0":"acidentes_df.isnull().sum()","0b224f40":"acidentes_df = acidentes_df.sort_values(by='data_inversa')\nacidentes_df.head(1).T","112f62c8":"import matplotlib.pyplot as plt\n%matplotlib inline\nacidentes_df.hist(figsize=(20,10))","d08ba1ea":"def regiao(x):\n    \n    if x in ['al' , 'ba', 'ce', 'ma', 'pb', 'pe', 'pi', 'rn','se']:\n        return 'nordeste'\n    if x in ['ac' , 'ap', 'am', 'pa', 'ro', 'rr', 'to']:\n        return 'norte'\n    if x in ['df' , 'go', 'ms', 'mt']:\n        return 'centro oeste'\n    if x in ['es' , 'mg', 'sp', 'rj']:\n        return 'sudeste'\n    if x in ['pr' , 'sc', 'rs']:\n        return 'sul'\n    \n\n\nacidentes_df['regiao'] = acidentes_df['uf'].transform(regiao)\n\n# Removendo a causa de acidente \"outras\", pois n\u00e3o informa muita coisa.\nacidentes_df = acidentes_df[acidentes_df['causa_acidente'] != 'outras']","17e85cd2":"#gerando output do DF para gerar a MCA - Analise de correspondencia multipla no R\nacidentes_df.to_csv('acidentes_df.csv')","7fb4dd32":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}","71fd9bb8":"# Caso nao queira rodar toda a etapa de tratamento dos dados\n#acidentes_df = pd.read_csv('.\/kaggle\/acidentes_df_csv')","93b1edc3":"#import pandas_profiling as pp\n#pp.ProfileReport(acidentes_df)","339c596a":"fig, ax = plt.subplots(figsize=(20, 10))\n\nax = sns.countplot(x=\"regiao\", hue=\"causa_acidente\", data=acidentes_df[acidentes_df['regiao'] == 'norte'], palette=\"Set3\")","8851f6bb":"fig, ax = plt.subplots(figsize=(20, 10))\nax = sns.countplot(x=\"regiao\", hue=\"causa_acidente\", data=acidentes_df[acidentes_df['regiao'] == 'nordeste'], palette=\"Set3\")","e2cf3ac4":"fig, ax = plt.subplots(figsize=(20, 10))\nax = sns.countplot(x=\"regiao\", hue=\"causa_acidente\", data=acidentes_df[acidentes_df['regiao'] == 'centro oeste'], palette=\"Set3\")","7ffa29ae":"fig, ax = plt.subplots(figsize=(20, 10))\nax = sns.countplot(x=\"regiao\", hue=\"causa_acidente\", data=acidentes_df[acidentes_df['regiao'] == 'sul'], palette=\"Set3\")","a666b023":"fig, ax = plt.subplots(figsize=(20, 10))\nax = sns.countplot(x=\"regiao\", hue=\"causa_acidente\", data=acidentes_df[acidentes_df['regiao'] == 'sudeste'], palette=\"Set3\")","4e969043":"centro_oeste = acidentes_df[acidentes_df['regiao'] == 'centro oeste']","46c8c4c9":"centro_oeste = centro_oeste[['causa_acidente', 'municipio']]\ncausa_acidentes_x_municipio = pd.crosstab(centro_oeste['causa_acidente'], centro_oeste['municipio'])","6786fd6b":"print(causa_acidentes_x_municipio.shape)\ncausa_acidentes_x_municipio","10812cb3":"def z_score(x):\n    \"\"\"Remove a m\u00e9dia e normaliza os pelo desvio padr\u00e3o\"\"\"\n    return (x - x.mean()) \/ x.std()","840d7fa0":"from sklearn.decomposition import PCA\n\n\npca = PCA(n_components=None)\nscore = pca.fit_transform(causa_acidentes_x_municipio.apply(z_score))","f352638d":"loadings = pd.DataFrame(pca.components_)\nloadings.index =   ['PC %s' % pc for pc in loadings.index + 1]\nloadings.columns = causa_acidentes_x_municipio.columns\nloadings","4458b0c2":"PCs = np.dot(loadings.values.T, causa_acidentes_x_municipio)","0ec1eb65":"font = {'family' : 'monospace',\n        'weight' : 'normal',\n        'size'   : 14}\n\nplt.rc('font', **font)","d10eba3b":"marker = dict(linestyle='none', marker='o', markersize=7, color='blue', alpha=0.5)\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(PCs[0], np.zeros_like(PCs[0]), label=\"Scores\", **marker)\n\n[ax.text(x, y, t) for x, y, t in zip(PCs[0], loadings.values[0, :], loadings.columns)]\n\nax.set_xlabel(\"PC1\")\n\n_ = ax.set_ylim(-1, 1)\nmarker = dict(linestyle='none', marker='o', markersize=7, color='blue', alpha=0.5)\n","63efd72d":"fig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(PCs[0], PCs[1], label=\"Scores\", **marker)\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\n\ntext = [ax.text(x, y, t) for x, y, t in zip(PCs[0], PCs[1], loadings.columns)]\n","f0eb7d19":"perc = pca.explained_variance_ratio_ * 100\n\nperc = pd.DataFrame(perc, columns=['Percentual de raz\u00e3o explicada'], index=['PC %s' % pc for pc in np.arange(len(perc)) + 1])\nax = perc.plot(kind='bar')","4d936a52":"marker = dict(linestyle='none', marker='o', markersize=7, color='blue', alpha=0.5)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(loadings.iloc[:, 0], loadings.iloc[:, 1], label=\"Loadings\", **marker)\nax.set_xlabel(\"non-projected PC1\")\nax.set_ylabel(\"non-projected PC2\")\nax.axis([-1, 1, -1, 1])\ntext = [ax.text(x, y, t) for x, y, t in zip(loadings.iloc[:, 0], loadings.iloc[:, 1], causa_acidentes_x_municipio.index)]","75829e52":"def describe_cluster(variavel, cluster_id):\n    \n    for x in range(0, cluster_id):\n        \n        \n        font = {'family' : 'monospace',\n        'weight' : 'normal',\n        'size'   : 22}\n\n        plt.rc('font', **font)\n    \n        fig, ax = plt.subplots(figsize=(26, 10))\n\n        plt.subplot(1, 2, 1)\n\n        summary = pd.DataFrame(variavel[variavel['cluster_id'] == x].describe()).T\n        summary.columns = ['Quantidade', 'M\u00e9dia', 'Desvio Padr\u00e3o', 'Min\u00edmo','25%','50%','75%', 'M\u00e1ximo']\n        summary = summary.T\n\n        table = plt.table(cellText=summary.values,\n                  rowLabels=summary.index,\n                  colLabels=summary.columns,\n                              cellLoc = 'right', rowLoc = 'center',\n          loc='right', bbox=[.1,.05,1.3,.95])\n\n        plt.axis('off')\n\n        plt.title(\"Descri\u00e7\u00e3o do Cluster \" + str(x) + \" - Quantidade de Acidentes \")\n        table.set_fontsize(22)\n        table.scale(3, 3)  \n        \n        plt.subplot(1, 2, 2)\n        plt.title(\"Amostra dos Munic\u00edpios do Cluster \" + str(x))\n        quantidade = variavel[variavel['cluster_id'] == x]['cluster_id'].count()\n        if  quantidade < 10:\n            sample = variavel[variavel['cluster_id'] == x].sample(quantidade).index\n        else: \n            sample = variavel[variavel['cluster_id'] == x].sample(10).index\n            \n        table = plt.table(cellText=pd.DataFrame(sample).values,\n                          cellLoc = 'right', rowLoc = 'center',\n          loc='top',\n                          bbox=[.25,.55,.45,.45])\n\n        plt.axis('off')\n        # may help","ecdd1876":"# Importar o algoritimo\/modelo\nfrom sklearn.cluster import KMeans","5e1d0534":"causa_acidentes_x_regiao_t = causa_acidentes_x_municipio.T","f29fd250":"W = causa_acidentes_x_regiao_t[['falta de atencao', 'ingestao de alcool']]\n\nfig, ax = plt.subplots(figsize=(16, 5))\n\n\nsummary = pd.DataFrame(W.describe()).T\nsummary.columns = ['Quantidade', 'M\u00e9dia', 'Desvio Padr\u00e3o', 'Min\u00edmo','25%','50%','75%', 'M\u00e1ximo']\nsummary = summary.T\n\ntable = plt.table(cellText=summary.values,\n          rowLabels=summary.index,\n          colLabels=summary.columns,\n          cellLoc = 'right', rowLoc = 'center',\n          loc='right', bbox=[.1,.05,.75,.75])\n\nplt.axis('off')\n\ntable.set_fontsize(22)\ntable.scale(3, 3)  # may help","670f1ffe":"# M\u00e9todo Elbow\n# C\u00e1lculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(W)\n    sse.append(kmeans.inertia_)","e518e89f":"# Plotando o gr\u00e1fico\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 15), sse, 'bx-')\nplt.title('M\u00e9todo Elbow')\nplt.xlabel('N\u00famero de clusters')\nplt.ylabel('SSE')\nplt.xticks(range(1, 15))\nplt.show()","c3fc5c29":"# Vamos usar 3 clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\ncluster_id = kmeans.fit_predict(W)","49d881d8":"# Agora vamos guardar os resultados no dataframe\nW['cluster_id'] = cluster_id","b404ef1c":"describe_cluster(W, 4)","99c534f0":"# Plotando os agrupamentos e os centro\u00eddes\nfig, ax = plt.subplots(figsize=(10, 5))\n\n\n\nsns.scatterplot(x=\"falta de atencao\", y=\"ingestao de alcool\", hue=\"cluster_id\", data=W, s=200, palette=\"viridis\")\nplt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n\n\n\nax.scatter(kmeans.cluster_centers_[:,0] ,\n           kmeans.cluster_centers_[:,1], \n           color='red', \n           marker=\"x\", s=100)\n\n\n#text = [ax.text(x, y, t) for x, y, t in zip(W.values[:,0], \n#                                            W.values[:,1], \n#                                            W.values[:,2])]\n\n\n\nplt.xlabel('Falta de Aten\u00e7\u00e3o')\nplt.ylabel('Ingest\u00e3o de Alcool')\nplt.show()","51cb181d":"X = causa_acidentes_x_regiao_t[['falta de atencao', 'velocidade incompativel']]\nfig, ax = plt.subplots(figsize=(16, 5))\n\n\nsummary = pd.DataFrame(X.describe()).T\nsummary.columns = ['Quantidade', 'M\u00e9dia', 'Desvio Padr\u00e3o', 'Min\u00edmo','25%','50%','75%', 'M\u00e1ximo']\nsummary = summary.T\n\ntable = plt.table(cellText=summary.values,\n          rowLabels=summary.index,\n          colLabels=summary.columns,\n          cellLoc = 'right', rowLoc = 'center',\n          loc='right', bbox=[.1,.05,.75,.75])\n\nplt.axis('off')\n\ntable.set_fontsize(22)\ntable.scale(3, 3)  # may help","d1f15032":"# M\u00e9todo Elbow\n# C\u00e1lculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n    sse.append(kmeans.inertia_)","bf6a80bc":"# Plotando o gr\u00e1fico\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 15), sse, 'bx-')\nplt.title('M\u00e9todo Elbow')\nplt.xlabel('N\u00famero de clusters')\nplt.ylabel('SSE')\nplt.xticks(range(1, 15))\nplt.show()","cba52a44":"# Vamos usar 3 clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\ncluster_id = kmeans.fit_predict(X)","40991fe8":"# Agora vamos guardar os resultados no dataframe\nX['cluster_id'] = cluster_id\n\n","b4bcfc7b":"describe_cluster(X, 4)","2e1978eb":"#Plotando os agrupamentos e os centro\u00eddes\nfig, ax = plt.subplots(figsize=(10, 5))\n\n\n\nsns.scatterplot(x=\"falta de atencao\", y=\"velocidade incompativel\", hue=\"cluster_id\", data=X, s=300, palette=\"viridis\")\nplt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n\n\nax.scatter(kmeans.cluster_centers_[:,0] ,\n           kmeans.cluster_centers_[:,1], \n           color='red', \n           marker=\"x\", s=100)\n\n\n\nplt.xlabel('Falta de Aten\u00e7\u00e3o')\nplt.ylabel('Velocidade Incompat\u00edvel')\n\nplt.show()","fa7f4090":"Y = causa_acidentes_x_regiao_t[['nao guardar distancia de seguranca', 'velocidade incompativel']]\n\nfig, ax = plt.subplots(figsize=(16, 5))\n\n\nsummary = pd.DataFrame(Y.describe()).T\nsummary.columns = ['Quantidade', 'M\u00e9dia', 'Desvio Padr\u00e3o', 'Min\u00edmo','25%','50%','75%', 'M\u00e1ximo']\nsummary = summary.T\n\ntable = plt.table(cellText=summary.values,\n          rowLabels=summary.index,\n          colLabels=summary.columns,\n          cellLoc = 'right', rowLoc = 'center',\n          loc='right', bbox=[.1,.05,.75,.75])\n\nplt.axis('off')\n\ntable.set_fontsize(22)\ntable.scale(3, 3)  # may help","7ec11791":"# M\u00e9todo Elbow\n# C\u00e1lculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(Y)\n    sse.append(kmeans.inertia_)","4b68061b":"# Plotando o gr\u00e1fico\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 15), sse, 'bx-')\nplt.title('M\u00e9todo Elbow')\nplt.xlabel('N\u00famero de clusters')\nplt.ylabel('SSE')\nplt.xticks(range(1, 15))\nplt.show()","464ded0b":"# Vamos usar 3 clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\ncluster_id = kmeans.fit_predict(Y)","b95ee45b":"# Agora vamos guardar os resultados no dataframe\nY['cluster_id'] = cluster_id\n","93a5b3dc":"describe_cluster(Y, 4)","670d16d9":"#Plotando os agrupamentos e os centro\u00eddes\nfig, ax = plt.subplots(figsize=(10, 5))\n\n\n\nsns.scatterplot(x=\"nao guardar distancia de seguranca\", y=\"velocidade incompativel\", hue=\"cluster_id\", data=Y, s=300, palette=\"viridis\")\nplt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n\n\nax.scatter(kmeans.cluster_centers_[:,0] ,\n           kmeans.cluster_centers_[:,1], \n           color='red', \n           marker=\"x\", s=100)\n\n\n\nplt.xlabel('N\u00e3o guardar dist\u00e2ncia de seguran\u00e7a')\nplt.ylabel('Velocidade incompat\u00edvel')\nplt.show()","c1660417":"pcs = pd.DataFrame(PCs)\n\npcs.columns = loadings.columns\n\nZ = pcs.T[[0, 1]]\n\nfig, ax = plt.subplots(figsize=(16, 5))\n\n\nsummary = pd.DataFrame(Z.describe()).T\nsummary.columns = ['Quantidade', 'M\u00e9dia', 'Desvio Padr\u00e3o', 'Min\u00edmo','25%','50%','75%', 'M\u00e1ximo']\nsummary = summary.T\n\ntable = plt.table(cellText=summary.values,\n          rowLabels=summary.index,\n          colLabels=summary.columns,\n          cellLoc = 'right', rowLoc = 'center',\n          loc='right', bbox=[.1,.05,.75,.75])\n\nplt.axis('off')\n\ntable.set_fontsize(22)\ntable.scale(3, 3)  # may help","dd66137f":"# M\u00e9todo Elbow\n# C\u00e1lculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(Z)\n    sse.append(kmeans.inertia_)","4bc9315b":"# Plotando o gr\u00e1fico\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 15), sse, 'bx-')\nplt.title('M\u00e9todo Elbow')\nplt.xlabel('N\u00famero de clusters')\nplt.ylabel('SSE')\nplt.xticks(range(1, 15))\nplt.show()","93321623":"# Vamos usar 3 clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\ncluster_id = kmeans.fit_predict(Z)","46c06d35":"# Agora vamos guardar os resultados no dataframe\nZ['cluster_id'] = cluster_id","63aa0d0b":"describe_cluster(Z, 4)","1251a701":"#Plotando os agrupamentos e os centro\u00eddes\nfig, ax = plt.subplots(figsize=(10, 5))\n\n\n\nsns.scatterplot(x=0, y=1, hue=\"cluster_id\", data=Z, s=300, palette=\"viridis\")\nplt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n\n\nax.scatter(kmeans.cluster_centers_[:,0] ,\n           kmeans.cluster_centers_[:,1], \n           color='red', \n           marker=\"x\", s=100)\n\n\n\n\nplt.xlabel('PC 1')\nplt.ylabel('PC 2')\nplt.show()\n","c88500c4":"# Bilbiotecas\n!pip install pmdarima\nimport statsmodels.api as sm\nfrom pmdarima.arima import auto_arima\nimport datetime as dt","4e344eea":"# Acidentes Time Series Centro Oeste\nacidentes_ts = acidentes_df[(acidentes_df['regiao'] == 'centro oeste')]\nacidentes_ts['com_vitimas'] = np.where((acidentes_ts['mortos'] > 0) |\n                                       (acidentes_ts['feridos'] > 0) |\n                                       (acidentes_ts['feridos_leves'] > 0) |\n                                       (acidentes_ts['feridos_graves'] > 0),\n                                       1,0)\n\nacidentes_ts['mes'] = acidentes_ts['data_inversa'].dt.month.apply(lambda x: \"{:02d}\".format(x))\nacidentes_ts['ano'] = acidentes_ts['data_inversa'].dt.year.astype(str)\n\n# Acidentes com Vitimas\nacidentes_cvit = acidentes_ts.groupby(['mes','ano'])['com_vitimas'].sum().reset_index()\nacidentes_cvit['dia'] = '01'\nacidentes_cvit['data'] = pd.to_datetime(acidentes_cvit['ano']+acidentes_cvit['mes']+acidentes_cvit['dia'])\nacidentes_cvit = acidentes_cvit[['data','com_vitimas']].set_index('data').sort_values('data')\n\n# Acidentes Totais\nacidentes_tot = acidentes_ts.groupby(['mes','ano'])['com_vitimas'].count().reset_index()\nacidentes_tot['dia'] = '01'\nacidentes_tot['data'] = pd.to_datetime(acidentes_tot['ano']+acidentes_tot['mes']+acidentes_tot['dia'])\nacidentes_tot = acidentes_tot[['data','com_vitimas']].set_index('data').rename(columns={'com_vitimas':'total'}).sort_values('data')","cf610145":"ax = acidentes_tot.plot(title='S\u00e9rie Hist\u00f3rica', figsize=(20,10))\nacidentes_cvit.plot(ax=ax)\n\nplt.title('Acidentes nas rodovias do Centro-Oeste | 2009-2019', fontsize=15, rotation=0)\nplt.xticks(fontsize=15, rotation=0)\nplt.yticks(fontsize=15)\nplt.xlabel('')\n\nax.legend(['Total de Acidentes', 'Acidentes com V\u00edtimas'], fontsize=14)\n\nplt.show()","49dc2ab9":"contagem = acidentes_ts.groupby('ano')['com_vitimas'].sum().reset_index().set_index('ano')\n\nfig, ax = plt.subplots(1,1, figsize=(20,10))\n\nplt.scatter(x=contagem.index, y=contagem['com_vitimas'], color='purple', linewidths=5, zorder=2)\nplt.plot(contagem['com_vitimas'], linewidth=3, zorder=1)\nplt.xticks(fontsize=18)\nplt.tick_params(axis='y', which='both', left=False, labelleft=False)\nplt.title('Total de Acidentes com V\u00edtimas por Ano', size=20)\n\nfor data, count in zip(contagem.index, contagem.values): \n    ax.annotate(count[0]\n               ,xytext=(data, count+50)\n               ,fontsize=15 \n               ,xy=(data, count)\n               )\nplt.show()","06e5e1f2":"# Mortos\nacidentes_mort = acidentes_ts.groupby(['mes','ano'])['mortos'].sum().reset_index()\nacidentes_mort['dia'] = '01'\nacidentes_mort['data'] = pd.to_datetime(acidentes_mort['ano']+acidentes_mort['mes']+acidentes_mort['dia'])\nacidentes_mort = acidentes_mort[['data','mortos']].set_index('data').sort_values('data')\n\nax = acidentes_cvit.plot(title='S\u00e9rie Hist\u00f3rica', figsize=(20,10))\nacidentes_mort.plot(ax=ax)\n\nplt.title('Mortalidade nos Acidentes em rodovias do Centro-Oeste | 2009-2019', fontsize=15, rotation=0)\nplt.xticks(fontsize=15, rotation=0)\nplt.yticks(fontsize=15)\nplt.xlabel('')\n\nax.legend(['Acidentes com V\u00edtimas', 'Acidentes com \u00d3bitos'], fontsize=14)\n\nplt.show()","61317f64":"res = sm.tsa.seasonal_decompose(acidentes_cvit,freq=12)\ndef plot_decompose(res):\n    fig, axes = plt.subplots(ncols=1, nrows=4, sharex=True, figsize=(16,12))\n    \n    res.observed.plot(ax=axes[0], legend=False)\n    axes[0].set_ylabel('Observado')\n    \n    res.trend.plot(ax=axes[1], legend=False)\n    axes[1].set_ylabel('Tend\u00eancia')\n    \n    res.seasonal.plot(ax=axes[2], legend=False)\n    axes[2].set_ylabel('Sazonalidade')\n    \n    axes[2].annotate('Fevereiro',fontsize=10 \n            ,xytext=(dt.datetime(2010,5,1), -33)\n            ,xy=(dt.datetime(2010,2,1), -31) \n            ,arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\", color='black'))\n\n    axes[2].annotate('Dezembro', fontsize=10\n            ,xytext=(dt.datetime(2011,3,1), 40)\n            ,xy=(dt.datetime(2010,12,1), 41)\n           ,arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\", color='black'))\n    \n    res.resid.plot(ax=axes[3], legend=False)\n    axes[3].set_ylabel('Res\u00edduo')\n    \n    plt.xlabel('')\n    \nplot_decompose(res)","6890b8c0":"%%time\nstep_wise=auto_arima(acidentes_cvit,\n                     start_p=0, start_q=0, \n                     max_p=3, max_q=3,\n                     start_P=0, start_Q=0, \n                     max_P=3, max_Q=3,\n                     d=1, max_d=1,\n                     D=1, max_D=1,\n                     m=12,\n                     trace=True, \n                     error_action='ignore', \n                     suppress_warnings=True, \n                     stepwise=True)","df7c8eea":"step_wise.summary()","d98492ee":"model = sm.tsa.statespace.SARIMAX(acidentes_cvit\n                                 ,order=(0, 1, 1)\n                                 ,seasonal_order=(0, 1, 1, 12)\n                                 ,enforce_stationarity=False\n                                 ,enforce_invertibility=False\n                                 )\nresults = model.fit()\nprint(results.summary()) ","a4650a6b":"results.plot_diagnostics(figsize=(16, 8))\nplt.show()","5088155e":"pred = results.get_prediction(start=pd.to_datetime('2015-01-01'), dynamic=False)\npred_ci = pred.conf_int()\n\nfig, ax = plt.subplots(figsize=(20, 8))\n\nacidentes_cvit.plot(ax=ax)\npred.predicted_mean.plot(ax=ax, alpha=0.8, color='r')\n\nax.legend(['Hist\u00f3rico', 'Previs\u00e3o'], fontsize=15)\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=0.07)\nax.set_xlabel('')\nax.set_ylabel('Pre\u00e7o')\nplt.xticks(fontsize=15)\nplt.show()","e1ba0b40":"y_forecasted = pred.predicted_mean\ny_truth = acidentes_cvit.loc['2015-01-01':, 'com_vitimas']\n\n# Compute the mean square error\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('O Erro Quadr\u00e1tico M\u00e9dio \u00e9 {}'.format(round(mse, 2)))","dd4a4179":"pred_uc = results.get_forecast(steps=12)\npred_ci = pred_uc.conf_int()\n\nfig, ax = plt.subplots(figsize=(20, 8))\n\nax = acidentes_cvit.plot(ax=ax)\npred_uc.predicted_mean.plot(ax=ax, color='g')\n\nax.legend(['Hist\u00f3rico', 'Previs\u00e3o'], fontsize=15)\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.06)\nax.set_xlabel('')\nax.set_ylabel('Pre\u00e7o')\nplt.xticks(fontsize=15)\nplt.show()","4a84e685":"# Intervalo de Confian\u00e7a e valor m\u00e9dio\npred_ci['Previs\u00e3o M\u00e9dia'] = (pred_ci.iloc[:, 0] +  pred_ci.iloc[:, 1]) \/ 2\npred_ci","f193e17e":"round(pred_ci['Previs\u00e3o M\u00e9dia'].sum())","a6e05f1f":"fig, ax = plt.subplots(1,1, figsize=(20,10))\n\npreds = pd.DataFrame(pd.concat([acidentes_cvit, pred_ci['Previs\u00e3o M\u00e9dia']]).sum(axis=1)).rename(columns={0:'com_vitimas'})\nplt.plot(preds, linewidth=2, zorder=1)\nplt.plot(preds.iloc[-12:,0], linewidth=3, color='y')\nax.legend(['Hist\u00f3rico', 'Previs\u00e3o'], fontsize=15)\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\n\nplt.axvline(x=\"2020-01-01\", ymin=0, ymax=700, ls='--')\nplt.show()","2b4a0694":" ### Plotando a primeira e segunda dimens\u00e3o(PC1 e PC2)","fc8c8794":"### Importa\u00e7\u00e3o, tratamento dos dados e concatena\u00e7\u00e3o","4aae7980":" ### Plotando a primeira dimens\u00e3o(PC1)","2ce4d3a2":"# Realizando e Visualizando Previs\u00f5es <a name=\"7\"><\/a>\nChegamos ao ponto principal da nossa an\u00e1lise, realizar as previ\u00f5es. Para isso iremos utilizar o atributo **\"get_forecast\"** que consegue computar os valores previstos com **N** passos a frente.","38b142fd":"#### K-Means - Falta de Aten\u00e7\u00e3o x Velocidade Incompat\u00edvel","8b705871":"# K-Means <a name=\"4\"><\/a>","80ea9900":"# Sele\u00e7\u00e3o dos melhores par\u00e2metros com o \"auto_arima\" <a name=\"4\"><\/a>\nDando continuidade \u00e0s previs\u00f5es, iremos selecionar o modelo automaticamente atrav\u00e9s da fun\u00e7\u00e3o \"auto_arima\", que ir\u00e1 nos entregar os melhores componentes para o modelo ARIMA e suas estat\u00edsticas.\n\nEm geral, as duas estat\u00edsticas levadas em considera\u00e7\u00e3o para a sele\u00e7\u00e3o do modelo, de tal forma que quanto menor o valor, melhor, s\u00e3o:\n\n- AIC: Akaike Information Criteria\n- BIC: Bayesian Information Criteria","885590f2":"### Feature Engineering\nCria\u00e7\u00e3o de uma coluna chamada \"regiao\" para possibilitar an\u00e1lises de dados agrupados.","cc561920":"## Decomposi\u00e7\u00e3o da S\u00e9rie Temporal\n\n\u00c9 poss\u00edvel, e muito importante, analisarmos esses tr\u00eas componentes separadamente de modo a avaliar melhor o comportamento individual de cada item, para isso iremos realizar sua decomposi\u00e7\u00e3o. Como nossos dados tratam-se de observa\u00e7\u00f5es mensais, a frequ\u00eancia passada como par\u00e2metro ser\u00e1 12, o que significa que para cada ponto iremos analisar a m\u00e9dia dos 6 meses anteriores e 6 posteriores. Por meio da decomposi\u00e7\u00e3o, \u00e9 poss\u00edvel identificar que nossos dados possuem uma clara tend\u00eancia de crescimento, e quanto \u00e0 sazonalidade, o gr\u00e1fico demonstra que existe efeito sazonal com picos m\u00e1ximos e m\u00ednimos nos meses de Dezembro e Fevereiro, respectivamente.","8dd90c1c":"# An\u00e1lise Estat\u00edstica","31be8d2d":"# An\u00e1lise Descritiva Explorat\u00f3ria <a name=\"2\"><\/a>","860cfa43":"### Tratamento dos missings","b7fa02c3":"Aqui nossa principal preocupa\u00e7\u00e3o deve ser de garantir que os res\u00edduos do modelo n\u00e3o s\u00e3o correlacionados e igualmente distribu\u00eddos. Se isto n\u00e3o for alcan\u00e7ado, \u00e9 indicativo de que os par\u00e2metros podem ser melhorados.\n\nNo gr\u00e1fico do canto superior direito, quando a linha vermelha do KDE (Kernel Density Estimation) est\u00e1 pr\u00f3xima da linha verde N(0,1), que significa \"Distribui\u00e7\u00e3o Normal com m\u00e9dia 0 e desvio padr\u00e3o 1\", \u00e9 um bom indicativo de que os res\u00edduos s\u00e3o igualmente distribu\u00eddos.\n\nNo canto inferior esquerdo, o gr\u00e1fico mostra que a distribui\u00e7\u00e3o ordenada dos res\u00edduos seguem a linha de tend\u00eancia das amostras tiradas de uma distribui\u00e7\u00e3o normal com N(0, 1), com alguns desvios.\n\nNo gr\u00e1fico de res\u00edduos ao longo do tempo, no canto superior esquerdo, n\u00e3o h\u00e1 nenhuma sazonalidade aparente e parece se tratar de \"ru\u00eddo branco\". O conceito de ru\u00eddo branco significa que os dados s\u00e3o aleat\u00f3rios e n\u00e3o podem ser preditos, pois n\u00e3o seguem um padr\u00e3o. Isto \u00e9 confirmado pelo \"correlograma\", no canto inferior direito, que mostra que os res\u00edduos possuem baixa correrela\u00e7\u00e3o com os pr\u00f3prios lags.","5f364579":"# Pr\u00e9processamento de Dados <a name=\"1\"><\/a>\nComo etapa inicial, ser\u00e1 necess\u00e1rio realizar o pr\u00e9processamento dos dados a fim de remover eventuais registros duplicados, remover ou substituir registros nulos e aplicar a correta formata\u00e7\u00e3o dos dados (data, n\u00famerico, categ\u00f3rico, etc).\n\n> Foram encontrados registros nulos com o campo escrito \"(null)\", foi necess\u00e1rio desenvolver fun\u00e7\u00e3o customizada para, tamb\u00e9m, atender a essas peculiaridades.\n\nA fun\u00e7\u00e3o em seguida tem por objetivo:\n- remover registros duplicados;\n- substituir valores nulos num\u00e9ricos por -1 (e tamb\u00e9m valores de string como \"(null)\" em colunas num\u00e9ricas que foram encontrados);\n- valores nulos qualitativos pelo registro mais frequente, dada que a quantidade encontrada foi m\u00ednima (m\u00e1ximo de 30 registros \"(null)\" por coluna)\n- manter e ordenar as colunas que aparecem em todos os datasets - os anos de 2017 a 2019 possuem as colunas \"latitude\", \"longitude\", \"regional\", \"delegacia\" e \"uop\", que n\u00e3o constam nos datasets anteriores.","625a2475":"# Mortalidade nos Acidentes\nA mortalidade dos acidentes nas rodovias do Centro-Oeste mostrou-se baixa em compara\u00e7\u00e3o com o total de acidentes com v\u00edtimas, e demonstra um comportamento, aparentemente, est\u00e1vel.","4b510289":"Agora vamos plotar a primeira e segunda PCs juntas. Esse tipo de gr\u00e1fico \u00e9 chamado de Score plot.","44211cbb":"Facilmente vemos que as cidades Bras\u00edlia, C\u00e1ceres, Morrinhos e An\u00e1polis t\u00eam uma quantidade de causas de acidentes diferentes do resto das cidades do Centro Oeste","ecb7aef6":"### Explora\u00e7\u00e3o quanto a ocorr\u00eancias de \"Missings\"","5639ba5d":"### Dataframe sem missings","856c3d1c":"Utilizando as duas dimens\u00f5es podemos ver mais claramente as cidades que se destoam da aglomera\u00e7\u00e3o Note que na \"segunda\" dimens\u00e3o est\u00e3o as diferen\u00e7as das cidades que estavam agrupadas no gr\u00e1fico anterior","382490de":"#### K-Means - Falta de Aten\u00e7\u00e3o x Ingest\u00e3o de Alcool","d1172f21":"An\u00e1lises das quantidades de causas de acidentes por munic\u00edpio da regi\u00e3o Centro Oeste. Sem realizar ajustes lineares para cada par de dados fica quase imposs\u00edvel visualizar algum padr\u00e3o ou tend\u00eancia nos dados acima. Esse \u00e9 um caso onde a PCA pode ajudar. Utilizando uma fun\u00e7\u00e3o para normalizar os dados. Remove-se a m\u00e9dia e normalizam-se os pelo desvio padr\u00e3o: (x - \u03bc \/ \u03c3)","d3845793":"# An\u00e1lise de Componentes Principais <a name=\"3\"><\/a>","7d54a163":"Onde a UF era missing, foi utilizado o valor em que o municipio era o mesmo e a UF n\u00e3o era nulo para a corre\u00e7\u00e3o:","c90e1719":"Iremos realizar a previs\u00e3o da quantidade de acidentes com v\u00edtimas para o ano de 2020 por meio de um modelo auto-regressivo integrado de m\u00e9dias m\u00f3veis sazonal, em ingl\u00eas chamado de SARIMA (Seasonal Autoregressive Integrated Moving Average). S\u00e9ries Temporais, em geral, podem ser classificadas como aditivas ou multiplicativas mas ambas possuem os mesmos 3 componentes:\n\nTend\u00eancia, que representa a dire\u00e7\u00e3o geral como os dados se desenvolvem ao longo do tempo;\nSazonalidade, padr\u00f5es de como os dados mudam em rela\u00e7\u00e3o a per\u00edodo determinado;\nErro, s\u00e3o varia\u00e7\u00f5es irregulares n\u00e3o explicadas pela tend\u00eancia ou sazonalidade.\nNo caso de uma S\u00e9ria Temporal Aditiva, um modelo pode ser explicado pela f\u00f3rmula:\n\nY[t] = T[t] + S[t] + e[t]\n\n> Onde:\n- Y[t] : Sa\u00edda do Modelo\n- T[t] : Componente de Tend\u00eancia do modelo\n- S[t] : Componente de Sazonalidade do modelo\n- e[t] : erro ou res\u00edduo\n\nAntes, fa\u00e7amos uma breve an\u00e1lise explorat\u00f3ria dos dados de acidentes totais e acidentes com v\u00edtimas.","22b3015b":"# Validando as previs\u00f5es <a name=\"6\"><\/a>\nJ\u00e1 ajustamos o modelo e agora podemos utiliz\u00e1-lo para realizar previs\u00f5es. Iremos come\u00e7ar comparando os valores preditos com os valores reais para nos auxiliar a entender sua precis\u00e3o.\n\nIremos utilizar **\"get_predition\"** a partir de uma data **\"X\"** e com um intervalo de confian\u00e7a **\"conf_int\"**.","d3a7ed14":"#### K-Means - PC 1 x PC 2","351da5fa":"A An\u00e1lise de Componentes Principais (em ingl\u00eas PCA) \u00e9 o nome comum dado \u00e0 t\u00e9cnica que usa princ\u00edpios de \u00e1lgebra linear para transformar vari\u00e1veis, possivelmente correlacionadas, em um n\u00famero menor de vari\u00e1veis chamadas de Componentes Principais.\n\nIremos analisar as causas mais comuns de acidente por regi\u00e3o no Brasil.","59cadd90":"# Previs\u00e3o 2020\nAp\u00f3s realizados todos os passos e encontrados os valores inferiores e superiores do intervalo de confian\u00e7a, podemos dizer que previs\u00e3o do n\u00famero total de acidentes com v\u00edtimas para o ano de 2020 ser\u00e1 de, aproximadamente:\n\n>**6.940** acidentes com v\u00edtimas.\n\nSeguindo a tend\u00eancia de crescimento que fora identificada nos gr\u00e1ficos da an\u00e1lise explorat\u00f3ria.","94a7b85c":"----\n# AN\u00c1LISE ESTAT\u00cdSTICA DOS ACIDENTES NAS RODOVIAS FEDERAIS BRASILEIRAS \n\n* Leandro Alencar \u2013 1931133007\n* Maycon Alves \u2013 1931133015\n* Nilson Michiles - 1931133032\n\n## Table of contents\n* [Pr\u00e9processamento de Dados](#1)\n* [An\u00e1lises Descritivas e Estat\u00edsticas](#2)\n* [An\u00e1lises de Componentes Principais](#3)\n* [K-Means (K-m\u00e9dias)](#4)\n* [S\u00e9rie Temporal](#5)\n----","d117458c":"# Visualiza\u00e7\u00e3o Completa\nAbaixo iremos visualizar a ilustra\u00e7\u00e3o gr\u00e1fica de todo hist\u00f3rico conhecido mais a previz\u00e3o realizada pelo modelo SARIMAX, em verde.","f1c5e912":"# S\u00e9rie Temporal <a name=\"5\"><\/a>","052003b7":"Em geral se busca componentes o suficiente para explicar entre 70-80% dos dados. nota-se que utilizamos mais que 70% da vari\u00e2ncia dos dados. Sa\u00edmos de uma dimens\u00e3o de (28 x 228) para uma dimens\u00e3o de (2 x 228).\n","71540c27":"#### K-Means - N\u00e3o guardar dist\u00e2ncia de seguran\u00e7a x Velocidade incompat\u00edvel","a5c2c4c9":"## Causa de acidentes x Regi\u00e3o","1e9bf73f":"# Ajustando o modelo SARIMA <a name=\"5\"><\/a>\nAp\u00f3s as diversas itera\u00e7\u00f5es que duraram aproximadamente 6 minutos, o modelo indicado foi o \"SARIMAX(0, 1, 1)x(0, 1, 1, 12)\". Observe que o \"X\" em SARIMA\"X\" exite pois \u00e9 poss\u00edvel utilizar uma s\u00e9ria \"ex\u00f3gena\", ou externa, mas n\u00e3o ser\u00e1 o nosso caso. Iremos aplicar o modelo SARIMA (Seasonal Autoregressive Integrated Moving Average).","60339314":"### Criando tabela cruzada entre regi\u00e3o e causas de acidentes.","2ffb8589":"Pode-se perceber que houve redu\u00e7\u00e3o no n\u00famero total de acidentes com a evolu\u00e7\u00e3o do tempo, por\u00e9m o n\u00famero de acidentes com v\u00edtimas n\u00e3o acompanhou essa redu\u00e7\u00e3o, aumentando do \u00edndice de acidentes com v\u00edtimas. \n\nNo gr\u00e1fico abaixo, fica mais evidente o aumento progressivo no n\u00famero total de acidentes com v\u00edtimas. Houve redu\u00e7\u00e3o no per\u00edodo de 2013 a 2016, por\u00e9m um grande aumento em 2017 e tend\u00eancia de crescimento  futuro, ap\u00f3s sua leve queda em 2018.","d1266f80":"Nos demais, foram substituidos pelo valor mais frequente.","ea524129":"Outro gr\u00e1fico comum para explorar os resultados \u00e9 o Loadings plot ou seja, a influ\u00eancia de cada vari\u00e1vel original nas componentes principais. Note que velocidade incompat\u00edvel se destacam da aglomera\u00e7\u00e3o central, esse tipo de acidente tem maior influ\u00eancia nos componentes principais.\n"}}