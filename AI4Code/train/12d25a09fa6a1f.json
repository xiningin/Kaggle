{"cell_type":{"21607f62":"code","51bf72fd":"code","6298cade":"code","ad4de367":"code","44091579":"code","494e8abe":"code","11d9ee5f":"code","4f0e44de":"code","1991a1e1":"code","040e289f":"code","0bad8cb5":"code","bdc9c568":"code","3b370c5c":"code","431d8a75":"code","33cf4034":"code","0a8039af":"code","ae172b65":"code","e182a751":"code","20f07939":"code","a8c3097a":"code","56d5ec58":"code","39edbefb":"code","dd61bd12":"code","16abada9":"code","4fd356bd":"code","e7f62b15":"code","9822986a":"code","0bbc7c2a":"code","c5c2a3d1":"code","1bed6b06":"code","7989c7a4":"code","a9c39476":"code","771eac92":"code","2ecc8e40":"code","b5d77f97":"code","1ef6c72a":"markdown","421697cc":"markdown","6a6b9639":"markdown","5f96f6eb":"markdown","a70491f9":"markdown","f2e1df06":"markdown","ac82d278":"markdown","b32f280a":"markdown","eed10e32":"markdown","0dcff504":"markdown","56e9a152":"markdown","4ed9a65d":"markdown","b3004405":"markdown","91e157e2":"markdown","d3d8eab2":"markdown","45b62ed5":"markdown"},"source":{"21607f62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51bf72fd":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","6298cade":"#Reading the file into a dataframe and making a copy\n\ndf_main = pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndf = df_main.copy()\ndf.head()","ad4de367":"df.shape","44091579":"#Checking for missing data\n\nmissing_data = df.isnull()\n\nmissing_data.sum()","494e8abe":"#Let's see what kind of data we have\n\ndf.info()","11d9ee5f":"#Let's check out how the data is distributed\n\ndf.describe()","4f0e44de":"cat_cols = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall', 'output']\ncont_cols = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']","1991a1e1":"fig, ax = plt.subplots(ncols = 3, nrows = 5, figsize = (30, 40))\nindex = 0\nax = ax.flatten()\n\nfor col in list(df.columns):\n    if col in cat_cols:\n        sns.countplot(data=df, x=col, ax=ax[index])\n        index += 1\n    else:\n        sns.distplot(df[col], ax=ax[index])\n        index += 1\nfig.delaxes(ax[14])\nplt.draw()","040e289f":"# Correlation heatmap\n\nhmap_mask = np.triu(df.corr(), k=1)\nplt.figure(figsize=(11,7))\nsns.heatmap(df.corr(), mask=hmap_mask, annot = True, fmt=\".2f\", cmap='coolwarm')","0bad8cb5":"from sklearn.model_selection import  train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report","bdc9c568":"#Extracting features\n\nX = df.drop('output', axis = 1)\nX.head()","3b370c5c":"#Extracting labels\n\ny = df.iloc[:,-1:]\ny.head()","431d8a75":"st_scaler = StandardScaler()\nX_norm=X.copy()\nX_norm[cont_cols]=pd.DataFrame(st_scaler.fit_transform(X_norm[cont_cols]), columns=[cont_cols])\nX_norm.head()","33cf4034":"X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size = 0.2, random_state = 65)","0a8039af":"from sklearn.linear_model import LogisticRegression","ae172b65":"#Creating model object\nmodel_lgr = LogisticRegression()\n\n#Training the model\nmodel_lgr.fit(X_train, y_train)\n\n#Prediction\ny_lgr = model_lgr.predict(X_test)\n\n#Accuracy\nacc_lgr = round(accuracy_score(y_test, y_lgr),4)*100\n\nprint(\"Accuracy with logistic regression is: \" + str(acc_lgr) + \" %\")","e182a751":"#Plotting confusion matrix\ncm_lgr = confusion_matrix(y_test, y_lgr)\ndisp_lgr = ConfusionMatrixDisplay(confusion_matrix=cm_lgr, display_labels=['Low', 'High'])\n\ndisp_lgr.plot()\nplt.title('Logistic Regression Classifier \\nConfusion Matrix')\nplt.xlabel('\\nPredicted risk')#, fontsize=12)\nplt.ylabel('True risk\\n')#, fontsize=12)\nplt.show()","20f07939":"from xgboost import XGBClassifier","a8c3097a":"#Creating model object\nmodel_xgb = XGBClassifier(learning_rate=0.1, max_depth=2, n_estimators=25)\n\n#Training the model\nmodel_xgb.fit(X_train, y_train)\n\n#Prediction\ny_xgb = model_xgb.predict(X_test)\n\n#Accuracy\nacc_xgb = round(accuracy_score(y_test, y_xgb),4)*100\n\nprint(\"\\n\\nAccuracy with XGBoost is: \" + str(acc_xgb) + \" %\")","56d5ec58":"cm_xgb = confusion_matrix(y_test, y_xgb)\ndisp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=['Low', 'High'])\n\ndisp_xgb.plot()\nplt.title('XGBoost Classifier \\nConfusion Matrix')\nplt.xlabel('\\nPredicted risk')\nplt.ylabel('True risk\\n')\nplt.show()","39edbefb":"from sklearn.svm import SVC","dd61bd12":"#Creating model object\nmodel_svm=SVC(kernel=\"rbf\", C=0.75, gamma= 0.5)\n\n#Training the model\nmodel_svm.fit(X_train,y_train)\n\n#Prediction\ny_svm=model_svm.predict(X_test)\n\n#Accuracy\nacc_svm = round(accuracy_score(y_test, y_svm),5)*100\n\n\nprint(\"Accuracy with SVM is: \" + str(acc_svm) + \" %\")","16abada9":"cm_svc = confusion_matrix(y_test, y_svm)\ndisp_svc = ConfusionMatrixDisplay(confusion_matrix=cm_svc, display_labels=['Low', 'High'])\n\ndisp_svc.plot()\nplt.title('Support Vector Classifier \\nConfusion Matrix')\nplt.xlabel('\\nPredicted risk')\nplt.ylabel('True risk\\n')\nplt.show()","4fd356bd":"import tensorflow as tf","e7f62b15":"#Setting random seed for reproducability\ntf.random.set_seed(25)\n\n#Building sequential model\nmodel = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(256, activation=\"relu\", input_shape=[13]),\n            tf.keras.layers.Dense(128, activation=\"relu\"),\n            tf.keras.layers.Dense(128, activation=\"relu\"),\n            tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n        ]\n        )\nmodel.summary()\n\n#Compiling the model\nmodel.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['binary_accuracy'])","9822986a":"#Train the model\nhistory = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0, shuffle=False)","0bbc7c2a":"plt.plot(history.history['val_binary_accuracy'], label='Validation accuracy')\nplt.plot(history.history['binary_accuracy'], label='Training accuracy')\nplt.title('Neural network classifier')\nplt.xlabel('\\nNumber of epochs')\nplt.ylabel('Accuracy\\n')\nplt.legend()\nplt.show()\nacc_dnn = round(history.history['val_binary_accuracy'][-1], 4)*100\nprint(\"\\nAccuracy with DNN is: \" + str(acc_dnn) + \" %\")","c5c2a3d1":"y_dnn = np.where(model.predict(X_test)<0.5, 0, 1)","1bed6b06":"cm_dnn = confusion_matrix(y_test, y_dnn)\ndisp_dnn = ConfusionMatrixDisplay(confusion_matrix=cm_dnn, display_labels=['Low', 'High'])\n\ndisp_dnn.plot()\nplt.title('DNN Classifier \\nConfusion Matrix')\nplt.xlabel('\\nPredicted risk')\nplt.ylabel('True risk\\n')\nplt.show()","7989c7a4":"Acc_df = pd.DataFrame(index=['Logistic Regression', 'XGBoost', 'SVM', 'DNN'], columns=['Accuracy [%]'])","a9c39476":"Acc_df['Accuracy [%]']=[acc_lgr, acc_xgb, acc_svm, acc_dnn]\nAcc_df","771eac92":"print(classification_report(y_test, y_xgb, digits=3))","2ecc8e40":"from xgboost import plot_importance","b5d77f97":"plot_importance(model_xgb)\nplt.xlim(0,15)\nplt.show()","1ef6c72a":"# Exploratory Data Analysis","421697cc":"Let's see how the features and labels are correlated with each other.","6a6b9639":"Looks like oldpeak (ST depression induced by exercise relative to rest) & caa (number of major vessels (0-3) colored by flourosopy) are the two most important features in predicting the likelihood of heart attack.","5f96f6eb":"Let's plot the continuous and categorical variables.","a70491f9":"# Logistic Regression","f2e1df06":"# Support Vector Machines","ac82d278":"# Getting started","b32f280a":"# Summary of results","eed10e32":"# Deep Neural Network","0dcff504":"It is evident that there are categorical as well as continuous data columns.","56e9a152":"As we can see, none of the variables are highly correlated. Hence, we shall not drop any of them. Let's start modelling.","4ed9a65d":"# XGBoost Classifier","b3004405":"# Modelling","91e157e2":"Let's split the data into features & labels, and normalise them.","d3d8eab2":"Let us see the relative importance of different features.","45b62ed5":"# Conclusion\n\n#### Clearly the best results are achieved with XGBoost classifier."}}