{"cell_type":{"79f20dd0":"code","7d426485":"code","f00e9c0b":"code","e2ae99de":"code","cb686147":"code","1b1413fe":"code","b4eae2f0":"code","55bff77a":"code","3f1f066d":"code","8838f3b9":"code","41b82903":"code","ba51442a":"code","5b87d96f":"code","0f7549d3":"code","aaaab682":"code","c73b1091":"code","7e1884f9":"code","f02b04ec":"code","43f9b3da":"code","6a07e577":"code","9e20c0b8":"code","10835dd5":"code","b71fe987":"code","8f016648":"code","5d716ba9":"code","f7e7a851":"code","8157ee66":"code","86d5e50d":"code","917874ce":"code","c22f6026":"code","d9671710":"code","2555de84":"code","78e9a010":"code","aaedb441":"code","bca02a33":"markdown","97488227":"markdown","e7af88b6":"markdown","aec6c674":"markdown","1152ea8e":"markdown","0b8622a1":"markdown","71a520e3":"markdown","c3e1df81":"markdown","9acb60cc":"markdown","1f68d42b":"markdown","62d5f786":"markdown","be609e80":"markdown","521edf3d":"markdown"},"source":{"79f20dd0":"# Imports Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Dropout\n\nfrom keras_tuner.tuners import RandomSearch\nfrom keras.callbacks import EarlyStopping","7d426485":"# Read data from inputted csv file\ntrain_data = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/train.csv')\n\n# Print first five rows from csv file\ntrain_data.head()","f00e9c0b":"# Read data from inputted csv file\neval_data = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/eval.csv')\n\n# Print first five rows from csv file\neval_data.head()","e2ae99de":"# Total of NaN values in each row of train data\ntrain_data.isnull().sum().sum()","cb686147":"# Total of NaN values in each row of eval data\neval_data.isnull().sum().sum()","1b1413fe":"# General information of train data\ntrain_data.info()","b4eae2f0":"# General information of eval data\neval_data.info()","55bff77a":"# Describe information of train data\ntrain_data.describe()","3f1f066d":"# Describe information of eval data\ntrain_data.describe()","8838f3b9":"#Drop unnecessary features from training set\ntrain_data = train_data.drop([\"id\", \"pubchem_id\"], axis = 1)","41b82903":"#Drop unnecessary features from test set\neval_data = eval_data.drop([\"pubchem_id\"], axis = 1)","ba51442a":"# Define target variable for use with models\ny = (train_data[\"Eat\"])\n\n# Define variables for use with models\nX = (train_data.drop(['Eat'], axis = 1))\n\n# Split Train and Test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","5b87d96f":"# Define callback to use early stopping with models to prevent overfitting\ncallback = EarlyStopping(monitor='val_loss', patience=10, verbose =1)","0f7549d3":"# First base Sequential model attempt\nmodel1 = Sequential()\n\n# Input layer\nmodel1.add(Flatten())\n\n# Hidden layers\nmodel1.add(Dense(128, activation='relu'))\nmodel1.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel1.add(Dense(1))\n\n# Compile model\nmodel1.compile(optimizer='adam', loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n\n# Fit model\nfitted1 = model1.fit(X_train, y_train, verbose=2, epochs=1000, validation_data=(X_test, y_test), callbacks=[callback])","aaaab682":"# Evaluate performance\nloss1, rmse1 = model1.evaluate(X_test, y_test)\n\nprint(\"Loss: \", loss1, \"RMSE: \", rmse1)","c73b1091":"# Second Sequential model attempt\nmodel2 = Sequential()\n\n# Input layer\nmodel2.add(Dense(425, input_dim=X_train.shape[1], activation='relu'))\n\n# Hidden layers\nmodel2.add(Dense(142, activation='relu'))\nmodel2.add(Dense(47))\n\n# Output layer\nmodel2.add(Dense(1))\n\n# Compile model\nmodel2.compile(loss='mean_squared_error', optimizer='adam', metrics=['RootMeanSquaredError'])\n\n# Fit model\nfitted2 = model2.fit(X_train, y_train, verbose=2, epochs=1000, validation_data=(X_test, y_test), callbacks=[callback])","7e1884f9":"# Evaluate performance\nloss2, rmse2 = model2.evaluate(X_test, y_test)\n\nprint(\"Loss: \", loss2, \"RMSE: \",rmse2)","f02b04ec":"def build_model(hp):\n    # Third Sequential model attempt\n    model3 = Sequential()\n    \n    # For loop to determine the amount of layers and neurons to use within tuner random search\n    for i in range(hp.Int('num_layers', 15, 30)):\n        model3.add(Dense(units=hp.Int('units_' + str(i), min_value=17, max_value=425, step=17), activation='relu'))\n    \n    # Output layer\n    model3.add(Dense(1, activation='linear'))\n    \n    # Compile model\n    model3.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n    \n    return model3","43f9b3da":"# Random search parameters\ntuner = RandomSearch(\n    build_model,\n    objective='val_mean_squared_error',\n    max_trials=10,\n    executions_per_trial=3)","6a07e577":"# Define callback to use early stopping with search and new model to prevent overfitting\nopto_callback = EarlyStopping(monitor='val_mean_squared_error', patience=10, verbose =1)\n\n# Random search to determine optimal model parameters\ntuner.search(X_train, y_train, epochs=1000, validation_data=(X_test, y_test), callbacks=[opto_callback])","9e20c0b8":"# Show optimal parameters of each random search result trial\ntuner.results_summary()","10835dd5":"# Optimal Sequential model attempt\nopto_model = Sequential()\n    \n# Input layer\nopto_model.add(Dense(255, activation='relu'))\n\n# Hidden layers\nopto_model.add(Dense(221, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(238, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(391, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(357, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(238, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(289, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(323, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(204, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(85, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(374, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(51, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(238, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(102, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(170, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(255, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(187, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(408, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(34, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(68, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(85, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(187, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(272, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(85, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(51, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(238, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(357, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(289, kernel_initializer='normal', activation='relu'))\nopto_model.add(Dense(255, kernel_initializer='normal', activation='relu'))\n\n# Output layer\nopto_model.add(Dense(1, activation='linear'))\n    \n# Compile model\nopto_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['RootMeanSquaredError'])\n\n# Fit model\nfitted3 = opto_model.fit(X_train, y_train, verbose=2, epochs=1000, batch_size = 64, validation_data=(X_test, y_test), callbacks=[opto_callback])","b71fe987":"# Evaluate performance\nloss3, rmse3 = opto_model.evaluate(X_test, y_test)\n\nprint(\"Loss: \", loss3, \"RMSE: \", rmse3)","8f016648":"# Make history callback from model 1 a dataframe so it can be used with plots and describe\nfitted1_df = pd.DataFrame(fitted1.history['root_mean_squared_error'])","5d716ba9":"# Build plot for model 1 fit\nplt.figure()\nplt.plot(fitted1_df)\nplt.title('RMSE of Model 1')\nplt.xlabel('Epoch')\nplt.ylabel('RMSE')\nplt.show()","f7e7a851":"# Make history callback from model 2 a dataframe so it can be used with plots and describe\nfitted2_df = pd.DataFrame(fitted2.history['root_mean_squared_error'])","8157ee66":"# Build plot for model 2 fit\nplt.figure()\nplt.plot(fitted2_df)\nplt.title('RMSE of Model 2')\nplt.xlabel('Epoch')\nplt.ylabel('RMSE')\nplt.show()","86d5e50d":"# Make history callback from model 3 a dataframe so it can be used with plots and describe\nfitted3_df = pd.DataFrame(fitted3.history['root_mean_squared_error'])","917874ce":"# Build plot for model 3 fit\nplt.figure()\nplt.plot(fitted3_df)\nplt.title('RMSE of Model 3')\nplt.xlabel('Epoch')\nplt.ylabel('RMSE')\nplt.show()","c22f6026":"print(\"Summary statistics of distribution from Model 1:\", fitted1_df.describe())","d9671710":"print(\"Summary statistics of distribution from Model 2:\", fitted2_df.describe())","2555de84":"print(\"Summary statistics of distribution from Model 3:\", fitted3_df.describe())","78e9a010":"X_test = eval_data.loc[:, eval_data.columns!='id']\n\npredictions = opto_model.predict(X_test)\n\npredictions = predictions.reshape(len(predictions),)\n\noutput = pd.DataFrame({'id': eval_data.id, 'Eat': predictions})\n\noutput.to_csv('sample_submission.csv', index=False)","aaedb441":"print(output.to_string())","bca02a33":"# Data Cleansing & Feature Engineering","97488227":"# Exploratory Data Analysis","e7af88b6":"**Checking for missing values indicated that there were no issues so nothing needs to be done regarding that.**","aec6c674":"# Summary Statistics of Validation Scores","1152ea8e":"**I tired doing some plots and PCA with the data but it seemed pretty unnecessary with how all the plots turned out looking so I just scrapped it. (I probably also didn't know what I was looking for)**","0b8622a1":"**The id and pubchem id are simply not needed so we can drop them.**","71a520e3":"# Visualize Distribution of Validation Scores","c3e1df81":"# Import Libraries","9acb60cc":"# Load Data","1f68d42b":"**Further feature engineering will be done through the neural networks.**","62d5f786":"# Build Models","be609e80":"# Submission","521edf3d":"**Model 3 appears to be the best by looking at the above data. The plots and mean from summary statistics show Model 3 will perform the best.**"}}