{"cell_type":{"902ae825":"code","e6a97a6f":"code","f68e30c5":"code","679267b7":"code","ecfe07f1":"code","08e90816":"code","e519a246":"markdown","e681ba6a":"markdown","0fd02379":"markdown","f7660237":"markdown","1c3979f9":"markdown","dee2f821":"markdown","1df26ae2":"markdown","bf2e2cb1":"markdown","ff822b59":"markdown","348cfbd9":"markdown","10059b2f":"markdown","a8b0794a":"markdown"},"source":{"902ae825":"from __future__ import print_function\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.datasets import imdb\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","e6a97a6f":"MAX_FEATURES = 20000\n# cut texts after this number of words (among top MAX_FEATURES most common words)\nMAX_SENTENCE_LENGTH = 80\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_FEATURES)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\n# View one example of our dataset before our preprocessing.\nprint(\"\\n\\nExample one before our preprocessing\")\nprint(x_train[0])\n\nprint('\\n\\nPad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_SENTENCE_LENGTH)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_SENTENCE_LENGTH)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\n# View one example of our dataset after our preprocessing.\nprint(\"\\n\\nExample one after our preprocessing\")\nprint(x_train[0])\n","f68e30c5":"EMBEDDING_SIZE = 128\nHIDDEN_LAYER_SIZE = 128\nBATCH_SIZE = 32\nNUM_EPOCHS = 15\n\nprint('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(MAX_FEATURES, EMBEDDING_SIZE, input_length=MAX_SENTENCE_LENGTH))\nmodel.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# summary of our model.\nmodel.summary()\n\n# Compile the model.\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n","679267b7":"\nprint('Train...')\nhistory = model.fit(x_train, y_train,\n          batch_size=BATCH_SIZE,\n          epochs=NUM_EPOCHS,\n          validation_data=(x_test, y_test))\n","ecfe07f1":"score, acc = model.evaluate(x_test, y_test,\n                            batch_size=BATCH_SIZE)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","08e90816":"plt.subplot(211)\nplt.title(\"Accuracy\")\nplt.plot(history.history[\"accuracy\"], color=\"g\", label=\"Train\")\nplt.plot(history.history[\"val_accuracy\"], color=\"b\", label=\"Validation\")\nplt.legend(loc=\"best\")\nplt.subplot(212)\nplt.title(\"Loss\")\nplt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\nplt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\nplt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","e519a246":"**Hope that you find this notebook helpful. More to come.**\n\n**Please upvote this, to keep me motivate for doing better.**\n\n**Thanks.**\n","e681ba6a":"## 4. import data and preprocessing.\n\nWe convert our input sentences to word index sequences, pad them to the `MAX_SENTENCE_LENGTH`\nwords. Since our output label in this case is binary (positive or negative sentiment), we don't need to process the labels.\n","0fd02379":"## 5. LSTM Model for sentiment analysis.\n\nNow let's define our LSTM Model architecture.\n* The input for each row is a sequence of word indices. The sequence length is given by `MAX_SENTENCE_LENGTH`. \n* The first dimension of the tensor is set to `None` to indicate that the **batch size** (the number of records fed to the network each time) is currently unknown at definition time; it is specified during run time using the `batch_size` parameter.\n* So assuming an as-yet undetermined batch size, the shape of the input tensor is `(None, MAX_SENTENCE_LENGTH, 1)`. \n* These tensors are fed into an **embedding layer** of size `EMBEDDING_SIZE` whose weights are initialized with small random values and learned during training. This layer will transform the tensor to a shape `(None,MAX_SENTENCE_LENGTH, EMBEDDING_SIZE)`. \n* The output of the embedding layer is fed into an LSTM with sequence length MAX_SENTENCE_LENGTH and output layer size `HIDDEN_LAYER_SIZE`, so the output of the LSTM is a tensor of shape `(None, HIDDEN_LAYER_SIZE, MAX_SENTENCE_LENGTH)`. By default, the LSTM will output a single tensor of shape `(None, HIDDEN_LAYER_SIZE)` at its last sequence `(return_sequences=False)`.\n* This is fed to a dense layer with output size of `1` with a sigmoid activation function, so it will output either `0` (negative review) or `1` **(positive review)**.\n* We compile the model using the binary cross-entropy loss function since it predicts a binary value, and the **Adam optimizer**, a good general purpose optimizer. Note that the hyperparameters `EMBEDDING_SIZE, HIDDEN_LAYER_SIZE, BATCH_SIZE and NUM_EPOCHS` were tuned experimentally over several runs:\n\nThe following diagram shows the structure of our RNN:\n\n![LSTM.PNG](attachment:LSTM.PNG)\n\nLet's review the code.","f7660237":"### Visualization of History of training.","1c3979f9":"## 6. Training and Evaluate the Model.\n\nWe then train the network for **10** epochs (`NUM_EPOCHS`) and batch size of **32** (`BATCH_SIZE`). At each epoch we validate the model using the test data.\n\nThe output of this step shows how the loss decreases and accuracy increases over multiple epochs:","dee2f821":"## 2. Why is sentiment analysis so important?\n\nSentiment analysis solves a number of genuine business problems:\n\n* It helps to predict customer behavior for a particular product.\n* It can help to test the adaptability of a product.\n* Automates the task of customer preference reports.\n* It can easily automate the process of determining how well did a movie run by analyzing the sentiments behind the movie's reviews from a number of platforms.\n* And many more!\n","1df26ae2":"### Model evaluation.","bf2e2cb1":"### References:\n* **Deep Learning with Keras**, *by Antonio Gulli and Sujit Pal, 2017.*","ff822b59":"# Sentiment Analysis in Python with keras and LSTM.\n\n### Table of interest:\n     Introduction.           \n1. Understanding sentiment analysis from a practitioner's perspective.\n2. Why is sentiment analysis so important?\n3. Formulating the problem statement of sentiment analysis.\n4. import data and preprocessing.\n5. LSTM Model for sentiment analysis.\n6. Training Model and Evaluate the model.\n","348cfbd9":"## 1. Understanding sentiment analysis from a practitioner's perspective.\n\nEssentially, sentiment analysis or sentiment classification fall into the broad category of text classification tasks where you are supplied with a phrase, or a list of phrases and your classifier is supposed to tell if the sentiment behind that is positive, negative or neutral. Sometimes, the third attribute is not taken to keep it a binary classification problem. In recent tasks, sentiments like **\"somewhat positive\"** and **\"somewhat negative\"** are also being considered. \nLet's understand with an example now.\n\nConsider the following phrases:\n\n    \"Titanic is a great movie.\"\n    \"Titanic is not a great movie.\"\n    \"Titanic is a movie.\"\n\nThe phrases correspond to short movie reviews, and each one of them conveys different sentiments. For example, the first phrase denotes positive sentiment about the film Titanic while the second one treats the movie as not so great (negative sentiment). Take a look at the third one more closely. There is no such word in that phrase which can tell you about anything regarding the sentiment conveyed by it. Hence, that is an example of neutral sentiment.\n\nNow, from a strict machine learning point of view, this task is nothing but a supervised learning task. You will supply a bunch of phrases (with the labels of their respective sentiments) to the machine learning model, and you will test the model on unlabeled phrases.\n\n","10059b2f":"## Introduction.\n\n**Natural language processing (NLP)** is the field who is dedicated exclusively to the automated understanding of human language (previously not using deep learning).\n\nPerhaps the best way to quickly get to know NLP is to consider a few of the many challenges\nthe NLP community seeks to solve. Here are a few types of classifcation problem that are\ncommon to NLP:\n* Using the characters of a document to predict where words start and end.\n* Using the words of a document to predict where sentences start and end.\n* Using the words in a sentence to predict the part of speech for each word.\n* Using words in a sentence to predict where phrases start and end.\n* Using words in a sentence to predict where named entity (person, place, thing) references start and end.\n* Using sentences in a document to predict which pronouns refer to the same person \/ place \/ thing.\n* Using words in a sentence to predict the sentiment of a sentence.\n\n**Sentiment Analysis** is a process of extracting opinions that have different polarities. By polarities, we mean ***positive, negative or neutral***. It is also known as opinion mining and polarity detection. With the help of sentiment analysis, you can find out the nature of opinion that is reflected in documents, websites, social media feed, etc. Sentiment Analysis is a type of classification where the data is classified into different classes. These classes can be binary in nature (positive or negative) or, they can have multiple classes (happy, sad, angry, etc.).\n\n**Sentiment analysis** is a vital topic in the field of NLP. It has easily become one of the hottest topics in the field because of its relevance and the number of business problems it is solving and has been able to answer.\n\n![NLP.PNG](attachment:NLP.PNG)\n\nSource: <a href=\"https:\/\/medium.com\/@tomyuz\/a-sentiment-analysis-approach-to-predicting-stock-returns-d5ca8b75a42\">Medium <\/a>","a8b0794a":"## 3. Formulating the problem statement of sentiment analysis:\n\nBefore understanding the problem statement of a sentiment classification task, you need to have a clear idea of general text classification problem. Let's formally define the problem of a general text classification task.\n\n    Input:\n        A document d\n        A fixed set of classes C = {c1,c2,..,cn}\n\n    Output: A predicted class c\u2208C\n\nThe document term here is subjective because in the text classification world. By document, it is meant tweets, phrases, parts of news articles, whole news articles, a full article, a product manual, a story, etc. The reason behind this terminology is word which is an atomic entity and small in this context. So, to denote large sequences of words, this term document is used in general. Tweets mean a shorter document whereas an article means a larger document.\n\n### IMDB movie reviews dataset\n#### We can predict whether people post positive or negative reviews.\nTe IMDB movie reviews dataset is a collection of **review -> rating** pairs that ofen look like\nthe following (this is an imitation, not pulled from IMDB):\n\n        \"This movie was terrible! The plot was dry, the acting\n        unconvincing, and I spilled popcorn on my shirt.\"\n                                            Rating: 1 (stars)\nTe entire dataset consists of around 50,000 of these pairs, where the input reviews are\nusually a few sentences and the output ratings are between 1 and 5 stars. This dataset consists of with over 25,000 reviews for training and 25,000 for the testing set.\n* Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n* Rating have been olso preprocessed, and each rating is encoded as 1 or 0 (positive or negative).\n* As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n"}}