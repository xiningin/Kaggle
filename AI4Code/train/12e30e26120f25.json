{"cell_type":{"37061bd7":"code","b4b47727":"code","2b71c156":"code","375f709b":"code","60c3dd85":"code","581731e1":"code","9c348e3f":"code","a55a9006":"code","6665ff3d":"code","880f5059":"code","71559046":"code","e3c85f7d":"code","55a642c1":"markdown"},"source":{"37061bd7":"# Data processing\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# Machine Learning\nimport optuna\nfrom catboost import CatBoostRegressor\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error","b4b47727":"input_dir = Path('..\/input\/tabular-playground-series-aug-2021\/')\ntrain_df = pd.read_csv(input_dir \/ 'train.csv')\ntest_df = pd.read_csv(input_dir \/ 'test.csv')\nsample_submission = pd.read_csv(input_dir \/ 'sample_submission.csv')","2b71c156":"train_df.head()","375f709b":"test_df.head()","60c3dd85":"sample_submission.head()","581731e1":"X = train_df.drop(['id', 'loss'], axis=1).values\ny = train_df['loss'].values\nX_test = test_df.drop(['id'], axis=1).values","9c348e3f":"# I've found many using MinMaxScaling but I've personally had better results with StandardScaling\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.fit_transform(X_test)","a55a9006":"y_min = y.min()\ny_max = y.max()\n\n# While it's probably rare that values will fall outside the y-min-max range, we should probably do it anyway.\ndef my_rmse(y_true, y_hat):\n    y_true[y_true < y_min] = y_min\n    y_true[y_true > y_max] = y_max\n    \n    y_true[y_hat < y_min] = y_min\n    y_true[y_hat > y_max] = y_max\n    \n    return mean_squared_error(y_true, y_hat, squared=False)","6665ff3d":"def objective(trial):\n    # Split the train data for each trial.\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.4)\n\n    param_grid = {\n        'depth': trial.suggest_int('depth', 6, 10), # Extremely prone to overfitting!\n        'iterations': trial.suggest_int('iterations', 400, 4000, 400), # Extremely prone to overfitting!\n        'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.03), # Most important parameter - the learning rate!\n        'random_strength': trial.suggest_discrete_uniform('random_strength', 1.0, 2.0, 0.1),\n        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 50), # L2 regularization\n    } \n    \n    reg = CatBoostRegressor(\n        grow_policy='Depthwise',\n        leaf_estimation_method='Newton', \n        bootstrap_type='Bernoulli',\n        thread_count=4,\n        loss_function='RMSE',\n        eval_metric='RMSE',\n        od_type='Iter',\n        task_type='GPU',\n        verbose=False,\n        early_stopping_rounds=400,\n        **param_grid\n    )\n    \n    reg.fit(X_train, y_train, verbose=False)\n\n    return my_rmse(y_valid, reg.predict(X_valid))","880f5059":"train_time = 1 * 10 * 60 # Train for up to ten minutes.\nstudy = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='CatBoost')\nstudy.optimize(objective, timeout=train_time)\n\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('\\tValue: {}'.format(trial.value))\nprint('\\tParams: ')\nfor key, value in trial.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","71559046":"# Fetch the best trial parameters and set some settings for the KFold predictions.\ncatb_params = trial.params\ncatb_params['grow_policy'] = 'Depthwise'\ncatb_params['leaf_estimation_method'] = 'Newton'\ncatb_params['bootstrap_type'] = 'Bernoulli'\ncatb_params['thread_count'] = 4\ncatb_params['loss_function'] = 'RMSE'\ncatb_params['eval_metric'] = 'RMSE'\ncatb_params['od_type'] = 'Iter'\ncatb_params['task_type'] = 'GPU'\ncatb_params['early_stopping_rounds'] = 400\n\nn_splits = 10\ntest_preds = None\nkf_rmse = []\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X, y)):\n    # Fetch the train-validation indices.\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    # Create and fit a new model using the best parameters.\n    model = CatBoostRegressor(**catb_params)\n    model.fit(X_train, y_train, verbose=False)\n    \n    # Validation predictions.\n    valid_pred = model.predict(X_valid)\n    rmse = my_rmse(y_valid, valid_pred)\n    print(f'Fold {fold+1}\/{n_splits} RMSE: {rmse:.4f}')\n    kf_rmse.append(rmse)\n    \n    # Use the model trained for 1\/n_splits of the output predictions.\n    if test_preds is None:\n        test_preds = model.predict(X_test)\n    else:\n        # This is kind of naughty for numerical accuracy (may overflow on other problems) but slightly quicker.\n        test_preds += model.predict(X_test)\n\ntest_preds \/= n_splits\nprint(f'Average KFold RMSE: {np.mean(np.array(kf_rmse)):.5f}')","e3c85f7d":"test_preds[test_preds < y_min] = y_min\ntest_preds[test_preds > y_max] = y_max\nsample_submission['loss'] = test_preds\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission","55a642c1":"Some optimal parameters used from:\nhttps:\/\/www.kaggle.com\/somayyehgholami\/1-tps-aug-21-xgboost-catboost\n\nGo throw an upvote!"}}