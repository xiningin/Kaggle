{"cell_type":{"0bb884f0":"code","e07e4601":"code","545c8734":"code","cb2ec42a":"code","c17d2be5":"code","982c48b4":"code","4620b221":"code","9df4cf3b":"code","c0b268e9":"code","004a1422":"code","0e084ffb":"code","9244f164":"code","cc9e6aca":"code","b22f7979":"code","a81c08fa":"code","56cf9986":"code","a98d0c6f":"code","4666957f":"code","1d443ee9":"code","dd85b5a3":"code","2dfeeafd":"code","9af7ffa2":"code","4f1978e3":"code","9c1656a1":"code","e7f2464e":"markdown","a3e631ec":"markdown","633af866":"markdown","bb1c379d":"markdown","8c79dcd0":"markdown","5c3d997f":"markdown","76b09251":"markdown","57962c82":"markdown"},"source":{"0bb884f0":"import gresearch_crypto\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport pickle\n\nimport time\nfrom datetime import datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDEBUG = False","e07e4601":"nrows = 100000 if DEBUG else None\n\ndtype={'Asset_ID': 'int8', 'Count': 'int32', 'row_id': 'int32', 'Count': 'int32',\n       'Open': 'float32', 'High': 'float32', 'Low': 'float32', 'Close': 'float32',\n       'Volume': 'float32', 'VWAP': 'float32'}\n\ntrain_df = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv', low_memory=False, dtype=dtype, nrows=nrows)\nasset_details = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\n\n#create dictionnary of weights\ndict_weights = {}\nfor i in range(asset_details.shape[0]):\n    dict_weights[asset_details.iloc[i,0]] = asset_details.iloc[i,1]\n\n# remove rows with missing targets - DO THIS AT THE END\n# train_df = train_df[~train_df.Target.isna()]\n\n# replace infinite VWAP with close price\ntrain_df.VWAP = np.where(np.isinf(train_df.VWAP),train_df.Close,train_df.VWAP)\n\n#filter to avoid time leakage with the data \nfilter_leakage = pd.to_datetime(train_df['timestamp'], unit='s') < '2021-06-01 00:00:00'\ntrain_df = train_df[filter_leakage]","545c8734":"train_df[train_df.Asset_ID == 2].head()","cb2ec42a":"ref_col = 'Close'","c17d2be5":"#Standardise prices\n\ndef standardise_prices(df,cols=['Open','High','Low','Close','VWAP'],by='Close'):\n    base = train_df[by].copy()\n    df['Open'] = train_df['Open'] \/ base\n    df['High'] = train_df['High'] \/ base\n    df['Low'] = train_df['Low'] \/  base\n    df['Close'] = train_df['Close'] \/ base\n    df['VWAP'] = train_df['VWAP'] \/ base\n    df['Price'] = base\n    return df\n\ndef calc_dollar_features(df, by='Price'):\n    train_df['Volume_dollar'] = train_df['Volume']*train_df[by]\n    train_df['volume_per_trade'] = train_df['Volume']\/train_df['Count']\n    train_df['dollar_per_trade'] = train_df['Volume_dollar']\/train_df['Count']\n    return df\n\ntrain_df['weights'] = train_df.Asset_ID.map(dict_weights).astype('float32')\ntrain_df = standardise_prices(train_df)\ntrain_df = calc_dollar_features(train_df)\n\n# log returns and estimated volatilities\ntrain_df['log_ret'] = np.log(train_df.Close\/train_df.Open)\ntrain_df['GK_vol'] = (1 \/ 2 * np.log(train_df.High \/ train_df.Low) ** 2 - \\\n    (2 * np.log(2) - 1) * np.log(train_df.Close \/ train_df.Open) ** 2).astype('float32')\ntrain_df['RS_vol'] = np.log(train_df.High\/train_df.Close)*np.log(train_df.High\/train_df.Open) + \\\n    np.log(train_df.Low\/train_df.Close)*np.log(train_df.Low\/train_df.Open)","982c48b4":"%%time\n\nfeatures_to_aggregate = ['Count','Open','High','Low','Close','Price','Volume','VWAP','Target','Volume_dollar','volume_per_trade','dollar_per_trade','log_ret','GK_vol','RS_vol']\n\nt, w, A_id = (train_df[col].values for col in ['timestamp','weights','Asset_ID'])\nids, index = np.unique(t, return_index=True)\n\nValues = train_df[features_to_aggregate].values\nsplits = np.split(Values, index[1:])\nsplits_w = np.split(w, index[1:])\nsplits_A_id = np.split(A_id, index[1:])\n\nout = []\n\nfor time_id, x, w, A_id in zip(ids.tolist(), splits, splits_w, splits_A_id):\n    outputs = np.float32(np.sum((x.T*w),axis=1)\/sum(w))\n    outputs = np.tile(outputs, (len(w), 1))\n    out.append(outputs)\n    \nout = np.concatenate(out,axis=0)","4620b221":"train_df[[s+'_M' for s in features_to_aggregate]] = out\n\ndel out, Values\ngc.collect()","9df4cf3b":"train_df = train_df.drop([ref_col,ref_col+'_M'],axis=1)","c0b268e9":"train_df.head()","004a1422":"def timestamp_to_date(timestamp):\n    return(datetime.fromtimestamp(timestamp))\n\nts = train_df.timestamp\nts = ts.apply(timestamp_to_date)","0e084ffb":"ts","9244f164":"train_df['sin_month'] = (np.sin(2 * np.pi * ts.dt.month\/12)).astype('float32')\ntrain_df['cos_month'] = (np.cos(2 * np.pi * ts.dt.month\/12)).astype('float32')\ntrain_df['sin_day'] = (np.sin(2 * np.pi * ts.dt.day\/31)).astype('float32')\ntrain_df['cos_day'] = (np.cos(2 * np.pi * ts.dt.day\/31)).astype('float32')\ntrain_df['sin_hour'] = (np.sin(2 * np.pi * ts.dt.hour\/24)).astype('float32')\ntrain_df['cos_hour'] = (np.cos(2 * np.pi * ts.dt.hour\/24)).astype('float32')\ntrain_df['sin_minute'] = (np.sin(2 * np.pi * ts.dt.minute\/60)).astype('float32')\ntrain_df['cos_minute'] = (np.cos(2 * np.pi * ts.dt.minute\/60)).astype('float32')","cc9e6aca":"# Generate the class\/group data\n\ntime_ids = train_df.timestamp.unique()\n\nn_fold = 5\nsplits = 0.6\nntimes = len(time_ids)\n\nembargo_train_test = 60*24*30\nembargo_fold = 60*24*30\n\ntime_per_fold = (ntimes - 5*embargo_train_test - 5*embargo_fold)\/5\ntrain_len = splits*time_per_fold \ntest_len = (1-splits)*time_per_fold\n\nfold_start = [np.int(i*(len(time_ids)+1)\/5) for i in range(6)]\n\nfor i in range(n_fold):\n    time_folds = time_ids[fold_start[i]:fold_start[i+1]-1]\n    df_fold = train_df[train_df.timestamp.isin(time_folds)]\n    df_fold.to_parquet('df_fold_'+str(i)+'.parquet')\n    \ndel train_df","b22f7979":"time_folds","a81c08fa":"gc.collect()","56cf9986":"%%time\n\nfeatures_to_lag = ['Price','Volume','VWAP','log_ret','RS_vol']\nlags = [2,5,15,30,60,120,300,1800,3750,10*24*60,30*24*60]\n\nfor fold in range(n_fold):\n    print('fold:'+str(fold))\n    df_fold = pd.read_parquet('df_fold_'+str(fold)+'.parquet')\n    \n    tmp = pd.DataFrame()\n    \n    for l in lags:\n        #print('lag:'+str(l))\n        tmp2 = df_fold[features_to_lag+['Asset_ID']].groupby('Asset_ID').transform(lambda s: s.rolling(l, min_periods=1).mean())\n        tmp2.columns = [str(c)+'_l_'+str(l) for c in tmp2.columns]\n        tmp = pd.concat([tmp,tmp2],axis=1)\n        \n    tmp.astype('float32').to_parquet('df_fold_'+str(fold)+'_lag.parquet')","a98d0c6f":"%%time\n\nfeatures_to_lag = ['Price_M','Volume_M','VWAP_M','log_ret_M','RS_vol_M']\nlags = [2,5,15,30,60,120,300,1800,3750,10*24*60,30*24*60]\n\nfor fold in range(n_fold):\n    print('fold:'+str(fold))\n    df_fold = pd.read_parquet('df_fold_'+str(fold)+'.parquet')\n    \n    tmp = pd.DataFrame()\n    \n    for l in lags:\n        #print('lag:'+str(l))\n        tmp2 = df_fold[features_to_lag+['Asset_ID']].groupby('Asset_ID').transform(lambda s: s.rolling(l, min_periods=1).mean())\n        tmp2.columns = [str(c)+'_l_'+str(l) for c in tmp2.columns]\n        tmp = pd.concat([tmp,tmp2],axis=1)\n        \n    tmp.astype('float32').to_parquet('df_fold_'+str(fold)+'_lag_M.parquet')","4666957f":"%%time\n\nfold = 0\nlags = [60,300,1800,3750,10*24*60,30*24*60]\n\nfor fold in range(n_fold):\n    print('fold:'+str(fold))\n    df_fold = pd.read_parquet('df_fold_'+str(fold)+'.parquet')\n    df_fold = df_fold[['Asset_ID','log_ret_M','log_ret']]\n    df_fold['log_ret_M2'] = df_fold['log_ret_M']**2\n    df_fold['log_ret_Mr'] = df_fold['log_ret_M']*df_fold['log_ret']\n    tmp = pd.DataFrame()\n    \n    for l in lags:\n        #print(l)\n        features_to_lag = ['log_ret_M2','log_ret_Mr']\n        #use min periods = l to match definition of target ?\n        tmp2 = df_fold[features_to_lag+['Asset_ID']].groupby('Asset_ID').transform(lambda s: s.rolling(l, min_periods=1).mean())\n        tmp2['beta'] = tmp2['log_ret_Mr'] \/ tmp2['log_ret_M2']\n        tmp2 = tmp2.drop(['log_ret_Mr','log_ret_M2'],axis=1)\n        \n        tmp2.columns = [str(c)+'_l_'+str(l) for c in tmp2.columns]\n        tmp = pd.concat([tmp,tmp2],axis=1)\n        tmp = tmp.loc[:,~tmp.columns.duplicated()]\n    \n    tmp.astype('float32').to_parquet('df_fold_'+str(fold)+'_beta.parquet')\n\ndel tmp2\ndel tmp","1d443ee9":"import sys\n\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https:\/\/stackoverflow.com\/a\/1094933\/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n                         key= lambda x: -x[1])[:20]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n","dd85b5a3":"del ts, df_fold, splits_w, splits_A_id, filter_leakage, ids, index, time_ids\ngc.collect()","2dfeeafd":"for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n                         key= lambda x: -x[1])[:20]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","9af7ffa2":"import os\n\ndict_fold = {}\n\nfor fold in range(n_fold):\n    print('fold:'+str(fold))\n    \n    df_fold = pd.read_parquet('df_fold_'+str(fold)+'.parquet')\n    time_ids = df_fold.timestamp.unique()\n    \n    test_train_len = len(time_ids) - embargo_train_test - embargo_fold\n    \n    train_start = embargo_fold + 1\n    train_end = embargo_fold + np.int(test_train_len*0.6) + 1\n    test_start = embargo_fold + np.int(test_train_len*0.6) + embargo_train_test + 1\n    test_end = len(df_fold.timestamp.unique())\n    \n    dict_fold['train_fold_'+str(fold)] = time_ids[train_start:train_end]\n    dict_fold['test_fold_'+str(fold)] = time_ids[test_start:test_end]\n\ndel df_fold","4f1978e3":"#scipy.stats.qmc.Halton","9c1656a1":"import os\n\nfor fold in range(n_fold):\n    \n    df_train_fold = pd.DataFrame()\n    df_test_fold = pd.DataFrame()\n    \n    df_read = pd.read_parquet(\"df_fold_\"+str(fold)+'.parquet')\n    \n    ind_train = df_read.timestamp.isin(dict_fold['train_fold_'+str(fold)])\n    ind_test = df_read.timestamp.isin(dict_fold['test_fold_'+str(fold)])\n    \n    df_train_fold = df_read[ind_train]\n    df_test_fold = df_read[ind_test]\n    \n    for file in os.listdir('.\/'):\n        if file == \"df_fold_\"+str(fold)+'.parquet':\n            continue\n            \n        elif file.startswith(\"df_fold_\"+str(fold)):\n            print(file)\n            df_read = pd.read_parquet(file)\n            \n            df_train_read = df_read[ind_train]\n            df_test_read = df_read[ind_test]\n            \n            print(df_read.info())\n            df_train_fold = pd.concat([df_train_fold,df_train_read],axis=1)\n            df_train_fold = df_train_fold.loc[:,~df_train_fold.columns.duplicated()]\n            \n            df_test_fold = pd.concat([df_test_fold,df_test_read],axis=1)\n            df_test_fold = df_test_fold.loc[:,~df_test_fold.columns.duplicated()]\n            os.remove('.\/'+file)\n            \n    df_train_fold.to_parquet('train_fold_'+str(fold)+'.parquet')\n    df_test_fold.to_parquet('test_fold_'+str(fold)+'.parquet')","e7f2464e":"# Crypto Forecasting - Feature engineering","a3e631ec":"# lagged Features","633af866":"# Merge data","bb1c379d":"### Training data is in the competition dataset as usual","8c79dcd0":"# Beta Features","5c3d997f":"# Cut data set in five\n\nFirst step to build independant folds. the idea is to end with something like that:","76b09251":"# time encoding","57962c82":"# Market Aggregation\n\ncode inspired from Slawek Biel work in optiver competition."}}