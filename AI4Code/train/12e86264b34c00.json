{"cell_type":{"2c979057":"code","b198355e":"code","c985073b":"code","9a0e3b05":"code","28398ffb":"code","4089538a":"code","fccf68fb":"code","65de9b46":"markdown","35810966":"markdown","c1fe86f0":"markdown","852deafc":"markdown"},"source":{"2c979057":"# test tokenizaion\nfrom nltk import sent_tokenize\nimport nltk\nnltk.download('punkt') #download the period, enter ...\n\n# steve jobs' speech\ntext_sample = 'I am honored to be with you today at your commencement from one of the finest universities in the world. I never graduated from college. Truth be told, this is the closest I\\'ve ever gotten to a college graduation. Today I want to tell you three stories from my life. That\\'s it. No big deal. Just three stories.'\n\nsentences = sent_tokenize(text=text_sample)\n\nprint(type(sentences),len(sentences))\nprint(sentences)","b198355e":"# word tokenization\nfrom nltk import word_tokenize\n\nsentence = 'The first story is about connecting the dots.'\nwords = word_tokenize(sentence)\nprint(type(words),len(words))\nprint(words)","c985073b":"from nltk import word_tokenize, sent_tokenize\n\n# function that break the document into word\n\ndef tokenize_text(text):\n    \n    # tokenize sentence\n    sentences = sent_tokenize(text)\n    \n    # tokenize word \n    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n    \n    return word_tokens\n\nword_tokens = tokenize_text(text_sample)\nprint(type(word_tokens),len(word_tokens))\nprint(word_tokens)","9a0e3b05":"import nltk\nnltk.download('stopwords')\n\n# available language\nfrom nltk.corpus import stopwords\nprint('-----what languages do it have?-----')\nprint(stopwords.fileids(),'\\n\\n')\n\n# check the english stop word\nprint('-----stop words of english-----')\nprint('the number of english stop word: ',len(nltk.corpus.stopwords.words('english')))\nprint(nltk.corpus.stopwords.words('english')[:10])\n","28398ffb":"# let's eliminate stop word in word_tokens\nstopwords = nltk.corpus.stopwords.words('english')\nall_tokens = []\n\n# eliminate stop word of sentence\nfor sentence in word_tokens:\n    filtered_words = []\n    \n    #eliminate stop word of word\n    for word in sentence:\n        #make all of word into small letter\n        word = word.lower()\n        # check whether word is stopword or not\n        if word not in stopwords:\n            filtered_words.append(word)\n            \n    all_tokens.append(filtered_words)\n    \nprint(all_tokens)","4089538a":"# stemming\nfrom nltk.stem import LancasterStemmer\nstemmer = LancasterStemmer()\n\nprint(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\nprint(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\nprint('You can see \\'amuse\\' recongnized into \\'amus\\' ')","fccf68fb":"# lemmatization\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nnltk.download('wordnet')\n\nlemma = WordNetLemmatizer()\n\nprint(lemma.lemmatize('amusing', 'v'),lemma.lemmatize('amuses', 'v'),lemma.lemmatize('amused', 'v'))\nprint(lemma.lemmatize('happier', 'a'),lemma.lemmatize('happiest', 'a'))\n\nprint('It is more accurate !')","65de9b46":"# stemming and lemmatization\n\nMany word change according to the grammer. The word 'work', for example, can be changed into 'worked', 'working', 'works', something like that. So we have to find the original form of the word using stemming and lemmatization. It have difference between these and lemmatization is more accurate.\n\n* stemming\n\nIt apply more general and simple way to find out the original form. So some spell can be ruined\n\n* lemmatization\n\nIt tranform the word according to word class, so it is more accurate. But It take more time and you should designate the word of class.","35810966":"# eliminate stop word\n\nstop word means the word that have no meaning in text analysis. For example, in english 'is', 'the', 'a', 'will' can be stop word. It is essential to make the sentence but it is not useful in text analysis. So we have to remove it.","c1fe86f0":"# Text analysis\n\nText analysis is literally analysis text that is unstructed data. According to this analysis, we can do text classification, sentiment analysis, summarization and clustering. It can be supervised or nonsupervised learning based on what you want to tract of this data. Process of TA(Text Analysis) is as follow\n\n> 1. preprocessing text data\n> 2. feature vector\n> 3. train\/predict\/evaluate ML model\n\nIn this notebook, We are going to do some preprocessing the text data in many aspect based on NLTK(Natural Language Toolkit for Python) package.\n\n* text tokenization\n* eliminate stop word\n* stemming and lemmatization\n\n","852deafc":"# text tokenization\n\n* Sentence tokenization\n\nIt usually break the document into sentence based on period(.) or enter(\\n) ... It is useful when the symetic meaning of sentence is important. We can make this using sent_tokenize(text=). And it return the type of list that contain sentences.\n\n* Word tokenization\n\nIt break the sentence into word based on space, comma(,), period, enter ... It is useful when the order of word is not important at all. But be carefull that it ignore the contextual meaning. Same as before, we can make this using word_tokenize(). And it return the type of list that contain words."}}