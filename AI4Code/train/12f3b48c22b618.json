{"cell_type":{"d6ee077e":"code","aee1151d":"code","3f1f053d":"code","f31367f3":"code","0ba902fd":"code","1023518f":"code","95ba14fc":"code","074e8a7a":"code","0743b0e5":"code","cefa2188":"code","2d3db64c":"code","969c3600":"code","e384e4ea":"code","eedcacc3":"code","b86c9175":"code","54be10c7":"code","b9db6df5":"code","a4ceb8bf":"code","cb24888c":"code","1e36f901":"code","6f6d8697":"code","a147e147":"code","58a3b4e4":"code","4fa5a3f8":"code","d5e4df78":"code","2d34e1cd":"code","73e977c0":"code","b3d2b531":"code","64ae9aa6":"code","f82f2474":"code","c7aab611":"code","a057deba":"markdown","7df9d1de":"markdown","5677ec1e":"markdown","5b4a5dee":"markdown","5b3a0f00":"markdown","d2acf858":"markdown","0459ae33":"markdown"},"source":{"d6ee077e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aee1151d":"import matplotlib as plt\n%matplotlib inline\nimport seaborn as sns","3f1f053d":"df = pd.read_csv('..\/input\/diabetescsv\/diabetes.csv')\ndf.describe()","f31367f3":"# Eye ball the imported dataset\ndf.info()\n\n# Take away : No missing values. All dimesions are numerics. So, no conversion required.\n# However, there are 0 values for Glucose, BllodPressure, SkinThickness, Insulin, BMI , Which cannot be correct. So, needs to \n# be treated.","0ba902fd":"df.shape # print dimension","1023518f":"df.Outcome.value_counts() # there are more data points for non-diabetics compared to diabetics,model which will be trained \n# using this data ideally it should be good in predicting the non-diabetics patients first.","95ba14fc":"sns.countplot(x='Outcome' , data =df);","074e8a7a":"# Check data types of dataset\n\ndf.dtypes # all data types are numeric. So, encoding is needed.","0743b0e5":"df.describe()\n\n#There are incorrect values i.e.0's in Glucose, BloodPressure, SkinThickness, Insulin, BMI. \n# replacing 0 with median of corresponding column.","cefa2188":"dataframe_temp = df.drop([\"Pregnancies\",\"Outcome\"],axis = 1)\ndataframe_temp\nmedians = dataframe_temp.median()\nprint(\"medians\",medians)\ndataframe_nonzero = dataframe_temp.replace(0,medians)\ndataframe_nonzero[\"Pregnancies\"] = df[\"Pregnancies\"]\ndataframe_nonzero[\"Outcome\"] = df[\"Outcome\"]\ndataframe_nonzero","2d3db64c":"corr = dataframe_nonzero.corr()\ncorr\n\n# Takeaway : outcome is positively corelated to Glucose feature.\n# Age & no. of pregencies have positive corelation.\n# BMI & Skin thickness has positive corelation\n# No other strong negetive corelation is observed.","969c3600":"sns.heatmap(corr)","e384e4ea":"sns.pairplot(dataframe_nonzero, diag_kind='kde', hue=\"Outcome\") # plotting pairplot","eedcacc3":"from sklearn.model_selection import train_test_split\nX = dataframe_nonzero.drop('Outcome', axis=1)\nY = dataframe_nonzero['Outcome']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=1)","b86c9175":"from sklearn import svm\nfrom sklearn.svm import SVC\n\nclf = svm.SVC(C = 100,gamma= \"scale\")\nclf.fit(X_train,Y_train)","54be10c7":"score1 = clf.score(X_test,Y_test)\nscore1","b9db6df5":"from sklearn import metrics\nY_pred = clf.predict(X_test)  \nprint( metrics.confusion_matrix(Y_test,Y_pred))","a4ceb8bf":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\n\n#Zscore\nfrom scipy.stats import zscore \nX_train_z = X_train.apply(zscore) # converting to Z score\nX_test_z = X_test.apply(zscore)","cb24888c":"# Model score on Minmax scaled values\nclf = svm.SVC(C = 10,gamma= \"scale\")\nclf.fit(X_train_scaled,Y_train)\nscore2 = clf.score(X_test_scaled,Y_test)\nscore2","1e36f901":"# Model score using zscore  values\nclf = svm.SVC(C = 10,gamma= \"scale\")\nclf.fit(X_train_z,Y_train)\nscore3 = clf.score(X_test_z,Y_test)\nscore3","6f6d8697":"clf = svm.SVC(C = 1000,gamma= \"scale\")\nclf.fit(X_train,Y_train)\nscore4 = clf.score(X_test,Y_test)\nprint(\"Model score for non-scaled datapoints\", score4)\n\n# model accuracy has increased on non-scaled data,however for scaled values with c = 1000, model accuracy is decreasing.","a147e147":"import multiprocessing \nfrom sklearn.model_selection import GridSearchCV","58a3b4e4":"param_grid = [    {        \n     'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],        \n     'C': [ 0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 1.8, 2.0, 2.5, 3.0 ]    } ]","4fa5a3f8":"gs = GridSearchCV(estimator=SVC(), param_grid=param_grid,scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())","d5e4df78":"gs.fit(X_train_scaled, Y_train)","2d34e1cd":"gs.best_estimator_","73e977c0":"gs.best_score_","b3d2b531":"from sklearn.metrics import roc_auc_score,roc_curve","64ae9aa6":"auc = roc_auc_score(Y_test,Y_pred)\nprint(\"AUC %0.3f\" %auc)","f82f2474":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier","c7aab611":"seed = 7\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RFC', RandomForestClassifier()))\nresults = []\nnames = []\nscoring = 'accuracy'\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train,\ncv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","a057deba":" AUC measures how true positive rate (recall) and false positive rate trade off ","7df9d1de":"we can try increasing either C or gamma to fit a more complex model.","5677ec1e":"Calculate AUC score and plot ROC curve","5b4a5dee":"Out of 768 data points, 500 are labeled as 0 and 268 as 1.\nOutcome 0 means No diabetes, outcome 1 means diabetes","5b3a0f00":"Scaling the datapoints using MinMax Scalar","d2acf858":"Linear Discriminant Analysis is giving a better accuracy of 77% as compared with the other models.","0459ae33":"Training Support vector Machines"}}