{"cell_type":{"5ed31d36":"code","799eba62":"code","8798fe81":"code","fda23882":"code","4ba4de4c":"code","e6a103c3":"code","c2e244a5":"code","e7c1f4d4":"code","78fd216d":"code","2e49b4c8":"code","85a0b18e":"code","0afc70c8":"code","3dfdfe78":"code","df343e5c":"code","b7e203ae":"code","129b08e8":"code","fb343ac8":"code","28c8e837":"code","a74e7c00":"code","30134029":"code","12b5178b":"code","2a5bfb72":"code","29d35b8b":"code","c9aa7687":"code","622dc562":"code","b6507967":"code","363143d7":"code","8ab5038d":"code","a6e4587a":"code","e4548f47":"code","1c65b4e9":"code","ff76177e":"code","cefec326":"code","6dfb8c0d":"code","8f900bf1":"code","b954848c":"code","bc767058":"code","caf683e5":"code","cfb97433":"code","0bb91eac":"code","5d1510d6":"code","356cef9c":"code","fd3bd9d5":"code","42d6e01c":"code","d3fddf5e":"code","b1408ce8":"code","4c0ad565":"code","fd237911":"code","aebd2e1d":"code","40ac27e0":"code","9836854d":"code","bd07f10a":"code","37fc38b9":"code","2672b7ea":"code","dbce9428":"code","4575d07f":"code","1ad6c0d7":"code","02571828":"code","2855214a":"code","d0b33224":"code","7e9fec62":"code","0ffef5c8":"code","47b77f5c":"code","77c9ac45":"code","413be1e3":"code","0900d2e8":"code","3d90287c":"code","980d274e":"markdown","58ef38ad":"markdown","d02bbc9d":"markdown","96212242":"markdown","6e35be94":"markdown","adc1e8af":"markdown","9924ff9e":"markdown","6cea01fb":"markdown","ba054222":"markdown","4c0de03b":"markdown","d6c300bd":"markdown","8579e97f":"markdown","557897ef":"markdown","9df83a06":"markdown","351f2fb8":"markdown","549563e4":"markdown","e020caf2":"markdown","80e068a4":"markdown","251b0c5e":"markdown","f2004b66":"markdown","34bff2d6":"markdown","77e331bb":"markdown","fd7d78ec":"markdown","8da427e5":"markdown","ee54b525":"markdown","448cb90c":"markdown","e8dcb2a4":"markdown","927e6df5":"markdown","df70a5a8":"markdown","20db8a78":"markdown","2cc30850":"markdown","eaa428dc":"markdown","c541c049":"markdown","407c129a":"markdown","d79ca4c7":"markdown","c5eceb01":"markdown","4a4ce44c":"markdown","e7f6661a":"markdown"},"source":{"5ed31d36":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\n%matplotlib inline\nimport os\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport pyarrow","799eba62":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 150)\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'","8798fe81":"raw_df = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\nraw_df","fda23882":"raw_df.info()","4ba4de4c":"raw_df.dropna(subset=['RainTomorrow'], inplace=True)","e6a103c3":"plt.title('No. of Rows Per Year');\nsns.countplot(x=pd.to_datetime(raw_df.Date).dt.year);","c2e244a5":"year = pd.to_datetime(raw_df.Date).dt.year\n\ntrain_df = raw_df[year < 2015]\nval_df = raw_df[year == 2015]\ntest_df = raw_df[year > 2015]","e7c1f4d4":"print(train_df.shape)\nprint(val_df.shape)\nprint(test_df.shape)","78fd216d":"input_cols = list(train_df.columns[1:-1])\ntarget_cols = train_df.columns[-1]","2e49b4c8":"input_cols,target_cols","85a0b18e":"X_train = train_df[input_cols].copy()\nY_train = train_df[target_cols].copy()\nX_val = val_df[input_cols].copy()\nY_val = val_df[target_cols].copy()\nX_test = test_df[input_cols].copy()\nY_test = test_df[target_cols].copy()","0afc70c8":"numeric_cols = list(X_train.select_dtypes(include=np.number).columns)\ncategorical_cols = list(X_train.select_dtypes(include='object').columns)","3dfdfe78":"numeric_cols, categorical_cols","df343e5c":"X_train[numeric_cols].isna().sum().sort_values(ascending=False)","b7e203ae":"imputer = SimpleImputer(strategy='mean')\nimputer.fit(raw_df[numeric_cols])","129b08e8":"X_train[numeric_cols] = imputer.transform(X_train[numeric_cols])\nX_val[numeric_cols] = imputer.transform(X_val[numeric_cols])\nX_test[numeric_cols] = imputer.transform(X_test[numeric_cols])","fb343ac8":"X_train[numeric_cols].isna().sum().sort_values(ascending=False)","28c8e837":"scaler = MinMaxScaler()\nscaler.fit(raw_df[numeric_cols])","a74e7c00":"X_train[numeric_cols] = scaler.transform(X_train[numeric_cols])\nX_val[numeric_cols] = scaler.transform(X_val[numeric_cols])\nX_test[numeric_cols] = scaler.transform(X_test[numeric_cols])","30134029":"categorical_cols","12b5178b":"X_train[categorical_cols].isna().sum().sort_values(ascending=False)","2a5bfb72":"X_train[categorical_cols] = X_train[categorical_cols].fillna('Unknown')\nX_val[categorical_cols] = X_val[categorical_cols].fillna('Unknown')\nX_test[categorical_cols] = X_val[categorical_cols].fillna('Unknown')","29d35b8b":"X_train[categorical_cols].isna().sum().sort_values(ascending=False)","c9aa7687":"encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\nencoder.fit(X_train[categorical_cols])","622dc562":"encoded_cols = list(encoder.get_feature_names(categorical_cols))","b6507967":"encoded_cols","363143d7":"X_train[encoded_cols] = encoder.transform(X_train[categorical_cols])\nX_val[encoded_cols] = encoder.transform(X_val[categorical_cols])\nX_test[encoded_cols] = encoder.transform(X_test[categorical_cols])","8ab5038d":"X_train = X_train[numeric_cols + encoded_cols]\nX_val = X_val[numeric_cols + encoded_cols]\nX_test = X_test[numeric_cols + encoded_cols]","a6e4587a":"model = DecisionTreeClassifier(random_state = 42)","e4548f47":"%%time\nmodel.fit(X_train, Y_train)","1c65b4e9":"X_train_pred = model.predict(X_train)\nX_train_pred","ff76177e":"pd.value_counts(X_train_pred)","cefec326":"train_probs = model.predict_proba(X_train)\ntrain_probs","6dfb8c0d":"print('Training Accuracy :',accuracy_score(X_train_pred,Y_train)*100)","8f900bf1":"print('Validation Acuracy :',model.score(X_val,Y_val)*100)","b954848c":"Y_val.value_counts() \/ len(Y_val)","bc767058":"plt.figure(figsize=(80,50))\nplot_tree(model, feature_names=X_train.columns, max_depth=2, filled=True);","caf683e5":"model.tree_.max_depth","cfb97433":"tree_text = export_text(model, max_depth=10, feature_names=list(X_train.columns))\nprint(tree_text)","0bb91eac":"model.feature_importances_","5d1510d6":"feature_importance_df = pd.DataFrame({\n    'Feature' : X_train.columns,\n    'Importance' : model.feature_importances_\n}).sort_values(by='Importance', ascending=False)\nfeature_importance_df","356cef9c":"plt.title('Feature Importance')\nsns.barplot(data = feature_importance_df.head(20), x='Importance', y='Feature');","fd3bd9d5":"model.tree_.max_depth","42d6e01c":"model = DecisionTreeClassifier(random_state=42, max_depth=3)","d3fddf5e":"model.fit(X_train, Y_train)","b1408ce8":"print('Accuracy in Training Dataset :',model.score(X_train, Y_train)*100)","4c0ad565":"print('Accuracy in Validation Dataset :',model.score(X_val, Y_val)*100)","fd237911":"plt.figure(figsize=(80,50))\nplot_tree(model, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_);","aebd2e1d":"tree_text = export_text(model, max_depth=10, feature_names=list(X_train.columns))\nprint(tree_text)","40ac27e0":"def max_depth_accuracy1(max_depth_val):\n    model = DecisionTreeClassifier(random_state=42, max_depth=max_depth_val)\n    model.fit(X_train, Y_train)\n    train_accuracy = model.score(X_train, Y_train)*100\n    val_accuracy = model.score(X_val, Y_val)*100\n    return {'Max_Depth' : max_depth_val, 'Training_Accuracy' : train_accuracy, 'Validation_Accuracy' : val_accuracy}","9836854d":"%%time\naccuracies_df1 = pd.DataFrame([max_depth_accuracy1(i) for i in range(1,48)])\naccuracies_df1","bd07f10a":"accuracies_df1.to_parquet('Accuracies_max_depth_tuning1.parquet')","37fc38b9":"accuracies_df1 = pd.read_parquet('Accuracies_max_depth_tuning1.parquet')","2672b7ea":"accuracies_df1","dbce9428":"plt.title('Training Accuracy Vs Validation Accuracy');\nplt.plot(accuracies_df1['Max_Depth'], accuracies_df1['Training_Accuracy']);\nplt.plot(accuracies_df1['Max_Depth'], accuracies_df1['Validation_Accuracy']);\nplt.legend(['Training Accuracy', 'Validation Accuracy']);\nplt.xticks(range(0,48, 2))\nplt.xlabel('Max Depth');\nplt.ylabel('Errors');","4575d07f":"model = DecisionTreeClassifier(random_state=42, max_depth=7)\nmodel.fit(X_train, Y_train)\nprint('Training Accuracy :', model.score(X_train,Y_train)*100)\nprint('Validation Accuracy :', model.score(X_val, Y_val)*100)","1ad6c0d7":"model = DecisionTreeClassifier(max_leaf_nodes=128, random_state=42)\nmodel.fit(X_train, Y_train)\nprint('Training Accuracy :', model.score(X_train,Y_train)*100)\nprint('Validation Accuracy :', model.score(X_val, Y_val)*100)","02571828":"model.tree_.max_depth","2855214a":"accuracies_df1.loc[accuracies_df1['Max_Depth'] == model.tree_.max_depth]","d0b33224":"model_text = export_text(model, feature_names=list(X_train.columns))\nprint(model_text)","7e9fec62":"model = DecisionTreeClassifier(max_leaf_nodes=128, random_state=42, max_depth=6)","0ffef5c8":"def max_depth_accuracy2(max_depth_val):\n    model = DecisionTreeClassifier(random_state=42, max_depth=max_depth_val, max_leaf_nodes=128)\n    model.fit(X_train, Y_train)\n    train_accuracy = model.score(X_train, Y_train)*100\n    val_accuracy = model.score(X_val, Y_val)*100\n    return {'Max_Depth' : max_depth_val, 'Training_Accuracy' : train_accuracy, 'Validation_Accuracy' : val_accuracy}","47b77f5c":"%%time\naccuracies_df2 = pd.DataFrame([max_depth_accuracy2(i) for i in range(1,14)])\naccuracies_df2","77c9ac45":"accuracies_df2.to_parquet('Accuracies_max_depth_tuning2.parquet')","413be1e3":"accuracies_df2 = pd.read_parquet('Accuracies_max_depth_tuning2.parquet')","0900d2e8":"plt.title('Training Accuracy Vs Validation Accuracy');\nplt.plot(accuracies_df2['Max_Depth'], accuracies_df2['Training_Accuracy']);\nplt.plot(accuracies_df2['Max_Depth'], accuracies_df2['Validation_Accuracy']);\nplt.legend(['Training Accuracy', 'Validation Accuracy']);\nplt.xticks(range(0,16, 2))\nplt.xlabel('Max Depth');\nplt.ylabel('Errors');","3d90287c":"model = DecisionTreeClassifier(max_depth=9, max_leaf_nodes=128, random_state=42)\nmodel.fit(X_train, Y_train)\nprint('Training Accuracy :', model.score(X_train,Y_train)*100)\nprint('Validation Accuracy :', model.score(X_val, Y_val)*100)","980d274e":"#### Save accuracies_df2 dataframe","58ef38ad":"# Encoding Categorical Columns","d02bbc9d":"##### Plotting Tuning Graph\nLet'us visualise the training accuracy and validation accuracy with different max_depths and max_leaf_nodes = 128.<br>","96212242":"## Training","6e35be94":"# Scaling Numeric Features","adc1e8af":"# Identify Xs & Ys","9924ff9e":"# Hyperparameter Tuning To Reduce Overfitting\n\nThe `DecisionTreeClassifier` accepts several arguments, some of which can be modified to reduce overfitting.<br>\n\n- `max_depth`\n- `max_leaf_nodes`","6cea01fb":"## Visualization\nWe'll visualize the decision tree learned from training data.","ba054222":"Although the training accuracy is 100%, the accuracy on the validation set is just about 79%, which is only marginally better then always predicting \"No\", i.e,predicting always 'No' also gives around 78.8 % accuracy.<br>\n\nThis is because of overfitting.<br>\n<b>Note :<\/b><br>\nDecision Trees tends to overfit.","4c0de03b":"# Training & Visualizing Decision Trees\nA decision tree in general parlance represents a hierarchical series of binary decisions:\n\nA decision tree in machine learning works in exactly the same way, and except that we let the computer figure out the optimal structure & hierarchy of decisions, instead of coming up with criteria manually.","d6c300bd":"## max_depth\nBy reducing the tree maximum depth can reduce overfitting.<br>\nMaximum depth (default) is 48 which is reduced to 3 to reduce overfittting as below.","8579e97f":"From the graph it can also be seen that training accuracy increases with increase in max_depth<br>\nwhile validation accuracy first increases (till max_depth = 7) and then decreases.<br>\nTherefore, optimal max_depth is 7.","557897ef":"## Evaluation","9df83a06":"The training set accuracy is close to 100%! But we can't rely solely on the training set accuracy, we must evaluate the model on the validation set too. \n\nWe can make predictions and compute accuracy in one step using `model.score`","351f2fb8":"# Train, Validation, Test Split","549563e4":"# Identify Numerical & Categorical Columns","e020caf2":"# Impute Missing Values","80e068a4":"#### Load saved accuracies_df1","251b0c5e":"# Import Libraries","f2004b66":"# Identify Inputs & Targets Columns","34bff2d6":"Since the max_depth value without manual constraint for which our model overfitted is 48.<br>\nAnd the max_depth value obviously can't be 0 (or lesser).<br>\nSo let's find what the best value of max_depth would be by trial and error method and use the max_depth for<br>\nwhich the errors of train and validation dataset is optimal.","77e331bb":"## max_leaf_nodes\nAnother way to control the size of complexity of a decision tree is to limit the number of leaf nodes. This allows branches of the tree to have varying depths. ","fd7d78ec":"It seems max_depth = 9 and max_leaf_nodes = 128 is the optimal hyperparameters","8da427e5":"#### Buiild Decision Tree with max_depth = 7","ee54b525":"***Note :***<br>\nFill Nans with 'Unknown' value in categorical columns","448cb90c":"Let's see the accuracies when max_depth was set to 12 while tuning max_depth parameter.<br>\nThey are not same because number of nodes in that case might be different.","e8dcb2a4":"##### Plotting Tuning Graph\nLet'us visualise the training accuracy and validation accuracy with different max_depths.<br>","927e6df5":"## Configurations","df70a5a8":"# Feature Importance\nDecision Trees can find iportance of features by itself.<br>\nBelow are thew importances of 119 features(total number of features in the training dataset)","20db8a78":"From the dataframe, it can be seen that the training accuracy increases with increase in max_depth.<br>\nIt is also to be noted that validation accuracy first increases and then decreases.<br>","2cc30850":"#### Visualisation","eaa428dc":"Seems prediction has more Nos.<br>\nThis is because the training set is also skewed","c541c049":"# Import Dataset","407c129a":"#### Load saved accuracies_df2","d79ca4c7":"### max_depth Tuning","c5eceb01":"#### Save accuracies_df1 dataframe","4a4ce44c":"While working with chronological data, it's often a good idea to separate the training, validation and test sets with time, so that the model is trained on data from the past and evaluated on data from the future.\n\nWe'll use the data till 2014 for the training set, data from 2015 for the validation set, and the data from 2016 & 2017 for the test set.  ","e7f6661a":"Remove rows for which target column is empty"}}