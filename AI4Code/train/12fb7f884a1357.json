{"cell_type":{"69e9445e":"code","36fefb43":"code","229b5b8b":"code","571b52c5":"code","595080d0":"code","ff98db7c":"code","12347ca2":"code","46e7a8d5":"code","21603ae6":"markdown","88d3bd53":"markdown","88414ef4":"markdown","4fdd2215":"markdown","2fa10113":"markdown","4c05e524":"markdown","2370bd1d":"markdown","42ded2c0":"markdown","83aaafe3":"markdown"},"source":{"69e9445e":"import json\nwith open('..\/input\/toeic-test\/toeic_test.json') as input_json:\n    data = json.load(input_json)\n\n# Data is a dictionary contain over 3000 toeic question\n# Let read the first question and familiar with format\ndata['1']","36fefb43":"# Convert data dict to list so that we can iterate them\nquestion_infors = []\n\nfor key, value in data.items():\n    question_infors.append(value)\n\nquestion_infors[0]","229b5b8b":"!pip install -U pytorch-pretrained-bert;","571b52c5":"import torch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM","595080d0":"class TOEICBert():\n    \"\"\"\n    Model using pretrained Bert for answering toeic question, running for each example\n    Bertmodel: we can choose bert large cased\/bert large uncased, etc\n    \n    Model return the answer for the question based on the highest probability\n    \"\"\"\n    def __init__(self, bertmodel):\n        self.use_cuda = torch.cuda.is_available()\n        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n        self.bertmodel = bertmodel\n        # Initial tokenizer to tokenize the question later\n        self.tokenizer = BertTokenizer.from_pretrained(self.bertmodel)\n        self.model = BertForMaskedLM.from_pretrained(self.bertmodel).to(self.device)\n         # We used pretrained BertForMaskedLM to fill in the blank, do not fine tuning so we set model to eval\n        self.model.eval()\n        \n    def get_score(self,question_tensors, segment_tensors, masked_index, candidate):\n        # Tokenize the answer candidate\n        candidate_tokens = self.tokenizer.tokenize(candidate)\n        # After tokenizing, we convert token to ids, (word to numerical)\n        candidate_ids = self.tokenizer.convert_tokens_to_ids(candidate_tokens)\n        predictions = self.model(question_tensors, segment_tensors)\n        predictions_candidates = predictions[0,masked_index, candidate_ids].mean()\n        return predictions_candidates.item()\n    \n    def predict(self,row):\n        # Tokenizing questions, convert '___' to '_' so that we can MASK it\n        question_tokens = self.tokenizer.tokenize(row['question'].replace('___', '_'))\n        masked_index = question_tokens.index('_')\n        # Assign [MASK] to blank that need to be completed\n        question_tokens[masked_index] = '[MASK]'\n        segment_ids = [0] * len(question_tokens)\n        segment_tensors = torch.tensor([segment_ids]).to(self.device)\n        question_ids = self.tokenizer.convert_tokens_to_ids(question_tokens)\n        question_tensors = torch.tensor([question_ids]).to(self.device)\n        candidates = [row['1'], row['2'], row['3'], row['4']]\n        # Return probabilities of answer choice [prob1, prob2, prob3, prob4]\n        predict_tensor = torch.tensor([self.get_score(question_tensors, segment_tensors,\n                                                masked_index, candidate) for candidate in candidates])\n        # Softmax the predict probability to return the index for maximum values\n        predict_idx = torch.argmax(predict_tensor).item()\n        return candidates[predict_idx]","ff98db7c":"Bertmodel  = 'bert-large-uncased'\nmodel = TOEICBert(Bertmodel)","12347ca2":"count = 0\nfor question in question_infors:\n    anwser_predict = model.predict(question)\n    if anwser_predict == question['anwser']:\n        count+=1\n\nnum_questions = len(question_infors)\nprint(f'The model predict {round(count\/num_questions,2) * 100} % of total {len(question_infors)} questions')","46e7a8d5":"def Answer_toeic(question):    \n    predict_anwser = model.predict(question)\n    anwser = question['anwser']\n    if predict_anwser == anwser:\n        print(f'The BertModel answer: {predict_anwser}')\n        print('This is right answer')\n    else:\n        print(f'The BertModel answer: {predict_anwser}')\n        print('This is wrong answer')\n        \n# now we have a TOEIC question on below:\nquestion = {'1': 'different',\n '2': 'differently',\n '3': 'difference',\n '4': 'differences',\n 'anwser': 'different',\n 'question': 'Matos Realty has developed two ___ methods of identifying undervalued properties.'}\n\n# Check the model\nAnswer_toeic(question)","21603ae6":"## Reference\n\nhttps:\/\/web.stanford.edu\/class\/cs224n\/slides\/Jacob_Devlin_BERT.pdf\n\nhttps:\/\/yashuseth.blog\/2019\/06\/12\/bert-explained-faqs-understand-bert-working\/#:~:text=BERT%20is%20a%20deep%20learning,task%2Dspecific%20fine%2Dtuning.\n\nhttps:\/\/towardsdatascience.com\/intuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e\n\nhttps:\/\/github.com\/graykode\/toeicbert\n\nhttp:\/\/jalammar.github.io\/illustrated-bert\/\n\nhttps:\/\/mlexplained.com\/2019\/01\/07\/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained\/","88d3bd53":"## Load data","88414ef4":"# **2. APPLYING BERT FOR ANSWERING TOEIC READING TEST** <a class=\"anchor\" id=\"2\"><\/a>\n\n[Go back to table of contents](#0.1)\n \n In this section, we'll use pytorch pretrain BERT for answering TOEIC Reading test (part 5). The TOEIC reading question example will be as follow:\n\n![toeic.JPG](attachment:toeic.JPG)\n\nOur task: we will return the best answer to complete the sentence based on 4 given choices. \nIdeas for solving: Because we have 4 answer choice, We will predict the probability for each answer and return the vector combining 4 probabilies:\n\nVector = [prob1, prob2, prob3, prob 4]. \n\nThen we softmax this vector to return the index of maximum probability ==> return answer\n\n### Data overview\n\nThe input data collected from ETS Toeic test and will be stored in dictionary. \n\n```python\n'5': {'1': 'qualified',\n  '2': 'qualifications',\n  '3': 'qualify',\n  '4': 'qualifying',\n  'anwser': 'qualifications',\n  'question': 'Once you have your resume with references and ___ , please submit it to the Human Resources Department on the 3rd floor.'}\n \n```\nThe key '5': indicate that this question is 5th question in total question dataset\n\nThe values: is a dictionary contain : \n* 4 anwser choices\n* The right answer\n* The question\n\nThe idea authority of using pretrained BERT for TOEIC test belongs to Tae Hwan Jung.  You can visit his profile in here  https:\/\/github.com\/graykode","4fdd2215":"## Creating model class","2fa10113":"## Load pretrained BERT\n\nThe heart part of this work, we will use  from pytorch pretrained Bert to predict the Blank. \n\nIn order to use BertForMaskedLM, the blank in question stored as '___', we will convert it to [MASK]. then we try to predict which [MASK] should be for each contextual question.\n\nCurrently, BertForMaskedLM has 6 pretrain embedding for english\n```python\n   bert-base-uncased`\n. `bert-large-uncased`\n. `bert-base-cased`\n. `bert-large-cased`\n. `bert-base-multilingual-uncased`\n. `bert-base-multilingual-cased\n```\nThis kernel used bert-large-uncased, you can play with others pretrain embedding and see the results too.","4c05e524":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n# **Table of Contents**\n\n\n1.\t[BERT QUICK UNDERSTANDING](#1)\n2.  [APPLYING BERT FOR ANSWERING TOEIC READING TEST](#2)","2370bd1d":"# **1. BERT QUICK UNDERSTANDING** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Go back to table of contents](#0.1)\n\n\nBERT has the same purpose as previously pretrained embedding (Fasttext, GLove, Word2voc), Allowing computers present the texts in embedding vectors. \n\nThe first difference between BERT vs (Fasttext, GLove, Word2voc) is:  \n* (Fasttex, Glove, Word2voc) trained on 1 direction context (left to right)\n* BERT trained on Bidirectional context:  (left to right) and (right to left).\n\n![BERT.JPG](attachment:BERT.JPG)\n\n\nAs we can see,  Context-free representation such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. Contextual models instead generate a representation of each word that is based on the other words in the sentence. BERT represents a word using both its previous and next context. \n\nBut it's not enough for explanation. There's something call 'MASK' that used in this Bidirectional context for making it become BERT model\n\nThat is, In Natural Bi-directional context, the word can can see themselves.  \n\n![See%20themself.JPG](attachment:See%20themself.JPG) \n\nIn order to train a deep bidirectional model, The authors randomly [masks] 15% tokens from the input, and the target is to\npredict the original token of the masked. That's call Masked Language Model(MLM).\n* Too little masking: Too expensive to train\n* Too much masking: Not enough context\n\nWe should be aware of that: we random 15% tokens to [MASK], but not 100% of the times. In fact, we will:\n\n* 80% of the time, replace tokens with [MASK]\n* 10% of the time, replace tokens to random word\n* 10% of the time, keep same\n\nOne last techniques use in BERT pretraing is Next Sentence Prediction(NSP).\n\nNSP is used for understanding the relationship between sentences during pre-training. When we have two sentences A and B, 50% of the time B is the actual next sentence that follows A and is labeled as IsNextSentence, and 50% of the time, it is a random sentence from the corpus labeled as NotNextSentence.\n\n![nSP.JPG](attachment:nSP.JPG)\n\n\nYou can visit https:\/\/nlp.stanford.edu\/seminar\/details\/jdevlin.pdf, The author **Jacob Devlin** do awesome work on BERT explanation.","42ded2c0":"## Play with your own Toeic questions","83aaafe3":"## Test model performance"}}