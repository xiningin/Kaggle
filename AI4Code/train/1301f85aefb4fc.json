{"cell_type":{"0ef20fa8":"code","78862a17":"code","1b29abec":"code","54c3cbd6":"code","bd2d3394":"code","c8ede114":"code","1d1d9b66":"code","2798ec8d":"code","8ebe60c3":"code","98655e07":"code","5f42184a":"code","b3fdb909":"code","cc1f5478":"code","11381037":"code","9a28ef66":"code","82f24f01":"code","b21c15da":"code","fa90c516":"code","44ba9628":"code","76505b04":"code","88159adb":"code","ee68a1a4":"code","159333aa":"code","3de83457":"code","334dc15c":"code","9df52fce":"code","ab41a094":"code","ebe1763f":"code","2de6bf10":"code","18ddeac0":"code","cc8f78b6":"code","05663995":"code","08fafb76":"code","b3cd1d2a":"code","af390b88":"code","2d702ae5":"code","2fc1a728":"code","b2588e14":"code","93680eeb":"code","588801e0":"code","5c8e24ee":"code","8c6335aa":"code","e6f39d1d":"code","4cef0553":"code","c80bcae3":"code","b9cfd117":"code","c3df4000":"code","a2c10513":"code","c0dba470":"code","cdbd2813":"code","70e26b68":"code","19c6c05b":"code","e7382e1e":"code","4e20566f":"code","943cbafb":"code","e18b1de7":"code","51b25999":"code","cc998709":"code","2c9a12c3":"code","50d714e6":"code","dc9d14ef":"code","6ed41527":"code","5ed8c008":"code","589ecae2":"code","5c0a36d5":"code","8bc06f9e":"code","f40c5d19":"code","2568dbba":"code","53cedd8d":"code","c16e271b":"code","83267e25":"code","57134e05":"code","996ea1d9":"code","3e5a4dfd":"code","b3dd02c1":"code","03fa7072":"code","30665ee2":"code","d208df5c":"code","1548ec46":"code","10719c64":"code","9f6e149b":"code","2c0d4a8b":"code","772ccd13":"code","c1ef3c6d":"code","34acf55d":"code","e9cb19c8":"code","7cb8b727":"code","1a4127b2":"code","3b52f044":"code","ebc15c9f":"code","cd9115fd":"code","7d539522":"code","1d8ec592":"code","22608764":"code","d3eff6bd":"code","7e0675b8":"code","3eae69c1":"code","ee6609b1":"code","4932586b":"code","dd87a49b":"code","7543015a":"code","15a83194":"code","a243233d":"code","63400cd5":"code","2fd8d16d":"code","c94e7ff5":"code","de4f963d":"code","283bb2be":"code","dee9cdeb":"code","6aeae397":"code","4d8bfe4f":"code","50a1ce74":"code","10e21540":"code","a3657c8f":"code","c7b6ded8":"code","042b452d":"code","d950606a":"code","3ca3c349":"code","f4aa4f76":"code","3d457ad9":"code","f4ef0773":"code","b653cede":"code","424ca071":"code","e332746a":"code","85bf7f12":"code","9e1b0767":"code","df874b3a":"code","65a7e3a5":"code","cd68310c":"code","942ea29f":"code","80d4712a":"code","58fe593d":"code","ab6573b9":"code","5f9d8187":"code","d0470703":"code","4df37fa1":"markdown","7444060e":"markdown","69f030fd":"markdown","53cad40f":"markdown","570543b4":"markdown","05708c4b":"markdown","8320b5b9":"markdown","973e7f07":"markdown","eb9ff072":"markdown","1ca080ee":"markdown","60d67496":"markdown","3074a7eb":"markdown","9c22ac21":"markdown","82006477":"markdown","550287ae":"markdown","51ae9d55":"markdown","50048ab1":"markdown","cd7ffafb":"markdown","c306b24f":"markdown","97430129":"markdown","143813db":"markdown","906fb986":"markdown","39ce46c7":"markdown","2883fedb":"markdown","e0632604":"markdown","251aaf40":"markdown","1932153e":"markdown"},"source":{"0ef20fa8":"# essential libraries\nimport numpy as np \nimport pandas as pd\n# for data visulization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#for data processing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\n\n# for modeling estimators\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\n\n# for measuring performance\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\n#for tuning parameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nfrom eli5.sklearn import PermutationImportance\n\n# Misc.\nimport os\nimport time\nimport gc\nimport random\nfrom scipy.stats import uniform\nimport warnings","78862a17":"pd.options.display.max_columns = 150\n\n# Read in data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","1b29abec":"ids=test['Id']\n","54c3cbd6":"train.head()","bd2d3394":"train.shape, test.shape","c8ede114":"train.info()   ","1d1d9b66":"test","2798ec8d":"sns.countplot(\"Target\", data=train)","8ebe60c3":" sns.countplot(x=\"r4t3\",hue=\"Target\",data=train)\n","98655e07":"sns.countplot(x=\"v18q\",hue=\"Target\",data=train)","5f42184a":"sns.countplot(x=\"v18q1\",hue=\"Target\",data=train)","b3fdb909":"sns.countplot(x=\"tamhog\",hue=\"Target\",data=train)","cc1f5478":"sns.countplot(x=\"hhsize\",hue=\"Target\",data=train)","11381037":"sns.countplot(x=\"abastaguano\",hue=\"Target\",data=train)","9a28ef66":"sns.countplot(x=\"noelec\",hue=\"Target\",data=train)","82f24f01":"train.select_dtypes('object').head()","b21c15da":"\n\nyes_no_map = {'no':0,'yes':1}\ntrain['dependency'] = train['dependency'].replace(yes_no_map).astype(np.float32)\ntrain['edjefe'] = train['edjefe'].replace(yes_no_map).astype(np.float32)\ntrain['edjefa'] = train['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    ","fa90c516":"yes_no_map = {'no':0,'yes':1}\ntest['dependency'] = test['dependency'].replace(yes_no_map).astype(np.float32)\ntest['edjefe'] = test['edjefe'].replace(yes_no_map).astype(np.float32)\ntest['edjefa'] = test['edjefa'].replace(yes_no_map).astype(np.float32)","44ba9628":"train[[\"dependency\",\"edjefe\",\"edjefa\"]].describe()","76505b04":" # Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)\n","88159adb":"train['v18q1'] = train['v18q1'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)","ee68a1a4":"train['v2a1'] = train['v2a1'].fillna(0)\ntest['v2a1'] = test['v2a1'].fillna(0)","159333aa":"train['rez_esc'] = train['rez_esc'].fillna(0)\ntest['rez_esc'] = test['rez_esc'].fillna(0)\ntrain['SQBmeaned'] = train['SQBmeaned'].fillna(0)\ntest['SQBmeaned'] = test['SQBmeaned'].fillna(0)\ntrain['meaneduc'] = train['meaneduc'].fillna(0)\ntest['meaneduc'] = test['meaneduc'].fillna(0)","3de83457":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)\n\n","334dc15c":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)\n\n","9df52fce":"train.drop(['Id','idhogar'], inplace = True, axis =1)\n\ntest.drop(['Id','idhogar'], inplace = True, axis =1)","ab41a094":"train.shape","ebe1763f":"test.shape","2de6bf10":"y = train.iloc[:,140]\ny.unique()\n","18ddeac0":"X = train.iloc[:,1:141]\nX.shape\n","cc8f78b6":"my_imputer = SimpleImputer()\nX = my_imputer.fit_transform(X)\nscale = ss()\nX = scale.fit_transform(X)\n#pca = PCA(0.95)\n#X = pca.fit_transform(X)","05663995":"X.shape","08fafb76":"#subjecting the same to test data\nmy_imputer = SimpleImputer()\ntest = my_imputer.fit_transform(test)\nscale = ss()\ntest = scale.fit_transform(test)\n#pca = PCA(0.95)\n#test = pca.fit_transform(test)\n","b3cd1d2a":"X.shape, y.shape,test.shape","af390b88":"X_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.2)\n","2d702ae5":" \nmodelrf = rf()","2fc1a728":"start = time.time()\nmodelrf = modelrf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","b2588e14":"classes = modelrf.predict(X_test)","93680eeb":"(classes == y_test).sum()\/y_test.size ","588801e0":"f1 = f1_score(y_test, classes, average='macro')\nf1","5c8e24ee":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    rf(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","8c6335aa":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","e6f39d1d":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","4cef0553":"modelrfTuned=rf(criterion=\"entropy\",\n               max_depth=77,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","c80bcae3":"start = time.time()\nmodelrfTuned = modelrfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","b9cfd117":"yrf=modelrfTuned.predict(X_test)","c3df4000":"yrf","a2c10513":"yrftest=modelrfTuned.predict(test)","c0dba470":"yrftest","cdbd2813":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","70e26b68":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","19c6c05b":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","e7382e1e":"modeletf = ExtraTreesClassifier()","4e20566f":"start = time.time()\nmodeletf = modeletf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","943cbafb":"classes = modeletf.predict(X_test)\n\nclasses","e18b1de7":"(classes == y_test).sum()\/y_test.size","51b25999":"f1 = f1_score(y_test, classes, average='macro')\nf1","cc998709":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    ExtraTreesClassifier( ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {   'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n)","2c9a12c3":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","50d714e6":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","dc9d14ef":"modeletfTuned=ExtraTreesClassifier(criterion=\"entropy\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","6ed41527":"start = time.time()\nmodeletfTuned = modeletfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","5ed8c008":"yetf=modeletfTuned.predict(X_test)","589ecae2":"yetftest=modeletfTuned.predict(test)","5c0a36d5":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","8bc06f9e":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","f40c5d19":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","2568dbba":"modelneigh = KNeighborsClassifier(n_neighbors=4)","53cedd8d":"start = time.time()\nmodelneigh = modelneigh.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60\n\n","c16e271b":"classes = modelneigh.predict(X_test)\n\nclasses","83267e25":"(classes == y_test).sum()\/y_test.size ","57134e05":"f1 = f1_score(y_test, classes, average='macro')\nf1","996ea1d9":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    KNeighborsClassifier(\n       n_neighbors=4         # No need to tune this parameter value\n      ),\n    {\"metric\": [\"euclidean\", \"cityblock\"]},\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n   )","3e5a4dfd":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","b3dd02c1":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","03fa7072":"modelneighTuned = KNeighborsClassifier(n_neighbors=4,\n               metric=\"cityblock\")","30665ee2":"start = time.time()\nmodelneighTuned = modelneighTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","d208df5c":"yneigh=modelneighTuned.predict(X_test)","1548ec46":"yneightest=modelneighTuned.predict(test)","10719c64":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","9f6e149b":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","2c0d4a8b":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","772ccd13":"modelgbm=gbm()","c1ef3c6d":"start = time.time()\nmodelgbm = modelgbm.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60\n","34acf55d":"classes = modelgbm.predict(X_test)\n\nclasses","e9cb19c8":"(classes == y_test).sum()\/y_test.size ","7cb8b727":"f1 = f1_score(y_test, classes, average='macro')\nf1","1a4127b2":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    gbm(\n               # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        \n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 2                # Number of cross-validation folds\n)","3b52f044":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","ebc15c9f":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","cd9115fd":"modelgbmTuned=gbm(\n               max_depth=84,\n               max_features=11,\n               min_weight_fraction_leaf=0.04840,\n               n_estimators=489)","7d539522":"start = time.time()\nmodelgbmTuned = modelgbmTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","1d8ec592":"ygbm=modelgbmTuned.predict(X_test)","22608764":"ygbmtest=modelgbmTuned.predict(test)","d3eff6bd":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","7e0675b8":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","3eae69c1":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","ee6609b1":"modelxgb=XGBClassifier()","4932586b":"start = time.time()\nmodelxgb = modelxgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","dd87a49b":"classes = modelxgb.predict(X_test)\n\nclasses","7543015a":"(classes == y_test).sum()\/y_test.size ","15a83194":"f1 = f1_score(y_test, classes, average='macro')\nf1","a243233d":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    XGBClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","63400cd5":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","2fd8d16d":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","c94e7ff5":"modelxgbTuned=XGBClassifier(criterion=\"gini\",\n               max_depth=4,\n               max_features=15,\n               min_weight_fraction_leaf=0.05997,\n               n_estimators=499)","de4f963d":"start = time.time()\nmodelxgbTuned = modelxgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","283bb2be":"yxgb=modelxgbTuned.predict(X_test)","dee9cdeb":"yxgbtest=modelxgbTuned.predict(test)","6aeae397":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","4d8bfe4f":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","50a1ce74":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","10e21540":"modellgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)","a3657c8f":"start = time.time()\nmodellgb = modellgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","c7b6ded8":"classes = modellgb.predict(X_test)\n\nclasses","042b452d":"(classes == y_test).sum()\/y_test.size ","d950606a":"f1 = f1_score(y_test, classes, average='macro')\nf1","3ca3c349":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    lgb.LGBMClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","f4aa4f76":"\n# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","3d457ad9":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","f4ef0773":"modellgbTuned = lgb.LGBMClassifier(criterion=\"gini\",\n               max_depth=5,\n               max_features=53,\n               min_weight_fraction_leaf=0.01674,\n               n_estimators=499)","b653cede":"start = time.time()\nmodellgbTuned = modellgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","424ca071":"ylgb=modellgbTuned.predict(X_test)","e332746a":"ylgbtest=modellgbTuned.predict(test)","85bf7f12":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","9e1b0767":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","df874b3a":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","65a7e3a5":"NewTrain = pd.DataFrame()\nNewTrain['yrf'] = yrf.tolist()\nNewTrain['yetf'] = yetf.tolist()\nNewTrain['yneigh'] = yneigh.tolist()\nNewTrain['ygbm'] = ygbm.tolist()\nNewTrain['yxgb'] = yxgb.tolist()\nNewTrain['ylgb'] = ylgb.tolist()\n\nNewTrain.head(5), NewTrain.shape","cd68310c":"NewTest = pd.DataFrame()\nNewTest['yrf'] = yrftest.tolist()\nNewTest['yetf'] = yetftest.tolist()\nNewTest['yneigh'] = yneightest.tolist()\nNewTest['ygbm'] = ygbmtest.tolist()\nNewTest['yxgb'] = yxgbtest.tolist()\nNewTest['ylgb'] = ylgbtest.tolist()\nNewTest.head(5), NewTest.shape","942ea29f":"NewModel=rf(criterion=\"entropy\",\n               max_depth=77,\n               max_features=6,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","80d4712a":"start = time.time()\nNewModel = NewModel.fit(NewTrain, y_test)\nend = time.time()\n(end-start)\/60","58fe593d":"ypredict=NewModel.predict(NewTest)","ab6573b9":"ylgbtest","5f9d8187":"submit=pd.DataFrame({'Id': ids, 'Target': ylgbtest})\nsubmit.head(5)","d0470703":"submit.to_csv('submit.csv', index=False)","4df37fa1":"### Dividing the data into predictors & target","7444060e":"## Modelling with XGBClassifier","69f030fd":"### Dropping unnecesary columns","53cad40f":"## Performing tuning using Bayesian Optimization.","570543b4":"## Modelling with GradientBoostingClassifier","05708c4b":"BUILDING A NEW DATASETS WITH Predicted Values using 6 models","8320b5b9":"                                            ACCURACY           f1                ACCURACY\n                                with default parameters       score          with  parameters tuned with Bayesian                                                                                             Optimization \n        RandomForestClassifier         77.87                   65.52            85.61\n        KNeighborsClassifier           80.70                   72.03            81.85 \n        ExtraTreesClassifier           77.98                   66.47            86.97\n        GradientBoostingClassifier     80.75                   67.03            91.42 \n        XGBoost                        78.03                   61.01            91.57\n        LightGBM                       93.41                   89.43            92.05 ","973e7f07":"## Calling required libraries for the work","eb9ff072":"### Final features selected for modeling","1ca080ee":"### Accuracy improved from 71.91% to 76.20%","60d67496":"### Splitting the data into train & test ","3074a7eb":"## Explore data and perform data visualization","9c22ac21":"# Modelling","82006477":"### Scaling  numeric features & applying PCA to reduce features","550287ae":"## Performing tuning using Bayesian Optimization.","51ae9d55":"## Performing tuning using Bayesian Optimization.","50048ab1":"## Reading the data","cd7ffafb":"## Performing tuning using Bayesian Optimization.","c306b24f":"## Modelling with Light Gradient Booster","97430129":"# Costa Rican Household Poverty Level Prediction\n\nProblem and Data Explanation\nThe data for this competition is provided in two files: train.csv and test.csv. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents one individual and each column is a feature, either unique to the individual, or for the household of the individual. The training set has one additional column, Target, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty.\n\nThis is a supervised multi-class classification machine learning problem:\n\nSupervised: provided with the labels for the training data\nMulti-class classification: Labels are discrete values with 4 classes\nThe Target values represent poverty levels as follows:\n\n1 = extreme poverty \n2 = moderate poverty \n3 = vulnerable households \n4 = non vulnerable households\n\nObjectives:\nObjective of this kernel is to perform modeling with the following estimators with default parameters & get accuracy\n        \n        GradientBoostingClassifier\n        RandomForestClassifier\n        KNeighborsClassifier\n        ExtraTreesClassifier\n        XGBoost\n        LightGBM\n        \n        Then perform tuning using Bayesian Optimization & compare the accuracy of the estimators. \n        In this kerenal very simple code is used so that beginners can understand the code.\n        Core Data fields\n\nId - a unique identifier for each row.\nTarget - the target is an ordinal variable indicating groups of income levels. \n1 = extreme poverty \n2 = moderate poverty \n3 = vulnerable households \n4 = non vulnerable households\nidhogar - this is a unique identifier for each household. This can be used to create household-wide features, etc. All rows in a given household will have a matching value for this identifier.\nparentesco1 - indicates if this person is the head of the household.\nThis data contains 142 total columns.\nAll Data fields\n\n","143813db":"## Modelling with KNeighborsClassifier","906fb986":"## Performing tuning using Bayesian Optimization.","39ce46c7":"## Modelling with Random Forest","2883fedb":"## Modelling with ExtraTreeClassifier","e0632604":"## Converting categorical objects into numericals ","251aaf40":"### Fill in missing values (NULL values)  using 1 for yes and 0 for no","1932153e":"## Performing tuning using Bayesian Optimization."}}