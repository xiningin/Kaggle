{"cell_type":{"64f1d928":"code","1dd6b26b":"code","9d2cc242":"code","755c2ce9":"code","e4965421":"code","e6df6cd5":"code","2e538a00":"code","fd692587":"code","6ef5fb33":"code","464d8385":"code","bfd8d705":"code","134c7047":"code","20984e1f":"code","286715a3":"code","e2505676":"code","9441b83e":"code","40360405":"code","78cee5c9":"code","a8172de7":"code","52bbbff7":"code","cf4921fb":"code","df332934":"code","6402ffa6":"code","a5ff48b9":"code","a48200bf":"code","2a0d7c86":"code","64c5628e":"code","3b7a4500":"code","04027f4e":"code","10199e18":"code","989521b6":"code","337292e2":"code","e530a360":"code","b3a16f17":"code","c48d2671":"code","f7316a74":"code","66fc9765":"code","7e9cf40b":"code","3f924fb4":"code","4ce9113b":"code","e90645c3":"code","44244d4f":"markdown","514e8aea":"markdown","b953cd41":"markdown","e69c141e":"markdown","3624534e":"markdown","0d9d9590":"markdown","070a28a8":"markdown","9f24ada5":"markdown","24f3f8f3":"markdown","0703d2da":"markdown","eb4451fb":"markdown","6d5e19aa":"markdown","6c3f3682":"markdown","c1ee3c55":"markdown","22acab8a":"markdown"},"source":{"64f1d928":"import pandas as pd\nimport numpy as np\nimport seaborn as sns","1dd6b26b":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ndf","9d2cc242":"df.isnull().sum()","755c2ce9":"df[df['Embarked'].isnull()]","e4965421":"#Adding a new column in df. Writing 1 whereever the data is missing otherwise 0\ndf['cabin_null_val'] = np.where(df['Cabin'].isnull(),1,0)","e6df6cd5":"df.head()","2e538a00":"#Checking the percentage relation of Survived column and cabin_null_val column\ndf.groupby(['Survived'])['cabin_null_val'].mean()","fd692587":"#The results say, there are higher percentage of people who died(0) have cabin record as null\n#Hence the relation is proved","6ef5fb33":"#Reading the dataset (Only specific columns for the simplcity)\ndf = pd.read_csv('..\/input\/titanic\/train.csv',usecols = ['Survived', 'Age', 'Fare'])\ndf.head()","464d8385":"#Check the percentage of null values\ndf.isnull().mean()","bfd8d705":"#Creating median of Age column\nmedian = df.Age.median()\nmedian","134c7047":"#Creating a function to replace all null values with the median\ndef median_col(df,variable,median):\n    df[variable+'_median'] = df[variable].fillna(median)","20984e1f":"#Calling the function and checking the dataframe again\nmedian_col(df,'Age',median)\ndf","286715a3":"#Comparing the two columns (Null values and filled values)\ndf[df['Age'].isnull()]","e2505676":"print(df['Age'].std())\nprint(df['Age_median'].std())","9441b83e":"#The Standard deviation (STD) does not have much of a difference","40360405":"import matplotlib.pyplot as plt\n%matplotlib inline","78cee5c9":"fig = plt.figure()\nax = fig.add_subplot(111)\ndf.Age.plot(kind = 'kde',ax=ax)\ndf.Age_median.plot(kind = 'kde',ax=ax, color = 'red')\nlines,labels = ax.get_legend_handles_labels()\nax.legend(lines,labels,loc = 'best')","a8172de7":"#As we can observe, due to the NaN values, the Age_median graph's density has been increased due to the median values","52bbbff7":"#Reading the dataset (Only specific columns for the simplcity)\ndf = pd.read_csv('..\/input\/titanic\/train.csv',usecols = ['Survived', 'Age', 'Fare'])\ndf.head()","cf4921fb":"#Checking null values\ndf.isnull().sum()","df332934":"def null_impute(df,variable):\n    df[variable+'_random']=df[variable]\n    #Fill the NA values with random sample\n    n = df[variable].isnull().sum()\n    random_sample = df[variable].dropna().sample(n,random_state = 0)\n    #Pandas need to have same index values to merge the data\n    random_sample.index = df[df[variable].isnull()].index\n    #Add the values to the variable_sample column\n    df.loc[df[variable].isnull(),variable+'_random'] = random_sample    ","6402ffa6":"#Calling the function\nnull_impute(df,'Age')\ndf.head()","a5ff48b9":"print(df['Age'].std())\nprint(df['Age_random'].std())","a48200bf":"#The Std is almost the same and better than the first method","2a0d7c86":"fig = plt.figure()\nax = fig.add_subplot(111)\ndf.Age.plot(kind = 'kde',ax=ax)\ndf.Age_random.plot(kind = 'kde',ax=ax, color = 'red')\nlines,labels = ax.get_legend_handles_labels()\nax.legend(lines,labels,loc = 'best')","64c5628e":"#It can be seen that the graph is almost the same","3b7a4500":"#Reading the dataset (Only specific columns for the simplcity)\ndf = pd.read_csv('..\/input\/titanic\/train.csv',usecols = ['Survived', 'Age', 'Fare'])\ndf.head()","04027f4e":"#Creating a new column and replacing it with 1 wherever I Null value is present else 0\ndf['Age_null'] = np.where(df.Age.isnull(),1,0)\ndf.head()","10199e18":"#Reading the dataset (Only specific columns for the simplcity)\ndf = pd.read_csv('..\/input\/titanic\/train.csv',usecols = ['Survived', 'Age', 'Fare'])\ndf.head()","989521b6":"#Plotting a histogram to understand the data.\n#We will be taking the end data (around 70+ from the X-axis) and relacing the nan values with it\ndf['Age'].plot(kind = 'hist', bins = 50)","337292e2":"#Checking the outliers before implementing. Outliers will be found\nsns.boxplot(data = df['Age'])","e530a360":"#Getting the mean of Data from the 3rd Standard deviation (Extreme end)\nextreme = df.Age.mean()+3*df.Age.std()","b3a16f17":"#Creating a function to replace the null values with extreme variable\ndef age_distribution_end(df,extreme,variable):\n    df[variable+'_end'] = df[variable].fillna(extreme)","c48d2671":"#Calling the function and checking the dataset\nage_distribution_end(df,extreme,'Age')\ndf.head()","f7316a74":"#Checking the boxplot again. Observe that the outliers are completely gone\nsns.boxplot(data = df['Age_end'])","66fc9765":"#Reading the dataset (Only specific columns for the simplcity)\ndf = pd.read_csv('..\/input\/titanic\/train.csv',usecols = ['Survived', 'Age', 'Fare'])\ndf.head()","7e9cf40b":"#Replacing the NaN values with 0 and 100\ndef impute_nan(df, variable):\n    df[variable+'_zero'] = df[variable].fillna(0)\n    df[variable+'_hundred'] = df[variable].fillna(100)","3f924fb4":"#Calling the function and checking the dataset\nimpute_nan(df, 'Age')\ndf.head()","4ce9113b":"df['Age_zero'].hist(bins=50)","e90645c3":"df['Age_hundred'].hist(bins=50)","44244d4f":"## 4. End of Distribution imputation\n\nIf there is suspicion that the missing value is not at random then capturing that information is important. In this scenario, one would want to replace missing data with values that are at the tails of the distribution of the variable.","514e8aea":"## 3. Capturing NaN values with new Features\n### When does it work?\n#### It works well when the data is not completely at random","b953cd41":"### Advantages and Disadvantages\n#### Advantages\n1. Easy to implement\n3. Capture the importance of missing values\n\n#### Disadvantage \n2. Creating additional feature (Curse of Dimentionality)","e69c141e":"## 1. Missing Completely at Random (MCAR)\n#### It refers to the values which have absolutely no relation with the data missing and any other values\n#### For eg. Embarked","3624534e":"## Different types of Missing Values and ways to handle them","0d9d9590":"### Advantages and Disadvantages\n#### Advantages\n1. Easy to implement\n3. Less Change or distortion in the original variance (Observe the graph)\n\n#### Disadvantage \n2. In every situation randomness won't work","070a28a8":"## 1. Mean Median Mode\n#### When should we apply?\n##### Mean Median Mode has the asumption that the data is missing completely at random (MCAR)\n##### We solve this by replacing the NaN values with the most frequent occurings of the variables","9f24ada5":"### Advantages and Disadvantages of End of Distribution Imputation\n#### Advantages\n1. Can bring out the importance of missing values\n\n#### Disadvantages\n1. Changes Co-variance\/variance\n2. May create biased data","24f3f8f3":"## Arbitrary value Imputation\n\nThis technique was derived from kaggle competition It consists of replacing NAN by an arbitrary value. In this technique, all the nan values are replaced by any one value which is decided by the data scientist","0703d2da":"# Techniques of handling the missing values\n### 1. Mean\/Median\/Mode\n### 2. Random Sample Imputation\n### 3. Capturing NaN values with new Features\n### 4. End of Distribution Imputation\n### 5. Arbitrary Imputation","eb4451fb":"## 1. Missing Completely Not at Random (MCNR)\n#### It refers to the values which have absolutely some relation with the data missing and any other values\n#### For eg. Cabin","6d5e19aa":"### Advantages and Disadvantages\n#### Advantages\n1. Easy to implement\n2. Faster to get a complete dataset\n\n#### Disadvantage \n1. Change or distortion in the original variance (Observe the graph)\n2. Impacts corelation","6c3f3682":"## Missing at Random (MAR)\n#### Example : Men hide their salary\n#### Explanation : So in the sex column, whenever there will be men the chances are high that the salary column will have null values","c1ee3c55":"### Advantages and Disadvantages of Arbitrary value Imputation\n#### Advantages\n1. Easy to implement\n2. Captures the importance of missingess if there is one\n\n#### Disadvantages\n1. Distorts the original distribution of the variable\n2. If missingess is not important, it may mask the predictive power of the original variable by distorting its distribution\n3. Hard to decide which value to use","22acab8a":"## 2. Random Sample Imputation\n#### When should we apply?\n##### Random Sample Imputation has the asumption that the data is missing completely at random (MCAR)\n##### It consists of replacing the random values from the dataset and we use this observation to replace the NaN values"}}