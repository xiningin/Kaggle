{"cell_type":{"814774d3":"code","4357fb03":"code","f382cb86":"code","c3056b15":"code","3ca706dd":"code","7376f702":"code","8ca1400b":"code","e91e9c22":"code","d89d90e8":"code","a63b17e5":"code","04fdddc1":"code","c24ebef6":"code","9835f12b":"code","39270e04":"code","7afe80d6":"code","ff857136":"code","4bf67fe7":"code","aaf41cb8":"code","6dc1fd03":"code","3ec68e90":"code","27f09150":"code","83b906d0":"code","a3cc2dac":"code","a05105fa":"code","d7a226f1":"code","5201e8ce":"code","b59b164c":"code","ca423d50":"code","33d9222b":"code","7f7f4944":"code","da5ee0e4":"code","f6506469":"code","5d7f38a2":"code","be800f93":"code","5906204b":"code","d2894652":"markdown","2b7d146a":"markdown","c42450ec":"markdown","48080714":"markdown","8c26a6c5":"markdown","d4c6a7ac":"markdown","858ec769":"markdown","afc1bf9f":"markdown","5539857e":"markdown","39e5302b":"markdown","ccebcad2":"markdown","c973ecb3":"markdown","60628d26":"markdown","5e1b75b7":"markdown","26df62ee":"markdown","c519d909":"markdown","104dd262":"markdown","6583b334":"markdown"},"source":{"814774d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.utils.multiclass import unique_labels\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4357fb03":"#Import standard libraries\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n''' to learn more about itertools visit\n    https:\/\/medium.com\/@jasonrigden\/a-guide-to-python-itertools-82e5a306cdf8'''\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","f382cb86":"#Import keras functions\n\nfrom keras import Sequential\n\n'''Since we are using transfer learning let's import the model that we want to implement.Let's use VGG 19(19 layers) and Resnet-50 (50 layers of residual units). \nResidual units allow us to add more layers onto the model without a degradation in accuracy.\nLet's try and compare the accuracy of the 2 models and see if the addtional layers do make a significant difference. '''\n\nfrom keras.applications import VGG19,ResNet50\n\n'Import the datagenerator to augment images'\nfrom keras.preprocessing.image import ImageDataGenerator\n\n'''Import the optimizers and leanring rate annealer (which will reduce the learning rate once a particular metric we choose(in this case validation error) \ndoes not reduce after a user defined number of epochs)'''\nfrom keras.optimizers import SGD,Adam\nfrom keras.callbacks import ReduceLROnPlateau\n\n'Lastly import the final layers that will be added on top of the base model'\nfrom keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n\n'Import to_categorical from the keras utils package to one hot encode the labels'\nfrom keras.utils import to_categorical","c3056b15":"#Import dataset\nfrom keras.datasets import cifar10","3ca706dd":"#Divide the data in Train, Validation and Test Datasets\n'I had to turn the Internet setting to on to download load the dataset'\n(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n\n","7376f702":"x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)","8ca1400b":"#Print the dimensions of the datasets to make sure everything's kosher\n\nprint((x_train.shape,y_train.shape))\nprint((x_val.shape,y_val.shape))\nprint((x_test.shape,y_test.shape))","e91e9c22":"#One hot encode the labels.Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10\n\ny_train=to_categorical(y_train)\ny_val=to_categorical(y_val)\ny_test=to_categorical(y_test)\n","d89d90e8":"# Lets print the dimensions one more time to see if things changed the way we expected\n\nprint((x_train.shape,y_train.shape))\nprint((x_val.shape,y_val.shape))\nprint((x_test.shape,y_test.shape))","a63b17e5":"#Data Augmentation Function: Let's define an instance of the ImageDataGenerator class and set the parameters.We have to instantiate for the Train,Validation and Test datasets\ntrain_generator = ImageDataGenerator(\n                                    rotation_range=2, \n                                    horizontal_flip=True,\n                                    zoom_range=.1 )\n\nval_generator = ImageDataGenerator(\n                                    rotation_range=2, \n                                    horizontal_flip=True,\n                                    zoom_range=.1)\n\ntest_generator = ImageDataGenerator(\n                                    rotation_range=2, \n                                    horizontal_flip= True,\n                                    zoom_range=.1) \n\n\n","04fdddc1":"#Fit the augmentation method to the data\n\ntrain_generator.fit(x_train)\nval_generator.fit(x_val)\ntest_generator.fit(x_test)","c24ebef6":"'''Learning Rate Annealer: The learning rate can be modified after a set number of epochs or after a certain condition is met. We will use the latter and change the learning rate if \nthe validation error does not reduce after a set number of epochs. To do this we will use the patience parameter.'''\n\nlrr= ReduceLROnPlateau(\n                       monitor='val_acc', #Metric to be measured\n                       factor=.01, #Factor by which learning rate will be reduced\n                       patience=3,  #No. of epochs after which if there is no improvement in the val_acc, the learning rate is reduced\n                       min_lr=1e-5) #The minimum learning rate ","9835f12b":"#Build the model\n\n'The first base model used is VGG19. The pretrained weights from the imagenet challenge are used'\nbase_model_1 = VGG19(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])\n\n'For the 2nd base model we will use Resnet 50 and compare the performance against the previous one.The hypothesis is that Resnet 50 should perform better because of its deeper architecture'\nbase_model_2 = ResNet50(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])","39270e04":"#Lets add the final layers to these base models where the actual classification is done in the dense layers\n\nmodel_1= Sequential()\nmodel_1.add(base_model_1) #Adds the base model (in this case vgg19 to model_1)\nmodel_1.add(Flatten()) #Since the output before the flatten layer is a matrix we have to use this function to get a vector of the form nX1 to feed it into the fully connected layers\n","7afe80d6":"model_1.summary()","ff857136":"#Add the Dense layers along with activation and batch normalization\nmodel_1.add(Dense(1024,activation=('relu'),input_dim=512))\nmodel_1.add(Dense(512,activation=('relu'))) \nmodel_1.add(Dense(256,activation=('relu'))) \n#model_1.add(Dropout(.3))#Adding a dropout layer that will randomly drop 30% of the weights\nmodel_1.add(Dense(128,activation=('relu')))\n#model_1.add(Dropout(.2))\nmodel_1.add(Dense(10,activation=('softmax'))) #This is the classification layer\n","4bf67fe7":"#Check final model summary\nmodel_1.summary()","aaf41cb8":"batch_size= 100\nepochs=50","6dc1fd03":"learn_rate=.001\n\nsgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\nadam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","3ec68e90":"#Compile the model\n#During model compiling the 3 main things we specify are loss function,optimizer and the metrics that need to be evaluated during the test and train processes.\n#Lets start by using the SGD optimizer\n#We will specify the loss as categoricl crossentropy since the labels are 1 hot encoded. IF we had integer labels,we'd have to use sparse categorical crossentropy as loss function.\nmodel_1.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n","27f09150":"model_1.fit_generator(train_generator.flow(x_train,y_train,batch_size=batch_size),\n                      epochs=epochs,\n                      steps_per_epoch=x_train.shape[0]\/\/batch_size,\n                      validation_data=val_generator.flow(x_val,y_val,batch_size=batch_size),validation_steps=250,\n                      callbacks=[lrr],verbose=1)","83b906d0":"#Plot the training and valiation loss\n'''The output of model.fit is a model.History object which is a record of metrics at each epoch. This can be used to graph the training and validation accuracy\nto see where they plateaued off and if overfitting can subsequently be avoided'''\n\nf,ax=plt.subplots(2,1) #Creates 2 subplots under 1 column\n\n#Assign the first subplot to graph training loss and validation loss\nax[0].plot(model_1.history.history['loss'],color='b',label='Training Loss')\nax[0].plot(model_1.history.history['val_loss'],color='r',label='Validation Loss')\n\n#Next lets plot the training accuracy and validation accuracy\nax[1].plot(model_1.history.history['acc'],color='b',label='Training  Accuracy')\nax[1].plot(model_1.history.history['val_acc'],color='r',label='Validation Accuracy')","a3cc2dac":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n#     print(cm)\n\n    fig, ax = plt.subplots(figsize=(7,7))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n\nnp.set_printoptions(precision=2)","a05105fa":"y_pred=model_1.predict_classes(x_test)\ny_true=np.argmax(y_test,axis=1)\n\n#Compute the confusion matrix\nconfusion_mtx=confusion_matrix(y_true,y_pred)\n","d7a226f1":"class_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']","5201e8ce":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_true, y_pred, classes=class_names,\n                      title='Confusion matrix, without normalization')\n","b59b164c":"# Plot normalized confusion matrix\nplot_confusion_matrix(y_true, y_pred, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n# plt.show()","ca423d50":"#Since we have already defined Resnet50 as base_model_2, let us build the sequential model.\n\nmodel_2=Sequential()\n#Add the Dense layers along with activation and batch normalization\nmodel_2.add(base_model_2)\nmodel_2.add(Flatten())\n\n\n#Add the Dense layers along with activation and batch normalization\nmodel_2.add(Dense(4000,activation=('relu'),input_dim=512))\nmodel_2.add(Dense(2000,activation=('relu'))) \nmodel_2.add(Dropout(.4))\nmodel_2.add(Dense(1000,activation=('relu'))) \nmodel_2.add(Dropout(.3))#Adding a dropout layer that will randomly drop 30% of the weights\nmodel_2.add(Dense(500,activation=('relu')))\nmodel_2.add(Dropout(.2))\nmodel_2.add(Dense(10,activation=('softmax'))) #This is the classification layer","33d9222b":"model_2.summary()","7f7f4944":"#Compile the model \n\nmodel_2.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])","da5ee0e4":"model_2.fit_generator(train_generator.flow(x_train,y_train,batch_size=batch_size),\n                     epochs=100,steps_per_epoch=x_train.shape[0]\/\/batch_size,\n                     validation_data=val_generator.flow(x_val,y_val,batch_size=batch_size),validation_steps=250,\n                      callbacks=[lrr],verbose=1)","f6506469":"f,ax=plt.subplots(2,1) #Creates 2 subplots under 1 column\n\n#Assign the first subplot to graph training loss and validation loss\nax[0].plot(model_2.history.history['loss'],color='b',label='Training Loss')\nax[0].plot(model_2.history.history['val_loss'],color='r',label='Validation Loss')\n\n#Next lets plot the training accuracy and validation accuracy\nax[1].plot(model_2.history.history['acc'],color='b',label='Training  Accuracy')\nax[1].plot(model_2.history.history['val_acc'],color='r',label='Validation Accuracy')","5d7f38a2":"y_pred_resnet=model_2.predict_classes(x_test)\ny_true=np.argmax(y_test,axis=1)\n\n#Compute the confusion matrix\nconfusion_mtx=confusion_matrix(y_true,y_pred_resnet)","be800f93":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_true, y_pred_resnet, classes=class_names,\n                      title='Confusion matrix, without normalization')","5906204b":"# Plot normalized confusion matrix\nplot_confusion_matrix(y_true, y_pred_resnet, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n# plt.show()","d2894652":"We can now begin the actual process of model building.I find that following a set process and following consistently makes learning this easier.So here is the process I follow:\n\n*  Define the Data Augmentation (ImageDataGenerator) and Learning Rate Annealer (ReduceOnPlateau) functions\n*  Build the model (Base Model + Flatten + Dense)\n*  Check model summary\n*  Initialize Batch Size,Number of Epochs\n*  Compile model\n*  Fit the model (We will use fit_generator since the data is fed to the model using an augmentation function\n*  Evaluate the model on test data\n","2b7d146a":"1. Let's now begin to evaluate our model.","c42450ec":"The next step is to define the learning rate for the optimizer we will use. I have chosen the SGD and Adam optimizer. The main difference between the 2 is that SGD uses the same learning rate for all parameters and updates all of them by the same amount. The learning rate does not change during training. Adam stands for Adaptive Moment estimation and maintains a separate learning rate for each parameter and updates them separately.\n\nTo understand this concept more please read this article:\nhttps:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/","48080714":"Now comes the fun part: Model Training. Since we're using a function to generate data, we have to use the argument fit_generator. Both the train data and the validation data will be generated using the augmentation methods we have previously defined. To use the fit_generator function we will define the following parameters:\n\ngenerator.flow(x_train,y_train,batch_size)\n\nHere we use generator.flow since the data is being generated from a numpy array. You could also have data available in folders in which case we would use flow_from_directory in which case the class names are inferred directly from the folder names within the train data folder\n\nMore information on this can be found by reading the official documentation:\nhttps:\/\/keras.io\/preprocessing\/image\/\n\n","8c26a6c5":"Now that we have our code for the confusion matrix, let's make predictions on the test set and see how this model has performed","d4c6a7ac":"To compare this with another model, let's try and use Resnet-50. Residual nets allow us to add deeper layers to the network, without having the problem of accuracy degradation. They do this by using skip connections, meaning they jump over layers.\n\nTo understand Resnets better please read the following links:\n\nhttps:\/\/towardsdatascience.com\/hitchhikers-guide-to-residual-networks-resnet-in-keras-385ec01ec8ff\n\nhttps:\/\/towardsdatascience.com\/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec","858ec769":"This is the official Keras documentation page on Optimizers.Since the choice o optimizer has a lot of influence on model performance, I'd reccomend giving it a read\n\n* https:\/\/keras.io\/optimizers\/","afc1bf9f":"This model starts overfitting at around 80 epochs where we find that the validation accuracy stays the same, however the training accuracy keeps increasing. The validation accuracy at this point is around 74%.Compared to the previous model, this model's performance is not that good. At the point of overfitting the previous model had a validation accuracy of 78%. IF this hypothesis is true, then we should see the result in the confusion matrix. So let us plot those now ","5539857e":"Analysis Of Results: \nLooking at the Normalized Confusion Matrix, we see that overall, the model has performed well, except in the case of cats. The model seems to misclassify cats and dogs with each other.","39e5302b":"Before we add the dense layers we must know the shape of the output of the flatten layer so we can define the input shape to the dense layer accordingly. The way I handle is I print the summary of the model after the flatten layer is added. ","ccebcad2":"So after the flatten layer we see the output to be a vector of (512,1).This will help in deciding the number of neurons in the dense layer following the flatten layer.","c973ecb3":"After checking the model summary, let's initialize the batchsize and number of epochs.You can leave the batch unspecified and it defaults to 32. Batch size is the number of samples over which the gradient is calculated. An epoch is just the number of times the entire dateset is passed through the network.\n\nAs a rule of thumb, as the batch size increases, the accuracy over each epoch SHOULD increase faster than if the batch size were smaller. This makes sense since we are training the model over more samples over each step. Therefore the number of epochs required to converge to a solution SHOULD THEORETICALLY be less. \n\nA lot of selection parameters have to do with the size and shape of training data. Since our training data set has 35K samples, we can choose a larger batch size.","60628d26":"The validation accuracy has stabalized at around 75%. Let's plot the Training Vs Validation losses and accuracy to see if overfitting has occured.\n","5e1b75b7":"From the figures it is evident that we could have run the model to 4 or 5 epochs. ","26df62ee":"The next step in model evaluation is building the confusion matrix. The following code is from the official documentation of the sklearn library. \nIt allows us to print a confusion matrix with labels and gives a choice of normalization. Id prefer to normalize since it'll give you the true positive percentage upfront\nwhich is far better than absolute numbers if we are going to comapre models\n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n","c519d909":"The comparison of the normalized matrices clearly shows that the First model (VGG 19) works better for this dataset. This could probably be because the images are only 32X32 pixels. For a resolution this low,perhaps the resnet model is overkill. To make sure, feel free to download the notebook and play with the hyperparameters. Good luck and happy learning!","104dd262":"Immediately the first differences we see are that ResNet50 has a little more than 16 million extra parameters to train which is to be expected since it is a deeper model. Also, the number of units before the Flatten layer are 4 times that of the previous model.","6583b334":"It's not necessary to use a data generator for validation data while fitting the model, however I find that it gives better validation accuracy if it is used.This is probably because themodel can learn more generalized features if validation data is also augmented.\nIve used only a few of the many available functionalities of the augment function.To know more about the capabilities read this article:\nhttps:\/\/towardsdatascience.com\/image-augmentation-for-deep-learning-using-keras-and-histogram-equalization-9329f6ae5085\n"}}