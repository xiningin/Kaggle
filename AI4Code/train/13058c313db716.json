{"cell_type":{"0b71e80e":"code","d1c7cb8f":"code","fa8b2a4a":"code","c71178ad":"code","e819a110":"code","3138be67":"code","0fbcc6cb":"markdown","8d6dc912":"markdown","5587ac56":"markdown"},"source":{"0b71e80e":"from fastai.core import Path, progress_bar\nfrom fastai.vision import imagenet_stats, Image\nimport cv2\nimport torch\nfrom torchvision import transforms\nimport PIL\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True","d1c7cb8f":"class capsSet():\n    def __init__(self, videos_path, bs=64):\n        \n        self.videos_path = videos_path\n        self.video_paths = videos_path.ls()\n        self.videos_num = len(self.video_paths)\n        \n        self.bs = bs\n        self.caps = [cv2.VideoCapture() for i in range(bs)]\n        \n        self.video_idx = 0\n        self.steps_taken = 0\n        \n        self.tfms = transforms.Compose([transforms.Resize((144,256), interpolation=2), transforms.ToTensor(), transforms.Normalize(*imagenet_stats)])\n        \n        self.frames_count = None\n        \n    def get_batch(self):\n        frames_batch = [self.get_frame(i) for i in range(self.bs)]\n        self.steps_taken += 1\n        return torch.stack(frames_batch)\n        \n    def get_frame(self, cap_id):\n        cap = self.caps[cap_id]\n        res, frame_bgr = cap.read()\n        if not res:\n            cap.open(str(self.video_paths[self.video_idx]))\n            self.video_idx = (self.video_idx+1) if self.video_idx<(self.videos_num-1) else (0)\n            frame_tensor = self.get_frame(cap_id)\n        else:\n            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n            frame_pil = PIL.Image.fromarray(frame_rgb)\n            frame_tensor = self.tfms(frame_pil)\n        return frame_tensor\n    \n    def get_frames_count(self, silent=False):\n        if self.frames_count is None:\n            n_frames = 0\n            cap = cv2.VideoCapture()\n            for video_path in progress_bar(self.video_paths, display=(not silent)):\n                cap.open(str(video_path))\n                n_frames += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            self.frames_count = n_frames\n        return self.frames_count","fa8b2a4a":"videos_path = Path('')#\/media\/nofreewill\/Datasets_nvme\/Visual\/Videos\/BestSet\/Videos\/Raw') # I have all the videos in here\ncapsset = capsSet(videos_path, bs=64)\n\nprint(capsset.videos_num)","c71178ad":"# for i in range(1):\n#     images = capsset.get_batch()\n# print(images.shape)","e819a110":"# img = images[0]\n# Image((img - img.min())\/(img.max()-img.min()+1e-6))","3138be67":"# capsset.get_frames_count()","0fbcc6cb":"https:\/\/gist.github.com\/nofreewill42\/1ab604a463561b118702d39a51bbb623","8d6dc912":"See \"more\" in gist.","5587ac56":"With having a bunch of videos in a folder, you can iterate through them with this class.\n\nIt creates batch size number of cv2 VideoCaptures and opens the first batch size number of videos, reading their next frames when you call get_batch().\nIf you reach the end of a video, it takes the next one that isn't open in the other VideoCaptures and opens that and reads that.\n\nI have downloaded 1908 videos from youtube with resolution of 144x256 and I have now 85M images in these videos taking only 32GBs of space.\nThe potential is that in consecutive frames, there are most likely the same objects present, with a slightly different position, orientation etc.\nRunning SimCLR this way could potentially improve results.\n\nI post it here so that you hopefully can use this class in this competition."}}