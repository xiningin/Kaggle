{"cell_type":{"785113c7":"code","cca30929":"code","d2bfbe9c":"code","91b22e3a":"code","e0a225d4":"code","dc41615f":"code","86bd5f04":"code","fee7ba15":"code","2874df0c":"code","71bfc85a":"code","799242ac":"code","0b21bcdd":"code","c949a495":"code","c98f1f9f":"code","21700839":"code","b617ddac":"code","c2cbb65d":"code","5efc3b7b":"code","6110474b":"code","ce10ca98":"code","0a1001bd":"code","4e11ecdb":"code","f197291e":"code","8b6e59b4":"code","93b5a9e8":"code","712466af":"markdown","b4551a3b":"markdown","0680152e":"markdown","e0d9944a":"markdown","f9d7e586":"markdown","b43738a3":"markdown","d4a96c4b":"markdown","c6152f51":"markdown","1f198365":"markdown","3887423e":"markdown","9e600b27":"markdown","1a69dc94":"markdown","7c190ec5":"markdown","02d9f1d9":"markdown","2492708e":"markdown","c8420009":"markdown","45966bf8":"markdown","a3470680":"markdown","beec8ec4":"markdown","1b8813e9":"markdown","1b926e1a":"markdown","40688b11":"markdown","fd88a5f6":"markdown","13007dfa":"markdown","e78aaef6":"markdown","1cb1865d":"markdown"},"source":{"785113c7":"# ! pip3 install transformers","cca30929":"import pandas as pd\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport copy\nfrom tqdm.notebook import tqdm\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import (\n    accuracy_score, \n    f1_score, \n    classification_report\n)\n\nfrom transformers import (\n    AutoTokenizer, \n    AutoModel,\n    get_linear_schedule_with_warmup\n)\n\nproject_dir = '..\/input\/avjanatahackresearcharticlesmlc\/av_janatahack_data\/'","d2bfbe9c":"! nvidia-smi","91b22e3a":"train_df = pd.read_csv(project_dir + 'train.csv')\ntrain_df.head()","e0a225d4":"# preprocessing\ndef clean_text(text):\n    text = text.split()\n    text = [x.strip() for x in text]\n    text = [x.replace('\\n', ' ').replace('\\t', ' ') for x in text]\n    text = ' '.join(text)\n    text = re.sub('([.,!?()])', r' \\1 ', text)\n    return text\n    \n\ndef get_texts(df):\n    titles = df['TITLE'].apply(clean_text)\n    titles = titles.values.tolist()\n    abstracts = df['ABSTRACT'].apply(clean_text)\n    abstracts = abstracts.values.tolist()\n    return titles, abstracts\n\n\ndef get_labels(df):\n    labels = df.iloc[:, 3:].values\n    return labels\n\ntitles, abstracts = get_texts(train_df)\nlabels = get_labels(train_df)\n\nfor t, a, l in zip(titles[:5], abstracts[:5], labels[:5]):\n    print(f'TITLE -\\t{t}')\n    print(f'ABSTRACT -\\t{a}')\n    print(f'LABEL -\\t{l}')\n    print('_' * 80)\n    print()","dc41615f":"# no. of samples for each class\ncategories = train_df.columns.to_list()[3:]\nplt.figure(figsize=(6, 4))\n\nax = sns.barplot(categories, train_df.iloc[:, 3:].sum().values)\nplt.ylabel('Number of papers')\nplt.xlabel('Paper type ')\nplt.xticks(rotation=90)\nplt.show()","86bd5f04":"# no of samples having multiple labels\nrow_sums = train_df.iloc[:, 3:].sum(axis=1)\nmultilabel_counts = row_sums.value_counts()\n\nplt.figure(figsize=(6, 4))\nax = sns.barplot(multilabel_counts.index, multilabel_counts.values)\nplt.ylabel('Number of papers')\nplt.xlabel('Number of labels')\nplt.show()","fee7ba15":"# title lengths\ny = [len(t.split()) for t in titles]\nx = range(0, len(y))\nplt.bar(x, y)","2874df0c":"# abstracts lengths\ny = [len(t.split()) for t in abstracts]\nx = range(0, len(y))\nplt.bar(x, y)","71bfc85a":"class Config:\n    def __init__(self):\n        super(Config, self).__init__()\n\n        self.SEED = 42\n        self.MODEL_PATH = 'allenai\/scibert_scivocab_uncased'\n        self.NUM_LABELS = 6\n\n        # data\n        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n        self.MAX_LENGTH1 = 20\n        self.MAX_LENGTH2 = 320\n        self.BATCH_SIZE = 5\n        self.VALIDATION_SPLIT = 0.25\n\n        # model\n        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.FULL_FINETUNING = True\n        self.LR = 3e-5\n        self.OPTIMIZER = 'AdamW'\n        self.CRITERION = 'BCEWithLogitsLoss'\n        self.SAVE_BEST_ONLY = True\n        self.N_VALIDATE_DUR_TRAIN = 3\n        self.EPOCHS = 1\n\nconfig = Config()","799242ac":"class TransformerDataset(Dataset):\n    def __init__(self, df, indices, set_type=None):\n        super(TransformerDataset, self).__init__()\n\n        df = df.iloc[indices]\n        self.titles, self.abstracts = get_texts(df)\n        self.set_type = set_type\n        if self.set_type != 'test':\n            self.labels = get_labels(df)\n\n        self.max_length1 = config.MAX_LENGTH1\n        self.max_length2 = config.MAX_LENGTH2\n        self.tokenizer = config.TOKENIZER\n\n    def __len__(self):\n        return len(self.titles)\n    \n    def __getitem__(self, index):\n        tokenized_titles = self.tokenizer.encode_plus(\n            self.titles[index], \n            max_length=self.max_length1,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids_titles = tokenized_titles['input_ids'].squeeze()\n        attention_mask_titles = tokenized_titles['attention_mask'].squeeze()\n        \n        tokenized_abstracts = self.tokenizer.encode_plus(\n            self.abstracts[index], \n            max_length=self.max_length2,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids_abstracts = tokenized_abstracts['input_ids'].squeeze()\n        attention_mask_abstracts = tokenized_abstracts['attention_mask'].squeeze()\n\n        if self.set_type != 'test':\n            return {\n                'titles': {\n                    'input_ids': input_ids_titles.long(),\n                    'attention_mask': attention_mask_titles.long(),\n                },\n                'abstracts': {\n                    'input_ids': input_ids_abstracts.long(),\n                    'attention_mask': attention_mask_abstracts.long(),\n                },\n                'labels': torch.Tensor(self.labels[index]).float(),\n            }\n\n        return {\n            'titles': {\n                'input_ids': input_ids_titles.long(),\n                'attention_mask': attention_mask_titles.long(),\n            },\n            'abstracts': {\n                'input_ids': input_ids_abstracts.long(),\n                'attention_mask': attention_mask_abstracts.long(),\n            }\n        }","0b21bcdd":"# train-val split\n\nnp.random.seed(config.SEED)\n\ndataset_size = len(train_df)\nindices = list(range(dataset_size))\nsplit = int(np.floor(config.VALIDATION_SPLIT * dataset_size))\nnp.random.shuffle(indices)\n\ntrain_indices, val_indices = indices[split:], indices[:split]","c949a495":"train_data = TransformerDataset(train_df, train_indices)\nval_data = TransformerDataset(train_df, val_indices)\n\ntrain_dataloader = DataLoader(train_data, batch_size=config.BATCH_SIZE)\nval_dataloader = DataLoader(val_data, batch_size=config.BATCH_SIZE)\n\nb = next(iter(train_dataloader))\nfor k, v in b.items():\n    if k == 'titles' or k == 'abstracts':\n        print(k)\n        for k_, v_ in b[k].items():\n            print(f'{k_} shape: {v_.shape}\\n')\n    else:\n        print(f'{k} shape: {v.shape}')","c98f1f9f":"class DualSciBert(nn.Module):\n    def __init__(self):\n        super(DualSciBert, self).__init__()\n\n        self.titles_model = AutoModel.from_pretrained(config.MODEL_PATH)\n        self.abstracts_model = AutoModel.from_pretrained(config.MODEL_PATH)\n        \n        self.dropout = nn.Dropout(0.25)\n        self.avgpool = nn.AvgPool1d(2, 2)\n        self.output = nn.Linear(768, config.NUM_LABELS)\n\n    def forward(\n        self,\n        input_ids_titles, \n        attention_mask_titles=None, \n        input_ids_abstracts=None,\n        attention_mask_abstracts=None\n        ):\n        \n        _, titles_features = self.titles_model(\n            input_ids=input_ids_titles,\n            attention_mask=attention_mask_titles\n        )\n        titles_features = titles_features.unsqueeze(1)\n        titles_features_pooled = self.avgpool(titles_features)\n        titles_features_pooled = titles_features_pooled.squeeze(1)\n        \n        _, abstracts_features = self.abstracts_model(\n            input_ids=input_ids_abstracts,\n            attention_mask=attention_mask_abstracts\n        )\n        abstracts_features = abstracts_features.unsqueeze(1)\n        abstracts_features_pooled = self.avgpool(abstracts_features)\n        abstracts_features_pooled = abstracts_features_pooled.squeeze(1)\n        \n        combined_features = torch.cat((\n            titles_features_pooled, \n            abstracts_features_pooled), \n            dim=1\n        )\n        x = self.dropout(combined_features)\n        x = self.output(x)\n        \n        return x","21700839":"class SiameseSciBert(nn.Module):\n    def __init__(self):\n        super(SiameseSciBert, self).__init__()\n\n        self.model = AutoModel.from_pretrained(config.MODEL_PATH)\n        self.dropout = nn.Dropout(0.25)\n        self.avgpool = nn.AvgPool1d(2, 2)\n        self.output = nn.Linear(768, config.NUM_LABELS)\n\n    def forward(\n        self,\n        input_ids_titles, \n        attention_mask_titles=None, \n        input_ids_abstracts=None,\n        attention_mask_abstracts=None\n        ):\n        \n        _, titles_features = self.model(\n            input_ids=input_ids_titles,\n            attention_mask=attention_mask_titles\n        )\n        titles_features = titles_features.unsqueeze(1)\n        titles_features_pooled = self.avgpool(titles_features)\n        titles_features_pooled = titles_features_pooled.squeeze(1)\n        \n        _, abstracts_features = self.model(\n            input_ids=input_ids_abstracts,\n            attention_mask=attention_mask_abstracts\n        )\n        abstracts_features = abstracts_features.unsqueeze(1)\n        abstracts_features_pooled = self.avgpool(abstracts_features)\n        abstracts_features_pooled = abstracts_features_pooled.squeeze(1)\n        \n        combined_features = torch.cat((\n            titles_features_pooled, \n            abstracts_features_pooled), \n            dim=1\n        )\n        x = self.dropout(combined_features)\n        x = self.output(x)\n        \n        return x","b617ddac":"class SiameseSciBertRNN(nn.Module):\n    def __init__(self):\n        super(SiameseSciBertRNN, self).__init__()\n\n        self.model = AutoModel.from_pretrained(config.MODEL_PATH)\n        self.dropout = nn.Dropout(0.3)\n        self.avgpool = nn.AvgPool1d(2, 2)\n        self.maxpool = nn.MaxPool1d(2, 2)\n        \n        self.rnn = nn.GRU(\n            input_size=768, \n            hidden_size=128, \n            batch_first=True,\n            bidirectional=True,\n        )\n        \n        self.output = nn.Linear(256, config.NUM_LABELS)\n\n    def forward(\n        self,\n        input_ids_titles, \n        attention_mask_titles=None, \n        input_ids_abstracts=None,\n        attention_mask_abstracts=None\n        ):\n        \n        titles_hidden_states, _ = self.model(\n            input_ids=input_ids_titles,\n            attention_mask=attention_mask_titles\n        )\n        self.rnn.flatten_parameters()\n        titles_rnn_out, _ = self.rnn(titles_hidden_states)\n        titles_rnn_feat = titles_rnn_out.mean(dim=1)\n        titles_rnn_feat = titles_rnn_feat.unsqueeze(1)\n        titles_rnn_feat_pooled = self.avgpool(titles_rnn_feat)\n        titles_rnn_feat_pooled = titles_rnn_feat_pooled.squeeze(1)\n        \n        abstracts_hidden_states, _ = self.model(\n            input_ids=input_ids_abstracts,\n            attention_mask=attention_mask_abstracts\n        )\n        self.rnn.flatten_parameters()\n        abstracts_rnn_out, _ = self.rnn(abstracts_hidden_states)\n        abstracts_rnn_feat = abstracts_rnn_out.mean(dim=1)\n        abstracts_rnn_feat = abstracts_rnn_feat.unsqueeze(1)\n        abstracts_rnn_feat_pooled = self.avgpool(abstracts_rnn_feat)\n        abstracts_rnn_feat_pooled = abstracts_rnn_feat_pooled.squeeze(1)\n\n        combined_features = torch.cat((\n            titles_rnn_feat_pooled, \n            abstracts_rnn_feat_pooled), \n            dim=1\n        )\n        x = self.dropout(combined_features)\n        x = self.output(x)\n        \n        return x","c2cbb65d":"device = config.DEVICE\ndevice","5efc3b7b":"def val(model, val_dataloader, criterion):\n    \n    val_loss = 0\n    true, pred = [], []\n    \n    # set model.eval() every time during evaluation\n    model.eval()\n    \n    for step, batch in enumerate(val_dataloader):\n        # unpack the batch contents and push them to the device (cuda or cpu).\n        b_input_ids_titles = batch['titles']['input_ids'].to(device)\n        b_attention_mask_titles = batch['titles']['attention_mask'].to(device)\n        b_input_ids_abstracts = batch['abstracts']['input_ids'].to(device)\n        b_attention_mask_abstracts = batch['abstracts']['attention_mask'].to(device)\n        b_labels = batch['labels'].to(device)\n\n        # using torch.no_grad() during validation\/inference is faster -\n        # - since it does not update gradients.\n        with torch.no_grad():\n            # forward pass\n            logits = model(\n                b_input_ids_titles, \n                b_attention_mask_titles,\n                b_input_ids_abstracts,\n                b_attention_mask_abstracts\n            )\n            \n            # calculate loss\n            loss = criterion(logits, b_labels)\n            val_loss += loss.item()\n\n            # since we're using BCEWithLogitsLoss, to get the predictions -\n            # - sigmoid has to be applied on the logits first\n            logits = torch.sigmoid(logits)\n            logits = np.round(logits.cpu().numpy())\n            labels = b_labels.cpu().numpy()\n            \n            # the tensors are detached from the gpu and put back on -\n            # - the cpu, and then converted to numpy in order to -\n            # - use sklearn's metrics.\n\n            pred.extend(logits)\n            true.extend(labels)\n\n    avg_val_loss = val_loss \/ len(val_dataloader)\n    print('Val loss:', avg_val_loss)\n    print('Val accuracy:', accuracy_score(true, pred))\n\n    val_micro_f1_score = f1_score(true, pred, average='micro')\n    print('Val micro f1 score:', val_micro_f1_score)\n    return val_micro_f1_score\n\n\ndef train(\n    model, \n    train_dataloader, \n    val_dataloader, \n    criterion, \n    optimizer, \n    scheduler, \n    epoch\n    ):\n    \n    # we validate config.N_VALIDATE_DUR_TRAIN times during the training loop\n    nv = config.N_VALIDATE_DUR_TRAIN\n    temp = len(train_dataloader) \/\/ nv\n    temp = temp - (temp % 100)\n    validate_at_steps = [temp * x for x in range(1, nv + 1)]\n    \n    train_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader, \n                                      desc='Epoch ' + str(epoch))):\n        # set model.eval() every time during training\n        model.train()\n        \n        # unpack the batch contents and push them to the device (cuda or cpu).\n        b_input_ids_titles = batch['titles']['input_ids'].to(device)\n        b_attention_mask_titles = batch['titles']['attention_mask'].to(device)\n        b_input_ids_abstracts = batch['abstracts']['input_ids'].to(device)\n        b_attention_mask_abstracts = batch['abstracts']['attention_mask'].to(device)\n        b_labels = batch['labels'].to(device)\n\n        # clear accumulated gradients\n        optimizer.zero_grad()\n\n        # forward pass\n        logits = model(\n            b_input_ids_titles, \n            b_attention_mask_titles,\n            b_input_ids_abstracts,\n            b_attention_mask_abstracts\n        )\n        \n        # calculate loss\n        loss = criterion(logits, b_labels)\n        train_loss += loss.item()\n\n        # backward pass\n        loss.backward()\n\n        # update weights\n        optimizer.step()\n        \n        # update scheduler\n        scheduler.step()\n\n        if step in validate_at_steps:\n            print(f'-- Step: {step}')\n            _ = val(model, val_dataloader, criterion)\n    \n    avg_train_loss = train_loss \/ len(train_dataloader)\n    print('Training loss:', avg_train_loss)","6110474b":"def run():\n    # setting a seed ensures reproducible results.\n    # seed may affect the performance too.\n    torch.manual_seed(config.SEED)\n\n    criterion = nn.BCEWithLogitsLoss()\n    \n    # define the parameters to be optmized -\n    # - and add regularization\n    if config.FULL_FINETUNING:\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n\n    num_training_steps = len(train_dataloader) * config.EPOCHS\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n\n    max_val_micro_f1_score = float('-inf')\n    for epoch in range(config.EPOCHS):\n        train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\n        val_micro_f1_score = val(model, val_dataloader, criterion)\n\n        if config.SAVE_BEST_ONLY:\n            if val_micro_f1_score > max_val_micro_f1_score:\n                best_model = copy.deepcopy(model)\n                best_val_micro_f1_score = val_micro_f1_score\n\n                model_name = 'scibertfft_dualinput_best_model'\n                torch.save(best_model.state_dict(), model_name + '.pt')\n\n                print(f'--- Best Model. Val loss: {max_val_micro_f1_score} -> {val_micro_f1_score}')\n                max_val_micro_f1_score = val_micro_f1_score\n\n    return best_model, best_val_micro_f1_score","ce10ca98":"model = DualSciBert()\nmodel.to(device);\nbest_model, best_val_micro_f1_score = run()","0a1001bd":"del model\nmodel = SiameseSciBert()\nmodel.to(device);\nbest_model, best_val_micro_f1_score = run()","4e11ecdb":"del model\nmodel = SiameseSciBertRNN()\nmodel.to(device);\nbest_model, best_val_micro_f1_score = run()","f197291e":"test_df = pd.read_csv(project_dir + 'test.csv')\ndataset_size = len(test_df)\ntest_indices = list(range(dataset_size))\n\ntest_data = TransformerDataset(test_df, test_indices, set_type='test')\ntest_dataloader = DataLoader(test_data, batch_size=config.BATCH_SIZE)\n\ndef predict(model):\n    val_loss = 0\n    test_pred = []\n    model.eval()\n    for step, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n        b_input_ids_titles = batch['titles']['input_ids'].to(device)\n        b_attention_mask_titles = batch['titles']['attention_mask'].to(device)\n        b_input_ids_abstracts = batch['abstracts']['input_ids'].to(device)\n        b_attention_mask_abstracts = batch['abstracts']['attention_mask'].to(device)\n\n        with torch.no_grad():\n            logits = model(\n                b_input_ids_titles, \n                b_attention_mask_titles,\n                b_input_ids_abstracts,\n                b_attention_mask_abstracts\n            )\n            logits = torch.sigmoid(logits)\n            logits = np.round(logits.cpu().numpy())\n            test_pred.extend(logits)\n\n    test_pred = np.array(test_pred)\n    return test_pred\n\n# test_pred = predict(best_model)","8b6e59b4":"def submit():\n    sample_submission = pd.read_csv(project_dir + 'sample_submission.csv')\n    ids = sample_submission['ID'].values.reshape(-1, 1)\n\n    merged = np.concatenate((ids, test_pred), axis=1)\n    submission = pd.DataFrame(merged, columns=sample_submission.columns).astype(int)\n    return submission\n\n# submission = submit()","93b5a9e8":"# submission_fname = f'submission_scibertfft_dualinput_microf1-{round(best_val_micro_f1_score, 4)}.csv'\n# submission.to_csv(submission_fname, index=False)","712466af":"Our **TransformerDataset** Class takes as input the **dataframe**, **indices** & **set_type**. We calculate the train \/ val set indices beforehand, pass it to **TransformerDataset** and slice the dataframe using these indices.","b4551a3b":"Now, we'll create a custom Dataset class inherited from the PyTorch Dataset class. We'll be using the **SciBERT tokenizer** that returns **input_ids** and **attention_mask**.<br><br>\nThe custom Dataset class will return a dict containing - <br>\n\n- titles\n    - input_ids\n    - attention_mask\n- abstracts\n    - input_ids\n    - attention_mask\n- labels\n    \n<br>","0680152e":"Coming to the most interesting part - the model architecture!<br>\nWe'll create classes named **DualSciBert, SiameseSciBert, SiameseSciBertRNN**, inherited from **torch.nn.Module**.","e0d9944a":"# Config","f9d7e586":"## Exploratory Data Analysis","b43738a3":"We're using just the abstract text, but concatenating the title text along with it performed better on the leaderboard. \n<br><br>\n## Preprocessing\n- Stripping extra whitespaces around the text.\n- Replacing escape characters with whitespace.\n- Padding all punctuations with whitespaces on both sides.\n\n#### Additional Tips:\n- Replacing Latex equations with a special token.\n- Data Augmentation. \n\n","d4a96c4b":"Here we experiment with applying a **bidirectional GRU** with **hidden_size=128**, on the **hidden states** output of SciBERT. Average Pooling is applied on the outputs of the **GRU**.","c6152f51":"# Submission","1f198365":"From the plots above we can infer that, **20** seems like a good choice for **titles MAX_LENGTH** and **320** for **abstracts MAX_LENGTH**.","3887423e":"The entire code is written using **PyTorch**.<br>\nWe'll be using the **transformers** library by [huggingface](https:\/\/github.com\/huggingface\/transformers) as they provide wrappers for multiple Transformer models.","9e600b27":"Load the test dataset, and initialize a DataLoader object for it.","1a69dc94":"# Engine","7c190ec5":"# Dataset & Dataloader","02d9f1d9":"Here we'll initialize PyTorch DataLoader objects for the training & validation data.<br>\nThese dataloaders allow us to iterate over them during training, validation or testing and return a batch of the Dataset class outputs.","2492708e":"Checking the GPU configurations. Kaggle's Tesla P100 GPU proves to be much faster for finetuning SciBERT on this dataset as compared to Google Colab's Tesla K80.","c8420009":"## SiameseSciBert","45966bf8":"# Model","a3470680":"# Imports","beec8ec4":"Here we define a Config class, which contains all the fixed parameters & hyperparameters required for **Dataset** creation as well as **Model** training.","1b8813e9":"## DualSciBert","1b926e1a":"### **[SciBERT](http:\/\/github.com\/allenai\/scibert)** \n* It is a BERT model trained on scientific text.<br>\n* SciBERT is trained on papers from the corpus of semanticscholar.org. Corpus size is 1.14M papers, 3.1B tokens. We use the full text of the papers in training, not just abstracts.\n\n### Multi Class vs Multi Label Classification\n* **Multi Class** - There are multiple categories but each instance is assigned only one, therefore such problems are known as multi-class classification problem.\n* **Multi Label** - There are multiple categories and each instance can be assigned with multiple categories, so these types of problems are known as multi-label classification problem, where we have a set of target labels.\n\n### Dual Input\nThe dataset contains two texts - **Title** & **Abstract**. In this notebook, we'll try out 2 different architectures (modifications\/additions to the pure SciBERT model) that handle multi textual input.\n1. Dual SciBERT<br>\n![Dual SciBERT](https:\/\/miro.medium.com\/max\/1400\/1*VAGuT28ban70hwbqZxEGjw.png)\n<br>\n2. Siamese SciBERT<br>\n![Siamese SciBERT](https:\/\/miro.medium.com\/max\/1400\/1*K8-8INIsJQZ2s3OWtlllBg.png)","40688b11":"# Data","fd88a5f6":"### Loss function used<br>\n- **BCEWithLogitsLoss** - Most commonly used loss function for Multi Label Classification tasks. Note that, PyTorch's BCEWithLogitsLoss is numerically stable than BCELoss.\n<br>\n\n### Optimizer used <br>\n- **AdamW** - Commonly used optimizer. Performs better than Adam.\n<br>\n\n### Scheduler used <br>\n- **get_linear_scheduler_with_warmup** from the **transformers** library.\n<br>","13007dfa":"Our engine consists of the training and validation step functions.","e78aaef6":"# Run","1cb1865d":"## SiameseSciBert + RNNs"}}