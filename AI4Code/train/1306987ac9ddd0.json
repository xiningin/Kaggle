{"cell_type":{"5c667cf4":"code","41266169":"code","0497c423":"code","e32e415f":"code","1914a4ac":"code","130c616b":"code","fbea9a10":"code","8e4a14ff":"code","0f1ba304":"code","561ee2ab":"code","3fa07763":"code","5baad16e":"code","7c65e257":"code","b676afe5":"code","0fc230e0":"code","9d751c6b":"code","ff2382a9":"code","bfc33ff4":"code","cc69fc10":"code","901e59bf":"code","e3bd1df8":"code","177315c0":"code","26157afc":"code","356bf2fa":"code","1afa8805":"code","0e8930e4":"code","8cc01464":"code","a109681d":"code","4b10e67a":"code","6850cbd9":"code","03c5b0c6":"code","454e0bae":"code","223451e1":"code","1af0f565":"code","91aac624":"code","cb2b7caa":"code","4b254476":"code","235cfa86":"code","6f5b7f85":"code","886a5673":"code","1c65743f":"code","950246ac":"code","6437067e":"code","21e4018c":"code","0a8ab1a1":"code","fd94e697":"code","3692d38f":"code","cea4aa8d":"code","8f2adfc8":"code","2f8dd633":"code","db7721c6":"code","53da9454":"code","930df88f":"code","8a42b9df":"code","04649cda":"code","d2b2ffea":"markdown","d9703fd3":"markdown","d2a23ae2":"markdown","fadf1144":"markdown","ddc4d044":"markdown","3566b097":"markdown","06d596fa":"markdown","b0926c47":"markdown","f7ab5619":"markdown","1d5907db":"markdown","f99e08af":"markdown","306a01fe":"markdown","f99cea15":"markdown","575cb561":"markdown","7c11724a":"markdown","084449df":"markdown","6a1b819b":"markdown","fad9d1fe":"markdown","e5f4241b":"markdown","c2f6ca28":"markdown","1f1f4759":"markdown","0d582096":"markdown","439611bf":"markdown","3540a1cb":"markdown","a45ec7c6":"markdown","55fc533c":"markdown","3fa18a67":"markdown","0541015d":"markdown","0d744393":"markdown","979d00cf":"markdown","d8cccbe9":"markdown","d4af5845":"markdown","c06ca716":"markdown","ca5fb4a8":"markdown","a22289d9":"markdown","97f87e47":"markdown","7b0bf179":"markdown","d5d99bcd":"markdown","a6c1912e":"markdown","98283877":"markdown","b1eaf691":"markdown","26f015aa":"markdown","cf648199":"markdown","60cabdea":"markdown","6a05b124":"markdown","4bc332ec":"markdown","63848812":"markdown","3f927540":"markdown","dc0b1c01":"markdown","2aeef96d":"markdown","08bb806b":"markdown","90232e05":"markdown","6d994705":"markdown","6c007328":"markdown","dc3af754":"markdown","adcfcef1":"markdown","293d27a5":"markdown","cfc100da":"markdown","6a0baafc":"markdown","1892940b":"markdown"},"source":{"5c667cf4":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd\n\n# sklearn prepocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# file system management\nimport os\n\n# suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# ignore warnings, update, changing etx\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","41266169":"# list files available\nprint(os.listdir('..\/input\/home-credit-default-risk'))","0497c423":"# training data\napp_train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","e32e415f":"# Testing data features\napp_test = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\nprint('Testing data shape:', app_test.shape)\napp_test.head()","1914a4ac":"app_train['TARGET'].value_counts()","130c616b":"app_train['TARGET'].astype(int).plot.hist() ;","fbea9a10":"# Function to calculate missing values by column\ndef missing_values_table(df):\n    # Total missing values\n    mis_val = df.isnull().sum()\n    \n    # percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n    \n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    \n    # rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns= {0: 'Missing Values', 1 : '% of Total Values'})\n    \n    # sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n    \n    # print some summary information\n    print('Your selected dataframe has ' + str(df.shape[1]) +\n         \" columns.\\n\"\n         \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n         \" columns that have missing values.\")\n    \n    # return the dataframe with missing in information\n    return mis_val_table_ren_columns","8e4a14ff":"# missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","0f1ba304":"# number of each type of column\napp_train.dtypes.value_counts()","561ee2ab":"# number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","3fa07763":"# create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # if 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # train on the training data\n            le.fit(app_train[col])\n            # transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            \n            # keep track of how many columns were label encoded\n            le_count += 1\n\nprint('%d columns were label encoded.' % le_count)","5baad16e":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Training Features shape: ', app_test.shape)","7c65e257":"train_labels = app_train['TARGET']\n\n# algin the training  and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis=1)\n\n# add the target back in\napp_train['TARGET'] = train_labels\n\nprint('training features shape: ', app_train.shape)\nprint('testing features shape: ', app_test.shape)","b676afe5":"(app_train['DAYS_BIRTH'] \/ -365).describe()","0fc230e0":"app_train['DAYS_EMPLOYED'].describe()","9d751c6b":"app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram');\nplt.xlabel('Days employment');","ff2382a9":"anom = app_train[app_train[\"DAYS_EMPLOYED\"] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 362543]\n\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom[\"TARGET\"].mean()))\nprint('There are %d anoamlous days of employment' % len(anom))","bfc33ff4":"# Create an anomalous flag column\napp_train[\"DAYS_EMPLOYED_ANOM\"] = app_train['DAYS_EMPLOYED'] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243:np.nan}, inplace=True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title='Days employment histogram');\nplt.xlabel('Days employment');","cc69fc10":"app_test[\"DAYS_EMPLOYED_ANOM\"] = app_test['DAYS_EMPLOYED'] == 365243\napp_test['DAYS_EMPLOYED'].replace({365243:np.nan}, inplace=True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test['DAYS_EMPLOYED_ANOM'].sum(), len(app_test)))","901e59bf":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()","e3bd1df8":"# display correlations\nprint('Most positive correlaionsa: \\n', correlations.tail(15))\nprint('\\nMost Negative correlations: \\n', correlations.head(15))","177315c0":"# Find the correlation of the positive days since birth and target\n\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","26157afc":"# set the style of plots\nplt.style.use('fivethirtyeight')\n\n# plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] \/ 365, edgecolor = 'k', bins=25)\nplt.title('Age of client');\nplt.xlabel('Age (years)');\nplt.ylabel('count');","356bf2fa":"plt.figure(figsize=(10, 6))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH']\/ 365, label= 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH']\/ 365, label= 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)');\nplt.ylabel('Density');\nplt.legend();\nplt.title('Distribution of Ages');","1afa8805":"# age information into a seperate dataframe\nage_data = app_train[['TARGET', \"DAYS_BIRTH\"]]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH']\/365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins=np.linspace(20, 70, num=11))\nage_data.head(10)","0e8930e4":"# Group by the bin and calculate averages\nage_groups = age_data.groupby('YEARS_BINNED').mean()\nage_groups","8cc01464":"plt.figure(figsize=(10, 8))\n\n# graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100*age_groups['TARGET'])\n\n# plot labeling\nplt.xticks(rotation = 75);\nplt.xlabel('Age group (years)');\nplt.ylabel('Failure to repay (%)');\nplt.title('Failure to repay by age group');","a109681d":"# Extract the EXT_SOURCE and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', \"EXT_SOURCE_2\", 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","4b10e67a":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot=True, vmax = 0.6)\nplt.title('correlation heatmap');","6850cbd9":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    # create a new subplot for each source\n    plt.subplot(3, 1, i+1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repai\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # label the plots\n    plt.title('distribution of %s by the target value' % source)\n    plt.legend();\n    plt.xlabel('%s' % source);\n    plt.ylabel('Density');\n\nplt.tight_layout(h_pad = 2.5)","03c5b0c6":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","454e0bae":"# make a new dataframe for polynomial features\n\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.fit_transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","223451e1":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\n\nprint('polynomial features shpae', poly_features.shape);","1af0f565":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","91aac624":"# create a dataframe of the features\npoly_features = pd.DataFrame(poly_features,\n                            columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# add in the target\npoly_features['TARGET'] = poly_target\n\n# find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# display most negative and most positive\nprint(poly_corrs.head(10));\nprint(poly_corrs.tail(5));","cb2b7caa":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test,\n                                 columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'EXT_SOURCE_4']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, \n                                                    join = 'inner', axis =1)\n# print out the new shape\nprint('training data with polynomial features shape', app_train_poly.shape);\nprint('testing data with polynomial features shape', app_test_poly.shape);","4b254476":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] \/ app_train_domain['DAYS_BIRTH']","235cfa86":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']","6f5b7f85":"plt.figure(figsize = (12, 20))\n# iterate through the new features\n\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', \n                             'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    # create a new subplot for each source\n    plt.subplot(4, 1, i+1)\n\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature],\n               label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature],\n               label = 'target == 1')\n    \n    # label the plots\n    plt.title('distribution of %s by the target value' % feature)\n    plt.xlabel('%s' % feature);\n    plt.ylabel('density');\n    plt.legend();\n\nplt.tight_layout(h_pad = 2.5)","886a5673":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\n# drop the target from the training data \nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = SimpleImputer(strategy = 'median')\n\n# scale each feature to 0-1\nscaler = MinMaxScaler(feature_range= (0, 1))\n\n# Fit on the training date\nimputer.fit(train)\n\n# transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('training data shape', train.shape)\nprint('testing data shape', test.shape)","1c65743f":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C=0.0001, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,\n          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","950246ac":"# make predictions\n# make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","6437067e":"# submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","21e4018c":"# save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index=False)","0a8ab1a1":"from sklearn.ensemble import RandomForestClassifier\n\n# make the random foreset classifier\nrandom_forest = RandomForestClassifier(n_estimators=100, random_state=50,\n                                      verbose =1, n_jobs= -1)","fd94e697":"# train on the training data\nrandom_forest.fit(train, train_labels)\n\n# extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, \n                                    'importance': feature_importance_values})\n\n# make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","3692d38f":"# make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index=False)","cea4aa8d":"poly_features_names = list(app_train_poly.columns)\n\n# impute the polynomial features\nimputer = SimpleImputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# scale the polynomial features\nscaler = MinMaxScaler(feature_range= (0,1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators= 100,\n                                           random_state=50, verbose = 1, n_jobs= -1)","8f2adfc8":"# train pn the training data \nrandom_forest_poly.fit(poly_features, train_labels)\n\n# make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","2f8dd633":"# make a submission dataframe\nsubmit = app_test['SK_ID_CURR']\nsubmit['TARGET'] = predictions\n\n# save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index=False)","db7721c6":"app_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = SimpleImputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","53da9454":"# make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index=False)","930df88f":"def plot_feature_importances(df):\n    \n    # sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    \n    # make a horizontal bar chartt of feature importance\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # need to reverse the index to plot most importance on top\n    ax.barh(list(reversed(list(df.index[:15]))),\n           df['importance_normalized'].head(15),\n           align = 'center', edgecolor = 'k')\n    \n    # set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # plot labeling\n    plt.xlabel('normalized importance');\n    plt.title('feature importances');\n    plt.show()\n    \n    return df","8a42b9df":"# show the feature importance for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","04649cda":"feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)","d2b2ffea":"### Back to Exploratory Data Analysis\n\n#### Anomalies : **[explain url](https:\/\/wkdtjsgur100.github.io\/anomaly\/)**\n\nOne problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method. **The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. To see these stats in years, we can multiple by -1 and divide by the number of days in a year:**","d9703fd3":"There is a clear trend: \nyounger applicants are more likely to not repay the loan!\nThe rate of failure to reapy is above 10% for the youngest three age groups and below 5% for the oldest age group.\n\nThis is information that could be directly used by the bank:\nbecause younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate younger clients, but it would be smart to take precautionary measures to help younger clients pay on time.","d2a23ae2":"### Encoding Categorical Variables\n\nBefor we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process.\n\n* Label encoding: assign each unique category in a categorical variable with an interger. No new columns are created. \n\n![image.png](attachment:5026e554-d271-4c6d-9644-41858781e64d.png)\n\n* One-hot encoding: create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns.\n\n![image.png](attachment:4a9b1efc-5057-4f83-bcae-571e35756ed2.png)\n\nThe problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories in random and does not reflect any inherent aspect of the category. In the example above, programmer recieves a 4 adn data scientist a 1, but if we did the same process again, the labels could be reversed or completely different. The acutal assignment of the integers is arbitrary. Therefore, when we perform label encoding, the model might use the relative value of the feature (for example preogrammer = 4 and data scientist = 1) to assign weights which is not what we want. if we only have two unque values for a categorical varibales (As Male\/Female), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the safe option.\n\nThere is some debate about the relative merits of these approaches, and some models can deal with label encoded categorical varibales with no issues. I think for categorical variables with many classes, one-hot encoding is the safest approach because it does not impose arbitrary values to categories. The only downside to one-hot encoding is that the numer of features can explode with categorical variables with many categories. To deal with this, we can perform one-hot-encoding followed with PCA or other dimensionality reduction methods to reduce the number of dimensions.\n\nIn this notebook, we will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. This process may need to change as we get further into the project, but for now, we will see where this gets us.","fadf1144":"We see that all four of our hand-engineered features made it into the top 15 most important! This should give us confidence that our domain knowledge was at least partially on track.","ddc4d044":"We will use LogisticRregression from scikit-learn for our first model.\nThe only change we will make from the default model settings is to lower the regularization parameter, C, which controls the amount of overfitting (a lower value should decrease overfitting). This will get us slightly better results than the default LogisticRegression, but it still will set a low bar for any future models.\n\nHere we use the familiar scikit-learn modeling syntax: we first create the model, then we train the model using .fit and then we make predictions on the testing data using .predict_proba.","3566b097":"Just out of curiosity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients.","06d596fa":"### Polynomial Features\n\nOne simple feature construction method is called polynomial features. In this method, we make features that are powers of existing features as well as interaction terms between existing features. For example, we can creative variables EXT_SOURCE_1 x EXT_SOURCE_2, EXT_SOURCE_1 x EXT_SOURCE_2^2, EXT_SOURCE_1^2 x EXT_SOURCE_2^2, and so on. These features that are a combination of multiple individual variables are called interaction terms because they capture the interactions between variables. In other words, while two variables by themselves may not have a strong influence on the target, combining them together into a single interaction variable might show a relationship with the target. Interaction terms are commonly used in statistical models to capture the effects of multiple variables, but I do not see them usesd as often in machine learning. Nonetheless, we can try out a few to see if they might help our model to predict whether or not a client will repay a loan.\n\nJake VanderPlas writes about polynomial features in his excellent book python for Data science for those who want more information.\n\nIn the following code, we create polynomiial features using the EXT_SOURCE variables and the DAYS_BIRTH variable. Scikit-learn has a useful class called PolynomialFeatures that creates the polynomials and the interaction terms up to a specified degree. We can use a degree of 3 to see the results (when we are creating polynomial features, we want to avoid using too high of a degree, both because the number of features scales exponentially with the degree, and because we can run into probles with overfitting).","b0926c47":"In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the EXT_SOURCE_1 and the DAYS_BIRTH (or equivalently YEARS_BIRTH), indicating that this feature may take into account the age of the client.","f7ab5619":"### Model Interpretation: Feature Importances\n\nAs a simple method to see which variables are the most relevant, we can look at the feature importances of the random forest. Given the correlations we saw in the exploratory data analysis, we should expect that the most important features are the EXT_SOURCE and the DAYS_BIRTH. We may use these feature importances as a method of dimensionality reduction in future work.","1d5907db":"### Exploratory Data Analysis\n\nExploratory Data Analysis(EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns or relationships within the data. The goal of EDA is to learn what our data can tell us. It is generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.","f99e08af":"The predictions must be in the format shown in the sample_submission.csv file, where there are only two columns: SK_ID_CURR and TARGET. We will create a dataframe in this format from the test set and the predictions called submit","306a01fe":"That doesn't look right. The maximum value is about 1000 years.","f99cea15":"### Introduction: Home Credit Default Risk Competition\n\n* **copyright (Will Koehrsen) : [URL](https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction)**\n* **This code is written by will Koehrsen, thanks for sharing.**\n\nThis notebook is intended for those who are new to machine learning competitions or want a gentle to the problem. I purposely avoid jumping into complicated models or joining together lots of data in order to show the basics of how to get started in machine learning. Any comments or suggestions are much appreciated.\n\nIn this notebook, we will take an initial look at the Home Credit default risk machine learning competition currently hosted on Kaggle. The objective of this competition is to use historical loan application data to predict whether or not an appicant will be repay a loan. This is a standard supervised classification task:\n\n* **Supervised** : The labels are included in the training data and the goal is to train a model to learn to predict the lables from the features.\n\n* **Classifictation** : The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)","575cb561":"### Visualize new Variables\n\nWe should explore these **domain knowledge** variables visually in a graph. For all of these, we will make the same KDE plot colored by the value of the TARGET.","7c11724a":"All three EXT_SOURCE features have negative correlations with the target, indicating that as the value of the EXT_SOURCE increases, the client is more likely to repay the loan. We can also see that DAYS_BIRTH is positively correlated with EXP_SOURCE_1 indicating that maybe one of the factors in this score is the client age.\n\nNext we can look at the distribution of each of these features colored by the value of the target. This will let us visulatize the effect of this variable on the target.","084449df":"This model scored 0.678 when submitted to the competition, exactly the same as that without the enigneered features. Given these results, it does not appear that our feature construction helped in this case.\n\n### Testing domain features\n\nNow we can test the domain features we made by hand.","6a1b819b":"Let's take a look a some of more significant correlations: the *DAYS_BIRTH* is the most positive correlations. Looking at the documentaion, *DAYS_BIRTH* is the age in days of the client at the time of the loan in negative days (for whatever reason). The correlation is positive, but the value of this feature is actually negative, meaning that as the client gets older, they are less likely to default on their laon (ie the target == 0). That's a little confusing, so we will take the absolute value of the feature and the the correlation will be negative.\n\n### Effect of Age on Repayment","fad9d1fe":"Now that the model has been trained, we can use it to make predictions. We want to predict the probabilities of not paying a loan, so we use the model predict.proba method. This returns an m x 2 array where m is the number of observations. The first column is the probability of the target being 0 and the second column is the probability of the target being 1 (so for a single row, the two columns must sum to 1). We want the probability the loan is not repaid, so we will select the second column.\n\nThe following code makes the predictions and selects the correct column.","e5f4241b":"Most of the categorical variables have a relatively small number of unique entries. We will need to find a way to deal with these categorical variables.","c2f6ca28":"The 'targe == 1' curve skews toward the younger end of the ragne. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket.\n\nTo make this graph, first we *cut* the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category.","1f1f4759":"This scores a 0.679 when submitted which probably shows that the engineered features do not help in this model (however they do help in the Gradient Boosting model at the end of the notebook).\n\nIn later notebooks, we will do more feature engineering by using the information from the other data sources. From experience, this will definitely help our model.","0d582096":"The distribution looks to be much more in line with what we would expect, and we also have created a new column to tell the model that these values were originally anomalous (because we will have to fill in the nans with some value, probably the median of the column). The other columns with *DAYS* in the dataframe look to be about what we expect with no obvious outliers.\n\nAs an extremely important note, anythin we do to the training data we also have to do to the testing data. Let's make sure to create the new column and fill in the existing column with *np.nan* in the testing data.","439611bf":"### Feature Engineering\n\nKaggle competitions are won by feature engineering: those win are those who can create the most useful features of the data. (This is true for the most part as the winning models, at least for structured data, all tend to be variants on gradient boosting). This represents one of the patterns in machine learning: feature engineering has a greater return on investment than model building and hyperparameter tuning. As Andrew NG is fond of saying: 'applied machine learning is basically feature engineering.'\n\nWhile choosing the right model and optimal settings are important, the model can only learn from the data it is given. Making sure this data is as relevant to the task as possible is the jov of the data scientis (and maybe some automated tools to help us out).\n\nFeature engineering refers to a geneal process and can involve both feature construction: adding new features from the existing data, and feature selection: choosing only the most important features or other methods of dimensionality reduction. There are many techniques we can use to both create features and select features.\n\nWe will do a lot of feature engineering when we start using the other data sources, but in this notebook we will try only two simple feature construction methods:\n* Ploynomial features\n* Domain knowledge features","3540a1cb":"### Correlations\n\nNow that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by lookinf for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the *.corr* dataframe method.\n\nThe correlation coefficient is not the greatest method to represent 'relevance' of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficient are:\n\n* 00~19: very weak\n* 20-39: weak\n* 40-59: moderate\n* 60-79: strong\n* 80-100: very strong","a45ec7c6":"Those ages look reasonable. There are no outliers for the age on either the high or low end. How about the days of employment?","55fc533c":"This is extremly interesting. It turns out that the anomalies have a lower rate of default.\n\nHandling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machie learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous.","3fa18a67":"### Logistic Regression Implementation\n\nHere I will focus on implementing the model rather than explaining the details, but for those who want to learn more about the theory of machine learning algorithms, I recommend both An Introduction to Statistical learning and Hands-on machine learning with scikit-learn and tensorflow. Both of these book present the theory and alo code needed to make the modlels. They both teach with the mindset that the best way to learn is by doing, and they are very effective.\n\nTo get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) adn normalizing the range of the features (feature scaling). The following code performs both of these preprocssing steps.","0541015d":"As expected, the most important features are those dealing with EXT_SOURCE and DAYS_BIRTH. We see that there are only a handful of features with a significant importance to the model, which suggests we may be able to drop many of the features without a decrease in performance (and we may even see an increase in performance.) Feature importances are not the most sophisticated method to interpret a model or perform dimensionality reduction, but they let us start to understand what factors our model takes into account when it makes predictions.","0d744393":"### Aligning Training and Testing Data\n\nThere need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. First we extract the target column from the training data (Because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set *axis = 1* to align the dataframes based on the columns and not on the rows.","979d00cf":"### Imports\n\nWe are using a typical data science stack: numpy, pandas, sklearn, matplotlib.","d8cccbe9":"### Examine Missing Values\n\nNext we can look at the number and percentage of missing values in each column.","d4af5845":"By itself, the distribution of age does not tell us much other than that there are no outliiers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a[ kernel density estimation plot](https:\/\/seongkyun.github.io\/study\/2019\/02\/03\/KDE\/) colored by the value ot the target. A kernel density estimate plot shows the distribution of a single variable and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn *kdeplot* for this graph.","c06ca716":"These predictinos will also be available when we run the entire notebook.\n\n**This model should score around 0.678 when submitted.**","ca5fb4a8":"### Baseline\n\nFor a naive baseline, we could guess the same value ofr all examples on the testing set. We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This will get us a Receiver Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition (random quessing on a classification task will score a 0.5).\n\nSince we already know what score we are going to get, we don't really need to make a naive baseline guess. Let's use a slightly more sophisticated model for our actual baseline: Logistic Regression.","a22289d9":"## Conclusions\n\nIn this notebook, we saw how to get started with a Kaggle machine learning competition. We first made sure to understand the data, our task, and the metric by which our submissions will be judged. Then, we performed a fairly simple EDA to try and identify relationships, trends, or anomalies that may help our modeling. Along the way, we performed necessary preprocessing steps such as encoding categorical variables, imputing missing values, and scaling features to a range. Then, we constructed new features out of the existing data to see if doing so could help our model.\n\nOnce the data exploration, data preparation, and feature engineering was complete, we implemented a baseline model upon which we hope to improve. Then we built a second slightly more complicated model to beat our first score. We also carried out an experiment to determine the effect of adding the engineering variables.\n\nWe followed the general outline of a machine learning project:\n\n1. Understand the problem and the data\n2. Data cleaning and formatting (this was mostly done for us)\n3. Exploratory Data Analysis\n4. Baseline model\n5. Improved model\n6. Model interpretation (just a little)\n\nMachine learning competitions do differ slightly from typical data science problems in that we are concerned only with achieving the best performance on a single metric and do not care about the interpretation. However, by attempting to understand how our models make decisions, we can try to improve them or examine the mistakes in order to correct the errors. In future notebooks we will look at incorporating more sources of data, building more complex models (by following the code of others), and improving our scores.\n\nI hope this notebook was able to get you up and running in this machine learning competition and that you are now ready to go out on your own - with help from the community - and start working on some great problems!\n\n**Running the notebook**: now that we are at the end of the notebook, you can hit the blue Commit & Run button to execute all the code at once. After the run is complete (this should take about 10 minutes), you can then access the files that were created by going to the versions tab and then the output sub-tab. The submission files can be directly submitted to the competition from this tab or they can be downloaded to a local machine and saved. The final part is to share the share the notebook: go to the settings tab and change the visibility to Public. This allows the entire world to see your work!","97f87e47":"The test set is considerably smaller and lacks a Target column.","7b0bf179":"### Domain Knowledge Features\n\nMaybe It's not entirely correct to call this 'domain knowledge' because i'm not a credit expert, but perhaps we could call this 'attempts at applying limited financial knowledge'. In this frame of mind, we can make a couple features that attempt to capture what we think may be important for telling whether a client will default on a loan. Here I'm going to use five features that were inspired by this script by Agular:\n\n* CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\n\n* ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income\n\n* CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due\n\n* DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age","d5d99bcd":"### Examine the Distribution of the Target Column\n\nThe target is what we are asked to predict: either a 0 for the loan was reparied on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.","a6c1912e":"### Label Encoding and One-Hoe Encoding\n\nLet's implement the policy described above: for any categorical variable (dtype == object) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique  categories, we will use one-hot encoding.\n\nFor label encoding, we use the scikit-learn labelencoder and for one-hot encoding, the pandas get_dummies(df) function","98283877":"### Improved Model: random forest\n\nTo try and beat the poor performance of our baseline, we can update the algorithm. Let's try using a Random Forest on the same training data to see how that affects performance. The Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest.","b1eaf691":"When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can handle missing values with no need for imputation. Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now.","26f015aa":"### Data\n\nThe data is proveided by Home Credit, a service dedicated to provided lines of credit (loans) to the unbanked population. Predicting whether of not a client will repay a loan or have difficulty is a critical business need, and Home Credit is hosting this competition on Kaggle to see what sort of models the machine learning community can develop to help them in this task.\n\nThese are 7 different sources of data:\n\n* application_train\/application_test: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training applicaton data comes with the TARGET indicating 0: the loan was repaid of 1: the loan was not repaid.\n\n* bureau: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n\n* precious_application: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n\n* POS_CASH_BALANCE: monthly data about previous point of sale of cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cahs loan, and s single previous loan can have many rows.\n\n* credit_card_balance: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n\n* installments_payment: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment.","cf648199":"Let's now look at the number of unique entries in each of the *object* (categorical) columns.","60cabdea":"1. \ub370\uc774\ud130\uc5d0 \uace0\uc720\uac12\uc774 \ubb34\uc5c7\uc774 \uc788\ub294\uc9c0 \uc54c\uace0 \uc2f6\ub2e4\uba74 unique \n\n2. \ucd1d \uace0\uc720\uac12\uc758 \uc218\uac00 \uba87 \uac1c\uc778\uc9c0 \uc54c\uace0 \uc2f6\ub2e4\uba74 nunique \n(default) \uacb0\uce21\uac12 \ud558\ub098\uc758 \uac12\uc73c\ub85c \ubbf8\ud655\uc778 >> dropna=True\n\n3. \uac12\ubcc4\ub85c \ub370\uc774\ud130\uc758 \uac1c\uc218\ub97c \uc54c\uace0 \uc2f6\ub2e4\uba74 value_counts\nhttps:\/\/mizykk.tistory.com\/103","6a05b124":"Moreover, we are provided with the definitions of all the columns (in HomeCredit_columns_description.csv) and an example of the expected submission file.\n\nIn this notebook, we will stick to using only the main application training and testing data. Although if we want to have any hope of seriously competing, we need to use all the data, for now we will stick to one file which should be more manageable. This will let us establish a baseline that we can then improve upon. With these projects, it's best to build up an understanding of the problems a little at a time rather than diving all the way in and getting completely lost.","4bc332ec":"### Column Types\n\nlet's look at the number of columns of each data type. *inf64* and *float64* are numerica variables (which can be either discrete or continuous). *object* columns contatin strings and are categorical features.","63848812":"The training and testing datasets now have the same features which is required for machine learning. The number of features has grown significantly due to one-hot encoding. At some point we probably will want to try dimensionality reduction to reduce the size of the datasets.","3f927540":"### Metric: ROC AUC\n\n#### explain url: https:\/\/bskyvision.com\/1165\n\nOnce we have a grasp of the data (reading through the column descriptions helps immensely), we need to understand the metric by which our submission is judged. In this case, it is a common classification metric known as the Receiver Operating Characteristic Area under the Curve (ROC AUC, also sometimes called AUROC).\n\nThe ROC AUC may sound intimidating, but is is relatively straighforward once you can get your heard around the two individual concepts. The Reciever operating Characterisitic (ROC) curve graphs the true positive rate versus the false positive rate:\n\n![image.png](attachment:a769d027-6a71-4ab2-bd27-ac446d7da72b.png)\n\nA single line on the top graph indicates the curve for a single model, and movement along a line indicates changing the threshold used for classifying a positive instance. The threshold starts at 0 in the upper right to and goes to 1 in the lower left. A curve that is to left and above another curve indicates a better model. For example, the blue model i better than the red model, which is better than the black diagonal line which indicates a better model. For example, the blue model is better than the red model, which is better than the black diagonal line which indicates a naive random guessing model.\n\nThe Area under the curve(AUC) explains itself by its name. It is simply the area under the ROC curve. (This is the integral of the curve.) This metric is between 0 and 1 with a better model scoring higher. A model that simply guesses at random will have an ROC AUC of 0.5.\n\nWhen we measure a classifier according to the ROC AUC, we do not generation 0 or 1 predictions, but rather a probability between 0 and 1. This may be confusing because we usually like to think in terms of accuracy, but when we get into problems with inbalanced classes (we will se this is the case), accuracy is not the best metric. For example, if I wanted to build a model that could detect terrorists with 99.999% accuracy, I would simply make a model that predicted every single person was not a terrorist. Clearly, this wolud not be effective (the recall would be zero) and we use more advanced metrics such as ROC AUC or the F1 score to more accurately reflect the performance of a classifier. A model with a high ROC AUC will also have a high accuracy, but the ROC AUC is a better representation of model performance.\n\n","dc0b1c01":"### Make Predictions using Engineered Features\n\nThis only way to see if the Polynomial Features and Domain knowledge improved the model is to train a test a model on these features! We can then compare the submission performance to that for the model without these features to gauge the effect of our feature engineering.","2aeef96d":"The submission has now been saved to the virtual environment in which our notebook is running. To access the submission, at the end of the notebook, we will hit the blue Commit & Run button at the upper right of the kernel. This runs the entire notebook and then lets us download any files that are created during the run.\n\nOnce we run the notebook, the files created are available in the Versions tab under the Output sub-tab. From here, the submission files can be submitted to the competition or downloaded. Since there are several models in this notebook, there will be multiple output files.\n\n**The logistic regression baseline should score around 0.671 when submitted.**","08bb806b":"EXT_SOURCE_3 displays the greatest difference between the values of the target. We can clearly see that this feature has some relationship to the likelihood of an applicant to repay a loan. The relationship is not very strong, in fact they are all considered very weak, but these variables will still be useful for a machine learning model to predict whether or not an applicant will repay a loan on time.","90232e05":"### Exterior Sources\n\nThe 3 variables with the strongest negative correlations wtih the target are *ext_source_1*, *ext_source_2*, and *ext_source_3*. According to the documentation, these features represents a 'normalized score from external data source'. I'm not sure with this exactly means, but it may be a cumulative sort of credit rating made using numerous sources of data.\n\nLet's take a look at these variables.\n\nFirst, we can show the correlations of the *ext_source* features with the target and with each other.","6d994705":"Several of new variables have a greater (in terms of absolute magnitude) correlation with the target than the original features. When we build machine learning models, we can try with and without these features to determine if they actually help the model learn.\n\nWe will add these features to a copy of the training and testing data and than evaluate models with and without the features. Many times in machine learning, the only way to know if an approach will work is to try it out!","6c007328":"The training has 307511 observations (each one a seperate loan) and 122 features (variables) including the TARGET (the label we want to predict).","dc3af754":"### Read in Data\n\nFirst, we can list all the available data files. There are a total of 9 lines: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 6 other files containing additional information about each loan.","adcfcef1":"There are 35 features with individual features raised to powers up to degree 3 and interaction terms. Now, we can see whether any of these new features are correlated with the target.","293d27a5":"This creates a considerable number of new features. To get the names we have to use the polynomial features get_feature_names method.","cfc100da":"As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often.\n\nLet's start looking at this varialbe. First, we can make a histogram of the age. We will put the x axis in years to make the plot a little more understandable.","6a0baafc":"From this information, we see this is an [imbalanced class problem](http:\/\/www.chioka.in\/class-imbalance-problem\/). There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance.","1892940b":"### Pairs Plot\n\nAs a final exploratroy plot, we can make a pairs plot of the EXT_SOURCE vaiables and the DAYS_BIRTH variable. The pairs Plot is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle."}}