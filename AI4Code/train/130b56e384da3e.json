{"cell_type":{"8e80dac4":"code","43db90cc":"code","96f60afb":"code","1c2eae7b":"code","c4374aec":"code","b8ad0de4":"code","6fa95338":"code","632cdd1b":"code","ed4564bd":"code","530ac31a":"code","24c3f1df":"code","86a32baf":"code","f30a91cf":"code","6b41af4e":"code","fc5f5f33":"code","085970ca":"code","decfb2ac":"code","696e68a1":"code","d8fbad30":"code","0646d32e":"code","297e49e3":"code","3fd0ab38":"markdown","89ab84f4":"markdown","6bae2b89":"markdown","c51234ca":"markdown","4ac2b7be":"markdown","d6be90cc":"markdown","adbbe727":"markdown","d0d47d4c":"markdown","aca8a12a":"markdown","35871cbd":"markdown","f382dd6f":"markdown","43a37f75":"markdown","4d190aa3":"markdown","9d288f9e":"markdown","0e827489":"markdown","0ea376dd":"markdown","b7e41871":"markdown","a3701106":"markdown","914f1bd3":"markdown","c9b16722":"markdown"},"source":{"8e80dac4":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import recall_score, f1_score, accuracy_score, precision_score,\\\n    confusion_matrix, precision_recall_curve, auc\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import SMOTE\n\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")","43db90cc":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.shape","96f60afb":"df.columns","1c2eae7b":"df.info()","c4374aec":"df.describe()","b8ad0de4":"vc = df.Class.value_counts()\nsns.barplot(x=vc.index, y=vc.values, data=vc)\nprint(vc)\nprint(vc\/vc.sum())","6fa95338":"fig, ax = plt.subplots(nrows=6, ncols=5, figsize=(20,20))\nfor i in range(28):\n    sns.distplot(df[df['Class'] == 0][f'V{i+1}'], ax=ax[i\/\/5,i%5], label='non-fraud')\n    sns.distplot(df[df['Class'] == 1][f'V{i+1}'], ax=ax[i\/\/5,i%5], label='fraud')\n    ax[i\/\/5,i%5].legend()\nsns.distplot(df[df['Class'] == 0]['Time'], ax=ax[5,3], label='non-fraud')\nsns.distplot(df[df['Class'] == 1]['Time'], ax=ax[5,3], label='fraud')\nax[5,3].legend()\nsns.distplot(df[df['Class'] == 0]['Amount'], ax=ax[5,4], label='non-fraud')\nsns.distplot(df[df['Class'] == 1]['Amount'], ax=ax[5,4], label='fraud')\nax[5,4].legend()","632cdd1b":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\nsns.scatterplot(x='V17', y='V14', hue='Class', data=df, ax=ax[0])\nsns.scatterplot(x='V12', y='V14', hue='Class', data=df, ax=ax[1])","ed4564bd":"corr = df.corr()\ncorr.Class","530ac31a":"class OneWaySkewReduction(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom transformer to reduce skewness. The process consists in\n    squaring the signal before using the box-cox transformation.\n    \"\"\"\n    def __init__(self):\n        # Stores the best box-cox lambda after fitting\n        self._params = {}\n    \n    def fit(self, X, y=None):\n        # For each column in the training data, find the lambda\n        # that maximizes the log-likelihood in the box-cox transf.\n        for col in X:\n            self._params[col] = None\n            _, self._params[col] = self.skew_reduction(X[col])\n        return self\n    \n    def transform(self, X, y=None):\n        _X = X.copy()\n        for col in X:\n            _X[col] = self.skew_reduction(X[col], self._params[col])\n        return _X\n    \n    def skew_reduction(self, X, lmbda=None):\n        # Custom transformation function\n        return stats.boxcox(np.power(X, 2) + np.finfo(float).eps, lmbda=lmbda)","24c3f1df":"X = df.drop('Class', axis=1)\ny = df['Class']\n\n# Find which coolumns in the training set have abs(skew) > 0.75\nskewed_cols = X.columns[np.where(X.skew().abs() > 0.75)]\n\n# Create the custom transformer\ntransformer = make_column_transformer(\n    (StandardScaler(), ['Time']),\n    (OneWaySkewReduction(), skewed_cols.values),\n    remainder='drop'\n)\n\n# Replace the raw values in X\nX[pd.Index(['Time']).append(skewed_cols)] = transformer.fit_transform(X)","86a32baf":"Xtr, Xte, Ytr, Yte = train_test_split(X, y, test_size=0.20,\\\n                                      random_state=0, stratify=y)","f30a91cf":"trvc = Ytr.value_counts()\ntevc = Yte.value_counts()\ntrvc = trvc\/trvc.sum()\ntevc = tevc\/tevc.sum()\nprint(\"Training dataset proportion\")\nprint(trvc)\nprint(\"\\nTesting dataset proportion\")\nprint(tevc)","6b41af4e":"smote = SMOTE(sampling_strategy='minority')\nXsm, Ysm = smote.fit_sample(Xtr, Ytr)\nXsm.shape","fc5f5f33":"# Metrics for comparison\nmtrs = {'Recall': recall_score, 'Accuracy': accuracy_score,\\\n        'Precision': precision_score, 'F1': f1_score}\n\n# Models (set random_state for reproducibility)\nmdls = {'LR Smote': LogisticRegression(random_state=0),\n        'LR Imb.': LogisticRegression(random_state=0),\n        'RF Smote': RandomForestClassifier(random_state=0),\n        'RF Imb.': RandomForestClassifier(random_state=0),\n        'AB Smote': AdaBoostClassifier(random_state=0),\n        'AB Imb.': AdaBoostClassifier(random_state=0)}","085970ca":"# Fit models using either the origintal training set or the\n# SMOTE-balanced one\nfor m in mdls:\n    if m.endswith('Smote'):\n        mdls[m].fit(Xsm, Ysm)\n    else:\n        mdls[m].fit(Xtr, Ytr)","decfb2ac":"# Compute the scoring metrics in the same dataset used for training\ntr_res = []\nfor m in mdls:\n    if m.endswith('Smote'):\n        Ypr = mdls[m].predict(Xsm)\n        tr_res.append([mtrs[mt](Ysm, Ypr) for mt in mtrs])\n    else:\n        Ypr = mdls[m].predict(Xtr)\n        tr_res.append([mtrs[mt](Ytr, Ypr) for mt in mtrs])\npd.DataFrame(tr_res, columns=mtrs.keys(), index=mdls.keys())","696e68a1":"te_res = {}\ncf_mat = {}\nfor m in mdls:\n    Ypr = mdls[m].predict(Xte)\n    te_res[m] = [mtrs[mt](Yte, Ypr) for mt in mtrs]\n    cf_mat[m] = confusion_matrix(Yte, Ypr)","d8fbad30":"fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(20,12))\nticks = ['Non-fraud','Fraud']\nk = 0\nfor i in cf_mat:\n    with sns.plotting_context('notebook', font_scale=1.5):\n        sns.heatmap(cf_mat[i]\/np.sum(cf_mat[i]), annot=True,\\\n                fmt='.2%', cmap='Blues', cbar=False, xticklabels=ticks,\\\n                yticklabels=ticks, ax=ax[k\/\/3,k%3], )\n    ax[k\/\/3,k%3].set_ylabel('True label', fontsize=18)\n    ax[k\/\/3,k%3].set_xlabel('Predicted label', fontsize=18)\n    ax[k\/\/3,k%3].set_title(i, fontsize=20)\n    k = k + 1","0646d32e":"pd.DataFrame(te_res, index=mtrs.keys()).T","297e49e3":"# Compute the precision-recall curve\nrpcurve = {}\nfor m in mdls:\n    Ysc = mdls[m].predict_proba(Xte)[:,1]\n    precision, recall, threshold = precision_recall_curve(Yte, Ysc)\n    rpcurve[m] = pd.DataFrame(np.vstack((precision, recall)).T, columns=['Precision', 'Recall'])\n\n# Plot the precision-recall curve\nfig, ax = plt.subplots(figsize=(10,5))\nfor m in mdls:\n    area = auc(rpcurve[m].Recall, rpcurve[m].Precision)\n    sns.lineplot(x='Recall', y='Precision', data=rpcurve[m], ax=ax, label=f'{m} ({area})')","3fd0ab38":"There are **284807** rows with **31** variables where only three of them are known: `Time`, `Amount` and `Class`. The other variables are not displayed due to privacy reasons.","89ab84f4":"The **Random Forest** classifier was the method that performed theh better in all metrics. It is possible to observe that the Recall in the imbalanced training dataset was low for two classifiers concerning the SMOTE-balanced dataset.","6bae2b89":"## 1. Exploratory analysis\n<a id=\"expana\"><\/a>\nThe first thing we'll do is a quick exploratory analysis to gather some information on our dataset.","c51234ca":"For some features, e.g. `V15`, the distribution of frauds is very similar to the distribution of non-frauds. If we consider that having different distributions for fraud and non-fraud cases will make a good predictor, this visual assessment can aid in choosing a subset of the features for the classification problem. Plotting `V14` against `V17` and `V12` it is possible to see some separability.","4ac2b7be":"## 2. Preprocessing\n<a id=\"prepro\"><\/a>\n\nThe preprocessing will be composed by:\n* Scaling the `Time` and `Amount` features\n* Skew reduction for every features with `abs(skew) > 0.75`\n* Balancing the training dataset using SMOTE technique","d6be90cc":"The confusion matrix for each classifier shows that the **Random Forest** classifier has an overall better performance than **Logistic Regression** and **Adaboost**.","adbbe727":"### 2.2. Spliting dataset\n<a id=\"spldts\"><\/a>\n\nWe'll create training and testing datasets keeping the proportion of fraud and non-fraud cases. From now on, the test dataset will only be used in the end of the kernel to check the trined models. We'll use 80% of the dataset for training and 20% for testing.","d0d47d4c":"All the features from `V1` to `V28` were previously transformed using PCA as stated in the dataset description.","aca8a12a":"As also said in the dataset description, this is a highly imbalanced set, with **99.83%** of non-fraud transactions with only **0.17%** of frauds.","35871cbd":"Although `cross_val_score` could be used here to choose which algorithm to use or even to optimize the parameters of each method with `GridSearchCV` or `RandomizedSearchCV`, I chose to continue this Notebook with pre-defined parameters and testing all three algorithms. Except for the RandomForest algorithm, using the SMOTE balancing procedure increased the F1 score in the training dataset. Will that be the case for the testing set?\n\n### 3.1. Results in the testing data\n<a id=\"rsltes\"><\/a>\n\nNow we'll predict the classes in the testing dataset.","f382dd6f":"### 2.3. Training dataset balancing (SMOTE)\n<a id=\"trnbal\"><\/a>\n\nWe'll keep the training data of non-fraud transactions without making any undersampling and will create other fraud samples using the SMOTE technique to achieve a 50%-50% proportion.","43a37f75":"# Credit Fraud Problem\n\n## 1. Introduction\n\nThis problem presents a highly imbalanced dataset. The main objective of this kernel is to test a balancing and skew reduction procedure in various predictive models to see if they can improve some performance metrics.\n\n### 1.1. Specific goals\n* Understand the dataset through an exploratory analysis to see each variable distribution segmented by the label (fraud and non-fraud).\n* Reduce the skewness of some variables.\n* Split the entire dataset into training and testing subframes keeping the original proportion of fraud and non-fraud in both subsets.\n* Use the SMOTE algorithm to balance the training dataset creating a 50%-50% set.\n* Test three known classifiers, namely RandomForest, AdaBoost, and LogisticRegressor.\n* Check their performance with Recall, Precision and F1 score.\n* Analyse the precision-recall curves to find which algorithm has the maximum area under them.\n\n### 1.2. Outline\n* [1. Exploratory analysis](#expana)\n* [2. Preprocessing](#prepro)\n  * [2.1. Skew reduction](#skwred)\n  * [2.2. Spliting dataset](#spldts)\n  * [2.3. Training dataset balancing (SMOTE)](#trnbal)\n* [3. Model fitting and results](#mdlfit)\n  * [3.1. Results in the testing data](#rsltes)\n* [4. Conclusion](#conclus)\n\n\n### 1.3. References\n* [https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets)\n* [https:\/\/www.kaggle.com\/joparga3\/in-depth-skewed-data-classif-93-recall-acc-now](https:\/\/www.kaggle.com\/joparga3\/in-depth-skewed-data-classif-93-recall-acc-now)\n* [https:\/\/towardsdatascience.com\/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156](https:\/\/towardsdatascience.com\/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156)\n* [https:\/\/medium.com\/@simonprdhm\/9-scikit-learn-tips-for-data-scientist-2a84ffb385ba](https:\/\/medium.com\/@simonprdhm\/9-scikit-learn-tips-for-data-scientist-2a84ffb385ba)","4d190aa3":"This can also be seen in the Pearson correlation coefficieints between the features and the output `Class`.","9d288f9e":"## 4. Conclusion\n<a id=\"conclus\"><\/a>\n\nWe dealt with imbalacing using the SMOTE algorithm to oversample the minority class and used a preprocessing technique to reduce the skewness in the data. These procedures were able to improve the F1 score in approximately **6%** when a default RandomForest classifier was used. In this same classifier, the SMOTE procedure helped to decrease the false-negative rate from **0.06%** to **0.04%** which is important in this problem, since we want to able to detect all fraud cases. A false positive in this problem can have a less financial impact on the company than a false negative. The same preprocessing procedures were responsible for the worse performances of LogisticRegression and AdaBoost techniques.\n\nThe following points are still worth discussing:\n* Are there other balancing procedures that will improve performance regardless the classifier?\n* Are there variables that can be removed to help improve classification results?\n* Is skew reduction needed in this dataset?","0e827489":"The use of `stratify` in `train_test_split` function allow us to keep roughly the same original imbalance in the training and testing datasets.","0ea376dd":"Another useful information is that the dataset doesn't have any `NaN` values, meaning we won't need to make any data imputation. Except for the output `Class`, there aren't any categorical features as well.","b7e41871":"It is interesting to see that altough the SMOTE procedure have improved all methods in the training phase, is led to very poor performances in two algorithms. It was able to improve **Recall**, which is very important for fraud detection in credit cards but at the expense of a very low **Precision**, also an important metric in this problem. Nevertheless, the SMOTE procedure combined with the RandomForest classifier was able to produce the best results among all tested combinations.\n\nAs suggested by the dataset description, we'll also check the the Precision-Recall curves for all methods. Again the **Random Forest** achieved a greater area under the precision-recall curve.","a3701106":"## 3. Model fitting and results\n<a id=\"mdlfit\"><\/a>\n\nWe will fit three models to the balanced training dataset and check their performance with the testing set: **logistic regression**, **random forest** and **adaboost**. Since, the testing dataset was created to maintain the original imbalance, accuracy is not a good metric to measure the performance of each method. We'll use the recall, precision and f1-score to do it.","914f1bd3":"This is also shown in the **F1 score**:","c9b16722":"### 2.1. Skew reduction\n<a id=\"skwred\"><\/a>\n\nWe'll create a custom transformer to apply our skew reduction method. Then, we'll apply this transformer to the varibles whose absolute skew is greater than 0.75."}}