{"cell_type":{"568a39f7":"code","283a6842":"code","04df8407":"code","f3f105e5":"code","ffa1fb11":"code","5112f22e":"code","0780a14d":"code","2e195e0c":"code","10302570":"code","2a21ba71":"code","28f7855f":"code","872eaa02":"code","277ad573":"code","9b97d8e1":"code","6280f773":"code","68fe68bd":"code","d02a2f0b":"code","35ada7b7":"code","5e2f2a3f":"code","6f2453e7":"code","5fc0444b":"code","397d9394":"code","4d60241c":"code","0dfc4736":"code","ec5bad4d":"code","b9b6ef87":"code","eec6ee34":"code","0717f8e7":"code","2d3af830":"code","9def3098":"code","229f865f":"code","ffb73df9":"code","29f77a1e":"markdown","2dfe8e68":"markdown","0e262074":"markdown","531a3588":"markdown","d56b96dc":"markdown","0e2d51f3":"markdown","16aa2b57":"markdown"},"source":{"568a39f7":"# install some python package.\n!pip install visdom","283a6842":"import os\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport visdom\nfrom graphviz import Digraph\nimport time\n%matplotlib inline\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as T\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nimport PIL\nfrom PIL import Image\nimport cv2\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve\n\n!ls ..\/input","04df8407":"face_path = '..\/input\/pins-face-recognition\/pins\/PINS'\npeople_list = os.listdir(face_path)\ndisplay(len(people_list), people_list[:5])\n\nperson2index = {person:i for i, person in enumerate(people_list)}\nindex2person = {i:person for i, person in enumerate(people_list)}\ndisplay(person2index, index2person)","f3f105e5":"# \u8ba9\u6211\u4eec\u770b\u4e0b\u4e0a\u9762\u7f16\u53f7\u6587\u4ef6\u5939\u4e0b\u6709\u4ec0\u4e48\nperson_path = os.path.join(face_path, people_list[0])\nperson_img_list = os.listdir(person_path)\ndisplay(person_img_list)","ffa1fb11":"%%time\nn = 3\nfig, ax = plt.subplots(n, n, figsize=(12, 12))\nfor i in range(n):\n    for j in range(n):\n        img_path = os.path.join(person_path, person_img_list[i*n + j])\n        img = plt.imread(img_path)\n        ax[i][j].imshow(img)","5112f22e":"%%time\n# \u5efa\u7acb\u4e00\u4e2adataframe\uff0c\u7528\u6765\u5efa\u7acb\u6570\u636e\u5230\u6807\u7b7e\u7684\u5173\u7cfb\uff0c\u65b9\u4fbf\u5efa\u7acbdataset\n# \u5176\u5b9e\u4f7f\u7528fastai\u7684from_folder\u66f4\u5feb\u7684\u5efa\u7acbdatabunch\uff0c\u4f46\u662f\u8fd9\u91cc\u4e3a\u4e86\u953b\u70bc\u80fd\u529b\u8d77\u89c1\uff0c\u6211\u51b3\u5b9a\u4f7f\u7528\u539f\u59cb\u7684\u65b9\u6cd5\u6765\u6784\u5efadataset\n# create the dataframe\nperson_df = pd.DataFrame()\nfor person in tqdm(os.listdir(face_path)):\n    person_imgs_path = os.path.join(face_path, person)\n    for img in os.listdir(person_imgs_path):\n        img_file_path = os.path.join(person_imgs_path, img)\n        label = person\n        dict_person = {'path':img_file_path, 'label':label}\n        person_df = person_df.append(pd.Series(dict_person), ignore_index=True)","0780a14d":"# \u5212\u5206\u6570\u636e\u96c6\uff0c\u91c7\u7528sklearn\u7684StratifiedKFold\u6765\u505a\u5904\u7406\n# \u4e09\u5927\u90aa\u62db\uff1aTTA\u3001Pseudo Label\u3001Ensemble\n# \u6211\u4e0d\u60f3\u7528\uff0c\u53ea\u6709\u5feb\u901f\u7684\u7b97\u6cd5\u624d\u6709\u610f\u4e49.\n\ndisplay(person_df.head(), f'the length of data is {len(person_df)}')\n \nx, y = person_df['path'], person_df['label']\n\nn_splits = 3\nFolds = StratifiedKFold(n_splits=n_splits)\n\ntrain_list = []\nfor train, val in Folds.split(x, y):\n    display(f'the length of train is {len(train)}, the length of val is {len(val)}')\n    train_list.append([train, val])","2e195e0c":"# \u6211\u4eec\u5728\u8fd9\u91cc\u53ef\u4ee5\u4f7f\u7528torch\u7684dataset\u6765\u5b9a\u4e49\u6570\u636e\uff0c\u5f53\u7136\u53ef\u4ee5\u4f7f\u7528\u9884\u8bfb\u53d6\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u6570\u636e\u7684\u8bfb\u53d6\u901f\u5ea6\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u4f7f\u7528\u6765\u8282\u7ea6\u5185\u5b58\nheight, width = 128, 128\ndata_transforms = T.Compose([\n        T.Resize(136),\n        T.CenterCrop(height),\n        T.RandomHorizontalFlip(),\n        T.ToTensor(),\n#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\nclass face_data(Dataset):\n    def __init__(self, df=person_df, transform=data_transforms):\n        super(face_data, self).__init__()\n        self.df = df\n        self.transform = transform\n        self.c = 100\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path, person = self.df.iloc[index, 1], self.df.iloc[index, 0]\n        target = person2index[person]\n#         img_data = cv2.imread(img_path)\n        img_data = PIL.Image.open(img_path)\n#         print(img_data.size)\n        data = self.transform(img_data)\n#         print(data.size)\n        return data, target","10302570":"# \u5148\u6d4b\u8bd5\u4e0bdataset\uff0c\u7136\u540e\u5728\u7edf\u4e00\u7684model\u3001loss\u548coptimizer\u4e0b\uff0c\u5bf9\u4e0d\u540c\u7684dataset\u8fdb\u884c\u8bad\u7ec3\uff0c\u6700\u7ec8\u5f97\u5230\u4e00\u4e2a\u6700\u597d\u7684\u7ed3\u679c\n# \u6a21\u578b\u4e5f\u53ef\u4ee5\u5728\u4ea4\u53c9\u9a8c\u8bc1\u96c6\u4e2d\u8fdb\u884c\u591a\u6b21\u7684\u8bad\u7ec3\ntrain_index_0, val_index_0 = train_list[0]\ntrain_df_0, val_df_0 = person_df.iloc[train_index_0], person_df.iloc[val_index_0]","2a21ba71":"display(train_df_0.shape, val_df_0.shape)\n\n# \u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\ntrain_dataset = face_data(df = train_df_0) \nval_dataset = face_data(df = val_df_0)\n\nbatch_size = 32\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)","28f7855f":"iter_dataload = iter(train_dataloader)\n# see the dataloader' data and label\nfor data, label in iter_dataload:\n#     display(data.size())\n#     image = torchvision.transforms.ToPILImage()(data[0])\n#     plt.imshow(image)\n#     display(label[0], index2person[label[0].item()])\n    img = torchvision.utils.make_grid(data).numpy()\n    img = np.transpose(img, (1, 2, 0))\n    plt.figure(figsize=(16, 9))\n    plt.imshow(img)\n    break","872eaa02":"!mkdir checkpoints\n!ls","277ad573":"# The code for configuration management.\nclass Config(object):\n    '''\n    the object is a good example which I have ever seen.\n    '''\n    env = 'default'\n    backbone = 'resnet18'   # The backbone can be changed to any module. I think efficientnet can be used for better feature extraction.\n    classify = 'softmax'    # sphereface, cosface and arcface are based on the modification of softmax.\n    num_classes = len(people_list)     # The number of person in the casia dataset.\n    metric = 'arc_margin'   # I think we can compose three margin.\n    easy_margin = False   \n    use_se = False          # Squeeze and Excitation submodule. The great idea is proposed in 2017. The 1st place in ILSVRC competetion.\n    loss = 'focal_loss'     # The loss for the easy and hard sample. And we can use weight for imblance sample.\n\n    display = False\n    finetune = False        # I think the finetune operation can get better results.\n\n#     train_root = '\/data\/Datasets\/webface\/CASIA-maxpy-clean-crop-144\/' # The face images path\n#     train_list = '\/data\/Datasets\/webface\/train_data_13938.txt'        # The file has the train data description\n#     val_list = '\/data\/Datasets\/webface\/val_data_13938.txt'            # The file has the test data description\n\n#     test_root = '\/data1\/Datasets\/anti-spoofing\/test\/data_align_256'   # The aligned face for recognition\n#     test_list = 'test.txt'\n\n#     lfw_root = '\/data\/Datasets\/lfw\/lfw-align-128'\n#     lfw_test_list = '\/data\/Datasets\/lfw\/lfw_test_pair.txt'\n\n    # For saving the model.\n    checkpoints_path = 'checkpoints'\n    load_model_path = 'models\/resnet18.pth'\n    test_model_path = 'checkpoints\/resnet18_40.pth'\n    save_interval = 10\n\n    train_batch_size = batch_size  # batch size\n    test_batch_size = 60\n\n    # The shape of  a face. But in some competetion the input_shape may be (6, 256, 256)\n    input_shape = (3, height, width)\n\n    optimizer = 'sgd'\n\n    use_gpu = True  # use GPU or not\n    gpu_id = '0'     # we train the model in the kaggle kernel, so we only have one gpu.\n    num_workers = 2   # how many workers for loading data\n    print_freq = 100  # print info every N batch\n\n    # You can set import ipdb\n    debug_file = '\/tmp\/debug'  # if os.path.exists(debug_file): enter ipdb\n    result_file = 'result.csv'\n\n    max_epoch = 50\n    lr = 1e-1  # initial learning rate\n    lr_step = 10\n    lr_decay = 0.95  # when val_loss increase, lr = lr*lr_decay\n    weight_decay = 5e-4","9b97d8e1":"class Visualizer(object):\n    def __init__(self, env='default', **kwargs):\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        self.vis.close()\n\n        self.iters = {}\n        self.lines = {}\n\n    def display_current_results(self, iters, x, name='train_loss'):\n        if name not in self.iters:\n            self.iters[name] = []\n\n        if name not in self.lines:\n            self.lines[name] = []\n\n        self.iters[name].append(iters)\n        self.lines[name].append(x)\n\n        self.vis.line(X=np.array(self.iters[name]),\n                      Y=np.array(self.lines[name]),\n                      win=name,\n                      opts=dict(legend=[name], title=name))\n\n    def display_roc(self, y_true, y_pred):\n        fpr, tpr, ths = roc_curve(y_true, y_pred)\n        self.vis.line(X=fpr,\n                      Y=tpr,\n                      # win='roc',\n                      opts=dict(legend=['roc'],\n                                title='roc'))","6280f773":"# a test network for visulization.\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.out = nn.Linear(32*7*7, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n#         print(x.size())\n        x = x.view(x.size(0), -1)  # (batch, 32*7*7)\n        out = self.out(x)\n        return out\n    \ninput_size = (32, 1, 28, 28)\nx = torch.rand(input_size)\nnet = CNN()\ny = net(x)\ny.size()","68fe68bd":"# visualize the cnn network\ndef make_dot(var, params=None):\n    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n    Blue nodes are the Variables that require grad, orange are Tensors\n    saved for backward in torch.autograd.Function\n    Args:\n        var: output Variable\n        params: dict of (name, Variable) to add names to node that\n            require grad (TODO: make optional)\n    \"\"\"\n    if params is not None:\n        assert isinstance(params.values()[0], Variable)\n        param_map = {id(v): k for k, v in params.items()}\n\n    node_attr = dict(style='filled',\n                     shape='box',\n                     align='left',\n                     fontsize='12',\n                     ranksep='0.1',\n                     height='0.2')\n    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n    seen = set()\n\n    def size_to_str(size):\n        return '('+(', ').join(['%d' % v for v in size])+')'\n\n    def add_nodes(var):\n        if var not in seen:\n            if torch.is_tensor(var):\n                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n            elif hasattr(var, 'variable'):\n                u = var.variable\n                name = param_map[id(u)] if params is not None else ''\n                node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n            else:\n                dot.node(str(id(var)), str(type(var).__name__))\n            seen.add(var)\n            if hasattr(var, 'next_functions'):\n                for u in var.next_functions:\n                    if u[0] is not None:\n                        dot.edge(str(id(u[0])), str(id(var)))\n                        add_nodes(u[0])\n            if hasattr(var, 'saved_tensors'):\n                for t in var.saved_tensors:\n                    dot.edge(str(id(t)), str(id(var)))\n                    add_nodes(t)\n    add_nodes(var.grad_fn)\n    return dot\n\n\ndef view_model(net, input_shape):\n    x = Variable(torch.randn(1, *input_shape))\n    y = net(x)\n    g = make_dot(y)\n    g.render('.\/net')\n    display(g)\n\n    params = list(net.parameters())\n    k = 0\n    for i in params:\n        l = 1\n        print(\"layer parameters size:\" + str(list(i.size())))\n        for j in i.size():\n            l *= j\n        print(\"layer parameters:\" + str(l))\n        k = k + l\n    print(\"total parameters:\" + str(k))\n\n\n# net = CNN()\n# input_shape = (1, 28, 28)\n# view_model(net, input_shape)\nx = Variable(torch.randn(1, 1, 28, 28))\ny = net(x)\ng = make_dot(y)\ng.render('.\/cnn')\ndisplay(g)\n#\n# params = list(net.parameters())\n# k = 0\n# for i in params:\n#     l = 1\n#     print(\"layer parameters:\" + str(list(i.size())))\n#     for j in i.size():\n#         l *= j\n#     print(\"layer parameters:\" + str(l))\n#     k = k + l\n# print(\"total parameters:\" + str(k))","d02a2f0b":"!ls","35ada7b7":"resnet18 = torchvision.models.resnet18(pretrained=True)\ninput_shape = (3, 224, 224)\nview_model(resnet18, input_shape)","5e2f2a3f":"# -*- coding: utf-8 -*-\nclass FocalLoss(nn.Module):\n\n    def __init__(self, weight=None, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.ce = torch.nn.CrossEntropyLoss(weight=weight)\n\n    def forward(self, input, target):\n        logp = self.ce(input, target)\n        p = torch.exp(-logp)\n        loss = (1 - p) ** self.gamma * logp\n        return loss.mean()\n    \n# Test focal loss function \n# The input is the input tensor and target tensor\n# The shape of input is (batch_size, C), andn the shape of target is (batch_size)","6f2453e7":"from __future__ import print_function\nfrom __future__ import division\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport math\n\n\nclass ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device=device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        # print(output)\n\n        return output","5fc0444b":"class AddMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin cosine distance: :\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        s: norm of input feature\n        m: margin\n        cos(theta) - m\n    \"\"\"\n\n    def __init__(self, in_features, out_features, s=30.0, m=0.40):\n        super(AddMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        phi = cosine - self.m\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cosine.size(), device=device)\n        # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        # print(output)\n\n        return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' \\\n               + 'in_features=' + str(self.in_features) \\\n               + ', out_features=' + str(self.out_features) \\\n               + ', s=' + str(self.s) \\\n               + ', m=' + str(self.m) + ')'","397d9394":"class SphereProduct(nn.Module):\n    r\"\"\"Implement of large margin cosine distance: :\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        m: margin\n        cos(m*theta)\n    \"\"\"\n    def __init__(self, in_features, out_features, m=4):\n        super(SphereProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.m = m\n        self.base = 1000.0\n        self.gamma = 0.12\n        self.power = 1\n        self.LambdaMin = 5.0\n        self.iter = 0\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform(self.weight)\n\n        # duplication formula\n        self.mlambda = [\n            lambda x: x ** 0,\n            lambda x: x ** 1,\n            lambda x: 2 * x ** 2 - 1,\n            lambda x: 4 * x ** 3 - 3 * x,\n            lambda x: 8 * x ** 4 - 8 * x ** 2 + 1,\n            lambda x: 16 * x ** 5 - 20 * x ** 3 + 5 * x\n        ]\n\n    def forward(self, input, label):\n        # lambda = max(lambda_min,base*(1+gamma*iteration)^(-power))\n        self.iter += 1\n        self.lamb = max(self.LambdaMin, self.base * (1 + self.gamma * self.iter) ** (-1 * self.power))\n\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))\n        cos_theta = cos_theta.clamp(-1, 1)\n        cos_m_theta = self.mlambda[self.m](cos_theta)\n        theta = cos_theta.data.acos()\n        k = (self.m * theta \/ 3.14159265).floor()\n        phi_theta = ((-1.0) ** k) * cos_m_theta - 2 * k\n        NormOfFeature = torch.norm(input, 2, 1)\n\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cos_theta.size())\n        one_hot = one_hot.cuda() if cos_theta.is_cuda else one_hot\n        one_hot.scatter_(1, label.view(-1, 1), 1)\n\n        # --------------------------- Calculate output ---------------------------\n        output = (one_hot * (phi_theta - cos_theta) \/ (1 + self.lamb)) + cos_theta\n        output *= NormOfFeature.view(-1, 1)\n\n        return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' \\\n               + 'in_features=' + str(self.in_features) \\\n               + ', out_features=' + str(self.out_features) \\\n               + ', m=' + str(self.m) + ')'","4d60241c":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 18-5-21 \u4e0b\u53485:26\n\n@author: ronghuaiyang\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn.utils.weight_norm as weight_norm\nimport torch.nn.functional as F\n\n\n# __all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n#            'resnet152']\n\n\nmodel_urls = {\n    'resnet18': 'https:\/\/download.pytorch.org\/models\/resnet18-5c106cde.pth',\n    'resnet34': 'https:\/\/download.pytorch.org\/models\/resnet34-333f7ec4.pth',\n    'resnet50': 'https:\/\/download.pytorch.org\/models\/resnet50-19c8e357.pth',\n    'resnet101': 'https:\/\/download.pytorch.org\/models\/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https:\/\/download.pytorch.org\/models\/resnet152-b121ed2d.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass IRBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True):\n        super(IRBlock, self).__init__()\n        self.bn0 = nn.BatchNorm2d(inplanes)\n        self.conv1 = conv3x3(inplanes, inplanes)\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.prelu = nn.PReLU()\n        self.conv2 = conv3x3(inplanes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.use_se = use_se\n        if self.use_se:\n            self.se = SEBlock(planes)\n\n    def forward(self, x):\n        residual = x\n        out = self.bn0(x)\n        out = self.conv1(out)\n        out = self.bn1(out)\n        out = self.prelu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.use_se:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.prelu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n                nn.Linear(channel, channel \/\/ reduction),\n                nn.PReLU(),\n                nn.Linear(channel \/\/ reduction, channel),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\nclass ResNetFace(nn.Module):\n    def __init__(self, block, layers, use_se=True):\n        self.inplanes = 64\n        self.use_se = use_se\n        super(ResNetFace, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.prelu = nn.PReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.bn4 = nn.BatchNorm2d(512)\n        self.dropout = nn.Dropout()\n        self.fc5 = nn.Linear(512 * 8 * 8, 512)\n        self.bn5 = nn.BatchNorm1d(512)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, use_se=self.use_se))\n        self.inplanes = planes\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, use_se=self.use_se))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.prelu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.bn4(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc5(x)\n        x = self.bn5(x)\n\n        return x\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n        #                        bias=False)\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=2)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        # self.avgpool = nn.AvgPool2d(8, stride=1)\n        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n        self.fc5 = nn.Linear(512 * 8 * 8, 512)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        # x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        # x = nn.AvgPool2d(kernel_size=x.size()[2:])(x)\n        # x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc5(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n    return model\n\n\ndef resnet_face18(use_se=True, **kwargs):\n    model = ResNetFace(IRBlock, [2, 2, 2, 2], use_se=use_se, **kwargs)\n    return model\n\n\n# Test resnet_face18\nx = torch.randn(2, 3, 128, 128)\nnet = resnet_face18()\ny = net(x)\ndisplay(y.size())","0dfc4736":"from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils import data\nimport torch.nn.functional as F\nimport torchvision\nimport torch\nimport numpy as np\nimport random\nimport time\nfrom torch.nn import DataParallel\nfrom torch.optim.lr_scheduler import StepLR\n\ndef save_model(model, save_path, name, iter_cnt):\n    save_name = os.path.join(save_path, name + '_' + str(iter_cnt) + '.pth')\n    torch.save(model.state_dict(), save_name)\n    return save_name","ec5bad4d":"opt = Config()\ndir(opt)\nopt.use_gpu","b9b6ef87":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 18-5-30 \u4e0b\u53484:55\n\n@author: ronghuaiyang\n\"\"\"\nfrom __future__ import print_function\nimport os\nimport cv2\nimport torch\nimport numpy as np\nimport time\nfrom torch.nn import DataParallel\n\n\ndef get_lfw_list(pair_list):\n    with open(pair_list, 'r') as fd:\n        pairs = fd.readlines()\n    data_list = []\n    for pair in pairs:\n        splits = pair.split()\n\n        if splits[0] not in data_list:\n            data_list.append(splits[0])\n\n        if splits[1] not in data_list:\n            data_list.append(splits[1])\n    return data_list\n\n\ndef load_image(img_path):\n    image = cv2.imread(img_path, 0)\n    if image is None:\n        return None\n    image = np.dstack((image, np.fliplr(image)))\n    image = image.transpose((2, 0, 1))\n    image = image[:, np.newaxis, :, :]\n    image = image.astype(np.float32, copy=False)\n    image -= 127.5\n    image \/= 127.5\n    return image\n\n\ndef get_featurs(model, test_list, batch_size=10):\n    images = None\n    features = None\n    cnt = 0\n    for i, img_path in enumerate(test_list):\n        image = load_image(img_path)\n        if image is None:\n            print('read {} error'.format(img_path))\n\n        if images is None:\n            images = image\n        else:\n            images = np.concatenate((images, image), axis=0)\n\n        if images.shape[0] % batch_size == 0 or i == len(test_list) - 1:\n            cnt += 1\n\n            data = torch.from_numpy(images)\n            data = data.to(torch.device(\"cuda\"))\n            output = model(data)\n            output = output.data.cpu().numpy()\n\n            fe_1 = output[::2]\n            fe_2 = output[1::2]\n            feature = np.hstack((fe_1, fe_2))\n            # print(feature.shape)\n\n            if features is None:\n                features = feature\n            else:\n                features = np.vstack((features, feature))\n\n            images = None\n\n    return features, cnt\n\n\ndef load_model(model, model_path):\n    model_dict = model.state_dict()\n    pretrained_dict = torch.load(model_path)\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict)\n\n\ndef get_feature_dict(test_list, features):\n    fe_dict = {}\n    for i, each in enumerate(test_list):\n        # key = each.split('\/')[1]\n        fe_dict[each] = features[i]\n    return fe_dict\n\n\ndef cosin_metric(x1, x2):\n    return np.dot(x1, x2) \/ (np.linalg.norm(x1) * np.linalg.norm(x2))\n\n\ndef cal_accuracy(y_score, y_true):\n    y_score = np.asarray(y_score)\n    y_true = np.asarray(y_true)\n    best_acc = 0\n    best_th = 0\n    for i in range(len(y_score)):\n        th = y_score[i]\n        y_test = (y_score >= th)\n        acc = np.mean((y_test == y_true).astype(int))\n        if acc > best_acc:\n            best_acc = acc\n            best_th = th\n\n    return (best_acc, best_th)\n\n\ndef test_performance(fe_dict, pair_list):\n    with open(pair_list, 'r') as fd:\n        pairs = fd.readlines()\n\n    sims = []\n    labels = []\n    for pair in pairs:\n        splits = pair.split()\n        fe_1 = fe_dict[splits[0]]\n        fe_2 = fe_dict[splits[1]]\n        label = int(splits[2])\n        sim = cosin_metric(fe_1, fe_2)\n\n        sims.append(sim)\n        labels.append(label)\n\n    acc, th = cal_accuracy(sims, labels)\n    return acc, th\n\n\ndef lfw_test(model, img_paths, identity_list, compair_list, batch_size):\n    s = time.time()\n    features, cnt = get_featurs(model, img_paths, batch_size=batch_size)\n    print(features.shape)\n    t = time.time() - s\n    print('total time is {}, average time is {}'.format(t, t \/ cnt))\n    fe_dict = get_feature_dict(identity_list, features)\n    acc, th = test_performance(fe_dict, compair_list)\n    print('lfw face verification accuracy: ', acc, 'threshold: ', th)\n    return acc","eec6ee34":"if opt.display:\n    visualizer = Visualizer()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n\n# train_dataset = Dataset(opt.train_root, opt.train_list, phase='train', input_shape=opt.input_shape)\n# trainloader = data.DataLoader(train_dataset,\n#                               batch_size=opt.train_batch_size,\n#                               shuffle=True,\n#                               num_workers=opt.num_workers)\ntrainloader = train_dataloader\n\n# identity_list = get_lfw_list(opt.lfw_test_list)\n# img_paths = [os.path.join(opt.lfw_root, each) for each in identity_list]\n\nprint('{} train iters per epoch:'.format(len(trainloader)))\n\nif opt.loss == 'focal_loss':\n    criterion = FocalLoss(gamma=2)\nelse:\n    criterion = torch.nn.CrossEntropyLoss()\n\nif opt.backbone == 'resnet18':\n    model = resnet_face18(use_se=opt.use_se)\nelif opt.backbone == 'resnet34':\n    model = resnet34()\nelif opt.backbone == 'resnet50':\n    model = resnet50()\n\nif opt.metric == 'add_margin':\n    metric_fc = AddMarginProduct(512, opt.num_classes, s=30, m=0.35)\nelif opt.metric == 'arc_margin':\n    metric_fc = ArcMarginProduct(512, opt.num_classes, s=30, m=0.5, easy_margin=opt.easy_margin)\nelif opt.metric == 'sphere':\n    metric_fc = SphereProduct(512, opt.num_classes, m=4)\nelse:\n    metric_fc = nn.Linear(512, opt.num_classes)\n\n# view_model(model, opt.input_shape)\nprint(model)","0717f8e7":"model.to(device)\nmodel = DataParallel(model)\nmetric_fc.to(device)\nmetric_fc = DataParallel(metric_fc)\n\nif opt.optimizer == 'sgd':\n    optimizer = torch.optim.SGD([{'params': model.parameters()}, {'params': metric_fc.parameters()}],\n                                lr=opt.lr, weight_decay=opt.weight_decay)\nelse:\n    optimizer = torch.optim.Adam([{'params': model.parameters()}, {'params': metric_fc.parameters()}],\n                                 lr=opt.lr, weight_decay=opt.weight_decay)\nscheduler = StepLR(optimizer, step_size=opt.lr_step, gamma=0.1)","2d3af830":"device","9def3098":"start = time.time()\nfor i in range(opt.max_epoch):\n    scheduler.step()\n\n    model.train()\n    for ii, data in enumerate(trainloader):\n        data_input, label = data\n        data_input = data_input.to(device)\n        label = label.to(device).long()\n        feature = model(data_input)\n        output = metric_fc(feature, label)\n        loss = criterion(output, label)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        iters = i * len(trainloader) + ii\n\n        if iters % opt.print_freq == 0:\n            output = output.data.cpu().numpy()\n            output = np.argmax(output, axis=1)\n            label = label.data.cpu().numpy()\n            # print(output)\n            # print(label)\n            acc = np.mean((output == label).astype(int))\n            speed = opt.print_freq \/ (time.time() - start)\n            time_str = time.asctime(time.localtime(time.time()))\n            print('{} train epoch {} iter {} {} iters\/s loss {} acc {}'.format(time_str, i, ii, speed, loss.item(), acc))\n            if opt.display:\n                visualizer.display_current_results(iters, loss.item(), name='train_loss')\n                visualizer.display_current_results(iters, acc, name='train_acc')\n\n            start = time.time()\n\n    if i % opt.save_interval == 0 or i == opt.max_epoch:\n        save_model(model, opt.checkpoints_path, opt.backbone, i)\n\n    model.eval()\n#     acc = lfw_test(model, img_paths, identity_list, opt.lfw_test_list, opt.test_batch_size)\n    if opt.display:\n        visualizer.display_current_results(iters, acc, name='test_acc')","229f865f":"!ls checkpoints\t","ffb73df9":"opt = Config()\nif opt.backbone == 'resnet18':\n    model = resnet_face18(opt.use_se)\nelif opt.backbone == 'resnet34':\n    model = resnet34()\nelif opt.backbone == 'resnet50':\n    model = resnet50()\n\nmodel = DataParallel(model)\n# load_model(model, opt.test_model_path)\nmodel.load_state_dict(torch.load(opt.test_model_path))\nmodel.to(device)\n\n# identity_list = get_lfw_list(opt.lfw_test_list)\n# img_paths = [os.path.join(opt.lfw_root, each) for each in identity_list]\n\nmodel.eval()\n# lfw_test(model, img_paths, identity_list, opt.lfw_test_list, opt.test_batch_size)","29f77a1e":"- 1. I want to read the code .\n- 2. Do some modification in the code.","2dfe8e68":"# 1 EDA\n\nI want to understand the code of ArcFace with pytorch and I like to explain why.","0e262074":"The focal loss is based on cross entropy loss.\n\n$$\n\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n           = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\n$$\n\nor in the case of the :attr:`weight` argument being specified:\n\n$$\n\\text{loss}(x, class) = weight[class] \\left(-x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\\right)\n$$\n\nExamples::\n\n    >>> loss = nn.CrossEntropyLoss()\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n    >>> output = loss(input, target)\n    >>> output.backward()","531a3588":"## The data class\nThe main modification we can do.\n\nSo we should understand our own face dataset. And write our own dataset class","d56b96dc":"The Script is about Arcface. I will do some explaining with code.","0e2d51f3":"# 2 Model","16aa2b57":"- ArcMarginProduct (Arcface)\n- AddMarginProduct (Cosface)\n- SphereProduct (Sphereface)"}}