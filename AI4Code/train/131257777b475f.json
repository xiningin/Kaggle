{"cell_type":{"3e845948":"code","7098b30e":"code","e665aaff":"code","20cc69b3":"code","338ab713":"code","cebbd163":"code","b734b92b":"code","2e5333f4":"code","43f82e58":"code","198727a1":"code","327f45e6":"code","60cdfac0":"code","4e976ef1":"code","fbb3072b":"code","d7a8e5a1":"code","235ec97a":"code","cbcd09ca":"code","0261ee8c":"code","0c655d4d":"code","58799167":"code","41a57cd3":"code","463377ff":"code","7b107aae":"markdown","7a7ff3aa":"markdown","462015ef":"markdown","a92b4278":"markdown","e7b03464":"markdown","49159d03":"markdown","c6f888cc":"markdown","c46335e3":"markdown","3438fe97":"markdown","b3007eb1":"markdown","3b16bd29":"markdown","b8a8dd6a":"markdown","9db95884":"markdown","c14fe2ba":"markdown","b9bde5cc":"markdown","71b2acc5":"markdown","22a843ac":"markdown","c91913fb":"markdown","20c5a437":"markdown"},"source":{"3e845948":"input_path = '..\/input\/bike-sharing-dataset'","7098b30e":"import numpy as np","e665aaff":"bikes_numpy = np.loadtxt(\"..\/input\/bike-sharing-dataset\/hour.csv\", \n                         dtype=np.float32,\n                         delimiter=\",\",\n                         skiprows=1,\n                         converters={1: lambda x:float(x[8:10])})\n","20cc69b3":"import torch","338ab713":"bikes = torch.from_numpy(bikes_numpy)","cebbd163":"bikes","b734b92b":"import pandas as pd","2e5333f4":"bikes_df = pd.read_csv('..\/input\/bike-sharing-dataset\/hour.csv')","43f82e58":"bikes_df.head()","198727a1":"len(bikes_df[:-3])","327f45e6":"bikes.shape, bikes.stride()","60cdfac0":"daily_bikes = bikes[:-3].view(-1, 24, bikes[:-3].shape[1])\ndaily_bikes.shape, daily_bikes.stride()","4e976ef1":"daily_bikes = daily_bikes.transpose(1,2)\ndaily_bikes.shape, daily_bikes.stride()","fbb3072b":"first_day = bikes[:24].long()\nweather_onehot = torch.zeros(first_day.shape[0], 4)\nfirst_day[:, 9]","d7a8e5a1":"weather_onehot.scatter_(\n    dim=1,\n    index=first_day[:,9].unsqueeze(1).long() -1,\n    value=1.0\n)","235ec97a":"torch.cat((bikes[:24], weather_onehot), 1)[:1]","cbcd09ca":"daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4, daily_bikes.shape[2])\ndaily_weather_onehot.shape","0261ee8c":"daily_weather_onehot.scatter_(\n    1, daily_bikes[:,9,:].long().unsqueeze(1) -1 , 1.0\n)\ndaily_weather_onehot.shape","0c655d4d":"daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1)","58799167":"daily_bikes[:,9,:] = (daily_bikes[:,9,:] - 1.0)\/3.0","41a57cd3":"temp = daily_bikes[:, 10, :]\ntemp_min = torch.min(temp)\ntemp_max = torch.max(temp)\ndaily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - temp_min)\n\/ (temp_max - temp_min))","463377ff":"temp = daily_bikes[:, 10, :]\ndaily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - torch.mean(temp))\n\/ torch.std(temp))\n","7b107aae":"This notebook is sourced from ch4 Deep learning with Pytorch","7a7ff3aa":"In the latter case, our variable will have 0 mean and unitary standard deviation. If our\nvariable were drawn from a Gaussian distribution, 68% of the samples would sit in the\n[-1.0, 1.0] interval.\n\nGreat: we\u2019ve built another nice dataset, and we\u2019ve seen how to deal with time series\ndata. For this tour d\u2019horizon, it\u2019s important only that we got an idea of how a time\nseries is laid out and how we can wrangle the data in a form that a network will digest","462015ef":"Then we scatter the one-hot encoding into the tensor in the C dimension. Since this\noperation is performed in place, only the content of the tensor will change:","a92b4278":"As we mentioned in the previous section, rescaling variables to the [0.0, 1.0] interval\nor the [-1.0, 1.0] interval is something we\u2019ll want to do for all quantitative variables,\nlike temperature (column 10 in our dataset). We\u2019ll see why later; for now, let\u2019s just say\nthat this is beneficial to the training process.\n\nThere are multiple possibilities for rescaling variables. We can either map their\nrange to [0.0, 1.0]","e7b03464":"note : the version of file originally used was  hour fixed version not the one we would be using ","49159d03":"For daily_bikes, the stride is telling us that advancing by 1 along the hour dimension (the second dimension) requires us to advance by 17 places in the storage (or\none set of columns); whereas advancing along the day dimension (the first dimension) requires us to advance by a number of elements equal to the length of a row in\nthe storage times 24 (here, 408, which is 17 \u00d7 24).\n We see that the rightmost dimension is the number of columns in the original\ndataset. Then, in the middle dimension, we have time, split into chunks of 24 sequential hours. In other words, we now have N sequences of L hours in a day, for C channels. To get to our desired N \u00d7 C \u00d7 L ordering, we need to transpose the tensor:","c6f888cc":"calling view on a tensor returns a new tensor that changes the number of dimensions and the striding information, without\nchanging the storage. This means we can rearrange our tensor at basically zero cost,\nbecause no data will be copied. Our call to view requires us to provide the new shape\nfor the returned tensor. We use -1 as a placeholder for \u201chowever many indexes are\nleft, given the other dimensions and the original number of elements.","c46335e3":"or subtract the mean and divide by the standard deviation:\n","3438fe97":"# Shaping the data by time period\n\n","b3007eb1":" Let\u2019s go back to our bike-sharing dataset. The first column is the index (the global\nordering of the data), the second is the date, and the sixth is the time of day. We have\neverything we need to create a dataset of daily sequences of ride counts and other\nexogenous variables. Our dataset is already sorted, but if it were not, we could use\ntorch.sort on it to order it appropriately","3b16bd29":"We might want to break up the two-year dataset into wider observation periods, like\ndays. This way we\u2019ll have N (for number of samples) collections of C sequences of length\nL. In other words, our time series dataset would be a tensor of dimension 3 and shape\nN \u00d7 C \u00d7 L. The C would remain our 17 channels, while L would be 24: 1 per hour of\nthe day. There\u2019s no particular reason why we must use chunks of 24 hours, though the\ngeneral daily rhythm is likely to give us patterns we can exploit for predictions. We\ncould also use 7 \u00d7 24 = 168 hour blocks to chunk by week instead, if we desired. All of\nthis depends, naturally, on our dataset having the right size\u2014the number of rows must\nbe a multiple of 24 or 168. Also, for this to make sense, we cannot have gaps in the\ntime series","b8a8dd6a":"thats 17379 hours and 17 columns.\n\nchoosing till bikes[:-3] since 17376 is devided by 24","9db95884":"concatenate along C dimension","c14fe2ba":"We mentioned earlier that this is not the only way to treat our \u201cweather situation\u201d variable. Indeed, its labels have an ordinal relationship, so we could pretend they are special values of a continuous variable. We could just transform the variable so that it runs\nfrom 0.0 to 1.0:","b9bde5cc":"lets do something about the weather situation,if we turn it into categorical then we can turn it into a one hot encoded vector","71b2acc5":"In a time series dataset such as this one, rows represent successive time-points: there is\na dimension along which they are ordered. Sure, we could treat each row as independent and try to predict the number of circulating bikes based on, say, a particular time\nof day regardless of what happened earlier. However, the existence of an ordering\ngives us the opportunity to exploit causal relationships across time. For instance, it\nallows us to predict bike rides at one time based on the fact that it was raining at an\nearlier time. For the time being, we\u2019re going to focus on learning how to turn our\nbike-sharing dataset into something that our neural network will be able to ingest in\nfixed-size chunks.","22a843ac":"Here we prescribed our original bikes dataset and our one-hot-encoded \u201cweather situation\u201d matrix to be concatenated along the column dimension (that is, 1). In other\nwords, the columns of the two datasets are stacked together; or, equivalently, the new\none-hot-encoded columns are appended to the original dataset. For cat to succeed, it\nis required that the tensors have the same size along the other dimensions\u2014the row\ndimension, in this case. Note that our new last four columns are 1, 0, 0, 0, exactly\nas we would expect with a weather value of 1.\nWe could have done the same with the reshaped daily_bikes tensor. Remember\nthat it is shaped (B, C, L), where L = 24. We first create the zero tensor, with the same\nB and L, but with the number of additional columns as C:","c91913fb":"Now we scatter ones into our matrix according to the corresponding level at each row, we need to use unsqueeze for this","20c5a437":"Our day started with weather \u201c1\u201d and ended with \u201c2,\u201d so that seems right.\nLast, we concatenate our matrix to our original dataset using the cat function.\nLet\u2019s look at the first of our results"}}