{"cell_type":{"641e2089":"code","ef76315a":"code","ac3d4171":"code","02a3ee25":"code","fb1529cd":"code","ada1fec5":"code","e41d9dd8":"code","3a460080":"code","96af8b71":"code","42d2c237":"code","1ade1129":"code","5bfff13a":"code","8c5af96d":"code","92a8c4bf":"code","fac87373":"code","90bad155":"code","8adb33f7":"code","76713cda":"code","b776799b":"code","8a644cdc":"code","e2c3982b":"code","905ba27a":"code","a03a20e5":"code","54f5772c":"code","831d39e3":"code","1d130a16":"code","08e945b5":"code","b52c2cfe":"markdown","baa76017":"markdown","2efa6ec3":"markdown","c6dc696c":"markdown","a1a0cc01":"markdown","98a2f19f":"markdown","7f08e206":"markdown","7d4094bd":"markdown","f214d397":"markdown","a3656f92":"markdown","4f752f6c":"markdown","6693db76":"markdown"},"source":{"641e2089":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ef76315a":"# Step 1 is to import both data sets\ntraining_data = pd.read_csv(\"..\/input\/train.csv\")\ntesting_data = pd.read_csv(\"..\/input\/test.csv\")\ntraining_data.shape, testing_data.shape","ac3d4171":"# Step two is to create columns which I will add to the respective datasets, \n# in order to know which row came from which dataset when I combine the datasets\n\ntraining_column = pd.Series([1] * len(training_data))\ntesting_column = pd.Series([0] * len(testing_data))\ntraining_column.shape, testing_column.shape","02a3ee25":"# Now we append them by creating new columns in the original data. We use the same column name\ntraining_data['is_training_data'] = training_column\ntesting_data['is_training_data'] = testing_column\ntraining_data.shape, testing_data.shape","fb1529cd":"# Now we can merge the datasets while retaining the key to split them later\ncombined_data = training_data.append(testing_data, ignore_index=True, sort=False)\ncombined_data.shape, combined_data['is_training_data'].unique()","ada1fec5":"# Let's double-check the 1s and 0s for the is_training_data column just to make sure it was implemented correctly\nfrom collections import Counter\nis_training_data_count = pd.DataFrame([Counter(combined_data['is_training_data']).keys(), Counter(combined_data['is_training_data']).values()])\nis_training_data_count.head()\n\n# Looks good!","e41d9dd8":"# Encode gender (if == female, True)\ncombined_data['female'] = combined_data.Sex == 'female'\n\n# Split out Title\ntitle = []\nfor i in combined_data['Name']:\n    period = i.find(\".\")\n    comma = i.find(\",\")\n    title_value = i[comma+2:period]\n    title.append(title_value)\ncombined_data['title'] = title\n\n# Replace the title values with an aliased dictionary\ntitle_arr = pd.Series(title)\ntitle_dict = {\n    'Mr' : 'Mr', \n    'Mrs' : 'Mrs',\n    'Miss' : 'Miss',\n    'Master' : 'Master',\n    'Don' : 'Formal',\n    'Dona' : 'Formal',\n    'Rev' : 'Religious',\n    'Dr' : 'Academic',\n    'Mme' : 'Mrs',\n    'Ms' : 'Miss',\n    'Major' : 'Formal',\n    'Lady' : 'Formal',\n    'Sir' : 'Formal',\n    'Mlle' : 'Miss',\n    'Col' : 'Formal',\n    'Capt' : 'Formal',\n    'the Countess' : 'Formal',\n    'Jonkheer' : 'Formal',\n}\ncleaned_title = title_arr.map(title_dict)\ncombined_data['cleaned_title'] = cleaned_title\n\n# Fill NaN of Age - first create groups to find better medians than just the overall median. \ngrouped = combined_data.groupby(['female','Pclass', 'cleaned_title'])  \n\n# And now fill NaN with the grouped medians\ncombined_data['Age'] = grouped.Age.apply(lambda x: x.fillna(x.median()))\n\n# Fill NaN of Embarked\ncombined_data['Embarked'] = combined_data['Embarked'].fillna(\"S\") \n\n# Fill NaN of Fare\ncombined_data['Fare'] = combined_data['Fare'].fillna(combined_data['Fare'].mode()[0]) \n\n# Fill NaN of Cabin with a U for unknown. Not sure cabin will help.\ncombined_data['Cabin'] = combined_data['Cabin'].fillna(\"U\") \n\n# Finding cabin group\ncabin_group = []\nfor i in combined_data['Cabin']:\n    cabin_group.append(i[0])\ncombined_data['cabin_group'] = cabin_group\n\n# Adding a family_size feature as it may have an inverse relationship to either of its parts\ncombined_data['family_size'] = combined_data.Parch + combined_data.SibSp + 1\n\n# Mapping ports to passenger pickup order\nport = {\n    'S' : 1,\n    'C' : 2,\n    'Q' : 3\n}\ncombined_data['pickup_order'] = combined_data['Embarked'].map(port)\n\n# Encode childhood\ncombined_data['child'] = combined_data.Age < 16\n\n# One-Hot Encoding the titles\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['cleaned_title'], prefix=\"C_T\")], axis = 1)\n\n# One-Hot Encoding the Pclass\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['Pclass'], prefix=\"PClass\")], axis = 1)\n\n# One-Hot Encoding the  cabin group\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['cabin_group'], prefix=\"C_G\")], axis = 1)\n\n# One-Hot Encoding the ports\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['Embarked'], prefix=\"Embarked\")], axis = 1)","3a460080":"combined_data.shape","96af8b71":"combined_data.isnull().sum()","42d2c237":"combined_data.head()","1ade1129":"# Now we split the data again\nnew_train_data=combined_data.loc[combined_data['is_training_data']==1]\nnew_test_data=combined_data.loc[combined_data['is_training_data']==0]\nnew_train_data.shape, new_test_data.shape","5bfff13a":"new_train_data.describe()","8c5af96d":"new_test_data.describe()","92a8c4bf":"# here is the expanded model set\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","fac87373":"# # This cell is just a reminder to research GridSearchCV at some point\n# from sklearn.model_selection import GridSearchCV ","90bad155":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits = 10, shuffle=True, random_state=0)","8adb33f7":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix","76713cda":"# For a first run, I'll try using the following features\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'child', 'Embarked_S', 'Embarked_C', \n            'Embarked_Q', 'pickup_order', 'C_T_Academic', 'C_T_Formal', 'C_T_Master', 'C_T_Miss', 'C_T_Mr', \n            'C_T_Mrs', 'C_T_Religious', 'C_G_A', 'C_G_B', 'C_G_C', 'C_G_D', 'C_G_E', 'C_G_F', 'C_G_G', \n            'C_G_T', 'C_G_U', 'family_size', 'PClass_1', 'PClass_2', 'PClass_3']\ntarget = 'Survived'","b776799b":"cvs_train_data = new_train_data[features]\ncvs_test_data = new_test_data[features]\ncvs_target = new_train_data['Survived']\n\ncvs_train_data.shape, cvs_test_data.shape","8a644cdc":"# Define the models\n################################################################################################### NOTE - I cannot justify setting random_state=0 for any reason other than reproducability of results\nmodel01 = RandomForestClassifier(n_estimators=10, random_state=0);\nmodel02 = DecisionTreeClassifier(random_state=0);\nmodel03 = LogisticRegression(solver='liblinear');\nmodel04 = KNeighborsClassifier(n_neighbors=15)\nmodel05 = GaussianNB()\nmodel06 = SVC(gamma='auto')\n\n# # Fit the models\n# model01.fit(new_train_data[features], new_train_data[target]);\n# model02.fit(new_train_data[features], new_train_data[target]);\n# model03.fit(new_train_data[features], new_train_data[target]);\n\n# Define a function to make reading recall score easier\ndef printCVPRAF(model_number):\n    print(\"CrossVal Precision: \", round(np.mean(cross_val_score(model_number, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='precision'))*100,2), \"%\")\n    print(\"CrossVal Recall: \", round(np.mean(cross_val_score(model_number, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='recall'))*100,2), \"%\")\n    print(\"CrossVal Accuracy: \", round(np.mean(cross_val_score(model_number, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2), \"%\")\n    print(\"CrossVal F1-Score: \", round(np.mean(cross_val_score(model_number, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='f1_macro'))*100,2), \"%\")\n    y_pred = cross_val_predict(model_number, cvs_train_data, cvs_target, cv=k_fold)\n    conf_mat = confusion_matrix(cvs_target, y_pred)\n    rows = ['Actually Died', 'Actually Lived']\n    cols = ['Predicted Dead','Predicted Lived']\n    print(\"\\n\",pd.DataFrame(conf_mat, rows, cols))\n    \n# Print results\nprint(\"Random Forest Classifier\")\nprintCVPRAF(model01);\nprint(\"\\n\\nDecision Tree Classifier\")\nprintCVPRAF(model02);\nprint(\"\\n\\nLogistic Regression\")\nprintCVPRAF(model03);\nprint(\"\\n\\nKNeighborsClassifier\")\nprintCVPRAF(model04);\nprint(\"\\n\\nGaussianNB\")\nprintCVPRAF(model05);\nprint(\"\\n\\nSVC\")\nprintCVPRAF(model06);","e2c3982b":"# Let's just print score here again to have it nearby\nprintCVPRAF(model03);","905ba27a":"# First we fit the model\nmodel03.fit(cvs_train_data, cvs_target)","a03a20e5":"# Then predict\nprediction = model03.predict(cvs_test_data)\nprediction.shape","54f5772c":"# Now let's make a dataframe for submission\nsubmission = pd.DataFrame({\n    \"PassengerId\" : new_test_data['PassengerId'],\n    \"Survived\" : prediction.astype(int)\n})","831d39e3":"# And send it to a csv\nsubmission.to_csv('submission.csv', index=False)","1d130a16":"# We can read the CSV again to check it\nsubmission_check = pd.read_csv('submission.csv')\nsubmission_check.head(), submission_check.describe()","08e945b5":"# Since Random Forest is a close second in accuracy, let's do that too, to see if it translates better to the competition\nmodel01.fit(cvs_train_data, cvs_target)\nprediction2 = model01.predict(cvs_test_data)\nsubmission2 = pd.DataFrame({\n    \"PassengerId\" : new_test_data['PassengerId'],\n    \"Survived\" : prediction2.astype(int)\n})\nsubmission2.to_csv('submission2.csv', index=False)\n\n# Whoa, it did *not* lol. Okay, further improvements will come in a new kernel.","b52c2cfe":"# Replacing the NaN fillers\nThe first thing I'm going to do is replace the way I fill NaNs. Instead of using only the training data, I will use train and test data combined. ","baa76017":"Now it looks like accuracy is the most important metric in this competition, so I will use Logistic Regression to predict the values of the test set.","2efa6ec3":"# Import Cross Val Predict so we can visualize Confusion Matrices","c6dc696c":"# I want to declare features \nAfter a first run, I will look at declaring features separately per model","a1a0cc01":"# Split the data back to where it was\nThis involves splitting on the is_training_data feature I added","98a2f19f":"# Exporting results of Logistic Regression","7f08e206":"# I'll send the results to the competition using the cleaned up code in the other \"3\" kernel.","7d4094bd":"# And now I'll run the models","f214d397":"# Use K-Fold instead of train_test_split","a3656f92":"However, due to using cross_val_score, I need to drop all the columns I'm not using. (?? maybe??)","4f752f6c":"# Include new models\nI want to use more and different models","6693db76":"## Now that the data is all together, we can replace the NaNs with more complete information\nThis is all of what was done in the last set of kernels."}}