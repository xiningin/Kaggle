{"cell_type":{"bdd6813c":"code","85175df6":"code","a50d158e":"code","da467770":"code","1fac3be5":"code","ee88df21":"code","13a01b39":"code","d54646a1":"code","c813904f":"code","34a97527":"code","c98ac008":"code","fe34d732":"code","b24a3608":"code","9c019ce7":"code","5af4a597":"code","57116bc0":"code","0c4d9347":"code","0960373b":"code","947f78a6":"code","001be067":"code","5309e889":"markdown","b2d4fe70":"markdown","0f99e2c2":"markdown","534ad0ff":"markdown","97af4ad8":"markdown","e750ee94":"markdown","a29ed669":"markdown","d0a8cc8d":"markdown","3774975d":"markdown","29126b16":"markdown","54b3de68":"markdown","994245b7":"markdown"},"source":{"bdd6813c":"import warnings\nwarnings.filterwarnings('ignore')\n\n#the basics\nimport pandas as pd, numpy as np\nimport math, re, gc, random, os, sys\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\n#tensorflow deep learning basics\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\n\n#for model evaluation\nfrom sklearn.model_selection import train_test_split, KFold","85175df6":"cfg = {\n    \n    'training_params': {\n        'method' : 'random',\n        'epochs' : 30,\n        'skf_folds' : 5,\n        'train_val_split' : .2,\n        'skf_repeats' : 1,\n        'random_repeats' : 20,\n        'feature_selection' : None,\n        'batch_size' : 64,\n        'seed' : 34,\n        'verbose' : 2\n    },\n    \n    \n    'model_params': {\n        'layers': 2,\n        'nodes': [1024, 1024],\n        'activations': [tf.keras.activations.swish] * 2,\n        'batch_norms': [True, True, True],\n        'weight_norms' : [True, True, True],\n        'dropouts' : [True, True, True],\n        'dropout_rates' : [.2, .4, .4]\n    }\n}","a50d158e":"#set how many hidden layers\ncfg['model_params']['layers'] = 3\n\n#select node count of layer\ncfg['model_params']['nodes'] = [1024, 512, 256]\n\n#select where to place dropout\ncfg['model_params']['dropouts'] = [1, 0, 0, 1]\n\n#select rate of dropouts\ncfg['model_params']['dropout_rates'] = [.2, 0, 0, .4]\n\n#select where to apply batch norm\ncfg['model_params']['batch_norms'] = [1, 1, 1, 1]\n\n#select where to apply weight norm\ncfg['model_params']['weight_norms'] = [0, 0, 0]\n\n#select swish activation for all layers\ncfg['model_params']['activations'] = [tf.keras.activations.swish] * 3","da467770":"def build_model(cfg, num_columns):\n    \n    #define for convenience\n    model_cfg = cfg['model_params']\n    \n    #initialize empty model\n    model = tf.keras.Sequential()\n    \n#############################################################\n#### Hidden layers\n#############################################################\n\n    for layer in range(model_cfg['layers']):\n\n        #add batch norm before activation\n        if model_cfg['batch_norms'][layer] and not model_cfg['dropouts'][layer]:\n            model.add(tf.keras.layers.BatchNormalization(input_shape=(num_columns,)))\n        \n        #add batch norm before activation, then dropout\n        if model_cfg['batch_norms'][layer] and model_cfg['dropouts'][layer]:\n            model.add(tf.keras.layers.BatchNormalization(input_shape=(num_columns,)))\n            model.add(tf.keras.layers.Dropout(model_cfg['dropout_rates'][layer]))\n            \n        #add only dropout without batch norm\n        if not model_cfg['batch_norms'][layer] and model_cfg['dropouts'][layer]:\n            model.add(tf.keras.layers.Dropout(model_cfg['dropout_rates'][layer],\n                      input_shape=(num_columns,)))\n            \n        #add activation layer with weight normalization\n        if model_cfg['weight_norms'][layer]:\n            model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(model_cfg['nodes'][layer], activation=model_cfg['activations'][layer])))\n             \n        #add activation layer without weight normalization\n        if not model_cfg['weight_norms'][layer]:\n            model.add(tf.keras.layers.Dense(model_cfg['nodes'][layer], activation=model_cfg['activations'][layer]))\n            \n################################################################\n#### Output layer\n################################################################\n\n    if model_cfg['batch_norms'][model_cfg['layers']] and model_cfg['dropouts'][model_cfg['layers']]:\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(model_cfg['dropout_rates'][layer]))\n\n    if model_cfg['batch_norms'][model_cfg['layers']] and not model_cfg['dropouts'][model_cfg['layers']]:\n        model.add(tf.keras.layers.BatchNormalization())\n\n    model.add(tf.keras.layers.Dense(206, activation='sigmoid'))\n    \n    #add compiler\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr = 5e-4), sync_period=10),\n                  loss='binary_crossentropy', metrics=['AUC'])   \n    \n    return model","1fac3be5":"example_model = build_model(cfg, num_columns = 100)\nexample_model.summary()","ee88df21":"#load files into memory as Pandas DataFrames\ntrain_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_sub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","13a01b39":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 2, 'D2': 3})\n    del df['sig_id']\n    \n    return df\n\n#process datasets\ntrain_features = preprocess(train_features)\ntest_features = preprocess(test_features)\ntrain_targets_scored = train_targets_scored.drop('sig_id', axis = 1)","d54646a1":"train_targets_scored = train_targets_scored.loc[train_features['cp_type'] == 0].reset_index(drop=True)\ntrain_features = train_features.loc[train_features['cp_type'] == 0].reset_index(drop=True)","c813904f":"# https:\/\/www.kaggle.com\/stanleyjzheng\/multilabel-neural-network-improved\ntop_feats1 = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n       870, 871, 872, 873, 874]\n\nprint(len(top_feats1))","34a97527":"#https:\/\/www.kaggle.com\/nicohrubec\/pytorch-multilabel-neural-network\ntop_feats2 = [  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n        16,  18,  19,  20,  21,  23,  24,  25,  27,  28,  29,  30,  31,\n        32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n        48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n        63,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,\n        93,  94,  95,  96,  97,  99, 100, 101, 103, 104, 105, 106, 107,\n       108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134,\n       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n       149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164,\n       165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180,\n       181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n       197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213,\n       214, 215, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 230,\n       231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245,\n       246, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n       261, 263, 265, 266, 268, 270, 271, 272, 273, 275, 276, 277, 279,\n       282, 283, 286, 287, 288, 289, 290, 294, 295, 296, 297, 299, 300,\n       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315,\n       316, 317, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n       332, 333, 334, 335, 338, 339, 340, 341, 343, 344, 345, 346, 347,\n       349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 368, 369, 370, 371, 372, 374, 375, 376, 377,\n       378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n       392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 405, 406, 407,\n       408, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422,\n       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n       436, 437, 438, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n       452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482,\n       483, 485, 486, 487, 488, 489, 491, 492, 494, 495, 496, 500, 501,\n       502, 503, 505, 506, 507, 509, 510, 511, 512, 513, 514, 516, 517,\n       518, 519, 521, 523, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n       534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547,\n       549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n       564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 577, 580,\n       581, 582, 583, 586, 587, 590, 591, 592, 593, 595, 596, 597, 598,\n       599, 600, 601, 602, 603, 605, 607, 608, 609, 611, 612, 613, 614,\n       615, 616, 617, 619, 622, 623, 625, 627, 630, 631, 632, 633, 634,\n       635, 637, 638, 639, 642, 643, 644, 645, 646, 647, 649, 650, 651,\n       652, 654, 655, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668,\n       669, 670, 672, 674, 675, 676, 677, 678, 680, 681, 682, 684, 685,\n       686, 687, 688, 689, 691, 692, 694, 695, 696, 697, 699, 700, 701,\n       702, 703, 704, 705, 707, 708, 709, 711, 712, 713, 714, 715, 716,\n       717, 723, 725, 727, 728, 729, 730, 731, 732, 734, 736, 737, 738,\n       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765,\n       766, 767, 769, 770, 771, 772, 774, 775, 780, 781, 782, 783, 784,\n       785, 787, 788, 790, 793, 795, 797, 799, 800, 801, 805, 808, 809,\n       811, 812, 813, 816, 819, 820, 821, 822, 823, 825, 826, 827, 829,\n       831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 844, 845,\n       846, 847, 848, 850, 851, 852, 854, 855, 856, 858, 860, 861, 862,\n       864, 867, 868, 870, 871, 873, 874]\n\nprint(len(top_feats2))","c98ac008":"sys.path.append('..\/input\/iterativestrat\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold","fe34d732":"def train_model(cfg):\n    \n#########################################################\n#### Random training loop\n#########################################################\n\n    if cfg['training_params']['method'] is 'random':\n        \n        #define for convenience\n        train_cfg = cfg['training_params']\n\n        #save training results\n        random_results = []\n        \n        for j in range(train_cfg['random_repeats']):\n\n            #get datasets\n            if train_cfg['feature_selection'] is not None:\n                train_dataset = train_features.iloc[:, train_cfg['feature_selection']]    \n            if train_cfg['feature_selection'] is None:\n                train_dataset = train_features\n                \n            train_targets = train_targets_scored\n\n            #create a validation set to evaluate our model(s) performance\n            train_ds, val_ds, train_targets, val_targets = train_test_split(train_dataset, train_targets,\n                                                                            test_size = train_cfg['train_val_split'])\n\n            #some callbacks we can use\n            sv = tf.keras.callbacks.ModelCheckpoint(f'net-{j}.h5', monitor = 'val_loss', verbose = 0,\n                                                    save_best_only = True, save_weights_only = True, mode = 'min')\n\n            reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n                                                                  verbose=train_cfg['verbose'], epsilon=1e-4, mode='min')\n            \n            if train_cfg['feature_selection'] is not None:\n                model = build_model(cfg, num_columns = len(train_cfg['feature_selection']))\n            else: model = build_model(cfg, num_columns = 875)   \n                \n            history = model.fit(train_ds, train_targets,\n                                validation_data = (val_ds, val_targets),\n                                epochs = train_cfg['epochs'], batch_size = train_cfg['batch_size'], \n                                callbacks = [reduce_lr_loss], verbose = train_cfg['verbose'])\n            print('')\n            random_results.append(min(history.history['val_loss']))\n            \n            print(f\"Neural Net {j + 1}: Epochs={train_cfg['epochs']}, Train AUC={round(max(history.history['auc']), 5)}, Train loss={round(min(history.history['loss']), 5)}, Validation AUC={round(max(history.history['val_auc']), 5)}, Validation loss={round(min(history.history['val_loss']), 5)}\")  \n            print('')\n        \n        return random_results\n\n#########################################################\n#### StratKFold training loop\n#########################################################\n\n    if cfg['training_params']['method'] is 'skf':\n        \n        #define for convenience\n        train_cfg = cfg['training_params']\n        \n        #get stratified kfold split object\n        rmskf = RepeatedMultilabelStratifiedKFold(n_splits=train_cfg['skf_folds'], n_repeats=train_cfg['skf_repeats'],\n                                                  random_state=train_cfg['seed'])\n        \n        #save training results\n        skf_results = []\n        \n        for f, (train_index, val_index) in enumerate(rmskf.split(train_features.values,\n                                                                 train_targets_scored.values)):\n\n            #get features\n            if train_cfg['feature_selection'] is not None:\n                train_dataset = train_features.iloc[train_index, train_cfg['feature_selection']].values\n                val_dataset = train_features.iloc[val_index, train_cfg['feature_selection']].values\n            if train_cfg['feature_selection'] is None:\n                train_dataset = train_features[train_index].values\n                val_dataset = train_features[val_index].values\n            \n            #get targets\n            train_targets = train_targets_scored.values[train_index]\n            val_targets = train_targets_scored.values[val_index]\n\n            #some callbacks we can use\n            sv = tf.keras.callbacks.ModelCheckpoint(f'fold-{f}.h5', monitor = 'val_loss', verbose = 0,\n                                                    save_best_only = True, save_weights_only = True, mode = 'min')\n            \n            reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3,\n                                                                  verbose=train_cfg['verbose'], epsilon=1e-4, mode='min')\n        \n            if train_cfg['feature_selection'] is not None: model = build_model(cfg, num_columns = len(train_cfg['feature_selection']))\n            else: model = build_model(cfg, num_columns = 875)   \n                \n            history = model.fit(train_dataset, train_targets,\n                                validation_data = (val_dataset, val_targets),\n                                epochs = train_cfg['epochs'],\n                                batch_size = train_cfg['batch_size'], \n                                callbacks = [reduce_lr_loss],\n                                verbose = train_cfg['verbose'])\n            \n            print('')\n            skf_results.append(min(history.history['val_loss']))\n            \n            print(f\"Fold {f + 1}: Epochs={train_cfg['epochs']}, Train AUC={round(max(history.history['auc']), 5)}, Train loss={round(max(history.history['val_loss']), 5)}, Validation AUC={round(max(history.history['val_auc']), 5)}, Validation loss={round(min(history.history['val_loss']), 5)}\")  \n            print('')\n\n        return skf_results","b24a3608":"cfg1 = cfg.copy()","9c019ce7":"#model specific\ncfg1['model_params']['layers'] = 2\ncfg1['model_params']['nodes'] = [1024, 1024]\ncfg1['model_params']['dropouts'] = [1, 1, 1]\ncfg1['model_params']['dropout_rates'] = [.2, .4, .4]\ncfg1['model_params']['batch_norms'] = [1, 1, 1]\ncfg1['model_params']['weight_norms'] = [1, 1, 1]\ncfg1['model_params']['activations'] = [tf.keras.activations.swish] * 2\n\n#training specific\ncfg1['training_params']['method'] = 'skf'    \ncfg1['training_params']['epochs'] = 30  \ncfg1['training_params']['skf_folds'] = 5   \ncfg1['training_params']['train_val_split'] = .2\ncfg1['training_params']['skf_repeats'] = 1\ncfg1['training_params']['random_repeats'] = 5\ncfg1['training_params']['feature_selection'] = None\ncfg1['training_params']['verbose'] = 0","5af4a597":"cfg2 = cfg.copy()","57116bc0":"#model specific\ncfg2['model_params']['layers'] = 2\ncfg2['model_params']['nodes'] = [1024, 1024]\ncfg2['model_params']['dropouts'] = [1, 1, 1]\ncfg2['model_params']['dropout_rates'] = [.2, .4, .4]\ncfg2['model_params']['batch_norms'] = [1, 1, 1]\ncfg2['model_params']['weight_norms'] = [1, 1, 1]\ncfg2['model_params']['activations'] = [tf.keras.activations.swish] * 2\n\n#training specific\ncfg2['training_params']['method'] = 'skf'    \ncfg2['training_params']['epochs'] = 30  \ncfg2['training_params']['skf_folds'] = 5   \ncfg2['training_params']['train_val_split'] = .2\ncfg2['training_params']['skf_repeats'] = 1\ncfg2['training_params']['random_repeats'] = 5\ncfg2['training_params']['feature_selection'] = top_feats1\ncfg2['training_params']['verbose'] = 0","0c4d9347":"cfg3 = cfg.copy()","0960373b":"#model specific\ncfg3['model_params']['layers'] = 2\ncfg3['model_params']['nodes'] = [1024, 1024]\ncfg3['model_params']['dropouts'] = [1, 1, 1]\ncfg3['model_params']['dropout_rates'] = [.2, .4, .4]\ncfg3['model_params']['batch_norms'] = [1, 1, 1]\ncfg3['model_params']['weight_norms'] = [1, 1, 1]\ncfg3['model_params']['activations'] = [tf.keras.activations.swish] * 2\n\n#training specific\ncfg3['training_params']['method'] = 'skf'    \ncfg3['training_params']['epochs'] = 30  \ncfg3['training_params']['skf_folds'] = 5   \ncfg3['training_params']['train_val_split'] = .2\ncfg3['training_params']['skf_repeats'] = 1\ncfg3['training_params']['random_repeats'] = 5\ncfg3['training_params']['feature_selection'] = top_feats2\ncfg3['training_params']['verbose'] = 0","947f78a6":"cfgs = [cfg1, cfg2, cfg3]\nresults = []\n\nfor cfg in cfgs:\n    histories = train_model(cfg)\n    results.append(histories)","001be067":"for f, result in enumerate(results):\n    print(f\"Model {f + 1} validation loss = {round(np.average(result), 7)}, STD = {round(np.std(result), 7)}\")","5309e889":"**And there you have it. Just like that, we were able to test different feature selections. But you can of course experiment with much more than that. Simply define a model and how to train it with the configuration and test it against as many other model configurations as you wish.**","b2d4fe70":"# Processing\n\n**Let's now perform basic preprocessing for our datasets and then consider some different feature selection options we can experiment with.**","0f99e2c2":"# NN Experiments\n\n**In this notebook, I want to create a concise yet thorough procedure to experiment with different neural network architectures and processing techniques for the [MoA competition](https:\/\/www.kaggle.com\/c\/lish-moa). In this version, we can change and experiment with the following by simply change the parameters in the below `cfg` dictionary**\n\n* Network depth\n* Network width\n* Activation functions\n* Batch normalization\n* Dropout\n* Feature selection","534ad0ff":"**Now you can insert previously extracted features from feature selection to compare how different feature sets perform. You could even randomly drop columns and compare their results if you wanted to. For now, I will steal some features extracted via permutation importance (see code [here](https:\/\/www.kaggle.com\/stanleyjzheng\/multilabel-neural-network-improved)) and store them as lists. I will use the selected features from [here](https:\/\/www.kaggle.com\/stanleyjzheng\/multilabel-neural-network-improved) and also those from [this notebook](https:\/\/www.kaggle.com\/nicohrubec\/pytorch-multilabel-neural-network) so we have two different feature sets to consider.**","97af4ad8":"**And now we are finally ready to begin the experiment. The below loop is all we need to run the experiments and report the results:**","e750ee94":"**Once that configuration is saved, you simply repeat and create as many configurations as you want to test. I have listed all the parameters in `cfg` that you can change below, but if you only wish to change a couple things, you don't have to re-define everything since the other `cfg`s are copied from the first `cfg`: just make sure the first `cfg` is how you want everything by default first.**","a29ed669":"**Once these columns have been saved to memory, we can simply pass them to the `cfg`**","d0a8cc8d":"**Control samples have all 0's for labels so we drop them from training:**","3774975d":"**Boom, there you have it.**","29126b16":"**This is how you run your own experiments. The configuration dictionary is split between model architecture specific parameters and training specific parameters.**\n\n**Training parameters**\n\n* `methods` - String; method to perform experimentation (either stratified kfold or random train\/val splitting)\n* `epochs` - Integer; how many epochs to train models for\n* `skf_folds` - Integer; number of folds for skf training method\n* `train_val_split`, Decimal; the split to use for random training method\n* `skf_repeats` - Integer; number of times to repeat skf fold training\n* `random_repeats` - Integer; how many NNs to train in random training method\n* `feature_selection` - List of integers; the integer locations of the columns to keep. If you want to use original columns, put `feature_selection = None`\n* `batch_size` - Integer; batch size to train with\n* `seed` - random seed\n* `verbose` - 0, 1, or 2: 0 for no training progress, 1 for single bar updates, 2 for per epoch updates\n\n**Model parameters**\n* `layers` - Integer; the number of hidden layers\n* `nodes` - List; the number of nodes in a given layer\n* `activations` - List of functions; activation functions for each layer\n* `batch_norms` - Boolean list; whether or not to add batch normalization on each layer\n* `weight_norms` - Boolean list; whether or not to add weight normalization on each layer\n* `dropouts` - Boolean list; whether or not to add dropout on each layer\n* `dropout_rates` - List of decimals; the dropout rates to use on dropout layers","54b3de68":"# Experiments\n\n**There are two main methods for running experiments; the first is randomly splitting data with `train_split_test` and training an ensemble of the same network on different splits. We will have more robust results the more NNs we train. The second method is less random: we use RepeatedMultiLabelStratifiedKFold to fold-train an ensemble of networks. Since the labels are algorithmically stratified, this should result in more stable experiments, but you may want to take advantage of the randomness of the first method for actual inference.**\n\n**Here I will use iterative stratification based on [this paper](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-642-23808-6_10) from [this GitHub repository](https:\/\/github.com\/trent-b\/iterative-stratification). Of course, internet is not allowed for this competition, so we will use [this Kaggle dataset](https:\/\/www.kaggle.com\/mudittiwari255\/iterativestrat) instead of `!pip intall iterative-stratification`.**\n\n**Now we can run some experiments. In this commit, I would like to see how the 3 different training feature sets perform. So we will use the same model architecture, but pass different `feature_selection` arguments to the `cfg` like this:**","994245b7":"# Changing NN Architecture\n\n**Example: if you want to create the following neural network,**\n\n* 3 layers - first with 1024 nodes, second with 512 nodes, and third with 256 nodes\n* dropout only before first layer and before output layer\n* batch normalization before all activation layers\n* no weight normalization\n* `swish` activation on all layers\n\n**you would change the `cfg` like so:**"}}