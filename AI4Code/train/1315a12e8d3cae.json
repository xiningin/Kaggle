{"cell_type":{"a1bb19e3":"code","f41ff69e":"code","5d7669a3":"code","68ecec02":"code","a4fd8878":"code","444a9162":"code","80380e79":"code","45795b43":"code","8da2451a":"code","382d7adc":"code","d68483b7":"code","ebabd3c7":"code","53176cf2":"code","a0a771d7":"code","fd5ac8d7":"code","0f5318ea":"code","31a728bc":"code","6557c093":"code","76e59127":"code","155419c3":"code","b41d434b":"markdown","418c050b":"markdown","1bf2f29e":"markdown","c764e955":"markdown","9f4a0d2d":"markdown","989ae5ec":"markdown","878acf18":"markdown","ebd01f34":"markdown","869fe8fd":"markdown","822c5ea1":"markdown","0a3b5cf9":"markdown","0842c917":"markdown"},"source":{"a1bb19e3":"#!pip install --upgrade pip\n!pip install fastai==0.7.0 ## Based on Fast.ai ML course\n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline","f41ff69e":"import numpy as np \nimport pandas as pd\nfrom IPython.display import display\nfrom fastai.imports import *\nfrom fastai.structured import *\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom pandas_summary import DataFrameSummary\nfrom matplotlib import pyplot as plt\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\nimport re\n\nimport shap\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp, get_dataset, info_plots\n\nimport IPython\nfrom IPython.display import display\nprint(os.listdir(\"..\/input\/\"))","5d7669a3":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\ntrain_df.head()","68ecec02":"train_df['SalePrice'] = np.log(train_df['SalePrice'])","a4fd8878":"train_cats(train_df)\napply_cats(test_df, train_df)","444a9162":"df_trn, y_trn, nas = proc_df(train_df, 'SalePrice')\ndf_test, _, _ = proc_df(test_df, na_dict=nas)\ndf_trn.head()","80380e79":"df_test.head()","45795b43":"df_test.drop(['LotFrontage_na', 'MasVnrArea_na', 'BsmtFinSF1_na', 'BsmtFinSF2_na', 'BsmtUnfSF_na', \n              'TotalBsmtSF_na', 'BsmtFullBath_na', 'BsmtHalfBath_na', 'GarageYrBlt_na', 'GarageCars_na',\n              'GarageArea_na'], axis =1, inplace = True)\ndf_trn.drop(['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na'], axis = 1, inplace = True)","8da2451a":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(train_X), train_y), rmse(m.predict(val_X), val_y),     ## RMSE of log of prices\n                m.score(train_X, train_y), m.score(val_X, val_y)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","382d7adc":"train_X, val_X, train_y, val_y = train_test_split(df_trn, y_trn, test_size=0.33, random_state=42)","d68483b7":"%time\nm = RandomForestRegressor(n_estimators=1, min_samples_leaf=3, n_jobs=-1, max_depth = 3, oob_score=True) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","ebabd3c7":"draw_tree(m.estimators_[0], train_X, precision=3)","53176cf2":"%time\nm = RandomForestRegressor(n_estimators=20, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","a0a771d7":"perm = PermutationImportance(m, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","fd5ac8d7":"for feat_name in val_X.columns:\n#for feat_name in base_features:\n    #pdp_dist = pdp.pdp_isolate(model=m, dataset=val_X, model_features=base_features, feature=feat_name)\n    pdp_dist = pdp.pdp_isolate(model = m, dataset=val_X, model_features=val_X.columns, feature=feat_name)\n\n    pdp.pdp_plot(pdp_dist, feat_name)\n\n    plt.show()","0f5318ea":"explainer = shap.TreeExplainer(m)\nshap_values = explainer.shap_values(val_X)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value, shap_values[1,:], val_X.iloc[1,:], matplotlib=True) ## change shap and val_X","31a728bc":"shap.summary_plot(shap_values, val_X)","6557c093":"shap.summary_plot(shap_values, val_X, plot_type=\"bar\")","76e59127":"pred = m.predict(df_test)\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission.head()","155419c3":"submission['SalePrice'] = np.exp(pred)   ## Convert log back \nsubmission.to_csv('rf_submission_v1.csv', index=False)","b41d434b":"We'll just use a Random Forest Regressor. For that, we need to convert all columns to numeric type. But there are some categorical variables too.","418c050b":"Initially, let's just fit a single decision tree to visualize it properly","1bf2f29e":"A single decision tree did not perform so badly. Now, let's bag a collection of trees to create a random forest.","c764e955":"## SHAP values for selected rows","9f4a0d2d":"We'll replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable. Fastai to the rescue again !!","989ae5ec":"## Submitting Predictions","878acf18":"### Split the data into training and validation sets","ebd01f34":"## Permuation importance of features","869fe8fd":"### Defining function to calculate the evaluation metric","822c5ea1":"## Partial Dependence Plots","0a3b5cf9":"We can now pass this processed data frame to Random Forest Regressor","0842c917":"The evaluation criteria is RMSE of log of Sales Price. So first, let's change the target variable to log"}}