{"cell_type":{"c4f20b84":"code","ac216b30":"code","7bbd7c31":"code","0586e957":"code","84d07ca5":"code","90bfab37":"code","c63486a1":"code","1d949a53":"code","d7cd9ee1":"code","355e7870":"code","fce0a9a9":"code","33f30b17":"code","5db5db7d":"code","ebf94d4d":"code","3e2c5505":"code","d6075ffc":"code","3d249c94":"code","64500124":"code","3e4b0856":"markdown","30a82c50":"markdown","a190240c":"markdown","c26e6c06":"markdown"},"source":{"c4f20b84":"!pip install torchsummary\n!python -m pip install git+https:\/\/github.com\/lessw2020\/Ranger-Deep-Learning-Optimizer.git","ac216b30":"import time\nimport copy\nimport pandas as pd\nimport numpy as np\nfrom ranger import Ranger\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom torchsummary import summary\nfrom sklearn.utils import resample\n\nfrom sklearn.decomposition import PCA\n\nimport matplotlib.pyplot as plt\n\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMRegressor\n\n%matplotlib inline\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","7bbd7c31":"def get_x_y(dt, target, to_tensor=True):\n    x = dt.drop([target], axis=1).copy()\n    y = dt[target].copy() \n    \n    if to_tensor:\n        x = torch.tensor(x.values, dtype=torch.float)\n        y = torch.tensor(y.values, dtype=torch.float)\n        x = x.to(device)\n        y = y.to(device)\n    return x, y\n\ndef create_recorder_dict():\n    return {'epoch_list': [], 'train_acc': [], 'train_loss': [], 'valid_acc': [], 'valid_loss': [], 'train_lr': []}\n\ndef early_stop_init():\n    global epoch_no_improve, n_epoch_stop, stop_flag\n    epoch_no_improve, n_epoch_stop, stop_flag = 0, 15, False\n    pass\n\ndef train(model_):\n    global model_recorder\n    since = time.time()\n    model_.train()\n    running_loss = 0.0\n    running_corrects = 0\n    sample_size = 0\n    data_set_size = x_train.size(0)\n    loop_time_start = time.time()\n\n    # zero the parameter gradients\n    optimizer.zero_grad()\n\n    # get predict\n    y_hat = model_(x_train)\n    _, predicts = torch.max(y_hat, 1)\n\n    # calculate loss\n    loss = criterion(y_hat, y_train.long())\n\n    # backpropagation\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n    \n    # statistics\n    running_loss += loss.item() * x_train.size(0)\n    running_corrects += torch.sum(predicts == y_train.data)\n\n    # batch sum in one epoch\n    sample_size += len(x_train)\n\n    t_ = time.time() - since\n    epoch_loss = running_loss \/ data_set_size\n    epoch_acc = running_corrects.double() \/ data_set_size\n\n    # recorder\n    model_recorder['train_acc'] += [epoch_acc]\n    model_recorder['train_loss'] += [epoch_loss]\n    # get learning rate\n    optimizer_state = optimizer.state_dict()['param_groups'][0]\n\n    model_recorder['train_lr'] += [optimizer_state['lr']]\n    print()\n    print('Epoch Time Costs: {:.0f}m {:.0f}s'.format(t_ \/\/ 60, t_ % 60))\n    print()\n    print('Train Set:\\t| Average Loss: {:3.4f}\\t| Accuracy: {:3.4f}\\t| Learning Rate: {}'.format(\n        epoch_loss,\n        epoch_acc,\n        optimizer_state['lr']\n    ))\n\n\ndef valid(model_, acc, wts):\n    global epoch_no_improve, n_epoch_stop, stop_flag\n    global model_recorder\n    model_.eval()\n    epoch_loss = 0\n    correct = 0\n    model_wts = wts\n    with torch.no_grad():\n\n        output = model_(x_valid)\n        epoch_loss += criterion(output, y_valid.long()).item() * x_valid.size(0)\n        predict = output.max(1, keepdim=True)[1]\n        correct += predict.eq(y_valid.view_as(predict)).sum().item()\n\n        epoch_loss \/= x_valid.size(0)\n        epoch_acc = float(correct) \/ x_valid.size(0)\n\n        # recorder\n        model_recorder['valid_acc'] += [epoch_acc]\n        model_recorder['valid_loss'] += [epoch_loss]\n        print(\"Test Set:\\t| Average Loss: {:.4f}\\t| Accuracy: {:3.4f}\\t|\\n\".format(epoch_loss, epoch_acc))\n\n    if epoch_acc > acc:\n        print('New high accuracy: {}'.format(epoch_acc))\n        print()\n        acc = epoch_acc\n        model_wts = copy.deepcopy(model_.state_dict())\n        epoch_no_improve = 0\n    else:\n        epoch_no_improve += 1\n        if epoch_no_improve == n_epoch_stop:\n            stop_flag = True\n    return acc, model_wts\n\n\ndef training_model(epochs_=25):\n\n    print(\"{:20s} {:^15s} {:20s}\".format('=' * 20, model.__class__.__name__, '=' * 20))\n    print(\"{:20s} {:^15s} {:20s}\".format('=' * 20, 'Start Training', '=' * 20))\n    global model_recorder_dict, model_recorder\n    # Create a empty recorder\n    model_recorder = create_recorder_dict()\n\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(epochs_):\n        print(\"{:20s} {:^15s} {:20s}\".format('-' * 20, 'Epoch: {}'.format(epoch + 1), '-' * 20))\n        train(model)\n        best_acc, best_model_wts = valid(model, best_acc, best_model_wts)\n        if stop_flag:\n            print('Epoch Not improve. Early Stop.')\n            break\n            \n        model_recorder['epoch_list'] += [epoch]\n\n\n    t_ = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(t_ \/\/ 60, t_ % 60))\n    print('Best Test Accuracy: {:4f}'.format(best_acc))\n\n    model_recorder_dict.update({model.__class__.__name__: model_recorder})\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n\n\ndef processing(dt):\n    dt.appearedHour = np.sin(2 * np.pi * dt.appearedHour \/ 24)\n    dt.appearedMinute = np.sin(2 * np.pi * dt.appearedMinute \/ 60)\n    dt.terrainType = dt.terrainType.astype(str)\n    dt = dt.drop('id', axis=1)\n    y = dt['class'].copy() \n    dt = dt.drop(['class'], axis=1).copy()\n    \n    df_bool = dt.select_dtypes(include=['bool'])\n\n    for co in df_bool.columns:  \n        dt[co] = [0 if x == False else 1 for x in dt[co].values]\n    \n    dt = pd.get_dummies(dt)\n    \n    data_scaler = StandardScaler()\n    data_scaler.fit(dt)\n    dt[dt.columns] = data_scaler.transform(dt)   \n        \n    dt['class'] = y\n    train = dt[:train_length]\n    test = dt[train_length:]\n\n    train_data, valid_data = train_test_split(train, random_state=21, train_size=0.8)\n\n    df_minority = train_data[train_data['class'].isin([1, 3, 4])]\n    df_majority = train_data[train_data['class'].isin([0, 2, 5])]\n\n    df_minority_upsampled = resample(df_minority, replace=True, n_samples=2000)\n    train_data = pd.concat([train_data, df_minority_upsampled])\n\n    x_train, y_train = get_x_y(train_data, 'class')\n    x_valid, y_valid = get_x_y(valid_data, 'class')  \n    \n    del test['class']\n    test = torch.tensor(test.values, dtype=torch.float)\n    test = test.to(device)\n    return x_train, y_train, x_valid, y_valid, test","0586e957":"class BasicBlock(nn.Module):\n    def __init__(self, _in, _h):\n        super(BasicBlock, self).__init__()\n        self.r1 = nn.ELU()\n        self.bb = nn.Sequential(\n            nn.Linear(_in, _h, bias = True),\n            nn.BatchNorm1d(_h),\n            nn.PReLU(),\n            nn.Dropout(p=0.15),\n            nn.Linear(_h, _h, bias = True),\n            nn.BatchNorm1d(_h),\n            nn.PReLU(),\n            nn.Dropout(p=0.15),\n            nn.Linear(_h, _h, bias = True),\n            nn.BatchNorm1d(_h),\n            nn.PReLU(),\n            nn.Dropout(p=0.15),\n            nn.Linear(_h, _in, bias = True),\n            nn.BatchNorm1d(_in),\n        )\n        \n    def forward(self, x):\n        identity = x\n        x = self.bb(x)\n        x += identity\n        x = self.r1(x)\n        return x    \n\nclass OXMLP_v1(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(OXMLP_v1, self).__init__()\n        self.fc1 = nn.Linear(D_in, H, bias = True)\n        modules = []\n        for i in range(10):\n            modules.append(BasicBlock(H, H))\n\n        self.sequential = nn.Sequential(*modules)\n        self.fc2 = nn.Linear(H, D_out, bias = False)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = x.float()\n        x = self.fc1(x)\n        x = self.sequential(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\nclass OXMLP_v0(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(OXMLP_v0, self).__init__()\n        self.bb = nn.Sequential(\n            nn.Linear(D_in, H, bias = True),\n            nn.PReLU(),\n            nn.Linear(H, D_out, bias = True),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        x = x.float()\n        x = self.bb(x)\n        return x","84d07ca5":"train_csv = pd.read_csv('\/kaggle\/input\/aia-dnn-classification-pokemongo-tpe-9\/train.csv')\ntest_csv = pd.read_csv('\/kaggle\/input\/aia-dnn-classification-pokemongo-tpe-9\/test.csv')\ntest_id = test_csv.id\nall_data = pd.concat([train_csv, test_csv])","90bfab37":"train_length = train_csv.shape[0]","c63486a1":"train_csv.isnull().values.any()","1d949a53":"test_csv.isnull().values.any()","d7cd9ee1":"x_train, y_train, x_valid, y_valid, x_test = processing(all_data)","355e7870":"stop_flag= False\nn_epoch_stop = 15\n\nclasses = 6\ninput_dim = x_train.size(1)\nhidden_layer = 256\nnum_epochs = 1000","fce0a9a9":"# Create Model Recorder Dictionary\nmodel_recorder_dict = {}\nmodel_recorder = create_recorder_dict()\n\nmodel_list = [\n#     OXMLP_v0(input_dim, hidden_layer, classes),    \n    OXMLP_v1(input_dim, hidden_layer, classes),    \n]\n \nlearning_rate_list = [.1, .01, .001, .0001, .00001]","33f30b17":"for m in model_list:\n    early_stop_init()\n    model = m.to(device)\n    summary(model, (input_dim,))\n    criterion = nn.CrossEntropyLoss().to(device)\n    optimizer = Ranger(model.parameters(),alpha=0.5 ,lr=0.1, weight_decay=.1)   # 0.59\n#     optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=.001)   # 0.54\n#     optimizer = torch.optim.AdamW(model.parameters(),lr=0.001, weight_decay=.001)  # 0.59\n#     optimizer = torch.optim.ASGD(model.parameters(),lr=0.1, weight_decay=.01)      # 0.56\n#     optimizer = torch.optim.Adadelta(model.parameters(),lr=0.1, weight_decay=0.01) # 0.54\n#     optimizer = torch.optim.Adagrad(model.parameters(),lr=0.001, weight_decay=0.001)  # 0.58\n#     optimizer = torch.optim.Adamax(model.parameters(),lr=0.001, weight_decay=0.001)  # 0.58\n    \n    scheduler = CosineAnnealingLR(optimizer, T_max=20)\n    \n    model = training_model(num_epochs)\n    \n    outputs = model(x_valid)\n    _, predicted = torch.max(outputs, 1)\n    print(classification_report(y_valid.tolist(), predicted.cpu(), zero_division=True))","5db5db7d":"for k, v in model_recorder_dict.items():\n    plt.plot(v['epoch_list'], v['train_acc'][:len(v['epoch_list'])], 'bo', label='Training accuracy')\n    plt.plot(v['epoch_list'], v['valid_acc'][:len(v['epoch_list'])], 'b', label='Validation accuracy')\n \n    \n    plt.title('{} Training and validation accuracy'.format(k))\n    plt.legend()\n    plt.figure()\n    plt.show()\n    plt.cla()\n    plt.plot(v['epoch_list'], v['train_loss'][:len(v['epoch_list'])], 'bo', label='Training loss')\n    plt.plot(v['epoch_list'], v['valid_loss'][:len(v['epoch_list'])], 'b', label='Validation loss')\n    plt.title('{} Training and validation loss'.format(k))\n    plt.legend()\n    plt.figure()\n    plt.show()","ebf94d4d":"color_list = ['b', 'g', 'r', 'c', 'n']\ncolor_ind = 0\nfor k, v in model_recorder_dict.items():\n    plt.plot(v['epoch_list'], v['valid_acc'][:len(v['epoch_list'])], color_list[color_ind], label=k)\n    color_ind += 1\n\nplt.legend()\nplt.show()","3e2c5505":"x_test.size()","d6075ffc":"x_train.size()","3d249c94":"outputs = model(x_test)\n_, predicted = torch.max(outputs, 1)\n\nsubmit = pd.DataFrame({'ID': test_id, 'class': predicted.cpu()})","64500124":"submit.to_csv('submission.csv', index=False)","3e4b0856":"### Models","30a82c50":"### utils","a190240c":"### Install Packages","c26e6c06":"### Configs"}}