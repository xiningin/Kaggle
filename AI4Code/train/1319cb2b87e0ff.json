{"cell_type":{"e28437bc":"code","365c9ec1":"code","8c596b4b":"code","dac1c76f":"code","bb727415":"code","086ded68":"code","2d616d67":"code","6c3180e7":"code","b2e58972":"code","1d21fcf0":"code","47af1556":"code","62fe5914":"code","4e5ff9c6":"code","c5195be1":"code","65eeab0f":"code","1ff88e9c":"code","e32d0927":"code","13f22e4b":"markdown","3cb1ba9e":"markdown","e19c6e27":"markdown","559471ba":"markdown","a105c1dd":"markdown","746d2401":"markdown","39449609":"markdown","252636f3":"markdown","0ee47f2b":"markdown","aa5cf8bd":"markdown","5d005d20":"markdown","fe4c6d3f":"markdown","495fcffd":"markdown","7ac6137c":"markdown","c2fe5df2":"markdown","f49834ef":"markdown","747457c9":"markdown","2a53f3e0":"markdown"},"source":{"e28437bc":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt","365c9ec1":"data = pd.read_csv ('..\/input\/wholesale-customers-data-set\/Wholesale customers data.csv')\ndata.head()","8c596b4b":"data.info()","dac1c76f":"cat_f = ['Channel', 'Region']\ncont_f = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']","bb727415":"data.describe()\n# data[cont_f].describe()","086ded68":"data = pd.get_dummies(data=data, columns=cat_f, drop_first = True)\ndata.head()","2d616d67":"all_colmns = data.columns\nall_colmns","6c3180e7":"scale = MinMaxScaler()\nscale.fit(data)\ndata_scaled = scale.transform(data)\ndata_scaled = pd.DataFrame(data_scaled, columns = all_colmns)\ndata_scaled.head()","b2e58972":"Sum_of_squared_distances = []\n\nK = range(1,15)\n\nfor k in K:\n    km = KMeans(n_clusters = k)\n    km = km.fit(data_scaled)\n    Sum_of_squared_distances.append(km.inertia_)\n    ","1d21fcf0":"plt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","47af1556":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import scale\n\nimport sklearn.metrics as sm\nfrom sklearn import datasets\nfrom sklearn.metrics import confusion_matrix, classification_report","62fe5914":"iris = datasets.load_iris()\n\nX = scale(iris.data)\n\ny = pd.DataFrame(iris.target)\n\nvariable_names = iris.feature_names\n\nX[0:10,]","4e5ff9c6":"clustering = KMeans(n_clusters = 3, random_state = 5)\n\nclustering.fit(X)","c5195be1":"iris_df = pd.DataFrame(iris.data)\n\niris_df.columns = ['Speal_length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']\n\ny.columns = ['Targets']","65eeab0f":"color_theme = np.array(['darkgray', 'lightsalmon', 'powderblue'])\n\nplt.subplot(1,2,1)\n\nplt.scatter(x = iris_df.Petal_Length, y = iris_df.Petal_Width, c = color_theme[iris.target], s = 50)\n\nplt.title('Ground Truth CLassification')\n\nplt.subplot(1,2,2)\n\nplt.scatter(x = iris_df.Petal_Length, y = iris_df.Petal_Width, c = color_theme[clustering.labels_], s = 50)\n\nplt.title('K-Means Classification')\n","1ff88e9c":"relabel = np.choose(clustering.labels_, [2,0,1]).astype(np.int64)\n","e32d0927":"plt.subplot(1,2,1)\n\nplt.scatter(x = iris_df.Petal_Length, y = iris_df.Petal_Width, c = color_theme[iris.target], s = 50)\n\nplt.title('Ground Truth CLassification')\n\nplt.subplot(1,2,2)\n\nplt.scatter(x = iris_df.Petal_Length, y = iris_df.Petal_Width, c = color_theme[relabel], s = 50)\n\nplt.title('K-Means Classification')","13f22e4b":"# K-Means Model\nFor each k value, we will initialise k-means and use the inertia attribute to identify the sum of squared distances of samples to the nearest cluster centre.\n\nAs k increases, the sum of squared distance tends to zero. \n\nImagine we set k to its maximum value n (where n is number of samples) each sample will form its own cluster meaning sum of squared distances equals zero.","3cb1ba9e":"# K-Means : Wholesale Customer from UCI","e19c6e27":"In the plot above the elbow is at k=5 indicating the optimal k for this dataset is 5","559471ba":"# K-Means Clustering\n\nThe k-means clustering algorithm is a simple **Unsupervised** algorithm that's used for quickly predicting groupings from within an ublabeled dataset.\nOne of the popular methods of clustering unlabelled data into k clusters.\n\n## Predictions are based on\n1. The number of cluster centers present (k)\n2. Nearest mean values (measured in Euclidian distance between observations).\n\n## K-Means use-Cases\n1. Market price and Cost Modeling\n2. Customer Segmentation\n3. Insurance CLaim Fraud Detection\n4. Hedge Fund Classification\n\n## Some tips\n* Scale your variables\n* Look at a scatterplot or the data table to estimate the number of centroids, or cluster centers, to set for the k parameter in the model.\n\n## Determine Optimal number of clusters for k-means clustering.\nOne of the trickier tasks in clustering is identifying the appropriate number of clusters k.\nThere are many ways to determine optimal number of clusters for k-means clustering, but `elbow` method is one of the easiest and popular way to estimate the value k. Another popular method of estimating k is through `silhouette` analysis.","a105c1dd":"# Building and Running your model","746d2401":"Option 1: We may directly use the features Channel and Region as is for the model training as they are already in numeric field. But it may not produce accurate result. \nOption 2: As we know these features represents some categorical (nomial data format). Better is to convert this into binary using pandas get_dummies method. \n\n## Why we are doing so?\nAs these features are in nomial and not ordinal. Order of the data point is not important here.. so better we convert into Binary. If we do not do it then model will consider it as oridnal and may not produce accurate result.","39449609":"# Lets Plot this\nPlot the sum of squared distances for k in the range specified above. If the plot looks like an arm, then the elbow on the arm is optimal k.","252636f3":"Lets see the Descriptive statistics analysis specially for continuous features.","0ee47f2b":"![image.png](attachment:image.png)\n","aa5cf8bd":"# Plotting your model outputs","5d005d20":"We are going to use the dataset on Wholesale Customers from UCI Machine Learning Repository. Please refer to site http:\/\/archive.ics.uci.edu\/ml\/datasets\/Wholesale+customers to get more details.\n\nHere i have downloaded the dataset and added in the Kaggle, so that we can directly use it.\n\nThe data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on diverse product categories.\n\nUCI has provided the Attribute details which are as below.\n## Attribute Information:\n\n1) FRESH: annual spending (m.u.) on fresh products (Continuous)\n\n2) MILK: annual spending (m.u.) on milk products (Continuous)\n\n3) GROCERY: annual spending (m.u.) on grocery products (Continuous)\n\n4) FROZEN: annual spending (m.u.) on frozen products (Continuous)\n\n5) DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)\n\n6) DELICATESSEN: annual spending (m.u.) on delicatessen products (Continuous)\n\n7) CHANNEL: customer channels - Horeca (Hotel\/Restaurant\/Cafe) or Retail channel (Nominal)\n\n8) REGION: customer regions - Lisnon, Oporto or Other (Nominal)\n","fe4c6d3f":"From the data type (Dtype) it seems all are on numeric, but per the UCI description we are sure that `Channel` and `Region` are categorical and rest are continuous features.\n\nSplit the Attributes into categorical and continuous features.","495fcffd":"![image.png](attachment:image.png)\n","7ac6137c":"# How to Determine the Optimal Number Of Clusters for K-Means with Python","c2fe5df2":"# How k-means works?\n* The process begins with k centroids initialised at random.\n* These centroids are used to assign points to its nearest cluster.\n* The mean of all points within the cluster is then used to update the position of the centroids.\n* The above steps are repeated until the values of the centroids stabilise.","f49834ef":"As i already mentioned in the tips that we have to scale the data.\nTo give equal importance to all features, we need to scale the continuous features. We will be using scikit-learn\u2019s MinMaxScaler as the feature matrix is a mix of binary and continuous features. \n\nOther alternatives includes StandardScaler.","747457c9":"# K-Means : Iris DataSet","2a53f3e0":"In the plot above, the elbow is at k = 3 indicating that the optimal k for dataset is 3."}}