{"cell_type":{"c84cf4e7":"code","6e6e033f":"code","d55ea3b2":"code","a5049ce7":"code","04b02c1f":"code","b707e3b8":"code","82d878aa":"code","3f66b74a":"code","22f18bac":"code","212d3824":"code","9a3490f5":"code","82874ee5":"code","4522230c":"code","e983e45c":"code","86a63d7e":"code","17d8cef9":"code","e9a4ef07":"code","c5e03ba7":"code","62110911":"code","0bd6a3ac":"code","157506b8":"code","977ba90d":"code","3426d8a5":"code","63ccbc39":"code","b1d0744d":"code","44261f4c":"code","091d89f5":"code","579885aa":"code","f1043227":"code","f45386c0":"code","86f0b950":"code","ce515a15":"code","d40dcc86":"code","4a43e15c":"code","7d22966e":"code","c6c8c957":"code","2e0a6c50":"markdown","83ae269b":"markdown","1ad3764f":"markdown","352163d7":"markdown","24062ea0":"markdown","0012a406":"markdown","99da73bb":"markdown","1cb55307":"markdown","1d5464a6":"markdown","c2ebc623":"markdown","21352bc1":"markdown","2894c6bb":"markdown","a41d5d3d":"markdown","4e7e599b":"markdown","d48f420c":"markdown","2a22cc11":"markdown","5f5d5f3f":"markdown","dc08c64b":"markdown","860a6c7f":"markdown","b456dae0":"markdown","42592b89":"markdown","f08ae772":"markdown","42b924d2":"markdown","354d977f":"markdown","54900cfd":"markdown","6bde3981":"markdown","32f67e18":"markdown","f80c379b":"markdown","0cce211f":"markdown","861f6b7b":"markdown","ff1bb0ea":"markdown","48fa3f12":"markdown","beeb5556":"markdown","3486ac85":"markdown","7970b929":"markdown"},"source":{"c84cf4e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV \nfrom sklearn.metrics import accuracy_score, confusion_matrix \nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor\nfrom sklearn.decomposition import PCA\n\n# warning library\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e6e033f":"cancer_data = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","d55ea3b2":"# we are browsing the data\ncancer_data.head()","a5049ce7":"# the columns \ncancer_data.columns","04b02c1f":"# We drop the columns we cannot use\ncancer_data.drop(['Unnamed: 32','id'], inplace = True, axis = 1)","b707e3b8":"# change the title of the feature\ncancer_data = cancer_data.rename(columns = {\"diagnosis\":\"target\"})","82d878aa":"cancer_data.head()","3f66b74a":"# we are looking at the size of our data.\ncancer_data.shape","22f18bac":"cancer_data.info()","212d3824":"# statistical summary of our data\ncancer_data.describe()","9a3490f5":"sns.countplot(cancer_data[\"target\"])\nprint(cancer_data.target.value_counts())","82874ee5":"cancer_data[\"target\"] = [1 if i.strip() == \"M\" else 0 for i in cancer_data.target] ","4522230c":"corr_matrix = cancer_data.corr()\n\nsns.clustermap(corr_matrix, annot = True, fmt = \".2f\")\nplt.title(\"Correlation Between Features\")\nplt.show()","e983e45c":"threshold = 0.75 \nfiltre = np.abs(corr_matrix[\"target\"]) > threshold\ncorr_features = corr_matrix.columns[filtre].tolist()\nsns.clustermap(cancer_data[corr_features].corr(), annot = True, fmt = \".2f\")\nplt.title(\"Correlation Between Features w Corr Theshold 0.75\")\nplt.show()","86a63d7e":"data_melted = pd.melt(cancer_data, id_vars = \"target\",\n                      var_name = \"features\",\n                      value_name = \"value\")\n\nplt.figure()\nsns.boxplot(x = \"features\", y = \"value\", hue = \"target\", data = data_melted)\nplt.xticks(rotation = 90) \nplt.show()","17d8cef9":"#pair plot\nsns.pairplot(cancer_data[corr_features], diag_kind = \"kde\", markers = \"+\", hue = \"target\")\nplt.show()","e9a4ef07":"y = cancer_data.target\nx = cancer_data.drop([\"target\"], axis = 1)\n\ncolumns = x.columns.tolist()\ncolumns","c5e03ba7":"clf = LocalOutlierFactor()\n\n# We see if it is outlier or not, -1 is outlier\ny_pred = clf.fit_predict(x)\n\n# we need outlier factor values \nX_score = clf.negative_outlier_factor_","62110911":"outlier_score = pd.DataFrame()\noutlier_score[\"score\"] = X_score\noutlier_score","0bd6a3ac":"# threshold\n# It will show us the ones with outlier value above 2.5\n\nthreshold = -2.5\nfiltre = outlier_score[\"score\"] < threshold\noutlier_index = outlier_score[filtre].index.tolist()\noutlier_index","157506b8":"plt.figure()\nplt.scatter(x.iloc[outlier_index,0],x.iloc[outlier_index,1],color = \"blue\", s = 50, label = \"outliers\")\nplt.scatter(x.iloc[:,0],x.iloc[:,1],color = \"k\", s = 3, label = \"Data Points\")\n\n# We do normalization for the plotting process\nradius = (X_score.max()- X_score) \/ (X_score.max() - X_score.min())\noutlier_score[\"radius\"] = radius\nplt.scatter(x.iloc[:,0],x.iloc[:,1],s = 1000*radius, edgecolors = \"r\", facecolors = \"none\", label = \"Outlier Scores\")\nplt.legend()\nplt.show()\n","977ba90d":"x = x.drop(outlier_index)\ny = y.drop(outlier_index).values","3426d8a5":"test_size = 0.3\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = test_size, random_state = 42)","63ccbc39":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train) \nX_test = scaler.transform(X_test) \n\nX_train_df = pd.DataFrame(X_train, columns = columns)\nX_train_df_describe = X_train_df.describe()\nX_train_df[\"target\"] = Y_train","b1d0744d":"# box plot \ndata_melted = pd.melt(X_train_df, id_vars = \"target\",\n                      var_name = \"features\",\n                      value_name = \"value\")\n\nplt.figure()\nsns.boxplot(x = \"features\", y = \"value\", hue = \"target\", data = data_melted)\nplt.xticks(rotation = 90)\nplt.show()","44261f4c":"# pair plot \nsns.pairplot(X_train_df[corr_features], diag_kind = \"kde\", markers = \"+\",hue = \"target\")\nplt.show()","091d89f5":"knn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(X_train, Y_train)\ny_pred = knn.predict(X_test)\ncm = confusion_matrix(Y_test, y_pred)\n\nacc = accuracy_score(Y_test, y_pred)\nscore = knn.score(X_test, Y_test)\n\nprint(\"Score : \", score)\nprint(\"Basic KNN Acc : \", acc)\n\nplot_confusion_matrix(knn, X_test, Y_test, cmap= \"RdPu\")  \nplt.show()","579885aa":"def KNN_Best_Params(x_train, x_test, y_train, y_test):\n    \n    k_range = list(range(1,31))\n    weight_options = [\"uniform\",\"distance\"]\n    print()\n    \n    param_grid = dict(n_neighbors = k_range, weights = weight_options)\n    \n    knn = KNeighborsClassifier()\n    grid = GridSearchCV(knn, param_grid, cv = 10, scoring = \"accuracy\")\n    grid.fit(x_train, y_train)\n    \n    print(\"Best training score : {} with paremeters : {}\".format(grid.best_score_, grid.best_params_))\n    print()\n    \n    knn = KNeighborsClassifier(**grid.best_params_)\n    knn.fit(x_train, y_train)\n    \n    y_pred_test = knn.predict(x_test)\n    y_pred_train = knn.predict(x_train)\n    \n    cm_test = confusion_matrix(y_test, y_pred_test)\n    cm_train = confusion_matrix(y_train, y_pred_train)\n    \n    acc_test = accuracy_score(y_test, y_pred_test)\n    acc_train = accuracy_score(y_train, y_pred_train)\n    \n    print(\"Test Score: {}, Train Score: {}\".format(acc_test, acc_train))\n    \n    plot_confusion_matrix(knn, x_test, y_test, cmap= \"RdPu\")  \n    plt.title(\"Test Confusion Matrix\", color = \"Darkred\")\n    plt.show()\n    \n    plot_confusion_matrix(knn, x_train, y_train, cmap= \"RdPu\")  \n    plt.title(\"Train Confusion Matrix\", color = \"Darkred\")\n    plt.show()\n       \n    return grid\n","f1043227":"grid = KNN_Best_Params(X_train, X_test, Y_train, Y_test)","f45386c0":"scaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)\n\npca = PCA(n_components = 2) \npca.fit(x_scaled)\nX_reduced_pca = pca.transform(x_scaled)\npca_data = pd.DataFrame(X_reduced_pca, columns = [\"p1\",\"p2\"])\npca_data[\"target\"] = y\nsns.scatterplot(x = \"p1\", y = \"p2\", hue = \"target\", data = pca_data)\nplt.title(\"PCA : p1 vs p2\")\n\nX_train_pca, X_test_pca, Y_train_pca, Y_test_pca = train_test_split(X_reduced_pca, y, test_size = test_size, random_state = 42)\n","86f0b950":"grid_pca = KNN_Best_Params(X_train_pca, X_test_pca, Y_train_pca, Y_test_pca)","ce515a15":"# We choose 4 colors\ncmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .05 # step size in the mesh\nX = X_reduced_pca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_pca.best_estimator_.n_neighbors, grid_pca.best_estimator_.weights))\nplt.show()","d40dcc86":"nca = NeighborhoodComponentsAnalysis(n_components = 2, random_state = 42)\nnca.fit(x_scaled, y)\nX_reduced_nca = nca.transform(x_scaled)\nnca_data = pd.DataFrame(X_reduced_nca, columns = [\"p1\",\"p2\"])\nnca_data[\"target\"] = y\nsns.scatterplot(x = \"p1\",  y = \"p2\", hue = \"target\", data = nca_data)\nplt.title(\"NCA: p1 vs p2\")\n\nX_train_nca, X_test_nca, Y_train_nca, Y_test_nca = train_test_split(X_reduced_nca, y, test_size = test_size, random_state = 42)","4a43e15c":"grid_nca = KNN_Best_Params(X_train_nca, X_test_nca, Y_train_nca, Y_test_nca)","7d22966e":"cmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .2 # step size in the mesh\nX = X_reduced_nca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_nca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_nca.best_estimator_.n_neighbors, grid_nca.best_estimator_.weights))\nplt.show()","c6c8c957":"knn = KNeighborsClassifier(**grid_nca.best_params_)\nknn.fit(X_train_nca,Y_train_nca)\ny_pred_nca = knn.predict(X_test_nca)\nacc_test_nca = accuracy_score(y_pred_nca,Y_test_nca)\nknn.score(X_test_nca,Y_test_nca)\n\n\ntest_data = pd.DataFrame()\ntest_data[\"X_test_nca_p1\"] = X_test_nca[:,0]\ntest_data[\"X_test_nca_p2\"] = X_test_nca[:,1]\ntest_data[\"y_pred_nca\"] = y_pred_nca\ntest_data[\"Y_test_nca\"] = Y_test_nca\n\nplt.figure()\nsns.scatterplot(x=\"X_test_nca_p1\", y=\"X_test_nca_p2\", hue=\"Y_test_nca\",data=test_data)\n\ndiff = np.where(y_pred_nca!=Y_test_nca)[0]\nplt.scatter(test_data.iloc[diff,0],test_data.iloc[diff,1],label = \"Wrong Classified\",alpha = 0.2,color = \"red\",s = 500)\nplt.show()","2e0a6c50":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > We convert our target property to int values. <\/p> <\/li>\n        <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > M --> 1 <\/p> <\/li>\n        <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > B --> 0 <\/p> <\/li>\n<\/ul>","83ae269b":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >We sent our data to the function we created above and displayed the results. <\/p> <\/li>\n<\/ul>","1ad3764f":"<a id ='2' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> Clening Data and Analysis <\/h2>","352163d7":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > We look at how many benign, how many malign tumors there are <\/p> <\/li>\n      <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > B --> Benign (357) <\/p> <\/li>\n          <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > M --> Malignant (212) <\/p> <\/li>\n<\/ul>","24062ea0":"![Breast-Cancer](attachment:Meme-Kanseri.jpg)","0012a406":"<a id ='6' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> Standrization <\/h2>","99da73bb":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >we detected the wrong classification and visualize. <\/p> <\/li>\n<\/ul>","1cb55307":"<a id ='11' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> Which is the wrong classification made ? <\/h2>","1d5464a6":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >We trained our model again by applying PCA (Principal Component Analysis). <\/p> <\/li>\n<\/ul>","c2ebc623":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >We wrote a function to find the best parameters of our Knn model. <\/p> <\/li>\n<\/ul>","21352bc1":"<a id ='3' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> Exploratory Data Analysis (EDA) <\/h2>","2894c6bb":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > We control our transaction over our data. <\/p> <\/li>\n<\/ul>","a41d5d3d":"<center><h1 style = \"background:#F8BBD0;border:0\">Introduction<\/h1><\/center>\n<p style = \"color:black;font-weight:500;text-indent:20px;font-size:16px\">The aim of the project is to determine whether the tumor is benign or malignant in people diagnosed with breast cancer. While making this determination, we will make use of the features in our data.<\/p>\n\n<p style = \"color:black;font-weight:500;text-indent:20px;font-size:16px\">First, we examined the data in detail.We detected missing data and outliers. Later, we made visualizations using the plotly library.We gave information about the graphics.We used different visualization techniques.Finally, we determined the wrong classification we made and finalized the notebook. <\/p>\n    \n\n<h2 style = \"background:#F8BBD0;border:0\">Content :<\/h2>\n\n<ul>\n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#1\" style = \"color:black;font-weight:bold\"> Load and Check Data <\/a> <\/li>\n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#2\" style = \"color:black;font-weight:bold\"> Clening Data and Analysis <\/a> <\/li> \n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#3\" style = \"color:black;font-weight:bold\"> Exploratory Data Analysis (EDA) <\/a> <\/li> \n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#4\" style = \"color:black;font-weight:bold\"> Outlier Detection <\/a> <\/li> \n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#5\" style = \"color:black;font-weight:bold\"> Train - Test Split <\/a> <\/li> \n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#6\" style = \"color:black;font-weight:bold\"> Standrization <\/a> <\/li> \n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#7\" style = \"color:black;font-weight:bold\"> Bassic KNN method <\/a> <\/li> \n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#8\" style = \"color:black;font-weight:bold\"> KNN Best Paramters <\/a> <\/li>\n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#9\" style = \"color:black;font-weight:bold\"> PCA ( Principal Component Analysis) <\/a> <\/li>\n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#10\" style = \"color:black;font-weight:bold\"> NCA (Neighborhood Components Analysis) <\/a> <\/li> \n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#11\" style = \"color:black;font-weight:bold\"> Which is the wrong classification made ? <\/a> <\/li>\n    <li style = \"color:#EC407A;font-size:15px\"> <a href = \"#12\" style = \"color:black;font-weight:bold\"> CONCLUSION <\/a> <\/li>\n  \n    \n<\/ul>\n    \n","4e7e599b":"<a id ='12' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> CONCLUSION <\/h2>\n\n<ul>\n    <li>Seaborn tutorial for beginner : <a href = 'https:\/\/www.kaggle.com\/rafetcan\/visualization-tutorial-with-seaborn'> https:\/\/www.kaggle.com\/rafetcan\/visualization-tutorial-with-seaborn <\/a> <\/li>\n    <li>Plotly tutorial for beginner : <a href = 'https:\/\/www.kaggle.com\/rafetcan\/plotly-tutorial-for-beginners'> https:\/\/www.kaggle.com\/rafetcan\/plotly-tutorial-for-beginners <\/a> <\/li>\n<\/ul>\n\n<p style = \"font-size:15px;text-indent:6px;\"> If you have any questions or suggestions I would be happy to hear <\/p>","d48f420c":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > \nWe divide our data into two as train and test. <\/p> <\/li>\n<\/ul>","2a22cc11":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >We take a look at our classification resulting from NCA (Neighborhood Components Analysis). <\/p> <\/li>\n<\/ul>","5f5d5f3f":"<a id ='10' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> NCA (Neighborhood Components Analysis) <\/h2>","dc08c64b":"<h2 style = \"background:#F8BBD0;border:0\">Import Libraries<\/h2>","860a6c7f":"<a id ='7' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> Bassic KNN method <\/h2>","b456dae0":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > In order to see the relationship a little better, we put a threshold <\/p> <\/li>\n<\/ul>","42592b89":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >We take a look at our classification resulting from PCA (Principal Component Analysis). <\/p> <\/li>\n<\/ul>","f08ae772":"<a id ='8' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> KNN Best Paramters <\/h2>","42b924d2":"<a id ='9' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> PCA ( Principal Component Analysis) <\/h2>","354d977f":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >We created the KNN model to make predictions. <\/p> <\/li>\n        <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > \nThen, we made an estimate and looked at our success results. <\/p> <\/li>\n     <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > \nFinally, we saw how many wrong guesses we made using a confusion matrix. <\/p> <\/li>\n<\/ul>","54900cfd":"<a id ='4' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> Outlier Detection <\/h2>","6bde3981":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >We trained our model again by applying NCA (Neighborhood Components Analysis). <\/p> <\/li>\n<\/ul>","32f67e18":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > We subtract the value we determined above from the data. <\/p> <\/li>\n<\/ul>","f80c379b":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >  We detected outliers above 2.5 <\/p> <\/li>\n        <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >  You can view this outlier from the graph above. <\/p> <\/li>\n<\/ul>","0cce211f":"<a id ='1' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\">Load and Check Data <\/h2>","861f6b7b":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >  We make the data DataFrame to see the outliers better. <\/p> <\/li>\n<\/ul>","ff1bb0ea":"<a id ='5' ><\/a>\n<h2 style = \"background:#F8BBD0;border:0\"> Train - Test Split <\/h2>","48fa3f12":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >The success and confusion matrix of our data obtained as a result of NCA (Neighborhood Components Analysis). <\/p> <\/li>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >As a result of the NCA (Neighborhood Components Analysis), we saw that we had only one wrong classification. <\/p> <\/li>\n<\/ul>","beeb5556":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > We draw the relationship matrix and examine the relationships between properties. <\/p> <\/li>\n        <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > If the relation between properties is 1 it means that it is true and -1 means it is inversely proportional. <\/p> <\/li>\n<\/ul>","3486ac85":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > \nWe standardize our data. <\/p> <\/li>\n        <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" > \nWe don't want big differences between features. Because, this affects the model badly. <\/p> <\/li>\n<\/ul>","7970b929":"<ul>\n    <li  style = \"color:#EC407A\" > <p style = \"color:black;font-weight:bold\" >The success and confusion matrix of our data obtained as a result of PCA (Principal Component Analysis). <\/p> <\/li>\n<\/ul>"}}