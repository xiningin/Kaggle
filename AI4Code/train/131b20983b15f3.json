{"cell_type":{"08c63b8b":"code","7fe43f24":"code","9c4f8386":"code","af208dcf":"code","126d65dc":"code","f2931e1f":"code","6a3aaad5":"code","4fc066b1":"code","e1fbae07":"code","901fe27c":"code","94e929e4":"code","46f31299":"code","0cc392b2":"code","894a9494":"code","784d39ac":"code","69832b17":"code","3b06f830":"code","508e9ab4":"code","80070f08":"code","c0594ac3":"markdown","6db18a7e":"markdown","fffa347a":"markdown","11fa5b2f":"markdown","f3aa7e13":"markdown","57e48077":"markdown","a4f11ff7":"markdown","8d59a4c0":"markdown","67604d78":"markdown","122419af":"markdown","a9f1b234":"markdown","9a9c7da1":"markdown"},"source":{"08c63b8b":"# Imports\nimport datetime\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Returns Panda DataFrame for provided CSV filename (without .csv extension).\nload = lambda name: pd.read_csv('..\/input\/{}.csv'.format(name))","7fe43f24":"\n# use to filter users who registered recently = not enough time to measure retention. Currently 4 months before latest date (\"26.5.2019\") in this version of the dataset.:\n## updated to latest date (Note: we could get this directly from the data, but it's cleaner to have magic variables defined upfront)\nEND_DATE = pd.to_datetime(\"2020-06-02\") #pd.to_datetime(\"26.5.2019\")\nRECENT_CUTOFF_DATE =  pd.to_datetime(\"2020-02-01\") #pd.to_datetime(\"25.1.2019\")  \n\nMINIMAL_RETENTION_HISTORY =  12 #12 # Time in weeks\nMINIMAL_RETENTION_WEEKS_TARGET = 24 # 36 #target we want to predict (in weeks). We'll filter data for users who can't have been active for less than this amount of time","9c4f8386":"# Information about public kernel versions. \n# Signup date table and user events table will be derived from this dataframe.\n# kernel_versions = load('KernelVersions')\nkernel_versions = pd.read_csv('..\/input\/KernelVersions.csv',usecols=['AuthorUserId','CreationDate'],parse_dates=[\"CreationDate\"],infer_datetime_format=True)\nprint(\"kernel versions\",kernel_versions.shape)\nkernel_versions[\"CreationDate\"] = kernel_versions[\"CreationDate\"].dt.floor('d')\nkernel_versions.drop_duplicates(inplace=True)\nprint(\"Date level dedupped kernel versions\",kernel_versions.shape[0])\n\n# competition submissions \n# Additional source of data on user events\/activity, to merge with kernels\n# Some rows are missing the ID. We drop them (and then reparse the data as  a integer, not float\/nan)\nsubmissions = pd.read_csv('..\/input\/Submissions.csv',usecols=[\"SubmittedUserId\",\"SubmissionDate\"],\n                          parse_dates=[\"SubmissionDate\"],infer_datetime_format=True).dropna()#load('KernelVersions')\nprint(\"submissions\",submissions.shape)\nsubmissions.SubmittedUserId = submissions.SubmittedUserId.astype(int)\nsubmissions[\"SubmissionDate\"] = submissions[\"SubmissionDate\"].dt.floor('d')\nprint(f\"Last\/max submissions date {submissions['SubmissionDate'].max()}\")\nsubmissions.drop_duplicates(inplace=True)\nprint(\"Date level dedupped submissions\",submissions.shape[0])\n","af208dcf":"submissions.rename(columns={\"SubmittedUserId\":\"UserId\",\"SubmissionDate\":\"ActivityDate\"},inplace=True)\nkernel_versions.rename(columns={\"AuthorUserId\":\"UserId\",\"CreationDate\":\"ActivityDate\"},inplace=True)\n\n# New dataframe that will contain aggregated data about user activity, for defining target and prediction instances:\ndf_labels = pd.concat([kernel_versions,submissions]).drop_duplicates()\nprint(df_labels.shape)\ndf_labels.tail()","126d65dc":"## basic users info (name..).  It can also be joined with more data, e.g. teams, organization\n## PerformanceTier is leaky! it's not time stamped.\nusers = pd.read_csv('..\/input\/Users.csv').set_index(\"Id\").drop_duplicates().drop([\"PerformanceTier\",\"UserName\"],axis=1) # drop UserName, it's probably redundant if we have display name\nprint(users.shape)\nusers.head()","f2931e1f":"## https:\/\/stackoverflow.com\/questions\/32958526\/pandas-agg-multiple-summaries-on-same-column\n## https:\/\/nbviewer.jupyter.org\/gist\/TomAugspurger\/6e052140eaa5fdb6e8c0\n\n# df_labels_agg = df_labels.groupby(\"UserId\", as_index=True).agg({'ActivityDate':{\"first_Activity\": \"min\", \"last_Activity\": \"max\"}})\ndf_labels_agg = df_labels.groupby(\"UserId\", as_index=True).agg({'ActivityDate':[\"max\",\"min\"]}).rename(columns={ \"min\":\"first_Activity\", \"max\":\"last_Activity\"})\ndf_labels_agg.columns = df_labels_agg.columns.droplevel() # drop ActivityDate multiaxis level\ndf_labels_agg.reset_index(inplace=True)\ndf_labels_agg.drop_duplicates(inplace=True)\n\n# add time between first and last event, in weeks:\ndf_labels_agg['weeks_retention'] = round((df_labels_agg['last_Activity'] - df_labels_agg['first_Activity']) \/ np.timedelta64(1, 'W'))\n\nprint(df_labels_agg.shape)\ndf_labels_agg.sample(5)","6a3aaad5":"print(df_labels_agg['weeks_retention'].describe())\n\n## 25%           0.000000\n## 50%           1.000000\n## over half aren't active after first usage\/week!\n\nprint(\"\\n If excluding users with <=2 weeks retention, median is:\",df_labels_agg[df_labels_agg['weeks_retention']>2][\"weeks_retention\"].median())\n\ndf_labels_agg['weeks_retention'].hist();","4fc066b1":"# MINIMAL_RETENTION_HISTORY = minimal amount of history for users\nprint(f\"MINIMAL_RETENTION_HISTORY (Weeks): {MINIMAL_RETENTION_HISTORY}\\n\")\ndf = df_labels_agg.loc[df_labels_agg['weeks_retention']>=MINIMAL_RETENTION_HISTORY].copy() \ndf.weeks_retention = df.weeks_retention.astype(int)\n\nprint(df['weeks_retention'].describe(percentiles=[.1,.25, .5, .75,.9]))\ndf['weeks_retention'].hist();","e1fbae07":"print(\"Last date in data\",df.last_Activity.max())\nprint(\"%i users with 7.5+ years veterancy\"% df.loc[df['weeks_retention']>(53*7.5)].shape[0])\n\nprint(\"\\n counts of first activity over time:\")\ndf.first_Activity.hist();","901fe27c":"df[\"weeks_from_endDate\"] = round((END_DATE - df_labels_agg['first_Activity']) \/ np.timedelta64(1, 'W'))\n# df.weeks_from_endDate.describe()\n\nprint(df.shape[0])\ndf = df.loc[df.first_Activity.dt.year>=2012]\nprint(\"Early years dropped:\",df.shape[0])\n\nprint(f\" \\nMINIMAL_RETENTION_WEEKS_TARGET (Weeks of minimum target history): {MINIMAL_RETENTION_WEEKS_TARGET} \\n\")\ndf = df.loc[df[\"weeks_from_endDate\"] >= MINIMAL_RETENTION_WEEKS_TARGET] \n# ensure we have users \n### RECENT_CUTOFF_DATE = users who registered within less time than our retention prediction target (i.e \"more than 5 months\")\nprint(f\"After dropping too recent registrations:\",df.shape[0]) ## f strings! :)\n\ndf.weeks_retention.describe()","94e929e4":"# ### joiners per month stats\n# df.set_index(\"last_Activity\")[\"UserId\"].resample(\"M\").count().describe()","46f31299":"print(df[[\"UserId\",\"last_Activity\"]].nunique())","0cc392b2":"df[\"time_of_prediction\"] = df[\"first_Activity\"] + pd.to_timedelta(MINIMAL_RETENTION_HISTORY,unit=\"W\")\ndf[f\"retention_target_{MINIMAL_RETENTION_WEEKS_TARGET}\"] = (df[\"weeks_retention\"]>MINIMAL_RETENTION_WEEKS_TARGET).astype(int)\nprint(df[f\"retention_target_{MINIMAL_RETENTION_WEEKS_TARGET}\"].mean())\ndf.drop([\"last_Activity\"],axis=1,inplace=True)","894a9494":"# some users aren't in the user data!\n# + D\u05e8\u05dd\u05e4 \u05e7\u05e9\u05e8\u05da\u05df\u05e7\u05e8 \u05d5\u05d3\u05e7\u05e8\u05d3 )\u05d2\u05df\u05db\u05db\u05e7\u05e8\u05e7\u05de\u05d0 \u05e0\u05e7\u05d9\u05e9\u05d4\u05df\u05dd\u05e8\nprint(df.join(users,on=\"UserId\",how=\"inner\").shape)\n\ndf = df.join(users,on=\"UserId\",how=\"left\")\nprint(df.shape[0])\ndf.RegisterDate = pd.to_datetime(df.RegisterDate)\ndf =  df.loc[df.RegisterDate.dt.year>2012]\nprint(\"Users with no registration date dropped + earliest users;\",df.shape[0])\ndf.tail(3)","784d39ac":"df.to_csv(f\"metaKaggle_churn-{MINIMAL_RETENTION_WEEKS_TARGET}_retention-{MINIMAL_RETENTION_HISTORY}_hist-labels_v1.csv.gz\",index=False,compression=\"gzip\")","69832b17":"# # Load KernelTags table.\n# kernel_tags = load('KernelTags')","3b06f830":"# # Create temporary table to determine if a user's first kernel has a tag.\n# user_first_kernel = kernel_versions.iloc[kernel_versions.groupby('AuthorUserId')['CreationDate'].idxmin()]\n# user_first_kernel = pd.merge(\n#     user_first_kernel,\n#     kernel_tags,\n#     how='left',\n#     on='KernelId',\n#     suffixes=('', '_kernel_tags'))\n\n# # If right side of join is n\/a, it's because user's first notebook has no tag\/category.\n# user_first_kernel.loc[pd.notnull(user_first_kernel.TagId), 'TagId'] = 'has_category'\n# user_first_kernel.loc[pd.isnull(user_first_kernel.TagId), 'TagId'] = 'no_category'\n# user_first_kernel = user_first_kernel.rename(columns={'TagId': 'has_category'})","508e9ab4":"# cohort = 'has_category'\n\n# augmented_kernel_users = pd.merge(\n#     kernel_users,\n#     user_first_kernel,\n#     left_on='Id',\n#     right_on='AuthorUserId',\n#     suffixes=('', '_b'))[['Id', cohort, 'RegisterDate']]\n\n# dim_users = pd.merge(\n#     kernel_user_events,\n#     augmented_kernel_users,\n#     how='left',\n#     on='Id')\n# dim_users['weeks_from_signup'] = round((dim_users['Date'] - dim_users['RegisterDate']) \/ np.timedelta64(1, 'W'))\n# dim_users = dim_users[['Id', 'weeks_from_signup', cohort]].drop_duplicates()\n# dim_users = dim_users[dim_users['weeks_from_signup'] <= 6]\n\n# assert dim_users['Id'].nunique() == dim_users[dim_users['weeks_from_signup'] == 0].shape[0]\n\n# cohort_size = (\n#     dim_users[dim_users['weeks_from_signup'] == 0]\n#     .groupby([cohort], as_index=False).agg('count')[[cohort, 'Id']]\n#     .rename(columns={'Id': 'cohort_size'})\n# )\n# cohort_size = cohort_size[cohort_size['cohort_size'] > 1000]\n\n\n# users_by_cohort = (pd.merge(\n#     dim_users,\n#     cohort_size,\n#     on=cohort)\n#  .groupby(['weeks_from_signup', cohort, 'cohort_size'], as_index=False)\n#  .agg('count')\n#  .rename(columns={'Id': 'user_count'})\n# )\n\n# users_by_cohort['pct'] = users_by_cohort['user_count'] \/ users_by_cohort['cohort_size'] * 100","80070f08":"# plt.figure(figsize=(8, 6))\n# for a, b in users_by_cohort.groupby([cohort]):\n#     plt.plot(b['weeks_from_signup'], b['user_count'] \/ b['cohort_size'] * 100.0, label=a)\n# plt.title('Kernels Retention Curve')\n# plt.ylabel('% Active Users')\n# plt.xlabel('Weeks From Signup')\n# plt.legend()\n# plt.show()","c0594ac3":"We still see a heavy tailed distribution (as expected), many users may open an account just for a course, or without being serious, or Kaggle may simply not be \"sticky\" enough. \n\nDefining a \"real\" user is tricky. We can use 30 days , but a timespan greater than a single competition (2-4 months) might also be valid. We can also go for something more data driven (based on typical retentions). \n\n* We'll want to remove the \"newest\"\/most recent users, as users who registered within less than 3 months from the present time (\"today\"), can't possibly have 6 months retention!\n* Let's also look at the users with crazy long veterancy (maybe they're kaggle founders? Grandmasters?)","6db18a7e":"* We see massive growth over the past few years in newly registered users! \n* Growth seems to accelerate around 2015-2015, even more so after the Google acquisition (~2017)\n\n* If we were more interested in classic retention, we would look at retention based on the start date (i.e cohorts). It seems reasonabe that retention will be higher in later dates. \n","fffa347a":"It appears that users who added a tag to their first kernel have substatially higher retention rates! Let's look at it in a table.","11fa5b2f":"\n## Filter earliest and oldest users:\n\n* We will **drop** the **early years'** (2010-~2012) data , both to remove bias for \"kaggle admins\" and the big shift in behavior and growth seen.  \n    * I'll keep users from the past ~ 6 years. I'll admit to [personal bias due to being registered for 6 years](https:\/\/www.kaggle.com\/danofer), albeit active for only ~4-5\n* We could focus on just the last 2-3 years , but that may be a bit much.\n\n* We also **drop** users who registered very **recently** (past ~3 months , as of 26.5.2019\" = date kernel was last updated!)\n    * They won't have had enough time to show retention for our target\n    * *RECENT_CUTOFF_DATE* = users who registered within less time than our retention prediction target (i.e \"more than 5 months\")\n    * *MINIMAL_RETENTION_WEEKS_TARGET* = target we want to predict (User retention time in weeks). We'll filter data for users who can't have been active for less than this amount of time\n    * We'll add a \"helper\" column with months between the first activity and the current date. This will make it easier to filter by duration , rather than dates (useful for future versions of this data)\n","f3aa7e13":"# User Retention - Meta Kaggle Dataset\n\nRetention is among the most important metrics to understand. It is crucial in the development of a successful product. In this notebook, we'll use the Meta Kaggle dataset to learn how to visualize, understand, and explore this key performance indicator.\n\nThis notebook focuses on at users which have created\/forked at least one notebook (i.e kernels).\n\n\u2022 I forked from: https:\/\/www.kaggle.com\/wguedes\/learning-about-user-retention-meta-kaggle\n\n## Overview - What is retention?\nRetention is our ability to make users come back to our product within a certain time period. \n\nIn our example, we would like users who created a notebook today to use Kernels again when they write their next notebook.\n\n#### What's a good retention rate?\n*There's no magic number.* \nLower is better. \n\n* Good lecture about retention\/growth\/churn by Facebook's Alex Schultz:  [lecture](https:\/\/www.youtube.com\/watch?v=n_yHZ_vKjno) \n\n","57e48077":"### A Side Note\nThere are many different ways to look at retention. \n\nE.g. looking at the percentage of users who are active N weeks after they've signed up. Some other common approaches are to look at month-over-month retention or look at 7-day (or 30-day) active instead of 1-day active.\n\n### So What?\n\n* To take our insights to the next level, we must make them actionable.\n\n## Actionability - Turn Insights Into Action\nWe now have our baseline. We know that by the 8th week, 2% of the users are still using Kernels. It's time to explore ways to increase retention.\n\nCohort analysis is a great way to develop more actionable insights. The overall user retention is what we've plotted above. However, there are certain groups of users who are more engaged than others. Cohort analysis can help us identify them.\n\n### Kernel Categories\nAuthors can *tag* their notebooks. To tag a notebook is to associate it with a category.\n\n<img src=\"https:\/\/image.ibb.co\/gZhAu8\/tag.png\" alt=\"tag\" style=\"width: 400px\" \/>\n\nLet's see if user's who add tags to their first notebook behave any differently than our current baseline (all users). For that, we'll join our user events table with the `KernelTags` dataset.\n","a4f11ff7":"#### Concat, aggregate the user event tables\n* Concat then (After renaming)\n    * Rename columns after aggregating multiple functions on the same columns: https:\/\/nbviewer.jupyter.org\/gist\/TomAugspurger\/6e052140eaa5fdb6e8c0\n    * https:\/\/stackoverflow.com\/questions\/32958526\/pandas-agg-multiple-summaries-on-same-column\n* Get \"duration\" of all users (max time between first and last event)\n* Filter users who weren't active for a minimal duration (We saw ~80% dropout over the first 1-2 weeks. Many users could be \"tests\", temporary or not serious). ","8d59a4c0":"## Clean data for export\n* Add prediction time-point\n* Add target column, defined by MINIMAL_RETENTION_WEEKS_TARGET  (Instead of modelling problem as regression)\n* Drop some target-related columns (last activity..)\n* MErge with users metadata (mainly to get creation date). \n    * I will get features from Tthe remaining data externally. ","67604d78":"## Exploring User Retention of Kernels\n\nWe will be looking at what percentage of new users come back in the following week, the week after, and so on. This will tell us how *sticky* Kernels is. Once we have a grasp in the overall user retention, we'll look at ways to make our insights actionable.\n\nIn the following sections we will:\n- Import libraries and load tables\n- Prepare the data for exploration (Data Wrangling).\n- Compute overall week-over-week (WoW) user retention.\n- Explore ways to make our analysis actionable.\n","122419af":"## Data Wrangling\/loading\n* Load the datasets (and parse as datetime). \n\n* We can strip the hours\/time component and leave just the date (then drop duplicateS). Our aggregation will do this for us anyway though\n    * df['dates'].dt.floor('d')   # (or:) \n    * df['dates'].dt.date ","a9f1b234":"Kaggle claims ~1 (million users)[http:\/\/blog.kaggle.com\/2017\/06\/06\/weve-passed-1-million-members\/], but the numbers we see here are far smaller. This may be due to the dataset having been sampled, or simply due to only a small portion of registered users actually being \"serious\", i.e code-writing, competition entering DS :)\n\n\n* We already know how the (retention curve)[https:\/\/www.kaggle.com\/wguedes\/learning-about-user-retention-meta-kaggle] looks, and regardless we want users who can be considered \"Active\". We could define this by medals (equivalently to \"premium\"\/\"gold\" clients), but that is a bit too harsh. \nLet's filter by a minimal duration(e.g. >1 month, although we see that a cutoff of ~>1 week is the real seperator), then predict subsequent activity (possibly multiple times per user).\n","9a9c7da1":"# User Retention - Meta Kaggle Dataset\n\nRetention is among the most important metrics to understand. It is crucial in the development of a successful product. In this notebook, we'll use the Meta Kaggle dataset to learn how to visualize, understand, and explore this key performance indicator.\n\nI'm particularly interested in the usage of Kernels. Therefore this notebook will only look at users which have created\/forked at least one notebook.\n\nIn the end, I hope you'll be able to apply what you've learned and look at the retention of your own product.\n\n<img src=\"https:\/\/image.slidesharecdn.com\/metricsandksfsforbuildingahighperformingcustomersuccessteam-150410171710-conversion-gate01\/95\/customer-success-best-practices-for-saas-retention-metrics-and-ksfs-for-building-a-high-performing-customer-success-team-10-638.jpg?cb=1428686468\" alt=\"thatdbegreat\" style=\"width: 400px\"\/>\n\n## Overview\nBefore we dive into the data, let's understand what user retention is and why it's important. \n\n### What is retention?\nAt a high level, retention is our ability to make users come back to our product within a certain time period. \n\nIn our example, we would like users who created a notebook today to use Kernels again when they write their next notebook.\n\n### Why is retention important?\n***Retention tells us if we're building something worth building***. It helps us understand if we're providing value to our users. In ther words, retention tells us whether or not our product has market fit.\n\nIf we provide no value, we don't have a sustainable business.\n\nAcquiring new users is also expensive. And if we can't retain them, all the money put into user acquisition and growth hacking tactics will go to waste. \n\n### Spoiler Alert! What's a good retention rate?\nMaybe you already know what retention is. You might be simply trying to figure out what is a good number for your retention rate.\n\nA good retention rate is 15%! No wait... It's 10%. But it could also be 28%, 95% is definitely good I guess - *There's no magic number.* \n\nPersonally, I was frustrated when I couldn't find that magic number. Only after watching Alex Schultz's  [lecture](https:\/\/www.youtube.com\/watch?v=n_yHZ_vKjno) about retention that I understood why no particular number existed . I highly encourage you to do the same!\n\n## Exploring User Retention of Kernels\nIt's time for us to look at the data! \n\nWe will be looking at what percentage of new users come back in the following week, the week after, and so on. This will tell us how *sticky* Kernels is. Once we have a grasp in the overall user retention, we'll look at ways to make our insights actionable.\n\nIn the following sections we will:\n- Import libraries and load tables\n- Prepare the data for exploration (Data Wrangling).\n- Compute overall week-over-week (WoW) user retention.\n- Explore ways to make our analysis actionable.\n\n\n## Libraries and Tables\n\nTo compute user retention, we need only two tables:\n* A table with user signup dates\n* A table with user events (used to determine the dates an user was active)\n\nWe'll derive these tables from the `KernelVersions` dataset + competition activity\n\n* Note that we will set the \"start\" date by the first \"Activity\" from these tables, not the user registration date . (A user might register than have no activity for a while, then become active. We'll use the registration date as a seperate feature, but not for defining the target)."}}