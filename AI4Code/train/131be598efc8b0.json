{"cell_type":{"b998cfcf":"code","5ba9aaaf":"code","a399206f":"code","3d6b988d":"code","3722926d":"code","fa339ec7":"code","fc8e56ad":"code","bd8c6afa":"code","77ba5b8c":"code","81cc0b24":"code","91b21404":"code","f3de3e27":"code","4e724031":"code","61f616c3":"code","f6bc7822":"code","52a04315":"code","da4d70ec":"code","49f00262":"code","16b11d29":"code","34c4c801":"code","a216ab3c":"code","75f61c42":"code","231abe15":"code","0364d469":"code","aded41c5":"code","c43d5a19":"code","fce0362e":"code","c5c1dbdb":"code","08e19d41":"markdown"},"source":{"b998cfcf":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import models, datasets, layers, optimizers, metrics, losses, utils","5ba9aaaf":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a399206f":"(train_values, train_labels), (test_values, test_labels) = datasets.reuters.load_data(num_words=15000)\n\n\nprint(f'{train_values.shape} Number of training records in reuters dataset')\nprint(f'{test_values.shape} Number of testing records in reuters dataset')","3d6b988d":"print('Values inside 1st record')\nprint(train_values[0])","3722926d":"print(f'Label of 1st record {train_labels[0]}')","fa339ec7":"# We have restricted number of words to 15000. Let check it\nmax([max(seq) for seq in train_values])","fc8e56ad":"# word index from inbuild dataset\nword2index = datasets.imdb.get_word_index()\nindex2word = {value: keyword for keyword, value in word2index.items()}","bd8c6afa":"# Let check words present in both dictionary word2index and index2word\nprint(f'Awesome word index is {word2index.get(\"awesome\")}')","77ba5b8c":"print(f'At 1187 index which word is present {index2word.get(1187)}')","81cc0b24":"# (index - 3) because 0,1,2 index's are already assign to padding, start of sequence and unknown respectively\nprint('Actual sentence present inside 1st record')\nprint(' '.join([index2word.get(index - 3, '?') for index in train_values[0]]))","91b21404":"# Let's vectorize sentences with onehot encoding approach\ndef onehot_vectorization(sequences, num_words=15000):\n    vectors = np.zeros((len(sequences), num_words))\n    for i, seq in enumerate(sequences):\n        vectors[i, seq] = 1.0\n        \n    return vectors","f3de3e27":"x_train = onehot_vectorization(train_values)\nx_test  = onehot_vectorization(test_values)\n\nprint(f'Shape of train values are {x_train.shape} and test values are {x_test.shape}')","4e724031":"# Let's print original vector and onehot vector of 1st record\nprint(train_values[0])\nprint(x_train[0])","61f616c3":"# Let's have look into the train and test labels\ny_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')\n\nprint(y_train)\nprint(y_test)","f6bc7822":"# Let's first of all figure out how many class we have\nprint(f'Number of unique output categories we have in reuters is {len(np.unique(y_train))}')","52a04315":"# Now you have seen the output labels in above section and based on that we have to choose loss function. Let's talk about it more\n# When you have values like [0, 1, 2, 3, 4, 5] in output labels it is suggest to use sparse_categorical_entropy as a loss function\n# Or you have option of converting this output labels into onehot encoding just like we did for our input records\n# We are going to use onehot version of our output records \n\ny_train = utils.to_categorical(y_train)\ny_test = utils.to_categorical(y_test)\n\nprint(y_train.shape)\nprint(y_test.shape)","da4d70ec":"# Split dataset into train and validation making train data of 15000 and 10000 records respectively\nrandom_shuf = np.arange(x_train.shape[0])\nnp.random.shuffle(random_shuf)\n\nx_valid = x_train[random_shuf[:2000]]\ny_valid = y_train[random_shuf[:2000]]\n\nx_train = x_train[random_shuf[2000:]]\ny_train = y_train[random_shuf[2000:]]\n\nprint(x_train.shape, y_train.shape)\nprint(x_valid.shape, y_valid.shape)","49f00262":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(15000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))\n\nmodel.compile(loss=losses.categorical_crossentropy, optimizer=optimizers.RMSprop(lr=0.001), metrics=metrics.categorical_accuracy)\nhistory = model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_valid, y_valid))","16b11d29":"history_dict = history.history\nprint(history_dict.keys())","34c4c801":"loss_values = history_dict.get('loss')\nval_loss_values = history_dict.get('val_loss')\n\nepochs = range(1, len(history_dict.get('loss')) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'o', label='Validtion loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","a216ab3c":"plt.clf()\n\nacc_values = history_dict.get('categorical_accuracy')\nval_acc_values = history_dict.get('val_categorical_accuracy')\n\nepochs = range(1, len(history_dict.get('categorical_accuracy')) + 1)\n\nplt.plot(epochs, acc_values, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc_values, 'o', label='Validtion accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","75f61c42":"# Try another network with more neurons then previously mentioned\nmodel = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(15000,)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))\n\nmodel.compile(loss=losses.categorical_crossentropy, optimizer=optimizers.RMSprop(lr=0.001), metrics=metrics.categorical_accuracy)\nhistory = model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_valid, y_valid))","231abe15":"loss_values = history_dict.get('loss')\nval_loss_values = history_dict.get('val_loss')\n\nepochs = range(1, len(history_dict.get('loss')) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'o', label='Validtion loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","0364d469":"plt.clf()\n\nacc_values = history_dict.get('categorical_accuracy')\nval_acc_values = history_dict.get('val_categorical_accuracy')\n\nepochs = range(1, len(history_dict.get('categorical_accuracy')) + 1)\n\nplt.plot(epochs, acc_values, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc_values, 'o', label='Validtion accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","aded41c5":"# By observing both graphs we can say that 8-9 epochs is more than enough for this architecture to perform best on both training and validation records\n# Now train same network with 9 epochs\n# By recompiling same network all the train weights will be lost and it will be retraining again from scratch\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(15000,)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))\n\nmodel.compile(loss=losses.categorical_crossentropy, optimizer=optimizers.RMSprop(lr=0.001), metrics=metrics.categorical_accuracy)\nhistory = model.fit(x_train, y_train, epochs=9, batch_size=512, validation_data=(x_valid, y_valid))","c43d5a19":"results = model.evaluate(x_test, y_test)\nprint(f'The loss on test records is {results[0]:.3f} and accuracy is {results[1]:.3f}')","fce0362e":"predictions = model.predict(x_test)\nprint(predictions)","c5c1dbdb":"# Now to find each categories we have to use argsmax\nprint(predictions[0])\nprint('\\n')\nprint(f'Maximum probability is {np.max(predictions[0]):.3f} at {np.argmax(predictions[0])} position')","08e19d41":"### As you can see we not getting good performs on this records one of the reason is bottleneck of information space between layer as our output categories are 46 in numbers so having 16 neurons in intermediate layer is not enough."}}