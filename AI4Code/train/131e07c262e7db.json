{"cell_type":{"62b1c52d":"code","d3043f19":"code","758c9150":"code","a01f092c":"code","bef43cfd":"code","01a47363":"code","09d894e0":"code","cc3d7ae2":"code","436ad2ff":"code","37dab362":"code","1ef365c0":"markdown","a9313dd1":"markdown","f90043ab":"markdown","0112a4a1":"markdown","3ac09ea7":"markdown","b79ff6d4":"markdown"},"source":{"62b1c52d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","d3043f19":"heart_data = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\nheart_data.head()","758c9150":"sns.histplot(data = heart_data, x= 'Age', hue = 'HeartDisease', stat = \"percent\", multiple = \"stack\")\nprint(f\"Mean Age: {np.mean(heart_data['Age'])}\")","a01f092c":"sns.countplot(data = heart_data, x = 'Sex', hue = 'HeartDisease')","bef43cfd":"sns.histplot(data = heart_data, x= 'RestingBP', hue = 'HeartDisease', multiple = \"stack\")","01a47363":"sns.histplot(data = heart_data, x= 'Cholesterol', hue = 'HeartDisease', multiple = \"stack\")","09d894e0":"heart_data_lr = pd.get_dummies(heart_data, columns = ['Sex','ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'])\nsns.heatmap(heart_data_lr.corr(), cmap = 'coolwarm')","cc3d7ae2":"X = heart_data_lr.drop(columns = ['HeartDisease'])\ny = heart_data_lr['HeartDisease']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = True)\n\nlr_model = LogisticRegression(solver='liblinear')\n\nlr_model.fit(X_train, y_train)\nlr_model_test = lr_model.predict(X_test)\n\nprint(accuracy_score(y_test, lr_model_test))","436ad2ff":"heart_data_clf = pd.get_dummies(heart_data, columns = ['Sex','ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'])\n\nX = heart_data_clf.drop(columns = ['HeartDisease'])\ny = heart_data_clf['HeartDisease']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = True)\n\nclf = DecisionTreeClassifier()\n\nclf.fit(X_train, y_train)\nclf_test = clf.predict(X_test)\n\nprint(accuracy_score(y_test, clf_test))","37dab362":"criterion = ['gini', \"entropy\"]\nsplitter = ['best', \"random\"]\nmax_depth = range (0, 11)\n\nmax_accuracy = 0.0\n\nfor crit in criterion:\n    for split in splitter:\n        for depth in max_depth:\n            clf = DecisionTreeClassifier()\n            clf.fit(X_train, y_train)\n            clf_test = clf.predict(X_test)\n            \n            if max_accuracy < accuracy_score(y_test, clf_test):\n                max_accuracy =  accuracy_score(y_test, clf_test)\n                best_crit = crit\n                best_split = split\n                best_depth = depth\n                \nprint(f\"The top criterion was: {best_crit}. The top splitter was: {best_split}. The best max depth was : {best_depth} Our accuracy was {max_accuracy}\")","1ef365c0":"## Try to optimize the decision tree for best results using a grid search","a9313dd1":"## Import Data","f90043ab":"## Logistic Regression","0112a4a1":"# Comparing logistic regression and a decision tree model to predict heart disease.","3ac09ea7":"## Importing our libraries","b79ff6d4":"## Decision Tree"}}