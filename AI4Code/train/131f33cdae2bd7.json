{"cell_type":{"ebc8abf3":"code","7c8d7f70":"code","b311471a":"code","3c4b501f":"code","a42946bb":"code","a3d456b0":"code","851e7eb2":"code","a2b3e639":"code","feef5f76":"code","0ba59d77":"code","1c13e143":"code","2f201a14":"code","c521a492":"code","6ea379c2":"code","d22d646a":"code","27f01def":"code","28a4e0ce":"code","6833b546":"code","76299833":"code","60694bfe":"code","c6afba1d":"code","de5ec485":"code","f9defbec":"code","d57c1a07":"code","85db288e":"code","f7e88de6":"code","85fc038f":"code","5a3f6d7c":"code","4da57db7":"code","6757d59f":"markdown","b84ad2b3":"markdown","3294643e":"markdown","c2bca539":"markdown","532ee2a6":"markdown","50f47405":"markdown","eca2e27b":"markdown","97820af0":"markdown","a2805505":"markdown","d5d9417e":"markdown","33399e52":"markdown","f592703d":"markdown","07e48b79":"markdown","a1f7e4bb":"markdown","82246568":"markdown","42c94cc1":"markdown","35a88e9a":"markdown"},"source":{"ebc8abf3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7c8d7f70":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndf_sample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","b311471a":"df_train.head(5)","3c4b501f":"df_test.head(5)","a42946bb":"df_sample.head(5)","a3d456b0":"## check for null values\ndf_train.isnull().sum()\ndf_test.isnull().sum()","851e7eb2":"## Most common keywords in train dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(9,6))\nsns.countplot(y = df_train.keyword, order= df_train.keyword.value_counts().iloc[:15].index)\nplt.title('Top 15 keywords')\nplt.show()","a2b3e639":"## Plot to checkout null values in train data\ndf_train.isna().sum().plot(kind = 'bar')\nplt.title('Plot to checkout null values in train data')\nplt.show()","feef5f76":"## Plot to checkout null values in test data\ndf_test.isna().sum().plot(kind = 'bar')\nplt.title('Plot to checkout null values in test data')\nplt.show()","0ba59d77":"df_train = df_train.drop(['location','keyword'],axis=1)\ndf_test = df_test.drop(['location','keyword'],axis=1)","1c13e143":"df_train.head(5), df_test.head(5)","2f201a14":"## Plot target feature values\n\nsns.countplot(x = 'target', data = df_train)\n","c521a492":"## Plot length of the train and test dataset\n\nplt.hist(df_train['text'].str.len(), label = 'train_tweets')\nplt.hist(df_test['text'].str.len(),label = 'test_tweets')\nplt.legend()\nplt.show()","6ea379c2":"## Checkout a disaster tweet\nd_t = df_train[df_train['target'] == 1]['text']\nfor i in range(1,5):\n    print(d_t[i])","d22d646a":"## Checkout a non disaster tweet\nnd_t = df_train[df_train['target'] != 1]['text']\nprint(nd_t.head(5))","27f01def":"## Word cloud of disaster and non-disaster tweets \n## to see most repeating word\n\nfrom wordcloud import WordCloud\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = [150, 50])\n\nwc1 = WordCloud().generate(''.join(d_t))\nax1.imshow(wc1)\nax1.axis('off')\nax1.set_title('Disaster tweets', fontsize = 18)\n\nwc2 = WordCloud().generate(''.join(nd_t))\nax2.imshow(wc2)\nax2.axis('off')\nax2.set_title('Non Disaster tweets', fontsize = 18)\n\n","28a4e0ce":"import re\nimport string\ndef clean_text(t):\n    # Convert to lower\n    t = t.lower()\n    # remove html tags\n    t = re.sub(r'\\[.*?\\]',' ', t)\n    # remove link\n    t = re.sub(r'https?:\/\/\\S+|www\\.\\S+',' ', t)\n    #remove line breaks\n    t = re.sub(r'\\n',' ',t)\n    #Remove trailing spaces, tabs\n    t  = re.sub('\\s+',' ',t)\n    # remove punctuation\n#     t = re.sub('[%s]' % re.escape(string.punctuation), t)\n    # Remove special characters\n    t = re.sub('\\w*\\d\\w*','',t)\n    return t\n\n## Apply clean function on random train string \ntest_str = df_train.loc[417, 'text']\nprint('Original text: '+test_str+'\\n')\nprint('Original text after cleaning '+clean_text(test_str))","6833b546":"## Applying clean function on train & test sets\ndf_train['text'] = df_train['text'].apply(lambda x:clean_text(x))\ndf_test['text'] = df_test['text'].apply(lambda x:clean_text(x))\n\n## checkout train after cleaning\ndf_train['text'].head(5)","76299833":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import RegexpTokenizer\n\n# tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\ntokenizer = RegexpTokenizer(r'\\w+')\n## Applying tokenization function on train & test sets\ndf_train['text'] = df_train['text'].map(tokenizer.tokenize)\ndf_test['text'] = df_test['text'].map(tokenizer.tokenize)\n## checkout train dataset tokens\ndf_train['text'].head(5)\n","60694bfe":"def remove_stopwords(t):\n    words = [w for w in t if w not in stopwords.words('english')]\n    return words\n\ndf_train['text'] =df_train['text'].apply(lambda x: remove_stopwords(x))\ndf_test['text'] =df_test['text'].apply(lambda x: remove_stopwords(x))\n\n## checkout train dataset without stopwords\ndf_train['text'].head(5)","c6afba1d":"def lem_words(t):\n    l = WordNetLemmatizer()\n    return [l.lemmatize(w) for w in t]\n\ndf_train['text'] =df_train['text'].apply(lambda x: lem_words(x))\ndf_test['text'] =df_test['text'].apply(lambda x: lem_words(x))\n\n## checkout train dataset with lemmatized words\ndf_train['text'].head(5)","de5ec485":"## Transform tokens into sentences \n\ndef combine_txt(t):\n    c  = ' '.join(t)\n    return c\n\ndf_train['text'] =df_train['text'].apply(lambda x: combine_txt(x))\ndf_test['text'] =df_test['text'].apply(lambda x: combine_txt(x))\n\n## checkout train dataset with lemmatized words\ndf_train['text'].head(5)","f9defbec":"from sklearn.feature_extraction.text import CountVectorizer\nc = CountVectorizer()\ntr_v = c.fit_transform(df_train['text'])\nte_v = c.fit_transform(df_test['text'])\n\nprint(tr_v[0].todense())\n","d57c1a07":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(min_df = 2, max_df = 0.5, ngram_range = (1,2))\ntr_t = tfidf.fit_transform(df_train['text'])\nte_t = tfidf.transform(df_test['text'])\n\n","85db288e":"import xgboost as xg\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nparam = xg.XGBClassifier(max_depth = 5, n_estimators = 500,\n                        learning_rate = 0.08, nthread = 10, colsample_bytree = 0.8)\n\nvector_score = cross_val_score(param, tr_v, df_train['target'],\n                              cv=5, scoring='f1')\nprint(vector_score)\n\ntfidf_score = cross_val_score(param, tr_t, df_train['target'],\n                              cv=5, scoring='f1')\nprint(tfidf_score)\n","f7e88de6":"from sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\nvector_score = cross_val_score(lg, tr_v, df_train['target'],\n                              cv=5, scoring='f1')\nprint(vector_score)\n\ntfidf_score = cross_val_score(lg, tr_t, df_train['target'],\n                              cv=5, scoring='f1')\nprint(tfidf_score)","85fc038f":"from sklearn.naive_bayes import MultinomialNB as mb\n\nm = mb()\nvector_score = cross_val_score(m, tr_v, df_train['target'],\n                              cv=5, scoring='f1')\nprint(vector_score)\n\ntfidf_score = cross_val_score(m, tr_t, df_train['target'],\n                              cv=5, scoring='f1')\nprint(tfidf_score)","5a3f6d7c":"m.fit(tr_t,df_train['target'])\npred = m.predict(te_t)","4da57db7":"s = pd.DataFrame({'Id':df_test['id'],\n                 'Target':pred})\ns.to_csv('s.csv',index = False)\ns = pd.read_csv('s.csv')\ns.head(5)","6757d59f":"# 3. Tokenization\n\nTokenize the cleaned sentences","b84ad2b3":"# 10. Prediction","3294643e":"Observation: Location has highest na values followed by keyword in both test and train datasets. Anyways, we do not need these features for prediction. Hence, we will drop these features from the dataset.","c2bca539":"\n[0.51104101 0.39851715 0.49324324 0.41164659 0.56161972]\n\n[0.4730473  0.39309684 0.4193849  0.39918117 0.55879752]\n\n","532ee2a6":"## Observation:\n\nWe observe some words like http , t, co , u^ are most prominent words in tweets and they have to be cleaned for better accurate results and lesser tags.","50f47405":"# 1. Load the dataset","eca2e27b":"## 6. Vectorizing text\n\nCountVectorizer is used to transform a given text into a vector on the basis of the frequency(count) of each word that occurs in the entire text.It involves counting the number of occurences each words appears in a document(text)","97820af0":"# 5.Lemmatization\n\nLemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item.\n\nExamples of lemmatization:\n\n1.playing ,plays and played all these 3 letters will be converted to play after lemmatization\n\n2.change , changing , changes , changed and changer all these letters will be converted to change after lemmatization","a2805505":"# 9. Naive bayes ","d5d9417e":"# 8. XGB Classifier","33399e52":"# 8. Logistic Regression","f592703d":"# 2. EDA","07e48b79":"# 2. Data cleaning","a1f7e4bb":"# Submission","82246568":"# 7. TFIDF\n\nIt stands for Term Frequency-Inverse document frequency.It is a techinque to quantify a word in documents,we generally compute a weight to each word which signifies the importance of the word which signifies the importance of the word in the document and corpus","42c94cc1":"## Observation:\n\nNaive bayes has the highest vector and tfidf score among the 3 algos.","35a88e9a":"# 4. Stopwords\n\nremove unnecessary words that do not carry any meaning"}}