{"cell_type":{"242441ce":"code","3d6bd198":"code","4b32024b":"code","44ff2c42":"code","98b244a9":"code","239b9647":"code","6c4837b5":"code","5abc4fe7":"code","f704a60c":"code","af2ac7e5":"code","497f1c3c":"code","e06537c7":"code","2c3fcae9":"code","6b77a4dd":"code","07aa8a05":"code","6e907448":"code","a959343f":"code","1ea7fdd1":"code","638221a3":"code","d318b96c":"code","97951dc0":"code","44263593":"code","75e9231d":"code","98d0e29f":"code","645e657f":"code","d11130fb":"code","cfdc4555":"code","e5ec9c91":"code","a078169e":"code","76455210":"code","465fc0ab":"code","ab532d48":"markdown","2b7e042d":"markdown","fed058ec":"markdown","493ff1b5":"markdown","f3f46ee3":"markdown","73802ec7":"markdown","1531fd8d":"markdown","b02b5fcd":"markdown","aaa995e8":"markdown","1a5422f5":"markdown","74f41a7e":"markdown","d95de064":"markdown","cc4bb5b4":"markdown","e05840cc":"markdown","ff4d72cd":"markdown","e431439f":"markdown","f773d3cc":"markdown","738a09f6":"markdown"},"source":{"242441ce":"# Importing pandas\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Loading dataset\ndf = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv',header=0)","3d6bd198":"# Inspecting data\ndf.head()","4b32024b":"# Inspecting basic information out of columns\ndf.info()","44ff2c42":"# Displaying summary statistics\ndf.describe()","98b244a9":"# Creating a list of Object data type columns\nobj_cols = df.select_dtypes(np.object).columns.tolist()\n# Checking the categorical values in the Object columns\ndef check_value_counts(col_list):\n  for col in col_list:\n    print('-----------------------------')\n    print(round((df[col].value_counts()\/df.shape[0])*100,2))\n    print('-----------------------------')\n\ncheck_value_counts(obj_cols)","239b9647":"# Assigning 0 and 1 to Yes and No\ndf['SeniorCitizen'] = df['SeniorCitizen'].map({0:'No',1:'Yes'})","6c4837b5":"#Binning the tenure column\ncut_labels = ['0-12', '13-24', '25-36', '37-48','49-60','61-72']\ncut_bins = [0, 12,24,36,48,60,72]\ndf['Tenure Period'] = pd.cut(df['tenure'], bins=cut_bins, labels=cut_labels)\ndf['Tenure Period'].value_counts()","5abc4fe7":"#Binning the MonthlyCharges column\ncut_labels = ['0-20', '21-40', '41-60', '61-80','81-100','101-120']\ncut_bins = [0, 20,40,60,80,100,120]\ndf['MonthlyCharges_Range'] = pd.cut(df['MonthlyCharges'], bins=cut_bins, labels=cut_labels)\ndf['MonthlyCharges_Range'].value_counts()","f704a60c":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'],errors='coerce')\ndf['TotalCharges'].describe()","af2ac7e5":"#Binning the Age column\ncut_labels = ['0-1000', '1001-2000','2001-4000','4001-6000','6001-8000','8001-10000']\ncut_bins = [0, 1000,2000,4000,6000,8000,10000]\ndf['TotalCharges_Range'] = pd.cut(df['TotalCharges'], bins=cut_bins, labels=cut_labels)\ndf['TotalCharges_Range'].value_counts()","497f1c3c":"# Dropping colummns that are not required\ncols_to_drop = ['customerID','MonthlyCharges','tenure','TotalCharges']\ndf.drop(labels=cols_to_drop,axis=1,inplace=True)","e06537c7":"# Sanity checks\ndf.head(4)","2c3fcae9":"# Checking count of null values by the columns\ndf.isna().sum()","6b77a4dd":"# Missing values imputation\ndf['TotalCharges_Range'].fillna(df['TotalCharges_Range'].mode()[0], inplace=True)\ndf['Tenure Period'].fillna(df['Tenure Period'].mode()[0], inplace=True)","07aa8a05":"# Importing LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Instantiating LabelEncoder\nle=LabelEncoder()\n\n# Iterating over all the values of each column and extract their dtypes\nfor col in df.columns.to_numpy():\n    # Comparing if the dtype is object\n    if df[col].dtypes in ('object','category'):\n    # Using LabelEncoder to do the numeric transformation\n        df[col]=le.fit_transform(df[col].astype(str))","6e907448":"# Sanity Check\ndf.head()","a959343f":"# Putting feature variable to X\nX = df.drop('Churn',axis=1)\n\n# Putting response variable to y\ny = df['Churn']","1ea7fdd1":"from sklearn.model_selection import train_test_split","638221a3":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\nX_train.shape, X_test.shape","d318b96c":"from sklearn.tree import DecisionTreeClassifier","97951dc0":"dt = DecisionTreeClassifier(max_depth=3,random_state=43)\ndt.fit(X_train, y_train)","44263593":"# Install required dependancy\n!pip install six\n!pip install pydotplus\n!pip install graphviz","75e9231d":"# Importing required packages for visualization\nfrom IPython.display import Image  \nfrom six import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydotplus, graphviz","98d0e29f":"# plotting tree with max_depth=3\ndot_data = StringIO()  \n\nexport_graphviz(dt, out_file=dot_data, filled=True, rounded=True,\n                feature_names=X.columns, \n                class_names=['Churn', \"Not Churn\"])\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","645e657f":"# Uncomment below line of code to save the Decision Tree Viz to a pdf file.\n#graph.write_pdf(\"dt_heartdisease.pdf\")","d11130fb":"from sklearn.metrics import confusion_matrix, accuracy_score","cfdc4555":"y_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)","e5ec9c91":"print(accuracy_score(y_train, y_train_pred))\nconfusion_matrix(y_train, y_train_pred)","a078169e":"print(accuracy_score(y_test, y_test_pred))\nconfusion_matrix(y_test, y_test_pred)","76455210":"# Let's check the overall accuracy.\ntrainaccuracy= accuracy_score(y_train, y_train_pred)\ntestaccuracy= accuracy_score(y_test, y_test_pred)\n\nconfusion_TRN = confusion_matrix(y_train, y_train_pred)\nconfusion_TST = confusion_matrix(y_test, y_test_pred)","465fc0ab":"TP = confusion_TRN[1,1] # true positive \nTN = confusion_TRN[0,0] # true negatives\nFP = confusion_TRN[0,1] # false positives\nFN = confusion_TRN[1,0] # false negatives\n\nTP_TST = confusion_TST[1,1] # true positive \nTN_TST = confusion_TST[0,0] # true negatives\nFP_TST = confusion_TST[0,1] # false positives\nFN_TST = confusion_TST[1,0] # false negatives\n\ntrainsensitivity= TP \/ float(TP+FN)\ntrainspecificity= TN \/ float(TN+FP)\n\ntestsensitivity= TP_TST \/ float(TP_TST+FN_TST)\ntestspecificity= TN_TST \/ float(TN_TST+FP_TST)\n\n# Let us compare the values obtained for Train & Test:\nprint('-'*30)\nprint('On Train Data')\nprint('-'*30)\nprint(\"Accuracy    : {} %\".format(round((trainaccuracy*100),2)))\nprint(\"Sensitivity : {} %\".format(round((trainsensitivity*100),2)))\nprint(\"Specificity : {} %\".format(round((trainspecificity*100),2)))\nprint('-'*30)\nprint('On Test Data')\nprint('-'*30)\nprint(\"Accuracy    : {} %\".format(round((testaccuracy*100),2)))\nprint(\"Sensitivity : {} %\".format(round((testsensitivity*100),2)))\nprint(\"Specificity : {} %\".format(round((testspecificity*100),2)))\nprint('-'*30)","ab532d48":"#### **Customer Churn in TELCOs**\n\nCompanies usually have a greater focus on customer acquisition and keep retention as a secondary priority. However, it can cost five times more to attract a new customer than it does to retain an existing one. Increasing customer retention rates by 5% can increase profits by 25% to 95%, according to research done by Bain & Company.\n\n_Churn_ is a metric that shows customers who stop doing business with a company or a particular service, also known as customer attrition. By following this metric, what most businesses could do was try to understand the reason behind churn numbers and tackle those factors, with reactive action plans\n","2b7e042d":"## 2. Exploratory Data Analysis","fed058ec":"### 2.3 Qualitative Data Analysis","493ff1b5":"Decision Trees are simple and intutive models, However the are high variance models i.e the slight change in train data may result in poor performance on the test as they try to overfit.\n\nAltough our model has stable results on `Train` and `Test data`, this model has low `Sensitivity` and high `Specificity`, to  further improve the performance we need to do Hyperparameter tuning.","f3f46ee3":"Since the data is categorical, the best strategy to impute them is by taking most frequent values","73802ec7":"### 2.5 Label Encoding","1531fd8d":"## 3. Model Building","b02b5fcd":"The main goal is to develop a machine learning model capable to predict customer churn based on the customer\u2019s data available.","aaa995e8":"<p>I'll using the A sample <a href=\"https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn\">Teleco Churn<\/a> dataset from Kaggle. The structure of this notebook is as follows:<\/p>\n<ul>\n<li>First, loading and viewing the dataset.<\/li>\n<li>The dataset has a mixture of both numerical and non-numerical features, that it contains values from different ranges, plus that it contains a number of missing entries.<\/li>\n<li>Preprocessing of the dataset to ensure the machine learning model we choose can make good classifications.<\/li>\n<li>After our data is in good shape, exploratory data analysis to build our intuitions.<\/li>\n<li>Finally, building a machine learning model that can predict if an individual would churn the service.<\/li>\n<\/ul>\n\n\n- `Author - Chinmay Gaikwad`\n- `Email - chinnmaygaikwad123@gmail.com`","1a5422f5":"## 1. Introduction","74f41a7e":"### 2.4 Missing values","d95de064":"### 2.1 Data Load","cc4bb5b4":"## 5. Model Evaluation","e05840cc":"## 4. Visualization","ff4d72cd":"## Thank You!","e431439f":"### 2.2 Data Summary","f773d3cc":"There are few quantitative features which we would be converting into categorical in order to perform classification using Decision Tree model.","738a09f6":"### 2.6 Train Test Split"}}