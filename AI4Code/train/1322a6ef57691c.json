{"cell_type":{"72de41a9":"code","c5fd2567":"code","3909cb6d":"code","3e318c39":"code","f00f3d08":"code","ce20d256":"code","7d1956f4":"code","36f75d2e":"markdown","076e91de":"markdown","7d1f4ce0":"markdown","c33af396":"markdown","d43faa4d":"markdown","d2875dbd":"markdown"},"source":{"72de41a9":"import cupy as cp\nimport cudf, cuml\nimport pandas as pd\nimport numpy as np\nfrom cuml.manifold import TSNE, UMAP\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score\n%matplotlib inline","c5fd2567":"train = cp.load('..\/input\/giba-s-fft-features-only\/TRAIN.npy')\ntest = cp.load(\"..\/input\/giba-s-fft-features-only\/TEST.npy\")","3909cb6d":"train_test = cp.vstack([train, test])","3e318c39":"%%time\numap = UMAP(n_components=2, n_neighbors=15, random_state=42)\ntrain_test_2D = umap.fit_transform(train_test)\ntrain_test_2D = cp.asnumpy(train_test_2D)\n\ntrain_2D = train_test_2D[:train.shape[0], :]\ntest_2D = train_test_2D[train.shape[0]:, :]\n\nplt.figure(figsize=(10,10))\nplt.scatter(train_2D[:,0], train_2D[:,1], alpha=0.3)\nplt.scatter(test_2D[:,0], test_2D[:,1], alpha=0.3)","f00f3d08":"%%time\numap = UMAP(n_components=2, n_neighbors=10, random_state=42)\ntrain_test_2D = umap.fit_transform(train_test)\ntrain_test_2D = cp.asnumpy(train_test_2D)\n\ntrain_2D = train_test_2D[:train.shape[0], :]\ntest_2D = train_test_2D[train.shape[0]:, :]\n\nplt.figure(figsize=(10,10))\nplt.scatter(train_2D[:,0], train_2D[:,1], alpha=0.3)\nplt.scatter(test_2D[:,0], test_2D[:,1], alpha=0.3)","ce20d256":"%%time\numap = UMAP(n_components=2, n_neighbors=7, random_state=42)\ntrain_test_2D = umap.fit_transform(train_test)\ntrain_test_2D = cp.asnumpy(train_test_2D)\n\ntrain_2D = train_test_2D[:train.shape[0], :]\ntest_2D = train_test_2D[train.shape[0]:, :]\n\nplt.figure(figsize=(10,10))\nplt.scatter(train_2D[:,0], train_2D[:,1], alpha=0.3)\nplt.scatter(test_2D[:,0], test_2D[:,1], alpha=0.3)","7d1956f4":"%%time\numap = UMAP(n_components=2, n_neighbors=5, random_state=42)\ntrain_test_2D = umap.fit_transform(train_test)\ntrain_test_2D = cp.asnumpy(train_test_2D)\n\ntrain_2D = train_test_2D[:train.shape[0], :]\ntest_2D = train_test_2D[train.shape[0]:, :]\n\nplt.figure(figsize=(10,10))\nplt.scatter(train_2D[:,0], train_2D[:,1], alpha=0.3)\nplt.scatter(test_2D[:,0], test_2D[:,1], alpha=0.3)","36f75d2e":"## With n_neighbors=5\n\nBelow this you basically lose all structure again","076e91de":"# UMAP\n\nThe default number of neighbors (`n_neighbors`) used in UMAP is 15, playing with it will give you different results. I've typically found reducing it gives interesting results while increasing it gives you a blob with less structure. Perhaps that's just a reflection of the size of the datasets I'm playing with","7d1f4ce0":"#\u00a0UMAP\n\nI noticed @tunguz's notebook didn't include UMAP, which is something I use a lot at work for quickly visualising datasets so I thought I would quickly expand on his work\n\n\nAnother small thing I've added is to colour the train and test data differently (blue and orange). It's hardly scientific, but my first impression here is the train and test sets look fairly similar and if there is a difference it seems there isn't much in the test that isn't in the train set. Obviously that's assuming the fft doesn't remove weird things :)\n\nThere's nothing stopping you from also doing this to the labels ;)\n\n----------------","c33af396":"In this notebook we'll do dimensionality reduction and visualization of the FFT features that were first used in this competition in [this Giba's notebook](https:\/\/www.kaggle.com\/titericz\/0-309-baseline-logisticregression-using-fft). I've created a stand-alone notebook that extracts those features, and it can be found [here](https:\/\/www.kaggle.com\/tunguz\/giba-s-fft-features-only).\n\nWe will make this visualization notebook with the Rapids library. [Rapids](https:\/\/rapids.ai) is an open-source GPU accelerated Data Sceince and Machine Learning library, developed and mainatained by [Nvidia](https:\/\/www.nvidia.com). It is designed to be compatible with many existing CPU tools, such as Pandas, scikit-learn, numpy, etc. It enables **massive** acceleration of many data-science and machine learning tasks, oftentimes by a factor fo 100X, or even more. \n\nRapids is still undergoing developemnt, and only recently has it become possible to use RAPIDS natively in the Kaggle Docker environment. If you are interested in installing and riunning Rapids locally on your own machine, then you should [refer to the followong instructions](https:\/\/rapids.ai\/start.html).","d43faa4d":"## With n_neighbors=10","d2875dbd":"## with n_neighbors=7"}}