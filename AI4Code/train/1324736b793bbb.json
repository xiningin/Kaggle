{"cell_type":{"66f2f468":"code","2ee0fc7a":"code","f7ff915d":"code","59cf5a90":"code","a3af3321":"code","94585bc7":"code","fb811ee2":"code","e0417469":"code","29a73a16":"code","cd86433b":"code","e399e9b3":"code","b82038d4":"code","591cd553":"code","26cc7a30":"code","ee78d685":"code","0f5a4dd9":"code","3e765f78":"code","16b380f2":"code","315d8852":"code","217027eb":"code","7274d81a":"code","f8d2fbbe":"markdown","1be5b0a4":"markdown","161e35b9":"markdown","736c0f94":"markdown","454c0c95":"markdown","ca788809":"markdown","525f6f55":"markdown","ef33ea12":"markdown","978fead7":"markdown","908ae851":"markdown","dc601e13":"markdown","d7cac4a0":"markdown","b137e8f1":"markdown","d0098432":"markdown","ff6388f9":"markdown","55c3aa03":"markdown","83895978":"markdown","23faba67":"markdown","47fecb5e":"markdown","60b52cad":"markdown","c518e7af":"markdown","5293997d":"markdown","e7697a28":"markdown","2f5cec7d":"markdown","77669b2b":"markdown","e6837dea":"markdown","7d7b18dd":"markdown","ac16f488":"markdown","5717160a":"markdown","78b5a75a":"markdown","af030747":"markdown"},"source":{"66f2f468":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2ee0fc7a":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport seaborn as sns # checking data balancing(heat map)\nimport plotly.express as px # Data Visualization\nimport matplotlib.pyplot as plt # heat map figure size\n\nfrom sklearn.model_selection import train_test_split # for splitting\nfrom sklearn.ensemble import RandomForestClassifier # for model\nfrom sklearn.metrics import f1_score # evaluation metric\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # for visualising confusion matrix","f7ff915d":"train_data = pd.read_csv('\/kaggle\/input\/squid-beans\/train.csv')\ntrain_data.head(10)","59cf5a90":"print('The shape of our training set: %s Beans and %s input features'%(train_data.shape[0],train_data.shape[1] - 1))\nprint(\"\\nColumns in our dataset: \" , train_data.columns)","a3af3321":"print(\"List of Numerical features: \\n\" , train_data.select_dtypes(include=np.number).columns.tolist())\nprint(\"\\n\\nList of Categorical features: \\n\" , train_data.select_dtypes(include=['object']).columns.tolist())","94585bc7":"train_data.isnull().sum()","fb811ee2":"px.histogram(train_data, x=\"Class\", color=\"Class\", width=600, height=400, title=\"Visualizaton of Data Distribution (Hover for the details)\") ","e0417469":"# Create a covariance matrix\ncorr = train_data.corr()\n\n# Creating a mask the size of our covariance matrix\nmask = np.zeros_like(corr, dtype=bool)\n\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11,9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220,10,as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr,mask=mask,cmap=cmap,vmax=1,center=0,square=True, \n            linewidth=.5, annot=True)\n\nax.set_title('Multi-Collinearity of Features')\n","29a73a16":"fig = px.scatter(train_data, x=\"MajorAxisLength\", y=\"MinorAxisLength\", color=\"Class\", symbol = \"Class\", hover_data=['Bean ID'], width=700, height = 600, \n                 title = \"Bean Class vs AxisLength (Hover for the details) <br \/>Correlation = 0.78\")\nfig.update_traces(marker_size=10, marker_line_width = 1,marker_line_color = \"black\")\nfig.show()","cd86433b":"fig = px.scatter(train_data,x=\"ShapeFactor1\", y=\"MinorAxisLength\", color=\"Class\", symbol = \"Class\", hover_data=['Bean ID'], width=700, height = 600, \n                title = \"Bean Class vs ShapeFactor1 & MinorAxisLength <br \/>Correlation = -0.96\")\nfig.update_traces(marker_size=10, marker_line_width = 1,marker_line_color = \"black\")\nfig.show()","e399e9b3":"fig = px.scatter(train_data, x=\"ShapeFactor2\", y=\"ShapeFactor3\", color=\"Class\", symbol = \"Class\", hover_data=['Bean ID'], width=700, height = 600, \n                title = \"Bean Class vs ShapeFactor2 and ShapeFactor3 <br \/>Correlation = 0.83\")\nfig.update_traces(marker_size=10, marker_line_width = 1,marker_line_color = \"black\")\nfig.show()","b82038d4":"feed = train_data[['MajorAxisLength', 'MinorAxisLength', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', 'ShapeFactor4', 'Class']]\n\n# Taking all independent variable columns\ndf_train_x = feed.drop('Class',axis = 1)\n\n# Target variable column\ndf_train_y = feed['Class']\n\nx_train, x_test, y_train, y_test = train_test_split(df_train_x, df_train_y, test_size=0.25, random_state=42)","591cd553":"model = RandomForestClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nprint(\"F1 score of Random Forest Classifier = \" , f1_score(y_test, y_pred, average='macro'))","26cc7a30":"# Plotting the confusion matrix\n\ncm = confusion_matrix(y_test, y_pred, labels = model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = model.classes_)\ndisp.plot()","ee78d685":"test_data = pd.read_csv('\/kaggle\/input\/squid-beans\/test.csv')\ntest_data.head(10)","0f5a4dd9":"test_data.describe()","3e765f78":"# Checking missing values\ntest_data.isnull().sum()","16b380f2":"df_test = test_data.drop('Bean ID', axis=1)\ndf_test","315d8852":"test_data_predictions = model.predict(df_test)\ntest_data['Class'] = test_data_predictions\ntest_data.info()","217027eb":"data = test_data[['Bean ID', 'Class']]\ndata.to_csv('submission.csv',index=False)","7274d81a":"from IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = data\n\n# create a link to download the dataframe\ncreate_download_link(df)","f8d2fbbe":"\ud83d\udccc **Observations:**\n1. Bean ID is not corelated to anyone, it's just an identification number so we will remove it.\n2. MajorAxisLength and MinroAxisLength are positively correlated (0.78).\n3. MinorAxisLength and ShapeFactor1 are negatively correlated (-0.96)\n4. ShapeFactor2 and ShapeFactor3 are positively correlated (0.83).\n5. ShapeFactor3 is not much related to Class(target variable).","1be5b0a4":"### **Link to Download CSV file** ","161e35b9":"## **4.2 Visualising Multi-Collinearity** \ud83d\uddfa\ufe0f\n* To see the correlation of each variable with every other variable.","736c0f94":"**Conclusions from Visualization**\n1. No Outliers are found in the dataset.\n2. MinorAxisLength and ShapeFactor1 are highly correlated (-0.96). If our dataset had been given with high no. of features then we would have been dropped any of the feature from these two. But since we have only 6 features (after removing Bean ID) we don't need to remove any feature to make our algo work better.   ","454c0c95":"# **6. Loading Testing Dataset** \ud83d\udcca","ca788809":"## **4.4 ShapeFactor1 and MinorAxisLength** ","525f6f55":"Since MajorAxisLength and MinorAxisLength are positively correlated (0.78) that's why we got the positively increasing plot. ","ef33ea12":"ShapeFactor1 and MinorAxisLength are highly negatively correlated.","978fead7":"# **4. Exploratory Data Analysis** \ud83d\udcc9","908ae851":"Since it's a classification problem we have to check the distribution of data points over each class.","dc601e13":"\ud83d\udccc **Observations:**\n1. We have 6 classes 0, 1, 2, 3, 4, 5.\n2. We have 176 beans (out of 200) of class 4 which shows that the given data is quite unbalanced.","d7cac4a0":"## **5.1 Random Forest Classifier** \ud83c\udf32\ud83c\udf32\ud83c\udf32\ud83c\udf32","b137e8f1":"# **7. Predicting Over Test Dataset** \ud83d\udcca","d0098432":"# **2. Loading Training Dataset** \ud83d\udcca","ff6388f9":"# **5. Building Machine Learning Model** \ud83e\udd16","55c3aa03":"Random forest consists of a large number of individual decision trees that operate as an ensemble. The fundamental concept behind random forest is a simple but powerful one \u2014 the wisdom of crowds. A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.","83895978":"## **4.5 ShapeFactor2 and ShapeFactor3** ","23faba67":"## **4.3 Major and Minor Axis Lengths** ","47fecb5e":"## **3.1 Checking Numerical & Categorical Features**","60b52cad":"#### **Dataset is loaded successfully!** \ud83d\ude03","c518e7af":"## **3.2 Checking Missing Values**","5293997d":"\ud83d\udccc **No missing value!**","e7697a28":"## **Checking Missing Values in Test Dataset**","2f5cec7d":"\ud83d\udccc **No categorical feature is found. Therefore we don't need any encoding!** <br\/>","77669b2b":"# **3. Data Preprocessing** \ud83e\uddd0","e6837dea":"\ud83d\udccc **We have 200 data points and 7 input features.**","7d7b18dd":"# **1. Importing Necessary Libraries** \ud83d\udcda ","ac16f488":"Since ShapeFactor2 and Shapefactor3 are highly positively correlated (0.83) that's why we got the positively increasing plot. ","5717160a":"## **4.1 Checking Data Distribution Over All Classes**","78b5a75a":"# **Let's save Vivek** \ud83d\udc66 **with Coffee** \u2615","af030747":"## **5.2 Confusion Matrix**"}}