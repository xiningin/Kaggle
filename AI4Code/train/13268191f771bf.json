{"cell_type":{"59273886":"code","c4159bfd":"code","928e72ba":"code","b4f70a34":"code","3d5d0372":"code","1f26aab6":"code","893101b6":"code","d746c4e8":"code","dd7068e4":"code","27bccd2d":"code","fd3da2f6":"code","d5a2166f":"code","3fa54a8b":"code","177b02fa":"code","e38885c1":"code","1383a0a6":"code","b5295fb3":"code","96ef82e4":"code","ba89a4c8":"code","b97d0774":"code","21d23542":"code","871d7dce":"code","ec6c050e":"code","19e1fdd8":"code","c68a2b6d":"code","5280c599":"code","c66ca09f":"code","52eb9211":"code","5a4eaf5c":"code","385dae7d":"code","73237c6f":"markdown","64507a2e":"markdown","81dfedab":"markdown"},"source":{"59273886":"!pip install --quiet attrdict","c4159bfd":"from attrdict import AttrDict\nfrom tqdm import tqdm\nimport os\nimport glob \nimport numpy as np\nfrom PIL import Image\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\nimport math\nimport numpy as np","928e72ba":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F","b4f70a34":"cfg = AttrDict(\n    metadata=AttrDict(\n        images_path=\"\/kaggle\/input\/rsna-miccai-png\",\n        labels_path='\/kaggle\/input\/training-labels\/train_labels.csv',\n        val_mod=3,        # 1\/3 of training images are kept for validation\n        limit_first_k=None, # Load only 10 images of train\/val\/test\n    ),\n    dataset=AttrDict(\n        input_keys=['T1wCE'],\n        img_size=128,     # Resize images to smaller size\n    ),\n    dataloader=AttrDict(\n        batch_size=1,\n        num_workers=8,\n    )\n)","3d5d0372":"def load_metadata(images_path: str, labels_path: str, val_mod=3, limit_first_k=None):\n    result = {'train': [], 'val': [], 'test': []}\n    \n    scan_id_to_label = {}\n    with open(labels_path, 'r') as f:\n        for i, line in enumerate(f):\n            if i > 0:\n                idx, label = line.split(',')\n                scan_id_to_label[idx.strip()] = int(label.strip())\n    \n    for items_key in ['train', 'test']:\n        all_files = list(os.listdir(f\"{images_path}\/{items_key}\"))\n        if limit_first_k:\n            all_files = all_files[:limit_first_k]\n            \n        for scan_id in tqdm(all_files):\n            scan_slices = {}\n\n            for filepath in glob.glob(f\"{images_path}\/{items_key}\/{scan_id}\/*\/*.png\"):\n                kind = filepath.split('\/')[-2]\n                slices = scan_slices.get(kind, [])\n                slice_id = filepath.split('\/')[-1].split('-')[-1].split('.')[0]\n                slices.append((slice_id, filepath))\n                scan_slices[kind] = slices\n            \n            for key in scan_slices:\n                slices = scan_slices[key]\n                slices.sort()\n                scan_slices[key] = [path for _, path in slices]\n            \n            key = items_key\n            if hash(scan_id) % val_mod == 0 and items_key == 'train':\n                key = 'val'\n                \n            result[key].append({\n                'scan': scan_id,\n                'label': scan_id_to_label.get(scan_id),\n                **scan_slices,\n            })\n    return AttrDict(result)","1f26aab6":"metadata = load_metadata(**cfg.metadata)\nprint(f\"{len(metadata.train)} train | {len(metadata.val)} val | {len(metadata.test)} test\")\n# print(metadata.train[1])","893101b6":"class Dataset3d(torch.utils.data.Dataset):\n    def __init__(self, metadata, input_keys, img_size=None):\n        super().__init__()\n        \n        self.metadata = metadata\n        self.input_keys = input_keys\n        self.img_size = img_size\n\n        # self.load()\n\n    def load(self, idx, prop):\n        img_size = self.img_size\n        filenames = self.metadata[idx].get(prop)\n        \n        result = []\n        for filename in filenames:\n            img = np.array(Image.open(filename))\n            img = (img \/ 255 - 0.5) * 2\n            img = cv.resize(img, (img_size, img_size),\n                            interpolation=cv.INTER_NEAREST)\n            result.append(img)\n        return np.array(result)\n\n    def __len__(self):\n        return len(self.metadata)\n    \n    def __getitem__(self, idx):\n        img_size = self.img_size\n        \n        result = {}\n        result['label'] = self.metadata[idx]['label']  \n        result['scan'] = self.metadata[idx]['scan']\n        for key in self.input_keys:\n            result[key] = self.load(idx, key)\n        \n        return result","d746c4e8":"dataset = AttrDict(\n  train=Dataset3d(metadata.train, **cfg.dataset),\n  val=Dataset3d(metadata.val, **cfg.dataset),\n  test=Dataset3d(metadata.test, **cfg.dataset),\n)","dd7068e4":"# item = dataset.train[2]\n# plt.figure(figsize=(5 * len(cfg.dataset.input_keys), 5))\n# print(f\"Scan: {item['scan']} (label: {item['label']})\")\n# for i, key in enumerate(cfg.dataset.input_keys):\n#     print(f\"  {key}: {item[key].shape}\")\n#     plt.subplot(1, len(cfg.dataset.input_keys), i + 1)\n#     plt.imshow(item[key][10])\n# plt.show()","27bccd2d":"dataloader = AttrDict(\n    train=torch.utils.data.DataLoader(dataset.train, shuffle=True, **cfg.dataloader),\n    val=torch.utils.data.DataLoader(dataset.train, shuffle=False, **cfg.dataloader),\n    test=torch.utils.data.DataLoader(dataset.train, shuffle=False, **cfg.dataloader),\n)","fd3da2f6":"# for batch in dataloader.train:\n#     print(batch)\n#     break","d5a2166f":"# print(len(metadata.train[2]['FLAIR']))","3fa54a8b":"dataloader.train","177b02fa":"# plt.figure(figsize=(6 * len(cfg.dataset.input_keys),6))\n# for i in range(4):\n#     plt.subplot(1, len(cfg.dataset.input_keys), i + 1)\n#     plt.imshow(dataset.train[i]['T2w'][20])\n# plt.show()\n","e38885c1":"# patient = 0\n# len(dataset.train[patient]['FLAIR'])","1383a0a6":"def get_images(dataset, patient: int, folder: str, train=True):\n    images = []\n    if train == True:\n        for img in dataset.train[patient][folder]:\n            # Exclude the blank images\n            if np.max(img)!=0:\n                images.append(img)\n            else:\n                pass\n    else:\n        for img in dataset.test[patient][folder]:\n            # Exclude the blank images\n            if np.max(img)!=0:\n                images.append(img)\n            else:\n                pass\n    \n    return images","b5295fb3":"# patient = 1\n\n# images = get_images(dataset, patient, 'T2w')\n# print('Nr of images:', len(images))\n\n# fig = plt.figure(figsize=(50,50))\n\n# c = 1\n# for image in images:\n#     ax = fig.add_subplot(len(images)\/\/10+1, 10, c)\n#     ax.imshow(image, cmap='gray')\n#     c+=1\n    \n#     plt.axis('off')\n    \n# fig.tight_layout()","96ef82e4":"len(metadata.train[0])","ba89a4c8":"label = 'T1wCE'\nmax_images, min_images = len(metadata.train[0][label]), len(metadata.train[0][label])\nfor i in metadata.train[1:]:\n    if len(i[label]) > max_images:\n        max_images = len(i[label])\n    if len(i[label]) < min_images:\n        min_images = len(i[label])\n\nprint(f'Min: {min_images}\\nMax: {max_images}')","b97d0774":"for batch in dataloader.train:\n    print(batch)\n    break","21d23542":"len(dataset.train[0]['T1wCE'])","871d7dce":"from tqdm import tqdm\n\n# TODO: n\/15\n\nlabel = 'T1wCE'\nsmall_dataset = []\nfor d in tqdm(dataloader.train):\n    small_dataset.append(d[label][:15])","ec6c050e":"from tqdm import tqdm\n\n# TODO: n\/15\n\nlabel = 'T1wCE'\nsmall_dataset = []\nfor d in tqdm(dataloader.train):\n#     T2w_dataset.append(d['T1w'][:15])\n    nr_photos = len(d[label])\n    if nr_photos > 15:\n        m = nr_photos \/ 15\n        if round(m) == math.floor(m): # nu facem padding\n            for i in range(math.floor(m)):\n                small_dataset.append(d[label][i::round(m)]) # trebuie completat si pentru Y\n        else: # facem padding\n            # add (15 - nr_photos % 15) of zero(128)\n            np.append(d[label],np.zeros(((15 - nr_photos % 15), 128, 128)), axis=0)\n            m = (nr_photos + (15 - nr_photos % 15))\/15\n            for i in range(m):\n                small_dataset.append(d[label][i::m])\n                      \n    else:\n          small_dataset.append(d[label])\n\n#     if d['T1w'] is None:\n#         continue\n#     else:\n#         T2w_dataset.append(d['T1w'][:15])","19e1fdd8":"small_dataset[0][0][0]","c68a2b6d":"class Convnet (nn.Module):\n    def __init__(self):\n        super(Convnet, self).__init__()\n        # 128 -> 124 -> 62\n        # 15 -> 11 ->\n        self.conv1 = nn.Sequential(\n            nn.Conv3d(\n                in_channels=1,\n                out_channels=16,\n                kernel_size=5,\n                stride=1,\n                padding=0,\n            ),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=2)\n        )\n        # 62 -> 58 -> 29\n        self.conv2 = nn.Sequential(\n            nn.Conv3d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=5,\n                stride=1,\n                padding=0,\n            ),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=2)\n        )\n        self.out = nn.Linear(32 * 29 * 29, 1)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        \n        x = x.view(x.size(0), -1)\n        output = self.out(x)\n        return output, x # return x for visualization","5280c599":"from torch import optim\n\ncnn = Convnet()\n\nloss_func = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cnn.parameters(), lr = 0.001)","c66ca09f":"import numpy as np\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef get_model(width=128, height=128, depth=15):\n    inputs = keras.Input((width, height, depth, 1))\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.GlobalAveragePooling3D()(x)\n    x = layers.Dense(units=512, activation=\"relu\")(x)\n    x = layers.Dropout(0.3)(x)\n\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n\n    model = keras.Model(inputs, outputs)\n    return model","52eb9211":"model = get_model(128,128,15)\n\nmodel.fit(\n    train[folder][pacient],\n    validation_data=validation_dataset,\n    epochs=10,\n)","5a4eaf5c":"def minMaxNormalize(volume):\n    # values between 0 and 1\n    min = 0\n    max = 255\n    volume[volume < min] = min\n    volume[volume > max] = max\n    volume = (volume - min) \/ (max - min)\n    volume = volume.astype(\"float32\")\n    return volume","385dae7d":"for epoch in range(2):\nrunning_loss = 0.0\nfor i, data in enumerate(trainloader, 0):\n  inputs, labels = data\n  optimizer.zero_grad()\n\n  # forward + backward + optimize\n  outputs = net(inputs)\n  loss = criterion(outputs, labels)\n  loss.backward()\n  optimizer.step()\n\n  # print statistics\n  running_loss += loss.item()\n  if i % 2000 == 1999:    # print every 2000 mini-batches\n      print('[%d, %5d] loss: %.3f' %\n            (epoch + 1, i + 1, running_loss \/ 2000))\n      running_loss = 0.0","73237c6f":"### Observatie\n - T1w este T1 weighted pre-contrast\n - T2wCE este T1 weighted post-contrast\n - T2w este T2 weighted\n - FLAIR = Fluid Attenuated Inversion Recovery\n - fiecare folder contine tipuri diferite de RMN (contrastul difera)\n - NU exista o regula de orientare a scanarilor (de ex. T1w contine rmn in plan sagital, dar si in plan coronal sau orizontal)\n - Plan Sagital = stanga-dreapta\n - Plan Coronal = fata-spate\n - Plan Orizontal = sus-jos\n<br\/>\n<br\/>\n\n[Link](https:\/\/case.edu\/med\/neurology\/NR\/MRI%20Basics.htm) explicatii la ce inseamna T1w, T2w, FLAIR.<br\/>","64507a2e":"### T1\nFat is depicted in white and water in black.<br\/>\nThe shape of the brain can be clearly seen, and morphological abnormalities are easy to detect (Atrophy, tumors, etc.)<br\/>\n\n### T2\nWater is painted white.<br\/>\nLesions appear white. Suitable for lesion evaluation.<br\/>\n\n### FLAIR\nIn T2, the spinal fluid (water) is white and the lesion is also white, so you have to look for the white in the white, which is difficult to understand.<br\/>\nFLAIR can be roughly thought of as T2, in which the water is also black, making it easier to find the lesion.<br\/>\n","81dfedab":"## #1 try - One folder only"}}