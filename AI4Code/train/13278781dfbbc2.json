{"cell_type":{"a7b9a49d":"code","7d5deef6":"code","4c946cad":"code","25c23c84":"code","96f46dc7":"code","2f216992":"code","11b2fa44":"code","9dec661b":"markdown","946708ad":"markdown","c8f3f772":"markdown","10ce9dd0":"markdown"},"source":{"a7b9a49d":"import pandas as pd\ndf = pd.read_pickle('..\/input\/preprocessingdata\/df.pkl')","7d5deef6":"df.info()","4c946cad":"X_train = df[df.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = df[df.date_block_num < 33]['item_cnt_month']\nX_valid = df[df.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = df[df.date_block_num == 33]['item_cnt_month']\nX_test = df[df.date_block_num == 34].drop(['item_cnt_month'], axis=1)\ndel df","25c23c84":"import lightgbm as lgb\nimport sklearn\nfeature_name = X_train.columns.tolist()\nfeature_name_indexes = [ \n                            'country_part', \n                            'item_category_common',\n                            'item_category_code', \n                            'city_code',\n    ]\ndef objective(trial):\n\n    lgb_train = lgb.Dataset(X_train[feature_name], Y_train)\n    lgb_eval = lgb.Dataset(X_valid[feature_name], Y_valid, reference=lgb_train)\n\n    params = {\n        'objective': 'rmse',\n        'metric': 'rmse',\n        'num_leaves': trial.suggest_int('num_leaves', 1000, 1500),\n        'min_data_in_leaf':10,\n        'feature_fraction':trial.suggest_uniform('feature_fraction', 0.6, 0.8),\n        'learning_rate': trial.suggest_uniform('feature_fraction', 0.01, 0.015),\n        'num_rounds': 1000,\n        'early_stopping_rounds': 30,\n        'seed': 1\n    }\n\n    evals_result = {}\n    gbm = lgb.train(\n            params, \n            lgb_train,\n            num_boost_round=3000,\n            valid_sets=(lgb_train, lgb_eval), \n            feature_name = feature_name,\n            categorical_feature = feature_name_indexes,\n            verbose_eval=50, \n            evals_result = evals_result,\n            )\n\n    preds = gbm.predict(X_valid)\n    loss = sklearn.metrics.mean_squared_error(Y_valid, preds)\n    return loss\n\n\n","96f46dc7":"import optuna\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","2f216992":"params = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'num_leaves': 1012,\n    'min_data_in_leaf':10,\n    'feature_fraction':0.622351664881,\n    'learning_rate': 0.01,\n    'num_rounds': 1000,\n    'early_stopping_rounds': 30,\n    'seed': 1\n}\nfeature_name_indexes = [ \n                        'country_part', \n                        'item_category_common',\n                        'item_category_code', \n                        'city_code',\n]\n\nlgb_train = lgb.Dataset(X_train[feature_name], Y_train)\nlgb_eval = lgb.Dataset(X_valid[feature_name], Y_valid, reference=lgb_train)\n\nevals_result = {}\ngbm = lgb.train(\n        params, \n        lgb_train,\n        num_boost_round=3000,\n        valid_sets=(lgb_train, lgb_eval), \n        feature_name = feature_name,\n        categorical_feature = feature_name_indexes,\n        verbose_eval=50, \n        evals_result = evals_result,\n        )","11b2fa44":"test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nY_test = gbm.predict(X_test[feature_name]).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('gbm_submission.csv', index=False)","9dec661b":"**using optuna to get the best parameters of model**\nyou can add some parameters that you want to tune inside the params object.","946708ad":"# Lightgbm + Optuna Hyperparameter tunning","c8f3f772":"# loading preprocessing data","10ce9dd0":"# After getting best parameter, train the LGBM model again."}}