{"cell_type":{"d6326762":"code","36b12c16":"code","c5f0bb11":"code","2ee11501":"code","f50fb49c":"code","9755a86c":"code","f886903f":"code","a024f7dd":"code","4f51bd40":"code","0da64114":"code","99e480c5":"code","48d00ab7":"code","fa75e509":"code","aca1c681":"code","1ed34ac9":"code","6c9c6d1e":"code","d5e34bc7":"code","5ed25e1a":"code","19eaa82c":"code","100cfc83":"code","2c6b5506":"code","877f0ab5":"code","a519da64":"code","18980bd6":"code","f9172022":"code","8374d721":"code","47f64791":"code","f8035b5d":"code","9a983815":"code","7afb9d13":"code","7cc325dd":"code","62227514":"code","853b57ab":"code","6719e3c2":"code","147ed875":"code","7279b0e6":"code","c50b81cb":"code","628fb90d":"code","02c251a4":"code","d92bb1b8":"code","ffd3cc56":"code","290f72ab":"code","d396cf1f":"code","c97c774d":"code","c9327321":"code","b055cdf2":"code","730ad5ac":"code","832e1a07":"code","afe21ea8":"code","6478520b":"code","31a6b884":"code","795d9c52":"code","7802a409":"code","4f586d57":"code","8b6f1b5a":"code","a7b016e0":"code","82de5107":"code","2dafb999":"code","d6851932":"code","24b474a9":"code","06fa1bda":"code","b80bac8f":"code","087ad2df":"code","13093f22":"code","29af9ce5":"code","e8c34baa":"code","e39be409":"code","7a0cad30":"code","5b13dfa0":"code","4add506d":"code","5ba4d091":"code","9b8e26fa":"code","900b678c":"code","d2d6be6c":"code","05292721":"code","599bdac3":"code","60c133e5":"code","b8623507":"code","7093683c":"code","7bae56aa":"code","74f445bf":"code","c9f64caa":"code","d84de536":"code","6602b4c1":"code","2fdc3843":"code","8958eb17":"markdown","1bfcc15e":"markdown","95c0bb60":"markdown","6c945429":"markdown","f1806591":"markdown","bcde8799":"markdown","b178b8eb":"markdown","7a238883":"markdown","12334334":"markdown","5126da15":"markdown","f19d3af8":"markdown","11dd84b3":"markdown","cefacb76":"markdown","eb617ec7":"markdown","cded037d":"markdown","16f9031f":"markdown","21629fd9":"markdown","d3fc7e0f":"markdown","654f17c1":"markdown","93f73550":"markdown","14a8ed0c":"markdown","5ca4d979":"markdown","d3678916":"markdown","8168970e":"markdown","f27fbe36":"markdown","26308c04":"markdown","728d5602":"markdown","ba394daf":"markdown","27365459":"markdown","988f608f":"markdown","ec487fcc":"markdown","a84e30f4":"markdown","e3f8a33d":"markdown","ffb0c33e":"markdown","ce79b42c":"markdown","5e7c2e88":"markdown","e805757e":"markdown","4209f4d5":"markdown","db8ab330":"markdown","a84b4555":"markdown","f227a370":"markdown","bb317c82":"markdown","08ed6e88":"markdown","b874c728":"markdown","74a17ff2":"markdown","68ee0706":"markdown","3e2711af":"markdown","d583d44d":"markdown","55f9070d":"markdown","f771f47c":"markdown","d170e9d5":"markdown","3d4d29d1":"markdown","c973841f":"markdown","75b938d0":"markdown","9c4310f1":"markdown","e06fb550":"markdown","aea8a8c1":"markdown","27ad24a0":"markdown","1c52daf6":"markdown","9da38223":"markdown","2b9db8cd":"markdown","9b097179":"markdown","87e61e0e":"markdown","b8e9c6c5":"markdown","119c6f82":"markdown","bc41c8f9":"markdown","bbfa550f":"markdown","7eb55e2f":"markdown","5f80a041":"markdown","b5d65022":"markdown","4af145c1":"markdown"},"source":{"d6326762":"# Import Packages\n\n#Dataframe packages\nimport json\nimport glob\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport cv2\nimport numpy as np\nfrom collections import Counter\nfrom functools import partial\nimport scipy as sp\n\n#Plot packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom wordcloud import WordCloud\n\n# LightGBM\nimport lightgbm as lgb\nimport scipy as sp\n\n# Load scikit's classifier library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold,RandomizedSearchCV\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.metrics import cohen_kappa_score,mean_squared_error, accuracy_score, confusion_matrix, f1_score,classification_report\n\nimport xgboost as xgb\n\n\n#Oversampling\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.decomposition import PCA","36b12c16":"sentimental_analysis = sorted(glob.glob('..\/input\/train_sentiment\/*.json'))\nprint('num of train sentiment files: {}'.format(len(sentimental_analysis)))","c5f0bb11":"# Define Empty lists\nscore=[]\nmagnitude=[]\npetid=[]\n\nfor filename in sentimental_analysis:\n         with open(filename, 'r') as f:\n            sentiment_file = json.load(f)\n         file_sentiment = sentiment_file['documentSentiment']\n         file_score =  np.asarray(sentiment_file['documentSentiment']['score'])\n         file_magnitude =np.asarray(sentiment_file['documentSentiment']['magnitude'])\n        \n            \n         score.append(file_score)\n         magnitude.append(file_magnitude)\n        \n         petid.append(filename.replace('.json','').replace('..\/input\/train_sentiment\/', ''))\n\n# Output with sentiment data for each pet\n# Output with sentiment data for each pet\nsentimental_analysis = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,pd.DataFrame(score, columns =['sentiment_document_score']),\n                                                pd.DataFrame(magnitude, columns =['sentiment_document_magnitude'])],axis =1)","2ee11501":"image_metadata =  sorted(glob.glob('..\/input\/train_metadata\/*.json'))\nprint('num of train metadata: {}'.format(len(image_metadata)))","f50fb49c":"description=[]\ntopicality=[]\nimageid=[]\n# Read Zip File and Export a Dataset with the Score and the ID\nfor filename in image_metadata:\n         with open(filename, 'r') as f:\n            d = json.load(f)\n            file_keys = list(d.keys())\n         if  'labelAnnotations' in file_keys:\n            file_annots = d['labelAnnotations']\n            file_topicality = np.asarray([x['topicality'] for x in file_annots])\n            file_description = [x['description'] for x in file_annots]\n            #Create a list of all descriptions and topicality\n            description.append(file_description)\n            topicality.append(file_topicality)\n            #Create a list with all image id name\n            imageid.append(filename.replace('.json','').replace('..\/input\/train_metadata\/',''))\n\n\n# Prepare the output by renaming all variables\ndescription=pd.DataFrame(description)\ntopicality=pd.DataFrame(topicality)\n\nnew_names = [(i,'metadata_description_'+str(i)) for i in description.iloc[:, 0:].columns.values]\ndescription.rename(columns = dict(new_names), inplace=True)\n\nnew_names = [(i,'metadata_topicality_'+str(i)) for i in topicality.iloc[:, 0:].columns.values]\ntopicality.rename(columns = dict(new_names), inplace=True)\n\n# Output with sentiment data for each pet\nimage_labelannot = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,topicality,description],axis =1)\n\n# create the PetId variable\nimage_labelannot['PetID'] = image_labelannot['ImageId'].str.split('-').str[0]\n\n\n","9755a86c":"##############\n# TOPICALITY #\n##############\n\nimage_labelannot['metadata_topicality_mean'] = image_labelannot.iloc[:,1:10].mean(axis=1)\nimage_labelannot['metadata_topicality_mean']  = image_labelannot.groupby(['PetID'])['metadata_topicality_mean'].transform('mean') \n\nimage_labelannot['metadata_topicality_max'] = image_labelannot.iloc[:,1:10].max(axis=1)\nimage_labelannot['metadata_topicality_max'] = image_labelannot.groupby(['PetID'])['metadata_topicality_max'].transform(max)\n\nimage_labelannot['metadata_topicality_min'] = image_labelannot.iloc[:,1:10].min(axis=1)\nimage_labelannot['metadata_topicality_min'] = image_labelannot.groupby(['PetID'])['metadata_topicality_min'].transform(min)\n\n\nimage_labelannot['metadata_topicality_0_mean']  = image_labelannot.groupby(['PetID'])['metadata_topicality_0'].transform('mean')\nimage_labelannot['metadata_topicality_0_max'] = image_labelannot.groupby(['PetID'])['metadata_topicality_0'].transform(max)\nimage_labelannot['metadata_topicality_0_min'] = image_labelannot.groupby(['PetID'])['metadata_topicality_0'].transform(min)\n\n\n###############\n# DESCRIPTION #\n###############\n\n# Create Features from the Images\nimage_labelannot['L_metadata_0_cat']=image_labelannot['metadata_description_0'].str.contains(\"cat\").astype(int)\nimage_labelannot['L_metadata_0_dog'] =image_labelannot['metadata_description_0'].str.contains(\"dog\").astype(int)\n\nimage_labelannot['L_metadata_any_cat']=image_labelannot.apply(lambda row: row.astype(str).str.contains('cat').any(), axis=1)\nimage_labelannot['L_metadata_any_dog']=image_labelannot.apply(lambda row: row.astype(str).str.contains('dog').any(), axis=1)\n\nimage_labelannot['L_metadata_0_cat_sum'] = image_labelannot.groupby(image_labelannot['PetID'])['L_metadata_0_cat'].transform('sum')\nimage_labelannot['L_metadata_0_dog_sum'] = image_labelannot.groupby(image_labelannot['PetID'])['L_metadata_0_dog'].transform('sum')\n\nimage_labelannot['L_metadata_any_cat_sum'] = image_labelannot.groupby(image_labelannot['PetID'])['L_metadata_any_cat'].transform('sum')\nimage_labelannot['L_metadata_any_dog_sum'] = image_labelannot.groupby(image_labelannot['PetID'])['L_metadata_any_dog'].transform('sum')\n\nimage_labelannot = image_labelannot[['PetID','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min','metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum','L_metadata_any_cat_sum','L_metadata_any_dog_sum']]\nimage_labelannot=image_labelannot.drop_duplicates('PetID')","f886903f":"color_score_mean=[]\ncolor_score_min=[]\ncolor_score_max=[]\n\ncolor_pixelfrac_mean=[]\ncolor_pixelfrac_min=[]\ncolor_pixelfrac_max=[]\n\nimageid=[]\n\n# Read Zip File and Export a Dataset with the Score and the ID\nfor filename in image_metadata:\n         with open(filename, 'r') as f:\n              d = json.load(f)\n              file_keys = list(d.keys())\n              if  'imagePropertiesAnnotation' in file_keys:\n                  file_colors = d['imagePropertiesAnnotation']['dominantColors']['colors']\n               \n                  file_color_score_mean = np.asarray([x['score'] for x in file_colors]).mean()\n                  file_color_pixelfrac_mean = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n                  file_color_score_min = np.asarray([x['score'] for x in file_colors]).min()\n                  file_color_pixelfrac_min = np.asarray([x['pixelFraction'] for x in file_colors]).min()\n\n\n                  file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n                  file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n\n\n              #Create a list with all image id name\n              imageid.append(filename.replace('.json','').replace('..\/input\/train_metadata\/', ''))\n\n              color_score_mean.append(file_color_score_mean)\n              color_score_min.append(file_color_score_min)\n              color_score_max.append(file_color_score_max)\n\n\n              color_pixelfrac_mean.append(file_color_pixelfrac_mean)\n              color_pixelfrac_min.append(file_color_pixelfrac_min)\n              color_pixelfrac_max.append(file_color_pixelfrac_max)\n\n      \nimage_properties = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'metadata_color_pixelfrac_mean':color_pixelfrac_mean}), pd.DataFrame({'metadata_color_pixelfrac_min':color_pixelfrac_min}),pd.DataFrame({'metadata_color_pixelfrac_max':color_pixelfrac_max}),pd.DataFrame({'metadata_color_score_mean':color_score_mean}),pd.DataFrame({'metadata_color_score_min':color_score_min}),pd.DataFrame({'metadata_color_score_max':color_score_max})],axis=1)\n\n\n# create the PetId variable\nimage_properties['PetID'] = image_properties['ImageId'].str.split('-').str[0]\n\n\n##############\n# COLOR INFO #\n##############\nimage_properties['metadata_color_pixelfrac_mean']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_mean'].transform('mean') \nimage_properties['metadata_color_pixelfrac_min']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_min'].transform(min) \nimage_properties['metadata_color_pixelfrac_max']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_max'].transform(max) \n\nimage_properties['metadata_color_score_mean']  = image_properties.groupby(['PetID'])['metadata_color_score_mean'].transform('mean') \nimage_properties['metadata_color_score_min']  = image_properties.groupby(['PetID'])['metadata_color_score_min'].transform(min) \nimage_properties['metadata_color_score_max']  = image_properties.groupby(['PetID'])['metadata_color_score_max'].transform(max)\n\nimage_properties=image_properties.drop_duplicates('PetID')\nimage_properties = image_properties.drop(['ImageId'], 1)","a024f7dd":"image_quality =sorted(glob.glob('..\/input\/train_images\/*.jpg'))\n\nblur=[]\nimage_pixel=[]\nimageid =[]\n\nfor filename in image_quality:\n              #Blur \n              image = cv2.imread(filename)\n              gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n              result = cv2.Laplacian(gray, cv2.CV_64F).var() \n              # Pixels\n              with Image.open(filename) as pixel:\n                  width, height = pixel.size\n              \n              pixel = width*height\n              \n              #image pixel size for each image\n              \n              image_pixel.append(pixel)\n              #blur for each image\n              blur.append(result)\n              #image id\n              imageid.append(filename.replace('.jpg','').replace('..\/input\/train_images\/', ''))\n                \n# Join Pixel, Blur and Image ID\nimage_quality = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,pd.DataFrame(blur, columns =['blur']),\n                                        pd.DataFrame(image_pixel,columns=['pixel'])],axis =1)\n\n# create the PetId variable\nimage_quality['PetID'] = image_quality['ImageId'].str.split('-').str[0]\n\n#Mean of the Mean\nimage_quality['pixel_mean'] = image_quality.groupby(['PetID'])['pixel'].transform('mean')\nimage_quality['blur_mean'] = image_quality.groupby(['PetID'])['blur'].transform('mean') \n\nimage_quality['pixel_min'] = image_quality.groupby(['PetID'])['pixel'].transform('min') \nimage_quality['blur_min'] = image_quality.groupby(['PetID'])['blur'].transform('min')\n\nimage_quality['pixel_max'] = image_quality.groupby(['PetID'])['pixel'].transform('max') \nimage_quality['blur_max'] = image_quality.groupby(['PetID'])['blur'].transform('max')\n\nimage_quality['pixel_sum'] = image_quality.groupby(['PetID'])['pixel'].transform('sum')\nimage_quality['blur_sum'] = image_quality.groupby(['PetID'])['blur'].transform('sum')\n\n\nimage_quality = image_quality.drop(['blur','pixel','ImageId'], 1)\nimage_quality=image_quality.drop_duplicates('PetID')","4f51bd40":"from math import copysign, log10\n\nhuMoments0=[]\nhuMoments1=[]\nhuMoments2=[]\nhuMoments3=[]\nhuMoments4=[]\nhuMoments5=[]\nhuMoments6=[]\nimageid =[]\n\nimage_info_train =sorted(glob.glob('..\/input\/train_images\/*.jpg'))\n\nfor filename in image_info_train:\n            if filename.endswith(\"-1.jpg\"): # Take only the moments of picture 1\n                image = cv2.imread(filename)\n                im = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n                # Calculate Moments\n                moments = cv2.moments(im)\n\n                # Calculate Hu Moments\n                huMoments = cv2.HuMoments(moments)\n                # Log scale hu moments\n                for i in range(0,7):\n                      huMoments[i] = round(-1* copysign(1.0, huMoments[i]) * log10(abs(huMoments[i])),2)\n\n                #image id\n                imageid.append(filename.replace('.jpg','').replace('..\/input\/train_images\/', ''))\n                huMoments0.append(huMoments[0])\n\n                huMoments1.append(huMoments[1])\n                huMoments2.append(huMoments[2])\n                huMoments3.append(huMoments[3])\n                huMoments4.append(huMoments[4])\n                huMoments5.append(huMoments[5])\n                huMoments6.append(huMoments[6])\n\nimage_moments= pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'huMoments0':np.concatenate(huMoments0,axis=0)}), \n                                     pd.DataFrame({'huMoments1':np.concatenate(huMoments1,axis=0)}),\n                                     pd.DataFrame({'huMoments2':np.concatenate(huMoments2,axis=0)}),\n                                     pd.DataFrame({'huMoments3':np.concatenate(huMoments3,axis=0)}),\n                                     pd.DataFrame({'huMoments4':np.concatenate(huMoments4,axis=0)}),\n                                     pd.DataFrame({'huMoments5':np.concatenate(huMoments5,axis=0)}),pd.DataFrame({'huMoments6':np.concatenate(huMoments6,axis=0)})],axis=1)\n            \n\n# create the PetId variable\nimage_moments['PetID'] = image_moments['ImageId'].str.split('-').str[0]\nimage_moments = image_moments[image_moments['ImageId'].apply(lambda x:x.endswith((\"-1\")))]\n","0da64114":"#Load Data\ntrain = pd.read_csv('..\/input\/train\/train.csv')\ntest = pd.read_csv('..\/input\/test\/test.csv')\nsample_submission = pd.read_csv('..\/input\/test\/sample_submission.csv')\n\nbreed =pd.read_csv('..\/input\/breed_labels.csv',usecols=[\"BreedID\", \"BreedName\"]) #A pet could have multiple breed\ncolor =pd.read_csv('..\/input\/color_labels.csv') #A pet could have multiple colors\nstate =pd.read_csv('..\/input\/state_labels.csv')\n\n# Add information about color, breed, state and sentiment data\ntrain = (pd.merge(train, breed.rename(columns={\"BreedName\": \"BreedName1\"}),  how='left', left_on=['Breed1'], right_on = ['BreedID']).drop('BreedID', axis=1))\ntrain = (pd.merge(train, breed.rename(columns={\"BreedName\": \"BreedName2\"}),  how='left', left_on=['Breed2'], right_on = ['BreedID']).drop('BreedID', axis=1))\n\ntrain = (pd.merge(train, color.rename(columns={\"ColorName\": \"ColorName1\"}),  how='left', left_on=['Color1'], right_on = ['ColorID']).drop('ColorID', axis=1))\ntrain = (pd.merge(train, color.rename(columns={\"ColorName\": \"ColorName2\"}),  how='left', left_on=['Color2'], right_on = ['ColorID']).drop('ColorID', axis=1))\ntrain = (pd.merge(train, color.rename(columns={\"ColorName\": \"ColorName3\"}),  how='left', left_on=['Color3'], right_on = ['ColorID']).drop('ColorID', axis=1))\n\ntrain = (pd.merge(train, state,  how='inner', left_on=['State'], right_on = ['StateID']).drop('StateID', axis=1))\n\n# Add information about sentimental analysis\ntrain = (pd.merge(train, sentimental_analysis,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add information about Metadata Images\ntrain = (pd.merge(train, image_properties,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntrain = (pd.merge(train, image_labelannot,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntrain = (pd.merge(train, image_moments,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add information about quality Images\ntrain = (pd.merge(train, image_quality,  how='left', left_on=['PetID'], right_on = ['PetID']))\n","99e480c5":"## Using the Kernel:https:\/\/www.kaggle.com\/bibek777\/stacking-kernels\n## Using the Kernel:https:\/\/www.kaggle.com\/bibek777\/stacking-kernels\n\n# state GDP: https:\/\/en.wikipedia.org\/wiki\/List_of_Malaysian_states_by_GDP\nstate_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n\n# state population: https:\/\/en.wikipedia.org\/wiki\/Malaysia\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\n\nstate_area ={\n    41336:19102,\n41325:9500,\n41367:15099,\n41401:243,\n41415:91,\n41324:1664,\n41332:6686,\n41335:36137,\n41330:21035,\n41380:821,\n41327:1048,\n41345:73631,\n41342:124450,\n41326:8104,\n41361:13035}\n\nstate_unemployment ={\n    41336 : 3.6,\n41325 :2.9,\n41367: 3.8,\n41324: 0.9,\n41332 : 2.7,\n41335: 2.6,\n41330: 3.4,\n41380: 2.9,\n41327: 2.1,\n41345 : 5.4,\n41342 : 3.3,\n41326: 3.2,\n41361: 4.2,\n41415: 7.8,\n41401: 3.3\n}\n\n# per 1000 population\nstate_birth_rate = {\n 41336:16.3,\n41325:17.0,\n41367:21.4,\n41401:14.4,\n41415:18.1,\n41324:16.0,\n41332:16.4,\n41335:17.0,\n41330:14.4,\n41380:17.5,\n41327:12.7,\n41345:13.7,\n41342:13.9,\n41326:16.6,\n41361:23.3,     \n}\n\ntrain[\"state_gdp\"] = train.State.map(state_gdp)\ntrain[\"state_population\"] = train.State.map(state_population)\ntrain[\"state_area\"] = train.State.map(state_area)\ntrain['state_unemployment']=train.State.map(state_unemployment)\ntrain['state_birth_rate']=train.State.map(state_birth_rate)","48d00ab7":"# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\ntrain['L_Color1'] = (pd.isnull(train['ColorName3']) & pd.isnull(train['ColorName2']) & pd.notnull(train['ColorName1'])).astype(int)\ntrain['L_Color2'] = (pd.isnull(train['ColorName3']) & pd.notnull(train['ColorName2']) & pd.notnull(train['ColorName1'])).astype(int)\ntrain['L_Color3'] = (pd.notnull(train['ColorName3']) & pd.notnull(train['ColorName2']) & pd.notnull(train['ColorName1'])).astype(int)\n\n# Breed (create a flag if the pet has 1 breed or 2)\ntrain['L_Breed1'] = (pd.isnull(train['BreedName2']) & pd.notnull(train['BreedName1'])).astype(int)\ntrain['L_Breed2'] = (pd.notnull(train['BreedName2']) & pd.notnull(train['BreedName1'])).astype(int)\n\n#Name (create a flag if the name is missing, with less than two letters)\ntrain['Name_Length']=train['Name'].str.len()\ntrain['L_Name_missing'] =  (pd.isnull(train['Name'])).astype(int)\n\n# Breed create columns\ntrain['L_Breed1_Siamese'] =(train['BreedName1']=='Siamese').astype(int)\ntrain['L_Breed1_Persian']=(train['BreedName1']=='Persian').astype(int)\ntrain['L_Breed1_Labrador_Retriever']=(train['BreedName1']=='Labrador Retriever').astype(int)\ntrain['L_Breed1_Terrier']=(train['BreedName1']=='Terrier').astype(int)\ntrain['L_Breed1_Golden_Retriever ']=(train['BreedName1']=='Golden Retriever').astype(int)\n\n#Description \ntrain['Description_Length']=train['Description'].str.len() \n\n# Fee Amount\ntrain['L_Fee_Free'] =  (train['Fee']==0).astype(int)\n\n#Add the Number of Pets per Rescuer \npets_total = train.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\ntrain= pd.merge(train, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\ntrain.count()\n\n# No photo\ntrain['L_NoPhoto'] =  (train['PhotoAmt']==0).astype(int)\n\n#No Video\ntrain['L_NoVideo'] =  (train['VideoAmt']==0).astype(int)\n\n#Log Age \ntrain['Log_Age']= np.log(train.Age+1) \n\n#Negative Score \ntrain['L_scoreneg'] =  (train['sentiment_document_score']<0).astype(int)\n\n#Quantity Amount >5\ntrain.loc[train['Quantity'] > 5, 'Quantity'] = 5","fa75e509":"# Normalize the Variable Description\ntrain['Description'] =train['Description'].fillna(\"<MISSING>\")\ntrain['Description'] = train['Description'].str.replace('\\d+', '')\ntrain['Description'] = train['Description'].str.lower()\ntrain[\"Description\"] = train['Description'].str.replace('[^\\w\\s]','')\n\n# Stop Words \nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\npat = r'\\b(?:{})\\b'.format('|'.join(stop))\ntrain['Description'] = train['Description'].str.replace(pat, '')\ntrain['Description'] = train['Description'].str.replace(r'\\s+', ' ')\n\n# Stem Words\ntrain['Description'] = train['Description'].astype(str).str.split()\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nporter_stemmer = PorterStemmer()\ntrain['Description']=train['Description'].apply(lambda x : [porter_stemmer.stem(y) for y in x])\n\ntrain['Description']=train['Description'].apply(lambda x : \" \".join(x))\n\ndef get_top_n_words(corpus, n=None):\n    from sklearn.feature_extraction.text import CountVectorizer\n\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    test=pd.DataFrame(words_freq[:n], columns=['words','freq']) \n    \n    sns.barplot(x='words', y='freq', data=test)\n\nget_top_n_words(train['Description'],10)\n\nfrom sklearn.decomposition import TruncatedSVD, NMF\n# Matrix Factorization for dimensionality reduction\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nsvd_ = TruncatedSVD(\n    n_components=5, random_state=1337)\nnmf_ = NMF(\n    n_components=5, random_state=1337)\n\ntfidf_col = TfidfVectorizer().fit_transform(train['Description'])\nsvd_col = svd_.fit_transform(tfidf_col)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('SVD_')\n\nnmf_col = nmf_.fit_transform(tfidf_col)\nnmf_col = pd.DataFrame(nmf_col)\nnmf_col = nmf_col.add_prefix('NMF_')\n\n# Concatenate all dataframes\ntrain = pd.concat([train,nmf_col,svd_col],axis=1)\n","aca1c681":"train.head(2)","1ed34ac9":"total = float(len(train)) # one person per row \nax =sns.countplot(x=\"AdoptionSpeed\", data=train,palette=\"Set3\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height,\n            '{:1.2f}'.format(height\/total),\n            ha=\"center\") \n","6c9c6d1e":"images_train = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_images\/*.jpg'))\n\nfor img in images_train[:30]:\n    image= Image.open(img)\n    pet_id = img.replace(\"..\/input\/petfinder-adoption-prediction\/train_images\/\",\"\").replace(\".jpg\",\"\")\n    pet_id= pet_id.split(\"-\")[0]\n    \n    print(pet_id)\n    plt.imshow(image)\n    plt.title((\"Name: {}\\nAdoptionSpeed: {}\".format(*list(map(str, train[train.PetID==pet_id][[\"Name\", \"AdoptionSpeed\"]].values.tolist()[0])))))\n    plt.show()","d5e34bc7":"def graphics (train, target, features, ncat):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,7))\n\n    sns.countplot(x=target,hue=i, data=train,palette=('#00a1ff','#f9bc86','#f85cc2')).set_title('Distribution of Adoption Speed per '+ i)\n \n    # CrossTab\n    cross = pd.crosstab(train[target],train[i],margins=True)\n    # Delete the All obs. per variable y\n    cross = cross.drop(cross.index[len(cross)-1])\n    \n    for j in ncat:\n        # Stacked Bar Plot\n        if j ==0:\n            Type1 = pd.DataFrame(cross.iloc[:,j]\/cross['All'])\n            ax1.bar(Type1.index.values, Type1[0], color='#00a1ff', label=cross.columns[0])\n        if j==1:\n            Type2 = pd.DataFrame(cross.iloc[:,j]\/cross['All'])\n            ax1.bar(Type2.index.values, Type2[0], bottom=Type1[0], color='#f9bc86', label=cross.columns[1])\n        if j ==2:\n            Type3 = pd.DataFrame(cross.iloc[:,j]\/cross['All'])\n            ax1.bar(Type3.index.values, Type3[0], bottom=[z+t for z,t in zip(Type1[0],Type2[0])], color='#f85cc2', label=cross.columns[2])\n        if j ==3:\n            Type4 = pd.DataFrame(cross.iloc[:,j]\/cross['All'])\n            ax1.bar(Type4.index.values, Type4[0], bottom=[n+z+t for n,z,t in zip(Type1[0],Type2[0],Type3[0])], color='#ef53c7', label=cross.columns[4])\n \n    # Add title and axis names\n    ax1.set_title('Adoption Speed vs '+i)\n    ax1.set(xlabel='Adoption Speed', ylabel=i)\n    ax1.legend(loc='upper right')","5ed25e1a":"def graphics_num (train, target, features):\n\n    num_analysis =train.groupby([features, target]).size().reset_index(name='counts')\n\n    fig, ax = plt.subplots(1, 2, figsize=(17,7))\n    # Add title and axis names\n    sns.lineplot(x=features, y=\"counts\", hue = target,data=num_analysis, ax=ax[0]).set_title(target+\" vs \"+features)\n    sns.boxplot(x=target, y=features, data=train, palette=\"Set1\", ax=ax[1]).set_title(target+\" vs \"+features)","19eaa82c":"x= 'AdoptionSpeed'\nfeatures =[\"Type\"]\n\nfor i in features:\n    graphics(train,'AdoptionSpeed','Type',ncat=range(0,2))    #1. Dog 2.Cat","100cfc83":"features=[\"Gender\"]\nfor i in features:\n    graphics(train,\"AdoptionSpeed\",i,ncat=range(0,3))    #1. Male, 2.Female 3. Mixed","2c6b5506":"# Draw a nested analysis per Target\nfeatures =['FurLength','Vaccinated','Dewormed','Sterilized','Health']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics(train,\"AdoptionSpeed\",i,ncat=range(0,3))   ","877f0ab5":"# FurLength (0 = Not Specified) - Health (0 = Not Specified) No missing values\n\ntrain.loc[train['FurLength'] == 0]\ntrain.loc[train['Health'] == 0 ]","a519da64":"#Count how many pets per Age (months) and Adoption Speed group\ngrouped_data = train.groupby(['AdoptionSpeed'])\ngrouped_data['Age'].describe()","18980bd6":"age_analysis =train.groupby(['Age', 'AdoptionSpeed']).size().reset_index(name='counts')\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24,10))\n\nsns.lineplot(x=\"Age\", y=\"counts\", hue = \"AdoptionSpeed\",data=age_analysis).set_title(\"Adoption Speed vs Age\")\n# CrossTab\ncross = pd.crosstab(train[\"AdoptionSpeed\"],train['MaturitySize'],margins=True)\n# Delete the All obs. per variable y\ncross = cross.drop(cross.index[len(cross)-1])\nprint(cross.columns)\n# Stacked Bar Plot\nType1 = pd.DataFrame(cross[1]\/cross['All'])\nType2 = pd.DataFrame(cross[2]\/cross['All'])\nType3 = pd.DataFrame(cross[3]\/cross['All'])\nType4 = pd.DataFrame(cross[4]\/cross['All'])\n\nax1.bar(Type1.index.values, Type1[0], label='Small', color='#6f07f9')\nax1.bar(Type2.index.values, Type2[0], bottom=Type1[0], color='#d6adf7', label='Medium')\nax1.bar(Type3.index.values, Type3[0], bottom=[z+t for z,t in zip(Type1[0], Type2[0])], label='Large', color='#ef53c7')\nax1.bar(Type4.index.values, Type4[0], bottom=[s+t+z for s,z,t in zip(Type1[0], Type2[0],Type3[0])], label='Extra Large', color='#3b006b')\n\n# Add title and axis names\nax1.set_title('Adoption Speed vs Maturity Size')\nax1.set(xlabel='Adoption Speed', ylabel=\"Maturity Size\")\nax1.legend(loc='upper right')\n","f9172022":"plt.figure(figsize=(14,6))\nsns.countplot(x=\"AdoptionSpeed\",hue=\"ColorName1\", data=train,palette=('#393938','#ffd700','#d87c01',\"#febf09\",\"#f9dfb0\",\"#707070\",\"#befbfc\")).set_title('Distribution of Adoption Speed per Color')","8374d721":"# Draw a nested analysis per Target\nfeatures =['L_Color1','L_Color2','L_Color3']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics(train,\"AdoptionSpeed\",i,ncat=range(0,2))   ","47f64791":"# Draw a nested analysis per Target\nfeatures =['L_Breed1','L_Breed2']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics(train,\"AdoptionSpeed\",i,ncat=range(0,2))   ","f8035b5d":"# Free Adoption \nfeatures =['L_Fee_Free']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics(train,\"AdoptionSpeed\",i,ncat=range(0,2))   ","9a983815":"# Free Adoption Dogs \nfeatures =['L_Fee_Free']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics(train.query('Type==1'),\"AdoptionSpeed\",i,ncat=range(0,2))   ","7afb9d13":"# Free Adoption Cats \nfeatures =['L_Fee_Free']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics(train.query('Type==2'),\"AdoptionSpeed\",i,ncat=range(0,2))   ","7cc325dd":"fee_analysis =train.query('Fee>0').groupby(['AdoptionSpeed'])\nfee_analysis['Fee'].describe()\n","62227514":"fig= plt.subplots(figsize=(18,8))\nax = sns.countplot(x=\"StateName\", data=train, order = train[\"StateName\"].value_counts().index)\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/2., p.get_height(), '%d' % int(p.get_height()), \n            fontsize=12, color='green', ha='center', va='bottom')","853b57ab":"fig= plt.subplots(figsize=(24,12))\nax = sns.countplot(x=\"StateName\",  hue=\"AdoptionSpeed\",data=train, palette=\"Set2\")","6719e3c2":"rescuer_analysis =train.groupby(['RescuerID']).size().reset_index(name='counts')\nrescuer_analysis['counts'].describe()","147ed875":"Top4 =rescuer_analysis.sort_values('counts',ascending=False).head(4)\nTop4","7279b0e6":"train_toprescuer = train.loc[train['RescuerID'].isin(Top4['RescuerID'].values.tolist())]\n\nfig = plt.subplots(figsize=(24,10))\n\nax = sns.countplot(x=\"RescuerID\",data=train_toprescuer ,hue=\"AdoptionSpeed\" ,palette=\"Set3\")\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/2., p.get_height(), '%d' % int(p.get_height()), \n            fontsize=12, color='blue', ha='center', va='bottom') ","c50b81cb":"# Best Rescuer for minimum 10 pets\nrescuer_speed =train.groupby(['RescuerID','AdoptionSpeed']).size().reset_index(name='N_pets_speed')\n\npets_total = train.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\nrescuer_analysis= pd.merge(pets_total, rescuer_speed, left_on='RescuerID', right_on='RescuerID', how='inner')\nrescuer_analysis['pct_pets'] = rescuer_analysis['N_pets_speed']\/rescuer_analysis['N_pets_total']\n\n#Big Rescuer\nrescuer_analysis.query(\"N_pets_total>10\").groupby(\"AdoptionSpeed\")['pct_pets'].describe().reset_index()\n","628fb90d":"#Small Rescuer\nrescuer_analysis.query(\"N_pets_total<10\").groupby(\"AdoptionSpeed\")['pct_pets'].describe().reset_index()","02c251a4":"train['PhotoAmt'].describe()","d92bb1b8":"fig = plt.subplots(figsize=(12,5))\nsns.distplot(train['PhotoAmt'])","ffd3cc56":"train.groupby('AdoptionSpeed')['PhotoAmt'].mean()","290f72ab":"# Photo? \nfeatures =['L_NoPhoto']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics(train,\"AdoptionSpeed\",i,ncat=range(0,2))   ","d396cf1f":"train['VideoAmt'].describe()","c97c774d":"fig = plt.subplots(figsize=(12,5))\nsns.distplot(train['VideoAmt'])","c9327321":"# Video? \nfeatures =['L_NoVideo']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics(train,\"AdoptionSpeed\",i,ncat=range(0,2))   ","b055cdf2":"train.groupby('AdoptionSpeed')['sentiment_document_score'].mean()","730ad5ac":"train.groupby('AdoptionSpeed')['sentiment_document_magnitude'].mean()","832e1a07":"graphics_num(train,'AdoptionSpeed','sentiment_document_magnitude')","afe21ea8":"features =['L_scoreneg']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics(train,\"AdoptionSpeed\",i,ncat=range(0,2))   ","6478520b":"#Impact on missing name or incorrect name\n\nfeatures =['L_Name_missing']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics(train,\"AdoptionSpeed\",i,ncat=range(0,2))   ","31a6b884":"graphics_num(train,'AdoptionSpeed','Name_Length')","795d9c52":"train_dog=train.loc[train['Type'] == 1]\ntrain_cat=train.loc[train['Type'] == 2]\n\ntext_cat = ','.join(str(v) for v in train_cat.Name)\ntext_dog = ','.join(str(v) for v in train_dog.Name)\nprint (\"There are {} words in the combination of all Name.\".format(len(text_dog)))","7802a409":"def word_cloud(text):\n    # Create a word cloud image\n    wc = WordCloud(background_color=\"white\", max_words=300,\n                   contour_width=3, contour_color='firebrick')\n\n    # Generate a wordcloud\n    wc.generate(text)\n\n    # show\n    plt.figure(figsize=[15,8])\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n\nword_cloud(text_dog)","4f586d57":"word_cloud(text_cat)","8b6f1b5a":"text_cat = ','.join(str(v) for v in train_cat.Description)\ntext_dog = ','.join(str(v) for v in train_dog.Description)\nprint (\"There are {} words in the combination of all Description for Cats.\".format(len(text_cat)))","a7b016e0":"graphics_num(train,'AdoptionSpeed','Description_Length')","82de5107":"# Draw a nested analysis per Target\nfeatures =['metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min','metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum','L_metadata_any_cat_sum','L_metadata_any_dog_sum']\ntarget = ['AdoptionSpeed']\nfor i in features:\n    graphics_num(train,\"AdoptionSpeed\",i)   ","2dafb999":"# Draw a nested analysis per Target\nfeatures =['pixel_mean','blur_mean','pixel_min','blur_min','pixel_max','blur_max','pixel_sum','blur_sum']\nfor i in features:\n    graphics_num(train,\"AdoptionSpeed\",i)","d6851932":"def plot_correlation_matrix(df):\n    corr = df.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(30, 15))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","24b474a9":"plot_correlation_matrix(train)","06fa1bda":"columns = train.columns\npercent_missing = train.isnull().sum() * 100 \/ len(train)\nmissing_value_df = pd.DataFrame({'column_name': columns,\n                                 'percent_missing': percent_missing})\n\nmissing_value_df =missing_value_df[missing_value_df['percent_missing']>0]\nmissing_value_df\n\n\nplt.figure(figsize=(20, 10))\nax = sns.barplot(x=\"column_name\", y=\"percent_missing\", data=missing_value_df, label='Sales')\nax.set_xticklabels(ax.get_xticklabels(),rotation=75)\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/2., p.get_height(), '%d' % round(p.get_height(),2) + '%', \n            fontsize=12, color='grey', ha='center', va='bottom') \n    \nplt.show()","b80bac8f":"# Cannot be used for this analysis (IDs, Texts...)\ntrain_analysis = train.drop([\"Name\",\"Description\",\"BreedName2\",\"ColorName3\",'Name','Breed1','Breed2','RescuerID','Description',\n                            'BreedName1','Color1', 'Color2', 'Color3','Age','State','ImageId'],axis=1)\n\nfor col in ['sentiment_document_score', 'sentiment_document_magnitude','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n           'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_dog_sum',\n           'L_metadata_0_cat_sum','L_metadata_any_dog_sum','L_metadata_any_cat_sum','pixel_mean','pixel_min','pixel_max','pixel_sum',\n           'blur_min','blur_max','blur_sum','blur_mean','metadata_color_pixelfrac_mean','metadata_color_pixelfrac_min',\n           'metadata_color_pixelfrac_max','metadata_color_score_mean','metadata_color_score_min','metadata_color_score_max',\n           'Description_Length','Name_Length','huMoments0','huMoments1','huMoments2','huMoments3','huMoments4','huMoments5','huMoments6']:\n    train_analysis[col].fillna((train_analysis[col].median()), inplace=True)\n    \n# replacing na values with No Color \ntrain_analysis[\"ColorName2\"].fillna(\"No Color\", inplace = True) ","087ad2df":"#Label Encoding Breed\n#One Hot Encoding: ColorName1,ColorName2,StateName\ntrain_analysis = pd.concat([train_analysis.drop('StateName', axis=1),pd.get_dummies(train_analysis['StateName'], prefix='State')], axis=1)\n\ncol=['ColorName1','ColorName2','Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength']\nfor i in col:\n    train_analysis = pd.concat([train_analysis.drop(i, axis=1),pd.get_dummies(train_analysis[i], prefix=i)], axis=1)","13093f22":"# FROM: https:\/\/www.kaggle.com\/myltykritik\/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    \n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)\n","29af9ce5":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.6, 1.7, 2.6, 3.6]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n    \ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","e8c34baa":"def evaluate(y_pred, y_true):\n  \n    cohen_kappa= cohen_kappa_score(y_true, y_pred)\n    accuracy=accuracy_score(y_true,y_pred)\n    f1=f1_score(y_true,y_pred,average='micro')\n    classification=classification_report(y_true,y_pred)\n    \n    #Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(20,6))\n    \n    sns.heatmap(cm, annot=True)\n    plt.title('Confusion matrix')\n    plt.figure(figsize = (5,4))\n    plt.show()\n    #Evaluation Metrics\n    print('Cohen Kappa: {:0.2f}.'.format(cohen_kappa))\n    print('Accuracy Score: {:0.2f}%.'.format(accuracy))\n    print('F1 Score: {:0.2f}%.'.format(f1))\n    ","e39be409":"print(train_analysis.columns)","7a0cad30":"#Extracting Features and Output\nids=train_analysis[['PetID']]\ntrain_analysis=train_analysis.drop(['PetID','State_Labuan'],axis=1)","5b13dfa0":"X, y = train_analysis.loc[:, train_analysis.columns != 'AdoptionSpeed'], train_analysis['AdoptionSpeed']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)\n#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)","4add506d":"plt.subplot(1, 2, 1)\nprob_train  =y_train.value_counts(normalize=True)\nprob_train.plot(kind='barh',figsize =(15,6))\nplt.title('Adoption Speed Repartition for the Training Set (75% Data)')\nplt.subplot(1, 2, 2)\n\nprob  =y_test.value_counts(normalize=True)\nprob.plot(kind='barh', figsize =(15,6))\nplt.title('Adoption Speed Repartition for the Test Set (25% Data)')\nplt.show()","5ba4d091":"model = lgb.LGBMRegressor()\nmodel.fit(X_train, y_train)\n\nfeature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,X_train.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(10, 17))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\n\nfeatures_selection = SelectFromModel(model, threshold='1.25*median') # The Threshold is the median of features importance*1.25 \nfeatures_selection.fit(X_train, y_train)\n\nfeatures_selection_support = features_selection.get_support()\nfeatures_selection = X_train.loc[:,features_selection_support].columns.tolist()\nfeatures_selection","9b8e26fa":"X_train =X_train.loc[:,features_selection]\nX_test = X_test.loc[:,features_selection]","900b678c":"print('Original dataset shape %s' % Counter(y_train))\n\nsampling_strategy= {4: 3148, 2: 3028, 3: 2444, 1: 2317, 0: 1000}\nros = RandomOverSampler(sampling_strategy= sampling_strategy, random_state=42)\n\nX_res, y_res = ros.fit_resample(X_train, y_train)\nprint('Resampled dataset shape %s' % Counter(y_res))","d2d6be6c":"from sklearn import preprocessing\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_scaled = min_max_scaler.fit_transform(X_train)\nX_res_scaled = min_max_scaler.fit_transform(X_res)\n\n# Instanciate a PCA object for the sake of easy visualisation\npca = PCA(n_components=2)\n# Fit and transform x to visualise inside a 2D feature space\nX_vis = pca.fit_transform(X_scaled)\nX_res_vis = pca.transform(X_res_scaled)\n\nplt.figure()\n\n# sp1\nplt.subplot(121)\nplt.scatter(X_vis[y_train == 0, 0],  X_vis[y_train == 0, 1],  c=\"navy\", alpha=0.5,label=\"Class 0\")\nplt.legend(loc='upper left')\n\nplt.subplot(122)\nplt.scatter(X_res_vis[y_res == 0, 0],  X_res_vis[y_res == 0, 1],  c=\"navy\", alpha=0.5,label=\"Class 0\")\n\nplt.legend(loc='upper left')\nplt.show()","05292721":"def cross_val(model,X_train,y_train):\n    X = X_train\n    y = y_train\n    coeff = np.empty((1,4))\n    cv_scores=[]\n    fold=1\n    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n    print(skf.get_n_splits(X, y))\n\n    for train_index, val_index in skf.split(X, y):\n        xtrain, xvalid = X[train_index], X[val_index]\n        ytrain, yvalid = y[train_index], y[val_index]\n\n        model.fit(\n            xtrain, ytrain,\n            eval_set=[(xvalid, yvalid)],\n            verbose=100,\n            early_stopping_rounds=100\n        )\n\n        #model.fit(xtrain, ytrain)\n        valid_preds = model.predict(xvalid, num_iteration=model.best_iteration_)\n        yvalid = np.array(yvalid).tolist()\n        optR = OptimizedRounder()\n        optR.fit(valid_preds, yvalid)\n\n        coefficients = optR.coefficients()\n        valid_p = optR.predict(valid_preds, coefficients)\n\n        scr = quadratic_weighted_kappa(yvalid, valid_p)\n        cv_scores.append(scr)\n\n        print(\"QWK = {}. Coef = {}\".format(scr, coefficients))\n        coefficients.reshape((4, 1))\n\n        coeff = np.vstack([coeff, coefficients])\n        fold += 1\n\n\n    coeff = np.delete(coeff, (0), axis=0)\n    global coefficient_mean\n    coefficient_mean = coeff.mean(axis=0)\n    print(\"Coef Mean ={}\".format(coefficient_mean))","599bdac3":"lgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'regression',\n'learning_rate': 0.005,\n'subsample': .8,\n'colsample_bytree': 0.8,\n'min_split_gain': 0.006,\n'min_child_samples': 150,\n'min_child_weight': 0.1,\n'n_estimators': 1000,\n'num_leaves': 80,\n'silent': -1,\n'verbose': -1,\n'max_depth': 11,\n'random_state': 2018\n}\n    \nlgb_model = lgb.LGBMRegressor(**lgb_params)\n\ncross_val(lgb_model,X_res,y_res)\n\n#Prediction\ny_pred=lgb_model.predict(X_train.values)\n","60c133e5":"print(coefficient_mean)","b8623507":"optR=OptimizedRounder()\ny_true = pd.DataFrame(y_train)\ny_true.reset_index(inplace=True,drop=False)\n\npredictions = optR.predict(y_pred, coefficient_mean).astype(int)\npred_lgb = pd.concat([pd.DataFrame(y_pred),y_true,pd.DataFrame(predictions)],axis=1,ignore_index=True)\npred_lgb.columns = ['y_pred','index','y_true','y_pred_class']\n\npred_lgb['y_pred_class'].value_counts()","7093683c":"evaluate(pred_lgb['y_pred_class'].tolist(), pred_lgb['y_true'].tolist())","7bae56aa":"y_pred=lgb_model.predict(X_test.values)\npredictions = optR.predict(y_pred, coefficient_mean).astype(int)\ny_test =pd.DataFrame(y_test)\ny_test.reset_index(drop=False, inplace=True)\n\n\npred_lgb = pd.concat([pd.DataFrame(y_pred),y_test,pd.DataFrame(predictions)],axis=1,ignore_index=True)\npred_lgb.columns = ['y_pred','index','y_true','y_pred_class']\npred_lgb['y_pred_class'].value_counts()","74f445bf":"evaluate(pred_lgb['y_pred_class'].tolist(), pred_lgb['y_true'].tolist())","c9f64caa":"lgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'regression',\n'learning_rate': 0.005,\n'subsample': .8,\n'colsample_bytree': 0.8,\n'min_split_gain': 0.006,\n'min_child_samples': 150,\n'min_child_weight': 0.1,\n'n_estimators': 1000,\n'num_leaves': 80,\n'silent': -1,\n'verbose': -1,\n'max_depth': 11,\n'random_state': 20\n}\n    \nlgb_model2 = lgb.LGBMRegressor(**lgb_params)\n\nlgb_model2.fit(X_res, y_res)\n\n#Prediction\ny_pred2=lgb_model2.predict(X_test)","d84de536":"optR=OptimizedRounder()\n\npredictions2 = optR.predict(y_pred2, coefficient_mean).astype(int)\n\npred_lgb2 = pd.concat([pd.DataFrame(y_pred2),y_true,pd.DataFrame(predictions2)],axis=1,ignore_index=True)\npred_lgb2.columns = ['y_pred_lgb2','index','y_true','y_pred_class']\n\npred_lgb2['y_pred_class'].value_counts()","6602b4c1":"lgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'regression',\n'learning_rate': 0.005,\n'subsample': .8,\n'colsample_bytree': 0.8,\n'min_split_gain': 0.006,\n'min_child_samples': 150,\n'min_child_weight': 0.1,\n'n_estimators': 1000,\n'num_leaves': 80,\n'silent': -1,\n'verbose': -1,\n'max_depth': 11,\n'random_state': 2019\n}\n   \nlgb_model3 = lgb.LGBMRegressor(**lgb_params)\n\nlgb_model3.fit(X_res, y_res)\n\n#Prediction\ny_pred=lgb_model3.predict(X_test.values)","2fdc3843":"optR=OptimizedRounder()\n\npredictions = optR.predict(y_pred, coefficient_mean).astype(int)\npred_lgb3 = pd.concat([pd.DataFrame(y_pred),y_true,pd.DataFrame(predictions)],axis=1,ignore_index=True)\npred_lgb3.columns = ['y_pred_lgb3','index','y_true','y_pred_class']\n\npred_lgb3['y_pred_class'].value_counts()","8958eb17":"## Train, Test & Validation Sets","1bfcc15e":"-  1 Color or multiple colors doesn't seem to have an impact on Adoption Speed","95c0bb60":"## Performance of the model","6c945429":"## LightGBM 2","f1806591":"The data is provided by [Petfinder.my](Petfinder.my) , a platform dedicated for pets adoption. \nThe objective is to predict at which speed a pet is adopted. \n\nThere are 6 different sources of data + Images + Metadata Images and Sentiment Data \n-   Train.csv\n-   Test.csv\n-   breed_labels.csv\n-   color_labels.csv\n-   state_labels.csv\n-   Images (zip file) from cats and dogs that are adopted\n-   Metadata Images (zip file) information about the Image using Google Vision API\n-   Sentiment Data is based on the Descriptions using Google's Natural Language API. \n","bcde8799":"\n -   0 - Pet was adopted on the same day as it was listed.\n -   1 - Pet was adopted between 1 and 7 days (1st week) after being listed.\n -   2 - Pet was adopted between 8 and 30 days (1st month) after being listed.\n -   3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.\n -   4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).\n","b178b8eb":"## LightGBM 3","7a238883":"In this step we will export the description and the topicality of each image. For more information on [Google API Vision]","12334334":"### Target vs dependant variables","5126da15":"In a first step we divide our data into a training(80% of Data) and a testing set (20% of Data). To tune the hyperparameter and avoid overfitting we used the technique of Cross Validation (CV). For K-Fold CV, we further split our training set into K number of subsets, called folds.We then iteratively fit the model K times, each time training the data on K-1 of the folds and evaluating on the Kth fold. For hyperparameter tuning, we perform many iterations of the entire K-Fold CV process, each time using different model settings. We then compare all of the models, select the best one! At the very end of training, we average the performance on each of the folds to come up with final validation metrics for the model.","f19d3af8":"### Target Analysis","11dd84b3":"-  The result is interesting for No adoption after 100 days the fee is less higher (in mean) than for the Adoption between 1 and 10 days \n- Free cats seems to be adopted faster","cefacb76":"-  In Kuala Lumpur and Pulau Pinang the process seems slower ","eb617ec7":"#### Pet's Name Analysis","cded037d":"## Python Packages","16f9031f":"## Optimized Boundaries","21629fd9":"-  Video and Photo Amount doesn't seem to have a huge impact on Adoption Speed","d3fc7e0f":"###### Rescuer with more pets have higher Adoption Rate? YES!","654f17c1":"-  Missing Name has an impact on Adoption Speed. Let's Analyse the Name !","93f73550":"- the First Rescuer seems to have faster adoption","14a8ed0c":"## Missing Values","5ca4d979":"## HU Moments","d3678916":"### Adoption Data ","8168970e":"## Features from Text Mining","f27fbe36":"#### Gender Analysis ","26308c04":"-  Most animals are charge free ","728d5602":"We will not keep variables with more than 50% of observations.\nName, Description, BreedName2, ColorName3 will not be included. For the others values we will replace the numeric by the median and create a category \"missing\" for categorical variables  ","ba394daf":"## Oversampling","27365459":"#### Color pets analysis","988f608f":"#### Fee Adoption ","ec487fcc":"-  More than one animals have tendance to be adopted after","a84e30f4":"*The aim of this notebook it's to have an overview on all the Data from the competition.*","e3f8a33d":"#### State analysis","ffb0c33e":"### Sentiment Data","ce79b42c":"-  Type 1: Dog - Type 2: Cat. We see that cats are adopted more faster than dogs. ","5e7c2e88":"### Image Quality Analysis","e805757e":"-  A Longer Fur favorise animal adoption (Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified))\n-  Vaccination doesn't seem to have an impact on adoption (1 = Yes, 2 = No, 3 = Not Sure)\n-  Dewormed doesn't seem to have an impact on adoption ...\n-  Sterilized either, they prefer non sterilized pets?\n-  Majority of Pets are healthy and in general they prefer health pets. ","4209f4d5":"-  Cat or dog with 2 breeds may have higher speed adption","db8ab330":"### Image Metadata","a84b4555":"-  Small and Large pets are adopted faster","f227a370":"Number of pets per Rescuer and List the Top 10 Rescuer","bb317c82":"-  We see some more negative score when the Adoption is slower\n-  Thus we create variable negative Score ","08ed6e88":"#### Video and Photo Analysis","b874c728":"-  In mean a Rescuer have around 2 pets, let's see the top Rescuer ! ","74a17ff2":"#### RescuerID analysis","68ee0706":"#### Health Analysis","3e2711af":"## Features Selection","d583d44d":"#### Sentiment Data Analysis","55f9070d":"## Adoption Data Exploration","f771f47c":"## Categorical Encoding","d170e9d5":"<img src=\"https:\/\/i.imgur.com\/NWIf9Gf.png\" alt=\"state\" title=\"state\" width=\"600\" height=\"400\" \/> ","3d4d29d1":"#### Breed analysis","c973841f":"[](http:\/\/)We are going to use 75% of the data for training and the remaining 25% to test the model. \nWe will tune the hyperparameters using cross validation datasets. ","75b938d0":"#### Age & Mature Size analysis","9c4310f1":"## Cross Validation","e06fb550":"*To improve the model we create different variables based on the data created by the Google API. *","aea8a8c1":"## Correlation Matrix","27ad24a0":"#### Animal Type Analysis","1c52daf6":"### Image Quality","9da38223":"## LightGBM model: optimize the boundaries","2b9db8cd":"#### Graphic function","9b097179":"## Image Metadata Analysis","87e61e0e":"# Kaggle Competition: Predict at which speed a pet is adopted","b8e9c6c5":"<img src=\"https:\/\/i.imgur.com\/amekoez.jpg\" alt=\"Cat\" title=\"Cat\" width=\"800\" height=\"600\" \/>","119c6f82":"# Modelisation","bc41c8f9":"## Evaluation with Quadratic Weighted Kappa","bbfa550f":"### Conclusion on her first analysis\nThe category **0** (pet was adopted on the same day as it was listed) concerns only 3% of the Dataset. The prediction will be hard for this category.  \nFeatures that have an impact on *Adoption Speed*:  \n\n- Type: cats are adopted faster than dogs\n- Mixed gender are adopted slower certainly due to the obligation to adopt more than one pet. \n- An animal with more fur is adopted faster\n- Older pets are adopted slower\n- Small pets are adopted faster \n- Mixed Breed seem to be adopted faster\n- Free cats seem to have an impact on the Adoption\n- Most Pets are adopted faster in Selangor and slower in Pulan Pinang and Kuala Lumpur. Selangor is the suburb of Kuala Lumpur.\n- The bigger rescuer seem to have faster adoption\n- Higher Sentiment Score -> fast adoption\n- If the Pet Name is missing the adoption is slower\n- Topicality and the image description seem to have an impact  \n\nFeatures with no impact or less impact on *Adoption Speed*:  \n\n- Vaccinated, Dewormed and Sterilized seem to have no impact on Adoption Speed?   \n- Colors seem to have no impact  ","7eb55e2f":"Image quality assessment aims to quantitatively represent the human perception of quality. To assign quality images we will add : pixels and  blur score using the variance of Laplacian.  \nThe following variables are created:\n-  Pixel of all images for a pet\n-  Pixel average for all pictures for a Pet\n-  Blur of all images for a pet\n-  Blur average for all pictures for a Pet","5f80a041":"## Datasets","b5d65022":"-  The dataset contains young pets, 50% of them have less than 6 months.\n-  We see that older pets are adopted slower or not adopted","4af145c1":"### Features Engineering"}}