{"cell_type":{"d8026203":"code","9be50bb8":"code","f870cd30":"code","c2fd5e83":"code","4925d7a7":"code","aff009ce":"code","888eecd4":"code","7fe5a68c":"code","d5308327":"code","67e8659f":"code","e9d0c45d":"code","27f88cf6":"code","cc494c30":"code","df83e002":"code","847e4980":"code","dbe6e2b9":"code","abf97327":"code","0351f706":"code","e90d407b":"code","ebe984e2":"code","ff493690":"code","1a9cc2c0":"code","48f65f51":"code","1d1527d9":"code","13030643":"code","fe0c8509":"code","88f2959e":"code","03a5d5de":"code","8f1683e9":"code","6f637f95":"code","64d611dc":"code","5a69d210":"code","5cbcac84":"code","c1f5a548":"code","ec499772":"code","ccbc0e34":"code","1c3405ec":"code","ec4e6593":"code","657dd6ec":"code","e9f87ba0":"code","52849eb3":"code","e236499c":"code","a1b0e16a":"code","39f4723d":"code","fb45e0fd":"markdown","918e9204":"markdown","1a69594d":"markdown","681a5a55":"markdown","9433901a":"markdown","8326d47d":"markdown","554056ce":"markdown","caa4cf38":"markdown","843e3dfc":"markdown","e429db80":"markdown","ed2bb342":"markdown","265603a2":"markdown","d0958c9d":"markdown","50e5f8b8":"markdown","df4e4a44":"markdown","9a52975e":"markdown","a5634a22":"markdown","b5bb2e1d":"markdown","b2cdc06f":"markdown","68d748bc":"markdown","0eb08bab":"markdown","e643f00e":"markdown","4ab10e93":"markdown","caa8a955":"markdown","459e5cad":"markdown","c96d390b":"markdown","914fdb5c":"markdown","a3d2e055":"markdown","bc2cf23f":"markdown","f8536cc5":"markdown","ff8dc25c":"markdown","2b86b018":"markdown","c0e65672":"markdown","7ff15149":"markdown","94d80001":"markdown","09ac5074":"markdown","e4f2d422":"markdown","bd970a32":"markdown","f33c38a7":"markdown","fbefc47c":"markdown"},"source":{"d8026203":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\n\npath_base = '\/Face Mask Dataset'\npath_train = path_base + '\/Train'\npath_val = path_base + '\/Validation'\npath_test = path_base + '\/Test'","9be50bb8":"from IPython.display import Image\nfrom IPython.display import display\nx = Image(filename=path_train + '\/WithMask\/509.png') \ny = Image(filename=path_train + '\/WithoutMask\/1658.png') \ndisplay(x, y)","f870cd30":"dif_categories = ['\/WithMask\/', '\/WithoutMask\/']\ndif_path = [path_train, path_val, path_test]\n\nall_images = []\naux = ''\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            #print(path + category + file)\n            aux = path + category + file\n            all_images.append(aux)\n            \nprint('Cantidad total de im\u00e1genes en el dataset:', len(all_images))","c2fd5e83":"withMask_count = len(os.listdir(path_train + '\/WithMask'))\nwithoutMask_count = len(os.listdir(path_train + '\/WithoutMask'))\nprint('Cantidad de im\u00e1genes en subcarpeta WithMask:', withMask_count)\nprint('Cantidad de im\u00e1genes en subcarpeta WithoutMask:', withoutMask_count)","4925d7a7":"# Se obtiene la cantidad de imagenes cuyo prefijo no es Augmented (con ese prefijo son las imagenes transformadas)\nname = ''\nall_withMask = []\nall_withoutMask = []\n\nfor category in dif_categories:\n    for file in os.listdir(path_train + category):\n        name = file\n        if category == '\/WithMask\/' and name.split('_',1)[0] != 'Augmented':\n            all_withMask.append(name)\n        #if name.split('_',1)[0] != 'Augmented':\n            #all_names.append(name)\n        if category == '\/WithoutMask\/' and name.split('_',1)[0] != 'Augmented':\n            all_withoutMask.append(name)\n        \nprint('Cantidad de im\u00e1genes en subcarpeta WithMask:', len(all_withMask))\nprint('Cantidad de im\u00e1genes en subcarpeta WithoutMask:', len(all_withoutMask))","aff009ce":"# Se obtiene la cantidad de imagenes cuyo prefijo no es Augmented (con ese prefijo son las imagenes transformadas)\nname = ''\nall_withMask = []\nall_withoutMask = []\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            name = file\n            if category == '\/WithMask\/' and name.split('_',1)[0] != 'Augmented':\n                all_withMask.append(name)\n        #if name.split('_',1)[0] != 'Augmented':\n            #all_names.append(name)\n            if category == '\/WithoutMask\/' and name.split('_',1)[0] != 'Augmented':\n                all_withoutMask.append(name)\n        \nprint('Cantidad de im\u00e1genes en subcarpeta WithMask:', len(all_withMask))\nprint('Cantidad de im\u00e1genes en subcarpeta WithoutMask:', len(all_withoutMask))","888eecd4":"from IPython.display import Image\n\nname = ''\ni = 0\n\nfor file in os.listdir(path_train + '\/WithMask\/'):\n        name = file\n        if name.split('_',1)[0] == 'Augmented':\n            im= Image(filename = path_train + '\/WithMask\/' + name) \n            display(im)\n            i += 1\n            if i == 10:\n                break\n            ","7fe5a68c":"import cv2\n\nimageSize = []\ndimensions = []\ndif_categories = ['\/WithMask\/', '\/WithoutMask\/']\ndif_path = [path_train, path_val, path_test]\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            imageSize = cv2.imread(path + category + file)\n            #print(imageSize)\n            ims = imageSize.shape[1::-1] # height, width\n            dimensions.append(ims)","d5308327":"print('Tama\u00f1o m\u00ednimo de imagen:', min(dimensions))\nprint('Tama\u00f1o m\u00e1ximo de imagen:', max(dimensions))","67e8659f":"dimensions","e9d0c45d":"imageSize224 = []\ndimensions224 = []\ndif_categories = ['\/WithMask\/', '\/WithoutMask\/']\ndif_path = [path_train, path_val, path_test]\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            if file.split('_',1)[0] == 'Augmented':\n                imageSize224 = cv2.imread(path + category + file)\n                ims = imageSize224.shape[1::-1] # height, width\n                dimensions224.append(ims)","27f88cf6":"print('Tama\u00f1o m\u00ednimo imagen:',min(dimensions224))\nprint('Tama\u00f1o m\u00e1ximo de imagen:', max(dimensions224))","cc494c30":"dimensions224","df83e002":"list_size_0 = list()\nlist_size_1 = list()\nfor dim in dimensions:\n    list_size_0.append(dim[0])\n    list_size_1.append(dim[1])\n\ndata = [list_size_0, list_size_1]","847e4980":"import matplotlib.pyplot as plt \n\nfig = plt.figure(figsize =(10, 7))\n# Creating plot \nplt.boxplot(data) \nplt.title('Gr\u00e1fico Caja Bigotes Dataset Completo Kaggle')\nplt.ylabel('P\u00edxeles')\nplt.xlabel('Alto \/ Ancho')  \n# show plot \nplt.show()","dbe6e2b9":"media = sum(list_size_0) \/ len(list_size_0)\nprint('Media:', media)\nimport statistics\nmediana = statistics.median(list_size_0)\nprint('Mediana:', mediana)","abf97327":"import cv2\n\nimageSizeNotAug = []\ndimensionsNotAug = []\ndif_categories = ['\/WithMask\/', '\/WithoutMask\/']\n#dif_categories = ['\/WithMask\/']\n#dif_categories = ['\/WithoutMask\/']\ndif_path = [path_train, path_val, path_test]\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            if file.split('_',1)[0] != 'Augmented':\n                imageSizeNotAug = cv2.imread(path + category + file)\n                ims = imageSizeNotAug.shape[1::-1] # height, width\n                dimensionsNotAug.append(ims)\n\nlist_size_0 = list()\nlist_size_1 = list()\nfor dim in dimensionsNotAug:\n    list_size_0.append(dim[0])\n    list_size_1.append(dim[1])\n\ndata = [list_size_0, list_size_1] \n\nfig = plt.figure(figsize =(10, 7))\n# Creating plot \nplt.boxplot(data) \nplt.title('Gr\u00e1fico Caja Bigotes Dataset Completo Kaggle')\nplt.ylabel('P\u00edxeles')\nplt.xlabel('Alto \/ Ancho') \n  \n# show plot \nplt.show()\n\nprint('Media Sin Augmentation: ', sum(list_size_0) \/ len(list_size_0))\nprint('Mediana Sin Augmentation: ', statistics.median(list_size_0))","0351f706":"path_base = 'C:\/Users\/alcac\/Desktop\/TFM\/TFM\/Modelos_definitivos\/Face Mask Dataset_86\/'\npath_train = path_base + 'Train'\npath_val = path_base + 'Validation'\npath_test = path_base + 'Test'","e90d407b":"# Prueba de poner todas las imagenes con el mismo tama\u00f1o \nfrom PIL import Image\n\nimg = []\ndif_categories = ['\/WithMask\/', '\/WithoutMask\/']\ndif_path = [path_train, path_val, path_test]\n\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            height = 86\n            width = 86\n            img = Image.open(path + category + file)\n            img = img.resize((height,width))\n            img.save(path + category + file) ","ebe984e2":"# Se comprueba de nuevo el tama\u00f1o de las imagenes\nimport cv2\n\nimageSize = []\ndimensions = []\ndif_categories = ['\/WithMask\/', '\/WithoutMask\/']\ndif_path = [path_train, path_val, path_test]\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            imageSize = cv2.imread(path + category + file)\n            ims = imageSize.shape[1::-1] # height, width\n            dimensions.append(ims)","ff493690":"dimensions","1a9cc2c0":"list_size_0 = list()\nlist_size_1 = list()\n\nes86x86 = 0\nNoes86x86 = 0\nfor dim in dimensions:\n    list_size_0.append(dim[0])\n    list_size_1.append(dim[1])\n    if list_size_0.append(dim[0]) == list_size_1.append(dim[1]):\n        es86x86 += 1\n    else:\n        Noes86x86 += 1\n        \nprint('Cantidad im\u00e1genes que es 86x86 =', es86x86)\nprint('Cantidad im\u00e1genes que NO es 86x86 =', Noes86x86)","48f65f51":"path_base = 'C:\/Users\/alcac\/Desktop\/TFM\/TFM\/Modelos_definitivos_115\/Face Mask Dataset_115\/'\npath_train = path_base + 'Train'\npath_val = path_base + 'Validation'\npath_test = path_base + 'Test'","1d1527d9":"# Prueba de poner todas las imagenes con el mismo tama\u00f1o \nfrom PIL import Image\n\nimg = []\ndif_categories = ['\/WithMask\/', '\/WithoutMask\/']\ndif_path = [path_train, path_val, path_test]\n\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            height = 115\n            width = 115\n            img = Image.open(path + category + file)\n            img = img.resize((height,width))\n            img.save(path + category + file) ","13030643":"# Se comprueba de nuevo el tama\u00f1o de las imagenes\nimport cv2\n\nimageSize = []\ndimensions = []\ndif_categories = ['\/WithMask\/', '\/WithoutMask\/']\ndif_path = [path_train, path_val, path_test]\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            imageSize = cv2.imread(path + category + file)\n            ims = imageSize.shape[1::-1] # height, width\n            dimensions.append(ims)","fe0c8509":"dimensions","88f2959e":"list_size_0 = list()\nlist_size_1 = list()\n\nes115x115 = 0\nNoes115x115 = 0\nfor dim in dimensions:\n    list_size_0.append(dim[0])\n    list_size_1.append(dim[1])\n    if list_size_0.append(dim[0]) == list_size_1.append(dim[1]):\n        es115x115 += 1\n    else:\n        Noes115x115 += 1\n        \nprint('Cantidad im\u00e1genes que es 115x115 =', es115x115)\nprint('Cantidad im\u00e1genes que NO es 115x115 =', Noes115x115)","03a5d5de":"path_base = 'C:\/Users\/alcac\/Desktop\/TFM\/TFM\/Modelos_definitivos_128\/Face Mask Dataset_128\/'\npath_train = path_base + 'Train'\npath_val = path_base + 'Validation'\npath_test = path_base + 'Test'","8f1683e9":"# Prueba de poner todas las imagenes con el mismo tama\u00f1o \nfrom PIL import Image\n\nimg = []\ndif_categories = ['\/WithMask\/', '\/WithoutMask\/']\ndif_path = [path_train, path_val, path_test]\n\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            height = 128\n            width = 128\n            img = Image.open(path + category + file)\n            img = img.resize((height,width))\n            img.save(path + category + file) ","6f637f95":"# Se comprueba de nuevo el tama\u00f1o de las imagenes\nimport cv2\n\nimageSize = []\ndimensions = []\ndif_categories = ['\/WithMask\/', '\/WithoutMask\/']\ndif_path = [path_train, path_val, path_test]\n\nfor path in dif_path:\n    for category in dif_categories:\n        for file in os.listdir(path + category):\n            imageSize = cv2.imread(path + category + file)\n            #print(imageSize)\n            ims = imageSize.shape[1::-1] # height, width\n            dimensions.append(ims)","64d611dc":"dimensions","5a69d210":"list_size_0 = list()\nlist_size_1 = list()\n\nes128x128 = 0\nNoes128x128 = 0\nfor dim in dimensions:\n    list_size_0.append(dim[0])\n    list_size_1.append(dim[1])\n    if list_size_0.append(dim[0]) == list_size_1.append(dim[1]):\n        es128x128 += 1\n    else:\n        Noes128x128 += 1\n        \nprint('Cantidad im\u00e1genes que es 128x128 =', es128x128)\nprint('Cantidad im\u00e1genes que NO es 128x128 =', Noes128x128)","5cbcac84":"import pandas as pd\nimport numpy as np\nimport keras\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom keras.layers import Dense, GlobalAveragePooling2D, Flatten, Input, BatchNormalization\nfrom keras.models import Model, Sequential\nfrom keras.applications import MobileNet, VGG16\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import preprocess_input as preprocess_vgg16\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom numpy import expand_dims\nimport matplotlib.pyplot as plt","c1f5a548":"path_base = 'Face Mask Dataset_128\/'\npath_train = path_base + 'Train\/'\npath_val = path_base + 'Validation\/'\npath_test = path_base + 'Test\/'","ec499772":"base_model16 = VGG16(weights='imagenet', include_top=False, input_shape=(128,128,3)) # capa top no se incluye\n# porque se incluye una capa propia y entrada de tama\u00f1o de im\u00e1genes especificada\n\n# se congelan las capas preentrenadas\nfor layer in base_model16.layers:\n    layer.trainable = False\n    \ndef modelo5(preent):    \n    x = base_model16.output # modelo base preentrenado\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(512, activation='relu')(x) # se a\u00f1ade otra capa mas\n    x = Dense(2, activation='softmax')(x) # 2 porque son las clases que queremos predecir\n    return x","ccbc0e34":"Modelo5 = modelo5(base_model16)\n\nmodel5 = Model(inputs = base_model16.input, outputs = Modelo5)\nmodel5.summary()","1c3405ec":"# Se define el generador:\ndatagen = ImageDataGenerator(preprocessing_function=preprocess_vgg16) \n# preprocessing_function = preprocess_vgg16\n\ntrain_generator = datagen.flow_from_directory(path_train,\n                                                    target_size=(128,128),\n                                                    color_mode='rgb',\n                                                    batch_size = 128,\n                                                    class_mode='categorical',\n                                                    shuffle=True) \n\nvalidation_generator = datagen.flow_from_directory(path_val,\n                                                    target_size=(128,128),\n                                                    color_mode='rgb',\n                                                    batch_size = 128,\n                                                    class_mode='categorical',\n                                                    shuffle=True) \n\ntest_generator = datagen.flow_from_directory(path_test,\n                                                    target_size=(128,128),\n                                                    color_mode='rgb',\n                                                    batch_size= 128,\n                                                    class_mode='categorical',\n                                                    shuffle=False) ","ec4e6593":"opt = Adam(lr=1e-5)\n\nmodel5.compile(optimizer=opt,loss='categorical_crossentropy',\n              metrics=['accuracy']) \n\nstep_size_train = train_generator.n\/\/train_generator.batch_size\nhistory5 = model5.fit_generator(generator = train_generator,\n                    steps_per_epoch = step_size_train,\n                    epochs = 30,\n                    validation_data = validation_generator,\n                    validation_steps = validation_generator.n\/\/validation_generator.batch_size)","657dd6ec":"%matplotlib inline\nplt.plot(history5.history['loss'])\nplt.plot(history5.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')","e9f87ba0":"%matplotlib inline\nplt.plot(history5.history['accuracy'])\nplt.plot(history5.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('acurracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')","52849eb3":"from numpy import expand_dims\nimport matplotlib.pyplot as plt\n\n# Fuente m\u00f3dulo Deep Learning\nclass_dict = {v:k for k, v in train_generator.class_indices.items()}\n\ndef predict_image(path):\n    img = image.load_img(path)\n    img = img.resize((128, 128))\n    data = expand_dims(image.img_to_array(img), 0)\n    data = preprocess_vgg16(data)\n    preds = model5.predict(data)\n    pred = np.argmax(preds)\n    pred = class_dict[pred]\n    print(pred)\n    return img","e236499c":"import os\npath_test_WithMask = path_test + '\/WithMask'\nfor (dirpath, dirnames, filenames) in os.walk(path_test_WithMask):\n    for filename in filenames:\n        if filename.endswith('.png'): \n            new_path = os.sep.join([dirpath, filename])\n            predict_image(new_path)","a1b0e16a":"import os\npath_test_WithoutMask = path_test + '\/WithoutMask'\nfor (dirpath, dirnames, filenames) in os.walk(path_test_WithoutMask):\n    for filename in filenames:\n        if filename.endswith('.png'): \n            new_path = os.sep.join([dirpath, filename])\n            predict_image(new_path)","39f4723d":"predict_image('sin_mascarilla3.png')","fb45e0fd":"Cantidad im\u00e1genes en total de las tres divisiones sin incluir las im\u00e1genes transformadas","918e9204":"Resumen del modelo:","1a69594d":"Se cargan las librer\u00edas:","681a5a55":"En este apartado se proceder\u00e1 a redimensionar el tama\u00f1o de todas las im\u00e1genes del dataset a 86x86 p\u00edxeles, 115x115 p\u00edxeles y 128x128 p\u00edxeles.","9433901a":"Cantidad im\u00e1genes en *train* incluyendo im\u00e1genes transformadas, donde se observa que est\u00e1 balanceado:","8326d47d":"Se observa que hay alguna imagen cuyo tama\u00f1o excede en comparaci\u00f3n con el tama\u00f1o del resto de im\u00e1genes.","554056ce":"Se definen los generadores con el preprocesado del modelo:","caa4cf38":"----------------------------------------------------","843e3dfc":"Ejemplos de im\u00e1genes con mascarilla y sin mascarilla del dataset de entrenamiento:","e429db80":"Se ha comprobado que el tama\u00f1o de todas las im\u00e1genes se ha establecido correctamente como 128x128.","ed2bb342":"Por \u00faltimo, se preparan las im\u00e1genes a 128x128 p\u00edxeles:","265603a2":"**3. Modelo Final**","d0958c9d":"## <center>M\u00c1STER BIG DATA & BUSINESS ANALYTICS TFM MASK-FINDER (FEBRERO 2021)<\/center>","50e5f8b8":"Se observa que el conjunto original, sin las im\u00e1genes transformadas, no estaba balanceado. En la siguiente salida se muestran 10 ejemplos de im\u00e1genes transformadas mediante la t\u00e9cnica de data Augmentation:","df4e4a44":"A continuaci\u00f3n, se analiza el tama\u00f1o de todas las im\u00e1genes del dataset:","9a52975e":"**2. Redimensi\u00f3n de datos**","a5634a22":"A la vista de los resultados, se observa que se tiene un conjunto de im\u00e1genes con tama\u00f1os diferentes entre s\u00ed, excepto las im\u00e1genes transformadas cuyo tama\u00f1o establecido es 224x224 como se puede observar a continuaci\u00f3n:","b5bb2e1d":"Se observa que la media y la mediana disminuyen, debido a que son im\u00e1genes con tama\u00f1os m\u00e1s peque\u00f1os en bastante de ellas.","b2cdc06f":"Se observa que tener un buen ajuste sin se\u00f1ales de sobreajuste.","68d748bc":"Se ha comprobado que el tama\u00f1o de todas las im\u00e1genes se ha establecido correctamente como 115x115.","0eb08bab":"Se comienza por preparar las im\u00e1genes a 86x86:","e643f00e":"Ejemplo predicci\u00f3n con imagen externa:","4ab10e93":"Estos resultados y comparaci\u00f3n de estad\u00edsticos descriptivos, nos llev\u00f3 a la conclusi\u00f3n de tomar un tama\u00f1o de imagen cercano a la media\/mediana de ambas clases (WithMask \/ WithoutMask), siendo este 86x86 p\u00edxeles, que est\u00e1 en la media de las medianas de las 2 clases y teniendo en cuenta la carga computacional al no ser muy grande. \n\nPor otra parte, teniendo en cuenta la informaci\u00f3n a\u00f1adida por el autor de las im\u00e1genes mediante data Augmentation, tambi\u00e9n se cree importante considerar esta informaci\u00f3n a la hora de elegir el tama\u00f1o. Por lo que se decide tomar el tama\u00f1o de la mediana (115x115 p\u00edxeles).\n\nFinalmente, tambi\u00e9n se considera tama\u00f1o 128x128 ya que este valor proporciona mayor facilidad en la divisi\u00f3n por capas en los modelos, as\u00ed como evitar la p\u00e9rdida de informaci\u00f3n en estos pasos al ser una potencia de dos. As\u00ed podr\u00e1 verse si el desempe\u00f1o var\u00eda mucho respecto a los otros tama\u00f1os y controlar que no haya anomal\u00edas. Se escoge este tama\u00f1o para ello debido a que es cercano al valor de las medias, y no es tan grande como 224x224 que causar\u00eda un gran impacto computacional.","caa8a955":"Teniendo en cuenta los diferentes tama\u00f1os de im\u00e1genes encontrados en el dataset, es necesario tomar una decisi\u00f3n sobre un tama\u00f1o uniforme de pixeles para indicar a los modelos que se vayan a emplear en este estudio.\nPara ello, primero se proceder\u00e1 a sopesar opciones mediante estad\u00edsticos descriptivos.","459e5cad":"Se realizan las predicciones para el conjunto de prueba:","c96d390b":"Total de im\u00e1genes en todo el dataset:","914fdb5c":"------------------------------------------------------------","a3d2e055":"Se definen los directorios correspondientes a una copia del dataset original donde se modificar\u00e1 el tama\u00f1o de las im\u00e1genes. Se copia el contenido en otra carpeta para no perder los im\u00e1genes con sus tama\u00f1os originales.","bc2cf23f":"-------------------------------------------------------------------------------------------------------","f8536cc5":"Una idea sobre los resultados obtenidos en total, es que las im\u00e1genes transformadas, es decir, aquellas im\u00e1genes que han sido sometidas a la t\u00e9cnica *data Augmentation*, tiene tama\u00f1o 224x224 lo que hace que la media y mediana se vean afectadas con respecto a los diferentes tama\u00f1os de las im\u00e1genes que supuestamente no est\u00e1n bajo tratamiento, al menos por esta t\u00e9cnica.\nEsto nos hizo plantearnos la b\u00fasqueda de la media y la mediana de las im\u00e1genes originales para ver cu\u00e1nto difer\u00eda.","ff8dc25c":"Cantidad im\u00e1genes en *train* sin incluir las im\u00e1genes trasnformadas para balancear los datos:","2b86b018":"**1. An\u00e1lisis de los datos**","c0e65672":"Se define el modelo y se congelan las capas preentrenadas:","7ff15149":"Gr\u00e1fico de cajas y bigotes del tama\u00f1o de im\u00e1genes originales m\u00e1s las transformadas que incluye el dataset de Kaggle:","94d80001":"Gr\u00e1fico precisi\u00f3n:","09ac5074":"Media y mediana del tama\u00f1o de im\u00e1genes originales m\u00e1s las transformadas que incluye el dataset de Kaggle:","e4f2d422":"Se entrena el modelo:","bd970a32":"Gr\u00e1fico p\u00e9rdida:","f33c38a7":"A continuaci\u00f3n se preparan las im\u00e1genes a 115x115:","fbefc47c":"Se ha comprobado que el tama\u00f1o de todas las im\u00e1genes se ha establecido correctamente como 86x86."}}