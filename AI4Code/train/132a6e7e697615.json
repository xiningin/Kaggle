{"cell_type":{"9c15a699":"code","9331e200":"code","a73d027a":"code","a1f4ec4f":"code","3db546e5":"code","f144e1bd":"code","40481a53":"code","998f31c6":"code","eb87c672":"code","a4613377":"code","a1d0569a":"code","91ef7415":"code","3cbdd718":"code","5fde31ee":"code","9259a7bc":"code","b570c78f":"code","2b628cac":"code","54ed0392":"code","13b77631":"code","d75babda":"code","84426769":"code","d0b32d27":"code","34c11c59":"code","62bd7228":"code","b826e054":"code","db1ddb41":"code","dd114b09":"code","2828aba8":"code","4f607e7e":"code","c04bba0e":"code","ab2cb498":"code","0b725ce9":"code","bda29b51":"code","942ff852":"code","cf412c5c":"code","66bca07a":"code","a5320e0e":"code","32a7d73a":"code","54fc292e":"code","f309bfd9":"code","331711e4":"code","9385e9c9":"markdown","7b2824b0":"markdown","90ec0085":"markdown","47d9ac11":"markdown","a91ad76c":"markdown","aab643cf":"markdown","77b15726":"markdown","2e6f3fca":"markdown","654fdc7d":"markdown","a49887cd":"markdown","9a8ea4b4":"markdown","e64882be":"markdown","6ba44f6e":"markdown","292f1f0c":"markdown","c7fe3e90":"markdown","22e805ca":"markdown","cfc3cc4b":"markdown","81e69682":"markdown","02288edc":"markdown","c3f073e9":"markdown"},"source":{"9c15a699":"#Imports \nimport lightgbm as lgb\nimport catboost as cat\nimport pandas as pd\nimport xgboost as xgb\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nimport os\nprint(os.listdir(\"..\/input\"))\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n\n\nrandom_seed = 2018","9331e200":"train = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')\nsub = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\nprint(train.columns)\nprint(test.columns)\n","a73d027a":"print(test.shape)\nprint(train.shape)","a1f4ec4f":"print(\"Number of NA Train for the variable budget : \\n\" + str(train[train.budget == 0].shape[0]) + \"\\n\")\nprint(\"Number of NA Test for the variable budget : \\n\" + str(test[test.budget == 0].shape[0]))","3db546e5":"train.loc[train['id'] == 16,'revenue'] = 192864         \ntrain.loc[train['id'] == 90,'budget'] = 30000000                  \ntrain.loc[train['id'] == 118,'budget'] = 60000000       \ntrain.loc[train['id'] == 149,'budget'] = 18000000       \ntrain.loc[train['id'] == 313,'revenue'] = 12000000       \ntrain.loc[train['id'] == 451,'revenue'] = 12000000      \ntrain.loc[train['id'] == 464,'budget'] = 20000000       \ntrain.loc[train['id'] == 470,'budget'] = 13000000       \ntrain.loc[train['id'] == 513,'budget'] = 930000         \ntrain.loc[train['id'] == 797,'budget'] = 8000000        \ntrain.loc[train['id'] == 819,'budget'] = 90000000       \ntrain.loc[train['id'] == 850,'budget'] = 90000000       \ntrain.loc[train['id'] == 1007,'budget'] = 2              \ntrain.loc[train['id'] == 1112,'budget'] = 7500000       \ntrain.loc[train['id'] == 1131,'budget'] = 4300000        \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       \ntrain.loc[train['id'] == 1542,'budget'] = 1             \ntrain.loc[train['id'] == 1570,'budget'] = 15800000       \ntrain.loc[train['id'] == 1571,'budget'] = 4000000        \ntrain.loc[train['id'] == 1714,'budget'] = 46000000       \ntrain.loc[train['id'] == 1721,'budget'] = 17500000       \ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      \ntrain.loc[train['id'] == 1885,'budget'] = 12             \ntrain.loc[train['id'] == 2091,'budget'] = 10             \ntrain.loc[train['id'] == 2268,'budget'] = 17500000       \ntrain.loc[train['id'] == 2491,'budget'] = 6              \ntrain.loc[train['id'] == 2602,'budget'] = 31000000       \ntrain.loc[train['id'] == 2612,'budget'] = 15000000       \ntrain.loc[train['id'] == 2696,'budget'] = 10000000      \ntrain.loc[train['id'] == 2801,'budget'] = 10000000       \ntrain.loc[train['id'] == 335,'budget'] = 2 \ntrain.loc[train['id'] == 348,'budget'] = 12\ntrain.loc[train['id'] == 470,'budget'] = 13000000 \ntrain.loc[train['id'] == 513,'budget'] = 1100000\ntrain.loc[train['id'] == 640,'budget'] = 6 \ntrain.loc[train['id'] == 696,'budget'] = 1\ntrain.loc[train['id'] == 797,'budget'] = 8000000 \ntrain.loc[train['id'] == 850,'budget'] = 1500000\ntrain.loc[train['id'] == 1199,'budget'] = 5 \ntrain.loc[train['id'] == 1282,'budget'] = 9              \ntrain.loc[train['id'] == 1347,'budget'] = 1\ntrain.loc[train['id'] == 1755,'budget'] = 2\ntrain.loc[train['id'] == 1801,'budget'] = 5\ntrain.loc[train['id'] == 1918,'budget'] = 592 \ntrain.loc[train['id'] == 2033,'budget'] = 4\ntrain.loc[train['id'] == 2118,'budget'] = 344 \ntrain.loc[train['id'] == 2252,'budget'] = 130\ntrain.loc[train['id'] == 2256,'budget'] = 1 \ntrain.loc[train['id'] == 2696,'budget'] = 10000000","f144e1bd":"test.loc[test['id'] == 3033,'budget'] = 250 \ntest.loc[test['id'] == 3051,'budget'] = 50\ntest.loc[test['id'] == 3084,'budget'] = 337\ntest.loc[test['id'] == 3224,'budget'] = 4  \ntest.loc[test['id'] == 3594,'budget'] = 25  \ntest.loc[test['id'] == 3619,'budget'] = 500  \ntest.loc[test['id'] == 3831,'budget'] = 3  \ntest.loc[test['id'] == 3935,'budget'] = 500  \ntest.loc[test['id'] == 4049,'budget'] = 995946 \ntest.loc[test['id'] == 4424,'budget'] = 3  \ntest.loc[test['id'] == 4460,'budget'] = 8  \ntest.loc[test['id'] == 4555,'budget'] = 1200000 \ntest.loc[test['id'] == 4624,'budget'] = 30 \ntest.loc[test['id'] == 4645,'budget'] = 500 \ntest.loc[test['id'] == 4709,'budget'] = 450 \ntest.loc[test['id'] == 4839,'budget'] = 7\ntest.loc[test['id'] == 3125,'budget'] = 25 \ntest.loc[test['id'] == 3142,'budget'] = 1\ntest.loc[test['id'] == 3201,'budget'] = 450\ntest.loc[test['id'] == 3222,'budget'] = 6\ntest.loc[test['id'] == 3545,'budget'] = 38\ntest.loc[test['id'] == 3670,'budget'] = 18\ntest.loc[test['id'] == 3792,'budget'] = 19\ntest.loc[test['id'] == 3881,'budget'] = 7\ntest.loc[test['id'] == 3969,'budget'] = 400\ntest.loc[test['id'] == 4196,'budget'] = 6\ntest.loc[test['id'] == 4221,'budget'] = 11\ntest.loc[test['id'] == 4222,'budget'] = 500\ntest.loc[test['id'] == 4285,'budget'] = 11\ntest.loc[test['id'] == 4319,'budget'] = 1\ntest.loc[test['id'] == 4639,'budget'] = 10\ntest.loc[test['id'] == 4719,'budget'] = 45\ntest.loc[test['id'] == 4822,'budget'] = 22\ntest.loc[test['id'] == 4829,'budget'] = 20\ntest.loc[test['id'] == 4969,'budget'] = 20\ntest.loc[test['id'] == 5021,'budget'] = 40 \ntest.loc[test['id'] == 5035,'budget'] = 1 \ntest.loc[test['id'] == 5063,'budget'] = 14 \ntest.loc[test['id'] == 5119,'budget'] = 2 \ntest.loc[test['id'] == 5214,'budget'] = 30 \ntest.loc[test['id'] == 5221,'budget'] = 50 \ntest.loc[test['id'] == 4903,'budget'] = 15\ntest.loc[test['id'] == 4983,'budget'] = 3\ntest.loc[test['id'] == 5102,'budget'] = 28\ntest.loc[test['id'] == 5217,'budget'] = 75\ntest.loc[test['id'] == 5224,'budget'] = 3 \ntest.loc[test['id'] == 5469,'budget'] = 20 \ntest.loc[test['id'] == 5840,'budget'] = 1 \ntest.loc[test['id'] == 5960,'budget'] = 30\ntest.loc[test['id'] == 6506,'budget'] = 11 \ntest.loc[test['id'] == 6553,'budget'] = 280\ntest.loc[test['id'] == 6561,'budget'] = 7\ntest.loc[test['id'] == 6582,'budget'] = 218\ntest.loc[test['id'] == 6638,'budget'] = 5\ntest.loc[test['id'] == 6749,'budget'] = 8 \ntest.loc[test['id'] == 6759,'budget'] = 50 \ntest.loc[test['id'] == 6856,'budget'] = 10\ntest.loc[test['id'] == 6858,'budget'] =  100\ntest.loc[test['id'] == 6876,'budget'] =  250\ntest.loc[test['id'] == 6972,'budget'] = 1\ntest.loc[test['id'] == 7079,'budget'] = 8000000\ntest.loc[test['id'] == 7150,'budget'] = 118\ntest.loc[test['id'] == 6506,'budget'] = 118\ntest.loc[test['id'] == 7225,'budget'] = 6\ntest.loc[test['id'] == 7231,'budget'] = 85\ntest.loc[test['id'] == 5222,'budget'] = 5\ntest.loc[test['id'] == 5322,'budget'] = 90\ntest.loc[test['id'] == 5350,'budget'] = 70\ntest.loc[test['id'] == 5378,'budget'] = 10\ntest.loc[test['id'] == 5545,'budget'] = 80\ntest.loc[test['id'] == 5810,'budget'] = 8\ntest.loc[test['id'] == 5926,'budget'] = 300\ntest.loc[test['id'] == 5927,'budget'] = 4\ntest.loc[test['id'] == 5986,'budget'] = 1\ntest.loc[test['id'] == 6053,'budget'] = 20\ntest.loc[test['id'] == 6104,'budget'] = 1\ntest.loc[test['id'] == 6130,'budget'] = 30\ntest.loc[test['id'] == 6301,'budget'] = 150\ntest.loc[test['id'] == 6276,'budget'] = 100\ntest.loc[test['id'] == 6473,'budget'] = 100\ntest.loc[test['id'] == 6842,'budget'] = 30","40481a53":"#Quick peak into NA columns\n\nfig = plt.figure(figsize=(15, 8))\ntrain.isna().sum().sort_values(ascending=True).plot(kind='barh',colors='red', fontsize=20)","998f31c6":"release_dates = pd.read_csv('..\/input\/release-dates\/release_dates_per_country.csv')\nrelease_dates['id'] = range(1,7399)\nrelease_dates.drop(['original_title','title'],axis = 1,inplace = True)\nrelease_dates.index = release_dates['id']\ntrain = pd.merge(train, release_dates, how='left', on=['id'])\ntest = pd.merge(test, release_dates, how='left', on=['id'])","eb87c672":"trainAdditionalFeatures = pd.read_csv('..\/input\/additionnal-features\/TrainAdditionalFeatures.csv')[['imdb_id','popularity2','rating','totalVotes']]\ntestAdditionalFeatures = pd.read_csv('..\/input\/additionnal-features\/TestAdditionalFeatures.csv')[['imdb_id','popularity2','rating','totalVotes']]\n\ntrain = pd.merge(train, trainAdditionalFeatures, how='left', on=['imdb_id'])\ntest = pd.merge(test, testAdditionalFeatures, how='left', on=['imdb_id'])","a4613377":"corr_mat = train.corr()\ncorr_mat.revenue.sort_values(ascending=False)","a1d0569a":"train['revenue'].describe()","91ef7415":"train.head()\n","3cbdd718":"sns.distplot(train['revenue'])","5fde31ee":"max_re= train[train['revenue']== max(train['revenue'])]\nmax_re.head()","9259a7bc":"Train = train.copy()\nTrain.sort_values('revenue',ascending=False,inplace=True)\nTrain =Train.head(20)\nTrain[['title','popularity','budget','genres','revenue','release_date','production_companies']]","b570c78f":"min_re = train[train['revenue']== min(train['revenue'])]\nmin_re.sample()","2b628cac":"cols =['revenue','budget','popularity','theatrical','runtime','release_year']\nsns.heatmap(train[cols].corr())\nplt.show()","54ed0392":"sns.set()\nx = np.array(train[\"budget\"])\ny = np.array(train[\"revenue\"])\nfig = plt.figure(1, figsize=(10, 8))\n#plt.plot([0,400000000],[0,1300000000],c=\"green\")\nsns.regplot(x, y)\nplt.xlabel(\"budget\", fontsize=10)  \nplt.ylabel(\"revenue\", fontsize=10)\nplt.title(\"Link between revenue and budget\", fontsize=10)","13b77631":"sns.set()\nx=train['revenue']\ny=train['popularity']\nplt.figure(figsize=(15,8))\nsns.regplot(x,y)\nplt.xlabel('popularity')\nplt.ylabel('revenue')\nplt.title('Relationship between popularity and revenue of a movie')","d75babda":"fig, ax = plt.subplots(1,2,figsize=(12,10))\nplt.subplot(1,2,1)\nnp.random.seed(123)\nax = plt.subplot(projection='3d')\nsize = len(train)\ncolors= np.random.rand(size)\nxs = np.array(train['totalVotes'])\nys = np.array(train['budget'])\nzs = np.array(train['revenue'])\nax.scatter(xs,ys, zs,c=colors ,marker='o')\nax.set_xlabel('Votes count')\nax.set_ylabel('Budget')\nax.set_zlabel('Revenue')\n\nplt.show()","84426769":"plt.figure(figsize=(15,12)) \na1 = sns.swarmplot(x='original_language', y='revenue', \n                   data=train[(train['original_language'].isin((train['original_language'].value_counts()[:10].index.values)))])\na1.set_title(\"Plot revenue by original language in the movie\", fontsize=20) \na1.set_xticklabels(a1.get_xticklabels(),rotation=45) \na1.set_xlabel('Original language', fontsize=18)\na1.set_ylabel('revenue', fontsize=18) \n\nplt.show()","d0b32d27":"plt.figure(figsize=(15,8))\nsns.countplot(train.release_year)\nplt.xticks(rotation=90)\nplt.xlabel('Years')\nplt.title('Amount of movies launched every year')","34c11c59":"def prepare(df):\n    global json_cols\n    global train_dict\n    #Here we format the Date ex = 21\/10\/18 <=> to a column with the month , one with the day and one with year\n\n    df[['release_month','release_day','release_year']]=df['release_date'].str.split('\/',expand=True).replace(np.nan, 0).astype(int)\n    df['release_year'] = df['release_year']\n     #in this case we're interested with the year one ,in the format before we will have values like this : 14,15,17,18,19\n    #to a better lecture we add a 2000 to values < 18 we're in 2019 right but the data doesnt contains 2019 movies\n    #same thing for values more than 18 so they are in 19's range so we add 1900\n    df.loc[ (df['release_year'] <= 18) & (df['release_year'] < 100), \"release_year\"] += 2000\n    df.loc[ (df['release_year'] > 18)  & (df['release_year'] < 100), \"release_year\"] += 1900\n    \n    rating_na = df.groupby([\"release_year\",\"original_language\"])['rating'].mean().reset_index()\n    df[df.rating.isna()]['rating'] = df.merge(rating_na, how = 'left' ,on = [\"release_year\",\"original_language\"])\n    \n    vote_count_na = df.groupby([\"release_year\",\"original_language\"])['totalVotes'].mean().reset_index()\n    df[df.totalVotes.isna()]['totalVotes'] = df.merge(vote_count_na, how = 'left' ,on = [\"release_year\",\"original_language\"])\n    \n    \n\n    df['budget'] = np.log1p(df['budget'])\n    \n    df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    df['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    df['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n\n    \n    df['_collection_name'] = df['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\n    le = LabelEncoder()\n    le.fit(list(df['_collection_name'].fillna('')))\n    df['_collection_name'] = le.transform(df['_collection_name'].fillna('').astype(str))\n    df['_num_Keywords'] = df['Keywords'].apply(lambda x: len(x) if x != {} else 0)\n    df['_num_cast'] = df['cast'].apply(lambda x: len(x) if x != {} else 0)\n    \n    releaseDate = pd.to_datetime(df['release_date']) \n    df['release_dayofweek'] = releaseDate.dt.dayofweek \n    df['release_quarter'] = releaseDate.dt.quarter     \n\n    df['_budget_runtime_ratio'] = df['budget']\/df['runtime'] \n    df['_budget_popularity_ratio'] = df['budget']\/df['popularity']\n    df['_budget_year_ratio'] = df['budget']\/(df['release_year']*df['release_year'])\n    df['_releaseYear_popularity_ratio'] = df['release_year']\/df['popularity']\n    df['_releaseYear_popularity_ratio2'] = df['popularity']\/df['release_year']\n\n\n    df['meanruntimeByYear'] = df.groupby(\"release_year\")[\"runtime\"].aggregate('mean')\n    df['meanPopularityByYear'] = df.groupby(\"release_year\")[\"popularity\"].aggregate('mean')\n    df['meanBudgetByYear'] = df.groupby(\"release_year\")[\"budget\"].aggregate('mean')\n    df['_popularity_theatrical_ratio'] = df['theatrical']\/df['popularity']\n    df['_budget_theatrical_ratio'] = df['budget']\/df['theatrical']\n    df['_popularity_totalVotes_ratio'] = df['totalVotes']\/df['popularity']\n    df['_totalVotes_releaseYear_ratio'] = df['totalVotes']\/df['release_year']\n    df['_budget_totalVotes_ratio'] = df['budget']\/df['totalVotes']\n    \n    \n    df['_rating_popularity_ratio'] = df['rating']\/df['popularity']\n    df['_rating_totalVotes_ratio'] = df['totalVotes']\/df['rating']\n    df['_budget_rating_ratio'] = df['budget']\/df['rating']\n    df['_runtime_rating_ratio'] = df['runtime']\/df['rating']\n    \n    \n    df['has_homepage'] = 0\n    df.loc[pd.isnull(df['homepage']) ,\"has_homepage\"] = 1\n    \n    df['isbelongs_to_collectionNA'] = 0\n    df.loc[pd.isnull(df['belongs_to_collection']) ,\"isbelongs_to_collectionNA\"] = 1\n    \n    df['isTaglineNA'] = 0\n    df.loc[df['tagline'] == 0 ,\"isTaglineNA\"] = 1 \n\n    df['isOriginalLanguageEng'] = 0 \n    df.loc[ df['original_language'] == \"en\" ,\"isOriginalLanguageEng\"] = 1\n    \n    df['isTitleDifferent'] = 1\n    df.loc[ df['original_title'] == df['title'] ,\"isTitleDifferent\"] = 0 \n\n    df['isMovieReleased'] = 1\n    df.loc[ df['status'] != \"Released\" ,\"isMovieReleased\"] = 0 \n\n    # get collection id\n    df['collection_id'] = df['belongs_to_collection'].apply(lambda x : np.nan if len(x)==0 else x[0]['id'])\n    \n    df['original_title_letter_count'] = df['original_title'].str.len() \n    df['original_title_word_count'] = df['original_title'].str.split().str.len() \n\n\n    df['title_word_count'] = df['title'].str.split().str.len()\n    df['overview_word_count'] = df['overview'].str.split().str.len()\n    df['tagline_word_count'] = df['tagline'].str.split().str.len()\n    \n    df['production_countries_count'] = df['production_countries'].apply(lambda x : len(x))\n    df['production_companies_count'] = df['production_companies'].apply(lambda x : len(x))\n    df['cast_count'] = df['cast'].apply(lambda x : len(x))\n    df['crew_count'] = df['crew'].apply(lambda x : len(x))\n\n    \n    \n\n    for col in ['genres', 'production_countries', 'spoken_languages', 'production_companies'] :\n        df[col] = df[col].map(lambda x: sorted(list(set([n if n in train_dict[col] else col+'_etc' for n in [d['name'] for d in x]])))).map(lambda x: ','.join(map(str, x)))\n        temp = df[col].str.get_dummies(sep=',')\n        df = pd.concat([df, temp], axis=1, sort=False)\n    df.drop(['genres_etc'], axis = 1, inplace = True)\n    \n    df = df.drop(['belongs_to_collection','genres','homepage','imdb_id','overview','runtime'\n    ,'poster_path','production_companies','production_countries','release_date','spoken_languages'\n    ,'status','title','Keywords','cast','crew','original_language','original_title','tagline', 'collection_id','movie_id'\n    ],axis=1)\n    \n    df.fillna(value=0.0, inplace = True) \n\n    return df","62bd7228":"test['revenue'] = np.nan\n\njson_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef get_dictionary(s):\n    try:\n        d = eval(s)\n    except:\n        d = {}\n    return d\n\nfor col in tqdm(json_cols + ['belongs_to_collection']) :\n    train[col] = train[col].apply(lambda x : get_dictionary(x))\n    test[col] = test[col].apply(lambda x : get_dictionary(x))\n\nprint(train.shape)\ntrain.head()","b826e054":"# parse json data and build category dictionary\ndef get_json_dict(df) :\n    global json_cols\n    result = dict()\n    for e_col in json_cols :\n        d = dict()\n        rows = df[e_col].values\n        for row in rows :\n            if row is None : continue\n            for i in row :\n                if i['name'] not in d :\n                    d[i['name']] = 0\n                d[i['name']] += 1\n        result[e_col] = d\n    return result\n\ntrain_dict = get_json_dict(train)\ntest_dict = get_json_dict(test)\n\n# remove cateogry with bias and low frequency\nfor col in json_cols :\n    \n    remove = []\n    train_id = set(list(train_dict[col].keys()))\n    test_id = set(list(test_dict[col].keys()))   \n    \n    remove += list(train_id - test_id) + list(test_id - train_id)\n    for i in train_id.union(test_id) - set(remove) :\n        if train_dict[col][i] < 10 or i == '' :\n            remove += [i]\n            \n    for i in remove :\n        if i in train_dict[col] :\n            del train_dict[col][i]\n        if i in test_dict[col] :\n            del test_dict[col][i]\n            \n    print(col, 'size :', len(train_id.union(test_id)), '->', len(train_dict[col]))","db1ddb41":"print(test.columns)\nprint(train.columns)","dd114b09":"# prepare data\nall_data = prepare(pd.concat([train, test]).reset_index(drop = True))\ntrain = all_data.loc[:train.shape[0] - 1,:]\ntest = all_data.loc[train.shape[0]:,:]                           \nprint(train.shape)\ntrain.head()","2828aba8":"features = list(train.columns)\nfeatures =  [i for i in features if i != 'id' and i != 'revenue']","4f607e7e":"def score(data, y):\n    validation_res = pd.DataFrame(\n    {\"id\": data[\"id\"].values,\n     \"transactionrevenue\": data[\"revenue\"].values,\n     \"predictedrevenue\": np.expm1(y)})\n\n    validation_res = validation_res.groupby(\"id\")[\"transactionrevenue\", \"predictedrevenue\"].sum().reset_index()\n    return np.sqrt(mean_squared_error(np.log1p(validation_res[\"transactionrevenue\"].values), \n                                     np.log1p(validation_res[\"predictedrevenue\"].values)))","c04bba0e":"class KFoldValidation():\n    def __init__(self, data, n_splits=5):\n        unique_vis = np.array(sorted(data['id'].astype(str).unique()))\n        folds = GroupKFold(n_splits)\n        ids = np.arange(data.shape[0])\n        \n        self.fold_ids = []\n        for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n            self.fold_ids.append([\n                    ids[data['id'].astype(str).isin(unique_vis[trn_vis])],\n                    ids[data['id'].astype(str).isin(unique_vis[val_vis])]\n                ])\n            \n    def validate(self, train, test, features, model, name=\"\", prepare_stacking=False, \n                 fit_params={\"early_stopping_rounds\": 500, \"verbose\": 100, \"eval_metric\": \"rmse\"}):\n        model.FI = pd.DataFrame(index=features)\n        full_score = 0\n        \n        if prepare_stacking:\n            test[name] = 0\n            train[name] = np.NaN\n        \n        for fold_id, (trn, val) in enumerate(self.fold_ids):\n            devel = train[features].iloc[trn]\n            y_devel = np.log1p(train[\"revenue\"].iloc[trn])\n            valid = train[features].iloc[val]\n            y_valid = np.log1p(train[\"revenue\"].iloc[val])\n                       \n            print(\"Fold \", fold_id, \":\")\n            model.fit(devel, y_devel, eval_set=[(valid, y_valid)], **fit_params)\n            \n            if len(model.feature_importances_) == len(features):  \n                model.FI['fold' + str(fold_id)] = model.feature_importances_ \/ model.feature_importances_.sum()\n\n            predictions = model.predict(valid)\n            predictions[predictions < 0] = 0\n            print(\"Fold \", fold_id, \" error: \", mean_squared_error(y_valid, predictions)**0.5)\n            \n            fold_score = score(train.iloc[val], predictions)\n            full_score += fold_score \/ len(self.fold_ids)\n            print(\"Fold \", fold_id, \" score: \", fold_score)\n            if prepare_stacking:\n                train[name].iloc[val] = predictions\n                \n                test_predictions = model.predict(test[features])\n                test_predictions[test_predictions < 0] = 0\n                test[name] += test_predictions \/ len(self.fold_ids)\n                \n        print(\"Final score: \", full_score)\n        return full_score\n","ab2cb498":"Kfolder = KFoldValidation(train)","0b725ce9":"lgbmodel = lgb.LGBMRegressor(n_estimators=10000, \n                             objective='regression', \n                             metric='rmse',\n                             max_depth = 5,\n                             num_leaves=30, \n                             min_child_samples=100,\n                             learning_rate=0.01,\n                             boosting = 'gbdt',\n                             min_data_in_leaf= 10,\n                             feature_fraction = 0.9,\n                             bagging_freq = 1,\n                             bagging_fraction = 0.9,\n                             importance_type='gain',\n                             lambda_l1 = 0.2,\n                             bagging_seed=random_seed, \n                             subsample=.8, \n                             colsample_bytree=.9,\n                             use_best_model=True)","bda29b51":"Kfolder.validate(train, test, features , lgbmodel, name=\"lgbfinal\", prepare_stacking=True)","942ff852":"lgbmodel.FI.mean(axis=1).sort_values()[180:250].plot(kind=\"barh\",title = \"Features Importance\", figsize = (10,10))","cf412c5c":"test['revenue'] =  np.expm1(test[\"lgbfinal\"])\ntest[['id','revenue']].to_csv('submission_lgb.csv', index=False)\ntest[['id','revenue']].head()","66bca07a":"xgbmodel = xgb.XGBRegressor(max_depth=5, \n                            learning_rate=0.01, \n                            n_estimators=10000, \n                            objective='reg:linear', \n                            gamma=1.45, \n                            seed=random_seed, \n                            silent=True,\n                            subsample=0.8, \n                            colsample_bytree=0.7, \n                            colsample_bylevel=0.5)\n\n","a5320e0e":"Kfolder.validate(train, test, features, xgbmodel, name=\"xgbfinal\", prepare_stacking=True)","32a7d73a":"catmodel = cat.CatBoostRegressor(iterations=10000, \n                                 learning_rate=0.01, \n                                 depth=5, \n                                 eval_metric='RMSE',\n                                 colsample_bylevel=0.8,\n                                 bagging_temperature = 0.2,\n                                 metric_period = None,\n                                 early_stopping_rounds=200,\n                                 random_seed=random_seed)\n\n","54fc292e":"Kfolder.validate(train, test, features , catmodel, name=\"catfinal\", prepare_stacking=True,\n               fit_params={\"use_best_model\": True, \"verbose\": 100})","f309bfd9":"train['Revenue_Dragon1'] = 0.4 * train[\"lgbfinal\"] + \\\n                               0.2 * train[\"xgbfinal\"] + \\\n                               0.4 * train[\"catfinal\"]","331711e4":"test['revenue'] =  np.expm1(0.4* test[\"lgbfinal\"]+ 0.4 * test[\"catfinal\"] + 0.2 * test[\"xgbfinal\"])\ntest[['id','revenue']].to_csv('submission_Dragon1.csv', index=False)\ntest[['id','revenue']].head()","9385e9c9":"Guess i'm right , a big budget can guarantee a good revenue.","7b2824b0":"What about the relation between popularity and revenue ","90ec0085":"**AYO ! Can we predict the revenue of a movie ?!**","47d9ac11":"Well we see clearly that Revenue depends on the popularity and the budget !","a91ad76c":"MODEls testing NVM\n","aab643cf":"Let's take a peak on the top 20 movies !","77b15726":"**General Idea about the DataSet**\nIn this kernel i'm working with TMDB Box Office DataSet which provides 7398 movies and a variety of metadata obtained from The Movie Database (TMDB). Movies are labeled with id. Data points include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries.","2e6f3fca":"![img](https:\/\/image.noelshack.com\/fichiers\/2019\/17\/2\/1556027084-2018-movies-wallpaper-by-the-dark-mamba-995-dc28v6u.jpg)","654fdc7d":"Working on progress dont forget to upvote if you liked the kernel :p","a49887cd":"Well seem's like The avengers got quite a great success ! Waiting for the END GAME , by the way ","9a8ea4b4":"What about the movie with the lowest revenue ?","e64882be":"Ohooo , lets investigate and look closer into those two datasets\n","6ba44f6e":"My sixth sense smells big correlation between the revenue and the initial budget lets check it closely ","292f1f0c":"Additional features from Kamal Chhirang which contains more informations about movies (Votes , popularity)","c7fe3e90":"**External Data (release dates , popularity and votes )**\n\nContains releases dates for every country.","22e805ca":"**I'll first trying to understand data and not to rush up into the plots since it's my first knowledge competition.**\nLet's see how the data looks like , from a closer view !","cfc3cc4b":"* **First things first: analysing 'revenue'**\n'revenue' is the reason of our quest. It's like when we're going to a party. We always have a reason to be there. Usually, women are that reason. (disclaimer: adapt it to dancing, men or alcohol, according to your preferences)\n\nUsing the women approach, let's see the little story about 'How we met 'Revenue'.\n","81e69682":"**We need some cleaning**\ni'll take in this case the cleaning of another KERNEL from https:\/\/www.kaggle.com\/zero92","02288edc":"Lets see which movie got the max revenue shall we ?","c3f073e9":"**MODEL**"}}