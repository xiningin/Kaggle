{"cell_type":{"b3cb25c3":"code","8e351bc7":"code","1478dc33":"code","ca6e9ffe":"code","26eed7d7":"code","d6681331":"code","6aa84dec":"code","e2af72a6":"code","23e7d29a":"code","9994d392":"code","751a596f":"code","8b7c1344":"code","3e5c1faa":"code","cb613920":"code","9f609fdd":"code","b87509dc":"code","29c182ee":"code","8a5154cd":"code","c1606599":"markdown","d1782b65":"markdown"},"source":{"b3cb25c3":"import numpy as np\nfrom numpy.fft import *\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom scipy.stats import multivariate_normal as normal\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport scipy as sp\nimport scipy.fftpack\nfrom scipy import signal\nfrom scipy.signal import butter, sosfiltfilt, freqz, filtfilt\nfrom sklearn import tree\nimport lightgbm as lgb\nimport xgboost as xgb\nimport gc\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nDATA_PATH = \"..\/input\/liverpool-ion-switching\"\n\ntrain_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n#test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n#submission_df = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))","8e351bc7":"class Equation(object):\n    \"\"\"Base class for defining PDE related function.\"\"\"\n\n    def __init__(self, eqn_config):\n        self.dim = eqn_config.dim\n        self.total_time = eqn_config.total_time\n        self.num_time_interval = eqn_config.num_time_interval\n        self.delta_t = self.total_time \/ self.num_time_interval\n        self.sqrt_delta_t = np.sqrt(self.delta_t)\n        self.y_init = None\n\n    def sample(self, num_sample):\n        \"\"\"Sample forward SDE.\"\"\"\n        raise NotImplementedError\n\n    def f_tf(self, t, x, y, z):\n        \"\"\"Generator function in the PDE.\"\"\"\n        raise NotImplementedError\n\n    def g_tf(self, t, x):\n        \"\"\"Terminal condition of the PDE.\"\"\"\n        raise NotImplementedError\n\n\nclass HJBLQ(Equation):\n    \"\"\"HJB equation in PNAS paper doi.org\/10.1073\/pnas.1718942115\"\"\"\n    def __init__(self, eqn_config):\n        super(HJBLQ, self).__init__(eqn_config)\n        self.x_init = np.zeros(self.dim)\n        self.sigma = np.sqrt(2.0)\n        self.lambd = 1.0\n\n    def sample(self, num_sample):\n        dw_sample = normal.rvs(size=[num_sample,\n                                     self.dim,\n                                     self.num_time_interval]) * self.sqrt_delta_t\n        x_sample = np.zeros([num_sample, self.dim, self.num_time_interval + 1])\n        x_sample[:, :, 0] = np.ones([num_sample, self.dim]) * self.x_init\n        for i in range(self.num_time_interval):\n            x_sample[:, :, i + 1] = x_sample[:, :, i] + self.sigma * dw_sample[:, :, i]\n        return dw_sample, x_sample\n\n    def f_tf(self, t, x, y, z):\n        return -self.lambd * tf.reduce_sum(tf.square(z), 1, keepdims=True)\n\n    def g_tf(self, t, x):\n        return tf.math.log((1 + tf.reduce_sum(tf.square(x), 1, keepdims=True)) \/ 2)\n\n","1478dc33":"from itertools import islice\n\ndef window(seq, n=2):\n    \"Sliding window width n from seq.  From old itertools recipes.\"\"\"\n    it = iter(seq)\n    result = tuple(islice(it, n))\n    if len(result) == n:\n        yield result\n    for elem in it:\n        result = result[1:] + (elem,)\n        yield result\n        \npairs = pd.DataFrame(window(train_df.loc[:,'open_channels']), columns=['state1', 'state2'])\ncounts = pairs.groupby('state1')['state2'].value_counts()\nalpha = 1 # Laplacian smoothing is when alpha=1\ncounts = counts.fillna(0)\nprobs = ((counts + alpha )\/(counts.sum()+alpha)).unstack()\n\nprobs","ca6e9ffe":"# reference https:\/\/www.kaggle.com\/friedchips\/on-markov-chains-and-the-competition-data\ndef create_axes_grid(numplots_x, numplots_y, plotsize_x=6, plotsize_y=3):\n    fig, axes = plt.subplots(numplots_y, numplots_x)\n    fig.set_size_inches(plotsize_x * numplots_x, plotsize_y * numplots_y)\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n    return fig, axes","26eed7d7":"\"\"\"\ntrain_df['rel_work'] = train_df['signal']**2 - (train_df['signal']**2).mean()\npairs = pd.DataFrame(window(train_df.loc[:,'rel_work']), columns=['state1', 'state2'])\nmeans = pairs.groupby('state1','state2')['rel_work'].mean()\nalpha = 1 # Laplacian smoothing is when alpha=1\nmeans = means.unstack()\nmeans\n\"\"\"","d6681331":"train_df.loc[:,'oc'] = train_df['open_channels'].shift(1)\ntrain_df.loc[:,'rel_work'] = train_df['signal']**2 - (train_df['signal']**2).mean()\nmeans = train_df.groupby(['open_channels','oc'])['rel_work'].mean()\nmeans = means.unstack()\nmeans\nfig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Markov Transition Matrix P for all of train')\nsns.heatmap(\n    means,\n    annot=True, fmt='.3f', cmap='Blues', cbar=False,\n    ax=axes, vmin=0, vmax=0.5, linewidths=2);","6aa84dec":"train_df","e2af72a6":"print('Occurence Table of State Transitions')\not = counts.unstack().fillna(0)\not\nfig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Occurence Table of State Transitions')\nsns.heatmap(\n    ot,\n    annot=True, fmt='.0f', cmap='Blues', cbar=False,\n    ax=axes, vmin=0, vmax=0.5, linewidths=2);","23e7d29a":"P = (ot)\/(ot.sum())\nCal = - P * np.log(P)\nCal\nfig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Markov Transition Matrix P for all of train')\nsns.heatmap(\n    Cal,\n    annot=True, fmt='.3f', cmap='Blues', cbar=False,\n    ax=axes, vmin=0, vmax=0.5, linewidths=2);","9994d392":"Caliber = Cal.sum().sum()\nCaliber","751a596f":"#reference https:\/\/www.kaggle.com\/teejmahal20\/a-signal-processing-approach-low-pass-filtering\n# some of TJ Klein's code, who helped me make this notebook with his examples\ndef butter_lowpass_filter(data, fc, fs, order=5):\n    normal_cutoff = fc \/ (fs\/2)\n    # Get the filter coefficients \n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    y = filtfilt(b, a, data)\n    sig = pd.DataFrame(y, columns = ['sig_filt'])\n    return sig","8b7c1344":"bs = 500000\nfs = 10000\nsfft = []\nsfft_10 = []\nsfft_20 = []\noutput = []\nws = 11\nwindow = signal.blackmanharris(ws)\nwindow_10 = signal.blackmanharris(ws*5)\nwindow_small = signal.blackmanharris(5)\nbs = int(bs)\n\nfor ii in range(10):  # perform band pass filter\n    i = ii*bs\n    fourier = rfft(train_df.iloc[i:i+bs,1])\n    # filter the power spectrum\n    fourierb = fourier\n    # Apply Blackman-Harris high pass filter. Use first half of window\n    for i in range(5):\n        fourier[i] = fourier[i]*(window[i])\n        fourier[i+int(bs\/4)] = fourier[i+int(bs\/4)]*(window[i])\n    \n    sf = irfft(fourier)\n    sfft = np.append(sfft,sf)\n    \n    # Apply Blackman-Harris notch filter to reduce 50Hz buzz noise\n    #\n    fourier[2500-28:2500+27] = fourier[2500-28:2500+27]*(1-window_10)\n    fourier[2500-28+int(bs\/4):2500+27+int(bs\/4)] = fourier[2500-28+int(bs\/4):2500+27+int(bs\/4)]*(1-window_10)\n     \n    sf_10 = irfft(fourier)\n    sfft_10 = np.append(sfft_10,sf_10)\n    \n    # Apply Blackman-Harris notch filter to cut out sawtooth noise with period of 10 second \n    # the index of 0.1Hz is 5 and cut out odd harmonics of 0.1Hz\n    for harm in range(1,11,2):# Use small Blackman window inverted for notching out harmonics of 0.1Hz\n        fourier[harm*5+i-2:harm*5+i+3] = fourier[harm*5+i-2:harm*5+i+3]*(1-window_small[i])\n        fourier[harm*5+i-2+int(bs\/4):harm*5+i+3+int(bs\/4)] = fourier[harm*5+i-2+int(bs\/4):harm*5+i+3+int(bs\/4)]*(1-window_small[i])\n     \n    sf_20 = irfft(fourier)\n    sfft_20 = np.append(sfft_20,sf_20)\n    \n    \ntrain_df['signal_f'] = 0.\ntrain_df['signal_f'] = sfft\ntrain_df['signal_f10'] = 0.\ntrain_df['signal_f10'] = sfft_10\ntrain_df['signal_f20'] = 0.\ntrain_df['signal_f20'] = sfft_20\n\n\n# Apply a low pass filter with 600Hz cutoff frequency\nfc = 600  # Cut-off frequency of the filter\ntrain_df['signal_f30'] = 0.\noutput = butter_lowpass_filter(train_df.iloc[:,4], fc, fs, 5)\ntrain_df['signal_f30'] = output['sig_filt']\n","3e5c1faa":"train_df.loc[:,'oc'] = train_df['open_channels'].shift(1)\ntrain_df.loc[:,'rel_work_30'] = train_df['signal_f30']**2 - (train_df['signal_f30']**2).shift(-1)\nmeans = train_df.groupby(['open_channels','oc'])['rel_work_30'].mean()\nmeans = means.unstack()\nmeans\nfig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Markov Transition Matrix P for all of train')\nsns.heatmap(\n    means,\n    annot=True, fmt='.3f', cmap='Reds', cbar=False,\n    ax=axes, vmin=0, vmax=0.5, linewidths=2);","cb613920":"\nfig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Markov Transition Matrix P for all of train')\nsns.heatmap(\n    P,\n    annot=True, fmt='.3f', cmap='Blues', cbar=False,\n    ax=axes, vmin=0, vmax=0.5, linewidths=2);","9f609fdd":"eig_values, eig_vectors = np.linalg.eig(np.transpose(P))\nprint(\"Eigenvalues :\", eig_values)","b87509dc":"# reference: http:\/\/kitchingroup.cheme.cmu.edu\/blog\/2013\/02\/03\/Using-Lagrange-multipliers-in-optimization\/\ndef func(X):\n    x = X[0]\n    y = X[1]\n    L = X[2] \n    return x + y + L * (x**2 + k * y)\n\ndef dfunc(X):\n    dL = np.zeros(len(X))\n    d = 1e-3 \n    for i in range(len(X)):\n        dX = np.zeros(len(X))\n        dX[i] = d\n        dL[i] = (func(X+dX)-func(X-dX))\/(2*d);\n    return dL","29c182ee":"from scipy.optimize import fsolve\n\n# this is the max\nX1 = fsolve(dfunc, [1, 1, 0])\nprint(X1, func(X1))\n\n# this is the min\nX2 = fsolve(dfunc, [-1, -1, 0])\nprint(X2, func(X2))","8a5154cd":"# Reference https:\/\/towardsdatascience.com\/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d\ndef macro_double_soft_f1(y, y_hat):\n    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n    Use probability values instead of binary predictions.\n    This version uses the computation of soft-F1 for both positive and negative class for each label.\n    \n    Args:\n        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n        \n    Returns:\n        cost (scalar Tensor): value of the cost function for the batch\n    \"\"\"\n    y = tf.cast(y, tf.float32)\n    y_hat = tf.cast(y_hat, tf.float32)\n    tp = tf.reduce_sum(y_hat * y, axis=0)\n    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n    tn = tf.reduce_sum((1 - y_hat) * (1 - y), axis=0)\n    soft_f1_class1 = 2*tp \/ (2*tp + fn + fp + 1e-16)\n    soft_f1_class0 = 2*tn \/ (2*tn + fn + fp + 1e-16)\n    cost_class1 = 1 - soft_f1_class1 # reduce 1 - soft-f1_class1 in order to increase soft-f1 on class 1\n    cost_class0 = 1 - soft_f1_class0 # reduce 1 - soft-f1_class0 in order to increase soft-f1 on class 0\n    cost = 0.5 * (cost_class1 + cost_class0) # take into account both class 1 and class 0\n    macro_cost = tf.reduce_mean(cost) # average on all labels\n    return macro_cost","c1606599":"**Help requested**\n\nThis problem might be solved using optimal control mathematics. We want to control a detector of the system's output states (how many open channels) so that the f1 score of the detector is maximized. \n\nThe Hamilton Jacobi Bellman equation offers a solution. The solution of the minumized partial derivative of the Hamiltonian is the solution for this problem. It should provide an estimate of \"open_channels\" at the highest possible f1 score. Actually, to frame the problem correctly for the HJB, we will seek to minumize (1-f1_score).\n\n![](https:\/\/www.elmtreegarden.com\/wp-content\/uploads\/2020\/04\/ion-channel-receiver.gif)\n\n**The HJB equation is composed with the following functions**:<br>\n**A**. The state equations relating input x and output y including history (x',y') in t = {0,tf). Train data.<br>\n**B**. Performance function: minumize(1- f1 score) modified to be differentiable<br>\n**C**. Constrains on the system: conservation of energy.<br>\n**D**. Initial conditions\n\n**A.** y(t) is the output of the receiver. y(t) is an integer in the range: 0 <= y <= 10  with 11 possible states.<br>\nBased on the known physics of the problem and previous experiments with features, we can make a second order differencial equation as a model for yt). Since we will improve this with control I will shoot for a F1_score of about 0.90 or better before optimizing a control for the system. Let our model be:\n\n                    y(t) = a0 + R f(t)^2 - R f(t-1)^2 + y(t-1)\n                                \n                    y'(t) = R\/2 f(t) \n                    \n\n**B.** The performance function, **J**(t) , is the f_1 score, f_1(y, y'). In our case we will 'soften' the function, using a function provided by reference 3. The natural form of the HJB is to minumize performance, so we will use the negative of the softened f1 score in the equation. <br>So **J**, the performance measure, is <br>\n\n                    **J**(y,t) = 1 - soft_f1_score(y(t),y'(t))\n\n**C.** Constrains on the system: conservation of energy. The Energy in the signal during transition is {x(t)^2 - x(t-1)^2} dt, where dt is time between samples (0.0001 s). The Energy in the transition is k dy(t)\/dt, where k is a constant.. where k is the energy of each ion transition. \"*Onsager phenomenological coefficients* \". <br>\n\n                    ky'(t) - R f(t)^2 = 0\n                    y'(t) = R\/k f(t)^2\n\n**D.** Initial conditions will be assumed are<br>\n\n                    y(0) = y'(0) = 0\n                    **J**(y(0),t=0) = 0\n\nThe HJB equation is:\n\n0 = **J**(y,t) + **H**{x, u\"{x, **J**(y,t), t}, **J**(y,t), t}<br>\n\nThe **u\"** that solves this partial diffencial equation (PDE) is the optimal control for the History. Now take the partial derivative with respect to x of the HJB equation.\n\nwhere **H** is the Hamiltonion\n\n\nReferences:\n1. Kirk, \"Optimal Control Theory\", 1970 Chapters 1-5<br>\n2. [E.T. Jaynes: Minimum Entropy Principle](https:\/\/pdfs.semanticscholar.org\/b326\/6b25cb2ff34634aff48434652bacb3fede9c.pdf)\n3. [Toward Data Science Blog](https:\/\/towardsdatascience.com\/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d)","d1782b65":"Constrains on our system will define it's behavior. The system is constrained by the law of conservation of Energy. Input is signal x and output is open channels y.\n\nOur signal x is current in pico-amperes (pA or 10^-12 Amps). The current squared x(t)^2 is proportional to instintainious Power is R * x(t)^2, where R is the resistance of the circuit. Transition Energy = TE = R*{x(t)^2-x(t-1)^2}* dt\n\nAssuming that each Ion that transitions through the channel requires energy to make this transition, eT. The energy to make n state transitions is n * eT, where -10 <= n <= 10  so our constraint is:\n\nTE - n * eT = 0   or\n\nR*{x(t)^2-x(t-1)^2} * dt - n * eT = 0\n\nLagrangian analysis seeks to find minimums and maxima for prediction purposes. First find the Lagrangian L such that:\n\nf(x,y) = L g(x,y)\n"}}