{"cell_type":{"5488d0ac":"code","d85671e4":"code","bef37cba":"code","5085199d":"code","d6ef532e":"code","9ae0c9ff":"code","3b02bafe":"code","e185ae03":"code","5360600d":"code","3ac1bc8f":"code","7f2b69d8":"markdown","d8e829fd":"markdown","f72dcd94":"markdown"},"source":{"5488d0ac":"# Dependencies\nfrom time import time\nfrom dateutil import parser\nfrom pandas.tseries.offsets import BDay\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nimport random as rnd\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nnp.random.seed(42)\n\n# Metrics AND FUNCTIONS\n# Standardize the data:\nfrom sklearn.preprocessing import StandardScaler\n\n## CLASSIFIERS LIST\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\n\n# Features\nf = {'firstMentionSentence':['median','std'],\n     'sentimentNeutral':['mean','std'], \n     'noveltyCount12H':['sum'],'noveltyCount24H':['sum'],'noveltyCount3D':['sum'],\n     'noveltyCount5D':['sum'],'noveltyCount7D':['sum'],\n     'relevance':['median'],  \n     'companyCount':['median'], \n     'sentimentNegative':['std'],\n     'sentimentWordCount':['median']}\n        \ndef pre_processing(mkt, nws):\n    \n## CONSOLIDATE TIME TO THE NEXT BUSINESS DAY\n\n    if mkt.time.dtype != 'datetime64[ns, UTC]':\n        mkt.time = mkt.time.apply(lambda x: parser.parse(x))\n        nws.time = nws.time.apply(lambda x: parser.parse(x))\n    nws['time'] = (nws['time'] - np.timedelta64(22,'h')).dt.ceil('1D') #.dt.date \n    mkt['time'] = mkt['time'].dt.floor('1D')\n    # Verify if business day, if not, roll to the next B day\n    offset = BDay()\n    nws.time = nws.time.apply(lambda x: offset.rollforward(x))\n    \n    ## TRIM\n    \n    mkt.drop(['assetName','open','returnsClosePrevRaw1','returnsOpenPrevRaw1',\n    'returnsClosePrevRaw10','returnsOpenPrevRaw10', \n    'returnsClosePrevMktres1'], axis=1, inplace=True)\n\n    nws.drop(['sourceTimestamp', 'firstCreated', 'sourceId', 'headline',\n    'takeSequence', 'provider', 'subjects', 'audiences','bodySize',\n    'headlineTag', 'marketCommentary', 'assetName',\n    'urgency', 'sentenceCount', 'wordCount', 'sentimentClass', 'sentimentPositive', 'volumeCounts12H',\n    'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D',\n    'volumeCounts7D'], axis=1, inplace=True)\n   \n    ## Break down the set of assets (10s)\n    \n    nws = expand_assets(nws)\n\n    ## FEATURE Engineering\n    \n    nws = nws.groupby(['time','assetCode']).agg(f)\n    # Correct the labels\n    col_name = ['_'.join(title) if isinstance(title, tuple) else title for title in nws.columns ]\n    nws.columns = col_name\n    # Regroup the noveltyCount variables into max, min, median and std. \n    current_col = [col for col in filter(lambda x: x.startswith('noveltyCount'), nws.columns)]\n    nws['novelty_median'] = nws[current_col].apply(np.median, axis=1)\n    nws['novelty_std'] = nws[current_col].apply(np.std, axis=1)\n    nws.drop(current_col, axis=1, inplace=True)\n    \n    # MERGING\n    \n    data = pd.merge(mkt, nws,  how='outer', left_on=['time','assetCode'], right_on = ['time','assetCode'])\n    del nws, mkt\n\n    # Set all Nans to 0\n    data = data.loc[(~data.volume.isnull()) & (~data.firstMentionSentence_median.isnull())].fillna(0)\n    return data\n\ndef expand_assets(nws):\n    \n    nws['assetCodes'] = nws['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\")   \n    assetCodes_expanded = list(chain(*nws['assetCodes']))\n    assetCodes_index = nws.index.repeat( nws['assetCodes'].apply(len) )\n    df = pd.DataFrame({'idx': assetCodes_index, 'assetCode': assetCodes_expanded})\n    # Create expandaded news (will repeat every assetCodes' row)\n    nws_expanded = pd.merge(df, nws, left_on='idx', right_index=True)\n    nws_expanded.drop(['idx','assetCodes'], axis=1, inplace=True)\n    return nws_expanded\n\ndef split_dataset(features, data):\n    \n    # Standardize\n    X = StandardScaler().fit_transform(data.loc[:, features].values)\n    y = np.array(data.target.values).reshape(X.shape[0],1)\n    training_size = np.floor(X.shape[0]*0.75).astype(int)\n    X_train = X[:training_size]\n    y_train = y[:training_size]\n    X_test = X[training_size:]\n    y_test = y[training_size:]\n    # Many training algorithms are sensitive to the order of the training instances, \n    # so it's generally good practice to shuffle them first:\n    rnd_idx = np.random.permutation(training_size)\n    X_train = X_train[rnd_idx]\n    y_train = y_train[rnd_idx]\n    return X_train, X_test, y_train, y_test\n\n# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    return df\n\ndef training(X_train,y_train):\n\n    # --- SGD\n    sgd_paste_clf = BaggingClassifier(\n        SGDClassifier(random_state=42,alpha=0.01,l1_ratio=0.25,loss='log',penalty='elasticnet'),\n        n_estimators=50, n_jobs=1, random_state=40)\n    \n    # --- DT\n    dt_paste_clf = BaggingClassifier(\n        DecisionTreeClassifier(random_state=42,max_leaf_nodes=91,min_samples_leaf=7,min_weight_fraction_leaf=0.01),\n        n_estimators=50, n_jobs=1, random_state=40)\n    \n    # --- Gradient Boosting\n    gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=35, random_state=42)\n\n    sgd_paste_clf.fit(X_train, y_train)\n    dt_paste_clf.fit(X_train, y_train)\n    gbrt_best.fit(X_train, y_train)\n\n    return sgd_paste_clf, dt_paste_clf, gbrt_best\n\ndef get_prediction(clf1, clf2, clf3, X_test):\n    yhat = {\n    'SGD': clf1.predict_proba(X_test),\n    'DT': clf2.predict_proba(X_test)} \n    yhat_GB = {'GB':clf3.predict(X_test)}\n\n    # Hard Voting with Soft Voting output\n    my_dict = pd.DataFrame({'clf': list(yhat.keys()),    # score = range(len(tst.valuesof sgd))\n            'tags': list(yhat.values())}, columns = ['clf', 'tags'])\n    tags = my_dict['tags'].apply(pd.DataFrame)\n    df_temp = pd.DataFrame()\n    for i,d in enumerate(yhat.keys()):\n        tags[i].columns = [d+'-1',d+'+1']\n        df_temp = pd.concat([df_temp, tags[i]], axis=1)\n    maxCol=lambda x: max(x.min(), x.max(), key=abs)\n\n    df_temp['SGD-1'] = df_temp['SGD-1'].apply(lambda y: y*(-1))\n    df_temp['SGD_Label'] = df_temp.loc[:,df_temp.columns.str.startswith('SGD')].apply(maxCol,axis=1)\n    df_temp['DT-1'] = df_temp['DT-1'].apply(lambda y: y*(-1))\n    df_temp['DT_Label'] = df_temp.loc[:,df_temp.columns.str.startswith('DT')].apply(maxCol,axis=1)\n\n    df_gb = pd.DataFrame(yhat_GB['GB'], columns=['GB'])\n    df_gb['GB-1'] = df_gb.GB.apply(lambda x: x-1 if x<=50 else x)\n    df_gb.columns = ['GB+1','GB-1']\n    df_gb['GB_Label'] = df_gb.loc[:,df_gb.columns.str.startswith('GB')].apply(maxCol,axis=1)\n\n    df_temp = pd.concat([df_temp,df_gb], axis=1)\n    df_temp['myHV'] = df_temp.loc[:,df_temp.columns.str.endswith('_Label')].apply(lambda l: np.bincount(l>=0).argmax(), axis=1)\n\n    return df_temp.apply(hVote, axis=1)\n\ndef hVote(r):\n    avg = []\n    for l in ['SGD_Label','DT_Label','GB_Label']:\n        if (r.myHV==0 and r[l]<0) or (r.myHV==1 and r[l]>0):\n            avg.append(r[l])\n    return np.mean(avg)\n\ndef make_predictions(predictions_template_df, market_obs_df, news_obs_df):\n    # Preprocessing\n    df = pre_processing(market_obs_df, news_obs_df)\n    df.reset_index(inplace=True)\n    df.drop(['time','index'], axis=1, inplace=True)\n    features = df.columns.difference(['assetCode'])\n        # On the iteration 386 there is so few news that when merged with the market the dataframe has no data. \n    if df.shape[0] > 0:\n        X_test = StandardScaler().fit_transform(df.loc[:, features].values)\n        # Prediction\n#         y_pred = model.predict_proba(X_test)\n#         df['confidenceValue'] = [p if p>=0.5 else 1-p for p in y_pred[:,1]]\n        df['confidenceValue'] = get_prediction(clf1, clf2, clf3, X_test)\n        # Merge prediction with the respective asset code\n        pred = pd.merge(predictions_template_df,df.loc[:,['assetCode','confidenceValue']],on='assetCode', how='outer').loc[:,[\"assetCode\", \"confidenceValue_y\"]].fillna(0)\n    else: \n        pred = predictions_template_df.copy()\n        pred.columns = ['assetCode','confidenceValue_y']\n    predictions_template_df.confidenceValue = pred.confidenceValue_y\n    \ndef get_prediction_2(clf, X_test):\n    return clf.predict(X_test)","d85671e4":"# First let's import the module and create an environment.\nfrom kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()","bef37cba":"# -------- ~ 700 s\nstart = time()\n\nmkt = reduce_mem_usage(market_train_df)\nnws = reduce_mem_usage(news_train_df)\ndel market_train_df, news_train_df\n\nmkt[\"returnsOpenNextMktres10\"] = mkt[\"returnsOpenNextMktres10\"] > 0 # .clip(-1, 1)\nmkt.rename(columns={'returnsOpenNextMktres10':'target'}, inplace=True)\n\ndata = pre_processing(mkt, nws)\n\n# Reset Index\ndata.reset_index(inplace=True)\ndata.drop(['time','assetCode','index','universe'], axis=1, inplace=True)\n\n# RESULTS\nprint('Preprocessing Completed:',time()-start, 'seconds')","5085199d":"data.head()","d6ef532e":"## ---------  ~ 1000s\nstart = time()\n\n## -------- Split dataset\nfeatures = data.columns.difference(['target'])\nX_train, X_test, y_train, y_test = split_dataset(features, data)\n\nclf1, clf2, clf3 = training(X_train, y_train)\n\nprint(time()-start)","9ae0c9ff":"# --- Analytics\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.dummy import DummyClassifier \n\n# Results\nres = pd.DataFrame()\n\n# Results\ndef get_res(res, clf_time, clf_name, X_train, y_test, y_pred, p):\n    return pd.concat([res,\n               pd.DataFrame({'data_size':str(X_train.shape),\n              'ETA': clf_time,\n              'Acc': accuracy_score(y_test, y_pred),\n              'Precision': precision_score(y_test, y_pred),\n              'Recall': recall_score(y_test, y_pred),\n              'F1': f1_score(y_test, y_pred),\n              'MSE': mean_squared_error(y_test, y_pred*1),\n              'AUC': roc_auc_score(y_test, y_pred), \n              'Params':p}, index=[clf_name])])\n\n# Function used in feature importance selection\ndef prep_res(r):\n    # Remove the ETA column as well as the Params and data size\n    r.drop(['ETA','Params','data_size'], inplace=True, axis=1)\n    r = r.stack().reset_index()\n    # Join the two indexes together and convert it to a df\n    # Merge \n    r['metric'] = r.apply(lambda row: row.level_0+' '+row.level_1, axis=1)\n    #res.drop(['level_0','level_1'], inplace=True, axis=1)\n    #res.set_index('metric',inplace=True)\n    r.columns = ['clf','metric','all','mix']\n    return r\n\ndef get_baseline(res, X_train, y_train, X_test, y_test, title):\n    ## BASELINE  \n    # Logistic Regression  95s\n    start = time()\n    log_clf = LogisticRegression(random_state=42)\n    log_clf.fit(X_train, y_train)\n    y_pred = log_clf.predict(X_test)\n    res = get_res(res, time()-start, 'LogReg', X_train, y_test, y_pred, title)\n\n    # SGD\n    start = time()\n    sgd_clf = SGDClassifier(random_state=42)\n    sgd_clf.fit(X_train, y_train)\n    y_pred = sgd_clf.predict(X_test)\n    res = get_res(res, time()-start, 'SGD', X_train, y_test, y_pred, title)\n\n    # Decision Tree\n    start = time()\n    tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n    tree_clf.fit(X_train, y_train)\n    # tree_clf.predict_proba(X_test)\n    tree_clf.predict(X_test)\n    res = get_res(res, time()-start, 'DecTree', X_train, y_test, y_pred, title)\n    return res","3b02bafe":"## --------- Results for Analysis\nstart = time()\nres = get_res(pd.DataFrame(), time()-start, 'SGD', X_train, y_test, clf1.predict(X_test), 'Final')\nstart = time()\nres = get_res(res, time()-start, 'DT', X_train, y_test, clf2.predict(X_test), 'Final')\nstart = time()\nres = get_res(res, time()-start, 'GBoost', X_train, y_test, clf3.predict(X_test)>=0.5, 'Final')\nstart = time()\nres = get_res(res, time()-start, 'HVoting', X_train, y_test, get_prediction(clf1, clf2, clf3, X_test)>0, 'Final')\n\nres.loc[:,res.columns.difference(['Params','data_size'])].head()","e185ae03":"## ------ Train whole dataset\n# ~ 1000 s\n\nX_train = np.vstack((X_train,X_test))\ny_train = np.vstack((y_train, y_test))\n\nclf1, clf2, clf3 = training(X_train, y_train)","5360600d":"def make_predictions_2(predictions_template_df, market_obs_df, news_obs_df):\n    # Preprocessing\n    df = pre_processing(market_obs_df, news_obs_df)\n    df.reset_index(inplace=True)\n    df.drop(['time','index'], axis=1, inplace=True)\n    features = df.columns.difference(['assetCode'])\n        # On the iteration 386 there is so few news that when merged with the market the dataframe has no data. \n    if df.shape[0] > 0:\n        X_test = StandardScaler().fit_transform(df.loc[:, features].values)\n        # Prediction\n#         y_pred = model.predict_proba(X_test)\n#         df['confidenceValue'] = [p if p>=0.5 else 1-p for p in y_pred[:,1]]\n        df['confidenceValue'] = get_prediction_2(clf3, X_test)\n        # Merge prediction with the respective asset code\n        pred = pd.merge(predictions_template_df,df.loc[:,['assetCode','confidenceValue']],on='assetCode', how='outer').loc[:,[\"assetCode\", \"confidenceValue_y\"]].fillna(0)\n    else: \n        pred = predictions_template_df.copy()\n        pred.columns = ['assetCode','confidenceValue_y']\n    predictions_template_df.confidenceValue = pred.confidenceValue_y","3ac1bc8f":"## ------ Test\n# ~ 510 s\n\nstart = time()\n\ndays = env.get_prediction_days()\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_predictions(predictions_template_df, market_obs_df, news_obs_df)\n    env.predict(predictions_template_df)\n\nenv.write_submission_file()\n\nprint('Done',time()-start)\n","7f2b69d8":"### Testing the competition's test set","d8e829fd":"### Training","f72dcd94":"### Preprocessing"}}