{"cell_type":{"394e1ed9":"code","505a0ab6":"code","7daee848":"code","f3602c64":"code","2157e23b":"code","8464ebfa":"code","ed75fb45":"code","31ba282b":"code","39360038":"code","bddbd0ad":"code","c27a302b":"code","6a24ba01":"code","d38739ad":"code","ffcdf20b":"code","65dbe84d":"code","8d5f44cd":"code","f54fd737":"code","b76c5ad4":"code","ae8366a3":"markdown","aeb2af42":"markdown","18313b9d":"markdown","d87d10b8":"markdown","6f0c0072":"markdown","0e9835af":"markdown","8eaadc6b":"markdown","884c32e2":"markdown","15fdf074":"markdown","3986b12a":"markdown","2d438a7e":"markdown","9095224b":"markdown","3da4915a":"markdown","93e9df50":"markdown"},"source":{"394e1ed9":"import gym\n\nimport random\nfrom collections import defaultdict\nimport numpy as np\n\n# this is all plotting stuff :\/\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter, MaxNLocator\n%matplotlib inline\n\nmatplotlib.style.use('ggplot')","505a0ab6":"env = gym.make('Taxi-v2')","7daee848":"env.render()","f3602c64":"env.action_space","2157e23b":"env.reset()\nenv.render()","8464ebfa":"obs, reward, done, _ = env.step(action=0)","ed75fb45":"env.render()","31ba282b":"done","39360038":"reward","bddbd0ad":"# start a new blackjack env\nenv = gym.make('Blackjack-v0')\n\n# number of games to play\nepisodes = 500000\n\n# sometimes you may want to discount rewards, I'm not going to cover this here\ngamma = 1.","c27a302b":"def get_epsilon(N_state_count, N_zero=100):\n    \"\"\"\n    This is our function to calculate epsilon and is core to how we are going to pick our next action.\n    \n    When we first start exploring our state-action space, we have little or no knowledge of the environment, meaning we\n    have little or no knowledge about what a good action might be. In this case we want to pick a random action (ie we\n    want a large epsilon). As our knowledge gets better, we can have more confidence in what we're doing and so we'd like\n    to pick what we know is a good action more often.\n    \n    We're initialising N_zero to 100, but this is a hyperparameter we can tune\n    \"\"\"\n    return N_zero \/ (N_zero + N_state_count)","6a24ba01":"def get_action(Q, state, state_count, action_size):\n    \"\"\"\n    Given our value function (Q) and state, what action should we take?\n    \n    If we haven't seen this state before we should pick an action at random, after all we have no information about\n    what action might be best.\n    \n    If we have infinite experience, what should we do? In this case we would like to pick the action with the\n    highest expected value all the time.\n    \n    To fulfil this we're going to use what is known as GLIE (Greedy in the Limit of Infinite Exploration).\n    \n    The idea is we pick the action with the highest expected value with a probability of `1 - epsilon` and a random\n    action with probability epsilon. At first epsilon is large but it eventually decays to zero as we play an infinite\n    number of games.\n    \n    Doing this guarentees we visit all possible states and actions.\n    \"\"\"\n    random_action = random.randint(0, action_size - 1)\n    \n    best_action = np.argmax(Q[state])\n        \n    epsilon = get_epsilon(state_count)\n    \n    return np.random.choice([best_action, random_action], p=[1. - epsilon, epsilon])","d38739ad":"def evaluate_policy(Q, episodes=10000):\n    \"\"\"\n    Helper function which helps us evaluate how good our policy is.\n    \n    We do this by playing 10000 games of blackjack and returning the win ratio.\n    \"\"\"\n    wins = 0\n    for _ in range(episodes):\n        state = env.reset()\n        \n        done = False\n        while not done:\n            action = np.argmax(Q[state])\n            \n            state, reward, done, _ = env.step(action=action)\n            \n        if reward > 0:\n            wins += 1\n        \n    return wins \/ episodes","ffcdf20b":"def monte_carlo(gamma=1., episodes=5000, evaluate=False):\n\n    # this is our value function, we will use it to keep track of the \"value\" of being in a given state\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n\n    # to decide what action to take and calculate epsilon we need to keep track of how many times we've\n    # been in a given state and how often we've taken a given action when in that state\n    state_count = defaultdict(float)\n    state_action_count = defaultdict(float)\n\n    # for keeping track of our policy evaluations (we'll plot this later)\n    evaluations = []\n\n    for i in range(episodes):\n        # evaluating a policy is slow going, so let's only do this every 1000 games\n        if evaluate and i % 1000 == 0:\n            evaluations.append(evaluate_policy(Q))\n    \n        # to update our value function we need to keep track of what states we were in and what actions\n        # we took throughout the game\n        episode = []\n    \n        # lets start a game!\n        state = env.reset()\n        done = False\n    \n        # and keep playing until it's done (recall this is something Gym will tell us)\n        while not done:\n            # so we're in some state, let's remember we've been here and pick an action using our\n            # function defined above\n            state_count[state] += 1\n            action = get_action(Q, state, state_count[state], env.action_space.n)\n\n            # when we take that action, recall Gym will give us a new state, some reward and if we are done\n            new_state, reward, done, _ = env.step(action=action)\n        \n            # save what happened, we're just going to keep the state, action and reward\n            episode.append((state, action, reward))\n        \n            state = new_state\n\n        # at this point the game is finished, we either won or lost\n        # so we need to take what happened and update our value function\n        G = 0\n    \n        # because you can only win or lose a game of blackjack we only get a reward at the end of the game\n        # (+1 for a win, 0 for a draw, -1 for a loss). So let's start at the end of the game and work\n        # backwards through our states to decide how good it was to be in a state\n        for s, a, r in reversed(episode):\n            new_s_a_count = state_action_count[(s, a)] + 1\n            \n            # we need some way of deciding how the game we just played impacted our value function. The\n            # standard approach here is to take the reward(s) we got playing over multiple games and\n            # taking the mean. We can update the mean as we go using what is known as incremental averaging\n            # https:\/\/math.stackexchange.com\/questions\/106700\/incremental-averageing\n            G = r + gamma * G\n            state_action_count[(s, a)] = new_s_a_count\n            Q[s][a] = Q[s][a] + (G - Q[s][a]) \/ new_s_a_count\n            \n    return Q, evaluations","65dbe84d":"Q_mc, evaluations = monte_carlo(episodes=500000, evaluate=True)","8d5f44cd":"def plot_value_function(Q, title=\"Value Function\"):\n    V = defaultdict(float)\n\n    for state, action_rewards in Q.items():\n        r1, r2 = action_rewards\n        action_value = np.max([r1, r2])\n        V[state] = action_value\n    \n    min_x = min(k[0] for k in V.keys())\n    max_x = max(k[0] for k in V.keys())\n    min_y = min(k[1] for k in V.keys())\n    max_y = max(k[1] for k in V.keys())\n\n    x_range = np.arange(min_x, max_x + 1)\n    y_range = np.arange(min_y, max_y + 1)\n    X, Y = np.meshgrid(x_range, y_range)\n\n    # Find value for all (x, y) coordinates\n    Z_noace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))\n    Z_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))\n\n    def plot_surface(X, Y, Z, title):\n        fig = plt.figure(figsize=(20, 10))\n        ax = fig.add_subplot(111, projection='3d')\n        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n                               cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n        ax.set_xlabel('Player sum')\n        ax.set_ylabel('Dealer showing')\n        ax.set_zlabel('Value')\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n        ax.set_title(title)\n        ax.view_init(ax.elev, 120)\n        fig.colorbar(surf)\n        plt.show()\n\n    plot_surface(X, Y, Z_noace, \"value function\")\n    plot_surface(X, Y, Z_ace, \"value function - usable ace\")","f54fd737":"plot_value_function(Q_mc)","b76c5ad4":"plt.plot([i * 1000 for i in range(len(evaluations))], evaluations)\nplt.xlabel('episode')\nplt.ylabel('win rate')","ae8366a3":"## Next steps and acknowledgements\n\nThis kernel is kind of a work in progress, I'd eventually like to explore the optimal policy more and use what is known as TD learning to achive the same result (hopefully in fewer games!)\n\nIf you're interested in learning more about RL then there are lots of fantastic resources out there, including:\n\n- [Reinforcement Learning: An Introduction (2nd edition)](http:\/\/incompleteideas.net\/book\/the-book-2nd.html) by Sutton and Barto\n- [David Silver's Reinforcement Learning course](https:\/\/www.youtube.com\/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n- [DeepMind's UCL lecture course](https:\/\/www.youtube.com\/watch?v=iOh7QUZGyiU&list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs)\n- [OpenAI's spinning up in Deep RL resource](https:\/\/blog.openai.com\/spinning-up-in-deep-rl\/)\n\nThis kernel is based upon a homework which was part of Prof Silver's course and a problem in Sutton and Barto's book.\n\nThe code for plotting the value function was a bit beyond my matplotlib skills, luckily I stumbled across [this github project](https:\/\/github.com\/dennybritz\/reinforcement-learning) which implemented nearly exactly what was needed. So thank you there.\n\nAlso, I want to point people to [this great blogpost](https:\/\/lilianweng.github.io\/lil-log\/2018\/02\/19\/a-long-peek-into-reinforcement-learning.html#sarsa-on-policy-td-control) from which I borrowed a few images - thank you!","aeb2af42":"And that's it!\n\nSo let's run it for half a million games!","18313b9d":"The objective of this game is to move the yellow block (the taxi) to where a passenger is, pick them up and drop them off in the desired location in as few moves as possible.\n\nEach env has an action space of things you the user (or your RL agent) can do. In this example there are 6 possible actions","d87d10b8":"# Learning to play Blackjack with Reinforcement Learning\n\nOver the past couple of years [DeepMind](https:\/\/deepmind.com\/) have had some amazing results playing [Atari games](https:\/\/www.nature.com\/articles\/nature14236) and [Go](https:\/\/www.nature.com\/articles\/nature16961). While this is fantastic for reinforcement learning, these successes give the impression that it's a new area of machine learning. In fact reinforcement learning has been around for a while, is very well mathematically established and can produce great results on simple games with very little code.\n\nIn this kernel we're going to use [OpenAI Gym](https:\/\/gym.openai.com\/) and a very basic reinforcement learning technique called Monte Carlo Control to learn how to play Blackjack.\n\nBut first, I want to talk a bit about Gym.","6f0c0072":"*note: I'm limiting the examples in this kernel to text based envs, there are much more visually interesting kernels out there if you have the ability to set up your kernel the right way*\n\nTo visualise the current state of an env, you simply call `.render()`","0e9835af":"Great!\n\nBut what use is this? Well we can use our value function to plot how good it is to be in a given state (with a greedy policy)\n\nAdapted from this [function](https:\/\/github.com\/dennybritz\/reinforcement-learning\/blob\/master\/lib\/plotting.py)","8eaadc6b":"Notice this is surprisingly noisey! Sadly that's a general feature of reinforcement learning","884c32e2":"which will then change the state","15fdf074":"calling `.step()` returns a few things, the new state (here I've called it `obs`), any reward and if the game is complete","3986b12a":"## What is OpenAI Gym?\n\nMuch like how MNIST or ImageNet is used to benchmark different image classifiction techniques, you can think of [OpenAI Gym](https:\/\/gym.openai.com\/) as a set of environments for reinforcement learning benchmarking.\n\nThere are environments ranging from the very simple (like Blackjack which we are going to use) to the highly complex (robotic hand manipulation, or Atari games). Many of these are \"classic\" tasks you might see in any reinforcement text book or online course (btw see the end of this kernel for some links) which lets you implement things you find :)\n\nHere, creating an env is very easy - for example to create the classic [Taxi game](https:\/\/gym.openai.com\/envs\/Taxi-v2\/) you just run","2d438a7e":"To perform an action we can use the `.step()` function with an action (one of the 6 we talked about above)","9095224b":"# Whistlestop tour of RL\n\nThere's lots of great resources out there on RL, just a few are linked at the end of this kernel, so I don't want to dwell on this too long, but I'd feel wrong publishing this kernel without a few sentences about what RL is all about.\n\nThink of a task, like driving a car or playing an Atari game - now how would you decide if you did well at that task? In RL we do this by measuring a \"reward\", in fact one of the central ideas of RL is the success in any task can be measured with a single scalar number. Our job in RL is simply to maximise this number.\n\nDefining this reward is easy for games (points in an Atari game, or +1 points for a win\/-1 for a loss, etc) which is part the reason they are studied so much.\n\nWe do this by visualising any task like so\n\n![rl](https:\/\/lilianweng.github.io\/lil-log\/assets\/images\/RL_illustration.png)\n\n*- image borrowed from [this awesome blog post](https:\/\/lilianweng.github.io\/lil-log\/2018\/02\/19\/a-long-peek-into-reinforcement-learning.html)*\n\nHere we train some \"agent\" to look at the environment (like Gym) and based upon the state maximise some reward.\n\nHow the agent decides what action to take in what state is called a policy.\n\nThere are a lot of ways to do this, here I'm going to implement one of the most simple approaches.\n\n\n## Monte-Carlo Control\n\nYou can think of Monte Carlo Control as a method of approximating optimal policies and it works as follows:\n\n- we keep a function which gives us the approximate \"value\" of being in a given state (typically called the Q-function), we'll represent this with a dict and initialise it randomly. You can think of this as \"if I find myself in state s, what reward can I expect at the end of the game? how good or bad is it to be there?\"\n- we'll then start playing our game (blackjack). When we start the game or take some action Gym will give us a state (our hand, the dealers open hand, if the game is complete etc).\n- using the value function, we decide our next action by looing at the available actions and picking the one with the highest value (we will inject some randomness here, which I'll talk about shortly)\n- at the end of the game we update the value function with what we learnt\n\nThis sounds basic, but as you play more and more games and you explore more and more states you will eventually reach the best possible policy\n\nThere are a couple of nice features of learning like this:\n\n- it's model free, meaning we're not going to explicitly build an internal model of how Blackjack works and use that to help, instead we are going to learn by playing\n- it's on policy, meaning we're going to \"learn on the job\", learn about a policy by experience with that policy","3da4915a":"We have two plots here, one for when you have an ace (an ace can be 1 or 10) and one for when you don't - notice there's an increase in the win rate around 10\/11 when you don't have an ace - that's because the next card you get might be an ace and take you straight to 21!\n\nRecall we also evaluated our policy as we went along, lets plot how good it got over time","93e9df50":"In reinforcement learning we learn what these actions are and how to use these them to carry out the objective.\n\nWhen you've met the objective (or the state of the environment is otherwise terminal.. ie you died in pacman) you can restart your env with `.reset()` (this returns the state)"}}