{"cell_type":{"a7617546":"code","219a6f7b":"code","0f26bc8e":"code","8b6d3080":"code","7e7db665":"code","8d6d3a92":"code","1fa8b813":"code","157e64e9":"code","721c45b9":"code","6b6753bb":"code","cde732a7":"code","c6de7f2d":"code","1ce5f86d":"code","2fa9e5fd":"code","c073e160":"code","ff2d783b":"code","458c7fdc":"code","2c98f8fc":"code","003a8a46":"code","baf9adaa":"code","f4b2ac28":"code","b4658417":"code","2d0f4dad":"code","4e870659":"code","75e569c7":"code","f095ec49":"code","024cc4e0":"markdown","f018a4ca":"markdown","b3ecd669":"markdown","97c01896":"markdown","0f679fe6":"markdown","312c4c50":"markdown","7b80d57c":"markdown","c0f7f267":"markdown","f0dfac4e":"markdown","6d2f9a53":"markdown","e9e637dd":"markdown","c1ae9d4d":"markdown","18491d5e":"markdown","79073f87":"markdown","1a986ece":"markdown","5d576d6c":"markdown","d3957f96":"markdown","b0afa1c3":"markdown","363af4f7":"markdown","4854d955":"markdown","ad5ee0ab":"markdown","1b2c9588":"markdown","0e092e1b":"markdown"},"source":{"a7617546":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","219a6f7b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score,KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","0f26bc8e":"import os\nprint(os.listdir(\"..\/input\"))","8b6d3080":"train_complete=pd.read_csv(\"..\/input\/train_V2.csv\")\ntrain_complete.head()","7e7db665":"train=train_complete.sample(100000,random_state =1)\ntrain.head()","8d6d3a92":"train=train.drop(['Id','groupId','matchId'],axis=1)\ntrain.head()","1fa8b813":"train.info()","157e64e9":"dftrain=train.copy()","721c45b9":"corr=dftrain.corr()\ncorr","6b6753bb":"\nplt.figure(figsize=(15,10))\nsns.heatmap(corr,annot=True)","cde732a7":"plt.figure(figsize=(15,10))\ncorr1=corr.abs()>0.5\nsns.heatmap(corr1,annot=True)","c6de7f2d":"plt.title('Correlation B\/w Winning % and other Independent Variable')\ndftrain.corr()['winPlacePerc'].sort_values(ascending=False).plot(kind='bar',figsize=(10,8))","1ce5f86d":"k = 10 #number of variables for heatmap\nf,ax = plt.subplots(figsize=(11, 11))\ncols = dftrain.corr().nlargest(k, 'winPlacePerc')['winPlacePerc'].index\ncm = np.corrcoef(dftrain[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","2fa9e5fd":"sns.set()\ncols = ['winPlacePerc', 'walkDistance', 'boosts', 'weaponsAcquired', 'damageDealt', 'killPlace']\nsns.pairplot(dftrain[cols], size = 2.5)\nplt.show()","c073e160":"train.info()","ff2d783b":"train_complete=pd.get_dummies(train)\ntrain_complete.head()","458c7fdc":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nx_features=list(train_complete)\nx_features.remove('winPlacePerc')\ndata_mat = train_complete[x_features].as_matrix()                                                                                                              \nvif = [ variance_inflation_factor( data_mat,i) for i in range(data_mat.shape[1]) ]\nvif_factors = pd.DataFrame()\nvif_factors['column'] = list(x_features)\nvif_factors['vif'] = vif     \nvif_factors.sort_values(by=['vif'],ascending=False)[0:10]","2c98f8fc":"x_features=list(train_complete)\nx_features.remove('winPlacePerc')\nx_features.remove('maxPlace')\nx_features.remove('numGroups')\nx_features.remove('winPoints')\nx_features.remove('rankPoints')\nx_features.remove('matchType_squad-fpp')\nx_features.remove('matchDuration')\ndata_mat = train_complete[x_features].as_matrix()                                                                                                              \nvif = [ variance_inflation_factor( data_mat,i) for i in range(data_mat.shape[1]) ]\nvif_factors = pd.DataFrame()\nvif_factors['column'] = list(x_features)\nvif_factors['vif'] = vif     \nvif_factors.sort_values(by=['vif'],ascending=False)[0:10]","003a8a46":"x=train_complete[x_features]\ny=train_complete[['winPlacePerc']]\n# Train Test Split\nvalidation_size = 0.30\nseed = 1\nx_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=validation_size, random_state=seed)","baf9adaa":"model_final=GradientBoostingRegressor(n_estimators=100,learning_rate=0.5)\nmodel_final.fit(x_train,y_train)\nprint(model_final.score(x_train,y_train))\nprint(model_final.score(x_validation,y_validation))","f4b2ac28":"test=pd.read_csv('..\/input\/test_V2.csv')\ndftest=test.drop(['Id','groupId','matchId'],axis=1)\ndftest.head()","b4658417":"dftest.info()","2d0f4dad":"test_complete=pd.get_dummies(dftest)\nx_test=test_complete[x_features]","4e870659":"    pred=model_final.predict(x_test)\n    pred[1:5]","75e569c7":"pred_df=pd.DataFrame(pred,test['Id'],columns=['winPlacePerc'])\npred_df.head()","f095ec49":"pred_df.to_csv('sample_submission.csv')","024cc4e0":"# Import the Train Data Set","f018a4ca":"#### Train Test Split","b3ecd669":"#### Hyper Parameter Tuning","97c01896":"**Here we need to convert the non numeric columns to numeric by the creating dummy variables**","0f679fe6":"# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=seed)\n    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='r2') \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, np.sqrt(cv_results.mean()), np.sqrt(cv_results.std()))\n    print(msg)","312c4c50":"#### Model Comparision","7b80d57c":"## Now lets the corelation of the variables with the target variable","c0f7f267":"We will try removing the variables and check the Variance Inflation Factor","f0dfac4e":"## Now lets deal with the multicoliearilty and remove the less important variables\n- We will calculate the ** variance inflation factor**  for the complete data set.\n- Feature selection by step wise method and arrive at final features which gives the best accuracy\n- VIF = 1\/(1-r2)\n- There is thumb rule that the variables which are more than 10 are highly corelated and hence we can remove then","6d2f9a53":"### Let's look at the top ten features which are corelated with the target variables.","e9e637dd":"**Removing the columns with unique id's**","c1ae9d4d":"There are no null values and no non numeric data columns . We can proceed with the visuals","18491d5e":"### Here we can see that walk distance,boosts,weapons Acquired, damage dealt,heals, kills, long kills,kills streaks, ride distance show good corelation with the target variables","79073f87":"# Sample Submission","1a986ece":"### Lets do a pair plot analysis for the top six features","5d576d6c":"# PUBG DATA SET\n<img src=\"https:\/\/saudigamer.com\/wp-content\/uploads\/2018\/09\/123.jpg\" width=\"10000px\">\n","d3957f96":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","b0afa1c3":"#### Final Model","363af4f7":"# Gradient Boost Algorithm Hyper Parameter Tuning\nn_estimators = np.arange(50,110,10)\nlearning_rate = np.arange(0.1,1.1,0.1)\nparam_grid = dict(n_estimators=n_estimators,learning_rate=learning_rate)\nmodel = GradientBoostingRegressor()\nkfold = KFold(n_splits=5, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='r2', cv=5)\ngrid_result = grid.fit(x_train, y_train)","4854d955":"# Spot-Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('GB', GradientBoostingRegressor()))# we can set the hyper parameters if needed","ad5ee0ab":"## Importing Test Data","1b2c9588":"# Modelling the Data\n- Train Test Split\n- Model Comparision\n- Hyper Parameter Tuning\n- Building the Final Model","0e092e1b":"**Variation Inflation Factor**"}}