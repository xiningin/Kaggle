{"cell_type":{"e9af91e3":"code","a511d3ef":"code","abafa5ad":"code","8a593761":"code","cbda6b18":"code","2775b1c8":"code","eeedeff4":"code","cca50637":"code","eced96c2":"code","91aa7c07":"code","91fd89fa":"code","84602c9c":"code","5315fcd9":"code","0b029cbf":"markdown","0d4bc7cb":"markdown","6fce7dbc":"markdown"},"source":{"e9af91e3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport tensorflow as tf\n\ntrain_df = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-3\/train.csv') # historical data \ntest_df = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-3\/test.csv') # predictions to be filled","a511d3ef":"train_df = train_df.fillna({'Province_State': 'Unknown'})\ntest_df = test_df.fillna({'Province_State': 'Unknown'})\ntrain_df['Country_Region']= train_df['Country_Region'].str.replace(\"'\", \"\")\ntrain_df['Province_State']= train_df['Province_State'].str.replace(\"'\", \"\")\ntest_df['Country_Region']= test_df['Country_Region'].str.replace(\"'\", \"\")\ntest_df['Province_State']= test_df['Province_State'].str.replace(\"'\", \"\")\n\ntrain_df.isna().sum()","abafa5ad":"def to_datetime(dt):\n    return datetime.datetime.strptime(dt, '%Y-%m-%d')\n\ndef count_days(dt):\n    return (dt - datetime.datetime.strptime('2020-01-22', \"%Y-%m-%d\")).days\n\ntrain_df['Date_dt'] = train_df['Date'].map(to_datetime)\ntrain_df['Day'] = train_df['Date_dt'].map(count_days)\ntest_df['Date_dt'] = test_df['Date'].map(to_datetime)\ntest_df['Day'] = test_df['Date_dt'].map(count_days)","8a593761":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# Min-Max Scaler\nscaler_c = MinMaxScaler(feature_range=(0, 100))\ntrain_df['ConfirmedCases_scaled'] = None\ntrain_df[['ConfirmedCases_scaled']] = scaler_c.fit_transform(train_df[['ConfirmedCases']])\n\nscaler_f = MinMaxScaler(feature_range=(0, 100))\ntrain_df['Fatalities_scaled'] = None\ntrain_df[['Fatalities_scaled']] = scaler_f.fit_transform(train_df[['Fatalities']])\n\n# Get dummy columns for geo location\ngeo_columns = []\nn_geo_columns = 306\nfor i in range(n_geo_columns):\n    geo_columns.append('Geo_{}'.format(i))\ntrain_df.drop(columns=geo_columns, inplace=True, errors='ignore')\n\nlbl_encoder = LabelEncoder()\nscaler_g = MinMaxScaler(feature_range=(0, 1))\nhot_encoder = OneHotEncoder(sparse=False)\ntrain_df['Geo'] = train_df['Country_Region'].astype(str) + '_' + train_df['Province_State'].astype(str)\ntrain_df[['Geo']] = lbl_encoder.fit_transform(train_df[['Geo']])\ntrain_df = pd.get_dummies(train_df, prefix_sep=\"_\", columns=['Geo'])\n\n\nprint(train_df.columns)\ntrain_df[['ConfirmedCases', 'ConfirmedCases_scaled', 'Fatalities', 'Fatalities_scaled',  'Geo_0']].head()    ","cbda6b18":"historical_steps = 14\nn_output_node = 1\n\ndef make_sequential_input(df):\n    \n    inputs_c, inputs_f, inputs_geo, targets_c, targets_f = [], [], [], [], []\n    \n    for i in range(len(df) - historical_steps - 1):\n        \n        if df.iloc[i]['Country_Region'] == df.iloc[i + historical_steps]['Country_Region'] and \\\n            df.iloc[i]['Province_State'] == df.iloc[i + historical_steps]['Province_State']:\n            \n            # iloc[a:b] startnig from index 'a' and ending before b\n            inputs_c.append(np.array(df.iloc[i : i + historical_steps][['ConfirmedCases_scaled']]).tolist()) # time seires until t-1\n            inputs_f.append(np.array(df.iloc[i : i + historical_steps][['Fatalities_scaled']]).tolist()) # time seires until t-1\n            inputs_geo.append(np.array(df.iloc[i + historical_steps][geo_columns]).tolist())\n            targets_c.append(np.array(df.iloc[i + historical_steps][['ConfirmedCases_scaled']]).tolist()) # result data at time t\n            targets_f.append(np.array(df.iloc[i + historical_steps][['Fatalities_scaled']]).tolist()) # result data at time t        \n              \n    return inputs_c, inputs_f, inputs_geo, targets_c, targets_f\n\n# Make sequential input for training and validation\ntrain_inputs, train_inputs_f, train_inputs_geo, train_targets_c, train_targets_f = make_sequential_input(train_df)\n\nprint('Train input shape: {}'.format(np.shape(train_inputs)))\nprint('Train input geo shape: {}'.format(np.shape(train_inputs_geo)))","2775b1c8":"import random\n\nmax_index = np.array(train_inputs).shape[0] - 1\nindices = []\n\nfor i in range(int(max_index*0.2)):\n    indices.append(random.randint(0, max_index))\n\nval_inputs = [ train_inputs[i] for i in indices ]\nval_inputs_f = [ train_inputs_f[i] for i in indices ]\nval_inputs_geo = [ train_inputs_geo[i] for i in indices  ] \nval_targets_c = [ train_targets_c[i] for i in indices ]\nval_targets_f = [ train_targets_f[i] for i in indices ]\n\ntrain_inputs = [ elem for i, elem in enumerate(train_inputs) if i not in indices ] \ntrain_inputs_f = [ elem for i, elem in enumerate(train_inputs_f) if i not in indices ] \ntrain_inputs_geo = [ elem for i, elem in enumerate(train_inputs_geo) if i not in indices ] \ntrain_targets_c = [ elem for i, elem in enumerate(train_targets_c) if i not in indices ] \ntrain_targets_f = [ elem for i, elem in enumerate(train_targets_f) if i not in indices ] \n\npd.set_option('display.max_colwidth', -1) \nprint('No. train data: {}'.format(len(train_inputs)))\nprint('No. validation data: {}'.format(len(val_inputs)))","eeedeff4":"from tensorflow.keras import Model, Sequential, backend as K\nfrom tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout\n\nn_output_node = 1\ninput_shape=np.array(train_inputs).shape[-2:]\ninput_shape_geo=np.array(train_inputs_geo).shape\n\nbatch_size = 128\nepochs = 150\nlr = 0.001\n\ndef create_model(model_type_name, load_weight, inputs=None, inputs_geo=None, targets=None, v_inputs=None, v_inputs_geo=None, v_targets=None):\n    \n    geo_input = Input(shape=(n_geo_columns,), name='input_geo')\n    \n    h_state = Dense(256)(geo_input)\n    h_state = Dropout(0.1)(h_state)\n    h_state = Dense(256, activation='relu')(h_state)\n    c_state = Dense(256)(geo_input)\n    c_state = Dropout(0.1)(c_state)\n    c_state = Dense(256, activation='relu')(c_state)\n\n    ts_input = Input(shape=input_shape, name='input_ts')\n    lstm = LSTM(256, return_sequences=True)(ts_input, initial_state=[ h_state, c_state ])\n    lstm = Dropout(0.1)(lstm)\n    lstm = Bidirectional(LSTM(128))(lstm)\n    lstm = Dropout(0.1)(lstm)\n    main_output = Dense(n_output_node, activation='relu', name='output_main')(lstm)\n    \n    model = Model(inputs=[ geo_input, ts_input ], outputs=main_output)    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=tf.keras.losses.MSLE, metrics=[ tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.MeanSquaredError() ])\n    \n    model_path = 'model_{}.h5'.format(model_type_name)\n    \n    if load_weight:\n        \n        model = tf.keras.models.load_model(model_path)\n    else:\n    \n        history = model.fit([ inputs_geo, inputs ],  targets, \\\n                    epochs=epochs, \\\n                    batch_size=batch_size, \\\n                    verbose=1, \\\n                    validation_data=({ 'input_geo': v_inputs_geo, 'input_ts': v_inputs },{ 'output_main': v_targets}))\n\n        scores = model.evaluate({ 'input_geo': inputs_geo, 'input_ts': inputs }, targets)\n        print(\"Model Accuracy: {}\".format(scores))\n        \n        plt.plot(history.history['loss']) \n        plt.plot(history.history['val_loss'])\n        plt.title('Loss over epochs')\n        plt.legend(['Train', 'Validation'], loc='best')\n        plt.ylabel('Loss')\n        plt.xlabel('Epoch')\n        plt.show()\n        \n        model.save(model_path) \n        \n    return model\n                        \n\nmodel_cases = create_model('case', False, np.array(train_inputs), np.array(train_inputs_geo), np.array(train_targets_c), np.array(val_inputs), np.array(val_inputs_geo), np.array(val_targets_c))\nmodel_fatality = create_model('fatality', False, np.array(train_inputs_f), np.array(train_inputs_geo), np.array(train_targets_f), np.array(val_inputs_f), np.array(val_inputs_geo), np.array(val_targets_f))","cca50637":"model_cases = create_model('case', load_weight=True)\nmodel_fatality = create_model('fatality', load_weight=True)","eced96c2":"def predict_cases(country, state):\n    \n    df = train_df[(train_df['Country_Region'] == country) & (train_df['Province_State'] == state) ]\n\n    inputs = np.array(df[['ConfirmedCases_scaled']][-historical_steps-1:-1])\n    inputs_geo = np.array(df.iloc[-1][geo_columns])\n    actuals = np.array(df.iloc[-1][['ConfirmedCases']])\n    \n    predictions = model_cases.predict([  np.array(inputs_geo).astype(np.float32).reshape(1, len(geo_columns)), np.array(inputs).reshape(1, input_shape[0], input_shape[1]) ]).reshape(-1).tolist()\n    \n    print('Inputs: {}, Pred: {}, Expected: {}'.format( \\\n        np.array(df[['ConfirmedCases']][-historical_steps-1:-1])[:,0].tolist(), \\\n        scaler_c.inverse_transform(np.array(predictions).reshape(-1,1)), \\\n        actuals))\n    \ndef predict_fatality(country, state):\n    \n    df = train_df[(train_df['Country_Region'] == country) & (train_df['Province_State'] == state) ]\n\n    inputs = np.array(df[['Fatalities_scaled']][-historical_steps-1:-1])\n    inputs_geo = np.array(df.iloc[-1][geo_columns])\n    actuals = np.array(df.iloc[-1][['Fatalities']])\n    \n    predictions = model_fatality.predict([  np.array(inputs_geo).astype(np.float32).reshape(1, len(geo_columns)), np.array(inputs).reshape(1, input_shape[0], input_shape[1]) ]).reshape(-1).tolist()\n    \n    print('Inputs: {}, Pred: {}, Expected: {}'.format( \\\n        np.array(df[['Fatalities']][-historical_steps-1:-1])[:,0].tolist(), \\\n        scaler_f.inverse_transform(np.array(predictions).reshape(-1,1)), \\\n        actuals))\n      \n\npredict_cases('Australia', 'Victoria')\npredict_cases('Australia', 'New South Wales')\npredict_cases('Korea, South', 'Unknown')\npredict_cases('Iran', 'Unknown')\npredict_cases('Italy', 'Unknown')\n\npredict_fatality('Australia', 'Victoria')\npredict_fatality('Australia', 'New South Wales')\npredict_fatality('Korea, South', 'Unknown')\npredict_fatality('Iran', 'Unknown')\npredict_fatality('Italy', 'Unknown')","91aa7c07":"import time\n\ntest_df['ConfirmedCases'] = None\ntest_df['Fatalities'] = None\n\ntemp_df = train_df.copy()\ntemp_df = pd.concat([ temp_df, test_df[test_df.Day > train_df.iloc[-1].Day] ], ignore_index=True)\ntemp_df['ConfirmedCases_scaled_predicted'] = None\ntemp_df['Fatalities_scaled_predicted'] = None\n\nday_predicting_from = 64\nday_predicting_to = 106\n\ncurrent_row = None\nhist_rows = None\n\ntic = time.perf_counter()\ncounter = 0\n\n# For each country and state\n# for c, s in test_df[(test_df.Country_Region=='Australia') & (test_df.Province_State=='New South Wales')].groupby(['Country_Region', 'Province_State']): \n# for c, s in test_df[(test_df.Country_Region=='Australia') & (test_df.Province_State=='Victoria')].groupby(['Country_Region', 'Province_State']): \n# for c, s in test_df[(test_df.Country_Region=='Korea, South') & (test_df.Province_State=='Unknown')].groupby(['Country_Region', 'Province_State']):  \n# for c, s in test_df[(test_df.Country_Region=='Italy') & (test_df.Province_State=='Unknown')].groupby(['Country_Region', 'Province_State']):  \n# for c, s in test_df[(test_df.Country_Region=='France') & (test_df.Province_State=='Unknown')].groupby(['Country_Region', 'Province_State']): \n# for c, s in test_df[(test_df.Country_Region=='China') & (test_df.Province_State=='Hubei')].groupby(['Country_Region', 'Province_State']):  \nfor c, s in test_df.groupby(['Country_Region', 'Province_State']):  \n    \n    toc = time.perf_counter()\n    \n    country = c[0]\n    state = c[1]\n    \n    # Traverse from Day 64 to the end\n    for day in range(day_predicting_from, day_predicting_to + 1):   \n        current_row = temp_df[ (temp_df.Country_Region == country) & (temp_df.Province_State == state) & (temp_df.Day == day)]\n        hist_rows = temp_df[(temp_df.Country_Region == country) & (temp_df.Province_State == state) & (temp_df.Day >= (day - historical_steps)) & (temp_df.Day < day)]\n        \n        # Only predict when historical steps exist\n        if not current_row.empty and not hist_rows.empty and hist_rows.shape[0] == historical_steps:    \n            input_geo = np.array(hist_rows.iloc[-historical_steps][geo_columns]).reshape(1, len(geo_columns))\n            input_c = np.array(hist_rows.iloc[-historical_steps:,][['ConfirmedCases_scaled']]).reshape(1, input_shape[0], input_shape[1])\n            input_f = np.array(hist_rows.iloc[-historical_steps:,][['Fatalities_scaled']]).reshape(1, input_shape[0], input_shape[1])\n            pred = model_cases.predict([ tf.convert_to_tensor(input_geo, np.float64), tf.convert_to_tensor(input_c, np.float64) ])\n            pred_f = model_fatality.predict([ tf.convert_to_tensor(input_geo, np.float64), tf.convert_to_tensor(input_f, np.float64) ])\n  \n            # Update the predict fields\n            current_idx = current_row.index.values[0]\n            temp_df.at[current_idx, 'ConfirmedCases_scaled_predicted'] = float(pred[0][0])\n            temp_df.at[current_idx, 'Fatalities_scaled_predicted'] = float(pred_f[0][0])\n            \n            last = hist_rows.iloc[-1,]\n            for col_name in geo_columns:\n                temp_df.at[current_idx, col_name] = last[col_name]\n            \n            # Update the existing fields if empty\n            if (current_row['ConfirmedCases'].values[0] == None) and (current_row['Fatalities'].values[0] == None):\n                temp_df.at[current_idx, 'ConfirmedCases_scaled'] = float(pred[0][0])\n                temp_df.at[current_idx, 'Fatalities_scaled'] = float(pred_f[0][0])\n    \n    toc = time.perf_counter()\n    counter = counter + 1\n    print('{:.2f} sec(s) taken - Geo: {},{}, Count: {}'.format((toc-tic), country, state, counter))\n        \n\ntemp_df['ConfirmedCases_inversed_predicted'] = None\ntemp_df['Fatalities_inversed_predicted'] = None\ntemp_df[['ConfirmedCases_inversed_predicted']] = scaler_c.inverse_transform(temp_df[['ConfirmedCases_scaled_predicted']])\ntemp_df[['Fatalities_inversed_predicted']] = scaler_f.inverse_transform(temp_df[['Fatalities_scaled_predicted']]) ","91fd89fa":"temp_df[(temp_df.Country_Region=='Australia') & (temp_df.Province_State=='New South Wales')] \\\n[['ForecastId', 'Day', 'Date', 'Country_Region', 'Province_State', 'ConfirmedCases', 'ConfirmedCases_scaled', 'ConfirmedCases_scaled_predicted', 'ConfirmedCases_inversed_predicted']] \\\n.iloc[-45:,:]\n","84602c9c":"def plot_by_country(country, state):\n\n    hist_df = temp_df[(temp_df.Country_Region == country) & (temp_df.Province_State == state) & (temp_df.Day <= 74)].groupby(['Country_Region', 'Province_State', 'Day', 'Date']).agg({'ConfirmedCases': 'first'}).reset_index()\n    pred_df = temp_df[(temp_df.Country_Region == country) & (temp_df.Province_State == state) & (temp_df.Day >= 64)].groupby(['Country_Region', 'Province_State', 'Day', 'Date']).agg({'ConfirmedCases_inversed_predicted': 'first'}).reset_index()\n\n    plt.title('{}, {}'.format(state, country))\n    plt.plot(hist_df.Day, hist_df.ConfirmedCases, label='Historical')\n    plt.plot(pred_df.Day, pred_df.ConfirmedCases_inversed_predicted, label='Predictive')\n    plt.axvline(x=day_predicting_from, color='r', linestyle='--', linewidth=1, label='2020-04-04')\n    plt.xlabel('Day')\n    plt.ylabel('Cases')\n    plt.legend()\n    plt.show()\n    \n[ plot_by_country(country, state) for country, state in [ \\\n    ('Australia', 'New South Wales'), ('Australia', 'Victoria'), ('Korea, South', 'Unknown'), ('China', 'Hubei'), ('Italy', 'Unknown'), ('France', 'Unknown')]]","5315fcd9":"tic = time.perf_counter()\n\nfor i in range(len(test_df)):\n    \n    country = test_df.at[i, 'Country_Region']\n    state = test_df.at[i, 'Province_State']\n    day = test_df.at[i, 'Day']\n    \n    current_df = temp_df[(temp_df.Country_Region == country) & (temp_df.Province_State == state) & (temp_df.Day == day)]\n    test_df.at[i, 'ConfirmedCases'] = current_df['ConfirmedCases_inversed_predicted'].values[0]\n    test_df.at[i, 'Fatalities'] = current_df['Fatalities_inversed_predicted'].values[0]\n    \n    if i%1000 == 0:\n        toc = time.perf_counter()\n        print('{:.2f} sec(s) taken'.format((toc-tic)))\n\ntest_df[['ForecastId', 'ConfirmedCases', 'Fatalities']].to_csv('submission.csv', index=False)","0b029cbf":"3. Analyse Results","0d4bc7cb":"2. Train LSTM Model with initial state","6fce7dbc":"# Predicting COVID-19 Infections with LSTM\n> by Yohan Chung\n\nThe pandemic of Coronavirus (COVID-19) became reality and many countries around the globe strive to contain further spread of the virus with social distancing as well as qurantining of those who contact the infected. The project aims to predict future of the infected by country and identify which country needs more attention. The prediction of the newly infected starts with reading data from the files train.csv and test.csv as below.\n\nThe datatsets are provided by Johns Hopkins University and include the COVID-19 confirmed cases & fatalities by country.\n\n1. Preprocess Data"}}