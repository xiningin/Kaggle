{"cell_type":{"8564ea5d":"code","786ffb75":"code","99fa534c":"code","3a857b3b":"code","d274758d":"code","869f2ac5":"code","09f848aa":"code","8810dfcb":"code","87646a81":"code","9038019d":"code","2ca4da0c":"code","cb295dff":"code","d25fe46a":"code","7bcc860a":"code","8045f46a":"code","bc1d16ff":"code","bd001a66":"code","186b39f2":"code","5ed31782":"code","a5a83f20":"code","f4d03040":"code","ef25690b":"code","2419bc16":"code","a9e4eb99":"code","14553565":"code","7b657de6":"code","25a2e8aa":"code","a76915f5":"code","095b12bf":"code","f418d259":"code","79e7b117":"code","d795296b":"markdown","5b977862":"markdown","7c23cda2":"markdown","2f6342d6":"markdown","c758c61f":"markdown","7ac6159d":"markdown","70f93ecf":"markdown","ecd35772":"markdown","58203a62":"markdown","b0269504":"markdown","bf46c9af":"markdown","70fc9ef3":"markdown","c2731585":"markdown","586ac4ba":"markdown","d7e480ec":"markdown","2153ba1f":"markdown","d55bb98f":"markdown","ba63f6d5":"markdown","cf627486":"markdown","987709c0":"markdown","9a103553":"markdown","a711b99e":"markdown","bd0d1b5d":"markdown","5da9acce":"markdown","4b5fda46":"markdown","a58e1ab6":"markdown","8ed44bcc":"markdown","72c0c831":"markdown","c7bc93f2":"markdown","f3f6f333":"markdown","bdfba1fd":"markdown","66353d9f":"markdown","2e755310":"markdown","545d00f0":"markdown","1e6e86eb":"markdown","198fc22e":"markdown","74aed275":"markdown","56671b5a":"markdown","879ee83a":"markdown","6246803a":"markdown","8575b72e":"markdown","7affe37d":"markdown"},"source":{"8564ea5d":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\n\npd.set_option('display.max_columns', 100)\n\n\nRFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n\n\n#TRAIN\/VALIDATION\/TEST SPLIT\n#VALIDATION\nVALID_SIZE = 0.20 # simple validation using train_test_split\nTEST_SIZE = 0.20 # test size using_train_test_split\n\n#CROSS-VALIDATION\nNUMBER_KFOLDS = 5 #number of KFolds for cross-validation\n\n\n\nRANDOM_STATE = 2018\n\nMAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50 #lgb early stop \nOPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\nVERBOSE_EVAL = 50 #Print out metric result\n\nIS_LOCAL = False\n\nimport os\n\nif(IS_LOCAL):\n    PATH=\"..\/input\/credit-card-fraud-detection\"\nelse:\n    PATH=\"..\/input\"\nprint(os.listdir(PATH))","786ffb75":"data_df = pd.read_csv(PATH+\"\/creditcard.csv\")","99fa534c":"print(\"Credit Card Fraud Detection data -  rows:\",data_df.shape[0],\" columns:\", data_df.shape[1])","3a857b3b":"# data_df.head()","d274758d":"data_df.describe()","869f2ac5":"total = data_df.isnull().sum().sort_values(ascending = False)\npercent = (data_df.isnull().sum()\/data_df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","09f848aa":"temp = data_df[\"Class\"].value_counts()\ndf = pd.DataFrame({'Class': temp.index,'values': temp.values})\n\ntrace = go.Bar(\n    x = df['Class'],y = df['values'],\n    name=\"Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)\",\n    marker=dict(color=\"Red\"),\n    text=df['values']\n)\ndata = [trace]\nlayout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',\n          xaxis = dict(title = 'Class', showticklabels=True), \n          yaxis = dict(title = 'Number of transactions'),\n          hovermode = 'closest',width=600\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='class')","8810dfcb":"class_0 = data_df.loc[data_df['Class'] == 0][\"Time\"]\nclass_1 = data_df.loc[data_df['Class'] == 1][\"Time\"]\n#plt.figure(figsize = (14,4))\n#plt.title('Credit Card Transactions Time Density Plot')\n#sns.set_color_codes(\"pastel\")\n#sns.distplot(class_0,kde=True,bins=480)\n#sns.distplot(class_1,kde=True,bins=480)\n#plt.show()\nhist_data = [class_0, class_1]\ngroup_labels = ['Not Fraud', 'Fraud']\n\nfig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\nfig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\niplot(fig, filename='dist_only')","87646a81":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=True)\n# showfliers if set to true will not show the box but will only show dots and if set to false then they will show the box plots.\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=False)\nplt.show();","9038019d":"tmp = data_df[['Amount','Class']].copy()\nclass_0 = tmp.loc[tmp['Class'] == 0]['Amount']\nclass_1 = tmp.loc[tmp['Class'] == 1]['Amount']\nclass_0.describe()","2ca4da0c":"class_1.describe()","cb295dff":"fraud = data_df.loc[data_df['Class'] == 1]\n\ntrace = go.Scatter(\n    x = fraud['Time'],y = fraud['Amount'],\n    name=\"Amount\",\n     marker=dict(\n                color='rgb(238,23,11)',\n                line=dict(\n                    color='red',\n                    width=1),\n                opacity=0.5,\n            ),\n    text= fraud['Amount'],\n    mode = \"markers\"\n)\ndata = [trace]\nlayout = dict(title = 'Amount of fraudulent transactions',\n          xaxis = dict(title = 'Time [s]', showticklabels=True), \n          yaxis = dict(title = 'Amount'),\n          hovermode='closest'\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='fraud-amount')","d25fe46a":"plt.figure(figsize = (14,14))\nplt.title('Credit Card Transactions features correlation plot (Pearson)')\ncorr = data_df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","7bcc860a":"s = sns.lmplot(x='V20', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V7', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","8045f46a":"s = sns.lmplot(x='V2', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V5', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","bc1d16ff":"var = data_df.columns.values\n\ni = 0\nt0 = data_df.loc[data_df['Class'] == 0]\nt1 = data_df.loc[data_df['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,28))\n\nfor feature in var:\n    i += 1\n    plt.subplot(8,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","bd001a66":"target = 'Class'\npredictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']","186b39f2":"train_df, test_df = train_test_split(data_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True )\ntrain_df, valid_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )\n# We make use of train_df dataset while splitting data for training and validation since tesing data is already less and taking aways more data will reduce \n# the efficiency of the outcomes and wont help in better tuning the hyper parameters.","5ed31782":"df_target = train_df[['Class']]\n\ndf_predictors = train_df[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']]\nprint(df_predictors)","a5a83f20":"# Performing SMOTE on the training data\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE()\nprint(df_predictors.shape)\nprint(df_target.shape)\nX_train_smote, y_train_smote = smote.fit_sample(df_predictors.astype('float'), df_target)\nprint(X_train_smote.shape)\nprint(y_train_smote.shape)","f4d03040":"# We convert the normal numpy array to DataFrame for input to XGBoost\nX_train_smote_xg_df = pd.DataFrame(X_train_smote, columns=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount'])\ny_train_smote_xg_df = pd.DataFrame(y_train_smote, columns=['Class'])\nprint(type(X_train_smote_xg_df))","ef25690b":"params = {\n          'boosting_type': 'gbdt',\n          'objective': 'binary',\n          'metric':'auc',\n          'learning_rate': 0.05,\n          'num_leaves': 7,  # we should let it be smaller than 2^(max_depth)\n          'max_depth': 4,  # -1 means no limit\n          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n          'max_bin': 100,  # Number of bucketed bin for feature values\n          'subsample': 0.9,  # Subsample ratio of the training instance.\n          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n          'nthread': 8,\n          'verbose': 0,\n          'scale_pos_weight':150, # because training data is extremely unbalanced \n         }","2419bc16":"dtrain = lgb.Dataset(df_predcitors, \n                     label=df_target,\n                     feature_name=predictors)\n\ndvalid = lgb.Dataset(valid_df[predictors].values,\n                     label=valid_df[target].values,\n                     feature_name=predictors)","a9e4eb99":"evals_results = {}\n\nmodel_no_SMOTE = lgb.train(params, \n                  dtrain, \n                  valid_sets=[dtrain, dvalid], \n                  valid_names=['train','valid'], \n                  evals_result=evals_results, \n                  num_boost_round=MAX_ROUNDS,\n                  early_stopping_rounds=2*EARLY_STOP,\n                  verbose_eval=VERBOSE_EVAL, \n                  feval=None)","14553565":"preds_no_smote = model.predict(test_df[predictors])","7b657de6":"roc_auc_score(test_df[target].values, preds)","25a2e8aa":"params = {\n          'boosting_type': 'gbdt',\n          'objective': 'binary',\n          'metric':'auc',\n          'learning_rate': 0.05,\n          'num_leaves': 7,  # we should let it be smaller than 2^(max_depth)\n          'max_depth': 4,  # -1 means no limit\n          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n          'max_bin': 100,  # Number of bucketed bin for feature values\n          'subsample': 0.9,  # Subsample ratio of the training instance.\n          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n          'nthread': 8,\n          'verbose': 0,\n          'scale_pos_weight':150, # because training data is extremely unbalanced \n         }","a76915f5":"dtrain = lgb.Dataset(X_train_smote_xg_df, \n                     label=y_train_smote_xg_df,\n                     feature_name=predictors)\n\ndvalid = lgb.Dataset(valid_df[predictors].values,\n                     label=valid_df[target].values,\n                     feature_name=predictors)","095b12bf":"evals_results = {}\n\nmodel = lgb.train(params, \n                  dtrain, \n                  valid_sets=[dtrain, dvalid], \n                  valid_names=['train','valid'], \n                  evals_result=evals_results, \n                  num_boost_round=MAX_ROUNDS,\n                  early_stopping_rounds=2*EARLY_STOP,\n                  verbose_eval=VERBOSE_EVAL, \n                  feval=None)\n","f418d259":"preds = model.predict(test_df[predictors])","79e7b117":"roc_auc_score(test_df[target].values, preds)","d795296b":"# <a id=\"5\"> The real transaction have a larger mean value, larger Q1, smaller Q3 and Q4 and larger outliers; fraudulent transactions have a smaller Q1 and mean, larger Q4 and smaller outliers.<\/a>\n","5b977862":"# <a id=\"2\">Loading all the required packages and libraries and also defining some constants<\/a>","7c23cda2":"### Area under curve\n\nLet's calculate the ROC-AUC score for the prediction.","2f6342d6":"There is no missing data in the entire dataset.","c758c61f":"### Define predictors and target values\n\nLet's define the predictor features and the target features. Categorical features, if any, are also defined. In our case, there are no categorical feature.","7ac6159d":"Looking to the **Time** feature, we can confirm that the data contains **284,807** transactions, during 2 consecutive days (or **172792** seconds).","70f93ecf":"# <a id=\"8\">References<\/a>\n\n[1] Credit Card Fraud Detection Database, Anonymized credit card transactions labeled as fraudulent or genuine, https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud  \n[2] Principal Component Analysis, Wikipedia Page, https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis  \n[3] RandomForrestClassifier, http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html  \n[4] ROC-AUC characteristic, https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic#Area_under_the_curve   \n[5] AdaBoostClassifier, http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html  \n[6] CatBoostClassifier, https:\/\/tech.yandex.com\/catboost\/doc\/dg\/concepts\/python-reference_catboostclassifier-docpage\/  \n[7] XGBoost Python API Reference, http:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html  \n[8] LightGBM Python implementation, https:\/\/github.com\/Microsoft\/LightGBM\/tree\/master\/python-package  \n[9] LightGBM algorithm, https:\/\/www.microsoft.com\/en-us\/research\/wp-content\/uploads\/2017\/11\/lightgbm.pdf   \n\n","ecd35772":"Let's start with a RandomForrestClassifier <a href='#8'>[3]<\/a>   model.","58203a62":"# <a id=\"4\">Check the data for the number of rows and columns<\/a>","b0269504":"## <a id=\"64\">LightGBM<\/a>\n\n\nLet's continue with another gradient boosting algorithm, LightGBM <a href='#8'>[8]<\/a> <a href='#8'>[9]<\/a>.\n\n\n### Define model parameters\n\nLet's set the parameters for the model. We will use these parameters only for the first lgb model.","bf46c9af":"# <a id=\"6\">Predictive models<\/a>  \n\n","70fc9ef3":"Let's plot the fraudulent transactions (amount) against time. The time is shown is seconds from the start of the time period (totaly 48h, over 2 days).","c2731585":"For some of the features we can observe a good selectivity in terms of distribution for the two values of **Class**: **V4**, **V11** have clearly separated distributions for **Class** values 0 and 1, **V12**, **V14**, **V18** are partially separated, **V1**, **V2**, **V3**, **V10** have a quite distinct profile, whilst **V25**, **V26**, **V28** have similar profiles for the two values of **Class**.  \n\nIn general, with just few exceptions (**Time** and **Amount**), the **features distribution for legitimate transactions (values of Class = 0)  is centered around 0**, sometime with a long queue at one of the extremities. In the same time, the fraudulent transactions (values of **Class = 1**) have a skewed (asymmetric) distribution.","586ac4ba":"## Features correlation","d7e480ec":"Let's check data imbalance with respect to the  **target** value, i.e. **Class**.","2153ba1f":"Now we divide the predictors and target columns","d55bb98f":"# <a id=\"1\">Introduction<\/a>  \n\nThe datasets contains transactions made by credit cards in **September 2013** by european cardholders. This dataset presents transactions that occurred in two days, where we have **492 frauds** out of **284,807 transactions**. The dataset is **highly unbalanced**, the **positive class (frauds)** account for **0.172%** of all transactions.  \n\nIt contains only numerical input variables which are the result of a **PCA transformation**.   \n\nDue to confidentiality issues, there are not provided the original features and more background information about the data.  \n\n* Features **V1**, **V2**, ... **V28** are the **principal components** obtained with **PCA**;  \n* The only features which have not been transformed with PCA are **Time** and **Amount**. Feature **Time** contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature **Amount** is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.   \n* Feature **Class** is the response variable and it takes value **1** in case of fraud and **0** otherwise.  \n\n","ba63f6d5":"# <a id=\"7\">Conclusions<\/a>","cf627486":"### Split data in train, test and validation set\n\nLet's define train, validation and test sets.","987709c0":"The ROC-AUC score obtained for the test set is **0.946**.\n\nWith SMOTE The ROC-AUC score obtained for the test set is **0.9768434103619289**. Therefore accuracy increases with SMOTE. ","9a103553":"We investigated the data, checking for data unbalancing, visualizing the features and understanding the relationship between different features. \nWe then investigated two predictive models. The data was split in 3 parts, a train set, a validation set and a test set. For the first three models, we only used the train and test set.  \n\nWe started with **RandomForrestClassifier**, for which we obtained an AUC scode of **0.85** when predicting the target for the test set.  \n\nWe followed with an **AdaBoostClassifier** model, with lower AUC score (**0.83**) for prediction of the test set target values.    \n\nWe then followed with an **CatBoostClassifier**, with the AUC score after training 500 iterations **0.86**.    \n\nWe then experimented with a **XGBoost** model. In this case, se used the validation set for validation of the training model.  The best validation score obtained was   **0.984**. Then we used the model with the best training step, to predict target value from the test data; the AUC score obtained was **0.974**.\n\nWe then presented the data to a **LightGBM** model. We used both train-validation split and cross-validation to evaluate the model effectiveness to predict 'Class' value, i.e. detecting if a transaction was fraudulent. With the first method we obtained values of AUC for the validation set around **0.974**. For the test set, the score obtained was **0.946**.   \nWith the cross-validation, we obtained an AUC score for the test prediction of  **0.93**.","a711b99e":"### Run the model\n\nLet's run the model, using the **train** function.","bd0d1b5d":"Best validation score  was obtained for round **96**, for which **AUC ~= 0.999185**.\nWith Best validation score  was obtained for round **96**, for which **AUC ~= 0.984437**.\n\nTherefore validation score increases with SMOTE\n\nLet's plot variable importance.","5da9acce":"<h1><center><font size=\"6\">Credit Card Fraud Detection Predictive Models<\/font><\/center><\/h1>\n\n\n<center><img src=\"https:\/\/images.unsplash.com\/photo-1563013544-824ae1b704d3?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80\" width=\"600\"><\/img><\/center>\n\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Load packages<\/a>  \n- <a href='#3'>Read the data<\/a>  \n- <a href='#4'>Check the data<\/a>  \n    - <a href='#41'>Glimpse the data<\/a>  \n    - <a href='#42'>Check missing data<\/a>\n    - <a href='#43'>Check data unbalance<\/a>\n- <a href='#5'>Data exploration<\/a>\n- <a href='#6'>Predictive models<\/a>  \n    - <a href='#61'>RandomForrestClassifier<\/a> \n    - <a href='#62'>AdaBoostClassifier<\/a>     \n    - <a href='#63'>CatBoostClassifier<\/a> \n    - <a href='#64'>XGBoost<\/a> \n    - <a href='#65'>LightGBM<\/a> \n- <a href='#7'>Conclusions<\/a>\n- <a href='#8'>References<\/a>\n","4b5fda46":"We can confirm that the two couples of features are inverse correlated (the regression lines for **Class = 0** have a negative slope while the regression lines for **Class = 1** have a very small negative slope).\n\nConclusion - If the value of the feature **decreases** then the amount is also **decreases**.\n","a58e1ab6":"Only **492** (or **0.172%**) of transaction are fraudulent. That means the data is highly unbalanced with respect with target variable **Class**.","8ed44bcc":"Fraudulent transactions have a distribution more even than valid transactions - are equaly distributed in time, including the low real transaction times, during night in Europe timezone.","72c0c831":"## <a id=\"41\">Printing the head of the data<\/a>\n\nWe start by looking to the data features (first 5 rows).","c7bc93f2":"# <a id=\"3\">Reading the data from the CSV file<\/a>","f3f6f333":"## Transactions in time","bdfba1fd":"We can confirm that the two couples of features are correlated (the regression lines for **Class = 0** has a larger positive slope, whilst the regression line for **Class = 1** have a smaller positive slope).\n\nConclusion - If value of the feature value **increases**, then the amount **increases**.\n\nLet's plot now the inverse correlated values.","66353d9f":"## <a id=\"41\">Here we describe all the attributes of the data.<\/a>\nLet's look into more details to the data.","2e755310":"Let's predict now the target for the test data.\n\n### Predict test data","545d00f0":"## Transactions amount","1e6e86eb":"### Prepare the model\n\nLet's prepare the model, creating the **Dataset**s data structures from the train and validation sets.","198fc22e":"## <a id=\"64\">Implementing LightGBM without SMOTE<\/a>","74aed275":"# <a id=\"5\"> Now, we have the amount distribution, with respect to the classes.<\/a>","56671b5a":"# <a id=\"5\">Data exploration<\/a>","879ee83a":"## <a id=\"43\">We plot a bar-graph to check for the data imbalance in the entire dataset<\/a>","6246803a":"## <a id=\"42\">Lets check if any data is missing in any column or row.<\/a>  ","8575b72e":"As expected, there is no notable correlation between features **V1**-**V28**. There are certain correlations between some of these features and **Time** (inverse correlation with **V3**) and **Amount** (direct correlation with **V7** and **V20**, inverse correlation with **V1** and **V5**).\n\n\nLet's plot the correlated and inverse correlated values on the same graph.\n\nLet's start with the direct correlated values: {V20;Amount} and {V7;Amount}.","7affe37d":"## Features density plot"}}