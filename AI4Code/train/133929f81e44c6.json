{"cell_type":{"93847de4":"code","71533f69":"code","ddd35e3a":"code","311bb9f3":"code","6dd6ecc0":"code","119ec7fb":"code","988132ab":"code","e7557327":"code","669c0630":"code","a06071ac":"code","6ed7696b":"markdown","47725ab1":"markdown","cacc0065":"markdown","a8785bd5":"markdown","5aacfdb0":"markdown","b7252d63":"markdown","df83b7d3":"markdown","5ad8f04d":"markdown","dc583b04":"markdown","0cf9e7f4":"markdown","97c27ea5":"markdown","79dc98d3":"markdown"},"source":{"93847de4":"! conda install -y -c conda-forge hvplot==0.5.2 bokeh==1.4.0","71533f69":"import numba\nimport numpy as np\nimport pandas as pd\nimport hvplot.pandas\nimport holoviews as hv\nfrom operator import add\nfrom toolz.curried import *\n\n\nhv.extension('bokeh')","ddd35e3a":"@numba.jit(nopython=True, nogil=True)\ndef factorial(n):\n    if n < 0:\n        return 0\n    elif n < 20:\n        val = 1\n        for k in range(1,n+1):\n            val *= k\n        return val\n    else:\n        return np.math.gamma(n)\n    \n@numba.jit(nopython=True, nogil=True)\ndef choose(n, r):\n    return factorial(n)\/(factorial(n-r)*factorial(r))\n\n@numba.jit(nopython=True, nogil=True)\ndef legendre(x:np.ndarray, n:int=1):\n    L = np.zeros(x.shape)\n    for k in range(0, n+1):\n        L = L + choose(n,k) * choose(n+k, k) * (((x-1)\/2)**k)\n    return L\n\n@numba.jit(nopython=True, nogil=True)\ndef basis(x: np.ndarray, d:int = 2):\n    L = legendre(x, n=0).reshape(-1,1)\n    for n in range(1,d+1):\n        L = np.hstack((L,legendre(x, n).reshape(-1,1)))\n    return L","311bb9f3":"%%timeit\nbasis(np.random.uniform(-1, 1, 500), d=5)","6dd6ecc0":"(pd.DataFrame(basis(x=np.random.uniform(-1, 1, 500), d=5))\n .assign(x=lambda d: d.loc[:,1])\n .sort_values('x')\n .melt(id_vars='x', var_name=['Order'])\n .hvplot.line(x='x', y='value', groupby='Order', title='Different Order Legendre Polynomials')\n .overlay())","119ec7fb":"\ndef plot(function=basis, degree=5, x=np.random.uniform(-1, 1, 500)):\n    return ((pd.DataFrame(function(x, d=degree), index=['Legendre' for _ in range(x.shape[0])])\n             .fillna(0)\n             .dot(np.random.uniform(-1,1, size=(degree+1,3)))\n             .assign(x=x.repeat(1))\n             .reset_index()\n             .rename(columns={'index':'Order'})\n              .melt(id_vars=['Order','x'], value_name='f(x)')\n              .sort_values('x')\n             .hvplot.line(x='x', y='f(x)', color='red', label='Legendre', groupby=['variable'])\n             .overlay()\n             .opts(show_legend=False)) *\n           (pd.DataFrame({d:x**d for d in range(degree+1)}, index=['Polynomial' for _ in range(x.shape[0])])\n             .fillna(0)\n             .dot(np.random.uniform(-1,1, size=(degree+1,3)))\n             .assign(x=x.repeat(1))\n             .reset_index()\n             .rename(columns={'index':'Order'})\n              .melt(id_vars=['Order','x'], value_name='f(x)')\n              .sort_values('x')\n             .hvplot.line(x='x', y='f(x)', color='green', label=\"Polynomial\", groupby=['variable'])\n             .overlay()\n             .opts(show_legend=False))).opts(title='degree ' + str(degree))\n\nx = np.random.uniform(-1, 1, 500)\nreduce(add, [plot(degree = d, x=x) for d in range(4)]).cols(2)","988132ab":"@numba.jit()\ndef E(num_projections =1000,\n      X_in = basis(np.random.uniform(-1, 1, 51),10),\n      X_out = basis(np.random.uniform(-1, 1, 100),10),\n      e_in = np.random.multivariate_normal(np.zeros(90), \n                                       np.diag(np.linspace(0.2,1.1, 90)), \n                                       111),\n      e_out = np.random.multivariate_normal(np.zeros(90), \n                                        np.diag(np.linspace(0.2,1.1, 90)), \n                                        100),\n      N = 10,\n      order = 2):\n    x_in_true = np.ascontiguousarray(X_in[:N,:order])\n    x_out_true = np.ascontiguousarray(X_out[:,:order])\n    X_in_2 = np.ascontiguousarray(X_in[:N,:2])\n    X_out_2 = np.ascontiguousarray(X_out[:,:2])\n    X_in_10 = np.ascontiguousarray(X_in[:N,:10])\n    x_out_10 =np.ascontiguousarray(X_out[:,:10])\n    e_in = np.ascontiguousarray(e_in[:N,:])\n    \n    E = np.zeros((num_projections, e_in.shape[1]))\n    for run in numba.prange(num_projections):\n        B_true = np.random.uniform(-1,1,size=(order,))\n        y_true_in = (x_in_true@B_true).reshape(-1,1) + e_in\n        y_true_out = (x_out_true@B_true).reshape(-1,1) + e_out\n\n        B_2 = np.linalg.pinv(X_in_2.T @ X_in_2) @ X_in_2.T @ y_true_in\n        mse_2 = np.ones((1,X_out_2.shape[0])) @ (y_true_out -  X_out_2 @ B_2)**2 \/ X_out_2.shape[0]\n\n        B_10 = np.linalg.pinv(X_in_10.T @ X_in_10) @ X_in_10.T @ y_true_in\n        mse_10 = np.ones((1,X_out_2.shape[0])) @ (y_true_out - x_out_10 @ B_10)**2 \/ X_out_2.shape[0]\n\n        E[run,:] = mse_10 - mse_2\n\n    return (np.ones((1,num_projections)) @ E \/num_projections).reshape(-1,)\n\n@numba.jit()\ndef experiment_2(order:int = 5, \n                  sigma:float = 0.2,\n                  sample_sizes_lower = 20,\n                  sample_sizes_upper = 50,\n                  x_in_seed:np.ndarray = np.random.uniform(-1, 1, 111),\n                  e_in:np.ndarray = np.random.multivariate_normal(np.zeros(90), \n                                                                   np.diag(np.linspace(0.2,1.1, 90)), \n                                                                   111),\n                  x_out_seed:np.ndarray = np.random.uniform(-1, 1, 100),\n                  e_out:np.ndarray = np.random.multivariate_normal(np.zeros(90), \n                                                                    np.diag(np.linspace(0.2,1.1, 90)), \n                                                                    100),\n                  num_projections=1000):\n    \n    \n    max_order = max(2,10, order)\n    \n    X_in = (basis(x=x_in_seed, d=max_order))\n    X_out = (basis(x=x_out_seed, d=max_order))\n\n    samples_sizes = np.arange(sample_sizes_lower, sample_sizes_upper)\n    results = np.zeros(((samples_sizes.shape[0]), \n                        e_in.shape[1]))\n    for index in numba.prange((samples_sizes.shape[0])):\n        results[index, :] = E(num_projections=num_projections,\n                              X_in=X_in,\n                              X_out=X_out,\n                              e_in = e_in,\n                              e_out = e_out,\n                              N=samples_sizes[index],\n                              order=5)\n        \n    return results","e7557327":"results2 = experiment_2()\n\ndf2 = (pd.DataFrame(data=results2,\n                    index=list(range(20, 50)),\n                    columns=np.linspace(0.2,1.1, 90))\n      .iloc[:,:]\n     .reset_index()\n     .rename(columns={'index':\"N\"})\n     .melt(id_vars='N',var_name='Sigma'))\n\ndataset2 = hv.Dataset(df2, vdims=[('value','E')])\n\nheatmap2 = hv.HeatMap(dataset2.aggregate(['Sigma', 'N'], np.sum),\n                     label='Out-of-Sample error')\n\nheatmap2.opts(width=800, \n              height=600, \n              colorbar=True,\n              xrotation=90,\n              logz=True,\n              colorbar_opts={'title':'E'},\n              tools=['hover'])","669c0630":"@numba.jit()\ndef experiment_1(true_order:int = np.array(list(range(1,41))), \n      sigma:float = 0.2,\n      sample_sizes = np.array(list(range(20,111))),\n      x_in_seed:np.ndarray = np.random.uniform(-1, 1, 111),\n      e_in:np.ndarray = np.random.normal(0, 0.2, 111),\n      x_out_seed:np.ndarray = np.random.uniform(-1, 1, 100),\n      e_out:np.ndarray = np.random.normal(0, 0.2, 100),\n      num_projections=1000):\n    \n    max_order = np.max(true_order)\n    \n    X_in = (basis(x=x_in_seed, d=max_order))\n    X_out = (basis(x=x_out_seed, d=max_order))\n    \n    H = []\n    for N in sample_sizes:\n        R = np.zeros(true_order.shape[0])\n        for index, order in np.ndenumerate(true_order):\n            x_in_true = np.ascontiguousarray(X_in[:N,:order])\n            x_out_true = np.ascontiguousarray(X_out[:,:order])\n            X_in_2 = np.ascontiguousarray(X_in[:N,:2])\n            X_out_2 = np.ascontiguousarray(X_out[:,:2])\n            X_in_10 = np.ascontiguousarray(X_in[:N,:10])\n            x_out_10 =np.ascontiguousarray(X_out[:,:10])\n\n            E = 0\n            for run in numba.prange(num_projections):\n                B_true = np.random.uniform(-1,1,size=(order,))\n                y_true_in = (x_in_true@B_true + np.ascontiguousarray(e_in[:N]))\n                y_true_out = (x_out_true@B_true + e_out)\n\n                B_2 = np.linalg.pinv(X_in_2.T @ X_in_2) @ X_in_2.T @ y_true_in\n                mse_2 = np.mean((y_true_out -  X_out_2 @ B_2)**2)\n\n                B_10 = np.linalg.pinv(X_in_10.T @ X_in_10) @ X_in_10.T @ y_true_in\n                mse_10 = np.mean((y_true_out - x_out_10 @ B_10)**2)\n\n                E = E + mse_10 - mse_2\n\n            R[index] = E\/num_projections\n        H.append(R)\n        \n    return H","a06071ac":"results1 = experiment_1()\n\ndf1 = (pd.DataFrame(data=np.vstack(results1),\n              index=np.array(list(range(20,111))),\n              columns=np.array(list(range(1,41))))\n     .reset_index()\n     .rename(columns={'index':\"N\"})\n     .melt(id_vars='N',var_name='Qf'))\n\ndf1.value.hvplot.hist(xlabel='E')\n\ndataset1 = hv.Dataset(df1, vdims=[('value','E')])\n\nheatmap1 = hv.HeatMap(dataset1.aggregate(['Qf', 'N'], np.sum),\n                     label='Out-of-Sample error').opts(xlabel='Order of Polynomial')\n\nheatmap1[:10, :].opts(width=800, \n                     height=600, \n                     colorbar=True,\n                      logz=True,\n                     colorbar_opts={'title':'E'},\n                     tools=['hover'])","6ed7696b":"\\begin{equation} \\label{eq:1}\n\\begin{array}{rl}\n& ((1-x^2)u')' +\\lambda u  = 0, \\quad -1<x<1, \\\\\n& \\lambda_n=n(n+1).\n\\end{array}\n\\end{equation}\n\nLegendre polynomials are eigen functions for the differential equation shown in the equation above. \n\n\\begin{equation} \\label{eq:2}\nL_q(x)=\\Sigma_{k=1}^N{{n}\\choose{k}}{{n+k}\\choose{k}}(\\frac{x-1}{2})^k\n\\end{equation}\n\nThis function provides an orthogonal polynomial basis function over the domain $-1\\leq x\\leq 1.$, which can be computed using the formula in the equation above. The reason orthogonality is important is that we are using Ordinary Least Squares Regression and risk our models not fitting bias in our estimates due to extreme multicollinearity.  ","47725ab1":"Last year, I took an amazing Machine Learning course. The course provided a strong theoretical grounding in Machine Learning, covering many important concepts in not only Bias-Variance decomposition but in the impact of Vapnik\u2013Chervonenkis Theory on model selection and out-of-sample error.  The course followed, in-depth, Abu Mustafa's Learning From Data textbook and mirrored, in parts, his amazing [Caltech Lecture Series](https:\/\/www.youtube.com\/watch?v=mbyG85GZ0PI).  One of the first practical experiments covered in the course was a simulation exercise designed to analyze the impact and dangers of overfitting. This exercise mirrors one of the exercises presented in Learning From Data and forms the inspiration for the book's cover art. \n\n[Numba](http:\/\/numba.pydata.org) is a Just-in-Time (JIT) compiler which translates Python functions into optimized machine code at runtime using an industry-standard LLVM compiler library.  Using this JIT computer, we can easily release Python's Global Interpreter Lock (GIL) and run fast parallel loops which have major performance advantages.\n\nThis notebook forms an extension to Abu Mustafa's simulation exercise, with a few simplifications here and there. The aim of this notebook is not just to provide insight into model selection and overfitting, but also to provide a brief tutorial on Numba. Using Numba required a lot more work as there were a number of functions which needed to be coded from scratch, like the factorial function and a choose operation, as they lack implementation or compatibility with Numba. The Numba project has come a long way since its release and now features compatibility and integration with Pandas and the Umap-learn Library.  ","cacc0065":"# Conclusion\nBeing able to simulate the effects of noise and model complexity on out-of-sample can be critical to our understanding.  While in this notebook, we focussed on simple functions and relatively constrained feature spaces, we can easily imagine how these results would transfer to high non-linear models in practice. While this notebook did not spend a huge amount of time walking through how to use Numba, I hope it gave a sense of its applications and patterns of use and has inspired where you can use it in your projects and products. ","a8785bd5":"# The Excercise\nIn this notebook, we are going to investigate the effects of noise, sample size and function complexity on out-of-sample error. In order to do this, we are going to randomly sample functions of increasing complexity, add noise to them and then try and fit models on basis of varying complexity.  We are going to construct a test set and a training set, and we are going to fit our linear model on the training set and evaluate it's out of sample error. In order to visualize its performance, we are going to plot a heatmap of the out-of-sample error against our sample sizes, function complexity and noise.  This exercise should demonstrate how complex models overfit to noise and the importance of model austerity in the production setting.  \n\nWe are going to be running hundreds of thousands of simulations in this notebook.  In order to run these simulations effectiveness, we will have to try out best to speed up Python's numerical capabilities, and for this, we will be using Numba. In order to generate our data for our simulation, we will be sampling data from a uniform distribution between -1 and 1. This data will then be used to compute Legendre Polynomials, which we will use as both the basis for generating our response variable as well as fitting the models we will be estimating. ","5aacfdb0":"In our first simulation, we are going to explore primarily sample size and out-of-sample error.  In order to do this, we are going to fix the complexity of the functions we are sampling from 5th order polynomials. The noise we add to our functions is going to be Gaussian distributed, and the weights we use to sample out functions are going to be uniformly distributed between -1 and 1. For each sample size and sigma value, we are going to run 1000 simulations- meaning we sample 1000 noisy functions and fit 1000 models.  What we are looking to evaluate in our heatmap is the difference in mean squared error between a model fit using a tenth-order polynomial basis and second-order polynomial basis and how it varies based on the number of training data we have and the amount of noise we add to our model. ","b7252d63":"### The Effects of Noise","df83b7d3":"# The Effects of Function Complexity\nFor our second experiment, we are going to look at the effects of function complexity on overfitting. We are going to be running a very similar simulation to our previous experiment except here we are doing to fix our noise ($\\sigma$) at 0.2, and we are going to vary the degree of the polynomials we are simulating and fitting. ","5ad8f04d":"In the plot below, you can see the shape of Legendre Polynomials of different orders. ","dc583b04":"Compared to ordinary polynomials, the Legendre Polynomials have quite a different shape, which becomes obvious when we compare random samples of these polynomials. Our ordinary polynomials tend to all look very similar as $x$, $x^2$ and $x^3$ are all highly correlated between 0 and 1. Using the Legendre Polynomials, we see far greater variability in our functions in this range. ","0cf9e7f4":"Looking at our plot, we can see that the higher the noise and the smaller the training set, the greater the out-of-sample error. This is fairly intuitive for anyone we have worked in statistical analysis before or has completed in a Kaggle competition.  The more noise our data and the less data we have, the higher our chance of over-fitting. ","97c27ea5":"Here, based on our simulation we can see that the while our higher-order polynomial tests to fit well to our complex model, on simple models with noise, the tenth order Legendre Polynomial Bases tends to overfit to the noise in the data and provide poor results. ","79dc98d3":"You can see how insanely fast our JIT-compiled node is, previously using plain Python this code was around 50x slower.  The reason we have such high variance is that the first call on our JIT functions compiles them after which we can benefit from the speed. "}}