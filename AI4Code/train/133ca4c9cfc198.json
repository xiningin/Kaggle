{"cell_type":{"ccd35469":"code","21f535dd":"code","f7ca4ee0":"code","df6fce74":"code","03de7e0e":"code","3245529e":"code","d13bc6f4":"code","943b84da":"code","53ac4104":"code","96261445":"code","6104e622":"code","b6c5bf64":"code","4ee913d2":"code","ca1cb5a2":"code","ce716420":"code","e17389de":"code","316d28ef":"code","23e0a603":"code","4b174df5":"code","4a659bd4":"code","ca249e5e":"code","bb41b24c":"code","62d34398":"code","70a1b60f":"code","bf1ef657":"code","b9f01ecc":"code","af79cdde":"code","ac33c428":"code","357a5962":"code","3b6a2816":"code","be495424":"code","904b75f0":"code","e83870d8":"code","3c8aace9":"code","388fe6df":"code","665b1db1":"code","6cc2e4ce":"code","8dacd35a":"code","2ee61b38":"code","9bc76045":"markdown","d5f63219":"markdown","c746d749":"markdown","25a5a760":"markdown","6b394bd4":"markdown","302648da":"markdown","500b2d30":"markdown","10fc757a":"markdown","7e136d7f":"markdown","891776a7":"markdown","b769462d":"markdown","79b5f3f1":"markdown","2e7a8074":"markdown","4b9d8fbb":"markdown","1ba5e443":"markdown","cd19185a":"markdown","b0b613f1":"markdown","bbea9cb1":"markdown","6a38072f":"markdown","cc553af4":"markdown","38835f7e":"markdown","4832ea84":"markdown","02bb25e4":"markdown"},"source":{"ccd35469":"#\u0130htiyac\u0131m\u0131z olan k\u00fct\u00fcphaneleri aktif ediyoruz\n#We are including packages\nimport numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom scipy.stats import shapiro\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","21f535dd":"#Verimizi df de\u011fi\u015fkenine atad\u0131k \n#we will denote with df to our dataset \ndf=pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\").copy()\ndf.head()\n","f7ca4ee0":"df=df.drop([\"id\",'Unnamed: 32'],axis=1)#We removed unnecessary variables\n#Gereksiz de\u011fi\u015fkenlerimiz \u00e7\u0131kard\u0131k","df6fce74":"df.info()#De\u011fi\u015fkenlerimizin bi\u00e7imlerinin do\u011fru oldu\u011funu g\u00f6r\u00fcyoruz, diagnosis i belki sonradan kategorik yapabiliriz fakat object ten s\u0131k\u0131nt\u0131 olmaz\n","03de7e0e":"df.describe().T#Veri setimizin betimsel istatistikleri \n","3245529e":"df.isnull().values.sum()#Veri setimizde eksik veri olmad\u0131\u011f\u0131n\u0131 \u00f6\u011freniyoruz\n#We haven't any missing value","d13bc6f4":"sns.boxplot(df.radius_mean,df.diagnosis)\n#We are seeing outliers","943b84da":"sns.boxplot(df.smoothness_mean,df.diagnosis)#Veri setimizi inceledi\u011fimizde ayk\u0131r\u0131 g\u00f6zlemlerin oldu\u011funu g\u00f6r\u00fcyoruz\n","53ac4104":"##Bask\u0131lama Y\u00f6ntemiyle Ayk\u0131r\u0131 de\u011ferlerin \u00e7\u00f6z\u00fcm\u00fc\nfor i in range(1,len(df.columns)):\n    Q1=df.iloc[:,i].quantile(0.25)\n    Q3=df.iloc[:,i].quantile(0.75)\n    IQR=Q3-Q1\n    alt_sinir=Q1-1.5*IQR\n    ust_sinir=Q3+1.5*IQR\n    df.iloc[:,i][(df.iloc[:,i]<alt_sinir)]=alt_sinir\n    df.iloc[:,i][(df.iloc[:,i]>ust_sinir)]=ust_sinir\n##Yukar\u0131da bask\u0131lama y\u00f6ntemi yapt\u0131k yani veri setimizdeki ayk\u0131r\u0131 g\u00f6zlemleri en yak\u0131n oldu\u011fu s\u0131n\u0131r noktas\u0131na e\u015fitledik\n##Outliers is assigned  to upper and lower limits ","96261445":"sns.boxplot(df.radius_mean)#Now we can see clean boxplot ","6104e622":"sns.boxplot(df.smoothness_mean)#G\u00f6rd\u00fc\u011f\u00fcm\u00fcz gibi veri setimizde u\u00e7 noktalar bask\u0131lama y\u00f6ntemiyle temizlendi","b6c5bf64":"df.describe().T #Temizlenen verimizin betimsel istatistiklerine tekrardan bakal\u0131m\n#Verinin ilk haliyle inceledi\u011fimiz standart sapmalarda azalma oldu\u011funu g\u00f6r\u00fcyoruz\n#Biraz daha normal da\u011f\u0131l\u0131ma do\u011fru y\u00f6neldi\u011fi farkediliyor","4ee913d2":"sns.countplot(df.diagnosis)#Kanser t\u00fcrlerinin veri setindeki say\u0131lar\u0131\n","ca1cb5a2":"sns.pairplot(df,hue=\"diagnosis\",kind=\"scatter\")\n#  Verimizdeki her de\u011fi\u015fkenin birbiriyle olan ili\u015fkilerini kategorik de\u011fi\u015fkenlerimizin k\u0131r\u0131l\u0131m\u0131nda inceledi\u011fimiz zaman\n#Melignant grubunun de\u011fi\u015fkenlerin \u00e7o\u011funda incelendi\u011finde Bening grubuna g\u00f6re daha b\u00fcy\u00fck  de\u011ferler ald\u0131\u011f\u0131 g\u00f6r\u00fcl\u00fcyor\n#  De\u011fi\u015fkenlerin da\u011f\u0131l\u0131m\u0131n\u0131 inceledi\u011fimizde normal da\u011f\u0131l\u0131m gibi duruyor fakat da\u011f\u0131l\u0131m grafi\u011fine bak\u0131ld\u0131\u011f\u0131nda bas\u0131kl\u0131\u011f\u0131n\u0131n \n#\u00e7ok d\u00fc\u015f\u00fck oldu\u011fu yani sivrilikler g\u00f6z\u00fcm\u00fcze \u00e7arp\u0131yor Shapiro_Wilk testi yap\u0131lmas\u0131nda fayda var.","ce716420":"sns.distplot(df.radius_mean,label=\"radius\")\nsns.distplot(df.texture_mean,label=\"texture\")\nplt.legend()","e17389de":"shapiro(df.texture_mean)#Ne kadar texture de\u011fi\u015fkeni normal da\u011f\u0131lm\u0131\u015f gibi de dursa shapiro testimizin p-value de\u011feri 0.05 den\n#k\u00fc\u00e7\u00fck oldu\u011funda H0:Veriler normal da\u011f\u0131l\u0131m g\u00f6sterir hipotezi red edilir","316d28ef":"df.corr()#Veri setimizin koralesyon grafi\u011fi\n#Correlation graphic","23e0a603":"plt.figure(figsize=(20, 20))\nsns.heatmap(df.corr(), annot=True)\n#we can see better like this","4b174df5":"sns.jointplot(x=df.symmetry_mean,y=df.symmetry_worst,kind=\"reg\")","4a659bd4":"X=df.drop([\"diagnosis\"],axis=1).copy()\ny=pd.DataFrame(df.diagnosis,dtype=\"category\")\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)\nlgbm=RandomForestClassifier().fit(X_train,y_train)\ny_pred=lgbm.predict(X_test)\naccuracy_score(y_test, y_pred)#Modelimizin do\u011fruluk oran\u0131","ca249e5e":"cross_val_score(lgbm, X_test, y_test, cv = 10).mean()#Cross valid skorumuz\n","bb41b24c":"#Modelimiz i\u00e7in en iyi parametreleri buluyoruz\n#We will choose best parameters for our model\nrf_params = {\"max_depth\": [2,5,8,10],\n            \"max_features\": [2,5,8],\n            \"n_estimators\": [10,500,1000],\n            \"min_samples_split\": [2,5,10]}\nrf_model = RandomForestClassifier()\n\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1, \n                           verbose = 2).fit(X_train,y_train)\nprint(\"En iyi parametreler: \" + str(rf_cv_model.best_params_))","62d34398":"#En iyi parametrelerle olu\u015fan yeni modelimiz\nlgbm=RandomForestClassifier(max_depth= 8, max_features= 8, min_samples_split= 5, n_estimators= 1000).fit(X_train,y_train)\ny_pred=lgbm.predict(X_test)\naccuracy_score(y_test, y_pred)","70a1b60f":"cross_val_score(lgbm, X_test, y_test, cv = 10).mean()\n","bf1ef657":"#Temel Bile\u015fen Analiziyle Modelleme Yap\u0131yoruz","b9f01ecc":"pca=df.drop([\"diagnosis\"],axis=1).copy()\nfrom sklearn.preprocessing import StandardScaler\npca1=StandardScaler().fit_transform(pca)","af79cdde":"from sklearn.decomposition import PCA\npca2=PCA(n_components=2)#Ka\u00e7 temel bile\u015fene ayr\u0131laca\u011f\u0131n\u0131 g\u00f6steriyor\npca2_fit=pca2.fit_transform(pca1)\n#We choose two components because we can explain easier than more components and we will see that we will take best scores\n","ac33c428":"bilesendf=pd.DataFrame(data=pca2_fit)\npca2.explained_variance_ratio_.cumsum()#Varyans\u0131 a\u00e7\u0131klama oran\u0131na bak\u0131ld\u0131\u011f\u0131nda 2 bile\u015fenle a\u00e7\u0131klayabildi\u011fini g\u00f6r\u00fcyoruz\n#We have sufficient explained variance ratio and this value must be least %66","357a5962":"plt.plot(pca2.explained_variance_ratio_.cumsum())\n","3b6a2816":"bilesenpca=pd.concat([bilesendf,df.diagnosis],axis=1).copy()\nbilesenpca.columns=[\"birinci_bilesen\",\"ikinci_bilesen\",\"diagnosis\"]\nbilesenpca.head()\n#Temel bile\u015fen analizi sonucunda olu\u015fan veri setimize ba\u011f\u0131ml\u0131 de\u011fi\u015fkenimizi ekleyerek \u00fczerinde modelleme yapacaz.\n#We added our dependent variable to pca dataset","be495424":"#LGBM algoritmas\u0131n\u0131 deniyoruz burada\nk=[]\nl=[]\nfor i in  range(0,30):\n    pca2=PCA(n_components=i)\n    pca2_fit=pca2.fit_transform(pca)\n    bilesendf=pd.DataFrame(data=pca2_fit)\n    X=bilesendf\n    y=df.diagnosis\n    X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.25)\n    lgbm_model=LGBMClassifier().fit(X_train,y_train)\n    k.append(accuracy_score(y_test,lgbm_model.predict(X_test)))\n    l.append(cross_val_score(lgbm_model, X_test, y_test, cv = 10).mean())\nk=pd.DataFrame(k)\nl=pd.DataFrame(l)\nkl=pd.concat([k,l],axis=1)\nkl.columns=[\"accurary\",\"cross\"]\nprint(kl[kl.accurary==max(kl.accurary)].index[0],\". components for best accurary score:\",kl[kl.accurary==max(kl.accurary)])\nprint(\"------------\")\nprint(kl[kl.cross==max(kl.cross)].index[0],\". components  for best cross_val_score de\u011feri:\",kl[kl.cross==max(kl.cross)])","904b75f0":"#RandomForest Algoritmas\u0131n\u0131 deniyoruz\nX=bilesendf\ny=df.diagnosis\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.25)\nlgbm_model=RandomForestClassifier().fit(X_train,y_train)\naccuracy_score(y_test,lgbm_model.predict(X_test))","e83870d8":"cross_val_score(lgbm_model, X_test, y_test, cv = 10).mean()","3c8aace9":"#Accurary score ve cross_val skorlar\u0131na bakt\u0131\u011f\u0131m\u0131z zaman en iyi sonu\u00e7lar\u0131 randomforest algoritmas\u0131 verdi bizde bu algoritmay\u0131 se\u00e7iyoruz\nprint(classification_report(y_test, lgbm_model.predict(X_test)))\n#Classification report","388fe6df":"# import the metrics class\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\n#Kurdu\u011fumuz modeldeki tahminlerin ka\u00e7 tanesinin do\u011fru ve yanl\u0131\u015f oldu\u011fu g\u00f6steriliyor","665b1db1":"proba=pd.DataFrame(lgbm_model.predict_proba(X))\nproba.columns=[\"B\",\"M\"]\nproba.head()#Malignant hastal\u0131\u011f\u0131 olma olas\u0131l\u0131\u011f\u0131 verilmi\u015ftir Yani Tahmin olas\u0131l\u0131klar\u0131","6cc2e4ce":"#Modelimizdeki bilesenlerin \u00f6nem d\u00fczeyleri\nImportance = pd.DataFrame({\"Importance\": lgbm_model.feature_importances_*100},\n                         index = bilesenpca.columns[:2])\nImportance.sort_values(by=\"Importance\",axis=0,ascending=True).plot(kind=\"barh\",color=\"red\")\nplt.xlabel(\"De\u011fi\u015fken \u00d6nem D\u00fczeyleri\")","8dacd35a":"sns.pairplot(data=bilesenpca,kind=\"reg\")#Temel bile\u015fen analizi sonucunda ili\u015fkisiz iki matris olu\u015fmu\u015f","2ee61b38":"sns.distplot(df.texture_mean)","9bc76045":"in above we can see in which components we have best accurary score and cross val score but we used LGBMClassifier ","d5f63219":"When we clean our dataset , mean and standart deviation dicreased. Now we can look distribution of variables.","c746d749":"Cross validation score nedir?\nNormalde veri setimizin belli bir oran\u0131n\u0131 test ve train olarak ikiye ay\u0131r\u0131r\u0131z.Mesela yukar\u0131da verinin %25 lik k\u0131sm\u0131n\u0131 test,\n%75 lik k\u0131sm\u0131n\u0131 ise train seti olarak ay\u0131rd\u0131k. Burada ama\u00e7 bilgisayar\u0131 train setiyle e\u011fitip test veri seti \u00fczerinden ise\ns\u0131nama yapmakt\u0131r. Daha do\u011fru sonu\u00e7lara ve a\u015f\u0131r\u0131 \u00f6\u011frenme sorununun \u00f6n\u00fcne ge\u00e7mek i\u00e7in ise train veri setimizi k tane par\u00e7aya \nay\u0131r\u0131r\u0131z. Mesela yukar\u0131da cv=10 k\u0131sma ay\u0131r\u0131yoruz. 10 par\u00e7adan birini \u00e7\u0131kar\u0131p modelimizi 9 par\u00e7al\u0131k train setiyle e\u011fitim,kalan\nbir par\u00e7ayla validation hatam\u0131z\u0131 \u00f6l\u00e7\u00fcyoruz. Bunu di\u011fer par\u00e7alara da uygulay\u0131p hepsinin ortalamas\u0131n\u0131 al\u0131yoruz. Bu yapt\u0131\u011f\u0131m\u0131z \ni\u015fleme cross validation score ad\u0131 verilir ve bu test hatam\u0131z\u0131n k\u00f6t\u00fc bir tahmin edicisidir ve genelde daha d\u00fc\u015f\u00fck bir do\u011fruluk\noran\u0131 \u00e7\u0131kmas\u0131n\u0131 bekleriz.\n","25a5a760":"### Temel Bile\u015fen Analizi ve Modelleme\n### PCA and Modelling","6b394bd4":"### Modelleme \n### Modelling","302648da":"We are seeing descriptive statistics. We can say some variables have got skewness ,and some variables have got normal distribution. However we need clean to data from outliers and missing values.","500b2d30":"We can see that we take best scores with pca ","10fc757a":"We saw cross_val_score and accuracy score with best parameters, and we will compare after we do pca","7e136d7f":"---TR---\nVerimizin ilk 5 sat\u0131r\u0131n\u0131 bakt\u0131\u011f\u0131m\u0131z zaman id ve Unnamed: 32 isimli iki tane gereksiz de\u011fi\u015fken var bunlar\u0131 veriden \u00e7\u0131karacaz\nAyriyeten bizim ba\u011f\u0131ml\u0131 de\u011fi\u015fkenimiz olan diagnosis'in kategorik de\u011fi\u015fken oldu\u011fu g\u00f6z\u00fcm\u00fcze \u00e7arp\u0131yor ve di\u011fer de\u011fi\u015fkenler float bi\u00e7iminde\n---ENG---\nWhen we look head of our dataset , we have unnecessary variables(id and Unnamed:32). We will remove from dataset. \nDependent variable(diagnosis) is looking like categorical variable.Apart from diagnosis ,the others are looking like float.","891776a7":"### Ayk\u0131r\u0131 G\u00f6zlem Analizi\n### Outliers Analysis","b769462d":"Do\u011fruluk oran\u0131m\u0131z (86+56)\/(86+56+3+4) den 0.95 dir\nHata oran\u0131m\u0131z (4+7)\/(86+56+7) den 0.048 dir\nKesinlik oran\u0131m\u0131z 86\/(86+4) den 0.96 d\u0131r. Kesinlik oran\u0131n\u0131n y\u00fcksek \u00e7\u0131kmas\u0131 ise s\u0131n\u0131fland\u0131rman\u0131n do\u011fru yap\u0131ld\u0131\u011f\u0131n\u0131 g\u00f6sterir.\nAnma oran\u0131m\u0131z ise 86\/(86+3) den 0.97 \u00e7\u0131km\u0131\u015ft\u0131r. Anma oran\u0131m\u0131z\u0131n y\u00fcksek \u00e7\u0131kmas\u0131 pozitif s\u0131n\u0131f\u0131n do\u011fru s\u0131n\u0131fland\u0131r\u0131ld\u0131\u011f\u0131n\u0131 g\u00f6sterir.","79b5f3f1":"Distribution of pca dataset and we didn't see correlation between first variable and second varible because this feature of pca ","2e7a8074":"### Breast Cancer Wisconsin Veri Analizi","4b9d8fbb":"### Modelin Ba\u015far\u0131s\u0131n\u0131n De\u011ferlendirilmesi","1ba5e443":"Kurdu\u011fumuz random forests s\u0131n\u0131fland\u0131rma algoritmas\u0131ndaki en iyi algoritma de\u011ferlerini elde etmek i\u00e7in parametrelere rastgele de\u011fer atayarak tek tek hepsi i\u00e7in farkl\u0131 de\u011ferler verdik. \nEn iyi parametre de\u011ferleri \u00fczerinden bir modelleme yapt\u0131\u011f\u0131m\u0131z zaman do\u011fruluk oranlar\u0131m\u0131z\u0131n artt\u0131\u011f\u0131n\u0131 g\u00f6r\u00fcyoruz.","cd19185a":"---TR---\nDe\u011fi\u015fkenlerimizi dokunmadan modelleme yapt\u0131\u011f\u0131m\u0131zda en y\u00fcksek do\u011fruluk oran\u0131n\u0131 RandomForests s\u0131n\u0131fland\u0131rma algoritmas\u0131 elde etti.\nRandomForests s\u0131n\u0131fland\u0131rmas\u0131 : temeli birden fazla karar a\u011fac\u0131n\u0131n \u00fcretti\u011fi tahminlerin bir aray\u0131 getirilerek de\u011ferlendirilmesine dayan\u0131r.\nRandomForests algoritmas\u0131n\u0131n di\u011fer bir tecih sebebi ise \u00e7oklu do\u011frusal ba\u011flant\u0131 ve a\u015f\u0131r\u0131 \u00f6\u011frenme problemleriyle ba\u015f edebilme \u00f6zelli\u011findendir.\n---ENG---\nI tried  various algorithms but i chose to RandomForests\n","b0b613f1":"According to shapiro test our variables not normal but we won't care because now this is not our topic.","bbea9cb1":"### Temizlenen Verinin Grafiksel Olarak \u0130ncelenmesi\n### Examine Of Clean Data","6a38072f":"Actual Label bizim ger\u00e7ekteki s\u0131n\u0131flar\u0131m\u0131z\nPredicted label tahminlerimizin s\u0131n\u0131flar\u0131d\u0131r.\nDo\u011fruluk:Ger\u00e7ekteki de\u011ferlerimizle tahmin edilen de\u011ferlerin do\u011fru s\u0131n\u0131fland\u0131r\u0131lanlar\u0131n t\u00fcm s\u0131n\u0131fland\u0131r\u0131lanlara b\u00f6l\u00fcnmesidir\nHata oran\u0131:Yapt\u0131\u011f\u0131m\u0131z yanl\u0131\u015f tahminlerin t\u00fcm tahminlere b\u00f6l\u00fcnmesidir.\nKesinlik:0 ken 0 se\u00e7ilen gruplar\u0131m\u0131z\u0131n 0 ken 0 ve 0 ken 1 se\u00e7ilen s\u0131n\u0131flar\u0131n toplam\u0131na b\u00f6l\u00fcnmesidir.\nAnma : 0 ken 0 se\u00e7ilen gruplar\u0131m\u0131z\u0131n 0 ken 0 ve 1 ken 0 se\u00e7ilen s\u0131n\u0131flar\u0131n toplam\u0131na b\u00f6l\u00fcnmesidir.\n","cc553af4":"We will not change types of our variables because there is no wrong thing","38835f7e":"When we look pairplot ,we can say \"if variable value increase ,independent variable is being Malignant\".\nVariables distribution are looking normal but we can say when look shapiro test. ","4832ea84":"### Eksik Veri Analizi\n### Missing Values Analysis","02bb25e4":"---TR---\nVeri setimizdeki de\u011fi\u015fkenlerin korelasyonlar\u0131n\u0131 inceledi\u011fimiz zaman baz\u0131 de\u011fi\u015fkenler aras\u0131nda \u00e7ok y\u00fcksek korelasyon ba\u011flar\u0131\nba\u011flar\u0131 oldu\u011fu g\u00f6r\u00fcl\u00fcyor ki baz\u0131lar\u0131na bakt\u0131\u011f\u0131m\u0131zda bile korelasyon de\u011ferimiz 1 olanlar var.Bu da \u00e7oklu lineer ba\u011f\u0131nt\u0131 \nproblemine neden olabilir ve de\u011fi\u015fken say\u0131m\u0131z\u0131n \u00e7oklu\u011fundan dolay\u0131 verimize ayriyeten temel bile\u015fen analizi yapacaz.\n---ENG---\nfirstly we have a problem . This problem multicollinearity. Maybe we can remove some variables that have correlation above %95 but we won't this because we have two solution. \n1)principal composite analysis\n2)algorithms like random forest"}}