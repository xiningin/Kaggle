{"cell_type":{"9706cfbe":"code","5703b4eb":"code","000382bb":"code","fb9dd63b":"code","b080cbdc":"code","8c14855e":"code","4d5d1b10":"code","e7438166":"code","03df5c64":"code","200de2b4":"markdown","450821ed":"markdown","0c9b96ca":"markdown","3398615a":"markdown","8940548b":"markdown","29aca3f6":"markdown","5f32a6a4":"markdown","55d4110b":"markdown","613a4315":"markdown","9a4b3ee9":"markdown","7e091542":"markdown","3fc4575d":"markdown","f05a9e57":"markdown","85781106":"markdown","85fe72ae":"markdown"},"source":{"9706cfbe":"import pandas as pd\n\n# Loading the spam data\n# ham is the label for non-spam messages\nspam = pd.read_csv('..\/input\/nlp-course\/spam.csv')\nspam.head(10)","5703b4eb":"import spacy\n\n# Create an empty model\nnlp = spacy.blank(\"en\")\n\n# Create the TextCategorizer with exclusive classes and \"bow\" architecture\ntextcat = nlp.create_pipe(\n              \"textcat\",\n              config={\n                \"exclusive_classes\": True,\n                \"architecture\": \"bow\"})\n\n# Add the TextCategorizer to the empty model\nnlp.add_pipe(textcat)","000382bb":"# Add labels to text classifier\ntextcat.add_label(\"ham\")\ntextcat.add_label(\"spam\")","fb9dd63b":"# Hold out a bit of data for testing\ntrain_texts = spam['text'].values\ntrain_labels = [{'cats': {'ham': label == 'ham',\n                          'spam': label == 'spam'}} \n                for label in spam['label']]","b080cbdc":"train_data = list(zip(train_texts, train_labels))\ntrain_data[:3]","8c14855e":"from spacy.util import minibatch\n\nspacy.util.fix_random_seed(1)\noptimizer = nlp.begin_training()\n\n# Create the batch generator with batch size = 8\nbatches = minibatch(train_data, size=8)\n# Iterate through minibatches\nfor batch in batches:\n    # Each batch is a list of (text, label) but we need to\n    # send separate lists for texts and labels to update().\n    # This is a quick way to split a list of tuples into lists\n    texts, labels = zip(*batch)\n    nlp.update(texts, labels, sgd=optimizer)","4d5d1b10":"import random\n\nrandom.seed(1)\nspacy.util.fix_random_seed(1)\noptimizer = nlp.begin_training()\n\nlosses = {}\nfor epoch in range(10):\n    random.shuffle(train_data)\n    # Create the batch generator with batch size = 8\n    batches = minibatch(train_data, size=8)\n    # Iterate through minibatches\n    for batch in batches:\n        # Each batch is a list of (text, label) but we need to\n        # send separate lists for texts and labels to update().\n        # This is a quick way to split a list of tuples into lists\n        texts, labels = zip(*batch)\n        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n    print(losses)","e7438166":"texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\ndocs = [nlp.tokenizer(text) for text in texts]\n    \n# Use textcat to get the scores for each doc\ntextcat = nlp.get_pipe('textcat')\nscores, _ = textcat.predict(docs)\n\nprint(scores)","03df5c64":"# From the scores, find the label with the highest score\/probability\npredicted_labels = scores.argmax(axis=1)\nprint([textcat.labels[label] for label in predicted_labels])","200de2b4":"# Bag of Words\nMachine learning models can't learn from raw text data. Instead, you need to convert the documents into a vector representation you can use as model input. Ideally these vectors will be close together for similar documents and far apart for dissimilar documents.\n\nA simple representation we can use is a variation of one-hot-encoding. For each document, you represent it as a vector of term frequencies for each term in the vocabulary. The vocabulary is built from all the tokens (terms) in the corpus (the collection of documents). \n\nAs an example, let's take the sentences \"Tea is life. Tea is love.\" and \"Tea is healthy, calming, and delicious.\" as our corpus. The vocabulary then is `{\"tea\", \"is\", \"life\", \"love\", \"healthy\", \"calming\", \"and\", \"delicious\"}` (ignoring punctuation).\n\nFor each document, we count up how many times a term occurs. We place that count in the appropriate element of a vector. The first sentence has \"tea\" twice and that is the first position in our vocabulary, so we put the number 2 in the first element of the vector. Our sentences as vectors then look like \n\n$$\n\\begin{align}\nv_1 &= \\left[\\begin{matrix} 2 & 2 & 1 & 1 & 0 & 0 & 0 & 0 \\end{matrix}\\right] \\\\\nv_2 &= \\left[\\begin{matrix} 1 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\end{matrix}\\right]\n\\end{align}\n$$\n\nThis is called the \"bag of words\" representation. You can see that documents with similar terms will have similar vectors. Typical vocabularies will have tens of thousands of terms normally, so these vectors will be very large.\n\nAnother common representation is TF-IDF (Term Frequency - Inverse Document Frequency) which is similar to bag of words except that each term count is scaled by the term's frequency in the corpus. Using TF-IDF can potentially improve your models, but you won't be needing it here. Feel free to look it up though!","450821ed":"# Building a Bag of Words model\n\nOnce you have your documents in a bag of words representation, you can use those vectors as input to any machine learning model. SpaCy handles the bag of words conversion and building a simple linear model for you with the `TextCategorizer` class.\n\nThe TextCategorizer is a SpaCy pipe. Pipes are classes for processing and transforming tokens. When you create a SpaCy model with `nlp = spacy.load('en_core_web_sm')`, there are default pipes that perform part of speech tagging, entity recognition, and other transformations. When you run text through a model `doc = nlp(\"Some text here\")`, the output of the pipes are attached to the tokens in the `doc` object. The lemmas for `token.lemma_` come from one of these pipes.\n\nYou can remove or add pipes to models. What we'll do here is create an empty model without any pipes (other than a tokenizer, all models always have a tokenizer). Then, we'll create a TextCategorizer pipe and add it to the empty model.","0c9b96ca":"Once your model is trained you can try making predictions. The easiest way to do this is by using the `predict()` method of the TextCategorizer pipe. First the input text needs to be tokenized with `nlp.tokenizer`. Then you pass the tokens to the predict method which returns scores. The scores are the probability the input text belongs to the classes.","3398615a":"# Text Classification with SpaCy\n\nA common task in NLP is text classification, things like spam detection, sentiment analysis, and tagging customer queries. In this tutorial, I'll show you how to build a text classification model with SpaCy. The classifier will be trained to detect spam messages. This is a very common use and spam detectors run in the background of nearly all email clients these days.","8940548b":"Evaluating the model is straightforward once you have the predictions. To measure the accuracy, calculate how many correct predictions are made on some test data, divided by the total number of predictions.","29aca3f6":"# Training a Text Categorizer Model\n\nNow we need to convert the labels in the data to the form TextCategorizer requires. For each example we create a dictionary with boolean values for each class. In this case, if a text is \"ham\", we need a dictionary `{'ham': True, 'spam': False}`. The model is looking for these labels inside another dictionary with the key `'cats'`.","5f32a6a4":"With the data prepared, we'll train the classifier. First, we'll set some random seeds so we get repeatable outcomes. Then we need to create an `optimizer` using `nlp.begin_training()`. SpaCy uses this optimizer to update the model. In general it's more efficient to train models in small batches. For this, SpaCy provides the `minibatch` function that returns a generator yielding minibatches for training. Finally, the minibatches are split into texts and labels, then used with `nlp.update` to update the model's parameters.","55d4110b":"The scores are used to predict a single class or label by choosing the label with the highest probability. You get the index of the highest probability with `scores.argmax`, then use the index to get the label string from `textcat.labels`.","613a4315":"# Making Predictions","9a4b3ee9":"Since the classes are either positive or negative sentiment, we set `\"exclusive_classes\"` to `True`. We've also configured it with the bag of words (`\"bow\"`) architecture. SpaCy provides a convolutional neural network architecture as well, but it's more complex than what you need for now.\n\nNext we'll add the labels to the model. Here \"ham\" are for the real messages, \"spam\" are spam messages.","7e091542":"Next up, you'll train a text classifier model to predict the sentiment of Yelp reviews.","3fc4575d":"---\n**[Natural Language Processing Home Page](https:\/\/www.kaggle.com\/learn\/natural-language-processing)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum) to chat with other Learners.*","f05a9e57":"**[Natural Language Processing Home Page](https:\/\/www.kaggle.com\/learn\/natural-language-processing)**\n\n---\n","85781106":"Then we combine the texts and labels into a single list.","85fe72ae":"This is just one training loop through the data. The model will typically need For this, you wrap it in another for loop and shuffle the training data at the begining of each loop. "}}