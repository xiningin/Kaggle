{"cell_type":{"dae5e9af":"code","0b35764b":"code","c90b9c9f":"code","424ea0e3":"code","b3d7df1b":"code","03585c81":"code","06a07dec":"code","e5a2f26e":"code","94ad0bf5":"code","f747eece":"code","6c5eac40":"code","1ec2506b":"markdown","23fc7ae1":"markdown","9c790cab":"markdown","df6c441a":"markdown","3222b47a":"markdown","93067b5c":"markdown"},"source":{"dae5e9af":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold,KFold,train_test_split, train_test_split\nfrom six.moves import urllib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, kurtosis, mode\nfrom sklearn import tree\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nfrom IPython.display import SVG\nfrom graphviz import Source\n\nnp.random.seed(seed=2019)","0b35764b":"train = pd.read_csv('..\/input\/train.csv')\ntrain, target = train.iloc[:, 2:], train['target']","c90b9c9f":"X = train\ny = target\n#print(' based on ' + str(X.shape[1]) + ' features')\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=2019)\n\n\ndtree=DecisionTreeRegressor(max_depth=4)\ndtree.fit(X_train,y_train)\ny_valid_pred = dtree.predict(X_valid)\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y_valid, y_valid_pred)))\n\ngraph = Source(tree.export_graphviz(dtree, out_file=None, feature_names=X.columns))\npng_bytes = graph.pipe(format='png')\nwith open('dtree_pipe.png','wb') as f:\n    f.write(png_bytes)\n\nfrom IPython.display import Image\nImage(png_bytes)","424ea0e3":"num_folds = 5\n\nX = train\ny = target\n\nfolds = KFold(n_splits=num_folds, random_state=2019)\noof = np.zeros(len(X))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X.values, y.values)):\n    X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n    X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n    \n    model = RandomForestRegressor(n_estimators=10, max_depth=4).fit(X_train, y_train)\n#     print(\"fold \" + str(fold_+1) + \" training score: {:<8.5f}\".format(roc_auc_score(y_train, model.predict(X_train))))\n#     print(\"fold \" + str(fold_+1) + \" validation score: {:<8.5f}\".format(roc_auc_score(y_valid, model.predict(X_valid))))\n    oof[val_idx] = model.predict(X.iloc[val_idx])\n    \nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y, oof)))","b3d7df1b":"# feature engineering section - 4 new features added\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\n\n# scaling\nss=StandardScaler()\nmms=MinMaxScaler()\n\nXs=ss.fit_transform(train)\nXmm=mms.fit_transform(train)","03585c81":"def plot_dists(feature_name):\n    #plot of distributions\n    plt.figure()\n    plt.title(feature_name + ' - StandardScaler')\n    sns.kdeplot(train.loc[target==0, feature_name + '_s'], label='class 0', color='red')\n    sns.kdeplot(train.loc[target==1, feature_name + '_s'], label='class 1', color='blue')\n    plt.figure()\n    plt.title(feature_name + ' - MinMaxScaler')\n    sns.kdeplot(train.loc[target==0, feature_name + '_m'], label='class 0', color='red')\n    sns.kdeplot(train.loc[target==1, feature_name + '_m'], label='class 1', color='blue')","06a07dec":"# norms\nfeature_name = 'L2_norm'\n\ntrain[feature_name + '_s']=np.linalg.norm(Xs,axis=1, ord=2)\ntrain[feature_name + '_m']=np.linalg.norm(Xmm,axis=1, ord=2)\n\nplot_dists(feature_name)","e5a2f26e":"# means\nfeature_name = 'mean'\n\ntrain[feature_name + '_s']=Xs.mean(axis=1)\ntrain[feature_name + '_m']=Xmm.mean(axis=1)\n\nplot_dists(feature_name)","94ad0bf5":"def generate_percentile(percentile, show_plot=False):\n    percentile = percentile\/100\n    feature_name = str(int(percentile*100)) + 'th_percentile'\n\n    train[feature_name + '_s']= np.percentile(Xs, q=percentile, axis=1)\n    train[feature_name + '_m']= np.percentile(Xmm, q=percentile, axis=1)\n    \n    if show_plot:\n        plot_dists(feature_name)\n    \ngenerate_percentile(0, show_plot=True) # min\ngenerate_percentile(2)\ngenerate_percentile(5)\ngenerate_percentile(10)\ngenerate_percentile(15)\ngenerate_percentile(25)\ngenerate_percentile(50, show_plot=True) # median\ngenerate_percentile(75)\ngenerate_percentile(85)\ngenerate_percentile(95)\ngenerate_percentile(98, show_plot=True)\ngenerate_percentile(100) # maximum","f747eece":"X = train\ny = target\n#print(' based on ' + str(X.shape[1]) + ' features')\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=2019)\n\n\ndtree=DecisionTreeRegressor(max_depth=4)\ndtree.fit(X_train,y_train)\ny_valid_pred = dtree.predict(X_valid)\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y_valid, y_valid_pred)))\n\ngraph = Source(tree.export_graphviz(dtree, out_file=None, feature_names=X.columns))\npng_bytes = graph.pipe(format='png')\nwith open('dtree_pipe.png','wb') as f:\n    f.write(png_bytes)\n\nfrom IPython.display import Image\nImage(png_bytes)","6c5eac40":"num_folds = 5\n\nX = train\ny = target\n\nfolds = KFold(n_splits=num_folds, random_state=2019)\noof = np.zeros(len(X))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X.values, y.values)):\n    X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n    X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n    \n    model = RandomForestRegressor(n_estimators=10, max_depth=4).fit(X_train, y_train)\n#     print(\"fold \" + str(fold_+1) + \" training score: {:<8.5f}\".format(roc_auc_score(y_train, model.predict(X_train))))\n#     print(\"fold \" + str(fold_+1) + \" validation score: {:<8.5f}\".format(roc_auc_score(y_valid, model.predict(X_valid))))\n    oof[val_idx] = model.predict(X.iloc[val_idx])\n    \nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y, oof)))","1ec2506b":"Now, I want to explore some feature interactions including:\n* norm of features\n* mean of features\n* different percentiles of features\n\nNotice that these simple summary statistics are row-wised and I used two different scaling methods before taking those measurements. Below you can find the distributions for every class of these features:","23fc7ae1":"Let us see wheter this conclusion works also in an ensemble of trees like a RandomForest regressor similar to the one above?\n\nI trained the exact same random forest as before on this dataset (including original dataset and these added features) and  it resulted in an increased AUC of 0.74056 in a 5 fold cross validation. ","9c790cab":"These types of features seemed very promising to me :) I thought I have found the magic at the beginning! but it turned out that these features are not the magical ones. Although I still suspect that it has something to do with these sort of stuff. Anyway, we will find out in a few hours after the top ranks (hopefully) share their magic with us.\n\nFinally, it was a good run. I learned a lot in this competition and I shared parts of it with you too. Good luck!","df6c441a":"If I were a decision tree I would have said that by looking at the distributions of some of these generated features such as 98th percentile, median (or 50th percentile) and L2_norm of StandardScaler I can make good splits :)\n\nlet's build a decision tree using old features along with these new features and see what a real decision tree thinks of these feature. \n\nAUC score jumped to 0.73147! Also, look at the tree below; many of the features we generated were chosen by the tree to make the splits at the very top of the tree! So, great! we introduced new features that couldn't have been identified by the tree itself and it increased the AUC score.","3222b47a":"In our journey to find the magic, @meylone and I explored one aspect of what a decision tree based algorithm cannot see in [What a decision tree cannot see?](https:\/\/www.kaggle.com\/mhviraf\/what-a-decision-tree-cannot-see) kernel. In this kernel we are going to explore another aspect of what a decision tree based algorithm cannot see. **Feature interactions**!\n\nAs discussed in many other kernels, decision trees split the variables one by one. For example, look at the tree below which resulted in AUC of 0.60348. It starts with `var_81` then continues by spliting `var_12` and so on so forth. ","93067b5c":"The above tree is the fundamental block of LightGBM, XGBoost, RandomForest, and Catboost. To get a sense of how feature interactions would affect a decision tree based algorithm, I have also trained a simple random forest before I add feature interactions and it resulted in AUC of 0.65519 in a 5 fold cross validation. "}}