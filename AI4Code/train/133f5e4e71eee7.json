{"cell_type":{"2219f59a":"code","cb8b4207":"code","176bdfbd":"code","4ef22d06":"code","e2444c61":"code","b09f406f":"code","e9e740bb":"code","49c027ef":"code","6f984b14":"code","3e024e5d":"code","4436cf51":"code","0763a390":"code","769a7e6e":"code","d246ff1b":"code","e103ffd8":"code","6599748d":"code","480b61e6":"code","a29f8920":"code","5a1b9513":"code","c8e838af":"code","1b8ee8a2":"code","befd5203":"code","353af5b0":"code","74618bd5":"code","9b9fd23a":"code","4533961e":"code","fe83be70":"code","38ccd21f":"code","c9f66e62":"code","8a355570":"code","4b2df019":"code","48241cf3":"code","94a366af":"code","79f6b7bd":"code","087ab39a":"code","5d8eb753":"code","476b01b3":"code","793ad4e3":"code","5732fa10":"code","d49cab00":"code","e4203636":"code","f994297a":"code","ea5bf6c7":"code","0d1812e9":"code","57ebc727":"code","87ea628a":"code","4043ffd1":"code","1ced8cde":"code","2d2c05d8":"code","76d428a3":"code","4509a74b":"code","65d1255f":"code","da97c946":"code","c65fd806":"code","5cbf4c3c":"code","c68a3e4f":"code","12d25286":"code","a29b8213":"markdown","044a5e96":"markdown","91ef0801":"markdown","5a2f92ba":"markdown","9726449f":"markdown","15b76493":"markdown","2f5e9e88":"markdown","abcad334":"markdown","cbe81f90":"markdown","a6ed1a08":"markdown","4954b356":"markdown","7f0b6043":"markdown","924fe8d2":"markdown","35138242":"markdown","e643a291":"markdown","e4a6f841":"markdown","ca66c752":"markdown","9702e1ea":"markdown","47f6d803":"markdown","67a398d7":"markdown","cbbb050d":"markdown","3a9ee47f":"markdown","08d8abe8":"markdown","18ad8218":"markdown","63fda446":"markdown","8ff62164":"markdown","e9863b78":"markdown","1f08e9f7":"markdown","c0f43701":"markdown","b43ca8e1":"markdown","5d1c5fad":"markdown","1946477d":"markdown","5de01a13":"markdown","81d80734":"markdown","a806ec76":"markdown","a1fb01a3":"markdown","da69df30":"markdown","e58c5f6c":"markdown","303f1041":"markdown","817624dc":"markdown","993de04c":"markdown","8d5b05de":"markdown","0f936d03":"markdown","c5d3900f":"markdown","97e4bdab":"markdown"},"source":{"2219f59a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb8b4207":"import math\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.corpus import wordnet # for pos tagging, pos is verb noun adj ect for lemmatization\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Lambda, Dense, Concatenate, Dropout, Softmax\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","176bdfbd":"djia_df = pd.read_csv(\"\/kaggle\/input\/stocknews\/upload_DJIA_table.csv\")\nnews_and_djia_df = pd.read_csv(\"\/kaggle\/input\/stocknews\/Combined_News_DJIA.csv\")\nnews_df = pd.read_csv(\"\/kaggle\/input\/stocknews\/RedditNews.csv\")\nnews_and_djia_df_og = pd.read_csv(\"\/kaggle\/input\/stocknews\/Combined_News_DJIA.csv\")","4ef22d06":"#news_and_djia_df.head(3)","e2444c61":"news_and_djia_df.info()","b09f406f":"news_columns=news_and_djia_df.columns[2:]\nnews_columns","e9e740bb":"for column in news_columns:\n    try:\n        news_and_djia_df[column].apply(lambda x : x.lower())\n    except:\n        print(column)","49c027ef":"news_and_djia_df['Top23'].apply(lambda x : type(x)).value_counts()","6f984b14":"news_and_djia_df['Top23'][news_and_djia_df['Top23'].apply(lambda x : type(x))!=type('something')]","3e024e5d":"news_and_djia_df['Top24'][news_and_djia_df['Top24'].apply(lambda x : type(x))!=type('something')]","4436cf51":"news_and_djia_df['Top25'][news_and_djia_df['Top25'].apply(lambda x : type(x))!=type('something')]","0763a390":"news_and_djia_df['Top23']=news_and_djia_df['Top23'].replace(float(\"nan\"),\"b'\");\nnews_and_djia_df['Top24']=news_and_djia_df['Top24'].replace(float(\"nan\"),\"b'\");\nnews_and_djia_df['Top25']=news_and_djia_df['Top25'].replace(float(\"nan\"),\"b'\");","769a7e6e":"news_and_djia_df['Top23'][277]","d246ff1b":"for column in news_columns:\n    try:\n        news_and_djia_df[column].apply(lambda x : x.lower())\n    except:\n        print(column)","e103ffd8":"def US_to_america(news):\n    return re.sub(r'US', 'usa', news)\n    #return re.sub(r'UK', 'united kingdom', news) unlike us, uk is not a word so it's probably ok\n    #return re.sub(r'EU', 'european union', news)\n\nfor column in news_columns:\n    news_and_djia_df[column]=news_and_djia_df[column].apply(lambda x : US_to_america(x))","6599748d":"for column in news_columns:\n    news_and_djia_df[column]=news_and_djia_df[column].apply(lambda x : x.lower())","480b61e6":"for column in news_columns[:3]:\n    mask1 = news_and_djia_df[column].apply(lambda x : x[:2])!=\"b'\"\n    mask2 = news_and_djia_df[column].apply(lambda x : x[:2])!='b\"'\n    mask = np.bitwise_and(mask1, mask2)\n    print(column)\n    print(news_and_djia_df[column][mask].head(3))\n    print()","a29f8920":"for column in news_columns:\n    mask1 = news_and_djia_df[column].apply(lambda x : x[:2])==\"b'\"\n    mask2 = news_and_djia_df[column].apply(lambda x : x[:2])=='b\"'\n    mask = np.logical_or(mask1, mask2)\n    for i in range(mask.shape[0]):\n        if mask.loc[i] == True:\n            news_and_djia_df.loc[mask.index[i],column] =  news_and_djia_df.loc[mask.index[i],column][2:]","5a1b9513":"print(news_and_djia_df.iloc[1601,3])\nnews_and_djia_df_og.iloc[1601,3]","c8e838af":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"u\\.s\\.\", \"usa\", phrase)\n    phrase = re.sub(r\"united states of america\", \"usa\", phrase)\n    phrase = re.sub(r\"american\", \"usa\", phrase)\n    phrase = re.sub(r\"russian\", \"russia\", phrase)\n    phrase = re.sub(r\"israeli\", \"israel\", phrase)\n    phrase = re.sub(r\"united nations\", \"un\", phrase)\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    #phrase = re.sub(r\"u\\.n\\.\", \"united nations\", phrase)\n    #phrase = re.sub(r\"un\", \"united nations\", phrase)\n    #phrase = re.sub(r\"u\\.s\\.a\\.\", \"america\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\nfor column in news_columns:\n    news_and_djia_df[column] = news_and_djia_df[column].apply(lambda x : decontracted(x))","1b8ee8a2":"def remove_punc(news):\n    news = re.sub('-', ' ', news)\n    news = re.sub(r'[^\\w\\s]', '', news) # remove everything except words, digits, underscores and white spaces\n    #news = re.sub(r'\\s[^\\d]{0,}[\\d]{1,}[^\\d]{0,}\\s', '', news) #  remove words with digits e.g. $12b \n    news = re.sub(r'[\\d]', '', news) # remove all digits\n    news = re.sub('_', ' ', news) # the previous row doesn't remove underscore\n    return news\n\nfor column in news_columns:\n    news_and_djia_df[column] = news_and_djia_df[column].apply(lambda x : remove_punc(x))","befd5203":"for column in news_columns:\n    news_and_djia_df[column] = news_and_djia_df[column].apply(lambda x: nltk.word_tokenize(x))","353af5b0":"wl = WordNetLemmatizer()\nwl.lemmatize('feet','n')","74618bd5":"tagged = nltk.pos_tag(['there','are','many','books','that','can','be','patiently','read'])\ntagged","9b9fd23a":"def get_wordnet_pos(treebank_tag):\n    \n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n    \ndef lemmatize_list(lista):\n    tagged = nltk.pos_tag(lista)\n    for i, ele in enumerate(lista):\n        lista[i] = wl.lemmatize(ele, get_wordnet_pos(tagged[i][1]))\n    return lista","4533961e":"lemmatize_list(['there','are','many','books','that','can','be','peacefully','read'])","fe83be70":"for column in news_columns:\n    news_and_djia_df[column] = news_and_djia_df[column].apply(lambda x : lemmatize_list(x))","38ccd21f":"stop_words=stopwords.words('english')\n#stop_words.append('u') # 'u' is often short for 'you' in casual english\nstop_words.append('one'); stop_words.append('two'); stop_words.append('three')\nstop_words.append('four'); stop_words.append('five'); stop_words.append('six')\nstop_words.append('seven'); stop_words.append('eight'); stop_words.append('nine')\nstop_words.append('ten'); stop_words.append('eleven'); stop_words.append('twelve')\nstop_words.append('thirteen'); stop_words.append('fourteen'); stop_words.append('fifteen')\nstop_words.append('sixteen'); stop_words.append('seventeen'); stop_words.append('eighteen')\nstop_words.append('nineteen'); stop_words.append('twenty'); stop_words.append('million')\nstop_words.append('billion'); stop_words.append('th'); stop_words.append('m') # million\nstop_words.append('b') # billion\n\nstop_words.append('say'); stop_words.append('people'); \n\nstop_words.remove('no'); stop_words.remove('not')\nstop_words.remove('above'); stop_words.remove('below')\n#stop_words.remove('before'); stop_words.remove('after') \nstop_words.remove('up'); stop_words.remove('down') \nstop_words.remove('over'); stop_words.remove('under')\n\ndef remove_stop_words(lista):\n    pt=0 # don't use a for loop because len(lista) keeps changing as we remove stop words.\n    while pt<len(lista):\n        if lista[pt] in stop_words:\n            lista.remove(lista[pt])\n        else:\n            pt+=1\n    return lista\n\nfor column in news_columns:\n    news_and_djia_df[column] = news_and_djia_df[column].apply(lambda x: remove_stop_words(x))","c9f66e62":"#stop_words","8a355570":"pd.options.display.max_colwidth = 300\nhead=100\nadd=20\nfor i in range(head,head+add):\n    print(i)\n    print(news_and_djia_df_og.iloc[i, 3])\n    print(news_and_djia_df.iloc[i, 3])\n    print()\npd.options.display.max_colwidth = 50","4b2df019":"def str_to_timestamp(date_string):\n    return pd.Timestamp(year= int(date_string[:4]), month = int(date_string[5:7]), day = int(date_string[8:]))\n\nnews_and_djia_df['Date'] = news_and_djia_df['Date'].apply(str_to_timestamp)","48241cf3":"df = news_and_djia_df[['Date','Label']]\ndf = df.assign(Date = df['Date'].apply(lambda x : x.weekday()))\ndf = df.rename(columns={\"Date\": \"weekday\"})","94a366af":"asdf = df['Label'].value_counts()\nprint(asdf)","79f6b7bd":"f, ax = plt.subplots(1,1, figsize=(5,5))\nax.pie(asdf)\nax.legend(['1','0'])\nplt.title('Labels');","087ab39a":"summary = df.groupby(by = 'weekday').sum()\nsummary = summary.rename(columns = {'Label' : 'counts of 1'})\nsummary","5d8eb753":"f, ax = plt.subplots(1,1, figsize=(5,5))\nplt.bar(['Mon','Tues','Wed','Thur','Fri'], summary['counts of 1'])\nax.set_xlabel('weekday')\nax.set_ylabel('days of market increase');","476b01b3":"f, ax = plt.subplots(3,3, figsize=(20,20))\n#ax[0,0].bar(['Mon','Tues','Wed','Thur','Fri'], summary['counts of 1'])\nfor i in range(9):\n    row = int(i\/3)\n    column = i % 3\n    long_string=''\n    tot = news_and_djia_df['Top'+str(i+1)].values.shape[0]\n    for j in range(tot):\n        lista = news_and_djia_df['Top'+str(i+1)][j]\n        for ele in lista:\n            long_string = long_string + ele + ' '\n    wc = WordCloud(background_color ='white', width = 800, height = 800, min_font_size = 10).generate(long_string)\n    ax[row, column].imshow(wc)","793ad4e3":"tot = news_and_djia_df['Top1'].values.shape[0]\nhuge_list = []\nfor i in range(tot):\n    for j in range(25):\n        huge_list = huge_list + news_and_djia_df.iloc[i,j+2]\n        \nfreq_dict = Counter(huge_list)","5732fa10":"freq_list = []\n\nfor key in freq_dict:\n    freq_list.append([key, freq_dict[key]])\n    \ndef return_freq(lista):\n    return lista[1]\n\nfreq_list.sort(key = return_freq, reverse = True)","d49cab00":"num =6 # num^2 is the number of most frequent words we want\nwords = [freq_list[i][0] for i in range(num**2)]\nfreqs = [freq_list[i][1] for i in range(num**2)]\nax = plt.subplots(1, 1, figsize=(23, 10))\nplt.bar(words, freqs)\nplt.ylabel(\"number of occurences\");","e4203636":"print(news_and_djia_df.iloc[1601,3])\nnews_and_djia_df_og.iloc[1601,3]","f994297a":"store_dicts=[]\nfor i in range(news_and_djia_df.shape[0]):\n    dic={}\n    for j in range(25):\n        lista=news_and_djia_df.iloc[i,j+2]\n        for ele in lista:\n            try:\n                dic[ele]+=1\n            except:\n                dic[ele]=1\n    store_dicts.append(dic)","ea5bf6c7":"main = np.zeros((tot, num**2))\n\nfor i in range(tot):\n    for j in range(num**2):\n        try:\n            main[i,j] = store_dicts[i][words[j]]\n        except:\n            _","0d1812e9":"main_df = pd.DataFrame(main)\nmain_df['label'] = news_and_djia_df['Label']","57ebc727":"next_5_day_label = news_and_djia_df['Label']\nnext_5_day_label = next_5_day_label[5:]\nfor i in range(5):\n    next_5_day_label = np.append(next_5_day_label, float('NAN'))\nmain_df['next_5_day_label'] = next_5_day_label\nmain_df.iloc[-10:]","87ea628a":"f, ax = plt.subplots(num, num, figsize=(20,20))\nbins=[0,1,2,3,4,5,6,7,8,9,10,11]\nfor i in range(num**2):\n        row = int(i\/num)\n        column = i % num\n        ax[row, column].hist(main_df[main_df['label']==1][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n        ax[row, column].hist(main_df[main_df['label']==0][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n        ax[row, column].legend([1,0])\n        ax[row, column].set_title(words[i])","4043ffd1":"f, ax = plt.subplots(num, num, figsize=(20,20))\nbins=[0,1,2,3,4,5,6,7,8,9,10,11]\nfor i in range(num**2):\n        row = int(i\/num)\n        column = i % num\n        ax[row, column].hist(main_df[main_df['next_5_day_label']==1][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n        ax[row, column].hist(main_df[main_df['next_5_day_label']==0][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n        ax[row, column].legend([1,0])\n        ax[row, column].set_title(words[i])","1ced8cde":"f, ax = plt.subplots(1, 1, figsize=(10,10))\nax.matshow(main_df.corr())","2d2c05d8":"print(words[1], words[16], ':high positive correlation')\nprint(words[13], words[30], ':high positive correlation')\nprint(words[15],words[7], ':high positive correlation')\nprint(words[0],words[10], ':high negative correlation')\nprint(words[13],words[19], ':high negative correlation')\nprint(words[13],words[20], ':high negative correlation')","76d428a3":"tot_words=[]\nfor i in range(tot):\n    tot_words.append(0)\n    for j in range(25):\n        tot_words[-1]+=len(news_and_djia_df.iloc[i,j+2])","4509a74b":"tf_df=main_df\nfor i in range(tot):\n    for j in range(num**2):\n        tf_df.iloc[i,j] = tf_df.iloc[i,j]\/tot_words[i]","65d1255f":"idf=[0]*(num**2)\nfor i in range(tot):\n    for j in range(num**2):\n        try:\n            store_dicts[i][words[j]]\n            idf[j]+=1\n        except:\n            None\n            \nfor i in range(num**2):\n    idf[i]=np.log(tot\/idf[i])","da97c946":"tf_idf_df=tf_df\nfor i in range(tot):\n    for j in range(num**2):\n        tf_idf_df.iloc[i,j]=tf_idf_df.iloc[i,j] * idf[j]","c65fd806":"f, ax = plt.subplots(num, num, figsize=(20,20))\nbins=[i\/1000 for i in range(11)]\nfor i in range(num**2):\n    row = int(i\/num)\n    column = i % num\n    ax[row, column].hist(tf_idf_df[tf_idf_df['label']==1][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n    ax[row, column].hist(tf_idf_df[tf_idf_df['label']==0][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n    ax[row, column].legend([1,0])\n    ax[row, column].set_title(words[i])","5cbf4c3c":"f, ax = plt.subplots(num, num, figsize=(20,20))\nbins=[i\/1000 for i in range(11)]\nfor i in range(num**2):\n    row = int(i\/num)\n    column = i % num\n    ax[row, column].hist(tf_idf_df[tf_idf_df['next_5_day_label']==1][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n    ax[row, column].hist(tf_idf_df[tf_idf_df['next_5_day_label']==0][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n    ax[row, column].legend([1,0])\n    ax[row, column].set_title(words[i])","c68a3e4f":"from sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nimport random\nstore_null=[] # store null accuracy\nstore_result=[] # store accuracy\nstore_auc=[] # store auc roc\n\ndata = pd.read_csv('..\/input\/stocknews\/Combined_News_DJIA.csv') # read in data\n   \nfor i in range(100):\n    \n    train_X, test_X, train_Y, test_Y = train_test_split(data.iloc[:,2:], data.iloc[:,1], test_size=0.2, random_state=None) \n\n    trainheadlines = []\n    for row in range(train_X.shape[0]):\n        trainheadlines.append(' '.join(str(x) for x in train_X.iloc[row])) # join together all 25 news\n\n    testheadlines = []\n    for row in range(test_X.shape[0]):\n        testheadlines.append(' '.join(str(x) for x in test_X.iloc[row])) # join together all 25 news\n    \n    # count TF-IDF on 2-grams\n    advancedvectorizer = TfidfVectorizer(min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2)) \n    advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n    advancedtest = advancedvectorizer.transform(testheadlines)\n    \n    # C is regularization parameter, when C gets larger regularization becomes smaller\n    advancedmodel = LogisticRegression(C=1000, solver='liblinear')\n    advancedmodel.fit(advancedtrain, train_Y)\n\n    preds13 = advancedmodel.predict(advancedtest) # binary prediciton\n    preds13prob = advancedmodel.predict_proba(advancedtest)[:,1] # probablity prediction\n    acc13 = accuracy_score(test_Y, preds13)\n    \n    store_null.append(sum(test_Y)\/test_Y.shape[0])\n    store_result.append(acc13)\n    store_auc.append(roc_auc_score(test_Y,preds13prob))\n\nprint('Average null accuracy: ',sum(store_null)\/len(store_null))\nprint('Average accuracy: ',sum(store_result)\/len(store_result))\nprint('Average AUC score: ',sum(store_auc)\/len(store_auc))","12d25286":"import statistics\n\nprint('Standard deviation of null accuracy: ', statistics.pstdev(store_null))\nprint('Standard deviation of accuracy: ', statistics.pstdev(store_result))\nprint('Standard deviation of AUC score: ', statistics.pstdev(store_auc))","a29b8213":"<a id=\"subsection-3\"><\/a>\n## (c) recover abbreviations (change they'll to they will, etc) ","044a5e96":"**Are there any news that doesn't begin with b'? It seems that the answer is yes. The news doesn't begin with b' starting from number 477 (except the rows with missing values, we imputed them with b').**","91ef0801":"**certain words in today's news affect next 5 today's market change**","5a2f92ba":"**it seems that almost every news start with b' so we are going to impute the missing values with b'**","9726449f":"<a id=\"subsection-1\"><\/a>\n## (a) before lowering case, change US to usa (otherwise US will become us which is not a country)","15b76493":"<a id=\"subsection-25\"><\/a>\n## (e) test how presence of frequent words affect the result 1 and 0 ","2f5e9e88":"<a id=\"subsection-23\"><\/a>\n## (c) word map for all news\n<a id=\"subsection-24\"><\/a>\n## & (d) see if the most frequent words are the same for top1, top2, etc ","abcad334":"**tf-idf**","cbe81f90":"<a id=\"subsection-7\"><\/a>\n## (g) remove stop words","a6ed1a08":"**total number of words**","4954b356":"**value_counts function is very useful in this case**","7f0b6043":"**make sure that we have taken care of the missing values**","924fe8d2":"**the fact that there are more 1's than 0's is maybe consistent with the fact that the stock market is growing on large time scale**","35138242":"<a id=\"subsection-2\"><\/a>\n## (b) now we can lower case all news","e643a291":"**create a dictionary that stores number of occurences for words in each day**<br>\n**[china, china, usa] would be {china:2, usa:1}**","e4a6f841":"<a id=\"section-3\"><\/a>\n# 3. Simple models for benchmark","ca66c752":"<a id=\"subsection-26\"><\/a>\n## (f) correlation matrix of features","9702e1ea":"**calculate idf**","47f6d803":"**maybe today's news will make good prediction for tomorrow's market change, so we try out this idea**","67a398d7":"**try lowering case but found some problems with top23 top24 and top25 news**","cbbb050d":"**certain words in today's news affect the market change 5 days later, remember to normaliza the histogram because the number of market growing days are more than dropping days**","3a9ee47f":"**replace the first column with timestamp**","08d8abe8":"<a id=\"subsection-22\"><\/a>\n## (b) Test how weekday Monday\/Tuesday\/etc affect the label","18ad8218":"**this tells us how today's news affect today's stock market movement**","63fda446":"**import all necessary libraries**","8ff62164":"<a id=\"subsection-4\"><\/a>\n## (d) remove everything except words and white space","e9863b78":"<a id=\"section-2\"><\/a>\n# 2. Ideas for EDA \n## (a) number of 1's vs 0's\n## (b) Test how weekday Monday\/Tuesday\/etc affect the label \n## (c) word map, top 1000 most frequent words \n## (d) see if the most frequent words are the same for top1, top2, etc \n## (e) test how presence of frequent words affect the result 1 and 0 \n## (f) correlation matrix of features","1f08e9f7":"**create a numpy array that stores the number of occurences for the most frequent words in each day**","c0f43701":"**create a new dataframe for date and label only**","b43ca8e1":"# read csv as pandas dataframe","5d1c5fad":"<a id=\"section-1\"><\/a>\n# 1. Feature preprocessing\n\n## (a) change US to us\n## (b) ower case \n## (c) recover abbreviations (change they'll to they will, etc)\n## (d) remove everything except words, underscores and white spaces\n## (e) tokenization\n## (f) lemmatization\n## (g) remove stop words","1946477d":"**count the number of 0's and 1's in the label**","5de01a13":"* [feature preprocessing](#section-1)\n    - [change US to us](#subsection-1)\n    - [lower case](#subsection-2)\n    - [recover abbreviations](#subsection-3)\n    - [remove everything except words, underscores and white spaces](#subsection-4)\n    - [tokenization](#subsection-5)\n    - [lemmatization](#subsection-6)\n    - [remove stop words](#subsection-7)\n* [Ideas for EDA](#section-2) \n    - [number of 1's vs 0's](#subsection-21)\n    - [Test how weekday Monday\/Tuesday\/etc affect the label](#subsection-22)\n    - [word map, top 1000 most frequent words](#subsection-23)\n    - [see if the most frequent words are the same for top1, top2, etc](#subsection-24)\n    - [test how presence of frequent words affect the result 1 and 0](#subsection-25)\n    - [correlation matrix of features](#subsection-26)\n    - [use tf-idf as features and repeat the analysis](#subsection-27)\n* [Simple models for benchmarks](#section-3) ","81d80734":"<a id=\"subsection-6\"><\/a>\n## (f) Lemmatization","a806ec76":"**I copied the code from the follow url by Yann Dubois** <br>\nhttps:\/\/stackoverflow.com\/questions\/43018030\/replace-apostrophe-short-words-in-python","a1fb01a3":"**the function below is taken from the top answer in https:\/\/stackoverflow.com\/questions\/15586721\/wordnet-lemmatization-and-pos-tagging-in-python**","da69df30":"<a id=\"subsection-5\"><\/a>\n## (e) tokenization ","e58c5f6c":"**remove the b' and b'' at the beginning of news**","303f1041":"<a id=\"subsection-27\"><\/a>\n## (g) repeat the analysis with tfidf","817624dc":"**use the idea of n-grams and TF-IDF, do logistic regressions**<br>\n**the idea is take from the two notebooks below**<br>\nhttps:\/\/www.kaggle.com\/ndrewgele\/omg-nlp-with-the-djia-and-reddit<br>\nhttps:\/\/www.kaggle.com\/lseiyjg\/use-news-to-predict-stock-markets","993de04c":"# Author of this notebook: Fanglida Yan","8d5b05de":"**certain words in today's news affect today's market change, remember to normaliza the histogram because the number of market growing days are more than dropping days**","0f936d03":"**calculate tf**","c5d3900f":"<a id=\"subsection-21\"><\/a>\n## (a) number of 1's vs 0's","97e4bdab":"**looks like Monday is correlated with decrease of DJIA**"}}