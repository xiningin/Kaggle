{"cell_type":{"1bb2509c":"code","d5c099e5":"code","848718cd":"code","675ee7b4":"code","82fdc87a":"code","b9eba68b":"code","604d59c3":"code","6715135a":"code","56f696ba":"code","b02b5597":"code","1429ffd8":"code","94168339":"code","be05dda2":"code","8f2dd4c2":"code","30100777":"code","7fc97888":"code","127ae210":"code","77c533dc":"markdown","7b76e9a4":"markdown","5f3f9c96":"markdown","d9e588a3":"markdown","7f847df4":"markdown","811e1583":"markdown","997c7f9d":"markdown","cd14bb35":"markdown","bf0924be":"markdown","eda9f8ee":"markdown","7e49c0a4":"markdown","8fe8ae83":"markdown","00096f15":"markdown","7eb9b3af":"markdown","0b540496":"markdown","4f881a82":"markdown","faaef7b8":"markdown","f8a910f4":"markdown","b5f42e4f":"markdown","8bdc6725":"markdown","95ddb74f":"markdown","02448d4f":"markdown","edc0c77b":"markdown","b6ec8e8f":"markdown","1332fff2":"markdown","714a8871":"markdown","001ccf40":"markdown","ffeba62e":"markdown"},"source":{"1bb2509c":"# Import torch and other required modules\nimport torch","d5c099e5":"# Example 1 - working (change this)\nt1 = torch.ones((5,5), requires_grad=True)\nt2 = t1 * 10\nt3 = t2.trace()\n\nt3.backward()\n\nprint(t1.grad)","848718cd":"# Example 2 - working\n\ninput_x = torch.randn((5,5), requires_grad=True)\nw = torch.randn((5,5), requires_grad=True)\n\nprint(input_x)\nprint()\nprint(w)\nprint()\n\ny = torch.sum(input_x * w)\ny.backward()\n\nprint(w.grad)\nprint()\nprint(input_x.grad)","675ee7b4":"# Example 3 - breaking (to illustrate when it breaks)\ninput_x = torch.randn((5,5), requires_grad=True)\nw = torch.randn((5,5), requires_grad=True)\n\ny = (input_x * w)\ny.backward()\n\nprint(w.grad)\nprint()\nprint(input_x.grad)","82fdc87a":"# Example 1 - working\nm = torch.rand(4,2)\nprint(m)\nidx = torch.tensor([0,2,4])\nn = torch.take(m,idx)\nprint(n)\n","b9eba68b":"# Example 2 - working\n\nm = torch.rand(4,2)\nprint(m)\nidx = torch.arange(3,7,2)\nprint(idx)\nn = torch.take(m,idx)\nprint(n)","604d59c3":"# Example 3 - breaking (to illustrate when it breaks)\n\nimport numpy as np\n\nm = torch.rand(4,2)\nprint(m)\nidx1 = np.array([0,2,4])\n\nn = torch.take(m,idx1)\n\nprint(n)\n","6715135a":"# Example 4 - breaking (to illustrate when it breaks)\n\nidx2 = torch.from_numpy(np.array([0,2,4]))\n\nmatr = np.linspace((1,2),(10,20),10)\nq = torch.take(matr,idx2)\nprint(q)\n","56f696ba":"# Example 1 - working\nmat1 = torch.randn(10, 3)\nmat2 = torch.randn(3, 4)\n\nmat3 = torch.matmul(mat1,mat2)\n\nprint(mat3)\nprint()\nprint(mat3.shape)","b02b5597":"# Example 2 - working\n\nmat1 = torch.randn(10, 3, 4)\nmat2 = torch.randn(10, 4, 5)\n\nmat3 = torch.matmul(mat1,mat2)\n\nprint(mat3)\nprint()\nprint(mat3.shape)","1429ffd8":"# Example 3 - breaking (to illustrate when it breaks)\n\nmat1 = torch.randn(10, 3)\nmat2 = torch.randn(10, 4)\n\nmat3 = torch.matmul(mat1,mat2)\n\nprint(mat3)\nprint()\nprint(mat3.shape)\n\n","94168339":"# Example 1 - working\n\nmat1 = torch.randn(10, 3)\n\nprint(mat1)\n\ntorch.unsqueeze(mat1, 2)","be05dda2":"# Example 2 - working\n\nmat2 = torch.randn(10, 4, 5)\n\nprint(mat2)\n\nprint(torch.unsqueeze(mat2, -1))","8f2dd4c2":"# Example 3 - breaking (to illustrate when it breaks)\n\nmat3 = torch.randn(5, 4)\n\nprint(mat3)\n\ntorch.unsqueeze(mat1,3)","30100777":"# Example 1 - working\n\nt1 = torch.tensor([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\nprint(t1)\n\nt1.view(9,1)","7fc97888":"# Example 2 - working\n\nt1 = torch.randn(5,6)\n\nprint(t1)\nprint()\n\nt2 = t1.view(-1, 5)\nprint()\n\nprint(t2.shape)\nprint()\n\nprint(t2)","127ae210":"# Example 3 - breaking (to illustrate when it breaks)\n\nt3 = torch.tensor([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\nprint(t3)\nt3 = t3.T\nprint(t3.view(1,9))","77c533dc":"Closing comments about when to use this function\n\n\n**We can make use of this function when we need to fetch the tensor values from a tensor using its indexes. We need to make sure that both the arguments are tensors. **","7b76e9a4":"Explanation about example","5f3f9c96":"Explanation about example\n\n***This function breaks because , If mat1 is a (b\u00d7n) tensor, mat2 should be a (n\u00d7m) tensor. \nOnly then, an output of (bxm) tensor can be produced as output.***","d9e588a3":"Explanation about example","7f847df4":"## Conclusion\n\nSummarize what was covered in this notebook, and where to go next","811e1583":"## Function 5 - View\n\nAdd some explanations","997c7f9d":"Explanation about example\n\n**> *This function can also be used to perform batch matrix multiplication by broadcasting matric products.***","cd14bb35":"Explanation about example","bf0924be":"## Reference Links\nProvide links to your references and other interesting articles about tensors\n* Official documentation for `torch.Tensor`: https:\/\/pytorch.org\/docs\/stable\/tensors.html\n* https:\/\/discuss.pytorch.org\/t\/difference-between-view-reshape-and-permute\/54157\/2\n* https:\/\/stackoverflow.com\/questions\/57237352\/what-does-unsqueeze-do-in-pytorch\n* https:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/introduction-to-pytorch-from-scratch\/","eda9f8ee":"Closing comments about when to use this function\n\n\nThis function can be used when we want the **shape of particular axis ** in a tensor to be **1**\n\n","7e49c0a4":"Explanation about example","8fe8ae83":"## Function 4 - torch.unsqueeze()\n\nThe way I like to think of it is: \n* It inserts **1** in the shape of tensor on the specified axis(second parameter)","00096f15":"Explanation about example","7eb9b3af":"Explanation about example\n\nThe Error occurs because **backward** function works on scalar output values only. \n\nWe need to have final scalar value inorder to calculate the gradients.","0b540496":"Explanation about the 3rd and 4th examples\n\n**The function breaks when either the of the 2 arguments are not tensor**","4f881a82":"Explanation about example\n\nAs mentioned in the error message, The expected value for the second parameter must be in the range of ** 0 to 2 ** , if we go beyond that, we are given an error","faaef7b8":"# Exploring 5 Interesting PyTorch Functions \n\n### Each Function has 3 examples \n\nAn short introduction about PyTorch and about the chosen functions. \n- ## Function 1 - backward()\n- ## Function 2 - torch.take()\n- ## Function 3 - torch.matmul()\n- ## Function 4 - torch.unsqueeze()\n- ## Function 5 - view()","f8a910f4":"Closing comments about when to use this function.\n\n\n* View function can be used to reshape the Tensor as per our need.\n* We can let the function to decide one axis given all other axes.\n* We need to make sure that it is a contiguous tensor.\n\n","b5f42e4f":"Explanation about example","8bdc6725":"Closing comments about when to use this function\n\n**This function is used in backpropogation to calculate the gradients. This function is vital in building neural networks.**","95ddb74f":"Closing comments about when to use this function\n\n\n***This function can be used to perform matrix multiplication between inputs and weights when building neural networks.***","02448d4f":"## Function 3 - torch.matmul\n\nPerforms Matrix Multiplication on the given 2 input tensors","edc0c77b":"## Function 2 - torch.take()\n\nThis Function is used to fetch and return the values in a tensor using the indexes (The indexes must also be as a tensor)","b6ec8e8f":"Explanation about example","1332fff2":"![](http:\/\/)Explanation about example","714a8871":"* ## Function 1 - backward()\n\nAdd some explanations","001ccf40":"Explanation about example","ffeba62e":"Explanation about example\n\n\n**\nThe above error occurs because after Transposing the Tensor, it becomse non-continguous.\nView function only works on contiguous memory. Thus, the error occurs.**"}}