{"cell_type":{"52c12531":"code","a469275c":"code","9d7f084b":"code","5de0aaec":"code","e1ba20b3":"code","c78f03be":"code","c1fea765":"code","884458ea":"code","a067fa06":"code","7230d634":"code","dff87386":"code","3c24f039":"code","ca9aae1f":"code","8f2e762d":"code","b4fae8b0":"code","e642e797":"code","6e7a9d17":"code","c9710f28":"code","949924dc":"code","22ca8f06":"code","5a6213dd":"code","4705efa7":"code","599e0703":"code","0e56e23d":"code","e3e0a116":"code","e3b61b62":"code","76056882":"code","c77e6ac9":"code","e36698de":"code","2fc9994a":"code","d68361b9":"code","3914e170":"code","199569ce":"code","b5386cbf":"code","070b8008":"code","d3aa8ca2":"code","0020f631":"code","0d92524d":"code","8f1660e8":"code","e26e3b6d":"code","26107715":"code","f23755af":"code","3dc66679":"code","21646843":"code","2a569e8c":"code","5af765e8":"code","ca5bc2bf":"code","916c76b4":"code","663a20c9":"code","b988b533":"markdown","6f3e7563":"markdown","2dce4fa9":"markdown","c930cd32":"markdown","1a7c414d":"markdown","778da9d4":"markdown","f52fc014":"markdown","2f918b45":"markdown","f6dc0b94":"markdown","8b81b682":"markdown","21a676b3":"markdown","cdce84d6":"markdown","44d5b2f3":"markdown","f348be7a":"markdown","6b4cda05":"markdown","0ac947af":"markdown","96dbe675":"markdown","81b02f29":"markdown","24b36954":"markdown","abb6a0a4":"markdown","fa117aea":"markdown","e1f10a51":"markdown","91a81334":"markdown"},"source":{"52c12531":"#Importing the necessary libraries\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\nfrom sklearn.ensemble import *\nfrom sklearn.feature_selection import *\nfrom sklearn.metrics import *\nfrom sklearn.tree import *\nfrom sklearn.svm import *\nfrom sklearn.neighbors import *\nfrom sklearn.linear_model import *\nimport xgboost as xgb\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#To avoid unnecessary warning\nimport warnings\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n        \n\n","a469275c":"train = pd.read_csv('..\/input\/used-cars-price-prediction\/train-data.csv',index_col= 0)\ntrain = train.reindex(np.random.permutation(train.index))\nprint(\"TRAIN SHAPE: \",train.shape)\ntrain.info()\ntrain.head()","9d7f084b":"test = pd.read_csv('..\/input\/used-cars-price-prediction\/test-data.csv',index_col= 0)\nprint(test.shape)\nprint(test.info())\ntest.head()","5de0aaec":"#percentage of missing values\npercent_missing = train.isnull().sum() * 100 \/ len(train)\nprint(percent_missing)","e1ba20b3":"percent_missing = test.isnull().sum() * 100 \/ len(test)\nprint(percent_missing)","c78f03be":"#dropping the \"New_Price\" column that has 86.3% of missing values \n\ntrain.drop(columns =['New_Price'],axis =1, inplace = True)\ntest.drop(columns =['New_Price'],axis =1, inplace = True)","c1fea765":"#Mileage attribute has the least percentage of missing values. Let's fill them up manually.\n\ntrain[train['Mileage'].isnull()]\n\n","884458ea":"#Thanks to Google!\n\ntrain.loc[4904, 'Mileage']  = '23.91 kmpl' \ntrain.loc[4446, 'Mileage']  = '140 kmpl'","a067fa06":"#Now,let's drop the rest of the rows with missing values \n\n\ntrain.dropna(how ='any',inplace = True)\ntest.dropna(how ='any',inplace = True)","7230d634":"#CHECKING IF ALL THE MISSING VALUES ARE TAKEN CARE OF\ntrain.info()\n","dff87386":"test.info()","3c24f039":"#Mileage - Before we change the datatype, we must extract the actual mileage in numbers without the \"Kmpl\" \ntrain['Mileage']= train['Mileage'].str[:-5]\ntrain['Mileage']=train['Mileage'].astype(float);\n\ntest['Mileage']= test['Mileage'].str[:-5]\ntest['Mileage']=test['Mileage'].astype(float);","ca9aae1f":"#Engine - Before we change the datatype, we must extract the actual engine cc in numbers without the \"CC\" string \n\ntrain['Engine'] = train['Engine'].str.strip('CC')\ntrain['Engine']= train['Engine'].astype(float);\n\ntest['Engine'] = test['Engine'].str.strip('CC')\ntest['Engine']= test['Engine'].astype(float);","8f2e762d":"train['Power'] = train['Power'].fillna(value = \"null\")\ntrain[\"Power\"]= train[\"Power\"].replace(\"null\", \"NaN\")\ntrain['Power'] = train['Power'].str.strip('bhp ')\ntrain['Power'] = train['Power'].astype(float)\n\ntrain.dropna(how ='any',inplace = True)","b4fae8b0":"test['Power'] = test['Power'].fillna(value = \"null\")\ntest[\"Power\"]= test[\"Power\"].replace(\"null\", \"NaN\")\ntest['Power'] = test['Power'].str.strip('bhp ')\ntest['Power'] = test['Power'].astype(float)\n\ntest.dropna(how ='any',inplace = True)","e642e797":"#Year\ntrain['Year'] = train['Year'].astype(str)\n\ntest['Year'] = test['Year'].astype(str)\n","6e7a9d17":"#CHECKING\ntrain.info()","c9710f28":"test.info()","949924dc":"train.to_csv('trainfinal.csv')\ntest.to_csv('testfinal.csv')","22ca8f06":"x = pd.read_csv('trainfinal.csv')\nprint(x.shape)\nx.head()","5a6213dd":"#dropping the unnamed:0 column\nx.drop(columns=['Unnamed: 0'],axis=1,inplace = True)","4705efa7":"x[\"breakdown\"] = x.Name.str.split(\" \")\nx[\"breakdown\"].head()","599e0703":"#Lets store the brand name in our new column\nbrand_list=[]\nfor i in range(len(x)):\n    a = x.breakdown[i][0]\n    brand_list.append(a)\n\nx['Brand'] = brand_list","0e56e23d":"# We don't need these columns now\nx.drop(columns=['Name','breakdown'],axis=1,inplace=True)","e3e0a116":"#Lets analyse the new attribute\nx['Brand'].unique()","e3b61b62":"duplic = {'ISUZU': 'Isuzu'}\nx.replace({\"Brand\": duplic},inplace = True) ","76056882":"#CHECKING\nx['Brand'].value_counts()\n\n#Sorted!","c77e6ac9":"#Lets encode our categorical values \n\nlabelencoder = LabelEncoder()\nlabel_array=[]\n\nlabel_array = ['Location','Year','Fuel_Type','Transmission','Owner_Type','Brand']\n\nfor ele in label_array:\n    x[ele] = labelencoder.fit_transform(x[ele])\n","e36698de":"#CHECKING\nx.head()","2fc9994a":"#feature selection\nX_fs = x[['Location', 'Year', 'Kilometers_Driven', 'Fuel_Type', 'Transmission', 'Owner_Type',\n       'Mileage', 'Engine', 'Power', 'Seats', 'Brand']]\n\ny_fs = x['Price']\n\n\ny_fs = y_fs*100\ny_fs = y_fs.astype(int)\n\n\nbestfeatures = SelectKBest(score_func=chi2, k=5)\nfit = bestfeatures.fit(X_fs,y_fs)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_fs.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nfea = pd.DataFrame(featureScores.nlargest(10,'Score'))\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","d68361b9":"selection= ExtraTreesRegressor()\nselection.fit(X_fs,y_fs)\n\nplt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X_fs.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()\n","3914e170":"#Preparing Training set \nX = np.array(x.drop(['Price'],axis = 1)) \nY = x.Price.values","199569ce":"#splitting into train and test set\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=25)","b5386cbf":"#selecting best models\n\nmodel_selc = [LinearRegression(),\n             DecisionTreeRegressor(),\n             RandomForestRegressor(n_estimators=10),\n             KNeighborsRegressor(),\n             GradientBoostingRegressor()]\n\nkfold = RepeatedKFold(n_splits=5, n_repeats=10, random_state= None)\ncv_results = []\ncv_results_mean =[]\nfor ele in model_selc:\n    cross_results = cross_val_score(ele, X_train, Y_train, cv=kfold, scoring ='r2')\n   \n    cv_results.append(cross_results)\n   \n    cv_results_mean.append(cross_results.mean())\n    print(\"\\n MODEL: \",ele,\"\\nMEAN R2:\",cross_results.mean() )\n","070b8008":"#Let's try xgboost now\nmy_xgb = xgb.XGBRegressor(objective='reg:linear',learning_rate = 0.1, n_estimators = 100,verbosity = 0,silent=True)\nxgb_results = cross_val_score(my_xgb, X_train, Y_train, cv=kfold, scoring ='r2')\nprint(\"\\n MODEL: XGBOOST\",\"\\nMEAN R2:\",xgb_results.mean() )","d3aa8ca2":"#We use GridSearch for fine tuning Hyper Parameters\n\nfrom sklearn.model_selection import *\n\n\nn_estimator_val = np.arange(100,400,100).astype(int)\nmax_depth_val = [2,3,4]\n\n\ngrid_params = { 'loss' : ['ls'] ,\n               'learning_rate' : [0.1],\n               'n_jobs': [-1],\n               'n_estimators' : n_estimator_val,\n               'max_depth' : max_depth_val\n              }","0020f631":"gs = GridSearchCV(xgb.XGBRegressor(silent= True),grid_params,verbose=1,cv=5,n_jobs =-1)\ngs_results = gs.fit(X_train,Y_train)","0d92524d":"#To Display the Best Score\ngs_results.best_score_","8f1660e8":"#To Display the Best Estimator\ngs_results.best_estimator_","e26e3b6d":"#To Display the Best Parameters\ngs_results.best_params_","26107715":"XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bynode=1, colsample_bytree=0.8, gamma=0.5,\n       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n       max_depth=4, min_child_weight=10, missing=None, n_estimators=1900,\n       n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=None, subsample=1.0, verbosity=1)","f23755af":"folds = 3\nparam_comb = 10\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nparams = { 'n_jobs': [-1],\n        'n_estimators' : n_estimator_val,\n        'learning_rate' : [0.1],\n        'min_child_weight': [9],\n        'gamma': [0.5],\n        'subsample': [0.6],\n        'colsample_bytree': [0.8, 1.0],\n        'max_depth': [3, 4]\n        }\nxgb_regrsv = xgb.XGBRegressor()\n","3dc66679":"random_search = RandomizedSearchCV(xgb_regrsv, params, n_iter=param_comb, scoring='r2', \n                                   n_jobs=-1, cv=5 )\n","21646843":"random_search.fit(X_train, Y_train);","2a569e8c":"random_search.best_score_","5af765e8":"random_search.best_estimator_","ca5bc2bf":"xgb_tuned = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bynode=1, colsample_bytree=0.8, gamma=0.5,\n       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n       max_depth=4, min_child_weight=9, missing=None, n_estimators=300,\n       n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1.0, verbosity=0)","916c76b4":"xgb_tuned.fit(X_train,Y_train)\ny_pred =xgb_tuned.predict(X_test)\n\nprint(\"Training set accuracy: \",xgb_tuned.score(X_train,Y_train))\nprint(\"Test set accuracy    : \",xgb_tuned.score(X_test,Y_test))","663a20c9":"\nprint(\"\\t\\tError Table\")\nprint('Mean Absolute Error      : ', mean_absolute_error(Y_test, y_pred))\nprint('Mean Squared  Error      : ', mean_squared_error(Y_test, y_pred))\nprint('Root Mean Squared  Error : ', np.sqrt(mean_squared_error(Y_test, y_pred)))\nprint('R Squared Error          : ', r2_score(Y_test, y_pred))","b988b533":"We could settle here but let's try randomized search cross validation.","6f3e7563":"After some more time of trying various hyperparameters and tuning,","2dce4fa9":"We can see that there are missing values in the following columns:\n* Engine          \n* Power            \n* Seats  \n* New_Price          ","c930cd32":"##### GRID SEARCH\n\nGrid Search can be thought of as an exhaustive and computationally expensive method for selecting a model.<br>\n\n**For example** : <br>\n\nSearching 20 different parameter values for each of 4 parameters will require 160,000 trials of cross-validation. This equates to 1,600,000 model fits and 1,600,000 predictions if 10-fold cross validation is used","1a7c414d":"# DATA GATHERING ","778da9d4":"Ensure to apply the changes to both the test and train sets inorder to maintain uniformity.","f52fc014":"This plot also validates our selection of important features. ","2f918b45":"# MODEL BUILDING","f6dc0b94":" ### Tuning HyperParameters for xgboost","8b81b682":"**The name column has a diverse number of values. \nLet's break it down and extract the brand name of the car.**\n","21a676b3":"# Feature engineering","cdce84d6":"AND THERE WE HAVE A WINNER TO GO THROUGH GRIDSEARCH FOR HYPER-PARAMTER TUNING!!","44d5b2f3":"Now, lets check how various metrics have evaluated our model on the test set","f348be7a":"We can see that \"Isuzu\" carries some duplicated values. \n<br>\nLet's sort that out","6b4cda05":"# DATA CLEANING AND ASSESSMENT","0ac947af":"# CONCLUSION\n\nThis model is ready to be deployed in a pipeline. <br>\nWe have accomplished the task of building a good model for our used cars' prediction purposes. \n\nWe will be adding the exploratory data analysis of the same dataset soon. <br>\n\n\n\n##### Please do let us know if you have any constructive feedbacks to help us improve our work.\n##### If you guys could learn something from our notebook, do upvote and support us! **\nThis brings us to the end of our first kaggle notebook. \n","96dbe675":"Therefore, the most important features are \"Kilometers driven\" followed by \"Engine\", \"Power\", \"Brand\".","81b02f29":"##### RANDOMIZED SEARCH\nRandomized Search sets up a grid of hyperparameter values and selects random combinations to train the model and score.\nThis allows you to explicitly control the number of parameter combinations that are attempted. \nThe number of search iterations is set based on time or resources. \n\n\nWhile it\u2019s possible that RandomizedSearchCV will not find as accurate of a result as GridSearchCV, it surprisingly picks the best result more often than not and in a fraction of the time it takes GridSearchCV would have taken. Given the same resources, Randomized Search can even outperform Grid Search. ","24b36954":"\n> Ever heard of the terminologies like used car, a pre-owned vehicle or a second hand car?  <br>\n> \n> Used cars are sold through a varirty of outlets, including franchise, independent car dealers and rental car companies.Have you ever wondered how their prices are evaluated for sale?\n> \n> You cannot take a wild guess at this because the Indian used car market was valued at USD 24.24 billion in 2019. \n> Let's build a model that helps the vendors evaluate the used cars.","abb6a0a4":"##### Quality issue 1:  Missing values","fa117aea":"##### Quality issue 2:  Erroneous values and datatypes","e1f10a51":"Some of our attributes' datatypes could be changed inorder to make the modelling process easier. Those attributes are \n* Mileage\n* Engine\n* Power\n* Year","91a81334":"# INTRODUCTION\n"}}