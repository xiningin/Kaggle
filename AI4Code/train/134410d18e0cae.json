{"cell_type":{"179352e5":"code","a74573a8":"code","0970bfaf":"code","d4654fad":"code","6f01aad6":"code","466ac66b":"code","d165c4c3":"code","a14885e5":"code","dd630267":"code","929c2cc7":"code","ab3f2afb":"code","29a46c63":"code","23d357bc":"code","916ee5e9":"code","757bbedf":"code","bfd62165":"code","384163ac":"code","8ee95003":"code","22a7de96":"code","34e56dd1":"markdown","4220bcc0":"markdown","a74b04bd":"markdown","104056b3":"markdown","73ff8131":"markdown","98dca352":"markdown","8fa16bd2":"markdown","30cb97c4":"markdown","e3fc351f":"markdown","f2c128e5":"markdown","3bf94d8a":"markdown","2bd0e8ad":"markdown","9d935f29":"markdown","d5928c07":"markdown","a19ce187":"markdown","9723a925":"markdown","01d25230":"markdown","d26cf5ff":"markdown","e9167a34":"markdown","1f93c245":"markdown","fda88518":"markdown","e925fcad":"markdown","5f43e5c1":"markdown","6abddf52":"markdown","ed9cfd34":"markdown","a670774a":"markdown"},"source":{"179352e5":"!pip install transformers","a74573a8":"import os\nimport gc\nimport copy\nimport datetime\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport transformers\nfrom transformers import (set_seed,\n                          TrainingArguments,\n                          Trainer,\n                          GPT2Config,\n                          GPT2Tokenizer,\n                          AdamW, \n                          get_linear_schedule_with_warmup,\n                          GPT2ForSequenceClassification)\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport plotly.graph_objects as go\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","0970bfaf":"df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\ndf.sample(10)","d4654fad":"def prep_text(text_df):\n    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\ndf[\"excerpt\"] = prep_text(df[\"excerpt\"])\ntest_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])","6f01aad6":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n\n# create folds\ndf = create_folds(df, num_splits=5)","466ac66b":"class CONFIG:\n    seed = 120\n    max_len = 305\n    train_batch = 16\n    valid_batch = 16\n    epochs = 10\n    learning_rate = 2e-5\n    splits = 5\n    n_labels=1\n    scaler = amp.GradScaler()\n    model='gpt2'\n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n    # default to left padding\n    tokenizer.padding_side = \"left\"\n    # Define PAD Token = EOS Token = 50256\n    tokenizer.pad_token = tokenizer.eos_token\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","d165c4c3":"sen_length = []\n\nfor sentence in tqdm(df[\"excerpt\"]):\n   \n    token_words = CONFIG.tokenizer.encode_plus(sentence)[\"input_ids\"]\n    sen_length.append(len(token_words))\n    \nprint('maxlenth of all sentences are  ', max(sen_length))","a14885e5":"def set_seed(seed = CONFIG.seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG.seed)","dd630267":"class GPT2Dataset(Dataset):\n    def __init__(self,df):\n        self.text = df['excerpt'].values\n        self.target = df['target'].values\n        self.max_len = use_tokenizer.model_max_length if CONFIG.max_len is None else CONFIG.max_len\n        self.tokenizer = CONFIG.tokenizer\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer('<|startoftext|>'+ text + '<|endoftext|>', truncation=True, max_length=self.max_len, padding=\"max_length\")\n\n        return {\n            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'target': torch.tensor(self.target[index], dtype=torch.float)\n        }","929c2cc7":"model = GPT2ForSequenceClassification.from_pretrained(\n    CONFIG.model,\n    num_labels = CONFIG.n_labels,\n    output_attentions = False,\n    output_hidden_states = False\n)\n\n\n# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n# otherwise the tokenizer and model tensors won't match up\nmodel.resize_token_embeddings(len(CONFIG.tokenizer))\n# fix model padding token id\nmodel.config.pad_token_id = model.config.eos_token_id\n# Tell pytorch to run this model on the GPU.\n\nmodel.cuda()","ab3f2afb":"param_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0}\n    ]  \noptimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)","29a46c63":"def get_data(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = GPT2Dataset(df_train)\n    valid_dataset = GPT2Dataset(df_valid)\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG.valid_batch, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","23d357bc":"train_dataloader,validation_dataloader=get_data(0)","916ee5e9":"def loss_fn(output,target):\n     return torch.sqrt(nn.MSELoss()(output,target))\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","757bbedf":"def run(optimizer,scheduler):\n    \n    scaler=CONFIG.scaler\n    training_stats = []\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_rmse = np.inf\n    total_t0 = time.time()\n    epochs=CONFIG.epochs\n    for epoch_i in range(0, epochs):\n        print(\"\")\n        print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n        t0 = time.time()\n        total_train_loss = 0\n        data_size=0\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            tr_loss=[]\n            b_input_ids = batch['ids'].to(CONFIG.device)\n            b_input_mask = batch['mask'].to(CONFIG.device)\n            b_labels = batch['target'].to(CONFIG.device)\n            batch_size = b_input_ids.size(0)\n            model.zero_grad() \n            with amp.autocast(enabled=True):\n                output= model(b_input_ids,labels=b_labels,attention_mask=b_input_mask,token_type_ids=None)          \n                loss = output[0]\n                tr_loss.append(loss.item()\/len(output))\n            scheduler.step()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        avg_train_loss = np.mean(tr_loss)    \n        training_time = format_time(time.time() - t0)\n        gc.collect()\n        print(\"\")\n        print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n        print(\"\")\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n        model.eval()\n        val_loss = 0\n        allpreds = []\n        alltargets = []\n        for batch in validation_dataloader:\n            losses = []\n            with torch.no_grad():\n                device=CONFIG.device\n                ids = batch[\"ids\"].to(device)\n                mask = batch[\"mask\"].to(device)\n                target = batch[\"target\"].to(device)\n                output= model(ids,labels=target,attention_mask=mask,token_type_ids=None) \n                loss = output[0]\n                losses.append(loss.item()\/len(output))\n                allpreds.append(output[1].detach().cpu().numpy())\n                alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n        allpreds = np.concatenate(allpreds)\n        alltargets = np.concatenate(alltargets)\n        val_rmse=mean_squared_error(alltargets, allpreds, squared=False)\n        losses = np.mean(losses)\n        gc.collect() \n        validation_time = format_time(time.time() - t0)\n        print(\"  Validation Loss: {0:.4f}\".format(losses))\n        print(\"  Validation took: {:}\".format(validation_time))\n        if val_rmse <= best_rmse:\n            print(f\"Validation RMSE Improved ({best_rmse} -> {val_rmse})\")\n            best_rmse = val_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"rmse{:.4f}_epoch{:.0f}.bin\".format(best_rmse, epoch_i)\n            torch.save(model.state_dict(), PATH)\n            print(\"Model Saved\")\n        training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': losses,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    ) \n    print(\"\")\n    print(\"Training complete!\")\n    return training_stats  ","bfd62165":"def Visualizations(training_stats):\n    pd.set_option('precision', 4)\n    df_stats = pd.DataFrame(data=training_stats)\n    df_stats = df_stats.set_index('epoch')\n    layout = go.Layout(template= \"plotly_dark\",title='GPT2 loss curve')\n    fig = go.Figure(layout=layout)\n    fig.add_trace(go.Scatter(x=df_stats.index, y=df_stats['Training Loss'],\n                    mode='lines+markers',\n                    name='Training Loss'))\n    fig.add_trace(go.Scatter(x=df_stats.index, y=df_stats['Valid. Loss'],\n                    mode='lines+markers',\n                    name='Validation Loss'))\n    fig.show()","384163ac":"# Defining LR Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=len(train_dataloader)*CONFIG.epochs\n)\nlrs = []\nfor epoch in range(1, CONFIG.epochs + 1):\n    if scheduler is not None:\n        scheduler.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()","8ee95003":"df1=run(optimizer,scheduler)","22a7de96":"Visualizations(df1)","34e56dd1":"# LET'S START","4220bcc0":"# <p style=\"color:#159364; font-family:cursive;\">GET THE PREPARED DATA<\/center><\/p>","a74b04bd":"# <p style=\"color:#159364; font-family:cursive;\">IMPORT THE LIBRARIES<\/center><\/p>","104056b3":"# <p style=\"color:#159364; font-family:cursive;\">OPTIMIZER<\/center><\/p>","73ff8131":"# <p style=\"color:#159364; font-family:cursive;\">LEARNING RATE SCHEDULER <\/center><\/p>","98dca352":"**RUN ON THE SET SCHEDULER**","8fa16bd2":"![openAI-GPT-2-3.png](attachment:03fa06cb-be6e-47fa-8b5c-9965cca351ac.png)","30cb97c4":"# <p style=\"color:#159364; font-family:cursive;\">FOLD:0<\/center><\/p>","e3fc351f":"* Every tensor passed to the model should be the same length.\n* \n* If the text is shorter than max_length number of tokens, it will be padded to the max_length using the padding token. In addition, an attention mask will be returned that needs to be passed to the model to tell it to ignore the padding tokens.\n* \n* If text is longer than max_length tokens, it will be truncated without the eos_token. This isn't a problem.","f2c128e5":"# <p style=\"color:#159364; font-family:cursive;\">LOOK AT THE DATA<\/center><\/p>","3bf94d8a":"ABSTRACT FROM THE PAPER:  [Language Models are Unsupervised Multitask Learners](https:\/\/d4mucfpksywv.cloudfront.net\/better-language-models\/language-models.pdf)\n> \n> Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples.\n> The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, **GPT-2**,is a 1.5B parameter Transformer that achieves\n> state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting\n> but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from\n> their naturally occurring demonstrations.\n> ","2bd0e8ad":"# <p style=\"color:#159364; font-family:cursive;\">A BIT OF PREPROCESSING<\/center><\/p>","9d935f29":"![20210513_201636.png](attachment:0824c8f3-4c6d-4135-ab35-e5c2e294e64b.png)","d5928c07":"# <p style=\"color:#159364; font-family:cursive;\">VISUALIZATION FUNCTION <\/center><\/p>","a19ce187":"\n<p style=\"color:#159364; font-family:cursive;\">INSTALL THE TRANSFORMERS PACKAGE FROM THE HUGGING FACE LIBRARY<\/center><\/p>\n","9723a925":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE LOSS AND TIME FUNCTIONS<\/center><\/p>","01d25230":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE DATASET CLASS<\/center><\/p>","d26cf5ff":"Code taken from:https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds","e9167a34":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","1f93c245":"**LINEAR SCHEDULE WITH WARMUP**","fda88518":" <h1 style=\"font-family:verdana;\"> <center>OpenAI GPT2\ud83d\udd25FINETUNING WITH PYTORCH  <\/center> <\/h1>","e925fcad":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE FUNCTION FOR TRAINING,VALIDATION AND RUNNING<\/center><\/p>","5f43e5c1":"# <p style=\"color:#159364; font-family:cursive;\">REPRODUCIBILITY<\/center><\/p>","6abddf52":"# <p style=\"color:#159364; font-family:cursive;\">CREATE FOLDS<\/center><\/p>","ed9cfd34":"# <p style=\"color:#159364; font-family:cursive;\">TRAINING CONFIGURATION<\/center><\/p>","a670774a":"# <p style=\"color:#159364; font-family:cursive;\">MODEL:GPT2 FOR SEQUENCE CLASSIFICATION from \ud83e\udd17 <\/center><\/p>"}}