{"cell_type":{"d7c73da4":"code","19c723fb":"code","07d835c4":"code","590ff1c2":"code","04d4a6e2":"code","49b1e1d9":"code","232bc20c":"code","77956bd5":"code","0950ebab":"code","9e87caa2":"code","7446e473":"code","9f78dd80":"code","07e97726":"code","90d517df":"code","f5c93f92":"code","f01f0c31":"code","970f8da7":"code","e2084052":"code","b021f29c":"code","f5b54385":"code","d1d23bf2":"code","49672eea":"code","3a46736e":"code","b6006d0f":"code","6655ea5a":"code","018df925":"code","c9bd3b53":"code","5e445319":"code","1ca3cb53":"code","238dcb5d":"code","c74e4a67":"code","7fd8ce91":"code","e5a49e7e":"code","ce94cd6c":"code","7b3dd1ff":"code","467acd77":"code","b65651cf":"code","edbbedcc":"code","03f5b826":"code","15afd0c0":"code","819341d2":"code","f00f370b":"code","669d9233":"code","7cfb5860":"code","87749ed0":"code","44b935ea":"code","8786784a":"code","b9a20606":"code","6243f223":"code","29e648ee":"code","ff58cfb0":"code","70831fd3":"code","b4a9837c":"code","a07e9aab":"code","fefb5f18":"code","b39447fe":"code","ceea60a1":"code","0cce1e21":"code","ecd5d385":"code","e6c06a5c":"code","00a31a06":"code","5dd07619":"markdown","06d0cf8f":"markdown","bbd88f70":"markdown","43e16eef":"markdown","ba6db603":"markdown","70fd9466":"markdown","a12193fa":"markdown","2ec66af1":"markdown","e6bd18a3":"markdown","9eecbf58":"markdown","c808dbb5":"markdown","484afded":"markdown","ab8fb017":"markdown","d1a47ad0":"markdown","7385b19c":"markdown","1a037215":"markdown","d5df39ad":"markdown","9be6e22f":"markdown"},"source":{"d7c73da4":"#!pip -q install --upgrade --ignore-installed catboost\n#!pip -q install --upgrade --ignore-installed lightgbm\n#!pip -q install --upgrade --ignore-installed xgboost\n#!pip -q install --upgrade --ignore-installed numpy, scipy","19c723fb":"import catboost as cb\nimport xgboost as xb\nimport lightgbm as lgb","07d835c4":"cb.__version__, xb.__version__, lgb.__version__\n# CatBoost,   XGBoost,   LightGBM","590ff1c2":"import pandas as pd\nimport numpy as np\nimport sklearn","04d4a6e2":"np.__version__, pd.__version__, sklearn.__version__","49b1e1d9":"from sklearn.datasets import load_boston","232bc20c":"dict_ = load_boston()","77956bd5":"dict_.keys()","0950ebab":"train_df = pd.DataFrame(dict_.data, index=np.arange(dict_.data.shape[0]), columns=dict_.feature_names)","9e87caa2":"train_df['target'] = dict_.target","7446e473":"train_df.head()","9f78dd80":"train_df.nunique()","07e97726":"from sklearn.ensemble import AdaBoostRegressor as AdaBoost\nfrom sklearn.tree import DecisionTreeRegressor as dt","90d517df":"base_est = dt(criterion                = 'mse',  # 'mse', 'friedman_mse', 'mae'\n              splitter                 = 'best', # 'best', 'random'\n              max_depth                = 2,      #     -->                                                                             ## Can help reduce Overfitting. (by dec.) ##\n              min_samples_split        = 10,     # int default = 2 (can overfit), can be float (0, 1.0].                               ## Can help reduce Overfitting. (by inc.) ##\n              min_samples_leaf         = 5,      # int deafult = 1 (can overfit), can be float (0, 1.0].                               ## Can help reduce Overfitting. (by inc.) ##\n              min_weight_fraction_leaf = 0.05,   # min fraction of total weight of all points that needs to be in a leaf node (float). ## Can help reduce Overfitting. (by dec.) ##\n              max_features             = 'sqrt', # int, float (0, 1.0], 'auto', 'sqrt', 'log2', None.                                  ## Can help reduce Overfitting. (by dec.) ##\n              random_state             = 2019,\n              max_leaf_nodes           = 5,      # int (best leaf nodes are selected by best impurity values), None.                   ## Can help reduce Overfitting. (by dec.) ##\n              min_impurity_decrease    = 0.05,   # min required impurity decrease because of a split.                                  ## Can help reduce Overfitting. (by inc.) ##\n              presort                  = False)","f5c93f92":"ab = AdaBoost(base_estimator = base_est,\n              n_estimators   = 300,\n              learning_rate  = 0.1,\n              loss           = 'linear',          # 'linear', 'square', 'exponential'\n              random_state   = 2019)","f01f0c31":"ab = ab.fit(X = train_df.drop('target', axis=1), y = train_df['target'], sample_weight=np.ones(train_df.shape[0]))","970f8da7":"print(\"Base Estimator\")\nprint(ab.base_estimator_)\n\nprint(\"\\nEstimator Erros:\")\nprint(ab.estimator_errors_[0:10])\n\nprint(\"\\nEstimator Weights:\")\nprint(ab.estimator_weights_[0:10])\n\nprint(\"\\nFeature Importances:\")\nprint(ab.feature_importances_)","e2084052":"base_ests_temp = ab.estimators_\nprint(len(base_ests_temp))","b021f29c":"dt_0 = base_ests_temp[0]\n\nprint(\"First Learner's Score (rmse):\")\nprint(np.power(dt_0.score(X = train_df.drop('target', axis=1), y = train_df['target']), 1.\/2))\n\nprint(\"\\nFirst Learner's Features' Importances\")\ndt_0.feature_importances_","f5b54385":"reg_model = xb.XGBRegressor(\n                    max_depth         =3,\n                    learning_rate     =0.8,\n                    silent            =True,\n                    objective         ='reg:linear',\n                    booster           ='gbtree',    # 'gbtree', 'gblinear', 'dart'\n                    nthread           =2,           # Number of parallel 'threads'\n                    #n_jobs           =,            # same as nthread \n                    #gamma             =0.01,       # Min Loss reduction required on partition\n                    #min_child_weight  =0.05,       # Min. sum of hessian weights needed in a child\n                    #max_delta_step    =0.001,      # Max delta step for each tree's weight estimation\n                    subsample         =0.8,         # Subsample of data for each tree\n                    colsample_bytree  =1.0,         # Subsample by column for each tree\n                    colsample_bylevel =0.9,         # Subsample by column by level in each tree\n                    #reg_alpha         =0.01,       # L1 regularization\n                    #reg_lambda        =0.02,       # L2 regularization\n                    #scale_pos_weight =,            # For Balancing of +ve and -ve weights  ## For Binary Classification. But it is present here also...  ##\n                    #base_score       =,            # Initial prediction score of all instances, global bias. ## Helpful in Classification. Set it to mean of observations for imbalanced dataset. ##\n                    seed              =2019,\n                    #random_state     =,            # same as seed\n                    missing           =0.0,         # None (np.nan), or some value to replace missing values.\n)\n\n# 'dart' is new booster algorithm in xgboost which uses Dropout for trees (randomly) to reduce overfitting.","d1d23bf2":"from sklearn.model_selection import train_test_split as tts\n\nXtrain, Xvalid, ytrain, yvalid = tts(train_df.drop('target', axis=1), train_df['target'])","49672eea":"reg_model = reg_model.fit(X=Xtrain, y=ytrain,\n              sample_weight=np.ones(Xtrain.shape[0]),\n              eval_set=[(Xtrain, ytrain), (Xvalid, yvalid)],      # A array of set, of X and y's, as [(X1, y1), (X2, y2) ... ]\n              eval_metric='rmse',                                 # Inbuilt eval metric or callable\n              early_stopping_rounds=50,                           # For early stopping\n              verbose=False,\n              xgb_model=None                                      # Used with keep_training_model=True\n             )","3a46736e":"reg_model.evals_result_['validation_1']['rmse'][0:10]","b6006d0f":"reg_model.feature_importances_","6655ea5a":"train_data = xb.DMatrix(Xtrain, label=ytrain, feature_names=train_df.drop('target', axis=1).columns.values)\nvalid_data = xb.DMatrix(Xvalid, label=yvalid, feature_names=train_df.drop('target', axis=1).columns.values)","018df925":"#\n# Top problem of Boosting is that it can overfit very easily. So most parameters you will see here are to reduce overfitting.\n# For better generality and reducing overfitting at the same time you will have to optimize select params (bec. of time boundation).\n# One more problem which `still` can occur is target leakage, which CatBoost tries to reduce.\n#\n\nparams = {\n    ####### Learning Task Params #######\n    'objective': 'reg:linear',                                                # 'reg:linear', 'reg:logistic', 'binary:logistic', 'binary:logitraw', 'binary:hinge', 'count:poisson' (deafult 'max_delta_step': 0.7), 'survival:cox', 'multi:softmax', 'multi:softprob', 'ran:pairwise', 'rank:ndcg', 'rank:map', 'reg:gamma' or 'reg:tweedie'.\n    'base_score': 0.5,                                                        # default = 0.5; initial pred score for all instances, global bias. Given sufficient iterations changing this won't have much effect\n    'eval_metric': 'rmse',                                                    # 'rmse', 'mae', 'error', 'error@t', 'merror', 'mlogloss', 'auc', 'aucpr', 'ndcg', 'map', 'ndcg@n', 'map@n', 'ndcg-', 'map-', 'ndcg@-', 'map@-', 'poisson-nloglik', 'gamma-nloglik', 'cox-nloglik', 'gamma-deviance' or 'tweedie-nloglik' (with specified 'tweedie_variance_power')\n    'seed': 2019,\n    \n    ####### General Parameters #######\n    'booster': 'gbtree',                                                      # 'gbtree', 'gblinear', 'dart'\n    'verbosity': 0,\n    'nthread': 2,                                                             # Number of threads to use. Default = max threads available.\n    'disable_default_eval_metric': 0,                                         # To disable default eval metric, that is always computed.\n    #'num_pbuffer':,                                                          # Automatically set by XGBoost. For storing prediction results of last boosting step. Set eqaul to number of training instances.\n    #'num_feature':,                                                          # Automatically set by XGBoost. Features used for boosting. Set to max features by XGBoost.\n    \n    ####### Params for Booster #######\n    'eta': 0.1,                                                               # alias: 'learning_rate'\n    #'gamma':0.01,                                                            # Min Loss reduction required on partition\n    #'min_child_weight':0.05,                                                 # Min. sum of hessian weights needed in a child\n    #'max_delta_step':0.001,                                                  # Max delta step for each tree's weight estimation. Max output for leaves = learning_rate*max_delta_step\n    'subsample':0.8,                                                          # Subsample of data for each tree\n    'colsample_bytree':1.0,                                                   # Subsample columns for each tree\n    'colsample_bylevel':0.9,                                                  # Subsample columns by level in each tree\n    'colsample_bynode':0.9,                                                   # Subsample columns by node in each tree\n    #'colsample_by*':0.8,                                                     # For setting all colsample_by* params\n    #'lambda': 0.01,                                                          # L2 reg\n    #'alpha': 0.02,                                                           # L1 reg\n    'tree_method': 'auto',                                                    # 'auto', 'exact', 'approx', 'hist', 'gpu_exact' or 'gpu_hist'\n    #'sketch_eps': 0.03,                                                      # For 'approx'tree method. For number of bins. n_bins = 1\/sketch_eps\n    #'scale_pos_weight': 0.5,                                                 # For Balancing of +ve and -ve weights  ## For Binary Classification ##\n    'updater': 'grow_colmaker,prune',                                         # Sequence of tree updaters to run. 'grow_colmaker,distcol,grow_histmaker,grow_skmaker,sync,refresh,prune'. For distributed with 'hist' tree method uses 'grow_histmaker,prune'.\n    'refresh_leaf': 1,                                                        # 0 -> Update only node stats, 1 -> Update both leaf and node stats (default).\n    'process_type': 'default',                                                # 'deafult' -> Normal Boosting. OR 'update' -> For each tree in model specified updaters are run on them. Can have smaller number of trees in final model. Use 'refresh,prune' with it. You cannot use updater that creates new trees with it.\n    #'grow_policy': 'depthwise',                                              # For 'hist' tree method. 'depthwise' -> Split nodes closest at root OR 'lossguide' -> Split at highest loss change.\n    #'max_leaves': 0,                                                         # For 'lossguide' grow policy. Max nodes to be added.\n    #'max_bin': 256,                                                          # For 'hist' tree method. Max bins for continuous features. higher value -> improves optimality of splits -> Higher computation time.\n    #'predictor': 'cpu_predictor',                                            # 'cpu_predictor' OR 'gpu_predictor' to be used with 'gpu_exact', 'gpu_hist' tree method.\n    #'num_parallel_tree': 1,                                                  # For Boosted Random Forest. For number of parallel trees during each iteration.\n    \n    ####### For 'reg:tweedie' objective #######\n    #'tweedie_variance_power':1.5,                                            # range:(1, 2); towards 1 -> Poisson dist, towards 2 -> Gamma dist\n    \n    ####### For 'gblinear' booster #######\n    #'lambda': 0,                                                             # L2 reg\n    #'alpha': 0,\n    #'updater': 'shotgun',                                                    # 'shotgun' -> hogwild parallelism (non-deterministic results) or 'coord_descent' -> multithreaded (deterministic)\n    #'feature_selector': 'cyclic',                                            # 'cyclic', 'shuffle', 'random', 'greedy', 'thrifty'\n    #'top_k': 10                                                              # For 'greedy' and 'thrifty' feature selectors. To speed them up, at cost of some accuracy.\n    \n    ####### For 'dart' booster #######\n    #'sample_type': 'uniform',                                                # 'uniform' or 'weighted'. For selecting trees to drop. 'weighted' uses tree weight used in calculating result.\n    #'normalize_type': 'tree',                                                # 'tree' or 'forest'. For setting weight of new and dropped trees.\n    #'rate_drop': 0.2,                                                        # Dropout rate\n    #'one_drop': True,                                                        # True for always dropping atleast one tree.\n    #'skip_drop': 0.3                                                         # Probability of skipping Dropout at each iteration. Higher priority than 'rate_drop' and 'one_drop'.\n}","c9bd3b53":"results = {} # Will hold evaluation results","5e445319":"reg_model = xb.train(params, \n                     train_data,                                              # DMatrix\n                     num_boost_round=1000,                                    # Number of Boosting rounds\/trees.\n                     evals=[(train_data, \"train\"), (valid_data, \"valid\")],\n                     obj=None,                                                # Customized Objective Function\n                     feval=None,                                              # Customized evaluation time\n                     maximize=False,                                          # if True maximize eval function\n                     early_stopping_rounds=50,\n                     evals_result=results,                                    # takes dict where results will be stored\n                     verbose_eval=100,\n                     xgb_model=None,\n                     callbacks=None,                                          # list of callback funcs or callback func\n                    )","1ca3cb53":"reg_model.best_iteration, reg_model.best_ntree_limit, reg_model.best_score","238dcb5d":"reg_model.get_split_value_histogram('CRIM').head()      # Get split values by feature name","c74e4a67":"results['train']['rmse'][-10:]","7fd8ce91":"kwargs = {}   # kwargs is not supported in sklearn. It may cause unexpected issues. (From LightGBM)","e5a49e7e":"reg_model = lgb.LGBMRegressor(\n                  boosting_type    ='gbdt',                                     # 'gbdt', 'dart', 'goss', 'rf'(RandomForest)\n                  num_leaves       =31,\n                  max_depth        =-1,\n                  learning_rate    =0.1,\n                  n_estimators     =100,                                        # Number of Boosted Trees to fit.\n                  subsample_for_bin=200000,                                     # Number of samples for costructing bins.\n                  objective        =None,                                       # string, callable or None; eg 'regression', 'binary', 'multiclass', 'lambdarank'\n                  class_weight     =None,                                       # For multiclass. dict, 'balanced' or None. Dict as {class_label: weight, ...}. For binary you can use 'is_unbalance' or 'scale_pos_weight'\n                  min_split_gain   =0.0,                                        # Min loss reduction required for splitting\n                  min_child_weight =0.001,                                      # Min. hessian weight sum required in a leaf.\n                  min_child_samples=20,                                         # Min data needed in a leaf\n                  subsample        =1.0,\n                  subsample_freq   =0,                                          # Frequency of using 'subsample' param\n                  colsample_bytree =1.0,                                        # subsample ratio of columns when constructing each tree\n                  reg_alpha        =0.0,                                        # L1 regularization\n                  reg_lambda       =0.0,                                        # L2 regularization\n                  random_state     =2019,\n                  n_jobs           =-1,                                         # Number of parallel threads.\n                  silent           =True,\n                  importance_type  ='split',                                    # Type of feature importances to be filled into 'feature_importancs_'. 'split' or 'gain' -> gains of split which use th feature.\n                  **kwargs                                                      # Other params for model supported by LightGBM\n)","ce94cd6c":"from sklearn.model_selection import train_test_split as tts\n\nXtrain, Xvalid, ytrain, yvalid = tts(train_df.drop('target', axis=1), train_df['target'])","7b3dd1ff":"feature_names = train_df.drop('target', axis=1).columns.values.tolist()\n\nreg_model = reg_model.fit(\n    Xtrain, ytrain,\n    sample_weight             =None,                                            # Array of weights for all data points\n    init_score                =None,                                            # Array of size n_samples for initial score of training data\n    eval_set                  =[(Xtrain, ytrain), (Xvalid, yvalid)],            # set of pairs as [(x1, y1), (x2, y2) ... ] to evaluate during training\n    eval_names                =[\"Train\", \"Validation\"],\n    eval_sample_weight        =[None, None],                                    # list of arrays as weights for eval data\n    eval_init_score           =None,                                            # list of arrays, initial score for eval data\n    eval_metric               ='rmse',                                          # string, list of strings, callable or None. eg: 'l2', 'logloss', 'ndcg'\n    early_stopping_rounds     =None,\n    verbose                   =50,\n    feature_name              =feature_names,                                   # list of strings. Feature names.\n    categorical_feature       ='auto',                                          # list of int, list of strings OR 'auto' -> pandas categorical is used to check. ## All values should be less than int32 ## ### Negative values treated as missing. ###\n    callbacks                 =None                                             # list of callbacks or None.\n)","467acd77":"reg_model.best_iteration_, reg_model.best_score_","b65651cf":"reg_model.feature_importances_","edbbedcc":"# Getting output of certain tree leaf\n\ntree_index = 0\nleaf_index = 0\n\nreg_model.booster_.get_leaf_output(tree_index, leaf_index)","03f5b826":"from sklearn.model_selection import train_test_split as tts\n\nXtrain, Xvalid, ytrain, yvalid = tts(train_df.drop('target', axis=1), train_df['target'])","15afd0c0":"train_dataset = lgb.Dataset(Xtrain, label=ytrain,\n                            reference=None,                    # If this Dataset is for validation, training Dataset should be used as reference.\n                            weight=None,                       # list or None. Weight for each instance.\n                            group=None,                        # list or None. Group\/query size for Dataset.\n                            init_score=None,                   # list or None. Initial score for dataset.\n                            silent=False,\n                            feature_name='auto',\n                            params=None,                       # dict or None. Other params for dataset\n                            free_raw_data=True                 # If True raw data is freed after constructing Dataset.\n                           )\n\nvalid_dataset = lgb.Dataset(Xvalid, label=yvalid,\n                            reference=train_dataset,           # If this Dataset is for validation, training Dataset should be used as reference.\n                            weight=None,                       # list or None. Weight for each instance.\n                            group=None,                        # list or None. Group\/query size for Dataset.\n                            init_score=None,                   # list or None. Initial score for dataset.\n                            silent=False,\n                            feature_name='auto',\n                            params=None,                       # dict or None. Other params for dataset\n                            free_raw_data=True                 # If True raw data is freed after constructing Dataset.\n                           )","819341d2":"#\n# Top problem of Boosting is that it can overfit very easily. So most parameters you will see here are to reduce overfitting.\n# For better generality and reducing overfitting at the same time you will have to optimize select params (bec. of time boundation).\n# One more problem which `still` 'can' occur is target leakage, which CatBoost tries to reduce.\n#\n\nparams = {\n    ####### Core Params #######\n    'objective':'regression',                          # 'regression', 'regression_l1', 'regression_l2', 'huber', 'fair', 'poisson', 'quantile', 'mape', 'gamma', 'tweedie', 'binary', 'multiclass', 'multiclassova', 'xentropy', 'xentlambda', 'lambdarank'\n    'boosting':'gbdt',                                 # 'gbdt', 'gbrt', 'rf', 'dart', 'goss'\n    #'num_iterations':100,                             # Number of Boosting iterations. [Internally LightGBM constructs num_class*num_iterations trees for `multiclass` classifications]\n    'learning_rate':0.1,                               # Shrinkage rate. ## In 'dart' it also affects normalizing weights of dropped trees. ##\n    'num_leaves':31,                                   # Max number of leaves in one tree\n    'tree_learner':'serial',                           # 'serial', 'feature' -> feature parallel tree learner, 'data' -> data paralle tree learner, 'voting' -> voting parallel tree learner\n    'num_threads':0,                                   # Best use as number of CPU cores. For parallel learning do not use all CPU cores. This might cause poor performance for network communications.\n    'device_type':'cpu',                               # 'cpu', 'gpu' ## Use smaller 'max_bin' for better speedup ## ## For better accuray with GPU use 'gpu_use_dp'=true at speed cost. ##\n    'seed':2019,\n    \n    ####### Learning Control Parameters #######\n    'max_depth':-1,                                    # For limiting max depth of tree model\n    'min_data_in_leaf':20,                             # Min data in each leaf\n    'min_sum_hessian_in_leaf':1e-3,                    # Min sum of hessians in each leaf\n    'bagging_fraction':1.0,                            # like 'feature_fraction' but this will randomly select part of data without resampling. ## To enable Bagging 'bagging_freq' should be set to non sero value as well. ##\n    'bagging_freq':0,                                  # int. Frequency of bagging. n -> enable bagging at every n iteration. ## To enable bagging set 'bagging_fraction' < 1.0 as well. ##\n    'bagging_seed':2019,\n    'feature_fraction':1.0,                            # Subsample of features to select for each iteration\n    'feature_fraction_seed':2,\n    #'early_stopping_round':0,\n    'max_delta_step':0.0,                              # Limit max output of tree leaves. Final max output of leaves = learning_rate*max_delta_step\n    'reg_alpha':0.0,                                   # L1 Regularization\n    'reg_lambda':0.0,                                  # L2 Regularization\n    'min_gain_to_split':0.0,                           # Min gain needed to perform split\n    \n    #'drop_rate':0.1,                                  # Used with 'dart' boosting. Dropout rate\n    #'max_drop':50,                                    # Used with 'dart' boosting. Max number of dropped trees\n    #'skip_drop':0.5,                                  # Used with 'dart' boosting. Probab of skipping Dropout procedure at each iteration.\n    #'xgboost_dart_mode':False,                        # Used with 'dart' boosting. To enable XGBoost's dart mode.\n    #'uniform_drop':False,                             # Used with 'dart' boosting. If you want to use Uniform Drop.\n    #'drop_seed':4,                                    # Used with 'dart' boosting.\n    \n    #top_rate:0.2,                                     # Used with 'goss' boosting. Retain ratio of large gradient data.\n    #other_rate:0.1,                                   # Used with 'goss' boosting. Retain ratio of small gradient data.\n    \n    'min_data_per_group':100,                          # Min data per categorical group\n    'max_cat_threshold':32,                            # Limit max threshold points in categorical features.\n    'cat_l2':10.0,                                     # L2 Regularization in categorical split\n    'cat_smooth':10.0,                                 # For reducing noises in categorical features, especially for 'categories' with few data points\n    'max_cat_to_onehot':4,                             # If categories less or wqual to this, One-vs-Other split algorithm will be used.\n    \n    #'top_k':20,                                       # Used in Voting Parallel. Larger value for accurate result at cost of speed.\n    \n    'monotone_constraints':None,                       # For constraining monotonic featurs. 0 -> no constraint, 1 -> monotonically inc, -1 -> monotonically dec; given for all features as \"-1,1,0,0...\"\n    'feaature_contri':None,                            # For controlling features' split gain. gain[i] = max(0, feature_contri[i])*gain[i]. specify all features in order as \"0.3, 0.1,...\"\n    \n    ####### Objective Parameters #######\n    'num_class':1,                                     # Used only in multiclass classification\n    'is_unbalance':False,                              # Used only in binary. ## Cannot be used with scale_pos_weight. Use only one of them. ##\n    #'scale_pos_weight':1.0,                           # Used only in binary. Weight of labels with +ve class. ## Cannot be used with is_unbalance. Use only one of them. ##\n    'sigmoid':1.0,                                     # >0.0; Used only in binary and multiclassova classifications, and lambdarank applications. Param for sigmoid function.\n    'reg_sqrt':False,                                  # Used only in Regression. Fit sqrt(label) instead of original and preds will automatically to squared. # Might be useful in large range labels. #\n    'alpha':0.9,                                       # >0.0; Used only in Huber, Quantile regression. Param for Huber Loss and Quantile Regression.\n    'fair_c':1.0,                                      # >0.0; Used only in Fair regression. Param for Fair Loss\n    'poisson_max_delta_step':0.7,                      # >0.0; Used only in Poisson regression. Param for poisson regression to safeguard optimization.\n    'tweedie_variance_power':1.5,                      # 1.0 <= x < 2.0; Used in Tweedie Regression. Used to control variance of tweedie distribution. Closer to 2 -> Gamma, closer to 1 -> Poisson.\n    'max_position':20,                                 # >0; Used only in lambdarank. Optimizes NDCG at this position\n    #'label_gain':                                     # Used only for lambdarank. Relevant gain for labels. eg: gain for label 2 will be 3; as \"1,2,3,7,15,31,63,...2^30-1\"\n    \n    ####### Metric Params #######\n    'metric':\"rmse\",                                   # 'None','l1', 'l2', 'l2_root', 'quantile', 'mpae', 'huber', 'fair', 'poisson', 'fair', 'poisson', 'gamma', 'gamma_deviance', 'tweedie', 'ndcg', 'map', 'auc', 'binary_logloss', 'binary_error', 'multi_logloss', 'multi_error', 'xentropy', 'xentlambda', 'kldiv'\n    'metric_freq':1,                                   # Freq of metric output\n    'eval_at':\"1,2,3,4,5\",                             # Used only with 'ndcg' and 'map'. NDCG and MAP evaluation positions separated by `,`.\n    \n    ####### IO Params #######\n    'verbosity': 1,\n    'max_bin': 255,                                    # Max number of bins that feature values will be bucketed in. ## LightGBM compresses memory as per this param. eg uint8_t for 255 ##\n    'min_data_in_bin':3,                               # Min data inside one bin\n    'bin_construct_sample_cnt':200000,                 # Number of data points sampled to construct histogram bins\n    'histogram_pool_size':-1.0,                        # Max cache size in MB for 'historical' histogram. <0 -> No Limit\n    'data_random_seed':1,                              # Random seed for data partitioning in parallel learning (excluding feature_parallel mode)\n    #'initscore_filename':\"\",                          # Path of file with training initial scores. ## Only works in case of loading data directly fom file ##\n    #'valid_data_initscores':\"\",                       # Path(s) of file(s) with validation initial scores. as f\"{path1},{path2}...\" ## Only works in case of loading data directly fom file ##\n    'pre_partition':False,                             # Used for Parallel Learning (excluding feature_parallel mode). True if training data is pre partitioned and diff machines use diff partitions\n    \n    'enable_bundle':True,                              # Set to False to disable Exclusive Feature Bundling (EFB).\n    'max_conflict_rate':0.0,                           # max conflict rate for bundles in EFB.\n    \n    'is_enable_sparse':True,                           # To enable\/disable sparse optimization\n    'sparse_threshold':0.8,                            # Threshold for zero elements percentage for treating a feature as a sparse one\n    'use_missing':True,                                # False to diable special handling of missing values\n    'zero_as_missing':False,                           # True for treating zero's as missing value. False -> NA as missing\n    \n    'two_round':False,                                 # If data is too big to fit in memory.  ## Works only when loading data directly from file ##\n    'header':False,                                    # True if input data is header. ## Works only when loading data directly from file ##\n    \n    'predict_raw_score':False,                         # False -> predict transformed scores.  # Used only in prediction task #\n    'predict_leaf_index':False,                        # True -> predct with leaf index of all trees. # Used only in prediction task #\n    'predict_contrib':False,                           # True -> predict SHAP values. # Used only in prediction task #\n    'num_iteration_predict':-1,                        # Number of trained iterations to be used in prediction. # Used only in prediction task #\n    'pred_early_stop':False,                           # True -> Use early stopping to speed up prediction. May affect accuracy. # Used only in prediction task #\n    'pred_early_stop_freq':10,                         # Freq of checking early stopping prediction. # Used only in prediction task #\n    'pred_early_stop_margin':10.0,                     # Threshold of margin in early stopping prediction. # Used only in prediction task #\n    \n    ####### Network Params #######\n    'num_machines':1,                                  # Number of machines for parallel learning. # Needed to be set for both socket and mpi versions #\n    'local_listen_port':12400,                         # TCP listen port for local machines.\n    'time_out':120,                                    # socket time-out in minutes.\n    \n    ####### GPU Params #######\n    'gpu_platform_id':-1,                              # OpenCL platform ID. Usually each GPU vendor exposes one OpenCL platform. -1 -> system-wide default plaform.\n    'gpu_device_id':-1,                                # OpenCL device ID in specified platform. Each GPU in selected platform has a unique device ID.\n    'gpu_use_dp':False,                                # True -> To use double precision math on GPU.\n}","f00f370b":"result = {}","669d9233":"reg_model = lgb.train(\n                      params,\n                      train_dataset,\n                      num_boost_round=100,                                      #\n                      valid_sets=[train_dataset, valid_dataset],                #\n                      valid_names=['Train', 'Valid'],                           #\n                      fobj=None,                                                #\n                      feval=None,                                               #\n                      init_model=None,                                          #\n                      feature_name='auto',                                      #\n                      categorical_feature='auto',                               #\n                      early_stopping_rounds=50,                                 #\n                      evals_result=result,                                      #\n                      verbose_eval=50,                                          #\n                      learning_rates=None,                                      #\n                      keep_training_booster=False,                              #\n                      callbacks=None                                            #\n)","7cfb5860":"reg_model.best_iteration, reg_model.best_score","87749ed0":"reg_model.feature_importance()","44b935ea":"result['Train']['rmse'][-10:]","8786784a":"reg_model = cb.CatBoostRegressor(\n                iterations=1000,                                        # Max number of trees. (default: 1000)\n                learning_rate=0.01,\n                depth=6,                                                # Depth of tree. Can be upto 16. (def: 6)\n                l2_leaf_reg=3.0,                                        # L2 Regularization. (def: 3.0)\n                model_size_reg=None,                                    # CPU only; [0, inf). Regularize model size. (def: None [=0.5])\n                rsm=0.95,                                               # CPU only;(0, 1]. Random subspace method. Percentage of features to use at each split selection. (def: None[=1])\n                loss_function='RMSE',                                   # CPU and GPU; string or object. 'RMSE', 'Logloss', 'MAE', 'CrossEntropy', 'Quantile', 'LogLinQuantile', 'Lq', 'MultiClass', 'MultiClassOneVsAll', 'MAPE', 'Poisson', 'PairLogit', 'PairLogitPairwise', 'QueryRMSE', 'QuerySoftMax', 'YetiRank', 'YetiRankPairwise'. (Custom: 'Quantile:alpha=0.1')\n                border_count=None,                                      # CPU and GPU; [1, 255]. Number of splits for numerical features. (def: 254 for CPU, 128 for GPU)\n                feature_border_type=None,                               # CPU and GPU; string. Binarization mode for numerical feature. 'Meadin', 'Uniform', 'UniformAndQuantile', 'MaxLogSum', 'MinEntropy', 'GreedyLogSum'. (def: 'GreedyLogSum')\n                fold_permutation_block_size=None,                       # CPU and GPU; int. Objects in dataset that are grouped in blocks before random permutations. Smaller -> slower training; Large -> quality degradation. (def: 1)\n                od_pval=None,                                           # CPU and GPU; float; [10^-10, 1-^-2] for best results. Threshold for IncToDec overfitting detector type. Larger -> Overfitting is detected earlier. (def: 0[=off]) # Validation Data should be given #  ## Do not use with Iter overfitting detectot type. ##\n                od_wait=None,                                           # CPU and GPU; int. Number of iterations after optimal result before stopping. Depends on Overfitting detector type: IncToDec -> Ignore overfitting detector when threshold is reached and countinue learning for specified iterations OR Iter -> like EarlyStoppingRound. (def: 20)\n                od_type=None,                                           # CPU and GPU; string. 'IncToDec' or 'Iter'. (def: 'IncToDec')\n                nan_mode=None,                                          # CPU and GPU; string. 'Forbidden' -> Missing value will give error, 'Min' -> taken as less than all other values. (a differentiating split can be made) OR 'MAX' -> larger than all other values (differentiating split can be made). (def: 'Min') ## This can be set for individual features with custom quantization and missing value modes input file. ##\n                counter_calc_method=None,                               # CPU and GPU; string. For calculating the Counter CTR Type. 'SkipTest' -> Objs from validation not considered OR 'Full' -> All objs from both learn and valid are considered. (def: None[='Full'])\n                leaf_estimation_iterations=None,                        # CPU and GPU; int. Number of gradient steps when calculating the values in leaves. (def: None[Depends on training objective])\n                leaf_estimation_method=None,                            # CPU and GPU; string. Method for calculating values in leaves. 'Newton' or 'Gradient'. (def: 'Gradient')\n                thread_count=None,                                      # CPU and GPU; int. CPU -> Optimizes speed, GPU -> given value for reading data from hard drive and does not affect training; During training one main thread and one thread for each GPU are used. (def: -1[= number of cores])\n                random_seed=None,\n                use_best_model=None,                                    # CPU and GPU; bool. True -> Use validation set to identify best iterations. (def: True[if validation is provided]). # Requires Validation data. #\n                best_model_min_trees=None,                              # CPU and GPU; int. Min number of trees that the best model should have. (def: None)  ## Should be used with 'use_best_model' param. ##\n                verbose=None,                                           # CPU and GPU; string. 'Silent', 'Verbose', 'Info', 'Debug'\n                silent=None,\n                logging_level=None,                                     # CPU and GPU; string. 'Silent', 'Verbose', 'Info', 'Debug'.  (def: None)\n                metric_period=None,                                     # CPU and GPU; int; >0. Freq of iterations to calc the values of obj and metrics. (def: 1)\n                ctr_leaf_count_limit=None,                              # CPU only; int. Max leaves with categorical features. Only leaves with top freq of values are selected. Reduces model size and memory at cost of quality. (def: None[=not limited])\n                store_all_simple_ctr=None,                              # CPU only; bool. Ignore cat features which are not used in feature combinations, when choosing candidates for exclusion. (def: None[= False]) ## Should be used with 'ctr_leaf_count_limit' ##\n                max_ctr_complexity=None,                                # CPU and GPU; int. Max number of categorical featurse that can be combined. (def: 4)\n                has_time=None,                                          # CPU and GPU; bool. True -> do not random permute during transforming cat features and choosing tree structure. (def: False)\n                allow_const_label=None,                                 # CPU and GPU; bool. Use with datasets having equal label values for all objects. (def: False)\n                one_hot_max_size=None,                                  # CPU and GPU; int. Max categoried cat feature to one hot encode. (def: 2)\n                random_strength=None,                                   # CPU and GPU; float. Amount of randomness for scoring splits when tree structure is selected. (def: 1) # Can avoid Overfitting. # ## Not supported for 'QueryCrossEntropy', 'YetiRankPairwise', 'PairLogitPairwise' loss functions. ##\n                name=None,                                              # CPU and GPU; string. Experiment name to display in visualization tools. (def: 'experiment')\n                ignored_features=None,                                  # CPU and GPU; list. Indices of features to exclude from training. If training should exclude 1,2,7,42,43,44,45 then \"1:2:7:42-45\". (def: None)\n                train_dir=None,                                         # CPU and GPU; string. Dir for storing the files generated during training. (def: 'catboost_info')\n                custom_metric=None,                                     # CPU only; string or list of strings. Metric values to output during training. Custom: 'Quantile:[alpha:0.1;...]'. 'RMSE', 'Logloss', 'MAE', 'CrossEntropy', 'Quantile', 'LogLinQuantile', 'Lq', 'MultiClass', 'MultiClassOneVsAll', 'MAPE', 'Poisson', 'PairLogit', 'PairLogitPairwise', 'QueryRMSE', 'QuerySoftMax', 'SMAPE', 'Recall', 'Precision', 'F1', 'TotalF1', 'Accuracy', 'BalancedAccuracy', 'BalancedErrorRate', 'Kappa', 'WKappa', 'LogLikelihoodOfPrediction', 'AUC', 'R2', 'NumErrors', 'MCC', 'BrierScore', 'HingeLoss', 'HammingLoss', 'ZeroOneLoss', 'MSLE', 'MedianAbsoluteError', 'PairAccuracy', 'AverageGain', 'PFound', 'NDCG', 'PrecisionAt', 'RecallAt', 'MAP', 'CtrFactor'.\n                eval_metric=None,                                       # CPU only; string or object.  'RMSE', 'Logloss', 'MAE', 'CrossEntropy', 'Quantile', 'LogLinQuantile', 'Lq', 'MultiClass', 'MultiClassOneVsAll', 'MAPE', 'Poisson', 'PairLogit', 'PairLogitPairwise', 'QueryRMSE', 'QuerySoftMax, 'SMAPE', 'Recall', 'Precision', 'F1', 'TotalF1', 'Accuracy', 'BalancedAccuracy', 'BalancedErrorRate', 'Kappa', 'WKappa', 'LogLikelihoodOfPrediction', 'AUC', 'R2', 'NumErrors', 'MCC', 'BrierScore', 'HingeLoss', 'HammingLoss', 'ZeroOneLoss', 'MSLE', 'MedianAbsoluteError', 'PairAccuracy', 'AverageGain', 'PFound', 'NDCG', 'PrecisionAt', 'RecallAt', 'MAP'.\n                bagging_temperature=None,                               # CPU and GPU; float. Param for Bayesian Bootstrap to assign random weights to objects. 0 -> All weight's are '1' OR 1 -> weight's sampled from exponential distribution. (def: 1)\n                save_snapshot=None,                                     # CPU and GPU; bool. To enable snapshotting for restoring the training progress after an interruption. (def: None)\n                snapshot_file=None,                                     # CPU and GPU; string. Name of file to save training progress in. (def: 'experiment.cbsnapshot')\n                snapshot_interval=None,                                 # CPU and GPU; int. Interval between saving snapshots in seconds. Last snapshot at end of training. (def: 600)\n                fold_len_multiplier=None,                               # CPU and GPU; float; > 1.0. Coeff. for changing lengths of folds. (def: 2) # Best valid. results with min values. #\n                used_ram_limit=None,                                    # CPU only; int. `Attempt` to limit amount of CPU memory used. # Often affects CTR calc memory usage. MB, KB, GB as '32gb'. # ## In some cases it is impossible to limit RAM usage. ##\n                gpu_ram_part=None,                                      # GPU only; float. How much of GPU RAM to use for training. (def: 0.95)\n                pinned_memory_size=None,                                # GPU only; int. How much pinned(page-locked) CPU RAM to use per GPU. (def: 1073741824)\n                allow_writing_files=None,                               # CPU only; bool. Allow to write 'analytical' snapshot files during training. (def: True) # Fasle -> snapshot and data viz tools will be unavailable. #\n                final_ctr_computation_mode=None,                        # CPU and GPU; string. Final CTR computation mode. 'Default' -> Compute final CTRs for learn and validation mode OR 'Skip' -> Do not compute final CTRs for learn and validation datasets. In this case the resulting model cannot be applied. Dec.s the size of resulting model. ## Can be useful for research purposes when only metric values have to be calculated. ##\n                approx_on_full_history=None,                            # CPU only; bool. Principlees of calculating approximated values. 'False' -> Use only a fraction of fold to calc approx values; size of fraction = 1\/coeff OR 'True' -> Use all preceding rows in the fold for calc approx values. (def: False)\n                boosting_type=None,                                     # CPU and GPU[only Plain mode for MultiClass loss on GPU]; string. Boosting scheme. 'Ordered' -> Usually provides better quality on small datasets OR 'Plain' -> The classic gradient boosting scheme. (def: depends on number of data points and learning mode)\n                simple_ctr=None,                                        # CPU and GPU; string. Binarization settings for simple categorical features. As 'CtrType[:TargetBorderCount=BorderCount][:TargetBorderType=BorderType][:CtrBorderCount=Count][:CtrBorderType=Type][:Prior=num_1\/denum_1]...'. CtrType: 'Borders', 'Buckets', 'BinarizedTargetMeanValue', 'Counter' (GPU: 'Borders', 'Buckets', 'FeatureFreq', 'FloatTargetMeanValue'); TargetBorderCount: [1, 255]; TargetBorderType: 'Mean', 'Uniform', 'UniformAndQuantiles', 'MaxLogSum', 'MinEntropy', 'GreedyLogSum'; CtrBorderCount: [1, 255]; CtrBorderType: 'Median', 'Uniform', 'UnifromAndQuantiles', 'MaxLogSum', 'GreedyLogSum'; Prior: number (adds value to numerator) (GPU: two slash delimited numbers -> first to num., second to den.)\n                combinations_ctr=None,                                  # CPU and GPU; string. Binarization settings for simple categorical features. As 'CtrType[:TargetBorderCount=BorderCount][:TargetBorderType=BorderType][:CtrBorderCount=Count][:CtrBorderType=Type][:Prior=num_1\/denum_1]...'. CtrType: 'Borders', 'Buckets', 'BinarizedTargetMeanValue', 'Counter' (GPU: 'Borders', 'Buckets', 'FeatureFreq', 'FloatTargetMeanValue'); TargetBorderCount: [1, 255]; TargetBorderType: 'Mean', 'Uniform', 'UniformAndQuantiles', 'MaxLogSum', 'MinEntropy', 'GreedyLogSum'; CtrBorderCount: [1, 255]; CtrBorderType: 'Median', 'Uniform', 'UnifromAndQuantiles', 'MaxLogSum', 'GreedyLogSum'; Prior: number (adds value to numerator) (GPU: two slash delimited numbers -> first to num., second to den.)\n                per_feature_ctr=None,                                   # CPU and GPU; string. Binarization settings for simple categorical features. As 'CtrType[:TargetBorderCount=BorderCount][:TargetBorderType=BorderType][:CtrBorderCount=Count][:CtrBorderType=Type][:Prior=num_1\/denum_1]...'. CtrType: 'Borders', 'Buckets', 'BinarizedTargetMeanValue', 'Counter' (GPU: 'Borders', 'Buckets', 'FeatureFreq', 'FloatTargetMeanValue'); TargetBorderCount: [1, 255]; TargetBorderType: 'Mean', 'Uniform', 'UniformAndQuantiles', 'MaxLogSum', 'MinEntropy', 'GreedyLogSum'; CtrBorderCount: [1, 255]; CtrBorderType: 'Median', 'Uniform', 'UnifromAndQuantiles', 'MaxLogSum', 'GreedyLogSum'; Prior: number (adds value to numerator) (GPU: two slash delimited numbers -> first to num., second to den.)\n                #ctr_target_border_count=None,                           # CPU and GPU; int; [1, 255]. Max number of borders to use in target binarization for categorical features that need it. (def: num_class-1(CPU), 1 otherwise) # Overrides one specified in 'simple_ctr', 'combinations_ctr', 'per_feature_ctr' #\n                task_type=None,                                         # CPU and GPU; Processing Unit (PU) to use for training. 'CPU' or 'GPU'. (def: 'CPU')\n                device_config=None,                                     # \n                devices=None,                                           # GPU only; string. IDs of the GPU devices to use for training. As \"0:2:4-7\" (def: None[=all])\n                bootstrap_type=None,                                    # CPU and GPU; string. Bootstrap type. 'Poisson'(GPU only), 'Bayesian', 'Bernoulli', 'No'. (def: 'Bayesian')\n                subsample=None,                                         # CPU and GPU; float. Sample rate for Bagging types 'Poisson', 'Bernoulli'. (def: 0.66)\n                dev_score_calc_obj_block_size=None,                     #\n                gpu_cat_features_storage=None,                          # GPU only; string. Method for storing cat features values. 'CpuPinnedMemory', 'GpuRam'. (def: None[='GPuRam']) # Use 'CpuPinnedMemory' if feature combinations are used and GPU mem. is not sufficient. #\n                data_partition=None,                                    # GPU only; string. Method for splitting input dataset btw multiple workers. 'FeatureParallel' -> split by features and calc their values on diff GPUs OR 'DocParallel' -> split by objects and calc each of these on certain GPU. (def: depends on learning mode and input datasets)\n                metadata=None,                                          # \n                early_stopping_rounds=None,                             # CPU and GPU; int. Set overfitting detector type to 'Iter' and stop training after these many iterations after seeing best so far. (def: False)\n                cat_features=None,                                      # \n                #growing_policy=None,                                    #\n                #min_samples_in_leaf=None,                               #\n                #max_leaves_count=None                                   #\n)","b9a20606":"from sklearn.model_selection import train_test_split as tts\n\nXtrain, Xvalid, ytrain, yvalid = tts(train_df.drop('target', axis=1), train_df['target'])","6243f223":"reg_model = reg_model.fit(\n             Xtrain, y=ytrain,                                          # catboost.Pool or list or array or DataFrame or Series or string(file)\n             cat_features=None,                                         # list, array, None. ## Use only if X is not catboost.Pool ##\n             sample_weight=None,                                        # list, array, DataFrame, Series, None. Inctance weights.\n             baseline=None,                                             # list, array, None. 2D array like data. ## Use only if X is not catboost.Pool ##\n             use_best_model=True,                                       # bool, None. Weather to use best model.\n             eval_set=[(Xtrain, ytrain), (Xvalid, yvalid)],              # catboost.Pool, list, None. List of pairs to evaluate as [(X1, y1), (X2, y2) ...]\n             verbose=100,                                               # bool, int. \n             logging_level=None,                                        # 'Silent', 'Verbose', 'Info', 'Debug'.\n             plot=True,                                                 # True to draw train and eval error in Jupyter Notebook.\n             column_description=None,                                   # \n             metric_period=None,                                        #\n             silent=None,                                               #\n             early_stopping_rounds=50,                                  # Activates 'Iter' overfitting detector with od_wait set ti early_stopping_rounds.\n             save_snapshot=None,                                        # bool, None. Enable progress snapshot.\n             snapshot_file=None,                                        # Learn progress snapshot file path, if None -> will use default filename.\n             snapshot_interval=None                                     # int. Interval btw saving snapshots (seconds).\n             )","29e648ee":"reg_model.best_iteration_, reg_model.best_score_","ff58cfb0":"reg_model.feature_importances_","70831fd3":"reg_model.evals_result_['validation_1']['RMSE'][-10:]","b4a9837c":"from sklearn.model_selection import train_test_split as tts\n\nXtrain, Xvalid, ytrain, yvalid = tts(train_df.drop('target', axis=1), train_df['target'])","a07e9aab":"from catboost import FeaturesData as FD         # Class to store features data in optimized form to pass to Pool constructor\nfrom catboost import Pool as P","fefb5f18":"train_fd = FD(\n              num_feature_data=Xtrain.drop(['CHAS', 'RAD'], axis=1).values.astype(np.float32),            # np.ndarray; dtype=np.float32\n              cat_feature_data=Xtrain.loc[:, ['CHAS', 'RAD']].astype(str).values,                         # np.ndarray; dtype=object. ## Elements must have bytese type, containing utf-8 encoded strings. ##\n              num_feature_names=Xtrain.drop(['CHAS', 'RAD'], axis=1).columns.values.tolist(),\n              cat_feature_names=['CHAS', 'RAD']\n)\n\nvalid_fd = FD(\n              num_feature_data=Xvalid.drop(['CHAS', 'RAD'], axis=1).values.astype(np.float32),            # np.ndarray; dtype=np.float32\n              cat_feature_data=Xvalid.loc[:, ['CHAS', 'RAD']].astype(str).values,                         # np.ndarray; dtype=object. ## Elements must have bytese type, containing utf-8 encoded strings. ##\n              num_feature_names=Xvalid.drop(['CHAS', 'RAD'], axis=1).columns.values.tolist(),\n              cat_feature_names=['CHAS', 'RAD']\n)","b39447fe":"#train_data = Pool(datset_desc_file_path, column_description=cd_file)\ntrain_data = P(\n                data=train_fd,\n                label=ytrain,\n                cat_features=None,\n)\n\nvalid_data = P(\n                data=valid_fd,\n                label=yvalid,\n                cat_features=None,\n)","ceea60a1":"params = {\n          ####### Common Params #######\n          'loss_function':'RMSE',                                   # CPU and GPU; string or object. 'RMSE', 'Logloss', 'MAE', 'CrossEntropy', 'Quantile', 'LogLinQuantile', 'Lq', 'MultiClass', 'MultiClassOneVsAll', 'MAPE', 'Poisson', 'PairLogit', 'PairLogitPairwise', 'QueryRMSE', 'QuerySoftMax', 'YetiRank', 'YetiRankPairwise'. (Custom: 'Quantile:alpha=0.1')\n          'custom_metric':None,                                     # CPU only; string or list of strings. Metric values to output during training. Custom: 'Quantile:[alpha:0.1;...]'. 'RMSE', 'Logloss', 'MAE', 'CrossEntropy', 'Quantile', 'LogLinQuantile', 'Lq', 'MultiClass', 'MultiClassOneVsAll', 'MAPE', 'Poisson', 'PairLogit', 'PairLogitPairwise', 'QueryRMSE', 'QuerySoftMax', 'SMAPE', 'Recall', 'Precision', 'F1', 'TotalF1', 'Accuracy', 'BalancedAccuracy', 'BalancedErrorRate', 'Kappa', 'WKappa', 'LogLikelihoodOfPrediction', 'AUC', 'R2', 'NumErrors', 'MCC', 'BrierScore', 'HingeLoss', 'HammingLoss', 'ZeroOneLoss', 'MSLE', 'MedianAbsoluteError', 'PairAccuracy', 'AverageGain', 'PFound', 'NDCG', 'PrecisionAt', 'RecallAt', 'MAP', 'CtrFactor'.\n          'eval_metric':'RMSE',                                     # CPU only; string or object.  'RMSE', 'Logloss', 'MAE', 'CrossEntropy', 'Quantile', 'LogLinQuantile', 'Lq', 'MultiClass', 'MultiClassOneVsAll', 'MAPE', 'Poisson', 'PairLogit', 'PairLogitPairwise', 'QueryRMSE', 'QuerySoftMax, 'SMAPE', 'Recall', 'Precision', 'F1', 'TotalF1', 'Accuracy', 'BalancedAccuracy', 'BalancedErrorRate', 'Kappa', 'WKappa', 'LogLikelihoodOfPrediction', 'AUC', 'R2', 'NumErrors', 'MCC', 'BrierScore', 'HingeLoss', 'HammingLoss', 'ZeroOneLoss', 'MSLE', 'MedianAbsoluteError', 'PairAccuracy', 'AverageGain', 'PFound', 'NDCG', 'PrecisionAt', 'RecallAt', 'MAP'.\n          'iterations':1000,                                        # CPU and GPU; int. Max number of trees. (default: 1000)\n          'learning_rate':0.01,\n          'random_seed':2019,\n          'l2_leaf_reg':3.0,                                        # CPU and GPU; int. L2 Regularization. (def: 3.0)\n          'bootstrap_type':'Bayesian',                              # CPU and GPU; string. Bootstrap type. 'Poisson'(GPU only), 'Bayesian', 'Bernoulli', 'No'. (def: 'Bayesian')\n          'bagging_temperature':1,                                  # CPU and GPU; float. Param for Bayesian Bootstrap to assign random weights to objects. 0 -> All weight's are '1' OR 1 -> weight's sampled from exponential distribution. (def: 1)\n          #'subsample':0.66,                                        # CPU and GPU; float. Sample rate for Bagging types 'Poisson', 'Bernoulli'. (def: 0.66)\n          #'sampling_frequency':'PerTreeLevel',                     # CPU and GPU; string. Frequency to sample weights and objects when building trees. 'PerTree', 'PerTreeLevel'.\n          'random_strength':1,                                      # CPU and GPU; float. Amount of randomness for scoring splits when tree structure is selected. (def: 1) # Can avoid Overfitting. # ## Not supported for 'QueryCrossEntropy', 'YetiRankPairwise', 'PairLogitPairwise' loss functions. ##\n          'use_best_model':True,                                    # CPU and GPU; bool. True -> Use validation set to identify best iterations. (def: True[if validation is provided]). # Requires Validation data. #\n          #'best_model_min_trees':None,                             # CPU and GPU; int. Min number of trees that the best model should have. (def: None)  ## Should be used with 'use_best_model' param. ##\n          'depth':6,                                                # CPU and GPU; int. Depth of tree. Can be upto 16. (def: 6)\n          #'ignored_features':None,                                 # CPU and GPU; list. Indices of features to exclude from training. If training should exclude 1,2,7,42,43,44,45 then \"1:2:7:42-45\". (def: None)\n          'one_hot_max_size':3,                                     # CPU and GPU; int. Max categoried cat feature to one hot encode. (def: 2)\n          'has_time':False,                                         # CPU and GPU; bool. True -> do not random permute during transforming cat features and choosing tree structure. (def: False)\n          'rsm':0.95,                                               # CPU only;(0, 1]. Random subspace method. Percentage of features to use at each split selection. (def: None[=1])\n          'nan_mode':'Min',                                         # CPU and GPU; string. 'Forbidden' -> Missing value will give error, 'Min' -> taken as less than all other values. (a differentiating split can be made) OR 'MAX' -> larger than all other values (differentiating split can be made). (def: 'Min') ## This can be set for individual features with custom quantization and missing value modes input file. ##\n          #'fold_permutation_block_size':1,                         # CPU and GPU; int. Objects in dataset that are grouped in blocks before random permutations. Smaller -> slower training; Large -> quality degradation. (def: 1)\n          'leaf_estimation_method':'Gradient',                      # CPU and GPU; string. Method for calculating values in leaves. 'Newton' or 'Gradient'. (def: 'Gradient')\n          'leaf_estimation_iterations':10,                          # CPU and GPU; int. Number of gradient steps when calculating the values in leaves. (def: None[Depends on training objective])\n          'leaf_estimation_backtracking':'AnyImprovment',          # Depends,string. 'No' -> do not use backtracking. 'AnyImprovement' -> reduce descent step to point where loss func value is smaller than previous step. 'Armijo' [CPU only] Reduce descent step until 'Armijo' condition is met.\n          'fold_len_multiplier':2,                                  # CPU and GPU; float; > 1.0. Coeff. for changing lengths of folds. (def: 2) # Best valid. results with min values. #\n          'approx_on_full_history':False,                           # CPU only; bool. Principlees of calculating approximated values. 'False' -> Use only a fraction of fold to calc approx values; size of fraction = 1\/coeff OR 'True' -> Use all preceding rows in the fold for calc approx values. (def: False)\n          #'class_weights':None,                                    # CPU and GPU. Used as multipliers for object weights. alias: 'scale_pos_weight' # For Classification probs #  ## You can set it for imbalanced dataset. ##\n          'boosting_type':'Ordered',                                # CPU and GPU[only Plain mode for MultiClass loss on GPU]; string. Boosting scheme. 'Ordered' -> Usually provides better quality on small datasets OR 'Plain' -> The classic gradient boosting scheme. (def: depends on number of data points and learning mode)\n          'allow_const_label':False,                                # CPU and GPU; bool. Use with datasets having equal label values for all objects. (def: False)\n          \n          ####### Overfitting Detection Settings #######\n          'early_stopping_rounds':50,                               # CPU and GPU; int. Set overfitting detector type to 'Iter' and stop training after these many iterations after seeing best so far. (def: False)\n          #'od_type':'IncToDec',                                    # CPU and GPU; string. 'IncToDec' or 'Iter'. (def: 'IncToDec')\n          #'od_pval':0,                                             # CPU and GPU; float; [10^-10, 1-^-2] for best results. Threshold for IncToDec overfitting detector type. Larger -> Overfitting is detected earlier. (def: 0[=off]) # Validation Data should be given #  ## Do not use with Iter overfitting detectot type. ##\n          #'od_wait':20,                                            # CPU and GPU; int. Number of iterations after optimal result before stopping. Depends on Overfitting detector type: IncToDec -> Ignore overfitting detector when threshold is reached and countinue learning for specified iterations OR Iter -> like EarlyStoppingRound. (def: 20)\n          \n          ####### Binarization Settings #######\n          'border_count':254,                                       # CPU and GPU; [1, 255]. Number of splits for numerical features. (def: 254 for CPU, 128 for GPU)\n          'feature_border_type':'GreedyLogSum',                     # CPU and GPU; string. Binarization mode for numerical feature. 'Meadin', 'Uniform', 'UniformAndQuantile', 'MaxLogSum', 'MinEntropy', 'GreedyLogSum'. (def: 'GreedyLogSum')\n          \n          ####### Multiclassification settings #######\n          #'classes_count':None,                                    # CPU and GPU; int. Upper limit for numeric class label.  ## If this is specified, labels in input data should be smaller than this value. ##\n          \n          ####### Performance settings #######\n          'thread_count':2,                                         # CPU and GPU; int. CPU -> Optimizes speed, GPU -> given value for reading data from hard drive and does not affect training; During training one main thread and one thread for each GPU are used. (def: -1[= number of cores])\n          'used_ram_limit':'2gb',                                   # CPU only; int. `Attempt` to limit amount of CPU memory used. # Often affects CTR calc memory usage. MB, KB, GB as '32gb'. # ## In some cases it is impossible to limit RAM usage. ##\n          #'gpu_ram_part':0.95,                                     # GPU only; float. How much of GPU RAM to use for training. (def: 0.95)\n          #'pinned_memory_size':1073741824,                         # GPU only; int. How much pinned(page-locked) CPU RAM to use per GPU. (def: 1073741824)\n          #'gpu_cat_features_storage':None,                         # GPU only; string. Method for storing cat features values. 'CpuPinnedMemory', 'GpuRam'. (def: None[='GPuRam']) # Use 'CpuPinnedMemory' if feature combinations are used and GPU mem. is not sufficient. #\n          #'data_partition':None,                                   # GPU only; string. Method for splitting input dataset btw multiple workers. 'FeatureParallel' -> split by features and calc their values on diff GPUs OR 'DocParallel' -> split by objects and calc each of these on certain GPU. (def: depends on learning mode and input datasets)\n          \n          ####### Processing Unit's settings #######\n          'task_type':'CPU',                                        # CPU and GPU; Processing Unit (PU) to use for training. 'CPU' or 'GPU'. (def: 'CPU')\n          #'devices':None,                                          # GPU only; string. IDs of the GPU devices to use for training. As \"0:2:4-7\" (def: None[=all])\n          \n          ####### Viz settings #######\n          'name':\"CatBoost Proj\",                                   # CPU and GPU; string. Experiment name to display in visualization tools. (def: 'experiment')\n          \n          ####### Output settings #######\n          #'logging_level':None,                                    # CPU and GPU; string. 'Silent', 'Verbose', 'Info', 'Debug'.  (def: None)\n          'metric_period':1,                                        # CPU and GPU; int; >0. Freq of iterations to calc the values of obj and metrics. (def: 1)\n          'verbose':True,                                           # CPU and GPU; string. 'Silent', 'Verbose', 'Info', 'Debug'\n          'train_dir':'catboost_info',                              # CPU and GPU; string. Dir for storing the files generated during training. (def: 'catboost_info')\n          'model_size_reg':0.5,                                     # CPU only; [0, inf). Regularize model size. (def: None [=0.5])\n          'allow_writing_files':True,                               # CPU only; bool. Allow to write 'analytical' snapshot files during training. (def: True) # Fasle -> snapshot and data viz tools will be unavailable. #\n          'save_snapshot':False,                                    # CPU and GPU; bool. To enable snapshotting for restoring the training progress after an interruption. (def: None)\n          'snapshot_file':\"experiment.cbsnapshot\",                  # CPU and GPU; string. Name of file to save training progress in. (def: 'experiment.cbsnapshot')\n          'snapshot_interval':600,                                  # CPU and GPU; int. Interval between saving snapshots in seconds. Last snapshot at end of training. (def: 600)\n          #'roc_file':None,                                         # CPU and GPU; string. Output file to save ROC curve. (def: None[=File not saved]) ## Can only be used in CV mode with 'LogLoss' loss. ##\n          \n          ####### CTR settings ########\n          #'simple_ctr':None,                                       # CPU and GPU; string. Binarization settings for simple categorical features. As 'CtrType[:TargetBorderCount=BorderCount][:TargetBorderType=BorderType][:CtrBorderCount=Count][:CtrBorderType=Type][:Prior=num_1\/denum_1]...'. CtrType: 'Borders', 'Buckets', 'BinarizedTargetMeanValue', 'Counter' (GPU: 'Borders', 'Buckets', 'FeatureFreq', 'FloatTargetMeanValue'); TargetBorderCount: [1, 255]; TargetBorderType: 'Mean', 'Uniform', 'UniformAndQuantiles', 'MaxLogSum', 'MinEntropy', 'GreedyLogSum'; CtrBorderCount: [1, 255]; CtrBorderType: 'Median', 'Uniform', 'UnifromAndQuantiles', 'MaxLogSum', 'GreedyLogSum'; Prior: number (adds value to numerator) (GPU: two slash delimited numbers -> first to num., second to den.)\n          #'combinations_ctr':None,                                 # CPU and GPU; string. Binarization settings for simple categorical features. As 'CtrType[:TargetBorderCount=BorderCount][:TargetBorderType=BorderType][:CtrBorderCount=Count][:CtrBorderType=Type][:Prior=num_1\/denum_1]...'. CtrType: 'Borders', 'Buckets', 'BinarizedTargetMeanValue', 'Counter' (GPU: 'Borders', 'Buckets', 'FeatureFreq', 'FloatTargetMeanValue'); TargetBorderCount: [1, 255]; TargetBorderType: 'Mean', 'Uniform', 'UniformAndQuantiles', 'MaxLogSum', 'MinEntropy', 'GreedyLogSum'; CtrBorderCount: [1, 255]; CtrBorderType: 'Median', 'Uniform', 'UnifromAndQuantiles', 'MaxLogSum', 'GreedyLogSum'; Prior: number (adds value to numerator) (GPU: two slash delimited numbers -> first to num., second to den.)\n          #'per_feature_ctr':None,                                  # CPU and GPU; string. Binarization settings for simple categorical features. As 'CtrType[:TargetBorderCount=BorderCount][:TargetBorderType=BorderType][:CtrBorderCount=Count][:CtrBorderType=Type][:Prior=num_1\/denum_1]...'. CtrType: 'Borders', 'Buckets', 'BinarizedTargetMeanValue', 'Counter' (GPU: 'Borders', 'Buckets', 'FeatureFreq', 'FloatTargetMeanValue'); TargetBorderCount: [1, 255]; TargetBorderType: 'Mean', 'Uniform', 'UniformAndQuantiles', 'MaxLogSum', 'MinEntropy', 'GreedyLogSum'; CtrBorderCount: [1, 255]; CtrBorderType: 'Median', 'Uniform', 'UnifromAndQuantiles', 'MaxLogSum', 'GreedyLogSum'; Prior: number (adds value to numerator) (GPU: two slash delimited numbers -> first to num., second to den.)\n          #'ctr_target_border_count':8,                             # CPU and GPU; int; [1, 255]. Max number of borders to use in target binarization for categorical features that need it. (def: num_class-1(CPU), 1 otherwise) # Overrides one specified in 'simple_ctr', 'combinations_ctr', 'per_feature_ctr' #\n          #'counter_calc_method':None,                              # CPU and GPU; string. For calculating the Counter CTR Type. 'SkipTest' -> Objs from validation not considered OR 'Full' -> All objs from both learn and valid are considered. (def: None[='Full'])\n          #'max_ctr_complexity':None,                               # CPU and GPU; int. Max number of categorical featurse that can be combined. (def: 4)\n          #'ctr_leaf_count_limit':None,                             # CPU only; int. Max leaves with categorical features. Only leaves with top freq of values are selected. Reduces model size and memory at cost of quality. (def: None[=not limited])\n          #'store_all_simple_ctr':None,                             # CPU only; bool. Ignore cat features which are not used in feature combinations, when choosing candidates for exclusion. (def: None[= False]) ## Should be used with 'ctr_leaf_count_limit' ##\n          #'final_ctr_computation_mode':None,                       # CPU and GPU; string. Final CTR computation mode. 'Default' -> Compute final CTRs for learn and validation mode OR 'Skip' -> Do not compute final CTRs for learn and validation datasets. In this case the resulting model cannot be applied. Dec.s the size of resulting model. ## Can be useful for research purposes when only metric values have to be calculated. ##\n          \n          #'silent':None,\n          #'device_config':None,                                    # \n          #'dev_score_calc_obj_block_size':None,                    #\n          #'metadata':None,                                         # \n          #'cat_features':None,                                     # \n          #'growing_policy':None,                                   #\n          #'min_samples_in_leaf':None,                              #\n          #'max_leaves_count':None                                  #\n}","0cce1e21":"reg_model = cb.train(\n                    pool=train_data,                                            # catboost.Pool or tuple (X, y)\n                    params=params,                                              # None or dict.  ## These params overrides all. ##\n                    logging_level=None,                                         # 'Silent', 'Verbose', 'Info', 'Debug'\n                    verbose=False,                                              # bool or int.\n                    iterations=1000,                                            # Number of boosting iterations.\n                    eval_set=valid_data,                                        # catboost.Pool or tuple (X, y) or list as [(X1, y1), (X2, y2) ...]\n                    plot=True,                                                  # Weather to draw train and eval error in Jupyter Notebook.\n                    metric_period=None,                                         # Freq of evaluating metrics.\n                    #early_stopping_rounds=50,                                  # Activates 'Iter' overfitting detector with 'od_wait' set to this value.\n                    save_snapshot=False,                                        # bool. Enable progress snapshotting.\n                    snapshot_file=\"\",                                           # string. Snapshot file path.\n                    snapshot_interval=None                                      # int. Interval btw saving snapshots(seconds).\n)","ecd5d385":"reg_model.best_iteration_, reg_model.best_score_","e6c06a5c":"reg_model.feature_importances_","00a31a06":"reg_model.evals_result_['validation_0']['RMSE'][-10:]","5dd07619":"You can also do it after converting your data which XGBoost can use effectively for speedup (for more info on speedups available in XGBoost look into XGBoost's portion of my post).","06d0cf8f":"### With ScikitLearn like API:","bbd88f70":"From LightGBM docs:\n\n#### Note\n\nA custom objective function can be provided for the ``objective`` parameter.\nIn this case, it should have the signature\n``objective(y_true, y_pred) -> grad, hess`` or\n``objective(y_true, y_pred, group) -> grad, hess``:\n\n    y_true : array-like of shape = [n_samples]\n        The target values.\n    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n        The predicted values.\n    group : array-like\n        Group\/query data, used for ranking task.\n    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n        The value of the gradient for each sample point.\n    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n        The value of the second derivative for each sample point.\n\nFor multi-class task, the y_pred is group by class_id first, then group by row_id.\nIf you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\nand you should group grad and hess in this way as well.","43e16eef":"# 7. CatBoost:","ba6db603":"# 3. Dataset","70fd9466":"#### Note\n\nCustom eval function expects a callable with following signatures:\n``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n``func(y_true, y_pred, weight, group)``\nand returns (eval_name, eval_result, is_bigger_better) or\nlist of (eval_name, eval_result, is_bigger_better):\n\n    y_true : array-like of shape = [n_samples]\n        The target values.\n    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n        The predicted values.\n    weight : array-like of shape = [n_samples]\n        The weight of samples.\n    group : array-like\n        Group\/query data, used for ranking task.\n    eval_name : string\n        The name of evaluation.\n    eval_result : float\n        The eval result.\n    is_bigger_better : bool\n        Is eval result bigger better, e.g. AUC is bigger_better.","a12193fa":"#### With Sklearn like API:","2ec66af1":"# 5. XGBoost:","e6bd18a3":"# 1. Install","9eecbf58":"### With XGBoost's API:","c808dbb5":"From XGBoost's docs, [here](https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/param_tuning.html).\n\n#### Control Overfitting\nWhen you observe high training accuracy, but low test accuracy, it is likely that you encountered overfitting problem.\n\nThere are in general two ways that you can control overfitting in XGBoost:\n\n1. The first way is to directly control model complexity.\n     * This includes `max_depth`, `min_child_weight` and `gamma`.\n1. The second way is to add randomness to make training robust to noise.\n     * This includes `subsample` and `colsample_bytree`.\n     * You can also reduce stepsize `eta`. Remember to increase `num_round` when you do so.\n\n\n#### Handle Imbalanced Dataset\nFor common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of XGBoost model, and there are two ways to improve it.\n\n1. If you care only about the overall performance metric (AUC) of your prediction\n    * Balance the positive and negative weights via `scale_pos_weight`\n    * Use AUC for evaluation\n1. If you care about predicting the right probability\n    * In such a case, you cannot re-balance the dataset\n    * Set parameter `max_delta_step` to a finite number (say 1) to help convergence","484afded":"# 6. LightGBM:","ab8fb017":"# 2. Import\n\n","d1a47ad0":"### With ScikitLearn like API:","7385b19c":"#### With CatBoost's API:","1a037215":"# 4. AdaBoost:","d5df39ad":"This kernel goes with a post I wrote on Towards Data Science under group posts `Concepts`(all notebooks can be found on [here](https:\/\/github.com\/PuneetGrov3r\/MediumPosts\/tree\/master\/Concepts)):\n\n1. [Clearing air around \"Boosting\"](https:\/\/towardsdatascience.com\/clearing-air-around-boosting-28452bb63f9e) (Understanding the current go-to algorithm for state of the art result.)\n\nI would really recommend to read the post, as this kernel is mainly code.\n\nTo get a better view at code, look [here](https:\/\/nbviewer.jupyter.org\/github\/PuneetGrov3r\/MediumPosts\/blob\/master\/Concepts\/Boosting.ipynb)\n\n### Index:\n1. Install\n1. Import\n1. Dataset\n1. AdaBoost\n1. XGBoost \n    * With Sklearn like API\n    * With XGBoost's API\n1. LightGBM\n    * With Sklearn like API\n    * With LightGBM's API\n1. CatBoost\n    * With Sklearn like API\n    * With CatBoost's API","9be6e22f":"### With LightGBM API:"}}