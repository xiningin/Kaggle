{"cell_type":{"373a5f94":"code","0ce8ee87":"code","59060d37":"code","ebfccf22":"code","e0267aba":"code","5d100b11":"code","9d5a68c2":"code","561c3b46":"code","7a68bf85":"code","dd28142c":"code","b764189f":"code","e1d5b923":"code","bf58e995":"code","aa9d5c52":"code","57843af1":"code","185a3705":"code","f5483552":"code","a990f6ce":"code","5cee80f7":"code","e8dd113c":"code","bfd0d1e1":"code","1281c902":"code","41c957e5":"code","e400bed1":"code","8f70d51f":"code","a1d5876d":"code","c6060bdc":"code","8055414f":"code","b378c023":"code","fa5a967a":"code","8690704f":"code","0a366094":"code","daee8765":"code","ff792602":"code","415e7403":"code","91d8348e":"code","6969ce16":"code","75c5e2d5":"code","2985f2d7":"code","0df9fae6":"code","d159dbe8":"code","3f2a815f":"code","8cad9e14":"code","74e39529":"code","d31ac824":"code","6b28687b":"code","c5c1b47c":"code","be0fe268":"code","d04ab587":"code","cdd5afbe":"code","88c01281":"code","8e8f9cdb":"code","d8ec88ae":"code","1beecbf5":"code","f671cde1":"code","a494f799":"code","1104d7f9":"code","6fdb3de9":"code","9f6ce44b":"code","4262c6f7":"code","89ab492c":"code","77c978e2":"code","d751cc23":"code","f7c166a1":"code","4c98eb03":"code","2bd2f2ea":"markdown","4dbb3c3e":"markdown","64c60292":"markdown","82e219b9":"markdown","8b2cff76":"markdown","fab568b6":"markdown","0144b392":"markdown","2126ea3b":"markdown","db35ad12":"markdown","f5268299":"markdown","9d81ed77":"markdown","a1a8340c":"markdown","9c48a02f":"markdown"},"source":{"373a5f94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0ce8ee87":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn","59060d37":"df = pd.read_csv('..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv',low_memory=False)\ndf.head()","ebfccf22":"df.info()","e0267aba":"df.isna().sum()","5d100b11":"df.columns","9d5a68c2":"fig,ax = plt.subplots()\nax.scatter(df['saledate'][:1000],df['SalePrice'][:1000])","561c3b46":"df['SalePrice'].plot.hist()","7a68bf85":"df.saledate.dtype","dd28142c":"# Import data again but this time parse dates\ndf = pd.read_csv('..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv',low_memory=False,\n                parse_dates=['saledate'])\ndf.saledate.dtype\n","b764189f":"df['saledate'][:5]","e1d5b923":"fig,ax = plt.subplots()\nax.scatter(df['saledate'][:1000],df['SalePrice'][:1000])","bf58e995":"df.head()","aa9d5c52":"df.head().T","57843af1":"# Sort dataframe by date order.\ndf.sort_values(by=['saledate'],inplace=True,ascending=True)\ndf.saledate.head()","185a3705":"# Make a copy of the original DataFrame.\ndf_tmp = df.copy()\n","f5483552":"df_tmp['saleYear'] = df_tmp.saledate.dt.year\ndf_tmp['saleMonth'] = df_tmp.saledate.dt.month\ndf_tmp['saleDay'] = df_tmp.saledate.dt.day\ndf_tmp['saleDayOfWeek'] = df_tmp.saledate.dt.dayofweek\ndf_tmp['saleDayOfYear'] = df_tmp.saledate.dt.dayofyear","a990f6ce":"df_tmp.head().T","5cee80f7":"#now we can remove saledate\ndf_tmp.drop('saledate',axis=1,inplace=True)","e8dd113c":"df_tmp.state.value_counts()","bfd0d1e1":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_jobs=-1,\n                             random_state=42)\nmodel.fit(df_tmp.drop('SalePrice',axis=1),df_tmp['SalePrice'])","1281c902":"pd.api.types.is_string_dtype(df_tmp['UsageBand'])","41c957e5":"# Find the columns which contains strings\nfor label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","e400bed1":"df_tmp.info()","8f70d51f":"for label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_tmp[label] = content.astype('category').cat.as_ordered()","a1d5876d":"df_tmp.info()","c6060bdc":"df_tmp.state.cat.categories","8055414f":"df_tmp.state.cat.codes","b378c023":"# check missing data\ndf_tmp.isnull().sum()\/len(df_tmp)","fa5a967a":"for label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","8690704f":"# Check for which nuemeric columns have null values:\nfor label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","0a366094":"# Fill missing rows with median\nfor label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            df_tmp[label+'_is_missing'] = pd.isnull(content)\n            df_tmp[label] = content.fillna(content.median())\n            ","daee8765":"df_tmp.isna().sum()","ff792602":"# Check for columns which aren't numeric:\nfor label,content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        print(label)","415e7403":"pd.Categorical(df_tmp['state']).codes","91d8348e":"# Turn categorical variables into numbers and fill missing\nfor label,content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        df_tmp[label+'_is_missing'] = pd.isnull(content)\n        df_tmp[label] = pd.Categorical(content).codes+1","6969ce16":"df_tmp.isnull().sum()","75c5e2d5":"df_tmp.info()","2985f2d7":"model = RandomForestRegressor(n_jobs=-1,\n                             random_state=42)\nmodel.fit(df_tmp.drop('SalePrice',axis=1),df_tmp['SalePrice'])","0df9fae6":"model.score(df_tmp.drop('SalePrice',axis=1),df_tmp['SalePrice'])","d159dbe8":"df_tmp.saleYear.value_counts()","3f2a815f":"# Splitting data into training and validation sets\ndf_val = df_tmp[df_tmp.saleYear== 2012]\ndf_train = df_tmp[df_tmp.saleYear != 2012]\nlen(df_val), len(df_train)","8cad9e14":"# Split into X and y\nX_train,y_train = df_train.drop('SalePrice',axis=1),df_train['SalePrice']\nX_valid,y_valid = df_val.drop('SalePrice',axis=1),df_val['SalePrice']","74e39529":"# Creating a RMSLE\nfrom sklearn.metrics import mean_squared_log_error,mean_absolute_error,r2_score\ndef rmsle(y_test,y_preds):\n    \n    return np.sqrt(mean_squared_log_error(y_test,y_preds))\ndef show_scores(model):\n    train_preds = model.predict(X_train)\n    val_preds = model.predict(X_valid)\n    scores = {'Training MAE':mean_absolute_error(y_train,train_preds),\n             'Valid MAE':mean_absolute_error(y_valid,val_preds),\n             'Traing RMSLE':rmsle(y_train,train_preds),\n             'valid Rmsle':rmsle(y_valid,val_preds),\n             'Training R^2': r2_score(y_train,train_preds),\n             'valid R^2':r2_score(y_valid,val_preds)}\n    return scores","d31ac824":"%%time\nmodel = RandomForestRegressor(n_jobs=-1,\n                             random_state=42)\nmodel.fit(X_train,y_train)","6b28687b":"model = RandomForestRegressor(n_jobs=-1,\n                             random_state=42,\n                              max_samples=10000,)","c5c1b47c":"%%time\n\nmodel.fit(X_train,y_train)","be0fe268":"show_scores(model)","d04ab587":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf_grid ={'n_estimators':np.arange(10,100,10),\n         'max_depth':[None,3,5,10],\n         'min_samples_split':np.arange(2,20,2),\n         'min_samples_leaf':np.arange(1,20,2),\n         'max_features':[0.5,1,'sqrt','auto'],\n         'max_samples':[10000]}\nrs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,\n                                                   random_state=42,),\n                                                   param_distributions=rf_grid,\n                                                   n_iter=5,\n                                                   cv=5,\n                                                   verbose=True)\nrs_model.fit(X_train,y_train)","cdd5afbe":"rs_model.best_params_","88c01281":"show_scores(rs_model)","8e8f9cdb":"%%time\nideal_model = RandomForestRegressor(n_estimators=90,\n                                   min_samples_split= 2,\n                                    min_samples_leaf= 13,\n                                   max_samples= 10000,\n                                   max_features= 'sqrt',\n                                   max_depth= None,\n                                   n_jobs=-1)\nideal_model.fit(X_train,y_train)","d8ec88ae":"show_scores(ideal_model)","1beecbf5":"df_test = pd.read_csv('..\/input\/bluebook-for-bulldozers\/Test.csv',\n                     low_memory=False,\n                     parse_dates=['saledate'])\ndf_test.head()","f671cde1":"test_preds = ideal_model.predict(df_test)","a494f799":"df_test.isnull().sum()","1104d7f9":"df_test.info()","6fdb3de9":"def preprocess_data(df):\n    \"\"\"\n    Perform transformations on df and return transformed df.\n    \n    \"\"\"\n    # parse date\n    df['saleYear'] = df.saledate.dt.year\n    df['saleMonth'] = df.saledate.dt.month\n    df['saleDay'] = df.saledate.dt.day\n    df['saleDayOfWeek'] = df.saledate.dt.dayofweek\n    df['saleDayOfYear'] = df.saledate.dt.dayofyear\n    \n    df.drop('saledate',axis=1,inplace=True)\n    \n    # Fill the numeric rows with median\n    for label,content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                df[label+'_is_missing']=pd.isnull(content)\n                df[label] = content.fillna(content.median())\n                \n    # Fill categorical missing data and turn categories into numbers\n    for label,content in df.items():\n        if not pd.api.types.is_numeric_dtype(content):\n            df[label+'_is_missing']=pd.isnull(content)\n            #we add +1 to category code\n            df[label] = pd.Categorical(content).codes+1\n            \n    \n    return df","9f6ce44b":"# process test data\ndf_test = preprocess_data(df_test)\ndf_test.head()","4262c6f7":"df_train.head()","89ab492c":"# Make predictions on updated test data\ntest_preds = ideal_model.predict(df_test)","77c978e2":"# We can find how the columns differ using sets\nset(X_train.columns)-set(df_test.columns)","d751cc23":"# Manually adjust df_test to have auctioneerID_is_missing column\ndf_test['auctioneerID_is_missing']=False\ndf_test","f7c166a1":"test_preds = ideal_model.predict(df_test)","4c98eb03":"# Format the predictions into the same format Kaggle is after:\ndf_preds = pd.DataFrame()\ndf_preds['SalesID'] =df_test['SalesID']\ndf_preds['SalesPrice'] = test_preds\ndf_preds\n","2bd2f2ea":"### Fill missing values","4dbb3c3e":"### Hyperparameter Tuning with RandomizedSearchCV","64c60292":"### Add datetime parameters for saledate column","82e219b9":"### Parsing Dates\n\nwhen we work with the time series data, we want to enrich the time & date component as much as possible.\n\nwe can do that by telling pandas which of our columns has dates in it using `parse_dates` parameter.","8b2cff76":"## Modelling","fab568b6":"### Sort DataFrame by saledate\nWhen working with time series data, it's a good idea to sort it by date.","0144b392":"### Convert String to categories\n\nOne way we can turn all of our data into numbers is by converting them into pandas categories.","2126ea3b":"# Make predictions on Test Data","db35ad12":"### preprocessing the data (getting the test dataset as training data)","f5268299":"### Fill numeric columns first","9d81ed77":"### Building an evaluation Function","a1a8340c":"# Filling and turning categorical variables into numbers","9c48a02f":"# Testing model on a subset( to Tune Hyperparameters)"}}