{"cell_type":{"703a2a37":"code","5701f2bb":"code","4138f031":"code","aea07a4c":"code","605bc976":"code","406f17dc":"code","3041aeac":"code","9832a418":"code","49b5f6b5":"code","055a00c8":"code","17010dad":"code","cc2f0502":"code","66d80e3e":"code","f75b787e":"code","e6497422":"code","7ad92f6e":"code","312ff93d":"code","c7224bc1":"code","e5226952":"code","ac2d021c":"code","5a571d66":"code","8e6be52c":"code","e47698be":"code","c8bff0bd":"code","c085d74d":"code","9705bf8b":"code","49ce4fe9":"code","c3e40bb0":"code","5a16f6ec":"code","c3e13af0":"code","2f597fe2":"code","5ebdfa84":"code","f9dc0ac1":"code","dacb614e":"code","e4f35fee":"code","d9020190":"code","c4f27733":"code","8aab4913":"code","50722450":"code","041a3ccc":"code","335acd0a":"code","2907c6ec":"code","16588d86":"code","0b1f688f":"code","c1737a85":"code","88e59e08":"code","ef33f0a6":"code","9f2f84c9":"code","b94252b6":"code","09cefd81":"code","d0b00b70":"code","8873e2fd":"code","66c36e35":"code","a748e8b1":"code","02f219a9":"code","8ab4ac51":"code","8762f343":"code","8b45bdb7":"code","d9972e8c":"code","c9c4d633":"code","fad472a1":"code","c23855c5":"code","cae3eba7":"code","802153a6":"code","b585c8a3":"code","cac6f7e2":"code","1d8ef211":"code","d53a7c4c":"code","9ba407b2":"code","81daeb17":"code","edef7090":"code","318da3a8":"markdown","c43f81cb":"markdown","04608d40":"markdown","a92d52df":"markdown","4b2267cb":"markdown","9c648810":"markdown","2e3b9561":"markdown","bce00444":"markdown","b10f1ae9":"markdown","34d16d46":"markdown","78879228":"markdown","e0e1b414":"markdown","bbc63de9":"markdown","16588982":"markdown","3848b7b5":"markdown","c40d2910":"markdown","d4154f3c":"markdown","e53b9e6a":"markdown","0b389f55":"markdown","d9051b16":"markdown","82243222":"markdown","5048a5ab":"markdown","2c7b7bd3":"markdown","1171c4c7":"markdown","244d7307":"markdown","51c62de3":"markdown","d1d8fee3":"markdown","5bb58617":"markdown","745228d2":"markdown","1445b521":"markdown","dc30d89c":"markdown","21abfdd2":"markdown","7fcc3b95":"markdown","05ee1d48":"markdown","52b88b96":"markdown","ccb363aa":"markdown","ca8c6577":"markdown","d30e7c81":"markdown","7a547bc6":"markdown","c664f789":"markdown","e8125557":"markdown","b9aa9889":"markdown","db01feb5":"markdown","657a0bb6":"markdown","a03c04e3":"markdown","7fc1eb5e":"markdown","e4b6f659":"markdown","bf791511":"markdown","4215de01":"markdown","39a7631f":"markdown","6fe22f1f":"markdown","f33efd85":"markdown","3b5d278b":"markdown","fe49d90e":"markdown","80529c38":"markdown","f3203660":"markdown","1fbf7777":"markdown","fb83ac73":"markdown","9e4a1396":"markdown","b7a79624":"markdown","5c24227a":"markdown","54ddc397":"markdown","d4653cbd":"markdown"},"source":{"703a2a37":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5701f2bb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\ninit_notebook_mode(connected=True)\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n    \n    \n","4138f031":"data = pd.read_csv(\"..\/input\/online-shoppers-intention\/online_shoppers_intention.csv\")","aea07a4c":"data.head(10)","605bc976":"data.describe()","406f17dc":"null_table = pd.DataFrame(data.isnull().sum().values.reshape(1,-1), columns = data.isnull().sum().index)\nnull_table = null_table.rename(index = {0:'Total Null Values'})\nnull_pct = null_table.iloc[0,:]\/12330 *100\nnull_pct = pd.DataFrame(null_pct.values.reshape(1,-1), columns = null_pct.index)\nnull_pct = null_pct.rename(index = {0 : 'Null %'})\nnull_table = null_table.append(null_pct)\nnull_table","3041aeac":"# Cleaning\ndata = data.dropna()\ndata","9832a418":"neg_ad_dur = data[data['Administrative_Duration'] < 0]\nneg_info_dur = data[data['Informational_Duration'] < 0]\nneg_prd_dur = data[data['ProductRelated_Duration'] < 0]\nprint(\" The length of each durations are {} ,{} , {}\".format(len(neg_ad_dur), len(neg_info_dur),len(neg_prd_dur)))","49b5f6b5":"# Dropping the negative Durations\ndata = data.drop(data[data['Administrative_Duration'] < 0].index)\ndata = data.drop(data[data['Informational_Duration'] < 0].index)\ndata = data.drop(data[data['ProductRelated_Duration'] < 0].index)\n#Checking , no negative values\ndata.describe()","055a00c8":"# Countplots on \nplt.style.use('fivethirtyeight')\nfig,ax = plt.subplots(nrows = 2, ncols = 4,figsize = (17,7))\nfig.tight_layout()\n\n#fig.suptitle('Countplots Of Some Features')    Main TItle\nax[0,0].bar(data['Administrative'].value_counts().index,data['Administrative'].value_counts().values,color = 'red')\nax[0,0].set_title('Administrative',size=13)\nax[0,0].set_xlim(0,20)\n\nax[0,1].bar(data['Informational'].value_counts().index,data['Informational'].value_counts().values,color = 'blue')\nax[0,1].set_title('Informational',size=13)\nax[0,1].set_xlim(0,10)\n\nax[0,2].bar(data['ProductRelated'].value_counts().index,data['ProductRelated'].value_counts().values,color = 'purple')\nax[0,2].set_title('ProductRelated',size=13)\nax[0,2].set_xlim(0,200)\n\nax[0,3].bar(data['OperatingSystems'].value_counts().index,data['OperatingSystems'].value_counts().values,color = 'yellow')\nax[0,3].set_title('OperatingSystems',size=13)\n\nax[1,0].bar(data['Browser'].value_counts().index,data['Browser'].value_counts().values,color = 'orange')\nax[1,0].set_title('Browser',size=13)\n\nax[1,1].bar(data['Region'].value_counts().index,data['Region'].value_counts().values,color = 'black')\nax[1,1].set_title('Region',size=13)\n\n\nax[1,2].bar(data['TrafficType'].value_counts().index,data['TrafficType'].value_counts().values,color = 'navy')\nax[1,2].set_title('TrafficType',size=13)\n\nfig.delaxes(ax[1,3])   # since it is a odd number plots, delete last subplot","17010dad":"# Lets see other categorical features with pie chart\nplt.style.use('ggplot')\nfig,ax = plt.subplots(3,2,figsize=(10,6))\nfig.set_figheight(12)\nfig.set_figwidth(12)\nplt.tight_layout()\n# Revenue\nax[0,0].pie(data['Revenue'].value_counts().values,labels = ['False','True'],explode = [0.1,0.1],shadow = True,autopct='%1.1f%%')\nax[0,0].set_title('Ratio of Revenue Availability')\n\n\n# Month() No Jan and April\nax[0,1].pie(data['Month'].value_counts().values,labels= data['Month'].value_counts().index[:],shadow = True, explode = [0.1,0.1,0.1,0.1,0.2,0.2,0.2,0.2,0.2,0.3],autopct='%1.0f%%')\nax[0,1].set_title('Ratio of Each Month')\n\n# Visitor Types\nax[1,0].pie(data['VisitorType'].value_counts().values,labels= data['VisitorType'].value_counts().index[:],explode = [0.1,0.1,0.3],shadow = True,autopct='%1.1f%%',colors=['green','red','blue'])\nax[1,0].set_title('Ratio of Visitor Types')\n\n# Weekend\nax[1,1].pie(data['Weekend'].value_counts().values,labels= data['Weekend'].value_counts().index[:],explode = [0.1,0.1],shadow = True,autopct='%1.1f%%')\nax[1,1].set_title('Ratio of Weekends')\n\n# Special Days\nax[2,0].pie(data['SpecialDay'].value_counts().values,labels= data['SpecialDay'].value_counts().index[:],explode = [0.1,0.2,0.2,0.3,0.3,0.4],shadow = True,autopct='%1.0f%%')\nax[2,0].set_title('Ratio of Special Days')\n\nfig.delaxes(ax[2,1])","cc2f0502":"# Lets see the Ratio of Revenue in each types\nplt.style.use('fivethirtyeight')\nfig,ax = plt.subplots(nrows = 2, ncols = 4,figsize = (17,10))\nfig.tight_layout(pad = 3)\n\n\nadm_rev = data[['Administrative','Revenue']]\nrev_p1 = pd.DataFrame(data.groupby('Revenue')['Administrative'].sum()).T\nrev_p1.plot.bar(stacked=True,ax=ax[0,0])\nax[0,0].set_xticklabels(['Administrative'], rotation=360)\nplt.legend(loc='best')\n\n\ninfo_rev = data[['Informational','Revenue']]\nrev_p2 = pd.DataFrame(data.groupby('Revenue')['Informational'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[0,1],color = ['black','white'])\nax[0,1].set_xticklabels(['Informational'], rotation=360)\nplt.legend(loc='best')\n\ninfo_rev = data[['ProductRelated','Revenue']]\nrev_p2 = pd.DataFrame(data.groupby('Revenue')['ProductRelated'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[0,2],color = ['purple','red'])\nax[0,2].set_xticklabels(['ProductRelated'], rotation=360)\nplt.legend(loc='best')\n\n\n\ninfo_rev = data[['OperatingSystems','Revenue']]\nrev_p2 = pd.DataFrame(data.groupby('Revenue')['OperatingSystems'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[0,3],color = ['green','yellow'])\nax[0,3].set_xticklabels(['OperatingSystems'], rotation=360)\n\n\ninfo_rev = data[['Browser','Revenue']]\nrev_p2 = pd.DataFrame(data.groupby('Revenue')['Browser'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[1,0],color = ['navy','orange'])\nax[1,0].set_xticklabels(['Browser'], rotation=360)\n\n\ninfo_rev = data[['Region','Revenue']]\nrev_p2 = pd.DataFrame(data.groupby('Revenue')['Region'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[1,1],color = ['black','red'])\nax[1,1].set_xticklabels(['Region'], rotation=360)\n\n\ninfo_rev = data[['TrafficType','Revenue']]\nrev_p2 = pd.DataFrame(data.groupby('Revenue')['TrafficType'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[1,2],color = ['blue','green'])\nax[1,2].set_xticklabels(['TrafficType'], rotation=360)\n\nfig.delaxes(ax[1,3])","66d80e3e":"# Lets see the Ratio of Revenue in each categorical features\nfig, ax = plt.subplots(2,2,figsize = (14,6))\nplt.tight_layout(pad= 3)\n\nmonth_rev = data[['Month','Revenue']]\nvis_rev = data[['VisitorType','Revenue']]\nweekends_rev = data[['Weekend','Revenue']]\nspd_rev = data[['SpecialDay','Revenue']]\n\nsns.countplot(x = month_rev['Month'],hue = month_rev['Revenue'],ax =ax[0,0]).set_title('Ratio of Revenue Of Each Months')\nsns.countplot(x = vis_rev['VisitorType'],hue = vis_rev['Revenue'],ax =ax[0,1]).set_title('Ratio of Revenue Of Each Visitor Types')\nsns.countplot(x = weekends_rev['Weekend'],hue = weekends_rev['Revenue'],ax =ax[1,0]).set_title('Ratio of Revenue In Weekdays Or Weekends')\nsns.countplot(x = spd_rev['SpecialDay'],hue = spd_rev['Revenue'],ax =ax[1,1]).set_title('Ratio of Revenue Of Each Special Days')","f75b787e":"# lETS SEE SOME DATA RELATED TO THE BOUNCE AND EXIT RATE\nsns.boxplot(x=data['Revenue'],y=data['BounceRates']).set_title('Distribution of BounceRates Depending on Availability of Making Revenue')\n","e6497422":"sns.boxplot(x=data['Revenue'],y=data['ExitRates']).set_title('Distribution of ExitRates Depending on Availability of Making Revenue')\n","7ad92f6e":"sns.boxenplot(x=data['Revenue'],y=data['PageValues']).set_title('Page Values depending on Availability of Making Revenue')","312ff93d":"sns.scatterplot(data['BounceRates'],data['ExitRates'],hue = data['VisitorType'],palette='Set3_r').set_title('ExitRates VS BounceRates')","c7224bc1":"# Exit\/Bounce Rates VS Page Values, Is higher the page value less Exit Rates?\n\nsns.scatterplot(x= data['PageValues'],y=data['ExitRates'],hue = data['Revenue']).set_title('Will the Exit Rates descrease as the Page Values Increase?')","e5226952":"sns.scatterplot(x= data['PageValues'],y=data['BounceRates'],hue = data['Revenue']).set_title('Will the Bounce Rates descreases as the Page Value Increases')","ac2d021c":"sns.scatterplot(x=data['Administrative_Duration'],y=data['PageValues'],hue = data['Revenue']).set_title('Page Values VS Administrative_Duration ')","5a571d66":"sns.scatterplot(x=data['Informational_Duration'],y=data['PageValues'],hue = data['Revenue']).set_title('Page Values VS Informational_Duration ')","8e6be52c":"sns.scatterplot(x=data['ProductRelated_Duration'],y=data['PageValues'],hue = data['Revenue']).set_title('Page Values VS ProductRelated_Duration ')","e47698be":"# Durations\nadmin_df = pd.DataFrame(data['Administrative_Duration'])\nadmin_df.rename(columns={'Administrative_Duration' :'Duration'},inplace = True)\nadmin_df['type'] = admin_df['Duration'].apply(lambda x:'Administrative')\n\ninform_df= pd.DataFrame(data['Informational_Duration'])\ninform_df.rename(columns={'Informational_Duration' :'Duration'},inplace =True)\ninform_df['type'] = inform_df['Duration'].apply(lambda x:'Informational')\n\nprod_df= pd.DataFrame(data['ProductRelated_Duration'])\nprod_df.rename(columns={'ProductRelated_Duration' :'Duration'},inplace =True)\nprod_df['type'] = prod_df['Duration'].apply(lambda x:'ProductRelated')\n\n\n\ndur_df = pd.concat([admin_df,inform_df,prod_df])\nsns.boxenplot(dur_df['type'], dur_df['Duration']).set_title('Each Types of Durations Distribution')\n\n","c8bff0bd":"# Wow, the gap is too big, so lets see them \ndur_df12 = pd.concat([admin_df,inform_df])\n#plt.ylim(0,500)\nsns.boxplot(dur_df12['type'],dur_df12['Duration']).set_title('Administrative and Informational Durations Distribution')","c085d74d":"pd.DataFrame(data['Administrative_Duration'].describe())","9705bf8b":"pd.DataFrame(data['Informational_Duration'].describe())","49ce4fe9":"# outliers of each columns:\n# Administrative :  High : 93.95 + 93.95 (IQR)\n#                   Low  : 0- 93.95\n# Outliers of Administrative is larger than 187.9 and lower than - 93.95\nad_zero = data[data['Administrative_Duration'] == 0]\noutlier_len = len(data[data['Administrative_Duration'] > 187.9]) \noutlier_prt = outlier_len \/ len(data['Administrative_Duration']) *100    # 12%\nprint(\"The % of Outliers of Administrative Duration Columns are \",outlier_prt)\nad_zero_prt = len(ad_zero)\/len(data['Administrative_Duration']) *100     # 47%\nprint(\"The % of 0 Values of Administrative Duration Columns are \",ad_zero_prt)\n\n# Informational :  High : 0\n#                   Low  : 0\n# Outliers of Administrative is larger than 187.9 and lower than - 93.95\ninfo_zero = data[data['Informational_Duration'] == 0]\noutlier_len2 = len(data[data['Informational_Duration'] > 0]) \noutlier_prt2 = outlier_len2 \/ len(data['Informational_Duration']) *100    \nprint(\"The % of Outliers of Informational Duration Column is \",outlier_prt2)\ninfo_zero_prt = len(info_zero)\/len(data['Informational_Duration']) *100     \nprint(\"The % of 0 Values of Informational Duration Column is \",info_zero_prt)\n","c3e40bb0":"\nprddur_rev = data.groupby('Revenue')['ProductRelated_Duration'].mean()\nprddur_rev = pd.DataFrame(prddur_rev)\nprddur_rev","5a16f6ec":"# WHich MOnth will have the most transactions near the SpecialDays ( =0 if close to the special days)?\nspcday_0 = data[data['SpecialDay'] == 0]\ndata_month = spcday_0.groupby('Month')['SpecialDay'].count()\nplt.figure(figsize=(13,6))\nplt.title('Number of Special Days In Each Months')\nplt.pie(data_month.values,labels = data_month.index,shadow =True, explode=[0.2,0.2,0.2,0.1,0.1,0.12,0.13,0.1,0.3,0.3],autopct='%1.0f%%')","c3e13af0":"fig , ax = plt.subplots(2,3,figsize = (14,7))\nplt.tight_layout(pad = 2)\nsns.boxplot(data['Month'],data['ExitRates'],ax=ax[0,0])\nsns.violinplot(data['Month'],data['BounceRates'],ax=ax[0,1])\nsns.violinplot(data['Month'],data['PageValues'],ax=ax[0,2])\nsns.boxenplot(data['Month'],data['Administrative_Duration'],ax = ax[1,0])\nsns.boxenplot(data['Month'],data['Informational_Duration'],ax = ax[1,1])\nsns.boxenplot(data['Month'],data['ProductRelated_Duration'],ax = ax[1,2])","2f597fe2":"# Customer Types\nfig , ax = plt.subplots(1,3,figsize = (20,5))\nax.flatten()\nplt.tight_layout(pad =3)\n\n\nsns.violinplot(data['VisitorType'],data['ExitRates'],ax = ax[0],hue = data['Revenue'])\n\nsns.violinplot(data['VisitorType'],data['BounceRates'],ax = ax[1],hue = data['Revenue'])\n\nsns.violinplot(data['VisitorType'],data['PageValues'],ax = ax[2],hue = data['Revenue'])\n","5ebdfa84":"fig , ax = plt.subplots(1,4,figsize = (20,5))\nax.flatten()\nplt.tight_layout(pad =3)\n\nsns.violinplot(data['OperatingSystems'],data['ExitRates'],ax = ax[0],split=True,hue = data['Revenue'])\nsns.violinplot(data['Browser'],data['ExitRates'],ax = ax[1],split=True,hue = data['Revenue'])\nsns.violinplot(data['Region'],data['ExitRates'],ax = ax[2],split=True,hue = data['Revenue'])\nsns.violinplot(data['TrafficType'],data['ExitRates'],ax = ax[3],split=True,hue = data['Revenue'])\n","f9dc0ac1":"fig , ax = plt.subplots(1,4,figsize = (20,5))\nax.flatten()\nplt.tight_layout(pad =3)\n\nsns.violinplot(data['OperatingSystems'],data['BounceRates'],ax = ax[0],split=True,hue = data['Revenue'])\nsns.violinplot(data['Browser'],data['BounceRates'],ax = ax[1],split=True,hue = data['Revenue'])\nsns.violinplot(data['Region'],data['BounceRates'],ax = ax[2],split=True,hue = data['Revenue'])\nsns.violinplot(data['TrafficType'],data['BounceRates'],ax = ax[3],split=True,hue = data['Revenue'])\n","dacb614e":"fig , ax = plt.subplots(1,4,figsize = (20,5))\nax.flatten()\nplt.tight_layout(pad =3)\n\nsns.boxenplot(data['OperatingSystems'],data['PageValues'],ax = ax[0],hue = data['Revenue'])\nsns.boxenplot(data['Browser'],data['PageValues'],ax = ax[1],hue = data['Revenue'])\nsns.boxenplot(data['Region'],data['PageValues'],ax = ax[2],hue = data['Revenue'])\nsns.boxenplot(data['TrafficType'],data['PageValues'],ax = ax[3],hue = data['Revenue'])","e4f35fee":"# Correlation with Revenue\ndata_corr = data.corr()['Revenue'] \nsns.barplot(data_corr[0:-1].index,data_corr[0:-1].values).set_title('Correlation with the Revenue')\nplt.xticks(rotation = 90)","d9020190":"plt.figure(figsize=(15,5))\nax = sns.heatmap(data.corr(),cmap='Blues',annot=True)\nax.set_title('The Correlation Heatmap')\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom+1,top-1)","c4f27733":"\n# Lets Cluster the Group \nX = data[['ProductRelated_Duration','BounceRates']]","8aab4913":"# THe Elbow Method\n\ninertia =[]\nfor i in range(1,11):\n    kms = KMeans(n_clusters=i,max_iter = 100 , n_init = 10,init = 'k-means++',random_state=100).fit(X)\n    inertia.append(kms.inertia_)\nplt.plot(range(1,11),inertia,marker ='X')","50722450":"# 3 Cluseters\nkms = KMeans(n_clusters=3,max_iter = 100 , n_init = 10,init = 'k-means++',random_state=100).fit(X)\npred = kms.fit_predict(X)","041a3ccc":"#plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=pred, s=50, cmap='viridis')\nplt.scatter(X.iloc[pred == 0,0],X.iloc[pred == 0,1],label='First',color ='green')\nplt.scatter(X.iloc[pred == 1,0],X.iloc[pred == 1,1],label ='Second',color='blue')\nplt.scatter(X.iloc[pred == 2,0],X.iloc[pred == 2,1],label='Third',color='yellow')\ncenters = kms.cluster_centers_\nplt.scatter(centers[:, 0],centers[:,1],s=100,color='red',label='Centroids')\n\nplt.legend(loc='best')","335acd0a":"print(data[pred == 0]['Revenue'].value_counts())\nprint(\"The Percentage of first cluster groups who made a transactions is : \" ,1341\/8758*100,\"%\")","2907c6ec":"print(data[pred == 1]['Revenue'].value_counts())\nprint(\"The Percentage of second cluster groups who made a transactions is : \" ,71\/139*100,\"%\")","16588d86":"print(data[pred == 2]['Revenue'].value_counts())\nprint(\"The Percentage of third cluster groups who made a transactions is : \" ,496\/1478*100,\"%\")","0b1f688f":"# Administrative The Elbow Method\ninertia = []\nY = data[['Administrative_Duration','BounceRates']]\nfor i in range(1,11):\n    kms = KMeans(n_clusters = i,max_iter=500,n_init = 10,init = 'k-means++',random_state = 100).fit(Y)\n    inertia.append(kms.inertia_)\nplt.plot(range(1,11),inertia,marker='X')","c1737a85":"#3 Clusters\n\nkms = KMeans(n_clusters = 3,max_iter=500,n_init = 10,init = 'k-means++',random_state = 100).fit(Y)\npred_a = kms.predict(Y)\n#plt.scatter(x=Y.iloc[:,0],y=Y.iloc[:,1],c=pred_a,cmap='viridis')\nplt.scatter(Y.iloc[pred_a == 0,0],Y.iloc[pred_a == 0,1],label='First',color ='green')\nplt.scatter(Y.iloc[pred_a == 1,0],Y.iloc[pred_a == 1,1],label ='Second',color='blue')\nplt.scatter(Y.iloc[pred_a == 2,0],Y.iloc[pred_a == 2,1],label='Third',color='yellow')\ncenters = kms.cluster_centers_\nplt.scatter(x=kms.cluster_centers_[:,0],y=kms.cluster_centers_[:,1],s=100,label='Centroids',color = 'red')\nplt.legend(loc ='best')","88e59e08":"#first group\nprint(data[pred_a == 0]['Revenue'].value_counts())\nprint(\"The Percentage of first cluster groups who made a transactions is : \" ,1493\/9062*100,\"%\")","ef33f0a6":"# second group\nprint(data[pred_a == 1]['Revenue'].value_counts())\nprint(\"The Percentage of first cluster groups who made a transactions is : \" ,382\/1193*100,\"%\")","9f2f84c9":"# third group\nprint(data[pred_a == 2]['Revenue'].value_counts())\nprint(\"The Percentage of first cluster groups who made a transactions is : \" ,33\/120*100,\"%\")","b94252b6":"# Informational The Elbow Method\ninertia = []\nZ= data[['Informational_Duration','BounceRates']]\nfor i in range(1,11):\n    kms = KMeans(n_clusters = i,max_iter=500,n_init = 10,init = 'k-means++',random_state = 100).fit(Z)\n    inertia.append(kms.inertia_)\nplt.plot(range(1,11),inertia,marker='X')","09cefd81":"#3 Clusters\n\nkms = KMeans(n_clusters = 3,max_iter=500,n_init = 10,init = 'k-means++',random_state = 100).fit(Z)\npred_i = kms.predict(Z)","d0b00b70":"#plt.scatter(x=Z.iloc[:,0],y=Z.iloc[:,1],c=pred_i,cmap='viridis')\nplt.scatter(Z.iloc[pred_i == 0,0],Z.iloc[pred_i == 0,1],label='First',color ='green')\nplt.scatter(Z.iloc[pred_i == 1,0],Z.iloc[pred_i == 1,1],label ='Second',color='blue')\nplt.scatter(Z.iloc[pred_i == 2,0],Z.iloc[pred_i == 2,1],label='Third',color='yellow')\ncenters = kms.cluster_centers_\nplt.scatter(x=kms.cluster_centers_[:,0],y=kms.cluster_centers_[:,1],s=100,label='Centroids',color = 'red')\nplt.legend(loc ='best')","8873e2fd":"#first group\nprint(data[pred_i == 0]['Revenue'].value_counts())\nprint(\"The Percentage of first cluster groups who made a transactions is : \" ,1719\/9896*100,\"%\")","66c36e35":"# second group\nprint(data[pred_i == 1]['Revenue'].value_counts())\nprint(\"The Percentage of first cluster groups who made a transactions is : \" ,158\/394*100,\"%\")\n","a748e8b1":"# third group\nprint(data[pred_i == 2]['Revenue'].value_counts())\nprint(\"The Percentage of first cluster groups who made a transactions is : \" ,31\/85*100,\"%\")","02f219a9":"data_true = data[data['Revenue'] == True][['ExitRates','BounceRates','Administrative_Duration','Informational_Duration','ProductRelated_Duration']]\ndata_true.describe()","8ab4ac51":"data_true = data[data['Revenue'] == False][['ExitRates','BounceRates','Administrative_Duration','Informational_Duration','ProductRelated_Duration']]\ndata_true.describe()","8762f343":"le = LabelEncoder()\ndata['Month'] = le.fit_transform(data['Month'])\ndata['VisitorType'] = le.fit_transform(data['VisitorType'])\ndata['Weekend'] = le.fit_transform(data['Weekend'])\ndata['Revenue'] = le.fit_transform(data['Revenue'])\ndata","8b45bdb7":"X = data.drop('Revenue',axis=1)\ny = data['Revenue']\n","d9972e8c":"# Lets make a classification model to classify whether the customer will make a transaction or not\n# RandomClassification Classifier\n\n\nX_test,X_train,y_test,y_train = train_test_split(X,y,test_size=0.3,random_state = 101)\nrfclf = RandomForestClassifier(n_estimators = 30,max_depth = 10,random_state = 101)\n\n\nrfclf.fit(X_train,y_train)\npred = rfclf.predict(X_test)\nprint(classification_report(y_test,pred))","c9c4d633":"# Lets Optimize the Random Forest Classifier using GridSearch\n\nparam_grid = {\n    'n_estimators' : [80,100,120,150],\n    'max_depth' : [7,10,15,20],\n    'min_samples_leaf' : [1,2,3,4],\n    'min_samples_split': [2,4,6,8]\n}\n\ngridsearch = GridSearchCV(estimator=rfclf,param_grid=param_grid,verbose = 1)\ngridsearch.fit(X_train,y_train)\n","fad472a1":"gridsearch.best_params_","c23855c5":"rfclf = RandomForestClassifier(n_estimators = 150,max_depth = 7,min_samples_leaf = 4, min_samples_split = 2,random_state = 101)\nrfclf.fit(X_train,y_train)\npred = rfclf.predict(X_test)\nprint(classification_report(y_test,pred))\n\n# 0 is False, 1 is True, the precision of detecting True has increased\nfrom sklearn.metrics import accuracy_score\nrfacc = accuracy_score(y_test,pred)","cae3eba7":"ax = sns.heatmap(confusion_matrix(y_test,pred),annot = True,cmap='Blues')\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom+0.5,top-0.5)\nplt.title('Confusion Matrix of RandomForestClassifier')","802153a6":"#Logistic Regression\n\nlregression = LogisticRegression(max_iter = 100)\nlregression.fit(X_train,y_train)\npred_i = lregression.predict(X_test)\nprint(classification_report(y_test,pred_i))\nlogregacc = accuracy_score(y_test,pred_i)","b585c8a3":"ax = sns.heatmap(confusion_matrix(y_test,pred_i),annot = True,cmap='YlGnBu')\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom+0.5,top-0.5)\nplt.title('Confusion Matrix of Logistic Regression')","cac6f7e2":"# SVM \n\n\nparam_grid = {'C':[0.1,1,10,100,1000],\n              'kernel':['rbf'],\n              'gamma' : [0.1,1,10,100,1000]}\ngridsearch = GridSearchCV(SVC(),param_grid = param_grid,verbose = 1)\ngridsearch.fit(X_train,y_train)\n\nsvm = SVC(C=100,kernel = 'rbf',gamma = 100)\nsvm.fit(X_train,y_train)\npred_s = svm.predict(X_test)\n\nprint(classification_report(y_test,pred_s))\nsvmacc = accuracy_score(y_test,pred_s)\n","1d8ef211":"ax = sns.heatmap(confusion_matrix(y_test,pred_s),annot = True,cmap='Blues_r')\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom+0.5,top-0.5)\nplt.title('Confusion Matrix of SupportVectorMachine')","d53a7c4c":"# KNN\n\nerror_rate = []\nfor i in range(1,15):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_k = knn.predict(X_test)\n    error_rate.append(np.mean(pred_k!= y_test))\nplt.plot(range(1,15),error_rate,marker='X',linestyle='dashed',markerfacecolor='red', markersize=10)","9ba407b2":"knn = KNeighborsClassifier(n_neighbors = 6 )\nknn.fit(X_train,y_train)\npred_k = knn.predict(X_test)\nprint(classification_report(y_test,pred_k))\nknnacc = accuracy_score(y_test,pred_k)","81daeb17":"ax = sns.heatmap(confusion_matrix(y_test,pred_k),annot = True,cmap='Blues')\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom+0.5,top-0.5)\nax.set_ylim(bottom+0.5,top-0.5)\nplt.title('Confusion Matrix of Logistic Regression')","edef7090":"accuracy_df = pd.DataFrame.from_dict({'Accuracy_Score' : [rfacc,logregacc,svmacc,knnacc] }) \n\naccuracy_df.rename(index = {0:'RandomForestClassifier',1:'Logistic Regression', 2: 'SVM',3:'KNN'})","318da3a8":"### *Importing Modules*","c43f81cb":"### *The Elbow Method*\n\n- To find the Optimal number of clusters ","04608d40":"#### - Compare to the previous clustering, the second group( Middle values of BounceRates and Administrative Duration) has slightly more customers with True Revenue","a92d52df":"### - Which month has the most number of purchases on the special days( special day == 0) ?","4b2267cb":"### - Then.. Is it really true that the customers with Low ExitRates,Bounce Rates and high Durations of stay are more likely to generate profits?","9c648810":"## *Logistic Regression*","2e3b9561":"### - __Stacked-bar charts which show the ratio between False Revenue and True Revenue of some numerical and categorical columns__","bce00444":"### *RandomForestClassifier*","b10f1ae9":"## *Cleaning Data*","34d16d46":"#### - The distribution of Administrative and Informational are very narrowly distributed compared to ProductRelated Column. Therefore, let's take a look at Administrative and Informational columns separately","78879228":"#### - __As we can see, as the each types of durations increase, the PageValues decrease. Customers who make the transactions tend to stay  less on website than those who do not make the transactions__","e0e1b414":"### *Clustering the Customers with Informational_Duration and BounceRates*","bbc63de9":"#### *- Based on the Clustering graphs and the tables, it is reasonable to say that the customers who have lower Exit\/Bounce Rates with longer stays are more likely to generate profits.*","16588982":"### - Scatterplots which show the correlations between two numerical features","3848b7b5":"### - Longer the ProductRelated Durations, more likely to make revenue?\n","c40d2910":"#### - Resultantly, Page Values has the highest correlation( around 0.5)  with Revenue compare to all other features. ","d4154f3c":"### - Distribution of ExitRates,BounceRates and PageValues of various categorical features","e53b9e6a":"# Classification\n### __Goal :__\n - To classify the customer whether he\/she is interested on purchasing the products online.\n\n### __Classification Models :__\n\n- I will be using 4 classification models : RandomForestClassifier,Logistic Regression, Support Vector Machine and KNearestNeighbors","0b389f55":"### - __Here are some Non-Stacked bar charts to depict the ratio between False Revenue and True Revenue of some categorical columns__","d9051b16":"# Thanks for reading my project!!","82243222":"- The Number of Clusters will be 3 as well.","5048a5ab":"## *KNearestNeighbors*","2c7b7bd3":"### - Distribution of ExitRates,BounceRates, Each of Durations and PageValues of the Categorical Columns","1171c4c7":"#### - As the result, the average duration of ProductRelated with true Revenue is higher than with the false Revenue","244d7307":"#### - *Now the data is all clean!!*","51c62de3":"#### - similar to the second clustering group,the second group( Middle values of BounceRates and Administrative Duration) also has slightly more customers with true Revenue.","d1d8fee3":"### -  __Countplots which count the number of customers in various features__","5bb58617":"# Conclusion :\n### - From my analysis, customers who are interested on buying the products :\n- Tend to stay longer on the website especially when they are on the website which is productrelated.\n- Less likely to bounce or exit\n- Have strong Correlation with the PageValues\n\n#### - Therefore, in order to increase the customers who generates the profits, the business should find a way to decrease the BounceRates and ExitRates and improve on the quality of the page.","745228d2":"#### __These are the tables with True Revenue__","1445b521":"- The Number of Clusters will be 3 since the value rigth after the significant drop of inertia is 3","dc30d89c":"### - *Data Preprocessing*\n\n- Since there are several categorical columns, therefore, we use LabelEncoder to convert the categorical columns to numerical","21abfdd2":"#### - However, if we take a look at the Duration columns, there are some negative durations which does not make sense. There was no further descriptions about the negative durations.","7fcc3b95":"#### - This table  shows the proportion of null variables in each columns","05ee1d48":"### *Loading the Data*","52b88b96":"#### - __According to the percentages above, both of the columns contain slightly high number of outliers and high proportion of  0 values.__","ccb363aa":"### *Clustering the Customers with ProductRelated_Duration and BounceRates*","ca8c6577":"#### -  Since there are only 33 negative durations for each of the durations, therefore, drop them as well.","d30e7c81":"### - Analysis of Administrative,Informational and ProductRelated Duration","7a547bc6":"### So...\n   - *RandomForestClassifier has the Highest Accuracy Score(~90) with highest values for Recall, Precision and F1 Score. It predicts the True Revenue most accurately compare to other models.*","c664f789":"### *Clustering the Customers with Administrative_Duration and BounceRates*","e8125557":"### - __Pie charts which show the quantity ratios of several categorical columns__","b9aa9889":"#### -  __As the BounceRates increases, the customers are more likely to exit the page__","db01feb5":"#### __These are the tables with False Revenue__","657a0bb6":"## __About The Data__ :\n\n- The Data from Kaggle : https:\/\/www.kaggle.com\/roshansharma\/online-shoppers-intention\n\n### *Source*:\n1. *C. Okan Sakar*\n\n   Department of Computer Engineering, Faculty of\n   Engineering and Natural Sciences, Bahcesehir University,\n   34349 Besiktas, Istanbul, Turkey\n    \n    \n2. *Yomi Kastro*\n\n   Inveon Information Technologies Consultancy and Trade,\n   34335 Istanbul, Turkey*\n    \n    \n### *Dataset and Attribute Information* :\n\n#### *Dataset* :\n- The dataset consists of feature vectors belonging to 12,330 sessions.\n    The dataset was formed so that each session\n    would belong to a different user in a 1-year period to avoid\n    any tendency to a specific campaign, special day, user\n    profile, or period.\n    \n#### *Attribute* :\n\n- *__Revenue__* => class whether it can make a revenue or not\n- *__Administrative, Administrative Duration, Informational, Informational Duration, Product Related and Product Related  Duration__*  => represent the number of different types of pages visited by the visitor in that session and total time spent in                        each of these page categories.\n- *__Bounce Rate__* => percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other                        requests to the analytics server during that session\n- *__Exit Rate__*   => the percentage that were the last in the session\n- *__Page Value__*  => feature represents the average value for a web page that a user visited before completing an e-commerce                                transaction\n- *__Special Day__* => indicates the closeness of the site visiting time to a specific special day (e.g. Mother\u2019s Day, Valentine's Day)                      in which the sessions are more likely to be finalized with transaction. For example, for Valentina\u2019s day, this value                  takes a nonzero value between February 2 and February 12, zero,before and after this date unless it is close to                        another special day, and its maximum value of 1 on February 8\n- *__Operating system,browser, region, traffic type__* => Different types of operating systems, browser, region and traffic type used to                                                         visit the website\n- *__Visitor type__* => Whether the customer is a returning or new visitor\n- *__Weekend__*     => A Boolean value indicating whether the date of the visit is weekend\n- *__Month__*        => Month of the year.\n\n\n\n    \n    \n","a03c04e3":"#### - Let's use the GridSearch to optimize our accuracy","7fc1eb5e":"#### - According to the first clustering graph, second group( cluster with the lowest PageValues and longer Productrelated duration) is the groups of customers who is more interested on purchasing the products","e4b6f659":"## *Clustering Different Customer Groups*","bf791511":"## *EDA On the Dataset*","4215de01":"#### - This model is not so helpful for our case since it can not detect the true values at all","39a7631f":"#### - From the graph, six number of neighbours tend to have the least error rates","6fe22f1f":"### - Let's see which feature is most likely to be correlated with the 'Revenue' Feature?","f33efd85":"#### -Before we go into the classifications, let's understand more about the customers intentions by clustering the customer groups. ","3b5d278b":"### *General analysis of the dataset*\n\n#### - *In order to understand more about the data, let's observe the dataset with some analysis tools*","fe49d90e":"#### - __By looking at the two graphs above, as the PageValues increase, the BounceRates\/ExitRates decrease. As the PageValues increase, the customers are more likely to make a transactions, which leads to lower Bounce\/Exit Rates__","80529c38":"#### - As the result, there is no improvement on the accuracy score. However, the gridsearch improved the precision when selecting the true values.","f3203660":"## *Support Vector Machine*","1fbf7777":"### *Correlation Between the Features*","fb83ac73":"#### - From the observations, Customers who actually made the transactions are less likely to Exit or Bounce from the website and the pagevalues is usually higher than the Customers who did not make the purchase.","9e4a1396":"- The Number of Clusters will be 3 as well.","b7a79624":"# *__Customer's Intention Clustering and Classification Project__*","5c24227a":"###  __About This Project__ :\n- In this project, I will analyze the Customer's Intentions based on the transactions made online in a one year duration.\n\n\n\n### __Goals__: \n- Cluster the groups using the BounceRates to identify which customers are more likely to be the interested(More likely to make a transaction) customers.\n- Classify the customers whether he\/she is a customer who is going to generate a revenue or not. I will use  LogisticRegression,SVM, KNN and RandomForestClassifier as the classification models.","54ddc397":"#### - The null values from the 1st column('Administrative') to 8th column('ExitRates') have less than 0.2% of null values, therefore, I will remove all the null values","d4653cbd":"### -  Distributions of ExitRates,BounceRates and PageValues between False\/True Revenue "}}