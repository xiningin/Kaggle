{"cell_type":{"40d6898a":"code","3039c7e4":"code","8c7302a1":"code","018cf877":"code","196911e1":"code","551102ab":"code","e5a4d627":"code","661074a0":"code","459c2048":"code","c6e2db69":"code","e8906c70":"code","8aeefa5f":"code","42562236":"code","7b466162":"code","8895ec33":"code","e8f2fb90":"code","6a178b47":"code","5a5bc2d5":"code","eae155d4":"code","a900e307":"code","3ed66693":"code","b9d0aac4":"code","ca2a1fad":"code","06e06a2b":"code","b4a89246":"code","3a1ddc76":"markdown","5dce1b71":"markdown","0c8df340":"markdown","f3043ce6":"markdown","5bf86356":"markdown","9053e52f":"markdown"},"source":{"40d6898a":"thresh = 0.125 # threshold of initial predictions\nnum_thresh = 3 # number of prediction required to be used for final output\nstep = 30\n\nnmin = 10\nnmax = 2000 # Higher recall\n\nIMSIZE = 1024\nprediou = 0.55 # use qishen's value\n\nfastcommit = True # if true, uses a shortened inference for commit.\n\n# Detection models\ncheck_end = \"..\/input\/nfleffdetmodel\/effdet4-end-1024-epoch8.bin\"\ncheck_side = \"..\/input\/nfleffdetmodel\/best-side-1024-effdet4-epoch6.bin\"\n\n# Classification models\ncheckpoint_classification_128_0 = \"..\/input\/classification-nfl\/resnet3d_128_mixup_all_epoch6_fold0.pth\"\ncheckpoint_classification_128_1 = \"..\/input\/classification-nfl\/resnet3d_128_mixup_all_epoch7_fold0.pth\"\ncheckpoint_classification_128_2 = \"..\/input\/classification-nfl\/resnet3d_128_mixup_all_epoch8_fold0.pth\"\ncheckpoint_classification_128_3 = \"..\/input\/classification-nfl\/res3d-ishigamifold.pth\"\ncheckpoint_classification_96_0 = \"..\/input\/nfl-classification-models\/res3d_ishigamisplit_96x96_best.pth\"\ncheckpoint_classification_96_1 = \"..\/input\/nfl-classification-models\/res3d_96x96_ishigamisplit_last.pth\"\ncheckpoint_classification_96_2 = \"..\/input\/nfl-classification-models\/mc3_96x96_ishigamisplit_last.pth\"\ncheckpoint_classification_96_3 = \"..\/input\/nfl-classification-models\/mc3_96x96_ishigamisplit_best.pth\"","3039c7e4":"# Submission\u304bCommit\u6642\u304b\u3069\u3046\u304b\u5b58\u5728\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u898b\u3066\u78ba\u8a8d\u3059\u308b\u3002commit\u6642\u306a\u3089\u3070\u9ad8\u901f\u5316\u306e\u305f\u3081\u306b\u4ff5\u3055\u3093\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u306b\u884c\u304f\u3002\nimport os\nif os.path.exists(\"..\/input\/nfl-impact-detection\/test\/57906_000718_Endzone.mp4\"):\n    commit = True\nelse:\n    commit = False","8c7302a1":"!pip install ..\/input\/nfl-lib\/timm-0.1.26-py3-none-any.whl\n!tar xfz ..\/input\/nfl-lib\/pkgs.tgz\n# for pytorch1.6\ncmd = \"sed -i -e 's\/ \\\/ \/ \\\/\\\/ \/' timm-efficientdet-pytorch\/effdet\/bench.py\"\n!$cmd","018cf877":"import sys\nsys.path.insert(0, \"timm-efficientdet-pytorch\")\nsys.path.insert(0, \"..\/input\/ttach-kaggle\/ttach\")\nsys.path.insert(0, \"omegaconf\")\nsys.path.insert(0, \"..\/input\/odachkaggle\/ODA-Object-Detection-ttA-main\")\nimport odach as oda\nimport ttach as tta\n\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport pandas as pd\nimport gc\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\nfrom effdet.efficientdet import HeadNet\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseed_everything(SEED)","196911e1":"def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n    video_path=f\"{video_dir}\/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv2.VideoCapture(video_path)\n    if only_with_impact:\n        boxes_all = video_labels.query(\"video == @video_name\")\n        print(video_path, boxes_all[boxes_all.impact == 1.0].shape[0])\n    else:\n        print(video_path)\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        if only_with_impact:\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            boxes_with_impact = boxes[boxes.impact == 1.0]\n            if boxes_with_impact.shape[0] == 0:\n                continue\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}\/{video_name}'.replace('.mp4',f'_{str(frame).zfill(3)}.png')\n        _ = cv2.imwrite(image_path, img)","551102ab":"DATA_ROOT_PATH = 'test_images'\n# Use dataset if in commitmode to save GPU\nif commit:\n    DATA_ROOT_PATH= \"..\/input\/nfl-impact-detection-train-frames\"\nout_dir = DATA_ROOT_PATH\n\nvideo_dir = '\/kaggle\/input\/nfl-impact-detection\/test'\nuniq_video = sorted([path.split('\/')[-1] for path in glob(f'{video_dir}\/*.mp4')])\nprint(uniq_video)\n\n# Generate images if in Test mode\nif not os.path.exists(out_dir):\n    !mkdir -p $out_dir\n    for video_name in uniq_video:\n        mk_images(video_name, pd.DataFrame(), video_dir, out_dir, only_with_impact=False)","e5a4d627":"def get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=IMSIZE, width=IMSIZE, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","661074a0":"class DatasetRetriever(Dataset):\n    def __init__(self, image_ids, dir=None, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n        self.dir = dir\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        if not commit:\n            image = cv2.imread(f'{DATA_ROOT_PATH}\/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n        else:\n            image = cv2.imread(f'{DATA_ROOT_PATH}\/{self.dir}\/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","459c2048":"import torchvision\n\n\ndef load_classification_model(ckpt_path: str):\n    if \"mc3\" in ckpt_path:\n        model = torchvision.models.video.mc3_18(pretrained=False)\n    else:\n        model = torchvision.models.video.r3d_18(pretrained=False)\n    model.fc = torch.nn.Linear(512, 1)\n    \n    ckpt = torch.load(ckpt_path)\n    if \"model_state_dict\" in ckpt.keys():\n        model.load_state_dict(ckpt[\"model_state_dict\"])\n    else:\n        model.load_state_dict(ckpt)\n    model.to(\"cuda\").eval()\n    return model","c6e2db69":"model_128_0 = load_classification_model(checkpoint_classification_128_0)\nmodel_128_1 = load_classification_model(checkpoint_classification_128_1)\nmodel_128_2 = load_classification_model(checkpoint_classification_128_2)\nmodel_128_3 = load_classification_model(checkpoint_classification_128_3)\nmodel_96_0 = load_classification_model(checkpoint_classification_96_0)\nmodel_96_1 = load_classification_model(checkpoint_classification_96_1)\nmodel_96_2 = load_classification_model(checkpoint_classification_96_2)\nmodel_96_3 = load_classification_model(checkpoint_classification_96_3)\n\nmodels_128 = [model_128_0, model_128_1, model_128_2, model_128_3]\nmodels_96 = []\n\n!nvidia-smi","e8906c70":"def load_net(checkpoint_path):\n    if \"effdet4\"in checkpoint_path:\n        config = get_efficientdet_config('tf_efficientdet_d4')\n    else:\n        config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n    config.num_classes = 2\n    config.image_size=IMSIZE\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\nnetend = load_net(check_end)\nnetside = load_net(check_side)","8aeefa5f":"def bb_intersection_over_union(A, B) -> float:\n    xA = max(A[0], B[0])\n    yA = max(A[1], B[1])\n    xB = min(A[2], B[2])\n    yB = min(A[3], B[3])\n\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA) * max(0, yB - yA)\n\n    if interArea == 0:\n        return 0.0\n\n    # compute the area of both the prediction and ground-truth rectangles\n    boxAArea = (A[2] - A[0]) * (A[3] - A[1])\n    boxBArea = (B[2] - B[0]) * (B[3] - B[1])\n\n    iou = interArea \/ float(boxAArea + boxBArea - interArea)\n    return iou\n\ndef df2box(df):\n    return np.array([df[\"x\"],df[\"y\"],df[\"w\"]+df[\"x\"],df[\"h\"]+df[\"y\"]]).T","42562236":"def make_predictions(images, net, score_threshold=0.1):\n    images = torch.stack(images).cuda().float()\n    predictions = []\n    with torch.no_grad():\n        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n        for i in range(images.shape[0]):\n            boxes = det[i].detach().cpu().numpy()[:,:4]    \n            scores = det[i].detach().cpu().numpy()[:,4]\n            labels = det[i].detach().cpu().numpy()[:,5]\n            indexes = np.where((scores > score_threshold)*(labels==2))[0]\n            #boxes = boxes[indexes]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            predictions.append({\n                'boxes': boxes[indexes],\n                'scores': scores[indexes],\n                'labels': labels[indexes],\n            })\n    return [predictions]\n\nfrom odach.wbf import *\ndef run_wbf(predictions, image_index, image_size=IMSIZE, iou_thr=prediou, skip_box_thr=thresh, weights=None):\n    boxes = [(prediction[image_index]['boxes']\/(image_size-1)).tolist()  for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n    \n    labels = [prediction[image_index]['labels'].tolist() for prediction in predictions]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    \n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels\n\nimport matplotlib.pyplot as plt","7b466162":"def find_match(boxes_list, new_box, idx, target_iou):\n    best_index = []\n    for i in range(len(boxes_list)):\n        box = boxes_list[i]\n\n        iou = bb_intersection_over_union(box, new_box)\n        if iou > target_iou:\n            best_index.append(idx[i])\n\n    return best_index\n\ndef find_matching_box(boxes_list, new_box, idx, target_iou):\n    best_index = []\n    for i in range(len(boxes_list)):\n        box = boxes_list[i]\n\n        iou = bb_intersection_over_union(box, new_box)\n        if iou > target_iou:\n            best_index.append(idx[i])\n\n    return best_index","8895ec33":"def predict(imgs, net, dir=None):\n    \n    dataset = DatasetRetriever(\n    image_ids=imgs,\n    dir=dir,\n    transforms=get_valid_transforms()\n    )\n\n    def collate_fn(batch):\n        return tuple(zip(*batch))\n\n    data_loader = DataLoader(\n        dataset,\n        batch_size=8,\n        shuffle=False,\n        num_workers=4,\n        drop_last=False,\n        collate_fn=collate_fn\n    )\n\n    result_image_ids = []\n    results_boxes = []\n    results_scores = []\n    results_frames = []\n    cnt = 0\n    # Inference!\n    for images, image_ids in data_loader:\n        predictions = make_predictions(images, net)\n        for i, image in enumerate(images):\n            box_list, score_list, label_list = run_wbf(predictions, image_index=i, skip_box_thr=thresh)\n            boxes = box_list\n            scores = score_list\n            image_id = image_ids[i]\n            boxes[:, 0] = (boxes[:, 0] * 1280 \/ IMSIZE)\n            boxes[:, 1] = (boxes[:, 1] * 720 \/ IMSIZE)\n            boxes[:, 2] = (boxes[:, 2] * 1280 \/ IMSIZE)\n            boxes[:, 3] = (boxes[:, 3] * 720 \/ IMSIZE)\n            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n            boxes = boxes.astype(np.int32)\n            boxes[:, 0] = boxes[:, 0].clip(min=0, max=1280-1)\n            boxes[:, 2] = boxes[:, 2].clip(min=0, max=1280-1)\n            boxes[:, 1] = boxes[:, 1].clip(min=0, max=720-1)\n            boxes[:, 3] = boxes[:, 3].clip(min=0, max=720-1)\n            result_image_ids += [image_id]*len(boxes)\n            results_boxes.append(boxes)\n            results_scores.append(scores)\n            cnt+=1\n    \n    box_df = pd.DataFrame(np.concatenate(results_boxes), columns=['x', 'y', 'w', 'h'])\n    test_df = pd.DataFrame({'scores':np.concatenate(results_scores), 'image_name':result_image_ids})\n    test_df = pd.concat([test_df, box_df], axis=1)\n    test_df['frame'] = test_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\n    test_df['gameKey'] = test_df.image_name.str.split('_').str[0].astype(int)\n    test_df['playID'] = test_df.image_name.str.split('_').str[1].astype(int)\n    test_df['view'] = test_df.image_name.str.split('_').str[2]\n    test_df['frame'] = test_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\n    test_df['video'] = test_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\n    test_df.sort_values('frame', inplace=True)\n    return test_df","e8f2fb90":"if fastcommit and commit:\n    uniq_video = uniq_video[0:2]\n    \nprint(uniq_video)\n\nfor iii,videoname in enumerate(uniq_video[::2]):\n    videoend = videoname\n    videoside = videoname[:-11]+\"Sideline.mp4\"\n    print(\"videoend:{}, videoside:{}\".format(videoend, videoside))\n    \n    ######################################\n    # Inference Videos\n    ######################################\n    for phase, vid in enumerate([videoend, videoside]):\n        # clear\n        if commit:\n            print(f'{DATA_ROOT_PATH}\/{vid[:-4]}\/')\n            imgs = np.array([path.split('\/')[-1] for path in glob(f'{DATA_ROOT_PATH}\/{vid[:-4]}\/*.png')])\n        else:\n            # for testmode\n            imgs = np.array([path.split('\/')[-1] for path in glob(f'{DATA_ROOT_PATH}\/{vid[:-4]}*.png')])\n            \n        # Get predictions\n        if phase == 0:\n            end_df = predict(imgs, netend, dir=vid[:-4])\n            print(\"end len:\", len(end_df))\n        else:\n            side_df = predict(imgs, netside, dir=vid[:-4])\n            print(\"side len:\", len(side_df))\n    \n    final_side = side_df[['x', 'y', 'w', 'h', 'frame']].values\n    final_end = end_df[['x', 'y', 'w', 'h', 'frame']].values\n    \n    # Make submission for both Side and Ends\n    for phase, final_outs in enumerate([final_end, final_side]):\n        if phase == 0:\n            result_image_ids = []\n            for i in final_outs:\n                result_image_ids.append(end_df.loc[0].image_name)\n        else:\n            result_image_ids = []\n            for i in final_outs:\n                result_image_ids.append(side_df.loc[0].image_name)\n        print(final_outs.shape)   \n\n        # make df if final_out has boxes..\n        try:\n            box_df = pd.DataFrame(final_outs[:, 0:4].astype(int), columns=['left', 'top', 'width', 'height'])\n            test_df = pd.DataFrame({'image_name':result_image_ids})\n            test_df = pd.concat([test_df, box_df], axis=1)\n            test_df['frame'] = final_outs[:,4].astype(int)\n            print(test_df.head())\n\n            #gameKey,playID,view,video,frame,left,width,top,height\n            test_df['gameKey'] = test_df.image_name.str.split('_').str[0].astype(int)\n            test_df['playID'] = test_df.image_name.str.split('_').str[1].astype(int)\n            test_df['view'] = test_df.image_name.str.split('_').str[2]\n            test_df['video'] = test_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\n            test_df = test_df[[\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",\"left\",\"width\",\"top\",\"height\"]]\n            # concat\n            if (iii==0) and (phase==0):\n                submit_df=test_df\n            else:\n                submit_df=pd.concat([test_df, submit_df], axis=0)\n        except:\n            # no prediction found\n            print(\"boxes not found\")","6a178b47":"submit_df","5a5bc2d5":"def filter_ops(positive_df):\n    # New Post-Processing Function!\n    start = positive_df[\"frame\"].min()\n    end = positive_df[\"frame\"].max()\n    step = 30\n    target_iou = 0.25\n\n    boxes = positive_df[['x', 'y', 'w', 'h', 'frame']].values\n    boxes[:,2] = boxes[:,0] + boxes[:,2]\n    boxes[:,3] = boxes[:,1] + boxes[:,3]\n    keep = np.ones(len(boxes))\n    idx = np.array(range(len(boxes)))\n\n    outs = []\n    \n    # Time-nms\n    for t in np.arange(start, end, step):\n        keys = (boxes[:,4]>=t)*((boxes[:,4]<=t+step)*(keep==1))\n        loop_boxes_init = boxes[keys]\n        loop_boxes = loop_boxes_init\n        loop_idx = idx[keys]\n\n        for box in loop_boxes_init:\n            # Find box that overlaps with box\n            hits = find_matching_box(loop_boxes, box, loop_idx, target_iou)\n            keep[hits] = 0\n            outs.append(hits)\n            # Remove the hit boxes from loop_boxes\n            keys = (boxes[:,4]>=t)*((boxes[:,4]<=t+step)*(keep==1))\n            loop_boxes = boxes[keys]\n            loop_idx = idx[keys]\n\n    # filter outputs by nms\n    choose_centerframe = False\n    final_outs = []\n    for out in outs:\n        if len(out)>=num_thresh: # threshold of detection num!\n            box = boxes[out]\n            maxscore = np.median(box[:,4])\n            box = box[box[:,4]==maxscore]\n            o = np.array(np.median(box, axis=0))\n            # TODO: \u6700\u3082\u30b9\u30b3\u30a2\u304c\u9ad8\u3044\u3082\u306e\u3092\u9078\u629e\n            final_outs.append(o)\n    return np.array(final_outs)","eae155d4":"def threshold_opt(preds_df, thresh, nmax=25, nmin=15):\n    videos = preds_df.video.unique()\n    for i,video in enumerate(videos):\n        dfs = preds_df[preds_df[\"video\"]==video].reset_index()\n        # Inference\n        val_dataset = HeadClassificationDataset(dfs, image_dir=Path(\"\"), img_size=128, transforms=get_valid_transforms(128), transforms2=get_valid_transforms(96))\n        val_loader = torchdata.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n        targets = []\n        count = 0\n        for img_128, img_96 in val_loader:\n            img_128 = img_128.to(\"cuda\")\n            img_96 = img_96.to(\"cuda\")\n            tmp = []\n            for model in models_128:\n                target = (torch.sigmoid(model(img_128)) + torch.sigmoid(model(torch.flip(img_128, [4])))) \/ 2\n                tmp.append(target.cpu().detach().numpy())\n\n            for model in models_96:\n                target = (torch.sigmoid(model(img_96)) + torch.sigmoid(model(torch.flip(img_96, [4])))) \/ 2\n                tmp.append(target.cpu().detach().numpy())\n\n            targets.append(np.mean(tmp, axis=0))\n            count += 1\n        # to array\n        ts = []\n        for t in targets:\n            ts.append(t[0])\n        targets = np.array(ts).reshape(-1)\n\n        # threshold loop\n        run_th = thresh\n        numbox = 0\n        tries = 0\n        while (numbox>nmax or numbox<nmin) and tries<=20:\n            try:\n                positive_df = dfs[targets>run_th]\n                pred = positive_df[positive_df[\"video\"]==video]\n                final_outs = filter_ops(pred)\n                numbox = len(final_outs)\n            except:\n                pass\n            if numbox>nmax:\n                run_th += 0.02\n            elif numbox<nmin:\n                run_th -= 0.02\n            tries += 1\n            \n        final_outs[:,2] = final_outs[:,2] - final_outs[:,0]\n        final_outs[:,3] = final_outs[:,3] - final_outs[:,1]\n        # to dataframe\n        box_df = pd.DataFrame(final_outs[:, 0:4].astype(int), columns=['left', 'top', 'width', 'height'])\n        test_df = pd.DataFrame({'gameKey':dfs.gameKey[:len(box_df)], \"playID\":dfs.playID[:len(box_df)], \"view\":dfs.view[:len(box_df)], \"video\":dfs.video[:len(box_df)]})\n        test_df = pd.concat([test_df, box_df], axis=1)\n        test_df['frame'] = final_outs[:,4].astype(int)\n\n        #gameKey,playID,view,video,frame,left,width,top,height\n        test_df = test_df[[\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",\"left\",\"width\",\"top\",\"height\"]]\n\n        if i == 0:\n            out_df = test_df\n        else:\n            out_df = pd.concat([out_df, test_df], axis=0)\n    return out_df","a900e307":"import torch.utils.data as torchdata\nfrom pathlib import Path\nclass HeadClassificationDataset(torchdata.Dataset):\n    def __init__(self, df: pd.DataFrame, image_dir: Path, img_size=128, transforms=None, transforms2=None):\n        self.df = df\n        self.image_dir = image_dir\n        self.transforms = transforms\n        self.transforms2 = transforms2\n        self.img_size = img_size\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx: int):\n        sample = self.df.loc[idx, :]\n        x, y, w, h,frame = sample.x, sample.y, sample.w, sample.h, sample.frame\n        #x = x + (np.random.randn(1)).astype(int)\n        #y = y + (np.random.randn(1)).astype(int)\n        #w = w + (np.random.randn(1)*2).astype(int)\n        #h = h + (np.random.randn(1)*2).astype(int)\n        image_id = sample.video[:-4]\n        vid = sample.video\n        frame_idx = sample.frame\n        prefix = image_id\n        if commit:\n            image_dir = Path(f\"{DATA_ROOT_PATH}\/{vid[:-4]}\/\")\n        else:\n            # for testmode\n            image_dir = Path(f\"{DATA_ROOT_PATH}\")\n        try:\n            all_images = []\n            for frame_diff in [-4, -3, -2, -1, 0, 1, 2, 3, 4]:\n                image_id = prefix + '_' + str(frame_idx+frame_diff).zfill(3) + '.png'\n                image = cv2.imread(str(image_dir \/ image_id))\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0))\n                all_images.append(image)\n        except:\n            print(\"oops\")\n            all_images = []\n            for frame_diff in [-0, -0, -0, -0, 0, 0, 0, 0, 0]:                    \n                image_id = prefix + '_' + str(frame_idx+frame_diff).zfill(3) + '.png'\n                print(image_id)\n                image = cv2.imread(str(image_dir \/ image_id))\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0))\n                all_images.append(image)\n        x, y, w, h,frame = sample.x, sample.y, sample.w, sample.h, sample.frame\n        all_images = np.concatenate(all_images, axis=2)\n        woffset = (self.img_size - w) \/\/ 2\n        hoffset = (self.img_size - h) \/\/ 2\n        left = max(x - woffset, 0)\n        right = min(left + self.img_size, all_images.shape[1])\n        top = max(y - hoffset, 0)\n        bottom = min(top + self.img_size, all_images.shape[0])\n        #print(top)\n        #print(all_images.shape)\n        cropped = all_images[top:bottom, left:right, :].astype(np.float32)\n        cropped \/= 255.0\n        if self.transforms is not None:\n            cropped = self.transforms(image=cropped)[\"image\"]\n        cropped = cropped.view(9,3,self.img_size,self.img_size).transpose(1,0)\n        ### 96 IMAGES\n        woffset = (96 - w) \/\/ 2\n        hoffset = (96 - h) \/\/ 2\n        left = max(x - woffset, 0)\n        right = min(left + 96, all_images.shape[1])\n        top = max(y - hoffset, 0)\n        bottom = min(top + 96, all_images.shape[0])\n        #print(top)\n        #print(all_images.shape)\n        cropped2 = all_images[top:bottom, left:right, :].astype(np.float32)\n        cropped2 \/= 255.0\n        if self.transforms2 is not None:\n            cropped2 = self.transforms2(image=cropped2)[\"image\"]\n        cropped2 = cropped2.view(9, 3, 96, 96).transpose(1,0)\n        return cropped, cropped2\n\ndef get_valid_transforms(img_size=128):\n    return A.Compose([\n        A.Resize(height=img_size, width=img_size, p=1),\n        ToTensorV2(p=1.0)\n    ], p=1.0)","3ed66693":"submit_df[\"x\"] = submit_df[\"left\"]\nsubmit_df[\"y\"] = submit_df[\"top\"]\nsubmit_df[\"w\"] = submit_df[\"width\"]\nsubmit_df[\"h\"] = submit_df[\"height\"]\n\nsubmit_df = submit_df[submit_df[\"frame\"]>=10]","b9d0aac4":"submit_df2 = threshold_opt(submit_df, 0.5, nmax=25, nmin=15)\nprint(len(submit_df), len(submit_df2))\nsubmit_df2.reset_index(inplace=True, drop=True)\nsubmit_df2.head()","ca2a1fad":"submit_df2.head()","06e06a2b":"# clearing working dir\n# be careful when running this code on local environment!\n# !rm -rf *\n!mv * \/tmp\/","b4a89246":"import nflimpact\nenv = nflimpact.make_env()\nenv.predict(submit_df2) # df is a pandas dataframe of your entire submission file","3a1ddc76":"# 1st stage: Detection","5dce1b71":"## filter functions..","0c8df340":"## make classification models","f3043ce6":"# Kickoff","5bf86356":"# 2nd Stage: Classification","9053e52f":"## detector misc"}}