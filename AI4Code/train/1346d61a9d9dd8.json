{"cell_type":{"6f71fa60":"code","9f7798d3":"code","35adf86f":"code","eee628b3":"code","d21c5982":"code","600e4cd9":"code","7e8fd910":"code","5bb17d30":"code","9ed1c38a":"code","5604f56c":"code","c31845a7":"code","1fe22cc5":"code","da1740c2":"code","d5a39cd2":"code","426eef0d":"code","501ea38f":"code","5959fe53":"code","45f502dc":"code","1d460d32":"code","99f421a6":"code","874667de":"code","85171cae":"code","8887ee89":"code","452bb081":"code","68aba8bb":"code","acf5685c":"code","63e194e6":"code","c0b8b78c":"code","fea66fa9":"code","c294e192":"code","e2379f61":"code","d2eb5c41":"code","73c3f33b":"code","b1dd0dd0":"code","34d52fc4":"code","2444d988":"code","88fce235":"code","bb466b4a":"code","96f7165e":"code","fa5db1dc":"markdown","d58e95d9":"markdown","a9b64604":"markdown","ddf0b0df":"markdown","a828035a":"markdown","5fee31d5":"markdown","aaef5632":"markdown","5cedb76c":"markdown","66a61c4c":"markdown","a2d44193":"markdown","a1b4aa04":"markdown","1c5f8af2":"markdown","dddcc636":"markdown","3f79ff37":"markdown","93fde09c":"markdown","6eee42c7":"markdown","1a825d74":"markdown","6c4f1582":"markdown","0a7cd63e":"markdown","2309330c":"markdown","7526508b":"markdown","31061650":"markdown","2d424f6a":"markdown","22c78179":"markdown","513b2065":"markdown","fd0425f6":"markdown","5a6978b8":"markdown","d6718dfc":"markdown","054cd0e4":"markdown","5b1a258c":"markdown","48204d2e":"markdown","25a04e9e":"markdown","d753698a":"markdown","9def127f":"markdown","c335813a":"markdown"},"source":{"6f71fa60":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9f7798d3":"%load_ext autoreload\n%autoreload 2","35adf86f":"import pandas as pd\nimport spacy\nfrom spacy.displacy.render import EntityRenderer\nfrom IPython.core.display import display, HTML","eee628b3":"def custom_render(doc, df, column, options={}, page=False, minify=False, idx=0):\n    renderer, converter = EntityRenderer, parse_custom_ents\n    renderer = renderer(options=options)\n    parsed = [converter(doc, df=df, idx=idx, column=column)]\n    html = renderer.render(parsed, page=page, minify=minify).strip()  \n    return display(HTML(html))\n\ndef parse_custom_ents(doc, df, idx, column):\n    \n    if column in df.columns:\n        entities = df[column][idx]\n        ents = [{'start': ent[1], 'end': ent[2], 'label': ent[3]} \n               for ent in entities]\n    else:\n        ents = [{'start': ent.start_char, 'end': ent.end_char, 'label': ent.label_}\n                for ent in doc.ents]\n    return {'text': doc.text, 'ents': ents, 'title': None}\n\ndef render_entities(idx, df, options={}, column='named_ents'):\n    text = df['text'][idx]\n    custom_render(nlp(text), df=df, column=column, options=options, idx=idx)","d21c5982":"options = {\n    'colors': {'COMPOUND': '#FE6BFE', 'PROPN': '#18CFE6', 'NOUN': '#18CFE6', 'NP': '#1EECA6', 'ENTITY': '#FF8800'}\n}","600e4cd9":"pd.set_option('display.max_rows', 25)\npd.options.mode.chained_assignment = None","7e8fd910":"nlp = spacy.load('en_core_web_sm')","5bb17d30":"PATH = '..\/input\/'","9ed1c38a":"!ls {PATH}","5604f56c":"file = '\/synodtexts\/synod-documents.csv'\ndf = pd.read_csv(f'{PATH}{file}')\n\nmini_df = df[:25]\nmini_df.index = pd.RangeIndex(len(mini_df.index))\n\n# df = mini_df\n\ndisplay(df)","c31845a7":"lower = lambda x: x.lower()","1fe22cc5":"df = pd.DataFrame(df['text'].apply(lower))\ndf.columns = ['text']\ndisplay(df)","da1740c2":"def extract_named_ents(text):\n    return [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in nlp(text).ents]\n\ndef add_named_ents(df):\n    df['named_ents'] = df['text'].apply(extract_named_ents)\n    \nadd_named_ents(df)\ndisplay(df)","d5a39cd2":"column = 'named_ents'\nrender_entities(9, df, options=options, column=column)","426eef0d":"def extract_nouns(text):\n    keep_pos = ['PROPN', 'NOUN']\n    return [(tok.text, tok.idx, tok.idx+len(tok.text), tok.pos_) for tok in nlp(text) if tok.pos_ in keep_pos]\n\ndef add_nouns(df):\n    df['nouns'] = df['text'].apply(extract_nouns)\n\nadd_nouns(df)\ndisplay(df)","501ea38f":"column = 'nouns'\nrender_entities(0, df, options=options, column=column)","5959fe53":"def extract_named_nouns(row_series):\n    ents = set()\n    idxs = set()\n    # remove duplicates and merge two lists\n    for noun_tuple in row_series['nouns']:\n        for named_ents_tuple in row_series['named_ents']:\n            if noun_tuple[1] == named_ents_tuple[1]: \n                idxs.add(noun_tuple[1])\n                ents.add(named_ents_tuple)\n        if noun_tuple[1] not in idxs:\n            ents.add(noun_tuple)\n    \n    return sorted(list(ents), key=lambda x: x[1])\n\ndef add_named_nouns(df):\n    df['named_nouns'] = df.apply(extract_named_nouns, axis=1)","45f502dc":"add_named_nouns(df)\ndisplay(df)","1d460d32":"column = 'named_nouns'\nrender_entities(1, df, options=options, column=column)","99f421a6":"text = \"And I say to thee: That thou art Peter; and upon this rock I will build my church, and the gates of hell shall not prevail against it.\"\n\nspacy.displacy.render(nlp(text), jupyter=True)","874667de":"def extract_noun_phrases(text):\n    return [(chunk.text, chunk.start_char, chunk.end_char, chunk.label_) for chunk in nlp(text).noun_chunks]\n\ndef add_noun_phrases(df):\n    df['noun_phrases'] = df['text'].apply(extract_noun_phrases)","85171cae":"def visualize_noun_phrases(text):\n    df = pd.DataFrame([text]) \n    df.columns = ['text']\n    add_noun_phrases(df)\n    column = 'noun_phrases'\n    render_entities(0, df, options=options, column=column)\n\nvisualize_noun_phrases(text)","8887ee89":"add_noun_phrases(df)\ndisplay(df)","452bb081":"column = 'noun_phrases'\nrender_entities(0, df, options=options, column=column)","68aba8bb":"def extract_compounds(text):\n    \"\"\"Extract compound noun phrases with beginning and end idxs. \n    \n    Keyword arguments:\n    text -- the actual text source from which to extract entities\n    \n    \"\"\"\n    comp_idx = 0\n    compound = []\n    compound_nps = []\n    tok_idx = 0\n    for idx, tok in enumerate(nlp(text)):\n        if tok.dep_ == 'compound':\n\n            # capture hyphenated compounds\n            children = ''.join([c.text for c in tok.children])\n            if '-' in children:\n                compound.append(''.join([children, tok.text]))\n            else:\n                compound.append(tok.text)\n\n            # remember starting index of first child in compound or word\n            try:\n                tok_idx = [c for c in tok.children][0].idx\n            except IndexError:\n                if len(compound) == 1:\n                    tok_idx = tok.idx\n            comp_idx = tok.i\n\n        # append the last word in a compound phrase\n        if tok.i - comp_idx == 1:\n            compound.append(tok.text)\n            if len(compound) > 1: \n                compound = ' '.join(compound)\n                compound_nps.append((compound, tok_idx, tok_idx+len(compound), 'COMPOUND'))\n\n            # reset parameters\n            tok_idx = 0 \n            compound = []\n\n    return compound_nps\n\ndef add_compounds(df):\n    \"\"\"Create new column in data frame with compound noun phrases.\n    \n    Keyword arguments:\n    df -- a dataframe object\n    \n    \"\"\"\n    df['compounds'] = df['text'].apply(extract_compounds)","acf5685c":"add_compounds(df)\ndisplay(df)","63e194e6":"column = 'compounds'\nrender_entities(0, df, options=options, column=column)","c0b8b78c":"def extract_comp_nouns(row_series, cols=[]):\n    return {noun_tuple[0] for col in cols for noun_tuple in row_series[col]}\n\ndef add_comp_nouns(df, cols=[]):\n    df['comp_nouns'] = df.apply(extract_comp_nouns, axis=1, cols=cols)\n\ncols = ['nouns', 'compounds']\nadd_comp_nouns(df, cols=cols)\ndisplay(df)","fea66fa9":"# take a look at all the nouns again\ncolumn = 'named_nouns'\nrender_entities(0, df, options=options, column=column)","c294e192":"# take a look at all the compound noun phrases again\ncolumn = 'compounds'\nrender_entities(0, df, options=options, column=column)","e2379f61":"# take a look at combined entities\ndf['comp_nouns'][0] ","d2eb5c41":"def drop_duplicate_np_splits(ents):\n    drop_ents = set()\n    for ent in ents:\n        if len(ent.split(' ')) > 1:\n            for e in ent.split(' '):\n                if e in ents:\n                    drop_ents.add(e)\n    return ents - drop_ents\n\ndef drop_single_char_nps(ents):\n    return {' '.join([e for e in ent.split(' ') if not len(e) == 1]) for ent in ents}\n\ndef drop_double_char(ents):\n    drop_ents = {ent for ent in ents if len(ent) < 3}\n    return ents - drop_ents\n\ndef keep_alpha(ents):\n    keep_char = set('-abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n    drop_ents = {ent for ent in ents if not set(ent).issubset(keep_char)}\n    return ents - drop_ents","73c3f33b":"!ls {PATH}","b1dd0dd0":"filename = '.\/topfreqwords\/freq_words.csv'\nfreq_words_df = pd.read_csv(f'{PATH}{filename}')\ndisplay(freq_words_df)","34d52fc4":"freq_words = freq_words_df['Word'].iloc[1:]\ndisplay(freq_words)","2444d988":"def remove_freq_words(ents):\n    freq_words = pd.read_csv('..\/input\/topfreqwords\/freq_words.csv')['Word'].iloc[1:]\n    for word in freq_words:\n        try:\n            ents.remove(word)\n        except KeyError:\n            continue # ignore the stop word if it's not in the list of abstract entities\n    return ents\n\ndef add_clean_ents(df, funcs=[]):\n    col = 'clean_ents'\n    df[col] = df['comp_nouns']\n    for f in funcs:\n        df[col] = df[col].apply(f)","88fce235":"funcs = [drop_duplicate_np_splits, drop_double_char, keep_alpha, drop_single_char_nps, remove_freq_words]\nadd_clean_ents(df, funcs)\ndisplay(df)","bb466b4a":"def visualize_entities(df, idx=0):\n    # store entity start and end index for visualization in dummy df\n    ents = []\n    abstract = df['text'][idx]\n    for ent in df['clean_ents'][idx]:\n        i = abstract.find(ent) # locate the index of the entity in the abstract\n        ents.append((ent, i, i+len(ent), 'ENTITY')) \n    ents.sort(key=lambda tup: tup[1])\n\n    dummy_df = pd.DataFrame([abstract, ents]).T # transpose dataframe\n    dummy_df.columns = ['text', 'clean_ents']\n    column = 'clean_ents'\n    render_entities(0, dummy_df, options=options, column=column)","96f7165e":"visualize_entities(df, 0)","fa5db1dc":"## Make Everything Lowercase, Only Display 'text' Column","d58e95d9":"## View Nouns","a9b64604":"## Add Compounds to Dataframe & Display","ddf0b0df":"## View Nouns Again","a828035a":"## View Named Entities in Europe Document","5fee31d5":"## Style Tags","aaef5632":"## View Most Frequent Words in English Language","5cedb76c":"## Gather Synod Data Table","66a61c4c":"## Extract Named Entities, Add New Column, Display Dataframe","a2d44193":"## Import Libraries","a1b4aa04":"## Edit Pandas Dataframes","1c5f8af2":"## View Noun Phrases Again","dddcc636":"## Add Combination to Dataframe and Display","3f79ff37":"## View Stopword Frequency Training Data","93fde09c":"## Add Noun Phrases to Dataframe & Display","6eee42c7":"## Combine Named Entities & Nouns","1a825d74":"## Visualize New Entities in Document","6c4f1582":"## Extract Compound Noun Phrases","0a7cd63e":"## Drop Enities in Most Common Words, Create New Column With Remaining Entities","2309330c":"## Add Cleaned Entities to Dataframe & Display","7526508b":"## View Noun Phrases in Europe Document","31061650":"## Verify Path of Document & Stopword Tables","2d424f6a":"## Load spaCy Pre-trained Language Model","22c78179":"## Combine Entities & Compound Noun Phrases, Display Dataframe","513b2065":"## Load spaCy Rendering for Custom Part-of-Speech (POS) Tags ","fd0425f6":"### Conclusion\n\nspaCy shows remarkable accuracy in discovering \"inside baseball\" language within documents. It accurately identified many of the ridiculous phrases central to the philosophy of the Church since the Second Vatican Council. Sillyness aside, it also identified several key theological terms and phrases you would have to be a theology geek like me to appreciate.\n\nIt did all of this with zero domain-specific training.\n\nGranted, the results are still a bit noisy. However, we could add our own training data and  overcome this in just a few lines of code.\n\n**That said, I cannot stress this enough -- we achieved the results above \"out-of-the-box\"!**\n\nIf it can do this with the overly obtuse language of Church documents, imagine what it can do with your industry.","5a6978b8":"## Combine and Visualize Noun Phrases in Sentence","d6718dfc":"## Display Dataframe","054cd0e4":"## View Combined Entities","5b1a258c":"## View Compounds in Document","48204d2e":"## Add Heuristics to Reduce Entity Count","25a04e9e":"## Extract All Nouns, Add Column, Display New Dataframe","d753698a":"## Import Auto Reload","9def127f":"## Show How this Works with Single Sentence","c335813a":"## View Named Nouns in Document"}}