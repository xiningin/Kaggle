{"cell_type":{"258c9b55":"code","145ea81e":"code","7e063fc6":"code","a9460051":"code","280bde27":"code","9e6116c3":"code","ac25257a":"code","ac4aace6":"code","4892040a":"code","6fecc334":"code","23a35c32":"code","ecc13a2e":"code","89fff40d":"code","078fc30f":"code","f074c0b0":"code","407d404d":"code","dadfb801":"code","b1bb1e1f":"code","4982d637":"code","3a910995":"code","fb6407f6":"code","b7ed4015":"code","a99969ec":"code","df585a2d":"code","d8592e59":"code","33dd078a":"code","07a6aae6":"code","9e47381b":"code","3bd5b41b":"code","0ada8686":"code","865959d7":"code","430282c3":"code","c4bb68ec":"code","1e71e9b3":"code","28027818":"code","e519b398":"code","65635f76":"code","5de83b4e":"code","c4c159b9":"code","783d4aa2":"markdown","1dbc0203":"markdown","e5c8f2c5":"markdown","a6474b38":"markdown","b4c1112d":"markdown","798b2ad5":"markdown","189269f7":"markdown","6eab7268":"markdown","9005dc26":"markdown","3477165d":"markdown","0cc479fa":"markdown","7b731905":"markdown"},"source":{"258c9b55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","145ea81e":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom spacy import displacy\nfrom spacy.util import minibatch, compounding\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","7e063fc6":"df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv')\ndf.shape ","a9460051":"df.head()","280bde27":"# Import label encoder\nfrom sklearn import preprocessing\n\nlabel_encoder = preprocessing.LabelEncoder()\ndf['Sentiment']= label_encoder.fit_transform(df['Sentiment'])\n  \ndf['Sentiment'].unique()","9e6116c3":"df.head()","ac25257a":"df.isnull().sum()","ac4aace6":"ax = df.Sentiment.value_counts().plot(kind='bar')\nfig = ax.get_figure()","4892040a":"df = df[['OriginalTweet','Sentiment']].dropna()\ndf.head()","6fecc334":"df.shape","23a35c32":"spacy_tok = spacy.load('en_core_web_sm') #English Language model for tokenization!\nsample_review=df.OriginalTweet[32]\nsample_review","ecc13a2e":"parsed_review = spacy_tok(sample_review)\nparsed_review","89fff40d":"!wget https:\/\/raw.githubusercontent.com\/tylerneylon\/explacy\/master\/explacy.py","078fc30f":"import explacy\nexplacy.print_parse_info(spacy_tok, 'Covid-19 has various symtoms') #Text for demonstration","f074c0b0":"explacy.print_parse_info(spacy_tok, 'India has help various countries with Covid-19 Resources') #Text for demonstration","407d404d":"explacy.print_parse_info(spacy_tok,df.OriginalTweet[2])","dadfb801":"tokenized_text = pd.DataFrame()\n\nfor i, token in enumerate(parsed_review):\n    tokenized_text.loc[i, 'text'] = token.text\n    tokenized_text.loc[i, 'lemma'] = token.lemma_,\n    tokenized_text.loc[i, 'pos'] = token.pos_\n    tokenized_text.loc[i, 'tag'] = token.tag_\n    tokenized_text.loc[i, 'dep'] = token.dep_\n    tokenized_text.loc[i, 'shape'] = token.shape_\n    tokenized_text.loc[i, 'is_alpha'] = token.is_alpha\n    tokenized_text.loc[i, 'is_stop'] = token.is_stop\n    tokenized_text.loc[i, 'is_punctuation'] = token.is_punct\n\ntokenized_text[:20]","b1bb1e1f":"from IPython.display import Image\nImage(\"https:\/\/d33wubrfki0l68.cloudfront.net\/00d54115351b0e18776433853e794b76b59ee97c\/eab3d\/static\/d0575562cdedb47340c00662c5c1b859\/80132\/example.png\")","4982d637":"spacy.displacy.render(parsed_review, style='ent', jupyter=True)","3a910995":"spacy.explain('GPE') # to explain POS tag","fb6407f6":"sentence_spans = list(parsed_review.sents)\nsentence_spans","b7ed4015":"displacy.render(parsed_review, style='dep', jupyter=True,options={'distance': 140})","a99969ec":"options = {'compact': True, 'bg': 'white','distance': 140,\n           'color': 'blue', 'font': 'Trebuchet MS'}\ndisplacy.render(parsed_review, jupyter=True, style='dep', options=options)","df585a2d":"spacy.explain(\"ADJ\") ,spacy.explain(\"det\") ,spacy.explain(\"ADP\") ,spacy.explain(\"prep\")  \n#just to understand what does the tag means!","d8592e59":"noun_chunks_df = pd.DataFrame()\n\nfor i, chunk in enumerate(parsed_review.noun_chunks):\n    noun_chunks_df.loc[i, 'text'] = chunk.text\n    noun_chunks_df.loc[i, 'root'] = chunk.root,\n    noun_chunks_df.loc[i, 'root.text'] = chunk.root.text,\n    noun_chunks_df.loc[i, 'root.dep_'] = chunk.root.dep_\n    noun_chunks_df.loc[i, 'root.head.text'] = chunk.root.head.text\n\nnoun_chunks_df[:20]","33dd078a":"!pip install scattertext\nimport scattertext as st\nnlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])","07a6aae6":"print(df.shape)","9e47381b":"df.head()","3bd5b41b":"nlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\ndf['parsed'] = df.OriginalTweet.apply(nlp)\ncorpus = st.CorpusFromParsedDocuments(df,\n                             category_col='Sentiment',\n                             parsed_col='parsed').build()","0ada8686":"df.head()","865959d7":"!pip install sense2vec==1.0.0a0","430282c3":"#Prepare data:\n#Let's prepare the data as SpaCy would like it. It accepts list of tuples of text and labels.","c4bb68ec":"df['tuples'] = df.apply(\n    lambda row: (row['OriginalTweet'],row['Sentiment']), axis=1)\ntrain = df['tuples'].tolist()\ntrain[:5]","1e71e9b3":"#functions from spacy documentation\ndef load_data(limit=0, split=0.8):\n    train_data = train\n    np.random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    texts, labels = zip(*train_data)\n    cats = [{'POSITIVE': bool(y)} for y in labels]\n    split = int(len(train_data) * split)\n    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n\ndef evaluate(tokenizer, textcat, texts, cats):\n    docs = (tokenizer(text) for text in texts)\n    tp = 1e-8  # True positives\n    fp = 1e-8  # False positives\n    fn = 1e-8  # False negatives\n    tn = 1e-8  # True negatives\n    for i, doc in enumerate(textcat.pipe(docs)):\n        gold = cats[i]\n        for label, score in doc.cats.items():\n            if label not in gold:\n                continue\n            if score >= 0.5 and gold[label] >= 0.5:\n                tp += 1.\n            elif score >= 0.5 and gold[label] < 0.5:\n                fp += 1.\n            elif score < 0.5 and gold[label] < 0.5:\n                tn += 1\n            elif score < 0.5 and gold[label] >= 0.5:\n                fn += 1\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    f_score = 2 * (precision * recall) \/ (precision + recall)\n    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n\n#(\"Number of texts to train from\",\"t\" , int)\nn_texts=30000\n#You can increase texts count if you have more computational power.\n\n#(\"Number of training iterations\", \"n\", int))\nn_iter=10","28027818":"nlp = spacy.load('en_core_web_sm')  # create english Language class","e519b398":"# add the text classifier to the pipeline if it doesn't exist\n# nlp.create_pipe works for built-ins that are registered with spaCy\nif 'textcat' not in nlp.pipe_names:\n    textcat = nlp.create_pipe('textcat')\n    nlp.add_pipe(textcat, last=True)\n# otherwise, get it, so we can add labels to it\nelse:\n    textcat = nlp.get_pipe('textcat')\n\n# add label to text classifier\ntextcat.add_label('POSITIVE')\n\n# load the dataset\nprint(\"Loading Covid Tweets data...\")\n(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\nprint(\"Using {} examples ({} training, {} evaluation)\"\n      .format(n_texts, len(train_texts), len(dev_texts)))\ntrain_data = list(zip(train_texts,\n                      [{'cats': cats} for cats in train_cats]))","65635f76":"# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\nwith nlp.disable_pipes(*other_pipes):  # only train textcat\n    optimizer = nlp.begin_training()\n    print(\"Training the model...\")\n    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n    for i in range(n_iter):\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n                       losses=losses)\n        with textcat.model.use_params(optimizer.averages):\n            # evaluate on the dev data split off in load_data()\n            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n              .format(losses['textcat'], scores['textcat_p'],\n                      scores['textcat_r'], scores['textcat_f']))","5de83b4e":"# test the trained model\ntest_text1 = \"Life is worth living than just existing.\"\ntest_text2=\"A strip about Covid that is funny, smart and has good art.\"\ndoc = nlp(test_text1)\ntest_text1, doc.cats","c4c159b9":"doc2 = nlp(test_text2)\ntest_text2, doc2.cats","783d4aa2":"### Thank you! Hope this Notebook was helpful, Kindly Up-Vote if it helped you in any ways!","1dbc0203":"#### Sence2vec\nThe idea is get something better than word2vec model.\n\nIt assight parts of speech tags like verb, noun , adjective to words, which will in turn be used to make sence of context.\n\nPlease book [VERB] my ticket.\nRead the book [NOUN].","e5c8f2c5":"#### Tokenization:\n* First step in any nlp pipeline is tokenizing text i.e breaking down paragraphs into sentenses and then sentenses into words, punctuations and so on.\n\n* we will load english language model to tokenize our english text.\n\n* Every language is different and have different rules. Spacy offers 8 different language models.","a6474b38":"#### Before starting with the approach I would really thank Miss.Poonam, She had worked previously for Food reviews dataset by Amazon, in which she had given a beautiful tutorial for SpaCy!\n\n#### I have used her [notebook](https:\/\/www.kaggle.com\/poonaml\/text-classification-using-spacy) as a reference for my approach regarding Covid-19 Text Classification Approach, dealing with Sentiments ranging from \"Extreamly Negative\"(Score: 0) to \"Extreamly Positive\"(Score: 5).","b4c1112d":"#### Dependency parsing:\nSyntactic Parsing or Dependency Parsing is process of identifyig sentenses and assigning a syntactic structure to it. As in Subject combined with object makes a sentence. Spacy provides parse tree which can be used to generate this structure.\n\n#### Sentense Boundry Detection:\nFiguring out where sentense starts and ends is very imporatnt part of nlp.","798b2ad5":"* We are going to tackle an interesting natural language processing problem i.e text classification. We will explore texual data using amazing spaCy library and build a text classification model.","189269f7":"* As you can see that the sentiments range from 0-100%, where the above text has positivity score of 0.99% which suggests that the tweet is quiet positive!","6eab7268":"Named Entity Recognition (NER)\nNamed entity Recognition automatically identifies named entities in a text and classifies them into predefined categories. Entities can be names of people, organizations, locations, times, quantities, monetary values, percentages, and more.\n\n\nSpacy figures out below entities automatically:","9005dc26":"There is not much difference between parsed review and original one. But we will see ahead what has actually happened. We can see how parsing has been done visually through explacy","3477165d":"#### SpaCy Text Categorizer\nWe will train a multi-label convolutional neural network text classifier on our food reviews, using spaCy's new TextCategorizer component.\n\nSpaCy provides classification model with multiple, non-mutually exclusive labels. You can change the model architecture rather easily, but by default, the TextCategorizer class uses a convolutional neural network to assign position-sensitive vectors to each word in the document. The TextCategorizer uses its own CNN model, to avoid sharing weights with the other pipeline components","0cc479fa":"Here, Since we are only concered with Sentiment and Tweet we will be extracting those columns seperately in our dataframe, also drop null values(if any!)","7b731905":"Part-of-speech tagging\nAfter tokenization we can parse and tag variety of parts of speech to paragraph text. SpaCy uses statistical models in background to predict which tag will go for each word(s) based on the context.\n\nLemmatization\nIt is the process of extracting uninflected\/base form of the word. Lemma can be like For eg.\n\nAdjectives: best, better \u2192 good Adverbs: worse, worst \u2192 badly Nouns: ducks, children \u2192 duck, child Verbs: standing,stood \u2192 stand"}}