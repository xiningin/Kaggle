{"cell_type":{"662a35b8":"code","09440514":"code","4be3f030":"code","b9e52daa":"code","41d82f4e":"code","a4590aba":"code","0cba4731":"code","13dd89db":"code","0933261d":"code","06e02a28":"code","8724991a":"code","7cfcc0ea":"code","76ee9133":"code","c5f96020":"code","0d7ef173":"code","1fa30bab":"code","653977ba":"code","7248604f":"markdown","4306d76e":"markdown","0e3401c3":"markdown","3cca32f5":"markdown","921729c4":"markdown","dae32b41":"markdown","4ce4127c":"markdown","5d97849d":"markdown","c94bf150":"markdown"},"source":{"662a35b8":"import os, sys, shutil\nimport time\nimport gc\nfrom contextlib import contextmanager\nfrom pathlib import Path\nimport random\nimport numpy as np, pandas as pd\nfrom tqdm import tqdm, tqdm_notebook\n\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.preprocessing import LabelBinarizer\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\n\nfrom matplotlib import pyplot as plt\n%config InlineBackend.figure_formats = ['retina']","09440514":"MAX_SEQUENCE_LENGTH = 512    # maximal possible sequence length is 512. Generally, the higher, the better, but more GPU memory consumed\nBATCH_SIZE = 16              # refer to the table here https:\/\/github.com\/google-research\/bert to adjust batch size to seq length\n\nSEED = 1234\nEPOCHS = 2                                   \nPATH_TO_DATA = Path(\"..\/input\/clickbait-news-detection\/\")\nWORK_DIR = Path(\"..\/working\/\")\n\nLRATE = 2e-5             # hard to tune with BERT, but this shall be fine (improvements: LR schedules, fast.ai wrapper for LR adjustment)\nACCUM_STEPS = 2          # wait for several backward steps, then one optimization step, this allows to use larger batch size\n                         # well explained here https:\/\/medium.com\/huggingface\/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255\nWARMUP = 5e-2            # warmup helps to tackle instability in the initial phase of training with large learning rates. \n                         # During warmup, learning rate is gradually increased from 0 to LRATE.\n                         # WARMUP is a proportion of total weight updates for which warmup is done. By default, it's linear warmup\nUSE_APEX = True         # using APEX shall speedup training (here we use mixed precision training), https:\/\/github.com\/NVIDIA\/apex","4be3f030":"# nice way to report running times\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')","b9e52daa":"# make results fully reproducible\ndef seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","41d82f4e":"if USE_APEX:\n    with timer('install Nvidia apex'):\n        # Installing Nvidia Apex\n        os.system('git clone https:\/\/github.com\/NVIDIA\/apex; cd apex; pip install -v --no-cache-dir' + \n                  ' --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/')\n        os.system('rm -rf apex\/.git') # too many files, Kaggle fails\n        from apex import amp","a4590aba":"device = torch.device('cuda')","0cba4731":"!wget -q https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip\n!unzip uncased_L-12_H-768_A-12.zip\nBERT_MODEL_PATH = Path('uncased_L-12_H-768_A-12\/')","13dd89db":"# Add the Bert Pytorch repo to the PATH using files from: https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\npackage_dir_a = \"..\/input\/ppbert\/pytorch-pretrained-bert\/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam, BertConfig\n\n# Translate model from tensorflow to pytorch\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    str(BERT_MODEL_PATH \/ 'bert_model.ckpt'),\n    str(BERT_MODEL_PATH \/ 'bert_config.json'),\n    str(WORK_DIR \/ 'pytorch_model.bin')\n)\n\nshutil.copyfile(BERT_MODEL_PATH \/ 'bert_config.json', WORK_DIR \/ 'bert_config.json')\nbert_config = BertConfig(str(BERT_MODEL_PATH \/ 'bert_config.json'))","0933261d":"# Converting the lines to BERT format\ndef convert_lines(example, max_seq_length, tokenizer):\n    max_seq_length -= 2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a) > max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"]) + [0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(f\"There are {longer} lines longer than {max_seq_length}\")\n    return np.array(all_tokens)","06e02a28":"with timer('Read data and convert to BERT format'):\n    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,\n                                              do_lower_case=True)\n    train_df = pd.read_csv(PATH_TO_DATA \/ \"train.csv\")\n\n    valid_df = pd.read_csv(PATH_TO_DATA \/ \"valid.csv\")\n    \n    train_val_df = pd.concat([train_df, valid_df])\n    \n    test_df = pd.read_csv(PATH_TO_DATA \/ \"test.csv\")\n    \n    print('loaded {} train + validation and {} test records'.format(\n        len(train_val_df), len(test_df)))\n\n    # Make sure all text values are strings\n    train_val_df['text'] = train_val_df['title'].astype(str).fillna(\"DUMMY_VALUE\") + '_' + train_val_df['text'].astype(str).fillna(\"DUMMY_VALUE\") \n    test_df['text'] = test_df['title'].astype(str).fillna(\"DUMMY_VALUE\") + '_' + test_df['text'].astype(str).fillna(\"DUMMY_VALUE\") \n    \n    X_train = convert_lines(train_val_df[\"text\"], MAX_SEQUENCE_LENGTH, tokenizer)\n    X_test = convert_lines(test_df[\"text\"], MAX_SEQUENCE_LENGTH, tokenizer)\n    \n    label_binarizer = LabelBinarizer()\n    y_train = label_binarizer.fit_transform(train_val_df['label'])\n    \n    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.long), \n                                               torch.tensor(y_train, dtype=torch.float))\n    test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n    \n#     del train_val_df, train_df, valid_df, test_df; gc.collect()","8724991a":"with timer('Setting up BERT'):\n    output_model_file = \"bert_pytorch.bin\"\n    seed_everything(SEED)\n    model = BertForSequenceClassification.from_pretrained(WORK_DIR,\n                                                          cache_dir=None,\n                                                          num_labels=y_train.shape[1])\n    model.zero_grad()\n    model = model.to(device)\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n         'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n         'weight_decay': 0.0}\n        ]\n\n    num_train_optimization_steps = int(EPOCHS * len(train_dataset) \/ BATCH_SIZE \/ ACCUM_STEPS)\n\n    optimizer = BertAdam(optimizer_grouped_parameters,\n                         lr=LRATE, warmup=WARMUP,\n                         t_total=num_train_optimization_steps)\n    if USE_APEX:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n    model = model.train()","7cfcc0ea":"with timer('Training'):\n    loss_history = []\n    tq = tqdm_notebook(range(EPOCHS))\n    for epoch in tq:\n        train_loader = torch.utils.data.DataLoader(train_dataset, \n                                                   batch_size=BATCH_SIZE, \n                                                   shuffle=True)\n        avg_loss = 0.\n        lossf = None\n        tk0 = tqdm_notebook(enumerate(train_loader), total=len(train_loader), leave=False)\n        optimizer.zero_grad()   \n        for i,(x_batch, y_batch) in tk0:\n            y_pred = model(x_batch.to(device), \n                           attention_mask=(x_batch>0).to(device), labels=None)\n            loss = nn.BCEWithLogitsLoss()(y_pred, y_batch.to(device))\n            if USE_APEX:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            if (i+1) % ACCUM_STEPS == 0:                     # Wait for several backward steps\n                optimizer.step()                             # Now we can do an optimizer step\n                optimizer.zero_grad()\n            if lossf:\n                lossf = 0.98 * lossf + 0.02 * loss.item()\n            else:\n                lossf = loss.item()\n            tk0.set_postfix(loss = lossf)\n            avg_loss += loss.item() \/ len(train_loader)\n            \n            loss_history.append(loss.item())\n            \n        tq.set_postfix(avg_loss=avg_loss)\n        \n# torch.save(model.state_dict(), output_model_file)","76ee9133":"plt.plot(range(len(loss_history)), loss_history);\nplt.ylabel('Loss'); plt.xlabel('Batch number');","c5f96020":"!nvidia-smi","0d7ef173":"with timer('Test predictions'):\n    # The following 3 lines are not needed but show how to download the model for prediction\n#     model = BertForSequenceClassification(bert_config, num_labels=y_train.shape[1])\n#     model.load_state_dict(torch.load(output_model_file))\n#     model.to(device)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.eval();\n    \n    test_pred_probs = np.zeros((len(X_test), y_train.shape[1]))\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n    for i,(x_batch,) in enumerate(tqdm_notebook(test_loader)):\n        pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n        test_pred_probs[i*BATCH_SIZE:(i+1)*BATCH_SIZE] = pred.detach().cpu().squeeze().numpy()","1fa30bab":"test_preds = label_binarizer.inverse_transform(test_pred_probs)\n\nsub_df = pd.read_csv(PATH_TO_DATA \/ 'sample_submission.csv', index_col='id')\nsub_df['label'] = test_preds \nsub_df.to_csv('submission.csv')","653977ba":"!head submission.csv","7248604f":"#### Creating a submission file","4306d76e":"#### Read and convert data \nKudos to [this Kernel](https:\/\/www.kaggle.com\/httpwwwfszyc\/bert-in-keras-taming). ","0e3401c3":"#### Download BERT model and convert it to PyTorch format","3cca32f5":"#### Test predictions","921729c4":"#### Settings","dae32b41":"#### Setting up BERT","4ce4127c":"#### Helper functions","5d97849d":"Here we walk through PyTorch BERT fine-tuning for classification, mostly based on [this Kernel](https:\/\/www.kaggle.com\/yuval6967\/toxic-bert-plain-vanila). \nSome comments on the used heurisitcs (batch accumulation, warmup) are added. \n\n#### Imports","c94bf150":"#### Training"}}