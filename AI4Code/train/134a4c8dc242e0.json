{"cell_type":{"301be98d":"code","1c27f2ff":"code","ee8e6585":"code","0814e23d":"code","68b6261f":"code","37f7e76e":"code","e12210c7":"code","5625fbb1":"code","24fa9dd6":"code","06a19dcb":"code","3a072b00":"code","a2b1a5f4":"code","d8741dd7":"code","7a321af1":"code","50a1988b":"code","15998540":"code","79e63304":"code","173c2098":"code","0dc22d4f":"code","e6a8b90d":"code","2d1ae672":"code","f8060665":"code","3e5173c0":"code","3e2d472f":"code","293fdffa":"code","169abccd":"code","304ed80f":"code","7ededf4c":"code","588eb572":"code","e414f956":"code","920623e7":"code","e9880875":"code","84940938":"code","a0e8c876":"code","fcf88f36":"code","1260c357":"code","e0d83575":"code","1b066021":"code","84d3004e":"code","a0f47d00":"code","1cf1128b":"code","6d0d75a0":"code","84f7b76d":"code","4d9cadc9":"code","f32db792":"code","c2b5fd8f":"code","c1717e58":"code","59bfc5e0":"code","df1e6713":"code","df130440":"code","ed1abb7a":"markdown","9fbdb651":"markdown","4dbf49ab":"markdown","dbb8a256":"markdown","c14ac48d":"markdown","54ad32ed":"markdown","df568ce9":"markdown","2cdf3e5d":"markdown","d2d39a0f":"markdown","9366feba":"markdown","1420b687":"markdown","c35dbf90":"markdown","eb834b1f":"markdown","ee968141":"markdown","b3f73f3a":"markdown","ec68a09c":"markdown"},"source":{"301be98d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom xgboost import XGBRegressor\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c27f2ff":"# Importing the data \nitem = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshop = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nsales_train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntestd = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsampl = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","ee8e6585":"sampl.head()","0814e23d":"# seeing teh basic structure of the dta in teh frame \ndata = [item,shop,sales_train,testd,item_categories,sampl]\nfor i in data:\n    print(i.info())\n    print('\\n')","68b6261f":"#Finding Any Null values\nsales_train.isna().sum()","37f7e76e":"# Some Anylysis Seeing Tools Imported\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import classification_report, confusion_matrix","e12210c7":"#Distribution Of sales Vs Shop Analysis in details and seeing how shop perform\nsns.set(rc={'figure.figsize':(40, 40)})\nsns.set_context(\"talk\", font_scale=1)\nsales_month_shop_id = pd.DataFrame(sales_train.groupby(['shop_id']).sum().item_cnt_day).reset_index()\nsales_month_shop_id.columns = ['shop_id', 'sum_sales']\nsns.barplot(x ='shop_id', y='sum_sales', data=sales_month_shop_id, palette='Paired')\nplt.title('Distribution of sales per shop');\ndel sales_month_shop_id","5625fbb1":"#Seeing Items with sales analysis\nsales_item_id = pd.DataFrame(sales_train.groupby(['item_id']).sum().item_cnt_day)\nplt.xlabel('item id')\nplt.ylabel('sales')\nplt.plot(sales_item_id)","24fa9dd6":"#Seeing the real max item and its name and its other info\nanom_item = sales_item_id.item_cnt_day.argmax()\nprint(anom_item)\nitem[item['item_id'] == 20602]","06a19dcb":"# we will try to plot how does the items matches\nsns.set(style = \"dark\")\nplt.plot(sales_train['item_id'], sales_train['item_price'], '*', color='Green');","3a072b00":"sales_train[sales_train['item_price'] > 50000]","a2b1a5f4":"print(item[item['item_id'] == 6066])\nprint(item[item['item_id'] == 11365])\nprint(item[item['item_id'] == 13199])","d8741dd7":"print(item_categories[item_categories['item_category_id'] == 75])\nprint(item_categories[item_categories['item_category_id'] == 9])\nprint(item_categories[item_categories['item_category_id'] == 69])","7a321af1":"print(shop[shop['shop_id'] == 12])\nprint(shop[shop['shop_id'] == 25])","50a1988b":"sales_train_sub = sales_train\nsales_train_sub['month'] = pd.DatetimeIndex(sales_train_sub['date']).month\nsales_train_sub['year'] = pd.DatetimeIndex(sales_train_sub['date']).year\nsales_train_sub.head(10)","15998540":"monthly_sales=sales_train_sub.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"item_cnt_day\"].agg(item_cnt_day = 'sum')\n\nmonthly_sales['date_block_num'] = monthly_sales.index.get_level_values('date_block_num') \nmonthly_sales['shop_id'] = monthly_sales.index.get_level_values('shop_id') \nmonthly_sales['item_id'] = monthly_sales.index.get_level_values('item_id') \nmonthly_sales.reset_index(drop=True, inplace=True)\n\nmonthly_sales = monthly_sales.reindex(['date_block_num','shop_id','item_id','item_cnt_day'], axis=1)\nmonthly_sales.head(10)","79e63304":"fig = plt.figure(figsize=(18,8))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ntestd['shop_id'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Shop ID Values in the Test Set (Normalized)')\n\nplt.subplot2grid((3,3), (1,0))\ntestd['item_id'].plot(kind='hist', alpha=0.7)\nplt.title('Item ID Histogram - Test Set')\n\nplt.show()","173c2098":"# Remove outliers\nsales_train = sales_train[sales_train.item_price <= 100000]\nsales_train = sales_train[sales_train.item_cnt_day <= 1000]\n\n# Adjusting negatice prices (change it for median values)\nmedian = sales_train[(sales_train.shop_id == 32) & (sales_train.item_id == 2973) & (sales_train.date_block_num == 4) & (sales_train.item_price > 0)].item_price.median()\nsales_train.loc[sales_train.item_price < 0, 'item_price'] = median","0dc22d4f":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nsales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\ntestd.loc[testd.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nsales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\ntestd.loc[testd.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nsales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\ntestd.loc[testd.shop_id == 10, 'shop_id'] = 11\n# \u0420\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443 \u0422\u0420\u041a \"\u041c\u0435\u0433\u0430\u0446\u0435\u043d\u0442\u0440 \u0413\u043e\u0440\u0438\u0437\u043e\u043d\u0442\"\nsales_train.loc[sales_train.shop_id == 39, 'shop_id'] = 40\ntestd.loc[testd.shop_id == 39, 'shop_id'] = 40","e6a8b90d":"shop.shop_name.unique()","2d1ae672":"shop.loc[shop.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshop['shop_category'] = shop['shop_name'].str.split(' ').map(lambda x:x[1]).astype(str)\ncategories = ['\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435,', '\u0422\u0426', '\u0422\u0420\u041a', '\u0422\u0420\u0426','\u0443\u043b.', '\u041c\u0430\u0433\u0430\u0437\u0438\u043d', '\u0422\u041a', '\u0441\u043a\u043b\u0430\u0434']\nshop.shop_category = shop.shop_category.apply(lambda x: x if (x in categories) else 'etc')\nshop.shop_category.unique()","f8060665":"shop.groupby(['shop_category']).sum()","3e5173c0":"from sklearn.preprocessing import LabelEncoder\ncategory = ['\u0422\u0426', '\u0422\u0420\u041a', '\u0422\u0420\u0426', '\u0422\u041a']\nshop.shop_category = shop.shop_category.apply(lambda x: x if (x in category) else 'etc')\nprint('Category Distribution', shop.groupby(['shop_category']).sum())\n\nshop['shop_category_code'] = LabelEncoder().fit_transform(shop['shop_category'])","3e2d472f":"shop['city'] = shop['shop_name'].str.split(' ').map(lambda x: x[0])\nshop.loc[shop.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshop['city_code'] = LabelEncoder().fit_transform(shop['city'])\nshop = shop[['shop_id','city_code', 'shop_category_code']]\n\nshop.head()","293fdffa":"print(len(item_categories.item_category_name.unique()))\nitem_categories.item_category_name.unique()","169abccd":"item_categories['type'] = item_categories.item_category_name.apply(lambda x: x.split(' ')[0]).astype(str)\nitem_categories.loc[(item_categories.type == '\u0418\u0433\u0440\u043e\u0432\u044b\u0435') | (item_categories.type == '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b'), 'category'] = '\u0418\u0433\u0440\u044b'\nitem_categories.loc[item_categories.type == 'PC', 'category'] = '\u041c\u0443\u0437\u044b\u043a\u0430'\ncategory = ['\u0418\u0433\u0440\u044b', '\u041a\u0430\u0440\u0442\u044b', '\u041a\u0438\u043d\u043e', '\u041a\u043d\u0438\u0433\u0438','\u041c\u0443\u0437\u044b\u043a\u0430', '\u041f\u043e\u0434\u0430\u0440\u043a\u0438', '\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b', '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435', '\u0427\u0438\u0441\u0442\u044b\u0435', '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b']\nitem_categories['type'] = item_categories.type.apply(lambda x: x if (x in category) else 'etc')\nprint(item_categories.groupby(['type']).sum())\nitem_categories['type_code'] = LabelEncoder().fit_transform(item_categories['type'])\n\n# if subtype is nan then type\nitem_categories['split'] = item_categories.item_category_name.apply(lambda x: x.split('-'))\nitem_categories['subtype'] = item_categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_categories['subtype_code'] = LabelEncoder().fit_transform(item_categories['subtype'])\nitem_categories = item_categories[['item_category_id','type_code', 'subtype_code']]\n\nitem_categories.head()","304ed80f":"sales_train['date']","7ededf4c":"sales_train['date'] = pd.to_datetime(sales_train['date'], format='%d.%m.%Y')\nsales_train['month'] = sales_train['date'].dt.month\nsales_train['year'] = sales_train['date'].dt.year\nsales_train = sales_train.drop(columns=['date'])\n\n# sales.head()\nto_append = testd[['shop_id', 'item_id']].copy()\n\nto_append['date_block_num'] = sales_train['date_block_num'].max() + 1\nto_append['year'] = 2015\nto_append['month'] = 11\nto_append['item_cnt_day'] = 0\nto_append['item_price'] = 0\n\nsales_train = pd.concat([sales_train, to_append], ignore_index=True, sort=False)\nsales_train.head()","588eb572":"period = sales_train[['date_block_num', 'year', 'month']].drop_duplicates().reset_index(drop=True)\nperiod['days'] = period.apply(lambda r: monthrange(r.year, r.month)[1], axis=1)\n\nsales_train = sales_train.drop(columns=['month', 'year'])\n\nperiod.head()\n","e414f956":"from itertools import product\nindex_cols = ['date_block_num', 'shop_id', 'item_id']\ngrid = [] \nfor block_num in sales_train['date_block_num'].unique():\n    cur_shops = sales_train.loc[sales_train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales_train.loc[sales_train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[[block_num], cur_shops, cur_items])), dtype='int16'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype = np.int16)\ngrid.head()","920623e7":"data = pd.merge(grid, shop, on='shop_id')\ndata = pd.merge(data, item, on='item_id')\ndata = pd.merge(data, item_categories, on='item_category_id')\ndata = pd.merge(data, period, on='date_block_num')\n","e9880875":"data","84940938":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom calendar import monthrange\nfrom itertools import product\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\n%matplotlib inline","a0e8c876":"type(data)","fcf88f36":"data1 = data[['date_block_num', 'year', 'month','days']]# 'item_price', 'item_cnt_day'\n","1260c357":"# Adjusting columns order\ndata = data[['date_block_num', 'year', 'month', 'days', 'city_code', 'shop_category_code', 'shop_id', 'item_category_id', 'type_code', 'subtype_code', 'item_id']] # 'item_price', 'item_cnt_day'\n\n# Downcasting values\nfor c in ['date_block_num', 'month', 'days', 'city_code', 'shop_category_code', 'shop_id', 'item_category_id', 'type_code', 'subtype_code']:\n    data[c] = data[c].astype(np.int8)\ndata['item_id'] = data['item_id'].astype(np.int16)\ndata['year'] = data['year'].astype(np.int16)\n\n# Remove unused and temporary datasets\ndel grid, shop, item, item_categories, to_append\n\ndata.head()","e0d83575":"aux = sales_train\\\n.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)\\\n.agg({'item_cnt_day' : 'sum', 'item_price' : 'mean'})\\\n.rename(columns= {'item_cnt_day' : 'item_cnt_month', 'item_price' : 'item_price_month'})\n\naux['item_cnt_month'] = aux['item_cnt_month'].astype(np.float16)\naux['item_price_month'] = aux['item_price_month'].astype(np.float16)\n\nmonth_summary = pd.merge(data, aux, how='left', on=['date_block_num', 'shop_id', 'item_id'])\\\n    .fillna(0.0).sort_values(by=['shop_id', 'item_id', 'date_block_num'])\n\ndel data, aux\n\nmonth_summary.head()","1b066021":"print('Min: {} and Max: {} item_cnt_month values'.format(month_summary['item_cnt_month'].min(), month_summary['item_cnt_month'].max()))","84d3004e":"month_summary['item_cnt_month'] = month_summary['item_cnt_month'].clip(0,20)","a0f47d00":"def agg_by(month_summary, group_cols, new_col, target_col = 'item_cnt_month', agg_func = 'mean'):\n    aux = month_summary\\\n        .groupby(group_cols, as_index=False)\\\n        .agg({target_col : agg_func})\\\n        .rename(columns= {target_col : new_col})\n    aux[new_col] = aux[new_col].astype(np.float16)\n\n    return pd.merge(month_summary, aux, how='left', on=group_cols)\n\ndef lag_feature(df, col, lags=[1,2,3,6,12]):\n    tmp = df[['date_block_num','shop_id','item_id', col]]\n    for i in lags:\n        shifted = tmp.copy()\n        cols = ['date_block_num','shop_id','item_id', '{}_lag_{}'.format(col, i)]\n        shifted.columns = cols\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(value={(cols[-1]) : 0.0})\n    return df\n\ndef agg_by_and_lag(month_summary, group_cols, new_col, lags=[1,2,3,6,12], target_col = 'item_cnt_month', agg_func = 'mean'):\n    tmp = agg_by(month_summary, group_cols, new_col, target_col, agg_func)\n    tmp = lag_feature(tmp, new_col, lags)\n    return tmp.drop(columns=[new_col])","1cf1128b":"month_summary = agg_by_and_lag(month_summary, ['date_block_num'], 'date_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_id'], 'date_item_avg_item_cnt', [1,2,3,6,12])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code'], 'date_city_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id'], 'date_shop_avg_item_cnt', [1,2,3,6,12])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_category_id'], 'date_cat_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'type_code'], 'date_type_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'subtype_code'], 'date_subtype_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code'], 'date_shop_category_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'item_category_id'], 'date_shop_cat_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'type_code'], 'date_shop_type_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'subtype_code'], 'date_shop_subtype_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code', 'subtype_code'], 'date_shop_category_subtype_avg_item_cnt', [1])\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code', 'item_id'], 'date_item_city_avg_item_cnt', [1])","6d0d75a0":"month_summary = agg_by_and_lag(month_summary, ['date_block_num'], 'date_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_id'], 'date_item_avg_item_price', [1,2,3,6,12], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code'], 'date_city_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id'], 'date_shop_avg_item_price', [1,2,3,6,12], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_category_id'], 'date_cat_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'type_code'], 'date_type_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'subtype_code'], 'date_subtype_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code'], 'date_shop_category_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'item_category_id'], 'date_shop_cat_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'type_code'], 'date_shop_type_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'subtype_code'], 'date_shop_subtype_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code', 'subtype_code'], 'date_shop_category_subtype_avg_item_price', [1], 'item_price_month')\n\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code', 'item_id'], 'date_item_city_avg_item_price', [1], 'item_price_month')","84f7b76d":"month_summary['item_shop_first_sale'] = month_summary['date_block_num'] - month_summary.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmonth_summary['item_first_sale'] = month_summary['date_block_num'] - month_summary.groupby('item_id')['date_block_num'].transform('min')","4d9cadc9":"month_summary.to_pickle('month_summary.pkl')\nmonth_summary.info()","f32db792":"month_summary = pd.read_pickle('month_summary.pkl')","c2b5fd8f":"def generate_subsample(month_summary, target='item_cnt_month'):\n    X_test = month_summary[month_summary['date_block_num'] == 34]\n    X_test = X_test.drop(columns=[target])\n\n    X_val = month_summary[month_summary['date_block_num'] == 33]\n    y_val = X_val[target]\n    X_val = X_val.drop(columns=[target])\n\n    X_train = month_summary[(month_summary['date_block_num'] >= 12) & (month_summary['date_block_num'] < 33)]\n    y_train = X_train[target]\n    X_train = X_train.drop(columns=[target])\n\n    return X_train, y_train, X_val, y_val, X_test","c1717e58":"X_train, y_train, X_val, y_val, X_test = generate_subsample(month_summary.drop(columns=['item_price_month']), 'item_cnt_month')\n\ndel month_summary","59bfc5e0":"def train_gbmodel(X_train, y_train, X_val, y_val):\n\n    RAND_SEED = 42\n\n    lgb_params = {'num_leaves': 2**8, 'max_depth': 19, 'max_bin': 107, #'n_estimators': 3747,\n              'bagging_freq': 1, 'bagging_fraction': 0.7135681370918421, \n              'feature_fraction': 0.49446461478601994, 'min_data_in_leaf': 2**8, # 88\n              'learning_rate': 0.015980721586917768, 'num_threads': 2, \n              'min_sum_hessian_in_leaf': 6,\n              'random_state' : RAND_SEED,\n              'bagging_seed' : RAND_SEED,\n              'boost_from_average' : 'true',\n              'boost' : 'gbdt',\n              'metric' : 'rmse',\n              'verbose' : 1}\n\n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_val = lgb.Dataset(X_val, label=y_val)\n\n    return lgb.train(lgb_params, lgb_train, \n                      num_boost_round=300,\n                      valid_sets=[lgb_train, lgb_val],\n                      early_stopping_rounds=20)","df1e6713":"# model_old_item = train_gbmodel(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]).clip(0, 20), X_val, y_val.clip(0, 20))\ngbm_model = train_gbmodel(X_train, y_train, X_val, y_val)\n\ny_hat = gbm_model.predict(X_val).clip(0, 20)\nprint(np.sqrt(mean_squared_error(y_val.clip(0, 20), y_hat)))\n\nwith open('.\/gbm_model.pickle', 'wb') as handle:\n    pickle.dump(gbm_model, handle)","df130440":"y_pred = gbm_model.predict(X_test).clip(0, 20)\n\nresult = pd.merge(testd, X_test.assign(item_cnt_month=y_pred), how='left', on=['shop_id', 'item_id'])[['ID', 'item_cnt_month']]\nresult.to_csv('submission.csv', index=False)","ed1abb7a":"# Data Preprossing","9fbdb651":"checcking essentials\n","4dbf49ab":"# features","dbb8a256":"# Shop Data Preprossesing","c14ac48d":"# Train Moodel\n","54ad32ed":"We Have Now seen how the outliers and what dtata they really hold","df568ce9":"# Date Data","2cdf3e5d":"# Train and test data","d2d39a0f":"# **EDA BEing done Very importnat**","9366feba":"# Mean Encoding","1420b687":"# Remove the outliers\n","c35dbf90":"We See three but two are very close to most so can be ignored and can even be taken ","eb834b1f":"# Split data","ee968141":"#  **Checking For Any Outliers IF Any**","b3f73f3a":"**Make The Data Being Used and Initialistation**","ec68a09c":"We will see that there are not many outliers but a few that is very harmful in the algo"}}