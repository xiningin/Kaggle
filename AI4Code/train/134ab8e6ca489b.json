{"cell_type":{"267975ad":"code","cfccaf2a":"code","fceea538":"code","26a0a122":"code","3c46afb7":"code","ea9cc1ce":"code","d460bca5":"code","17f9ef55":"code","984fd2f5":"code","0762a212":"code","636831dc":"code","1bed9fa8":"code","ffd32eed":"code","9eab08ae":"code","1c407545":"code","b5dd6353":"code","1ab1f32e":"code","9552e194":"code","124c5fb7":"code","ef51dff3":"code","6af6590f":"code","06f85a25":"code","378dbbce":"code","d95fa3f1":"code","aa8e5da3":"code","ce5c3649":"code","bb9118b4":"code","d1df8154":"code","7425f75e":"code","b7ff6b23":"code","0ddf8163":"code","4357787b":"code","6cf991fb":"code","6718a07a":"code","4c2c147d":"code","ff26dcae":"code","b042b6aa":"code","7dd4726d":"code","61f8d513":"code","e25a5708":"code","92c0678d":"code","0f3fda07":"code","12ed0741":"markdown","325863fe":"markdown","81f9aa14":"markdown","ea6036eb":"markdown","c7e3d2f4":"markdown","8abaf307":"markdown","fcdc957b":"markdown","34c13ab1":"markdown","333fc10d":"markdown","523e31d7":"markdown","d7bf39c4":"markdown","f2f5c19c":"markdown","d743cb73":"markdown","27d69eb2":"markdown","9974b163":"markdown"},"source":{"267975ad":"import numpy as np\nimport pandas as pd\n\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.graph_objects as go\n\nfrom sklearn.model_selection import cross_val_score","cfccaf2a":"tweets_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntweets_train","fceea538":"tweets_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntweets_test","26a0a122":"#tweets = pd.concat([tweets_train, tweets_test])\n#tweets = tweets[['keyword', 'location', 'text', 'target']].copy()\n#tweets.reset_index(inplace=True)\n#tweets","3c46afb7":"# tweets.info()","ea9cc1ce":"# I am going to replace Location with 1 or 0\n\nfor tweets in [tweets_train, tweets_test]:\n    tweets['location'] = tweets['location'].apply(lambda x : 0 if x == None else 1)","d460bca5":"# I am going to replace Keyword with 1 or 0\n\nfor tweets in [tweets_train, tweets_test]:\n    tweets['keyword'] = tweets['keyword'].apply(lambda x : 0 if x == None else 1)","17f9ef55":"# Replace urls with 'url' code or just delete them\n\nfor tweets in [tweets_train, tweets_test]:\n    tweets['text'] = tweets['text'].str.replace('http\\S+|www.\\S+', 'url', regex=True, case=False)","984fd2f5":"# Delete html tags\n\nfor tweets in [tweets_train, tweets_test]:\n    tweets['text'] = tweets['text'].str.replace('<.*?>', '', regex=True, case=False)","0762a212":"# Replace mentions with 'mention' code or just delete them\n\nfor tweets in [tweets_train, tweets_test]:\n    tweets['text'] = tweets['text'].str.replace('@[a-zA-Z0-9_]*', '', regex=True, case=False)","636831dc":"# I am going to keep the hashtags for now\n\n#for tweets in [tweets_train, tweets_test]:\n#    tweets.loc[:, 'text'] = tweets['text'].str.replace('#[a-zA-Z0-9_]*', '', regex=True, case=False)","1bed9fa8":"# Now we are going to keep only letters and spaces\n\nfor tweets in [tweets_train, tweets_test]:\n    tweets['text'] = tweets['text'].str.replace('[^a-zA-Z ]', '', regex=True)","ffd32eed":"from nltk.tokenize import word_tokenize\n\nfor tweets in [tweets_train, tweets_test]:\n    tweets['text_words'] = tweets['text'].apply(lambda x : word_tokenize(x))","9eab08ae":"!pip install pyspellchecker","1c407545":"# Lets count the number of spelling errors on each tweet\n\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\n\nfor tweets in [tweets_train, tweets_test]:\n    tweets['spelling_mistakes'] = tweets['text_words'].apply(lambda x : len(spell.unknown(x)))","b5dd6353":"# Lets count the length of each tweet\n\nfor tweets in [tweets_train, tweets_test]:\n    tweets['tweet_size'] = tweets['text'].apply(lambda x : len(x))","1ab1f32e":"# Lets count the number of CAPITAL LETTERS\n\nfor tweets in [tweets_train, tweets_test]:\n    tweets['tweet_caps'] = tweets['text'].apply(lambda x : sum(1 for c in x if c.isupper()))","9552e194":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(lowercase=True, stop_words='english', token_pattern='[a-zA-Z]{3,15}')\nvectorizer = vectorizer.fit(tweets_train['text'])","124c5fb7":"bag_of_words_train = vectorizer.transform(tweets_train['text'])\n\nbag_of_words_test = vectorizer.transform(tweets_test['text'])","ef51dff3":"tweets_train['spelling_mistakes'] = tweets_train['spelling_mistakes'] \/ tweets_train['spelling_mistakes'].max()\ntweets_train['tweet_size'] = tweets_train['tweet_size'] \/ tweets_train['tweet_size'].max()\ntweets_train['tweet_caps'] = tweets_train['tweet_caps'] \/ tweets_train['tweet_caps'].max()","6af6590f":"tweets_test['spelling_mistakes'] = tweets_test['spelling_mistakes'] \/ tweets_test['spelling_mistakes'].max()\ntweets_test['tweet_size'] = tweets_test['tweet_size'] \/ tweets_test['tweet_size'].max()\ntweets_test['tweet_caps'] = tweets_test['tweet_caps'] \/ tweets_test['tweet_caps'].max()","06f85a25":"from scipy import sparse\n\nfor col in ['keyword', 'location', 'spelling_mistakes', 'tweet_size', 'tweet_caps']:\n    \n    bag_of_words_train = sparse.hstack((\n        bag_of_words_train, \n        np.array(tweets_train[col]).reshape(-1, 1)\n    ))\n    \n    bag_of_words_test = sparse.hstack((\n        bag_of_words_test, \n        np.array(tweets_test[col]).reshape(-1, 1)\n    ))","378dbbce":"X_train = bag_of_words_train.toarray()\ny_train = tweets_train['target']\n\nX_val = bag_of_words_test.toarray()","d95fa3f1":"#from sklearn.preprocessing import MinMaxScaler\n\n#scaler = MinMaxScaler()\n\n#X_train = scaler.fit_transform(X_train)\n#X_val = scaler.transform(X_val)","aa8e5da3":"from wordcloud import WordCloud","ce5c3649":"# Disaster WordCloud\n'''\ndisaster_sum_words = X_train.loc[:,'abc':].sum(axis=0)\ndisaster_sum_words.sort_values(ascending=False)\n\nwordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", repeat=False)\nwordcloud.generate_from_frequencies(disaster_sum_words)\n\nplt.figure(figsize = (10, 10))\nplt.title(\"Top words in disaster tweets\")\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n'''","bb9118b4":"# Non-Disaster WordCloud\n'''\ndisaster_sum_words = X_val.loc[:,'abc':].sum(axis=0)\ndisaster_sum_words.sort_values(ascending=False)\n\nwordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", repeat=False)\nwordcloud.generate_from_frequencies(disaster_sum_words)\n\nplt.figure(figsize = (10, 10))\nplt.title(\"Top words in non-disaster tweets\")\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n'''","d1df8154":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(max_iter=5000, C=10)\n\n# check the accuracy with cross validation\nscores = cross_val_score(lr_model, X_train, y_train, cv=5)\n\n# now lets train our final model\nlr_model.fit(X_train, y_train)\n\nfinal_score = lr_model.score(X_train, y_train)\n\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","7425f75e":"from sklearn.naive_bayes import GaussianNB\n\nnbmodel = GaussianNB()\n\n# check the accuracy with cross validation\nscores = cross_val_score(nbmodel, X_train, y_train, cv=5)\n\n# now lets train our final model\nnbmodel.fit(X_train, y_train)\n\nfinal_score = nbmodel.score(X_train, y_train)\n\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","b7ff6b23":"from sklearn.neighbors import KNeighborsClassifier\n\nk_values = [x for x in range(1, 20, 2)]\nscores = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    scores.append(knn.score(X_train, y_train))\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=k_values, y=scores, mode='lines+markers'))\nfig.update_layout(xaxis_title=\"knn score\", yaxis_title=\"k\")\nfig.show()","0ddf8163":"# define the model\nknn_model = KNeighborsClassifier(n_neighbors=5)\n\n# check the accuracy with cross validation\nscores = cross_val_score(knn_model, X_train, y_train, cv=5)\n\n# now lets train our final model\nknn_model.fit(X_train, y_train)\n\nfinal_score = knn_model.score(X_train, y_train)\n\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","4357787b":"from sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n\nrfc_model.fit(X_train, y_train)\n\nscores = cross_val_score(rfc_model, X_train, y_train, cv=5)\n\nfinal_score = rfc_model.score(X_train, y_train)\n\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","6cf991fb":"from sklearn.naive_bayes import MultinomialNB\n\n# bayes ingenuo multinomiales\nmnb_model = MultinomialNB()\n\nmnb_model.fit(X_train, y_train)\n\nscores = cross_val_score(mnb_model, X_train, y_train, cv=5)\n\nfinal_score = mnb_model.score(X_train, y_train)\n\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","6718a07a":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","4c2c147d":"X_train.shape[1]","ff26dcae":"input_shape = X_train.shape[1]\n\n# definicion del modelo\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=(input_shape)),\n        layers.Dense(128, activation=\"relu\"),\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\n\n# Construir el modelo y ver la arquitectura\nmodel.build(input_shape)\nmodel.summary()","b042b6aa":"# Compilation\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","7dd4726d":"# ejecutar training\nhistory = model.fit(X_train, y_train, epochs=10, verbose=1, validation_split=0.4)","61f8d513":"predictions = model.predict(X_val)\npredictions","e25a5708":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nsample_submission['target'] = predictions\nsample_submission['target'] = sample_submission['target'].apply(lambda x : 1 if x > .8 else 0)\n\nsample_submission.to_csv(\"sample_submission.csv\", index=False)","92c0678d":"sample_submission.head()","0f3fda07":"sample_submission","12ed0741":"# Predictions","325863fe":"## 3. KNN","81f9aa14":"## 1. Logistic Regression","ea6036eb":"# Load data","c7e3d2f4":"# Vectorizer","8abaf307":"# Word Tokenizer","fcdc957b":"## 3. Random Forest","34c13ab1":"## 2. Naive Bayes","333fc10d":"# Training and Validation Datasets","523e31d7":"## 4. Multinomial NB","d7bf39c4":"# WordClouds","f2f5c19c":"# Models","d743cb73":"# Normalization","27d69eb2":"# Additional features","9974b163":"## 6. Neural Network"}}