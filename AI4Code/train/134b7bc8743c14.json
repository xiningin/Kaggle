{"cell_type":{"ef3a4941":"code","28d52f67":"code","c0851a59":"code","a7e8e2d4":"code","62ef5e6c":"code","9743d93b":"code","9dfc48cc":"code","08546a52":"code","e9d0aee8":"code","f39f7609":"code","e7811961":"code","4cd34223":"code","1a687963":"code","f01c105b":"code","d9ce3172":"code","73a8fbac":"code","4ef33a32":"code","924d270a":"code","f058ef6b":"code","ff0848f5":"code","3d856e70":"code","b650d37d":"code","9e1192e2":"code","6b02c3ae":"code","6cf2ed06":"code","f5b356a4":"code","fa39511d":"code","6e4f4cf2":"code","2199a73c":"code","57568d8c":"code","55fe1737":"code","85588c41":"code","605eb330":"code","045b3f69":"code","7fcdfec7":"code","a470f434":"code","ce40d274":"code","49b2359f":"code","b6695a0b":"code","2a6b0b28":"code","900918b8":"code","cd314e4e":"code","741b6397":"code","6cbdb080":"code","72e21356":"code","5f573c63":"code","af489d35":"code","d3a34f1e":"code","7010967d":"code","37566783":"code","1cb58568":"code","a6257e68":"code","cadcd860":"code","a9f8f71d":"code","deeddd29":"code","fd4cec7e":"code","c09492c0":"code","155ca10b":"code","346f049f":"code","e0358ba9":"markdown","07e5c50b":"markdown","bf6299a9":"markdown","fb3cd9bf":"markdown","297d3e35":"markdown","89d3ff54":"markdown","5ee85ba2":"markdown","cb36cb00":"markdown","6823ed87":"markdown","7d70d930":"markdown","0e40e3d9":"markdown","63ddcf5b":"markdown","774dc2e7":"markdown","09157f78":"markdown","60bbe224":"markdown","5d5d996b":"markdown","f423cc14":"markdown","876114dc":"markdown","d726e16b":"markdown","dbf4409a":"markdown","4dd9607d":"markdown","40fa06d0":"markdown","1194659f":"markdown","5be3cdf0":"markdown","5f39e21d":"markdown","da99e37f":"markdown","02ee81b9":"markdown","3c4c5f88":"markdown","7c404635":"markdown","837910f2":"markdown","9a797ef0":"markdown","b10d4ed9":"markdown","1dbed399":"markdown","355bf49b":"markdown","b8566df0":"markdown","cc539149":"markdown","8687541e":"markdown","d9b84712":"markdown","31b82d02":"markdown","e01cd567":"markdown","e4f55441":"markdown","20334f7e":"markdown","b12428b8":"markdown","eae5f454":"markdown","66794ff9":"markdown","b57340b7":"markdown","be658c5a":"markdown","a97261b6":"markdown","49566113":"markdown","a50e8f02":"markdown","3b7ef71d":"markdown","2bfa6e0e":"markdown","e3459b28":"markdown"},"source":{"ef3a4941":"# Generic Python Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nimport missingno as msno\nimport warnings\n# import mlflow\nimport time\nfrom pathlib import Path\n\n# ML Model Libraries\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict\nfrom sklearn.preprocessing import LabelBinarizer\nimport xgboost as xgb","28d52f67":"pd.options.mode.chained_assignment = None  # default='warn'\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", message=\"Variables are collinear\")","c0851a59":"data_path = Path(\"..\/input\")\n\n# Raw Data\ndf_train = pd.read_csv(data_path \/ \"train.csv\")\ndf_test = pd.read_csv(data_path \/ 'test.csv')\ndf_gender_sub = pd.read_csv(data_path \/ 'gender_submission.csv')\n\nlist_of_df = [df_train, df_test]\nlist_of_df_names = [\"Train\", \"Test\"]","a7e8e2d4":"all_features = df_train.drop('Survived', axis='columns')\nfinal_test = df_test\nTarget_feature = df_train.loc[:,'Survived']\n\n# Train Test Split for in notebook testing without submitting results to Kaggle\nX_train, X_test, y_train, y_test = train_test_split(all_features,Target_feature,test_size=0.3,random_state=42)","62ef5e6c":"for i in range(2):\n    print(\"The \" + list_of_df_names[i] + \" dataset has a shape of: \", list_of_df[i].shape)","9743d93b":"df_train.head(3)","9dfc48cc":"df_test.head(3)","08546a52":"print(\"Train\")\nprint(20*'-')\nprint(df_train.isnull().sum())","e9d0aee8":"msno.matrix(df_train.sample(250))\nmsno.bar(df_train)","f39f7609":"print(\"Train\")\nprint('-'*40)\nprint(df_train.info())","e7811961":"# Continuous Data Plot\ndef cont_plot(df, feature_name, target_name, palettemap, hue_order, feature_scale): \n    df['Counts'] = \"\" # A trick to skip using an axis (either x or y) on splitting violinplot\n    fig, [axis0,axis1] = plt.subplots(1,2,figsize=(10,5))\n    sns.distplot(df[feature_name], ax=axis0)\n    axis0.set_xlim(left=0)\n    sns.violinplot(\n        x=feature_name, \n        y=\"Counts\", \n        hue=target_name, \n        hue_order=hue_order, \n        data=df,    \n        palette=palettemap, \n        split=True, \n        orient='h', \n        ax=axis1\n    )\n    axis1.set_xlim(left=0)\n    axis1.set_xticks(feature_scale)\n    plt.show()\n    df.drop([\"Counts\"], axis=\"columns\")\n\n\n# Categorical\/Ordinal Data Plot\ndef cat_plot(df, feature_name, target_name, palettemap): \n    fig, [axis0,axis1] = plt.subplots(1,2,figsize=(10,5))\n    df[feature_name].value_counts().plot.pie(autopct='%1.1f%%',ax=axis0)\n    sns.countplot(\n        x=feature_name, \n        hue=target_name, \n        data=df,\n        palette=palettemap,\n        ax=axis1\n    )\n    plt.show()\n\n\nsurvival_palette = {0: \"red\", 1: \"green\"}  # Color map for visualization","4cd34223":"df_train.loc[:,['Pclass', 'Survived']].groupby('Pclass', as_index=False).mean().sort_values(by='Survived', ascending=False)","1a687963":"cat_plot(df_train, 'Pclass','Survived', survival_palette)","f01c105b":"df_train.loc[:,['Sex', 'Survived']].groupby('Sex', as_index=False).mean().sort_values(by='Survived', ascending=False)","d9ce3172":"cat_plot(df_train, 'Sex','Survived', survival_palette)","73a8fbac":"cont_plot(df_train.loc[:,['Age','Survived']].dropna(axis=0), 'Age', 'Survived', survival_palette, [1, 0], range(0,100,10))","4ef33a32":"df_train.loc[:,['SibSp', 'Survived']].groupby('SibSp', as_index=False).mean().sort_values(by='Survived', ascending=False)","924d270a":"cat_plot(df_train, 'SibSp','Survived', survival_palette)","f058ef6b":"df_train.loc[:,['Parch', 'Survived']].groupby('Parch', as_index=False).mean().sort_values(by='Survived', ascending=False)","ff0848f5":"cat_plot(df_train, 'Parch','Survived', survival_palette)","3d856e70":"cont_plot(df_train.loc[:,['Fare','Survived']].dropna(axis=0), 'Fare', 'Survived', survival_palette, [1, 0], range(0,601,100))","b650d37d":"df_train['Counts'] = \"\"\nfig, axis = plt.subplots(1,1,figsize=(10,5))\nsns.violinplot(x='Fare', y=\"Counts\", hue='Survived', hue_order=[1, 0], data=df_train,\n               palette=survival_palette, split=True, orient='h', ax=axis)\naxis.set_xticks(range(0,100,10))\naxis.set_xlim(0,100)\nplt.show()\ndf_train = df_train.drop([\"Counts\"], axis=\"columns\")","9e1192e2":"df_train.loc[:,['Embarked', 'Survived']].groupby('Embarked', as_index=False).mean().sort_values(by='Survived', ascending=False)","6b02c3ae":"cat_plot(df_train, 'Embarked','Survived', survival_palette)","6cf2ed06":"df_train.Ticket.head(20)","f5b356a4":"df_train.Cabin.head(20)","fa39511d":"colormap = plt.cm.viridis\nsns.heatmap(df_train.corr(),\n            linewidths=0.1, \n            vmax=1.0, \n            square=True, \n            cmap=colormap, \n            linecolor='white', \n            annot=True)\nplt.show()","6e4f4cf2":"titles = set()\nfor name in df_train['Name']:\n    # This takes each name and splits them into two lists, separating the surnames from the rest of the name.\n    # Then the rest of the name is selected using list indexing and split into two lists. This time separating the honorific from the rest of the name.\n    # The honorific is selected using list indexing and whitespace is stripped, resulting the cleaned honorifics.\n    titles.add(name.split(',')[1].split('.')[0].strip())\nprint(titles)","2199a73c":"class HonorificExtractor(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    Custom SK-learn transformer.\n    Extracts honorifics from a string type column and groups them further into:\n    Mr, Miss, Mrs, Master, Scholar, Religious, Officer and Noble.\n    NaN is assumed to be Mr.\n    \"\"\"\n    \n    def __init__(self, column):\n        self.column = column\n    \n    title_dictionary = {\n        \"Mr\": \"Mr\",\n        \"Miss\": \"Miss\",\n        \"Mrs\": \"Mrs\",\n        \"Master\": \"Master\",\n        \"Dr\": \"Scholar\",\n        \"Rev\": \"Religious\",\n        \"Col\": \"Officer\",\n        \"Major\": \"Officer\",\n        \"Mlle\": \"Miss\",\n        \"Don\": \"Noble\",\n        \"Dona\": \"Noble\",\n        \"the Countess\": \"Noble\",\n        \"Ms\": \"Mrs\",\n        \"Mme\": \"Mrs\",\n        \"Capt\": \"Noble\",\n        \"Lady\": \"Noble\",\n        \"Sir\": \"Noble\",\n        \"Jonkheer\": \"Noble\"\n    }\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # This takes each name and splits them into two lists, separating the surnames from the rest of the name.\n        # Then the rest of the name is selected using list indexing and split into two lists. This time separating the honorific from the rest of the name.\n        # The honorific is selected using list indexing and whitespace is stripped, resulting the cleaned honorifics.\n        self.X_temp = X[self.column].map(lambda name:name.split(',')[1].split('.')[0].strip())\n        X['Title'] = self.X_temp.map(self.title_dictionary)\n        return X","57568d8c":"test_title = HonorificExtractor(column='Name').fit_transform(df_train)\ncat_plot(test_title, 'Title','Survived', survival_palette)","55fe1737":"print(\"There are:\", df_train.Age.isnull().sum(), \"missing age values\")","85588c41":"grouped_median_train = df_train.groupby([\"Sex\",\"Pclass\", \"Embarked\", \"Title\"], as_index=False).median()\ngrouped_median_train = grouped_median_train.loc[:,[\"Sex\", \"Pclass\", \"Embarked\", \"Title\", \"Age\"]]\ngrouped_median_train.head(3)\ngrouped_median_train.loc[:, :].loc[0, 'Age']","605eb330":"class AgeImputer(BaseEstimator, TransformerMixin):\n\n    \"\"\"\n    Custom SK-Learn Transformer.\n    Groups the data by Sex, Pclass, Embarked and Title, then calculates the median.\n    The missing age data is then imputed based on these conditions.\n    If\n    \"\"\"\n    \n    def fit(self, X, y=None):\n        self.grouped_median_train = X.groupby(['Sex','Pclass', 'Embarked', 'Title'], as_index=False).median()\n        self.grouped_median_train = self.grouped_median_train.loc[:,['Sex', 'Pclass', 'Embarked', 'Title', 'Age']]\n        self.median_age = X.Age.median()\n        return self\n\n    def fill_age(self, row):\n        condition = (\n            (self.grouped_median_train['Sex'] == row['Sex']) | (self.grouped_median_train['Sex'] is None) &\n            (self.grouped_median_train['Pclass'] == row['Pclass']) | (self.grouped_median_train['Pclass'] is None) &\n            (self.grouped_median_train['Title'] == row['Title']) | (self.grouped_median_train['Title'] is None) &\n            (self.grouped_median_train['Embarked'] == row['Embarked']) | (self.grouped_median_train['Embarked'] is None)\n        )\n        \n        return self.grouped_median_train.loc[condition, 'Age'].values[0]\n\n    def transform(self, X):\n        # a function that fills the missing values of the Age variable\n        X['Age'] = X.apply(lambda row: self.fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n        return X.copy()","045b3f69":"test_age = AgeImputer().fit_transform(df_train)\ncont_plot(test_age.loc[:,['Age','Survived']].dropna(axis=0), 'Age', 'Survived', survival_palette, [1, 0], range(0,100,10))","7fcdfec7":"class AgeBinner(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    Custom SK-learn transformer.\n    Bins ages into categorical bins.\n    The bin intervals are infered by eye from the cont_plot for the Age data.\n    \"\"\"\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        bins = pd.IntervalIndex.from_tuples([(0, 10), (10, 30), (30, 60), (60,100)])\n        X['CategoricalAge'] = pd.cut(X['Age'], bins=bins)\n        return X","a470f434":"test_age_bin = AgeBinner().fit_transform(df_train)\ncat_plot(test_age_bin, 'CategoricalAge','Survived', survival_palette)","ce40d274":"class FareBinning(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    Custom SK-learn transformer.\n    Bins fares into categorical bins\n    The bin intervals are infered by eye from the cont_plot for the Fare data.\n    \"\"\"\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X.Fare.fillna(X.Fare.mean(), inplace=True)\n        bins = pd.IntervalIndex.from_tuples([(0, 30), (30, 90), (90,600)])\n        X['CategoricalFare'] = pd.cut(X['Fare'], bins=bins)\n        return X","49b2359f":"test_fare_bin = FareBinning().fit_transform(df_train)\ncat_plot(test_fare_bin, 'CategoricalFare','Survived', survival_palette)","b6695a0b":"class HasCabin(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    Custom SK-learn transformer.\n    Groups the cabins into categories based on the first letter in the cabin code.\n    If a field is null it is filled with \"No Assigned Cabin\"\n    \"\"\"\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X.Cabin = X.Cabin.str[0]\n        X.Cabin = X.Cabin.fillna(\"U\")\n        return X","2a6b0b28":"temp = df_train\ntest_cabin = HasCabin().fit_transform(temp)\ncat_plot(test_cabin, 'Cabin','Survived', survival_palette)","900918b8":"class FamilyCreator(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    Custom SK-learn transformer.\n    Creates a new feature called FamilySize by adding together SibSp and Parch.\n    \"\"\"\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X['FamilySize'] = X['SibSp'] + X['Parch'] + 1\n        return X","cd314e4e":"test_family = FamilyCreator().fit_transform(df_train)\ncont_plot(test_family.loc[:,['FamilySize','Survived']].dropna(axis=0), 'FamilySize', 'Survived', survival_palette, [1, 0], range(0,15,1))","741b6397":"class FamilyBinner(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    Custom SK-learn transformer.\n    Creates a new feature called FamilyBin.\n    Bins the families into three bins based on the magnitude of the FamilySize feature.\n    \"\"\"\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X['Family'] = ''\n        X.loc[X['FamilySize'] == 0, 'Family'] = 'alone'\n        X.loc[(X['FamilySize'] > 0) & (X['FamilySize'] <= 2), 'Family'] = 'small'\n        X.loc[(X['FamilySize'] > 2) & (X['FamilySize'] <= 5), 'Family'] = 'medium'\n        X.loc[X['FamilySize'] > 5, 'Family'] = 'large'\n        return X","6cbdb080":"test_family_bin = FamilyBinner().fit_transform(df_train)\ncat_plot(test_family_bin, 'Family','Survived', survival_palette)","72e21356":"class IsAlone(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    Custom SK-learn transformer.\n    Engineers new feature to determine whether individual is alone on the Titanic.\n    Flag = 0 or 1\n    \"\"\"\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X['IsAlone'] = 0\n        X.loc[X['FamilySize'] == 1, 'IsAlone'] = 1\n        return X","5f573c63":"test_alone = IsAlone().fit_transform(df_train)\ncat_plot(test_alone, 'IsAlone','Survived', survival_palette)","af489d35":"class TicketProcesser(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    Custom SK-learn transformer.\n    Engineers new feature to determine whether individual is alone on the Titanic.\n    In order to reduce the number of tickets and to group similar ticket identifiers together, I have taken the first two letters of the ticket to be the ticket ID.\n    \"\"\"\n    \n    def CleanTicket(self, ticket):\n        ticket = ticket.replace('.', '').replace('\/', '').split()\n        ticket = map(lambda t : t.strip(), ticket)\n        ticket = list(filter(lambda t : not t.isdigit(), ticket))\n        if len(ticket) > 0:\n            return ticket[0][:2]\n        else: \n            return 'XXX'\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X['Ticket'] = X['Ticket'].map(self.CleanTicket)\n        return X","d3a34f1e":"test_ticket = TicketProcesser().fit_transform(df_train)\ncat_plot(test_ticket, 'Ticket','Survived', survival_palette)","7010967d":"class DenseTransformer(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    Custom SK-learn transformer.\n    Returns a dense array if the array is sparse.\n    \"\"\"\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if scipy.sparse.issparse(X) == True:\n            X = X.todense()\n        return X","37566783":"class FeatureDropper(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    Custom SK-learn transformer.\n    Drops features which are used for feature engineering but won't be used in the model.\n    \"\"\"\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n#         X = X.drop([\n#             \"Fare\",\n#             'Age',\n#             'SibSp',\n#             'Parch',\n#             'FamilySize',\n#             'Cabin',\n#             'IsAlone'\n#         ], axis=\"columns\")\n        return X","1cb58568":"PrePreprocessingPipe = Pipeline(\n    steps=[\n        (\"he\", HonorificExtractor(column=\"Name\")),\n        (\"fc\", FamilyCreator()),\n        (\"famb\", FamilyBinner()),\n        (\"ia\", IsAlone()),\n        (\"ai\", AgeImputer()),\n        (\"ab\", AgeBinner()),\n        (\"farb\", FareBinning()),\n        (\"cg\", HasCabin()),\n        (\"fd\", FeatureDropper())\n    ]\n)","a6257e68":"numeric_features = [\n    'SibSp', \n    'Parch',\n    'Age',\n    'Fare',\n    'FamilySize'\n]\n\nnumeric_transformer = Pipeline(\n    steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())\n    ]\n)","cadcd860":"categorical_features = [\n    'Embarked', \n    'Sex', \n    'Pclass', \n    'CategoricalAge', \n    'CategoricalFare', \n    'Title', \n    'Ticket', \n    'Cabin',\n    'Family',\n    'IsAlone'\n]\n\ncategorical_transformer = Pipeline(\n    steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)","a9f8f71d":"PreprocessingPipeline = Pipeline(\n    steps=[\n        (\"pp\", PrePreprocessingPipe),\n        (\"ct\", ColumnTransformer(\n            transformers=[\n                ('num', numeric_transformer, numeric_features),\n                ('cat', categorical_transformer, categorical_features)\n            ]\n        ))\n    ]\n)","deeddd29":"RFC = RandomForestClassifier(n_estimators=50, max_features='sqrt')\nModel = Pipeline(\n    steps=[\n        ('pp', PreprocessingPipeline),\n        ('to_dense', DenseTransformer()),\n        ('classifier', RFC)\n    ]\n)\n\nModel = Model.fit(all_features, Target_feature)\nfeatures = pd.DataFrame()\nfeatures['importance'] = Model.get_params(deep=True)['classifier'].feature_importances_\nprint(\"There are:\", len(features), \"features in the raw preprocessed data.\")","fd4cec7e":"model = Pipeline(\n    steps=[\n        ('pp', PreprocessingPipeline),\n        ('to_dense', DenseTransformer()),\n        ('fi_selector', SelectFromModel(RFC, prefit=True))\n    ]\n)","c09492c0":"train_reduced = model.transform(all_features)\ntest_reduced = model.transform(final_test)\n\nprint(\"The shape of the reduced train dataset is: \", train_reduced.shape)\nprint(\"The shape of the reduced test dataset is: \", test_reduced.shape)\n\nprint(\"\\nTherefore there are\", train_reduced.shape[1], \"features after the features with the highest feature importances have been selected.\")","155ca10b":"classifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True, gamma='scale'),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=100),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(solver='lbfgs')\n]\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog = pd.DataFrame(columns=log_cols)\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=2)\n\nacc_dict = {}\n\nfor train_index, test_index in sss.split(all_features.values, Target_feature.values): \n    Xtrain, Xtest = all_features.iloc[train_index], all_features.iloc[test_index]\n    ytrain, ytest = Target_feature.iloc[train_index], Target_feature.iloc[test_index]\n\n    for clf in classifiers:\n        name = clf.__class__.__name__\n        Model = Pipeline(\n            steps=[\n                ('pp', PreprocessingPipeline),\n                ('to_dense', DenseTransformer()),\n                ('classifier', clf)\n            ]\n        )\n        Model.fit(Xtrain, ytrain)\n        train_predictions = Model.predict(Xtest)\n        acc = accuracy_score(ytest, train_predictions)\n        if name in acc_dict:\n            acc_dict[name] += acc\n        else:\n            acc_dict[name] = acc\n            \nfor clf in acc_dict:\n    acc_dict[clf] = acc_dict[clf] \/ 10.0\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n\nprint(\"Without Feature Importances\")\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\nplt.show()\n\nprint(log)\n\nacc_dict = {}\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog = pd.DataFrame(columns=log_cols)\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=2)\n\nfor train_index, test_index in sss.split(train_reduced, Target_feature.values): \n    Xtrain, Xtest = train_reduced[train_index], train_reduced[test_index]\n    ytrain, ytest = Target_feature.iloc[train_index], Target_feature.iloc[test_index]\n    \n    for clf in classifiers:\n        name = clf.__class__.__name__\n        Model = Pipeline(\n            steps=[\n                ('classifier', clf)\n            ]\n        )\n        Model.fit(Xtrain, ytrain)\n        train_predictions = Model.predict(Xtest)\n        acc = accuracy_score(ytest, train_predictions)\n        if name in acc_dict:\n            acc_dict[name] += acc\n        else:\n            acc_dict[name] = acc\n            \nfor clf in acc_dict:\n    acc_dict[clf] = acc_dict[clf] \/ 10.0\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n\nprint(\"\\n\\nWith Feature Importances\")\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\nplt.show()\n\nprint(log)","346f049f":"classifiers = [\n#     KNeighborsClassifier(3),\n    SVC(),\n#     DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n#     GradientBoostingClassifier(),\n#     GaussianNB(),\n    LogisticRegression()\n]\n\nparameter_grid = [\n#     {\n#         \"n_neighbors\": [2, 3, 4],\n#         \"weights\": [\"uniform\", \"distance\"],\n#         \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n#         \"leaf_size\" : [10, 15, 20],\n#     },\n    {\n        \"C\": [6, 8, 10],\n        \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n        \"shrinking\": [True, False],\n        \"probability\": [True, False],\n        \"gamma\": [2.0, 2.5, 3.0, \"scale\"]\n    },\n#     {\n#         \"criterion\": [\"gini\", \"entropy\"],\n#         \"splitter\": [\"best\", \"random\"],\n#         \"max_features\": [\"auto\", \"sqrt\", \"log2\", None],\n#         \"class_weight\": [\"balanced\", None],\n#         \"presort\": [True, False]\n#     },\n    {\n        \"max_depth\" : [16, 18, 20],\n        \"n_estimators\": [100, 50, 10],\n        \"max_features\": [\"sqrt\", \"auto\", \"log2\"],\n        \"min_samples_split\": [2, 3],\n        \"min_samples_leaf\": [1, 2],\n        \"bootstrap\": [True, False]\n    },\n    {\n        \"n_estimators\": [60, 80],\n        \"algorithm\": [\"SAMME.R\"],\n        \"learning_rate\": [1.2, 1.4]\n    },\n#     {\n#         \"loss\": [\"deviance\", \"exponential\"],\n#         \"learning_rate\": [0.08, 0.1, 0.12],\n#         \"n_estimators\": [90, 100, 110],\n#         \"criterion\": [\"friedman_mse\", \"mse\", \"mae\"],\n#     },\n#     {\n#         \"var_smoothing\" : [1e-9, 2e-9]\n#     },\n    {\n        \"penalty\": [\"l2\"],\n        \"dual\": [False],\n        \"tol\": [1e-5],\n        \"C\": [30, 35, 40],\n        \"fit_intercept\": [True, False],\n        \"solver\": [\"newton-cg\"],\n        \"max_iter\": [200, 400, 1000]\n    }\n]\n\nacc_dict = {}\ncv_dict = {}\n\nrun_gs = False\n\nif run_gs:\n    \n    Xtrain, Xtest, ytrain, ytest = train_test_split(train_reduced,Target_feature,test_size=0.3,random_state=42)\n    for clf in range(len(classifiers)):\n        \n        start_time = time.time()\n        cross_validation = StratifiedKFold(n_splits=10, random_state=22)\n    \n        grid_search = GridSearchCV(\n            classifiers[clf],\n            scoring=\"accuracy\",\n            param_grid=parameter_grid[clf],\n            cv=cross_validation,\n            verbose=0,\n            n_jobs=-1\n        )\n        \n        grid_search.fit(Xtrain, ytrain)\n        model = grid_search\n        parameters = grid_search.best_params_\n        \n        prediction=grid_search.predict(Xtest)\n        print(\"--------------The Accuracy of the {}\".format(classifiers[clf].__class__.__name__), \"----------------------------\")\n        print('The accuracy of the', str(classifiers[clf].__class__.__name__), 'is', round(accuracy_score(prediction, ytest)*100,2))\n        \n        result = cross_val_score(grid_search, Xtrain, ytrain, cv=10, scoring='accuracy')\n        print('The cross validated score for', str(classifiers[clf].__class__.__name__), 'is:', round(result.mean()*100,2))\n        y_pred = cross_val_predict(grid_search, Xtrain, ytrain, cv=10)\n        sns.heatmap(confusion_matrix(ytrain, y_pred), annot=True, fmt='3.0f', cmap=\"summer\")\n        plt.title('Confusion_matrix', y=1.05, size=15)\n        plt.show()\n        \n        print(\"Classifier: {}\".format(classifiers[clf].__class__.__name__))\n        print('Best score: {}'.format(grid_search.best_score_))\n        print('Best parameters: {}'.format(grid_search.best_params_))\n        \n        acc = round(accuracy_score(prediction, ytest)*100,2)\n        if name in acc_dict:\n            acc_dict[name] += acc\n        else:\n            acc_dict[name] = acc\n            \n        del model\n    \n        elapsed_time = time.time() - start_time\n        print(\"Time taken\", round(elapsed_time, 2), \"seconds \\n\")\n        print(\"-\"*40, \"\\n\")\n        \n    plt.xlabel('Accuracy')\n    plt.title('Classifier Accuracy')\n    sns.set_color_codes(\"muted\")\n    sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n    plt.show()\n            \nelse:\n    \n    parameters = [\n#         {'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 3, 'weights': 'distance'},\n        {'C': 8, 'gamma': 'scale', 'kernel': 'rbf', 'probability': True, 'shrinking': True},\n#         {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': None, 'presort': True, 'splitter': 'random'},\n        {'bootstrap': False, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 50} ,\n        {'algorithm': 'SAMME.R', 'learning_rate': 1.2, 'n_estimators': 80},\n#         {'criterion': 'mse', 'learning_rate': 0.12, 'loss': 'exponential', 'n_estimators': 90},\n#         {'var_smoothing': 1e-09},\n        {'C': 30, 'dual': False, 'fit_intercept': True, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg', 'tol': 1e-05}   \n    ]\n    \n    estimator_names = [\n#         \"knc\",\n        \"svc\",\n#         \"dtc\",\n        \"rfc\",\n        \"abc\",\n#         \"gbc\",\n#         \"gnb\",\n        \"lr\"\n    ]\n    \n    csv = [\n#         \"submission_knc.csv\",\n        \"submission_svc.csv\",\n#         \"submission_dtc.csv\",\n        \"submission_rfc.csv\",\n        \"submission_abc.csv\",\n#         \"submission_gbc.csv\",\n#         \"submission_gnb.csv\",\n        \"submission_lr.csv\",        \n    ]\n    \n    estimators = []\n    for clf in range(len(classifiers)):\n        model = classifiers[clf].set_params(**parameters[clf])\n        model.fit(train_reduced, Target_feature)\n        y_predict = model.predict(test_reduced)\n        df_results = pd.DataFrame({\"PassengerId\": final_test.PassengerId, \"Survived\": y_predict})\n        df_results.to_csv(csv[clf], index=False)\n        estimators.append((estimator_names[clf], classifiers[clf].set_params(**parameters[clf])))\n                         \n    ensemble = VotingClassifier(estimators=estimators, voting='hard')\n    classifiers.append(ensemble)\n    estimator_names.append(\"ensemble\")\n    csv_ensemble = \"submission_ensemble.csv\"\n    \n    ensemble.fit(train_reduced, Target_feature)\n    y_predict = model.predict(test_reduced)\n    df_results = pd.DataFrame({\"PassengerId\": final_test.PassengerId, \"Survived\": y_predict})\n    df_results.to_csv(csv_ensemble, index=False)\n                           \n    for clf, label in zip(classifiers, estimator_names):\n        scores = cross_val_score(clf, train_reduced, Target_feature, cv=5, scoring='accuracy')\n        print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","e0358ba9":"Pclass represents the class of ticket held by each passenger on the Titanic. People who have first class tickets are represented by, Pclass = 1. People in second class, Pclass = 2 and people in third class, Pclass = 3. Approximately a quarter of passengers are in Pclass1. Pclass1 is the only class which has more survivors. 20% of passengers are in Pclass2 wich has a slightly worse than 50% survival rate. 55% of passengers are in Pclass3, and approximatly 1\/5 people from Pclass3 survived.\n\nThe Pclass of a person matters for that person to survive or not, therefore this feature is important.","07e5c50b":"The majority of cabins are NaN. I could group these into another cagegory: \"Unknown Cabin\".\nThe I can extract information about the known cabins by taking the first letter from cabin code and using that as a categorical feature.","bf6299a9":"#### Head\n<a id=\"head\"><\/a>\nExamine what each dataset looks like.","fb3cd9bf":"The test dataset is missing the `Survived` flag.","297d3e35":"#### Age Binning","89d3ff54":"#### Cabin","5ee85ba2":"Titanic Dataset\n===============\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, I will try to predict what sorts of people were likely to survive.\n\n\nI used the following links as references:  \nhttps:\/\/www.kaggle.com\/jatturat\/finding-important-factors-to-survive-titanic  \nhttps:\/\/ahmedbesbes.com\/how-to-score-08134-in-titanic-kaggle-challenge.html","cb36cb00":"Model Testing\n-------------\n<a id=\"testing\"><\/a>\n### Compare Model Performance With and Without Feature Importances\n<a id=\"fi_comparison\"><\/a>","6823ed87":"#### Null Values\n<a id=\"null\"><\/a>\nExamine the number of null values in the train\/test datasets","7d70d930":"#### Drop features wich are highly correlated.","0e40e3d9":"#### Sparse Array --> Dense","63ddcf5b":"### Fare","774dc2e7":"Remove Warnings\n---------------\n\n<a id=\"warnings\"><\/a>","09157f78":"This dataframe will help us impute missing age values based on different criteria.","60bbe224":"#### Fare Binning","5d5d996b":"Import Data\n-----------\n\n<a id=\"data\"><\/a>","f423cc14":"The majority of passengers on the Titanic were male, 65%. A majority of females survived and a majority of males did not.\nThe sex of a person matters for whether that person survived or not, therfore this feature is important.","876114dc":"Table of Contents\n-----------------\n1. [Import Libraries](#libraries)\n2. [Remove Warnings](#warnings)\n3. [Import Data](#data)\n4. [Exploritory Data Analysis](#eda)  \n    4.1 [Overview of the Data](#overview)  \n    4.1 [Univariate Visualisations and Analysis](#univariate)  \n5. [Feature Engineering](#fe)  \n6. [Model Building](#model)  \n    6.1 [Pipeline](#pipe)  \n    6.2 [Feature Importances](#fi)  \n7. [Model Testing](#testing)  \n    7.1 [Compare Model Performance With and Without Feature Importances](#fi_comparison)  \n    7.2 [Grid Search Models, Create an Ensemble Model and Generate Submission Files](#gs_em\")","d726e16b":"#### Age","dbf4409a":"#### DataFrame Attributes\n<a id=\"info\"><\/a>\nExamine the info for the training dataset including datatype, number of entries, null\/not-null","4dd9607d":"Feature Importances have improved the performance of the decision tree based classifiers.","40fa06d0":"On average individuals who paid <\u00a350 for their ticket died and individuals who paid >\u00a350 survived. To get a more granular idea of what's going on I will focus on the lower fares which were purchased by the majority of people.","1194659f":"Multivariate Analysis\n---------------------","5be3cdf0":"Exploritory Data Analysis\n-------------------------\n<a id=\"eda\"><\/a>\n\n### Overview of the Data\n<a id=\"overview\"><\/a>\n\n#### Shape \n<a id=\"shape\"><\/a>\nPrint out the shape of each dataset","5f39e21d":"#### Number of Parents and Children","da99e37f":"Import Libraries\n----------------\n\n<a id=\"libraries\"><\/a>","02ee81b9":"There were more survivors than fatalities among passangers aged <15. There were fewer survivors than fatalities among passangers between the ages of 15 and 35. Among passengers aged >35 and <60 the numbers of survivors and fatalities is equal again. There are few survivors aged >60. These differences could make it suitable to bin the ages.","3c4c5f88":"The rest of this EDA will focus on the Train dataset to simulate real life situations.","7c404635":"This class has been built as an sk-learn transformer so that it can be applied to the train and test datasets in an sklearn pipeline.","837910f2":"#### Sex","9a797ef0":"Heatmap of Null values in train dataset.","b10d4ed9":"The pipeline stages above are combined into the Pre-processing pipeline below. This makes use of the ColumnTransformer transformer to apply the different transformations to numerical and categorical features.","1dbed399":"There is nothing useful here so I will not use the ticket number in the model.","355bf49b":"Feature Engineering\n-------------------\n<a id=\"fe\"><\/a>\n#### Title","b8566df0":"Then I split up the features into numeric and categorical so that different transformer classes can be applied to each.","cc539149":"This engineered feature provides value as the survival of an individual depends strongly on their title.","8687541e":"Similar to SibSp, people with few numbers parents\/children on board had the best survivability.\n\nBecause SibSp and Parch have similar meaning meaning, \"Family\". If I combine these 2 features together, maybe I could see more differences in each classes of these features.","d9b84712":"### Extract Feature Importances\n<a id=\"fi\"><\/a>","31b82d02":"### Univariate Analysis\n<a id=\"univariate\"><\/a>\n#### Visualisation Functions","e01cd567":"Model\n-----\n<a id=\"model\"><\/a>\n### Pipeline\n<a id=\"pipe\"><\/a>\nFirst build the Pre-Pre-Processing Pipeline. This contains the custom feature engineering transformers.","e4f55441":"### Grid Search Models, Create an Ensemble Model and Generate Submission Files\n<a id=\"gs_em\"><\/a>\n\nModels which performed poorly have been comented out to try to maximise the performance of the voting classifier.","20334f7e":"#### Impute Ages","b12428b8":"#### Passenger Class","eae5f454":"#### Ticket Binning","66794ff9":"#### Number of Siblings and Spouses","b57340b7":"#### Alone Flag","be658c5a":"The fields with the most null values are `Age` and `Cabin`. The majority of `Cabin` fields are null, however this doesn't mean that the data is useless. In this case it may be a fair assumption that passengers with cheaper tickets have no assigned `cabin` rather than the data being missing. This means that the data might still hold some value. This will be investigated in the [multivariate analysis](#multivariate) section. The missing `Age` fields will be imputed in the feature engineering.","a97261b6":"The threshold at which the number of people who survived > number of people who died is \u00a330","49566113":"#### Cabin","a50e8f02":"SibSp represents the number of siblings and spouses a person has.\n\nOnly people with 1 sibling or spouse has a >50% chance of survival.","3b7ef71d":"#### Family Size","2bfa6e0e":"#### Embarked Location","e3459b28":"#### Ticket"}}