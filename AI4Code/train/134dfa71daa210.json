{"cell_type":{"6855c5fb":"code","8642965d":"code","fdff40be":"code","71268e36":"code","af4cc253":"code","8b381a92":"code","4ffe7a77":"code","cf81a172":"code","b2d6543e":"code","6ea392f8":"code","bbb4bc29":"code","de680d04":"code","b8d9f00d":"code","d784df7a":"code","026108fa":"code","01d7b417":"code","91f58172":"code","5c28d14e":"code","ac96f639":"code","e91ba727":"code","eb90518c":"code","f786a8c1":"code","f5ef6906":"code","8a99c298":"code","7028512d":"code","f7c57ce0":"code","d291cd82":"code","e0944961":"code","57f43040":"code","cb443dfa":"code","7e5fbfac":"code","7dceae96":"code","0cc796cf":"code","58faa1a6":"code","6dcbb4ef":"code","039a39c2":"code","77b0f5a8":"code","a997a635":"code","e78788de":"code","c029b254":"code","a8f90626":"code","ba7a2e66":"code","6101fe71":"code","b17da04c":"code","f0060850":"code","84706c36":"code","23d55905":"code","a8bebeeb":"code","e36240f2":"code","c20f7eb4":"code","83144c5a":"code","731c10ff":"code","9599e4d8":"code","aa18644a":"code","8c3ad17b":"code","95a9ec77":"code","77565d5c":"code","44c18f8d":"code","ec7cf6ac":"code","aff1cda8":"code","07ce3bf7":"code","7ffe7dd5":"code","68a48d81":"code","00ad2548":"code","e152b959":"code","77978b7b":"code","0f7fc695":"code","d66bd87b":"code","18e413bc":"code","feb5d8a2":"code","1a4f314f":"code","cc1bee8e":"code","4de3e340":"code","01e08624":"code","d5535566":"code","dc9d7332":"code","d7cab7e1":"code","4dc2fbdf":"code","b84ce72e":"code","69dfcb50":"code","276e5c8e":"markdown","cf4e813f":"markdown","f2fb6803":"markdown","7e42136a":"markdown","9896c17f":"markdown","2dc701af":"markdown","a88178c7":"markdown","6b5d1411":"markdown","afab7099":"markdown","db95f5f2":"markdown","b567c555":"markdown","87e27556":"markdown","e92afdf6":"markdown","f1f2e54a":"markdown","9643a44d":"markdown","7ec26b5d":"markdown"},"source":{"6855c5fb":"!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers\/transformers-master\/","8642965d":"import pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nDATA_DIR = '..\/input\/google-quest-challenge'","fdff40be":"!ls ..\/input","71268e36":"os.listdir(\"..\/input\/roberta-transformers-pytorch\/roberta-base\")","af4cc253":"os.listdir(\"..\/input\/qaxlnetbasecasedaugdiffswaanswer\")","8b381a92":"os.listdir(\"..\/input\/qaxlnetbasecasedaugdiffswaquestion\")","4ffe7a77":"os.listdir(\"..\/input\/qabertbaseuncasedaugdiffswaanswer\")","cf81a172":"os.listdir(\"..\/input\/qabertbaseuncasedaugdiffswaquestion\")","b2d6543e":"os.listdir(\"..\/input\/qabertbasecasedaugdiffswaanswer\")","6ea392f8":"os.listdir(\"..\/input\/qabertbasecasedaugdiffswaquestion\")","bbb4bc29":"os.listdir(\"..\/input\/qabertbasecasedaugdiffv2swa\")","de680d04":"os.listdir(\"..\/input\/qabertuncasedaugdiffv2swa\")","b8d9f00d":"os.listdir(\"..\/input\/qaxlnetbasecasedaugdiff\")","d784df7a":"os.listdir(\"..\/input\/qarobertabasecasedaugdiffswaquestion\")","026108fa":"sub = pd.read_csv(f'{DATA_DIR}\/sample_submission.csv')\nsub.head()","01d7b417":"TARGET_COLUMNS = sub.columns.values[1:].tolist()\nTARGET_COLUMNS","91f58172":"train = pd.read_csv(f'{DATA_DIR}\/train.csv')\ntrain.head()","5c28d14e":"test = pd.read_csv(f'{DATA_DIR}\/test.csv')\ntest.head()","ac96f639":"import torch\n#import torch.utils.data as data\nfrom torchvision import datasets, models, transforms\nfrom transformers import *\nfrom sklearn.utils import shuffle\nimport random\nfrom math import floor, ceil\nfrom sklearn.model_selection import GroupKFold\n\nMAX_LEN = 512\n#MAX_Q_LEN = 250\n#MAX_A_LEN = 259\nSEP_TOKEN_ID = 102\n\nclass QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, df, model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", train_mode=True, labeled=True):\n        self.df = df\n        self.train_mode = train_mode\n        self.labeled = labeled\n        self.max_len = max_len\n        self.content = content\n        bert_tokenizer_path = '..\/input\/pretrained-bert-models-for-pytorch\/' + model_type + '-vocab.txt'\n        xlnet_tokenizer_path = '..\/input\/xlnet-pretrained-models-pytorch\/' + model_type + '-spiece.model'\n        roberta_tokenizer_path = '..\/input\/roberta-transformers-pytorch\/roberta-base\/vocab.json'\n        roberta_tokenizer_merges_file = '..\/input\/roberta-transformers-pytorch\/roberta-base\/merges.txt'\n        if model_type == \"bert-base-uncased\":\n            self.tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_path)\n        elif model_type == \"bert-base-cased\":\n            self.tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_path)\n        elif model_type == \"xlnet-base-cased\":\n            self.tokenizer = XLNetTokenizer.from_pretrained(xlnet_tokenizer_path)\n        elif model_type == \"roberta-base\":\n            self.tokenizer = RobertaTokenizer(vocab_file=roberta_tokenizer_path, merges_file=roberta_tokenizer_merges_file)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        token_ids, seg_ids = self.get_token_ids(row)\n        if self.labeled:\n            labels = self.get_label(row)\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\n    def __len__(self):\n        return len(self.df)\n\n    def select_tokens(self, tokens, max_num):\n        if len(tokens) <= max_num:\n            return tokens\n        if self.train_mode:\n            num_remove = len(tokens) - max_num\n            remove_start = random.randint(0, len(tokens)-num_remove-1)\n            return tokens[:remove_start] + tokens[remove_start + num_remove:]\n        else:\n            return tokens[:max_num\/\/2] + tokens[-(max_num - max_num\/\/2):]\n        \n    def trim_input_single_content(self, title, content, max_sequence_length=512, \n                t_max_len=30, c_max_len=512-30-4, num_token=3):\n\n        t = self.tokenizer.tokenize(title)\n        c = self.tokenizer.tokenize(content)\n\n        t_len = len(t)\n        c_len = len(c)\n\n        if (t_len+c_len+num_token) > max_sequence_length:\n\n            if t_max_len > t_len:\n                t_new_len = t_len\n                c_max_len = c_max_len + floor((t_max_len - t_len)\/2)\n            else:\n                t_new_len = t_max_len\n\n            if c_max_len > c_len:\n                c_new_len = c_len \n            else:\n                c_new_len = c_max_len\n\n\n            if t_new_len+c_new_len+num_token > max_sequence_length:\n                raise ValueError(\"New sequence length should be less or equal than %d, but is %d\" \n                                 % (max_sequence_length, (t_new_len+c_new_len+num_token)))\n            \n            # truncate\n            if len(t) - t_new_len > 0:\n                t = t[:t_new_len\/\/4] + t[len(t)-t_new_len+t_new_len\/\/4:]\n            else:\n                t = t[:t_new_len]\n\n            if len(c) - c_new_len > 0:\n                c = c[:c_new_len\/\/4] + c[len(c)-c_new_len+c_new_len\/\/4:]\n            else:\n                c = c[:c_new_len]\n\n        # some bad cases\n        if (len(t) + len(c) + num_token > max_sequence_length):\n            more_token = len(t) + len(c) + num_token - max_sequence_length\n            c = c[:(len(c)-more_token)]\n        \n        return t, c\n            \n    def trim_input(self, title, question, answer, max_sequence_length=MAX_LEN, \n                t_max_len=30, q_max_len=239, a_max_len=239, num_token=4):\n\n        t = self.tokenizer.tokenize(title)\n        q = self.tokenizer.tokenize(question)\n        a = self.tokenizer.tokenize(answer)\n\n        t_len = len(t)\n        q_len = len(q)\n        a_len = len(a)\n\n        if (t_len+q_len+a_len+num_token) > max_sequence_length:\n\n            if t_max_len > t_len:\n                t_new_len = t_len\n                a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n                q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n            else:\n                t_new_len = t_max_len\n\n            if a_max_len > a_len:\n                a_new_len = a_len \n                q_new_len = q_max_len + (a_max_len - a_len)\n            elif q_max_len > q_len:\n                a_new_len = a_max_len + (q_max_len - q_len)\n                q_new_len = q_len\n            else:\n                a_new_len = a_max_len\n                q_new_len = q_max_len\n\n\n            if t_new_len+a_new_len+q_new_len+num_token > max_sequence_length:\n                raise ValueError(\"New sequence length should be %d, but is %d\" \n                                 % (max_sequence_length, (t_new_len+a_new_len+q_new_len+num_token)))\n\n            \n            # truncate\n            if len(t) - t_new_len > 0:\n                t = t[:t_new_len\/\/4] + t[len(t)-t_new_len+t_new_len\/\/4:]\n            else:\n                t = t[:t_new_len]\n\n            if len(q) - q_new_len > 0:\n                q = q[:q_new_len\/\/4] + q[len(q)-q_new_len+q_new_len\/\/4:]\n            else:\n                q = q[:q_new_len]\n\n            if len(a) - a_new_len > 0:\n                a = a[:a_new_len\/\/4] + a[len(a)-a_new_len+a_new_len\/\/4:]\n            else:\n                a = a[:a_new_len]\n\n        return t, q, a\n        \n    def get_token_ids(self, row):\n        \n        num_token = 4\n        \n        if self.content == \"Question\":\n            num_token -= 1\n        elif self.content == \"Answer\":\n            num_token -= 1\n        \n        if self.content == \"Question_Answer\":   \n            t_max_len=30\n            q_max_len=int((self.max_len-t_max_len-num_token)\/2)\n            a_max_len=(self.max_len-t_max_len - num_token - int((self.max_len-t_max_len-num_token)\/2))\n        elif self.content == \"Question\":\n            t_max_len=30\n            q_max_len=self.max_len-t_max_len-num_token\n            a_max_len=0\n        elif self.content == \"Answer\":\n            t_max_len=30\n            q_max_len=0\n            a_max_len=self.max_len-t_max_len-num_token  \n        else:\n            raise NotImplementedError\n        \n        if self.content == \"Question_Answer\":\n            t_tokens, q_tokens, a_tokens = self.trim_input(row.question_title, row.question_body, row.answer, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, q_max_len=q_max_len, a_max_len=a_max_len, num_token=num_token)\n        elif self.content == \"Question\":\n            t_tokens, c_tokens = self.trim_input_single_content(row.question_title, row.question_body, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, c_max_len=q_max_len, num_token=num_token)\n        elif self.content == \"Answer\":\n            t_tokens, c_tokens = self.trim_input_single_content(row.question_title, row.answer, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, c_max_len=a_max_len, num_token=num_token)\n        else:\n            raise NotImplementedError\n\n        if self.content == \"Question_Answer\":\n            tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + q_tokens + ['[SEP]'] + a_tokens + ['[SEP]']\n        elif ((self.content == \"Question\") or (self.content == \"Answer\")):\n            tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + c_tokens + ['[SEP]']\n        else:\n            raise NotImplementedError\n                \n        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        if len(token_ids) < self.max_len:\n            token_ids += [0] * (self.max_len - len(token_ids))\n        ids = torch.tensor(token_ids)\n        seg_ids = self.get_seg_ids(ids)\n        \n        return ids, seg_ids\n    \n    def get_seg_ids(self, ids):\n        seg_ids = torch.zeros_like(ids)\n        seg_idx = 0\n        first_sep = True\n        for i, e in enumerate(ids):\n            seg_ids[i] = seg_idx\n            if e == self.tokenizer.sep_token_id:\n                if first_sep:\n                    first_sep = False\n                else:\n                    seg_idx = 1\n        pad_idx = torch.nonzero(ids == 0)\n        seg_ids[pad_idx] = 0\n\n        return seg_ids\n\n    def get_label(self, row):\n        #print(row[TARGET_COLUMNS].values)\n        return torch.tensor(row[TARGET_COLUMNS].values.astype(np.float32))\n\n    def collate_fn(self, batch):\n        token_ids = torch.stack([x[0] for x in batch])\n        seg_ids = torch.stack([x[1] for x in batch])\n    \n        if self.labeled:\n            labels = torch.stack([x[2] for x in batch])\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\ndef get_test_loader(model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", batch_size=4):\n    df = pd.read_csv(f'{DATA_DIR}\/test.csv')\n    ds_test = QuestDataset(df, model_type, max_len=max_len, content=content, train_mode=False, labeled=False)\n    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=ds_test.collate_fn, drop_last=False)\n    loader.num = len(df)\n    \n    return loader, ds_test.tokenizer\n        \ndef get_train_val_loaders(model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", batch_size=4, val_batch_size=4, ifold=0):\n    df = pd.read_csv(f'{DATA_DIR}\/train.csv')\n    df = shuffle(df, random_state=42)\n    #split_index = int(len(df) * (1-val_percent))\n    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n    for fold, (train_idx, valid_idx) in enumerate(gkf):\n        if fold == ifold:\n            df_train = df.iloc[train_idx]\n            df_val = df.iloc[valid_idx]\n            break\n\n    #print(df_val.head())\n    #df_train = df[:split_index]\n    #df_val = df[split_index:]\n\n    print(df_train.shape)\n    print(df_val.shape)\n\n    ds_train = QuestDataset(df_train, model_type, max_len=max_len, content=content)\n    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=ds_train.collate_fn, drop_last=True)\n    train_loader.num = len(df_train)\n\n    ds_val = QuestDataset(df_val, model_type, max_len=max_len, content=content, train_mode=False)\n    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=2, collate_fn=ds_val.collate_fn, drop_last=False)\n    val_loader.num = len(df_val)\n    val_loader.df = df_val\n\n    return train_loader, val_loader, ds_train.tokenizer\n\ndef test_train_loader():\n    loader, _, _ = get_train_val_loaders(\"xlnet-base-cased\", 512, \"Question\", 4, 4, 1)\n    for ids, seg_ids, labels in loader:\n        print(ids)\n        print(seg_ids.numpy())\n        print(labels)\n        break\ndef test_test_loader():\n    loader, _ = get_test_loader(\"roberta-base\", 512, \"Question\", 4)\n    for ids, seg_ids in loader:\n        print(ids)\n        print(seg_ids)\n        break","e91ba727":"test_test_loader()","eb90518c":"test_train_loader()","f786a8c1":"from transformers import *\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass QuestModel(nn.Module):\n    def __init__(self, model_type=\"xlnet-base-cased\", tokenizer=None, n_classes=30, hidden_layers=[-1, -3, -5, -7, -9]):\n        super(QuestModel, self).__init__()\n        self.model_name = 'QuestModel'\n        self.model_type = model_type\n        self.hidden_layers = hidden_layers\n        if model_type == \"bert-base-uncased\":\n            bert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\n            bert_config = BertConfig.from_json_file(bert_model_config)\n            bert_config.output_hidden_states = True\n            # haven't update question_answer model\n            if n_classes == 30:\n                bert_config.hidden_dropout_prob = 0.1\n            else:\n                bert_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/' + model_type)\n            self.bert_model = BertModel.from_pretrained(model_path, config=bert_config)   \n        elif model_type == \"bert-base-cased\":\n            bert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-cased\/bert_config.json'\n            bert_config = BertConfig.from_json_file(bert_model_config)\n            bert_config.output_hidden_states = True\n            model_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/' + model_type)\n            self.bert_model = BertModel.from_pretrained(model_path, config=bert_config)   \n        elif model_type == \"xlnet-base-cased\":\n            xlnet_model_config = '..\/input\/xlnet-pretrained-models-pytorch\/xlnet-base-cased-config.json'\n            xlnet_config = XLNetConfig.from_json_file(xlnet_model_config)\n            xlnet_config.output_hidden_states = True\n            xlnet_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/xlnet-pretrained-models-pytorch\/' + model_type + '-pytorch_model.bin')\n            self.xlnet_model = XLNetModel.from_pretrained(model_path, config=xlnet_config)   \n        elif model_type == \"xlnet-large-cased\":\n            xlnet_model_config = '..\/input\/xlnet-pretrained-models-pytorch\/xlnet-large-cased-config.json'\n            xlnet_config = XLNetConfig.from_json_file(xlnet_model_config)\n            xlnet_config.output_hidden_states = True\n            xlnet_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/xlnet-pretrained-models-pytorch\/' + model_type + '-pytorch_model.bin')\n            self.xlnet_model = XLNetModel.from_pretrained(model_path, config=xlnet_config)  \n        elif model_type == \"roberta-base\":\n            roberta_model_config = '..\/input\/roberta-transformers-pytorch\/roberta-base\/config.json'\n            roberta_config = RobertaConfig.from_json_file(roberta_model_config)\n            roberta_config.output_hidden_states = True\n            roberta_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/roberta-transformers-pytorch\/roberta-base\/pytorch_model.bin')\n            self.roberta_model = RobertaModel.from_pretrained(model_path, config=roberta_config)  \n            self.roberta_model.resize_token_embeddings(len(tokenizer)) \n        \n        if model_type == \"bert-base-uncased\":\n            self.hidden_size = 768\n        elif model_type == \"bert-large-uncased\":\n            self.hidden_size = 1024\n        elif model_type == \"bert-base-cased\":\n            self.hidden_size = 768\n        elif model_type == \"xlnet-base-cased\":\n            self.hidden_size = 768\n        elif model_type == \"xlnet-large-cased\":\n            self.hidden_size = 1024\n        elif model_type == \"roberta-base\":\n            self.hidden_size = 768\n        else:\n            raise NotImplementedError\n            \n        self.fc_1 = nn.Linear(self.hidden_size * len(hidden_layers), self.hidden_size)\n        self.fc = nn.Linear(self.hidden_size, n_classes)\n            \n        self.selu = nn.SELU()\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n\n    def forward(self, ids, seg_ids):\n        attention_mask = (ids > 0)\n        \n        if ((self.model_type == \"bert-base-uncased\") \\\n            or (self.model_type == \"bert-base-cased\") \\\n            or (self.model_type == \"bert-large-uncased\") \\\n            or (self.model_type == \"bert-large-cased\")):\n        \n            outputs = self.bert_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            hidden_states = outputs[2]\n            \n            # pooled_out = outputs[1] #  N * 768\n        \n            # sequence_out = torch.unsqueeze(outputs[0][:, 0], dim=-1) # N * 512 * 768 * 1, hidden_states[-1]\n            # fuse_hidden = sequence_out\n            \n            # 13 (embedding + 12 transformers) for base\n            # 26 (embedding + 25 transformers) for large\n            \n            # concat hidden\n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = torch.mean(hidden_states[hidden_layer], dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = torch.mean(hidden_states[hidden_layer], dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n                    \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n        \n        elif ((self.model_type == \"xlnet-base-cased\") \\\n            or (self.model_type == \"xlnet-large-cased\")):\n\n            attention_mask = attention_mask.float()\n            outputs = self.xlnet_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            hidden_states = outputs[1]\n            \n            # last_hidden_out = outputs[0]\n            # mem = outputs[1], when config.mem_len > 0\n            \n            # concat hidden, summary_type=\"first\", first_dropout = 0\n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n        \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n        elif (self.model_type == \"roberta-base\"):\n\n            attention_mask = attention_mask.float()\n            outputs = self.roberta_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            # outputs = self.roberta_model(input_ids=ids, attention_mask=attention_mask)\n            hidden_states = outputs[2]\n            \n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n        \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n            \n            \n            \n        for j, dropout in enumerate(self.dropouts):\n            \n            if j == 0:\n                logit = self.fc(dropout(h))\n            else:\n                logit += self.fc(dropout(h))\n                \n        return logit \/ len(self.dropouts)\n    \ndef test_model(model_type=\"bert-base-cased\", hidden_layers=[-1, -3, -5, -7, -9]):\n    x = torch.tensor([[1,2,3,4,5, 0, 0], [1,2,3,4,5, 0, 0]])\n    seg_ids = torch.tensor([[0,0,0,0,0, 0, 0], [0,0,0,0,0, 0, 0]])\n    model = QuestModel(model_type=model_type, hidden_layers=hidden_layers)\n\n    y = model(x, seg_ids)\n    print(y)","f5ef6906":"test_model(model_type=\"bert-base-cased\", hidden_layers=[-3, -4, -5, -6, -7])","8a99c298":"def create_bert_base_uncased_models():\n    models = []\n    for i in range(10):\n        model = QuestModel(model_type=\"bert-base-uncased\", hidden_layers=[-1, -3, -5, -7, -9])\n        model.load_state_dict(torch.load(f'..\/input\/qabertuncasedaugdiffv2swa\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_models():\n    models = []\n    for i in range(10):\n        model = QuestModel(model_type=\"bert-base-cased\", hidden_layers=[-1, -3, -5, -7, -9])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbasecasedaugdiffv2swa\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qaxlnetbasecasedaugquestionanswerswa\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qaxlnetbasecasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qaxlnetbasecasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\n\ndef create_bert_base_uncased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-uncased\", n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbaseuncasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_uncased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-uncased\", n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbaseuncasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-cased\", n_classes=21, hidden_layers=[-2, -4, -6, -8, -10])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbasecasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-cased\", n_classes=9, hidden_layers=[-2, -4, -6, -8, -10])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbasecasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_roberta_base_models(tokenizer):\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"roberta-base\", tokenizer=tokenizer, n_classes=30, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qarobertabaseaugdiffswa\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_roberta_base_question_models(tokenizer):\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"roberta-base\", tokenizer=tokenizer, n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qarobertabasecasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_roberta_base_answer_models(tokenizer):\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"roberta-base\", tokenizer=tokenizer, n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qarobertabasecasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models","7028512d":"from tqdm import tqdm\nimport torch\ndef predict(models, test_loader):\n    all_scores = []\n    with torch.no_grad():\n        for ids, seg_ids in tqdm(test_loader, total=test_loader.num \/\/ test_loader.batch_size):\n            ids, seg_ids = ids.cuda(), seg_ids.cuda()\n            scores = []\n            for model in models:\n                model = model.cuda()\n                outputs = torch.sigmoid(model(ids, seg_ids)).cpu()\n                scores.append(outputs)\n            all_scores.append(torch.mean(torch.stack(scores), 0))\n\n    all_scores = torch.cat(all_scores, 0).numpy()\n    \n    return all_scores","f7c57ce0":"test_loader, tokenizer = get_test_loader(model_type=\"roberta-base\", content=\"Question_Answer\", batch_size=32)","d291cd82":"roberta_base_models = create_roberta_base_models(tokenizer)\nroberta_base_preds = predict(roberta_base_models, test_loader)","e0944961":"del roberta_base_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","57f43040":"test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", batch_size=32)","cb443dfa":"xlnet_base_cased_models = create_xlnet_base_cased_models()\nxlnet_base_cased_preds = predict(xlnet_base_cased_models, test_loader)","7e5fbfac":"del xlnet_base_cased_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","7dceae96":"test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", content=\"Question\", batch_size=32)","0cc796cf":"xlnet_base_cased_question_models = create_xlnet_base_cased_question_models()\nxlnet_base_cased_question_preds = predict(xlnet_base_cased_question_models, test_loader)","58faa1a6":"del xlnet_base_cased_question_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","6dcbb4ef":"test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", content=\"Answer\", batch_size=32)","039a39c2":"xlnet_base_cased_answer_models = create_xlnet_base_cased_answer_models()\nxlnet_base_cased_answer_preds = predict(xlnet_base_cased_answer_models, test_loader)","77b0f5a8":"del xlnet_base_cased_answer_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","a997a635":"xlnet_base_cased_question_answer_preds = np.concatenate([xlnet_base_cased_question_preds, xlnet_base_cased_answer_preds], axis=1)","e78788de":"test_loader, tokenizer = get_test_loader(model_type=\"roberta-base\", content=\"Question\", batch_size=32)","c029b254":"roberta_base_question_models = create_roberta_base_question_models(tokenizer)\nroberta_base_question_preds = predict(roberta_base_question_models, test_loader)","a8f90626":"del roberta_base_question_models, test_loader, tokenizer\ntorch.cuda.empty_cache()\ngc.collect()","ba7a2e66":"test_loader, tokenizer = get_test_loader(model_type=\"roberta-base\", content=\"Answer\", batch_size=32)","6101fe71":"roberta_base_answer_models = create_roberta_base_answer_models(tokenizer)\nroberta_base_answer_preds = predict(roberta_base_answer_models, test_loader)","b17da04c":"del roberta_base_answer_models, test_loader, tokenizer\ntorch.cuda.empty_cache()\ngc.collect()","f0060850":"roberta_base_question_answer_preds = np.concatenate([roberta_base_question_preds, roberta_base_answer_preds], axis=1)","84706c36":"test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", content=\"Question\", batch_size=32)","23d55905":"bert_base_cased_question_models = create_bert_base_cased_question_models()\nbert_base_cased_question_preds = predict(bert_base_cased_question_models, test_loader)","a8bebeeb":"del bert_base_cased_question_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","e36240f2":"test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", content=\"Answer\", batch_size=32)","c20f7eb4":"bert_base_cased_answer_models = create_bert_base_cased_answer_models()\nbert_base_cased_answer_preds = predict(bert_base_cased_answer_models, test_loader)","83144c5a":"del bert_base_cased_answer_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","731c10ff":"bert_base_cased_question_answer_preds = np.concatenate([bert_base_cased_question_preds, bert_base_cased_answer_preds], axis=1)","9599e4d8":"test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", content=\"Question\", batch_size=32)","aa18644a":"bert_base_uncased_question_models = create_bert_base_uncased_question_models()\nbert_base_uncased_question_preds = predict(bert_base_uncased_question_models, test_loader)","8c3ad17b":"del bert_base_uncased_question_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","95a9ec77":"test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", content=\"Answer\", batch_size=32)","77565d5c":"bert_base_uncased_answer_models = create_bert_base_uncased_answer_models()\nbert_base_uncased_answer_preds = predict(bert_base_uncased_answer_models, test_loader)","44c18f8d":"del bert_base_uncased_answer_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","ec7cf6ac":"bert_base_uncased_question_answer_preds = np.concatenate([bert_base_uncased_question_preds, bert_base_uncased_answer_preds], axis=1)","aff1cda8":"test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", batch_size=32)","07ce3bf7":"bert_base_cased_models = create_bert_base_cased_models()\nbert_base_cased_preds = predict(bert_base_cased_models, test_loader)","7ffe7dd5":"del bert_base_cased_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","68a48d81":"test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", batch_size=32)","00ad2548":"bert_base_uncased_models = create_bert_base_uncased_models()\nbert_base_uncased_preds = predict(bert_base_uncased_models, test_loader)","e152b959":"del bert_base_uncased_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","77978b7b":"# preds = bert_base_uncased_question_answer_preds\npreds = 0.25 * (0.5 * bert_base_uncased_preds + 0.5 * bert_base_uncased_question_answer_preds) \\\n         + 0.25 * (0.5 * bert_base_cased_preds + 0.5 * bert_base_cased_question_answer_preds) \\\n         + 0.25 * (0.5 * xlnet_base_cased_preds + 0.5 * xlnet_base_cased_question_answer_preds) \\\n         + 0.25 * (0.5 * roberta_base_preds + 0.5 * roberta_base_question_answer_preds)\n# preds = bert_base_uncased_preds\n# preds = roberta_base_question_answer_preds","0f7fc695":"sub[TARGET_COLUMNS] = bert_base_uncased_preds\nsub.to_csv('submission_bert_base_uncased.csv', index=False)\nsub[TARGET_COLUMNS] = bert_base_cased_preds\nsub.to_csv('submission_bert_base_cased.csv', index=False)\nsub[TARGET_COLUMNS] = xlnet_base_cased_preds\nsub.to_csv('submission_xlnet_base_cased.csv', index=False)\n# sub[TARGET_COLUMNS] = xlnet_base_cased_preds\n# sub.to_csv('submission_xlnet_base_cased.csv', index=False)","d66bd87b":"sub[TARGET_COLUMNS] = preds","18e413bc":"sub.head()","feb5d8a2":"test = pd.read_csv(f'{DATA_DIR}\/test.csv')","1a4f314f":"test = test.set_index('qa_id').join(sub.set_index('qa_id'))","cc1bee8e":"test.head()","4de3e340":"from sklearn.preprocessing import MinMaxScaler\n    \ndef postprocessing(oof_df):\n    \n    ################################################# round to i \/ 90 (i from 0 to 90)\n    oof_values = oof_df[TARGET_COLUMNS].values\n    DEGREE = 90\n    oof_values = np.around(oof_values * DEGREE) \/ DEGREE  ### 90 To be changed\n    oof_df[TARGET_COLUMNS] = oof_values\n    \n    return oof_df","01e08624":"test = postprocessing(test)","d5535566":"for column in TARGET_COLUMNS:\n    print(test[column].value_counts())","dc9d7332":"sub = test[TARGET_COLUMNS].reset_index()","d7cab7e1":"sub[ sub[TARGET_COLUMNS] > 1.0] = 1.0\nsub[ sub[TARGET_COLUMNS] < 0] = 0","4dc2fbdf":"sub['question_type_spelling'].value_counts()","b84ce72e":"sub.to_csv('submission.csv', index=False)","69dfcb50":"sub.head()","276e5c8e":"## predict with xlnet-base-cased question and answer","cf4e813f":"## predict with roberta-base","f2fb6803":"## predict with bert-base-cased question and answer","7e42136a":"# Assign postprocessed result","9896c17f":"* ### bert-base (uncased-10-fold, cased-10-fold) swa + xlnet (5 folds) + bert-base-uncased (question + answer) swa + bert-base-cased (question + answer) swa + xlnet (question + answer) swa + roberta (question + answer) + postprocessing (seefun's version)","2dc701af":"## Build Model","a88178c7":"**Pytorch BERT baseline**","6b5d1411":"## predict with bert-base-cased","afab7099":"### Define dataset","db95f5f2":"## predict with roberta-base question and answer","b567c555":"### Generate Submission","87e27556":"## predict with bert-base-uncased question and answer","e92afdf6":"## predict with bert-base-uncased","f1f2e54a":"# Postprocessing","9643a44d":"### Required Imports\n\nI've added imports that will be used in training too","7ec26b5d":"## predict with xlnet-base-cased"}}