{"cell_type":{"4f9e67d5":"code","c9935d6b":"code","675cd659":"code","c2afefbf":"code","621610e6":"code","d8450a61":"code","eaf5638a":"code","a8c6dd20":"code","576b3ad1":"code","2a33908b":"code","659a3e5c":"code","98e70921":"code","ace3a3f5":"code","ab730097":"code","170c8c32":"code","6a7ad250":"code","2cd42556":"code","6219e90e":"code","07ecaa01":"code","64db40c8":"code","177479d1":"code","1d383f4b":"code","e3b1eb46":"code","4669ecd8":"code","f43ba98d":"code","fa69006b":"code","f013f4f1":"code","d431d927":"code","2ddcb8fb":"code","1203ef63":"code","ab286f31":"code","2b7f53e9":"code","d573afb1":"code","09046016":"code","66bd3f80":"code","fe2aedf6":"code","676ea95f":"code","f2729dbb":"code","2fa131c3":"code","3aa553bd":"code","a64fdb31":"code","28438a41":"code","b0cb523d":"code","9a0dcd19":"code","4f542a6d":"code","e76088b6":"code","4ede7da3":"code","6fc04ad1":"code","73da4190":"code","6dab9309":"code","d5370648":"code","0f203030":"code","3de60bef":"code","20d24536":"code","63225eac":"code","0e6fab4e":"code","91be3b1c":"code","d4b72c1c":"code","28f855e6":"code","9302c180":"code","c38bcd6e":"code","f143bf61":"code","19966b70":"code","ec6095f8":"code","d0de1158":"code","9d7e80e3":"code","126d47b1":"code","fe26edfe":"code","9c55294d":"code","dc3f9673":"code","ce4bf8da":"code","3a0f33c0":"code","736ebb30":"code","0d002d4e":"code","8206bc90":"markdown","74e7ec48":"markdown","aeefdf48":"markdown","790cf117":"markdown","f7be754b":"markdown","d7b66287":"markdown","02b32d21":"markdown","611312a3":"markdown","84ed518a":"markdown","ceea88ed":"markdown","b5699e32":"markdown","970d1216":"markdown","86265d1c":"markdown","223b2cfe":"markdown","91a2cda1":"markdown","7929f9b8":"markdown","1ce47005":"markdown","d4011584":"markdown","d9df1ef8":"markdown","7376c4b3":"markdown","4f6b691e":"markdown","6cfda65f":"markdown","247dcb45":"markdown","d68fc43c":"markdown","76fab95f":"markdown","f106f5a4":"markdown","09140ee1":"markdown","44cb25e1":"markdown","7ca16839":"markdown","3cecc18d":"markdown","91042c10":"markdown","156476d3":"markdown","c78b4515":"markdown","1b2154fc":"markdown","23960494":"markdown","41c253c8":"markdown","bf80488d":"markdown","c834a77b":"markdown","a22b8c75":"markdown","1c4efd6d":"markdown","039b90c7":"markdown","4ab25dfd":"markdown"},"source":{"4f9e67d5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# For Imputation\nfrom sklearn.preprocessing import LabelEncoder\n\n# For data preprocessing\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\n\n# For model building\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# For visualizing the descision tree\nfrom sklearn import tree\n\n\n\n","c9935d6b":"\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n#import the necessary modelling algos.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score,auc\nfrom sklearn.model_selection import GridSearchCV","675cd659":"from sklearn.metrics import (accuracy_score,\n                             f1_score,\n                             log_loss,\n                             roc_auc_score,\n                             roc_curve,\n                             confusion_matrix)\nfrom sklearn.model_selection import (cross_val_score,\n                                     GridSearchCV,\n                                     RandomizedSearchCV,\n                                     learning_curve,\n                                     validation_curve,\n                                     train_test_split)\n\nfrom sklearn.pipeline import make_pipeline # For performing a series of operations\n\nfrom sklearn.metrics import plot_confusion_matrix","c2afefbf":"df = pd.read_csv('..\/input\/mri-and-alzheimers\/oasis_longitudinal.csv')","621610e6":"df.head()","d8450a61":"df.shape","eaf5638a":"# getting a feel of the data types of the columns\n\ndf.info()","a8c6dd20":"df.isnull().sum()","576b3ad1":"df.describe() # for numerical cols","2a33908b":"df.skew()","659a3e5c":"df.MMSE.fillna(df.MMSE.median(),inplace=True)","98e70921":"df.SES.fillna(df.SES.median(),inplace=True)","ace3a3f5":"df.isnull().sum()","ab730097":"df.drop(columns='Hand',axis=1,inplace=True)","170c8c32":"df.head()","6a7ad250":"df.isnull().sum()","2cd42556":"# Reversing using mapping\nses_map = {5:1,4:2,3:3,2:4,1:5}\ndf.SES = df.SES.map(ses_map)\n","6219e90e":"df.head()","07ecaa01":"df.SES.value_counts()","64db40c8":"df_copy = df.copy()\ndf.to_csv('oasis_longitude.csv')","177479d1":"df.dtypes","1d383f4b":"gender_map = {'M':0, 'F':1}\ndf['Gender'] = df['M\/F'].map(gender_map)","e3b1eb46":"df.tail()","4669ecd8":"df.dtypes","f43ba98d":"df.drop(columns='M\/F',axis=1,inplace=True)","fa69006b":"df.head()","f013f4f1":"df.Group.value_counts()","d431d927":"target_map = {'Nondemented':0,'Demented':1,'Converted':2}\n\ndf['Group'] = df.Group.map(target_map)","2ddcb8fb":"df.Group.value_counts()","1203ef63":"corr = df.corr()\ncorr","ab286f31":"# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the figure\nfig, ax = plt.subplots(figsize=(12,8))\n\n# Generate a custom colormap\ncmap = sns.diverging_palette(250, 10, s=80, l=55, n=9, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio (mask to not display upper triangle part)\nsns.heatmap(corr, mask=mask, cmap=cmap, ax=ax, annot=True);\nplt.savefig('corr.png')","2b7f53e9":"plt.figure(figsize=(12,8))\nsns.countplot(df['CDR'])\nplt.title('Distribution of CDR Levels')\nplt.xlabel('CDR LEVEL')\nplt.ylabel('COUNT')\nplt.savefig('CDR_distribution.png')","d573afb1":"sns.factorplot(x='CDR',y='SES',data=df,kind='box',size=5,aspect=1)","09046016":"a = df.SES.value_counts()","66bd3f80":"list(a.index)","fe2aedf6":"# Create list of indicies of SES counts\nses_count = df['SES'].value_counts()\nses_indexes = list(ses_count.index)\n\n# Plot of distribution of scores for building categories\nplt.figure(figsize=(12, 8))\n\n# Plot each building\nfor s in ses_indexes:\n    # Select the SES category\n    subset = df[df['SES'] == s]\n    \n    # Density plot of CDR scores\n    sns.kdeplot(subset['CDR'],\n               label = s, shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('CDR Score', size = 20);\nplt.ylabel('Density', size = 20); \nplt.title('Density Plot of CDR Scores by SES', size = 28);\nplt.savefig('SES_CDR.png')","676ea95f":"sns.factorplot(x='CDR',kind='count',col='SES',data=df)","f2729dbb":"df.EDUC.value_counts()","2fa131c3":"df.dtypes","3aa553bd":"# Create list of indicies of SES counts\nedu_count= df['EDUC'].value_counts()\nedu_index = list(edu_count.index)\n\n# Plot of distribution of scores for building categories\nplt.figure(figsize=(12, 8))\n\n# Plot each building\nfor el in edu_index:\n    # Select the SES category\n    subset = df[df['EDUC'] == el]\n    \n    # Density plot of CDR scores\n    sns.kdeplot(subset['CDR'],\n               label = el, shade = False, alpha = 0.8,bw=0.5);\n    \n# label the plot\nplt.xlabel('CDR Score', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of CDR Scores by Years of Education', size = 28);\n#plt.xlim([0,2]);\nplt.savefig('EDU_CDR.png')","a64fdb31":"# Min and Max years of education among subjects\nmin_edu = df.loc[df['EDUC']==12]\nmax_edu = df.loc[df['EDUC']==16]\n\n# Stack them into a combine dataframe\nedu_concat = pd.concat([min_edu,max_edu])\nedu_concat.head()","28438a41":"# Create list of indicies of SES counts\nedu_= edu_concat['EDUC'].value_counts()\nedu_index = list(edu_.index)\n\n# Plot of distribution of scores for building categories\nplt.figure(figsize=(12, 8))\n\n# Plot each building\nfor el in edu_index:\n    # Select the SES category\n    subset = edu_concat[edu_concat['EDUC'] == el]\n    \n    # Density plot of CDR scores\n    sns.kdeplot(subset['CDR'],\n               label = el, shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('CDR Score', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of CDR Scores by Years of Education', size = 28);\n#plt.xlim([0,2]);\nplt.savefig('EDU_CDR.png')","b0cb523d":"# Create list of indicies of SES counts\ngender_count= df['Gender'].value_counts()\ngender_indicies = list(gender_count.index)\n\n# Plot of distribution of scores for building categories\nplt.figure(figsize=(12, 10))\n\n# Plot each building\nfor g in gender_indicies:\n    # Select the SES category\n    subset = df[df['Gender']==g]\n    \n    # Density plot of CDR scores\n    sns.kdeplot(subset['CDR'],\n               label = g, shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('CDR Score', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of CDR Scores by Gender', size = 28, );\nplt.savefig('Gender_CDR.png')","9a0dcd19":"fig = plt.figure(figsize=(12,8))\nsns.catplot(x='CDR',y='Age',data=df,hue='Gender')\nplt.savefig('Age_CDR.png')","4f542a6d":"fig = plt.figure(figsize=(12,8))\nsns.catplot(x='CDR',y='MMSE',data=df, hue='Gender')\nplt.savefig('MMSE_CDR')","e76088b6":"fig = plt.figure(figsize=(12,8))\nsns.catplot(x='CDR',y='eTIV',data=df)","4ede7da3":"fig = plt.figure(figsize=(12,8))\nsns.catplot(x='CDR',y='ASF',data=df)","6fc04ad1":"fig = plt.figure(figsize=(12,8))\nsns.catplot(x='CDR', y='nWBV', data=df)","73da4190":"df.shape","6dab9309":"df.head()","d5370648":"selected_df = df.drop(['Subject ID','MRI ID','CDR'],axis=1)\n","0f203030":"selected_df.head()","3de60bef":"# Rename columns\nrename_cols_dict = {'EDUC':'Education',\n                   'Group':'Diagnosis'}\nselected_df.rename(rename_cols_dict,axis=1,inplace=True)\nselected_df.head()","20d24536":"selected_df.dtypes","63225eac":"target = selected_df.Diagnosis.values\n\npredictors_df = selected_df.drop(['Diagnosis'],axis=1)","0e6fab4e":"predictors_df.head()","91be3b1c":"predictors_df.dtypes","d4b72c1c":"plt.figure(figsize=(12,8))\nsns.countplot(selected_df.Diagnosis)\nplt.title('Distribution of Diagnosis')\nplt.xlabel('Diagnosis')\nplt.ylabel('COUNT')\nplt.savefig('Diagnosis_distribution.png')","28f855e6":"x_train,x_test,y_train,y_test = train_test_split(predictors_df,target,test_size=0.2,stratify=target,random_state=1)","9302c180":"print(\"Training Data - Predictors\",x_train.shape)\nprint(\"Testing Data - Predictors\",x_test.shape)\nprint(\"Training Data - Target\",y_train.shape)\nprint(\"Testing Data - Target\",y_test.shape)","c38bcd6e":"from sklearn.pipeline import make_pipeline # For performing a series of operations\n\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.preprocessing import StandardScaler","f143bf61":"# Build random forest classifier\nmethods_data = {'Original': (x_train,y_train)}\n\nfor method in methods_data.keys():\n    pip_rf = make_pipeline(StandardScaler(),\n                           RandomForestClassifier(n_estimators=500,\n                                                  class_weight=\"balanced\",\n                                                  random_state=123))\n    hyperparam_grid = {\n        \"randomforestclassifier__n_estimators\": [10, 50, 100, 500],\n        \"randomforestclassifier__max_features\": [\"sqrt\", \"log2\", 0.4, 0.5],\n        \"randomforestclassifier__min_samples_leaf\": [1, 3, 5],\n        \"randomforestclassifier__criterion\": [\"gini\", \"entropy\"]}\n    \n    gs_rf = GridSearchCV(pip_rf,\n                         hyperparam_grid,\n                         scoring=\"f1_macro\",\n                         cv=10,\n                         n_jobs=-1)\n    \n    gs_rf.fit(methods_data[method][0], methods_data[method][1])\n    \n    print(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters for {} data:\".format(method))\n    for hyperparam in gs_rf.best_params_.keys():\n        print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_rf.best_params_[hyperparam])\n    \n    print(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_rf.best_score_) * 100))","19966b70":"# Refit RF classifier using best params\nclf_rf = make_pipeline(StandardScaler(),\n                       RandomForestClassifier(n_estimators=10,\n                                              criterion=\"gini\",\n                                              max_features=0.4,\n                                              min_samples_leaf=3,\n                                              class_weight=\"balanced\",\n                                              n_jobs=-1,\n                                              random_state=123))\n\n\nclf_rf.fit(x_train, y_train)","ec6095f8":"# Build Gradient Boosting classifier\npip_gb = make_pipeline(StandardScaler(),\n                       GradientBoostingClassifier(loss=\"deviance\",\n                                                  random_state=123))\n\nhyperparam_grid = {\"gradientboostingclassifier__max_features\": [\"log2\", 0.5],\n                   \"gradientboostingclassifier__n_estimators\": [100, 300, 500],\n                   \"gradientboostingclassifier__learning_rate\": [0.001, 0.01, 0.1],\n                   \"gradientboostingclassifier__max_depth\": [1, 2, 3]}\n\ngs_gb = GridSearchCV(pip_gb,\n                      param_grid=hyperparam_grid,\n                      scoring=\"f1_macro\",\n                      cv=10,\n                      n_jobs=-1)\n\ngs_gb.fit(x_train, y_train)\n\nprint(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\nprint(\"-\" * 25)\nfor hyperparam in gs_gb.best_params_.keys():\n    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_gb.best_params_[hyperparam])\n\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_gb.best_score_) * 100))","d0de1158":"# Build logistic model classifier\npip_logmod = make_pipeline(StandardScaler(),\n                           LogisticRegression(class_weight=\"balanced\"))\n\nhyperparam_range = np.arange(0.5, 20.1, 0.5)\n\nhyperparam_grid = {\"logisticregression__penalty\": [\"l1\", \"l2\"],\n                   \"logisticregression__C\":  hyperparam_range,\n                   \"logisticregression__fit_intercept\": [True, False]\n                  }\n\ngs_logmodel = GridSearchCV(pip_logmod,\n                           hyperparam_grid,\n                           scoring=\"accuracy\",\n                           cv=2,\n                           n_jobs=-1)\n\ngs_logmodel.fit(x_train, y_train)\n\nprint(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\nprint(\"-\" * 25)\nfor hyperparam in gs_logmodel.best_params_.keys():\n    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_logmodel.best_params_[hyperparam])\n\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_logmodel.best_score_) * 100))","9d7e80e3":"estimators = {\"RF\": clf_rf,\n              \"LR\": gs_logmodel,\n              \"GBT\": gs_gb\n             }\n\n# Print out accuracy score on test data\nprint(\"The accuracy rate on test data are:\")\nfor estimator in estimators.keys():\n    print(\"{}: {:.2f}%\".format(estimator,\n        accuracy_score(y_test, estimators[estimator].predict(x_test)) * 100\n          ))","126d47b1":"predictions = gs_gb.predict(x_test)","fe26edfe":"predictions.shape","9c55294d":"selected_df.Diagnosis.value_counts()","dc3f9673":"model_names=['RandomForestClassifier','Logistic Regression','GradientBoostingClassifier']\nmodels = [clf_rf,gs_logmodel,gs_gb]","ce4bf8da":"def compare_models(model):\n    clf=model\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    \n    # Calculating various metrics\n    \n    acc.append(accuracy_score(pred,y_test))\n    #prec.append(precision_score(pred,y_test))\n    #rec.append(recall_score(pred,y_test))\n    #auroc.append(roc_auc_score(pred,y_test))","3a0f33c0":"acc=[]\nprec=[]\nrec=[]\nauroc=[]","736ebb30":"for model in models:\n    compare_models(model)\n","0d002d4e":"d={'Modelling Algo':model_names,'Accuracy':acc}\nmet_df=pd.DataFrame(d)\nmet_df","8206bc90":"* **Drop features that are highly correlated when using Linear Classifiers**\n* **We can drop CDR column and keep Group Column as both represent the same thing**\n* **We can drop Subject Id and MRI ID as they are irrevelant**","74e7ec48":"* **GBT has better accuracy, but its not enough**","aeefdf48":"## Data Wrangling","790cf117":"## Summary of EDA","f7be754b":"### ASF vs CDR","d7b66287":"## Feature Scaling","02b32d21":"> For SES","611312a3":"### Splitting into training and testing data","84ed518a":"### Dropping Hand Column\n> It contains only one kind of values and hence isnt useful for our model. These type of features are called ***Zero Varaince Predictor*** and should be avoided","ceea88ed":"Using Stratify to maintaine the same ratio of target variable values in both train and test dataset","b5699e32":"### Define Target & Predictor feature(s)","970d1216":"## Imputing Missing Values","86265d1c":"## Age vs CDR","223b2cfe":"## eTIV vs CDR","91a2cda1":"## Choosing Evaluation Metric\n\nWe will be going forward with AUC and Diagnostic Odds Ratio","7929f9b8":"**The problem we have is a Multi-Class Classification Problem**","1ce47005":"### Correlated Features\n\n* **ASF and eTIV - drop one**\n* **Visit and MR_Delay - drop one**","d4011584":"## Data Visualization","d9df1ef8":"* **MMSE: ** From the plots above we can infer that high MMSE scores relate with low CDR levels. Therefore, MMSE is an important feature in predicting CDR Levels\n\n* **SES: ** Couldn't understand much from the plots to certainly say that SES has an influence on CDR scores. But would like to keep it. Going with my Intiution!\n\n* **Gender: ** Gender did suggest that females are heralthier than males and hence it is an important feature\n\n* **ASF: ** No idea!\n\n* **eTIV: ** No Idea!\n\n* **nWBV: ** As Normal Whole Brain Volume decreases CDR Increases. nWBV has an influence on CDR\n\n* **EDUC: ** As seen in the above plots lower Education subject has slighlty greater CDR score than the subjects with higher Education. \n\n","7376c4b3":"**Saving clean data**","4f6b691e":"**MMSE Scores below 25 have a higher probability of getting CDR**\n* The Ones with moderate Dementia have MMSE < 25\n* The Ones with Mild Dementia have MMSE < 25","6cfda65f":"* **Between ASF and eTIV there is a high negative correlation**\n* **Between MR Delay and Visit there is a high positive correlation**\n\n**Hence we will have to drop any one from both sets**","247dcb45":"Not helpful lets plot for the top 2 values of EDUC","d68fc43c":"## Does Gender have an effect?","76fab95f":"**No More Missing Values**","f106f5a4":"* **High SES group (4) have CDR score 0 as a common value**\n* **Low SES group (1) have CDR score 0.5 as a common value**","09140ee1":"### Encoding M\/F - Gender","44cb25e1":"* **eTIV & ASF plots doesnt signify much**\n* **However, nWBV vs CDR, the normal Whole Brain Volume decreases as CDR level increases.**","7ca16839":"### Plotting against Target Variable","3cecc18d":"* **SES has 19 missing values**\n* **MMSE has 2 missing values**","91042c10":"**Not really insightful, but cdr scores for the age range of 65 -85 vary a lot. But that in no way indicates that age influences CDR score**","156476d3":"# EDA","c78b4515":"## Checking if Education has an effect on CDR","1b2154fc":"* Not a remarkable insight but subject with 12 Years of education has slightly greater CDR score than subject with 16 years of education ","23960494":"**Todo - Label Encode Diagnosis column**","41c253c8":"1 = Female have lower CDR level than male (0). Females seems to be healthier according to the dataset at hand","bf80488d":"## MMSE vs CDR","c834a77b":"## Feature Selection","a22b8c75":"## Reversing the order of SES values\n> Like CDR, SES is also a level based feature. In CDR the value start from 0 and goes till 2 defining the seriousness of dementia. Whereas in SES , SES=1 (Highest Status) and SES = 5 (Lowest Status) which is the opposite of the trend in CDR.\n\n> Therefore reversing the values of SES so that, SES = 1 (Lowest Status) and SES = 5 (Highest Status)","1c4efd6d":"> For MMSE","039b90c7":"> **Checking the distribution of the target variable**","4ab25dfd":"## nWBV vs CDR"}}