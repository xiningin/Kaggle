{"cell_type":{"1d8675b1":"code","cfa1e9f0":"code","266e7bae":"code","5af4657f":"code","4c8f9082":"code","076fbf76":"code","83203827":"code","7c0dbaee":"code","903f6de6":"code","f277a6bc":"code","196f0c26":"code","68ab3442":"code","c2eda5a4":"code","967ceeaf":"code","6ad74112":"code","0f19b8bb":"code","698b14bd":"code","6497f7b9":"code","d0df0def":"code","e0e165a8":"code","129d6337":"code","431b14e5":"code","d43408ed":"code","b2d05cae":"code","28eea1e7":"code","f26c7739":"code","2596d001":"code","cb22995c":"code","5a7b2615":"code","c1cd6792":"code","fc90ae93":"code","ae19cd97":"code","ef230ba3":"code","d00d9698":"code","8b3d342f":"code","ec76431a":"code","7fbf0f03":"code","6454fa6b":"code","1a17d75a":"code","0ddd38c3":"code","6e5922cc":"code","6f7d32b7":"code","65be6b0c":"code","9819d333":"code","63ab0961":"code","05fa8edb":"markdown","90be7997":"markdown","8b861ba1":"markdown","ac75aceb":"markdown","ac150e57":"markdown","cbd42d17":"markdown","b0fad964":"markdown","7b9f3cad":"markdown","6e1536f4":"markdown"},"source":{"1d8675b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cfa1e9f0":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport random\nimport seaborn as sns\nimport re\nimport datetime\nimport math\nfrom pathlib import Path\n# from sklearn.model_selection import train_test_split","266e7bae":"train_data=pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\")","5af4657f":"train_data.head(), train_data.tail()","4c8f9082":"countries=set(train_data['country'])\ncountries","076fbf76":"products=set(train_data['product'])\nproducts","83203827":"store_type=set(train_data['store'])\nstore_type","7c0dbaee":"# Country -wise Sale \ntrain_data['country'].value_counts()\n# Inference -> Same sales  in each of the Country","903f6de6":"# Product Wise Sale\ntrain_data['product'].value_counts()\n# Inference -> Same sales of each of the three Product","f277a6bc":"# Store wise Sale\ntrain_data['store'].value_counts()\n# Inference -> Same sales at both the types of stores","196f0c26":"print('Train data dates ranging from:', train_data['date'].min(), '->', train_data['date'].max())","68ab3442":"train_data['date'] = pd.to_datetime(train_data['date'])","c2eda5a4":"train_data['Year'] = train_data.date.dt.year\ntrain_data['Month'] = train_data.date.dt.month","967ceeaf":"# First let us Plot the Frequencies of Sales throughout the Time Frame\nplt.figure(figsize=(9,9))\nsns.lineplot(x = 'date', y = 'num_sold', data = train_data, label = 'daily')\n# Inference -> December - January Festive season records the Highest amount of Sales\n# April Easter Season gets second hgihest Spike in Sales","6ad74112":"plt.figure(figsize=(9,9))\nsns.lineplot(x = 'date', y = 'num_sold', data = train_data,hue='store')\n# Inference -> Kaggle RAMA Sales > Kaggle Mart Sales","0f19b8bb":"plt.figure(figsize=(9,9))\nsns.lineplot(x = 'date', y = 'num_sold', data = train_data,hue='product')\n# Inference ->  Sales of Hats > Sales of Mugs > Sales of Stickers","698b14bd":"plt.figure(figsize=(9,9))\nsns.lineplot(x = 'date', y = 'num_sold', data = train_data,hue='country')\n# Inference ->  Sales of Norway > Sales of Sweden > Sales of Finland","6497f7b9":"plt.figure(figsize=(9,9))\nsns.lineplot(x = 'Year', y = 'num_sold', data = train_data,hue='country')\n# sns.lineplot(x = 'month', y = 'num_sold', data = train_data,hue='country')\n# Inference ->  Sales of Norway > Sales of Sweden > Sales of Finland","d0df0def":"train_data.describe()\n# Neglect it for row_id\n# Daily Average for items Sold-> 388","e0e165a8":"# Get a General trend of Increase in Sales, so the test data will definitely be an extrapolation\nplt.figure(figsize=(9,9))\nsns.lineplot(x = 'Year', y = 'num_sold', data = train_data,hue='store')","129d6337":"#  Now we will Segregate the Data Country-wise and further do some Visualization before moving\n# to Feature Engineering\ntrain_data_Finland = train_data[train_data.country == \"Finland\"]\ntrain_data_Norway = train_data[train_data.country == \"Norway\"]\ntrain_data_Sweden = train_data[train_data.country == \"Sweden\"]","431b14e5":"# Now we will analyse the trends for each Country as Well\ntrain_data_Finland['num_sold'].describe(),train_data_Norway['num_sold'].describe(),train_data_Sweden['num_sold'].describe()","d43408ed":"train_monthly_country = train_data.set_index('date').groupby([pd.Grouper(freq = 'M'), 'country'])[['num_sold']].mean()\nplt.figure(figsize = (9, 9))\nsns.lineplot(x = 'date', y = 'num_sold', hue = 'country', data = train_monthly_country)","b2d05cae":"train_data.groupby(['country', 'store', 'product']).num_sold.agg(['max', 'mean'])","28eea1e7":"# Importing the Datasets for completeness\ntrain=pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\ngdp=pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\nsample_sub=pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')","f26c7739":"train.head(),test.head(),gdp.head(),sample_sub.head()","2596d001":"# Defining sMAPE\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\ndef SMAPE2(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return diff.squeeze()\n","cb22995c":"import holidays\nimport lightgbm\nimport pickle\nfrom datetime import datetime\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nimport dateutil.easter as easter","5a7b2615":"# Feature engineering\n\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom scipy import stats\nimport ipywidgets as widgets\nfrom learntools.time_series.style import *  # plot style settings\nfrom learntools.time_series.utils import (create_multistep_example,\n                                          load_multistep_data,\n                                          make_lags,\n                                          make_multistep_target,\n                                          plot_multistep)","c1cd6792":"# Importing Libs\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n","fc90ae93":"# The Model that is trained is Hybrid Model GBM + XGboost + Linear\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb","ae19cd97":"# Handling the Missing Values\ndef handle_missing_values(df):\n    # Any numerical data must be replaced by 0 \n    for data in df.select_dtypes(\"number\"):\n        df[data] = df[data].fillna(0)\n    # Categorical Data must be replaced by None\n    for data in df.select_dtypes(\"category\"):\n        df[data] = df[data].fillna(\"None\")\n    return df\n","ef230ba3":"def fourier_features(index, freq, order):\n    time = np.arange(len(index), dtype=np.float32)\n    k = 2 * np.pi * (1 \/ freq) * time\n    features = {}\n    for i in range(1, order + 1):\n        features.update({\n            f\"sin_{freq}_{i}\": np.sin(i * k),\n            f\"cos_{freq}_{i}\": np.cos(i * k),\n        })\n    return pd.DataFrame(features, index=index)","d00d9698":"# time series data common new feature  \nDATE = \"date\"\nYEAR = \"year\"\nMONTH = \"month\"\nWEEK = \"week\"\nDAY = \"day\"\nDAYOFYEAR = \"dayofyear\"\nDAYOFMONTH = \"dayofMonth\"\nDAYOFWEEK = \"dayofweek\"\nWEEKDAY = \"weekday\"","8b3d342f":"def get_basic_ts_features(df):\n    \n    gdp.set_index('year', inplace=True)\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n    \n    df['gdp'] = np.log(df.apply(get_gdp, axis=1))\n    \n    for country in ['Finland', 'Norway']:\n        df[country] = df.country == country\n    for store in ['KaggleMart']:\n        df[store] = df['store'] == store\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        df[product] = df['product'] == product\n    \n\n    df[WEEKDAY] = df[DATE].dt.weekday == 4\n    df[WEEKDAY] = df[DATE].dt.weekday >= 5\n    \n    # 21 days cyclic for lunar\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 22, 1):\n        df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'Finland_sin{k}'] = df[f'sin{k}'] * df['Finland']\n        df[f'Finland_cos{k}'] = df[f'cos{k}'] * df['Finland']\n        df[f'Norway_sin{k}'] = df[f'sin{k}'] * df['Norway']\n        df[f'Norway_cos{k}'] = df[f'cos{k}'] * df['Norway']\n        df[f'store_sin{k}'] = df[f'sin{k}'] * df['KaggleMart']\n        df[f'store_cos{k}'] = df[f'cos{k}'] * df['KaggleMart']\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n        df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n    \n    # End of year - Christmas Season\n    for d in range(24, 32):\n        df[f\"dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d)\n    # Start of the new Year\n    for d in range(1, 13):\n        df[f\"jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d)\n    # May\n    for d in list(range(1, 10)) + list(range(17, 25)):\n        df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d)\n    # June\n    for d in list(range(6, 14)):\n        df[f\"june{d}\"] = (df.date.dt.month == 6) & (df.date.dt.day == d)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    for d in list(range(-5, 6)):\n        df[f\"wed_june{d}\"] = (df.date - wed_june_date == np.timedelta64(d, \"D\"))\n        \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    for d in list(range(0, 10)):\n        df[f\"sun_nov{d}\"] = (df.date - sun_nov_date == np.timedelta64(d, \"D\"))\n        \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    for d in list(range(0, 9)) + list(range(50, 60)) + list(range(40, 46)):\n        df[f\"easter{d}\"] = (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      \n#     df.drop(columns=[DATE], inplace = True)\n    \n    return df","ec76431a":"target_column = train.columns.difference(\n    test.columns)[0]  \n","7fbf0f03":"train[DATE] = pd.to_datetime(train[DATE])\ntest[DATE] = pd.to_datetime(test[DATE])","6454fa6b":"# Feature Engineering\ntrain = get_basic_ts_features(train)\ntest = get_basic_ts_features(test)","1a17d75a":"train.head(),test.head()","0ddd38c3":"def encoding_func(data):\n    output = data.copy()\n     \n    for col in data.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        \n#     for colname,col in output.iteritems():\n#                 output[colname] = LabelEncoder().fit_transform(col)\n    return output","6e5922cc":"train_1=encoding_func(train)\ntrain_1","6f7d32b7":"test_1=encoding_func(test)","65be6b0c":"X = train_1.set_index([DATE]).sort_index()\nX_test = test.set_index([DATE]).sort_index()","9819d333":"train_data_final = train.set_index(['date', 'country', 'store', 'product']).sort_index()","63ab0961":"#  Last Cell\n# sub['num_sold'] = np.round(sub['num_sold']).astype(int)\n# import pandas as pd\n# sub = pd.read_csv('..\/input\/tps-jan-2022-automated-ensembling\/submission.csv')\n# sub.num_sold *= 0.99\n# sub.to_csv('submission.csv', index=False)\n# sub","05fa8edb":"<h2> Data Cleaning and Feature Engineering","90be7997":"We have all the dependencies in place and Now we can start with Data Cleaning\nand Feature Engineering","8b861ba1":"Reference for EDA:\nhttps:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense\n","ac75aceb":"Since this is a Time Series Forecasting Question the Test Dates would be dates in the \nfuture from 2019.","ac150e57":"<h3> Reference for Feature Engineering and Hybrid Model <\/h3>\n<h4> https:\/\/www.kaggle.com\/teckmengwong\/tps2201-hybrid-time-series\n","cbd42d17":"<h2> EDA <\/h2>","b0fad964":"<h4>\nSo obviously there ia a Trend of Norway > Sweden > Finland <br>\nSo the Feature Engineering further will require a row of GDP Like how a lot of people have done <br>\nSo we will have to bring the GDP Dataset as an added feature for the Model <br>\n","7b9f3cad":"<h2> Gist of the the Data <\/h2>\n<h4>\nRow Ids <br>\nDatetime Column indicating this is a Time Series Data <br>\nCountries - Finland, Norway, Sweeden <br>\nStore Types - Kaggle Mart and Kaggle Rama <br>\nProducts - Hats, Mugs, Stickers <br>\n<\/h4>\n","6e1536f4":"Feature Engineering Ref: https:\/\/www.kaggle.com\/teckmengwong\/tps2201-hybrid-time-series#Fine-tuning"}}