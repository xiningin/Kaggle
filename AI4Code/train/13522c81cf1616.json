{"cell_type":{"05a1404a":"code","3cf7d012":"code","4a2fa5e4":"code","8b58e3e0":"code","eb603ee2":"code","8bd79bb3":"code","d2113984":"code","c9d57cf8":"code","189bb09d":"code","7397e32a":"code","6c03d1e4":"code","5f4ae8e3":"code","ad5bcf04":"code","38b2ef9c":"code","57a6f874":"markdown","d34c1f5c":"markdown"},"source":{"05a1404a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport json\nfrom pandas.io.json import json_normalize\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3cf7d012":"#just read data in json format. Probably the data came from a NoSQL database\nX= pd.read_json('\/kaggle\/input\/sao-paulo-real-estate-prediction\/source-4-ds-train.json',orient='columns',lines=True)\nX.head()\n","4a2fa5e4":"#some columns have nested data.Create a function to unnest\ndef unnest_column(df,nested_columns):\n  for position in range(len(nested_columns)):\n    column=nested_columns[position]\n    un=df[column].apply(pd.Series)\n    df=df.combine_first(un)\n    #X=X.join(un,lsuffix='_left', rsuffix='_right')\n    df=df.drop(columns=[column]) \n  return df\n\n    \n    \nnested=['address','geoLocation','pricingInfos','location']\nX=unnest_column(X,nested)\n\nX.head()","8b58e3e0":"#get rid of outliers\nX=X.drop(X[(X['bathrooms']>6)].index)\nX=X.drop(X[(X['bedrooms']>6)].index)\nX=X.drop(X[(X['parkingSpaces']>6)].index)\nX=X.drop(X[(X['suites']>5)].index)\nX=X.drop(X[(X['totalAreas']>700) | (X['totalAreas']<28)].index)\nX=X.drop(X[(X['usableAreas']>650) | (X['usableAreas']<28)].index)\nX=X.drop(X[(X['price']>5000000) | (X['price']<100000)].index)\n\n\n# yearlyIptu is the city tax. Varies from neighborhood and property size, so it's a good feature to predict price\nX.yearlyIptu=X.yearlyIptu.replace(0,np.nan)\nX.usableAreas=X.usableAreas.replace(0,np.nan)\n\n#creates a column tax\/size\nX['paramIptu']=X.yearlyIptu\/X.usableAreas\n\n#group by neighborhood and extracts the mean \nX['paramIptu']=X.groupby('neighborhood').transform(lambda x: x.fillna(x.mean()))['paramIptu']\n\n#fills the  missing data with the neighborhood mean , them multiply by size\nX['estimatedIptu']=(X.usableAreas*X.paramIptu) \nX.yearlyIptu=X['yearlyIptu'].fillna(X['estimatedIptu'])\n\n#some had to be filled with min value\nX.yearlyIptu=X['yearlyIptu'].fillna(X.yearlyIptu.min())\n\n#rental properties doesn't matter here\nX=X[X.businessType !='RENTAL']\n\n\n# select the features \nX_subset=X[[ 'parkingSpaces','lat','lon','suites','usableAreas','bathrooms','bedrooms','yearlyIptu','totalAreas','neighborhood']]\ny=X['price']\n\n#fill missing data\ndef feature_fill(df):\n    df.totalAreas=df['totalAreas'].fillna(df['usableAreas'])\n    df.usableAreas=df['usableAreas'].fillna(df['totalAreas'])\n    df.lat=df['lat'].fillna(df['lat'].mean())\n    df.lon=df['lon'].fillna(df['lon'].mean())\n    df.suites=df['suites'].fillna(0)\n    df.parkingSpaces=df['parkingSpaces'].fillna(0)\n    df.bedrooms=df['bedrooms'].fillna(1)\n    df.bathrooms=df['bathrooms'].fillna(1)\n    df.bathrooms = df['bathrooms'].replace([0,0.0], 1)\n    df.totalAreas=df['totalAreas'].fillna(df['totalAreas'].mean())\n    df.usableAreas=df['usableAreas'].fillna(df['usableAreas'].mean())\n    df=df[[ 'parkingSpaces','lat','lon','suites','usableAreas','bathrooms','bedrooms','yearlyIptu','neighborhood']]\n    return df\n\nX_subset=feature_fill(X_subset)\n","eb603ee2":"X_subset.isna().sum()","8bd79bb3":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.cm as cm\nfrom random import randint\n\nX_data=X_subset[['parkingSpaces','lat','lon','suites','usableAreas','bathrooms','bedrooms','yearlyIptu']]\nX_data2=normalize(X_data)\npca=PCA()\npca.fit(X_data2)\nX_2=pca.transform(X_data2)\n\n\n##Plot PCA\nplt.plot(range(1,len(pca.components_)+1),pca.explained_variance_ratio_,'-o')\nplt.xlabel('components')\nplt.ylabel('% explained variance')\nplt.title(\"Screen plot\")\nplt.show()\n","d2113984":"def find_relevant_variable(component, threshold):\n    variables = []\n    weights = []\n    for i in range (0,len(component)):\n        if abs(component[i])>threshold:\n            variables.append(X_data.columns[i])\n            weights.append(component[i])\n    return variables, weights","c9d57cf8":"print(find_relevant_variable(pca.components_[0],0.1)[0])\n","189bb09d":"def retain_explanatory_components(pca, threshold):\n    components = []\n    importance = []\n    \n    for i in range (0,len(pca.components_)):\n        if pca.explained_variance_[i]>threshold:\n            components.append(pca.components_[i])\n            importance.append(pca.explained_variance_ratio_[i])\n    return components, importance","7397e32a":"print(pca.components_[2], pca.explained_variance_ratio_[2])","6c03d1e4":"X_original=X_subset[['parkingSpaces','lat','lon','suites','usableAreas','bathrooms','bedrooms','yearlyIptu','neighborhood']]\nX_simplified=X_subset[['usableAreas','lat','lon','yearlyIptu','neighborhood']]\n","5f4ae8e3":"from sklearn import metrics, svm\nfrom sklearn.metrics import mean_squared_error,  mean_absolute_error, r2_score\nfrom sklearn.linear_model import Lasso, ElasticNetCV, SGDRegressor, LinearRegression, Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_simplified, y, test_size=0.25, random_state=42)\n#grid_params ={'n_neighbors':[9,10,15,20],\n#              'weights':['uniform','distance'],\n#              'metric':['euclidean','manhattan']}\n#neigh = GridSearchCV(KNeighborsRegressor(),grid_params,verbose=1,cv=3,n_jobs=8)\n\n\n#neigh=KNeighborsRegressor(metric='manhattan', n_neighbors=12, weights='distance', n_jobs=8)\n#neigh.fit(X_train, y_train)\n#y_pred=neigh.predict(X_test)\n\n#did lasso and elastic net regression, but with poor results.\n#lasso can return the most important features in prediction\n\n#lasso=Lasso(alpha=0.0001)\n#lasso.fit(X_train,y_train)\n#y_pred=lasso.predict(X_test)\n\n\n#numeric_features = ['parkingSpaces','lat','lon','suites','usableAreas','bathrooms','bedrooms','yearlyIptu']\nnumeric_features = ['usableAreas','lat','lon','yearlyIptu']\nnumeric_transformer = Pipeline(steps=[(('scaler', StandardScaler())),])\ncategorical_features = ['neighborhood']\ncategorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(transformers=[\n     ('cat', categorical_transformer, categorical_features),\n        ('num', numeric_transformer, numeric_features)])\n\nneigh= Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', KNeighborsRegressor( leaf_size=30, metric='manhattan', n_neighbors=14, weights='distance', n_jobs=8))])\n\nneigh.fit(X_train, y_train)\ny_pred=neigh.predict(X_test)\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('R2 Score:', r2_score(y_test,y_pred)) \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n","ad5bcf04":"#Heteroscedasticity\nerrors=y_test-y_pred\nplt.scatter(X_test['usableAreas'],errors,color='b')\n","38b2ef9c":"#just the code to predict the prices on test data \nX_pred= pd.read_json('\/kaggle\/input\/real-estate-prediction-challenge\/source-4-ds-test.json',orient='columns',lines=True)\nX_pred=unnest_column(X_pred,nested)\nX_pred=feature_arrange(X_pred).fillna(0)\ny_pred_test=neigh.predict(X_pred)\ny_pred_test","57a6f874":"Don't forget to analyze the features. Not all of them are important.\nPCA Analysys","d34c1f5c":"This is a \"hire challenge\" wich I failed. \n\nThis is not The kernel I submitted. This is one I did later, the first one was messy.\n\nThe challenge consists in predict the apartment buindings prices at Sao Paulo, Brazil. \nI will be very glad if someone could tell me how to get a better score.\n\nMeanwhile, I'll give my 2 cents about what I learned on this challenge\n\n1. Keep your notebook clean and neat\n\nThose challenges are not a competition, they will evaluate your notebook as a whole, not just the score. \nWrite it to be easy to predict new data and write as it will be deployed\n\n2. Write good code\n\nTakes time at the beggining, but saves you a lot when playing with the features and hyperparameters. **The developer saves the data scientist's time**. Write functions and reuse the code always it's possible\n\n3. Learn how to use the [so called scikit-learn gems ](https:\/\/heartbeat.fritz.ai\/some-essential-hacks-and-tricks-for-machine-learning-with-python-5478bc6593f2 ) \n\nPipeline, Grid-search, One-hot encoding and Polynomial Feature Generation makes your life easier. Learn how to use and abuse of these resources\n\n4. Play with features\nOnce you get it working, will be easy to try diferent feature sets, transformations and grid search. Split the data if it takes to much time. \n\n\nWhat else could I do to improve my score?\nThank you."}}