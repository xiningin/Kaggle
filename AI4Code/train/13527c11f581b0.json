{"cell_type":{"71b2e711":"code","61b49b31":"code","b78b0535":"code","cde229cb":"code","d65c4554":"code","f715e561":"code","8792b110":"code","eed744ae":"code","118a6b4c":"code","0906df46":"code","767d77fe":"code","71a284e5":"code","cc957900":"code","90c43768":"code","0891bada":"code","7f732f7c":"code","8bec688a":"code","65c041eb":"code","44c00600":"code","32f6642e":"code","ae102462":"code","aa922fab":"markdown","3cc99ee8":"markdown","8dc55d35":"markdown","3bf55842":"markdown","39d7910d":"markdown","940a1ddc":"markdown","4034620b":"markdown","237c5832":"markdown","6b2f1bae":"markdown","732cf197":"markdown","d0536c41":"markdown","c7088d2a":"markdown","3df378a3":"markdown","b743cea2":"markdown","0ee0d713":"markdown","655e130b":"markdown","23e52c3c":"markdown","847de1e9":"markdown","de04e65c":"markdown","80d80e61":"markdown","a7cdf084":"markdown"},"source":{"71b2e711":"# load libraries\nimport os\n####*IMPORANT*: Have to do this line *before* importing tensorflow\nos.environ['PYTHONHASHSEED']=str(1)\n\nimport tensorflow as tf\nimport numpy as np\nimport random\n\ndef reset_random_seeds():\n   os.environ['PYTHONHASHSEED']=str(1)\n   tf.random.set_seed(1)\n   np.random.seed(1)\n   random.seed(1)\n    \nreset_random_seeds()\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom keras import models\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.layers import Dropout, Flatten, Input, Dense","61b49b31":"#https:\/\/www.kaggle.com\/ibtesama\/siim-baseline-keras-vgg16\ntrain_dir='\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'\ntest_dir='\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/test\/'\ntrain=pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/test.csv')\nsubmission=pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\ntrain.head()","b78b0535":"labels=[]\ndata=[]\nfor i in range(train.shape[0]):\n    data.append(train_dir + train['image_name'].iloc[i]+'.jpg')\n    labels.append(train['target'].iloc[i])\ndf=pd.DataFrame(data)\ndf.columns=['images']\ndf['target']=labels\n\ntest_data=[]\nfor i in range(test.shape[0]):\n    test_data.append(test_dir + test['image_name'].iloc[i]+'.jpg')\ndf_test=pd.DataFrame(test_data)\ndf_test.columns=['images']\n\nX_train, X_val, y_train, y_val = train_test_split(df['images'],df['target'], test_size=0.2, random_state=1234)\n\ntrain=pd.DataFrame(X_train)\ntrain.columns=['images']\ntrain['target']=y_train\n\nvalidation=pd.DataFrame(X_val)\nvalidation.columns=['images']\nvalidation['target']=y_val\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,horizontal_flip=True)\nval_datagen=ImageDataGenerator(rescale=1.\/255)\ntrain_generator = train_datagen.flow_from_dataframe(\n    train,\n    x_col='images',\n    y_col='target',\n    target_size=(224, 224),\n    batch_size=8,\n    shuffle=True,\n    class_mode='raw')\n\nvalidation_generator = val_datagen.flow_from_dataframe(\n    validation,\n    x_col='images',\n    y_col='target',\n    target_size=(224, 224),\n    shuffle=False,\n    batch_size=8,\n    class_mode='raw')","cde229cb":"model = models.Sequential()\nmodel.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation='relu',\n                        input_shape=(224, 224, 3)))\nmodel.add(layers.MaxPooling2D((2, 2),strides=2))\nmodel.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2),strides=2))\nmodel.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2),strides=2))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(units=1, activation='sigmoid'))\nmodel.summary()","d65c4554":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nfit = model.fit_generator(train_generator, steps_per_epoch=5, epochs=2,\n                         validation_data=validation_generator, validation_steps=5)","f715e561":"metrics = list(fit.history.keys())\nloss_values = fit.history[metrics[2]]\nval_loss_values = fit.history[metrics[0]]\nacc_values = fit.history[metrics[3]]\nval_acc_values = fit.history[metrics[1]]\nprint(\"\\nFinal validation loss function is\", val_loss_values[-1])\nprint(\"Final validation accuracy is\", val_acc_values[-1])\n\n# summarize history for accuracy\nplt.plot(fit.history['accuracy'])\nplt.plot(fit.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(fit.history['loss'])\nplt.plot(fit.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","8792b110":"# Extracts the outputs of the all the layers\nlayer_outputs = [layer.output for layer in model.layers]\n# Creates a model that will return these outputs, given the model input:\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n\nlayer_names = []\nfor layer in model.layers:\n    layer_names.append(layer.name)\n    \nlayer_names","eed744ae":"import cv2\n\nimg=cv2.imread(validation.images[0])\nimg = cv2.resize(img, (224,224))\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg = img.astype(np.float32)\/255.\n\nclass_names = ['Benign', 'Malignant']\n\nplt.imshow(img)\nplt.axis('off')\nplt.title(class_names[validation.target[0]], fontsize=12)\nplt.show()","118a6b4c":"img1 = cv2.imread(validation.images[0])\/255.\nimg1 = cv2.resize(img1, (224, 224), 3)\nimg1 = img1.reshape((1, 224, 224, 3))\nimg1.shape","0906df46":"activations = activation_model.predict(img1)\nlen(activations)","767d77fe":"first_layer_activation = activations[0]\nprint(first_layer_activation.shape)","71a284e5":"plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')\nplt.show()","cc957900":"plt.matshow(first_layer_activation[0, :, :, 27], cmap='viridis')\nplt.show()","90c43768":"plt.matshow(first_layer_activation[0, :, :, 10], cmap='viridis')\nplt.show()","0891bada":"layer_names = []\nfor layer in model.layers:\n    layer_names.append(layer.name)\n\nimages_per_row = 16\n\n# Now let's display our feature maps\nfor layer_name, layer_activation in zip(layer_names, activations):\n    \n    if layer_name == 'flatten': \n        break\n    # This is the number of features in the feature map\n    n_features = layer_activation.shape[-1]\n\n    # The feature map has shape (1, size, size, n_features)\n    size = layer_activation.shape[1]\n\n    # We will tile the activation channels in this matrix\n    n_cols = n_features \/\/ images_per_row\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n    # We'll tile each filter into this big horizontal grid\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            # Post-process the feature to make it visually palatable\n            channel_image -= channel_image.mean()\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size,\n                         row * size : (row + 1) * size] = channel_image\n\n    # Display the grid\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n    \nplt.show()","7f732f7c":"img=cv2.imread(validation.images.iloc[32])\nimg = cv2.resize(img, (224,224))\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg = img.astype(np.float32)\/255.\n\nclass_names = ['Benign', 'Malignant']\n\nplt.imshow(img)\nplt.axis('off')\nplt.title(class_names[validation.target.iloc[32]], fontsize=12)\nplt.show()","8bec688a":"img1 = cv2.imread(validation.images.iloc[32])\/255.\nimg1 = cv2.resize(img1, (224, 224), 3)\nimg1 = img1.reshape((1, 224, 224, 3))\nprint(img1.shape)\n\nactivations = activation_model.predict(img1)\nprint(len(activations))\n\nfirst_layer_activation = activations[0]\nprint(first_layer_activation.shape)","65c041eb":"plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')\nplt.show()","44c00600":"plt.matshow(first_layer_activation[0, :, :, 27], cmap='viridis')\nplt.show()","32f6642e":"plt.matshow(first_layer_activation[0, :, :, 10], cmap='viridis')\nplt.show()","ae102462":"layer_names = []\nfor layer in model.layers:\n    layer_names.append(layer.name)\n\nimages_per_row = 16\n\n# Now let's display our feature maps\nfor layer_name, layer_activation in zip(layer_names, activations):\n    \n    if layer_name == 'flatten': \n        break\n    # This is the number of features in the feature map\n    n_features = layer_activation.shape[-1]\n\n    # The feature map has shape (1, size, size, n_features)\n    size = layer_activation.shape[1]\n\n    # We will tile the activation channels in this matrix\n    n_cols = n_features \/\/ images_per_row\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n    # We'll tile each filter into this big horizontal grid\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            # Post-process the feature to make it visually palatable\n            channel_image -= channel_image.mean()\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size,\n                         row * size : (row + 1) * size] = channel_image\n\n    # Display the grid\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n    \nplt.show()","aa922fab":"It is a 222x222 feature map with 32 channels.  Below we can visualize some of the different channels:  ","3cc99ee8":"## Visualize Activations - Benign\n\nFirst lets extract all of the layer names from our network:","8dc55d35":"# Visualizing Activation from a CNN\n\n![image.png](attachment:image.png)\n\nSkin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection\u2014potentially aided by data science\u2014can make treatment more effective.\n\nCurrently, dermatologists evaluate every one of a patient's moles to identify outlier lesions or \u201cugly ducklings\u201d that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account \u201ccontextual\u201d images within the same patient to determine which images represent a melanoma. If successful, classifiers would be more accurate and could better support dermatological clinic work.\n\nAs the leading healthcare organization for informatics in medical imaging, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation in a multi-disciplinary community. SIIM is joined by the International Skin Imaging Collaboration (ISIC), an international effort to improve melanoma diagnosis. The ISIC Archive contains the largest publicly available collection of quality-controlled dermoscopic images of skin lesions.\n\nIn this competition, you\u2019ll identify melanoma in images of skin lesions. In particular, you\u2019ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists.\n\nMelanoma is a deadly disease, but if caught early, most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people.","3bf55842":"## Load Data","39d7910d":"The deeper the layer of the neural network, the more abstract the features become. The first layer detects edges.  This particular network is very sparse, and become more so as the depth increases (blank filters).  This means the pattern encoded by the filter isn't in the input image.   ","940a1ddc":"## Preprocess images","4034620b":"## Purpose of this notebook\n\nThe purpose of this notebook is to show how convnets view malignant and benign moles within this competition.  \n\nFran\u00e7ois Chollet states, \"It is often said that deep learning models are \"black boxes\", learning representations that are difficult to extract and present in a human-readable form. While this is partially true for certain types of deep learning models, it is definitely not true for convnets. The representations learned by convnets are highly amenable to visualization, in large part because they are representations of visual concepts. Since 2013, a wide array of techniques have been developed for visualizing and interpreting these representations.\" \n\nThis notebook will focus specifically on activation visualization, we will train a small convnet and visualize how this networks views different images.  \n\nThis notebook has been adapted from https:\/\/github.com\/fchollet\/deep-learning-with-python-notebooks\/blob\/master\/5.4-visualizing-what-convnets-learn.ipynb.  ","237c5832":"The 5th channel looks like it is detecting hair (see green lines around hair) or the darker color, with the image of the mole basically removed.   ","6b2f1bae":"The 28th channel is focusing on the color white, just like in the benign image.","732cf197":"Similar to the benign mole, the 5th channel in the malignant image confirms that this channel is detecting a darker color, not hair.  ","d0536c41":"The 11th channel appears to focus on the mole itself.","c7088d2a":"Next we can plot a visualization of all the activations within the netowrk.  ","3df378a3":"## Evaluate Convnet","b743cea2":"## Now Lets Visualize A Malignant Mole\n## Visualize Activations - Malignant","0ee0d713":"## Train Convnet","655e130b":"This is the activation of the first convolution layer for the benign image:","23e52c3c":"The 11th channel is focusing on lighter colors moles, and not the darker color malignant mole.","847de1e9":"## Conclusion\n\nVizualizing activations of a convnet is useful to understand the \"black box\".  As Fran\u00e7ois Chollet stated, and we have now seen, the represenations learned by convets are able to be visualized fairly easily because they are representations of visual concepts.  ","de04e65c":"## Build Convnet","80d80e61":"The 28th channel appears to focus on the white coloring around the mole on the right side.  ","a7cdf084":"This is the input image that will be used for the convnet visualization.  It is a picture, from the validation set, of a benign mole.  The model is fed the input image, which then returns the layer activations of the original modle.  The model has 1 input and 8 outputs.  "}}