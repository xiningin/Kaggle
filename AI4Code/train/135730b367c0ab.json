{"cell_type":{"4155c201":"code","5e0281d0":"code","d294b2d5":"code","7df52d19":"code","ff93bc39":"code","898fec46":"code","028219ac":"code","162db2b0":"code","a388c480":"code","f81694aa":"code","3c3eb75f":"code","8ab331b9":"code","822e4a36":"code","1d52cfe7":"code","cd70a729":"code","95f125d5":"code","de433136":"code","1ee5c511":"code","ef1cefcf":"code","10a5cf9b":"code","eda911b2":"code","59228632":"code","31907f73":"code","0a9f27f0":"code","12e45b1a":"code","6cbce0f5":"code","8bec7d24":"code","74c096f1":"code","2c2f097f":"code","19ed6e83":"code","8ad61021":"code","1ded6a1d":"code","ca47ed74":"code","cc55ba61":"code","b3f7fdf7":"code","07586e45":"code","cc1c4537":"code","316686ef":"code","3af96680":"code","4dfa4184":"code","7ca20e90":"code","918babc6":"code","f7c53508":"code","edfb8fe6":"code","1653eefb":"code","f8f985ce":"code","146b4bb2":"markdown","863db164":"markdown","e41094cf":"markdown","e75fb4ce":"markdown","64ef9873":"markdown","ccf2483c":"markdown","84914e56":"markdown","60774c88":"markdown","7f279140":"markdown","26e87786":"markdown","cd604934":"markdown","2a6efa5d":"markdown","3ae8e3f6":"markdown","a3b020f2":"markdown","142072e8":"markdown","79285edf":"markdown","2646bc44":"markdown","b08b3d8a":"markdown","8e123879":"markdown","f290d0ce":"markdown","9af4b262":"markdown","05206d76":"markdown","55cca114":"markdown","0d3d5e36":"markdown","4354bbd8":"markdown","50d0ffbb":"markdown","becdd016":"markdown","9447e388":"markdown","56e85e0b":"markdown","1205c489":"markdown","b64ed7e6":"markdown","db1c4a51":"markdown"},"source":{"4155c201":"import numpy as np \nimport pandas as pd \nimport nltk\nimport string as s\nimport re\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom sklearn.feature_extraction.text  import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics  import f1_score,accuracy_score\nfrom sklearn.metrics import  confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom lightgbm import LGBMClassifier","5e0281d0":"train_data=pd.read_csv(\"\/kaggle\/input\/ag-news-classification-dataset\/train.csv\",header=0,names=['classid','title','desc'])\ntest_data=pd.read_csv(\"\/kaggle\/input\/ag-news-classification-dataset\/test.csv\",header=0,names=['classid','title','desc'])","d294b2d5":"train_data.head()","7df52d19":"test_data.head()","ff93bc39":"train_x=train_data.desc[:70000]\ntest_x=test_data.desc\ntrain_y=train_data.classid[:70000] \ntest_y=test_data.classid","898fec46":"df=train_data[:70000]\nsns.countplot(df.classid);","028219ac":"from PIL import Image\nc_mask = np.array(Image.open(\"\/kaggle\/input\/masks\/comment.png\"))\nu_mask = np.array(Image.open(\"\/kaggle\/input\/masks\/upvote.png\"))","162db2b0":"stopwordset= set(STOPWORDS)\nmorestop={'lt','gt','href','HREF','quot','aspx'}\nstopwordset= stopwordset.union(morestop)","a388c480":"world = df.desc[df.classid[df.classid==1].index]","f81694aa":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 500 ,background_color=\"white\",contour_width=3, contour_color='firebrick',mask=c_mask, stopwords=stopwordset,colormap='Dark2').generate(\" \".join(world))\nplt.imshow(wordcloud,interpolation = 'bilinear');\nplt.axis(\"off\");","3c3eb75f":"sports = df.desc[df.classid[df.classid==2].index]","8ab331b9":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 500 ,background_color=\"white\",contour_width=3, contour_color='skyblue',mask=u_mask, stopwords=stopwordset,colormap='Dark2').generate(\" \".join(sports))\nplt.imshow(wordcloud,interpolation = 'bilinear');\nplt.axis(\"off\");","822e4a36":"biz = df.desc[df.classid[df.classid==3].index]","1d52cfe7":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 500 ,background_color=\"white\",contour_width=3, contour_color='firebrick',mask=c_mask, stopwords=stopwordset,colormap='Dark2').generate(\" \".join(biz))\nplt.imshow(wordcloud,interpolation = 'bilinear');\nplt.axis(\"off\");","cd70a729":"sci = df.desc[df.classid[df.classid==4].index]","95f125d5":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 500 ,background_color=\"white\",contour_width=3, contour_color='skyblue',mask=u_mask, stopwords=stopwordset,colormap='Dark2').generate(\" \".join(sci))\nplt.imshow(wordcloud,interpolation = 'bilinear');\nplt.axis(\"off\");","de433136":"def remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ntrain_x=train_x.apply(remove_urls)\ntest_x=test_x.apply(remove_urls)","1ee5c511":"def remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ntrain_x=train_x.apply(remove_html)\ntest_x=test_x.apply(remove_html)","ef1cefcf":"def word_tokenize(txt):\n    tokens = re.findall(\"[\\w']+\", txt)\n    return tokens\ntrain_x=train_x.apply(word_tokenize)\ntest_x=test_x.apply(word_tokenize)","10a5cf9b":"def lowercasing(lst):\n    new_lst=[]\n    for  i in  lst:\n        i=i.lower()\n        new_lst.append(i) \n    return new_lst\ntrain_x=train_x.apply(lowercasing)\ntest_x=test_x.apply(lowercasing)","eda911b2":"def remove_stopwords(lst):\n    stop=stopwords.words('english')\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_stopwords)\ntest_x=test_x.apply(remove_stopwords)  ","59228632":"def remove_punctuations(lst):\n    new_lst=[]\n    for i in lst:\n        for  j in  s.punctuation:\n            i=i.replace(j,'')\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_punctuations) \ntest_x=test_x.apply(remove_punctuations)","31907f73":"def remove_numbers(lst):\n    nodig_lst=[]\n    new_lst=[]\n\n    for i in  lst:\n        for j in  s.digits:\n            i=i.replace(j,'')\n        nodig_lst.append(i)\n    for i in  nodig_lst:\n        if  i!='':\n            new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_numbers)\ntest_x=test_x.apply(remove_numbers)","0a9f27f0":"import nltk\n\ndef stemming(text):\n    porter_stemmer = nltk.PorterStemmer()\n    roots = [porter_stemmer.stem(each) for each in text]\n    return (roots)\n\ntrain_x=train_x.apply(stemming)\ntest_x=test_x.apply(stemming)","12e45b1a":"lemmatizer=nltk.stem.WordNetLemmatizer()\ndef lemmatzation(lst):\n    new_lst=[]\n    for i in lst:\n        i=lemmatizer.lemmatize(i)\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lemmatzation)\ntest_x=test_x.apply(lemmatzation)","6cbce0f5":"def remove_extrawords(lst):\n    stop=['href','lt','gt','ii','iii','ie','quot','com']\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_extrawords)\ntest_x=test_x.apply(remove_extrawords)","8bec7d24":"train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\ntest_x=test_x.apply(lambda x: ''.join(i+' '  for i in x))","74c096f1":"tfidf=TfidfVectorizer(min_df=3)\ntrain_1=tfidf.fit_transform(train_x)\ntest_1=tfidf.transform(test_x)\nprint(\"No. of features extracted\")\nprint(len(tfidf.get_feature_names()))\n\n\ntrain_arr=train_1.toarray()\ntest_arr=test_1.toarray()","2c2f097f":"NB_MN=MultinomialNB(alpha=0.16)\nNB_MN.fit(train_arr,train_y)\npred=NB_MN.predict(test_arr)\n\nprint(\"first 20 actual labels\")\nprint(test_y.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","19ed6e83":"def eval_model(y,y_pred):\n    print(\"F1 score of the model\")\n    print(f1_score(y,y_pred,average='micro'))\n    print(\"Accuracy of the model\")\n    print(accuracy_score(y,y_pred))\n    print(\"Accuracy of the model in percentage\")\n    print(round(accuracy_score(y,y_pred)*100,3),\"%\")","8ad61021":"def confusion_mat(color):\n    cof=confusion_matrix(test_y, pred)\n    cof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])\n    sns.set(font_scale=1.5)\n    plt.figure(figsize=(8,8));\n\n    sns.heatmap(cof, cmap=color,linewidths=1, annot=True,square=True, fmt='d', cbar=False,xticklabels=['World','Sports','Business','Science'],yticklabels=['World','Sports','Business','Science']);\n    plt.xlabel(\"Predicted Classes\");\n    plt.ylabel(\"Actual Classes\");\n    ","1ded6a1d":"eval_model(test_y,pred)\n    \na=round(accuracy_score(test_y,pred)*100,3)","ca47ed74":"confusion_mat('YlGnBu')","cc55ba61":"DT=DecisionTreeClassifier()\nDT.fit(train_arr,train_y)\npred=DT.predict(test_arr)\n\nprint(\"first 20 actual labels\")\nprint(test_y.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","b3f7fdf7":"eval_model(test_y,pred)\n    \nb=round(accuracy_score(test_y,pred)*100,3)\n","07586e45":"confusion_mat('Blues')","cc1c4537":"NB=GaussianNB(var_smoothing=0.1)\nNB.fit(train_arr,train_y)\npred=NB.predict(test_arr)","316686ef":"eval_model(test_y,pred)\n    \nc=round(accuracy_score(test_y,pred)*100,3)","3af96680":"confusion_mat('Greens')","4dfa4184":"SGD=SGDClassifier(early_stopping=True,penalty='l2',alpha=0.00001)\nSGD.fit(train_arr,train_y)\npred=SGD.predict(test_arr)","7ca20e90":"eval_model(test_y,pred)\n    \nd=round(accuracy_score(test_y,pred)*100,3)","918babc6":"confusion_mat('Reds')","f7c53508":"lgbm=LGBMClassifier(learning_rate=0.35)\nlgbm.fit(train_arr,train_y)\npred=lgbm.predict(test_arr)","edfb8fe6":"eval_model(test_y,pred)\n\ne=round(accuracy_score(test_y,pred)*100,3)","1653eefb":"confusion_mat('YlOrBr')","f8f985ce":"sns.set()\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nModels = ['MultinomialNB', 'DecisionTree', 'GaussianNB', 'SGD','LGBM']\nAccuracy=[a,b,c,d,e]\nax.bar(Models,Accuracy,color=['#702963','#8a2be2','#9966cc','#df73ff','#702763']);\nfor i in ax.patches:\n    ax.text(i.get_x()+.1, i.get_height()-5.5, str(round(i.get_height(),2))+'%', fontsize=15, color='white')\nplt.title('Comparison of Different Classification Models');\nplt.ylabel('Accuracy');\nplt.xlabel('Classification Models');\n\nplt.show();","146b4bb2":"## Removal of Punctuation Symbols","863db164":"### Evaluation of Results","e41094cf":"# Training of Model\n\n### Model 1- Multinomial Naive Bayes","e75fb4ce":"### Model 5 - Light Gradient Boosting Classifier","64ef9873":"### Importing libraries","ccf2483c":"### Model 4 - Stochastic Gradient Descent Classifier","84914e56":"**Function for Displaying the Confusion Matrix**\n\nThis function displays the confusion matrix of the model","60774c88":"## Conversion of Data to Lowercase","7f279140":"## Removal of Stopwords","26e87786":"### World News","cd604934":"## Splitting Data into Input and Label ","2a6efa5d":"## Lemmatization of Data","3ae8e3f6":"### Sports News","a3b020f2":"### Evaluation of Results","142072e8":"### Model 3 - Gaussian Naive Bayes","79285edf":"# Preprocessing of Data\n\nThe data is preprocessed, in NLP it is also known as text normalization. Some of the most common methods of text normalization are \n* Tokenization\n* Lemmatization\n* Stemming\n","2646bc44":"## Removal of Numbers(digits)","b08b3d8a":"## Comparison of Accuracies of Different Models","8e123879":"**Function for evaluation of model**\n\nThis function finds the F1-score and Accuracy of the trained model","f290d0ce":"## Feature Extraction\n \n Features are extracted from the dataset and TF-IDF(Term Frequency - Inverse Document Frequency) is used for this purpose.","9af4b262":"### Evaluation of Model","05206d76":"# Classification of News Articles \n\nIt is a notebook for multiclass classification of News articles which are having classes numbered 1 to 4, where \n>1 is \"World News\" <br>\n>2 is \"Sports News\" <br>\n>3 is \"Business News\" <br>\n>4 is \"Science-Technology News\"\n\nI have used various models for classification of the News articles. The classification algorithms used are:-\n\n1. Multinomial Naive Bayes\n2. Decision Tree \n3. Gaussian Naive Bayes\n4. Stochastic Gradient Descent Classifier\n5. LGBM (light gradient boosting machine) Classifier\n","55cca114":"## Removal of HTML tags","0d3d5e36":"### Science and Technology News","4354bbd8":"## Removal of URLs","50d0ffbb":"## Stemming of Data","becdd016":"### Evaluation of Model","9447e388":"### Evaluation of Results","56e85e0b":"### Business News","1205c489":"## WordCloud of News Articles of Different Types","b64ed7e6":"## Tokenization of Data","db1c4a51":"### Model 2 - Decision Tree Classifier"}}