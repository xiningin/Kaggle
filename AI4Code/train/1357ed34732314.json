{"cell_type":{"acfe1b44":"code","fde43943":"code","a34bf445":"code","853f69fc":"code","a4dec89f":"code","47cd6630":"code","3d5b3c75":"code","0189b9aa":"code","53e940d0":"code","c9370473":"markdown","12d6921c":"markdown","b4a8de1c":"markdown","65547bd2":"markdown","514ccb64":"markdown","c2e1642a":"markdown","74aadba3":"markdown","ff8fd70e":"markdown","f6745e22":"markdown","ac530a7d":"markdown","131e2701":"markdown","d2a0b3e4":"markdown","0a6e38fb":"markdown","289b6ea4":"markdown","e9a99e7e":"markdown","57a6c6cf":"markdown","95622373":"markdown","2af711d7":"markdown","00966e6b":"markdown","9d86dbe4":"markdown","31c5b542":"markdown"},"source":{"acfe1b44":"#Numpy is used so that we can deal with array's, which are necessary for any linear algebra\n# that takes place \"under-the-hood\" for any of these algorithms.\n\nimport numpy as np\n\n\n#Pandas is used so that we can create dataframes, which is particularly useful when\n# reading or writing from a CSV.\n\nimport pandas as pd\n\n\n#Matplotlib is used to generate graphs in just a few lines of code.\n\nimport matplotlib.pyplot as plt\n\n\n#Sklearn is a very common library that allows you to implement most basic ML algorithms.\n#Train_test_split will allow us to quickly split our dataset into a training set and a test set.\n\nfrom sklearn.model_selection import train_test_split\n\n\n#LinearRegression is the class of the algorithm we will be using.\n\nfrom sklearn.linear_model import LinearRegression\n\n\n#This will allow us to evaluate our fit using the R^2 score. \n\nfrom sklearn.metrics import r2_score\n","fde43943":"#read dataset from csv\ndataset = pd.read_csv('\/kaggle\/input\/sample-salary-data\/Salary_Data.csv')\n\n#set independent variable using all rows, and all columns except for the last one.\nX = dataset.iloc[:, :-1].values\n\n#set the dependent variable using all rows, but ony the last column.\ny = dataset.iloc[:, 1].values\n\n#Lets look at our data\ndataset","a34bf445":"#This will create x and y variables for training and test sets.\n#Here we are using 25% of our examples for the test set.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","853f69fc":"#this sets the object regressor to the class of LinearRegression from the Sklearn library.\nregressor = LinearRegression()\n\n#this fits the model to our training data.\nregressor.fit(X_train, y_train)","a4dec89f":"#Predict on our test set.\ny_pred = regressor.predict(X_test)","47cd6630":"#here is the function, we simply pass in the x and y we want to plot.\n\ndef plot_results(x,y):\n    plt.scatter(x, y, color = 'red')\n    plt.plot(x, regressor.predict(x), color = 'blue')\n    plt.title('Salary vs Experience')\n    plt.xlabel('Years of Experience')\n    plt.ylabel('Salary')\n    plt.show()\n    ","3d5b3c75":"#Visualize the training set\n\nplot_results(X_train, y_train)","0189b9aa":"#Visualize the test set\n\nplot_results(X_test, y_test)","53e940d0":"#calculate the R^2 score\nscore = r2_score(y_test, y_pred)\n\n#print out our score properly formatted as a percent.\nprint(\"R^2 score:\", \"{:.0%}\".format(score))","c9370473":"### Conceptual Overview\n\nLinear Regression is **\u201csupervised\u201d  \u201cregression\u201d** algorithm.\n\n\n![classic2.png](attachment:classic2.png)","12d6921c":"We do this for both the \u201cslope\u201d and the \u201cintercept\u201d, which ultimately gives us our best fitting line.\n\n![best_fit.png](attachment:best_fit.png)","b4a8de1c":"Finally, we will calculate the R^2 score, which tells us how much ofthe variation in our dependent variable can be explained by our independent variable. ","65547bd2":"Implementing this algorithm, we fit a line using **\u201cleast squares\u201d**, where we try to minimize the squared distances between the data-points and our line.\n\n![good-bad.png](attachment:good-bad.png)","514ccb64":"We want the point at which the loss is minimized, which we find by taking the \u201cderivative\u201d of our \u201closs function\u201d.  Simply, this tells us the slope at any given point, so we calculate the value where the slope = 0.\n\n![slope2.png](attachment:slope2.png)","c2e1642a":"Repeating this step, we see how the loss changes based on the rotation.\n\n![repeat.png](attachment:repeat.png)","74aadba3":"With our data loaded, we now need to split the data into training and test sets.","ff8fd70e":"With our model built, we can now use it for generating predictions.\n\nWe will use our test set so we can see how well it did.","f6745e22":"We measure the distances between the points and the line, square them, then sum them.  We\u2019ll call this our \u201closs\u201d.\n\n![calc1.png](attachment:calc1.png)","ac530a7d":"With our imports complete, we now read in the data using Pandas.\n\nWe will set a independent variable (X) and a dependent variable (y).","131e2701":"Supervised meaning we use labeled data to train the model.\n\n![sup_vs_unsup.png](attachment:sup_vs_unsup.png)","d2a0b3e4":"> # Enough to be Dangeous: Simple Linear Regression\n\nThis is the 1st notebook of my \"Enough to be Dangeous\" notebook series\n\nSee the other notebooks here:\n\n[Multiple Linear Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-multiple-linear-regression)\n\n[Polynomial Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-polynomial-regression)\n\n\nThis notebook is separated into two parts:\n\n1) Conceptual Overview:  I will introduce the topic in 200 words or less.\n\n2) Implementation:  I will implement the algorithm in as few lines as possible. \n\n","0a6e38fb":"In its simplest form, we start by drawing any line through our data.\n\n![drawline.png](attachment:drawline.png)","289b6ea4":"Regression meaning we predict a numerical value, instead of a \u201cclass\u201d.\n\n![Class_vs_reg.png](attachment:Class_vs_reg.png)","e9a99e7e":"### Implementation\n\nIn this problem, we will look at the relationship between work experience and Salary.  This is a regression problem because we are hoping to be able to forecast the salary based on number of years of experience.  \n\nThe first step is to start with **\"imports\"**.  These are **\"libraries\"** of pre-written code that will help us significantly.","57a6c6cf":"Next, we rotate the line and calculate the loss. \n\n![rotate1.png](attachment:rotate1.png)","95622373":"We had an R^2 of 0.98, meaning that 98% of the variation in salary can be explained by changes in years of experience. \n","2af711d7":"Using the formula for our line, we can predict the output values for any input.\n\n![predict.png](attachment:predict.png)","00966e6b":"Now, its time to load the model.","9d86dbe4":"Finally, we will visualize the results.  \n\nI will build a function to reduce the amount of code needed, since we will use it twice.","31c5b542":"Overall, Linear Regression fast, intuitive, and a great starting point for simple regression problems, but its application is limited to 2-dimensional datasets."}}