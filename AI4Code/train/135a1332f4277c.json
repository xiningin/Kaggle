{"cell_type":{"cf9f2019":"code","6728d18c":"code","bb80386c":"code","2dfaeb2f":"code","fcecd1de":"code","4c7fe360":"code","2597a9c0":"code","4417af01":"code","04964831":"code","381440c1":"code","fa549f54":"code","d0602c63":"code","bbca46ed":"code","36821d46":"code","cabfb384":"code","a8ac41a0":"code","66e60e1f":"code","64e21039":"code","1377bdc5":"code","deb0df93":"code","c0d4ed57":"code","3d826ef5":"code","80556cf1":"code","6ce59884":"code","906f26d7":"code","5b60dbd4":"code","baad1d1d":"code","6b14dd87":"code","df9dfabc":"code","e48bb64e":"code","6e5fe695":"code","f27e0e5a":"code","aababbf8":"code","34f084a8":"code","4cba2940":"code","1e488ba6":"code","8fa56702":"code","fd6e709c":"markdown","6a326be3":"markdown"},"source":{"cf9f2019":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6728d18c":"train = pd.read_csv('..\/input\/cancer\/Train.csv')                                ","bb80386c":"train.head()","2dfaeb2f":"train.describe()","fcecd1de":"train.info()","4c7fe360":"#As there are only N and R, Label encoding of Outcome column wont cause any problem\nfrom sklearn.preprocessing import LabelEncoder\n\nle= LabelEncoder()\ntrain['Outcome']= le.fit_transform(train['Outcome'])","2597a9c0":"train['Outcome'].value_counts()","4417af01":"train=train.replace(['?'],'NaN')","04964831":"train.isnull().sum()","381440c1":"train['Lymph_Node_Status'].value_counts()","fa549f54":"#Since '0' is the mode for Lymph Node Status. We fill all NaN with '0'\ntrain['Lymph_Node_Status']= train['Lymph_Node_Status'].replace('NaN', int(0))","d0602c63":"train['Lymph_Node_Status']= train['Lymph_Node_Status'].astype(int)","bbca46ed":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorr= train.corr()\nf, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(corr,linewidths=.5, vmin=0, vmax=1,)","36821d46":"train.columns","cabfb384":"train.iloc[0,:]","a8ac41a0":"#Performing upsampling for R type of cancer\nfor i in range(0,198):\n    if train['Outcome'][i]== 1:\n        train = train.append(train.iloc[i,:], ignore_index= True)","66e60e1f":"from sklearn.utils import shuffle\ntrain = shuffle(train)\ntrain.reset_index(inplace=True, drop=True)","64e21039":"train.tail()","1377bdc5":"#Since the Areas and Perimeters are covered by radius, we'll drop them\ndrop= ['ID','perimeter_mean', 'area_mean','perimeter_std_dev', 'area_std_dev', 'Worst_perimeter', 'Worst_area']\ntrain1 = train.drop(drop, axis= 1)\ntrain = train.drop('ID', axis=1)","deb0df93":"corr1= train1.corr()\nf, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(corr1,linewidths=.5, vmin=0, vmax=1,)","c0d4ed57":"from scipy.stats import skew, kurtosis\nprint(skew(train1['Time']))\nprint(kurtosis(train1['Time']))\n\n\n#Inference-- Moderately skewed and Flat peak kurtosis which is not much problematic","3d826ef5":"plt.scatter(train['Tumor_Size'],train['Time'] )\n#we cant see any trend between the two","80556cf1":"plt.hist((train['Tumor_Size']), bins= 30)","6ce59884":"train","906f26d7":"#Checking outlier via Zscore\n\nfrom scipy import stats\nzscore= stats.zscore(train['Time']) \n\nfor z in zscore:\n    if z>=3 or z<=-3:\n        print('True')\n\n#Cannot see any outlier\n   ","5b60dbd4":"from scipy.special import inv_boxcox\ny,l= stats.boxcox(train['Time'])\nprint(inv_boxcox(y,l))","baad1d1d":"time1= train1['Time']\ntarget1= train1['Outcome']\ntrain1_ot= train1.drop(['Outcome','Time'], axis=1)\ntrain_ot= train.drop(['Outcome','Time'], axis=1)","6b14dd87":"from sklearn.preprocessing import RobustScaler as rs\ntrain11=rs().fit(train1_ot).transform(train1_ot) #with dropped features like Area and Perimeter\ntrain12= rs().fit(train_ot).transform(train_ot) #original dataset\n","df9dfabc":"from sklearn.model_selection import GridSearchCV as gsc,train_test_split as tts\nfrom sklearn.ensemble import RandomForestClassifier as rfc\n","e48bb64e":"train_1,valtrain1,target_1,valtarget1=tts(train12,target1,test_size=0.2, random_state= 42)","6e5fe695":"'''gsc= gsc(estimator= rfc(), \n                  param_grid= {'max_depth': range(5,11), \n                              'n_estimators': (50,40,30),\n                               'criterion' :['gini', 'entropy']\n                              }, cv=5, scoring= 'f1'\n                 , verbose=1, n_jobs= -1)\ngrid_result= gsc.fit(train_1, target_1)\nbest_params= grid_result.best_params_\nprint(best_params)'''","f27e0e5a":"clf=rfc(criterion = 'gini', max_depth= 8, n_estimators= 50)\n\nclf.fit(train_1, target_1)\npred= clf.predict(valtrain1)","aababbf8":"from sklearn.metrics import f1_score, confusion_matrix, mean_squared_error as mse\nprint(confusion_matrix(valtarget1, pred))","34f084a8":"from sklearn.model_selection import GridSearchCV as gsc\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(\n    objective= 'binary:logistic',\n    verbose=2, \n    subsample=0.6,\n    nthread=4,\n    seed=42\n)\nparameters = {\n    'max_depth': range (2, 10, 1),\n    'n_estimators': (50,30, 40),\n    'learning_rate': [0.1, 0.01, 0.05]\n}\n\ngrid_search = gsc(\n    estimator=xgb,\n    param_grid=parameters,\n    scoring = 'f1',\n    n_jobs = -1,\n    cv = 7,\n    verbose=True)\ngrid_result= grid_search.fit(train_1, target_1)\nbest_params= grid_result.best_params_\nprint(best_params)","4cba2940":"from xgboost import XGBClassifier\nxgb= XGBClassifier(learning_rate= 0.1, max_depth= 6, n_estimators=40, objective= 'binary:logistic',subsample=0.6)\nxgb.fit(train_1,target_1)\npre= xgb.predict(valtrain1)\nprint(f1_score(valtarget1, pre))","1e488ba6":"print(confusion_matrix(valtarget1, pre))","8fa56702":"train1_t= train1.drop(['Time'], axis=1)\ntrain_t= train.drop(['Time'], axis=1)","fd6e709c":"# We will consider that time is not given while building the classifier as practically we cant tell the time until we dont know whether the type of cancer is N or R","6a326be3":"# While building regressor, we assume that we know the type of cancer (N\/R) and that's why only Time is dropped below"}}