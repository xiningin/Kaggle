{"cell_type":{"263b7425":"code","91db4fa3":"code","6ab926e6":"code","0691e86d":"code","290d7d3d":"code","d5d18e9a":"code","99f8ea42":"code","35a7c1bd":"code","d2997bd8":"code","f3ff3cf9":"code","efe0ac88":"code","d15293a2":"code","5182ff3a":"code","2221765b":"code","ed00650d":"code","65f3d501":"code","12c4a61e":"code","77f74300":"code","9be68f92":"code","1260f1ba":"code","bef8f703":"code","eea8481f":"code","a85a3c6f":"code","d9b1674a":"code","eb3278ef":"code","3de9b302":"code","90063581":"code","a43726d4":"markdown","dcaecbf0":"markdown","2301d018":"markdown","a4ec7777":"markdown","608cc956":"markdown","9f03e486":"markdown","63b53c72":"markdown","cb7c45b1":"markdown","f476288e":"markdown","dc9ed696":"markdown","f364dca9":"markdown","178efac5":"markdown","1cede4ef":"markdown"},"source":{"263b7425":"import numpy as np \nimport pandas as pd\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \nfrom wordcloud import WordCloud, STOPWORDS","91db4fa3":"tweets_df = pd.read_csv(\"\/kaggle\/input\/omicron-rising\/omicron.csv\")","6ab926e6":"print(f\"data shape: {tweets_df.shape}\")","0691e86d":"tweets_df.info()","290d7d3d":"tweets_df.describe()","d5d18e9a":"tweets_df.head()","99f8ea42":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","35a7c1bd":"missing_data(tweets_df)","d2997bd8":"def unique_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    uniques = []\n    for col in data.columns:\n        unique = data[col].nunique()\n        uniques.append(unique)\n    tt['Uniques'] = uniques\n    return(np.transpose(tt))","f3ff3cf9":"unique_values(tweets_df)","efe0ac88":"def most_frequent_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    items = []\n    vals = []\n    for col in data.columns:\n        itm = data[col].value_counts().index[0]\n        val = data[col].value_counts().values[0]\n        items.append(itm)\n        vals.append(val)\n    tt['Most frequent item'] = items\n    tt['Frequence'] = vals\n    tt['Percent from total'] = np.round(vals \/ total * 100, 3)\n    return(np.transpose(tt))","d15293a2":"most_frequent_values(tweets_df)","5182ff3a":"def plot_count(feature, title, df, size=1, ordered=True):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    if ordered:\n        g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    else:\n        g = sns.countplot(df[feature], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()    ","2221765b":"plot_count(\"user_name\", \"User name\", tweets_df,4)","ed00650d":"plot_count(\"user_location\", \"User location\", tweets_df,4)","65f3d501":"plot_count(\"source\", \"Source\", tweets_df,4)","12c4a61e":"stopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=50,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","77f74300":"show_wordcloud(tweets_df['text'], title = 'Prevalent words in tweets')","9be68f92":"india_df = tweets_df.loc[tweets_df.user_location==\"India\"]\nshow_wordcloud(india_df['text'], title = 'Prevalent words in tweets from India')","1260f1ba":"us_df = tweets_df.loc[tweets_df.user_location==\"United States\"]\nshow_wordcloud(us_df['text'], title = 'Prevalent words in tweets from US')","bef8f703":"us_df = tweets_df.loc[tweets_df.user_location==\"United Kingdom\"]\nshow_wordcloud(us_df['text'], title = 'Prevalent words in tweets from UK')","eea8481f":"us_df = tweets_df.loc[tweets_df.user_location==\"Canada\"]\nshow_wordcloud(us_df['text'], title = 'Prevalent words in tweets from Canada')","a85a3c6f":"india_df = tweets_df.loc[tweets_df.user_location==\"South Africa\"]\nshow_wordcloud(india_df['text'], title = 'Prevalent words in tweets from South Africa')","d9b1674a":"india_df = tweets_df.loc[tweets_df.user_location==\"Switzerland\"]\nshow_wordcloud(india_df['text'], title = 'Prevalent words in tweets from Switzerland')","eb3278ef":"us_df = tweets_df.loc[tweets_df.user_location==\"London\"]\nshow_wordcloud(us_df['text'], title = 'Prevalent words in tweets from London')","3de9b302":"def plot_features_distribution(features, title, df, isLog=False):\n    plt.figure(figsize=(12,6))\n    plt.title(title)\n    for feature in features:\n        if(isLog):\n            sns.distplot(np.log1p(df[feature]),kde=True,hist=False, bins=120, label=feature)\n        else:\n            sns.distplot(df[feature],kde=True,hist=False, bins=120, label=feature)\n    plt.xlabel('')\n    plt.legend()\n    plt.show()\n","90063581":"tweets_df['hashtags'] = tweets_df['hashtags'].replace(np.nan, \"['None']\", regex=True)\ntweets_df['hashtags'] = tweets_df['hashtags'].apply(lambda x: x.replace('\\\\N',''))\ntweets_df['hashtags_count'] = tweets_df['hashtags'].apply(lambda x: len(x.split(',')))\nplot_features_distribution(['hashtags_count'], 'Hashtags per tweet (all data)', tweets_df)","a43726d4":"### User location","dcaecbf0":"### Tweet source","2301d018":"<h1>Omicron is Rising<\/h1>\n<h2>Tweets about the new Covid-19 strain, Omicron<\/h2>\n\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F769452%2F35db2dd68238bfd958efdabebc9fef8f%2Fcovid-19-4961257_1280-e1586986896105.jpg?generation=1595760042647275&alt=media\" width=\"600\"><\/img>\n\n\n# Introduction\n\n\nThe Dataset ([Omicron Rising](https:\/\/www.kaggle.com\/gpreda\/omicron-rising)) we are using here is collected using Twitter API, **tweepy** and Python package. The Dataset will be updated constantly, so that you can have insights in the discussions around this new strain of Covid-19 virus. \n\n\nThe dataset contains the following columns:\n* id  \n* user_name  \n* user_location  \n* user_description  \n* user_created  \n* user_followers  \n* user_friends  \n* user_favorites  \n* user_verified  \n* date  \n* text  \n* hashtags  \n* source   \n* retweets    \n* favorites   \n* is_retweet \n\nThe main payload is the **text** field, containing the tweets texts.\n\n\nThis Notebook is just a starter Kernel for analysis of this Dataset.\n\n\n","a4ec7777":"## Visualize the data distribution","608cc956":"### Hashtags analysis","9f03e486":"### Unique values","63b53c72":"## Load data","cb7c45b1":"# Data exploration\n\n\n## Glimpse the data","f476288e":"### Text wordcloauds","dc9ed696":"### Most frequent values","f364dca9":"### User name","178efac5":"# Data preparation\n\n## Load packages","1cede4ef":"### Missing data"}}