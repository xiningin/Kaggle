{"cell_type":{"9d9916e1":"code","965c37a4":"code","2a09bac4":"code","b5199069":"code","23a9d0d0":"code","34dad065":"code","1241f266":"code","adcbf1a4":"code","0f058350":"code","685e97d7":"code","d9967afb":"code","07802e9e":"code","c9a26aff":"code","890a562c":"code","d7b92501":"code","eab91d2d":"code","ff991eec":"code","ee89f1b5":"code","03fe5221":"code","7b6dd8e3":"code","f8792b21":"code","ff19ffd1":"code","fb223223":"code","b0d15196":"code","cb5c2e22":"code","f6a2b38c":"code","d103fcf9":"code","5155b46c":"code","d2c1bd85":"code","2b01f246":"code","74e28761":"code","414f9b42":"code","19671f28":"code","ea17e39f":"code","c5e81e4b":"code","aca7cba1":"code","503b739e":"code","0fb272b1":"code","3969020b":"code","31489568":"code","88d364fc":"code","1fe1acf4":"code","7158b69e":"code","bf856556":"code","7107beb8":"code","f48bedd0":"code","425b8bce":"code","e869423a":"code","91d36ba5":"code","c10eec5c":"code","4c7d1c88":"code","3b9bd4d8":"code","08a577e9":"code","6550ad17":"code","c3f08435":"code","4ef1bd94":"code","727236ae":"code","b716d2dd":"code","49055757":"code","c9c7846a":"code","d00e50bc":"code","dd748e38":"code","0f4c6a14":"code","977045e2":"code","382f5b92":"code","f8cddf6e":"code","a0691b1b":"code","dc738181":"code","9fb9cc1b":"code","4f0e6d4f":"code","b2509126":"code","70edaca1":"code","88c923dd":"code","198cc746":"code","d967e548":"code","35f1ce74":"code","89bdd5ce":"code","d7aadd5a":"code","76ee8b5f":"code","65a2697d":"code","3d9e6271":"code","08cac225":"code","f9ef0c3f":"code","5ee19e73":"code","b3f2d317":"code","562b0476":"code","0720443b":"code","e2acd03e":"code","d515afc4":"code","73bcaa49":"code","ca673acb":"code","1135c95f":"code","a1e7bc6b":"code","b5973b35":"code","9cdae76f":"code","d4815857":"code","998c101c":"code","ad879713":"code","a1e61a2a":"code","3d22f4f6":"code","b575c8ca":"code","e41a137b":"code","b7f7d65c":"code","b86ef643":"code","bab5d3a7":"code","7923f98d":"code","da882491":"code","fa92f03e":"code","99181774":"code","9e1aaa32":"code","c50ffdc8":"code","9c9fa4e1":"code","46802890":"code","14be9789":"code","11e83e67":"code","0cd49995":"code","ee3d709b":"code","810aea9d":"code","f2967e3c":"code","ee0d3dc5":"code","df3e7877":"code","e5f32fb3":"code","6ca8c8bf":"markdown","0cca24ab":"markdown","049a2628":"markdown","d285747c":"markdown","119b1a24":"markdown","e5899fb2":"markdown","91ac9f6f":"markdown","df25c61e":"markdown","a4c9f3ad":"markdown","b6a2a827":"markdown","61de676e":"markdown","e9889e4e":"markdown","b4b758a4":"markdown","2d50d212":"markdown","be07fa1c":"markdown","ed534ee9":"markdown","05e6d384":"markdown","9863ceda":"markdown","10628206":"markdown","82d6e2c7":"markdown","d06c5aa3":"markdown","d483c406":"markdown","3d3f5dd8":"markdown","334c46cc":"markdown","b2a27a12":"markdown","a2fe9e77":"markdown","9db1dd75":"markdown","ebba292a":"markdown","3dfa491c":"markdown","06e54525":"markdown","1fe13c80":"markdown","9953ebe7":"markdown","78572c31":"markdown","81b7a830":"markdown","ce118068":"markdown","f49f98ca":"markdown","b8291043":"markdown","9a1eb7c2":"markdown","7de1e0e4":"markdown","07098dda":"markdown","2f45632e":"markdown","9b31cb1b":"markdown","e71a1899":"markdown","f4112ee1":"markdown","20a364aa":"markdown","687c7e9f":"markdown","e5c73756":"markdown","7207ef3d":"markdown","140e30a8":"markdown","26d2ed09":"markdown","9258caf9":"markdown","28362607":"markdown","b0cbb66b":"markdown","c4365513":"markdown","728ae43a":"markdown","14852a8f":"markdown","4072f79d":"markdown","b34fde6d":"markdown","dfd48168":"markdown","8233219b":"markdown","56849bab":"markdown","ef6a0b36":"markdown","92f59d93":"markdown","dd0ac6d4":"markdown","f4e2aadf":"markdown","07522dc2":"markdown","ef1139ba":"markdown","884ac367":"markdown","2a2900b8":"markdown","fd0ce3b4":"markdown","0b6e300e":"markdown","d6ad145f":"markdown","8079d5bf":"markdown","ea5df0d3":"markdown","68de9059":"markdown","0159c4cd":"markdown","c97cc894":"markdown","be9e5587":"markdown","87b15b28":"markdown","334606f3":"markdown","e11b0238":"markdown","cd9942c7":"markdown","87a17080":"markdown","6c920b8b":"markdown","b8140cac":"markdown","3f7a0bde":"markdown","b50cc451":"markdown","930ac715":"markdown","98c53135":"markdown","ce9bf1ca":"markdown","efcb5e8b":"markdown","fe8f677f":"markdown","7a302a58":"markdown","d30218db":"markdown","52d5f7fe":"markdown","2798387d":"markdown","112d5547":"markdown","8a6ac5df":"markdown","c2789872":"markdown","c72bf5af":"markdown","b6c401c8":"markdown","fe07d682":"markdown","ee3f2167":"markdown"},"source":{"9d9916e1":"# Import packages\n\n## Basic data processing\nimport numpy as np\nimport pandas as pd\n\n## Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\nimport folium\nimport folium.plugins\n\n## Modelling\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, plot_confusion_matrix, roc_auc_score, balanced_accuracy_score, mean_squared_error, r2_score\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom xgboost.sklearn import XGBClassifier, XGBRegressor\n\n## Model Explanatory\nimport shap  # package used to calculate Shap values\nimport graphviz\nimport eli5\nfrom pdpbox import pdp, get_dataset, info_plots\n\n## Settings\npd.set_option('display.max_columns', 500) # Able to display more columns.","965c37a4":"# Load the dataset\ndata_df = pd.read_csv(\"..\/input\/us-accidents\/US_Accidents_Dec20_Updated.csv\")\ndata_df.info() # show entries, dtypes, memory useage.","2a09bac4":"# Have a look\ndata_df.head(5)","b5199069":"# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html\n# Basic statistic on Ordinal, Interval and Ratio data.\nOIR_columns = ['Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Weather_Timestamp', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\ndata_df[OIR_columns].describe() #Exclude Time; Time can be transformed to second\/minute\/hour\/day\/month\/year separately, but it is no need to do it here.","23a9d0d0":"# Basic statistic on Nominal data\ndata_df.loc[:, ~data_df.columns.isin(OIR_columns)].astype(\"object\").describe() # All the Nominal data can be treated as \"object\" type for simplicity.","34dad065":"# Category Visualization\n# https:\/\/plotly.com\/python\/sunburst-charts\/\ndata_category = dict(\n    character=[\"Basic\", \"Location\", \"Environment\", \"Infrastructure\", 'ID', 'Severity', 'Start_Time', 'End_Time', 'Distance(mi)', 'Description', 'Start_Lat', 'Start_Lng','End_Lat', 'End_Lng', 'Number', 'Street','Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Weather_Timestamp', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', 'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop'],\n    parent=[\"\",\"\",\"\",\"\",\"Basic\", \"Basic\", \"Basic\", \"Basic\", \"Basic\", \"Basic\", \"Location\", \"Location\", \"Location\", \"Location\", \"Location\", \"Location\", \"Location\", \"Location\", \"Location\", \"Location\", \"Location\", \"Location\", \"Location\", \"Location\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Environment\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\", \"Infrastructure\"],\n)\n\ncategory_fig =px.sunburst(\n    data_category,\n    names='character',\n    parents='parent'\n)\ncategory_fig.update_layout(\n    autosize=False,\n    width=600,\n    height=600,\n    title={\n        'text': \"Data Categorizing\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\ncategory_fig.show()","1241f266":"# Irrelevant columns\n'''\nID: ID is unique and meaningless for the dataset.\nDescription: I don't do text mining, therefore It's useless.\nCountry: All the data is from US, therefore all the data is the same.\nWeather_Timestamp: The timestamp of weather observation record. It's useless here.\n'''\nirrelavant_columns = ['ID','Description','Country','Weather_Timestamp']\ndata_preprocessed_df = data_df.drop(irrelavant_columns, axis=1)","adcbf1a4":"# Replace the empty data with NaN\ndata_preprocessed_df.replace(\"\", float(\"NaN\"), inplace=True)\ndata_preprocessed_df.replace(\" \", float(\"NaN\"), inplace=True)\n\n# Count missing value(NaN, na, null, None) of each columns, Then transform the result to a pandas dataframe. \ncount_missing_value = data_preprocessed_df.isna().sum() \/ data_preprocessed_df.shape[0] * 100\ncount_missing_value_df = pd.DataFrame(count_missing_value.sort_values(ascending=False), columns=['Missing%'])","0f058350":"# Visualize the percentage(>0) of Missing value in each column.\nmissing_value_df = count_missing_value_df[count_missing_value_df['Missing%'] > 0]\n\nplt.figure(figsize=(15, 10)) # Set the figure size\nmissing_value_graph = sns.barplot(y = missing_value_df.index, x = \"Missing%\", data=missing_value_df, orient=\"h\")\nmissing_value_graph.set_title(\"Percentage Missing value of each feature\", fontsize = 20)\nmissing_value_graph.set_ylabel(\"Features\")","685e97d7":"## Drop the column with Missing value(>40%)\nmissing_value_40_df = count_missing_value_df[count_missing_value_df['Missing%'] > 40]\ndata_preprocessed_df.drop(missing_value_40_df.index, axis=1, inplace=True)\nmissing_value_40_df","d9967afb":"# Convert Time to datetime64[ns]\ndata_preprocessed_df['Start_Time'] = pd.to_datetime(data_preprocessed_df['Start_Time'])\ndata_preprocessed_df['End_Time'] = pd.to_datetime(data_preprocessed_df['End_Time'])","07802e9e":"# Display all the missing value\nmissing_value_df","c9a26aff":"# Categorize the missing value to numerical and categorical for imputation purpose\nnumerical_missing = ['Wind_Speed(mph)', 'End_Lng', 'End_Lat', 'Visibility(mi)','Humidity(%)', 'Temperature(F)', 'Pressure(in)']\ncategorical_missing = ['Weather_Condition','Wind_Direction', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight', 'Side']","890a562c":"# Drop all the instance with NaN\/NA\/null\ndata_preprocessed_dropNaN_df = data_preprocessed_df.dropna()\ndata_preprocessed_dropNaN_df.reset_index(drop=True, inplace=True)","d7b92501":"# Imputation by corresponding class Median value \ndata_preprocessed_median_df = data_preprocessed_df.copy()\n\n# For numerical columns\nfor column_name in numerical_missing:\n    data_preprocessed_median_df[column_name] = data_preprocessed_median_df.groupby('Severity')[column_name].transform(lambda x:x.fillna(x.median()))\n\n# # For categorical columns(Majority value imputation)\n# https:\/\/medium.com\/analytics-vidhya\/best-way-to-impute-categorical-data-using-groupby-mean-mode-2dc5f5d4e12d\nfor column_name in categorical_missing:\n    data_preprocessed_median_df[column_name] = data_preprocessed_median_df.groupby('Severity')[column_name].transform(lambda x:x.fillna(x.fillna(x.mode().iloc[0])))\n\n# Drop NaN and reset index\ndata_preprocessed_median_df.dropna(inplace=True)","eab91d2d":"# Imputation by corresponding class Mean value \ndata_preprocessed_mean_df = data_preprocessed_df.copy()\n\n# For numerical columns\nfor column_name in numerical_missing:\n    data_preprocessed_mean_df[column_name] = data_preprocessed_mean_df.groupby('Severity')[column_name].transform(lambda x:x.fillna(x.mean()))\n\n# For categorical columns(Majority value imputation)\nfor column_name in categorical_missing:\n    data_preprocessed_mean_df[column_name] = data_preprocessed_mean_df.groupby('Severity')[column_name].transform(lambda x:x.fillna(x.fillna(x.mode().iloc[0])))\n    \n# Drop NaN \ndata_preprocessed_mean_df.dropna(inplace=True)","ff991eec":"# Save these datasets to local\n#data_preprocessed_dropNaN_df.to_csv('preprocessed_dropNaN.csv', index=False)\n#data_preprocessed_median_df.to_csv('preprocessed_median.csv', index=False)\n#data_preprocessed_mean_df.to_csv('preprocessed_mean.csv', index=False)","ee89f1b5":"# Choose the best dataset base on the performance of modeling\ndata_best_df = data_preprocessed_dropNaN_df.copy()\n#data_best_df = data_preprocessed_dropNaN_df[data_preprocessed_dropNaN_df['City'] == 'Orlando'].copy()\n#data_best_df = data_preprocessed_median_df[data_preprocessed_dropNaN_df['City'] == 'Orlando'].copy()\n#data_best_df = data_preprocessed_mean_df[data_preprocessed_dropNaN_df['City'] == 'Orlando'].copy()\n\n# Reset index\ndata_best_df.reset_index(inplace=True)","03fe5221":"# Count the number of each severity, transform the result to pandas dataframe\nseverity_counts = data_best_df[\"Severity\"].value_counts()\nseverity_counts_df = pd.DataFrame(severity_counts)\n\n# Calculate the proportion of each Severity\nseverity_percentage_df = severity_counts_df \/ sum(severity_counts_df[\"Severity\"]) * 100","7b6dd8e3":"# Visualize the distribution of accidents severity\nseverity_fig = make_subplots(\n    rows=1, cols=2, \n    specs=[[{\"type\": \"xy\"}, {\"type\": \"domain\"}]])\n\nseverity_fig.add_trace(go.Bar(x=severity_counts_df.index, \n                              y=severity_counts_df[\"Severity\"],\n                              text=severity_counts_df[\"Severity\"],\n                              textposition='outside',\n                              showlegend=False),\n                              1, 1)\n\nseverity_fig.add_trace(go.Pie(labels=severity_percentage_df.index, \n                     values=severity_percentage_df[\"Severity\"],\n                     showlegend=True),\n                     1, 2)\n\nseverity_fig.update_layout(\n                  height=600, \n                  width=1500,\n                  title={\n                  'text': \"The distribution of accidents severity\",\n                  'font': {'size': 24},\n                  'y':0.95,\n                  'x':0.5,\n                  'xanchor': 'center',\n                  'yanchor': 'top'},\n                  xaxis1_title = 'Severity',\n                  yaxis1_title = 'Counts',\n                  legend_title_text=\"Severity\"\n                 )\nseverity_fig.update_xaxes(type='category')\nseverity_fig.show()","f8792b21":"# Calculate the mean distance of each Severity\nmean_distance = data_best_df.groupby('Severity')[\"Distance(mi)\"].mean().round(2)\nmean_distance_df = pd.DataFrame(mean_distance)\n\nmean_distance_fig = px.bar(mean_distance_df, \n                                   x = mean_distance_df.index, \n                                   y = \"Distance(mi)\", \n                                   labels={\"index\": \"Severity\"},\n                                   text=\"Distance(mi)\")\nmean_distance_fig.update_layout(\n    autosize=False,\n    width=600,\n    height=500,\n    title={\n        'text': \"Mean Distance(mi) of each Severity\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    xaxis={\n        'type':'category'\n    })\nmean_distance_fig.show()","ff19ffd1":"# Overview of the US traffic accidents \n# https:\/\/plotly.com\/python\/choropleth-maps\/\nstate_accidents_count = data_best_df[\"State\"].value_counts()\nfig = go.Figure(data=go.Choropleth(locations=state_accidents_count.index, \n                                           z=state_accidents_count.values.astype(float), \n                                           locationmode=\"USA-states\", \n                                           colorscale=\"Reds\",\n                                           colorbar_title = \"Frequency\"\n                                  ))\nfig.update_layout(\n    height=600, \n    width=1500,\n    title={\n        'text': \"Frequency distribution of US Accidents\",\n        'y':0.9,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'},\n        geo_scope=\"usa\")\nfig.show()","fb223223":"# Top 10 States with the Most Accidents\nstate_accidents_count_top10 = state_accidents_count[:10]\nstate_accidents_count_top10_df = pd.DataFrame(state_accidents_count_top10)\n\nstate_accidents_count_top10_fig = px.bar(state_accidents_count_top10_df, \n                                         x = state_accidents_count_top10_df.index, \n                                         y = \"State\", \n                                         labels={\"index\": \"State\", \"State\": \"Counts\"},\n                                         text=\"State\")\nstate_accidents_count_top10_fig.update_layout(\n    autosize=False,\n    width=1000,\n    height=600,\n    title={\n        'text': \"Top 10 States with the Most Accidents\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'})\nstate_accidents_count_top10_fig.update_yaxes(categoryorder = \"total ascending\")\nstate_accidents_count_top10_fig.show()","b0d15196":"# Top 10 States with the Most Accidents in a view of severity \nplt.figure(figsize=(20, 8))\nax = sns.countplot(x=\"State\", \n              data=data_best_df, \n              order=data_best_df['State'].value_counts()[:10].index, \n              hue='Severity',\n              palette='tab10')\nplt.title(\"Top 10 States with the Most Accidents\", fontsize = 22)\n#for p in ax.patches:\n#        ax.annotate(p.get_height(), (p.get_x(), p.get_height()+1000))\nplt.show()","cb5c2e22":"# Top 10 Counties with the Most Accidents\ncounty_accidents_count = data_best_df[\"County\"].value_counts()\ncounty_accidents_count_top10 = county_accidents_count[:10]\ncounty_accidents_count_top10_df = pd.DataFrame(county_accidents_count_top10)\n\ncounty_accidents_count_top10_fig = px.bar(county_accidents_count_top10_df, \n                                        x = county_accidents_count_top10_df.index, \n                                        y = \"County\", \n                                        labels={\"index\": \"County\", \"County\": \"Counts\"},\n                                        text=\"County\")\ncounty_accidents_count_top10_fig.update_layout(\n    autosize=False,\n    width=1000,\n    height=600,\n    title={\n        'text': \"Top 10 Counties with the Most Accidents\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'})\ncounty_accidents_count_top10_fig.update_yaxes(categoryorder = \"total ascending\")\ncounty_accidents_count_top10_fig.show()","f6a2b38c":"# Top 10 Counties with the Most Accidents in a view of severity\nplt.figure(figsize=(20, 8))\nax = sns.countplot(x=\"County\", \n                   data = data_best_df, \n                   order=data_best_df['County'].value_counts()[:10].index, \n                   hue = 'Severity')\nplt.title(\"Top 10 Counties with the Most Accidents\", fontsize = 22)\nfor p in ax.patches:\n        ax.annotate(p.get_height(), (p.get_x(), p.get_height()+400))\nplt.show()","d103fcf9":"# Top 10 Cities with the Most Accidents\ncity_accidents_count = data_best_df[\"City\"].value_counts()\ncity_accidents_count_top10 = city_accidents_count[:10]\ncity_accidents_count_top10_df = pd.DataFrame(city_accidents_count_top10)\n\ncity_accidents_count_top10_fig = px.bar(city_accidents_count_top10_df, \n                                        x = city_accidents_count_top10_df.index, \n                                        y = \"City\", \n                                        labels={\"index\": \"City\", \"City\": \"Counts\"},\n                                        text=\"City\")\ncity_accidents_count_top10_fig.update_layout(\n    autosize=False,\n    width=1000,\n    height=600,\n    title={\n        'text': \"Top 10 Cities with the Most Accidents\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'})\ncity_accidents_count_top10_fig.update_yaxes(categoryorder = \"total ascending\")\ncity_accidents_count_top10_fig.show()","5155b46c":"# Top 10 Cities with the Most Accidents in a view of severity\nplt.figure(figsize=(20, 8))\nax = sns.countplot(x=\"City\", \n                   data = data_best_df, \n                   order=data_best_df['City'].value_counts()[:10].index, \n                   hue = 'Severity')\nplt.title(\"Top 10 Cities with the Most Accidents\", fontsize = 22)\nfor p in ax.patches:\n        ax.annotate(p.get_height(), (p.get_x(), p.get_height()+200))\nplt.show()","d2c1bd85":"# Top 10 Streets with the Most Accidents\nstreet_accidents_count = data_best_df[\"Street\"].value_counts()\nstreet_accidents_count_top10 = street_accidents_count[:10]\nstreet_accidents_count_top10_df = pd.DataFrame(street_accidents_count_top10)\n\nstreet_accidents_count_top10_fig = px.bar(street_accidents_count_top10_df, \n                                          x = street_accidents_count_top10_df.index, \n                                          y = \"Street\", \n                                          labels={\"index\": \"Street\", \"Street\": \"Counts\"},\n                                          text=\"Street\")\nstreet_accidents_count_top10_fig.update_layout(\n    autosize=False,\n    width=1000,\n    height=600,\n    title={\n        'text': \"Top 10 Streets with the Most Accidents\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'})\nstreet_accidents_count_top10_fig.update_yaxes(categoryorder = \"total ascending\")\nstreet_accidents_count_top10_fig.show()","2b01f246":"# Top 10 Streets with the Most Accidents in a view of severity\nplt.figure(figsize=(20, 8))\nax = sns.countplot(x=\"Street\", \n              data = data_best_df, \n              order=data_best_df['Street'].value_counts()[:10].index, \n              hue = 'Severity')\nplt.title(\"Top 10 Streets with the Most Accidents\", fontsize = 22)\nfor p in ax.patches:\n        ax.annotate(p.get_height(), (p.get_x(), p.get_height()+100))\nplt.show()","74e28761":"# Top 10 Zipcode with the Most Accidents\nzipcode_accidents_count = data_best_df[\"Zipcode\"].value_counts()\nzipcode_accidents_count_top10 = zipcode_accidents_count[:10]\nzipcode_accidents_count_top10_df = pd.DataFrame(zipcode_accidents_count_top10)\n\nzipcode_accidents_count_top10_fig = px.bar(zipcode_accidents_count_top10_df, \n                                           x = zipcode_accidents_count_top10_df.index, \n                                           y = \"Zipcode\", \n                                           labels={\"index\": \"Zipcode\", \"Zipcode\": \"Counts\"},\n                                           text=\"Zipcode\")\nzipcode_accidents_count_top10_fig.update_layout(\n    autosize=False,\n    width=1000,\n    height=600,\n    title={\n        'text': \"Top 10 Zipcode with the Most Accidents\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'})\nzipcode_accidents_count_top10_fig.update_yaxes(categoryorder = \"total ascending\")\nzipcode_accidents_count_top10_fig.show()","414f9b42":"# Top 10 Zipcode with the Most Accidents in a view of severity\nplt.figure(figsize=(20, 8))\nax = sns.countplot(x=\"Zipcode\", \n              data = data_best_df, \n              order=data_best_df['Zipcode'].value_counts()[:10].index, \n              hue = 'Severity')\nplt.title(\"Top 10 Zipcode with the Most Accidents\", fontsize = 22)\nfor p in ax.patches:\n        ax.annotate(p.get_height(), (p.get_x(), p.get_height()+10))\nplt.show()","19671f28":"# Accidents distribution by street Side\n# Set up the matplotlib figure\nf, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n# Pie chart\ndata_best_df[\"Side\"].value_counts().plot.pie(autopct=\"%.1f%%\", ylabel='', ax=axes[0])\n\nsns.countplot(x=\"Side\", \n              data = data_best_df, \n              order=data_best_df['Side'].value_counts().index, \n              hue = 'Severity',\n             ax=axes[1])\nfor p in axes[1].patches:\n        axes[1].annotate(p.get_height(), (p.get_x()+0.05, p.get_height()+100))\n\n# Common title\nplt.suptitle(\"Accidents distribution by street Side\", y=0.95, fontsize=20)\nplt.show()","ea17e39f":"# Extract year\nyear_accidents_count = data_best_df[\"Start_Time\"].dt.year.value_counts()\nyear_accidents_count_df = pd.DataFrame(year_accidents_count)\n\nyear_accidents_count_fig = px.bar(year_accidents_count, \n                                  x = year_accidents_count.index, \n                                  y = \"Start_Time\", \n                                  labels={\"index\": \"Year\", \"Start_Time\": \"Counts\"},\n                                  text=\"Start_Time\")\nyear_accidents_count_fig.update_layout(\n    autosize=False,\n    width=800,\n    height=600,\n    title={\n        'text': \"Accidents yearly change\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'})\nyear_accidents_count_fig.show()","c5e81e4b":"# The number of registered motor vehicles\n# https:\/\/hedgescompany.com\/automotive-market-research-statistics\/auto-mailing-lists-and-marketing\/\n\nregistered_vehicles_df = pd.DataFrame(data=[264.0, 270.4, 279.1, 284.5, 286.9], columns=['Numbers(million)'], index=[2016, 2017, 2018, 2019, 2020])\n\nregistered_vehicles_fig = px.bar(registered_vehicles_df, \n                                  x = registered_vehicles_df.index, \n                                  y = \"Numbers(million)\", \n                                  labels={\"index\": \"Year\"},\n                                  text=\"Numbers(million)\")\nregistered_vehicles_fig.update_layout(\n    autosize=False,\n    width=800,\n    height=600,\n    title={\n        'text': \"Registered motor vehicles yearly change\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'})\nregistered_vehicles_fig.show()","aca7cba1":"# Calcute and Concat yearly accidents change rate and yearly registered vehicles change rate, then compare them in one graph.\nyear_accidents_pct_df = year_accidents_count_df.sort_index().pct_change().rename(columns={'Start_Time':'Accidents'})\nregistered_vehicles_pct_df = registered_vehicles_df.pct_change().rename(columns={'Numbers(million)' :'Vehicles'})\ncomparison_pct_df = pd.concat([year_accidents_pct_df, registered_vehicles_pct_df], axis=1).dropna()\n\ncomparison_pct_fig = go.Figure()\ncomparison_pct_fig.add_trace(go.Scatter(x=comparison_pct_df.index, y=comparison_pct_df['Accidents'],\n                    mode='lines+markers',\n                    name='Accidents'))\ncomparison_pct_fig.add_trace(go.Scatter(x=comparison_pct_df.index, y=comparison_pct_df['Vehicles'],\n                    mode='lines+markers',\n                    name='Vehicles'))\n\ncomparison_pct_fig.update_layout(\n    autosize=False,\n    width=800,\n    height=600,\n    title={\n        'text': \"Accidents Vs Vehicles (Number)\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'},\n        xaxis_title ='Year',\n        yaxis_title = 'Percentage Rate(%)')\ncomparison_pct_fig.update_xaxes(type='category')\ncomparison_pct_fig.show()","503b739e":"# Extract Month\n# https:\/\/plotly.com\/python\/categorical-axes\/\nmonth_accidents_count = data_best_df[\"Start_Time\"].dt.month.value_counts()\nmonth_accidents_count_df = pd.DataFrame(month_accidents_count)\nmonth_accidents_count_df.sort_index(inplace=True)\n\nmonth_accidents_count_fig = px.bar(month_accidents_count_df, \n                                   x = month_accidents_count_df.index, \n                                   y = \"Start_Time\", \n                                   labels={\"index\": \"Month\", \"Start_Time\": \"Counts\"},\n                                   text=\"Start_Time\")\nmonth_accidents_count_fig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title={\n        'text': \"Accidents monthly change\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    xaxis={\n        'type':'category'\n    })\nmonth_accidents_count_fig.show()","0fb272b1":"# Extract Week\nweek_accidents_count = data_best_df[\"Start_Time\"].dt.day_name().value_counts()\nweek_order = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nweek_accidents_count_df = pd.DataFrame(week_accidents_count).reindex(week_order)\n\nweek_accidents_count_fig = px.bar(week_accidents_count_df, \n                                  x = week_accidents_count_df.index, \n                                  y = \"Start_Time\", \n                                  labels={\"index\": \"Week\", \"Start_Time\": \"Counts\"},\n                                  text=\"Start_Time\")\nweek_accidents_count_fig.update_layout(\n    autosize=False,\n    width=800,\n    height=500,\n    title={\n        'text': \"Accidents weekly change\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'})\n#week_accidents_count_fig.update_xaxes(categoryorder = \"total ascending\")\nweek_accidents_count_fig.show()","3969020b":"# Transform Month\/week\/Hour to different features\ndata_best_df[\"Month\"] = data_best_df[\"Start_Time\"].dt.month\ndata_best_df[\"Week\"] = data_best_df[\"Start_Time\"].dt.day_name()\ndata_best_df[\"Hour\"] = data_best_df[\"Start_Time\"].dt.hour\n\n# Monthly view with weeks\ndata_best_df.groupby('Month')['Week'].value_counts().unstack()[week_order].plot.bar(\n    figsize=(20, 8),\n    ylabel='Counts',\n    width=.8\n)\nplt.title(\"Accidents Monthly change in a view of week\", fontsize = 22)\nplt.show()","31489568":"# Extract Hour (Weekday)\nweekdays_lst = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\nhour_accidents_weekdays_count = data_best_df[data_best_df['Week'].isin(weekdays_lst)][\"Start_Time\"].dt.hour\nhour_accidents_weekdays_count_df = pd.DataFrame(hour_accidents_weekdays_count.value_counts())\nhour_accidents_weekdays_count_df.sort_index(inplace=True)\n\nhour_accidents_weekdays_count_fig = px.bar(hour_accidents_weekdays_count_df, \n                                   x = hour_accidents_weekdays_count_df.index, \n                                   y = \"Start_Time\", \n                                   labels={\"index\": \"Hour\", \"Start_Time\": \"Counts\"},\n                                   text=\"Start_Time\")\nhour_accidents_weekdays_count_fig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title={\n        'text': \"Accidents hourly change(Weekdays)\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    xaxis={\n        'type':'category'\n    })\nhour_accidents_weekdays_count_fig.show()","88d364fc":"# Extract Hour (Weekend)\nweekend_lst = ['Saturday', 'Sunday']\nhour_accidents_weekend_count = data_best_df[data_best_df['Week'].isin(weekend_lst)][\"Start_Time\"].dt.hour\nhour_accidents_weekend_count_df = pd.DataFrame(hour_accidents_weekend_count.value_counts())\nhour_accidents_weekend_count_df.sort_index(inplace=True)\n\nhour_accidents_weekend_count_fig = px.bar(hour_accidents_weekend_count_df, \n                                   x = hour_accidents_weekend_count_df.index, \n                                   y = \"Start_Time\", \n                                   labels={\"index\": \"Hour\", \"Start_Time\": \"Counts\"},\n                                   text=\"Start_Time\")\nhour_accidents_weekend_count_fig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title={\n        'text': \"Accidents hourly change(Weekend)\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    xaxis={\n        'type':'category'\n    })\nhour_accidents_weekend_count_fig.show()","1fe1acf4":"# Weekly view with hours\ndata_best_df.groupby('Week')['Hour'].value_counts().unstack().reindex(week_order).plot.bar(\n    figsize=(22, 8),\n    ylabel='Counts',\n    width=.9\n)\nplt.title(\"Accidents Weekly change in a view of hour\", fontsize = 22)\nplt.show()","7158b69e":"# Calculate the mean Duration of each Severity\ndata_best_df[\"Duration\"] = (data_best_df['End_Time'] - data_best_df['Start_Time']).dt.total_seconds() \/ 3600\n\nmean_duration = data_best_df.groupby('Severity')[\"Duration\"].mean().round(2)\nmean_duration_df = pd.DataFrame(mean_duration)\n\nmean_duration_fig = px.bar(mean_duration_df, \n                                   x = mean_duration_df.index, \n                                   y = \"Duration\", \n                                   labels={\"index\": \"Severity\"},\n                                   text=\"Duration\")\nmean_duration_fig.update_layout(\n    autosize=False,\n    width=600,\n    height=500,\n    title={\n        'text': \"Mean Duration of each Severity\",\n        'y':0.95,\n        'x':0.5,\n        'font': {'size': 24},\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    xaxis={\n        'type':'category'\n    })\nmean_duration_fig.show()","bf856556":"# Weather condition\nfig = plt.figure(figsize = (15, 10))\nsns.countplot(y='Weather_Condition', \n              data=data_best_df, \n              order=data_best_df['Weather_Condition'].value_counts()[:15].index)\\\n.set_title(\"Top 15 Weather_Condition\", fontsize = 22)\nplt.show()","7107beb8":"# Weather condition by mean of the Severity\nweather_mean_severity = data_best_df.groupby('Weather_Condition')['Severity'].mean().sort_values(ascending=False)\nweather_mean_severity_df = pd.DataFrame(weather_mean_severity[:25])\n\nplt.figure(figsize=(15, 10)) # Set the figure size\nweather_mean_severity_graph = sns.barplot(y = weather_mean_severity_df.index, x = \"Severity\", data=weather_mean_severity_df, orient=\"h\")\nweather_mean_severity_graph.set_title(\"Weather Condition with mean of the severity\", fontsize = 20)\nweather_mean_severity_graph.set_ylabel(\"Weather_Condition\")","f48bedd0":"fig = plt.figure(figsize = (15, 10))\nsns.countplot(y='Wind_Direction', \n              data=data_best_df, \n              order=data_best_df['Wind_Direction'].value_counts()[:15].index)\\\n.set_title(\"Top 15 Wind_Direction\", fontsize = 22)\nplt.show()","425b8bce":"# Wind direction by mean of the Severity\nwind_mean_severity = data_best_df.groupby('Wind_Direction')['Severity'].mean().sort_values(ascending=False)\nwind_mean_severity_df = pd.DataFrame(wind_mean_severity)\n\nplt.figure(figsize=(15, 10)) # Set the figure size\nwind_mean_severity_graph = sns.barplot(y = wind_mean_severity_df.index, x = \"Severity\", data=wind_mean_severity_df, orient=\"h\")\nwind_mean_severity_graph.set_title(\"Wind direction with mean of the severity\", fontsize = 20)\nwind_mean_severity_graph.set_ylabel(\"Wind_Direction\")","e869423a":"# Set up the matplotlib figure\nf, axes = plt.subplots(5, 2, figsize=(20, 30))\nsns.distplot(data_best_df['Temperature(F)'], ax=axes[0, 0]).set_title('Temperature(F) Distribution')\ndata_best_df[\"Severity\"].groupby(pd.cut(data_best_df['Temperature(F)'], 10)).mean().plot(ylabel='Mean Severity', title='Mean Severity of Temperature(F)', ax=axes[0, 1])\n\nsns.distplot(data_best_df['Humidity(%)'], ax=axes[1, 0]).set_title('Humidity(%) Distribution')\ndata_best_df[\"Severity\"].groupby(pd.cut(data_best_df['Humidity(%)'], 10)).mean().plot(ylabel='Mean Severity', title='Mean Severity of Humidity(%)', ax=axes[1, 1])\n\nsns.distplot(data_best_df['Pressure(in)'], ax=axes[2, 0]).set_title('Pressure(in) Distribution')\ndata_best_df[\"Severity\"].groupby(pd.cut(data_best_df['Pressure(in)'], 10)).mean().plot(ylabel='Mean Severity', title='Mean Severity of Pressure(in)', ax=axes[2, 1])\n\nsns.distplot(data_best_df['Visibility(mi)'], ax=axes[3, 0]).set_title('Visibility(mi) Distribution')\ndata_best_df[\"Severity\"].groupby(pd.cut(data_best_df['Visibility(mi)'], 10)).mean().plot(ylabel='Mean Severity', title='Mean Severity of Visibility(mi)', ax=axes[3, 1])\n\nsns.distplot(data_best_df['Wind_Speed(mph)'], ax=axes[4, 0]).set_title('Wind_Speed(mph) Distribution')\ndata_best_df[\"Severity\"].groupby(pd.cut(data_best_df['Wind_Speed(mph)'], 10)).mean().plot( ylabel='Mean Severity', title='Mean Severity of Wind_Speed(mph)', ax=axes[4, 1])\n\nplt.suptitle(\"Temperature, Humidity, Pressure, Visibility and Wind Speed - Distribution && Mean Severity\", y=0.95, fontsize=20)\nplt.plot()","91d36ba5":"# Accidents distribution by Sunrise && Sunset\n# Set up the matplotlib figure\nf, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n# Pie chart\ndata_best_df[\"Sunrise_Sunset\"].value_counts().plot.pie(autopct=\"%.1f%%\", ylabel='', ax=axes[0])\n\nsns.countplot(x=\"Sunrise_Sunset\", \n              data = data_best_df, \n              order=data_best_df['Sunrise_Sunset'].value_counts().index, \n              hue = 'Severity',\n             ax=axes[1])\nfor p in axes[1].patches:\n        axes[1].annotate(p.get_height(), (p.get_x()+0.025, p.get_height()+100))\n\n# Common title\nplt.suptitle(\"Accidents distribution by Sunrise && Sunset\", y=0.95, fontsize=20)\nplt.show()","c10eec5c":"# Show the severity distribution of each category(True\/False)\nf, axes = plt.subplots(13, 2, figsize=(20, 80))\ninfrastructure_features = ['Traffic_Signal', 'Crossing', 'Station','Amenity', 'Bump', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout','Stop', 'Traffic_Calming', 'Turning_Loop']\nfor infrastructure_feature, number in zip(infrastructure_features,range(0,13)):\n    data_best_df[infrastructure_feature].value_counts().plot.pie(autopct=\"%.2f%%\", ylabel='',ax=axes[number, 0]).set_title(f'{infrastructure_feature} Distribution')\n    sns.countplot(x=infrastructure_feature, \n              data=data_best_df, \n              order=data_best_df[infrastructure_feature].value_counts().index, \n              hue='Severity',\n              ax=axes[number, 1]).set_title(f'{infrastructure_feature} Analysis')\n    # Add number on corresponding bar                                     \n    for p in axes[number, 1].patches:\n        axes[number, 1].annotate(p.get_height(), (p.get_x()+0.025, p.get_height()+100))","4c7d1c88":"## Only choose a city because of the resources limitation.\ndata_best_df = data_preprocessed_dropNaN_df[data_preprocessed_dropNaN_df['City'] == 'Orlando'].copy()\n#data_best_df = data_preprocessed_median_df[data_preprocessed_dropNaN_df['City'] == 'Orlando'].copy()\n#data_best_df = data_preprocessed_mean_df[data_preprocessed_dropNaN_df['City'] == 'Orlando'].copy()\n\n# Reset index\ndata_best_df.reset_index(inplace=True)","3b9bd4d8":"# Choose relevant features\nrelevant_features = ['Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng','Side',\n       'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)',\n       'Wind_Direction', 'Wind_Speed(mph)', 'Weather_Condition', 'Amenity',\n       'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n       'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal',\n       'Turning_Loop', 'Sunrise_Sunset']\ndata_modelling_df = data_best_df[relevant_features].copy()","08a577e9":"# Duration = End_Time - Start_Time; Create a new feature for modeling.\ndata_modelling_df['Duration'] = (data_modelling_df['End_Time'] - data_modelling_df['Start_Time']).dt.total_seconds() \/ 3600\ndata_modelling_df.drop('End_Time', axis=1, inplace=True)","6550ad17":"# Transform Month\/week\/Hour to different features\ndata_modelling_df[\"Month\"] = data_modelling_df[\"Start_Time\"].dt.month\ndata_modelling_df[\"Week\"] = data_modelling_df[\"Start_Time\"].dt.dayofweek\ndata_modelling_df[\"Hour\"] = data_modelling_df[\"Start_Time\"].dt.hour\ndata_modelling_df.drop(\"Start_Time\", axis=1, inplace=True)","c3f08435":"# Select features that are suitable for One Hot Encoding\none_hot_features = ['Wind_Direction', 'Weather_Condition']\n\n# Wind_Direction Categorizing\ndata_modelling_df.loc[data_modelling_df['Wind_Direction'].str.startswith('C'), 'Wind_Direction'] = 'C' #Calm\ndata_modelling_df.loc[data_modelling_df['Wind_Direction'].str.startswith('E'), 'Wind_Direction'] = 'E' #East, ESE, ENE\ndata_modelling_df.loc[data_modelling_df['Wind_Direction'].str.startswith('W'), 'Wind_Direction'] = 'W' #West, WSW, WNW\ndata_modelling_df.loc[data_modelling_df['Wind_Direction'].str.startswith('S'), 'Wind_Direction'] = 'S' #South, SSW, SSE\ndata_modelling_df.loc[data_modelling_df['Wind_Direction'].str.startswith('N'), 'Wind_Direction'] = 'N' #North, NNW, NNE\ndata_modelling_df.loc[data_modelling_df['Wind_Direction'].str.startswith('V'), 'Wind_Direction'] = 'V' #Variable","4ef1bd94":"# Weather_Condition Categorizing\n# Fair, Cloudy, Clear, Overcast, Snow, Haze, Rain, Thunderstorm, Windy, Hail, Thunder, Dust, Tornado\ndata_modelling_df['Weather_Fair'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Fair', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Cloudy'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Cloudy', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Clear'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Clear', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Overcast'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Overcast', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Snow'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Snow|Wintry|Sleet', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Haze'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Smoke|Fog|Mist|Haze', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Rain'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Rain|Drizzle|Showers', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Thunderstorm'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Thunderstorms|T-Storm', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Windy'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Windy|Squalls', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Hail'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Hail|Ice Pellets', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Thunder'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Thunder', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Dust'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Dust', case=False, na = False), 1, 0)\ndata_modelling_df['Weather_Tornado'] = np.where(data_modelling_df['Weather_Condition'].str.contains('Tornado', case=False, na = False), 1, 0)","727236ae":"# Transform the one-hot features, then delete them\nonehot_df = pd.get_dummies(data_modelling_df['Wind_Direction'], prefix='Wind')\ndata_modelling_df = pd.concat([data_modelling_df, onehot_df], axis=1)\ndata_modelling_df.drop(one_hot_features, axis=1, inplace=True)","b716d2dd":"# Select features that are suitable for Label Encoding\nlabel_encoding_features = ['Side', 'Amenity','Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway','Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal','Turning_Loop', 'Sunrise_Sunset']\n\n# Label Encoding\nfor feature in label_encoding_features:\n    data_modelling_df[feature] = LabelEncoder().fit_transform(data_modelling_df[feature])","49055757":"data_modelling_df","c9c7846a":"# Display the correlation table for continuous features\n# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/style.html\ndef style_corr(v, props=''):\n    return props if (v < -0.4 or v > 0.4) and v != 1 else None\n\ncontinuous_feature = ['Start_Lat', 'Start_Lng', 'Temperature(F)','Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Duration']\ndata_modelling_df[continuous_feature].corr().style.applymap(style_corr, props='color:red;')","d00e50bc":"# Show the heatmap\nplt.figure(figsize=(12,9))\nsns.heatmap(data_modelling_df[continuous_feature].corr(), cmap=\"coolwarm\", annot = True, fmt='.3f').set_title('Pearson Correlation for continuous features', fontsize=22)","dd748e38":"# Find the data with all the same value\nunique_counts = data_modelling_df.drop(continuous_feature, axis=1).astype(\"object\").describe().loc['unique']\nfeature_all_same = list(unique_counts[unique_counts == 1].index)\ndata_modelling_df.drop(feature_all_same, axis=1, inplace=True)","0f4c6a14":"# Display the correlation table for categorical features\ndata_modelling_df.drop(continuous_feature, axis=1).corr(method='spearman').style.applymap(style_corr, props='color:red;')","977045e2":"# Show the heatmap\nplt.figure(figsize=(35,20))\nsns.heatmap(data_modelling_df.drop(continuous_feature, axis=1).corr(method='spearman'), cmap=\"coolwarm\", annot = True, fmt='.3f').set_title('Spearman Correlation for categorical features', fontsize=22)","382f5b92":"# Train\/Test Split\nX_reg = data_modelling_df.drop([\"Severity\", \"Duration\"], axis=1)\nY_reg = data_modelling_df.Duration\nx_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, Y_reg, test_size = 0.3, random_state=0)\nprint(f'Train Reg: {x_train_reg.shape} \\n Test Reg: {x_test_reg.shape}')\nreg_feature_names = x_train_reg.columns.tolist()","f8cddf6e":"# Store the information for Adj R2.\n# Adjusted R-squared can be negative when R-squared is close to zero.\n# Adjusted R-squared value always be less than or equal to R-squared value.\n# http:\/\/net-informations.com\/ds\/psa\/adjusted.htm\n'''\nDescription: Calculate Adj R2 for Regression\nArgs:\n    test_data: The test dataset(only feature)\n    r2_score: r2 score of the model\nReturn: adj_r2_score\n'''\ndef adj_r2(test_data, r2_score):\n    records_num = test_data.shape[0]\n    feature_num = test_data.shape[1]\n    adj_r2_score = 1 - ((records_num - 1) \/ (records_num - feature_num - 1) * (1 - r2_score))\n    return adj_r2_score","a0691b1b":"# Linear Regression\nlreg = LinearRegression(fit_intercept=False, normalize=True).fit(x_train_reg, y_train_reg)\nlreg_predictions = lreg.predict(x_test_reg)\nlreg_rmse = mean_squared_error(y_test_reg, lreg_predictions, squared=False)\nlreg_r2 = r2_score(y_test_reg, lreg_predictions)\nlreg_adj_r2 = adj_r2(x_test_reg, lreg_r2)\nprint(f'RMSE: {lreg_rmse} \\n R2: {lreg_r2} \\n Adj_R2: {lreg_adj_r2}')\n# Show feature importance as a table\neli5.show_weights(lreg, feature_names = reg_feature_names)","dc738181":"# SVR\nsv_reg = SVR(C=10, gamma=1).fit(x_train_reg, y_train_reg)\nsv_reg_predictions = sv_reg.predict(x_test_reg)\nsv_reg_rmse = mean_squared_error(y_test_reg, sv_reg_predictions, squared=False)\nsv_reg_r2 = r2_score(y_test_reg, sv_reg_predictions)\nsv_reg_adj_r2 = adj_r2(x_test_reg, sv_reg_r2)\nprint(f'RMSE: {sv_reg_rmse} \\n R2: {sv_reg_r2} \\n Adj_R2: {sv_reg_adj_r2}')\n# Show feature importance as a table\n#eli5.show_weights(sv_reg, feature_names = reg_feature_names)","9fb9cc1b":"# Decision Tree \ndt_reg = DecisionTreeRegressor(random_state=0).fit(x_train_reg, y_train_reg)\ndt_reg_predictions = dt_reg.predict(x_test_reg)\ndt_reg_rmse = mean_squared_error(y_test_reg, dt_reg_predictions, squared=False)\ndt_reg_r2 = r2_score(y_test_reg, dt_reg_predictions)\ndt_reg_adj_r2 = adj_r2(x_test_reg, dt_reg_r2)\nprint(f'RMSE: {dt_reg_rmse} \\n R2: {dt_reg_r2} \\n Adj_R2: {dt_reg_adj_r2}')\n# Show feature importance as a table\neli5.show_weights(dt_reg, feature_names = reg_feature_names)","4f0e6d4f":"# Get the feature importance as a dataframe  \ndt_reg_importances_df = pd.DataFrame(pd.Series(dt_reg.feature_importances_, index=X_reg.columns), columns=['Importance']).sort_values('Importance', ascending=False)\n# Visualize the feature importance of the trained tree\nplt.figure(figsize=(15, 10))\nmissing_value_graph = sns.barplot(y = dt_reg_importances_df.index, x = \"Importance\", data=dt_reg_importances_df, orient=\"h\")\nmissing_value_graph.set_title(\"Feature importance by Decision Tree Regression\", fontsize = 20)\nmissing_value_graph.set_ylabel(\"Features\")","b2509126":"# Gradient Boosting Regression\ngbt_reg = GradientBoostingRegressor(learning_rate=0.1, max_depth=20, min_impurity_decrease=0.1, min_samples_leaf=10, n_estimators=200, random_state=0).fit(x_train_reg, y_train_reg)\ngbt_reg_predictions = gbt_reg.predict(x_test_reg)\ngbt_reg_rmse = mean_squared_error(y_test_reg, gbt_reg_predictions, squared=False)\ngbt_reg_r2 = r2_score(y_test_reg, gbt_reg_predictions)\ngbt_reg_adj_r2 = adj_r2(x_test_reg, gbt_reg_r2)\nprint(f'RMSE: {gbt_reg_rmse} \\n R2: {gbt_reg_r2} \\n Adj_R2: {gbt_reg_adj_r2}')\n# Show feature importance as a table\neli5.show_weights(gbt_reg, feature_names = reg_feature_names)","70edaca1":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\n# Calculate Standard Deviation of each feature for all the trees\ngbt_reg_importances_std = np.std([tree[0].feature_importances_ for tree in gbt_reg.estimators_], axis=0)\ngbt_reg_importances = pd.Series(gbt_reg.feature_importances_, index=X_reg.columns)\ngbt_reg_importances_df = pd.DataFrame(gbt_reg_importances, columns=['Importance'])\ngbt_reg_importances_df['Std'] = gbt_reg_importances_std\ngbt_reg_importances_df.sort_values('Importance', ascending=True, inplace=True)\n\nfig, ax = plt.subplots(figsize=(15,10))\ngbt_reg_importances_df['Importance'].plot.barh(xerr=gbt_reg_importances_df['Std'], color='cornflowerblue', ax=ax)\nax.set_title(\"Feature importances using MDI of Gradient Boosting Regression\", fontsize = 22)\nax.set_xlabel(\"Mean decrease in impurity\")\nfig.tight_layout()","88c923dd":"# Random Forest Regression\nrf_reg = RandomForestRegressor(random_state=0).fit(x_train_reg, y_train_reg)\nrf_reg_predictions = rf_reg.predict(x_test_reg)\nrf_reg_rmse = mean_squared_error(y_test_reg, rf_reg_predictions, squared=False)\nrf_reg_r2 = r2_score(y_test_reg, rf_reg_predictions)\nrf_reg_adj_r2 = adj_r2(x_test_reg, rf_reg_r2)\nprint(f'RMSE: {rf_reg_rmse} \\n R2: {rf_reg_r2} \\n Adj_R2: {rf_reg_adj_r2}')\n# Show feature importance as a table\neli5.show_weights(rf_reg, feature_names = reg_feature_names)","198cc746":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\n# Calculate Standard Deviation of each feature for all the trees\nrf_reg_importances_std = np.std([tree.feature_importances_ for tree in rf_reg.estimators_], axis=0)\nrf_reg_importances = pd.Series(rf_reg.feature_importances_, index=X_reg.columns)\nrf_reg_importances_df = pd.DataFrame(rf_reg_importances, columns=['Importance'])\nrf_reg_importances_df['Std'] = rf_reg_importances_std\nrf_reg_importances_df.sort_values('Importance', ascending=True, inplace=True)\n\nfig, ax = plt.subplots(figsize=(15,10))\nrf_reg_importances_df['Importance'].plot.barh(xerr=rf_reg_importances_df['Std'], color='cornflowerblue', ax=ax)\nax.set_title(\"Feature importances using MDI of Random Forest Regression\", fontsize = 22)\nax.set_xlabel(\"Mean decrease in impurity\")\nfig.tight_layout()","d967e548":"# XGB Regression\nxgb_reg = XGBRegressor(learning_rate=0.1, max_depth=30, n_estimators=50, random_state=0).fit(x_train_reg, y_train_reg)\nxgb_reg_predictions = xgb_reg.predict(x_test_reg)\nxgb_reg_rmse = mean_squared_error(y_test_reg, xgb_reg_predictions, squared=False)\nxgb_reg_r2 = r2_score(y_test_reg, xgb_reg_predictions)\nxgb_reg_adj_r2 = adj_r2(x_test_reg, xgb_reg_r2)\nprint(f'RMSE: {xgb_reg_rmse} \\n R2: {xgb_reg_r2} \\n Adj_R2: {xgb_reg_adj_r2}')\n# Show feature importance as a table\neli5.show_weights(xgb_reg, feature_names = reg_feature_names)","35f1ce74":"# Get the feature importance as a dataframe  \nxgb_reg_importances_df = pd.DataFrame(pd.Series(xgb_reg.feature_importances_, index=X_reg.columns), columns=['Importance']).sort_values('Importance', ascending=False)\n# Visualize the feature importance of the trained tree\nplt.figure(figsize=(15, 10))\nmissing_value_graph = sns.barplot(y = xgb_reg_importances_df.index, x = \"Importance\", data=xgb_reg_importances_df, orient=\"h\")\nmissing_value_graph.set_title(\"Feature importance by XGB Regression\", fontsize = 20)\nmissing_value_graph.set_ylabel(\"Features\")","89bdd5ce":"# Multi-layer Perceptron regressor.\nmlpr_reg = MLPRegressor(learning_rate='invscaling', hidden_layer_sizes=(50, 75, 100), random_state=0).fit(x_train_reg, y_train_reg)\nmlpr_reg_predictions = mlpr_reg.predict(x_test_reg)\nmlpr_reg_rmse = mean_squared_error(y_test_reg, mlpr_reg_predictions, squared=False)\nmlpr_reg_r2 = r2_score(y_test_reg, mlpr_reg_predictions)\nmlpr_reg_adj_r2 = adj_r2(x_test_reg, mlpr_reg_r2)\nprint(f'RMSE: {mlpr_reg_rmse} \\n R2: {mlpr_reg_r2} \\n Adj_R2: {mlpr_reg_adj_r2}')","d7aadd5a":"# Gather all the Regression performance in one table\nreg_results = pd.DataFrame([(lreg_rmse, lreg_r2, lreg_adj_r2), (sv_reg_rmse, sv_reg_r2, sv_reg_adj_r2), (dt_reg_rmse, dt_reg_r2, dt_reg_adj_r2), (gbt_reg_rmse, gbt_reg_r2, gbt_reg_adj_r2), (rf_reg_rmse, rf_reg_r2, rf_reg_adj_r2), (xgb_reg_rmse, xgb_reg_r2, xgb_reg_adj_r2), (mlpr_reg_rmse, mlpr_reg_r2, mlpr_reg_adj_r2)], \n             columns=['RMSE','R2','Adj_R2'], \n             index= ['Linear Regression',\n                    'Support Vector Machine',\n                    'Decision Tree',\n                    'Gradient Boosting Tree',\n                    'Random Forest',\n                    'XGBoost',\n                    'Multi-layer Perceptron'])\nreg_results.sort_values(by=['RMSE'])","76ee8b5f":"# Train\/Test Split\nX_cla = data_modelling_df.drop(\"Severity\", axis=1)\nY_cla = data_modelling_df.Severity\nx_train_cla, x_test_cla, y_train_cla, y_test_cla = train_test_split(X_cla, Y_cla, test_size = 0.3, random_state=0, stratify=Y_cla)\nprint(f'Train Cla: {x_train_cla.shape} \\n Test Cla: {x_test_cla.shape}')\ncla_feature_names = x_train_cla.columns.tolist()","65a2697d":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.plot_confusion_matrix.html#sklearn.metrics.plot_confusion_matrix\n# normalize must be one of {'true', 'pred', 'all', None}\n'''\nDescription: Plot the confusion matrix\nArgs:\n    classifier: The classifier\nReturn: None\n'''\ndef draw_confusion_matrix(classifier):\n    fig, ax = plt.subplots(figsize=(12, 6))\n    plot_confusion_matrix(classifier, x_test_cla, y_test_cla, cmap=plt.cm.Blues, normalize=None, ax=ax)\n    ax.set_title(\"Confusion Matrix\", fontsize = 15)\n    plt.show()","3d9e6271":"# Logistic Regression\nlogistic_reg = LogisticRegression(C=10, fit_intercept=False, solver='liblinear')\nlogistic_reg.fit(x_train_cla, y_train_cla)\nlogistic_reg_predictions = logistic_reg.predict(x_test_cla)\nlogistic_reg_results = classification_report(y_test_cla, logistic_reg_predictions, zero_division=True, output_dict=True)\n\n# Confusion matrix and Classification report\ndraw_confusion_matrix(logistic_reg)\nprint(classification_report(y_test_cla, logistic_reg_predictions, zero_division=True))\n\n# balanced_accuracy\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\nlogistic_balanced_accuracy = balanced_accuracy_score(y_test_cla, logistic_reg_predictions)\nprint(f'balanced_accuracy: {logistic_balanced_accuracy}')\n\n# ROC_AUC score\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\nlogistic_roc_ovo_macro = roc_auc_score(y_test_cla, logistic_reg.predict_proba(x_test_cla), multi_class='ovo', average='macro') #Insensitive to class imbalance when average == 'macro'\nlogistic_roc_ovr_weighted = roc_auc_score(y_test_cla, logistic_reg.predict_proba(x_test_cla), multi_class='ovr', average='weighted') #Sensitive to class imbalance even when average == 'macro'\nprint(f\"roc_ovo_macro: {logistic_roc_ovo_macro}\") \nprint(f\"roc_ovr_weighted: {logistic_roc_ovr_weighted}\")\n\n# Show feature importance as a table\neli5.show_weights(logistic_reg, feature_names = cla_feature_names)","08cac225":"# SVC\nsv_cla = SVC(C=10, gamma=0.1, probability=True, kernel='rbf')\nsv_cla.fit(x_train_cla, y_train_cla)\nsv_cla_predictions = sv_cla.predict(x_test_cla)\nsv_cla_results = classification_report(y_test_cla, sv_cla_predictions, zero_division=True, output_dict=True)\n\n# Confusion matrix and Classification report\ndraw_confusion_matrix(sv_cla)\nprint(classification_report(y_test_cla, sv_cla_predictions, zero_division=True))\n\n# balanced_accuracy\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\nsv_cla_balanced_accuracy = balanced_accuracy_score(y_test_cla, sv_cla_predictions)\nprint(f'balanced_accuracy: {sv_cla_balanced_accuracy}')\n\n# ROC_AUC score\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\nsv_cla_roc_ovo_macro = roc_auc_score(y_test_cla, sv_cla.predict_proba(x_test_cla), multi_class='ovo', average='macro') #Insensitive to class imbalance when average == 'macro'\nsv_cla_roc_ovr_weighted = roc_auc_score(y_test_cla, sv_cla.predict_proba(x_test_cla), multi_class='ovr', average='weighted') #Sensitive to class imbalance even when average == 'macro'\nprint(f\"roc_ovo_macro: {sv_cla_roc_ovo_macro}\") \nprint(f\"roc_ovr_weighted: {sv_cla_roc_ovr_weighted}\")\n\n# Show feature importance as a table\n#eli5.show_weights(sv_cla, feature_names = cla_feature_names)","f9ef0c3f":"# Decision Tree Classification\ndt_cla = DecisionTreeClassifier(random_state=0)\ndt_cla.fit(x_train_cla, y_train_cla)\ndt_cla_predictions = dt_cla.predict(x_test_cla)\ndt_cla_results = classification_report(y_test_cla, dt_cla_predictions, zero_division=True, output_dict=True)\n\n# Confusion matrix and Classification report\ndraw_confusion_matrix(dt_cla)\nprint(classification_report(y_test_cla, dt_cla_predictions, zero_division=True))\n\n# balanced_accuracy\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\ndt_cla_balanced_accuracy = balanced_accuracy_score(y_test_cla, dt_cla_predictions)\nprint(f'balanced_accuracy: {dt_cla_balanced_accuracy}')\n\n# ROC_AUC score\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\ndt_cla_roc_ovo_macro = roc_auc_score(y_test_cla, dt_cla.predict_proba(x_test_cla), multi_class='ovo', average='macro') #Insensitive to class imbalance when average == 'macro'\ndt_cla_roc_ovr_weighted = roc_auc_score(y_test_cla, dt_cla.predict_proba(x_test_cla), multi_class='ovr', average='weighted') #Sensitive to class imbalance even when average == 'macro'\nprint(f\"roc_ovo_macro: {dt_cla_roc_ovo_macro}\") \nprint(f\"roc_ovr_weighted: {dt_cla_roc_ovr_weighted}\")\n\n# Show feature importance as a table\neli5.show_weights(dt_cla, feature_names = cla_feature_names)","5ee19e73":"# Get the feature importance as a dataframe  \ndt_cla_importances_df = pd.DataFrame(pd.Series(dt_cla.feature_importances_, index=X_cla.columns), columns=['Importance']).sort_values('Importance', ascending=False)\n# Visualize the feature importance of the trained tree\nplt.figure(figsize=(15, 10))\nmissing_value_graph = sns.barplot(y = dt_cla_importances_df.index, x = \"Importance\", data=dt_cla_importances_df, orient=\"h\")\nmissing_value_graph.set_title(\"Feature importance by Decision Tree Classification\", fontsize = 20)\nmissing_value_graph.set_ylabel(\"Features\")","b3f2d317":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.export_graphviz.html\ndt_cla_graph = export_graphviz(dt_cla, out_file=None, max_depth=2, filled=True, feature_names=cla_feature_names)\ngraphviz.Source(dt_cla_graph)","562b0476":"# 1-D pdp plot\ndt_cla_pdp_goals = pdp.pdp_isolate(model=dt_cla, dataset=x_test_cla, model_features=cla_feature_names, feature='Duration')\n# plot it\npdp.pdp_plot(dt_cla_pdp_goals, 'Duration')\nplt.show()","0720443b":"# 2D Partial Dependence Plots\nfeatures_to_plot = ['Start_Lng', 'Start_Lat']\ndt_cla_pdp_2D = pdp.pdp_interact(model=dt_cla, dataset=x_test_cla, model_features=cla_feature_names, features=features_to_plot)\npdp.pdp_interact_plot(pdp_interact_out=dt_cla_pdp_2D, feature_names=features_to_plot, plot_type='contour')\nplt.show()","e2acd03e":"# Gradient Boosting Classification\ngbt_cla = GradientBoostingClassifier(learning_rate=0.1, max_depth=10, min_impurity_decrease=0.1, min_samples_leaf=2, n_estimators=100, random_state=0)\ngbt_cla.fit(x_train_cla, y_train_cla)\ngbt_cla_predictions = gbt_cla.predict(x_test_cla)\ngbt_cla_results = classification_report(y_test_cla, gbt_cla_predictions, zero_division=True, output_dict=True)\n\n# Confusion matrix and Classification report\ndraw_confusion_matrix(gbt_cla)\nprint(classification_report(y_test_cla, gbt_cla_predictions, zero_division=True))\n\n# balanced_accuracy\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\ngbt_cla_balanced_accuracy = balanced_accuracy_score(y_test_cla, gbt_cla_predictions)\nprint(f'balanced_accuracy: {gbt_cla_balanced_accuracy}')\n\n# ROC_AUC score\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\ngbt_cla_roc_ovo_macro = roc_auc_score(y_test_cla, gbt_cla.predict_proba(x_test_cla), multi_class='ovo', average='macro') #Insensitive to class imbalance when average == 'macro'\ngbt_cla_roc_ovr_weighted = roc_auc_score(y_test_cla, gbt_cla.predict_proba(x_test_cla), multi_class='ovr', average='weighted') #Sensitive to class imbalance even when average == 'macro'\nprint(f\"roc_ovo_macro: {gbt_cla_roc_ovo_macro}\") \nprint(f\"roc_ovr_weighted: {gbt_cla_roc_ovr_weighted}\")\n# Show feature importance as a table\neli5.show_weights(gbt_cla, feature_names = cla_feature_names)","d515afc4":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\n# Calculate Standard Deviation of each feature for all the trees\ngbt_cla_importances_std = np.std([tree[0].feature_importances_ for tree in gbt_cla.estimators_], axis=0)\ngbt_cla_importances = pd.Series(gbt_cla.feature_importances_, index=X_cla.columns)\ngbt_cla_importances_df = pd.DataFrame(gbt_cla_importances, columns=['Importance'])\ngbt_cla_importances_df['Std'] = gbt_cla_importances_std\ngbt_cla_importances_df.sort_values('Importance', ascending=True, inplace=True)\n\nfig, ax = plt.subplots(figsize=(15,10))\ngbt_cla_importances_df['Importance'].plot.barh(xerr=gbt_cla_importances_df['Std'], color='cornflowerblue', ax=ax)\nax.set_title(\"Feature importances using MDI of Gradient Boosting Classification\", fontsize = 22)\nax.set_xlabel(\"Mean decrease in impurity\")\nfig.tight_layout()","73bcaa49":"# Random Forest Classification\nrf_cla = RandomForestClassifier(random_state=0)\nrf_cla.fit(x_train_cla, y_train_cla)\nrf_cla_predictions = rf_cla.predict(x_test_cla)\nrf_cla_results = classification_report(y_test_cla, rf_cla_predictions, zero_division=True, output_dict=True)\n\n# Confusion matrix and Classification report\ndraw_confusion_matrix(rf_cla)\nprint(classification_report(y_test_cla, rf_cla_predictions, zero_division=True))\n\n# balanced_accuracy\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\nrf_cla_balanced_accuracy = balanced_accuracy_score(y_test_cla, rf_cla_predictions)\nprint(f'balanced_accuracy: {rf_cla_balanced_accuracy}')\n\n# ROC_AUC score\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\nrf_cla_roc_ovo_macro = roc_auc_score(y_test_cla, rf_cla.predict_proba(x_test_cla), multi_class='ovo', average='macro') #Insensitive to class imbalance when average == 'macro'\nrf_cla_roc_ovr_weighted = roc_auc_score(y_test_cla, rf_cla.predict_proba(x_test_cla), multi_class='ovr', average='weighted') #Sensitive to class imbalance even when average == 'macro'\nprint(f\"roc_ovo_macro: {rf_cla_roc_ovo_macro}\") \nprint(f\"roc_ovr_weighted: {rf_cla_roc_ovr_weighted}\")\n# Show feature importance as a table\neli5.show_weights(rf_cla, feature_names = cla_feature_names)","ca673acb":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\n# Calculate Standard Deviation of each feature for all the trees\nrf_cla_importances_std = np.std([tree.feature_importances_ for tree in rf_cla.estimators_], axis=0)\nrf_cla_importances = pd.Series(rf_cla.feature_importances_, index=X_cla.columns)\nrf_cla_importances_df = pd.DataFrame(rf_cla_importances, columns=['Importance'])\nrf_cla_importances_df['Std'] = rf_cla_importances_std\nrf_cla_importances_df.sort_values('Importance', ascending=True, inplace=True)\n\nfig, ax = plt.subplots(figsize=(15,10))\nrf_cla_importances_df['Importance'].plot.barh(xerr=rf_cla_importances_df['Std'], color='cornflowerblue', ax=ax)\nax.set_title(\"Feature importances using MDI of Random Forest Classification\", fontsize = 22)\nax.set_xlabel(\"Mean decrease in impurity\")\nfig.tight_layout()","1135c95f":"# XGB Classification\n# https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn\nxgb_cla = XGBClassifier(learning_rate=0.3, max_depth=20, n_estimators=100, eval_metric='mlogloss', random_state=0)\nxgb_cla.fit(x_train_cla, y_train_cla)\nxgb_cla_predictions = xgb_cla.predict(x_test_cla)\nxgb_cla_results = classification_report(y_test_cla, xgb_cla_predictions, zero_division=True, output_dict=True)\n\n# Confusion matrix and Classification report\ndraw_confusion_matrix(xgb_cla)\nprint(classification_report(y_test_cla, xgb_cla_predictions, zero_division=True))\n\n# balanced_accuracy\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\nxgb_cla_balanced_accuracy = balanced_accuracy_score(y_test_cla, xgb_cla_predictions)\nprint(f'balanced_accuracy: {xgb_cla_balanced_accuracy}')\n\n# ROC_AUC score\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\nxgb_cla_roc_ovo_macro = roc_auc_score(y_test_cla, xgb_cla.predict_proba(x_test_cla), multi_class='ovo', average='macro') #Insensitive to class imbalance when average == 'macro'\nxgb_cla_roc_ovr_weighted = roc_auc_score(y_test_cla, xgb_cla.predict_proba(x_test_cla), multi_class='ovr', average='weighted') #Sensitive to class imbalance even when average == 'macro'\nprint(f\"roc_ovo_macro: {xgb_cla_roc_ovo_macro}\") \nprint(f\"roc_ovr_weighted: {xgb_cla_roc_ovr_weighted}\")\n# Show feature importance as a table\neli5.show_weights(xgb_cla, feature_names = cla_feature_names)","a1e7bc6b":"# Get the feature importance as a dataframe  \nxgb_cla_importances_df = pd.DataFrame(pd.Series(xgb_cla.feature_importances_, index=X_cla.columns), columns=['Importance']).sort_values('Importance', ascending=False)\n# Visualize the feature importance of the trained tree\nplt.figure(figsize=(15, 10))\nmissing_value_graph = sns.barplot(y = xgb_cla_importances_df.index, x = \"Importance\", data=xgb_cla_importances_df, orient=\"h\")\nmissing_value_graph.set_title(\"Feature importance by XGB Classification\", fontsize = 20)\nmissing_value_graph.set_ylabel(\"Features\")","b5973b35":"# 1-D pdp plot\nxgb_cla_pdp_goals = pdp.pdp_isolate(model=xgb_cla, dataset=x_test_cla, model_features=cla_feature_names, feature='Crossing')\n# plot it\npdp.pdp_plot(xgb_cla_pdp_goals, 'Crossing')\nplt.show()","9cdae76f":"# 2D Partial Dependence Plots\nfeatures_to_plot = ['Side', 'Duration']\nxgb_cla_pdp_2D = pdp.pdp_interact(model=xgb_cla, dataset=x_test_cla, model_features=cla_feature_names, features=features_to_plot)\npdp.pdp_interact_plot(pdp_interact_out=xgb_cla_pdp_2D, feature_names=features_to_plot, plot_type='contour')\nplt.show()","d4815857":"# Multi-layer Perceptron classifier. \nmlpc_cla = MLPClassifier(activation='tanh', hidden_layer_sizes=(100, 100), learning_rate='invscaling', random_state = 0)\nmlpc_cla.fit(x_train_cla, y_train_cla)\nmlpc_cla_predictions = mlpc_cla.predict(x_test_cla)\nmlpc_cla_results = classification_report(y_test_cla, mlpc_cla_predictions, zero_division=True, output_dict=True)\n\n# Confusion matrix and Classification report\ndraw_confusion_matrix(mlpc_cla)\nprint(classification_report(y_test_cla, mlpc_cla_predictions, zero_division=True))\n\n# balanced_accuracy\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\nmlpc_cla_balanced_accuracy = balanced_accuracy_score(y_test_cla, mlpc_cla_predictions)\nprint(f'balanced_accuracy: {mlpc_cla_balanced_accuracy}')\n\n# ROC_AUC score\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\nmlpc_cla_roc_ovo_macro = roc_auc_score(y_test_cla, mlpc_cla.predict_proba(x_test_cla), multi_class='ovo', average='macro') #Insensitive to class imbalance when average == 'macro'\nmlpc_cla_roc_ovr_weighted = roc_auc_score(y_test_cla, mlpc_cla.predict_proba(x_test_cla), multi_class='ovr', average='weighted') #Sensitive to class imbalance even when average == 'macro'\nprint(f\"roc_ovo_macro: {mlpc_cla_roc_ovo_macro}\") \nprint(f\"roc_ovr_weighted: {mlpc_cla_roc_ovr_weighted}\")","998c101c":"# Gather all the classification performance in one table\ncla_results = pd.DataFrame([\n    (logistic_balanced_accuracy, logistic_reg_results['accuracy'], logistic_reg_results['weighted avg']['precision'], logistic_reg_results['weighted avg']['recall'], logistic_reg_results['weighted avg']['f1-score'], logistic_roc_ovo_macro, logistic_roc_ovr_weighted),\n    (sv_cla_balanced_accuracy, sv_cla_results['accuracy'], sv_cla_results['weighted avg']['precision'], sv_cla_results['weighted avg']['recall'], sv_cla_results['weighted avg']['f1-score'], sv_cla_roc_ovo_macro, sv_cla_roc_ovr_weighted),\n    (dt_cla_balanced_accuracy, dt_cla_results['accuracy'], dt_cla_results['weighted avg']['precision'], dt_cla_results['weighted avg']['recall'], dt_cla_results['weighted avg']['f1-score'], dt_cla_roc_ovo_macro, dt_cla_roc_ovr_weighted),\n    (gbt_cla_balanced_accuracy, gbt_cla_results['accuracy'], gbt_cla_results['weighted avg']['precision'], gbt_cla_results['weighted avg']['recall'], gbt_cla_results['weighted avg']['f1-score'], gbt_cla_roc_ovo_macro, gbt_cla_roc_ovr_weighted),\n    (rf_cla_balanced_accuracy, rf_cla_results['accuracy'], rf_cla_results['weighted avg']['precision'], rf_cla_results['weighted avg']['recall'], rf_cla_results['weighted avg']['f1-score'], rf_cla_roc_ovo_macro, rf_cla_roc_ovr_weighted),\n    (xgb_cla_balanced_accuracy, xgb_cla_results['accuracy'], xgb_cla_results['weighted avg']['precision'], xgb_cla_results['weighted avg']['recall'], xgb_cla_results['weighted avg']['f1-score'], xgb_cla_roc_ovo_macro, xgb_cla_roc_ovr_weighted), \n    (mlpc_cla_balanced_accuracy, mlpc_cla_results['accuracy'], mlpc_cla_results['weighted avg']['precision'], mlpc_cla_results['weighted avg']['recall'], mlpc_cla_results['weighted avg']['f1-score'], mlpc_cla_roc_ovo_macro, mlpc_cla_roc_ovr_weighted)], \n    columns=['Accuracy(Balanced)', 'Accuracy','Precision(Weighted_avg)', 'Recall(Weighted_avg)', 'F1-score(Weighted_avg)', 'Roc_ovo(macro)', 'Roc_ovr(weighted)'], \n    index= ['Logistics Regression',\n            'Support Vector Machine',\n            'Decision Tree',\n            'Gradient Boosting Tree',\n            'Random Forest',\n            'XGBoost',\n            'Multi-layer Perceptron'])\n\ncla_results.sort_values(by=['F1-score(Weighted_avg)'], ascending=False)","ad879713":"# Form the train data for sampling\ntrain_cla_df = pd.concat([x_train_cla, y_train_cla], axis=1)\n\n# Over-sampling and Under-sampling \nsize_l = len(train_cla_df[train_cla_df[\"Severity\"]==2].index)\nsize_s = len(train_cla_df[train_cla_df[\"Severity\"]==1].index)\n\ntrain_cla_over = pd.DataFrame()\ntrain_cla_under = pd.DataFrame()\n\nfor i in range(1,5):\n    class_df = train_cla_df[train_cla_df[\"Severity\"]==i]\n    train_cla_over = train_cla_over.append(class_df.sample(size_l, random_state=1, replace=True))\n    train_cla_under = train_cla_under.append(class_df.sample(size_s, random_state=1, replace=False))\n\nprint(f'Over-sampling: \\n{train_cla_over.Severity.value_counts()}')\nprint(f'Under-sampling: \\n{train_cla_under.Severity.value_counts()}')","a1e61a2a":"# Try on over-sampling data\n# XGB Classification\nxgb_cla = XGBClassifier(learning_rate=0.3, max_depth=20, n_estimators=100, eval_metric='mlogloss', random_state=0)\nxgb_cla.fit(train_cla_over.drop('Severity', axis=1), train_cla_over['Severity'])\nxgb_cla_predictions_over = xgb_cla.predict(x_test_cla)\n\n# Confusion matrix and Classification report\ndraw_confusion_matrix(xgb_cla)\nprint(classification_report(y_test_cla, xgb_cla_predictions_over, zero_division=True))","3d22f4f6":"# Try on under-sampling data\n# XGB Classification\nxgb_cla = XGBClassifier(learning_rate=0.3, max_depth=20, n_estimators=100, eval_metric='mlogloss', random_state=0)\nxgb_cla.fit(train_cla_under.drop('Severity', axis=1), train_cla_under['Severity'])\nxgb_cla_predictions_under = xgb_cla.predict(x_test_cla)\n\n# Confusion matrix and Classification report\ndraw_confusion_matrix(xgb_cla)\nprint(classification_report(y_test_cla, xgb_cla_predictions_under, zero_division=True))","b575c8ca":"# Orlando latitude and longitude values\n# https:\/\/www.latlong.net\/place\/orlando-fl-usa-1947.html\norlando_lat = 28.538336\norlando_long = -81.379234\n\n# Generate a map of Orlando\norlando_map = folium.Map(location=[orlando_lat, orlando_long], zoom_start=12)\n\n# Instantiate a mark cluster object for the incidents in the dataframe\naccidents = folium.plugins.MarkerCluster().add_to(orlando_map)\n\n# Loop through the dataframe and add each data point to the mark cluster\nfor lat, lng, label in zip(x_test_cla['Start_Lat'], x_test_cla['Start_Lng'], xgb_cla_predictions.astype(str)):\n    if label == '4':\n        folium.Marker(\n            location=[lat, lng],\n            icon=folium.Icon(color=\"red\", icon=\"warning-sign\"), #https:\/\/getbootstrap.com\/docs\/3.3\/components\/\n            popup=label,\n            ).add_to(accidents)\n    elif label == '3':\n        folium.Marker(\n            location=[lat, lng],\n            icon=folium.Icon(color=\"lightred\", icon=\"warning-sign\"),\n            popup=label,\n            ).add_to(accidents)\n    elif label == '2':\n        folium.Marker(\n            location=[lat, lng],\n            icon=folium.Icon(color=\"orange\", icon=\"warning-sign\"),\n            popup=label,\n            ).add_to(accidents)\n    elif label == '1':\n        folium.Marker(\n            location=[lat, lng],\n            icon=folium.Icon(color=\"beige\", icon=\"warning-sign\"),\n            popup=label,\n            ).add_to(accidents)\n# Display map\norlando_map","e41a137b":"######Stop line######\nThe following code can not run out at Kaggle because of the resource limitation; Have already done it on Google Colab. Therefore, stop here.","b7f7d65c":"# Linear Regression\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html\nlr_pipe = make_pipeline(LinearRegression())\n# lr_pipe.get_params().keys()\nlr_param = {\n    'linearregression__fit_intercept': [True, False],\n    'linearregression__normalize': [True, False]\n}\n\nlr_search = GridSearchCV(lr_pipe, \n                         lr_param,\n                         scoring=\"neg_root_mean_squared_error\",\n                         n_jobs=-1,\n                         cv = 5)\nlr_search.fit(X_reg, Y_reg)\nprint(f'Best Params: {lr_search.best_params_} \\nBest score: {-(lr_search.best_score_)}')","b86ef643":"# SVR\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVR.html\n# https:\/\/7125messi.github.io\/post\/svm%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3\/\nsvr_pipe = make_pipeline(StandardScaler(), SVR())\nsvr_param = {\n    'svr__C': [0.1, 1.0, 10],\n    'svr__gamma': [1, 0.1, 0.01]\n}\n\nsvr_search = GridSearchCV(svr_pipe, \n                         svr_param,\n                         scoring=\"neg_root_mean_squared_error\",\n                         n_jobs=-1,\n                         cv = 5)\nsvr_search.fit(X_reg, Y_reg)\nprint(f'Best Params: {svr_search.best_params_} \\nBest score: {-(svr_search.best_score_)}')","bab5d3a7":"# Decision Tree Regression\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html\ndtr_pipe = make_pipeline(DecisionTreeRegressor(random_state=0))\ndtr_param = {\n    'decisiontreeregressor__max_depth': [5, 10, 20],\n    'decisiontreeregressor__min_samples_leaf': [2, 5, 10],\n    'decisiontreeregressor__min_impurity_decrease': [0.1, 0.2, 0.5]\n    \n}\n\ndtr_search = GridSearchCV(dtr_pipe, \n                         dtr_param,\n                         scoring=\"neg_root_mean_squared_error\",\n                         n_jobs=-1,\n                         cv = 5)\ndtr_search.fit(X_reg, Y_reg)\nprint(f'Best Params: {dtr_search.best_params_} \\nBest score: {-(dtr_search.best_score_)}')","7923f98d":"# Gradient Boosting Regression\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html\ngbtr_pipe = make_pipeline(GradientBoostingRegressor(random_state=0))\ngbtr_param = {\n    'gradientboostingregressor__learning_rate': [0.1, 0.3, 0.7],\n    'gradientboostingregressor__max_depth': [5, 10, 20],\n    'gradientboostingregressor__n_estimators': [50, 100, 200],\n    'gradientboostingregressor__min_impurity_decrease': [0.1, 0.2, 0.5],\n    'gradientboostingregressor__min_samples_leaf': [2, 5, 10]\n    \n}\n\ngbtr_search = GridSearchCV(gbtr_pipe, \n                         gbtr_param,\n                         scoring=\"neg_root_mean_squared_error\",\n                         n_jobs=-1,\n                         cv = 5)\ngbtr_search.fit(X_reg, Y_reg)\nprint(f'Best Params: {gbtr_search.best_params_} \\nBest score: {-(gbtr_search.best_score_)}')","da882491":"# Random Forest Regression\nrfr_pipe = make_pipeline(RandomForestRegressor(random_state=0))\nrfr_param = {\n    'randomforestregressor__max_depth': [5, 10, 20],\n    'randomforestregressor__n_estimators': [50, 100, 200],\n    'randomforestregressor__min_impurity_decrease': [0.1, 0.2, 0.5],\n    'randomforestregressor__min_samples_leaf': [2, 5, 10],\n    \n}\n\nrfr_search = GridSearchCV(rfr_pipe, \n                         rfr_param,\n                         scoring=\"neg_root_mean_squared_error\",\n                         n_jobs=-1,\n                         cv = 5)\nrfr_search.fit(X_reg, Y_reg)\nprint(f'Best Params: {rfr_search.best_params_} \\nBest score: {-(rfr_search.best_score_)}')","fa92f03e":"# XGB Regression\nxgbr_pipe = make_pipeline(XGBRegressor(random_state=0))\nxgbr_param = {\n              'xgbregressor__learning_rate': [0.1, 0.3, 0.7],\n              'xgbregressor__max_depth': [30, 50, 100],\n              'xgbregressor__n_estimators': [50, 100, 200]\n             }\n\nxgbr_search = GridSearchCV(xgbr_pipe, \n                         xgbr_param,\n                         scoring=\"neg_root_mean_squared_error\",\n                         n_jobs=-1,\n                         cv = 5)\nxgbr_search.fit(X_reg, Y_reg)\nprint(f'Best Params: {xgbr_search.best_params_} \\nBest score: {-(xgbr_search.best_score_)}')","99181774":"# Multi-layer Perceptron regressor\nmlpr_pipe = make_pipeline(MLPRegressor(random_state=0))\nmlpr_param = {\n              'mlpregressor__activation': ['logistic', 'tanh', 'relu'],\n              'mlpregressor__hidden_layer_sizes': [(100,), (50, 100), (50, 75, 100)],\n              'mlpregressor__learning_rate': ['invscaling', 'adaptive']\n             }\n\nmlpr_search = GridSearchCV(mlpr_pipe, \n                         mlpr_param,\n                         scoring=\"neg_root_mean_squared_error\",\n                         n_jobs=-1,\n                         cv = 5)\nmlpr_search.fit(X_reg, Y_reg)\nprint(f'Best Params: {mlpr_search.best_params_} \\nBest score: {-(mlpr_search.best_score_)}')","9e1aaa32":"pd.DataFrame([-(lr_search.best_score_), -(svr_search.best_score_), -(dtr_search.best_score_), -(gbtr_search.best_score_), -(rfr_search.best_score_), -(xgbr_search.best_score_), -(mlpr_search.best_score_)], \n             columns=['RMSE'], \n             index= ['Linear Regression',\n                    'Support Vector Machine',\n                    'Decision Tree',\n                    'Gradient Boosting Tree',\n                    'Random Forest',\n                    'XGBoost',\n                    'Multi-layer Perceptron']).sort_values(by=['RMSE'])","c50ffdc8":"## Regression Results\nBest Params: {'linearregression__fit_intercept': False, 'linearregression__normalize': True} \nBest score: 5.223340476296619\n    \nBest Params: {'svr__C': 10, 'svr__gamma': 1} \nBest score: 3.7407103357168365\n    \nBest Params: {'decisiontreeregressor__max_depth': 20, 'decisiontreeregressor__min_impurity_decrease': 0.1, 'decisiontreeregressor__min_samples_leaf': 2} \nBest score: 4.1500331172512475\n    \nBest Params: {'gradientboostingregressor__learning_rate': 0.1, 'gradientboostingregressor__max_depth': 20, 'gradientboostingregressor__min_impurity_decrease': 0.1, 'gradientboostingregressor__min_samples_leaf': 10, 'gradientboostingregressor__n_estimators': 200} \nBest score: 2.5778462024599933 \n\nBest Params: {'randomforestregressor__max_depth': 20, 'randomforestregressor__min_impurity_decrease': 0.1, 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__n_estimators': 200} \nBest score: 3.9892858414976233\n    \nBest Params: {'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 30, 'xgbregressor__n_estimators': 50} \nBest score: 2.9783876039053268\n\nBest Params: {'mlpregressor__activation': 'relu', 'mlpregressor__hidden_layer_sizes': (50, 75, 100), 'mlpregressor__learning_rate': 'invscaling'} \nBest score: 3.7417429844614176","9c9fa4e1":"# Model performance","46802890":"# Logistic Regression\nlogistic_pipe = make_pipeline(LogisticRegression(solver='liblinear'))\nlogistic_param = {\n    'logisticregression__C': [0.1, 1.0, 10],\n    'logisticregression__fit_intercept': [True, False]\n}\n\nlogistic_search = GridSearchCV(logistic_pipe, \n                      logistic_param,\n                      scoring=\"f1_weighted\",\n                      n_jobs=-1,\n                      cv = 5)\nlogistic_search.fit(X_cla, Y_cla)\nprint(f'Best Params: {logistic_search.best_params_} \\nBest score: {logistic_search.best_score_}')","14be9789":"# SVC\nsvc_pipe = make_pipeline(SVC())\nsvc_param = {\n    'svc__C': [0.1, 1.0, 10],\n    'svc__kernel': ['rbf', 'sigmoid'],\n    'svc__gamma': [1, 0.1, 0.01]\n}\n\nsvc_search = GridSearchCV(svc_pipe, \n                         svc_param,\n                         scoring=\"f1_weighted\",\n                         n_jobs=-1,\n                         cv = 5)\nsvc_search.fit(X_cla, Y_cla)\nprint(f'Best Params: {svc_search.best_params_} \\nBest score: {svc_search.best_score_}')","11e83e67":"# Decision Tree Classification\ndtc_pipe = make_pipeline(DecisionTreeClassifier(random_state=0))\ndtc_param = {\n    'decisiontreeclassifier__max_depth': [5, 10, 20],\n    'decisiontreeclassifier__min_samples_leaf': [2, 5, 10],\n    'decisiontreeclassifier__min_impurity_decrease': [0.1, 0.2, 0.5]\n}\n\ndtc_search = GridSearchCV(dtc_pipe, \n                         dtc_param,\n                         scoring=\"f1_weighted\",\n                         n_jobs=-1,\n                         cv = 5)\ndtc_search.fit(X_cla, Y_cla)\nprint(f'Best Params: {dtc_search.best_params_} \\nBest score: {dtc_search.best_score_}')","0cd49995":"# Gradient Boosting Classification\ngbtc_pipe = make_pipeline(GradientBoostingClassifier(random_state=0))\ngbtc_param = {\n    'gradientboostingclassifier__learning_rate': [0.1, 0.3, 0.7],\n    'gradientboostingclassifier__max_depth': [5, 10, 20],\n    'gradientboostingclassifier__n_estimators': [50, 100, 200],\n    'gradientboostingclassifier__min_samples_leaf': [2, 5, 10],\n    'gradientboostingclassifier__min_impurity_decrease': [0.1, 0.2, 0.5]\n}\n\ngbtc_search = GridSearchCV(gbtc_pipe, \n                         gbtc_param,\n                         scoring=\"f1_weighted\",\n                         n_jobs=-1,\n                         cv = 5)\ngbtc_search.fit(X_cla, Y_cla)\nprint(f'Best Params: {gbtc_search.best_params_} \\nBest score: {gbtc_search.best_score_}')","ee3d709b":"# Random Forest Classification\nrfc_pipe = make_pipeline(RandomForestClassifier(random_state=0))\nrfc_param = {\n    'randomforestclassifier__max_depth': [5, 10, 20],\n    'randomforestclassifier__n_estimators': [50, 100, 200]\n    'randomforestclassifier__min_impurity_decrease': [0.1, 0.2, 0.5],\n    'randomforestclassifier__min_samples_leaf': [2, 5, 10],\n}\n\nrfc_search = GridSearchCV(rfc_pipe, \n                         rfc_param,\n                         scoring=\"f1_weighted\",\n                         n_jobs=-1,\n                         cv = 5)\nrfc_search.fit(X_cla, Y_cla)\nprint(f'Best Params: {rfc_search.best_params_} \\nBest score: {rfc_search.best_score_}')","810aea9d":"# XGB Classification\n# https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/param_tuning.html\n# https:\/\/stats.stackexchange.com\/questions\/243207\/what-is-the-proper-usage-of-scale-pos-weight-in-xgboost-for-imbalanced-datasets\nxgbc_pipe = make_pipeline(XGBClassifier(eval_metric='mlogloss', random_state=0))\nxgbc_param = {\n              'xgbclassifier__learning_rate': [0.1, 0.3, 0.7],\n              'xgbclassifier__max_depth': [5, 10, 20],\n              'xgbclassifier__n_estimators': [50, 100, 200],\n              'xgbclassifier__scale_pos_weight': [5, 10, 30]}\n\nxgbc_search = GridSearchCV(xgbc_pipe, \n                         xgbc_param,\n                         scoring=\"f1_weighted\",\n                         n_jobs=-1,\n                         cv = 5)\nxgbc_search.fit(X_cla, Y_cla)\nprint(f'Best Params: {xgbc_search.best_params_} \\nBest score: {xgbc_search.best_score_}')","f2967e3c":"# Multi-layer Perceptron classifier\nmlpc_pipe = make_pipeline(MLPClassifier(random_state=0))\nmlpc_param = {\n              'mlpclassifier__activation': ['logistic', 'tanh', 'relu'],\n              'mlpclassifier__hidden_layer_sizes': [(50,), (100,100)],\n              'mlpclassifier__learning_rate': ['invscaling', 'adaptive']\n             }\n\nmlpc_search = GridSearchCV(mlpc_pipe, \n                         mlpc_param,\n                         scoring=\"f1_weighted\",\n                         n_jobs=-1,\n                         cv = 5)\nmlpc_search.fit(X_cla, Y_cla)\nprint(f'Best Params: {mlpc_search.best_params_} \\nBest score: {mlpc_search.best_score_}')","ee0d3dc5":"pd.DataFrame([logistic_search.best_score_, svc_search.best_score_, dtc_search.best_score_, gbtc_search.best_score_, rfc_search.best_score_, xgbc_search.best_score_, mlpc_search.best_score_], \n             columns=['F1-score'],\n             index= ['logistics Regression',\n                    'Support Vector Machine',\n                    'Decision Tree',\n                    'Gradient Boosting Tree',\n                    'Random Forest',\n                    'XGBoost',\n                    'Multi-layer Perceptron']).sort_values(by=['F1-score'], ascending=False)","df3e7877":"## Classification Results\nBest Params: {'logisticregression__C': 10, 'logisticregression__fit_intercept': False} \nBest score: 0.7994132420956104\n\nBest Params: {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'} \nBest score: 0.8193413563558885\n\nBest Params: {'decisiontreeclassifier__max_depth': 5, 'decisiontreeclassifier__min_impurity_decrease': 0.1, 'decisiontreeclassifier__min_samples_leaf': 2} \nBest score: 0.7886393322476093\n    \nBest Params: {'gradientboostingclassifier__learning_rate': 0.1, 'gradientboostingclassifier__max_depth': 10, 'gradientboostingclassifier__min_impurity_decrease': 0.1, 'gradientboostingclassifier__min_samples_leaf': 2, 'gradientboostingclassifier__n_estimators': 100} \nBest score: 0.8858655029961581\n    \nBest Params: {'randomforestclassifier__max_depth': 5, 'randomforestclassifier__min_impurity_decrease': 0.1, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 50} \nBest score: 0.7886393322476093\n    \nBest Params: {'xgbclassifier__learning_rate': 0.3, 'xgbclassifier__max_depth': 20, 'xgbclassifier__n_estimators': 100, 'xgbclassifier__scale_pos_weight': 5} \nBest score: 0.8922868776993844\n    \nBest Params: {'mlpclassifier__activation': 'tanh', 'mlpclassifier__hidden_layer_sizes': (100, 100), 'mlpclassifier__learning_rate': 'invscaling'} \nBest score: 0.8205639817810895","e5f32fb3":"# Model performance","6ca8c8bf":"The information that we can find here:\n* **Distance**: 25%\/50% percentile are all 0; 75% percentile is 0.279 which is quite abnormal(too small).\n* **Precipitation**: 25%\/50%\/75% percentile are all 0 which is also quite abnormal.","0cca24ab":"<a id=\"2.4.2\"><\/a>\n### 2.4.2. Median imputation","049a2628":"<a id=\"6\"><\/a>\n# 6. Conclusion","d285747c":"Generally speaking, the longer the duration the higher the severity level will be.","119b1a24":"<a id=\"3.2.5\"><\/a>\n### 3.2.5. What is the top 10 Zipcode with the most accidents?","e5899fb2":"<a id=\"5.2\"><\/a>\n## 5.2. End Time Prediction","91ac9f6f":"<a id=\"5.1.1.6\"><\/a>\n#### 5.1.1.6. XGBoost","df25c61e":"<a id=\"5.1.2.3\"><\/a>\n#### 5.1.2.3. Decision Tree","a4c9f3ad":"#### Weather Categorizing\n\n| Weather      | Contain  |  Key words |\n| :---         |  :----  |  :----  |\n| Fair         | 'Fair \/ Windy' | \u2018Fair\u2019|\n| Cloudy       |'Mostly Cloudy', 'Partly Cloudy', 'Scattered Clouds', 'Cloudy \/ Windy', 'Partly Cloudy \/ Windy', 'Mostly Cloudy \/ Windy', 'Funnel Cloud' | 'Cloud' |\n| Clear        | 'Clear' | 'Clear' |\n| Overcast     | 'Overcast' | 'Overcast' |\n| Snow         |'Light Snow', 'Wintry Mix', 'Heavy Snow', 'Snow', 'Light Snow \/ Windy', 'Blowing Snow', 'Snow \/ Windy', 'Snow and Sleet',  'Blowing Snow \/ Windy',  'Sleet', 'Light Snow and Sleet',   'Light Snow with Thunder', 'Light Snow Showers', 'Heavy Snow with Thunder','Heavy Snow \/ Windy',  'Light Sleet', 'Heavy Sleet',  'Snow and Sleet \/ Windy', 'Thunderstorms and Snow',  'Light Thunderstorms and Snow', 'Heavy Blowing Snow','Light Sleet \/ Windy', 'Sleet \/ Windy', 'Snow Showers', 'Light Blowing Snow', 'Light Snow Shower','Drifting Snow','Low Drifting Snow','Light Snow and Sleet \/ Windy',  'Snow Grains',  'Light Snow Grains',  'Rain and Sleet', 'Thunder \/ Wintry Mix', 'Thunder \/ Wintry Mix \/ Windy', 'Wintry Mix \/ Windy' | 'Snow', 'Wintry', 'Sleet' |\n| Haze         | 'Smoke',  'Fog',  'Mist',  'Shallow Fog',  'Haze \/ Windy', 'Patches of Fog', 'Light Freezing Fog',  'Fog \/ Windy', 'Smoke \/ Windy', 'Partial Fog', 'Patches of Fog \/ Windy','Light Haze','Light Fog' | 'Smoke',  'Fog',  'Mist', 'Haze' |\n| Rain         | 'Light Rain', 'Rain', 'Light Drizzle', 'Light Rain Shower',  'Heavy Rain',  'Light Freezing Rain',  'Drizzle', 'Rain \/ Windy',  'Drizzle and Fog', 'Light Rain with Thunder',  'Light Rain \/ Windy', 'Heavy Drizzle',  'Heavy Rain \/ Windy',  'Showers in the Vicinity',  'Light Freezing Drizzle', 'Light Drizzle \/ Windy', 'Heavy Rain Shower',   'Rain Showers',  'Light Rain Showers',  'Rain Shower', 'Freezing Rain','Light Freezing Rain \/ Windy', 'Drizzle \/ Windy','Light Rain Shower \/ Windy', 'Freezing Drizzle', 'Heavy Freezing Rain','Heavy Rain Showers','Heavy Freezing Drizzle', 'Rain and Sleet', 'Freezing Rain \/ Windy' | 'Rain', 'Drizzle', 'Showers' |\n| Thunderstorm |  'Thunderstorms and Rain', 'Light Thunderstorms and Rain', 'Heavy Thunderstorms and Rain', 'T-Storm', 'Heavy T-Storm',  'Heavy T-Storm \/ Windy',  'T-Storm \/ Windy',  'Heavy Thunderstorms and Snow', 'Thunderstorms and Snow',   'Light Thunderstorms and Snow', 'Light Thunderstorm', 'Heavy Thunderstorms with Small Hail' | 'Thunderstorms', 'T-Storm' |\n| Windy        | 'Fair \/ Windy','Cloudy \/ Windy','Partly Cloudy \/ Windy', 'Mostly Cloudy \/ Windy','Light Snow \/ Windy','Fog \/ Windy', 'Smoke \/ Windy','Rain \/ Windy','Light Rain \/ Windy','Heavy Rain \/ Windy','Light Drizzle \/ Windy', 'Blowing Dust \/ Windy', 'Heavy T-Storm \/ Windy',  'T-Storm \/ Windy',   'Squalls \/ Windy',  'Thunder \/ Windy',  'Blowing Snow \/ Windy',   'Squalls', 'Heavy Snow \/ Windy',  'Snow and Sleet \/ Windy', 'Light Freezing Rain \/ Windy',   'Patches of Fog \/ Windy', 'Light Rain Shower \/ Windy', 'Light Sleet \/ Windy', 'Sleet \/ Windy', 'Light Snow and Sleet \/ Windy',  'Widespread Dust \/ Windy',  'Thunder \/ Wintry Mix \/ Windy', 'Wintry Mix \/ Windy', 'Thunder and Hail \/ Windy',  'Freezing Rain \/ Windy' | 'Windy', 'Squalls' |\n| Hail         |  'Small Hail', 'Light Ice Pellets',  'Ice Pellets', 'Thunder and Hail','Light Hail','Heavy Ice Pellets', 'Hail', 'Heavy Thunderstorms with Small Hail', 'Thunder and Hail \/ Windy' | 'Hail', 'Ice Pellets' |\n| Thunder      | 'Thunder in the Vicinity', 'Thunder',   'Thunder \/ Windy',  'Light Snow with Thunder', 'Heavy Snow with Thunder','Thunder and Hail',   'Thunder \/ Wintry Mix',  'Thunder \/ Wintry Mix \/ Windy', 'Thunder and Hail \/ Windy' | 'Thunder' |\n| Dust         | 'Dust Whirls',  'Sand \/ Dust Whirlwinds', 'Sand \/ Dust Whirls Nearby', 'Blowing Sand', 'Blowing Dust \/ Windy', 'Widespread Dust', 'Blowing Dust',  'Widespread Dust \/ Windy' | 'Dust'|\n| Tornado      | 'Tornado' | 'Tornado' |\n| N\/A          | 'N\/A Precipitation' | 'N\/A Precipitation' |\n  ","b6a2a827":"<a id=\"4.3\"><\/a>\n## 4.3. Label Encoding","61de676e":"<a id=\"5.1.2.4\"><\/a>\n#### 5.1.2.4. Gradient Boost Tree","e9889e4e":"<a id=\"5.3\"><\/a>\n## 5.3. Severity Prediction","b4b758a4":"<a id=\"5.2.3\"><\/a>\n### 5.2.3 Visualization ","2d50d212":"<a id=\"3.2.3\"><\/a>\n### 3.2.3. What are the top 10 cities with the most accidents?","be07fa1c":"<a id=\"5.1.1.3\"><\/a>\n#### 5.1.1.3. Decision Tree","ed534ee9":"Most of the accidents happened at the right side of the road which is quite an interesting finding.","05e6d384":"The peak of the weekend is during 12:00 ~ 16:00.","9863ceda":"<a id=\"2.4.3\"><\/a>\n### 2.4.3. Mean imputation","10628206":"<a id=\"5.3.1\"><\/a>\n### 5.3.1 Parameter Tuning","82d6e2c7":"The reason to specify the specific data type of each variable is to choose suitable statistical strategy to do analysis.","d06c5aa3":"<a id=\"5.1.2.1\"><\/a>\n#### 5.1.2.1. Logistic Regression","d483c406":"<a id=\"3.3.1\"><\/a>\n### 3.3.1. Accidents yearly change","3d3f5dd8":"Accidents mostly happened at daytime.","334c46cc":"<a id=\"5.1.1.4\"><\/a>\n#### 5.1.1.4. Gradient Boosting Tree","b2a27a12":"The decrease of the accidents number at weekends mainly because the decrease at 7:00\/8:00(Go to work) and 16:00\/17:00(go back home). The peak of the weekend is during 12:00 ~ 16:00.","a2fe9e77":"<a id=\"3.4.2\"><\/a>\n### 3.4.2. Top 15 Wind_Direction","9db1dd75":"<a id=\"4.4\"><\/a>\n## 4.4. Correlation Analysis\n\n> https:\/\/www-bmj-com.ezproxy.nottingham.ac.uk\/about-bmj\/resources-readers\/publications\/statistics-square-one\/11-correlation-and-regression\n* **very weak**: 0 ~ 0.19\n* **weak**: 0.2 ~ 0.39\n* **moderate**: 0.4 ~ 0.59\n* **strong**: 0.6 ~ 0.79\n* **very strong**: 0.8 ~ 1","ebba292a":"Generally speaking, the longer the influence distance the higher the severity level will be.","3dfa491c":"There is a drop in the number of accidents during the weekend.","06e54525":"**Freezing rain with windy**, **light blowing Snow** and **patches of fog** with windy are the top 3 dangeous weather condition. ","1fe13c80":"#### Statistic View:\n* Distance: 25%\/50% percentile are all 0; 75% percentile is 0.279 which is quite abnormal(too small).\n* Precipitation: 25%\/50%\/75% percentile are all 0 which is also quite abnormal.\n* Street: **1-5N** seems to be a common street name that an accidents would happen. The name of the street could mean something or contain some special characters.\n* Side: It seems that accidents usually happen on the **right side** of the road.\n* City\/County: **Los Angeles** is the place that traffic accidents are likely to happen.\n* State: **CA(California)** is the state that traffic accidents are likely to happen.\n* Wind_Direction: **CALM** is the most frequent wind direction.\n* Weather_Condition: **Fair** is the most frequent weather condition.\n\n#### Location Analysis:\n* Top 10 states: CA, FL, TX, NC, SC, NY, OR, VA, PA, IL\n* Top 10 cities: Houston, Charlotte, Los Angeles, Miami, Dallas, Austin, Raleigh, Orlando, Sacamento, Baton Rouge.\n* Most of the accidents happened at the right side of the road which is quite an interesting finding.\n\n#### Time Analysis:\n* The number of the accidents are increasing every year(2016~2020).\n* It seems that there is an increase in the number of the accidents from July(7) to December(12). Fall and winter is reasonable to be more dangerous. However, the number of the accidents in January and February are so much lower than December, which is quite interesting.\n* There is a drop in the number of accidents during the weekend.\n* It seems that 7:00\/8:00(Go to work) and 16:00\/17:00(go back home) are the time where accidents happen during a day. \n* The decrease of the accidents number at weekends mainly because the decrease at 7:00\/8:00(Go to work) and 16:00\/17:00(go back home). The peak of the weekend is during 12:00 ~ 16:00.\n\n#### Environment Analysis:                        \n* **Freezing rain with windy**, **light blowing Snow** and **patches of fog** with windy are the top 3 dangeous weather condition. \n* It doesn't show that the wind direction have a significant influence on severity.\n* Temperature(F):  lower temperature -> higher Severity  \n* Humidity(%):  higher humidity -> higher Severity  \n* Pressure(in):  lower pressure -> higher Severity  \n* Visibility(mi):  lower visibility -> higher Severity  \n* Wind_Speed(mph): higher wind speed -> higher Severity  \n* In summary, all the results fit with cold, chill, freezing weather condition. Eg. Freezing rain with windy, snow and so on.\n* Accidents mostly happened at daytime.\n\n#### Infrastructure Analysis:\n* The accident usually happend at the palce where there are less traffic facilities.\n\n#### Correlation:\nThere are weak relationship between:\n* **Pressure** and **Temperature**\n* **Wind_Speed** and **Humidity**\n* **Visibility** and **Humidity**\n\nThere are Moderate relationship between:\n* **Bump** and **Traffic_Calming**\n* **Weather_thunder** and **Weather_Thunderstorm** \n\nThere are Strong relationship between:\n* **Crossing** and **Traffic_Signal**\n    \n#### Modeling:\nBest model to accidents End-time prediction: Gradient Boost Tree; {'gradientboostingregressor__learning_rate': 0.1, \n                                                                   'gradientboostingregressor__max_depth': 20, \n                                                                   'gradientboostingregressor__min_impurity_decrease': 0.1, \n                                                                   'gradientboostingregressor__min_samples_leaf': 10, \n                                                                   'gradientboostingregressor__n_estimators': 200}   \nBest model to accidents Severity prediction: Xgboost; {'xgbclassifier__learning_rate': 0.3, \n                                                       'xgbclassifier__max_depth': 20, \n                                                       'xgbclassifier__n_estimators': 100, \n                                                       'xgbclassifier__scale_pos_weight': 5}  ","9953ebe7":"<a id=\"4.1\"><\/a>\n## 4.1. Feature choosing","78572c31":"<a id=\"1.3\"><\/a>\n## 1.3. Statistical View ","81b7a830":"<a id=\"5.1.2.2\"><\/a>\n#### 5.1.2.2. Support Vector Machine","ce118068":"<a id=\"2.4\"><\/a>\n## 2.4. Data Imputation\n> Choose the suitable imputation tech which can highly represent the central tendency of the data.","f49f98ca":"<a id=\"3.4\"><\/a>\n## 3.4. Environment Analysis","b8291043":"<a id=\"2.3\"><\/a>\n## 2.3. Data type correcting","9a1eb7c2":"<a id=\"5.1.2\"><\/a>\n### 5.1.2. Severity Prediction","7de1e0e4":"Moderate relationship: \n* Bump and Traffic_Calming    \n* Weather_thunder and Weather_Thunderstorm.     \n\nStrong relationship: \n* Crossing and Traffic_Signal","07098dda":"It seems that 7:00\/8:00(Go to work) and 16:00\/17:00(go back home) are the time where accidents happen during a day. ","2f45632e":"<a id=\"4.2\"><\/a>\n## 4.2. One Hot Encoding","9b31cb1b":"<a id=\"3.1.2\"><\/a>\n### 3.1.2. What is the relationship between distance and severity?","e71a1899":"<a id=\"5.3.3\"><\/a>\n### 5.3.3 Visualization ","f4112ee1":"<a id=\"3.3.3\"><\/a>\n### 3.3.3. Week","20a364aa":"<a id=\"5.1.2.7\"><\/a>\n#### 5.1.2.7. Multi-layer Perceptron Classification","687c7e9f":"<a id=\"4\"><\/a>\n# 4. Feature Engineering","e5c73756":"<a id=\"5.1.1.5\"><\/a>\n#### 5.1.1.5. Random Forest","7207ef3d":"<a id=\"1.4\"><\/a>\n## 1.4. Data Categorizing\n\n* **Basic info**:  'ID', 'Severity', 'Start_Time', 'End_Time', 'Distance(mi)', 'Description'\n\n* **Location**(Geographic factor):  'Start_Lat', 'Start_Lng','End_Lat', 'End_Lng', 'Number', 'Street','Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code'\n  \n* **Environment**(Environmental factor):  'Weather_Timestamp', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', 'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight'\n \n* **Infrastructure**(Infrastructural factor):  'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop'","140e30a8":"The accident usually happend at the palce where there are less traffic facilities.","26d2ed09":"<a id=\"3\"><\/a>\n# 3. Data Analysis","9258caf9":"It seems Over-sampling and Under-sampling technique doesn't help to improve the model performance. Need to explore more methods.","28362607":"It seems that accidents number of all the week changes proportionally with month.","b0cbb66b":"<a id=\"3.2\"><\/a>\n## 3.2. Location Analysis","c4365513":"<a id=\"3.3.4\"><\/a>\n### 3.3.4. Hour","728ae43a":"<a id=\"3.2.1\"><\/a>\n### 3.2.1. What are the top 10 states with the most accidents?","14852a8f":"# Table of Content\n1. [Data Overview](#1)\n    * [1. Load Data](#1.1)\n    * [2. Data Type](#1.2)\n    * [3. Statistical View](#1.3)\n    * [4. Data Categorizing](#1.4)\n2. [Data Preprocessing](#2)\n    * [1. Drop irrelevant columns](#2.1)\n    * [2. Drop the column with Missing Value(>40%)](#2.2)\n    * [3. Data type correcting](#2.3)\n    * [4. Data Imputation](#2.4)\n        * [1. Drop all NaN\/NA\/null](#2.4.1)\n        * [2. Median imputation](#2.4.2)\n        * [3. Mean imputation](#2.4.3)\n3. [Data Analysis](#3)\n    * [1. Basic Analysis](#3.1)\n        * [1. What is the distribution of accidents severity?](#3.1.1)\n        * [2. What is the relationship between distance and severity?](#3.1.2)\n    * [2. Location Analysis](#3.2)\n        * [1. What are the top 10 states with the most accidents?](#3.2.1)\n        * [2. What are the top 10 counties with the most accidents?](#3.2.2)\n        * [3. What are the top 10 cities with the most  accidents?](#3.2.3)\n        * [4. What are the top 10 streets with the most accidents?](#3.2.4)\n        * [5. What are the top 10 Zipcode with the most accidents?](#3.2.5)\n        * [6. What are the accidents distribution by street Side?](#3.2.6)\n    * [3. Time Analysis](#3.3)\n        * [1. Accidents yearly change](#3.3.1)\n        * [2. Accidents monthly change](#3.3.2)\n        * [3. Accidents weekly change](#3.3.3)\n        * [4. Accidents hourly change](#3.3.4)\n        * [5. Duration and Severity](#3.3.5)\n    * [4. Environment Analysis](#3.4)\n        * [1. Top 15 Weather_Condition](#3.4.1)\n        * [2. Top 15 Wind_Direction](#3.4.2)\n        * [3. Environment Attribute(numerical) distribution and the relationship with Severity](#3.4.3)\n        * [4. Accidents distribution by Sunrise && Sunset](#3.4.4)\n    * [5. Infrastructure Analysis](#3.5)\n4. [Feature Engineering](#4)\n    * [1. Feature Choosing](#4.1)\n    * [2. One Hot Encoding](#4.2)\n    * [3. Label Encoding](#4.3)\n    * [4. Correlation Analysis](#4.4)\n5. [Modelling](#5)\n    * [1. Workflow Demonstration](#5.1)\n        * [1. End-Time Prediction](#5.1.1)\n            * [1. Linear Regression](#5.1.1.1)\n            * [2. Support Vector Machine](#5.1.1.2)\n            * [3. Decision Tree](#5.1.1.3)\n            * [4. Gradient Boosting Tree](#5.1.1.4)\n            * [5. Random Forest](#5.1.1.5)\n            * [6. XGBoost](#5.1.1.6)\n            * [7. Multi-layer Perceptron Regression](#5.1.1.7)\n            * [8. Model Comparison](#5.1.1.8)\n        * [2. Severity Prediction](#5.1.2)\n            * [1. Logistics Regression](#5.1.2.1)\n            * [2. Support Vector Machine](#5.1.2.2)\n            * [3. Decision Tree](#5.1.2.3)\n            * [4. Gradient Boosting Tree](#5.1.2.4)\n            * [5. Random Forest](#5.1.2.5)\n            * [6. XGBoost](#5.1.2.6)\n            * [7. Multi-layer Perceptron Classification](#5.1.1.7)\n            * [8. Model Comparison](#5.1.2.8)\n            * [9. Deal with imbalance data](#5.1.2.9)\n            * [10. Severity Prediction Visualization](#5.1.2.10)\n    * [2. End-Time Prediction](#5.2)\n        * [1. Parameter Turning](#5.2.1)\n        * [2. Model Comparison](#5.2.2)\n        * [3. Visualization](#5.2.3)\n    * [3. Severity Prediction](#5.3)\n        * [1. Parameter Turning](#5.3.1)\n        * [2. Model Comparison](#5.3.2)\n        * [3. Visualization](#5.3.3)\n6. [Conclusion](#6)","4072f79d":"As we can see from the graph, **level 2** is the most frequent severity which is 76.1% of the total. That means our target variable(label) is quite **unbalanced**.","b34fde6d":"## [Key Findings](#6)","dfd48168":"<a id=\"3.3.2\"><\/a>\n### 3.3.2. Month","8233219b":"<a id=\"3.3.5\"><\/a>\n### 3.3.5. Duration and Severity","56849bab":"<a id=\"5\"><\/a>\n# 5. Modelling","ef6a0b36":"The number of the accidents are increasing every year(2016~2020). The accidents change rate are much higher than registered vehicles change rate. ","92f59d93":"<a id=\"3.4.4\"><\/a>\n### 3.4.4. Accidents distribution by Sunrise && Sunset","dd0ac6d4":"Temperature(F):  lower temperature -> higher Severity  \nHumidity(%):  higher humidity -> higher Severity  \nPressure(in):  lower pressure -> higher Severity  \nVisibility(mi):  lower visibility -> higher Severity  \nWind_Speed(mph): higher wind speed -> higher Severity  \nIn summary, all the results fit with cold, chill, freezing weather condition. Eg. Freezing rain with windy, snow and so on.","f4e2aadf":"<a id=\"5.2.1\"><\/a>\n### 5.2.1 Parameter Turning","07522dc2":"<a id=\"5.1.1.2\"><\/a>\n#### 5.1.1.2. Support Vector Machine","ef1139ba":"The information that we can find here:\n* Street: **1-5N** seems to be a common street name that an accidents would happen. The name of the street could mean something or contain some special characters.\n* Side: It seems that accidents usually happen on the **right side** of the road.\n* City\/County: **Los Angeles** is the place that traffic accidents are likely to happen.\n* State: **CA(California)** is the state that traffic accidents are likely to happen.\n* Wind_Direction: **CALM** is the most frequent wind direction.\n* Weather_Condition: **Fair** is the most frequent weather condition.\n\nWhile, the results of Wind_Direction\/Weather_Condition could not be so much valuable, perhaps they are the most frequent conditions in US.","884ac367":"<a id=\"3.1\"><\/a>\n## 3.1. Basic Analysis","2a2900b8":"<a id=\"1\"><\/a>\n# 1. Data Overview","fd0ce3b4":"<a id=\"5.3.2\"><\/a>\n### 5.3.2 Model Comparison","0b6e300e":"It seems that there is an increase in the number of the accidents from July(7) to December(12). Fall and winter is reasonable to be more dangerous. However, the number of the accidents in January and February are so much lower than December, which is quite interesting.","d6ad145f":"<a id=\"3.2.4\"><\/a>\n### 3.2.4. What is the top 10 Streets with the most accidents?","8079d5bf":"<a id=\"2\"><\/a>\n# 2. Data Preprocessing","ea5df0d3":"The dataset has **2906610** entries and **46** features + **1** target variable. Automatic Type Recognition: 13 of them are bool type, 13 of them are float64, 1 of them is int64, 20 of them are object. Memory usage: **790.0+ MB**.","68de9059":"<a id=\"2.4.1\"><\/a>\n### 2.4.1. Drop all NaN\/NA\/null ","0159c4cd":"<a id=\"5.1.2.5\"><\/a>\n#### 5.1.2.5. Random Forest","c97cc894":"<a id=\"1.2\"><\/a>\n## 1.2. Data Type\n\n> [NOIR](https:\/\/www.questionpro.com\/blog\/nominal-ordinal-interval-ratio\/): Nominal, Ordinal, Interval, Ratio.   \nSpecify the data type of each variable for the following statistic analysis.\n\n| Variable              | Type     | Description | Nullable |\n| :---                  |  :----:  |  :----      |  :----:  |\n| ID                    | Nominal  | This is a unique identifier of the accident record. | No |\n| Severity              | Ordinal  |Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay). | No |\n| Start_Time            | Interval | Shows start time of the accident in local time zone.  | No |\n| End_Time              | Interval | Shows end time of the accident in local time zone. End time here refers to when the impact of accident on traffic flow was dismissed.  | No |\n| Start_Lat             | Interval | Shows latitude in GPS coordinate of the start point.  | No |\n| Start_Lng             | Interval | Shows longitude in GPS coordinate of the start point.  | No |\n| End_Lat               | Interval | Shows latitude in GPS coordinate of the end point.  | Yes |\n| End_Lng               | Interval | Shows longitude in GPS coordinate of the end point.  | Yes |\n| Distance(mi)          | Ratio    | The length of the road extent affected by the accident.  | No |\n| Description           | Nominal  | Shows natural language description of the accident.  | No |\n| Number                | Nominal  | Shows the street number in address field.  | Yes |\n| Street                | Nominal  | Shows the street name in address field.  | Yes |\n| Side                  | Nominal  | Shows the relative side of the street (Right\/Left) in address field.  | Yes |\n| City                  | Nominal  | Shows the city in address field.  | Yes |\n| County                | Nominal  | Shows the county in address field.  | Yes |\n| State                 | Nominal  | Shows the state in address field.  | Yes |\n| Zipcode               | Nominal  | Shows the zipcode in address field.  | Yes |\n| Country               | Nominal  | Shows the country in address field.  | Yes |\n| Timezone              | Nominal  | Shows timezone based on the location of the accident (eastern, central, etc.).  | Yes |\n| Airport_Code          | Nominal  | Denotes an airport-based weather station which is the closest one to location of the accident.  | Yes |\n| Weather_Timestamp     | Interval | Shows the time-stamp of weather observation record (in local time).  | Yes |\n| Temperature(F)        | Interval | Shows the temperature (in Fahrenheit).  | Yes |\n| Wind_Chill(F)         | Interval | Shows the wind chill (in Fahrenheit).  | Yes |\n| Humidity(%)           | Interval | Shows the humidity (in percentage).  | Yes |\n| Pressure(in)          | Interval | Shows the air pressure (in inches).  | Yes |\n| Visibility(mi)        | Interval | Shows visibility (in miles).  | Yes |\n| Wind_Direction        | Nominal  | Shows wind direction.  | Yes |\n| Wind_Speed(mph)       | Ratio    | Shows wind speed (in miles per hour).  | Yes |\n| Precipitation(in)     | Ratio    | Shows precipitation amount in inches, if there is any.  | Yes |\n| Weather_Condition     | Nominal  | Shows the weather condition (rain, snow, thunderstorm, fog, etc.)  | Yes |\n| Amenity               | Nominal  | A POI annotation which indicates presence of amenity in a nearby location.  | No |\n| Bump                  | Nominal  | A POI annotation which indicates presence of speed bump or hump in a nearby location.  | No |\n| Crossing              | Nominal  | A POI annotation which indicates presence of crossing in a nearby location.  | No |\n| Give_Way              | Nominal  | A POI annotation which indicates presence of give_way in a nearby location.  | No |\n| Junction              | Nominal  | A POI annotation which indicates presence of junction in a nearby location.  | No |\n| No_Exit               | Nominal  | A POI annotation which indicates presence of no_exit in a nearby location.  | No |\n| Railway               | Nominal  | A POI annotation which indicates presence of railway in a nearby location. | No | \n| Roundabout            | Nominal  | A POI annotation which indicates presence of roundabout in a nearby location.  | No |\n| Station               | Nominal  | A POI annotation which indicates presence of station in a nearby location.  | No |\n| Stop                  | Nominal  | A POI annotation which indicates presence of stop in a nearby location.  | No |\n| Traffic_Calming       | Nominal  | A POI annotation which indicates presence of traffic_calming in a nearby location.  | No |\n| Traffic_Signal        | Nominal  | A POI annotation which indicates presence of traffic_signal in a nearby loction.  | No |\n| Turning_Loop          | Nominal  | A POI annotation which indicates presence of turning_loop in a nearby location.  | No |\n| Sunrise_Sunset        | Nominal  | Shows the period of day (i.e. day or night) based on sunrise\/sunset. | Yes |\n| Civil_Twilight        | Nominal  | Shows the period of day (i.e. day or night) based on civil twilight. | Yes |\n| Nautical_Twilight     | Nominal  | Shows the period of day (i.e. day or night) based on nautical twilight. | Yes |\n| Astronomical_Twilight | Nominal  | Shows the period of day (i.e. day or night) based on astronomical twilight. | Yes |","be9e5587":"<a id=\"5.1.2.10\"><\/a>\n#### 5.1.2.10. Severity Prediction Visualization","87b15b28":"<a id=\"3.2.6\"><\/a>\n### 3.2.6. What are the accidents distribution by street Side?","334606f3":"<a id=\"5.1.2.6\"><\/a>\n#### 5.1.2.6. XGBoost","e11b0238":"<a id=\"5.1.1.1\"><\/a>\n#### 5.1.1.1. Linear Regression","cd9942c7":"<a id=\"3.4.3\"><\/a>\n### 3.4.3. Environment Attribute(numerical) distribution and the relationship with Severity","87a17080":"After Data Imputation:\n* **data_preprocessed_dropNaN_df**: The dataset that simply drop all the NaN values.\n* **data_preprocessed_median_df**: The dataset that use grouped median imputation for numerical data and grouped majority value imputation for categorical data.\n* **data_preprocessed_mean_df**: The dataset that use grouped mean imputation for numerical data and grouped majority value imputation for categorical data.","6c920b8b":"<a id=\"3.3\"><\/a>\n## 3.3. Time Analysis","b8140cac":"<a id=\"5.2.2\"><\/a>\n### 5.2.2 Model Comparison","3f7a0bde":"<a id=\"3.5\"><\/a>\n## 3.5. Infrastructure Analysis\n> Include 'Traffic_Signal', 'Crossing', 'Station','Amenity', 'Bump', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout','Stop', 'Traffic_Calming', 'Turning_Loop'.  ","b50cc451":"<a id=\"3.4.1\"><\/a>\n### 3.4.1. Top 15 Weather_Condition","930ac715":"<a id=\"5.1.2.9\"><\/a>\n#### 5.1.2.9. Deal with Imbalanced data","98c53135":"<a id=\"5.1.1\"><\/a>\n### 5.1.1. End-Time Prediction","ce9bf1ca":"## Reference\n1. https:\/\/www.kaggle.com\/art12400\/us-car-accidents-severity-analysis; Good idea on categorizing the causality of the accidents. Weather analysis and Infrastructure analysis are thoughtful.\n2. https:\/\/www.kaggle.com\/zhuochenglin\/us-accidents-project-visualization-and-modeling; Good idea to go in to the combination of month and weekdays, Hour and weekdays.\n3. https:\/\/www.kaggle.com\/fabriziorossi\/accident-severity-prediction; Very comprehensive, from analysis to prediction. PR and ROC curve are very impressive for this multi-classification scenario. However, the sampling method only can be applied on training dataset, that's very important.\n4. https:\/\/www.kaggle.com\/raiaman15\/comprehensive-analysis-and-prediction#Exploratory-Data-Analysis; Very comprehensive, even analyze the description. However, the sampling method only can be applied on training dataset, that's very important.\n5. https:\/\/www.kaggle.com\/parhamzm\/us-accidents-analysis; Impressive idea on categorize the Street type into Highway and others.\n6. https:\/\/www.kaggle.com\/kalilurrahman\/us-accidents-db-eda-using-python-data-viz-tools; Really good visualization, especially plot the position of each accidents with Severity on a real map. ","efcb5e8b":"<a id=\"2.1\"><\/a>\n## 2.1. Drop irrelevant columns","fe8f677f":"It doesn't show that the wind direction have a significant influence on severity.","7a302a58":"<a id=\"5.1.1.8\"><\/a>\n#### 5.1.1.8. Model Comparison","d30218db":"<a id=\"5.1.2.8\"><\/a>\n#### 5.1.2.8. Model Comparison","52d5f7fe":"<a id=\"3.1.1\"><\/a>\n### 3.1.1. What is the distribution of accidents severity?","2798387d":"<a id=\"3.2.2\"><\/a>\n### 3.2.2. What are the top 10 counties with the most accidents?","112d5547":"## Acknowledgements\n* Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. \u201cA Countrywide Traffic Accident Dataset.\u201d, 2019.\n* Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.","8a6ac5df":"<a id=\"2.2\"><\/a>\n## 2.2. Drop the column with Missing Value(>40%)","c2789872":"<a id=\"5.1.1.7\"><\/a>\n#### 5.1.1.7. Multi-layer Perceptron Regression","c72bf5af":"<a id=\"5.1\"><\/a>\n## 5.1. Workflow Demonstration","b6c401c8":"There are weak relationship between:\n* **Pressure** and **Temperature**\n* **Wind_Speed** and **Humidity**\n* **Visibility** and **Humidity**","fe07d682":"<a id=\"1.1\"><\/a>\n## 1.1. Load Data","ee3f2167":"# US Traffic Accidents Analysis and Prediction\nThis notebook aims to analyze the US traffic accidents and build models on accidents severity.   \n* [**Author**](https:\/\/www.linkedin.com\/in\/chi-wang-22a337207\/)\n* [**Dataset**](https:\/\/www.kaggle.com\/sobhanmoosavi\/us-accidents)"}}