{"cell_type":{"7913b03d":"code","c318ff44":"code","0ec3d823":"code","31e9bf5d":"code","96e80169":"code","3ea2025c":"code","4add2b1c":"code","5eb0396e":"code","d93b488a":"code","9c9d730d":"code","6b67970a":"code","3aec441a":"code","6f019397":"code","0f90ed9a":"code","1e53548a":"code","c45c3568":"code","09bec58c":"code","6c3ccb2c":"code","521622be":"code","7e4832ca":"code","f7159d50":"code","cf47f0ba":"code","2e70f2ba":"code","ed4233d3":"code","bad78a0c":"code","12255371":"code","99469fd0":"code","04041ce3":"code","ae5d9085":"code","7fe4ce8e":"code","7742def6":"code","b0fa9bb7":"code","585ce394":"code","1bfe0591":"code","82ab242f":"code","ba1c470a":"code","18bbf295":"code","7d773bab":"code","57c3a4cb":"code","251a97fd":"code","054ba837":"code","8d347fa6":"code","2acde6d6":"code","6d5a32f5":"code","54391285":"code","3690a07b":"code","68be0b5d":"code","2215d824":"code","eacfa8e9":"code","37ee66e0":"code","a8342684":"code","3f2b9a79":"code","9e1742f8":"code","4cf92d2e":"code","87e12195":"code","377f40cc":"code","faeca70d":"code","89d72a06":"code","0e64943c":"code","0902086d":"code","1faff177":"code","82b1ffec":"code","8d1c870b":"code","31525334":"code","5e869c18":"code","bca0da3c":"code","0e73fec3":"code","071b2280":"code","bb9981cb":"code","ba673e25":"code","600f2629":"code","345b3c3e":"code","b879037b":"code","34002fd1":"code","18605b0d":"code","17333fd7":"code","22b5e623":"code","fa68f284":"code","6abce819":"code","fada70a3":"code","4fd0e688":"code","66139a67":"code","916e9fd9":"markdown","73c564bf":"markdown","f4a6b66a":"markdown","239d6df8":"markdown","ca471dce":"markdown","52905ddf":"markdown","dda50a92":"markdown","8c3920d6":"markdown","9fc38ae4":"markdown"},"source":{"7913b03d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization, Input\nfrom keras.models import Sequential, save_model\nfrom keras.utils import np_utils\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.decomposition import PCA\nfrom sklearn.utils import class_weight\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.metrics import log_loss","c318ff44":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","0ec3d823":"moa_train_feat = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\nmoa_train_targ_NS = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\nmoa_train_targ_S = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nmoa_test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')","31e9bf5d":"moa_train_feat","96e80169":"####Drop ID's\nmoa_train_targ_S=moa_train_targ_S.drop(moa_train_targ_S.columns[0],axis=1)\nmoa_train_targ_S.head()","3ea2025c":"####Drop ID's\nmoa_train_feat =moa_train_feat.drop(moa_train_feat.columns[0],axis=1)\nmoa_train_feat.head() ","4add2b1c":"moa_train_targ_S.dtypes","5eb0396e":"####Drop ID's but Save Test ID's\ntest_id=moa_test['sig_id']\nmoa_test=moa_test.drop(moa_test.columns[0],axis=1)\nmoa_test.head()","d93b488a":"test_id","9c9d730d":"moa_train_feat=moa_train_feat[moa_train_feat['cp_type'] != 'ctl_vehicle']\n","6b67970a":"indexs_list2=moa_train_feat.index.values.tolist() ","3aec441a":"moa_train_targ_S=moa_train_targ_S.iloc[indexs_list2]\n","6f019397":"#df = moa_train_targ_S.sum(axis=1)","0f90ed9a":"#df_no_label=(df==0)\n#df_no_label","1e53548a":"####Make List of Indexs\n#moa_train_targ_S[df_no_label]\n#indexs_list2=moa_train_targ_S[df_no_label].index.values.tolist() ","c45c3568":"#moa_train_targ_S = moa_train_targ_S.drop(indexs_list2) ","09bec58c":"#moa_train_feat = moa_train_feat.drop(indexs_list2) ","6c3ccb2c":"#moa_train_targ_S","521622be":"#moa_train_feat","7e4832ca":"####One Hot Code Train Columns: cp_type and cp_dose\ndummies=moa_train_feat[['cp_type','cp_dose']]\ncat_columns = ['cp_type','cp_dose']","f7159d50":"dummies2=pd.get_dummies(dummies, prefix_sep=\"_\",\n                              columns=cat_columns)\ndummies2","cf47f0ba":"moa_train_feat['cp_type']=dummies2['cp_type_trt_cp']\nmoa_train_feat['cp_dose']=dummies2['cp_dose_D1']","2e70f2ba":"###Remove Low Variance Features\nprint(moa_train_feat.shape)\nfrom sklearn import feature_selection as fs\n## Define the variance threhold and fit the threshold to the feature array. \nsel = fs.VarianceThreshold(threshold=.7)\nmoa_train_feat_vt = sel.fit_transform(moa_train_feat)\n\n## Print the support and shape for the transformed features\nprint(sel.get_support())\nprint(moa_train_feat.shape)\n\nmoa_train_feat=moa_train_feat[moa_train_feat.columns[sel.get_support(indices=True)]] ","ed4233d3":"one_hot_moa_train_feat=moa_train_feat.copy()","bad78a0c":"#dummies2=dummies2.multiply(dummies2['cp_type_trt_cp'], axis=0)\n#dummies2=dummies2.reset_index()","12255371":"###Remove Categorical Columns\n#moa_train_feat=moa_train_feat.drop(['cp_type','cp_dose'],axis=1)","99469fd0":"###Insert Dummies\n#dummies2=dummies2[['cp_dose_D1','cp_dose_D2']]\n#one_hot_moa_train_feat=dummies2.join(moa_train_feat)\n#one_hot_moa_train_feat","04041ce3":"####One Hot Code Columns: cp_type and cp_dose\ndummies3=moa_test[['cp_type','cp_dose']]","ae5d9085":"dummies4=pd.get_dummies(dummies3, prefix_sep=\"_\",\n                              columns=cat_columns)\ndummies4","7fe4ce8e":"moa_test['cp_type']=dummies4['cp_type_trt_cp']\nmoa_test['cp_dose']=dummies4['cp_dose_D1']\n","7742def6":"test_control_group=moa_test['cp_type'] == 0","b0fa9bb7":"#dummies4=dummies4.multiply(dummies4['cp_type_trt_cp'], axis=0)\n#dummies4","585ce394":"#moa_test=moa_test.drop(['cp_type','cp_dose','cp_time'],axis=1)\n#moa_test","1bfe0591":"top_feats3=list(sel.get_support(indices=True))","82ab242f":"sel.get_support(indices=True)","ba1c470a":"one_hot_moa_test","18bbf295":"###Remove Same Variance Threshold Columns from Test Set\n#moa_test=moa_test[moa_test.columns[sel.get_support(indices=True)]] \n#moa_test\none_hot_moa_test=moa_test.iloc[:, top_feats3]\n","7d773bab":"one_hot_moa_test","57c3a4cb":"#dummies4=dummies4[['cp_dose_D1','cp_dose_D2']]\n#one_hot_moa_test=dummies4.join(moa_test)\n#one_hot_moa_test","251a97fd":"combined_x=one_hot_moa_train_feat.copy()","054ba837":"combined_y=moa_train_targ_S.copy()","8d347fa6":"filter_col_g = [col for col in combined_x if col.startswith('g-')]\ngenes=combined_x[filter_col_g]\ngenes.head()","2acde6d6":"filter_col_c = [col for col in combined_x if col.startswith('c-')]\ncells=combined_x[filter_col_c]\ncells.head()","6d5a32f5":"filter_col_c_test = [col for col in one_hot_moa_test if col.startswith('c-')]\ncells_test=one_hot_moa_test[filter_col_c_test]\ncells_test.head()","54391285":"filter_col_g_test = [col for col in one_hot_moa_test if col.startswith('g-')]\ngenes_test=one_hot_moa_test[filter_col_g_test]\ngenes_test.head()","3690a07b":"###Add PCA Features###\npca_c = PCA(.9)\npca_g = PCA(.9)\n\n#fit PCA on Training Set\npca_c.fit(cells)\npca_g.fit(genes)\n\n### Apply PCA Mapping to Training and Test Set: Converts to a np.array\npca_cells_train = pca_c.transform(cells)\npca_genes_train = pca_g.transform(genes)\npca_cells_test = pca_c.transform(cells_test)\npca_genes_test = pca_g.transform(genes_test)\n\n#####Create Dataframe of PCA Features\nPCA_g_train=pd.DataFrame(pca_genes_train)\nPCA_c_train=pd.DataFrame(pca_cells_train)\nPCA_g_test=pd.DataFrame(pca_genes_test)\nPCA_c_test=pd.DataFrame(pca_cells_test)","68be0b5d":"PCA_g_train = PCA_g_train.reset_index()\ndel PCA_g_train['index']\n\nPCA_c_train = PCA_c_train.reset_index()\ndel PCA_c_train['index']\n\nPCA_g_test = PCA_g_test.reset_index()\ndel PCA_g_test['index']\n\nPCA_c_test = PCA_c_test.reset_index()\ndel PCA_c_test['index']\n","2215d824":"print(PCA_g_train.shape)\nprint(PCA_c_train.shape)\nprint(PCA_g_test.shape)\nprint(PCA_c_test.shape)\nprint(one_hot_moa_test.shape)\nprint(combined_x.shape)","eacfa8e9":"PCA_train=pd.merge(PCA_g_train, PCA_c_train,right_index=True, left_index=True)\nPCA_test=pd.merge(PCA_g_test, PCA_c_test,right_index=True, left_index=True)","37ee66e0":"one_hot_moa_test = one_hot_moa_test.reset_index()\ndel one_hot_moa_test['index']\n\ncombined_x = combined_x.reset_index()\ndel combined_x['index']","a8342684":"one_hot_moa_test=one_hot_moa_test.join(PCA_test)","3f2b9a79":"combined_x=combined_x.join(PCA_train)","9e1742f8":"one_hot_moa_test","4cf92d2e":"combined_x","87e12195":"#one_hot_moa_train_feat=pd.merge(PCA_g_train, PCA_c_train,right_index=True, left_index=True)\n#one_hot_moa_train_feat=pd.merge(dummies_train,one_hot_moa_train_feat,right_index=True, left_index=True)\n#one_hot_moa_test=pd.merge(PCA_g_test, PCA_c_test,right_index=True, left_index=True)\n#one_hot_moa_test=pd.merge(dummies_test,one_hot_moa_test,right_index=True, left_index=True)","377f40cc":"#top_feats2=list(np.array(top_feats))\n#top_feats3=[1]+top_feats2\n#top_feats3=top_feats3[:-1]\n#print(len(top_feats3))\n#top_feats3=top_feats\ntop_feats3=list(sel.get_support(indices=True))","faeca70d":"#combined_x=combined_x.iloc[:, top_feats3]\n#combined_x","89d72a06":"combined_y","0e64943c":"combined_x.iloc[:, 7:]","0902086d":"X=np.array(combined_x)\ninput_dim=X.shape[1]\nX.shape","1faff177":"X.shape[1]","82b1ffec":"Y=np.array(combined_y)\nnum_classes=Y.shape[1]\nY.shape","8d1c870b":"import tensorflow_addons as tfa","31525334":"def create_model(num_columns):\n    model = Sequential()\n    model.add(Input(num_columns))\n    model.add( BatchNormalization() )\n    model.add( Dropout(0.5))\n    model.add(Dense(units=800, kernel_initializer='glorot_uniform', activation='swish'))\n    model.add( BatchNormalization() )\n    model.add( Dropout(0.5))\n    model.add(Dense(units=400,activation='swish'))\n    model.add( BatchNormalization() )\n    model.add( Dropout(0.5) )\n    model.add(Dense(units=num_classes,activation='sigmoid'))\n    opt = keras.optimizers.Adam(learning_rate=3e-3)\n    model.compile( optimizer=opt, loss='binary_crossentropy')\n    return model\n    \n\n#metrics=[tf.keras.metrics.AUC(name='auc')]\n#tf.keras.metrics.AUC(name='auc')\n#tf.keras.metrics.Recall(name='recall')\n#tf.keras.metrics.Precision(name='precision')","5e869c18":"combined_y","bca0da3c":"####Get Length of Test\nl=len(one_hot_moa_test)-1\nl","0e73fec3":"##Empty Predictions Set\nss = combined_y.copy()\nss = ss.reset_index()\ndel ss['index']\nss=ss.loc[0:l,:]\nss.loc[:, combined_y.columns] = 0\nss","071b2280":"##Empty Validation Set\nres = combined_y.copy()\nres = res.reset_index ()\nres.loc[:, combined_y.columns] = 0\ndel res['index']\nres","bb9981cb":"one_hot_moa_test.values[:, top_feats3].shape","ba673e25":"combined_y.shape","600f2629":"combined_x.shape","345b3c3e":"N_STARTS = 4\nimport tensorflow as tf\ntf.random.set_seed(42)\n\n####This iterates through starts:\n\nfor seed in range(N_STARTS):\n#####This iteraties through folds n, validation indexes te, and train indexes tr:    \n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=5, random_state=seed, shuffle=True).split(combined_y, combined_y)):\n        print(f'Fold {n}')\n    \n        model = create_model(input_dim)\n        #checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n        #cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n          #                           save_weights_only = True, mode = 'min')\n        \n####This fits the model to each fold and validation set. .values avoids creating a np array:\n\n        model.fit(combined_x.values[tr],\n                  combined_y.values[tr],\n                  validation_data=(combined_x.values[te], combined_y.values[te]),\n                  epochs=28, batch_size=128,\n                  callbacks=[reduce_lr_loss], verbose=2\n                 )\n        \n        #model.load_weights(checkpoint_path)\n####Makes predictions for each fold & seed:\n        test_predict = model.predict(one_hot_moa_test.values[:, :])\n        val_predict = model.predict(combined_x.values[te])\n####Sum Predictions for Each Epoch     \n        ss.loc[:, combined_y.columns] += test_predict\n        res.loc[te, combined_y.columns] += val_predict\n        print('')\n        \n####After all summed, Divide summed predictions by the number of starts times the number of folds:     \nss.loc[:, combined_y.columns] \/= ((n+1) * N_STARTS)\nres.loc[:, combined_y.columns] \/= N_STARTS","b879037b":"####Estimate Validation Loss of Averaged Results\ndef metric(y_true, y_pred):\n    metrics = []\n    for _target in combined_y.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","34002fd1":"print(f'OOF Metric: {metric(combined_y, res)}')","18605b0d":"####Set Controls to 0\nss.loc[(test_control_group), combined_y.columns] = 0\n","17333fd7":"test_id=pd.DataFrame(test_id)\nss\ntest_id","22b5e623":"ss=pd.merge(test_id, ss, how='inner', left_index=True, right_index=True)\nss=pd.DataFrame(ss)\nss","fa68f284":"ss.dtypes","6abce819":"###Check for nulls\npd.DataFrame(ss.isnull().sum(axis = 0)).sum()","fada70a3":"df =pd.DataFrame(ss.describe()).max(axis=1)\ndf","4fd0e688":"ss.describe()","66139a67":"ss.to_csv('submission.csv', index=False)","916e9fd9":"# One Hot Code and Remove Low Variance Features","73c564bf":"**Note: Since it doesn't make sense to consider the timing or dose size of a placebo, we are going to multiply the columns by the treatment status and get rid of treatment status all together.** ","f4a6b66a":"**One Hot Code Training Set**","239d6df8":"**One Hot Code Test Set**","ca471dce":"# Remove Control Rows","52905ddf":"# PCA Each Group and Add to Dataset","dda50a92":"Special thanks to https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2","8c3920d6":"# Types of Over and Undersampling Using imblearn\n\n1) **Random Under Sampling:** We can use the function RandomUnderSampler(sampling_strategy= 'not minority') or 'majority' to adjust the relationship between the minority class and majority class. We can use dict to input a dictionary with keys corresponding to classes and values corresponding to desired # of samples for each class. Finally, the default is to sample without replacement but that can be altered by utilizing replacement=True. Note: For binary classification, you can input a float with the desired minority \/ majority ratio. \n\n\n**Here we reach a 2 to 1 balance:** \ndefine undersample strategy\n\nundersample = RandomUnderSampler(sampling_strategy='not minority')\n\nsampling_strategy = {0: 10, 1: 15, 2: 20}\n\nrus = RandomUnderSampler(sampling_strategy=sampling_strategy)\n\nfit and apply the transform\n\nX_under, y_under = undersample.fit_resample(X, y)\n\n2) **Random Over Sampling:** \n\nros = RandomOverSampler(random_state=0)\n\nros.fit(X, y)\n\nX_resampled, y_resampled = ros.sample(X, y)\n\n3) **Over Sampling Using SMOTE:** This method takes two minority class points that are close to each other and creates a synthetic point repeatedly until the desired balance is acheived. \n\n**Here we reach a 2 to 1 balance:** \noversample = SMOTE(sampling_strategy='not minority') \nX_over, y_over = oversample.fit_resample(X, y)\n\n\n4) **Nearest Neighbor Under Sampling and SMOTE Over Sampling (SMOTEENN)**: This strategy combines the above approaches to acheive the desired ratio. \n\n**Here we reach a 2 to 1 balance:** \ndefine sampling strategy\nsample = SMOTEENN(sampling_strategy='not minority')\nfit and apply the transform\nX_over, y_over = sample.fit_resample(X, y)\n\n4)Bagging: Bootstrap resampling with replacement from under represented classes. \n\n**Challenges:** \n\n1) Must reduce dimensionality to make k nearest neighbor approach useable. It has trouble with high dimensional data. Use PCA. Note: Using PCA may change the angle rendering Cos similarity unuseable. \n\n2) We ultimately should take into consideration that some observations have multiple classes associated with them. Possible Solution: Expand to a single class representing All Variations.  \n\n3) SMOTE relies on a distance measure. Distance measures do not work well at high dimensions. Vectors are not similar. \n\n4) Repeating categories appearing once increases log loss. ","9fc38ae4":"## Adjusted Neural Network Model with Weights"}}