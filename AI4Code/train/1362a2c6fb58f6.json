{"cell_type":{"eb129872":"code","f6a5071c":"code","48a3e083":"code","f035fbcd":"code","d81c89b6":"code","2be11829":"code","87af0db8":"code","87372e40":"code","ee379177":"code","f57feded":"code","22a39aeb":"code","dede7484":"code","436d6f90":"code","5b935a0e":"code","09ef7c90":"code","fe1dd6c1":"markdown","04220365":"markdown","b4f895b5":"markdown","452c7aea":"markdown","50f74112":"markdown","a17ad6fd":"markdown","6d45896d":"markdown"},"source":{"eb129872":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, r2_score\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6a5071c":"# Import data and get intuition\ndata = pd.read_csv(\"\/kaggle\/input\/insurance\/insurance.csv\")\n\nprint(data.describe(), '\\n', data.keys())","48a3e083":"# Substitue binary genders\ndata['sex'] = [1 if ele == 'female' else 0 for ele in data['sex'].values]\n# Substitute binary smoker\ndata['smoker'] = [1 if ele == 'yes' else 0 for ele in data['smoker'].values]\n","f035fbcd":"sns.heatmap(data=data.corr(), annot=True)\nplt.show()","d81c89b6":"# Split data into testing, validation and training\ntraining_data = data[len(data)\/\/3:]\ntesting_data = data[:len(data) \/\/ 3]\nvalidation_data = training_data[:len(training_data) \/\/ 2]\n\n# Seperate label and features\nx_training = training_data[['bmi', 'age', 'smoker']]\ny_training = np.array(training_data['charges']).reshape(len(training_data), 1)\nx_validation = validation_data[['bmi', 'age', 'smoker']]\ny_validation = np.array(validation_data['charges']).reshape(len(validation_data), 1)\nx_testing = testing_data[['bmi', 'age', 'smoker']]\ny_testing = np.array(testing_data['charges']).reshape(len(validation_data), 1)\n\n# Scale features\nscaler = StandardScaler()\nscaler.fit(x_training, y_training)\nx_training = scaler.transform(x_training)\nscaler.fit(x_validation, y_validation)\nx_validation = scaler.transform(x_validation)\nscaler.fit(x_testing, y_testing)\nx_testing = scaler.transform(x_testing)","2be11829":"# Perform Gradient Descent \nm = x_training.shape[0]\n# Add bias term to x\nx_training = np.c_[np.ones(shape=(x_training.shape[0], 1)), x_training]\ntheta = np.random.randn(x_training.shape[1], 1)\niterations = 10000\nlearning_rate = 0.001\ncost_history = np.zeros(shape=(iterations, 1))\n\nfor i in range(iterations):\n        cost_history[i] = 1 \/ (2 * m) * np.sum((x_training.dot(theta) - y_training) ** 2)\n        gradient = 1\/m * (x_training.T.dot(np.dot(x_training, theta) - y_training))\n        theta -= learning_rate*gradient","87af0db8":"# Plot the cost function over the epochs\nplt.plot(range(len(cost_history)), cost_history)\nplt.show()","87372e40":"# Add bias node\nx_validation = np.c_[np.ones(shape=(x_validation.shape[0], 1)), x_validation]\nx_testing = np.c_[np.ones(shape=(x_testing.shape[0], 1)), x_testing]\n\ny_pred_validation = x_validation.dot(theta)\ny_pred_training = x_training.dot(theta)\ny_pred_testing = x_testing.dot(theta)","ee379177":"print(f'MAE Training: {mean_absolute_error(y_pred=y_pred_training, y_true=y_training)}')\nprint(f'MAE Validation: {mean_absolute_error(y_pred=y_pred_validation, y_true=y_validation)}')\nprint(f'MAE Testing: {mean_absolute_error(y_pred=y_pred_testing, y_true=y_testing)}')","f57feded":"print(f'Correlation Coefficient for the testing data: {r2_score(y_testing, y_pred_testing)}')","22a39aeb":"# Split data into testing, validation and training\ntraining_data = data[len(data)\/\/3:]\ntesting_data = data[:len(data) \/\/ 3]\nvalidation_data = training_data[:len(training_data) \/\/ 2]\n\n# Seperate label and features\nx_training = training_data[['bmi', 'age', 'smoker']].values\ny_training = np.array(training_data['charges']).reshape(len(training_data), 1)\nx_validation = validation_data[['bmi', 'age', 'smoker']].values\ny_validation = np.array(validation_data['charges']).reshape(len(validation_data), 1)\nx_testing = testing_data[['bmi', 'age', 'smoker']].values\ny_testing = np.array(testing_data['charges']).reshape(len(validation_data), 1)\n\n# Add powers of the most correlated feature\n# bmi = x_training[:, 0]**2\nage = x_training[:, 1] **2\nx_training = np.c_[x_training, age]\n\n# bmi = x_validation[:, 0]**2\nage = x_validation[:, 1] **2\nx_validation = np.c_[x_validation, age]\n\n# bmi = x_testing[:, 0]**2\nage = x_testing[:, 1] **2\nx_testing = np.c_[x_testing, age]\n\n# Scale features\nscaler = StandardScaler()\nscaler.fit(x_training, y_training)\nx_training = scaler.transform(x_training)\nscaler.fit(x_validation, y_validation)\nx_validation = scaler.transform(x_validation)\nscaler.fit(x_testing, y_testing)\nx_testing = scaler.transform(x_testing)\nprint(x_testing)","dede7484":"# Perform Gradient Descent \nm = x_training.shape[0]\n# Add bias term to x\nx_training = np.c_[np.ones(shape=(x_training.shape[0], 1)), x_training]\ntheta = np.random.randn(x_training.shape[1], 1)\niterations = 10000\nlearning_rate = 0.001\ncost_history = np.zeros(shape=(iterations, 1))\nfor i in range(iterations):\n        cost_history[i] = 1 \/ (2 * m) * np.sum((x_training.dot(theta) - y_training) ** 2)\n        gradient = 1\/m * (x_training.T.dot(np.dot(x_training, theta) - y_training))\n        theta -= learning_rate*gradient","436d6f90":"# Plot the cost function over the epochs\nplt.plot(range(len(cost_history)), cost_history)\nplt.show()","5b935a0e":"# Add bias node\nx_validation = np.c_[np.ones(shape=(x_validation.shape[0], 1)), x_validation]\nx_testing = np.c_[np.ones(shape=(x_testing.shape[0], 1)), x_testing]\n\ny_pred_validation = x_validation.dot(theta)\ny_pred_training = x_training.dot(theta)\ny_pred_testing = x_testing.dot(theta)\n\n# Print Metrics\nprint(f'MAE Training: {mean_absolute_error(y_pred=y_pred_training, y_true=y_training)}')\nprint(f'MAE Validation: {mean_absolute_error(y_pred=y_pred_validation, y_true=y_validation)}')\nprint(f'MAE Testing: {mean_absolute_error(y_pred=y_pred_testing, y_true=y_testing)}')","09ef7c90":"print(f'Correlation Coefficient for the testing data: {r2_score(y_testing, y_pred_testing)}')","fe1dd6c1":"The variables with pseudo strong correlations are age, bmi, and smoker with a large value. Now we split data to train algorithm. ","04220365":"Alright! We were able to increase the score by 0.21.","b4f895b5":"Not too shabby but we can do better. Why not try to add powers of each feature as a new feature? \nFirst let's split the data again to have \"new\" data not normalized.","452c7aea":"Now we explore the data.","50f74112":"Now we can predict values with the given parameters.","a17ad6fd":"Not bad, the avg charge is 13270. The model isn't overfitting the data, and if it were due the size of the data set and the number of features using l1 or l2 norm would not make much difference. To be sure, let's calculate the correlation coefficient for the test data (R^2). ","6d45896d":"> Out of the six variables given to predict the target data, charges, two of them are categorical. Either a binary sex or four regions. \nLet's pseudo-encode them."}}