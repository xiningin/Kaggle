{"cell_type":{"94b27f94":"code","1ae53d67":"code","5f716c86":"code","a84cc72f":"code","98bc49f7":"code","1ad08d1a":"code","fed57a3c":"code","5b880d41":"code","746eb285":"code","28c175bf":"code","344e3ef2":"code","5f9a1979":"code","2225aced":"code","30f1680d":"code","5b903038":"code","f27f0fd0":"code","9d5a3781":"code","66645e84":"code","f46176e7":"code","7b1319ad":"code","d45d0057":"code","7f95fdc8":"code","8b7bfa69":"code","efe38fcd":"code","305899d0":"code","e313f787":"code","d3974e98":"code","ded74422":"code","0d161999":"code","1556f0a6":"code","fd1af667":"code","b134568e":"code","e037d3e4":"code","c086d158":"code","2ca0ffb6":"code","fc872c2c":"code","725a12b5":"markdown","ac699354":"markdown","43d0878c":"markdown","32e961cb":"markdown","11001a25":"markdown","ee4b913a":"markdown","612dbe0c":"markdown"},"source":{"94b27f94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%pip install autoviz\nfrom autoviz.AutoViz_Class import AutoViz_Class\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nimport time\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom category_encoders import OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom scipy import stats\nimport os\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, make_scorer\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ae53d67":"dataset = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndataset.head()","5f716c86":"dataset.info()","a84cc72f":"dataset.isna().sum()","98bc49f7":"#Data Analysis\nimport pandas_profiling\nreport = pandas_profiling.ProfileReport(dataset)\ndisplay(report)","1ad08d1a":"import warnings\nwarnings.filterwarnings(\"ignore\")\nAV=AutoViz_Class()\nreport2=AV.AutoViz(\"\/kaggle\/input\/titanic\/train.csv\")","fed57a3c":"%pip install sweetviz\nimport sweetviz as sv\nadvert_report = sv.analyze(dataset)\nadvert_report.show_html('Advertising.html')","5b880d41":"dataset2=dataset.drop(columns=['Name','Cabin'])","746eb285":"\ndataset2['Age'].fillna(dataset2['Age'].mean(), inplace=True)\n\npermutation = np.random.permutation(dataset2['Embarked'])\nempty_is = np.where(permutation == \"\")\npermutation = np.delete(permutation, empty_is)\nend = len(permutation)\ndataset2['Embarked'] = dataset2['Embarked'].apply(lambda x: permutation[np.random.randint(end)] if pd.isnull(x) else x)","28c175bf":"dataset2.isna().sum()","344e3ef2":"dataset2.describe()","5f9a1979":"dataset2.info()","2225aced":"dataset2[\"Family_Count\"]=dataset2.SibSp+dataset2.Parch\ndataset2 = dataset2.drop(columns=['SibSp','Parch'])","30f1680d":"#Buidling dashboard\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_style(\"dark\",{\"axes.facecolor\":\"black\"})\n\nf,axes = plt.subplots(3,2,figsize=(15,15))\nk1=sns.violinplot(data=dataset2,x=\"Sex\",y=\"Pclass\",ax=axes[0,0])\nk2=sns.violinplot(data=dataset2,x=\"Embarked\",y=\"Pclass\",ax=axes[0,1])\nk3=sns.violinplot(data=dataset2,x=\"Pclass\",y=\"Survived\",ax=axes[1,0])\n#k4=sns.violinplot(data=dataset,x=\"Pclass\",y=\"Age\",ax=axes[1,1],palatte=\"YlorRd\")\naxes[1,1].hist(dataset2.Age,bins=15)\nk4=sns.violinplot(data=dataset2,x=\"Pclass\",y=\"Age\",ax=axes[2,0])\nk5=sns.violinplot(data=dataset2,x=\"Family_Count\",y=\"Pclass\",ax=axes[2,1])\n#k1.set(xlim=(0,85))\nplt.show()","5b903038":"#One Hot encoding\ndataset2 = pd.get_dummies(dataset2,columns=['Pclass','Sex','Embarked'])","f27f0fd0":"dataset2.columns.values","9d5a3781":"dataset2.info()","66645e84":"\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.title(\"Correlation of data with Survived\",fontsize=35,color='DarkBlue',fontname='DejaVu Sans')\ndataset2.corrwith(dataset2.Survived).plot.bar(figsize=(15,10),fontsize=20,rot=75,grid=True)\nplt.show()","f46176e7":"#Heatmap: data or feature correlation with each other\nsns.set(style='white')\n\n#Compute the correlational matrix\ncorr = dataset2.corr()\n\n#Generate mask for the upper triange\nmask=np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)]=True\n\n#generate custom diverging colormap\ncmap=sns.diverging_palette(220,10,as_cmap=True)\n\n# Setup the matplotlib figure\nf, ax = plt.subplots(figsize=(18,15))\n\nsns.heatmap(corr,cmap=cmap,mask=mask,square=True,center=0,linewidths=0.5,fmt='g',vmax=0.3,cbar_kws={'shrink':0.5})\nplt.title(\"Correlation of data with each other\",fontsize=35,color='DarkBlue',fontname='DejaVu Sans')\nplt.show()","7b1319ad":"# Removing extra columns and target\n#dataset[\"Family_Count\"]=dataset.SibSp+dataset.Parch\ntarget = dataset['Survived']\nPassenger = dataset['PassengerId']\ndataset= dataset.drop(columns=['Survived','PassengerId','Ticket','Name','Cabin'])","d45d0057":"dataset.head()","7f95fdc8":"def defineBestModelPipeline(df, target, categorical_columns, numeric_columns):\n    # Splitting into Train and Test Set\n    x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.10, random_state=42)\n\n# 1st -> Numeric Transformers\n    numeric_transformer_1 = Pipeline(steps=[('imp', IterativeImputer(max_iter=30, random_state=42)),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_2 = Pipeline(steps=[('imp', IterativeImputer(max_iter=20, random_state=42)),\n                                            ('scaler', StandardScaler())])\n    \n    numeric_transformer_3 = Pipeline(steps=[('imp', SimpleImputer(strategy='mean')),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_4 = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n                                            ('scaler', StandardScaler())])\n\n# 2nd -> Categorical Transformer\n    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n    \n\n# 3rd -> Combining both numerical and categorical pipelines\n    data_transformations_1 = ColumnTransformer(transformers=[('num', numeric_transformer_1, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_2 = ColumnTransformer(transformers=[('num', numeric_transformer_2, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_3 = ColumnTransformer(transformers=[('num', numeric_transformer_3, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_4 = ColumnTransformer(transformers=[('num', numeric_transformer_4, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    # And finally, we are going to apply these different data transformations to RandomSearchCV,\n# trying to find the best imputing strategy, the best feature engineering strategy\n    # and the best model with it's respective parameters.\n    # Below, we just need to initialize a Pipeline object with any transformations we want, on each of the steps.\n    pipe = Pipeline(steps=[('data_transformations', data_transformations_1),('feature_eng', PCA()),('clf', SVC())])\n\n    params_grid = [\n        {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [KNeighborsClassifier()],\n                     'clf__n_neighbors': stats.randint(1, 50),\n                     'clf__metric': ['minkowski', 'euclidean']},\n\n    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [LogisticRegression()],\n                     'clf__penalty': ['l1', 'l2'],\n                     'clf__C': stats.uniform(0.01, 10)},\n\n\n    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [SVC()],\n                     'clf__C': stats.uniform(0.01, 1),\n                     'clf__gamma': stats.uniform(0.01, 1),\n                     'clf__kernel':['linear','rbf']},\n\n    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [RandomForestClassifier()],\n                     'clf__n_estimators': stats.randint(10, 175),\n                     'clf__max_features': [None, \"auto\", \"log2\"],\n                     'clf__max_depth': [None, stats.randint(1, 5)],\n                     'clf__random_state': stats.randint(1, 49)},\n\n    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [GradientBoostingClassifier()],\n                     'clf__n_estimators': stats.randint(10, 100),\n                     'clf__learning_rate': stats.uniform(0.01, 0.7),\n                     'clf__max_depth': [None, stats.randint(1, 6)]}\n\n    ]\n# Now, we fit a RandomSearchCV to search over the grid of parameters defined above\n    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    \n    best_model_pipeline = RandomizedSearchCV(pipe, params_grid, n_iter=500, \n                                             scoring=metrics, refit='accuracy', \n                                             n_jobs=-1, cv=5, random_state=21)\n\n    best_model_pipeline.fit(x_train, y_train)\n\n    print(\"\\n\\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[0])\n    print(\"\\n\\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[1])\n    print(\"\\n\\n#---------------- Best Classifier found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[2])\n    print(\"\\n\\n#---------------- Best Estimator's average Accuracy Score on CV (validation set) ----------------#\\n\\n\", best_model_pipeline.best_score_)\n    \n    return x_train, x_test, y_train, y_test, best_model_pipeline","8b7bfa69":"categorical_columns=['Pclass','Sex','Embarked']\nnumeric_columns=['Age', 'Fare', 'SibSp', 'Parch']\n# Calling the function above, returing train\/test data and best model's pipeline\nx_train, x_test, y_train, y_test, best_model_pipeline = defineBestModelPipeline(dataset, target, categorical_columns, numeric_columns)","efe38fcd":"# Function responsible for checking our model's performance on the test data\ndef testSetResultsClassifier(classifier, x_test, y_test):\n    predictions = classifier.predict(x_test)\n    \n    results = []\n    f1 = f1_score(y_test, predictions)\n    precision = precision_score(y_test, predictions)\n    recall = recall_score(y_test, predictions)\n    accuracy = accuracy_score(y_test, predictions)\n    \n    results.append(f1)\n    results.append(precision)\n    results.append(recall)\n    results.append(accuracy)\n    \n    print(\"\\n\\n#---------------- Test set results (Best Classifier) ----------------#\\n\")\n    print(\"F1 score, Precision, Recall, Accuracy:\")\n    print(results)\n    \n    return results","305899d0":"# Checking best model's performance on test data\ntest_set_results = testSetResultsClassifier(best_model_pipeline, x_test, y_test)\n","e313f787":"# Visualizing all results and metrics, from all models, obtained by the RandomSearchCV steps\ndf_results = pd.DataFrame(best_model_pipeline.cv_results_)\n\ndisplay(df_results)","d3974e98":"# Based on above results , we received best accuracy from LogisticRegression (C=3.148239614927082) model. Lets use this model and check it on Test data\n# and get predictions on test data\nimport os\nvalidation_set = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nvalidation_set.head()","ded74422":"validation_set.isna().sum()","0d161999":"# Removing extra columns\nPassenger_Test = validation_set['PassengerId']\nvalidation_set[\"Family_Count\"]=validation_set.SibSp+validation_set.Parch\nvalidation_set = validation_set.drop(columns=['PassengerId','Ticket','Name','Cabin'])","1556f0a6":"validation_set.head()","fd1af667":"# Applying best_model_pipeline\n# Step 1 -> Transforming data the same way we did in the training set;\n# Step 2 -> making predictions using the best model obtained by RandomSearchCV.\ntest_predictions = best_model_pipeline.predict(validation_set)\nprint(test_predictions)","b134568e":"test_predictions=pd.DataFrame({\"Survived\":test_predictions})\ntest_predictions","e037d3e4":"Final_Result = pd.concat([Passenger_Test,test_predictions],axis=1).dropna()\nFinal_Result = Final_Result.sort_values(by='PassengerId', ascending=True)\nFinal_Result","c086d158":"Final_Result.drop(Final_Result.columns.difference(['PassengerId', 'Survived']), axis=1, inplace=True) # Selecting only needed columns\nFinal_Result.head(10)","2ca0ffb6":"Final_Result.to_csv(\"Predictions for Titanic Project with Pipeline_Survival.csv\",index=False)","fc872c2c":"Final_Result.count()","725a12b5":"Using the power of both automatic EDA libraries listed above, we can observe each variable's behaviour individually, with plots that goes from Histograms to Boxplots, Correlation Matrix and much more. It speeds up time and minimizes the effort spent on the initial process of our work.\n\nWe can gather some really useful information from both reports. Let's now point some of them out:\n\n    Our classes are not that much disbalanced. We have ~38% of the passengers into class \"1\" (survived) and ~62% of the passengers into class \"0\" (didn't survive).\n\n    The \"Pclass\" column, that informs us about the passenger's ticket class, shows us that ~55% of them are on class 3, ~24% of them are on class 2 and ~21% on class 1.\n\n    Most of the passengers into this dataset are male: ~35% of the passengers are female, and ~65% are male.\n\n    Almost 20% of the values in the \"Age\" column are missing. We can fill out these nulls with various techniques, such as filling them with the distribution's mean\/ median. The ages distribution is a little bit skewed, with it's mean being around 30 years old, and it's standard deviation being close to 15. The oldest passenger we have in this dataset is 80 years old.\n\n    According to the \"SibSP\" column, most of the passengers (~68%) didn't have any spouses or siblings aboard the ship. That is also applied when we check out the \"Parch\" column.\n\n    The distribution of Fares is much more skewed. It's mean value is around 32, with it's standard deviation being close to 50. It's minimum value is 0, and it's maximum value is 512.3292. That means that we're going to have to deal with this column carefully if we plan to use models such as SVMs.\n\n    When ckecking the \"Embarked\" column, it shows us that 72.3% of the passengers embarked at Southampton port, 18.9% of the passengers at Cherbourg port and 8.6% of the passengers at Queenstown port.\n\n    \"Fare\" values are higher for passengers with \"Pclass\" = 1, lower for passengers with \"Pclass\" = 2 and even lower for passengers with \"Pclass\" = 3. Logically, it looks like the classification of \"Pclass\" is defined by the value of the passenger's fare.\n\n","ac699354":"**Validation\/ Test data set**","43d0878c":"**Business Problem:** To predict the Titanic passenger survival.\n\n**Data Dictionary:** Variable Definition Key PassengerId Passenger Id of the Passenger Name Name of the Passenger survival Survival 0 = No, 1 = Yes (Binary)--> This is Target variable. pclass Ticket class 1 = Upper, 2 = Middle, 3 = Lower sex Sex female\/male (categorical field) Age Age in years\nsibsp # of siblings \/ spouses aboard the Titanic\nparch # of parents \/ children aboard the Titanic\nticket Ticket number\nfare Passenger fare\ncabin Cabin number\nembarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton (categorical field)\n\nThis Problem prediction comes under \"Classification\" problem hence needs to explore more on classification models.\n","32e961cb":"Above correlation shows that \"Survived\" is positevely related with\n\n    around 55% + correlation with females.\n    around 27% + correlation with Pclass1.\n    around 25% + correlation with Fare.\n    around 18% + correlation with Embarked_C.\n\nAbove correlation shows that \"Survived\" is negatively related with\n\n    around 54% - correlation with males.\n    around 31% - correlation with Pclass3.\n    around 17% - correlation with Embarked_S.\n    Less the age more chances of Survival.\n\nSummary:\n\n    As Passenger class \"Pclass\" increases, chances of survival increases. Upper class Passengers are having more survival probabilities.\n    Female Passengers are having more chances of survival.\n    Less the age of Passenger, more chances of Survival.\n    Passengers embarked from Port C = Cherbourg are having more probability of survival and Passengers embarked from Port S = Southampton are having less probability of survival.\n\n","11001a25":"**#Visualization**","ee4b913a":"**#Exploratory data Analysis**","612dbe0c":"#For age columns we will use 'missig at random'[MAR] and we will try to impute it with average age of the dataset.\n# For Embarked, only 2 values are missing. There are 2 options\n#1. Embarked missing values can be filled by randomly selecting its categorical value OR\n#2. By complete case Analysis, we can delete these 2 rows \n# I have selected 1 st option without deleting entire row.\n\n#Pipeline Modeling\n#-> Step 1: fill null values from numerical columns.\n#-> Step 2: normalize numerical features, so they will be in the same scale.\n#-> Step 3: fill null values from categorical features.\n#-> Step 4: OneHotEncode categorical features.\n#-> Step 5: fit a Machine Learning model and evaluate it."}}