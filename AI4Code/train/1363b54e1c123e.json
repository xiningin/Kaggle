{"cell_type":{"eb2e473d":"code","9b07896a":"code","13c56319":"code","583f57a8":"code","b3f32ce0":"code","8839e636":"code","a6c1ee26":"code","8abeddff":"code","8bd48c30":"code","bc93a914":"code","390b1fad":"code","acea9380":"code","448cf76d":"code","66ce2ce1":"code","d5333fec":"code","603d71a7":"code","62c8e941":"code","5c7762ea":"code","10b17d9f":"code","a1622fb6":"code","e9c59472":"code","f6c60422":"code","32b86ed6":"code","f08e34d8":"markdown","f30a6238":"markdown","56302eff":"markdown","6ebc959f":"markdown","40e2f019":"markdown","90887063":"markdown","0da876e5":"markdown","bb94d2b5":"markdown","dbba0f0c":"markdown","15241d89":"markdown","69818bf9":"markdown","37fdf4e1":"markdown","11effa87":"markdown"},"source":{"eb2e473d":"# importing common libraries...\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","9b07896a":"data_train = pd.read_csv(\"..\/input\/train.csv\")  # loading the data","13c56319":"data_train.info()  # checking for data types","583f57a8":"print(list(data_train.any().isnull()))   # there is no null value in our columns, which is great","b3f32ce0":"data_train.describe()   # I see that in most of the cases values are distributed between -1.00 and 1.00  ...","8839e636":"data_train.head(20)","a6c1ee26":"data_train.drop([\"rn\"],axis=1,inplace=True)    ## removing the rn column.","8abeddff":"data_train.head(10)  ## As seen, problem solved!! :)","8bd48c30":"# importing data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","bc93a914":"plt.figure(figsize=(30,20))\nsns.heatmap(data=(data_train.corr()*data_train.corr()),cmap=\"BuPu\",vmin=0.4)\nplt.show()\n\n# what I do here is: only showing correlation x on => ((x^2)>0.4).  By this we only see highly correlated columns. and seems like there are many of them.","390b1fad":"# Let's see our labels.\nlabels = list(data_train.activity.unique())\nprint(labels) ","acea9380":"# they are strings, we should make them numerical values.  for this i will use scikit-learn\n\n# importing and setting\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n# fitting each possible label\nle.fit(labels)\n# updating our data with LabelEncoder\ndata_train.activity = le.transform(data_train.activity)","448cf76d":"y_train = data_train.activity.values.reshape(-1,1)  # scikit doesn't likes when it is like (n,). it rathers (n,m).. that's why I used reshape","66ce2ce1":"x_train = data_train.values ","d5333fec":"from sklearn.model_selection import train_test_split \nx_tr,x_tst,y_tr,y_tst = train_test_split(x_train,y_train,test_size=0.2,random_state=42)","603d71a7":"from sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier(n_neighbors=1,algorithm=\"auto\")\n\nknn_model.fit(x_tr,y_tr.ravel())\n\ny_head = knn_model.predict(x_tst)","62c8e941":"knn_model.score(x_tst,y_tst)","5c7762ea":"n = range(1,30)\nresults = []\nfor i in n:\n    #print(i)\n    knn_tester = KNeighborsClassifier(n_neighbors=i)\n    knn_tester.fit(x_tr,y_tr.ravel())\n    results.append(knn_tester.score(x_tst,y_tst))","10b17d9f":"plt.clf()\n\nplt.suptitle(\"SCORES\",fontsize=18)\n\nplt.figure(figsize=(20,10))\nplt.plot(n,results,c=\"red\",linewidth=4)\nplt.xlabel(\"n neighbors\")\nplt.ylabel(\"score\")\nplt.show()","a1622fb6":"from sklearn.metrics import confusion_matrix","e9c59472":"conf = confusion_matrix(y_pred=y_head,y_true=y_tst)\nconf = conf.astype('float') \/ conf.sum(axis=1)[:, np.newaxis]","f6c60422":"plt.figure(figsize=(20,20))\nsns.heatmap(conf,annot=True,cmap=\"summer\")\nplt.show()","32b86ed6":"## I am currently improving my skills on data science. If you have any advice or comment, make sure you show it.  Best Regards.","f08e34d8":"## EDA","f30a6238":"## Visual EDA","56302eff":"\n# In Overview of this dataset, I realize that column \"rn\" is pretty much an id number of activity. But this is ruining the dataset !!! because labels grouped in order of \"rn\" column and this would effect our models very dramatically. (Which is basically cheating)\n\nTo be fair, I would rather making a model for real life scenarios without correlated ID numbers.  So, I will drop that \"rn\" column.","6ebc959f":"## Conclusion\n\nTo sum up, I see that in some datasets, it is not very good to increase the amount of n_neighbors","40e2f019":"### PreProcessing the data","90887063":"### Before training my completed model, I want to check it's accuracy\nfor this I will use train_test_split on my training data and then I will be able to compare","0da876e5":"# Hello and welcome to my notebook\n## you will find:\n        * EDA\n        * Visual EDA\n        * K-Nearest-Neighbors approach to the data\n        * Comparison and Conclusion","bb94d2b5":"## As you can see above, model reached 97.6% of success.\nwhich is acceptable for me but we should tune and see the best amount of n_neighbors for our data","dbba0f0c":"As we see above, our data has too many columns and they are sensor results. So they are not practically the best data to viusalize.","15241d89":"### Here is the Confusion Matrix","69818bf9":"## K Nearest Neighbors approach","37fdf4e1":"now I can create my initial model to estimate my final accuracy.","11effa87":"# Comparison\nfo this my range will be 1 to 10 neighbors."}}