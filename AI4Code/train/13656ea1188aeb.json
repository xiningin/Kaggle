{"cell_type":{"667d261c":"code","29ac0290":"code","931853b3":"code","6407c939":"code","d3b88733":"code","b2ff9f7f":"code","e3794b5f":"code","5bd20ce5":"code","43b31b3a":"code","0848b2ac":"code","7142e3b7":"code","68e36608":"code","92238df8":"code","f63f565a":"code","9290045c":"code","f4f72280":"code","2124da40":"code","93440431":"code","a2058196":"code","b4f86ab6":"markdown","e99a7b02":"markdown","cfabe8c6":"markdown","33a9b16f":"markdown","9a5b56a1":"markdown","b20eac3f":"markdown","fe543086":"markdown","bf696c1b":"markdown"},"source":{"667d261c":"import numpy as np # linear algebra\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import statements required for Plotly \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, log_loss, classification_report)\n#from imblearn.over_sampling import SMOTE\nimport xgboost\n\n# Import and suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","29ac0290":"data=pd.read_csv(\"\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\ndata.head()","931853b3":"!pip install openpyxl","6407c939":"data.to_excel('hr.xlsx')","d3b88733":"df =pd.read_excel(\"hr.xlsx\", sheet_name=0)\ndf = df.drop(['Unnamed: 0'], axis=1)\ndf.head()","b2ff9f7f":"df.columns","e3794b5f":"display(df.isnull().any())","5bd20ce5":"target_map = {'Yes':1, 'No':0}\n# Use the pandas apply method to numerically encode our attrition target variable\ndf[\"Attrition_numerical\"] = df[\"Attrition\"].apply(lambda x: target_map[x])","43b31b3a":"# creating a list of only numerical values\nnumerical = [u'Age', u'DailyRate', u'DistanceFromHome', \n             u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction',\n             u'HourlyRate', u'JobInvolvement', u'JobLevel', u'JobSatisfaction',\n             u'MonthlyIncome', u'MonthlyRate', u'NumCompaniesWorked',\n             u'PercentSalaryHike', u'PerformanceRating', u'RelationshipSatisfaction',\n             u'StockOptionLevel', u'TotalWorkingYears',\n             u'TrainingTimesLastYear', u'WorkLifeBalance', u'YearsAtCompany',\n             u'YearsInCurrentRole', u'YearsSinceLastPromotion',u'YearsWithCurrManager']\ndata = [\n    go.Heatmap(\n        z=df[numerical].astype(float).corr().values, # Generating the Pearson correlation\n        x=df[numerical].columns.values,\n        y=df[numerical].columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n#         text = True ,\n        opacity = 1.0\n        \n    )\n]\n\n\nlayout = go.Layout(\n    title='Pearson Correlation of numerical features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700,\n    \n)\n\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')","0848b2ac":"# Refining our list of numerical variables\nnumerical = [u'Age', u'DailyRate',  u'JobSatisfaction',\n       u'MonthlyIncome', u'PerformanceRating',\n        u'WorkLifeBalance', u'YearsAtCompany', u'Attrition_numerical']\n\n#g = sns.pairplot(attrition[numerical], hue='Attrition_numerical', palette='seismic', diag_kind = 'kde',diag_kws=dict(shade=True))\n#g.set(xticklabels=[])","7142e3b7":"# Drop the Attrition_numerical column from attrition dataset first - Don't want to include that\ndf = df.drop(['Attrition_numerical'], axis=1)\n\n# Empty list to store columns with categorical data\ncategorical = []\nfor col, value in df.iteritems():\n    if value.dtype == 'object':\n        categorical.append(col)\n\n# Store the numerical columns in a list numerical\nnumerical = df.columns.difference(categorical)","68e36608":"# Store the categorical data in a dataframe called attrition_cat\ndf_cat = df[categorical]\ndf_cat = df_cat.drop(['Attrition'], axis=1) # Dropping the target column","92238df8":"df_cat = pd.get_dummies(df_cat)\ndf_cat.head(3)","f63f565a":"# Store the numerical features to a dataframe attrition_num\ndf_num = df[numerical]","9290045c":"# Concat the two dataframes together columnwise\ndf_final = pd.concat([df_num, df_cat], axis=1)","f4f72280":"# Define a dictionary for the target mapping\ntarget_map = {'Yes':1, 'No':0}\n# Use the pandas apply method to numerically encode our attrition target variable\ntarget = df[\"Attrition\"].apply(lambda x: target_map[x])\ntarget.head(3)","2124da40":"data = [go.Bar(\n            x=df[\"Attrition\"].value_counts().index.values,\n            y= df[\"Attrition\"].value_counts().values\n    )]\n\npy.iplot(data, filename='basic-bar')","93440431":"# Import the train_test_split method\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Split data into train and test sets as well as for validation and testing\ntrain, test, target_train, target_val = train_test_split(df_final, \n                                                         target, \n                                                         train_size= 0.80,\n                                                         random_state=0);\n#train, test, target_train, target_val = StratifiedShuffleSplit(attrition_final, target, random_state=0);","a2058196":"from sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\n\ndecision_tree = tree.DecisionTreeClassifier(max_depth = 4)\ndecision_tree.fit(train, target_train)\n\n# Predicting results for test dataset\ny_pred = decision_tree.predict(test)\n\n# Export our trained model as a .dot file\nwith open(\"tree1.dot\", 'w') as f:\n     f = tree.export_graphviz(decision_tree,\n                              out_file=f,\n                              max_depth = 4,\n                              impurity = False,\n                              feature_names = df_final.columns.values,\n                              class_names = ['No', 'Yes'],\n                              rounded = True,\n                              filled= True )\n        \n#Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n\n# Annotating chart with PIL\nimg = Image.open(\"tree1.png\")\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage(\"sample-out.png\", height=2000, width=1900)","b4f86ab6":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5499C7;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\n\n\nLoading Libraries\n<\/h1>\n<\/div>\n\n\n","e99a7b02":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5499C7;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\n\n\n\n\n\nTree Based Approach\n<\/h1>\n<\/div>\n\n","cfabe8c6":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5499C7;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\n\n\n\nData load\n<\/h1>\n<\/div>\n\n\n","33a9b16f":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5499C7;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\n\n\n\n\nDefine a dictionary for the target mapping\n<\/h1>\n<\/div>\n\n\n","9a5b56a1":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5499C7;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\n\n\n\n\nFeature Engineering & Categorical Encoding\n<\/h1>\n<\/div>\n","b20eac3f":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5499C7;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\n\n\n\n\nImplementing Machine Learning Models\n<\/h1>\n<\/div>\n\n","fe543086":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5499C7;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\n\n\n\n\nCorrelation Matrix\n<\/h1>\n<\/div>","bf696c1b":"# Looking for NaN"}}