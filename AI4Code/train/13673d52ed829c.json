{"cell_type":{"ffd8a443":"code","5484d714":"code","11bc0c45":"code","4a3d81b5":"code","399cde34":"code","8f669915":"code","c4321509":"code","d13eb297":"code","095ee8f7":"code","149de45f":"code","b793b752":"code","9b34e09d":"code","03215a53":"code","51c52aa4":"code","2130497b":"code","12dc8429":"code","1c7753ba":"code","78411f14":"code","a9eb8c96":"code","b43fda6f":"code","09f29571":"code","66238fe9":"code","c8fbc285":"code","5e010a57":"code","99e4b46c":"code","3fe03fa7":"code","4af7fd35":"code","c3f8b01b":"markdown","83efba85":"markdown","455f7bda":"markdown","16474a7c":"markdown","96c1334f":"markdown","9197268b":"markdown","3d8ab64c":"markdown","ee047c8d":"markdown","d96c46c8":"markdown"},"source":{"ffd8a443":"import sys\nsys.path.append(\"..\/input\/timm-pytorch-image-models\/pytorch-image-models-master\")","5484d714":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport glob\nimport gc\nfrom tqdm.auto import tqdm\nimport os\nimport sys\nimport time\nimport random\nimport math\nimport re\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nimport timm\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport cupy\nimport cuml\nimport cudf\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n# from cuml.decomposition import PCA, TruncatedSVD\nfrom cuml.manifold import UMAP\n\nfrom transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel, DistilBertTokenizer, DistilBertModel\nimport transformers\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom gensim.corpora import Dictionary\nfrom gensim.models import Word2Vec\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","11bc0c45":"class CFG:\n    debug = False\n    check_ram = False\n    calc_cv = True\n    \n    # phash\n    n_components = 32\n    \n    # tfidf\n    max_features = 25000\n    tfidf_thresh = 0.75\n    \n    # wrod2vec\n    epochs = 100\n    vector_size = 512  # embedding size\n    window = 5\n    \n    \n    # image models\n    \n    image_model_name1 = \"eca_nfnet_l0\"\n    size1 = 512\n    image_pretrained_path1 = \"..\/input\/shopee-pretrained\/nfnet_5\/arcface_512x512_nfnet_l0(mish).pt\"\n    image_loss_module1 = \"curricularface\"\n#     pca_components = 128\n    \n    \n    image_model_name2 = \"swin_small_patch4_window7_224\"\n    size2 = 224\n    image_pretrained_path2 = \"..\/input\/shopee-pretrained\/swin1\/curricularface_224x224_vit_base_patch16_224.pt\"\n    image_loss_module2 = \"curricularface\"\n  \n    \n    image_model_name3 = \"vit_base_patch16_224\"\n    size3 = 224\n    image_pretrained_path3 = \"..\/input\/shopee-pretrained\/vit1\/curricularface_224x224_vit_base_patch16_224.pt\"\n    image_loss_module3 = \"curricularface\"\n    \n    \n    # bert models\n    transformer_model1 = '..\/input\/sentence-transformer-models\/paraphrase-xlm-r-multilingual-v1\/0_Transformer'\n    max_length1 = 64\n    bert_pretrained_path1 = \"..\/input\/shopee-pretrained\/arcface_sbert3\/sentence_transfomer_xlm_best_loss_num_epochs_30_arcface.pth\"\n    bert_loss_module1 = \"arcface\"\n    \n    transformer_model2 = \"..\/input\/distilbert-base-indonesian\"\n    max_length2 = 32\n    bert_pretrained_path2 = \"..\/input\/shopee-pretrained\/distilbert2\/distilbert_curricularface_30_.pth\"\n    bert_loss_module2 = \"curricularface\"\n    \n    # others\n    n_neighbors = 50\n    \n    thresh = 0.36\n    \n    \n    classes = 11014\n    scale = 30\n    margin = 0.5\n    fc_dim = 512\n    ","4a3d81b5":"PATH = \"..\/input\/shopee-product-matching\/\"\n\ntrain_df = pd.read_csv(PATH + \"train.csv\")\n# train_df = pd.read_csv(\"..\/input\/shopee-cv-splitting-way\/train_folds.csv\")\n# train_df = pd.read_csv(\"..\/input\/shopee-cv-folds\/train_label_group_5folds.csv\")\ntest_df = pd.read_csv(PATH + \"test.csv\")\nsample_submission = pd.read_csv(PATH + \"sample_submission.csv\")","399cde34":"train_df[\"image\"] = train_df[\"image\"].apply(lambda x: PATH + \"train_images\/\" + x)\n# if not CFG.check_ram:\ntest_df[\"image\"] = test_df[\"image\"].apply(lambda x: PATH + \"test_images\/\" + x)\n\ntmp = train_df.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain_df['target'] = train_df.label_group.map(tmp)\n\nif len(test_df) != 3:\n    CFG.calc_cv = False\n\nif CFG.check_ram and (len(test_df)==3):\n    test_df = pd.concat([train_df, train_df], axis=0).reset_index()\n    \nif CFG.calc_cv:\n    test_df = train_df","8f669915":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(42)","c4321509":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target, row[col]) )\n        fp = len(np.setdiff1d(row[col], row.target))\n        fn = len(np.setdiff1d(row.target, row[col]))\n#         return fn\n        return 2 * n \/ (2 * n + fp + fn)\n#         return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score\n\ndef get_score(df, col):\n    return df.apply(getMetric(col),axis=1).mean()","d13eb297":"def predict(features, df, thresh=0.90, chunk=1024):\n    features = cupy.asarray(features)\n    pred = []\n    CHUNK = chunk\n    CTS = len(features) \/\/ CHUNK\n    if len(features)%CHUNK!=0: CTS += 1\n    for j in range(CTS):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(features))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n#         cts = np.matmul(phash_vector, phash_vector[a:b].T).T\n        cts = cupy.matmul(features, features[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>thresh)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            pred.append(o)\n        del cts\n    return pred\n\ndef get_neighbor_images(image_embeddings, df, chunk=4096, thresh=0.5, n_neighbors=50, show=False, metric=\"euclidean\"):\n    preds = []\n    CHUNK = chunk\n    model = NearestNeighbors(n_neighbors=n_neighbors, metric=metric)\n    model.fit(image_embeddings)\n\n    print('Finding similar images...')\n    CTS = len(image_embeddings)\/\/CHUNK\n    if len(image_embeddings)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(image_embeddings))\n        print('chunk',a,'to',b)\n        distances, indices = model.kneighbors(image_embeddings[a:b,])\n        if (j == 0) and show:\n            show_barplot(distances, indices, df)\n\n        for k in range(b-a):\n            IDX = np.where(distances[k,]<thresh)[0]\n            IDS = indices[k,IDX]\n            o = df.iloc[IDS].posting_id.values\n            preds.append(o)\n            \n    return preds\n\ndef displayDF(train, random=False, COLS=6, ROWS=4, path=\"\"):\n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            if random: row = np.random.randint(0,len(train))\n            else: row = COLS*k + j\n            name = train[\"image\"].values[row]\n            title = train[\"title\"].values[row]\n            title_with_return = \"\"\n            for i,ch in enumerate(title):\n                title_with_return += ch\n                if (i!=0)&(i%20==0): title_with_return += '\\n'\n            img = cv2.imread(path+name)[:, :, ::-1]\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()\n        \ndef show_barplot(distances, indices,  df, num_show=8):\n    for k in range(num_show):\n        plt.figure(figsize=(20,3))\n        plt.plot(np.arange(50), cupy.asnumpy(distances[k,]),'o-')\n#         plt.plot(np.arange(50), distances[k,],'o-')\n        plt.title('Text Distance From Train Row %i to Other Train Rows'%k,size=16)\n        plt.ylabel('Distance to Train Row %i'%k,size=14)\n        plt.xlabel('Index Sorted by Distance to Train Row %i'%k,size=14)\n        plt.show()\n        \n        cluster = df.loc[cupy.asnumpy(indices[k,:8])]\n#         cluster = df.loc[indices[k,:8]]\n        displayDF(cluster, random=False, ROWS=2, COLS=4)\n        print( df.loc[cupy.asnumpy(indices[k,:8]), ['title','label_group']] )\n#         print( df.loc[indices[k,:10], ['title','label_group']] )","095ee8f7":"def combine_for_sub(row, cols, count=1):\n    x = np.concatenate([row[col] for col in cols])\n    unique, counts = np.unique(x, return_counts=True)\n    return ' '.join(unique[counts>=count])\n\ndef combine_for_cv(row, cols, count=1):\n    x = np.concatenate([row[col] for col in cols])\n    unique, counts = np.unique(x, return_counts=True)\n    return unique[counts>=count]","149de45f":"def remove_emoji(title):\n    \"\"\"\n    title: str\n    example:\n    >>>title = '\\\\xe2\\\\x9d\\\\xa4 RATU \\\\xe2\\\\x9d\\\\xa4 MAYCREATE MOISTURIZING SPRAY'\n    >>>removed = remove_emoji(title)\n    >>>removed\n    ' RATU  MAYCREATE MOISTURIZING SPRAY'\n    \"\"\"\n    matches_spans = [m.span() for m in re.finditer(r\"\\\\x[0-9a-fA-F][0-9a-fA-F]\", title)]\n    spans = []\n    if len(matches_spans) > 0:\n        for i, (span1, span2) in enumerate(zip(matches_spans[:-1], matches_spans[1:])):\n            if i == 0:\n                spans.append([span1[0], -1])\n            if span1[1] != span2[0]:\n                spans[-1][1] = span1[1]\n                spans.append([span2[0], -1])\n        spans[-1][1] = matches_spans[-1][1]\n        emojis = []\n        for span in spans:\n            emojis.append(title[span[0]:span[1]])\n\n        for emj in emojis:\n            title = title.replace(emj, \"\")\n        return title\n    else:\n        return title\n    \ndef lower_title(title):\n    return title.lower()\n    \ndef unify_units(title):\n    title += \" \" # insert space in order to recognize the last units\n    \n    # gram, kg, mg\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(gram|grm|gr\\s|g\\s)\", r\"\\1gram \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)kg\", r\"\\1kg \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)mg\", r\"\\1mg \", title)\n    \n    # L, ml\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(liter|l\\s)\", r\"\\1liter \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)ml\", r\"\\1ml \", title)\n    \n    # m, cm, km, mm\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(meter|m\\s)\", r\"\\1meter \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)cm\", r\"\\1cm \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)km\", r\"\\1km \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)mm\", r\"\\1mm \", title)\n    \n    # pcs, pieces\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(pcs|pieces)\", r\"\\1pcs \", title)\n    \n    # watt\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)watt\", r\"\\1watt \", title)\n    \n    # A mA mAh\n#     title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(a)\", r\"\\1amper\", title)\n#     title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(ma)\", r\"\\1ma\", title)\n#     title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(mah)\", r\"\\1mha\", title)\n    \n    # GB MB TB\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)gb\", r\"\\1gb \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)mb\", r\"\\1mb \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)tb\", r\"\\1tb \", title)\n    \n    # ply\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)ply\", r\"\\1ply \", title)\n    \n    # inch\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)inch\", r\"\\1inch \", title)\n    \n    return title\n\ndef remove_letters(title):\n    title = re.sub('[!\"#$\\'\\\\\\\\()*:;<=>?@[\\\\]^_`{|}~\u300c\u300d\u3014\u3015\u201c\u201d\u3008\u3009\u300e\u300f\u3010\u3011\uff06\uff0a\u30fb\uff08\uff09\uff04\uff03\uff20\u3002\u3001\uff1f\uff01\uff40\uff0b\uffe5\uff05]', \" \", title)\n    \n    # \/ and -\n    title = re.sub(r\"([a-z0-9])(-|\/)([a-z])\", \"\\1 \\3\", title)\n    title = re.sub(r\"([a-z])(-|\/)([a-z0-9])\", \"\\1 \\3\", title)\n    \n    title = re.sub(r\"\\s(-|\/|\\+)\\s\", \" \", title)\n    \n    \n    # remove sequential space\n    title = title.strip()\n    title = re.sub(r\"\\s{2,}\", \" \", title)\n    \n    \n    # -\n#     title = re.sub(r\"([a-z0-9])\/([a-z])\", \"\\1 \\2\", title)\n#     title = re.sub(r\"([a-z])\/([a-z0-9])\", \"\\1 \\2\", title)\n    \n#     code_regex = re.compile('[!\"#$%&\\'\\\\\\\\()*+,-\/:;<=>?@[\\\\]^_`{|}~\u300c\u300d\u3014\u3015\u201c\u201d\u3008\u3009\u300e\u300f\u3010\u3011\uff06\uff0a\u30fb\uff08\uff09\uff04\uff03\uff20\u3002\u3001\uff1f\uff01\uff40\uff0b\uffe5\uff05]')\n#     # .  \u306f\u524a\u9664\u3057\u3061\u3083\u3044\u304b\u3093\n#     title = code_regex.sub(' ', title)\n#     title = title.strip()\n    return title\n\ndef convert_comma(title):\n    title = re.sub(r\"([0-9]),([0-9])\", r\"\\1.\\2\", title)\n    return title\n    \ndef text_preprocessing(text):\n    text = remove_emoji(text)\n    text = text.lower()\n    text = convert_comma(text)\n    text = unify_units(text)\n#     text = remove_letters(text)\n    \n\n    return text","b793b752":"def predict_tfidf(df, title_col, max_features=25000, thresh=0.5, norm=False):\n    df = df.loc[:, [\"posting_id\", title_col]]\n    gf = cudf.DataFrame(df)\n    model = TfidfVectorizer(stop_words=\"english\", binary=True, max_features=25000, dtype=np.float32)\n    text_embeddings = model.fit_transform(gf[title_col]).toarray()\n    print(text_embeddings.shape)\n#     text_embeddings = cupy.asnumpy(text_embeddings)\n    del model\n    gc.collect()\n    if norm:\n        text_embeddings = cupy.asnumpy(text_embeddings)\n        text_embeddings = text_embeddings.astype(np.float16)\n        text_embeddings = normalize(text_embeddings)\n        text_embeddings = cupy.asarray(text_embeddings)\n    print(\"start predicting\")\n    preds = predict(text_embeddings, df, thresh=thresh)\n    del text_embeddings\n    return preds","9b34e09d":"def get_word2vec_embeddings(titles, vector_size=50, window=5, min_count=1, workers=4, epochs=1500, negative=10, sg=1):\n    titles_tokens = [title.split() for title in titles]\n    dictionary = Dictionary(titles_tokens)\n    bow_corpus = [dictionary.doc2bow(token) for token in titles_tokens]\n    print(\"start word2vec training\")\n    t0 = time.time()\n    model = Word2Vec(titles_tokens, vector_size=vector_size, window=window, min_count=min_count, workers=workers, epochs=epochs, negative=negative, sg=sg)\n    print(f\"end word2vec training {time.time()-t0:.1f}s\")\n    embeddings = []\n    for sentence in titles:\n        title = sentence.split()\n        vectors = [model.wv[w] for w in title]\n        embeddings.append(np.mean(vectors, axis=0))\n    model.save(\"word2vec_pretrained.bin\")\n    return np.array(embeddings)","03215a53":"class ShopeeTextDataset(Dataset):\n    def __init__(self, title, label, tokenizer=None, max_length=64):\n        self.title = title\n        self.label = label\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.title)\n    def __getitem__(self, idx):\n        title = self.title[idx]\n        label = self.label[idx]\n        \n        title = self.tokenizer(title, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n#         print(title)\n        input_ids = title['input_ids'][0]\n        attention_mask = title['attention_mask'][0]\n        \n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label}","51c52aa4":"class ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='bert-base-uncased',\n                 pooling='mean_pooling',\n                 use_fc=True,\n                 fc_dim=512,\n                 dropout=0.0,\n                 loss_module='softmax',\n                 s=30.0,\n                 margin=0.50,\n                 ls_eps=0.0,\n                 theta_zero=0.785,\n                 distil=False):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n        if distil:\n            self.transformer = transformers.DistilBertModel.from_pretrained(model_name)\n        else:\n            self.transformer = transformers.AutoModel.from_pretrained(model_name)\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.pooling = pooling\n        self.use_fc = use_fc\n    \n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self.relu = nn.ReLU()\n            self._init_params()\n            final_in_features = fc_dim\n\n        self.loss_module = loss_module\n        if loss_module == 'arcface':\n            self.final = ArcMarginProduct(final_in_features, n_classes,\n                                          scale=s, margin=margin, easy_margin=False, ls_eps=ls_eps)\n        elif loss_module == \"curricularface\":\n            self.final = CurricularFace(final_in_features, n_classes, \n                                           s=s, m=margin)\n        else:\n            self.final = nn.Linear(final_in_features, n_classes)\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, input_ids,attention_mask, label=None):\n        feature = self.extract_feat(input_ids,attention_mask)\n        if label is None:\n            return feature\n        if self.loss_module == 'arcface':\n            logits, loss = self.final(feature, label)\n            return feature, logits, loss\n        else:\n            logits = self.final(feature)\n        return feature, logits\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        \n        features = x[0]\n        features = features[:,0,:]\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n            features = self.relu(features)\n\n        return features","2130497b":"def get_bert_embeddings(title, transformer_model, pretrained_path, max_length=64, loss_module=\"arcface\"):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if \"distilbert\" in transformer_model:\n        tokenizer =  transformers.DistilBertTokenizer.from_pretrained(transformer_model)\n        distil = True\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n        distil = False\n    \n    ds = ShopeeTextDataset(title, np.zeros_like(title), tokenizer, max_length=max_length)\n    data_loader = DataLoader(ds, batch_size=32, shuffle=False, pin_memory=False)\n    \n#     model = ShopeeNLPModel(model_name=\"bert_base_uncased\", out_dims=CFG.bert_dims)\n    \n    model = ShopeeNet(n_classes=CFG.classes, model_name=transformer_model, pooling=\"clf\",\n                     use_fc=True, fc_dim=CFG.fc_dim, loss_module=loss_module, distil=distil)\n    model.load_state_dict(torch.load(\n        pretrained_path, map_location=\"cpu\"))\n    model.to(device)\n    model.eval()\n    feats = []\n    with torch.no_grad():\n        for data in tqdm(data_loader):\n            txt = data[\"input_ids\"].to(device)\n            mask = data[\"attention_mask\"].to(device)\n            feat = model(txt, mask)\n            feats.append(feat.detach().cpu().numpy())\n    del model\n    gc.collect()\n    feats = np.concatenate(feats)\n    return feats","12dc8429":"# phash","1c7753ba":"def phash2bin(phash):\n    return format(int(phash, 16), \"64b\")\n\ndef vectorize_bin(bins):\n    \"\"\"\n    bins: np.array \n    vectorize_bin(train_df[\"bin\"].values)\n    \"\"\"\n    def vectorize_row(row):\n        return [int(r) for r in list(row)]\n    list_bin = [vectorize_row(r) for r in bins]\n    return np.array(list_bin)\n\ndef get_phash_embeddings(df, n_components=32):\n    print(\"getting phash embeddings\")\n    df[\"phash_bin\"] = df[\"image_phash\"].apply(phash2bin)\n    features = vectorize_bin(df[\"phash_bin\"].values)\n    print(features.shape)\n    pca = PCA(n_components=n_components)\n    embeddings = pca.fit_transform(features)\n    return embeddings","78411f14":"class ShopeeImageDataset(Dataset):\n    def __init__(self, x, transforms=None, cc_transforms=None):\n        \"\"\"\n        x: np.array, \n        \"\"\"\n        self.x = x\n        self.transforms = transforms\n        self.cc_transforms = cc_transforms\n    def __len__(self):\n        return len(self.x)\n    def __getitem__(self, idx):\n        image = cv2.imread(self.x[idx])[:, :, ::-1]\n        if self.transforms:\n            image_ = self.transforms(image=image)[\"image\"]\n            if self.cc_transforms:\n                cropped = self.cc_transforms(image=image)[\"image\"]\n            else:\n                cropped = image_\n        return {\"image\": image_, \"cropped\": cropped}","a9eb8c96":"def get_transforms(size, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    return A.Compose([\n        A.Resize(size, size),\n#         FaceHiding(p=1.0),\n#         FaceMosaic(p=1.0),\n        A.Normalize(mean, std),\n        ToTensorV2(),\n    ])\ndef get_cc_transforms(size, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], resized_size=600):\n    return A.Compose([\n        A.Resize(resized_size, resized_size),\n        A.CenterCrop(size, size),\n#         FaceHiding(p=1.0),\n#         FaceMosaic(p=1.0),\n        A.Normalize(mean, std),\n        ToTensorV2(),\n    ])","b43fda6f":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - cosine*cosine)\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n    \n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n        return output, nn.CrossEntropyLoss()(output,label)\n\ndef l2_norm(input, axis = 1):\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n\n    return output\n\nclass CurricularFace(nn.Module):\n    def __init__(self, in_features, out_features, s = 30, m = 0.50):\n        super(CurricularFace, self).__init__()\n\n        print('Using Curricular Face')\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.m = m\n        self.s = s\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.threshold = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n        self.kernel = nn.Parameter(torch.Tensor(in_features, out_features))\n        self.register_buffer('t', torch.zeros(1))\n        nn.init.normal_(self.kernel, std=0.01)\n\n    def forward(self, embbedings, label):\n        embbedings = l2_norm(embbedings, axis = 1)\n        kernel_norm = l2_norm(self.kernel, axis = 0)\n        cos_theta = torch.mm(embbedings, kernel_norm)\n        cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\n        with torch.no_grad():\n            origin_cos = cos_theta.clone()\n        target_logit = cos_theta[torch.arange(0, embbedings.size(0)), label].view(-1, 1)\n\n        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m #cos(target+margin)\n        mask = cos_theta > cos_theta_m\n        final_target_logit = torch.where(target_logit > self.threshold, cos_theta_m, target_logit - self.mm)\n\n        hard_example = cos_theta[mask]\n        with torch.no_grad():\n            self.t = target_logit.mean() * 0.01 + (1 - 0.01) * self.t\n        cos_theta[mask] = hard_example * (self.t + hard_example)\n        cos_theta.scatter_(1, label.view(-1, 1).long(), final_target_logit)\n        output = cos_theta * self.s\n        return output, nn.CrossEntropyLoss()(output,label)","09f29571":"class ShopeeModel4(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = \"eca_nfnet_l0\",\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = True,\n        loss_module=\"arcface\"):\n\n\n        super(ShopeeModel4,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif 'efficientnet' in model_name:\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif 'nfnet' in model_name:\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n        \n        elif (\"swin\" in model_name) or (\"vit\" in model_name):\n            final_in_features = self.backbone.head.in_features\n            self.backbone.head = nn.Identity()\n        \n        if (\"swin\" in model_name) or (\"vit\" in model_name):\n            self.pooling = nn.Identity()\n        else:\n            self.pooling =  nn.AdaptiveAvgPool2d(1)\n#         self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.0)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n            \n        if loss_module == \"curricularface\":\n            self.final = CurricularFace(final_in_features, \n                                               n_classes, \n                                               s=scale, \n                                               m=margin)\n        elif loss_module == \"arcface\":\n            self.final = ArcMarginProduct(final_in_features,\n                                            n_classes,\n                                            scale = scale,\n                                            margin = margin,\n                                            easy_margin = False,\n                                            ls_eps = 0.0)\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label=None):\n        feature = self.extract_feat(image)\n        if self.training:\n            logits = self.final(feature, label)\n            return logits\n        else:\n            return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x","66238fe9":"class Mish_func(torch.autograd.Function):\n\n    \"\"\"from: https:\/\/github.com\/tyunist\/memory_efficient_mish_swish\/blob\/master\/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        v = 1. + i.exp()\n        h = v.log()\n        grad_gh = 1.\/h.cosh().pow_(2)\n        grad_hx = i.sigmoid()\n        grad_gx = grad_gh *  grad_hx\n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx\n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","c8fbc285":"class ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = \"tf_efficientnet_b7_ns\",\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = True):\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label=None):\n        features = self.extract_features(image)\n        if self.training:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n\n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x","5e010a57":"def get_image_embeddings_from_model(x, model, transforms=None, batch_size=16, tta=False, cc_transforms=None):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    dataset = ShopeeImageDataset(x, transforms, cc_transforms)\n    dataloader = DataLoader(dataset, shuffle=False, batch_size=batch_size,\n                            pin_memory=False, num_workers=2)\n    \n    model.eval()\n    model.to(device)\n    preds = []\n    with torch.no_grad():\n        for data in tqdm(dataloader):\n            image = data[\"image\"].to(device)\n#             image = image.to(device)\n            if tta:\n                cropped = data[\"cropped\"].to(device)\n                bs, _, size, _ = image.shape\n                image = torch.stack([image, cropped,\n#                                      image.flip(-1), cropped.flip(-1), image.flip(-2), cropped.flip(-2),\n#                                      image.flip(-1).flip(-2), cropped.flip(-1).flip(-2)\n                                    ], 0)\n                image = image.view(-1, 3, size, size)\n            pred = model(image)\n            if tta:\n                pred = pred.view(2, bs, -1).mean(0)\n            preds.append(pred.detach().cpu().numpy())\n            del pred\n            \n    preds = np.concatenate(preds, axis=0)\n    return preds","99e4b46c":"def get_image_embeddings(images, model_name, pretrained_path, size, replace_mish=False, tta=True, loss_module=\"arcface\"):\n\n    if model_name == \"tf_efficientnet_b7_ns\":\n        model = ShopeeModel(pretrained=False, model_name=model_name)\n    else:\n        model = ShopeeModel4(pretrained=False, model_name=model_name, loss_module=loss_module)\n    if replace_mish:\n        existing_layer = torch.nn.SiLU\n        new_layer = Mish()\n        model = replace_activations(model, existing_layer, new_layer)\n        print(\"replaced mish\")\n        \n    model.load_state_dict(\n        torch.load(pretrained_path, map_location=\"cpu\")\n    )\n    model.eval()\n    \n    if \"vit\" in model_name:\n        mean = [0.5, 0.5, 0.5]\n        std = [0.5, 0.5, 0.5]\n    else:\n        mean=[0.485, 0.456, 0.406]\n        std=[0.229, 0.224, 0.225]\n     \n    transforms = get_transforms(size, mean=mean, std=std)\n    if tta:\n        cc_transforms = get_cc_transforms(size, mean=mean, std=mean, resized_size=int(size*1.2))\n    else:\n        cc_transforms = None\n    image_embedding = get_image_embeddings_from_model(images, model, transforms=transforms, batch_size=32, tta=True, cc_transforms=cc_transforms)\n    return image_embedding\n","3fe03fa7":"if __name__ == \"__main__\":\n    print(\"start\")\n    \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    if len(test_df) == 3:\n        CFG.n_neighbors = 2\n        CFG.n_components = 1\n        CFG.pca_components = 1\n        \n    # ----- text preprocessing ----------\n    test_df[\"preprocessed_title\"] = test_df[\"title\"].apply(text_preprocessing)\n    titles = test_df[\"preprocessed_title\"].values\n    \n    # ------- phash ----------\n    phash_embeddings = get_phash_embeddings(test_df, n_components=CFG.n_components)\n    \n    \n    # ------ TFIDF -----------\n    tfidf_pred = predict_tfidf(test_df, \"preprocessed_title\", thresh=CFG.tfidf_thresh, max_features=CFG.max_features)\n    test_df[\"tfidf_pred\"] = tfidf_pred\n    \n    if CFG.calc_cv:\n        score = get_score(test_df, \"tfidf_pred\")\n        print(f\"CV for tfidf is {score}\")\n        \n    del tfidf_pred\n    gc.collect()\n    \n    \n    # --------- word2vec -----------\n\n    word2vec_embeddings = get_word2vec_embeddings(titles, vector_size=CFG.vector_size, window=CFG.window, epochs=CFG.epochs)\n    \n        \n    # ------- Image -------------\n    images = test_df[\"image\"].values\n    image_embeddings1 = get_image_embeddings(images, CFG.image_model_name1, CFG.image_pretrained_path1,\n                                 CFG.size1, replace_mish=True, tta=True, loss_module=CFG.image_loss_module1)\n    \n    image_embeddings2 = get_image_embeddings(images, CFG.image_model_name2, CFG.image_pretrained_path2,\n                                 CFG.size2, replace_mish=False, tta=True, loss_module=CFG.image_loss_module2)\n    \n    image_embeddings3 = get_image_embeddings(images, CFG.image_model_name3, CFG.image_pretrained_path3,\n                                 CFG.size3, replace_mish=False, tta=True, loss_module=CFG.image_loss_module3)\n    \n\n#     image_embeddings_pca = PCA(n_components=CFG.pca_components).fit_transform(image_embeddings1)\n    \n    \n    # ------- bert ---------------\n    bert_embeddings1 = get_bert_embeddings(titles, CFG.transformer_model1, CFG.bert_pretrained_path1, max_length=CFG.max_length1, loss_module=CFG.bert_loss_module1)\n    bert_embeddings2 = get_bert_embeddings(titles, CFG.transformer_model2, CFG.bert_pretrained_path2, max_length=CFG.max_length2, loss_module=CFG.bert_loss_module2)\n\n    \n    # ---------- concat --------\n    embeddings = np.concatenate([image_embeddings1,\n                                 bert_embeddings1,\n                                 phash_embeddings,\n                                 word2vec_embeddings,\n#                                  image_embeddings_pca,\n                                 image_embeddings2,\n                                 bert_embeddings2,\n                                 image_embeddings3,\n                                ], axis=1)\n    print(\"embedding shape \",embeddings.shape)\n    pred = get_neighbor_images(embeddings, test_df, thresh=CFG.thresh, n_neighbors=CFG.n_neighbors, metric=\"cosine\")\n    test_df[\"pred\"] = pred\n    \n    del embeddings, image_embeddings1, bert_embeddings1\n    gc.collect()\n    \n    if CFG.calc_cv:\n        score = get_score(test_df, \"pred\")\n        print(\"Score is \", score)\n        \n    # ---------- combine ------------\n    pred_cols = [\"tfidf_pred\", \"pred\"]\n    if CFG.calc_cv:\n        test_df[\"matches\"] = test_df.apply(lambda x: combine_for_cv(x, pred_cols, count=1), axis=1)\n        score = get_score(test_df, \"matches\")\n        print(\"cv score: \", score)\n        test_df.to_csv(\"oof.csv\", index=False)\n    test_df[\"matches\"] = test_df.apply(lambda x: combine_for_sub(x, pred_cols, count=1), axis=1)\n    \n    test_df[['posting_id','matches']].to_csv('submission.csv',index=False)","4af7fd35":"pd.read_csv(\"submission.csv\").head()","c3f8b01b":"# Image Enbeddings","83efba85":"# bert ","455f7bda":"# Arcmargin product, CurricularFace","16474a7c":"# Image model","96c1334f":"# main","9197268b":"# word2vec","3d8ab64c":"# TFIDF","ee047c8d":"# text preprocessing","d96c46c8":"# Image Dataset"}}