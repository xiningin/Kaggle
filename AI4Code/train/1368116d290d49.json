{"cell_type":{"c72b5a76":"code","2c303064":"code","dc456a12":"code","1b7d2416":"code","e5e48508":"code","349a9416":"code","9e19d13e":"code","4d58b8e1":"code","a2b7f456":"code","33b1ac30":"code","19d8a524":"code","24ee62ce":"code","ee899a8c":"code","4e6a2f27":"code","055f47b0":"code","9b369f01":"code","5560fdfd":"code","37d826b2":"code","161a6aa1":"code","66905b36":"code","b6a6dc6e":"code","cb28262b":"code","3ea91dec":"code","5bbc94a7":"code","e5a17ead":"code","fadbcbff":"code","3df418fd":"code","ef356ac8":"code","219f1151":"markdown","4eefd031":"markdown","390e6acd":"markdown"},"source":{"c72b5a76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c303064":"#importing libaries for various classification techniques\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n#from pandas.plotiing import scatter_matrix\nfrom sklearn.linear_model import LogisticRegression # logistic regression\nfrom sklearn import svm # support vector machine\nfrom sklearn.ensemble import RandomForestClassifier #Random_forest\nfrom sklearn.tree import DecisionTreeClassifier #Decision tree\nfrom sklearn.naive_bayes import GaussianNB #Naive_bayes\nfrom sklearn.neighbors import KNeighborsClassifier #K nearest neighbors\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","dc456a12":"#Reading the data set\ndf = pd.read_csv('\/kaggle\/input\/income-dataset\/income_data1.csv')\ndf.info()\ndf.head()","1b7d2416":"#Replacing the some special character columns names with proper names \n\ndf.rename(columns={'capital-gain': 'capital gain', 'capital-loss': 'capital loss', 'native-country': 'country','hours-per-week': 'hours per week','marital-status': 'marital'}, inplace=True)\ndf.columns","e5e48508":"#Finding the special characters in the data frame\ndf.isin(['?']).sum(axis=0)","349a9416":"#Replacing the special character to nan and then drop the columns\ndf['country'] = df['country'].replace('?',np.nan)\ndf['workclass'] = df['workclass'].replace('?',np.nan)\ndf['occupation'] = df['occupation'].replace('?',np.nan)\n#Dropping the NaN rows now \ndf.dropna(how='any',inplace=True)","9e19d13e":"#Assigning the numeric values to the string type variables\nnumber = LabelEncoder()\ndf['workclass'] = number.fit_transform(df['workclass'])\ndf['education'] = number.fit_transform(df['education'])\ndf['marital'] = number.fit_transform(df['marital'])\ndf['occupation'] = number.fit_transform(df['occupation'])\ndf['relationship'] = number.fit_transform(df['relationship'])\ndf['race'] = number.fit_transform(df['race'])\ndf['gender'] = number.fit_transform(df['gender'])\ndf['country'] = number.fit_transform(df['country'])\ndf['income'] = number.fit_transform(df['income'])","4d58b8e1":"df.head(5)","a2b7f456":"#Here we were grouping the each columns with prefernce to the income set\ndf.groupby('education').income.mean().plot(kind='bar')","33b1ac30":"df.groupby('workclass').income.mean().plot(kind='bar')","19d8a524":"df.groupby('gender').income.mean().plot(kind='bar')","24ee62ce":"df.groupby('race').income.mean().plot(kind='bar')","ee899a8c":"df.groupby('relationship').income.mean().plot(kind='bar')\n","4e6a2f27":"df.groupby('occupation').income.mean().plot(kind='bar')\n","055f47b0":"df.groupby('marital').income.mean().plot(kind='bar')","9b369f01":"# summarize the class distribution\ntarget = df.values[:,-1]\ncounter = Counter(target)\nfor k,v in counter.items():\n\tper = v \/ len(target) * 100\n\tprint('Class=%s, Count=%d, Percentage=%.3f%%' % (k, v, per))","5560fdfd":"#Train_Test splitting\nX = df.drop(['income'],axis=1)\ny = df['income']\nX.head()","37d826b2":"y.head()","161a6aa1":"#Declaring the train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4,random_state=0)","66905b36":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)","b6a6dc6e":"X_train.head()","cb28262b":"# Logistic Regression\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\ny_pred = LR.predict(X_test)\nscore_LR = LR.score(X_test,y_test)\nprint('The accuracy of the Logistic Regression model is', score_LR)\ntargets = ['<=50k' , '>50k']\nprint(classification_report(y_test, y_pred,target_names=targets))","3ea91dec":"# Support Vector Classifier (SVM\/SVC)\nfrom sklearn.svm import SVC\nsvc = SVC(gamma=0.22)\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nscore_svc = svc.score(X_test,y_test)\nprint('The accuracy of SVC model is', score_svc)\ntargets = ['<=50k' , '>50k']\nprint(classification_report(y_test, y_pred,target_names=targets))","5bbc94a7":"# Random Forest Classifier\nRF = RandomForestClassifier()\nRF.fit(X_train, y_train)\ny_pred = RF.predict(X_test)\nscore_RF = RF.score(X_test,y_test)\nprint('The accuracy of the Random Forest Model is', score_RF)\ntargets = ['<=50k' , '>50k']\nprint(classification_report(y_test, y_pred,target_names=targets))","e5a17ead":"# Decision Tree\nDT = DecisionTreeClassifier()\nDT.fit(X_train,y_train)\ny_pred = DT.predict(X_test)\nscore_DT = DT.score(X_test,y_test)\nprint(\"The accuracy of the Decision tree model is \",score_DT)\ntargets = ['<=50k' , '>50k']\nprint(classification_report(y_test, y_pred,target_names=targets))","fadbcbff":"# Gaussian Naive Bayes\nGNB = GaussianNB()\nGNB.fit(X_train, y_train)\ny_pred = GNB.predict(X_test)\nscore_GNB = GNB.score(X_test,y_test)\nprint('The accuracy of Gaussian Naive Bayes model is', score_GNB)\ntargets = ['<=50k' , '>50k']\nprint(classification_report(y_test, y_pred,target_names=targets))","3df418fd":"# K-Nearest Neighbors\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nscore_knn = knn.score(X_test,y_test)\nprint('The accuracy of the KNN Model is',score_knn)\ntargets = ['<=50k' , '>50k']\nprint(classification_report(y_test, y_pred,target_names=targets))","ef356ac8":"tabular_form = {'CLASSIFICATION':['LogisticRegression','SupportVectorClassifier','RandomForestClassifier','DecisionTree','GaussianNaiveBayes','K-NearestNeighbors'],\n                'ACCURACY':[score_LR,score_svc,score_RF,score_DT,score_GNB,score_knn]\n                }\ntf = pd.DataFrame(tabular_form,columns= ['CLASSIFICATION','ACCURACY'])\nprint(tf)","219f1151":"# various classification techniques","4eefd031":"# Here we were grouping the each columns with prefernce to the income set\n","390e6acd":"# Use various classification techniques and indicate the results in tabular form"}}