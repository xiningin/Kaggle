{"cell_type":{"67e6ff02":"code","ecce7c4d":"code","2879c9ff":"code","5c668e7b":"code","abc36fd8":"code","b6bb4a8f":"code","190f9ebf":"code","2917ce4c":"code","2ece607f":"code","ad2d8e36":"code","e3207052":"code","709f9caf":"code","944e052c":"code","da21ef6d":"code","8a3ba006":"code","3b71b864":"code","371567d7":"code","c2cb7082":"code","29ebeea1":"code","43797595":"code","f9e2f3cb":"markdown"},"source":{"67e6ff02":"!pip install -q imageio\n!pip install -q opencv-python\n!pip install -q git+https:\/\/github.com\/tensorflow\/docs","ecce7c4d":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow_docs.vis import embed\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom tqdm import tqdm\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.collections import LineCollection\nimport matplotlib.patches as patches\n\nimport imageio\nfrom IPython.display import HTML, display","2879c9ff":"KEYPOINT_DICT = {\n    'nose': 0,\n    'left_eye': 1,\n    'right_eye': 2,\n    'left_ear': 3,\n    'right_ear': 4,\n    'left_shoulder': 5,\n    'right_shoulder': 6,\n    'left_elbow': 7,\n    'right_elbow': 8,\n    'left_wrist': 9,\n    'right_wrist': 10,\n    'left_hip': 11,\n    'right_hip': 12,\n    'left_knee': 13,\n    'right_knee': 14,\n    'left_ankle': 15,\n    'right_ankle': 16\n}\n\nKEYPOINT_EDGE_INDS_TO_COLOR = {\n    (0, 1): 'm',\n    (0, 2): 'c',\n    (1, 3): 'm',\n    (2, 4): 'c',\n    (0, 5): 'm',\n    (0, 6): 'c',\n    (5, 7): 'm',\n    (7, 9): 'm',\n    (6, 8): 'c',\n    (8, 10): 'c',\n    (5, 6): 'y',\n    (5, 11): 'm',\n    (6, 12): 'c',\n    (11, 12): 'y',\n    (11, 13): 'm',\n    (13, 15): 'm',\n    (12, 14): 'c',\n    (14, 16): 'c'\n}","5c668e7b":"def _keypoints_and_edges_for_display(keypoints_with_scores,\n                                     height,\n                                     width,\n                                     keypoint_threshold=0.11):\n\n  keypoints_all = []\n  keypoint_edges_all = []\n  edge_colors = []\n  num_instances, _, _, _ = keypoints_with_scores.shape\n  for idx in range(num_instances):\n    kpts_x = keypoints_with_scores[0, idx, :, 1]\n    kpts_y = keypoints_with_scores[0, idx, :, 0]\n    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n    kpts_absolute_xy = np.stack(\n        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n    kpts_above_thresh_absolute = kpts_absolute_xy[\n        kpts_scores > keypoint_threshold, :]\n    keypoints_all.append(kpts_above_thresh_absolute)\n\n    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n          kpts_scores[edge_pair[1]] > keypoint_threshold):\n        x_start = kpts_absolute_xy[edge_pair[0], 0]\n        y_start = kpts_absolute_xy[edge_pair[0], 1]\n        x_end = kpts_absolute_xy[edge_pair[1], 0]\n        y_end = kpts_absolute_xy[edge_pair[1], 1]\n        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n        keypoint_edges_all.append(line_seg)\n        edge_colors.append(color)\n  if keypoints_all:\n    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n  else:\n    keypoints_xy = np.zeros((0, 17, 2))\n\n  if keypoint_edges_all:\n    edges_xy = np.stack(keypoint_edges_all, axis=0)\n  else:\n    edges_xy = np.zeros((0, 2, 2))\n  return keypoints_xy, edges_xy, edge_colors\n\n\ndef draw_prediction_on_image(\n    image, keypoints_with_scores, crop_region=None, close_figure=False,\n    output_image_height=None):\n\n  height, width, channel = image.shape\n  aspect_ratio = float(width) \/ height\n  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n  # To remove the huge white borders\n  fig.tight_layout(pad=0)\n  ax.margins(0)\n  ax.set_yticklabels([])\n  ax.set_xticklabels([])\n  plt.axis('off')\n\n  im = ax.imshow(image)\n  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n  ax.add_collection(line_segments)\n  # Turn off tick labels\n  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n\n  (keypoint_locs, keypoint_edges,\n   edge_colors) = _keypoints_and_edges_for_display(\n       keypoints_with_scores, height, width)\n\n  line_segments.set_segments(keypoint_edges)\n  line_segments.set_color(edge_colors)\n  if keypoint_edges.shape[0]:\n    line_segments.set_segments(keypoint_edges)\n    line_segments.set_color(edge_colors)\n  if keypoint_locs.shape[0]:\n    scat.set_offsets(keypoint_locs)\n\n  if crop_region is not None:\n    xmin = max(crop_region['x_min'] * width, 0.0)\n    ymin = max(crop_region['y_min'] * height, 0.0)\n    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n    rect = patches.Rectangle(\n        (xmin,ymin),rec_width,rec_height,\n        linewidth=1,edgecolor='b',facecolor='none')\n    ax.add_patch(rect)\n\n  fig.canvas.draw()\n  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n  image_from_plot = image_from_plot.reshape(\n      fig.canvas.get_width_height()[::-1] + (3,))\n  plt.close(fig)\n  if output_image_height is not None:\n    output_image_width = int(output_image_height \/ height * width)\n    image_from_plot = cv2.resize(\n        image_from_plot, dsize=(output_image_width, output_image_height),\n         interpolation=cv2.INTER_CUBIC)\n  return image_from_plot\n\ndef to_gif(images, fps):\n  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n  imageio.mimsave('.\/animation.gif', images, fps=fps)\n  return embed.embed_file('.\/animation.gif')\n\ndef progress(value, max=100):\n  return HTML(\"\"\"\n      <progress\n          value='{value}'\n          max='{max}',\n          style='width: 100%'\n      >\n          {value}\n      <\/progress>\n  \"\"\".format(value=value, max=max))","abc36fd8":"model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n\nif \"tflite\" in model_name:\n  if \"movenet_lightning_f16\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/lightning\/tflite\/float16\/4?lite-format=tflite\n    input_size = 192\n  elif \"movenet_thunder_f16\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/thunder\/tflite\/float16\/4?lite-format=tflite\n    input_size = 256\n  elif \"movenet_lightning_int8\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/lightning\/tflite\/int8\/4?lite-format=tflite\n    input_size = 192\n  elif \"movenet_thunder_int8\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/thunder\/tflite\/int8\/4?lite-format=tflite\n    input_size = 256\n  else:\n    raise ValueError(\"Unsupported model name: %s\" % model_name)\n\n  # Initialize the TFLite interpreter\n  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n  interpreter.allocate_tensors()\n\n  def movenet(input_image):\n\n    # TF Lite format expects tensor type of uint8.\n    input_image = tf.cast(input_image, dtype=tf.uint8)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n    # Invoke inference.\n    interpreter.invoke()\n    # Get the model prediction.\n    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n    return keypoints_with_scores\n\nelse:\n  if \"movenet_lightning\" in model_name:\n    module = hub.load(\"https:\/\/tfhub.dev\/google\/movenet\/singlepose\/lightning\/4\")\n    input_size = 192\n  elif \"movenet_thunder\" in model_name:\n    module = hub.load(\"https:\/\/tfhub.dev\/google\/movenet\/singlepose\/thunder\/4\")\n    input_size = 256\n  else:\n    raise ValueError(\"Unsupported model name: %s\" % model_name)\n\n  def movenet(input_image):\n\n    model = module.signatures['serving_default']\n\n    # SavedModel format expects tensor type of int32.\n    input_image = tf.cast(input_image, dtype=tf.int32)\n    # Run model inference.\n    outputs = model(input_image)\n    # Output is a [1, 1, 17, 3] tensor.\n    keypoint_with_scores = outputs['output_0'].numpy()\n    return keypoint_with_scores","b6bb4a8f":"def overlay(image):\n    input_image = tf.expand_dims(image, axis=0)\n    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n\n    # Run model inference.\n    keypoint_with_scores = movenet(input_image)\n\n    # Visualize the predictions with image.\n    display_image = tf.expand_dims(image, axis=0)\n    display_image = tf.cast(tf.image.resize_with_pad(\n        display_image, 1280, 1280), dtype=tf.int32)\n    output_overlay = draw_prediction_on_image(\n        np.squeeze(display_image.numpy(), axis=0), keypoint_with_scores)\n\n    return output_overlay","190f9ebf":"!mkdir sample1\n!mkdir sample2\n!ls","2917ce4c":"def video_2_frames(video_file='\/kaggle\/input\/drone-videos\/Berghouse Leopard Jog.mp4', image_dir='\/kaggle\/working\/sample1\/', image_file='img_%s.png'):\n    i = 0\n    cap = cv2.VideoCapture(video_file)\n    while(cap.isOpened()):\n        flag, frame = cap.read()\n        if flag == False:\n            break\n        cv2.imwrite(image_dir+image_file % str(i).zfill(6), frame) \n        i += 1\n    cap.release()","2ece607f":"video_2_frames()","ad2d8e36":"paths0=[]\nfor file in os.listdir('\/kaggle\/working\/sample1'):\n    paths0+=[os.path.join('\/kaggle\/working\/sample1',file)]\npaths0[0:5]","e3207052":"paths1=[]\nfor item in paths0:\n    if item[-4:]=='.png':\n        paths1+=[item]\npaths1[0:5]","709f9caf":"img1 = cv2.imread(paths1[0]) \nplt.figure(figsize=(10,10))\nplt.imshow(cv2.cvtColor(img1,cv2.COLOR_BGR2RGB))\n_ = plt.axis('off')","944e052c":"from tqdm import tqdm\nfor i in tqdm(range(0,len(paths1),2)):\n    image = tf.io.read_file(paths1[i])\n    image = tf.image.decode_jpeg(image)\n    image2 = overlay(image)\n    file=paths1[i].split('\/')[-1]\n    cv2.imwrite('sample2\/'+file, cv2.cvtColor(image2,cv2.COLOR_BGR2RGB))","da21ef6d":"paths1a=[]\nfor file in os.listdir('\/kaggle\/working\/sample2'):\n    paths1a+=[os.path.join('\/kaggle\/working\/sample2',file)]\npaths1a[0:5]","8a3ba006":"order=[]\nfor item in paths1a:\n    order+=[int(item[:-4].split('_')[-1])]\npaths2=pd.DataFrame(paths1a)\npaths2[1]=order\npaths2.columns=['path','int']\npaths2=paths2.sort_values('int')\npaths3=paths2['path'].tolist()\npaths3[0:5]","3b71b864":"paths4=[]\nfor i,item in enumerate(paths3):\n    if i%10==0:\n        paths4+=[item]\npaths4[0:5]","371567d7":"img1 = cv2.imread(paths4[1]) \nY1, X1, channels1 = img1.shape\nframe_rate1 = 6\nprint(img1.shape)","c2cb7082":"img1 = cv2.imread(paths4[4]) \nplt.figure(figsize=(10,10))\nplt.imshow(cv2.cvtColor(img1,cv2.COLOR_BGR2RGB))\n_ = plt.axis('off')","29ebeea1":"fourcc = cv2.VideoWriter_fourcc('m','p','4','v') \nvideo0 = cv2.VideoWriter('MoveNet_jogging.mp4',fourcc,frame_rate1,(X1,Y1))\n\nfor item in paths4:\n    img1 = cv2.imread(item)\n    img1 = cv2.resize(img1,(X1,Y1) ) \n    video0.write(img1)\n\nvideo0.release()","43797595":"!rm -rf sample1\n!rm -rf sample2","f9e2f3cb":"# MoveNet\nhttps:\/\/tfhub.dev\/s?q=movenet"}}