{"cell_type":{"2b0beb7d":"code","e4625d1c":"code","0af7fb90":"code","61220d82":"code","d7a4f33c":"code","a369f9f9":"code","805535d5":"code","04ff062e":"code","6b77ec24":"code","289cff13":"code","4948638e":"code","74bc431c":"markdown","5989859d":"markdown","001e9478":"markdown","8a254c80":"markdown","ab3cd13c":"markdown","28c24ac5":"markdown","5f0fed80":"markdown","06f744b3":"markdown","fc83e6c0":"markdown","0372a926":"markdown","0ed6316e":"markdown","e33c983e":"markdown"},"source":{"2b0beb7d":"import math\nimport numpy as np\n\nfrom bisect import bisect_left\nfrom functools import reduce\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline","e4625d1c":"def plotter(samples, grid=None, title='',):\n    \"\"\"\n    Plots a Square with width and height one and draws the samples inside.\n    If grid is provided it draws the given grid on the square.\n    \"\"\"\n\n    if grid is not None:\n        grid_i = grid[0]\n        grid_j = grid[1]\n        all_axis = np.zeros(samples.shape)\n        all_axis[:, 1] = 1\n\n    plt.scatter(samples[:, 0], samples[:, 1])\n    plt.title(title)\n    axes = plt.gca()\n    axes.set_xlim([-0.1, 1.1])\n    axes.set_ylim([-0.1, 1.1])\n    axes.set_aspect('equal')\n    if grid is not None:\n        for idx in range(len(grid_i)):\n            plt.plot([grid_i[idx], grid_i[idx]], all_axis[idx], alpha=0.2, color='b', dashes=[6, 2])\n        for idx in range(len(grid_j)):\n            plt.plot(all_axis[idx], [grid_j[idx], grid_j[idx]], alpha=0.2, color='b', dashes=[6, 2])\n\n        # add grid lines of the last elements of the square\n        plt.plot([all_axis[1], all_axis[1]], all_axis[idx], alpha=0.2, color='b', dashes=[6, 2])\n        plt.plot(all_axis[idx], [all_axis[1], all_axis[1]], alpha=0.2, color='b', dashes=[6, 2])\n\n    plt.show()\n\n\ndef get_factors(n):\n    \"\"\"\n    get factors and make it a set to remove duplicates\n    then make it a lit to sort in in increasing order\n    \"\"\"\n    \n    factors = list(set(reduce(list.__add__, ([i, n \/\/ i] for i in range(1, int(n ** 0.5) + 1) if n % i == 0))))\n    factors.sort()\n\n    # return all factors execept the original number (n)\n    return factors[:-1]\n\ndef take_closest(my_list, number):\n    \"\"\"\n    Assumes my_list is sorted. Returns closest value to number.\n    If two numbers are equally close, return the smallest number.\n    \"\"\"\n\n    pos = bisect_left(my_list, number)\n    if pos == 0:\n        return my_list[0]\n    if pos == len(my_list):\n        return my_list[-1]\n    before = my_list[pos - 1]\n    after = my_list[pos]\n    if after - number < number - before:\n        return after\n    else:\n        return before\n\n\ndef two_factors_to_n(n):\n    \"\"\"\n    n is an integer.\n    The function returns two integers that multiplied together give n -> int1 * int2 = n\n    \"\"\"\n\n    factors = get_factors(n)\n    int1 = take_closest(factors, math.sqrt(n))\n    int2 = n \/\/ int1\n    return int1, int2","0af7fb90":"N = 16 # number of random samples we will generate\nuniform = np.random.uniform # just make this call a bit cleaner","61220d82":"def random_sampling(N, get_grid=False):\n    return uniform(0, 1, (N, 2))\n    \nplotter(random_sampling(N=N), title='random sampling')","d7a4f33c":"def regular_sampling(N, get_grid=False):\n        samples = np.zeros((N, 2))\n\n        # split N into two factors in such a way N = nx * ny\n        ny, nx = two_factors_to_n(N)\n\n        # get all the combinations of nx and ny\n        j, i = np.meshgrid(range(nx), range(ny))\n\n        # get grid values\n        grid_i = i[:, 0] \/ nx\n        grid_j = j[0] \/ ny\n\n        j, i = j.flatten(), i.flatten()\n\n        # get samples\n        samples[:, 0] = (i + 0.5) \/ nx\n        samples[:, 1] = (j + 0.5) \/ ny\n\n        if get_grid:\n            return samples, [grid_i, grid_j]\n        return samples\n\nplotter(*regular_sampling(N=N, get_grid=True), title='regular sampling')","a369f9f9":"def jittered_sampling(N, get_grid=False):\n        samples = np.zeros((N, 2))\n\n        # split N into two factors in such a way N = nx * ny\n        ny, nx = two_factors_to_n(N)\n\n        # get all the combinations of nx and ny\n        j, i = np.meshgrid(range(ny), range(nx))\n\n        # get grid values\n        grid_i = i[:, 0] \/ nx\n        grid_j = j[0] \/ ny\n\n        # flatten idxs to get samples\n        j, i = j.flatten(), i.flatten()\n\n        # get samples\n        samples[:, 0] = uniform(i \/ nx, (i + 1) \/ nx)\n        samples[:, 1] = uniform(j \/ ny, (j + 1) \/ ny)\n\n        if get_grid:\n            return samples, [grid_i, grid_j]\n        return samples\n\nplotter(*jittered_sampling(N, get_grid=True), title='jittered sampling')","805535d5":"def half_jittered_sampling(N, get_grid=False):\n    samples = np.zeros((N, 2))\n\n    # split N into two factors in such a way N = nx * ny\n    ny, nx = two_factors_to_n(N)\n\n    # get all the combinations of nx and ny\n    j, i = np.meshgrid(range(ny), range(nx))\n\n    # get grid values\n    grid_i = i[:, 0] \/ nx\n    grid_j = j[0] \/ ny\n\n    # flatten idxs to get samples\n    j, i = j.flatten(), i.flatten()\n\n    # get samples\n    samples[:, 0] = uniform((i + 0.25) \/ nx, (i + 0.75) \/ nx)\n    samples[:, 1] = uniform((j + 0.25) \/ ny, (j + 0.75) \/ ny)\n\n    if get_grid:\n        return samples, [grid_i, grid_j]\n    return samples\n    \nplotter(*half_jittered_sampling(N, get_grid=True), title='half-jittered sampling')","04ff062e":"def poisson_disk_sampling(N, d, get_grid=False):\n    def get_mask(samples):\n        j, i = np.meshgrid(range(N), range(N))\n        j, i = j.flatten(), i.flatten()\n\n        dist = ((samples[i, 0] - samples[j, 0]) ** 2 + (samples[i, 1] - samples[j, 1]) ** 2).reshape(\n            (N, N))\n\n        upper_diag_idx = np.triu_indices(N)\n        dist[upper_diag_idx] = float('inf')\n        mask = np.any(dist < d ** 2, axis=1)\n        return mask\n\n    samples = uniform(0, 1, (N, 2))\n    mask = get_mask(samples)\n\n    # run the sampling on the elements that are closer than 'd' until\n    # the distances between samples are higher than 'd'\n    while np.any(mask):\n        samples[mask] = uniform(0, 1, (N, 2))[mask]\n        mask = get_mask(samples)\n\n    return samples\n    \nplotter(poisson_disk_sampling(N, 0.2), title='poisson sampling')","6b77ec24":"def nrooks_sampling(N, get_grid=False):\n    samples = np.zeros((N, 2))\n    i = np.arange(0, N)\n    samples_x = uniform(i \/ N, (i + 1) \/ N)\n    samples_y = uniform(i \/ N, (i + 1) \/ N)\n\n    # get grid values\n    grid_i = grid_j = i \/ N\n\n    # randomly shuffle samples over X \n    np.random.shuffle(samples_x)\n    # np.random.shuffle(samples_y)\n\n    samples[:, 0] = samples_x\n    samples[:, 1] = samples_y\n\n    if get_grid:\n        return samples, [grid_i, grid_j]\n    return samples\n    \nplotter(*nrooks_sampling(N, get_grid=True), title='n-rooks sampling')","289cff13":"# todo","4948638e":"# todo","74bc431c":"## Sobol Sequences","5989859d":"## Poisson Disk Sampling\n\n\nA simple way to avoid clumping of sample points is to generate\na sequence of samples, and reject a new sample if it is too\nclose to an existing sample [2, 3]. This method, called Poisson\ndisk sampling (also minimum-separation Poisson samling","001e9478":"## Random Sampling\n\nThe simplest way to choose $N$ points $(x_i; y_i),\\; i \\in [1,N]$ from the unit square is to pick every $x_i$ and $y_i$ independently by setting them to canonical random pairs. A canonical random number is uniformly distributed on $[0, 1]$","8a254c80":"## Jittered Sampling\n\n\nJittered sampling is another name for classical Monte Carlo stratifed sampling [1]. Typically, the unit square is partitioned into a set of equal area rectangles, and a point is chosen uniformly from each rectangle.","ab3cd13c":"## Halton sequences","28c24ac5":"Define a variable to control de number of samples","5f0fed80":"## Regular Sampling\n\nWe could simply place the pairs evenly in a grid pattern (as is done in traditional quadrature methods). This will prevent clumping of samples, but may introduce spatial regularity into the error across many pixels.","06f744b3":"#### Imports and utility functions","fc83e6c0":"## Quasi-Monte Carlo Method\n\n*source: https:\/\/en.wikipedia.org\/wiki\/Quasi-Monte_Carlo_method*\n\nIn numerical analysis, the quasi-Monte Carlo method is a method for numerical integration and solving some other problems using low-discrepancy sequences (also called quasi-random sequences or sub-random sequences). This is in contrast to the regular Monte Carlo method or Monte Carlo integration, which are based on sequences of pseudorandom numbers.\n\nMonte Carlo and quasi-Monte Carlo methods are stated in a similar way. The problem is to approximate the integral of a function f as the average of the function evaluated at a set of points $x_1, ..., x_N$:\n\n$$\\int_{[0, 1]^s}{f(u)du} = \\frac{1}{N}\\sum_{n=1}^{N}f(x_i)$$\n\nSince we are integrating over the $s$-dimensional unit cube, each $x_i$ is a vector of $s$ elements. The difference between quasi-Monte Carlo and Monte Carlo is the way the $x_i$ are chosen. Quasi-Monte Carlo uses a low-discrepancy sequence such as the Halton sequence, the Sobol sequence, or the Faure sequence, whereas Monte Carlo uses a pseudorandom sequence. The advantage of using low-discrepancy sequences is a faster rate of convergence. Quasi-Monte Carlo has a rate of convergence close to $O(1\/N)$, whereas the rate for the Monte Carlo method is $O(N^{\u22120.5})$.\n\n## Notes about the notebook\n\nThe following code and comments are highly based on the paper *Discrepancy as a Quality Measure for Sample Distributions* [link](http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download;jsessionid=CA4BAB1AED19B3A8C990BE63450DE355?doi=10.1.1.40.7922&rep=rep1&type=pdf)","0372a926":"## Half Jittered Sampling\n\nOne problem with jittered sampling is that limited clumping\ncan take place (up to four sample points could coincide).\nThis problem can be lessened by choosing points nearer to\nthe center of each square. Cook did this by using a truncated\nGaussian distribution to choose from each square. A simpler\n(and probably worse) method is to choose samples that\nare half-jittered, where points are chosen uniformly within a\nrectangle, half the width of the full rectangle.","0ed6316e":"## References\n\n> [1] Robert L. Cook, Thomas Porter, and Loren Carpenter.\n> Distributed ray tracing. Computer Graphics, 18(4):165{\n> 174, July 1984. ACM Siggraph '84 Conference Proceedings.\n\n> [2] Mark A. Z. Dippe and Erling Henry Wold. Antialiasing\n> through stochastic sampling. Computer Graphics,\n> 19(3):69{78, July 1985. ACM Siggraph '85 Conference\n> Proceedings.\n\n> [3] Robert L. Cook. Stochastic sampling in computer\n> graphics. ACM Transactions on Graphics, 5(1):51{72,\n> January 1986.\n\n> [4] Peter Shirley. Physical ly Based Lighting Calculations\n> for Computer Graphics. PhD thesis, University of Illinois\n> at Urbana-Champaign, November 1990.","e33c983e":"## Nrooks Sampling\n\nSampling\nCook et al. [3] generated sample distributions for a ninedimensional\nspace by separately partitioning \ndimensional subspaces (pixel area, camera lens area, re-\nection direction, shadow ray direction), and one onedimensional\nsubspace (time), and then associating the strata\nof each dimension at random. He called this technique uncorrelated\njittering.\n\nThis idea can be applied in two dimensions by randomly associating\nrows and columns of a $N$ by $N$ grid, so that each\nrow and column will have one sample [4]. A particularly\ndescriptive name for this strategy is $N$-rooks sampling, because\nan acceptable set of sample cells will be the squares\nof an $N$ by $N$ chessboard with $N$ rooks that cannot capture\neach other in one move."}}