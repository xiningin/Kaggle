{"cell_type":{"311bdc6e":"code","f71314a9":"code","bea4958c":"code","efdee06d":"code","3b4fd293":"code","64e79ea9":"code","2ce749fb":"code","dc81713f":"code","10811a6a":"code","657ce17b":"code","0975d05c":"code","a242ba5c":"code","5916ac9b":"code","61df9da5":"code","3aa11f63":"code","3b349ca9":"code","a24fc85a":"code","7289ec8e":"code","d1cbcaa6":"code","b3c878a0":"code","bccc586e":"code","89f43344":"code","f5707d84":"code","5285f409":"code","39b6ecf8":"code","d1e7d91e":"code","22fd6e21":"code","c5f47be9":"code","0ee60ac1":"code","6bd9520d":"code","7f892e3a":"code","2224cdf5":"code","a54c1e92":"code","0b77d905":"code","d3901f33":"code","b90da338":"code","21ce000d":"code","b6056279":"code","ec513512":"code","b07a9820":"code","a4e7fab0":"code","e0b59413":"code","816e1f9c":"code","09dc4b0e":"code","bd0a2c01":"code","692e5888":"markdown","617a8c70":"markdown","6d1af613":"markdown","c6719319":"markdown","8acd6153":"markdown","b5890a67":"markdown","a92e04ef":"markdown","def535e1":"markdown","f966886b":"markdown","40c97f1d":"markdown","1ff37c7a":"markdown","8ef5a268":"markdown","17a5ae4a":"markdown","deef73d0":"markdown","e9dbf153":"markdown","f2269f9d":"markdown","a4f04c8e":"markdown","54aa2e7a":"markdown","9bff0167":"markdown","00d6a47f":"markdown","b765daf6":"markdown","d90e1117":"markdown","6aacece8":"markdown","e39f39e7":"markdown","7e15c803":"markdown","069ac828":"markdown","dcfae293":"markdown","864eac65":"markdown","0b399aef":"markdown","6126e114":"markdown","ce9947c9":"markdown","428f51b5":"markdown","477b3ab4":"markdown","1625d362":"markdown","f89ac7b0":"markdown"},"source":{"311bdc6e":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom scipy import stats\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport plotly.graph_objects as go\nimport warnings\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","f71314a9":"df = pd.read_csv('\/kaggle\/input\/lead-scoring-x-online-education\/Leads X Education.csv')\ndf.head()","bea4958c":"df.info()","efdee06d":"df.describe()","3b4fd293":"# Identify columns that contain \"Select\" level\nfind_select = df.loc[: , (df == 'Select').any()] \n\n# Function converts \"Select\" level to NaN\ndef convert_to_NaN(df,col_names):\n    for col in col_names:\n        df[col][df[col] == 'Select'] = None\n        df[col].fillna(value=np.nan, inplace=True)\n        \nconvert_to_NaN(df,find_select.columns)","64e79ea9":"# Amount of missing data in each column\nnull_values = round(df.isnull().mean().sort_values(ascending = False)*100,2)\nnull_values","2ce749fb":"df.drop(['Prospect ID','Lead Number','How did you hear about X Education','Lead Profile'], axis=1, inplace=True)","dc81713f":"# The Conversion rate\nlabels = df['Converted'].value_counts().index\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=df['Converted'].value_counts())])\nfig.show()","10811a6a":"# Numerical input features in boxplots\nplt.figure(figsize=(14,5))\ni=1\nweb_interact = ['TotalVisits', 'Total Time Spent on Website','Page Views Per Visit']\n\nfor col in web_interact:\n    plt.subplot(1,3,i)\n    sns.boxplot(df[col])\n    i +=1\n    plt.tight_layout()","657ce17b":"# Numerical input features in distribution plots\nplt.figure(figsize=(14,5))\ni=1\nweb_interact = ['TotalVisits', 'Total Time Spent on Website','Page Views Per Visit']\n\nfor col in web_interact:\n    plt.subplot(1,3,i)\n    sns.distplot(df[col])\n    i +=1\n    plt.tight_layout()","0975d05c":"# Assigned scores in boxplots\nplt.figure(figsize=(12,5))\ni=1\nscore = ['Asymmetrique Activity Score','Asymmetrique Profile Score']\n\nfor col in score:\n    plt.subplot(1,2,i)\n    sns.boxplot(df[col])\n    i +=1\n    plt.tight_layout()","a242ba5c":"# Assigned scores in distribution plots\nplt.figure(figsize=(12,5))\ni=1\nfor col in score:\n    plt.subplot(1,2,i)\n    sns.distplot(df[col])\n    i +=1\n    plt.tight_layout()","5916ac9b":"pd.qcut(df['Asymmetrique Activity Score'], q=3)","61df9da5":"df.drop(['Asymmetrique Activity Index','Asymmetrique Profile Index'], axis=1, inplace=True)","3aa11f63":"# 2 features relation\nsns.pairplot(df, hue='Converted');","3b349ca9":"cat_df = df.select_dtypes(include='object')\n\n# Check for data variety in each column\ncat_df.nunique()","a24fc85a":"# Percentage of values in each feature\na = []\nfor col in cat_df.columns:\n    a.append(round(cat_df[col].value_counts()\/cat_df[col].count()*100,2))\na","7289ec8e":"df.drop(['Do Not Email', 'Do Not Call','Country','What matters most to you in choosing a course',\n         'Search','Magazine','Newspaper Article','X Education Forums','Newspaper','Digital Advertisement',\n         'Through Recommendations','Receive More Updates About Our Courses',\n         'Update me on Supply Chain Content','Get updates on DM Content','City',\n         'I agree to pay the amount through cheque'],axis=1,inplace=True)","d1cbcaa6":"# Current features with null values to impute\nplt.figure(figsize=(10,8))\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False);","b3c878a0":"new_df = df.copy().drop('Converted',axis=1)","bccc586e":"num_vars = new_df.select_dtypes(include=['float64','int64'])","89f43344":"# Cap the outliers  \n\ndef outliers_treatment(col_lst):\n    for col in col_lst:\n        percentiles = new_df[col].quantile([0.01,0.99]).values\n        new_df[col][new_df[col]<=percentiles[0]] = percentiles[0] # replace left outliers with left limit\n        new_df[col][new_df[col]>=percentiles[1]] = percentiles[1] # replace left outliers with right limit\n        \noutliers_treatment(num_vars.columns.tolist())","f5707d84":"# Numerical features df received outliers treatment\nnum_vars_ot = new_df.select_dtypes(include=['float64','int64'])","5285f409":"# These columns are treated separately\n# This df should concatenate into the final data set for training the model\n\nimpute_it = IterativeImputer(verbose=2, tol=1e-10)\nnum_vars_it = pd.DataFrame(impute_it.fit_transform(num_vars_ot),columns=num_vars.columns)","39b6ecf8":"# Missing data percentage\nnew_cat = new_df.select_dtypes(include='object')\nround(new_cat.isnull().mean()*100,2)","d1e7d91e":"# Grouping values with <5% count\ndef replace_value(col_lst):\n    for col in col_lst:\n        # Get the source list with less than 5% count\n        other_val = new_df[col].value_counts(normalize=True).loc[lambda x:x<0.05].index.tolist()\n        # Replace with \"Others\" level\n        new_df[col] = new_df[col].replace(other_val,'Others')\n        \nreplace_value(new_cat.columns.tolist())","22fd6e21":"# Categorical features with respect to conversion rate\nplt.figure(figsize=(12,15))\n\ni=1\nfor col in new_cat.columns:\n    plt.subplot(3,3,i)\n    sns.countplot(new_df[col],hue=df['Converted'])\n    i +=1\n    plt.xticks(rotation=90)\n    plt.tight_layout();","c5f47be9":"# Drop Specialization and Tags\nnew_df = new_df.drop(['Specialization','Tags'],axis=1)","0ee60ac1":"# Fill in \"Not Sure\" for missing value in Lead Quality:\nnew_df['Lead Quality'] = new_df['Lead Quality'].fillna(value='Not Sure')","6bd9520d":"# Fill in missing value with mode\nfill_mode = lambda col:col.fillna(col.mode()[0])\n\nnew_df = new_df.apply(fill_mode,axis=0)","7f892e3a":"# Categorical Columns\ncat_col = new_df.select_dtypes(include='object').columns.tolist()\n\nnew_df_encd = pd.get_dummies(new_df,prefix_sep=\"_\",columns=cat_col,drop_first=True)","2224cdf5":"old_num_col = num_vars.columns.tolist()\n\n# Drop old numerical columns\nnew_df_encd.drop(old_num_col,axis=1,inplace=True)\n\n# Concatenate the treated numerical features (num_vars_it) to the encoded categorical features\nX = pd.concat([num_vars_it,new_df_encd],axis=1)\n\n# The complete df \nleads_df = pd.concat([df['Converted'],X],axis=1)\n\nleads_df.head()","a54c1e92":"# Features Coorelation\ndata = [go.Heatmap(\n        z= leads_df.corr().values,\n        x= leads_df.columns.values,\n        y= leads_df.columns.values,\n        colorscale='RdBu_r',\n        opacity = 1.0 )]\n\nlayout = go.Layout(\n    title='Pearson Correlation of Input Features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks=''),\n    width = 900, height = 900)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()","0b77d905":"# Top 10 features that correlates with Coverted\nleads_df.corr()['Converted'].sort_values(ascending=False).head(10)","d3901f33":"X = leads_df.drop('Converted',axis=1)\ny = leads_df['Converted']","b90da338":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","21ce000d":"# Instantiate the Logistic Regression Model\nlogmodel = LogisticRegression(solver='liblinear')\nlogmodel.fit(X_train, y_train)","b6056279":"# Create a pipeline \npipe = make_pipeline(StandardScaler(),logmodel)\npipe.fit(X_train,y_train)","ec513512":"# The score order is: accuracy, precision, recall\ntrain_score = [] \nscoring = ['accuracy','precision','recall'] # specify the type of scoring \nscores = cross_validate(pipe, X_train, y_train, cv=10, scoring=scoring)\n\ntrain_score.append(scores['test_accuracy'].mean())\ntrain_score.append(scores['test_precision'].mean())\ntrain_score.append(scores['test_recall'].mean())\n\n# The model's score on training set\ntrain_scores = pd.DataFrame(train_score,columns=['Train Set'],\n                            index=['Accuracy','Precision','Recall'])\ntrain_scores","b07a9820":"# Predicted label (0,1) on the train set\ny_train_pred = cross_val_predict(logmodel,X_train,y_train,cv=10)\n\n# Probability of y_train \ny_train_pred_prob = cross_val_predict(logmodel,X_train,y_train,cv=10,method='predict_proba')\n\n# Outputs of the Train set:\ntrain_output = pd.DataFrame({'True Converted':y_train.values,'Predict Converted':y_train_pred,\n                             'Predict Probability':y_train_pred_prob[:,1]})\ntrain_output.head()","a4e7fab0":"# Finding precision, recall, and thresholds arrays\np, r, thresholds = precision_recall_curve(train_output['True Converted'], train_output['Predict Probability'])\npr_auc = metrics.auc(r,p)\n\n# Precison,recall vs Threshold chart\nplt.title(\"Precision-Recall vs Threshold Chart\")\nplt.plot(thresholds, p[: -1], \"b--\", label=\"Precision\")\nplt.plot(thresholds, r[: -1], \"r--\", label=\"Recall\")\nplt.ylabel(\"Precision, Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"lower left\")\nplt.ylim([0,1]);","e0b59413":"# f score\nfscore = (2 * p * r) \/ (p + r)\n# locate the index of the largest f score\nix = np.argmax(fscore)\nprint('Best Threshold=%.2f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\nprint('Precision score = %.2f, Recall score = %.2f' %(r[ix], p[ix]))\n\n# Precision vs Recall chart\nplt.plot(r, p, marker='.', label='Logistic',markersize=0.5)\nplt.scatter(r[ix], p[ix], marker='o', color='black', label='Best')\n\nplt.title('Precision-Recall Trade-off Chart')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend();","816e1f9c":"# Apply the model on X_test\ny_test_pred = pipe.predict(X_test)\n\n# Probability of predicted y_test\ny_test_pred_prob = pipe.predict_proba(X_test)[:,1]\n\n# Create a df for Test Set output\ntest_output = pd.DataFrame({'True Converted':y_test.values,'Predict Probability':y_test_pred_prob})\n\n# Predicted Converted Label in which optimal threshold applies\ntest_output['Predict Converted'] = test_output['Predict Probability'].apply(lambda x: 1 if x > 0.33 else 0)\n\n# Lead Score\ntest_output['Lead Score'] = round(test_output['Predict Probability']*100)\n\ntest_output.head()","09dc4b0e":"# The model's score on test set\ntest_score = []\n\nconf_matrix = confusion_matrix(y_test,y_test_pred)\nprint(classification_report(y_test,y_test_pred))\n'\\n'\nprint(conf_matrix)\n\ntn = conf_matrix[0,0]\nfp = conf_matrix[0,1]\ntp = conf_matrix[1,1]\nfn = conf_matrix[1,0]\n\ntotal = tn + fp + tp + fn\naccuracy  = (tp + tn) \/ total # Accuracy Rate\nprecision = tp \/ (tp + fp) # Positive Predictive Value\nrecall    = tp \/ (tp + fn) # True Positive Rate\nerror = (fp + fn) \/ total # Missclassification Rate\n\ntest_score.append(accuracy)\ntest_score.append(precision)\ntest_score.append(recall)\n\ntest_scores = pd.DataFrame(test_score,columns=['Test Set'],\n                            index=['Accuracy','Precision','Recall'])\ntest_scores","bd0a2c01":"# Compare the train and test scores\npd.concat([train_scores,test_scores],axis=1)","692e5888":"### C.1.2. Multiple Imputation","617a8c70":"Using the infortmation from Insights of Categorical Features in EDA section above, the following actions are executed:","6d1af613":"### C.2.1. Imputation","c6719319":"## C.2. Categorical Features","8acd6153":"### Insights:\n* Working professional are more likely to convert than Unemployed or Others groups. \n* The Specialization feature mainly provides details of working professional group. Thus, we can use the high level feature \"Occupation\" and drop the \"Specialization\". \n* For missing data, it seems ok to impute the mode of the column. \n* Except the Lead Quality feature with more than 50% data missing. From a business standpoint, a \"Not Sure\" value can replace these missing values\n* Tags feature carry similar inputs from occupation, last activity. Will drop this feature","b5890a67":"Note: There is a level call \"Select\" that should be treated as Null values. The \"Select\" level means the viewer\/customers did not make a selection","a92e04ef":"* There are outliers in TotalVisits, Page Views Per Visit, Activity Score. Besides, there are missing values in all 4 columns graphed above. Therefore, I'm going to treat the outliers and fill in missing data. \n* As seen above, scores are equally binned into 3 groups, which then identified as indices. It makes sense to keep either the score or the index because they are related to each other. ","def535e1":"### C.1.1. Outliers treatment","f966886b":"The **optimal threshold** to determine the binary class **is 0.33**\n\nThe two charts and the model's score on train set are in agreement pointing at the optimal threshold of 0.33 locates where Precision is 0.85 and Recall is 0.75","40c97f1d":"### C.2.2. Encoding","1ff37c7a":"# B. Importing the Dataset and Preview","8ef5a268":"### D.1. Train - Test Split","17a5ae4a":"### B.2.2 Categorical Features","deef73d0":"# F. Conclusion","e9dbf153":"## B.1. Missing data","f2269f9d":"The missing values in each numerical features will be imputed with relation to each other by using the Multiple Imputation method","a4f04c8e":"# A. Business Goal\n\n***Background:*** X Education provides online courses to industry professionals. Many professionals who are interested in the courses land on the website and browse for courses. X education advertises its courses across several marketing platforms such as Google, Olark chat, etc. Once visitors land on the website, they might perform engagement activities such as browsing courses, filling up forms, or watching some videos. When visitors fill up forms providing their email address or phone number, they get converted to leads. The company also acquires leads through past referrals. Once leads are acquired, employees from the sales team phone and email campaigns. Through this process, a fraction of generated leads gets converted into customers. However, the typical lead conversion rate at X education is around 30%, which is something this notebook attempts to improve.\n\nThe company goal is to build **a logistic regression model wherein you need to assign a lead score** to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. \n\n## Question: \n**What factors affect lead-to-customer conversion the most?**\n\nSome to consider\n* Audience: student, professionals, unpemployed, city and country of residence, etc\n* Marketing: websites, search engines, media platforms, referals, etc\n* Sale: method of engaging the leads (calls, texts, emails, free materials, etc)","54aa2e7a":"The logistic regression model built with threshold of 0.33 is able to produce 86% accuracy and 85% precision on identify the users that will likely to convert to cutomers. \n\nThe main features that the company should focus on to increase the conversion rate are: Website contents (increase Time Spent on Website), Working Professional, Sending SMS, Customers who fill Add From\n\nThe results and findings from this notebook is summarized [here](https:\/\/medium.com\/@nguyenpham111\/tips-to-improve-conversion-rate-for-online-educational-providers-fd84c9a43226)","9bff0167":"### D.4. Test The Model","00d6a47f":"## C.3. Ensemble the Final Clean Data Set","b765daf6":"The optimal threshold is the point that results in the best balance of precision and recall. This is the same as optimizing the F-measure","d90e1117":"### Question:\n**We observe 45.65% of missing Asymmetrique Activity and Profile Score data. Is there a rational way to inpute without breaking business logic?**\n\n**Insights**: \n* *Assumption*: The activity and profile scores look like a result of another algorithm that take in the users' web interaction such as: time spent on website, amount of visits, page views, personal info provided and such.  \n* This scoring system can be an important input from another team. They can carry some weighs and business assessment.\n* Because they are multimodal distributions, it is not appropriate to choose a univariate to impute the missing data. \n* I'm going to use Multiple Imputation method, which estimates each feature from all others. ","6aacece8":"## C.1. Numerical Features","e39f39e7":"## E. Model Evaluation","7e15c803":"## B.2. EDA","069ac828":"### D.3. Tune the Optimal Threshold that classify labels 0 vs 1","dcfae293":"### B.2.1. Numerical Features","864eac65":"**Majority of categorical features have 25-51% of missing values**\n* In general, values with less than 5% of data share similar meaning. Most of them are inactive or indecisive action. I'm going to group these values into \"Others\" level\n* As mentioned earlier, I'm going to explored and impute appropriate values for missing values in Data Preparation section.\n\n**Columns have very few variations in values can be dropped:**\nDo Not Email, Do Not Call, Country, What matters most to you in choosing a course, Search, Magazines, Newspapers Articles, X Education Forums, Newspapers, Digital Advertisement, Through Recommendations, Receive More Updates About Our Courses, Update me on Supply Chain Content, Get updates on DM Content, City, I agree to pay the amount through cheque\n","0b399aef":"### Question:\n**What other features data engineers can implement to improve the model?**\n\nThe evaluation scores indicate that the model is not over-fitting and a minimal chance of data leakage. We can improve the dataset by collecting some more features:\n* Improve quality of the survey or form questions to receive more user inputs (reduce NaN values)\n* Improve algorithm for the Activity and Profile Score to produce complete results\n* Time stamp visiting the websites for seasonality analysis","6126e114":"* In general, people are more likely to convert if they spend more time on website, regardless to the amount of visits and number of pages viewed.\n* The activity and profile scores of whom converted seem to be in correlation with Total Time Spent of Website ","ce9947c9":"### D.2. Train the models\n\nApply cross validation with k-fold = 10 on the train set\n","428f51b5":"### Insights:\n* The target column \"Converted\" doens't have any null values\n* Drop columns with more than 70% of missing values: How did you hear about X Education, Lead Profile\n* An equal portion of missing data are present in Assymetrique Index and Score columns (45.65%)\n* Features with <2% of missing data can be imputed with appropriate values\n* Features with 25%-50% of missing data will be explored for possibility of imputation or dropping","477b3ab4":"# D. Modeling ","1625d362":"We observe an imbalanced labels in Converted from the dataset. Therefore, a precision-recall analysis is more approriate than ROC","f89ac7b0":"# C. Data Preparation"}}