{"cell_type":{"439e7b44":"code","7d71a03f":"code","99cd33d9":"code","e06f1e3e":"code","88941761":"code","fb746ec2":"code","0dc89d8e":"code","3d72dd67":"code","03d6cfee":"code","263d8b63":"code","c66ecf23":"code","1d1beff0":"code","8c3fb3d0":"code","f38d8987":"code","ba7b0e4e":"code","b0a49faf":"code","36e01729":"code","c72c36f7":"code","87ab8c3f":"code","455248aa":"code","3f2b3d38":"code","ea8a68c7":"code","e8d3d28f":"code","fb4a5c2f":"code","62cf0142":"code","cbff25dd":"markdown","20f56bd3":"markdown","aabeb12d":"markdown","848ded85":"markdown","e3a5e4a8":"markdown","2c8ac622":"markdown","842035a1":"markdown","6efc33d5":"markdown","d2238399":"markdown","a30a913c":"markdown"},"source":{"439e7b44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom itertools import chain\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# plot\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.graph_objects as go\n\n# text preprocessing\nimport re\nimport nltk\n# uncomment if not not downloaded\n#nltk.download('stopwords')\n#nltk.download('punkt')\n#nltk.download('brown')\n#nltk.download('names')\n#nltk.download('wordnet')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('universal_tagset')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob\n#from normalise import normalise\n\n\n# feature Engineering and feature Selection\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel,SelectKBest,chi2,mutual_info_classif\n\n# ML model\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,OrdinalEncoder\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold, cross_val_score\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d71a03f":"train = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-solving-the-citizens-grievances\/dataset\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-solving-the-citizens-grievances\/dataset\/test.csv\")\ntest['importance']=-1\ntrain['label'] = 'train'\ntest['label'] = 'test'\ncombined = pd.concat([train,test],axis=0)\ncombined.shape","99cd33d9":"# Step 1: Create Features from the Given Data\n# In the second step, we will vectorize the data\n\ndef combine_and_count_issues(df):\n    print('Combining and Counting Total Issues')\n    issue_columns = [x for x in list(df.columns) if 'issue' in x]\n    df['issues'] = df[issue_columns].apply(lambda x: '. '.join([val for val in x if pd.notna(val)]), axis=1)\n    df['total_issues'] = df[issue_columns].apply(lambda x:  sum([int(pd.notna(i)) for i in x]),axis=1) \n    df.drop(issue_columns, axis=1, inplace=True)\n    df[['issues']].fillna('',inplace=True)\n    return df\n\n# This function encodes the respondents and counts the total respondents\ndef encode_and_count_respondents(df):\n    print('Encoding and Counting Respondents')\n    dict1 = dict([(c,[b]) for b,c in zip( df['respondentOrderEng'],df['country.name'])])\n    dict2 = dict([(c,[b]) for b,c in zip( combined['respondentOrderEng'],combined['respondent.0'])]) \n    dict1.update(dict2)\n    df['respondent.0'] = df['respondent.0'].apply(lambda x: dict1[x][0])\n    df['respondent.1'] = df['respondent.1'].apply(lambda x: dict1[x][0] if pd.notnull(x) else x)\n    df['respondent.2'] = df['respondent.2'].apply(lambda x: dict1[x][0] if pd.notnull(x) else x)\n    df['respondent.3'] = df['respondent.3'].apply(lambda x: dict1[x][0] if pd.notnull(x) else x)\n    df['respondent.4'] = df['respondent.4'].apply(lambda x: dict1[x][0] if pd.notnull(x) else x)\n    respondent_cols = [col for col in list(df.columns) if'respondent.' in col]\n    df['total_respondents'] = df[respondent_cols].apply(lambda x:  sum([int(pd.notna(i)) for i in x]),axis=1) \n    #df.drop(respondent_cols, axis=1, inplace=True)\n    df.drop('respondentOrderEng', axis=1, inplace=True)\n    return df\n\n# Remove cols not important for Modelling\ndef drop_cols(df):\n    cols =['parties.0','parties.2', 'country.alpha2', 'parties.1', \\\n           'country.name', 'docname', 'appno', 'ecli', 'kpdate','sharepointid','originatingbody_name']\n    df.drop(cols, axis=1, inplace=True)\n    return df\n\n# This function removes features where there is no variance\ndef remove_constant_cols(df):\n    print('Remove Constant Columns')\n    for col in df.columns:\n        if df[col].nunique()==1:\n            print(col,end=', ' )\n            del df[col]\n    return df\n\n# This function generates new date features\ndef generate_date_features(df):\n    print('generate date features')\n    df['days_between_intro_decision'] = (pd.to_datetime(df['decisiondate']) - pd.to_datetime(df['introductiondate'])).dt.days\n    df['days_between_intro_judgement'] = (pd.to_datetime(df['judgementdate']) - pd.to_datetime(df['introductiondate'])).dt.days\n    df['days_between_decision_judgement'] = (pd.to_datetime(df['judgementdate']) - pd.to_datetime(df['decisiondate'])).dt.days\n    df.drop(['decisiondate','introductiondate','judgementdate'], axis=1, inplace=True)\n    return df\n\n# Encoding for few more feature columns\ndef col_encode(df):\n    print('One-hot encoding Relevcant Rows')\n    le = LabelEncoder()\n    # Item Ids are in ascending order of judgement date..So let's convert it using label encoder\n    df['itemid'] = df['itemid'].apply(lambda x: x[4:7])  \n    pd.get_dummies(df,columns=['doctypebranch'])\n    df.drop(['doctypebranch'], axis=1, inplace=True )\n    df['separateopinion'] = le.fit_transform(df['separateopinion'])   \n    return df\n\n# Fill Any missing values with 0 except for issues\ndef fill_missing(df):\n    print('Replace NA values in Numerical Columns with 0')\n    for col in df.columns:\n        if col not in ['issues','label']:\n            df[col].fillna(0,inplace=True)\n            df[col] = df[col].astype('int')\n    return df\n\ndef calculate_articles_paragraph(df):\n    articles_columns = [x for x in list(df.columns) if 'article' in x and '_article' not in x]\n    df['total_articles'] = df[articles_columns].apply(lambda x:  sum([i for i in x]),axis=1)\n    paragraph_columns = [x for x in list(df.columns) if 'paragraphs' in x]\n    df['total_paragraphs'] = df[paragraph_columns].apply(lambda x:  sum([i for i in x]),axis=1)\n    return df","e06f1e3e":"combined = combine_and_count_issues(combined)\ncombined = encode_and_count_respondents(combined)\ncombined = drop_cols(combined)\ncombined = remove_constant_cols(combined)\ncombined = generate_date_features(combined)\ncombined = col_encode(combined)\ncombined = fill_missing(combined)\ncombined = calculate_articles_paragraph(combined)","88941761":"# Text Processing\n\ndef tokenize(text):\n    return nltk.word_tokenize(text)\n\ndef remove_numbers(text):\n    return re.sub(r'\\d+', '', text)\n\ndef words_lemmatizer(text, encoding=\"utf8\"):\n    words = nltk.word_tokenize(text)\n    lemma_words = []\n    wl = WordNetLemmatizer()\n    for word in words:\n        pos = find_pos(word)\n        lemma_words.append(wl.lemmatize(word, pos))\n    return \" \".join(lemma_words)\n\ndef stemmer(text):\n    porter_stemmer=nltk.PorterStemmer()\n    words = [porter_stemmer.stem(word) for word in nltk.word_tokenize(text)]\n    return \" \".join(words)\n\n\n# Function to find part of speech tag for a word\ndef find_pos(word):\n    # Part of Speech constants\n    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n    \n    # Adjective tags - 'JJ', 'JJR', 'JJS'\n    if pos.lower()[0] == 'j':\n        return 'a'\n    # Adverb tags - 'RB', 'RBR', 'RBS'\n    elif pos.lower()[0] == 'r':\n        return 'r'\n    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n    elif pos.lower()[0] == 'v':\n        return 'v'\n    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n    else:\n        return 'n'\n\ndef remove_stopwords(text, lang='english'):\n    \"\"\"\n    :param text: text input\n    :return: text with stopwords removed\n    :rtype: str \n    \"\"\"\n    words = nltk.word_tokenize(text)\n    lang_stopwords= [i for i in stopwords.words(lang) if i not in ['not', 'no']]\n    stopwords_removed= [w for w in words if w not in lang_stopwords]\n    return \" \".join(stopwords_removed)","fb746ec2":"def do_preprocessing(df):\n    \"\"\"\n    create handcrafted features\n    \"\"\"\n    df['Issue_cleaned'] = df['issues'].str.lower().\\\n                                str.replace('[^\\w\\s]|_', ' ').\\\n                                apply(remove_numbers).\\\n                                apply(remove_stopwords)\n    df['Issues_cleaned_lemma'] = df['Issue_cleaned'].apply(words_lemmatizer)\n    df['Issues_cleaned_stem'] = df['Issue_cleaned'].apply(stemmer)\n    # hand-crafted features\n    df['words'] = df['issues'].apply(tokenize)\n    df['text__len'] = df['words'].apply(len)\n    df['sent__num'] = df['issues'].apply(lambda x: len(nltk.sent_tokenize(x)))\n    df['digit__cnt'] = df['words'].apply(lambda x: sum([re.search('\\d', i) is not None for i in x]))\n    df['bracket__cnt'] = df['words'].apply(lambda x: sum([re.search('\\(|\\)|\\[|\\]', i) is not None for i in x]))\n    df['equal__cnt'] = df['issues'].apply(lambda x: len(re.findall('=|<-', x)))\n    df['verb__cnt'] = df['issues'].apply(lambda x: sum([tag[0]=='V' for token, tag in TextBlob(x).tags]))\n    df['noun__cnt'] = df['issues'].apply(lambda x: sum([tag[0]=='N' for token, tag in TextBlob(x).tags]))\n    df['adv__cnt'] = df['issues'].apply(lambda x: sum([tag[0]=='R' for token, tag in TextBlob(x).tags]))\n    df['adj__cnt'] = df['issues'].apply(lambda x: sum([tag[0]=='J' for token, tag in TextBlob(x).tags]))\n    df['criminal__cnt'] = df['issues'].apply(lambda x: len(re.findall('criminal|crime', x)))\n    #df['keywrds__weight'] = df['Comment_cleaned_lemma'].apply(lambda x: compute_weight(x))    \n    df['nonStop__cnt'] = df['words'].apply(lambda x: len([i for i in x if i not in stopwords.words('english')]))\n    df['continuousChar__cnt'] = df['issues'].apply(lambda x: len([match.group() for match in re.compile(r'([a-z])\\1{2,}').finditer(x)]))\n    df['continuousDigit__cnt'] = df['issues'].apply(lambda x: len([match.group() for match in re.compile(r'([0-9])\\1{2,}').finditer(x)]))\n    df['continuousPunct__cnt'] = df['issues'].apply(lambda x: len([match.group() for match in re.compile(r'([\\W|_])\\1{2,}').finditer(x)]))\n    return df","0dc89d8e":"combined_train = combined.query('label == \"train\"').drop(['label'] , axis=1)\ncombined_train = do_preprocessing(combined_train)","3d72dd67":"# Train-Test Split in 80:20 Ratio\nX_train, X_test, Y_train, Y_test = train_test_split(combined_train.drop('importance', axis=1),combined_train['importance'],test_size=0.2,stratify=combined_train['importance'])","03d6cfee":"# Vectorize the issue column and add the generated features to the X_train matrix\nvect = TfidfVectorizer(stop_words='english',ngram_range=(1,3),#token_pattern=r'b[^\\d\\W]+\\b',\n                       min_df=5,binary=True)\nX_train_dtm = vect.fit_transform(X_train['Issues_cleaned_lemma'])\ndf1 = pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())","263d8b63":"# Merge X_train with features matrix\nX_train.drop(['Issues_cleaned_lemma','issues','Issue_cleaned','Issues_cleaned_stem','words'],axis = 1, inplace = True)\nX_train.reset_index(drop=True, inplace = True)\nres = pd.concat([X_train, df1], axis=1)","c66ecf23":"# Prepare X_test Matrix\nX_test_dtm =  vect.transform(X_test['Issues_cleaned_lemma'])\ndf_test = pd.DataFrame(X_test_dtm.toarray(), columns=vect.get_feature_names())\nX_test.drop(['Issues_cleaned_lemma','issues','Issue_cleaned','Issues_cleaned_stem','words'],axis = 1, inplace = True)\nX_test.reset_index(drop=True, inplace = True)\nres_test = pd.concat([X_test, df_test], axis=1)","1d1beff0":"# feature selection\ndef select_features(X_train, y_train, X_test,k):\n    fs = SelectKBest(score_func=mutual_info_classif, k=k)\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs","8c3fb3d0":"combined_test = combined.query('label == \"test\"')\ncombined_test = do_preprocessing(combined_test)\ntest = combined_test.drop(['issues','label','Issue_cleaned','Issues_cleaned_stem','words', 'importance'] , axis=1)","f38d8987":"X_test_dtm_s=vect.transform(test['Issues_cleaned_lemma'])\ndf2 = pd.DataFrame(X_test_dtm_s.toarray(), columns=vect.get_feature_names())\ndel test['Issues_cleaned_lemma']\ntest.reset_index(drop = True,inplace = True)\nres2 = pd.concat([test, df2], axis=1)","ba7b0e4e":"# With 200 features\nX_train_fs,X_test_fs = select_features(res, Y_train, res_test,200)\nXG200 = XGBClassifier()\nxg_train = cross_val_predict(XG200, X_train_fs, Y_train, cv=5, \n                                  n_jobs=-1, method=\"predict\")\nprint(\"cv score: Train \", accuracy_score(xg_train, Y_train) * 100)\n\nXG200.fit(X_train_fs,Y_train)\nxg_test = XG200.predict(X_test_fs)\n\nprint(\"cv score: Train \", accuracy_score(xg_test, Y_test) * 100)","b0a49faf":"# With 150 features\nX_train_fs,X_test_fs = select_features(res, Y_train, res_test,150)\nXG = XGBClassifier()\nxg_train = cross_val_predict(XG, X_train_fs, Y_train, cv=5, \n                                  n_jobs=-1, method=\"predict\")\nprint(\"cv score: Train \", accuracy_score(xg_train, Y_train) * 100)\n\nXG.fit(X_train_fs,Y_train)\nxg_test = XG.predict(X_test_fs)\n\nprint(\"cv score: Test \", accuracy_score(xg_test, Y_test) * 100)","36e01729":"X_train_fs,X_test_fs = select_features(res, Y_train, res2,150)\nxg_test_set = XG.predict(X_test_fs)\n\ntest = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-solving-the-citizens-grievances\/dataset\/test.csv\")\nsub = pd.DataFrame(columns=[\"appno\",\"importance\"])\nsub[\"appno\"] = test.appno\nsub[\"importance\"] = xg_test_set\nsub.to_csv(\"result_tunedxg150.csv\", index=False)","c72c36f7":"X_train_fs,X_test_fs = select_features(res, Y_train, res2,200)\nxg_test_set = XG200.predict(X_test_fs)\n\nsub = pd.DataFrame(columns=[\"appno\",\"importance\"])\nsub[\"appno\"] = test.appno\nsub[\"importance\"] = xg_test_set\nsub.to_csv(\"result_tunedxg200.csv\", index=False)","87ab8c3f":"from catboost import CatBoostClassifier\n\n# With 150 features\nX_train_fs,X_test_fs = select_features(res, Y_train, res_test,150)\n\ncat150 = CatBoostClassifier(random_state=40,n_estimators=1005)\ncat_pred = cross_val_predict(cat150, X_train_fs, Y_train, cv=5, \n                                  n_jobs=-1, method=\"predict\")\nprint(\"cv score: \", accuracy_score(cat_pred, Y_train) * 100)\n\ncat150.fit(X_train_fs,Y_train)","455248aa":"cat_test = cat150.predict(X_test_fs)\n\nprint(\"cv score: Test \", accuracy_score(cat_test, Y_test) * 100)","3f2b3d38":"# With 200 features\nX_train_fs,X_test_fs = select_features(res, Y_train, res_test,200)\n\ncat200 = CatBoostClassifier(random_state=40,n_estimators=1005)\ncat_pred = cross_val_predict(cat200, X_train_fs, Y_train, cv=5, \n                                  n_jobs=-1, method=\"predict\")\nprint(\"cv score: \", accuracy_score(cat_pred, Y_train) * 100)\n\ncat200.fit(X_train_fs,Y_train)\ncat_test = cat200.predict(X_test_fs)\n\nprint(\"cv score: Test \", accuracy_score(cat_test, Y_test) * 100)","ea8a68c7":"X_train_fs,X_test_fs = select_features(res, Y_train, res2,150)\ncat_test = cat150.predict(X_test_fs)\n\nsub[\"appno\"] = test.appno\nsub[\"importance\"] = cat_test\nsub.to_csv(\"result_cat150.csv\", index=False)","e8d3d28f":"# With 200 Features\nX_train_fs,X_test_fs = select_features(res, Y_train, res_test,200)\n\n\nlg200 = LGBMClassifier(learning_rate=0.01,max_depth=7,n_estimators=1000)\n\n\nlg200_pred = cross_val_predict(lg200, X_train_fs, Y_train, cv=5, \n                                  n_jobs=-1, method=\"predict\")\nprint(\"cv score: \", accuracy_score(lg200_pred, Y_train) * 100)\n\nlg200.fit(X_train_fs,Y_train)\nlg200_test = lg200.predict(X_test_fs)\n\nprint(\"cv score: Test \", accuracy_score(lg200_test, Y_test) * 100)","fb4a5c2f":"# With 150 Features\nX_train_fs,X_test_fs = select_features(res, Y_train, res_test,150)\n\n\nlg150 = LGBMClassifier(learning_rate=0.01,max_depth=7,n_estimators=1000)\n\n\nlg150_pred = cross_val_predict(lg150, X_train_fs, Y_train, cv=5, \n                                  n_jobs=-1, method=\"predict\")\nprint(\"cv score: \", accuracy_score(lg150_pred, Y_train) * 100)\n\nlg150.fit(X_train_fs,Y_train)\nlg150_test = lg150.predict(X_test_fs)\n\nprint(\"cv score: Test \", accuracy_score(lg150_test, Y_test) * 100)","62cf0142":"from sklearn.ensemble import  VotingClassifier\neclf1 = VotingClassifier(estimators=[('lgb', lg150), ('xg', XG), ('cat', cat150)], voting='soft')\nX_train_fs,X_test_fs = select_features(res, Y_train, res_test,150)\neclf1 = eclf1.fit(X_train_fs, Y_train)\nvpred = eclf1.predict(X_test_fs)\n\nprint(\"cv score: Test \", accuracy_score(vpred, Y_test) * 100)","cbff25dd":"\n**Model Building : xgboost, LGBM, Catboost, Voting Classifier**","20f56bd3":"LGBM Training and Prediction","aabeb12d":"Voting Classifier","848ded85":"Catboost Modeling and Prediction","e3a5e4a8":"XGBOOST","2c8ac622":"Voting Classifier gives the highest accuracy","842035a1":"Feature Engineering\nPart 2 : Text Features\n\n1) Lemmatize the 'issues' column\n\n2) Generate Hand Crafted Features : Calculate the number of times a POS ( Parts of Speech) occured.\nCalculate the number of times a sequence of Digits occured. This would indicate the number of articles referenced\n3) Create a special Column for Criminal grievances\n\n\nOnce basic text pre-processing is done, generate features from the text Column using TF-IDF with ngrams between 1 and 5","6efc33d5":"LGBM","d2238399":"Feature Engineering :\nPart-1\n\n1) Intiution says that a severe crime translates to multiple violations. How do we measure multiple violations ? By counting the total number of issues a case is registered under. This is done by combine_and_count_issues function\n\n\n2) Again, we seek to establish whether No of respondents is related to severity of a grievance.\nH0 : There is no relationship between number of respondents\n\nH1 : There is a relationship\n<\/br>\nencode_and_count_respondents is the function for it\n\n\n3) Many Columns do not have much variance, and therefore do not contribute to the output . These were removed Functions : drop_cols , remove_constant_cols\n\n\n4) Date Columns were compared to calculated the number of days elapsed between milestones Functions: generate_date_features\n\n\n5) Item-Id indicates an unique id for a case. However, the numbers were in increasing order. So, this could indicate the recency of a case. This column is label-encoded to give it a relationship such that a particular Case came before the others The other columns were one-hot encoded, as they can't be compared on a nominal scale. Functions: col_encode\n\n\n6) A simple function to impute the missing values with 0. This is because most of the features are kind of dummy categorical variables, so no need to impute with mean or median\n\n\n7) A function to count the total number of articles or paragraphs referenced per case Function: calculate_articles_paragraph","a30a913c":"Feature Selection :\n\nFor doing a feature selection, We have to split the dataset firt. Due to presence of multiple labels in the output column, We do a stratify sampling to generate the train and test sets.\n\n\nFeature Selection is done using a mutual_info_classifier, as it was found to be empirically better than chi-square test"}}