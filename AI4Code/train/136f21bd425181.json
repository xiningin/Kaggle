{"cell_type":{"40f936e6":"code","f08d8b29":"code","3a735c21":"code","d279956d":"code","60c84d00":"code","d332dffa":"code","8c41d070":"code","bb4e0902":"code","03a379df":"code","9abebd7f":"code","13a74530":"code","838fb176":"code","8265c5e7":"code","56a8731e":"code","cd6317ac":"code","265214d6":"code","591a2998":"code","7c8e098d":"code","ad7a4f4b":"code","d32eb189":"code","e0d8b91e":"code","bbb8917e":"code","2287a21b":"code","677f3579":"code","91066a5d":"code","5f73d8df":"code","04579ddf":"code","e137f7b4":"code","f6fd84db":"code","0469535f":"code","0aa60512":"code","ce5bf750":"code","fd927f6a":"code","5feb79e5":"code","a3eaa1bb":"code","3ea96ffd":"markdown","c02965a8":"markdown","3ee5b3b0":"markdown","a0d9a7e6":"markdown","398d07c6":"markdown","3f750cbd":"markdown","4ab6b5ca":"markdown","dd2392c8":"markdown","73642bbd":"markdown","bf22de90":"markdown","96961259":"markdown","ddebc684":"markdown","5fcf25e9":"markdown","9bd6920d":"markdown","b410dd1b":"markdown","0b128582":"markdown","fbd6efed":"markdown","31a64b42":"markdown","93b28844":"markdown","08cc56ac":"markdown","f63504be":"markdown","23bc0853":"markdown","77c8b310":"markdown","e04dfb3c":"markdown","9da53bd6":"markdown","001c035d":"markdown","3cc817a7":"markdown","25627cee":"markdown","b77e5ecf":"markdown"},"source":{"40f936e6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score, roc_auc_score, mean_squared_error, mean_absolute_error\nfrom imblearn.over_sampling import RandomOverSampler\n\nimport warnings\nwarnings.filterwarnings('ignore')","f08d8b29":"df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\n\ndf.head(3)","3a735c21":"df_copy = df.copy()","d279956d":"df.loc[df.quality >= 7, 'quality'] = 1\ndf.loc[(df.quality < 7) & (df.quality != 1), 'quality'] = 0","60c84d00":"sns.pairplot(df, hue='quality');","d332dffa":"plt.title('Class distribution')\nsns.countplot(x='quality', data=df);","8c41d070":"plt.figure(figsize=(10, 10))\nplt.xticks(rotation='vertical')\nsns.boxplot(data=df);","bb4e0902":"SEED = 9\nDECIMALS = 2","03a379df":"x = df.drop('quality', axis=1).values\ny = df.quality.values\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=SEED, stratify=y)","9abebd7f":"def get_f1_rocauc(model, dec=DECIMALS):\n    preds = model.predict(x_test)\n    f1 = f1_score(y_test, preds)\n    roc_auc = roc_auc_score(y_test, preds)\n    f1_round = np.round(f1, dec)\n    roc_auc_round = np.round(roc_auc, dec)\n    \n    print(f'F1 score: {f1_round}, ROC AUC: {roc_auc_round}')","13a74530":"rfc_base = RandomForestClassifier(n_jobs=-1, random_state=SEED)\nrfc_base.fit(x_train, y_train)\n\nget_f1_rocauc(rfc_base)","838fb176":"xgbc_base = xgb.XGBClassifier(n_jobs=-1, random_state=SEED)\nxgbc_base.fit(x_train, y_train)\n\nget_f1_rocauc(xgbc_base)","8265c5e7":"lgbmc_base = lgb.LGBMClassifier(n_jobs=-1, random_state=SEED)\nlgbmc_base.fit(x_train, y_train)\n\nget_f1_rocauc(lgbmc_base)","56a8731e":"ros = RandomOverSampler(random_state=SEED)\nx_resampled, y_resampled = ros.fit_resample(x_train, y_train)","cd6317ac":"rfc_os = RandomForestClassifier(n_jobs=-1, random_state=SEED)\nrfc_os.fit(x_resampled, y_resampled)\n\nget_f1_rocauc(rfc_os)","265214d6":"xgbc_os = xgb.XGBClassifier(n_jobs=-1, random_state=SEED)\nxgbc_os.fit(x_resampled, y_resampled)\n\nget_f1_rocauc(xgbc_os)","591a2998":"lgbmc_os = lgb.LGBMClassifier(n_jobs=-1, random_state=SEED)\nlgbmc_os.fit(x_resampled, y_resampled)\n\nget_f1_rocauc(lgbmc_os)","7c8e098d":"skf = StratifiedKFold(4, shuffle=True, random_state=SEED)","ad7a4f4b":"def train_gscv_model(estimator, param_grid, task, skf=skf):\n    if task == 'class':\n        model = GridSearchCV(estimator, param_grid, scoring='f1', n_jobs=-1, cv=skf)\n        model.fit(x, y)\n        print('Best f1 score: ', np.round(model.best_score_, DECIMALS))\n        print('Best params: ', model.best_params_)\n    elif task == 'reg':\n        model = GridSearchCV(estimator, param_grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=skf)\n        model.fit(x, y)\n        print('Best MSE: ', np.round(model.best_score_, DECIMALS))\n        print('Best params: ', model.best_params_)\n    else:\n        raise ValueError(f'{task} task is not exist... yet')","d32eb189":"param_rfc = {\n    'n_estimators': [100, 200, 300],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [5, None],\n    'class_weight': ['balanced', 'balanced_subsample', None]\n}\n\ntrain_gscv_model(rfc_base, param_rfc, 'class')","e0d8b91e":"param_xgbc = {\n    'n_estimators': [100, 200, 300]\n}\n\ntrain_gscv_model(xgbc_base, param_xgbc, 'class')","bbb8917e":"param_lgbmc = {\n    'boosting_type': ['gbdt', 'dart', 'goss', 'rf'],\n    'max_depth': [5, -1],\n    'learning_rate': [.1, .001, .0001],\n    'n_estimators': [100, 200, 300],\n    'class_weight': ['balanced', None]\n}\n\ntrain_gscv_model(lgbmc_base, param_lgbmc, 'class')","2287a21b":"DECIMALS = 3","677f3579":"def get_reg_scores(model, dec=DECIMALS):\n    preds = model.predict(x_test)\n    mae = mean_absolute_error(y_test, preds)\n    mse = mean_squared_error(y_test, preds)\n    mae_round = np.round(mae, dec)\n    mse_round = np.round(mse, dec)\n    \n    print(f'MAE: {mae_round}, MSE: {mse_round}')","91066a5d":"x = df_copy.drop('quality', axis=1).values\ny = df_copy.quality.values\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=SEED)","5f73d8df":"rfr_base = RandomForestRegressor(n_jobs=-1, random_state=SEED)\nrfr_base.fit(x_train, y_train)\n\nget_reg_scores(rfr_base)","04579ddf":"xgbr_base = xgb.XGBRegressor(n_jobs=-1, random_state=SEED)\nxgbr_base.fit(x_train, y_train)\n\nget_reg_scores(xgbr_base)","e137f7b4":"lgbmr_base = lgb.LGBMRegressor(n_jobs=-1, random_state=SEED)\nlgbmr_base.fit(x_train, y_train)\n\nget_reg_scores(lgbmr_base)","f6fd84db":"x_resampled, y_resampled = ros.fit_resample(x_train, y_train)","0469535f":"rfr_os = RandomForestRegressor(n_jobs=-1, random_state=SEED)\nrfr_os.fit(x_resampled, y_resampled)\n\nget_reg_scores(rfr_os)","0aa60512":"xgbr_os = xgb.XGBRegressor(n_jobs=-1, random_state=SEED)\nxgbr_os.fit(x_resampled, y_resampled)\n\nget_reg_scores(xgbr_os)","ce5bf750":"lgbmr_os = lgb.LGBMRegressor(n_jobs=-1, random_state=SEED)\nlgbmr_os.fit(x_resampled, y_resampled)\n\nget_reg_scores(lgbmr_os)","fd927f6a":"param_rfr = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [5, None]\n}\n\ntrain_gscv_model(rfr_base, param_rfr, 'reg')","5feb79e5":"param_xgbr = {\n    'n_estimators': [100, 200, 300]\n}\n\ntrain_gscv_model(xgbr_base, param_xgbr, 'reg')","a3eaa1bb":"param_lgbmr = {\n    'boosting_type': ['gbdt', 'dart', 'goss', 'rf'],\n    'max_depth': [5, -1],\n    'learning_rate': [.1, .001, .0001],\n    'n_estimators': [100, 200, 300],\n    'class_weight': ['balanced', None]\n}\n\ntrain_gscv_model(lgbmr_base, param_lgbmr, 'reg')","3ea96ffd":"Similar to classification task I'm gonna try these 3 models:\n1. Random Forest Resressor\n2. XGB Resressor\n3. LGBM Resressor","c02965a8":"Next step I'd tune each model's hyperparameters.","3ee5b3b0":"# 2.1 EDA","a0d9a7e6":"# 2.4 Tuning","398d07c6":"To solve this task dataset isn't need to be modified.","3f750cbd":"# 3. Regression task","4ab6b5ca":"In this notebook, I'd train both classification and regression models with different techniques and compare them. Will they be effective or not?","dd2392c8":"# 2.3 Random oversampling","73642bbd":"# 3.2 Random oversampling","bf22de90":"Well, oversampling technique has achieved decent score compare to baseline models. ","96961259":"Oversampling didn't improve base algorithms.","ddebc684":"# 2. Classification task","5fcf25e9":"As you can see without a lot of work and deep dive into the dataset I could improve classification score.\nBut this trick didn't work out with regression score cause of many outliers in features.\nSo how can you make score higher?\n1. Feature engineering (create new features)\n2. Try different models\n3. More hyperparameters tuning\n4. Preprocessing (in case of regression)","9bd6920d":"# 1. Data description\n\n**Features:**\n1. fixed acidity - most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n2. volatile acidity - the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n3. citric acid - found in small quantities, citric acid can add 'freshness' and flavor to wines\n4. residual sugar - the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram\/liter and wines with greater than 45 grams\/liter are considered sweet\n5. chlorides - the amount of salt in the wine\n6. free sulfur dioxide - the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n7. total sulfur dioxide - amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine\n8. density - the density of water is close to that of water depending on the percent alcohol and sugar content\n9. pH - describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n10. sulphates - a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant\n11. alcohol - the percent alcohol content of the wine\n12. quality - output variable (based on sensory data, score between 0 and 10)\n\n**Tips by author**\n\nWhat might be an interesting thing to do, is aside from using regression modelling, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good\/1' and the remainder as 'not good\/0'.\nThis allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value.\nWithout doing any kind of feature engineering or overfitting you should be able to get an AUC of .88 (without even using random forest algorithm).","b410dd1b":"Somehow models with tuning showed even worse score.","0b128582":"Since the dataset is imbalanced you can perform oversample technique to make our classes equal by using **RandomOverSampler**!","fbd6efed":"# 3.1 Baseline","31a64b42":"As we can see **LGBM Classifier** is the **best** here.","93b28844":"# 3.3 Tuning","08cc56ac":"I'd like to try these 3 models since they got high score in most cases:\n1. Random Forest Classifier\n2. XGB Classifier\n3. LGBM Classifier","f63504be":"# 4. What is next?","23bc0853":"Well, surprisingly, Random Forest Regressor better than other models!","77c8b310":"Thanks for reading! I hope you have found here any useful information for yourself.\n\nFeel free to comment this notebook.","e04dfb3c":"**Summarize:** in case of regression task, oversamping technique or tuning didn't improve base score at all! Furthermore *Random Forest Regressor* got the best score among other gradient boosting models. That suprised me to be honest.","9da53bd6":"According to pairplot you can clearly see that you **cannot** perform any linear model to solve this task.","001c035d":"**Summarize:** the best algorith here is *LGBM Classisifier*. Hyperparameters tuning doesn't impove score compare to oversampling technique.","3cc817a7":"# 2.2 Baseline","25627cee":"Since I will modify data, I need to copy original dataset.","b77e5ecf":"Original dataset's size is not that huge so I'd do 4-fold CV instead of common 5-fold. It may help to improve a score."}}