{"cell_type":{"eadf95d2":"code","93c97e66":"code","9b3ffb9b":"code","f6a9ceea":"code","a464ec87":"code","34da596c":"code","fb1a0e2c":"code","9b5c0f5b":"code","9b6a8338":"code","9137a6d7":"code","5e04445e":"code","bb9f4001":"code","d5c7997a":"code","d7b9ba5e":"code","4e8035af":"code","05ac1807":"code","49dd9523":"code","94f7e05c":"code","7a0c14fc":"code","3ce76967":"code","12fd2e7e":"code","d204450b":"code","d039cfd0":"code","d33e09f4":"code","acf6f8be":"code","d9da6477":"code","74060cf1":"code","20ab8ca5":"code","e4aadc0e":"code","388f68a4":"code","73003f8a":"code","91900094":"code","a464cfcd":"code","427b5aa4":"code","c5752bd9":"code","e885dc84":"code","95caaf2d":"markdown","0f31ca57":"markdown","711fd476":"markdown"},"source":{"eadf95d2":"! pip install gdown","93c97e66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9b3ffb9b":"!gdown --id 1LHo-ICoeKAOBotSv2gbWF0K7fAbrQQxq\n!gdown --id 15k6YNv4MxGt8N7LelCLfGx7xKSPPuhzD","f6a9ceea":"import gzip\nimport pickle\n\nwith open('.\/test_without_label.pkl', 'rb') as f:\n    test_set = pickle.load(f)\n\nwith open('.\/train_with_label.pkl', 'rb') as f:\n    train_set = pickle.load(f)","a464ec87":"each_sentence = []\nlabel = []\nmisspell_index_list = []\nmisspell_word_list = []\nfor dataset in train_set:\n  if len(dataset) == 2:\n      sentence = \" \".join(dataset[0])\n      each_sentence.append(sentence)\n      category = dataset[1][0][2]\n      misspell_index = dataset[1][0][0]\n      misspell_word = dataset[1][0][1]\n      label.append(category)\n      misspell_index_list.append(misspell_index)\n      misspell_word_list.append(misspell_word)\n  else:\n    print(dataset)","34da596c":"each_sentence = []\nlabel = []\nmisspell_index_list = []\nmisspell_word_list = []\nfor dataset in train_set:\n  if len(dataset) == 2:\n      sentence = \" \".join(dataset[0])\n      each_sentence.append(sentence)\n      category = dataset[1][0][2]\n      misspell_index = dataset[1][0][0]\n      misspell_word = dataset[1][0][1]\n      label.append(category)\n      misspell_index_list.append(misspell_index)\n      misspell_word_list.append(misspell_word)\n  else:\n    print(dataset)","fb1a0e2c":"misspell_sentence = []\nfor dataset in test_set:\n    misspell_sentence.append(\" \".join(dataset))","9b5c0f5b":"misspell_sentence","9b6a8338":"!pip install pythainlp\n!pip install datasets\n!pip install attacut\n!pip -q install thai2transformers==0.1.1","9137a6d7":"import numpy as np # linear algebra\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport pythainlp \nimport transformers\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nimport tqdm\nimport keras\nfrom keras.layers import *\nimport tensorflow as tf\nfrom keras.optimizers import *\nfrom tokenizers import BertWordPieceTokenizer\nfrom datasets import load_dataset\n\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\n\nfrom tensorflow.keras.callbacks  import *\n\n\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport torch\n\n#datasets\nfrom datasets import load_dataset\n\n#transformers\nfrom transformers import (\n    CamembertTokenizer,\n    AutoTokenizer,\n    AutoModel,\n    AutoModelForMaskedLM,\n    AutoModelForSequenceClassification,\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    pipeline,\n)\n\n#thai2transformers\nimport thai2transformers\nfrom thai2transformers.preprocess import process_transformers\nfrom thai2transformers.metrics import (\n    classification_metrics, \n    multilabel_classification_metrics,\n)\n\nfrom thai2transformers.tokenizers import (\n    ThaiRobertaTokenizer,\n    ThaiWordsNewmmTokenizer,\n    ThaiWordsSyllableTokenizer,\n    FakeSefrCutTokenizer,\n    SEFR_SPLIT_TOKEN\n)\n\nfrom transformers.modeling_roberta import (\n    RobertaPreTrainedModel,\n    RobertaModel,\n    RobertaConfig,\n    RobertaClassificationHead,\n)\nfrom transformers.modeling_tf_roberta import TFRobertaMainLayer","5e04445e":"print(\"Check max len\")\nmax_len = 0\nfor sentence in each_sentence:\n    if max_len < len(sentence.split()):\n        max_len = len(sentence.split())\nfor sentene in misspell_sentence:\n    if max_len < len(sentence.split()):\n        max_len = len(sentence.split())\nprint(\"Maxlen = %d words\"% max_len)","bb9f4001":"train = pd.DataFrame({\"Text\":each_sentence,\"Label\":label})","d5c7997a":"Label_dict = {\"misspelled\":0,\"morphed\":1,\"abbreviation\":2,\"ws\":3,\"other\":4,\"spoonerism\":5,\"new\":6}\ntrain.replace(Label_dict,inplace=True)","d7b9ba5e":"train = train.astype({\"Label\":int})","4e8035af":"sns.countplot(x=\"Label\",data=train)","05ac1807":"train.Label.value_counts()","49dd9523":"max_len = 92\nwangchan_tokenizer = CamembertTokenizer.from_pretrained(\"airesearch\/wangchanberta-base-att-spm-uncased\", use_fast=True)","94f7e05c":"import tensorflow as tf\nfrom transformers import TFRobertaForSequenceClassification,TFRobertaModel","7a0c14fc":"class RobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(RobertaClass, self).__init__()\n        self.l1 = RobertaModel.from_pretrained(\"airesearch\/wangchanberta-base-att-spm-uncased\")\n        self.pre_classifier = torch.nn.Linear(768, 768)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(768, 7)\n\n    def forward(self, input_ids, attention_mask):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n        pooler = torch.nn.ReLU()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output","3ce76967":"model = RobertaClass()\nmodel.to(\"cuda\")","12fd2e7e":"LEARNING_RATE =  1e-5\nloss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","d204450b":"def calcuate_accuracy(preds, targets):\n    n_correct = (preds==targets).sum().item()\n    return n_correct","d039cfd0":"train","d33e09f4":"class SentimentData():\n    def __init__(self, dataframe, tokenizer,setting, max_len = 92):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.Text\n        if setting == \"train\":\n            self.targets = self.data.Label\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True,truncation=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }\n","acf6f8be":"new_label = []\nfor i in label:\n    new_label.append(Label_dict[i])\nnew_label = np.array(new_label)","d9da6477":"from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(each_sentence,new_label, test_size=0.15, random_state=42,shuffle=True)","74060cf1":"train = pd.DataFrame({\"Text\":X_train,\"Label\":y_train})\nvalidation = pd.DataFrame({\"Text\":X_validation,\"Label\":y_validation})\ntest = pd.DataFrame({\"Text\":misspell_sentence })","20ab8ca5":"x_train = SentimentData(train,wangchan_tokenizer,\"train\")\nx_validation = SentimentData(validation,wangchan_tokenizer,\"train\")","e4aadc0e":"TRAIN_BATCH_SIZE,VALID_BATCH_SIZE = 32,32\ntrain_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(x_train, **train_params)\ntesting_loader = DataLoader(x_validation, **test_params)","388f68a4":"device=\"cuda\"\ndef train(epoch):\n    tr_loss = 0\n    n_correct = 0\n    nb_tr_steps = 0\n    nb_tr_examples = 0\n    model.train()\n    for _,data in tqdm(enumerate(training_loader, 0)):\n        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        loss = loss_function(outputs, targets)\n        tr_loss += loss.item()\n        big_val, big_idx = torch.max(outputs.data, dim=1)\n        n_correct += calcuate_accuracy(big_idx, targets)\n\n        nb_tr_steps += 1\n        nb_tr_examples+=targets.size(0)\n        \n        if _%100==0:\n            loss_step = tr_loss\/nb_tr_steps\n            accu_step = (n_correct*100)\/nb_tr_examples \n            print(f\"Training Loss per 100 steps: {loss_step}\")\n            print(f\"Training Accuracy per 100 steps: {accu_step}\")\n\n        optimizer.zero_grad()\n        loss.backward()\n        # # When using GPU\n        optimizer.step()\n\n    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)\/nb_tr_examples}')\n    epoch_loss = tr_loss\/nb_tr_steps\n    epoch_accu = (n_correct*100)\/nb_tr_examples\n    print(f\"Training Loss Epoch: {epoch_loss}\")\n    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n    \n    acc = valid(model, testing_loader)\n    print(\"Accuracy on validation data = %0.2f%%\" % acc)\n\n    return ","73003f8a":"device = \"cuda\"","91900094":"def valid(model, testing_loader):\n    model.eval()\n    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(testing_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n            outputs = model(ids, mask).squeeze()\n            loss = loss_function(outputs, targets)\n            tr_loss += loss.item()\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n            n_correct += calcuate_accuracy(big_idx, targets)\n\n            nb_tr_steps += 1\n            nb_tr_examples+=targets.size(0)\n            \n            if _%100==0:\n                loss_step = tr_loss\/nb_tr_steps\n                accu_step = (n_correct*100)\/nb_tr_examples\n                print(f\"Validation Loss per 100 steps: {loss_step}\")\n                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n    epoch_loss = tr_loss\/nb_tr_steps\n    epoch_accu = (n_correct*100)\/nb_tr_examples\n    print(f\"Validation Loss Epoch: {epoch_loss}\")\n    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n    \n    return epoch_accu","a464cfcd":"EPOCHS = 5\nfor epoch in range(EPOCHS):\n    train(epoch)","427b5aa4":"PATH = \"\/kaggle\/working\/model.h5\"\ntorch.save(model.state_dict(), PATH)","c5752bd9":"model.load_state_dict(torch.load(PATH))","e885dc84":"model","95caaf2d":"70% of the data is misspell ","0f31ca57":"Training data","711fd476":"Check class distribution"}}