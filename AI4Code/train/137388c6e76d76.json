{"cell_type":{"b95fe7c8":"code","a2d62c10":"code","37c21a10":"code","d7e3c641":"code","4b09ad9c":"code","74e397fd":"code","09e20c96":"code","ecab8621":"code","b778bfa4":"code","6ac88101":"code","ded16f66":"code","2c264da0":"code","d27b4996":"code","a9240a39":"code","fcf7bb09":"code","6b090c9c":"code","94b47f99":"code","723db536":"code","a8b517c9":"code","a5ae47e3":"code","6c8b0ba5":"code","b8ab5486":"code","080da6b2":"code","1a6a75c5":"code","904e2065":"code","c8547a12":"code","bc662aa6":"code","2329d181":"code","04dabc38":"code","b4ab2704":"code","9b8c3391":"code","953fc3f3":"code","fde4f4fa":"code","cd91a021":"code","25355f62":"code","7456c5cb":"code","ee0223ae":"code","127c9059":"code","84a89104":"code","4dc15672":"code","2525b34b":"code","d7b92f5b":"code","0793f3d7":"markdown","dad36d7a":"markdown","eb5b04b6":"markdown","84d0a255":"markdown","7e840cd0":"markdown","97275bbb":"markdown","1f483d7f":"markdown","6d3b08c0":"markdown","e242edc6":"markdown","30729785":"markdown","4bfc9f99":"markdown","3ff660de":"markdown","12f90624":"markdown","a0abe4ca":"markdown","5e2e13f3":"markdown","9a613cc9":"markdown","a62b1a3d":"markdown","24cfbbe0":"markdown","0733db3e":"markdown","16a37926":"markdown","04fdf9e5":"markdown","d08e944f":"markdown","7940716f":"markdown","36caec46":"markdown","c5d8e113":"markdown","1139f28d":"markdown","f5ba8185":"markdown","208f00ea":"markdown","8a643bab":"markdown","45423722":"markdown","c3190286":"markdown","10b14930":"markdown","c9cd022e":"markdown","a7366bdd":"markdown","90b4cc9b":"markdown","9f3612dc":"markdown","500e6173":"markdown","3d17a04c":"markdown","836dabae":"markdown","bf46b749":"markdown","8c35c39b":"markdown","031d1d54":"markdown","79c056c1":"markdown"},"source":{"b95fe7c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2d62c10":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","37c21a10":"dataset = pd.read_csv ('..\/input\/breast-cancer-wisconsin-data\/data.csv')","d7e3c641":"dataset.shape","4b09ad9c":"dataset.columns","74e397fd":"dataset.info ()","09e20c96":"dataset.head ()","ecab8621":"dataset.drop ('id', axis = 1, inplace = True)\ndataset.drop ('Unnamed: 32', axis = 1, inplace = True)","b778bfa4":"dataset.shape ","6ac88101":"dataset.head ()","ded16f66":"dataset.isna ()","2c264da0":"dict = {}\nfor i in list(dataset.columns):\n    dict[i] = dataset[i].value_counts().shape[0]\n\npd.DataFrame(dict,index=[\"unique count\"]).transpose()","d27b4996":"from sklearn.preprocessing import LabelEncoder\nlabelencoder_Y = LabelEncoder()\ndataset.diagnosis = labelencoder_Y.fit_transform(dataset.diagnosis)\ndataset.head (10)","a9240a39":"df = pd.DataFrame (dataset, columns = ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',\t'perimeter_se',\t'area_se',\t'smoothness_se',\t'compactness_se',\t'concavity_se',\t'concave points_se',\t'symmetry_se',\t'fractal_dimension_se',\t'radius_worst',\t'texture_worst',\t'perimeter_worst',\t'area_worst',\t'smoothness_worst',\t'compactness_worst',\t'concavity_worst',\t'concave points_worst',\t'symmetry_worst',\t'fractal_dimension_worst'])\ndf.corr ()","fcf7bb09":"corr_Matrix = df.corr ()\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap (corr_Matrix, linewidths = 0.5, annot = True, fmt= '.1f',ax=ax)\nplt.show ()","6b090c9c":"label = []\nfor i in range (30):\n  if corr_Matrix.diagnosis[i+1]<0.5 or i>=10 :\n    label.append (dataset.columns.values[i+1])\ndataset.drop (labels = label, axis = 1, inplace = True)\ndataset.head ()","94b47f99":"sns.pairplot(dataset, hue = \"diagnosis\")\nplt.show()","723db536":"sns.countplot (x = 'diagnosis',data = dataset)\nplt.show ()","a8b517c9":"# X = Matrix of Features\n# Y = Response Feature\n\nX = dataset.iloc [:, 1:].values\nY = dataset.iloc [:, 0].values\nX.shape","a5ae47e3":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.25, random_state = 1)","6c8b0ba5":"X_train.shape","b8ab5486":"Y_train.shape","080da6b2":"X_test.shape","1a6a75c5":"Y_test.shape","904e2065":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler ()\nX_train = sc.fit_transform (X_train)\nX_test = sc.transform (X_test)","c8547a12":"print (X_train [:5, :])","bc662aa6":"print (X_test [:5, :])","2329d181":"from sklearn.metrics import confusion_matrix, accuracy_score","04dabc38":"from sklearn.linear_model import LogisticRegression \nclassifier_log = LogisticRegression ()\nclassifier_log.fit (X_train, Y_train)\nY_pred_log = classifier_log.predict (X_test)\ncm_log = confusion_matrix (Y_test, Y_pred_log)\nacc_log = accuracy_score (Y_test, Y_pred_log)","b4ab2704":"from sklearn.neighbors import KNeighborsClassifier\nclassifier_knn = KNeighborsClassifier ()\nclassifier_knn.fit (X_train, Y_train)\nY_pred_knn = classifier_knn.predict (X_test)\ncm_knn = confusion_matrix (Y_test, Y_pred_knn)\nacc_knn = accuracy_score (Y_test, Y_pred_knn)","9b8c3391":"from sklearn.naive_bayes import GaussianNB\nclassifier_nb = GaussianNB ()\nclassifier_nb.fit (X_train, Y_train)\nY_pred_nb = classifier_nb.predict (X_test)\ncm_nb = confusion_matrix (Y_test, Y_pred_nb)\nacc_nb = accuracy_score (Y_test, Y_pred_nb)","953fc3f3":"from sklearn.svm import SVC\nclassifier_svm = SVC (kernel = 'rbf', random_state = 0)\nclassifier_svm.fit (X_train, Y_train)\nY_pred_svm = classifier_svm.predict (X_test)\ncm_svm = confusion_matrix (Y_test, Y_pred_svm)\nacc_svm = accuracy_score (Y_test, Y_pred_svm)","fde4f4fa":"from sklearn.tree import DecisionTreeClassifier\nclassifier_dtc = DecisionTreeClassifier (criterion = 'entropy', random_state = 0)\nclassifier_dtc.fit (X_train, Y_train)\nY_pred_dtc = classifier_dtc.predict (X_test)\ncm_dtc = confusion_matrix (Y_test, Y_pred_dtc)\nacc_dtc = accuracy_score (Y_test, Y_pred_dtc)","cd91a021":"from sklearn.ensemble import RandomForestClassifier\nclassifier_rfc = RandomForestClassifier (n_estimators = 100, criterion = 'entropy', random_state = 1)\nclassifier_rfc.fit (X_train, Y_train)\nY_pred_rfc = classifier_rfc.predict (X_test)\ncm_rfc = confusion_matrix (Y_test, Y_pred_rfc)\nacc_rfc = accuracy_score (Y_test, Y_pred_rfc)","25355f62":"prediction_columns = [\"NAME OF MODEL\", \"ACCURACY SCORE\"]\ndf_pred = {\"NAME OF MODEL\" : [\"LOGISTIC REGRESSION\", \"K-NN\", \"NAIVE BAYES\", \"SVM\", \"DECISION TREE\", \"RANDOM FOREST\"],\n           \"ACCURACY SCORE \" : [acc_log, acc_knn, acc_nb, acc_svm, acc_dtc, acc_rfc]}\ndf_predictions = pd.DataFrame (df_pred)\ndf_predictions","7456c5cb":"from sklearn.model_selection import GridSearchCV","ee0223ae":"parameters = [{'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}]\ngrid_search = GridSearchCV(estimator = classifier_log,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_log = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_accuracy_log)\nprint(best_parameters)","127c9059":"parameters = [{'n_neighbors': [3,5,7,10,13,15], 'weights': ['uniform', 'distance'],\n                'p': [1,2]}]\ngrid_search = GridSearchCV(estimator = classifier_knn,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_knn = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_accuracy_knn)\nprint(best_parameters)","84a89104":"parameters = [{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'rbf'],\n                'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = classifier_svm,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_svm = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_accuracy_svm)\nprint(best_parameters)","4dc15672":"parameters = [{'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150], \n                'max_leaf_nodes': [2,4,6,10,15,30,40,50,100], 'min_samples_split': [2, 3, 4]}]\ngrid_search = GridSearchCV(estimator = classifier_dtc,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_dtc = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_accuracy_dtc)\nprint(best_parameters)","2525b34b":"parameters = [{'n_estimators': [100,200,300],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [10,25,50,'none'],\n               'min_samples_leaf': [1, 2], \n               'min_samples_split': [2, 5]}]\ngrid_search = GridSearchCV(estimator = classifier_rfc,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_rfc = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_accuracy_rfc)\nprint(best_parameters)","d7b92f5b":"prediction_columns = [\"NAME OF MODEL\", \"ACCURACY SCORE\", \"BEST ACCURACY (AFTER HYPER-PARAMETER TUNING)\"]\ndf_pred = {\"NAME OF MODEL\" : [\"LOGISTIC REGRESSION\", \"K-NN\", \"NAIVE BAYES\", \"SVM\", \"DECISION TREE\", \"RANDOM FOREST\"],\n           \"ACCURACY SCORE \" : [acc_log, acc_knn, acc_nb, acc_svm, acc_dtc, acc_rfc],\n           \"BEST ACCURACY (AFTER HYPER-PARAMETER TUNING)\" : [best_accuracy_log, best_accuracy_knn, \"-\", best_accuracy_svm, best_accuracy_dtc, best_accuracy_rfc]}\ndf_predictions = pd.DataFrame (df_pred)\ndf_predictions","0793f3d7":"# HYPER-PARAMETER TUNING\n\nWe wil now try to boost our model algorithms and see whether is it possible for us to achieve any increase in the accuracy  scores by making any changes in the parameter values. The technique that we will be using provides us with the optimum parameter values using which we can get the maximum accuracy possible. Also, there is the possibilty that a new model is found to have the highest accuracy after the paramter tuning is done.","dad36d7a":"### ENCODING THE CATEGORICAL VARIABLE\n\nTo ensure that the entire dataset is of a continuous numerical form, we will be encoding the categorial variable **DIAGNOSIS** and converting into a numerical form, preferably into 0s and 1s.\n\nFor this, we will be making use of the **LabelEncoder** class from the **Preprocessing** module of the **Sklearn** library","eb5b04b6":"### DECISION TREE MODEL","84d0a255":"# BASIC VISUALISATION OF DATASET\n\nAfter doing a theoretical analysis in the previous section, we will be moving on to visual analysis of the dataset. This will include quite a number of scatter plots, bar plots, etc. between the different features and how they affect one other and how they will affect the working algorithm. We will also be getting a general idea about which features will play a more active role while determining the accuracy of the model.","7e840cd0":"### SPLITTING DATASET INTO DEPENDENT AND INDEPENDENT VARIABLES\n\nNow finally we will be splitting the updated dataset we have into two parts. The first is a collection of the independent variables and is called the **MATRIX OF FEATURES**. The other is a collection of the dependent variables and is known as **RESPONSE FEATURE**.","97275bbb":"### DECISION TREE MODEL","1f483d7f":"### RANDOM FOREST MODEL","6d3b08c0":"### RANDOM FOREST MODEL","e242edc6":"### REMOVING UNNEEDED FEATURES\n\nThe columns **id** and **Unnamed: 32** don't play any role in enhancing the accuracy of the model and hence, are redundant and we can drop them from the dataset.","30729785":"### CHECKING THE NUMBER OF UNIQUE VALUES\n\nNext we will be checking how many unique values does each feature have, in order to get a much better understanding of the dataset we are working on.","4bfc9f99":"### FEATURE SCALING\n\nIn any dataset, there could be features that dominate over others while evaluating the accuracy. We don't want that. We want all features to have a more or less equal say in deciding the accuracy. Also if a feature in the dataset is big in scale compared to others then in algorithms where Euclidean Distance is measured this big scaled feature becomes dominating and needs to be normalized.\n\nFor this we'll be using one of the most used feature scaling method there is, **STANDARD SCALER**. This method assumes your data to be normally distributed within each feature and scales them in such a way that the distribution becomes centred around **0** with a standard deviation of **1**.","3ff660de":"### K-NN MODEL","12f90624":"### SVM MODEL","a0abe4ca":"# EXPLORATORY ANALYSIS OF DATASET\n\nIn this section, we will be performing some basic operations on the dataset in order to analyse the data as a s]whole. For example, we will checking out the size of the dataset, what are the different features, what are the input types of the features, etc. to name a few.","5e2e13f3":"### NAIVE BAYES MODEL","9a613cc9":"# DATA PREPROCESSING\n\nThis section involves transforming the raw data that we have into a more understandable format for the algorithm to process. This is done so that the data which we will be feeding into the algorithm is not garbage and we don't get false predictions in return. This includes techniques like normalization, scaling of features, encoding of data, splitting the data into training and test sets, etc.","a62b1a3d":"### COUNTPLOT VISUAL\n\nThe count plot will give us a more clearer picture regarding the actual number of data points for each diagnosis.\n\nIn the plot below, **0** corresponds to **BENIGN** and **1** corresponds to **MALIGNANT**.","24cfbbe0":"### SPLITTING THE MATRIX OF FEATURES AND RESPONSE FEATURE INTO TRAINING AND TEST SETS\n\nAs the names suggest, the model algorithms are trained using the **TRAINING SET** and then the model algorithms apply their learnings from training onto the **TEST SET** to get the predicted values which are then compared to the actual values to get the accuracy.\n\nUsually this division is done in a range of a 60-40 split to a 80-20 split depending on the size of the original dataset. ","0733db3e":"From the result of the above function we can see that we have only 1 categorical data feature and the rest are continuous data features. ","16a37926":"### LOGISTIC REGRESSION MODEL","04fdf9e5":"From the table above it is fairly evident that the **SUPPORT VECTOR MACHINE** has the highest accuracy score of **0.923077 (92.30%)** for our dataset.","d08e944f":"### NAIVE BAYES MODEL\n\nThe naive bayes algorithm doesn't have any hyper-parameter to tune, so we have nothing to perform grid search over.","7940716f":"# ABOUT DATASET\n\nAny tumour (abnormal growth of cells) in the human body can be broadly classified into two types - **Benign** (non-cancerous cell growth) and **Malignant** (cancerous cell growth). This dataset is a collection of all those patients whose bodies were examined to have a tumour and have been classified to be either \"Benign\" or \"Malignant\" on the basis of a collection of features specific to the cell growth like radius of cells, surface area of growth, etc.\n\n# ABOUT NOTEBOOK\n\nThis notebook is a means by which I have tried to create an algorithm to replicate the best accuracy possible while predicting the nature of the tumour present. For this, I will be comparing the results of the different classification algorithms for best accuracy. We will also be applying the different techniques for dimensionality reduction and analysing its effects on the accuracy of the model.","36caec46":"### FORMING FINAL WORKING DATASET\n\nNow that we have identified the key features that will play the major role while making predictions, we are going to drop the rest of the features from the dataset.","c5d8e113":"In the above plots, **0** corresponds to **BENIGN** and **1** corresponds to **MALIGNANT**.\n\nThe visuals show a trend that has been followed throughout each plot. At lower values of the features, the diagnosis is predominantly **BENIGN** and at higher values, **MALIGNANT** has been the chief diagnosis. \n\nNot much can be said about the number of data points for each type of diagnosis but it does appear like the **BENIGN** data points are more compactly situated as compared to the scattered **MALIGNANT** data points.","1139f28d":"### ACCURACY COMPARISON","f5ba8185":"To conclude this notebook, it is fairly evident that it is the **SUPPORT VECTOR MACHINE** model that has come out triumphant with the highest accuracies both before and after the hyper-parameter tuning. It ended up with an accuracy of **0.923077 (92.37%)** before hyper-parameter tuning and **0.936434 (93.64%)** after and is hence, the best suited model out of the rest for the given dataset.","208f00ea":"# MODEL IMPLEMENTATIONS\n\nIn this section we will be training our data onto the different classification models that we have like K-NN, Logistic Regression, Naive Bayes, etc. and calculating the model accuracy for each algorithm to see which would be best suited for our dataset.\n\nAlso, we will be optimizing our model using methods which could result in a new classification algorithm having the best accuracy.","8a643bab":"### LOGISTIC REGRESSION MODEL","45423722":"From the above heatmap we can infer a few things :-\n\n1. The **\"_se\"** features are very weakly co-related (ranging from 0.0 to 0.5) with the response variable **DIAGNOSIS** and we won't be considering them for our final working dataset.\n\n2. The **\"_mean\"** and **\"_worst\"** features apart from being very strongly co-related to the response variable **DIAGNOSIS** are also very strongly co-related to their corresponding selves. For example, **\"radius_mean\"** has a correlation of 1.0 with **\"radius_worst\"**. This implies that we don't need to consider both **\"_mean\"** and **\"_worst\"** features altogether but we can make use of either set. For this notebook, we will be making use of the **\"_mean\"** features.\n\nFrom the **\"_mean\"** features we will be selecting only those which have a correlation of 0.5 and above with the response variable **DIAGNOSIS**.","c3190286":"From the above table, it is clearly visible that the **DIAGNOSIS** feature is taking 0s and 1s as values.","10b14930":"### PAIRPLOT VISUALS\n\nNext is a visual representation of how the remaining features apart from the response feature **DIAGNOSIS** are related to each other.","c9cd022e":"From the count plot, it can be easily inferred that there are more **BENIGN** diagnosed data points than **MALIGNANT** diagnosed data points.","a7366bdd":"### LIBRARIES","90b4cc9b":"### DATASET","9f3612dc":"### SVM MODEL","500e6173":"### CORRELATION MATRIX AND HEATMAP\n\nThe **Correlation Matrix** as the name suggests is a matrix which shows us how each feature variable of the dataset is co-related to each other.\n\nThe **Heatmap** is a more visually pleasing way to show the relationships between features.","3d17a04c":"Since all the entries of the **isna ()** function are **false**, we can conclude that there is no missing data in the dataset.","836dabae":"### K-NN MODEL","bf46b749":"# FINAL ACCURACIES AFTER HYPER-PARAMETER TUNING","8c35c39b":"# CONCLUSION","031d1d54":"### CHECKING FOR MISSING DATA\n\nNext we need to check for any missing data that might be present in the dataset. For this, we will be using the **isna ()** function of the **Pandas** library","79c056c1":"# IMPORTING THE LIBRARIES AND DATASET"}}