{"cell_type":{"c018e421":"code","efb9fdd5":"code","1178f855":"code","375545d7":"code","c71e8d0e":"code","92a19608":"code","159a0642":"code","244f686e":"code","c04d2462":"code","103a155a":"code","375416d9":"code","553f23f3":"code","266ac928":"code","21a6513e":"code","0eb5ed85":"code","f8b1bee6":"code","395de7f4":"code","85413df9":"code","5906789e":"code","e7a92d02":"code","2da02b0f":"code","1ab0da3c":"code","ae67f42c":"code","d808cf9d":"code","7d456bd6":"code","7d01dbdb":"code","9d247ba7":"code","20cf94a9":"code","1afd168b":"code","bf2ef484":"code","66b020b0":"code","8362162b":"code","27e48e99":"code","3486239a":"code","53ded4f1":"code","0f128ee1":"code","8a00b812":"code","1e4b7f40":"code","e7d81494":"code","7285c92f":"code","4aa38367":"code","4f5dd97d":"code","1c2566d9":"code","dc3c8f0c":"code","d38c9926":"code","3089ae58":"code","87d7289c":"code","a92a99de":"code","d40cebe4":"code","cb86e504":"code","64d3a3da":"markdown","981d37aa":"markdown","3cdb27c4":"markdown","7a91186b":"markdown","464cb521":"markdown","86599410":"markdown","2d58ecb0":"markdown","b360a35e":"markdown","c3ef617d":"markdown","185c014e":"markdown","674b8bdf":"markdown","b983b328":"markdown","c52feeca":"markdown","0dc7edfd":"markdown","290aeed8":"markdown","c9ac3c77":"markdown","29c2d12b":"markdown","62bbd678":"markdown","f37ab8ba":"markdown","0e118484":"markdown","38a0c223":"markdown","9f5fb9e8":"markdown","6abdafc7":"markdown","f650d7dc":"markdown","77d8d5b6":"markdown","bc20a072":"markdown","b915d715":"markdown","b7f99174":"markdown","6f266161":"markdown","f67cf74e":"markdown","6c3dcaa9":"markdown","2c8c8eda":"markdown","9695f9fe":"markdown","79162413":"markdown","5c352595":"markdown","ee9bb89a":"markdown","fe487665":"markdown","35557ed6":"markdown","186deb6f":"markdown","9115a5e4":"markdown","6aa09596":"markdown","8b78e4ef":"markdown","5b745f51":"markdown","b639e42e":"markdown","59292af4":"markdown","1cb209f4":"markdown"},"source":{"c018e421":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly as py\nimport plotly.graph_objs as go\nfrom sklearn.cluster import KMeans\n%matplotlib inline\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport warnings\nimport os\nwarnings.filterwarnings(\"ignore\")\npy.offline.init_notebook_mode(connected = True)","efb9fdd5":"url = \"https:\/\/dota2.gamepedia.com\/Table_of_hero_attributes\"\nhtml = urlopen(url)","1178f855":"soup = BeautifulSoup(html, 'lxml')\ntype(soup)","375545d7":"title = soup.title\nprint(title)","c71e8d0e":"rows = soup.find_all('tr')\n\nall_rows = []\nfor row in rows:\n    row_td = row.find_all('td')\n    row_td = str(row_td)\n    row_td = BeautifulSoup(row_td, \"lxml\").get_text()\n    row_td = row_td.replace(\"\\r\",\"\")\n    row_td = row_td.replace(\" \",\"\")\n    row_td = row_td.replace(\"\\n\",\"\")\n    all_rows.append(row_td)\n\nprint(all_rows[:10])","92a19608":"contents = pd.DataFrame(all_rows)\ncontents.head(10)","159a0642":"contents = contents[0].str.split(',', expand=True)\ncontents.head(10)","244f686e":"contents[0] = contents[0].str.strip('[]')\ncontents.head(10)","c04d2462":"contents[28] = contents[28].str.strip('[]')\ncontents.head(10)","103a155a":"col_labels = soup.find_all('th')\nall_header = []\ncol_str = str(col_labels)\ncol_str = col_str.replace(\"\\r\",\"\")\ncol_str = col_str.replace(\" \",\"\")\ncol_str = col_str.replace(\"\\n\",\"\")\ncol_str = col_str.replace(\"[\",\"\")\ncol_str = col_str.replace(\"]\",\"\")\ncleantext2 = BeautifulSoup(col_str, \"lxml\").get_text()\nall_header.append(cleantext2)\nprint(all_header)","375416d9":"headers = pd.DataFrame(all_header)\nheaders.head()","553f23f3":"headers = headers[0].str.split(',',expand=True)\nheaders.head()","266ac928":"frames = [headers,contents]\nfinal = pd.concat(frames)\nfinal.head()","21a6513e":"final = final.rename(columns=final.iloc[0])\nfinal.head()","0eb5ed85":"unnecessary = list(range(120,136))\nprint(unnecessary)","f8b1bee6":"#Dropping unnecessary rows\nunnecessary = list(range(120,136))\nfinal.drop(0, axis = 0, inplace = True) #inplace = True to overwrite\nfinal.drop(unnecessary, axis = 0, inplace = True)\n#Dropping unnecesasry columns\nfinal.drop(final.columns[29], axis = 1, inplace = True) ","395de7f4":"final.head()","85413df9":"### Getting the MAINATT (because MAINATT is on images in the website) #CHEATSHEET - https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/\nMAINATT = []\nmain_attribute = soup.select('table tr td:nth-of-type(2) > a')\nfor i in main_attribute:\n    MAINATT.append(i[\"title\"])\n\nfinal.drop(\"A\", axis = 1, inplace = True)\nfinal['MAINATT'] = MAINATT\nfinal.head()","5906789e":"final.tail()","e7a92d02":"final.to_csv('dota2.csv', index = False)\ndf = final","2da02b0f":"df.info()","1ab0da3c":"df['MAINATT'] = df['MAINATT'].map({'Intelligence':0, 'Strength':1, 'Agility': 2})\nprint(df)","ae67f42c":"df.drop('HERO', axis =1, inplace = True)\ndf[df.columns] = df[df.columns].astype('float')\ndf['MAINATT'] = df['MAINATT'].astype('int')\ndf['MAINATT'] = df['MAINATT'].astype('category')\ndf.info()","d808cf9d":"colors = {0: \"#4CA9FF\", 1: \"#FF4343\", 2: \"#2FFE64\"}\ndf1 = df.loc[:, 'STR':'INT+']\ndf1['MAINATT'] = df['MAINATT']\nsns.pairplot(df1,palette = colors, hue = 'MAINATT')","7d456bd6":"df2 = df.loc[:, 'INT30':'DMG(MAX)']\ndf2['MAINATT'] = df['MAINATT']\nsns.pairplot(df2,palette = colors, hue = 'MAINATT')","7d01dbdb":"#Commenting out because notebook size limit is reached\n#sns.distributions._has_statsmodels = False\n#df3 = df.loc[:, 'RG':'ATKBS']\n#df3['MAINATT'] = df['MAINATT']\n#sns.pairplot(df3,palette = colors, hue = 'MAINATT',diag_kind = \"kde\")","9d247ba7":"sns.distributions._has_statsmodels = True\nplt.rcParams['figure.figsize'] = (15.0, 15.0)\nplt.title(\"Correlation Plot\")\nsns.heatmap(df.corr())","20cf94a9":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\ndf_numeric = add_constant(df)\nVIF_frame = pd.Series([variance_inflation_factor(df_numeric.values, i) \n               for i in range(df_numeric.shape[1])], \n              index=df_numeric.columns).to_frame()\n\nVIF_frame.drop('const', axis = 0, inplace = True) \nVIF_frame.rename(columns={VIF_frame.columns[0]: 'VIF'},inplace = True)\nVIF_frame[~VIF_frame.isin([np.nan, np.inf, -np.inf]).any(1)]","1afd168b":"df.drop('DMG(MIN)', axis = 1, inplace = True)\ndf.drop('DMG(MAX)', axis = 1, inplace = True)","bf2ef484":"dv = pd.DataFrame(df['MAINATT'])\ndf.drop('MAINATT',axis=1,inplace=True)\ndf.info()","66b020b0":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf = pd.DataFrame(scaler.fit_transform(df), \n                                      index = df.index,\n                                      columns = df.columns)\ndf.head()","8362162b":"from sklearn.decomposition import PCA\npca = PCA().fit(df)\nd = {'Proportion of Variance':pca.explained_variance_ratio_.tolist(),\n    'Cumulative Proportion':np.cumsum(pca.explained_variance_ratio_).tolist()\n    }\ni = range(1,len(pca.explained_variance_ratio_.tolist())+1)\npc_no = ['PC ' + str(e) for e in i]\nImpOfComp = pd.DataFrame(d,index = pc_no)\nImpOfComp.style.format(\"{:.3f}\")","27e48e99":"plt.figure(figsize=(15,6)) #Adjust as necessary\nsns.set_style(\"whitegrid\")\nax = sns.lineplot(x = range(1,len(pc_no)+1), y = \"Cumulative Proportion\", data=ImpOfComp)\nax.set(xlabel='Number of Principal Components', ylabel='Cumulative Explained Variance')\nplt.rcParams[\"axes.labelsize\"] = 15","3486239a":"i = range(1,len(pca.explained_variance_ratio_.tolist())+1)\npc_no = ['PC ' + str(e) for e in i]\nlc = pd.DataFrame(pca.components_,columns = df.columns,index = pc_no)\nfinal_lc = lc[0:13]\nfinal_lc","53ded4f1":"n_pca = PCA(n_components = 13)\nx_pca = n_pca.fit_transform(df)\ndf_pca = pd.DataFrame(x_pca,columns=final_lc.index)\ndf_pca.head(12)","0f128ee1":"pcomps = abs(pd.DataFrame(n_pca.components_,columns=df.columns))\nplt.figure(figsize=(15,8))\nsns.heatmap(pcomps,cmap='YlGnBu')","8a00b812":"X1 = df_pca.iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state = 823, algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('WCSS')\nplt.title('Elbow Method Diagram')\nplt.show()","1e4b7f40":"algorithm = (KMeans(n_clusters = 3 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 823, algorithm='elkan') )\nalgorithm.fit(X1)\nlabels = algorithm.labels_\ncentroids = algorithm.cluster_centers_\ncentroids","e7d81494":"df['Cluster'] = labels\ndf_pca['Cluster'] = labels\n##### Mapping the values to match the groups of original dataset (0 - Int, 1 - Str , 2 - Agi)\ndf_pca['Cluster'] = df_pca['Cluster'].map({0:1, 2:0, 1:2})\ndf_pca.head()","7285c92f":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state = 823)\ndf_dv = df_pca.copy()\ndf_dv.drop('Cluster', axis = 1, inplace = True)\nrfc.fit(df_dv,df['Cluster'])\nfeatures = df_dv.columns.tolist()\nfeature_value = rfc.feature_importances_\nd = {'Features' : features, 'Values' : feature_value}\nfi = pd.DataFrame(d).sort_values('Values', ascending = False).reset_index()\nfi\nplt.rcParams['figure.figsize'] = (20.0, 5.0)\nax = sns.barplot(x=fi['Features'], y = fi['Values'], data = fi, palette=\"Greens_d\")","4aa38367":"sns.pairplot(df_pca[['PC 1','PC 2','PC 4','Cluster']], palette = colors ,hue='Cluster');","4f5dd97d":"fig, axs = plt.subplots(ncols=4,nrows=2, figsize = (20,10))\nsns.boxplot(x = \"Cluster\", y = \"PC 1\", palette = colors, data = df_pca, ax=axs[0][0])\nsns.stripplot(x='Cluster',y='PC 1', palette = colors, data = df_pca, jitter=True, ax=axs[0][0])\n\nsns.boxplot(x=\"Cluster\", y=\"PC 2\", palette = colors, data = df_pca, ax=axs[0][1])\nsns.stripplot(x='Cluster',y='PC 2', palette = colors, data = df_pca, jitter=True, ax=axs[0][1])\n\nsns.boxplot(x=\"Cluster\", y=\"PC 4\", palette = colors, data = df_pca, ax=axs[0][2])\nsns.stripplot(x='Cluster',y='PC 4', palette = colors, data = df_pca, jitter=True, ax=axs[0][2])\n\nsns.boxplot(x=\"Cluster\", y=\"PC 10\", palette = colors, data = df_pca, ax=axs[0][3])\nsns.stripplot(x='Cluster',y='PC 10', palette = colors, data = df_pca, jitter=True, ax=axs[0][3])\n\nsns.violinplot(x=\"Cluster\", y=\"PC 1\", palette = colors, data = df_pca, ax=axs[1][0])\nsns.violinplot(x=\"Cluster\", y=\"PC 2\", palette = colors, data = df_pca, ax=axs[1][1])\nsns.violinplot(x=\"Cluster\", y=\"PC 4\", palette = colors, data = df_pca, ax=axs[1][2])\nsns.violinplot(x=\"Cluster\", y=\"PC 10\", palette = colors, data = df_pca, ax=axs[1][3])","1c2566d9":"df_pca_grouped = df_pca.groupby('Cluster',as_index=False).mean()\ndf_pca_grouped = df_pca_grouped[['PC 1','PC 2','PC 4','PC 10','Cluster']]\ndf_pca_grouped","dc3c8f0c":"# Libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom math import pi\n \ncategories = ['PC 1','PC 2','PC 4','PC 10']\nN = len(categories)\n \nangles = [n \/ float(N) * 2 * pi for n in range(N)]\nangles += angles[:1]\n \nax = plt.subplot(111, polar=True)\n\nax.set_theta_offset(pi \/ 2)\nax.set_theta_direction(-1)\n \nplt.xticks(angles[:-1], categories)\n \nax.set_rlabel_position(0)\nplt.yticks([-3,-1.5,0,1.5,3], [\"-3\",\"-1.5\",\"0\",\"1.5\",\"3\"], color=\"grey\", size=7)\nplt.ylim(-3,3)\n\nvalues=df_pca_grouped.loc[0].drop('Cluster').values.flatten().tolist()\nvalues += values[:1]\nax.plot(angles, values, linewidth=1, linestyle='solid', label=\"Cluster 0\")\nax.fill(angles, values, 'b', alpha=0.2, color = \"#4CA9FF\")\n \nvalues=df_pca_grouped.loc[1].drop('Cluster').values.flatten().tolist()\nvalues += values[:1]\nax.plot(angles, values, linewidth=1, linestyle='solid', label=\"Cluster 1\")\nax.fill(angles, values, 'r', alpha=0.5, color = \"#FF4343\")\n\nvalues=df_pca_grouped.loc[2].drop('Cluster').values.flatten().tolist()\nvalues += values[:1]\nax.plot(angles, values, linewidth=1, linestyle='solid', label=\"Cluster 1\")\nax.fill(angles, values, 'r', alpha=0.2, color = \"#2FFE64\")\n \n# Add legend\nplt.rcParams['figure.figsize'] = (10.0, 10.0)","d38c9926":"data = go.Scatter3d(\n    x= df_pca['PC 1'],\n    y= df_pca['PC 2'],\n    z= df_pca['PC 4'],\n    mode='markers',\n     marker=dict(\n        color= df_pca['Cluster'],\n        size= 18,\n        opacity=0.8,\n        colorscale = 'Geyser'\n     )\n)\ndata = [data]\nlayout = go.Layout(\n    title= 'Clusters',\n    scene = dict(\n            xaxis = dict(title  = 'PC 1'),\n            yaxis = dict(title  = 'PC 2'),\n            zaxis = dict(title  = 'PC 4')\n        )\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)","3089ae58":"df_original = pd.read_csv('.\/dota2.csv',sep = \",\", header = 0)","87d7289c":"df_original['MAINATT']","a92a99de":"df_pca['Cluster'] = df_pca['Cluster'].map({1:\"Strength\", 0:\"Intelligence\", 2:\"Agility\"})\ndf_pca['Cluster']","d40cebe4":"from sklearn.metrics import classification_report,confusion_matrix\ndata = confusion_matrix(df_original['MAINATT'], df_pca['Cluster'])\ndf_cm = pd.DataFrame(data, columns=np.unique(df_original['MAINATT']), index = np.unique(df_original['MAINATT']))\ndf_cm.index.name = 'Predicted'\ndf_cm.columns.name = 'Actual'\nplt.figure(figsize = (11,8))\nsns.set(font_scale=1.5)\nax = sns.heatmap(df_cm,cmap = 'Purples', annot=True,annot_kws={\"size\": 16})# font size\nax.set_title('Confusion Matrix')\n#For cmaps:\n#https:\/\/matplotlib.org\/3.1.1\/gallery\/color\/colormap_reference.html\nprint(\"Classification Report: \")\nprint(classification_report(df_original['MAINATT'], df_pca['Cluster']))","cb86e504":"df_pca['Original'] = df_original['MAINATT']\ndf_compare = df_pca.loc[:, 'Cluster':'Original']\ndf_compare['HERO'] = df_original['HERO']\ndf_compare.loc[df_compare['Cluster'] != df_compare['Original']]","64d3a3da":"##### Pairplots using the top 3 important features for the clusters","981d37aa":"##### The principal components are expressed as a linear combination of your original features. The higher the absolute value of the coeffcient, the more important it is. How large is important is up to the specialized knowledge of the data scientist. Let's visualize the feature importance through a heatmap.","3cdb27c4":"##### Radar Chart","7a91186b":"### Initial Exploratory Data Analysis & Visualization","464cb521":"### In this notebook, web scraping is demonstrated using beautiful soup. Afterwards, Principal Component Analysis and K-Means Clustering is performed. We do these with the Dota2 heroes dataset by looking at their attributes. If you find this notebook helpful or interesting, please upvote :)","86599410":"##### In the plot of PC 1 vs PC 2, we can actually visualize how well the clustering segmented the heroes into strength type, agility type & intelligence type!","2d58ecb0":"### That's all. I hope you learned something from this notebook!! :)","b360a35e":"##### Specify the URL","c3ef617d":"##### Contents","185c014e":"### Getting the Table","674b8bdf":"##### The table above shows how each of the Principal Components are built. For instance, PC1 has 0.264279 STR, 0.288449 STR+ and so on and so forth.","b983b328":"##### Dropping Hero column and converting everything else to float except MAINATT","c52feeca":"##### Let's investigate - \n##### These are the heroes that were clustered wrongly by KMeans:","0dc7edfd":"##### This is now the new dataset ready for clustering.","290aeed8":"##### We can see that at around PC13 we can already explain >90% of our dataset, so we can try to ignore the rest of the components. We can further do some visualizations for that:\n##### Visualizing Cumulative Explained Variance vs Number of Components\n","c9ac3c77":"!Note!: The colors of the clusters have changed. I'm still finding a way to create a customized colorscale for 3d plots but no luck :). Please do comment if you can help me :)\n##### Upon investigating, the uppermost datapoint actually represents the Night Stalker. He is a very unique hero, because his daytime and night time vision values (VS-D and VS - N) are opposite from everyone else (because he is a night stalker :)). \n![image.png](attachment:image.png)\n\nSnipped from the [website](http:\/\/)https:\/\/dota2.gamepedia.com\/Table_of_hero_attributes where I scraped.","29c2d12b":"##### And now finally, let's check how the clustering went by looking at it's accuracy (comparing it with the original clustering)","62bbd678":"##### Now let's see which principal components are important to the clustering we just did. Let's utilize the RandomForest Feature Importance Plot","f37ab8ba":"##### Transforming the Data into Principal Components","0e118484":"##### Import the Libraries","38a0c223":"### Scaling the Features","9f5fb9e8":"##### Inserting the cluster labels to the dataframe","6abdafc7":"### Initialization","f650d7dc":"![image.png](attachment:image.png)\n\n[Img Src](https:\/\/www.muycomputer.com\/wp-content\/uploads\/2016\/05\/DOTA-2-1.jpg)\n\nDataset was gathered from: https:\/\/dota2.gamepedia.com\/Table_of_hero_attributes\nYou can also view the complete list of features description there.","77d8d5b6":"## Principal Component Analysis","bc20a072":"##### Dropping the DV","b915d715":"##### Let's see if there are features that are highly correlated","b7f99174":"##### Let's confirm the visualization by looking at the Variance Inflation Factor","6f266161":"##### We'll not drop both DMG(MIN) & DMG(MAX)","f67cf74e":"## KMeans Clustering","6c3dcaa9":"##### Box Plots and Violin Plots","2c8c8eda":"##### BeautifulSoup Object","9695f9fe":"##### Take a look at the linear combinations of the original features for the <whatevernumber> principal components.","79162413":"##### A bulk of the misclassified heroes are clustered as strength type while its original type is Agility. And if you play Dota, you can actually explain some of these. For instance, Ursa was clustered as strength primarily because that hero is known as someone with high strength attribute. The same can be said with Ogre Magi - he is a hero that might be intelligence type but is actually a durable and strong hero. Also, all dota players know that Medusa is Agility type hero, but she has been clustered as Intelligence type by the algorithm probably because of his high Intelligence attribute. I'm guessing Dota 2 creator gave her high intelligence attribute because of her skill \"Mana Shield\".","5c352595":"## Web Scraping","ee9bb89a":"##### We perform PCA for Dimensionality Reduction to try to cluster the dataset\n*Note: Principal Component Analysis can only be used for datasets containing purely numerical variables\/columns. \n\nPCA is a method of dimensionality reduction, an unsupervised learning technique which is meant to find patterns in your data. It is unsupervised because it doesn't require a teacher or a label of dependent variables for training and testing unlike in supervised learning.\n\nPCA is simply a method of feature extraction, which is one of the means of dimensionality reduction. What PCA does is that it creates or extracts new features called principal components which are basically linear combinations of your original features, that explain the most variance of the dataset. Take note this is different from the variance error in the bias-variance that you already know. The first step is to create the best fitting line that explains the most variance on your dataset which is done by calculating the sum of the squared distances. The larger the sum of the squared distances of the points when projected along that line, the better fit it is. That first line created is called the principal component 1 and the SSD there is called Eigenvalue for PC 1, and this means that this principal component which is a linear combination of your original features, is the principal component that explains the most variance in your data. Then, the PC2 or the second principal component is the line that is orthogonal to this, and so on and so forth.\n","fe487665":"### Clusters Initialization and Finding K.","35557ed6":"##### Mapping the MAINATT feature","186deb6f":"##### 3D Plots","9115a5e4":"##### Outputting the Dataset for future use","6aa09596":"##### Check the title","8b78e4ef":"### Tabulating the Principal Components\n##### This would help us see the importance of each component and up to which principal component gives us a cumulative explained variance of >= 90%.","5b745f51":"##### Inserting the table headers","b639e42e":"##### Let's import the data back","59292af4":"##### The plot above actually tells us that PC1, PC2 & PC4 are the more important PCs for clustering","1cb209f4":"##### PairPlots"}}