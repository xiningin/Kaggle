{"cell_type":{"a4d3d560":"code","6361687c":"code","dec73979":"code","e9eeb47a":"code","9a8d3c78":"code","23c6b8af":"code","777292b5":"code","d767c8f6":"code","2f6ad950":"code","402d0afb":"code","b9f41b42":"code","5ab3f7f1":"code","5538fc8e":"code","34ed8b50":"code","7c1cfb71":"code","65f81045":"code","5b827a47":"code","91767d34":"code","87e3da5b":"code","78615224":"code","d35f4e98":"code","325107f0":"code","8c053451":"markdown","26e0868c":"markdown","39e4bcc5":"markdown","035a7314":"markdown","c1f19d71":"markdown","5f090924":"markdown","dcfb91f4":"markdown","1e2ae70b":"markdown","4e1cb702":"markdown","474ef58c":"markdown","922ad3bb":"markdown","55defb32":"markdown","7738b35e":"markdown","295eccc1":"markdown","ac460179":"markdown"},"source":{"a4d3d560":"%%capture\n# Install Weights and Biases.\n!pip install wandb -q","6361687c":"import tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nimport tensorflow_addons as tfa\n\nimport os\nimport re\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom functools import partial\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n# Imports for augmentations. \nfrom albumentations import (\n    Compose, RandomCrop, RandomResizedCrop, HorizontalFlip, VerticalFlip, Resize \n)","dec73979":"import wandb\nfrom wandb.keras import WandbCallback\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\")\n\nwandb.login(key=wandb_api)","e9eeb47a":"# Set the random seeds\ndef seed_everything():\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n    tf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n    \nseed_everything()","9a8d3c78":"WORKING_DIR_PATH = '..\/input\/hpa-single-cell-image-classification\/'\n\n# RGB images of size 256x256.\nTRAIN_512 = '..\/input\/hpa256x256dataset\/train\/rgb\/'\n\nIMG_WIDTH = 224\nIMG_HEIGHT = 224\nBATCH_SIZE = 64\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","23c6b8af":"df_train = pd.read_csv(WORKING_DIR_PATH+'train.csv')\ndf_train.head()","777292b5":"run = wandb.init(project='hpa', job_type='consume_split')\nartifact = run.use_artifact('ayush-thakur\/hpa\/split:v0', type='dataset')\nSPLIT_CSV_PATH = artifact.download()\nrun.finish()","d767c8f6":"df_train_split = pd.read_csv(SPLIT_CSV_PATH+'\/train_split.csv')\ndf_val_split = pd.read_csv(SPLIT_CSV_PATH+'\/val_split.csv')\n\nprint(df_train_split.head(), '\\n', df_val_split.head())","2f6ad950":"# Ref: https:\/\/www.kaggle.com\/divyanshuusingh\/eda-image-segmentation\nLABELS= {\n0: \"Nucleoplasm\",\n1: \"Nuclear membrane\",\n2: \"Nucleoli\",\n3: \"Nucleoli fibrillar center\",\n4: \"Nuclear speckles\",\n5: \"Nuclear bodies\",\n6: \"Endoplasmic reticulum\",\n7: \"Golgi apparatus\",\n8: \"Intermediate filaments\",\n9: \"Actin filaments\",\n10: \"Microtubules\",\n11: \"Mitotic spindle\",\n12: \"Centrosome\",\n13: \"Plasma membrane\",\n14: \"Mitochondria\",\n15: \"Aggresome\",\n16: \"Cytosol\",\n17: \"Vesicles and punctate cytosolic patterns\",\n18: \"Negative\"\n}","402d0afb":"# Ref: https:\/\/github.com\/tensorflow\/tensorflow\/issues\/16044\n@tf.function\ndef multiple_one_hot(cat_tensor, depth_list):\n    \"\"\"Creates one-hot-encodings for multiple categorical attributes and\n    concatenates the resulting encodings\n\n    Args:\n        cat_tensor (tf.Tensor): tensor with mutiple columns containing categorical features\n        depth_list (list): list of the no. of values (depth) for each categorical\n\n    Returns:\n        one_hot_enc_tensor (tf.Tensor): concatenated one-hot-encodings of cat_tensor\n    \"\"\"\n    one_hot_enc_tensor = tf.one_hot(cat_int_tensor[:,0], depth_list[0], axis=1)\n    for col in range(1, len(depth_list)):\n        add = tf.one_hot(cat_int_tensor[:,col], depth_list[col], axis=1)\n        one_hot_enc_tensor = tf.concat([one_hot_enc_tensor, add], axis=1)\n\n    return one_hot_enc_tensor\n\n@tf.function\ndef decode_image(img):\n    # convert the compressed string to a 3D uint8 tensor\n    img = tf.image.decode_png(img, channels=3)\n    # Normalize image\n    img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n    # resize the image to the desired size\n    return img\n\ndef resize_val_image(image, label):\n    return tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH]), label\n\n@tf.function\ndef load_image(df_dict):\n    # Load image\n    image = tf.io.read_file(TRAIN_512+df_dict['ID']+'.png')\n    image = decode_image(image)\n    \n    # Parse label\n    label = tf.strings.split(df_dict['Label'], sep='|')\n    label = tf.strings.to_number(label, out_type=tf.int32)\n    label = tf.reduce_sum(tf.one_hot(indices=label, depth=19), axis=0)\n    \n    return image, label","b9f41b42":"# Define the augmentation policies. Note that they are applied sequentially with some probability p.\ntransforms = Compose([\n                RandomResizedCrop(height=IMG_HEIGHT, width=IMG_WIDTH, always_apply=True),\n                HorizontalFlip(),\n                VerticalFlip()\n        ])\n\n# Apply augmentation policies.\ndef aug_fn(image):\n    data = {\"image\":image}\n    aug_data = transforms(**data)\n    aug_img = aug_data[\"image\"]\n\n    return aug_img","5ab3f7f1":"@tf.function\ndef apply_augmentation(image, label):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n    aug_img.set_shape((IMG_HEIGHT, IMG_WIDTH, 3))\n    \n    return aug_img, label","5538fc8e":"# Consume training CSV \ntrain_ds = tf.data.Dataset.from_tensor_slices(dict(df_train_split))\nval_ds = tf.data.Dataset.from_tensor_slices(dict(df_val_split))\n\n# Training Dataset\ntrain_ds = (\n    train_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .map(apply_augmentation, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n# Validation Dataset\nval_ds = (\n    val_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .map(resize_val_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)","34ed8b50":"def get_label_name(labels):\n    l = np.where(labels == 1.)[0]\n    label_names = []\n    for label in l:\n        label_names.append(LABELS[label])\n        \n    return '-'.join(str(label_name) for label_name in label_names)\n\ndef show_batch(image_batch, label_batch):\n  plt.figure(figsize=(20,20))\n  for n in range(25):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      plt.title(get_label_name(label_batch[n].numpy()))\n      plt.axis('off')","7c1cfb71":"# Training batch\nimage_batch, label_batch = next(iter(train_ds))\nshow_batch(image_batch, label_batch)","65f81045":"# Validation batch\nimage_batch, label_batch = next(iter(val_ds))\nshow_batch(image_batch, label_batch)","5b827a47":"def get_model():\n    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet')\n    base_model.trainabe = True\n\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n    x = base_model(inputs, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(len(LABELS), activation='sigmoid')(x)\n    \n    return Model(inputs, outputs)\n\ntf.keras.backend.clear_session()\nmodel = get_model()\nmodel.summary()","91767d34":"earlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=10, verbose=0, mode='min',\n    restore_best_weights=True\n)\n\nlronplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=5, verbose=0,\n    mode='auto', min_delta=0.0001, cooldown=0, min_lr=0\n)","87e3da5b":"# Initialize model\ntf.keras.backend.clear_session()\nmodel = get_model()\n\n# Compile model\nmodel.compile('adam', \n              loss=tfa.losses.SigmoidFocalCrossEntropy(), \n              metrics=[tf.keras.metrics.AUC(multi_label=True)])\n\n# Initialize W&B run\nrun = wandb.init(entity='ayush-thakur', project='hpa', job_type='train')\n\n# Train\nmodel.fit(train_ds, \n          epochs=50,\n          validation_data=val_ds,\n          callbacks=[WandbCallback(),\n                     earlystopper])\n\n# Close W&B run\nrun.finish()","78615224":"model.evaluate(val_ds)","d35f4e98":"model.save('effnet_multilabel_1.h5')","325107f0":"run = wandb.init(project='hpa', job_type='model')\nartifact = run.use_artifact('ayush-thakur\/hpa\/split:v0', type='dataset')\n\nartifact_model = wandb.Artifact('multi-label-model', type='model')\n\n# Add a file to the artifact's contents\nartifact_model.add_file('effnet_multilabel_1.h5')\n\n# Save the artifact version to W&B and mark it as the output of this run\nrun.log_artifact(artifact_model)\n\nrun.finish()","8c053451":"# \ud83d\udc24 Model","26e0868c":"# \ud83d\udcf2 Callbacks","39e4bcc5":"# \ud83d\udcc0 Set Hyperparameters","035a7314":"# \ud83d\ude8b Train with W&B","c1f19d71":"### Get Train-Val Split from saved W&B Artifact","5f090924":"### Augmentation using Albumentations","dcfb91f4":"# Work In Progress -> Stay Tuned. ","1e2ae70b":"# \ud83d\udca5 Includes\n\nThis Kaggle kernel will document my primary exploits for this competition. Here I am trying to train an simple multi-label image classifier. The idea is to experiment few techniques before leveraging weak supervision for instance segmentation.\n\nKey features:\n\n* Simple to follow along code. \n* Binary crossentropy as loss function. `AUC` as metric.\n* Early stopping regularization monitoring `val_auc`.\n* Using EfficientNetB0 with input image size 256x256. \n\nUpdates:\n\n* Using Focal Loss.\n\nHere are some of my other kernels. \n\n* [RGB 512x512 Dataset Creation+Versioning with W&B](https:\/\/www.kaggle.com\/ayuraj\/hpa512x512dataset)\n    * I used this kernel to build RGB dataset of size 512x512 pixels. I am leaving out the yellow channel for now. My input data pipeline initially took approx 50 minutes per epoch when I was loading images(channels) separately and then stacking them. By using an already stacked image the ETA per epoch is approx 3 mins. \n    * I am also using Weights and Biases(W&B) in this kernel to store train-validation split as artifacts. \n\n* [HPA: Segmentation Mask Visualization with W&B](https:\/\/www.kaggle.com\/ayuraj\/hpa-segmentation-mask-visualization-with-w-b)\n    * In this kernel I am attempting to understand the use of HPA Cell Segmentation tool provided by the competition hosts. \n    * I am also using W&B's interactive visualization to play with the segmentation masks that are logged. \n    * Here's a short report summarizing the result: http:\/\/bit.ly\/play-with-segmentation-masks\n    \n    \n**If you like the work, upvote for encouragement. Let's discuss ideas in the comment section.** ","4e1cb702":"## Visualize Batch","474ef58c":"We will use Weights and Biases for experiment tracking.","922ad3bb":"# Prepare `tf.data` Dataloader","55defb32":"The `train.csv` holds ID for the training images and image-level labels. **There are a total of 21,806 training images.**","7738b35e":"![image.png](attachment:image.png)","295eccc1":"# Save Model as Artifacts","ac460179":"\n# \ud83e\uddf0 Setups, Installations and Imports"}}