{"cell_type":{"b9a1e8c6":"code","4cae313a":"code","09e4b77e":"code","83f9e3af":"code","3f7ad56e":"code","c2d1ea0e":"code","03676949":"code","5ffa1f71":"code","aede6b38":"code","7253613e":"code","c5e46bb7":"code","00bd2d51":"code","aaf4d72b":"code","c47f5216":"code","15ad83b5":"code","3407655f":"code","b6968cc4":"code","a8d439e0":"code","4d67f764":"code","99ae8eb8":"code","fb3966b8":"code","6b792fe1":"code","bf49ec59":"code","bcbfda09":"code","54d7f20c":"code","7ca290a6":"code","641685fc":"code","171fc54a":"code","bb41c68d":"code","2233568b":"code","5c4f81d1":"code","f5e375f9":"code","ee2d600b":"code","3876711e":"code","cb2dd42d":"code","df16a087":"code","3f10d7ac":"code","0fcebf21":"code","c5b9fa5d":"code","a8caa336":"code","3b18ca84":"markdown","c2651bc2":"markdown","a21a1311":"markdown","96b4f40e":"markdown","9cb68323":"markdown","3a494cc5":"markdown","2b399000":"markdown","9052626a":"markdown","fe898d52":"markdown","d7778778":"markdown","a1d71ba4":"markdown","8a80387a":"markdown","6520549e":"markdown","c0fa3135":"markdown","6950f1ac":"markdown","da0269ba":"markdown","25df2965":"markdown","2d533d7d":"markdown","e83007e7":"markdown","7d8453cc":"markdown","a28edfed":"markdown","38661d6a":"markdown","1eceebea":"markdown","45599ad8":"markdown","5bd7aa12":"markdown","b718b71d":"markdown","401d2407":"markdown","204d312e":"markdown","e5b87e23":"markdown","8b4078f8":"markdown","e7c3169e":"markdown","087a0964":"markdown","9b7541dd":"markdown","70f77c62":"markdown","3656aa8a":"markdown","d9856367":"markdown","79fadd00":"markdown","7fbece90":"markdown","af8cc4a3":"markdown"},"source":{"b9a1e8c6":"#Let's import necessary dependencies \nimport pandas as pd\nimport warnings\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline ","4cae313a":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","09e4b77e":"warnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\n\n#Read data for analysis\ndata=pd.read_csv('..\/input\/covtype.csv')","83f9e3af":"print('Data Dimension:')\nprint('Number of Records:', data.shape[0])\nprint('Number of Features:', data.shape[1])","3f7ad56e":"#Names of columns\nprint('Feature Names')\nprint(data.columns)","c2d1ea0e":"#A huge list!\nprint(data.info())","03676949":"\nplt.figure(figsize=(6,4))\nsns.countplot(y=data.dtypes ,data=data)\nplt.xlabel(\"Data Type Count\")\nplt.ylabel(\"Data types\")\n","5ffa1f71":"#Let's check for missing values once again\ndata.isnull().sum()","aede6b38":"data.describe()","7253613e":"print('Skewness of the below features:')\nprint(data.skew())","c5e46bb7":"skew=data.skew()\nskew_df=pd.DataFrame(skew,index=None,columns=['Skewness'])\nplt.figure(figsize=(15,7))\nsns.barplot(x=skew_df.index,y='Skewness',data=skew_df)\nplt.xticks(rotation=90)","00bd2d51":"class_dist=data.groupby('Cover_Type').size()\nclass_label=pd.DataFrame(class_dist,columns=['Size'])\nplt.figure(figsize=(8,6))\nsns.barplot(x=class_label.index,y='Size',data=class_label)","aaf4d72b":"for i,number in enumerate(class_dist):\n    percent=(number\/class_dist.sum())*100\n    print('Cover_Type',class_dist.index[i])\n    print('%.2f'% percent,'%')","c47f5216":"data.head()","15ad83b5":"cont_data=data.loc[:,'Elevation':'Horizontal_Distance_To_Fire_Points']\n\nbinary_data=data.loc[:,'Wilderness_Area1':'Soil_Type40']\n\nWilderness_data=data.loc[:,'Wilderness_Area1': 'Wilderness_Area4']\n\nSoil_data=data.loc[:,'Soil_Type1':'Soil_Type40']","3407655f":"#Iterate via columns of data having only binary features\nfor col in binary_data:\n    count=binary_data[col].value_counts()\n    print(col,count)","b6968cc4":"print('Soil Type',' Occurence_count')\nfor col in binary_data:\n    count=binary_data[col].value_counts()[1] #considering all one's among 1 and 0's in each soil type\n    if count < 1000:\n        print(col,count)","a8d439e0":"# data_num = data.select_dtypes([np.int, np.float]) #If you need to select only numeric features. \n#Here we already have all numeric Data.\n\nfor i, col in enumerate(cont_data.columns):\n    plt.figure(i)\n    sns.distplot(cont_data[col])","4d67f764":"# %%time\ndata['Cover_Type']=data['Cover_Type'].astype('category') #To convert target class into category\n\nfor i, col in enumerate(cont_data.columns):\n    plt.figure(i,figsize=(8,4))\n    sns.boxplot(x=data['Cover_Type'], y=col, data=data, palette=\"coolwarm\")","99ae8eb8":"%%time\nfor i, col in enumerate(binary_data.columns):\n    plt.figure(i,figsize=(6,4))\n    sns.countplot(x=col, hue=data['Cover_Type'] ,data=data, palette=\"rainbow\")","fb3966b8":"%%time\n#If someone can help me with function to reverse one hot coding, please let me know in comment. I know this is not the robust way.\ndef rev_code(row):\n    for c in Soil_data.columns:\n        if row[c]==1:\n            return c  \n\ndata['Soil_Type']=Soil_data.apply(rev_code, axis=1) #Time consuming","6b792fe1":"%%time\ndef rev_code(row):\n    for c in Wilderness_data.columns:\n        if row[c]==1:\n            return c  \n\ndata['Wilderness_Type']=Wilderness_data.apply(rev_code, axis=1) #Time consuming","bf49ec59":"%%time\nplt.figure(figsize=(16,8))\nsns.countplot(x='Wilderness_Type', hue='Cover_Type',data=data, palette=\"rainbow\")\nplt.xticks(rotation=90)","bcbfda09":"%%time\nplt.figure(figsize=(16,8))\nsns.countplot(x='Soil_Type', hue='Cover_Type',data=data, palette=\"rainbow\")\nplt.xticks(rotation=90)","54d7f20c":"soil_counts = []\nfor num in range(1,41):\n    col = ('Soil_Type' + str(num))\n    this_soil = data[col].groupby(data['Cover_Type'])\n    totals = []\n    for value in this_soil.sum():\n        totals.append(value)\n    total_sum = sum(totals)\n    soil_counts.append(total_sum)\n    print(\"Total Trees in Soil Type {0}: {1}\".format(num, total_sum))\n    percentages = [ (total*100 \/ total_sum) for total in totals]\n    print(\"{0}\\n\".format(percentages))\nprint(\"Number of trees in each soil type:\\n{0}\".format(soil_counts))","7ca290a6":"plt.figure(figsize=(15,8))\nsns.heatmap(cont_data.corr(),cmap='magma',linecolor='white',linewidths=1,annot=True)","641685fc":"g = sns.PairGrid(cont_data)\ng.map(plt.scatter)","171fc54a":"# %%time\n# g = sns.PairGrid(cont_data)\n# g.map_diag(plt.hist)\n# g.map_upper(sns.kdeplot)\n# g.map_lower(sns.kdeplot)","bb41c68d":"X=data.loc[:,'Elevation':'Soil_Type40']\ny=data['Cover_Type']","2233568b":"#Features to be removed before the model\nrem=['Hillshade_3pm','Soil_Type7','Soil_Type8','Soil_Type14','Soil_Type15',\n     'Soil_Type21','Soil_Type25','Soil_Type28','Soil_Type36','Soil_Type37']","5c4f81d1":"#Remove the unwanted features\nX.drop(rem, axis=1, inplace=True)","f5e375f9":"#Splitting the data into  train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=101)","ee2d600b":"%%time\n#Setup arrays to store training and test accuracies\nneighbors = np.arange(1,7)\ntrain_accuracy =np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.score(X_test, y_test) ","3876711e":"#Generate plot\nplt.figure(figsize=(10,6))\nplt.title('k-NN Varying number of neighbors')\nplt.plot(neighbors, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\n#plt.show()","cb2dd42d":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(n_neighbors=5) #Using Eucledian distance","df16a087":"#Fit the model\nknn.fit(X_train,y_train)","3f10d7ac":"#Get accuracy. Note: In case of classification algorithms score method represents accuracy.\nAccuracy=knn.score(X_test,y_test)\nprint('KNN Accuracy:',Accuracy)","0fcebf21":"import scipy.stats as ss\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import zscore\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns","c5b9fa5d":"%%time\nMLA = []\nZ = [LinearSVC() , DecisionTreeClassifier() , LogisticRegression() , GaussianNB() ,RandomForestClassifier() , \n     GradientBoostingClassifier()]\nX = [\"LinearSVC\" , \"DecisionTreeClassifier\" , \"LogisticRegression\" , \"GaussianNB\" ,\"RandomForestClassifier\" , \n     \"GradientBoostingClassifier\"]\n\nfor i in range(0,len(Z)):\n    model = Z[i]\n    model.fit( X_train , y_train )\n    pred = model.predict(X_test)\n    MLA.append(accuracy_score(pred , y_test))","a8caa336":"d = { \"Algorithm\" : X, \"Accuracy\" : MLA }\n\ndfm = pd.DataFrame(d)\ndfm","3b18ca84":"* > The above plots more or less tells us about the skewness that we saw earlier. Let's dig down into Bivariate and Multivariate Analysis\n* > Let's check for distribution with respect to our target. This is where magic happens!","c2651bc2":"**#Inferences:**\n> **This tells me lots of valuable insights. Mostly regarding the soil types. Wanna know? Ok, let me not hide it from you**.\n* It's Just that, there are some of the Soil types which consists of very few counts.  \n* Statistically speaking, for half a million records, balance number per soil type (total 40 in number) is 581012\/40 = 14.5k\n* Whereas, here we see a different figure. I know that data need not be balanced all the times. But may be we can get rid of really small size features. Isn't it?\n* Let me list down those along with there size. I'm displaying the Soil type having less than 1000 occurence size","a21a1311":"* > There's lot of scope for Data Viz. as far this dataset is concerned. My objective was a surface walkthrough the dataset. I would roll out new versions on this part by part. \n* >Let's now wind it up by Data Modelling. Another Excitement, right?","96b4f40e":"**Not bad. KNN works great here. Lazy learner is doing a good work at differentiating a CoverType. **","9cb68323":"**Thank you for opening this script!**\n\n**I have made all efforts to walkthrough this dataset and have tried making it simple and steady so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts.**\n\n**Please upvote this kernel if you find it useful for your understanding at this data. Your comments on how we can improve this kernel is always  welcome. Thanks.**\n\n**My other exploratory studies can be accessed here : https:\/\/www.kaggle.com\/roshanchoudhary\/kernels** (Very few as of now)","3a494cc5":"**I'll put the accuracies obtained by various other classification techniques. Try to enhance it more via Cross Validation may be.**\n**Let me know in comment if you manage to raise your accuracies. **\n> **I'm gonna do it too. Those are my next steps, such as CV, more insights, Feature engineering etc. I'll roll out updates part by part**","2b399000":"**#Inferences:**\n1.  Cover_Type 1 and 2 i.e **Spruce\/Fir** and **Lodgepole Pine** seems to dominate the area. \n2.  Also the Cover_Type 4 i.e **Cottonwood\/Willow** is minimal compare to the rest","9052626a":"* ** I have tried various Classification algorithms out of which KNN served the best.**\n* ** Algorithms such as RandomForest and DecisionTree are doing a decent job here. So please explore.**","fe898d52":"*  **Try to surpass these accuracies. **\n*  **My objective was to 'Get to know' the Forest Cover Type Dataset for which I tried to articulate it step by step.**\n*  **If you liked it, please let me know with a upvote, It serves a Motivation. Good Luck and Thank you for spending your time here! :)**","d7778778":"**Data Modelling**","a1d71ba4":"> I'll do the same for Wilderness Area","8a80387a":"**Looks like we got many binary independent features. Good!**\n**Now let us understand the data type of each features**","6520549e":">Nice! Now, Let's convert the whole data into few Mini datasets. I'll make use of it in plots\n* cont_data - Data without binary features i.e continuous features\n* binary_Data - Data having all binary features [Wilderness Areas + Soil Types]\n* wilderness_Data - Binary Wilderness Areas\n* Soil_Data - Binary Soil Types","c0fa3135":"* I know this will make more sense in a visual such as bar graph right? I'm excited to see it too. But let's infer more from the numbers as of now. \n* We'll do plottings once we start with Bivariate and Multivariate analysis. \n* We'll see if we need to really drop the above soil types. \n* We can only confirm on it if it is not aligned (give any relation) to our target variable i.e Cover_Type. So, please wait, do not conclude. Climax is yet to come :D","6950f1ac":"* >Couple of features are have a good amount of co-relation. Guess which one? I'll tell you.\n* >  Hillshade_9am ~ Hillshade_3pm and Aspect ~ Hillshade_3pm","da0269ba":"* X = Input or independent variables\n* y=  Target variable ('Cover_Type')","25df2965":"> Neighbor value = 5 yeilds the best result. Let's go by that for now. ","2d533d7d":"> But I'm interested in percentwise distribution of each class. Let's check","e83007e7":"**Skewness**\n>The skewness for a normal distribution is zero, and any symmetric data should have a skewness near zero. \n>Negative values for the skewness indicate data that are skewed left and positive values for the skewness indicate data that are skewed right. \n>By skewed left, it means that the left tail is long relative to the right tail. Similarly, skewed right means that the right tail is long relative to the left tail.","7d8453cc":"* > So the plot does justice to the distribution which each class but I want to have a single feature having Soil_Type corresponding to each row. \n* > Let's see if I can do it.  This will help me to visualize it better, instead of counting 0's and 1's in each one hot coded Soil types.","a28edfed":"*  Plots looks cool right? What's Even more cool you know?\n*  The insights. Let's figure out very general insights\n*  There are couple of features which shows not much of variance with respect to classes\n*  And features such as 'Elevation', 'slope' and 'horizontal distance to road_ways does a good job","38661d6a":"**#Inferences:**\n> Some of the Variables are heavily skewed hence need to be corrected or transformed  on a later stage. ","1eceebea":"* >Above two plots tells  us the count of trees in each class considering Wilderness and Soil Type.\n* >Soil_Type plot is not very clear since it's  too vast. So let's go by the number. We'll see how many and what type of Cover_Type we have under each soil Type","45599ad8":"* > This gives us the relation and its shape with respect to other features. Various inferences can be drwan out.\n* > Pairgrid plot is just awesome. And it's even more awesome when it's combined with KDE clusters. \n* > But for considerably heavy data, its time consuming. Be aware before running the below plot.","5bd7aa12":"**Let's get started with plots based EDA (Exploratory Data Analysis) **\n*  Fun begins here, am I right?\n* Data Distribution of features via Histograms. Although I love box plots more than histograms, we'll use boxplot to check distribution with respect to categorical variable. In our case that is Cover_Type, having 7 different category of classes.","b718b71d":">Let us take a step to remove the features with low Std deviation as demonstrated earlier. \n>Also I'll remove one of the co-related variable","401d2407":"**We forgot to check the Data distribution for each feature. Spend some good time here. Lot's of inferences I believe**","204d312e":"**Data_Dictionary**\n\n1. Elevation = Elevation in meters.\n2. Aspect = Aspect in degrees azimuth.\n3. Slope = Slope in degrees.\n4. Horizontal_Distance_To_Hydrology = Horizontal distance to nearest surface water features.\n5. Vertical_Distance_To_Hydrology = Vertical distance to nearest surface water features.\n6. Horizontal_Distance_To_Roadways = Horizontal distance to nearest roadway.\n7. Hillshade_9am = Hill shade index at 9am, summer solstice. Value out of 255.\n8. Hillshade_Noon = Hill shade index at noon, summer solstice. Value out of 255.\n9. Hillshade_3pm = Hill shade index at 3pm, summer solstice. Value out of 255.\n10. Horizontal_Distance_To_Fire_Point = sHorizontal distance to nearest wildfire ignition points.\n11. Wilderness_Area1 = Rawah Wilderness Area\n12. Wilderness_Area2 = Neota Wilderness Area\n13. Wilderness_Area3 = Comanche Peak Wilderness Area\n14. Wilderness_Area4 = Cache la Poudre Wilderness Area\n\n**Soil_Type1 to Soil_Type40 [Total 40 Types]**\n\n**Cover_TypeForest Cover Type designation. Integer value between 1 and 7, with the following key:**\n1. Spruce\/Fir\n2.  Lodgepole Pine\n3.  Ponderosa Pine\n4.  Cottonwood\/Willow\n5.  Aspen\n6.  Douglas-fir\n7.  Krummholz","e5b87e23":"1. **So we have complete Numeric Data, Even Better!!**\n2. **Also there doesn't seem to be any missing value. Good work at Data Collection**","8b4078f8":"**Oh common let us check the data atleast, enough with size and dimension**","e7c3169e":" **Explore Data Dimension and count of values without any sneak peek in Data**","087a0964":"**Did we check the co-relation??**\n * > No we didn't. This is something that I usually check first. No, problem. it's never too late.\n * > Let's better vizualise it via heatmap. All in one!","9b7541dd":"**#Inferences:**\n1. Few of the features looks skewed, we'll see those later.\n2. No missing Values (We say this for the third time :p)\n3. Wilderness Area and Soil Type are one hot coded.\n4. Scales are different over the whole data, hence might need to scale for some required algorithms.","70f77c62":"> Let's do something similar for our binary features. This time we'll use countplot.","3656aa8a":"\n**I want to see the number of  values counts within each features, mainly for the Binary types**","d9856367":"**How about the class balance? We'll see**","79fadd00":"> Yup! It's done. Looks like we have a desired single Soil_Type and Wilderness_Type feature. Let's now use count plot against our Target Cover_Type","7fbece90":"> Let's visualize the change in accuracies with respect to train and test data at different neighbors ","af8cc4a3":"* > Here, First i want to check the shape of continous features with respect to the target class. Hence I'll use the continuous_data (cont_data) and plot a boxplot against target. \n* > You can also look at violinplot here, It's visually appealing. "}}