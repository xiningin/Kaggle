{"cell_type":{"f6a19d82":"code","f7adc58d":"code","816a277a":"code","0e24bb13":"code","05427738":"code","5cb461e1":"code","5fec3e55":"code","52458c4f":"code","aca8182c":"code","e38be21a":"code","6820d99e":"code","4db27191":"code","dfc64ff5":"code","7e608f9c":"code","9edcbc24":"code","f8d8f744":"code","214da138":"code","1b347604":"code","b638ae91":"code","17c9449d":"code","60f48c57":"code","de4de438":"code","3869ba15":"code","a2384337":"code","e60cd724":"code","c05d918b":"markdown","6a693b85":"markdown","24890a12":"markdown","dfd02bc6":"markdown","fa5d9c6a":"markdown","6b6b2ad5":"markdown","3a93d4aa":"markdown","b2d33527":"markdown","a1fdef91":"markdown","2af3af39":"markdown"},"source":{"f6a19d82":"import pandas as pd\nimport numpy as np\n\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nFEATURES = [\n    'total_acc_x_',\n    'total_acc_y_',\n    'total_acc_z_',\n\n    'body_acc_x_',\n    'body_acc_y_',\n    'body_acc_z_',\n\n    'body_gyro_x_',\n    'body_gyro_y_',\n    'body_gyro_z_'\n]\n\nTARGETS = [\n    'WALKING',\n    'WALKING_UPSTAIRS',\n    'WALKING_DOWNSTAIRS',\n    'SITTING',\n    'STANDING',\n    'LAYING'\n]","f7adc58d":"train_x = np.fromfile('..\/input\/con_train_x.bin').reshape((7352, 128, 9))\ntrain_y = to_categorical(pd.read_csv('..\/input\/con_train_y.csv')[['label']])\ntest_x = np.fromfile('..\/input\/con_test_x.bin').reshape((2947, 128, 9))","816a277a":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.3, random_state=42)","0e24bb13":"pd.Series(dict(zip(TARGETS, train_y.sum(axis=0))))","05427738":"X_train.shape # (sample, timestamp, feature_index)","5cb461e1":"test_x.shape","5fec3e55":"sample = X_train[0]\n\nf, ax = plt.subplots(nrows=sample.shape[1], figsize=(16,15))\n\nfor i in range(sample.shape[1]):\n    ax[i].set_title(FEATURES[i])\n    ax[i].plot(sample[:, i])","52458c4f":"from xgboost import XGBClassifier\n\n\nxgb_train_x = X_train.reshape(len(X_train), -1)\nxgb_val_x = X_val.reshape(len(X_val), -1)\n\nxgb_train_y = np.argmax(y_train, axis=1)\nxgb_val_y = np.argmax(y_val, axis=1)","aca8182c":"xgbc = XGBClassifier(n_estimators=100, random_state=0, n_jobs=-1)","e38be21a":"xgbc.fit(xgb_train_x, xgb_train_y)","6820d99e":"preds = xgbc.predict(xgb_val_x)","4db27191":"accuracy_score(np.argmax(y_val, axis=1), preds)","dfc64ff5":"xgb_train_x = train_x.reshape(len(train_x), -1)\nxgb_test_x = test_x.reshape(len(test_x), -1)\n\nxgb_train_y = np.argmax(train_y, axis=1)","7e608f9c":"xgbc.fit(xgb_train_x, xgb_train_y)","9edcbc24":"xgbc.predict(xgb_test_x)","f8d8f744":"from keras.models import Model\nfrom keras.layers import Input, Dense, Add, Activation, Conv1D, GlobalAveragePooling1D\nfrom keras.utils import np_utils\nimport numpy as np\nimport keras \nfrom keras.callbacks import ReduceLROnPlateau\n\n \ndef build_resnet(input_shape, n_feature_maps, nb_classes):    \n    x = Input(shape=(input_shape))\n    conv_x = keras.layers.normalization.BatchNormalization()(x)\n    conv_x = keras.layers.Conv1D(n_feature_maps, 8, padding='same')(conv_x)\n    conv_x = keras.layers.normalization.BatchNormalization()(conv_x)\n    conv_x = Activation('relu')(conv_x) \n\n    conv_y = keras.layers.Conv1D(n_feature_maps, 5, padding='same')(conv_x)\n    conv_y = keras.layers.normalization.BatchNormalization()(conv_y)\n    conv_y = Activation('relu')(conv_y)\n\n    conv_z = keras.layers.Conv1D(n_feature_maps, 3, padding='same')(conv_y)\n    conv_z = keras.layers.normalization.BatchNormalization()(conv_z)\n\n    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n    if is_expand_channels:\n        shortcut_y = keras.layers.Conv1D(n_feature_maps, 1, padding='same')(x)\n        shortcut_y = keras.layers.normalization.BatchNormalization()(shortcut_y)\n    else:\n        shortcut_y = keras.layers.normalization.BatchNormalization()(x)\n\n    y = Add()([shortcut_y, conv_z])\n    y = Activation('relu')(y)\n\n    x1 = y\n    conv_x = keras.layers.Conv1D(n_feature_maps*2, 8, padding='same')(x1)\n    conv_x = keras.layers.normalization.BatchNormalization()(conv_x)\n    conv_x = Activation('relu')(conv_x)\n\n\n    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5, padding='same')(conv_x)\n    conv_y = keras.layers.normalization.BatchNormalization()(conv_y)\n    conv_y = Activation('relu')(conv_y)\n\n\n    conv_z = keras.layers.Conv1D(n_feature_maps*2, 3, padding='same')(conv_y)\n    conv_z = keras.layers.normalization.BatchNormalization()(conv_z)\n\n    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n    if is_expand_channels:\n        shortcut_y = keras.layers.Conv1D(n_feature_maps*2, 1, padding='same')(x1)\n        shortcut_y = keras.layers.normalization.BatchNormalization()(shortcut_y)\n    else:\n        shortcut_y = keras.layers.normalization.BatchNormalization()(x1)\n\n    y = Add()([shortcut_y, conv_z])\n    y = Activation('relu')(y)\n\n    x1 = y\n    conv_x = keras.layers.Conv1D(n_feature_maps*2, 8, padding='same')(x1)\n    conv_x = keras.layers.normalization.BatchNormalization()(conv_x)\n    conv_x = Activation('relu')(conv_x)\n\n    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5, padding='same')(conv_x)\n    conv_y = keras.layers.normalization.BatchNormalization()(conv_y)\n    conv_y = Activation('relu')(conv_y)\n\n    conv_z = keras.layers.Conv1D(n_feature_maps*2, 3, padding='same')(conv_y)\n    conv_z = keras.layers.normalization.BatchNormalization()(conv_z)\n\n    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n    if is_expand_channels:\n        shortcut_y = keras.layers.Conv1D(n_feature_maps*2, 1, padding='same')(x1)\n        shortcut_y = keras.layers.normalization.BatchNormalization()(shortcut_y)\n    else:\n        shortcut_y = keras.layers.normalization.BatchNormalization()(x1)\n\n    y = Add()([shortcut_y, conv_z])\n    y = Activation('relu')(y)\n\n    full = keras.layers.pooling.GlobalAveragePooling1D()(y)   \n    out = Dense(nb_classes, activation='softmax')(full)\n\n    return x, out","214da138":"inputs, outputs = build_resnet(train_x.shape[1:], n_feature_maps=64, nb_classes=y_train.shape[1])","1b347604":"model = Model(inputs, outputs)\noptimizer = keras.optimizers.Adam(lr=0.0001)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])","b638ae91":"chck = keras.callbacks.ModelCheckpoint(\"har_nn_bw.hdf\", save_best_only=True)","17c9449d":"history = model.fit(\n    X_train, y_train, \n    validation_data=(X_val, y_val), epochs=10, batch_size=32, shuffle=True,\n    callbacks=[chck]\n)","60f48c57":"pd.DataFrame({\n    'loss': history.history['loss'],\n    'val_loss': history.history['val_loss']\n}).plot()","de4de438":"model.load_weights(\"har_nn_bw.hdf\")","3869ba15":"preds = model.predict(X_val)","a2384337":"accuracy_score(np.argmax(y_val, axis=1), np.argmax(preds, axis=1))","e60cd724":"test_y = pd.read_csv('..\/input\/con_sample.csv')\ntest_y['label'] = np.argmax(model.predict(test_x), axis=1)\n\ntest_y.to_csv('submission.csv', index=False)","c05d918b":"Predict","6a693b85":"## NN Model (ResNet)","24890a12":"  Predicting test data","dfd02bc6":"---","fa5d9c6a":"---","6b6b2ad5":"Classes is balanced","3a93d4aa":"## GB model","b2d33527":"Score","a1fdef91":"----","2af3af39":"**Human Activity Recognition Using Smartphones Dataset**\n\nThe experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. \n\nThe sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings\/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain. See 'features_info.txt' for more details. \n"}}