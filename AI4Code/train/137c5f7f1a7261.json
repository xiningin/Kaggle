{"cell_type":{"90df3f1d":"code","c68a01ff":"code","7feab3a0":"code","74bbe4a6":"code","3332ff45":"code","d78a2b81":"code","779a6447":"code","6d591a99":"code","ade9ac46":"code","f508beb0":"code","d2af2d01":"code","6f5462c9":"code","318ec5e6":"code","1f4aa26a":"code","ab7d4731":"code","f0a02331":"code","68a8da39":"code","1f2d3c45":"code","091f1437":"code","aa0e4591":"code","296eff68":"code","66242587":"code","66bdaf67":"code","ce192eb1":"code","05727c87":"code","1d8ccbe9":"code","57b535c3":"code","32772ccf":"code","421b39e9":"markdown","45940023":"markdown","8d79ebf9":"markdown","2b4c7179":"markdown","3613e0c8":"markdown","5a99010e":"markdown","3026387c":"markdown","d8bbea79":"markdown","c1ac4750":"markdown","0b7b25ec":"markdown","f2f49356":"markdown","a91286bf":"markdown","70d263db":"markdown","2cadebec":"markdown","4ed92ad7":"markdown"},"source":{"90df3f1d":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nfrom urllib.request import urlopen\nimport numpy as np\nimport cv2\n\ndef download_image(url):\n    data = urlopen(url).read()\n    data = np.frombuffer(data, np.uint8)\n    image = cv2.imdecode(data, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\nimage = download_image('https:\/\/cdn.syg.ma\/attachments\/5ddb603e49c47795eea036b6ffbdca417c4588de\/store\/5b58b74e4ea3f1a3d791b47c6a0739347688562226c3fec3b276f5939763\/file.jpeg')\n\nplt.figure(figsize=(10, 10))\nplt.imshow(image)\nplt.show()","c68a01ff":"import torch\nimport albumentations as A\n\neffects = A.Compose([\n        #A.Resize(224, 224)\n        A.SmallestMaxSize(max_size = 224),\n        A.CenterCrop(224, 224)\n        ])\n\n\ntorch.tensor([effects(image = image)['image']\/255]).shape","7feab3a0":"plt.imshow(effects(image = image)['image'])","74bbe4a6":"import torch\nimport torch.nn as nn\nimport albumentations as A\n\neffects = A.Compose([\n        A.SmallestMaxSize(max_size = 224),\n        A.CenterCrop(224, 224)\n        ])\n\n# 3 input channels - 3 colors\n# 40 output channels - 40 filters\n# kernel size - 3\n\nfrom random import seed\n\n#torch.seed(666)\n#seed(666)\nconvo_layer_1 = nn.Conv2d(3, 40, 3, stride=1, padding=0)\n\nimg_tensor = torch.tensor([effects(image = image)['image']\/255]).permute(0,3,1,2).float()\n\nprint(img_tensor)\n\noutput = convo_layer_1(img_tensor)\n\nprint(f\"Batch size: {output.shape[0]}\\nOutput channels: {output.shape[1]}\\nWidth: {output.shape[2]}\\nHeight: {output.shape[3]}\")\n\nplt.imshow(np.squeeze(output[0,0,:,:].detach().numpy()*255), interpolation='nearest', cmap=\"gray\")\nplt.show()","3332ff45":"from torchvision import utils\n\ndef visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n    n,c,w,h = tensor.shape\n\n    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n\n    rows = np.min((tensor.shape[0] \/\/ nrow + 1, 64))    \n    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n    print(grid.shape)\n    plt.figure( figsize=(nrow,rows) )\n    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n    print(grid.numpy().transpose((1, 2, 0)).shape)\n\n\nfilter_ = convo_layer_1.weight.data.clone()\nvisTensor(filter_, ch=0, allkernels=False)\n\nplt.axis('off')\nplt.ioff()\nplt.show()","d78a2b81":"filter_[1, 2, :, :]","779a6447":"from math import ceil\n\ndef show_layer(output):\n    print(f\"Batch size: {output.shape[0]}\\nOutput channels: {output.shape[1]}\\nWidth: {output.shape[2]}\\nHeight: {output.shape[3]}\")\n    columns = 8\n    rows = ceil(output.shape[1]\/columns)\n    fig=plt.figure(figsize=(3.125*columns, rows*3.125))\n    for i in range(1, output.shape[1] +1):\n        img = np.squeeze(output[0,i-1,:,:].detach().numpy()*255)\n        fig.add_subplot(rows, columns, i)\n        plt.imshow(img, interpolation='nearest', cmap=\"gray\")\n    plt.show()","6d591a99":"show_layer(nn.ReLU()(output))","ade9ac46":"#pooling = nn.MaxPool2d(2)\n\npooling = nn.AvgPool2d(2)\n\n#pooling = nn.AdaptiveMaxPool2d(100)\n\noutput = pooling(convo_layer_1(img_tensor))\n\nshow_layer(output)","f508beb0":"# 40 - input channels equals output layers from previous layer\n# 50 - number of filters = output channels\n# 3x3 filters\n\nconvo_layer_2 = nn.Conv2d(40, 150, 3, stride=2)\n\noutput = convo_layer_2(pooling(convo_layer_1(img_tensor)))\n\nshow_layer(output)","d2af2d01":"pooling_2 = nn.AvgPool2d(2)\n\noutput = pooling_2(convo_layer_2(pooling(convo_layer_1(img_tensor))))\n\nshow_layer(output)","6f5462c9":"drop_1 = nn.Dropout2d(0.5)\n\noutput = drop_1(pooling_2(convo_layer_2(pooling(convo_layer_1(img_tensor)))))\n\nshow_layer(output)","318ec5e6":"convo_layer_3 = nn.Conv2d(150, 100, 3, stride=1)\n\noutput = nn.ReLU()(convo_layer_3(drop_1(pooling_2(convo_layer_2(pooling(convo_layer_1(img_tensor)))))))\n\nshow_layer(output)","1f4aa26a":"batch_norm = nn.BatchNorm2d(100)\n\noutput = batch_norm(nn.ReLU()(convo_layer_3(drop_1(pooling_2(nn.ReLU()(convo_layer_2(pooling(nn.ReLU()(convo_layer_1(img_tensor))))))))))\n\nshow_layer(output)","ab7d4731":"pooling_3 = nn.AdaptiveAvgPool2d(15) # we want to have 30x30 output image\n\noutput = pooling_3( nn.ReLU()(batch_norm(nn.ReLU()(convo_layer_3(drop_1(pooling_2(nn.ReLU()(convo_layer_2(pooling(nn.ReLU()(convo_layer_1(img_tensor))))))))))\n\n                  ))\nshow_layer(output)","f0a02331":"output.shape","68a8da39":"(output == 0).float().mean()","1f2d3c45":"res = output.view(1, -1)\nres.shape","091f1437":"linear_1 = nn.Linear(output.shape[1]*output.shape[2]*output.shape[3], 256*2)","aa0e4591":"linear_1 ","296eff68":"res = linear_1(res)","66242587":"res.shape, linear_1.weight.shape, linear_1.bias.shape","66bdaf67":"linear_2 = nn.Linear(256*2, 10)","ce192eb1":"res = linear_2(res)\n\nres","05727c87":"softmax = nn.Softmax(dim=1)\n\nsoftmax(res)","1d8ccbe9":"import torch.optim as optim\n\ncriterion = torch.nn.CrossEntropyLoss()\n#optimizer = torch.optim.SGD(lr = 0.0003)","57b535c3":"target = torch.Tensor([1])#.view(-1,1)\n\nloss = criterion(softmax(res), target.long())\n\nloss.backward(retain_graph=True)","32772ccf":"loss","421b39e9":"Batch normalization:","45940023":"\nHere add dropout:\n\n","8d79ebf9":"Second linear layer:","2b4c7179":"Third convolution layer:\n\n```\nconvo_layer_3 = nn.Conv2d(50, 60, 1, stride=1)\n```","3613e0c8":"Transform this tensor to one dimension:","5a99010e":"Second pooling:","3026387c":"So, we get tensor with following shape:","d8bbea79":"## Pooling layers\n\nHere we add first max pooling layer:\n\n```\npooling = nn.MaxPool2d(2)\n```","c1ac4750":"Here we apply adaptive max pooling strategy. It allows to decide what size will have output.\n\nThird pooling:","0b7b25ec":"Second convolution layer:\n\n```\nconvo_layer_2 = nn.Conv2d(40, 50, 3, stride=1)\n```","f2f49356":"# How convolutions work\n\nThis notebook shows procces of convolution. Main goal of this notebook is demistifying of CNNs. You schould realize that convolution is a simple process of feature extraction and has no magick.\n\nFistly, let's download the photo:","a91286bf":"Fist linear layer:","70d263db":"# Feature extraction layers\n\nThe goal of those layers is creation of set of features, that describe objects that we want to classify.\n\n## Fist convolution layer\n\nThe main kind of feature extraction layers is convolution layer. This layer applies filter to image matrix to detect variuos edges on image.\n\nHere we add first convolution layer:\n\n```\nnn.Conv2d(3, 40, 3, stride=1)\n```\n\nConvolution layer in PyTorch should have count of input channels, count of output channels (filters) and kernel size (size of filter matrix).  \n\n","2cadebec":"My idea is to create prototype of neural network with following structure:\n\n```\n<-- 1 convolution layer with 40 3x3 filters -->\n                \u2193 \n<-- 1 max pooling with kernel 2x2 -->\n                \u2193\n<-- 2 convolution layer with 40 3x3 filters --> \n                \u2193\n<-- 2 max pooling with kernel 2x2 -->\n                \u2193\n<-- dropout layer 0.5 -->\n                \u2193\n<-- 3 convolution layer with 40 3x3 filters --> \n                \u2193\n<-- 3 max pooling with kernel 2x2 -->\n                \u2193\n<-- batch normalization layer -->\n                \u2193\n<-- 1 linear layer -->\n                \u2193\n<-- 2 linear layer -->\n                \u2193\n<-- softmax activation -->\n                \u2193\n<-- output layer with 10 values -->      \n```","4ed92ad7":"Finally, softmax activation for 10 neurons:"}}