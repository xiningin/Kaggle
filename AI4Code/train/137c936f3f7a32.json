{"cell_type":{"d4ba1756":"code","b7562768":"code","0c0ee259":"code","84e84e75":"code","864bc51a":"code","92da4ea1":"code","e7a1f6f3":"code","abd57139":"code","7a7d550a":"code","a3454b82":"code","2945b2ff":"code","a5e172e0":"code","59375ffc":"code","21ceeba9":"code","0e86720c":"code","c9e1333f":"code","f99f0636":"code","0a777d23":"code","796ca438":"code","9224c017":"code","7ff6df8a":"code","133199fd":"code","0b790683":"code","84fedd96":"code","8d3eb346":"code","58f0146c":"code","3e25773f":"code","6556d4c2":"markdown","6ae7d278":"markdown","037c1e5a":"markdown","009af1dc":"markdown","88e023b7":"markdown","fa2b77fc":"markdown","ef74fa05":"markdown","f122b3af":"markdown","99a37626":"markdown","d0f7f671":"markdown","24b3aece":"markdown","f1bc5ee9":"markdown","f50a45e5":"markdown","29ded666":"markdown","a06efc72":"markdown","e8927845":"markdown","4674646e":"markdown","0b01c495":"markdown","e34be60c":"markdown","39539028":"markdown","8c4e90ea":"markdown","86f96b96":"markdown","41c74b4e":"markdown","baa6f3ea":"markdown","88ed75e3":"markdown"},"source":{"d4ba1756":"%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport random as rnd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\npd.options.display.max_columns = 100\npd.options.display.max_rows = 100\nmatplotlib.style.use('ggplot')","b7562768":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain.describe()","0c0ee259":"test.describe()","84e84e75":"corr = train.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\nplt.figure(figsize=(12, 12))\nsns.heatmap(corr, vmax=1, square=True)","864bc51a":"k = 20 #number of variables for heatmap\ncols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nplt.figure(figsize=(12, 12))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","92da4ea1":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","e7a1f6f3":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","abd57139":"from scipy.stats import norm, skew\nfrom scipy import stats\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","7a7d550a":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","a3454b82":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","2945b2ff":"ntrain = train.shape[0]\nntest = test.shape[0]\n\n# get the targets\ny_train_sale = train.SalePrice.values\n\n# combine train and test\ncombined = pd.concat((train, test)).reset_index(drop=True)\ncombined.drop(['SalePrice'], axis=1, inplace=True)","a5e172e0":"all_data_na = (combined.isnull().sum() \/ len(combined)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data","59375ffc":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","21ceeba9":"combined['MasVnrArea'] = combined['MasVnrArea'].fillna(0.0)\ncombined[\"MasVnrType\"] = combined[\"MasVnrType\"].fillna(\"None\")\ncombined['LotFrontage'] = combined['LotFrontage'].fillna(combined['LotFrontage'].median())\ncombined['BsmtFinSF1'] = combined['BsmtFinSF1'].fillna(0.0)\ncombined['BsmtFinSF2'] = combined['BsmtFinSF2'].fillna(0.0)\ncombined['BsmtUnfSF'] = combined['BsmtUnfSF'].fillna(0.0)\ncombined['TotalBsmtSF'] = combined['TotalBsmtSF'].fillna(0.0)\ncombined['BsmtFullBath'] = combined['BsmtFullBath'].fillna(0)\ncombined['BsmtHalfBath'] = combined['BsmtHalfBath'].fillna(0)\ncombined['GarageYrBlt'] = combined['GarageYrBlt'].fillna(0)\ncombined['GarageCars'] = combined['GarageCars'].fillna(0)\ncombined['GarageArea'] = combined['GarageArea'].fillna(0)\ncombined['GarageFinish'] = combined['GarageFinish'].fillna('None')\n\n# using the most frequent zone\ncombined['MSZoning'] = combined['MSZoning'].fillna(combined['MSZoning'].mode()[0])\n\ncombined = combined.drop(['Utilities'], axis=1)\n\n# most common functionality\ncombined[\"Functional\"] = combined[\"Functional\"].fillna(\"Typ\")\n\ncombined['Electrical'] = combined['Electrical'].fillna(combined['Electrical'].mode()[0])\ncombined['KitchenQual'] = combined['KitchenQual'].fillna(combined['KitchenQual'].mode()[0])\ncombined['Exterior1st'] = combined['Exterior1st'].fillna(combined['Exterior1st'].mode()[0])\ncombined['Exterior2nd'] = combined['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0])\ncombined['SaleType'] = combined['SaleType'].fillna(combined['SaleType'].mode()[0])\ncombined['MSSubClass'] = combined['MSSubClass'].fillna(\"None\")\ncombined['PoolQC'] = combined['PoolQC'].fillna('None')\ncombined['MiscFeature'] = combined['MiscFeature'].fillna('None')\ncombined['Alley'] = combined['Alley'].fillna('None')\ncombined['Fence'] = combined['Fence'].fillna('None')\ncombined['FireplaceQu'] = combined['FireplaceQu'].fillna('None')\ncombined[\"LotFrontage\"] = combined.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    combined[col] = combined[col].fillna('None')\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    combined[col] = combined[col].fillna('None')","0e86720c":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(combined[c].values)) \n    combined[c] = lbl.transform(list(combined[c].values))","c9e1333f":"combined['TotalSF'] = combined['TotalBsmtSF'] + combined['1stFlrSF'] + combined['2ndFlrSF']","f99f0636":"combined = pd.get_dummies(combined)","0a777d23":"train = combined[:ntrain]\ntest = combined[ntrain:]","796ca438":"from sklearn.model_selection import train_test_split, cross_val_score\nx_train, x_test, y_train, y_test = train_test_split(train, y_train_sale, test_size=0.1, random_state=200)","9224c017":"from sklearn import ensemble, tree, linear_model\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb","7ff6df8a":"def get_score(prediction, lables):    \n    print('R2: {}'.format(r2_score(prediction, lables)))\n    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction, lables))))\n\ndef train_test(estimator, x_trn, x_tst, y_trn, y_tst):\n    prediction_train = estimator.predict(x_trn)\n    \n    get_score(prediction_train, y_trn)\n    prediction_test = estimator.predict(x_tst)\n    \n    get_score(prediction_test, y_tst)","133199fd":"GBR = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=3, max_features='sqrt',\n                                               min_samples_leaf=15, min_samples_split=10, loss='huber').fit(x_train, y_train)\ntrain_test(GBR, x_train, x_test, y_train, y_test)","0b790683":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0007000000000000001, random_state=1)).fit(x_train, y_train)\ntrain_test(lasso, x_train, x_test, y_train, y_test)","84fedd96":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11).fit(x_train, y_train)\ntrain_test(model_lgb, x_train, x_test, y_train, y_test)","8d3eb346":"GB_model = GBR.fit(train, y_train_sale)\ngbr_labels = np.expm1(GB_model.predict(test))\n\nlasso_model = lasso.fit(train, y_train_sale)\nlasso_labels = np.expm1(lasso_model.predict(test))\n\nlgb_model = model_lgb.fit(train, y_train_sale)\nlgb_labels = np.expm1(lgb_model.predict(test))","58f0146c":"# scores decided on testing for a few values\noutput = lgb_labels*0.50 + lasso_labels*0.25 + gbr_labels*0.25","3e25773f":"pd.DataFrame({'Id': test.Id, 'SalePrice': output}).to_csv('submission.csv', index =False)","6556d4c2":"# Load train and test data ","6ae7d278":"# Function for Scoring, Training and Testing","037c1e5a":"# Import Libraries","009af1dc":"**Lasso**\n> Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection\/parameter elimination.","88e023b7":"# Preprocessing","fa2b77fc":"**Top 20 variables correlated with SalePrice with score**","ef74fa05":"**Finding Skewness in SalePrice**","f122b3af":"**Finding features with NA values**","99a37626":"# Split the data into Train and Test","d0f7f671":"# What is EDA?\nExploratory Data Analysis (EDA) is an approach\/philosophy for data analysis that employs a variety of techniques (mostly graphical) to :                                                                                                  \n* Maximize insight into a data set;\n* Uncover underlying structure;\n* Extract important variables;\n* Detect outliers and anomalies;\n* Test underlying assumptions;\n* Develop parsimonious models; and\n* Determine optimal factor settings.","24b3aece":"# Import libraries for Ensemble modeling","f1bc5ee9":"# What is Ensemble Learning?\n> An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially bagging) tend to reduce problems related to over-fitting of the training data.\n\n> Empirically, ensembles tend to yield better results when there is a significant diversity among the models.Many ensemble methods, therefore, seek to promote diversity among the models they combine.Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity.","f50a45e5":"# Modeling ","29ded666":"**Filling NAs**                                                                                                          \nUsing the variable description file provided, the features with missing values can be filled as below.","a06efc72":"**Generating Dummies**","e8927845":"**Finding outliers in GrLivArea**","4674646e":"**Label Encoding of some categorical features**","0b01c495":"**Getting Correlation between variables**","e34be60c":"# *Please upvote the kernel if you find it insightful!*","39539028":"**Light GBM**\n> Light GBM is a gradient boosting framework that uses tree based learning algorithm.Light GBM is prefixed as \u2018Light\u2019 because of its high speed. It can handle the large size of data and takes lower memory to run.\n\n> It grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.","8c4e90ea":"**So SalePrice is skewed and it needs to be normally distributed.**","86f96b96":"**Deleting the 2 outliers in bottom right**","41c74b4e":"**Combining all area features to a single feature**","baa6f3ea":"**Gradient Boosting Regressor**\n> Gradient boosting is a machine learning technique for regression problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n\n> GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.","88ed75e3":"# Ensembling"}}