{"cell_type":{"f006bad5":"code","763cdf62":"code","8a0db916":"code","43ad96c8":"code","c3e85199":"code","546e220c":"code","03d079cb":"code","144b82bf":"code","fce84513":"code","0caa34ea":"code","85995061":"code","ed234257":"code","5ec0f3f8":"code","7d0d6e7d":"code","a099bd44":"markdown","5a2091c8":"markdown","99065497":"markdown","3876e031":"markdown","ab0b50a2":"markdown","23cba918":"markdown","46b353a8":"markdown","b0478935":"markdown","26261cdd":"markdown","9397c63e":"markdown","09f6b63c":"markdown","54a6ff1f":"markdown","ca19758c":"markdown","241d6ea7":"markdown","849be812":"markdown"},"source":{"f006bad5":"#All imports\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [15, 5]\nimport numpy\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom skimage import morphology\nimport openslide\n\n#Setup code\nslide_dir = \"..\/input\/prostate-cancer-grade-assessment\/train_images\/\"\nannotation_dir = \"..\/input\/prostate-cancer-grade-assessment\/train_label_masks\/\"\ntrain_data_df = pd.read_csv(\"..\/input\/prostate-cancer-grade-assessment\/train.csv\")\nsample_id_list = list(train_data_df[\"image_id\"].sample(5))\nsample_slides = [f\"{slide_dir}{slide_id}.tiff\" for slide_id in sample_id_list]\nsample_annotations = [f\"{annotation_dir}{slide_id}_mask.tiff\" for slide_id in sample_id_list]\n\ndef get_disk_size(numpy_image):\n    \"\"\" Returns size in MB of numpy array on disk.\"\"\"\n    return (numpy_image.size * numpy_image.itemsize) \/ 1000000\n\ndef plot_figures(figures, nrows = 1, ncols=1):\n    #https:\/\/stackoverflow.com\/a\/11172032\n    \"\"\"Plot a dictionary of figures.\n\n    Parameters\n    ----------\n    figures : <title, figure> dictionary\n    ncols : number of columns of subplots wanted in the display\n    nrows : number of rows of subplots wanted in the figure\n    \"\"\"\n\n    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n    for ind,title in enumerate(figures):\n        axeslist.ravel()[ind].imshow(figures[title], aspect='auto')\n        axeslist.ravel()[ind].set_title(title)\n    plt.tight_layout()\n    return plt\n\nprint(\"^ All imports and setup code in above hidden code block. ^\")","763cdf62":"def downsample(wsi, downsampling_factor=16):\n    #Select the min downsampling factor \n    #Between the input value and the available values from the slide\n    downsampling_factor = min(wsi.level_downsamples, \n                              key=lambda x: abs(x - downsampling_factor))\n    \n    #Set the level of the slide by the downsampling\n    level = wsi.level_downsamples.index(downsampling_factor)\n    \n    #Read and convert to numpy array\n    slide = wsi.read_region((0, 0), level, wsi.level_dimensions[level])\n    numpy_slide = np.array(slide)[:, :, :3]\n    \n    return numpy_slide\n\n#Set up example slide\nexample_id = \"037504061b9fba71ef6e24c48c6df44d\"\nexample_slide = f\"{slide_dir}{example_id}.tiff\"\n\n#Open slide as wsi\nwsi = openslide.open_slide(example_slide)\n\ndef display_downsample(factor):\n    display_slide = downsample(wsi, factor)\n    return (f\"Factor:\\n{factor}\\nSize (MB):\\n{get_disk_size(display_slide)}\",display_slide)\n\npotential_lvls = wsi.level_downsamples\ndownsample_results = [display_downsample(factor) for factor in potential_lvls]\ndownsample_dict = {k:v for k,v in downsample_results}\nplt = plot_figures(downsample_dict, 1, len(downsample_dict))\nplt.show()","8a0db916":"def otsu_filter(channel, gaussian_blur=True):\n    \"\"\"Otsu filter.\"\"\"\n    if gaussian_blur:\n        channel = cv2.GaussianBlur(channel, (5, 5), 0)\n    channel = channel.reshape((channel.shape[0], channel.shape[1]))\n\n    return cv2.threshold(\n        channel, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n\ndef detect_tissue(wsi, sensitivity = 3000, downsampling_factor=64):\n    \n    \"\"\"\n    Find RoIs containing tissue in WSI.\n    Generate mask locating tissue in an WSI. Inspired by method used by\n    Wang et al. [1]_.\n    .. [1] Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, Andrew\n    H. Beck, \"Deep Learning for Identifying Metastatic Breast Cancer\",\n    arXiv:1606.05718\n    \n    Parameters\n    ----------\n    wsi: OpenSlide\/AnnotatedOpenSlide class instance\n        The whole-slide image (WSI) to detect tissue in.\n    downsampling_factor: int\n        The desired factor to downsample the image by, since full WSIs will\n        not fit in memory. The image's closest level downsample is found\n        and used.\n    sensitivity: int\n        The desired sensitivty of the model to detect tissue. The baseline is set\n        at 5000 and should be adjusted down to capture more potential issue and\n        adjusted up to be more agressive with trimming the slide.\n        \n    Returns\n    -------\n    -Binary mask as numpy 2D array, \n    -RGB slide image (in the used downsampling level, in case the user is visualizing output examples),\n    -Downsampling factor.\n    \"\"\"\n    \n    # Get a downsample of the whole slide image (to fit in memory)\n    downsampling_factor = min(\n        wsi.level_downsamples, key=lambda x: abs(x - downsampling_factor))\n    level = wsi.level_downsamples.index(downsampling_factor)\n\n    slide = wsi.read_region((0, 0), level, wsi.level_dimensions[level])\n    slide = np.array(slide)[:, :, :3]\n\n    # Convert from RGB to HSV color space\n    slide_hsv = cv2.cvtColor(slide, cv2.COLOR_BGR2HSV)\n\n    # Compute optimal threshold values in each channel using Otsu algorithm\n    _, saturation, _ = np.split(slide_hsv, 3, axis=2)\n\n    mask = otsu_filter(saturation, gaussian_blur=True)\n\n    # Make mask boolean\n    mask = mask != 0\n\n    mask = morphology.remove_small_holes(mask, area_threshold=sensitivity)\n    mask = morphology.remove_small_objects(mask, min_size=sensitivity)\n\n    mask = mask.astype(np.uint8)\n    mask_contours, tier = cv2.findContours(\n        mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n\n    return mask_contours, tier, slide, downsampling_factor\n\ndef draw_tissue_polygons(mask, polygons, polygon_type,\n                              line_thickness=None):\n        \"\"\"\n        Plot as numpy array detected tissue.\n        Modeled WSIPRE github package\n        \n        Parameters\n        ----------\n        mask: numpy array \n            This is the original image represented as 0's for a starting canvas\n        polygons: numpy array \n            These are the identified tissue regions\n        polygon_type: str (\"line\" | \"area\")\n            The desired display type for the tissue regions\n        polygon_type: int\n            If the polygon_type==\"line\" then this parameter sets thickness\n\n        Returns\n        -------\n        Nunmpy array of tissue mask plotted\n        \"\"\"\n        \n        tissue_color = 1\n\n        for poly in polygons:\n            if polygon_type == 'line':\n                mask = cv2.polylines(\n                    mask, [poly], True, tissue_color, line_thickness)\n            elif polygon_type == 'area':\n                if line_thickness is not None:\n                    warnings.warn('\"line_thickness\" is only used if ' +\n                                  '\"polygon_type\" is \"line\".')\n\n                mask = cv2.fillPoly(mask, [poly], tissue_color)\n            else:\n                raise ValueError(\n                    'Accepted \"polygon_type\" values are \"line\" or \"area\".')\n\n        return mask\n    \n#Base Example\ntissue_contours, tier, downsampled_slide, downsampling_factor = detect_tissue(wsi, 3000,64)\nbase_slide_mask = np.zeros(downsampled_slide.shape[:2])\ntissue_slide = draw_tissue_polygons(base_slide_mask, tissue_contours,'line', 2)\nbase_size = get_disk_size(downsampled_slide)\nplt.imshow(tissue_slide)\nplt.show()\nprint(\"^ Code hidden above for Tissue Detection Algorithm. ^\")","43ad96c8":"#Sensitivity Tests\ndef sensitivity_test(sensitivity):\n    \"\"\"Take in a given sensitivity and return tissue_slide\"\"\"\n    tissue_contours, tier, downsampled_slide, downsampling_factor = detect_tissue(wsi, sensitivity,64)\n    base_slide_mask = np.zeros(downsampled_slide.shape[:2])\n    tissue_slide = draw_tissue_polygons(base_slide_mask, tissue_contours,'line', 2)\n    return (f\"Sensitivity:\\n{sensitivity}\",tissue_slide)\n\nto_test = [i for i in range(0,7500,1500)]\nsensitvity_results = [sensitivity_test(sensitivity) for sensitivity in to_test]\nsensitvity_dict = {k:v for k,v in sensitvity_results}\nsensitvity_dict[\"Basic Slide\"] = downsampled_slide\nplt = plot_figures(sensitvity_dict, 1, len(sensitvity_dict))\nplt.show()","c3e85199":"def tissue_cutout(tissue_slide, tissue_contours, slide):\n    #https:\/\/stackoverflow.com\/a\/28759496\n    crop_mask = np.zeros_like(tissue_slide) # Create mask where white is what we want, black otherwise\n    cv2.drawContours(crop_mask, tissue_contours, -1, 255, -1) # Draw filled contour in mask\n    tissue_only = np.zeros_like(slide) # Extract out the object and place into output image\n    tissue_only[crop_mask == 255] = slide[crop_mask == 255]\n    return tissue_only\n\ntissue_only_slide = tissue_cutout(tissue_slide, tissue_contours, downsampled_slide)\nplt.imshow(tissue_only_slide)\nplt.show()\n\ncurrent_size = get_disk_size(tissue_only_slide)\ncurrent_pct = current_size \/ base_size\nprint(f\"Slide Size on Disk: {current_size:.2f}MB\")\nprint(f\"% of original image: {current_pct*100:.2f}%\")","546e220c":"min_rect_bounding = tissue_only_slide.copy()\nfor c in tissue_contours:\n    rect = cv2.minAreaRect(c)\n    box = cv2.boxPoints(rect)\n    box = np.int0(box)\n    cv2.drawContours(min_rect_bounding,[box],0,(0,255,255),4)\nplt.imshow(min_rect_bounding)\nplt.show()","03d079cb":"simple_rect_bound = min_rect_bounding.copy()\nboxes = []\nfor c in tissue_contours:\n    (x, y, w, h) = cv2.boundingRect(c)\n    boxes.append([x,y, x+w,y+h])\n\nboxes = np.asarray(boxes)\nleft = np.min(boxes[:,0])\ntop = np.min(boxes[:,1])\nright = np.max(boxes[:,2])\nbottom = np.max(boxes[:,3])\n\ncv2.rectangle(simple_rect_bound, (left,top), (right,bottom), (255, 0, 0), 4)\n\nplt.imshow(simple_rect_bound)\nplt.show()","144b82bf":"smart_bounding_boxes = min_rect_bounding.copy()\nall_bounding_rect = cv2.minAreaRect(np.concatenate(tissue_contours))\nall_bounding_box = cv2.boxPoints(all_bounding_rect)\nall_bounding_box = np.int0(all_bounding_box)\ncv2.drawContours(smart_bounding_boxes,[all_bounding_box],0,(255,0,0),4)\nplt.imshow(smart_bounding_boxes)\nplt.show()","fce84513":"simple_crop = smart_bounding_boxes.copy()\ncrop_mask = np.zeros_like(simple_crop)\n(y, x) = np.where(tissue_slide == 1)\n(topy, topx) = (np.min(y), np.min(x))\n(bottomy, bottomx) = (np.max(y), np.max(x))\nsimple_crop = simple_crop[topy:bottomy+1, topx:bottomx+1]\nplt.imshow(simple_crop)\nplt.show()\n\ncurrent_size = get_disk_size(simple_crop)\ncurrent_pct = current_size \/ base_size\nprint(f\"Slide Size on Disk: {current_size:.2f}MB\")\nprint(f\"% of original image: {current_pct*100:.2f}%\")","0caa34ea":"def getSubImage(rect, src_img):\n    width = int(rect[1][0])\n    height = int(rect[1][1])\n    box = cv2.boxPoints(rect)\n\n    src_pts = box.astype(\"float32\")\n    dst_pts = np.array([[0, height-1],\n                        [0, 0],\n                        [width-1, 0],\n                        [width-1, height-1]], dtype=\"float32\")\n    M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n    warped = cv2.warpPerspective(src_img, M, (width, height))\n    return warped\n\nsmart_bounding_crop = smart_bounding_boxes.copy()\nsmart_bounding_crop = getSubImage(all_bounding_rect,smart_bounding_crop)\nplt.imshow(smart_bounding_crop)\nplt.show()\n\ncurrent_size = get_disk_size(smart_bounding_crop)\ncurrent_pct = current_size \/ base_size\nprint(f\"Slide Size on Disk: {current_size:.2f}MB\")\nprint(f\"% of original image: {current_pct*100:.2f}%\")","85995061":"row_not_blank =  [row.all() for row in ~np.all(smart_bounding_crop == [255,   0,   0],axis=1)]\ncol_not_blank =  [col.all() for col in ~np.all(smart_bounding_crop == [255,   0,   0],axis=0)]\nbounded_cut = smart_bounding_crop[row_not_blank,:]\nbounded_cut = bounded_cut[:,col_not_blank]\nplt.imshow(bounded_cut)\nplt.show()\n\ncurrent_size = get_disk_size(bounded_cut)\ncurrent_pct = current_size \/ base_size\nprint(f\"Slide Size on Disk: {current_size:.2f}MB\")\nprint(f\"% of original image: {current_pct*100:.2f}%\")","ed234257":"def detect_and_crop(image_location:str, sensitivity:int=3000, \n                    downsample_rate:int=16, show_plots:str=\"simple\"):\n    \n    #Set-up dictionary for plotting\n    verbose_plots = {}\n    \n    #Open Slide\n    wsi = openslide.open_slide(image_location)\n    \n    #Get returns from detect_tissue()\n    (tissue_contours, tier, \n     downsampled_slide, \n     downsampling_factor) = detect_tissue(wsi,\n                                          sensitivity,downsample_rate)\n    #Add Base Slide to verbose print\n    verbose_plots[f\"Base Slide\\n{get_disk_size(downsampled_slide):.2f}MB\"] = downsampled_slide\n    \n    #Get Tissue Only Slide\n    base_slide_mask = np.zeros(downsampled_slide.shape[:2])\n    tissue_slide = draw_tissue_polygons(base_slide_mask, tissue_contours,'line', 5)\n    base_size = get_disk_size(downsampled_slide)\n    tissue_only_slide = tissue_cutout(tissue_slide, tissue_contours, downsampled_slide)\n    #Add Tissue Only to verbose print\n    verbose_plots[f\"Tissue Detect\\nNo Change\"] = tissue_slide\n    \n    #Get minimal bounding rectangle for all tissue contours\n    if len(tissue_contours) == 0:\n        img_id = image_location.split(\"\/\")[-1]\n        print(f\"No Tissue Contours - ID: {img_id}\")\n        return None, 1.0\n    \n    all_bounding_rect = cv2.minAreaRect(np.concatenate(tissue_contours))\n    #Crop with getSubImage()\n    smart_bounding_crop = getSubImage(all_bounding_rect,tissue_only_slide)\n    #Add Bounding Boxes to verbose print\n    verbose_plots[f\"Bounding Boxes\\n{get_disk_size(smart_bounding_crop):.2f}MB\"] = smart_bounding_crop\n\n    #Crop empty space\n    #Remove by row\n    row_not_blank =  [row.all() for row in ~np.all(smart_bounding_crop == [255,0,0],\n                                                   axis=1)]\n    space_cut = smart_bounding_crop[row_not_blank,:]\n    #Remove by column\n    col_not_blank =  [col.all() for col in ~np.all(smart_bounding_crop == [255,0,0],\n                                                   axis=0)]\n    space_cut = space_cut[:,col_not_blank]\n    #Add Space Cut Boxes to verbose print\n    verbose_plots[f\"Space Cut\\n{get_disk_size(space_cut):.2f}MB\"] = space_cut\n    \n    #Get size change\n    start_size = get_disk_size(downsampled_slide)\n    final_size = get_disk_size(space_cut)\n    pct_change = final_size \/ start_size\n    \n    if show_plots == \"simple\":\n        print(f\"Percent Reduced from Base Slide to Final: {(1- pct_change)*100:.2f}\")\n        plt.imshow(space_cut)\n        plt.show() \n    elif show_plots == \"verbose\":\n        print(f\"Percent Reduced from Base Slide to Final: {(1- pct_change)*100:.2f}\")\n        plt = plot_figures(verbose_plots, 1, len(verbose_plots))\n        plt.show()\n    elif show_plots == \"none\":\n        pass\n    else:\n        pass\n    return space_cut, (1-pct_change)\n    \nnumpy_result, pct_change = detect_and_crop(image_location=example_slide, show_plots=\"verbose\")","5ec0f3f8":"for sample in sample_slides:\n    detect_and_crop(image_location=sample, show_plots=\"verbose\")","7d0d6e7d":"from statistics import mean \nfrom multiprocessing import Pool\nfrom tqdm.notebook import tqdm\nimport gc\nbaseline_pct = .05\nbaseline_count = int(baseline_pct*len(train_data_df))\nbaseline_slide_ids = list(train_data_df[\"image_id\"].sample(baseline_count))\nbaseline_slide_locs = [f\"{slide_dir}{slide_id}.tiff\" for slide_id in baseline_slide_ids]\n#Nested in funciton to not take up more memory and allow mulitprocessing\ndef baseline_check(image_id):\n    numpy_result, pct_change = detect_and_crop(image_location=image_id, show_plots=\"none\")\n    del numpy_result\n    gc.collect()\n    return pct_change\n\nwith Pool(processes=4) as pool:\n    avg_pct_reduced = list(\n        tqdm(pool.imap(baseline_check, baseline_slide_locs), total = len(baseline_slide_locs))\n    )\n\nprint(f\"The averge size reduced reduced from a {baseline_pct:.0%} sample of slides is {mean(avg_pct_reduced):.2%}\")","a099bd44":"# Next Steps\n1. Exploring how this methodology effects the mask slides\n2. Timing the processing and making it more efficient\n3. Checking Effectiveness on lower downsamples","5a2091c8":"# Cropping the Image\n\n## Basic cropping \nNow that ROIs have been indentified we can now crop the image based on the bounding boxes. We start with a simple rectangluar crop","99065497":"# Sensitivity Exploration\nWhen considering what sensitivty to choose we dont want a high enough number to remove noise, but also a low enough number to make sure we capture all of the tissue. Initial research leads to a conclusion of 3000, but I want to do more work on this in next steps.","3876e031":"## Getting Simple Rectangle Bounding Box\nThis example makes a simple rectangle that captures all of the identified countors\n","ab0b50a2":"# Cutting out all space except for the identified Tissue\nOnce we have identified a given sensitivity and the areas it identifies as tissue we can then cut out the rest of the image. As you can see even thought we have \"cut out\" all non-tissue parts of the slide the size has not changed.","23cba918":"# Baseline for Corpus: 67.8%\nHere we try to get a average for the reduciton capacity of this code across the data corpus. We do this by sampling from the whole and taking an average of the recoded size reduciton. I have run the sampler over the whole corpus and come out with 68.95%. For speed of commiting the notebook,  I have it set to only 5% in this notebook. ","46b353a8":"# Whole Pipeline: Start Here for Code Only\nNow that we have explained each of the steps along the way we can put the whole pipeline together.","b0478935":"## Remove space between countors in final bounding box\nNow that we have everything but the minimum bounding boxes we can cut any blank space between the inner most areas of the contours. For visuals the bounding boxes were left, but in the full pipeline below they are removed.","26261cdd":"## Crop with Smart Bounding and Rotation to Minimum Size\nTo better reduce size we take the Smart Bounding Box we calulated and rotate the image to minimize the size while keeping everything within the bounding box. Since Prostate Biopsies are not orientated in a specific way this does not effect prediciton.","9397c63e":"# Tissue Detection and Size Optimization\n\n## Objective:\nThis notebook leverages academic research and previous repositories in pathology to idnetiify regions of interst (ROIs) in provided slides and optimally crop the full image to contain a the maximum amount of tissue informaiton while minimizing the overall size. This can be used as a preprocessing step in order to remove large swaths of the images that do not conatin meningful data for prediciton. It also greatly improves speed and memory capacity.\n\n## Results:\nI have [another notebook](https:\/\/www.kaggle.com\/dannellyz\/tissue-detect-png-512x512-pre-process\/) that takes the lessons from this process and directly applies them to the data corpus as pre-processing. Additionally I have made the [pre-processed images](https:\/\/www.kaggle.com\/dannellyz\/panda-preprocessing-tissue-detection\/) available as well.","09f6b63c":"# Getting Bounding Boxes\n\n### Starting with simple boxes\nThe first method experimented with finds a minimum bounding rectangle for all of the identified countors.","54a6ff1f":"## Min-Rectagle Bounding Boxes\nWith the goal of minimizing the total area of the images the next method implemented concatenates all tissue contours found and builds a cumulative minimum boudning rectanlge.","ca19758c":"![tissue_logo.001.jpeg](attachment:tissue_logo.001.jpeg)\n\nTissue Detection is a key aspect to research in the domain of computer vision applied to cancer classification. My main focus in this competition so far has been exploring previous work done in this domain and furthering its application towards this dataset. Notebooks in this collection include the following.\n* [Base Notebook **(Currently Here)**](https:\/\/www.kaggle.com\/dannellyz\/panda-tissue-detection-size-optimization-70) : Tissue Detection Intro and First Application\n* [Base Dataset Generation](https:\/\/www.kaggle.com\/dannellyz\/tissue-detect-td-conv-png-512x512): Notebook to export images to zip file\n* [Scaling Bounding Boxes](https:\/\/www.kaggle.com\/dannellyz\/tissue-detect-scaling-bounding-boxes-4xfaster): 4x speed increase to base notebook\n* [Tissue Dection Metadata Analysis](https:\/\/www.kaggle.com\/dannellyz\/tissue-detection-bounding-box-metadata-eda-viz\/): Exploring features from bounding boxes discovery on the slides","241d6ea7":"# Detecting Tissue\n\nThis notebook draws on the work of [Lu\u00eds](https:\/\/github.com\/luisvalesilva) and his [WSIPRE](https:\/\/github.com\/luisvalesilva\/wsipre) work available on Github.\n\n## Leveraging Academic Research\n\nPreviously pubilshed work in the domain of [Deep Learning for Identifying Metastatic Breast Cancer](https:\/\/arxiv.org\/pdf\/1606.05718.pdf) lends itself nicely to the detection on Prostate Cells in the PANDA challenge esepcially in the area of pre-processing. An excerpt from the paper explaining the methodology: \n\n> To reduce computation time and to focus our analysis on regions of the slide most likely to contain cancer metastasis, we first identify tissue within the WSI and exclude background white space. To achieve this, we adopt a threshold based segmentation method to automatically detect the background region. In particular, we first transfer the original image from the RGB color space to the HSV color space, then the optimal threshold values in each channel are computed using the [Otsu algorithm](https:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?tp=&arnumber=4310076), and the final mask images are generated by combining the masks from H and S channels.\n\nThis work informs the method `def detect_tissue`:\n\n    Find RoIs containing tissue in WSI.\n    Generate mask locating tissue in an WSI. Inspired by method used by\n    Wang et al. [1]_.\n    .. [1] Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, Andrew\n    H. Beck, \"Deep Learning for Identifying Metastatic Breast Cancer\",\n    arXiv:1606.05718\n    \n    Parameters\n    ----------\n    wsi: OpenSlide\/AnnotatedOpenSlide class instance\n        The whole-slide image (WSI) to detect tissue in.\n    downsampling_factor: int\n        The desired factor to downsample the image by, since full WSIs will\n        not fit in memory. The image's closest level downsample is found\n        and used.\n    sensitivity: int\n        The desired sensitivty of the model to detect tissue. The baseline is set\n        at 5000 and should be adjusted down to capture more potential issue and\n        adjusted up to be more agressive with trimming the slide.\n        \n    Returns\n    -------\n    -Binary mask as numpy 2D array, \n    -RGB slide image (in the used downsampling level, in case the user is visualizing output examples),\n    -Downsampling factor.\n    \nThe method `def draw_tissue_polygons`, from WSIPRE, allows for vizualizaiton of this deteciton:\n\n    Parameters\n    ----------\n    wsi: OpenSlide\/AnnotatedOpenSlide class instance\n        The whole-slide image (WSI) to detect tissue in.\n    downsampling_factor: int\n        The desired factor to downsample the image by, since full WSIs will\n        not fit in memory. The image's closest level downsample is found\n        and used.\n    sensitivity: int\n        The desired sensitivty of the model to detect tissue. The baseline is set\n        at 5000 and should be adjusted down to capture more potential issue and\n        adjusted up to be more agressive with trimming the slide.\n    Returns\n    -------\n    Binary mask as numpy 2D array, RGB slide image (in the used\n    downsampling level, in case the user is visualizing output examples)\n    and downsampling factor.\n\n","849be812":"# Getting Strated with the Numpy Images\n\n## Basic Slide Downsample\nDownsampling is a process to take the Whole Slide Image (WSI) and get lower lever representations of the slide. The higher the downsampling rate the less resolution the image will have. to start this walk through we use a very high downsampling in order to prove the concept and at the end with a low downsampling to show the effectiveness.\n![Downsample Example](http:\/\/dicom.nema.org\/Dicom\/DICOMWSI\/sup145_fromword_files\/image010.gif)\n\n## As Numpy Array\nOnce the image has been downsampled there are many ways it could be represented as best described in the Notebook [Getting Started with the PANDA Dataset -> Using Matplotlib](https:\/\/www.kaggle.com\/wouterbulten\/getting-started-with-the-panda-dataset#Visualizing-masks-(using-matplotlib)). For this notebook we will be working with the slides as Numpy Arrays. As the below examples illustrate the choosen downsample has a big imapct on the size of the representation"}}