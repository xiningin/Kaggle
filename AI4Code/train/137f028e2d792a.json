{"cell_type":{"485507a6":"code","7a6dfa63":"code","0fd96d43":"code","9e5f25fb":"code","e0772607":"code","d045338f":"code","406680e5":"code","d4442711":"code","faf5055e":"code","8027f2ea":"code","40833bce":"code","555e6569":"code","dd2e3010":"code","77cd40b7":"code","27e600ba":"code","c7376257":"code","7900b10f":"code","2169e640":"code","3cb0e63d":"code","0d8a586d":"code","230f84c8":"code","50a1552d":"code","8b1d4a9b":"code","ee5be007":"code","ebafe923":"code","55ce4393":"markdown","70b683a4":"markdown","b54c6370":"markdown","591d9b95":"markdown","916df7cf":"markdown","deb3bd8e":"markdown","f9346ce1":"markdown","2d6d96dd":"markdown","cadc2933":"markdown","7e074605":"markdown","8e3fe887":"markdown","c79cd4a6":"markdown","f5b0e25b":"markdown","0c0ac6f9":"markdown","c4314a3d":"markdown","9c6fc05a":"markdown","f1412527":"markdown","d0603dfe":"markdown","6d6c9854":"markdown","41e35357":"markdown","13d3c5eb":"markdown","7885af7f":"markdown","333a430a":"markdown","acfccc83":"markdown","50cb6aac":"markdown","fa4d2520":"markdown"},"source":{"485507a6":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import KNNImputer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom matplotlib.pyplot import figure\nfrom sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, MinMaxScaler\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\n\n\n%pip install ppscore\nimport ppscore as pps","7a6dfa63":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprint(\"length of train_data : {}\".format(len(train_data)))\nprint(\"length of test_data : {}\".format(len(test_data)))\n\n# check if it is balanced\ntrain_data.Survived.value_counts()","0fd96d43":"display(train_data.head(20))\nprint(\"NULL VALUES IN TRAINING DATA\")\nprint(train_data.isna().sum())\nprint(\"NULL VALUES IN TEST DATA\")\nprint(test_data.isna().sum())\n","9e5f25fb":"# check, is there any correlation between first letter of cabin and surviving\n# null values are replaced with the letter \"N\"\nfor df in [train_data, test_data]:\n    df[\"Cabin\"].loc[df[\"Cabin\"].notnull()] = df[\"Cabin\"].loc[df[\"Cabin\"].notnull()].apply(lambda x: x[0])\n    df[\"Cabin\"].fillna(\"N\", inplace = True)\n\nsns.barplot(x = \"Cabin\", y = \"Survived\", data= train_data.sort_values(\"Cabin\"), orient = \"v\")\n\ntrain_data.Cabin.value_counts()","e0772607":"for df in [train_data, test_data]:\n    df[\"Cabin\"].loc[df[\"Cabin\"] != \"N\"] = 1\n    df[\"Cabin\"].loc[df[\"Cabin\"] == \"N\"] = 0\nsns.barplot(x = \"Cabin\", y = \"Survived\", data= train_data, orient = \"v\")\n\ntrain_data.Cabin.value_counts()\n","d045338f":"for df in [train_data, test_data]:\n    df.replace(\"male\", 0, inplace = True)\n    df.replace(\"female\", 1, inplace = True)","406680e5":"for df in [train_data, test_data]:\n    df[\"Title\"] = df[\"Name\"].apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip())\n    \ntitles = np.unique(train_data[\"Title\"])\n\nfor title in titles: \n    count = (train_data[\"Title\"] == title).sum()\n    temp = train_data[train_data[\"Title\"] == title][\"Survived\"]\n    surviving_perc = temp.sum() \/ len(temp)\n    print(\"{} occured {} times -- surviving percentage is : {}\".format(title, count, surviving_perc))","d4442711":"for df in [train_data, test_data]:\n    df.replace([\"Capt\", \"Col\", \"Don\", \"Dona\", \"Dr\", \"Jonkheer\", \"Major\",\"Rev\"], \"Others\", inplace = True)\n    df.replace([\"Miss\", \"Mlle\", \"Mme\", \"Mrs\", \"Ms\"], \"Mrs\", inplace = True)\n    df.replace([\"Lady\", \"Sir\", \"the Countess\"], \"Royal\", inplace = True)\n\nsns.barplot(x = \"Title\", y = \"Survived\", data= train_data, orient = \"v\")\n\ntrain_data.Title.value_counts()\n","faf5055e":"train_data[\"Family Number\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\ntest_data[\"Family Number\"] = test_data[\"SibSp\"] + train_data[\"Parch\"]\n\ntest_data[\"Alone\"] = np.nan\ntrain_data[\"Alone\"] = np.nan\n\nfor df in [test_data, train_data]:\n    df[\"Alone\"].loc[df[\"Family Number\"] == 0 ] = 1\n    df[\"Alone\"].loc[df[\"Family Number\"] != 0 ] = 0\n    \nsns.barplot(x = \"Alone\", y = \"Survived\", data= train_data, orient = \"v\")\n","8027f2ea":"combine = pd.concat([train_data, test_data])\ntickets = np.unique(combine[\"Ticket\"])\ncombine[\"GroupSize\"] = 0\n# combine[\"Same Ticket Surv Perc\"] = 0\n\nfor ticket in tickets: \n    group_size = len(combine.loc[combine[\"Ticket\"] == ticket,\"Survived\"])\n    combine.loc[combine[\"Ticket\"] == ticket, \"GroupSize\"] = group_size\n","40833bce":"combine[\"Family Size\"] = np.nan\ncombine.loc[(combine[\"Family Number\"] == 0), \"Family Size\"] = \"Small\"\ncombine.loc[(combine[\"Family Number\"] > 0) & (combine[\"Family Number\"] < 4), \"Family Size\" ] = \"Medium\"\ncombine.loc[(combine[\"Family Number\"] >= 4), \"Family Size\"] = \"Big\"\n","555e6569":"combine[\"Embarked\"].value_counts()\ncombine[\"Embarked\"].fillna(\"S\", inplace = True)\ncombine['Fare'] = combine['Fare'].fillna(combine.groupby('Pclass')['Fare'].transform('mean'))","dd2e3010":"train_data = combine.loc[combine[\"Survived\"].notnull()]\nfig, axs = plt.subplots(3,3, figsize = (15,10))\n\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train_data, orient=\"v\", ax=axs[0][0])\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train_data, orient=\"v\", ax=axs[0][1])\nsns.barplot(x=\"Family Size\", y=\"Survived\", data=train_data, orient=\"v\", ax=axs[0][2])\nsns.barplot(x=\"Cabin\", y=\"Survived\", data=train_data, orient=\"v\", ax=axs[1][0])\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=train_data, orient=\"v\",ax=axs[1][1])\nsns.barplot(x=\"Title\", y=\"Survived\", data=train_data, orient=\"v\",ax=axs[1][2])\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train_data, ax = axs[2][0])\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train_data, ax = axs[2][1])\nsns.barplot(x=\"Alone\", y=\"Survived\", data=train_data, ax = axs[2][2])\n\nplt.figure(figsize=(10,6))\nsns.catplot(x=\"Survived\", y=\"Fare\", data=train_data)\n\n\nplt.figure(figsize=(10,6))\nsns.catplot(x=\"Survived\", y=\"Age\", data=train_data)\n","77cd40b7":"encoded_columns = [\"Title\", \"Embarked\", \"Family Size\"]\n\nencoded_features = []\nfor column in encoded_columns:\n    encoded_arr = OneHotEncoder().fit_transform(combine[column].values.reshape(-1, 1)).toarray()\n    n = combine[column].nunique()\n    cols = ['{}_{}'.format(column, n) for n in range(1, n + 1)]\n    encoded_df = pd.DataFrame(encoded_arr, columns=cols)\n    encoded_df.index = combine.index\n    encoded_features.append(encoded_df)\ncombine = pd.concat([combine, *encoded_features[:3]], axis=1)\n\ndrop_columns = encoded_columns + [\"PassengerId\", \"Name\",\"Ticket\"]\ncombine.drop(columns = drop_columns , inplace = True)\n\nsurvived = combine[\"Survived\"]\ncombine.drop(columns = [\"Survived\"], inplace = True)","27e600ba":"imputer = KNNImputer()\nimputer.fit(combine)\ncombine[:] = imputer.transform(combine)","c7376257":"combine[\"AgeGroup\"] = \"old\"\ncombine.loc[combine[\"Age\"] <= 15, \"AgeGroup\"] = \"children\"\ncombine.loc[(combine[\"Age\"] > 15) & (combine[\"Age\"] <= 25), \"AgeGroup\"] = \"young\"\ncombine.loc[(combine[\"Age\"] > 25) & (combine[\"Age\"] <= 35), \"AgeGroup\"] = \"adult\"\ncombine.loc[(combine[\"Age\"] > 35) & (combine[\"Age\"] <= 55), \"AgeGroup\"] = \"mature\"\n\ncombine[\"AgeGroup\"].value_counts()","7900b10f":"combine[\"Survived\"] = survived\ntrain_data = combine.loc[combine[\"Survived\"].notnull()]\ntest_data = combine.loc[combine[\"Survived\"].isna()]\n\nsns.barplot(x = \"AgeGroup\", y = \"Survived\", data= train_data, orient = \"v\")\n","2169e640":"encoded_arr = OneHotEncoder().fit_transform(combine[\"AgeGroup\"].values.reshape(-1, 1)).toarray()\nencoded_features = []\nn = combine[\"AgeGroup\"].nunique()\ncols = ['{}_{}'.format(\"AgeGroup\", n) for n in range(1, n + 1)]\nencoded_df = pd.DataFrame(encoded_arr, columns=cols)\nencoded_df.index = combine.index\nencoded_features.append(encoded_df)\ncombine = pd.concat([combine, *encoded_features[:1]], axis=1)\n\ncombine.drop(columns = [\"AgeGroup\"], inplace = True)\n","3cb0e63d":"scaler = MinMaxScaler()\nscaler.fit(combine)\ncombine[:] = scaler.transform(combine)\n\ncombine[\"Survived\"] = survived\ntrain_data = combine.loc[combine[\"Survived\"].notnull()]\ntest_data = combine.loc[combine[\"Survived\"].isna()]","0d8a586d":"\nfigure(num=None, figsize=(20, 20), dpi=80, facecolor='w', edgecolor='k')\nVar_Corr = train_data.corr()\nsns.heatmap(Var_Corr, xticklabels=Var_Corr.columns, yticklabels=Var_Corr.columns, annot=True)","230f84c8":"figure(num=None, figsize=(20, 20), dpi=80, facecolor='w', edgecolor='k')\n\nmatrix_df = pps.matrix(train_data)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\nmatrix_df = matrix_df.apply(lambda x: round(x, 2)) # Rounding matrix_df's values to 0,XX\n\nsns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.75, annot=True)","50a1552d":"combine.drop(columns = [\"Age\",\"Parch\", \"SibSp\", \"GroupSize\", \"Family Number\", \"Title_1\",\"Title_4\", \"Title_5\", \"Embarked_2\",\"AgeGroup_1\", \"AgeGroup_3\", \"AgeGroup_4\", \"AgeGroup_5\"], inplace = True)\n\ntrain_data = combine.loc[combine[\"Survived\"].notnull()]\ntest_data = combine.loc[combine[\"Survived\"].isna()]\n\nX = train_data.drop(columns = [\"Survived\"])\ny = train_data[\"Survived\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=42)","8b1d4a9b":"#construct pipeline\n\nfeature_engineering = [\n    None,\n    PCA(n_components = round(X.shape[1]*0.9)),\n    PCA(n_components = round(X.shape[1]*0.8)),\n    PCA(n_components = round(X.shape[1]*0.7)),\n    PCA(n_components = round(X.shape[1]*0.6)),\n    PCA(n_components = round(X.shape[1]*0.5)),\n    PCA(n_components = round(X.shape[1]*0.3)),\n    PCA(n_components = round(X.shape[1]*0.1)),\n    PolynomialFeatures(degree = 1),\n    PolynomialFeatures(degree = 2),\n    PolynomialFeatures(degree = 3)\n]\n\nparams = [\n    {\n        \"feature_eng\": feature_engineering,\n        \"clf\": [GaussianNB()]\n    },\n    {\n        \"feature_eng\": feature_engineering,\n        \"clf\": [SGDClassifier()],\n        \"clf__loss\": [\"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n        \"clf__penalty\": [\"l1\", \"l2\", \"elasticnet\"]\n    },   \n    {\n        \"feature_eng\": feature_engineering,\n        \"clf\": [DecisionTreeClassifier()],\n        \"clf__criterion\": [\"gini\", \"entropy\"],\n        \"clf__max_depth\": [3,4,5,6,7,10,15]        \n    },   \n    {\n        \"feature_eng\": feature_engineering,\n        \"clf\": [RandomForestClassifier()],\n        \"clf__max_depth\": [3,4,5,6,7,10,15]\n    }, \n    {\n        \"feature_eng\": feature_engineering,\n        \"clf\": [SVC()],\n        \"clf__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"]\n    }, \n    {\n        \"feature_eng\": feature_engineering,\n        \"clf\": [KNeighborsClassifier()],\n        \"clf__n_neighbors\": [3,5,10,15,30,40]\n    }, \n    {\n        \"feature_eng\": feature_engineering,\n        \"clf\": [GradientBoostingClassifier()],\n        \"clf__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n        \"clf__n_estimators\": [70,100,120,150]\n    }, \n    {\n        \"feature_eng\": feature_engineering,\n        \"clf\": [XGBClassifier()],\n        \"clf__n_estimators\": [5,10,30,50,70,100,120]\n    }, \n]\npipe = Pipeline(steps=[('feature_eng', None),('clf', DecisionTreeClassifier())])","ee5be007":"clf = RandomizedSearchCV(pipe, params, n_iter=200, scoring=\"accuracy\", refit='accuracy',n_jobs=-1, cv=5, random_state=42)\nclf.fit(X_train, y_train)\n\npd.DataFrame(clf.cv_results_).sort_values(by = [\"mean_test_score\"], ascending = False).iloc[:10]","ebafe923":"y_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred)","55ce4393":"**Drop low correlated features and split data into training and test data.**","70b683a4":"Let's fill missing values for \"Embarked\" and \"Fare\" before we impute \"Age\".","b54c6370":"Import data and check if it is balanced.","591d9b95":"Test model","916df7cf":"# -Family Size","deb3bd8e":"# KNNImputer for missing values of \"Age\".","f9346ce1":"**First Assumptions:**\n* First letter of **\"Cabin\"** looks meaningfull.\n* Too many null values for **Cabin**. We should check if there is any relationship between Cabin being null and **\"Survived\"**. If not, we should drop **Cabin**.\n* We can extract **titles** from **\"Name\"** and add as a new feature.\n* After getting **titles**, we can drop **\"Name\"**.\n* People with same Ticket Id should be a family or group. We can create **\"GroupSize\"** as a new feature.\n* We can create a new feature named **\"Family Size\"** with summing up **\"Parch\"** and **\"SibSp\"** variables. \n* We can fill null values for **\"Embarked\"** with most frequent value.\n* We can fill null values for **\"Fare\"** with median or mean.\n* We should impute **\"Age\"**.\n* Check relationships between rest of the features and **\"Survived\"**.","2d6d96dd":"Have a quick look on some features.","cadc2933":"# -Ticket to GroupSize","7e074605":"Let's transform \"Sex\" feature to numeric values. 0 for male, 1 for female.","8e3fe887":"# Linear Correlation HeatMap","c79cd4a6":"# Training Model\n\nI will use pipeline for workflow. And RandomizedSearchCV to find the best model. ","f5b0e25b":"# Data Exploration and Feature Engineering","0c0ac6f9":"# - Name","c4314a3d":"Looks like data is balanced, so we don't need to resample.\nNot lets have a quick look on data.","9c6fc05a":"As we can see, Age, SibSp, Parch, Family Number, GroupSize, Title_1, Title_4, Title_5, Embarked_2 features have low correlation with Survived. We may consider to drop these features before training. But first, let's check non-linear correlation aswell.","f1412527":"# OneHotEncoder for categorical features. ","d0603dfe":"\"Title\" has high cardinality and most of the values are rare. We should re-group values according to their similarities. ","6d6c9854":"Let's create new feature called **\"AgeGroup\"**","41e35357":"# -Family Size","13d3c5eb":"Search for the best model.","7885af7f":"We can see that **children** most likely to survive. So, we can keep that variable. \nNow, OneHotEncode this feature too.","333a430a":"# - Cabin","acfccc83":"# Import Necessary Libraries","50cb6aac":"We can see that approximately %30 of null values are not survived. Null's are less likely to survive compare to non-null values. We can rearrange \"N\" as 1, and 0 for the others. ","fa4d2520":"#  Scale the data for better performance in model."}}