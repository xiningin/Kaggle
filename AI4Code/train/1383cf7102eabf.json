{"cell_type":{"24f11ad8":"code","51497550":"code","5a41a3ec":"code","9b50a96c":"code","16232fea":"code","324678ec":"code","5ad7a2a6":"code","28f59666":"code","8b319160":"code","764a395d":"code","edb2ae00":"code","6196d854":"code","5b95ec86":"code","efaa1984":"code","7d5e4efc":"code","b0b5f314":"code","68663552":"markdown","937af46f":"markdown","0c4be696":"markdown","1db31545":"markdown","0077e87b":"markdown","4c1ea141":"markdown"},"source":{"24f11ad8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51497550":"data = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndata.head()","5a41a3ec":"data = data.drop('Id', axis=1)\ndata.head()","9b50a96c":"data.shape","16232fea":"data.info()","324678ec":"data.describe()","5ad7a2a6":"data.isnull().any()","28f59666":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt","8b319160":"sns.set(style='darkgrid')\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection = '3d')\n\n#setosa\nXs = data.iloc[:50, 0]\nYs = data.iloc[:50, 1]\nZs = data.iloc[:50, 2]\n\n#versicolor\nXve = data.iloc[50:100, 0]\nYve = data.iloc[50:100, 1]\nZve = data.iloc[50:100, 2]\n\n#virginica\nXvi = data.iloc[100:, 0]\nYvi = data.iloc[100:, 1]\nZvi = data.iloc[100:, 2]\n\ncol = ['red', 'blue', 'green']\nax.set_xlabel(\"SepalLengthCm\")\nax.set_ylabel(\"SepalWidthCm\")\nax.set_zlabel(\"PetalLengthCm\")\n\nax.scatter(Xs, Ys, Zs, c='r')\nax.scatter(Xve, Yve, Zve, c='b')\nax.scatter(Xvi, Yvi, Zvi, c='g')\n\nplt.show()","764a395d":"def entropy(y):\n    hist = np.bincount(y)\n    ps = hist\/len(y)\n    return - np.sum([p * np.log2(p) for p in ps if p > 0])","edb2ae00":"class Node:\n    \n    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n    \n    def is_leaf_node(self):\n        return self.value is not None","6196d854":"class Decission_Tree:\n    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.root = None\n    \n    def predict(self, X):\n        X = X.to_numpy()\n        return np.array([self.traverse_tree(x, self.root) for x in X])\n    \n    def fit(self, X, y):\n        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n        X = X.to_numpy()\n        self.root = self.grow_tree(X, y)\n        \n    def grow_tree(self, X, y, depth=0):\n        n_samples, n_features = X.shape\n        n_labels = len(np.unique(y))\n        \n        # stopping criteria\n        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n            leaf_value = self.most_common_label(y)\n            return Node(value=leaf_value)\n        \n        # array of random columns in Dataset\n        feats_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n                \n        best_feat, best_tresh = self.best_criteria(X, y, feats_idxs)\n        \n        left_idx, right_idx = self.split(X[:, best_feat], best_tresh)\n\n        left = self.grow_tree(X[left_idx, :], y[left_idx], depth+1)\n        right = self.grow_tree(X[right_idx, :], y[right_idx], depth+1)\n        return Node(best_feat, best_tresh, left, right)\n        \n    def best_criteria(self, X, y, feats_idxs):\n        best_gain = -1\n        split_idx, split_tresh = None, None\n        \n        for feats_idx in feats_idxs:\n            df = pd.DataFrame(X[:, feats_idx], columns=['X_col'])\n            df['y'] = y\n            df = df.sort_values(by=['X_col'], ascending=True)\n            \n            X_col = df.X_col\n            y = df.y\n            \n            X_col = X_col.to_numpy()\n            y = y.to_numpy()\n            for val in X_col:\n                gain = self.information_gain(y, X_col, val)\n                if gain > best_gain:\n                    best_gain = gain\n                    split_idx = feats_idx\n                    split_tresh = val\n            \n        return split_idx, split_tresh\n    \n    def information_gain(self, y, X_col, thresh):\n        parent_entropy = entropy(y)\n        \n        left, right = self.split(X_col, thresh)\n\n        if len(left) == 0 or len(right) == 0:\n            return 0\n        \n        n = len(y)\n        n_l, n_r = len(left), len(right)\n        e_l, e_r = entropy(y[left]), entropy(y[right])\n        \n        child_entropy = (n_l \/ n) * e_l + (n_r \/ n) * e_r\n        \n        ig = parent_entropy - child_entropy\n        return ig\n    \n    def split(self, X_col, split_tresh):\n        \n        left_idxs = np.argwhere(X_col <= split_tresh).flatten()\n        right_idxs = np.argwhere(X_col > split_tresh).flatten()\n        \n        return left_idxs, right_idxs\n    \n    def most_common_label(self, y):\n        counter = Counter(y)\n        most_common = counter.most_common(1)[0][0]\n        return most_common\n    \n    def traverse_tree(self, x, node):\n        if node.is_leaf_node():\n            return node.value\n\n        if x[node.feature] <= node.threshold:\n            return self.traverse_tree(x, node.left)\n        \n        return self.traverse_tree(x, node.right)","5b95ec86":"def accuracy(y_true, y_pred):\n    acc = np.sum(y_true == y_pred)\/len(y_true)\n    return acc","efaa1984":"if __name__ == '__main__':\n    X = data.drop('Species', axis=1)\n    y = data.Species\n    \n    le = LabelEncoder()\n    y = le.fit_transform(y)\n    \n    X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    \n    DT = Decission_Tree(max_depth=4)\n    DT.fit(X_train, y_train)\n    \n    y_pred = DT.predict(x_test)","7d5e4efc":"m = {'y_test' : y_test, 'y_pred' : y_pred}\n\nframe = pd.DataFrame(m) \n\nframe.head(10)","b0b5f314":"acc = accuracy(y_test, y_pred)\n    \nprint('accuracy : ', acc)","68663552":"### Main function","937af46f":"### Create Node","0c4be696":"### accuracy","1db31545":"### Calculate Entropy","0077e87b":"## Decision Tree ","4c1ea141":"### Calculate accuracy"}}