{"cell_type":{"d16ce274":"code","1e385009":"code","86ff60ec":"code","ef2fff99":"code","3c7b35f2":"code","a1eebf36":"code","e179e07f":"code","a93b989d":"code","01e7320b":"code","30ba23d7":"code","7a5ab505":"code","1179a187":"code","15025520":"code","5707ceec":"code","632b4b88":"code","f0a21c7f":"code","5491a863":"code","d3b2855d":"code","0f83f44e":"code","03fc3063":"code","ebe0f6f3":"code","98c3524e":"code","ada7a014":"code","279cbc24":"code","eacb0329":"code","10120067":"code","fdd57b96":"code","fedad68d":"code","4e368c84":"code","fffd1c2d":"code","8c62d04c":"code","02bd03e3":"code","16cd4817":"code","7cc33807":"code","38937145":"code","f25dcd9c":"code","cfeffdb6":"code","a79e7c25":"code","0f81d244":"code","82ef2530":"code","f66159e8":"code","7512681f":"code","e25f219e":"code","23ef9a2b":"code","4fb9b6ba":"code","ece0d311":"markdown","52b9fc53":"markdown","c9335c85":"markdown","f4c21770":"markdown","23bdf6a8":"markdown"},"source":{"d16ce274":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import plot_roc_curve\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score","1e385009":"df_uncleaned =  pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nTarget = df_uncleaned.SalePrice\ndf_uncleaned.drop(\"SalePrice\", axis=1, inplace=True)","86ff60ec":"# Check Numerical Data\ndf_uncleaned_num = df_uncleaned.select_dtypes(exclude=\"object\")\ndf_test_num = df_test.select_dtypes(exclude=\"object\")\ndf_uncleaned_num.shape","ef2fff99":"# Check Categorical Data\ndf_uncleaned_cat = df_uncleaned.select_dtypes(include=\"object\")\ndf_test_cat = df_test.select_dtypes(include=\"object\")\ndf_uncleaned_cat.shape","3c7b35f2":"# Columns will use OneHot Encoder\nonehot = ['MSZoning', 'LandContour', 'Street', 'Alley', 'LotShape', 'Utilities', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', \n         'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', \n          'CentralAir', 'Electrical', 'GarageType', 'PavedDrive', 'MiscFeature', 'SaleType', 'SaleCondition']\n\n# Columns will use Label Encoder\nlabenc = ['LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure','BsmtFinType1', \n          'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',\n         'PoolQC', 'Fence']\n\n# Categorical features assigned to Numerical features\nnum_cat = ['MSSubClass', 'OverallQual', 'OverallCond']","a1eebf36":"# OneHot Encoder\ndf_cat_oh = pd.get_dummies(df_uncleaned_cat[onehot], dummy_na=True)\ndf_test_oh = pd.get_dummies(df_test_cat[onehot], dummy_na=True)\ndf_test_oh.shape\ndf_cat_oh.shape","e179e07f":"# Feature Selection using Variance \nvt = VarianceThreshold(threshold=.03)\n\n_ = vt.fit(df_cat_oh)\n\n# Mask\nmask = vt.get_support()\n\n# Shape before Variance Selection\nprint(df_cat_oh.shape)\n\n#Shape After Variance Selection\nprint(df_cat_oh.loc[:, mask].shape)\n\ndf_cat_oh = df_cat_oh.loc[:, mask]\ncol_cat = df_cat_oh.columns\ndf_test_oh = df_test_oh[col_cat]","a93b989d":"# print the unique value counts for all the features\ncols = df_uncleaned_cat[labenc].shape[1]\nfor i in range(cols):\n    print(df_uncleaned_cat[labenc].iloc[:, i].value_counts())","01e7320b":"# replace with numbers\nclass_map = {'Ex': 5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'Na':0, 'NA':0, 'Gtl':0, 'Mod':1, 'Sev':2, 'Av':3, 'Mn':2, 'No':1,\n            'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ':2, 'Unf':1, 'Typ':1, 'Min1':2, 'Min2':3, 'Mod':4, 'Maj1':5, 'Maj2':6, \n            'Sev':7, 'Sal':8, 'Fin':3, 'RFn':2, 'GdPrv':4, 'MnPrv':3, 'GdWo':2, 'MnWw':1}\ndf_cat_lab = df_uncleaned_cat[labenc].replace(class_map)\ndf_test_lab = df_test_cat[labenc].replace(class_map)","30ba23d7":"# fill na with 0\ndf_cat_lab = df_cat_lab.fillna(0)\ndf_test_lab = df_test_lab.fillna(0)\n\n# get the full categorical data\ndf_cleaned_cat = pd.concat([df_cat_lab, df_cat_oh], axis=1)\ndf_test_cleaned_cat = pd.concat([df_test_lab, df_test_oh], axis=1)","7a5ab505":"#Fill MasVnrArea with the mean\ndf_uncleaned_num.MasVnrArea.fillna(df_uncleaned_num.MasVnrArea.mean(), inplace=True)\ndf_test_num.MasVnrArea.fillna(df_test_num.MasVnrArea.mean(), inplace=True)\n3\n#Get the index for the row with null for GarageYrBlt\nnull_id_gyb = df_uncleaned_num.GarageYrBlt.isnull()[df_uncleaned_num.GarageYrBlt.isnull()==1].index\nnot_null_id_gyb = df_uncleaned_num.GarageYrBlt.isnull()[df_uncleaned_num.GarageYrBlt.isnull()==0].index\nnull_id_lf = df_uncleaned_num.LotFrontage.loc[df_uncleaned_num.LotFrontage.isnull()==1].index\nnot_null_id_lf = df_uncleaned_num.LotFrontage.loc[df_uncleaned_num.LotFrontage.isnull()==0].index","1179a187":"def get_null(df_uncleaned_num):    \n    \n    #Get the index for the row with null for GarageYrBlt\n    null_id_gyb = df_uncleaned_num.GarageYrBlt.isnull()[df_uncleaned_num.GarageYrBlt.isnull()==1].index\n    not_null_id_gyb = df_uncleaned_num.GarageYrBlt.isnull()[df_uncleaned_num.GarageYrBlt.isnull()==0].index\n    null_id_lf = df_uncleaned_num.LotFrontage.loc[df_uncleaned_num.LotFrontage.isnull()==1].index\n    not_null_id_lf = df_uncleaned_num.LotFrontage.loc[df_uncleaned_num.LotFrontage.isnull()==0].index\n    \n    return null_id_gyb, null_id_lf\n\ntest_id_gyb, test_id_lf = get_null(df_test_num)","15025520":"# Drop the columns with null values\ndf_pred_num = df_uncleaned_num.drop(['Id', 'LotFrontage', 'GarageYrBlt'], axis=1)\ntest_pred_num = df_test_num.drop(['Id', 'LotFrontage', 'GarageYrBlt'], axis=1)\n\n# Concat the numerical data \\w categorical data\ndf_pred = pd.concat([df_pred_num, df_cleaned_cat], axis=1)\ntest_pred = pd.concat([test_pred_num, df_test_cleaned_cat], axis=1)\n\n# Standardize the Data\ndf_pred = (df_pred - df_pred.min())\/(df_pred.max() - df_pred.min())\ntest_pred = (test_pred - df_pred.min())\/(df_pred.max() - df_pred.min())\n\n# Slice the data for Training and Prediction\ndf_pred_gyb = df_pred.iloc[not_null_id_gyb]\ndf_pred_lf = df_pred.iloc[not_null_id_lf]\n\ndf_null_gyb = df_pred.iloc[null_id_gyb]\ndf_null_lf = df_pred.iloc[null_id_lf]\n\ntest_null_gyb = test_pred.iloc[test_id_gyb]\ntest_null_lf = test_pred.iloc[test_id_lf]\n\n# Get the data for predicting\nGYB_train = df_uncleaned_num.GarageYrBlt.iloc[not_null_id_gyb]\nLF_train = df_uncleaned_num.LotFrontage.iloc[not_null_id_lf]","5707ceec":"# Split the train & test dataset\nX_train, X_test, y_train, y_test = train_test_split(df_pred_gyb, GYB_train, test_size=0.3)","632b4b88":"# # Create the parameter grid based on the results of random search \n# param_grid = {\n#     'bootstrap': [True, False],\n#     'max_depth': [20, 30],\n#     #'max_features': [0, 1, 2],\n#     'min_samples_leaf': [3, 4, 5],\n#     'min_samples_split': [4, 6, 8, 10],\n#     'n_estimators': [100]\n# }\n# # Create a based model\n# rf = RandomForestRegressor()\n# # Instantiate the grid search model\n# grid_search_1 = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, verbose = 2)","f0a21c7f":"# # find the best parameters using grid research\n# grid_search_1.fit(X_train, y_train)\n# grid_search_1.best_params_","5491a863":"# create random forest regressor to predict null values\nrfr1 = RandomForestRegressor(\n    bootstrap = True,\n    max_depth = 30,\n    min_samples_leaf = 3,\n    min_samples_split = 10,\n    n_estimators = 100)\nrfr1.fit(X_train, y_train)\ny_pred = rfr1.predict(X_test)\nmetrics.mean_squared_error(y_test, y_pred)","d3b2855d":"# get the predicted values replacing the null\nnull_pred = rfr1.predict(df_null_gyb)\ngby_null = [int(i) for i in np.around(null_pred)]\n\ndf_filled = df_uncleaned_num[:]\ndf_filled.GarageYrBlt.loc[null_id_gyb] = gby_null","0f83f44e":"X_train, X_test, y_train, y_test = train_test_split(df_pred_lf, LF_train, test_size=0.3)","03fc3063":"# # Create the parameter grid based on the results of random search \n# param_grid_2 = {\n#     'bootstrap': [True],\n#     'max_depth': [10, 20, 50, 100],\n#     #'max_features': [50, 4],\n#     'min_samples_leaf': [2, 3, 4],\n#     'min_samples_split': [8, 10],\n#     'n_estimators': [100]\n# }\n# # Create a based model\n# rf_2 = RandomForestRegressor()\n# # Instantiate the grid search model\n# grid_search_2 = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, verbose = 3)","ebe0f6f3":"# grid_search_2.fit(X_train, y_train)\n# grid_search_2.best_params_","98c3524e":"rfr2 = RandomForestRegressor(\n    bootstrap = True,\n    max_depth = 30,\n    min_samples_leaf = 3,\n    min_samples_split = 4,\n    n_estimators = 100)\nrfr2.fit(X_train, y_train)\ny_pred = rfr2.predict(X_test)\nmetrics.mean_squared_error(y_test, y_pred)","ada7a014":"lf_null = rfr2.predict(df_null_lf)\ndf_filled.LotFrontage.iloc[null_id_lf] = [int(i) for i in np.round(lf_null)]\ndf_filled.LotFrontage.isnull().sum()","279cbc24":"def pred_fill(model, df_null):\n    null_pred = model.predict(df_null)\n    fill_null = [int(i) for i in np.around(null_pred)]\n\n    return fill_null","eacb0329":"# do the same thing for test dataset\ntest_gyb = pred_fill(rfr1, test_null_gyb.fillna(test_null_gyb.mean()))\ntest_lf = pred_fill(rfr2, test_null_lf.fillna(test_null_gyb.mean()))\ntest_filled = df_test_num[:]\ntest_filled.LotFrontage.iloc[test_id_lf] = test_lf\ntest_filled.GarageYrBlt.iloc[test_id_gyb] = test_gyb","10120067":"# drop ID\ndf_filled = df_filled.drop([\"Id\"], axis=1)\n\n# make sure get rid of the null values\ndf_cleaned = pd.concat([df_filled, df_cleaned_cat], axis=1)\nsum(df_cleaned.isnull().sum())","fdd57b96":"test_filled = test_filled.drop([\"Id\"], axis=1).fillna(test_filled.mean())\ndf_test_cleaned = pd.concat([test_filled, df_test_cleaned_cat], axis=1)","fedad68d":"df_test_cleaned.shape","4e368c84":"# scaling \nscaler = MinMaxScaler()\nstd_df = (df_cleaned - df_cleaned.min())\/(df_cleaned.max() - df_cleaned.min())\nstd_test = (df_test_cleaned - df_cleaned.min())\/(df_cleaned.max() - df_cleaned.min())\nX_train, X_test, y_train, y_test = train_test_split(std_df, Target, test_size=0.3)","fffd1c2d":"# feature selection\nmodels = [Lasso(), Ridge(), RandomForestRegressor(), XGBRegressor()]\n\nselect_features  = {}\nfeature_selected = []\nfor model in models:\n    sel_ = SelectFromModel(model)\n    sel_.fit(std_df, Target)\n    mask = sel_.get_support()\n    \n    for col in (X_train.iloc[:, mask].columns):\n        if col not in select_features:\n            select_features[col] = 1\n        else:\n            select_features[col] += 1\n        \nfor key, val in select_features.items():\n    if val >= 2:\n        feature_selected.append(key)\n\n        \nlen(feature_selected)","8c62d04c":"# create XGB regressor\nxgb1 = XGBRegressor(\n     learning_rate =0.01,\n     n_estimators=6700,\n     max_depth = 3,\n     min_child_weight = 3,\n     gamma=0.0,\n     verbosity = 1,\n     colsample_bytree= 0.85,\n     subsample = 0.95,\n     reg_alpha=100,\n     reg_lambda= 1e-05,\n     seed=42\n)","02bd03e3":"# compute mse\nxgb1.fit(X_train[feature_selected], y_train)\ny_pred = xgb1.predict(X_test[feature_selected])\nmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\nmse","16cd4817":"test_pred = xgb1.predict(std_test[feature_selected])","7cc33807":"# from sklearn.model_selection import cross_val_score\n# cv_results = cross_val_score(xgb1, X_test[feature_selected], y_test, \n#                             cv=5, \n#                             verbose=2, \n#                             scoring='neg_root_mean_squared_error')\n# np.mean(cv_results)","38937145":"# params = {\"n_estimators\": [6700, 8200, 9400]}\n# gsearch(X_train[feature_selected], y_train, params, xgb1, cv=5)","f25dcd9c":"# param_test1 = {\n#  'max_depth':[i for i in range(3,10,2)],\n#  'min_child_weight':[j for j in range(1,6,2)]\n# }\n# gsearch(X_train[feature_selected], y_train, param_test1, xgb1)","cfeffdb6":"# param_test2 = {\n#  'gamma':[i\/10 for i in range(5)],\n# }\n\n# gsearch(X_train[feature_selected], y_train, param_test2, xgb1)","a79e7c25":"# param_test4 = {\n#  'subsample':[i\/100. for i in range(85,101, 5)],\n#  'colsample_bytree':[i\/100. for i in range(85,101, 5)]\n# }\n# gsearch(X_train[feature_selected], y_train, param_test4, xgb1,cv=3)","0f81d244":"# param_test5 = {\n#  'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n# }\n# gsearch(X_train[feature_selected], y_train, param_test5, xgb1)","82ef2530":"# param_test6 = {\n#  'reg_lambda':[1e-5, 1e-2, 0.1, 1, 100]\n# }\n# gsearch(X_train[feature_selected], y_train, param_test6, xgb1)","f66159e8":"def gsearch(X, y, params, estimator, score = 'neg_root_mean_squared_error', verbose=2, cv=5):\n    gs = GridSearchCV(estimator = estimator, param_grid = params, \n                      scoring = score, verbose = verbose, cv=cv, )\n    gs.fit(X, y)\n    print(\"Best parameters:\\n\")\n    print(gs.best_params_)\n    print()\n    print(\"Best score:\\n\")\n    print(gs.best_score_)","7512681f":"def MSE(y_pred, y_true):\n    mse = metrics.mean_squared_error(y_true, y_pred)\n    return mse","e25f219e":"# for i in range(100):\n#     res = []\n#     X_train, X_test, y_train, y_test = train_test_split(std_df, Target, test_size=0.3)\n#     xgb1.fit(X_train[feature_selected], y_train)\n#     y_pred = xgb1.predict(X_test[feature_selected])\n#     mse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n#     res.append(mse)\n#     print('For the %d th test process, the mse is %5f' % (i+1,mse))","23ef9a2b":"# check the feature importance\npd.DataFrame(xgb1.get_booster().feature_names, xgb1.feature_importances_)","4fb9b6ba":"output = pd.DataFrame()\noutput[\"id\"] = df_test.Id\noutput[\"SalePrice\"] = test_pred\noutput.to_csv(\"output1.csv\", index=False)","ece0d311":"## Categorical Dataset\n* manually select the features will use OneHot Encoder and Label Encoder\n* select feature using Variance threshold\n* fill none values with 0","52b9fc53":"# **Data preprocessing**\n\n* split the dataset into numerical and categorical dataset","c9335c85":"## Numerical Dataset\n* fill the null values with mean\n* select feature with essential amount of null values and use random forest regressor to predict the null values\n* Scale the dataset","f4c21770":"### Tune the parameters using cross validation","23bdf6a8":"# Construct the Model\n* XGB Regressor"}}