{"cell_type":{"e70bb80e":"code","1467bf6a":"code","95ee4d6d":"code","379c291f":"code","df961e41":"code","a012423e":"code","ed54de00":"code","17ac0a3a":"code","c238e4c5":"code","2489cb2b":"code","561d8d4b":"code","3d501ec6":"markdown","ef8811ac":"markdown","5e7938e6":"markdown","cd25fdc8":"markdown","c180d49b":"markdown","d56ed5fa":"markdown","9d8849df":"markdown","8909640a":"markdown","18683337":"markdown","655039ee":"markdown","082114e9":"markdown","e81532e7":"markdown"},"source":{"e70bb80e":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import datasets #Import Iris Dataset\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1467bf6a":"pip install graphviz","95ee4d6d":"iris_data = datasets.load_iris()","379c291f":"X = iris_data.data[:,0:3]","df961e41":"Y = iris_data.target","a012423e":"tree_clf = DecisionTreeClassifier(max_depth=4, random_state=42, min_samples_leaf=3)","ed54de00":"tree_clf.fit(X,Y)","17ac0a3a":"\nvisualise_tree = export_graphviz(tree_clf, out_file='iris-tree.dot',\nrounded=True,\nfilled=True)\n\n","c238e4c5":"!dot -Tpng iris-tree.dot -o iris-tree.png -Gdpi=600\n","2489cb2b":"from IPython.display import Image\nImage(filename = 'iris-tree.png')","561d8d4b":"tree.plot_tree(tree_clf)","3d501ec6":"# #  Import Necessary Libraries","ef8811ac":"To use visualization, can be done either via export_graphviz or tree.plot_tree as shown below","5e7938e6":"For the descision tree, we will be using a popular dataset : iris dataset ","cd25fdc8":"\n# Introduction to decision trees:\n\nUpvote you if find this useful \n\n    1. What are decision trees?\n    2. DecisionTreeClassifier \/ DecisionTreeRegressor\n    3. Gini\/Entropy\n    4. References : https:\/\/towardsdatascience.com\/enchanted-random-forest-b08d418cb411#.hh7n1co54\n    https:\/\/www.kaggle.com\/willkoehrsen\/visualize-a-decision-tree-w-python-scikit-learn","c180d49b":"X --> Feature Columns","d56ed5fa":"## Gini vs Entropy:\n\nGini : ---> Measure of Impurity <br>\nEntropy : ---> Measure of Impurity just like Gini\n\n<div style=\"font-size:20px; color:blue\">\nGini impurity does not intake logarithmic function in order to calculate gini index, like in case of entropy. However, both Entropy and gini are lowest when probabilty of (y\/n) is 0.\nWhile Entropy is from 0 to 1 for p (probability 0 to P) gini is from range 0 t 0.5\n<\/div>\nNote : The above mentioned CART Algorithm uses Gini Impurity for growing a decision tree\n\n<div style=\"font-size:20px; color:blue\">\nEntropy leads to information gain, for each node, there is entropy, analogous to the concept of thermodynamics where low entropy level means more stable (more homogenity in the node). Entropy is calculated for a single node for a particular feature.\nInformation gain suggests, considering the entropy at level l, how will split be there for sub-levels.\nEntropy 0 means most stable (completely pure) whereas 1 means most impure \n\nInformation Gain is calculated with the help of Entropy and level l and Entropy at sublevel l+1 :\nHigher the value of Information Gain, lower will be the entropy\n    <\/div>","9d8849df":"Here We Will be importing the following libraries:","8909640a":"**Load the Iris Dataset**","18683337":"\nDecision Trees can be used in both Classification as well as Regression\n\n* DecisionTreeClassifier\n* DecisionTreeRegressor\n<div style=\"width:75%\"><h1>DecisionTreeClassifier Popular Hyperparameters : <\/h1>\n<\/div>\n<div style=\"color:green; font-size:22px\">\ncriterion  --> Criterion for split, gini (default) or entropy\n<br>\n**splitter** --> best or random (use best feature for making splitting decisions or choose at random (default, best)\n<br>\n**max_depth** ---> maximum depth of the tree (to avoid overfitting, default is None)\n<br>\nNote : Use max_depth=3 as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth.\n<br>\n**min_samples_split** ---> minimum no. of samples required to split a node (default =2)\n<br>\n**min_samples_leaf** ---> minimum samples in a leaf (default =1 )\n<br>\n<\/div>\n*Use min_samples_split or min_samples_leaf to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try min_samples_leaf=5 as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While min_samples_split can create arbitrarily small leaves, min_samples_leaf guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, min_samples_leaf=1 is often the best choice.*","655039ee":"What are decision Trees? \n\nMachine learning algorithm based on trees, for a **predictive model approach** used for **supervised learning**\nBoth Classification and Regression Problems can be dealt by decision trees. It makes decision by randomly selecting a few features making rules and making informed decision by rules. Decision trees are one of the most powerful machine learning algorthms and are a basis to Random Forests.\n\n<p><a href=\"https:\/\/commons.wikimedia.org\/wiki\/File:Manual_decision_tree.jpg#\/media\/File:Manual_decision_tree.jpg\"><img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/c6\/Manual_decision_tree.jpg\" alt=\"Manual decision tree.jpg\"><\/a><br>By &lt;a href=\"\/\/commons.wikimedia.org\/w\/index.php?title=User:Gokul_Jadhav&amp;amp;action=edit&amp;amp;redlink=1\" class=\"new\" ><a href=\"https:\/\/commons.wikimedia.org\/w\/index.php?curid=4404547\">Link<\/a><\/p>\n\n\nSome Attributes of Decision Trees :\n*  Can be easily visualized (with a library export_graphviz in sklearn module)\n*  Can handle both categorical as well as numeric data\n*  Chances of over fitting is higher than Linear Models\/SVMs\n*  Does not necessarily require removal of outliers\n*  Uses locally most optimal solution as uses greedy approach (Per node, the most optimal is selected instead of global optima, ie no particular rule is followed, as a result learning from decision tree poses a problem of NP Completeness)\n ","082114e9":"Y --> Label Column","e81532e7":"In this notebook, we will be working upon the Iris DataSet. I will be coding in python. Here decision trees will be implemented[](http:\/\/) via scikit-learn library which implements \"CART\" algorithm.\n\n\n\nWhat exactly is the CART algorithm :\nThe CART algorithm stands for Classification and Regression Tree, where there is split of a node into 2, based upon a particular feature k, and a particular threshold (In below example,x<=2.45 in level 0)"}}