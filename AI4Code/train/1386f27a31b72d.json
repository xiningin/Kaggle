{"cell_type":{"51451b3d":"code","adc218bd":"code","65fa8572":"code","14e3e4fe":"code","fbab5db5":"code","b4f3adc3":"code","98fb8920":"code","c151e405":"code","d98ae928":"code","643d14d7":"code","15cde837":"code","584f2430":"code","7335747f":"code","1fe67529":"code","a1902661":"code","5aae12bf":"code","8ee3043e":"code","c9a86766":"code","ba3a965d":"code","29d01151":"code","14cb876c":"code","ca3a6a45":"code","51071712":"code","5ce77229":"code","7c6534fb":"code","520872e1":"code","860bcc00":"code","d43ea649":"code","83370ad4":"code","6de848d9":"code","476d9b15":"code","f17c395f":"code","fd985476":"code","fab0b228":"code","41e226b8":"code","7a1532e2":"code","799cfbcb":"code","0bc5f1cb":"code","bc191985":"code","44a3c7c9":"code","6c53e9cb":"code","a414a7b4":"code","67c67fa4":"code","d1d8a507":"code","b48abf37":"code","dc535bef":"code","54359d5c":"code","98858464":"code","db97c584":"code","c445315c":"code","00b0bbf8":"code","219cc6c0":"code","8693b208":"code","61921f3c":"code","bc0dc517":"code","6e7d7874":"code","46d98cc3":"code","496ef0bb":"code","0b04b5c7":"code","4c95087e":"code","fc49f6f0":"code","2a5a5186":"code","097f4a58":"code","ad7ce16b":"code","5a5ba3ce":"code","fb0a843f":"code","68c193a3":"code","7bee94a9":"code","0741c0ec":"code","702e3fbb":"code","acfc6f23":"code","1c6c2b28":"code","2657999a":"code","5c7a793e":"code","393b4a12":"code","fce51e5f":"code","501e9cba":"code","d536e498":"code","04d540d7":"code","48ebbf08":"code","66be6610":"code","f18f4f77":"code","cb855502":"code","1628ec68":"code","795add6e":"code","64fce0ca":"code","eff8ddbb":"code","e4bbe933":"code","4a326857":"code","8642846c":"code","5eb3d4b9":"code","0e27a32a":"code","3af0a188":"code","bf70c122":"code","24533cca":"code","243e68ea":"code","a3c1951c":"code","0c83e76c":"code","82849fc4":"code","645abea6":"code","70892672":"code","f2e69e6e":"code","737b6bfb":"code","78f2b2ec":"code","f0c2c24a":"code","6201cc2a":"code","615ec548":"code","25488bc5":"code","189e7013":"code","aefc7684":"code","d876cfc9":"code","0cf49892":"code","4ecef40f":"code","b4c8bea5":"code","31f9c5d3":"code","b6f2faee":"code","1e884011":"code","23bd43ce":"code","7c9acdb3":"code","83ae391a":"code","d0c42036":"code","385796c0":"code","7ac0151e":"code","5c7cd703":"code","c855e6b4":"code","022d8dbd":"code","ab014c9d":"code","73fd59a8":"code","76926570":"code","79e7c144":"code","bed46676":"code","85173dbb":"code","414ad61a":"code","63d7bf28":"code","33872c48":"code","1167f766":"code","ae5939bd":"code","9fa1b462":"code","7ffb0b53":"code","1756d287":"code","dab47060":"code","4cbb651e":"markdown","fb872c02":"markdown","5e57c21c":"markdown","7ea592d8":"markdown","25525e6e":"markdown","4bfee6b1":"markdown","6bd51ba3":"markdown","b162e362":"markdown","5629a903":"markdown","9ee1afaa":"markdown","aba28dbf":"markdown","bdd8daac":"markdown","ca8657b9":"markdown","35c8e7a2":"markdown","a2220bcc":"markdown","40f50d10":"markdown","684f6126":"markdown","4a23c808":"markdown","393b01b6":"markdown","0010de43":"markdown","830c7308":"markdown","2578c843":"markdown","4548ada5":"markdown","4b1877c4":"markdown","6dcfe904":"markdown","fd08874f":"markdown","dda42b9d":"markdown","1058de38":"markdown","322ee342":"markdown","0c85e445":"markdown","cbc4dc94":"markdown","a243ae2f":"markdown","4ff9e0fb":"markdown","fc58a901":"markdown","a9ed763d":"markdown","8d62b7f2":"markdown","15ef2c91":"markdown","58b40204":"markdown","211bc7bc":"markdown","121387da":"markdown","e85441f5":"markdown","b628c385":"markdown","11b5cf00":"markdown","6f6a370d":"markdown","11ed1fbf":"markdown","10d3a2bb":"markdown","bb2be7e5":"markdown","62022c30":"markdown","aafb1a4b":"markdown","2fd7317f":"markdown","0d030661":"markdown","fa948886":"markdown"},"source":{"51451b3d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport scipy.stats as stats\nfrom scipy.stats import chi2\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","adc218bd":"#load train and test data\ntrainDf = pd.read_csv('..\/input\/titanic\/train.csv')\ntestDf = pd.read_csv('..\/input\/titanic\/test.csv')","65fa8572":"trainDf.head()","14e3e4fe":"trainDf.shape","fbab5db5":"trainDf.info()","b4f3adc3":"trainDf.describe()","98fb8920":"testDf.head()","c151e405":"testDf.shape","d98ae928":"testDf.info()","643d14d7":"#plot missing data using msno.matrix\ndef plotMissingData(data):\n   msno.matrix(data)","15cde837":"#create dataframe and store missing data counts and percentages\ndef drawMissingTable(data):\n    count = data.isnull().sum()\n    precentage = data.isnull().mean().round(4) * 100\n\n    return pd.DataFrame({'count': count, 'precentage': precentage}).sort_values('count', ascending=False)","584f2430":"plotMissingData(trainDf)","7335747f":"drawMissingTable(trainDf)","1fe67529":"missingTrainDf = trainDf.copy()","a1902661":"missingTrainDf['cabin_NA'] = np.where(missingTrainDf['Cabin'].isnull(), 1, 0)","5aae12bf":"missingTrainDf.head()","8ee3043e":"missingTrainDf['Cabin'].value_counts()","c9a86766":"missingTrainDf[missingTrainDf['Cabin'].isnull()]","ba3a965d":"missingTrainDf[missingTrainDf['Cabin'].notnull()]","29d01151":"#plot missing data of Cabin by Survived\nplt.figure(figsize=(10, 6))\nplt.pie(missingTrainDf.groupby('Survived')['cabin_NA'].sum(), labels=['No','Yes'], autopct='%1.1f%%')\nplt.show()","14cb876c":"missingTrainDf.groupby('Survived')['cabin_NA'].sum()","ca3a6a45":"#Chi-Square test\ndef ChiSqr(table):\n    stat, p, dof, expected = stats.chi2_contingency(table)\n\n    print('dof=%d' % dof)\n    print(expected)\n\n    alpha = 0.05\n    print('significance=%.2f, p=%.4f' % (alpha, p))\n\n    if p <= alpha:\n        print('Result: Reject H0')\n    else:\n        print('Result: Do not reject H0')","51071712":"cabinTb = pd.crosstab(missingTrainDf['Survived'], missingTrainDf['cabin_NA'])\ncabinTb","5ce77229":"ChiSqr(cabinTb)","7c6534fb":"#plot {traget} missing data vs 'Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked'\ndef plotCategoricalMissingData(data, hue):\n    columns = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']\n    plt.figure(figsize=(25, 10))\n\n    count = 1\n    for column in columns:\n        plt.subplot(2, 3, count)\n        ax = sns.countplot(x=column, data=data,  hue=hue)\n        \n        for p in ax.patches:\n            ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+2))\n            \n        count = count + 1\n        plt.grid() \n        plt.legend('')\n\n    plt.legend(loc='upper right',  labels=['Complete', 'Missing'])\n    plt.show()","520872e1":"plotCategoricalMissingData(missingTrainDf, 'cabin_NA')","860bcc00":"#plot Age distirbution by Cabin missing data\nplt.figure(figsize=(10,6))\n\nsns.histplot(x=\"Age\", kde=True, data=missingTrainDf, color='skyblue', hue='cabin_NA', element='step')\n\nplt.suptitle('Age Distribution - Cabin Missing')\nplt.legend(labels=['Missing', 'Complete'])\nplt.grid() \nplt.show()","d43ea649":"#plot Fare distirbution by Cabin missing data\nplt.figure(figsize=(20,6))\n\nsns.histplot(x=\"Fare\", kde=True, data=missingTrainDf, element='step', color='skyblue', hue='cabin_NA')\n\nplt.suptitle('Fare Distribution - Cabin Missing')\nplt.legend(labels=['Missing', 'Complete'])\nplt.grid() \nplt.show()","83370ad4":"missingTrainDf['embarked_NA'] = np.where(missingTrainDf['Embarked'].isnull(), 1, 0)","6de848d9":"missingTrainDf['Embarked'].isnull().sum()","476d9b15":"missingTrainDf[missingTrainDf['embarked_NA'] == 1]","f17c395f":"missingTrainDf[missingTrainDf['Cabin'].str.startswith('B2', na=False)]","fd985476":"#impute Embarked missing value with 'S'\ntrainDf['Embarked'] = trainDf['Embarked'].fillna('S')","fab0b228":"trainDf.loc[trainDf['PassengerId'].isin([62, 830])]","41e226b8":"missingTrainDf['age_NA'] = np.where(missingTrainDf['Age'].isnull(), 1, 0)","7a1532e2":"missingTrainDf[missingTrainDf['age_NA'] == 1]","799cfbcb":"plt.figure(figsize=(10, 6))\nplt.pie(missingTrainDf.groupby('Survived')['age_NA'].sum(), labels=['No','Yes'], autopct='%1.1f%%')\nplt.show()","0bc5f1cb":"missingTrainDf.groupby('Survived')['age_NA'].sum()","bc191985":"ageTb = pd.crosstab(missingTrainDf['Survived'], missingTrainDf['age_NA'])\nageTb","44a3c7c9":"ChiSqr(ageTb)","6c53e9cb":"missingTrainDf.groupby('Survived')['age_NA'].sum()","a414a7b4":"plotCategoricalMissingData(missingTrainDf, 'age_NA')","67c67fa4":"plt.figure(figsize=(20,6))\n\nsns.histplot(x=\"Fare\", kde=True, data=missingTrainDf, element='step', color='skyblue', hue='age_NA')\n\nplt.suptitle('Fare Distribution - Age Missing')\nplt.legend(labels=['Missing', 'Complete'])\nplt.grid() \nplt.show()","d1d8a507":"#group by Sex, Pclass and Parch\ngroupedAge = trainDf.groupby(['Sex','Pclass', 'Parch'])  \ngroupedAge['Age'].median()","b48abf37":"#impute missing values usign grouped age median\ntrainDf['Age'] = groupedAge['Age'].apply(lambda x: x.fillna(x.median()))","dc535bef":"trainDf.loc[trainDf['PassengerId'].isin([6, 32, 46, 56, 18, 899])]","54359d5c":"missingTrainDf[missingTrainDf['Fare'] == missingTrainDf['Fare'].min()]","98858464":"missingTrainDf[missingTrainDf['Fare'] == missingTrainDf['Fare'].max()]","db97c584":"trainDf['Fare'] = np.where(trainDf['Fare'] < 1, np.nan, trainDf['Fare'])","c445315c":"trainDf[trainDf['PassengerId'] == 180]","00b0bbf8":"trainDf[trainDf['Fare'].isnull()]","219cc6c0":"groupedFare = trainDf.groupby('Pclass')  \ngroupedFare['Fare'].median()","8693b208":"#impute missing values usign grouped Fare median\ntrainDf['Fare'] = groupedFare['Fare'].apply(lambda x: x.fillna(x.median()))","61921f3c":"trainDf.loc[trainDf['PassengerId'].isin([180, 264, 414, 598, 675, 816])]","bc0dc517":"drawMissingTable(trainDf)","6e7d7874":"missingTestDf = testDf.copy()","46d98cc3":"plotMissingData(missingTestDf)","496ef0bb":"drawMissingTable(missingTestDf)","0b04b5c7":"testDf[testDf['Age'].isnull()]","4c95087e":"#group by Sex, Pclass and Parch\ngroupedAge = testDf.fillna(testDf['Age'].median()).groupby(['Sex','Pclass', 'Parch'])  \ngroupedAge['Age'].median()","fc49f6f0":"testDf['Age'] = groupedAge['Age'].apply(lambda x: x.fillna(x.median()))","2a5a5186":"testDf.loc[testDf['PassengerId'].isin([902, 928, 1305, 1309, 1024, 1234])]","097f4a58":"testDf[testDf['Fare'].isnull()]","ad7ce16b":"testDf[testDf['Fare'] == testDf['Fare'].min()]","5a5ba3ce":"groupedFare = testDf.groupby('Pclass')  \ngroupedFare['Fare'].median()","fb0a843f":"#impute missing values usign grouped Age median\ntestDf['Fare'] = np.where(testDf['Fare'] < 1, np.nan, testDf['Fare'])","68c193a3":"#impute missing values usign grouped Fare median\ntestDf['Fare'] = groupedFare['Fare'].apply(lambda x: x.fillna(x.median()))","7bee94a9":"testDf.loc[testDf['PassengerId'].isin([1044, 1158, 1264])]","0741c0ec":"#plot target variable vs other columns\ndef plotCategricalVariable(data, columns, target):\n    fig = plt.figure(figsize=(25, 10))\n\n    count = 1\n    for column in columns:\n        plt.subplot(2,3,count)\n        ax = sns.countplot(x=column, data=data,  hue=target)\n        \n        for p in ax.patches:\n            ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x(), p.get_height() + 2))\n        \n        count = count + 1\n        plt.legend('')\n\n    plt.legend(loc='upper right')\n    plt.show()","702e3fbb":"#plot survival probability of target variable\ndef plotSurvivalProbability(data, target, size=1):\n    sns.factorplot(x=target, y='Survived', data=data, kind='bar', aspect=size)\n    \n    plt.ylabel('Survival Probability')\n    plt.show()","acfc6f23":"#create data frame and store value counts and percentages\ndef valueCount(data, traget):\n    return pd.DataFrame({\n        'Count': data[traget].value_counts(),\n        'Precent(%)': data[traget].value_counts(normalize=True) * 100\n    })","1c6c2b28":"valueCount(trainDf, 'Survived')","2657999a":"trainDf['Name'].value_counts()","5c7a793e":"#apply on both train and test data\ntrainDf['title'] = trainDf['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\ntestDf['title'] = testDf['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())","393b4a12":"valueCount(trainDf, 'title')","fce51e5f":"data = [trainDf, testDf]\nfor dataset in data:\n    dataset['title'] = dataset['title'].replace(['Lady', 'the Countess','Capt', 'Col','Don', 'Dr','Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['title'] = dataset['title'].replace('Mlle', 'Miss')\n    dataset['title'] = dataset['title'].replace('Ms', 'Miss')\n    dataset['title'] = dataset['title'].replace('Mme', 'Mrs')\n    dataset['title'] = dataset['title'].replace('Master', 'Mr')","501e9cba":"plotSurvivalProbability(trainDf, 'title')","d536e498":"valueCount(trainDf, 'Sex')","04d540d7":"plotSurvivalProbability(trainDf, 'Sex')","48ebbf08":"plotCategricalVariable(trainDf, ['Survived', 'Pclass', 'SibSp', 'Parch', 'Embarked'], 'Sex')","66be6610":"valueCount(trainDf, 'Embarked')","f18f4f77":"plotSurvivalProbability(trainDf, 'Embarked')","cb855502":"#create new dataframe with latitude and longitude\ncols = {'Port': ['Southampton', 'Cherbourg', 'Queesntown'], \n        'lat': [50.90984604447305, 49.64640430610172, 51.849607638998094],\n        'lon': [-1.4356916440426455, -1.6115986306815056, -8.299957213678002],\n        'Embarked Count': [644, 168, 77],\n        'Survived Count': [217, 93, 30]\n       }\n\ndfMap = pd.DataFrame(cols)","1628ec68":"import plotly.express as px\nfig = px.scatter_mapbox(dfMap, \n                        lat='lat', \n                        lon='lon',\n                        hover_data=['Port', 'Embarked Count', 'Survived Count'], \n                        color='Port', \n                        size='Embarked Count',\n                        zoom=5)\n\nfig.update_layout(mapbox_style='stamen-terrain')\nfig.show()","795add6e":"plotCategricalVariable(trainDf, ['Survived', 'Pclass', 'SibSp', 'Parch', 'Sex'], 'Embarked') ","64fce0ca":"valueCount(trainDf, 'Pclass')","eff8ddbb":"plotSurvivalProbability(trainDf, 'Pclass')","e4bbe933":"plotCategricalVariable(trainDf, ['Survived', 'Embarked', 'SibSp', 'Parch', 'Sex'], 'Pclass')","4a326857":"trainDf['family_size'] = trainDf['SibSp'] + trainDf['Parch'] + 1\ntestDf['family_size'] = testDf['SibSp'] + testDf['Parch'] + 1","8642846c":"#create family_group feature\ndef familyGroup(row):\n    if row['family_size'] == 1:\n        family = 1\n    elif (row['family_size'] == 2):\n        family = 2\n    elif ((row['family_size'] >= 3) & (row['family_size'] <= 6)):\n        family = 3\n    else:\n        family = 4\n        \n    return family\n\n#apply on both train and test data\ntrainDf['family_group'] = trainDf.apply(familyGroup, axis=1)\ntestDf['family_group'] = trainDf.apply(familyGroup, axis=1)","5eb3d4b9":"trainDf['family_group'].isnull().sum()","0e27a32a":"testDf['family_group'].isnull().sum()","3af0a188":"trainDf.head()","bf70c122":"valueCount(trainDf, 'family_group')","24533cca":"plotSurvivalProbability(trainDf, 'family_group')","243e68ea":"plotCategricalVariable(trainDf, ['Survived', 'Embarked', 'Pclass', 'Sex'], 'family_group')","a3c1951c":"trainDf['Cabin'].value_counts()","0c83e76c":"trainDf['deck'] = [x[0] if isinstance(x, str) else 'M' for x in trainDf['Cabin']]\ntestDf['deck'] = [x[0] if isinstance(x, str) else 'M' for x in testDf['Cabin']]","82849fc4":"valueCount(trainDf, 'deck')","645abea6":"plotSurvivalProbability(trainDf, 'deck')","70892672":"plotCategricalVariable(trainDf, ['Survived', 'Embarked', 'Pclass', 'Sex'], 'deck')","f2e69e6e":"trainDf['Age'].describe()","737b6bfb":"pd.qcut(trainDf['Age'], 5)","78f2b2ec":"#create age_group feature\ndef ageGroup(row):\n    if row['Age'] <= 19:\n        age = 1\n    elif ((row['Age'] > 19) & (row['Age'] <= 26)):\n        age = 2\n    elif ((row['Age'] > 26) & (row['Age'] <= 30)):\n        age = 3\n    elif ((row['Age'] > 30) & (row['Age'] <= 40)):\n        age = 4\n    else:\n        age = 5\n        \n    return age\n\n#apply on both train and test data\ntrainDf['age_group'] = trainDf.apply(ageGroup, axis=1)\ntestDf['age_group'] = testDf.apply(ageGroup, axis=1)","f0c2c24a":"trainDf['age_group'].isnull().sum()","6201cc2a":"testDf['age_group'].isnull().sum()","615ec548":"trainDf.head()","25488bc5":"plotSurvivalProbability(trainDf, 'age_group')","189e7013":"fig = plt.figure(figsize=(25, 10))\n\nplt.subplot(241)\nsns.histplot(data=trainDf, x='Age', kde=True, color='skyblue', element='step')\n\nplt.subplot(242)\nstats.probplot(trainDf['Age'], dist='norm', plot=plt)\n\nplt.subplot(243)\nsns.histplot(data=trainDf, x='Age', kde=True, color='skyblue', hue='Survived', element='step')\n\nplt.subplot(244)\nax = sns.countplot(data=trainDf, x='age_group')\nfor p in ax.patches:\n    ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x(), p.get_height() + 2))\n        \nplt.subplot(245)\nax = sns.countplot(data=trainDf, x='age_group', hue='Survived')\nfor p in ax.patches:\n    ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x(), p.get_height() + 2))\n\nplt.subplot(246)\nax = sns.countplot(data=trainDf, x='age_group', hue='Sex')\nfor p in ax.patches:\n    ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x(), p.get_height() + 2))\n\nplt.subplot(247)\nax = sns.countplot(data=trainDf, x='age_group', hue='Pclass')\nfor p in ax.patches:\n    ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x(), p.get_height() + 2))\n    \nplt.subplot(248)\nsns.scatterplot(data=trainDf, x='Fare', y='Age')\n\nplt.show()","aefc7684":"import scipy.stats  as stats\n\ndfClean = trainDf.dropna()\nstats.pearsonr(dfClean['Age'], dfClean['Fare'])","d876cfc9":"trainDf['Fare'].describe()","0cf49892":"pd.qcut(trainDf['Fare'], 5)","4ecef40f":"#create fare_group feature\ndef fareGroup(row):\n    if row['Fare'] <= 7.896:\n        fare = 1\n    elif ((row['Fare'] > 7.896) & (row['Fare'] <= 11.242)):\n        fare = 2\n    elif ((row['Fare'] > 11.242) & (row['Fare'] <= 23)):\n        fare = 3\n    elif ((row['Fare'] > 23) & (row['Fare'] <= 41.579)):\n        fare = 4\n    else:\n        fare = 5\n        \n    return fare\n\n#apply on both train and test data\ntrainDf['fare_group'] = trainDf.apply(fareGroup, axis=1)\ntestDf['fare_group'] = testDf.apply(fareGroup, axis=1)","b4c8bea5":"trainDf['fare_group'].isnull().sum()","31f9c5d3":"testDf['fare_group'].isnull().sum()","b6f2faee":"fig = plt.figure(figsize=(25, 6))\n\nplt.subplot(131)\nsns.histplot(data=trainDf, x='Fare', kde=True, color='skyblue', element='step')\n\nplt.subplot(132)\nstats.probplot(trainDf['Fare'], dist='norm', plot=plt)\n\nplt.subplot(133)\nsns.histplot(data=trainDf, x='Fare', kde=True, color='skyblue', hue='Survived', element='step')\n\nplt.show()","1e884011":"plotSurvivalProbability(trainDf, 'fare_group')","23bd43ce":"plotCategricalVariable(trainDf, ['Survived', 'Embarked', 'Pclass', 'Sex'], 'fare_group')","7c9acdb3":"trainDf['fare_round'] = trainDf['Fare'].round(2)\ntestDf['fare_round'] = testDf['Fare'].round(2)","83ae391a":"#one-hot encode using dummy variables\ntrainDf = pd.get_dummies(trainDf, columns =['Sex', 'Embarked', 'title'], drop_first=True)\ntestDf = pd.get_dummies(testDf, columns =['Sex', 'Embarked', 'title'], drop_first=True)","d0c42036":"deck = {\n    'A': 9,\n    'B': 8,\n    'C': 7,\n    'D': 6,\n    'E': 5,\n    'F': 4,\n    'G': 3,\n    'T': 2,\n    'M': 1\n}\n\n#apply on both train and test data\ntrainDf['deck'] = trainDf['deck'].map(deck)\ntestDf['deck'] = testDf['deck'].map(deck)","385796c0":"trainDf.head()","7ac0151e":"testDf.head()","5c7cd703":"#drop unwanted variables from train and test datasets\ntrainSelected = trainDf.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Fare'], axis=1)\ntestSelected = testDf.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Fare'], axis=1)","c855e6b4":"trainSelected.head()","022d8dbd":"testSelected.head()","ab014c9d":"X = trainSelected.drop('Survived', axis=1)\nY = trainSelected['Survived']","73fd59a8":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7, random_state=50)","76926570":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score","79e7c144":"#train preliminary tree\ndtPreliminary = DecisionTreeClassifier(random_state=42) \ndtPreliminary.fit(x_train, y_train)","bed46676":"plt.figure(figsize=(25, 10))\ntree.plot_tree(dtPreliminary, filled=True, rounded=True, class_names=['No', 'yes'], feature_names=X.columns)\n\nplt.suptitle('Preliminary Tree')\nplt.show()","85173dbb":"plot_confusion_matrix(dtPreliminary, x_test, y_test, display_labels=['No', 'Yes'])","414ad61a":"dtp = dtPreliminary.predict(x_test)\n\nprint('Accuracy: {}'.format(accuracy_score(y_test, dtp)))\nprint('Recall: {}'.format(recall_score(y_test, dtp)))\nprint('Precision: {}'.format(precision_score(y_test, dtp)))\nprint('F1: {}'.format(f1_score(y_test, dtp, average=None)))","63d7bf28":"#determine values for alpha\npath = dtPreliminary.cost_complexity_pruning_path(x_train, y_train)\nccp_alphas = path.ccp_alphas\nccp_alphas = ccp_alphas[:-1]","33872c48":"#cross validate to find the best alpha\nalpha = []\n\nfor ccp_alpha in ccp_alphas:\n    dtcv = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) \n    scores = cross_val_score(dtcv, X, Y, cv=10, scoring='accuracy')\n    alpha.append([ccp_alpha, np.mean(scores), np.std(scores)])\n    \nalphaDf = pd.DataFrame(alpha, columns=['Alpha', 'Mean Accuracy', 'Standard Deviation'])","1167f766":"#plot mean accuracy and aplha\nalphaDf.plot(x='Alpha', y='Mean Accuracy', yerr='Standard Deviation', marker='o', linestyle='--', figsize=(25, 10))\nplt.show()","ae5939bd":"alphaDf.sort_values(by=['Mean Accuracy'], ascending=False)","9fa1b462":"#train final tree with selected alpha\ndtFinal = DecisionTreeClassifier(random_state=42, ccp_alpha=0.005324) \ndtFinal.fit(x_train, y_train)","7ffb0b53":"plt.figure(figsize=(25, 10))\ntree.plot_tree(dtFinal, filled=True, rounded=True, class_names=['No', 'yes'], feature_names=X.columns)\n\nplt.suptitle('Final Tree')\nplt.show()","1756d287":"plot_confusion_matrix(dtFinal, x_test, y_test, display_labels=['No', 'Yes'])","dab47060":"dtf = dtFinal.predict(x_test)\n\nprint('Accuracy: {}'.format(accuracy_score(y_test, dtf)))\nprint('Recall: {}'.format(recall_score(y_test, dtf)))\nprint('Precision: {}'.format(precision_score(y_test, dtf)))\nprint('F1: {}'.format(f1_score(y_test, dtf, average=None)))","4cbb651e":"Based on our findings, we can conclude that:\n1. 70.6% of missing data belongs to non-survived passengers and only 29.4% of missing data belongs to survived passengers.\n2. Male passengers have a higher number of missing data when compare to females.\n3. Lower class have a higher number of missing data.\n4. According to SipSp and Parch, a higher number of missing age data belongs to passengers who traveled alone.\n5. Survived and age missign data are also related(Chi-Square statistical test).\n\nTherefore, we can assume that cabin data missing at random. How do we handle age missing values? \n1. Mean, Median imputation.\n2. Grouped mean, median imputation.\n3. Multivariate imputation (KNNImputer or IterativeImputer).\n\nWhich one do we use, I think we need to go with either Grouped or Multivariate methods. Because in this scenario simple imputation method such as mean or median will not be accurate.","fb872c02":"According to the confusion matrix, our tree was able to correctly predict 78(73%) survived passengers and 140(86%) non survived passengers. Accuracy, recall and precision are better than the preliminary tree. Therefore, we can conclude that our pruned tree is better than the full-size tree :)\n\nHope you've enjoyed my work, if you like my work and need to share something with me leave a comment :)","5e57c21c":"We can group passengers based on the fare they paid. Therefore, here I have chosen five fare groups and created a new feature called \"fare_group\".","7ea592d8":"The Chi-Square test is statistically significant at p-value(0.05). Therefore, we can conclude that Survived and cabin missign data are related.","25525e6e":"Finally, our training dataset does not have any missing values :). Let's move to test data.\n\n### Test Data","4bfee6b1":"1. The Boat Deck\n2. A Deck\n3. B Deck\n4. C Deck\n5. D Deck\n6. E Deck\n7. F Deck\n8. G Deck\n9. Orlop Decks\n10. Tank Top\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/0\/0d\/Olympic_%26_Titanic_cutaway_diagram.png\/220px-Olympic_%26_Titanic_cutaway_diagram.png)\n\nhttps:\/\/en.wikipedia.org\/wiki\/Titanic","6bd51ba3":"Let's plot three ports on the map using latitude and longitude. I used Google Maps to get the latitude and longitude of the locations.","b162e362":"We can see that there are 18 different titles, I think we can reduce and generalize these titles into a few. To do that let's create a new mapping.","5629a903":"<div class='alert alert-block alert-success' id='dpp'><strong>Data Pre-Processing<\/strong><\/div>\n\n#### 1. Round  \"Fare\"\n\nFare contains 4 decimal places, with 2 decimal places decision tree was able to improve model performance.","9ee1afaa":"As you see on the table, the highest accuracy is 0.818252 but the error is +-4. Therefore, let's try to find higher accuracy with less error. Ok, you can see accuracy = 0.811461 has less error +-2. Therefore, I will use that alpha = 0.005324 on our final tree.","aba28dbf":"We can group passengers based on their age. Therefore, here I have chosen five age groups and created a new feature called \"age_group\".","bdd8daac":"Ok, now our pruned tree is ready and you can see how smaller is the pruned tree. Moreover, even though we provided 17 features, the tree has only used only 6 features. Let's see how it performs on the testing data.","ca8657b9":"Ok, now our tree is ready and it is a huge tree. Let's see how it performs on the testing data.","35c8e7a2":"Ok, our X and Y are ready, it is time to split data into training and testing sets. Here, I use 70% - 30% ratio. My training set will take 70% of the data and the test set will take 30% of data.","a2220bcc":"The training dataset contains 891 rows and 12 columns. \n\n1. Categorical variables - PassengerId, Survived, Pclass, SibSp, Parch, Name, Sex, Ticket, Cabin and Embarked\n1. Numerical variables - Age, Fare\n\n#### Test Data","40f50d10":"We can see that also on test data, there are three variables that contain missing data. 78.23% of cabin data are missing, 20.57% of age data are missing and 0.24% of fare data are also missing. Assuming train and test data have the same qualities, I will handle test missing data using the approaches used in training data.\n\n#### 1. Cabin\n\nAs we dissected in training data section, I will remove cabin data from the dataset.\n\n#### 2. Age\n\nI will use grouped median for handle missing data of age variable.","684f6126":"Based on our findings we can conclude that:\n1. Distribution is not normally distributed, it is right-skewed.\n2. Passengers who paid high fare had highest survival chance.","4a23c808":"Based on our findings, we can conclude that:\n1. Survival probability of the female passenger is higher than male passengers.\n\nHowever, it is too early to make such a conclusion. Therefore, let's study other variables as well.\n\n#### 2. Sex","393b01b6":"<div class=\"alert alert-block alert-info\"><strong>Content<\/strong><\/div>\n\n<div class=\"list-group\">\n    <a class=\"list-group-item list-group-item-action\" href=\"#mda\">Missing Data Analysis<\/a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#eda\">Exploratory Data Analysis & Feature Engineering<\/a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#dpp\">Data Pre-Processing<\/a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#mtt\">Model Training & Testting<\/a>\n<\/div>","0010de43":"#### 4. Fare","830c7308":"According to the above data, we can see that the data distribution is imbalanced(non-survived passengers(61.6%) and survived passengers(38.4%)). Let's study categorical varaibles.\n\n\n#### 2. Name","2578c843":"According to the confusion matrix, our tree was able to correctly predict 67(63%) survived passengers and 140(86%) non survived passengers. Accuracy, Recall and Precision are not that much good. \n\n* So can we do better? \n* Will this tree perform better on testing data?\n\nAlso, decision trees are well known for overfitting to the training data. One method to reduce overfitting is to prune the tree. Here I will use cost complexity pruning to prune our tree.","4548ada5":"Based on our findings, we can conclude that:\n\n1. 70% of missing data belongs to non-survived passengers and only 30% of missing data belongs to survived passengers.\n2. Some passengers owned few cabins such as passengerId => 8, 89, 873.\n3. Higher number of missing data belongs to male passengers.\n4. Higher number of missing data belongs to passengers who traveled lower and middle class.\n5. According to SipSp and Parch, a higher number of missing data belongs to passengers who traveled alone.\n6. There are significantly higher amounts of missing data are reported passenger age between 16 - 36.\n7. Survived and missing data are related(Chi-Square statistical test).\n\nTherefore, we can assume that cabin data missing at random. How do we handle cabin missing values? I think, we have to consider two things here:\n1. Missing value percentage\n2. Nature of the variable(data)\n\nMissing value percentage is more than 60% and cabin variable contains more than 100 different values and hard to predict. Therefore, we can do two things:\n1. Remove entire varaible\n2. We can create another category for the missing values and replace missing values\n\nI will go with first option.","4b1877c4":"Based on our findings we can conclude that:\n1. Majority of passengers (60.2%) were single.\n2. Highest number of single passengers were boarded from Southampton port.\n3. Large families were traveled on third-class tickets. \n4. Highest survival rate reported by small and medium families. On the other hand, the lowest survival rate reported by large families.\n\n#### 5. Cabin\n\nIf you carefully check, you will notice that the cabin number contains two details,\n1. letter = deck\n2. number = cabin number\n\nWe can group data using deck number since Titanic only had 10 decks and will be easier for our analysis. There are 70% of cabin data are missing(check missing data section). Therefore, I will replace missing data with \"Missing\" this way we will be able to get a proper idea about the data.","6dcfe904":"According to the scatter plot, we can see that there is no correlation between Age and Fare. To further clarify this, let's measures the statistical relationship between the two variables using Pearson correlation.\n\nhttps:\/\/www.statisticssolutions.com\/free-resources\/directory-of-statistical-analyses\/pearsons-correlation-coefficient\/","fd08874f":"#### Training Data","dda42b9d":"Based on our findings we can conclude that:\n1. 55.1% of passengers were on third-class tickets, 24.2%  of passengers were on first-class tickets and 20.6% of passengers were on second-class tickets.\n2. Highest survival probability (55%) were reported passengers who traveled on first-class tickets and the lowest survival probability (24.2%) was reported passengers who traveled on third-class tickets.\n3. 93% of passengers who were boarded from port Queenstown were on third-class tickets.\n4. Majority of male passengers were traveled on third class and almost half of the female passengers were travelled on first class and second class.\n\n#### 4. SibSp and Parch\n\nSibSp and Parch are containing data about family by combining these two variables we group passengers into groups:\n\n* 1 Passenger = 1\n* 2 Passengers = 2\n* 3 - 6 Passengers = 3\n* 7 Passengers > = 4","1058de38":"#### 3. Fare\n\n","322ee342":"<div class='alert alert-block alert-success' id='mtt'><strong>Model Training & Testing<\/strong><\/div>\n\nNow that we have completed data pre-processing, we are ready for formatting data for modeling. The first step is to split data into two parts:\n1.  X - Columns of data that we use to make classification\n2.  Y - Column of data that we use to make a prediction","0c85e445":"The test dataset contains 418 rows and 11 columns. ","cbc4dc94":"<div class=\"alert alert-block alert-success\" id='mda'><strong>Missing Data Aanalysis<\/strong><\/div>\n\nMissing data can have different types of impacts such as:\n1. Missing data can reduce statistical power.\n2. The lost data can cause bias in the estimation of parameters.\n3. Missing data can reduce the representativeness of the samples.\n4. Could lead to invalid conclusions.\n   \nMissing Data Assumptions:\n1. Missing Completely and Random \u2014 MCAR\n2. Missing at Random \u2014 MAR\n3. Missing Not at Random \u2014 MNAR\n\nDifferent types of missing data need to be treated differently in order for any analysis to be meaningful. There are different types of missing data handling methods are available for use and I've listed a few of them below.\n\n1. Deletion\n2. Single Imputation Methods\n3. Multiple Imputation Methods\n\nWant to know more about missing data, here I've attached some articles:\n\n* https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3668100\/ \n* https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK493614\/#:~:text=Missing%20data%20are%20typically%20grouped,the%20observed%20and%20unobserved%20data.&text=In%20other%20words%2C%20no%20systematic,and%20those%20with%20complete%20data\n* https:\/\/www.inwt-statistics.com\/read-blog\/understanding-and-handling-missing-data.html \n* https:\/\/www.mastersindatascience.org\/learning\/how-to-deal-with-missing-data\/ \n* https:\/\/www.displayr.com\/different-types-of-missing-data\/#_edn1\n* https:\/\/towardsdatascience.com\/how-to-handle-missing-data-8646b18db0d4\n\n### Training Data","a243ae2f":"It seems that there is a high number of missing data are related to non-survived passengers. To further clarify this let's Chi-Square test.\n\nThe hypothesis we are going to test:\n* H0: Survived and Age missing data are not related\n* H1: Survived and Age missing data are related","4ff9e0fb":"<div class=\"alert alert-block alert-success\"><strong>The Data<\/strong><\/div>","fc58a901":"Chi-Square test is statistically significant at p-value(0.05). Therefore, we can conclude that Survived and age missign data are also releated.","a9ed763d":"#### 3. Ordinal Encoding\n\nEncode \"deck\", here I'm using Ordinal Encoding since these values are ordinal.","8d62b7f2":"We can see that there are three variables that contain missing data. 77.1% of cabin data are missing, 19.87% of age data are missing and 0.22% of embarked data are also missing. Let's testing data for missing values.\n\n\n#### 1. Cabin","15ef2c91":"#### 2. One-Hot Encoding\n\nEncode \"Sex\" and \"Embarked\" and \"title\", here I'm using One-Hot Encoding since these two values are nominal.","58b40204":"We can see that there are passengers who traveled without paying anything and the ticket number of four of them is mentioned as a \"Line\". But according to history, the Titanic was a luxurious ship and tickets were expensive and the minimum price of the third class ticket was \u00a33 to \u00a38.\n\n* http:\/\/www.jamescamerononline.com\/TitanicFAQ.htm\n* https:\/\/autumnmccordckp.weebly.com\/tickets-and-accomodations.html\n\nI think, even though we assume that four passengers were crew members other data should not be accurate. On the other hand, the highest fare is 512 and all three passengers were on the same ticket. When we see the data distribution it seems this is an outlier but I think it is not. Because 1st class ticket was ranged from \u00a330 - \u00a3870.","211bc7bc":"Let me give you a quick recap before we move to model training and testing, \n\n1. We first addressed missing data on both training and testing datasets.\n2. Then we explore all the variables on the training dataset and created few new features:\n    * title\n    * family_size\n    * family_group\n    * deck\n    * age_group\n    * fare_group\n3. After that, we did the data pre-processing.\n\nOk, I think now you have a good idea about the dataset. Let's move to model training and testing.","121387da":"Based on our findings, we can conclude that:\n1. Survival probability of passengers who were on deck B, C, D, E and F are higher than others.\n2. Regardless of deck survival probability of passengers who traveled in first-class is higher than others.\n\n#### 6. Age","e85441f5":"Based on our findings we can conclude that:\n1. Distribution seems to be normally distributed (gaussian distribution).\n2. Passengers were between 5 months to 80 years old.\n3. Almost half of the passengers were adults (age 25 - 64).\n4. Highest survival probability reported by children(58.4%) and lowest survival probability reported by seniors(9.09%).\n5. Age and Fare do not have any correlation.\n\n#### 6. Fare","b628c385":"<div class='alert alert-block alert-success' id='eda'><strong>Exploratory Data Analysis & Feature Engineering<\/strong><\/div>\n\nOk, It's time to start exploratory data analysis. I will use training data for exploratory data analysis. Let's study our target variable first.","11b5cf00":"Based on our findings, we can conclude that:\n1. Both passengers were females and survived. \n2. Both of them traveled in the same cabin under the same ticket number.\n3. Both of them traveled in first class and did not have any relatives on board.\n4. One passenger was in her 60s and another one was in her 30s.\n5. All other passengers that traveled in the B section (B20 - B22) were boarded in Southampton. \n\nBased on above findings we can assume that:\n1. Both passengers were boarded in the same port.\n2. Since both passengers under the same ticket, there is a chance that one lady traveled as a maid.\n3. All other passengers were in the B section boarded in Southampton port. Therefore we can assume that these 2 passengers were also boarded from the same port.\n\nIt seems that data has missing at random or missing, not at random. How do we verify our assumptions, only way to do that is to study the past. The easy way to study the past is to do some google search :) Let's try to search passenger names on google. Yes, our assumptions are correct \"Mrs. Stone boarded the Titanic in Southampton on 10 April 1912 and was traveling in first class with her maid Amelie Icard. She occupied cabin B-28.\" \n\n* https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html\n* https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/amelia-icard.html\n\nWe can update those two missing data with \"S\".","6f6a370d":"Based on our findings we can conclude that:\n1. Majority of passengers (72.4%) were boarded from Southampton port.\n2. More than half of male and female passengers were boarded from Southampton port.\n3. Highest survival probability (55%) was reported for passengers who boarded from Cherbourg port and the lowest survival probability (33%) was reported for passengers who boarded from Southampton port.\n\n#### 3. Pclass","11ed1fbf":"\n#### 2. Embraked","10d3a2bb":"Now it's time to understand more about missing data. Therefore let's try to understand missing data in training data first. I will start with cabin data.","bb2be7e5":"It seems that there is a high number of missing data are related to non-survived passengers. To further clarify this let's do the statistical test(Chi-Square ).\n\nThe hypothesis we are going to test:\n\n* H0: Survived and Cabin Missing data are not related\n* H1: Survived and Cabin Missing data are related\n\nWant to learn about the Chi-Square test:\n\nhttps:\/\/www.youtube.com\/watch?v=LE3AIyY_cn8&ab_channel=statslectures\nhttps:\/\/machinelearningmastery.com\/chi-squared-test-for-machine-learning\/","62022c30":"#### 3. Age","aafb1a4b":"#### 1. Survived","2fd7317f":"Based on our findings we can conclude that:\n1. 64.7% of passengers are male and 35.2% are female passengers\n2. Survival probability of female passengers is higher than male passengers, only 109 male passengers were survived, when compared with total passengers it is 12.2%\n\n#### 2. Embarked\n\nAs you know, after leaving Southampton on 10 April 1912, Titanic called Cherbourg in France and Queenstown in Ireland, before heading west to New York.","0d030661":"Ok, now our test dataset also does not contain any missing data. ","fa948886":"### Decision Tree\n\nIt's time to train our decision tree. First, I will train the preliminary tree and test the performance of the preliminary tree using test data."}}