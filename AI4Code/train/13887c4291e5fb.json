{"cell_type":{"1c9f4767":"code","d783d562":"code","e036b06b":"code","d5cfc952":"code","85956e8d":"code","d68d528a":"code","ff84a8b8":"code","7cf124e9":"code","11776bd1":"code","c2736089":"code","6da101b4":"code","093fe9f5":"code","ca34e14d":"code","5fa3091a":"code","6342571f":"code","53d190dd":"code","e05dad71":"code","37cd809e":"code","44632dcf":"code","c5abfbd6":"code","2a2a958c":"code","d68f0047":"code","83b7833f":"code","4590d063":"code","df9197de":"code","fcb61e3b":"code","3f33e07a":"code","b1d89b81":"code","0c436021":"code","bb30bdc0":"code","3ec22e28":"code","3391f884":"code","75e74f62":"code","655f5319":"code","534065ec":"code","ec74db97":"code","0cdcfd4e":"code","a5af84fb":"code","2222741d":"code","afeff520":"code","bbe668c2":"code","d13dcb21":"code","dc3d9b0b":"code","8ba9be56":"code","c9608a39":"code","2713a538":"code","bd9700b9":"markdown","8f682a57":"markdown","25ee918b":"markdown","405d0e2b":"markdown","a8718572":"markdown","70962b66":"markdown"},"source":{"1c9f4767":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d783d562":"df=pd.read_csv(\"..\/input\/startup-success-prediction\/startup data.csv\")\ndf.head()","e036b06b":"df.info()\n\n# We have 49 columns and 923 rows, some columns have missing values","d5cfc952":"# Number of missing values\n\ndf.isnull().sum().sort_values(ascending=False).head(10)","85956e8d":"# Let's look at percentage of missing values\n\nprint(\"Percentage of missing values in 'closed_at' column: % {:.2f}\". format((df.closed_at.isnull().sum())\/len(df)*100))\nprint(\"Percentage of missing values in 'Unnamed: 6' column: % {:.2f}\". format((df[\"Unnamed: 6\"].isnull().sum())\/len(df)*100))\nprint(\"Percentage of missing values in 'age_last_milestone_year' column  : % {:.2f}\". format((df.age_last_milestone_year.isnull().sum())\/len(df)*100))\nprint(\"percentage of missing values in 'age_first_milestone_year' column : % {:.2f}\". format((df.age_first_milestone_year.isnull().sum())\/len(df)*100))","d68d528a":"#  \"Unnamed: 6\", \"Unnamed: 0\", \"id\", \"closed_at\" columns are not necessary so drop it\ndf.drop([\"Unnamed: 6\"],axis=1, inplace=True)\ndf.drop([\"Unnamed: 0\"], axis=1, inplace=True)\ndf.drop([\"id\"], axis=1, inplace=True)\ndf.drop([\"closed_at\"], axis=1, inplace=True)","ff84a8b8":"for index, row in df.iterrows():\n    if row['state_code']!=row['state_code.1']:\n        print(index, row['state_code'], row['state_code.1'])\n\n# \"state_code\" column and \"state_code.1\" column must be the same, so we should drop the \"state_code.1\" and also, \n# \"state_code.1\" column has a one missing value in the 515. row. ","7cf124e9":"df.drop([\"state_code.1\"], axis=1, inplace=True)","11776bd1":"# \"status_closed\" column is for prediction of startup success and this is binary classification so we should convert numerical variable by using get_dummies( function) in pandas\ndf=pd.get_dummies(df, columns=[\"status\"], drop_first=True)","c2736089":"# Year columns must be converted to datetime type\n\ndf.founded_at=pd.to_datetime(df.founded_at)\ndf.first_funding_at=pd.to_datetime(df.first_funding_at)\ndf.last_funding_at=pd.to_datetime(df.last_funding_at)\n\ndf.head(3)","6da101b4":"# What does it mean \"age_first_funding_year\", \"age_last_funding_year\", \"age_first_milestone_year\", \"age_last_milestone_year\" , let's together analyze it\n\nplt.figure(figsize=(18,3),dpi=100)\n\nplt.subplot(1,4,1)\nsns.scatterplot((df[\"first_funding_at\"].dt.year - df[\"founded_at\"].dt.year), df[\"age_first_funding_year\"])\nplt.xlabel(\"Difference 'Founded' and 'First Funding'\")\n\nplt.subplot(1,4,2)\nsns.scatterplot((df[\"last_funding_at\"].dt.year- df[\"founded_at\"].dt.year), df[\"age_last_funding_year\"])\nplt.xlabel(\"Difference 'Founded' and 'Last Funding'\");\n\nplt.subplot(1,4,3)\nsns.scatterplot(df[\"age_first_funding_year\"], df[\"age_first_milestone_year\"])\n\nplt.subplot(1,4,4)\nsns.scatterplot(df[\"age_last_funding_year\"], df[\"age_last_milestone_year\"]);\n\n\n# As we see the graph, we can say high correlation between funding date and age funding. Difference between \"last_funding_at\" and \"founded_at\" is related \"age_last_funding_year\".\n# \"age_first_funding_year\" and \"age_last_funding_year\" have negative values,it shouldn't be and also it can not be that \"founded\" date higher than \"first_funding_at\" and \"last_funding_at\"\n# So we must get the absolute value of columns including negative value","093fe9f5":"age=[\"age_first_funding_year\",\"age_last_funding_year\",\"age_first_milestone_year\",\"age_last_milestone_year\"]\n\nfor a in range(len(age)):\n    print(\"Is there any negative value in '{}' column  : {} \".format(age[a],(df[age[a]]<0).any()))\n          ","ca34e14d":"# Which rows have negative values? look at one of them\nfor index, rows in df.iterrows():\n    if rows[\"age_first_funding_year\"]<0:\n        print(index, rows[\"age_first_funding_year\"])","5fa3091a":"# we must get the absolute value of columns including negative value\n\ndf[\"age_first_funding_year\"]=np.abs(df[\"age_first_funding_year\"])\ndf[\"age_last_funding_year\"]=np.abs(df[\"age_last_funding_year\"])\ndf[\"age_first_milestone_year\"]=np.abs(df[\"age_first_milestone_year\"])\ndf[\"age_last_milestone_year\"]=np.abs(df[\"age_last_milestone_year\"])","6342571f":"age=[\"age_first_funding_year\",\"age_last_funding_year\",\"age_first_milestone_year\",\"age_last_milestone_year\"]\n\nfor a in range(len(age)):\n    print(\"Is there any negative value in '{}' column  : {} \".format(age[a],(df[age[a]]<0).any()))\n    \n# Now, we get rid of negative values","53d190dd":"# After we get the absolute, visualize relationships\n\nplt.figure(figsize=(16,3),dpi=100)\n\nplt.subplot(1,4,1)\nsns.scatterplot(np.abs(df[\"first_funding_at\"].dt.year - df[\"founded_at\"].dt.year), df[\"age_first_funding_year\"])\nplt.xlabel(\"Difference 'Founded' and 'First Funding'\")\n\nplt.subplot(1,4,2)\nsns.scatterplot(np.abs(df[\"last_funding_at\"].dt.year- df[\"founded_at\"].dt.year), df[\"age_last_funding_year\"])\nplt.xlabel(\"Difference 'Founded' and 'Last Funding'\");\n\nplt.subplot(1,4,3)\nsns.scatterplot(df[\"age_first_funding_year\"], df[\"age_first_milestone_year\"])\n\nplt.subplot(1,4,4)\nsns.scatterplot(df[\"age_last_funding_year\"], df[\"age_last_milestone_year\"]);","e05dad71":"# I will fill the missing values by mean() function\n\ndf[\"age_first_milestone_year\"].fillna((df[\"age_first_milestone_year\"].mean()), inplace=True)\ndf[\"age_last_milestone_year\"].fillna((df[\"age_last_milestone_year\"].mean()), inplace=True)\n","37cd809e":"# After we get the absolute, visualize relationships\n\nplt.figure(figsize=(16,3),dpi=100)\n\nplt.subplot(1,2,1)\nsns.scatterplot(df[\"age_first_funding_year\"], df[\"age_first_milestone_year\"])\n\nplt.subplot(1,2,2)\nsns.scatterplot(df[\"age_last_funding_year\"], df[\"age_last_milestone_year\"]);","44632dcf":"df.describe()","c5abfbd6":"# To find how much there are outliers in dataset, we should use only continuous variables, because rest of numerical variables are binary variables including 0 and 1\n\nvariable=[\"age_first_funding_year\",\"age_last_funding_year\",\"age_first_milestone_year\",\"age_last_milestone_year\"]\n\nplt.figure(figsize=(17,3),dpi=100)\nfor i in range(len(variable)):\n    plt.subplot(1,4,i+1)\n    plt.title(\"{}\". format(variable[i]))\n    plt.boxplot(df[variable[i]]);","2a2a958c":"variable=[\"age_first_funding_year\",\"age_last_funding_year\",\"age_first_milestone_year\",\"age_last_milestone_year\"]\n\nplt.figure(figsize=(17,3),dpi=100)\nfor i in range(len(variable)):\n    plt.subplot(1,4,i+1)\n    plt.title(\"{}\". format(variable[i]))\n    sns.distplot(df[variable[i]], color=\"orange\");\n    ","d68f0047":"# For only one column, analyze that number of outliers\n\nfrom scipy.stats import zscore\n\nzscores=zscore(df[\"age_first_funding_year\"])\n\nfor threshold in range(1,8,1):\n    print(\"Threshold value: {}\". format(threshold))\n    print(\"Number of outliers: {}\".format(len(np.where(zscores>threshold)[0])))\n    print(\"------------------------\")\n","83b7833f":"plt.figure(figsize=(15,7),dpi=100)\n\ndf[\"log_first_fundig\"]=np.log(df[\"age_first_funding_year\"]+1)\nplt.subplot(2,4,1)\nplt.xlabel(\"log_first_fundig\")\nplt.boxplot(df[\"log_first_fundig\"])   \n\nplt.subplot(2,4,5)\nsns.distplot(df[\"log_first_fundig\"] , color=\"green\");\n\n\ndf[\"log_last_fundig\"]=np.log(df[\"age_last_funding_year\"]+1)\nplt.subplot(2,4,2)\nplt.xlabel(\"log_last_fundig\")\nplt.boxplot(df[\"log_last_fundig\"])   \n\nplt.subplot(2,4,6)\nsns.distplot(df[\"log_last_fundig\"], color=\"green\")\n\n\ndf[\"log_first_milestone\"]=np.log(df[\"age_first_milestone_year\"]+1)\nplt.subplot(2,4,3)\nplt.xlabel(\"log_first_milestone\")\nplt.boxplot(df[\"log_first_milestone\"])   \n\nplt.subplot(2,4,7)\nsns.distplot(df[\"log_first_milestone\"], color=\"green\")\n\n\ndf[\"log_last_milestone\"]=np.log(df[\"age_last_milestone_year\"]+1)\nplt.subplot(2,4,4)\nplt.xlabel(\"log_last_milestone\")\nplt.boxplot(df[\"log_first_fundig\"])   \n\nplt.subplot(2,4,8)\nsns.distplot(df[\"log_last_milestone\"], color=\"green\");","4590d063":"plt.figure(figsize=(16,4),dpi=100)\n\n\n# \"avg_participants\"  column has negative value but it shouldn't be, so firstly we should get the absolute of the column\ndf[\"avg_participants\"]=np.abs(df[\"avg_participants\"])\n\n\nplt.subplot(1,4,1)\nplt.title(\"Avg Participant Outliers\")\nplt.boxplot(df[\"avg_participants\"])\n\nplt.subplot(1,4,2)\nplt.title(\"Histogram of Avg Participants\")\nsns.distplot(df[\"avg_participants\"], color=\"green\")\n\nplt.subplot(1,4,3)\ndf[\"log_avg_participants\"]=np.log(df[\"avg_participants\"]+1)\nplt.title(\"Logaritmic Avg Participants\")\nplt.boxplot(np.log(df[\"log_avg_participants\"]))\n\nplt.subplot(1,4,4)\nplt.title(\"Histogram of Logaritmic Avg Participants\")\nsns.distplot(np.log(df[\"log_avg_participants\"]), color=\"green\");\n\n# After we get the logaritmic of \"avg_participant\" column, we get rid of the outliers but anyway this column still is not normal distribution","df9197de":"df_state=df.groupby([\"state_code\"])[\"funding_total_usd\"].sum().sort_values(ascending=False).reset_index().head(12)\n\nplt.figure(figsize=(18,6), dpi=100)\nplt.subplot(2,2,1)\nplt.ylabel(\"First 10 state\")\nplt.xlabel(\"Total USD of Funding\")\nsns.barplot(df_state[\"state_code\"],df_state[\"funding_total_usd\"], palette=\"Greens\")\n\n\nplt.subplot(2,2,2)\ndf_funding=df.groupby([\"state_code\"])[\"funding_rounds\"].sum().sort_values(ascending=False).reset_index().head(12)\nsns.barplot(df_funding[\"state_code\"], df_funding[\"funding_rounds\"], palette=\"Greens\")\n\nplt.subplot(2,2,3)\nsns.countplot(df[\"state_code\"])\nplt.xticks(rotation=55);\n","fcb61e3b":"plt.figure(figsize=(18,4),dpi=100)\nplt.xticks(rotation=45)\nplt.title(\"Category Type Counts\")\nsns.countplot(df[\"category_code\"], edgecolor=sns.color_palette(\"dark\"));","3f33e07a":"plt.figure(figsize=(18,4),dpi=100)\n\nplt.xticks(rotation=42)\nplt.title(\"According to category top500\")\nsns.countplot(x=df[\"category_code\"], hue=df[\"is_top500\"], palette=\"Greens\");","b1d89b81":"plt.figure(figsize=(18,4),dpi=100)\n\nplt.xticks(rotation=42)\nplt.title(\"According to category has_angel\")\nsns.countplot(x=df[\"category_code\"], hue=df[\"has_angel\"], palette=\"Greens\");","0c436021":"plt.figure(figsize=(18,4),dpi=100)\n\nplt.xticks(rotation=42)\nplt.title(\"According to category status_closed\")\nsns.countplot(x=df[\"category_code\"], hue=df[\"status_closed\"], palette=\"Greens\")\nplt.legend(loc=1);","bb30bdc0":"plt.figure(figsize=(18,4),dpi=100)\n\nplt.xticks(rotation=42)\nplt.title(\"According to category total USD\")\nsns.barplot(x=df[\"category_code\"], y=df[\"funding_total_usd\"], palette=\"Greens\");","3ec22e28":"plt.figure(figsize=(18,4),dpi=100)\n\nplt.xticks(rotation=42)\nplt.title(\"According to category avg_participants\")\nsns.barplot(x=df[\"category_code\"], y=df[\"avg_participants\"], palette=\"Greens\");","3391f884":"plt.figure(figsize=(16,4),dpi=100)\n\nplt.subplot(1,3,1)\nsns.barplot(df[\"is_top500\"], df[\"funding_total_usd\"], palette=\"Greens\")\n\nplt.subplot(1,3,2)\nsns.barplot(df[\"has_angel\"], df[\"funding_total_usd\"], palette=\"Greens\")\n\nplt.subplot(1,3,3)\nsns.countplot(df.milestones, palette=\"Greens\");","75e74f62":"plt.figure(figsize=(22,4),dpi=100)\n\n\ncol=[\"log_first_fundig\",\"log_last_fundig\",\"log_first_milestone\",\"log_last_milestone\",\"log_avg_participants\"]\n\n\nfor i in range(len(col)):\n    plt.subplot(1,5,i+1)\n    sns.barplot(df[\"funding_rounds\"],df[col[i]], palette=\"Greens\");\n","655f5319":"plt.figure(figsize=(20,3),dpi=100)\nplt.subplot(1,3,1)\nsns.scatterplot(df[\"age_first_funding_year\"],df[\"age_last_funding_year\"], label=\"first&last funding\", palette=\"Greens\")\nsns.scatterplot(df[\"age_first_milestone_year\"], df[\"age_last_milestone_year\"], label=\"first&last milestone\", palette=\"Blues\")\nplt.legend()\n\nplt.subplot(1,3,2)\nsns.distplot(df[\"age_first_funding_year\"], label=\"first_funding\")\nsns.distplot(df[\"age_last_funding_year\"], label=\"last_funding\")\nsns.distplot(df[\"age_first_milestone_year\"], label=\"first_milestone\")\nsns.distplot(df[\"age_last_milestone_year\"], label=\"last_milestone\")\nplt.xlabel(\"first_funding, last_funding, first_milestone, last_milestone\")\nplt.legend()\n\n\nplt.show()","534065ec":"# The most relational columns with target variable(status_closed) are below:\n\nplt.figure(figsize=(4,8),dpi=100)\n\nfocus_cols = ['status_closed']\ndf_corr=df.corr().filter(focus_cols).drop(focus_cols)\nsns.heatmap(df_corr, annot=True, fmt='.2f');","ec74db97":"from scipy.stats import ttest_ind\n\n# we get the null hypothesis that both groups have equal means.\n\nttest=ttest_ind(df[\"has_angel\"],df[\"funding_total_usd\"])\nprint(\"Is there any differences between means of has_angel and funding_total_usd?\")\nprint(\"--\"*40)\nprint(\"t statistic: {:3f} p_value: {:3f}\". format(ttest[0],ttest[1]),\"\\n\",\"\\n\")\n\n\nprint(\"Is there any differences between means of is_top500 and funding_total_usd?\")\nprint(\"--\"*40)\nttest2=ttest_ind(df[\"is_top500\"],df[\"funding_total_usd\"])\nprint(\"t statistic: {:3f} p_value: {:3f}\". format(ttest2[0],ttest2[1]))\n\n\n# In order to p_value is less than 0.05, we reject the H0 hypothesis so, there is not differences between mean of variables","0cdcfd4e":"# Test whether group differences are significant.\n\n\nttest_3=ttest_ind(df[\"funding_rounds\"], df[\"log_first_fundig\"])    \nprint(\"'funding_rounds' and 'log_first_fundig' t statistic: {:.4f}, p_value: {:.4f}\". format(ttest_3[0], ttest_3[1]))\n   \n    \nttest_4=ttest_ind(df[\"funding_rounds\"], df[\"log_last_fundig\"])    \nprint(\"'funding_rounds' and 'log_last_fundig' t statistic: {:.4f}, p_value: {:.4f}\". format(ttest_4[0], ttest_4[1]))\n\nttest_5=ttest_ind(df[\"funding_rounds\"], df[\"log_first_milestone\"])    \nprint(\"'funding_rounds' and 'log_first_milestone' t statistic: {:.4f}, p_value: {:.4f}\". format(ttest_5[0], ttest_5[1]))\n\nttest_6=ttest_ind(df[\"funding_rounds\"], df[\"log_last_milestone\"])    \nprint(\"'funding_rounds' and 'log_last_milestone' t statistic: {:.4f}, p_value: {:.4f}\". format(ttest_6[0], ttest_6[1]))\n\nttest_7=ttest_ind(df[\"funding_rounds\"], df[\"log_avg_participants\"])    \nprint(\"'funding_rounds' and 'log_avg_participants' t statistic: {:.4f}, p_value: {:.4f}\". format(ttest_7[0], ttest_7[1]))\n\n# In order to p_value is less than 0.05, rejected H0 hypothesis so, there is not difference between means","a5af84fb":"column=[\"log_first_fundig\",\"log_last_fundig\",\"log_first_milestone\",\"log_last_milestone\",\"log_avg_participants\"]\ncolu=[\"age_first_funding_year\",\"age_last_funding_year\",\"age_first_milestone_year\",\"age_first_milestone_year\",\"avg_participants\"]\n\nplt.figure(figsize=(18,3), dpi=100)\nfor j in range(len(colu)):\n    plt.subplot(1,5,j+1)\n    sns.distplot(df[colu[j]], color=\"orange\")\n    \nplt.figure(figsize=(18,3), dpi=100)\nfor i in range(len(column)):\n    plt.subplot(1,5,i+1)\n    sns.distplot(df[column[i]], color=\"green\");","2222741d":"# Test it, whether these variables are normal distribution, for this i will use jarque-bera test function \n\nfrom scipy.stats import jarque_bera\n\ndist=[\"log_first_fundig\", \"log_last_fundig\", \"log_first_milestone\", \"log_last_milestone\", \"log_avg_participants\"]\njarq_df=pd.DataFrame(columns=[\"variable\",\"test statistic\",\"p_value\"])\n\n\nfor d in range(len(dist)):\n    jarq=jarque_bera(df[dist[d]])\n    jarq_df=jarq_df.append({\"variable\":dist[d],\n                   \"test statistic\":jarq[0],\n                   \"p_value\":jarq[1]}, ignore_index=True)\n\ndisplay(jarq_df)    \n\n\n# All of the variables are not the normal distribution because of rejected the H0 hypothesis.\n# H0 --> have normal dstribution\n# HA --> not normal distribution","afeff520":"from sklearn.preprocessing import normalize\n\ndf[\"norm_log_first_funding\"]=normalize(np.array(df[\"log_first_fundig\"]).reshape(1,-1)).reshape(-1,1)\ndf[\"norm_log_last_funding\"]=normalize(np.array(df[\"log_last_fundig\"]).reshape(1,-1)).reshape(-1,1)\ndf[\"norm_log_first_milestone\"]=normalize(np.array(df[\"log_first_milestone\"]).reshape(1,-1)).reshape(-1,1)\ndf[\"norm_log_last_milestone\"]=normalize(np.array(df[\"log_last_milestone\"]).reshape(1,-1)).reshape(-1,1)\ndf[\"norm_log_avg_participants\"]=normalize(np.array(df[\"log_avg_participants\"]).reshape(1,-1)).reshape(-1,1)","bbe668c2":"column2=[\"norm_log_first_funding\",\"norm_log_last_funding\",\"norm_log_first_milestone\",\"norm_log_last_milestone\",\"norm_log_avg_participants\"]\n\nplt.figure(figsize=(18,3), dpi=100)\nfor i in range(len(column2)):\n    plt.subplot(1,5,i+1)\n    sns.distplot(df[column2[i]], color=\"orange\");\n    \nprint(\"Minimum values is norm_log_first_funding\", df[\"norm_log_first_funding\"].min())\nprint(\"Maximum values is norm_log_first_funding\", df[\"norm_log_first_funding\"].max())   \n# Still these columns are not normal distribution","d13dcb21":"# Let's now, try StandardScaler()\n\nfrom sklearn.preprocessing import scale\n\ndf[\"scaled_log_first_funding\"]=scale(df[\"log_first_fundig\"])\ndf[\"scaled_log_last_funding\"]=scale(df[\"log_last_fundig\"])\ndf[\"scaled_log_first_milestone\"]=scale(df[\"log_first_milestone\"])\ndf[\"scaled_log_last_milestone\"]=scale(df[\"log_last_milestone\"])\ndf[\"scaled_log_avg_participants\"]=scale(df[\"log_avg_participants\"])","dc3d9b0b":"column3=[\"scaled_log_first_funding\",\"scaled_log_last_funding\",\"scaled_log_first_milestone\",\"scaled_log_last_milestone\",\"scaled_log_avg_participants\"]\n\nplt.figure(figsize=(18,3), dpi=100)\nfor i in range(len(column3)):\n    plt.subplot(1,5,i+1)\n    sns.distplot(df[column3[i]], color=\"orange\");","8ba9be56":"from scipy.stats.mstats import winsorize\n\ndf[\"winsorize_first_funding\"]=winsorize(df[\"age_first_funding_year\"], (0,0.10))\n\n# For \"age_first_funding\" column we analyze whether there are normal distribution\n\ncolumn4=[\"age_first_funding_year\",\"log_first_fundig\",\"winsorize_first_funding\",\"norm_log_first_funding\",\"scaled_log_first_funding\"]\n\nplt.figure(figsize=(18,3),dpi=100)\n\nfor i in range(len(column4)):\n    plt.subplot(1,5,i+1)\n    sns.distplot(df[column4[i]], color=\"orange\");    \n\n# None of this columns are not normal distribution but we need to select columns closer to normal distribution and these are logaritmic columns.\n","c9608a39":"# I have decided to continue with only logaritmic columns in this dataset, so i will drop that is created new\n\ndf.drop([\"winsorize_first_funding\",\"scaled_log_avg_participants\",\"scaled_log_last_milestone\",\"scaled_log_first_milestone\",\"scaled_log_last_funding\",\"scaled_log_first_funding\"],\n             axis=1, inplace=True)","2713a538":"# Target variable must be in the end\n\ncols = [col for col in df if col != 'status_closed'] + ['status_closed'] \ndf=df[cols]\n\ndf.head()","bd9700b9":"# Test it Statistically","8f682a57":"### Are these variables really normal distribution, calculate it statistically?","25ee918b":"# We have applied exploratory data analysis in this dataset for now, then we will try to predict classification problem using various machine learning algorithms","405d0e2b":"# Before the feature selection, we have to normalize the some columns","a8718572":"# Visualizing","70962b66":"# Handling The Outliers"}}