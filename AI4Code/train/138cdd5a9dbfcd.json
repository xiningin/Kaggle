{"cell_type":{"f95d5dcf":"code","cb7a9e72":"code","75d419e0":"code","89b4c933":"code","fa3a2697":"code","90a5d6dc":"code","eaca8218":"code","52fac7c4":"code","7b7b458a":"code","965d25a1":"code","b039704c":"code","af789b47":"code","cf4b7542":"code","15226a11":"code","e28aeeff":"code","e31d9a24":"code","2d0e1c44":"code","4ddccd1b":"code","fd0f311f":"code","ca5e0aa0":"code","a972408b":"code","f032d64f":"code","8b2711a6":"code","bba280a4":"code","07ecd9af":"code","2bf4ec58":"code","7e44455a":"code","9ea574e0":"code","06be92f0":"code","cb7e661f":"code","efcddedd":"code","b9ab4435":"code","3ecfb287":"code","abcf6ad0":"code","de9fa12e":"code","5a2a16e1":"code","361d154c":"code","4509db2b":"markdown","c0839f55":"markdown","48235736":"markdown","7895ccc0":"markdown","3ce530ad":"markdown","d3ca51ee":"markdown","1a6218e4":"markdown","8b8b58db":"markdown","8f57cd19":"markdown","62b77d8c":"markdown","a847c8f5":"markdown","fb28c1ff":"markdown","30a6e58b":"markdown","584149df":"markdown","32c01299":"markdown","6e104f2a":"markdown","b14d6340":"markdown","3a70a90d":"markdown","d1c99560":"markdown","d4533460":"markdown","9d2ba3fa":"markdown","6c184170":"markdown","ddaa9706":"markdown","002b97e6":"markdown","4b53e76d":"markdown"},"source":{"f95d5dcf":"import pandas as pd\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n%matplotlib inline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport time\nfrom itertools import product","cb7a9e72":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","75d419e0":"INPUTFOLDER = '..\/input\/competitive-data-science-predict-future-sales\/'\n\nitem_categories = pd.read_csv(os.path.join(INPUTFOLDER, 'item_categories.csv'))\nitems           = pd.read_csv(os.path.join(INPUTFOLDER, 'items.csv'))\nsales           = pd.read_csv(os.path.join(INPUTFOLDER, 'sales_train.csv'))\nshops           = pd.read_csv(os.path.join(INPUTFOLDER, 'shops.csv'))\ntest            = pd.read_csv(os.path.join(INPUTFOLDER, 'test.csv'))","89b4c933":"MONTHS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nLINEWIDTH=2\nALPHA=.6\n\ndfp = sales[['date', 'date_block_num','item_cnt_day']].copy()\n\n# Extract the year and the month from the date column into indepedent columns\ndfp['date']  = pd.to_datetime(dfp['date'], format='%d.%m.%Y')\ndfp['year']  = dfp['date'].dt.year\ndfp['month'] = dfp['date'].dt.month\ndfp.drop(['date'], axis=1, inplace=True)\n\n# Sum the number of sold items for each date_block_num (which is the consecutive month number from January 2013 to October 2015)\ndfp = dfp.groupby('date_block_num', as_index=False)\\\n       .agg({'year':'first', 'month':'first', 'item_cnt_day':'sum'})\\\n       .rename(columns={'item_cnt_day':'item_cnt_month'}, inplace=False)\n\nplt.figure(figsize=(16,6))\n# Plot the sales of the year 2013\nplt.plot(MONTHS, dfp[dfp.year==2013].item_cnt_month, '-o', color='steelblue', linewidth=LINEWIDTH, alpha=ALPHA,label='2013')\n\n# Plot the sales of the year 2014\nplt.plot(MONTHS, dfp[dfp.year==2014].item_cnt_month, '-o', color='seagreen', linewidth=LINEWIDTH, alpha=ALPHA,label='2014')\n\n# Plot the sales of the year 2015 until October\nplt.plot(MONTHS[:10], dfp[dfp.year==2015].item_cnt_month, '-o', color='maroon', linewidth=LINEWIDTH, alpha=ALPHA,label='2015')\n\n# Capturing the trend between October and November (For year 2013 and 2014)\ndelta_2013 = dfp.iloc[10].item_cnt_month - dfp.iloc[9].item_cnt_month\ndelta_2014 = dfp.iloc[22].item_cnt_month - dfp.iloc[21].item_cnt_month\navg_delta = (delta_2013 + delta_2014) \/ 2\n# Add the average to the previous month (October 2015)\nnov_2015 = dfp.iloc[33].item_cnt_month + avg_delta\n\n# MONTHS[9:11] equals ['Oct', 'Nov']\nplt.plot(MONTHS[9:11], [dfp.iloc[33].item_cnt_month, nov_2015], '--o', color='gray', linewidth=LINEWIDTH, alpha=ALPHA, label='Prediction', zorder=-1)\n\n# Axes parameters\nax = plt.gca()\nax.set_title('Sales per month')\nax.set_ylabel('# of items')\nax.grid(axis='y', color='gray', alpha=.2)\n    \n# Remove the frame off the chart\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\nplt.legend(loc=2, title='Legend')\nplt.show()\n\ndel dfp","fa3a2697":"# Top N \nN=15\n\ndef get_ratio(year, topn, N):\n    # Get total sold items for each year\n    total = dfp.loc[year].item_cnt_year.sum()\n    ratio = topn\/total*100\n    return \"{0}: the total of the top {1} best selling items is {2} over a total of {3} for that year, which represents {4:.2f}%\".format(year, N, topn, total, ratio)\n\ndfp = sales[['date', 'item_id', 'item_cnt_day']].copy()\ncats = item_categories.copy()\n\n# Extract the year from the date column\ndfp['year'] = pd.to_datetime(dfp['date'], format='%d.%m.%Y').dt.year\ndfp.drop('date', axis=1, inplace=True)\ndfp.item_cnt_day = dfp.item_cnt_day.astype(int)\n\n# Remove returns\ndfp = dfp[dfp.item_cnt_day>0]\n\n# Add the category of each item\ndfp = dfp.merge(items[['item_id','item_category_id']], how='left', on='item_id')\n\n# Number of categories sold each year\ndfp = dfp.groupby(['year', 'item_category_id'])\\\n       .agg({'item_cnt_day':'sum'})\\\n       .rename(columns={'item_cnt_day':'item_cnt_year'}, inplace=False)\n\n# Top N categories sold \ntop = dfp['item_cnt_year'].groupby('year', group_keys=False).nlargest(N)\n# Convert top to a dataframe\ntop = pd.DataFrame(top).reset_index()\n# Add category type to be plotted lated\ntop = top.merge(cats[['item_category_id','item_category_name']], how='left', on='item_category_id')\n\n# To print the top selling categories for each year\n#print(top)\n\nyears = [2013, 2014, 2015]\nfig, axes = plt.subplots(1, 3, figsize=(16,6))\n\n#Prepare colors for the top N\ncolors = [[] for i in range(3)]\nfor alpha in np.arange(N, 0, -1)\/N:\n    colors[0].append((.275, .51, .706, alpha))\n    colors[1].append((.18, .55, .34, alpha))\n    colors[2].append((.5, 0, 0, alpha))\n    \nfor ax, year, cs in zip(axes, years, colors):\n    # Get top items for each year\n    year_filter = top[top.year==year]\n    plot_sizes = year_filter.item_cnt_year\n    plot_labels = year_filter.item_category_name.str[:15]#+'('+plot_sizes.astype(str)+')'\n    \n    # Get the ratio\n    print(get_ratio(year, plot_sizes.sum(), N))\n    \n    # Plot the pie\n    ax.pie(plot_sizes, labels=plot_labels, radius=1.5, colors=cs,labeldistance=.5, rotatelabels=True, startangle=90, wedgeprops={\"edgecolor\":\"1\",'linewidth': .5})\n    # Set titles below pies\n    ax.set_title(year, y=-0.2)\n\n# Space pies\nfig.tight_layout()\nfig.suptitle('Top selling categories for each year', fontsize=16)\nplt.show()\n\ndel dfp","90a5d6dc":"fig, axes = plt.subplots(2, 1)\nplt.subplots_adjust(hspace=0.5)\n\nflierprops = dict(marker='o', markerfacecolor='cornflowerblue', markersize=6, markeredgecolor='navy')\n\n_ = axes[0].boxplot(x=sales.item_cnt_day, flierprops=flierprops, vert=False)\n_ = axes[1].boxplot(x=sales.item_price, flierprops=flierprops, vert=False)\n\n_ = axes[0].set_title('item_cnt_day')\n_ = axes[1].set_title('item_price')","eaca8218":"sales = sales[(sales.item_price<100000)&(sales.item_price>0)]\nsales = sales[(sales.item_cnt_day>0)&(sales.item_cnt_day<1000)]\n\n# Remove duplicate shops\nsales.loc[sales.shop_id==0, 'shop_id'] = 57\ntest.loc[test.shop_id==0, 'shop_id'] = 57\n\nsales.loc[sales.shop_id==1, 'shop_id'] = 58\ntest.loc[test.shop_id==1, 'shop_id'] = 58\n\nsales.loc[sales.shop_id==10, 'shop_id'] = 11\ntest.loc[test.shop_id==10, 'shop_id'] = 11","52fac7c4":"# Correct the name of a shop\nshops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"',\"shop_name\"] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\n# The first part of the shop_name is the city e.g.Serguiev Possad\nshops[\"shop_city\"] = shops.shop_name.str.split(' ').map(lambda x: x[0])\n# The second part of the shop_name is the category e.g. shopping center\nshops[\"shop_category\"] = shops.shop_name.str.split(\" \").map(lambda x: x[1])\nshops.loc[shops.shop_city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"shop_city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\" ","7b7b458a":"# Feature encoding\nshops[\"shop_city\"] = LabelEncoder().fit_transform(shops.shop_city)\nshops[\"shop_category\"] = LabelEncoder().fit_transform(shops.shop_category)\nshops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]\nshops.head()","965d25a1":"item_categories[\"category_type\"] = item_categories.item_category_name.apply(lambda x: x.split(\" \")[0]).astype(str)\n# The category_type \"Gamming\" and \"accesoires\" becomes \"Games\"\nitem_categories.loc[(item_categories.category_type==\"\u0418\u0433\u0440\u043e\u0432\u044b\u0435\")|(item_categories.category_type==\"\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b\"), \"category_type\"] = \"\u0418\u0433\u0440\u044b\"\nitem_categories[\"split\"] = item_categories.item_category_name.apply(lambda x: x.split(\"-\"))\nitem_categories[\"category_subtype\"] = item_categories.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())","b039704c":"item_categories[\"category_type\"] = LabelEncoder().fit_transform(item_categories.category_type)\nitem_categories[\"category_subtype\"] = LabelEncoder().fit_transform(item_categories.category_subtype)\nitem_categories = item_categories[[\"item_category_id\", \"category_type\", \"category_subtype\"]]\nitem_categories.head()","af789b47":"sales = sales.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)\\\n          .agg({'item_cnt_day':'sum'})\\\n          .rename(columns={'item_cnt_day':'item_cnt_month'}, inplace=False)\n        \ntest['date_block_num'] = 34\ntest['item_cnt_month'] = 0\ndel test['ID']\n\ndf = sales.append(test)\ndf","cf4b7542":"matrix = []\n# Try creating a matrix of product(sales['date_block_num'].unique(), sales.shop_id.unique(), sales.item_id.unique()) which are about 45m lines\nfor num in df['date_block_num'].unique(): \n    tmp = df[df.date_block_num==num]\n    matrix.append(np.array(list(product([num], tmp.shop_id.unique(), tmp.item_id.unique())), dtype='int16'))\n    #matrix.append(np.array(list(product([num], shops.shop_id, items.item_id)), dtype='int16'))\n\n# Turn the grid into a dataframe\nmatrix = pd.DataFrame(np.vstack(matrix), columns=['date_block_num', 'shop_id', 'item_id'], dtype=np.int16)\n\n# Add the features from sales data to the matrix\nmatrix = matrix.merge(df, how='left', on=['date_block_num', 'shop_id', 'item_id']).fillna(0)\n\n#Merge features from shops, items and item_categories:\nmatrix = matrix.merge(shops, how='left', on='shop_id')\nmatrix = matrix.merge(items[['item_id','item_category_id']], how='left', on='item_id')\nmatrix = matrix.merge(item_categories, how='left', on='item_category_id')\n\n# Add month\nmatrix['month'] = matrix.date_block_num%12\n# Clip counts\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0, 20)","15226a11":"# Set columns types to control the matrix' size\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix['month'] = matrix['month'].astype(np.int8)\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].astype(np.int32)\nmatrix['shop_category'] = matrix['shop_category'].astype(np.int8)\nmatrix['shop_city'] = matrix['shop_city'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['category_type'] = matrix['category_type'].astype(np.int8)\nmatrix['category_subtype'] = matrix['category_subtype'].astype(np.int8)\nmatrix","e28aeeff":"print('{0:.2f}'.format(matrix.memory_usage(index=False, deep=True).sum()\/(2**20)), 'MB')","e31d9a24":"def lag_feature(df, lags, col):\n    print(col)\n    for i in lags:\n        shifted = df[[\"date_block_num\", \"shop_id\", \"item_id\", col]].copy()\n        shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col+\"_lag_\"+str(i)]\n        shifted.date_block_num += i\n        df = df.merge(shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(0)\n    return df","2d0e1c44":"# lag the target item_cnt_month\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'item_cnt_month')","4ddccd1b":"# shop\/date_block_num aggregates lags\ngb = matrix.groupby(['shop_id', 'date_block_num'],as_index=False)\\\n          .agg({'item_cnt_month':'sum'})\\\n          .rename(columns={'item_cnt_month':'cnt_block_shop'}, inplace=False)\nmatrix = matrix.merge(gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'cnt_block_shop')\nmatrix.drop('cnt_block_shop', axis=1, inplace=True)","fd0f311f":"# item\/date_block_num aggregates lags\ngb = matrix.groupby(['item_id', 'date_block_num'],as_index=False)\\\n          .agg({'item_cnt_month':'sum'})\\\n          .rename(columns={'item_cnt_month':'cnt_block_item'}, inplace=False)\nmatrix = matrix.merge(gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'cnt_block_item')\nmatrix.drop('cnt_block_item', axis=1, inplace=True)","ca5e0aa0":"# category\/date_block_num aggregates lags\ngb = matrix.groupby(['category_type', 'date_block_num'],as_index=False)\\\n          .agg({'item_cnt_month':'sum'})\\\n          .rename(columns={'item_cnt_month':'cnt_block_category'}, inplace=False)\nmatrix = matrix.merge(gb, how='left', on=['category_type', 'date_block_num']).fillna(0)\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'cnt_block_category')\nmatrix.drop('cnt_block_category', axis=1, inplace=True)","a972408b":"# matrix.to_csv('matrix.csv', index=False)\n# matrix = pd.read_csv('matrix.csv')\nmatrix","f032d64f":"from sklearn.preprocessing import StandardScaler\n\ndef standard_mean_enc(df, col):\n    mean_enc = df.groupby(col).agg({'item_cnt_month': 'mean'})\n    scaler = StandardScaler().fit(mean_enc)\n    return {v: k[0] for v, k in enumerate(scaler.transform(mean_enc))}","8b2711a6":"cols_to_mean_encode = ['shop_category', 'shop_city', 'item_category_id', 'category_type', 'category_subtype']\n\nfor col in cols_to_mean_encode:\n    # Train on the train data\n    mean_enc = standard_mean_enc(matrix[matrix.date_block_num < 33].copy(), col) # X_train, y_train\n    # Apply to Train, Validation and Test\n    matrix[col] = matrix[col].map(mean_enc)\nmatrix","bba280a4":"# Remove the 2013's sales data\nmatrix = matrix[matrix.date_block_num>=12] \nmatrix.reset_index(drop=True, inplace=True)\nmatrix","07ecd9af":"X_train = matrix[matrix.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = matrix[matrix.date_block_num < 33]['item_cnt_month']\nX_val = matrix[matrix.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val =  matrix[matrix.date_block_num == 33]['item_cnt_month']\nX_test = matrix[matrix.date_block_num == 34].drop(['item_cnt_month'], axis=1)","2bf4ec58":"X_train.drop('date_block_num', axis=1, inplace=True)\nX_val.drop('date_block_num', axis=1, inplace=True)\nX_test.drop('date_block_num', axis=1, inplace=True)","7e44455a":"splits = []\nfor block in [27, 28, 29, 30, 31, 32]:\n    train_idxs = matrix[matrix.date_block_num < block].index.values\n    test_idxs = matrix[matrix.date_block_num == block].index.values\n    splits.append((train_idxs, test_idxs))\nsplits","9ea574e0":"from sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\n\nhyper_params = {'max_depth': [3, 4, 5, 6, 7, 8, 9], \n                'gamma': [0, 0.5, 1, 1.5, 2, 5], \n                'subsample': [0.6, 0.7, 0.8, 0.9, 1], \n                'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1], \n                'learning_rate': [0.01, 0.1, 0.2, 0.3],\n                'max_bin' : [256, 512, 1024]\n               }\n\nxgbr = XGBRegressor(seed = 13, tree_method = \"hist\") #gpu_hist\nclf = RandomizedSearchCV(estimator = xgbr, \n                   param_distributions = hyper_params,\n                   n_iter = 2, #500\n                   scoring = 'neg_root_mean_squared_error',\n                   cv = splits,\n                   verbose=3)\nclf.fit(X_train, y_train)\n\nprint(\"Best parameters:\", clf.best_params_)\nprint(\"Lowest RMSE: \", -clf.best_score_)","06be92f0":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nyhat_val_lr = lr.predict(X_val).clip(0, 20)\nprint('Validation RMSE:', mean_squared_error(y_val, yhat_val_lr, squared=False)) #Validation RMSE: 0.9645168655662343\nyhat_test_lr = lr.predict(X_test).clip(0, 20)","cb7e661f":"from xgboost import XGBRegressor\n\nts = time.time()\n\nxgb = XGBRegressor(seed = 13, \n    tree_method = \"hist\", #gpu_hist\n    subsample = 0.9,\n    max_depth = 9,\n    learning_rate = 0.1,\n    gamma = 2,\n    colsample_bytree = 0.9\n    )\nxgb.fit(\n    X_train,y_train,\n    eval_metric=\"rmse\",\n    eval_set=[(X_train, y_train), (X_val, y_val)],\n    verbose=True,\n    early_stopping_rounds = 10\n    )\nprint('Training took: {0}s'.format(time.time()-ts))\nyhat_val_xgb = xgb.predict(X_val).clip(0, 20)\nprint('Valdation RMSE:', mean_squared_error(y_val, yhat_val_xgb, squared=False)) #Valdation RMSE: 0.9273184120626018\nyhat_test_xgb = xgb.predict(X_test).clip(0, 20)","efcddedd":"import pickle\npickle.dump(xgb, open(\"xgboost.pickle.dat\", \"wb\"))\n#loaded_model = pickle.load(open(\"xgboost_base.pickle.dat\", \"rb\"))","b9ab4435":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(xgb, (10,14))","3ecfb287":"y_train_meta = matrix[matrix.date_block_num.isin([27, 28, 29, 30, 31, 32])].item_cnt_month","abcf6ad0":"X_train_meta = [[],[]]\nfor block in [27, 28, 29, 30, 31, 32]:\n    print('Block:', block)\n    # X and y Train for blocks from 12 to block\n    X_train_block = matrix[matrix.date_block_num < block].drop(['date_block_num', 'item_cnt_month'], axis=1)\n    y_train_block = matrix[matrix.date_block_num < block].item_cnt_month\n    # X and y Test for block\n    X_val_block = matrix[matrix.date_block_num == block].drop(['date_block_num', 'item_cnt_month'], axis=1)\n    #y_test_block = matrix[matrix.date_block_num == block].item_cnt_month\n    \n    # Fit first model \n    print(' LR fitting ...')\n    lr.fit(X_train_block, y_train_block)\n    print(' LR fitting ... done')\n    # Append prediction results on X_val_block to X_train_meta (first column)\n    X_train_meta[0] += list(lr.predict(X_val_block).clip(0, 20))\n    \n    # Fit second model\n    print(' XGB fitting ...')\n    xgb.fit(\n        X_train_block, y_train_block,\n        eval_metric=\"rmse\",\n        eval_set=[(X_train_block, y_train_block)],\n        #eval_set=[(X_train_block, y_train_block), (X_val_block, y_test_block)],\n        verbose=0,\n        early_stopping_rounds = 10\n    )\n    print(' XGB fitting ... done')\n    # Append prediction results on X_val_block to X_train_meta (second column)\n    X_train_meta[1] += list(xgb.predict(X_val_block).clip(0, 20))\n# Turn list into dataframe\nX_train_meta = pd.DataFrame({'yhat_lr': X_train_meta[0], 'yhat_xgb': X_train_meta[1]})","de9fa12e":"plt.scatter(X_train_meta.yhat_lr, X_train_meta.yhat_xgb)","5a2a16e1":"stacking = LinearRegression()\nstacking.fit(X_train_meta, y_train_meta)\n\n#Squared: If True returns MSE value, if False returns RMSE value.\nyhat_train_meta = stacking.predict(X_train_meta).clip(0, 20)\nprint('Meta Training RMSE:', mean_squared_error(y_train_meta, yhat_train_meta, squared=False))\n# Meta Training RMSE: 0.813971713370181\n\nyhat_val_meta = stacking.predict(np.vstack((yhat_val_lr, yhat_val_xgb)).T).clip(0, 20)\nprint('Meta Validation RMSE:', mean_squared_error(y_val, yhat_val_meta, squared=False))\n# Meta Validation RMSE: 0.9184725317670576\n\nyhat_test_meta = stacking.predict(np.vstack((yhat_test_lr, yhat_test_xgb)).T).clip(0, 20)","361d154c":"submission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": yhat_test_meta\n})\nsubmission.to_csv('submission_stacking.csv', index=False)\n# Public score 0.92466","4509db2b":"* Add 'city' and 'category' to shops:\\ -The first part of the shop_name is the city e.g.Serguiev Possad \\ -The second part of the shop_name is the category e.g. \u0422\u0426 (shopping center)","c0839f55":"# Loading Data","48235736":"# Importing Modules","7895ccc0":"* Compute monthly sales, in the same representation as the test data:","3ce530ad":"### Removing Few Columns","d3ca51ee":"* Add type and subtype to item_categories:","1a6218e4":"# Hyper Parameters Tuning","8b8b58db":"# Model Evaluation\n\n## Linear Regression","8f57cd19":"# Exploratory Data Analysis\n* Plot the number of items sold from Jan to Dec for the years 2013, 2014 and 2015:","62b77d8c":"# Stacking","a847c8f5":"## Create a feature matrix:","fb28c1ff":"# Feature Engineering\n### Lagged Features","30a6e58b":"# Importing Pickel","584149df":"# Submission","32c01299":"* Best Selling Categories for each year","6e104f2a":"# Splitting Data","b14d6340":"* Detecting outliers in item_price and item_cnt_day","3a70a90d":"# Ploting XGBoost Feaure Importance","d1c99560":"* Serialize and Deserialize the XGBoost model with Pickle:","d4533460":"# Data preparation\n* Removing outliers from item_price and item_cnt_day, and duplicate shops:","9d2ba3fa":"# Esembling\n## Meta Features","6c184170":"# Feature Encoding","ddaa9706":"# Label mean encodings\n* Mean encoding and scaling : first split the data into Train and Validation, estimate encodings on Train, then apply them to Validation set:","002b97e6":"# XGBoost","4b53e76d":"# Feature Engineering"}}