{"cell_type":{"36726e0e":"code","ce154e53":"code","21fdcf72":"code","0b46dae7":"code","af796f8a":"code","e4e937a9":"code","723be70d":"code","4c70c9ac":"code","3b0ab35e":"code","56a2e4dc":"code","95701533":"code","c70c438a":"code","cf2c031c":"code","db3d1353":"code","a9dd582d":"code","806356fa":"code","3b405e15":"markdown","04aa8f23":"markdown","d4f384f3":"markdown","a5fad8a1":"markdown","10299369":"markdown","1af1b3b5":"markdown","a50df569":"markdown","4449355c":"markdown","a260f9e1":"markdown"},"source":{"36726e0e":"# Imports\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n","ce154e53":"imgs_path = \"..\/input\/dont-stop-until-you-drop\/images\/train_images\/\" # this path contains images,\n\ntest_path = \"..\/input\/dont-stop-until-you-drop\/images\/test_images\/\" #contains paths of images for testing\n\npaths_labels = pd.read_csv(\"..\/input\/dont-stop-until-you-drop\/train.csv\") #contain the train image paths and classes\n\nimgs_data_list = np.asarray(os.listdir(imgs_path)) #converting it to an array to be able to access the paths\n\ntest_data_list = np.asarray(os.listdir(test_path)) # the test images as list, we should use this to predict classes and submit our result\n\nprint('----csv file shape----')\nprint(paths_labels.shape)\nprint('----csv file head----')\nprint(paths_labels.head())\n\nprint('----images len----')\nprint(len(imgs_data_list))\n\nprint('----images sample----')\nprint(imgs_data_list[0:5])\n\nprint('----test len----')\nprint(len(test_data_list))\n\nprint('----test sample----')\nprint(test_data_list[0:5])\n\n\nprint('-------------------------------')\n\nimport cv2\n\nclass_names = list(paths_labels['class_6'].unique())\n\nfor i in class_names:\n    pose = cv2.imread(imgs_path + paths_labels.iloc[np.where(paths_labels[\"class_6\"]==i)[0][0]][0], cv2.IMREAD_COLOR )\n    print('------ pose: ' + str(i) + '----------')\n    plt.imshow(pose)\n    plt.show()\n    ","21fdcf72":"from sklearn.model_selection import train_test_split\n\ndataset_train, dataset_valid = train_test_split(\n    paths_labels,\n    test_size=0.25, random_state=5\n)\n\nprint(dataset_train.shape)\nprint(dataset_valid.shape)","0b46dae7":"import shutil\n\n# creating the empty folders for each category in temporary memory\nfor i in class_names:\n    os.makedirs(os.path.join('..\/kaggle\/temp\/train_', str(i)))\n    os.makedirs(os.path.join('..\/kaggle\/temp\/valid_', str(i)))\n\n# copying each image in their respective temporary folders for train data\nfor c in class_names:\n    for i in list(dataset_train[dataset_train['class_6']==c].image_id):\n        get_image = os.path.join(imgs_path, i) # Path to Images\n        move_image_to_cat = shutil.copy(get_image, '..\/kaggle\/temp\/train_\/'+str(c))\n\n# copying each image in their respective temporary folders for train data\nfor c in class_names:\n    for i in list(dataset_valid[dataset_valid['class_6']==c].image_id):\n        get_image = os.path.join(imgs_path, i) # Path to Images\n        move_image_to_cat = shutil.copy(get_image, '..\/kaggle\/temp\/valid_\/'+str(c))","af796f8a":"from tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '..\/kaggle\/temp\/train_\/',\n    labels='inferred',\n    label_mode='categorical',\n    image_size=[299, 299],#this value should be compatible with your model, here inceptionv3\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\n\nds_valid_ = image_dataset_from_directory(\n    '..\/kaggle\/temp\/valid_\/',\n    labels='inferred',\n    label_mode='categorical',\n    image_size=[299, 299],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n\n# Data Pipeline for converting images to tensorflow float32 formar\n\n# function to do the reformatting\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\n# automatically tune by tensorflow\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\n\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)","e4e937a9":"pretrained_base = tf.keras.applications.inception_v3.InceptionV3(\n    include_top=True, weights='imagenet', input_tensor=None,\n    input_shape=None, pooling=None, classes=1000,\n    classifier_activation='softmax'\n)\n\npretrained_base.trainable = False","723be70d":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel1 = keras.Sequential([\n    \n    pretrained_base,\n    layers.Flatten(),\n\n    layers.Dense(6, activation='sigmoid'), # last layer we have to set 6 for our out puts, because of having 6 classes\n])\n\nmodel1.summary()","4c70c9ac":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor = 'val_categorical_accuracy',\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=5, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","3b0ab35e":"model1.compile(\n    optimizer='adam',\n    loss='CategoricalCrossentropy',\n    metrics=['categorical_accuracy'],\n)\n\nhistory1 = model1.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=30,\n    callbacks=[early_stopping], # put your callbacks in a list\n    verbose=1, # use verbos 1 if you want to see the progress bar, if not set it to 0\n)","56a2e4dc":"#import pandas as pd\n\nhistory_frame1 = pd.DataFrame(history1.history)\nhistory_frame1.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame1.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot();\n\nprint('---- printing the best validation scores ------------')\n\nprint(\"Minimum validation loss: {}\".format(history_frame1['val_loss'].min()))\n\nprint(\"Minimum validation accuracy: {}\".format(history_frame1['val_categorical_accuracy'].max()))","95701533":"#from tensorflow import keras\n#from tensorflow.keras import layers\n\nmodel2 = keras.Sequential([\n\n    # First Convolutional Block\n    layers.Conv2D(filters=32, kernel_size=7, activation=\"relu\", padding='same',\n                  # give the input dimensions in the first layer\n                  # [height, width, color channels(RGB)]\n                  input_shape=[299, 299, 3]),\n    layers.MaxPool2D(),\n\n    # Second Convolutional Block\n    layers.Conv2D(filters=64, kernel_size=5, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n\n    # Third Convolutional Block\n    layers.Conv2D(filters=128, kernel_size=5, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    \n    # 4th Convolutional Block\n    layers.Conv2D(filters=256, kernel_size=5, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    \n    # 5th Convolutional Block\n    layers.Conv2D(filters=514, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    \n    # 6th Convolutional Block\n    layers.Conv2D(filters=720, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    \n    # 7th Convolutional Block\n    layers.Conv2D(filters=1080, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n\n    # Classifier Head\n    layers.Flatten(),\n    #layers.Dense(units=6, activation=\"relu\"),\n    layers.Dense(units=6, activation=\"sigmoid\"),\n])\nmodel2.summary()","c70c438a":"model2.compile(\n    optimizer='adam',\n    loss='CategoricalCrossentropy',\n    metrics=['categorical_accuracy'],\n)\n\nhistory2 = model2.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=30,\n    callbacks=[early_stopping], # put your callbacks in a list\n    verbose=1, # use verbos 1 if you want to see the progress bar, if not set it to 0\n)\n\n#optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n#import pandas as pd\n\nhistory_frame2 = pd.DataFrame(history2.history)\nhistory_frame2.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame2.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot();\n\nprint('---- printing the best validation scores ------------')\n\nprint(\"Minimum validation loss: {}\".format(history_frame2['val_loss'].min()))\n\nprint(\"Minimum validation accuracy: {}\".format(history_frame2['val_categorical_accuracy'].max()))","cf2c031c":"RunNum = 15\n\nloss1 = []\naccuracy1 = []\nloss2 = []\naccuracy2 = []\n\nfor i in range(RunNum):\n    \n    history1 = model1.fit(\n        ds_train,\n        validation_data=ds_valid,\n        epochs=30,\n        callbacks=[early_stopping], # put your callbacks in a list\n        verbose=0, # use verbos 1 if you want to see the progress bar, if not set it to 0\n    )\n\n    history2 = model2.fit(\n        ds_train,\n        validation_data=ds_valid,\n        epochs=30,\n        callbacks=[early_stopping], # put your callbacks in a list\n        verbose=0, # use verbos 1 if you want to see the progress bar, if not set it to 0\n    )\n\n    loss1.append(np.min(history1.history['val_loss']))\n    accuracy1.append(np.max(history1.history['val_categorical_accuracy']))\n    \n    loss2.append(np.min(history2.history['val_loss']))\n    accuracy2.append(np.max(history2.history['val_categorical_accuracy']))\n    \n    print('---- run ' + str(i) + '-----')\n    print(\"Max accuracy model1: {}\".format(np.max(history1.history['val_categorical_accuracy'])))\n    print(\"Max accuracy model2: {}\".format(np.max(history2.history['val_categorical_accuracy'])))\n    \nmodel1_score = pd.DataFrame({'loss': loss1, 'accuracy':accuracy1})\nmodel2_score = pd.DataFrame({'loss': loss2, 'accuracy':accuracy2}) \n\nprint(model1_score.head())","db3d1353":"# boxplot\n\nplt.figure(figsize=(20, 3))\nplt.subplot(131)\nplt.boxplot(model1_score.accuracy)\nplt.axis('on')\nplt.title('Model1 - accuracy')\nplt.subplot(132)\nplt.boxplot(model2_score.accuracy)\nplt.axis('on')\nplt.title('Model2 - accuracy')\nplt.show();","a9dd582d":"# we also need to conver our test images to float numbers, however, when reading the test images\n# since there is no defined category for them, we have to set labels and label_mode to None\n# and then reading our file, resizing it and converting it to float32\n\nds_test_ = image_dataset_from_directory(\n    '..\/input\/dont-stop-until-you-drop\/images\/test_images',\n    labels=None,\n    label_mode=None,\n    image_size=[299, 299],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\ndef convert_test_to_float(image):\n    # we don't have labes here so a new function is defined\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n\nds_test = (\n    ds_test_\n    .map(convert_test_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\n\n# now we can predict test images\n\npredictions = model1.predict(ds_test)\n\n# here our prediction will have 6 columns for each image representing the predicted probability to be belong\n# to each class. So we assume the maximum value would be our best choice as the class prediction for that image\n\n#argmax will return the index of maximum value in a column (when setting axis = 1) \n# and that index is equal to our category number 0,1,2,3,4,5\n\nscore = np.argmax(predictions,axis=1) \n\nresults = pd.DataFrame({'image_id': test_data_list,'class_6':score})\n\n#results.set_index(\"image_id\", inplace=True)\n\nresults.head()","806356fa":"# and saving it in the csv file format\nresults.to_csv('outputfile.csv', index=False)","3b405e15":"# Step 1: Load and prepare data\nThe detailed explanation of this step can be found in [basics for beginner](https:\/\/www.kaggle.com\/arminkeshavarzi\/basics-for-beginners-shanghai-competiton), we skipped the explanation to have just the codes here.","04aa8f23":"# Step 5: predict and generate submission file\n\nSince our models are still basic, we don't need to submite our data. However, if you want to see the process of submission, the steps are as follow.","d4f384f3":"In above, the algorithm will be stopped if we find there is a overfitting isseu, or there is no improvment in validation categorical accruacy.","a5fad8a1":"# Step 2: Define a pre-trained model as a base model\nBased on [computer vision course - The Convolutional Classifier](https:\/\/www.kaggle.com\/ryanholbrook\/the-convolutional-classifier) section, a pretrained model for image classification can help us to reach a good score with an efficient time. There construced based on many tuned layers based on enamorous images. For more information on using these models please refer to [Transfer learning and fine-tuning](https:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning).\n\nKeep in mind that improvment and trying to find a better model based on our problem, will be the topic of our future notebooks.\n\nHere we want to use [inceptionV3](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/inception_v3\/InceptionV3)\n\ntf.keras.applications.inception_v3.InceptionV3(\n    include_top=True, weights='imagenet', input_tensor=None,\n    input_shape=None, pooling=None, classes=1000,\n    classifier_activation='softmax'\n)\n\n**include_top**: Boolean, whether to include the fully-connected layer at the top, as the last layer of the network. Default to True.\n\n**weights**: One of -> None (random initialization), imagenet (pre-training on ImageNet), or the path to the weights file to be loaded. Default to imagenet.\n\n**input_tensor**: Optional Keras tensor (i.e. output of layers.Input()) to use as image input for the model. input_tensor is useful for sharing inputs between multiple different networks. Default to None.\n\n**input_shape**: Optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (299, 299, 3) (with channels_last data format) or (3, 299, 299) (with channels_first data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 75. E.g. (150, 150, 3) would be one valid value. input_shape will be ignored if the input_tensor is provided.\n\n**pooling**: Optional pooling mode for feature extraction when include_top is False.\nNone (default) means that the output of the model will be the 4D tensor output of the last convolutional block.\navg means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. max means that global max pooling will be applied.\n\n**classes**: Optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Default to 1000.\n\n**classifier_activation**: A str or callable. The activation function to use on the \"top\" layer. Ignored unless include_top=True. Set classifier_activation=None to return the logits of the \"top\" layer. When loading pretrained weights, classifier_activation can only be None or \"softmax\".\n","10299369":"We can see the pre-trained model has better accuracies. However, we still need to improve our models and also tune their parameter, then we can compare model in more accurate way. In the next notebook we frst add agumentation technique to improve our model.","1af1b3b5":"When fitting our data, we can see at some point in charts the train scores starts to sparate from the validations scores (look at the charts in the [basics for beginner](https:\/\/www.kaggle.com\/arminkeshavarzi\/basics-for-beginners-shanghai-competiton) notebook). This indicates that, after such posint we have the overfitting issue (if this is not case, we can assume we have underfitting and we need to increase the epocs to avoid that).\n\nHere we use a concept called early stopping, to avoid the overfitting. When seeing the separation in train and valid scores, the early stopping will stop our algorithm and will return the best model.\n\nHere we create simple early stopping by using callback function in keras based on the [Overfitting and Underfitting](https:\/\/www.kaggle.com\/ryanholbrook\/overfitting-and-underfitting) section in [Intro to Deep Learning](https:\/\/www.kaggle.com\/learn\/intro-to-deep-learning) course, and also you can explore Keras variety of useful [callbacks](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks).\n\n[**tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=0, verbose=0,\n    mode='auto', baseline=None, restore_best_weights=False\n)**](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping)\n\nAssuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates.\n\n**monitor**: Quantity to be monitored.\n\n**min_delta**: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n\n**patience**: Number of epochs with no improvement after which training will be stopped.\n\n**verbose**: verbosity mode.\n\n**mode**: One of {\"auto\", \"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing; in \"max\" mode it will stop when the quantity monitored has stopped increasing; in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity.\n\n**baseline**: Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline.\n\n**restore_best_weights**: Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used. An epoch will be restored regardless of the performance relative to the baseline. If no epoch improves on baseline, training will run for patience epochs and restore weights from the best epoch in that set.","a50df569":"# Introduction\n\nIn the [basics for beginner](https:\/\/www.kaggle.com\/arminkeshavarzi\/basics-for-beginners-shanghai-competiton\/notebook?scriptVersionId=80495474) notebook we mostley tried to prepare our data to make them ready to use in an image classification model for [Yoga images recognition competiton](https:\/\/www.kaggle.com\/c\/dont-stop-until-you-drop\/overview). Howver, the introduced model was one of the very basics one and also was a pre trained mod. Here we want to go little deep, and introduce our self basic constructed model and make a comparsion between a pre-trained model and one that we developed. These models are also basics one, and we will go deeper in future notebooks.\n\nIn this notebook we will use concepts based on [Intro to Deep Learning](https:\/\/www.kaggle.com\/learn\/intro-to-deep-learning) by [Ryan Holbrook](https:\/\/www.kaggle.com\/ryanholbrook). and [computer vision](https:\/\/www.kaggle.com\/learn\/computer-vision) courses.\n","4449355c":"# Step 3: Define a self constructed model\nNow we want to construct a new model that will make predictions based on our self developed layers. This model will be much simpler than the pre-trained model, but we will have more options to tune the parameters or make changes.\n\nFor image detection we use convolutional neural networks (CNN), which have a good performance in these kind of problems.\nIn tenserflow. keras, we can construct each layer of CNN by Conv2D. So lets see the [tenserflow instruction](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Conv2D) here:\n\nFor a comprehensive introduction about this layer please refer to [computer vision](https:\/\/www.kaggle.com\/learn\/computer-vision) course in kaggle.\n\n**tf.keras.layers.Conv2D(\n    filters, kernel_size, strides=(1, 1), padding='valid',\n    data_format=None, dilation_rate=(1, 1), groups=1, activation=None,\n    use_bias=True, kernel_initializer='glorot_uniform',\n    bias_initializer='zeros', kernel_regularizer=None,\n    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n    bias_constraint=None)**\n    \nThis layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.\n\nWhen using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\". You can use None when a dimension has variable size.\n\nNote: For filter, kernel and activation definetion, please refer to [Convolution and ReLU section\n](https:\/\/www.kaggle.com\/ryanholbrook\/convolution-and-relu)\n\n**filters**: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). \n\n**kernel_size**: An integer or tuple\/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n\n**strides**: An integer or tuple\/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n\n**padding**: One of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding with zeros evenly to the left\/right or up\/down of the input. When padding=\"same\" and strides=1, the output has the same size as the input.\n\n**data_format**: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height, width, channels) while channels_first corresponds to inputs with shape (batch_size, channels,height, width). It defaults to the image_data_format value found in your Keras config file at ~\/.keras\/keras.json. If you never set it, then it will be channels_last.\n\n**dilation_rate**: An integer or tuple\/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1.\n\n**groups**: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters \/ groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups.\n\n**activation**: Activation function to use. If you don't specify anything, no activation is applied (see [keras.activations](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/activations)).\n\n**use_bias**: Boolean, whether the layer uses a bias vector.\n\n**kernel_initializer**: Initializer for the kernel weights matrix (see [keras.initializers](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/initializers)). Defaults to 'glorot_uniform'.\n\n**bias_initializer**: Initializer for the bias vector (see [keras.initializers](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/initializers)). Defaults to 'zeros'.\n\n**kernel_regularizer**: Regularizer function applied to the kernel weights matrix (see [keras.regularizers](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/regularizers)).\n\n**bias_regularizer**: Regularizer function applied to the bias vector (see [keras.regularizers](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/regularizers)).\n\n**activity_regularizer**: Regularizer function applied to the output of the layer (its \"activation\") (see [keras.regularizers](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/regularizers)).\n\n**kernel_constraint**:Constraint function applied to the kernel matrix (see [keras.constraints](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/constraints)).\n\n**bias_constraint**: Constraint function applied to the bias vector (see [keras.constraints](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/constraints)).\n\n\nAfter Conv2D layer, we want to intensify the extracted features in order to have a better detection, we can do this with another layer called [Maximum Pooling](https:\/\/www.kaggle.com\/ryanholbrook\/maximum-pooling).\n\nIn tenserflow, we can add this layer by [MaxPool2D](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/MaxPool2D):\n\n**tf.keras.layers.MaxPool2D(\n    pool_size=(2, 2), strides=None, padding='valid', data_format=None)**\n    \n\npool_size: integer or tuple of 2 integers, window size over which to take the maximum. (2, 2) will take the max value over a 2x2 pooling window. If only one integer is specified, the same window length will be used for both dimensions.\n\nstrides:Integer, tuple of 2 integers, or None. Strides values. Specifies how far the pooling window moves for each pooling step. If None, it will default to pool_size.\n\npadding: One of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding evenly to the left\/right or up\/down of the input such that output has the same height\/width dimension as the input.\n\ndata_format: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width). It defaults to the image_data_format value found in your Keras config file at ~\/.keras\/keras.json. If you never set it, then it will be \"channels_last\".\n\nNow we are ready to define our sequence. Here we define this sequence based on the [computer vision course section Custom Convnets](https:\/\/www.kaggle.com\/ryanholbrook\/custom-convnets). Also for having a little better model, we repeat these layser multiple times. However, we may change this in order to improve it in future notebooks.\n","a260f9e1":"# Step 4: Comparing two models\n\nNow we run both models multiple times and store each final scores for both the train and validation data. Then we plot each model box-plot (for their scores). We can use the statistical hypothesis to check if there is a meaningful diffrence in mean or variance. However, we can deduce this just by looking at the charts and avoid to implement the computations.\n\nIf some part of the one chart covers another one, then we can conclude there is no meaningful diffreneces in mean. Also, smaller range of chart indicate smaller variance and the smaller variance is better."}}