{"cell_type":{"123942b1":"code","e20bb61a":"code","88d9cabb":"code","6e85cd2e":"code","45093a81":"code","212c4a55":"code","86e832a3":"code","679bc2b6":"code","4d85e607":"code","5c461f04":"code","81626f25":"code","d6560100":"code","2340cccf":"code","d5fc46a9":"code","95c0034f":"code","31cb673b":"code","41e14152":"code","8ee1090e":"code","c0779e78":"code","050503a0":"code","4b89c123":"code","4ece26f4":"code","965ec7de":"code","d25a645d":"code","5f1792fd":"code","a46133c2":"code","5a99b132":"code","597e8140":"code","34a3ccc4":"code","c15cf524":"code","8e42b542":"code","5edb8689":"code","17ead44c":"code","2f3efb36":"code","22d3d9c6":"code","0c15ae4e":"code","89bb4be1":"code","850ce2d1":"code","1617112e":"code","24c1348a":"code","266e15c1":"code","55a201c0":"code","e684228e":"code","e552cf14":"code","958eec19":"code","b8dcefc8":"code","e99ed655":"code","dddd7438":"code","084c7ee9":"code","b7ec0207":"code","0cbdf855":"code","13559b5d":"code","10ab815b":"code","871426df":"code","2d95080b":"code","8aa9c4d9":"code","e43245f6":"code","64205c09":"code","71a2ed7c":"code","fe18ffb4":"code","6d24ff83":"code","f5211c3b":"code","65a29fde":"code","0124a9df":"code","dd3bf1af":"code","6700cb74":"code","3b123927":"code","999ec5a3":"code","9c57f4b9":"code","eb3abb0c":"code","111fee0c":"code","cf5009e5":"code","b83cca5e":"code","a0d2136d":"code","76f22f84":"code","67be1618":"code","1e5a5648":"code","ca6a2ebe":"code","5b951904":"code","2f04e9dd":"code","86f84bfb":"code","fbafd42e":"code","9e7b8f2d":"code","befc7c34":"code","ce6c68ec":"code","412c20a7":"code","5d5bc9ed":"code","d862de01":"code","77912a56":"code","238f509e":"code","ad468689":"code","ef4a3209":"code","c7b1a82f":"code","d43b3f8d":"code","b200464a":"code","e72a0bbf":"code","719422e9":"code","4f5e7542":"code","824b547f":"code","8611f672":"code","ae7b4c23":"code","cde97295":"code","562a1114":"code","753f6351":"code","7d0819f7":"code","b23c493c":"code","014be52b":"code","43d0e510":"code","23e16c2e":"code","e2d9610d":"code","6972f33d":"code","b0c3ad10":"code","31da6a6f":"code","f453b6ad":"code","fb7f132e":"code","ce9cb410":"code","7a47562a":"code","cf128dc3":"code","364ca029":"code","af0b7a9a":"code","cf8afd93":"code","2b5c9d28":"code","9142a635":"code","07e03e24":"code","68120b53":"code","37a4f0b5":"code","2476b743":"code","02da8e5a":"code","2b38c6f5":"code","7e2b6ad0":"code","da6b1037":"markdown","8bb390cd":"markdown","a1971c28":"markdown","ec9dcf6a":"markdown","42383cd0":"markdown","ce6cabb0":"markdown","ae62f135":"markdown","8718af88":"markdown","3ce17938":"markdown","d9bc9f9b":"markdown","835cec34":"markdown","baf1c04e":"markdown","3bfa88fa":"markdown","f0febddb":"markdown","5de9cb61":"markdown","b08e3480":"markdown","37cdf87d":"markdown","24ed6298":"markdown","d6842f89":"markdown","595f8cbd":"markdown","b30b36ae":"markdown","0eb0862c":"markdown","7fb84d1c":"markdown","d22fe1bc":"markdown","50aaf3f5":"markdown","0940f4dc":"markdown","efbdedda":"markdown","2e823d69":"markdown","05991a8a":"markdown","715ad78a":"markdown","10a5a4ed":"markdown","735d0499":"markdown","875afebf":"markdown","c2911b56":"markdown","3b2da940":"markdown","01784a44":"markdown","c0bbf150":"markdown","062efde9":"markdown","b7e52c8b":"markdown","f034931d":"markdown","dd6728fd":"markdown","3ece90bf":"markdown","418530ee":"markdown","79eef4f1":"markdown","432f0835":"markdown","63cdec46":"markdown","01c42c6f":"markdown","bc1b6d1b":"markdown","b7aa2118":"markdown","3c5611d4":"markdown","34b8873c":"markdown","d35d0b57":"markdown","aa2387fc":"markdown","9cf81e60":"markdown","9efdbe95":"markdown","d320afda":"markdown","51190fc0":"markdown","2e129d91":"markdown","cd8243a7":"markdown","7a9138cc":"markdown","61593d58":"markdown","072692f7":"markdown","5457a4a1":"markdown","e694549c":"markdown","d8e55195":"markdown","5bc03c3c":"markdown","96511a8b":"markdown","3cfd4643":"markdown","6bda06c0":"markdown","a930ad29":"markdown","f8b78abb":"markdown","fef01602":"markdown","83d06a7e":"markdown","780ae923":"markdown","df1f7514":"markdown","3dab42fb":"markdown","04384974":"markdown","0439749d":"markdown","54848462":"markdown","d5e43749":"markdown","d9c5037d":"markdown","094dd979":"markdown","1dbf038f":"markdown","107e7602":"markdown","7c7cdd47":"markdown","389e46e3":"markdown","9ad4fd6e":"markdown","5dfdd674":"markdown","b9a38df0":"markdown","73e3b39b":"markdown","2114fc88":"markdown","cf3b3aed":"markdown","4ab4dad9":"markdown","34046fd0":"markdown","70f30411":"markdown","e95e6343":"markdown","735d5b35":"markdown","7f8e9e17":"markdown","7e8b38b2":"markdown","dfb9c72f":"markdown","6e1390b3":"markdown","e38b12ac":"markdown","f59120b1":"markdown","c715c465":"markdown","cd7a6516":"markdown","bb8378ed":"markdown","bded4697":"markdown","0fa61304":"markdown","7f66cb1b":"markdown","6c7045f4":"markdown","fa0e58c4":"markdown","e09b65f5":"markdown","c577b54b":"markdown","4da7574f":"markdown","db6dcb9d":"markdown","6d71d1b8":"markdown","9647672d":"markdown"},"source":{"123942b1":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport re\nfrom datetime import timedelta\nfrom textblob import TextBlob  # for sentiment analysis\nfrom wordcloud import WordCloud  # for creating cloud of words\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings('ignore')\n\n\npd.set_option('display.max_rows', 50)  # Show more rows\npd.set_option('display.max_columns', 50)  # Show more columns\nplt.style.use('ggplot')  # Nice plotting\n\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e20bb61a":"# fix the version of the packages so that the experiments are reproducible:\n!pip freeze > requirements.txt","88d9cabb":"# Lock a RANDOM SEED to keep experiments reproducible.\nRANDOM_SEED = 42","6e85cd2e":"# colors = ['#001c57', '#50248f', '#a6a6a6', '#38d1ff']\ncolors = ['#50248f', '#38d1ff']\nsns.palplot(sns.color_palette(colors))","45093a81":"DATA_DIR = '\/kaggle\/input\/sf-dst-restaurant-rating\/'\ndf_train = pd.read_csv(DATA_DIR+'\/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'\/sample_submission.csv')","212c4a55":"display(df_train.sample(3))\ndf_train.info()","86e832a3":"display(df_test.sample(3))\ndf_test.info()","679bc2b6":"# For the correct processing of features, combine train and test sets into a one dataset\n\ndf_train['sample'] = 1  # train\ndf_test['sample'] = 0  # test\n# as we have to predict rating, in test set we just fill it with 0\ndf_test['Rating'] = 0\n\ndf = df_test.append(df_train, sort=False).reset_index(\n    drop=True)  # combine sets","4d85e607":"display(df.sample(3))\ndf.info()","5c461f04":"dtype_df = df.dtypes.reset_index()\ndtype_df.columns = ['Count', 'Column Type']\ndtype_df.groupby('Column Type').agg('count').reset_index()","81626f25":"for i, j in enumerate(df.columns):\n    print(j, type(df.loc[1][i]))","d6560100":"obj = df.dtypes[df.dtypes == 'object'].index\nprint(obj)","2340cccf":"for i in obj:\n    print(f'Col Name: {i}, Content: {df[i].unique()}\\n')","d5fc46a9":"df.rename(columns={'Restaurant_id': 'restaurant_id',\n                   'City': 'city',\n                   'Cuisine Style': 'cuisine_style',\n                   'Ranking': 'ranking',\n                   'Rating': 'rating',\n                   'Price Range': 'price_range',\n                   'Number of Reviews': 'reviews_number',\n                   'Reviews': 'reviews',\n                   'URL_TA': 'url_ta',\n                   'ID_TA': 'id_ta'}, inplace=True)\n# show the data\ndf.head(1)","95c0034f":"# Plot missing values\ncols = df.columns\nfig, ax = plt.subplots(figsize=(7, 7))\nsns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colors))\n\n# Show in percents\nfor col in df.columns:\n    pct_missing = np.mean(df[col].isnull())\n    print(f'{col} - {round(pct_missing*100)}%')","31cb673b":"plt.figure(figsize=(15, 5))\nplt.subplot(121)\nsns.distplot(df_train.Rating.values, bins=10, color=colors[0])\nplt.title('Rating Distribution\\n', fontsize=15)\nplt.xlabel('Rating')\nplt.ylabel('Quantity (frequency)')\n\nplt.subplot(122)\nsns.boxplot(df_train.Rating.values, color=colors[1])\nplt.title('Rating Distribution\\n', fontsize=15)\nplt.xlabel('Rating')","41e14152":"df_train['Rating'].describe()","8ee1090e":"print(f'Unique Id quantity: {df.restaurant_id.nunique()}')\ndf['restaurant_id'].value_counts()","c0779e78":"plt.figure(figsize=(15, 5), dpi=100)\nsns.countplot(df['city'], order=df['city'].value_counts().index)\nplt.xticks(rotation=45)\nplt.title('Cities Distribution\\n', fontsize=15)\nplt.xlabel('City Name')\nplt.ylabel('Quantity (frequency)')\n\nprint(f'Total Number of Cities in DataSet: {df.city.nunique()}')","050503a0":"# Fix Oporto\ndf['city'] = df['city'].replace(['Oporto'], 'Porto')","4b89c123":"# Before we start, we need to save an information in the dataset, where there were a missing values\n# Probably, it can improve our MAE\n\n# Create a column where indicate that Cuisine is not presented for this restaurant\ndf['cuisine_style_empty'] = df['cuisine_style'].isnull().astype('uint8')\n\n# Fill missing values in column with 'unknown'\ndf['cuisine_style'] = df['cuisine_style'].fillna(\"['unknown']\")\n\n# convert string in the column into a list\ndf['cuisine_style'] = df['cuisine_style'].apply(\n    lambda x: eval(x))  # transform to a list","4ece26f4":"# Create separate dataframe for the analysis\n\ndf1 = df[['city', 'cuisine_style', 'ranking', 'rating',\n          'reviews_number']].explode('cuisine_style')\n\n# -1 cos we already filled missed value. Dont count it\nprint(df1['cuisine_style'].nunique()-1)","965ec7de":"df_cuisine_style = df1['cuisine_style'].value_counts(\n).sort_values(ascending=False)[:10]\n\ncount_ths = np.arange(0, 1.3e4, 5e3)\ncount = np.arange(0, 20, 5)\n\n\nfig = plt.figure(figsize=(15, 5))\nax = plt.subplot()\n\n\nplt.bar(x=df_cuisine_style.index, height=df_cuisine_style, color=colors[0])\n\nplt.yticks(count_ths, count)\nplt.xticks(rotation=45)\nplt.ylabel('Total Places (Thousands)')\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nplt.title('Top 10 most common cuisines')\nax.tick_params(direction='out', length=0, width=0, colors='black')","d25a645d":"df_cuisine_style = df1.groupby(\n    'cuisine_style').reviews_number.sum().sort_values(ascending=False)[:10]\n\ncount_ths = np.arange(0, 3.3e6, 5e5)\ncount = np.arange(0, 9.3, 0.5)\n\nfig = plt.figure(figsize=(15, 5))\nax = plt.subplot()\n\nplt.bar(x=df_cuisine_style.index, height=df_cuisine_style, color=colors[1])\n\nplt.yticks(count_ths, count)\nplt.xticks(rotation=45)\nplt.ylabel('Total Reviews (Million)')\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nplt.title('Top 10 most reviewed cuisines')\nax.tick_params(direction='out', length=0, width=0, colors='black')","5f1792fd":"plt.figure(figsize=(15, 5))\nplt.subplot(121)\nsns.distplot(df.ranking.values, bins=25, color=colors[0])\nplt.title('Ranking Distribution\\n', fontsize=15)\nplt.xlabel('Ranking')\nplt.ylabel('Quantity (frequency)')\n\nplt.subplot(122)\nsns.boxplot(df.ranking.values, color=colors[1])\nplt.title('Ranking Distribution\\n', fontsize=15)\nplt.xlabel('Ranking')","a46133c2":"df['ranking'].describe()","5a99b132":"plt.figure(figsize=(15, 5))\n\nfor city in (df['city'].value_counts())[0:10].index:\n    sns.distplot(df['ranking'][df['city'] == city], kde=False, label=city)\n\nplt.legend(prop={'size': 10})\nplt.title('Ranking Distribution among cities\\n', fontsize=15)\nplt.xlabel('Ranking')\nplt.ylabel('Quantity (frequency)')","597e8140":"df['price_range'] = df['price_range'].fillna('NA')  # fill NaN by NA\nprice_ranges = {'$': 'Cheap', '$$ - $$$': 'Medium',\n                '$$$$': 'High', 'NA': 'NotAvailable'}  # rename\ndf['price_range'] = df['price_range'].map(price_ranges)","34a3ccc4":"plt.figure(figsize=(10, 5))\n\nsns.boxplot(x='price_range', y='rating', data=df)\n\nplt.title('Price Range to Rating\\n', fontsize=15)\nplt.xlabel('Price range')\nplt.ylabel('Rating')","c15cf524":"df_valid_rev = df[df['price_range'] != 'NotAvailable']\ncities = df_valid_rev['city'].unique()\n\nfig_price_ranges, ax_price_ranges = plt.subplots(16, 2, figsize=(15, 40))\n\nind = 1\nfor i in range(1, 17):\n    for j in range(1, 3):\n        if (ind <= len(cities)):\n            city = cities[ind-1]\n            ind += 1\n            city_revs = df_valid_rev[df_valid_rev.city == city]\n            df_revs = pd.DataFrame(city_revs.groupby(\n                ['city', 'price_range']).restaurant_id.count())\n            df_revs = df_revs.reset_index()\n            fig_price_ranges.add_subplot(ax_price_ranges[i-1, j-1])\n            sns.barplot(data=df_revs, x='city', y='restaurant_id', hue='price_range', hue_order=[\n                        'Cheap', 'Medium', 'High'], palette=['#a6a6a6', '#38d1ff', '#50248f'])\n            plt.ylabel('Reviews - Price Range')","8e42b542":"# summ the reviews by city and price range\nreviews_city_price = pd.DataFrame(df_valid_rev.groupby(['city', 'price_range']).reviews_number.sum())\n# retrieving the number of restaurants by city and price\nplaces_city_price = pd.DataFrame(df_valid_rev.groupby(['city', 'price_range']).restaurant_id.count())\n\n# Add the corresponding city\nreviews_city_price['restaurants_number'] = places_city_price['restaurant_id']\n\n# let's calculate the average number of reviews for each price range\nreviews_city_price['avg_reviews'] = round(reviews_city_price.reviews_number \/ reviews_city_price.restaurants_number, 1)","5edb8689":"reviews_city_price.head()","17ead44c":"fig_price_ranges, ax_price_ranges = plt.subplots(16, 2, figsize=(20, 40))\n\nind = 1\nfor i in range(1, 17):\n    for j in range(1, 3):\n        if (ind <= len(cities)):\n            city = cities[ind-1]\n            ind += 1\n            city_revs = pd.DataFrame(\n                reviews_city_price.loc[(city), 'avg_reviews'])\n            city_revs['city'] = city\n            city_revs = city_revs.reset_index()\n            fig_price_ranges.add_subplot(ax_price_ranges[i-1, j-1])\n            sns.barplot(data=city_revs, x='city', y='avg_reviews', hue='price_range', hue_order=[\n                        'Cheap', 'Medium', 'High'], palette=['#a6a6a6', '#38d1ff', '#50248f'])\n            plt.ylabel('Avg No of Revs')","2f3efb36":"print(df['reviews'][0])\nprint(df['reviews'][3])\nprint(f'Govno {type(df.loc[1][\"reviews\"])}')","22d3d9c6":"# create a template for search\nlrx = re.compile('\\[\\[.*\\]\\]')\nnan = None\n\n\ndef review_extraction(row):\n    '''Function is called for extracting data from column \n    reviews and splitting it out into a separate columns\n    INPUT: Whole dataset\n    OUTPUT: Dataset with additional columns'''\n\n    cell = row['reviews']\n    aux_list = [[], []]  # create an auxilliary list for saving temp.data\n    if type(cell) == str and lrx.fullmatch(cell):  # compare with searech template\n        aux_list = eval(cell)  # transform into a list\n\n    row['first_review'] = aux_list[0][1] if len(aux_list[0]) > 1 else nan\n    row['last_review'] = aux_list[0][0] if len(aux_list[0]) > 0 else nan\n\n    row['first_date'] = pd.to_datetime(\n        aux_list[1][1] if len(aux_list[1]) > 1 else nan)\n    row['last_date'] = pd.to_datetime(aux_list[1][0] if len(\n        aux_list[1]) > 0 else nan, format='%m\/%d\/%Y', errors='coerce')\n\n    row['first_date'] = pd.to_datetime(row['first_date'])\n    row['last_date'] = pd.to_datetime(row['last_date'])\n\n    return row","0c15ae4e":"# NOTE!!!\n# The function takes very long time.\n# Be patient if you run this notebook in your PC\n\n# apply the function to dataset and see the result\ndf = df.apply(review_extraction, axis=1)\n\n# show data\ndf.sample(3)","89bb4be1":"# Create a function to transform date to days\n\ndef get_days(timedelta):\n    '''transform date to a day'''\n    return timedelta.days","850ce2d1":"# find a diffderence between date of the first review and the last one\n# add this information into a new column\n\ndf['diff_rev'] = df['last_date'] - df['first_date']\n\n# call the function and get difference in days\ndf['diff_rev'] = df['diff_rev'].apply(get_days)\n\n# show data\ndf.sample(3)","1617112e":"# Plot it\nsns.boxplot(df.diff_rev.values, color=colors[0])\nplt.title('Days between comments Distribution\\n', fontsize=15)\nplt.xlabel('Days')","24c1348a":"# simply revert a sign  to a positive where it is negative\ndf['diff_rev'] = df['diff_rev'].apply(lambda x: abs(x))","266e15c1":"CURRENT_DATE = pd.to_datetime('12\/01\/2021')","55a201c0":"df['days_from_last_rev'] = df['last_date'].apply(\n    lambda date: CURRENT_DATE - date)\ndf['days_from_last_rev'] = df['days_from_last_rev'].apply(get_days)","e684228e":"df.sample(3)","e552cf14":"# check how many cells with reviews have no revierw\ndf[['first_review', 'last_review']].isnull().sum()","958eec19":"# check how many cells of reviews_number have no review\ndf['reviews_number'].isnull().sum()","b8dcefc8":"# create a aux. df with only missed reviews for analyze\n\nno_rev_num = df[df['reviews_number'].isnull()]\nno_rev_num.sample(2)","e99ed655":"no_rev_num[['first_review', 'last_review']].isnull().sum()","dddd7438":"# Create a column which indicate that review is not avaliable\ndf['first_review_miss'] = df['first_review'].isnull().astype('uint8')\ndf['last_review_miss'] = df['last_review'].isnull().astype('uint8')","084c7ee9":"# Replace Nan values with 'No comment' for further data proceeding\ndf['last_review'] = df['last_review'].fillna('no comment')\ndf['first_review'] = df['first_review'].fillna('no comment')\n\n# show data\ndf.sample(2)","b7ec0207":"df_sentiment = df[['first_review', 'last_review']]\n\n# show data\ndf_sentiment.sample(3)","0cbdf855":"# Create a function to clean comments\n\ndef cleanTxt(text):\n    '''Function is called for cleaning text from trash\n    INPUT: dirty string\n    OUTPUT: More or less clean string'''\n\n    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Remove @\n    text = re.sub(r'#', '', text)  # remove #\n    text = re.sub('^a-zA-Z', ' ', text)\n    text = re.sub(r'https?:\\\/\\\/\\S+', '', text)  # remove hyperlink\n    text = re.sub(r'\ud83d\udc4d\ud83c\udffb', '', text)\n    # there are much more emoji. I don't know how to identify them so far\n    text = re.sub(r'\ud83c\udf55', '', text)\n\n    text = text.lower()\n    text = text.strip()\n    #text = text.split()\n    return text","13559b5d":"# Apply function to clean a text\n\ndf_sentiment['first_review'] = df_sentiment['first_review'].apply(cleanTxt)\ndf_sentiment['last_review'] = df_sentiment['last_review'].apply(cleanTxt)","10ab815b":"# Create a function to get the subjectivity\ndef get_subjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\n# Create a function to get the polarity\ndef get_polarity(text):\n    return TextBlob(text).sentiment.polarity","871426df":"# Create new cols and call the func\n\ndf_sentiment['subjectivity_fst'] = df_sentiment['first_review'].apply(\n    get_subjectivity)\n\ndf_sentiment['subjectivity_snd'] = df_sentiment['last_review'].apply(\n    get_subjectivity)\n\ndf_sentiment['polarity_fst'] = df_sentiment['first_review'].apply(get_polarity)\ndf_sentiment['polarity_snd'] = df_sentiment['last_review'].apply(get_polarity)\n\n# show data\ndf_sentiment.sample(4)","2d95080b":"# Plot Word Cloud\nall_words_1 = ' '.join(\n    [reviews for reviews in df_sentiment['first_review'] if reviews != 'no comment'])\nall_words_2 = ' '.join(\n    [reviews for reviews in df_sentiment['last_review'] if reviews != 'no comment'])\nall_words = all_words_1 + all_words_2\nwordCloud = WordCloud(width=500, height=300, random_state=21,\n                      max_font_size=119).generate(all_words)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordCloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","8aa9c4d9":"# Create a function to compute the negative, neutral and positive analysis\n\ndef get_analysis(score):\n    if score < 0:\n        return 'negative'\n    elif score == 0:\n        return 'neutral'\n    else:\n        return 'positive'","e43245f6":"df_sentiment['analysis_fst'] = df_sentiment['polarity_fst'].apply(get_analysis)\ndf_sentiment['analysis_snd'] = df_sentiment['polarity_snd'].apply(get_analysis)\ndf_sentiment.sample(5)","64205c09":"# # Print all of the positive reviews\n\n# j=1\n# sortedDF_1 = df_sentiment.sort_values(by='polarity_fst')\n# for i in range(0,sortedDF_1.shape[0]):\n#   if (sortedDF_1['analysis_1'][i] == 'positive'):\n#     print(str(j)+ ')' +sortedDF_1['first_review'][i])\n#     print()\n#     j += 1","71a2ed7c":"# Get the percentage of positive reviews\np_rev = df_sentiment[df_sentiment['analysis_fst'] == 'positive']\np_rev = p_rev['first_review']\nround((p_rev.shape[0] \/ df_sentiment.shape[0])*100, 1)","fe18ffb4":"# Get the percentage of positive reviews\np_rev = df_sentiment[df_sentiment['analysis_snd']=='positive']\np_rev = p_rev['last_review']\nround((p_rev.shape[0] \/ df_sentiment.shape[0])*100, 1)","6d24ff83":"# Get the percentage of negative reviews\nn_rev = df_sentiment[df_sentiment['analysis_fst'] == 'negative']\nn_rev = n_rev['first_review']\nround((n_rev.shape[0] \/ df_sentiment.shape[0])*100, 1)","f5211c3b":"# Get the percentage of negative reviews\nn_rev = df_sentiment[df_sentiment['analysis_snd']=='negative']\nn_rev = n_rev['last_review']\nround((n_rev.shape[0] \/ df_sentiment.shape[0])*100, 1)","65a29fde":"# plot and visualize\n\nplt.title('Sentiment Analysis of first comments')\nplt.xlabel('Sentiment')\nplt.ylabel('Counts')\ndf_sentiment['analysis_fst'].value_counts().plot(kind='bar')\nplt.show()\n\n# show the value counts\n\ndf_sentiment['analysis_fst'].value_counts()","0124a9df":"# plot and visualize\n\nplt.title('Sentiment Analysis of last comments')\nplt.xlabel('Sentiment')\nplt.ylabel('Counts')\ndf_sentiment['analysis_snd'].value_counts().plot(kind='bar')\nplt.show()\n\n# show the value counts\n\ndf_sentiment['analysis_snd'].value_counts()","dd3bf1af":"df_sentim = pd.concat(\n    [df, df_sentiment[['analysis_fst', 'analysis_snd']]], axis=1)\ncities = df_sentim['city'].unique()\n\nfig_sentiment_ranges, ax_sentiment_ranges = plt.subplots(16, 2, figsize=(15, 40))\n\nind = 1\nfor i in range(1, 17):\n    for j in range(1, 3):\n        if (ind <= len(cities)):\n            city = cities[ind-1]\n            ind += 1\n            city_revs = df_sentim[df_sentim.city == city]\n            df_revs = pd.DataFrame(city_revs.groupby(\n                ['city', 'analysis_snd']).restaurant_id.count())\n            df_revs = df_revs.reset_index()\n            fig_sentiment_ranges.add_subplot(ax_sentiment_ranges[i-1, j-1])\n            sns.barplot(data=df_revs, x='city', y='restaurant_id', hue='analysis_snd',palette=['#a6a6a6', '#38d1ff', '#50248f'])\n            plt.ylabel('Reviews - Price Range')","6700cb74":"# transform to numeric\ndf['id_ta'] = df['id_ta'].apply(lambda x: int(x[1:]))","3b123927":"# Check the counts of id\ndf['id_ta'].value_counts()","999ec5a3":"# Create a list with restaurants which might be in chain\n\nchained_rest_list = list(df['restaurant_id'].value_counts()[\n    df['restaurant_id'].value_counts() > 1].index)\n\n# If it is in chain, we add in a new column the identificator '1', otherwise '0'\ndf['chained_rest'] = df[df['restaurant_id'].isin(\n    chained_rest_list)].restaurant_id.apply(lambda x: 1)\ndf['chained_rest'] = df['chained_rest'].fillna(0)\n\n# Check\ndf['chained_rest'].value_counts()","9c57f4b9":"all_cities = df['city'].value_counts().index","eb3abb0c":"# Cos we don't have too many cities, let's create a dict where mention whether the city is a capital\ncapital = [True, True, True, False, True, False, True, \n           True, True, True, True, True, False, False, \n           False, True, True, True, True, True, True, \n           True, False, False, False, False, True, \n           True, True, True, True]\n\ncapital_dict = dict(zip(list(all_cities), capital))","111fee0c":"df['capital'] = df['city'].map(capital_dict)\n\n# show data\ndf.head(2)","cf5009e5":"city_population = {\n    'London': 8787892,\n    'Paris': 2187526,\n    'Madrid': 3300000,\n    'Barcelona': 1593075,\n    'Berlin': 3726902,\n    'Milan': 1331586,\n    'Rome': 2860000,\n    'Prague': 1300000,\n    'Lisbon': 505526,\n    'Vienna': 1900000,\n    'Amsterdam': 872080,\n    'Brussels': 144784,\n    'Hamburg': 1840000,\n    'Munich': 1558395,\n    'Lyon': 506615,\n    'Stockholm': 975904,\n    'Budapest': 1752286,\n    'Warsaw': 1720398,\n    'Dublin': 1793579 ,\n    'Copenhagen': 1330993,\n    'Athens': 3090508,\n    'Edinburgh': 476100,\n    'Zurich': 402275,\n    'Porto': 237559,\n    'Geneva': 196150,\n    'Krakow': 779115,\n    'Oslo': 697549,\n    'Helsinki':  656229,\n    'Bratislava': 563682,\n    'Luxembourg': 626108,\n    'Ljubljana': 295504\n}","b83cca5e":"city_country = {\n    'London': 'United Kingdom',\n    'Paris': 'France',\n    'Madrid': 'Spain',\n    'Barcelona': 'Spain',\n    'Berlin': 'Germany',\n    'Milan': 'Italy',\n    'Rome': 'Italy',\n    'Prague': 'Czech',\n    'Lisbon': 'Portugal',\n    'Vienna': 'Austria',\n    'Amsterdam': 'Netherlands',\n    'Brussels': 'Belgium',\n    'Hamburg': 'Germany',\n    'Munich': 'Germany',\n    'Lyon': 'France',\n    'Stockholm': 'Sweden',\n    'Budapest': 'Hungary',\n    'Warsaw': 'Poland',\n    'Dublin': 'Ireland' ,\n    'Copenhagen': 'Denmark',\n    'Athens': 'Greece',\n    'Edinburgh': 'Schotland',\n    'Zurich': 'Switzerland',\n    'Porto': 'Portugal',\n    'Geneva': 'Switzerland',\n    'Krakow': 'Poland',\n    'Oslo': 'Norway',\n    'Helsinki': 'Finland',\n    'Bratislava': 'Slovakia',\n    'Luxembourg': 'Luxembourg',\n    'Ljubljana': 'Slovenija'\n}","a0d2136d":"# add columns with the city population\ndf['city_population'] = df['city'].map(city_population)\n\n# add column with countries\ndf['country'] = df['city'].map(city_country)","76f22f84":"df['new_city'] = df['city']\n# Create a top Cites list (more than 70% in Dataset)\ntop_cities_list = df['new_city'].value_counts()[\n    df['new_city'].value_counts() > np.percentile((df['new_city'].value_counts().values), 70)].index.tolist()","67be1618":"cities_to_drop = list(set(all_cities)-set(top_cities_list))\ndf.loc[df['new_city'].isin(cities_to_drop), 'new_city'] = 'Other'","1e5a5648":"plt.figure(figsize=(15, 5), dpi=100)\nsns.countplot(df['new_city'], order=df['new_city'].value_counts().index)\nplt.xticks(rotation=45)\nplt.title('Cities Distribution\\n', fontsize=15)\nplt.xlabel('City Name')\nplt.ylabel('Quantity (frequency)')\n\nprint(f'Total Number of Cities in DataSet: {df.city.nunique()}')","ca6a2ebe":"from sklearn.preprocessing import OneHotEncoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_city = pd.DataFrame(OH_encoder.fit_transform(df[['new_city']]))\n\n# Adding column names to the encoded data set.\nOH_city.columns = OH_encoder.get_feature_names(['new_city'])\n\n# Show data\nOH_city.head(2)","5b951904":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()  # create an object\nle.fit(df['country'])\ndf['country_CODE'] = le.transform(df['country'])\n\n# show data\ndf.head(2)","2f04e9dd":"df['cuisine_count'] = df['cuisine_style'].apply(lambda x: len(x))\n\n# show data\ndf.head(2)","86f84bfb":"cuisine_rare_lst = df.explode('cuisine_style')['cuisine_style'].value_counts()[\n    df.explode('cuisine_style')['cuisine_style'].value_counts() < 50].index.tolist()","fbafd42e":"def get_cuisine_rare(row):\n    '''Function called for creating a number of\n    rare cuisins\n    INPUT: A cell from dataset\n    OUTPUT: Number of rare cuisins'''\n\n    number = 0\n    for i in cuisine_rare_lst:\n        if i in row:\n            number += 1  # count qty of rare cuisines\n    return number","9e7b8f2d":"# create a column with rare cuisine and call func\ndf['rare_cuisine'] = df['cuisine_style'].apply(get_cuisine_rare)","befc7c34":"# Create a global variable (dict) with cuisines and related region of it\n# I don't know how to use NLP for this case, so this work need to be done by hands :(\n\ncuisine_region = {\n    'France': ['French', 'Central European'],\n    'Sweden': ['Swedish', 'Scandinavian'],\n    'United Kingdom': ['British'],\n    'Germany': ['German', 'Central European'],\n    'Italy': ['Pizza', 'Italian'],\n    'Slovakia': ['Eastern European'],\n    'Austria': ['Austrian'],\n    'Spain': ['Spanish'],\n    'Ireland': ['Irish'],\n    'Belgium': ['Belgian'],\n    'Switzerland': ['Swiss'],\n    'Poland': ['Polish', 'Ukrainian'],\n    'Hungary': ['Hungarian'],\n    'Denmark': ['Scandinavian'],\n    'Netherlands': ['Dutch'],\n    'Portugal': ['Portuguese'],\n    'Czech': ['Czech'],\n    'Norway': ['Norwegian', 'Scandinavian'],\n    'Finland': ['Central European'],\n    'Schotland': ['Scottish'],\n    'Slovenija': ['Slovenian'],\n    'Greece': ['Greek'],\n    'Luxembourg': ['Central European']\n}","ce6c68ec":"# Create a function for identification of local cuisine\n\ndef get_local_cuisine(row):\n    '''Function called for identifying\n    whether restaurant includes local\n    cuisine or not\n    INPUT: A cell from dataset\n    OUTPUT: 1 - if includes\n            0 - if does not include'''\n\n    local_cuis = cuisine_region[row['country']]\n    for i in local_cuis:\n        if i in row['cuisine_style'] and i != '':\n            return 1\n    return 0","412c20a7":"# create a column with identificator, whether cuisine is local\ndf['local_cuisine'] = df.apply(get_local_cuisine, axis=1)\n\n# count them\ndf['local_cuisine'].value_counts()","5d5bc9ed":"# create a func to identify, whether restaurant includes vegan food\n\ndef get_vegan(row):\n    vegan_cuis = ['Vegetarian Friendly', 'Vegan Options',  # All vegan food in my oppinion\n                  'Gluten Free Options', 'Healthy', ]\n    for i in vegan_cuis:\n        if i in row['cuisine_style'] and i != '':\n            return 1\n    return 0","d862de01":"# create a new column with identification, whether restaurant includes a vegan food\ndf['vegan_include'] = df.apply(get_vegan, axis=1)\n\n# Show Data\ndf.sample(2)","77912a56":"city_restaurant = dict(df['city'].value_counts())\ndf['restaurant_qty'] = df['city'].map(city_restaurant)","238f509e":"# devide ranking in dataset by quantity of restaurants in a city\ndf['equiv_ranking'] = df['ranking']\/df['restaurant_qty']","ad468689":"plt.figure(figsize=(15, 7))\n\nfor city in (df['city'].value_counts())[0:10].index:\n    sns.distplot(df['equiv_ranking'][df['city'] == city],\n                 kde=False, label=city)\n\nplt.legend(prop={'size': 10})\nplt.title('Equivalent ranking Distribution among cities\\n', fontsize=15)\nplt.xlabel('Equivalent ranking')\nplt.ylabel('Quantity (frequency)')","ef4a3209":"df['people_per_restaur'] = df['city_population']\/df['restaurant_qty']\n# Show Data\ndf.head(2)","c7b1a82f":"df['reviews_in_city'] = df['city'].apply(lambda x: df.groupby(\n    ['city'])['reviews_number'].sum().sort_values(ascending=False)[x])","d43b3f8d":"df['equivalent_rank_reviews'] = df['ranking'] \/ df['reviews_in_city']\n\n# Show Data\ndf.sample(2)","b200464a":"df['price_range'] = df.price_range.astype('category')","e72a0bbf":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()  # create an object\nle.fit(df['price_range'])\ndf['price_range'] = le.transform(df['price_range'])\n\n# show data\ndf.sample(2)","719422e9":"# NOTE!!! This func takes long for computing\n# Alternative way is to fill all by 0\n# This taken for learning purpose\n# If you run this notebook in your PC, just fill all by zeros\n\n\ndf['reviews_number'] = df.apply(\n    lambda row: 1 if np.isnan(row['reviews_number']) and (row['last_review_miss'] == 0 or row['first_review_miss'] == 0) else row['reviews_number'], axis=1\n)","4f5e7542":"np.isnan(df['reviews_number']).sum()","824b547f":"df['reviews_number'] = df['reviews_number'].fillna(0)","8611f672":"df['diff_rev'] = df['diff_rev'].fillna(0)\ndf['days_from_last_rev'] = df['days_from_last_rev'].fillna(0)\n\ndisplay(df['diff_rev'].isna().sum())\ndf['days_from_last_rev'].isna().sum()","ae7b4c23":"# No impact on MAE.\n# Better to keep Polarity columns\n\n# le.fit(df_sentiment['analysis_fst'])\n# le.fit(df_sentiment['analysis_snd'])\n\n# df_sentiment['analysis_fst'] = le.transform(df_sentiment['analysis_fst'])\n# df_sentiment['analysis_snd'] = le.transform(df_sentiment['analysis_snd'])\n\n# #show data\n# df_sentiment.head(2)","cde97295":"cols_to_drop = ['restaurant_id', 'city', 'new_city', 'cuisine_style',\n                'url_ta', 'reviews', 'first_review',\n                'last_review', 'first_date', 'last_date', 'country']","562a1114":"df_to_model = df.drop(cols_to_drop, axis=1)","753f6351":"# Concat dataframes\n\ndf_to_model = pd.concat(\n    [df_to_model, df_sentiment[['polarity_fst', 'polarity_snd']]], axis=1)","7d0819f7":"df_to_model = pd.concat([df_to_model, OH_city], axis=1)","b23c493c":"# cols_to_drop22 = ['new_city_Barcelona','cuisine_style_empty','new_city_Paris',\n#                  'last_review_miss','new_city_London','price_range',\n#                  'new_city_Lisbon','chained_rest','new_city_Berlin','rare_cuisine',\n#                  'first_review_miss','capital']","014be52b":"cols_to_drop2 = ['new_city_Lisbon','chained_rest','new_city_Berlin','rare_cuisine',\n                 'first_review_miss','capital']","43d0e510":"df_to_model = df_to_model.drop(cols_to_drop2, axis=1)","23e16c2e":"# Plot missing values\ncols = df_to_model.columns\nfig, ax = plt.subplots(figsize=(5, 5))\nsns.heatmap(df_to_model[cols].isnull(),cmap=sns.color_palette(colors))","e2d9610d":"df_to_model['id_ta'].duplicated().sum()","6972f33d":"X = df_to_model.corr()\n\n# Chek the determinant of matrix X (via eginvalues)\nevals,evec = np.linalg.eig(X)\nev_product = np.prod(evals) # product of evalues same as determinant\n\n\nfig, ax = plt.subplots(figsize=(50, 50))\nsns.heatmap(X, vmax=.7, square=True, annot=True)\nprint(f'Rank of Matrix: {np.linalg.matrix_rank(X)}')\nprint(f'Determinat of matrix :{ev_product}')\nprint(f'Shape of matrix :{np.shape(X)}')","b0c3ad10":"# Extract the part of dataframe for testing\ntrain_data = df_to_model.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_to_model.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.rating.values\nX = train_data.drop(['rating'], axis=1)","31da6a6f":"# Let's use the special function train_test_split to split test data\n# allocate 20% of the data for validation (parameter test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","f453b6ad":"# Check\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape\n","fb7f132e":"# Import:\nfrom sklearn.ensemble import RandomForestRegressor # for creating and training a model\nfrom sklearn import metrics # for assessing model accuracy\n","ce9cb410":"# Create a model (do not change parameters)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","7a47562a":"# Train the model on a test dataset\nmodel.fit(X_train, y_train)\n\n# Use a trained model to predict restaurant ratings in a test sample.\n# The predicted values are written to the y_pred variable\ny_pred = model.predict(X_test)","cf128dc3":"# It can be observed that the difference in that real ratings are always multiples of 0.5\n# Write a function to round the predicted ratings accordingly\ndef round_rating_pred(rating_pred):\n    if rating_pred <= 0.5:\n        return 0.0\n    if rating_pred <= 1.5:\n        return 1.0\n    if rating_pred <= 1.75:\n        return 1.5\n    if rating_pred <= 2.25:\n        return 2.0\n    if rating_pred <= 2.75:\n        return 2.5\n    if rating_pred <= 3.25:\n        return 3.0\n    if rating_pred <= 3.75:\n        return 3.5\n    if rating_pred <= 4.25:\n        return 4.0\n    if rating_pred <= 4.75:\n        return 4.5\n    return 5.0","364ca029":"# Round it\nfor i in range(len(y_pred)):\n    y_pred[i] = round_rating_pred(y_pred[i])","af0b7a9a":"# \u0421\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f (y_pred) \u0441 \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u043c\u0438 (y_test), \u0438 \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u043d\u0438 \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f\n# \u041c\u0435\u0442\u0440\u0438\u043a\u0430 \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f Mean Absolute Error (MAE) \u0438 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","cf8afd93":"# 1st approx. MAE: 0.16425\n# 2nd approx. MAE: 0.1639375 - changed analysis to polarity\n# 3nd approx. MAE: 0.163 - dropped useless cols","2b5c9d28":"# Check most important Features\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","9142a635":"# To identify what cols are not impact on a result\na = list(feat_importances.sort_values(ascending=False).head(15).reset_index()['index'])\nb = set(df_to_model.columns)\ncols_to_drop = list(b-set(a))","07e03e24":"test_data.sample(10)","68120b53":"test_data = test_data.drop(['rating'], axis=1)","37a4f0b5":"sample_submission","2476b743":"predict_submission = model.predict(test_data)","02da8e5a":"# Round\nfor i in range(len(predict_submission)):\n    predict_submission[i] = round_rating_pred(predict_submission[i])","2b38c6f5":"predict_submission","7e2b6ad0":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","da6b1037":"<a id=\"sec3.2\"><\/a>\n# [3.2 City column](#sec3.2)","8bb390cd":"Step 1: Create a total number of a restaurants in a single city","a1971c28":"With a reference to <a id=\"sec2.5\"><\/a>\n[2.5 Ranking column](#sec2.5) let's create a column with equivalent ranking.","ec9dcf6a":"<a id=\"sec3.4\"><\/a>\n# [3.4 Ranking column](#sec3.4)","42383cd0":"Our dataset has 16751 cuisines that belong to the country of origin.[](http:\/\/)","ce6cabb0":"<a id=\"sec1\"><\/a>\n# [1.  Load and Chek the Dataset](#sec1)","ae62f135":"Step 2: Create an equivalent ranking","8718af88":"Create a column with with equivalent to reviews number ranking","3ce17938":"The determinant of corr matrix is 0. Huston, we got a problem!!!","d9bc9f9b":"<a id=\"sec3.7\"><\/a>\n# [3.7 Time Difference between reviews](#sec3.7)","835cec34":"<a id=\"sec3.5\"><\/a>\n# [3.5 Price range column](#sec3.5)","baf1c04e":"--- \nThe dataset has 125 unique cuisines.\n\nVegetarian Friendly places are clearly the most common ones around Europe, followed by mostly European-style cuisine.\n\n---","3bfa88fa":"<a id=\"sec3.10\"><\/a>\n# [3.10 Resulting dataframe verification](#sec3.10)","f0febddb":"For a more convenient analyze let's withdraw columns with reviews only and create a new data frame.","5de9cb61":"Add one more feature which tells that restaurant has in its set a rare cuisine.\n\nAssume rare value if it is met in dataset less than 50 times.\n\n**VER_3 UPD:** No Impact on MAE","b08e3480":"Well, that is clearly obvious,that some data in columns pretend to be as a list, however it is string or float type.\n\nSuch as:\n - column Cuisine Style is looks like a list, but has float64 type;\n - column Reviews looks like nested list with following template [[ comment_1 , comment_2 ], [date1 , date2]], but in fact it has str type of data.","37cdf87d":"It is obvious that all cities have more positive sentiments than negative. It does not reveal interesting information","24ed6298":"<a id=\"sec3.1\"><\/a>\n# [3.1 Restaurant_Id column](#sec3.1)","d6842f89":"<a id=\"sec2.1\"><\/a>\n# [2.1 Target Variable analysis](#sec2.1)","595f8cbd":"<a id=\"sec2.4\"><\/a>\n# [2.4 Cuisine_style column](#sec2.4)","b30b36ae":"<a id=\"sec2.8.1\"><\/a>\n# [2.8.1 Pre-processing and analyse of missing values](#sec2.8.1)","0eb0862c":"**VER_3 UPD:** No Impact on MAE","7fb84d1c":"Owerwhelming majority of missing reviews refer to a 'First' placed feedback","d22fe1bc":"<a id=\"sec5\"><\/a>\n# [5.  SUBMISSION](#sec5)","50aaf3f5":"<a id=\"sec3\"><\/a>\n# [3. Feature Engineering and tidy dataset up](#sec3)","0940f4dc":"---\n\nRestaurants with high prices getting low ratings less often than restaurants with a medium and cheap level\n\n---","efbdedda":"With a reference to <a id=\"sec2.2\"><\/a>\n[2.2 Restaurant_Id column](#sec2.2), we may add one additional column which depicts, whether the restaurant belongs to the chain or not.","2e823d69":"<a id=\"sec3.6\"><\/a>\n# [3.6 Reviews number column](#sec3.6)","05991a8a":"Let's add a column with information about whether the Restaurant is in capital or not","715ad78a":"---\n\nWell, the total quantity of rows is 50k while number of unique id is 13k.\n\nWe may see some dublicates here. But:\n\nWith a reference to a column description it may says us that we have some chain restaurants in dataset.\n\n---","10a5a4ed":"<a id=\"sec2.5\"><\/a>\n# [2.5 Ranking column](#sec2.5)","735d0499":"Let's see what type of data in each column-cell","875afebf":"<a id=\"sec3.3\"><\/a>\n# [3.3 Cuisine_style column](#sec3.3)","c2911b56":"## Description\n\nSometimes, some dishonest restaurants cheating TripAdvisor and their guests by winding up the rating higher than it should be.\n\nThe main aim of the project is to try to predict the rating of the restaurant with given data.\n\nIn addition, the side-aim is to provide an analyze of the data set.\n\nIn case if the predictions of our model have significant differences from the actual result, then, most likely we found a dishonest restaurant. ","3b2da940":"Let's add a new feature - the quantity of cuisine in a restaurant.","01784a44":"<a id=\"sec2.6\"><\/a>\n# [2.6 Price range column](#sec2.6)","c0bbf150":"<a id=\"sec1.2\"><\/a>\n# [1.2 Show the data types](#sec1.2)","062efde9":"### Import Libraries","b7e52c8b":"<a id=\"sec2.8.2\"><\/a>\n# [2.8.2 Sentiment Analysis](#sec2.8.2)","f034931d":"<a id=\"sec3.9\"><\/a>\n# [3.9 MAP the Data frame to a model](#sec3.9)","dd6728fd":"That's pretty interesting. The review number actually doesn't show accurate information with a number of reviews. For example, in the feedback columns we have at least one review but it is not shown in the review's number.\n\nProbably we need to drop the columns with the number of reviews or fill somehow the missing data. We will decide it in section 3 'Feature engineering'\n\nLet's go ahead with analyze","3ece90bf":"---\n\nThe chart is more or less simillar with above one.But it's notable that the Vegan, European and <a id=\"ref1\"><\/a>\n[Gluten Free](#ref1) Options are very likely to be reviewed by the customers.\n\nPeople are not tend to review restaurants where cuisins are not shown. So we made a right desicion to keep (but not delete) rows with missing cuisins by replacing nan value to 'unknown'. Perhaps this info may help us in further modeling.\n\n---","418530ee":"Let's create a new column where we identify the sentiment itself","79eef4f1":"<a id=\"sec2.3\"><\/a>\n# [2.3 City column](#sec2.3)","432f0835":"Is it important to know (perhaps), whether the cuisine belongs to the region where it is coming from?\n\nLet's add this feature.\n\n**VER_3 UPD:** No Impact on MAE","63cdec46":"Encode the city column","01c42c6f":"What about the positiveness? Which city is most positive?","bc1b6d1b":"<a id=\"sec2\"><\/a>\n# [2. Exploratory Data Analysis](#sec2)","b7aa2118":"Add the population and add information to what country the city belongs.","3c5611d4":"Encode the Country column","34b8873c":"We're not going to analyse this column. Just drop it in feture engineering.","d35d0b57":"Let's see into the distribution of negative and positive feedbacks","aa2387fc":"First of all, with a reference to <a id=\"sec1.3\"><\/a>\n[1.3 Missing values](#sec1.3), we know that this column has 35% of nan values and presented as dollar symbol\n\nLet's replace these symbols in the dataset for the three price ranges with more intuitive (cheap - medium - high ranges) and replace the NaN ones (set as not available)","9cf81e60":"Let's encode relevant columns\n\nV3_UPD: No impact on MAE. Kepp polarity instead","9efdbe95":"How is it distributed?","d320afda":"Let's check how many positive feedback we have and how they changed","51190fc0":"Do customers use a price range as a parameter to leave negative or positive review?\n\nThen let's sort out the 'NotAvailable' price range. This won't give us interesting info","2e129d91":"<a id=\"sec1.1\"><\/a>\n# [1.1  Show a basic info](#sec1.1)","cd8243a7":"# Find A Dishonest Restaurant","7a9138cc":"---\n\nThe overwhelming majority of restaurants presented in the dataset located in London, Paris, Madrid. \n\nAll cities are European.\n\nThe city of Oporto is not identified. It is actually the name of the restaurant in Porto(Portugal)\n\nMost likely the city shall have a name as Porto instead of Oporto.\n\n---","61593d58":"Now, let's sort our dataset and compare mising reviews with missing reviews number","072692f7":"Generate subjectivity and polarity","5457a4a1":"<img src=\"https:\/\/4.bp.blogspot.com\/_15hf9pFgwks\/S0Tx2j4jl3I\/AAAAAAAAAWo\/0YQwpN51z5E\/s320\/Fig+2-1+Fortune+teller.jpg\" width=\"400px\">\n","e694549c":"Let's see to most frequent words","d8e55195":"## Resume\n---\n\n - DataSet has 50k rows and 10 columns with fetures.\n - Column 'cuisine_style' has 23% of missing values\n - Column 'price_range' has 35% of missing values\n - Column 'reviews_number' has 6% of missing values while column 'reviews' has no any single missing value. Here is a discrepancy. If we visually check the content of the column 'reviews', we find the following value :'[[],[]]'. Definatelly it is a missing value which need to be treated in further data-processing.\n - Type in particular cell sometimes differ with pd.dtypes. Need to take care about that in further.\n \n ---\n","5bc03c3c":"---\n\nWe already know that this column has nan values. Also we know that The most reviewable cuisin is vegan\n\nMissing values will be filled in Feature engineering section.\n\nImportant Note. \n\nFilling the missing data must be done in a complex with proceeding the reviews column as we note that there is some descripancy between review numbers and revies itself\n\n---","96511a8b":"Check a top 10 of the most common cuisine styles","3cfd4643":"<a id=\"sec1.3\"><\/a>\n# [1.3 Missing values](#sec1.3)","6bda06c0":"Same thing in negative feedback but not so significant","a930ad29":"Well, we have increased positive feedbacks since the first review placed on the website and the last review. Interesting why?","f8b78abb":"Let's check how much negative feedback we have and how they changed","fef01602":"Do not drop duplicates. Somehow it has a negative impact on MAE","83d06a7e":"As we can see, here are negative values. That might mean that some dates are not refer to a first placed comment. Means it is inverted.\n\nLet's fix it","780ae923":"<a id=\"sec2.9\"><\/a>\n# [2.9 url_ta column](#sec2.9)\n","df1f7514":"---\n\nAs we may see the ranking has normal distribution in each separate city. And as we already know , London is taking a top place by presented restaurants. Not surprised, why do we have a shifting of the distribution in a left side. Big cities has lots of restaurants.\n\nThen in a further feature engeniring section (below) we need to consider this by creating equivalent ranking.\n\n---","3dab42fb":"The Target variable has a normal distribution shifted to the right side of 1 to 5. The first and third quartiles are in the range from 3.5 to 4.5, the median is 4. Also outliers has been observed for target variable.","04384974":"Simply replace the NaN values by zeros","0439749d":"Create a column with mean value of people to a single restaurant in a city","54848462":"First of all let's drop non-numeric columns","d5e43749":"<img src=\"http:\/\/idwbi.com\/wp-content\/uploads\/2017\/01\/Data-Cleansing-Blog.jpg\" width=\"400px\">","d9c5037d":"As was mentioned above the content has structural data - [ [ ],[ ] ].\n\n**However** it is not a list, but just a string type of variable.\n\nThen, we want to extract the data form the colums 'reviews' into a 4 independent colums and remove original one from the dataset.\n\nIn the end we shall get following columns in our dataset:\n\n***first_review*** - we put review No.1 (if any).\n\n***first_date*** - we put the date when the review was added\n\n***last_review*** - we put review No.2 (if any).\n\n***last_date*** - we put the date when the review was added\n\n***Diff_rev*** - Time difference in days between first and second review","094dd979":"In overwhelming majority of cities the most discussed restaurants belong to the high priced range: the high the expected quality and the higher the attention paid by the customers.\n\nException is only Bratislava. People tend to review not only expensive restaurants but also cheap. Interesting why?","1dbf038f":"After running this code several times, it is decided to drop columns which have no significant effect on MAE","107e7602":"As we may see, the most frequent words are 'Nice, good', 'food' etc. So people tend to remain more positive reviews and often mention 'food'.","7c7cdd47":"Let's observe an empty data","389e46e3":"Before we start, let's create two new vectors that indicate to us what 'review' has missing values.","9ad4fd6e":"Let's sort cities by most common in dataset. Rest of them we call as 'Other'\n\nTo do so, create a new column ' new_city'.\n\nThe column ' city we will use further and then drop it\n","5dfdd674":"---\n\nThe Ranking distribution shifted to the left side and scattered from 1 to 16444. The first and third quartiles are in the range from 972 to 5241, the median is 2278.\n\nHowever, with a reference to the data description, the Ranking is the place that this restaurant occupies among all restaurants in its city. So we cannot observe it separately from cities.\n\n---","b9a38df0":"A restaurant's rating may depend not only on how much time has passed between the last two reviews, but also on how many days have passed since the last review was posted to the current date.\n\nCreate a relevant column","73e3b39b":"Ok. we filled some missed reviews number. Rest of them fill with 0","2114fc88":"Clean the text","cf3b3aed":"What about to take a look into the average number of reviews per price range: does the price range somehow influence the possibility of a customer to leave a review?","4ab4dad9":"As we may see, the last plasced comments are more positive. Also we have less neutral comments, but it is connected with that thing, we filled missing values by 'unknown' in a first comment. So that 'unknown' gives us more neutral sentiments.","34046fd0":"<a id=\"sec4\"><\/a>\n# [4.  Model](#sec4)","70f30411":"With a reference to our EDA, we noticed that the Vegan and <a id=\"ref1\"><\/a>\n[Gluten Free](#ref1) Options are very likely to be reviewed by the customers. Let's add the feature, whether a restaurant includes vegan food.\n\n**VER_3 UPD:** No Impact on MAE","e95e6343":"<a id=\"sec2.7\"><\/a>\n# [2.7 Reviews number column](#sec2.7)","735d5b35":"<a id=\"sec2.2\"><\/a>\n# [2.2 Restaurant_Id column](#sec2.2)","7f8e9e17":"The restaurants are reviewed as Medium-priced. This is probably due to travellers trying to save and settling for more regular menus.\n\nThe only exception are the two cities in Switzerland (Geneva and Zurich) where the Higher ranged restaurants are at least as many (or more than, like in Geneva) as the Cheaper ones. Do people go to Switzerland looking for expensive places to eat or is it just the cost of life higher in Switzerland? Maybe we need to add a cost of life column in our dataset. Perhaps it will impact on the final prediction result","7e8b38b2":"<a id=\"sec3.8\"><\/a>\n# [3.8. Sentiment analyze data frame](#sec3.8)","dfb9c72f":"Concat our additional data frames what we got during EDA and Feature eng.","6e1390b3":"Step 3. Check distribution of a normalized ranking","e38b12ac":"<a id=\"sec2.8\"><\/a>\n# [2.8 Reviews column](#sec2.8)","f59120b1":"Which are the cuisines that people tend to review?","c715c465":"Well, now it looks better than in section 2.5. Distribution is normal","cd7a6516":"Create a column with time difference between first and last reviews","bb8378ed":"Here we have some duplicates here. As per the dataset description, this is a univocal identifier for each restaurant. Drop duplicates in the feature engineering section.","bded4697":"# INDEX\n1. [Load and Chek the Dataset](#sec1)\n    * [1.1  Show a basic info](#sec1.1)  \n    * [1.2 Show the data types](#sec1.2)\n    * [1.3 Missing values](#sec1.3)\n2. [Exploratory Data Analysis](#sec2)\n    * [2.1 Target Variable analysis](#sec2.1)\n    * [2.2 Restaurant_Id column](#sec2.2)\n    * [2.3 City column](#sec2.3)\n    * [2.4 Cuisine_style column](#sec2.4)\n    * [2.5 Ranking column](#sec2.5)\n    * [2.6 Price range column](#sec2.6)\n    * [2.8 Reviews column](#sec2.8)\n    * * [2.8.1 Pre-processing and analyse of missing values](#sec2.8.1)\n    * * [2.8.2 Sentiment Analysis](#sec2.8.2)\n    * [2.9 url_ta column](#sec2.9)\n    * [2.10 id_ta column](#sec2.10)\n3. [Feature Engineering and tidy dataset up](#sec3)\n    * [3.1 Restaurant_Id column](#sec3.1)\n    * [3.2 City column](#sec3.2)\n    * [3.3 Cuisine_style column](#sec3.3)\n    * [3.4 Ranking column](#sec3.4)\n    * [3.5 Price range column](#sec3.5)\n    * [3.6 Reviews number column](#sec3.6)\n    * [3.7 Time Difference between reviews](#sec3.7)\n    * [3.8. Sentiment analyze data frame](#sec3.8)\n    * [3.9 MAP the Data frame to a model](#sec3.9)\n    * [3.10 Resulting dataframe verification](#sec3.10)\n4. [Model](#sec4)\n5. [SUBMISSION](#sec5)","0fa61304":"Use the sort of smart filling of missing values. As we know from EDA section, there is a column 'Last review' with some comments, while the Reviews number is empty. So we might assume that it is a kind of mistake.\n\nLet's fill the NaN in this manner:\n\nIf there is 1 comment in a \u2019Last Review\u2018 or 'First review', we put '1' into a Review Numbers column.\n\nIn case of nor the first comment neither the second filled, put 0","7f66cb1b":"Rename columns removing spaces and substituting capital letters","6c7045f4":"Well, it is more interesting. Let's briefly see at the content of object data.","fa0e58c4":"<img src=\"https:\/\/www.netclipart.com\/pp\/m\/349-3494556_forex-scams-by-dishonest-person-lying-cartoon.png\" width=\"400px\">","e09b65f5":"First of all, we have to manage the list of cuisines in a way that we can use the data and produce some statistics.","c577b54b":"<img src=\"https:\/\/i.imgflip.com\/4tm1ve.jpg\" width=\"400px\">","4da7574f":"Let's re-call what have we seen in section 1 of this notebook","db6dcb9d":"<a id=\"sec2.10\"><\/a>\n# [2.10 id_ta column](#sec2.10)","6d71d1b8":"### Column Defination\n\nRestaurant_id \u2014 restaurant \/ restaurant chain identification number;\n\nCity \u2014 In what city it is located;\n\nCuisine Style \u2014 related to a restaurant cuisine;\n\nRanking \u2014 the place that this restaurant occupies among all restaurants in its city;\n\nRating \u2014 restaurant rating according to TripAdvisor (Target Variable);\n\nPrice Range \u2014 restaurant price range;\n\nNumber of Reviews \u2014 Number of Reviews ;\n\nReviews \u2014 data about two reviews that are displayed on the restaurant's website;\n\nURL_TA \u2014 URL on TripAdvisor;\n\nID_TA \u2014 Identificator of restaurant in TripAdvisor's DataBase.\n","9647672d":"So, let's plot a distribution of a ranking depend on city (take 10-top cities in dataset)"}}