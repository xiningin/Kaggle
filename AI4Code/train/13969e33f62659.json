{"cell_type":{"f3a62f28":"code","56174b49":"code","516b57d3":"code","b2c7ce2f":"code","07c02b62":"code","5390d5d6":"code","fece1dcf":"code","46021210":"code","8ae04374":"code","29277346":"code","e68c4f59":"code","5ab0ac4d":"code","191bf5b3":"code","90a623ac":"code","c593f8f1":"code","3439706e":"code","33daaad7":"code","1610a0ab":"code","faddf3ab":"code","f0f99514":"code","dc1267e7":"code","f6cd85f1":"code","d6503bd3":"code","a6cc48e7":"code","33d56eb8":"code","bb139b8a":"code","043b3469":"code","80702f7f":"code","2f441a43":"code","135a4403":"code","848d8598":"code","9e0f2f3b":"code","2f997830":"code","941b3788":"code","84eff1d1":"code","910f076e":"code","3c8c62d1":"code","60f3799a":"code","1b42b9b8":"code","27f058a7":"markdown","1fcb4bba":"markdown","6d970567":"markdown","a9efa450":"markdown","967e9fb0":"markdown","0d0c4278":"markdown","be7d2c9f":"markdown","2a086adc":"markdown","de3766f4":"markdown","f9d8f13d":"markdown"},"source":{"f3a62f28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","56174b49":"df = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf_o2 = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv')","516b57d3":"df.head()","b2c7ce2f":"df.columns","07c02b62":"#df = df.astype({'sex':'category', 'exng':'category', 'cp':'category', 'fbs':'category', 'restecg':'category', 'exng':'category', 'caa':'category', 'output':'category'})","5390d5d6":"display(df.info())\ndisplay(df.describe())","fece1dcf":"df.hist(figsize=(16,8))\nplt.show()","46021210":"plt.figure(figsize=(10, 10))\nsns.heatmap(df.corr(), annot=True)","8ae04374":"sns.countplot(df['output'])\nplt.show()","29277346":"sns.displot(x='age', hue='output', data=df, alpha=0.6)\nplt.show()","e68c4f59":"attack = df[df['output']==1]\nsns.displot(attack.age, kind='kde')\nplt.show()","5ab0ac4d":"sns.displot(attack.age, kind='ecdf')\nplt.grid(True)\nplt.show()","191bf5b3":"ranges = [0, 30, 40, 50, 60, 70, np.inf]\nlabels = ['0-30', '30-40', '40-50', '50-60', '60-70', '70+']\n\nattack['age'] = pd.cut(attack['age'], bins=ranges, labels=labels)\nattack['age'].head()","90a623ac":"sns.countplot(attack.age)\nplt.show()","c593f8f1":"fig, ax = plt.subplots(figsize=(8, 5))\nsns.countplot(x='sex', hue='age', data=attack, ax=ax)\nax.set_xticklabels(['Female', 'Male'])\nplt.legend(loc='upper left')\nplt.show()","3439706e":"attack = df[df['output'] == 1]\nsns.displot(x='age', kind='kde', hue='sex', data=attack)\nplt.grid(True)\nplt.show()","33daaad7":"sns.displot(x='age', kind='ecdf', hue='sex', data=attack)\nplt.grid(True)\nplt.show()","1610a0ab":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler","faddf3ab":"# creating a copy of df\ndf1 = df\nscaler = StandardScaler()\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df1, columns = cat_cols, drop_first = True)\n\nX = df1.drop(['output'],axis=1)\ny = df1[['output']]\n\ndf1[con_cols] = scaler.fit_transform(X[con_cols])\n\n# defining the features and target\nX = df1.drop(['output'],axis=1)\ny = df1[['output']]\n\n# scaling the continuous featuree\nprint(\"The first 5 rows of X are\")\nprint(X.head())","f0f99514":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\nprint(\"The shape of X_train is      \", X_train.shape)\nprint(\"The shape of X_test is       \",X_test.shape)\nprint(\"The shape of y_train is      \",y_train.shape)\nprint(\"The shape of y_test is       \",y_test.shape)","dc1267e7":"lr = LogisticRegression(random_state=42)\n\nknn = KNeighborsClassifier()\npara_knn = {'n_neighbors':np.arange(1, 50)}\n\ngrid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5)\n\ndt = DecisionTreeClassifier()\npara_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5)\n\nrf = RandomForestClassifier()\n\n# Define the dictionary 'params_rf'\nparams_rf = {\n    'n_estimators':[100, 350, 500],\n    'min_samples_leaf':[2, 10, 30]\n}\ngrid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)","f6cd85f1":"dt = DecisionTreeClassifier(criterion='gini', max_depth=9, min_samples_leaf=10, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=2, random_state=42)","d6503bd3":"# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt), ('Random Forest', rf)]","a6cc48e7":"# Iterate over the pre-defined list of classifiers\nfor clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_pred, y_test) \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))","33d56eb8":"classifiers = [('Logistic Regression', lr), ('Random Forest', rf), ('K Nearest Neighbours', knn)]","bb139b8a":"# Import VotingClassifier from sklearn.ensemble\nfrom sklearn.ensemble import VotingClassifier\n\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers, voting='soft')     \n\n# Fit vc to the training set\nvc.fit(X_train, y_train)   \n\n# Evaluate the test set predictions\ny_pred = vc.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_pred, y_test)\nprint('Voting Classifier: {:.3f}'.format(accuracy))","043b3469":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator=rf, n_estimators=100, random_state=1)\n\nada.fit(X_train, y_train)\n\ny_pred = ada.predict(X_test)\n\naccuracy_score(y_pred, y_test)","80702f7f":"# Create a pd.Series of features importances\nimportances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nplt.figure(figsize=(10, 10))\nimportances_sorted.plot(kind='barh', color='lightgreen')\nplt.title('Features Importances')\nplt.show()","2f441a43":"# Perform the necessary imports\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\n\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create a PCA instance: pca\npca = PCA()\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler, pca)\n\n# Fit the pipeline to 'samples'\npipeline.fit(X_train)\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features)\nplt.show()","135a4403":"# Import necessary modules\nfrom sklearn.metrics import roc_curve\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob_ada = ada.predict_proba(X_test)[:,1]\ny_pred_prob_lr = lr.predict_proba(X_test)[:,1]\ny_pred_prob_vc = vc.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_ada)\nfpr1, tpr1, thresholds = roc_curve(y_test, y_pred_prob_lr)\nfpr2, tpr2, thresholds = roc_curve(y_test, y_pred_prob_vc)\n# fpr: False Positive Rate\n# tpr: True Positive Rate\n\n# Plot ROC curve\nplt.figure(figsize=(10, 10))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='ADA', alpha=0.7)\nplt.plot(fpr1, tpr1, label='LR', alpha=0.7)\nplt.plot(fpr2, tpr2, label='VC', alpha=0.7)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()","848d8598":"samples = df.drop('output', axis=1)\nsamples.head()","9e0f2f3b":"# Perform the necessary imports\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\n\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create a PCA instance: pca\npca = PCA()\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler, pca)\n\n# Fit the pipeline to 'samples'\npipeline.fit(samples)\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features)\nplt.show()","2f997830":"scaled_samples = StandardScaler().fit_transform(samples)\n\n# Create a PCA model with 10 components: pca\npca = PCA(n_components=10)\n\n# Fit the PCA instance to the scaled samples\npca.fit(scaled_samples)\n\n# Transform the scaled samples: pca_features\npca_features = pca.transform(scaled_samples)\n\n# Print the shape of pca_features\nprint(pca_features.shape)","941b3788":"df2 = pd.DataFrame(pca_features)\ndf2.head()","84eff1d1":"from sklearn.cluster import KMeans\n\nks = range(1, 8)\ninertias = []\n\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k)\n    \n    # Fit model to samples\n    model.fit(pca_features)\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()","910f076e":"model2 = KMeans(n_clusters=2)\n\nmodel2.fit(df2)\n\nlabels = model2.predict(pca_features)","3c8c62d1":"sub = pd.DataFrame({'Labels':labels, 'Output':df.output})\npd.crosstab(sub.Output, sub.Labels)","60f3799a":"from sklearn.metrics import classification_report\nprint(classification_report(df.output, labels))","1b42b9b8":"fpr, tpr, thresholds = roc_curve(sub.Output, sub.Labels)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, alpha=0.7)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()","27f058a7":"#### ECDF reveals that 80% data lies below age 60 year old people. Which means that people with age greater than 60 have 20% of getting heart attack and people with age greater than 50 have 60% of getting a heart attack.","1fcb4bba":"## Classifier 2","6d970567":"grid_knn.fit(X_train, y_train)\ndisplay(grid_knn.best_params_)\n\ngrid_dt.fit(X_train, y_train)\ndisplay(grid_dt.best_params_)\n\ngrid_rf.fit(X_train, y_train)\ndisplay(grid_rf.best_params_)","a9efa450":"#### Males have high numbers here but it's because number of males are high in this dataset, we have to find out which gnder have highest chances of heart attack","967e9fb0":"#### we don't need smoothing here","0d0c4278":"#### Both genders have equal chances the only difference is males have high chances of getting heart attack at early age i.e. after age 30, and females chances of getting heart attack a bit late compared to males, but the chances of getting heart attack is high in females after age 70.","be7d2c9f":"## Basic EDA","2a086adc":"## About this dataset\n\n* Age : Age of the patient\n\n* Sex : Sex of the patient\n\n* exang: exercise induced angina (1 = yes; 0 = no)\n\n* ca: number of major vessels (0-3)\n\n* cp : Chest Pain type chest pain type\n\n* Value 1: typical angina\n* Value 2: atypical angina\n* Value 3: non-anginal pain\n* Value 4: asymptomatic\n* trtbps : resting blood pressure (in mm Hg)\n\n* chol : cholestoral in mg\/dl fetched via BMI sensor\n\n* fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n* rest_ecg : resting electrocardiographic results\n\n* Value 0: normal\n* Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n* Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n* thalach : maximum heart rate achieved\n\n* target : 0= less chance of heart attack 1= more chance of heart attack","de3766f4":"## Classifier 1","f9d8f13d":"#### Age do affect chances of getting heart attack. People with age between 40-60 gets heart attack most and getting a heart attack after age 60 is 20 percent."}}