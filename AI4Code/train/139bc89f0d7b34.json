{"cell_type":{"886b6c21":"code","0570302c":"code","20a676d1":"code","e57dd633":"code","4a812f2a":"code","4eefee8f":"code","2ec3eab8":"code","d8364766":"code","363c5374":"code","b591b6cf":"code","7721d021":"code","03529e80":"code","5bd11c74":"code","45e208b4":"code","de037985":"code","9a740e39":"code","ead8495b":"code","fb8efdc0":"code","669f3a4d":"code","02c4675d":"code","d1bea085":"code","edc9ac61":"code","1e08f4b3":"code","b13d2720":"code","cdd2d6af":"code","6a9547e5":"code","4145654a":"code","5f1ef124":"code","090a754a":"code","91500ef1":"code","ca4af5cf":"code","280c49a1":"code","bf4efe9e":"markdown","60bcf876":"markdown","43bf29de":"markdown","50ddc35c":"markdown","99706082":"markdown","057755d6":"markdown","810837f4":"markdown","6308968a":"markdown"},"source":{"886b6c21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0570302c":"df = pd.read_csv(\"\/kaggle\/input\/gameofthrones\/game-of-thrones.csv\", delimiter=',', encoding='utf8')\ndf.head()","20a676d1":"df.isnull().sum()","e57dd633":"import nltk \nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nimport re\nimport string\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB","4a812f2a":"##Code by Taha07  https:\/\/www.kaggle.com\/taha07\/data-scientists-jobs-analysis-visualization\/notebook\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'red',\n                      height =2000,\n                      width = 2000\n                     ).generate(str(df[\"Speaker\"]))\nplt.rcParams['figure.figsize'] = (12,12)\nplt.axis(\"off\")\nplt.imshow(wordcloud)\nplt.title(\"Game of Thrones Speakers\")\nplt.show()","4eefee8f":"##Code by Taha07  https:\/\/www.kaggle.com\/taha07\/data-scientists-jobs-analysis-visualization\/notebook\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'blue',\n                      height =2000,\n                      width = 2000\n                     ).generate(str(df[\"Episode\"]))\nplt.rcParams['figure.figsize'] = (12,12)\nplt.axis(\"off\")\nplt.imshow(wordcloud)\nplt.title(\"Game of Thrones Episodes\")\nplt.show()","2ec3eab8":"#Code by Andrea Sindico  https:\/\/www.kaggle.com\/asindico\/love-for-poems\/notebook\n\ndef removePunctuation(x):\n    x = x.lower()\n    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n    x = x.replace('\\r','')\n    x = x.replace('\\n','')\n    x = x.replace('  ','')\n    x = x.replace('\\'','')\n    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)","d8364766":"#https:\/\/stackoverflow.com\/questions\/51534586\/add-and-remove-words-from-the-nltk-stopwords-list\n\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\n#add words that aren't in the NLTK stopwords list\nnew_stopwords = ['off', 'in', 'the', 'of']\nnew_stopwords_list = stop_words.union(new_stopwords)\n\n#remove words that are in NLTK stopwords list\nnot_stopwords = {'savages', ' wildlings', 'castel', 'Winterfell'} \nfinal_stop_words = set([word for word in new_stopwords_list if word not in not_stopwords])\n\nprint(final_stop_words)","363c5374":"#Code by Andrea Sindico  https:\/\/www.kaggle.com\/asindico\/love-for-poems\/notebook\n\ndef processText(x):\n    x= removePunctuation(x)\n    #x= removeStopwords(x)\n    return x","b591b6cf":"from nltk.tokenize import sent_tokenize, word_tokenize\nGoT = pd.Series([word_tokenize(processText(x)) for x in df['Text']])\nGoT.head(10)","7721d021":"#Code by Andrea Sindico  https:\/\/www.kaggle.com\/asindico\/love-for-poems\/notebook\n\nfrom gensim.models import word2vec\n#num_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\nmodel = word2vec.Word2Vec(GoT, workers=num_workers, #size=num_features,  was removed\n                          min_count = min_word_count,\n                          window = context, sample = downsampling)","03529e80":"from gensim import utils\nimport gensim\nimport logging\nfrom timeit import default_timer\nimport threading\nfrom six.moves import range\nfrom six import itervalues, string_types\nfrom gensim import matutils\nfrom numpy import float32 as REAL, ones, random, dtype\nfrom types import GeneratorType\nfrom gensim.utils import deprecated\nimport os\nimport copy","5bd11c74":"#I don't know where I found this snippet\n\ndef most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None):\n        \"\"\"Deprecated, use self.wv.most_similar() instead.\n        Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n        \"\"\"\n        return self.wv.most_similar(positive, negative, topn, restrict_vocab, indexer)","45e208b4":"model.wv.most_similar('winter')","de037985":"df[\"Speaker\"].value_counts()","9a740e39":"labels = 'TYRION', 'CERSEI', 'JON', 'JAIME', 'DAENERYS'\nsizes = [337, 149, 136, 79, 56]  #must have same number labels, sizes and explode\nexplode = (0, 0.2, 0, 0, 0)  # only \"explode\" the 2nd slice \n\nfig1, ax1 = plt.subplots(figsize=(10,10))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","ead8495b":"#Since most similar is now in KeyedVectors I copied the snippet below\n#https:\/\/github.com\/RaRe-Technologies\/gensim\/blob\/develop\/gensim\/models\/keyedvectors.py\n\nfrom gensim import models\nfrom gensim.models import KeyedVectors\n\n\nimport gensim.downloader as api\nword_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data \n# Check the \"most similar words\", using the default \"cosine similarity\" measure.\nresult = word_vectors.most_similar(positive=['dragon', 'queen'], negative=['winter'])\n\nmost_similar_key, similarity = result[0]  # look at the first match\nprint(f\"{most_similar_key}: {similarity:.4f}\")","fb8efdc0":"#https:\/\/github.com\/RaRe-Technologies\/gensim\/blob\/develop\/gensim\/models\/keyedvectors.py\n\nresult = word_vectors.most_similar(positive=['castle', 'north'], negative=['crown'])\n\nmost_similar_key, similarity = result[0]  # look at the first match\nprint(f\"{most_similar_key}: {similarity:.4f}\")","669f3a4d":"#https:\/\/github.com\/RaRe-Technologies\/gensim\/blob\/develop\/gensim\/models\/keyedvectors.py\n\nresult = word_vectors.most_similar(positive=['wall', 'ship'], negative=['savages'])\n\nmost_similar_key, similarity = result[0]  # look at the first match\nprint(f\"{most_similar_key}: {similarity:.4f}\")","02c4675d":"#33187th row, 1st Column Text\n\ndf.iloc[33187,0]","d1bea085":"#33187th row, 1st Column Text\n\ndf.iloc[33189,0]","edc9ac61":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision\nimport re","1e08f4b3":"doc_1 = \"ARYA stands just behind the wolf figurehead of her ship. She looks out at the unknown seas ahead, and she smiles.\"\ndoc_2 = \"JON and TORMUND are mounted behind the gate that leads from Castle Black to the tunnel through the Wall. The Gate rises before them.\"","b13d2720":"words_doc1 = {'stands', 'just', 'behind', 'the', 'wolf', 'figurehead', 'of', 'her', 'ship', 'she', 'looks', 'out', 'at', 'unknown', 'seas', 'ahead', 'and', 'smiles' }\nwords_doc2 = {'and', 'are', 'mounted', 'behind', 'the', 'gate', 'that', 'leads', 'from', 'Castle', 'Black', 'to', 'tunnel', 'through', 'Wall', 'Gate', 'rises', 'before', 'them'}","cdd2d6af":"#Code by https:\/\/studymachinelearning.com\/jaccard-similarity-text-similarity-metric-in-nlp\/\n\ndef Jaccard_Similarity(doc1, doc2): \n    \n    # List the unique words in a document\n    words_doc1 = set(doc1.lower().split()) \n    words_doc2 = set(doc2.lower().split())\n    \n    # Find the intersection of words list of doc1 & doc2\n    intersection = words_doc1.intersection(words_doc2)\n\n    # Find the union of words list of doc1 & doc2\n    union = words_doc1.union(words_doc2)\n        \n    # Calculate Jaccard similarity score \n    # using length of intersection set divided by length of union set\n    return float(len(intersection)) \/ len(union)","6a9547e5":"doc_1 = \"ARYA stands just behind the wolf figurehead of her ship. She looks out at the unknown seas ahead, and she smiles.\"\ndoc_2 = \"JON and TORMUND are mounted behind the gate that leads from Castle Black to the tunnel through the Wall. The Gate rises before them.\"\n\nJaccard_Similarity(doc_1,doc_2)","4145654a":"#Code by Dexter https:\/\/www.kaggle.com\/soul9862\/the-movies-recommend-analysis-cosine-similarity\/notebook\n\ntfidf = TfidfVectorizer(stop_words='english')\ntfidf_matrix = tfidf.fit_transform(df['Text'])\nprint(tfidf_matrix.shape)","5f1ef124":"#Code by Dexter https:\/\/www.kaggle.com\/soul9862\/the-movies-recommend-analysis-cosine-similarity\/notebook\n\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","090a754a":"indices = pd.Series(df.index, index=df['Speaker']).drop_duplicates()\nprint(indices)","91500ef1":"idx = indices['CERSEI']\nprint(idx)","ca4af5cf":"#Code by Dexter https:\/\/www.kaggle.com\/soul9862\/the-movies-recommend-analysis-cosine-similarity\/notebook\n\ndef get_recommendations(title, cosine_sim=cosine_sim):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:11]\n    movie_indices = [i[0] for i in sim_scores]\n\n    return df['Speaker'].iloc[movie_indices]","280c49a1":"get_recommendations('SEPTA MORDANE')","bf4efe9e":"#Cosine Similarity.","60bcf876":"#Get the set of unique words for each document","43bf29de":"![](https:\/\/www.gameofthronesquote.com\/images\/arya-stark\/septa-mordane-used-to-crack-my-knuckles-cause-i-8ug.jpg)gameofthronesquote.com","50ddc35c":"![](https:\/\/images-americanas.b2w.io\/produtos\/2192369944\/imagens\/game-of-thrones-music-from-the-hbo-series-season-6-original-tv-soundtrack\/2192369944_1_large.jpg)americanas.com.br","99706082":"<iframe width=\"713\" height=\"401\" src=\"https:\/\/www.youtube.com\/embed\/pGe6hhmPV1w\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>","057755d6":"#The Jaccard similarity between doc_1 and doc_2 is 0.08333333333333333 .","810837f4":"#Let's try some Jaccard Similarity","6308968a":"#To avoid error below\n\n\"ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\"\n\nInstead of writting just one name of the characters I wrote the last one from input 15, since it's different from the others. On the snippet below get_recommendations.\n\nSepta Mordane has only one line."}}