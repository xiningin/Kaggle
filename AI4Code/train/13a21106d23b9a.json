{"cell_type":{"610af2b2":"code","80974e13":"code","38cbc04a":"code","d3a4e9b8":"code","b749ead2":"code","c52c9e1d":"code","a56ad276":"code","70928cde":"code","44874386":"code","5de4ddfe":"code","5bf41eea":"code","78bcc64d":"code","d60d2997":"code","d6ecb3fe":"code","1172e5d5":"code","2dd6da5a":"code","8bf4fd15":"code","8b0d7a38":"code","61591d17":"markdown","5b566ae6":"markdown","d6d50ccc":"markdown","7c9a7bf9":"markdown","7239d13a":"markdown","d4c94c29":"markdown","90dbf8d7":"markdown","9177b1f0":"markdown","0efb4990":"markdown"},"source":{"610af2b2":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom tqdm import tqdm\nimport random\n\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\npath_root = '..\/input\/optiver-realized-volatility-prediction'\npath_data = '..\/input\/optiver-realized-volatility-prediction'\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}\n\nDEBUG = False","80974e13":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):\n    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n    \n    #Book features\n    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet\/stock_id={}\/'.format(dataType, stock_id)))\n    df_book['stock_id'] = stock_id\n    cols = key + [col for col in df_book.columns if col not in key]\n    df_book = df_book[cols]\n    \n    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] +\n                                    df_book['ask_price1'] * df_book['bid_size1']) \/ (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] +\n                                    df_book['ask_price2'] * df_book['bid_size2']) \/ (df_book['bid_size2'] + df_book['ask_size2'])\n    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return).fillna(0)\n    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return).fillna(0)\n    \n    features_to_apply_realized_volatility = ['log_return'+str(i+1) for i in range(2)]\n    stock_stat = df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_realized_volatility]\\\n                        .agg(realized_volatility).reset_index()\n\n    #Trade features\n    trade_stat =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet\/stock_id={}'.format(dataType, stock_id)))\n    trade_stat = trade_stat.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    trade_stat['stock_id'] = stock_id\n    cols = key + [col for col in trade_stat.columns if col not in key]\n    trade_stat = trade_stat[cols]\n    trade_stat['trade_log_return1'] = trade_stat.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n    trade_stat = trade_stat.groupby(by = ['stock_id', 'time_id'])[['trade_log_return1']]\\\n                           .agg(realized_volatility).reset_index()\n    #Joining book and trade features\n    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n    \n    return stock_stat\n\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n    \n    if DEBUG and dataType == 'train':\n        stock_ids = stock_ids[:10]\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n","38cbc04a":"train = pd.read_csv(os.path.join(path_data, 'train.csv'))\n%time train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\ntrain = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\nprint('Train shape: {}'.format(train.shape))\ndisplay(train.head(2))\n\ntest = pd.read_csv(os.path.join(path_data, 'test.csv'))\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\nprint('Test shape: {}'.format(test.shape))\ndisplay(test.head())","d3a4e9b8":"scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n\ntrain_scaled = scaler.fit_transform(train.iloc[:, 3:])\ntrain_ = train.join(pd.DataFrame(train_scaled), how='left')\n\ntest_scaled = scaler.transform(test.iloc[:, 3:])\ntest_ = test.join(pd.DataFrame(test_scaled), how='left')\n\n\ntrain, test = train_, test_\ntrain.shape, test.shape","b749ead2":"## import libraries\n\n#PyTorch \n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils import data","c52c9e1d":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","a56ad276":"class OptiveDataset(Dataset):\n    def __init__(self, X, Y, emb_cols=['stock_id', 'time_id']):\n        X = X.copy()\n        self.X1 = X.loc[:,emb_cols].copy().values.astype(np.int64) #categorical columns\n        self.X2 = X.drop(columns=emb_cols).copy().values.astype(np.float32) #numerical columns\n        self.y = Y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return (self.X1[idx], self.X2[idx]), self.y[idx]\n    \nclass OptiveDatasetTest(Dataset):\n    def __init__(self, X, emb_cols=['stock_id', 'time_id']):\n        X = X.copy()\n        self.X1 = X.loc[:,emb_cols].copy().values.astype(np.int64) #categorical columns\n        self.X2 = X.drop(columns=emb_cols).copy().values.astype(np.float32) #numerical columns\n        \n    def __len__(self):\n        return len(self.X1)\n    \n    def __getitem__(self, idx):\n        return (self.X1[idx], self.X2[idx])","70928cde":"train_dataset = OptiveDataset(train.drop(['target', 'time_id'], axis=1), train['target'], emb_cols=['stock_id'])\ntrain_dl = DataLoader(train_dataset, batch_size=4, shuffle=True)\n\n#test the dataset class\nfor (emb, count), target in train_dl:\n    print((emb.shape, count.shape), target.shape)\n    break;","44874386":"# from https:\/\/discuss.pytorch.org\/t\/implementation-of-swish-a-self-gated-activation-function\/8813\/2\ndef swish(x):\n    return x * torch.sigmoid(x)","5de4ddfe":"class OptiverModel(nn.Module):\n    def __init__(self, embedding_sizes=16, num_embeddings=max(train['stock_id'])+1):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, embedding_sizes)\n        self.emb_drop = nn.Dropout(0.25)\n        \n        self.bn1 = nn.BatchNorm1d(6)\n        self.lin1 = nn.Linear(embedding_sizes+6, 32)\n        self.lin2 = nn.Linear(32, 128)\n        self.lin3 = nn.Linear(128, 64)\n        self.lin4 = nn.Linear(64, 32)\n        self.lin5 = nn.Linear(32, 1)    \n\n    def forward(self, x_cat, x_cont):\n        x1 = self.emb(x_cat)\n        x1 = torch.flatten(x1, end_dim=1)\n        #x1 = self.emb_drop(x1)\n        \n        x2 = self.bn1(x_cont)\n\n        x = torch.cat([x1, x2], 1)\n        x = swish(self.lin1(x))\n        x = swish(self.lin2(x))\n        x = swish(self.lin3(x))\n        x = swish(self.lin4(x))\n        x = self.lin5(x)\n        #x = torch.sigmoid(x)\n        \n        \n        return x","5bf41eea":"model = OptiverModel(embedding_sizes=24,)\n#emb.shape, count.shape\nout = model(emb, count)\n\nprint(out, target)\n#model","78bcc64d":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef RMSELoss(yhat,y):\n    return torch.sqrt(torch.mean((yhat-y)**2))\n\ndef RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) \/ y_true) ** 2 ))\n\ndef train_epoch(train_dl, valid_dl, model, loss_fn, opt, sch, epoch, fold, device=device):\n    # taining loop\n    model.train()\n    running_loss_ = 0\n    \n    pbar = tqdm(enumerate(train_dl), total=len(train_dl))\n    for i, ((cats, counts), targets) in pbar:\n        cats, counts, targets = cats.to(device), counts.to(device), targets.unsqueeze(1).to(device)\n        \n        opt.zero_grad()\n        y_pred = model(cats, counts)\n        loss = loss_fn(y_pred.float(), targets.float())\n        \n        loss.backward()\n        opt.step()\n        \n        running_loss_ += loss.item()\n        if (i+1) % 100 == 0:\n            pbar.set_description(f\"running loss:{running_loss_ \/ (i+1): 0.6f}\")\n    \n    sch.step(loss)\n\n    epoch_loss = running_loss_ \/ len(train_dl)\n    #print(f'==> Epoch {epoch} TRAIN loss: {epoch_loss:.6f}')\n    \n    # Validation loop\n    model.eval()\n    valid_loss = 0\n    best_loss = np.inf\n    \n    for i, ((cats, counts), targets) in enumerate(valid_dl):\n        cats, counts, targets = cats.to(device), counts.to(device), targets.unsqueeze(1).to(device)\n        \n        with torch.no_grad():\n            y_pred = model(cats, counts)\n            val_loss = loss_fn(y_pred.float(), targets.float())\n            \n        valid_loss += val_loss.item() * targets.shape[0]\n    sch.step(valid_loss)\n    \n    valid_epoch_loss = valid_loss \/ len(valid_dl)\n    print(f'==>FOLD:{fold}, Epoch {epoch} VALID loss: {valid_epoch_loss:.8f}')\n    \n    #if valid_epoch_loss < best_loss:\n    #    best_loss = valid_epoch_loss\n    #    torch.save(model.state_dict(), f'FOLD{fold}_optive_model.pth')\n    \n    model.train()\n    return model, epoch_loss, valid_epoch_loss","d60d2997":"def perpare_dataset(train, valid, test=None, batch_size=128, drop_cols=['target', 'time_id'], emb_cols=['stock_id']):\n    train_dataset = OptiveDataset(train.drop(drop_cols, axis=1), train['target'], emb_cols=emb_cols)\n    valid_dataset = OptiveDataset(valid.drop(drop_cols, axis=1), valid['target'], emb_cols=emb_cols)    \n    \n    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n    \n    return train_dl, valid_dl","d6ecb3fe":"n_folds = 5\nepochs = 25\n\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2021)\nseed_everything(42)\n\nfor fold_idx, (dev_index, val_index) in enumerate(kf.split(range(len(train)))):\n        \n    train_ = train.loc[dev_index,].reset_index(drop=True)\n    valid_ = train.loc[val_index, ].reset_index(drop=True)\n    \n    train_dl, valid_dl = perpare_dataset(train_, valid_)\n    \n    model = OptiverModel(embedding_sizes=24,).to(device)\n    #loss_fn = nn.MSELoss().to(device)\n    loss_fn = RMSELoss\n    \n    opt = optim.Adam(model.parameters(), lr=0.001)\n    sch = optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.2, patience=4)\n    \n    bst_loss = np.inf\n    counter = 0\n    for epoch in range(epochs):\n        model, epoch_loss, valid_epoch_loss = train_epoch(train_dl, valid_dl, \n                                                                   model, loss_fn, opt, \n                                                                   sch, epoch, fold_idx, device=device)\n        \n        # simple early stop\n        if bst_loss < valid_epoch_loss:\n            counter += 1\n        else:\n            bst_loss = valid_epoch_loss\n            bst_epoch = epoch\n            counter = 0\n            torch.save(model.state_dict(), f'FOLD{fold_idx}_optive_model.pth')\n\n        \n        # break after 5 epochs\n        if counter > 7:\n            break\n        \n    print(f'FOLD: {fold_idx}, BEST EPOCH: {bst_epoch}, BEST LOSS: {bst_loss}')","1172e5d5":"test_dataset = OptiveDatasetTest(test.drop(['row_id', 'time_id'], axis=1), emb_cols=['stock_id'])\ntest_dl = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\ntest_preds = []\nmodel_paths = glob.glob('.\/*.pth')\n\nfor model_path in model_paths:\n    #model_path = '.\/FOLD0_optive_model.pth'\n    model.load_state_dict(torch.load(model_path))\n    model.to(torch.device('cpu'))\n    model.eval()\n\n    y_preds = []\n    with torch.no_grad():\n        for x_cat, x_cont in test_dl:\n            y_preds += [model(x_cat, x_cont).detach().cpu().numpy()[0][0]]\n    test_preds.append(y_preds)\n    \ny_preds = np.mean(test_preds, axis=0)","2dd6da5a":"y_preds","8bf4fd15":"test__ = test.copy()\ntest__['target'] = y_preds\ntest__[['row_id', 'target']].to_csv('submission.csv',index = False)","8b0d7a38":"test__[['row_id', 'target']]","61591d17":"# Test and prediction","5b566ae6":"<center><h3><span style='color:red'>UPVOTE<\/span> if you find it interesting<\/h3><hr>\nNotebook still under modification <\/center>","d6d50ccc":"## Torch DATASET","7c9a7bf9":"<center><h3 style='color:red'>Optiver Realized Volatility PyTorch Baseline<\/h3><br>KASSEM@ELCAISERI<HR><\/center>","7239d13a":"In this notebook:\n* A simple PyTorch NN starter using stock Embedding.\n\n\nCredits to:\n* https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data\n* https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/250324\n* https:\/\/www.kaggle.com\/lucasmorin\/tf-keras-nn-with-stock-embedding\n\n**I hope it will be useful for beginners. By creating new variables you can easily improve this model.**\n\n\n## updates:\n**V4**\n* use RMSELoss as loss fuction\n* add scheduler\n* test the results.\n\n**V6**\n* change model hyperparmeters {emb_size=29, emb_drop_out=0.25}\n* Train for 5 folds && Test and submit for the 5 folds\n\n**V8**\n* Implement of 'SWISH' : a self-gated activation function\n* join main DF with Data Normalize","d4c94c29":"## Model","90dbf8d7":"## Data Normailze","9177b1f0":"## Train and test datasets","0efb4990":"## Apply Torch"}}