{"cell_type":{"7e837375":"code","b05d1170":"code","e253730c":"code","d19410a6":"code","dd1b7c5d":"code","3c24277c":"code","44b252e7":"code","70ad4540":"code","f0170b05":"code","bf8c85e6":"code","5d99df6c":"code","256711e1":"code","17aaa20b":"code","0fdf729e":"code","7bfe78c2":"code","0f57c022":"code","a2148070":"code","6c882af4":"code","d2819635":"code","2b79e98f":"code","d60dd550":"code","47cd403c":"code","1c7042ef":"code","352420d2":"code","2ff130a7":"code","8c6503e8":"code","205247fc":"code","02f0dd57":"markdown","7f7c15f5":"markdown","23e60fb9":"markdown","8c7355a7":"markdown","2c333afb":"markdown","b7b5d524":"markdown","2e9a23c1":"markdown","ea41f88b":"markdown","fcb6bc9e":"markdown"},"source":{"7e837375":"!pip install rudalle==0.4.0 > \/dev\/null\n!pip install bitsandbytes-cuda110==0.25.0 > \/dev\/null\n!pip install timm==0.4.12 > \/dev\/null","b05d1170":"import multiprocessing\nimport torch\nfrom psutil import virtual_memory\n\nram_gb = round(virtual_memory().total \/ 1024**3, 1)\n\nprint('CPU:', multiprocessing.cpu_count())\nprint('RAM GB:', ram_gb)\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"cuDNN version:\", torch.backends.cudnn.version())\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device.type)\n\n!nvidia-smi","e253730c":"# PARAMS:\nN_EPOCHS = 40\nEARLY_STOP = True\nMAX_LR = 1e-5\nBATCH_SIZE = 1 # used 2 in my experiments \nLOSS_IMG_WEIGHT = 10**3\nSAVE_EVERY = 2000\nFINAL_DIV_FACTOR = 500","d19410a6":"import os\nimport PIL\nimport random\nimport torchvision\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport bitsandbytes as bnb\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom rudalle.pipelines import generate_images, show, super_resolution, cherry_pick_by_clip, convert_emoji_to_rgba, show_rgba\nfrom rudalle import get_rudalle_model, get_tokenizer, get_vae, get_realesrgan, get_ruclip, get_emojich_unet, utils\nfrom rudalle.utils import seed_everything\nseed_everything(42)","dd1b7c5d":"def merge_pil_images(pil_images, nrow=16):\n    merged_images = [pil_image.resize((128, 128)) for pil_image in pil_images]\n    merged_images = utils.pil_list_to_torch_tensors(merged_images)\n    merged_images = torchvision.utils.make_grid(merged_images, nrow=nrow)\n    merged_images = torchvision.transforms.functional.to_pil_image(merged_images.detach())\n    return merged_images\n\n\nclass EmojiDataset(Dataset):\n\n    def __init__(self, df, data_dir, tokenizer, text_seq_length=128, scale_ratio=1.0):       \n        self.data_dir = data_dir\n        self.text_seq_length = text_seq_length\n        self.tokenizer = tokenizer\n        self.image_size = 256\n        self.samples = []\n        self.image_transform = T.Compose([\n            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n            T.RandomResizedCrop(self.image_size, scale=(scale_ratio, 1.), ratio=(1.0, 1.)),\n            T.ToTensor()\n        ])\n        self.texts = df['text'].values\n        self.image_ids = df.index.values\n\n    def __len__(self):\n        return self.texts.shape[0]\n\n    def __getitem__(self, idx):\n        image_id, text = self.image_ids[idx], self.texts[idx]\n        text = text.lower()\n        image = PIL.Image.open(f'{self.data_dir}\/{image_id}.png')\n        image = self.image_transform(image)\n        text = self.tokenizer.encode_text(text, text_seq_length=self.text_seq_length)\n        return text, image","3c24277c":"df = pd.read_csv('..\/input\/russian-emoji\/marking.csv', index_col='image_id')\ndf['text'] = df['text'].str.lower()\ndf['text_length'] = df['text'].apply(len)\ndf['word_count'] = df['text'].apply(lambda x: len(x.split()))\ndf.head()","44b252e7":"tokenizer = get_tokenizer()\ntrain_dataset = EmojiDataset(df, data_dir='..\/input\/russian-emoji\/images', tokenizer=tokenizer)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)","70ad4540":"idx = random.randint(0, len(train_dataset)-1)\nencoded, image = train_dataset[idx]\n\nprint(tokenizer.decode_text(encoded))\n\nplt.imshow(image.permute(1,2,0).cpu().numpy());","f0170b05":"wc, c = WordCloud(), Counter()\nfor text in df['text']:\n    c.update(wc.process_text(text))    \nwc.fit_words(c)\nplt.figure(figsize=(7,7));\nplt.imshow(wc, interpolation='bilinear');\nplt.axis(\"off\");","bf8c85e6":"wc, c = WordCloud(), Counter()\nfor text in df['name']:\n    c.update(wc.process_text(text))    \nwc.fit_words(c)\nplt.figure(figsize=(7,7));\nplt.imshow(wc, interpolation='bilinear');\nplt.axis(\"off\");","5d99df6c":"text_value_counts = pd.DataFrame(df['text'].value_counts())\nax = sns.histplot(data=text_value_counts, x=\"text\");\nax.set_title('Duplicated text count histogram');\nax.set_xlabel('duplicates count');","256711e1":"pil_images = []\ntexts = text_value_counts[text_value_counts['text'] == 2].index.values\nfor text in texts[:4]:\n    image_ids = df[df['text'] == text].index\n    pil_images += [Image.open(f'..\/input\/russian-emoji\/images\/{image_id}.png') for image_id in image_ids]\nmerge_pil_images(pil_images, 2)","17aaa20b":"pil_images = []\ntexts = text_value_counts[text_value_counts['text'] == 6].index.values\nfor text in texts[25:35]:\n    image_ids = df[df['text'] == text].index\n    pil_images += [Image.open(f'..\/input\/russian-emoji\/images\/{image_id}.png') for image_id in image_ids]\nmerge_pil_images(pil_images, 6)","0fdf729e":"g = sns.displot(df, x=\"text_length\", col=\"group\",\n    binwidth=2, height=4, aspect=1, facet_kws=dict(margin_titles=True),\n    col_wrap=3, kde=True);","7bfe78c2":"g = sns.displot(df, x=\"word_count\", col=\"group\",\n    binwidth=0.5, height=4, aspect=1, facet_kws=dict(margin_titles=True),\n    col_wrap=3, kde=True);","0f57c022":"def train(model, vae, optimizer, scheduler, train_loader, n_epochs, save_every=2000):\n    os.makedirs('\/kaggle\/working\/saved_models', exist_ok=True)\n    model.train()\n    vae.eval()\n    device = model.get_param('device')\n    loss_logs = []\n    progress = tqdm(total=len(train_loader), desc='finetuning goes brrr')\n    save_counter = 0\n    for epoch in range(n_epochs):\n        for encoded, images in train_loader:\n            bs = images.shape[0]\n            save_counter+=1\n            if EARLY_STOP and save_counter > 100:\n                print('Stopped early')\n                return\n            model.zero_grad()\n            optimizer.zero_grad()\n            attention_mask = torch.tril(torch.ones((bs, 1, 1152, 1152), device=device))\n            with torch.no_grad():\n                codebooks = vae.get_codebook_indices(images.to(device))\n            input_ids = torch.cat((encoded.to(device), codebooks.long()), dim=1)\n            loss, loss_values = model.forward(input_ids, attention_mask, return_loss=True)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()     \n            if save_counter % save_every == 0:\n                print(f'Saveing checkpoint here Emojich_{save_counter}.pt')\n                plt.plot(loss_logs)\n                plt.show()\n                torch.save(model.state_dict(), os.path.join('\/kaggle\/working\/saved_models', f\"Emojich_{save_counter}.pt\"))\n            loss_logs += [loss.detach().item()]\n            progress.update()\n            progress.set_postfix({\"loss\": loss.item()})\n    plt.plot(loss_logs)\n    plt.show()\n    torch.save(model.state_dict(), os.path.join('\/kaggle\/working\/saved_models', \"Emojich_last.pt\"))","a2148070":"def freeze(\n    model,\n    freeze_emb=False,\n    freeze_ln=False,\n    freeze_attn=True,\n    freeze_ff=True,\n    freeze_other=False,\n):\n    for name, p in model.module.named_parameters():\n        name = name.lower()\n        if 'ln' in name or 'norm' in name:\n            p.requires_grad = not freeze_ln\n        elif 'embeddings' in name:\n            p.requires_grad = not freeze_emb\n        elif 'mlp' in name:\n            p.requires_grad = not freeze_ff\n        elif 'attn' in name:\n            p.requires_grad = not freeze_attn\n        else:\n            p.requires_grad = not freeze_other\n    return model","6c882af4":"device = 'cuda'\nmodel = get_rudalle_model('Malevich', pretrained=True, fp16=True, device=device, loss_img_weight=LOSS_IMG_WEIGHT, mlp_activation='gelu')\nvae = get_vae().to(device)","d2819635":"model = freeze(model)\n\ntorch.cuda.empty_cache()\n\noptimizer = bnb.optim.Adam8bit(model.parameters(), lr=MAX_LR)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=MAX_LR,\n    steps_per_epoch=len(train_loader), \n    epochs=N_EPOCHS,\n    final_div_factor=FINAL_DIV_FACTOR,\n)\n\ntrain(model, vae, optimizer, scheduler, train_loader, n_epochs=N_EPOCHS, save_every=SAVE_EVERY)","2b79e98f":"import gc\ndef _optimizer_to(optimizer, device):\n    for param in optimizer.state.values():\n        if isinstance(param, torch.Tensor):\n            param.data = param.data.to(device)\n            if param._grad is not None:\n                param._grad.data = param._grad.data.to(device)\n        elif isinstance(param, dict):\n            for subparam in param.values():\n                if isinstance(subparam, torch.Tensor):\n                    subparam.data = subparam.data.to(device)\n                    if subparam._grad is not None:\n                        subparam._grad.data = subparam._grad.data.to(device)\n\ncpu_device = torch.device('cpu')\n_optimizer_to(optimizer, cpu_device)\ndel optimizer\ndel scheduler\ngc.collect()\ntorch.cuda.empty_cache()","d60dd550":"checkpoint = torch.load('..\/input\/emojich\/pytorch_model.bin', map_location='cpu')\nmodel.load_state_dict(checkpoint)","47cd403c":"pil_images = []\nfor text in [\n    '\u0437\u0438\u043c\u0430 \u0432 \u0433\u043e\u0440\u0430\u0445', # winter in the mountain\n    '\u0444\u043b\u0430\u0433 \u0437\u043e\u043c\u0431\u0438 \u0430\u043f\u043e\u043a\u0430\u043b\u0438\u043f\u0441\u0438\u0441\u0430', # zombie apocalypse flag\n    '\u0444\u043b\u0430\u0433 C\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', # Sberbank flag\n    '\u0445\u0440\u0430\u043c \u0412\u0430\u0441\u0438\u043b\u0438\u044f \u0411\u043b\u0430\u0436\u0435\u043d\u043d\u043e\u0433\u043e',  # St. basil 's cathedral\n    '\u0432\u0438\u0448\u043d\u0435\u0432\u0430\u044f \u0434\u0435\u0432\u044f\u0442\u043a\u0430',  # cherry lada 2109\n    '\u0414\u043e\u043d\u0430\u043b\u044c\u0434 \u0422\u0440\u0430\u043c\u043f \u0438\u0437 \u043b\u0435\u0433\u043e',  # Donald Trump from LEGO\n    '\u0447\u0435\u043b\u043e\u0432\u0435\u043a \u043a\u0443\u0448\u0430\u0435\u0442 \u044f\u0431\u043b\u043e\u043a\u043e',  # a human eats an apple\n    '\u0435\u0436\u0438\u043a \u0432 \u0433\u043e\u043b\u0443\u0431\u043e\u0439 \u0448\u0430\u043f\u043a\u0435',  # hedgehog in a blue hat\n    '\u0432\u043e\u043b\u043a \u0432 \u043e\u0432\u0435\u0447\u044c\u0435\u0439 \u0448\u043a\u0443\u0440\u0435',  # a wolf in sheep's clothing\n    '\u043a\u0440\u043e\u043b\u0438\u043a \u0441\u0438\u043d\u0435\u0433\u043e \u0446\u0432\u0435\u0442\u0430',  # blue rabbit\n    '\u0440\u043e\u0437\u043e\u0432\u0430\u044f \u0430\u043b\u044c\u043f\u0430\u043a\u0430 \u0443\u043b\u044b\u0431\u0430\u0435\u0442\u0441\u044f',  # pink alpaca smiles\n    '\u0430\u0440\u0444\u0430 \u0432 \u0444\u043e\u0440\u043c\u0435 \u0443\u043b\u0438\u0442\u043a\u0438',  # a snail-shaped harp\n]:\n    seed_everything(42)\n    for top_k, top_p, images_num in [\n        (2048, 0.995, 16),\n    ]:\n        pil_images += generate_images(text, tokenizer, model, vae, top_k=top_k, images_num=images_num, top_p=top_p, bs=8)[0]","1c7042ef":"merge_pil_images(pil_images, 16)","352420d2":"model.to(cpu_device)\nvae.to(cpu_device)\ndel model\ndel vae\ngc.collect()\ntorch.cuda.empty_cache()","2ff130a7":"device = 'cuda'\nrealesrgan = get_realesrgan('x2', device=device)\nemojich_unet = get_emojich_unet('unet_effnetb5').to(device)","8c6503e8":"sr_images = super_resolution(pil_images, realesrgan)\nrgba_images, _ = convert_emoji_to_rgba(sr_images, emojich_unet,  device=device)","205247fc":"for i in range(12):\n    show_rgba(rgba_images[i*16+1])","02f0dd57":"Many samples with different skin color \\\nExamples of emoji with 6 duplicated text:","7f7c15f5":"In Russian language these entities have different concept, but have equal text \\\nExamples of emoji with 2 duplicated text:","23e60fb9":"# [Telegram Stickers](https:\/\/telegram.me\/stickers)\n\nPreparing emojis for [Telegram Stickers](https:\/\/telegram.me\/stickers) format (512x512, RGBA) using Unet - model was trained on pseudo-labeled emojis generated with \"[Emojich](https:\/\/huggingface.co\/sberbank-ai\/rudalle-Emojich)\" ","8c7355a7":"# Train [Emojich](https:\/\/huggingface.co\/sberbank-ai\/rudalle-Emojich)","2c333afb":"# [Emoji Dataset](https:\/\/www.kaggle.com\/shonenkov\/russian-emoji)","b7b5d524":"text length distribution by emoji group:","2e9a23c1":"word count distribution by emoji group:","ea41f88b":"# Generation with [Emojich](https:\/\/huggingface.co\/sberbank-ai\/rudalle-Emojich)","fcb6bc9e":"# Emoji ruDALL-E\n\n![](https:\/\/huggingface.co\/sberbank-ai\/rudalle-Emojich\/resolve\/main\/pics\/emojich_rgba_100.png)\n\nModel was trained by [Sber AI](https:\/\/github.com\/sberbank-ai) using pretrained [ruDALL-E (XL) Malevich](https:\/\/www.kaggle.com\/shonenkov\/rudalle-example-generation)\n* Task: `text2image generation`\n* Num Parameters: `1.3 B`\n* Training Data Volume: `120 million text-image pairs` & [`2749 text-emoji pairs`](https:\/\/www.kaggle.com\/shonenkov\/russian-emoji)\n\n[![Telegram](https:\/\/img.shields.io\/badge\/Telegram-Stickers-blue?style=for-the-badge&logo=data:image\/svg%2bxml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtOS40MTcgMTUuMTgxLS4zOTcgNS41ODRjLjU2OCAwIC44MTQtLjI0NCAxLjEwOS0uNTM3bDIuNjYzLTIuNTQ1IDUuNTE4IDQuMDQxYzEuMDEyLjU2NCAxLjcyNS4yNjcgMS45OTgtLjkzMWwzLjYyMi0xNi45NzIuMDAxLS4wMDFjLjMyMS0xLjQ5Ni0uNTQxLTIuMDgxLTEuNTI3LTEuNzE0bC0yMS4yOSA4LjE1MWMtMS40NTMuNTY0LTEuNDMxIDEuMzc0LS4yNDcgMS43NDFsNS40NDMgMS42OTMgMTIuNjQzLTcuOTExYy41OTUtLjM5NCAxLjEzNi0uMTc2LjY5MS4yMTh6IiBmaWxsPSIjMDM5YmU1Ii8+PC9zdmc+)](https:\/\/telegram.me\/addstickers\/SberAI_ruDALLE)\n\nAuthors:\n\n+ [Alex Shonenkov](https:\/\/kaggle.com\/shonenkov)\n+ [Daria Bakshandaeva](https:\/\/kaggle.com\/dariabakshandaeva)\n+ [Denis Dimitrov](https:\/\/kaggle.com\/ddimitrov)"}}