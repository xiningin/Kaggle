{"cell_type":{"54d7a0c8":"code","6f3d0b26":"code","1f4d2718":"code","863d7ff1":"code","2a0ec1b6":"code","9474c16d":"code","c0db9f08":"code","b33989d3":"code","3369bdd0":"code","f36adf66":"code","b3075c18":"code","d2166bb5":"code","1eb0ffcb":"code","8d9fa34b":"code","5f30c48a":"code","2bd745eb":"code","de9972c6":"code","e0ef4889":"code","d7bcf9b8":"code","60e2f200":"code","d6372475":"code","2e0d922c":"code","524d6d68":"code","a13b397b":"code","e2cd41ae":"code","1a25ac28":"code","36d21d70":"code","0baa8792":"code","0f0cbec2":"code","998aa6e8":"code","14d22828":"code","76180fdd":"code","401a76fe":"code","5de639b4":"code","407a9414":"code","ae0ec56a":"code","8a6fe69e":"code","b578cd17":"code","193e8b90":"code","527b987a":"code","8dfbc743":"code","be33727f":"code","65ac62b8":"code","c63b7eb7":"code","a1793361":"code","a84c70ee":"code","4a157559":"code","c6c372d9":"code","890f6fa8":"code","b323fc6e":"code","a3910f75":"code","9c0c10da":"code","a450aa01":"code","72ae9b86":"code","afdca7d5":"code","ad54a658":"code","fe1eff4b":"code","e55f9b47":"code","a62a88cc":"code","7e9966e7":"code","8e88a871":"code","baea0559":"code","d6528883":"code","495b217a":"code","943aa2f1":"code","56ced116":"code","08f1589c":"code","ab0fc794":"code","1d4f8d31":"code","242c7044":"code","8f8ab8e8":"code","e11dc853":"code","dd550ad5":"code","c6500d72":"code","0c51804e":"code","6af10f41":"code","33db890d":"code","54318659":"code","7142407c":"code","7244c40f":"code","c1abbc97":"code","ad7d26ca":"code","d249683f":"code","074e3dc5":"code","cb2743c9":"code","0207ab6c":"code","2f1b27cd":"code","942ed3ab":"code","4de6cec1":"code","0917a4f2":"code","aff85653":"code","818eb4b7":"code","ef439bed":"code","7f5bb21d":"code","a69ca149":"code","338df5e5":"code","879405e1":"code","6cdf2c03":"code","2c86da1a":"markdown","9ad8fd37":"markdown","e7258e9f":"markdown","8509bb01":"markdown","00debc46":"markdown","db72444a":"markdown","e148ae62":"markdown","efda676f":"markdown","a784a9b1":"markdown","783ad4ac":"markdown","ef73a3f8":"markdown","f2d1ea7f":"markdown","b2f94a2d":"markdown","722ca4aa":"markdown","d7f7a827":"markdown","c601fc6e":"markdown","026f1bc2":"markdown","61475dc5":"markdown","219e3946":"markdown","3372699b":"markdown","0fd74b06":"markdown","ba07ab27":"markdown","1dff4d91":"markdown","74eea3d3":"markdown","ab7b52d7":"markdown","58eaa637":"markdown","02c84485":"markdown"},"source":{"54d7a0c8":"import pandas as pd\nimport pandas as pd\nimport numpy as np\nimport csv\n# Load libraries\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport nltk\n\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport re #regex applies a regular expression to a string and returns the matching substrings. \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport nltk \nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\nimport nltk.corpus\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import BlanklineTokenizer\nfrom nltk.tokenize import TweetTokenizer\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\n\nfrom nltk.tokenize import word_tokenize\nimport gensim\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pickle\nfrom multiprocessing import Pool\nimport seaborn as sns\nimport random\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import GridSearchCV\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nimport pickle\nimport re\nfrom collections import Counter\nfrom string import punctuation\n\nimport json\nfrom wordcloud import WordCloud\n\nimport plotly\n\nimport plotly.graph_objs as go \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ncf.go_offline()\n\nfrom IPython.display import IFrame\nimport folium\nfrom folium import plugins\nfrom folium.plugins import MarkerCluster, FastMarkerCluster, HeatMapWithTime\n\npd.set_option('display.max_colwidth', -1)\nplt.style.use('seaborn-white')\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport string\nimport re \nfrom sklearn.model_selection import train_test_split # function for splitting data to train and test sets\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import TweetTokenizer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom bs4 import BeautifulSoup\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nimport numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nplt.style.use('ggplot')\nimport re\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import (LSTM, \n                          Embedding, \n                          BatchNormalization,\n                          Dense, \n                          TimeDistributed, \n                          Dropout, \n                          Bidirectional,\n                          Flatten, \n                          GlobalMaxPool1D)\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import (\n    precision_score, \n    recall_score, \n    f1_score, \n    classification_report,\n    accuracy_score\n)\n","6f3d0b26":"#from google.colab import files\n#uploaded = files.upload()\n","1f4d2718":"train = pd.read_csv('..\/input\/m-dataset\/train (1).csv',encoding = \"ISO-8859-1\", engine='python', error_bad_lines=False)\ntest = pd.read_csv('..\/input\/m-dataset\/test (1).csv',encoding = \"ISO-8859-1\", engine='python', error_bad_lines=False)","863d7ff1":"#visualize the train dataframe where we have 2 columns : a column for the text and the other one for the label of each text , we have two labels ( 0 \u00e9)\ntrain","2a0ec1b6":"print(train.columns.tolist())\n#printing the column names of the dataframe","9474c16d":"train['SentimentText'] ","c0db9f08":"test","b33989d3":"# check the number of positive vs. negative tagged sentences\npositives = train['Sentiment'][train.Sentiment== 0]\nnegatives = train['Sentiment'][train.Sentiment == 1]\n\nprint('number of positve tagged sentences is:  {}'.format(len(positives)))\nprint('number of negative tagged sentences is: {}'.format(len(negatives)))\nprint('total length of the train is:            {}'.format(train.shape[0]))","3369bdd0":"# get unique label counts\ntrain.groupby('Sentiment').describe() #the describe function is very useful for descriptive statistic in python","f36adf66":"# get a word count per sentence column\ndef word_count(sentence):\n    return len(sentence.split())\n    \ntrain['word count'] = train['SentimentText'].apply(word_count)\ntrain.head(10)","b3075c18":"# Sort dataframe by word count column\ntrain = train.sort_values(by='word count',ascending=False)\n#We want ot visualize the dataframe sorted\ntrain.head(20)","d2166bb5":"# plot word count distribution for both positive and negative sentiments\nx = train['word count'][train.Sentiment == 1]\ny = train['word count'][train.Sentiment == 0]\nplt.figure(figsize=(12,6))\nplt.xlim(0,45)\nplt.xlabel('word count')\nplt.ylabel('frequency')\ng = plt.hist([x, y], color=['r','b'], alpha=0.5, label=['Positive','negative'])\nplt.legend(loc='upper right')","1eb0ffcb":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)","8d9fa34b":"train['length'] = train['SentimentText'].apply(length)","5f30c48a":"#Average length in the texts\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(train[train['Sentiment'] == 0]['length'], alpha = 0.6, bins=bins, label='Negative') \nplt.hist(train[train['Sentiment'] == 1]['length'], alpha = 0.8, bins=bins, label='Positive')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","2bd745eb":"# get most common words in training dataset\nall_words = [] #all_words is an empty list \nfor line in list(train['SentimentText']):\n    words = line.split()\n    for word in words:\n        all_words.append(word.lower())\n    \n    \nCounter(all_words).most_common(50)","de9972c6":"# plot word frequency distribution of the top words, we will plot the distribution of 50  Top words \nplt.figure(figsize=(12,5))\nplt.title('Top 50 most common words')\nplt.xticks(fontsize=13, rotation=90)\nfd = nltk.FreqDist(all_words)\nfd.plot(25,cumulative=False)\n\n# log-log plot\nword_counts = sorted(Counter(all_words).values(), reverse=True)\nplt.figure(figsize=(12,5))\nplt.loglog(word_counts, linestyle='-', linewidth=1.5)\nplt.ylabel(\"Freq\")\nplt.xlabel(\"Word Rank\")\nplt.title('log-log plot of words frequency')","e0ef4889":"print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.ItemID.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.ItemID.unique())))","d7bcf9b8":"train['Sentiment'].value_counts()  #the unique values of the labels , we have two labels 0 and 1 . ","60e2f200":"value = train['Sentiment'].value_counts() \nvalue","d6372475":"fig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\nplt.tight_layout()\n\ntrain.groupby('Sentiment').count()['ItemID'].plot(kind='pie', ax=axes[0], labels=['NEGATIVE', 'POSITIVE'])\nsns.countplot(x=train['Sentiment'], hue=train['Sentiment'], ax=axes[1])\n\naxes[0].set_ylabel('')\naxes[1].set_ylabel('')\naxes[1].set_xticklabels(['Negative', 'Positive'])\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Target Distribution in Training Set', fontsize=13)\naxes[1].set_title('Target Count in Training Set', fontsize=13)\n\nplt.show()\n","2e0d922c":"train","524d6d68":"# word_count\ntrain['word_count'] = train['SentimentText'].apply(lambda x: len(str(x).split()))\ntest['word_count'] = test['SentimentText'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ntrain['unique_word_count'] = train['SentimentText'].apply(lambda x: len(set(str(x).split())))\ntest['unique_word_count'] = test['SentimentText'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ntrain['stop_word_count'] = train['SentimentText'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest['stop_word_count'] = test['SentimentText'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# url_count\ntrain['url_count'] = train['SentimentText'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ntest['url_count'] = test['SentimentText'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ntrain['mean_word_length'] = train['SentimentText'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest['mean_word_length'] = test['SentimentText'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ntrain['char_count'] = train['SentimentText'].apply(lambda x: len(str(x)))\ntest['char_count'] = test['SentimentText'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ntrain['punctuation_count'] = train['SentimentText'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest['punctuation_count'] = test['SentimentText'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ntrain['hashtag_count'] = train['SentimentText'].apply(lambda x: len([c for c in str(x) if c == '#']))\ntest['hashtag_count'] = test['SentimentText'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ntrain['mention_count'] = train['SentimentText'].apply(lambda x: len([c for c in str(x) if c == '@']))\ntest['mention_count'] = test['SentimentText'].apply(lambda x: len([c for c in str(x) if c == '@']))\n","a13b397b":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\nTWEETS = train['Sentiment'] == 1\n\nfig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n\nfor i, feature in enumerate(METAFEATURES):\n    sns.distplot(train.loc[~TWEETS][feature], label='NEGATIVE', ax=axes[i][0], color='green')\n    sns.distplot(train.loc[TWEETS][feature], label='POSITIVE', ax=axes[i][0], color='red')\n\n    sns.distplot(train[feature], label='Training', ax=axes[i][1])\n    sns.distplot(test[feature], label='Test', ax=axes[i][1])\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis='x', labelsize=12)\n        axes[i][j].tick_params(axis='y', labelsize=12)\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n\nplt.show()","e2cd41ae":"# helper function to clean tweets\ndef processTweet(tweet):\n    # Remove HTML special entities (e.g. &amp;)\n    tweet = re.sub(r'\\&\\w*;', '', tweet)\n    #Convert @username to AT_USER\n    tweet = re.sub('@[^\\s]+','',tweet)\n    # Remove tickers\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # To lowercase\n    tweet = tweet.lower()\n    # Remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*\\\/\\w*', '', tweet)\n    # Remove hashtags\n    tweet = re.sub(r'#\\w*', '', tweet)\n    # Remove Punctuation and split 's, 't, 've with a space for filter\n    tweet = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', tweet)\n    #remove (@)\n    tweet = tweet.replace(\"@\",'')\n    #remove (;)\n    tweet = tweet.replace(\";\",'')\n    #remove (\u00bf\u00bf)\n    tweet = tweet.replace(\"\u00bf\",'')\n    #remove (\\\\)\n    tweet = tweet.replace(\"\\\\\",\" \")\n    tweet = tweet.replace(\"\u00f0\",\" \")\n    tweet = tweet.replace(\"\u00f8\u00b5\u00f8\",\" \")\n    tweet = tweet.replace(\"\u00f8\",\" \")\n    tweet = tweet.replace(\"\u00ee\u00b5\u00ee \u00ee\u00bd\u00ee\",\" \")\n    tweet = tweet.replace(\"\u00f8\u00b3\",\" \")\n    tweet = tweet.replace(\"\u00ef\",\" \")\n    tweet = tweet.replace(\"\u00ba\",\" \")\n    tweet = tweet.replace(\"\u00f8\u00b9\",\" \")\n    tweet = tweet.replace(\"\u00bd\",\" \")\n    tweet = tweet.replace(\"\u00e0\u00b9\",\" \")\n    tweet = tweet.replace(\"\u00be\",\" \")\n    tweet = tweet.replace(\"\u00e6\",\" \")\n    tweet = tweet.replace(\"\u00ec\",\" \")\n    tweet = tweet.replace(\"\u00b5\",\" \")\n    # Remove words with 2 or fewer letters\n    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n    # Remove whitespace (including new line characters)\n    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n    # Remove single space remaining at the front of the tweet.\n    tweet = tweet.lstrip(' ') \n    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n    tweet = ''.join(c for c in tweet if c <= '\\uFFFF') \n    return tweet\n    \n# ______________________________________________________________\n#this function is fore correcting the spelling of some common words\ndef decontract(text):\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    return text\n# clean dataframe's text column\ntrain['SentimentText'] = train['SentimentText'].apply(processTweet)\ntrain['SentimentText'] = train['SentimentText'].apply(decontract)  \n\n# preview some cleaned tweets\ntrain['SentimentText'].head()","1a25ac28":"test['SentimentText'] = test['SentimentText'].astype(str).apply(processTweet)\ntest['SentimentText'] = test['SentimentText'].astype(str).apply(decontract)","36d21d70":"train['SentimentText'][1] #there is no (;;;;;;;;) #it is removed by the function","0baa8792":"train.head(100)","0f0cbec2":"corpus=[]\n    \nfor x in train['SentimentText'].str.split():\n    for i in x:\n        corpus.append(i)","998aa6e8":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:30] \n    \n\n\nx,y=zip(*top)\nplt.rcParams[\"figure.figsize\"] = (20,10)\n#There is also this workaround in case you want to change the size without using the figure environment.\n#So in case you are using plt.plot() for example, you can set a tuple with width and height.\nplt.bar(x,y , color ='green')","14d22828":"dic=defaultdict(int)\nfor word in corpus:\n    if word not in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:30] \n    \n\n\nx,y=zip(*top)\nplt.rcParams[\"figure.figsize\"] = (20,10)\nplt.bar(x,y , color ='red')","76180fdd":"plt.figure(figsize=(10,5))\nimport string\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n      \n#x,y=zip(*dic.items())\n#plt.barh(x,y ,color = 'purple')\ndic","401a76fe":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\ntrain['SentimentText'] = train['SentimentText'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntrain.head(10)","5de639b4":"def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\nN = 100\n\n# Unigrams\ndisaster_unigrams = defaultdict(int)\nnondisaster_unigrams = defaultdict(int)\n\nfor tweet in train[TWEETS]['SentimentText']:\n    for word in generate_ngrams(tweet):\n        disaster_unigrams[word] += 1\n        \nfor tweet in train[~TWEETS]['SentimentText']:\n    for word in generate_ngrams(tweet):\n        nondisaster_unigrams[word] += 1\n        \ndf_disaster_unigrams = pd.DataFrame(sorted(disaster_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_unigrams = pd.DataFrame(sorted(nondisaster_unigrams.items(), key=lambda x: x[1])[::-1])\n\n# Bigrams\ndisaster_bigrams = defaultdict(int)\nnondisaster_bigrams = defaultdict(int)\n\nfor tweet in train[TWEETS]['SentimentText']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        disaster_bigrams[word] += 1\n        \nfor tweet in train[~TWEETS]['SentimentText']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        nondisaster_bigrams[word] += 1\n        \ndf_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\n\n# Trigrams\ndisaster_trigrams = defaultdict(int)\nnondisaster_trigrams = defaultdict(int)\n\nfor tweet in train[TWEETS]['SentimentText']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        disaster_trigrams[word] += 1\n        \nfor tweet in train[~TWEETS]['SentimentText']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        nondisaster_trigrams[word] += 1\n        \ndf_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])","407a9414":"fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_disaster_unigrams[0].values[:N], x=df_disaster_unigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_nondisaster_unigrams[0].values[:N], x=df_nondisaster_unigrams[1].values[:N], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common unigrams in POSITIVE Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common unigrams in NEGATIVE Tweets', fontsize=15)\n\nplt.show()","ae0ec56a":"fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common bigrams in POS Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common bigrams in NEG Tweets', fontsize=15)\n\nplt.show()","8a6fe69e":"fig, axes = plt.subplots(ncols=2, figsize=(20, 50), dpi=100)\n\nsns.barplot(y=df_disaster_trigrams[0].values[:N], x=df_disaster_trigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_nondisaster_trigrams[0].values[:N], x=df_nondisaster_trigrams[1].values[:N], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=11)\n\naxes[0].set_title(f'Top {N} most common trigrams in POS Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common trigrams in NEG Tweets', fontsize=15)\n\nplt.show()","b578cd17":"\"\"\"!pip install pyspellchecker\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)\n'correct me please'\ntrain['SentimentText']=train['SentimentText'].apply(lambda x : correct_spellings(x))\"\"\"","193e8b90":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 20\n                         ).generate(\" \".join(corpus[:100]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","527b987a":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nimport torch\n\nimport warnings\nwarnings.simplefilter('ignore')","8dfbc743":"def cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer\n\nlist_corpus = train[\"SentimentText\"].tolist()\nlist_labels = train[\"Sentiment\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","be33727f":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='POSITIVE')\n            blue_patch = mpatches.Patch(color='blue', label='NEGATIVE')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_counts, y_train)\nplt.show()","65ac62b8":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","c63b7eb7":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_tfidf, y_train)\nplt.show()","a1793361":"#from google.colab import files\ntrain.to_csv('trainG_clean.csv',index = False)\ntest.to_csv('test_clean.csv',index = False)","a84c70ee":"#files.download('trainG_clean.csv')","4a157559":"#files.download('test_clean.csv')","c6c372d9":"#Make a copy of the dataframes\ntrainG = train.copy() \ntest = test.copy() ","890f6fa8":"#https:\/\/medium.com\/@vasista\/sentiment-analysis-using-svm-338d418e3ff1\n\n#from google.colab import files\nimport io \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn import svm\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,accuracy_score\nimport six.moves.cPickle as pickle\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nimport numpy as np \nimport pandas as pd\n\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Span\nfrom spacy import displacy","b323fc6e":"#uploaded = files.upload()","a3910f75":"#trainG = pd.read_csv(io.BytesIO(uploaded['trainG (2).csv'])) \n#test = pd.read_csv(io.BytesIO(uploaded['test_clean (2).csv']))","9c0c10da":"x = trainG['SentimentText'].astype(str)\ny = trainG['Sentiment']","a450aa01":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","72ae9b86":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pickle\nfrom multiprocessing import Pool","afdca7d5":"tfidf = TfidfVectorizer(encoding='utf-8',\n                       ngram_range=(1,3),\n                       max_df=1.0,\n                       min_df=10,\n                       max_features=500,\n                       norm='l2',\n                       sublinear_tf=True)","ad54a658":"train_features = tfidf.fit_transform(X_train.values.astype('U')).toarray()\n\nprint(train_features.shape)","fe1eff4b":"test_features = tfidf.transform(X_test.values.astype('U')).toarray()\nprint(test_features.shape)","e55f9b47":"train_labels = y_train\ntest_labels = y_test","a62a88cc":"import pandas as pd\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","7e9966e7":"mnb_classifier = MultinomialNB()\nsvm_classifier = LinearSVC()\n","8e88a871":"mnb_classifier.fit(train_features, train_labels)\nsvm_classifier.fit(train_features, train_labels)","baea0559":"mnb_prediction = mnb_classifier.predict(test_features)\nsvm_prediction = svm_classifier.predict(test_features)","d6528883":"training_accuracy_mnb = accuracy_score(train_labels, mnb_classifier.predict(train_features))\ntraining_accuracy_svm = accuracy_score(train_labels, svm_classifier.predict(train_features))\nprint(training_accuracy_mnb)\nprint(training_accuracy_svm)\n","495b217a":"testing_accuracy_mnb = accuracy_score(test_labels, mnb_prediction)\ntesting_accuracy_svm = accuracy_score(test_labels, svm_prediction)\nprint(testing_accuracy_mnb)\nprint(testing_accuracy_svm)","943aa2f1":"print(classification_report(test_labels, mnb_prediction))\n","56ced116":"print(classification_report(test_labels, svm_prediction))","08f1589c":"conf_matrix_mnb = confusion_matrix(test_labels, mnb_prediction)\nconf_matrix_svm = confusion_matrix(test_labels, svm_prediction)\nprint(conf_matrix_mnb)\nprint(conf_matrix_svm)","ab0fc794":"test_vectorizer =tfidf.transform(test['SentimentText'].values.astype('U')).toarray()","1d4f8d31":"test_vectorizer.shape","242c7044":"final_predictions_mnb = mnb_classifier.predict(test_vectorizer)\nfinal_predictions_svm = svm_classifier.predict(test_vectorizer)\n","8f8ab8e8":"submission_df = pd.DataFrame()","e11dc853":"submission_df['Id'] = test['ItemID']\nsubmission_df['target_mnb'] = final_predictions_mnb\nsubmission_df['target_svm'] = final_predictions_svm","dd550ad5":"submission_df['target_mnb'].value_counts()\n","c6500d72":"submission_df['target_svm'].value_counts()","0c51804e":"submission = submission_df.to_csv('resultat_MNB_SVM.csv',index = False)\n#files.download('resultat_MNB_SVM.csv')","6af10f41":"train_df, test_df = train_test_split(trainG, test_size = 0.2, random_state = 42)\nprint(\"Training data size : \", train_df.shape)\nprint(\"Test data size : \", test_df.shape)","33db890d":"top_words = 10000\ntokenizer = Tokenizer(num_words=top_words)\ntokenizer.fit_on_texts(train_df['SentimentText'])\nlist_tokenized_train = tokenizer.texts_to_sequences(train_df['SentimentText'])\n\nmax_review_length = 85\nX_train = pad_sequences(list_tokenized_train, maxlen=max_review_length)\ny_train = train_df['Sentiment']","54318659":"embedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","7142407c":"history = model.fit(X_train,y_train,epochs = 3, batch_size=64, validation_split=0.25)","7244c40f":"import sklearn.metrics as metrics\nfrom sklearn.metrics import f1_score\nlist_tokenized_test = tokenizer.texts_to_sequences(test_df['SentimentText'])\nX_test = pad_sequences(list_tokenized_test, maxlen=max_review_length)\ny_test = test_df['Sentiment']\nprediction = model.predict(X_test)\ny_pred = (prediction > 0.5)\nprint(\"Accuracy of the model : \", accuracy_score(y_pred, y_test))\nprint('F1-score: ', f1_score(y_pred, y_test))\nprint('Confusion matrix:')\nconfusion_matrix(y_test,y_pred)","c1abbc97":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ad7d26ca":"tweet_1 = train_df.SentimentText.values\ntest_1 = test_df.SentimentText.values\nsentiments = train_df.Sentiment.values","d249683f":"word_tokenizer = Tokenizer()  #word tokenization \nword_tokenizer.fit_on_texts(tweet_1) \nvocab_length = len(word_tokenizer.word_index) + 1 ","074e3dc5":"def metrics(pred_tag, y_test):\n    print(\"F1-score: \", f1_score(pred_tag, y_test))\n    print(\"Precision: \", precision_score(pred_tag, y_test))\n    print(\"Recall: \", recall_score(pred_tag, y_test))\n    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n    print(\"-\"*50)\n    print(classification_report(pred_tag, y_test))\ndef embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)","cb2743c9":"def plot(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('A ',fontsize=16)\n        ax[idx].set_ylabel('B',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)","0207ab6c":"longest_train = max(tweet_1, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\npadded_sentences = pad_sequences(embed(tweet_1), length_long_sentence, padding='post')\ntest_sentences = pad_sequences(\n    embed(test_1), \n    length_long_sentence,\n    padding='post'\n)","2f1b27cd":"#Download the zip file\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","942ed3ab":"#Unzip it\n!unzip glove*.zip","4de6cec1":"#Get the exact path of where the embedding vectors are extracted using\n!ls\n!pwd","0917a4f2":"embeddings_dictionary = dict()\nembedding_dim = 100\nglove_file = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\nglove_file.close()","aff85653":"embedding_matrix = np.zeros((vocab_length, embedding_dim))\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","818eb4b7":"X_train, X_test, y_train, y_test = train_test_split(\n    padded_sentences, \n    sentiments, \n    test_size=0.25\n)","ef439bed":"def BLSTM():\n    model = Sequential()\n    model.add(Embedding(input_dim=embedding_matrix.shape[0], \n                        output_dim=embedding_matrix.shape[1], \n                        weights = [embedding_matrix], \n                        input_length=length_long_sentence))\n    model.add(Bidirectional(LSTM(length_long_sentence, return_sequences = True, recurrent_dropout=0.2)))\n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","7f5bb21d":"model = BLSTM()\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs = 7,\n    batch_size = 32,\n    validation_data = [X_test, y_test],\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n)","a69ca149":"plot(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","338df5e5":"loss, accuracy = model.evaluate(X_test, y_test)\nprint('Loss:', loss)\nprint('Accuracy:', accuracy)","879405e1":"preds = model.predict_classes(X_test)\nmetrics(preds, y_test) ","6cdf2c03":"model.load_weights('model.h5')\npreds = model.predict_classes(X_test)\nmetrics(preds, y_test)","2c86da1a":"These embeddings  look  cleanly separated. Let's see if we can still fit a useful model on them.","9ad8fd37":"![precison.png](data:image\/png;base64,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)","e7258e9f":"# As you noticed here our dictionnary is empty which means we don't have any special character and our data is clean from them. ","8509bb01":"**Word tokenization **is the process of splitting a large sample of text into words. This is a requirement in natural language processing tasks where each word needs to be captured and subjected to further analysis like classifying and counting them for a particular sentiment etc.\n","00debc46":"I also created a log-log((https:\/\/en.wikipedia.org\/wiki\/Log%E2%80%93log_plot)) plot for the words frequency which is similar to the previous frequency graph but includes all words and is plotted on a base 10 logarithimic scale which helps us visualize the rapidly diminishing frequency of words as their rank drops.\nThe word distribution present in this data dictionary is a very common phenomenon in large samples of words as shown by Zipf's law((https:\/\/simple.wikipedia.org\/wiki\/Zipf%27s_law)) where the most frequent word will occur about twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\nIt would be intersting to see if this holds true after we remove words like i, and and is from the observation above.","db72444a":"# Nouvelle section","e148ae62":"# Target Analysis","efda676f":"To obtain a vector representation for words we can use an unsupervised learning algorithm called GloVe (Global Vectors for Word Representation), which focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together","a784a9b1":"# **Pre-processing 1: Clean tweet text by removing links, special characters......\u00b6**\nThis step is one that we could spend a lot of time on but the goal is always to find the best balance\nThe bulk of the work in NLP is done on feature engineering and this is one of the processes that could alter the features you end up with.\n\nI created a funtion to do bulk formatting \/ clean up for every tweet in the dataset. The function is commented to show what every line is doing.","783ad4ac":"# Now we will implement the LSTM model agaib but with more improvements using the GLove pretrained model ","ef73a3f8":"Distributions of meta features in classes and datasets can be helpful to identify Positive\/Negative tweets. It looks like some tweets are written in a more formal way with longer words compared to other tweets .The meta features used for the analysis are : \n\n\n1.    word_count number of words in text \n2.   unique_word_count number of unique words in text\n3.   stop_word_count number of stop words in text\n4.   url_count number of urls in text\n5.   mean_word_length average character count in words\n6.   char_count number of characters in text\n7.   punctuation_count number of punctuations in text\n8.   hashtag_count number of hashtags (#) in text\n9.   mention_count number of mentions (@) in text\n\n\n\n\n\n\n\n\n\n\n","f2d1ea7f":"# Bag of Words Counts","b2f94a2d":"# Removing Stop words\nWhat are Stop words?\n\nStop Words: A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n\nWe would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages. You can find them in the nltk_data directory. home\/pratima\/nltk_data\/corpora\/stopwords is the directory address.(Do not forget to change your home directory name)","722ca4aa":"#we can also correct the spelling of words but it takes a lot of time. ","d7f7a827":"# TF IDF ","c601fc6e":"\"\"\"In the cell above we extract the most common words in the dataset and list the top words.\nPerhaps to no surprise we encounter words like i, and and is as they are very highly used in human expressions. These kind of words usually appear equally in both negative and positive oriented expressions and as such they bring very little information that can be incorporated in the model so we will have to get rid of them down the road.\nIn the text preprocessing steps later on we will learn how to deal with these common words that don't add much to the feature space.\nBelow is a graph showing the frequency of the first 50 words.","026f1bc2":"We are going to use LSTM (long short-term memory) model because it solves a vanishing gradient problem","61475dc5":"# Bigrams\nThere are no common bigrams exist in both classes because the context is clearer.\n\nMost common bigrams in disaster tweets are giving more information about the disasters than unigrams, but punctuations have to be stripped from words.\n\nMost common bigrams in non-disaster tweets are mostly about reddit or youtube, and they contain lots of punctuations. Those punctuations have to be cleaned out of words as well.","219e3946":"we want to check if our code works well ! \nhere is the text before our cleaning step : ![cleaning_mariem.JPG](data:image\/jpeg;base64,\/9j\/4AAQSkZJRgABAQEAYABgAAD\/4RDaRXhpZgAATU0AKgAAAAgABAE7AAIAAAAFAAAISodpAAQAAAABAAAIUJydAAEAAAAKAAAQyOocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAE5haW0AAAAFkAMAAgAAABQAABCekAQAAgAAABQAABCykpEAAgAAAAMwNgAAkpIAAgAAAAMwNgAA6hwABwAACAwAAAiSAAAAABzqAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMjAyMDoxMjowOCAxMDo0MzozOQAyMDIwOjEyOjA4IDEwOjQzOjM5AAAATgBhAGkAbQAAAP\/hCxdodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw\/eHBhY2tldCBiZWdpbj0n77u\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\/Pv\/bAEMABwUFBgUEBwYFBggHBwgKEQsKCQkKFQ8QDBEYFRoZGBUYFxseJyEbHSUdFxgiLiIlKCkrLCsaIC8zLyoyJyorKv\/bAEMBBwgICgkKFAsLFCocGBwqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKv\/AABEIAJsE9AMBIgACEQEDEQH\/xAAfAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgv\/xAC1EAACAQMDAgQDBQUEBAAAAX0BAgMABBEFEiExQQYTUWEHInEUMoGRoQgjQrHBFVLR8CQzYnKCCQoWFxgZGiUmJygpKjQ1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4eLj5OXm5+jp6vHy8\/T19vf4+fr\/xAAfAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgv\/xAC1EQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4+Tl5ufo6ery8\/T19vf4+fr\/2gAMAwEAAhEDEQA\/APoiisyy1oX2o3ltHY3SR2khie6cx+WzgA7QA+7v3UCo01+zn1D7HHLIspLKu6F1RyvUK5G1iOeAT0PpQBr0VmJrVodVOnLcK90ELmMAnAGM5PQH5hxnPIou9aFnqtnZy2N0Y7x\/LjulMflh9rNtI37+in+HHvQBp0UUUAFFFVLXVLK9u7i2tJ1llt8eaFBwuc9+h5BHHQgigC3RRRQAUUVFFdW8800MM8cktuwWZEcFoyQGAYdiQQeexFAEtFFFABRVO61ays9SstPuZtl1flxbR7GPmbF3NyBgYHrirlABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAQywsxzHIyH1FReRcf8\/Un6f4VbooAqeRcf8\/Un6f4UeRcf8\/Un6f4VbooAqeRcf8\/Un6f4UeRcf8\/Un6f4VbooAqeRcf8AP1J+n+FHkXH\/AD9Sfp\/hVuigCp5Fx\/z9Sfp\/hQILjvdSfp\/hVuigBkabFx1p9FFABUMsLMcxyMh9RU1FAFTyLj\/n6k\/T\/CjyLj\/n6k\/T\/CrdFAFTyLj\/AJ+pP0\/wo8i4\/wCfqT9P8Kt0UAVPIuP+fqT9P8KPIuP+fqT9P8Kt0UAVPIuP+fqT9P8ACjyLj\/n6k\/T\/AAq3RQBFFEU5dy7dyaloooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAMDQv9brf\/YTf\/wBASsuPXNN1rxZBB\/aVnHDp85CRtOokuJ9pXCrnO0bj9T7DnoLLRRY6jeXMd9dPHdyGV7VxH5auQBuBCbu3diKttEynjke1AHKXmqaRB8QbOFL6zjZbWaN085ARIzoQpGfvHnjqa09c\/wCP7w\/\/ANhL\/wBoyVsLExPPAqpd6KLzVbO8lvroR2b+ZHaqI\/LL7WXcTs39GP8AFj2oA06KKKACuX0jUdMPjLUYrS8tCr29vHCkcq4bbvyqgHt6DpXUUUB0sFFFFABXmmhad4f0X4la1a3+rXdrfS3cE2n291rlwDdKYEGQjy4m+dXXkNjGOgAr0uijZ3HfRo4LTtHv9c1rxBdHxBqcEtlrASyjWdvJgRUiZlMYIEgbJGHyBn5QDknnvHfiqSLX9QGmzvbX+mTQKiPrk8Mrj5HZksUQpNFhiC74\/j5AUGvXqKFpbyt+mpLTafmef+J\/EWgaN8RtEa48TQ2U5kkjvrWbVykaJ5DFC8BfYuW24baCTjmo\/EUFxf614sY6tqdvHpulQ3FpFaXjwpHLsmO\/5SN33R8pyp7qeK9EopNXjZbj05rlXS55LrR7O4mOZJYEdyBjJKgmrVFFU3d3FFWVgooopDCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qNq\/3R+VAFeirG1f7o\/Kjav90flQBXoqxtX+6Pyo2r\/dH5UAV6KsbV\/uj8qKAFqN7iGNiryqrDqCakqtEitdXG5QfmXqP9kUATRyxy58t1bHXBp9VpnW1DyRx8nBbjg1YVtyhsEZGcHqKAFooooAKKKKACiiigAooooAKKKKACiuL0b4mWuteKm0CHw54itrmNsTS3NiEihBBKs53kqG2nBI5rqLbVbK71O80+2nEl1Y7PtEYB\/d7xlcnpyOeKOlw2di5RRRQAUVxXiH4m2vhzxIuizeG\/Ed7cSDMMllYiSOfChm8s7wW2g84HFN1n4qaVoWv3umXek63KmniNru+t7Pzbe3DqGBdg2QMH07GjpcdmdvRUdvcRXdtFcW0iyQzIHjdTwykZBH4VJRsLcKK5XX\/iBp2gaz\/Zf9navqdykay3A02yacWyNnDSEdAcHpk8dK6iKRZokkjOUdQynGMg0bq4bOw6iiqmp6rZaParc6lOIImlSFWIJy7sFUcDuSBQBboqtqN5\/Z+m3F59nuLryIzJ5Fsm+STAztVe5PYVJaXH2qzhuPKlh82NX8qZdrpkZww7EdxQBLRRRQAUUhIVSTwAMmqul6pZ61pkOoaZOLi1nBMcoBAYAkd+eoNAFuiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiuX8R+PbDw7qsemnTtW1S8aITSQ6XZmcwxkkB3xjAyD6njpQB1FFR21xHd2sVzASY5kWRCVIJBGRweRUlGwbhRVPVNVstF0977VJxb20ZVWkIJwWIUDAyeSQKfqN5\/Z+m3F59nuLryIzJ5Fsm+STAztVe5PYUdLhu7FmiorS4+1WcNx5UsPmxq\/lTLtdMjOGHYjuKlo2DcKKKKACiskeJLKfwxJrulpcaparGzollEXkm2kghFOCTkEVheF\/iVa+K9ZfT7Tw74hszEzxzXF7YiOKF1GSjsGO1uRweeRR1sD0Vzs6KKKACiqdrqtle6he2NrOJLmxKLcxgH92WG5RnpyOeKuUAFFFFABRRRQAUVV1XUYtI0e81K5V2hs4HnkWMAsVVSxAyQM4HrS6bfxappVpqFurrFdQpMiuAGCsARnGeeaALNFFFABRRRQAUVBe3lvp1hPe3soit7eNpZZCOFUDJPHtTrW5ivLSG6tn3wzIskbYI3KRkHB9jQBLRRRQAUUUUAFVilwlxK0SxsrkH5mx2xVmigCGJZWZjchMZBUDnFTUUUAFFFFABRRRQAUUUUAFFFFABRRRQB5h4V8QXU\/xY1yabwt4itLbVlt4Ybm508pFGYUfcXbOADkYIznPasbSvA+h+F\/jBOD4N1aa3knt5NJv7USywWrbf3jSNv4G\/+9n6Yr2iihaNPsEtU13PA9R0DXD8UZrrU7HW1vm1ZXtNVstOluEjtt42oZBcKiJt4YGMkDPWma98P7m+1PVrxtM1aSS4fV5MJ5wVyrhrYADjBYllH8R9a9\/opJWil5P8bf5Fc2t\/NP8AP\/M8l8QeIL+z8b+Fp28K+Jb9dGtpBdTWunNIsjTQoBsOeSDkNnGD60mr32r6b4z8aWdj4V1jUpNcht4rOeO2xbZFvsO+ViAAC3v0I4r1uiqlaWj8\/wARRfLa3Qy\/DGlyaH4T0rS53EktnaRQOw6EqoBx7cVqUUUSbk22SlZWPKPiPDP\/AMJFJPoug+KIde8hUs9V0UAwTcZVJ8nAUMcHcOnesj4k6N4i1K80WfxBpV7qloumIssWm2kl0Ib3OXfy454iO2GJYcYxXt1FTbS39df8yr6\/15f5HkTeC28Sal4QtPEEWr3lhFocy3Us6SW7u+Y9qTbWO1u+0tyV74rM1PwPJq\/wW0G51bQtQv8AV9LZUNufMFwLfz\/nQJkZJjAxxnGMV7hRTev33\/Fv9bCWn5fhb\/gnkHiTTdAm+DVvYWHgXxGYGeb7BpwtpTPaT4fbJIu8sF3EnksORxUN9Y3HjPwP4Y0+58P65aJY+Zb3UNzbPA7FLJsN8pzsZ9qgnGTxXstFEveUl3GnZp9r\/ieBWvhTxJZeDNZtdKstUV7uz0u4uo5PMMk7FW+0qu5lJbGAUDKcDHGaXT\/D1+3wu8VWemadrtrHPLafZ9Nm0uW2MREql3hDzTM2QMk54x+XvlFO+rff+remgtreR5fpfw606DxF4l0CTTryTw\/fWVrMRPNKUmuAzlm8zOd2QpOD+lQ\/BbRLLQ7ae2fwtq2kazHEEvby7jcQXOHOBGSxU4GOgFerUUlp\/Xr\/AJ2E9f6\/rsFFFFAwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACvLviVCTrqT2Og+KF1hbbZZ6voIBU8kiObkDYGxncOh4Neo0UmrjTseJ\/EHSPE2p2nhuXxJpd3qlsmn4vrawtXudl4cfMYopoj0z824gc8c1bTwbNr48CWGuR6xd2EdldreSXEUltJghSkc21iV6AYLc7a9hoqv87\/AJ\/5i109LHiGpeA5NV+CUEd7ot9d6ppFxMtlA5kEywfaeQFJG7Ma8ZzxjFW9f0zw9L8HVsNO8BeJVheaY2Wn\/ZpTPbXGxgsrr5hbZk9yR7V7JRU2umu\/9f156jTs0zxq+sbjxn4H8Mafc+H9ctEsfMt7qG5tngdilk2G+U52M+1QTjJ4rHtfCniSy8Gaza6VZaor3dnpdxdRyeYZJ2Kt9pVdzKS2MAoGU4GOM177RTl7zb72\/D\/MS0SXY8D0\/wAPX7fC7xVZ6Zp2u2sc8tp9n02bS5bYxESqXeEPNMzZAyTnjH5ddo\/w80628S+JNAl0+8k8O39layuJ5pSk04di7eZnO7IUnB\/SvTqKHr\/Xlb\/ggtFb+tzyb4L6DZaCZ7Wbwrq2la3FEyXd\/dRuLe4HmfKI2LFScbegHTrXSeELDUbaPxniCW2nudZuJLR5oyocGJArjI5XI6jjiu1ooeu\/a34p\/oC0d\/O54P8ADbQtS0\/xxaTapp2u2N4sUw1SaTTJmgvjsPDztcOr88grGMntW54S0K6svGFlc3+k6ivh6V7g6LaSRsf7LZjyZU25QON20sfkBxwTXrlFO+q\/r+vIXf8Ar+vM8Y0HwTonhb4v3Kf8IdqzxNPFJpOoW4lkt7YeX85kffx82fvA17PRRS6JD63CvF9Rt\/Fdpo+u6Hp2mXraY11d2uxbQnzhdPIyyocZKplQSDj5znpXtFFHW47u1keC+I\/DOpy+I9Whk8P67eeIGu0\/sHWLaRltLOABdoLBtqYw2QQc+2a6vUfACeLfiLrw8QLqCaebG08mSBmiR5wHHmK3RmTJwOQN3I6V6fRR0SfT\/L+n66i72\/rW54h8RPDl3eeIdRttV8P654kiOmRx6HNbMxit5VRg7ylWUby2DyDnpivWfCsMtv4O0aG4jeKWOxhR43UqysEAIIPQ1rUU07Jr+uv+YPVp9v8Agf5Hm\/xU06e9v9HfUdG1XXPDkYlF7YaUzeY0pA8tiqkFlHzdCMVi+KNLhudC8K7fCniC48IwQypcaHAri6jfgRGRA+4gYb+LjOa9ioqbaW\/r+u3YOtzxi20\/XdB8LeGNRn0TVruDTdamuI9OiXz7q3tHR1jUjPJG4cZ4zUP\/AAj+r654cji1DR9T09r3xobqaFFYSQwOpy+5ei4ONw4969toqlo7\/wBdP8g6W\/rr\/meP\/wDCuYLjwh4y8PjSLp7S0vXn0SCWSUAP9nG0oxPzDeWHJIznNdF8JLDTNO8NSw6Z4a1XQZgYxdrqUToZ5QgBdNzHK5z0x9K76iktPw\/AHrb5\/iFFFFABRRRQAUUUUAFFFFABUUzSLt8sqPXcuf6ipajm7UAQ+Zcf34v+\/Z\/xo8y4\/vxf9+z\/AI0tFACeZcf34v8Av2f8aPMuP78X\/fs\/402WWOCF5ZnWOONSzu5wFA5JJ7Cq+narp+sWxudJv7a+gDFTLbTLIoYdRlSRnmgC15lx\/fi\/79n\/ABo8y4\/vxf8Afs\/40tFACeZcf34v+\/Z\/xo8y4\/vxf9+z\/jS0UAJ5lx\/fi\/79n\/GjzLj+\/F\/37P8AjS0UAJ5lx\/fi\/wC\/Z\/xo8y4\/vxf9+z\/jS0UAJ5lx\/fi\/79n\/ABo8y4\/vxf8Afs\/40tc7e+I5bO4kV\/s6Isvlqz5GTuwB16k4FAHQ+Zcf34v+\/Z\/xo8y4\/vxf9+z\/AI1X0+4e7sY5pAoZs5C9OCRVmgBPMuP78X\/fs\/40eZcf34v+\/Z\/xpaKAE8y4\/vxf9+z\/AI0eZcf34v8Av2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/xoLBVLMQABkk9qp6brOmazC82j6jaX8SNtd7WdZVU9cEqTg0AXPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/xpaKAE8y4\/vxf9+z\/AI0eZcf34v8Av2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/xpaKAE8y4\/vxf9+z\/AI0eZcf34v8Av2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/xpaKAE8y4\/vxf9+z\/AI0eZcf34v8Av2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/xpaKAE8y4\/vxf9+z\/AI0eZcf34v8Av2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/xpaKAE8y4\/vxf9+z\/AI0eZcf34v8Av2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/xpaKAE8y4\/vxf9+z\/AI0eZcf34v8Av2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/xpaKAE8y4\/vxf9+z\/AI0eZcf34v8Av2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/xpaKAE8y4\/vxf9+z\/AI0eZcf34v8Av2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/xpaKAE8y4\/vxf9+z\/AI0eZcf34v8Av2f8aWigBPMuP78X\/fs\/40eZcf34v+\/Z\/wAaWigBPMuP78X\/AH7P+NHmXH9+L\/v2f8aSSRIo2kldURAWZmOAoHUk0RSxzwpLC6yRyKGR0OQwPIIPcUAL5lx\/fi\/79n\/GjzLj+\/F\/37P+NLRQAnmXH9+L\/v2f8aPMuP78X\/fs\/wCNLRQAnmXH9+L\/AL9n\/GjzLj+\/F\/37P+NLRQAnmXH9+L\/v2f8AGjzLj+\/F\/wB+z\/jTILiG6hEtrNHNGSVDxsGGQcEZHoQR+FQ22p2F5d3FraXtvPcWpC3EMUqs8JPQMAcrnB6+lAFnzLj+\/F\/37P8AjR5lx\/fi\/wC\/Z\/xpaKAE8y4\/vxf9+z\/jR5lx\/fi\/79n\/ABpaKAE8y4\/vxf8Afs\/40eZcf34v+\/Z\/xpagt760u5p4rW6hnktn8udI5AxibGdrAdD7GgCbzLj+\/F\/37P8AjR5lx\/fi\/wC\/Z\/xrGuPGfhe0upLa68SaRDPExSSKS+iVkYcEEFsg+1awuYDc\/ZxNGZ9nmeVuG7bnG7HXGeM0AP8AMuP78X\/fs\/40eZcf34v+\/Z\/xpaq2Op2GqRySabe294kUhika3lWQI46qSDwRkcUAWfMuP78X\/fs\/40eZcf34v+\/Z\/wAaZBcwXUZe2mjmRXZC0bBgGU4YcdwQQR2IqSgBPMuP78X\/AH7P+NHmXH9+L\/v2f8aWo0uYJLiWCOaN5oceZGrAsmemR2z2oAf5lx\/fi\/79n\/GjzLj+\/F\/37P8AjS0UAJ5lx\/fi\/wC\/Z\/xo8y4\/vxf9+z\/jS0UAJ5lx\/fi\/79n\/ABo8y4\/vxf8Afs\/40tFACeZcf34v+\/Z\/xo8y4\/vxf9+z\/jS0UAJ5lx\/fi\/79n\/GjzLj+\/F\/37P8AjS0UAJ5lx\/fi\/wC\/Z\/xo8y4\/vxf9+z\/jS0UAJ5lx\/fi\/79n\/ABo8y4\/vxf8Afs\/40tFACeZcf34v+\/Z\/xo8y4\/vxf9+z\/jS0UAJ5lx\/fi\/79n\/GjzLj+\/F\/37P8AjS0UAJ5lx\/fi\/wC\/Z\/xo8y4\/vxf9+z\/jS0UAJ5lx\/fi\/79n\/ABo8y4\/vxf8Afs\/40tFACeZcf34v+\/Z\/xo8y4\/vxf9+z\/jS0UAJ5lx\/fi\/79n\/GjzLj+\/F\/37P8AjVXUdV07R7YXGr39rYQMwQS3UyxKW64yxAzwePao9M17SNb83+xtVstQ8nHmfZLhJdmc4ztJxnB\/KgDShaVnPmMhGP4VI\/rU1QxfeP0qagAooooAKKKKACiiigAqObtUlRzdqAIqKKKAOI+LzY+HVykjlLeW5t47lgcYiMyhufTFVvCllZaT8WvEthodvDa2IsbSSW3t1CRpKd3RRwCVwffrXb6hp9pq2nT2GpW6XNrcIUlikGQw\/wA96z\/DfhLQ\/CNnJa+HdPSzilffJh2dnOMcsxJP0zxRHR\/12t\/wfUHqv673\/wCAbNFFFABRRRQAUUUUAFFFFABXlXjqwil1CG9keVnh1O3WOPediE3ABbb3Yg4yc4HTGTn1WuT1Tw7NqNxJ51r5kX2gTJ+8A5V9ynr6gGhaNPzDoze0X\/kEQ\/8AAv8A0I1eqppkMlvp0UUy7XXORnPc1boAKKKKACiiigAooooAKKKKAOc8ek\/8IXeL\/wAs5Hhjm5\/5ZNKiyfhsLU1o1g+JlqLaNEWXSJRPs4yEljEWR7b5MfU10FzbQ3lrLbXcSTQTIUkjcZV1IwQR6VR0jw9p2iNK9hHN5koVXluLmSeQqudq75GZgoycLnAyeOaFo7\/1s0D1Vv63Rp0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAVNWnkttFvZ4G2yxW8jo2M4IUkHmuIvdX8Ri1sL6WfU7fT5NOt5ReabZw3K+cwJkM8RBkKD5P9UBwWyR276eCO6tpIJ13RSoUdc4yCMEcVj3Pg3RLrZvt549kCWxFveTQiSJQQqSBHHmAAkfNnqfU0ldN\/L9b\/AKB\/wf0\/4JZ1C7vLjwtPd+HPJuryS1MtmX4SRiuV6noeOp+prkF8T6hpmm6lHNeak2pIIhFba1Yxo0RaVYzIJIAscsY3qcKSRg5POB3V1p9peabJp91bxyWksZieEr8pTGMY9MVn23hXR7aG5ja3lu1uo\/Jm+3XMt0Wj5+TMrMQvJOBxTe77AtkUtOl1HTPFkej3uqzarDc2T3KyXEMaSRMjopGY1VSrbxgEZBU8ntW8XXWvW2oRNZNqkGliEBrjSLeC4kSUvjMkUgZmQDB\/dgt97PatvSvDum6NNJNZRzNNKoRprm5luJNo6KHkZiFGSdoOOabqnhvTdXulubtLhJ1j8rzbW8mtmZM52sY2UsM5IByBk46mh9P67jXU5vUfEEuptZroWsancSSWMdwIdGsIQWD7sSyPcgoinYcISrcH73aLQdZ1rxXb6RbnVJNMkbSI764uLWCItNI7FQAJFZQo2kkAZyy4IHXo7rwfod20RezaJY4Ft\/Ltp5IEeJfuxuiMFdBkjawIwSO5pH8HaI1haWaW0sEVkjR25t7uWF40PVN6MG2Hj5SccDjgU+v9ef8AwBf1+X\/BKfw+EreBrcTSqZTNchpYlwC3nyZKg54zyM5\/Gub8LtqcOnWOlW2tXKHUtU1EyXbxQtJGsU0mRH8m3c5wSWDADdgDjHoOl6VZaJpsdhpduttaxbvLiUnC5JY4z2yTxVOXwvo8unLYm1ZYY53uY2jmkSSKV2ZmdJFYOpJZuhHBI6cUMS2sczca1rcVrNpqapuubfXYdO+3+Qm94ZEV8lduzeBJjIUDKg45Ird8UXN\/ongLULiyvXkvrW1JjuZ0QszjozAKF\/IAe1W4PDWk21lDaw2mIobkXalpXZmmznzGcnc7Z6lic96u39hbapp81lfR+bbzrskTcV3D6jBpdLf1sv1uPrf+t3\/wDkZI9fXxBdaUfFF2Il08XomW1t\/MSTcy7ATHt8vjOCpbgfN1zVXxnqOn6VpOt6oRJZ6joX2jyo0AAu1QSbV7\/OpYAEn7g9a7ZtNtGv3vWizcPB9nZ9x5jyTtxnHUnnrWJqfhSG9g0PSoba3TR9LminAeRmkUwjEaKCDkdMsWzgEYOcgW1n\/W\/wCjXzQf1+X6p\/JlttQn8P8Agn7fr8\/n3NnZiS6kVAPMkC8gBR3bgAe1cN4Vu7fQvEukMyXyT61A8Gpy3WnT2yPdlmmQhpEUEkvMoAOcbfSvSdQ0611S3WC+j82JZUlCbyoLIwZc4IyAwBweDjmk1HS7PVreOHUIRMkcyTp8xUq6MGVgQQQQRTv71\/6\/rX8EH2bf15HGeH28QyafrVtpmmaVPbSarfIJbu+kQ8zODmMQsCPbdz7Ve0HTf7G8XW2m+c0\/2TQIYfNYYL7ZGGcdunSunsdPtdNjljsovLWWZ53G4nLuxZjye5JOOlUtS8Nabqt+t7dfbI7lYvKElrfz2xKZzg+W655PepWlv66NfqOWvN5v9blzU43l0m6SKeS2domCyxBSyHHUbgRn6g1wHg7+1TY+E9Lh1q5itJNCW8lIihLkgxBUBMeNuGI6bvfPNeg21jDaWC2cZmeJVK5nneVyD6u5LHr3NV7LQtN05rRrO38s2Vr9jg+djsh+X5eTz91eTzx1qo6N\/wBdH+rRL1Vv63X\/AATipPFOt3As9Ptjey3F3fagGlsY7fzligmKqq+cRGOCoJIY4B4ydwsnUfFQg0myvHn0yW61ZrUTzpbvNLbeQ7hmVC0avkYyOMqDtwStdLP4V0e4s1tntnREuHukeKeSOSOV2LOyyKwZclmzgjgkdOKdbeGtKtEtlhtmza3DXMbvM7uZSpUuzsSznaxGWJ7egpLz\/rYpvV28\/wBbHL\/2zrMU82hf2pJJO2tJYR6lJDH5qRG3E5O0KEL\/AHlB245BIOOb\/hWC5tfGXiaG8vmv3QWuJ5EVXK7GIDBAFz9AOMcVt3Hh3SrqO8Se13C9mWeYiRg3mKFCurA5RgEXBXBBGetR2HhfStLW6+wxTxveMjXEv2uVpZSv3SZCxYntnPI4PFHT+uyFLVq39bmvRVeWxima4LvOPtEYifZcSLgDPK4YbD8x+ZcHpzwMK1pG0jOWmy0XlHE7gbfUDOA3+0OfegCeiq6WcSSROGnJiiMS5ncgqcckE4ZvlHzHJ688nJFYxQi3CPOfs6lE33EjZBGPmyfnPHVsmgCxRVZLCGNYAr3BFu5dN1zIckgg7st84+Y4DZA4x0GD7DFjG+4\/13nf8fEn3s5x977v+z932oAlnieaMLHPJAdytvjCkkAgkfMCMHoe+DwQeaiktpZBOFvriPzcbCqx\/ucdduVOc\/7WfbFQXunNLbyLaTTRyS3EUzsbiTgKy7gvPygqv3RhSScjk5iu9MurhNTEd08RugiwlZ5B5WBgsMH5TnsvBwM9TQBdltpZGuCl9PF5sYRAix\/uTz865U5JyPvbh8o465d5MnnF\/tMu3y9nl4XaD\/f+7nP449qo3um3Vw+ptBcsn2qzWGFfPkURuN\/zcH5fvLyuD8vXgVOLOYakZzM5iFsIQhlb5mzncR0Bx3HPJ9BQx\/1+RLFbSxmDfezy+VFsfesY848fO2FGG4P3cD5jx0wkdtKnkbr64k8rO\/csf77PTdhRjH+zj3zVOx0y6tjpnnXTyfZLNoZiZpG81zswxyfm+63zNluevJotdMuoV00S3TsbUu037+RvM3AgKcn5gCerc\/KKbEXEtZlWINf3D7JWdiyx\/vAc4Q4X7oyMYwflGSec2ayYdLu40tFa6c+TeyzyEzyHdGxkKpyfmA3KMHgY4AwMa1IAooooAKKKKACiiigAooooAKKKKAK9\/eRadp1ze3J2w20TSyH0VQSf0FZPhDT5bbRRfX\/zalqZF3eP\/tMBtT6Iu1B\/u+9a1\/YW+qadcWN9H5ltcxtFKgYruUjBGQQRx6VOqhVCqMADAHpQuv8AX9dPuB9P6\/rqSRfeP0qaoYvvH6VNQAUUUUAFFFFABRRRQAUyRC2MU+igCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigCHym9RR5TeoqaigBiIVbJx0p9FFABRRRQAUUUUAFFFFABRRRQAUUUUAUNbi1SfRp4dBuYLW\/kAWK4uELrFkgM23uQuSAeCcZ4zXE\/BeGW38O65BcXD3UsWu3aPPJ96UgqCx9zjNei1keHfDVn4ZgvYrCSeRb29lvZDMwJDyHJAwBxxx1PvRHRv0\/VBLVJef6M16KKKACiiigAooooAKKKKACuI8baVoka3FzfWp1TWtSH2fSoJAGkikC8eQcZiAPzs4PHUngCu3rnLvwi8\/iK41m28QapZXM8Sw7YUtnWNF\/hTzIWKgnkjPJ+gpNX0Y07am3p8U8GmWsV5L51xHCiyyD+NwACfxNWKZCjRQRxvK8zKoBkcAM59TgAZPsAPan1Td3clKysFFFFIYUUUUAFFFFABRRRQBz\/jm4mtvB12baR4nleGAyRsVZFklRGYEdCFYnPaqOnaXY+G\/H0Gn6FZR2NleaZLLNBboEi8yKSNVfaONxEjAnqcDOcCul1HT7bVtNuLC+j8y3uEKSKCQcH0I5B7gjpVDSPDqaXeS3txqN7ql5JEsIuL0x7kjBzsURoqgZOScZJxknAwR0lf+tn+T1B6q39bo2KKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArnPH3\/Il3f\/AF1g\/wDRyV0dUtX0uDWtMksbppEikZGJjIDfKwYdQe6igCp4t1a50Lwpf6lYRxS3NugaNJs7WJYDBxz3rCn1fxZFq2oaX9o0bzrWxS\/W4+xylSrF18ox+b1ymfM3f8A9Oo1nSoNc0ifTrtpEhnADNEQGGCDxkEdvSopdDtpdXutRZ5RNdWa2bqCNoRWdgRxnOXPt04qXfp\/Wj\/Ww1br\/AFrr+FzmJ\/FV4xiv9P0eG4u5vD63yIkZebLOvyAjllG7O0DJI45p1n4vv7jw\/q89nc2GtX1ltCQWdnNBPES21vNtXYyDbgsBkFwCABwTrjwfZJDCkN5fQNBpq6dHLDMEdEBBDggffyo9j0IIpY\/Ca7bt7rWdUub26iWEXzSRxTQorFlCeUiqPmJJypz0bI4q3bp5\/m7fhb\/MmN+vl+Sv+pgx+P2g8OaheSX+narcwXUVtHHaWs8EsbySeXtmtmLyqQ244HLAcAd1i8capaWmoG9s3vDFFGbW6OlXWnRPLJII1iZZsn7zKdyk8E8DHOy3gqzuYL0atfX2pXN2kaG7naNJYhGxePZ5aIqlXJYHGc9SQAKmXwtHNaXdvrGq6lq63UYiY3UqJsUHI2rEqKGzzvxu4HPApf1\/X9feMyNPXWY\/idAmuzWNw39jymOazheEHMseVKM79Mfe3c56DHMuqyap\/wALP02GG+jSwbSbqSS3aJjuIeIE53gbvmGDtOBuHO7jR0vwqmn65\/a9zq2pane\/Zjah7xo8CMsGwFjRRnI64ycnOeMW77Q4b7WrLU\/tNxb3FpHJD+62bZY32lkbcp4yinjB460nsl6\/je35oFu36fhb\/I4zwXqus6X4c8HpfNYyadqUKW0cUcTrNBiFnVzIXKvkRnI2Ljd1OOZNK8fanqdxY3dvZS3FjfXCxi1i0a8DQxs20Sm6I8pgOGI2gAE4Y7fm6e38K2Ntp+h2iS3Bj0Rla3LMuXxG0fz8c8OemOcVFYeEY9NniWz1fU4tOhlMsWmLKiwock7QwQSbcnOwuV7YxxVtpzuD20\/rf\/gGf8RtO\/tbT9GshcSWzy6tD5c8X3onCuVYfQgH8KpQa1Jqfijw3HqKLBqdjNdQX0KnhZBBncv+wwIZT6H1BrsNR0uDU2s2uGkU2dytzHsIGWUEAHI6fMapXPhTTbnxhZ+JSJI9QtYXhyjAJKrAgbxjkjJwcjqetQr2a7v9F\/kEtbf11Zy+lePtT1O4sbu3s5bixvrhYxaxaNeBoY2baJTdEeU4HDEYAAJwx2\/Ncg8bX3maRDc29uJGu5rbVGUNiLZL5CFBk43SPGRnPy7u9a1h4Rj02eJbPV9Ti06GUyxaYsqLChyTtDBBJtyc7C5XtjHFNufBGlXMmvOXuY311ES4McuPLKrgNHx8rd888gU9l\/X3fmPS7LvhvU7jWdGXULhI0SeWRrcR55h3kRseTklQG445rjPCWrSaDod8rfvFn+1XVnGzH5pRcvGyD0BZoTx3dq9AsLKDTdNtrGzTZb20SwxL6KowB+QrITwbpqxaZGXuGGm3cl3Flx87OzMVbA5UMwIHqq+lDtey2\/r\/AIcWu\/8AX9dDlvD13eaDZ3Glae0UuoX\/AIgmtluLkFkVhCJJJWUEFuEY7cjJPUVrS+KNYs\/tOlXMdjPrC3kFrBPGrpA4mBZZGQszLtCvlNxztHI3cak3hGxlhnVLi6hmkvzqEdzG6iS3mK7cplSMYyMMGBBOc0weDLJtPuYLm8vrm6uLhbl9QkkUTiVMeW67VCLtwAFC7euQcnKWyv5X+Vr\/AK\/1srb28\/zf\/A\/rfJ1LxVrmiWGvQXiafd6jpttDdW8sMbxRTpIxXDIXYqQyN\/Ecgg+orW03U9Xj8Vy6PrL2U4ezF3DJaQvFsG\/ayMGdt3VcMMd+BTf+ELtJbHUob6\/vr241MRrcXkzRiUqhyiqFQIoHPAXqxJyTmtY6VAdfGr7pPtAtTa7cjZtLBs4xnOR60\/tLt\/wH+tgezt5fmr\/qczqV5q1n4+vpYryA2dtohuFtXhc\/MGbv5mM5UZO3kce9I\/iPV7bwpYalqWqaJYTahskjEltK5RWTcIkiEm6eQnj5Svsp6Hd1Hw5BqOqi\/a7uoHa0ks5o4Sm2aNuzblJypJIKke+RxUN14VhmtNLjtdQvbGfSo\/Kt7q3MZk2bAhDB0ZDkAfw9RxilG6Vn\/Wrf+Q38Wn9aL\/gnPaf4su9TTTP7QtLSeZdfawMsllLAQBA7iRIpTvifBC8k8FvWs\/SPFFxpXh7RtK0\/5J7hLq4ef+zri+8tFuCoHlQ4bkt94sAMd811Vj4IsLFo2F5fTtHqX9p7p5VYtMYjGcnb0IJOOx6YGBQPBNnDb2A0\/UL+wurASrFeQNGZCkjbnRg6MjKTg8rkbRg9cnb+ui\/UOt\/63f6GZF4p1+\/j0W2t7WCxvL+e4gllvbOZVAiXcJVhYo+GA+6xBG7qcZPbLuCDeQWxyQMAn6VkQeG4Ip9OuJb29up9PaV1luJQzStICGLcYA54C7QOgAHFX\/ss23H264\/13mZ2x\/dz\/q\/u\/d7Z+9\/tU+oiW4WdowLWSON9yktJGXG3I3DAI5IyAc8HnB6VFKl8RP5NxboWx5G+Bm2eu75xuzzjG3HvUF7b3wt5PsV3M0slzEy5EYEUe5A6jK8rtDHnLcnBHGIbyLVnTU\/sk7ozhBZ\/6v5Gx8x5XpnqGyeDjGRQMuypelrjyLi3QNGBAHgZvLfnLNhxuX7vyjaeDzzw\/bc+cT5sXleXgL5R3b\/72d3T2xn3qhfR6qX1M2c0gVrNRaKpj+Wb58kZU\/7H3sjjp1zOI74akS0rG2FqFIATDS5+8OMg49TjkcdaTH\/X5f195LCl6pt\/PuLdwsRE+yBl8yTj5ly52r975TuPI545I0vR9n864t2258\/bAy+Z6bfnO3HfO7PtVOxi1ZTpf22d32WbC9J8vDzfJgnCjn7\/AN3C9eOmEtYtWC6Z9onclTIbvd5fzKQdoOF6gleVwODVMRdSO+CxeZc27ESsZStuw3R87VHznDD5cscg4PAzxZrIhi1cJZiWeQsL2VpifL5gzJsU4X08sfLg9Mk8516QdQooooAKKKKACiiigAooooAKKKKAOS+IElxJbaPp1vA1ymo6isM9uJfLEyCOR9jt2QlBuwCSu4YbOCeDI4NP1PWNKTTU0i4iaKd7C1lEloiupCvCdiY3FG3AqPmUnHOT0Gq6VDq9mIZpJYXjdZYZ4GAkhkXo6kgjPsQQQSCCCRUGj6FHpEl1O13dX95dspnursrvcKMKuEVVUAE8BR1JOSSaFpf+uwS1t\/Xc1KKKKACiiigAooooAKKKKACkLBeppajm7UAO8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAn8xfX9KPMX1\/SoKKAJ\/MX1\/SjzF9f0qCigCfzF9f0o8xfX9KgooAsBwx4NLUMX3j9KmoAKKKKAP\/Z)","3372699b":"# N-gram Analysis","0fd74b06":"# Unigrams\u00b6\nMost common unigrams exist in both classes are mostly punctuations, stop words or numbers. It is better to clean them before modelling since they don't give much information about target.\n\nMost common unigrams in positive tweets are already giving information about disasters. It is very hard to use some of those words in other contexts.\n\nMost common unigrams in negative tweets are verbs. This makes sense because most of those sentences have informal active structure since they are coming from individual users.","ba07ab27":"Class distributions are ..% for 0 (Negative) and ..% for 1 (Postive). Classes are almost equally separated so they don't require any stratification by target in cross-validation.","1dff4d91":"Now we want to zoom our tweets to detect if there are still special characters or no ","74eea3d3":"We need to perform tokenization - the processing of segmenting text into sentences of words. In the process we throw away punctuation and extra symbols too. The benefit of tokenization is that it gets the text into a format that is easier to convert to raw numbers, which can actually be used for processing","ab7b52d7":"# We will Now pply the LSTM Model which is more complicated than the MNB and the SVM ","58eaa637":"Considering the size of the dataset, the labels seem to be 'about' evenly distributed at 33494 vs. 41703 for positive and negative respectively.\nNext, we want to see the number of words contained in every sentence so I created a function to extract this information and appended it to a column next to the text column","02c84485":"# ***Text Pre-processing***\nText pre-processing is important for the Natural Language Processing (NLP) task of sentiment extraction so as to remove any unnecesary characteristics in the data which would make the final trained model a poor generalizer.\nText pre-processing can involve many things like removing unwanted characters like emojis, properly formatting the text to remove extra spaces or any other information in the text that we don't believe would make a great feature for our model. We'll see some examples below.\nWe also have to make sure that the information we pass the model is in a format that computers can understand. That is in digits not words. We'll also go through some of these steps below.\nAfter this pre-processing step, our data should be ready to be fed to the right machine learning (ML) algorithim to extract valuable information that it can use to do classification on unseen data.\n\n# **NOTE**: \n\nText pre-processing is done to the training data to facilitate the model's learning but for the text pre-processing examples below, I will use a copy of the twitter dataset since it is messier than our training data and as such there are more opportunities to demonstrate the text processing techniques.\n\nTherefore the actual training of the model will take the original training data and coresponding labels during training."}}