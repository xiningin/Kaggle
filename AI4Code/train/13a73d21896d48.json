{"cell_type":{"0482beb0":"code","4299dfb6":"code","f3930cf6":"code","80a7bae8":"code","6eb2cf98":"code","d9e4f900":"code","aa25d8d0":"code","f10e3adf":"code","48e7a28a":"code","3b478efc":"code","6cd6bf02":"code","7e7107c8":"code","e4612455":"code","557a9d4e":"code","208cc2e9":"code","f013d739":"code","c00cc74f":"code","c4c0b2c8":"code","5df05231":"code","a6b8cb04":"code","1b3ef0db":"code","41069634":"code","bc1f6959":"code","d4826e81":"code","adbfa5cd":"code","8deedf82":"code","bcc191bc":"code","fc54e884":"code","3cd20232":"code","fbb57897":"code","87cb5178":"code","0f0a74da":"code","9c5428e2":"code","ef4e2081":"code","796c3a03":"code","59048a80":"code","b34f67ea":"code","958aed45":"code","975a91cf":"code","80ea3cea":"code","ed71386c":"code","05c1de7f":"code","b0501a5d":"code","6304deba":"code","9af051a8":"code","1c7e141f":"code","f2ced555":"code","6987afac":"code","84ecfc1d":"code","bae18c64":"code","f67ff241":"code","a71a1f6e":"code","482fe8f4":"code","c0d1021f":"code","926639cc":"code","5fa8fa53":"code","9d5ea338":"code","580b7401":"code","c0737c4a":"code","ed215069":"code","523728f4":"code","dcab1006":"code","9a4ac72a":"code","950a4a28":"code","cb7dfb0a":"code","78e54e75":"code","c4237baa":"code","712e2b95":"code","ca0ec36f":"code","70ac7d0b":"code","4547aa81":"code","f703d352":"code","f2792ffa":"code","553b189f":"code","ab0476a5":"code","07fe9cb7":"code","00be268a":"code","3d2ab7a1":"code","4672f2f5":"code","0af6bca0":"code","02e2907c":"code","fd870abe":"code","174a2f6f":"code","6e76ec27":"code","5267819d":"code","61bea17f":"code","64b806cb":"code","2ce0d4dd":"code","f8da1e7d":"code","a5d4ce53":"code","fe3f2891":"markdown","55025747":"markdown","178146dd":"markdown","9e7753be":"markdown","33d51b30":"markdown","51b4a3cc":"markdown","3ce2f99b":"markdown","ed57ab3f":"markdown","d325d6e3":"markdown","c92afae1":"markdown","6c91c246":"markdown","b7f49a0c":"markdown","5cc976b8":"markdown","32ad0153":"markdown","40c48aa4":"markdown","7b3fff89":"markdown","e3709544":"markdown","59aa3654":"markdown","4be5eedf":"markdown","d86d1c8e":"markdown","bc11070b":"markdown","4c5913b1":"markdown","2161b276":"markdown","040345e6":"markdown","795cf05e":"markdown","5aaab517":"markdown","6d7314fa":"markdown","bf9868aa":"markdown","914f0089":"markdown","09516e43":"markdown","00dc583b":"markdown","7b619728":"markdown","3c4b44de":"markdown","20fed010":"markdown","38d9f52e":"markdown","c766bebc":"markdown"},"source":{"0482beb0":"from pandas import set_option\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# for preprocessing the data\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import log_loss\n# the model\nfrom sklearn.linear_model import LogisticRegression\n\n# for combining the preprocess with model training\nfrom sklearn.pipeline import Pipeline\n\n# for optimizing parameters of the pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","4299dfb6":"train = pd.read_csv('..\/input\/train_values.csv',index_col='patient_id')","f3930cf6":"train_labels = pd.read_csv('..\/input\/train_labels.csv',index_col='patient_id')","80a7bae8":"train.shape","6eb2cf98":"test = pd.read_csv('..\/input\/test_values.csv', index_col='patient_id')","d9e4f900":"train.head()","aa25d8d0":"train.shape","f10e3adf":"train.dtypes","48e7a28a":"set_option('display.width', 100)\nset_option('precision', 3)\ntrain.describe()","3b478efc":"train_labels.head()","6cd6bf02":"test.head()","7e7107c8":"train_labels.shape","e4612455":"test.shape","557a9d4e":"train_labels.groupby('heart_disease_present').size()","208cc2e9":"train_labels.heart_disease_present.value_counts().plot.bar(title='Number with Heart Disease')","f013d739":"selected_features = ['age', \n                     'sex', \n                     'max_heart_rate_achieved', \n                     'resting_blood_pressure']\n#train_values_subset = train[selected_features]","c00cc74f":"sns.pairplot(train.join(train_labels), \n             hue='heart_disease_present', \n             vars=selected_features)","c4c0b2c8":"set_option('display.width', 100)\nset_option('precision', 3)\ncorrelations = train.corr(method='pearson')\ncorrelations","5df05231":"sns.heatmap(train.corr(method='pearson'),annot=True, cmap='terrain', linewidths=0.1)\nfig=plt.gcf()\nfig.set_size_inches(8,6)\nplt.show()","a6b8cb04":"sns.pairplot(train)\nplt.show()","1b3ef0db":"train.skew()","41069634":"train.isnull().sum()\n","bc1f6959":"test.isnull().sum()","d4826e81":"one_hot_encoded_training_predictors = pd.get_dummies(train)\none_hot_encoded_test_predictors = pd.get_dummies(test)\ntrain,test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)","adbfa5cd":"from xgboost import plot_importance\nmodel = xgb.XGBClassifier()\nmodel.fit(train, train_labels.heart_disease_present)\n# plot feature importance\nplot_importance(model)\nplt.show()","8deedf82":"from numpy import sort\nfrom sklearn.feature_selection import SelectFromModel\nX_train, X_test, y_train, y_test = train_test_split(train, train_labels.heart_disease_present, test_size=0.10, random_state=7)\n# fit model on all training data\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\n# make predictions for test data and evaluate\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n# Fit model using each importance as a threshold\nthresholds = sort(model.feature_importances_)\nfor thresh in thresholds:\n\t# select features using threshold\n\tselection = SelectFromModel(model, threshold=thresh, prefit=True)\n\tselect_X_train = selection.transform(X_train)\n\t# train model\n\tselection_model = xgb.XGBClassifier()\n\tselection_model.fit(select_X_train, y_train)\n\t# eval model\n\tselect_X_test = selection.transform(X_test)\n\ty_pred = selection_model.predict(select_X_test)\n\tpredictions = [round(value) for value in y_pred]\n\taccuracy = accuracy_score(y_test, predictions)\n\tprint(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))","bcc191bc":"train_df = train","fc54e884":"train_X, test_X, train_Y, test_Y = train_test_split(train, train_labels.heart_disease_present, test_size=0.10, stratify=train_labels.heart_disease_present, random_state=42)\n\nprint(train_X.shape, test_X.shape)\nprint()\nprint('Number of rows in Train dataset:',train_X.shape[0])\n#print(train_Y['heart_disease_present'].value_counts())\nprint()\nprint('Number of rows in Test dataset:',test_X.shape[0])\n#print(test_Y['heart_disease_present'].value_counts())","3cd20232":"scaler = StandardScaler()\ntrain_X = scaler.fit_transform(train_X)\ntest_X = scaler.transform(test_X)","fbb57897":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nmodel = xgb.XGBClassifier()\nn_estimators = range(50, 400, 50)\nparam_grid = dict(n_estimators=n_estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(train_X, train_Y.values.ravel())\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n# plot\nplt.errorbar(n_estimators, means, yerr=stds)\nplt.title(\"XGBoost n_estimators vs Log Loss\")\nplt.xlabel('n_estimators')\nplt.ylabel('Log Loss')\nplt.savefig('n_estimators.png')","87cb5178":"# grid search\nmodel = xgb.XGBClassifier()\nmax_depth = range(1, 11, 2)\nprint(max_depth)\nparam_grid = dict(max_depth=max_depth)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold, verbose=1)\ngrid_result = grid_search.fit(train_X, train_Y.values.ravel())\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n# plot\nplt.errorbar(max_depth, means, yerr=stds)\nplt.title(\"XGBoost max_depth vs Log Loss\")\nplt.xlabel('max_depth')\nplt.ylabel('Log Loss')\nplt.savefig('max_depth.png')","0f0a74da":"%%time\n\nmodel_base = xgb.XGBClassifier(max_depth=1,\n                        subsample=0.33,\n                        objective='binary:logistic',\n                        n_estimators=50,\n                        learning_rate = 0.12)\neval_set = [(train_X, train_Y), (test_X, test_Y)]\nmodel_base.fit(train_X, train_Y.values.ravel(), early_stopping_rounds=15, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","9c5428e2":"# make predictions for test data\ny_pred = model_base.predict(test_X)\npredictions = [round(value) for value in y_pred]","ef4e2081":"# evaluate predictions\naccuracy = accuracy_score(test_Y, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","796c3a03":"# retrieve performance metrics\nresults = model_base.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","59048a80":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.metrics import roc_auc_score","b34f67ea":"train = train_X\nvalid = test_X\n\ny_train = train_Y\ny_valid = test_Y\n","958aed45":"def objective(space):\n\n    clf = xgb.XGBClassifier(n_estimators = 50,\n                            max_depth = space['max_depth'],\n                            min_child_weight = space['min_child_weight'],\n                            subsample = space['subsample'])\n\n    eval_set  = [( train, y_train), ( valid, y_valid)]\n\n    clf.fit(train, y_train,\n            eval_set=eval_set, eval_metric=\"auc\",\n            early_stopping_rounds=30)\n\n    pred = clf.predict_proba(valid)[:,1]\n    auc = roc_auc_score(y_valid, pred)\n    print (\"SCORE:\", auc)\n\n    return{'loss':1-auc, 'status': STATUS_OK }\n\n\n","975a91cf":"space ={\n        'max_depth': hp.choice(\"x_max_depth\", np.arange(5, 25, dtype=int)),\n        'min_child_weight': hp.choice ('x_min_child',np.arange(1, 10, dtype=int)),\n        'subsample': hp.uniform ('x_subsample', 0.8, 1)\n    }","80ea3cea":"trials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=100,\n            trials=trials)\n\nprint (best)","ed71386c":"%%time\n\nmodel = xgb.XGBClassifier(max_depth=19,\n                        subsample=0.8325428224507427,\n                          min_child = 6,\n                        objective='binary:logistic',\n                        n_estimators=50,\n                        learning_rate = 0.1)\neval_set = [(train_X, train_Y), (test_X, test_Y)]\nmodel.fit(train_X, train_Y.values.ravel(), early_stopping_rounds=15, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","05c1de7f":"# make predictions for test data\ny_pred_hyperopt = model.predict(test_X)\npredictions = [round(value) for value in y_pred_hyperopt]","b0501a5d":"# evaluate predictions\naccuracy = accuracy_score(test_Y, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","6304deba":"# retrieve performance metrics\nresults = model.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","9af051a8":"#X_train, X_test, y_train, y_test = train_test_split(train, train_labels.heart_disease_present,test_size=.1, random_state=42)\n\nprint(train_X.shape)\nprint(test_X.shape)\n\nprint(train_Y.shape)\nprint(test_Y.shape)","1c7e141f":"dtrain = xgb.DMatrix(train_X, label=train_Y)\ndtest = xgb.DMatrix(test_X, label=test_Y)","f2ced555":"params = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    'objective':'binary:logistic',\n     'n_estimators' :50,\n     'learning_rate' : 0.12\n}","6987afac":"params['eval_metric'] = \"logloss\"","84ecfc1d":"num_boost_round = 999","bae18c64":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10,\n    \n)\nprint(\"Best log loss: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))","f67ff241":"cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=42,\n    nfold=5,\n    metrics={'logloss'},\n    early_stopping_rounds=10\n)\ncv_results","a71a1f6e":"cv_results['test-logloss-mean'].min()","482fe8f4":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(1,25)\n    for min_child_weight in range(3,20)\n]","c0d1021f":"# Define initial best params and MAE\nmin_logloss = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'logloss'},\n        early_stopping_rounds=10\n    )\n    # Update best MAE\n    mean_logloss = cv_results['test-logloss-mean'].min()\n    boost_rounds = cv_results['test-logloss-mean'].argmin()\n    print(\"\\tlogloss {} for {} rounds\".format(min_logloss, boost_rounds))\n    if mean_logloss < min_logloss:\n        min_logloss = mean_logloss\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, logloss: {}\".format(best_params[0], best_params[1], min_logloss))","926639cc":"params['max_depth'] = 1\nparams['min_child_weight'] = 4","5fa8fa53":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(1,10)]\n    for colsample in [i\/10. for i in range(1,10)]\n]","9d5ea338":"min_logloss = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'logloss'},\n        early_stopping_rounds=10\n    )\n    # Update best score\n    mean_logloss = cv_results['test-logloss-mean'].min()\n    boost_rounds = cv_results['test-logloss-mean'].argmin()\n    print(\"\\tlogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n    if mean_logloss < min_logloss:\n        min_logloss = mean_logloss\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, logloss: {}\".format(best_params[0], best_params[1], min_logloss))","580b7401":"params['subsample'] = 0.8\nparams['colsample_bytree'] = 0.1","c0737c4a":"%time\n# This can take some time\u2026\nmin_logloss = float(\"Inf\")\nbest_params = None\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    # We update our parameters\n    params['eta'] = eta\n    # Run and time CV\n    %time cv_results = xgb.cv(params,dtrain,num_boost_round=num_boost_round,seed=42,nfold=5,metrics=['logloss'],early_stopping_rounds=10)\n    # Update best score\n    mean_logloss = cv_results['test-logloss-mean'].min()\n    boost_rounds = cv_results['test-logloss-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_logloss, boost_rounds))\n    if mean_logloss < min_logloss:\n        min_logloss = mean_logloss\n        best_params = eta\nprint(\"Best params: {}, logloss: {}\".format(best_params, min_logloss))","ed215069":"params['eta'] = 0.3","523728f4":"params","dcab1006":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=30\n)","9a4ac72a":"num_boost_round = model.best_iteration + 1\nbest_model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")]\n)","950a4a28":"print(train_X.shape)\nprint(test_X.shape)\n\nprint(train_Y.shape)\nprint(test_Y.shape)","cb7dfb0a":"xgb_model = xgb.XGBClassifier(params = params)\neval_set = [(train_X, train_Y), (test_X, test_Y)]\nxgb_model.fit(train_X, train_Y.values.ravel(), early_stopping_rounds=15, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","78e54e75":"# make predictions for test data\ny_pred_gs = xgb_model.predict(test_X)\npredictions = [round(value) for value in y_pred_gs]","c4237baa":"# evaluate predictions\naccuracy = accuracy_score(test_Y, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","712e2b95":"# retrieve performance metrics\nresults = xgb_model.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","ca0ec36f":"###################################################################################","70ac7d0b":"scaler = StandardScaler()\ntrain_df = scaler.fit_transform(train_df)","4547aa81":"in_sample_preds = xgb_model.predict_proba(train_df)","f703d352":"log_loss(train_labels.heart_disease_present, in_sample_preds)","f2792ffa":"test_values_subset = test","553b189f":"test_values_subset = scaler.transform(test_values_subset)","ab0476a5":"predictions = xgb_model.predict_proba(test_values_subset)[:, 1]","07fe9cb7":"predictions = np.round(predictions, 2)","00be268a":"submission_format = pd.read_csv('..\/input\/submission_format.csv', index_col='patient_id')","3d2ab7a1":"my_submission = pd.DataFrame(data=predictions,\n                             columns=submission_format.columns,\n                             index=submission_format.index)","4672f2f5":"my_submission.head(10)","0af6bca0":"my_submission.to_csv('submission_result.csv', index='patient_id')\nsubmission = pd.read_csv('submission_result.csv')","02e2907c":"submission.head()","fd870abe":"sample_preds = model_base.predict_proba(train_df)","174a2f6f":"log_loss(train_labels.heart_disease_present, sample_preds)","6e76ec27":"predictions_base = model_base.predict_proba(test_values_subset)[:, 1]","5267819d":"predictions_base = np.round(predictions_base, 2)","61bea17f":"submission_format_base = pd.read_csv('..\/input\/submission_format.csv', index_col='patient_id')","64b806cb":"submission_base = pd.DataFrame(data=predictions_base,\n                             columns=submission_format_base.columns,\n                             index=submission_format_base.index)","2ce0d4dd":"submission_base.head(10)","f8da1e7d":"submission_base.to_csv('submission_baseline.csv', index='patient_id')\nsubmission_b = pd.read_csv('submission_baseline.csv')","a5d4ce53":"submission_b.head()","fe3f2891":"#### XGBoost Parameters\n\n1. General Parameters: Guide the overall functioning\n2. Booster Parameters: Guide the individual booster (tree\/regression) at each step\n3. Learning Task Parameters: Guide the optimization performed\n\nFor more detail documentation, Please refer the blog post of [XGBoost](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/)","55025747":"#### Check for missing values","178146dd":"#### Submit base model predictions","9e7753be":"Resources:\n1. [Dataset](https:\/\/www.drivendata.org\/competitions\/54\/machine-learning-with-a-heart\/data\/)\n2. [XgBoost hyper parameter tuning blog](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/)\n3. [Hyperopt](https:\/\/www.dataiku.com\/learn\/guide\/code\/python\/advanced-xgboost-tuning.html)\n","33d51b30":"**Best params**: \n* subsample = 0.8\n* colsample = 0.1,\n* logloss: 0.40887080000000003","51b4a3cc":"#### Skew of Univariate Distributions","3ce2f99b":"As you can see we stopped before reaching the maximum number of boosting rounds, that\u2019s because after the 40th tree, adding more rounds did not lead to improvements of log loss on the test dataset.","ed57ab3f":"#### Load the test and training dataset","d325d6e3":"#### Baseline Score","c92afae1":"#### Submit the predictions ","6c91c246":"#### Using XGBoost\u2019s CV\n\nIn order to tune the other hyperparameters, we will use the cv function from XGBoost.\nt allows us to run cross-validation on our training dataset and returns a mean log loss.\n\nThe following are the parameters which are passed to the function:\n\n* **params**: our dictionary of parameters.\n* **dtrain** matrix.\n* **num_boost_round**: number of boosting rounds. Here we will use a large number again and count **early_stopping_rounds** to find the optimal number of rounds before reaching the maximum.\n* **seed**: random seed. It's important to set a seed here, to ensure we are using the same folds for each step so we can properly compare the scores with different parameters.\n* **nfold**: the number of folds to use for cross-validation\n* **metrics**: the metrics to use to evaluate our model, here we use log loss.","b7f49a0c":"#### Parameters num_boost_round and early_stopping_rounds","5cc976b8":"### Feature data example\n\nHere's an example of one of the rows in the dataset so that you can see the kinds of values you might expect in the dataset. Some are binary, some are integers, some are floats, and some are categorical. There are no missing values. \n\n|field |value |\n|------|------|\n|slope_of_peak_exercise_st_segment |2 |\n|thal | normal |\n|resting_blood_pressure | 125 |\n|chest_pain_type | 3 |\n|num_major_vessels | 0 |\n|fasting_blood_sugar_gt_120_mg_per_dl| 1 |\n|resting_ekg_results | 2 |\n|serum_cholesterol_mg_per_dl | 245 |\n|oldpeak_eq_st_depression | 2.4 |\n|sex | 1 |\n|age | 51 |\n|max_heart_rate_achieved | 166 |\n|exercise_induced_angina | 0 |","32ad0153":"XGBoost has a large number of advanced parameters, which can all affect the quality and speed of your model.\n\n* **max_depth** : int\n     Maximum tree depth for base learners.\n* **learning_rate** : float\n     Boosting learning rate (XGBoost's \"eta\")\n* **n_estimators** : int\n     Number of boosted trees to fit.\n* **silent** : boolean\n     Whether to print messages while running boosting.\n* **objective** : string\n     Specify the learning task and the corresponding learning objective.\n* **nthread** : int\n     Number of parallel threads used to run XGBoost.\n* **gamma** : float\n     Minimum loss reduction required to make a further partition on a leaf node of the tree.\n* **min_child_weight** : int\n     Minimum sum of instance weight(hessian) needed in a child.\n* **max_delta_step** : int\n     Maximum delta step we allow each tree's weight estimation to be.\n* **subsample** : float\n     Subsample ratio of the training instance.\n* **colsample_bytree** : float\n     Subsample ratio of columns when constructing each tree.\n* **base_score**:\n     The initial prediction score of all instances, global bias.\n* **seed** : int\n     Random number seed.\n* **missing** : float, optional\n     Value in the data which needs to be present as a missing value.\n     If None, defaults to np.nan.","40c48aa4":"**Best params**: \n* max_depth = 1,\n* min_child_weight = 4, \n* logloss: 0.4457426","7b3fff89":"Our goal is to predict the binary class heart_disease_present, which represents whether or not a patient has heart disease:\n\n**0 represents no heart disease present**  \n**1 represents heart disease present**","e3709544":"#### One hot Encoding","59aa3654":"#### Feature Selection","4be5eedf":"#### Hyperparameter tuning in XGBoost","d86d1c8e":"#### Correlations Between Attributes","bc11070b":"#### Using Hyperopt For Grid Searching","4c5913b1":"#### Data Type For Each Attribute","2161b276":"#### Parameter ETA\n\nThe ETA parameter controls the learning rate. It corresponds to the shrinkage of the weights associated to features after each round, in other words it defines the amount of \"correction\" we make at each step","040345e6":"#### The params dictionary","795cf05e":"x_max_depth': 19, 'x_min_child': 6, 'x_subsample': 0.8325428224507427","5aaab517":"### Dataset\n\nThere are 14 columns in the dataset, where the patient_id column is a unique and random identifier. The remaining 13 features are described in the section below.\n\n* **slope_of_peak_exercise_st_segment** (type: int): the slope of the peak exercise ST segment, an electrocardiography read out indicating quality of blood flow to the heart\n* **thal** (type: categorical): results of thallium stress test measuring blood flow to the heart, with possible values normal, fixed_defect, reversible_defect\n* **resting_blood_pressure** (type: int): resting blood pressure\n* **chest_pain_type** (type: int): chest pain type (4 values)\n* **num_major_vessels** (type: int): number of major vessels (0-3) colored by flourosopy\n* **fasting_blood_sugar_gt_120_mg_per_dl** (type: binary): fasting blood sugar > 120 mg\/dl\n* **resting_ekg_results** (type: int): resting electrocardiographic results (values 0,1,2)\n* **serum_cholesterol_mg_per_dl** (type: int): serum cholestoral in mg\/dl\n* **oldpeak_eq_st_depression** (type: float): oldpeak = ST depression induced by exercise relative to rest, a measure of abnormality in electrocardiograms\n* **sex** (type: binary): 0: female, 1: male\n* **age** (type: int): age in years\n* **max_heart_rate_achieved** (type: int): maximum heart rate achieved (beats per minute)\n* **exercise_induced_angina** (type: binary): exercise-induced chest pain (0: False, 1: True)","6d7314fa":"**Best params**: \n* ETA = 0.3,\n* logloss: 0.40887080000000003","bf9868aa":"This score is less than our baseline score. We have to tune the model further.\nThe classification error is very high in test dataset","914f0089":"#### Feature Importance","09516e43":"let's update our params","00dc583b":"#### Parameters subsample and colsample_bytree\n\nInstead of using the whole training set every time, we can build a tree on slightly different data at each step, which makes it less likely to overfit to a single sample or feature.\n\n* **subsample** corresponds to the fraction of observations (the rows) to subsample at each step. By default it is set to 1 meaning that we use all rows.\n* **colsample_bytree corresponds** to the fraction of features (the columns) to use. By default it is set to 1 meaning that we will use all features.","7b619728":"cv returns a table where the rows correspond to the number of boosting trees used, here again, we stopped before the 999 rounds.\n\nThe 4 columns correspond to the mean and standard deviation of logloss on the test dataset and on the train dataset. For this tutorial we will only try to improve the mean test logloss. We can get the lowest logloss score from cv with:","3c4b44de":"#### Import the Libraries","20fed010":"#### Dimensions of the Data","38d9f52e":"#### Class Distribution","c766bebc":"#### Parameters max_depth and min_child_weight\n\nThose parameters add constraints on the architecture of the trees.\n \n* **max_depth** is the maximum number of nodes allowed from the root to the farthest leaf of a tree. Deeper trees can model more complex relationships by adding more nodes, but as we go deeper, splits become less relevant and are sometimes only due to noise, causing the model to overfit.\n* **min_child_weight** is the minimum weight (or number of samples if all samples have a weight of 1) required in order to create a new node in the tree. A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit.\n"}}