{"cell_type":{"7294f30e":"code","ae63593f":"code","498e5625":"code","7fe916b8":"code","6a954ce8":"code","0ffd2618":"code","e28fa0d9":"code","a3aa8962":"code","6ac6e977":"code","48b4bcc1":"code","8ba0d175":"markdown","a4950f2c":"markdown"},"source":{"7294f30e":"!pip install -q git+https:\/\/github.com\/philferriere\/cocoapi.git#subdirectory=PythonAPI\nfrom pathlib import Path\nimport random\nfrom typing import Any, Callable, List, Optional, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pycocotools.coco import COCO\nimport skimage.io as io\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import io, transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm.auto import tqdm\n\n%matplotlib inline","ae63593f":"ROOT_PATH = Path(\"\/kaggle\/input\/coco-2017-dataset\/coco2017\/\")\nBATCH_SIZE = 64\nIMAGE_SIZE = (128, 128)","498e5625":"train_annotations = COCO(ROOT_PATH \/ \"annotations\/instances_train2017.json\")\nvalid_annotations = COCO(ROOT_PATH \/ \"annotations\/instances_val2017.json\")","7fe916b8":"cat_ids = train_annotations.getCatIds(supNms=[\"person\", \"vehicle\"])\ntrain_img_ids = []\nfor cat in cat_ids:\n    train_img_ids.extend(train_annotations.getImgIds(catIds=cat))\n    \ntrain_img_ids = list(set(train_img_ids))\nprint(f\"Number of training images: {len(train_img_ids)}\")\n\nvalid_img_ids = []\nfor cat in cat_ids:\n    valid_img_ids.extend(valid_annotations.getImgIds(catIds=cat))\n    \nvalid_img_ids = list(set(valid_img_ids))\nprint(f\"Number of validation images: {len(valid_img_ids)}\")","6a954ce8":"i = 22\nimg_data = train_annotations.loadImgs(train_img_ids[i])\nann_ids = train_annotations.getAnnIds(imgIds=img_data[0]['id'], catIds=cat_ids, iscrowd=None)\nanns = train_annotations.loadAnns(ann_ids)\nmask = np.max(np.stack([train_annotations.annToMask(ann) * ann[\"category_id\"] for ann in anns]), axis=0)\n\nimg = io.read_image(str(ROOT_PATH \/ \"train2017\" \/ img_data[0][\"file_name\"]))\/255.0\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.imshow(TF.to_pil_image(img))\nplt.subplot(122)\nplt.imshow(mask)\nplt.show()","0ffd2618":"class ImageData(Dataset):\n    def __init__(\n        self, \n        annotations: COCO, \n        img_ids: List[int], \n        cat_ids: List[int], \n        root_path: Path, \n        transform: Optional[Callable]=None\n    ) -> None:\n        super().__init__()\n        self.annotations = annotations\n        self.img_data = annotations.loadImgs(img_ids)\n        self.cat_ids = cat_ids\n        self.files = [str(root_path \/ img[\"file_name\"]) for img in self.img_data]\n        self.transform = transform\n        \n    def __len__(self) -> int:\n        return len(self.files)\n    \n    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.LongTensor]:\n        ann_ids = self.annotations.getAnnIds(\n            imgIds=self.img_data[i]['id'], \n            catIds=self.cat_ids, \n            iscrowd=None\n        )\n        anns = self.annotations.loadAnns(ann_ids)\n        mask = torch.LongTensor(np.max(np.stack([self.annotations.annToMask(ann) * ann[\"category_id\"] \n                                                 for ann in anns]), axis=0)).unsqueeze(0)\n        \n        img = io.read_image(self.files[i])\n        if img.shape[0] == 1:\n            img = torch.cat([img]*3)\n        \n        if self.transform is not None:\n            return self.transform(img, mask)\n        \n        return img, mask","e28fa0d9":"def train_transform(\n    img1: torch.LongTensor, \n    img2: torch.LongTensor\n) -> Tuple[torch.LongTensor, torch.LongTensor]:\n    params = transforms.RandomResizedCrop.get_params(img1, scale=(0.5, 1.0), ratio=(0.75, 1.33))\n    \n    img1 = TF.resized_crop(img1, *params, size=IMAGE_SIZE)\n    img2 = TF.resized_crop(img2, *params, size=IMAGE_SIZE)\n    \n    # Random horizontal flipping\n    if random.random() > 0.5:\n        img1 = TF.hflip(img1)\n        img2 = TF.hflip(img2)\n        \n    return img1, img2","a3aa8962":"train_data = ImageData(train_annotations, train_img_ids, cat_ids, ROOT_PATH \/ \"train2017\", train_transform)\nvalid_data = ImageData(valid_annotations, valid_img_ids, cat_ids, ROOT_PATH \/ \"val2017\", train_transform)\n\ntrain_dl = DataLoader(\n    train_data,\n    BATCH_SIZE, \n    shuffle=True, \n    drop_last=True, \n    num_workers=4,\n    pin_memory=True,\n)\n\nvalid_dl = DataLoader(\n    valid_data,\n    BATCH_SIZE, \n    shuffle=False, \n    drop_last=False, \n    num_workers=4,\n    pin_memory=True,\n)","6ac6e977":"img, mask = train_data[22]\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.imshow(TF.to_pil_image(img))\nplt.subplot(122)\nplt.imshow(mask.squeeze())\nplt.show()","48b4bcc1":"x, y = next(iter(train_dl))\nprint(x.shape, y.shape)\n# for x, y in tqdm(train_dl):\n#     continue","8ba0d175":"# Coco Semantic Segmentation in PyTorch - Data Prep\n\nThis post describes how to use the coco dataset for semantic segmentation. Kudos to [this blog](https:\/\/towardsdatascience.com\/master-the-coco-dataset-for-semantic-image-segmentation-part-1-of-2-732712631047) for giving me the necessary hints to create this.","a4950f2c":"Run the following commented out section to see how long data loading takes. It takes approximately 10 minutes to run through an epoch without any modelling."}}