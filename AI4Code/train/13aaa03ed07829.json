{"cell_type":{"365e4d9a":"code","a7a77bf0":"code","81800a58":"code","e79b1835":"code","2ddb3f76":"code","8f9a7339":"code","d58210d9":"code","1430bcbc":"code","8fd86011":"code","4fe5e81e":"markdown"},"source":{"365e4d9a":"import numpy as np\nimport pandas as pd","a7a77bf0":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/churn-predictions-personal\/Churn_Predictions.csv\", index_col=0)\ndf.head()","81800a58":"print(df.shape)\ndf.tail()","e79b1835":"df.info()","2ddb3f76":"df.describe()","8f9a7339":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nimport warnings\nwarnings.filterwarnings('ignore')","d58210d9":"def Normalize(df):\n    cols = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']\n    return (df[cols] - df[cols].mean()) \/ df[cols].std()\n\ndef Categorize(df):\n    cols = ['Geography', 'Gender']\n    return df[cols].astype('category')\n    \ndef OneHot(df):\n#     cols = ['NumOfProducts']\n    cols = ['Geography', 'Gender', 'NumOfProducts']\n    return pd.get_dummies(df[cols], drop_first=True)","1430bcbc":"FunctionTransformer(OneHot, validate=False).fit_transform(df).head()","8fd86011":"preprocess = FeatureUnion([\n    ('normalize', FunctionTransformer(Normalize, validate=False)),\n#     ('categorize', FunctionTransformer(Categorize, validate=False)),\n    ('onehot', FunctionTransformer(OneHot, validate=False)),\n])\n\npipe = Pipeline([\n    ('union', preprocess),\n    ('clf', LogisticRegression())\n])\n\nX_train, X_test, y_train, y_test = train_test_split(df, df[['Exited']], test_size=0.3)\npipe.fit(X_train, y_train)\naccuracy = pipe.score(X_test, y_test)\nprint('acc: {:.2f}'.format(accuracy))","4fe5e81e":"Ayush Modi\n#general_discussions\nhttps:\/\/bertelsmannai.slack.com\/archives\/CQEPP77BP\/p1579177141420800\n\nHello everyone!\nSo today, instead of working further through the lessons, I decided to take a break and build a model on this Churn Modeling dataset.\nSo you're given a dataset from a bank of 10,000 customers, with details about them like Credit Score, Age, Salaries, Balance, No. of products(like whether the customer has a credit card, whether they have a loan, etc), Geography, etc\nWhat's the motive? Well, the bank's been seeing unusual Churn rates(Churn's when people leave the company), so you wanna look into the data collected and give them some insights. The 'exited' column indicates whether a person left the bank or not, 1 indicating the person did leave the bank, 0 indicating that they didn't. So your task would be given some information about a customer, predict whether or not they end up leaving the bank.\nThe reason I'm sharing this here is that I was able to do the problem with just the knowledge I've gained from lesson 3 and lesson 5 (I didn't know anything about PyTorch prior to the course). Therefore, if I can, then so can you.\nI'd like to encourage all of you to give the problem a try. If you've completed everything in the course till lesson 5, you should be able to do this! To break it down, it's a simple binary classification task(but unlike the notebooks, this one isn't on images. That shouldn't be a problem though, the same basic idea still works :))\n(As heads up, the notebooks I had to go back and refer to, primarily were the ones in lesson 3 - 3.35 for data manipulation using pandas, and a couple of notebooks from lesson 5 for defining my model using nn.Sequential(), and that's pretty much it! Easy innit?)\nSo if you'd like to give it a try, well go ahead! Perhaps we could compare our accuracies to get the game goin! Drop in yours by replying to the thread!\nI'm attaching the dataset, which you'll need to download and then work on it. Good luck! Tag everyone who you think might be interested <3\nPS. The best accuracy I could achieve was 87.7% .. Time for you to beat that ;) (edited) "}}