{"cell_type":{"a802ad59":"code","0b0feb1b":"code","caa336c7":"code","c157b493":"code","c0ceb3be":"code","3cbeae2e":"code","5393f7be":"code","27994755":"code","79f468f1":"code","3d0c1837":"code","df924d2f":"code","ad58c433":"code","cc6a6a1c":"code","805967a8":"code","d9b93b5d":"code","556b2c45":"code","7a9f5618":"code","c0582167":"code","40728078":"code","de1be22f":"markdown","2fd7227e":"markdown","7841d34d":"markdown","0b5e0866":"markdown","65a83ad6":"markdown","e9b279e9":"markdown","21d9b264":"markdown","a34bcdaf":"markdown","331cc461":"markdown","d6131151":"markdown","efdd0f85":"markdown"},"source":{"a802ad59":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import LinearSVC\n\n\nimport os\nprint(os.listdir(\"..\/input\"))","0b0feb1b":"df=pd.read_csv('..\/input\/train.csv')","caa336c7":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('target', data=df, palette=colors)\nplt.title('Class Distributions \\n (0: No Transaction || 1: Transaction)', fontsize=14)","c157b493":"print('No Transaction', round(df['target'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Transaction', round(df['target'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","c0ceb3be":"from sklearn.preprocessing import StandardScaler\n\nX = df.drop('target', axis=1)\nX=df.drop('ID_code', axis=1)\ny = df['target']\nX = StandardScaler().fit_transform(X)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nX_pca_df = pd.DataFrame(data = X_pca)","3cbeae2e":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Transaction', round(df['target'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Transaction', round(df['target'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    Xtrain_pca, Xtest_pca = X_pca_df.iloc[train_index], X_pca_df.iloc[test_index]\n    ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n\n\n# Turn into an array\nXtrain_pca = Xtrain_pca.values\nXtest_pca = Xtest_pca.values\nytrain = ytrain.values\nytest = ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(ytrain))\nprint(test_counts_label\/ len(ytest))","5393f7be":"def samplecompare (Xtrain,Xtest,ytrain,ytest):\n    clf = LinearSVC( random_state=123)\n    clf.fit(Xtrain, ytrain)\n    score=clf.score(Xtest,ytest)\n    \n    w = clf.coef_[0]\n\n    a = -w[0] \/ w[1]\n\n    x_min=Xtrain[:,0].min()\n    x_max=Xtrain[:,0].max()\n\n    y_min=Xtrain[:,1].min()\n    y_max=Xtrain[:,1].max()\n\n\n    xx = np.linspace(x_min,x_max,10)\n    yy = a * xx - clf.intercept_[0] \/ w[1]\n    h0 = plt.plot(xx, yy, 'k-', label=\"non weighted div\")\n\n    plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c = ytrain)\n    plt.ylim([y_min,y_max])\n    plt.xlim([x_min,x_max])\n    plt.show()\n    \n    print('Accuaracy score of this sampling method ' +str(score))\n    return","27994755":"from imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state=42)\nXtrain_under, ytrain_under = rus.fit_resample(Xtrain_pca, ytrain)\n\nXtest_under, ytest_under = rus.fit_resample(Xtest_pca, ytest)","79f468f1":"sns.countplot(ytrain_under, palette=colors)\nplt.title('Class Distributions \\n (0: No Transaction || 1: Transaction)', fontsize=14)","3d0c1837":"samplecompare(Xtrain_under,Xtest_under,ytrain_under,ytest_under)","df924d2f":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\n\nXtrain_over, ytrain_over = ros.fit_resample(Xtrain_pca, ytrain)\n\nXtest_over, ytest_over = ros.fit_resample(Xtest_pca, ytest)","ad58c433":"sns.countplot(ytrain_over, palette=colors)\nplt.title('Class Distributions \\n (0: No Transaction || 1: Transaction)', fontsize=14)","cc6a6a1c":"samplecompare(Xtrain_over, Xtest_over,ytrain_over,ytest_over)","805967a8":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=42)\nXtrain_sm, ytrain_sm = sm.fit_resample(Xtrain_pca, ytrain)\nXtest_sm, ytest_sm = sm.fit_resample(Xtest_pca, ytest)","d9b93b5d":"sns.countplot(ytrain_sm, palette=colors)\nplt.title('Class Distributions \\n (0: No Transaction || 1: Transaction)', fontsize=14)","556b2c45":"samplecompare(Xtrain_sm,Xtest_sm,ytrain_sm,ytest_sm)","7a9f5618":"from imblearn.over_sampling import ADASYN\nadn=ADASYN(random_state=1)\nXtrain_ad, ytrain_ad = adn.fit_resample(Xtrain_pca, ytrain)\nXtest_ad, ytest_ad = adn.fit_resample(Xtest_pca, ytest)","c0582167":"sns.countplot(ytrain_ad, palette=colors)\nplt.title('Class Distributions \\n (0: No Transaction || 1: Transaction)', fontsize=14)","40728078":"samplecompare(Xtrain_ad,Xtest_ad,ytrain_ad,ytest_ad)","de1be22f":"### Synthetic Minority Over-sampling TEchnique (SMOTE)","2fd7227e":"## sub-sampling","7841d34d":" Developing a model on imbalanced data is problematic as the model could be biased and inaccurate. It is therefore important to make the data more balanced before estimating a model. \n \n In this notebook, we will scrutinize the effect of different sampling methods (under-sampling, over-sampling, SMOTE and ADASYN) on the performance of a Linear support vector classification model. ","0b5e0866":"### Adaptive Synthetic sampling method (ADASYN)\n","65a83ad6":"### Creating function to compare performance of different sampling models","e9b279e9":"### Over-sample","21d9b264":"# Conclusion","a34bcdaf":"### Splitting the data","331cc461":"# PCA\nThe original dataset contains 200 explanatory variables. As it is not the goal of this notebook to create the optimal model and to decrease calculation time , it was decided to decrease the dimensionality of the dataset considerably. \n\nIn the end, the dimensionality was reduced to 2 dimensions as this makes the visualization of the output more straightforward.","d6131151":"The imbalenced dataset that was chosen for this test, is the Santander Customer Transaction dataset. The target value of this dataset is a boolean indicating whether a certain transaction took place. As these transactions are quite rare, the dataset is highly imbalanced.","efdd0f85":"### Under-sampling"}}