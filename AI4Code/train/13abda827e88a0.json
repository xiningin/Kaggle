{"cell_type":{"af74b40c":"code","77f65c3c":"code","190445e0":"code","173a16cb":"code","8001aacb":"code","104960fe":"code","db5bb795":"code","aa591de6":"code","8c5cf15c":"code","436f223a":"code","7888b4fb":"code","5a8f3c5f":"code","534c4ba3":"code","c23b877b":"code","962b3066":"code","0471994a":"code","7b289af3":"code","e0ce7991":"code","176c389c":"code","d924f4c6":"code","4f19cc23":"code","26867e3e":"code","b8079706":"code","12ca5800":"code","cb880aa8":"code","69539733":"code","855f5d16":"code","90939c1c":"code","6ba34456":"code","88f512c7":"code","df815ed6":"code","da512855":"code","ebd39bd2":"code","67b9ea4c":"code","2bee9a2a":"code","179f57ea":"code","00ca0ef2":"code","c72b2bc1":"code","95c6ff65":"code","3703fc02":"code","c2725fd5":"markdown","40d4e1ff":"markdown","8e11edad":"markdown","3005c8a2":"markdown","d41644d8":"markdown","c9ec5063":"markdown","129ac18d":"markdown","5ba04eba":"markdown","52a6b4fa":"markdown","4394eaf8":"markdown","79818381":"markdown","57cf57bf":"markdown","4dd1e689":"markdown","1da97e2c":"markdown","4d765d09":"markdown","30c1b46d":"markdown","4b84485d":"markdown","0d91b1e5":"markdown","7e4fab19":"markdown","579dd6ad":"markdown","f40b8eb6":"markdown","dea00f08":"markdown","53a6e107":"markdown","4795bab9":"markdown","d8594bc0":"markdown","a300db47":"markdown","0b908026":"markdown","03022f51":"markdown","362e895c":"markdown","c94d9767":"markdown","54e63303":"markdown","7263d0e5":"markdown","fa1580f7":"markdown","9776b84b":"markdown","76eb4094":"markdown","96a24605":"markdown","82623b51":"markdown","d7163a95":"markdown","912e8a21":"markdown"},"source":{"af74b40c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\nfrom itertools import cycle, islice\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold","77f65c3c":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","190445e0":"train_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv', index_col='TransactionID')\nsample_submission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')","173a16cb":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)","8001aacb":"del train_transaction, train_identity\ndel test_transaction, test_identity\ngc.collect()","104960fe":"def downcast_df_float_columns(df):\n    list_of_columns = list(df.select_dtypes(include=[\"float64\"]).columns)\n        \n    if len(list_of_columns)>=1:\n        max_string_length = max([len(col) for col in list_of_columns])\n        print(\"downcasting float for:\", list_of_columns, \"\\n\")\n        \n        for col in list_of_columns:\n            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n    else:\n        print(\"no columns to downcast\")\n    gc.collect()\n    print(\"done\")","db5bb795":"def downcast_df_int_columns(df):\n    list_of_columns = list(df.select_dtypes(include=[\"int32\", \"int64\"]).columns)\n        \n    if len(list_of_columns)>=1:\n        max_string_length = max([len(col) for col in list_of_columns])\n        print(\"downcasting integers for:\", list_of_columns, \"\\n\")\n        \n        for col in list_of_columns:\n            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n    else:\n        print(\"no columns to downcast\")\n    gc.collect()\n    print(\"done\")","aa591de6":"train.head()","8c5cf15c":"train.info()","436f223a":"train.describe()","7888b4fb":"plt.bar(['train', 'test'], [len(train), len(test)], width=0.2, color='b')\nplt.title(\"Number of train set and test set instances \")\nplt.show()","5a8f3c5f":"plt.bar(['train', 'test'], [train.isnull().any().sum(), test.isnull().any().sum()], width=0.2, color='g')\nplt.title(\"Number of column with null values\")\nplt.show()","534c4ba3":"train_missing_values = train.isnull().sum().sort_values(ascending=False) \/ len(train)\ntest_missing_values = test.isnull().sum().sort_values(ascending=False) \/ len(test)\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\nsns.barplot(list(train_missing_values.keys()[:10]), train_missing_values[:10], ax=axes[0])\nsns.barplot(list(test_missing_values.keys()[:10]), test_missing_values[:10], ax=axes[1])\nplt.show()","c23b877b":"def show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() \/ 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.2f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n\nplt.figure(figsize=(5, 4))\nax = sns.barplot([\"fraud\", \"not fraud\"],\n            [len(train[train.isFraud == 1])\/len(train),\n             len(train[train.isFraud == 0])\/len(train)])\nshow_values_on_bars(ax)\nplt.show()","962b3066":"train[\"hour\"] = np.floor(train[\"TransactionDT\"] \/ 3600) % 24\ntest[\"hour\"] = np.floor(train[\"TransactionDT\"] \/ 3600) % 24","0471994a":"plt.plot(train.groupby('hour').mean()['isFraud'], color='r')\nax = plt.gca()\nax2 = ax.twinx()\n_ = ax2.hist(train['hour'], alpha=0.3, bins=24)\nax.set_xlabel('Encoded hour')\nax.set_ylabel('Fraction of fraudulent transactions')\n\nax2.set_ylabel('Number of transactions')\nplt.show()","7b289af3":"train[\"DeviceType\"].value_counts(dropna=False).plot.bar()\nplt.show()","e0ce7991":"plt.figure(figsize=(8, 8))\nsns.barplot(train[\"DeviceInfo\"].value_counts(dropna=False)[:15], \n            train[\"DeviceInfo\"].value_counts(dropna=False).keys()[:15])\nplt.show()","176c389c":"my_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(train.P_emaildomain.value_counts())))\ntrain.P_emaildomain.value_counts().plot.bar(figsize=(20, 10), color=my_colors)\nplt.show()","d924f4c6":"plt.figure(figsize=(6, 6))\nplt.pie([np.sum(train[(train['P_emaildomain'] == 'protonmail.com')].isFraud.values),\n                                 len(train[(train['P_emaildomain'] == 'protonmail.com')].isFraud.values) - \n                                 np.sum(train[(train['P_emaildomain'] == 'protonmail.com')].isFraud.values)],\n        labels=['isFraud', 'notFraud'], autopct='%1.1f%%')\nplt.show()","4f19cc23":"train['is_proton_mail'] = (train['P_emaildomain'] == 'protonmail.com') | (train['R_emaildomain']  == 'protonmail.com')\ntest['is_proton_mail'] = (test['P_emaildomain'] == 'protonmail.com') | (test['R_emaildomain']  == 'protonmail.com')","26867e3e":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n\nprint('done')","b8079706":"a = np.zeros(train.shape[0])\ntrain[\"lastest_browser\"] = a\na = np.zeros(test.shape[0])\ntest[\"lastest_browser\"] = a\ndef setbrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\ntrain=setbrowser(train)\ntest=setbrowser(test)","12ca5800":"plt.figure(figsize=(6, 6))\nplt.pie([np.sum(train[(train['lastest_browser'] == True)].isFraud.values),\n                                 len(train[(train['lastest_browser'] == True)].isFraud.values) - \n                                 np.sum(train[(train['lastest_browser'] == True)].isFraud.values)],\n        labels=['isFraud', 'notFraud'], autopct='%1.1f%%', colors=['y', 'g'])\nplt.show()","cb880aa8":"train_missing_values = [str(x) for x in train_missing_values[train_missing_values > 0.80].keys()]\ntest_missing_values = [str(x) for x in test_missing_values[test_missing_values > 0.80].keys()]\n\ndropped_columns = train_missing_values + test_missing_values","69539733":"dropped_columns = dropped_columns + [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\ndropped_columns = dropped_columns + [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\ndropped_columns.remove('isFraud')\n\ntrain.drop(dropped_columns, axis=1, inplace=True)\ntest.drop(dropped_columns, axis=1, inplace=True)\n\nlen(dropped_columns)","855f5d16":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')","90939c1c":"train['uid'] = train['card1'].astype(str)+'_'+train['card2'].astype(str)\ntest['uid'] = test['card1'].astype(str)+'_'+test['card2'].astype(str)\n\ntrain['uid2'] = train['uid'].astype(str)+'_'+train['card3'].astype(str)+'_'+train['card5'].astype(str)\ntest['uid2'] = test['uid'].astype(str)+'_'+test['card3'].astype(str)+'_'+test['card5'].astype(str)\n\ntrain['uid3'] = train['uid2'].astype(str)+'_'+train['addr1'].astype(str)+'_'+train['addr2'].astype(str)\ntest['uid3'] = test['uid2'].astype(str)+'_'+test['addr1'].astype(str)+'_'+test['addr2'].astype(str)\n\ntrain['TransactionAmt_check'] = np.where(train['TransactionAmt'].isin(test['TransactionAmt']), 1, 0)\ntest['TransactionAmt_check']  = np.where(test['TransactionAmt'].isin(train['TransactionAmt']), 1, 0)\n\ntrain['TransactionAmt'] = np.log1p(train['TransactionAmt'])\ntest['TransactionAmt'] = np.log1p(test['TransactionAmt'])    ","6ba34456":"for feature in ['id_36']:\n    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n        \nfor feature in ['id_01', 'id_31', 'id_35', 'id_36']:\n    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","88f512c7":"for col in ['card1']: \n    valid_card = pd.concat([train[[col]], test[[col]]])\n    valid_card = valid_card[col].value_counts()\n    valid_card = valid_card[valid_card>2]\n    valid_card = list(valid_card.index)\n\n    train[col] = np.where(train[col].isin(test[col]), train[col], np.nan)\n    test[col]  = np.where(test[col].isin(train[col]), test[col], np.nan)\n\n    train[col] = np.where(train[col].isin(valid_card), train[col], np.nan)\n    test[col]  = np.where(test[col].isin(valid_card), test[col], np.nan)","df815ed6":"numerical_columns = list(test.select_dtypes(exclude=['object']).columns)\n\ntrain[numerical_columns] = train[numerical_columns].fillna(train[numerical_columns].median())\ntest[numerical_columns] = test[numerical_columns].fillna(train[numerical_columns].median())\nprint(\"filling numerical columns null values done\")","da512855":"categorical_columns = list(filter(lambda x: x not in numerical_columns, list(test.columns)))\ncategorical_columns[:5]","ebd39bd2":"train[categorical_columns] = train[categorical_columns].fillna(train[categorical_columns].mode())\ntest[categorical_columns] = test[categorical_columns].fillna(train[categorical_columns].mode())\nprint(\"filling numerical columns null values done\")","67b9ea4c":"from sklearn.preprocessing import LabelEncoder\n\nfor col in categorical_columns:\n    le = LabelEncoder()\n    le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n    train[col] = le.transform(list(train[col].astype(str).values))\n    test[col] = le.transform(list(test[col].astype(str).values))","2bee9a2a":"labels = train[\"isFraud\"]\ntrain.drop([\"isFraud\"], axis=1, inplace=True)","179f57ea":"X_train, y_train = train, labels\ndel train, labels\ngc.collect()","00ca0ef2":"lgb_submission=sample_submission.copy()\nlgb_submission['isFraud'] = 0","c72b2bc1":"n_fold = 5\nfolds = KFold(n_fold)","95c6ff65":"for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n    print(fold_n)\n    \n    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n    \n    lgbclf = lgb.LGBMClassifier(\n            num_leaves= 512,\n            n_estimators=512,\n            max_depth=9,\n            learning_rate=0.064,\n            subsample=0.85,\n            colsample_bytree=0.85,\n            boosting_type= \"gbdt\",\n            reg_alpha=0.3,\n            reg_lamdba=0.243\n    )\n    \n    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n    lgbclf.fit(X_train_,y_train_)\n    \n    del X_train_,y_train_\n    print('finish train')\n    pred=lgbclf.predict_proba(test)[:,1]\n    val=lgbclf.predict_proba(X_valid)[:,1]\n    print('finish pred')\n    del lgbclf, X_valid\n    print('ROC accuracy: {}'.format(roc_auc_score(y_valid, val)))\n    del val,y_valid\n    lgb_submission['isFraud'] = lgb_submission['isFraud']+ pred\/n_fold\n    del pred\n    gc.collect()","3703fc02":"lgb_submission.insert(0, \"TransactionID\", np.arange(3663549, 3663549 + 506691))\nlgb_submission.to_csv('prediction.csv', index=False)","c2725fd5":"## Prepare Data for Modeling\n\n### Fill Nan Values\n\nFirst we should fill null values for both categorical and numerical values","40d4e1ff":"## Import Libraries","8e11edad":"Create submission file with name `prediction.csv`","3005c8a2":"Now, we find out categorical columns","d41644d8":"### Check Email Address","c9ec5063":"We can see Windows, iOS and MacOS are the most popular operating systems","129ac18d":"### Label Encoding","5ba04eba":"## Create Test Set and Train Set","52a6b4fa":"## Feature Engineering\n\nFirst we are going to drop columns with more than 80 precent of null values in both train and test set","4394eaf8":"Now we should `join` train and test dataset.\n\nWe use merge method.","79818381":"### Delve into TransactionDT\n\nAs we can see in **[this](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100071#latest-577632)** discussion, probably TransactionDT column is in seconds.\n\nWe can add day's hour to out dataset. **[Day and Time - powerful predictive feature?)](https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature)**","57cf57bf":"### Change Browser Label\n\nWe can check if browser is the lastest version of browser or not","4dd1e689":"We use cross validation with 5 folds","1da97e2c":"## Exploring Data","4d765d09":"Because of using cross validation, we use this step later when we want to find best model.\n\nNow, we have to remove isFraud column from daraframe","30c1b46d":"### Check Device Info","4b84485d":"We can see 10.7% of transactions which are done by users with lastest browser are fraud","0d91b1e5":"Then we should load csv datasets in pandas' dataframes.\n\n`TransactionID` is the index of all files, so we use index_col to use this column when loading files.","7e4fab19":"There are more columns with at least one null value in train set than in test set","579dd6ad":"<h1 style=\"text-align:center; color:DarkBlue\">Fraud Detection<\/h1>\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/en\/thumb\/2\/21\/IEEE_logo.svg\/500px-IEEE_logo.svg.png\" style=\"display:block;margin-left:25%;margin-right:auto;width:50%;\"\/>\n\n<div style=\"margin-left: 10px\">\n<a style=\"cursor:pointer\">1. Intorduction<\/a><br>\n<a style=\"cursor:pointer\">2. IEEE Fraud Detection<\/a><br>\n<a style=\"cursor:pointer\">3. Import Libraries<\/a><br>\n<a style=\"cursor:pointer\">4. Loading Files<\/a><br>\n<a style=\"cursor:pointer\">5. Reduce Memory Size<\/a><br>\n<a style=\"cursor:pointer\">6. Exploring Data<\/a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; <span>&#8226;<\/span>&emsp;Visualization<\/a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span>&#8226;<\/span>&emsp;Delve into TransactionDT<\/a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span>&#8226;<\/span>&emsp;Check Device Info<\/a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span>&#8226;<\/span>&emsp;Check Email Address<\/a><br>\n<a style=\"cursor:pointer\">7. Feature Engineering<\/a><br>\n<a style=\"cursor:pointer\">8. Prepare Data for Modeling<\/a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span>&#8226;<\/span>&emsp;Fill Nan Values<\/a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span>&#8226;<\/span>&emsp;Label Encoding<\/a><br>\n<a style=\"cursor:pointer\">9. Create Test Set and Train Set<\/a><br>\n<a style=\"cursor:pointer\">10. Finding Best Model<\/a>\n<\/div>\n\n<br>\n\nThanks to this kernels (Refrences) : \n* [feature-engineering-lightgbm-corrected](https:\/\/www.kaggle.com\/davidcairuz\/feature-engineering-lightgbm-corrected\")\n* [eda-and-models](https:\/\/www.kaggle.com\/artgor\/eda-and-models)\n* [feature-engineering-lightgbm-corrected](https:\/\/www.kaggle.com\/davidcairuz\/feature-engineering-lightgbm-corrected)\n* [reducing-memory-size-for-ieee](https:\/\/www.kaggle.com\/mjbahmani\/reducing-memory-size-for-ieee)\n* [day-and-time-powerful-predictive-feature](https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature)\n\n<br>\n\n## Intorduction\nIn law, `fraud` is intentional deception to secure unfair or unlawful gain, or to deprive a victim of a legal right. Fraud can violate civil law (i.e., a fraud victim may sue the fraud perpetrator to avoid the fraud or recover monetary compensation), a criminal law (i.e., a fraud perpetrator may be prosecuted and imprisoned by governmental authorities), or it may cause no loss of money, property or legal right but still be an element of another civil or criminal wrong.The purpose of fraud may be monetary gain or other benefits, for example by obtaining a passport, travel document, or driver's license, or mortgage fraud, where the perpetrator may attempt to qualify for a mortgage by way of false statements. [Fraud](https:\/\/en.wikipedia.org\/wiki\/Fraud)\n<br>\n\n`Fraud detection` is a set of activities undertaken to prevent money or property from being obtained through false pretenses. Fraud detection is applied to many industries such as banking or insurance. In banking, fraud may include forging checks or using stolen credit cards. Other forms of fraud may involve exaggerating losses or causing an accident with the sole intent for the payout.<br> [Data analysis techniques for fraud detection](https:\/\/en.wikipedia.org\/wiki\/Data_analysistechniques_for_fraud_detection)","f40b8eb6":"Changing label of emails with their address from P_emaildomain and R_emaildomain columns","dea00f08":"We can `visualize` to see if there is any relationship between day's hour and fraud\n\nIt's showing that in which hours, the rate of fraud is `greater`","53a6e107":"We have to use classification method on this dataset and find out which of these instances are seem to be fraud; actually, we should find the probability of being fraud for each of these instances.\n<br><br>\nAs you can see the dataset, it is too big and working on that may take a lot of time, therefore we should reduce the size of data either by using PCA(Principle Component Analysis) or by downcasting integer and float columns.\n<br>\n<br>\nThe submission is measured with calculating  `AUC - ROC Curve`\n<br><br>\n<b>What is AUC - ROC Curve?<\/b><br>\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\nThe ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.<br>[Understanding AUC - ROC Curve](https:\/\/towardsdatascience.com\/understanding-auc-roc-curve-68b2303cc9c5)\n<br>\n\n<img src=\"https:\/\/miro.medium.com\/max\/722\/1*pk05QGzoWhCgRiiFbz-oKQ.png\" style=\"display:block;margin-left:25%;margin-right:auto;width:40%;\"\/>","4795bab9":"Then we should drop columns that have more than 90 precent of a same value.\n\nWe must notice that the label column (isFraud) also has more than 90% of the same value, therefore we have to remove it from dropped columns.","d8594bc0":"Train dataset has 399 float, 3 int and 31 object(category) columns.","a300db47":"then, fill missing values in categorical columns","0b908026":"We can see that more than 40% of instnces which have `P_emaildomain` equal to `protonmail.com` are Fraud and this shows that we should check if P_emaildomain or R_emaildomain is equal to protonmail.com or not","03022f51":"Adding some feature to make prediction better","362e895c":"Check which type of devices has been used","c94d9767":"This is a function that downcast the integer columns","54e63303":"## Finding Best Model\n\nuse sample_submission file for final submission","7263d0e5":"Another way which is told earlier is to `downcast` float and integer columns.\n\nThis is a function that downcast the float columns","fa1580f7":"Show the percentage of fraud and not-fraud instances\n\nWe can see up to 97% of instances are not fruad and only 3% of data are labeled fraud; as a result, we ought to take care of our model not being `overfitted`","9776b84b":"## Reduce Memory Size\n\nAs mentioned above, this dataset is feeding on memory, so we should use anything to reduce memory usage.\n\nOne we to dimnish memory usage is calling `garbage collector` just after we don't need a dataframe.\n\nTo dimnish `memory usage`, we delete previous dataframes and call garbage collector to collet this unrefrence data.","76eb4094":"Encode categorical columns","96a24605":"## Loading Files\n\nFirst we should check what files we have in input directory.\n\nWe have four files two of which are train and the other two are test files.","82623b51":"## IEEE Fraud Detection\n<div style=\"text-align: justify\">In this competition, we will benchmark `machine learning models` on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. We also have the opportunity to create new features to improve your results. [Vesta Coopration](https:\/\/trustvesta.com\/)<\/div>\n<br>\n\n<img src=\"http:\/\/news.mit.edu\/sites\/mit.edu.newsoffice\/files\/styles\/news_article_image_top_slideshow\/public\/images\/2018\/MIT-Fraud-Detection-PRESS_0.jpg?itok=laiU-5nR\" style=\"display:block;margin-left:25%;margin-right:auto;width:50%;\"\/>","d7163a95":"Find missing values in train and test set\n\nThese are columns with the most `null instances` in both train set and test set","912e8a21":"Using LGBM for finding best model"}}