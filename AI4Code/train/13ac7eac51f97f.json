{"cell_type":{"6c083dc8":"code","1ce6b247":"code","753fc08f":"code","c000584c":"code","3254bdda":"code","ebf22309":"code","3e3699c7":"code","8b05d0ab":"code","8af88513":"code","9cb0ef1f":"code","e6ffc4b7":"code","284cf2f5":"code","18b915c5":"code","9a5e0500":"code","808eee16":"code","3196e78d":"code","cc259250":"code","42128666":"code","b958bb3c":"code","b76c0ddf":"code","ba0f552b":"code","a5ad5342":"code","1ba0673d":"code","9b43de04":"code","b9a38cab":"code","31cd2476":"code","5d853117":"code","95abc9ff":"code","e492f19c":"code","be4ae2f7":"code","a958fa49":"code","e250d3f7":"code","c143fcff":"code","a638e0cc":"code","cd9f134a":"code","82bac73d":"code","7e9d5666":"code","c16faf18":"code","21fe98ec":"code","f4898e12":"code","e08e6202":"code","dbe5b38e":"code","42accd99":"code","ae4724cd":"code","0134bc55":"code","fa88ffdd":"code","415ef655":"code","d553fc26":"code","948cb2d2":"code","b6f5496a":"code","5b469d4a":"code","ffecee3d":"code","10e085f4":"code","a3428b5a":"code","a34b157e":"code","1809a485":"code","560f46b3":"code","0abfd2a9":"code","48a07bb6":"code","34d0ddd1":"code","985b8047":"code","6fd23302":"code","7d3a9bba":"code","09c60afe":"code","667ada05":"code","928f555a":"code","ee503685":"code","73d7c460":"code","c946b295":"code","0cacc6e5":"code","8a325964":"code","95f27aed":"code","18aa3cab":"code","0454b39a":"code","654f0426":"code","565e82e0":"markdown","f43eafea":"markdown","8048411e":"markdown","2f730fc8":"markdown","59ea056d":"markdown","1cf20711":"markdown","5c8121bb":"markdown","8e5c43b3":"markdown","c34ba8db":"markdown"},"source":{"6c083dc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ce6b247":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()","753fc08f":"#importing dataset and displaying\ntrain_df=pd.read_excel(r'..\/input\/flight-fare-prediction-mh\/Data_Train.xlsx')\ntrain_df","c000584c":"#displaying the max columns\npd.set_option('display.max_columns',None)\ntrain_df.head()","3254bdda":"#displaying the info of each column\ntrain_df.info()","ebf22309":"#unique values in the duration column of dataset\ntrain_df['Duration'].value_counts()","3e3699c7":"#dropping the NAN value null of the dataset\ntrain_df.dropna(inplace=True)\ntrain_df.shape","8b05d0ab":"#showing the sum of all the null values  in each column of data\ntrain_df.isnull().sum()","8af88513":"#extracting the day of journey and month of journey from the Date of journey column\n#as both day and month are required and model will not understand string values\n\ntrain_df[\"Journe_day\"] = pd.to_datetime(train_df['Date_of_Journey'],format=\"%d\/%m\/%Y\").dt.day\ntrain_df[\"Journey_month\"] = pd.to_datetime(train_df['Date_of_Journey'], format= \"%d\/%m\/%Y\").dt.month\ntrain_df.head()","9cb0ef1f":"train_df.drop([\"Date_of_Journey\"],axis=1,inplace=True)","e6ffc4b7":"## Departure time is when a plane leaves the gate. \n# Similar to Date_of_Journey we can extract values from Dep_Time\n\n#Extracting hours\ntrain_df[\"Dep_hour\"] = pd.to_datetime(train_df[\"Dep_Time\"]).dt.hour\n\n#Extracting ,minutes\ntrain_df[\"Dep_min\"]=pd.to_datetime(train_df[\"Dep_Time\"]).dt.minute\n\n#Now we can drop Dep_Time as it is of no use\ntrain_df.drop(['Dep_Time'],axis=1, inplace=True)","284cf2f5":"train_df.head()","18b915c5":"# Arrival time is when the plane pulls up to the gate.\n# Similar to Date_of_Journey we can extract values from Arrival_Time\n\n\n#Extracting hours\ntrain_df[\"Arrival_hour\"] = pd.to_datetime(train_df.Arrival_Time).dt.hour\n\n#Extracting Mintes\ntrain_df[\"Arrival_min\"] = pd.to_datetime(train_df.Arrival_Time).dt.minute\n\n# Now we can drop Arrival_Time as it is of no use\ntrain_df.drop([\"Arrival_Time\"],axis=1, inplace=True)","9a5e0500":"train_df.head()","808eee16":"# Time taken by plane to reach destination is called Duration\n# It is the differnce betwwen Departure Time and Arrival time\n\n\n# Assigning and converting Duration column into list\nduration = list(train_df[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration","3196e78d":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ntrain_df[\"Duration_hours\"] = duration_hours\ntrain_df[\"Duration_mins\"] = duration_mins","cc259250":"train_df.drop([\"Duration\"], axis = 1, inplace = True)\n","42128666":"train_df.head()","b958bb3c":"#count the number of times each category is there\ntrain_df['Airline'].value_counts()","b76c0ddf":"# From graph we can see that Jet Airways Business have the highest Price.\n# Apart from the first Airline almost all are having similar median\n\n# Airline vs Price\nsns.catplot(y=\"Price\", x=\"Airline\", data=train_df.sort_values(\"Price\",ascending=False),kind=\"boxen\" ,height=5,aspect=3)\nplt.show()\n","ba0f552b":"# As Airline is Nominal Category data we will perform OneHotEncoding\n#we cannot differentiate between categories of Airline\n#we will take get_dummies which is part of OneHotEncoding\n\nAirline = train_df[[\"Airline\"]]\n\nAirline = pd.get_dummies(Airline, drop_first= True)  # we will drop first feature which is not required\n\nAirline.head()","a5ad5342":"#counting the number of times each category in Source happens\ntrain_df[\"Source\"].value_counts()","1ba0673d":"#category plot\n#Source vs Price\nsns.catplot(x = \"Source\",y = \"Price\", data = train_df.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 5, aspect = 3)\nplt.show()\n","9b43de04":"# As Source is Nominal Categorical data we will perform OneHotEncoding\n#we cannot differentiate between categories of Source as it is categorical data\n#we will perform get_dummies which is part of OneHotEncoding\nSource = train_df[[\"Source\"]]\n\nSource = pd.get_dummies(Source, drop_first= True)\n\nSource.head()","b9a38cab":"train_df[\"Destination\"].value_counts()","31cd2476":"# Destination vs Price\nsns.catplot(y = \"Price\", x = \"Destination\", data = train_df.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 6, aspect = 3)\nplt.show()\n","5d853117":"#As Destination is Nominal Categorical data we will perform OneHotEncoding\nDestination = train_df[['Destination']]\n\n\nDestination = pd.get_dummies(Destination , drop_first = True)\n\nDestination.head()","95abc9ff":"\n#Route and Total Stops doing same thing \n#From Route we will come to know the number of stops\ntrain_df[\"Route\"]","e492f19c":"#We will drop Route and Additional_Info as Additional_Info contains\n   # 80% no_info\n\n\ntrain_df.drop([\"Route\" , \"Additional_Info\"], axis=1, inplace=True)","be4ae2f7":"train_df.head()","a958fa49":"#Now total_Stops again is a Categorical Fature\n\ntrain_df[\"Total_Stops\"].value_counts()","e250d3f7":"#So we came to know1 stop occurs 5625 times\n#2 stops occur 1520 times\n#As this is case of Ordinal Categorical data type we use LabelEncoder\n# Here values are assigned with corresponding types\n\ntrain_df.replace({\"non-stop\":0 , \"1 stop\":1 , \"2 stops\":2 ,\"3 stops\":3 ,\"4 stops\":4} ,inplace=True)","c143fcff":"train_df.head()","a638e0cc":"# Concatenate dataframe --> train_df + Airline + Source + Destination\n\n\ndf_train = pd.concat([train_df ,Airline,Source,Destination] ,axis=1)","cd9f134a":"#checking the first five rows of new dataframe\ndf_train.head()","82bac73d":"# Drop Airline Source and Destination as we have already converted them into OneHotEncoding\n\ndf_train.drop([\"Airline\" , \"Source\", \"Destination\"] ,  axis=1 , inplace =True)\ndf_train.head()\n","7e9d5666":"# showing the shape of new dataframe\n\ndf_train.shape","c16faf18":"# As we are seeing we are not given the PRICE in test data which we have to predict\ntest_df=pd.read_excel(r'..\/input\/flight-fare-prediction-mh\/Test_set.xlsx')\ntest_df","21fe98ec":"# Preprocessing steps\n\nprint(\"Test data info\")\nprint(\"*\" *75)\nprint(test_df.info())","f4898e12":"print(\"Null values :\")\nprint(\"*\"*75)\ntest_df.dropna(inplace = True)\nprint(test_df.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_df[\"Journey_day\"] = pd.to_datetime(test_df.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_df[\"Journey_month\"] = pd.to_datetime(test_df[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_df.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_df[\"Dep_hour\"] = pd.to_datetime(test_df[\"Dep_Time\"]).dt.hour\ntest_df[\"Dep_min\"] = pd.to_datetime(test_df[\"Dep_Time\"]).dt.minute\ntest_df.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_df[\"Arrival_hour\"] = pd.to_datetime(test_df.Arrival_Time).dt.hour\ntest_df[\"Arrival_min\"] = pd.to_datetime(test_df.Arrival_Time).dt.minute\ntest_df.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\nduration = list(test_df[\"Duration\"])\n\nfor i in range(len(duration)):\n    if (len(duration[i].split())!=2):                       # Chech if duration contains only hours and minutes\n        if \"h\" in duration[i]:\n           duration[i] = duration[i].strip() +\"0m\"          #Adds 0 minutes\n        else:\n           duration[i] = \"0h\" + duration[i]                 # adds 0 hours\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(str(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(str(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n    \n    \n# Adding Duration column to test set\ntest_df[\"Duration_hours\"] = duration_hours\ntest_df[\"Duration_mins\"] = duration_mins\ntest_df.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_df[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_df[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_df[\"Source\"].value_counts())\nSource = pd.get_dummies(test_df[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_df[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_df[\"Destination\"], drop_first = True)\n\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_df.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_df.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndf_test = pd.concat([test_df, Airline, Source, Destination], axis = 1)\n\ndf_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", df_test.shape)","e08e6202":"df_test.head()","dbe5b38e":"df_train.shape","42accd99":"df_train.columns","ae4724cd":"# Extracting the independent variables \n\nX = df_train[['Total_Stops', 'Journe_day', 'Journey_month', 'Dep_hour',\n       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\n       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n       'Airline_Jet Airways', 'Airline_Jet Airways Business',\n       'Airline_Multiple carriers',\n       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\n       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n       'Destination_Kolkata', 'Destination_New Delhi']]\nX.head()\n","0134bc55":"#Extracting the dependent Variable\n\nY=df_train.iloc [:,1]\nY.head()","fa88ffdd":"# Finds correlation between Independent and dependent attributes\ncorr = train_df.corr()\nplt.figure(figsize = (18,18))\nsns.heatmap(corr , annot = True, cmap = \"RdYlGn\")\n\nplt.show()","415ef655":"# Important feature using ExtraTreesRegressor\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X,Y)","d553fc26":"print(selection.feature_importances_)","948cb2d2":"# plot graph of Feature importances for better visualization\n\nplt.figure(figsize = (12,8))\nfeat_imp = pd.Series(selection.feature_importances_ , index= X.columns)\nfeat_imp.nlargest(20).plot(kind ='barh')\nplt.show()\n","b6f5496a":"#splitting the dependent ad independent variable into training and testing data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","5b469d4a":"#Fitting the RandomForest Model on Xtrain and Ytrain\nfrom sklearn.ensemble import RandomForestRegressor\nreg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, Y_train)","ffecee3d":"#predicting the value on X_test\n\nY_pred = reg_rf.predict(X_test)\n","10e085f4":"reg_rf.score(X_train, Y_train)","a3428b5a":"reg_rf.score(X_test, Y_test)","a34b157e":"#plotting the distribution plot and we find the Gaussian plot\n\nsns.distplot(Y_test-Y_pred)\nplt.show()","1809a485":"plt.scatter(Y_test, Y_pred, alpha = 0.5)\nplt.xlabel(\"Y_test\")\nplt.ylabel(\"Y_pred\")\nplt.show()","560f46b3":"from sklearn import metrics","0abfd2a9":"#finding the Errors \n\nprint('MAE:', metrics.mean_absolute_error(Y_test, Y_pred))\nprint('MSE:', metrics.mean_squared_error(Y_test, Y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))","48a07bb6":"metrics.r2_score(Y_test, Y_pred)","34d0ddd1":"from sklearn.model_selection import RandomizedSearchCV","985b8047":"#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","6fd23302":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","7d3a9bba":"rf_random = RandomizedSearchCV( estimator = reg_rf , param_distributions= random_grid , scoring='neg_mean_squared_error',\n                               n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","09c60afe":"rf_random.fit(X_train , Y_train)","667ada05":"rf_random.best_params_","928f555a":"prediction = rf_random.predict(X_test)","ee503685":"plt.figure(figsize = (8,8))\nsns.distplot(Y_test-prediction)\nplt.show()\n","73d7c460":"plt.figure(figsize = (8,8))\nplt.scatter(Y_test, prediction, alpha = 0.5)\nplt.xlabel(\"Y_test\")\nplt.ylabel(\"Y_pred\")\nplt.show()","c946b295":"print('MAE:', metrics.mean_absolute_error(Y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(Y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, prediction)))","0cacc6e5":"print(\"R2 Score of Our Model is : \")\nprint()\nmetrics.r2_score(Y_test, prediction)","8a325964":"import pickle","95f27aed":"my_file= open('flight_fa.pkl' , 'wb')","18aa3cab":"pickle.dump(rf_random , my_file)","0454b39a":"model = open('flight_fa.pkl' ,'rb')\nforest =pickle.load(model)","654f0426":"pred = forest.predict(X_test)\nmetrics.r2_score(Y_test , pred)","565e82e0":"# Saving And Exporting Our Model","f43eafea":"## **We have seen that Total_Stops is the most important feature then decreasing in the particuar order**","8048411e":"# **Handling Categorical Data**","2f730fc8":"# **Fitting Model using Random forest**\n\n1. Split dataset into train and test set in order to prediction w.r.t X_test\n\n1. If needed do scaling of data\n\n   Scaling is not done in Random forest\n   \n1. Import Model\n\n1. Fit the data\n\n1. predict w.r.t X_test\n\n1. In regression check RSME Score , MAE score\n\n1. Plot Graph","59ea056d":"# **Test Set**\n\n## **Now we will perform above steps in Test data. We are not combining train and test data because of data leakage due to which our test data will know some of the results which is not good for the project\u00b6**","1cf20711":"# **Feature selection**\n\nFinding out the best feature which will contribute and have good relation with target variable. Following are some of the feature selection methods,\n\n1. # **Heatmap**\n\n1. # **Feature Importances**\n\n1. # **SelectKBest**","5c8121bb":"# **Hyperparameter Tuning**\n\n### We are using RandomizedSearchCV for hypertuning our Model then fit the Model and Check best parameters and best Score","8e5c43b3":"## **Now Finally we have seen the best values**","c34ba8db":"### Since our model's performance has been incresed .. We can export it now for our future references"}}