{"cell_type":{"d5294ad2":"code","40699603":"code","77835495":"code","edfa8c14":"code","8756824d":"code","5b83cb32":"code","9074a5e5":"code","600d95f4":"code","6f30ea89":"code","7bf7fa1e":"code","19d75cbb":"code","95a967f1":"code","5381c702":"code","69bd2a8c":"code","ef4297a3":"code","d0fdf6a8":"code","d10a6afd":"code","3d9422a2":"code","1602c099":"code","9d4c66fe":"code","3b89ce11":"code","0b92310e":"code","a5c44750":"code","160c55ec":"code","9c2b3f67":"code","03812fb4":"code","94c6a200":"code","d65cd357":"code","94fc3a91":"code","3fde8ed7":"code","22cd63c1":"code","5bd38d60":"code","72c4554d":"code","81446c2c":"code","22248793":"markdown","95f26dce":"markdown","a3851c55":"markdown","68016c7d":"markdown","ae422c12":"markdown","67ebec4d":"markdown","7f1ae7a4":"markdown","98e2da85":"markdown","51c2863b":"markdown","d5ce7fc1":"markdown","1cc0ce86":"markdown","a3ec4e67":"markdown","74f59b32":"markdown","da4be0ea":"markdown","c861c77d":"markdown","05ceafd0":"markdown","0918081f":"markdown","f88a6494":"markdown","b5c7fbe5":"markdown","aec50e15":"markdown","a8827152":"markdown","8005356c":"markdown","c15c7084":"markdown","09adebbc":"markdown","10839687":"markdown","b8f0ef92":"markdown","7a6a3e16":"markdown"},"source":{"d5294ad2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport warnings\nimport matplotlib.pyplot as plt\n%matplotlib inline\nwarnings.simplefilter(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","40699603":"# load the train set\ntrain_titanic = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_titanic.head()","77835495":"# load the test set\ntest_titanic = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_titanic.head()","edfa8c14":"print('shape train set: ',train_titanic.shape)\ntrain_titanic.info()\ntrain_titanic.describe()\n# checking null values\ntrain_titanic.isnull().sum()","8756824d":"# let's see the correlation among colums\ntrain_titanic.corr()","5b83cb32":"print(\"value counts: \\n\",train_titanic.Pclass.value_counts())\nprint(\"how many null entries: \",train_titanic.Pclass.isnull().sum()) # No missing points\nprint(train_titanic.groupby('Pclass').median())\nprint(train_titanic.groupby('Pclass').mean())","9074a5e5":"sns.countplot(x=train_titanic.Survived, hue=train_titanic.Pclass)","600d95f4":"sns.violinplot(x=train_titanic.Survived, y = train_titanic.Pclass)","6f30ea89":"sns.violinplot(x=train_titanic.Survived, y = train_titanic.Pclass, hue=train_titanic.Sex)","7bf7fa1e":"sns.countplot(x=train_titanic.Survived, hue=train_titanic.SibSp)","19d75cbb":"sns.violinplot(x=train_titanic.Survived, y = train_titanic.SibSp, hue=train_titanic.Sex)","95a967f1":"# load the train set (this is needed only if you re-run some cells)\ntrain_titanic = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_titanic.drop(['PassengerId', 'Name', 'Cabin'], inplace=True, axis = 1)\ntrain_titanic.head()","5381c702":"train_titanic.Ticket.value_counts()","69bd2a8c":"train_titanic.drop(['Ticket'], inplace=True, axis = 1)","ef4297a3":"most_frequent_embarked = train_titanic['Embarked'].value_counts()[:1].index.tolist()[0]\ntrain_titanic['Embarked'].fillna(most_frequent_embarked, inplace = True)\naverage_age = train_titanic['Age'].mean()\ntrain_titanic['Age'].fillna(average_age, inplace = True)\n# checking null values\ntrain_titanic.isnull().sum()","d0fdf6a8":"train_titanic.Sex = train_titanic.Sex.map({'female':1, 'male':0})\n# one hot encoding\ndummies = pd.get_dummies(train_titanic['Embarked'])\ntrain_titanic = pd.concat([train_titanic, dummies], axis=1)\ntrain_titanic.drop(['Embarked'], inplace=True, axis = 1)\ntrain_titanic.head()","d10a6afd":"X_train = train_titanic.drop(\"Survived\", axis=1)\nY_train = train_titanic[\"Survived\"]","3d9422a2":"from sklearn.neighbors import KNeighborsClassifier\n\nk = 10\nKNN=KNeighborsClassifier(n_neighbors = k)\nKNN.fit(X_train, Y_train)\nY_prediction_KNN = KNN.predict(X_train)\n# plotting the confusion matrix in heatmap\nfrom sklearn import metrics\nmatrix = metrics.confusion_matrix(Y_train, Y_prediction_KNN)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","1602c099":"from sklearn.naive_bayes import GaussianNB\n\nGaussNB = GaussianNB()\nGaussNB.fit(X_train, Y_train)\nY_prediction_GaussNB = GaussNB.predict(X_train)\n# plotting the confusion matrix in heatmap\nmatrix = metrics.confusion_matrix(Y_train, Y_prediction_GaussNB)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","9d4c66fe":"from sklearn.svm import SVC\n\nSVM = SVC(probability = True)\nSVM.fit(X_train, Y_train)\nY_prediction_SVM = SVM.predict(X_train)\n# plotting the confusion matrix in heatmap\nmatrix = metrics.confusion_matrix(Y_train, Y_prediction_SVM)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","3b89ce11":"from sklearn.tree import DecisionTreeClassifier\n\nDecisionTree = DecisionTreeClassifier()\nDecisionTree.fit(X_train, Y_train)\nY_prediction_DecisionTree = DecisionTree.predict(X_train)\n# plotting the confusion matrix in heatmap\nmatrix = metrics.confusion_matrix(Y_train, Y_prediction_DecisionTree)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","0b92310e":"from sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(X_train, Y_train)\nY_prediction_LR = LR.predict(X_train)\n# plotting the confusion matrix in heatmap\nmatrix = metrics.confusion_matrix(Y_train, Y_prediction_LR)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","a5c44750":"from sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier()\nRF.fit(X_train, Y_train)\nY_prediction_RF = RF.predict(X_train)\n# plotting the confusion matrix in heatmap\nmatrix = metrics.confusion_matrix(Y_train, Y_prediction_RF)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","160c55ec":"from sklearn.neural_network import MLPClassifier\n\nNN = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                     hidden_layer_sizes=(32, 32, 32, 32, 32), random_state=1)\nNN.fit(X_train, Y_train)\nY_prediction_NN = NN.predict(X_train)\n# plotting the confusion matrix in heatmap\nmatrix = metrics.confusion_matrix(Y_train, Y_prediction_NN)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","9c2b3f67":"from sklearn.svm import SVC\n\nSVM = SVC(probability=True, kernel='poly', degree=1)\nSVM.fit(X_train, Y_train)\nY_prediction_SVM = SVM.predict(X_train)\n# plotting the confusion matrix in heatmap\nmatrix = metrics.confusion_matrix(Y_train, Y_prediction_SVM)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","03812fb4":"from sklearn.ensemble import VotingClassifier\n\nKNN = KNeighborsClassifier(n_neighbors = k)\nNAIVE = GaussianNB()\nSVM = SVC(probability = True)\nDT = DecisionTreeClassifier()\nLR = LogisticRegression()\nRF = RandomForestClassifier()\nNN = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                     hidden_layer_sizes=(32, 32, 32, 32, 32), random_state=1)\nEnsemble = VotingClassifier( \n    estimators= [('KNN',KNN),\n                 ('NB',NAIVE),\n                 ('SVM',SVM),\n                 ('DT',DT),\n                 ('LR',LR),\n                 ('RF',RF),\n                 ('NN', NN)\n                ], voting = 'soft', \n    weights = [1.,1.,1.,1.,1.,1.,1.])\n\nEnsemble.fit(X_train,Y_train)\nY_pred = Ensemble.predict(X_train)\nmatrix = metrics.confusion_matrix(Y_train, Y_pred)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","94c6a200":"print('Precision : ', np.round(metrics.precision_score(Y_train, Y_pred)*100,2))\nprint('Accuracy : ', np.round(metrics.accuracy_score(Y_train, Y_pred)*100,2))\nprint('Recall : ', np.round(metrics.recall_score(Y_train, Y_pred)*100,2))\nprint('F1 score : ', np.round(metrics.f1_score(Y_train, Y_pred)*100,2))\nprint('AUC : ', np.round(metrics.roc_auc_score(Y_train, Y_pred)*100,2))\n","d65cd357":"print('Precision : ', np.round(metrics.precision_score(Y_train, Y_prediction_RF)*100,2))\nprint('Accuracy : ', np.round(metrics.accuracy_score(Y_train, Y_prediction_RF)*100,2))\nprint('Recall : ', np.round(metrics.recall_score(Y_train, Y_prediction_RF)*100,2))\nprint('F1 score : ', np.round(metrics.f1_score(Y_train, Y_prediction_RF)*100,2))\nprint('AUC : ', np.round(metrics.roc_auc_score(Y_train, Y_prediction_RF)*100,2))","94fc3a91":"from sklearn.model_selection import GridSearchCV\n\nKNN = KNeighborsClassifier(n_neighbors = k)\nNAIVE = GaussianNB()\nSVM = SVC(probability = True)\nDT = DecisionTreeClassifier()\nLR = LogisticRegression()\nRF = RandomForestClassifier()\nNN = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                     hidden_layer_sizes=(32, 32, 32, 32, 32), random_state=1)\nEnsemble = VotingClassifier( \n    estimators= [('KNN',KNN),\n                 ('NB',NAIVE),\n                 ('SVM',SVM),\n                 ('DT',DT),\n                 ('LR',LR),\n                 ('RF',RF),\n                 ('NN', NN)\n                ], \n    voting = 'soft',\n    weights = [1.,1.,1.,1.,1.,1.,1.]\n    )\n\nparams = {'weights': [[1.,1.,1.,1.,1.,1.,1.],\n                       [10.,1.,1.,1.,1.,1.,1.],\n                       [1.,10.,1.,1.,1.,1.,1.],\n                       [1.,1.,10.,1.,1.,1.,1.],\n                       [1.,1.,1.,10.,1.,1.,1.],\n                       [1.,1.,1.,1.,10.,1.,1.],\n                       [1.,1.,1.,1.,1.,10.,1.],\n                       [1.,1.,1.,1.,1.,1.,10.]\n                      ]\n         }\n\nEnsemble.fit(X_train,Y_train)\ngrid = GridSearchCV(estimator=Ensemble, param_grid = params) #, cv=5)\ngrid = grid.fit(X_train, Y_train)\nY_pred = grid.predict(X_train)\nmatrix = metrics.confusion_matrix(Y_train, Y_pred)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()\nprint('the best parameters are: ', grid.best_params_)","3fde8ed7":"KNN = KNeighborsClassifier(n_neighbors = k)\nNAIVE = GaussianNB()\nSVM = SVC(probability = True)\nDT = DecisionTreeClassifier()\nLR = LogisticRegression()\nRF = RandomForestClassifier()\nNN = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                     hidden_layer_sizes=(32, 32, 32, 32, 32), random_state=1)\nEnsemble = VotingClassifier( \n    estimators= [('KNN',KNN),\n                 ('NB',NAIVE),\n                 ('SVM',SVM),\n                 ('DT',DT),\n                 ('LR',LR),\n                 ('RF',RF),\n                 ('NN', NN)\n                ], \n    voting = 'hard',\n    weights = [1.,1.,1.,1.,1.,1.,1.]\n    )\n\nparams = {'weights': [[1.,1.,1.,1.,1.,10.,1.], # original \"best\" weights combination\n                      [10.,1.,1.,1.,1.,10.,1.],\n                      [1.,10.,1.,1.,1.,10.,1.],\n                      [1.,1.,10.,1.,1.,10.,1.],\n                      [1.,1.,1.,10.,1.,10.,1.],\n                      [1.,1.,1.,1.,10.,10.,1.],\n                     ]\n         }\n\nEnsemble.fit(X_train,Y_train)\ngrid = GridSearchCV(estimator=Ensemble, param_grid = params) #, cv=5)\ngrid = grid.fit(X_train, Y_train)\nY_pred_grid = grid.predict(X_train)\nmatrix = metrics.confusion_matrix(Y_train, Y_pred)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()\nprint('the best parameters are: ', grid.best_params_)","22cd63c1":"print(\"GRID\\n\")\nprint('Precision : ', np.round(metrics.precision_score(Y_train, Y_pred_grid)*100,2))\nprint('Accuracy : ', np.round(metrics.accuracy_score(Y_train, Y_pred_grid)*100,2))\nprint('Recall : ', np.round(metrics.recall_score(Y_train, Y_pred_grid)*100,2))\nprint('F1 score : ', np.round(metrics.f1_score(Y_train, Y_pred_grid)*100,2))\nprint('AUC : ', np.round(metrics.roc_auc_score(Y_train, Y_pred_grid)*100,2))\n\nprint(\"\\nRandom forest\")\nprint('Precision : ', np.round(metrics.precision_score(Y_train, Y_prediction_RF)*100,2))\nprint('Accuracy : ', np.round(metrics.accuracy_score(Y_train, Y_prediction_RF)*100,2))\nprint('Recall : ', np.round(metrics.recall_score(Y_train, Y_prediction_RF)*100,2))\nprint('F1 score : ', np.round(metrics.f1_score(Y_train, Y_prediction_RF)*100,2))\nprint('AUC : ', np.round(metrics.roc_auc_score(Y_train, Y_prediction_RF)*100,2))","5bd38d60":"# reloading is needed just if you run multiple times this cell\ntest_titanic = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_titanic_Aux = test_titanic.drop(['PassengerId', 'Name', 'Cabin','Ticket'], axis = 1)\n# filling some empty entries\naverage_fare = test_titanic_Aux['Fare'].mean()\ntest_titanic_Aux['Fare'].fillna(average_fare, inplace = True)\naverage_age = test_titanic_Aux['Age'].mean()\ntest_titanic_Aux['Age'].fillna(average_age, inplace = True)\n# mapping sex\ntest_titanic_Aux.Sex = test_titanic_Aux.Sex.map({'female':1, 'male':0})\n# embark one hot encoding\ndummies = pd.get_dummies(test_titanic_Aux['Embarked'])\ntest_titanic_Aux = pd.concat([test_titanic_Aux, dummies], axis=1)\ntest_titanic_Aux.drop(['Embarked'], inplace=True, axis = 1)\n\npredictions = grid.predict(test_titanic_Aux)\noutput = pd.DataFrame({'PassengerId': test_titanic.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)","72c4554d":"# passenger class\nPclass = 1    \n# sex (1 for female, 0 for male)\nSex = 0\n# age (between 1 and 99)\nAge = 28\n# number of siblings \/ spouses aboard the Titanic (between 0 and 8)\nSibSp = 2\n# number of parents \/ children aboard the Titanic (between 0 and 6)\nParch = 1\n# Passenger fare (between 0 and 512.3292)\nFare = 250.0\n# Embarked C = Cherbourg, Q = Queenstown, S = Southampton (one must be 1, the other 2 must be zero)\nC = 1\nQ = 0\nS = 0\n\nEverythingOK = True\nif Pclass not in [1,2,3]:\n    print('Pclass (passenger class), has to be either 1,2 or 3')\n    EverythingOK = False\nif Sex not in [0,1]:\n    print('Sex has to be either 0 or 1')\n    EverythingOK = False\nif Age<1 or Age>99:\n    print('Age has to be within 1 and 99')\n    EverythingOK = False\nif SibSp < 0 or SibSp > 8:\n    print('SibSp has to be within 0 and 8')\n    EverythingOK = False\nif Parch < 0 or Parch > 6:\n    print('Parch has to be within 0 and 6')\n    EverythingOK = False\nif Fare < 0 or Fare > 512.3292:\n    print('Fare has to be within 0 and 512.3292')\n    EverythingOK = False\nif C not in [0,1] or Q not in [0,1] or S not in [0,1] or C+Q+S!=1:\n    print('Choose one single embark location please')\n    EverythingOK = False\nif EverythingOK:\n    \n    data = {'Pclass': [Pclass],\n            'Sex': [Sex],\n            'Age': [Age],\n            'SibSp': [SibSp],\n            'Parch': [Parch],\n            'Fare': [Fare],\n            'C': [C],\n            'Q': [Q],\n            'S': [S],\n        }\n    df = pd.DataFrame(data, \n                      columns = ['Pclass','Sex','Age','SibSp','Parch','Fare','C','Q','S']\n                     )\n    Y_pred_single = grid.predict(df)[0]\n    if Y_pred_single == 0:\n        print('I\\'m sorry Leo :(\\n')\n    else:\n        print('you did it! You survived the Titanic crash! :D\\n')\nelse:\n    print('please double check your input data and try again!')","81446c2c":"RF = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=2)\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nRF.fit(X_train[features], Y_train)\nY_prediction_RF = RF.predict(X_train[features])\n# plotting the confusion matrix in heatmap\nmatrix = metrics.confusion_matrix(Y_train, Y_prediction_RF)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","22248793":"decision tree classifier","95f26dce":"We can see that there is a correlation. In fact, people in the first class are more likely to survive than people in the third class. \n\nNow we explore \"SibSP\" (low correlation).","a3851c55":"On the ranking list, it states that the follwoing piece of code return an accuracy of 100%. In my case it doesn't and, more interestingly, each time I run my code, I get slightly different solutions.","68016c7d":"Now our dataset is ready to be use for some more \"numerical\" analysys.","ae422c12":"Now let's see if you would survive the shipwreck! Change the data hereafter and compute your chance of survival!","67ebec4d":"random forest","7f1ae7a4":"Now let's fill the missing values of \"embarked\" with the most occurring value and \"age\" with the average value.","98e2da85":"We got rid of the empty entries. Now, in order to perform analysys, we need to change some string\/boolean value into numbers. Hence, we convert \"Sex\" and \"Embarked\" to categorical features. For \"Sex\" we can use a single boolean, for \"Embarked\" we use a one-hot encoding.","51c2863b":"Let's start by loading some useful libraries, the train set and the test set.\nThen let's take a look at both.","d5ce7fc1":"and of course a neural network because why not","1cc0ce86":"It's not clear if there is correlation or not from above. It's difficult to tell by seeing the absolute terms.","a3ec4e67":"This results are good, but worse than the random forest!","74f59b32":"Let's try a support vector machine as well","da4be0ea":"It seems that \"Pclass\" is the one with most (in absolute term) correlation wrt \"Survived\" and \"SibSp\" is the least correlated. Let's explore those a bit just for fun. ","c861c77d":"Among the 12 colums: two are not very significative (index and name), and 3 colums (age, cabin, embarked) are not completly filled. Among the incomplete colums,\"Cabin\" has a lot of empty entries (see above). So, we will discard index, name and cabin. ","05ceafd0":"As we can see, depending on the performance score, one may be better than the other. Anyhow, both of them reach about 98% in almost all metrics! For the submission, we will use the grid search estimator.","0918081f":"let's analyze more what ticket entails","f88a6494":"This does not look like very useful information. So we discard it.","b5c7fbe5":"Now let's try a simple K-NN algorithm and see how it performs","aec50e15":"logistic regression","a8827152":"We could keep looking in the neighbourhood of the new best weight but we are going to stop there.\nLet's now confront the random forest (best \"single\" method) with the grid.","8005356c":"the Random Forest seems the best option, but we know that a weighted average of every algorithm is, most likely, the best solution we can get. Let's do that then.","c15c7084":"Instead of using an uniform voting scheme (where each algorithm has the same weight), Maybe it's best to use a weighted voting system.\n\n**WARNING!** the following cell takes quite some times to execute","09adebbc":"We can clearly see that Pclass is correlated with Survived! Indeed, people in first class are more likely to survive than others. Hereafter, we show the same conclusion via graph plots.","10839687":"the best parameter combination is: [1.0, 1.0, 1.0, 1.0, 1.0, 10.0, 1.0].\nWe can explore weights in this neighbour a more in depth.\n\n**WARNING!** the following cell takes quite some times to execute","b8f0ef92":"# **Titanic - ML from disater** \n\nwould you survive the most famous ship werck ever? Or will you be left behind like Leo? Find it out with this notebook!","7a6a3e16":"Not bad but let's see if we can do better. Let's use a gaussian Nayve bayes."}}