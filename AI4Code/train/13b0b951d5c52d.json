{"cell_type":{"78aa2b72":"code","ff121521":"code","6e685b33":"code","2c1fc118":"code","c36d7377":"code","db610da1":"code","ee4363fd":"code","8cfd4409":"code","924846de":"code","cb68a344":"code","a50b81a9":"code","46e8a3a5":"code","15a67e40":"markdown","f32afa1e":"markdown","3bfd97ff":"markdown","1ba267c9":"markdown","6fe2703d":"markdown","07a6d289":"markdown","e5f1dc0d":"markdown","12a731c8":"markdown","f3b0c32b":"markdown","b0ddf4be":"markdown","be1f3a63":"markdown","ec4ef29b":"markdown","405ca83d":"markdown","2616bf19":"markdown","ac32c5a3":"markdown"},"source":{"78aa2b72":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\nimport imblearn\n\n#read the csv data from env and store it\ndf = pd.read_csv('\/kaggle\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv')\n\n# drop cols that will not be used\ndf.drop('Firstname', axis=1, inplace=True)\ndf.drop('Lastname', axis=1, inplace=True)\ndf.drop('PassengerId', axis=1, inplace=True)\n\n#transform category col to numeric (M=1, F=0, C=1, P=0)\ndf['Sex'].replace('M', 1, inplace=True)\ndf['Sex'].replace('F', 0, inplace=True)\ndf['Category'].replace('C', 1, inplace=True)\ndf['Category'].replace('P', 0, inplace=True)\ndf.sample(5)","ff121521":"sur_counts = df['Survived'].value_counts()\ndeath_toll_rate = sur_counts[0] \/ (sur_counts[0] + sur_counts[1])\nprint(f'The naive baseline is: {death_toll_rate * 100:.2f}%')\n\nsur_counts.plot(kind='bar', title='Classes')\nplt.show()","6e685b33":"#check if there is a correlation between survived people and their country, \n#examine the two major countries(Sweden and Estonia) where most of the passengers and crew come from\nsweden_counts = df.Survived[df['Country'] == 'Sweden'].value_counts()\nestonia_counts = df.Survived[df['Country'] == 'Estonia'].value_counts()\nfinland_counts = df.Survived[df['Country'] == 'Finland'].value_counts()\n\ncountry_df = pd.DataFrame({'country' : ['Sweden', 'Estonia', 'Finland'], 'Dead' : [sweden_counts[0], estonia_counts[0], finland_counts[0]], 'Survived' : [sweden_counts[1], estonia_counts[1], finland_counts[1]]})\ncountry_df.plot(kind='bar', stacked=True, x='country')\nplt.title(\"Country regard survive rate\")\nplt.xlabel(\"Country\")\nplt.ylabel(\"death toll\")\nplt.show()","2c1fc118":"country = df.Country.unique()\n# other_country = country[(country != 'Estonia') & (country != 'Sweden')].values\n#remove Estonia and Sweden\nother_country = country[2:]\ndf.Country = df.Country.replace(to_replace=other_country, value='Other')\ndf = pd.get_dummies(df, columns = ['Country'])","c36d7377":"from sklearn.decomposition import PCA\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTETomek\nfrom collections import Counter\n\n#https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets#t3\ndef plot_2d_space(X, y, label, axis=plt):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        axis.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    if axis == plt:\n        axis.title(label)\n    else:\n        axis.title.set_text(label)\n    axis.legend(loc='upper right')  \n    \nf = plt.figure(figsize=(10,10))\npca = PCA(n_components=2)\nX = pca.fit_transform(df.loc[:, df.columns != 'Survived'])\ny = df.Survived\nplot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')\n\nfig, axis = plt.subplots(2,2,figsize=(15,15))\n        \n# Under sample randomsampler\nrus = RandomUnderSampler(random_state=42)\nX_rus, y_rus = rus.fit_resample(X, y)\n\n# print('resampled dataset shape:', Counter(y_rus))\nplot_2d_space(X_rus, y_rus, 'Random under-sampling', axis=axis[0,0])\n\n#Tomkel links\n\ntl = TomekLinks(sampling_strategy='majority')\nX_tl, y_tl= tl.fit_sample(X, y)\n\nplot_2d_space(X_tl, y_tl, 'Tomek links under-sampling', axis=axis[0,1])\n\n#over sample using SMOTE\n\nsmote = SMOTE(sampling_strategy='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\n\n# print('resampled dataset shape:', Counter(y_sm))\n\nplot_2d_space(X_sm, y_sm, 'SMOTE over-sampling', axis=axis[1,0])\n\n#over sample using combined SMOTE and Tomek links\n\nsmt = SMOTETomek(sampling_strategy='auto')\nX_smt, y_smt = smt.fit_sample(X, y)\n\n# print('resampled dataset shape:', Counter(y_smt))\n\nplot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links', axis=axis[1,1])\nplt.show()","db610da1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n#standardize the Age column \nscaler = StandardScaler()\ndf['Age'] = scaler.fit_transform(df[['Age']])\nX = df.loc[:, df.columns != 'Survived']\ny = df.Survived\n# Under sample randomsampler\nX_rus, y_rus = rus.fit_resample(X, y)\n\n\n#Tomkel links\nX_tl, y_tl= tl.fit_sample(X, y)\n\n#over sample using SMOTE\nX_sm, y_sm = smote.fit_sample(X, y)\n\n#over sample using combined SMOTE and Tomek links\nX_smt, y_smt = smt.fit_sample(X, y)\n","ee4363fd":"#logreg with SMOTE strategy\nlogreg = LogisticRegression(random_state=0, C=1)\nlogreg.fit(X_sm, y_sm)\npred_y = logreg.predict(X_sm)\nprint('1st Confusion Matrix:')\nprint(confusion_matrix(y_sm, pred_y))\nprint('\\nAccuracy score: ', accuracy_score(y_sm, pred_y))\n\n# #logreg with random under sampling\nlogreg.fit(X_rus, y_rus)\npred_y = logreg.predict(X_rus)\nprint('\\n2nd Confusion Matrix:')\nprint(confusion_matrix(y_rus, pred_y))\nprint('Accuracy score: ', accuracy_score(y_rus, pred_y))\n\n#logreg with tomkel links sampling strategy\nlogreg.fit(X_tl, y_tl)\npred_y = logreg.predict(X_tl)\nprint('\\n3rd Confusion Matrix: \\n')\nprint(confusion_matrix(y_tl, pred_y))\nprint('\\nAccuracy score: ', accuracy_score(y_tl, pred_y))","8cfd4409":"from sklearn.ensemble import GradientBoostingClassifier\ngbrt = GradientBoostingClassifier(random_state=0)\n\n#with SMOTE sampling strategy\ngbrt.fit(X_sm, y_sm)\npred_y = gbrt.predict(X_sm)\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(y_sm, pred_y))\nprint('Accuracy score: ', accuracy_score(y_sm, pred_y))\n\n#with Tomkel Link sampling strategy\ngbrt.fit(X_tl, y_tl)\npred_y = gbrt.predict(X_tl)\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(y_tl, pred_y))\nprint('\\nAccuracy score: ', accuracy_score(y_tl, pred_y))\n\n#try gbrt with SMOTE w\/ Tomek links sampling strategy\ngbrt.fit(X_smt, y_smt)\npred_y = gbrt.predict(X_smt)\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(y_smt, pred_y))\nprint('Accuracy score: ', accuracy_score(y_smt, pred_y))\n\n#try random under sampling strategy\ngbrt.fit(X_rus, y_rus)\npred_y = gbrt.predict(X_rus)\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(y_rus, pred_y))\nprint('Accuracy score: ', accuracy_score(y_rus, pred_y))","924846de":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(random_state = 0)\n\n#with SMOTE sampling strategy\nforest.fit(X_sm, y_sm)\npred_y = forest.predict(X_sm)\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(y_sm, pred_y))\nprint('\\nAccuracy score: ', accuracy_score(y_sm, pred_y))\n\n#with Tomkel Link sampling strategy\nforest.fit(X_tl, y_tl)\npred_y = forest.predict(X_tl)\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(y_tl, pred_y))\nprint('\\nAccuracy score: ', accuracy_score(y_tl, pred_y))\n\n#with SMOTE+Tomkel Link sampling strategy\nforest.fit(X_smt, y_smt)\npred_y = forest.predict(X_smt)\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(y_smt, pred_y))\nprint('\\nAccuracy score: ', accuracy_score(y_smt, pred_y))\n\n#with random under sampling strategy\nforest.fit(X_rus, y_rus)\npred_y = forest.predict(X_rus)\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(y_rus, pred_y))\nprint('\\nAccuracy score: ', accuracy_score(y_rus, pred_y))\n\n#without resampling\nforest.fit(X, y)\npred_y = forest.predict(X)\nprint('Confusion Matrix: \\n')\nprint(confusion_matrix(y, pred_y))\nprint('\\nAccuracy score: ', accuracy_score(y, pred_y))","cb68a344":"from sklearn.model_selection import GridSearchCV\n\nparams = {'bootstrap': [True, False],\n          'max_depth': [40, 50, 60, None],\n          'max_features': ['auto'],\n          'min_samples_leaf': [1, 2, 4],\n          'min_samples_split': [2, 5, 10],\n          'n_estimators': [20, 100, 200]}\n\ngs_cv = GridSearchCV(RandomForestClassifier(random_state = 0, n_jobs = -1), params, cv=5, scoring='accuracy')\ngs_cv.fit(X_smt, y_smt)\n\nprint(f'best params: {gs_cv.best_estimator_}')","a50b81a9":"gs_cv.best_score_","46e8a3a5":"forest = RandomForestClassifier(bootstrap=False, max_depth=50, n_estimators=200,\n                       n_jobs=-1, random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X_smt, y_smt, random_state = 42)\n\nforest.fit(X_smt, y_smt)\npred_y = forest.predict(X_smt)\nprint(f'train accuracy: {accuracy_score(y_smt, pred_y)}')\n\nforest.fit(X_train, y_train)\npred_y = forest.predict(X_test)\nprint(f'test accuracy: {accuracy_score(y_test, pred_y)}')\n\nprint(confusion_matrix(y_test, pred_y))","15a67e40":"# Conclusion\nIn this notebook, the best result is achieved through applying Random Forest model on resampled datasets with SMOTE+Tomkel Links sampling strategy. The training accuracy is 94% and test accuracy is 84%.","f32afa1e":"## Inbalanced Datasets","3bfd97ff":"## Random Forest Model","1ba267c9":"## Gradient Boosting Regression Tree(GBRT) model","6fe2703d":"# Fine-tune the Forest's Parameters","07a6d289":"From the plot, we can see people from Estonian have a higher chance of survival in this catastrophe comparing to that of people from Sweden.\n\nIt confirms that we might be able to achieve better result if we set the country as one of the features. However, this is largely affected by how we resample our original dataset. By adding around 700+ more survivals to the dataset, or ignoring the same number of victims, would drastically add noise to the dataset\n\nAlso we need to use the one-hot encoding to factorize the 'Country' column, giving each sample unique columns for each country being presented in the data.\nThe pd.get_dummies() function would do that. \n\nHowever, after experiments, the approach to set samples' value in the country column that is other than (Estonia, Sweden) as \"Other\", then apply one-hot encoding to the country achieve better results in this dataset. ","e5f1dc0d":"# Acknowledgement:\nhttps:\/\/towardsdatascience.com\/working-with-sparse-data-sets-in-pandas-and-sklearn-d26c1cfbe067\n\nhttps:\/\/www.kaggle.com\/christianlillelund\/find-hate-towards-asians-in-tweets-svm \n\nhttps:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets\n\nhttps:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets#t3\n","12a731c8":"## Country Column","f3b0c32b":"## next apply logistic regression model\n1. standardize the Age column","b0ddf4be":"Random Forest model achieves even better results than the GBRT model especially on datasets resampled by SMOTE strategy and SMOTE+Tomkel Links strategy (~94%)","be1f3a63":"# Oversampling and Undersampling","ec4ef29b":"From the graph it is clear that the dataset is highly **inbalanced** with around **86%** of class 0. And it gives the naive baseline in which the classifier predicts that every sample value is 0(assume everyone died). It's not surprising that the naive classifer could achieve high accurancy because of the nature of dataset inbalance. One of the ways to deal with inbalanced dataset is **oversampling** the minority(in this case, the survived people)","405ca83d":"# Import and Preprocess the Dataset","2616bf19":"GBRT with Tomkel Links sampling strategy and GBRT with random under sampling strategy both achieved accuracy score higher than the baseline(~88%)\n\n*Notice that GBRT is a tree based algorithm, so normalization of the Age column is not really necessary here. It is the same case for the following Random Forest Model as well. ","ac32c5a3":"From the Confusion Matrix we notice that the Logistic regression with the Tomkel links sampling strategy almost does the same prediction as the baseline naive model dose(predict that everyone is dead). Thus it achieves a rather higher accuracy score(~86%)."}}