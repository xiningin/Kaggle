{"cell_type":{"5a400bc1":"code","b14d281e":"code","7d3767c2":"code","9d77a367":"markdown","7c389e9a":"markdown","b8a5015d":"markdown"},"source":{"5a400bc1":"%%writefile submission.py\nfrom collections import defaultdict\nfrom itertools import chain, combinations\nimport random\nimport sys\nfrom typing import *\n\nimport numpy as np\nfrom pydash import flatten\n\n\nclass RPSNaiveBayes():\n    def __init__(self, max_memory=20, verbose=True):\n        self.max_memory = max_memory\n        self.verbose    = verbose\n        self.history = {\n            \"opponent\": [],\n            \"rotn\":     [],\n            \"expected\": [],\n            \"action\":   [],\n        }\n        # self.root_keys = ['action','opponent','rotn','expected']\n        self.root_keys = ['action','opponent']\n        self.keys = [\n            \",\".join(combo)\n            for n in range(1,len(self.root_keys)+1)        \n            for combo in combinations(self.root_keys, n)\n        ]\n        # self.keys = ['action', 'opponent', 'rotn', 'action,opponent', 'action,rotn', 'opponent,rotn', 'action,opponent,rotn']\n        self.memory = {\n            key: defaultdict(lambda: np.array([0,0,0]))\n            for key in self.keys\n        }\n        \n    def __call__(self, obs, conf):\n        return self.agent(obs, conf)\n\n\n    # obs  {'remainingOverageTime': 60, 'step': 1, 'reward': 0, 'lastOpponentAction': 0}\n    # conf {'episodeSteps': 10, 'actTimeout': 1, 'runTimeout': 1200, 'signs': 3, 'tieRewardThreshold': 20, 'agentTimeout': 60}\n    def agent(self, obs, conf):\n        # print('obs', obs)\n        self.update_state(obs, conf)\n\n        views          = self.get_current_views()\n        log_likelihood = self.get_log_likelihood(views)\n        probability    = self.get_probability(log_likelihood)\n\n        expected = random.choices( population=[0,1,2], weights=probability, k=1 )[0]\n        action   = int(expected + 1) % conf.signs\n        self.history['expected'].insert(0, expected)\n        self.history['action'].insert(0, action)\n\n        if self.verbose:\n            print(f'step = {obs.step:4d} | action = {action} | expected = {expected} | probability', probability.round(3), 'log_likelihood', log_likelihood.round(3))\n\n        return int(action)\n\n\n    def update_state(self, obs, conf):\n        if obs.step > 0:\n            rotn = obs.lastOpponentAction - self.history['action'][0] \n\n            self.history['opponent'].insert(0, obs.lastOpponentAction % conf.signs)\n            self.history['rotn'].insert(0, rotn)\n\n        for keys in self.memory.keys():\n            memories = self.get_new_memories(keys)\n            for value, path in memories:\n                self.memory[keys][path][value] += 1\n\n\n    def get_key_min_length(self, keys: str) -> int:\n        min_length = min([ len(self.history[key]) for key in keys.split(',') ])\n        return min_length\n\n\n    def get_new_memories(self, keys: Union[str,List[str]]) -> List[Tuple[Tuple,int]]:\n        min_length = self.get_key_min_length(keys)\n        min_length = min(min_length, self.max_memory)\n        memories   = []\n        for n in range(1,min_length):\n            value = self.history[\"opponent\"][0]\n            paths = []\n            for key in keys.split(','):\n                path = self.history[key][1:n]\n                if len(path): paths.append(path)\n            paths = tuple(flatten(paths))\n            if len(paths):\n                memories.append( (value, paths) )\n        return memories\n\n\n    def get_current_views(self) -> Dict[str, List[Tuple[int]]]:\n        views = {\n            keys: [\n                tuple(flatten([value, paths]))\n                for (value, paths) in self.get_new_memories(keys)\n            ]\n            for keys in self.memory.keys()\n        }\n        return views\n\n\n    def get_log_likelihood(self, views: List[Tuple]) -> np.ndarray:\n        log_likelihoods = np.array([.0,.0,.0])\n        for keys in self.memory.keys():\n            count = np.sum( np.array(list(self.memory[keys].values())).shape )\n            for path in views[keys]:\n                try:\n                    n_unique = 3 ** len(path)\n                    freqs = self.memory[keys][path] * n_unique    \n                    probs = (freqs + 1) \/ ( count + n_unique )    # Laplacian Smoothing\n                    log_likelihood = [\n                        np.log(probs[a]) - np.log(probs[b] + probs[c])\n                        if (probs[b] + probs[c]) > 0 else 0.0\n                        for a, b, c in [ (0,1,2), (1,2,0), (2,0,1) ]\n                    ]\n                    log_likelihood = [ n if not np.isnan(n) else 0.0 for n in log_likelihood ]\n                    log_likelihoods += np.array(log_likelihood)\n                except ZeroDivisionError: pass\n\n        return log_likelihoods\n\n    \n    def get_probability(self, log_likelihood: np.ndarray) -> np.ndarray:\n        probability = np.exp(log_likelihood)\n        probability[ probability == np.inf ] = sys.maxsize \/ len(probability) \/ 2\n        probability = probability \/ np.sum(probability)\n        return probability\n        \n            \n    \n    \ninstance = RPSNaiveBayes()\ndef kaggle_agent(obs, conf):\n    return instance.agent(obs, conf)\n","b14d281e":"%run submission.py","7d3767c2":"from kaggle_environments import make\nimport random\nagent = RPSNaiveBayes(verbose=False)\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 100}, debug=True)\nenv.run([\"submission.py\", lambda obs, conf: obs.step % conf.signs ])\n# env.run([\"submission.py\", '..\/input\/rock-paper-scissors-xgboost\/submission.py'])\n# env.run([\"submission.py\", 'submission.py'])\nenv.render(mode=\"ipython\", width=600, height=600)","9d77a367":"# Evaluation","7c389e9a":"# Rock Paper Scissors - Naive Bayes\n\nThis is a Naive Bayes implemention of Rock Paper Scissors\n\nIt keeps track of previous memory patterns, along with the next move, and computes the log likelihood of each pattern repeating.","b8a5015d":"# Further Reading\n\nThis notebook is part of a series exploring Rock Paper Scissors:\n\nPredetermined\n- [PI Bot](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-pi-bot)\n- [Anti-PI Bot](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-anti-pi-bot)\n- [De Bruijn Sequence](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-de-bruijn-sequence)\n\nRNG\n- [Random Agent](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-random-agent)\n- [Random Seed Search](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-random-seed-search)\n- [RNG Statistics](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-rng-statistics)\n\nOpponent Response\n- [Anti-Rotn](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-anti-rotn)\n- [Sequential Strategies](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-sequential-strategies)\n\nStatistical \n- [Weighted Random Agent](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-weighted-random-agent)\n- [Statistical Prediction](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-statistical-prediction)\n- [Anti-Rotn Weighted Random](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-anti-rotn-weighted-random)\n\nMemory Patterns\n- [Naive Bayes](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-naive-bayes)\n- [Memory Patterns](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-memory-patterns)\n\nDecision Tree\n- [XGBoost](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-xgboost)\n- [Multi Stage Decision Tree](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-multi-stage-decision-tree)\n- [Decision Tree Ensemble](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-decision-tree-ensemble)\n\nEnsemble\n- [Multi Armed Stats Bandit](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-multi-armed-stats-bandit)\n\nRoShamBo Competition Winners\n- [Iocaine Powder](https:\/\/www.kaggle.com\/jamesmcguigan\/rps-roshambo-comp-iocaine-powder)\n- [Greenberg](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-greenberg)"}}