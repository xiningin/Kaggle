{"cell_type":{"39729764":"code","45656433":"code","731cdabc":"code","e5f115ac":"code","ecfc545a":"code","247984d6":"code","6c82b0ed":"code","e2ce179f":"code","b9797564":"code","27d6b619":"code","9f88b46d":"code","f9a1f829":"code","0f78e395":"markdown","ec0cf9eb":"markdown","71cb8527":"markdown","46942d1f":"markdown"},"source":{"39729764":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","45656433":"df=pd.read_csv(\"..\/input\/Iris.csv\")","731cdabc":"df.head()","e5f115ac":"df.Species.value_counts()","ecfc545a":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(df.Species)\ndf[\"Species\"]=le.transform(df.Species)\nprint(le.classes_)","247984d6":"temp=df.apply(lambda row: row['PetalLengthCm'] + row['PetalWidthCm'], axis=1)\nprint(df.Species.corr(pd.Series(temp)))\ndf['new1']=temp","6c82b0ed":"temp=df.apply(lambda row: abs(row['SepalLengthCm'] - row['PetalLengthCm']), axis=1)\nprint(df.Species.corr(pd.Series(temp)))\ndf['new2']=temp","e2ce179f":"import xgboost as xgb\nimport matplotlib.pyplot as plt\n\ntrain_y = df[\"Species\"].values\ntrain_X = df.drop(['Species','Id'], axis=1)\n\nxgb_params = {\n    'n_trees': 520, \n    'eta': 0.0045,\n    'max_depth': 4,\n    'subsample': 0.98,\n    'objective': 'multi:softmax',\n    'num_class':3 ,\n    'eval_metric': 'merror',\n   # 'base_score': np.mean(train_y), # base prediction = mean(target)\n    'silent': 1\n}\n\nfinal = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params), final, num_boost_round=200, maximize=True)\nfig, ax = plt.subplots(figsize=(6,2))\nxgb.plot_importance(model, max_num_features=7, height=0.8, ax=ax, color = 'coral')\nprint(\"Feature Importance by XGBoost\")\nplt.show()\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=500, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=420)\nmodel.fit(train_X, train_y)\nfeat_names = train_X.columns.values\n\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:7]\n\nplt.subplots(figsize=(6,2))\nplt.title(\"Feature importances by RandomForestRegressor\")\nplt.ylabel(\"Features\")\nplt.barh(range(len(indices)), importances[indices], color=\"green\", align=\"center\")\nplt.yticks(range(len(indices)), feat_names[indices], rotation='horizontal')\nplt.ylim([-1, len(indices)])\nplt.show()","b9797564":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=0.2, random_state=42)","27d6b619":"from sklearn import svm\nfrom sklearn.metrics import accuracy_score\nclf = svm.SVC()\nclf.fit(x_train, y_train)\npred=clf.predict(x_valid)\naccuracy_score(y_valid, pred)","9f88b46d":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=0.3, random_state=42)","f9a1f829":"from sklearn import svm\nfrom sklearn.metrics import accuracy_score\nclf = svm.SVC()\nclf.fit(x_train, y_train)\npred=clf.predict(x_valid)\naccuracy_score(y_valid, pred)","0f78e395":"### Simple dataset no need for tuning ... 100% accuracy with 20% test set and 30% test set. ","ec0cf9eb":"## Upvote if you like ","71cb8527":"## The new features are doing great , check the next graph for importance. ","46942d1f":"## Slight Feature Engineering "}}