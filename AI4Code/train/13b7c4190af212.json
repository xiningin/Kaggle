{"cell_type":{"a0e437ee":"code","b24c09a1":"code","b6c4984d":"code","f22d4116":"code","f113acd9":"code","6a749638":"code","d80de4f1":"code","32f2dc37":"code","9d54eca4":"code","3fa2f923":"code","b904f3ba":"code","95624d53":"code","68d3aa57":"code","d7472077":"code","89284948":"code","2e163429":"code","cf5a7a22":"code","bf5d1b4f":"code","99ab73d3":"code","563acadc":"code","f96cd84b":"code","0892bdcc":"code","0d9f8650":"code","6b442ed2":"code","5a38ac6d":"markdown","349b63de":"markdown","ffe6581d":"markdown","89dbc02c":"markdown","3d1e7c9c":"markdown","866068d3":"markdown","dc4275f4":"markdown","5cce588a":"markdown","c24ae013":"markdown","3bd26432":"markdown","4cc2bbce":"markdown","7b7c571d":"markdown","e4301206":"markdown"},"source":{"a0e437ee":"import torch\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","b24c09a1":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n        super(TransformerModel, self).__init__()\n        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n        self.model_type = 'Transformer'\n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.ninp = ninp\n        self.decoder = nn.Linear(ninp, ntoken)\n        self.init_weights()\n\n    def _generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        if self.src_mask is None or self.src_mask.size(0) != len(src):\n            device = src.device\n            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n            self.src_mask = mask\n\n        src = src.cuda()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n        self.src_mask = self.src_mask.cuda()\n        output = self.transformer_encoder(src, self.src_mask)\n        output = self.decoder(output)\n        return F.log_softmax(output, dim=-1)    \n    \n    def generate_sequence(self, sequence, seq_size=2):\n        sequence = sequence.unsqueeze(1)\n        generate_step = 0\n        while generate_step < seq_size:\n          output_word = torch.argmax(self.forward(sequence)[-1, :], dim=1).unsqueeze(0)\n          sequence = torch.cat((sequence, output_word), dim=0)\n          generate_step += 1\n        sequence = sequence.squeeze(1)\n        return sequence\n\n    \nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) \/ d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","b6c4984d":"import pandas as pd\ndummy_dataset = pd.read_csv('..\/input\/dummylanguage\/train.tsv', sep=\"\\t\", header=None)\ndummy_dataset.head()","f22d4116":"import torchtext\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.datasets import LanguageModelingDataset\nfrom torchtext.data import Example, RawField, BucketIterator, BPTTIterator\n\nDUMMY_TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), init_token='<sos>', eos_token='<eos>')\nmy_dataset = LanguageModelingDataset(\"..\/input\/dummylanguage\/train.tsv\", DUMMY_TEXT)\nDUMMY_TEXT.build_vocab(my_dataset)\ntrain_iter = BPTTIterator(my_dataset, batch_size=3, bptt_len=4)","f113acd9":"def words_in_sequence(seq, torch_text_field):\n    seq_list = seq.tolist()\n    words = [torch_text_field.vocab.itos[word_idx] for word_idx in seq_list]\n    return ' '.join(words)\n\ndef print_train_target_iterator(bptt_iterator, torch_text_field, only_first_batch=False):\n    for i in bptt_iterator:\n        for (train, target) in zip(i.text, i.target): \n            print(f'train: {words_in_sequence(train, torch_text_field)}')\n            print(f'target: {words_in_sequence(target, torch_text_field)}')\n            print()\n        if only_first_batch:\n            break","6a749638":"print_train_target_iterator(train_iter, DUMMY_TEXT)\n","d80de4f1":"ntokens = len(DUMMY_TEXT.vocab.stoi) # the size of vocabulary\nemsize = 20 # embedding dimension\nnhid = 1 # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 1 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 1 # the number of heads in the multiheadattention models\ndropout = 0.2 # the dropout value\nmodel = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)","32f2dc37":"print(model)","9d54eca4":"for i in train_iter:\n    print(f'Model input dimensions: {i.text.shape}')\n    break\nprint(f'Model output dimensions: {model(i.text).shape}')\n","3fa2f923":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\nmodel.train() \nfor j in range(100):\n    for i in train_iter:\n        optimizer.zero_grad()\n        output = model(i.text)\n        loss = criterion(output.view(-1, output.shape[-1]), i.target.view(-1).cuda())\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n        print(f'Loss: {loss.item()}')\n    ","b904f3ba":"def predict_sequence(source_sentence, torch_text_field, model, seq_size=2):\n    print(f\"Source: {' '.join(source_sentence)}\")\n    generated_sequence = model.generate_sequence(torch_text_field.numericalize([source_sentence]).to(device).squeeze(1),\n                                                 seq_size)\n    print(f'Result: {words_in_sequence(generated_sequence, torch_text_field)}')","95624d53":"model.eval()\n\npredict_sequence([\"abc\",\"def\"], DUMMY_TEXT, model)\npredict_sequence([\"lmn\",\"opq\"], DUMMY_TEXT, model, 3)\n","68d3aa57":"TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), init_token='<sos>', eos_token='<eos>', lower=True)\ntrain_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\nTEXT.build_vocab(train_txt)\n\ndef batchify(data, bsz):\n    data = TEXT.numericalize([data.examples[0].text])\n    # Divide the dataset into bsz parts.\n    nbatch = data.size(0) \/\/ bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\ntrain_data = batchify(train_txt, batch_size)\nval_data = batchify(val_txt, eval_batch_size)\ntest_data = batchify(test_txt, eval_batch_size)","d7472077":"train_iter = BPTTIterator(train_txt, batch_size=20, bptt_len=35)","89284948":"ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\nemsize = 200 # embedding dimension\nnhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2 # the number of heads in the multiheadattention models\ndropout = 0.2 # the dropout value\nmodel = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)","2e163429":"print(model)","cf5a7a22":"print_train_target_iterator(train_iter, TEXT, True)\n","bf5d1b4f":"for i in train_iter:\n    print(f'Model input dimensions: {i.text.shape}')\n    break\nprint(f'Model output dimensions: {model(i.text).shape}')","99ab73d3":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=5.0)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\nbptt = 35 \n\nimport time\ndef train():\n    model.train() \n    total_loss = 0.\n    start_time = time.time()\n    for batch, i in enumerate(train_iter):\n        data = i.text\n        targets = i.target\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output.view(-1, output.shape[-1]), i.target.view(-1).cuda())\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n        total_loss += loss.item()\n        log_interval = 200\n        if batch % log_interval == 0 and batch > 0:\n            cur_loss = total_loss \/ log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}\/{:5d} batches | '\n                  'lr {:02.2f} | ms\/batch {:5.2f} | '\n                  'loss {:5.2f} | ppl {:8.2f}'.format(\n                    epoch, batch, len(train_data) \/\/ bptt, scheduler.get_lr()[0],\n                    elapsed * 1000 \/ log_interval,\n                    cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(eval_model, data_source):\n    def get_batch(source, i):\n        seq_len = min(bptt, len(source) - 1 - i)\n        data = source[i:i+seq_len]\n        target = source[i+1:i+1+seq_len].view(-1)\n        return data, target\n    \n    eval_model.eval() \n    total_loss = 0.\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            output = eval_model(data)\n            output_flat = output.view(-1, ntokens)\n            total_loss += len(data) * criterion(output_flat, targets).item()\n    return total_loss \/ (len(data_source) - 1)","563acadc":"import time\nbest_val_loss = float(\"inf\")\nepochs = 4 \nbest_model = None\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train()\n    val_loss = evaluate(model, val_data)\n    print('-' * 89)\n    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                     val_loss, math.exp(val_loss)))\n    print('-' * 89)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model = model\n\n    scheduler.step()","f96cd84b":"test_loss = evaluate(best_model, test_data)\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)","0892bdcc":"model.eval()\n\npredict_sequence([\"american\",\"football\"], TEXT, model)\n\n","0d9f8650":"predict_sequence([\"conrad\",\"said\"], TEXT, model, 7)","6b442ed2":"predict_sequence([\"players\",\"play\"], TEXT, model, 4)","5a38ac6d":"Now let's evaluate some samples of Transformer trained on Wiki2Text dataset :) ","349b63de":"Many thanks in advance for any comments or suggestions! \n# Acknowledgements: \n* [Pytorch Transformer Tutorial](https:\/\/pytorch.org\/tutorials\/beginner\/transformer_tutorial.html)\n* Implementation of `generate_sequence()` method from [Medium post](https:\/\/medium.com\/analytics-vidhya\/use-torchtext-and-transformer-to-create-your-quote-language-model-step-by-step-95ffc0192e12)","ffe6581d":"Now applying Transformer on WikiText2 dataset - same as Pytorch tutorial. Only difference is that we will use `BPTTIterator`: ","89dbc02c":"We could see how this `BPTTIterator` works on the dummy dataset:\n* It iterates in batches of data (batch size is 3 in this example)\n* The target variable for each entry of the batch is the next word of the sequence: For example: abc -> def\n* Since sequence size is 4, we define `bptt_len` as 4 as well, so it stops iterating at the last token ","3d1e7c9c":"Code below is basically copy and paste from tutorial, except:\n* Adding `.cuda()` on tensors to assure all processing to GPU \n* Adding `generate_sequence()` method to apply the model iterativelly on a sequence and generate an output sequence - This code was based on the following [Medium post ](https:\/\/medium.com\/analytics-vidhya\/use-torchtext-and-transformer-to-create-your-quote-language-model-step-by-step-95ffc0192e12)","866068d3":"I have created a dummy dataset to be trained as a language modelling. ","dc4275f4":"This notebook is built based on [Pytorch Transformer Tutorial.](https:\/\/pytorch.org\/tutorials\/beginner\/transformer_tutorial.html)\n\nThe purpose of this notebook is to understand Pytorch Transformer Tutorial in more details.  \n\nIt differs from original tutorial in mainly 3 aspects:\n1. Applying `Transformer` model on a simplified\/dummy dataset first just to have an idea of how it works\n2. Using `BPTTIterator` module to iterate through training data instead of using explicit defined `get_batch` function (although this function is still using on model evaluation)\n3. Running on GPU to speed up computation\n","5cce588a":"* Input to the model is: 4x3 matrix (sequence length x batch size) \n* Model output: 4x3x16 (16 is vocabulary size)\n* Therefore, for a single input word, in order to predict the next word, the model output is a [LogSoftmax](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.LogSoftmax.html?highlight=logsoftmax) on the values of the vocabulary ","c24ae013":"We are using a single head attention model with embedding dimension `20` and the size of vocabulary `16`:","3bd26432":"Adding dummy dataset as language modeling and declaring `BPTTIterator` on this dataset. ","4cc2bbce":"Declaring a simplified `TransformerModel` to apply on dummy dataset: ","7b7c571d":"Now lets evaluate if model could learn the language model on the dummy dataset:","e4301206":"Training the model on dummy dataset. Since we have a single batch, we are training 100 epochs:"}}