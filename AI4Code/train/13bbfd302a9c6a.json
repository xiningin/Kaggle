{"cell_type":{"cf7503d4":"code","d2c08e40":"code","755bf7f3":"code","b3a5a218":"code","c2e3b68c":"code","6c9dc0bc":"code","154d866f":"code","c663e774":"code","44d66078":"code","e3ff4a1c":"code","0c65e45c":"code","38857c2a":"code","21579666":"code","6dce4e05":"code","8d827338":"code","66480368":"code","157aa77a":"code","e32b81d8":"code","ba20ac4b":"code","bd68b52c":"code","e8f12b8f":"code","0f0136bd":"markdown","8b5be9b2":"markdown","112274c9":"markdown","2d80bf92":"markdown","8b7cd48a":"markdown","66ab8913":"markdown","a84ecf04":"markdown","859ec400":"markdown","ee43e690":"markdown","e6aa16a1":"markdown","1bc7090b":"markdown","da4405bb":"markdown","18ee43a4":"markdown","5cb46264":"markdown","f04c9c98":"markdown","1d217cb0":"markdown","61d20f2e":"markdown","9613a1e5":"markdown","d79dc0a0":"markdown","529d7812":"markdown","f057eaf4":"markdown","3afcb94e":"markdown","1ae11d56":"markdown","7bdb90bb":"markdown","834c859a":"markdown","c21ad01f":"markdown","7d542ca1":"markdown","0647b546":"markdown","a121b33e":"markdown","d2227fc1":"markdown","048a375b":"markdown","1bcd34e2":"markdown","076eea5a":"markdown","c29d0490":"markdown"},"source":{"cf7503d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport sklearn\nfrom sklearn import impute\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2c08e40":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","755bf7f3":"y = train_data[\"Survived\"]\nfeatures = [\"Age\", \"Pclass\", \"SibSp\", \"Parch\"]\nX = train_data[features]","b3a5a218":"plt.figure(figsize = (16,8))\nsns.swarmplot(x = train_data[\"Pclass\"], y = train_data[\"Fare\"], hue = train_data[\"Survived\"])\nplt.hlines(y = 30,xmin = -1, xmax = 3, color = 'red')\nL = plt.legend()\nL.get_texts()[0].set_text(\"Perished\")\nL.get_texts()[1].set_text(\"Survived\")","c2e3b68c":"train_data.describe()","6c9dc0bc":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer()","154d866f":"X = pd.DataFrame(imputer.fit_transform(X))","c663e774":"numSex = train_data[\"Sex\"]\nnumSex\n\nfor i in range(len(numSex)): \n    if numSex[i] == \"male\":\n        numSex[i] = 0\n    if numSex[i] == \"female\":\n        numSex[i] = 1\n        \nprint (numSex)","44d66078":"numSex = numSex[:,np.newaxis]\nprint(numSex.shape)\nX = np.append(X,numSex,1)","e3ff4a1c":"X_train = X[:700,:]\nX_val = X[700:,:]\ny_train = y[:700]\ny_val = y[700:]","0c65e45c":"from tensorflow import keras\nfrom tensorflow.keras import layers \nfrom tensorflow.keras import callbacks","38857c2a":"from tensorflow.keras.callbacks import EarlyStopping\n\ncallback = EarlyStopping(\n    monitor = 'val_loss',\n    min_delta=0.001, \n    patience=20, \n    restore_best_weights=True,\n)","21579666":"model = keras.Sequential([\n\n    layers.Dense(100, activation='relu', input_shape=[5]),\n    layers.Dense(75, activation='relu'),  \n    layers.Dense(50, activation='relu'),\n    layers.Dense(10, activation='relu'),  \n    layers.Dense(1, activation='sigmoid'),\n    \n])","6dce4e05":"model.compile(\n    optimizer = 'adam',\n    loss = 'binary_crossentropy'\n)","8d827338":"X_train = tf.convert_to_tensor(X_train ,dtype=tf.float32)\nX_val = tf.convert_to_tensor(X_val ,dtype=tf.float32)","66480368":"history = model.fit(\nX_train,y_train,\nvalidation_data=(X_val, y_val),\nbatch_size = 700,\nepochs = 500,\ncallbacks= [callback],\nverbose=0\n)","157aa77a":"\nlossdf = pd.DataFrame(history.history)\nlossdf.loc[:, ['loss', 'val_loss']].plot()\nplt.figure(figsize=(16,8))\nprint()\nprint(\"Minimum loss on validation set: {}\".format(lossdf['val_loss'].min()))","e32b81d8":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nX_test = test_data[features]\nX_test = pd.DataFrame(imputer.transform(X_test))\nTest_sex = test_data[\"Sex\"]\nfor i in range(len(Test_sex)): \n    if Test_sex[i] == \"male\":\n        Test_sex[i] = 0\n    if Test_sex[i] == \"female\":\n        Test_sex[i] = 1\n        \nTest_sex = Test_sex[:,np.newaxis]\nX_test = np.append(X_test,Test_sex,1)\n\nX_test = tf.convert_to_tensor(X_test,dtype = tf.float32)","ba20ac4b":"y_test = model.predict(X_test)\n\nprint(y_test[4])","bd68b52c":"predictions = []\n\nfor i in range(len(y_test)):\n    if y_test[i] >= 0.5:\n        predictions.append(\"1\")\n    else:\n        predictions.append(\"0\")\n        \nprint (predictions)","e8f12b8f":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","0f0136bd":"![sigmoid paint.png](attachment:f99d0cfa-5e35-48e9-861a-6de53847ea0d.png)","8b5be9b2":"Finally, lets convert the data into a CSV file with each prediction coresponding to each passanger ID. The CSV is the file that will submitted to the competition.","112274c9":"(Very rough sketch)","2d80bf92":"The graph above represents the basic trends that the loss of the training and validation sets will follow after each epoch as the neural network is being trained. (Note that this graph is a quick sketch and not an actual graph formed using any data)\n\nAs you can see after a certain number of epochs, the loss of the validation set starts to increase, meaning our neural network is becoming less accurate. \n\n\"But how can this be, surely the more we train it the more accurate it will become, the loss of the training set is still decreasing?\"\n\nThis is a problem known as overfitting, where our neural network learns the trends in our training set too well, and not the trends in the actual scenario. Although the training set is the largest amount of true data we have, it is still limited, meaning that not every single feature that could be true or false is part of it. By learning the exact trends of the training set, our neural network will incorrectly classify data that does not exactly match these trends, but in reality, matches the actual trends in the scenario. \n\nThis is where the validation set comes in. This is a subset of the training set that the algorithm hasn't used to train, meaning that it's data the neural network hasn't seen before, but knows the answer to. We can run this through the neural network after every epoch to see how accurate our neural network will be on new data.\n\nAbout halfway across the graph you can see that the loss of our validation set is at its lowest.\n","8b7cd48a":"As you can see. Each one of our prediction is decimal value, not 0 or 1. In order to use this data we have to convert it into 1s and 0s.\n\nSince each value is a decimal between 0 and 1, we can interoperate a percentage chance of the person surviving \n\nFor our predictions, were going to predict that anyone who had a 50% or higher chance of surviving, survived. So our threshold value is 0.5.\n\nBy creating an empty array, we can compare each one of our predictions to sort them int survived(1) and perished(0) and append them in order to the array\n\n","66ab8913":"![binary crossentrapy equation( logi cost function).png](attachment:dda7a8e7-5aa8-45df-a939-b36766dca5cd.png)","a84ecf04":"Let\u2019s have a closer look at the numeric data that will be used in our neural network","859ec400":"**The neural network**\n\nLet\u2019s begin by importing the more specific sub libraries we'll need to construct our neural network\n","ee43e690":"Now let\u2019s make our predictions on the test data.\n\nFirstly, just like our test data, lets select our features, and convert our 'Sex' feature to numeric values.  \n","e6aa16a1":"This is what the predictions looks like after each example has been assigned the correct label","1bc7090b":"**Next lets have a look at our training data**","da4405bb":"If we stop training the neural network half way through we will have the most accurate weights for our scenario.","18ee43a4":"Here we are converting the training set and validation set into the correct data type for tensorflow functions ","5cb46264":"Next is our early stopping callback function:\n* monitor: the data the function looks for the changes in \n* min_delta: the minimum amount of change in the loss before the function starts to stop the training algorithm\n* patience: how many epochs the algorithm waits after a change in the loss is below the min data to makes sure that the change wasn't an anomalous value\n* restore_best_weights: after waiting for the number of epochs that the patience is set to, it will restore the most accurate weights\n","f04c9c98":"![sigmoid equation paint.png](attachment:2445d068-16fe-4fdd-aeb0-925da406cc11.png)","1d217cb0":"Model.predict feeds our test data into the neural network and produces the predicted value for each value as 1D array, which is called Y_test ","61d20f2e":"As you can see, there are some third class fares that are higher than most common fare in first class (indicated by the red line) and a majority of those passengers in third class perished compared to first class passengers who mostly survived. \n\nUsing this data would negatively affect our neural network. Instead we will just use the Pclass feature as it represents the survival based on class trend better than fare.\n","9613a1e5":"Lastly, lets break down our data into the actual training set and the validation set. Normally, Raw data is split up by these ratios:\n* Training set: 60%\n* Validation set: 20%\n* Test set: 20%\n\nSince we are already given a test set as our submission data and being given a relatively small data set of under 1000 examples (891), we will break down our training data into around 80% training and 20% validation\n\n891 * 0.8 = 712.8 \u2248 700\n\nBy looking at some examples, we can tell that the data isn't ordered in any way, so there is little reason to pick randomly from the training set\n","d79dc0a0":"**Titanic Tensor flow neural netowrk**\n\nHi there, and welcome to my first notebook for the titanic competition that also contains a written description\n\nTo begin with lets import all the main libraries required for the task ahead:\n* Tensorflow: to create and train the neural network\n* Numpy: for mathematical edits and simply matrices operations\n* Pandas data frames: to allow easy reading and modifications to CSV files\n* Matplotlib and pyplot: to visualize the training of the neural network\n* Seaborn: to allow easy plotting of more complicated graphs\n* sklearn and impute: to allow a simple way to fill in the gaps in the data provided \n\nAs well as importing the libraries lets import the data needed for the competition\n3 CSV files are in the data provided for the competition:\n* Train.csv: for the training set and what we will derive the validation set from\n* Test.csv: the unlabelled data that we will be submitting the predictions for\n* gender_submission.csv: an example submission done by making predictions on one feature. Demonstrates the format and data type the submission should be made in\n","529d7812":"![loss graph early stopping.png](attachment:dd33c348-0a23-41c1-b37b-4fb3060adc8a.png)","f057eaf4":"As you can see each example has 12 features. For this neural network, we are only going to use the relevant numeric features as well as 'Sex' which we will convert to numerical values later on.\n\nFirst we will separate the feature dictating whether each person Survived or Perished('Survived') as this is what our neural network will be training against. \n\nAfter that we will create our training set out of the 4 main numeric features we will be using:\n* Age\n* Pclass: the class of cabin the passenger was staying in\n* SibSp: number of siblings \/ spouses aboard\n* Parch: number of parents \/ children aboard\n\n","3afcb94e":"Next let's append the list of values to our current training set so they apply with the same passengers","1ae11d56":"Sigmoid function and graph:","7bdb90bb":"where:\n* J(\u03b8): loss\n* m: number of examples\n* y(i): the label(true value) of that i\u2019th training example\n* h(X(i)): hypothesis of X, the current predicted value produced by the neural network for the i\u2019th training example.   \n\nDepending on what the label for each example is, the function compares the hypothesis to that value alone. It does this by multiplying the label(Y(i)) to the specific part of the function that calculates the difference for the corresponding label. For example:\n\n* If y(i) = 0, then y(i) log(h(x(i))) = 0. So only (1-y(i)) log(1-h(x(i)))  will be calculated.\n* If y(i) = 1, (1-y(i)) log(1-h(x(i))) = 0. So only y(i) log(h(x(i)))  will be calculated.\n\nSince our final activation function is the sigmoid function, our h(X) will always be between 0 and 1 since the 0 and 1 are the 2 asymptotes of the function.\n\n","834c859a":"Now let\u2019s design the neural network\nI have decided to put the hidden layers in a 100, 75, 50, 10 structure with each layer having the rectified linear unit function as their activation function.\n\nSince we are doing a classification problem with two groups, the output layer has one node that is activated by the sigmoid function, which will tell us the likely hood of the person surviving as a decimal value between 0 and 1.\n","c21ad01f":"**Early stopping call back function**\n\nCall back functions are functions that run after every epoch while training the neural network. The early stopping call back will stop the algorithm if the overall change to the loss of the validation set is below a certain amount. This can stop the algorithm from over fitting the data and having a high variance.\n","7d542ca1":"Now lets train the model.\n\nX_train and Y_train are the training sets and X_val and Y_val are inputted as the validation set.\n\nBatch size is the number of data points we use each epoch to train our algorithm, since the data set is relatively small, we'll use a full batch(all the values) as computing over 700 examples isn't very computationally expensive.\n\nNext is the epochs, which is the number of training cycles the algorithm will take. Since we are using early stopping, we can set this to a very high number as the algorithm will stop before it every reaches it, allowing it more freedom to the most accurate value.\n\nLastly we let the algorithm know what callbacks we are using, which in this case is just early stopping.\n\nVerbose is a simple parameter which lets the algorithm know whether to print the loss data and current epoch while it is training. Since we'll be plotting our data we don't need to see this.   \n","0647b546":"Next let define our optimizer and loss function.\n\nFor the optimizer we will chose 'adam' as it's a well known algorithm that more efficient and effective than stochastic gradient decent. \n\nFor the loss functions we will use 'binary_crossentropy' as the data we are training against (y) is comprised of 0's and 1's\n\nThe binary cross entropy loss function is:\n","a121b33e":"Why leave out Fare?\n\nFare is a numeric value that is have left out of the list. Let\u2019s plot the data to see why.\n\nThe graph used will be a swarm plot with Fare being the continuous variable and Pclass as the categorical data. It is well known that on the titanic, the first class passengers were prioritised when deciding who to give life boat positions to, followed by the second class passengers. Fare is a similar indicator to class as the first class cabins were the most expensive, followed by second and third class. weather ach passenger survived or perished will also be indicated on the graph.   \n","d2227fc1":"As you can see our neural network makes giant improvements at the begging and the steps in loss function decrease over the number of epochs. algorithm stops at around 150 epochs meaning our early stopping call back function is working.","048a375b":"Next let\u2019s add 'Sex' to our data. Since women and children were prioritized when deciding who would take the small number of available spaces, 'Sex' is a very important feature to have when prediction who survived.\n\nTo convert the feature into numeric values we will scan through all the values and set all the 'male' values to '0' and 'female' values to '1'\n","1bcd34e2":"As you can see 'Age' has a lower count that all the other features, meaning it is missing some values. Let's solve this.\n\nUsing the impute functions from the sklearn library, we can use a simple imputer to fill in the missing values. A simple imputer will replace all the missing values with mean value of that feature which in our case is for 'Age' is about 30 years old.\n","076eea5a":"![loss graph.png](attachment:973f369a-5b4b-4740-a4cb-ef6df21be25f.png)","c29d0490":"Let\u2019s plot the loss after each epoch to see how the neural network did. "}}