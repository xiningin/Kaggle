{"cell_type":{"1d9c4bc3":"code","3671e52a":"code","f9c0db07":"code","3f38d6f9":"code","4ff9809d":"code","4e9c4804":"code","2aecedfe":"code","b1cf5fc0":"code","823d05e1":"code","8dc9336e":"code","89ddf976":"code","99e1a6e5":"code","b06f5821":"code","8d4c95e5":"code","33a3c9a3":"code","6dfa6a34":"code","2ccaaa50":"code","90d5764d":"code","954dba51":"code","6f34f778":"code","e29451e7":"code","95ab8b1c":"code","2b065770":"code","bd28c4e8":"code","3d4eb73f":"code","ec7120a9":"code","0ecba638":"code","cfea92c2":"code","c3168bb0":"code","84431e11":"code","dceb96ff":"code","b510b9b0":"code","d6b52c80":"code","6b0a3b5d":"code","b1e5be95":"code","60948875":"code","fcfd2f53":"code","6b5d46d3":"code","6fcd918a":"code","d63fe381":"code","5140795c":"code","51081bac":"code","6adc68b5":"code","16ff6753":"code","a766d941":"code","11490b4c":"code","b6a02ee7":"code","f46514de":"code","a82944b6":"code","d0d8b8d0":"code","822c128f":"code","4c61675e":"code","d260eb87":"code","9010e7d8":"code","2e650da0":"code","b5d6a988":"code","8c2354ca":"code","95fb9b43":"code","43053743":"code","41ec2591":"code","7a8e3d3e":"code","15918335":"code","a186f07e":"code","62289676":"code","432c42a7":"code","d1b374fb":"code","d23761e6":"code","f1e27850":"code","349fd7a5":"code","11710e43":"code","6fe65aa5":"markdown","be4ef3f9":"markdown","871477c7":"markdown","508cb76b":"markdown","a3683716":"markdown","89b8fdc5":"markdown","2966ced8":"markdown","49663842":"markdown","5c8bb77f":"markdown","b6046959":"markdown","ef797d1b":"markdown","9d0d8b30":"markdown","02ea4ac7":"markdown","b701f733":"markdown","5a4c1ef3":"markdown","9831727e":"markdown","cded3dc8":"markdown","e219a607":"markdown","0ee76d67":"markdown","941be1e7":"markdown","d494d7f2":"markdown","adc7d3c3":"markdown","edefcd81":"markdown","c8059eb6":"markdown","2ba99b08":"markdown","eaa03ce8":"markdown","bb997473":"markdown","004c392b":"markdown","9404e02f":"markdown","6ce04208":"markdown","c5b05f7c":"markdown","45198367":"markdown","0c2b8993":"markdown","f47c89cb":"markdown","3dc479da":"markdown","87a4353c":"markdown","d288358e":"markdown","d83d728b":"markdown","2d120db2":"markdown","201fc12c":"markdown","6964c559":"markdown","940e1a3b":"markdown","a1ffb9fd":"markdown","7eb50e9a":"markdown","6c88f904":"markdown","2cc831fa":"markdown","9c981f68":"markdown","f7ab4aa5":"markdown","7097e58d":"markdown","880e02e0":"markdown"},"source":{"1d9c4bc3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.graph_objs as go \nimport plotly.express as px\nfrom scipy.stats import chi2_contingency\nplt.style.use(\"ggplot\")","3671e52a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f9c0db07":"df20 = pd.read_csv(\"..\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv\")\ndf19 = pd.read_csv(\"..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\")\ndf18 = pd.read_csv(\"\/kaggle\/input\/dataset2018\/multipleChoiceResponses.csv\")\n","3f38d6f9":"#Espacio muestral a trabajar: Colombia\ndf20c = df20.groupby('Q3').get_group('Colombia')\ndf19c = df19.groupby('Q3').get_group('Colombia')\ndf18c = df18.groupby('Q3').get_group('Colombia')","4ff9809d":"#Types of ask\nask20 = df20.loc[0].to_dict()\nask19 = df19.loc[0].to_dict()\nask18 = df18.loc[0].to_dict()","4e9c4804":"#What is your age (Colombia)?\nage_18c = pd.DataFrame(pd.Series(df18c.Q2.value_counts().sort_index()))\nage_19c = pd.DataFrame(pd.Series(df19c.Q1.value_counts().sort_index()))\nage_20c = pd.DataFrame(pd.Series(df20c.Q1.value_counts().sort_index()))\n\n#What is your age (worldwide)?\nage_18 = pd.DataFrame(pd.Series(df18.Q2.value_counts().sort_index()))\nage_19 = pd.DataFrame(pd.Series(df19.Q1.value_counts().sort_index()))\nage_20 = pd.DataFrame(pd.Series(df20.Q1.value_counts().sort_index()))","2aecedfe":"print(f'In 2020 there were {age_20.Q1.sum()} people surveyed')\nprint(f'In 2019 there were {age_19.Q1.sum()} people surveyed')\nprint(f'In 2018 there were {age_18.Q2.sum()} people surveyed')","b1cf5fc0":"age_18c = age_18c.rename_axis('Range').reset_index()\nage_19c = age_19c.rename_axis('Range').reset_index()\nage_20c = age_20c.rename_axis('Range').reset_index()\nage_18c.rename(columns = {'Q2':'Count'}, inplace = True) \nage_19c.rename(columns = {'Q1':'Count'}, inplace = True) \nage_20c.rename(columns = {'Q1':'Count'}, inplace = True) \nage_18c['year'] = 2018\nage_19c['year'] = 2019\nage_20c['year'] = 2020\n\nage_18 = age_18.rename_axis('Range').reset_index()\nage_19 = age_19.rename_axis('Range').reset_index()\nage_20 = age_20.rename_axis('Range').reset_index()\nage_18.rename(columns = {'Q2':'Count'}, inplace = True) \nage_19.rename(columns = {'Q1':'Count'}, inplace = True) \nage_20.rename(columns = {'Q1':'Count'}, inplace = True) \nage_18['year'] = 2018\nage_19['year'] = 2019\nage_20['year'] = 2020","823d05e1":"color18 = 'rgb(125, 64, 55)'\ncolor19 = 'rgb(240, 141, 51)'\ncolor20 = 'rgb(26, 125, 80)'\n\nfig = go.Figure(data=[\n    go.Bar(name='2018', x=age_18c['Range'], y=age_18c['Count'], marker_color = color18),\n    go.Bar(name='2019', x=age_19c['Range'], y=age_19c['Count'], marker_color = color19),\n    go.Bar(name='2020', x=age_20c['Range'], y=age_20c['Count'], marker_color = color20)\n])\n# Change the bar mode\nfig.update_layout(\n    title='Age ranges in Colombia',\n    xaxis = dict(\n        title='Age ranges',\n        tickfont_size=14,\n    ),\n    yaxis=dict(\n        title='Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),\n    legend=dict(\n        x=0,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15, # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1 # gap between bars of the same location coordinate.\n)\nfig.show()","8dc9336e":"c20 = pd.crosstab(df20.Q1, df20.Q3)\nc19 = pd.crosstab(df19.Q1, df19.Q3)\nc18 = pd.crosstab(df18.Q2, df18.Q3)\n\nc20 = c20.drop(['What is your age (# years)?'],axis=0)\nc19 = c19.drop(['What is your age (# years)?'],axis=0)\nc18 = c18.drop(['What is your age (# years)?'],axis=0)\nc20 = c20.drop(['In which country do you currently reside?'],axis=1)\nc20 = c20.drop(['Ghana', 'United Arab Emirates', 'Sri Lanka', 'Nepal', 'Taiwan', 'Saudi Arabia'],axis=1)\nc19 = c19.drop(['In which country do you currently reside?'],axis=1)\nc18 = c18.drop(['In which country do you currently reside?'],axis=1)\nc18 = c18.drop(['I do not wish to disclose my location'],axis=1)\n\ncol_ref = c20.columns\n\nc19 = c19[col_ref]\nc18 = c18[col_ref]\n\nff20 = np.log(c20+1)\nff19 = np.log(c19+1)\nff18 = np.log(c18+1)\n#style image\nstyle2 = \"Blues\"\nstyle1 = 'Greens'","89ddf976":"plt.figure(figsize=(15,13))\nsns.set(font_scale=1.1)\nax1 = plt.subplot(131)\nax1 = sns.heatmap(ff20.T, cmap=style1, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('Country',fontsize=0)\nax1.set_xlabel('Age range',fontsize=0)\n# title\nax1.set_title('2020',fontsize=18)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(ff19.T, cmap=style1, \n           linewidth=0.3, cbar=False);\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Country',fontsize=0)\nax2.set_xlabel('Age range',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nCOUNTRY VS. AGE RANGE \\n\\n2019',fontsize=18)\n\nax3 = plt.subplot(133, sharey=ax1)\nax3 = sns.heatmap(ff18.T, cmap=style1, \n           linewidth=0.3, cbar_kws={'label': 'Logarithm'})\nplt.setp(ax3.get_yticklabels(), visible=False)\nax3.set_ylabel('Country',fontsize=0)\nax3.set_xlabel('Age range',fontsize=0)\n# title\nax3.set_title('2018',fontsize=18);","99e1a6e5":"#What is your gender for year in Colombia? \nsex_18 = pd.DataFrame(pd.Series(df18c.Q1.value_counts().sort_index()))\nsex_19 = pd.DataFrame(pd.Series(df19c.Q2.value_counts().sort_index()))\nsex_20 = pd.DataFrame(pd.Series(df20c.Q2.value_counts().sort_index()))","b06f5821":"sex_18 = sex_18.rename_axis('Sex').reset_index()\nsex_19 = sex_19.rename_axis('Sex').reset_index()\nsex_20 = sex_20.rename_axis('Sex').reset_index()\nsex_18.rename(columns = {'Q1':'Count'}, inplace = True) \nsex_19.rename(columns = {'Q2':'Count'}, inplace = True) \nsex_20.rename(columns = {'Q2':'Count'}, inplace = True) \nsex_18['year'] = 2018\nsex_19['year'] = 2019\nsex_20['year'] = 2020\nsex_20.loc[sex_20['Sex']==\"Man\", \"Sex\"] = \"Male\"\nsex_20.loc[sex_20['Sex']==\"Woman\", \"Sex\"] = \"Female\"","8d4c95e5":"fig = go.Figure(data=[\n    go.Bar(name='2018', x=sex_18['Sex'], y=sex_18['Count'], marker_color = color18),\n    go.Bar(name='2019', x=sex_19['Sex'], y=sex_19['Count'], marker_color = color19),\n    go.Bar(name='2020', x=sex_20['Sex'], y=sex_20['Count'], marker_color = color20)\n])\n# Change the bar mode\nfig.update_layout(\n    title='Gender in Colombia',\n    xaxis = dict(\n        title='Gender',\n        tickfont_size=14,\n    ),\n    yaxis=dict(\n        title='Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),\n    legend=dict(\n        x=0,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15, # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1 # gap between bars of the same location coordinate.\n)\nfig.show()","33a3c9a3":"g20 = pd.crosstab(df20.Q2, df20.Q3)\ng19 = pd.crosstab(df19.Q2, df19.Q3)\ng18 = pd.crosstab(df18.Q1, df18.Q3)\n\ng20 = g20.drop(['Nonbinary'],axis=0)\ng20 = g20.drop(['Prefer not to say'],axis=0)\ng20 = g20.drop(['Prefer to self-describe'],axis=0)\ng20 = g20.drop(['What is your gender? - Selected Choice'],axis=0)\ng19 = g19.drop(['Prefer not to say'],axis=0)\ng19 = g19.drop(['Prefer to self-describe'],axis=0)\ng19 = g19.drop(['What is your gender? - Selected Choice'],axis=0)\ng18 = g18.drop(['Prefer not to say'],axis=0)\ng18 = g18.drop(['Prefer to self-describe'],axis=0)\ng18 = g18.drop(['What is your gender? - Selected Choice'],axis=0)\ng20 = g20.drop(['In which country do you currently reside?'],axis=1)\ng20 = g20.drop(['Ghana', 'United Arab Emirates', 'Sri Lanka', 'Nepal', 'Taiwan', 'Saudi Arabia'],axis=1)\ng19 = g19.drop(['In which country do you currently reside?'],axis=1)\ng18 = g18.drop(['In which country do you currently reside?'],axis=1)\ng18 = g18.drop(['I do not wish to disclose my location'],axis=1)\n\ncol_ref = g20.columns\n\ng19 = g19[col_ref]\ng18 = g18[col_ref]\n\nffg20 = np.log(g20+1).T\nffg20 = ffg20[['Woman',\"Man\"]]\nffg20.rename(columns={'Woman': 'Female', 'Man': 'Male'}, inplace=True)\nffg19 = np.log(g19+1).T\nffg18 = np.log(g18+1).T\n","6dfa6a34":"plt.figure(figsize=(6,15))\nsns.set(font_scale=1.1)\nax1 = plt.subplot(131)\nax1 = sns.heatmap(ffg20, cmap= style1 , \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('Country',fontsize=0)\nax1.set_xlabel('Gender',fontsize=0)\n# title\nax1.set_title('2020',fontsize=18)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey= ax1)\nax2 = sns.heatmap(ffg19, cmap= style1 , \n           linewidth=0.3, cbar=False);\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Country',fontsize=0)\nax2.set_xlabel('Gender',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nCOUNTRY VS. GENDER \\n\\n2019',fontsize=18)\n\nax3 = plt.subplot(133, sharey= ax1)\nax3 = sns.heatmap(ffg18,  cmap= style1 ,\n           linewidth=0.3, cbar_kws={'label': 'Logarithm'});\nplt.setp(ax3.get_yticklabels(), visible=False)\nax3.set_ylabel('Country',fontsize=0)\nax3.set_xlabel('Gender',fontsize=0)\n# title\nax3.set_title('2018',fontsize=18);","2ccaaa50":"#What is the highest level of formal education that you have attained or plan to attain within the next 2 years for year in Colombia?\nedu_18 = pd.DataFrame(pd.Series(df18c.Q4.value_counts().sort_index()))\nedu_19 = pd.DataFrame(pd.Series(df19c.Q4.value_counts().sort_index()))\nedu_20 = pd.DataFrame(pd.Series(df20c.Q4.value_counts().sort_index()))","90d5764d":"edu_18 = edu_18.rename_axis('level_education').reset_index()\nedu_19 = edu_19.rename_axis('level_education').reset_index()\nedu_20 = edu_20.rename_axis('level_education').reset_index()\nedu_18.rename(columns = {'Q4':'Count'}, inplace = True) \nedu_19.rename(columns = {'Q4':'Count'}, inplace = True) \nedu_20.rename(columns = {'Q4':'Count'}, inplace = True) \nedu_18['year'] = 2018\nedu_19['year'] = 2019\nedu_20['year'] = 2020","954dba51":"edu_18.loc[edu_18['level_education']==\"Some college\/university study without earning a bachelor\u2019s degree\", \"level_education\"] = \"Without Bachelor's degree\"\nedu_19.loc[edu_19['level_education']==\"Some college\/university study without earning a bachelor\u2019s degree\", \"level_education\"] = \"Without Bachelor's degree\"\nedu_20.loc[edu_20['level_education']==\"Some college\/university study without earning a bachelor\u2019s degree\", \"level_education\"] = \"Without Bachelor's degree\"","6f34f778":"fig = go.Figure(data=[\n    go.Bar(name='2018', x=edu_18['level_education'], y=edu_18['Count'], marker_color = color18),\n    go.Bar(name='2019', x=edu_19['level_education'], y=edu_19['Count'], marker_color = color19),\n    go.Bar(name='2020', x=edu_20['level_education'], y=edu_20['Count'], marker_color = color20)    \n])\n# Change the bar mode\nfig.update_layout(\n    title='Highest level of formal education in Colombia',\n    xaxis = dict(\n        title='Education level',\n        tickfont_size=14,\n    ),\n    yaxis=dict(\n        title='Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),\n    legend=dict(\n        x=0,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15, # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1 # gap between bars of the same location coordinate.\n)\nfig.show()","e29451e7":"s20 = pd.crosstab(df20.Q3, df20.Q4)\ns20 = s20.drop(['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'], axis=1)\ns20 = s20.drop(['In which country do you currently reside?'],axis=0)\ns20.rename(columns = {'Some college\/university study without earning a bachelor\u2019s degree':'Without bachelor degree'}, inplace = True)\ns20.rename(columns = {'No formal education past high school':'Informal education'}, inplace = True)\ns19 = pd.crosstab(df19.Q3, df19.Q4)\ns19 = s19.drop(['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'], axis=1)\ns19 = s19.drop(['In which country do you currently reside?'],axis=0)\ns19.rename(columns = {'Some college\/university study without earning a bachelor\u2019s degree':'Without bachelor degree'}, inplace = True)\ns19.rename(columns = {'No formal education past high school':'Informal education'}, inplace = True)\n\ns18 = pd.crosstab(df18.Q3, df18.Q4)\ns18 = s18.drop(['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'], axis=1)\ns18 = s18.drop(['In which country do you currently reside?'],axis=0)\ns18.rename(columns = {'Some college\/university study without earning a bachelor\u2019s degree':'Without bachelor degree'}, inplace = True)\ns18.rename(columns = {'No formal education past high school':'Informal education'}, inplace = True)\n\n#mach country in all dataset\ns20 = s20.T.drop(['Ghana', 'United Arab Emirates', 'Sri Lanka', 'Nepal', 'Taiwan', 'Saudi Arabia'],axis=1)\ns19 = s19.T[col_ref]\ns18 = s18.T[col_ref]\n#log scale for grapic\nss20 = np.log(s20+1)\nss19 = np.log(s19+1)\nss18 = np.log(s18+1)\n","95ab8b1c":"plt.figure(figsize=(20,15))\nsns.set(font_scale=1.1)\nax1 = plt.subplot(131)\nax1 = sns.heatmap(ss20.T, cmap=style1, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('Country',fontsize=0)\nax1.set_xlabel('Level education',fontsize=0)\n# title\nax1.set_title('2020',fontsize=18)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(ss19.T, cmap=style1, \n           linewidth=0.3, cbar=False);\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Country',fontsize=0)\nax2.set_xlabel('Education level',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nCOUNTRY VS. EDUCATION LEVEL \\n\\n2019',fontsize=18)\n\nax3 = plt.subplot(133, sharey=ax1)\nax3 = sns.heatmap(ss18.T, cmap=style1, \n           linewidth=0.3);\nplt.setp(ax3.get_yticklabels(), visible=False)\nax3.set_ylabel('Country',fontsize=0)\nax3.set_xlabel('Level  education',fontsize=0)\n# title\nax3.set_title('2018',fontsize=18);","2b065770":"work_18 = pd.DataFrame(pd.Series(df18c.Q6.value_counts().sort_index()))\nwork_19 = pd.DataFrame(pd.Series(df19c.Q5.value_counts().sort_index()))\nwork_20 = pd.DataFrame(pd.Series(df20c.Q5.value_counts().sort_index()))","bd28c4e8":"work_18 = work_18.rename_axis('work').reset_index()\nwork_19 = work_19.rename_axis('work').reset_index()\nwork_20 = work_20.rename_axis('work').reset_index()\nwork_18.rename(columns = {'Q6':'Count'}, inplace = True) \nwork_19.rename(columns = {'Q5':'Count'}, inplace = True) \nwork_20.rename(columns = {'Q5':'Count'}, inplace = True) \nwork_18['year'] = 2018\nwork_19['year'] = 2019\nwork_20['year'] = 2020\nwork_20.loc[work_20['work']==\"Currently not employed\", \"work\"] = \"Not employed\"","3d4eb73f":"fig = go.Figure(data=[\n    go.Bar(name='2018', x=work_18['work'], y=work_18['Count'], marker_color = color18),\n    go.Bar(name='2019', x=work_19['work'], y=work_19['Count'], marker_color = color19),\n    go.Bar(name='2020', x=work_20['work'], y=work_20['Count'], marker_color = color20)    \n])\n# Change the bar mode\nfig.update_layout(\n    title='Current or most recent job title',\n    xaxis = dict(\n        title='Job title',\n        tickfont_size=14,\n    ),\n    yaxis=dict(\n        title='Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),\n    legend=dict(\n        x=0,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15, # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1 # gap between bars of the same location coordinate.\n)\nfig.show()","ec7120a9":"w20 = pd.crosstab(df20.Q3, df20.Q5)\nw20 = w20.drop(['Select the title most similar to your current role (or most recent title if retired): - Selected Choice'], axis=1)\nw20 = w20.drop(['Machine Learning Engineer'], axis=1)\nw20.rename(columns = {'Currently not employed':'Not employed'}, inplace = True)\nw19 = pd.crosstab(df19.Q3, df19.Q5)\nw19 = w19.drop(['Select the title most similar to your current role (or most recent title if retired): - Selected Choice'], axis=1)\nw18 = pd.crosstab(df18.Q3, df18.Q6)\nw18 = w18.drop(['Select the title most similar to your current role (or most recent title if retired): - Selected Choice'], axis=1)\nw20 = w20.drop(['In which country do you currently reside?'], axis=0)\n#mach country in all dataset columns\nw20 = w20.T.drop(['Ghana', 'United Arab Emirates', 'Sri Lanka', 'Nepal', 'Taiwan', 'Saudi Arabia'],axis=1)\nw19 = w19.T[col_ref]\nw18 = w18.T[col_ref]\n\n#log scale for grapic\nww20 = np.log(w20+1)\nww19 = np.log(w19+1)\nww18 = np.log(w18+1)\n#mach country in all dataset row\nrow_ref = ww20.T.columns\nww20 = ww20.T\nww19 = ww19.T[row_ref]\nww18 = ww18.T[row_ref]","0ecba638":"sns.set(font_scale=1.1)\nplt.figure(figsize=(20,15))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(ww20, cmap=style1, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('Country',fontsize=0)\nax1.set_xlabel('Job title',fontsize=0)\n# title\nax1.set_title('2020',fontsize=18)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(ww19, cmap=style1, \n           linewidth=0.3, cbar=False);\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Country',fontsize=0)\nax2.set_xlabel('Job title',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nCOUNTRY VS. JOB TABLE \\n\\n2019',fontsize=18)\n\nax3 = plt.subplot(133, sharey=ax1)\nax3 = sns.heatmap(ww18, cmap=style1, \n           linewidth=0.3);\nplt.setp(ax3.get_yticklabels(), visible=False)\nax3.set_ylabel('Country',fontsize=0)\nax3.set_xlabel('Job title',fontsize=0)\n# title\nax3.set_title('2018',fontsize=18);","cfea92c2":"je20 = pd.crosstab(df20.Q5, df20.Q4)\nje20.rename(columns = {'Some college\/university study without earning a bachelor\u2019s degree':'Without bachelor degree'}, inplace = True)\nje20 = je20.drop(['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'], axis=1)\nje20 = je20.drop(['Select the title most similar to your current role (or most recent title if retired): - Selected Choice'],axis=0)\nje20 = je20.T\nje20.rename(columns = {'Currently not employed':'Not employed'}, inplace = True)\nje20 = je20.drop(['Machine Learning Engineer'],axis=1)\nje19 = pd.crosstab(df19.Q5, df19.Q4)\nje19 = je19.drop(['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'],axis=1)\nje19.rename(columns = {'Some college\/university study without earning a bachelor\u2019s degree':'Without bachelor degree'}, inplace = True)\nje19 = je19.drop(['Select the title most similar to your current role (or most recent title if retired): - Selected Choice'], axis=0)\nje19 = je19.T\nje18 = pd.crosstab(df18.Q6, df18.Q4)\nje18 = je18.drop(['Select the title most similar to your current role (or most recent title if retired): - Selected Choice'], axis=0)\nje18 = je18.drop(['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'],axis=1)\nje18.rename(columns = {'Some college\/university study without earning a bachelor\u2019s degree':'Without bachelor degree'}, inplace = True)\nje18 = je18.T\n\n#log scale for grapic\nje20 = np.log(je20+1)\nje19 = np.log(je19+1)\nje18 = np.log(je18+1)","c3168bb0":"#mach country in all dataset row\nrow_ref = je20.columns\nje19 = je19[row_ref]\nje18 = je18[row_ref]","84431e11":"sns.set(font_scale=1.1)\nplt.figure(figsize=(20,5))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(je20, cmap=style2, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('Education level',fontsize=18)\nax1.set_xlabel('Job title',fontsize=0)\n# title\nax1.set_title('2020',fontsize=25)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(je19, cmap=style2, \n           linewidth=0.3, cbar=False);\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Country',fontsize=0)\nax2.set_xlabel('Job title',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nLEVEL EDUCATION VS. JOB TITLE \\n\\n2019',fontsize=25)\n\nax3 = plt.subplot(133, sharey=ax1)\nax3 = sns.heatmap(je18, cmap=style2, \n           linewidth=0.3);\nplt.setp(ax3.get_yticklabels(), visible=False)\nax3.set_ylabel('Country',fontsize=0)\nax3.set_xlabel('Job title',fontsize=0)\nax3.set_title('2018',fontsize=25);","dceb96ff":"plt.figure(figsize=(25,8))\nsns.set(font_scale=1.1)\nax1 = plt.subplot(131)\nax1 = sns.heatmap(je20.corr(),  linewidth=0.3,  cmap=style2,cbar=False);\nax1.set_ylabel('Job title',fontsize=0)\nax1.set_xlabel('Job title',fontsize=0)\n\n# title\nax1.set_title('2020',fontsize=18)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(je19.corr(),linewidth=0.3, cmap=style2, cbar=False);\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Country',fontsize=0)\nax2.set_xlabel('Job title',fontsize=0)\n# title\nax2.set_title('CORRELATION MATRIX \\n\\n2019',fontsize=18)\n\nax3 = plt.subplot(133, sharey=ax1)\nax3 = sns.heatmap(je18.corr(), linewidth=0.3,cmap=style2);\nplt.setp(ax3.get_yticklabels(), visible=False)\nax3.set_ylabel('Country',fontsize=0)\nax3.set_xlabel('Job title',fontsize=0)\nax3.set_title('2018',fontsize=18);","b510b9b0":"language_18 = pd.DataFrame({'Count': [df18c.Q16_Part_1.value_counts()[0],df18c.Q16_Part_2.value_counts()[0],df18c.Q16_Part_3.value_counts()[0],\n                                      df18c.Q16_Part_4.value_counts()[0],df18c.Q16_Part_5.value_counts()[0],df18c.Q16_Part_6.value_counts()[0],\n                                      df18c.Q16_Part_7.value_counts()[0],df18c.Q16_Part_8.value_counts()[0],df18c.Q16_Part_9.value_counts()[0],\n                                      df18c.Q16_Part_10.value_counts()[0],df18c.Q16_Part_12.value_counts()[0],\n                                      df18c.Q16_Part_13.value_counts()[0],df18c.Q16_Part_14.value_counts()[0],df18c.Q16_Part_15.value_counts()[0],\n                                      df18c.Q16_Part_16.value_counts()[0],df18c.Q16_Part_17.value_counts()[0],df18c.Q16_Part_18.value_counts()[0]], \n                                      'Programming_Languages': [\"Python\", \"R\", \"SQL\",\"Bash\",\"Java\" ,\"Javascript\",\"VBA\",\"C\",\"MATLAB\",\"Scala\",\n                                       \"Go\",\"C#\",\"PHP\",\"Ruby\",\"SAS\",\"None\",\"Other\"]},columns=['Count', 'Programming_Languages'])\nlanguage_18 = language_18.sort_values('Count',ascending=False)","d6b52c80":"language_19 = pd.DataFrame({'Count': [df19c.Q18_Part_1.value_counts()[0],df19c.Q18_Part_2.value_counts()[0],df19c.Q18_Part_3.value_counts()[0],\n                         df19c.Q18_Part_4.value_counts()[0],df19c.Q18_Part_5.value_counts()[0],df19c.Q18_Part_6.value_counts()[0],\n                         df19c.Q18_Part_7.value_counts()[0],df19c.Q18_Part_8.value_counts()[0],df19c.Q18_Part_9.value_counts()[0],\n                         df19c.Q18_Part_10.value_counts()[0],df19c.Q18_Part_11.value_counts()[0],df19c.Q18_Part_12.value_counts()[0]], 'Programming_Languages': [\"Python\", \"R\", \"SQL\",\"C\",\"C++\",\"Java\",\"Javascript\",\n                                                                                  \"TypeScript\",\"Bash\",\"MATLAB\",\"None\",\"Other\"]}, \n                  columns=['Count', 'Programming_Languages'])\nlanguage_19 = language_19.sort_values('Count',ascending=False)","6b0a3b5d":"language_20 = pd.DataFrame({'Count': [df20c.Q7_Part_1.value_counts()[0],df20c.Q7_Part_2.value_counts()[0],df20c.Q7_Part_3.value_counts()[0],\n                         df20c.Q7_Part_4.value_counts()[0],df20c.Q7_Part_5.value_counts()[0],df20c.Q7_Part_6.value_counts()[0],\n                         df20c.Q7_Part_7.value_counts()[0],df20c.Q7_Part_8.value_counts()[0],df20c.Q7_Part_9.value_counts()[0],\n                         df20c.Q7_Part_10.value_counts()[0],df20c.Q7_Part_11.value_counts()[0],df20c.Q7_Part_12.value_counts()[0],\n                         df20c.Q7_OTHER.value_counts()[0]], 'Programming_Languages': [\"Python\", \"R\", \"SQL\",\"C\",\"C++\",\"Java\",\"Javascript\",\n                                                                                  \"Julia\",\"Swift\",\"Bash\",\"MATLAB\",\"None\",\"Other\"]}, \n                  columns=['Count', 'Programming_Languages'])\nlanguage_20 = language_20.sort_values('Count',ascending=False)","b1e5be95":"language_18['year'] = 2018\nlanguage_19['year'] = 2019\nlanguage_20['year'] = 2020\nlanguage_c = pd.concat([language_18, language_19,language_20])","60948875":"fig = go.Figure(data=[\n    go.Bar(name='2018', x=language_18['Programming_Languages'], y=language_18['Count'], marker_color = color18),\n    go.Bar(name='2019', x=language_19['Programming_Languages'], y=language_19['Count'], marker_color = color19),\n    go.Bar(name='2020', x=language_20['Programming_Languages'], y=language_20['Count'], marker_color = color20)    \n])\n# Change the bar mode\nfig.update_layout(\n    title='Most used programming languages in Colombia',\n    xaxis = dict(\n        title='Programming Language',\n        tickfont_size=14,\n    ),\n    yaxis=dict(\n        title='Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),\n    legend=dict(\n        x=0.90,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15, # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1 # gap between bars of the same location coordinate.\n)\nfig.show()","fcfd2f53":"lang_20 = pd.DataFrame({'Count': [df20.Q7_Part_1.value_counts()[0],df20.Q7_Part_2.value_counts()[0],df20.Q7_Part_3.value_counts()[0],\n                         df20.Q7_Part_4.value_counts()[0],df20.Q7_Part_5.value_counts()[0],df20.Q7_Part_6.value_counts()[0],\n                         df20.Q7_Part_7.value_counts()[0],df20.Q7_Part_8.value_counts()[0],df20.Q7_Part_9.value_counts()[0],\n                         df20.Q7_Part_10.value_counts()[0],df20.Q7_Part_11.value_counts()[0],df20.Q7_Part_12.value_counts()[0],\n                         df20.Q7_OTHER.value_counts()[0]], 'Programming_Languages': [\"Python\", \"R\", \"SQL\",\"C\",\"C++\",\"Java\",\"Javascript\",\n                                                                                  \"Julia\",\"Swift\",\"Bash\",\"MATLAB\",\"None\",\"Other\"]}, \n                  columns=['Count', 'Programming_Languages'])\nlang_20 = lang_20.sort_values('Count',ascending=False)\n\nlang_19 = pd.DataFrame({'Count': [df19.Q18_Part_1.value_counts()[0],df19.Q18_Part_2.value_counts()[0],df19.Q18_Part_3.value_counts()[0],\n                         df19.Q18_Part_4.value_counts()[0],df19.Q18_Part_5.value_counts()[0],df19.Q18_Part_6.value_counts()[0],\n                         df19.Q18_Part_7.value_counts()[0],df19.Q18_Part_8.value_counts()[0],df19.Q18_Part_9.value_counts()[0],\n                         df19.Q18_Part_10.value_counts()[0],df19.Q18_Part_11.value_counts()[0],df19.Q18_Part_12.value_counts()[0]], 'Programming_Languages': [\"Python\", \"R\", \"SQL\",\"C\",\"C++\",\"Java\",\"Javascript\",\n                                                                                  \"TypeScript\",\"Bash\",\"MATLAB\",\"None\",\"Other\"]}, \n                  columns=['Count', 'Programming_Languages'])\nlang_19= lang_19.sort_values('Count',ascending=False)\n\n\nlang_18 = pd.DataFrame({'Count': [df18.Q16_Part_1.value_counts()[0],df18.Q16_Part_2.value_counts()[0],df18.Q16_Part_3.value_counts()[0],\n                                      df18.Q16_Part_4.value_counts()[0],df18.Q16_Part_5.value_counts()[0],df18.Q16_Part_6.value_counts()[0],\n                                      df18.Q16_Part_7.value_counts()[0],df18.Q16_Part_8.value_counts()[0],df18.Q16_Part_9.value_counts()[0],\n                                      df18.Q16_Part_10.value_counts()[0],df18.Q16_Part_12.value_counts()[0],\n                                      df18.Q16_Part_13.value_counts()[0],df18.Q16_Part_14.value_counts()[0],df18.Q16_Part_15.value_counts()[0],\n                                      df18.Q16_Part_16.value_counts()[0],df18.Q16_Part_17.value_counts()[0],df18.Q16_Part_18.value_counts()[0]], \n                                      'Programming_Languages': [\"Python\", \"R\", \"SQL\",\"Bash\",\"Java\" ,\"Javascript\",\"VBA\",\"C\",\"MATLAB\",\"Scala\",\n                                       \"Go\",\"C#\",\"PHP\",\"Ruby\",\"SAS\",\"None\",\"Other\"]},columns=['Count', 'Programming_Languages'])\n\nlang_18 = lang_18.sort_values('Count',ascending=False)\n","6b5d46d3":"fig = go.Figure(data=[\n    go.Bar(name='2018', x=lang_18['Programming_Languages'], y=lang_18['Count'], marker_color = color18),\n    go.Bar(name='2019', x=lang_19['Programming_Languages'], y=lang_19['Count'], marker_color = color19),\n    go.Bar(name='2020', x=lang_20['Programming_Languages'], y=lang_20['Count'], marker_color = color20)\n    \n    \n])\n# Change the bar mode\nfig.update_layout(\n    title='Most used programming languages worldwide',\n    xaxis = dict(\n        title='Programming Language',\n        tickfont_size=14,\n    ),\n    yaxis=dict(\n        title='Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),\n    legend=dict(\n        x=0.90,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15, # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1 # gap between bars of the same location coordinate.\n)\nfig.show()","6fcd918a":"skc = pd.DataFrame(df20c.groupby('Q16_Part_1').get_group('  Scikit-learn ').Q5.value_counts())\nskc = skc.rename_axis('Job title').reset_index()\nskc.rename(columns = {'Q5':'Scikit-learn'}, inplace = True) \ntfc = pd.DataFrame(df20c.groupby('Q16_Part_2').get_group('  TensorFlow ').Q5.value_counts())\ntfc = tfc.rename_axis('Job title').reset_index()\ntfc.rename(columns = {'Q5':'TensorFlow'}, inplace = True) \nskc['TensorFlow'] = tfc['TensorFlow']\nkrc = pd.DataFrame(df20c.groupby('Q16_Part_3').get_group(' Keras ').Q5.value_counts())\nkrc = krc.rename_axis('Job title').reset_index()\nkrc.rename(columns = {'Q5':'Keras'}, inplace = True)\nskc[\"Keras\"] = krc['Keras']\npyc = df20c.groupby('Q16_Part_4').get_group(' PyTorch ').Q5.value_counts()\npyc = pyc.rename_axis('Job title').reset_index()\npyc.rename(columns = {'Q5':'PyTorch'}, inplace = True)\nskc[\"PyTorch\"] = pyc['PyTorch']\nfac = df20c.groupby('Q16_Part_5').get_group(' Fast.ai ').Q5.value_counts()\nfac = fac.rename_axis('Job title').reset_index()\nfac.rename(columns = {'Q5':'Fast.ai'}, inplace = True)\nskc[\"Fast.ai\"] = fac['Fast.ai']\nxgc = df20c.groupby('Q16_Part_7').get_group(' Xgboost ').Q5.value_counts()\nxgc = xgc.rename_axis('Job title').reset_index()\nxgc.rename(columns = {'Q5':'Xgboost'}, inplace = True)\nskc[\"Xgboost\"] = xgc['Xgboost']\nlic = df20c.groupby('Q16_Part_8').get_group(' LightGBM ').Q5.value_counts()\nlic = lic.rename_axis('Job title').reset_index()\nlic.rename(columns = {'Q5':'LightGBM '}, inplace = True)\nskc[\"LightGBM \"] = lic['LightGBM ']\ncac = df20c.groupby('Q16_Part_9').get_group(' CatBoost ').Q5.value_counts()\ncac = cac.rename_axis('Job title').reset_index()\ncac.rename(columns = {'Q5':'CatBoost '}, inplace = True)\nskc[\"CatBoost \"] = cac['CatBoost ']\nprc = df20c.groupby('Q16_Part_10').get_group(' Prophet ').Q5.value_counts()\nprc= prc.rename_axis('Job title').reset_index()\nprc.rename(columns = {'Q5':' Prophet'}, inplace = True)\nskc[\"Prophet \"] = prc[' Prophet']\nh2c = df20c.groupby('Q16_Part_11').get_group(' H2O 3 ').Q5.value_counts()\nh2c = h2c.rename_axis('Job title').reset_index()\nh2c.rename(columns = {'Q5':'H2O 3'}, inplace = True)\nskc[\"H2O 3\"] = h2c['H2O 3']\ncarc = df20c.groupby('Q16_Part_12').get_group(' Caret ').Q5.value_counts()\ncarc = carc.rename_axis('Job title').reset_index()\ncarc.rename(columns = {'Q5':'Caret'}, inplace = True)\nskc[\"Caret\"] = carc['Caret']\nskc.rename(index={0:'Student',1:'Data Scientist',2:'Software Engineer',3:'Currently not employed',4 :'Other', \n                5:'Machine Learning Engineer',6:'Data Analyst',7:'Research Scientist',8:'Business Analyst',9:'Product\/Project Manager',\n                10:'Data Engineer',11:'Statistician',12:'DBA\/Database Engineer'}, inplace=True)\nskc= skc.drop(['Job title'],axis=1)\nskc = np.log(skc+1)","d63fe381":"skec = pd.DataFrame(df20c.groupby('Q16_Part_1').get_group('  Scikit-learn ').Q4.value_counts())\nskec = skec.rename_axis('Job title').reset_index()\nskec.rename(columns = {'Q4':'Scikit-learn'}, inplace = True) \ntfec = pd.DataFrame(df20.groupby('Q16_Part_2').get_group('  TensorFlow ').Q4.value_counts())\ntfec = tfec.rename_axis('Job title').reset_index()\ntfec.rename(columns = {'Q4':'TensorFlow'}, inplace = True) \nskec['TensorFlow'] = tfec['TensorFlow']\nkrec = pd.DataFrame(df20c.groupby('Q16_Part_3').get_group(' Keras ').Q4.value_counts())\nkrec = krec.rename_axis('Job title').reset_index()\nkrec.rename(columns = {'Q4':'Keras'}, inplace = True)\nskec[\"Keras\"] = krec['Keras']\npyec = df20c.groupby('Q16_Part_4').get_group(' PyTorch ').Q4.value_counts()\npyec = pyec.rename_axis('Job title').reset_index()\npyec.rename(columns = {'Q4':'PyTorch'}, inplace = True)\nskec[\"PyTorch\"] = pyec['PyTorch']\nfaec = df20c.groupby('Q16_Part_5').get_group(' Fast.ai ').Q4.value_counts()\nfaec = faec.rename_axis('Job title').reset_index()\nfaec.rename(columns = {'Q4':'Fast.ai'}, inplace = True)\nskec[\"Fast.ai\"] = faec['Fast.ai']\nxgec = df20c.groupby('Q16_Part_7').get_group(' Xgboost ').Q4.value_counts()\nxgec = xgec.rename_axis('Job title').reset_index()\nxgec.rename(columns = {'Q4':'Xgboost'}, inplace = True)\nskec[\"Xgboost\"] = xgec['Xgboost']\nliec = df20c.groupby('Q16_Part_8').get_group(' LightGBM ').Q4.value_counts()\nliec = liec.rename_axis('Job title').reset_index()\nliec.rename(columns = {'Q4':'LightGBM '}, inplace = True)\nskec[\"LightGBM \"] = liec['LightGBM ']\ncaec = df20c.groupby('Q16_Part_9').get_group(' CatBoost ').Q4.value_counts()\ncaec = caec.rename_axis('Job title').reset_index()\ncaec.rename(columns = {'Q4':'CatBoost '}, inplace = True)\nskec[\"CatBoost \"] = caec['CatBoost ']\nprec = df20c.groupby('Q16_Part_10').get_group(' Prophet ').Q4.value_counts()\nprec = prec.rename_axis('Job title').reset_index()\nprec.rename(columns = {'Q4':' Prophet'}, inplace = True)\nskec[\"Prophet \"] = prec[' Prophet']\nh2ec = df20c.groupby('Q16_Part_11').get_group(' H2O 3 ').Q4.value_counts()\nh2ec = h2ec.rename_axis('Job title').reset_index()\nh2ec.rename(columns = {'Q4':'H2O 3'}, inplace = True)\nskec[\"H2O 3\"] = h2ec['H2O 3']\ncarec = df20c.groupby('Q16_Part_12').get_group(' Caret ').Q4.value_counts()\ncarec = carec.rename_axis('Job title').reset_index()\ncarec.rename(columns = {'Q4':'Caret'}, inplace = True)\nskec[\"Caret\"] = carec['Caret']\nskec.rename(index={0:'Master\u2019s degree',1:'Bachelor\u2019s degree',2:'Doctoral degree',3:'Without bachelor degree',4 :'Professional degree', \n                5:'I prefer not to answer',6:'No formal education past high school'}, inplace=True)\ns2c= skec.drop(['Job title'],axis=1)\ns2c = np.log(s2c+1)","5140795c":"sns.set(font_scale=1.1)\nplt.figure(figsize=(20,5))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(skc.T, cmap=style2, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('Framework',fontsize=18)\nax1.set_xlabel('Job title',fontsize=18)\n# title\nax1.set_title('CONTINGENCY TABLE \\nFRAMEWORK VS. JOB TITLE \\n\\nColombia 2020',fontsize=20)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(s2c.T, cmap=style2, \n           linewidth=0.3, cbar_kws={'label': 'Logarithm'});\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Framework',fontsize=0)\nax2.set_xlabel('Education level',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nFRAMEWORK VS. EDUCATION LEVEL \\n\\nColombia 2020',fontsize=20);","51081bac":"# Job title vs. Framework DataFrame building\nsk = pd.DataFrame(df20.groupby('Q16_Part_1').get_group('  Scikit-learn ').Q5.value_counts())\nsk = sk.rename_axis('Job title').reset_index()\nsk.rename(columns = {'Q5':'Scikit-learn'}, inplace = True) \ntf = pd.DataFrame(df20.groupby('Q16_Part_2').get_group('  TensorFlow ').Q5.value_counts())\ntf = tf.rename_axis('Job title').reset_index()\ntf.rename(columns = {'Q5':'TensorFlow'}, inplace = True) \nsk['TensorFlow'] = tf['TensorFlow']\nkr = pd.DataFrame(df20.groupby('Q16_Part_3').get_group(' Keras ').Q5.value_counts())\nkr = kr.rename_axis('Job title').reset_index()\nkr.rename(columns = {'Q5':'Keras'}, inplace = True)\nsk[\"Keras\"] = kr['Keras']\npy = df20.groupby('Q16_Part_4').get_group(' PyTorch ').Q5.value_counts()\npy = py.rename_axis('Job title').reset_index()\npy.rename(columns = {'Q5':'PyTorch'}, inplace = True)\nsk[\"PyTorch\"] = py['PyTorch']\nfa = df20.groupby('Q16_Part_5').get_group(' Fast.ai ').Q5.value_counts()\nfa = fa.rename_axis('Job title').reset_index()\nfa.rename(columns = {'Q5':'Fast.ai'}, inplace = True)\nsk[\"Fast.ai\"] = fa['Fast.ai']\nmx = df20.groupby('Q16_Part_6').get_group(' MXNet ').Q5.value_counts()\nmx = mx.rename_axis('Job title').reset_index()\nmx.rename(columns = {'Q5':'MXNet'}, inplace = True)\nsk[\"MXNet\"] = mx['MXNet']\nxg = df20.groupby('Q16_Part_7').get_group(' Xgboost ').Q5.value_counts()\nxg = xg.rename_axis('Job title').reset_index()\nxg.rename(columns = {'Q5':'Xgboost'}, inplace = True)\nsk[\"Xgboost\"] = xg['Xgboost']\nli = df20.groupby('Q16_Part_8').get_group(' LightGBM ').Q5.value_counts()\nli = li.rename_axis('Job title').reset_index()\nli.rename(columns = {'Q5':'LightGBM '}, inplace = True)\nsk[\"LightGBM \"] = li['LightGBM ']\nca = df20.groupby('Q16_Part_9').get_group(' CatBoost ').Q5.value_counts()\nca = ca.rename_axis('Job title').reset_index()\nca.rename(columns = {'Q5':'CatBoost '}, inplace = True)\nsk[\"CatBoost \"] = ca['CatBoost ']\npr = df20.groupby('Q16_Part_10').get_group(' Prophet ').Q5.value_counts()\npr = pr.rename_axis('Job title').reset_index()\npr.rename(columns = {'Q5':' Prophet'}, inplace = True)\nsk[\"Prophet \"] = pr[' Prophet']\nh2 = df20.groupby('Q16_Part_11').get_group(' H2O 3 ').Q5.value_counts()\nh2 = h2.rename_axis('Job title').reset_index()\nh2.rename(columns = {'Q5':'H2O 3'}, inplace = True)\nsk[\"H2O 3\"] = h2['H2O 3']\ncar = df20.groupby('Q16_Part_12').get_group(' Caret ').Q5.value_counts()\ncar = car.rename_axis('Job title').reset_index()\ncar.rename(columns = {'Q5':'Caret'}, inplace = True)\nsk[\"Caret\"] = car['Caret']\nsk.rename(index={0:'Student',1:'Data Scientist',2:'Software Engineer',3:'Currently not employed',4 :'Other', \n                5:'Machine Learning Engineer',6:'Data Analyst',7:'Research Scientist',8:'Business Analyst',9:'Product\/Project Manager',\n                10:'Data Engineer',11:'Statistician',12:'DBA\/Database Engineer'}, inplace=True)\nsk= sk.drop(['Job title'],axis=1)\nsk = np.log(sk+1)","6adc68b5":"ske = pd.DataFrame(df20.groupby('Q16_Part_1').get_group('  Scikit-learn ').Q4.value_counts())\nske = ske.rename_axis('Job title').reset_index()\nske.rename(columns = {'Q4':'Scikit-learn'}, inplace = True) \ntfe = pd.DataFrame(df20.groupby('Q16_Part_2').get_group('  TensorFlow ').Q4.value_counts())\ntfe = tfe.rename_axis('Job title').reset_index()\ntfe.rename(columns = {'Q4':'TensorFlow'}, inplace = True) \nske['TensorFlow'] = tfe['TensorFlow']\nkre = pd.DataFrame(df20.groupby('Q16_Part_3').get_group(' Keras ').Q4.value_counts())\nkre = kre.rename_axis('Job title').reset_index()\nkre.rename(columns = {'Q4':'Keras'}, inplace = True)\nske[\"Keras\"] = kre['Keras']\npye = df20.groupby('Q16_Part_4').get_group(' PyTorch ').Q4.value_counts()\npye = pye.rename_axis('Job title').reset_index()\npye.rename(columns = {'Q4':'PyTorch'}, inplace = True)\nske[\"PyTorch\"] = pye['PyTorch']\nfae = df20.groupby('Q16_Part_5').get_group(' Fast.ai ').Q4.value_counts()\nfae = fae.rename_axis('Job title').reset_index()\nfae.rename(columns = {'Q4':'Fast.ai'}, inplace = True)\nske[\"Fast.ai\"] = fae['Fast.ai']\nmxe = df20.groupby('Q16_Part_6').get_group(' MXNet ').Q4.value_counts()\nmxe = mxe.rename_axis('Job title').reset_index()\nmxe.rename(columns = {'Q4':'MXNet'}, inplace = True)\nske[\"MXNet\"] = mxe['MXNet']\nxge = df20.groupby('Q16_Part_7').get_group(' Xgboost ').Q4.value_counts()\nxge = xge.rename_axis('Job title').reset_index()\nxge.rename(columns = {'Q4':'Xgboost'}, inplace = True)\nske[\"Xgboost\"] = xge['Xgboost']\nlie = df20.groupby('Q16_Part_8').get_group(' LightGBM ').Q4.value_counts()\nlie = lie.rename_axis('Job title').reset_index()\nlie.rename(columns = {'Q4':'LightGBM '}, inplace = True)\nske[\"LightGBM \"] = lie['LightGBM ']\ncae = df20.groupby('Q16_Part_9').get_group(' CatBoost ').Q4.value_counts()\ncae = cae.rename_axis('Job title').reset_index()\ncae.rename(columns = {'Q4':'CatBoost '}, inplace = True)\nske[\"CatBoost \"] = cae['CatBoost ']\npre = df20.groupby('Q16_Part_10').get_group(' Prophet ').Q4.value_counts()\npre = pre.rename_axis('Job title').reset_index()\npre.rename(columns = {'Q4':' Prophet'}, inplace = True)\nske[\"Prophet \"] = pre[' Prophet']\nh2e = df20.groupby('Q16_Part_11').get_group(' H2O 3 ').Q4.value_counts()\nh2e = h2e.rename_axis('Job title').reset_index()\nh2e.rename(columns = {'Q4':'H2O 3'}, inplace = True)\nske[\"H2O 3\"] = h2e['H2O 3']\ncare = df20.groupby('Q16_Part_12').get_group(' Caret ').Q4.value_counts()\ncare = care.rename_axis('Job title').reset_index()\ncare.rename(columns = {'Q4':'Caret'}, inplace = True)\nske[\"Caret\"] = care['Caret']\nske.rename(index={0:'Master\u2019s degree',1:'Bachelor\u2019s degree',2:'Doctoral degree',3:'Without bachelor degree',4 :'Professional degree', \n                5:'I prefer not to answer',6:'No formal education past high school'}, inplace=True)\ns2= ske.drop(['Job title'],axis=1)\ns2 = np.log(s2+1)","16ff6753":"sns.set(font_scale=1.1)\nplt.figure(figsize=(20,5))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(sk.T, cmap=style2, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('Framework',fontsize=18)\nax1.set_xlabel('Job title',fontsize=18)\n# title\nax1.set_title('CONTINGENCY TABLE \\nFRAMEWORK VS. JOB TITLE \\n\\nWorldwide 2020',fontsize=20)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(s2.T, cmap=style2, \n           linewidth=0.3, cbar_kws={'label': 'Logarithm'});\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Framework',fontsize=0)\nax2.set_xlabel('Education level',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nFRAMEWORK VS. EDUCATION LEVEL \\n\\nWorldwide 2020',fontsize=20)","a766d941":"lrc = pd.DataFrame(df20c.groupby('Q17_Part_1').get_group('Linear or Logistic Regression').Q5.value_counts())\nlrc = lrc.rename_axis('Job title').reset_index()\nlrc.rename(columns = {'Q5':'Linear or Logistic Regression'}, inplace = True) \ndtc = pd.DataFrame(df20c.groupby('Q17_Part_2').get_group('Decision Trees or Random Forests').Q5.value_counts())\ndtc = dtc.rename_axis('Job title').reset_index()\ndtc.rename(columns = {'Q5':'Decision Trees or Random Forests'}, inplace = True) \nlrc['Decision Trees or Random Forests'] = dtc['Decision Trees or Random Forests']\ngbc = pd.DataFrame(df20c.groupby('Q17_Part_3').get_group('Gradient Boosting Machines (xgboost, lightgbm, etc)').Q5.value_counts())\ngbc = gbc.rename_axis('Job title').reset_index()\ngbc.rename(columns = {'Q5':'Gradient Boosting Machines (xgboost, lightgbm, etc)'}, inplace = True) \nlrc['Gradient Boosting Machines'] = gbc['Gradient Boosting Machines (xgboost, lightgbm, etc)']\nbac = pd.DataFrame(df20c.groupby('Q17_Part_4').get_group('Bayesian Approaches').Q5.value_counts())\nbac = bac.rename_axis('Job title').reset_index()\nbac.rename(columns = {'Q5':'Bayesian Approaches'}, inplace = True) \nlrc['Bayesian Approaches'] = bac['Bayesian Approaches']\nevc = pd.DataFrame(df20c.groupby('Q17_Part_5').get_group('Evolutionary Approaches').Q5.value_counts())\nevc = evc.rename_axis('Job title').reset_index()\nevc.rename(columns = {'Q5':'Evolutionary Approaches'}, inplace = True) \nlrc['Evolutionary Approaches'] = evc['Evolutionary Approaches']\ndnc = pd.DataFrame(df20c.groupby('Q17_Part_6').get_group('Dense Neural Networks (MLPs, etc)').Q5.value_counts())\ndnc = dnc.rename_axis('Job title').reset_index()\ndnc.rename(columns = {'Q5':'Dense Neural Networks (MLPs, etc)'}, inplace = True) \nlrc['Dense Neural Networks'] = dnc['Dense Neural Networks (MLPs, etc)']\ncnc = pd.DataFrame(df20c.groupby('Q17_Part_7').get_group('Convolutional Neural Networks').Q5.value_counts())\ncnc = cnc.rename_axis('Job title').reset_index()\ncnc.rename(columns = {'Q5':'Convolutional Neural Networks'}, inplace = True) \nlrc['Convolutional Neural Networks'] = cnc['Convolutional Neural Networks']\ngnc = pd.DataFrame(df20c.groupby('Q17_Part_8').get_group('Generative Adversarial Networks').Q5.value_counts())\ngnc = gnc.rename_axis('Job title').reset_index()\ngnc.rename(columns = {'Q5':'Generative Adversarial Networks'}, inplace = True) \nlrc['Generative Adversarial Networks'] = gnc['Generative Adversarial Networks']\nrnc = pd.DataFrame(df20c.groupby('Q17_Part_9').get_group('Recurrent Neural Networks').Q5.value_counts())\nrnc = rnc.rename_axis('Job title').reset_index()\nrnc.rename(columns = {'Q5':'Recurrent Neural Networks'}, inplace = True) \nlrc['Recurrent Neural Networks'] = rnc['Recurrent Neural Networks']\ntnc = pd.DataFrame(df20c.groupby('Q17_Part_10').get_group('Transformer Networks (BERT, gpt-3, etc)').Q5.value_counts())\ntnc = tnc.rename_axis('Job title').reset_index()\ntnc.rename(columns = {'Q5':'Transformer Networks (BERT, gpt-3, etc)'}, inplace = True) \nlrc['Transformer Networks'] = tnc['Transformer Networks (BERT, gpt-3, etc)']\nlrc.rename(index={0:'Student',1:'Data Scientist',2:'Software Engineer',3:'Currently not employed',4 :'Other', \n                5:'Machine Learning Engineer',6:'Data Analyst',7:'Research Scientist',8:'Business Analyst',9:'Product\/Project Manager',\n                10:'Data Engineer',11:'Statistician',12:'DBA\/Database Engineer'}, inplace=True)\nlrc= lrc.drop(['Job title'],axis=1)\nlrc = np.log(lrc+1)","11490b4c":"lrec = pd.DataFrame(df20c.groupby('Q17_Part_1').get_group('Linear or Logistic Regression').Q4.value_counts())\nlrec = lrec.rename_axis('Job title').reset_index()\nlrec.rename(columns = {'Q4':'Linear or Logistic Regression'}, inplace = True) \ndtec = pd.DataFrame(df20c.groupby('Q17_Part_2').get_group('Decision Trees or Random Forests').Q4.value_counts())\ndtec = dtec.rename_axis('Job title').reset_index()\ndtec.rename(columns = {'Q4':'Decision Trees or Random Forests'}, inplace = True) \nlrec['Decision Trees or Random Forests'] = dtec['Decision Trees or Random Forests']\ngbec = pd.DataFrame(df20c.groupby('Q17_Part_3').get_group('Gradient Boosting Machines (xgboost, lightgbm, etc)').Q4.value_counts())\ngbec = gbec.rename_axis('Job title').reset_index()\ngbec.rename(columns = {'Q4':'Gradient Boosting Machines (xgboost, lightgbm, etc)'}, inplace = True) \nlrec['Gradient Boosting Machines'] = gbec['Gradient Boosting Machines (xgboost, lightgbm, etc)']\nbaec = pd.DataFrame(df20c.groupby('Q17_Part_4').get_group('Bayesian Approaches').Q4.value_counts())\nbaec = baec.rename_axis('Job title').reset_index()\nbaec.rename(columns = {'Q4':'Bayesian Approaches'}, inplace = True) \nlrec['Bayesian Approaches'] = baec['Bayesian Approaches']\nevec = pd.DataFrame(df20c.groupby('Q17_Part_5').get_group('Evolutionary Approaches').Q4.value_counts())\nevec = evec.rename_axis('Job title').reset_index()\nevec.rename(columns = {'Q4':'Evolutionary Approaches'}, inplace = True) \nlrec['Evolutionary Approaches'] = evec['Evolutionary Approaches']\ndnec = pd.DataFrame(df20c.groupby('Q17_Part_6').get_group('Dense Neural Networks (MLPs, etc)').Q4.value_counts())\ndnec = dnec.rename_axis('Job title').reset_index()\ndnec.rename(columns = {'Q4':'Dense Neural Networks (MLPs, etc)'}, inplace = True) \nlrec['Dense Neural Networks'] = dnec['Dense Neural Networks (MLPs, etc)']\ncnec = pd.DataFrame(df20c.groupby('Q17_Part_7').get_group('Convolutional Neural Networks').Q4.value_counts())\ncnec = cnec.rename_axis('Job title').reset_index()\ncnec.rename(columns = {'Q4':'Convolutional Neural Networks'}, inplace = True) \nlrec['Convolutional Neural Networks'] = cnec['Convolutional Neural Networks']\ngnec = pd.DataFrame(df20c.groupby('Q17_Part_8').get_group('Generative Adversarial Networks').Q4.value_counts())\ngnec = gnec.rename_axis('Job title').reset_index()\ngnec.rename(columns = {'Q4':'Generative Adversarial Networks'}, inplace = True) \nlrec['Generative Adversarial Networks'] = gnec['Generative Adversarial Networks']\nrnec = pd.DataFrame(df20c.groupby('Q17_Part_9').get_group('Recurrent Neural Networks').Q4.value_counts())\nrnec = rnec.rename_axis('Job title').reset_index()\nrnec.rename(columns = {'Q4':'Recurrent Neural Networks'}, inplace = True) \nlrec['Recurrent Neural Networks'] = rnec['Recurrent Neural Networks']\ntnec = pd.DataFrame(df20c.groupby('Q17_Part_10').get_group('Transformer Networks (BERT, gpt-3, etc)').Q4.value_counts())\ntnec = tnec.rename_axis('Job title').reset_index()\ntnec.rename(columns = {'Q4':'Transformer Networks (BERT, gpt-3, etc)'}, inplace = True) \nlrec['Transformer Networks'] = tnec['Transformer Networks (BERT, gpt-3, etc)']\nlrec.rename(index={0:'Master\u2019s degree',1:'Bachelor\u2019s degree',2:'Doctoral degree',3:'Without bachelor degree',4 :'Professional degree', \n                5:'I prefer not to answer',6:'No formal education past high school'}, inplace=True)\nlr2c = lrec.drop(['Job title'],axis=1)\nlr2c = np.log(lr2c+1)","b6a02ee7":"sns.set(font_scale=1.1)\nplt.figure(figsize=(20,5))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(lrc.T, cmap=style2, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('ML algorithms',fontsize=18);\nax1.set_xlabel('Job title',fontsize=18);\n# title\nax1.set_title('CONTINGENCY TABLE \\nML algorithms VS. JOB TITLE \\n\\nColombia 2020',fontsize=20);\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(lr2c.T, cmap=style2, \n           linewidth=0.3, cbar_kws={'label': 'Logarithm'});\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('ML algorithms',fontsize=0)\nax2.set_xlabel('Education level',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nML algorithms VS. EDUCATION LEVEL \\n\\nColombia 2020',fontsize=20);","f46514de":"lr = pd.DataFrame(df20.groupby('Q17_Part_1').get_group('Linear or Logistic Regression').Q5.value_counts())\nlr = lr.rename_axis('Job title').reset_index()\nlr.rename(columns = {'Q5':'Linear or Logistic Regression'}, inplace = True) \ndt = pd.DataFrame(df20.groupby('Q17_Part_2').get_group('Decision Trees or Random Forests').Q5.value_counts())\ndt = dt.rename_axis('Job title').reset_index()\ndt.rename(columns = {'Q5':'Decision Trees or Random Forests'}, inplace = True) \nlr['Decision Trees or Random Forests'] = dt['Decision Trees or Random Forests']\ngb = pd.DataFrame(df20.groupby('Q17_Part_3').get_group('Gradient Boosting Machines (xgboost, lightgbm, etc)').Q5.value_counts())\ngb = gb.rename_axis('Job title').reset_index()\ngb.rename(columns = {'Q5':'Gradient Boosting Machines (xgboost, lightgbm, etc)'}, inplace = True) \nlr['Gradient Boosting Machines'] = gb['Gradient Boosting Machines (xgboost, lightgbm, etc)']\nba = pd.DataFrame(df20.groupby('Q17_Part_4').get_group('Bayesian Approaches').Q5.value_counts())\nba = ba.rename_axis('Job title').reset_index()\nba.rename(columns = {'Q5':'Bayesian Approaches'}, inplace = True) \nlr['Bayesian Approaches'] = ba['Bayesian Approaches']\nev = pd.DataFrame(df20.groupby('Q17_Part_5').get_group('Evolutionary Approaches').Q5.value_counts())\nev = ev.rename_axis('Job title').reset_index()\nev.rename(columns = {'Q5':'Evolutionary Approaches'}, inplace = True) \nlr['Evolutionary Approaches'] = ev['Evolutionary Approaches']\ndn = pd.DataFrame(df20.groupby('Q17_Part_6').get_group('Dense Neural Networks (MLPs, etc)').Q5.value_counts())\ndn = dn.rename_axis('Job title').reset_index()\ndn.rename(columns = {'Q5':'Dense Neural Networks (MLPs, etc)'}, inplace = True) \nlr['Dense Neural Networks'] = dn['Dense Neural Networks (MLPs, etc)']\ncn = pd.DataFrame(df20.groupby('Q17_Part_7').get_group('Convolutional Neural Networks').Q5.value_counts())\ncn = cn.rename_axis('Job title').reset_index()\ncn.rename(columns = {'Q5':'Convolutional Neural Networks'}, inplace = True) \nlr['Convolutional Neural Networks'] = cn['Convolutional Neural Networks']\ngn = pd.DataFrame(df20.groupby('Q17_Part_8').get_group('Generative Adversarial Networks').Q5.value_counts())\ngn = gn.rename_axis('Job title').reset_index()\ngn.rename(columns = {'Q5':'Generative Adversarial Networks'}, inplace = True) \nlr['Generative Adversarial Networks'] = gn['Generative Adversarial Networks']\nrn = pd.DataFrame(df20.groupby('Q17_Part_9').get_group('Recurrent Neural Networks').Q5.value_counts())\nrn = rn.rename_axis('Job title').reset_index()\nrn.rename(columns = {'Q5':'Recurrent Neural Networks'}, inplace = True) \nlr['Recurrent Neural Networks'] = rn['Recurrent Neural Networks']\ntn = pd.DataFrame(df20.groupby('Q17_Part_10').get_group('Transformer Networks (BERT, gpt-3, etc)').Q5.value_counts())\ntn = tn.rename_axis('Job title').reset_index()\ntn.rename(columns = {'Q5':'Transformer Networks (BERT, gpt-3, etc)'}, inplace = True) \nlr['Transformer Networks'] = tn['Transformer Networks (BERT, gpt-3, etc)']\nlr.rename(index={0:'Student',1:'Data Scientist',2:'Software Engineer',3:'Currently not employed',4 :'Other', \n                5:'Machine Learning Engineer',6:'Data Analyst',7:'Research Scientist',8:'Business Analyst',9:'Product\/Project Manager',\n                10:'Data Engineer',11:'Statistician',12:'DBA\/Database Engineer'}, inplace=True)\nlr= lr.drop(['Job title'],axis=1)\nlr = np.log(lr+1)","a82944b6":"lre = pd.DataFrame(df20.groupby('Q17_Part_1').get_group('Linear or Logistic Regression').Q4.value_counts())\nlre = lre.rename_axis('Job title').reset_index()\nlre.rename(columns = {'Q4':'Linear or Logistic Regression'}, inplace = True) \ndte = pd.DataFrame(df20.groupby('Q17_Part_2').get_group('Decision Trees or Random Forests').Q4.value_counts())\ndte = dte.rename_axis('Job title').reset_index()\ndte.rename(columns = {'Q4':'Decision Trees or Random Forests'}, inplace = True) \nlre['Decision Trees or Random Forests'] = dte['Decision Trees or Random Forests']\ngbe = pd.DataFrame(df20.groupby('Q17_Part_3').get_group('Gradient Boosting Machines (xgboost, lightgbm, etc)').Q4.value_counts())\ngbe = gbe.rename_axis('Job title').reset_index()\ngbe.rename(columns = {'Q4':'Gradient Boosting Machines (xgboost, lightgbm, etc)'}, inplace = True) \nlre['Gradient Boosting Machines'] = gbe['Gradient Boosting Machines (xgboost, lightgbm, etc)']\nbae = pd.DataFrame(df20.groupby('Q17_Part_4').get_group('Bayesian Approaches').Q4.value_counts())\nbae = bae.rename_axis('Job title').reset_index()\nbae.rename(columns = {'Q4':'Bayesian Approaches'}, inplace = True) \nlre['Bayesian Approaches'] = bae['Bayesian Approaches']\neve = pd.DataFrame(df20.groupby('Q17_Part_5').get_group('Evolutionary Approaches').Q4.value_counts())\neve = eve.rename_axis('Job title').reset_index()\neve.rename(columns = {'Q4':'Evolutionary Approaches'}, inplace = True) \nlre['Evolutionary Approaches'] = eve['Evolutionary Approaches']\ndne = pd.DataFrame(df20.groupby('Q17_Part_6').get_group('Dense Neural Networks (MLPs, etc)').Q4.value_counts())\ndne = dne.rename_axis('Job title').reset_index()\ndne.rename(columns = {'Q4':'Dense Neural Networks (MLPs, etc)'}, inplace = True) \nlre['Dense Neural Networks'] = dne['Dense Neural Networks (MLPs, etc)']\ncne = pd.DataFrame(df20.groupby('Q17_Part_7').get_group('Convolutional Neural Networks').Q4.value_counts())\ncne = cne.rename_axis('Job title').reset_index()\ncne.rename(columns = {'Q4':'Convolutional Neural Networks'}, inplace = True) \nlre['Convolutional Neural Networks'] = cne['Convolutional Neural Networks']\ngne = pd.DataFrame(df20.groupby('Q17_Part_8').get_group('Generative Adversarial Networks').Q4.value_counts())\ngne = gne.rename_axis('Job title').reset_index()\ngne.rename(columns = {'Q4':'Generative Adversarial Networks'}, inplace = True) \nlre['Generative Adversarial Networks'] = gne['Generative Adversarial Networks']\nrne = pd.DataFrame(df20.groupby('Q17_Part_9').get_group('Recurrent Neural Networks').Q4.value_counts())\nrne = rne.rename_axis('Job title').reset_index()\nrne.rename(columns = {'Q4':'Recurrent Neural Networks'}, inplace = True) \nlre['Recurrent Neural Networks'] = rne['Recurrent Neural Networks']\ntne = pd.DataFrame(df20.groupby('Q17_Part_10').get_group('Transformer Networks (BERT, gpt-3, etc)').Q4.value_counts())\ntne = tne.rename_axis('Job title').reset_index()\ntne.rename(columns = {'Q4':'Transformer Networks (BERT, gpt-3, etc)'}, inplace = True) \nlre['Transformer Networks'] = tne['Transformer Networks (BERT, gpt-3, etc)']\nlre.rename(index={0:'Master\u2019s degree',1:'Bachelor\u2019s degree',2:'Doctoral degree',3:'Without bachelor degree',4 :'Professional degree', \n                5:'I prefer not to answer',6:'No formal education past high school'}, inplace=True)\nlr2= lre.drop(['Job title'],axis=1)\nlr2 = np.log(lr2+1)","d0d8b8d0":"sns.set(font_scale=1.1)\nplt.figure(figsize=(20,5))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(lr.T, cmap=style2, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('ML algorithms',fontsize=18)\nax1.set_xlabel('Job title',fontsize=18)\n# title\nax1.set_title('CONTINGENCY TABLE \\nML algorithms VS. JOB TITLE \\n\\nWideworld 2020',fontsize=20)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(lr2.T, cmap=style2, \n           linewidth=0.3, cbar_kws={'label': 'Logarithm'});\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('ML algorithms',fontsize=0)\nax2.set_xlabel('Education level',fontsize=18);\n# title\nax2.set_title('CONTINGENCY TABLE \\nML algorithms VS. EDUCATION LEVEL \\n\\nWorldwide 2020',fontsize=20);","822c128f":"s20 = pd.crosstab(df20.Q24, df20.Q4)\ns20= s20.drop(['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'], axis=1)\ns20 = s20.drop(['What is your current yearly compensation (approximate $USD)?'],axis=0)\ns20.rename(columns = {'Some college\/university study without earning a bachelor\u2019s degree':'Without bachelor degree'}, inplace = True)\n\ns19 = pd.crosstab(df19.Q10, df19.Q4)\ns19= s19.drop(['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'], axis=1)\ns19 = s19.drop(['What is your current yearly compensation (approximate $USD)?'],axis=0)\ns19.rename(columns = {'Some college\/university study without earning a bachelor\u2019s degree':'Without bachelor degree'}, inplace = True)\n\ns18 = pd.crosstab(df18.Q9, df18.Q4)\ns18= s18.drop(['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'], axis=1)\ns18 = s18.drop(['What is your current yearly compensation (approximate $USD)?'],axis=0)\ns18 = s18.drop(['I do not wish to disclose my approximate yearly compensation'],axis=0)\ns18.rename(columns = {'Some college\/university study without earning a bachelor\u2019s degree':'Without bachelor degree'}, inplace = True)\n#log scale for grapic\ns20l = np.log(s20+1)\ns19l = np.log(s19+1)\ns18l = np.log(s18+1)","4c61675e":"sns.set(font_scale=1.1)\nplt.figure(figsize=(20,5))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(s20l.T, cmap=style2, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('Education level',fontsize=18)\nax1.set_xlabel('Job title',fontsize=0)\n# title\nax1.set_title('2020',fontsize=18)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(s19l.T, cmap=style2, \n           linewidth=0.3, cbar=False);\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Country',fontsize=0)\nax2.set_xlabel('Salary',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nEDUCATION LEVEL VS. SALARY \\n\\n2019',fontsize=18)\n\nax3 = plt.subplot(133, sharey=ax1)\nax3 = sns.heatmap(s18l.T, cmap=style2, \n           linewidth=0.3);\nplt.setp(ax3.get_yticklabels(), visible=False)\nax3.set_ylabel('Country',fontsize=0)\nax3.set_xlabel('Job title',fontsize=0)\nax3.set_title('2018',fontsize=18);","d260eb87":"def hyp_test(contingency, alpha = 0.05):\n  '''\n  Inputs:\n  contingency: contingency table to be resolved\n  alpha = significance level (0.05 by default)\n\n  Output:\n  This function only prints the result of a hypothesis test. Giving information\n  about rejection or acceptance of the null hypothesis:\n  Null hypothesis is rejected if variables in contingency table are dependent.\n  Null hypothesis is accepted if variables in contingency table are independent.\n  '''\n  c, p, dof, expected = chi2_contingency(contingency)\n\n  if p < alpha:\n    print(f'p-value: {p} is less than significance level: {alpha}')\n    print(f'Variables are dependent. Null hypothesis is rejected.')\n  else:\n    print(f'p-value: {p} is greater than significance level: {alpha}')\n    print(f'Variables are independent. Null hypothesis is accepted.')\n  \n  return None","9010e7d8":"sc20 = pd.crosstab(df20.Q24, df20.Q3)\n\nsc20 = sc20.drop(['What is your current yearly compensation (approximate $USD)?'],axis=0)\nsc20 = sc20.drop(['In which country do you currently reside?'],axis=1)\nsc20 = sc20.drop(['Ghana', 'United Arab Emirates', 'Sri Lanka', 'Nepal', 'Taiwan', 'Saudi Arabia'],axis=1)\n\nsc19 = pd.crosstab(df19.Q10, df19.Q3)\nsc19 = sc19.drop(['What is your current yearly compensation (approximate $USD)?'],axis=0)\n\nsc18 = pd.crosstab(df18.Q9, df18.Q3)\nsc18 = sc18.drop(['What is your current yearly compensation (approximate $USD)?'],axis=0)\nsc18 = sc18.drop(['I do not wish to disclose my approximate yearly compensation'],axis=0)\n\ncol_ref = sc20.columns\nsc19 = sc19[col_ref]\nsc18 = sc18[col_ref]\n\nsc20l = np.log(sc20+1)\nsc19l = np.log(sc19+1)\nsc18l = np.log(sc18+1)","2e650da0":"sns.set(font_scale=1.1)\nplt.figure(figsize=(20,15))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(sc20l.T, cmap=style1, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('Country',fontsize=0)\nax1.set_xlabel('Job title',fontsize=0)\n# title\nax1.set_title('2020',fontsize=18)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(sc19l.T, cmap=style1, \n           linewidth=0.3, cbar=False);\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Country',fontsize=0)\nax2.set_xlabel('Salary',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nCOUNTRY VS. SALARY \\n\\n2019',fontsize=18)\n\nax3 = plt.subplot(133, sharey=ax1)\nax3 = sns.heatmap(sc18l.T, cmap=style1, \n           linewidth=0.3);\nplt.setp(ax3.get_yticklabels(), visible=False)\nax3.set_ylabel('Country',fontsize=0)\nax3.set_xlabel('Job title',fontsize=0)\nax3.set_title('2018',fontsize=18);","b5d6a988":"hyp_test(sc18)","8c2354ca":"hyp_test(sc19)","95fb9b43":"hyp_test(sc20)","43053743":"sg20 = pd.crosstab(df20.Q2, df20.Q24)\nsg19 = pd.crosstab(df19.Q2, df19.Q10)\nsg18 = pd.crosstab(df18.Q1, df18.Q9)\n\nsg20 = sg20.drop(['What is your current yearly compensation (approximate $USD)?'],axis=1)\nsg20 = sg20.drop(['Nonbinary'],axis=0)\nsg20 = sg20.drop(['Prefer not to say'],axis=0)\nsg20 = sg20.drop(['Prefer to self-describe'],axis=0)\nsg20 = sg20.drop(['What is your gender? - Selected Choice'],axis=0)\nsg19 = sg19.drop(['What is your current yearly compensation (approximate $USD)?'],axis=1)\nsg19 = sg19.drop(['Prefer not to say'],axis=0)\nsg19 = sg19.drop(['Prefer to self-describe'],axis=0)\nsg19 = sg19.drop(['What is your gender? - Selected Choice'],axis=0)\nsg18 = sg18.drop(['What is your current yearly compensation (approximate $USD)?'],axis=1)\nsg18 = sg18.drop(['I do not wish to disclose my approximate yearly compensation'],axis=1)\nsg18 = sg18.drop(['Prefer not to say'],axis=0)\nsg18 = sg18.drop(['Prefer to self-describe'],axis=0)\nsg18 = sg18.drop(['What is your gender? - Selected Choice'],axis=0)\n\nsg20l = np.log(sg20+1)\nsg19l = np.log(sg19+1)\nsg18l = np.log(sg18+1)","41ec2591":"sns.set(font_scale=1.1)\nplt.figure(figsize=(6,10))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(sg20l.T, cmap=style2, \n           linewidth=0.3, cbar=False);\nax1.set_ylabel('Salary',fontsize=18)\nax1.set_xlabel('Job title',fontsize=0)\n# title\nax1.set_title('2020',fontsize=18)\n#plt.title(title, loc='left',fontsize=18)\n\nax2 = plt.subplot(132, sharey=ax1)\nax2 = sns.heatmap(sg19l.T, cmap=style2, \n           linewidth=0.3, cbar=False);\nplt.setp(ax2.get_yticklabels(), visible=False)\nax2.set_ylabel('Country',fontsize=0)\nax2.set_xlabel('Gender',fontsize=18)\n# title\nax2.set_title('CONTINGENCY TABLE \\nGENDER VS. SALARY \\n\\n2019',fontsize=18)\n\nax3 = plt.subplot(133, sharey=ax1)\nax3 = sns.heatmap(sg18l.T, cmap=style2, \n           linewidth=0.3);\nplt.setp(ax3.get_yticklabels(), visible=False)\nax3.set_ylabel('Country',fontsize=0)\nax3.set_xlabel('Job title',fontsize=0)\nax3.set_title('2018',fontsize=18);","7a8e3d3e":"hyp_test(sg18)","15918335":"hyp_test(sg19)","a186f07e":"hyp_test(sg20)","62289676":"def country_gender(country):\n  '''\n  Input:\n  country: Name of the country you want to study\n\n  Output:\n  nw: DataFrame grouped by the chosen country\n  '''\n  #list of countries\n  u=df20.Q3.unique()[1:]\n  i = np.where(u == country)\n  c = u[i[0][0]]\n  df_country = df20.groupby('Q3').get_group(c)\n  df_country.Q2.value_counts()\n\n  nw=pd.crosstab(df_country.Q24, df_country.Q2)\n\n  return nw","432c42a7":"country = 'India'\nci = country_gender(country)\nci = ci.drop(['Prefer not to say', 'Prefer to self-describe'],axis=1)\ncil = np.log(ci+1)\n\nsns.set(font_scale=1.1)\nplt.figure(figsize=(35,2))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(cil.T, cmap='Reds', \n           linewidth=0.3);\nax1.set_ylabel('Gender',fontsize=18)\nax1.set_xlabel('Salary',fontsize=18)\n# title\nax1.set_title('CONTINGENCY TABLE \\nSALARY VS. GENDER \\n\\n'+ country + ' 2020',fontsize=18);","d1b374fb":"hyp_test(ci)","d23761e6":"country = 'Colombia'\nci = country_gender(country)\n#ci = ci.drop(['Prefer not to say', 'Prefer to self-describe'],axis=1)\ncil = np.log(ci+1)\n\nsns.set(font_scale=1.1)\nplt.figure(figsize=(35,2))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(cil.T, cmap=style2, \n           linewidth=0.3);\nax1.set_ylabel('Gender',fontsize=18)\nax1.set_xlabel('Salary',fontsize=18)\n# title\nax1.set_title('CONTINGENCY TABLE \\nSALARY VS. GENDER \\n\\n'+ country + ' 2020',fontsize=18);","f1e27850":"hyp_test(ci)","349fd7a5":"country = 'China'\nci = country_gender(country)\n#ci = ci.drop(['Prefer not to say', 'Prefer to self-describe'],axis=1)\nci = ci.drop(['Prefer not to say'],axis=1)\ncil = np.log(ci+1)\n\nsns.set(font_scale=1.1)\nplt.figure(figsize=(35,2))\nax1 = plt.subplot(131)\nax1 = sns.heatmap(cil.T, cmap='Reds', \n           linewidth=0.3);\nax1.set_ylabel('Gender',fontsize=18)\nax1.set_xlabel('Salary',fontsize=18)\n# title\nax1.set_title('CONTINGENCY TABLE \\nSALARY VS. GENDER \\n\\n'+ country + ' 2020',fontsize=18);","11710e43":"hyp_test(ci)","6fe65aa5":"When the salary data is analyzed according to the independent gender of the country, 2D cannot conclude with a contingency table, he cannot conclude anything yet. Therefore, he raises a hypothesis test to verify if the economic remuneration is independent of the worker's gender:","be4ef3f9":"# Highest level of formal education ","871477c7":"### India","508cb76b":"# ML algorithms vs. Job title and Framework vs. Education level for 2020 worldwide","a3683716":"With total certainty, 2D can guarantee that the salary depends on the country where someone works, in this case, the economic remuneration in Colombia is not the highest, compared to the highest salaries received in developed countries such as: Austral\u00eda, Canada, Germany, Japan, Sweden, United Kingdom and United States of America.\n\nFinally, 2D proposes to review if in this work there is equity gender in terms of economic remuneration.","89b8fdc5":"2D notes the marked similarity on a global level, and that is that in all countries there are more men than women working in this area, which is sad and disappointing. Yet he continues to ask the 'oracle' with perhaps something more encouraging, which is the level of formal education.","2966ced8":"# ML algorithms vs. Job title and Framework vs. Education level for 2020 in Colombia","49663842":"From the global contingency table, 2D verifies that indeed the frameworks most used in jobs in Colombia are not particular of a country, but are the most frequented worldwide. But there is a different finding with respect to Colombia and it is the following: The most used frameworks at work are also the most used with a university degree, also as a curiosity, it is found that the people surveyed who work with a student degree and use the Caret framework, they mostly have a degree with Master or Doctor.","5c8bb77f":"For India, which is the country with the largest number of people who have the dataset given by the oracle, it is found that there is a dependence in salary with gender, that is, it is a country where there is no gender equality. \n\nObserving this conclusion, 2D asks the same question but with his country.","b6046959":"## Salary vs. Country","ef797d1b":"2D is still excited, because regardless of the country, Python is the most used programming language, and SQL continues in the top 3.","9d0d8b30":"### Salary gender dependency by country","02ea4ac7":"# Framework vs. Job title and Framework vs. Education level for 2020 in Colombia","b701f733":"# Salary","5a4c1ef3":"Here 2D clarifies the information obtained above: it is more probable to find someone without university degrees working in a company with the student title; in the same way for people with a professional degree and non-formal higher education. Doctors and Masters normally serve as data scientists, but doctors as expected, can largely immerse themselves in scientific research. With this contingency table there is a new question for 2D: will there be any **correlation** between the different job titles for people with the levels of education presented in this table?","9831727e":"In this case, 2D observes that worldwide the most common job titles are students and data scientists. In developed countries, as expected, there is more scientific research in the field. To contrast this information, 2D wants to see what relationship exists between job title and level education, independent of the country.","cded3dc8":"With nostalgia, he observes that the number of women is considerably less than that of men, and not to mention the inclusion of the different groups or LGTBI communities. That is, although we are talking about the sexiest profession of the century, it is still a profession dominated by the male gender. Is this the same result globally?","e219a607":"In this case, 2D learns that as in India, in China there is no gender equality.\n\n2D could ask the oracle many more questions but with the information obtained, he already knows what decision to make and change his work situation, so he thanks the oracle and goes on.","0ee76d67":"The first thing that occurs to 2D is to verify in what age range is the sample of people related to Data Science in his country and worldwide graphically.","941be1e7":"Seeing this result, 2D jumps with joy, because without a doubt Python is the most used language by the people surveyed. But this is not the only thing that excites him.He also sees that SQL is in the top 3 of the most used languages and as time passes, the number of users increases. With these clear conclusions, 2D reviews the same results globally:","d494d7f2":"In this case, 2D quickly observes that the Evolutionary Approaches algorithms are rarely used in jobs and with people with an academic degree.\n\n2D consideres that he has a good basic knowledge about the local and global condition of the job of a DS, in the same way the basic and essential skills for this work. Therefore he desires to inquire about the economic contribution that would be obtained by dedicating itself to this profession.","adc7d3c3":"# Most used programming languages","edefcd81":"What is a Data Scientist? A quick Google search for this word gives more than 3 thousand million (3.170.000.000) results, an overwhelming number that shows the importance of this area in our society but also indicates how difficult it can be to say something original about the topic. Fortunately, curiosity, wanting to face a question and creativity are the main tools in the human intellect that allow people to do new things and advance on a particular topic.\n\nTaking as a premise the need to answer 'real' questions, people like the authors of this notebook, who are recent graduates in the area of Pyshics and Engineering and want to get a job in a developing country, see before their eyes the opportunity to adapt their academic skills to the sexiest job of the century: Data Science. Therefore, some questions are generated: \n\n- What is a Data Scientist?\n- At a social and local level, how is a Data Scientist described?\n- In relation to the above social conditions, what skills must a Data Scientist possess?\n- At the intersection of acquired knowledge and social environment, what is the possible salary of a Data Scientist?\n\nFor the first question, a summarized answer is given by [Favio V\u00e1zquez](https:\/\/medium.com\/@faviovazquez): \u201ca Data Scientist is a person(...) in charge of analyzing business\/organization problems and offering a structured solution that begins by turning this problem into a valid and comprehensive question. Then, using the scientific method, programming and computational tools, develop codes that prepare, clean and analyze data to create models and answer the initial question\u201d.\n\nIn this answer is the solution to the initial concerns of the young authors of this notebook. The questions generated by them in a precise way do not try to solve the problems of a business or organization, but to clarify the trade of a Data Scientist from a local perspective and how it is found worldwide, and thus be able to shape background skills with the objective of getting a job in Data Science.\n\nFavio's definition gives how to do it: using scientific method. And this is where the Kaggle Platform and this wonderful competition come in, since it gives the essential raw material with which the definition of Data Science will be applied to answer concerns of the authors. In this case, questionas are above.\n\nTo make this fun, we will analyze Kaggle's data by telling a story:\n\n## A journey to a successful Data Science World\n\n2D is the nickname of the main character of this story, who is a guy who lives in Colombia. He has studied everything he likes in life, had excellent grades at the university but after competing his studies he decides to look for a job, with such bad luck that his knowledge in basic science is not recognized by the managers of the Business. Despite this, he is not disappointed and looks for a job at the university, since he loves teaching and masters with a certain degree of detail those subjects that are the foundations of science and engineering, but desafotunately there are no vacancies in the universities of his region.\n\nDays go by and 2D disappointed takes refuge among his books, but due to chance in one of his statistics books that contains Pearson's tests, those that allow you to refute or approve a hypothesis in a categorical data set. This event makes him rethink the way he sees work life and he gets to the task of looking for the most in-demand job in today's times. Investigating a little more about the subject and with great surprise for him, he finds out that knowledge in basic science is a great help in the work of a Data Scientist.\n\nWith his spirit up, he undertakes a journey through the network, in search of more useful information for him. In the beginning everything he finds is wonderful but very general, which is good but he wants more concrete and precise information. After a few days he comes to a big page: *Kaggle*, which is an 'oracle' that gives answers to everything he needs.\n\nTo ask any question, the 'oracle' gives a set of data that under a programming language and an armament (set of libraries) the person asking the questions can obtain an answer. Therefore, 2D loads its best weapons (shown below) for the exploration and data visualization using Python. ","c8059eb6":"Sort by age ranges:","2ba99b08":"To 2D's surprise, he finds that the population mode in Colombia has an age range of [25-29] years, which is a range in which his age is found. He also realizes that in general, every year the number of people over the age of 22 increases.\n\n2D wants to review the behavior of the age range for each country by year, so he decides to contrast the information in a contingency table for all the countries that are repeated in the surveys for each year.","eaa03ce8":"In this case, 2D is glad that his country has gender equality, because the salary does not depend on gender. Finally, 2D reviews the same hypothesis for China.","bb997473":"Once the information is obtained, 2D groups the data by Colombia, since it is the country where he resides and wants to know as much as possible about Data Science in this region.","004c392b":"# Job title vs level education","9404e02f":"# Framework vs. Job title and Framework vs. Education level for 2020 worldwide","6ce04208":"## Salary vs. Gender","c5b05f7c":"# Colombian journey through Data Science","45198367":"In general, the highest educational level of people who are dedicated to data science is with a master's degree, as a particular case, the doctoral degree in Switzerland is similar to the number of masters. With these results, little by little the mood of 2D is better, since he observes that focusing his knowledge in the areas of DS and ML can increase the possibility of obtaining a job.\n\nSo, he decides to ask the oracle what is the title of the most common jobs that surveyed people usually work on.","0c2b8993":"In the previous table, 2D has discovered that if he wants to get a job in Colombia as a student or data scientist, the main frameworks that he must master are: Scikit-learn, TensorFlow, Keras and Xgboost. This news brings him joy and also an objective in which to apply his time to have a great Curriculum Vitae in these lines of work. He also realizes that people with a higher educational level prefer TensorFlow framework, over other options and with this he decides to verify if these conclusions remain worldwide.","f47c89cb":"With this visual contigence table, 2D recalls a visualization 'trick' of his basic university courses, and it is to transform the variables to a logarithmic scale in order to better read the information. With this, 2D realizes, for example, that in India the mode in the age range is [18-21], being the lowest age range and maintaining this range in the 3 years in which the surveys were conducted. It is also curious that the distribution of the age range in the United States of America in 2018 was concentrated in [25-29], that is, the mode value, but in 2019 and 2020 this concentration of values is dispersed to higher ranges, although the mode remains. Finally 2D finds that the worldwide mode range is [25-29], the same as his country of origin.\n\nWith a smile at the pleasant result found, 2D decides to see what the proportion of men and women is like in his country and worldwide.","3dc479da":"A pleasant surprise for 2D to realize that his country, a developing country, a large part of the population that is dedicated to the field of Data Science (DS) and Machine learning (ML) have a master's degree. This means, the mode in the Colombian data set is to have a master's degree. 2D then asks the same question worldwide, showing the results in a contingency table.","87a4353c":"The correlation matrix of the previous contingency table allows 2D to see more clearly the correlation between the job titles. Of particular interest, he observes that there is practically no correlation between the people who work with the degree of students and the scientific researchers (as expected). In the same way, he observes that scientific researchers do mostly work in statistics and data. 2D also evidence that researchers do not work on Bussiness.\n\nWith what he has learned so far, 2D pauses and reflects: his age range, his basic studies, and the definition of DS up to this point all match his personal tastes. Is it possible then that his computer skills will be coupled to the mentioned field?\n\nTo do this, he decides to ask the oracle about the most used programming languages in the field of DS, both locally and globally","d288358e":"### Colombia","d83d728b":"Reseting the index on every DataFrame, renaming column names and adding the corresponding year survey:","2d120db2":"## Pearson\u2019s Chi-Squared Test\nThe Chi-Squared test is a statistical hypothesis test that assumes (the null hypothesis) that the observed frequencies for a categorical variable match the expected frequencies for the categorical variable.\n\nThe variables are considered independent if the observed and expected frequencies are similar, that the levels of the variables do not interact, are not dependent.\n\nWe can interpret the test statistic in the context of the chi-squared distribution with the requisite number of degress of freedom as follows:\n\n- If Statistic >= Critical Value: significant result, reject null hypothesis (H0), dependent.\n- If Statistic < Critical Value: not significant result, fail to reject null hypothesis (H0), independent.\n\nIn terms of a p-value and a chosen significance level (alpha), the test can be interpreted as follows:\n\n- If p-value <= alpha: significant result, reject null hypothesis (H0), dependent.\n- If p-value > alpha: not significant result, fail to reject null hypothesis (H0), independent.\n\nThe degrees of freedom for the chi-squared distribution is calculated based on the size of the contingency table as:\n\n$(rows - 1) \\times (cols - 1)$\n","201fc12c":"2D glimpses with his question very valuable information. The table tells him that the most common algorithms used in the jobs are: Lineal Regression, Decision Tree, Gradient Boosting and Convolutional Neural Networks. He recognizes in its result two important aspects: a considerable number of people with a Master's degree frequently work with algorithms from the Convolutional Neural Network and Transformer Networks, which is a recent technology only used by a group of Masters who work as students or DS. With these results, he verifies how it is globally.","6964c559":"# Age range","940e1a3b":"For 2D, this result was expected: the mode in most recent job titles is Data Scientist, but there is an unexpected result, many people have job title as Students. Now, 2d will see the result worldwide in a contingency table.","a1ffb9fd":"Once the weapons are loaded, 2D knows that the more data the better conclusions, so he asks the oracle for the available data of the Machine Learning and Data Science topic of the years available. The very kind oracle gives the data of a population sample related to Data Science from the last 3 years.","7eb50e9a":"### China","6c88f904":"## Salary vs. Education level","2cc831fa":"Where the population sample for each year had the following number of respondents:","9c981f68":"As he explores the data, 2D realizes that the oracle offers data sets with the following types of questions:","f7ab4aa5":"# Gender","7097e58d":"# Current or most recent job title","880e02e0":"Apparently, salary does not depend on the academic level, but fortunately 2D remembers how this adventure began, he wanted to implement the Person's method to test his hypothesis: is the academic level independent of the economic remuneration?\n"}}