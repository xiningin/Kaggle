{"cell_type":{"46895944":"code","6ddb33eb":"code","dc565eda":"code","097c0a91":"code","385f7c02":"code","0997bb3e":"code","b7c50445":"code","366da2b1":"code","6e61d3be":"code","cad92ef9":"code","7d43470a":"code","eaffce46":"code","327439f1":"code","4f4b43da":"code","7b668057":"code","1ca5d55c":"markdown","55069ee5":"markdown","eaa84017":"markdown"},"source":{"46895944":"import time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport gc\nprint(\"Data:\\n\",os.listdir(\"..\/input\"))\n\n# Models Packages\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\n\n# Viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","6ddb33eb":"import h2o\nfrom h2o.automl import H2OAutoML\n# Set it according to kernel limits\nh2o.init(max_mem_size = \"10G\")","dc565eda":"print(\"\\nData Load Stage\")\ntraining = pd.read_csv('..\/input\/train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])#.sample(1000)\ntraindex = training.index\ntesting = pd.read_csv('..\/input\/test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])#.sample(1000)\ntestdex = testing.index\ny = training.deal_probability.copy()\nprint('Train shape: {} Rows, {} Columns'.format(*training.shape))\nprint('Test shape: {} Rows, {} Columns'.format(*testing.shape))","097c0a91":"# Combine Train and Test\ndf = pd.concat([training,testing],axis=0)\ndel training, testing\ngc.collect()\nprint('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))\n\nprint(\"Feature Engineering\")\ndf[\"price\"] = np.log(df[\"price\"]+0.001)\ndf[\"price\"].fillna(-999,inplace=True)\ndf[\"image_top_1\"].fillna(-999,inplace=True)\n\nprint(\"\\nCreate Time Variables\")\ndf[\"Weekday\"] = df['activation_date'].dt.weekday\ndf[\"Weekd of Year\"] = df['activation_date'].dt.week\ndf[\"Day of Month\"] = df['activation_date'].dt.day\n\n# Remove Dead Variables\ndf.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n\nprint(\"\\nEncode Variables\")\ncategorical = [\"user_id\",\"region\",\"city\",\"parent_category_name\",\"category_name\",\"item_seq_number\",\"user_type\",\"image_top_1\"]\nmessy_categorical = [\"param_1\",\"param_2\",\"param_3\",\"title\",\"description\"] # Need to find better technique for these\nprint(\"Encoding :\",categorical + messy_categorical)\n\n# Encoder:\nlbl = preprocessing.LabelEncoder()\nfor col in categorical + messy_categorical:\n    df[col] = lbl.fit_transform(df[col].astype(str))\n    \nX = df.loc[traindex,:].copy()\nprint(\"Training Set shape\",X.shape)\ntest = df.loc[testdex,:].copy()\nprint(\"Submission Set Shape: {} Rows, {} Columns\".format(*test.shape))\ndel df\ngc.collect()\n","385f7c02":"test.drop(\"deal_probability\",axis=1, inplace=True)","0997bb3e":"# Training and Validation Set\n#X_train, X_valid, y_train, y_valid = train_test_split(\n#    X, y, test_size=0.10, random_state=23)\n","b7c50445":"#del X, y\n#gc.collect()","366da2b1":"htrain = h2o.H2OFrame(X)\n#hval = h2o.H2OFrame(X_valid)\nhtest = h2o.H2OFrame(test)","6e61d3be":"#del X_train,X_valid,test\n#gc.collect()","cad92ef9":"x =htrain.columns\ny ='deal_probability'\nx.remove(y)\n\n# Set maximum runtime according to Kaggle limits\naml = H2OAutoML(max_runtime_secs = 18000)\naml.train(x=x, y =y, training_frame=htrain)\n\nprint('Generate predictions...')\nhtrain.drop(['deal_probability'])\n#preds = aml.leader.predict(hval)\n#preds = preds.as_data_frame()\n","7d43470a":"#print('RMSLE H2O automl leader: ', np.sqrt(metrics.mean_squared_error(y_valid, preds)))","eaffce46":"aml.leader","327439f1":"preds = aml.leader.predict(htest)\npreds = preds.as_data_frame()","4f4b43da":"#preds","7b668057":"lgsub = pd.DataFrame(preds.predict.values,columns=[\"deal_probability\"],index=testdex)\nlgsub['deal_probability'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\nlgsub.to_csv(\"lgsub.csv\",index=True,header=True)\n#print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)\/60))\nprint(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","1ca5d55c":"### Initializaing\nInitializing H2O AutoML instance, you can pass in your custom setting\n- See here for more details: http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html","55069ee5":"H2O has been dedicated to make ML accessible by non data science expert. One of the major achievements is the AutoML package they developed (and the enterprise version: Driverless AI) It aims to eliminate some of the most time-consuming (yet repetitive) work out of the data science pipeline. (ex. feature engineering, crosss validation, proper targt encoding etc). . How it works is that you specify the feature, set how long you want it to run, and the max memory it can use, and then.........just let it run for as long as you want. At the end, it will evaluate all the models it created, stack ensemble them, and give you the final result.\n\nI am not sure whether this package is able to handle the complexity of industry-level data science project (where most of the time is spent on cleaning\/find the right data...), but it seems like a \"Perfec\" tool for Kaggle. Knowing this actually makes me a little bit anxious. Indeed, we are here to learn about machine learning and practice our skills in data science, but at the end of the day, this is a competition site. Do you remember the frustration you had we you see an blend of blend of blend public kernel solution earns a silver? This is a similar feeling to me. As these packages become more and more sophisticated, a kaggle competition could turn into a pure competition of computation resources. Just randomly initiate some AutoML instance, let them run for as long you can, create ensemble, and then repeat the process. Indeed...someone with 0 machine learning experience will be able to get good scores this way, especially for competitions with anonymized data.\n\nThink of it another way, these packages might be able to raise the bar to a new level. I heard from a GM that, 5 years ago, if you know how to do stacking properly, you are already in the top 100 range. Nowadays, almost every kaggler knows how to do stacking (thanks to all generous contributors!) With packages like these, it is no longer enough to play around with sklearn and keras to get a descent score, you have to out-smart these auto-x packages (created by some of the most respectable Grand Masters) to be able to maintain a good stadnding. Challenging yet exiting! \n\nAnyways, here you go. All features come from public kernels. I will add references later (cannot remember which kernels I got them from...but I will figure it out)\n\n#### To replicate the 0.225 score, asumme you have 4 cores, set the run time to 28800s (on your own machine ofc), and give as much ram as you can.  Each run would have different iniialization so scores might be different from time to time. ","eaa84017":"# Automated Machine Learning with H2O AutoML"}}