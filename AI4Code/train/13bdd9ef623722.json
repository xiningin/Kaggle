{"cell_type":{"d5d1dd14":"code","9a6f50c3":"code","b1dd6413":"code","d3f897da":"code","b4809afd":"code","86bacd88":"code","75d446bf":"code","1c22f413":"code","be0f4c7a":"code","54a354cc":"code","75a78bdb":"code","9dd61d20":"code","96be1c1b":"code","4fbae365":"markdown","a346d156":"markdown","f01f3088":"markdown","a2f0c3a0":"markdown","0c07f171":"markdown","2234658d":"markdown","3d099d15":"markdown","99c89317":"markdown","16645359":"markdown","e1c2d166":"markdown","98c933ff":"markdown","70b6b09b":"markdown","f5de3652":"markdown","ccd138bb":"markdown","2340732e":"markdown"},"source":{"d5d1dd14":"\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"..\/input\/ts-course-data\/book_sales.csv\",\n    index_col='Date',\n    parse_dates=['Date'],\n).drop('Paperback', axis=1)\n\ndf.head()","9a6f50c3":"\nimport numpy as np\n\ndf['Time'] = np.arange(len(df.index))\n\ndf.head()","b1dd6413":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\n%config InlineBackend.figure_format = 'retina'\n\nfig, ax = plt.subplots()\nax.plot('Time', 'Hardcover', data=df, color='0.75')\nax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))\nax.set_title('Time Plot of Hardcover Sales');","d3f897da":"\ndf['Lag_1'] = df['Hardcover'].shift(1)\ndf = df.reindex(columns=['Hardcover', 'Lag_1'])\n\ndf.head()","b4809afd":"\nfig, ax = plt.subplots()\nax = sns.regplot(x='Lag_1', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))\nax.set_aspect('equal')\nax.set_title('Lag Plot of Hardcover Sales');","86bacd88":"\nfrom pathlib import Path\nfrom warnings import simplefilter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nsimplefilter(\"ignore\")  # ignore warnings to clean up output cells\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 4))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n%config InlineBackend.figure_format = 'retina'\n\n\n# Load Tunnel Traffic dataset\ndata_dir = Path(\"..\/input\/ts-course-data\")\ntunnel = pd.read_csv(data_dir \/ \"tunnel.csv\", parse_dates=[\"Day\"])\n\n# Create a time series in Pandas by setting the index to a date\n# column. We parsed \"Day\" as a date type by using `parse_dates` when\n# loading the data.\ntunnel = tunnel.set_index(\"Day\")\n\n# By default, Pandas creates a `DatetimeIndex` with dtype `Timestamp`\n# (equivalent to `np.datetime64`, representing a time series as a\n# sequence of measurements taken at single moments. A `PeriodIndex`,\n# on the other hand, represents a time series as a sequence of\n# quantities accumulated over periods of time. Periods are often\n# easier to work with, so that's what we'll use in this course.\ntunnel = tunnel.to_period()\n\ntunnel.head()","75d446bf":"df = tunnel.copy()\n\ndf['Time'] = np.arange(len(tunnel.index))\n\ndf.head()","1c22f413":"from sklearn.linear_model import LinearRegression\n\n# Training data\nX = df.loc[:, ['Time']]  # features\ny = df.loc[:, 'NumVehicles']  # target\n\n# Train the model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Store the fitted values as a time series with the same time index as\n# the training data\ny_pred = pd.Series(model.predict(X), index=X.index)","be0f4c7a":"\nax = y.plot(**plot_params)\nax = y_pred.plot(ax=ax, linewidth=3)\nax.set_title('Time Plot of Tunnel Traffic');","54a354cc":"df['Lag_1'] = df['NumVehicles'].shift(1)\ndf.head()","75a78bdb":"from sklearn.linear_model import LinearRegression\n\nX = df.loc[:, ['Lag_1']]\nX.dropna(inplace=True)  # drop missing values in the feature set\ny = df.loc[:, 'NumVehicles']  # create the target\ny, X = y.align(X, join='inner')  # drop corresponding values in target\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\ny_pred = pd.Series(model.predict(X), index=X.index)","9dd61d20":"\nfig, ax = plt.subplots()\nax.plot(X['Lag_1'], y, '.', color='0.25')\nax.plot(X['Lag_1'], y_pred)\nax.set_aspect('equal')\nax.set_ylabel('NumVehicles')\nax.set_xlabel('Lag_1')\nax.set_title('Lag Plot of Tunnel Traffic');","96be1c1b":"\nax = y.plot(**plot_params)\nax = y_pred.plot()","4fbae365":"The model actually created is (approximately): `Vehicles = 22.5 * Time + 98176`. Plotting the fitted values over time shows us how fitting linear regression to the time dummy creates the trend line defined by this equation.","a346d156":"Time-step features let you model **time dependence**. A series is time dependent if its values can be predicted from the time they occured. In the *Hardcover Sales* series, we can predict that sales later in the month are generally higher than sales earlier in the month.\n\n### Lag features\n\nTo make a **lag feature** we shift the observations of the target series so that they appear to have occured later in time. Here we've created a 1-step lag feature, though shifting by multiple steps is possible too.","f01f3088":"Linear regression with a lag feature produces the model:\n\n```\ntarget = weight * lag + bias\n```\n\nSo lag features let us fit curves to *lag plots* where each observation in a series is plotted against the previous observation.","a2f0c3a0":"### Time-step feature\n\nProvided the time series doesn't have any missing dates, we can create a time dummy by counting out the length of the series.","0c07f171":"When creating lag features, we need to decide what to do with the missing values produced. Filling them in is one option, maybe with 0.0 or \"backfilling\" with the first known value. Instead, we'll just drop the missing values, making sure to also drop values in the target from corresponding dates.","2234658d":"### Lag feature\n\nPandas provides us a simple method to lag a series, the `shift` method.","3d099d15":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https:\/\/www.kaggle.com\/learn\/time-series\/discussion) to chat with other learners.*","99c89317":"The procedure for fitting a linear regression model follows the standard steps for scikit-learn.","16645359":"This series records the number of hardcover book sales at a retail store over 30 days. Notice that we have a single column of observations `Hardcover` with a time index `Date`.\n\n# Linear Regression with Time Series #\n\nFor the first part of this course, we'll use the linear regression algorithm to construct forecasting models. Linear regression is widely used in practice and adapts naturally to even complex forecasting tasks.\n\nThe **linear regression** algorithm learns how to make a weighted sum from its input features. For two features, we would have:\n\n```\ntarget = weight_1 * feature_1 + weight_2 * feature_2 + bias\n```\n\nDuring training, the regression algorithm learns values for the parameters `weight_1`, `weight_2`, and `bias` that best fit the `target`. (This algorithm is often called *ordinary least squares* since it chooses values that minimize the squared error between the target and the predictions.) The weights are also called *regression coefficients* and the `bias` is also called the *intercept* because it tells you where the graph of this function crosses the y-axis.\n\n### Time-step features\n\nThere are two kinds of features unique to time series: time-step features and lag features.\n\nTime-step features are features we can derive directly from the time index. The most basic time-step feature is the **time dummy**, which counts off time steps in the series from beginning to end.","e1c2d166":"You can see from the lag plot that sales on one day (`Hardcover`) are correlated with sales from the previous day (`Lag_1`). When you see a relationship like this, you know a lag feature will be useful.\n\nMore generally, lag features let you model **serial dependence**. A time series has serial dependence when an observation can be predicted from previous observations. In *Hardcover Sales*, we can predict that high sales on one day usually mean high sales the next day.\n\n---\n\nAdapting machine learning algorithms to time series problems is largely about feature engineering with the time index and lags. For most of the course, we use linear regression for its simplicity, but these features will be useful whichever algorithm you choose for your forecasting task.\n\n# Example - Tunnel Traffic #\n\n*Tunnel Traffic* is a time series describing the number of vehicles traveling through the Baregg Tunnel in Switzerland each day from November 2003 to November 2005. In this example, we'll get some practice applying linear regression to time-step features and lag features.\n\nThe hidden cell sets everything up.","98c933ff":"The lag plot shows us how well we were able to fit the relationship between the number of vehicles one day and the number the previous day.","70b6b09b":"The best time series models will usually include some combination of time-step features and lag features. Over the next few lessons, we'll learn how to engineer features modeling the most common patterns in time series using the features from this lesson as a starting point.\n\n# Your Turn #\n\nMove on to the Exercise, where you'll begin [**forecasting Store Sales**](https:\/\/www.kaggle.com\/kernels\/fork\/19615998) using the techniques you learned in this tutorial.","f5de3652":"# Welcome to Time Series! #\n\n**Forecasting** is perhaps the most common application of machine learning in the real world. Businesses forecast product demand, governments forecast economic and population growth, meteorologists forecast the weather. The understanding of things to come is a pressing need across science, government, and industry (not to mention our personal lives!), and practitioners in these fields are increasingly applying machine learning to address this need.\n\nTime series forecasting is a broad field with a long history. This course focuses on the application of modern machine learning methods to time series data with the goal of producing the most accurate predictions. The lessons in this course were inspired by winning solutions from past Kaggle forecasting competitions but will be applicable whenever accurate forecasts are a priority.\n\nAfter finishing this course, you'll know how to:\n- engineer features to model the major time series components (*trends*, *seasons*, and *cycles*),\n- visualize time series with many kinds of *time series plots*,\n- create forecasting *hybrids* that combine the strengths of complementary models, and\n- adapt machine learning methods to a variety of forecasting tasks.\n\nAs part of the exercises, you'll get a chance to participate in our [Store Sales - Time Series Forecasting](https:\/\/www.kaggle.com\/c\/29781) Getting Started competition. In this competition, you're tasked with forecasting sales for *Corporaci\u00f3n Favorita* (a large Ecuadorian-based grocery retailer) in almost 1800 product categories.\n\n# What is a Time Series? #\n\nThe basic object of forecasting is the **time series**, which is a set of observations recorded over time. In forecasting applications, the observations are typically recorded with a regular frequency, like daily or monthly.","ccd138bb":"What does this prediction from a lag feature mean about how well we can predict the series across time? The following time plot shows us how our forecasts now respond to the behavior of the series in the recent past.","2340732e":"Linear regression with the time dummy produces the model:\n\n```\ntarget = weight * time + bias\n```\n\nThe time dummy then lets us fit curves to time series in a *time plot*, where `Time` forms the x-axis."}}