{"cell_type":{"26795fdc":"code","88a4f3ca":"code","90cf5af8":"code","21452c16":"code","6474d41e":"code","ffa91cfd":"code","7c98ba01":"code","7b3ed9b6":"code","5a4c3e8b":"code","5e2db609":"code","438cbe07":"code","089bb424":"code","c6e22a47":"code","f486f12d":"code","2a4f3c2c":"code","086199e9":"code","33fdd022":"code","812f5f26":"code","89ea102a":"code","f643a87b":"code","e3871a35":"code","5355c45f":"code","219d096c":"code","9123626d":"code","5519abba":"code","dff25799":"code","13d9d999":"code","56e76cc8":"code","543d4682":"code","a35269e7":"code","078da1a8":"code","6ef1c76f":"code","99c61ba2":"code","8a7760d9":"code","f4a1c96a":"code","13d6e390":"code","23490504":"code","0fa309f3":"code","115663db":"code","198bd957":"code","8534c9bb":"code","72f6248f":"code","19a87efd":"code","4d5555c1":"code","c20373aa":"code","bf93b478":"code","6d9fe2cd":"code","c17a4e48":"code","97d6f11a":"code","04084369":"code","d85dda4f":"markdown","88a3343f":"markdown","18e7fb51":"markdown","67763ba0":"markdown","3a2db135":"markdown","ebc853c9":"markdown","7187bc26":"markdown","1ba8059c":"markdown","4bdec340":"markdown","d07bb786":"markdown","5c5d2dce":"markdown","3b80be0d":"markdown","a1dac795":"markdown","f3acd170":"markdown","4316f16d":"markdown","e3bc9cb2":"markdown","de8a6685":"markdown","0ffc6a1b":"markdown","d65efe02":"markdown","d8e31920":"markdown"},"source":{"26795fdc":"#Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import norm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","88a4f3ca":"file_path = \"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\"\ndiabetes = pd.read_csv(file_path)\ndiabetes.shape","90cf5af8":"diabetes.head()","21452c16":"diabetes.info()","6474d41e":"print(\"No. of people without diabetes: \",diabetes[\"Outcome\"].value_counts()[0])\nprint(\"No. of people with diabetes : \",diabetes[\"Outcome\"].value_counts()[1])\nprint(\"Percent of people with diabetes : \",round(diabetes[\"Outcome\"].value_counts()[1]\/len(diabetes.index)*100,2), \"%\")","ffa91cfd":"sns.countplot(\"Outcome\", data=diabetes);","7c98ba01":"#Descriptive Statistics - Five Point Summary\ndiabetes.describe().T","7b3ed9b6":"#Distribution of each feature\n\nsns.set_style(\"darkgrid\")\n\nfig, ax2 = plt.subplots(4, 2, figsize=(16, 16))\n\nsns.distplot(diabetes['Pregnancies'],ax=ax2[0][0], fit=norm)\nsns.distplot(diabetes['Glucose'],ax=ax2[0][1], fit=norm)\nsns.distplot(diabetes['BloodPressure'],ax=ax2[1][0], fit=norm)\nsns.distplot(diabetes['SkinThickness'],ax=ax2[1][1], fit=norm)\nsns.distplot(diabetes['Insulin'],ax=ax2[2][0], fit=norm)\nsns.distplot(diabetes['BMI'],ax=ax2[2][1], fit=norm)\nsns.distplot(diabetes['DiabetesPedigreeFunction'],ax=ax2[3][0], fit=norm)\nsns.distplot(diabetes['Age'],ax=ax2[3][1], fit=norm)","5a4c3e8b":"#Outliers analysis of each feature\n\nsns.set_style(\"darkgrid\")\n\nfig, ax2 = plt.subplots(4, 2, figsize=(16, 16))\n\nsns.boxplot(diabetes['Pregnancies'],ax=ax2[0][0])\nsns.boxplot(diabetes['Glucose'],ax=ax2[0][1])\nsns.boxplot(diabetes['BloodPressure'],ax=ax2[1][0])\nsns.boxplot(diabetes['SkinThickness'],ax=ax2[1][1])\nsns.boxplot(diabetes['Insulin'],ax=ax2[2][0])\nsns.boxplot(diabetes['BMI'],ax=ax2[2][1])\nsns.boxplot(diabetes['DiabetesPedigreeFunction'],ax=ax2[3][0])\nsns.boxplot(diabetes['Age'],ax=ax2[3][1])","5e2db609":"sns.countplot(\"Pregnancies\", data=diabetes);","438cbe07":"pd.crosstab(diabetes[\"Pregnancies\"], diabetes[\"Outcome\"]).plot()","089bb424":"plt.figure(figsize=(10,10))\nsns.pairplot(diabetes, diag_kind='kde', hue=\"Outcome\")\nplt.show()","c6e22a47":"plt.figure(figsize=(10,10))\nsns.pairplot(diabetes, kind='reg', hue='Outcome')\nplt.show()","f486f12d":"plt.figure(figsize=(10,10))\nsns.heatmap(diabetes.corr(), cmap='magma', vmin = -1, vmax = 1, annot=True, fmt=\"0.2f\", square=True, linewidths=0.2)\nplt.show()","2a4f3c2c":"sns.lmplot(\"Age\", \"Glucose\", data=diabetes, hue='Outcome');","086199e9":"sns.lmplot(\"BloodPressure\", \"Glucose\", data=diabetes, hue='Outcome');","33fdd022":"sns.countplot(\"Pregnancies\", hue=\"Outcome\", data=diabetes)","812f5f26":"sns.lmplot(y=\"Insulin\",x=\"Glucose\", hue=\"Outcome\", data=diabetes);","89ea102a":"diabetes.columns","f643a87b":"#All features except Pregnancies - replacing 0s with NaNs\ndiabetes[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']] = diabetes[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']].replace(to_replace=0, value=np.nan)","e3871a35":"diabetes.head()","5355c45f":"diabetes.info()","219d096c":"print(\"Number of missing values in dataframe : \\n\", diabetes.isnull().sum())\nprint(\"-------------------------\")\nprint(\"Percentage of columnwise missing values in dataframe : \\n\", round(diabetes.isnull().mean() * 100, 2))","9123626d":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\n\ndiabetes_cols = diabetes.columns\n\ndiabetes = imputer.fit_transform(diabetes)\n\ndiabetes = pd.DataFrame(diabetes, columns = diabetes_cols)\n\ndiabetes.head()","5519abba":"X = diabetes[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\ny = diabetes['Outcome']\nprint(\"Shape of X :\", X.shape)\nprint(\"Shape of y :\",y.shape)","dff25799":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=diabetes[\"Outcome\"], random_state=24)","13d9d999":"X_train.shape","56e76cc8":"y_train.shape","543d4682":"X_test.shape","a35269e7":"y_test.shape","078da1a8":"X_train.sample(3)","6ef1c76f":"X_test.sample(3)","99c61ba2":"from sklearn.preprocessing import MinMaxScaler\nmmScaler = MinMaxScaler()\nX_train_scaled = mmScaler.fit_transform(X_train.values)\nX_test_scaled = mmScaler.fit_transform(X_test.values)\n\nX_train = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\nX_test = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)","8a7760d9":"X_train.sample(3)","f4a1c96a":"X_test.sample(3)","13d6e390":"#Logistic Regression with liblinear solver\nlog_reg_ml = LogisticRegression(solver = \"liblinear\")\nlog_reg_ml.fit(X_train, y_train)","23490504":"#Accuracy score for train set\nlog_reg_ml.score(X_train, y_train)","0fa309f3":"#Accuracy score for test set\nlog_reg_ml.score(X_test, y_test)","115663db":"#Let us predict the Test set\ny_test_pred = log_reg_ml.predict(X_test)","198bd957":"#Let us predict the corresponding Probabilities for Test set\ny_test_pred_prob = log_reg_ml.predict_proba(X_test)","8534c9bb":"#Confusion Matrix\ncm = confusion_matrix(y_test, y_test_pred, labels=[1, 0])\nplt.figure(figsize = (7,5))\nsns.heatmap(cm, annot=True, square=True, fmt = \".2f\")\nplt.show()","72f6248f":"TP = cm[1,1] # True Positive\nFN = cm[0,0] # False Negative\nFP = cm[0,1] # False Positive\nTN = cm[1,0] # True Negative","19a87efd":"precision = TP\/(TP+FP)\nprecision","4d5555c1":"recall = TP\/(TP+FN)\nrecall","c20373aa":"sensitivity = TP\/(TP+FN)\nsensitivity","bf93b478":"specificity = TN \/ (TN + FP)\nspecificity","6d9fe2cd":"#Receiver Operator Characteristic Curve \nroc_auc_score(y_test, y_test_pred)","c17a4e48":"# Defining the function to plot the ROC curve\ndef draw_roc(y_test, y_test_pred ):\n    fpr, tpr, thresholds = roc_curve( y_test, y_test_pred,\n                                              drop_intermediate = False )\n    auc_score = roc_auc_score(y_test, y_test_pred)\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None\n\n# Calling the function\ndraw_roc(y_test, y_test_pred)","97d6f11a":"#Precision-Recall curve\np, r, thresholds = precision_recall_curve(y_test, y_test_pred)\nplt.plot(thresholds, p[:-1], \"b-\")\nplt.plot(thresholds, r[:-1], \"g-\")\nplt.show()","04084369":"print(classification_report(y_test, y_test_pred, labels=[1, 0]))","d85dda4f":"# Data Preprocessing","88a3343f":"Let us check the shapes of predictors and target","18e7fb51":"1. Split the data into train and test\n2. Impute for Missing Values\n3. Data Transformation (Scaling)","67763ba0":"# Exploratory Data Analysis (EDA)","3a2db135":"Let us check the bivariate regression plots for all the feature combinations","ebc853c9":"Let us check the bivariate scatterplots across all the feature combinations","7187bc26":"Except for \"BMI\" every other feature is having strong positive skewness and also displaying kurtosis. Distributions differ heavily from the Normal Distribution (Bell Curve).","1ba8059c":"# Model Evaluation & Performance Metrics","4bdec340":"Let us plot the Histograms for all the numeric features to understand their distribution.","d07bb786":"We can see that the ratio of Outcomes is 0:1 :: 65% : 35% (which is not bad and acceptable). Though we can use Stratified sampling during our train-test-split exercise to handle the class imbalance.","5c5d2dce":"We can see that there are 7 Predictors and 1 Target column (Outcome). All the columns are quantitative in nature.\nThough there are no missing values, we need to check for inconsistent\/zeroes values in the features. ","3b80be0d":"Let us visualise the NaNs now in the dataset","a1dac795":"Let us observe the Outliers and Quantile distribution of data using BoxPlots. ","f3acd170":"Imputing the NaNs with Median values","4316f16d":"> # ML Model Building","e3bc9cb2":"Let us plot the frequencies of Number of positive vs Negative cases. ","de8a6685":"Let us check'0s' in some of the features and impute them. We will include \"Pregnancies\" column from this imputation since '0' Pregnancies is a valid information.","0ffc6a1b":"Before we fit the model, let us do scaling of features to handle outliers and different feature scales","d65efe02":"**Objective:**\n\n*Predict which people are likely to develop diabetes.*\n\n**About the dataset:**\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n**Data Dictionary:**\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n- Pregnancies: Number of times pregnant\n- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n- BloodPressure: Diastolic blood pressure (mm Hg)\n- SkinThickness: Triceps skin fold thickness (mm)\n- Insulin: 2-Hour serum insulin (mu U\/ml)\n- BMI: Body mass index (weight in kg\/(height in m)^2)\n- DiabetesPedigreeFunction: Diabetes pedigree function\n- Age: Age (years)\n- Outcome: Class variable (0 or 1)","d8e31920":"Splitting the data into train and test sets. We will use Stratified sampling to handle class imbalance."}}