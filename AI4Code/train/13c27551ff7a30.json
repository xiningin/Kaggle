{"cell_type":{"98cd5e8e":"code","ee23aac6":"code","117badb5":"code","a4ae1ecb":"code","c984e464":"code","c2952aea":"code","d1e9da53":"code","027cbfad":"code","5b8d03e4":"code","e7704b25":"code","3d6094f0":"code","4e016b10":"code","170e0f76":"code","7fe73e24":"code","451a71ab":"code","43a8c04c":"code","ae82796b":"code","6c9cc81d":"code","961293dd":"code","676be0db":"code","3a7af006":"code","63e69148":"code","856fad90":"code","c0ff3614":"code","01de02d4":"code","c69b18c1":"code","1db3e03f":"code","c4ea26e7":"code","342e7e42":"code","59cadfae":"code","f944dd3a":"code","fed2ef40":"code","7ee46b66":"code","6d1e5db8":"code","88d55924":"code","47363c6f":"code","033139e5":"code","1ef506cd":"code","90a7ca69":"code","eedd8459":"code","b32ade90":"code","922492cc":"code","2f6712d8":"code","ac1fd949":"code","948864d7":"markdown","bce0f1d5":"markdown","7905f5b6":"markdown","5104cda8":"markdown","04cb787d":"markdown","6c24c361":"markdown","d292afd7":"markdown","e7af35d9":"markdown","615ece94":"markdown","921c522d":"markdown","9e2c38c2":"markdown","b58ae8e8":"markdown","b7da65ae":"markdown","1ec85665":"markdown","34182491":"markdown","5d6064c4":"markdown","29369322":"markdown","46ee9347":"markdown","b3e89ff1":"markdown","f8ad0cf4":"markdown","28149fdf":"markdown","ad5375dd":"markdown","09dee9ac":"markdown","b26a3b0d":"markdown","64f41a97":"markdown","ce9c1113":"markdown","2bb62104":"markdown","d5a62f7c":"markdown","78f60419":"markdown","d54c1f0e":"markdown","856fa370":"markdown","a8caf8b6":"markdown","2abb4823":"markdown","20fe34a0":"markdown","e610768f":"markdown","78b96b49":"markdown","acad609c":"markdown","98e601af":"markdown","2acb216d":"markdown"},"source":{"98cd5e8e":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding, Bidirectional\nimport pandas as pd\nimport numpy as np\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import TimeDistributed, Activation, Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support","ee23aac6":"import os\n\nfilenames = []\nfor i in os.listdir('..\/input\/large-conll-data-2002'):\n    filenames.append(i)","117badb5":"for i in filenames:\n    base_file = i\n    name, ext = base_file.split('.conll')\n    new_file = '{}.{}'.format(name, 'txt')\n\n    with open('..\/input\/large-conll-data-2002\/'+ base_file , 'r') as f1:\n        with open(new_file, 'w') as f2:\n            f2.write(f1.read())\n    ","a4ae1ecb":"all_x = []\nfor i in os.listdir('.\/'):\n    print(i)\n    raw = open('.\/'+ i, 'r').readlines()\n    point = []\n    for line in raw:\n        stripped_line = line.strip().split(' ')\n    #     try:\n    #         if 'B-' in stripped_line[1]:\n    #             s = stripped_line\n    #             s[1] = s[1].replace('B-', 'I-')\n    #     except:\n    #         pass\n        point.append(stripped_line)\n        if line == '\\n':\n            all_x.append(point[:-1])\n            point = []\n    all_x = all_x[:-1]","c984e464":"len(all_x)","c2952aea":"# clean all_x\n\nclean_all_x = []\n\nfor sent_list in all_x:\n    point = []\n    for sent in sent_list:\n        if len(sent) != 1:\n            point.append(sent)\n            \n    clean_all_x.append(point)","d1e9da53":"len(clean_all_x)","027cbfad":"lengths = [len(x) for x in clean_all_x]\nprint ('Input sequence length range: ', max(lengths), min(lengths))","5b8d03e4":"for i in range(len(clean_all_x)):\n    if len(clean_all_x[i]) == 563:\n        print(i)\n        print()\n        print(clean_all_x[i][:150])\n        break","e7704b25":"np.array(clean_all_x).shape","3d6094f0":"ind_list = []\n\nfor i in range(len(clean_all_x)):\n    for j in range(len(clean_all_x[i])):\n        if clean_all_x[i][j][1] == 'B-null' or clean_all_x[i][j][1] == 'I-null':\n            ind_list.append([i, j, clean_all_x[i][j]])\n            \nind_list","4e016b10":"replace_list = ['B-SECTION', \n                'I-SECTION','I-SECTION','I-SECTION','B-SECTION', 'I-SECTION','B-ACT','B-ACT','I-ACT', 'B-CITATION',\n                'I-CITATION','I-CITATION','I-CITATION','I-CITATION','I-CITATION',\n                'B-ACT','B-SECTION','I-SECTION'\n               ]","170e0f76":"for ind, i in enumerate(ind_list):\n    print(i)\n    print('Before---', clean_all_x[i[0]][i[1]])\n    clean_all_x[i[0]][i[1]][1] = replace_list[ind]\n    print('After---', clean_all_x[i[0]][i[1]])\n    print()","7fe73e24":"maxlen = 50\nmaxlen","451a71ab":"all_text = []\n\nfor sent in clean_all_x:\n#     print()\n#     print(len(sent))\n    if len(sent) <= maxlen:\n#         print(\"I entered\")\n        new_sent = []\n        for i in range(len(sent)):\n            new_sent.append(sent[i])\n                \n        all_text.append(new_sent)\n        \n    else:\n#         print(\"I entered here\")\n        times = len(sent) \/\/ maxlen\n#         print(\"times \", times)\n        start = 0\n        for i in range(times):\n            new_sent = []\n            for j in range(maxlen):\n                new_sent.append(sent[start+j])\n#                 print('Done')\n            start += j\n#             print(start)\n            all_text.append(new_sent)","43a8c04c":"len(all_text)","ae82796b":"X = [[c[0] for c in x] for x in all_text]\ny = [[c[1] for c in y] for y in all_text]\n\nprint('len of X ---', len(X), 'len of y ---', len(y))","6c9cc81d":"def encode(x, n):\n    result = np.zeros(n)\n    result[x] = 1\n    return result\n\n# total list of tags or labels\nlabels = list(set([c for x in y for c in x]))\n\n# dictionary mapping label to an integer\nlabel2ind = {label: (index + 1) for index, label in enumerate(labels)}\n\n# dictionary mapping integer to the label\nind2label = {(index + 1): label for index, label in enumerate(labels)}\n\n# add 0 index as \"Padding\" label\nind2label[0] = 'Padding'\n\n# maximum number of label or total tags \nmax_label = max(label2ind.values()) + 1\n\n# replace the label with its index (integer) and add 0 in the remaining (50- length of sentence)\n# e.g. if the sentence length is 30 then there are 30 tags , so replace those 30 tags with its index or integer \n# specified in label2ind dictionary and fill 20 remaining with 0 so it makes a total length of 50\n\ny_enc = [[label2ind[c] for c in ey] + [0] * (maxlen - len(ey)) for ey in y]\n\n# Do one hot encoding of the labels \ny_enc = [[encode(c, max_label) for c in ey] for ey in y_enc]\ny_enc = pad_sequences(y_enc, maxlen=maxlen)\ny_enc[3][:4]","961293dd":"ind2label","676be0db":"print('shape of features X', np.array(X).shape)\nprint('shape of label y', np.array(y_enc.shape))","3a7af006":"new_X = []\npad = []\nfor j, seq in enumerate(X):\n    new_seq = []\n    for i in range(maxlen):\n        try:\n            new_seq.append(seq[i])\n        except:\n            pad.append(j)\n            new_seq.append(\"PADword\")\n    new_X.append(new_seq)\n\nprint('Count of padded sentences----', len(set(pad)))\nprint('Count of non-padedd sentences----', len(X) - len(set(pad)))","63e69148":"print('shape of features X', np.array(new_X).shape)\nprint('shape of label y', np.array(y_enc.shape))","856fad90":"batch_size = 10","c0ff3614":"x_train = new_X[0:3200]\ny_train = y_enc[0:3200]\n\nx_test = new_X[3200:3400]\ny_test = y_enc[3200:3400]\n\nprint ('Training and testing tensor shapes:', np.array(x_train).shape, np.array(x_test).shape, y_train.shape, y_test.shape)","01de02d4":"from keras.models import Model, Input\nfrom keras.layers.merge import add\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\nimport tensorflow.compat.v1 as tf\nimport tensorflow_hub as hub\ntf.disable_eager_execution()\nfrom tensorflow.compat.v1.keras import backend as K\nsess = tf.Session()\nK.set_session(sess)","c69b18c1":"elmo_model = hub.Module(\"https:\/\/tfhub.dev\/google\/elmo\/2\", trainable=True)\nsess.run(tf.global_variables_initializer())\nsess.run(tf.tables_initializer())\n\n\ndef ElmoEmbedding(x):\n    return elmo_model(inputs={\n                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n                            \"sequence_len\": tf.constant(batch_size*[maxlen])\n                      },\n                      signature=\"tokens\",\n                      as_dict=True)[\"elmo\"]","1db3e03f":"out_size = len(label2ind) + 1\n\ninput_text = Input(shape=(maxlen,), dtype=tf.string)\n\nembedding = Lambda(ElmoEmbedding, output_shape=(maxlen, 1024))(input_text)\n\nx = Bidirectional(LSTM(units=512, return_sequences=True,\n                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n\nx_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n                           recurrent_dropout=0.2, dropout=0.2))(x)\n\nx = add([x, x_rnn])  # residual connection to the first biLSTM\n\nout = TimeDistributed(Dense(out_size, activation=\"softmax\"))(x)\n\nmodel = Model(input_text, out)\n\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","c4ea26e7":"model.summary()","342e7e42":"!mkdir saved_models","59cadfae":"from keras.callbacks import ModelCheckpoint\n\n# checkpoint\nfilepath=\".\/saved_models\/model_epoch{epoch:02d}_loss{loss:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, mode='min')\ncallbacks_list = [checkpoint]","f944dd3a":"history = model.fit(np.array(x_train), \n                    y_train, \n                    batch_size=batch_size, \n                    epochs=20,\n                    validation_data=(np.array(x_test), y_test),\n                    callbacks=callbacks_list,\n                    verbose=1)","fed2ef40":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,5))\nplt.plot(history.history['loss'], color='r')\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()","7ee46b66":"plt.figure(figsize=(10,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()","6d1e5db8":"!pip install seqeval","88d55924":"from seqeval.metrics import classification_report","47363c6f":"def pred2label(pred):\n    out = []\n    for pred_i in pred:\n        out_i = []\n        for p in pred_i:\n            p_i = np.argmax(p)\n            out_i.append(ind2label[p_i].replace(\"Padding\", \"O\"))\n        out.append(out_i)\n    return out\n\ndef pred_format(pred):\n    new_test_pred = []\n\n    for i in pred:\n        for j in i:\n            new_test_pred.append(j)\n    \n    return new_test_pred","033139e5":"test_pred = []\ntest_real = []\n\nj = 0\nfor i in range(10, 100, 10):\n    pred = model.predict(np.array(new_X[3400+j:3400+i]), verbose=1)\n    test_pred.append(pred2label(pred))\n    test_real.append(pred2label(y_enc[3400+j:3400+i]))\n    j = i\n    \nprint('Before --------', np.array(test_real).shape, np.array(test_pred).shape)\n\ntest_pred = pred_format(test_pred)\ntest_real = pred_format(test_real)\n\nprint('After --------', np.array(test_real).shape, np.array(test_pred).shape)","1ef506cd":"import warnings\n\nwarnings.filterwarnings('ignore')\n\nprint(classification_report(test_real, test_pred))","90a7ca69":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score","eedd8459":"def sklearn_formatter(pred):\n    out = []\n    for i in pred:\n        for j in i:\n            out.append(j)\n            \n    return out\n\nprint('Before --------', np.array(test_real).shape, np.array(test_pred).shape)\n\nsklearn_test_real = sklearn_formatter(test_real)\nsklearn_test_pred = sklearn_formatter(test_pred)\n\nprint('After --------', np.array(sklearn_test_real).shape, np.array(sklearn_test_pred).shape)","b32ade90":"print('Accuracy Score --', accuracy_score(sklearn_test_real,sklearn_test_pred))\n\nprint('F1 Score --', f1_score(sklearn_test_real,sklearn_test_pred, average='micro'))\n\nprint('Precision Score --', precision_score(sklearn_test_real,sklearn_test_pred, average='micro'))\n\nprint('Recall Score --', recall_score(sklearn_test_real,sklearn_test_pred, average='micro'))\n\nprint('Cohen Kappa Score --', cohen_kappa_score(sklearn_test_real,sklearn_test_pred))","922492cc":"input_text = input()\ntokens = input_text.split(' ')\n\nif len(tokens) < maxlen:\n    rem = maxlen - len(tokens)\n    for i in range(rem):\n        tokens.append('PADword')\n        \nwoke = [tokens] +[['PADword']*50]+ [['PADword']*50]+ [['PADword']*50]+ [['PADword']*50]+ [['PADword']*50]+[['PADword']*50]+[['PADword']*50]+[['PADword']*50]+[['PADword']*50]\n\ntest_pred = model.predict(np.array(woke), verbose=1)\n\npred_labels = pred2label(test_pred)\nprint()\nfor tok, lab in zip(woke, pred_labels):\n    for i , j in zip(tok, lab):\n        if j.startswith('B') or j.startswith('I'):\n            print(i, '-', j)","2f6712d8":"test_pred = []\ntest_real = []\n\nj = 0\nfor i in range(10, 3400, 10):\n    pred = model.predict(np.array(new_X[j:i]), verbose=1)\n    test_pred.append(pred2label(pred))\n    test_real.append(pred2label(y_enc[j:i]))\n    j = i\n    \nprint('Before --------', np.array(test_real).shape, np.array(test_pred).shape)\n\ntest_pred = pred_format(test_pred)\ntest_real = pred_format(test_real)\n\nprint('After --------', np.array(test_real).shape, np.array(test_pred).shape)","ac1fd949":"import warnings\n\nwarnings.filterwarnings('ignore')\n\nprint(classification_report(test_real, test_pred))","948864d7":"## SEQEVAL METRICS","bce0f1d5":"Import Libraries","7905f5b6":"## MODEL EVALUATION","5104cda8":"## EVALUATION ON REMAINING 90 UNSEEN DATA","04cb787d":"************************************************************","6c24c361":"Separate tokens and tags from every line and save tokens in X and tags in y.","d292afd7":"There are some tokens which are labelled as B-null or I-null. Find out them and convert them into their proper format","e7af35d9":"*****************************************************************","615ece94":"# LEGAL PARSER\n\n***************************************************************\nThis is an NER notebook named Legal Parser which extracts SECTION, ACTS and CITATION from any facts (sentences) in any judgement (legal documents).\n\nI have manually labelled the some indian legal judgements using INCEpTION tool.","921c522d":"## CHECKPOINT","9e2c38c2":"***************************************************************************************","b58ae8e8":"# MANUAL TESTING\n\n***************************************************************\n\nRun the following snippet and input the sentence you want to check. Make sure the sentence is not more than 50 \n\nOr if you want you can add some trimming code in the snippet","b7da65ae":"## DETAILS\n\n******************************************************************\n\n**MODEL** - Elmo + BiLSTM (512 hidden units) \n\n**DATA FORMAT** - 2002 conll data \n\n**ANNOTATION** - IOB Format\n\n**TOOL FOR ANNOTATION** - INCEpTION Tool\n\n","1ec85665":"after this process, data size increased from 3332 to 3497","34182491":"******************************************************","5d6064c4":"Sklearn requires the format of y_true and y_pred as a single array like [ 'label1', 'label2', .... ] and not list of list like Seqeval ","29369322":"## CODE IMPLEMENTATION","46ee9347":"Seqeval requires the format as to be list of list [ [ 'label1', 'label2', ... ] ]","b3e89ff1":"All the lines in the files contains two things token and tag but some lines have only tags and we don't want that hence we need to remove them and save it in the clean_all_x list","f8ad0cf4":"## SKLEARN METRICS","28149fdf":"## TRAINING","ad5375dd":"Folder to save the model on every epoch","09dee9ac":"## MODEL ARCHITECTURE","b26a3b0d":"**************************************************************","64f41a97":"Now similary pad X ","ce9c1113":"## TRAIN TEST SPLIT","2bb62104":"* ## TOOL USAGE INSTRUCTION\n\n*****************************************************************************\n**DOWNLOAD SITE** - https:\/\/inception-project.github.io\/downloads\/\n\nAfter going on this site, click on the downloadable JAR File named as \"*INCEpTION 0.17.4 (executable JAR)*\" and download it.\n\n**PRE-REQUISITES** - Install java in your system \n\n**HOW TO RUN** - \n            \n* Open terminal\n* Make sure java is installed\n* Type\n    \n      cd \/usr\/bin\/\n      \n* Then type\n\n      java -jar \/home\/peepstake\/Downloads\/inception-app-standalone-0.17.3.jar\n\n* Visit localhost:8080 on your web browser and INCEpTION would be running there","d5a62f7c":"For elmo embedding, it is required that both training and testing data should be divisible by batch_size. Hence divide the data accordingly","78f60419":"## Elmo EMBEDDING","d54c1f0e":"## TRAINING PLOTS","856fa370":"## PREPROCESSING\n************************************************************\nlarge-conll-data-2002 folder contains all the annotated files in .conll format.\n\nIterate through that directory and write all the file names in a list named as filenames.\n","a8caf8b6":"Now iterate through all the files and save the \"**token tags**\" (e.g. Section I-SECTION ) one by one in a list all_x","2abb4823":"************************************************************\n","20fe34a0":"If the sentences are more than 50, trim them into multiple lines of size 50 or less than it .\n\ne.g. if the length of sentence is 150, then divide that line into 3 parts of size 50 each.","e610768f":"What is the maximum and minimum length of sentences ?","78b96b49":"Now change the format of all files from .conll to .txt and it will be saved in the \/kaggle\/working directory","acad609c":"Now encode y ","98e601af":"## EVALUATION ON 3400 DATA","2acb216d":"Set a maximum length upto which the sentences should be trimmed or padded. "}}