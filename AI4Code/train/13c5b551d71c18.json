{"cell_type":{"3e76933b":"code","06763df2":"code","effaac91":"code","3d5d6c41":"code","d6aeee72":"code","efd957bb":"code","95c6f680":"code","f00e4011":"code","fd64ccb6":"code","da6886d2":"code","fa8792a0":"code","5692312c":"code","9cbe1606":"code","ff05eb50":"code","3b648b1a":"code","03be2e2d":"code","5797cf61":"code","67910438":"code","e5ae0aca":"code","d028e742":"code","f15fec61":"code","88806550":"code","77d85fd9":"code","db73c115":"code","47286825":"code","a30091f0":"code","44998a87":"code","0cf00710":"code","d5e915a9":"code","029613c9":"code","5a57b9c3":"code","4ebde4e3":"code","677aaaf0":"code","92336438":"code","4f7e80ec":"code","e3635a3d":"code","dbd62e8f":"code","ba217535":"code","358b1858":"code","817c46d2":"code","69b1bd67":"code","2bae5b2d":"code","b52adc02":"code","ac909b38":"code","13ab89e3":"code","ce89d9cb":"code","eb376d44":"code","d4137f7c":"code","ae8aa775":"code","45a74d81":"code","23a7c06f":"code","c254a939":"code","c961236b":"markdown","be1fff5f":"markdown","27c0852a":"markdown","3f332495":"markdown","169690c5":"markdown","48936dca":"markdown","34a262bb":"markdown","5475b90e":"markdown","e5982cc7":"markdown","bdb548c5":"markdown","a6607323":"markdown","8c692931":"markdown","bb3fb8c0":"markdown","b800bbc9":"markdown","d76034cc":"markdown","c6490202":"markdown","a3e7fae3":"markdown","393250ad":"markdown","882879c1":"markdown","cbfc2dc0":"markdown","bfda2a67":"markdown","463b4932":"markdown","8463038a":"markdown","0a5fdd52":"markdown","e1a7c22f":"markdown","8435cabb":"markdown","97d7e527":"markdown","6be38fb2":"markdown","4cb13fa6":"markdown","f80477f8":"markdown","65d46d36":"markdown","956a2843":"markdown","4896b9c5":"markdown","7c0dcef6":"markdown","e1c0b108":"markdown","5e5e1fc5":"markdown","3a87ad66":"markdown","ba92bd89":"markdown","98f67fbc":"markdown"},"source":{"3e76933b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport warnings\nfrom scipy import stats\nwarnings.filterwarnings('ignore')","06763df2":"df = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')","effaac91":"## Creating a new column for Classification\ndf['Chance of Admit Class'] = df['Chance of Admit '].apply(lambda x:1 if x>0.80 else 0)\ndf.head()","3d5d6c41":"## Shape of data\nprint(' Shape of Data \\n Rows :',df.shape[0],', Columns : ',df.shape[1])","d6aeee72":"## Checking for null values\nmissing_values = df.isnull().sum() * 100\/len(df)\nmissing_values_df = pd.DataFrame({'Column_name':df.columns,'Missing_percent':missing_values})\nmissing_values_df","efd957bb":"## Lets see the information about the data \nprint('Information for the data ')\ndf.info()","95c6f680":"## Replacing columns names as some of them has extra spaces.\ndf = df.rename(columns = {'LOR ':'LOR','Chance of Admit ':'Chance of Admit'})","f00e4011":"# lets see the distribution for the target variable\nprint('Skewness of chance of admit : ',df['Chance of Admit'].skew())\nplt.figure(figsize = (10,5))\nsns.distplot(df['Chance of Admit'],kde = True,color = 'g',fit = stats.norm)\nplt.show()","fd64ccb6":"fig, ax = plt.subplots(figsize=(15,6))\nrs = stats.probplot(df['Chance of Admit'],plot = ax)\nplt.show()","da6886d2":"### Research or no Research\n\ndf.Research.value_counts().plot.barh(color = 'g')\nplt.title('Research Vs No Research')\nplt.ylabel('Counts')\nplt.show()","fa8792a0":"print('No. of students who had done Research out of ',df['Research'].count(),' :',df[df['Research'] == 1].count()[1])\nprint('No. of students who hadnt done Research out of ',df['Research'].count(),' :',df[df['Research'] == 0].count()[1])","5692312c":"print('This implies the avg\/min\/max score by Students who has done research or not for GRE out of ',df['GRE Score'].max())\nprint(pd.pivot_table(df,columns = 'Research',values = 'GRE Score',aggfunc=['mean','min','max']))\npd.pivot_table(df,values='GRE Score',index=df['Research'],aggfunc=[\"mean\",'max']).plot.barh(alpha=0.4,figsize = (13,6))\nplt.title('Mean and Max GRE Scores')\nplt.show()","9cbe1606":"print('This implies the avg\/min\/max score by Students who has done research or not for TOEFL out of ',df['TOEFL Score'].max())\nprint(pd.pivot_table(df,columns = 'Research',values = 'TOEFL Score',aggfunc=['mean','min','max']))\n\npd.pivot_table(df,values='TOEFL Score',index=df['Research'],aggfunc=[\"mean\",'max']).plot.barh(alpha=0.4,figsize = (13,6))\nplt.title('Mean and Max TOEFL Scores')\nplt.show()","ff05eb50":"print('This implies the avg\/min\/max score by Students who has done research or not for SOP out of ',df['SOP'].max())\nprint(pd.pivot_table(df,columns = 'Research',values = 'SOP',aggfunc=['mean','min','max']))\n\npd.pivot_table(df,values='SOP',index=df['Research'],aggfunc=[\"mean\",'max']).plot.barh(alpha=0.4,figsize = (13,6))\nplt.title('Mean and Max SOP Scores')\nplt.show()","3b648b1a":"print('This implies the avg\/min\/max score by Students who has done research or not for LOR out of ',df['LOR'].max())\npd.pivot_table(df,columns = 'Research',values = 'LOR',aggfunc=['mean','min','max'])\n\npd.pivot_table(df,values='LOR',index=df['Research'],aggfunc=[\"mean\",'max']).plot.barh(alpha=0.4,figsize = (13,6))\nplt.title('Mean and Max LOR Scores')\nplt.show()","03be2e2d":"pd.pivot_table(df,columns ='Research' ,values = 'CGPA',aggfunc = ['mean','max','min'])","5797cf61":"colss = ['GRE Score','TOEFL Score','SOP','LOR']\nfor col in colss:\n    mean = pd.pivot_table(df,values=col,index=df['Research'],aggfunc=\"mean\")\n    ax = mean.plot.barh(alpha=0.4,color = 'y')\nplt.show()","67910438":"num_cols = ['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA']\nprint('Those Plots indicates distribution for every feature')\nfig, ax  = plt.subplots(2,3,figsize = (10,5))\ncount = 0\nfor i in range(2):\n    for j in range(3):\n        s = num_cols[count+j]\n        sns.distplot(df[s].values,ax = ax[i][j],fit = stats.norm,color = 'g')\n        ax[i][j].set_title(s,fontsize = 15)\n        fig = plt.gcf()\n        fig.set_size_inches(15,10)\n        plt.tight_layout()\n    count = count+j+1         ","e5ae0aca":"fig, ax  = plt.subplots(2,3,figsize = (10,5))\ncount = 0\nfor i in range(2):\n    for j in range(3):\n        s = num_cols[count+j]\n        sns.boxplot(df[s].values,ax = ax[i][j],color = 'c')\n        ax[i][j].set_title(s,fontsize = 15)\n        fig = plt.gcf()\n        fig.set_size_inches(15,10)\n        plt.tight_layout()\n    count = count+j+1 ","d028e742":"print('Data points of Gre score with target variable')\nf, ax = plt.subplots(1,2,figsize = (15,5))\nsns.scatterplot(x = 'GRE Score',y = 'Chance of Admit',data = df,ax = ax[0])\nsns.scatterplot(x = 'GRE Score',y = 'Chance of Admit',hue = 'Research',data = df,ax = ax[1],palette = 'colorblind')\nplt.show()","f15fec61":"print('Data points of TOEFL score with target variable')\nf, ax = plt.subplots(1,2,figsize = (15,5))\nsns.scatterplot(x = 'TOEFL Score',y = 'Chance of Admit',data = df,ax = ax[0])\nsns.scatterplot(x = 'TOEFL Score',y = 'Chance of Admit',hue = 'Research',data = df,ax = ax[1],palette = 'dark')\nplt.show()","88806550":"print('Data points of CGPA with target variable')\nf, ax = plt.subplots(1,2,figsize = (15,5))\nsns.scatterplot(x = 'CGPA',y = 'Chance of Admit',data = df,ax = ax[0])\nsns.scatterplot(x = 'CGPA',y = 'Chance of Admit',hue = 'Research',data = df,ax = ax[1],palette = 'bright')\nplt.show()","77d85fd9":"print('The detailed Statistical Summary for this data :')\ndf[num_cols].describe().T","db73c115":"### Probability Distribution and the bar plot of CGPA \nprint('The worst, best, and average of the CGPA Scores')\ny = np.array([df[\"CGPA\"].min(),df[\"CGPA\"].mean(),df[\"CGPA\"].max()])\nx = [\"Worst\",\"Average\",\"Best\"]\nplt.figure(figsize = (10,7))\nplt.bar(x,y,color = 'g')\nplt.title(\"CGPA Scores\")\nplt.xlabel(\"Level\")\nplt.ylabel(\"CGPA Score\")\nplt.show()","47286825":"### Probability Distribution and the bar plot of GRE Score \nprint('The worst, best, and average of the GRE Scores')\ny = np.array([df[\"GRE Score\"].min(),df[\"GRE Score\"].mean(),df[\"GRE Score\"].max()])\nx = [\"Worst\",\"Average\",\"Best\"]\nplt.figure(figsize = (10,7))\nplt.bar(x,y,color = 'y')\nplt.title(\"GRE Score\")\nplt.xlabel(\"Levels of betterness\")\nplt.ylabel(\"GRE Score\")\nplt.show()\n","a30091f0":"### Probability Distribution and the bar plot of TOEFL Score\nprint('The worst, best, and average of the TOEFL Scores')\ny = np.array([df[\"TOEFL Score\"].min(),df[\"TOEFL Score\"].mean(),df[\"TOEFL Score\"].max()])\nx = [\"Worst\",\"Average\",\"Best\"]\nplt.figure(figsize = (10,7))\nplt.bar(x,y,color = 'y')\nplt.title(\"TOEFL Score\")\nplt.xlabel(\"Levels of betterness\")\nplt.ylabel(\"TOEFL Score\")\nplt.show()","44998a87":"s = df[df[\"Chance of Admit\"] >= 0.75][\"University Rating\"].value_counts().head(5)\nplt.title(\"University Ratings of Candidates with an 75% acceptance chance\")\ns.plot(kind='bar',figsize=(18, 8),color = 'g')\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.show()","0cf00710":"f, ax  = plt.subplots(1,2,figsize = (18,8))\n\nsns.countplot(df['University Rating'],ax = ax[0],palette = 'bright')\nax[0].set_title('University Rating Counts')\n\nax[1].pie(df['University Rating'].value_counts(),\n          labels = df['University Rating'].value_counts().index,autopct = '%1.1f',\n         explode = [0,0,0,0,0.3])\nax[1].set_title('Percent Proportion of each Rating')            \nplt.show()","d5e915a9":"cmap = plt.get_cmap(\"tab20c\")\ninner_colors = cmap(np.array([1, 2, 5, 6, 9, 10]))\nf, ax = plt.subplots(1,2,figsize = (15,5))\nax[0].pie(df['Chance of Admit Class'].value_counts(),\n        labels = df['Chance of Admit Class'].value_counts().index,autopct = '%1.1f',\n       colors = inner_colors)\nax[0].set_title('Who gets the admit or not')\n\nsns.countplot(df['Chance of Admit Class'],palette = 'bright',ax = ax[1])\nax[1].set_title('Counts')\nplt.show()","029613c9":"sns.lineplot(x = 'GRE Score',y = 'Chance of Admit',data = df)","5a57b9c3":"line_cols = ['GRE Score','TOEFL Score','CGPA','SOP',\"LOR\",]\nfig, ax  = plt.subplots(2,2,figsize = (10,5))\ncount = 0\nfor i in range(2):\n    for j in range(2):\n        s = line_cols[count+j]\n        sns.lineplot(df[s].values,y = df['Chance of Admit'],\n                     hue = 'Research',data = df,ax = ax[i][j])\n        ax[i][j].set_title(s,fontsize = 15)\n        fig = plt.gcf()\n        fig.set_size_inches(15,10)\n        plt.tight_layout()\n    count = count+j+1  ","4ebde4e3":"## Lets look towards the bias of the data\nplt.figure(figsize = (15,5))\nplt.subplot(131)\nsns.violinplot(df['GRE Score'],orient = 'v',color = 'g')\n\nplt.subplot(133)\nsns.violinplot(df['TOEFL Score'],orient = 'v',color = 'y')\n\nplt.subplot(132)\nsns.violinplot(df['SOP'],orient = 'v',color = 'm')","677aaaf0":"sns.pairplot(df,hue = 'Chance of Admit Class',palette = 'bright')\nplt.show()","92336438":"### Lets check the correlation \nplt.figure(figsize = (18,8))\nsns.heatmap(df.corr(),annot = True,cbar = False,cmap = 'YlGnBu',mask = np.triu(df.corr(),1))\nplt.show()\n","4f7e80ec":"features = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n       'LOR', 'CGPA', 'Research']\nX = df[features]\ny = df['Chance of Admit']","e3635a3d":"import statsmodels.api as sm ","dbd62e8f":"Xc = sm.add_constant(X)\nlin_reg = sm.OLS(y,Xc).fit()\nlin_reg.summary()","ba217535":"X = X.drop(['SOP','University Rating'],1)\nXc = sm.add_constant(X)\nlin_reg = sm.OLS(y,Xc).fit()\nlin_reg.summary()","358b1858":"from sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","817c46d2":"X = df[features]\ny = df['Chance of Admit']\ntrainX ,testX , trainY, testY = train_test_split(X, y, train_size = 0.7,random_state = 5) ","69b1bd67":"lin_reg = LinearRegression()\npredY = lin_reg.fit(trainX,trainY).predict(testX)\nm1 = r2_score(testY,predY)\nprint('Accuracy\/RSquared : ',r2_score(testY, predY))\nprint('Root Mean Squared Error : ',np.sqrt(mean_squared_error(testY,predY)))","2bae5b2d":"dec_tree = DecisionTreeRegressor()\npredY = dec_tree.fit(trainX,trainY).predict(testX)\nm2 = dec_tree.score(testX,testY)\nprint('Accuracy : ',dec_tree.score(testX, testY))\nprint('Root Mean Squared Error : ',np.sqrt(mean_squared_error(testY,predY)))\n\n","b52adc02":"ran_f = RandomForestRegressor(n_estimators = 100)\npredY = ran_f.fit(trainX,trainY).predict(testX)\nm3 = ran_f.score(testX, testY)\nprint('Accuracy : ',ran_f.score(testX, testY))\nprint('Root Mean Squared Error : ',np.sqrt(mean_squared_error(testY,predY)))","ac909b38":"from sklearn.model_selection import GridSearchCV\nrf = RandomForestRegressor(n_jobs = -1)\nparam_grid = {\n                'n_estimators' : [50,100],\n                'max_depth' : [5,10,20],\n                'bootstrap' : [True,False],\n                'max_features' : ['auto','sqrt','log2']\n            }\ngrid = GridSearchCV(estimator=rf,param_grid=param_grid,cv = 5,n_jobs = -1)\ngrid.fit(trainX,trainY)\ngrid.best_params_","13ab89e3":"rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n                      max_features='log2', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n                      oob_score=False, random_state=None, verbose=0,\n                      warm_start=False) \npredY = rf.fit(trainX,trainY).predict(testX)\nm4 = rf.score(testX, testY)\nprint('Accuracy : ',rf.score(testX, testY))\nprint('Root Mean Squared Error : ',np.sqrt(mean_squared_error(testY,predY)))","ce89d9cb":"print('---------- Accuracy -----------')\nprint('Linear Regression : ',m1)\nprint('Decision Tree : ',m2)\nprint('Random Forest : ',m3)\nprint('Random Forest with Hyper Parameter Tuning : ',m4)","eb376d44":"features = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n       'LOR', 'CGPA', 'Research']\nX = df[features]\ny = df['Chance of Admit Class']","d4137f7c":"import scipy.stats as st\nXc = sm.add_constant(X)\nst.chisqprob = lambda chisq, Xc: st.chi2.sf(chisq, Xc)\ncols=Xc.columns[:-1]\nmodel=sm.Logit(y,Xc)\nresult=model.fit()\nresult.summary()\n","ae8aa775":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix","45a74d81":"trainX, testX, trainY, testY = train_test_split(X, y, train_size = 0.7,random_state = 7)","23a7c06f":"## Function to run models\ndef model(algo):\n    predY = algo.fit(trainX,trainY).predict(testX)\n    name = str(algo).split('(')[0]\n    print('\\n----------',name,'-----------')\n    print('Accuracy : ',accuracy_score(testY,predY))\n    print('Confusion Matrix \\n',confusion_matrix(testY,predY))\n#     sns.heatmap(confusion_matrix(testY,predY),annot = True,cmap = 'coolwarm')","c254a939":"lr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier(n_estimators=100)\nadb = AdaBoostClassifier()\n\nmodels = [lr,dt,rf,adb]\nfor algo in models:\n    model(algo)","c961236b":"Features Used : \n                   * 'GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n                  * 'LOR', 'CGPA', 'Research'\nTarget Variable : \n                   * 'Chance of Admit Class'","be1fff5f":"* So as we can see there are 5 Integers and 4 Floats\n* We dont have any null values and no data cleaning is required as of now\n* Lets move toward EDA","27c0852a":"### Statistical Approach","3f332495":"Observations: The university ranking with higher has low chances for admit if person falls below 0.80 percent and the university ranking with low can accept ","169690c5":"* We will go with ordinary least square method\n* Features used : \n        GRE Scores, TOEFL Scores, 'University Rating', 'SOP',\n       'LOR', 'CGPA', 'Research'\n* Target Variable  : Chance of Admit      ","48936dca":"![image.png](attachment:image.png)","34a262bb":"#### Observations :\nData Points are somewhat equally distributed in the GRE Score , TOEFL Score, CGPA with respect to target variable    ","5475b90e":"#### Decision Tree Regressor","e5982cc7":"* Overall  \n* GRE Score Highest = 340, Lowest = 290\n* TOEFL Score HIghest = 120, Lowest = 92\n* SOP Highest = 5.0, Lowest = 1.0\n* LOR Highest = 5.0, Lowest = 1.0\n* CGPA Highest = 9.92, Lowest = 6.8","bdb548c5":"Approaches :\n    * Data Cleaning\n    * Exploratory Data Analysis : Data Visualisations\n    * Regression : Statistical Approach and Machine Learning\n    * Classification : Statistical and Machine Learning\n        \n        ","a6607323":"Observation : \n* GRE Scores and chance of admits were higher of researchers .\n* Toefl Scores and chance of admits were higher for those students who had done research\n* Top scorers in CGPA with chance of admits are higher for those who had done research\n* Top scorers who has done research have higher SOP","8c692931":"## Admission for Masters Prediction","bb3fb8c0":"Observation :\n* GRE Score was having high positive correlation with target variable\n","b800bbc9":" For classification problem we are creating a new target varaible 'chance of admit class' as If the chance of admit is greater than 80%, the candidate will receive the \u201c1\u201d label. If the chance of admit is lesser than 80%, the candidate will receive the \u201c0\u201d label. ","d76034cc":"#### Machine Learning Approach","c6490202":"## Lets take a look at data","a3e7fae3":"#### Descriptive Statistics","393250ad":"* Observation : Students who has done research are more in numbers","882879c1":"As we can see we get R Squared value as 0.822\nBut we can see p values of SOP and University were lesser that the confidence value or lesser than than the p values(0.5)\nWe will try dropping those features and then try again.","cbfc2dc0":"* Observation : Probplot calculates a best fit line for all the data points. As we can see only some point at the end are little bit far.","bfda2a67":"#### Machine Learning Approach","463b4932":"So we have done some \n* Data Exploration\n* Data Vizualization\n* Regression (Target Variable : Chance of Admit)\n    * Statistical\n    * Machine Learning\n        * Linear Regression\n        * Decision Tree\n        * Random Forest\n* Classification (Chance of Admit Class)\n    * Statistical\n    * Machine Learning\n        * Logistic Regression\n        * Decision Tree\n        * Random Forest\n        * AdaBoost","8463038a":"Observations : \n* Chance of admit class is students who gets the admit who has score higher than 0.80\n* So there are less number of students getting admit which is barely 28.4","0a5fdd52":"Observation :\n* Now we can see that the Rsquare is still 0.82\n* That means SOP and University Rating werent affecting anything\n* And also the pvalues are lesser than the confidence level.","e1a7c22f":"#### Statistical Approach","8435cabb":"###### So through the above data frame we can see there arent any null values.","97d7e527":"### Classification","6be38fb2":" Observations:\n1. With this plot we can check skewness for each variable .\n2. GRE and Toefl score were normally distributed .\n3. SOP was a litlle left skewed .\n4. CGPA was normally distributed .","4cb13fa6":"##### Observations: We can see df[chance of admit] lies between -1 to 1 . Its value is :-0.28. We dont Require any transformations on the data","f80477f8":"#### Observations:\n    . We can see only LOR has outliers.","65d46d36":"### Exploratory Data Analysis","956a2843":"### Regression","4896b9c5":"#### HyperParamtermer Tuning on Random Forest","7c0dcef6":"The dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are : \n1. GRE Scores ( out of 340 ) \n2. TOEFL Scores ( out of 120 ) \n3. University Rating ( out of 5 ) \n4. Statement of Purpose and Letter of Recommendation Strength ( out of 5 ) \n5. Undergraduate GPA ( out of 10 ) \n6. Research Experience ( either 0 or 1 ) \n7. Chance of Admit ( ranging from 0 to 1 )\n8. Chance of Admit Class* : binary 0 and 1\n* it was created for classification where chance of admit > 0.80 = 1 else 0\nThis is based on Indian Perspective\n\n* For Regression : Target Variable (Continuous) : Chance of admit\n* For Classifcation : Target Variable(Categorical) : Chance of Admit Class","e1c0b108":"Observations :\n* Random Forest Classifier gave the best result with Accuracy : 0.94","5e5e1fc5":"#### Random Forest Regressor","3a87ad66":"### Model Building \n* First we will go with the statistical approach\n        * Ordinary Least Squares (OLS) for regression\n        * \n* We will perform Regression using Target Variable as Chance of Admit [0 - 100]\n        * We will try many different machine learning models\n            * Linear Regression\n            * Decision Tree\n            * Random Forest\n            * AdaBoost Regressor\n* Then will perform Classification using target variable as Chance of Admit Class ('0','1')  \n        * We will try many different machine learning models\n            * Logistic Regression\n            * Decision Tree Classifier\n            * Random Forest Classifier\n            * AdaBoost Classifier","ba92bd89":"Observations : \n* Through Statistical approach we got R2 score of 0.82\n* Through Machine Learning Approach\n    * Linear Regression : 0.77\n    * Decision Tree : 0.43\n    * Random Forest : 0.73\n* Random Forest with Hyperparameter Tuning\n    * Accuracy : 0.76\n        \n* So linear Regression gave the best results       ","98f67fbc":"##### Linear Regression"}}