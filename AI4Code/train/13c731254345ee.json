{"cell_type":{"da569cac":"code","3ef72e31":"code","bb408243":"code","393a3724":"code","7c089dff":"code","0095a71b":"code","3297b8ff":"code","e8813554":"code","565d29e9":"code","d0fd5275":"code","1f2b4f3c":"code","94a546c5":"code","4c1f824e":"code","01eef1e2":"code","d5496391":"code","83c49cab":"code","7b25f3a2":"code","eab6a9ff":"markdown","668b10e6":"markdown","f0b41f38":"markdown","121d992a":"markdown","24bd615c":"markdown","2eb2b16a":"markdown","7e832473":"markdown","e4ff2de7":"markdown","dd2374a6":"markdown","71570830":"markdown","07bd7200":"markdown","0d886f5e":"markdown","0e433d01":"markdown","09aec4b6":"markdown","a59b29cf":"markdown"},"source":{"da569cac":"import numpy as np\nimport pandas as pd","3ef72e31":"train_df=pd.read_csv('\/kaggle\/input\/fraud-prediction\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/fraud-prediction\/test.csv')\ndf_desc=pd.read_csv('\/kaggle\/input\/fraud-prediction\/column_Desc.csv')","bb408243":"from xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\nimport datetime","393a3724":"def uniquelo(df):\n    col=[]\n    vals=[]\n    for a in df.columns:\n        col.append(a)\n        vals.append(len(df[a].unique()))\n    percent_missing = df.isnull().sum() * 100 \/ len(df)\n    d = {'Unique_val': vals,'percent_missing': percent_missing, 'Data_Types' :df.dtypes.values}\n    return pd.DataFrame(data=d)","7c089dff":"out=uniquelo(train_df)\n# out.sort_values('percent_missing', inplace=True,ascending=False)\nout['Column Info']=df_desc.Description.values\nout","0095a71b":"train_df['BIRTHDT'] = pd.to_datetime(train_df['BIRTHDT'], format='%Y-%m-%d')\nl = [(datetime.datetime.now()-i).days\/365 for i in train_df['BIRTHDT']]\nl = [int(i) if np.isnan(i)==False else np.nan for i in l]\ntrain_df['AGE'] = l","3297b8ff":"test_df['BIRTHDT'] = pd.to_datetime(test_df['BIRTHDT'], format='%Y-%m-%d')\nl = [(datetime.datetime.now()-i).days\/365 for i in test_df['BIRTHDT']]\nl = [int(i) if np.isnan(i)==False else np.nan for i in l]\ntest_df['AGE'] = l","e8813554":"train_df.drop(['BIRTHDT','REPORTEDDT','LOSSDT','N_PAYTO_NAME_cleaned_root','Prov_Name_All_final_root','INSUREDNA_cleaned_root',\n                   'N_REFRING_PHYS_final_root','CLMNT_NA_cleaned_root','N_PRVDR_NAME_NONPHYS_cleaned_root'],axis=1,inplace=True)\n\ntest_df.drop(['BIRTHDT','REPORTEDDT','LOSSDT','N_PAYTO_NAME_cleaned_root','Prov_Name_All_final_root','INSUREDNA_cleaned_root',\n                   'N_REFRING_PHYS_final_root','CLMNT_NA_cleaned_root','N_PRVDR_NAME_NONPHYS_cleaned_root'],axis=1,inplace=True)","565d29e9":"uniquelo(train_df)","d0fd5275":"for a in train_df.columns:\n    if (train_df[a].dtype!='object') and (a!='TARGET'):\n        train_df[a].fillna(train_df[a].mean(),inplace=True)\n        test_df[a].fillna(train_df[a].mean(),inplace=True)\n        \n    elif (a=='TARGET') or (a=='CLAIMNO'):\n        continue\n    \n    else:\n        train_df[a].fillna(train_df[a].mode()[0],inplace=True)\n        test_df[a].fillna(train_df[a].mode()[0],inplace=True)","1f2b4f3c":"y = train_df['TARGET']\ntrain_df.drop(['TARGET'],axis=1,inplace=True)","94a546c5":"x = pd.DataFrame()\ntest = pd.DataFrame()\n\nfor i in train_df.columns:\n    if (train_df[i].dtype!='object') or (i=='CLAIMNO'):\n        continue\n    \n    else:\n        train_dummy = pd.get_dummies(train_df[i])\n        test_dummy = pd.get_dummies(test_df[i])\n#         if(a.columns[0] in (b.columns)):\n#             b.drop(a.columns[0],axis=1,inplace=True)\n#         a.drop(a.columns[0],axis=1,inplace=True)\n        missing_test = set(train_dummy.columns)-set(test_dummy.columns)\n#         train_miss = set(b.columns)-set(a.columns)\n\n        for k in missing_test:\n            test_dummy[k] = 0\n            \n        for p in train_dummy.columns:\n            #print(j)\n            x[i+'_'+str(p)] = train_dummy[p]\n            test[i+'_'+str(p)] = test_dummy[p]","4c1f824e":"from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn import model_selection","01eef1e2":"clf = RandomForestClassifier(random_state=42,verbose=3, n_estimators=50)","d5496391":"model = BaggingClassifier(base_estimator = clf, n_estimators = 10, n_jobs=-1, verbose=3, random_state = 240)","83c49cab":"model.fit(x,y)","7b25f3a2":"l = model.predict_proba(test)[:,1]\n\nclaim_no = test_df['CLAIMNO']\n\noutput = pd.DataFrame(columns=['CLAIMNO','TARGET'])\n\noutput['CLAIMNO'] = claim_no\n\noutput['TARGET'] = l\n\noutput.to_csv('For_Kernel3.csv',index=False)\n\noutput","eab6a9ff":"### Here's the final output:","668b10e6":"### My eventual output was a weighted ensemble of other techniques too, but here just giving some base model as output.","f0b41f38":"### There are lot of variables which aren't much useful, with too many unique values or too many missing values. So they are not useful for our classification problem. We will also create some derived values using information already present in our other columns.\n\n### Let's start","121d992a":"### Now we shall drop columns which either have too many unique values or too many missing values or might not be useful for our classification","24bd615c":"### Now filling null values:","2eb2b16a":"#### The k-fold I didn't perform here:\nkfold = model_selection.KFold(n_splits = 3, random_state = 240)\n\nresults = model_selection.cross_val_score(model, x, y, cv = kfold, verbose=3, n_jobs=-1)","7e832473":"### Since there was no reference of using with to find distance, I first removed the variables and then fit the model, but using them in the model somehow gave better results. So that's why keeping them.","e4ff2de7":"### I've already performed Cross-Validation on Colab and since it was k-fold(5), it consumed whole RAM, so just mentioning the k-fold cv without using it and also reducing the number of estimators while performing the rest of the modelling part and giving the output below:\n\n### I used RandomForest with Bagging","dd2374a6":"### Let's check our dataset now:","71570830":"#### First we create Age Column:","07bd7200":"### Importing the train and test data","0d886f5e":"### Let's see the column info and it's information about missing value and unique values:","0e433d01":"### Now making our Target as Response Variable:","09aec4b6":"### We see below that there are some labels in some columns that are present in train dataset but not in test, and also vice-versa. So we will after dummyfying, create columns of these values and assign it 0, so that we get fixed set of columns in our both train and test data:","a59b29cf":"### Making dataset of explanatory variables, and dummyfying categorical ones:"}}