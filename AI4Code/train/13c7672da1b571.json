{"cell_type":{"6343a4fb":"code","d66b4dd7":"code","fa1fd271":"code","37ef9bfb":"code","bcd15bf2":"code","afdaeccc":"code","5daf7108":"code","e84e5b75":"code","60bd8d59":"markdown","f9d9d8b2":"markdown","8c4ab460":"markdown","a6d912e6":"markdown","7dac1f6a":"markdown","f10bf867":"markdown","611d2ff7":"markdown","e20336a9":"markdown"},"source":{"6343a4fb":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom xgboost import XGBRegressor\n\nX = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id') \nX_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8,\n                                                      test_size=0.2,\n                                                      random_state=0)","d66b4dd7":"print(f'There is a total of {len(X.columns)} features.')\n\ncols_with_missing = set([col for col in X.columns if X[col].isnull().any()])\ncols_with_missing.update([col for col in X_test.columns if X_test[col].isnull().any()])\nprint(f'There are {len(cols_with_missing)} features with missing entries.')\n\ncat = (X.dtypes == 'object')\ncategorical_cols = list(cat[cat].index)\nprint(f'There are {len(categorical_cols)} categorical features.')\n\nnum = (X.dtypes != 'object')\nnum_cols = list(num[num].index)\nprint(f'There are {len(num_cols)} numerical features.')","fa1fd271":"def apply_pca(X, standardize=True):\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\npca_features = [col for col in num_cols if not X[col].isnull().any()]\npca, X_pca, loadings = apply_pca(X[pca_features])\n\nprint('The first 10 PCA components explain these amounts of variance:')\nprint(pca.explained_variance_ratio_[:10])\n\nfig, axes = plt.subplots(2, 5, figsize=(12, 6))\n\nfor idx, ax in enumerate(axes.flat):\n    sns.boxenplot(data=X_pca,  y= f'PC{idx+1}', ax=ax)\n    ax.set_title(f'PC{idx+1}', fontsize=14)\n\nfig.supxlabel('First 10 PC components distribution in train set', ha='center')\nfig.tight_layout()\nplt.show()\nplt.close(fig)\n","37ef9bfb":"def drop_missing_values(data, data_test, cols_with_missing, verbose=False):\n    cols_to_drop = [col for col in cols_with_missing if data[col].isnull().sum() > missing_threshold]\n\n    if verbose:\n        print('Columns with missing values: ')\n        print(cols_with_missing)\n        print('\\n Columns which will be dropped due to a lot of missing values: ')\n        print(cols_to_drop)\n\n    data = data.drop(cols_to_drop, axis=1)\n    data_test = data_test.drop(cols_to_drop, axis=1)\n\n    return data, data_test, cols_to_drop\n\n\ndef impute_missing_values(X, X_test, cols_with_missing, fill_with_median):\n\n    data = X.copy()\n    data_test = X_test.copy()\n\n    cols_with_missing_num = [col for col in cols_with_missing if data[col].dtype != \"object\"]\n    cols_with_missing_cat = [col for col in cols_with_missing if data[col].dtype == \"object\"]\n\n    for column in cols_with_missing_num:\n        if column in fill_with_median:\n            data[column] = data[column].fillna(data[column].median())\n            data_test[column] = data_test[column].fillna(data_test[column].median())\n        else:\n            data[column] = data[column].fillna(0)\n            data_test[column] = data_test[column].fillna(0)\n\n    for column in cols_with_missing_cat:\n        data[column] = data[column].fillna('NA')\n        data_test[column] = data_test[column].fillna('NA')\n\n    data.columns = X.columns\n    data_test.columns = X_test.columns\n\n    return data, data_test\n\n\ndef remove_low_mi(X, y, X_test, mutual_inf_threshold, verbose=False):\n\n    data = X.copy()\n    data_test = X_test.copy()\n\n    num = (data.dtypes != 'object')\n    num_cols = list(num[num].index)\n\n    mi_scores = make_mi_scores(data[num_cols], y, 'auto')\n    unimportant_columns = [index for index, score in mi_scores.iteritems()\n                           if score < mutual_inf_threshold]\n    if verbose:\n        print('\\nColumns which will be dropped: ')\n        print(unimportant_columns)\n\n    high_mi_columns = list(set(num_cols) - set(unimportant_columns))\n    high_mi = data.drop(unimportant_columns, axis=1)\n    high_mi_test = data_test.drop(unimportant_columns, axis=1)\n\n    return high_mi, high_mi_test, high_mi_columns\n\n\ndef make_mi_scores(X, y, discrete_features):\n\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef create_pca_features(X, X_test, columns, pca_components_to_include):\n\n    data = X.copy()\n    data_test = X_test.copy()\n\n    pca, X_pca, loadings = apply_pca(data[columns], standardize=False)\n    pca_test, X_pca_test, loadings_test = apply_pca(data_test[columns], standardize=False)\n\n    first_n_components = [f\"PC{i+1}\" for i in range(pca_components_to_include)]\n    data_after_pca = data.join(X_pca[first_n_components])\n    data_after_pca_test = data_test.join(X_pca_test[first_n_components])\n\n    return data_after_pca, data_after_pca_test\n\n\ndef one_hot_encoding(X, X_test, categorical_cols, low_cardinality_threshold):\n\n    data = X.copy()\n    data_test = X_test.copy()\n\n    low_cardinality_cols = [col for col in categorical_cols\n                            if data[col].nunique() < low_cardinality_threshold]\n    high_cardinality_cols = list(set(categorical_cols)-set(low_cardinality_cols))\n\n    lc_X = data.drop(high_cardinality_cols, axis=1)\n    lc_X_test = data_test.drop(high_cardinality_cols, axis=1)\n\n    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    oh_cols = pd.DataFrame(OH_encoder.fit_transform(lc_X[low_cardinality_cols]))\n    oh_cols_test = pd.DataFrame(OH_encoder.transform(lc_X_test[low_cardinality_cols]))\n\n    oh_cols.index = X.index\n    oh_cols_test.index = X_test.index\n    num_X = data.drop(categorical_cols, axis=1)\n    num_X_test = data_test.drop(categorical_cols, axis=1)\n    oh_X = pd.concat([num_X, oh_cols], axis=1)\n    oh_X_test = pd.concat([num_X_test, oh_cols_test], axis=1)\n\n    return oh_X, oh_X_test\n\ndef ordinal_encoding(X, X_test, ordinal_cols, categories):\n    \n    data = X.copy()\n    data_test = X_test.copy()\n    \n    category_list = [categories for i in range(len(ordinal_cols))]\n\n    ORD_encoder = OrdinalEncoder(categories = category_list)\n    ord_cols = pd.DataFrame(ORD_encoder.fit_transform(data[ordinal_cols]), columns=ordinal_cols)\n    ord_cols_test = pd.DataFrame(ORD_encoder.transform(data_test[ordinal_cols]), columns=ordinal_cols)\n\n    ord_cols.index = X.index\n    ord_cols_test.index = X_test.index\n    num_X = data.drop(ordinal_cols, axis=1)\n    num_X_test = data_test.drop(ordinal_cols, axis=1)\n    ord_X = pd.concat([num_X, ord_cols], axis=1)\n    ord_X_test = pd.concat([num_X_test, ord_cols_test], axis=1)\n    \n    return ord_X, ord_X_test","bcd15bf2":"def preprocessing_pipeline(X, y, X_test,\n                           missing_threshold=100,\n                           mutual_inf_threshold=0.05,\n                           low_cardinality_threshold=15,\n                           pca_components_to_include=10,\n                           verbose=False):\n\n    data = X.copy()\n    data_test = X_test.copy()\n    if verbose:\n        print('\\nNumber of features at the beginning:')\n        print(len(data.columns), len(data_test.columns))\n\n    cols_with_missing = set([col for col in data.columns if data[col].isnull().any()])\n    cols_with_missing.update([col for col in data_test.columns if data_test[col].isnull().any()])\n    data, data_test, dropped_columns = drop_missing_values(data, data_test, cols_with_missing, verbose)\n    if verbose:\n        print('\\nNumber of features after dropping missing values:')\n        print(len(data.columns), len(data_test.columns))\n\n    cols_with_few_missing = list(set(cols_with_missing) - set(dropped_columns))\n    fill_with_median = ['LotFrontage']\n    imputed, imputed_test = impute_missing_values(data, data_test,\n                                                  cols_with_few_missing,\n                                                  fill_with_median)\n    msg = \"There are still NaN values in the data after imputation!\"\n    assert not [col for col in imputed.columns if imputed[col].isnull().any()], msg\n    assert not [col for col in imputed_test.columns if imputed_test[col].isnull().any()], msg\n    if verbose:\n        print('\\nNumber of features after imputation:')\n        print(len(imputed.columns))\n        print(len(imputed_test.columns))\n\n    high_mi, high_mi_test, high_mi_columns = remove_low_mi(imputed, y, imputed_test,\n                                                           mutual_inf_threshold, verbose)\n    if verbose:\n        print('\\nNumber of features after dropping low MI features:')\n        print(len(high_mi.columns))\n        print(len(high_mi_test.columns))\n\n    data_after_pca, data_after_pca_test = create_pca_features(high_mi, high_mi_test,\n                                                              high_mi_columns,\n                                                              pca_components_to_include)\n\n    if verbose:\n        print('\\nNumber of features after adding PCA components:')\n        print(len(data_after_pca.columns))\n        print(len(data_after_pca_test.columns))\n        \n    ordinal_features = ['ExterQual','ExterCond','BsmtQual','BsmtCond',\n                        'HeatingQC','KitchenQual','FireplaceQu','GarageQual',\n                        'GarageCond', 'PoolQC']\n    categories = ['NA','Po','Fa','TA','Gd','Ex']   #ordered by increasing value\n    \n    cat = (data_after_pca.dtypes == 'object')\n    categorical_cols = list(cat[cat].index)\n    ordinal_cols = [col for col in categorical_cols if col in ordinal_features]\n    non_ordinal_cols = [col for col in categorical_cols if not col in ordinal_features]\n    \n    OH_X, OH_X_test = one_hot_encoding(data_after_pca, data_after_pca_test,\n                                       non_ordinal_cols,\n                                       low_cardinality_threshold)    \n    if verbose:\n        print('\\nNumber of features after OHE:')\n        print(len(OH_X.columns))\n        print(len(OH_X_test.columns))\n    \n    ord_X, ord_X_test = ordinal_encoding(OH_X, OH_X_test, ordinal_cols, categories)\n    \n    if verbose:\n        print('\\nNumber of features after ordinal encoding:')\n        print(len(ord_X.columns))\n        print(len(ord_X_test.columns))\n        \n    return ord_X, ord_X_test","afdaeccc":"smallest_mae = 30_000\n\nfor missing_threshold in [100, 1000]:\n    for mutual_inf_threshold in [0.05, 0.1, 0.15]:\n        for low_cardinality_threshold in [40,]:\n            for pca_components_to_include in [0, 10]:\n                print(\n                    f'running: missing_threshold = {missing_threshold}, '\n                    f' mutual_inf_threshold = {mutual_inf_threshold}, '\n                    f'low_cardinality_threshold = {low_cardinality_threshold}, '\n                    f'pca_components_to_include = {pca_components_to_include}'\n                )\n\n                processed_X_train, processed_X_valid = preprocessing_pipeline(X_train, y_train, X_valid,\n                                                                              missing_threshold=missing_threshold,   \n                                                                              mutual_inf_threshold=mutual_inf_threshold,\n                                                                              low_cardinality_threshold=low_cardinality_threshold,\n                                                                              pca_components_to_include=pca_components_to_include)\n                \n                model = XGBRegressor(n_estimators=5_000, learning_rate = 0.01)\n                model.fit(processed_X_train, y_train, early_stopping_rounds=15,\n                          eval_set=[(processed_X_valid, y_valid)],verbose=False)\n                preds_test = model.predict(processed_X_valid)\n\n                mae = mean_absolute_error(y_valid, preds_test)\n                print(f'mae = {mae}')\n                print('-----------------')\n                if mae < smallest_mae:\n                    smallest_mae = mae\n                    best_params = missing_threshold, mutual_inf_threshold, low_cardinality_threshold, pca_components_to_include\n\nprint(f'smallest MAE: {smallest_mae}')\nprint(f'best parameters: {best_params}')","5daf7108":"processed_X, processed_X_test = preprocessing_pipeline(X, y, X_test, \n                                                       missing_threshold=best_params[0],      \n                                                       mutual_inf_threshold=best_params[1],\n                                                       low_cardinality_threshold=best_params[2])\n\nbest_score = 30_000\nfor max_depth in [2,4,6]:\n    for reg_alpha in [0, 0.5, 1]:\n        for min_child_weight in [1,2]:\n            xgb_params = dict(\n                max_depth=max_depth,                      # maximum depth of each tree - try 2 to 10\n                learning_rate=0.01,                       # effect of each tree - try 0.0001 to 0.1\n                n_estimators=1500,                        # number of trees (that is, boosting rounds) - try 1000 to 8000\n                min_child_weight=min_child_weight,        # minimum number of houses in a leaf - try 1 to 10\n                colsample_bytree=0.7,                     # fraction of features (columns) per tree - try 0.2 to 1.0\n                subsample=0.7,                            # fraction of instances (rows) per tree - try 0.2 to 1.0\n                reg_alpha=reg_alpha,                      # L1 regularization (like LASSO) - try 0.0 to 10.0\n                reg_lambda=1.0,                           # L2 regularization (like Ridge) - try 0.0 to 10.0\n                num_parallel_tree=1,                      # set > 1 for boosted random forests\n            )\n            \n            xgb = XGBRegressor(**xgb_params)\n            score = cross_val_score(xgb, processed_X, y, cv=5, scoring=\"neg_mean_absolute_error\")\n            score = -1 * score.mean()\n            if score < best_score:\n                best_score = score\n                best_xgb_params = xgb_params\n            print(score)\n            print('..............')\n\nprint(best_score)\nprint(best_xgb_params)","e84e5b75":"processed_X, processed_X_test = preprocessing_pipeline(X, y, X_test, \n                                                       missing_threshold=best_params[0],      \n                                                       mutual_inf_threshold=best_params[1],\n                                                       low_cardinality_threshold=best_params[2])\nprint(len(processed_X.columns), len(processed_X_test.columns))\n\nmodel = XGBRegressor(**best_xgb_params)\nmodel.fit(processed_X, y)\npreds_test = model.predict(processed_X_test)\n\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)\nprint('output saved')","60bd8d59":"Finally, the best parameters will be used to the generate predictions on the entire training set.","f9d9d8b2":"Check for outliers by using PCA and visualising first 10 components, which explain most of the variance in the data.","8c4ab460":"Find best XGBRegressor parameters.","a6d912e6":"Below, I define the function which will preprocess data. It takes as arguments: \n- a train set `X` \n- a test\/validation set `X_test` \n- the target output as `y`.  \n\nIt works on copies of the `X` and `X_test` data, and returns these two datasets after completing all preprocessing steps, which include:\n- removing columns which have more than `missing_threshold` missing entries,\n- imputing missing data in the rest of the numerical columns with missing data using the median value or 0,\n- imputing missing data in categorical columns with a new value (often the NaN in categorical data has the meaning of a new category, e.g. \"the absence of a garage\") \n- removing features have mutual information with the target smaller than `mutual_inf_threshold`,\n- creates new features by applying PCA to existing numerical features and then adding as new features the first `pca_components_to_include` components\n- dropping categorical data that have cardinality larger than `low_cardinality_threshold`,\n- performing One Hot Encoding on the low-cardinality categorical data.\n","7dac1f6a":"This notebook is based entirely on the knowledge which I gained from doing the Machine Learning courses on Kaggle and reading sklearn\/pandas documentation.  \n\nI tried here to create an automated pipeline for feature preparation, which does:\n- remove numerical columns with a large amount of missing data,\n- impute missing data in categorical columns and in the numerical columns which have most entries and a small part of missing data,\n- removes features have small mutual information with the target,\n- creates new features by applying PCA to numerical features,\n- performs One Hot Encoding on the categorical data.\n\nNext, the preprocessed data is fed into a simple XGBRegressor model, which can be run with early stopping rounds, when performed on the train\/validations sets.\n\n","f10bf867":"Next, four loops are run to find the optimal values of the hyperparameters of the preprocessing pipeline. \nNote that XGBRegressor is run with early stopping rounds to speed up the evaluation.","611d2ff7":"First, analyze general facts about the data (if there are missing values and what types of data occur).","e20336a9":"The last thing which remains would be optimize the XGBRegressor parameters, e.g. using GridSearchCV."}}