{"cell_type":{"45d9a7c5":"code","b6d84bb1":"code","37311f89":"code","60ea08b0":"code","4760a105":"code","68baf88a":"code","7bd61d64":"code","1f21dce5":"code","dfa3df5d":"code","912573a4":"code","1017b95b":"code","3d84dcf3":"code","48a954b5":"code","aa0a12fa":"code","14acb4b4":"code","1e9880b0":"code","d8b5d2b1":"code","de406457":"code","8e6f3c35":"markdown"},"source":{"45d9a7c5":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom xgboost import cv\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 12})","b6d84bb1":"train = pd.read_csv(\"..\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/train.csv\")\ntest = pd.read_csv(\"..\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/test.csv\")","37311f89":"test.drop(columns=\"ID\", inplace=True)\ntest[\"TARGET\"] = 2","60ea08b0":"all_data = pd.concat([train,test],axis=0)\nall_data.drop(columns=[\"TXN_TRM\",\"MC_NAME\"], inplace=True)\ncat_cols = ['CST_NR', 'CC_NR', 'DAY_OF_MONTH',\n       'TXN_SOURCE', 'TXN_ENTRY', 'CITY', 'COUNTRY',\n   'MC_ID', 'MCC_CODE']","4760a105":"all_data[cat_cols] = all_data[cat_cols].fillna(\"missing\")","68baf88a":"# Label encoding categorical features\nfrom sklearn import preprocessing\nfor col in cat_cols:\n    print(col)\n    le = preprocessing.LabelEncoder()\n    all_data[col] = le.fit_transform(all_data[col].values)","7bd61d64":"all_data.drop(columns = [\"CC_NR\",\"CST_NR\", 'MC_ID', 'MCC_CODE',\"DAY_OF_WEEK\",\"DAY_OF_MONTH\",\"CITY\",\"COUNTRY\",\"TXN_TIME\",\"TXN_SOURCE\",\"TXN_ENTRY\"],  inplace=True)","1f21dce5":"all_data['TXN_AMNT'] = (all_data['TXN_AMNT'] -all_data['TXN_AMNT'].mean()) \/ all_data['TXN_AMNT'].std()    \n ","dfa3df5d":"#df_z_scaled.plot(kind='bar')","912573a4":"all_data['TXN_AMNT'] = StandardScaler().fit_transform(all_data['TXN_AMNT'].values.reshape(-1, 1))","1017b95b":"from sklearn.preprocessing import MinMaxScaler\n# apply normalization techniques\nall_data['TXN_AMNT'] = MinMaxScaler().fit_transform(np.array(all_data['TXN_AMNT']).reshape(-1,1))\nall_data['TXN_AMNT'].value_counts(normalize=True)","3d84dcf3":"# Splitting back to train and test\ntest = all_data.iloc[len(train):]\ntrain = all_data.iloc[:len(train)]\ntrain.shape,test.shape","48a954b5":"train.describe().T","aa0a12fa":"test.describe().T","14acb4b4":"train = train.drop(['TARGET'], axis=1)\ntest = test.drop(['TARGET'], axis=1)","1e9880b0":"# select only the numerical features\nX_test  = test.select_dtypes(include=['number']).copy()\nX_train = train.select_dtypes(include=['number']).copy()\n\n#X_test = X_test*1.415\n\n# add the train\/test labels\nX_train[\"AV_label\"] = 0\nX_test[\"AV_label\"]  = 1\n\n# make one big dataset\nall_data = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n\n# shuffle\nall_data_shuffled = all_data.sample(frac=1)\n\n# create our DMatrix (the XGBoost data structure)\nX = all_data_shuffled.drop(['AV_label'], axis=1)\ny = all_data_shuffled['AV_label']\nXGBdata = xgb.DMatrix(data=X,label=y)\n\n# our XGBoost parameters\nparams = {\"objective\":\"binary:logistic\",\n          \"eval_metric\":\"logloss\",\n          'learning_rate': 0.05,\n          'max_depth': 5, }\n\n# perform cross validation with XGBoost\ncross_val_results = cv(dtrain=XGBdata, params=params, \n                       nfold=5, metrics=\"auc\", \n                       num_boost_round=200,early_stopping_rounds=20,\n                       as_pandas=True)\n\n# print out the final result\nprint((cross_val_results[\"test-auc-mean\"]).tail(1))","d8b5d2b1":"classifier = XGBClassifier(eval_metric='logloss',use_label_encoder=False)\nclassifier.fit(X, y)\nfig, ax = plt.subplots(figsize=(12,4))\nplot_importance(classifier, ax=ax)\nplt.show();","de406457":"classifier = XGBClassifier(eval_metric='logloss',use_label_encoder=False)\nclassifier.fit(X, y)\nfig, ax = plt.subplots(figsize=(12,10))\nplot_importance(classifier, ax=ax)\nplt.show();","8e6f3c35":"# What is Adversarial Validation?\nThe objective of any predictive modelling project is to create a model using the training data, and afterwards apply this model to the test data. However, for the best results it is essential that the training data is a representative sample of the data we intend to use it on (*i.e.* the test data), otherwise our model will, at best, under-perform, or at worst, be completely useless.   \n\n***Adversarial Validation*** is a very clever and very simple way to let us know if our test data and our training data are similar; we combine our `train` and `test` data, labeling them with say a `0` for the training data and a `1` for the test data, mix them up, then see if we are able to correctly re-identify them using a binary classifier.\n\nIf we cannot correctly classify them, *i.e.* we obtain an area under the [receiver operating characteristic curve](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) (ROC) of 0.5 then they are indistinguishable and we are good to go.\n\nHowever, if we can classify them (ROC > 0.5) then we have a problem, either with the whole dataset or more likely with some features in particular, which are probably from  different distributions in the test and train datasets.\nIf we have a problem, we can look at the feature that was most out of place. The problem may be that there were values that were only seen in, say, training data, but not in the test data. If the contribution to the ROC is very high from one feature, it may well be a good idea to remove that feature from the model.\n\n\n## Adversarial Validation to reduce overfitting\nThe key to avoid overfitting is to create a situation where the local cross-vlidation (CV) score is representative of the competition score. When we have a ROC of 0.5 then your local data is representative of the test data, thus your local CV score should now be representative of the Public LB score.\n\nProcedure:\n\n* drop the training data target column \n* label the `test` and `train` data with `0` and `1` (it doesn't really matter which is which)\n* combine the training and test data into one big dataset\n* perform the binary classification, for example using XGboost\n* look at our AUC ROC score\n"}}