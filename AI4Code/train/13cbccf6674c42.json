{"cell_type":{"15ef042c":"code","16b48872":"code","4bf3623a":"code","2976d2dd":"code","0e5e25fa":"code","f99e184b":"code","c0db0b07":"code","6271c7df":"code","84ce6b28":"code","ed3cbfac":"code","c27db926":"code","268fde4a":"code","06889b6f":"code","c6178500":"code","5a5fc041":"code","7de7a5b9":"code","f94435bb":"code","ad7027f5":"code","2e8b27b3":"code","6a6ef10f":"code","81eb400a":"code","b40a2844":"code","d8f0c717":"code","96800569":"code","bb2a52dc":"code","b14c3ae4":"code","bb31331e":"code","ce8aff40":"code","eed41c96":"code","b2d3d510":"code","7a68eb3d":"code","bbd1fd30":"code","cb7f294d":"code","f83a0f20":"code","5152c191":"code","9181f7cb":"code","ba491460":"code","b86b924e":"code","9185bf08":"code","314b885c":"code","72982d8c":"code","f9dca42a":"code","fcf4fe15":"code","03c62bdc":"code","7792e60d":"code","c1b226f2":"code","543292ff":"code","7c38abee":"code","75ce0149":"code","98f97931":"code","c3c1a5c0":"code","e594833c":"code","beab9735":"code","9543c60c":"code","37a6144d":"code","8b3a7cb9":"code","f8defbaa":"code","91db6f2d":"code","8b316f9f":"code","f0859d8d":"code","acb300a8":"code","6e4ad50e":"code","c22e5616":"code","a4261382":"code","eeb62eaa":"code","baf910f2":"code","73f29a97":"code","d27f6a4a":"code","93651a36":"code","daedeb16":"code","54741db9":"markdown","15c8e1f5":"markdown","502aa919":"markdown","acaaa84a":"markdown","851b6568":"markdown","e225dbdf":"markdown","bb77864a":"markdown","d3233fca":"markdown","17c1b83c":"markdown","59abc250":"markdown","cfff5294":"markdown","ed4390e1":"markdown","701ce51d":"markdown","1bfab160":"markdown","403ef82d":"markdown","8b6ff5c1":"markdown","e7d3f7a6":"markdown","d6981028":"markdown","ab65e03f":"markdown","89445814":"markdown","b94362bf":"markdown","24e70636":"markdown","1ab03cf9":"markdown","77303065":"markdown"},"source":{"15ef042c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor\n\nimport shap\nshap.initjs()\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\n\n# import lightgbm as lgb\n# from lightgbm import LGBMClassifier\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import RobustScaler , OneHotEncoder , LabelEncoder\nfrom sklearn.ensemble import IsolationForest\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# pd.set_option('use_inf_as_na', True) ## this makes the fillna very slow!! \n## may be faster to just : df.replace([np.inf, -np.inf], np.nan)","16b48872":"def weightedclasst(x):\n    \"\"\"based on : https:\/\/www.kaggle.com\/c\/widsdatathon2020\/discussion\/127987 but made to use ordinals\"\"\"\n    if pd.isna(x):\n        return np.nan\n    if x < 15: return 0 \n    elif x >= 15 and x < 16: return 1\n    elif x >=16 and x < 18.5: return 2 \n    elif x >= 18.5 and x < 25: return 3 \n    elif x >= 25 and x < 30: return 4 \n    elif x >= 30 and x < 35: return 5 \n    elif x >= 35: return ","4bf3623a":"TARGET_COL = \"hospital_death\"","2976d2dd":"df = pd.read_csv(\"\/kaggle\/input\/widsdatathon2020\/training_v2.csv\")#.sample(5234)\nprint(df.shape)\ndisplay(df.nunique())\nprint()\n\ndf.head()","0e5e25fa":"display(list(df.columns))","f99e184b":"train, test = train_test_split(df, test_size=0.3)\n#test = pd.read_csv(\"\/kaggle\/input\/widsdatathon2020\/unlabeled.csv\")\nprint(test.shape)\ndisplay(test.nunique())\ntest.head()","c0db0b07":"df[\"icu_id_isin_test\"] = df['icu_id'].isin(test['icu_id'])\ntest[\"icu_id_isin_test\"] = True\n\ntest[\"icu_id_isin_train\"] = test['icu_id'].isin(df['icu_id'])\ndf[\"icu_id_isin_train\"] = True\n\n\ndf[\"hospital_id_isin_test\"] = df['hospital_id'].isin(test['hospital_id'])\ntest[\"hospital_id_isin_test\"] = True\n\ntest[\"hospital_id_isin_train\"] = test['hospital_id'].isin(df['hospital_id'])\ndf[\"hospital_id_isin_train\"] = True","6271c7df":"# display(pd.concat([df,test])[[\"hospital_id_isin_train\",\"hospital_id_isin_test\",\"icu_id_isin_train\",\"icu_id_isin_test\"]].describe())\n\ntest[[\"hospital_id_isin_train\",\"icu_id_isin_train\"]].describe()","84ce6b28":"USELESS_COLS = [\"patient_id\",\n                \"hospital_id_isin_train\",\"icu_id_isin_train\",\"icu_id_isin_test\",\n#                 'icu_id'  # doesn't cover test ?  - we still keep hospital ID though - which is as bad - we still want it for a count feature maybe? \n               ] \n# \"encounter_id\"  is useless (seemingly) for prediction as is patient number (all singletons, no apparent ordering). but we need encounter number for predictions","ed3cbfac":"df = pd.concat([train,test])\n\ndf.drop(USELESS_COLS,axis=1,inplace=True)\nprint(df.shape)","c27db926":"## get medium cardinality columns - possible categoricals +- icd codes ?\ncardinal_cols = [c for c in df.columns if( 1<df[c].nunique()<3000)]\ncardinal_cols =  [c for c in cardinal_cols if not((c.startswith(\"h1\")) | (c.startswith(\"d1\")))]\nprint(len(cardinal_cols))\nprint(cardinal_cols)","268fde4a":"df[cardinal_cols].nunique()","06889b6f":"## impute BMI and add BMI ranges\ndf[\"diff_bmi\"] = df['bmi'].copy() # orig BMI values\ndf['bmi'] = df['weight']\/((df['height']\/100)**2)\ndf[\"diff_bmi\"] = df[\"diff_bmi\"]-df['bmi']\n\n6 \n#     else: return -1 \n                                \ndf['weightclass'] = df['bmi'].map(weightedclasst)\ndf['weightclass'].value_counts()","c6178500":"display(df[['weightclass',\"bmi\",\"diff_bmi\"]].describe())","5a5fc041":"## count missing , 0 values per row (very noisy proxy for # tests run\/monitored)\ndf[\"row_nan_sum\"] = df.drop([TARGET_COL,\"hospital_id_isin_test\"],axis=1).isna().sum(axis=1)\n# test[\"row_nan_sum\"] = test.isna().sum(axis=1)\n\ndf[\"row_zero_count\"] = (df.drop([TARGET_COL,\"hospital_id_isin_test\"],axis=1) == 0).sum(axis=1)\n# test[\"row_zero_count\"] = (test == 0).sum(axis=1)\n\n## can use death prob as baseline?  - there are missing values, in addition to \"-1\" - may be also missing value proxy ?  - we'll do joint feature\n## -1 seems to be a nan proxy\ndf['apache_4a_icu_death_prob'] = df['apache_4a_icu_death_prob'].replace(-1,np.nan)\ndf['apache_4a_hospital_death_prob'] = df['apache_4a_hospital_death_prob'].replace(-1,np.nan)\ndf[\"max_apache_4a_death_prob\"] = df[['apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']].max(axis=1)","7de7a5b9":"## count missing values in d1 labs , and h1 labs\nd_cols = [c for c in df.columns if(c.startswith(\"d1\"))]\nh_cols = [c for c in df.columns if(c.startswith(\"h1\"))]\n\ndf[\"dailyLabs_row_nan_count\"] = df[d_cols].isna().sum(axis=1)\ndf[\"hourlyLabs_row_nan_count\"] = df[h_cols].isna().sum(axis=1)\n\ndf[\"diff_labTestsRun_daily_hourly\"] = df[\"dailyLabs_row_nan_count\"] - df[\"hourlyLabs_row_nan_count\"]","f94435bb":"lab_col = [c for c in df.columns if((c.startswith(\"h1\")) | (c.startswith(\"d1\")))]\n\n# prefix (d1_, h1_) and Suffix (_min, max) removal from String list using map() + lambda \nlab_col_names = list(set(list(map(lambda i: i[ 3 : -4], lab_col))))\n\nprint(\"len lab_col\",len(lab_col))\nprint(\"len lab_col_names\",len(lab_col_names))\nprint(\"lab_col_names\\n\",lab_col_names)\n\nlab_col_names =  ['pao2fio2ratio', 'wbc', 'arterial_ph', 'bilirubin',\n  'glucose', 'mbp_noninvasive', 'calcium', 'spo2', 'inr', 'platelets', 'hco3',\n  'creatinine', 'sysbp_invasive', 'mbp_invasive', 'resprate', 'temp', 'sysbp',\n  'sysbp_noninvasive', 'heartrate', 'sodium', 'diasbp_invasive', 'bun', 'arterial_po2',\n  'lactate', 'hematocrit', 'diasbp_noninvasive',\n  'mbp', 'albumin', 'arterial_pco2', 'diasbp', 'hemaglobin', 'potassium']","ad7027f5":"df[lab_col].isna().mean()","2e8b27b3":"first_h = []\nfor v in lab_col_names:\n    df[v+\"_d1_value_range\"] = df[f\"d1_{v}_max\"].subtract(df[f\"d1_{v}_min\"])\n    \n    df[v+\"_h1_value_range\"] = df[f\"h1_{v}_max\"].subtract(df[f\"h1_{v}_min\"])\n#     df[v+\"_h1_value_range_normalized\"] = df[f\"h1_{v}_max\"].subtract(df[f\"h1_{v}_min\"]).div(df[f\"h1_{v}_max\"])\n    \n    # daily change in value range - hour vs day. could do subtract or div here.. \n    df[v+\"_tot_change_value_range_normed\"] = abs((df[v+\"_d1_value_range\"].div(df[v+\"_h1_value_range\"])))#.div(df[f\"d1_{v}_max\"]))\n    \n    # Cases where there's no reading in the first hour, but only  later in day ?\n    df[v+\"_started_after_firstHour\"] = ((df[f\"h1_{v}_max\"].isna()) & (df[f\"h1_{v}_min\"].isna())) & (~df[f\"d1_{v}_max\"].isna())\n    first_h.append(v+\"_started_after_firstHour\")\n    \n    ## Did a reading get more extreme after the first hour. \n    ## This misses cases where the readying is in an unhealthy region and \"improves\"!!\n    df[v+\"_day_more_extreme\"] = ((df[f\"d1_{v}_max\"]>df[f\"h1_{v}_max\"]) | (df[f\"d1_{v}_min\"]<df[f\"h1_{v}_min\"]))\n    df[v+\"_day_more_extreme\"].fillna(False)","6a6ef10f":"df[\"total_Tests_started_After_firstHour\"] = df[first_h].sum(axis=1)\ndf[\"total_Tests_started_After_firstHour\"].describe()","81eb400a":"more_extreme_cols = [c for c in df.columns if(c.endswith(\"_day_more_extreme\"))]\n\ndf[\"total_day_more_extreme\"] = df[more_extreme_cols].sum(axis=1)","b40a2844":"df[first_h].describe()","d8f0c717":"## interaction type features: \n## SB derived features - col interactions\n### May consider adding features of h1 \/ D1 (for each specific variable)\n\ndf[\"d1_resprate_div_mbp_min\"] = df[\"d1_resprate_min\"].div(df[\"d1_mbp_min\"])\ndf[\"d1_resprate_div_sysbp_min\"] = df[\"d1_resprate_min\"].div(df[\"d1_sysbp_min\"])\ndf[\"d1_lactate_min_div_diasbp_min\"] = df[\"d1_lactate_min\"].div(df[\"d1_diasbp_min\"])\ndf[\"d1_heartrate_min_div_d1_sysbp_min\"] = df[\"d1_heartrate_min\"].div(df[\"d1_sysbp_min\"])\ndf[\"apache_icu_div_apache_hospital\"] = df[\"apache_4a_icu_death_prob\"].div(df[\"apache_4a_hospital_death_prob\"])\ndf[\"d1_hco3_div\"]= df[\"d1_hco3_max\"].div(df[\"d1_hco3_min\"])\n\n\ndf[\"apache_hospital_minus_apache_icu\"] = df[\"apache_4a_hospital_death_prob\"] - df[\"apache_4a_icu_death_prob\"]\n\ndf[\"d1_resprate_times_resprate\"] = df[\"d1_resprate_min\"].multiply(df[\"d1_resprate_max\"])\n\ndf[\"left_average_spo2\"] = (2*df[\"d1_spo2_max\"] + df[\"d1_spo2_min\"])\/3","96800569":"## could be done by batch more efficiently with groupby.agg({count\"}) - but i'm lazy..\n##  size counts NaN values, count does not. \n\n\n# df[\"icu_id_count\"] = df.groupby([\"icu_id\"])[\"encounter_id\"].transform(\"size\")\n# df[\"hospital_id_count\"] = df.groupby(['hospital_id'])[\"encounter_id\"].transform(\"size\")\n\ndf[\"apache_2_diagnosis_count\"] = df.groupby([\"apache_2_diagnosis\"])[\"encounter_id\"].transform(\"size\")\ndf[\"apache_3j_diagnosis_count\"] = df.groupby([\"apache_3j_diagnosis\"])[\"encounter_id\"].transform(\"size\")\n\n## V18 \ndf[\"hospital_admit_source_count\"] = df.groupby([\"hospital_admit_source\"])[\"encounter_id\"].transform(\"size\")\ndf[\"apache_3j_bodysystem_count\"] = df.groupby([\"apache_3j_bodysystem\"])[\"encounter_id\"].transform(\"size\")\ndf[\"apache_2_bodysystem_count\"] = df.groupby([\"apache_2_bodysystem\"])[\"encounter_id\"].transform(\"size\")","bb2a52dc":"# sum of chronic diseases or complicators  - we may want to ignore the cancers\/tumors +- immunosuppresasnts cancers or seperate\ndf[\"total_chronic\"] = df[[\"aids\",\"cirrhosis\", 'diabetes_mellitus', 'hepatic_failure']].sum(axis=1)\ndf[\"total_cancer_immuno\"] = df[[ 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].sum(axis=1)\n# df.head(50)[[\"total_chronic\",\"total_cancer_immuno\",\"aids\",\"cirrhosis\", 'diabetes_mellitus', 'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].drop_duplicates()\n\n## could also add extreme age\/weight for this - \ndf[\"has_complicator\"] = df[[\"aids\",\"cirrhosis\", 'diabetes_mellitus', 'hepatic_failure',\n                            'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].max(axis=1)\ndf[[\"has_complicator\",\"total_chronic\",\"total_cancer_immuno\",\"has_complicator\"]].describe()","b14c3ae4":"df[\"has_complicator\"].sum()","bb31331e":"print(df[['apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob',]].isna().sum())\n\ndf[['apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob',\"max_apache_4a_death_prob\"]].hist();","ce8aff40":"## proxy for identifying patient \"clusters\" - i.e possibly the same patient across encounters\n### take relatively \"immutable\" patient attributes\/demographics +- rounding (e.g. age changes over time, height may be recorded incorrectly)\n### unsure how to regard things like cancer.. depends on time scale covered by study (months? years?)\n### what about height\/weight\/BMI ?? \n\nimport math\ndef myround(x, base=5):\n    \"\"\"https:\/\/stackoverflow.com\/questions\/2272149\/round-to-5-or-other-number-in-python\"\"\"   \n    if math.isnan(x): return (np.nan) # handle missing values\n    else:\n        return int(base * round(x\/base))\n\ndf[\"rounded_age\"] = df['age'].apply(lambda x: myround(x,base=8))\ndf[\"rounded_height\"] = df['height'].apply(lambda x: myround(x,base=3))\n\ndf.head()[['rounded_age', 'rounded_height','age','height']].drop_duplicates()\n\nIDENTIFYING_COLS = ['rounded_age', 'rounded_height',  'ethnicity', 'gender' , \"aids\", 'diabetes_mellitus']  # , 'solid_tumor_with_metastasis'\n## weight , BMI ?  ##  'immunosuppression', 'leukemia', 'lymphoma',\n","eed41c96":"## hash these values into a new \"id\"\ndf[\"hash_person_profile\"] = df[IDENTIFYING_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\n\nprint(df[\"hash_person_profile\"].nunique())\ndisplay(df[[\"hash_person_profile\",'rounded_age', 'rounded_height','age','height']].tail(2))\n\ndf.drop(['rounded_age', 'rounded_height'],axis=1,inplace=True)","b2d3d510":"df[\"hash_person_profile_apache_death_risk_mean\"] = df.groupby([\"hash_person_profile\"])[\"max_apache_4a_death_prob\"].transform(\"mean\")\n\ndf[\"hash_person_profile_size\"] = df.groupby([\"hash_person_profile\"])[\"max_apache_4a_death_prob\"].transform(\"size\")","7a68eb3d":"## get integer rounded versions - this risks duplicaiton.. . It's pointless on the diag2 - they're all 3 digit integers (with nans) \n## we use pandas's \"Int64\" - supports null values!\n#http:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/integer_na.html\n\n# #  ## pointless - no difference in values ! (but we do get an int instead of f laot - which would mess up joins to icd text desc)\n### + plus - messes with catboost - unsupported\n# df[\"apache_2_diagnosis\"] = df[\"apache_2_diagnosis\"].astype(\"Int64\")\ndf[\"apache_2_diagnosis\"] = df[\"apache_2_diagnosis\"].fillna(0)\n\n# df[\"int_apache_3j_diag\"] = df[\"apache_3j_diagnosis\"].fillna(0).astype(\"Int64\")  ## gives error -  cannot safely cast non-equivalent float64 to int64\ndf[\"int_apache_3j_diag\"] =  df[\"apache_3j_diagnosis\"].fillna(0).astype(str).str.split(\".\",expand=True)[0].astype(int)\n\ndf.tail()[[\"int_apache_3j_diag\",\"apache_3j_diagnosis\",\"apache_2_diagnosis\"]].drop_duplicates()","bbd1fd30":"df[\"apache_2_diagnosis\"].describe()","cb7f294d":"df[[\"int_apache_3j_diag\",\"apache_3j_diagnosis\",\"apache_2_diagnosis\"]].nunique()","f83a0f20":"df[\"apache_3j_diagnosis\"].astype(str).apply(len).describe()","5152c191":"df.loc[df[\"apache_2_diagnosis\"].astype(str).str.len()<4][[\"apache_2_diagnosis\",\"apache_3j_diagnosis\"]]\n\n## these 0.25 variables look weird... ? ","9181f7cb":"df.head(10)[[\"int_apache_3j_diag\",\"apache_3j_diagnosis\",\"apache_2_diagnosis\"]].drop_duplicates()","ba491460":"# import math\n\n# df[\"grouped_apache_3j_diag\"] = df['int_apache_3j_diag'].map(math.floor(100))\n## round to nearest 100 - second digit\ndf[\"grouped_apache_3j_diag\"] = df['int_apache_3j_diag'].fillna(0).astype(int).apply(lambda x: round(x,-2))\/\/100\n\n# marke\/parse  as categorical","b86b924e":"df.head(10)[[\"grouped_apache_3j_diag\", \"int_apache_3j_diag\"]].drop_duplicates()","9185bf08":"df[[\"grouped_apache_3j_diag\", \"int_apache_3j_diag\"]].nunique()","314b885c":"df = df*1  ## force booleans to be integers","72982d8c":"#clf = IsolationForest(n_estimators=120, max_samples=1100, max_features=0.9, n_jobs=3,behaviour=\"new\",contamination='auto')\n#clf2 = IsolationForest(n_estimators=150, max_samples=600, max_features=1.0, n_jobs=3,behaviour=\"new\", contamination='auto')","f9dca42a":"c_cols = [c for c in df.columns if df[c].dtype ==\"O\"]\nprint(c_cols)\n## manually change for purposes of sklearn friendly encoding\n# c_cols = ['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source',\n# 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem'] # \"grouped_apache_3j_diag\"","fcf4fe15":"### pd.set_option('use_inf_as_na', True)  - to handle infs \n# X = df._get_numeric_data().drop([TARGET_COL],axis=1).copy()\nX = df.drop([TARGET_COL],axis=1).copy()\n\nX.replace([np.inf, -np.inf], np.nan,inplace=True)\n\nfor c in c_cols:\n    le = LabelEncoder()\n    X[c] = le.fit_transform(X[c].astype(str))\n\nX = X.drop([\"hash_person_profile\",\"icu_id\",\"hospital_id\",\"hash_person_profile_apache_death_risk_mean\"],axis=1,errors=\"ignore\")*1\n# X = X.fillna(-1)\n# X = X*1  ## force booleans to be integers\n\nX.dropna(thresh = 1000, axis = 1,inplace=True)\nprint(X.shape)\n## https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\n# label encode (could also one hot) transform object cols \n# X.dtypes()","03c62bdc":"#%%time\n#clf.fit(X.fillna(-1))\n#df[\"isolation_forest_score_1\"] = clf.score_samples(X.fillna(-1))","7792e60d":"#%%time\n## additional model, with less missing values (drop cols by threshhold) and alternate imputation (median)\n\n#thresh = len(X) * .5\n#X.dropna(thresh = thresh, axis = 1,inplace=True)\n#print(X.shape)\n#X.fillna(X.median(),inplace=True)\n#print(\"filled\")\n#clf2.fit(X)\n#df[\"isolation_forest_score_2\"] = clf2.score_samples(X)","c1b226f2":"#del(X)","543292ff":"#df.drop([\"icu_id\",\"hospital_id\"],axis=1).loc[~df[TARGET_COL].isna()].to_csv(\"wids_train_v3.csv.gz\",index=False,compression=\"gzip\")\n#df.drop([\"icu_id\",\"hospital_id\"],axis=1).loc[df[TARGET_COL].isna()].to_csv(\"wids_test_v3.csv.gz\",index=False,compression=\"gzip\")","7c38abee":"print([c for c in df.columns if 7<df[c].nunique()<800])\n## \n# categorical_cols = ['hospital_id','apache_3j_bodysystem', 'apache_2_bodysystem',\n# \"hospital_admit_source\",\"icu_id\",\"ethnicity\"]","75ce0149":"## print non numeric columns : We may need to\n## define them as categorical \/ encode as numeric with label encoder, depending on ml model used\nprint([c for c in df.columns if (1<df[c].nunique()) & (df[c].dtype != np.number)& (df[c].dtype != int) ])","98f97931":"categorical_cols =  [\n 'ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type',\n                     'apache_3j_bodysystem', 'apache_2_bodysystem',\n   # \"hash_person_profile\",\n                     \"int_apache_3j_diag\",\n#     \"apache_3j_diagnosis\",\n#     \"apache_2_diagnosis\",  # fill na fails on it\n  #  \"grouped_apache_3j_diag\"\n                    ]\n### 'hospital_id',   - From SB  Isee range features also get value from this. but also, it's a feature that may not generalize to test\n### same for icu_id","c3c1a5c0":"display(df[categorical_cols].dtypes)\ndisplay(df[categorical_cols].tail(3))\ndisplay(df[categorical_cols].isna().sum())","e594833c":"df[categorical_cols] = df[categorical_cols].fillna(\"\").astype(str)\n\n# # same transformation for test data\n# test[categorical_cols] = test[categorical_cols].fillna(\"\")\n\ndf[categorical_cols].isna().sum()","beab9735":"### drop columns we think don't generalize to test at this point \n\ndf.drop([\"icu_id\",\"hospital_id\"],axis=1,inplace=True,errors=\"ignore\")","9543c60c":"## useful \"hidden\" function - df._get_numeric_data()  - returns only numeric columns from a pandas dataframe. Useful for scikit learn models! \n\nX_train = df.loc[train.index].drop([TARGET_COL],axis=1)\ny_train = df.loc[train.index][TARGET_COL].astype(int)\n\ntest_target = df.loc[test.index][TARGET_COL].astype(int)\ntest = df.loc[test.index].drop([TARGET_COL],axis=1)\n","37a6144d":"display(df[categorical_cols].dtypes)\ndf[categorical_cols].tail()","8b3a7cb9":"## catBoost Pool object\ntrain_pool = Pool(data=X_train,label = y_train,\n                  cat_features=categorical_cols,\n#                   baseline= X_train[\"max_apache_4a_death_prob\"].fillna(X_train[\"max_apache_4a_death_prob\"].median()) # ['apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob'\"],## baseline doesn't work with pool ## remove nans first!!\n#                   group_id = X_train['joint_key']\n                 )\n\n### OPT\/TODO:  do train test split for early stopping then this : \n\n# eval_pool = Pool(data=X_test,label = y_test,cat_features=categorical_cols,\n# #                   baseline= X_train[\"mean_month_weekend_unit_sales\"], ##\n# #                   group_id = X_test['joint_key']\n#                  )","f8defbaa":"model_basic = CatBoostClassifier(verbose=False,# task_type=\"CPU\",depth=7,\n#                                   iterations=7,\n                                 metric_period=4,\n                                 )\nmodel_basic.fit(train_pool, plot=True,silent=True )\nprint(model_basic.get_best_score())","91db6f2d":"model = CatBoostClassifier(verbose=False, task_type=\"CPU\",depth=10, #eval_metric=\"AUC\",\n                           iterations=1600,\n                           learning_rate=0.06,\n                           metric_period=4)#,learning_rate=0.1, task_type=\"GPU\",)\nmodel.fit(X_train,y_train,\n                  cat_features=categorical_cols,\n                  baseline= (0.2 + X_train[\"max_apache_4a_death_prob\"].fillna(X_train[\"max_apache_4a_death_prob\"].mean())), # remove nans.. \n          plot=True,silent=True)\nprint(model.get_best_score())","8b316f9f":"# ### hyperparameter tuning example grid for catboost : \n# grid = {\n#     'learning_rate': [0.04],#[0.035, 0.1],\n#         'depth': [6, 9],\n#         'l2_leaf_reg': [1, 3,7],\n#         \"iterations\": [2000],\n# }\n\n# model = CatBoostClassifier(#eval_metric='AUC',\n#                            task_type=\"GPU\",\n# #     use_best_model=True,\n#     early_stopping_rounds=20,)\n\n# ## can also do randomized search - more efficient typically, especially for large search space - `randomized_search`\n# grid_search_result = model.grid_search(grid, \n#                                        train_pool,\n#                                        plot=True,\n#                                        refit = True, #  refit best model on all data\n#                                       partition_random_seed=42)\n\n# print(model.get_best_score())\n\n# print(\"best model params: \\n\",grid_search_result[\"params\"])","f0859d8d":"model = model_basic","acb300a8":"feature_importances = model.get_feature_importance(train_pool)\nfeature_names = X_train.columns\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n    if score >= 0.24:\n        print('{0}: {1:.2f}'.format(name, score))","6e4ad50e":"#explainer = shap.TreeExplainer(model)\n#shap_values = explainer.shap_values(train_pool)\n\n# visualize the training set predictions\n# SHAP plots for all the data is very slow, so we'll only do it for a sample. Taking the head instead of a random sample is dangerous! \n#shap.force_plot(explainer.expected_value,shap_values[:,:30000], X_train.iloc[:,:30000])","c22e5616":"# # absolute importance to model, without directionality\n# shap.summary_plot(shap_values, X_train, plot_type=\"bar\")","a4261382":"# summarize the effects of all the features\n#shap.summary_plot(shap_values, X_train)","eeb62eaa":"# shap.dependence_plot(\"bmi\", shap_values, X_train)","baf910f2":"test","73f29a97":"pred1 = model.predict(test,prediction_type='Probability')[:,1]\npred2 = model_basic.predict(test,prediction_type='Probability')[:,1]\ntest[\"hospital_death_prob\"] = (pred1 + pred2)\/2\n\n#print(\"train target mean\",df[TARGET_COL].mean())\n#print(\"predictions target mean\",test[\"hospital_death\"].mean())","d27f6a4a":"test","93651a36":"test[[\"encounter_id\",\"hospital_death\",\"hospital_death_prob\",\"apache_4a_hospital_death_prob\"]].to_csv(\"solution_for_NRI.csv\",index=False)","daedeb16":"# https:\/\/gist.github.com\/aswalin\/595ac73f91c6268f9ca449a4ee05dee1#file-catboost-ipynb\n%matplotlib inline \nfrom catboost import *\nimport matplotlib.pyplot as plt\n\nfi = model.get_feature_importance(Pool(X_train, label=y_train,cat_features=categorical_cols),type=\"Interaction\")\n\nfi_new = []\nfor k,item in enumerate(fi):  \n    first = X_train.dtypes.index[fi[k][0]]\n    second = X_train.dtypes.index[fi[k][1]]\n    if first != second:\n        fi_new.append([first + \"_\" + second, fi[k][2]])\n        \nfeature_score = pd.DataFrame(fi_new,columns=['Feature-Pair','Score'])\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\nplt.rcParams[\"figure.figsize\"] = (16,7)\nax = feature_score.plot('Feature-Pair', 'Score', kind='bar', color='c')\nax.set_title(\"Pairwise Feature Importance\", fontsize = 14)\nax.set_xlabel(\"features Pair\")\nplt.show()","54741db9":"## Export semi enriched data","15c8e1f5":"* We Fill in empty string for missing  values in the string columns , otherwise catboost will give an error - \"CatBoostError: Invalid type for cat_feature: cat_features must be integer or string, real number values and NaN values should be converted to string.\"\n","502aa919":"## Train a basic model","acaaa84a":"## Build & Tune catboost\/GBM models\n* We could make a validation subset for early stopping - allowing us to more easily tune our models hyperparameters\n*  we can go with a defualt model for now - it gets good results ,as we'll see\n\n* We can run on the GPU, giving a speed boost - to do this modify the kaggle kernel to use the GPU, and in the `fit` parameters set `task_type = \"GPU\"`\n\n* Catboost and lgbm have `Pool`\/`Dataset` objects, that can be used \"internally\" by them for some functions, e.g. to efficienctly CV\n\n","851b6568":"# 1st place (component) kernel\n\n### More competitive version of my starter kernel + features\n\n* https:\/\/www.kaggle.com\/danofer\/wids-2020-starter-catboost-0-902-lb\n* Uses GPU for the catboost, can be disabled\n* V8 : + features, disable model CV, disable GPU , remove hospital and icu id\n\n* V11 - try running without hospital_count or icu_count (don't know if they add information vs the test set? )  + export enriched data + add 2 features on poerson_hash\n\n* V12 - more features (some to add):  + Drop ml models used to speed up (Ideally\/ToDO - replace with  CV to see if added features contribute)\n    * (normalized) value range : ` (max - min )\/(max)`\n    * daily diff median : `d1 ((max + min) \/2) \/ h1(((max + min) \/2))`\n    * Is value in first hour the most extreme? (`if min d1 < min h1  OR max d1 > max d1`) \n    * added feature for (max) presence of any complicator (chronic disease, cancer etc')\n    \n* V13 - feature for how many d1 tests were run total \n    * also todo : polynomial features ( + feature selection)\n    \n* V16 - add bmi imputation and ranges. tuned features of # tests run per day and hour (nan counts)\n* V17 - changed BMI to replace old values + add flag for different values (i.e cases where BMI was incorrectly calculated) + Added 2 isolation forest features + Added grouped bins of apache 3j diag codes + set it and the the int rounded 3j codes as categorical\n\n* V18 - add some more count features for categoricals (hospital admit source..) , - based on : https:\/\/www.kaggle.com\/binaicrai\/fork-of-fork-of-wids-lgbm-gs#Visualization \n    * + made ordinal bmi function be >= 35 , not 34-40.  \n    * changed daily change in value range  feature, removed division\/normalizing by daily max\n         df[v+\"_tot_change_value_range_normed\"]","e225dbdf":"## Load train\/test data\n* If doing feature engineering, we could combine the 2 dataframes together then split them after. But we'll go for a naive approach in this kernel","bb77864a":"## row level features on lab tests\n* count tests done\n* could do most extreme result (requires standardizing (z-score) or normalizing the mall first). ","d3233fca":"## Time Series like feature interactions\n\n* (normalized) value range : ` (max - min )\/(max)` OR value range (no normalizing)?\n    * Currently I calc only for daily (d1) variable ranges ,could also do first hour\n* daily diff median : `d1 ((max + min) \/2) \/ h1(((max + min) \/2))`\n* Is value in first hour the most extreme? (`if min d1 < min h1  OR max d1 > max d1`) \n\n*  what about cases where there's no reading in the first hour, but only in later hours?\n    \n* First we get the names of variables that are relevant so we can apply to all the cols efficiently","17c1b83c":"# Adversarial validation - hospital_id is disjoint!\n* Run externally - we have perfect seperation on basis of ICU ID or hospital id \n* For now I add variables to check overlaps\n* validation set should possibly be split by icu_id or hospital id , rather than random . This also affects features we can create\n\n\n* There is a small subset of hospital_ids that overlap - ~ 1K rows\n* ICU and hospital ID are synonymous for this purpose\n","59abc250":"## Train model(s)","cfff5294":"### concat train and test for easier feature engineering\n* e.g. less rows of code\n* for any predictive imputation (e.g. of apache model death prob)","ed4390e1":"## Features importances\n\n* What are the most important features for predicting death? \n\n* We also look also at Shapley values : https:\/\/github.com\/slundberg\/shap\n    * Shap + Catboost tutorial :  https:\/\/github.com\/slundberg\/shap\/blob\/master\/notebooks\/tree_explainer\/Catboost%20tutorial.ipynb\n","701ce51d":"* As we would expect, age is important. \n* The top features are precalculated predictors of risk of death (which likely take age into account). \n    * In our final ensemble model, we dropped the APACHE IV precalculated risk score!\n\n* We see that there's a difference between hospitals , although it's not an especially clear or linear feature. One explanation may be differences in skill of departments\/doctors inside each hospital, with these \"latent\"\/hidden variables interacting with other factors in our dataset","1bfab160":"# Get predictions on test set and export for submission\n\n* You can \"ensemble\"\/average the predictions from the 2 catboost models as a quick improvement , even if there isn't much diversity added\n\n* The difference in mean predictions can also be explained by the train data having discrete (0\/1) labels, while our predictions are continous (0,1]).","403ef82d":"###### Interaction Feature Importance\n* Note: We would ideally do this on the test subset (not used here)\n    * Code source: https:\/\/towardsdatascience.com\/deep-dive-into-catboost-functionalities-for-model-interpretation-7cdef669aeed","8b6ff5c1":"### count type features \n* Count of a variable. Useful for rare things. \n* only relevant for categorical type features. \n\n* V18 - add hospital encounter (despite it being low cardinalty + clean it","e7d3f7a6":"#### Add subcode group features\n* based on codebook. Could do same for main code?? \n* basically prefie groups.\n* can attach text lookup from [external (private for now) dataset of codes](https:\/\/www.kaggle.com\/danofer\/apache-iiij-icu-diagnosis-codes#ANZICS-APD-Data-Dictionary.pdf): https:\/\/www.kaggle.com\/danofer\/apache-iiij-icu-diagnosis-codes#ANZICS-APD-Data-Dictionary.pdf\n    * For Apache III : simple high level - round to nearest 100 , and treat as categorical\/ohe. fill na with 0\n\n* We lack a codebook for the apache 2 codes (and they cover a much smaller range : 100-300 , which is odd). So we don't group for now\n\n* It is BAD to have 3 columns duplicating the same variable! (we could have it twice maybe , but we are still adding noise with the duplication)","d6981028":"## Add Features\n\n* Consider predictive imputation (e.g. of apache model death prob)\n* Consider externally joining ICD code textual descriptions\n    * icd codes are hierarchical - can take prefix. \n     * the icd codes here (from diag) have surprisingly low cardinality\n     \n     \n * Consider adding \"normalization\" of features - e.g. BMI \/ \"healthy\" bmi for gender\/age group?  , same for other variables - then sum \"unhealthy\".non normal readings","ab65e03f":"### ICD code features ?\n    * Externally - join with textual icd9 code descriptors\n    * Features: get prefix of code -> betetr generalization\n    * count \"length\" of code - proxy for detail assigned? (This is problematic due to missing values resulting in trailing zeroes - annoying to clean )\n        * There's only a few hundred codes in the data, may not add much to add generalizing features..\n        \n        \n    * TODO: Consider checking raw data - are zeroes being added incorrectly ? \n    \n    Skipping for now.. ","89445814":"### if using cartboost or lgbm, we can define categorical variables\n\n* catboost hyperparam tuning : https:\/\/colab.research.google.com\/github\/catboost\/tutorials\/blob\/master\/python_tutorial.ipynb#scrollTo=nSteluuu_mif\n\n\n* We see that many clearly continous numeric variables have relatively low cardinality (e.g. icu sensor readings) - making it tricky to define them automatically. \n* Categorical columns are not necessarily string columns, could be numerical - e.g. hospital codes.. \n","b94362bf":"## continue with remaining  features engineering","24e70636":"# Isolation forest anomaly score\n* As a sklearn model - requires missing values to be imputed. And categoricals to be handled ideally\n* this is just a TOY example for this feature - I don't handle the cateogircals for it properly and do naive imputation \n* the score of the model = probablity of a point being an anomaly","1ab03cf9":"### Hyperparameter search\n* After training an actual model - improvements : \n     * We can do a gridsearch for best hyperparameters, such as learning rate, etc' \n     * Another improvement: split evaluation set from train , and use it for early stopping + tun\n      * I leave this to the reader :)  \n      \n  * Another hyperparam example with hyperopt \/ optuna \/ :  https:\/\/github.com\/catboost\/tutorials\/blob\/master\/python_tutorial.ipynb\n  \n  * Params - https:\/\/catboost.ai\/docs\/concepts\/parameter-tuning.html","77303065":"* `encounter_id ,\tpatient_id\t` are unique. We could probably drop, but there may be leaks from them so we'll keep (And we need them for the submission)\n* Their being unique means they are not candidates for generating historical features from them per patient. \n    * We could try the hospital ID or surgery type or apache codes for that purpose"}}