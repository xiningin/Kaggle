{"cell_type":{"ec9b6888":"code","342ce4a4":"code","e6c0e05e":"code","0797e742":"code","1bce2991":"code","8eff5ae3":"code","def02039":"code","53c8c6e2":"code","e440f13d":"code","6701d6db":"code","f5494cb4":"code","30444250":"code","1ff56c4b":"code","00cd45d7":"code","b04d8fb8":"code","7e232bac":"code","fc64c60f":"code","73cfcc43":"code","52b6e1b7":"code","ddb03f4b":"code","6cff2e41":"code","007ecc02":"code","77a008d7":"code","d00846b4":"code","04a6a935":"code","46e52536":"code","022c1378":"code","230eb4de":"code","a2e72d85":"code","132b898a":"code","1b56c42d":"markdown","f68850ed":"markdown","1b9df7c0":"markdown","4d45ec60":"markdown","3de942fa":"markdown","d397ae19":"markdown","07c64204":"markdown","c75ebcab":"markdown","ce62e452":"markdown","9139823e":"markdown","ff43cd76":"markdown","f34899a1":"markdown","b896c745":"markdown","281cac1c":"markdown","cd57fbfe":"markdown","e3c436c1":"markdown","e7eff36c":"markdown","3a97c345":"markdown","00fb92f1":"markdown","09dfcbed":"markdown","32abe59f":"markdown","93c7a3e7":"markdown","e739506d":"markdown","7a0ce9f7":"markdown","18199a97":"markdown","bdea92cd":"markdown","8f4ce1b1":"markdown","98b61a21":"markdown","44053eff":"markdown","21456a47":"markdown","c17f9590":"markdown","0d7af1ec":"markdown","11d2f618":"markdown","df1ed946":"markdown","82eb7697":"markdown","6dde9963":"markdown","adba24b5":"markdown","97a99416":"markdown","e48ae2ed":"markdown","210b741b":"markdown","4c17458a":"markdown","6822b713":"markdown","c012c506":"markdown"},"source":{"ec9b6888":"# data analysis\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import r2_score\n\n# visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\n%matplotlib inline\n\n#Model\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n#Classifiers\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB","342ce4a4":"# load train.csv to pandas data frame, using 'PassengerId' as the index\ntitanic_df = pd.read_csv('..\/input\/titanic-train-clean\/titanic_train_clean.csv' , index_col='PassengerId')\n\n# Preview the data\ntitanic_df.head()","e6c0e05e":"# create pipeline using standardScaler and SVM classifier\nsvm_clf = make_pipeline(StandardScaler(), SVC())\n\n# create X and y\nsvm_X = titanic_df.drop(columns=['Survived', 'Embarked'])\nsvm_y = titanic_df.Survived\n\n# map title to numeric values\nsvm_X.Title = svm_X.Title.map({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Officer': 5, 'Royalty': 6})\n\n# split data in to training and testing subsets\nsvm_X_train , svm_X_test , svm_y_train, svm_y_test = train_test_split(svm_X, svm_y, random_state=1)\n\n# fit the data to the model\nsvm_clf.fit(svm_X_train, svm_y_train)\n\n# predict our test data\nsvm_y_pred = svm_clf.predict(svm_X_test)","0797e742":"# check the accuracy of our prediction\nprint('Accuracy: {0}%'.format((accuracy_score(svm_y_pred, svm_y_test)*100).round(2)))","1bce2991":"# drop title from our X data sets\nsvm_X_train = svm_X_train.drop(columns=['Title'])\nsvm_X_test = svm_X_test.drop(columns=['Title'])\n\n# fit the data to the model\nsvm_clf.fit(svm_X_train, svm_y_train)\n\n# predict our test data\nsvm_y_pred = svm_clf.predict(svm_X_test)\n\n# check the accuracy of our prediction\nprint('Accuracy: {0}%'.format((accuracy_score(svm_y_pred, svm_y_test)*100).round(2)))","8eff5ae3":"# generate classification report\nprint(classification_report(svm_y_test, svm_y_pred, target_names=['Died', 'Survived']))","def02039":"# Separate predictors and response\nada_X = titanic_df.drop(columns=['Survived', 'Embarked'])\nada_y = titanic_df.Survived\n\n# map title to numeric values\nada_X.Title = ada_X.Title.map({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Officer': 5, 'Royalty': 6})\n\n# split data in to training and testing subsets 80% train and 20% test\nada_X_train , ada_X_test , ada_y_train, ada_y_test = train_test_split(ada_X, ada_y, test_size = 0.2, shuffle=False)\n\n# Create AdaBoost classifier object\nada_classifier = AdaBoostClassifier(n_estimators = 100, learning_rate = 1)\n\n# Train AdaBoost classifier\nada_model = ada_classifier.fit(ada_X_train, ada_y_train)\n\n# Predict the response for test dataset\nada_y_pred = ada_model.predict(ada_X_test)","53c8c6e2":"# Model accuracy\nprint('Accuracy: {0}%'.format((accuracy_score(ada_y_pred, ada_y_test)*100).round(2)))","e440f13d":"# drop title from X values\nada_X = ada_X.drop(columns='Title')\n\n# split data in to training and testing subsets 80% train and 20% test\nada_X_train , ada_X_test , ada_y_train, ada_y_test = train_test_split(ada_X, ada_y, test_size = 0.2, shuffle=False)\n\n# Train AdaBoost classifier\nada_model = ada_classifier.fit(ada_X_train, ada_y_train)\n\n# Predict the response for test dataset\nada_y_pred = ada_model.predict(ada_X_test)\n\n# Model accuracy\nprint('Accuracy: {0}%'.format((accuracy_score(ada_y_pred, ada_y_test)*100).round(2)))","6701d6db":"# generate classification report\nprint(classification_report(ada_y_test, ada_y_pred, target_names=['Died', 'Survived']))","f5494cb4":"# create X and y\nknn_X = titanic_df.drop(columns=['Survived', 'Embarked'])\nknn_y = titanic_df.Survived\n\n# map title to numeric values\nknn_X.Title = knn_X.Title.map({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Officer': 5, 'Royalty': 6})\n\n# split data in to training and testing subsets\nknn_X_train , knn_X_test , knn_y_train, knn_y_test = train_test_split(knn_X, knn_y, test_size = 0.4, random_state=42, stratify = knn_y)","30444250":"#Setup sample arrays to test most accurate K value\nneighbors = np.arange(1,20)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# load in each k value 1-20 and test\nfor i, k in enumerate(neighbors):\n\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(knn_X_train, knn_y_train)\n    \n    #Check accuracy on the training set\n    train_accuracy[i] = knn.score(knn_X_train, knn_y_train)\n    \n    #Check accuracy on the test set\n    test_accuracy[i] = knn.score(knn_X_test, knn_y_test)","1ff56c4b":"#Generate plot\nplt.title('k-NN Varying number of neighbors')\nplt.plot(neighbors, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","00cd45d7":"#Load k neighbors to classifier\nknn = KNeighborsClassifier(n_neighbors= 13)\n\n#Fit data to model\nknn.fit(knn_X_train, knn_y_train)\n\n#Collect predications form classifier\nknn_y_pred = knn.predict(knn_X_test)\n\n#Print classification report\nprint(classification_report(knn_y_test,knn_y_pred, target_names=['Died', 'Survived']))\n\n# Model accuracy\nprint('Accuracy: {0}%'.format((accuracy_score(knn_y_pred, knn_y_test)*100).round(2)))","b04d8fb8":"# create train and test\nrf_train = titanic_df.drop(columns=['Survived','Embarked'])\nrf_test = titanic_df.Survived\n\n# map title to numeric values\nrf_train.Title = rf_train.Title.map({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Officer': 5, 'Royalty': 6})\n# split data in to training and testing subsets\nrf_X_train , rf_X_test , rf_y_train, rf_y_test = train_test_split(rf_train, rf_test, random_state=1)","7e232bac":"# create make_pipeline object using MinMaxScaler and Random Forest Classifier that uses no hyperparameters\nrf_clf = make_pipeline(MinMaxScaler(), RandomForestClassifier())\n\n# fit the data to the model\nrf_clf.fit(rf_X_train, rf_y_train)\n\n# predict our test data\nrf_y_pred = rf_clf.predict(rf_X_test)\n#Classification report\nprint(classification_report(rf_y_test, rf_y_pred, target_names=['Died', 'Survived']))\n#Accuracy Score\nprint('Accuracy: {0}%'.format((accuracy_score(rf_y_pred, rf_y_test)*100).round(2)))\nlabels = ['Yes','No']\nplot_confusion_matrix(rf_clf, rf_X_test, rf_y_test, display_labels=labels, normalize=None)\nplt.show()","fc64c60f":"rf_clf = make_pipeline(MinMaxScaler(),RandomForestClassifier(criterion='gini',\n# rf_clf = RandomForestClassifier(criterion='gini',\n    n_estimators=700,\n    min_samples_split=10, \n    min_samples_leaf=1, \n    max_features='auto', \n    oob_score=True, \n    random_state=1,\n    n_jobs=-1)\n)\nrf_clf.fit(rf_X_train, rf_y_train)\nrf_y_pred = rf_clf.predict(rf_X_test)\n\nprint(classification_report(rf_y_test, rf_y_pred, target_names=['Died', 'Survived']))\nprint('Accuracy: {0}%'.format((accuracy_score(rf_y_pred, rf_y_test)*100).round(2)))\nlabels = ['Yes','No']\nplot_confusion_matrix(rf_clf, rf_X_test, rf_y_test, display_labels=labels, normalize=None)\nplt.show()","73cfcc43":"'''\n# ***WARNING: The gridsearch may take a few minutes to run***\nrf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10], \"min_samples_split\" : [2, 4, 10, 12, 16], \"n_estimators\": [50, 100, 400, 700, 1000]}\n\nrf_gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n\nrf_gs = rf_gs.fit(rf_X_train, rf_y_train)\n# print(gs.best_score_)\nprint(rf_gs.best_params_)\n# print(gs.cv_results_)\n\n# new rf, with gridsearch hyperparameters\nrf_clf = make_pipeline(MinMaxScaler(), RandomForestClassifier(**rf_gs.best_params_))\nrf_clf.fit(rf_X_train, rf_y_train)\nrf_y_pred = rf_clf.predict(rf_X_test)\n\nprint(classification_report(rf_y_test, rf_y_pred, target_names=['Died', 'Survived']))\nprint('Accuracy: {0}%'.format((accuracy_score(rf_y_pred, rf_y_test)*100).round(2)))\nlabels = ['Yes','No']\nplot_confusion_matrix(rf_clf, rf_X_test, rf_y_test, display_labels=labels, normalize=None)\nplt.show()\n'''","52b6e1b7":"'''\nrf_clf = RandomForestClassifier(**rf_gs.best_params_)\nrf_clf.fit(rf_X_train, rf_y_train)\nrf_y_pred = rf_clf.predict(rf_X_test)\n\n#Visualize the important features\nfeature_imp = pd.Series(rf_clf.feature_importances_, index=rf_train.columns).sort_values(ascending=False)\nplt.figure(figsize=(10,6))\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.tight_layout()\n'''","ddb03f4b":"# create new train and test, dropping non-Important UniqueTicket and IsChild features too.\nrf_new_train = titanic_df.drop(columns=['UniqueTicket','IsChild','Survived','Embarked'])\nrf_new_test = titanic_df.Survived\n\n# map title to numeric values\nrf_new_train.Title = rf_new_train.Title.map({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Officer': 5, 'Royalty': 6})\n# split data in to new training and testing subsets\nrf_new_X_train , rf_new_X_test , rf_new_y_train, rf_new_y_test = train_test_split(rf_new_train, rf_new_test, random_state=1)","6cff2e41":"'''\n# ***WARNING: The gridsearch may take a few minutes to run***\nrf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10], \"min_samples_split\" : [2, 4, 10, 12, 16], \"n_estimators\": [50, 100, 400, 700, 1000]}\n\nrf_gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n\nrf_gs = rf_gs.fit(rf_new_X_train, rf_new_y_train)\n\n# new rf, with gridsearch hyperparameters\nrf_clf = make_pipeline(MinMaxScaler(), RandomForestClassifier(**rf_gs.best_params_))\nrf_clf.fit(rf_new_X_train, rf_new_y_train)\nrf_new_y_pred = rf_clf.predict(rf_new_X_test)\n\nprint(rf_gs.best_params_)\nprint(classification_report(rf_new_y_test, rf_new_y_pred, target_names=['Died', 'Survived']))\nprint('Accuracy: {0}%'.format((accuracy_score(rf_new_y_pred, rf_new_y_test)*100).round(2)))\nlabels = ['Yes','No']\nplot_confusion_matrix(rf_clf, rf_new_X_test, rf_new_y_test, display_labels=labels, normalize=None)\nplt.show()\n'''","007ecc02":"# Separate predictors and response\nLR_X = titanic_df.drop(columns=['Survived', 'Embarked'])\nLR_y = titanic_df.Survived\n\n# map title to numeric values\nLR_X.Title = LR_X.Title.map({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Officer': 5, 'Royalty': 6})\n\nLR_X_train, LR_X_test, LR_y_train, LR_y_test = train_test_split(LR_X, LR_y, test_size = 0.2, random_state = 0)","77a008d7":"LRegression = LinearRegression()\nLRegression.fit(LR_X_train, LR_y_train)\nLR_y_pred = LRegression.predict(LR_X_test) ","d00846b4":"result = round(LRegression.score(LR_X_train, LR_y_train) * 100, 2) \nprint(\"Accuracy {0}%\".format(result)) ","04a6a935":"#print(classification_report(LR_y_test, LR_y_pred, target_names=['Died', 'Survived']))","46e52536":"# Model accuracy\n#print('Accuracy: {0}%'.format((accuracy_score(LR_y_pred, LR_y_test)*100).round(2)))","022c1378":"# Removing unwanted string labels - They can be used if encoded into floats, but we will ignore it for now\ncopy_titanic_df = titanic_df.drop(labels=['Title' , 'Embarked'] , axis=1)\n\n# splitting response and predictor variables\nLogR_y = copy_titanic_df['Survived'].values\nLogR_X = copy_titanic_df.iloc[:,1:].values\n\n# splitting data into train and test\nLogR_X_train, LogR_X_test, LogR_y_train, LogR_y_test = train_test_split(LogR_X, LogR_y, random_state=21, test_size=0.2)","230eb4de":"# modelling Logistic Regression\nLogR_clf = LogisticRegression()\nLogR_clf.fit(LogR_X_train, LogR_y_train)\nLogR_y_pred = LogR_clf.predict(LogR_X_test) \nprint(\"Accuracy on our test set: {0}%\".format((LogR_clf.score(LogR_X_test, LogR_y_test)*100).round(2)))\nprint(\"Accuracy on our train set: {0}%\".format((LogR_clf.score(LogR_X_train, LogR_y_train)*100).round(2)))\nprint(\"\\n\",classification_report(LogR_y_test, LogR_y_pred, target_names=['Died', 'Survived']))","a2e72d85":"# Model Evaluation using visuals\n# Predicting train data\ntrain_preds = LogR_clf.predict(LogR_X_train)\ncm = confusion_matrix(LogR_y_train, train_preds)\n\n# Displaying confusion matrix on train data\nplt.figure(figsize=(6,6))\nplt.title('Confusion matrix on train data')\nsns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Greens, cbar=False)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n\n# Predicting test data\ntest_preds = LogR_clf.predict(LogR_X_test)\ncm = confusion_matrix(LogR_y_test, test_preds)\n\n# Displaying confusion matrix on test data\nplt.figure(figsize=(6,6))\nplt.title('Confusion matrix on test data')\nsns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Reds, cbar=False)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()","132b898a":"# Separating data into response and precdicatbles\nNB_X = titanic_df.drop(columns=['Survived', 'Embarked'])\nNB_Y = titanic_df.Survived\n\nNB_X.Title = NB_X.Title.map({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Officer': 5, 'Royalty': 6})\n\n# splitting training and test data\nNB_X_train , NB_X_test , NB_Y_train, NB_Y_test = train_test_split(NB_X, NB_Y, random_state=1)\n\n# Modelling and prediction\ngaussian = GaussianNB()\ngaussian.fit(NB_X_train, NB_Y_train)\nNB_Y_pred = gaussian.predict(NB_X_test)\n\nprint(\"Accuracy: {0}%\\n\".format((gaussian.score(NB_X_test, NB_Y_test)*100).round(2)))\nprint(classification_report(NB_Y_test, NB_Y_pred, target_names=['Died', 'Survived']))","1b56c42d":"**Data Preparation**  \nThere are two issues to be addressed in our data set before we can use the SVM classifier:  \n* The SVM classifier does not support labelled input data. Out dataset contains 2 labelled features - Embarked and Title.  \n * Previous examination of the data (assignment 2) found embarked's correlation to be a derivative of Pclass and Sex. This feature will be dropped as Pclass and Sex are retained.  \n * Title will initially be remapped to numeric values. As this implies an ordinal relationship between the values, we should test the performance of the classifier using mapping vs dropping the feature to ensure we are not degrading our prediction.\n* As support vector machines are not scale invarient, we can improve the accuracy of our model by preprocessing the dataset SciKit Learn's StandardScaler.  \n(Ref: https:\/\/scikit-learn.org\/stable\/modules\/svm.html#tips-on-practical-use)","f68850ed":"From this analysis we can determine two spikes of increased accuracy. The first at a K value of 13 and the second at 18. Moving forward we will use a neighbors value of 13.","1b9df7c0":"# 1. Preparation <a class=\"anchor\" id=\"1\"><\/a>","4d45ec60":"Dropping Title improves the accuracy of our SVM classifier.","3de942fa":"***IsChild*** - Analysis of survival across different age brackets found that children had a much higher survival rate than adults. A new feature was created to represent if the passenger is a child (i.e. 15 years or younger).","d397ae19":"**Evaluation**\n\nAs we did in the previous classifier's evaluation we will be using SciLearn's accuracy_score to evaluate the basic performance. \n","07c64204":"## 3.4 Random Forest <a class=\"anchor\" id=\"3_4\"><\/a>","c75ebcab":"### 2.2.2 Data Cleaning <a class=\"anchor\" id=\"2_2_2\"><\/a>","ce62e452":"## 1.2 Load Data from File <a class=\"anchor\" id=\"1_2\"><\/a>\nThe source data for this machine learning experimentation (titanic_train_clean.csv) has been previously cleaned and pruned during the data preparation and feature selection stages completed in assignment 2. The original source data can be found at https:\/\/www.kaggle.com\/c\/titanic\/data.\n\nA summary of the data preparation and updated data dictionary can be found in [Data Overview](#2) below.","9139823e":"**Evalulation**  \nPerformance of an SVM classifier is normally done using the classification rate or error rate. We will use SciLearn's accuracy_score to evaluate the basic performance of our model.","ff43cd76":"## 2.2 Data Preparation Summary <a class=\"anchor\" id=\"2_2\"><\/a>\nA brief summary of the data clearning, feature engineering and feature selection performed during assignment 2.","f34899a1":"# 3. Machine Learning Experimentation <a class=\"anchor\" id=\"3\"><\/a>","b896c745":"## 3.6 Logistic Regression <a class=\"anchor\" id=\"3_6\"><\/a>","281cac1c":"**Further Evaluation**\n\nWe performed further evaluation of the AdaBoost classifier using SciLearn's classification_report as we did for the previous classifier. ","cd57fbfe":"***Embarked*** - As only 2 of 891 values were missing, these were simply filled using the most common embarked value.","e3c436c1":"## 3.7 Naive Bayes <a class=\"anchor\" id=\"3_7\"><\/a>","e7eff36c":"**Data Preparation**\n\nAdaboost does not support labelled features so we need to either remove or modify those features to suit AdaBoost.\n* From previous analysis of the data from Assignment 2 we found that embarked's correlation to be a derivative of Sex and Pclass, therefor we will drop Embarked as Sex and Pclass will be kept. \n* Title will be mapped to numeric values, however just like the in the previous classifier testing we will also compare the accuracy of mapping the values to dropping the Title feature.","3a97c345":"## 3.1 Support Vector Machine <a class=\"anchor\" id=\"3_1\"><\/a>","00fb92f1":"## 3.2 AdaBoost <a class=\"anchor\" id=\"3_2\"><\/a>","09dfcbed":"***UniqueTicket*** - The original data set included the ticket number of each passenger. This field contained a significant percentage of unique values and offered little information gain. A new field was calculated to represent if the passengers ticket is unique within the dataset, or a duplicate.","32abe59f":"**Retest the accuracy after dropping Title.**","93c7a3e7":"****3.4.5.1 Visualizing unimportant features****","e739506d":"## 1.1 Import Relevant Libraries <a class=\"anchor\" id=\"1_1\"><\/a>","7a0ce9f7":"Load in librarys and format correctly.","18199a97":"**3.4.1 Import libraries, create train and test sets for Random Forest Classifier**","bdea92cd":"***Title*** - The original data included the passenger name which contained the title, first name and last name of the passenger. As each passenger name was unique the field offered very little information gain. The title of each passenger was extracted and then normalised to a defined list.","8f4ce1b1":"**3.4.5.2 Drop UniqueTicket and IsChild Features**","98b61a21":"**3.4.2 Random Forest 1: No hyperparameters**","44053eff":"## 3.5 Linear Regression <a class=\"anchor\" id=\"3_5\"><\/a>","21456a47":"**3.4.4 Random Forest: With Gridsearch tuned hyperparameters**","c17f9590":"**Retest with Dropping Title**","0d7af1ec":"We will determine the most accurate K value to use for neighbors by running tests wiht variying neighbor inputs.","11d2f618":"### 2.2.1 Feature Engineering <a class=\"anchor\" id=\"2_2_1\"><\/a>\nThree of the features contained in this dataset were engineered from the original dataset:\n* Title\n* UniqueTicket\n* IsChild","df1ed946":"### 2.2.3 Dimensionality Reduction <a class=\"anchor\" id=\"2_2_3\"><\/a>\nFive features were removed from the original data set as analysis determined they offered little or no information gain:\n* Name\n* SibSp (# of siblings or spouses also onboard)\n* Parch (# parents of children also onboard)\n* Ticket\n* Cabin","82eb7697":"# SIT307 T1 2021\n# Assignment 3 - Machine Learning Challenge\n***Group 5*** - Rhys McMillan (218335964), Brenton Fleming (217603898), Neb Miletic (218489118), Sean Pain (218137385), Oliver Bennett (218143462), Muhammad Sibtain (219345654), Asim Arshad (219337467)  \n  \n***Data*** - Titanic: Machine Learning From Disaster (https:\/\/www.kaggle.com\/c\/titanic\/data)","6dde9963":"# 2. Data Overview <a class=\"anchor\" id=\"2\"><\/a>\nA brief overview of the dataset features.\n## 2.1 Data Dictionary <a class=\"anchor\" id=\"2_1\"><\/a>\nThe following data dictionary has been updated to reflect the cleaned dataset:\n<table>\n    <tr>\n        <th>Variable<\/th>\n        <th>Definition<\/th>\n        <th>Key<\/th>\n    <\/tr>\n    <tr>\n        <td>Survived<\/td>\n        <td>Did the passenger survive?<\/td>\n        <td>1 = Yes, 0 = No<\/td>\n    <\/tr>\n    <tr>\n        <td>Pclass<\/td>\n        <td>Ticket class<\/td>\n        <td>1 = 1st, 2 = 2nd, 3 = 3rd<\/td>\n    <\/tr>\n    <tr>\n        <td>sex<\/td>\n        <td>Sex<\/td>\n        <td>1 = Female, 0 = Male<\/td>\n    <\/tr>\n    <tr>\n        <td>Age<\/td>\n        <td>Age in years<\/td>\n        <td><\/td>\n    <\/tr>\n    <tr>\n        <td>Fare<\/td>\n        <td>Passenger fare<\/td>\n        <td><\/td>\n    <\/tr>\n    <tr>\n        <td>Embarked<\/td>\n        <td>Port of Embarkation<\/td>\n        <td>C = Cherbourg, Q = Queenstown, S = Southampton<\/td>\n    <\/tr>\n    <tr>\n        <td>Title<\/td>\n        <td>Title of the passenger (extracted from name)<\/td>\n        <td><\/td>\n    <\/tr>\n    <tr>\n        <td>UniqueTicket<\/td>\n        <td>Was the passenger ticket number unique?<\/td>\n        <td>1 = Yes, 0 = No<\/td>\n    <\/tr>\n    <tr>\n        <td>IsChild<\/td>\n        <td>Is the passenger a child (15 years or younger)?<\/td>\n        <td>1 = Yes, 0 = No<\/td>\n    <\/tr>\n<\/table>","adba24b5":"***Age*** - Missing values were imputed using multivariate linear regression based on Title and Pclass.","97a99416":"**3.4.3 Random Forest: With hyperparameters (trial and error)**","e48ae2ed":"**3.4.6 Random Forest with 'non-important' features & Gridsearch Tuned Hyperparameters**","210b741b":"**Further Evaluation**  \nWe can further evaluate the performance of our SVM classifier using SciLearn's classification_report to see the precision, recall and f1-score for our model.","4c17458a":"## Table of Contents\n\n* [1. Preparation](#1)\n    * [1.1 Import Relevant Libraries](#1_1)\n    * [1.2 Load Data from File](#1_2)\n* [2. Data Overview](#2)\n    * [2.1 Data Dictionary](#2_1)\n    * [2.2 Data Preparation Summary](#2_2)\n        * [2.2.1 Feature Engineering](#2_2_1)\n        * [2.2.2 Data Cleaning](#2_2_2)\n        * [2.2.3 Dimensionality Reduction](#2_2_3)\n* [3. Machine Learning Experimentation](#3)\n    * [3.1 Support Vector Machine](#3_1)\n    * [3.2 AdaBoost](#3_2)\n    * [3.3 k-Nearest  Neighbors](#3_3)\n    * [3.4 Random Forest](#3_4)\n    * [3.5 Linear Regression](#3_5)\n    * [3.6 Classifier 6](#3_6)\n    * [3.7 Classifier 7](#3_7)","6822b713":"Two of the features in this dataset required cleaning:\n* Age\n* Embarked","c012c506":"## 3.3 K-Nearest Neighbours <a class=\"anchor\" id=\"3_3\"><\/a>"}}