{"cell_type":{"4ab50709":"code","19e04f48":"code","0196f897":"code","9b8d3bc4":"code","f4bd9891":"code","75f0f851":"code","d5cc9483":"code","ec4355b9":"code","8afe7d05":"code","a9ecd68d":"code","7592d9f2":"code","6a270f06":"code","af5a447e":"code","9d00fa1d":"code","5f894e17":"code","5d0de90d":"code","95e6ef04":"code","d9b945fd":"code","dddff3e1":"code","17a52387":"code","180e2ab0":"markdown","dae81553":"markdown","105795b4":"markdown","c2515e79":"markdown","8770940b":"markdown","0ace2e5e":"markdown","fb6b6bcc":"markdown","0dbe8a09":"markdown","84985ca4":"markdown","1834f081":"markdown"},"source":{"4ab50709":"# we need the instance normalization layer which hasn't made it to mainline keras yet\n!pip install -qq git+https:\/\/www.github.com\/keras-team\/keras-contrib.git","19e04f48":"import scipy\nfrom keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nimport datetime\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nimport os\nfrom IPython.display import Image\nfrom keras.utils.vis_utils import model_to_dot\nfrom tqdm import tqdm_notebook","0196f897":"class CycleGAN():\n    def __init__(self, img_rows, img_cols, channels_A, channels_B):\n        # Input shape\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.channels_A = channels_A\n        self.channels_B = channels_B\n        self.img_shape_A = (self.img_rows, self.img_cols, self.channels_A)\n        self.img_shape_B = (self.img_rows, self.img_cols, self.channels_B)\n        # Calculate output shape of D (PatchGAN)\n        patch_r = int(self.img_rows \/ 2**4)\n        patch_c = int(self.img_cols \/ 2**4)\n        self.disc_patch = (patch_r, patch_c, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = 32*2\n        self.df = 64*2\n\n        # Loss weights\n        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n\n        optimizer = Adam(0.0002, 0.5)\n\n        # Build and compile the discriminators\n        self.d_A = self.build_discriminator(self.img_shape_A)\n        self.d_B = self.build_discriminator(self.img_shape_B)\n        self.d_A.compile(loss='mse',\n            optimizer=optimizer,\n            metrics=['accuracy'])\n        self.d_B.compile(loss='mse',\n            optimizer=optimizer,\n            metrics=['accuracy'])\n\n        #-------------------------\n        # Construct Computational\n        #   Graph of Generators\n        #-------------------------\n\n        # Build the generators\n        self.g_AB = self.build_generator(self.img_shape_A, self.img_shape_B)\n        self.g_BA = self.build_generator(self.img_shape_B, self.img_shape_A)\n\n        # Input images from both domains\n        img_A = Input(shape=self.img_shape_A, name='ImageA')\n        img_B = Input(shape=self.img_shape_B, name='ImageB')\n\n        # Translate images to the other domain\n        fake_B = self.g_AB(img_A)\n        fake_A = self.g_BA(img_B)\n        # Translate images back to original domain\n        reconstr_A = self.g_BA(fake_B)\n        reconstr_B = self.g_AB(fake_A)\n        # Identity mapping of images\n        img_A_id = self.g_BA(img_B)\n        img_B_id = self.g_AB(img_A)\n\n        # For the combined model we will only train the generators\n        self.d_A.trainable = False\n        self.d_B.trainable = False\n\n        # Discriminators determines validity of translated images\n        valid_A = self.d_A(fake_A)\n        valid_B = self.d_B(fake_B)\n\n        # Combined model trains generators to fool discriminators\n        self.combined = Model(inputs=[img_A, img_B],\n                              outputs=[ valid_A, valid_B,\n                                        reconstr_A, reconstr_B,\n                                        img_A_id, img_B_id ])\n        self.combined.compile(loss=['mse', 'mse',\n                                    'mae', 'mae',\n                                    'mae', 'mae'],\n                            loss_weights=[  1, 1,\n                                            self.lambda_cycle, self.lambda_cycle,\n                                            self.lambda_id, self.lambda_id ],\n                            optimizer=optimizer)\n\n    def build_generator(self, in_img_shape, out_img_shape):\n        \"\"\"U-Net Generator\"\"\"\n\n        def conv2d(layer_input, filters, f_size=4):\n            \"\"\"Layers used during downsampling\"\"\"\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            d = InstanceNormalization()(d)\n            return d\n\n        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n            \"\"\"Layers used during upsampling\"\"\"\n            u = UpSampling2D(size=2)(layer_input)\n            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n            if dropout_rate:\n                u = Dropout(dropout_rate)(u)\n            u = InstanceNormalization()(u)\n            u = Concatenate()([u, skip_input])\n            return u\n\n        # Image input\n        d0 = Input(shape=in_img_shape)\n\n        # Downsampling\n        d1 = conv2d(d0, self.gf)\n        d2 = conv2d(d1, self.gf*2)\n        d3 = conv2d(d2, self.gf*4)\n        d4 = conv2d(d3, self.gf*8)\n\n        # Upsampling\n        u1 = deconv2d(d4, d3, self.gf*4)\n        u2 = deconv2d(u1, d2, self.gf*2)\n        u3 = deconv2d(u2, d1, self.gf)\n\n        u4 = UpSampling2D(size=2)(u3)\n        output_img = Conv2D(out_img_shape[-1], kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n\n        return Model(d0, output_img, name='Gen_{}_{}_{}-{}'.format(*in_img_shape, out_img_shape[-1]))\n\n    def build_discriminator(self, img_shape):\n\n        def d_layer(layer_input, filters, f_size=4, normalization=True):\n            \"\"\"Discriminator layer\"\"\"\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if normalization:\n                d = InstanceNormalization()(d)\n            return d\n\n        img = Input(shape=img_shape)\n\n        d1 = d_layer(img, self.df, normalization=False)\n        d2 = d_layer(d1, self.df*2)\n        d3 = d_layer(d2, self.df*4)\n        d4 = d_layer(d3, self.df*8)\n\n        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n\n        return Model(img, validity, name='Disc_{}_{}_{}'.format(*img_shape))","9b8d3bc4":"BATCH_SIZE = 64\nEPOCHS = 50 \nIMG_SIZE = (64, 128)","f4bd9891":"cg = CycleGAN(IMG_SIZE[0], IMG_SIZE[1], 3, 1)","75f0f851":"Image(model_to_dot(cg.combined, show_shapes=True).create_png())","d5cc9483":"Image(model_to_dot(cg.g_AB, show_shapes=True).create_png())","ec4355b9":"Image(model_to_dot(cg.d_A, show_shapes=True).create_png())","8afe7d05":"import h5py\nfrom skimage.util import montage as montage2d\nnorm_stack = lambda x: np.clip((x-127.0)\/127.0, -1, 1)\ndef norm_stack(x):\n    # calculate statistics on first 20 points\n    mean = np.mean(x[:20])\n    std = np.std(x[:20])\n    return (1.0*x-mean)\/(2*std)","a9ecd68d":"# load the data file and extract dimensions\ndata_dir = os.path.join('..', 'input', 'eye-gaze')\nwith h5py.File(os.path.join(data_dir, 'gaze.h5'),'r') as t_file:\n    print(list(t_file.keys()))\n    assert 'image' in t_file, \"Images are missing\"\n    assert 'look_vec' in t_file, \"Look vector is missing\"\n    look_vec = t_file['look_vec'][()]\n    assert 'path' in t_file, \"Paths are missing\"\n    print('Images found:',len(t_file['image']))\n    for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n        print('image',ikey,'shape:',ival.shape)\n        img_width, img_height = ival.shape\n    syn_image_stack = norm_stack(np.expand_dims(np.stack([a for a in t_file['image'].values()],0), -1))\n    print(syn_image_stack.shape, 'loaded')\nplt.matshow(montage2d(syn_image_stack[0:9, :, :, 0]), cmap = 'gray')","7592d9f2":"# load the data file and extract dimensions\nhelen_eye_dir = '..\/input\/rgb-eye-balls-hdf5\/'\nmontage_rgb = lambda x: np.clip(0.5*np.stack([montage2d(x[..., i]) for i in range(x.shape[-1])], -1)+0.5, 0, 1) \nwith h5py.File(os.path.join(helen_eye_dir,'eye_balls_rgb.h5'),'r') as t_file:\n    real_image_stack = norm_stack(t_file['image'][()])\nplt.imshow(montage_rgb(real_image_stack[0:16, :, :]))","6a270f06":"from sklearn.model_selection import train_test_split\nfrom scipy.ndimage import zoom\nclass loader_class():\n    def __init__(self, a_stack, b_stack, goal_size=None):\n        if goal_size is not None:\n            a_stack = zoom(a_stack, (1, goal_size[0]\/a_stack.shape[1], goal_size[1]\/a_stack.shape[2], 1), order=2)\n            b_stack = zoom(b_stack, (1, goal_size[0]\/b_stack.shape[1], goal_size[1]\/b_stack.shape[2], 1), order=2)\n        self.a_stack = train_test_split(a_stack, test_size=0.25, random_state=2019)\n        self.b_stack = train_test_split(b_stack, test_size=0.25, random_state=2019)\n        self.n_batches = 0\n    \n    def load_batch(self, batch_size, loops=5):\n        train_a = self.a_stack[0]\n        train_b = self.b_stack[0]\n        for _ in range(loops):\n            a_idx = np.random.permutation(np.arange(train_a.shape[0]))\n            b_idx = np.random.permutation(np.arange(train_b.shape[0]))\n            seq_len = min(a_idx.shape[0], b_idx.shape[0])\/\/batch_size*batch_size\n            for i in range(0, seq_len, batch_size):\n                self.n_batches+=1\n                c_len = min(batch_size, seq_len-i)\n                yield train_a[a_idx[i:i+c_len]], train_b[b_idx[i:i+c_len]]\n    \n    def load_data(self, domain=\"A\", batch_size=1, is_testing=False):\n        if domain==\"A\":\n            train_x, test_x = self.a_stack\n        elif domain==\"B\":\n            train_x, test_x = self.b_stack\n        else:\n            raise ValueError(\"Unknown domain\")\n        if is_testing:\n            train_x = test_x\n        out_idx = np.random.choice(range(train_x.shape[0]), size=batch_size)\n        \n        return train_x[out_idx]\n\nloader_obj = loader_class(a_stack=real_image_stack, b_stack=syn_image_stack, goal_size=IMG_SIZE)\nloader_obj.load_data(domain=\"A\", batch_size=1, is_testing=True).shape","af5a447e":"# sanity check on the tool\nfor _, (a, b) in zip(range(2), loader_obj.load_batch(8)):\n    print(a.shape, b.shape)","9d00fa1d":"def sample_images(cyc_gan, data_loader, epoch, batch_i):\n    plt.close('all')\n    r, c = 2, 3\n    np.random.seed(batch_i)\n    imgs_A = data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n    imgs_B = data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n\n    # Translate images to the other domain\n    fake_B = cyc_gan.g_AB.predict(imgs_A)\n    fake_A = cyc_gan.g_BA.predict(imgs_B)\n    # Translate back to original domain\n    reconstr_A = cyc_gan.g_BA.predict(fake_B)\n    reconstr_B = cyc_gan.g_AB.predict(fake_A)\n\n    gen_imgs = [imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B]\n\n    titles = ['Original', 'Translated', 'Reconstructed']\n    fig, axs = plt.subplots(r, c, figsize=(10, 5))\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            c_img = np.clip(0.5 * gen_imgs[cnt][0]+0.5, 0, 1)\n            if c_img.shape[-1]==1:\n                c_img = c_img[:, :, 0]\n            axs[i,j].imshow(c_img, cmap='gray', vmin=0, vmax=1)\n            axs[i, j].set_title('{} {}'.format(titles[j], 'A' if i==0 else 'B'))\n            axs[i,j].axis('off')\n            cnt += 1\n    fig.savefig(\"{:03d}_{:03d}.png\".format(epoch, batch_i))\nsample_images(cg, loader_obj, 0, 0)","5f894e17":"start_time = datetime.datetime.now()\n\n# Adversarial loss ground truths\nvalid = np.ones((BATCH_SIZE,) + cg.disc_patch)\nfake = np.zeros((BATCH_SIZE,) + cg.disc_patch)\n\nfor epoch in tqdm_notebook(range(EPOCHS), desc='Epochs'):\n    for batch_i, (imgs_A, imgs_B) in tqdm_notebook(enumerate(loader_obj.load_batch(BATCH_SIZE)), desc='Batch'):\n\n        # ----------------------\n        #  Train Discriminators\n        # ----------------------\n\n        # Translate images to opposite domain\n        fake_B = cg.g_AB.predict(imgs_A)\n        fake_A = cg.g_BA.predict(imgs_B)\n\n        # Train the discriminators (original images = real \/ translated = Fake)\n        dA_loss_real = cg.d_A.train_on_batch(imgs_A, valid)\n        dA_loss_fake = cg.d_A.train_on_batch(fake_A, fake)\n        dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n\n        dB_loss_real = cg.d_B.train_on_batch(imgs_B, valid)\n        dB_loss_fake = cg.d_B.train_on_batch(fake_B, fake)\n        dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n\n        # Total disciminator loss\n        d_loss = 0.5 * np.add(dA_loss, dB_loss)\n        # ------------------\n        #  Train Generators\n        # ------------------\n\n        # Train the generators\n        g_loss = cg.combined.train_on_batch([imgs_A, imgs_B],\n                                                [valid, valid,\n                                                imgs_A, imgs_B,\n                                                imgs_A, imgs_B])\n\n        elapsed_time = datetime.datetime.now() - start_time\n    \n    # Plot the progress at each epoch\n    print (\"[Epoch %d\/%d] [Batch %d\/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n                                                            % ( epoch, EPOCHS,\n                                                                batch_i, loader_obj.n_batches,\n                                                                d_loss[0], 100*d_loss[1],\n                                                                g_loss[0],\n                                                                np.mean(g_loss[1:3]),\n                                                                np.mean(g_loss[3:5]),\n                                                                np.mean(g_loss[5:6]),\n                                                                elapsed_time))\n    \n    sample_images(cg, loader_obj, epoch, 0)","5d0de90d":"sample_images(cg, loader_obj, EPOCHS, 1)","95e6ef04":"sample_images(cg, loader_obj, EPOCHS, 2)","d9b945fd":"sample_images(cg, loader_obj, EPOCHS, 3)","dddff3e1":"sample_images(cg, loader_obj, EPOCHS, 4)","17a52387":"sample_images(cg, loader_obj, EPOCHS, 5)","180e2ab0":"## Preview Model Output","dae81553":"# Setup CycleGAN Code\nThe code below has been lightly adapted from the code at https:\/\/github.com\/eriklindernoren\/Keras-GAN\/tree\/master\/cyclegan by @eriklindernoren","105795b4":"## Low-Res Unity Eyes\nHere we take the low-resolution unity eyes as the $A$ input to the models","c2515e79":"# Build Models","8770940b":"## Generators\nOnly show the A and $A\\rightarrow B$ since the others are trivial versions of them.","0ace2e5e":"# Goal\nHere the goal is to use cyclegan for going back and forth between low-resolution grey-scale eyes (generated in Unity) and high-resolution color eyes from real-images. It will have to learn color-ification as well as change of perspective. The idea is basically to see where these models work well and what problems frequently come up.\n\n- An interesting overview of CycleGAN can be found [here](https:\/\/www.slideshare.net\/NaverEngineering\/finding-connections-among-images-using-cyclegan)","fb6b6bcc":"# Train Model","0dbe8a09":"## Discriminator","84985ca4":"# Data Loaders\nHere we setup the data-loaders","1834f081":"## High-res RGB images\nHere we take images from the [Helen-Eye dataset](https:\/\/www.kaggle.com\/kmader\/helen-eye-dataset) as the B-stack where we want to convert to."}}