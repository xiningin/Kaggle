{"cell_type":{"d16f0fbf":"code","bb5fd720":"code","47d4e156":"code","3936c1c9":"code","77c47b40":"code","aaed314d":"code","1977c82b":"code","25c259ef":"code","ea3630f9":"code","b05df649":"code","31fb673a":"code","213fb666":"code","d773e58f":"code","5c96bb3a":"code","48211bc5":"code","5cc46cfb":"code","d01a31b7":"code","0e6f541c":"code","6d449c04":"code","e47f5039":"code","0b9a2980":"code","6eab3db2":"code","18c6dbe2":"code","dcd78543":"code","6e1f55ed":"code","0e67e716":"code","ff7d31bb":"code","eafc0d1b":"code","b2a4dd7c":"code","1d1b477e":"code","f4b27a10":"code","14667a19":"code","c0ff8841":"code","73a2cc14":"code","a93f2fd9":"code","6ee740ac":"code","ab1087c7":"code","45535dc3":"code","db8e88ad":"code","c895a403":"code","a9eae35a":"code","21eb2055":"code","fdac4fac":"code","719478b6":"code","e40a7974":"code","945fdecf":"code","55c161d0":"code","27f73a51":"code","304c82e0":"code","aee62654":"code","ecb453bd":"code","128ab834":"code","ae87d27c":"code","763e36d7":"code","55246591":"code","cdc088b3":"code","90d0a0da":"code","970479d4":"code","ed908102":"code","d507c7dd":"code","be05569d":"code","214d0653":"code","edb806da":"code","844fc34e":"code","2b06fbb5":"code","de028aff":"code","20d402bc":"code","a55fa6cb":"code","3085a3f9":"code","e395cf77":"code","6c06f8bb":"code","c715381f":"code","cd93f308":"code","5a39f757":"code","d739324c":"code","37ad15b1":"code","6b1fb307":"code","ab63487a":"code","c93d4b2a":"markdown","5af1862b":"markdown","25460984":"markdown","85095e5f":"markdown","720fd05a":"markdown","14718743":"markdown","efadfb68":"markdown","5e260b8a":"markdown","dd4b5da2":"markdown","72aec246":"markdown","58eb16cd":"markdown","76dedce1":"markdown","c95b329b":"markdown","9aa45c5b":"markdown","329362ec":"markdown","b11b28aa":"markdown","ad1f8cf0":"markdown","22890e2f":"markdown","bb851286":"markdown","45af08ea":"markdown","fa8ae3f4":"markdown","ec248361":"markdown","c8c0b9fd":"markdown","ca53d30b":"markdown","42b37139":"markdown","95500514":"markdown","59a30953":"markdown","28210a4d":"markdown","0cdb5409":"markdown","deeeba0a":"markdown","b6de790a":"markdown","109c4408":"markdown","52952480":"markdown","739f14b9":"markdown","aaca6b50":"markdown","6a41864c":"markdown","3d52ee50":"markdown","1d8229d1":"markdown","9c7f32c6":"markdown","34e1517e":"markdown","aff25d88":"markdown","a1215105":"markdown","3ea01172":"markdown","907a4cbd":"markdown","b18d365d":"markdown","dba35351":"markdown","3698425d":"markdown","9f8c8c84":"markdown","993de25a":"markdown","0f2f576f":"markdown","34ea4a1c":"markdown","2222b621":"markdown","1146e637":"markdown","2e936d6c":"markdown","505c2535":"markdown","58174388":"markdown","37145e45":"markdown","8504d5a5":"markdown","f5ef2fe0":"markdown","b54bfc76":"markdown","2e543a1f":"markdown","6e897281":"markdown","d57a911a":"markdown","cc649ebc":"markdown","2621b6b0":"markdown","fc45b2d8":"markdown","3782b1e9":"markdown","8af490cc":"markdown","0b9192af":"markdown","21a00a05":"markdown","0453fd84":"markdown","aac23722":"markdown","f9abccd9":"markdown","490ff145":"markdown","af0b147e":"markdown","779641cc":"markdown","5d2882dc":"markdown","86a396a6":"markdown","d06cf19f":"markdown"},"source":{"d16f0fbf":"from plotly.offline import init_notebook_mode, iplot_mpl, download_plotlyjs, plot, iplot\nimport plotly_express as px\nimport plotly.figure_factory as ff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport warnings\nwarnings.filterwarnings('ignore')\ninit_notebook_mode(connected=True)\nimport pandas_profiling\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix,f1_score,accuracy_score\nfrom sklearn.metrics import precision_score, roc_auc_score, recall_score, roc_curve, precision_recall_curve\nfrom pandas import DataFrame\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nimport graphviz\nfrom sklearn import tree\nfrom sklearn.tree.export import export_text\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom eli5 import show_prediction\nimport shap\nfrom sklearn.feature_extraction import DictVectorizer\nfrom yellowbrick.classifier import confusion_matrix, classification_report, ROCAUC, DiscriminationThreshold\nfrom yellowbrick.target import ClassBalance, FeatureCorrelation\nfrom yellowbrick.classifier import DiscriminationThreshold\nfrom yellowbrick.model_selection import CVScores, RFECV, LearningCurve,  ValidationCurve\n\nfrom pdpbox import pdp, info_plots\nfrom pdpbox.pdp import pdp_interact, pdp_interact_plot\n\nfrom yellowbrick.features import RadViz, PCA, pca_decomposition\nimport lime\nimport lime.lime_tabular\nplt.rcParams['figure.dpi'] = 300\n%config InlineBackend.figure_format = 'svg'\n# Need to load JS vis in the notebook\nshap.initjs() ","bb5fd720":"data=pd.read_csv(\"..\/input\/credit-card-customers\/BankChurners.csv\")\n\ndf=data.copy()\n\n#the last two columns are noise\ndf=df.iloc[:,1:-2]\n\ndf.info()","47d4e156":"pandas_profiling.ProfileReport(df)","3936c1c9":"#Avg_Open_to_Buy is highly correlated with Credit_Limit, we can delete it  \ndf.drop(['Avg_Open_To_Buy'],axis=1,inplace=True)","77c47b40":"#We create the features matrix\nX =df.iloc[:,1:]\n\n#Target DataSet\ny=df['Attrition_Flag']\n\n#we replace the target vector with binary values\ny=y.replace({'Existing Customer': 0,\"Attrited Customer\": 1})","aaed314d":"df[['Attrition_Flag']].head(3)","1977c82b":"fig = px.pie(df, 'Attrition_Flag', title='Pie Chart Target distribution', \n             color_discrete_sequence=px.colors.qualitative.Safe).update_traces(hoverinfo='label+value', \n  textinfo='percent', textfont_size=16,\n).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","25c259ef":"d=df[['Gender','Education_Level','Marital_Status','Card_Category','Income_Category']]\nd['churn']=y\nd.head(3)","ea3630f9":"#We transform the discrete features in dummy variables\nd=pd.get_dummies(d)\n\n#We drop the duplicated dummy variable gender\nd.drop(['Gender_F'],axis=1,inplace=True)","b05df649":"fig, axes, summary_df = info_plots.target_plot(\n    df=d, feature='Gender_M', feature_name='Gender', target='churn',figsize=(11, 7))\n_ = axes['bar_ax'].set_xticklabels(['Female', 'Male'])","31fb673a":"fig, axes, summary_df = info_plots.target_plot(\n    df=d, feature=['Education_Level_College',\n       'Education_Level_Doctorate', 'Education_Level_Graduate',\n       'Education_Level_High School', 'Education_Level_Post-Graduate',\n       'Education_Level_Uneducated', 'Education_Level_Unknown'], feature_name='Education', \n    target='churn',figsize=(11, 7)\n)\n_ = axes['bar_ax'].set_xticklabels(['College','Doctorate','Graduate','High School','Post-Graduate','Uneducated','Unknown'])","213fb666":"fig, axes, summary_df = info_plots.target_plot(\n    df=d, feature=['Marital_Status_Divorced', 'Marital_Status_Married',\n       'Marital_Status_Single', 'Marital_Status_Unknown',], feature_name='Marital_Status', \n    target='churn',figsize=(11, 7)\n)\n_ = axes['bar_ax'].set_xticklabels(['Divorced','Married','Single','Unknown'])","d773e58f":"fig, axes, summary_df = info_plots.target_plot(\n    df=d, feature=['Card_Category_Blue',\n       'Card_Category_Gold', 'Card_Category_Platinum', 'Card_Category_Silver',], feature_name='Card-Category', \n    target='churn',figsize=(11, 7)\n)\n_ = axes['bar_ax'].set_xticklabels(['Category_Blue','Category_Gold','Category_Platinum','Category_Silver'])","5c96bb3a":"fig, axes, summary_df = info_plots.target_plot(\n    df=d, feature=['Income_Category_$120K +', 'Income_Category_$40K - $60K',\n       'Income_Category_$60K - $80K', 'Income_Category_$80K - $120K',\n       'Income_Category_Less than $40K', 'Income_Category_Unknown'], feature_name='Income_Category', \n    target='churn',figsize=(11, 7)\n)\n_ = axes['bar_ax'].set_xticklabels(['$120K','$40K - $60K','$60K - $80K','$80K - $120K',\n                                    'Less than $40K','Unknown'])","48211bc5":"ec=df[df['Attrition_Flag']=='Existing Customer']\nac=df[df['Attrition_Flag']=='Attrited Customer']\n\nfig = make_subplots(rows=7, cols=2, subplot_titles=(\"Customer_Age\",\"Dependent_count\",\n \"Months_on_book\",\"Total_Relationship_Count\",\"Months_Inactive_12_mon\",\n\"Contacts_Count_12_mon\",\"Credit_Limit\",\"Total_Revolving_Bal\",\n \"Total_Amt_Chng_Q4_Q1_1\",\"Total_Trans_Amt\",\"Total_Trans_Ct\",\n\"Total_Ct_Chng_Q4_Q1\",\"Avg_Utilization_Ratio\"))\n\nclass0=\"0\"\nclass1=\"1\"\n\nCustomer_Age_0 = go.Box(x=ec.Customer_Age,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nCustomer_Age_1 = go.Box(x=ac.Customer_Age,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\nDependent_count_0 = go.Box(x=ec.Dependent_count,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nDependent_count_1 = go.Box(x=ac.Dependent_count,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\nMonths_on_book_0 = go.Box(x=ec.Months_on_book,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nMonths_on_book_1 = go.Box(x=ac.Months_on_book,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\nTotal_Relationship_Count_0 = go.Box(x=ec.Total_Relationship_Count,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nTotal_Relationship_Count_1 = go.Box(x=ac.Total_Relationship_Count,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\n\nMonths_Inactive_12_mon_0 = go.Box(x=ec.Months_Inactive_12_mon,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nMonths_Inactive_12_mon_1 = go.Box(x=ac.Months_Inactive_12_mon,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\n\nContacts_Count_12_mon_0 = go.Box(x=ec.Contacts_Count_12_mon,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nContacts_Count_12_mon_1 = go.Box(x=ac.Contacts_Count_12_mon,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\n\nCredit_Limit_0 = go.Box(x=ec.Credit_Limit,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nCredit_Limit_1 = go.Box(x=ac.Credit_Limit,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\nTotal_Revolving_Bal_0 = go.Box(x=ec.Total_Revolving_Bal,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nTotal_Revolving_Bal_1 = go.Box(x=ac.Total_Revolving_Bal,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\nTotal_Amt_Chng_Q4_Q1_0 = go.Box(x=ec.Total_Amt_Chng_Q4_Q1,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nTotal_Amt_Chng_Q4_Q1_1 = go.Box(x=ac.Total_Amt_Chng_Q4_Q1,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\nTotal_Trans_Amt_0 = go.Box(x=ec.Total_Trans_Amt,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nTotal_Trans_Amt_1 = go.Box(x=ac.Total_Trans_Amt,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\n\nTotal_Trans_Ct_0 = go.Box(x=ec.Total_Trans_Ct,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nTotal_Trans_Ct_1 = go.Box(x=ac.Total_Trans_Ct,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\nTotal_Ct_Chng_Q4_Q1_0 = go.Box(x=ec.Total_Ct_Chng_Q4_Q1,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nTotal_Ct_Chng_Q4_Q1_1 = go.Box(x=ac.Total_Ct_Chng_Q4_Q1,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\nAvg_Utilization_Ratio_0 = go.Box(x=ec.Avg_Utilization_Ratio,name=class0,marker_color='rgba(93, 164, 214, 0.5)')\nAvg_Utilization_Ratio_1 = go.Box(x=ac.Avg_Utilization_Ratio,name=class1,marker_color='rgba(255, 144, 14, 0.5)')\n\n\nfig.append_trace(Customer_Age_0, 1, 1)\nfig.append_trace(Customer_Age_1, 1, 1)\n\nfig.append_trace(Dependent_count_0, 1, 2)\nfig.append_trace(Dependent_count_1, 1, 2)\n\nfig.append_trace(Months_on_book_0, 2, 1)\nfig.append_trace(Months_on_book_1, 2, 1)\n\nfig.append_trace(Total_Relationship_Count_0, 2, 2)\nfig.append_trace(Total_Relationship_Count_1, 2, 2)\n\nfig.append_trace(Months_Inactive_12_mon_0, 3, 1)\nfig.append_trace(Months_Inactive_12_mon_1, 3, 1)\n\nfig.append_trace(Contacts_Count_12_mon_0, 3, 2)\nfig.append_trace(Contacts_Count_12_mon_1, 3, 2)\n\nfig.append_trace(Credit_Limit_0, 4, 1)\nfig.append_trace(Credit_Limit_1, 4, 1)\n\nfig.append_trace(Total_Revolving_Bal_0, 4, 2)\nfig.append_trace(Total_Revolving_Bal_1, 4, 2)\n\n\nfig.append_trace(Total_Amt_Chng_Q4_Q1_0, 5, 1)\nfig.append_trace(Total_Amt_Chng_Q4_Q1_1, 5, 1)\n\n\nfig.append_trace(Total_Trans_Amt_0, 5, 2)\nfig.append_trace(Total_Trans_Amt_1, 5, 2)\n\nfig.append_trace(Total_Trans_Ct_0, 6, 1)\nfig.append_trace(Total_Trans_Ct_1, 6, 1)\n\nfig.append_trace(Total_Ct_Chng_Q4_Q1_0 , 6, 2)\nfig.append_trace(Total_Ct_Chng_Q4_Q1_1, 6, 2)\n\nfig.append_trace(Avg_Utilization_Ratio_0 , 7, 1)\nfig.append_trace(Avg_Utilization_Ratio_1, 7, 1)\n\n\nfig.update_layout(height=1500, width=700,\n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)', showlegend=False)\n\nfig.update_layout(title_text=\"Numerical variables Box-plot distribution per class {no churn: 0 & churn: 1}\")\n\nfig.show()\n","5cc46cfb":"n=X[['Customer_Age', 'Dependent_count', 'Months_on_book',\n       'Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',\n       'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Trans_Ct',\n       'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']]\nn['churn']=y","d01a31b7":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Customer_Age', feature_name='Customer_Age', \n    target='churn',figsize=(11, 7)\n)","0e6f541c":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Dependent_count', feature_name='Dependent_count', \n    target='churn',figsize=(11, 7)\n)","6d449c04":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Months_on_book', feature_name='Months_on_book', \n    target='churn',figsize=(11, 7)\n)","e47f5039":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Total_Relationship_Count', feature_name='Total_Relationship_Count', \n    target='churn',figsize=(11, 7)\n)","0b9a2980":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Months_Inactive_12_mon', feature_name='Months_Inactive_12_mon', \n    target='churn',figsize=(11, 7)\n)","6eab3db2":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Contacts_Count_12_mon', feature_name='Contacts_Count_12_mon', \n    target='churn',figsize=(11, 7)\n)","18c6dbe2":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Total_Revolving_Bal', feature_name='Total_Revolving_Bal', \n    target='churn',figsize=(11, 7)\n)","dcd78543":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Total_Amt_Chng_Q4_Q1', feature_name='Total_Amt_Chng_Q4_Q1', \n    target='churn',figsize=(11, 7)\n)","6e1f55ed":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Total_Trans_Amt',num_grid_points=8, feature_name='Total_Trans_Amt', \n    target='churn',figsize=(11, 7)\n)","0e67e716":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Total_Trans_Ct', feature_name='Total_Trans_Ct', \n    target='churn',figsize=(11, 7)\n)","ff7d31bb":"fig, axes, summary_df = info_plots.target_plot(\n    df=n, feature='Avg_Utilization_Ratio', feature_name='Avg_Utilization_Ratio', \n    target='churn',figsize=(11, 7)\n)","eafc0d1b":"X.head()","b2a4dd7c":"X['Av_Trans']=X['Total_Trans_Amt']\/X['Total_Trans_Ct']","1d1b477e":"X['Av_Trans']","f4b27a10":"X['Amt_used_per_Limit']=X['Total_Trans_Amt']\/X['Credit_Limit']","14667a19":"X['Amt_used_per_Limit']","c0ff8841":"X['Dis_Total_Trans_Amt']=pd.cut(X['Total_Trans_Amt'], [0,10584,18485], include_lowest=False)","73a2cc14":"X['Dis_Total_Trans_Amt']","a93f2fd9":"X.head()","6ee740ac":"#We transform the discrete features into dummy variables \nX=pd.get_dummies(X)\n\n#We drop the duplicated dummy variable gender\nX.drop(['Gender_F'],axis=1,inplace=True)","ab1087c7":"X.head()","45535dc3":"# Instaniate the visualizer\nvisualizer = FeatureCorrelation(\n    method='mutual_info-classification', feature_names=X.columns.tolist(), sort=True,\nsize=(2900, 3000))\n\nvisualizer.fit(X, y,)              # Fit the data to the visualizer\nvisualizer.show()                  # Finalize and render the figure","db8e88ad":"visualizer = PCA(scale=True, proj_features=True,size=(3000, 2000),alpha=0.75)\nvisualizer.fit_transform(X, y)\nvisualizer.show()","c895a403":"# Specify the target classes\nclasses = [\"no churn\", \"churn\"]\n\n# Instantiate the visualizer\nvisualizer = RadViz(classes=classes,size=(2400, 3000),alpha=0.2)\n\nvisualizer.fit(X, y)           # Fit the data to the visualizer\nvisualizer.transform(X)        # Transform the data\nvisualizer.show()              # Finalize and render the figure","a9eae35a":"# Stratified Train_test split\nrandom_state = 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = random_state, stratify = y)","21eb2055":"model = DecisionTreeClassifier(class_weight='balanced')\nmodel.fit(X_train, y_train)","fdac4fac":"tree_rules = export_text(model,show_weights=True, feature_names=list(X.columns))\nprint(tree_rules)","719478b6":"# DOT data\ndot_data = tree.export_graphviz(model, out_file=None, \n                                feature_names=list(X.columns),class_names=y.astype(str),\n                                max_depth=2,proportion=False, filled=True,)\n\n# Draw graph\ngraph = graphviz.Source(dot_data, format=\"png\") \ngraph","e40a7974":"# define evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=2)\n# evaluate model\nscores = cross_val_score(model, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)","945fdecf":"scores.mean()","55c161d0":"visualizer = CVScores(model, cv=cv, scoring='f1', random_state=2, size=(5500, 1000))\n\nvisualizer.fit(X_train, y_train) # Fit the data to the visualizer\n\nplt.tick_params(\n    axis='x',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected\n    bottom=False,      # ticks along the bottom edge are off\n    top=False,         # ticks along the top edge are off\n    labelbottom=False) \n\nvisualizer.show() \n# Finalize and render the figure","27f73a51":"# Instantiate the visualizer with the classification model\nconfusion_matrix(\n    model,\n    X_train, y_train, X_test, y_test,\n    classes=['no churn', 'churn']\n)\nplt.show()","304c82e0":"# Instantiate the visualizer\nvisualizer = classification_report(model, X_train, y_train, X_test, y_test, \n                                   classes=['non_churn', 'churn'], support=True)","aee62654":"# define grid\nparameters = {'min_samples_leaf' : range(1,10,1),'min_samples_split' : range(1,10,1),\n              'max_depth': range(1,30,2),'criterion':['gini','entropy']}\n\n# define evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=2)\n\n# define grid search\ngrid = GridSearchCV(estimator=model, param_grid=parameters, n_jobs=-1, cv=cv, scoring='f1')\n\n# execute the grid search\ngrid_result = grid.fit(X_train, y_train)","ecb453bd":"viz = ValidationCurve(\n    model, param_name=\"max_depth\", param_range=range(1,10,1),\n    logx=True, cv=cv, scoring=\"f1\", n_jobs=8,size=(3700, 2000))\n\nviz.fit(X, y)\nviz.show()","128ab834":"# Instantiate the visualizer with the classification model\nconfusion_matrix(\n    grid_result,\n    X_train, y_train, X_test, y_test,\n    classes=['non_churn', 'churn']\n)\nplt.show()","ae87d27c":"# Instantiate the visualizer\nvisualizer = classification_report(grid_result, X_train, y_train, X_test, y_test, \n                                   classes=['non_churn', 'churn'], support=True)","763e36d7":"model1 = RandomForestClassifier(random_state=0,n_estimators=1000,class_weight='balanced')\n\nmodel1.fit(X_train, y_train)\n\n# define evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=2)\n\n# evaluate model\nscores = cross_val_score(model1, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n\nscores.mean()","55246591":"# Create the learning curve visualizer\nsizes = np.linspace(0.3, 1.0, 20)\n\n# Instantiate the visualizer\nvisualizer = LearningCurve(\n    model1, cv=cv, scoring='f1', train_sizes=sizes, n_jobs=-1,size=(3700, 2000))\n\nvisualizer.fit(X_train, y_train)        # Fit the data to the visualizer\nvisualizer.show()                       # Finalize and render the figu","cdc088b3":"# Instantiate the visualizer with the classification model\nconfusion_matrix(\n    model1,\n    X_train, y_train, X_test, y_test,\n    classes=['non_churn', 'churn']\n)\nplt.show()","90d0a0da":"# Instantiate the visualizer\nvisualizer = classification_report(model1, X_train, y_train, X_test, y_test, \n                                   classes=['non_churn', 'churn'], support=True)","970479d4":"predicted_proba = model1.predict_proba(X_test)\npredicted_proba ","ed908102":"#scores per model\ny_score_ = model.predict_proba(X_test)[:,1]\ny_score = grid_result.predict_proba(X_test)[:,1]\nm1_y_score=model1.predict_proba(X_test)[:,1]\n\n# Precision-recall curve\nprecision_, recall_, thresholds_ = precision_recall_curve(y_test, y_score_)\nprecision, recall, thresholds = precision_recall_curve(y_test, y_score)\nm1_precision,m1_recall,m1_tresholds=precision_recall_curve(y_test, m1_y_score)\n\n#Models Data\nframe = { 'Precision': precision, 'Recall': recall} \npc=pd.DataFrame(frame) \n\nframe_ = { 'Precision_': precision_, 'Recall_': recall_} \npc_=pd.DataFrame(frame_) \n\nframe__ = { 'Precision': m1_precision, 'Recall': m1_recall} \nm1=pd.DataFrame(frame__) \n\n\n#figure\nfig = go.Figure()\n  \nfig.add_trace(go.Scatter(x=pc.Recall,y=pc.Precision, fill='tozeroy',text='Grid Search CV model',\n                         name='Grid-SearchCV DT')) # fill down to xaxis\nfig.add_trace(go.Scatter(x=pc_.Recall_, y=pc_.Precision_, fill='tozeroy',text=\"Default decision tree model\",\n                         name=\"Default Balanced DT\")) # fill to trace0 y\n\nfig.add_trace(go.Scatter(x=m1.Recall, y=m1.Precision, fill='tonexty',text=\"Random Forest model\",\n                         name=\"Random Forest\")) # fill to trace0 y\n\n\nfig.update_layout(\n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)', showlegend=True)\n\nfig.update_layout(title_text=\"Precision-Recall curve\",xaxis_title=\"Recall\",\n    yaxis_title=\"Precision\",\n    legend_title=\"MODELS\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"RebeccaPurple\"\n    )\n)\n\nfig.show()","d507c7dd":"tresholds=np.linspace(0,1,100).tolist()\npre=[]\nrec=[]\n\nfor threshold in np.linspace(0,1,100):\n    predicted = (predicted_proba [:,1] >= threshold).astype('int')\n    \n    rec.append(recall_score(y_test, predicted).tolist())\n    pre.append(precision_score(y_test,predicted).tolist())\n    \ntr= pd.DataFrame({'tresholds': tresholds,'precision':pre,'recall':rec})\ntr=tr.iloc[:-1,:]\n\n\n\n#figure\nfig = go.Figure()\n\n\nfig.add_trace(go.Scatter(x=tr.tresholds, y=tr.precision, fill='tonexty',text=\"Precision\",\n                         name=\"Precision\")) # fill to trace0 y\n\n\nfig.add_trace(go.Scatter(x=tr.tresholds, y=tr.recall, fill='tozeroy',text=\"Recall\",\n                         name=\"Recall\")) # fill to trace0 y\n\nfig.update_layout(\n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)', showlegend=True)\n\nfig.update_layout(title_text=\"Precision-Recall vs Threshold\",xaxis_title=\"Threshold\",\n    yaxis_title=\"Precision \/ Recall\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"RebeccaPurple\"\n    )\n                 )\n\nfig.add_shape(type='line',\n                x0=0.326,\n                y0=0,\n                x1=0.326,\n                y1=1,\n                line=dict(color='Red', dash=\"dot\"),\n                xref='x',\n                yref='y')\n\nfig.add_annotation(\n        x=0.326,\n        y=0.90,\n        xref=\"x\",\n        yref=\"y\",\n        text=\"Recall 0.90\",\n        showarrow=True,\n        font=dict(\n            family=\"Courier New, monospace\",\n            size=16,\n            color=\"#ffffff\"\n            ),\n        align=\"center\",\n        arrowhead=2,\n        arrowsize=1,\n        arrowwidth=2,\n        arrowcolor=\"#636363\",\n        ax=20,\n        ay=-30,\n        bordercolor=\"#c7c7c7\",\n        borderwidth=2,\n        borderpad=4,\n        bgcolor=\"coral\",\n        opacity=0.8\n        )\n\nfig.add_annotation(\n        x=0.326,\n        y=0,\n        xref=\"x\",\n        yref=\"y\",\n        text=\"Probability Threshold 0.33\",\n        showarrow=True,\n        font=dict(\n            family=\"Courier New, monospace\",\n            size=16,\n            color=\"#ffffff\"\n            ),\n        align=\"center\",\n        arrowhead=2,\n        arrowsize=1,\n        arrowwidth=2,\n        arrowcolor=\"#636363\",\n        ax=20,\n        ay=-30,\n        bordercolor=\"#c7c7c7\",\n        borderwidth=2,\n        borderpad=4,\n        bgcolor=\"violet\",\n        opacity=0.8\n        )\n\nfig.show()","be05569d":"# Instantiate the classification model and visualizer\n\nvisualizer = DiscriminationThreshold(model1,size=(3700, 2000)) \nvisualizer.fit(X_train, y_train)        # Fit the data to the visualizer      # Final\nvisualizer.show()","214d0653":"# y_pred = clf.predict(X_test)  # default threshold is 0.5\ny_pred = (model1.predict_proba(X_test)[:,1] >= 0.3265306) # set threshold as 0.326\n\npred=pd.DataFrame({'y_pred':y_pred.tolist(),'y_test':y_test})\npred.y_pred=pred.y_pred.replace({False: 0,True: 1})\n\nrecall_score(pred.y_test, pred.y_pred)","edb806da":"precision_score(pred.y_test, pred.y_pred)","844fc34e":"eli5.show_weights(model1, feature_names = X_test.columns.tolist())","2b06fbb5":"from pdpbox import pdp, info_plots\nfrom pdpbox.pdp import pdp_interact, pdp_interact_plot\n\n# Create the data that we will plot\npdp_Total_Trans_C = pdp.pdp_isolate(model=model1, dataset=X_train, model_features=X_train.columns,\n                                    feature='Total_Trans_Ct')\n\nfig, axes = pdp.pdp_plot(pdp_Total_Trans_C, 'Total_Trans_Ct',figsize=(12, 7), plot_pts_dist=True\n)\nplt.show()","de028aff":"# Create the data that we will plot\nTotal_Trans_Amt  = pdp.pdp_isolate(model=model1, dataset=X_test, model_features=X_test.columns.tolist(),\n                                   feature='Total_Trans_Amt',num_grid_points=50)\n\n# plot it\npdp.pdp_plot(Total_Trans_Amt, 'Total_Trans_Amt',plot_pts_dist=True,figsize=(12, 7))\nplt.show()","20d402bc":"fig, axes, summary_df = info_plots.actual_plot(\nmodel=model1, X=X_test, feature='Total_Trans_Ct',figsize=(12, 7),\n    feature_name='Total_Trans_Ct',predict_kwds={}\n)","a55fa6cb":"fig, axes, summary_df = info_plots.actual_plot(\nmodel=model1, X=X_test, feature='Total_Trans_Amt',figsize=(12, 7),\n    feature_name='Total_Trans_Amt',predict_kwds={}\n)","3085a3f9":"# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\nfeatures = ['Total_Trans_Amt', 'Total_Trans_Ct']\ninteraction  =  pdp_interact(model=model1, dataset=X_test, \n                                 model_features=X_test.columns.tolist(), features=features)\n\npdp = interaction.pdp.pivot_table(\n    values='preds', \n    columns=features[0], \n    index=features[1]\n)[::-1] # Slice notation to reverse index order so y axis is ascending\n\n\nsns.heatmap(pdp, annot=True, fmt='.2f', cmap='Greens')\nplt.title('Partial Dependence on Total_Trans_Amt & Total_Trans_Ct');\n","e395cf77":"fig, axes, summary_df = info_plots.actual_plot_interact(\n    model=model1, X=X_test, features=['Total_Trans_Amt', 'Total_Trans_Ct'], feature_names=['Total_Trans_Amt', 'Total_Trans_Ct']\n,figsize=(11, 12)\n)\n","6c06f8bb":"perm = PermutationImportance(model1, random_state=1).fit(X_test, y_test)\n\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","c715381f":"explainer = shap.TreeExplainer(model1)\nshap_values = explainer.shap_values(X_test)","cd93f308":"shap.summary_plot(shap_values[1], X_test,alpha=0.75,max_display=30)","5a39f757":"data_row = 523\ndata_row = X_test.iloc[data_row]\nprint(\"Predicted probability\\nNon-Churn - Churn\\n\", model1.predict_proba(data_row.values.reshape(1, -1)))\nshap_values = explainer.shap_values(data_row)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_row)","d739324c":"shap.decision_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[523,:])","37ad15b1":"predict_fn_rf = lambda x: model1.predict_proba(x).astype(float)\nX_ = X_train.values\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_,feature_names = X_train.columns,class_names=['No churn',\"Churn\"],kernel_width=5)","6b1fb307":"j=523\nchoosen_instance = X_test.iloc[[j]].values[0]\nexp = explainer.explain_instance(choosen_instance, predict_fn_rf, num_features=20)\nexp.show_in_notebook(show_table=True)","ab63487a":"show_prediction(model1, X_test.iloc[523],feature_names = X_test.columns.tolist(), \n                show_feature_values=True)","c93d4b2a":"#  <a id='#1'>4. Decision Trees for Imbalanced Classification <\/a>","5af1862b":"The data have a similar number of men and women. Women have almost 3 points more average churn than men.","25460984":"We observe how those clients with a high number of transactions do not usually churn the services. There is no churn observation with a Total Transaction Amount> 10584. (see box-plots distribution) We can generate a discrete variable that cuts these continuous data into two intervals (0, 10584] and (10584, 18485]","85095e5f":"We observe that the maximum churn prediction occurs when the total of transactions is over 44 and the amount is 2699.","720fd05a":"This figure tells us  how effective an estimator is on the data that it has been trained on as well as how generalizable it is to new input to different hyperparameter ranges (max depth) on the training and test data. ","14718743":"Using the variables Total_Trans_Amt and Total_Trans_Ct we can generate a variable that reproduces the average transaction made by user.","efadfb68":"# <a id='#1'> 6. Random forest model <\/a>\n\nRandom Forests consist of many decision trees. To classify a new instance, each decision tree provides a classification for the input data; random forest collects all the rankings and chooses the most voted prediction as a result. In addition, a subset of features is randomly selected from the optional features to grow the tree at each node. Each tree is grown without pruning. Basically, the random forest allows a large number of weak or weakly correlated classifiers but forms a very strong joint classifier.\n\n- We will generate the model with 1000 trees.","5e260b8a":"### <a id='#1'> 4.3 Evaluation with cross-validation <\/a>\n\nThe Cross-Validation function randomly divides the training set into K distinct subsets and train and evaluate the decision tree model k times, choosing a different subset for evaluation each time and training in the other k subset folds. The result is a matrix with the k evaluation metrics. We will divide the data set into 10 subsets (k = 10).","dd4b5da2":"The partial dependency graphs show us if the churn probability increases or decreases with the different values of the variables. Negative values (on the y-axis) mean that according to the model the positive churn class is less likely and positive values mean that the positive class is more likely. Values at 0 do not imply any impact on the class probability.\n   \n   If the partial dependence of a variable is close to zero for the entire range of the variable, this indicates that the model has no relationship between the variable and the target class. The higher the range, the greater the influence of the variable on the class probability.","72aec246":"### <a id='#1'>1.1 Pandas Profiling Report<\/a>","58eb16cd":"We observe that in PCA the data are separated into two clusters. In the cluster with high PC1 values, all observations are without churn. Each feature of the set is projected to its direction of maximum variance. We see that Total_Trans_Ct, Total_rans_Amt and the dummy variable of Total_Trans_Amt are projected onto this cluster.","76dedce1":"The Random Forest gets a much higher f-1 score than previous models.","c95b329b":"### <a id='#1'> 4.4 Model Testing: Confusion Matrix and Classification Report <\/a>\n\nWe analyze the classificatory performance of our model by examining the confusion matrix and the recall and precision of the model. The confusion matrix gives us the false negatives (lower left value) and false positives (upper right value). Correctly classified values are on the main diagonal. (class 0 as 0 and class 1 as 1)","9aa45c5b":"![1%20ZDa8yIJ_toq2r0zzQQ4fVA.jpeg](attachment:1%20ZDa8yIJ_toq2r0zzQQ4fVA.jpeg)","329362ec":"We observe that the platinum card users have the highest average churn and the silver ones the least, but there are only 20 platinum observations, so it may be statistical noise. The vast majority of observations have the Blue card.","b11b28aa":"**PRECISION**\n\nPrecision is defined as the number of true positives divided by the number of true positives plus the number of false positives. False positives are instances that the model incorrectly labels \"churn\" when they are actually negative. It expresses the proportion of the data points that our model says are relevant when in fact they were relevant.\n\n\n**RECALL**\n\nThe recall is the number of true positives divided by the number of true positives plus the number of false negatives. True positives are data points classified as positive by the model that are actually positive (correct), and false negatives are data points that the model identifies as negative that are actually positive (incorrect). In the case of churn, the true positives are customers who left credit card services and are correctly identified, and the false negatives would be individuals labeled \"no churn\" by the model, but who actually left the services. It expresses the ability to find all relevant instances in a data set.\n\n**F1-SCORE**\n\nIn some situations, we may want to maximize recall or precision at the expense of the other metric. In the case that we are trying to \"churn\" customers, we can accept a lower precision if we can obtain a higher recall (we want to find many customers who intend to abandon credit card services). However, in cases where we want to find an optimal combination of precision and recall, we can combine the two metrics using what is called an F1 score.\n\nThe F1 score is the harmonic mean of precision and recall taking into account both metrics. The harmonic mean is used instead of a simple average because it penalizes extreme values.","ad1f8cf0":"###  <a id='#1'>2.1 Target <\/a>","22890e2f":"The Yellobrick package offers us the same visualization with the f1 score included. The maximum f1 score is achieved with the 0.35 treshold.","bb851286":"As the results may vary due to the stochastic nature of the evaluation procedure, we run the Cross-validation multiple times and compute the mean of the result. We observe that the model has obtained a mean of 0.79 of the metric f1.","45af08ea":"# <a id='#1'> 11 Permutation importance <\/a>\n\nIn permutation importance we ask ourselves the question, if we randomly mix a feature of the validation data, leaving the target variable and all other features the same, how would that affect the accuracy of the model predictions with that attribute with mixed data?\n\nIt is clear that randomly rearranging an attribute in the model will produce less accurate predictions, since the processed data does not correspond to actual observations. The accuracy of the model will be particularly affected if we mix in a variable that the model relied heavily on for predictions. In this case, changing the Total_Trans_Ct will cause bad predictions. The importance of each attribute, then, will be computed from the deterioration of the model's performance.\n\nThus, with this systematic approeach, we compute the importance of each feature for the predictive power of the model.","fa8ae3f4":"# <a id='#1'>1. DataSet preparation<\/a>","ec248361":"# <a id='#1'> 12. Model interpretability with Shapley Additive explanations (SHAP) <\/a>\n\nThe Sahpley Additive explanations are game theory approach to explain the result of any machine learning model. Tree SHAP allows us to give an explanation of the behavior of the model, in particular of how each feature impacts the predictions of the model. Each outcome \/ prediction is viewed as a sum of the contribution of each individual feature.\n\nThe idea behind SHAP is to provide full interpretability to machine learning models.\n\n- The **global interpretability** seeks to understand the general structure of the model. This involves doing a study on how the model works in general, not just on a specific prediction.\n\n\n- The **local interpretability** of the models is to provide detailed explanations of why an individual prediction was made. This helps stakeholders to trust the model and know how to integrate its recommendations with other decision factors.\n\n### <a id='#1'> 12.1 Global interpretability with Summary plot <\/a>\nThis figure gives us a huge amount of information about the structure of the model. In the figure we can find:\n\n- Most important attribute of the model on the y-axis in descending order (at the top, the most important).\n\n\n- The SHAP value of the observations on the x-axis and shows whether the effect of that value caused a higher or lower prediction.\n\n \n- The value of each attribute with colors. A high value is represented by red, while a low value is represented by blue.\n\n\n- Each point represents a result of a prediction.\n","c8c0b9fd":"Using the variables Total_Trans_Amt and Credit_Limit we can generate a new variable that denotes the amount used by customer in relation to their credit limit.","ca53d30b":"With this threehold, the recall score is 0.9 and we managed to capture 90% of the customers with churn in the test Set data set, but, of course,  we sacrifice the precision we had with the threshold of 0.5, since we have a precision of 0.84.","42b37139":"We have 39 features. Many features may not be useful \/ important but we will use them all so as not to detract predictive power to our model.","95500514":"### <a id='#1'> Decision Trees <\/a>\n\nDecision trees are supervised learning algorithms that are used for both classification and regression tasks. We can use decision trees for data sets in which we have both continuous and discrete attributes. The main idea of decision-trees is to find the descriptive features that contain the most \"information\" about the target class that we want to predict and then divide the data set along the values of these features so that the values of the target characteristics for the sub_datasets are as pure as possible.\n\n\nThis process of finding the \"most informative\" feature is done until we reach a stopping criterion where we finally end up in so-called leaf nodes. The leaf nodes contain the predictions that we will make for the new query instances presented to our trained model. This becomes possible since the model has learned the underlying structure of the data and can therefore, given some assumptions, can make predictions about the target class value of the query instances.\n\nA decision tree mainly contains a root node, interior nodes, and leaf nodes that are then connected by branches.","59a30953":"Here we can examine the score for each of the cross-validation validations. (in 10 repetitions of 10 fold cross-validation).","28210a4d":"In the interval [0,0.07) the average churn is 30%.","0cdb5409":"### <a id='#1'> 3.1 We transform discrete variables into dummy variables <\/a>\n\nScikit learn's Decision Tress algorithm cannot process discrete variables, so we have to convert discrete attributes to dummy variables.","deeeba0a":"The Lime package also details how the probabilities of churn or not vary according to the value of each attribute. If the attribute color is orange, it drives the client's churn probability, if it is blue, it drives the no churn probability.","b6de790a":"We did not observe very significant differences with age. The youngest age groups are the ones with the least average churn.","109c4408":"We observe that from total_Trans_Ct > 54, the churn average goes down a lot, becoming almosty 0 from total_Trans_Ct > 90.","52952480":"### <a id='#1'> 5.1 Confusion Matrix & Classification Report<\/a>","739f14b9":"We observed that the model has relatively many more false negatives (\"churn\" clients who were not identified as such) than false positives (clients who did not leave the services but were identified as \"churn\".)","aaca6b50":"We can also examine the partial dependencies of interactions between features:","6a41864c":"### <a id='#1'> 3.3 X Circumference Projection <\/a>\n\nRadViz is a multivariate data visualization algorithm that plots each attribute uniformly around the circumference of a circle and then plots points inside the circle such that the point normalizes its values on the axes from the center to each arc. This mechanism allows as many dimensions as can easily fit in a circle, expanding the dimensionality of the display. It gives us an idea of the separability between classes.","3d52ee50":"### <a id='#1'>4.2 Default class-balanced Decision Tree<\/a>","1d8229d1":"### <a id='#1'> 4.1 Stratified train test split <\/a>\n\nWe keep 20% of the observations as test set (out of sample data) to be able to predict with samples not used in training and to have an estimate of the errors. With the stratify = y method, we make sure that the two data sets have the same per-rate per class as the original data set.","9c7f32c6":"# <a id='#1'>2. Exploratory data analysis <\/a>","34e1517e":"# <a id='#1'> 13. Conclusions <\/a>\n\nIn this project, decision-trees classificatory algorithms have been built to predict the churn of customers of a bank in relation to credit card services.\n\n- In sections 1,2 and 3, the features and their relationship and dependence with the target class have been studied.\n\n\n- In sections 4,5,6, 7 and 8 different algorithms with trees have been modeled and evaluated. We have studied their statistical validity and predictive capacity by analyzing different metrics. The random forest algorithm with 1000 trees is the model with the highest predictive power. With this model, we built a predictor with 90% sensitivity (fraction of relevant instances that have been recovered).\n\n\n- Sections 9, 10, 11 and 12 have been devoted to examining the structural interpretability of the classificatory model, analyzing the importance \/ contribution of its features and the interpretative casuistry of its predictions.","aff25d88":"## <a id='#1'> 7. Models Precision-Recall curve <\/a>\n\nWe can examine the Precision-Recall curve of the 3 computed models and analyze their predictive quality. A Recall-Precision curve is a graph of the precision (y-axis) and recall (x-axis) for different thresholds.","a1215105":"As we can see, the higher the probability Threshold of the model, the lower the Recall. Similarly, increasing the Treshold increases the accuracy of the model. Thus, to build a high recall predictor we have to modify the standard predictor that has the treshold 0.5 to 0.326.","3ea01172":"We proceed to modify the threshold to achieve a model with a 90% recall.","907a4cbd":"A high area under the curve represents both high recall and high precision, where high precision is related to a low false positive rate and high recovery is related to a low false negative rate. We observe how the Random Forest has much more predictive power than the two individual decision tree models.\n \n**So how do we know if we have to sacrifice precision for recall or vice versa to find churn clients?**\n\nThis is where data mining meets specific knowledge of different contexts. If we calculate that the cost of letting clients escape with undetected churn (False negatives) is very high, we can choose a higher recall sacrificing precision, or at best we consider that finding 80% of clients with churn is good enough to business and we do not want unnecessary disruptions with customers while maintaining high precision.","b18d365d":"### <a id='#1'> 12.2 Local interpretability with Force plot & individual Decision plot <\/a>\n\nWe can drill down an individual prediction to show the impact of each feature of the model on it. This approach is tremendously useful to make our predictions more interpretable. SHAP values do this in a way that guarantees a very important property. Specifically, we decompose a prediction with the following equation:\n\n**sum (SHAP values for all attributes) = pred_for_instance - pred_for_baseline_values**","dba35351":"Let's see how the tree represented in the figure makes predictions.\n\n   - **Samples:** refers to how many instances the condition is met. For example, 3022 observations have a Total_Trans_Ct <= 57.5 and 5079 observations have a Total_Trans_Ct> 57.5.\n   \n\n   - **Value:** tells you how many training instances of each class this node applies to: for example, the lower left node Total_Ct_Chng_Q4_Q1 <= 0.866 applies to value = [173.363, 2183.91] class 0, class 1.\n  \n    \n   - **Gini:** measures impurity. A node is \"Pure\" (gini = 0) if all the training instances it applies belong to the same class. for example, at left node of depth 8, Contacts_Count_12_mon <= 1.50, all instances belong to class 0, so the node is pure.\n   \n- | --- Total_Trans_Ct <= 57.50\n- | | --- Total_Revolving_Bal <= 656.50\n- | | | --- Total_Ct_Chng_Q4_Q1 <= 0.87\n- | | | | --- Months_Inactive_12_mon <= 1.50\n- | | | | | --- Contacts_Count_12_mon <= 1.50\n- | | | | | | --- weights: [7.15, 0.00] class: 0\n\n\nStarting at the root node (depth 0), this node asks if the total number of transactions is <= 57.5. If true, it moves down to the child node and asks another question, and so on.\n\n**Example decision rule for predicting churn**\n\n- | --- Total_Trans_Ct <= 57.50\n- | | --- Total_Revolving_Bal <= 656.50\n- | | | --- Total_Ct_Chng_Q4_Q1 <= 0.87\n- | | | | --- Months_Inactive_12_mon <= 1.50\n- | | | | | --- Contacts_Count_12_mon> 1.50\n- | | | | | | --- Total_Ct_Chng_Q4_Q1 <= 0.49\n- | | | | | | | --- Customer_Age> 30.50\n- | | | | | | | | --- Total_Revolving_Bal <= 430.50\n- | | | | | | | | | --- Education_Level_Doctorate <= 0.50\n- | | | | | | | | | | --- Income_Category_ $ 40K - $ 60K <= 0.50\n- | | | | | | | | | | | --- Total_Relationship_Count <= 3.50\n- | | | | | | | | | | | | --- weights: [0.00, 40.44] class: 1\n\n\n**When the observation meets all these criteria, upon reaching the. sheet Total_Relationship_Count <= 3.50, node; if yes, the tree predicts the person is likely to churn**","3698425d":"# <a id='#1'> 9. Feature importance with Random Forest Classification: Information Gain <\/a>\n\nOne of the main advantages of decision tree models is their interpretability. Thus, we can study the importance of model features  by examining the relative importance of each feature to perform the prediction task. Performing this analysis can help us better understand the data in our model. This is measured  the importance of each feature in the algorithm training by examining the average amount of impurity that the nodes of the tree that that attribute manages to reduce. These data must be analyzed with caution, since algorithms with trees tend to overestimate the importance of attributes with high cardinality and highly correlated attributes cause distortions in the valuation.","9f8c8c84":"# <a id='#1'>Predicting credit card churn with tree based models<\/a>","993de25a":"### <a id='#1'> 6.1 Model probabilities <\/a>\n\nThe predict_proba method of scikit learn allows us to obtain the probabilities that the observation is of a certain class.","0f2f576f":"We observe that PhD and Postgraduate students have a higher percentage of churn.","34ea4a1c":"Ass the average number of transactions per user increases, the probability of churn increases slightly. We observe that from around 45 the churn probability progressively decreases.","2222b621":"Married people are somewhat less prone to churn.","1146e637":"### <a id='#1'>3.2 Mutual Info-classification <\/a>\n\nThis figure details the mutual information of each feature with the target. Mutual information measures the reduction of uncertainty for one variable given a known value of another. The mutual information between two random variables X and Y can be formally established as follows:\n\n     I (X; Y) = H (X) - H (X | Y)\n\nWhere I (X; Y) is the mutual information for X and Y, H (X) is the entropy for X and H (X | Y) is the conditional entropy for X given Y.\n\nMutual information is a measure of dependency or \"mutual dependency\" between two random variables. As such, the measure is symmetric, which means that I (X, Y) = I (Y, X). It measures the average reduction in uncertainty about x that results from learning the value of y; or vice versa, the average amount of information that x conveys about y.","2e936d6c":"# <a id='#1'> 3. Feature engineering and feature matrix examination <\/a>\n\nThe creation of new variables from the data you have available can improve the predictive power of our algorithms. This is where the stage of data exploration of  and the knowledge domain become very important.","505c2535":"SHAP summary charts give us a bird's-eye view of the model structure and the importance of the features.\n   \n- We observe, for example, that the point in the upper left was a person who had a high value of the number of Transactions, reducing the probability of churn more than 30%.\n\n\n- We also see one person that her low age decreased her probability of churn by more than 30%.\n\n\n- Low values of Total_Trans_Ct are associated with a higher probability of churn, and high values, with a lower probability.\n\n  \n- High values of Av_Trans are associated with a greater possibility of churn.\n   \nWe observe how the model largely ignores many features. That means they either have little or no effect on the prediction.","58174388":"This figure shows us the mean predictions for each combination of feature values.","37145e45":"### <a id='#1'> 2.3 Distributions of numerical attributes by churn <\/a>\n\nIn this figure we can visualize in box-charts all the distributions of all the numeric variables by the classes of the target variable (no churn: 0, churn: 1).","8504d5a5":"We observe that the variables with the highest score are Total_Trans_Amt and Total_Trans_Ct.","f5ef2fe0":"### <a id='#1'>3.3 PCA Projection matrix X <\/a>\n\nThe PCA Decomposition Viewer uses principal component analysis to decompose data so that each instance can be represented in a 2-d scatter plot. .","b54bfc76":"Eli 5 shows us the contribution of each attribute on the prediction of the model.","2e543a1f":"We can examine the text of the set of rules generated by the decision tree.","6e897281":"# <a id='#1'> 10. Partial dependency plots & prediction distributions per target plots <\/a>\n\nPartial dependency graphs show the dependency between the target class and a feature , removing the effects of all other features. These graphs can show if the relationship between the linear, monotonic target class, ... In short, they help us to better understand the dependencies between the attributes of our model and the target class. To compute these graphs, we predict the probability of the target class with new data, but changing the values of the variable before making a prediction. Thus, we first predict the churn probability for the different values that the attribute takes and then we plot how the probability of the target class changes for the different values that the attribute takes.\n\nWe analyze the graphs of the 2 variables with the highest feature importance. The blue area shows us the confidence interval.","d57a911a":"We observe that the observations with incomes of less than 40K and those of more than 120K have the highest average churn.","cc649ebc":"# <a id='#1'> 5. Hyperparameter tuning with GridSearch Cross-Validation <\/a>\n\nScikit-Learn's GridSearchCV method optimizes the algorithm using cross-validation by evaluating all possible combinations of hyperparameter values. Finds the best combination of hyperparameter values from the Decision Tree.\n\nHyperparameter tuning is the process of tuning the parameters present in the algorithm. Machine learning algorithms never learn these parameters. These are tuned so that we can get good performance per model. The goal of hyperparameter fitting is to find those parameters where the model performance is better and the error rate is lower.","2621b6b0":"We observe, for example, for the Total_Trans_Ct, the observations of class 0 have a distribution with high values (median of 71) in relation to the median of 43 for class 1.","fc45b2d8":"### <a id='#1'>8.1 Precision-Recall vs Threshold<\/a>","3782b1e9":"### <a id='#1'> 2.2 Discrete Features distributions by average churn <\/a>\n\nWe examine the distribution by the mean of churn for each value of our discrete variables. These figures give us the count and the mean churn for each discrete variable value.","8af490cc":"### <a id='#1'> 2.4 Distributions of numerical features by average churn mean <\/a>\nWe perform the same analysis as with the discrete attributes, this time separating the values of the numerical variables into intervals. 1.","0b9192af":"The target we want to predict is the variable Attrition Falg. We observed that of the 10,000 observations, 16.1% were attried \/ churned.","21a00a05":"The decision plot figure shows us how the model makes the decision that the 523 observation of the X_test will be churn:","0453fd84":"### <a id='#1'> 6.2 Model learning curve <\/a>\n\nThe learning curve of the model shows us the relationship between the F-1 score and the cross-validation tests of the model with an increasing number of training samples. This visualization is normally used to show two things:\n\n     How much the model benefits from more data (eg, do we have \"enough data\" or will the estimator improve with new data.\n\n     If the estimator is more sensitive to error due to variance versus error due to bias.\n\nThe increasing progression of the curve denotes that the model will benefit from more data to generalize better.\n\nThe curves are plotted with the mean scores; however, the variability during cross validation is shown with the shaded areas representing one standard deviation above and below the mean for all cross validations.","aac23722":"# <a id='#1'>8. 90% Recall Predictor <\/a>\n\nIn the previous figure we observe that if we want a model capable of letting only 10% of the users escape with churn (Recall of 90%), we will have to settle for an accuracy of 83%. (Random Forest model) Accessing the probabilities that the model computes, we can execute the treshold (or decisional limit of probability) that we want to make predictions.","f9abccd9":"Our prediction is 0.04 while the base value is 0.4998\n\n- The values of the characteristics that cause an increase in the churn probability are in pink and their visual size shows the magnitude of the effect of the characteristic.\n\n- The values of the characteristics that decrease the prediction are in blue. The greatest impact comes from Total_Trans_CT = 105 decreasing the prediction\n\nIf you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output. If we subtract the blue and pink lengths, that subtraction is equal to the distance from the base value to the output (in this case 0.04)","490ff145":"### <a id='#1'> Balanced Decision Trees <\/a>\n\nIn binary decision trees, if the frequency of class A is much higher than the frequency of class B, then class B will become the ruling class and its tree will lean towards the ruling class. To avoid this casuistry, we pass the parameter **class_weight = 'balanced'**, to perform an automatic adjustment according to the proportion of frequencies of each class and thus, each class will be treated with the same importance and the number of records of each class in the tree nodes will be the same.\n\nAs such, this modification of the decision tree algorithm is known as a class-weighted decision tree.","af0b147e":"We note that the most important attributes are Total_Trans_Ct and Total_Trans_Amt.\n\nThe values towards the top are the most important attributes, and those towards the bottom are the least important. The first Weight number in each row shows how much the performance of the model decreased with a random mix on each variable (in this case, using \"precision\" as the performance metric).\n\nTo compute the randomness of the process, the randomness of the computation is measured by repeating the process with multiple combinations. The number after \u00b1 measures how much performance varied from one reorganization to the next.\n\nIf negative values appear, it means that in these cases, the predictions on the mixed data turned out to be more accurate than the real data, which indicates that we are dealing with features that contribute noise to the model. The smaller the sample of data, the more likely it is that this type of situation will o ccur.\n\n The permutation importance is great because it gives us a simple numerical measure to see what characteristics are important to a model. the downside is that it doesn't tell us what kind of importance every feature has. If a characteristic has medium permutation importance, that could mean that it has a large effect for some specific predictions, but maybe little overall effect.","779641cc":"\nIn this project we have built tree-based models capable of predicting the credit cards churn. With the insights obtained, the bussiness can carry out proactive policies capable of providing better services and modifying customer decisions.\n\nThe data consists of 10,000 clients who mention their age, salary, marital status, credit card limit, credit card category, etc.\nThus, the 18 discrete and numerical attributes will help us to predict the \"churn\" of credit card users. The data has little need for pre-processing.\n\nWe only have 16.07% of customers who have abandoned credit card services. Therefore, we have an unbalanced data set. Although decision-tree algorithms are effective for the classification of balanced databases, in unbalanced datasets they can generate a bias. When the data is dominated by examples of one class, the criteria used to select a division point it will be considered optimal, when in fact, many examples of the minority class are ignored. This problem can be overcome by modifying the criteria used in the algorithm to evaluate the split points to take into account the importance of each class.\n","5d2882dc":"In these two figures we can examine the distributions of the predictions made by the model in the different intervals of the feature values.","86a396a6":"With GridSearch CV the model has somewhat more solid results.","d06cf19f":"We have no missing values."}}