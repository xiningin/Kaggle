{"cell_type":{"580edf56":"code","e068a127":"code","7831ad2d":"code","b3a52880":"code","404554bb":"code","db011a9b":"code","81cf1053":"code","b5e7744e":"code","c8bfc401":"code","af20a9f7":"code","017a7507":"code","95294eed":"code","559f8aff":"code","262e06ae":"code","c8e464c7":"code","6d1b4310":"code","ad3f8a44":"code","42cf95e7":"code","3302476c":"code","49129fc1":"code","d3dd0818":"code","7842ab82":"code","229ea989":"code","dbe975ed":"code","25d85b01":"code","8f488b23":"code","3d1c4f11":"code","d0a6c194":"code","5b0f3d92":"code","413cdf21":"code","06439dbb":"code","465e290c":"code","4b63df78":"code","ac819fb9":"code","3125838f":"code","5ff43615":"code","6566483b":"code","e9f87144":"code","94349afd":"code","d205c00a":"markdown","b81f497f":"markdown","e0432cd5":"markdown","9f0b95f6":"markdown","4e97755f":"markdown","a7a24942":"markdown","6a0a9d2d":"markdown","587e935a":"markdown","0cbf4698":"markdown","7d47da3c":"markdown"},"source":{"580edf56":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV, cross_val_score, cross_validate\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport joblib\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom pandas.core.common import SettingWithCopyWarning\nfrom sklearn.exceptions import ConvergenceWarning\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 170)\npd.set_option('display.max_rows', 20)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","e068a127":"########################################################################################\n# YOU CAN PUT INTO A FILE NAMED HELPERS AND USE THE FOLLOWING FUNCTIONS\n########################################################################################\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.60, 0.75, 0.95, 0.99, 1]).T)\n\n\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n\n\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\n\n\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    \"\"\"\n\n    It gives the names of categorical, numerical and categorical but cardinal variables in the data set.\n     Note: Categorical variables with numerical appearance are also included in categorical variables.\n\n    Parameters\n    ------\n        dataframe: dataframe\n                The dataframe from which variable names are to be retrieved\n        cat_th: int, optional\n                class threshold for numeric but categorical variables\n        car_th: int, optinal\n                class threshold for categorical but cardinal variables\n\n    Returns\n    ------\n        cat_cols: list\n                Categorical variable list\n        num_cols: list\n                Numeric variable list\n        cat_but_car: list\n                Categorical view cardinal variable list\n\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n\n\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = total number of variables\n        num_but_cat is inside cat_cols.\n        The sum of 3 lists with return is equal to the total number of variables: cat_cols + num_cols + cat_but_car = number of variables\n\n    \"\"\"\n\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\n\ndef target_summary_with_cat(dataframe, target, categorical_col):\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\ndef target_summary_with_num(dataframe, target, numerical_col):\n    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")\n\n\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (15, 15)})\n        sns.heatmap(corr, cmap=\"RdBu\")\n        plt.show()\n    return drop_list\n\n\ndef outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef replace_with_thresholds(dataframe, variable, q1=0.05, q3=0.95):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable, q1=0.05, q3=0.95)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n\ndef grab_outliers(dataframe, col_name, index=False):\n    low, up = outlier_thresholds(dataframe, col_name)\n    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].head())\n    else:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))])\n\n    if index:\n        outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].index\n        return outlier_index\n\n\ndef remove_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    df_without_outliers = dataframe[~((dataframe[col_name] < low_limit) | (dataframe[col_name] > up_limit))]\n    return df_without_outliers\n\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n\n\ndef missing_vs_target(dataframe, target, na_columns):\n    temp_df = dataframe.copy()\n    for col in na_columns:\n        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)\n    na_flags = temp_df.loc[:, temp_df.columns.str.contains(\"_NA_\")].columns\n    for col in na_flags:\n        print(pd.DataFrame({\"TARGET_MEAN\": temp_df.groupby(col)[target].mean(),\n                            \"Count\": temp_df.groupby(col)[target].count()}), end=\"\\n\\n\\n\")\n\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts())) # S\u0131n\u0131f Say\u0131s\u0131\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\ndef rare_encoder(dataframe, rare_perc, cat_cols):\n    temp_df = dataframe.copy()\n    rare_columns = [col for col in cat_cols if (temp_df[col].value_counts() \/ len(temp_df) < rare_perc).sum() > 1]\n\n    for col in rare_columns:\n        tmp = temp_df[col].value_counts() \/ len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[col] = np.where(temp_df[col].isin(rare_labels), 'Rare', temp_df[col])\n\n    return temp_df\n","7831ad2d":"# We are reading the hitters data.\ndf = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ncheck_df(df)","b3a52880":"# DEPENDENT VARIABLE ANALYSIS\n\ndf[\"Salary\"].describe()\nsns.distplot(df.Salary)\nplt.show()\n\n# We observe that the salary density is around 500","404554bb":"# SELECTING CATEGORY AND NUMERICAL VARIABLES\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\n# CATEGORICAL VARIABLE ANALYSIS\nrare_analyser(df, \"Salary\", cat_cols);","db011a9b":"# NUMERICAL VARIABLE ANALYSIS\nfor col in num_cols:\n    num_summary(df, col, plot=False)","81cf1053":"# OUTLIERS OBSERVATION ANALYSIS\nfor col in num_cols:\n    print(col, check_outlier(df, col))\n    \n# We observed no outliers","b5e7744e":"df.isnull().sum()\n# We observed that there are 59 empty observations in the Salary variable.","c8bfc401":"missing_values_table(df)\n# The sum of the nulls in the Salary variable corresponds to 18% of the data set","af20a9f7":"def hitter_first_preprocessing(dataframe, model):\n    new_df = dataframe.copy()\n    new_df.columns = [col.upper() for col in new_df.columns]\n    \n    # SELECTING CATEGORY AND NUMERICAL VARIABLES\n    cat_cols, num_cols, cat_but_car = grab_col_names(new_df)\n    \n    new_df.dropna(inplace=True)\n\n    new_df = pd.get_dummies(new_df, columns=cat_cols, drop_first=True)\n    \n    y = new_df['SALARY']\n    X = new_df.drop(\"SALARY\", axis=1)\n\n    model_tuned = model(random_state=46).fit(X, y)\n\n    # Salary de\u011fi\u015fkenini en \u00e7ok a\u00e7\u0131klayan de\u011fi\u015fkeni ara\u015ft\u0131ran fonksiyon.\n    # Bu de\u011fere g\u00f6re feature \u00fcretece\u011fiz\n    def plot_importance(model, features, num=len(X), save=False):\n        feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                         ascending=False)[0:num])\n        plt.title('Features')\n        plt.tight_layout()\n        plt.show()\n\n\n    plot_importance(model_tuned, X)\n\n    # Salary ile en \u00e7ok korelasyona sahip olanlar\u0131 g\u00f6steriyoruz\n    high_corr = new_df.corr()\n    print(high_corr['SALARY'].sort_values(ascending=False)[1:20])","017a7507":"hitter_first_preprocessing(df, LGBMRegressor)\n# Variables with the most correlated dependent variable and feature importance analysis were performed. \n# Our goal is to create the new features and drop features based on this analysis.","95294eed":"def hitter_data_preprocessing():\n    df = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\n    \n    ############################################\n    # FEATURE ENGINEERING\n    ############################################\n    df.columns = [col.upper() for col in df.columns]\n    \n    df['NEW_HITRATIO'] = df['HITS'] \/ df['ATBAT']\n    df['NEW_RUNRATIO'] = df['HMRUN'] \/ df['RUNS']\n    df['NEW_CHITRATIO'] = df['CHITS'] \/ df['CATBAT']\n    df['NEW_CRUNRATIO'] = df['CHMRUN'] \/ df['CRUNS']\n\n    df['NEW_AVG_ATBAT'] = df['CATBAT'] \/ df['YEARS']\n    df['NEW_AVG_HITS'] = df['CHITS'] \/ df['YEARS']\n    df['NEW_AVG_HMRUN'] = df['CHMRUN'] \/ df['YEARS']\n    df['NEW_AVG_RUNS'] = df['CRUNS'] \/ df['YEARS']\n    df['NEW_AVG_RBI'] = df['CRBI'] \/ df['YEARS']\n    df['NEW_AVG_WALKS'] = df['CWALKS'] \/ df['YEARS']\n\n    # We use the following function to specify categorical, numerical, categorical but cardinal variables.\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    \n#     We exclude variables that carry the same information as the variable we created from our data so that there is no bias.\n    df = df.drop(\n    ['ATBAT', 'HITS', 'HMRUN',\n     'RUNS', 'RBI', 'WALKS', 'ASSISTS', 'ERRORS', \"PUTOUTS\", 'LEAGUE', 'NEWLEAGUE',\n     'DIVISION'], axis=1)\n    \n    \n    df_eksik = df[df[\"SALARY\"].isnull()]  # Those with salary information were defined as another dataframe.\n    df_eksik= df_eksik.drop([\"SALARY\"], axis=1)\n\n    df.dropna(inplace=True) #na values were dropped.\n\n    y = df['SALARY']\n    X = df.drop(\"SALARY\", axis=1)\n#     We define the dependent variable and the independent variables in the data frame as X and y.\n    \n    return df, df_eksik, X, y\n    ","559f8aff":"# You have to run this code to see the result from here.\n\n\n# df, df_eksik, X, y = hitter_data_preprocessing()\n","262e06ae":"# The function that freezes the 5 models whose MSE scores are low.\n\ndef best_mse_scores(X, y, random_state = 1, best_model_size = 5):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n    models = [('LR', LinearRegression().fit(X_train, y_train)),\n              (\"Ridge\", Ridge(random_state=random_state).fit(X_train, y_train)),\n              (\"Lasso\", Lasso(random_state=random_state).fit(X_train, y_train)),\n              (\"ElasticNet\", ElasticNet(random_state=random_state).fit(X_train, y_train)),\n              ('KNN', KNeighborsRegressor().fit(X_train, y_train)),\n              ('CART', DecisionTreeRegressor(random_state=random_state).fit(X_train, y_train)),\n              ('RF', RandomForestRegressor(random_state=random_state).fit(X_train, y_train)),\n              ('SVR', SVR().fit(X_train, y_train)),\n              ('GBM', GradientBoostingRegressor(random_state=random_state).fit(X_train, y_train)),\n              (\"XGBoost\", XGBRegressor(objective='reg:squarederror', random_state=random_state).fit(X_train, y_train)),\n              (\"LightGBM\", LGBMRegressor(random_state=random_state).fit(X_train, y_train))\n              ]\n    \n    all_models = []\n    for name, regressor in models:\n        rmse = np.mean(np.sqrt(-cross_val_score(regressor, X_test, y_test, cv=40, scoring=\"neg_mean_squared_error\")))\n        values = dict(name=name,RMSE = rmse)\n        all_models.append(values)\n        \n    all_models_df = pd.DataFrame(all_models)\n    all_models_df = all_models_df.sort_values(all_models_df.columns[1])\n    best_models_df = all_models_df[0:best_model_size]\n    return best_models_df\n","c8e464c7":"# You have to run this code to see the result from here.\n\n\n# best_models_df, X_train, X_test, y_train, y_test = best_mse_scores(X, y, random_state= 1, best_model_size = 2)\n# best_models_df\n","6d1b4310":"# The function that converts the entered values to be used as hyperparameters.\n\ndef hyperparameter_str_to_int_converting(converting_list= []):\n    converted_list = []\n    for i in converting_list:\n        if len(str(i)) != 0:\n            if str(i).isdigit():\n                converted_list.append(int(i))\n            elif i is None:\n                converted_list.append(i)\n            else:\n                try:\n                    float(i)\n                    converted_list.append(float(i))\n                except:\n                    # string de\u011ferdir 'auto' gibi onu direk ekle\n                    if str(i) == 'None':\n                        converted_list.append(9)\n                    else:\n                        converted_list.append(i)\n                \n    return converted_list","ad3f8a44":"# Function that allows us to enter the hyperparameter list of the models with the best score from outside.\n\ndef set_hyperparameters(best_models_df, is_set_custom_hyperparameter = False):\n    hyperparameter_list = []\n    \n    gbm_params = {\"learning_rate\": [0.1],\n              \"max_depth\": [3],\n              \"n_estimators\": [100],\n              \"subsample\": [1]}\n\n    rf_params = {\"max_depth\": [None],\n                 \"max_features\": [\"auto\"],\n                 \"min_samples_split\": [2],\n                 \"n_estimators\": [100]}\n\n\n    xgboost_params = {\"learning_rate\": [None],\n                      \"max_depth\": [None],\n                      \"n_estimators\": [100],\n                      \"colsample_bytree\": [None]}\n\n    lightgbm_params = {\"learning_rate\": [0.1],\n                       \"n_estimators\": [100],\n                       \"colsample_bytree\": [1.0]}\n\n    elastic_params = {\"alpha\": [1]}\n\n    lasso_params = {\"alpha\": [1]}\n\n    knn_params = {'n_neighbors': [5]}\n\n    cart_params = {\"max_depth\": [None],\n                  \"min_samples_split\": [2]}\n\n    svr_params = {\"kernel\": \"rbf\",\n                 \"max_iter\": -1}\n\n\n    if is_set_custom_hyperparameter:\n        for col in best_models_df['name']:\n            print(f\"{col} enter hyperparameters for model, enter other number with comma to enter multiple(,)\")\n            if (col == \"GBM\"):\n                new_params = {}\n                for model, value in gbm_params.items():\n                    print(\"Parameter Name: {}, \u00d6n tan\u0131ml\u0131 de\u011feri(default): {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                gbm_param = new_params\n                hyperparameter_list.append(dict(gbm_paramms= gbm_param))\n\n            elif col == \"RF\":\n                new_params = {}\n                for model, value in rf_params.items():\n                    print(\"Parameter Name: {}, \u00d6n tan\u0131ml\u0131 de\u011feri(default): {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                rf_param = new_params\n                hyperparameter_list.append(dict(rf_params= rf_param))\n            elif col == \"XGBoost\":\n                new_params = {}\n                for model, value in xgboost_params.items():\n                    print(\"Parameter Name: {}, \u00d6n tan\u0131ml\u0131 de\u011feri(default): {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                xgboost_param = new_params\n                hyperparameter_list.append(dict(xgboost_params = xgboost_param))\n\n            elif col == \"LightGBM\":\n                new_params = {}\n                for model, value in lightgbm_params.items():\n                    print(\"Parameter Name: {}, \u00d6n tan\u0131ml\u0131 de\u011feri(default): {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                lightgbm_param = new_params\n                hyperparameter_list.append(dict(lightgbm_params = lightgbm_param))\n\n            elif col == \"ElasticNet\":\n                new_params = {}\n                for model, value in elastic_params.items():\n                    print(\"Parameter Name: {}, \u00d6n tan\u0131ml\u0131 de\u011feri(default): {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                elastic_param = new_params\n                hyperparameter_list.append(dict(elastic_params = elastic_param))\n\n            elif col == \"Lasso\":\n                new_params = {}\n                for model, value in lasso_params.items():\n                    print(\"Parameter Name: {}, \u00d6n tan\u0131ml\u0131 de\u011feri(default): {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                lasso_param = new_params\n                hyperparameter_list.append(dict(lasso_params = lasso_param))\n\n            elif col == \"KNN\":\n                new_params = {}\n                for model, value in knn_params.items():\n                    print(\"Parameter Name: {}, \u00d6n tan\u0131ml\u0131 de\u011feri(default): {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                knn_param = new_params\n                hyperparameter_list.append(dict(knn_params = knn_param))\n\n            elif col == \"CART\":\n                new_params = {}\n                for model, value in cart_params.items():\n                    print(\"Parameter Name: {}, \u00d6n tan\u0131ml\u0131 de\u011feri(default): {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                cart_param = new_params\n                hyperparameter_list.append(dict(cart_params = cart_param))\n\n            elif col == \"SVR\":\n                new_params = {}\n                for model, value in svr_params.items():\n                    print(\"Parameter Name {}, \u00d6n tan\u0131ml\u0131 de\u011feri(default): {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                svr_param = new_params\n                hyperparameter_list.append(dict(svr_params = svr_param))\n    else:\n        for col in best_models_df['name']:\n            if (col == \"GBM\"):\n                hyperparameter_list.append(dict(gbm_paramms= gbm_params))\n\n            elif col == \"RF\":\n                hyperparameter_list.append(dict(rf_params= rf_params))\n                \n            elif col == \"XGBoost\":\n                hyperparameter_list.append(dict(xgboost_params = xgboost_params))\n\n            elif col == \"LightGBM\":\n                hyperparameter_list.append(dict(lightgbm_params = lightgbm_params))\n\n            elif col == \"ElasticNet\":\n                hyperparameter_list.append(dict(elastic_params = elastic_params))\n\n            elif col == \"Lasso\":\n                hyperparameter_list.append(dict(lasso_params = lasso_params))\n\n            elif col == \"KNN\":\n                hyperparameter_list.append(dict(knn_params = knn_params))\n\n            elif col == \"CART\":\n                hyperparameter_list.append(dict(cart_params = cart_params))\n\n            elif col == \"SVR\":\n                hyperparameter_list.append(dict(svr_params = svr_params))\n                \n    return hyperparameter_list\n    \n        ","42cf95e7":"# You have to run this code to see the result from here.\n\n# hyperparameter_list = set_hyperparameters(best_models_df)\n# hyperparameter_list","3302476c":"# The function that gives the scores according to the default parameters of the models and according to the best hyperparameters\n\ndef final_models_with_hyperparameters(hyperparameter_list, random_state = 1, X_train=[], X_test=[], y_train=[], y_test=[]):\n    regressors = []\n    \n    for col in hyperparameter_list:\n        for model_params, params_value in col.items():\n            if model_params == \"gbm_paramms\":\n                 regressors.append(('GBM', GradientBoostingRegressor(random_state = random_state).fit(X_train, y_train), params_value))\n                    \n            elif model_params == \"rf_params\":\n                 regressors.append((\"RF\", RandomForestRegressor(random_state = random_state).fit(X_train, y_train), params_value))\n                    \n            elif model_params == \"xgboost_params\":\n                 regressors.append(('XGBoost', XGBRegressor(random_state = random_state,objective='reg:squarederror').fit(X_train, y_train), params_value))\n                    \n            elif model_params == \"lightgbm_params\":\n                 regressors.append(('LightGBM', LGBMRegressor(random_state = random_state).fit(X_train, y_train), params_value))\n                    \n            elif model_params == \"elastic_params\":\n                 regressors.append((\"ElasticNet\", ElasticNet(random_state = random_state).fit(X_train,y_train), params_value))\n                    \n            elif model_params == \"lasso_params\":\n                regressors.append((\"Lasso\", Lasso(random_state = random_state).fit(X_train,y_train), params_value))\n            \n            elif model_params == \"knn_params\":\n                regressors.append(('KNN', KNeighborsRegressor().fit(X_train,y_train), params_value))\n            \n            elif model_params == \"cart_params\":\n                regressors.append(('CART', DecisionTreeRegressor(random_state = random_state).fit(X_train,y_train), params_value))\n            \n            elif model_params == \"svr_params\":\n                regressors.append(('SVR', SVR().fit(X_train,y_train), params_value))\n    \n    best_models = {}\n\n    for name, regressor, params in regressors:\n        print(f\"########## {name} ##########\")\n        rmse = np.mean(np.sqrt(-cross_val_score(regressor, X_test, y_test, cv=20, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE (Before): {round(rmse, 4)} ({name}) \")\n\n        gs_best = GridSearchCV(regressor, params, cv=5, n_jobs=-1, verbose=False).fit(X_train, y_train)\n\n        final_model = regressor.set_params(**gs_best.best_params_)\n\n        rmse = np.mean(np.sqrt(-cross_val_score(final_model, X_test, y_test, cv=40, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n        best_models[name] = final_model\n    \n    return best_models","49129fc1":"# You have to run this code to see the result from here.\n\n# final_best_models = final_models_with_hyperparameters(hyperparameter_list)","d3dd0818":"# final_best_models","7842ab82":"# dataframe_na_values = Salary has 59 null values.\n# if we send this dataframe_na_values value to the function to fill them with the prediction value, \n# it will fill the empty values and return with the error value\n\n# dataframe_na_values will only return the prediction model if this value is not entered\n\ndef voting_model_with_hyperparameter(best_models, dataframe_na_values = [], na_target_name = '', original_dataframe = [], X_train=[], X_test=[], y_train=[], y_test=[]):\n    ######################################################\n    # # Stacking & Ensemble Learning\n    ######################################################\n    estimators = []\n    \n    for key, value in best_models.items():\n        estimators.append((key, value))\n\n    voting_reg = VotingRegressor(estimators)\n\n    voting_reg.fit(X_train, y_train)\n\n    rmse = np.mean(np.sqrt(-cross_val_score(voting_reg, X_test, y_test, cv=40,\n                                     scoring=\"neg_mean_squared_error\")))\n    print(\"RMSE : \", rmse)\n    \n    if len(dataframe_na_values) > 0:\n        dataframe_na_values[na_target_name] = voting_reg.predict(dataframe_na_values)\n        final_df = original_dataframe.merge(dataframe_na_values, how='outer')\n        return final_df, rmse\n    \n    return voting_reg","229ea989":"# We fill in the missing salary values.\n# Our aim is to make the model estimate the missing salary values and merge it with df, \n# and then create a new more accurate modeling.\n\n# final_df, rmse = voting_model_with_hyperparameter(final_best_models, dataframe_na_values = df_eksik,na_target_name = \"SALARY\", original_dataframe=df )\n# final_df.head()","dbe975ed":"# hitter_first_preprocessing(final_df, GradientBoostingRegressor)","25d85b01":"# we examine the correlation between variables\n# Dropping the high ones will increase the performance of the model, since the ones \n# with high correlation with each other actually mean the same thing.\n\n\n# high_corr=final_df.corr()\n# for i in high_corr.columns:\n#     print(high_corr[i].sort_values(ascending=False)[1:10])","8f488b23":"# We looked at the correlations, we looked at the feature importance. Accordingly, we removed some variables from our model.\n\n# final_df = final_df.drop(['CATBAT','CWALKS','CHMRUN','NEW_HITRATIO','NEW_RUNRATIO','NEW_AVG_ATBAT',\n#            'NEW_AVG_HMRUN','NEW_AVG_WALKS'], axis=1)\n","3d1c4f11":"\n# y = final_df['SALARY']\n# X = final_df.drop(\"SALARY\", axis=1)\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","d0a6c194":"# best_models_df, X_train, X_test, y_train, y_test = best_mse_scores(X, y, random_state= 1, best_model_size = 5)\n# best_models_df","5b0f3d92":"# hyperparameter_list = set_hyperparameters(best_models_df)\n# hyperparameter_list","413cdf21":"# final_best_models = final_models_with_hyperparameters(hyperparameter_list)\n# final_best_models","06439dbb":"# voting_reg = voting_model_with_hyperparameter(final_best_models )\n# voting_reg","465e290c":"\n######################################################\n# Prediction for a New Observation\n######################################################\n\n# random_user = X.sample(1,random_state=5)\n# voting_reg.predict(random_user)\n","4b63df78":"# final_df[final_df.index== random_user.index[0]]['SALARY']","ac819fb9":"# # We combine all the operations we have done so far with the custom pipeline function in a certain order.\n\ndef hitter_custom_pipeline(random_state=1, best_model_size= 2, is_set_custom_hyperparameter = False, save_model=False):\n    print('1')\n    df, df_eksik, X, y = hitter_data_preprocessing()\n    \n    print('2')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n    \n    best_models_df = best_mse_scores(X, y, random_state= random_state, best_model_size = best_model_size)\n    print('3')\n    hyperparameter_list = set_hyperparameters(best_models_df, is_set_custom_hyperparameter)\n    print('4')\n    final_best_models = final_models_with_hyperparameters(hyperparameter_list, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n    print('5')\n    # We fill in the missing salary values\n    final_df, rmse = voting_model_with_hyperparameter(final_best_models, dataframe_na_values = df_eksik,na_target_name = \"SALARY\", original_dataframe=df, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n    print('6')\n    # We looked at the correlations and at the feature importance. Accordingly, we removed some variables from our model.\n    final_df = final_df.drop(['CATBAT','CWALKS','CHMRUN','NEW_HITRATIO','NEW_RUNRATIO','NEW_AVG_ATBAT',\n           'NEW_AVG_HMRUN','NEW_AVG_WALKS'], axis=1)\n    y = final_df['SALARY']\n    X = final_df.drop(\"SALARY\", axis=1)\n    print('7')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n    print('8')\n    best_models_df = best_mse_scores(X, y, random_state= 1, best_model_size = best_model_size)\n    print('9')\n    hyperparameter_list = set_hyperparameters(best_models_df, is_set_custom_hyperparameter)\n    print('10')\n    final_best_models = final_models_with_hyperparameters(hyperparameter_list, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n    print('11')\n    voting_reg = voting_model_with_hyperparameter(final_best_models, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n    \n    if save_model:\n        os.chdir(\".\/\")\n        joblib.dump(voting_reg, \"voting_reg_hitters.pkl\") # We are converting the pipeline we have created to pickle.\n        print(\"voting_reg_hitters has been created\")\n\n    return voting_reg, X, y, final_df","3125838f":"# is_set_custom_hyperparameter = False to enter hyperparameter, it must be set to True\n# Pipeline that will work with otherwise predefined hyperparameters\n\n\nvoting_reg, X, y, final_df = hitter_custom_pipeline(best_model_size= 2, is_set_custom_hyperparameter = False, save_model=True)\nvoting_reg\n","5ff43615":"######################################################\n# Prediction for a New Observation\n######################################################\n##Let's use our salary forecasting pickle we created for salary forecasting\nvoting_reg = joblib.load(\".\/voting_reg_hitters.pkl\")\n\nrandom_user = X.sample(1,random_state=2)\nvoting_reg.predict(random_user)","6566483b":"# Let's check how accurate the estimated salary value is with its real value.\n\nfinal_df[final_df.index== random_user.index[0]]['SALARY']","e9f87144":"# When we run the pipeline function by entering hyperparameters, we get the following results.\n\n\n# voting_reg, X, y, final_df = hitter_custom_pipeline(best_model_size= 2, is_set_custom_hyperparameter = True)\n# voting_reg","94349afd":"# voting_reg, X, y, final_df = hitter_custom_pipeline(best_model_size= 2, is_set_custom_hyperparameter = True)\n# voting_reg\n# When we run this, we get the following prediction values\n\n\n# 1\n# Observations: 322\n# Variables: 30\n# cat_cols: 3\n# num_cols: 27\n# cat_but_car: 0\n# num_but_cat: 0\n# 2\n# 3\n# GBM modeli i\u00e7in hiperparametreleri giriniz, birden \u00e7ok girmek i\u00e7in virg\u00fclle di\u011fer say\u0131y\u0131 yaz\u0131n\u0131z\n# Parametre Ad\u0131: learning_rate, \u00d6n tan\u0131ml\u0131 de\u011feri: [0.1]\n#  0.1,0.01\n# Parametre Ad\u0131: max_depth, \u00d6n tan\u0131ml\u0131 de\u011feri: [3]\n#  3,5,7\n# Parametre Ad\u0131: n_estimators, \u00d6n tan\u0131ml\u0131 de\u011feri: [100]\n#  100,300,500,600\n# Parametre Ad\u0131: subsample, \u00d6n tan\u0131ml\u0131 de\u011feri: [1]\n#  1,0.5,0.6,0.7\n# RF modeli i\u00e7in hiperparametreleri giriniz, birden \u00e7ok girmek i\u00e7in virg\u00fclle di\u011fer say\u0131y\u0131 yaz\u0131n\u0131z\n# Parametre Ad\u0131: max_depth, \u00d6n tan\u0131ml\u0131 de\u011feri: [None]\n#  5,8,15,None\n# Parametre Ad\u0131: max_features, \u00d6n tan\u0131ml\u0131 de\u011feri: ['auto']\n#  2,5,7,9,auto\n# Parametre Ad\u0131: min_samples_split, \u00d6n tan\u0131ml\u0131 de\u011feri: [2]\n#  2,4,5,7\n# Parametre Ad\u0131: n_estimators, \u00d6n tan\u0131ml\u0131 de\u011feri: [100]\n#  100,200,300,400,500\n# 4\n# ########## GBM ##########\n# RMSE (Before): 199.1793 (GBM) \n# RMSE (After): 177.0177 (GBM) \n# GBM best params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 600, 'subsample': 0.5}\n\n# ########## RF ##########\n# RMSE (Before): 201.3148 (RF) \n# RMSE (After): 167.831 (RF) \n# RF best params: {'max_depth': 15, 'max_features': 5, 'min_samples_split': 2, 'n_estimators': 100}\n\n# 5\n# RMSE :  170.76784502310116\n# 6\n# 7\n# 8\n# 9\n# KNN enter hyperparameters for model, enter other number with comma to enter multiple(,)\n# Parameter Name: n_neighbors, \u00d6n tan\u0131ml\u0131 de\u011feri: [5]\n#  1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\n# RF enter hyperparameters for model, enter other number with comma to enter multiple(,)\n# Parametre Ad\u0131: max_depth, \u00d6n tan\u0131ml\u0131 de\u011feri: [None]\n#  5,8,15,None\n# Parameter Name: max_features, default: ['auto']\n#  2,5,7,9,auto\n# Parameter Name: min_samples_split, default: [2]\n#  2,4,5,7\n# Parameter Name: n_estimators, default: [100]\n#  100,200,300,400,500\n# 10\n# ########## KNN ##########\n# RMSE (Before): 212.3876 (KNN) \n# RMSE (After): 178.4536 (KNN) \n# KNN best params: {'n_neighbors': 15}\n\n# ########## RF ##########\n# RMSE (Before): 215.2845 (RF) \n# RMSE (After): 184.2157 (RF) \n# RF best params: {'max_depth': 9, 'max_features': 2, 'min_samples_split': 2, 'n_estimators': 100}\n\n# 11\n# RMSE :  176.65639968646988\n# VotingRegressor(estimators=[('KNN', KNeighborsRegressor(n_neighbors=15)),\n#                             ('RF',\n#                              RandomForestRegressor(max_depth=9, max_features=2,\n#                                                    random_state=1))])","d205c00a":"# <h1 style=\"color:#1e81b0; display: inline\"> EDA ANALYSIS <\/h1>","b81f497f":"# <h2 style=\"color:#e28743\">The success score of our model (RMSE)<\/h2>\n\n##### Estimated value is 247,992. The actual value is 228,414\n##### We found our RMSE Value to be 171.842. however, our model makes a good estimation, except for outliers. Since we do not do outlier suppression, the difference between the real values and the real values is very large when outlier values are estimated. This is the reason that increases the RMSE.\n##### As a result, our model makes its predictions correct until there is an average deviation of 171 units between the predictions and the actual values.","e0432cd5":"# <h2 style=\"color:#063970\"> LET DATA BE YOUR BEST FRIEND :D <\/h2>     ","9f0b95f6":"\n## <h1 style=\"color:#1e81b0\"> PROJECT: SALARY PREDICT\u0130ON WITH MACHINE LEARNING <\/h1>\n\n\n### Business Problem\n\n  Can a machine learning project be implemented to estimate the salaries of baseball players whose salary information and career statistics for 1986 are shared?\n\n ### Dataset story\n\n  This dataset was originally taken from the StatLib library at Carnegie Mellon University.\n  The dataset is part of the data used in the 1988 ASA Graphics Section Poster Session.\n  Salary data originally from Sports Illustrated, April 20, 1987.\n  1986 and career statistics are from the 1987 Baseball Encyclopedia Update, published by Collier Books, Macmillan Publishing Company, New York.\n\n\n* AtBat: Number of hits with a baseball bat during the 1986-1987 season\n* Hits: the number of hits in the 1986-1987 season\n* HmRun: Most valuable hits in the 1986-1987 season\n* Runs: The points he earned for his team in the 1986-1987 season\n* RBI: The number of players a batter had jogged when he hit\n* Walks: Number of mistakes made by the opposing player\n* Years: Player's playing time in major league (years)\n* CAtBat: Number of hits during the player's career\n* CHits: The number of hits the player has made throughout his career\n* CHmRun: The player's most valuable number during his career\n* CRuns: The number of points the player has earned for his team during his career\n* CRBI: The number of players the player has made during his career\n* CWalks: The number of mistakes the player has made to the opposing player during his career\n* League: A factor with A and N levels showing the league in which the player played until the end of the season\n* Division: a factor with levels E and W indicating the position played by the player at the end of 1986\n* PutOuts: Helping your teammate in-game\n* Assists: Number of assists made by the player in the 1986-1987 season\n* Errors: the number of errors of the player in the 1986-1987 season\n* Salary: The salary of the player in the 1986-1987 season (over thousand)\n* NewLeague: a factor with A and N levels indicating the player's league at the start of the 1987 season","4e97755f":"# <h1 style=\"color:#1e81b0\"> CUSTOM PIPELINE <\/h1>   ","a7a24942":"# <h1 style=\"color:#1e81b0\"> PART 1: FEATURE IMPORTANCE AND CORRELATION ANALYSIS <\/h1> ","6a0a9d2d":"# <h2 style=\"color:#e28743\"> Feature Engineering <\/h2>   ","587e935a":"# <h2 style=\"color:#1e81b0\"> We saved our model as Pickle <\/h2>    ","0cbf4698":"# <h1 style=\"color:#1e81b0\"> PART 2: FEATURE ENGINEERING AND MODELING <\/h1>  ","7d47da3c":"# <h1 style=\"color:#1e81b0\"> PART 3: ESTABLISHING THE MOST ACCURATE PAY ESTIMATION MODEL WITH PREDICT SALARY VALUES <\/h1>  \n"}}