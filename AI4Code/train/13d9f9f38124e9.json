{"cell_type":{"227269cf":"code","6717d0b3":"code","af630b5b":"code","54840840":"code","5d8c462d":"code","26aeae9a":"code","03421ada":"code","8f29ff1f":"code","bb5b899a":"code","23b27eef":"code","45156b0e":"code","039d8767":"code","7a81f429":"code","019f90e9":"code","433527c5":"code","d233bdbc":"code","63bc5d97":"code","c31c8a31":"code","79dc79b0":"code","f66e34c9":"code","5725be3f":"code","41c41fc2":"code","3357146b":"code","e516c126":"code","9f0414f8":"code","a3a7bb8a":"code","2f68e67c":"code","363374d3":"code","fa183cf9":"code","c09c5770":"code","ee284c8d":"code","aa3e4d43":"code","f56e1bf8":"code","51df3ef6":"code","f726ea92":"code","58c84920":"code","8bf3a186":"code","9c44db6e":"code","6760440b":"code","34ae59ea":"code","11955335":"code","68e2b28b":"code","ff3117ec":"code","32f2d879":"code","cf1d00a7":"code","fcb96cef":"code","5888b3fd":"code","97ec92c6":"code","02d0c7fc":"code","2490e47a":"markdown","60bf54e2":"markdown","956fb31c":"markdown","25a617ad":"markdown","030967e1":"markdown","5fae686f":"markdown","f4b7f476":"markdown","e5406142":"markdown","8cec9f61":"markdown","e984e0e7":"markdown","5077b68b":"markdown","6e6f3a46":"markdown","f3aab05f":"markdown","e3653740":"markdown","7c2cc5bb":"markdown","34b69040":"markdown","bc3242c5":"markdown","644a633d":"markdown","4620a28e":"markdown","293a6751":"markdown","015e1b64":"markdown","41d833f0":"markdown","4c0b617c":"markdown","8b8cf58d":"markdown","b3b0c6e3":"markdown","03585cca":"markdown","b0af0065":"markdown","ab712cb3":"markdown","ba7d1b43":"markdown","4cba9d3d":"markdown"},"source":{"227269cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6717d0b3":"#Esentials\nimport pandas as pd\nimport numpy as np\nimport math\nimport os\n\n# Statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n#Visualization\n#import dabl\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Limiting floats output to 2 decimal points\npd.set_option('display.float_format', lambda x: '{:.2f}'.format(x)) \n\n# the below code will show all the columns of the datasets\npd.set_option('max_columns', 82)\n\n# Scikit Learn Librraies for Model building\nimport sklearn.metrics as metrics\nfrom sklearn.preprocessing import power_transform\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n#!pip install mlxtend\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n\n#!pip install lightgbm\nfrom lightgbm import LGBMRegressor\n\n#pip install xgboost\nfrom xgboost import XGBRegressor\n\n\n#print(os.getcwd())","af630b5b":"X = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\").set_index(\"Id\")\nY_train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\").set_index(\"Id\")\nY_test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\").set_index(\"Id\")","54840840":"print(\"The dimension of X(Train) Dataset is:\", X.shape)\nprint(X.duplicated().sum(),'Duplicate rows.')\nX.head()","5d8c462d":"print(\"The dimension of Y(Train) Dataset is:\", Y_train.shape)\nprint(Y_train.duplicated().sum(),'Duplicate rows.')\nY_train.head()","26aeae9a":"print(\"The dimension of Y(Test) Dataset is:\", Y_test.shape)\nprint(Y_test.duplicated().sum(),'Duplicate rows.')\nY_test.head()","03421ada":"# the below code is not working in kaggle notebook\n#dabl.plot(X, target_col='SalePrice') ","8f29ff1f":"sns.set_color_codes(palette='dark')\nsns.distplot(X['SalePrice'] , fit=norm);\n\n#Now plot the distribution\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(X['SalePrice'], plot=plt)\nplt.show()","bb5b899a":"# Visualizing the skewness of the dataset\n\nf4, axes4 = plt.subplots(2, 1, figsize=(15, 9), sharex=False)\nf4.subplots_adjust(hspace= .5)\n\nX.skew().plot(ax=axes4[0]).set_title('Skewness in X dataset')\nY_train.skew().plot(ax=axes4[1]).set_title('Skewness in Y dataset')","23b27eef":"X.info()","45156b0e":"Y_train.info()","039d8767":"X.describe(percentiles= [.25, .5, .75, .99] )","7a81f429":"X.describe(exclude = [np.number])","019f90e9":"Y_train.describe(percentiles= [.25, .5, .75, .99])","433527c5":"Y_train.describe(exclude = [np.number])","d233bdbc":"print(\"A sample of 50 data points are taken and observed if missing data is occuring.\")\nmsno.matrix(X.sample(50))","63bc5d97":"print(\"Correlation of missing values in X dataset\")\n\nmsno.heatmap(X)","c31c8a31":"print(\"Correlation of missing values in Y dataset\")\n\nmsno.heatmap(Y_train)","79dc79b0":"# Return list of columns that contain missing values in X Dataset:\n\ndef get_missing_columns(X):\n    return X.columns[X.isna().any()].tolist()\nprint(len(get_missing_columns(X)), \"columns have missing data!!\")\nprint(\"\\nColumns names in a list:\", get_missing_columns(X) )","f66e34c9":"# Return list of columns that contain missing values in Y Dataset:\n\ndef get_missing_columns(Y_train):\n    return Y_train.columns[Y_train.isna().any()].tolist()\nprint(len(get_missing_columns(Y_train)), \"columns have missing data!!\")\nprint(\"\\nColumns names in a list:\", get_missing_columns(Y_train))","5725be3f":"# Graph showing Missing Values in X & Y dataset (in numbers)\n\nX_null = [feature for feature in X.columns if (X[feature].isnull().sum() > 0)]\nY_null = [feature for feature in Y_train.columns if (Y_train[feature].isnull().sum() > 0)]\n\nf, axes = plt.subplots(2, 1, figsize=(15, 9), sharex=False)\nf.subplots_adjust(hspace= .5)\n\nX[X_null].isnull().sum().plot(kind='bar', ax=axes[0]).set_title('Features with missing data in X Dataset')\nY_train[Y_null].isnull().sum().plot(kind='bar', ax=axes[1]).set_title('Features with missing data in Y Dataset')","41c41fc2":"# Graph showing Missing Values in X & Y dataset (in Percentage)\n\nf1, axes1 = plt.subplots(2, 1, figsize=(15, 9), sharex=False)\nf1.subplots_adjust(hspace= .5)\n\nX[X_null].isnull().mean().plot(kind='bar', ax=axes1[0]).set_title('PERCENTAGE of missing Features in X Dataset')\nY_train[Y_null].isnull().mean().plot(kind='bar', ax=axes1[1]).set_title('PERCENTAGE of missing Features in Y Dataset')","3357146b":"# table showing percentages of the missing values\nprint(\"The attributes in X dataset which has more than 40 % of the missing values\")\nX_NA = [(c, X[c].isna().mean()*100) for c in X]\nX_NA = pd.DataFrame(X_NA, columns=[\"column_name\", \"percentage\"])\nX_NA = X_NA[X_NA.percentage > 40]\nX_NA.sort_values(\"percentage\", ascending=False)","e516c126":"# table showing percentages of the missing values\nprint(\"The attributes in Y dataset which has more than 40 % of the missing values\")\nY_NA = [(c, Y_train[c].isna().mean()*100) for c in Y_train]\nY_NA = pd.DataFrame(Y_NA, columns=[\"column_name\", \"percentage\"])\nY_NA = Y_NA[Y_NA.percentage > 40]\nY_NA.sort_values(\"percentage\", ascending=False)","9f0414f8":"# dropping these variables \ndrop_columns = ['PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu']\nX.drop(X[drop_columns], inplace = True, axis=1)\nY_train.drop(Y_train[drop_columns], inplace = True, axis=1)\nX.shape, Y_train.shape","a3a7bb8a":"X_cat = X.select_dtypes(include=['object'])\nX_NULL = pd.DataFrame(X_cat.isna().sum(), columns=[\"Missing\"])\nX_NULL.sort_values(\"Missing\", ascending=False).head(11)","2f68e67c":"Y_cat = Y_train.select_dtypes(include=['object'])\nY_NULL = pd.DataFrame(Y_cat.isna().sum(), columns=[\"Missing\"])\nY_NULL.sort_values(\"Missing\", ascending=False).head(17)","363374d3":"X_col = ['GarageCond','GarageQual','GarageFinish','GarageType','BsmtExposure','BsmtFinType2','BsmtCond','BsmtFinType1','BsmtQual','MasVnrType','Electrical']\nX[X_col] = X[X_col].fillna(X.mode().iloc[0])\n\nY_col = ['GarageCond','GarageQual','GarageFinish','GarageType', 'BsmtCond','BsmtExposure', 'BsmtQual', 'BsmtFinType1','BsmtFinType2','MasVnrType','MSZoning','Functional','Utilities','KitchenQual','Exterior1st','Exterior2nd','SaleType']\nY_train[Y_col] = Y_train[Y_col].fillna(Y_train.mode().iloc[0])","fa183cf9":"X_num = X.select_dtypes(exclude=['object'])\nX_NULL_num = pd.DataFrame(X_num.isna().sum(), columns=[\"Missing\"])\nX_NULL_num.sort_values(\"Missing\", ascending=False).head(3)","c09c5770":"Y_num = Y_train.select_dtypes(exclude=['object'])\nY_NULL_num = pd.DataFrame(Y_num.isna().sum(), columns=[\"Missing\"])\nY_NULL_num.sort_values(\"Missing\", ascending=False).head(11)","ee284c8d":"X_col1 = ['LotFrontage','GarageYrBlt','MasVnrArea']\nX[X_col1] = X[X_col1].fillna(X.mean().iloc[0])\n\nY_col1 = ['LotFrontage','GarageYrBlt','MasVnrArea','BsmtHalfBath','BsmtFullBath','TotalBsmtSF','GarageCars','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','GarageArea']\nY_train[Y_col1] = Y_train[Y_col1].fillna(Y_train.mean().iloc[0])","aa3e4d43":"#X Dataset\noutliers_features = ['MSSubClass', 'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch']\n\nfor feature in outliers_features:\n    IQR = X[feature].quantile(0.75) - X[feature].quantile(0.25)\n    lower_boundary = X[feature].quantile(0.25) - (IQR*3)\n    upper_boundary = X[feature].quantile(0.75) + (IQR*3)\n    print(feature, lower_boundary, upper_boundary)\n    \nfor feature in outliers_features:\n    IQR = X[feature].quantile(0.75) - X[feature].quantile(0.25)\n    lower_boundary = X[feature].quantile(0.25) - (IQR*3)\n    upper_boundary = X[feature].quantile(0.75) + (IQR*3)\n    X.loc[X[feature]<=lower_boundary, feature] = lower_boundary\n    X.loc[X[feature]>=upper_boundary, feature] = upper_boundary\n    \nX[outliers_features].describe()    ","f56e1bf8":"# Y Dataset\noutliers_features = ['MSSubClass', 'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch']\n\nfor feature in outliers_features:\n    IQR = Y_train[feature].quantile(0.75) - Y_train[feature].quantile(0.25)\n    lower_boundary = Y_train[feature].quantile(0.25) - (IQR*3)\n    upper_boundary = Y_train[feature].quantile(0.75) + (IQR*3)\n    print(feature, lower_boundary, upper_boundary)\n    \nfor feature in outliers_features:\n    IQR = Y_train[feature].quantile(0.75) - Y_train[feature].quantile(0.25)\n    lower_boundary = Y_train[feature].quantile(0.25) - (IQR*3)\n    upper_boundary = Y_train[feature].quantile(0.75) + (IQR*3)\n    Y_train.loc[Y_train[feature]<=lower_boundary, feature] = lower_boundary\n    Y_train.loc[Y_train[feature]>=upper_boundary, feature] = upper_boundary\n    \nY_train[outliers_features].describe()    ","51df3ef6":"corr_plot = X.corr()\nplt.subplots(figsize=(20,9))\nsns.heatmap(corr_plot, vmax= 0.5, center=0 ,cmap= 'viridis' ,linewidths= .9 ,linecolor='white')","f726ea92":"corr_features = corr_plot.index[abs(corr_plot['SalePrice']) < 0.3]\ncorr_features","58c84920":"X.drop(['BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', 'HalfBath', 'KitchenAbvGr', 'KitchenAbvGr', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'EnclosedPorch' ,'MiscVal', 'MoSold', 'YrSold'], axis=1, inplace=True)\nY_train.drop(['BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', 'HalfBath', 'KitchenAbvGr', 'KitchenAbvGr', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'EnclosedPorch' ,'MiscVal', 'MoSold', 'YrSold'], axis=1, inplace=True)\nX.shape, Y_train.shape","8bf3a186":"Y = pd.concat([Y_train, Y_test], axis=1)\nHP = pd.concat([X,Y], axis=0)\nprint('Dimension of HP dataset is',HP.shape)\nHP.head()","9c44db6e":"HP = pd.get_dummies(HP)\nprint('Dimension of HP dataset is',HP.shape)\nHP.head()","6760440b":"# Remove any duplicated column names\nHP = HP.loc[:,~HP.columns.duplicated()]","34ae59ea":"#Log Transforming the dataset\n\ncols_skew = [col for col in HP if '_2num' in col or '_' not in col]\nHP[cols_skew].skew().sort_values()\ncols_unskew = HP[cols_skew].columns[abs(HP[cols_skew].skew()) > 1]\nfor col in cols_unskew:\n    HP[col] = np.log1p(HP[col])\nHP.head()    ","11955335":"X = HP.iloc[:1460,:]\nY = HP.iloc[:1459,:]\n\nX_train = X.drop('SalePrice', axis=1) #fit\nX_test = X['SalePrice']               #fit         \ny_train = Y.drop('SalePrice', axis=1) #predict\ny_test = Y['SalePrice']               #compare\n\n\nprint(\"The dimension of X_train is:\", X_train.shape)\nprint(\"The dimension of X_test is:\", X_test.shape)\nprint(\"The dimension of Y_train is:\", y_train.shape)\nprint(\"The dimension of Y_test is:\", y_test.shape)","68e2b28b":"reg = RandomForestRegressor(n_estimators= 700,\n                          max_depth= 20,\n                          min_samples_split= 15,\n                          min_samples_leaf= 1,\n                          max_features= 'auto',\n                          oob_score = True,\n                          random_state = 200)\n\nreg.fit(X_train, X_test)\nRFR_y_pred = reg.predict(y_train)\n\nprint('The Out-of-bag score (oob) for Random Forest Regressor Model is',round((reg.oob_score_),2))\nprint('Root Mean Squared error: ' + str(math.sqrt(metrics.mean_squared_error(y_test, RFR_y_pred))))\nprint('R2 score: ', round(r2_score(y_test, RFR_y_pred),2))","ff3117ec":"GBoost = GradientBoostingRegressor(n_estimators= 500,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=200)\n\nGBoost.fit(X_train, X_test)\nGBoost_y_pred = GBoost.predict(y_train)\n\nprint('Root Mean Squared error: ' + str(math.sqrt(metrics.mean_squared_error(y_test, GBoost_y_pred))))\nprint('R2 score: ', round(r2_score(y_test, GBoost_y_pred),2))","32f2d879":"lgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators= 100,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=200)\n\nlgbm.fit(X_train, X_test)\nlgbm_y_pred = lgbm.predict(y_train)\n\nprint('Root Mean Squared error: ' + str(math.sqrt(metrics.mean_squared_error(y_test, lgbm_y_pred))))\nprint('R2 score: ', round(r2_score(y_test, lgbm_y_pred),2))","cf1d00a7":"xgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators= 500,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=200)\n\nxgboost.fit(X_train, X_test)\nxgboost_y_pred = xgboost.predict(y_train)\n\nprint('Root Mean Squared error: ' + str(math.sqrt(metrics.mean_squared_error(y_test, xgboost_y_pred))))\nprint('R2 score: ', round(r2_score(y_test, xgboost_y_pred),2))","fcb96cef":"stack_gen = StackingCVRegressor(regressors=(reg, GBoost, lgbm, xgboost),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\n\nstack_gen.fit(X_train, X_test)\nstack_gen_y_pred = xgboost.predict(y_train)\n\nprint('Root Mean Squared error: ' + str(math.sqrt(metrics.mean_squared_error(y_test, stack_gen_y_pred))))","5888b3fd":"Predict = (RFR_y_pred*.2 + GBoost_y_pred*.2 + lgbm_y_pred*.2 + xgboost_y_pred*.2 + stack_gen_y_pred*.2)\n\nprint('Root Mean Squared error: ' + str(math.sqrt(metrics.mean_squared_error(y_test, Predict))))\nprint('R2 score: ', round(r2_score(y_test, Predict),2))","97ec92c6":"# Final Predicted Dataset to be uploaded:\nsub = pd.DataFrame()\nsub['SalePrice'] = np.expm1(Predict)\n\nsub.to_csv('submission.csv', index=False)","02d0c7fc":"fig, ax = plt.subplots()\nax.scatter(y_test, Predict)\nax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nax.set_xlabel('Actual')\nax.set_ylabel('Predicted')\nplt.show()\nplt.savefig('Actual VS Predicted.png')","2490e47a":"<div class=\"alert alert-block alert-info\">\n<h1>16. XGBoost Regressor <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"16\"><\/a>\n\n[Back to Table of Contents](#0.1)","60bf54e2":"<div class=\"alert alert-block alert-info\">\n<h1>17. Stacking up all the models <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"17\"><\/a>\n\n[Back to Table of Contents](#0.1)","956fb31c":"## House Prices: \n- Predicting sale price using Advanced Regression Techniques\n- House prices are predicted using RandomForest Regressor, Gradient Boosting Regression, Light Gradient Boosting Machine Regressor","25a617ad":"<div class=\"alert alert-block alert-info\">\n<h1>6. Data Preprocessing <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","030967e1":"<div class=\"alert alert-block alert-info\">\n<h1>15. Light Gradient Boosting Machine Regressor <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"15\"><\/a>\n\n[Back to Table of Contents](#0.1)","5fae686f":"<div class=\"alert alert-block alert-info\">\n<h1>12. Splitting Dataset for model Building<\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"12\"><\/a>\n\n[Back to Table of Contents](#0.1)","f4b7f476":"<div class=\"alert alert-block alert-warning\">\n<h1>Observation<\/h1><\/div> <\/h1><\/div><a class=\"anchor\"><\/a>\n\n* Below features do not have good correlation with the target variable. <br>\n   -- 'MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'\n* It is advisable to drop these variables before modeling   ","e5406142":"<div class=\"alert alert-block alert-info\">\n<h1> Table of Contents<\/h1><\/div><a class=\"anchor\" id=\"0.1\"><\/a>\n\n1. [Importing Libraries](#1)\n2. [Loading Dataset](#2)\n3. [Visualizing the distribution of the dataset](#3)\n4. [Target Variable](#4)\n5. [Skewness of the Numerical data](#5)\n6. [Data Preprocessing](#6)\n7. [Missing Value Treatment](#7)\n8. [Outlier Detection](#8)\n9. [Data Correlation](#9)\n10. [Concatinating the datasets](#10)\n11. [Applying One Hot Encoding](#11)\n12. [Splitting Dataset for model Building](#12) <br>\n--- Prediction Models --- <br>\n13. [Random Forest Regressor](#13)\n14. [Gradient Boosting Regression](#14)\n15. [Light Gradient Boosting Machine Regressor](#15)\n16. [XGBoost Regressor](#16)\n17. [Stacking up all the models](#17)","8cec9f61":"<div class=\"alert alert-block alert-info\">\n<h1>11. Applying One Hot Encoding<\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"11\"><\/a>\n\n[Back to Table of Contents](#0.1)","e984e0e7":"<div class=\"alert alert-block alert-info\">\n<h1>10. Concatinating the datasets<\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"10\"><\/a>\n\n[Back to Table of Contents](#0.1)","5077b68b":"<div class=\"alert alert-block alert-info\">\n<h1>14. Gradient Boosting Regression<\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"14\"><\/a>\n\n[Back to Table of Contents](#0.1)","6e6f3a46":"<div class=\"alert alert-block alert-warning\">\n<h1>Final Observation<\/h1><\/div> <\/h1><\/div><a class=\"anchor\"><\/a>\n\n* 3 Models are built on the final dataset:\n    - RandomForest Regressor : RMSE = 88874.12 | R2 score = 2.81\n    - Gradient Boosting Regressior : RMSE = 93297.90 | R2 score = -7.11\n    - Light Gradient Boosting Machine Regressor : RMSE = 88302.01 | R2 score = 4.06\n* A combination of all the 2 models is giving: RMSE = 86360.01 | R2 score = 8.23\n* The combination model is giving the best score. Hence using the same for prediction.","f3aab05f":"### Numerical Variables","e3653740":"<div class=\"alert alert-block alert-info\">\n<h1>7. Missing Value Treatment <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","7c2cc5bb":"<div class=\"alert alert-block alert-info\">\n<h1>13. Random Forest Regressor<\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"13\"><\/a>\n\n[Back to Table of Contents](#0.1)","34b69040":"<div class=\"alert alert-block alert-warning\">\n<h1>Observation<\/h1><\/div> <\/h1><\/div><a class=\"anchor\"><\/a>\n\n- 5 attributes in both X & Y dataset have more than 40% of the data missing\n- Most of times NA means lack of subject described by attribute, like missing pool, fence, no garage and basement.\n- It is advisable to drop these featues. ","bc3242c5":"<div class=\"alert alert-block alert-info\">\n<h1>3. Visualizing the distribution of the dataset <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","644a633d":"___","4620a28e":"<div class=\"alert alert-block alert-warning\">\n<h1>Observation<\/h1><\/div> <\/h1><\/div><a class=\"anchor\"><\/a>\n\n- Value near -1 : if one variable is present then the other is likely to be missing.\n- Value near  0 : there is no dependency between 2 variables.\n- Value near -1 : if one variable is present then the other is also likely to be present.","293a6751":"<div class=\"alert alert-block alert-info\">\n<h1>1. Importing Libraries<\/h1><\/div><a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","015e1b64":"<div class=\"alert alert-block alert-info\">\n<h1>2. Loading Dataset <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","41d833f0":"<div class=\"alert alert-block alert-info\">\n<h1>8. Outlier Detection<\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","4c0b617c":"## Fill missing values for categorical column (using its own most frequent value)","8b8cf58d":"<div class=\"alert alert-block alert-warning\">\n<h1>Observation<\/h1><\/div> <\/h1><\/div><a class=\"anchor\"><\/a>\n\n- Target is continuous variable so this is a regression problem.\n- SalePrice is right skewed.\n- SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed.\n- As for linear models, target variable should be normally distributed\n- Using box-cox transform in target variable","b3b0c6e3":"## Fill missing values for numerical column (with respective mean value)","03585cca":"<div class=\"alert alert-block alert-info\">\n<h1>5. Skewness of the Numerical data <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","b0af0065":"<div class=\"alert alert-block alert-warning\">\n<h1>Observation<\/h1><\/div> <\/h1><\/div><a class=\"anchor\"><\/a>\n\n- Few variables are highly skewed.\n- Log Transformation reduced the skewness in the data.","ab712cb3":"<div class=\"alert alert-block alert-info\">\n<h1>9. Data Correlation<\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"9\"><\/a>\n\n[Back to Table of Contents](#0.1)","ba7d1b43":"### Categorical variables","4cba9d3d":"<div class=\"alert alert-block alert-info\">\n<h1>4. Target Variable <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}