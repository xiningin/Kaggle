{"cell_type":{"e1b61543":"code","54ad478a":"code","ece64761":"code","e78bb76e":"code","815fd9a5":"code","9583b00e":"code","326da38b":"code","ba1e9065":"code","85a06986":"code","48c6539a":"code","b5c41c0e":"code","145d69a8":"code","0556ef7a":"code","bed8152e":"code","1a4e7a82":"code","63c94836":"code","6891b970":"code","3d83dbae":"code","4b910a70":"code","22d94385":"code","bc955feb":"code","a0a98acc":"code","5d40b73c":"code","f21382c5":"code","3783974b":"code","887f966b":"code","b2efbd9c":"code","7446f46c":"code","11a613b0":"code","93cebbb8":"code","088688f8":"code","cc676105":"code","76237dc8":"markdown","e1363948":"markdown","5c90c08a":"markdown","fb9b3b2f":"markdown","7dfa70cb":"markdown","296cc4aa":"markdown","bd99ff7f":"markdown","0a0a8537":"markdown","d3ac4439":"markdown","191e27b3":"markdown","49c6e1e0":"markdown","d790062e":"markdown","64358b2e":"markdown","30f08da2":"markdown","d4dd9bf5":"markdown","607a896b":"markdown","1914e5e9":"markdown","fd88aeec":"markdown","3cc1b0ec":"markdown","f453a7c2":"markdown","40fcac2d":"markdown","34c22050":"markdown"},"source":{"e1b61543":"# Import the neccessary libraries for data manipulation and visual representation\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as matplot\nimport seaborn as sns\n%matplotlib inline","54ad478a":"df = pd.read_csv(\"..\/input\/hr-analytics\/HR_comma_sep.csv\")\ndf.info()","ece64761":"df.head()","e78bb76e":"df['Work_accident'] = df['Work_accident'] == 1\ndf['left'] = df['left'] == 1\ndf['promotion_last_5years'] = df['promotion_last_5years'] == 1\ndf.dtypes","815fd9a5":"df = df.rename(columns={'satisfaction_level': 'satisfaction', \n                        'last_evaluation': 'evaluation',\n                        'number_project': 'projectCount',\n                        'average_montly_hours': 'averageMonthlyHours',\n                        'time_spend_company': 'yearsAtCompany',\n                        'Work_accident': 'workAccident',\n                        'promotion_last_5years': 'hadPromotion',\n                        'Department' : 'department',\n                        })\ndf.tail()","9583b00e":"print('Turnover rate was: %.2f%%' % (len(df[df.left == 1]) \/ len(df) * 100))","326da38b":"df.shape","ba1e9065":"df.describe()","85a06986":"df.corrwith(df.left)","48c6539a":"sns.boxplot(x=df.left,y=df.satisfaction)","b5c41c0e":"sns.heatmap(abs(df.corr()))","145d69a8":"fig, ax = plt.subplots(figsize=(12, 8))\nsns.scatterplot(ax = ax, x=df.averageMonthlyHours,y=df.evaluation, hue=df.projectCount, palette=\"seismic\")","0556ef7a":"fig, ax = plt.subplots(figsize=(12, 8))\nsns.scatterplot(ax = ax, x=df.averageMonthlyHours,y=df.evaluation, hue=df.left)","bed8152e":"df.yearsAtCompany.describe()","1a4e7a82":"sns.boxplot(y=df.yearsAtCompany,x=df.left)","63c94836":"fig, (ax1, ax2) = plt.subplots(2, 2, figsize=(18,12))\nsns.boxplot(ax = ax1[0],y=df.yearsAtCompany,x=df.hadPromotion,hue=df.left)\nsns.boxplot(ax = ax1[1], y=df.yearsAtCompany, x=df.projectCount, hue=df.left)\nsns.boxplot(ax = ax2[0], y=df.yearsAtCompany, x=df.evaluation, orient='h', hue=df.left)\nsns.boxplot(ax = ax2[1], y=df.yearsAtCompany, x=df.workAccident, hue=df.left)","6891b970":"df['jobStability'] = df.yearsAtCompany.apply(lambda y: 'lessThan3Years' if y <= 3 else 'averageYears' if y < 5  else 'moreThan5Years')\ndf[['jobStability','yearsAtCompany']]","3d83dbae":"df[['hadPromotion','left']].value_counts()","4b910a70":"print('Only {:.2%} of the workforce had a promotion in the last 5 years. From those that had, only {:.2%} have left'.format(len(df[df.hadPromotion]) \/ len(df), len(df[df.hadPromotion & df.left])\/ len(df[df.hadPromotion])))\nprint('Remember that our average turnover is {:.2%}'.format(len(df[df.left == 1]) \/ len(df)))","22d94385":"df[['workAccident','left']].value_counts(normalize=True)","bc955feb":"print('{:.2%} of the workforce had a work accident. From those that had, only {:.2%} have left'.format(len(df[df.workAccident]) \/ len(df), len(df[df.workAccident & df.left])\/ len(df[df.workAccident])))","a0a98acc":"# Import necessary machine learning libraries\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","5d40b73c":"features = df[['satisfaction','averageMonthlyHours','evaluation','projectCount','workAccident','hadPromotion']].copy()\nfeatures = pd.concat([features,pd.get_dummies(df['jobStability'])], axis=1)\nfeatures['workAccident'] = features['workAccident'].apply(lambda v: 1 if v else 0)\nfeatures['hadPromotion'] = features['hadPromotion'].apply(lambda v: 1 if v else 0)\nX = features\nX[0:5]","f21382c5":"y = df['left'].values\ny[0:5]","3783974b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\nX_model_train, X_validation, y_model_train, y_validation = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\nprint ('Model train set:', X_model_train.shape,  y_model_train.shape)\nprint ('Validation (hyperparameters test) set:', X_validation.shape,  y_validation.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","887f966b":"df_y = pd.DataFrame(1 if left else 0 for left in y)\ndf_y.mean()","b2efbd9c":"y_null_predict = np.repeat(False,len(y_test))\nprint('Null model F1: ', f1_score(y_test, y_null_predict, average='weighted'))","7446f46c":"def train_tree_with_validation():\n    f1_scores = {}\n    max_depth = range(1,20)\n    impurity_decrease = [0.1,0.01,0.001,0.0001,0]\n    for d in max_depth:\n        for i in impurity_decrease:\n            dTree = DecisionTreeClassifier(criterion=\"gini\", max_depth = d, min_impurity_decrease=i)\n            dTree.fit(X_model_train,y_model_train)\n            f1_scores[(d,i)] = f1_score(y_validation, dTree.predict(X_validation))\n    return max(f1_scores.keys(), key=(lambda key: f1_scores[key]))","11a613b0":"best_hyper_values = train_tree_with_validation()\ndTree = DecisionTreeClassifier(criterion=\"gini\", max_depth = best_hyper_values[0], min_impurity_decrease=best_hyper_values[1])\ndTree.fit(X_train,y_train)\nprint('Trained a decision tree with max_depth = {} and min_impurity_decrease = {}'.format(best_hyper_values[0],best_hyper_values[1]))\nprint('F1 score on test set: {:.3}'.format(f1_score(y_test, dTree.predict(X_test), average='weighted')))","93cebbb8":"dTree = DecisionTreeClassifier(criterion=\"gini\", max_depth = 4, min_impurity_decrease=0.01)\ndTree.fit(X_train,y_train)\nprint('F1 score with max_depth=4 on the test set: {:.3}'.format(f1_score(y_test, dTree.predict(X_test), average='weighted')))","088688f8":"dTree = DecisionTreeClassifier(criterion=\"gini\", max_depth = 4, min_impurity_decrease=0.01)\ndTree.fit(X,y)\n\nfig, ax = plt.subplots(figsize=(40,30))\ntree.plot_tree(dTree, feature_names=features.columns,label='none',filled=True,proportion=True,impurity=False,rounded=True, max_depth=4)\nplt.show()","cc676105":"reason1 = df[(df.satisfaction < 0.465)]\nreason2 = df[(df.satisfaction < 0.465) & (df.projectCount == 2) & (df.evaluation <= 0.575) ]\nreason3 = df[(df.satisfaction <= 0.115) &(df.projectCount >=3)]\nreason4 = df[(df.satisfaction > 0.465) & (df.yearsAtCompany >= 5) & (df.evaluation > 0.815) & (df.averageMonthlyHours > 216)]\n\nprint('1. In general, keep satisfaction high. - Satisfaction < 0.465: {:.2%} left'.format(len(reason1[reason1.left])\/ len(reason1)))\nprint('2. Unhappy, low performers working on only 2 projects leave or are fired - Satisfaction < 0.465, projectCount == 2, evaluation <= 0.575: {:.2%} left'.format(len(reason2[reason2.left])\/ len(reason2)))\nprint('3. Very unhappy, working on 3 or more projects leave - Satisfaction <= 0.115, projectCount >= 3: {:.2%} left'.format(len(reason3[reason3.left])\/ len(reason3)))\nprint('4. Long time overworked top-performers leave - Satisfaction > 0.465, yearsAtCompany >= 5, evaluation > 0.815, workingHours > 216: {:.2%} left'.format(len(reason4[reason4.left])\/ len(reason4)))","76237dc8":"Years at the company seems to be a categorical value:\n  1. 3 years or less\n  1. Between 3 and 5\n  1. More than 5\n  \nWhen the employee is between 3 and 5 years at the company, if he has or has not a promotion seems to be a decisive factor for the turnover. Moreover, in this specific range of 3-5 years, working on exactly 3 projects seems to be an unfortunate combination. The remaining attributes does not seem to correlate with years and turnover. We will create a new categorical attribute to reflect that discovery called *jobStability*.","e1363948":"## Recomendations\n\nThe decision tree above allow us to give management recomendations to avoid turnover:\n* The single most important attribute is employee satisfaction. Management should focus on keeping all employees with **satisfaction above 0.465**\n* Failing that, management should pay close attention on:\n     * the employess with low satisfaction that are working only on 2 projects and had an evaluation score below 0.575. They will probabily leave or be fired\n     * the employees with extremely low satisfaction (0.115 or less) working on 3 or more projects. They will surely leave. \n* Even for the workers that have a high satisfaction, they should pay attention on old employees (5 or more years at the company) with evaluation above 0.815 (top performers) that are working, in average, more than 216 hours per month.","5c90c08a":"Even though this decision tree had a high F1 score, it had a quite significant depth (9) to enable us to create meaningfull recomendations. If accuracy was the only thing that we were aiming for, there are better techniques to model the data, like Random Forest and XBoost. As such, we will reduce the max_depth to help us create management recomendations, even if it reduces the F1 for that.","fb9b3b2f":"Now we will separate the train, validation and test sets","7dfa70cb":"With that, we end our exploratory data analysis and start to model our classifier.","296cc4aa":"If all those people left in the same year (there are no indications in the dataset for the date when people left), this is a **very** significant turnover rate. In fact, in 2017, LinkedIn analysis finds [an average worldwide turnover rate of 10.9% in their platform](https:\/\/business.linkedin.com\/talent-solutions\/blog\/trends-and-research\/2018\/the-3-industries-with-the-highest-turnover-rates). Our dataset has turnover rates higher than the top industry - Technology (Software) - with an average of 13.2%.\n\nSo lets investigate further!","bd99ff7f":"Using the boxplot above, we can see that people who left had a consistenly lower satisfaction rate. We will use this information as part of our [feature set](https:\/\/en.wikipedia.org\/wiki\/Feature_(machine_learning)) for the modeling step.\n\n## Average Hours, evaluation and project count\nSo let's move to the other attributes. We will begin with the [Pearson Correlation](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient):","0a0a8537":"## Statistical overview","d3ac4439":"Indeed, we can see that the number of people that left the company after receiving a promotion in the last 5 years is significantly lower. We will make this attribute part of our feature set.","191e27b3":"## Data preparation for modeling\n\nIn this section we will create our feature set (X) and expected outputs (Y). To do so, we will also normalize and covert labels to numbers.","49c6e1e0":"## Work accident","d790062e":"Finally, we use the entire dataset and plot the final decision tree:","64358b2e":"# Data Analysis\n\n## Current turnover rates\n\nIn the context of human resources, turnover is the act of replacing an employee with a new employee \\[[1]\\]. An organization\u2019s turnover is measured as a percentage rate, which is referred to as its turnover rate, using this formula:\n\n$$ T = \\frac{left}{avg. employees} * 100 $$\n\n[1]: https:\/\/en.wikipedia.org\/wiki\/Turnover_(employment)\n\nSo we calculate the turnover of our dataset with:","30f08da2":"Project count, average monthly hours and evaluation have medium correlations among themselves. So let's see them in a scatterplot","d4dd9bf5":"## Effects of promotions on turnover\n\nIntuitively, a promotion, which is an essential part of the many rewards distributed by organizations, should affect the quitting behavior of individual employees.","607a896b":"Interesting enough, it looks like the same clusters. These three attributes together (evaluation, monthlyHours, projectCount) seems to be related to the turnover. We will include then in our feature set.\n\n## Years at the company\n\nWe begin by exploring the years attribute and comparing it with other features. See the data below","1914e5e9":"Turnover is not strongly correlated with any single variable. But it has a medium correlation with satisfaction which is enough for further analysis of this variable.\n\n## Satisfaction","fd88aeec":"The image above shows two strange clusters:\n 1. A black one, with evaluation centered on 0.5, average monthly hours around 140 and project count around 2\n 1. A red one, with high evaluation scores (centered on 0.9), average monthly hours above 250 and project count above 6\n \nNow, lets see who have left the company from this evaluation\/monthly hours plot. We will change the project count colors to use the turnover information","3cc1b0ec":"# Describing turnover\n\nWe will use a [Decision Tree](https:\/\/scikit-learn.org\/stable\/modules\/tree.html) to explain the factors that are making people leave the company and to let us give management recommendations. We will begin by preparing our data for the machine learning technique.","f453a7c2":"Check the null model accuracy:","40fcac2d":"# People Analytics\n\nMy goal with this notebook is to understand what factors contribute to employee turnover based on the [HR Analytics dataset](https:\/\/www.kaggle.com\/giripujar\/hr-analytics). To do so, I will follow a standard data science methodology with business problem, [data preparation](https:\/\/en.wikipedia.org\/wiki\/Data_preparation), [exploratory data analysis](https:\/\/en.wikipedia.org\/wiki\/Exploratory_data_analysis) and [model building](https:\/\/towardsdatascience.com\/seven-major-steps-for-building-a-data-science-model-c1761408dd17) to predict and describe the underlying factors that contribute to employee turnover.\n\n# Business problem\n\nImplementing workplace policies that benefit workers and help boost employee retention is not just a \u201cnice\u201d thing for businesses to do for their employees. Maintaining\na stable workforce by reducing employee turnover, using data science to understand their causes and consequences, also makes good business sense, as it can result in significant cost savings to employers.\n\nIn fact, according to the [Work Institute\u2019s 2019 Retention Report](https:\/\/info.workinstitute.com\/hubfs\/2019%20Retention%20Report\/Work%20Institute%202019%20Retention%20Report%20final-1.pdf#page=9), employee turnover costs businesses more than 600 billion dollars every year. They estimate that for each employee you lose, it will cost you up to 33\\% of their annual salary to replace them.\n\n## Goal\n\n> **What factors contribute to employee turnover and what are our recomendation for managers to avoid that?**\n\n# Data Sources\n\nThis notebook is based on the [HR Analytics dataset](https:\/\/www.kaggle.com\/giripujar\/hr-analytics). It was uploaded by Giri Pujar in 2018 and, currently, is evaluated as a Bronze data set. There are no indications if this is a fictional or an anonymized dataset. One of the secondary goals of this notebook is to find clues that suggest if  this was created using rules and random values or not.\n\n## Obtain and scrub","34c22050":"And train our decision tree (it may take a while to run)"}}