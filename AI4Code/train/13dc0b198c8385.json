{"cell_type":{"22bfee0e":"code","8cd52eb3":"code","f1ef8e71":"code","dc2fcf5b":"code","01bd7612":"code","a898b6a5":"code","9b7ed5ba":"code","5a3488f8":"code","7591ffaa":"code","40b46cf8":"code","106a7709":"code","12917be1":"code","51f8cc04":"code","c2beb99c":"code","a7ce3477":"code","cbee5d01":"code","120f03b6":"code","f5c94061":"code","673e416f":"code","38c0475a":"code","7459b659":"code","968751c4":"code","81ac4a23":"code","0d49de06":"code","b42f60f4":"code","c35c5ae0":"markdown","f4334de8":"markdown","e27ce938":"markdown","7bd273af":"markdown"},"source":{"22bfee0e":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf","8cd52eb3":"print('Loading data...')\ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","f1ef8e71":"print(train.shape)\nprint(test.shape)","dc2fcf5b":"train.head()","01bd7612":"test.head()","a898b6a5":"train_X = train.iloc[:, 1:].values.astype('float32')\ntrain_y = train.iloc[:, 0].values.astype('float32') ","9b7ed5ba":"print(train_X.shape)\nprint(train_y.shape)","5a3488f8":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing","7591ffaa":"lb = preprocessing.LabelBinarizer()\nlb.fit(range(0,10))\nprint(lb.classes_)\ny_onehot = lb.transform(train_y)\nprint(y_onehot[0], train_y[0])\n\nX_train, X_val, y_train, y_val = map(lambda x : np.array(x).astype(np.float32), train_test_split(train_X, y_onehot, test_size=0.2))","40b46cf8":"X_train.shape","106a7709":"n_input = 784  #MNIST data input (img shape: 28*28)\nn_classes = 10 #MNIST total classes (0-9 digits)\nhidden_layer_size = 50","12917be1":"#Input data and labels in batch\nX_input = tf.placeholder(tf.float32,[None, n_input])\ny_teacher = tf.placeholder(tf.float32,[None, n_classes])\n\n#drop out to avoid overlearning\nkeep_prob_input = tf.placeholder(tf.float32)\n\n##First layer##\nX_input_layer = tf.nn.dropout(X_input, keep_prob=keep_prob_input)\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.01)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.0, shape=shape)\n    return tf.Variable(initial)\n\nW_fc1 = weight_variable([n_input, hidden_layer_size])\nb_fc1 = bias_variable([hidden_layer_size])\n\n###Use relu function as activation function\nh_fc1 = tf.nn.relu(tf.matmul(X_input_layer, W_fc1)+ b_fc1)\n\nkeep_prob = tf.placeholder(tf.float32)\n\n###output after drop out\nh_fc1_dout = tf.nn.dropout(h_fc1, keep_prob)\n","51f8cc04":"##Second layer##\nW_fc2 = weight_variable([hidden_layer_size, hidden_layer_size])\nb_fc2 = bias_variable([hidden_layer_size])\nh_fc2 = tf.nn.relu(tf.matmul(h_fc1_dout, W_fc2)+ b_fc2)\nh_fc2_dout = tf.nn.dropout(h_fc2, keep_prob)","c2beb99c":"##Third layer##\nW_fc3 = weight_variable([hidden_layer_size, hidden_layer_size])\nb_fc3 = bias_variable([hidden_layer_size])\nh_fc3 = tf.nn.relu(tf.matmul(h_fc2_dout, W_fc3)+ b_fc3)\nh_fc3_dout = tf.nn.dropout(h_fc3, keep_prob)","a7ce3477":"##Fourth layer##\nW_fc4 = weight_variable([hidden_layer_size, n_classes])\nb_fc4 = bias_variable([n_classes])\ny_out = tf.nn.softmax(tf.matmul(h_fc3_dout, W_fc4)+ b_fc4)","cbee5d01":"#Cost function\n#learning_rate = tf.placeholder(tf.float32)\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_teacher*tf.log(y_out), reduction_indices=[1]))\n\n#Optimization function\noptimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cross_entropy)\n\n#Iteration\n\ncorrect_prediction = tf.equal(tf.argmax(y_out, 1), tf.argmax(y_teacher, 1))\n\n#Accuracy\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","120f03b6":"#Hyper parameters\nbatch_size = 100\nepoch_size = 20\nbest_accuracy = 0.0","f5c94061":"init = tf.global_variables_initializer()\n\nimport random\nsaver = tf.train.Saver()\n\nsess = tf.Session()\nsess.run(init)\n\nsaver.save(sess, 'mnist_fc_best')\n\n\ndef random_sample(X, y, size=100):\n    idx = range(0, len(y))\n    random_idx = random.sample(idx, size)\n    return X[random_idx, :], y[random_idx, :]","673e416f":"for epoch in range(1,epoch_size+1):\n    \n    for i in range(int(len(y_train)\/batch_size)):\n        X_batch, y_batch = random_sample(X_train, y_train, batch_size)\n        \n        if i == 0:\n            print(\"=====================\")\n            train_accuracy = sess.run(accuracy, feed_dict = {\n                    X_input:X_batch, y_teacher:y_batch, keep_prob_input:1.0, keep_prob:1.0\n            })\n            print(\"{} : training accuracy {}%\".format(epoch, train_accuracy*100))\n            \n            val_accuracy = sess.run(accuracy, feed_dict={\n                X_input: X_val, y_teacher: y_val, keep_prob_input: 1.0, keep_prob: 1.0})\n            \n            print(\"{} : validation accuracy {}%\".format(epoch, val_accuracy*100))\n            \n    \n            if val_accuracy >= best_accuracy:\n                saver.save(sess, 'mnist_fc_best')\n                best_accuracy = val_accuracy\n                print(\"Validation accuracy improved: {}%. Saving the network.\".format(val_accuracy*100))\n            else:\n                saver.restore(sess, 'mnist_fc_best')\n                print(\"restore!!!! now : {}, before : {}\".format(val_accuracy*100, best_accuracy*100))\n                \n        sess.run(optimizer, feed_dict={\n                X_input: X_batch, y_teacher: y_batch, keep_prob_input: 0.9, keep_prob: 1.0})\n        \n","38c0475a":"saver.restore(sess,'mnist_fc_best')","7459b659":"test_X = test.values.astype('float32')","968751c4":"test_X.shape","81ac4a23":"predictions = sess.run([tf.argmax(y_out, 1)], feed_dict={X_input: test_X, keep_prob_input: 0.9, keep_prob: 1.0})","0d49de06":"print(predictions)","b42f60f4":"results = pd.DataFrame({'ImageId': pd.Series(range(1, len(predictions[0]) + 1)), 'Label': pd.Series(predictions[0])})\nresults.to_csv('tensorflor_result.csv', index=False)","c35c5ae0":"#### Load data ","f4334de8":"This is the Kernel for practicing multilayer perceptron using tensorflow by Kaggle Digit Recognizer.\nhttps:\/\/www.tensorflow.org\/install\/\n\nIt would be great if you could share your comments about the improvements.\nI am a beginner in the third month of learning python at this time.\n\nMany thanks for this GutHub. \nhttps:\/\/github.com\/tsu-nera\/kaggle\/blob\/master\/digit-recognizer\/multi-layer-network.ipynb","e27ce938":"#### Make a Graph for tensorflow","7bd273af":"#### Training"}}