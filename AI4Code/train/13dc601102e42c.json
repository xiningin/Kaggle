{"cell_type":{"7f0ecf73":"code","4318234d":"code","7c7eee33":"code","e6fd3a8f":"code","5838db7e":"code","21d4209f":"code","72a3001d":"code","f2226feb":"code","b0cd081b":"code","a25db8a8":"code","6e755906":"code","fc0c9e4f":"code","2ca3b5ba":"code","24d8017c":"code","711b2178":"code","08619932":"code","13f1c261":"code","ce524a96":"code","7bc33e5a":"code","95e2edfc":"code","93e2aedf":"code","819648f8":"code","e30e033c":"code","cf32bfae":"code","27f74842":"code","0fd013e9":"code","3cc33196":"code","7624f191":"code","d0edae25":"code","eb8a2c1e":"code","c942e851":"code","3111e661":"code","4fadb257":"code","8c40f016":"code","b4e6a390":"code","39809b3b":"code","c1ce2c82":"markdown","00a5bbbd":"markdown","0e8cd5d7":"markdown","98df545a":"markdown","c35f1b39":"markdown","fcab201e":"markdown","471bfd0f":"markdown","db149cb6":"markdown","7732b767":"markdown","d85cddf1":"markdown","0395958f":"markdown","1439d85f":"markdown","a64a3aca":"markdown","f475f678":"markdown","54b5819c":"markdown","bf9aa8eb":"markdown","95114008":"markdown","6e8ad9f3":"markdown","12dc8875":"markdown","8cd0ab99":"markdown","2f255606":"markdown","fdb55ca8":"markdown","e0d73c44":"markdown","76a0a8d5":"markdown","3b08af23":"markdown","257a075a":"markdown","2c3b3e42":"markdown","7d5275d7":"markdown","3fefc459":"markdown","303a916e":"markdown","011124fb":"markdown","f56021e4":"markdown","2cd73a50":"markdown","5505aab5":"markdown","51a0ced6":"markdown","2aca3413":"markdown","89e94885":"markdown","7f782d50":"markdown","940c4a47":"markdown"},"source":{"7f0ecf73":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn import metrics\nimport os\n%matplotlib inline\nprint(os.listdir(\"..\/input\"))","4318234d":"original_data = pd.read_csv('..\/input\/BlackFriday.csv')","7c7eee33":"original_data.head()","e6fd3a8f":"original_data.columns","5838db7e":"original_data.describe()","21d4209f":"original_data.info()","72a3001d":"sns.set_style('whitegrid')\nplt.figure(figsize=(16,9))\nsns.heatmap(original_data.isnull(),cmap=\"viridis\",cbar=False,yticklabels=False)","f2226feb":"plt.figure(figsize=(16,9))\nsns.distplot(original_data['Purchase'],bins=80,kde=False)","b0cd081b":"sns.countplot(x='Gender',data=original_data,hue='Marital_Status')","a25db8a8":"plt.figure(figsize=(16,9))\nsns.countplot(x='Age',data=original_data,hue='Gender')","6e755906":"plt.figure(figsize=(16,9))\nsns.countplot(x='Occupation',data=original_data,hue='Gender')","fc0c9e4f":"plt.figure(figsize=(16,9))\nsns.boxplot(x='Age',y='Purchase',data=original_data)","2ca3b5ba":"plt.figure(figsize=(16,9))\nsns.violinplot(x='Occupation',y='Purchase',data=original_data)","24d8017c":"sns.countplot(x='City_Category',data=original_data)","711b2178":"sns.violinplot(x='City_Category',y='Purchase',data=original_data)","08619932":"plt.figure(figsize=(16,9))\nsns.countplot(x='Stay_In_Current_City_Years',data=original_data)","13f1c261":"plt.figure(figsize=(16,9))\nsns.boxplot(x='Stay_In_Current_City_Years',y='Purchase',data=original_data)","ce524a96":"plt.figure(figsize=(16,9))\ndata = original_data['Product_ID'].value_counts().sort_values(ascending=False)[:10]\nsns.barplot(x=data.index,y=data.values)","7bc33e5a":"plt.figure(figsize=(16,9))\ndata = original_data['User_ID'].value_counts().sort_values(ascending=False)[:10]\nsns.barplot(x=data.index,y=data.values)","95e2edfc":"original_data.columns","93e2aedf":"original_data['Product_Category_2'].fillna(0, inplace=True)\noriginal_data['Product_Category_3'].fillna(0, inplace=True)","819648f8":"original_data['Product_Category_2'] = original_data['Product_Category_2'].astype(int)\noriginal_data['Product_Category_3'] = original_data['Product_Category_3'].astype(int)","e30e033c":"original_data.Stay_In_Current_City_Years = original_data.Stay_In_Current_City_Years.replace('4+',4)\noriginal_data['Stay_In_Current_City_Years'] = original_data['Stay_In_Current_City_Years'].astype(int)","cf32bfae":"original_data.head()","27f74842":"X = original_data.iloc[:,2:11].values\ny = original_data.iloc[:,11].values","0fd013e9":"lb_x_1 = LabelEncoder()\nX[:,0] = lb_x_1.fit_transform(X[:,0])\nlb_x_2 = LabelEncoder()\nX[:,1] = lb_x_2.fit_transform(X[:,1])\nlb_x_4 = LabelEncoder()\nX[:,3] = lb_x_2.fit_transform(X[:,3])\nlb_x_3 = LabelEncoder()\nX[:,2] = lb_x_3.fit_transform(X[:,2])","3cc33196":"onh = OneHotEncoder(categorical_features=[1,2,3])\nX = onh.fit_transform(X).toarray()","7624f191":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","d0edae25":"model = Sequential()\n\n# The Input Layer :\nmodel.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n\n# The Hidden Layers :\nmodel.add(Dense(256, kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(256, kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nmodel.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nmodel.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nmodel.summary()","eb8a2c1e":"checkpoint_name = '{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","c942e851":"def get_best_weight(num_ext, ext):\n    tmp_file_name = os.listdir('.')\n    test = []\n    num_element = -num_ext\n    all_weights_file = [k for k in tmp_file_name if '.hdf5' in k]\n    for x in range(0, len(all_weights_file)):\n        test.append(all_weights_file[x][:num_element])\n        float(test[x])\n\n    lowest = min(test)\n    return str(lowest) + ext","3111e661":"model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)","4fadb257":"weights_file = get_best_weight(5, \".hdf5\")\nmodel.load_weights(weights_file)\nmodel.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])","8c40f016":"predictions = model.predict(X_test)","b4e6a390":"plt.figure(figsize=(16,9))\nplt.scatter(y_test[:500],predictions[:500]) #Taken only 500 points here for better visualisation\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')","39809b3b":"print('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","c1ce2c82":"**Takeaway** :-  The purchase amount is also well distributed among the occupation.However we know the fact that most of the people who came for shopping were from occupation 4 and 0 and still the purchase range for these groups is similar to others.\n<br\/>\nAnd the group belonging to the Occupation 13 despite being least in number have purchased amount similar to others.Now that means that **Occupation 13** is the most highly paid job here.","00a5bbbd":"**Takeaway** :- Most of the customers belong to the age group of 26-35 which is quite obvious as most of the customers who came for shopping were unmarried.","0e8cd5d7":"**Well enough** ! Lets visualise what we have done","98df545a":"<center><h2>------------------------ Thank You ------------------------<\/h2><\/center>","c35f1b39":"With that done we have completed the part 2 of this kernel.\n<br\/>\nNow we will move to the next part where we will develop our model using **Artificial Neural Networks**.\n<br\/><br\/>\n<h2>PART 3 :- Building model  in ANN<\/h2>","fcab201e":"<h3>Top ten most sold items<\/h3>","471bfd0f":"Now let's have a look on how the data looks like and explore its datatypes.","db149cb6":"<h2>So let's get started...<\/h2>\n<p>We start with importing our favourite libraries<\/p>","7732b767":"Lets have a look what we have done to our data so far...","d85cddf1":"**Takeaway** :- Most of the customers who came for shopping are males and unmarried. \n<h3>Well this is getting even more interesting...<\/h3>","0395958f":"We also apply **OneHotEncoding** to some of our columns. Please note we have not included Gender column here as it is not required beacuse it is already in binary form of 0 and 1.","1439d85f":"Lets recollect our data first...","a64a3aca":"**Takeaway** :- Most of the purchase amount of the shopping lies between 5000-10000 (Hmm..interesting).","f475f678":"The dataset here is a sample of the transactions made in a retail store. The store wants to know better the customer purchase behaviour against different products. Specifically, here the problem is a regression problem where we are trying to predict the dependent variable (the amount of purchase) with the help of the information contained in the other variables.","54b5819c":"**Takeaway** :- The purchasing amount is well distributed among all cities but as we know from above that the least number of people who came for shopping were from city A.\nDespite this fact there purchasing amount is similar to others which means people from city A spent more which means they are richer than others.","bf9aa8eb":"**Takeaway** :- Most of the people who came for shopping hail from City B, from which we can infer that the sale was in City B or people of city B are richer as compared to other cities.","95114008":"We now start with processing our data.\n<p>At first we will fill the missing data of the Product_Category_2 and Product_Category_3 with 0 because the data these columns refer to the quantity of products belonging to those category and so no data available for them means the purchase did not include any products from that category,therefore we can replace NaN with 0 here.<\/p>","6e8ad9f3":"<h3>Top ten most valuable customers<\/h3>","12dc8875":"<h3>Lets rotate the wheel now ! <\/h3>\nWe start with the training now...","8cd0ab99":"![](https:\/\/www.thesierraleonetelegraph.com\/wp-content\/uploads\/2018\/11\/black-friday-sale.jpg)","2f255606":"<h3>And finally lets print out some of the evaluation metrics for our model.<\/h3>","fdb55ca8":"<center>For any clarifications and suggestions please comment below. <br\/>**  Also if you like this kernel please vote for it as this will keep me motivated  **.<\/center>","e0d73c44":"Now we replace the string '4+' with integer 4 and changing the datatype to int.","76a0a8d5":"Now a major part is done in data preprocessing and now we move to splitting our data in **training** and **test** sets.","3b08af23":"We also need to change there datatypes from float to int.","257a075a":"**Takeaway** :- Most of the people who came for shopping had recently shifted to the city which is quite obvious as lots of things are required to set up the home after shifting.","2c3b3e42":"**Takeaway** :- The purchase amount in each age group is almost similar which means most of the products which were on sale were general items that could be used by all age groups.\n<br\/>\n**But wait a moment...**\nWe already know that mostly the sale was attended by the age group of 26-35 and the age group which had the least number of population is 0-17, however the twist here is that there purchase amount is almost similar to the others.","7d5275d7":"<p>The data exploration was very interesting, is'nt it ?<\/p>\n<h3>Now lets move on and try to build a prediction model based upon this<\/h3>","3fefc459":"<h2>PART 2:- Data Preprocessing<\/h2>","303a916e":"<center><h1>BLACK FRIDAY SALE ANALYSIS<\/h1><\/center>","011124fb":"We will also add Checkpoint callback to our model","f56021e4":"From above its clear that a large chunk of data is not available for 'Product_Category_2' and 'Product_Category_3'.\n<p>We will handle this part later in this kernel.<\/p>","2cd73a50":"**Takeaway** :- For this we cannot say much as we are unware of the occupation here.However its clear that people are in Occupation 4 and 0 outnumbered the others which some what means that these are well paid jobs or they have enough time for shopping.\nAlso, we can witness that male workers are outnumbering the female workers.","5505aab5":"We now remove the first two columns of data as they are not required here.","51a0ced6":"<p>From above it seems like we have some null values residing out there.<\/p>\n<p>Lets find out in more detail :<\/p>","2aca3413":"<h3>And the magic begins ! <\/h3>\n<p>We will now make predictions on the Test set<\/p>","89e94885":"From the above result we get the best Weights file.","7f782d50":"<h2>PART 1:- Data Analysis and Visualisation<\/h2>","940c4a47":"We start with **Label Encoding** our data to make this data machine readable."}}