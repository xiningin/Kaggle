{"cell_type":{"5b49bdf2":"code","6a9ced49":"code","d570d3d9":"code","b70cb518":"code","e7b80c47":"code","4b643f6b":"code","f5eba364":"code","6181b49b":"code","0be396b5":"code","682efead":"code","e72dba8d":"code","a21009de":"code","5a5faef9":"code","b5ab2147":"code","3462de9a":"code","46f4471c":"code","ee7174c2":"code","7e013a8c":"code","57bc4889":"code","7ab35c4e":"code","982225ad":"code","5a655072":"code","50a5796d":"code","fe3f1df8":"code","8ec71aaa":"code","70737379":"code","48b036e5":"code","0265574e":"code","a4167b0c":"code","8ce9b2e1":"code","ac120c6d":"code","339af4cf":"code","190e9e3f":"code","0b73bc8e":"code","eb1afc27":"code","7d661994":"code","7739999a":"code","880383c0":"code","f79b1ac3":"code","157787ba":"code","ddb421bb":"code","ee139cf7":"code","3f36307c":"code","dbc8dc22":"code","ec1d1e01":"code","8ef3a714":"code","2900ac99":"code","113125df":"code","c7a5422a":"code","1bb93d7b":"code","72e18ea8":"code","10331f62":"code","082b3bf4":"code","d3e10e87":"code","b10a42cc":"code","61b4c030":"code","c1c59d3e":"code","cec979e3":"code","b5f8c957":"code","aaba46f6":"markdown","38737676":"markdown","1a4a1c58":"markdown","bfa7e0bc":"markdown","85c912cc":"markdown","bd8690a4":"markdown","72811fc1":"markdown","417cff01":"markdown","0ed432fc":"markdown","912158bc":"markdown","60bbb98e":"markdown","eff17f30":"markdown","772817ec":"markdown","4e9f9c94":"markdown","c8380e88":"markdown"},"source":{"5b49bdf2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6a9ced49":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.head(10)","d570d3d9":"train.shape","b70cb518":"train.isnull()","e7b80c47":"train.describe()","4b643f6b":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap = 'viridis')","f5eba364":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',data = train)","6181b49b":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue = 'Sex',data= train,palette = 'RdBu_r')","0be396b5":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')","682efead":"sns.distplot(train['Age'].dropna(),kde=False,color = 'darkred',bins=10)","e72dba8d":"train['Age'].hist(bins=30,color='darkred',alpha=0.3)","a21009de":"train['Age'].hist(bins=30,color='darkred',alpha=0.3)","5a5faef9":"sns.countplot(x='SibSp',data=train)","b5ab2147":"train['Fare'].hist(color='green',bins=40,figsize=(8,4))","3462de9a":"ds = np.log1p(train['Age'])\n\nsns.histplot(data = train, x = ds, kde = True)","46f4471c":"def process_data(df):\n    df.Age = df[['Age', 'Pclass']].apply(impute_age, axis = 1)\n    df['Fare'] = np.log1p(df['Fare'])\n    df['Age'] = np.sqrt(df['Age'])\n    df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis = 1, inplace = True)\n    return df","ee7174c2":"plt.figure(figsize=(12,7))\nsns.boxplot(x='Pclass',y='Age',data=train,palette = 'winter')","7e013a8c":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass == 1:\n            return 37\n        \n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age\n        ","57bc4889":"train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)","7ab35c4e":"sns.heatmap(train.isnull(),yticklabels = False,cbar=False,cmap='viridis')","982225ad":"train.drop('Cabin',axis=1,inplace=True)","5a655072":"train.head()","50a5796d":"train.dropna(inplace=True)","fe3f1df8":"train.info()","8ec71aaa":"pd.get_dummies(train['Embarked'],drop_first=True).head()","70737379":"sex = pd.get_dummies(train['Sex'],drop_first = True)\nembark=pd.get_dummies(train['Embarked'],drop_first = True)","48b036e5":"train.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)","0265574e":"train.head()","a4167b0c":"train = pd.concat([train,sex,embark],axis=1)","8ce9b2e1":"train.head()","ac120c6d":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","339af4cf":"train.drop('Survived',axis=1).head()","190e9e3f":"train['Survived'].head()","0b73bc8e":"train.describe().T.style.background_gradient(subset = ['mean','50%'], cmap = 'Reds').background_gradient(subset = ['min', 'max'], cmap = 'Blues')","eb1afc27":"from sklearn.model_selection import train_test_split","7d661994":"X_train , X_test, y_train, y_test = train_test_split(train.drop('Survived',axis=1),\n                                                    train['Survived'],test_size =0.30,\n                                                    random_state=101)","7739999a":"from sklearn.linear_model import LogisticRegression","880383c0":"x= train.iloc[:, [2,3]].values  \ny= train.iloc[:, 4].values  ","f79b1ac3":"logmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","157787ba":"predictions = logmodel.predict(X_test)\nfrom sklearn.metrics import confusion_matrix","ddb421bb":"accuracy = confusion_matrix(y_test,predictions)\naccuracy\n","ee139cf7":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nprint(cross_val_score(logmodel,x,y,cv=5))\naccuracy = accuracy_score(y_test,predictions)*100\naccuracy","3f36307c":"from sklearn.metrics import classification_report\naccuracy = accuracy_score(y_test,predictions)*100\nprint(classification_report(predictions,y_test))","dbc8dc22":"from sklearn.tree import DecisionTreeClassifier \nclassifier= DecisionTreeClassifier(criterion='entropy', random_state=0)  \nclassifier.fit(X_train, y_train)  ","ec1d1e01":"#Predicting the test set result  \ny_pred= classifier.predict(X_test)  ","8ef3a714":"accuracyy = accuracy_score(y_test,y_pred)*100\naccuracyy\n","2900ac99":"print(cross_val_score(classifier,x,y,cv=5))","113125df":"print(classification_report(y_pred,y_test))","c7a5422a":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train, y_train)\n","1bb93d7b":"pred =knn.predict(X_test)\naccuracyKNN =accuracy_score(pred,y_test)*100\nprint(\"The accuracy OF KNN Model :\",accuracyKNN)\nprint(cross_val_score(knn,x,y,cv=5))\n","72e18ea8":"print(classification_report(pred,y_test))","10331f62":"from sklearn.svm import SVC\nX_train , X_test, y_train, y_test = train_test_split(train.drop('Survived',axis=1),\n                                                    train['Survived'],test_size =0.25,\n                                                    random_state=101)\nclassifiers = SVC(kernel='linear', random_state=0)  \nclassifiers.fit(X_train, y_train)\ny_predicted = classifiers.predict(X_test)  \n#Train ","082b3bf4":"accuracySVC =accuracy_score(y_predicted,y_test)*100\nprint(\"The accuracy OF Support vector :\",accuracySVC)\nprint(cross_val_score(classifiers,x,y,cv=5))","d3e10e87":"print(classification_report(y_predicted,y_test))","b10a42cc":"from sklearn.naive_bayes import GaussianNB\nclassifierss = GaussianNB()\nclassifierss.fit(X_train, y_train)\ny_predictt = classifierss.predict(X_test)  ","61b4c030":"accuracyNB =accuracy_score(y_predictt,y_test)*100\nprint(\"The accuracy OF GaussianNB :\",accuracyNB)","c1c59d3e":"print(classification_report(y_predictt,y_test))","cec979e3":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nparameters = [{\n'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n'C': [1,2,3,300,500],\n'max_iter': [1000,100000]}]\nclf = GridSearchCV(\n        SVC(), parameters, scoring='accuracy'\n    )\nclf.fit(X_train, y_train)\nprint(clf.best_params_)","b5f8c957":"from prettytable import PrettyTable\nTable = PrettyTable([\"Algorithm\", \"Accuracy\"])\nTable.add_row([\"LogisticRegression\", accuracy])\nTable.add_row([\"Decision Tree\", accuracyy])\nTable.add_row([\"KNN\", accuracyKNN])\nTable.add_row([\"SVC\", accuracySVC])\nTable.add_row([\"gaussianNB\", accuracyNB])\nprint(Table)","aaba46f6":"Now apply the function","38737676":"# Gaussian NB ","1a4a1c58":"our data is ready for the model","bfa7e0bc":"# Train Test Split","85c912cc":"# Data Cleaning","bd8690a4":"lets go ahead and drop the cabin column and the row in Embarked is NaN","72811fc1":"we want to fill the missing data instead of just droping the missing age data rows . one way to do thisby filling in the mean age of all pasanger(impulation) .however we can check the average age by passenger class for example","417cff01":"# Missing data\n","0ed432fc":"# DecisionTree ","912158bc":"# Converting Categorical Features\nnow we have to convert categorical features to dummy variables using pandas otherwise our machine learning algorithm won't be able to directly take in those fetures as inputs","60bbb98e":"# Support vector algorithm","eff17f30":"Lets check that heat map again","772817ec":"# Building a Logistic Regression model\nlets start by splitting the data into training set and test set(there is another test.csv that we have we will use all this data for trainimng","4e9f9c94":"# Training and Predicting","c8380e88":"# KNN Algorithm"}}