{"cell_type":{"a1bef76d":"code","8be118e4":"code","ccaff6d3":"code","b8d5cf97":"code","85430069":"code","97c88aab":"code","7bab12cf":"code","0afbe44c":"code","81b566cc":"code","685bdfe8":"code","229e2746":"code","2ba29be0":"code","f19b49c4":"code","60a04375":"code","3466fbb0":"code","de4bcbde":"code","0dc934ed":"code","96e3f4ad":"code","7fb03a64":"code","33ce78dd":"code","6588110b":"code","6595c725":"code","eae9f51a":"code","527a07cc":"code","cbdef055":"code","ba45df94":"code","3458312a":"code","082a4e21":"code","f37e0f5d":"code","5d7a77a1":"code","eba0a375":"code","f5b48a80":"code","a5fb1e26":"code","0a12e831":"code","c5c0dd08":"code","08b20899":"code","83e99956":"code","019f51e5":"code","1538bcd3":"code","e98f4987":"code","c64fc0ca":"code","9b937d45":"code","5b18eb68":"code","ef09232b":"code","686a869f":"code","3628d09f":"code","b448176f":"code","7a27574c":"code","e30ec5e0":"code","6a0bd2dd":"code","b1d1686c":"code","6b930d61":"code","e9764778":"code","2c7f37e5":"code","60bd2ee3":"code","9b35ec9c":"code","0d4b4d98":"code","a78492af":"code","cf215f0b":"code","15a22846":"code","0da6b309":"code","45190b1d":"code","3e0bc8b5":"code","65c686f4":"code","88107a98":"code","823a786e":"code","3baab75b":"code","ecb2acba":"code","49f1a1c9":"code","80d40071":"code","66f91add":"code","9063c7d2":"code","059323c6":"code","724845fb":"code","8d689796":"code","e691342e":"code","9bcc2210":"code","ea8cf8d7":"code","f5fb5c43":"code","deb4acf1":"code","f4bc05b1":"code","9a3839e0":"markdown","ae7c8546":"markdown","e304dbb3":"markdown","58b4db18":"markdown","13dbe78f":"markdown","bed9f043":"markdown","50936185":"markdown","4d306b47":"markdown","8038eb88":"markdown","485a325d":"markdown","5edb06b9":"markdown","0f94a6a1":"markdown","143ac14f":"markdown","d16b07bf":"markdown","dfb07897":"markdown","97f9d4bf":"markdown","5606f95b":"markdown","91b7aa07":"markdown","e7bead6c":"markdown"},"source":{"a1bef76d":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.linear_model import Ridge,RidgeCV\nfrom sklearn.linear_model import Lasso,LassoCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import explained_variance_score\n\nimport os\nfrom sklearn.preprocessing import scale \n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","8be118e4":"# Importing train.csv\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","ccaff6d3":"train.shape","b8d5cf97":"train.describe()","85430069":"test.describe()\n","97c88aab":"# Targt Variable\ntrain.SalePrice.describe()","7bab12cf":"sns.distplot(train['SalePrice']);\nprint('Skewness:',train['SalePrice'].skew())\nprint('Kurtosis:',train['SalePrice'].kurt())","0afbe44c":"train['SalePrice_log'] = np.log1p(train['SalePrice'])\nsns.distplot(train['SalePrice_log']);\nprint('Skewness:',train['SalePrice_log'].skew())\nprint('Kurtosis:',train['SalePrice_log'].kurt())\ntrain.drop('SalePrice',axis = 1,inplace = True)","81b566cc":"train['SalePrice_log'].head()","685bdfe8":"# #finding skewness and kurtosis for all\n# for col in num_cols:\n#     print('{:15}'.format(col), \n#           'Skewness: {:05.2f}'.format(train[col].skew()) , \n#           '   ' ,\n#           'Kurtosis: {:06.2f}'.format(train[col].kurt())  \n#          )","229e2746":"#Null Columns\ntrain.columns[train.isnull().sum() > 0 ]","2ba29be0":"test.columns[test.isnull().sum() > 0 ]","f19b49c4":"print(test.MasVnrType.head(10))\nprint(train.MasVnrType.head(10))\n\n","60a04375":"#There are \"None\" in data, so replacing with null\n\ntrain = train.replace('None',np.nan)\ntest =test.replace('None',np.nan)\n","3466fbb0":"#SUm of nulls\ntrain.loc[:, train.isnull().sum() > 0].isnull().sum().sort_values()\n\n","de4bcbde":"test.loc[:, test.isnull().sum() > 0].isnull().sum().sort_values()\n","0dc934ed":"test.drop([\"Id\"],axis = 1,inplace = True)\ntrain.drop([\"Id\"],axis = 1,inplace = True)\n","96e3f4ad":"#dropping columns which have more null dATA+\ntrain.drop([\"Fence\",\"Alley\",\"MiscFeature\",\"PoolQC\",\"FireplaceQu\",\"MasVnrType\"],axis = 1,inplace =True)\ntest.drop([\"Fence\",\"Alley\",\"MiscFeature\",\"PoolQC\",\"FireplaceQu\",\"MasVnrType\"],axis = 1,inplace =True)","7fb03a64":"#Deciding about LotFrontage\ntest['LotFrontage'].describe()","33ce78dd":"train.LotFrontage.fillna(train['LotFrontage'].median(),inplace = True)\ntest.LotFrontage.fillna(test['LotFrontage'].median(),inplace = True)","6588110b":"test.loc[:, test.isnull().sum() > 0].isnull().sum().sort_values()\n","6595c725":"#Replace all the empty garage with \"no\" as it doesn't have any garage\ntrain.GarageType.fillna(\"no\",inplace = True)\ntrain.GarageYrBlt.fillna(0,inplace = True)\ntrain.GarageFinish.fillna(\"no\",inplace = True)\ntrain.GarageQual.fillna(\"no\",inplace = True)\ntrain.GarageCond.fillna(\"no\",inplace = True)\n\n#Replace all the empty garage with \"no\" as it doesn't have any garage\ntest.GarageType.fillna(\"no\",inplace = True)\ntest.GarageYrBlt.fillna(0,inplace = True)\ntest.GarageFinish.fillna(\"no\",inplace = True)\ntest.GarageQual.fillna(\"no\",inplace = True)\ntest.GarageCond.fillna(\"no\",inplace = True)","eae9f51a":"test.loc[:, test.isnull().sum() > 0].isnull().sum().sort_values()\n","527a07cc":"train.BsmtQual.fillna(\"no\",inplace = True)\ntrain.BsmtCond.fillna(\"no\",inplace = True)\ntrain.BsmtFinType1.fillna(\"no\",inplace = True)\ntrain.BsmtExposure.fillna(\"no\",inplace = True)\ntrain.BsmtFinType2.fillna(\"no\",inplace = True)\n\n\ntest.BsmtQual.fillna(\"no\",inplace = True)\ntest.BsmtCond.fillna(\"no\",inplace = True)\ntest.BsmtFinType1.fillna(\"no\",inplace = True)\ntest.BsmtExposure.fillna(\"no\",inplace = True)\ntest.BsmtFinType2.fillna(\"no\",inplace = True)\n","cbdef055":"train.loc[:, train.isnull().sum() > 0].isnull().sum().sort_values()\n","ba45df94":"\ntrain['Electrical'].mode().iloc[0] \n","3458312a":"train.MasVnrArea.fillna(0,inplace = True)\ntrain.Electrical.fillna(train['Electrical'].mode().iloc[0],inplace = True)\n\ntest.MasVnrArea.fillna(0,inplace = True)\ntest.Electrical.fillna(test['Electrical'].mode().iloc[0],inplace = True)\n","082a4e21":"test.loc[:, test.isnull().sum() > 0].isnull().sum().sort_values()\n","f37e0f5d":"test.ExterQual.value_counts()","5d7a77a1":"test.MasVnrArea.fillna(0,inplace = True)\ntest.MSZoning.fillna(test.MSZoning.mode().iloc[0],inplace = True)\ntest.Functional.fillna(test.Functional.mode().iloc[0],inplace = True)\ntest.BsmtFullBath.fillna(test.BsmtFullBath.mode().iloc[0],inplace = True)\ntest.BsmtHalfBath.fillna(test.BsmtHalfBath.mode().iloc[0],inplace = True)\n\ntest.BsmtFinSF1.fillna(0,inplace = True)\ntest.BsmtFinSF2 .fillna(0,inplace = True)\ntest.BsmtUnfSF.fillna(0,inplace = True)\ntest.TotalBsmtSF.fillna(0,inplace = True)\n\ntest.BsmtUnfSF.fillna(0,inplace = True)\ntest.TotalBsmtSF.fillna(0,inplace = True)\ntest.GarageCars.fillna(0,inplace = True)\ntest.GarageArea.fillna(0,inplace = True)\n\ntest.SaleType.fillna(test.SaleType.mode().iloc[0],inplace = True)\ntest.KitchenQual.fillna(test.KitchenQual.mode().iloc[0],inplace = True)\ntest.Utilities.fillna(test.Utilities.mode().iloc[0],inplace = True)\n\ntest.Exterior1st.fillna(test.Exterior1st.mode().iloc[0],inplace = True)\ntest.Exterior2nd.fillna(test.Exterior2nd.mode().iloc[0],inplace = True)\n","eba0a375":"print(train.shape)\nprint(test.shape)","f5b48a80":"train.head(20)","a5fb1e26":"#seperating numerical data from categorical data for plotting correlation\nnum_cols = train.dtypes[train.dtypes != \"object\"].index\ncat_cols = train.dtypes[train.dtypes == \"object\"].index","0a12e831":"num_cols_test = test.dtypes[test.dtypes != \"object\"].index\ncat_cols_test = test.dtypes[test.dtypes == \"object\"].index","c5c0dd08":"#High correlations\ncor_numVar = train[num_cols].corr()\ncorHigh = cor_numVar[abs(cor_numVar['SalePrice_log']) > 0.5]\n\nplt.figure(figsize = (16, 10))\nsns.heatmap(train[corHigh.index].corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","08b20899":"#Encode the ordinal data, starting 0 as low\n\n# for cat in cat_cols_test:\n#     print('--'*40)\n#     print(cat)\n#     print(test[cat].value_counts())","83e99956":"test.ExterQual.value_counts()","019f51e5":"#Encode the ordinal data, starting 0 as low\n\n#basement\ntrain['BsmtFinType1'] = train['BsmtFinType1'].map({'no':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntrain['BsmtFinType2'] = train['BsmtFinType2'].map({'no':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntrain['BsmtExposure'] = train['BsmtExposure'].map({'no':0,'No':1,'Mn':2,'Av':3,'Gd':4})\ntrain['BsmtCond'] = train['BsmtCond'].map({'no':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntrain['BsmtQual'] = train['BsmtQual'].map({'no':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\n\n#Quality related\ntrain['ExterQual'] = train['ExterQual'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntrain['ExterCond'] = train['ExterCond'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntrain['HeatingQC'] = train['HeatingQC'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntrain['KitchenQual'] = train['KitchenQual'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n#train['FireplaceQu'] = train['FireplaceQu'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n\ntrain['GarageQual'] = train['GarageQual'].map({'no':0,'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntrain['GarageCond'] = train['GarageCond'].map({'no':0,'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n\ntrain['GarageFinish'] = train['GarageFinish'].map({'no':0,'Unf':1,'RFn':2,'Fin':3})\ntrain['GarageType'] = train['GarageType'].map({'no':0,'Detchd':1,'CarPort':2,'BuiltIn':3,\n                                                         'Basment':4,'Attchd':5,'2Types':6})\n\n#house realated\ntrain['LotShape'] = train['LotShape'].map({'Reg':3,'IR1':2,'IR2':1,'IR3':0})\ntrain['LandContour'] = train['LandContour'].map({'Low':0,'HLS':1,'Bnk':2,'Lvl':3})\ntrain['Utilities'] = train['Utilities'].map({'ELO': 0,'NoSeWa':1,'NoSewr':2,'AllPub':3})\ntrain['BldgType'] = train['BldgType'].map({'Twnhs':0,'TwnhsE':1,'Duplex':2,'2fmCon':3,'1Fam':4})\ntrain['HouseStyle'] = train['HouseStyle'].map({'1Story':0,'1.5Fin':1,'1.5Unf':2,'2Story':3,\n                                                         '2.5Fin':4,'2.5Unf':5,'SFoyer':6,'SLvl':7})\ntrain['Functional'] = train['Functional'].map({'Sal':0,'Sev':1,'Maj2':2,'Maj1':3,\n                                                         'Mod':4,'Min2':5,'Min1':6,'Typ':7})\n\n#others\ntrain['LandSlope'] = train['LandSlope'].map({'Gtl':0,'Mod':1,'Sev':2})\ntrain['Street'] = train['Street'].map({'Grvl':0,'Pave':1})\n#train['MasVnrType'] = train['MasVnrType'].map({'None':0,'BrkCmn':1,'BrkFace':2,'CBlock':3,'Stone':4})\ntrain['CentralAir'] = train['CentralAir'].map({'N':0,'Y':1})\ntrain['PavedDrive'] = train['PavedDrive'].map({'N':0,'P':1,'Y':2})\n\n","1538bcd3":"#Encode the ordinal data, starting 0 as low\n\n#basement\ntest['BsmtFinType1'] = test['BsmtFinType1'].map({'no':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntest['BsmtFinType2'] = test['BsmtFinType2'].map({'no':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntest['BsmtExposure'] = test['BsmtExposure'].map({'no':0,'No':1,'Mn':2,'Av':3,'Gd':4})\ntest['BsmtCond'] = test['BsmtCond'].map({'no':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest['BsmtQual'] = test['BsmtQual'].map({'no':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\n\n#Quality related\ntest['ExterQual'] = test['ExterQual'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['ExterCond'] = test['ExterCond'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['HeatingQC'] = test['HeatingQC'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['KitchenQual'] = test['KitchenQual'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n#train['FireplaceQu'] = train['FireplaceQu'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n\ntest['GarageQual'] = test['GarageQual'].map({'no':0,'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['GarageCond'] = test['GarageCond'].map({'no':0,'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n\ntest['GarageFinish'] = test['GarageFinish'].map({'no':0,'Unf':1,'RFn':2,'Fin':3})\ntest['GarageType'] = test['GarageType'].map({'no':0,'Detchd':1,'CarPort':2,'BuiltIn':3,\n                                                         'Basment':4,'Attchd':5,'2Types':6})\n\n#house realated\ntest['LotShape'] = test['LotShape'].map({'Reg':3,'IR1':2,'IR2':1,'IR3':0})\ntest['LandContour'] = test['LandContour'].map({'Low':0,'HLS':1,'Bnk':2,'Lvl':3})\ntest['Utilities'] = test['Utilities'].map({'ELO': 0,'NoSeWa':1,'NoSewr':2,'AllPub':3})\ntest['BldgType'] = test['BldgType'].map({'Twnhs':0,'TwnhsE':1,'Duplex':2,'2fmCon':3,'1Fam':4})\ntest['HouseStyle'] = test['HouseStyle'].map({'1Story':0,'1.5Fin':1,'1.5Unf':2,'2Story':3,\n                                                         '2.5Fin':4,'2.5Unf':5,'SFoyer':6,'SLvl':7})\ntest['Functional'] = test['Functional'].map({'Sal':0,'Sev':1,'Maj2':2,'Maj1':3,\n                                                         'Mod':4,'Min2':5,'Min1':6,'Typ':7})\n\n#others\ntest['LandSlope'] = test['LandSlope'].map({'Gtl':0,'Mod':1,'Sev':2})\ntest['Street'] = test['Street'].map({'Grvl':0,'Pave':1})\n#test['MasVnrType'] = test['MasVnrType'].map({'None':0,'BrkCmn':1,'BrkFace':2,'CBlock':3,'Stone':4})\ntest['CentralAir'] = test['CentralAir'].map({'N':0,'Y':1})\ntest['PavedDrive'] = test['PavedDrive'].map({'N':0,'P':1,'Y':2})\n\n","e98f4987":"pd.options.display.float_format = '{:.2f}'.format\nnp.set_printoptions(suppress = False)\ntrain.describe().transpose()","c64fc0ca":"#Years converting to Age\n\nCurrentYr = pd.to_numeric(datetime.datetime.now().year)\n\nYearBuilt = pd.to_numeric(train.YearBuilt)\nHouseAge = CurrentYr - YearBuilt\nlist(map(lambda x : x if x < 1000 else 0,HouseAge))\ntrain[\"HouseAge\"] = HouseAge\ntrain.drop(\"YearBuilt\",axis = 1,inplace = True)\n\nYearRemoAdd = pd.to_numeric(train.YearRemodAdd)\nYearRemoAdd = CurrentYr - YearRemoAdd\nlist(map(lambda x : x if x < 1000 else 0,YearRemoAdd))\ntrain[\"RemodAge\"] = YearRemoAdd\ntrain.drop(\"YearRemodAdd\",axis = 1,inplace = True)\n\n\nGarYrblt = pd.to_numeric(train.GarageYrBlt)\nGarageAge = CurrentYr - GarYrblt\nlist(map(lambda x : x if x < 1000 else 0,GarageAge))\ntrain[\"GarageAge\"] = GarageAge\ntrain.drop([\"GarageYrBlt\"],axis =1,inplace = True)\n\nYrSold = pd.to_numeric(train.YrSold)\nSoldAge = CurrentYr - YrSold\nlist(map(lambda x : x if x < 1000 else 0,SoldAge))\ntrain[\"SoldAge\"] = SoldAge\ntrain.drop([\"SoldAge\"],axis =1,inplace = True)\n","9b937d45":"#Years converting to Age\n\nCurrentYr = pd.to_numeric(datetime.datetime.now().year)\n\nYearBuilt = pd.to_numeric(test.YearBuilt)\nHouseAge = CurrentYr - YearBuilt\nlist(map(lambda x : x if x < 1000 else 0,HouseAge))\ntest[\"HouseAge\"] = HouseAge\ntest.drop(\"YearBuilt\",axis = 1,inplace = True)\n\nYearRemoAdd = pd.to_numeric(test.YearRemodAdd)\nYearRemoAdd = CurrentYr - YearRemoAdd\nlist(map(lambda x : x if x < 1000 else 0,YearRemoAdd))\ntest[\"RemodAge\"] = YearRemoAdd\ntest.drop(\"YearRemodAdd\",axis = 1,inplace = True)\n\n\nGarYrblt = pd.to_numeric(test.GarageYrBlt)\nGarageAge = CurrentYr - GarYrblt\nlist(map(lambda x : x if x < 1000 else 0,GarageAge))\ntest[\"GarageAge\"] = GarageAge\ntest.drop([\"GarageYrBlt\"],axis =1,inplace = True)\n\nYrSold = pd.to_numeric(test.YrSold)\nSoldAge = CurrentYr - YrSold\nlist(map(lambda x : x if x < 1000 else 0,SoldAge))\ntest[\"SoldAge\"] = SoldAge\ntest.drop([\"SoldAge\"],axis =1,inplace = True)\n","5b18eb68":"#Create some other features\n\ntrain['Total_sqr_footage'] = (train['BsmtFinSF1'] + train['BsmtFinSF2'] +\n                                 train['1stFlrSF'] + train['2ndFlrSF'])\ntrain['Total_Bathrooms'] = (train['FullBath'] + (0.5*train['HalfBath']) + \n                               train['BsmtFullBath'] + (0.5*train['BsmtHalfBath']))\n\ntrain['Total_porch_sf'] = (train['OpenPorchSF'] + train['3SsnPorch'] +\n                              train['EnclosedPorch'] + train['ScreenPorch'] +\n                             train['WoodDeckSF'])\n\n#simplified features\ntrain['haspool'] = train['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain['has2ndfloor'] = train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain['hasgarage'] = train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain['hasbsmt'] = train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain['hasfireplace'] = train['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","ef09232b":"#Create some other features\n\ntest['Total_sqr_footage'] = (test['BsmtFinSF1'] + test['BsmtFinSF2'] +\n                                 test['1stFlrSF'] + test['2ndFlrSF'])\ntest['Total_Bathrooms'] = (test['FullBath'] + (0.5*test['HalfBath']) + \n                               test['BsmtFullBath'] + (0.5*test['BsmtHalfBath']))\n\ntest['Total_porch_sf'] = (test['OpenPorchSF'] + test['3SsnPorch'] +\n                              test['EnclosedPorch'] + test['ScreenPorch'] +\n                             test['WoodDeckSF'])\n\n#simplified features\ntest['haspool'] = test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntest['has2ndfloor'] = test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntest['hasgarage'] = test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntest['hasbsmt'] = test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntest['hasfireplace'] = test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","686a869f":"num_cols = train.dtypes[train.dtypes != \"object\"].index\nnum_cols\ncat_cols = train.dtypes[train.dtypes == \"object\"].index\ncat_cols\n","3628d09f":"num_cols_test = test.dtypes[train.dtypes != \"object\"].index\nnum_cols_test\ncat_cols_test = test.dtypes[train.dtypes == \"object\"].index\ncat_cols_test\n","b448176f":"\n# Get the dummy variables for the feature\nstatus = pd.get_dummies(train[cat_cols],drop_first = True)\nstatus_test = pd.get_dummies(test[cat_cols],drop_first = True)\n# Check what the dataset 'status' looks like\nstatus_test.head()\n","7a27574c":"\ntrain.drop(cat_cols, axis = 1,inplace = True)\ntest.drop(cat_cols, axis = 1,inplace = True)\n","e30ec5e0":"# Add the results to the original housing dataframe\ntrain = pd.concat([train, status], axis = 1)\ntest = pd.concat([test, status_test], axis = 1)\n# Now let's see the head of our dataframe.\ntest.head()","6a0bd2dd":"train.head()","b1d1686c":"# Split the train dataset into X and y\ny_train = train.pop('SalePrice_log')\nX_train = train","6b930d61":"num_cols = train.dtypes[train.dtypes != \"object\"].index\nnum_cols","e9764778":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n\nX_train.head()","2c7f37e5":"test[num_cols_test] = scaler.fit_transform(test[num_cols_test])\n\ntest.head()","60bd2ee3":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","9b35ec9c":"# Running RFE with the output number of the variable equal to 10\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 20)             # running RFE\nrfe = rfe.fit(X_train, y_train)","0d4b4d98":"col = X_train.columns[rfe.support_]\nX_train_rfe = X_train[col]","a78492af":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)","cf215f0b":"lm = sm.OLS(y_train,X_train_rfe).fit()   # Running the linear model","15a22846":"print(lm.summary())","0da6b309":"X_train_new = X_train_rfe.drop(['MSZoning_FV'], axis=1)\nX_train_new = X_train_new.drop(['BsmtFinSF1'], axis=1)\nX_train_new = X_train_new.drop(['Total_sqr_footage'], axis=1)\n","45190b1d":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()\n#Let's see the summary of our linear model\nprint(lm.summary())","3e0bc8b5":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","65c686f4":"X_train_new = X_train_new.drop(['RoofMatl_CompShg'], axis=1)\nX_train_new = X_train_new.drop(['PoolArea'], axis=1)\n","88107a98":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","823a786e":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()\n#Let's see the summary of our linear model\nprint(lm.summary())","3baab75b":"#droping related to Roofmaterial\nX_train_new = X_train_new.drop(['RoofMatl_WdShake'], axis=1)\nX_train_new = X_train_new.drop(['RoofMatl_Tar&Grv'], axis=1)\nX_train_new = X_train_new.drop(['RoofMatl_WdShngl'], axis=1)\nX_train_new = X_train_new.drop(['RoofMatl_Membran'], axis=1)\nX_train_new = X_train_new.drop(['RoofMatl_Metal'], axis=1)\nX_train_new = X_train_new.drop(['RoofMatl_Roll'], axis=1)\n","ecb2acba":"\n#model5\n# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()\n#Let's see the summary of our linear model\nprint(lm.summary())","49f1a1c9":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","80d40071":"#test set\nX_train_new = X_train_new.drop([\"const\"],axis = 1)\nX_test_new = test[X_train_new.columns]\n","66f91add":"print(X_train_new.shape)\nprint(X_test_new.shape)","9063c7d2":"X_train_new.columns","059323c6":"# Predict \nX_test_new = sm.add_constant(X_test_new)\ny_pred = lm.predict(X_test_new)","724845fb":"alphas = 10**np.linspace(10,-2,100)*0.5\nalphas\n\nridge = Ridge(normalize = True)\ncoefs = []\n\nfor a in alphas:\n    ridge.set_params(alpha = a)\n    ridge.fit(X_train_new, y_train)\n    coefs.append(ridge.coef_)\n    \nnp.shape(coefs)\nprint(coefs)","8d689796":"#Finding ALpha\n\nridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\nridgecv.fit(X_train_new, y_train)\nridgecv.alpha_\n","e691342e":"#fitting on model\nridge4 = Ridge(alpha = ridgecv.alpha_, normalize = True)\nridge4.fit(X_train_new, y_train)\n\n# Adding a constant variable \nX_test_new = test[X_train_new.columns]\n\ny_pred_ridge = ridge4.predict(X_test_new)\n","9bcc2210":"pd.Series(ridge4.coef_, index = X_train_new.columns).sort_values(ascending = False)","ea8cf8d7":"results = pd.Series(y_pred_ridge)","f5fb5c43":"converted_results = [(np.exp(x)) for x in [i for i in results]]","deb4acf1":"submit = pd.Series(converted_results)\n\nsubmit","f4bc05b1":"submit.to_csv('submit.csv', sep=',',index = False)\n","9a3839e0":"### Ridge regression","ae7c8546":"### Creating New Features","e304dbb3":"There are outliers in data, So lets replace Null values with median but not mean","58b4db18":"#### RFE","13dbe78f":"### Normalizing data ","bed9f043":"##VIF\n\nVariance Inflation Factor detects multicollinearity in regression analysis. \nFor a variable whose VIF mora than 10 is considered to be highly correlated.","50936185":"### Understanding data","4d306b47":"### Feature Engineering","8038eb88":"#### Dummy variable creation","485a325d":"#### Cleaning Data","5edb06b9":"#### Correlations","0f94a6a1":"#### Loading libraries, data","143ac14f":"# House Prices EDA+RFE+Ridge Regression","d16b07bf":"As we observe, there is much difference in the values of max compared with mean and 75 percentile,\nwhich infers the data is not normally distributed. So, first will make the data normal","dfb07897":"From 1460 records, we remained with 1338 rcords which is good enough to train","97f9d4bf":"### Modelling\n","5606f95b":"##### Scaling","91b7aa07":"So,need to reduce those field's who has VIF more than 10, But one by one which are not correlated with each other","e7bead6c":"### Overview:\n\n**This kerenel includes an EDA of house prices and then the feaures are reduced by using automatic feature \nreduction technique RFE and then used Ridge Regression to improve Overfitting**"}}