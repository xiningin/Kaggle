{"cell_type":{"e48c6d69":"code","889a00c9":"code","52805c06":"code","c32a851d":"code","3f7ec46f":"code","1a9a780f":"code","07edfbab":"code","694ec679":"code","cb7e4518":"code","8ec9cca7":"code","f493b6fe":"code","9c5b06b9":"code","27b10b45":"code","c86695ec":"code","f0e67445":"code","1af2c0fb":"code","f1b629ce":"code","8a57e69c":"code","e44fdf77":"code","7eb8f8d5":"code","ef6dcf7a":"code","4e460146":"code","90c700d2":"code","73b13e3e":"code","34187d8b":"code","2db2e768":"markdown","cbb7325a":"markdown","c495d899":"markdown"},"source":{"e48c6d69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","889a00c9":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_data.head()","52805c06":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","c32a851d":"train_data.info()\ntrain_data.describe()","3f7ec46f":"train_data['Age'] = train_data['Age'].fillna(-1)\ntrain_data.isnull().sum()","1a9a780f":"test_data['Age'] = test_data['Age'].fillna(-1)\ntest_data.isnull().sum()","07edfbab":"test_data['Fare'] = test_data['Fare'].fillna(-1)\ntest_data.isnull().sum()","694ec679":"women = train_data.loc[train_data.Sex == 'female']['Survived']\nrate_women = sum(women)\/len(women)\nprint('% of women who survived:', rate_women)\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\nprint(\"% of men who survived:\", rate_men)","cb7e4518":"train_data['Embarked'].fillna(embarked_mode)","8ec9cca7":"\n\ndata = [train_data, test_data]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'travelled_alone'] = 'No'\n    dataset.loc[dataset['relatives'] == 0, 'travelled_alone'] = 'Yes'\n","f493b6fe":"train_data.head()\n","9c5b06b9":"test_data.head()","27b10b45":"test_data.isnull().sum()","c86695ec":"from sklearn.ensemble import RandomForestClassifier\n\n# set seed for reproducibility\nSEED = 1\n\ny = train_data[\"Survived\"]\n\nfeatures = [\n            \"Pclass\", \n            \"Sex\", \n            #\"SibSp\", \n            #\"Parch\", \n            \"travelled_alone\", \n            \"Age\", \n            \"Fare\"\n           ]\nX = pd.get_dummies(train_data[features])\nX_test_original = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=SEED)\nmodel.fit(X, y)\npredictions = model.predict(X_test_original)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('Submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\nmodel.score(X, y)","f0e67445":"output.describe()","1af2c0fb":"#model.feature_importances_","f1b629ce":"#X.columns","8a57e69c":"# install pandas profiling\n#!pip install https:\/\/github.com\/pandas-profiling\/pandas-profiling\/archive\/master.zip","e44fdf77":"#import pandas_profiling\n#Report = pandas_profiling.ProfileReport(train_data)\n#Report","7eb8f8d5":"#import matplotlib.pyplot as plt\n## Create a pd.Series of features importances\n#importances = pd.Series(data=model.feature_importances_,\n#                        index= X.columns)\n\n## Sort importances\n#importances_sorted = importances.sort_values()\n\n## Draw a horizontal barplot of importances_sorted\n#importances_sorted.plot(kind='barh', color='lightgreen')\n#plt.title('Features Importances')\n#plt.show()","ef6dcf7a":"# \u0e14\u0e39\u0e27\u0e48\u0e32\u0e21\u0e35\u0e2d\u0e30\u0e44\u0e23\u0e43\u0e2b\u0e49\u0e1b\u0e23\u0e31\u0e1a\u0e1a\u0e49\u0e32\u0e07\n#rf.get_params()","4e460146":"# \u0e2a\u0e23\u0e49\u0e32\u0e07 grid \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e2b\u0e49 fit \u0e01\u0e27\u0e32\u0e14 \u0e44\u0e1b\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\n#from sklearn.model_selection import GridSearchCV\n\n# Define a grid of hyperparameter 'params_rf'\n#params_rf = {\n#            'n_estimators': [300, 400, 500],\n#            'max_depth': [4, 6, 8],\n#            'min_samples_leaf': [0.1, 0.2],\n#            'max_features': ['log2', 'sqrt']\n #           }\n\n# Instantiate 'grid_rf'\n#grid_rf = GridSearchCV(estimator=rf,\n #                      param_grid=params_rf,\n#                       cv=3,\n#                       scoring='neg_mean_squared_error',\n#                       verbose=1,\n#                       n_jobs=1)\n","90c700d2":"# Fit 'grid_rf' to the training set\n#grid_rf.fit(X_train, y_train)","73b13e3e":"# Extract the best hyperparameters from 'grid_rf'\n#best_hyperparams = grid_rf.best_params_\n#print('Best hyperparameters:\\n', best_hyperparams)","34187d8b":"# Extract the best model from 'grid_rf'\n#best_model = grid_rf.best_estimator_\n\n# Predict the test set labels\n#y_pred = best_model.predict(X_test)\n\n# Evaluate the test set RMSE\n#rmse_test = MSE(y_test, y_pred)**(1\/2)\n\n# print the test set RMSE\n#print('Test RMSE of best model: {:.3f}'.format(rmse_test))\n","2db2e768":"### EDA","cbb7325a":"### Visualizing features importances","c495d899":"# \u0e25\u0e2d\u0e07 finetune \u0e14\u0e39 "}}