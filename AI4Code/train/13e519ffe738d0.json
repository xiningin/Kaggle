{"cell_type":{"a9948dce":"code","4e99848d":"code","42fb3247":"code","dbe41509":"code","064769f5":"code","6a6890b1":"code","7d4fd4fe":"code","07c66bad":"code","f7e09df5":"code","451ed26a":"code","fd197318":"code","abc6222a":"code","aacbe72c":"code","01b8af61":"code","2e1599b2":"code","86577c0d":"code","def11170":"code","ba8f222e":"code","b13ef70a":"code","0f9f5b96":"code","5bc46253":"code","fc99aed7":"code","eb7ae604":"code","3c0ab4ae":"code","99ed7fd3":"code","909e835d":"code","1c215bed":"code","dc8d486b":"code","ad33fcba":"code","6d7b2132":"code","dd0dcfec":"code","ce454b3a":"code","1206574a":"code","969b4492":"code","2a957c0d":"code","65fd4e83":"code","83226b8d":"code","57525fc3":"markdown","3244d2a9":"markdown","d03106d6":"markdown","ae0b81e7":"markdown","4a843c1f":"markdown","ef879d04":"markdown","57c1d050":"markdown","03b48205":"markdown","d41c0c68":"markdown","b8881d1b":"markdown","e79cee66":"markdown","e3897ed0":"markdown","ecb9c15b":"markdown","2a8675f1":"markdown","ba84d4fd":"markdown","fb8ad6ad":"markdown","b81b567a":"markdown","8736d236":"markdown","d0afb022":"markdown","2d4474cf":"markdown","4300d235":"markdown","c93596df":"markdown","8396e64c":"markdown","25ca2fe6":"markdown","d4967495":"markdown","c764514c":"markdown","ba9c7b09":"markdown","99c421e4":"markdown","f0d96f06":"markdown","c2f819df":"markdown"},"source":{"a9948dce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e99848d":"import pickle # to read .p files\nfrom keras.utils import to_categorical\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.callbacks import EarlyStopping\nimport pandas as pd\nimport matplotlib.pyplot as plt","42fb3247":"train_file = open(\"\/kaggle\/input\/germantrafficsigns\/train.p\", \"rb\")\ntrain_data_full = pickle.load(train_file)\ntrain_file.close()\n\nvalid_file = open(\"\/kaggle\/input\/germantrafficsigns\/valid.p\", \"rb\")\nvalid_data_full = pickle.load(valid_file)\nvalid_file.close()\n\ntest_file = open(\"\/kaggle\/input\/germantrafficsigns\/test.p\", \"rb\")\ntest_data_full = pickle.load(test_file)\ntest_file.close()","dbe41509":"print(train_data_full.keys())\nprint(\"features shape: \", train_data_full[\"features\"].shape)\nprint(\"labels shape: \", train_data_full[\"labels\"].shape)","064769f5":"train_data = train_data_full[\"features\"]\nvalid_data = valid_data_full[\"features\"]\ntest_data = test_data_full[\"features\"]\n\n# normalization\ntrain_data = train_data\/255.0\nvalid_data = valid_data\/255.0\ntest_data = test_data\/255.0\n\n# prepare labels\ntrain_labels = to_categorical(train_data_full[\"labels\"]) \nvalid_labels = to_categorical(valid_data_full[\"labels\"])\ntest_labels = to_categorical(test_data_full[\"labels\"])","6a6890b1":"train_data[100]\npicture = train_data[100]\nplt.imshow(picture)\nplt.show()","7d4fd4fe":"signnames = pd.read_csv(\"\/kaggle\/input\/germantrafficsigns\/signnames.csv\")\nprint(signnames.head())\nnumber_of_classes = len(signnames[\"ClassId\"])\nprint(\"number of classes = \", number_of_classes)","07c66bad":"# build the first model: without Batch Normalization\nmy_model = models.Sequential()\nmy_model.add(layers.Conv2D(filters = 20, kernel_size = (3,3), activation = \"relu\", input_shape = (32, 32, 3)))\nmy_model.add(layers.MaxPooling2D(pool_size = (2,2)))\nmy_model.add(layers.Dropout(0.3))\nmy_model.add(layers.Conv2D(filters = 40, kernel_size = (3,3), activation = \"relu\"))\nmy_model.add(layers.MaxPooling2D(pool_size = (2,2)))\nmy_model.add(layers.Dropout(0.3))\nmy_model.add(layers.Conv2D(filters = 80, kernel_size = (3,3), activation = \"relu\"))\n\nmy_model.add(layers.Flatten())\nmy_model.add(layers.Dropout(0.3))\nmy_model.add(layers.Dense(units = 64, activation = \"relu\"))\nmy_model.add(layers.Dense(units = number_of_classes, activation = \"softmax\"))\n\n#save initial weights for later experiments\ninitial_weights= my_model.get_weights()\n\nmy_model.summary()","f7e09df5":"#Build a model with Batch Normalization\nmy_model_norm = models.Sequential()\nmy_model_norm.add(layers.Conv2D(filters = 20, kernel_size = (3,3), activation = \"relu\", input_shape = (32, 32, 3)))\nmy_model_norm.add(layers.MaxPooling2D(pool_size = (2,2)))\nmy_model_norm.add(layers.Dropout(0.3))\nmy_model_norm.add(layers.BatchNormalization())\nmy_model_norm.add(layers.Conv2D(filters = 40, kernel_size = (3,3), activation = \"relu\"))\nmy_model_norm.add(layers.MaxPooling2D(pool_size = (2,2)))\nmy_model_norm.add(layers.Dropout(0.3))\nmy_model_norm.add(layers.BatchNormalization())\nmy_model_norm.add(layers.Conv2D(filters = 80, kernel_size = (3,3), activation = \"relu\"))\n\nmy_model_norm.add(layers.Flatten())\nmy_model_norm.add(layers.Dropout(0.3))\nmy_model_norm.add(layers.BatchNormalization())\nmy_model_norm.add(layers.Dense(units = 64, activation = \"relu\"))\nmy_model_norm.add(layers.BatchNormalization())\nmy_model_norm.add(layers.Dense(units = number_of_classes, activation = \"softmax\"))\n\n#save initial weights for later experiments\ninitial_weights_norm = my_model_norm.get_weights()\n\nmy_model_norm.summary()","451ed26a":"# train model\noptimizer_use = optimizers.RMSprop(learning_rate = 0.001, rho = 0.9, momentum = 0.0) # these values are the default values indeed\nmy_model.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'val_categorical_accuracy', patience = 8, restore_best_weights = True)]\nhistory = my_model.fit(train_data, train_labels, epochs = 20, batch_size = 32, validation_data = (valid_data, valid_labels), callbacks = callbacks_list)","fd197318":"print(history.history.keys())\nacc_values = history.history['categorical_accuracy']\nval_acc_values = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values) + 1)\nplt.plot(epochs, acc_values, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","abc6222a":"evaluations = my_model.evaluate(test_data, test_labels)","aacbe72c":"# train model\noptimizer_use = optimizers.RMSprop(learning_rate = 0.001, rho = 0.9, momentum = 0.0) # these values are the default values indeed\nmy_model_norm.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'val_categorical_accuracy', patience = 8, restore_best_weights = True)]\nhistory = my_model_norm.fit(train_data, train_labels, epochs = 20, batch_size = 32, validation_data = (valid_data, valid_labels), callbacks = callbacks_list)","01b8af61":"print(history.history.keys())\nacc_values_norm = history.history['categorical_accuracy']\nval_acc_values_norm = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values_norm) + 1)\nplt.plot(epochs, acc_values_norm, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values_norm, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","2e1599b2":"evaluations = my_model_norm.evaluate(test_data, test_labels)","86577c0d":"epochs = range(1, len(acc_values) + 1)\nepochs_norm = range(1, len(acc_values_norm) + 1)\nplt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy without normalization')\nplt.plot(epochs_norm, val_acc_values_norm, 'r', label = 'Validation accuracy with normalization')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","def11170":"# reset the initial weights of the model\nmy_model.set_weights(initial_weights)\n# train model\noptimizer_use = optimizers.RMSprop(learning_rate = 0.001, rho = 0.2, momentum = 0.0) # use small rho value\nmy_model.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'val_categorical_accuracy', patience = 8, restore_best_weights = True)]\nhistory = my_model.fit(train_data, train_labels, epochs = 20, batch_size = 32, validation_data = (valid_data, valid_labels), callbacks = callbacks_list)\n\nacc_values_small_rho = history.history['categorical_accuracy']\nval_acc_values_small_rho = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values_small_rho) + 1)\nplt.plot(epochs, acc_values_small_rho, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values_small_rho, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","ba8f222e":"# reset the initial weights of the model\nmy_model_norm.set_weights(initial_weights_norm)\n# train model\noptimizer_use = optimizers.RMSprop(learning_rate = 0.001, rho = 0.2, momentum = 0.0) # use small rho value\nmy_model_norm.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'val_categorical_accuracy', patience = 8, restore_best_weights = True)]\nhistory = my_model_norm.fit(train_data, train_labels, epochs = 20, batch_size = 32, validation_data = (valid_data, valid_labels), callbacks = callbacks_list)\n\nacc_values_norm_small_rho = history.history['categorical_accuracy']\nval_acc_values_norm_small_rho = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values_norm_small_rho) + 1)\nplt.plot(epochs, acc_values_norm_small_rho, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values_norm_small_rho, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","b13ef70a":"epochs = range(1, len(acc_values_small_rho) + 1)\nepochs_norm = range(1, len(acc_values_norm_small_rho) + 1)\nplt.plot(epochs, val_acc_values_small_rho, 'b', label = 'Validation accuracy without normalization')\nplt.plot(epochs_norm, val_acc_values_norm_small_rho, 'r', label = 'Validation accuracy with normalization')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","0f9f5b96":"# reset the initial weights of the model\nmy_model.set_weights(initial_weights)\n# train model\noptimizer_use = optimizers.RMSprop(learning_rate = 0.001, rho = 0.9, momentum = 0.0) # default values\nmy_model.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'val_categorical_accuracy', patience = 8, restore_best_weights = True)]\nhistory = my_model.fit(train_data, train_labels, epochs = 20, batch_size = 6400, validation_data = (valid_data, valid_labels), callbacks = callbacks_list) # large batch_size","5bc46253":"acc_values_large_bs = history.history['categorical_accuracy']\nval_acc_values_large_bs = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values_large_bs) + 1)\nplt.plot(epochs, acc_values_large_bs, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values_large_bs, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","fc99aed7":"# reset the initial weights of the model\nmy_model_norm.set_weights(initial_weights_norm)\n# train model\noptimizer_use = optimizers.RMSprop(learning_rate = 0.001, rho = 0.9, momentum = 0.0) # default values\nmy_model_norm.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'val_categorical_accuracy', patience = 8, restore_best_weights = True)]\nhistory = my_model_norm.fit(train_data, train_labels, epochs = 20, batch_size = 6400, validation_data = (valid_data, valid_labels), callbacks = callbacks_list) # large batch_size","eb7ae604":"acc_values_norm_large_bs = history.history['categorical_accuracy']\nval_acc_values_norm_large_bs = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values_norm_large_bs) + 1)\nplt.plot(epochs, acc_values_norm_large_bs, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values_norm_large_bs, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","3c0ab4ae":"# reset the initial weights of the model\nmy_model.set_weights(initial_weights)\n# train model\noptimizer_use = optimizers.RMSprop(learning_rate = 0.001, rho = 0.9, momentum = 0.9) # increased momentum\nmy_model.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'val_categorical_accuracy', patience = 5, restore_best_weights = True)]\nhistory = my_model.fit(train_data, train_labels, epochs = 20, batch_size = 6400, validation_data = (valid_data, valid_labels), callbacks = callbacks_list) # large batch_size","99ed7fd3":"acc_values_large_momentum = history.history['categorical_accuracy']\nval_acc_values_large_momentum = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values_large_momentum) + 1)\nplt.plot(epochs, acc_values_large_momentum, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values_large_momentum, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","909e835d":"epochs = range(1, len(acc_values_large_bs) + 1)\nepochs_momentum = range(1, len(acc_values_large_momentum) + 1)\nplt.plot(epochs, val_acc_values_large_bs, 'b', label = 'Validation accuracy: large batch size, momentum = 0')\nplt.plot(epochs_momentum, val_acc_values_large_momentum, 'r', label = 'Validation accuracy: large batch size, large momentum')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","1c215bed":"history = my_model.fit(train_data, train_labels, epochs = 20, batch_size = 6400, validation_data = (valid_data, valid_labels), callbacks = callbacks_list) # large batch_size\n\nacc_values = history.history['categorical_accuracy']\nval_acc_values = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values) + 1)\nplt.plot(epochs, acc_values, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","dc8d486b":"# reset the initial weights of the model\nmy_model.set_weights(initial_weights)\n# train model\noptimizer_use = optimizers.RMSprop(learning_rate = 0.001, rho = 0.9, momentum = 0.9) # increased momentum\nmy_model.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'val_categorical_accuracy', patience = 8, restore_best_weights = True)]\nhistory = my_model.fit(train_data, train_labels, epochs = 20, batch_size = 32, validation_data = (valid_data, valid_labels), callbacks = callbacks_list) # small batch_size","ad33fcba":"acc_values_smallBS_largeMomentum = history.history['categorical_accuracy']\nval_acc_values_smallBS_largeMomentum = history.history['val_categorical_accuracy']\nepochs_smallBS_largeMomentum = range(1, len(acc_values_smallBS_largeMomentum) + 1)\nplt.plot(epochs_smallBS_largeMomentum, acc_values_smallBS_largeMomentum, 'b', label = 'Training accuracy')\nplt.plot(epochs_smallBS_largeMomentum, val_acc_values_smallBS_largeMomentum, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","6d7b2132":"# reset the initial weights of the model\nmy_model_norm.set_weights(initial_weights_norm)\n# train model\noptimizer_use = optimizers.RMSprop(learning_rate = 0.001, rho = 0.9, momentum = 0.9) # increased momentum\nmy_model_norm.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'val_categorical_accuracy', patience = 8, restore_best_weights = True)]\nhistory = my_model_norm.fit(train_data, train_labels, epochs = 20, batch_size = 32, validation_data = (valid_data, valid_labels), callbacks = callbacks_list) # small batch_size","dd0dcfec":"acc_values_smallBS_largeMomentum_norm = history.history['categorical_accuracy']\nval_acc_values_smallBS_largeMomentum_norm = history.history['val_categorical_accuracy']\nepochs_smallBS_largeMomentum_norm = range(1, len(acc_values_smallBS_largeMomentum_norm) + 1)\nplt.plot(epochs_smallBS_largeMomentum_norm, acc_values_smallBS_largeMomentum_norm, 'b', label = 'Training accuracy')\nplt.plot(epochs_smallBS_largeMomentum_norm, val_acc_values_smallBS_largeMomentum_norm, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","ce454b3a":"# reset the initial weights of the model\nmy_model.set_weights(initial_weights)\n# train model\noptimizer_use = optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999)\nmy_model.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'categorical_accuracy', patience = 5, restore_best_weights = True)]\nhistory = my_model.fit(train_data, train_labels, epochs = 20, batch_size = 6400, validation_data = (valid_data, valid_labels), callbacks = callbacks_list) # large batch_size\n\nacc_values = history.history['categorical_accuracy']\nval_acc_values = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values) + 1)\nplt.plot(epochs, acc_values, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","1206574a":"# reset the initial weights of the model\nmy_model.set_weights(initial_weights)\n# train model\noptimizer_use = optimizers.Adam(learning_rate = 0.002, beta_1 = 0.9, beta_2 = 0.999) # increased learning_rate\nmy_model.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'categorical_accuracy', patience = 5, restore_best_weights = True)]\nhistory = my_model.fit(train_data, train_labels, epochs = 20, batch_size = 6400, validation_data = (valid_data, valid_labels), callbacks = callbacks_list) # large batch_size","969b4492":"acc_values_large_lr = history.history['categorical_accuracy']\nval_acc_values_large_lr = history.history['val_categorical_accuracy']\nepochs_large_lr = range(1, len(acc_values_large_lr) + 1)\nplt.plot(epochs_large_lr, acc_values_large_lr, 'b', label = 'Training accuracy')\nplt.plot(epochs_large_lr, val_acc_values_large_lr, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","2a957c0d":"plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy, learning rate = 0.001')\nplt.plot(epochs_large_lr, val_acc_values_large_lr, 'r', label = 'Validation accuracy, learning rate = 0.002')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","65fd4e83":"# reset the initial weights of the model\nmy_model.set_weights(initial_weights)\n# train model\noptimizer_use = optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999)\nmy_model.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'categorical_accuracy', patience = 5, restore_best_weights = True)]\nhistory = my_model.fit(train_data, train_labels, epochs = 20, batch_size = 32, validation_data = (valid_data, valid_labels), callbacks = callbacks_list) # large batch_size\n\nacc_values = history.history['categorical_accuracy']\nval_acc_values = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values) + 1)\nplt.plot(epochs, acc_values, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","83226b8d":"my_model_without_Dropout = models.Sequential()\nmy_model_without_Dropout.add(layers.Conv2D(filters = 20, kernel_size = (3,3), activation = \"relu\", input_shape = (32, 32, 3)))\nmy_model_without_Dropout.add(layers.MaxPooling2D(pool_size = (2,2)))\n#my_model_without_Dropout.add(layers.Dropout(0.3))\nmy_model_without_Dropout.add(layers.Conv2D(filters = 40, kernel_size = (3,3), activation = \"relu\"))\nmy_model_without_Dropout.add(layers.MaxPooling2D(pool_size = (2,2)))\n#my_model_without_Dropout.add(layers.Dropout(0.3))\nmy_model_without_Dropout.add(layers.Conv2D(filters = 80, kernel_size = (3,3), activation = \"relu\"))\n\nmy_model_without_Dropout.add(layers.Flatten())\n#my_model_without_Dropout.add(layers.Dropout(0.3))\nmy_model_without_Dropout.add(layers.Dense(units = 64, activation = \"relu\"))\nmy_model_without_Dropout.add(layers.Dense(units = number_of_classes, activation = \"softmax\"))\n\n# train model\noptimizer_use = optimizers.RMSprop(learning_rate = 0.001, rho = 0.9) # default values\nmy_model_without_Dropout.compile(optimizer = optimizer_use, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])\ncallbacks_list = [EarlyStopping(monitor = 'categorical_accuracy', patience = 5, restore_best_weights = True)]\nhistory = my_model_without_Dropout.fit(train_data, train_labels, epochs = 20, batch_size = 32, validation_data = (valid_data, valid_labels), callbacks = callbacks_list) # large batch_size\n\nacc_values = history.history['categorical_accuracy']\nval_acc_values = history.history['val_categorical_accuracy']\nepochs = range(1, len(acc_values) + 1)\nplt.plot(epochs, acc_values, 'b', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values, 'r', label = 'Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","57525fc3":"Build the models. \n\nWe will use 3 convolutional layers with MaxPooling and Dropout layers in between. Dropout is used for a regularization. \nThe convolutional layers are followed by the dense classifier. The final layer will make an output of the 43 values after the \"softmax\" activation. The i-th value represents a probability of the picture to contain the i-th road sign from the \"signnames\" file.\n\nWe will prepare the 2 different models: with the BatchNormalization layers in between and without. The batch normalization layers standardize inputs of the layers using mean and variance of the input variables.  ","3244d2a9":"We have achieved the 97% of the validation accuracy. Let\u2019s make a plot. ","d03106d6":"Now reset rho to 0.9 and have a look on the batch_size parameter. With this parameter being increased the larger batches of data will be used simultaneously to calculate the gradients and determine a direction of the loss function descent. And with the large batch_size value we may face a gradient compete situation: a sum of many significant noncollinear directions (vectors) may lead to an insignificant overall progress per step. From this point of view it may be beneficial to use more steps with the directions being calculated using smaller amounts of elements. ","ae0b81e7":"To finish with the Adam optimizer, let's run it with the smaller batch_size.","4a843c1f":"As we can see, accumulation of the stochastic gradients of the small random batches led us to the oscillating unstable model in this case. So the increased momentum parameter may enhance the model performance in some cases, or decrease it under the other circumstances.\n\nHowever, we have already seen a way to suppress the fluctuations: apply batch normalization. Let's try the normalized model for this case (large momentum and small batch size):","ef879d04":"That is much better! Although there are some oscillations, there overall performance of the model is not bad and significantly better than of the non-normalized variant.\n\nNow we try another optimizer (namely, Adam) for completeness. We will use the default parameters beta_1 = 0.9 and beta_2 = 0.999. In some sense they corespond to the momentum and rho of the RMSprop optimizer respectively (with some slightly modified formulas). Anyways, beta_1 is responsible for the gradients accumulations from the next steps to calculate the new direction and beta_2 takes into account the previous gradients in order to fullfil a kind of the gradients normalization to increase the convergence rate (Adagrad idea to improve performance in the case of the elongated shape of the loss function). \n\nLet's try Adam with a large value of batch_size parameter (of course we can expect that with the batch_size = 32 things will be much better, but here we just try to get an understanding of how these parameters work).","57c1d050":"Get some information about data","03b48205":"Train the model without batch normalization. As we want to play with the model parameters we will specify them explicitly even if their values correspond to the default ones.","d41c0c68":"Read information about classes of the road signs.","b8881d1b":"What can be done further? One could try to use the features extraction from the previously pre-trained neural network models, like VGG or Inceptionv3. That we will do later in the next codes. \n\nAnother idea could be to get some overfitted model and use data augmentation with the ImageDataGenerator. \n\nHowever here we want to concentrate on the investigation of some of the model parameters and see what happens if we choose them not efficiently. For example, let's have a look on the following ones: \n1. rho of the RMSprop optimizer: discounting factor for the history\/coming gradient. Defaults to 0.9. The larger values of rho: take the previous gradients into account with a larger priority in order to normalize gradients (Adagrad idea to improve performance in the case of the elongated shape of the loss function).\n2. momentum of the RMSprop optimizer: the larger values of momentum: take the previously accumulated gradients into account with a larger priority in order to determine the new direction of descent.\n3. batch_size: a size of the batch to calculate gradients. \n4. What if we do not use the Dropout layers.","e79cee66":"It looks like that the model will reach better performance with the further iterations. Let's make 20 epochs more (we do not restore the initial weights in this case, but continue with the previously trained model instead). ","e3897ed0":"Here we want to train a convolutional neural network for the road signs classification. We will use a rather naive approach: we will not use the feature extraction from the pre-trained models, but build our own model instead. The main idea here is to play around with some parameters in order to get feeling of their influence on the model and how they work.  ","ecb9c15b":"First of all let's start with the rho. If we decrease this parameter, than the new gradients (directions of the loss function descents) are taken into account with much larger priority than the previous directions. Since we work on relatively small random batches of data, here we can expect some accuracy oscillations. ","2a8675f1":"So, we reached the 95% of the validation accuracy. Of course, the convergence with the smaller batch size was much faster. However, that is a good example how momentum can increase performance of a model. \n\nOn the contrary, a momentum increase for the model with a small batch_size parameter may make things worse. Since the batches are small and stochastic, it doesn't make sense to accumulate them since every such batch represents only a small part of the whole big picture.  ","ba84d4fd":"Now let's train the normalized version:","fb8ad6ad":"Features represent the pictures of a size 32 x 32 with 3 channels. \n\nNow prepare data for the training.","b81b567a":"To accelerate convergence we could increase the momentum parameter. It takes into account the previous directions (gradients) accumulated in the overall momentum. Let's try momentum = 0.9 what means use 90% of the momentum and 10% of the new gradient for each step. That should not provide no benefits on the early steps (because there is no momentum still accumulated), but may work better on the later epochs.","8736d236":"Read data","d0afb022":"It looks not bad. Let\u2019s make an evaluation.","2d4474cf":"That looks good: 97% of accuracy and fast convergence. ","4300d235":"In this case the normalized version performs much better: the overall accuracy is higher and oscillations are lower. The batch normalization layers helped to avoid the unstable behaviour of the model caused by \"short memory\" of the previous gradients. A comparison of the validation accuracy graph is shown on the next plot.","c93596df":"Nice, it works: we reached the 87% of the validation accuracy instead of 80% for the learning_rate = 0.001 after 20 epochs. The variant with the larger learning rate is advantageous on all the epochs.","8396e64c":"Show one of the pictures. Just to see how it looks like.","25ca2fe6":"That looks much better: we achieved 90% of the validation accuracy in comparison with 58% without momentum after 20 epochs. The next plot compares accuracies of the \"with momentum\" and \"without momentum\" variants. As we predicted, the increased momentum provides significant benefits after some epochs.","d4967495":"As it was predicted, a significant overfitting can be observed in this case: the validation accuracy is lower than the training one.","c764514c":"As we expected, we observe significant validation accuracy fluctuations around the training accuracy. The overall accuracy is lower than in case of rho = 0.9\n\nHow will the normalized version behave in this case? We know that the Batch Normalization layers help to standardize inputs of the layers. Thus we may hope that some fluctuations will be removed.","ba9c7b09":"Here we also observe a slow convergence rate because of the large batch size: we achieved only the 80% of the validation accuracy. If the gradient compete is the case, we could try to increase the learning_rate parameter (the step size for the optimization algorithm) hoping that this could accelerate convergence. Let's increase the learning rate in 2 times. A more dramatic step size increase will lead to a lower convergence rate or even cause fluctuations without convergence in this case.","99c421e4":"Ok. We have an evaluation accuracy of 95% - 96% for the both versions. However the next plot shows faster convergence for the normalized variant.","f0d96f06":"As it was predicted, here we have a much slower convergence: we have reached only the 58% of the validation accuracy after 20 epochs. Of course, mathematically the procedure should converge to the optimum, but it may take unreasonable number of epochs on practice.\n\nThe normalization procedure will not help in this case: we have a problem of the gradients compete, but the variance of the inputs is not large. Indeed, the batch normalization may make things even worse by suppressing already small gradients. The next run will demonstrate this.","c2f819df":"Finally we have a look on the importance of the Dropout layers. We train a model without drops and see what happens. We can expect an overfitting of the model. For the optimizer we switch to the RMSprop again."}}