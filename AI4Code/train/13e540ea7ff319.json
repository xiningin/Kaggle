{"cell_type":{"bdb064b0":"code","0e033307":"code","97eb0959":"code","6ccd48ce":"code","5a47cdd4":"code","0d7455f3":"code","81217ec4":"code","9a0e276a":"code","7bc02811":"code","b47baab3":"code","a75c15a5":"code","3406237b":"code","82086a4e":"code","8e5ae5ef":"code","2afbf0f6":"code","feb49bc2":"code","53558185":"code","f38ad685":"code","790d0713":"code","e5dda3ef":"code","08c4f688":"code","ba8cfdf5":"code","fe18ebd0":"code","88704469":"code","864fcc7f":"code","f1b93120":"markdown","756f5a22":"markdown","9a719e56":"markdown","c053622d":"markdown","c5ccb2c9":"markdown","f72811fb":"markdown","2e1c34d1":"markdown","93687987":"markdown","805a90be":"markdown","5181b6f9":"markdown","c289af37":"markdown","e446be3f":"markdown","69be4cc9":"markdown","a15f6c04":"markdown","396cc7b2":"markdown","4c4a6841":"markdown","34eee012":"markdown","9324f26a":"markdown","2773c7e5":"markdown","3ebf62aa":"markdown","46d3ef4c":"markdown","90738612":"markdown","02b158b1":"markdown","7903900e":"markdown","02ac9697":"markdown","9f140da3":"markdown","dd4598b9":"markdown"},"source":{"bdb064b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\nimport warnings\nwarnings.filterwarnings('ignore')","0e033307":"# Import libraries for data wrangling, preprocessing and visualization\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n# Importing libraries for building the neural network\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold, train_test_split\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nseed = 8\nnp.random.seed(seed)","97eb0959":"# Input data files are available in the \"..\/input\/\" directory.\n# Read data file\ndata = pd.read_csv(\"..\/input\/heart.csv\", header=0)\n# Take a look at the data\ndata.head(10)","6ccd48ce":"# Take a end of file\ndata.tail(10)","5a47cdd4":"print('Number of rows in the dataset: ',data.shape[0])\nprint('Number of columns in the dataset: ',data.shape[1])","0d7455f3":"data.info()","81217ec4":"data.describe()","9a0e276a":"# Any empty values?\ndata.isnull().sum()","7bc02811":"# draw histogram of the features \nhist = data.hist(bins=10, figsize=(16,10))","b47baab3":"plt.figure(figsize=(8,4))\nsns.distplot(data['age'],kde=False,bins=10)\nprint (\"Age max:\", data['age'].max(), \" min:\", data['age'].min())","a75c15a5":"plt.figure(figsize=(12,8))\n# Chest Pain\nplt.subplot(221)\nplt.title(\"Chest Pain types\")\nlabels = 'Chest Pain Type:0','Chest Pain Type:1','Chest Pain Type:2','Chest Pain Type:3'\nsizes = [len(data[data['cp'] == 0]),len(data[data['cp'] == 1]),\n         len(data[data['cp'] == 2]),len(data[data['cp'] == 3])]\nplt.pie(sizes, explode=(0, 0,0,0), labels=labels,autopct='%1.1f%%', shadow=True, startangle=180)\n# blood sugar\nplt.subplot(222)    \nplt.title(\"Blood sugar\")\nlabels = 'fasting blood sugar < 120 mg\/dl','fasting blood sugar > 120 mg\/dl'\nsizes = [len(data[data['fbs'] == 0]),len(data[data['cp'] == 1])]\nplt.pie(sizes, explode=(0.1, 0), labels=labels, autopct='%1.1f%%', shadow=True, startangle=180)","3406237b":"# draw a heatmap\nsns.set_style('whitegrid')\nplt.figure(figsize=(15,8))\nsns.heatmap(data.corr(), annot = True, linewidths=.2)\nplt.show()","82086a4e":"rcParams['figure.figsize'] = 5,3\nplt.bar(data['target'].unique(), data['target'].value_counts())\nplt.xticks([0, 1])\nplt.xlabel('Target')\nplt.ylabel('Count')\nplt.title('Count of each Target Class')","8e5ae5ef":"plt.figure(figsize=(15,6))\nsns.countplot(x='age',data = data, hue = 'target')\nplt.show()","2afbf0f6":"# Select the columns to use for prediction in the neural network\nX= data.drop('target',axis=1)\nY=data['target']\nprint (X.shape, Y.shape, data.columns)","feb49bc2":"# split data into train, test\nX_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=39, shuffle=True)\n#kipping y since value already 1 or 0\n# encoder = LabelEncoder()\n# encoder.fit(Y)\n# encoded_Y = encoder.transform(Y)\n\n# normalize data\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(X_train_scaled)\nX_test_scaled = scaler.fit_transform(X_test)\nX_test = pd.DataFrame(X_test_scaled)\n\nprint (X_train.shape, y_train.shape)\nprint (X_train.shape, y_test.shape)\nprint (data.columns)","53558185":"#let's build a xgboot classifier to find out feature importance\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nbest_xgc_score=0\nmodel = XGBClassifier(max_depth=7)\nmodel.fit(X_train,y_train,eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric='logloss', verbose=False)\npredictions = model.predict(X_test)\nbest_xgc_score = accuracy_score(y_test,predictions)\nprint (\"XGBClassifier accuracy: \", best_xgc_score)","f38ad685":"print (X.columns) \nprint (model.feature_importances_*100)\nprint (model.classes_)  # output\n# visualize it\nplt.figure(figsize=(16,5))\nrf_scores=model.feature_importances_*100\nplt.bar([i for i in range(len(X.columns))], rf_scores, width = 0.8)\nfor i in range(len(X.columns)):\n    plt.text(i, rf_scores[i], rf_scores[i])\nplt.xlabel('Feature')\nplt.ylabel('Scores')\nplt.title('Feature importances')","790d0713":"# Define some useful callbacks\n#Reduce learning rate when a metric has stopped improving.\nreducelrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n# Stop training when a monitored quantity has stopped improving. \n# By default, mode is set to \u2018auto\u2018 and knows that you want to minimize loss or maximize accuracy.\nearly_stopping_monitor=EarlyStopping(monitor='val_loss',verbose=1, patience=30, baseline=0.4, )\n# Save the model after every epoch.\nbest_trained_model_file= 'best_trained_model.h5'\ncheckpoint = ModelCheckpoint(best_trained_model_file, verbose=0, monitor='val_loss',save_best_only=True, mode='auto')  \n#place callbacks want to enable on this list\ncallbacks=[checkpoint, reducelrp]","e5dda3ef":"# create model with fully connected layers with dropout regulation\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=13, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(6, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=\"Adamax\", metrics=['accuracy'])\nmodel.summary()","08c4f688":"%%time\n# fit the model\nprint (\"trainning model....  please wait!\")\nhistory=model.fit(X_train, y_train, validation_split=0.33, epochs=100, batch_size=6, callbacks=callbacks,verbose=0)\nplt.plot(history.history['acc'])\nplt.show()\nprint (\"model training - finished\")","ba8cfdf5":"print(\"Evaluate model against trained data\")\nscore = model.evaluate(X_train, y_train, verbose=0)\nprint(\"score %s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n\nprint(\"Evaluate model against new data\")\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint(\"score %s: %.2f%%\" % (model.metrics_names[1], score[1]*100))","fe18ebd0":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(\"Model prediction test\")\n# prediction return class type (1 or 0)\ny_pred_class = model.predict_classes(X_test)\n# prediction return proability percentage\ny_pred_prob = model.predict(X_test)\n\nprint (\"#  original | predicted  | probability  \")\nfor idx, label in enumerate(y_test):\n    print (\"%s     | %s  | %s |   %.2f%%\" % (str(idx), str(label), str(y_pred_class[idx]), float(y_pred_prob[idx])*100))\n\n# manually calculate accuracy rate\nprint(\"\")\ncount = len([\"ok\" for idx, label in enumerate(y_test) if label == y_pred_class[idx]])\nprint (\"Manually calculated accuracy is: %.2f%%\" % ((float(count) \/ len(y_test))*100))\n# using accuracy_score()\nprint (\"Keras accuracy_score() is: %.2f%%\" %  (accuracy_score(y_test, y_pred_class)*100))\nprint(\"\")\nprint (\"Simple confusion matrix \")\ncm = confusion_matrix(y_test,y_pred_class)\nprint (cm)","88704469":"%%time\n# define 10-fold cross validation test harness\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncvscores = []\nprint (\"running model performance validation... please wait!\")\nfor train, test in kfold.split(X, Y):\n    # create model\n    model = Sequential()\n    model.add(Dense(12, input_dim=13, kernel_initializer='uniform', activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(6, kernel_initializer='uniform', activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=\"Adamax\", metrics=['accuracy'])\n    # Fit the model\n    history=model.fit(X_train, y_train, epochs=100, batch_size=6, verbose=0)    \n    # evaluate the model\n    scores = model.evaluate(X_test, y_test, verbose=0)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1] * 100)\n\nprint (\"done.\")\nprint (\"summary report on mean and std.\")\n# The average and standard deviation of the model performance \nprint(\"%.2f%% (+\/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))","864fcc7f":"# save trained model\n#trained_model_file=\"trained_heart_model.h5\"\n#model.save_weights(trained_model_file)\n#print(\"Saved trained model to disk as h5 file :\", trained_model_file)","f1b93120":"Observation: digram with discrete bars indicate that is actually a categorical variable, they are target, slope, tahi, sex, fbs, exang, cp, ca.  \nCheck if we need to handle category data ","756f5a22":"* model performace summary report on mean and std [  85.82% (+\/- 0.33%) }","9a719e56":"**Are any correlation between features?**\nUnderstanding correction among features which will be useful for model feature selection","c053622d":"> Observation: result suggest not tight correction between features ","c5ccb2c9":"**What age has high risk of heat disase?**","f72811fb":"**What is the dataset size, shape and colums?**","2e1c34d1":"![](http:\/\/)**Run prediction test**","93687987":"**Build Model**","805a90be":"**Evaluate model**","5181b6f9":"Onservation: pretty even dataset for output label","c289af37":"Observation: above result show  chols column std is very high at 51%., follow by trestbps with 17.5%.  therefore, we need do some scaling \/ normalization of the data before training the model.","e446be3f":"**Model performance validation**\nHow good does the model perform against unseen data?","69be4cc9":"## Classification of Patient Heart Disease with Keras - MLP\nUsing neural network model to learn a mapping between patient profile attributes as input features and if has heart disease as an output feature that is a label.","a15f6c04":"> **Load training data **","396cc7b2":"The dataset contains the following features:\n1. age(in years)\n2. sex: (1 = male; 0 = female)\n3. cp: chest pain type\n4. trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n5. chol: serum cholestoral in mg\/dl\n6. fbs: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n7. restecg: resting electrocardiographic results\n8. thalach: maximum heart rate achieved\n9. exang: exercise induced angina (1 = yes; 0 = no)\n10. oldpeak: ST depression induced by exercise relative to rest\n11. slope: the slope of the peak exercise ST segment\n12. ca: number of major vessels (0-3) colored by flourosopy\n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n14. target: 1 or 0 ( with 1 = Yes has disease, and 0= No disease ","4c4a6841":"Onservation: pretty even dataset for output label","34eee012":"1. Observation: no empty values found which is good.","9324f26a":"![](http:\/\/)[](http:\/\/)**Basic Data Analysis of features and label**\nhere's let's do some basic data analysis to get more understanding of the data.","2773c7e5":"**How many people has chest pain and high blood sugar?**","3ebf62aa":"** Train model**","46d3ef4c":"> **What are features data looks like?**","90738612":"**Define network**","02b158b1":"**Prepare data for training**","7903900e":"**Which features are the most important in relation to heart disease?**  let's find out by using XGBoost classifier","02ac9697":"Observation:people at 41-15, 51-59 amount highest one with heart disease.","9f140da3":"**What is the age distribution on this dataset?**","dd4598b9":"**Target Label**"}}