{"cell_type":{"2108362c":"code","2449771c":"code","349901ec":"code","55731eb3":"code","4c5c83d8":"code","8195a304":"code","11a7f3e5":"code","5b4e2735":"code","8703ff36":"code","7c0f6d0a":"code","8b59b723":"code","12285a06":"code","21283f46":"code","5429f89b":"code","924ce669":"code","81a313b9":"code","d535fd4b":"code","472b5b87":"code","27d030be":"code","5be1f804":"code","8353e8ed":"code","b517573e":"code","f1070110":"code","f35f9865":"code","0bb985e5":"code","8acc6142":"code","7b159a5b":"code","16d7c3d4":"code","02f1d6de":"code","c707feac":"code","b211db39":"code","6f3dd955":"code","3523205e":"code","07391568":"code","4dfba0a6":"code","dd4b2683":"code","efbb2135":"code","c3976409":"code","52bac62b":"code","8af34818":"code","50a36709":"code","4e7982f8":"code","795d9eb6":"code","06cd12ff":"code","4c7a5f80":"code","72992aab":"markdown","eaf2a302":"markdown","2d952c17":"markdown","32b38aa1":"markdown","aca32500":"markdown","7c5f2a97":"markdown","2920164e":"markdown","9f542657":"markdown","360761c5":"markdown","a0525b57":"markdown","e5365dd2":"markdown","bef58d9d":"markdown","b789241a":"markdown","4b27882a":"markdown","d74abd54":"markdown","a52a2ee5":"markdown","8ef47c35":"markdown","7bb1eda9":"markdown","b16700f4":"markdown","ad0a28c1":"markdown","5145365a":"markdown","557f45b5":"markdown","c0c5836a":"markdown","1523c982":"markdown","44781d41":"markdown","abe067b6":"markdown","5a85115d":"markdown","ecee5715":"markdown","3d8fc161":"markdown","6b85924a":"markdown","c8128bf8":"markdown","8d6d73f3":"markdown","d6115db3":"markdown"},"source":{"2108362c":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Convolution1D, Flatten, Dropout, \\\n                                    LSTM, GRU, Input, RepeatVector, SimpleRNN\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport os\n\nimport sklearn.metrics\nfrom sklearn.preprocessing import MinMaxScaler\nimport sklearn.model_selection\nfrom sklearn.linear_model import LinearRegression\nnp.random.seed(123)","2449771c":"data = pd.read_csv('\/kaggle\/input\/uop-aml-hw3\/admData20.csv')","349901ec":"data","55731eb3":"plt.plot(data.AdmittedNum)\nplt.plot()","4c5c83d8":"grouped = data.groupby(by='ExpStartDate')\n\nX =  []\nX_aux = []\ny = []\nfor g in grouped.groups:\n    X.append(grouped.get_group(g).AppliedNum.values)\n    X_aux.append(grouped.get_group(g).Budget.iloc[0])\n    y.append(grouped.get_group(g).AdmittedNum)","8195a304":"X = pd.DataFrame([[i for i in j] for j in X])\ny = pd.DataFrame([[i for i in j] for j in y])\nX = np.array(X)\ny = np.array(y)\nX_aux = np.array(X_aux)\n\nX = X \/ X_aux[:, None]\ny = y \/ X_aux[:, None]\n\nX_train, X_test, X_aux_train, X_aux_test, y_train, y_test = sklearn.model_selection.train_test_split(X, X_aux, y)","11a7f3e5":"def train_gen(X, y, foresight=5):\n    train = []\n    target = []\n    for i in range(len(X)):\n        X_seq = X[i, ~np.isnan(X[i,:])]\n        y_seq = y[i, ~np.isnan(y[i,:])]\n        \n        X_seq_train = X_seq[:-foresight]\n        y_seq_train = y_seq[:-foresight]\n        y_seq_test = y_seq[-foresight:]\n    \n        train.append(np.array([X_seq_train, y_seq_train]))\n        target.append(y_seq_test)\n    return train, target","5b4e2735":"def train_enc_dec(model, train_gen=train_gen, X_train=X_train, y_train=y_train, epochs=50):\n    epochs = epochs\n    history = []\n#     val_history = []\n    X_train_seq, y_train_seq = train_gen(X_train, y_train)\n    \n#     X_t, X_val, y_t, y_val = sklearn.model_selection.train_test_split(X_train_seq, y_train_seq, test_size=0.1)\n    \n    for e in range(epochs):\n        for i in range(len(X_train_seq)):\n            hist = model.fit(np.array([X_train_seq[i].T]), [[y_train_seq[i]]], \n                             epochs=1, \n                             batch_size=1, \n                             validation_split=0,\n                             verbose=0)\n            history.append(hist)\n        \n#         val_temp = []\n#         for i in range(len(X_val)):\n#             val_temp.append(model.evaluate(np.array([X_val[i].T]), [[y_val[i]]], \n#                                            verbose=0))\n    return history","8703ff36":"def print_report(y_true, y_pred):\n    print(f'MSE: {np.mean((np.array(y_true) - y_pred) ** 2) \/ len(y_true[0])}')\n    print(f'MAE: {np.mean(np.abs(np.array(y_true) - y_pred)) \/ len(y_true[0])}')","7c0f6d0a":"# https:\/\/stackoverflow.com\/questions\/43117654\/many-to-many-sequence-prediction-with-different-sequence-length\n\ndef create_LSTM_model():\n\n    seq_input = Input(shape=(None, 2), name='seq_input') # unknown timespan, fixed feature size of 2\n    x = LSTM(4, return_sequences=True, activation='relu')(seq_input)\n    x = LSTM(4, return_sequences=True, activation='relu')(x)\n    x = LSTM(1, return_sequences=False, activation='relu')(x)\n    # aux_input = Input(shape=(1), name='aux_input')\n\n    # x = tf.keras.layers.concatenate([aux_input, seq_x])\n    # x = Dense(1, activation='relu')(x)\n    x = RepeatVector(5)(x)\n\n    out = LSTM(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n\n    model.compile(loss='mean_squared_error',\n                  optimizer='adam',\n                  metrics=['mae'])\n    return model","8b59b723":"model = create_LSTM_model()\nmodel.summary()","12285a06":"history = train_enc_dec(model)","21283f46":"X_test_seq, y_test_seq = train_gen(X_test, y_test)\ny_pred = np.array([model.predict(np.array([X_test_seq[i].T])) for i in range(len(X_test))])\ny_pred = np.squeeze(y_pred)","5429f89b":"print_report(y_test_seq, y_pred)","924ce669":"y_pred = y_pred * X_aux_test[:, None]\ny_test_seq = y_test_seq * X_aux_test[:, None]\nprint('For unscaled:')\nprint_report(y_test_seq, y_pred)","81a313b9":"y_test_unscaled = [ts * aux for ts, aux in zip([y_t[1] for y_t in X_test_seq], X_aux_test)]\nfor i in range(len(y_test_seq)):\n    plt.plot(np.append(y_test_unscaled[i], y_test_seq[i]), label='True')\n    plt.plot(np.append(y_test_unscaled[i], y_pred[i]), label='Prediction')\n    plt.legend()\n    plt.show()","d535fd4b":"def create_GRU_model():\n\n    seq_input = Input(shape=(None, 2), name='seq_input') # unknown timespan, fixed feature size of 2\n    x = GRU(4, return_sequences=True, activation='relu')(seq_input)\n    x = GRU(4, return_sequences=True, activation='relu')(x)\n    x = GRU(1, return_sequences=False, activation='relu')(x)\n    # aux_input = Input(shape=(1), name='aux_input')\n\n    # x = tf.keras.layers.concatenate([aux_input, seq_x])\n    # x = Dense(1, activation='relu')(x)\n    x = RepeatVector(5)(x)\n\n    out = GRU(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n\n    model.compile(loss='mean_squared_error',\n                  optimizer='adam',\n                  metrics=['mae'])\n    return model","472b5b87":"model = create_GRU_model()\nmodel.summary()","27d030be":"history = train_enc_dec(model)","5be1f804":"X_test_seq, y_test_seq = train_gen(X_test, y_test)\ny_pred = np.array([model.predict(np.array([X_test_seq[i].T])) for i in range(len(X_test))])\ny_pred = np.squeeze(y_pred)","8353e8ed":"print_report(y_test_seq, y_pred)","b517573e":"y_pred = y_pred * X_aux_test[:, None]\ny_test_seq = y_test_seq * X_aux_test[:, None]\nprint('For unscaled:')\nprint_report(y_test_seq, y_pred)","f1070110":"y_test_unscaled = [ts * aux for ts, aux in zip([y_t[1] for y_t in X_test_seq], X_aux_test)]\nfor i in range(len(y_test_seq)):\n    plt.plot(np.append(y_test_unscaled[i], y_test_seq[i]), label='True')\n    plt.plot(np.append(y_test_unscaled[i], y_pred[i]), label='Prediction')\n    plt.legend()\n    plt.show()","f35f9865":"def plot_preds(model):\n\n    plt.figure(figsize=(20, 8))\n    plt.subplot(1, 3, 1)\n    axes = plt.gca()\n    axes.set_ylim([0, 2])\n\n    for j, y in enumerate(y_train):\n        row = [None for i in range(j)]\n        row += list(y)\n        plt.plot(row, 'b', label='true')\n\n    plt.subplot(1, 3, 2)\n    axes = plt.gca()\n    axes.set_ylim([0, 2])\n    y_pred = model.predict(X_val)\n\n    plt.plot(X_val[0,:5,0], 'b')\n    \n    for j, prediction in enumerate(y_pred):\n        row = [None for i in range(j + 5)]\n        row += list(prediction)\n        plt.plot(row, 'r')\n\n\n    for j, y in enumerate(y_val):\n        row = [None for i in range(j + 5)]\n        row += list(y)\n        plt.plot(row, 'b')\n\n    plt.legend()\n\n    plt.subplot(1, 3, 3)\n    axes = plt.gca()\n    axes.set_ylim([0, 2])\n    y_pred = model.predict(X_test)\n\n    plt.plot(X_test[0,:5,0], 'b')\n    \n    for j, prediction in enumerate(y_pred):\n        row = [None for i in range(j + 5)]\n        row += list(prediction)\n        plt.plot(row, 'r')\n\n\n    for j, y in enumerate(y_test):\n        row = [None for i in range(j + 5)]\n        row += list(y)\n        plt.plot(row, 'b')\n\n    plt.legend()\n    plt.plot()","0bb985e5":"OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001)\nEPOCHS = 50\nBATCH_SIZE = 64\nLOOK_BACK = 20\nes2 = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=400, restore_best_weights=True)","8acc6142":"def train_and_report(creation_func):\n    model = creation_func()\n    print(model.summary())\n    \n    history = model.fit(X_train, y_train,\n                        batch_size=128, \n                        epochs=700, \n                        validation_split=0.0,\n                        use_multiprocessing=True,\n                        verbose=0,\n                        validation_data=(X_val, y_val), \n                        callbacks=[es2]\n                       )\n    \n    plt.title('Loss')\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    \n    try:\n        y_pred_val = model.predict(X_val)[:,:,0]\n        y_pred_test = model.predict(X_test)[:,:,0]\n    except:\n        y_pred_val = model.predict(X_val)\n        y_pred_test = model.predict(X_test)\n        \n    print('For validation set:')\n    print_report(y_val, y_pred_val)\n\n    print('For test set:')\n    print_report(y_test, y_pred_test)\n    \n    plot_preds(model)","7b159a5b":"def inverse_transform(y, scaler, column):\n    return y * (scaler.data_max_[column] - scaler.data_min_[column]) + scaler.data_min_[column]","16d7c3d4":"def create_seq(dataset, look_back=LOOK_BACK, foresight=5):\n    X, y = [], []\n    for i in range(len(dataset) - look_back - foresight):\n        X.append(np.array(dataset[i:(i + look_back)].loc[:, ['AdmittedNum', 'AppliedNum', 'WeeksBeforeStart']]) \n                 \/ np.array(dataset.Budget[i:(i + look_back)])[:, None])\n        y.append(dataset.iloc[(i + look_back) : (i + look_back + foresight), 0] \n                 \/ np.array(dataset.Budget[(i + look_back) : (i + look_back + foresight)]))\n    return np.array(X), np.array(y)","02f1d6de":"data2 = data.loc[:, ['AdmittedNum', 'AppliedNum', 'WeeksBeforeStart', 'Budget']]\n\ntrainTr = round(0.6 * len(data))\nvalTr = round((0.8 * len(data)))\ndata_train, data_val, data_test = data2[:trainTr], data2[trainTr:valTr], data2[valTr:]\n\nX_train, y_train = create_seq(data_train)\nX_val, y_val = create_seq(data_val)\nX_test, y_test = create_seq(data_test)\n\nX_train = np.reshape(np.array(X_train), (X_train.shape[0], X_train.shape[1], 3))\nX_val = np.reshape(np.array(X_val), (X_val.shape[0], X_val.shape[1], 3))\nX_test = np.reshape(np.array(X_test), (X_test.shape[0], X_test.shape[1], 3))","c707feac":"def create_GRU():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = GRU(4, return_sequences=False, activation='relu', dropout=0.3)(seq_input)\n#     x = GRU(16, return_sequences=True, activation='relu')(x)\n#     x = GRU(4, return_sequences=False, activation='relu')(x)\n\n    # aux_input = Input(shape=(1), name='aux_input')\n    # x = tf.keras.layers.concatenate([aux_input, seq_x])\n    # x = Dense(1, activation='relu')(x)\n    \n    x = RepeatVector(5)(x)\n\n#     x = GRU(2, return_sequences=True, activation='relu')(x)\n#     x = GRU(2, return_sequences=True, activation='relu')(x)\n    out = GRU(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","b211db39":"train_and_report(create_GRU)","6f3dd955":"def create_LSTM():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = LSTM(4, return_sequences=False, activation='relu', dropout=0.3)(seq_input)\n#     x = LSTM(16, return_sequences=True, activation='relu')(x)\n#     x = LSTM(4, return_sequences=False, activation='relu')(x)\n\n    # aux_input = Input(shape=(1), name='aux_input')\n    # x = tf.keras.layers.concatenate([aux_input, seq_x])\n    # x = Dense(1, activation='relu')(x)\n    \n    x = RepeatVector(5)(x)\n\n#     x = LSTM(2, return_sequences=True, activation='relu')(x)\n#     x = LSTM(2, return_sequences=True, activation='relu')(x)\n    out = LSTM(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","3523205e":"train_and_report(create_LSTM)","07391568":"def create_CNN():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Convolution1D(filters=2, kernel_size=5, input_shape=(2, LOOK_BACK), activation='relu')(seq_input)\n#     x = Dropout(0.3)(x)\n#     x = Convolution1D(filters=4, kernel_size=3, activation='relu')(x)\n#     x = Dropout(0.5)(x)\n    x = Flatten()(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(4, activation='relu')(x)\n#     x = Dropout(0.3)(x)\n    \n    out = Dense(5)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","4dfba0a6":"train_and_report(create_CNN)","dd4b2683":"def create_CNN_GRU():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Convolution1D(filters=2, kernel_size=5, input_shape=(2, LOOK_BACK), activation='relu')(seq_input)\n    x = Convolution1D(filters=4, kernel_size=3, activation='relu')(x)\n    x = Flatten()(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(4, activation='relu')(x)\n    x = Dropout(0.1)(x)\n\n    x = RepeatVector(5)(x)\n\n    out = GRU(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","efbb2135":"train_and_report(create_CNN_GRU)","c3976409":"def create_CNN_LSTM():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Convolution1D(filters=2, kernel_size=5, input_shape=(2, LOOK_BACK), activation='relu')(seq_input)\n    x = Convolution1D(filters=4, kernel_size=3, activation='relu')(x)\n    x = Flatten()(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(4, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    \n    x = RepeatVector(5)(x)\n\n    out = LSTM(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","52bac62b":"train_and_report(create_CNN_LSTM)","8af34818":"def create_DNN():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Flatten()(seq_input)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(4, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    out = Dense(5, activation='relu')(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","50a36709":"train_and_report(create_DNN)","4e7982f8":"def create_DNN_with_GRU_out():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Flatten()(seq_input)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(4, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = RepeatVector(5)(x)\n\n    out = GRU(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","795d9eb6":"train_and_report(create_DNN_with_GRU_out)","06cd12ff":"def create_DNN_with_LSTM_out():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Flatten()(seq_input)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(4, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = RepeatVector(5)(x)\n\n    out = LSTM(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","4c7a5f80":"train_and_report(create_DNN_with_LSTM_out)","72992aab":"----------------------\n----------------------\n----------------------","eaf2a302":"---------","2d952c17":"Custom train cycle.  \n**NOTE:** validation is not included here due to sample size - after grouping training is 6 samples, splitting it into additional set looks inappropriate.","32b38aa1":"### DNN with GRU output","aca32500":"**Note:** Model selection cycle was not included due to its size. Each model presented is best found one in its subclass.","7c5f2a97":"---------\n---------","2920164e":"-----------\n-----------","9f542657":"Reporting MSE and MAE.","360761c5":"--------","a0525b57":"## GRU-based model","e5365dd2":"# <center>RNNs, Encoder-Decoder, 1d CNN<\/center>\n## <div align=right>Made by Ihor Markevych<\/div>","bef58d9d":"### 1d CNN","b789241a":"-----------","4b27882a":"## 1d CNN with LSTM output","d74abd54":"**NOTE:** in printed report, in predictions plots red lines correspond to predicted sequences, blue lines to true data.","a52a2ee5":"### Helper functions","8ef47c35":"-----------\n-----------\n-----------","7bb1eda9":"### DNN with LSTM output","b16700f4":"### Preprocessing","ad0a28c1":"## Conclusion","5145365a":"Overall, all models gave comparable performance.  \nModel that fully LSTM-based model gave best performance, however, as it was said, all models had comparable error.  \nThis fact can be explained by cyclic pattern of the data and small data sample.  \n  \nStacking recurrent layers was explored, it made models worse, which may be explained by sample size.  \n  \nAuxilary input was tried. However, it appeared that best way of incorporating it is to scale each value by Budget - in this case we assume that we now Budget for future (which is usually the case).\n\nEncoder-decoder model was also explored. This model takes an input of a sequence of variable length and predicts next five weeks (limited by single admission date). This is the most accurate model and the most logical from the use case - after the start of new admission date user should wait for two-three weeks. Then model can start making predictions and adjust them by taking into account newly available data each week.","557f45b5":"## Encoder-decoder (taking each admission date separately)","c0c5836a":"### LSTM-based model","1523c982":"Generate training datasets of sequences of variable length.","44781d41":"-----------\n-----------","abe067b6":"### GRU-based model","5a85115d":"-----------------\n-----------------","ecee5715":"### 1d CNN with GRU output","3d8fc161":"------------","6b85924a":"### Helper functions","c8128bf8":"## Taking whole timespan","8d6d73f3":"### Dense NN","d6115db3":"### LSTM-based model"}}