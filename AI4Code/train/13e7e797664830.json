{"cell_type":{"7ba0a955":"code","87bcd439":"code","96b473b1":"code","f6dbdd60":"code","55fd42a1":"code","118dd46b":"code","f79b7a00":"code","0d1f7e97":"code","73eb8c2b":"code","012040e0":"code","eba6e116":"code","50ffbec2":"code","1f0e4571":"code","22a2ba7c":"code","79dd1053":"code","ce95f857":"code","af17ac33":"code","8a8b37fc":"code","9ec537ff":"code","b4e6cde1":"code","ce4c5165":"code","be529b3b":"code","4c8a2920":"code","3ea9f989":"code","6a309c1c":"code","5fa102d2":"code","e70573c3":"code","c9fb50d8":"code","250d5070":"code","ef98c00e":"code","8b72505c":"code","c013c4d1":"code","1470073b":"code","1dd02024":"code","8f78788f":"code","45693193":"code","a8c8379a":"code","2f7763cc":"code","2b2a7b42":"code","4f0a59a1":"code","774f9651":"code","adc934fa":"code","56ba64d2":"code","d93db67c":"markdown","fc972a7b":"markdown","c7fa6955":"markdown","517a582a":"markdown","cbf817a6":"markdown","509f4387":"markdown","d20bc2f6":"markdown","496dab2c":"markdown","0e84e4a0":"markdown","91f56517":"markdown","abef7cff":"markdown","0f7e3166":"markdown","5f70dd09":"markdown","767f4114":"markdown"},"source":{"7ba0a955":"text = \"\"\"The US Senate has voted to confirm judge Brett Kavanaugh to the supreme court, handing Donald Trump a major victory and America a bench expected to tilt to the right for the next generation.\nThe president will hold a ceremony for Kavanaugh at the White House on Monday evening and he is expected to take his place on the court on Tuesday.\nAfter a bitter fight on Capitol Hill dominated by partisan entrenchment and the allegations of sexual assault against Kavanaugh, the 53-year-old federal judge was sworn in by supreme court chief justice John Roberts on Saturday evening just a few hours after Republicans won the confirmation vote 50 to 48.\nFurious protesters hammered on the huge front doors beneath the white columns of the majestic court building on Capitol Hill as Kavanaugh was being sworn in, following a day of demonstrations that saw many arrested but were more muted than days earlier, as it became clear the ultra-conservative\u2019s confirmation was all but inevitable.\"\"\"","87bcd439":"# We can split the text-chunk into something like sentences.\nsplit_text = text.split('.')\nprint(split_text)","96b473b1":"# print out the first stentence\nsentence_3 = split_text[2]\nprint(sentence_3)","f6dbdd60":"# Let's create tokens\ntokens_sentence_3 = [word for word in sentence_3.split(' ')]\nprint(tokens_sentence_3)","55fd42a1":"# Let's lowercase all these tokens and clean up the \\n (new line command)\n# Also we will replace \"()\" as well as make sure that only words lend in our list\ntokens_sentence_3_lower = [word.lower().strip() for word in sentence_3.split(' ')]\nprint('### OUTPUT1 ###')\nprint(tokens_sentence_3_lower)\nprint('\\n')\n    \ntokens_sentence_3_lower = [word.replace('(','').replace(')','') \n                           for word in tokens_sentence_3_lower if word.isalpha()]\n\nprint('### OUTPUT2 ###')\nprint(tokens_sentence_3_lower)\n","118dd46b":"# Removing stopwords\n\nstopwords_en = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n                'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n                \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n                'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n                'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n                'they', 'them', 'their', 'theirs', 'themselves', 'what', \n                'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', \n                'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n                'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', \n                'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n                'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', \n                'between', 'into', 'through', 'during', 'before', 'after', 'above', \n                'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n                'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n                'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n                'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n                'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', \n                'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \n                \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n                \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n                'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n                'won', \"won't\", 'wouldn', \"wouldn't\"]","f79b7a00":"tokens_sentence_3_clean = [word for word in tokens_sentence_3_lower if word not in stopwords_en]\nprint(tokens_sentence_3_clean)","0d1f7e97":"import nltk","73eb8c2b":"# Tokenizing sentences\nfrom nltk.tokenize import sent_tokenize\n\n# Tokenizing words\nfrom nltk.tokenize import word_tokenize\n\n# Tokenizing Tweets!\nfrom nltk.tokenize import TweetTokenizer","012040e0":"# Let's get our stences.\n# Note that the full-stops at the end of each sentence are still there\nsentences = sent_tokenize(text)\nprint(sentences)","eba6e116":"# Use word_tokenize to tokenize the third sentence: tokenized_sent\ntokenized_sent = word_tokenize(sentences[2])\n\n# Make a set of unique tokens in the entire scene: unique_tokens\nunique_tokens = set(word_tokenize(text))\nprint(unique_tokens)","50ffbec2":"tweets = [\"On behalf of @FLOTUS Melania & myself, THANK YOU for today's update & GREAT WORK! #SouthernBaptist @SendRelief,\u2026 https:\/\/t.co\/4yZCeXCt6n\",\n\"I will be going to Texas and Louisiana tomorrow with First Lady. Great progress being made! Spending weekend working at White House.\",\n\"Stock Market up 5 months in a row!\",\n\"'President Donald J. Trump Proclaims September 3, 2017, as a National Day of Prayer' #HurricaneHarvey #PrayForTexas\u2026 https:\/\/t.co\/tOMfFWwEsN\",\n\"Texas is healing fast thanks to all of the great men & women who have been working so hard. But still so much to do. Will be back tomorrow!\"]","1f0e4571":"# We can use the tweet tokenizer to parse these tweets:\n\ntknzr = TweetTokenizer()\ntweets_tokenized = [tknzr.tokenize(tweet) for tweet in tweets]\nprint(tweets_tokenized)","22a2ba7c":"# Get out all hashtags using loops\n\nhashtags = []\n\nfor tweet in tweets_tokenized:\n    hashtags.extend([word for word in tweet if word.startswith('#')])\n    \nprint(hashtags)","79dd1053":"# We import the Counter module from python's standard collections\n\nfrom collections import Counter\n\nword_tokenized = word_tokenize(text)\nbow = Counter(word_tokenized)\nprint(bow.most_common())","ce95f857":"# Let's add some preprocessing\n\nfrom nltk.corpus import stopwords\n\nenglish_stopwords = stopwords.words('english')\n\nword_tokenized = word_tokenize(text)\n\n# lowercasing\ncleaned_word_tokenized = [word.lower().strip() for word in word_tokenized]\n# replacing some unwanted things\ncleaned_word_tokenized = [word.replace('(','').replace(')','') for word in cleaned_word_tokenized if word.isalpha()]\n# removing stopwords\ncleaned_word_tokenized = [word for word in cleaned_word_tokenized if word not in english_stopwords]\n\nbow = Counter(cleaned_word_tokenized)\nprint(bow.most_common())","af17ac33":"# Let's import a lemmatizer from NLTK and try how it works\nfrom nltk.stem import WordNetLemmatizer\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Lemmatize all tokens into a new list: lemmatized\nlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in cleaned_word_tokenized]\n\n# Create the bag-of-words: bow\nbow = Counter(lemmatized)\n\n# Print the 10 most common tokens\nprint(bow.most_common(10))","8a8b37fc":"# We start by importing the data, ~1900 Abstracts\/Titles from Scopus\nimport pandas as pd\n\nabstracts = pd.read_csv('https:\/\/github.com\/daniel-hain\/SDC_ML_intro_2018\/raw\/master\/notebooks\/data\/abstracts.csv')","9ec537ff":"# Let's inspect the data\nabstracts.head()","b4e6cde1":"# Tokenize each abstract\nabstracts['tokenized'] = abstracts['Abstract'].map(lambda t: word_tokenize(t))","ce4c5165":"# lowecase, strip and ensure it's words\nabstracts['tokenized'] = abstracts['tokenized'].map(lambda t: [word.lower().strip() for word in t if word.isalpha()])","be529b3b":"# lemmarize and remove stopwords\nabstracts['tokenized'] = abstracts['tokenized'].map(lambda t: [wordnet_lemmatizer.lemmatize(word) for word in t if word not in stopwords_en])","4c8a2920":"# We start by importing and initializing a Gensim Dictionary. \n# The dictionary will be used to map between words and IDs\n\nfrom gensim.corpora.dictionary import Dictionary\n\n# Create a Dictionary from the articles: dictionary\ndictionary = Dictionary(abstracts['tokenized'])","3ea9f989":"# And this is how you can map back and forth\n# Select the id for \"firm\": firm_id\nfirm_id = dictionary.token2id.get(\"firm\")\n\n# Use computer_id with the dictionary to print the word\nprint(dictionary.get(firm_id))","6a309c1c":"# Create a Corpus: corpus\n# We use a list comprehension to transform our abstracts into BoWs\ncorpus = [dictionary.doc2bow(abstract) for abstract in abstracts['tokenized']]","5fa102d2":"# Print the first 10 word ids with their frequency counts from the fifth document\nprint(corpus[10][:10])\n\n# This is the same what we did before when we were counting words with the Counter (just in big)","e70573c3":"# Sort the doc for frequency: bow_doc\nbow_doc = sorted(corpus[10], key=lambda w: w[1], reverse=True)\n\n# Print the top 5 words of the document alongside the count\nfor word_id, word_count in bow_doc[:10]:\n    print(dictionary.get(word_id), word_count)","c9fb50d8":"# Import the TfidfModel from Gensim\nfrom gensim.models.tfidfmodel import TfidfModel\n\n# Create and fit a new TfidfModel using the corpus: tfidf\ntfidf = TfidfModel(corpus)\n\n# Calculate the tfidf weights of doc: tfidf_weights\ntfidf_weights = tfidf[corpus[10]]\n\n# Print the first five weights\nprint(tfidf_weights[:5])","250d5070":"# Sort the weights from highest to lowest: sorted_tfidf_weights\nsorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n\n# Print the top 5 weighted words\nfor term_id, weight in sorted_tfidf_weights[:10]:\n    print(dictionary.get(term_id), weight)","ef98c00e":"# Now we can transform the whole corpus\ntfidf_corpus = tfidf[corpus]","8b72505c":"# Just like before, we import the model\nfrom gensim.models.lsimodel import LsiModel\n\n# And we fir it on the tfidf_corpus pointing to the dictionary as reference and the number of topics.\n# In more serious settings one would pick between 300-400\nlsi = LsiModel(tfidf_corpus, id2word=dictionary, num_topics=100)","c013c4d1":"# Once the model is ready, we can inspect the topics\nlsi.show_topics(num_topics=10)","1470073b":"# And just as before, we can use the trained model to transform the corpus\nlsi_corpus = lsi[tfidf_corpus]","1dd02024":"# Load the MatrixSimilarity\nfrom gensim.similarities import MatrixSimilarity\n\n# Create the document-topic-matrix\ndocument_topic_matrix = MatrixSimilarity(lsi_corpus)\ndocument_topic_matrix = document_topic_matrix.index","8f78788f":"# Let's identify some clusters in our corpus\n\n# We import KMeans form the Sklearn library\nfrom sklearn.cluster import KMeans\n\n# Instatiate a model with 4 clusters\nkmeans = KMeans(n_clusters=10)\n\n# And fit it on our matrix\nkmeans.fit(document_topic_matrix)","45693193":"# Let's annotate our abstracts with the assigned cluster number\nabstracts['cluster'] = kmeans.labels_","a8c8379a":"# We can try to visualize our documents using TSNE - an approach for visualizing high-dimensional data\n\n# Import the module first\nfrom sklearn.manifold import TSNE\n\n# And instantiate\ntsne = TSNE()\n\n# Let's try to boil down the 100 dimensions into 2\nvisualization =  tsne.fit_transform(document_topic_matrix)","2f7763cc":"# Import plotting library\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2b2a7b42":"plt.figure(figsize=(10,10))\nsns.scatterplot(visualization[:,0],visualization[:,1], \n                data = abstracts, palette='RdBu', \n                hue=abstracts.cluster, \n                legend='full')","4f0a59a1":"# Preprocessing\nabstracts['title_tok'] = abstracts['Title'].map(lambda t: word_tokenize(t))\nabstracts['title_tok'] = abstracts['title_tok'].map(lambda t: [word.lower().strip() for word in t if word.isalpha()])\nabstracts['title_tok'] = abstracts['title_tok'].map(lambda t: [wordnet_lemmatizer.lemmatize(word) for word in t if word not in stopwords_en])","774f9651":"# Collectiong\n\nCluster = 4\n\ncluster_titles = []\nfor x in abstracts[abstracts['cluster'] == Cluster]['title_tok']:\n    cluster_titles.extend(x)","adc934fa":"# Transfortm into tf_idf format\ntitles_tfidf = tfidf[dictionary.doc2bow(cluster_titles)]","56ba64d2":"# Sort the weights from highest to lowest: sorted_tfidf_weights\ntitles_tfidf = sorted(titles_tfidf, key=lambda w: w[1], reverse=True)\n\n# Print the top 5 weighted words\nfor term_id, weight in titles_tfidf[:20]:\n    print(dictionary.get(term_id), weight)","d93db67c":"The transformed corpus is much more interesting in terms of analysis than the pure bag of words representation. In fact, you could transform it now into a matrix and perform clustering and other unsupervised machine learning.\n\n![surprise](http:\/\/www.jaclynfriedman.com\/wp-content\/uploads\/2018\/06\/giphy-23.gif)\n\n**Surprise**: This is exactly what topic modelling is about! Algorithms like LSI are closely related to PCA, NMF and SVD.\n\n","fc972a7b":"#### TF-IDF - Term Frequency - Inverse Document Frequency\n\nA token is importan for a document if appears very often\nA token becomes less important for comparaison across a corpus if it appears all over the place in the corpus\n\n*Innovation* in a corpus of abstracts talking about innovation is not that important\n\n\n\\begin{equation*}\nw_{i,j} = tf_{i,j}*log(\\frac{N}{df_i})\n\\end{equation*}\n\n- $w_{i,j}$ = the TF-IDF score for a term i in a document j\n- $tf_{i,j}$ = number of occurence of term i in document j\n- $N$ = number of documents in the corpus\n- $df_i$ = number of documents with term i\n\n\nWe will use TF-IDF to transform our corpus. However, first we need to fir the TF-IDF model.","c7fa6955":"So far you learned some basic unicode string manipulation and I also introduced NLTK. If you want to lean more about traditional NLP, check out the [free online book on NLTK](https:\/\/www.nltk.org\/book\/). You will learn old school NLP along with Python (and general programming foundations).\n\nWhen it comes to comparing documents (this is often what we want), simple \"keyword counts\" may be too simplistic and sure, we can do better \u2013 we can do topic modeling. One amazing library for working with state of the art topic models is Gensim.\n\n![gensim](https:\/\/rare-technologies.com\/wp-content\/uploads\/2017\/01\/atmodel_plot-855x645.png)\n\nLet's try to work with a bigger dataset.\n\nGensim allows you to work with a large number of high-performant NLP models including word embedding techniques.  We will be using something more traditional: TF-IDF and LSI","517a582a":"### Bag of words model\n\nIn order for a computer to understand text we need to somehow find a useful representation.\nIf you need to compare different texts e.g. articles, you will probably go for keywords. These keywords may come from a keyword-list with for example 200 different keywords\nIn that case you could represent each document with a (sparse) vector with 1 for \"keyword present\" and 0 for \"keyword absent\"\nWe can also get a bit more sophoistocated and count the number of times a word from our dictionary occurs.\nFor a corpus of documents that would give us a document-term matrix\n![example](https:\/\/i.stack.imgur.com\/C1UMs.png)\n\nLet's try creating a bag of words model from our initial example.","cbf817a6":"One important part of text preprocessing is normalization. Here we can use stemmers and lematizers to aggregate plural forms and similar. This can be extremely useful if working with languages that have a rich morphology such as Russian or Turkish.\n\n![example_stemm](https:\/\/image.slidesharecdn.com\/lightweightnaturallanguageprocessingnlp-120314154200-phpapp01\/95\/lightweight-natural-language-processing-nlp-34-728.jpg?cb=1331814243)","509f4387":"> # Natural Language Processing (NLP) in Python\n## An introduction to key concepts and techniques\n\nIn this notebook we are going to explore central concepts of NLP and their implementation in modern high-level Python libraries.\nThis is aimed to be a very general introduction to make this field more approacheable and also provide some familiarity with the specific jargon. While NLP offers many opportunities as a technique (or actually an array of different techniques) for social science research the application is yet limited but growing.\n\nThe research field of NLP itself has been turn upside-down and developed a lot since the introduction of word embeddings around 2013 and the growth of deep learning (neural network models) in the past 3-4 years. Particularly recurrent neural networks and the LSTM (Long short-term memory) variation shifted the research field.\n\n![nlp problems](https:\/\/image.slidesharecdn.com\/lang-detect-161011092815\/95\/nlp-project-full-cycle-16-638.jpg)\n\n\nThis workshop aims at presenting established techniques that I think are most useful in a social science research setting.\n\nTo be more specific, below we will explore:\n\n- basic string manipulation\n- tokens and tokenization + some preprocessing\n- the Bag-of-Words model\n- topic modeling (and its close relation to dimensionality reduction \/ unsupervised machine learning)","d20bc2f6":"Now let's explore the different clusters. For that we will look at the titles. We could do it \"manually\" but why not using NLP for that, too.\nWe will preprocess the titles, just as we did witht he abstracts and then use TF-IDF of the title-token-sum of each cluster to see which tokens are most important in which cluster.","496dab2c":"## Installing some packackages that we are going to use","0e84e4a0":"**Introducing Lambda Functions** Python allows you to write short functions in one line using the *lambda* keyword with a variable and a \":\". \nBelow we will transform the abstract column into a new one that we call tokenized compressing our preprocessing pipeline into 3 lines\n\nWe combine our lambda functions with the Pandas method \"map\" that apply this function to every row.","91f56517":"Introducing NLTK, which will make your life much easier","abef7cff":"### Basic string manipulation and tokenization\n\nIn the following we will just juse basic python string manipulation\n\nYou can do much much more if you learn using regular expressions (RegEx) but that would go too far - and you can learn some of it in the DC course.\n\nLet's start with a recent news text form the Guardian.","0f7e3166":"Sure, one could do so much more to pre-process. We could try to identify bi-grams, remove prepositions, verbs etc. But already this brings us rather far.\n\nNow we will dive into Gensim further transform our abstracts using more advanced techniques.","5f70dd09":"At this point, our corpus is a document-topic matrix. in corpus-format. We can create a full matrix using the built in MatrixSimilarity function (which is actually used for similarity-queries)","767f4114":"Let's see how this works with teweets using a well known example"}}