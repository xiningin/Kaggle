{"cell_type":{"e95c12c0":"code","1d1890fc":"code","5168ad3f":"code","d40e45d5":"code","1a1157af":"code","0f677d58":"code","08a4c51c":"code","484aa326":"code","dd0a5608":"code","292be391":"code","32487bbb":"code","57d1330d":"code","37214c88":"code","31ca9334":"code","5f77e555":"markdown","4b84ece8":"markdown","51e1162b":"markdown","d8142146":"markdown","4f1fe2d9":"markdown","367243f2":"markdown"},"source":{"e95c12c0":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\n\nplt.ion()   # interactive mode\nnp.random.seed(1234)\ntorch.manual_seed(1234)","1d1890fc":"train_transform =  transforms.Compose([\n        transforms.Resize(256),\n        # image resizing + data augmentation\n        transforms.RandomResizedCrop(224),         \n        # data augmentation\n        transforms.RandomHorizontalFlip(),         \n        # convert image to PyTorch Tensor\n        transforms.ToTensor(),    \n        # z score normalization\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n    ])\n\nval_test_transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])","5168ad3f":"# For running in VS Code\n# data_dir = '.\/data\/hymenoptera_data'\n\n# For running in Kaggle\ndata_dir = '..\/input\/hymenoptera-data\/data\/hymenoptera_data'\n\ntrain_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), train_transform)\nvalid_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), val_test_transform)\ntest_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), val_test_transform)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)","d40e45d5":"# use GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","1a1157af":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=2)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.convResidual1 = nn.Conv2d(64, 128, kernel_size=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv21 = nn.Conv2d(64, 32, kernel_size=1)\n        self.bn31 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv22 = nn.Conv2d(32, 128, kernel_size=5)\n        self.bn32 = nn.BatchNorm2d(128)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv23 = nn.Conv2d(128, 128, kernel_size=1)\n        self.bn33 = nn.BatchNorm2d(128)\n        self.relu = nn.ReLU(inplace=True)\n        self.zeroPad1 = nn.ZeroPad2d(2)\n\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.convResidual2 = nn.Conv2d(128, 256, kernel_size=1)\n        self.bn4 = nn.BatchNorm2d(256)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv31 = nn.Conv2d(128, 64, kernel_size=1)\n        self.bn51 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv32 = nn.Conv2d(64, 256, kernel_size=3)\n        self.bn52 = nn.BatchNorm2d(256)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv33 = nn.Conv2d(256, 256, kernel_size=1)\n        self.bn53 = nn.BatchNorm2d(256)\n        self.relu = nn.ReLU(inplace=True)\n        self.zeroPad2 = nn.ZeroPad2d(1)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n        self.flat = nn.Flatten()\n        self.fc = nn.Linear(256, 2, bias=True)\n\n    def forward(self, X):\n        X = self.conv1(X)\n        X = self.bn1(X)\n        X = self.relu(X)\n\n        residual1 = X\n        residual1 = self.convResidual1(residual1)\n\n        X = self.conv21(X)\n        X = self.bn31(X)\n        X = self.relu(X)\n        X = self.conv22(X)\n        X = self.bn32(X)\n        X = self.relu(X)\n        X = self.conv23(X)\n        X = self.bn33(X)\n        X = self.relu(X)\n        X = self.zeroPad1(X)\n\n        X = X + (residual1 * 2)\n        X = self.bn2(X)\n        X = self.relu(X)\n\n        X = self.maxpool(X)\n\n        residual2 = X\n        residual2 = self.convResidual2(residual2)\n\n        X = self.conv31(X)\n        X = self.bn51(X)\n        X = self.relu(X)\n        X = self.conv32(X)\n        X = self.bn52(X)\n        X = self.relu(X)\n        X = self.conv33(X)\n        X = self.bn53(X)\n        X = self.relu(X)\n        X = self.zeroPad2(X)\n\n        X = X + (residual2 * 3)\n        X = self.bn4(X)\n        X = self.relu(X)\n\n        X = self.avgpool(X)\n        X = self.flat(X)\n        X = self.fc(X)\n\n        return X\n","0f677d58":"# load and print the pretrained model\nmodel_ft = Net()\n\n# model_ft = models.resnet18(pretrained=True)\nprint(model_ft)","08a4c51c":"# move the model to GPU if available\nmodel_ft = model_ft.to(device)","484aa326":"# loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model_ft.parameters(), lr=0.01)","dd0a5608":"# Decay LR by a factor of 0.1 every 7 epochs\nscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","292be391":"epochs = 100\n \ntrain_mean_losses = []\nvalid_mean_losses = []\n\nvalid_best_loss = np.inf\n\nfor i in range(epochs):  \n    #===============================================================\n    # training \n    train_losses = []\n    \n    print(\"=========================================================\")\n    print(\"Epoch {}\".format(i))\n    \n    for iteration, batch_data in enumerate(train_loader):\n        X_batch, y_batch = batch_data\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        optimizer.zero_grad()\n        \n        out = model_ft(X_batch)\n        loss = criterion(out, y_batch.squeeze())\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_losses.append(loss)\n    \n    train_mean_loss = torch.mean(torch.stack(train_losses))\n    print('training loss: {:10.8f}'.format(train_mean_loss))\n    \n    train_mean_losses.append(train_mean_loss)\n    \n    #===============================================================\n    # validation\n    valid_losses = []\n    with torch.set_grad_enabled(False):\n        for iteration, batch_data in enumerate(valid_loader):\n            X_batch, y_batch = batch_data\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            out = model_ft(X_batch)\n            loss = criterion(out, y_batch.squeeze())\n            valid_losses.append(loss)\n            \n        valid_mean_loss = torch.mean(torch.stack(valid_losses))\n        print('validation loss: {:10.8f}'.format(valid_mean_loss))\n        \n        valid_mean_losses.append(valid_mean_loss)\n        \n        if valid_mean_loss.cpu().numpy()[()] < valid_best_loss:\n            valid_best_loss = valid_mean_loss\n            torch.save(model_ft.state_dict(), \"best_model.pth\")\n            best_epoch = i\n    #===============================================================\n    \n    scheduler.step()","32487bbb":"train_loss_list = []\nvalid_loss_list = []\n\nfor loss in train_mean_losses:\n    train_loss_list.append(loss.cpu().detach().numpy())\n\nfor loss in valid_mean_losses:\n    valid_loss_list.append(loss.cpu().detach().numpy())","57d1330d":"import matplotlib.pyplot as plt\n\nplt.plot(range(1,epochs+1), train_loss_list)\nplt.plot(range(1,epochs+1), valid_loss_list)\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.title('Train and Validation Loss Plot')\nplt.show()","37214c88":"model_ft.load_state_dict(torch.load(\"best_model.pth\"))","31ca9334":"test_predictions = np.empty((0,2))\ntest_gt = np.empty((0))\nmodel_ft = model_ft.to(device)\n\nwith torch.no_grad():\n    for iteration, batch_data in enumerate(test_loader):\n        X_batch, y_batch = batch_data  \n        X_batch_dev =  X_batch.to(device)\n        y_batch_dev =  y_batch.to(device)     \n        out = model_ft(X_batch_dev)\n        \n        test_predictions = np.append(test_predictions, out.cpu().numpy(), \n                                     axis=0)\n        test_gt = np.append(test_gt, y_batch, \n                                     axis=0)\n        \n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\n\ntest_predictions = np.array(test_predictions)\ntest_predictions = np.argmax(np.array(test_predictions), axis=1)\n\nprint(\"=========================================================\\n\")\nprint(\"Predicted Class:\")\nprint(test_predictions)\nprint(\"\\nGround Truth:\")\nprint(test_gt)\n\nprint(\"\\n=========================================================\\n\")\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(test_gt, test_predictions))\n\nprint(\"\\n=========================================================\\n\")\naccuracy = accuracy_score(test_gt, test_predictions)\nprint(\"Accuracy: {}\".format(accuracy))\n\nf1 = f1_score(test_gt, test_predictions, average='macro')\nprint(\"F1 Score: \", f1)\n\nprint(\"\\n=========================================================\\n\")\nprint(\"Classification Report:\")\nprint(classification_report(test_gt, test_predictions))","5f77e555":"<font size=\"5\">Model Evaluation<\/font>","4b84ece8":"<font size=\"5\">Import data, Preprocess (Transform), and create data loaders<\/font>","51e1162b":"<font size=\"5\">Model Training<\/font>","d8142146":"<font size=\"5\">Model Creation<\/font>","4f1fe2d9":"<font size=\"5\">Libaries Import<\/font>","367243f2":"<font size=\"5\">Model Testing<\/font>"}}