{"cell_type":{"50d4943d":"code","f6435cf5":"code","6bf076d7":"code","b9e19ded":"code","43f54b14":"code","f86c9efe":"code","63596841":"markdown","007a806d":"markdown","2f866f68":"markdown","ba9d3525":"markdown","3ed24987":"markdown","f1008fc9":"markdown"},"source":{"50d4943d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import Pool\nfrom catboost import CatBoost\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6435cf5":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest  = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')","6bf076d7":"features = [f'cont{x}' for x in range(1,15)]\ndata = train[features]\nX_test = test[features]\ntarget = train['target']\ndata.head()","b9e19ded":"train_data = (data-data.mean())\/data.std()\ntest_data = (X_test - X_test.mean())\/X_test.std()\ntrain_data.describe()","43f54b14":"params = {\n    'early_stopping_rounds' : 300,\n    'loss_function' : 'RMSE',\n    'num_boost_round' : 5000,\n    'learning_rate' : 0.005,\n    'max_depth' : 15,\n    'verbose' : 200,\n    'random_seed' : 42,\n    'task_type': 'GPU'}\n\npreds = np.zeros(test.shape[0])\n\nkf = KFold(n_splits = 7, random_state = 42, shuffle = True)\n\nrmse= []\nn=0\nfor trn_idx, test_idx in kf.split(train[features], target):\n    X_train, X_test = train[features].iloc[trn_idx], train[features].iloc[test_idx]\n    y_train, y_test = target.iloc[trn_idx], target.iloc[test_idx]\n    \n    train_pool = Pool(X_train, label = y_train)\n    test_pool = Pool(X_test, label = y_test)\n    \n    model = CatBoost(params)\n    model.fit(train_pool, eval_set = [test_pool])  \n    preds += model.predict(test[features])\/kf.n_splits\n    \n    rmse.append(mean_squared_error(y_test, model.predict(X_test), squared=False))\n    print(n+1, rmse[n])\n    n+=1\n\nprint(f\"mean RMSE for all the folds is {rmse}\")","f86c9efe":"sub['target']=preds\nsub.to_csv('submission.csv', index = False)\nsub","63596841":"### Importing data\n\n","007a806d":"### Catboost modeling\n \nparams is not the optimal solution. So please try it with various variables.\nIf you can't use the GPU, delete task_type.\nI referred to [Ensemble using XGBoost and LGBM with EDA](https:\/\/www.kaggle.com\/jyotmakadiya\/ensemble-using-xgboost-and-lgbm-with-eda) when making a model of Catboost. Please take a look at it because it contains very useful information!","2f866f68":"### Checking the data","ba9d3525":"### z-score normalization","3ed24987":"# Tabular-Catboost\n[Catboost](https:\/\/catboost.ai\/) is an open-source software library.I will introduce it because I used it for studying.\nCatboost has the following features. ([wikipedia](https:\/\/en.wikipedia.org\/wiki\/Catboost))\n* Ordered Boosting to overcome over fitting\n* Native handling for categorical features\n* Using Oblivious Trees or Symmetric Trees for faster execution\n> ","f1008fc9":"### Output"}}