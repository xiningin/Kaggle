{"cell_type":{"523bc8bf":"code","927d4f01":"code","70c6696e":"code","d157f4f2":"code","643eb0db":"code","e9aef851":"code","8711de00":"code","e4bddbe0":"code","f2e52542":"code","4fefa4e2":"code","fdf230be":"code","2aeead2a":"code","94f82271":"code","29908486":"code","e1e791ef":"code","757b3c17":"code","0b684e1e":"code","83619151":"code","bc606308":"code","1bb67e3a":"code","faae01fe":"code","cbb78c18":"code","a4ccef27":"code","ca39c551":"code","e6aa960a":"code","a3ae2f76":"code","b64cf3a9":"code","88544d5c":"code","faccb9a6":"code","98fdef14":"code","505a192a":"code","5c5f95e3":"code","8d059349":"code","4fb05782":"code","d196c8d8":"code","1ec65079":"code","0589c2ac":"code","c93118ab":"code","50ccb0f0":"code","d4ae40c1":"code","31dea649":"code","1176e3d8":"code","10a4c86b":"code","e088413b":"code","15aa7ba2":"code","fac9364e":"code","a728dbf6":"code","153b316d":"code","bf22358f":"code","b05385a4":"code","13c15d6a":"code","1978d1dc":"code","36b18845":"code","d5fa837e":"code","a4bf9f72":"code","3f2acce0":"code","7a28e87c":"code","cd17c439":"code","c9c20923":"code","f9633ad6":"code","69f7b065":"code","8f04d788":"code","074e3706":"code","ebf07e00":"code","eddadd4a":"code","e69e20f5":"markdown","f5da343a":"markdown","db5cc452":"markdown","05b71f8f":"markdown","5dfc3a6e":"markdown","cedb7c70":"markdown","090a71d9":"markdown","c33999c0":"markdown","2573e6c0":"markdown","03f2641c":"markdown","d260650f":"markdown","7916536e":"markdown","08ca6ee4":"markdown","74f523ec":"markdown","1bffd96a":"markdown","99614c70":"markdown","227e8c08":"markdown","3f883c02":"markdown","4d2ef19e":"markdown","e12e37a3":"markdown","8fef619a":"markdown","7511be48":"markdown","cff54b91":"markdown","a0266f18":"markdown","92238a03":"markdown","457d97d7":"markdown","e1de0850":"markdown"},"source":{"523bc8bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nplt.style.use(\"seaborn-notebook\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\"\"\"Machine learning models.\"\"\"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom xgboost import XGBClassifier\n\n\n\"\"\"Classification (evaluation) metrices.\"\"\"\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve, roc_auc_score","927d4f01":"# read data\ndf = pd.read_csv(\"\/kaggle\/input\/world-happiness-report-2021\/world-happiness-report.csv\")","70c6696e":"# show first five row of data\n# Afghanistan is first because of sorted by letters.\ndf.head(4)","d157f4f2":"# describe basic statistics of data\ndf.describe()","643eb0db":"# information about data\ndf.info()","e9aef851":"import missingno as msno\nmsno.matrix(df,figsize=(10,2),fontsize=12)\n# We can miss data with this plot.\nprint(\"Which column have missing values; {}\".format(list(df.columns[df.isnull().any()])))","8711de00":"# read data\ndf2021 = pd.read_csv(\"\/kaggle\/input\/world-happiness-report-2021\/world-happiness-report-2021.csv\")","e4bddbe0":"# show first five row of data\n# This dataset sorted by Ladder score.\ndf2021.head()","f2e52542":"# describe basic statistics of data\ndf2021.describe()","4fefa4e2":"# information about data\ndf2021.info()\n# Data seems fully filled.","fdf230be":"# unique countries\ndf2021['Country name'].unique()","2aeead2a":"# count regional indicator\nsns.countplot(df2021[\"Regional indicator\"])\nplt.xticks(rotation = 60)\nplt.show()","94f82271":"# Feature exctraction\nprint(df2021[\"Regional indicator\"].value_counts())\nRegions = { \"Sub-Saharan Africa\":1,\n\"Western Europe\":2,\n\"Latin America and Caribbean\":3,\n\"Middle East and North Africa\":4,\n\"Central and Eastern Europe\":5,\n\"Commonwealth of Independent States\":6,\n\"Southeast Asia\":7,\n\"South Asia\":8,\n\"East Asia\":9,\n\"North America and ANZ\":10}\ndf2021['Regional indicator encode'] = [Regions[i] for i in df2021[\"Regional indicator\"]]\n# df2021[\"Regional indicator encode\"].value_counts() check please\ndf2021.head()","29908486":"# distribution of feature set 1\nlist_features = [\"Social support\", \"Freedom to make life choices\", \"Generosity\", \"Perceptions of corruption\"]\nsns.boxplot(data = df2021.loc[:, list_features], orient = \"h\", palette = \"Set3\")\nplt.show()\n# Generosity seems a different topic except for others.\n# Perceptions of corruption have a lot of outliers because some regions are seriously corrupted.","e1e791ef":"# distribution of feature set 2\nlist_features = [\"Ladder score\", \"Logged GDP per capita\"]\nsns.boxplot(data = df2021.loc[:, list_features], orient = \"h\", palette = \"Set2\")\nplt.show()\n# # Unfortunately Ladder score also has a lot of outliers.","757b3c17":"# distribution of feature set 3\nlist_features = [\"Healthy life expectancy\"]\nbplot = sns.boxplot(data = df2021.loc[:, list_features], orient = \"v\", palette = \"Set2\",whis=[1,100])\nbplot = sns.stripplot(y='Healthy life expectancy', #x='Regional indicator', \n                   data=df2021, \n                   jitter=True, \n                   marker='o', \n                   alpha=0.5,\n                   color='black')\nplt.show()","0b684e1e":"import scipy\nfrom scipy.stats import iqr\n\n# We can use some statistical analysis\n# First quartile (Q1)\nQ1 = np.percentile(df2021[\"Healthy life expectancy\"], 25, interpolation = 'midpoint')\n  \n# Third quartile (Q3)\nQ3 = np.percentile(df2021[\"Healthy life expectancy\"], 75, interpolation = 'midpoint')\n  \n# Interquaritle range (IQR)\nIQR = scipy.stats.iqr(df2021[\"Healthy life expectancy\"],interpolation = 'midpoint')\n\n# Mean\nprint(\"Mean: \", df2021[\"Healthy life expectancy\"].mean())\n# Median\nprint(\"Median: \", df2021[\"Healthy life expectancy\"].median())\n# STD\nprint(\"STD: \", df2021[\"Healthy life expectancy\"].std())\n# Min and max value\nprint(\"Min: \", df2021[\"Healthy life expectancy\"].min())\nprint(\"Max: \", df2021[\"Healthy life expectancy\"].max())\n# IQR \nprint(\"IQR: \",IQR)\nprint(\"IQR1: {},IQR3: {}\".format(Q1,Q3))\n","83619151":"# I visualized with histogram and I can see data seems normal distribution.\nsns.histplot(x=\"Ladder score\", data=df2021,kde=True)\nplt.show()","bc606308":"# But we have another objective tests for show the normalisation.\n# They are including skewness, kurtosis, Smirnov Kolmogorov, and Shapiro Wilk tests.\n# Pandas already have skewness and Kurtosis function\nprint(\"Skewness score for this feature: {}\".format(df2021[\"Ladder score\"].skew()))\nprint(\"Kurtosis score for this feature: {}\".format(df2021[\"Ladder score\"].kurtosis()))\n","1bb67e3a":"from scipy import stats\n\nprint(\"Kolmogorov and Smirnov test's p value: {}\".format(stats.kstest(df2021[\"Ladder score\"], 'norm', alternative = 'two-sided',mode='asymp' )[1])) \n# We must choose to the parameter of \"two-sided\" for \"alternative=\" because our hypothesis is this distribution identical to normal distribution.\n# If the p-value is higher than a (is usually set a=0.05 or 0.01), the hypothesis is accepted and,\n# it is interpreted that the distribution does not show a significant difference from the normal distribution.\n# In here, our feature distribution isn't normal, because our p-value is near the zero.\n# I think is KS test implementation doesn't calculate properly every time for normal distribution. shapiro wilk easier than kstest. Lets try the shapiro wilk test.","faae01fe":"print(\"Shapiro Wilk test p value: {} \".format(stats.shapiro(df2021[\"Ladder score\"])[1]))\n# Thats it, our p-value is robustly bigger than the level of significance.\n# Finally, we can describe this feature as identical for normal distribution.","cbb78c18":"# QQ plot is also available for showing the normal distrubution.\nmeasurements = list(df2021[\"Ladder score\"].values.astype(float))\nimport numpy as np \nimport pylab \nimport scipy.stats as stats\nstats.probplot(measurements, dist=\"norm\", plot=pylab)\npylab.show()","a4ccef27":"# Yes, we can use +\/- 2 SD for the discrimination of the feature.\ndf2021[\"Ladder score\"].median() #--> 5,534\ndf2021[\"Ladder score\"].std() #--> 1,073\n# 5.534-\/+(2*1.073) # +2 SD : 7.68 and -2 SD : 3.38 Its very rare region, I choose 7.4 and 3.5.\n","ca39c551":"\ndf2021_happiest_unhappiest = df2021[(df2021.loc[:, \"Ladder score\"] > 7.4) | (df2021.loc[:, \"Ladder score\"] < 3.5)]\ndf2021_happiest_unhappiest.head()\n","e6aa960a":"sns.barplot(x = \"Ladder score\", y = \"Country name\", data=df2021_happiest_unhappiest, palette = \"coolwarm\")\nplt.title(\"Happiest and Unhappiest Countries in 2021\")\nplt.show()\n# We can say here happiness is differentiating by regions, as between regions Europe and Sub-Saharan Africa.","a3ae2f76":"plt.figure(figsize = (15,8))\nsns.kdeplot(df2021[\"Ladder score\"], hue = df2021[\"Regional indicator\"], fill = True, linewidth = 2)\nplt.axvline(df2021[\"Ladder score\"].mean(), c = \"black\")\nplt.title(\"Ladder Score Distribution by Regional Indicator\")\nplt.show()\n# Ladder score highest at Western Europe and lowest one South Asia\n# When we look at the density plot, Sub-Saharan Africa is really the worst.","b64cf3a9":"# I have adjusted for Ladder score and, I want to plot these data this way.\n\nfig = px.choropleth(df.sort_values(\"year\"), \n                    locations = \"Country name\", \n                    color = \"Life Ladder\", #call from Life Ladder --> from df\n                    locationmode = \"country names\",\n                    animation_frame = \"year\")\nfig.update_layout(title = \"Life Ladder Comparison by Countries\")\nfig.show()","88544d5c":"print(\"Shapiro Wilk test p value: {} \".format(stats.shapiro(df2021[\"Generosity\"])[1]))\nsns.histplot(x=\"Generosity\", data=df2021, kde=True) # We can easily see, this plot doesn't fit for normal distribution. Shapiro Wilk already prove that. \n# This plot right tailed.\nplt.show()","faccb9a6":"# QQ plot is doesn't fit the diagonal line because this feature doesn't have normal distributing.\nmeasurements = list(df2021[\"Generosity\"].values.astype(float))\nimport numpy as np \nimport pylab \nimport scipy.stats as stats\nstats.probplot(measurements, dist=\"norm\", plot=pylab)\npylab.show()","98fdef14":"# define outlier properties\nflierprops = dict(marker='H', markerfacecolor='red', markersize=5,  markeredgecolor='red') # Adjust for outlier value\nbplot = sns.boxplot(data = df2021.loc[:, \"Generosity\"], orient = \"v\", palette = \"Set2\",whis=[0,100],showmeans=True,flierprops=flierprops)\nbplot = sns.stripplot(y='Generosity', #x='Regional indicator', \n                   data=df2021, \n                   jitter=True, \n                   marker='o', \n                   alpha=0.5,\n                   color='black')\nplt.show()","505a192a":"# These cutoffs were intuitively adjusted.\ndf2021_g = df2021[(df2021.loc[:, \"Generosity\"] > 0.4) | (df2021.loc[:, \"Generosity\"] < -0.2)]\nsns.barplot(x = \"Generosity\", y = \"Country name\", data = df2021_g, palette = \"coolwarm\")\nplt.title(\"Most Generous and Most Ungenerous Countries in 2021\")\nplt.show()\n# Right side have high Generosity score but left side doesn't.","5c5f95e3":"fig = px.choropleth(df.sort_values(\"year\"),\n                   locations = \"Country name\",\n                   color = \"Generosity\",\n                   locationmode = \"country names\",\n                   animation_frame = \"year\")\nfig.update_layout(title = \"Generosity Comparison by Countries\")\nfig.show()","8d059349":"sns.swarmplot(x = \"Regional indicator\", y = \"Generosity\", data = df2021)\nplt.xticks(rotation = 60)\nplt.title(\"Generous Distribution by Regional Indicator in 2021\")\nplt.show()","4fb05782":"pop = pd.read_csv(\"\/kaggle\/input\/world-population-19602018\/population_total_long.csv\")\nprint(pop.shape)\npop.head()\n","d196c8d8":"#Take countr names\n# Step 1 choose Feature column\ndf2021[\"Country name\"]\n# Define unique values\ndf2021[\"Country name\"].value_counts()\n# Append their in the new index column and set this\ndf2021[\"Country name\"].value_counts().reset_index()[\"index\"]\n# Convert to list for iteration function\nall_countries = df2021[\"Country name\"].value_counts().reset_index()[\"index\"].tolist()","1ec65079":"country_continent = {}\nfor i in range(len(df2021)):\n    country_continent[df2021[\"Country name\"][i]] = df2021[\"Regional indicator\"][i]\nall_countries = df[\"Country name\"].value_counts().reset_index()[\"index\"].tolist()\nall_countries_2021 = df2021[\"Country name\"].value_counts().reset_index()[\"index\"].tolist()\n\n# Define missing countries at 2021 dataset\n\nfor x in all_countries:\n    if x not in all_countries_2021:\n        print(x)","0589c2ac":"# Now we add to the main data frame.\n\nregion = []\nfor i in range(len(df)):\n    if df['Country name'][i] == 'Angola':\n        region.append(\"Sub-Saharan Africa\")\n    elif df['Country name'][i] == 'Belize':\n        region.append(\"Latin America and Caribbean\")\n    elif df['Country name'][i] == 'Congo (Kinshasa)':\n        region.append(\"Sub-Saharan Africa\")\n    elif df['Country name'][i] == 'Syria':\n        region.append(\"Middle East and North Africa\")\n    elif df['Country name'][i] == 'Trinidad and Tobago':\n        region.append(\"Latin America and Caribbean\")\n    elif df['Country name'][i] == 'Cuba':\n        region.append(\"Latin America and Caribbean\")\n    elif df['Country name'][i] == 'Qatar':\n        region.append(\"Middle East and North Africa\")\n    elif df['Country name'][i] == 'Sudan':\n        region.append(\"Middle East and North Africa\")\n    elif df['Country name'][i] == 'Central African Republic':\n        region.append(\"Sub-Saharan Africa\")\n    elif df['Country name'][i] == 'Djibouti':\n        region.append(\"Sub-Saharan Africa\")\n    elif df['Country name'][i] == 'Somaliland region':\n        region.append(\"Sub-Saharan Africa\")\n    elif df['Country name'][i] == 'South Sudan':\n        region.append(\"Middle East and North Africa\")\n    elif df['Country name'][i] == 'Somalia':\n        region.append(\"Sub-Saharan Africa\")\n    elif df['Country name'][i] == 'Oman':\n        region.append(\"Middle East and North Africa\")\n    elif df['Country name'][i] == 'Guyana':\n        region.append(\"Latin America and Caribbean\")\n    elif df['Country name'][i] == 'Guyana':\n        region.append(\"Latin America and Caribbean\")\n    elif df['Country name'][i] == 'Bhutan':\n        region.append(\"South Asia\")\n    elif df['Country name'][i] == 'Suriname':\n        region.append(\"Latin America and Caribbean\")\n    else:\n        region.append(country_continent[df['Country name'][i]])\n        \ndf[\"region\"] = region","c93118ab":"df.head()","50ccb0f0":"all_countries = df[\"Country name\"].value_counts().reset_index()[\"index\"].tolist()\nall_countries_pop = pop[\"Country Name\"].value_counts().reset_index()[\"index\"].tolist()\n# Delete missing countries\ndel_cou = []\nfor x in all_countries:\n    if x not in all_countries_pop:\n        del_cou.append(x)\ndel_cou\n","d4ae40c1":"df.columns","31dea649":"# Temporary dataframe\n\npop_df = df[['Log GDP per capita', 'Life Ladder', 'Country name', 'year', 'Social support', 'Healthy life expectancy at birth',\n       'Freedom to make life choices', 'Generosity',\"region\",'Perceptions of corruption']].copy()\npop_df.head()","1176e3d8":"pop_df[\"Country name\"].isin([\"Afghanistan\"])\n","10a4c86b":"pop_df = pop_df[~pop_df[\"Country name\"].isin(del_cou)]\npop_df = pop_df[~pop_df.year.isin([2006,2005,2007,2018,2019,2020,2021])]\npop_dict = {x:{} for x in range(2008,2018)}\nfor i in range(len(pop)):\n    if(pop[\"Year\"][i] in range(2008,2018)):\n        pop_dict[pop[\"Year\"][i]][pop[\"Country Name\"][i]] = pop[\"Count\"][i]\n\n# The bitwise operator ~ (pronounced as the tilde) is a complement operator. It takes a one-bit operand and returns its complement. \n        #If the operand is True, it returns False, and if it is False, it returns True.\n# In the pop_df dataset we just defined, we edit the features that we do not want to include with the \"tilde\" operant.\n# We have defined a new empty dictionary as named pop_dict.\n# then we iterating to overall the indexes with the for a loop.\n# if this index value is between 2008-2018, we add the date of that year and the population number of that country in that year to the pop_dict dictionary.","e088413b":"population = [] # write new epty list\nfor i in pop_df.index: # iterate on index\n    population.append(pop_dict[pop_df[\"year\"][i]][pop_df[\"Country name\"][i]]) # append dictionary set to empty list\npop_df[\"population\"] = population # add a new column on the pop_df","15aa7ba2":"pop_df.head()","fac9364e":"fig = px.scatter(pop_df, \n                 x = \"Log GDP per capita\",\n                 y = \"Life Ladder\",\n                 animation_frame = \"year\",\n                 animation_group = \"Country name\",\n                 size = \"population\", \n                 template = \"plotly_white\",\n                 color = \"region\", \n                 hover_name = \"Country name\", \n                 size_max = 90)\nfig.update_layout(title = \"Life Ladder and Log GDP per capita Comparison by Countries via Regions for each Year\")\nfig.show()\n# From the Factfulness: [https:\/\/en.wikipedia.org\/wiki\/Factfulness:_Ten_Reasons_We're_Wrong_About_the_World_%E2%80%93_and_Why_Things_Are_Better_Than_You_Think]","a728dbf6":"fig = px.scatter(pop_df, \n                 x = \"Freedom to make life choices\",\n                 y = \"Life Ladder\",\n                 animation_frame = \"year\",\n                 animation_group = \"Country name\",\n                 size = \"population\",\n                 template = \"plotly_white\",\n                 color = \"region\", \n                 hover_name = \"Country name\", \n                 size_max = 60)\nfig.update_layout(title = \"Life Ladder and Freedom Comparison by Countries via Regions for each Year\")\nfig.show()","153b316d":"fig = px.scatter(pop_df, \n                 x = \"Perceptions of corruption\",\n                 y = \"Life Ladder\",\n                 animation_frame = \"year\",\n                 animation_group = \"Country name\",\n                 size = \"population\",\n                 template = \"plotly_white\",\n                 color = \"region\", \n                 hover_name = \"Country name\", \n                 size_max = 60)\nfig.update_layout(title = \"Life Ladder and Corruption Comparison by Countries via Regions for each Year\")\nfig.show()","bf22358f":"df.columns","b05385a4":"sns.heatmap(df.corr(), annot = True, fmt = \".2f\", linewidth = .7)\nplt.title(\"Relationship Between Features \")\nplt.show()\n\n","13c15d6a":"# Filtered correlation\ndfCorr = df.corr()\nfilteredDf = dfCorr[((dfCorr >= .5) | (dfCorr <= -.5)) & (dfCorr !=1.000)]\nplt.figure(figsize=(30,10))\nsns.heatmap(filteredDf, annot=True, cmap=\"Reds\")\nplt.show()","1978d1dc":"# Correlation and Covariance between the features\n\ndef get_feature_correlation(df, top_n=None, corr_method='spearman',\n                            remove_duplicates=True, remove_self_correlations=True):\n\n    corr_matrix_abs = df.corr(method=corr_method)\n    \n    corr_matrix_abs_us = corr_matrix_abs.unstack().reset_index()\n    sorted_correlated_features = corr_matrix_abs_us \n\n    # Remove comparisons of the same feature\n    if remove_self_correlations:\n        sorted_correlated_features = sorted_correlated_features[\n            (sorted_correlated_features.level_0 != sorted_correlated_features.level_1)\n        ]\n\n    # Remove duplicates\n    if remove_duplicates:\n        sorted_correlated_features = sorted_correlated_features.iloc[:-2:2]\n\n    # Create meaningful names for the columns\n    sorted_correlated_features.columns = ['Feature 1', 'Feature 2', 'Correlation']\n    corrdf = pd.DataFrame(sorted_correlated_features)\n    corrdf['Correlation Result'] = ['Inverse Proportion' if corrdf[[\"Correlation\"]].values.tolist()[i][0] < 0 \n                                    else 'Direct Proportion' for i in range(len(corrdf[[\"Correlation\"]].values.tolist()))]\n    if top_n:\n        return sorted_correlated_features[:top_n]\n\n    return sorted_correlated_features\n\n\n\ndef get_feature_covariance(df, min_periods=None, ddof=1,\n                            remove_duplicates=True, top_n=None, remove_self_cov=True):\n\n    cov_matrix_abs = df.cov(min_periods=None, ddof=1)\n    cov_matrix_abs_us = cov_matrix_abs.unstack()\n    sorted_cov_features = cov_matrix_abs_us.reset_index()\n\n    # Remove comparisons of the same feature\n    if remove_self_cov:\n        sorted_cov_features = sorted_cov_features[\n            (sorted_cov_features.level_0 != sorted_cov_features.level_1)\n        ]\n\n    # Remove duplicates\n    if remove_duplicates:\n        sorted_cov_features = sorted_cov_features.iloc[:-2:2]\n\n    # Create meaningful names for the columns\n    sorted_cov_features.columns = ['Feature 1 cov', 'Feature 2 cov', 'Covariance']\n    covdf = pd.DataFrame(sorted_cov_features)\n    covdf['Covariance Result'] = [\"Doesn't Related\" if covdf[[\"Covariance\"]].values.tolist()[i][0] < 0 else 'Related' for i in range(len(covdf[[\"Covariance\"]].values.tolist()))]\n    if top_n:\n        return sorted_cov_features[:top_n]\n\n    return sorted_cov_features\n\ndef get_corr_cov_df (df,bound= 0.5): #bound input is a filter, default I decided to 0.5.\n    \n    covdf = pd.DataFrame(get_feature_covariance(df))\n    corrdf = pd.DataFrame(get_feature_correlation(df))\n    corrcovdf = pd.concat([corrdf, covdf],ignore_index=False, axis=1, join = 'outer',keys=None,)\n    corrcovdf = corrcovdf[((corrcovdf[\"Correlation\"] >= bound) | (corrcovdf[\"Correlation\"]<= -bound)) & (corrcovdf[\"Correlation\"] !=1.000)]\n    corrcovdf = corrcovdf.sort_values(by= ['Correlation'], ascending=False)\n    corrcovdf = corrcovdf.drop(columns=['Feature 1 cov','Feature 2 cov'])\n    my_df = corrcovdf.copy()\n    return(my_df)\n\nget_corr_cov_df(df,0.5)\n","36b18845":"sns.clustermap(df.corr(), center = 0, cmap = \"vlag\", dendrogram_ratio = (0.1, 0.2), annot = True, linewidths = .7, figsize=(10,10))\nplt.show()","d5fa837e":"# Let's the try understanding for the cause of Life Ladder with machine learning.","a4bf9f72":"# The most and least happy country\n# df.sort_values sorts the numeric values in ascending oreder\n# If 'ignore_index'=False, the original index of the dataframe won't change after sorting\n\nleast_happy_country=df.sort_values(by='Life Ladder', ignore_index=True)['Country name'].iloc[0]\nmost_happy_country=df.sort_values(by='Life Ladder', ignore_index=True)['Country name'].iloc[-1]\n\nprint(f'The most happy country is {most_happy_country}, and unfortunately the least happy country is {least_happy_country}')","3f2acce0":"# Examination for missing values:\nmsno.matrix(df,figsize=(10,2),fontsize=12)\nprint(\"Which column have missing values; {}\".format(list(df.columns[df.isnull().any()])))","7a28e87c":"# Fill nan values with complex function\n\n\ndef fillna (df,nancolumn,guidecolumn):\n    \"\"\"\nThis function have three parameters\n1. df = pd.DataFrame\n2. nancolumn = contain to NaN values\n3. guidecolumn = I used this paramter for filter\nFor example; feature of 'a' have nan values in df DataFrame, I want to filling that.\nEasisest way df.fillna(something) etc.\nBut it is make bias\nIn this kernel we have a lot of countries in DataFrame\nFor example; pop.loc[:,('Generosity','Country name')][(pop['Country name']=='Japan')] <-- Try this\nFeature of Generosity include ten values for Japan, just one values is NaN in this array.\nBest way is for replace this value, change with mean of other unique Japan values. This def doing this.\n\n\"\"\"\n    uniquelist = list(df[(df[nancolumn].isna())][guidecolumn].unique())\n    print(\"\\nCHANGE COLUMN: {}\".format(nancolumn))\n    print('Unique values are: ',uniquelist)\n    for i in uniquelist:\n        summary = []\n        isnanvalues = list(df.loc[df[guidecolumn].isin([i])][nancolumn].isna().reset_index(drop=True))\n        for j in range(len(isnanvalues)):\n            if isnanvalues[j] == True:\n                pass\n            else:\n                summary.append(list(df.loc[df[guidecolumn].isin([i])][nancolumn].reset_index(drop=True))[j])\n        if len(summary) > 0:\n            changes = df[(~df[nancolumn].isna())&(df[guidecolumn]==i)][nancolumn].mean(skipna=True)\n            print('Unique value is {} in {} feature'.format(i,nancolumn))\n            mask = df[nancolumn].isna() & df[guidecolumn].eq(i)\n            df.loc[mask, nancolumn] = changes\n            print('Change with:',changes)\n        elif len(summary) == 0:\n            print('Unique value is {} in {} feature'.format(i,nancolumn))\n            changes = df[nancolumn].mean(skipna=True)\n            print('Change with:',changes)\n            mask = df[nancolumn].isna() & df[guidecolumn].eq(i)\n            df.loc[mask, nancolumn] = changes","cd17c439":"list_of_missed_values = list(df.columns[df.isnull().any()])\nfor i in list_of_missed_values:\n    fillna (df=df, nancolumn=i,guidecolumn='Country name')\nmsno.matrix(df,figsize=(10,2),fontsize=12)","c9c20923":"df.columns","f9633ad6":"y=df['Life Ladder']\n\ncol_to_consider=['year', 'Log GDP per capita',\n       'Social support', 'Healthy life expectancy at birth',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Positive affect', 'Negative affect',\n       'region']\n\nX= df[col_to_consider]","69f7b065":"# kategorik kolonlar (object)\n\ncat_cols = X.select_dtypes(include='object').columns # region\n\n\nX = pd.get_dummies(X, columns=['region'])\nxcol = X.columns\nX","8f04d788":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X,columns=xcol)","074e3706":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=10,\n    n_informative=3,\n    n_redundant=0,\n    n_repeated=0,\n    n_classes=2,\n    random_state=0,\n    shuffle=False,\n)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n","ebf07e00":"from sklearn.ensemble import RandomForestClassifier\n\nfeature_names = [f\"feature {i}\" for i in range(X.shape[1])]\nforest = RandomForestClassifier(random_state=0)\nforest.fit(X_train, y_train)","eddadd4a":"# Getting the feature importance\nfeature_importances_=forest.feature_importances_\nfeature_importances=pd.DataFrame({'Feature_name':col_to_consider, 'Feature_importance':feature_importances_})\n\nfig, ax=plt.subplots(1, figsize=(15,8))\nsns.barplot(x='Feature_name', y='Feature_importance', data=feature_importances, ax=ax)\n\n# For making the graph look good\nplt.xticks(fontsize=12, rotation=30); # Rotating the names by 30 degrees as the names were mixing with each other \nplt.yticks(fontsize=14);\n\nplt.xlabel('Feature name',fontsize=18)\nplt.ylabel('Feature importance',fontsize=18)\n","e69e20f5":"## Analysis Content\n1. [Python Libraries](#1)\n1. [Data Content](#2)\n1. [Read and Analyse Data](#3)\n1. [Data Distributions in 2021](#4)\n1. [Happiest and Unhappiest Countries in 2021](#5)\n1. [Ladder Score Distribution by Regional Indicator](#6)\n1. [Ladder Score Distribution by Countries in Map View](#7)\n1. [Most Generous and Most Ungenerous Countries in 2021](#8)\n1. [Generous Distribution by Countries in Map View](#9)\n1. [Generous Distribution by Regional Indicator in 2021](#10)\n1. [Relationship Between Happiness and Income](#11)\n1. [Relationship Between Happiness and Freedom](#12)\n1. [Relationship Between Happiness and Corruption](#13)\n1. [Relationship Between Features](#14)\n1. [Machine Learning Model](#16)\n1. [Conclusion](#15)","f5da343a":"## Introduction\n* The World Happiness Report is a landmark survey of the state of global happiness. \n* The report continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. \n* Leading experts across fields \u2013 economics, psychology, survey analysis, national statistics, health, public policy and more \u2013 describe how measurements of well-being can be used effectively to assess the progress of nations. \n* The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.\n\n![mutluluk.JPG](attachment:76ae63e6-c7ed-4512-a0b5-4ac821637e81.JPG)","db5cc452":"# WORLD HAPPINESS EXPLANATORY DATA ANALYSIS","05b71f8f":"We have many factors like GDP, Social support, Health, Freedom, Corruption, Dystopia, and Generosity for deciding how happy a country is. Now let's find which factor influence more to the happiness index. For this purpose, we will make a Machine Learning model and using the [feature importance](https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html), we can know which factor is most important for happiness.\n\n![Random Forest Decission](attachment:5df6afde-51b2-4333-b328-eb3c1244558c.png)","5dfc3a6e":"<a id='2'><\/a>\n## Data Content\n* The happiness scores and rankings use data from the Gallup World Poll. \n    * Gallup World Poll: In 2005, Gallup began its World Poll, which continually surveys citizens in 160 countries, representing more than 98% of the world's adult population. The Gallup World Poll consists of more than 100 global questions as well as region-specific items.\n* The columns following the happiness score estimate the extent to which each of six factors \u2013 economic production, social support, life expectancy, freedom, absence of corruption, and generosity \u2013 contribute to making life evaluations higher in each country than they are in Dystopia, a hypothetical country that has values equal to the world\u2019s lowest national averages for each of the six factors. \n* **Ladder score:** Happiness score or subjective well-being. This is the national average response to the question of life evaluations.\n* **Logged GDP per capita:** The GDP-per-capita time series from 2019 to 2020 using countryspecific forecasts of real GDP growth in 2020.\n* **Social support:** Social support refers to assistance or support provided by members of social networks to an individual.\n* **Healthy life expectancy:** Healthy life expectancy is the average life in good health - that is to say without irreversible limitation of activity in daily life or incapacities - of a fictitious generation subject to the conditions of mortality and morbidity prevailing that year.\n* **Freedom to make life choices:** Freedom to make life choices is the national average of binary responses to the GWP question \u201cAre you satisfied or dissatisfied with your freedom to choose what you do with your life?\u201d ... It is defined as the average of laughter and enjoyment for other waves where the happiness question was not asked\n* **Generosity:** Generosity is the residual of regressing national average of response to the GWP question \u201cHave you donated money to a charity in the past month?\u201d on GDP per capita.\n* **Perceptions of corruption:** The measure is the national average of the survey responses to two questions in the GWP: \u201cIs corruption widespread throughout the government or not\u201d and \u201cIs corruption widespread within businesses or not?\u201d\n* **Ladder score in Dystopia:** It has values equal to the world\u2019s lowest national averages. Dystopia as a benchmark against which to compare contributions from each of the six factors. Dystopia is an imaginary country that has the world's least-happy people. ... Since life would be very unpleasant in a country with the world's lowest incomes, lowest life expectancy, lowest generosity, most corruption, least freedom, and least social support, it is referred to as \u201cDystopia,\u201d in contrast to Utopia\n* World Happiness Report Official Website: https:\/\/worldhappiness.report\/","cedb7c70":"## What will you learn from this project?\n* Bivariate data analysis\n* Multivariate data analysis\n* Seaborn library visualization techniques: bar, box, kde, swarm, heatmap, clustermap\n* Plotly library visualization techniques: animated plot and world map","090a71d9":"The Shapiro-Wilk test is really most appropriate for normality tests.  I also want to show another test aka Kolmogorov Smirnov.","c33999c0":"![image.png](attachment:457a90de-d7d0-49f1-a9be-ad7ddfe0b487.png)tolist","2573e6c0":"<a id='14'><\/a>\n## Relationship Between Features ","03f2641c":"<a id='6'><\/a>\n## Ladder Score Distribution by Regional Indicator","d260650f":"<a id='5'><\/a>\n## Happiest and Unhappiest Countries in 2021","7916536e":"<a id='15'><\/a>\n## Conclusion\n* We can say with this model, which factor is most important for happiness --> Money :D","08ca6ee4":"Acknowledge:\n- https:\/\/www.kaggle.com\/sonukiller99\/world-is-happy-or-is-it-eda-ml\n- DATAI team","74f523ec":"<a id='9'><\/a>\n## Generous Distribution by Countries in Map View","1bffd96a":"<a id='16'><\/a>\n## Machine Learning model for;\n  ### which factor is most important for happiness","99614c70":"<a id='12'><\/a>\n## Relationship Between Happiness and Freedom","227e8c08":"<a id='11'><\/a>\n## Relationship Between Happiness and Income","3f883c02":"<a id='3'><\/a>\n## Read and Analyse Data","4d2ef19e":"## Normal distribution tests","e12e37a3":"<a id='13'><\/a>\n## Relationship Between Happiness and Corruption","8fef619a":"![It is image for IQR](attachment:a7aa3140-6fc1-4bb8-a5e7-7ea2017ac5ea.png)","7511be48":"* Skewness\n* If skewness is 0, the data are perfectly symmetrical, although it is quite unlikely for real-world data. As a general rule of thumb:\n    * If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n    * If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n    * If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n        * [Tutorial for Skewness](https:\/\/community.gooddata.com\/metrics-and-maql-kb-articles-43\/normality-testing-skewness-and-kurtosis-241)\n* Kurtosis\n    * If the kurtosis is close to 0, then a normal distribution is often assumed.  These are called mesokurtic distributions.  If the kurtosis is less than zero, then the distribution is light tails and is called a platykurtic distribution.  If the kurtosis is greater than zero, then the distribution has heavier tails and is called a leptokurtic distribution.       \n        * [Tutorial for Kurtosis](https:\/\/www.spcforexcel.com\/knowledge\/basic-statistics\/are-skewness-and-kurtosis-useful-statistics#kurtosis)\n\n","cff54b91":"<a id='1'><\/a>\n## Python Libraries\n* In this section, we import used libraries during this kernel.","a0266f18":"<a id='10'><\/a>\n## Generous Distribution by Regional Indicator in 2021","92238a03":"<a id='4'><\/a>\n## Data Distributions in 2021\n* Unique Countries\n* Count Regional Indicator\n* Distribution of Remaining Features","457d97d7":"<a id='8'><\/a>\n## Most Generous and Most Ungenerous Countries in 2021","e1de0850":"<a id='7'><\/a>\n## Ladder Score Distribution by Countries in Map View"}}