{"cell_type":{"b6f71b0b":"code","eff6a4b8":"code","35007b90":"code","84aab765":"code","a0b2c430":"code","b298986e":"code","2f7ae35b":"code","421a2da7":"code","7d02d26b":"code","bfae8ba6":"code","1f2961de":"code","feab2b78":"code","d573d12b":"code","cdcf6a54":"code","434cacf9":"code","51d27411":"code","828aea1e":"code","a8156d77":"code","5fce5587":"code","4f145666":"code","912d2338":"code","c9a85d7c":"code","555fb4c0":"code","b77e9541":"code","40b7b5d7":"code","368a2ae3":"code","038a9311":"code","1567b6a9":"code","1443012c":"code","279098c5":"markdown","9e4890f1":"markdown","e6348940":"markdown","45c72950":"markdown","960e364c":"markdown","293fdbba":"markdown","50061f02":"markdown","425e32c0":"markdown","76baa2f4":"markdown","992c6df5":"markdown","e8b1ad1b":"markdown","0a79b811":"markdown","a66ccf67":"markdown","c9eabee3":"markdown","24de9d59":"markdown","3ab03c47":"markdown","5fe1e893":"markdown","a5d8eba7":"markdown","58b0ec42":"markdown","befcbac5":"markdown","f94919c6":"markdown","584654fc":"markdown","33a05a23":"markdown","4946d96c":"markdown","fc3f42ae":"markdown","f822cffe":"markdown","f3154aad":"markdown","fb30907b":"markdown","33573ebc":"markdown","a1e51699":"markdown","b18651b3":"markdown","e9754915":"markdown","3674945f":"markdown","bc7d69af":"markdown","1bda5a7b":"markdown","7e50d03a":"markdown","cc738f6e":"markdown"},"source":{"b6f71b0b":"import pandas as pd\nimport numpy as np\n\nimport torch \nfrom torch import tensor\nimport matplotlib.pyplot as plt\nimport random\nimport os\n\nimport imageio\nfrom IPython.display import Image\n\nclass GeradorGIF():\n    \"\"\"Gera um GIF a partir de uma sequ\u00eancia de gr\u00e1ficos\"\"\"\n    \n    fig_number = 0\n    tempfiles = []\n    filename = \"\"\n    path = \"\"\n    frame_duration=None\n    \n    def __init__(self, path, filename, frame_duration=None):\n        \"\"\"path - o diret\u00f3rio onde o GIF ser\u00e1 salvo.\n        filename - o nome do arquivo GIF a ser salvo.\"\"\"\n        \n        self.fig_number = 0\n        self.tempfiles = []\n        self.path = path\n        self.filename = filename\n        self.frame_duration=frame_duration\n\n    def add(self, fig):\n        \"\"\"Esse m\u00e9todo deve ser chamado para cada gr\u00e1fico do matplotlib que se queira adicionar ao GIF.\n        \n        fig - Um objeto matplotlib.pyplot.figure, que ser\u00e1 colocado como um quadro do GIF\"\"\"\n        \n        self.fig_number += 1\n        filename = self.path+\"\/tmp_\"+str(self.fig_number)+\"_\"+self.filename+\".png\"\n        plt.savefig(filename)\n        self.tempfiles.append(filename)\n        \n    def finish(self):\n        \"\"\"Esse m\u00e9todo deve ser chamado quando todas os gr\u00e1ficos necess\u00e1rios j\u00e1 tiverem sido adicionados. \n        Ele ir\u00e1 gerar o arquivo GIF final.\"\"\"\n        if self.frame_duration is None:\n            with imageio.get_writer(self.path+\"\/\"+self.filename+\".gif\", mode='I') as writer:\n                for filename in self.tempfiles:\n                    image = imageio.imread(filename)\n                    writer.append_data(image, meta={\"fps\":1})  \n        else:\n            frames = []\n            for filename in self.tempfiles:\n                frames.append(imageio.imread(filename))\n            kargs = { 'duration': self.frame_duration }\n            imageio.mimsave(self.path+\"\/\"+self.filename+\".gif\", frames, 'GIF', **kargs)\n            del frames\n\n        for filename in set(self.tempfiles):\n            os.remove(filename) \n                \n    def show(self, size=None):    \n        \"\"\"Mostra o arquivo GIF gerado.\"\"\"\n        if size is None:\n            im = Image(filename=f'{self.path}\/{self.filename}.gif')\n        else:\n            im = Image(filename=f'{self.path}\/{self.filename}.gif', width=size[0],height=size[1]) \n        return im","eff6a4b8":"treinamento = pd.DataFrame({\"localizacao\":[\"centro\",\"zona norte\",\"praias\"], \"tamanho\":[50,65,30], \"numero_quartos\":[2,3,1], \"preco\":[300,400,120]})\ntreinamento","35007b90":"def loss(previsao, treinamento):\n    return (previsao-treinamento).abs().mean()\n\nf1 = tensor([5000.0, 20.0, 5.0])\nf = tensor(treinamento.preco)\n\nerro = loss(f1, f)\nprint(f\"Erro: {erro:.3f}\") ","84aab765":"from sklearn.metrics import mean_absolute_error\n\nerro = mean_absolute_error(f1, f)\nprint(f\"Erro: {erro:.3f}\") ","a0b2c430":"f2 = np.array([4500, 50, 10])\n\nerro = mean_absolute_error(f2, f)\nprint(f\"Erro: {erro:.3f}\") ","b298986e":"def f_erro(x):\n    #ignore essa equa\u00e7\u00e3o. ela n\u00e3o tem nenhum significado em especial. \u00e9 s\u00f3 para gerar um gr\u00e1fico ilustrativo.\n    return 0.5*x**2-10*x+55 \n\nx = np.linspace(0,20,20)\ny = f_erro(x)\n\nax =  plt.subplots()[1]\nax.set_title('Curva dos erros para todas as poss\u00edveis fun\u00e7\u00f5es f')\nplt.xlabel(\"Poss\u00edveis solu\u00e7\u00f5es para f\")\nplt.ylabel(\"Erro\")\nax.set_xticklabels([])\nax.set_yticklabels([])\nplot = plt.plot(x,y)\n","2f7ae35b":"p1_x = 1\np1_y = f_erro(p1_x)\n\nax =  plt.subplots()[1]\nax.set_title('Erro da fun\u00e7\u00e3o f1')\nplt.xlabel(\"Poss\u00edveis solu\u00e7\u00f5es para f\")\nplt.ylabel(\"Erro\")\nplt.plot(x,y, linestyle='dashed')\nplt.scatter(p1_x, p1_y, color=\"red\")\nplt.text(p1_x+0.5, p1_y, \"loss de f1 = 1731\")\nax.set_xticklabels([])\nrejeito = ax.set_yticklabels([])","421a2da7":"p1_x = 1\np1_y = f_erro(p1_x)\np2_x = 2\np2_y = f_erro(p2_x)\n\n\nax =  plt.subplots()[1]\nax.set_title('Erro das fun\u00e7\u00f5es f1 e f2')\nplt.xlabel(\"Poss\u00edveis solu\u00e7\u00f5es para f\")\nplt.ylabel(\"Erro\")\nplt.plot(x,y, linestyle='dashed')\nplt.scatter(p1_x, p1_y, color=\"red\")\nplt.text(p1_x+0.5, p1_y, \"loss de f1 = 1731\")\nplt.scatter(p2_x, p2_y, color=\"red\")\nplt.text(p2_x+0.5, p2_y, \"loss de f2 = 1553\")\nax.set_xticklabels([])\nrejeito = ax.set_yticklabels([])","7d02d26b":"p1_x = tensor(1.0).requires_grad_()\np1_y = f_erro(p1_x)\np1_y.backward()\ntamanho_seta = 3\nder_x = tamanho_seta\nder_y = p1_x.grad*tamanho_seta\n\nax =  plt.subplots()[1]\nax.set_title('Derivada do erro em f1')\nax.arrow(p1_x, p1_y, der_x, der_y, head_width=1, color=\"green\")\nplt.xlabel(\"Poss\u00edveis solu\u00e7\u00f5es para f\")\nplt.ylabel(\"Erro\")\nplt.plot(x,y, linestyle='dashed')\nplt.scatter(p1_x.data, p1_y.data, color=\"red\")\nplt.text(p1_x+0.5, p1_y, \"loss de f1\")\nax.set_xticklabels([])\nrejeito = ax.set_yticklabels([])","bfae8ba6":"p1_x = tensor(1.0).requires_grad_()\np1_y = f_erro(p1_x)\np2_x = tensor(2.0).requires_grad_()\np2_y = f_erro(p2_x)\np2_y.backward()\ntamanho_seta = 3\nder_x = tamanho_seta\nder_y = p2_x.grad*tamanho_seta\n\nax =  plt.subplots()[1]\nax.set_title('Derivada do erro em f2')\nax.arrow(p2_x, p2_y, der_x, der_y, head_width=1, color=\"green\")\nplt.xlabel(\"Poss\u00edveis solu\u00e7\u00f5es para f\")\nplt.ylabel(\"Erro\")\nplt.plot(x,y, linestyle='dashed')\nplt.scatter(p1_x.data, p1_y.data, color=\"red\")\nplt.scatter(p2_x.data, p2_y.data, color=\"red\")\nplt.text(p1_x+0.5, p1_y, \"loss de f1\")\nplt.text(p2_x+0.5, p2_y, \"loss de f2\")\nax.set_xticklabels([])\nrejeito = ax.set_yticklabels([])","1f2961de":"p1_x = tensor(1.0).requires_grad_()\nlearning_rate = 2.015\n\ngeradorGIF = GeradorGIF(\".\/\",\"lr_muito_grande\",0.5)\n\nfor i in range(8):\n    p1_y = f_erro(p1_x)\n    ax =  plt.subplots()[1]\n    ax.set_title('Taxa de aprendizado muito alta')\n    plt.xlabel(\"Poss\u00edveis solu\u00e7\u00f5es para f\")\n    plt.ylabel(\"Erro\")\n    plt.plot(x,y, linestyle='dashed')\n    plt.scatter(p1_x.data, p1_y.data, color=\"red\")\n    ax.set_xticklabels([])\n    rejeito = ax.set_yticklabels([])\n    p1_y.backward()\n    p1_x.data -= p1_x.grad.data*learning_rate\n    p1_x.grad = None\n    geradorGIF.add(ax)\n    plt.close()\n\ngeradorGIF.finish()\ngeradorGIF.show((500,500))","feab2b78":"p1_x = tensor(1.0).requires_grad_()\nlearning_rate = 0.01\n\ngeradorGIF = GeradorGIF(\".\/\",\"lr_muito_pequena\",0.5)\n\nfor i in range(20):\n    p1_y = f_erro(p1_x)\n    ax =  plt.subplots()[1]\n    ax.set_title('Taxa de aprendizado muito baixa')\n    plt.xlabel(\"Poss\u00edveis solu\u00e7\u00f5es para f\")\n    plt.ylabel(\"Erro\")\n    plt.plot(x,y, linestyle='dashed')\n    plt.scatter(p1_x.data, p1_y.data, color=\"red\")\n    ax.set_xticklabels([])\n    rejeito = ax.set_yticklabels([])\n    p1_y.backward()\n    p1_x.data -= p1_x.grad.data*learning_rate\n    p1_x.grad = None\n    geradorGIF.add(ax)\n    plt.close()\n\ngeradorGIF.finish()\ngeradorGIF.show((500,500))","d573d12b":"p1_x = tensor(1.0).requires_grad_()\nlearning_rate = 1.95\n\ngeradorGIF = GeradorGIF(\".\/\",\"lr_grande\",0.5)\n\nfor i in range(20):\n    p1_y = f_erro(p1_x)\n    ax =  plt.subplots()[1]\n    ax.set_title('Taxa de aprendizado grande')\n    plt.xlabel(\"Poss\u00edveis solu\u00e7\u00f5es para f\")\n    plt.ylabel(\"Erro\")\n    plt.plot(x,y, linestyle='dashed')\n    plt.scatter(p1_x.data, p1_y.data, color=\"red\")\n    ax.set_xticklabels([])\n    rejeito = ax.set_yticklabels([])\n    p1_y.backward()\n    p1_x.data -= p1_x.grad.data*learning_rate\n    p1_x.grad = None\n    geradorGIF.add(ax)\n    plt.close()\n\ngeradorGIF.finish()\ngeradorGIF.show((500,500))","cdcf6a54":"p1_x = tensor(1.0).requires_grad_()\nlearning_rate = 0.4\n\ngeradorGIF = GeradorGIF(\".\/\",\"lr_ideal\",0.5)\n\nfor i in range(20):\n    p1_y = f_erro(p1_x)\n    ax =  plt.subplots()[1]\n    ax.set_title('Taxa de aprendizado ideal')\n    plt.xlabel(\"Poss\u00edveis solu\u00e7\u00f5es para f\")\n    plt.ylabel(\"Erro\")\n    plt.plot(x,y, linestyle='dashed')\n    plt.scatter(p1_x.data, p1_y.data, color=\"red\")\n    ax.set_xticklabels([])\n    rejeito = ax.set_yticklabels([])\n    p1_y.backward()\n    p1_x.data -= p1_x.grad.data*learning_rate\n    p1_x.grad = None\n    geradorGIF.add(ax)\n    plt.close()\n\ngeradorGIF.finish()\ngeradorGIF.show((500,500))","434cacf9":"#definindo a semente aleat\u00f3ria para os resultados ficarem consistentes\ntorch.manual_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\n\n#gerando um tensor com os dados de treinamento\npos_x = torch.arange(20).float()\npos_y = pos_x**2-20*pos_x+torch.randn(20)*5+110\ntreino = torch.stack((pos_x, pos_y))\n\nplt.scatter(treino[0],treino[1])\nplt.title('Dados de treinamento')\nplt.ylabel(\"problemas de sa\u00fade\")\nt = plt.xlabel(\"quantidade de comida\")","51d27411":"#definindo a fun\u00e7\u00e3o que tentar\u00e1 modelar os dados de treinamento\ndef f(quantidade_comida, pesos):\n    x = quantidade_comida\n    a, b, c = pesos\n    return a*(x**2) + b*x + c","828aea1e":"#gerando os pesos iniciais aleatoriamente\npesos = torch.randn(3).requires_grad_()","a8156d77":"#substituindo os pesos aleat\u00f3rios por outros fixos s\u00f3 por quest\u00f5es did\u00e1ticas (o gr\u00e1fico fica melhor)\npesos = tensor([-0.87, 1, 150]).requires_grad_()\n\nprint(f\"a={pesos[0]:.3f}, b={pesos[1]:.3f}, c={pesos[2]:.3f}\")","5fce5587":"def plotar_treino_vs_predicoes(treino, predicoes, geradorGIF=None):\n    fig = plt.figure()\n    ax =  plt.subplots()[1]\n    ax.set_title('Predi\u00e7\u00e3o x Dados de treinamento')\n    ax.scatter(treino[0], treino[1])\n    ax.plot(treino[0], predicoes.detach().numpy(), color='red')\n    ax.set_ylim(0,150)\n    plt.ylabel(\"problemas de sa\u00fade\")\n    t = plt.xlabel(\"quantidade de comida\")\n    \n    if geradorGIF is not None:\n        geradorGIF.add(fig)\n        plt.close()        ","4f145666":"predicoes = f(treino[0], pesos)\nplotar_treino_vs_predicoes(treino, predicoes)","912d2338":"erro = loss(predicoes, treino[1])\nprint(f\"Erro atual: {erro}\")","c9a85d7c":"erro.backward() #calcula a derivada\ntaxa_aprendizado = 0.001\n\npesos.data -= pesos.grad.data * taxa_aprendizado\npesos.grad = None\nprint(f\"a={pesos[0]:.3f}, b={pesos[1]:.3f}, c={pesos[2]:.3f}\")","555fb4c0":"predicoes = f(treino[0], pesos)\nerro = loss(predicoes, treino[1])\nprint(f\"Erro atual: {erro}\")","b77e9541":"predicoes = f(treino[0], pesos)\nplotar_treino_vs_predicoes(treino, predicoes)","40b7b5d7":"passos_treinamento = []\n\ndef step(treino, pesos, gravar_passo=False, geradorGIF=None):\n    predicoes = f(treino[0], pesos)\n    theloss = loss(predicoes, treino[1])\n    theloss.backward()\n    if gravar_passo:\n        plotar_treino_vs_predicoes(treino, predicoes, geradorGIF)\n        passos_treinamento.append(torch.clone(pesos.data))\n    pesos.data -= pesos.grad.data*0.001\n    pesos.grad = None\n    return pesos","368a2ae3":"geradorGIF = GeradorGIF(\".\/\", \"aprendizado\")\n\nplt.rcParams.update({'figure.max_open_warning': 0})\nfor i in range(20000):\n    gravar_passo = (((i % 500)==0) or (i<10))\n    pesos = step(treino, pesos , gravar_passo, geradorGIF)\n        \ngeradorGIF.finish()","038a9311":"geradorGIF.show((500,500))","1567b6a9":"def gerar_amostra(centro, itens, dispersao):\n    return np.linspace(centro-dispersao\/2.0,centro+dispersao\/2.0,itens,endpoint=True)\n\n#gerando uma amostra dos poss\u00edveis pesos durante a otimiza\u00e7\u00e3o\na = gerar_amostra(1.2256, 40, 4)\nb = gerar_amostra(-26.0045, 40, 40)\nc = gerar_amostra(141.5775, 40, 7)\n\n#esse o valor m\u00e1xio de loss que colocaremos no gr\u00e1fico, para evitar que ele fique muito grande\nmaxloss= 150\n\n#definindo uma fun\u00e7\u00e3o que encontra o erro a partir dos parametros (pesos)\ndef lossparam(params):\n    previsao = f(treino[0], params)\n    return loss(previsao, treino[1])\n\n#calculando todos os valoers de loss para o espa\u00e7o de possibilidades de a e b com c fixo\npontos = []\nfor ax in a:\n    for bx in b:\n        l = lossparam(tensor([ax,bx,141.5775]))\n        if l<maxloss:\n            pontos.append([ax,bx,l])\n\ngradiente_ab = tensor(pontos)  \n\n#calculando todos os valores de loss para o espa\u00e7o de possibilidades de a e c com b fixo\npontos = []\nfor ax in a:\n    for cx in c:\n        l = lossparam(tensor([ax, -26.0045, cx]))\n        if l<maxloss:\n            pontos.append([ax,cx,l])\n            \ngradiente_ac = tensor(pontos)\n\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(1, 2, 1, projection='3d')\nax.set_title('Loss em fun\u00e7\u00e3o dos\\npesos \"a\" e \"b\"', fontsize=10)\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_zticklabels([])\nplt.xlabel(\"a\")\nplt.ylabel(\"b\")\nax.set_zlabel(\"loss    \")\nax.plot_trisurf(gradiente_ab[:,0], gradiente_ab[:,1], gradiente_ab[:,2],\n                cmap='viridis', edgecolor='none');\n\nax = fig.add_subplot(1, 2, 2, projection='3d')\nax.set_title('Loss em fun\u00e7\u00e3o dos\\npesos \"a\" e \"c\"', fontsize=10)\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_zticklabels([])\nplt.xlabel(\"a\")\nplt.ylabel(\"c\")\nax.set_zlabel(\"loss    \")\nax.plot_trisurf(gradiente_ac[:,0], gradiente_ac[:,1], gradiente_ac[:,2],\n                cmap='viridis', edgecolor='none');","1443012c":"geradorGIF = GeradorGIF(\".\/\", \"aprendizado_completo\")\n\nhist = passos_treinamento\nfor i in range(len(hist)):\n    fig = plt.figure(figsize=plt.figaspect(0.5))\n    \n    #modelo\n    ax = fig.add_subplot(1, 3, 1)\n    ax.set_title('Predi\u00e7\u00e3o x Dados de treinamento', fontsize=10)\n    ax.scatter(treino[0], treino[1])\n    ax.plot(treino[0], f(treino[0], hist[i]).detach().numpy(), color='red')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_ylim(0,150)\n    plt.ylabel(\"Vari\u00e1vel dependente\")\n    t = plt.xlabel(\"Vari\u00e1veis independentes\")\n        \n    #gradiente a, b\n    ax = fig.add_subplot(1, 3, 2, projection='3d')  \n    ax.set_title('Loss em fun\u00e7\u00e3o dos\\npesos \"a\" e \"b\"', fontsize=8)\n    ax.plot_trisurf(gradiente_ab[:,0], gradiente_ab[:,1], gradiente_ab[:,2],\n                    cmap='viridis', edgecolor='none', alpha = 0.5);\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_zticklabels([])\n    plt.xlabel(\"a\")\n    plt.ylabel(\"b\")\n    ax.set_zlabel(\"loss    \")\n    ax.view_init(0, -30)\n    ax.scatter(float(hist[i][0]), float(hist[i][1]), lossparam(hist[i]), color='red')\n    \n    #gradiente a, c\n    ax = fig.add_subplot(1, 3, 3, projection='3d') \n    ax.set_title('Loss em fun\u00e7\u00e3o dos\\npesos \"a\" e \"c\"', fontsize=8)\n    ax.plot_trisurf(gradiente_ac[:,0], gradiente_ac[:,1], gradiente_ac[:,2],\n                    cmap='viridis', edgecolor='none', alpha = 0.5);\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_zticklabels([])\n    plt.xlabel(\"a\")\n    plt.ylabel(\"c\")\n    ax.set_zlabel(\"loss    \")\n    ax.view_init(0, -30)\n    ax.scatter(float(hist[i][0]), float(hist[i][2]), lossparam(hist[i]), color='red')\n    \n    geradorGIF.add(fig)\n    plt.close()\n    \ngeradorGIF.finish()\n\ngeradorGIF.show()","279098c5":"\u00d3timo! Estamos na dire\u00e7\u00e3o certa. Ent\u00e3o qualquer que tenha sido a altera\u00e7\u00e3o de par\u00e2metros de $f1$ para $f2$, se eu tentar a mesma coisa novamente, \u00e9 bem poss\u00edvel que consiga resultados ainda melhores. Se, por outro lado, em um passo desses a mudan\u00e7a resultar em um erro maior, eu posso tentar uma mudan\u00e7a de par\u00e2metros em outra dire\u00e7\u00e3o ou posso tamb\u00e9m tentar uma mudan\u00e7a de par\u00e2metros menor. \n\nEsse m\u00e9todo descrito, apesar de funcionar, n\u00e3o \u00e9 muito eficiente. As mudan\u00e7as de par\u00e2metro acontecem sem um crit\u00e9rio definido e diversos desses passos intermedi\u00e1rios (tentativas de otimiza\u00e7\u00e3o) s\u00e3o infrut\u00edferos.\n\nVoltando ao exemplo da montanha, existe uma estrat\u00e9gia melhor de descer do que a que foi apresentada. A pessoa pode fazer a descida apenas sentindo o solo abaixo de seus p\u00e9s. Sempre que ela nota que a encosta est\u00e1 inclinada em determinada dire\u00e7\u00e3o, ela d\u00e1 um pequeno passo nessa dire\u00e7\u00e3o e repete o processo. Idealmente, depois de v\u00e1rios passos ela ter\u00e1 chegado ao ponto mais baixo da montanha. \n\nTrazendo essa ideia para a quest\u00e3o da redu\u00e7\u00e3o do erro, chegamos ao que chamados de Gradiente Descendente. A forma matem\u00e1tica de saber a inclina\u00e7\u00e3o de uma fun\u00e7\u00e3o \u00e9 calcular a sua derivada. E a forma matem\u00e1tica de dar um passo \u00e9 fazer um pequeno incremento na dire\u00e7\u00e3o da derivada.\n\nSe voc\u00ea n\u00e3o sabe calcular a derivada, n\u00e3o precisa se preocupar com isso. Esse \u00e9 um c\u00e1lculo feito de maneira trasparente pelas bibliotecas de aprendizado de m\u00e1quina. Na maioria das vezes s\u00f3 precisamos utilizar essas ferramentas como uma caixa preta. Entretanto, pelo menos o conceito de derivada \u00e9 importante para entender a descida de gradiente.\n\nComo podemos ver na figura a seguir, a derivada \u00e9 a inclina\u00e7\u00e3o da reta que tangencia a curva em determinado ponto. Significa que naquele ponto espec\u00edfico da curva, ela tem exatamente a mesma inclina\u00e7\u00e3o da reta encontrada. Al\u00e9m disso, quanto maior o valor num\u00e9rico da derivada, mais inclinada \u00e9 a reta. ","9e4890f1":"E podemos perceber que a solu\u00e7\u00e3o est\u00e1 um pouco mais pr\u00f3xima dos pontos de treinamento. *Obs.: O passo foi pequeno, ent\u00e3o a mudan\u00e7a \u00e9 pequena tamb\u00e9m.*","e6348940":"Como foi explicado antes, para a maioria dos problemas reais, \u00e9 impratic\u00e1vel calcular toda a curva de **Loss** em fun\u00e7\u00e3o dos par\u00e2metros poss\u00edveis. Mas apenas para fins did\u00e1ticos iremos fazer isso aqui.\n\nNote que os par\u00e2metros poss\u00edveis s\u00e3o tr\u00eas (a, b e c), al\u00e9m do pr\u00f3prio valor de **Loss**. Dessa forma temos um espa\u00e7o de quatro dimens\u00f5es, de modo que um gr\u00e1fico que o represente tamb\u00e9m \u00e9 impratic\u00e1vel. \n\nSendo assim, mostraremos o espa\u00e7o de quatro dimens\u00f5es (a, b, c, loss) a partir de dois gr\u00e1ficos tridimensionais (a, b, loss) e (a, c, loss). Esses gr\u00e1ficos s\u00e3o a \"montanha\" a ser descida. Observe tamb\u00e9m que os dois gr\u00e1ficos a seguir representam a mesma montanha. S\u00f3 foram desmembrados para tornar a visualiza\u00e7\u00e3o pratic\u00e1vel.","45c72950":"A pr\u00f3xima situa\u00e7\u00e3o tamb\u00e9m mostra um passo grande, um pouco menor que o primeiro exemplo. O modelo n\u00e3o converge para uma solu\u00e7\u00e3o ou demora muito tempo para isso.","960e364c":"Agora podemos fazer a descida de gradiente utilizando a seguinte f\u00f3rmula. \n\n$Novos Pesos = Pesos Atuais - Derivada Pesos Atuais*Taxa Aprendizado$\n\nEnt\u00e3o um passo de otimiza\u00e7\u00e3o do erro seria um pequeno incremento com base nos pesos atuais e na dire\u00e7\u00e3o da derivada. Esse pequeno incremento deve ser proporcional \u00e0 taxa de aprendizado, que costuma ser algo entre 0,001 e 0,1, mas pode ser qualquer valor. No meu exemplo vou usar 0,001 para que o passo seja bem pequeno.","293fdbba":"Bem, espero que voc\u00eas tenham gostado desse artigo. Para mim essas visualiza\u00e7\u00f5es foram de grande utilidade para entender e fixar os conceitos de Gradiente Descendente, Loss e taxa de aprendizado. ","50061f02":"J\u00e1 o exemplo abaixo \u00e9 o contr\u00e1rio. Temos um passo muito pequeno. Nesse caso, apesar do modelo estar sempre seguindo na dire\u00e7\u00e3o correta, ele demora muito tempo para chegar a uma solu\u00e7\u00e3o. ","425e32c0":"O problema \u00e9 que para tra\u00e7ar esse gr\u00e1fico eu teria que calcular o erro para cada poss\u00edvel solu\u00e7\u00e3o candidata. Na pr\u00e1tica isso \u00e9 impratic\u00e1vel em problemas reais j\u00e1 que o espa\u00e7o de poss\u00edvieis solu\u00e7\u00f5es pode ser imenso.\n\nEnt\u00e3o o que pode ser feito na pr\u00e1tica?\n\nO nosso problema \u00e9 de encontar a fun\u00e7\u00e3o com menor erro, ou em outras palavras, o ponto m\u00ednimo de uma curva de erro. A dificuldade \u00e9 que n\u00e3o conhecemos a curva de erro, j\u00e1 que \u00e9 impratic\u00e1vel calcular todos os pontos dela. \n\nEssa situa\u00e7\u00e3o \u00e9 semelhante a algu\u00e9m que est\u00e1 em algum ponto de uma montanha e quer chegar ao ponto mais baixo do vale. S\u00f3 que essa pessoa est\u00e1 com os olhos vendados e n\u00e3o conhece o caminho para a parte de baixo. \n\nAinda no exemplo da montanha, essa pessoa poderia chegar \u00e0 parte de baixo usando uma estrat\u00e9gia simples. Daria um passo em uma dire\u00e7\u00e3o qualquer e depois perceberia se isso a fez subir ou descer. Caso ela tenha descido, dar\u00e1 um passo na mesma dire\u00e7\u00e3o e perceber\u00e1 novamente se continua descendo. Por outro lado, caso ela d\u00ea um passo e perceba que acabou subindo, voltar\u00e1 para onde estava antes e tentar\u00e1 dar um passo em outra dire\u00e7\u00e3o. Depois de diversos passo, idas e voltas, ela ter\u00e1 chegado \u00e0 parte de baixo. \n\nTrazendo esse conceito para o caso de otimiza\u00e7\u00e3o do erro da fun\u00e7\u00e3o, uma alternativa \u00e9 come\u00e7ar com uma solu\u00e7\u00e3o qualquer, como fizemos acima. Seria a nossa $f1$ e calcular o erro dela. Note que agora n\u00e3o temos mais a op\u00e7\u00e3o de ter um gr\u00e1fico mapeando todas as poss\u00edveis solu\u00e7\u00f5es candidatas em nosso problema de otimiza\u00e7\u00e3o, mas ele ser\u00e1 mantido com linha pontilhada para sinalizar que \u00e9 apenas para fins did\u00e1tico. Durante o aprendizado de m\u00e1quina, tudo o que teremos s\u00e3o os pontos vermelhos.","76baa2f4":"Utilizando a mesma fun\u00e7\u00e3o **Loss** j\u00e1 definida acima, podemos chegar ao erro m\u00e9dio dessa solu\u00e7\u00e3o.","992c6df5":"<img src=\"https:\/\/github.com\/jrodrigosousa\/blog\/blob\/master\/input\/recursos-para-artigo-de-gradiente-descendente\/aprendizado_completo.gif?raw=true\">","e8b1ad1b":"Bem, realmente o erro de $f2$ foi menor do que o erro de $f1$, mas o que foi feito na segunda tentativa para que isso acontecesse? \u00c9 o que vamos explicar a seguir.","0a79b811":"Ent\u00e3o, para finalizar, podemos colocar tudo isso dentro de uma fun\u00e7\u00e3o python para ir executando passos gradativos e verificando a otimiza\u00e7\u00e3o do modelo.","a66ccf67":"J\u00e1 se o tamanho do passo for o ideal, o modelo converge para uma solu\u00e7\u00e3o em um tempo razo\u00e1vel.","c9eabee3":"<img src=\"https:\/\/github.com\/jrodrigosousa\/blog\/blob\/master\/input\/recursos-para-artigo-de-gradiente-descendente\/lr_muito_pequena.gif?raw=true\" width=\"500px\">","24de9d59":"Ent\u00e3o agora que eu j\u00e1 consigo medir o quanto a minha proposta de solu\u00e7\u00e3o est\u00e1 errada, vamos a uma segunda tentativa. Vamos supor que a minha segunda tentativa de solu\u00e7\u00e3o $f2$ d\u00ea os resultados abaixo para os dados de treinamento. *Obs.: Por enquanto n\u00e3o vamos nos preocupar com a f\u00f3rmula de $f2$ ou que mudan\u00e7as ela tem com rela\u00e7\u00e3o a $f1$. Vamos entrar nesses detalhes adiante.*\n\n$f2(centro, 50, 2) = 4500$\n\n$f2(zona\\_norte, 65, 3) = 50$\n\n$f2(praias, 30, 1) = 10$\n\nNesse caso podemos calcular o erro e ver se o resultado foi melhor ou n\u00e3o:","3ab03c47":"O aprendizado de m\u00e1quina (machine learning) \u00e9 baseado fundamentalmente na capacidade em que as m\u00e1quinas t\u00eam de encontrar padr\u00f5es em dados durante uma etapa de treinamento. Nesse treinamento, os algoritmos de aprendizado refinam o seu entendimento sobre os padr\u00f5es inerentes a esses dados, ou seja, eles aprendem com a experi\u00eancia, obtendo cada vez melhores resultados em suas predi\u00e7\u00f5es. \n\nMas como \u00e9 que esses algoritmos conseguem aprender ao longo do tempo?\n\nO cora\u00e7\u00e3o desse aprendizado \u00e9 o Gradiente Descendente, que \u00e9 uma t\u00e9cnica de otimiza\u00e7\u00e3o que permite encontrar par\u00e2metros cada vez melhores para um modelo. Ao final do processo esse modelo encontraria uma boa aproxima\u00e7\u00e3o com rela\u00e7\u00e3o aos dados oferecidos.\n\nNesse artigo eu vou tentar explicar o conceito do Gradiente Descendente de uma maneira intuitiva e com o m\u00ednimo de matem\u00e1tica poss\u00edvel. Fa\u00e7o isso porque acredito que, em um primeiro momento, o mais importante seja a forma\u00e7\u00e3o de um entendimento de alto n\u00edvel sobre o assunto e os mecanismos que servem de base \u00e0 t\u00e9cnica. A matem\u00e1tica \u00e9 um segundo passo para aqueles que quiserem aprofundar, mas \u00e9 totalmente opcional para aqueles que se interessam apenas em realizar treinamento de modelos de machine learning. \n\nNo gr\u00e1fico abaixo, temos \u00e0 esquerda um processo de treinamento onde o algoritmo progressivamente aprende a fazer melhores predi\u00e7\u00f5es com rela\u00e7\u00e3o aos dados de treinamento. \u00c0 direita a representa\u00e7\u00e3o do Gradiente Descendente utilizado para obter esses resultados. Ao final desse artigo voc\u00ea ser\u00e1 capaz de endender o processo de treinamento representado por essa figura, mas, al\u00e9m disso, tamb\u00e9m ser\u00e1 capaz de implementar esse processo com suas pr\u00f3prias m\u00e3os. \n\n<img src=\"https:\/\/github.com\/jrodrigosousa\/blog\/blob\/master\/input\/recursos-para-artigo-de-gradiente-descendente\/aprendizado_completo.gif?raw=true\">\n\nAbaixo uma listagem dos pontos que iremos abordar:\n\n* A fun\u00e7\u00e3o Loss\n* O Gradiente Descendente\n* Taxa de aprendizado (Learning Rate)\n* Uma implementa\u00e7\u00e3o simples e pr\u00e1tica do Gradiente Descendente","5fe1e893":"Depois, mudamos um pouco os par\u00e2metros da tentativa chegando a $f2$ e verificamos se o resultado foi melhor ou n\u00e3o. *Obs.: Ainda estamos em um exemplo conceitual. Adiante veremos como podemos mudar os par\u00e2metros entre as tentativas.*","a5d8eba7":"Nesse contexto e avaliando os dados de treinamento, eu resolvi que uma fun\u00e7\u00e3o quadr\u00e1tica ($a.x^2+b.x+c$) ser\u00e1 uma boa aproxima\u00e7\u00e3o para o meu modelo. Perceba que a defini\u00e7\u00e3o da fun\u00e7\u00e3o \u00e9 uma condi\u00e7\u00e3o pr\u00e9via, n\u00e3o fazendo parte do processo de treinamento.\n\nMas note que a minha fun\u00e7\u00e3o $f$ \u00e9 parametrizada n\u00e3o apenas com os meus dados de entrada (x = quantidade de comida), mas com alguns pesos (a, b e c). O objetivo desses pesos \u00e9 que eles possam ser ajustados ao longo do processo de treinamento de modo que a fun\u00e7\u00e3o resultante tamb\u00e9m se aproxime da solu\u00e7\u00e3o desejada.","58b0ec42":"<img src=\"https:\/\/github.com\/jrodrigosousa\/blog\/blob\/master\/input\/recursos-para-artigo-de-gradiente-descendente\/aprendizado.gif?raw=true\" width=\"500px\">","befcbac5":"Vamos ver a qualidade da predi\u00e7\u00e3o de **f** considerando os pesos aleat\u00f3rios inicialmente definidos. No gr\u00e1fico a seguir os dados de treinamento s\u00e3o os pontos em azul e a linha em vermelho \u00e9 a predi\u00e7\u00e3o inicial.","f94919c6":"Como resultado podemos ver a medida que o aprendizado evolui, a predi\u00e7\u00e3o do nosso modelo (linha vermelha) est\u00e1 mais pr\u00f3xima dos dados de treinamento (pontos azuis).","584654fc":"# Implementando o Gradiente Descendente\n\nPara essa segunda parte da explica\u00e7\u00e3o, vamos tomar um exemplo diferente. Vamos supor que estou estudando a taxa de incid\u00eancia de doen\u00e7as em uma popula\u00e7\u00e3o de ratos e a rela\u00e7\u00e3o dessa taxa com a alimenta\u00e7\u00e3o dos mesmos. \n\nNesse caso, ap\u00f3s algumas medi\u00e7\u00f5es eu chego no seguinte conjunto de dados de treinamento:","33a05a23":"\u00c9, parece que os resultados n\u00e3o foram muito bons. Mas o quanto ser\u00e1 que eles est\u00e3o errados? \u00c9 a\u00ed que entra a fun\u00e7\u00e3o **Loss**. Com ela podemos definir numericamente o valor desse erro. \n\nExistem diversas formas de definir a fun\u00e7\u00e3o **Loss**, mas uma forma muito comum \u00e9 utilizar o Erro Absoluto M\u00e9dio, que \u00e9 a m\u00e9dia da dist\u00e2ncia entre os valores dos dados de treinamento e da previs\u00e3o. \n\n> Exemplo:\n>\n> Para o meu dado de entrada (\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c,50,2), o resultado de $f$ \u00e9 300 (conforme dados de treinamento) e o resultado de $f1$ \u00e9 5000 (conforme mostrado acima). Nesse caso a dist\u00e2ncia dos resultados \u00e9 $5000-300 = 4700$. Para o segundo ponto, a dist\u00e2ncia \u00e9 $20-400=-380$ e para o terceiro \u00e9 $5-120=115$. \n> O Erro Absoluto M\u00e9dio (MAE) \u00e9 a m\u00e9dia desses valores absolutos (positivos): $MAE = (4700+380+115)\/3 = 1731,66$\n\nVamos definir a nossa fun\u00e7\u00e3o **Loss** e calcular o erro para $f1$.","4946d96c":"Ok, ent\u00e3o nessa situa\u00e7\u00e3o, mesmo sem saber qual \u00e9 a solu\u00e7\u00e3o para o meu problema (a fun\u00e7\u00e3o $f$), eu posso saber o quanto uma solu\u00e7\u00e3o proposta est\u00e1 errada (**Loss**) comparando o quanto os resultados dessa minha solu\u00e7\u00e3o s\u00e3o diferentes daqueles encontrados no meu cojundo de dados de treinamento. Vamos tentar dessa forma.\n\nA minha primeira tentativa de solu\u00e7\u00e3o ser\u00e1 $f1$ e vamos supor que ela d\u00ea os resultados abaixo para os dados de treinamento. *Obs.: No momento n\u00e3o vamos nos preocupar com a f\u00f3rmula de $f1$. Estamos s\u00f3 dando um exemplo de alto n\u00edvel.*\n\n$f1(centro, 50, 2) = 5000$\n\n$f1(zona\\_norte, 65, 3) = 20$\n\n$f1(praias, 30, 1) = 5$","fc3f42ae":"# Taxa de aprendizagem (Learning Rate)\n\nMas qual o tamanho do passo para encontar uma nova tentativa? Isso tem alguma import\u00e2ncia?\n\nNo exemplo abaixo podemos ver que um passo muito grande pode levar a erros cada vez maiores. O ponto vermelho, que representa a minha tentativa de solu\u00e7\u00e3o, fica cada vez mais longe da parte de baixo do gr\u00e1fico, onde o erro \u00e9 o m\u00ednimo poss\u00edvel. \n\nIsso acontece porque o movimento est\u00e1 na dire\u00e7\u00e3o correta, mas passa (e muito) o ponto m\u00ednimo da curva. De acordo com o gradiente do ponto, quando ele est\u00e1 no lado direito, ele sabe que deve ir para a esquerda, mas quando faz isso ele faz um deslocamento muito grande e chega at\u00e9 o lado esquerdo do gr\u00e1fico em um erro maior do que estava antes. Depois disso ele tenta corrigir, voltando para a direita, mas novamente d\u00e1 um passo muito grande e aumenta o erro ainda mais. E o ciclo continua sempre aumentando o erro a cada nova tentativa.","f822cffe":"Inicialmente eu n\u00e3o sei quais os melhores valores para os pesos. Na verdade, o processo de treinamento consiste exatamente em encontrar esses valores a partir da experi\u00eancia. Ao longo dos v\u00e1rios ciclos de treinamento, vou testar novos valores para os pesos (a, b e c), gerar novas fun\u00e7\u00f5es $f1$, $f2$, $f3$... at\u00e9 que o meu erro seja o m\u00ednimo poss\u00edvel. \n\nSendo assim, os valores iniciais para os pesos podem ser aleat\u00f3rios, j\u00e1 que ser\u00e3o refinados ao longo do processo.","f3154aad":"Ok, agora que j\u00e1 sei qual a inclina\u00e7\u00e3o da curva de erro para $f1$, eu posso escolher com mais facilidade em qual dire\u00e7\u00e3o darei um passo para encontrar $f2$. Depois de encontrar $f2$ \u00e9 s\u00f3 repetir o processo quantas vezes forem necess\u00e1rias at\u00e9 que n\u00e3o seja mais poss\u00edvel encontrar outro ponto de erro menor que o atual. ","fb30907b":"Para finalizar, colocaremos tudo junto em um \u00fanico gr\u00e1fico onde ser\u00e1 poss\u00edvel acompanhar o processo de aprendizado. \n\n\u00c0 direita vemos a evolu\u00e7\u00e3o da qualidade da predi\u00e7\u00e3o com rela\u00e7\u00e3o aos dados de treinamento. \n\n\u00c0 esquerda vemos a descida de gradiente. Como j\u00e1 foi explicado antes, a superf\u00edcie colorida \u00e9 a representa\u00e7\u00e3o do erro para cada conjunto de pesos poss\u00edvel. O ponto vermelho \u00e9 a escolha de valores para os pesos (a, b e c) que est\u00e1 sendo testada no momento e est\u00e1 gerando a linha vermelha \u00e0 direita.\n\nObserve que \u00e0 medida em que o ponto vermelho desce, a curva fica mais pr\u00f3xima dos dados de treinamento.","33573ebc":"Dessa forma, supondo que os dados de treinamento est\u00e3o corretos e sem ru\u00eddo, temos que:\n\n$f(centro, 50, 2) = 300$\n\n$f(zona\\_norte, 65, 3) = 400$\n\n$f(praias, 30, 1) = 120$","a1e51699":"<img src=\"https:\/\/github.com\/jrodrigosousa\/blog\/blob\/master\/input\/recursos-para-artigo-de-gradiente-descendente\/lr_muito_grande.gif?raw=true\" width=\"500px\">","b18651b3":"E finalmente colocamos os passos dentro de um loop para que seja executado diversas vezes.","e9754915":"Conforme vemos logo acima, chegamos a um valor que representa o erro de $f1$ com rela\u00e7\u00e3o a $f$, mesmo sem levar em considera\u00e7\u00e3o as f\u00f3rmulas de uma ou outra fun\u00e7\u00e3o. Conseguimos isso apenas comparando os resultados produzidos por cada uma e verificando o quanto s\u00e3o diferentes.\n\nMas normalmente n\u00e3o \u00e9 necess\u00e1rio definir a sua pr\u00f3pria fun\u00e7\u00e3o Loss, porque as bibliotecas de aprendizado de m\u00e1quina j\u00e1 disponibilizam diversas possibilidades. Abaixo podemos ver como chegar ao mesmo valor com a sklearn.","3674945f":"# Gradiente Descendente - Conceito\n\nPerceba que para cada solu\u00e7\u00e3o candidata eu tenho um valor de erro diferente, de modo que seria poss\u00edvel tra\u00e7ar um gr\u00e1fico dos valores de erro em fun\u00e7\u00e3o das solu\u00e7\u00f5es candidatas. Imagine que fiz\u00e9ssemos isso para o exemplo do pre\u00e7o dos im\u00f3veis e cheg\u00e1ssemos ao gr\u00e1fico abaixo. \n\nNele \u00e9 poss\u00edvel perceber que existe uma determinada solu\u00e7\u00e3o que tem um erro menor do que todas as outras (o ponto mais baixo da curva). Essa seria a solu\u00e7\u00e3o \u00f3tima para o problema. ","bc7d69af":"# Estimando o erro de uma solu\u00e7\u00e3o (**Loss**)\n\nImagine que voc\u00ea tenha um problema a ser resolvido, mas n\u00e3o sabe ainda exatamente qual \u00e9 a solu\u00e7\u00e3o. Imagine tamb\u00e9m que tudo o que voc\u00ea sabe no momento \u00e9 o quanto voc\u00ea est\u00e1 errado sobre alguma tentativa de solu\u00e7\u00e3o que puder criar. Se voc\u00ea tiver uma forma de medir esse erro numericamente, pode ir realizando diversas tentativas de solu\u00e7\u00e3o at\u00e9 encontrar alguma que esteja menos errada que todas as outras, n\u00e3o \u00e9? Por hora vamos chamar de **Loss** essa forma de calcular o quanto uma solu\u00e7\u00e3o est\u00e1 errada.\n\nMas se eu n\u00e3o sei qual a solu\u00e7\u00e3o correta, como eu posso saber o quanto as minhas propostas de solu\u00e7\u00e3o est\u00e3o erradas?\n\nEm machine learning, o seu problema normalmente \u00e9 encontrar alguma forma de saber um resultado $y$ a partir de um conjunto de dados de entrada $X$. Por exemplo, podemos estar interessados em encontrar o pre\u00e7o de venda de um im\u00f3vel a partir de algumas informa\u00e7\u00f5es que dispomos sobre ele como localiza\u00e7\u00e3o, tamanho e n\u00famero de quartos.\n\nA representa\u00e7\u00e3o do meu problema poderia ser escrita assim:\n\n<center>\n    $f(localizacao, tamanho, numero\\_quartos) = preco\\_venda$\n<\/center>\n\nou\n\n<center>\n    $f(X) = y$\n    <br>\n    para $X = localizacao, tamanho, numero\\_quartos$     e     $y = preco\\_venda$\n    <br>\n    <br>\n<\/center>\n\nE na verdade, o seu problema de machine learning \u00e9 encontrar essa fun\u00e7\u00e3o $f$ capaz de transformar as entradas em um resultado correto.\n\nNormalmente tamb\u00e9m est\u00e3o dispon\u00edveis um grande n\u00famero de dados de treinamento:","1bda5a7b":"Se calcularmos novamente o erro, iremos perceber que est\u00e1 um pouco menor. ","7e50d03a":"<img src=\"https:\/\/github.com\/jrodrigosousa\/blog\/blob\/master\/input\/recursos-para-artigo-de-gradiente-descendente\/lr_ideal.gif?raw=true\" width=\"500px\">","cc738f6e":"<img src=\"https:\/\/github.com\/jrodrigosousa\/blog\/blob\/master\/input\/recursos-para-artigo-de-gradiente-descendente\/lr_grande.gif?raw=true\" width=\"500px\">"}}