{"cell_type":{"3ca191bc":"code","9ce6634e":"code","2e80578c":"code","194527aa":"code","b29d3681":"code","a8b6a3a8":"code","73ee21b0":"code","daeb76f7":"code","4d4b6346":"code","d6380457":"code","517bdd3d":"code","05712ec9":"code","0bea561e":"code","e664eb44":"code","a1561005":"code","066d5a2e":"code","a8646457":"code","6f2042eb":"code","a83fbc81":"code","5369f527":"code","feebbdb0":"code","2ce43dd4":"code","8bb5c822":"code","66bb22bb":"code","4d667546":"code","baa8f852":"code","17ca3b3f":"markdown","2aa3df6b":"markdown","579a18c0":"markdown","f9c261c3":"markdown","06d31523":"markdown","ec67847f":"markdown","a1662003":"markdown","0edab518":"markdown","6eacc344":"markdown","55c86137":"markdown","160bbec4":"markdown","a690659f":"markdown","cbd4eb5d":"markdown","86ac3377":"markdown","f805643e":"markdown","6337c86c":"markdown","9a465af6":"markdown","afaa7b53":"markdown","5e464b9c":"markdown","c5a348f6":"markdown","47400681":"markdown","11a222be":"markdown"},"source":{"3ca191bc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as implt\nfrom PIL import Image \nimport seaborn as sns\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","9ce6634e":"train_dataset='..\/input\/chest-xray-pneumonia\/chest_xray\/train'\ntest_dataset='..\/input\/chest-xray-pneumonia\/chest_xray\/test'\n\ntrain_normal='..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL'\ntrain_pneumonia='..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA'\n\ntest_normal='..\/input\/chest-xray-pneumonia\/chest_xray\/test\/NORMAL'\ntest_pneumonia='..\/input\/chest-xray-pneumonia\/chest_xray\/test\/PNEUMONIA'\n","2e80578c":"# information about train dataset\ncategory_names=os.listdir(train_dataset) # we have 2 classes which are names humans and horses\ntrain_images_number=[]\nfor category in category_names:\n    folder=train_dataset+'\/'+category\n    train_images_number.append(len(os.listdir(folder)))\nsns.barplot(x=train_images_number,y=category_names).set_title('Number of Training Images per Category');","194527aa":"# information about train dataset\ncategory_names=os.listdir(test_dataset) # we have 2 classes which are names humans and horses\ntest_images_number=[]\nfor category in category_names:\n    folder=test_dataset+'\/'+category\n    test_images_number.append(len(os.listdir(folder)))\nsns.barplot(x=test_images_number,y=category_names).set_title('Number of Training Images per Category');","b29d3681":"normal_sample=implt.imread(train_normal+'\/IM-0122-0001.jpeg')\npneumonia_sample=implt.imread(train_pneumonia+'\/person1000_virus_1681.jpeg')\nplt.subplot(1,2,1)\nplt.title('Normal Sample')\nplt.imshow(normal_sample)\nplt.subplot(1,2,2)\nplt.title('Pneumonia Sample')\nplt.imshow(pneumonia_sample)\nplt.show()","a8b6a3a8":"img_size=50\nnormal_test=[]\npneumonia_test=[]\nnormal_train=[]\npneumonia_train=[]\nnormal_labels=np.zeros(1575)\npneumonia_labels=np.ones(4265)\nfor i in os.listdir(train_normal):\n     if os.path.isfile(train_dataset + \"\/NORMAL\/\" + i):\n            normal=Image.open(train_dataset+'\/NORMAL\/'+i).convert('L') # converting grey scale\n            normal=normal.resize((img_size,img_size),Image.ANTIALIAS) # resizing to 50,50\n            normal=np.asarray(normal)\/255 # bit format\n            normal_train.append(normal)\nfor i in os.listdir(train_pneumonia):\n     if os.path.isfile(train_dataset + \"\/PNEUMONIA\/\" + i):\n            pneumonia=Image.open(train_dataset+'\/PNEUMONIA\/'+i).convert('L') # converting grey scale\n            pneumonia=pneumonia.resize((img_size,img_size),Image.ANTIALIAS) # resizing to 50,50\n            pneumonia=np.asarray(pneumonia)\/255 # bit format\n            pneumonia_train.append(pneumonia)\nfor i in os.listdir(test_normal):\n     if os.path.isfile(test_dataset + \"\/NORMAL\/\" + i):\n            normal=Image.open(test_dataset+'\/NORMAL\/'+i).convert('L') # converting grey scale\n            normal=normal.resize((img_size,img_size),Image.ANTIALIAS) # resizing to 50,50\n            normal=np.asarray(normal)\/255 # bit format\n            normal_test.append(normal)\nfor i in os.listdir(test_pneumonia):\n     if os.path.isfile(test_dataset + \"\/PNEUMONIA\/\" + i):\n            pneumonia=Image.open(test_dataset+'\/PNEUMONIA\/'+i).convert('L') # converting grey scale\n            pneumonia=pneumonia.resize((img_size,img_size),Image.ANTIALIAS) # resizing to 50,50\n            pneumonia=np.asarray(pneumonia)\/255 # bit format\n            pneumonia_test.append(pneumonia)\nX1=np.concatenate((normal_train,normal_test),axis=0)\nX2=np.concatenate((pneumonia_train,pneumonia_test),axis=0)\nX=np.concatenate((X1,X2),axis=0)\nY=np.concatenate((normal_labels,pneumonia_labels),axis=0).reshape(X.shape[0],1)\nprint('X shape:',X.shape)\nprint('Y shape:',Y.shape)","73ee21b0":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.15,random_state=42)\nnumber_of_train=X_train.shape[0]\nnumber_of_test=X_test.shape[0]","daeb76f7":"X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)","4d4b6346":"x_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","d6380457":"## 1 -Initializing Parametres\n# So what we need is dimension 2500 that is number of pixels as a parameter for our initialize method(def)\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b","517bdd3d":"## Forward Propagation\n# calculation of z\n#z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","05712ec9":"## 2-Forward Propagation Steps\n# find z = w.T*x+b\n# y_head = sigmoid(z)\n# loss(error) = loss(y,y_head)\n# cost = sum(loss)\ndef forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z) # probabilistic 0-1\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    return cost ","0bea561e":"## 3-Backward Propagation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","e664eb44":"## 4-Updating Parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)","a1561005":"## Prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","066d5a2e":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 2500\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","a8646457":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","6f2042eb":"# Size of layers and initializing parameters weights and bias\ndef initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters","a83fbc81":"def forward_propagation_NN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","5369f527":"def compute_cost_NN(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost","feebbdb0":"# Backward Propagation\ndef backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","2ce43dd4":"# update parameters\ndef update_parameters_NN(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","8bb5c822":"# prediction\ndef predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","66bb22bb":"# 2 - Layer neural network\ndef two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n         # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n         # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n     # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)\n    ","4d667546":"# reshaping\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","baa8f852":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","17ca3b3f":"# 2-Layer Neural Network\n* Size of layers and initializing parameters weights and bias\n* Forward propagation\n* Loss function and Cost function\n* Backward propagation\n* Update Parameters\n* Prediction with learnt parameters weight and bias\n* Create Model","2aa3df6b":"# Create Model\n* Lets put all them together","579a18c0":"* We make prediction.\n* Now lets put them all together.","f9c261c3":"# Loss function and Cost function\n* Loss and cost functions are same with logistic regression\n* Cross entropy function","06d31523":"# Prediction with learnt parameters weight and bias\n* Lets write predict method that is like logistic regression.","ec67847f":"# Train test split","a1662003":"# Add Dataset","0edab518":"# Processing Dataset\n* In order to create image array, I concatenate zero sign and one sign arrays\n* Then I create label array 0 for zero sign images and 1 for one sign images.","6eacc344":"# L Layer Neural Network\n* In this tutorial our model will have 2 hidden layer with 8 and4 nodes, respectively. Because when number of hidden layer and node increase, it takes too much time.","55c86137":"* As you can see, we have 4964 images and each image has 2500 pixels in image train array.\n* Also, we have 876 images and each image has 2500 pixels in image test array.\n* Then lets take transpose for implement Deep Learning Model","160bbec4":"# Logistic Regression-Step by Step","a690659f":"# Backward propagation","cbd4eb5d":"#  Welcome to my notebook .\n\n** I created this notebook for explore Deep Learning Basics Algorithims and implement Machine Learning Models. \n\n** My dataset is about to chest-xray-pneumonia.My first work on Image Dataset.\n\n** If you vote up me I will be happy.\n","86ac3377":"* Now we have 3 dimensional input array (X) so we need to make it flatten (2D) in order to use as input for our first deep learning model.\n* Our label array (Y) is already flatten(2D) so we leave it like that.\n* Lets flatten X array(images array)","f805643e":"* # Forward propagation\n* Forward propagation is almost same with logistic regression.\n* The only difference is we use tanh function and we make all process twice.\n* Also numpy has tanh function. So we do not need to implement it.","6337c86c":"# Update Parameters\n* Updating parameters also same with logistic regression.\n* We actually do alot of work with logistic regression","9a465af6":"* Up to this point we create 2 layer neural network and learn how to implement","afaa7b53":"# Upload Libraries","5e464b9c":"# ANN ","c5a348f6":"* 1 Upload  relevant libraries\n* 2 Add dataset \n* 3 Processing dataset\n* 4 Train test split\n* 5 Logistic Regression\n* 6 Logistic Regreesion with sklearn\n* 7 ANN(Artificial Neural Network)\n* 8 ANN with Keras","47400681":"* Now lets learn how to implement L layer neural network with keras","11a222be":"# Logistic Regression with sklearn"}}