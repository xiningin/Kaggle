{"cell_type":{"920286e9":"code","c51d26b4":"code","2b40b2df":"code","7d28aa89":"code","10893563":"code","ab3dd232":"code","fb523854":"code","fa7f5bb4":"code","b7f85fd6":"code","2dde6575":"code","ff345d9b":"code","7c7a4c6b":"code","f6598591":"code","130c3c54":"code","fbea3216":"code","761f833d":"code","c8efb659":"code","76aa9a6f":"code","a0b2fafe":"code","fbe12555":"code","2ec4017d":"code","04657cbc":"code","3cb6e305":"code","73090901":"code","a5e6321d":"code","9d5943a3":"code","2ff8fcd9":"code","3e5336ac":"code","ce5af51f":"code","c27f44bc":"code","c6e1f3f5":"code","3c912d1c":"code","2f68a117":"code","67ef8e08":"code","3c923c9c":"code","1c6bfbb4":"code","1063482c":"code","da9a3f0a":"code","93688ca3":"code","6868b3b6":"code","cfd74ca9":"code","802f685b":"code","59d1c8af":"code","26c9075a":"code","77a816a9":"code","ee0e809a":"code","ba53a951":"code","cdba9a01":"code","1b136222":"code","3725e55e":"code","38412bf1":"code","5a498cc0":"code","10acb208":"code","4adabe70":"code","ebc58baf":"code","c3d43b47":"code","2bbbf6ef":"code","07f3e638":"code","4f6afaf0":"code","89b08d68":"code","e91c7f22":"code","06f76cc9":"code","512480a9":"code","7b7239c7":"code","18fbc9b8":"markdown","9d5c0043":"markdown","368e6fb8":"markdown","c4a840a7":"markdown","d0d5577e":"markdown","25d13c86":"markdown","a086a808":"markdown","0d8bdfa5":"markdown","897f39d6":"markdown","730d67e7":"markdown","e3082792":"markdown","831972c0":"markdown","4cf5a71e":"markdown","4402b2bd":"markdown","f804ab9e":"markdown","aebda614":"markdown","c4f87cee":"markdown","1ac0f02c":"markdown","42f2dc61":"markdown","5e2f8253":"markdown"},"source":{"920286e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c51d26b4":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk import bigrams\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport string\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\nimport re\nimport pickle","2b40b2df":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain_df.head(10)","7d28aa89":"train_df.info()","10893563":"# find out missing values\nmissing_count = train_df.isnull().sum()\n\nmissing_count","ab3dd232":"missing_perc = train_df.isnull().mean().round(3)*100\npd.DataFrame({'count':missing_count, 'percentage':missing_perc}).sort_values('count', ascending=False)","fb523854":"# keyword and location are also categorical data\ntrain_df['location'].value_counts()","fa7f5bb4":"# looking at the top 10 keywords from both classes\ndisaster = train_df[train_df['target'] == 1]['location'].value_counts()\nnon_disaster = train_df[train_df['target'] == 0]['location'].value_counts()\n\ndis = disaster[:10]\nnon_dis = non_disaster[:10]\n\nfig = plt.figure(figsize=(10,5))\nfig, (sub1, sub2) = plt.subplots(1,2, figsize=(20,10))\n\nsns.barplot(x=dis.values, y=dis.index, ax=sub1, color='red')\nsub1.set(xlabel='counts', ylabel='locations', title='Disaster Tweets')\n\nsns.barplot(x=non_dis.values, y=non_dis.index, ax=sub2, color='blue')\nsub2.set(xlabel='counts', ylabel='locations', title='Non-Disaster Tweets')\n\nplt.show()","b7f85fd6":"train_df['keyword'].value_counts()","2dde6575":"# looking at the top 10 keywords from both classes\ndisaster = train_df[train_df['target'] == 1]['keyword'].value_counts()\nnon_disaster = train_df[train_df['target'] == 0]['keyword'].value_counts()\n\ndis = disaster[:10]\nnon_dis = non_disaster[:10]\n\nfig = plt.figure(figsize=(10,5))\nfig, (sub1, sub2) = plt.subplots(1,2, figsize=(20,10))\n\nsns.barplot(x=dis.values, y=dis.index, ax=sub1, color='red')\nsub1.set(xlabel='counts', ylabel='keyword', title='Disaster Tweets')\n\nsns.barplot(x=non_dis.values, y=non_dis.index, ax=sub2, color='blue')\nsub2.set(xlabel='counts', ylabel='keyword', title='Non-Disaster Tweets')\n\nplt.show()","ff345d9b":"# checking the target class distribution\nsns.set_theme(style='whitegrid')\nsns.countplot(x='target', data=train_df, palette=['blue', 'red'])","7c7a4c6b":"# filling the NaN values in location and keyword\ntrain_df['location'] = train_df['location'].fillna(\"\")\ntrain_df['keyword'] = train_df['keyword'].fillna(\"\")","f6598591":"train_df.isnull().sum()","130c3c54":"# pre-preprocessing functions\nfrom bs4 import BeautifulSoup\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer","fbea3216":"def cleanpunc(sentence):\n    cleaned = [char for char in sentence if char not in string.punctuation]\n    cleaned = ''.join(cleaned)\n    return cleaned\n\ndef decontracted(sentence):\n    # specific\n    sentence = re.sub(r\"won't\", \"will not\", sentence)\n    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n\n    # general\n    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n    return sentence    \n\ndef remove_emoji(sentence):\n    # taken from: https:\/\/www.kaggle.com\/lovroselic\/disaster-ls\n    emoji_pattern = re.compile(\"[\"\n                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                    u\"\\U00002702-\\U000027B0\"\n                    u\"\\U000024C2-\\U0001F251\"\n                    \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', sentence)\n\nstopw = stopwords.words('english')\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess(sentence):\n    sentence = remove_emoji(sentence)\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n    sentence = BeautifulSoup(sentence, 'html.parser').get_text()\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    sentence = \" \".join([stemmer.stem(word) for word in sentence.split()])\n    sentence = \" \".join(lemmatizer.lemmatize(word) for word in sentence.split())\n    sentence = cleanpunc(sentence)\n    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopw)\n    return sentence\n","761f833d":"# combining the keyword field with text\ntrain_df['Text'] = train_df['text']+\" \"+train_df['keyword']","c8efb659":"train_df.head()","76aa9a6f":"train_df.Text = train_df.Text.apply(preprocess)\ntrain_df.Text.head(10)","a0b2fafe":"# storing the pre-processed text as a list\nppt = list(train_df.Text.values)\npickle.dump(ppt, open('ppt.pkl', 'wb'))","fbe12555":"len(ppt)","2ec4017d":"bow = CountVectorizer(max_features=6000, ngram_range=(1,3))\nbow_df = bow.fit_transform(train_df['Text']).toarray()\nbow_df.shape","04657cbc":"bow_X = bow_df\nprint(bow_X.shape)","3cb6e305":"pickle.dump(bow_X, open('bow_X.pkl', 'wb'))","73090901":"tfidf = TfidfVectorizer(max_features=6000, ngram_range=(1,3))\ntfidf_X = tfidf.fit_transform(train_df['Text']).toarray()\ntfidf_X.shape","a5e6321d":"print(tfidf_X.shape)","9d5943a3":"pickle.dump(tfidf_X, open('tfidf_X.pkl', 'wb'))","2ff8fcd9":"hv = HashingVectorizer(alternate_sign=False,n_features=6000, ngram_range=(1,3))\nhv_X = hv.fit_transform(train_df['Text']).toarray()\nhv_X.shape","3e5336ac":"pickle.dump(hv_X, open('hv_X.pkl', 'wb'))","ce5af51f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tqdm import tqdm","c27f44bc":"from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn import metrics","c6e1f3f5":"y = train_df['target'].values\ny.shape","3c912d1c":"# train test split for bow features\nX_trainb, X_testb, y_trainb, y_testb = train_test_split(bow_X, y, test_size=0.25, random_state=42)\nprint(X_trainb.shape, y_trainb.shape)\nprint(X_testb.shape, y_testb.shape)","2f68a117":"# train test split for tfidf features\nX_traint, X_testt, y_traint, y_testt = train_test_split(tfidf_X, y, test_size=0.25, random_state=42)\nprint(X_traint.shape, y_traint.shape)\nprint(X_testt.shape, y_testt.shape)","67ef8e08":"# train test split for hashing vec features\nX_trainh, X_testh, y_trainh, y_testh = train_test_split(hv_X, y, test_size=0.25, random_state=42)\nprint(X_trainh.shape, y_trainh.shape)\nprint(X_testh.shape, y_testh.shape)","3c923c9c":"def get_roc_curve(X_train, y_train, X_test, y_test, model, title):\n    \n    model.fit(X_train, y_train)\n\n    y_pred_tr = model.predict(X_train)\n    y_pred_te = model.predict(X_test)\n    fpr_tr, tpr_tr, _tr = metrics.roc_curve(y_train,  y_pred_tr)\n    fpr_te, tpr_te, _te = metrics.roc_curve(y_test,  y_pred_te)\n    auc_tr_bow_lr = metrics.roc_auc_score(y_train, y_pred_tr)\n    auc_te_bow_lr = metrics.roc_auc_score(y_test, y_pred_te)\n    plt.plot(fpr_tr,tpr_tr,label=\"Train, auc=\"+str(auc_tr_bow_lr))\n    plt.plot(fpr_te,tpr_te,label=\"Test, auc=\"+str(auc_te_bow_lr))\n    plt.legend(loc=4)\n    plt.title(title)\n    plt.show()","1c6bfbb4":"def train_lr(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Function to perform grid search for hyperparam tuning for LR\n    \"\"\"\n    gsc = GridSearchCV(\n        estimator = SGDClassifier(loss='log'),\n        param_grid = {\n            'alpha': [0.00001, 0.00005, 0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 20, 50, 100]\n        }, cv=5, scoring='accuracy', n_jobs=-1)\n    result = gsc.fit(X_train,y_train)\n    best_params = result.best_params_\n    print(f\"The best params obtained are:{best_params}\")\n    print(f\"Training accuracy: {gsc.score(X_train,y_train)}\")\n    print(f\"Test accuracy: {gsc.score(X_test,y_test)}\")\n    best_lr = SGDClassifier(loss='log', alpha=best_params['alpha'])\n    return best_params, best_lr","1063482c":"# training with bow\nprint(\"Training with Bag-of-Words features:\")\nparams, model = train_lr(X_trainb, y_trainb, X_testb, y_testb)","da9a3f0a":"get_roc_curve(X_trainb, y_trainb, X_testb, y_testb, model, \"Bow+LR\")","93688ca3":"# training with tfidf\nprint(\"Training with Tfidf features:\")\nparams, model = train_lr(X_traint, y_traint, X_testt, y_testt)","6868b3b6":"get_roc_curve(X_traint, y_traint, X_testt, y_testt, model, \"tfidf+LR\")","cfd74ca9":"# training with Hashing Vectorizer\nprint(\"Training with HashingVectorizer features:\")\nparams, model = train_lr(X_trainh, y_trainh, X_testh, y_testh)\n\nget_roc_curve(X_trainh, y_trainh, X_testh, y_testh, model, \"Hv+LR\")","802f685b":"def train_svm(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Function to perform grid search for hyperparam tuning for SVM\n    \"\"\"\n    gsc = GridSearchCV(\n        estimator = SGDClassifier(loss='hinge'),\n        param_grid = {\n            'alpha': [0.00001, 0.00005, 0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 20, 50, 100],\n            'early_stopping': [True]\n        }, cv=5, scoring='accuracy', n_jobs=-1)\n    result = gsc.fit(X_train,y_train)\n    best_params = result.best_params_\n    print(f\"The best params obtained are:{best_params}\")\n    print(f\"Training accuracy: {gsc.score(X_train,y_train)}\")\n    print(f\"Test accuracy: {gsc.score(X_test,y_test)}\")\n    best_svm = SGDClassifier(loss='hinge', alpha=best_params['alpha'])\n    return best_params, best_svm","59d1c8af":"# training with bow\nprint(\"Results of SVM:\")\nprint()\nprint(\"Training with Bag-of-Words features:\")\nparams, model = train_svm(X_trainb, y_trainb, X_testb, y_testb)\nget_roc_curve(X_trainb, y_trainb, X_testb, y_testb, model, \"Bow+SVM\")\n\n# training with tfidf\nprint(\"Training with Tfidf features:\")\nparams, model = train_svm(X_traint, y_traint, X_testt, y_testt)\nget_roc_curve(X_traint, y_traint, X_testt, y_testt, model, \"tfidf+SVM\")\n\n# training with Hashing Vectorizer\n# training with tfidf\nprint(\"Training with HashingVectorizer features:\")\nparams, model = train_svm(X_trainh, y_trainh, X_testh, y_testh)\nget_roc_curve(X_trainh, y_trainh, X_testh, y_testh, model, \"Hv+SVM\")","26c9075a":"def train_mnb(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Function to perform grid search for hyperparam tuning for multinomial naive bayes\n    \"\"\"\n    gsc = GridSearchCV(\n        estimator = MultinomialNB(),\n        param_grid = {\n            'alpha': np.logspace(-4, 3, num=25)\n        }, cv=5, scoring='accuracy', n_jobs=-1)\n    result = gsc.fit(X_train,y_train)\n    best_params = result.best_params_\n    print(f\"The best params obtained are:{best_params}\")\n    print(f\"Training accuracy: {gsc.score(X_train,y_train)}\")\n    print(f\"Test accuracy: {gsc.score(X_test,y_test)}\")\n    best_mnb = MultinomialNB(alpha=best_params['alpha'])\n    return best_params, best_mnb","77a816a9":"# training with bow\nprint(\"Results of MNB:\")\nprint()\nprint(\"Training with Bag-of-Words features:\")\nparams, model = train_mnb(X_trainb, y_trainb, X_testb, y_testb)\nget_roc_curve(X_trainb, y_trainb, X_testb, y_testb, model, \"Bow+MNB\")\n\n# training with tfidf\nprint(\"Training with Tfidf features:\")\nparams, model = train_mnb(X_traint, y_traint, X_testt, y_testt)\nget_roc_curve(X_traint, y_traint, X_testt, y_testt, model, \"tfidf+MNB\")\n\n# training with Hashing Vectorizer\nprint(\"Training with HashingVectorizer features:\")\nparams, model = train_mnb(X_trainh, y_trainh, X_testh, y_testh)\nget_roc_curve(X_trainh, y_trainh, X_testh, y_testh, model, \"Hv+MNB\")","ee0e809a":"def train_dt(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Function to perform grid search for hyperparam tuning for Decision Tree Classifier\n    \"\"\"\n    gsc = GridSearchCV(\n        estimator = DecisionTreeClassifier(),\n        param_grid = {\n            'criterion': ['gini', 'entropy'],\n            'max_depth': [2, 4, 10, 20, 35, 50, 75, 100]\n        }, cv=5, scoring='accuracy', n_jobs=-1)\n    result = gsc.fit(X_train,y_train)\n    best_params = result.best_params_\n    print(f\"The best params obtained are:{best_params}\")\n    print(f\"Training accuracy: {gsc.score(X_train,y_train)}\")\n    print(f\"Test accuracy: {gsc.score(X_test,y_test)}\")\n    best_dt = DecisionTreeClassifier(criterion=best_params['criterion'], max_depth=best_params['max_depth'])\n    return best_params, best_dt","ba53a951":"# training with bow\nprint(\"Results of Decision Tree:\")\nprint()\nprint(\"Training with Bag-of-Words features:\")\nparams, model = train_dt(X_trainb, y_trainb, X_testb, y_testb)\nget_roc_curve(X_trainb, y_trainb, X_testb, y_testb, model, \"Bow + Decision Tree\")\n\n# training with tfidf\nprint(\"Training with Tfidf features:\")\nparams, model = train_dt(X_traint, y_traint, X_testt, y_testt)\nget_roc_curve(X_traint, y_traint, X_testt, y_testt, model, \"tfidf+ Decision Tree\")\n\n# training with Hashing Vectorizer\nprint(\"Training with HashingVectorizer features:\")\nparams, model = train_dt(X_trainh, y_trainh, X_testh, y_testh)\nget_roc_curve(X_trainh, y_trainh, X_testh, y_testh, model, \"Hv+ Decision Tree\")","cdba9a01":"def train_rf(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Function to perform grid search for hyperparam tuning for Random Forest Classifier\n    \"\"\"\n    gsc = GridSearchCV(\n        estimator = RandomForestClassifier(),\n        param_grid = {\n            'max_depth': [4, 10, 20, 75, 100],\n            'n_estimators': [10, 50, 100, 200, 300]\n        }, cv=5, scoring='accuracy', n_jobs=-1)\n    result = gsc.fit(X_train,y_train)\n    best_params = result.best_params_\n    print(f\"The best params obtained are:{best_params}\")\n    print(f\"Training accuracy: {gsc.score(X_train,y_train)}\")\n    print(f\"Test accuracy: {gsc.score(X_test,y_test)}\")\n    best_rf = RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'])\n    return best_params, best_rf","1b136222":"# training with bow\nprint(\"Results of Random Forest Classifier:\")\nprint()\nprint(\"Training with Bag-of-Words features:\")\nparams, model = train_rf(X_trainb, y_trainb, X_testb, y_testb)\nget_roc_curve(X_trainb, y_trainb, X_testb, y_testb, model, \"Bow + Random Forest\")\n\n# training with tfidf\nprint(\"Training with Tfidf features:\")\nparams, model = train_rf(X_traint, y_traint, X_testt, y_testt)\nget_roc_curve(X_traint, y_traint, X_testt, y_testt, model, \"tfidf+ Random Forest\")\n\n# training with Hashing Vectorizer\nprint(\"Training with HashingVectorizer features:\")\nparams, model = train_rf(X_trainh, y_trainh, X_testh, y_testh)\nget_roc_curve(X_trainh, y_trainh, X_testh, y_testh, model, \"Hv+ Random Forest\")","3725e55e":"def train_knn(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Function to perform grid search for hyperparam tuning for Random Forest Classifier\n    \"\"\"\n    gsc = GridSearchCV(\n        estimator = KNeighborsClassifier(),\n        param_grid = {\n            'n_neighbors': [2, 5, 10, 20, 35, 50, 100]\n        }, cv=5, scoring='accuracy', n_jobs=-1)\n    result = gsc.fit(X_train,y_train)\n    best_params = result.best_params_\n    print(f\"The best params obtained are:{best_params}\")\n    print(f\"Training accuracy: {gsc.score(X_train,y_train)}\")\n    print(f\"Test accuracy: {gsc.score(X_test,y_test)}\")\n    best_knn = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'])\n    return best_params, best_knn","38412bf1":"# training with bow\nprint(\"Results of K-Nearest Neighbors Classifier:\")\nprint()\nprint(\"Training with Bag-of-Words features:\")\nparams, model = train_knn(X_trainb, y_trainb, X_testb, y_testb)\nget_roc_curve(X_trainb, y_trainb, X_testb, y_testb, model, \"Bow + KNeighbors\")\n\n# training with tfidf\nprint(\"Training with Tfidf features:\")\nparams, model = train_knn(X_traint, y_traint, X_testt, y_testt)\nget_roc_curve(X_traint, y_traint, X_testt, y_testt, model, \"tfidf+ KNeighbors\")\n\n# training with Hashing Vectorizer\nprint(\"Training with HashingVectorizer features:\")\nparams, model = train_knn(X_trainh, y_trainh, X_testh, y_testh)\nget_roc_curve(X_trainh, y_trainh, X_testh, y_testh, model, \"Hv+ KNeighbors\")","5a498cc0":"# making a submission with BOW + LR\nlr_model = SGDClassifier(loss='log', alpha=0.001)\nlr_model.fit(bow_X, y)","10acb208":"test_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_df.head(10)","4adabe70":"# filling the NaN values in location and keyword\ntest_df['location'] = test_df['location'].fillna(\"\")\ntest_df['keyword'] = test_df['keyword'].fillna(\"\")","ebc58baf":"test_df['Text'] = test_df['text']+\" \"+test_df['keyword']\ntest_df.head()","c3d43b47":"test_df.Text = test_df.Text.apply(preprocess)\nprint(test_df.Text.head(10))\n","2bbbf6ef":"test_bow = bow.fit_transform(test_df['Text']).toarray()\ntest_bow.shape","07f3e638":"test_pred = lr_model.predict(test_bow)","4f6afaf0":"sample_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsample_sub.head(10)","89b08d68":"len(sample_sub.target)","e91c7f22":"sample_sub['target'] = test_pred\nsample_sub.head()","06f76cc9":"sample_sub.tail()","512480a9":"sample_sub[sample_sub['target'] == 1]","7b7239c7":"sample_sub.to_csv('submission_lr_bow.csv', index=False)","18fbc9b8":"* Duplicate locations exist like USA and United States.\n* Locations like **USA, United States, UK, London, New York and Canada** are among the top frequently occurring locations in case of both classes of tweets.","9d5c0043":"#### Loading the Dataset:","368e6fb8":"#### 2. SVM","c4a840a7":"### Vectorizing","d0d5577e":"#### 5. Random Forest Classifier","25d13c86":"* Total Number of records = 7613\n* The columns 'keyword' and 'location' contain a number of NaN values","a086a808":"* Best Test Accuracy for Logistic Regression is with: BOW features ~ 0.8067\n* Best Test AUC for Logistic Regression is with: BOW features ~ 0.7965","0d8bdfa5":"#### 4. Decision Tree Classifier","897f39d6":"Keywords like **ruin, deluge, wrecked, siren, fear, explode** occur in non-disaster class, which can be misleading","730d67e7":"#### 3. Multinomial Naive Bayes","e3082792":"## Modelling:","831972c0":"### 3. Using HashingVectorizer","4cf5a71e":"'location' contains 33.3% missing values. Hence removing rows having any NaN value is not an option as it would lead to considerable information loss.","4402b2bd":"* Best Test Accuracy is with BOW features ~ 0.7978\n* Best Test AUC is with BOW features ~ 0.7948\n","f804ab9e":"#### 1. Logistic Regression\n","aebda614":"#### 7. K-Neighbors Classifier","c4f87cee":"### 2. Using Tf-Idf Vectorizer","1ac0f02c":"* Best Test Accuracy is with Tfidf features ~ 0.8067\n* Best Test AUC is with BOW features ~ 0.79 (marginally better than tfidf)","42f2dc61":"### Preprocessing:","5e2f8253":"#### 1. Using Bag-of-Words:"}}