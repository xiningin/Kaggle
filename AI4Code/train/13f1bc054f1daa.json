{"cell_type":{"59b55441":"code","d70581f5":"code","ed02f354":"code","49382ebc":"code","8508c459":"code","c607d4f4":"code","72e8d461":"code","a6c5ad33":"code","2893d3ef":"code","b875f32d":"code","ac1e012a":"code","6067912a":"code","4a197fe0":"code","13b68ab8":"code","e468adaa":"code","8b416b88":"code","1d0adab2":"code","6618be45":"code","e5b805b4":"code","2b1ada96":"code","96fcd95e":"code","feff915b":"code","05170e79":"code","680040c0":"code","563ee63a":"code","a551c0f6":"markdown","2891a90f":"markdown","0acb7edb":"markdown","a2549fe7":"markdown","c07672c4":"markdown","07ac3cda":"markdown","62facc28":"markdown","7e8d5e5d":"markdown","f742a4a3":"markdown","1fc2f2ad":"markdown","06c5c287":"markdown","354b597f":"markdown","d840a42a":"markdown","d4dd8432":"markdown","37c02c8c":"markdown","0b8dda62":"markdown","032825bb":"markdown","1a7e275d":"markdown","c1fc60db":"markdown","f52500a2":"markdown"},"source":{"59b55441":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.metrics import f1_score\nimport graphviz\nfrom sklearn import tree","d70581f5":"test = pd.read_csv('..\/input\/liverpool-ion-switching\/test.csv')\ntrain = pd.read_csv('..\/input\/liverpool-ion-switching\/train.csv')\ntrain.head()","ed02f354":"plt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,train.shape[0],res),train.signal[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()","49382ebc":"plt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,train.shape[0],res),train.open_channels[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Channels Open',size=16); \nplt.title('Training Data Open Channels - 10 batches',size=20)\nplt.show()","8508c459":"for k in range(10):\n    a = int( np.random.uniform(0,train.shape[0]-50000) )\n    b=a+5000; res=10\n    print('#'*25)\n    print('### Random %i to %i'%(a,b))\n    print('#'*25)\n    plt.figure(figsize=(20,5))\n    plt.plot(range(a,b,res),train.signal[a:b][0::res])\n    plt.plot(range(a,b,res),train.open_channels[a:b][0::res])\n    plt.show()","c607d4f4":"plt.figure(figsize=(20,5))\nres = 1000; let = ['A','B','C','D','E','F','G','H','I','J']\nplt.plot(range(0,test.shape[0],res),test.signal[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(21): plt.plot([j*100000,j*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+200000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7,let[k],size=16)\nplt.xlabel('Row',size=16); plt.ylabel('Channels Open',size=16); \nplt.title('Test Data Signal - 4 batches - 10 subsamples',size=20)\nplt.show()","72e8d461":"train2 = train.copy()\n\na=500000; b=600000 # CLEAN TRAIN BATCH 2\ntrain2.loc[train.index[a:b],'signal'] = train2.signal[a:b].values - 3*(train2.time.values[a:b] - 50)\/10.","a6c5ad33":"batch=2; a=500000*(batch-1); b=500000*batch; res=50\nplt.figure(figsize=(20,5))\nplt.plot(range(a,b,res),train.signal[a:b][0::res])\nplt.title('Training Batch 2 with Slant Drift',size=16)\nplt.figure(figsize=(20,5))\nplt.plot(range(a,b,res),train2.signal[a:b][0::res])\nplt.title('Training Batch 2 without Slant Drift',size=16)\nplt.show()","2893d3ef":"def f(x,low,high,mid): return -((-low+high)\/625)*(x-mid)**2+high -low\n\n# CLEAN TRAIN BATCH 7\nbatch = 7; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,-1.817,3.186,325)\n# CLEAN TRAIN BATCH 8\nbatch = 8; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,-0.094,4.936,375)\n# CLEAN TRAIN BATCH 9\nbatch = 9; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,1.715,6.689,425)\n# CLEAN TRAIN BATCH 10\nbatch = 10; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,3.361,8.45,475)","b875f32d":"plt.figure(figsize=(20,5))\nplt.plot(train.time[::1000],train.signal[::1000])\nplt.title('Training Batches 7-10 with Parabolic Drift',size=16)\nplt.figure(figsize=(20,5))\nplt.plot(train2.time[::1000],train2.signal[::1000])\nplt.title('Training Batches 7-10 without Parabolic Drift',size=16)\nplt.show()","ac1e012a":"batch = 1; a = 500000*(batch-1); b = 500000*batch\nbatch = 2; c = 500000*(batch-1); d = 500000*batch\nX_train = np.concatenate([train2.signal.values[a:b],train2.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train2.open_channels.values[a:b],train2.open_channels.values[c:d]]).reshape((-1,1))\n\nclf1s = tree.DecisionTreeClassifier(max_depth=1)\nclf1s = clf1s.fit(X_train,y_train)\nprint('Training model 1s channel')\npreds = clf1s.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\ntree_graph = tree.export_graphviz(clf1s, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal'], class_names = ['0', '1'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph)  ","6067912a":"batch = 3; a = 500000*(batch-1); b = 500000*batch\nbatch = 7; c = 500000*(batch-1); d = 500000*batch\nX_train = np.concatenate([train2.signal.values[a:b],train2.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train2.open_channels.values[a:b],train2.open_channels.values[c:d]]).reshape((-1,1))\n\nclf1f = tree.DecisionTreeClassifier(max_depth=1)\nclf1f = clf1f.fit(X_train, y_train)\nprint('Training model 1f channel')\npreds = clf1f.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\ntree_graph = tree.export_graphviz(clf1f, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal'], class_names = ['0', '1'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph) ","4a197fe0":"batch = 4; a = 500000*(batch-1); b = 500000*batch\nbatch = 8; c = 500000*(batch-1); d = 500000*batch\nX_train = np.concatenate([train2.signal.values[a:b],train2.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train2.open_channels.values[a:b],train2.open_channels.values[c:d]]).reshape((-1,1))\n\nclf3 = tree.DecisionTreeClassifier(max_leaf_nodes=4)\nclf3 = clf3.fit(X_train,y_train)\nprint('Training model 3 channel')\npreds = clf3.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\ntree_graph = tree.export_graphviz(clf3, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal'], class_names = ['0', '1','2','3'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph) ","13b68ab8":"batch = 6; a = 500000*(batch-1); b = 500000*batch\nbatch = 9; c = 500000*(batch-1); d = 500000*batch\nX_train = np.concatenate([train2.signal.values[a:b],train2.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train2.open_channels.values[a:b],train2.open_channels.values[c:d]]).reshape((-1,1))\n\nclf5 = tree.DecisionTreeClassifier(max_leaf_nodes=6)\nclf5 = clf5.fit(X_train, y_train)\nprint('Trained model 5 channel')\npreds = clf5.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\ntree_graph = tree.export_graphviz(clf5, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal'], class_names = ['0', '1','2','3','4','5'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph) ","e468adaa":"batch = 5; a = 500000*(batch-1); b = 500000*batch\nbatch = 10; c = 500000*(batch-1); d = 500000*batch\nX_train = np.concatenate([train2.signal.values[a:b],train2.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train2.open_channels.values[a:b],train2.open_channels.values[c:d]]).reshape((-1,1))\n\nclf10 = tree.DecisionTreeClassifier(max_leaf_nodes=8)\nclf10 = clf10.fit(X_train, y_train)\nprint('Trained model 10 channel')\npreds = clf10.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\ntree_graph = tree.export_graphviz(clf10, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal'], class_names = [str(x) for x in range(11)],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph) ","8b416b88":"# ORIGINAL TRAIN DATA\nplt.figure(figsize=(20,5))\nr = train.signal.rolling(30000).mean()\nplt.plot(train.time.values,r)\nfor i in range(11): plt.plot([i*50,i*50],[-3,8],'r:')\nfor j in range(10): plt.text(j*50+20,6,str(j+1),size=20)\nplt.title('Training Signal Rolling Mean. Has Drift wherever plot is not horizontal line',size=16)\nplt.show()\n\n# TRAIN DATA WITHOUT DRIFT\nplt.figure(figsize=(20,5))\nr = train2.signal.rolling(30000).mean()\nplt.plot(train2.time.values,r)\nfor i in range(11): plt.plot([i*50,i*50],[-3,8],'r:')\nfor j in range(10): plt.text(j*50+20,6,str(j+1),size=20)\nplt.title('Training Signal Rolling Mean without Drift',size=16)\nplt.show()","1d0adab2":"plt.figure(figsize=(20,5))\nlet = ['A','B','C','D','E','F','G','H','I','J']\nr = test.signal.rolling(30000).mean()\nplt.plot(test.time.values,r)\nfor i in range(21): plt.plot([500+i*10,500+i*10],[-3,6],'r:')\nfor i in range(5): plt.plot([500+i*50,500+i*50],[-3,6],'r')\nfor k in range(4): plt.text(525+k*50,5.5,str(k+1),size=20)\nfor k in range(10): plt.text(505+k*10,4,let[k],size=16)\nplt.title('Test Signal Rolling Mean. Has Drift wherever plot is not horizontal line',size=16)\nplt.show()","6618be45":"test2 = test.copy()","e5b805b4":"# REMOVE BATCH 1 DRIFT\nstart=500\na = 0; b = 100000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=510\na = 100000; b = 200000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=540\na = 400000; b = 500000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.","2b1ada96":"# REMOVE BATCH 2 DRIFT\nstart=560\na = 600000; b = 700000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=570\na = 700000; b = 800000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=580\na = 800000; b = 900000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.","96fcd95e":"# REMOVE BATCH 3 DRIFT\ndef f(x):\n    return -(0.00788)*(x-625)**2+2.345 +2.58\na = 1000000; b = 1500000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - f(test2.time[a:b].values)","feff915b":"plt.figure(figsize=(20,5))\nres = 1000\nplt.plot(range(0,test2.shape[0],res),test2.signal[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.title('Test Signal without Drift',size=16)\nplt.show()\n\nplt.figure(figsize=(20,5))\nr = test2.signal.rolling(30000).mean()\nplt.plot(test2.time.values,r)\nfor i in range(21): plt.plot([500+i*10,500+i*10],[-2,6],'r:')\nfor i in range(5): plt.plot([500+i*50,500+i*50],[-2,6],'r')\nfor k in range(4): plt.text(525+k*50,5.5,str(k+1),size=20)\nfor k in range(10): plt.text(505+k*10,4,let[k],size=16)\nplt.title('Test Signal Rolling Mean without Drift',size=16)\nplt.show()","05170e79":"sub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\n\na = 0 # SUBSAMPLE A, Model 1s\nsub.iloc[100000*a:100000*(a+1),1] = clf1s.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n\na = 1 # SUBSAMPLE B, Model 3\nsub.iloc[100000*a:100000*(a+1),1] = clf3.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n\na = 2 # SUBSAMPLE C, Model 5\nsub.iloc[100000*a:100000*(a+1),1] = clf5.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n\na = 3 # SUBSAMPLE D, Model 1s\nsub.iloc[100000*a:100000*(a+1),1] = clf1s.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n\na = 4 # SUBSAMPLE E, Model 1f\nsub.iloc[100000*a:100000*(a+1),1] = clf1f.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n\na = 5 # SUBSAMPLE F, Model 10\nsub.iloc[100000*a:100000*(a+1),1] = clf10.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n\na = 6 # SUBSAMPLE G, Model 5\nsub.iloc[100000*a:100000*(a+1),1] = clf5.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n\na = 7 # SUBSAMPLE H, Model 10\nsub.iloc[100000*a:100000*(a+1),1] = clf10.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n\na = 8 # SUBSAMPLE I, Model 1s\nsub.iloc[100000*a:100000*(a+1),1] = clf1s.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n\na = 9 # SUBSAMPLE J, Model 3\nsub.iloc[100000*a:100000*(a+1),1] = clf3.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\n\n # BATCHES 3 AND 4, Model 1s\nsub.iloc[1000000:2000000,1] = clf1s.predict(test2.signal.values[1000000:2000000].reshape((-1,1)))","680040c0":"plt.figure(figsize=(20,5))\nres = 1000\nplt.plot(range(0,test.shape[0],res),sub.open_channels[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.title('Test Data Predictions',size=16)\nplt.show()","563ee63a":"sub.to_csv('submission.csv',index=False,float_format='%.4f')","a551c0f6":"## Training Data Drift\nWe observe drift whereever the following plot is not a horizontal line. We see drift in batches 2, 7, 8, 9, 10.","2891a90f":"# Remove Training Data Drift\nThis is a demonstration to show slant drift removal. We could also remove the parabolic drift in batches 7, 8, 9, 10 if we wanted. Below we will only train our models with batches 1, 3, 4, 5, 6. But after removing training drift, we can include the data from batches 2, 7, 8, 9, 10 in our training if we want.","0acb7edb":"## 1 Fast Open Channel","a2549fe7":"# Make Five Simple Models\nWe will make one model for each different type of signal we observed above.","c07672c4":"## Reflection\nFrom this plot we can locate the 5 models in action. And we can recognize the added drift. Batch 1 appears to be 5 subsamples where A, B, C, D, E were created by models 1s, 3, 5, 1s, 1f respectively. Model 1s is the model with maximum 1 channel open with low prob. Model 1f is the model with maximum 1 channel open with high prob. And models 3, 5, 10 are models with maximum 3, 5, 10 channels respectively. We observe slant drift in subsamples A, B, E, G, H, I. We observe parabola draft in batch 3. ","07ac3cda":"# Predict Test\n","62facc28":"# Test Data\nLet's display the test data signal","7e8d5e5d":"## 5 Open Channels","f742a4a3":"# Display Test Predictions","1fc2f2ad":"# Load Libraries and Data","06c5c287":"## 3 Open Channels","354b597f":"# Description of Data\nThe training data is recordings in time. At each 10,000th of a second, the strength of the signal was recorded and the number of ion channels open was recorded. It is our task to build a model that predicts the number of open channels from signal at each time step. Furthermore we are told that the data was recorded in batches of 50 seconds. Therefore each 500,000 rows is one batch. The training data contains 10 batches and the test data contains 4 batches. Let's display the number of open channels and signal strength together for each training batch.","d840a42a":"# Correlation Between Signal and Open Channels\nLet's look closely at random intervals of signal and open channels to observe how they relate. We notice that they are highly correlated and move up and down together. Therefore we can probabily predict open channels from the one feature signal. The only complication is the synthetic drift that was added. So we will remove it.","d4dd8432":"## 1 Slow Open Channel","37c02c8c":"## Reflection\nFrom the plots above, it looks like they used 5 different synthetic models. One model produced maximum 1 channel open with low probability (batches 1 and 2). One model produced maximum 1 channel open with high probability (batches 3 and 7). One model produced maximum 3 channels open (batches 4 and 8). One model produced maximum 5 channels open (batches 6 and 9) and one model produced maximum 10 channels open (batches 7 and 10). Furthermore drift was added to batches 7, 8, 9, 10. And the beginning of batch 2.\n\nAccording to the paper [here][1], the data is synthesized. Also \"electrophysiological\" noise and drift were added. Drift is a signal bias causing the signal to no longer be a horizontal line like batches 2, 7, 8, 9, 10 above.\n\n> Data description and dataset construction. Ion channel dwell-times were\nsimulated using the method of Gillespie 43 from published single channel models.\nChannels are assumed to follow a stochastic Markovian process and transition\nfrom one state to the next simulated by randomly sampling from a lifetime\nprobability distribution calculated for each state. Authentic \u201celectrophysiological\u201d\nnoise was added to these events by passing the signal through a patch-clamp\namplifier and recording it back to file with CED\u2019s Signal software via an Axon\nelectronic \u201cmodel cell\u201d. In some datasets additional drift was applied to the final\ndata with Matlab. Two different stochastic gating models, (termed M1 and M2)\nwere used to generate semi-synthetic ion channel data. M1 is a low open probability model from ref. 41 (Fig. 3a, b), typically no more than one ion channel opens\nsimultaneously. Model M2 is from refs. 42,44 and has a much higher open probability (Fig. 3c, d), consequently up to five channels opened simultaneously and there are few instances of zero channels open.\n\n\n[1]: https:\/\/www.nature.com\/articles\/s42003-019-0729-3\n","0b8dda62":"# Remove Test Data Drift","032825bb":"# One Feature Model Scores LB 0.930!\nIn this notebook, we will explore the Kaggle Ion Comp data and explore a one feature model. The LB result of 0.930 is enlightening.\n\nHere we manually remove signal drift. Note that it is better to use machine learning to remove drift, but doing it by hand once allows us to understand its nature and build better models later.","1a7e275d":"## Test Data Drift\nWe observe drift in test subsamples A, B, E, G, H, I and test batch 3.\n","c1fc60db":"# Analyze Test Data Drift\nLet's plot the drift in the training and test data","f52500a2":"## 10 Open Channels"}}