{"cell_type":{"072ebc71":"code","b76f81cc":"code","c444cae5":"code","50816ec8":"code","d7bc7994":"code","1f392994":"code","ea62b805":"code","018c1878":"code","62b48108":"code","430c2ddb":"code","22ee96cf":"code","2463acad":"code","4db1838c":"code","b4bf90f2":"code","10761c72":"code","68071f94":"code","628fe03d":"code","4fed9678":"code","e95f793e":"markdown","8e431f32":"markdown","f0f55e8a":"markdown","6d2696c7":"markdown"},"source":{"072ebc71":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom io import StringIO","b76f81cc":"csv_data = \\\n... \"\"\"A,B,C,D\n... 1.0,2.0,3.0,4.0\n... 5.0,6.0,,8.0\n... 10.0,11.0,12.0\"\"\"\ndf = pd.read_csv(StringIO(csv_data))\ndf","c444cae5":"# We always get Numpy array from Pandas Dataframe\ndf.values","50816ec8":"df.isnull().sum()","d7bc7994":"# Delete Samples\ndf.dropna(axis = 0)","1f392994":"# Delete Columns\ndf.dropna(axis = 1)","ea62b805":"# Parameters of 'dropna' method.\n## When there are NaN values in all column, delete samples.\ndf.dropna(how='all')","018c1878":"## When numbers of values is less than certain standard point, delete samples.\ndf.dropna(thresh=4)","62b48108":"## When there are NaN values in certain columns, delete samples.\ndf.dropna(subset = ['C'])","430c2ddb":"from sklearn.impute import SimpleImputer","22ee96cf":"# We have to input data as numpy array, not pandas dataframe\n## mean by columns\nimr = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nimr = imr.fit(df.values)\nimputed_data = imr.transform(df.values)\nimputed_data","2463acad":"## median by columns\nimr = SimpleImputer(missing_values = np.nan, strategy = 'median')\nimr = imr.fit(df.values)\nimputed_data = imr.transform(df.values)\nimputed_data","4db1838c":"## most frequent by columns\n### It is imputated by first value in such columns when number of frequency of values is same.\nimr = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\nimr = imr.fit(df.values)\nimputed_data = imr.transform(df.values)\nimputed_data","b4bf90f2":"# It can be worked by this way\n## mean by columns\nimputed_data = df.copy()\nimputed_data['C'] = df['C'].fillna(df['C'].mean())\nimputed_data['D'] = df['D'].fillna(df['D'].mean())\nimputed_data","10761c72":"# If you want to imputate by samples, you can do like this\nfrom sklearn.preprocessing import FunctionTransformer\nftr_imr = FunctionTransformer(lambda x: imr.fit_transform(x.T).T, validate = False)\nimputed_data = ftr_imr.fit_transform(df.values)\nimputed_data","68071f94":"# Or like this\ndf_t = df.T.copy()\nimr = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nimr = imr.fit(df_t)\nimputed_data = imr.transform(df_t)\nimputed_data.T","628fe03d":"# You can also handle NaN data by samples like this\nimputed_data = df.copy()\nimputed_data.iloc[1] = imputed_data.iloc[1].fillna(imputed_data.iloc[1].mean())\nimputed_data.iloc[2] = imputed_data.iloc[2].fillna(imputed_data.iloc[2].mean())\nimputed_data","4fed9678":"# If you have very big data, you can use 'for' contruction.\nimputed_data_by_sample = df.T.copy()\nfor i in range(len(imputed_data_by_sample)):\n    imputed_data_by_sample.iloc[i] = imputed_data_by_sample.iloc[i].fillna(imputed_data_by_sample.iloc[i].mean())\n\n    \nimputed_data_by_column = df.copy()\nfor i in range(len(imputed_data_by_column)):\n    imputed_data_by_column.iloc[i] = imputed_data_by_column.iloc[i].fillna(imputed_data_by_column.iloc[i].mean())\n    \n    \nimputed_data_by_sample.T, imputed_data_by_column","e95f793e":"### We can check NaN data by this way.","8e431f32":"### interpolation method","f0f55e8a":"### Delete samples or columns","6d2696c7":"# The ways of handling NaN data\n1. Delete samples or columns that has NaN data.\n    - It is very risky way that can be imposible to analyze stably or lose necessary important imformation.\n2. Predict NaN data by using interpolation method.\n    - Mean\n    - median\n    - most_frequent\n    - constant"}}