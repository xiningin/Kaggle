{"cell_type":{"5791191a":"code","ec7bed25":"code","73074218":"code","c21fbee6":"code","495fe319":"code","c4c2f9ac":"code","a44f7cf6":"code","5fd14691":"code","a7e54637":"code","53f93600":"code","5c853d43":"code","4e92a214":"code","9679ea80":"code","e365a6ec":"code","7e2661df":"code","03b4325a":"code","660dbbf7":"code","1eed257a":"code","0bbd3fad":"code","18c2b75a":"code","2d25cab0":"code","8cd8659a":"code","6d985e2c":"code","2aa90274":"markdown","21213325":"markdown","b00cf0d9":"markdown","98744b90":"markdown","27de3afc":"markdown","64a7c879":"markdown","269acce2":"markdown","36cce85c":"markdown"},"source":{"5791191a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split # train test split package\nfrom sklearn.linear_model import LinearRegression # Linear Regression model\nfrom sklearn.ensemble import RandomForestRegressor # RF Regression\nfrom sklearn.tree import DecisionTreeRegressor # DT Regression\nfrom sklearn.metrics import r2_score, mean_squared_error as mse # r2_score, how much of our independent variable describes the dependent var?\n\n# RMSE of 8 means that there is a chance that our prediction deviates -8 or +8 compared with the actual data- Y pred = 90, Y test = 82 to 98\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec7bed25":"df_ori = pd.read_csv('\/kaggle\/input\/dummy-advertising-and-sales-data\/Dummy Data HSS.csv')\ndf = df_ori.copy()\ndf","73074218":"df = pd.get_dummies(df, drop_first = True)\ndf","c21fbee6":"df = df[['TV', 'Radio', 'Social Media', \n       'Influencer_Mega', 'Influencer_Micro', 'Influencer_Nano', 'Sales']]\ndf","495fe319":"df = df.fillna(df.mean())","c4c2f9ac":"df.info()","a44f7cf6":"df","5fd14691":"x = df.iloc[:,0:-1]\ny = df.iloc[:,-1:]","a7e54637":"x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1)","53f93600":"lrr = LinearRegression()\nlrr.fit(x_train, y_train)","5c853d43":"y_pred = lrr.predict(x_test)","4e92a214":"print(y_test.values)\nprint(\"-----\")\nprint(y_pred)","9679ea80":"r2_score(y_test, y_pred)","e365a6ec":"mse(y_test, y_pred)**0.5","7e2661df":"df","03b4325a":"lrr.coef_","660dbbf7":"lrr.intercept_","1eed257a":"rfr = RandomForestRegressor(max_depth = 16, n_estimators = 5)\nrfr.fit(x_train, y_train)\ny_pred = rfr.predict(x_test) # we UPDATE the Y_pred here, so be careful\nprint(r2_score(y_test, y_pred))\nprint(mse(y_test, y_pred)**0.5)","0bbd3fad":"dtr = DecisionTreeRegressor()\ndtr.fit(x_train, y_train)\ny_pred = dtr.predict(x_test) # we UPDATE the Y_pred here, be careful\nprint(r2_score(y_test, y_pred))\nprint(mse(y_test, y_pred)**0.5)","18c2b75a":"# Print feature importances\nfrom sklearn.inspection import permutation_importance\nprint(\"Feature Importances: \")\npimp = permutation_importance(lrr, x_test, y_test, random_state = 1)\n\nfor i in pimp.importances_mean.argsort()[-10:]:\n    print(x.columns[i], pimp.importances_mean[i])","2d25cab0":"# Print feature importances\nfrom sklearn.inspection import permutation_importance\nprint(\"Feature Importances: \")\npimp = permutation_importance(rfr, x_test, y_test, random_state = 1)\n\nfor i in pimp.importances_mean.argsort()[-10:]:\n    print(x.columns[i], pimp.importances_mean[i])","8cd8659a":"parameters = {\n    \"n_estimators\":[5,10,50,100,250],\n    \"max_depth\":[2,4,8,16,32,None]\n    }\n\nfrom sklearn.model_selection import GridSearchCV\ncv = GridSearchCV(rfr, parameters, cv = 20, n_jobs = -1) # CHANGE THE MODEL HERE OTHER THAN LINEAR REGRESSION\ncv.fit(x_train, y_train)\n\ncv.best_params_\n\n# for my eyes only: see this\n# https:\/\/www.datasciencelearner.com\/how-to-improve-accuracy-of-random-forest-classifier\/\n\ndef display(results):\n    print(f'Best parameters are: {results.best_params_}')\n    print(\"\\n\")\n    mean_score = results.cv_results_['mean_test_score']\n    std_score = results.cv_results_['std_test_score']\n    params = results.cv_results_['params']\n    for mean,std,params in zip(mean_score,std_score,params):\n        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')\n\ndisplay(cv)","6d985e2c":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = lrr, X = x_train, y = y_train, cv = 10, n_jobs = -1)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","2aa90274":"Sales = 3.5 TV + 0.1 Radio + 0.06 Soc Media + 0.5 Mega + 0.4 Micro + 0.4 Nano - 0.30\n\n* For every 1 increase in TV, the sales will increase by 3.5\n* For every 1 increase in Soc Media, the sales will increase only 0.06\n* If we do not spend any promotional budget, our sales is predicted to decrease by - 0.3 units\n\nOur Linear Regression Model results in 99.2% accuracy, 8.2 RMSE, and the above regression equation.","21213325":"Overall conclusion of our model is:\n\n* Linear Regression is the best regression model with average accuracy of 99.36%\n* TV, Radio, and Micro influencers should be applied in our promotional activities - because it has the highest importance\n* We whould not use Social Media and other influencers as it has the lowest number of importance","b00cf0d9":"# Feature Importances\n\nWhat is the most important feature that predict the sales?","98744b90":"Outline:\n\n* Preprocessing\n* Justify X and Y\n* Split Training and Testing Data\n* Fit \/ Train the Data USING MULTIPLE REGRESSION MODELS\n* Evaluate which has the highest R2 and lowest RMSE\n* Try to perform Grid Search CV (if the regression model is NOT Linear Regression)\n* Update the regression model with the best n_estimators and max_depth\n* Try to perform K-Fold Cross Validation\n* Conclude the regression models and provide recommendations based on the given data\n\nP.S. Find dataset that is relevant for performing Regression (PAY ATTENTION ON THE DATA TYPE, PARTICULARLY DEPENDENT VARIABLE MUST BE RATIO\/INTERVAL).","27de3afc":"# REGRESSION\n\n* Justify X and Y\n* Split Training and Testing Data\n* Train (Fit) Data\n* Prediction Linear Regression\n* Evaluate the Results of Linear Regression \n\n----\n\n* Prediction using Random Forest\n* Prediction using Decision Tree\n* Choose the Best Regression Model\n\n----\n\nP.s. We can perform Cross Validation (LATER)","64a7c879":"To avoid overfitting and underfitting, you can use Cross Validation","269acce2":"By using cross validation (reiteration of several training and testing data sets), we can infer that the accuracy is consistent on 99.36% with std deviation of 0.54%.","36cce85c":"Conclusion:\n\n* Linear Regression is the regression model that has the highest accuracy and lowest RMSE\n* We can use Linear Regression if we want to predict our Sales based on the Ad Budget and Type of Influencer"}}