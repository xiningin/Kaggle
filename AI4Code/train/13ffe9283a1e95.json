{"cell_type":{"0bf1d874":"code","9782f791":"code","1dbc1820":"code","b3b74601":"code","9d457dc8":"code","e489e69a":"code","d1bf9cc1":"code","f38366d5":"code","48645560":"code","91d477df":"code","2f52f0fb":"code","13e6cfc3":"code","b781893a":"code","767b13eb":"code","954ae922":"code","248b2d5c":"code","0dc1cffc":"code","55656c47":"code","fa0d538c":"code","27c64a2e":"code","1def97c0":"code","db3d82f7":"code","663b63d6":"code","1f7cc19d":"markdown","1066ca11":"markdown","c9f3ebda":"markdown","054a43b0":"markdown","9d65cf9f":"markdown","f9fa932c":"markdown","78a81e6b":"markdown","d0004acc":"markdown","1d6a6bcd":"markdown","b8fde1e7":"markdown","d9ba02bb":"markdown","48e85e6b":"markdown","e4a90721":"markdown","fc05d9bf":"markdown","7e3b740e":"markdown","b2df6c5e":"markdown","56259a21":"markdown","8eac0d70":"markdown","f8c35e68":"markdown","a31a5ac5":"markdown","f4bd2edb":"markdown","917437fe":"markdown","3587959c":"markdown","0e662e18":"markdown","0171314d":"markdown","da288444":"markdown","fae4dbe6":"markdown","ae07ad60":"markdown","b1190ff2":"markdown","d6515e2a":"markdown"},"source":{"0bf1d874":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9782f791":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, plot_confusion_matrix, roc_curve\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom imblearn.over_sampling import SMOTENC \nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import AdaBoostClassifier\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, chi2, SelectFromModel, mutual_info_classif\nfrom warnings import filterwarnings\nfrom sklearn.inspection import permutation_importance","1dbc1820":"#reading data\ndata=pd.read_csv(\"\/kaggle\/input\/credit-card-customers\/BankChurners.csv\")\ndata.drop(list(data.iloc[:,[0,-2,-1]]),inplace=True,axis=1)\ndata.describe().T.style.format(\"{0:.2f}\").bar(\n    subset=['mean'], align='zero').background_gradient(\n    subset=['std'], cmap='Blues')","b3b74601":"#Changing Existing customer to 0 and Attrited Customer to 1\nmap={'Attrited Customer':1,\"Existing Customer\":0}\ndata[\"Attrition_Flag\"].replace(map,inplace=True)","9d457dc8":"data.head()","e489e69a":"data.isnull().sum()","d1bf9cc1":"data.dtypes","f38366d5":"col_num=data.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\ncol_non_num=data.select_dtypes(include=[\"object\"]).columns.tolist()\nprint(\"Unique values in Non numeric columns are: \")\nfor col in col_non_num:\n    print(col.title(),\" : \",data[col].unique())\nprint(\"\\n\")\nprint(\"Number of unique values in numeric columns are:\")\ndata[col_num].nunique()","48645560":"sns.set_theme(style=\"darkgrid\")\nfig,axs=plt.subplots(5,1,figsize=(15,25))\ni=0\nfig.suptitle(\"Count plot of Non-numeric features\",y=0.9,fontweight=\"bold\")\nfor col in col_non_num:\n    b=sns.countplot(data=data,y=col,ax=axs[i],hue=\"Attrition_Flag\",palette=\"flare\")\n    b.set_xlabel(\" \")\n    b.legend(labels=[\"Existing Customer\",\"Attrited Customer\"],loc = 2, bbox_to_anchor = (1,1))\n    for p in b.patches:\n        height = p.get_height() \n        width = p.get_width()\n        axs[i].text(x = width+3,\n                y = p.get_y()+(height\/2),\n                s = \"{:.0f}\".format(width),\n                va = \"center\")\n    i+=1","91d477df":"fig,axs=plt.subplots(5,3,figsize=(25,25))\nfig.suptitle(\"Distribution plot of numeric features\",y=0.9,fontweight=\"bold\",fontsize=20)\nfor col,ax in zip(col_num[1:],axs.flatten()):\n    b=sns.histplot(x=col,ax=ax,color=\"lightcoral\",data=data,hue=\"Attrition_Flag\",kde=True)\n    b.legend(labels=[\"Attrited Customer\",\"Existing Customer\"])\n    b.set_ylabel(\"\")\n    plt.subplots_adjust(wspace=.25)\nfig.delaxes(axs[4,2])","2f52f0fb":"fig,axs=plt.subplots(4,4,figsize=(25,20))\nfig.suptitle(\"Box plot of numeric features to detect outliers\",y=0.9,fontweight=\"bold\",fontsize=20)\nfor col,ax in zip(col_num[1:],axs.flatten()):\n    b=sns.boxplot(y=data[col],ax=ax,color=\"darksalmon\")\n    b.set_ylabel(col,fontsize=15)\n    plt.subplots_adjust(wspace=.25)\nfig.delaxes(axs[3,2])\nfig.delaxes(axs[3,3])","13e6cfc3":"col_outliers=data[col_num].iloc[:,[1,3,5,6,7,9,10,11,12,13]].columns.tolist()\ncol_outliers","b781893a":"#detecting columns with outliers and replacing data with outliers with np.nan\ndef outlier_trt(data,col):\n    X=data[col].copy()\n    q3=X.quantile(0.75)\n    q1=X.quantile(0.25)\n    iqr=q3-q1\n    for idx,entry in enumerate(data[col]):\n        if (entry>q3+1.5*iqr):\n            data.loc[idx,col]=q3+1.5*iqr\n        elif (entry<q1-1.5*iqr):\n            data.loc[idx,col]=q1-1.5*iqr\n    return X[(X>q3+1.5*iqr)|(X<q1-1.5*iqr)]\ncol_outliers=data[col_num].iloc[:,[1,3,5,6,7,9,10,11,12,13]].columns.tolist() #\nfor col in col_outliers:\n    outlier_trt(data,col)\n","767b13eb":"fig,axs=plt.subplots(4,4,figsize=(25,20))\nfig.suptitle(\"Box plot of numeric features after outlier adjustment\",y=0.9,fontweight=\"bold\",fontsize=20)\nfor col,ax in zip(col_num[1:],axs.flatten()):\n    b=sns.boxplot(y=data[col],ax=ax,color=\"plum\")\n    b.set_ylabel(col,fontsize=15)\n    plt.subplots_adjust(wspace=.25)\nfig.delaxes(axs[3,2])\nfig.delaxes(axs[3,3])","954ae922":"col_num.remove('Attrition_Flag')","248b2d5c":"x=data.copy()\ny=x.pop(\"Attrition_Flag\")\ntrain_ratio = 0.75\nvalidation_ratio = 0.15\ntest_ratio = 0.10\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0,test_size=1-train_ratio)\nx_val,x_test,y_val,y_test = train_test_split(x_test,y_test,random_state=0,test_size=test_ratio\/(test_ratio + validation_ratio))\ntrain_list=[x_train,y_train]\nval_list=[x_val,y_val]\ntest_list=[x_test,x_val]\nprint(\"Training set: \",x_train.shape,\"\\nValidation set: \",x_val.shape,\"\\nTesting set shape: \",x_test.shape)","0dc1cffc":"fig,ax=plt.subplots(1,1)\nplt.pie(x=y_train.value_counts(),labels=[\"Existing customers\",\"Attired Customers\"],colors=sns.color_palette('pastel'),autopct=\"%.1f%%\")\nfig.suptitle(\"Number of existing and attrired customer present in the training set\",y=1,fontweight=\"bold\",fontsize=15)\nplt.show()","55656c47":"obj_lst=x.columns[x.dtypes==\"object\"].tolist()\nlst=x.columns.tolist()\ncat_idx=[i for i,ele in enumerate(lst) if ele in obj_lst]\nsmotenc=SMOTENC(cat_idx,random_state=1,sampling_strategy='not majority')\nx_train,y_train=smotenc.fit_resample(x_train,y_train)\nfig,ax=plt.subplots(1,1,figsize=(7,4))\nfig.suptitle(\"Number of existing and attrired customer present in the training set after oversampling\",y=1,fontweight=\"bold\",fontsize=15)\nax.set_xticks([0,1])\nplt.pie(x=y_train.value_counts(),labels=[\"Existing customers\",\"Attired Customers\"],colors=sns.color_palette('pastel'),autopct=\"%.1f%%\")\nplt.show()\nprint(\"Shape of new training, validation and testing set is as follows: \")\nprint(\"Training set: \",x_train.shape,\"\\nValidation set: \",x_val.shape,\"\\nTesting set shape: \",x_test.shape)","fa0d538c":"num_transformer=StandardScaler()\ncat_transformer=OneHotEncoder(handle_unknown='ignore',sparse=False)\npreprocessor=ColumnTransformer(\n    transformers=[\n        (\"num\",num_transformer,col_num),\n        (\"cat\",cat_transformer,col_non_num)\n    ],\n        remainder=\"passthrough\")\nrfc=RandomForestClassifier(random_state=0)\nada=AdaBoostClassifier(random_state=0)\ngbc=GradientBoostingClassifier(random_state=0)\nmodels=[rfc,ada,gbc]\nfor model in models:\n    my_pipeline=Pipeline(steps=[\n                                   (\"preprocessor\",preprocessor),\n                                   (\"model\",model)\n                               ])\n    my_pipeline.fit(x_train,y_train)\n    preds=my_pipeline.predict(x_val)\n    print(\"Report for \",str(model))\n    print('Accuracy Score : {0:.2f}%'.format(accuracy_score(y_val,preds)*100))\n    print('Precision Score : {0:.2f}%'.format(precision_score(y_val,preds)*100))\n    print('Recall Score : {0:.2f}%'.format(recall_score(y_val,preds)*100))\n    print('F1 Score : {0:.2f}%'.format(f1_score(y_val,preds)*100))\n    cm=confusion_matrix(y_val,preds)\n    b=sns.heatmap(cm, annot=True,fmt=\"\",cbar=False,linecolor=\"white\",linewidths=1.5,cmap=\"Pastel1\")\n    b.set(xlabel=\"Predicted values\",ylabel=\"Actual values\")\n    b.set_title(\"Confusion matrix\",fontweight=\"bold\",fontsize=13)\n    plt.show()\n    print(\"\\n\")","27c64a2e":"space = {\n        \"loss\": hp.choice('loss', [\"deviance\", \"exponential\"]),\n        'learning_rate': hp.choice('learning_rate', [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]),\n        'subsample': hp.uniform('subsample', 0,1),\n        'criterion': hp.choice('criterion', [\"friedman_mse\", \"mse\", \"mae\"]),\n        'min_samples_leaf': hp.choice('min_samples_leaf',[0.1,0.2,0.3,0.4,0.5,1,2,3,4]),\n        'min_samples_split' : hp.choice('min_samples_split',[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,2,3,4]),\n        'n_estimators' : hp.choice('n_estimators', [10,50,100,150,200]),\n        \"max_features\" : hp.choice(\"max_features\", [\"auto\", \"sqrt\", \"log2\"])\n        }\ndef objective(space):\n    model = GradientBoostingClassifier(\n                                   loss = space[\"loss\"],\n                                   learning_rate = space['learning_rate'], \n                                   subsample = space['subsample'],\n                                   criterion = space['criterion'],\n                                   min_samples_leaf = space['min_samples_leaf'],\n                                   min_samples_split = space['min_samples_split'],\n                                   n_estimators = space['n_estimators'],\n                                   max_features = space['max_features'],\n                                   random_state=0\n                                      )\n    my_pipeline=Pipeline(steps=[\n                               (\"preprocessor\",preprocessor),\n                               (\"model\",model)\n                               ])\n    my_pipeline.fit(x_train,y_train)\n    accuracy =my_pipeline.score(x_val,y_val)\n    return {'loss': -accuracy, 'status': STATUS_OK }\ntrials = Trials()\nbest = fmin(fn= objective,\n            space= space,\n            algo= tpe.suggest,\n            max_evals = 25,\n            trials= trials)\nbest","1def97c0":"parameters=[\"loss\",\"learning_rate\",\"criterion\",\"min_samples_leaf\",\"min_samples_split\",\"n_estimators\",\"max_features\"]\nlst_in_lst=[[\"deviance\", \"exponential\"],\n            [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n            [\"friedman_mse\", \"mse\", \"mae\"],\n            [0.1,0.2,0.3,0.4,0.5,1,2,3,4],\n            [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,2,3,4],\n            [10,50,100,150,200],\n            [\"auto\", \"sqrt\", \"log2\"]]\ndct={}\nfor param,lst in zip(parameters,lst_in_lst):\n    i=0\n    dct2={}\n    for ele in lst:\n        dct2[i]=ele\n        i+=1\n    dct[param]=dct2\nmodel =GradientBoostingClassifier(\n                                  loss = dct[\"loss\"][best[\"loss\"]],\n                                  learning_rate = dct[\"learning_rate\"][best[\"learning_rate\"]],\n                                  subsample=best[\"subsample\"],\n                                  criterion = dct[\"criterion\"][best[\"criterion\"]], \n                                  min_samples_leaf = dct[\"min_samples_leaf\"][best[\"min_samples_leaf\"]], \n                                  min_samples_split = dct[\"min_samples_split\"][best[\"min_samples_split\"]], \n                                  n_estimators = dct[\"n_estimators\"][best[\"n_estimators\"]], \n                                  max_features = dct[\"max_features\"][best[\"max_features\"]],\n                                  random_state=0\n                                 )\nmy_pipeline=Pipeline(steps=[\n                               (\"preprocessor\",preprocessor),\n                               (\"model\",model)\n                           ])\nmy_pipeline.fit(x_train,y_train)\ny_pred= my_pipeline.predict(x_test)\nprint('Accuracy Score : {0:.2f}%'.format(accuracy_score(y_test,y_pred)*100))\nprint('Precision Score : {0:.2f}%'.format(precision_score(y_test,y_pred)*100))\nprint('Recall Score : {0:.2f}%'.format(recall_score(y_test,y_pred)*100))\nprint('F1 Score : {0:.2f}%'.format(f1_score(y_test,y_pred)*100))\ncm=confusion_matrix(y_test,y_pred)\nb=sns.heatmap(cm, annot=True,fmt=\"\",cbar=False,linecolor=\"white\",linewidths=1.5,cmap=\"Pastel1\")\nb.set(xlabel=\"Predicted values\",ylabel=\"Actual values\")\nb.set_title(\"Confusion matrix\",fontweight=\"bold\",fontsize=13)\nplt.show()","db3d82f7":"result = permutation_importance(my_pipeline,x_train,y_train,n_repeats=10,random_state=42,n_jobs=2)\nforest_importances = pd.Series(result.importances_mean, index=x_train.columns.tolist())\nfig, ax = plt.subplots(figsize=(10,5))\nsns.barplot(x=forest_importances,y=forest_importances.index)\nax.set_title(\"Feature importances using permutation on full model\",fontweight=\"bold\",fontsize=15)\nax.set_ylabel(\"\")\nfig.tight_layout()\nplt.show()\nprint(\"Feature importance scores of each feature sorted in ascending order: \")\nforest_importances.sort_values(ascending=False)","663b63d6":"y_score=my_pipeline.predict_proba(x_test)[:,1]\nfalse_positive_rate, true_positive_rate, threshold =roc_curve(y_test,y_score)\nplt.subplots(1, figsize=(10,5))\nplt.title('Receiver Operating Characteristic - GradientBoostingClassifier')\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0,1],ls=\"--\")\nplt.plot([0,0],[1,0],c=\".7\") \nplt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('roc_auc_score for GradientBoostingClassifier: ',roc_auc_score(y_test, y_score))","1f7cc19d":"Dividing all features into \"numeric\" and \"non_numeric\" category.","1066ca11":"Following mentioned features have outliers in their data:","c9f3ebda":"Data is split into training , validation and testing set. Following ration is considered for each of the three:\n1. Training set: 75%\n2. Validation set: 15%\n3. Testing set: 10%","054a43b0":"# 1. Data Cleaning","9d65cf9f":"Area under curve is found out to be 0.993333.","f9fa932c":"Checking all features for their respective data type (dtype).\n","78a81e6b":"Checking all columns for NaN or null values.","d0004acc":"Breif glimpse of the data and how it is distributed.","1d6a6bcd":"Best hyperparameters for GradientBoostingClassifier are described below.","b8fde1e7":"As it can be seen, data to be predicted is fairly overlapped. Also it can be seen that \"Total_Amt_Chng_Q4_Q1\" and \"Customer_Age\" follows normal distribution.","d9ba02bb":"It can be seen that all models have performed well with good accuracy. Lets tune further the model with highest accuracy, GradientBoostingClassifier.","48e85e6b":"Each feature and its relevance to the data is explained below:\n1.  Attrition_Flag:  Internal event (customer activity) variable - if the account is closed or not\n2.  Customer_Age:    Demographic variable - Customer's Age in Years\n3.  Gender:          Demographic variable - M= Male, F= Female\n4.  Dependent_count: Demographic variable - Number of dependents\n5.  Education_Level: Demographic variable - Educational Qualification of the account holder (example: high school, college                           graduate, etc.)\n6.  Marital_Status:  Demographic variable - Married, Single, Divorced, Unknown\n7.  Income_Category: Demographic variable - Annual Income Category of the account holder (<$40K, $40K - 60K, $60K - $80K,                            $80K-$120K, >)\n8.  Card_Category:   Product Variable - Type of Card (Blue, Silver, Gold, Platinum)\n9. Months_on_book:  Period of relationship with bank\n10. Total_Relationship_Count: Total no. of products held by the customer\n11. Months_Inactive_12_mon: No. of months inactive in the last 12 months\n12. Contacts_Count_12_mon: No. of Contacts in the last 12 months\n13. Credit_Limit: Credit Limit on the Credit Card\n14. Total_Revolving_Bal: Total Revolving Balance on the Credit Card\n15. Avg_Open_To_Buy: Open to Buy Credit Line (Average of last 12 months)\n16. Total_Amt_Chng_Q4_Q1: Change in Transaction Amount (Q4 over Q1)\n17. Total_Trans_Amt: Total Transaction Amount (Last 12 months)\n18. Total_Trans_Ct: Total Transaction Count (Last 12 months)\n19. Total_Ct_Chng_Q4_Q1: Change in Transaction Count (Q4 over Q1)\n20. Avg_Utilization_Ratio: Average Card Utilization Ratio\n\n\n","e4a90721":"Lets address this imbalance by applying oversamplling technique \"SMOTENC\" and re-check this imbalance","fc05d9bf":"# 3. Data preprocessing and Model selection","7e3b740e":"Outlier adjustment is done in the following code and graphs are re plotted to make sure that each outlier is taken care of.","b2df6c5e":"Now, lets detect outliers in our data in order to pre-process it better to feed data in our model.","56259a21":"Shape of all three training, validation and testing set is printed below.","8eac0d70":"Replacing \"Existing Cutomer\" with digit 0 and and \"Attirted Customer\" with digit 1.","f8c35e68":"# 2. Exploratory Data Analysis","a31a5ac5":"Lets analyze the customer distribution among all numeric features.","f4bd2edb":"Importing all libraries","917437fe":"# 5. Prediction of test data\nTuned model is used to make predictions on test data.","3587959c":"Now lets see how imbalanced is our training set.","0e662e18":"Viewing the first five rows of data after making the above mentioned modification.","0171314d":"It is observed that number of existing customers are way more higher than number of attirted customers. Data is imbalanced. We'll try to fix that in data-preprocessing phase.","da288444":"# 4. Model Tuning","fae4dbe6":"After dividing the features into above mentioned two categories, lets see the number of unique values in each category.","ae07ad60":"Overall accuracy of of the tuned model comes out to be 97.73%. Features with most importance are found out. Following graph is plotted to show the importance score of each feature.","b1190ff2":"For data preprocessing we ll apply OneHotEncoder for handling categorical variables and  StandardScalar for scaling numerical data. Three models namely, RandomForestClassifier, AdaBoostClassifier and GradientBoostingClassifier are compared.","d6515e2a":"Firstly, exploring the non-numeric columns, and analyzing the number of attirted and existing customers in each of the following features namely:\n1. Gender\n2. Education_Level\n3. Marital_Status\n4. Income_Category\n5. Card_Category"}}