{"cell_type":{"269a79d8":"code","93eb0bf9":"code","a854e922":"code","e8a505da":"code","bf9159c7":"code","fd3dce70":"code","188158e7":"code","e3a4c3e5":"code","9d0667f3":"code","8afcf28b":"code","17b90465":"code","98ab6462":"code","fe3318b3":"code","99cc1f80":"code","393a2812":"code","55361972":"code","8306f732":"code","6ab23da9":"code","d0796658":"code","81657ea8":"code","3593d0b9":"code","a74bf445":"code","9287ecb0":"code","38cb6438":"code","c09c6826":"code","f33a5af3":"code","54f29c89":"code","4e93ff56":"code","bd2d2807":"code","875a6649":"code","7b0a0d67":"code","5d1da406":"code","75a24348":"code","11a230d3":"code","258c1719":"code","f6be3986":"code","09884eb5":"code","c629ed7a":"code","5cb021ce":"code","837f9191":"code","3143c99e":"markdown","d508f523":"markdown","cc6c9a9a":"markdown","df62edc1":"markdown","30a42f9b":"markdown","d2cdde3a":"markdown","1b8293bd":"markdown","23f45aa8":"markdown","a982efba":"markdown","842c0c4a":"markdown","6840770f":"markdown","5529b068":"markdown","3879d6b5":"markdown","37055224":"markdown","14342c00":"markdown","9150f458":"markdown","b2c400ea":"markdown","41b9e7b0":"markdown","39bd7398":"markdown","b11c0645":"markdown","7ff266be":"markdown","ad0a60f8":"markdown","c58d994a":"markdown","0084318e":"markdown"},"source":{"269a79d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport re\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom wordcloud import WordCloud, STOPWORDS\n%matplotlib inline\n\nimport random\nimport plotly\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \ninit_notebook_mode(connected=True)\nimport plotly.offline as offline\nimport plotly.graph_objs as go\n\n# Model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import log_loss\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))","93eb0bf9":"train_data = pd.read_json('..\/input\/train.json') # store as dataframe objects\ntest_data = pd.read_json('..\/input\/test.json')","a854e922":"train_data.info()","e8a505da":"train_data.shape # 39774 observations, 3 columns","bf9159c7":"print(\"The training data consists of {} recipes\".format(len(train_data)))","fd3dce70":"print(\"First five elements in our training sample:\")\ntrain_data.head()","188158e7":"test_data.info()","e3a4c3e5":"test_data.shape # 9944 observations, 2 columns","9d0667f3":"print(\"The test data consists of {} recipes\".format(len(test_data)))","8afcf28b":"print(\"First five elements in our test sample:\")\ntest_data.head()","17b90465":"print(\"Number of cuisine categories: {}\".format(len(train_data.cuisine.unique())))\ntrain_data.cuisine.unique()","98ab6462":"#Define a function to generate randoms colors for further visualizations\ndef random_colours(number_of_colors):\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","fe3318b3":"trace = go.Table(\n                header=dict(values=['Cuisine','Number of recipes'],\n                fill = dict(color=['#EABEB0']), \n                align = ['left'] * 5),\n                cells=dict(values=[train_data.cuisine.value_counts().index,train_data.cuisine.value_counts()],\n                align = ['left'] * 5))\n\nlayout = go.Layout(title='Number of recipes in each cuisine category',\n                   titlefont = dict(size = 20),\n                   width=500, height=650, \n                   paper_bgcolor =  'rgba(0,0,0,0)',\n                   plot_bgcolor = 'rgba(0,0,0,0)',\n                   autosize = False,\n                   margin=dict(l=30,r=30,b=1,t=50,pad=1),\n                   )\ndata = [trace]\nfig = dict(data=data, layout=layout)\niplot(fig)","99cc1f80":"#  Label distribution in percents\nlabelpercents = []\nfor i in train_data.cuisine.value_counts():\n    percent = (i\/sum(train_data.cuisine.value_counts()))*100\n    percent = \"%.2f\" % percent\n    percent = str(percent + '%')\n    labelpercents.append(percent)","393a2812":"trace = go.Bar(\n            x=train_data.cuisine.value_counts().values[::-1],\n            y= [i for i in train_data.cuisine.value_counts().index][::-1],\n            text =labelpercents[::-1],  textposition = 'outside', \n            orientation = 'h',marker = dict(color = random_colours(20)))\nlayout = go.Layout(title='Number of recipes in each cuisine category',\n                   titlefont = dict(size = 25),\n                   width=1030, height=450, \n                   plot_bgcolor = 'rgba(0,0,0,0)',\n                   \n                   margin=dict(l=75,r=110,b=50,t=60),\n                   )\ndata = [trace]\nfig = dict(data=data, layout=layout)\niplot(fig, filename='horizontal-bar')","55361972":"print('Maximum Number of Ingredients in a Dish: ',train_data['ingredients'].str.len().max())\nprint('Minimum Number of Ingredients in a Dish: ',train_data['ingredients'].str.len().min())","8306f732":"trace = go.Histogram(\n    x= train_data['ingredients'].str.len(),\n    xbins=dict(start=0,end=80,size=1),\n   marker=dict(color='#fbca5f'),\n    opacity=0.75)\ndata = [trace]\nlayout = go.Layout(\n    title='Distribution of Recipe Length',\n    xaxis=dict(title='Number of ingredients'),\n    yaxis=dict(title='Count of recipes'),\n    bargap=0.1,\n    bargroupgap=0.2)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","6ab23da9":"boxplotcolors = random_colours(21)\nlabels = [i for i in train_data.cuisine.value_counts().index][::-1]\ndata = []\nfor i in range(20):\n    trace = {\n            \"type\": 'violin',\n            \"y\": train_data[train_data['cuisine'] == labels[i]]['ingredients'].str.len(),\n            \"name\": labels[i],\n            \"box\": {\n                \"visible\": True\n            },\n            \"meanline\": {\n                \"visible\": True\n            }\n        }\n    data.append(trace)\nlayout = go.Layout(\n    title = \"Recipe Length Distribution by cuisine\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"Box Plot Styling Outliers\")","d0796658":"print(\"Word Cloud Function..\")\nstopwords = set(STOPWORDS)\nsize = (20,10)\n\ndef cloud(text, title, stopwords=stopwords, size=size):\n    \"\"\"\n    Function to plot WordCloud\n    Includes: \n    \"\"\"\n    # Setting figure parameters\n    mpl.rcParams['figure.figsize']=(10.0,10.0)\n    mpl.rcParams['font.size']=12\n    mpl.rcParams['savefig.dpi']=100\n    mpl.rcParams['figure.subplot.bottom']=.1 \n    \n    # Processing Text\n    wordcloud = WordCloud(width=1600, height=800,\n                          background_color='black',\n                          stopwords=stopwords,\n                         ).generate(str(text))\n    \n    # Output Visualization\n    fig = plt.figure(figsize=size, dpi=80, facecolor='k',edgecolor='k')\n    plt.imshow(wordcloud,interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title, fontsize=50,color='y')\n    plt.tight_layout(pad=0)\n    plt.show()\n    \n# Data Set for Word Clouds\ntrain_data[\"ing\"] = train_data.ingredients.apply(lambda x: list(map(str, x)), 1).str.join(' ')\n# All\ncloud(train_data[\"ing\"].values, title=\"All Cuisine\", size=[8,5])","81657ea8":"y = train_data.cuisine.copy()\nprint(\"Cuisine WordClouds\")\ncloud_df = pd.concat([train_data.loc[train_data.index,'ing'], y],axis=1)\nfor cuisine_x in y.unique():\n    cloud(cloud_df.loc[cloud_df.cuisine == cuisine_x, \"ing\"].values, title=\"{} Cuisine\".format(cuisine_x.capitalize()), size=[8,5])\ntrain_data.drop('ing',axis=1,inplace=True)","3593d0b9":"train_data['seperated_ingredients'] = train_data['ingredients'].apply(','.join)\ntest_data['seperated_ingredients'] = test_data['ingredients'].apply(','.join)","a74bf445":"import nltk\nfrom collections import Counter","9287ecb0":"train_data['for ngrams']=train_data['seperated_ingredients'].str.replace(',',' ')","38cb6438":"import networkx as nx\ndef generate_ngrams(text, n):\n    words = text.split(' ')\n    iterations = len(words) - n + 1\n    for i in range(iterations):\n       yield words[i:i + n]\ndef net_diagram(*cuisines):\n    ngrams = {}\n    for title in train_data[train_data.cuisine==cuisines[0]]['for ngrams']:\n            for ngram in generate_ngrams(title, 2):\n                ngram = ','.join(ngram)\n                if ngram in ngrams:\n                    ngrams[ngram] += 1\n                else:\n                    ngrams[ngram] = 1\n    ngrams_mws_df = pd.DataFrame.from_dict(ngrams, orient='index')\n    ngrams_mws_df.columns = ['count']\n    ngrams_mws_df['cusine'] = cuisines[0]\n    ngrams_mws_df.reset_index(level=0, inplace=True)\n\n    ngrams = {}\n    for title in train_data[train_data.cuisine==cuisines[1]]['for ngrams']:\n            for ngram in generate_ngrams(title, 2):\n                ngram = ','.join(ngram)\n                if ngram in ngrams:\n                    ngrams[ngram] += 1\n                else:\n                    ngrams[ngram] = 1\n    \n    ngrams_mws_df1 = pd.DataFrame.from_dict(ngrams, orient='index')\n    ngrams_mws_df1.columns = ['count']\n    ngrams_mws_df1['cusine'] = cuisines[1]\n    ngrams_mws_df1.reset_index(level=0, inplace=True)\n    cuisine1=ngrams_mws_df.sort_values('count',ascending=False)[:25]\n    cuisine2=ngrams_mws_df1.sort_values('count',ascending=False)[:25]\n    df_final=pd.concat([cuisine1,cuisine2])\n    g = nx.from_pandas_edgelist(df_final,source='cusine',target='index')\n    cmap = plt.cm.RdYlGn\n    colors = [n for n in range(len(g.nodes()))]\n    k = 0.35\n    pos=nx.spring_layout(g, k=k)\n    nx.draw_networkx(g,pos, node_size=df_final['count'].values*8, cmap = cmap, node_color=colors, edge_color='grey', font_size=15, width=3)\n    plt.title(\"Top 25 Bigrams for %s and %s\" %(cuisines[0],cuisines[1]), fontsize=30)\n    plt.gcf().set_size_inches(30,30)\n    plt.show()\n    plt.savefig('network.png')","c09c6826":"net_diagram('chinese','thai')","f33a5af3":"net_diagram('indian','chinese')","54f29c89":"# Prepare the data \nfeatures = [] # list of list containg the recipes\nfor item in train_data['ingredients']:\n    features.append(item)\n    \n# Test Sample - only features - the target variable is not provided.\nfeatures_test = [] # list of lists containg the recipes\nfor item in test_data['ingredients']:\n    features_test.append(item)","4e93ff56":"# Both train and test samples are processed in the exact same way\n# Train\nfeatures_processed= [] # here we will store the preprocessed training features\nfor item in features:\n    newitem = []\n    for ingr in item:\n        ingr.lower() # Case Normalization - convert all to lower case \n        ingr = re.sub(\"[^a-zA-Z]\",\" \",ingr) # Remove punctuation, digits or special characters \n        ingr = re.sub((r'\\b(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\b'), ' ', ingr) # Remove different units  \n        newitem.append(ingr)\n    features_processed.append(newitem)\n\n# Test \nfeatures_test_processed= [] \nfor item in features_test:\n    newitem = []\n    for ingr in item:\n        ingr.lower() \n        ingr = re.sub(\"[^a-zA-Z]\",\" \",ingr)\n        ingr = re.sub((r'\\b(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\b'), ' ', ingr) \n        newitem.append(ingr)\n    features_test_processed.append(newitem) ","bd2d2807":"# Check for empty instances in train and test samples after processing before proceeding to next stage of the analysis    \ncount_m = []    \nfor recipe in features_processed:\n    if not recipe:\n        count_m.append([recipe])\n    else: pass\nprint(\"Empty instances in the preprocessed training sample: \" + str(len(count_m))) ","875a6649":"count_m = []    \nfor recipe in features_test_processed:\n    if not recipe:\n        count_m.append([recipe])\n    else: pass\nprint(\"Empty instances in the preprocessed test sample: \" + str(len(count_m)))    ","7b0a0d67":"# Binary representation of the training set will be employed\nvectorizer = CountVectorizer(analyzer = \"word\",\n                             ngram_range = (1,1), # unigrams\n                             binary = True, #  (the default is counts)\n                             tokenizer = None,    \n                             preprocessor = None, \n                             stop_words = None,  \n                             max_df = 0.99) # any word appearing in more than 99% of the sample will be discarded","5d1da406":"# Fit the vectorizer on the training data and transform the test sample\ntrain_X = vectorizer.fit_transform([str(i) for i in features_processed])\ntest_X =  vectorizer.transform([str(i) for i in features_test_processed])","75a24348":"# Extract the target variable\ntarget = train_data['cuisine']","11a230d3":"# Apply label encoding on the target variable (before model development)\nlb = LabelEncoder()\ntrain_Y = lb.fit_transform(target)","258c1719":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_X, train_Y , random_state = 0)","f6be3986":"### building the classifiers\nclfs = []\n\nrfc = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\nrfc.fit(X_train, y_train)\nprint('RFC LogLoss {score}'.format(score=log_loss(y_test, rfc.predict_proba(X_test))))\nclfs.append(rfc)\n\nlogreg = LogisticRegression(random_state = 42)\nlogreg.fit(X_train, y_train)\nprint('LogisticRegression LogLoss {score}'.format(score=log_loss(y_test, logreg.predict_proba(X_test))))\nclfs.append(logreg)\n\nsvc = SVC(random_state=42,probability=True, kernel='linear')\nsvc.fit(X_train, y_train)\nprint('SVC LogLoss {score}'.format(score=log_loss(y_test, svc.predict_proba(X_test))))\nclfs.append(svc)\n","09884eb5":"### finding the optimum weights\n\npredictions = []\nfor clf in clfs:\n    predictions.append(clf.predict_proba(X_test))\n\ndef log_loss_func(weights):\n    ''' scipy minimize will pass the weights as a numpy array '''\n    final_prediction = 0\n    for weight, prediction in zip(weights, predictions):\n            final_prediction += weight*prediction\n\n    return log_loss(y_test, final_prediction)\n    \n#the algorithms need a starting value, right not we chose 0.5 for all weights\nstarting_values = [0.5]*len(predictions)\n\ncons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n#our weights are bound between 0 and 1\nbounds = [(0,1)]*len(predictions)\n\nres = minimize(log_loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n\nprint('Ensamble Score: {best_score}'.format(best_score=res['fun']))\nprint('Best Weights: {weights}'.format(weights=res['x']))","c629ed7a":"# Ensemble Unigram model (baseline model) \nvclf=VotingClassifier(estimators=[('clf1',RandomForestClassifier(n_estimators = 50,random_state = 42)),\n                                  ('clf2',LogisticRegression(random_state = 42)),\n                                  ('clf3',SVC(kernel='linear',random_state = 42,probability=True))\n                                  ], \n                                    voting='soft', weights = [0.05607363, 0.70759724, 0.23632913]) \nvclf.fit(train_X, train_Y)","5cb021ce":"# 5-fold Cross validation of  the results\nkfold = model_selection.KFold(n_splits=5, random_state=42)\nvalscores = model_selection.cross_val_score(vclf, train_X, train_Y, cv=kfold)\nprint('Mean accuracy on 5-fold cross validation: ' + str(np.mean(valscores)))","837f9191":"# Generate predictions on test sample\npredictions = vclf.predict(test_X) \npredictions = lb.inverse_transform(predictions)\npredictions_final = pd.DataFrame({'cuisine' : predictions , 'id' : test_data.id }, columns=['id', 'cuisine'])\npredictions_final.to_csv('Final_submission.csv', index = False)","3143c99e":"## **Table of Contents**\n\n1. [Ingest](#Ingent)\n2. [EDA](#EDA)\n3. [Model](#Model)\n4. [Conclusion](#Conclusion)\n\n","d508f523":"## Text Processing\n### 1) Prepare the train and test samples for model development","cc6c9a9a":"As we can see from the histgram, the distribution of recipe length is right-skewed. Most of the recipes have lengths less than 20.","df62edc1":"# Overview\nFood is tied closely to geographic and cultural associations. This project aims to predict the category of a dish's cuisine given a list of its ingredients. \n\n### What kind of cuisine do you like? \ud83d\ude0b\ud83c\udf7d\n![Word Cuisine](https:\/\/www.dfordelhi.in\/wp-content\/uploads\/2018\/10\/food3.jpeg)\n\nAn example of a recipe node in train.json:\n {\n \"id\": 24717,\n \"cuisine\": \"indian\",\n \"ingredients\": [\n     \"tumeric\",\n     \"vegetable stock\",\n     \"tomatoes\",\n     \"garam masala\",\n     \"naan\",\n     \"red lentils\",\n     \"red chili peppers\",\n     \"onions\",\n     \"spinach\",\n     \"sweet potatoes\"\n ]\n },","30a42f9b":"Recipe Length Distribution by cuisine","d2cdde3a":"### **Network Diagrams for Bigrams**","1b8293bd":"# Conclusion<a><\/a>","23f45aa8":"### **Word Cloud**\n\nWordCloud is a technique to show which words are the most frequent among the given text. We use word cloud here to see which ingredients are most frequently used in a cuisine.","a982efba":"There are total 20 different cuisines (categories) which we need to predict, which indicates a  **multi-class classification** problem.","842c0c4a":"### *Catch a glimpse of the training data:*","6840770f":"\n# Ingest <a><\/a>\nLoad necessary libraries  and import json train and test files \n","5529b068":"Based on the table and plot shown above, we can see that in the training dataset, Italian and Mexican cuisines are the most common ones, and Irish, Jamaican, Russian and Brazilian are rare in the dataset.\n\n\n### **2) Dig into the ingredients**","3879d6b5":"The distribution of recipe length","37055224":"The ingredents are processed in the following aspects:\n\n1. Case Normalization - convert all to lower case \n2. Remove punctuation, digits or special characters \n3. Remove different units","14342c00":"Our model is validated via 5-fold cross validation and the measured average accuracy on train sample is 79%. \n\nThis project is a good practice to process text data for classification problems.\n","9150f458":"# EDA<a><\/a>","b2c400ea":"### *Also take a look at the test data*","41b9e7b0":"### Define a function to find the optimum weights of ensemble model based on log loss.\n\nLogarithmic loss measures the performance of a classification model where the prediction input is a probability value between 0 and 1. The goal of our machine learning models is to minimize this value.\n\n*Accuracy is the count of predictions where your predicted value equals the actual value. Accuracy is not always a good indicator because of its yes or no nature. Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. This gives us a more nuanced view into the performance of our model.*","39bd7398":"### **1) Explore the target: cuisine**\n\nThe number of recipes in each cuisine category in the training data.","b11c0645":"According to the two network diagrams above, we can see that compared with Indian cuisine, Chinese cuisine seems to have more common ingredients with Thai cuisine.","7ff266be":"### 3) Model development\n\nEnsemble learning is usually used to average the predictions of different models to get a better prediction.\n\nA Unigram Ensemble model which combines the decisions of the following three classifiers is created to predict the cuisines.\n1. Random Forest\n2. Logistic Regression\n3. Linear Support Vector Classifier","ad0a60f8":"# **Model**","c58d994a":"### 2) Feature engineering\n\nThe textual data is transformed binary numbers to prepare for model building.","0084318e":"From the box plots of recipe length distributions, we can observe: \n\n* Outliers exist in each cuisine;\n* Most of the Asian cuisines (Vietnamese, Thai, Indian) turn out to have larger recipes on average than most of the rest cuisines\n"}}