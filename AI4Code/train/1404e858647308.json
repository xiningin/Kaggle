{"cell_type":{"d6262fad":"code","48af84f0":"code","9af6fd73":"code","e905e0de":"code","12034bfb":"code","22164460":"code","8320ba91":"code","27089666":"code","d49d6fb8":"code","d739508e":"code","42464907":"code","82483848":"code","4f9c71cc":"code","7f54743c":"code","cd4c7d6f":"code","a72ae75c":"code","72b9f7d5":"code","cae78f7a":"code","74ed202c":"code","2ebaa793":"code","60ffb36d":"code","12ec7b7d":"code","14d271b2":"code","3b01de09":"code","635150e3":"code","6e5d9555":"code","0a25deb0":"code","3976ab53":"code","365f8e25":"code","c2fd11ca":"code","c59bde4b":"code","7dec045f":"code","8b2a744b":"code","79addd71":"code","52a01de4":"code","9eb3f08f":"code","47ef25f3":"code","5613a215":"code","201fcf72":"code","d13a95b3":"code","b532ba20":"code","3732f06e":"code","96ad638a":"code","6d335237":"code","a6339889":"code","8c606989":"code","fb9c8b12":"code","be566277":"code","9cd76097":"code","771263f8":"code","6f2f09ec":"code","f5df5eda":"code","cff3c2b2":"code","24fefa33":"code","669b40c5":"code","535760a1":"code","ed40172c":"code","b5f2d605":"code","940712e6":"code","1a453408":"code","ac6f84d8":"code","ab86284e":"code","614c2268":"code","1e881f03":"code","7de7afc5":"code","5afe94ff":"code","2da7961e":"code","62a151f1":"code","03bee35f":"code","202e9c68":"code","be40e48b":"code","1cec063b":"code","4b21f836":"code","8bea9d07":"code","ff0ae2f0":"code","6c318caf":"code","34ccb85a":"code","430700d2":"markdown","929c1b08":"markdown","09498c8e":"markdown","b85abfde":"markdown","e7343678":"markdown","2dec90cb":"markdown","b56c359a":"markdown","74fafd2c":"markdown","5b5194d2":"markdown","ac23675b":"markdown","58a7cd30":"markdown","95b98f60":"markdown","e175aa63":"markdown","4ad13674":"markdown","58018862":"markdown","b02b5c51":"markdown","f37c0327":"markdown","60105615":"markdown","2625057e":"markdown","0a718cb0":"markdown","aa0eaf10":"markdown","a1533742":"markdown","0e28a4ab":"markdown","6f2bc83d":"markdown","ef12d48f":"markdown"},"source":{"d6262fad":"from IPython.display import Image\nImage(\"..\/input\/imagefold\/forecasting_image.jpg\")","48af84f0":"import os\nimport datetime\n\nimport IPython\nimport IPython.display\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False","9af6fd73":"zip_path = tf.keras.utils.get_file(\n    origin='https:\/\/storage.googleapis.com\/tensorflow\/tf-keras-datasets\/jena_climate_2009_2016.csv.zip',\n    fname='jena_climate_2009_2016.csv.zip',\n    extract=True)\ncsv_path, _ = os.path.splitext(zip_path) #We load the dataset in a csv_file","e905e0de":"df = pd.read_csv(csv_path) #let's read the csv file with pandas","12034bfb":"def preprocessing(data):\n    \n    # Getting rid of outliers\n    data.loc[df['wv (m\/s)'] == -9999.0, 'wv (m\/s)'] = 0.0\n    data.loc[df['max. wv (m\/s)'] == -9999.0, 'max. wv (m\/s)'] = 0.0\n    \n    # Taking values every hours\n    data = data[5::6]# df[start,stop,step]\n    \n    wv = data.pop('wv (m\/s)')\n    max_wv = data.pop('max. wv (m\/s)')\n\n    # Convert to radians.\n    wd_rad = data.pop('wd (deg)')*np.pi \/ 180\n\n    # Calculate the wind x and y components.\n    data['Wx'] = wv*np.cos(wd_rad)\n    data['Wy'] = wv*np.sin(wd_rad)\n\n    # Calculate the max wind x and y components.\n    data['max Wx'] = max_wv*np.cos(wd_rad)\n    data['max Wy'] = max_wv*np.sin(wd_rad)\n    \n    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n    timestamp_s = date_time.map(datetime.datetime.timestamp)\n    \n    day = 24*60*60 # Time is second within a single day\n    year = 365.2425*day # Time in second withon a year\n\n    data['Day sin'] = np.sin(timestamp_s * (2*np.pi \/ day))\n    data['Day cos'] = np.cos(timestamp_s * (2*np.pi \/ day))\n    data['Year sin'] = np.sin(timestamp_s * (2*np.pi \/ year))\n    data['Year cos'] = np.cos(timestamp_s * (2*np.pi \/ year))\n    \n    return(data)","22164460":"def split(data):\n    \n    n = data.shape[0]\n    \n    train_df = data.iloc[0: n * 70 \/\/100] # \"iloc\" because we have to select the lines at the indicies 0 to int(n*0.7) compared to \"loc\"\n    val_df = data.iloc[n * 70 \/\/100 : n * 90 \/\/100]\n    test_df = data.iloc[n * 90 \/\/100:]\n    \n    return(train_df, val_df, test_df)","8320ba91":"df_processed = preprocessing(df)\n\ntrain_df, val_df, test_df = split(df_processed)\n\ntrain_mean = train_df.mean() # returns a one column panda dataframe (serie) containing the mean of every columns\ntrain_std = train_df.std() # same with standard deviation\n\ntrain_df = (train_df - train_mean)\/train_std # As simple as that !\nval_df = (val_df - train_mean)\/train_std\ntest_df = (test_df - train_mean)\/train_std","27089666":"type(train_df) # Right now, data has a type Panda Dataframe, we'll need to turn it to numpy array.","d49d6fb8":"lookback = 48 # Looking at all features for the past 2 days\ndelay = 24 # Trying to predict the temperature for the next day\nwindow_length = lookback + delay\nbatch_size = 32 # Features will be batched 32 by 32.","d739508e":"def create_dataset(X, y, delay=24):\n    # X and y should be pandas dataframes\n    Xs, ys = [], []\n    for i in range(lookback, len(X)-delay):\n        v = X.iloc[i-lookback:i].to_numpy() # every one hour, we take the past 48 hours of features\n        Xs.append(v)\n        w = y.iloc[i+delay] # Every timestep, we take the temperature the next delay (here one day)\n        ys.append(w)\n    return(np.array(Xs), np.array(ys))","42464907":"X_train, y_train = create_dataset(train_df, train_df['T (degC)'], delay = delay)\nX_val, y_val = create_dataset(val_df, val_df['T (degC)'], delay = delay)","82483848":"print(\"X_train shape is {}: \".format(X_train.shape))\nprint(\"y_train shape is {}: \".format(y_train.shape))\n\nprint(\"\\nX_val shape is {}: \".format(X_val.shape))\nprint(\"y_val shape is {}: \".format(y_val.shape))","4f9c71cc":"def naive_eval_arr(X, y, lookback, delay):\n    batch_maes = []\n    for i in range(0, len(X)):\n        preds = X[i, -1, 1] #For all elements in the batch, we are saying the prediction of temperature is equal to the last temperature recorded within the 48 hours\n        mae = np.mean(np.abs(preds - y[i]))\n        batch_maes.append(mae)\n    return(np.mean(batch_maes))\n\nnaive_loss_arr = naive_eval_arr(X_val, y_val, lookback = lookback, delay = delay)\n\nnaive_loss_arr = round(naive_eval_arr(X_val, y_val, lookback = lookback, delay = delay),2) # Round the value\nprint(naive_loss_arr)","7f54743c":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras.optimizers import RMSprop","cd4c7d6f":"# Let's start with a simple Dense model\nmodel = Sequential([\n    Flatten(input_shape=(lookback, 19)),\n    Dense(32, activation='relu'),\n    Dense(1) # We try to predict only one value for now\n])","a72ae75c":"model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mae')\nhistory = model.fit(X_train, y_train, epochs = 30, validation_data = (X_val, y_val), batch_size = 32)","72b9f7d5":"# Let's define a function to plot graphs, it will be usefull since we'll built a lot of them !\ndef plot(history, naive_loss):\n    \n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(loss)+1)\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.axhline(y=naive_loss, color ='r')\n\n    plt.legend()\n    plt.show()","cae78f7a":"plot(history, naive_loss_arr)","74ed202c":"def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=32, step=1):\n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    while True:\n        if shuffle == True: # pay attention ! We are not shuffeling timesteps but elemnts within a batch ! It is important to keep the data in time order\n            rows = np.random.randint(min_index + lookback, max_index-delay-1, size=batch_size) # return an array containing size elements ranging from min_index+lookback to max_index\n        else:\n            if i + batch_size >= max_index-delay-1: #Since we are incrementing on \"i\". If its value is greater than the max_index --> start from the begining\n                i = min_index + lookback # We need to start from the indice lookback, since we want to take lookback elements here.\n            rows = np.arange(i, min(i + batch_size, max_index)) # Just creating an array that contain the indices of each sample in the batch\n            i+=len(rows) # rows represents the number of sample in one batch\n            \n        samples = np.zeros((len(rows), lookback\/\/step, data.shape[-1])) # shape = (batch_size, lookback, nbr_of_features)\n        targets = np.zeros((len(rows),)) #Shape = (batch_size,)\n        \n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step) #From one indice given by rows[j], we are picking loockback previous elements in the dataset\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1] #We only want to predict the temperature for now,since [1], the second column\n        yield samples, targets # The yield that replace the return to create a generator and not a regular function.","2ebaa793":"data_train = train_df.to_numpy()\ntrain_gen = generator(data = data_train, lookback = lookback, delay =delay, min_index = 0, max_index = len(data_train), shuffle = True, batch_size = batch_size)\n\ndata_val = val_df.to_numpy()\nval_gen = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0, max_index = len(data_val), batch_size = batch_size)\n\ndata_test = test_df.to_numpy()\ntest_gen = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0, max_index = len(data_test), batch_size = batch_size)","60ffb36d":"train_gen # It is a generator","12ec7b7d":"print(next(iter(train_gen))[0].shape) # Here we picked one batch of samples","14d271b2":"print(next(iter(train_gen))[1].shape)","3b01de09":"def naive_eval_gen(generator):\n    batch_maes = []\n    for step in range(len(data_val)-lookback):\n        samples, targets = next(generator)\n        preds = samples[:,-1,1] #For all elements in the batch, we are saying the prediction of temperature is equal to the last temperature recorded within the 48 hours\n        mae = np.mean(np.abs(preds - targets))\n        batch_maes.append(mae)\n    return(np.mean(batch_maes))\n\nnaive_loss_gen = round(naive_eval_gen(val_gen),2)\nprint(naive_loss_gen)","635150e3":"model = Sequential([\n    Flatten(input_shape=(lookback, 19)),\n    Dense(32, activation='relu'),\n    Dense(1)\n])","6e5d9555":"model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mae')\nhistory = model.fit_generator(train_gen, epochs = 30, steps_per_epoch = data_train.shape[0]\/\/batch_size, validation_data = val_gen, validation_steps = data_val.shape[0]\/\/batch_size)","0a25deb0":"plot(history, naive_loss_gen)","3976ab53":"train_df_it = tf.data.Dataset.from_tensor_slices(data_train) # *from_tensor_slices* is the function to call to turn numpy array to tf.data.Dataset","365f8e25":"train_df_it # It is a Dataset that contains elements of shape (19,)","c2fd11ca":"for element in train_df_it:\n    print(element)\n    break # Use break if you don't want to see the whole dataset","c59bde4b":"for element in train_df_it.as_numpy_iterator():\n    print(element)\n    break","7dec045f":"train_it = train_df_it.window(size = window_length, shift = 1, drop_remainder = True)\ntrain_it = train_it.flat_map(lambda window: window.batch(window_length))\ntrain_it = train_it.map(lambda windows: (windows[:48,:], windows[-1,1]))","8b2a744b":"train_df_it = tf.data.Dataset.from_tensor_slices(data_train)\nval_df_it = tf.data.Dataset.from_tensor_slices(data_val)\n\n\n# window(size, shift=None, stride=1, drop_remainder=False)\ntrain_it = train_df_it.window(size = window_length, shift = 1, drop_remainder = True)\ntrain_it = train_it.flat_map(lambda window: window.batch(window_length))\ntrain_it = train_it.shuffle(buffer_size = df.shape[0]).batch(batch_size)\ntrain_it = train_it.map(lambda windows: (windows[:,:48,:], windows[:,-1,1]))\n\n#train_it = train_it.map(lambda windows: (windows[:,:48,:], windows[:,-1,1]))\n\n\nval_it = val_df_it.window(window_length, shift = 1, drop_remainder = True)\nval_it = val_it.flat_map(lambda window: window.batch(window_length))\nval_it = val_it.shuffle(buffer_size = df.shape[0]).batch(batch_size)\nval_it = val_it.map(lambda windows: (windows[:,:48,:], windows[:,-1,1]))\n\n#val_it = val_it.map(lambda windows: (windows[:,:48,:], windows[:,-1,1]))\n","79addd71":"def naive_eval_it(iterator):\n    batch_maes = []\n    for tupple in iterator.as_numpy_iterator():\n        samples, targets = tupple[0], tupple[1]\n        for sample, target in zip(samples, targets):\n            preds = sample[-1,1]\n            mae = np.mean(np.abs(preds - target))\n            batch_maes.append(mae)\n    return(np.mean(batch_maes))\n\nnaive_loss_it = round(naive_eval_it(val_it),2)\nprint(naive_loss_it)","52a01de4":"model = Sequential([\n    Flatten(input_shape=(lookback, 19)),\n    Dense(32, activation='relu'),\n    Dense(1)\n])","9eb3f08f":"model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')\nhistory = model.fit(train_it, epochs = 30, validation_data = val_it)","47ef25f3":"plot(history, naive_loss_it)","5613a215":"lookback = 48\ndelay = 24\nwindow_length = lookback + delay\nbatch_size = 32\nbuffer_size = len(train_df)","201fcf72":"# Still using these:\nprint(\"train_df shape is: {}\".format(train_df.shape))\nprint(\"val_df shape is: {}\".format(val_df.shape))\nprint(\"test_df shape is: {}\".format(test_df.shape))","d13a95b3":"def create_arr_dataset(data, lookback, delay):\n    data = data.to_numpy()\n    X, y = [], []\n    for i in range(lookback, len(data)-delay):\n        X.append(data[i-lookback: i,:])\n        y.append(data[i:i+delay, 1])\n    X = np.array(X)\n    y = np.array(y)\n    return(X, y)","b532ba20":"X_train, y_train = create_arr_dataset(train_df, lookback = lookback, delay = delay)\nX_val, y_val = create_arr_dataset(val_df, lookback = lookback, delay = delay)","3732f06e":"print(\"X_train shape is: {}, and y_train shape is: {}\".format(X_train.shape, y_train.shape))\nprint(\"X_val shape is: {}, and y_val shape is: {}\".format(X_val.shape, y_val.shape))","96ad638a":"def naive_loss_arr2(X, y, lookback, delay):\n    batch_maes, batch_rmses = [],  []\n    for sample, target in zip(X, y):\n        preds = []\n        for i in range(delay):\n            preds.append(sample[-1,1]) # For the next 8 hours, we predict the value to be the same as the one in the last timestep of the sample\n        preds = np.array(preds)\n        mae = np.mean(np.abs(preds - target))\n        rmse = np.sqrt(np.mean((preds - target)**2))\n        batch_maes.append(mae)\n        batch_rmses.append(rmse)\n    return(np.mean(batch_maes), np.mean(batch_rmses))\n\nnaive_loss_arr2_mae, naive_loss_arr2_rmse = naive_loss_arr2(X_val, y_val, lookback = lookback, delay= delay)\n\nnaive_loss_arr2_mae = round(naive_loss_arr2_mae,2)\nnaive_loss_arr2_rmse = round(naive_loss_arr2_rmse,2)\n\nprint(\"mae is: {}, rmse is: {}\".format(naive_loss_arr2_mae, naive_loss_arr2_rmse))","6d335237":"model = Sequential([\n    Flatten(input_shape=(lookback, 19)),\n    Dense(32, activation='relu'),\n    Dense(delay) # Pay attention, now we want to predict the temperature for the next 8 hours\n])","a6339889":"model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mae')\nhistory = model.fit(X_train, y_train, epochs = 30, validation_data = (X_val, y_val))","8c606989":"plot(history, naive_loss_arr2_mae)","fb9c8b12":"def generator_2(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=32, step=1):\n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    while True:\n        if shuffle == True: # pay attention ! We are not shuffeling timesteps but elemnts within a batch ! It is important to keep the data in time order\n            rows = np.random.randint(min_index + lookback, max_index-delay-1, size=batch_size) # return an array containing size elements ranging from min_index+lookback to max_index\n        else:\n            if i + batch_size >= max_index-delay-1: #Since we are incrementing on \"i\". If its value is greater than the max_index --> start from the begining\n                i = min_index + lookback # We need to start from the indice lookback, since we want to take lookback elements here.\n            rows = np.arange(i, min(i + batch_size, max_index)) # Just creating an array that contain the indices of each sample in the batch\n            i+=len(rows) # rows represents the number of sample in one batch\n            \n        samples = np.zeros((len(rows), lookback\/\/step, data.shape[-1])) # shape = (batch_size, lookback, nbr_of_features)\n        targets = np.zeros((len(rows),delay)) #Shape = (batch_size,delay)\n        \n        for j, row in enumerate(rows): # We loop here for batch_size ie 32 loops\n            indice_samples = range(rows[j] - lookback, rows[j], step) #From one indice given by rows[j], we are picking loockback previous elements in the dataset\n            indice_targets = range(rows[j], rows[j]+delay, step)\n            samples[j] = data[indice_samples]\n            targets[j] = data[:,1][indice_targets] #We only want to predict the temperature for now,since [1], the second column\n        yield samples, targets # The yield that replace the return to create a generator and not a regular function.","be566277":"data_train = train_df.to_numpy()\ntrain_gen = generator_2(data = data_train, lookback = lookback, delay =delay, min_index = 0, max_index = len(data_train), shuffle = True, batch_size = batch_size)\n\ndata_val = val_df.to_numpy()\nval_gen = generator_2(data = data_val, lookback = lookback, delay =delay, min_index = 0, max_index = len(data_val), batch_size = batch_size)\n\ndata_test = test_df.to_numpy()\ntest_gen = generator_2(data = data_val, lookback = lookback, delay =delay, min_index = 0, max_index = len(data_test), batch_size = batch_size)","9cd76097":"train_gen","771263f8":"def naive_loss_gen2(generator):\n    batch_maes, batch_rmses = [],  []\n\n    for batch_number in range(0, 500):\n        batch_sample, batch_target = next(generator)\n\n        preds = np.zeros((batch_size, delay))\n        for j, sample in enumerate(batch_sample):\n            for i in range(0, delay):\n                preds[j,i] = sample[-1,1]\n\n            mae = np.mean(np.abs(preds[j] - batch_target))\n            rmse = np.sqrt(np.mean((preds[j] - batch_target)**2))\n    \n            batch_maes.append(mae)\n            batch_rmses.append(rmse)\n    \n    return(np.mean(batch_maes), np.mean(batch_rmses))\n\nnaive_loss_gen2_mae, naive_loss_gen2_rmse = naive_loss_gen2(val_gen)\n\nnaive_loss_gen2_mae = round(naive_loss_gen2_mae,2)\nnaive_loss_gen2_rmse = round(naive_loss_gen2_rmse,2)\n\nprint(\"mae is: {}, rmse is: {}\".format(naive_loss_gen2_mae, naive_loss_gen2_rmse))","6f2f09ec":"model = Sequential([\n    Flatten(input_shape=(lookback, 19)),\n    Dense(32, activation='relu'),\n    Dense(delay)\n])","f5df5eda":"train_gen2 = generator_2(data = data_train, lookback = lookback, delay =delay, min_index = 0, max_index = len(data_train), shuffle = True, batch_size = batch_size)\nval_gen2 = generator_2(data = data_val, lookback = lookback, delay =delay, min_index = 0, max_index = len(data_val), batch_size = batch_size)\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss='mae')\nhistory = model.fit_generator(train_gen2, epochs = 30, steps_per_epoch = data_train.shape[0]\/\/batch_size, validation_data = val_gen2, validation_steps = data_val.shape[0]\/\/batch_size)","cff3c2b2":"plot(history, naive_loss_gen2_mae)","24fefa33":"data_train = train_df.to_numpy()\ndata_val = val_df.to_numpy()\ndata_test = test_df.to_numpy()\n\ntrain_df_it = tf.data.Dataset.from_tensor_slices(data_train)\nval_df_it = tf.data.Dataset.from_tensor_slices(data_val)\ntest_df_it = tf.data.Dataset.from_tensor_slices(data_test)\n\n# window(size, shift=None, stride=1, drop_remainder=False)\ntrain_it = train_df_it.window(size = window_length, shift = 1, drop_remainder = True)\ntrain_it = train_it.flat_map(lambda window: window.batch(window_length))\ntrain_it = train_it.shuffle(buffer_size = df.shape[0]).batch(batch_size)\ntrain_it = train_it.map(lambda windows: (windows[:,:lookback,:], windows[:,lookback:lookback+delay,1]))\n\n\nval_it = val_df_it.window(window_length, shift = 1, drop_remainder = True)\nval_it = val_it.flat_map(lambda window: window.batch(window_length))\nval_it = val_it.shuffle(buffer_size = df.shape[0]).batch(batch_size)\nval_it = val_it.map(lambda windows: (windows[:,:lookback,:], windows[:,lookback:lookback+delay,1]))\n\ntest_it = test_df_it.window(window_length, shift = 1, drop_remainder = True)\ntest_it = test_it.flat_map(lambda window: window.batch(window_length))\ntest_it = test_it.shuffle(buffer_size = df.shape[0]).batch(batch_size)\ntest_it = test_it.map(lambda windows: (windows[:,:lookback,:], windows[:,lookback:lookback+delay,1]))","669b40c5":"print(next(iter(val_it))[0].shape)\nprint(next(iter(val_it))[1].shape)","535760a1":"def naive_eval_it2(iterator):\n    batch_maes, batch_rmses = [],  []\n    for tupple in iterator.as_numpy_iterator():\n        samples, targets = tupple[0], tupple[1]\n        for sample, target in zip(samples, targets):\n            preds = []\n            for i in range(0,delay):\n                preds.append(sample[-1,1])\n            preds = np.array(preds)\n            \n            mae = np.mean(np.abs(preds - target))\n            rmse = np.sqrt(np.mean((preds - target)**2))\n    \n            batch_maes.append(mae)\n            batch_rmses.append(rmse)\n    return(np.mean(batch_maes), np.mean(batch_rmses))\n\nnaive_loss_it2_mae, naive_loss_it2_rmse = naive_eval_it2(val_it)\n\nnaive_loss_it2_mae = round(naive_loss_it2_mae,2)\nnaive_loss_it2_rmse = round(naive_loss_it2_rmse,2)\n\nprint(\"mae is: {}, rmse is: {}\".format(naive_loss_it2_mae, naive_loss_it2_rmse))","ed40172c":"model_linear = Sequential([\n    Flatten(input_shape=(lookback, 19)),\n    Dense(32, activation='relu'),\n    Dense(delay)\n])","b5f2d605":"model_linear.compile(optimizer=tf.keras.optimizers.Adam(), loss='mae')\nhistory_linear = model_linear.fit(train_it, epochs = 30, validation_data = val_it)","940712e6":"plot(history_linear, naive_loss_it2_mae)","1a453408":"from keras.layers import LSTM, GRU, SimpleRNN","ac6f84d8":"model_LSTM = Sequential([\n    LSTM(30, return_sequences = True, input_shape = [None, 19], dropout=0.2, recurrent_dropout = 0.2),\n    LSTM(30, dropout=0.2, recurrent_dropout = 0.2),\n    Dense(delay)\n])","ab86284e":"model_LSTM.compile(optimizer=tf.keras.optimizers.Adam(), loss='mae')\nhistory_LSTM = model_LSTM.fit(train_it, epochs = 30, validation_data = val_it)","614c2268":"plot(history_LSTM, naive_loss_it2_mae)","1e881f03":"tupple_test = test_it.as_numpy_iterator()\ntupple_test = next(tupple_test)\nbatch_sample_test, batch_target_test = tupple_test \nfor sample_test, target_test in zip(batch_sample_test, batch_target_test):\n    \n    # Arvesting data for the reals points\n    sample_points, target_points = sample_test[:,1], target_test\n    \n    # Arvesting data for the naive model\n    naive_points = np.zeros(delay)\n    for i in range(delay):\n        naive_points[i] = sample_test[-1,1]\n    \n    # Arvesting data for the linear evaluation\n    ypreds_linear = model_linear.predict(batch_sample_test)\n    linear_points = ypreds_linear[0,:]\n    \n    # Arvesting data for the LSTM evaluation\n    ypreds_LSTM = model_LSTM.predict(batch_sample_test)\n    LSTM_points = ypreds_LSTM[0,:]\n    break","7de7afc5":"real_points = np.concatenate((sample_points, target_points), axis=None)\nnaive_points = np.concatenate((sample_points, naive_points), axis=None)\nlinear_points = np.concatenate((sample_points, linear_points), axis=None)\nLSTM_points = np.concatenate((sample_points, LSTM_points), axis=None)","5afe94ff":"epochs = range(1, window_length+1)\nplt.figure()\nplt.plot(epochs, real_points, 'bo', label='Real data')\nplt.plot(epochs, naive_points, 'b', label='Naive evaluation')\nplt.plot(epochs, linear_points, 'r', label='Linear evaluation')\nplt.plot(epochs, LSTM_points, 'g', label='LSTM evaluation')\nplt.title('Compararing the evaluation from different models')\n#plt.axhline(y=naive_loss, color ='r')\n\nplt.legend()\nplt.show()","2da7961e":"def create_arr_dataset(data, lookback, delay):\n    data = data.to_numpy()\n    X = np.zeros(data.shape[0], lookback, data.shape[1])\n    y = np.zeros(data.shape[0], lookback, delay)\n    for i in range(lookback, len(data)-delay):\n        X[i] = data[i-lookback:i,:]\n        for j in range(lookback):\n            y[i,j] = data[i-lookback + j : i + j + delay,1]\n    return(X, y)","62a151f1":"from keras.layers import MaxPooling1D, Conv1D, GlobalMaxPooling1D, Dense, GRU, LSTM, SimpleRNN","03bee35f":"model_CNN = Sequential([\n    Conv1D(filters = 32, kernel_size = 5, activation='relu', input_shape = input_shape[1:]),\n    MaxPooling1D(3),\n    Conv1D(filters = 32, kernel_size = 5, activation='relu'),\n    GlobalMaxPooling1D(),\n    Dense(delay)\n])","202e9c68":"model_CNN.compile(optimizer=tf.keras.optimizers.Adam(), loss='mae')\nhistory_CNN = model_CNN.fit(train_it, batch_size = batch_size, steps_per_epoch = len(train_df)\/\/batch_size, epochs = 30, validation_data = val_it)","be40e48b":"loss = history_CNN.history['loss']\nval_loss = history_CNN.history['val_loss']\n\nepochs = range(1, len(loss)+1)\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\n\nplt.legend()\nplt.show()","1cec063b":"input_shape = (batch_size, lookback, 19)","4b21f836":"#tf.keras.layers.Conv1D(filters,kernel_size,strides=1,padding=\"valid\")\n\n\nmodel_mix = Sequential([\n    Conv1D(filters = 20, kernel_size = 4,strides = 2, padding = 'valid', input_shape = input_shape[1:]),\n    GRU(20, return_sequences = True, dropout = 0.1, recurrent_dropout = 0.1),\n    GRU(20, dropout = 0.1, recurrent_dropout = 0.1),\n    Dense(delay)\n])","8bea9d07":"model_mix.compile(optimizer=tf.keras.optimizers.Adam(), loss='mae')\nhistory_mix = model_mix.fit(train_it, batch_size = batch_size, epochs = 15, validation_data = val_it)","ff0ae2f0":"tupple_test = test_it.as_numpy_iterator()\ntupple_test = next(tupple_test)\nbatch_sample_test, batch_target_test = tupple_test \nfor sample_test, target_test in zip(batch_sample_test, batch_target_test):\n    \n    # Data for the reals points\n    sample_points, target_points = sample_test[:,1], target_test\n    \n    # Data for the LSTM evaluation\n    ypreds_mix = model_mix.predict(batch_sample_test)\n    mix_points = ypreds_mix[0,:]\n    break","6c318caf":"real_points = np.concatenate((sample_points, target_points), axis=None)\nmix_points = np.concatenate((sample_points, mix_points), axis=None)\n\nepochs = range(1, window_length+1)\nplt.figure()\nplt.plot(epochs, real_points, 'bo', label='Real data')\nplt.plot(epochs, mix_points, 'g', label='conv1D + GRU layers evaluation')\nplt.title('Compararing the evaluation from different models')\n\nplt.legend()\nplt.show()","34ccb85a":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=0, mode='auto',min_lr=0.0001)\n\ncallbacks = [reduce_lr]\n\nmodel_mix = Sequential([\n    Conv1D(filters = 20, kernel_size = 4,strides = 2, padding = 'valid', input_shape = input_shape[1:], use_bias=True),\n    GRU(32, return_sequences = True, dropout = 0.1, recurrent_dropout = 0.1),\n    GRU(32, dropout = 0.1, recurrent_dropout = 0.1),\n    Dense(delay)\n])\n\nmodel_mix.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss='mae')\nhistory_mix = model_mix.fit(train_it, batch_size = batch_size, epochs = 15, validation_data = val_it, callbacks = callbacks)","430700d2":"At this point, it will be usefull to have a clear idea on what the conv1D layer does, first, lets see the arguments: **tf.keras.layers.Conv1D(filters,kernel_size,strides=1,padding=\"valid\")** where:\n- filters: Nbr of output filters\n- kernel_size: length of the window\n- strides: the number of steps we take when moving from one window to another. If greater than one, then the output will be way shorter (less timesteps).\n- padding: \"valid\" means no padding --> We do not ask the layer to build build artifical timesteps at the left and right of our whole sequence. As a resultn the first output of the Conv1D layer will be based on the input shape [0;kernel_size]. Because the *padding* is not \"same\" the output will be a bit shorter than initial (we are loosing the first two and last two timesteps).","929c1b08":"# Forecasting temperature the next hour given the 48 previous features","09498c8e":"We can see that the simple Dense model performed better than the naive one. However, we barely are reaching 0.28","b85abfde":"## Generator","e7343678":"## Preparing the data\n\nThe aim here is to build model that will predict future values. According to TensorFlow website, data that can be fed to a model can have be the following type:\n- arrays\n- iterators\n- generatos\n\nWe'll explore all of these types, through various model.","2dec90cb":"With this graph we can easily understand why the **mae** and **rmse** are very high compared to the one calculated for the linear evaluation. Moreover, we can clearly see that it won't be esay to make a model that perform better than the linear one on this kind of problem.","b56c359a":"This next part is very important !\n\nTo have insight on how well our models will perform, we need a reference. The usual way to do it is to evaluate the performance on a model that predict the wanted value as the last value of the train set. Let's say the temperature of today is 28\u00b0C, the prediction for tommorow, given this model, will be 28\u00b0C. We do this for all samples and evaluate the error on this. This is call a *naive evaluation*.\n\nThis might sounds stupid be you will that it is not an easy thing to beat.","74fafd2c":"# TensorFlow time series, sequences and predictions.\nThis notebook is the second part of my \"TensorFlow certification guide\" kernels. You will find the first part focusing on Natural Language processing (NPL) [here](https:\/\/www.kaggle.com\/foucardm\/tensorflow-certification-guide-text-data).\n\nAs usual, you will find the TensorFlow Developer Certificate handbook [here](https:\/\/www.tensorflow.org\/extras\/cert\/TF_Certificate_Candidate_Handbook.pdf) but everything related to the subject is detailed below.\n\nThis part will be focused on time series and forecasting methods using both CNN and RNN. More precisely, all the following points will be detailed:\n- Train, tune and use time series, sequence and prediction models;\n- Prepare data for time series learning;\n- Understand Mean Average Error (MAE) and how it can be used to evaluate accuracy of sequence models;\n- Use RNNs and CNNs for time series, sequnece and forecasting models;\n- Identify when to use trainling versus centred windows;\n- Use TensorFlow for forecasting;\n- Prepare features and lables;\n- Identify and compensate for sequence bias;\n- Adjust the learning rate dynamically in time series, sequence and prediction models.\n\nWe will be using a dataset containing weather information recorded every 10 minutes over several years. There are many features we will explore and as you guessed it, we'll try to predict the weather !","5b5194d2":"## Generator\n\nIn TensorFlow, a generator is a tf.Dataset whose elements are generated by a generator (but all tf.Dataset are not generators !). So what is a generator ?? Basically, it is a Python function that returns an infinite sequence of training data (generaly the data is a tupple (X,y)). To build such function, you need to use **yield** at the end of it and not **return**.","ac23675b":"## Arrays","58a7cd30":"Each element within the dataset are tf.Tensor and not numpy arrays:","95b98f60":"There is no use spending to much time on the next code. It is data preprocessing and it is not the goal of this kernel.","e175aa63":"To access the values within one specific tensor you have to use this trick: use *as_numpy_iterator*","4ad13674":"## Iterator\nA tf.Dataset is an iterator. It is way different from a pandas dataframe since you cannot directly access the data.","58018862":"# Preparing Data for timeseries forecasting","b02b5c51":"As we can see, this model performed really badly on the data. Let's see if it is the case with all kind of input data we can feed to the model","f37c0327":"This is the accuracy of the naive model, we can now try to beat it with more sophiticated ones.","60105615":"Just like before, and because it is a good exercice, we will create a nice model to have our fisrt evaluation of accuracy.","2625057e":"## Arrays","0a718cb0":"## Combining CNN and RNN\nLet's first see how to use a CNN model for timeseries forecasting.","aa0eaf10":"## Iterator","a1533742":"Just like before, we need a reference for the next evaluations.","0e28a4ab":"# Forecasting the temperature for the next 8 hours given the 48 previous hours","6f2bc83d":"Ok now, we have to make this data ready to be fed into our model:","ef12d48f":"## Time to build a powerfull model"}}