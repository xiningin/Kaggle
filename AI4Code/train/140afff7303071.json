{"cell_type":{"c589bbb7":"code","be7988ae":"code","97307e7c":"code","19bd2735":"code","bc3d92ba":"code","cd559878":"code","0aedd9b2":"code","10bc53ef":"code","42258b5e":"code","0620c045":"code","c27d49a7":"code","c456885d":"code","f193c9ad":"code","6b26ea2e":"code","2478c526":"code","298fff0b":"code","0d674b6b":"code","0c119045":"code","96b86f04":"code","22542712":"markdown","5750c64b":"markdown","a0a5e769":"markdown","5e2d18e1":"markdown","f2e09309":"markdown","b70c08ae":"markdown","8a3986e7":"markdown","4c772087":"markdown","b43870cc":"markdown","93d9f5db":"markdown","b0258300":"markdown","5f92674c":"markdown"},"source":{"c589bbb7":"import os \nimport cv2\nimport numpy as np \nimport pandas as pd\nimport csv\nimport datetime\nimport random\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.utils import shuffle\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedStratifiedKFold, cross_val_score, KFold,StratifiedKFold \nimport pyprind\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","be7988ae":"data = []\ndatetime_object = datetime.date.today() #I need it to save file bc I will train a lot \nprint ('datetime_object',datetime_object)\ndata = pd.read_csv('..\/input\/vietnamese-handwritten-ocr\/aug_word_data4.csv')\nprint ('data',data.head(4))\n","97307e7c":"labels = []\nimages = []\n\nlabels = data.iloc[: ,0].to_list() #If you want to take 100 image to test, use data.iloc[: 100 ,0].to_list()\nimages = data.iloc[: ,1].to_list()\nprint ('\\nThe number of data', len(labels))\nmax_str_len = max([len(str(label)) for i,label in enumerate(labels)])\nprint ('max_str_len',max_str_len)\n\ndata = [] #This code here for not exploding the RAM","19bd2735":"count = 0\nfor i, label in enumerate(labels):\n    if type(label) == float:\n        del labels[i]\n        del images[i]\n        count +=1\nprint ('The number of invalid data:',count)\nprint ('The number of valid data left:',len(labels))","bc3d92ba":"t = []\nfor i,image in enumerate(images):\n    image = np.array(image.split(' '), dtype = float)\n    t.append(image)\nimages = []","cd559878":"images = t\nt = []\nprint ('len images', len(images))","0aedd9b2":"images = np.array(images).reshape(-1, 128, 32, 1)","10bc53ef":"X_train, X_valid, y_train, y_valid = train_test_split(images, labels, train_size= 0.85, shuffle = True)\n\nimages = []\nlabels = [] #This code here for not exploding the RAM\n\nprint ('\\nlen(X_train)',len(X_train))\nprint ('len(X_valid)',len(X_valid))\nprint ('\\n X_train.shape',X_train.shape)\nprint ('\\n X_valid.shape',X_valid.shape)","42258b5e":"plt.figure(num='char',figsize=(9,18))\nfor i in range(6):\n    rand = random.randint(0, len(X_train))\n    plt.subplot(3,3,i+1) \n    plt.title(y_train[rand])\n    plt.imshow(np.squeeze(X_train[rand,:,:,]))\n    plt.axis('off')\nplt.show()","0620c045":"alphabets = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYabcdefghijklmnopqrstuvxyz\u00c0\u00c1\u00c2\u00d4\u00da\u00dd\u00e0\u00e1\u00e2\u00e3\u00e8\u00e9\u00ea\u00ec\u00ed\u00f2\u00f3\u00f4\u00f5\u00f9\u00fa\u00fd\u0102\u0103\u0110\u0111\u0129\u0169\u0192\u01a0\u01a1\u01af\u01b0\u1ea1\u1ea2\u1ea3\u1ea4\u1ea5\u1ea6\u1ea7\u1ea9\u1eab\u1ead\u1eaf\u1eb1\u1eb3\u1eb5\u1eb7\u1eb9\u1ebb\u1ebd\u1ebf\u1ec0\u1ec1\u1ec2\u1ec3\u1ec5\u1ec6\u1ec7\u1ec9\u1ecb\u1ecd\u1ecf\u1ed0\u1ed1\u1ed2\u1ed3\u1ed5\u1ed7\u1ed9\u1edb\u1edd\u1ede\u1edf\u1ee1\u1ee3\u1ee5\u1ee6\u1ee7\u1ee9\u1eeb\u1eed\u1eef\u1ef1\u1ef3\u1ef7\u1ef9'\nprint ('the number of characters:', len(alphabets))\n#max_str_len = 15 # max length of input labels\nnum_of_characters = len(alphabets) + 1 # +1 for ctc pseudo blank\nnum_of_timestamps = 31  # max length of predicted labels # \u0110\u1eb7t num_of_timestamps <= shape(last Dense model)\n\n\ndef label_to_num(label):\n    label_num = []\n    for ch in label:\n        label_num.append(alphabets.find(ch))\n        \n    return np.array(label_num)\n\ndef num_to_label(num):\n    ret = \"\"\n    for ch in num:\n        if ch == -1:  # CTC Blank\n            break\n        else:\n            ret+=alphabets[ch]\n    return ret","c27d49a7":"train_y = np.ones([len(X_train), max_str_len]) * -1\ntrain_label_len = np.zeros([len(X_train), 1])\ntrain_input_len = np.ones([len(X_train), 1]) * (num_of_timestamps-2)\ntrain_output = np.zeros([len(X_train)])\n\nfor i in range(len(X_train)):\n    train_label_len[i] = len(y_train[i])\n    train_y[i, 0:len(y_train[i])]= label_to_num(y_train[i])  \n\nprint ('len train_y',len(train_y))","c456885d":"valid_y = np.ones([len(X_valid), max_str_len]) * -1\nvalid_label_len = np.zeros([len(X_valid), 1])\nvalid_input_len = np.ones([len(X_valid), 1]) * (num_of_timestamps-2)\nvalid_output = np.zeros([len(X_valid)])\n\nfor i in range(len(X_valid)):\n    valid_label_len[i] = len(y_valid[i])\n    valid_y[i, 0:len(y_valid[i])]= label_to_num(y_valid[i])  \n    \nprint ('len valid_y', len(valid_y))\n#print('\\n True label_train  : ',y_train[10] , '\\ntrain_y : ',train_y[10],'\\ntrain_label_len : ',train_label_len[10], '\\ntrain_input_len : ', train_input_len[10])","f193c9ad":"def build_word_model(alphabets, max_str_len, img_width = 128,img_height = 32):\n    # Inputs to the model\n\n    input_img = layers.Input(\n        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n    )\n    \n    # First conv block\n    x = layers.Conv2D(\n        64,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv1\",\n    )(input_img)\n    x = layers.MaxPooling2D((2, 2),strides = 2, name=\"pool1\")(x)\n\n    # Second conv block\n    x = layers.Conv2D(\n        128,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv2\",\n    )(x)\n    x = layers.MaxPooling2D((2, 2), strides = 2, name=\"pool2\")(x)\n\n    # Third conv block\n    x = layers.Conv2D(\n        256,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv3\",\n    )(x)\n  \n\n    # Fourth conv block\n    x = layers.Conv2D(\n        256,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv4\",\n    )(x)\n\n    x = layers.MaxPooling2D((1, 2), name=\"pool4\")(x)\n\n    # Fifth conv block\n    x = layers.Conv2D(\n        512,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv5\",\n    )(x)\n\n    x = layers.BatchNormalization(momentum = 0.8, name=\"BatchNormalization_1\")(x)\n    \n\n    # Sixth conv block\n    x = layers.Conv2D(\n        512,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv6\",\n    )(x)\n\n    x = layers.BatchNormalization(momentum = 0.8, name=\"BatchNormalization_2\")(x)\n\n    x = layers.MaxPooling2D((1, 2), name=\"pool6\")(x)\n\n    # Seventh conv block\n    x = layers.Conv2D(\n        512,\n        (2, 2),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"valid\",\n        name=\"Conv7\",\n    )(x)\n\n\n    # The number of filters in the last layer is 512. Reshape accordingly before\n    # passing the output to the RNN part of the model\n\n    new_shape = (31,512) #Kh\u00f4ng c\u1ea7n downsampling #N\u00ean coi shape l\u1edbp tr\u01b0\u1edbc\n\n    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n\n    #x = layers.Dense(512, activation=\"relu\", name=\"dense1\")(x)\n    #x = layers.Dense(256, activation=\"relu\", name=\"dense2\")(x)\n  \n    #x = layers.Dropout(0.2)(x)\n    \n    def attention_rnn(inputs):\n        # inputs.shape = (batch_size, time_steps, input_dim)\n        input_dim = int(inputs.shape[2])\n        timestep = int(inputs.shape[1])\n        a = layers.Permute((2, 1))(inputs) #Permutes the dimensions of the input according to a given pattern.\n        a = layers.Dense(timestep, activation='softmax')(a) #\/\/ Alignment Model + Softmax\n        a = layers.Lambda(lambda x: keras.backend.mean(x, axis=1), name='dim_reduction')(a)\n        a = layers.RepeatVector(input_dim)(a)\n        a_probs = layers.Permute((2, 1), name='attention_vec')(a)\n        output_attention_mul = layers.multiply([inputs, a_probs], name='attention_mul') #\/\/ Weighted Average \n        return output_attention_mul\n\n    x = attention_rnn(x)\n    # RNNs\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n\n    # Output layer\n    y_pred = layers.Dense(len(alphabets) + 1 , activation=\"softmax\", name=\"last_dense\")(x) # y pred\n    word_model = keras.models.Model(inputs=input_img, outputs=y_pred, name=\"functional_1\")\n\n    def ctc_lambda_func(args):\n        y_pred, labels, input_length, label_length = args\n        # the 2 is critical here since the first couple outputs of the RNN\n        # tend to be garbage\n        y_pred = y_pred[:, 2:, :]\n        return tf.keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n\n    labels = layers.Input(name='gtruth_labels', shape=[max_str_len], dtype='float32')\n    input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n    label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n\n    ctc_loss = keras.layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n    word_model_CTC = keras.models.Model(inputs=[input_img, labels, input_length, label_length], outputs=ctc_loss, name = \"ocr_model_v1\")\n    \n    return word_model, word_model_CTC\n\nword_model, word_model_CTC = build_word_model(alphabets = alphabets, max_str_len = max_str_len)\nword_model_CTC.summary()","6b26ea2e":"epochs = 100\nbatch_size = 128\nearly_stopping_patience = 20\n\ndef scheduler(epoch):\n    if epoch <= 25:\n        return 1e-3  \n    elif 25 < epoch <= 30:\n        return 1e-4\n    else:\n        return 1e-5\n\n# Add early stopping\nmy_callbacks = [\n    tf.keras.callbacks.LearningRateScheduler(scheduler),\n    tf.keras.callbacks.ModelCheckpoint(filepath='.\/model_word\/word_model_{epoch:02d}_'+str(datetime_object)+'.h5', \n                                    save_freq='epoch',\n                                    monitor='val_loss',\n                                    mode='auto',\n                                    save_best_only=True,\n                                    period = 5),\n    tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n    )\n]\n\nword_model_CTC.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, \n                    optimizer=keras.optimizers.Adam(),                  \n                    )\n\nhistory = word_model_CTC.fit(x=[X_train, train_y, train_input_len, train_label_len], y=train_output, \n                validation_data=([X_valid, valid_y, valid_input_len, valid_label_len], valid_output),\n                epochs = epochs, \n                batch_size = batch_size,\n                callbacks = my_callbacks,\n                )\n\n\n# list all data in history\nprint(history.history.keys())\n\n# summarize history for loss\n\nfig, ax = plt.subplots()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.savefig('.\/model_word\/word_model_{}_loss.png'.format(datetime_object))\nplt.show()\n\n\nwith open('.\/model_word\/word_model_{}.txt'.format(datetime_object), 'w', encoding='utf-8') as f:\n    f.write('len(X_train): {} \\nlen (X_valid): {} \\n'.format(len(X_train), len (X_valid)))\n    f.write('max_str_len: {} \\nnum_of_characters: {} \\nnum_of_timestamps: {} \\n'.format(max_str_len,num_of_characters,num_of_timestamps))\n    f.write('batch_size: {} \\nepochs: {} \\n'.format(batch_size,epochs) )","2478c526":"word_model.save('.\/model_word\/word_model_last.h5')","298fff0b":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) \/ float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    # use O(min(m, n)) space\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    # initialize distance matrix\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    # calculate levenshtein distance\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n  \n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) \/ ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n \n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) \/ ref_len\n    return cer","0d674b6b":"preds = word_model.predict(X_valid)\n#print('\\n preds',preds)\ndecoded = tf.keras.backend.get_value(tf.keras.backend.ctc_decode(preds, input_length=np.ones(preds.shape[0])*preds.shape[1], \n                                   greedy=True)[0][0])\n#print ('\\n decoded',decoded)\nprediction = []\nfor i in range(len(X_valid)):\n    prediction.append(num_to_label(decoded[i]))\n    \n#print ('\\n predict',num_to_label(decoded[0]))\n\ny_true = y_valid\ncorrect_char = 0\ntotal_char = 0\ncorrect = 0\ntest_cer, test_wer = [], []\n\nfor i in range(len(X_valid)):\n    pr = prediction[i]\n    tr = y_true[i]\n    total_char += len(tr)\n    \n    for j in range(min(len(tr), len(pr))):\n        if tr[j] == pr[j]:\n            correct_char += 1\n            \n    if pr == tr :\n        correct += 1 \n    \n    test_cer.append(cer(tr, pr))\n    test_wer.append(wer(tr, pr))\n                  \navg_cer = sum(test_cer)\/len(test_cer)   \navg_wer = sum(test_wer)\/len(test_wer)\n\nprint ('Average CER: %.2f%%' %(avg_cer*100))\nprint ('Average WER: %.2f%%' %(avg_wer*100))\nprint('Correct characters predicted : %.2f%%' %(correct_char*100\/total_char))\nprint('Correct words predicted      : %.2f%%' %(correct*100\/len(X_valid)))","0c119045":"with open('.\/model_word\/word_model_{}.txt'.format(datetime_object), 'a', encoding='utf-8') as f:\n    f.write('\\nAverage CER: %.2f%%' %(avg_cer*100))\n    f.write('\\nAverage WER: %.2f%%' %(avg_wer*100))\n    f.write('\\nCorrect characters predicted : %.2f%%' %(correct_char*100\/total_char))\n    f.write('\\nCorrect words predicted      : %.2f%%' %(correct*100\/len(X_valid)))","96b86f04":"for index in range (20):\n    i = random.randint(0, len(X_valid))\n    print ('true label',y_valid[i])\n    print ('predicted',prediction[i],'\\n')","22542712":"# SPLIT DATA INTO TRAIN_VALID SET\nSplit data using SKlearn. \n\nI got some issue with RAM. I guess that bc my variable's memory is huge so I have to delete some variables and it works. But I have to find out another way to combat with this ","5750c64b":"# EVALUATE ON VALID SET","a0a5e769":"<a href=\".\/model_word\/word_model_15_2021-10-01.h5\"> Download File <\/a>","5e2d18e1":"Visualize some train data","f2e09309":"# BUILD CRNN + CTC LOSS MODEL\n\n* y_true: tensor (samples, max_string_length) containing the truth labels.\n* y_pred: tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax.\n* input_length: tensor (samples, 1) containing the sequence length of slices coming out from RNN for each batch item in y_pred.\n* label_length: tensor (samples, 1) containing the sequence length of label for each batch item in y_true.\n\n**We need 2 model. The final model with CTC loss is for trainning and the model is for testing anf inferencing**","b70c08ae":"# TRAINING\n","8a3986e7":"## Visualize on valid set ","4c772087":"First we'll load data. You can find the original dataset [here](http:\/\/tc11.cvc.uab.es\/datasets\/HANDS-VNOnDB2018_1)  \nI have preprocessing the data before import it into csv file \n1. Coverting `.ikml` file into `.png` file\n2. Reshape into 128x32 image for CRNN model\n3. Tranpose, standardize images\nYou guys can find the source code in this repo [here](https:\/\/github.com\/huyhoang17\/Vietnamese_Handwriting_Recognition)\n\nI also used a lot of Data Augmentation methods for my project like:\n\n1. Add blob & line noise\n2. Random cutout (horizontal and vertical)\n3. Elastics Transformation (10% of data)\n\nMy problem is much different from the orginal one so I wil change a lot of things like the model, CTC loss, data augmentation and so on,...\n","b43870cc":"# LOAD DATASET\nIn short, this dataset in `.csv` file includes over 110.000 image of Vietnamese word. \n\nThere are 2 columns: image (array) and label ","93d9f5db":"Our dataset have some trouble that some ground truth are nan\/float type. So we have to delete them","b0258300":"## Create metrics for evaluating \n1. CER\n2. WER\n3. Levenshtein distance\n4. Correct characters rate\n5. Correct words rate","5f92674c":"# PREPARE FOR CTC LOSS\nI have done a notebook [here](datetime_object)\n\n* train_y contains the true labels converted to numbers and padded with -1. \n* The length of each label is equal to max_str_len.\n* train_label_len contains the length of each true label (without padding)\n* train_input_len contains the length of each predicted label. \n* The length of all the predicted labels is constant i.e number of timestamps - 2.\n* train_output is a dummy output for ctc loss."}}