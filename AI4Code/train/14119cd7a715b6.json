{"cell_type":{"0ce3daf0":"code","65395a72":"code","315eb624":"code","eb3577a7":"code","ae03b618":"code","97ba194c":"code","ea69b620":"code","b5f0da7d":"code","a79628b7":"code","6fb074e9":"code","71bb1ee5":"code","89f91afc":"code","8bdb8c7e":"code","96dbabb6":"code","f7a47d14":"code","498e8582":"code","3fbeb6ee":"code","58b4f029":"code","9a22b38b":"code","4058bf6f":"code","2b47a4f4":"code","ae708e05":"code","0a94f3ea":"code","ec706599":"code","c888dc7e":"code","415d6d3d":"code","0a67f695":"code","ae0ee84e":"code","56425931":"code","9bf1ed22":"code","8c9dc49a":"code","cbb2d6c9":"code","1dededf1":"code","39310847":"code","c99d8eb6":"code","a3c7a354":"code","6a62d6e1":"code","ee07b49c":"code","7866e6a4":"code","24f2cf1b":"code","a5718413":"code","b2be11ff":"code","8f62a8b8":"code","1fd08bae":"code","f4a7e305":"code","2592cc16":"code","86013cc7":"code","1970d4d3":"markdown","4a27d90e":"markdown","26856a7b":"markdown","dbe34694":"markdown","0b062984":"markdown","a4b67b1e":"markdown","107f23ad":"markdown","a7512619":"markdown","61c3ef86":"markdown","d7f6914f":"markdown","6a1d2550":"markdown","e69a3c5c":"markdown","23400a6b":"markdown","04c7fe3d":"markdown","4dc9f215":"markdown","46a489aa":"markdown","86c44ab1":"markdown","25bc2142":"markdown","6ed10b52":"markdown","ef261abf":"markdown","8f6eef21":"markdown","4f9a871c":"markdown","c0469e7b":"markdown","4c99b2b6":"markdown","ebc98200":"markdown"},"source":{"0ce3daf0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport pandas as pd\nimport numpy as np\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport gc","65395a72":"train_set = pd.read_csv('\/kaggle\/input\/preprocessed-sales-data\/train_set.csv')\nvalidation_set = pd.read_csv('\/kaggle\/input\/preprocessed-sales-data\/validation_set.csv')\ntest_set = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')","315eb624":"train_set.head().T","eb3577a7":"# Creating training and validation sets\nx_train = train_set.drop(['item_cnt_month','date_block_num'],axis=1)\ny_train = train_set['item_cnt_month'].astype(int)\n\nx_val = validation_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\ny_val = validation_set['item_cnt_month'].astype(int)","ae03b618":"latest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nx_test = pd.merge(test_set, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nx_test['year'] = 2015\nx_test['month'] = 9\nx_test.drop('item_cnt_month', axis=1, inplace=True)\nx_test = x_test[x_train.columns]","97ba194c":"ts = time.time()\nsets = [x_train, x_val,x_test]\nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n\nx_test.fillna(x_test.mean(), inplace=True)\nprint('Time taken : ',time.time()-ts)","ea69b620":"x_test.head().T","b5f0da7d":"all_f = ['shop_id', 'item_id', 'item_cnt', 'mean_item_cnt', 'transactions', 'year',\n       'month', 'item_cnt_mean', 'item_cnt_std',\n       'item_cnt_shifted1', 'item_cnt_shifted2', 'item_cnt_shifted3',\n       'item_trend', 'shop_mean', 'item_mean', 'shop_item_mean', 'year_mean',\n       'month_mean']","a79628b7":"x_tr = x_train[all_f]\nx_va = x_val[all_f]\nx_te = x_test[all_f]","6fb074e9":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","71bb1ee5":"def models(model, x_tr, y_train, x_va, x_te):\n    '''\n    This model is used to train the model and make predictions.\n    - model : the model in usage (LR, KN, RF)\n    - x_tr : x_train dataframe\n    - y_train : y_train dataframe\n    - x_va : x_validation dataframe\n    - x_te : x_test dataframe\n    '''\n    \n    #training the model\n    model.fit(x_tr,y_train)\n    \n    #storing the predictions for `train` and `validation` sets\n    train_pred = model.predict(x_tr)\n    val_pred = model.predict(x_va)\n    test_pred = model.predict(x_te)\n    \n    return train_pred, val_pred, test_pred","89f91afc":"# Training the first model: Linear Regression\nLR = LinearRegression(n_jobs=-1)\nM1_train, M1_val, M1_test = models(LR,x_tr,y_train,x_va,x_te)","8bdb8c7e":"print('Train rmse for LINEAR REGRESSION:', np.sqrt(mean_squared_error(y_train, M1_train)))\nval_lr = np.sqrt(mean_squared_error(y_val, M1_val))\nprint('Validation rmse for LINEAR REGRESSION:', np.sqrt(mean_squared_error(y_val, M1_val)))","96dbabb6":"perm = PermutationImportance(LR, random_state=42).fit(x_va, y_val)\neli5.show_weights(perm, feature_names = x_va.columns.tolist())","f7a47d14":"x_t_knn = x_tr[:100000]\ny_t_knn = y_train[:100000]\n\nscaler = MinMaxScaler()\nscaler.fit(x_t_knn)\nscaled_x_t_knn = scaler.transform(x_t_knn)\nscaled_x_v = scaler.transform(x_va)\n\nKN = KNeighborsRegressor(n_neighbors=20, leaf_size=15,n_jobs=-1)\nM2_train, M2_val, M2_test = models(KN,scaled_x_t_knn,y_t_knn,scaled_x_v,x_te)","498e8582":"print('Train rmse for KNN:', np.sqrt(mean_squared_error(y_t_knn, M2_train)))\nval_knn = np.sqrt(mean_squared_error(y_val, M2_val))\nprint('Validation rmse for KNN:', np.sqrt(mean_squared_error(y_val, M2_val)))","3fbeb6ee":"x_va_knn = x_va[:10000]\ny_val_knn = y_val[:10000]\nperm = PermutationImportance(KN, random_state=42).fit(x_va_knn, y_val_knn)\neli5.show_weights(perm, feature_names = x_va.columns.tolist())","58b4f029":"# Training the third model: Random Forest\nts = time.time()\nRF = RandomForestRegressor(n_jobs=-1,n_estimators=40, max_depth=8, random_state=42)\nM3_train, M3_val,M3_test = models(RF,x_tr,y_train,x_va,x_te)\nprint('Total time taken : ',time.time()-ts)","9a22b38b":"print('Train rmse for RANDOM FOREST:', np.sqrt(mean_squared_error(y_train, M3_train)))\nval_rf = np.sqrt(mean_squared_error(y_val, M3_val))\nprint('Validation rmse for RANDOM FOREST:', np.sqrt(mean_squared_error(y_val, M3_val)))","4058bf6f":"perm = PermutationImportance(RF, random_state=42).fit(x_va, y_val)\neli5.show_weights(perm, feature_names = x_va.columns.tolist())","2b47a4f4":"val_predictions = {'LR': M1_val,\n                     'KN': M2_val,\n                     'RF': M3_val}\n\nval_predictions = pd.DataFrame(val_predictions)","ae708e05":"val_predictions.head(10).T","0a94f3ea":"test_predictions = {'LR': M1_test,\n                   'KN': M2_test,\n                   'RF': M3_test}\n\ntest_predictions = pd.DataFrame(test_predictions)","ec706599":"test_predictions.head(10).T","c888dc7e":"# Stacking Model \nstack_model = LinearRegression()\nstack_model.fit(val_predictions, y_val)\n\nstack_val_preds = stack_model.predict(val_predictions)\nstack_test_preds = stack_model.predict(test_predictions)","415d6d3d":"val_stack = np.sqrt(mean_squared_error(stack_val_preds,y_val))\nprint('Validation rmse for STACKING:', np.sqrt(mean_squared_error(stack_val_preds,y_val)))","0a67f695":"perm = PermutationImportance(stack_model, random_state=42).fit(val_predictions, y_val)\neli5.show_weights(perm, feature_names = val_predictions.columns.tolist())","ae0ee84e":"blend_df_valid = pd.concat([x_va,val_predictions],axis=1)\nblend_df_test=pd.concat([x_te,test_predictions],axis=1)","56425931":"blend_df_valid.head().T","9bf1ed22":"blend_model = LinearRegression()\nblend_model.fit(blend_df_valid,y_val)\nblend_val_preds = blend_model.predict(blend_df_valid)\nblend_test_preds = blend_model.predict(blend_df_test)","8c9dc49a":"val_blend = np.sqrt(mean_squared_error(blend_val_preds,y_val))\nprint('Validation rmse for BLENDING :', np.sqrt(mean_squared_error(blend_val_preds,y_val)))","cbb2d6c9":"perm = PermutationImportance(blend_model, random_state=42).fit(blend_df_valid, y_val)\neli5.show_weights(perm, feature_names = blend_df_valid.columns.tolist())","1dededf1":"del(LR,M1_train, M1_val, M1_test,perm)\ndel(x_t_knn,y_t_knn)\ndel(scaled_x_t_knn,scaled_x_v)\ndel(x_va_knn,y_val_knn)\ndel(KN,M2_train, M2_val, M2_test)\ndel(RF,M3_train,M3_val,M3_test)\ngc.collect()","39310847":"from sklearn.ensemble import BaggingRegressor\nfrom sklearn import tree\nfrom sklearn.svm import SVR\n\nbagging_model = BaggingRegressor(base_estimator=LinearRegression(),n_estimators=20, random_state=0) \nbagging_model.fit(x_tr, y_train)\n\nbagging_val_preds = bagging_model.predict(x_va)\nbagging_test_preds = bagging_model.predict(x_te)","c99d8eb6":"val_bagging = np.sqrt(mean_squared_error(bagging_val_preds,y_val))\nprint('Validation rmse for BAGGING :', np.sqrt(mean_squared_error(bagging_val_preds,y_val)))","a3c7a354":"from xgboost import XGBRegressor\nxgb_model = XGBRegressor(random_state=42, colsample_bylevel=1,\n                         colsample_bytree=0.5, learning_rate=0.1, max_depth=5,\n                         n_estimators=20, n_jobs=-1, objective='reg:linear')\nxgb_model.fit(x_tr,y_train,eval_metric=\"rmse\", \n              eval_set=[(x_tr, y_train), (x_va, y_val)], \n              verbose=10, \n              early_stopping_rounds=15)","6a62d6e1":"val_xgboost = 0.79400","ee07b49c":"from sklearn.ensemble import AdaBoostRegressor\nada_model = AdaBoostRegressor(random_state=42,n_estimators=20,learning_rate=0.1)\n\nada_model.fit(x_tr,y_train)","7866e6a4":"ada_train_preds = ada_model.predict(x_tr)\nada_val_preds = ada_model.predict(x_va)\n\nprint('Train rmse for AdaBoost :', np.sqrt(mean_squared_error(ada_train_preds,y_train)))\nval_adaboost =  np.sqrt(mean_squared_error(ada_val_preds,y_val))\nprint('Validation rmse for AdaBoost :', np.sqrt(mean_squared_error(ada_val_preds,y_val)))","24f2cf1b":"import lightgbm as lgb\n\ntrain_data=lgb.Dataset(x_tr,label=y_train)\nparams = {'num_iterations':50,'max_depth':10,'learning_rate':0.001}\nlgb_model= lgb.train(params, train_data, 100)\n\nlgb_train_preds = lgb_model.predict(x_tr)\nlgb_val_preds = lgb_model.predict(x_va)\n","a5718413":"val_lgb =  np.sqrt(mean_squared_error(lgb_val_preds,y_val))\n\nprint('Train rmse for LightGBM :', np.sqrt(mean_squared_error(lgb_train_preds,y_train)))\nprint('Validation rmse for LightGBM  :', val_lgb)","b2be11ff":"del(stack_model,stack_val_preds,stack_test_preds)\ndel(blend_model,blend_val_preds,blend_test_preds)\ndel(bagging_model,bagging_val_preds,bagging_test_preds)\ndel(xgb_model)\ngc.collect()","8f62a8b8":"x_tr['shop_id'] = x_tr['shop_id'].astype(int)\nx_tr['item_id'] = x_tr['item_id'].astype(int)\nx_tr['year'] = x_tr['year'].astype(int)\nx_tr['month'] = x_tr['month'].astype(int)\n\nx_va['shop_id'] = x_va['shop_id'].astype(int)\nx_va['item_id'] = x_va['item_id'].astype(int)\nx_va['year'] = x_va['year'].astype(int)\nx_va['month'] = x_va['month'].astype(int)\n","1fd08bae":"from catboost import CatBoostRegressor\nfeatures =  [0, 1, 5, 6]\ncat_model = CatBoostRegressor(iterations=100, verbose=50, depth=5)\ncat_model.fit(x_tr, y_train,cat_features=features,eval_set=(x_va, y_val))","f4a7e305":"cat_train_preds = cat_model.predict(x_tr)\ncat_val_preds = cat_model.predict(x_va)\n\nval_cat = np.sqrt(mean_squared_error(cat_val_preds,y_val))\nprint('Train rmse for CatBoost :', np.sqrt(mean_squared_error(cat_train_preds,y_train)))\nprint('Validation rmse for CatBoost  :', val_cat)","2592cc16":"RMSE = [val_lr, val_knn, val_rf, val_stack, val_blend, val_bagging, val_xgboost, val_adaboost, val_lgb, val_cat]\nimport seaborn as sns \nimport matplotlib.pyplot as plt\ny_ax = ['Linear Regression' ,'KNN', 'Random Forest Regression','Stacking', 'Blending','Bagging' ,'XGBoost', 'AdaBoost', 'LightGBM','CatBoost']\nx_ax = RMSE\nout = pd.DataFrame()\nout['RMSE'] = RMSE\nout['Algorithms'] = y_ax","86013cc7":"# sns.barplot(x=x_ax,y=y_ax,linewidth=1.5,edgecolor=\"0.1\")\n# plt.xlabel('RMSE')\nimport plotly.express as px\n\nfig = px.bar(out,y=out['RMSE'], x=out['Algorithms'], \n             color=out['Algorithms'])\nfig.update_layout(title='Algorithms with RMSE', title_x=0.5)\nfig.show()","1970d4d3":"<a class=\"anchor\" id=\"toc\"><\/a>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 600px;\">\n<h1>Contents<\/h1>\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#1.1\">1 Preprocessing Data<\/a><\/li>\n    \n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#2.1\">2 Training Base Learners<\/a><\/li>\n\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#3.1\">3 Advanced Ensemble Learning<\/a><\/li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n            <li style=\"list-style: outside none none !important;\"><a href=\"#3.1\">3.1 Stacking<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#3.2\">3.2 Blending<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#3.3\">3.3 Bagging<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#3.4\">3.4 Boosting<\/a><\/li>           \n      <\/ul>    \n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#4\">4 Boosting In Depth<\/a><\/li>\n     <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\"> \n    <li style=\"list-style: outside none none !important;\"><a href=\"#4.1\">4.1 XGBoost<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#4.2\">4.2 AdaBoost<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#4.3\">4.3 LightGBM<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#4.4\">4.4 CatBoost<\/a><\/li>\n        \n\n<\/ul>\n<\/div>","4a27d90e":"The **Hyperparameters** that can be used tuned in XGBoost model are:\n\n* **n_estimators** - Total number of trees\n* **learning_rate** - this determines the impact of each tree on the final outcome\n* **random_state** - random number seed so that same random numbers are genereated each time\n* **max_depth** - Maximum depth to which tree can grow\n* **subsample** - the fraction of observations to be selected for each tree.\n* **objective** - this is used to define the loss function. here it is 'linear'\n* **colsample_bylevel** - random feature selection levels\n* **colsample_bytree** - random feature selection at tree\n\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++","26856a7b":"\n# Advanced Ensemble Learning\n\n### Stacking\n<a class=\"anchor\" id=\"3.1\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>\n\nStacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on a complete training set, then the meta-model is trained on the outputs of the base level model as features.\n\n[Source of information](https:\/\/blog.statsbot.co\/ensemble-learning-d1dcd548e936)\n![img](https:\/\/1.bp.blogspot.com\/-S8ss-zVfpRM\/V1qKcxfCvNI\/AAAAAAAAD0I\/8UUFyrE4MqQYYuWSxrOOvX3zRfw93nCLwCLcB\/s640\/Stacking.png)\nNow we will make 2 new dataframes of `validation` and `test`","dbe34694":"Lower value of RMSE tells us that the model is more better","0b062984":"# Boosting in Depth\n<a class=\"anchor\" id=\"3.4\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>\n\nBoosting is an ensemble modeling technique which attempts to build a strong classifier from the number of weak classifiers. It is done building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added.\n![Img](https:\/\/miro.medium.com\/max\/544\/1*m2UHkzWWJ0kfQyL5tBFNsQ.png)\n\n### XGBoost\n<a class=\"anchor\" id=\"4.1\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\n\n[Source of Information](https:\/\/xgboost.readthedocs.io\/en\/latest\/)\n","a4b67b1e":"## AdaBoost\n<a class=\"anchor\" id=\"4.2\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>\n\nAdaBoost can be used to boost the performance of any machine learning algorithm. It is best used with weak learners. These are models that achieve accuracy just above random chance on a classification problem. The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level. Because these trees are so short and only contain one decision for classification, they are often called decision stumps.","107f23ad":"Took 5 mins for model to run. xD","a7512619":"The **Hyperparameters** that can be used tuned in CatBoost model are:\n* **learning_rate** - this determines the impact of each tree on the final outcome\n* **iterations** - The maximum number of trees that can be built\n* **depth** - Defines the depth of the trees.\n\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++","61c3ef86":"Okay, so we have a drop of 0.01(which is an improvement :P). \nThough it might not be helpful now, but has significant impact in kaggle competitons. \nAlso, Linear Regression plays an important role in making the predictions\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++","d7f6914f":"We will be using only the above features","6a1d2550":"### Bagging\n<a class=\"anchor\" id=\"3.3\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>\n\nBootstrap aggregating, also called bagging (from bootstrap aggregating), is an ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n\n[Source of information](https:\/\/en.wikipedia.org\/wiki\/Bootstrap_aggregating)\n\n![Img](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/c\/c8\/Ensemble_Bagging.svg\/1920px-Ensemble_Bagging.svg.png)","e69a3c5c":"## How to interpret Feature Importances\n\n\n1. The first number in each row shows how much model performance decreased with a random shuffling.\n2. The number after the \u00b1 measures how performance varied from one-reshuffling to the next.\n3. The features `shop_item_mean`, `item_cnt_mean`, `item_cnt` have higher highest impact on predictions.","23400a6b":"The **Hyperparameters** that can be used tuned in AdaBoost model are:\n\n* **n_estimators** - Total number of trees\n* **learning_rate** - this determines the impact of each tree on the final outcome\n* **random_state** - random number seed so that same random numbers are genereated each time\n* **base_estimator** - Model that is used to ensemble. At default we have `Decision Tree`\n\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++","04c7fe3d":"### Blending\n<a class=\"anchor\" id=\"3.2\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>\n\nIn stacking we used only the `predictions` in the meta model, but blending is slighlty different than stacking.The `validation set` and the `predictions` are used to build a model which is run on the test set. \n![Img](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/05\/image-5-300x228.png)","4dc9f215":"# Introduction \n\n#### In the first part we saw some basics of Ensemble learning.... \n\n#### In this notebook we will try to understand some of the advanced ensemble learning methods and implement them...\n\n\n<center><img src=\"data:image\/jpeg;base64,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\"><\/center>\n\n\n\n#### Jokes apart, without any more delay lets get into ensemble learning\n\n#### **NOTE :** This notebook on Ensemble Learning is divided into 2 parts and we are in **Part-2** \n\n## For Part-1 click [here](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-1) \n\n  <font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>\n----------------------------------------------------------------------------------------------------------------------------------------------","46a489aa":"Highest impact was from `Linear Regression` model. This might be because we used the Ensemble model also with LR\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++","86c44ab1":"As we can see the negative values the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets. As we chose **only ~1%** of the validation set we might be having this.","25bc2142":"\n# Preprocessing the data\n\nBelow datasets are picked from [here](https:\/\/www.kaggle.com\/dimitreoliveira\/model-stacking-feature-engineering-and-eda)\n\n<a class=\"anchor\" id=\"1.1\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>","6ed10b52":"## LightGBM\n<a class=\"anchor\" id=\"4.3\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>\n\nLight GBM beats all the other algorithms when the dataset is extremely large. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset.\nLightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern.\n\n[Source of Information](https:\/\/courses.analyticsvidhya.com\/courses\/take\/ensemble-learning-and-ensemble-learning-techniques\/texts\/11145884-lightgbm)","ef261abf":"The **Hyperparameters** that can be used tuned in LightGBM model are:\n* **learning_rate** - this determines the impact of each tree on the final outcome\n* **num_iterations** - this tells us the number of boosting iterations to be performed.\n* **max_depth** - Maximum depth to which tree can grow\n\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++","8f6eef21":"> The Stacked model is slightly better when compared to the three base models.","4f9a871c":"As we can see that there is no much of improvement from the base learner `Linear Regression` version.\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++","c0469e7b":"## CatBoost\n<a class=\"anchor\" id=\"4.4\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>\n\nCatBoost is mainly used mainly for categorical classification. One-Hot-Encoding is not necessary for this model. CatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms. It can be even used on regression problems.","4c99b2b6":"# Training Base Learners\n<a class=\"anchor\" id=\"2.1\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>","ebc98200":"Nice, now we see for 3 different models we have a slight difference in important features. This will help the stacking model to learn better from all the three models.\n\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"}}