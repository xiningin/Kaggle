{"cell_type":{"f907ad0c":"code","d5bc0053":"code","5d543896":"code","67e628bf":"code","b6501f73":"code","d5b12182":"code","c318f491":"code","31ae5103":"code","7da1c25e":"code","9ca85016":"code","d632b5af":"code","3d88764a":"code","73694923":"code","7b7612e4":"code","654f63cd":"code","3c3c0b93":"code","96bed515":"code","e771667a":"code","443e9fc5":"code","dc47eedf":"code","069691f3":"code","9dd5db01":"code","20462a82":"code","c7375b63":"code","314795c4":"code","02b71bea":"code","178b47e0":"code","8259448e":"code","ef9f782e":"code","ceccccdb":"code","fe41f206":"code","eee437d4":"code","f418d919":"code","7f3784b8":"code","ed58136b":"code","a28be706":"code","e75215f6":"code","38c1bccd":"code","53d35104":"code","337cf609":"code","79aa643d":"code","c3f534aa":"code","de098cab":"code","ac7cfc76":"code","98331d13":"code","49826454":"code","9d0178d3":"code","2decc4c7":"code","ddda9175":"code","aa64f10c":"code","7e9901fb":"code","d5034dda":"code","b8914023":"code","81bc310e":"code","9f5f64df":"code","02c0a668":"code","fb4507fb":"code","7d63aaf3":"code","cbab0cd2":"code","4b347c69":"code","6da08687":"code","4f47b30b":"code","f2178d4b":"code","5faca6b3":"code","300226b9":"code","49e945da":"code","61cafd44":"code","7502aff3":"code","860bda38":"code","2dd9f5c7":"code","f8410604":"code","c7394333":"code","f6cc3bf2":"code","ae39aa03":"code","c0036f26":"code","8de26076":"code","96a44a96":"code","b20ebff1":"code","159796af":"code","d1b97fe8":"code","ee29db33":"code","5e73f032":"code","44702bd4":"code","82724c19":"code","c35c34b0":"code","3bda5719":"code","cc5550c5":"code","e86cdf37":"code","618e5d1e":"code","66e1c24e":"code","b6cbef94":"code","be653e2a":"code","ea9744ee":"code","5fe4edaa":"code","61a68035":"code","db840e39":"code","e1b0c78f":"code","9b8a4f98":"code","bc62fc1f":"code","9e4facda":"code","d1592449":"markdown","f307d0c5":"markdown","8386817f":"markdown","2d537107":"markdown","efce7f9b":"markdown","567e6170":"markdown","ae39d919":"markdown","c2e4682f":"markdown","4fff68b9":"markdown","4a01e31c":"markdown","99ff7f91":"markdown","39d03542":"markdown","087819ea":"markdown","bfaa549f":"markdown","0ec25bc9":"markdown","565ff361":"markdown","8bc27888":"markdown","9e77d7ed":"markdown","d60f8593":"markdown","e70c3735":"markdown"},"source":{"f907ad0c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d5bc0053":"import nltk\nimport string\nimport re\nimport numpy as np\nimport pandas as pd\nimport pickle\n#import lda\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\n\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook\n#from bokeh.transform import factor_cmap\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)","5d543896":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)\n\nprint(os.listdir(\"..\/input\"))\n","67e628bf":"pwd","b6501f73":"ls ..\/input\/mercari-price-suggestion-challenge\/","d5b12182":"import pandas as pd\n\ntrain = pd.read_csv(f'..\/input\/mercari\/train.tsv', sep='\\t') #sep='\\t'\u3067\u30bf\u30d6\u533a\u5207\u308a\ntest = pd.read_csv(f'..\/input\/mercari\/test.tsv', sep='\\t')","c318f491":"print(train.shape)\nprint(test.shape)","31ae5103":"train.info()","7da1c25e":"train.head()","9ca85016":"train.dtypes","d632b5af":"train.price.describe() #e\u306e\u6a2a\u306e\u6570\u5b57\u3060\u305110\u500d","3d88764a":"import matplotlib.pyplot as plt\n\ntrain['price'].hist(bins = 30, range=(0, 250))\nplt.xlabel('price')\nplt.ylabel('count')","73694923":"#\u81ea\u7136\u5bfe\u6570\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n(train['price'] + 1).apply(np.log).hist(bins = 30, range=(0, 7))\nplt.xlabel('log(price + 1)')\nplt.ylabel('count')\nplt.xlim(1, 7)","7b7612e4":"plt.hist(train.loc[train['shipping'] == 0, 'price'].dropna(),\n        range=(0, 250), bins=30, alpha=0.5, label='0')\nplt.hist(train.loc[train['shipping'] == 1, 'price'].dropna(),\n        range=(0, 250), bins=30, alpha=0.5, label='1')\nplt.xlabel('price')\nplt.ylabel('count')\nplt.legend(title='shipping')","654f63cd":"print(train.loc[train['shipping'] == 0, 'price'].mean()) #\u9001\u6599\u8cfc\u5165\u8005\nprint(train.loc[train['shipping'] == 1, 'price'].mean()) #\u9001\u6599\u51fa\u54c1\u8005","3c3c0b93":"#\u81ea\u7136\u5bfe\u6570\nplt.hist((train.loc[train['shipping'] == 0, 'price'] + 1).apply(np.log).dropna(),\n        range=(1, 7), bins=30, alpha=0.5, label='0')\nplt.hist((train.loc[train['shipping'] == 1, 'price'] + 1).apply(np.log).dropna(),\n        range=(1, 7), bins=30, alpha=0.5, label='1')\nplt.xlabel('log(price + 1)')\nplt.ylabel('count')\nplt.legend(title='shipping')","96bed515":"print((train.loc[train['shipping'] == 0, 'price'] + 1).apply(np.log).mean()) #\u9001\u6599\u8cfc\u5165\u8005\nprint((train.loc[train['shipping'] == 1, 'price'] + 1).apply(np.log).mean()) #\u9001\u6599\u51fa\u54c1\u8005","e771667a":"#The number of category_name\ntrain['category_name'].nunique()","443e9fc5":"#top 5\ntrain['category_name'].value_counts()[:5]","dc47eedf":"#count null.\ntrain['category_name'].isnull().sum()","069691f3":"#survied details of categories\ndef split_cat(text):\n    try:\n        return text.split(\"\/\")\n    except:\n        return (\"No data\", \"No data\", \"No data\")","9dd5db01":"#edit category_name\ntrain['general_cat'], train['subcat1'], train['subcat2'] = zip(*train['category_name'].apply(lambda x : split_cat(x)))\ntrain.head()","20462a82":"#repeat the same step for the test set\ntest['general_cat'], test['subcat1'], test['subcat2'] = zip(*test['category_name'].apply(lambda x : split_cat(x)))\ntest.head()","c7375b63":"#The number of subcat1\ntrain['subcat1'].nunique()","314795c4":"train['general_cat'].value_counts()","02b71bea":"train['subcat1'].value_counts()[:5]","178b47e0":"#The number of subcat2\ntrain['subcat2'].nunique()","8259448e":"train['subcat2'].value_counts()[:5]","ef9f782e":"#values\u306b\u3059\u308b\u3053\u3068\u3067\u914d\u5217\u306b\u5909\u63db\nx = train['general_cat'].value_counts().index.values.astype(str) #\u5927\u30ab\u30c6\u30b4\u30ea\u30fc\u306e\u914d\u5217\ny = train['general_cat'].value_counts().values #\u5927\u30ab\u30c6\u30b4\u30ea\u30fc\u5225\u4ef6\u6570\u306e\u914d\u5217\nunique_pct = [(\"%.2f\"%(v*100))+\"%\"for v in (y\/len(train))] #\u5404general_cat\u30e9\u30d9\u30eb\u306e\u51fa\u73fe\u7387","ceccccdb":"import plotly.offline as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Bar(x=x, y=y, text=unique_pct)\nlayout = dict(title = 'Number of Items by general_cat',\n             yaxis = dict(title = 'Count'),\n             xaxis = dict(title = 'general_cat'))\nfig = dict(data=[trace1], layout=layout)\npy.iplot(fig)","fe41f206":"#\u4e0a\u304b\u308915\u4ef6\u307e\u3067\u306e\u30b5\u30d6\u30ab\u30c6\u30b4\u30ea1\u3092\u540c\u69d8\u306b\u51fa\u73fe\u7387\u8a08\u7b97\nx = train['subcat1'].value_counts().index.values.astype('str')[:15]\ny = train['subcat1'].value_counts().values[:15]\n\nsubcat1_pct = [(\"%.2f\"%(v*100))+\"%\" for v in (y\/(len(train)))]","eee437d4":"trace1 = go.Bar(x=x, y=y, text=subcat1_pct,\n               marker=dict(\n               color = y,colorscale='Portland', showscale=True,\n               reversescale = False\n               ))\nlayout = dict(title='Number of Items by subcat1(~15)',\n             yaxis = dict(title='Count'),\n             xaxis = dict(title='subcat1'))\nfig = dict(data=[trace1], layout=layout)\npy.iplot(fig)","f418d919":"#\u3068\u308a\u3042\u3048\u305a\u7bb1\u3072\u3052\u56f3\u4f5c\u308b\ngeneral_cats = train['general_cat'].unique()\ngeneral_cats","7f3784b8":"#\u5927\u30ab\u30c6\u30b4\u30ea\u30fc\u5225\u306e\u5024\u6bb5\u3092\u30ea\u30b9\u30c8\u3067\u307e\u3068\u3081\u308b(\u3053\u308c\u304c\u6a2a\u8ef8)\nx = [train.loc[train['general_cat'] == cat, 'price'] for cat in general_cats] ","ed58136b":"#price\u306flog(price + 1)\u3067\u6b63\u898f\u5316\u3059\u308b\ndata = [go.Box(x=np.log(x[i] + 1), name=general_cats[i]) for i in range(len(general_cats))]","a28be706":"layout = dict(title='Price distribution by general_cat',\n             yaxis = dict(title='Category'),\n             xaxis = dict(title='log(price + 1)'))\nfig = dict(data=data, layout=layout)\npy.iplot(fig)","e75215f6":"train['brand_name'].nunique()","38c1bccd":"x = train['brand_name'].value_counts().index.values.astype('str')[:10]\ny = train['brand_name'].value_counts().values[:10]","53d35104":"trace1 = go.Bar(x=x, y=y, \n                 marker=dict(\n                 color = y,colorscale='Portland',showscale=True,\n                 reversescale = False\n                 ))\nlayout = dict(title= 'Top 10 Brand by Number of Items',\n               yaxis = dict(title='Brand Name'),\n               xaxis = dict(title='Count'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)","337cf609":"import re\nimport string\nfrom sklearn.feature_extraction import stop_words\n\n#item_decription\u306b\u542b\u307e\u308c\u308b\u5358\u8a9e\u9577\u3092\u8abf\u3079\u308b(\u5b57\u53e5\u89e3\u6790\u306f\u5f8c\u307b\u3069)\ndef wordCount(text):\n    #\u5c0f\u6587\u5b57\u306b\u5909\u63db\u3057\u3066\u6b63\u898f\u8868\u73fe\u3092\u53d6\u308a\u9664\u304f\n    #try:\n        text = str(text).lower()\n        #\u6b63\u898f\u8868\u73fe\u30d1\u30bf\u30fc\u30f3\u6587\u5b57\u5217\u3092\u30b3\u30f3\u30d1\u30a4\u30eb\u3057\u3066\u6b63\u898f\u8868\u73fe\u30d1\u30bf\u30fc\u30f3\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') #[!\"#$%&'()*+,-.\/:;<=>?@[]^_`{|}~0-9\\\\r\\\\t\\\\n]\n        #text\u5185\u306e\u7279\u6b8a\u6587\u5b57\u3092\u534a\u89d2\u7a7a\u767d\u3067\u7f6e\u63db\n        txt = regex.sub(\" \", text)\n        #tokenize\n        #remove words in stop words\n        words = [w for w in txt.split(\" \") \\\n                if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3] #\u5185\u5305\u8868\u8a18(\u51e6\u7406, \u30eb\u30fc\u30d7, \u6761\u4ef6)\n        #words\u306b\u306f\u6587\u5b57\u65703\u4ee5\u4e0a\u304b\u3064stop_words\u4ee5\u5916\u304c\u30ea\u30b9\u30c8\u3068\u3057\u3066\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\n        return len(words)\n    #except:\n        #return 0","79aa643d":"#\u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u3092\u53d6\u308a\u9664\u3044\u305f\u5f8c\u306e\u5358\u8a9e\u306e\u7dcf\u6570\u3092\u30ab\u30a6\u30f3\u30c8\u3057\u305f\u5217\u3092\u8ffd\u52a0\ntrain['desc_len'] = train['item_description'].apply(lambda x : wordCount(x))\ntest['desc_len'] = test['item_description'].apply(lambda x : wordCount(x))","c3f534aa":"train.head()","de098cab":"test.head()","ac7cfc76":"df = train.groupby('desc_len')['price'].mean().reset_index()\ndf","98331d13":"trace1 = go.Scatter(\n    x = df['desc_len'],\n    y = np.log(df['price']), #df\u3067\u5e73\u5747\u4fa1\u683c0\u306e\u3082\u306e\u304c\u7121\u3044\u305f\u3081,log(price)\u3067\u826f\u3044(\u771f\u6570\u3092+1\u3057\u306a\u304f\u3066\u3044\u3044)\n    mode = 'lines+markers',\n    name = 'lines+markers'\n)\nlayout = dict(title= 'Average log(price) by description length',\n             yaxis = dict(title='Average log(price)'),\n             xaxis = dict(title='Description length'))\nfig = dict(data=[trace1], layout=layout)\npy.iplot(fig)","49826454":"#item_description\u5185\u306e\u6b20\u640d\u5024\u3092\u9664\u304f\ntrain = train[pd.notnull(train['item_description'])]","9d0178d3":"from nltk.corpus import stopwords\n\nstopwords.words('english')","2decc4c7":"#(\u6e96\u5099)\u30ea\u30b9\u30c8\u306e\u6f14\u7b97\na = []\na += 't'\na += 'e'\na += 's'\na += 't'\na","ddda9175":"from nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom janome.tokenizer import Tokenizer\n\nstop = set(stopwords.words('english'))\n\n#tokenizer\u3068\u306f\u5b57\u53e5\u89e3\u6790\u306e\u610f\u5473\u3067,\u6587(item_description)\u3092\u610f\u5473\u306e\u3042\u308b\u30c8\u30fc\u30af\u30f3\u5358\u4f4d\u306b\u5206\u5272\u3059\u308b.\ndef tokenize(text):\n    \"\"\"\n    sent_tokenize(): segment text into sentences\n    word_tokenize(): break sentences into words\n    \"\"\"\n    try:\n        #\u6b63\u898f\u8868\u73fe\u30d1\u30bf\u30fc\u30f3\u6587\u5b57\u5217\u3092\u30b3\u30f3\u30d1\u30a4\u30eb\u3057\u3066\u6b63\u898f\u8868\u73fe\u30d1\u30bf\u30fc\u30f3\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\n        #[!\"#$%&'()*+,-.\/:;<=>?@[]^_`{|}~0-9\\\\r\\\\t\\\\n]\n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') \n        #text\u5185\u306e\u7279\u6b8a\u6587\u5b57\u3092\u534a\u89d2\u7a7a\u767d\u3067\u7f6e\u63db\n        txt = regex.sub(\" \", text)\n        \n        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n        tokens = []\n        for token_by_sent in tokens_:\n            tokens += token_by_sent\n        #filter(lambda\u5f0f, \u30a4\u30c6\u30e9\u30d6\u30eb\u30aa\u30d6\u30b8\u30a7\u30af\u30c8),\u30a4\u30c6\u30e9\u30d6\u30eb\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067lamda\u5f0f\u3092\u6e80\u305f\u3059\u3082\u306e\u3060\u3051\u3092\u62bd\u51fa\n        tokens = list(filter(lambda t : t.lower() not in stop, tokens))\n        #\u6570\u5b57\u306a\u3069\u3092\u542b\u3080\u30c8\u30fc\u30af\u30f3\u3092\u9664\u304f\n        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n        \n        return filtered_tokens \n        \n    except TypeError as e:\n        print(text, e)","aa64f10c":"\"\"\"\nfrom collections import Counter\n\n\n#item_description\u3092tokenize\u3057\u3066\u307f\u308b(\u5b9f\u884c\u306b\u6642\u9593\u304b\u304b\u308b)\n\n\n#\u5358\u8a9e\u8f9e\u66f8\u306e\u4f5c\u6210\ncat_dicts = dict()\nfor cat in general_cats:\n    #general_cat\u3054\u3068\u306bitem_description\u306e\u6587\u5b57\u5217\u3092\u62bd\u51fa\n    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n    #sentence\u304b\u3089token\u306b\u5909\u63db\n    #{\u5927\u30ab\u30c6\u30b4\u30ea\u30fc:token\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u5316\u3057\u305fitemdescription\u6587\u5b57\u5217}\n    cat_dicts[cat] = tokenize(text)\n\n    \n# flat list of all words combined\nflat_list = [item for sublist in list(cat_dicts.values()) for item in sublist]\n#Counter()\u306f\u51fa\u73fe\u56de\u6570\u304c\u591a\u3044\u9806\u306b\u8981\u7d20\u3092\u53d6\u5f97\nallWordCount = Counter(flat_list)\n#\u4e0a\u4f4d20\u4f4d\u307e\u3067\u53d6\u5f97\nall_top10 = allWordCount.most_common(20)\nx = [w[0] for w in all_top10]\ny = [w[1] for w in all_top10]\n\"\"\"","7e9901fb":"\"\"\"\ntrace1 = go.Bar(x=x, y=y, text=subcat1_pct)\nlayout = dict(title= 'Word Frequency',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Word'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)\n\"\"\"","d5034dda":"# apply the tokenizer into the item descriptipn column\ntrain['token'] = train['item_description'].map(tokenize)\ntest['token'] = test['item_description'].map(tokenize)","b8914023":"#index\u518d\u632f\u308a\u5206\u3051\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)","81bc310e":"train.head()","9f5f64df":"test.head()","02c0a668":"#\u4e00\u65e6\u6574\u5f62\u30c7\u30fc\u30bf\u4fdd\u5b58\n#train.to_csv('re_train.csv', sep='\\t')\n#test.to_csv('re_test.csv', sep='\\t')","fb4507fb":"#train = pd.read_csv('re_train.csv', sep='\\t')\n#test = pd.read_csv('re_test.csv', sep='\\t')","7d63aaf3":"for descriptions, tokens in zip(train['item_description'].head(), train['token'].head()):\n    print('description:', descriptions)\n    print('token:', tokens)\n    print()","cbab0cd2":"from collections import Counter\n\n\n#item_description\u3092tokenize\u3057\u3066\u307f\u308b(\u5b9f\u884c\u306b\u6642\u9593\u304b\u304b\u308b)\n\n\n#\u5358\u8a9e\u8f9e\u66f8\u306e\u4f5c\u6210\ncat_dicts = dict()\nfor cat in general_cats:\n    #general_cat\u3054\u3068\u306bitem_description\u306e\u6587\u5b57\u5217\u3092\u62bd\u51fa\n    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n    #sentence\u304b\u3089token\u306b\u5909\u63db\n    #{\u5927\u30ab\u30c6\u30b4\u30ea\u30fc:token\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u5316\u3057\u305fitemdescription\u6587\u5b57\u5217}\n    cat_dicts[cat] = tokenize(text)\n\n#\u4e0a\u4f4d4\u4f4d\u307e\u3067\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u3067\u6700\u3082\u51fa\u73fe\u56de\u6570\u306e\u591a\u3044\u5358\u8a9e\u3092\u898b\u3064\u3051\u308b\nwomen100 = Counter(cat_dicts['Women']).most_common(100)\nbeauty100 = Counter(cat_dicts['Beauty']).most_common(100)\nkids100 = Counter(cat_dicts['Kids']).most_common(100)\nelectronics100 = Counter(cat_dicts['Electronics']).most_common(100)","4b347c69":"from wordcloud import WordCloud\n\n#wordcloud\u3092\u751f\u6210\u3059\u308b\ndef generate_wordcloud(tup):\n    wordcloud = WordCloud(background_color = 'white',\n                         max_words=50, max_font_size=40,\n                         random_state=42\n                         ).generate(str(tup))\n    return wordcloud","6da08687":"fig, axes = plt.subplots(2, 2, figsize=(30, 15))\n\nax = axes[0, 0]\nax.imshow(generate_wordcloud(women100), interpolation='bilinear')\nax.axis('off')\nax.set_title('Women Top 100', fontsize=30)\n\nax = axes[0, 1]\nax.imshow(generate_wordcloud(beauty100))\nax.axis('off')\nax.set_title('Beauty Top 100', fontsize=30)\n\nax = axes[1, 0]\nax.imshow(generate_wordcloud(kids100))\nax.axis('off')\nax.set_title('Kids Top 100', fontsize=30)\n\nax = axes[1, 1]\nax.imshow(generate_wordcloud(electronics100))\nax.axis('off')\nax.set_title('Electronics Top 100', fontsize=30)","4f47b30b":"#\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10,\n                            max_features=180000,\n                            tokenizer=tokenize,\n                            ngram_range=(1, 2))","f2178d4b":"all_desc = np.append(train['item_description'].values, test['item_description'].values)\n#TFidfVectorizer\u30af\u30e9\u30b9\u306efit_transform\u30e1\u30bd\u30c3\u30c9\u3067\u5358\u8a9e\u3092\u30d9\u30af\u30c8\u30eb\u5316\nvz = vectorizer.fit_transform(list(all_desc))","5faca6b3":"#  create a dictionary mapping the tokens to their tfidf values\n#{\u5358\u8a9e\u540d:idf\u5024}\u306edict\u751f\u6210\ntfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_)) \ntfidf = pd.DataFrame(columns=['tfidf']).from_dict(\n                    dict(tfidf), orient='index')\ntfidf.columns = ['tfidf']","300226b9":"tfidf.sort_values(by=['tfidf'], ascending=True).head(10)","49e945da":"tfidf.sort_values(by=['tfidf'], ascending=False).head(10)","61cafd44":"#train\u3068test\u30c7\u30fc\u30bf\u3092\u30b3\u30d4\u30fc\ntrn = train.copy()\ntst = test.copy()\n\n#train\u3068test\u3067\u30e9\u30d9\u30eb\u5206\u3051\ntrn['is_train'] = 1\ntst['is_test'] = 0\n\nsample_sz = 15000\ncombined_df = pd.concat([trn, tst])\n#\u30e9\u30f3\u30c0\u30e0\u306b\u884c\u3092\u62bd\u51fa\ncombined_sample = combined_df.sample(n=sample_sz)\nvz_sample = vectorizer.fit_transform(list(combined_sample['item_description']))","7502aff3":"from sklearn.decomposition import TruncatedSVD\n\nn_comp=30\nsvd = TruncatedSVD(n_components=n_comp, random_state=42)\nsvd_tfidf = svd.fit_transform(vz_sample)","860bda38":"from sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)","2dd9f5c7":"tsne_tfidf = tsne_model.fit_transform(svd_tfidf)","f8410604":"tsne_tfidf","c7394333":"#\u53ef\u8996\u5316\u30e9\u30a4\u30d6\u30e9\u30eabokeh\nimport bokeh.plotting as bp\nfrom bokeh.plotting import show, figure, output_notebook\n\noutput_notebook() #notebook\u51fa\u529b\u306b\u306f\u3053\u306e1\u884c\u304c\u5fc5\u8981\n#\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u4f5c\u6210\nplot_tfidf = bp.figure(plot_width=700, plot_height=600,\n                      title='tfidf clustering of the item_description',\n                      tools=\"pan,wheel_zoom,box_zoom,reset,hover\",\n                      x_axis_type=None, y_axis_type=None, min_border=1)","f6cc3bf2":"combined_sample.reset_index(inplace=True, drop=True)","ae39aa03":"tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y']) #2\u6b21\u5143\u30c7\u30fc\u30bftnse_tfidf\u3092x, y\u30b3\u30e9\u30e0\u306b\u7f6e\u63db\ntfidf_df['description'] = combined_sample['item_description']\ntfidf_df['tokens'] = combined_sample['token']\ntfidf_df['category'] = combined_sample['general_cat']","c0036f26":"plot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips = {\"description\": \"@description\", \"tokens\": \"@tokens\", \"category\": \"@category\"}\nshow(plot_tfidf)","8de26076":"from sklearn.cluster import MiniBatchKMeans\n\nnum_clusters = 30 #\u5e83\u304f\u3068\u3063\u305f\n#\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nkmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n                              init='k-means++',\n                              n_init=1,\n                              init_size=1000, batch_size=1000, verbose=0, max_iter=1000)","96a44a96":"kmeans = kmeans_model.fit(vz) #Compute the centroids on X by chunking it into mini-batches.\nkmeans_clusters = kmeans.predict(vz) #Compute cluster centers and predict cluster index for each sample.\nkmeans_distance = kmeans.transform(vz) #Compute clustering and transform X to cluster-distance space.","b20ebff1":"kmeans","159796af":"len(kmeans_clusters)","d1b97fe8":"kmeans_distance","ee29db33":"sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nlen(sorted_centroids)","5e73f032":"terms = vectorizer.get_feature_names()\n\nfor i in range(num_clusters):\n    print(\"Cluster %d:\" % i)\n    aux = ''\n    for j in range(i, 30):\n        aux += terms[j] + ' | '\n    print(aux)\n    print() ","44702bd4":"#repeat the same step for the sample\nkmeans = kmeans_model.fit(vz_sample) #Compute the centroids on X by chunking it into mini-batches.\nkmeans_clusters = kmeans.predict(vz_sample) #Compute cluster centers and predict cluster index for each sample.\nkmeans_distance = kmeans.transform(vz_sample) #Compute clustering and transform X to cluster-distance space.\ntsne_kmeans = tsne_model.fit_transform(kmeans_distance)","82724c19":"colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])","c35c34b0":"kmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y']) #2\u6b21\u5143\u30c7\u30fc\u30bftnse_kmeans\u3092x, y\u30b3\u30e9\u30e0\u306b\u7f6e\u63db\nkmeans_df['cluster'] = kmeans_clusters\nkmeans_df['description'] = combined_sample['item_description']\nkmeans_df['category'] = combined_sample['general_cat']","3bda5719":"output_notebook() #notebook\u51fa\u529b\u306b\u306f\u3053\u306e1\u884c\u304c\u5fc5\u8981\n#\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u4f5c\u6210\nplot_kmeans = bp.figure(plot_width=700, plot_height=600,\n                      title='KMeans clustering of the item_description',\n                      tools=\"pan,wheel_zoom,box_zoom,reset,hover\",\n                      x_axis_type=None, y_axis_type=None, min_border=1)","cc5550c5":"#\u30af\u30e9\u30b9\u30bf\u30fc\u3054\u3068\u306e\u8272\u5206\u3051\u306fsource\u3067\u8a2d\u5b9a\u3057\u305f\nsource = ColumnDataSource(data=dict(x=kmeans_df['x'], y=kmeans_df['y'],\n                                   color=colormap[kmeans_clusters],\n                                   description=kmeans_df['description'],\n                                   category=kmeans_df['category'],\n                                   cluster=kmeans_df['cluster']))\n\n\nplot_kmeans.scatter(x='x', y='y', color='color', source=source)\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover.tooltips = {\"cluster\": \"@cluster\", \"description\": \"@description\", \"category\": \"@category\"}\nshow(plot_kmeans)","e86cdf37":"from sklearn.feature_extraction.text import CountVectorizer\ncvectorizer = CountVectorizer(min_df=4,\n                             max_features=180000,\n                             tokenizer=tokenize,\n                             ngram_range=(1, 2))","618e5d1e":"cvz = cvectorizer.fit_transform(combined_sample['item_description'])","66e1c24e":"from sklearn.decomposition import LatentDirichletAllocation\n\nlda_model = LatentDirichletAllocation(n_components=20,\n                                     learning_method='online',\n                                     max_iter=20,\n                                     random_state=42)","b6cbef94":"X_topics = lda_model.fit_transform(cvz)","be653e2a":"n_top_words = 10\ntopic_summaries = []\n\ntopic_word = lda_model.components_  # get the topic words\nvocab = cvectorizer.get_feature_names()\n\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))","ea9744ee":"# reduce dimension to 2 using tsne\ntsne_lda = tsne_model.fit_transform(X_topics)","5fe4edaa":"unnormalized = np.matrix(X_topics)\ndoc_topic = unnormalized\/unnormalized.sum(axis=1)\n\nlda_keys = []\nfor i, tweet in enumerate(combined_sample['item_description']):\n    lda_keys += [doc_topic[i].argmax()]\n\nlda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\nlda_df['description'] = combined_sample['item_description']\nlda_df['category'] = combined_sample['general_cat']\nlda_df['topic'] = lda_keys\nlda_df['topic'] = lda_df['topic'].map(int)","61a68035":"#\u53ef\u8996\u5316\nplot_lda = bp.figure(plot_width=700,\n                     plot_height=600,\n                     title=\"LDA topic visualization\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","db840e39":"source = ColumnDataSource(data=dict(x=lda_df['x'], y=lda_df['y'],\n                                    color=colormap[lda_keys],\n                                    description=lda_df['description'],\n                                    topic=lda_df['topic'],\n                                    category=lda_df['category']))\n\nplot_lda.scatter(source=source, x='x', y='y', color='color')\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover = plot_lda.select(dict(type=HoverTool))\nhover.tooltips={\"description\":\"@description\",\n                \"topic\":\"@topic\", \"category\":\"@category\"}\nshow(plot_lda)","e1b0c78f":"def prepareLDAData():\n    data = {\n        'vocab': vocab,\n        'doc_topic_dists': doc_topic,\n        'doc_lengths': list(lda_df['len_docs']),\n        'term_frequency':cvectorizer.vocabulary_,\n        'topic_term_dists': lda_model.components_\n    } \n    return data","9b8a4f98":"\"\"\"\nimport pyLDAvis\n\nlda_df['len_docs'] = combined_sample['token'].map(len)\nldadata = prepareLDAData()\npyLDAvis.enable_notebook()\nprepared_data = pyLDAvis.prepare(**ldadata)\n\"\"\"","bc62fc1f":"\"\"\"\nimport IPython.display\nfrom IPython.core.display import display, HTML, Javascript\n\nh = IPython.display.display(HTML(html_string))\nIPython.display.display_HTML(h)\n\"\"\"","9e4facda":"#\u6574\u5f62\u30c7\u30fc\u30bf\u4fdd\u5b58\ntrain.to_csv('re_train.csv', sep='\\t')\ntest.to_csv('re_test.csv', sep='\\t')","d1592449":"In order to plot these clusters, first we will need to reduce the dimension of the distances to 2 using tsne: ","f307d0c5":"# K-means Clustering\n\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306b\u306fMiniBatchKmeans\u3092\u4f7f\u3046\u3068\u8a08\u7b97\u6642\u9593\u304c\u65e9\u304f\u6e08\u3080","8386817f":"vz\u306f\u300ckaggle\u3067\u52dd\u3064\u30c7\u30fc\u30bf\u5206\u6790\u300dp203\u306e\u30c6\u30fc\u30d6\u30eb\u3092\u884c\u5217\u306b\u3057\u305f\u3082\u306e\n* \u884c\u306e\u7dcf\u6570\u306fdescription\u306e\u7dcf\u6570\n* \u5217\u306e\u7dcf\u6570\u306f\u5358\u8a9e\u306e\u7a2e\u985e\u306e\u7dcf\u6570","2d537107":"# t-SNE\u3067tfidf\u30d9\u30af\u30c8\u30eb\u306e\u30b5\u30a4\u30ba\u30922\u6b21\u5143\u5727\u7e2e\u3059\u308b","efce7f9b":"t-SNE\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u81a8\u5927\u306a\u305f\u3081,\u59cb\u3081\u306b\u524d\u51e6\u7406\u306e\u524d\u51e6\u7406\u3068\u3057\u3066SVD\u3092\u7528\u3044\u3066\u6b21\u5143\u309230\u306b\u5727\u7e2e\u3059\u308b","567e6170":"Now we can reduce the dimension from 30 to 2 using t-SNE!","ae39d919":"Note: It's a shame that by putting the HTML of the visualization using pyLDAvis, it will distort the layout of the kernel, I won't upload in here. But if you follow the below code, there should be an HTML file generated with very interesting interactive bubble chart that visualizes the space of your topic clusters and the term components within each topic.","c2e4682f":"## **Pre-processing:  tokenization**\n\nMost of the time, the first steps of an NLP project is to **\"tokenize\"** your documents, which main purpose is to normalize our texts. The three fundamental stages will usually include: \n* item_description\u3092\u30c6\u30ad\u30b9\u30c8\u3068\u3057\u3066\u6587\u5b57\u5217\u306b\u5909\u63db\u3057,\u305d\u308c\u3092tokenizer\u3067\u610f\u5473\u306e\u3042\u308b\u5358\u8a9e\u3092\u62bd\u51fa\u3059\u308b\u305f\u3081\u306b\u30c8\u30fc\u30af\u30f3\u5358\u4f4d\u3067\u5206\u5272\u3059\u308b.\n* \u53e5\u8aad\u70b9\u3068\u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u3092\u9664\u304f\n* \u30c8\u30fc\u30af\u30f3\u3092\u5c0f\u6587\u5b57\u5909\u63db\u3059\u308b\n* 3\u3064\u4ee5\u4e0a\u306e\u30c8\u30fc\u30af\u30f3\u304c\u3042\u308bdescription\u3060\u3051\u3092\u8003\u616e\u3059\u308b.","4fff68b9":"# **category_name**","4a01e31c":"2\u6b21\u5143\u306b\u5727\u7e2e\u3067\u304d\u305f\u306e\u3067\u53ef\u8996\u5316\u3067\u304d\u308b","99ff7f91":"# **price**","39d03542":"Woman's products are popular.","087819ea":"# Brand Name\n\u307b\u3068\u3093\u3069\u304c\u6b20\u640d\u5024\u3067\u3042\u308b\u3053\u3068\u306b\u6ce8\u610f.","bfaa549f":"# LDA(Latent Dirichlet Allocation)","0ec25bc9":"# Category price distribution","565ff361":"re.escape\npattern \u4e2d\u306e\u7279\u6b8a\u6587\u5b57\u3092\u30a8\u30b9\u30b1\u30fc\u30d7\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u6b63\u898f\u8868\u73fe\u30e1\u30bf\u6587\u5b57\u3092\u542b\u307f\u3046\u308b\u4efb\u610f\u306e\u30ea\u30c6\u30e9\u30eb\u6587\u5b57\u5217\u306b\u30de\u30c3\u30c1\u3057\u305f\u3044\u6642\u306b\u4fbf\u5229\u3002\nstring.punctuation: !\"#$%&'()*+,-.\/:;<=>?@[\\]^_`{|}~","8bc27888":"# Item Description (\u30a2\u30a4\u30c6\u30e0\u8aac\u660e)","9e77d7ed":"idf\u5024\u304c\u4f4e\u3044\u30ef\u30fc\u30b9\u30c810\u306e\u5358\u8a9e\n\u898b\u3066\u5206\u304b\u308b\u901a\u308a\u3001\u666e\u6bb5\u4f7f\u3046\u4e00\u822c\u7684\u306a\u7528\u8a9e\u3067\u91cd\u8981\u5ea6\u304c\u4f4e\u3044\u3068\u3044\u3048\u308b.","d60f8593":"idf\u5024\u304c\u9ad8\u3044\u30c8\u30c3\u30d710\u306e\u5358\u8a9e\n\u3053\u308c\u3089\u306e\u7279\u5fb4\u7684\u306a\u5358\u8a9e\u304b\u3089\u30ab\u30c6\u30b4\u30ea\u30fc\u3092\u63a8\u6e2c\u3067\u304d\u308b.","e70c3735":"# Pre_processing:tf-idf\ntf-idf\u3092\u7528\u3044\u305f\u91cd\u8981\u5ea6\u306e\u9ad8\u3044\u5358\u8a9e\u306e\u62bd\u51fa\u3092\u884c\u3046.\n\n* tf (term frequency) : \u3042\u308b\u30c6\u30ad\u30b9\u30c8\u3067\u306e\u305d\u306e\u5358\u8a9e\u306e\u51fa\u73fe\u6bd4\u7387\n* idf (inverse document frequency) : \u305d\u306e\u5358\u8a9e\u304c\u5b58\u5728\u3059\u308b\u30c6\u30ad\u30b9\u30c8\u306e\u5272\u5408\u306e\u9006\u6570\n\nidf\u306f\u7279\u5b9a\u306e\u30c6\u30ad\u30b9\u30c8\u306b\u3057\u304b\u51fa\u73fe\u3057\u306a\u3044\u5358\u8a9e\u306e\u91cd\u8981\u5ea6\u3092\u9ad8\u3081\u308b\u50cd\u304d\u3092\u3059\u308b."}}