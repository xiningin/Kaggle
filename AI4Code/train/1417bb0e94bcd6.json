{"cell_type":{"557419e4":"code","21c2d409":"code","45f7dea9":"code","02f93893":"code","5d5f3dc7":"code","49aa5d7e":"markdown"},"source":{"557419e4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, gc, sys, warnings, random, math, datetime, psutil, pickle\n\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport lightgbm as lgb\n\ntrain = pd.read_pickle('..\/input\/ashraedataminification\/train_df.pkl')\ntest = pd.read_pickle('..\/input\/ashraedataminification\/test_df.pkl')\ntarget = 'meter_reading'\n\n## Simple \"Memory profilers\" to see memory usage\ndef get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]\/2.**30, 2) \nprint('Memory in Gb', get_memory_usage())","21c2d409":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        if col!=target:\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\ngc.collect()\nprint('Memory in Gb', get_memory_usage())","45f7dea9":"# force the model to use the weather data instead of dates, to avoid overfitting to the past history\nfeatures = [col for col in train.columns if col not in [target, 'DT_Y', 'DT_M', 'DT_W', 'DT_D', 'DT_day_month', 'DT_week_month']]\nfolds = 4\nseed = 42\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\nmodels = []\n\n## stratify data by building_id\nfor tr_idx, val_idx in tqdm(kf.split(train, train['building_id']), total=folds):\n    tr_x, tr_y = train[features].iloc[tr_idx], train[target][tr_idx]\n    vl_x, vl_y = train[features].iloc[val_idx], train[target][val_idx]\n    print({'train size':len(tr_x), 'eval size':len(vl_x)})\n\n    tr_data = lgb.Dataset(tr_x, label=tr_y)\n    vl_data = lgb.Dataset(vl_x, label=vl_y)  \n    clf = lgb.LGBMRegressor(n_estimators=6000,\n                            learning_rate=0.28,\n                            feature_fraction=0.9,\n                            subsample=0.2,  # batches of 20% of the data\n                            subsample_freq=1,\n                            num_leaves=20,\n                            metric='rmse')\n    clf.fit(tr_x, tr_y,\n            eval_set=[(vl_x, vl_y)],\n            early_stopping_rounds=50,\n            verbose=200)\n    models.append(clf)\n    \ngc.collect()\n","02f93893":"# split test data into batches\nset_size = len(test)\niterations = 50\nbatch_size = set_size \/\/ iterations\n\nmeter_reading = []\nfor i in tqdm(range(iterations)):\n    pos = i*batch_size\n    fold_preds = [np.expm1(model.predict(test[features].iloc[pos : pos+batch_size])) for model in models]\n    meter_reading.extend(np.mean(fold_preds, axis=0))\n","5d5f3dc7":"submission = pd.read_csv('..\/input\/ashrae-energy-prediction\/sample_submission.csv')\nsubmission['meter_reading'] = np.clip(meter_reading, a_min=0, a_max=None) # clip min at zero\nsubmission.to_csv('submission.csv', index=False)","49aa5d7e":"This notebook is built on top of the [data minification one](https:\/\/www.kaggle.com\/jiaofenx\/ashrae-data-minification)."}}