{"cell_type":{"976d9b93":"code","5450df4b":"code","6a9b5122":"code","3270c98f":"code","05934fce":"code","0aca91bc":"code","b9b9ba39":"code","905654ee":"markdown"},"source":{"976d9b93":"!git clone https:\/\/github.com\/bayartsogt-ya\/AliceMind.git","5450df4b":"%cd AliceMind\/StructBERT","6a9b5122":"ls","3270c98f":"%%time\n!source download_checkpoint.sh","05934fce":"# !mkdir output\n!ls data\/commonlit\/fold_0\n!ls config\/large_bert_config.json\n!ls config\/vocab.txt\n!ls models\/en_model","0aca91bc":"!python run_classifier_multi_task.py \\\n    --data_dir data\/commonlit\/fold_0 \\\n    --bert_config_file config\/large_bert_config.json \\\n    --task_name commonlit \\\n    --vocab_file config\/vocab.txt \\\n    --output_dir output \\\n    --init_checkpoint models\/en_model \\\n    --max_seq_length 256 \\\n    --do_train \\\n    --do_eval \\\n    --lr_decay_factor 0.1 \\\n    --dropout 0.1 \\\n    --train_batch_size 8 \\\n    --eval_batch_size 8 \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 10 \\\n    --warmup_proportion 0.1 \\\n    --save_model  \\\n    --seed 1000 \\\n    --num_workers 2\n# \\\n#     --debug","b9b9ba39":"\"\"\"\nground_truth | prediction\n------------ | ----------\n0.19968727231025696\t-0.264123797416687\n0.08814816176891327\t-0.5634984374046326\n-1.1981923580169678\t-1.7749090194702148\n-1.1999571323394775\t-0.5880406498908997\n\"\"\"\n!head output\/commonlit.eval_logits_EP7","905654ee":"# StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\n\n* [Paper](https:\/\/arxiv.org\/abs\/1908.04577)\n* [Official github repo](https:\/\/github.com\/alibaba\/AliceMind\/tree\/main\/StructBERT)\n* [Forked repo for this CommonLit Readability Prize Competition](https:\/\/github.com\/bayartsogt-ya\/AliceMind\/tree\/main\/StructBERT)\n\n![Screen Shot 2021-07-27 at 3.49.58 PM.png](attachment:ca1e882d-05b4-42d6-9f80-b0cf02c5a464.png)\n\n## Introduction\nWe extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. \nSpecifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential \norder of words and sentences, which leverage language structures at the word and sentence levels, \nrespectively.\n\n## Results\nThe results of GLUE & CLUE tasks can be reproduced using the hyperparameters listed in the following \"Example usage\" section.\n#### structbert.en.large\n[GLUE benchmark](https:\/\/gluebenchmark.com\/leaderboard)\n\n|Model| MNLI | QNLIv2 | QQP | SST-2 | MRPC | \n|--------------------|-------|-------|-------|-------|-------|\n|structbert.en.large |86.86% |93.04% |91.67% |93.23% |86.51% |\n\n#### structbert.ch.large\n[CLUE benchmark](https:\/\/www.cluebenchmarks.com\/)\n\n|Model | CMNLI | OCNLI | TNEWS | AFQMC |\n|--------------------|-------|-------|-------|-------|\n|structbert.ch.large |84.47% |81.28% |68.67% |76.11% | \n"}}