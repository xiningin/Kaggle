{"cell_type":{"3adbbb32":"code","6368028d":"code","3611c898":"code","e124e6d1":"code","e6ad10cb":"code","c689412e":"markdown","11839ca7":"markdown","d0330319":"markdown","838e4066":"markdown"},"source":{"3adbbb32":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport math\nimport time\nfrom sklearn.naive_bayes import GaussianNB,CategoricalNB\nimport numpy as np\nfrom sklearn import metrics\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6368028d":"df=pd.read_csv(\"\/kaggle\/input\/adults\/adult.data\",header=None,names=['Age','Workplace','fnlwgt','education','education num','marital-stauts','occupation','relationship','race','sex','captial gain','capital loss','hours per week','native country','Salaray'])\ndf","3611c898":"def removeQuestionMark(outCol,inpCol):\n    for i in inpCol:\n        repl=X[i].value_counts().keys().tolist()[0]\n        X[i]=X[i].replace(to_replace=' ?',value=repl)\n    rep=y[outCol[0]].value_counts().keys().tolist()[0]\n    y[outCol[0]]=y[outCol[0]].replace(to_replace=' ?',value=rep)\n    \ndef Splitting(X,y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    return [X_train,y_train,X_test,y_test]\n\ndef CategTraining(X_train,y_train,outCol):\n    trainCount=y_train[outCol[0]].value_counts().to_dict()\n    col1=list(X_train.columns)\n    outputList=y_train[outCol[0]].value_counts().keys().tolist()\n    trainDict=dict([(key, []) for key in col1])\n    for i in range(0,len(col1)):\n        inputList=X_train[col1[i]].value_counts().keys().tolist()\n\n        proxy=dict([(key, dict([(keys, []) for keys in outputList])) for key in inputList])\n        trainDict[col1[i]]=proxy\n    for i in trainDict.keys():\n        for j in trainDict[i].keys():\n            for k in trainDict[i][j].keys():\n                num=(X_train.loc[(X_train[i]==j) &  (y_train[outCol[0]]==k)]).shape[0]\n                den=trainCount[k]\n                prob=num\/den\n                trainDict[i][j][k]=prob\n    return trainDict\n\ndef CategTesting(outCol,trainDict,y_train,y_test,X_test):\n    trainProb=(y_train[outCol[0]].value_counts()\/y_train.shape[0]).to_dict()\n    outputList=y_train[outCol[0]].value_counts().keys().tolist()\n    testDict1=dict([(key,dict([(keys, []) for keys in outputList])) for key in y_test.index])\n    for i in testDict1.keys():\n        for j in testDict1[i].keys():\n            prob=1\n            l=0\n            for k in trainDict.keys():\n                prob=trainDict[k][X_test.loc[i][l]][j]*prob\n                l=l+1\n            testDict1[i][j]=prob*trainProb[j]\n    return testDict1\n\ndef CategPredict(testDict,y_test):\n    size=X_test.shape[0]\n    predict=dict((key,[]) for key in y_test.index)\n    for i in predict.keys():\n        maxi=0\n        l=''\n        for j in testDict[i].keys():\n            if(testDict[i][j]>maxi):\n                maxi=testDict[i][j]\n                l=j\n        predict[i]=l\n    accuracy=0\n    count=0\n    for i in predict.keys():\n        if(y_test.loc[i][0]==predict[i]):\n            count=1+count\n    accuracy=count*100\/size\n    print(accuracy)\n\ndef Probop(y_test,posOp):\n    Probop=dict([(key,[]) for key in posOp])\n    for i in Probop.keys():\n        Probop[i]=(y_test['Salaray'].value_counts()[i])\/y_test.shape[0]\n    return Probop\n\ndef ContTraining(X_train,y_train):\n    NumericDict=dict([(key,dict([(keys, []) for keys in posOp])) for key in X_train.columns])\n    Inplist=dict([(key,dict([(keys, []) for keys in posOp])) for key in X_train.columns])\n    for i in NumericDict.keys():\n        for j in NumericDict[i].keys():\n            Inplist[i][j]=(y_train.loc[y_train['Salaray']==j]).index.tolist()\n    for i in NumericDict.keys():\n        for j in NumericDict[i].keys():\n            count=len(Inplist[i][j])\n            su=0\n            for k in range(0,count):\n                su+=(X_train.loc[Inplist[i][j][k]])[i]\n            NumericDict[i][j]=[su\/count]\n    for i in Inplist.keys():\n        for j in Inplist[i].keys():\n            count=len(Inplist[i][j])\n            diff=0\n            for k in range(0,count):\n                diff+=((X_train.loc[Inplist[i][j][k]])[i]-NumericDict[i][j][0])**2\n            NumericDict[i][j].append(diff\/count)\n    return NumericDict    \n\ndef ContTesting(X_test,posOp,NumericDict):\n    testInd=X_test.index\n    prediction=dict([(key,dict([(keys,dict([(key,[]) for key in posOp])) for keys in testInd])) for key in X_test.columns])\n    for i in prediction.keys():\n        for j in posOp:\n            den=math.sqrt(2*math.pi*(NumericDict[i][j][1]**2))\n            count=len(prediction[i])\n            for k in range(0,count):\n                num=math.exp(-(X_test.loc[testInd[k]][i]-NumericDict[i][j][0]\/(2*math.pow(NumericDict[i][j][1],2))))\n                prediction[i][testInd[k]][j]=num\/den\n    Predict=dict([(key,dict([(keys, []) for keys in posOp])) for key in y_test.index])\n    for i in Predict.keys():\n        for j in posOp:\n            prob=1\n            for k in prediction.keys():\n                if(prediction[k][i][j]!=0):\n                    prob=prob*prediction[k][i][j]\n            Predict[i][j]=prob\n    return Predict\n\ndef ContPredict(prediction,ProbOp,y_test):\n    testInd=y_test.index\n    FinalPrediction=dict([(keys,[]) for keys in testInd])\n    for i in testInd:\n        maxi=0\n        l=''\n        for j in posOp:\n            prob=1\n            for k in prediction.keys():\n                if(prediction[k][i][j]!=0):\n                    prob=prob*prediction[k][i][j]\n            prob=prob*ProbOp[j]\n            if(prob>maxi):\n                maxi=prob\n                l=j\n        FinalPrediction[i]=l \n    acc=0\n    count=0\n    length=len(testInd)\n    for i in range(0,length):\n            if(FinalPrediction[testInd[i]]==y_test.iloc[i,0]):\n                count+=1\n    acc=count*100\/y_test.shape[0]\n    print(acc)\n    \ndef Predict(testDict,prediction,ProbOp,y_test):\n    testInd=y_test.index\n    FinalPrediction=dict([(keys,[]) for keys in testInd])\n    for i in FinalPrediction.keys():\n        maxi=0\n        pr=''\n        p=0\n        for j in posOp:\n            p=prediction[i][j]*testDict[i][j]\n            if(p>maxi):\n                maxi=p\n                pr=j\n        FinalPrediction[i]=pr\n    acc=0\n    count=0\n    length=len(testInd)\n    for i in range(0,length):\n            if(FinalPrediction[testInd[i]]==y_test.iloc[i,0]):\n                count+=1\n    acc=count*100\/y_test.shape[0]\n    return acc","e124e6d1":"X=df[['Workplace','education','race','sex','marital-stauts','occupation','relationship','native country','Age','fnlwgt','education num','captial gain','capital loss','hours per week']]\ny=df[['Salaray']]\n[x_train,y_train,x_test,y_test]=Splitting(X,y)\ninpCol=list(X.columns)\noutCol=list(y.columns)\nremoveQuestionMark(outCol,inpCol)\nX_train=x_train[['Workplace','education','race','sex','marital-stauts','occupation','relationship','native country']]\nX_test=x_test[['Workplace','education','race','sex','marital-stauts','occupation','relationship','native country']]\ntrainDict=CategTraining(X_train,y_train,outCol)\ntestDict=CategTesting(outCol,trainDict,y_train,y_test,X_test)\nX_train=x_train[['Age','fnlwgt','education num','captial gain','capital loss','hours per week']]\nX_test=x_test[['Age','fnlwgt','education num','captial gain','capital loss','hours per week']]\nposOp=y_train['Salaray'].value_counts().keys().tolist()\nsize=y_train.shape[0]\nProbOp=Probop(y_test,posOp)\nNumericDict=ContTraining(X_train,y_train)\nprediction=ContTesting(X_test,posOp,NumericDict)\nprint(\"The accuracy obtained from scratched Naive Bayes is \"+str(Predict(testDict,prediction,ProbOp,y_test)))","e6ad10cb":"X=df[['Workplace','education','race','sex','marital-stauts','occupation','relationship','native country','Age','fnlwgt','education num','captial gain','capital loss','hours per week']]\ny=df[['Salaray']]\n[x_train,y_train,x_test,y_test]=Splitting(X,y)\ninpCol=list(X.columns)\noutCol=list(y.columns)\nremoveQuestionMark(outCol,inpCol)\n[x_train,y_train,x_test,y_test]=Splitting(X,y)\nX_train=x_train[['Workplace','education','race','sex','marital-stauts','occupation','relationship','native country']]\nX_test=x_test[['Workplace','education','race','sex','marital-stauts','occupation','relationship','native country']]\ntrainDict=CategTraining(X_train,y_train,outCol)\ntestDict=CategTesting(outCol,trainDict,y_train,y_test,X_test)\nX_train=x_train[['Age','fnlwgt','education num','captial gain','capital loss','hours per week']]\nX_test=x_test[['Age','fnlwgt','education num','captial gain','capital loss','hours per week']]\nposOp=y_train['Salaray'].value_counts().keys().tolist()\nsize=y_train.shape[0]\nProbOp=Probop(y_test,posOp)\nclf = GaussianNB()\nclf.fit(X_train,y_train)\ns=clf.predict_proba(X_test)\nr=0\nc=0\nprediction=dict([(key,dict([(keys, []) for keys in posOp])) for key in y_test.index])\nfor i in prediction.keys():\n    c=0\n    for j in prediction[i].keys():\n        prediction[i][j]=s[r][c]\n        c=c+1\n    r=r+1\nprint(\"The accuracy obtained after using sklearn of Gaussian Naive Bayes and scratched categNaive Bayes is \"+str(Predict(testDict,prediction,ProbOp,y_test)))","c689412e":"Now that we have wrote all the functions for our algorithm of Naive Bayes we need to use them for our objetive of predicting the salary of a person.\nWe first split the dataset,remove null values,train only the categorical columns of the data and obtain the posterior probabilities and then test it to obtain the poseterior probabilities for the test dataset,\nSimilarly we do the same for continoue columns and put it up in the predict function where the accuracy of our algorithm is obtained.","11839ca7":"The cell below houeses all the custom made functions from removing null values in form of question marks to splitting the entire dataset,to training and testing of the data in categorical bayes theorem and continous bayes theorem by using their respective formulas and then predicting on the whole test dataset by combining both of the probabilities obtained in the predict function.\nThe sklearn library wasn't available back when I had coded this notebook but for a mixed dataset it's not available.To  efficienty predicts for the whole dataset I have used Gaussian Naive Bayes of sklearn for continous Columns a and my scratched Naive Bayes for categorical Columns and then combined them to predict the Salary.\nSo right please browse through and understand each and every function rightly so.","d0330319":"# The Objective of this Notebook is to predict the Salary of any random person taken in the world.\n# The  target attribute Salary has two values either greater than 50 k or less than or equal to 50k which makes it a classification task.The algorithm in consideration below is that of Naive Bayes and for fun I decided to code the entire algorithm with scratch which was super helpful to understand the math behind the algorithm.\n# Hope you find it useful.","838e4066":"Importing all the needed libraries and importing the dataset,you can find it on this link http:\/\/archive.ics.uci.edu\/ml\/datasets\/Adult"}}