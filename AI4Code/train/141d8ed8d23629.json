{"cell_type":{"66f50f33":"code","45ba83c5":"code","2f6fc40a":"code","054df83e":"code","ff855fe3":"code","4e0b71a0":"code","932ee3ba":"code","315c6acd":"code","b956d7ed":"code","b797c0b3":"code","f6d4086e":"code","056cda26":"code","d6bf29ae":"code","4e179cdd":"code","53b817f9":"code","aa1de21d":"code","375b2a84":"code","9b764925":"code","76e5c1c6":"code","a84f9bcc":"code","1d5ee530":"code","4cb77fc1":"code","36cc78cf":"code","64254f46":"code","e378659d":"code","18a50e14":"code","8a0a988f":"code","f7f9e3ed":"markdown","612b38b0":"markdown","85c08847":"markdown","9cccde61":"markdown","9caa7723":"markdown","66001f8a":"markdown","79f7c79d":"markdown","6512bee5":"markdown","d777af40":"markdown","75183a27":"markdown","6dd4c646":"markdown","de68850d":"markdown","8875948b":"markdown","e3a3910d":"markdown","b1bed367":"markdown","000e2e36":"markdown","64a6c21c":"markdown","3efcf973":"markdown","1ea55741":"markdown","b13bf0a8":"markdown","dcef16ed":"markdown","05bdfad8":"markdown","c9301d52":"markdown"},"source":{"66f50f33":"import time\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nimport gpxpy.geo # Get the haversine distance\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nimport math\nfrom prettytable import PrettyTable","45ba83c5":"\nbase_path = \"..\/input\/taxidemandfarepredictiondataset\/\"\n# we speed the process by decreasing the dimensionality\ncolumns=['tpep_pickup_datetime',\n           'tpep_dropoff_datetime',\n           'trip_distance',\n           'pickup_longitude',\n           'pickup_latitude',\n           'dropoff_longitude',\n           'dropoff_latitude',\n           'total_amount']\n\n\n\n\ndf_2015_1 = pd.read_csv(f'{base_path}yellow_tripdata_2015-01.csv', usecols=columns, nrows=1000000)\ndf_2015_2 = pd.read_csv(f'{base_path}yellow_tripdata_2015-02.csv', usecols=columns, nrows=1000000)\ndf_2015_3 = pd.read_csv(f'{base_path}yellow_tripdata_2015-03.csv', usecols=columns, nrows=1000000)\n\ndf_2016_1 = pd.read_csv(f'{base_path}yellow_tripdata_2016-01.csv', usecols=columns, nrows=1000000)\ndf_2016_2 = pd.read_csv(f'{base_path}yellow_tripdata_2016-02.csv', usecols=columns, nrows=1000000)\ndf_2016_3 = pd.read_csv(f'{base_path}yellow_tripdata_2016-03.csv', usecols=columns, nrows=1000000)\n\ndf_2015 = df_2015_1.append(df_2015_2).append(df_2015_3)\ndf_2016 = df_2016_1.append(df_2016_2).append(df_2016_3)\n\noriginal_2015_len = df_2015.shape[0]\noriginal_2016_len = df_2016.shape[0]","2f6fc40a":"\n'''\nIn this fucntion we will drop all the longitude and latitude which are 0s or empty or nan etc\nand we will only take trip which is between 5$ to 45$ removing upper and lower bounds (Outlier removal)\n\nFor that you can make quantiles and take quantiles between your specefic range\n'''\ndef clean_data(df, test=False, predict=False):\n    df = df.dropna(how='any', axis='rows')\n    df = df[(df.dropoff_latitude != 0) | (df.dropoff_longitude != 0)]\n    df = df[(df.pickup_latitude != 0) | (df.pickup_longitude != 0)]\n    \n    if \"total_amount\" in list(df):\n        df = df[df.total_amount.between(5, 45)]\n    \n    return df\n\ndf_2015 = clean_data(df_2015)\ndf_2016 = clean_data(df_2016)","054df83e":"# to decide where to start removing outliers\ndef remove_outliers(data, start=0, end=100):\n    data=np.sort(data)\n    for i in np.linspace(start, end, 10):\n        i=round(i, 6)\n        print(str(i).zfill(5) + \" percentile value is \" + str(round(data[int(len(data)*(float(i)\/100))-1], 1)))\n    print(str(float(end)).zfill(3) + \" percentile value is \" + str(data[-1]))","ff855fe3":"# drop rows with coordinates outside NYC \ndef clean_coordinates(df):\n    nrows = df.shape[0]\n    df.drop(df.index[\n        \n            ~((df['pickup_latitude'].between(40.496115395170364, 40.91553277700258)) &\n              (df['pickup_longitude'].between(-74.25559136315209, -73.7000090639354))) \n        \n    ], inplace=True)\n    print(\"Number of rows removed due to wrong coordinates is {}\".format(nrows - df.shape[0]))\n    \nclean_coordinates(df_2015)\nclean_coordinates(df_2016)","4e0b71a0":"def clean_trip_duration(df):\n    # convert from object to datetime\n    df['tpep_pickup_datetime']  = pd.to_datetime(df['tpep_pickup_datetime'])\n    df['tpep_dropoff_datetime']  = pd.to_datetime(df['tpep_dropoff_datetime'])\n    \n    # copute the time diffrance between pickup & dropoff\n    # to covert from nanosecondes to minutes we devide by 1000000000 then by 60\n    # store trip_duratin column\n    trip_duration = np.array(df['tpep_dropoff_datetime']-df['tpep_pickup_datetime'])\n    trip_duration = trip_duration\/1000000000\/60\n    df['trip_duration'] = trip_duration.astype(float)\n    \n    # drop all records that have trip_duration > 2 hours\n    #                            trip_duration <= 0\n    #                            trip_distance <= 0\n    nrows = df.shape[0]\n    df.drop(df[(df['trip_duration'] > 160) | \n               (df['trip_duration'] <= 0)].index, inplace = True)\n    print(\"Number of rows removed due to wrong trip_duration {}\".format(nrows - df.shape[0]))\n    \n    \nclean_trip_duration(df_2015)\nclean_trip_duration(df_2016)","932ee3ba":"def clean_pickuptime(df):\n    return df.rename(columns={'tpep_pickup_datetime': 'pickup_time'})\n\ndf_2015 = clean_pickuptime(df_2015)\ndf_2016 = clean_pickuptime(df_2016)\n\n\n","315c6acd":"def clean_trip_distance(df):\n    nrows = df.shape[0]\n    df.drop(df[(df['trip_distance'] <= 0) | (df['trip_distance'] > 77.5)].index, inplace = True)\n    print(\"Number of rows removed due to speed outliers {}\".format(nrows - df.shape[0]))\n    \nclean_trip_distance(df_2015)\nclean_trip_distance(df_2016)","b956d7ed":"def compute_speed(df):\n    # computing Taxi speed average (mile\/hour)\n    df['speed'] = df['trip_distance']\/df['trip_duration']*60\n    \ndef clean_speed(df):\n\n    # Removing speed anomaly\/outliers\n    nrows = df.shape[0]\n    df.drop(df[((df['speed'] <= 0) | (df['speed'] > 63.0))].index, inplace = True)\n    print(\"Number of rows removed due to speed outliers {}\".format(nrows - df.shape[0]))\n\n\ncompute_speed(df_2015)\ncompute_speed(df_2016)    \nclean_speed(df_2015)\nclean_speed(df_2016)","b797c0b3":"from datetime import datetime, timedelta\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nfrom pandarallel import pandarallel\n\n\n#Clustering pickups\nprint(\"Getting clusters\")\ncoord = df_2015[[\"pickup_latitude\", \"pickup_longitude\"]].values\nregions = MiniBatchKMeans(n_clusters = 30, batch_size = 10000).fit(coord)\n\nprint(\"Predicting clusters\")\ncluster_column = regions.predict(df_2015[[\"pickup_latitude\", \"pickup_longitude\"]])\ncluster_column_2016 = regions.predict(df_2016[[\"pickup_latitude\", \"pickup_longitude\"]])\ndf_2015[\"pickup_cluster\"] = cluster_column\ndf_2016[\"pickup_cluster\"] = cluster_column_2016\n\n\n","f6d4086e":"# Replacing mins and sec with 0\nprint(\"Removing Hours and seconds\")\npandarallel.initialize()\ndf_2015['pickup_time'] = df_2015.pickup_time.parallel_apply(lambda x : pd.to_datetime(x).replace(minute=0, second=0) + timedelta(hours=1))\ndf_2016['pickup_time'] = df_2016.pickup_time.parallel_apply(lambda x : pd.to_datetime(x).replace(minute=0, second=0) + timedelta(hours=1))\n","056cda26":"\nprint(\"Group by Cluster and time\")\ndf2 = df_2015.groupby(['pickup_time','pickup_cluster']).size().reset_index(name='count')\ndf1 = df_2016.groupby(['pickup_time','pickup_cluster']).size().reset_index(name='count')\n\nprint(\"Converting counts to demand percentage\")\ndf2['count'] = df2['count'].parallel_apply(lambda x :  (x \/ df2['count'].max()))\ndf1['count'] = df1['count'].parallel_apply(lambda x :  (x \/ df1['count'].max()))\n\n\nprint(\"Getting month, days, hours, day of week\")\ndf2['month'] = pd.DatetimeIndex(df2['pickup_time']).month\ndf2['day'] = pd.DatetimeIndex(df2['pickup_time']).day\ndf2['dayofweek'] = pd.DatetimeIndex(df2['pickup_time']).dayofweek\ndf2['hour'] = pd.DatetimeIndex(df2['pickup_time']).hour\n\n\ndf1['month'] = pd.DatetimeIndex(df1['pickup_time']).month\ndf1['day'] = pd.DatetimeIndex(df1['pickup_time']).day\ndf1['dayofweek'] = pd.DatetimeIndex(df1['pickup_time']).dayofweek\ndf1['hour'] = pd.DatetimeIndex(df1['pickup_time']).hour\n","d6bf29ae":"# training X and y\nX_2015_1 = df2[['pickup_cluster', 'month', 'day', 'hour', 'dayofweek']]\ny_2015_1 = df2['count']\n\n\n# training X and y\nX_2016_1 = df1[['pickup_cluster', 'month', 'day', 'hour', 'dayofweek']]\ny_2016_1 = df1['count']\n\nprint(len(X_2015_1))\nprint(len(y_2015_1))\n","4e179cdd":"\n# X_2016.to_csv(\"X_2016_X.csv\")\n# y_2016.to_csv(\"X_2016_Y.csv\")\nfrom sklearn.model_selection import train_test_split\nX_2015, X_2016, y_2015, y_2016 = train_test_split(\n     X_2015_1.values, y_2015_1.values, test_size=0.33, random_state=42)","53b817f9":"print('model training 0\/3 (creating model)', end='\\r')\nLReg = LinearRegression()\n\nprint('model training 1\/3 (fitting model)', end='\\r')\nLReg.fit(X_2015, y_2015)\n\nprint('model training 2\/3 (training model)', end='\\r')\nLReg_y_pred = LReg.predict(X_2016)\n\nprint('model training 3\/3 done!           ', end='\\r')","aa1de21d":"print('model training 0\/3 (creating model)', end='\\r')\nRFRegr = RandomForestRegressor()\n\nprint('model training 1\/3 (fitting model)', end='\\r')\nRFRegr.fit(X_2015, y_2015)\n\nprint('model training 2\/3 (training model)', end='\\r')\nRFRegr_y_pred = RFRegr.predict(X_2016)\n\nprint('model training 3\/3 done!           ', end='\\r')","375b2a84":"print('model training 0\/3 (creating model)', end='\\r')\nGBRegr = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n\nprint('model training 1\/3 (fitting model)', end='\\r')\nGBRegr.fit(X_2015, y_2015)\n\nprint('model training 2\/3 (training model)', end='\\r')\nGBRegr_y_pred = GBRegr.predict(X_2016)\n\nprint('model training 3\/3 done!           ', end='\\r')","9b764925":"def model_evaluation(algorithem_name, X_Test, y_pred, y_true):\n    \n    # R2 and Adjasted R2\n    r2 = r2_score(y_true, y_pred)\n    adj_r2 = 1-(1-r2)*((len(X_Test)-1)\/(len(X_Test)-X_Test.shape[1]-1))\n    # MSE and RMSE\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = math.sqrt(mse)\n    \n    # print in table\n    x = PrettyTable()\n    x.add_row(['R2', r2])\n    x.add_row(['Adjusted R2', adj_r2])\n    x.add_row(['MSE',mse])\n    x.add_row(['RMSE', rmse])\n    x.title = algorithem_name\n    print(x)\n    \n","76e5c1c6":"model_evaluation('y True',X_Test=X_2016, y_pred=y_2016, y_true=y_2016)","a84f9bcc":"model_evaluation('Linear Regression',X_Test=X_2016, y_pred=LReg_y_pred, y_true=y_2016)","1d5ee530":"model_evaluation('Random Forest',X_Test=X_2016, y_pred=RFRegr_y_pred, y_true=y_2016)","4cb77fc1":"model_evaluation('Gradient Boosting',X_Test=X_2016, y_pred=GBRegr_y_pred, y_true=y_2016)","36cc78cf":"LReg_y_pred = LReg.predict(X_2016_1)\nRFRegr_y_pred = RFRegr.predict(X_2016_1)\nGBRegr_y_pred = GBRegr.predict(X_2016_1)","64254f46":"model_evaluation('y True',X_Test=X_2016_1, y_pred=y_2016_1, y_true=y_2016_1)","e378659d":"model_evaluation('Linear Regression',X_Test=X_2016_1, y_pred=LReg_y_pred, y_true=y_2016_1)","18a50e14":"model_evaluation('Linear Regression',X_Test=X_2016_1, y_pred=RFRegr_y_pred, y_true=y_2016_1)","8a0a988f":"model_evaluation('Linear Regression',X_Test=X_2016_1, y_pred=GBRegr_y_pred, y_true=y_2016_1)","f7f9e3ed":"### 3. <font color='red'>**pickup_time**<\/font>","612b38b0":"### 4. <font color='red'>**Saving Preprocessed DataFrame for Later processing**<\/font>","85c08847":"### 1. <font color='red'>**pickup_latitude**<\/font> and <font color='red'>**pickup_longitude**<\/font>  \n[NYC Coordinates Source](https:\/\/data.cityofnewyork.us\/Transportation\/NYC-Taxi-Zones\/d3c5-ddgc)","9cccde61":"### 2. <font color='red'>**trip_duration**<\/font> - **tpep_pickup_datetime** and **tpep_dropoff_datetime**","9caa7723":"<h1 align=\"center\">Models Training<\/h1> ","66001f8a":"<h1 align=\"center\">Data Cleaning<\/h1> ","79f7c79d":"## Preprocessing \n\nPreprocessing Includes some basic tasks like\n\n1. Removing outliers\n2. Build more features which help our model to learn things\n3. Identifying and removing null values","6512bee5":"### 3. <font color='red'>**Random Forest**<\/font> ","d777af40":"## Testing","75183a27":"### 4. <font color='red'>**trip_distance**<\/font>","6dd4c646":"## Gradiant Boasting","de68850d":"### Actual","8875948b":"### 4. <font color='red'>**Gradient Boosting**<\/font>","e3a3910d":"## Random Forest","b1bed367":"### 5. <font color='red'>**speed**<\/font> - trip_distance\/trip_duration","000e2e36":"### Linear Regression","64a6c21c":"### 1. <font color='red'>**y_True**<\/font> ","3efcf973":"# Taxi Demand Prediction (NYC Taxi) (Regression Problem)\n\n## Story\n\nIn this notebook we can see that how can we generate a column from a dataset according to our problem need and than create a machine learning models to train our dataset\n\n\n### About Dataset\n\nI took the dataset from New-York Governemnt site i.e (https:\/\/www1.nyc.gov\/site\/tlc\/about\/tlc-trip-record-data.page) and data is of 2015 and 2016 (jan, feb, mar). This dataset will be used for many purposes like predict Total fare of the trip and many more but we use it for the taxi demand prediction. The cool thing is their is no column in the csv for demand of that taxi in specefic area, we will try to do some experiments, to create it and make a machine learning model on that dataset. Hope you guys like the work \n\n\n### Things to learn\n\n1. Feature Engineering\n2. How to handle large Dataset (csv of 1.5GB)\n3. Machine Learning Techniques\n4. Regression\n\n## Cautions\n\nI take less data from csv as kaggle not allow me to use more than 16GB of ram. If you take all of the data than you will get 97% accuracy on validation data and 92% accuracy on test data","1ea55741":"\n\n### 6. <font color='red'>**K-Means with respect to longitude and latitude**<\/font>\n","b13bf0a8":"<h1 align=\"center\">Loading Data<\/h1> \n\nLoading Data into pandas DataFrame their are alot of column names into the csv but we will only take the specefic 1\n\n**Colums We will use:**\n1. tpep_pickup_datetime    : Pick up Datetime\n2. tpep_dropoff_datetime   : Drop Off Datetime\n3. trip_distance           : Distance of Trip\n4. pickup_longitude        : PickUp longitude\n5. pickup_latitude         : Pickup Latitude\n6. dropoff_longitude       : Dropoff Longitude\n7. dropoff_latitude        : Dropoff Latitude\n8. total_amount            : Total Fare amount","dcef16ed":"<h1 align=\"center\">Models Evaluation<\/h1> ","05bdfad8":"### 4. <font color='red'>**Split data into train and test, X and y**<\/font>","c9301d52":"### 2. <font color='red'>**Linear Regression**<\/font>"}}