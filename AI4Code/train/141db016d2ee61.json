{"cell_type":{"4a0276e8":"code","29aaf9a2":"code","7edf2fff":"code","29a690ad":"code","3dcf0dfe":"code","c3a9a1e7":"code","065db3fa":"code","0ff9324b":"code","3869776e":"code","db5a692a":"code","712e2d15":"code","d37197aa":"code","cb4e75ce":"code","b32bbf11":"code","837ddc3e":"code","77207229":"code","ec94ba9e":"code","2dedafe3":"code","fc58e1b3":"code","f4c1daf3":"code","60a51ed5":"code","9a7b60ea":"code","1c155627":"code","bf8386a5":"code","ae00985a":"code","f3f760d6":"code","90271a56":"code","6d269097":"code","7fb83583":"code","ce4111d9":"code","5fe5a1bd":"code","2c44f47a":"markdown","85c1d4bf":"markdown","9ea88315":"markdown","1b9823d4":"markdown","63dac651":"markdown","58d8d698":"markdown","b0915a0f":"markdown","a07108ef":"markdown","57cbbf10":"markdown","8a92ec2c":"markdown","855af709":"markdown","b6ae2610":"markdown","8cb127d8":"markdown","af0ea73a":"markdown","ccd05b9c":"markdown","1023fcf6":"markdown","e9a73144":"markdown","fa9d6821":"markdown","cd8461ba":"markdown"},"source":{"4a0276e8":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nnltk.download('punkt')","29aaf9a2":"df = pd.read_csv('..\/input\/tennis-articles\/tennis_articles.csv', encoding = \"ISO-8859-1\")","7edf2fff":"df.head(10)","29a690ad":"df.info()","3dcf0dfe":"#I will drop the article_title column.\n# Reason: Well I am trying to keep things simple and easy.","c3a9a1e7":"df.drop(['article_title'], axis = 1, inplace=True)","065db3fa":"df.head()","0ff9324b":"#lets look at the first article_text\ndf['article_text'][0]\n","3869776e":"df['article_text'][1]\n","db5a692a":"df['article_text'][2]","712e2d15":"from nltk.tokenize import sent_tokenize\nsentences = []\nfor s in df['article_text']:\n  sentences.append(sent_tokenize(s))\n\nsentences = [y for x in sentences for y in x]\n\n# Above I have used list comprehension technique instead of conventional for loop method.","d37197aa":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove*.zip","cb4e75ce":"word_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","b32bbf11":"len(word_embeddings)","837ddc3e":"clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")","77207229":"print(clean_sentences[0])\nprint(clean_sentences[1])\nprint(clean_sentences[2])","ec94ba9e":"clean_sentences = [s.lower() for s in clean_sentences]","2dedafe3":"nltk.download('stopwords')","fc58e1b3":"def remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new","f4c1daf3":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","60a51ed5":"clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]","9a7b60ea":"clean_sentences[0:5]","1c155627":"word_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","bf8386a5":"sentence_vectors = []\nfor i in clean_sentences:\n  if len(i) != 0:\n    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])\/(len(i.split())+0.001)\n  else:\n    v = np.zeros((100,))\n  sentence_vectors.append(v)","ae00985a":"from sklearn.metrics.pairwise import cosine_similarity","f3f760d6":"similarity_matrix = np.zeros([len(sentences), len(sentences)])\n# The above code will help me in forming the matrix of the size of sentences. ","90271a56":"for i in range(len(sentences)):\n  for j in range(len(sentences)):\n    if i != j:\n      similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]","6d269097":"print(similarity_matrix.shape)","7fb83583":"import networkx as nx\n\nnx_graph = nx.from_numpy_array(similarity_matrix)\nscores = nx.pagerank(nx_graph)","ce4111d9":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)","5fe5a1bd":"# Extract top 10 sentences as the summary\nfor i in range(10):\n  print(ranked_sentences[i][1])","2c44f47a":"In this case we are splitting the paragraph into sentences.","85c1d4bf":"# **3. Remove punctuations, special characters and numbers.**","9ea88315":"# **7. Converting similarity matrix sim_mat into a graph**","1b9823d4":"**converting to lower case**\n\n**Reason:**\n\nI think for your particular use-case, it would be better to convert it to lowercase because ultimately, you will need to predict the words given a certain context. You probably won't be needing to predict sentence beginnings in your use-case. Also, if a noun is predicted you can capitalize it later. However consider the other way round. (Assuming your corpus is in English) Your model might treat a word which is in the beginning of a sentence with a capital letter different from the same word which appears later in the sentence but without any capital latter. This might lead to decline in the accuracy. Whereas I think, lowering the words would be a better trade off.","63dac651":"# **4. Removing stops words**","58d8d698":"**What are the stop words?**\n\nThese are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Examples of a few stop words in English are \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cso\u201d, \u201cwhat\u201d.\n\n**Why we remove the stop words?**\n\nStop words are available in abundance in any human language. By removing these words, we remove the low-level information from our text in order to give more focus to the important information. In order words, we can say that the removal of such words does not show any negative consequences on the model we train for our task.\nRemoval of stop words definitely reduces the dataset size and thus reduces the training time due to the fewer number of tokens involved in the training.","b0915a0f":"# **8. Summarization**","a07108ef":"# **5. Vector representation of sentences**","57cbbf10":"I am going to use Glove for word embedding.\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. \nTraining is performed on aggregated global word-word co-occurrence statistics from a corpus, \nand the resulting representations showcase interesting linear substructures of the word vector space\n\nRead more here https:\/\/nlp.stanford.edu\/projects\/glove\/","8a92ec2c":"# **Preprocessing**","855af709":"# **2. WORD EMBEDDING (Then spliting the sentecnec into words.)**","b6ae2610":"I will use cosine similarity for finding the similarity between the sentecnes. Sentences which has highest similairyt will be of more importance and we will rank them according to that and later on we will form the summarization using that. \n\n[Read more on cosine similarity.](https:\/\/www.machinelearningplus.com\/nlp\/cosine-similarity\/#:~:text=Cosine%20similarity%20is%20a%20metric,in%20a%20multi%2Ddimensional%20space.&text=The%20smaller%20the%20angle%2C%20higher%20the%20cosine%20similarity.)","8cb127d8":"Doing this will help in processing the text faster.","af0ea73a":"The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings","ccd05b9c":"In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text.\nWord embeddings are a type of word representation that allows words with similar meaning to have a similar representation.","1023fcf6":"what is tokenization\n\nTokenization is a way of separating a piece of text into smaller units called tokens. \nHere, tokens can be either words, characters, or subwords. \nHence, tokenization can be broadly classified into 3 types\n1.word, 2.character, and 3.subword (n-gram characters) tokenization.","e9a73144":"# **6. Similarity matrix**","fa9d6821":"Sorting the sentences on the basis of highest score","cd8461ba":"# **1. TOKENIZATION (Spliting the whole paragraph into sentence)**"}}