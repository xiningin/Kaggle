{"cell_type":{"5e9f29a8":"code","22eaee38":"code","a40c5f0e":"code","750d6a88":"code","52b3abee":"code","29a51671":"code","7d3aacfd":"code","fc35971f":"code","1aa8a6d2":"code","0f5bcd72":"code","98b51367":"code","4b990365":"code","3e1f2c76":"code","3114153b":"code","9a39a13b":"code","19f4f06b":"code","ead81f20":"code","8e9c940f":"code","029dc4de":"code","829d831a":"code","cc16a9fc":"code","75e7fd0f":"code","0e14494a":"code","204e8fe5":"code","f23f3dc1":"code","d31a2752":"code","7162da03":"code","6b84ada9":"code","bfdc4d21":"code","0185498e":"code","b0722f48":"code","0a5b4cab":"code","b29238f8":"code","e668a7f1":"code","c5de6234":"code","bf11c59c":"code","6b0b8b2d":"code","5f9f1338":"code","f08bb614":"code","f18c8da4":"code","4d40bdc4":"code","8310647d":"code","656b0501":"code","e4616308":"code","3ce24f45":"code","29126489":"code","f728c9e1":"code","e6c307b7":"code","2d8fa86d":"code","86f3fff3":"code","33a21930":"code","21259e07":"code","95424981":"code","945360d5":"code","9a544a67":"code","4e08ad55":"code","6350587b":"code","73520549":"code","c163ee9b":"code","af9e6e35":"code","b543e2a1":"code","a6fb43f6":"code","cc6e335b":"code","ab523eba":"code","7fbf648e":"code","68c6c823":"code","85ad1dbc":"code","bcf52c52":"code","4b0188c4":"code","2c429504":"code","252b1c6f":"code","80b85427":"code","cb222aa1":"code","082e2d00":"code","d31f4e80":"code","529cc13a":"code","6dc74c79":"code","4cc14ec9":"code","214afa98":"code","76d97803":"code","5cbe3fde":"code","df1ddde1":"code","02ccf8b2":"code","1f5a0a75":"code","595bf5a9":"code","1b0e57e8":"code","9fdaf0df":"code","027f2e76":"code","f8e70a83":"code","7a1483ea":"code","c188a67a":"code","14075625":"code","ec659135":"code","706c7ec7":"code","10432aea":"code","2cd0ab51":"code","6c79c1dc":"code","e76200ed":"code","1ee3a855":"code","0b059640":"code","f3dd4359":"code","45624d97":"code","a21d17f0":"code","a1bd3542":"code","87128802":"code","b5cf58f1":"code","c457a53d":"code","867b7eff":"code","7bc45ef2":"code","15e938e1":"code","55f6160c":"code","2564eec4":"code","b0d03d20":"code","5777122c":"markdown","444e3092":"markdown","7a3bffec":"markdown","534b4276":"markdown","0694d416":"markdown","376efb83":"markdown","5b6c76eb":"markdown","ad8ab464":"markdown","e6ac426b":"markdown","052f7281":"markdown","2ceb7a7b":"markdown","2b81a1f2":"markdown","f10ab9d3":"markdown","70f09aeb":"markdown","877ba4e4":"markdown","63dd16d3":"markdown","d46b872d":"markdown","10806577":"markdown","2fb89069":"markdown","b6ef7cc4":"markdown","ab9f68b3":"markdown","4dbe8d3b":"markdown","40f66daa":"markdown","c6193284":"markdown","2050a376":"markdown","44cd284b":"markdown","2dff7314":"markdown","e77f6cc4":"markdown","99287abf":"markdown","b9e6bf3b":"markdown","165c4425":"markdown","cc8a80ef":"markdown","54a38d7c":"markdown","aaf43ea6":"markdown","7eea68a7":"markdown","236b6080":"markdown","bae193a3":"markdown","abd41612":"markdown","f21ecd98":"markdown","4393b6e6":"markdown","776f3977":"markdown","e0c6aa74":"markdown","e2c921c4":"markdown","55da1ac9":"markdown","bd180ddb":"markdown","9e36b75f":"markdown","c13a0176":"markdown","9b98c568":"markdown","007e8be1":"markdown","8f40015e":"markdown","fe40c32f":"markdown","67ac20e5":"markdown","945abded":"markdown","f48269a4":"markdown","ae26340c":"markdown","a6bdc4bc":"markdown","b31475a3":"markdown","74363167":"markdown","e0b9dd42":"markdown","26eff504":"markdown","f6c82191":"markdown","d8075c9b":"markdown","71c18b1a":"markdown","a0777c90":"markdown","72aab131":"markdown","6c01f98f":"markdown","20d68dcb":"markdown","7e7129cd":"markdown","7e152db8":"markdown","ed36f291":"markdown","19935685":"markdown","8df794be":"markdown","75c6d1b0":"markdown","ea7a8614":"markdown","1458f73f":"markdown","5fb0c86f":"markdown","e96c95b0":"markdown","064fb78b":"markdown","b853c6cd":"markdown","d15b8541":"markdown","ce3ae6dd":"markdown","5a1b1bda":"markdown","74a0e22f":"markdown","c171959e":"markdown","64f53c2e":"markdown","8f047e16":"markdown","fb138ca1":"markdown","6a8bc80a":"markdown","d5330159":"markdown","54e49809":"markdown","2253700a":"markdown","5720a664":"markdown","fc6aee2a":"markdown","0f632f75":"markdown","bef6b3b1":"markdown","b0a4fbc0":"markdown","76deae5a":"markdown","b8786aea":"markdown","a3903324":"markdown","f2a499d9":"markdown","bb2acdd6":"markdown","0b466c7b":"markdown","fe9f9eec":"markdown","2f2ddf86":"markdown","fbf19f0e":"markdown","4e490734":"markdown","8edee66b":"markdown","c4bcdcd3":"markdown","bcfa8606":"markdown","8e5d982d":"markdown","caddbb3f":"markdown","5903945a":"markdown","443f51b6":"markdown","6cc62732":"markdown","d6bc9b86":"markdown","8eb5ac5a":"markdown","734cb950":"markdown","2cfb3ad5":"markdown","2d6c5a22":"markdown","96edbeb0":"markdown","e0d51df7":"markdown","79873fbf":"markdown","f53f9ad0":"markdown","d7fc4707":"markdown","b4aae657":"markdown","4c0e321e":"markdown","ab2a4a09":"markdown","21448cfd":"markdown","84bc9775":"markdown","74c51bb5":"markdown","db9dfbfa":"markdown","1343a006":"markdown","45ec73f5":"markdown","4c4c0d55":"markdown","438d9d23":"markdown","1ce097ab":"markdown","18593629":"markdown","d3e2e247":"markdown","79cf1146":"markdown","1541cc12":"markdown","caba1cb1":"markdown","33ce1ee9":"markdown","50118341":"markdown","71b04961":"markdown","8553e281":"markdown","52deae3d":"markdown","c06412bc":"markdown","1c782552":"markdown","3c37692e":"markdown","d5deb56c":"markdown","dd9ef762":"markdown","c9bdbcfb":"markdown","71b5eba7":"markdown","eeacb761":"markdown","7dfc9356":"markdown","f5a75b23":"markdown","324f0b57":"markdown","f92255e3":"markdown","63f9bd41":"markdown","59e67570":"markdown","e0cd197e":"markdown","663b0034":"markdown","90c68cf4":"markdown","9b83157a":"markdown","b2402e91":"markdown","c0c6d42d":"markdown","736a52d2":"markdown","4487b853":"markdown","a1490872":"markdown","e7b016df":"markdown","9f9967ef":"markdown","2341b71e":"markdown","732fdb5f":"markdown","83eed355":"markdown","ea5b166c":"markdown","bf3b6dd9":"markdown","44c5c30b":"markdown","ab038a72":"markdown","81913779":"markdown","141689e8":"markdown"},"source":{"5e9f29a8":"from io import BytesIO\nimport requests\nfrom PIL import Image\nresponse = requests.get(\"https:\/\/external-content.duckduckgo.com\/iu\/?u=http%3A%2F%2F1.bp.blogspot.com%2F-qKqxEyuXEQo%2FVD71mHi8sDI%2FAAAAAAAAPhg%2FVwDyvAlXDnY%2Fs1600%2Frgicontransparent.png&f=1&nofb=1\")\nimg = Image.open(BytesIO(response.content))\nimg.resize((300,300), Image.ANTIALIAS)","22eaee38":"# Ignore all your warnings\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Loading Libraries\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import f1_score\nimport pylab \nimport scipy.stats as stats\nfrom scipy.stats import boxcox\n\nimport re\nimport pickle\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport os\nfrom wordcloud import WordCloud\nfrom matplotlib_venn import venn2\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom textblob import TextBlob","a40c5f0e":"response = requests.get(\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/google-research\/human_computable_dimensions_1.png\")\nImage.open(BytesIO(response.content))","750d6a88":"# This image is created by me for better understanding of problem\nresponse = requests.get(\"https:\/\/i.postimg.cc\/NFNFYPPG\/regression.png\")\nImage.open(BytesIO(response.content))","52b3abee":"import os \nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","29a51671":"train_df = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")","7d3aacfd":"# train_df data\nprint(f\"shape of train_df: {train_df.shape} \\n{'='*50}\")\ntrain_df.head(2)","fc35971f":"# test_df data\nprint(f\"shape of test_df: {test_df.shape} \\n{'='*50}\")\ntest_df.head(2)","1aa8a6d2":"target_vars = sample_submission.columns[1:]\nfor idx,target in enumerate(target_vars):\n    print(idx+1,\":\",target)","0f5bcd72":"# sample of Target variable\ntrain_df[target_vars].head()","98b51367":"x_columns = [columns for columns in train_df.columns if columns not in sample_submission.columns[1:]]\nfor idx,x_var in enumerate(x_columns):\n    print(idx+1,\":\",x_var)","4b990365":"# sample of dependent variables\ntrain_df[x_columns].head()","3e1f2c76":"# train_df data\nprint(f\"shape of train_df: {train_df.shape} \\n{'='*50}\")","3114153b":"# This includes both independent variables and target variables\ntrain_df.info()","9a39a13b":"# sample\nsample=train_df.iloc[0]\nsample_question = sample[['qa_id', 'question_title', 'question_body']]\nsample_answer = sample[['answer']]\nsample_question_target_labels = sample[['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']]\nsample_answer_target_labels = sample[['answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']]","19f4f06b":"for i in sample_question.index:\n    print(i,\":\",sample_question[i],\"\\n\")","ead81f20":"print(sample_answer[0])","8e9c940f":"print(sample_question_target_labels)","029dc4de":"print(sample_answer_target_labels)","829d831a":"target_vars = sample_submission.columns[1:]\nfor idx,target in enumerate(target_vars):\n    print(idx+1,\":\",target)","cc16a9fc":"plt.figure(figsize=(28,20))\nfor idx,target in enumerate(target_vars): \n    sns.distplot(train_df[target],ax=plt.subplot(5,6,idx+1))\n    plt.grid()\nplt.show()","75e7fd0f":"corr = train_df[target_vars].corr()\n\n# plot the heatmap\nplt.figure(figsize=(16,14))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,      \n        vmin=-1, vmax=1, center=0,)\nplt.show()","0e14494a":"## Distribution plots for highly positive Correlated labels\nhigh_pos_corr_feat = [['answer_type_procedure','question_fact_seeking'],\n[\"question_type_instructions\",\"answer_type_instructions\"],\n['question_type_reason_explanation','answer_type_reason_explanation'],\n['question_interestingness_self','question_interestingness_others'],\n['answer_level_of_information','answer_helpful'],\n['answer_plausible','answer_helpful'],\n['answer_satisfaction','answer_helpful']]\n\nprint(\"****Distribtion Plots for postitive Correlation Target labels****\\n\\n\")\nfor idx, feat in enumerate(high_pos_corr_feat):\n    print(f\"plot: {idx+1} \")\n    plt.figure(figsize=(8,4))\n    sns.distplot(train_df[f'{feat[0]}'], label=f\"{feat[0]}\")\n    sns.distplot(train_df[f'{feat[1]}'], label=f\"{feat[1]}\")\n    plt.legend()\n    plt.xlabel(None)\n    plt.title(f\"Distribuion of  {feat[0]}  V\/S  {feat[1]}\\n\")\n    plt.grid()\n    plt.show()\n\n\n## Distribution plots for highly negative Correlated labels\nhigh_neg_corr_feat = [['question_fact_seeking','question_opinion_seeking']]\n\nprint(\"\\n\\n****Distribtion Plots for high Negative Correlation Target labels****\\n\")\nfor idx, feat in enumerate(high_neg_corr_feat):\n    print(f\"plot: {idx+1} \")\n    plt.figure(figsize=(8,4))\n    sns.distplot(train_df[f'{feat[0]}'], label=f\"{feat[0]}\")\n    sns.distplot(train_df[f'{feat[1]}'], label=f\"{feat[1]}\")\n    plt.legend()\n    plt.xlabel(None)\n    plt.title(f\"Distribuion of  {feat[0]}  V\/S  {feat[1]}\\n\")\n    plt.grid()\n    plt.show()","204e8fe5":"# Utility function to plot lineplot and distplot using seaborn\ndef plot_sns(data,feature,color='lightblue',title=None,subtitle=None):\n    \n    \"\"\"   \n    Utility function to plot lineplot and distplot using seaborn\n    \n    plot_sns(data,feature,color='lightblue',title=None,subtitle=None):\n    \n    data = data \n    feature = coulum name\n    color = color of plot\n    title = Either 'length' or 'number' based on which to plot. Otherwise by default='None'\n    subtitle = Either 'train_df' or 'test_df'. Otherwise by default='None'  \n    \n    \"\"\"    \n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n    \n    # line plot\n    sns.lineplot(np.arange(len(data)),data,ax=ax1,color=color)    \n    if title=='number':\n        ax1.set(xlabel=f\"Idx of {feature}\", ylabel=f\"Number of words in {feature}\", title=f'Number of words in {feature} in {subtitle}\\n')\n    elif title=='length':\n        ax1.set(xlabel=f\"Idx of {feature}\", ylabel=f\"Length of {feature}\", title=f'Length of {feature} in {subtitle}\\n')   \n    ax1.grid()\n\n    # distribution plot\n    sns.distplot(data,ax=ax2,color=color)\n    if title=='number':\n        ax2.set(xlabel=f\"Idx of {feature}\", ylabel=f\"Number of words in {feature}\", title=f'Number of words in {feature} in {subtitle}\\n')\n    elif title=='length':\n        ax2.set(xlabel=f\"Idx of {feature}\", ylabel=f\"Length of {feature}\", title=f'Length of {feature} in {subtitle}\\n')   \n    ax2.grid()\n    plt.show()\n\n#=======================================================================================================================================================================================    \n# Utility function to plot bar graph for both train and test using seaborn\ndef plot_bar(train_data,test_data,feature=None,x_label=None, y_label=None):\n    \n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n\n    # for train_df\n    sns.barplot(train_data,np.arange(len(train_data)),ax=ax1)\n    ax1.set(xlabel=f\"{x_label}\", ylabel=f\"{y_label} {feature}\", title='train_df\\n')\n    ax1.grid()\n    \n    # for test_df\n    sns.barplot(test_data,np.arange(len(test_data)),ax=ax2)\n    ax2.set(xlabel=f\"{x_label}\", ylabel=f\"{y_label} {feature}\", title='test_df\\n')\n    ax2.grid()\n    plt.show()\n    \n#=======================================================================================================================================================================================  \n# Utility function to plot requency of most popular words\ndef word_frequency_plot(dataframe, title=None):\n    list_of_all_words = []\n    for sent in dataframe:\n        list_of_all_words.extend(sent.split())\n\n    top_50_words = pd.Series(list_of_all_words).value_counts()[:50]\n    top_50_words_prob_dist = top_50_words.values\/sum(top_50_words.values)\n\n    #  plot of frequency of polpular words in train\n    plt.figure(figsize=(16,7))\n    sns.barplot(top_50_words.index, top_50_words_prob_dist)\n    plt.xlabel(\"words\")\n    plt.ylabel(\"frequency\")\n    plt.title(f\"Frequency of most popular words {title}\\n\")\n    plt.xticks(rotation=70)\n    plt.grid()\n    plt.show()\n\n#=======================================================================================================================================================================================\n# Utility function to check if feature or variable follows Normal distribution using Q-Q Plot   \ndef q_q_plot(train_data, test_data, feature_name=None):\n    \"\"\"\n    # code refer: https:\/\/stackoverflow.com\/a\/13865874\n    \"\"\"\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n    \n    measurements = train_data\n    stats.probplot(measurements, dist=\"norm\", plot=ax1)\n    ax1.set(title=f'train : Q-Q Plot for {feature_name} \\n')\n\n    measurements = test_data\n    stats.probplot(measurements, dist=\"norm\", plot=ax2)\n    ax2.set(title=f'test : Q-Q Plot for {feature_name} \\n')\n    plt.show()\n        \n#=======================================================================================================================================================================================    \n# Utility function for box plot\ndef box_plot(train_data, test_data, feature_name=None):\n    \n    # for train data\n    plt.figure(figsize=(26,4))\n    sns.violinplot(train_data,color='darkred')\n    plt.title(f'Train : violinplot Plot for {feature_name} \\n')\n    plt.xlabel(f\"{feature_name}\")\n    plt.ylabel(f\"Distribution\")   \n    plt.grid()\n    plt.show()\n    \n    # for test data\n    plt.figure(figsize=(26,4))\n    sns.violinplot(test_data,color='orangered')\n    plt.title(f'Test : violinplot Plot for {feature_name} \\n')\n    plt.xlabel(f\"{feature_name}\")\n    plt.ylabel(f\"Distribution\")   \n    plt.grid()\n    plt.show()","f23f3dc1":"#Counts of repeated question_title in train\npd.DataFrame(train_df['question_title'].value_counts())","d31a2752":"#Counts of repeated question_title in test\npd.DataFrame(test_df['question_title'].value_counts())","7162da03":"# Number of repeated question_title in train\nn_repeated_question_title_train = train_df['question_title'].value_counts().values\n\n# Number of repeated question_title in test\nn_repeated_question_title_test = test_df['question_title'].value_counts().values\n\n# plot for Number of repeated question_title in train and test\nplot_bar(n_repeated_question_title_train, n_repeated_question_title_test,feature='question_title',x_label=\"Number of times repeated same question \",y_label=\"Counts of repeated\")","6b84ada9":"# Length of question_title in train\nlen_question_title_train = sorted(train_df['question_title'].apply(lambda x: len(x)),reverse=True)\n\n# Length of question_title in test\nlen_question_title_test = sorted(test_df['question_title'].apply(lambda x: len(x)),reverse=True)\n\n# plot for train_df\nplot_sns(len_question_title_train,\"question_title\",color='darkblue',title='length',subtitle='train_df')\n\n# plot for test_df\nplot_sns(len_question_title_test,\"question_title\",color='lightblue',title='length',subtitle='test_df')","bfdc4d21":"# Box plot of Length of question_title in train and test\nbox_plot(len_question_title_train, len_question_title_test, \"question_title\")","0185498e":"# Checking weather len_question_title follows normal distribution using Q-Q plot\nq_q_plot(len_question_title_train, len_question_title_test, \"len_question_title\")","b0722f48":"# number of words in question_title in train\nn_words_in_question_title_train = sorted(train_df['question_title'].apply(lambda x: len(x.split(\" \"))),reverse=True)\n\n# number of words in question_title in test\nn_words_in_question_title_test = sorted(test_df['question_title'].apply(lambda x: len(x.split(\" \"))),reverse=True)","0a5b4cab":"# plot for train_df\nplot_sns(n_words_in_question_title_train,\"question_title\",color='darkred',title='number',subtitle='train_df')\n\n# plot for test_df\nplot_sns(n_words_in_question_title_train,\"question_title\",color='orangered',title='number',subtitle='test_df')","b29238f8":"# Box plot of Length of question_title in train and test\nbox_plot(n_words_in_question_title_train, n_words_in_question_title_test, \"question_title\")","e668a7f1":"# Checking weather  n_words_in_question_title follows normal distribution using Q-Q plot\nq_q_plot(n_words_in_question_title_train, n_words_in_question_title_test, \"n_words_in_question_title\")","c5de6234":"# refer: https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python\n\n# For train_df\ntext_train = \" \".join(word for word in train_df['question_title'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_train)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of question_title in train\\n\")\nplt.axis(\"off\")\nplt.show()\n\n#===================================================================\n\n# For test_df\ntext_test = \" \".join(word for word in test_df['question_title'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_test)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of question_title in test\")\nplt.axis(\"off\")\nplt.show()","bf11c59c":"# Frequency of most popular 50 words in train_df\nword_frequency_plot(train_df['question_title'], title='train')\n\n# Frequency of most popular words in test_df\nword_frequency_plot(test_df['question_title'], title='test')","6b0b8b2d":"plt.figure(figsize=(9,5))\nvenn2([set(train_df['question_title'].unique()), set(test_df['question_title'].unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common question_title in training and test data\", fontsize=15)\nplt.show()","5f9f1338":"#Counts of repeated questions in train\ntrain_df['question_body'].value_counts().values","f08bb614":"#Counts of repeated questions in test\ntest_df['question_body'].value_counts().values","f18c8da4":"# Number of repeated question_body in train\nn_repeated_question_body_train = train_df['question_body'].value_counts().values\n\n# Number of repeated question_body in test\nn_repeated_question_body_test = test_df['question_body'].value_counts().values\n\n# plot for Number of repeated question_body in train and test\nplot_bar(n_repeated_question_body_train, n_repeated_question_body_test,feature='question_title',x_label=\"Number of times repeated same question \",y_label=\"Counts of repeated\")\n","4d40bdc4":"# Length of question_title in train\nlen_question_body_train = sorted(train_df['question_body'].apply(lambda x: len(x)),reverse=True)\n\n# Length of question_title in test\nlen_question_body_test = sorted(test_df['question_body'].apply(lambda x: len(x)),reverse=True)\n\n# plot for train_df\nplot_sns(len_question_body_train,\"question_body\",color='darkblue',title='length',subtitle='train_df')\n\n# plot for test_df\nplot_sns(len_question_body_test,\"question_body\",color='lightblue',title='length',subtitle='test_df')\n","8310647d":"# Box plot of length of question_body\nbox_plot(len_question_body_train, len_question_body_test, \"len_question_body\" )","656b0501":"box_cox_len_question_body_train = boxcox(len_question_body_train)[0]\nbox_cox_len_question_body_test =  boxcox(len_question_body_test)[0]\n\ntrain_df['box_cox_len_question_body'] = box_cox_len_question_body_train\ntest_df['box_cox_len_question_body'] = box_cox_len_question_body_test\n\n# Checking weather box cox transformed len_question_body_box_cox follows normal distribution or not using Q-Q plot\nq_q_plot(box_cox_len_question_body_train, box_cox_len_question_body_test, \"box-cox transformed length of question_body \")","e4616308":"# number of words in question_title in train\nn_words_in_question_body_train = sorted(train_df['question_body'].apply(lambda x: len(x.split(\" \"))),reverse=True)\n\n# number of words in question_title in test\nn_words_in_question_body_test = sorted(test_df['question_body'].apply(lambda x: len(x.split(\" \"))),reverse=True)\n\n# plot for train_df\nplot_sns(n_words_in_question_body_train,\"question_body\",color='darkred',title='number',subtitle='train_df')\n\n# plot for test_df\nplot_sns(n_words_in_question_body_test,\"question_body\",color='orangered',title='number',subtitle='test_df')","3ce24f45":"# Box plot of length of question_body\nbox_plot(n_words_in_question_body_train, n_words_in_question_body_test, \"n_words_in_question_body\" )","29126489":"box_cox_n_words_in_question_body_train = boxcox(n_words_in_question_body_train)[0]\nbox_cox_n_words_in_question_body_test =  boxcox(n_words_in_question_body_test)[0]\n\n# Saving box_cox_n_words_in_question_body as feature\ntrain_df['box_cox_n_words_in_question_body'] = box_cox_n_words_in_question_body_train \ntest_df['box_cox_n_words_in_question_body'] = box_cox_n_words_in_question_body_test\n\n# Checking weather box cox transformed len_question_body_box_cox follows normal distribution or not using Q-Q plot\nq_q_plot(box_cox_n_words_in_question_body_train, box_cox_n_words_in_question_body_test, \"box-cox transformed number of words in question_body \")","f728c9e1":"# refer: https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python\n\n# For train_df\ntext_train = \" \".join(word for word in train_df['question_body'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_train)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of question_title in train\\n\")\nplt.axis(\"off\")\nplt.show()\n\n#===================================================================\n\n# For test_df\ntext_test = \" \".join(word for word in test_df['question_body'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_test)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of question_title in test\")\nplt.axis(\"off\")\nplt.show()","e6c307b7":"# Frequency of most popular worlds in question_body of train\nword_frequency_plot(train_df['question_body'], title='train')\n\n# Frequency of most popular worlds in question_body of test_df\nword_frequency_plot(test_df['question_body'], title='test')","2d8fa86d":"# for train_df\nn_count_question_user_name_train=train_df['question_user_name'].value_counts().values\n\n# for test_df\nn_count_question_user_name_test=test_df['question_user_name'].value_counts().values\n\n# plot for Distribution of counts question_user_name in train and test\nplot_bar(n_count_question_user_name_train, n_count_question_user_name_test,feature='question_user_name',x_label=\"Number of questions  \",y_label=\"Counts of \")","86f3fff3":"# for train_df\nn_user_unique_question_train = train_df.drop_duplicates(subset=['question_title'])['question_user_name'].value_counts()\n\n# for test_df\nn_user_unique_question_test = test_df.drop_duplicates(subset=['question_title'])['question_user_name'].value_counts()\n\n# plot for Which user has most number of unique question based on question_title?\nplot_bar(n_user_unique_question_train.values, n_user_unique_question_test.values,feature='question_user_name',x_label=\"Number of questions  \",y_label=\"Counts of \")","33a21930":"# Top 10 user who has asked most number of unique question\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n\nsns.barplot(n_user_unique_question_train[:10].values,n_user_unique_question_train[:10].index,ax=ax1)\nax1.set(xlabel = \"number of unique question\", ylabel=f\"top 10  question_user_name\", title='train_df\\n')\nax1.grid()\n\n\nsns.barplot(n_user_unique_question_test[:10].values,n_user_unique_question_test[:10].index , ax=ax2)\nax2.set(xlabel = \"number of unique question\", ylabel=f\"top 10  question_user_name\", title='test_df\\n')\nax2.grid()\nplt.show()","21259e07":"n_user_unique_question_train.describe()","95424981":"n_user_unique_question_test.describe()","945360d5":"print(f\"Unique number of question_user_name in Train : {len(train_df['question_user_name'].unique())}\")\nprint(f\"Unique number of question_user_name in Test : {len(test_df['question_user_name'].unique())}\")","9a544a67":"# Venn plot of  Common unique question_user_name in train and test\n# refer: https:\/\/www.kaggle.com\/codename007\/start-from-here-quest-complete-eda-fe\n\nplt.figure(figsize=(9,5))\n\nvenn2([set(train_df.question_user_name.unique()), set(test_df.question_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common question_user_name in training and test data\", fontsize=15)\nplt.show()","4e08ad55":"# Finding the unique question users asked unique questions based on question_title\nunique_question_user_with_unique_questions = pd.DataFrame(train_df.groupby(['question_user_name',\"question_title\"])[\"question_title\"].unique())\nunique_question_user_with_unique_questions.reset_index(level=1,drop=True,inplace=True)\nunique_question_user_with_unique_questions.reset_index(inplace=True)\n\n# Find the number of words in each question_title asked by unique question_user\nnumber_of_words = unique_question_user_with_unique_questions['question_title'].apply(lambda x : len(x[0].split()))\nunique_question_user_with_unique_questions[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of question_title asked by each unique question_user\nunique_question_user_with_unique_questions_title_agg_train = unique_question_user_with_unique_questions.groupby('question_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nunique_question_user_with_unique_questions_title_agg_train = unique_question_user_with_unique_questions_title_agg_train.reset_index(level=0)\n\n# Renaming column names\nunique_question_user_with_unique_questions_title_agg_train.rename({'sum': 'sum_title_len', 'min': 'min_title_len', 'max': 'max_title_len', 'mean': 'mean_title_len', 'median': \"median_title_len\"}, axis=1, inplace=True)\n\n# merging to train dataframe\ntrain_df = pd.merge(left=train_df ,right=unique_question_user_with_unique_questions_title_agg_train ,how='inner',on=\"question_user_name\")","6350587b":"# Finding the unique question users asked unique questions based on question_title\nunique_question_user_with_unique_questions = pd.DataFrame(test_df.groupby(['question_user_name',\"question_title\"])[\"question_title\"].unique())\nunique_question_user_with_unique_questions.reset_index(level=1,drop=True,inplace=True)\nunique_question_user_with_unique_questions.reset_index(inplace=True)\n\n# Find the number of words in each question_title asked by unique question_user\nnumber_of_words = unique_question_user_with_unique_questions['question_title'].apply(lambda x : len(x[0].split()))\nunique_question_user_with_unique_questions[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of question_title asked by each unique question_user\nunique_question_user_with_unique_questions_title_agg_test = unique_question_user_with_unique_questions.groupby('question_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nunique_question_user_with_unique_questions_title_agg_test = unique_question_user_with_unique_questions_title_agg_test.reset_index(level=0)\n\n# Renaming column names\nunique_question_user_with_unique_questions_title_agg_test.rename({'sum': 'sum_title_len', 'min': 'min_title_len', 'max': 'max_title_len', 'mean': 'mean_title_len', 'median': \"median_title_len\"}, axis=1, inplace=True)\n\n# merging to test dataframe\ntest_df = pd.merge(left=test_df ,right=unique_question_user_with_unique_questions_title_agg_test ,how='inner',on=\"question_user_name\")","73520549":"# Ploting user behaviour\nuser_behaviour_column_on_queston_titile = ['sum_title_len', 'min_title_len', 'max_title_len', 'mean_title_len','median_title_len']\n\nfor idx,column in enumerate(user_behaviour_column_on_queston_titile):\n    \n    train_set = unique_question_user_with_unique_questions_title_agg_train.sort_values(by= column ,ascending=False)[column]\n    test_set = unique_question_user_with_unique_questions_title_agg_test.sort_values(by= column ,ascending=False)[column]\n    \n    print(f\"\\n{idx+1}: Plot for {column}\")\n    \n    # Ploting lineplot for trainset\n    f, (ax1, ax2 , ax3 , ax4 ) = plt.subplots(1, 4, figsize=(24,5))\n    \n    sns.lineplot(np.arange(len(train_set)), train_set,ax=ax1, color = \"darkred\")\n    ax1.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax1.grid()\n\n    # Ploting lineplot for testset\n    sns.lineplot(np.arange(len(test_set)), test_set,ax=ax2 ,color = \"orangered\")\n    ax2.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax2.grid()\n    \n    # Ploting distplot for trainset\n    sns.distplot(train_set,ax=ax3, color = \"darkred\")\n    ax3.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax3.grid()\n\n    # Ploting distplot for testset\n    sns.distplot( test_set, ax=ax4 ,color = \"orangered\")\n    ax4.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax4.grid()\n    plt.show()","c163ee9b":"# Q-Q Plot of Box Cox transformed user_behaviour_column_on_queston_titile features\nfor idx,column_name in enumerate(user_behaviour_column_on_queston_titile):\n    \n    # Box Cox Transform\n    boxcox_transformed_feature_train = boxcox(train_df[column_name])[0]\n    boxcox_transformed_feature_test = boxcox(test_df[column_name])[0]\n    \n    # Saving the transformed user_behaviour_column_on_queston_titile column in dataframe\n    train_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_train\n    test_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_test\n    \n    print(f\"\\n{idx+1}: Q-Q Plot for box cox transformed feature of {column_name}\\n\")\n    \n    # Q-Q Plot of transformed Feature\n    q_q_plot(boxcox_transformed_feature_train, boxcox_transformed_feature_test, f\"box cox transformed {column_name}\")\n    ","af9e6e35":"# Finding the unique question users asked unique questions based on question_title\nunique_question_user_with_unique_questions = pd.DataFrame(train_df.groupby(['question_user_name',\"question_title\"])[\"question_body\"].unique())\nunique_question_user_with_unique_questions.reset_index(level=1,drop=True,inplace=True)\nunique_question_user_with_unique_questions.reset_index(inplace=True)\n\n# Find the number of words in each question_title asked by unique question_user\nnumber_of_words = unique_question_user_with_unique_questions['question_body'].apply(lambda x : len(x[0].split()))\nunique_question_user_with_unique_questions[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of question_title asked by each unique question_user\nunique_question_user_with_unique_questions_body_agg_train = unique_question_user_with_unique_questions.groupby('question_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nunique_question_user_with_unique_questions_body_agg_train = unique_question_user_with_unique_questions_body_agg_train.reset_index(level=0)\n\n# Renaming column names\nunique_question_user_with_unique_questions_body_agg_train.rename({'sum': 'sum_body_len', 'min': 'min_body_len', 'max': 'max_body_len', 'mean': 'mean_body_len', 'median': \"median_body_len\"}, axis=1, inplace=True)\n\n# merging to train dataframe\ntrain_df = pd.merge(left=train_df ,right=unique_question_user_with_unique_questions_body_agg_train ,how='inner',on=\"question_user_name\")","b543e2a1":"# Finding the unique question users asked unique questions based on question_title\nunique_question_user_with_unique_questions = pd.DataFrame(test_df.groupby(['question_user_name',\"question_title\"])[\"question_body\"].unique())\nunique_question_user_with_unique_questions.reset_index(level=1,drop=True,inplace=True)\nunique_question_user_with_unique_questions.reset_index(inplace=True)\n\n# Find the number of words in each question_title asked by unique question_user\nnumber_of_words = unique_question_user_with_unique_questions['question_body'].apply(lambda x : len(x[0].split()))\nunique_question_user_with_unique_questions[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of question_title asked by each unique question_user\nunique_question_user_with_unique_questions_body_agg_test = unique_question_user_with_unique_questions.groupby('question_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nunique_question_user_with_unique_questions_body_agg_test = unique_question_user_with_unique_questions_body_agg_test.reset_index(level=0)\n\n# Renaming column names\nunique_question_user_with_unique_questions_body_agg_test.rename({'sum': 'sum_body_len', 'min': 'min_body_len', 'max': 'max_body_len', 'mean': 'mean_body_len', 'median': \"median_body_len\"}, axis=1, inplace=True)\n\n# merging to test dataframe\ntest_df = pd.merge(left=test_df ,right=unique_question_user_with_unique_questions_body_agg_test ,how='inner',on=\"question_user_name\")","a6fb43f6":"unique_question_user_with_unique_questions_body_agg_test","cc6e335b":"# Ploting user behaviour\nuser_behaviour_column_on_queston_body = ['sum_body_len', 'min_body_len', 'max_body_len', 'mean_body_len','median_body_len']\n\nfor idx,column in enumerate(user_behaviour_column_on_queston_body):\n    \n    train_set = unique_question_user_with_unique_questions_body_agg_train.sort_values(by= column ,ascending=False)[column]\n    test_set = unique_question_user_with_unique_questions_body_agg_test.sort_values(by= column ,ascending=False)[column]\n    \n    print(f\"\\n{idx+1}: Plot for {column}\")\n    \n    # Ploting lineplot for trainset\n    f, (ax1, ax2 , ax3 , ax4 ) = plt.subplots(1, 4, figsize=(24,5))\n    \n    sns.lineplot(np.arange(len(train_set)), train_set,ax=ax1, color = \"darkred\")\n    ax1.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax1.grid()\n\n    # Ploting lineplot for testset\n    sns.lineplot(np.arange(len(test_set)), test_set,ax=ax2 ,color = \"orangered\")\n    ax2.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax2.grid()\n    \n    # Ploting distplot for trainset\n    sns.distplot(train_set,ax=ax3, color = \"darkred\")\n    ax3.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax3.grid()\n\n    # Ploting distplot for testset\n    sns.distplot( test_set, ax=ax4 ,color = \"orangered\")\n    ax4.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax4.grid()\n    plt.show()","ab523eba":"# Q-Q Plot of Box Cox transformed user_behaviour_column_on_queston_body features\nfor idx,column_name in enumerate(user_behaviour_column_on_queston_body):\n    \n    print(f\"Feature name: {column_name}\")\n    \n    # Continues the loop if box cox fails to transform\n    if sum(train_df[f'{column_name}']<1)>0:\n        print(f\"{idx+1}: Box Cox Transformation can not be applied on feature '{column_name}' because all the values of data must be positive for transformation\\n\\n\")\n        continue\n\n    # Box Cox Transform\n    boxcox_transformed_feature_train = boxcox(train_df[column_name])[0]\n    boxcox_transformed_feature_test = boxcox(test_df[column_name])[0]\n    \n             \n    # Saving the transformed user_behaviour_column_on_queston_body column in dataframe\n    train_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_train\n    test_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_test\n    \n    print(f\"{idx+1}: Q-Q Plot for box cox transformed feature of {column_name}\\n\")\n    \n    # Q-Q Plot of transformed Feature\n    q_q_plot(boxcox_transformed_feature_train, boxcox_transformed_feature_test, f\"box cox transformed {column_name}\")\n    ","7fbf648e":"n_repeated_answer_train = sum(train_df['answer'].value_counts().values>1)\nn_repeated_answer_test = sum(test_df['answer'].value_counts().values>1)\n\nprint(f\"Number of repeated answer in train: {n_repeated_answer_train}\")\nprint(f\"Number of repeated answer in test: {n_repeated_answer_test}\")","68c6c823":"# Length of answer in train\nlen_answer_train = sorted(train_df['answer'].apply(lambda x: len(x)),reverse=True)\n\n# Length of answer in test\nlen_answer_test = sorted(test_df['answer'].apply(lambda x: len(x)),reverse=True)\n\n# plot for train_df\nplot_sns(len_answer_train,\"answer\",color='darkblue',title='length',subtitle='train_df')\n\n# plot for test_df\nplot_sns(len_answer_test,\"answer\",color='lightblue',title='length',subtitle='test_df')\n","85ad1dbc":"# Box plot of length of question_body\nbox_plot(len_answer_train, len_answer_test, \"length of answer \" )","bcf52c52":"box_cox_len_answer_train = boxcox(len_answer_train)[0]\nbox_cox_len_answer_test =  boxcox(len_answer_test)[0]\n\ntrain_df['box_cox_len_answer'] = box_cox_len_answer_train\ntest_df['box_cox_len_answer'] = box_cox_len_answer_test\n\n# Checking weather box cox transformed len_question_body_box_cox follows normal distribution or not using Q-Q plot\nq_q_plot(box_cox_len_question_body_train, box_cox_len_question_body_test, \"box-cox transformed length of answer  \")","4b0188c4":"# number of words in answer in train\nn_words_in_answer_train = sorted(train_df['answer'].apply(lambda x: len(x.split(\" \"))),reverse=True)\n\n# number of words in answer in test\nn_words_in_answer_test = sorted(test_df['answer'].apply(lambda x: len(x.split(\" \"))),reverse=True)\n\n# plot for train_df\nplot_sns(n_words_in_answer_train,\"answer\",color='darkred',title='number',subtitle='train_df')\n\n# plot for test_df\nplot_sns(n_words_in_answer_test,\"answer\",color='orangered',title='number',subtitle='test_df')\n","2c429504":"# Box plot of length of question_body\nbox_plot(n_words_in_answer_train, n_words_in_answer_test, \"number of words in answer\" )","252b1c6f":"box_cox_n_words_in_answer_train = boxcox(n_words_in_answer_train)[0]\nbox_cox_n_words_in_answer_test =  boxcox(n_words_in_answer_test)[0]\n\ntrain_df['box_cox_n_words_in_answer'] = box_cox_n_words_in_answer_train\ntest_df['box_cox_n_words_in_answer'] = box_cox_n_words_in_answer_test\n\n# Checking weather box cox transformed len_question_body_box_cox follows normal distribution or not using Q-Q plot\nq_q_plot(box_cox_n_words_in_answer_train, box_cox_n_words_in_answer_test, \"box-cox transformed of n_words_in_answer \")","80b85427":"# refer: https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python\n\n# For train_df\ntext_train = \" \".join(word for word in train_df['answer'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_train)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of answer in train\\n\")\nplt.axis(\"off\")\nplt.show()\n\n#===================================================================\n\n# For test_df\ntext_test = \" \".join(word for word in test_df['answer'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_test)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of answer in test\")\nplt.axis(\"off\")\nplt.show()","cb222aa1":"# Frequency of most popular worlds in answer of train\nword_frequency_plot(train_df['answer'], title='train')\n\n# Frequency of most popular worlds in answer of test_df\nword_frequency_plot(test_df['answer'], title='test')","082e2d00":"# Number of unique answer_user in train and test\nprint(f'Number of unique answer_user in train: {len(train_df[\"answer_user_name\"].unique())}')\nprint(f'Number of unique answer_user in test: {len(test_df[\"answer_user_name\"].unique())}')","d31f4e80":"plt.figure(figsize=(9,5))\nvenn2([set(train_df.answer_user_name.unique()), set(test_df.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common  answer_user in both train and test\", fontsize=15)\nplt.show()","529cc13a":"# for train_df\nn_count_answer_user_name_train=train_df['answer_user_name'].value_counts().values\n\n# for test_df\nn_count_answer_user_name_test=test_df['answer_user_name'].value_counts().values\n\n# plot for Distribution of counts answer_user_name in train and test\nplot_bar(n_count_answer_user_name_train, n_count_answer_user_name_test,feature='answer_user_name',x_label=\"Number of answer  \",y_label=\"Counts of \")","6dc74c79":"# for train_df\nn_user_unique_answer_train = train_df.drop_duplicates(subset=['question_title'])['answer_user_name'].value_counts()\n\n# for test_df\nn_user_unique_answer_test = test_df.drop_duplicates(subset=['question_title'])['answer_user_name'].value_counts()\n\n# plot of Which user has answered most number of unique question based on question_title?\nplot_bar(n_user_unique_answer_train.values, n_user_unique_answer_test.values,feature='answer_user_name',x_label=\"Number of questions  \",y_label=\"Counts of \")","4cc14ec9":"# Users who has answered the same question more than once but differently\nuser_answered_same_ques_twice = pd.DataFrame(train_df.groupby(['question_title','answer_user_name'])['answer_user_name'].agg(['count'])).sort_values(by='count',ascending=False)\nuser_answered_same_ques_twice = user_answered_same_ques_twice.reset_index(level=1)\nuser_answered_same_ques_twice.head(20)","214afa98":"print(f' Number of unique users who has answerd the same question more than once: {sum(user_answered_same_ques_twice[\"count\"]>1)} out of {len(train_df[\"answer_user_name\"].unique())}\\\n ({round(sum(user_answered_same_ques_twice[\"count\"]>1) \/ len(train_df[\"answer_user_name\"].unique()),4)})%')","76d97803":"# Top 10 user who has asked most number of unique question\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n\nsns.barplot(n_user_unique_answer_train[:10].values,n_user_unique_answer_train[:10].index,ax=ax1)\nax1.set(xlabel = \"number of unique question\", ylabel=f\"top 10 answer_user_name\", title='train_df\\n')\nax1.grid()\n\n\nsns.barplot(n_user_unique_answer_test[:10].values,n_user_unique_answer_test[:10].index , ax=ax2)\nax2.set(xlabel = \"number of unique question\", ylabel=f\"top 10  answer_user_name\", title='test_df\\n')\nax2.grid()\nplt.show()","5cbe3fde":"n_user_unique_answer_train.describe()","df1ddde1":"n_user_unique_answer_test.describe()","02ccf8b2":"# Find the number of words in each answer \ntemp = train_df[['answer_user_name','answer']]\nnumber_of_words = temp['answer'].apply(lambda x : len(x[0].split()))\ntemp[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of answer answered by each unique answer_user\nanswer_user_agg_behaviour_train = temp.groupby('answer_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nanswer_user_agg_behaviour_train = answer_user_agg_behaviour_train.reset_index(level=0)\n\n# Renaming column names\nanswer_user_agg_behaviour_train.rename({'sum': 'sum_answer_len', 'min': 'min_answer_len', 'max': 'max_answer_len', 'mean': 'mean_answer_len', 'median': \"median_answer_len\"}, axis=1, inplace=True)\n\n# merging to train dataframe\ntrain_df = pd.merge(left=train_df ,right=answer_user_agg_behaviour_train ,how='inner',on=\"answer_user_name\")","1f5a0a75":"# Find the number of words in each answer \ntemp = test_df[['answer_user_name','answer']]\nnumber_of_words = temp['answer'].apply(lambda x : len(x[0].split()))\ntemp[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of answer answered by each unique answer_user\nanswer_user_agg_behaviour_test = temp.groupby('answer_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nanswer_user_agg_behaviour_test = answer_user_agg_behaviour_test.reset_index(level=0)\n\n# Renaming column names\nanswer_user_agg_behaviour_test.rename({'sum': 'sum_answer_len', 'min': 'min_answer_len', 'max': 'max_answer_len', 'mean': 'mean_answer_len', 'median': \"median_answer_len\"}, axis=1, inplace=True)\n\n# merging to train dataframe\ntest_df = pd.merge(left=test_df ,right=answer_user_agg_behaviour_test ,how='inner',on=\"answer_user_name\")","595bf5a9":"# Ploting user behaviour\nuser_behaviour_column_on_answer = ['sum_answer_len', 'sum_answer_len', 'min_answer_len', 'mean_answer_len','median_answer_len']\n\nfor idx,column in enumerate(user_behaviour_column_on_answer):\n    \n    train_set = answer_user_agg_behaviour_train.sort_values(by= column ,ascending=False)[column]\n    test_set = answer_user_agg_behaviour_test.sort_values(by= column ,ascending=False)[column]\n    \n    print(f\"\\n{idx+1}: Plot for {column}\")\n    \n    # Ploting lineplot for trainset\n    f, (ax1, ax2 , ax3 , ax4 ) = plt.subplots(1, 4, figsize=(24,5))\n    \n    sns.lineplot(np.arange(len(train_set)), train_set,ax=ax1, color = \"darkred\")\n    ax1.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax1.grid()\n\n    # Ploting lineplot for testset\n    sns.lineplot(np.arange(len(test_set)), test_set,ax=ax2 ,color = \"orangered\")\n    ax2.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax2.grid()\n    \n    # Ploting distplot for trainset\n    sns.distplot(train_set,ax=ax3, color = \"darkred\")\n    ax3.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax3.grid()\n\n    # Ploting distplot for testset\n    sns.distplot( test_set, ax=ax4 ,color = \"orangered\")\n    ax4.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax4.grid()\n    plt.show()","1b0e57e8":"# Q-Q Plot of Box Cox transformed user_behaviour_column_on_answer features\nfor idx,column_name in enumerate(user_behaviour_column_on_answer[:2]):\n    \n    print(f\"Feature name: {column_name}\")\n    \n    # Continues the loop if box cox fails to transform\n    if sum(train_df[f'{column_name}']<1)>0:\n        print(f\"{idx+1}: Box Cox Transformation can not be applied on feature '{column_name}' because all the values of data must be positive for transformation\\n\\n\")\n        continue\n\n    # Box Cox Transform\n    boxcox_transformed_feature_train = boxcox(train_df[column_name])[0]\n    boxcox_transformed_feature_test = boxcox(test_df[column_name])[0]\n    \n             \n    # Saving the transformed user_behaviour_column_on_answer column in dataframe\n    train_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_train\n    test_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_test\n    \n    print(f\"{idx+1}: Q-Q Plot for box cox transformed feature of {column_name}\\n\")\n    \n    # Q-Q Plot of transformed Feature\n    q_q_plot(boxcox_transformed_feature_train, boxcox_transformed_feature_test, f\"box cox transformed {column_name}\")\n    ","9fdaf0df":"# Venn diagram for train_df\nplt.figure(figsize=(16,8))\nplt.subplot(211)\nvenn2([set(train_df.question_user_name.unique()), set(train_df.answer_user_name.unique())], set_labels = ('question_user set', 'answer_user set') )\nplt.title(\"Common users who has asked the question and answered by himself in train data\", fontsize=15)\n\n# Venn diagram for test_df\nplt.subplot(212)\nvenn2([set(test_df.question_user_name.unique()), set(test_df.answer_user_name.unique())], set_labels = ('question_user set', 'answer_user set') )\nplt.title(\"Common users who has asked the question and answered by himself in test data\", fontsize=15)\nplt.show()","027f2e76":"# unique categories\nprint(train_df['category'].unique())","f8e70a83":"# For train_df\nprint(f\"Unique number of category: {len(train_df['category'].unique())}\\n\")\n\ncategory_dist_df_train = pd.DataFrame(train_df['category'].unique(),columns=['category'])\ncategory_dist_df_train[\"values_count\"] = train_df['category'].value_counts().values\ncategory_dist_df_train[\"distribution\"] = train_df['category'].value_counts().values\/sum(train_df['category'].value_counts().values)\ncategory_dist_df_train","7a1483ea":"# For test_df\nprint(f\"Unique number of category: {len(test_df['category'].unique())}\\n\")\n\ncategory_dist_df_test = pd.DataFrame(test_df['category'].unique(),columns=['category'])\ncategory_dist_df_test[\"values_count\"] = test_df['category'].value_counts().values\ncategory_dist_df_test[\"distribution\"] = test_df['category'].value_counts().values\/sum(test_df['category'].value_counts().values)\ncategory_dist_df_test","c188a67a":"f, (ax1, ax2  ) = plt.subplots(1, 2, figsize=(24,7))\n\n# Categories dist for train_df\nax1.pie(category_dist_df_train.values_count, labels=category_dist_df_train.category, shadow=True, autopct='%.1f%%')\nax1.set( title='Categories distribution: train\\n')\n\n\n# Categories dist for test_df\nax2.pie(category_dist_df_test.values_count, labels=category_dist_df_test.category, shadow=True, autopct='%.1f%%')\nax2.set(title='Categories distribution: test\\n')\nplt.show()\n","14075625":"\n# Venn diagram for train_df\nplt.figure(figsize=(16,8))\nplt.subplot(111)\nvenn2([set(train_df.host.unique()), set(test_df.host.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common number of Host in train and test\", fontsize=15)\nplt.show()","ec659135":"# For train_df\nhost_dist_df_train = pd.DataFrame(train_df['host'].value_counts().index,columns=['host'])\nhost_dist_df_train[\"values_count\"] = train_df['host'].value_counts().values\nhost_dist_df_train[\"distribution\"] = train_df['host'].value_counts().values\/sum(train_df['category'].value_counts().values)\n\n# For test_df\nhost_dist_df_test = pd.DataFrame(test_df['host'].value_counts().index,columns=['host'])\nhost_dist_df_test[\"values_count\"] = test_df['host'].value_counts().values\nhost_dist_df_test[\"distribution\"] = test_df['host'].value_counts().values\/sum(train_df['category'].value_counts().values)","706c7ec7":"print(f\"Unique number of host in train: {len(train_df['host'].unique())}\\n\")\n\n# plot for distribution of host in train\nplt.figure(figsize=(28,8))\nsns.barplot(x=host_dist_df_train['host'], y=host_dist_df_train['distribution'])\nplt.title(\"Host distribution: Train\\n\")\nplt.xticks(rotation=85)\nplt.grid()\nplt.show()","10432aea":"print(f\"Unique number of host in train: {len(test_df['host'].unique())}\\n\")\n\n# plot for distribution of host in train\nplt.figure(figsize=(28,8))\nsns.barplot(x=host_dist_df_test['host'], y=host_dist_df_test['distribution'])\nplt.title(\"Host distribution: Test\\n\")\nplt.xticks(rotation=85)\nplt.grid()\nplt.show()","2cd0ab51":"n_self_question_answer_users_df_train = pd.DataFrame(columns=['n_of_self_question_answer_users'])\n\nfor idx, host_name in enumerate(train_df['host'].unique()):\n    ques_user =set(train_df[train_df['host']==host_name]['question_user_name'])\n    ans_user =set(train_df[train_df['host']==host_name]['answer_user_name'])\n    n_self_question_answer_users = len(ques_user.intersection(ans_user))\n    n_self_question_answer_users_df_train.loc[host_name] = n_self_question_answer_users\n\nn_self_question_answer_users_df_train.sort_values(by ='n_of_self_question_answer_users' ,ascending=False,inplace=True)\n\n\n# plot for distribution of top 10 host where number of users  has asked the question and answered by himself\nplt.figure(figsize=(16,7))\nsns.barplot(x = n_self_question_answer_users_df_train.head(10).index, y = n_self_question_answer_users_df_train['n_of_self_question_answer_users'].head(10))\nplt.title(\"Host distribution: Train\\n\")\nplt.xticks(rotation=85)\nplt.grid()\nplt.show()","6c79c1dc":"n_self_question_answer_users_df_test = pd.DataFrame(columns=['n_of_self_question_answer_users'])\n\nfor idx, host_name in enumerate(test_df['host'].unique()):\n    ques_user =set(test_df[test_df['host']==host_name]['question_user_name'])\n    ans_user =set(test_df[test_df['host']==host_name]['answer_user_name'])\n    n_self_question_answer_users = len(ques_user.intersection(ans_user))\n    n_self_question_answer_users_df_test.loc[host_name] = n_self_question_answer_users\n\nn_self_question_answer_users_df_test.sort_values(by ='n_of_self_question_answer_users' ,ascending=False,inplace=True)\n\n\n# plot for distribution of top 10 host where number of users  has asked the question and answered by himself\nplt.figure(figsize=(16,7))\nsns.barplot(x = n_self_question_answer_users_df_test.head(10).index, y = n_self_question_answer_users_df_test['n_of_self_question_answer_users'].head(10))\nplt.title(\"Host distribution: Test\\n\")\nplt.xticks(rotation=85)\nplt.grid()\nplt.show()","e76200ed":"# refer: https:\/\/www.kaggle.com\/kabure\/qa-eda-and-nlp-modelling-insights-and-data-vis\n\nhost = train_df.groupby(['host'])['url'].nunique().sort_values(ascending=False)\ncategory = train_df.groupby(['category'])['url'].nunique().sort_values(ascending=False)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Unique URL by Host and Categories', size=22)\n\nplt.subplot(211)\ng0 = sns.barplot(x=category.index, y=category.values, color='blue')\ng0.set_title(\"Unique Questions by category\", fontsize=22)\ng0.set_xlabel(\"Category Name\", fontsize=19)\ng0.set_ylabel(\"Total Count\", fontsize=19)\n#g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nfor p in g0.patches:\n    height = p.get_height()\n    g0.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.1f}%'.format(height\/category.sum()*100),\n            ha=\"center\",fontsize=11) \n\nplt.subplot(212)\ng1 = sns.barplot(x=host[:20].index, y=host[:20].values, color='blue')\ng1.set_title(\"TOP 20 HOSTS with more UNIQUE questions\", fontsize=22)\ng1.set_xlabel(\"Host Name\", fontsize=19)\ng1.set_ylabel(\"Total Count\", fontsize=19)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=85)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.1f}%'.format(height\/host.sum()*100),\n            ha=\"center\",fontsize=11) \n    \nplt.subplots_adjust(hspace = 0.3, top = 0.90)\n\nplt.show()","1ee3a855":"# refer: https:\/\/www.kaggle.com\/kabure\/qa-eda-and-nlp-modelling-insights-and-data-vis\n\nimport matplotlib.gridspec as gridspec # to do the grid of plots\n\ngrid = gridspec.GridSpec(3, 3)\nplt.figure(figsize=(16,3*4))\n\nplt.suptitle('Intersection QA USERS \\nQuestions and Answers by different CATEGORIES', size=20)\n\nfor n, col in enumerate(train_df['category'].value_counts().index):\n    ax = plt.subplot(grid[n])\n    venn2([set(train_df[train_df.category == col]['question_user_name'].value_counts(dropna=False).index), \n           set(train_df[train_df.category == col]['answer_user_name'].value_counts(dropna=False).index)],\n      set_labels=('Question Users', 'Answer Users'), )\n    ax.set_title(str(col), fontsize=15)\n    ax.set_xlabel('')\n    #plt.subplots_adjust(top = 0.98, wspace=.9, hspace=.9)\n    \nplt.subplots_adjust(top = 0.9, hspace=.1)\n\nplt.show()","0b059640":"# refer: https:\/\/www.kaggle.com\/kabure\/qa-eda-and-nlp-modelling-insights-and-data-vis\n\ngrid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,4.5*4))\n\nplt.suptitle('Intersection QA USERS - TOP 15 \\nQuestions and Answers by different HOSTS', size=20)\ntop_host = train_df['host'].value_counts()[:15].index\nfor n, col in enumerate(top_host):\n    ax = plt.subplot(grid[n])\n    venn2([set(train_df[train_df.host == col]['question_user_name'].value_counts(dropna=False).index), \n           set(train_df[train_df.host == col]['answer_user_name'].value_counts(dropna=False).index)],\n      set_labels=('Question Users', 'Answer Users'), )\n    ax.set_title(str(col), fontsize=15)\n    ax.set_xlabel('')\n    #plt.subplots_adjust(top = 0.98, wspace=.9, hspace=.9)\n    \nplt.subplots_adjust(top = 0.9, hspace=.1)\n\nplt.show()","f3dd4359":"# Tokenize each item in the review column\nfrom nltk import word_tokenize\nword_tokens = [word_tokenize(question) for question in train_df.question_body]\n\n# Create an empty list to store the length of the reviews\nlen_tokens = []\n\n# Iterate over the word_tokens list and determine the length of each item\nfor i in range(len(word_tokens)):\n     len_tokens.append(len(word_tokens[i]))\n\n# Create a new feature for the lengh of each review\ntrain_df['question_n_words'] = len_tokens","45624d97":"grid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,6*4))\n\nplt.suptitle('Title and Question Lenghts by Different Categories \\nThe Mean in RED - Also 5% and 95% lines', size=20)\ncount=0\ntop_cats = train_df['category'].value_counts().index\nfor n, col in enumerate(top_cats):\n    for i, q_t in enumerate(['question_title', 'question_body', 'question_n_words']):\n        ax = plt.subplot(grid[count])\n        if q_t == 'question_n_words':\n            sns.distplot(train_df[train_df['category'] == col][q_t], bins = 50, \n                         color='g', label=\"RED - 50%\") \n            ax.set_title(f\"Distribution of {str(col)} \\nQuestion #Total Words Distribution\", fontsize=15)\n            ax.axvline(train_df[train_df['category'] == col][q_t].quantile(.95))\n            ax.axvline(train_df[train_df['category'] == col][q_t].quantile(.05))\n            mean_val = train_df[train_df['category'] == col][q_t].mean()\n            ax.axvline(mean_val, color='red' )\n            ax.set_xlabel('')            \n        else:\n            sns.distplot(train_df[train_df['category'] == col][q_t].str.len(), bins = 50, \n                         color='g', label=\"RED - 50%\") \n            ax.set_title(f\"Distribution of {str(col)} \\n{str(q_t)}\", fontsize=15)\n            ax.axvline(train_df[train_df['category'] == col][q_t].str.len().quantile(.95))\n            ax.axvline(train_df[train_df['category'] == col][q_t].str.len().quantile(.05))\n            mean_val = train_df[train_df['category'] == col][q_t].str.len().mean()\n            ax.axvline(mean_val, color='red' )\n            #ax.text(x=mean_val*1.1, y=.02, s='Holiday in US', alpha=0.7, color='#334f8d')\n            ax.set_xlabel('')\n        count+=1\n        \nplt.subplots_adjust(top = 0.90, hspace=.4, wspace=.15)\nplt.show()","a21d17f0":"# Scaling targets or labels with mean=0, and variance=1 (getting targets ready for PCA)\nsc=StandardScaler(with_mean=True)\nscalar_targets = sc.fit_transform(train_df[target_vars])\n\n# Pca fitting and transform\npca = PCA()\npca_component = pca.fit_transform(scalar_targets)\n\n# variance explained by top 2 eigen vector values\nprint(f\"variance explained by top 2 eigen vector values: {round(sum(pca.explained_variance_ratio_[:2]),2)} %\")","a1bd3542":"pca__target_component_1 = pca_component[:,0]\npca__target_component_2 = pca_component[:,1]\n\n# Ploting\nplt.figure(figsize=(12,7))\nsns.scatterplot(pca__target_component_1, pca__target_component_2, hue=train_df['category'])\nplt.title(\"Visualisation of pca__target_component_1 V\/S pca__target_component_2 \")\nplt.xlabel(\"pca__target_component_1\")\nplt.ylabel(\"pca__target_component_2\")\nplt.show()","87128802":"from wordcloud import WordCloud, STOPWORDS\n\nstopwords = set(STOPWORDS)\n\nfor idx, cat in enumerate(train_df['category'].value_counts().index):\n    \n    f, (ax1, ax2 ) = plt.subplots(1, 2, figsize=(16,5))\n    \n    wordcloud_question = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=100, \n        width=400, height=280,\n        random_state=42,\n    ).generate(\" \".join(train_df[train_df['category'] == cat]['question_body'].astype(str)))\n    \n    print(f\" {idx+1}: category {cat}\")\n    ax1.imshow(wordcloud_question)\n    ax1.set(title=f'Category: {cat}\\n Question_body')\n    ax1.axis('off')\n    \n    wordcloud_anser = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=100, \n        width=400, height=280,\n        random_state=42,\n    ).generate(\" \".join(train_df[train_df['category'] == cat]['answer'].astype(str)))\n    \n    ax2.imshow(wordcloud_anser)\n    ax2.set(title=f'Category: {cat}\\n Answer')\n    ax2.axis('off')","b5cf58f1":"pol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ntrain_df['q_title_polarity'] = train_df['question_title'].apply(pol)\ntrain_df['q_title_subjectivity'] = train_df['question_title'].apply(sub)\n\ntrain_df[['question_title', 'category', 'q_title_polarity', 'q_title_subjectivity']].head()","c457a53d":"#Polarity and subjectivity plot\nplt.figure(figsize=(12,5))\ng = sns.scatterplot(x='q_title_polarity', y='q_title_subjectivity', \n                    data=train_df, hue='category')\ng.set_title(\"Sentiment Analyzis (Polarity x Subjectivity) of question_title by 'Category' Feature\", fontsize=16)\ng.set_xlabel(\"Polarity distribution\",fontsize=18)\ng.set_ylabel(\"Subjective \",fontsize=18)\nplt.show()","867b7eff":"pol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ntrain_df['q_body_polarity'] = train_df['question_body'].apply(pol)\ntrain_df['q_body_subjectivity'] = train_df['question_body'].apply(sub)\n\ntrain_df[['question_body', 'category', 'q_body_polarity', 'q_body_subjectivity']].head()","7bc45ef2":"#Polarity and subjectivity plot\nplt.figure(figsize=(12,5))\ng = sns.scatterplot(x='q_body_polarity', y='q_body_subjectivity', \n                    data=train_df, hue='category')\ng.set_title(\"Sentiment Analyzis (Polarity x Subjectivity) of question_body by 'Category' Feature\", fontsize=16)\ng.set_xlabel(\"Polarity distribution\",fontsize=18)\ng.set_ylabel(\"Subjective \",fontsize=18)\nplt.show()","15e938e1":"pol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ntrain_df['answer_polarity'] = train_df['answer'].apply(pol)\ntrain_df['answer_subjectivity'] = train_df['answer'].apply(sub)\n\ntrain_df[['answer', 'category', 'answer_polarity', 'answer_subjectivity']].head()","55f6160c":"#Polarity and subjectivity plot\nplt.figure(figsize=(12,5))\ng = sns.scatterplot(x='answer_polarity', y='answer_subjectivity', \n                    data=train_df, hue='category')\ng.set_title(\"Sentiment Analyzis (Polarity x Subjectivity) of answer by 'Category' Feature\", fontsize=16)\ng.set_xlabel(\"Polarity distribution\",fontsize=18)\ng.set_ylabel(\"Subjective \",fontsize=18)\nplt.show()","2564eec4":"# Saving all the transformed features and behaviour features into pickle file\n\"\"\"train_df.to_pickle(\"train_df.pkl\")\ntest_df.to_pickle(\"test_df.pkl\")\"\"\"","b0d03d20":"all_feat = [col for col in train_df.columns if col not in target_vars]\nprint(\" All the features after all the box cox transformation and user behaviour analyses:\\n\")\nfor idx,f in enumerate(all_feat):\n    print(f\"{idx+1}: {f}\")","5777122c":"#### Observation\n* No, Length of question_title does not follows normal distribution.","444e3092":"### 5.5.3 Number of words in question_body in train and test ","7a3bffec":"* Each of the Target Labels name is self explanatory\n* All the 30 Taget Labes has values between range [0,1]","534b4276":"#### First chart: \nThe most common category in Stackexchange f\u00f3rums are:\n\n1 - Technology(41.8%)\n\n2 - Stackoverflow (21.2%)\n\n3 - Culture (14.7%)\n\n4 - Science (11.2)\n\n5 - Life Arts(10.9)\n\n* It's not so unbalanced, so it can be useful to train the model.\n\n\n#### Second Chart: \n* The most common category is the Stackoverflow with 20.6% of the total open topics(questions);\n\n* After the StackOverflow we can see that only 7 categories have a ratio highest than 2%, we have in total 59;\n\n* The group of Stackoverflow, eletronics, superuser, serverfault, english, math, tex, physics and askubuntu together has almost 46% of the total questions.","0694d416":"## 5.7. EDA : answer Feature","376efb83":"* There are 29 answer_user_name which is common in both train and test.","5b6c76eb":"* It's interesting to note that programmers host has almost 1:3 ratio between QA Users, and only one in the intersection.\n\n* Only for curiosity, we can take a look on one some of the intersection users to see what type of questions was done by them","ad8ab464":"#### 5.7.3.1. Box plot of number of words in question_body","e6ac426b":"* The number of words in question_body in train and test is less than 500 \n* the distribution number of words in question_body is highly skewed towards right indicates that there are few question_body which has very high number of words in question_body( greater than 500 and upto 7000 words). This could be because of some questions are from programming contex and the syntax of words maybe counted as indiviaudal word.\n* It also looks like number of words in question_body follows log normal distribution. If yes, we can transform it to nomal distribution using Box-Cox transform and apply linear model on top of transformed feature for better result. ","052f7281":"#### Observation:\n\n1. Almost all taget labels are imbalanced and skewed but some are highly skewed like 'question_not_really' , 'question_type_spelling' etc. So we need to take special care for those labels.\n\n2. Wee need to make sure not to overfit on highly skewed target labels.\n    ","2ceb7a7b":"## 5.6 EDA : question_user_name feature","2b81a1f2":"### 5.10.3. Number of users who has asked question and answered by themself (by Host) in train","f10ab9d3":"### 5.6.2 Which user has asked most number of unique question based on question_title?","70f09aeb":"#### 5.8.1.3. Number of answer given by users","877ba4e4":"* As punctuations are not removed from anser, most frequent words are either punctuations or programming syntax. So we need to plot this graph again after removing punctuations for accurate word frequency plot.","63dd16d3":"\n* There is no common question which is occured in both train and test.","d46b872d":"#### 5.8.1.2. Common answer_user in both train and test","10806577":"* The data for this competition includes questions and answers from various StackExchange properties. Your task is to predict target values of 30 labels for each question-answer pair.\n\n* The list of 30 target labels are the same as the column names in the sample_submission.csv file. Target labels with the prefix question_ relate to the question_title and\/or question_body features in the data. Target labels with the prefix answer_ relate to the answer feature.\n\n* Each row contains a single question and a single answer to that question, along with additional features. The training data contains rows with some duplicated questions (but with different answers). The test data does not contain any duplicated questions.\n\n* This is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Therefore, predictions must also be in that range.\n\n* Since this is a synchronous re-run competition, you only have access to the Public test set. For planning purposes, the re-run test set is no larger than 10,000 rows, and less than 8 Mb uncompressed.\n\n* Additional information about the labels and collection method will be provided by the competition sponsor in the forum.\n","2fb89069":"### 5.8.2. Number of users who has answered most number of unique question?","b6ef7cc4":"#### for test_df","ab9f68b3":"#### Description of number of unique question asked by user (train)","4dbe8d3b":"# 1. Problem Statement\n#### Source(kaggle): https:\/\/www.kaggle.com\/c\/google-quest-challenge\/overview","40f66daa":"* Most of Categories is common in both train and test set.","c6193284":"### 5.8.1. Distribution of answer_user and number of answer answerd by user in train and test","2050a376":"#### Lets find out those users  who has answered the same question more than once but having different answer (train_df)?","44cd284b":"### 5.7.4. WordCloud of answer in train and test ","2dff7314":"#### Sample","e77f6cc4":"## 3.1. Multi Label Regression problem\n\nIt is clearly mentioned in Kaggle Competition page:  \n\"This is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Therefore, predictions must also be in that range.\"\n\n#### Predict the continous value between range [0,1] for 30 target labels\n\n","99287abf":"#### For train_df","b9e6bf3b":"## 3.2 Evaluation Metric","165c4425":"* Number of words in question title does not follows normal distribution.","cc8a80ef":"#### 4.8.2.4. Description of number of unique question asked by user (train)","54a38d7c":"#### For test_df","aaf43ea6":"* From this plot it can be interprated that people tend to self question and answer on the host website 'stackoverflow.com' most.\n\n* `Stack Overflow is a question and answer site for professional and enthusiast programmers`","7eea68a7":"## 5.6.5. Question User behaviour on question_body\n* How many number of words user usually ask in question_body? \n* What is the minimum number of words in question_title asked by each user?\n* Similary \"max_title_len\" , \"mean_title_len\" , \"median_title_len\" ?","236b6080":"# 5. Exploratory Data Analyses","bae193a3":"#### Distribution For train_df","abd41612":"## 4.10. EDA: Host featue","f21ecd98":"#### 5.5.2.2 Box Cox transform of length of question_body and Q-Q plot of transformed feature","4393b6e6":"#### 5.8.1.1. Number of unique answer_user in train and test","776f3977":"* Most users has asked only one question in both train and test dataset.\n* Few users has asked more than 1 questions (upto 9) in the train and (upto 5) in te test","e0c6aa74":"### 5.10.5. PCA visualisation of target labels hue by categories\n\nrefer : https:\/\/www.kaggle.com\/kabure\/qa-eda-and-nlp-modelling-insights-and-data-vis\n\n#### This is to just visualise how important is categorical feature in distintuising taget labels?  (like pseudo feature importance)\n\nApplying Machine learning model and then finding feature importance would definaltely be a better choice nonetheless.","e2c921c4":"* Wordclouds of words in question_body without preprocessing roughly shows that there is huge similarity in words of question_body in both train and test data.\n\n* Just like keywords in question_title, here also  most of the keywords are in the contex of programing. So while doing preprocessing, we need to keep this in mind that we do not remove programing syntax from question_body.","55da1ac9":"* Wordclouds of words in answer without preprocessing roughly shows that there is huge similarity in words of question_body in both train and test data.\n\n* Just like keywords in question_body, here also  most of the keywords are in the contex of programing. So while doing preprocessing, we need to keep this in mind that we do not remove programing syntax from answer.","bd180ddb":"### 5.10.3. Top 10 host where number of users  has asked the question and answered by himself","9e36b75f":"#### 5.7.2.2 Box Cox transform of length of answer and Q-Q plot of transformed feature","c13a0176":"* we can see that few of the targets which have high positive correlation have very similar distributions and some are even overlaping.\n* Same can be observed for targets which have high negative correlation.","9b98c568":"#### Distribution For test_df","007e8be1":"## 5.6.3 Common unique question_user_name in train and test (Venn Diagram)","8f40015e":"#### 5.4.2.1 Box plot of Length of question_title in train and test","fe40c32f":"### 5.7.2. Length of answer in train and test ","67ac20e5":"# 2. Business Objective and Constrains","945abded":"### 5.10.2. Distribution of host in train and test","f48269a4":"## 5.10.Some More Importand EDA plots","ae26340c":"#### Special Thanks to these Kaggle kernals and refers \n\n* https:\/\/www.kaggle.com\/dimitreoliveira\/google-quest-eda-and-use-baseline\n\n* https:\/\/www.kaggle.com\/codename007\/start-from-here-quest-complete-eda-fe\n\n* https:\/\/www.kaggle.com\/kabure\/qa-eda-and-nlp-modelling-insights-and-data-vis","a6bdc4bc":"* Many of the questions occured only 1 time but still very large numbers of question occured more than once in the train data.\n* Maximum number of times a question occured repeatedly in train data is 12.\n* Test data do not have any repeative occurance of questions.","b31475a3":"### 5.4.2 Length of question_title in train and test ","74363167":"Reason\n\n","e0b9dd42":"## Note:\n### THIS KERNEL IS NOT FINISHED\n\n* Stay tuned  for more machine learning and deppe leaning and NLP concepts in Part 2\n\n*** If this kernel was useful for you, please upvote the kernel","26eff504":"### 5.7.3. Number of words in answer in train and test ","f6c82191":"## 5.9. EDA: category Feature","d8075c9b":"#### Unique number of question_user_name","71c18b1a":"#### 5.5.3.2 Box Cox transform of number of words in question_body and Q-Q plot of transformed feature","a0777c90":"* This plot is showing some good insights. The WordClouds of Questions and Answers is almost similar of each of the categories.\n* This means that if question is in the programing contex or belong to stackoverflow category then answer is also in the same contex. ( Isn't it common sense :) )\n* Similary we can plot Wordclouds of each host as well. but we will plot it after preprocessing the text and revisit all the wordcloud plots again.","72aab131":"* As punctuations are not removed from question_body, most frequent words are either punctuations or programming syntax. So we need to plot this graph again after removing punctuations for accurate word frequency plot.","6c01f98f":"#### For train_df","20d68dcb":"#### File descriptions\n* train.csv - the training data (target labels are the last 30 columns)\n* test.csv - the test set (you must predict 30 labels for each test set row)\n* sample_submission.csv - a sample submission file in the correct format; column names are the 30 target labels","7e7129cd":"## 5.2. Sample Datapoint (train_df)","7e152db8":"# 4. Loading The Dataset","ed36f291":"* Obviously the plot for repeated question_body and repeated question_title are same. \n* This also indicates there is no repeated question having different question_title but same question_body or viceversa.","19935685":"* The length of most of answer in train and test is less than 2000 \n* the distribution length of answer is highly skewed towards right indicates that there are few answer which has very large length( > 2000) . This could be becuase of reason that programming contex question has also the answer in coding format and having large set of syntax.\n* It looks like length of answer follows log normal distribution. If yes, we can transform it to nomal distribution using Box-Cox transform and apply linear model on top of transformed feature for better result.  Let's Find it out.","8df794be":"#### Plot for test_df","75c6d1b0":"#### 5.8.2.2. Number of unique users who has answerd the same question more than once?","ea7a8614":"## 5.8. EDA : answer_user_name feature","1458f73f":"* Host stackoverflow.com has majority of distribution in both train set and test set. This could be the reason why most of the keywords in WordClouds has programing syntax.","5fb0c86f":"### 5.6.1 Distribution of question_user_name and number of question asked by user in train and test","e96c95b0":"#### 5.5.2.1 Box plot of length of question_body","064fb78b":"### 5.3.1. Distribution of target labels","b853c6cd":"### 5.4.5 Frequency of most popular words in train and test","d15b8541":"## 6. Insights and Findings","ce3ae6dd":"* Number of words in most of the question_title in train and test lies between ~ (2,20) words \n* Very few question_title has number of words greater than 20 making the plot skewed toward the right.\n* It also looks like the number of words in question_title follows Normal distribution with slight skew toward the right. Let's find it out using Q-Q plot.","5a1b1bda":"#### Sample Question Target Labels","74a0e22f":"### 4.2 Independent variables","c171959e":"* Most of these Q-Q Plots do looks alike tey follows normal distribution. So just keep these transformed feature","64f53c2e":"### Case Study 1\n\n# Google QUEST Q&A Labeling\nImproving automated understanding of complex question answer content.\n","8f047e16":"#### Top 10 user who has asked most number of unique question","fb138ca1":"#### 5.7.3.2 Box Cox transform of number of words in answer and Q-Q plot of transformed feature","6a8bc80a":"## 5.5 EDA: Dependent variable question_body","d5330159":"## 5.3. EDA: Target Labels","54e49809":"### 5.10.8 Polarity and subjectivity analyses of question_body\n\nrefer: https:\/\/www.kaggle.com\/kabure\/qa-eda-and-nlp-modelling-insights-and-data-vis","2253700a":"### 5.4.1. Number of repeated question_title in train and test","5720a664":"* The length of most of question_title in train and test lies between ~ (20,100) \n* Very few question_title has very large length.\n* It also looks like the length of question_title follows Normal distribution with slight skeweness toward the right . Let's find it out using Q-Q plot.","fc6aee2a":"#### 5.5.3.1. Box plot of number of words in question_body","0f632f75":"* The number of words in answer in train and test is less than 400 \n\n* the distribution number of words in answer is highly skewed towards right indicates that there are few question_body which has very high number of words in answer greater than 400 and upto 8000 words in train and upto 2500 in test.\n\n* This could be because of some questions are from programming contex and the syntax of words maybe counted as indiviaudal word.\n\n* It also looks like number of words in question_body follows log normal distribution. If yes, we can transform it to nomal distribution using Box-Cox transform and apply linear model on top of transformed feature for better result. ","bef6b3b1":"Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.\n\nHumans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well\u2026yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.","b0a4fbc0":"#### Observation\n\n1. There are some obvious high positive correlation e.g.\n    * answer_type_instructions is highly correlated to question_type_instructions.\n    * answer_type_procedure is highly correlated to question_type_procedure.\n    * answer_type_reason_explanation  is highly correlated to question_type_reason_explanation.\n    * question_interestingness_self is highly co related with question_interestingness_others \n    \n    \n2. Some other high posive correlation \n    * answer_level_of_information is highly correlated to answer_helpful.\n    * answer_plausable is highly correlated to answer_helpful.\n    * answer_satisfaction is highly correlated to answer_helpful , answer_level_of_information , answer_plausable and answer_relavance.\n    \n    \n3. There are also some high negative correlation is present\n    * question_fact_seeking is negatively correlated question_opinion_seeking.\n    * heatmap indicating light red color refers to all negatively correlated labels.\n    ","76deae5a":"### 5.4.4 WordCloud of question_title in train and test ","b8786aea":"#### 5.4.3.2 Checking weather n_words_in_question_title follows normal distribution or not using Q-Q plot","a3903324":"#### Plot for train_df","f2a499d9":"#### For train_df","bb2acdd6":"We have 3 csv files available\n* 1. train.csv\n* 2. test.csv\n* 3. sample_submission.csv\n\n#### 'train.csv' contain all the independent variable and 30 target labels on which we need to train the model.\n\n#### 'test.csv' contain only independent variable on which we have to predict.\n\n#### 'sample_submission.csv' contain the format of how to predict all the target values.","0b466c7b":"## Workflow\n\n#### 1. Problem Statement and source\n#### 2. Business Constrain and Dataset Loading\n#### 3. Machine learning Formulation and Evalution mtric\n#### 4. Loading Dataset\n#### 5. Exploratory Data Analyses\n#### 6. Insights and Findings\n#### 7. Preprocessing and Feature engineering \n#### 8. Modeling and Hyperparameter tuning \n#### 9. Results and Conclusion\n","fe9f9eec":"#### Ploting user behaviour based on queston_titile","2f2ddf86":"## Problem Defination","fbf19f0e":"#### Which Question is most popular?\nWhich question has maximum number of answers?","4e490734":"#### Sample Answer Target Labels","8edee66b":"* Ploting wordclouds of words without preprocessing roughly shows that there is huge similarity in words of question_title in both train and test data.\n* Also most of the keywords looks alike they are in the contex of programing. So while doing preprocessing, we need to keep this in mind that we do not remove programing syntax from question_title thinking as unnecessary word.","c4bcdcd3":"## About Dataset","bcfa8606":"## 5.4. EDA: Dependent Variable Questions_title","8e5d982d":"#### for test_df\n","caddbb3f":"#### 5.4.3.1 Box plot of number of words in question_title in train and test","5903945a":"### 5.10.7 Polarity and subjectivity analyses of question_title\n\nBig thangs to this kernal: https:\/\/www.kaggle.com\/kabure\/qa-eda-and-nlp-modelling-insights-and-data-vis","443f51b6":"#### All the features after all the box cox transformation and user behaviour analyses","6cc62732":"#### Sample Answer","d6bc9b86":"### 5.10.9 Polarity and subjectivity analyses of answer\n\nrefer: https:\/\/www.kaggle.com\/kabure\/qa-eda-and-nlp-modelling-insights-and-data-vis","8eb5ac5a":"#### 4.8.2.5. Description of number of unique question asked by user (test)","734cb950":"# 3. Machine Learning Formulation and Evaluation Metric\n","2cfb3ad5":"#### 1. Provide tremendous value in differentiating subjective aspects of question-answering (like humans)\n\n#### 2. No Low Latency required \n\n#### 3. Model Interpretability is helpful\n\n#### 4. Feature Importance is helpful\n\n","2d6c5a22":"refer: https:\/\/www.kaggle.com\/codename007\/start-from-here-quest-complete-eda-fe\n\n* A Venn diagram uses overlapping circles or other shapes to illustrate the logical relationships between two or more sets of items. Often, they serve to graphically organize things, highlighting how the items are similar and different.","96edbeb0":"* Yes, It looks that Box Cox transfromed feature of len_answer follows normal distribution\n","e0d51df7":"### 5.5.5 Frequency of most popular worlds in question_body of train and test","79873fbf":"## 5.6.4. User behaviour on question_title\n* How many number of words user usually ask in question_title? \n* What is the minimum number of words in question_title asked by each user?\n* Similary \"max_title_len\" , \"mean_title_len\" , \"median_title_len\" ?","f53f9ad0":"* The quantiles of the distributions are very similar in all categories in both title and question lenghts.","d7fc4707":"* All of the plots are either sligtly or heavely skewed toward right \n* All these plots shows explicit behaviour of question_user_name on question_title and some of the plots even looks alike pareto distribution(80-20 rule), e.g. `Plot for sum_title_len`.\n* Sum of user behaviour on queston_titile plots also looks like they follows log normal distribution.\n* Let's Transfrom those features and check weather transformed feature follows Noraml distribution using Q-Q Plot.  ","b4aae657":" Submissions are evaluated on the mean column-wise Spearman's correlation coefficient.\n \n The Spearman's rank correlation is computed for each target column, and the mean of these values is calculated for the submission score","4c0e321e":"* Looking At above plots, It is not clear exactly that transformed features follows normal distribution or not.\n* Let's just keep box cox transformed features and test these features importance while modeling","ab2a4a09":"* This plot and above this plot indicates that there are few users in dataset who has answered the same question more than once but answer is different.\n\n* Just like we have seen in question_users, here also mostly answer_users has answerd only on question.\n \n* And very few answer_users has answerd large number of questions upto 15 in train data and upto 6 in test data .","21448cfd":"### 5.10.1. Number of unique questions by category","84bc9775":"### 5.5.2 Length of question_body in train and test ","74c51bb5":"### 5.10.6. WordClouds of Questions Texts in each Categories","db9dfbfa":"* There are few users who like to ask the question and answer it by self. Can be clearly seen in above plot.","1343a006":"* Few of the plots of users behaviour on answer are heavely skewed toward right.\n* Top two plots also very similar to log normal distribution\n* Transfrom these log normal alike features using box cox and check weather transformed features follows Noraml distribution or not using Q-Q plot","45ec73f5":"* Yes, It kinda look that Box Cox transfromed feature of len_question_body_box_cox_train roughly follows normal distribution\n","4c4c0d55":"#### 5.4.2.2.  Checking weather len_question_title follows normal distribution using Q-Q plot\n\nIf len_question_title follows Normal or Gaussian distribution , We can make use of these features in Logistic Regression like model which is the generalisation of gaussian Naive Bayes for better results","438d9d23":"* There are 37  question_user_name who is present in both train and test data","1ce097ab":"* Guess we cannot tranform these user behaviour features using box cox thus we cannot plot Q-Q plot. Therefore cannot use these transformed featues","18593629":"### 5.4.3 Number of words in question_title in train and test ","d3e2e247":"#### Ploting answer_user behaviour based on answer","79cf1146":"### 5.3.2. Correlation between target labels","1541cc12":"#### for test_df\n","caba1cb1":"#### Defination:\n\n\"The Spearman's rank-order correlation is the nonparametric version of the Pearson product-moment correlation. Spearman's correlation coefficient, (\u03c1, also signified by rs) measures the strength and direction of association between two ranked variables.\"","33ce1ee9":"Observation: ","50118341":"## 5.8.4 answer_user behaviour based on answer body\n* How many number of words user usually answered in an answer? \n* What is the minimum number of words in answer answered by each user?\n* Similary \"max_title_len\" , \"mean_title_len\" , \"median_title_len\" ?","71b04961":"* All the plots of users behaviour on queston_body are heavely skewed toward right.\n* Transfrom these log normal alike features using box cox and check weather transformed features follows Noraml distribution or not using Q-Q plot","8553e281":"#### 5.6.4.1  Q-Q Plot of Box Cox transformed user_behaviour_column_on_queston_titile features","52deae3d":"### Plot (pie chart) for category distribution in train and test","c06412bc":"* The distribution of Categories is not similar in train set and test set\n* LIGE_ARTS has majority of distribution in train but CULTURE category holds majority of distribution in test set.","1c782552":"#### Ploting user behaviour based on queston_body","3c37692e":"Unfortunately, it\u2019s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That\u2019s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.\n\nIn this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!\n\nDemonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.","d5deb56c":"* PCA plot of targets using category feature is definately showing some patterns.\n* Category feature would definately be helpful in distinguish datapoints and regressing target values.","dd9ef762":"#### 5.7.2.1 Box plot of length of answers","c9bdbcfb":"### 5.5.1 Number of repeated question_body  in train and test","71b5eba7":"## 5.1. Overview of train_df","eeacb761":"#### But why Spearman's correlation coefficient?","7dfc9356":"### 5.7.1. Any same or repeated answer in train and test","f5a75b23":"### 5.10.2 Number of users who has asked question and answered by themself (by category) in train","324f0b57":"###  5.4.6 Common question_title in both train and test\n\nrefer: https:\/\/www.kaggle.com\/codename007\/start-from-here-quest-complete-eda-fe","f92255e3":"#### Sample Question","63f9bd41":"* Yes, It kinda look that Box Cox transfromed feature of  number of words in question_body roughly follows normal distribution\n","59e67570":"## 5.8.8. Common users who has asked the question and answered by himself in train_df and test_df","e0cd197e":"* It looks like box-cox transformed of n_words_in_answer follows normal distribution. So keep this feature and test this feature importance while modeling  ","663b0034":"#### For train_df","90c68cf4":"### 4.1 Dependent variables or Target Labels","9b83157a":"#### Note:\n\n* keep this in mind that \"Correlation is not necessary mean causation\". ","b2402e91":"### Special thanks to this kernal for below plots\nhttps:\/\/www.kaggle.com\/kabure\/qa-eda-and-nlp-modelling-insights-and-data-vis","c0c6d42d":"### 5.5.4 WordCloud of question_body in train and test ","736a52d2":"#### Observation\n\n* \n","4487b853":"## Saving all the transformed features and behaviour features into pickle file","a1490872":"### 4.10.1. Common number of Host in train and test","e7b016df":"### 5.4.0. Utility Fuctions","9f9967ef":"### 5.10.4 Title and Body Lenghts of questions by each category","2341b71e":"###  'Mean column-wise Spearman's correlation coefficient'","732fdb5f":"#### 5.8.2.3. Top 10 user who has answered most number of unique question","83eed355":"### 5.3.3 Plot of Target labels with high Correlation (either positive or negative)","ea5b166c":"* As I have not removed punctuations from question_title, most frequent words are punctuations. So we need to plot this graph again after removing punctuations from question_title for accurate word frequency plot.","bf3b6dd9":"#### Observations\n* There is around 6k datapoints in training data\n* There is no null values in any of the columns but there is some repeated questions present in data which is answered by different users ( clearly mentioned on Kaggle Competition page )","44c5c30b":"#### Common users who has asked question and answered by himself in train_df","ab038a72":"### 5.7.5. Frequency of most popular worlds in answer of train and test","81913779":"#### Description of number of unique question asked by user (test)","141689e8":"* The length of most of question_body in train and test is less than 2000 \n* the distribution length of question_body is highly skewed towards right indicates that there are few question_body which has very large length( > 2000) .\n* It looks like length of question_body follows log normal distribution. If yes, we can transform it to nomal distribution using Box-Cox transform and apply linear model on top of transformed feature for better result. "}}