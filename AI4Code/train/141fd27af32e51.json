{"cell_type":{"29aaf5b5":"code","79188e1e":"code","f42d7c3d":"code","ac01450b":"code","0e2d1d8f":"code","287ad1f3":"code","a2927e8f":"code","a08c5e28":"code","5fc4f59b":"code","1d1d3a83":"code","0484d404":"code","feab88b9":"code","a7ea7eae":"code","76641c82":"code","0cf01c02":"code","2451be2c":"code","618405e4":"code","914d776c":"code","6a633888":"code","ae46c467":"code","deeeda05":"code","367a0275":"code","da54b1e0":"code","1617f491":"markdown","7b4668d7":"markdown","74186e5e":"markdown","48f021d1":"markdown","b17f8fc3":"markdown","78be2e73":"markdown","7483011f":"markdown","433ecd7b":"markdown","2d1e4cff":"markdown","fe827a99":"markdown","aaa643b1":"markdown","e60c6c6c":"markdown","90f190fc":"markdown","1f22ee86":"markdown","fd87bac0":"markdown","305e82e4":"markdown","cefc8dca":"markdown","1eb37077":"markdown","0d0ecaa1":"markdown","1d29c125":"markdown","d18d32e6":"markdown","aeae8c29":"markdown","00dd4472":"markdown","60890525":"markdown","fc1ceca7":"markdown","cbe216e7":"markdown","81008855":"markdown","70dd86bb":"markdown","b819c65f":"markdown","7372717b":"markdown"},"source":{"29aaf5b5":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","79188e1e":"train_data = np.genfromtxt(\"..\/input\/digit-recognizer\/train.csv\",delimiter=',')\ntest_data = np.genfromtxt(\"..\/input\/digit-recognizer\/test.csv\",delimiter=',')\nsample_data = np.genfromtxt(\"..\/input\/digit-recognizer\/sample_submission.csv\",delimiter=',')","f42d7c3d":"print(\"train_data= \" , train_data.shape)\nprint(\"test_data= \" , test_data.shape)\nprint(\"sample_data= \" , sample_data.shape)","ac01450b":"print(\"train_data= \\n\",train_data,\"\\n\")\nprint(\"test_data= \\n\",test_data,\"\\n\")\nprint(\"sample_data= \\n\",sample_data,\"\\n\")","0e2d1d8f":"train_x = train_data[1::,1::]\ntrain_y = train_data[1::,0]","287ad1f3":"new_y=[]\nfor i in train_y:\n    a=np.zeros(10)\n    a[int(i)]=1\n    new_y.append(a)\ntrain_y=np.array(new_y)","a2927e8f":"print(\"train_x= \",train_x.shape)\nprint(\"train_y= \",train_y.shape)","a08c5e28":"evalu_x=train_x[:4200]\ntrain_x=train_x[4200:]\n\nevalu_y=train_y[:4200]\ntrain_y=train_y[4200:]\n\nprint(\"train_x= \",train_x.shape)\nprint(\"train_y= \",train_y.shape)\nprint(\"eval_x= \",evalu_x.shape)\nprint(\"eval_y= \",evalu_y.shape)","5fc4f59b":"test_data=test_data[1::]\nsample_data=sample_data[1::]","1d1d3a83":"image_to_show=12345 #We have 42000 images, so choose this value between 42000 and -42000 \n\nprint(\"Digit= \",train_y[image_to_show])\nplt.imshow(train_x[image_to_show].reshape(28,28))\nplt.show()","0484d404":"def mean_squared_error(y, y_hat):\n    return np.mean(np.power(y-y_hat,2))\n\ndef mean_squared_error_der(y,y_hat):\n    return (y-y_hat)","feab88b9":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))\n\ndef sigmoid_der(x):\n    return sigmoid(x)*(1-sigmoid(x))\n    ","a7ea7eae":"class dense_layer:\n    update_count=0\n    delta=0\n    \n    def __init__(self,input_shape,output_shape,is_last):\n        self.input_shape=input_shape\n        self.output_shape=output_shape\n        self.is_last=is_last\n        self.weights=np.random.random((self.input_shape,self.output_shape))-0.5\n        self.vw=np.zeros((self.input_shape,self.output_shape))\n        self.sw=np.zeros((self.input_shape,self.output_shape))\n        \n    def feed_forward(self,x):\n        self.input_values=x\n        self.output = sigmoid(np.dot(self.input_values,self.weights))\n        return self.output\n    \n    def backprop(self,expected=0,next_layer_gamma=0,next_layer_weights=0):\n        if self.is_last:\n            self.error=mean_squared_error_der(expected,self.output)\n        else:\n            self.error=np.dot(next_layer_gamma,next_layer_weights.T)\n        \n        self.gamma=self.error*sigmoid_der(self.output)\n          \n        self.delta+=np.dot(self.input_values.T,self.gamma)\n    \n    def update_weights(self):\n        self.update_count+=1\n        \n        self.sw=self.sw*beta1+self.delta*(1-beta1)\n        self.vw=self.vw*beta2+self.delta**2*(1-beta2)\n        \n        swc=self.sw\/(1-beta1**self.update_count)\n        vwc=self.vw\/(1-beta2**self.update_count)\n        \n        self.weights+=swc\/(np.power(vwc,1\/2)+epsilon)*lr\n        \n        self.delta=0","76641c82":"l1=dense_layer(784,392,False)\nl2=dense_layer(392,98,False)\nl3=dense_layer(98,49,False)\nl4=dense_layer(49,10,True)","0cf01c02":"beta1=0.99\nbeta2=0.999\nepsilon=000000000000.1\nlr=0.001\n\nbatch_size=512\nepochs=30","2451be2c":"def fit(x,y):\n    error_list=[]\n    for ep in range(epochs):\n        seen_points=0\n        error=0\n        \n        for i in range(x.shape[0]):\n            \n            o1=l1.feed_forward(x[i].reshape(1,-1))\n            o2=l2.feed_forward(o1)\n            o3=l3.feed_forward(o2)\n            o4=l4.feed_forward(o3)\n            \n            l4.backprop(y[i])\n            l3.backprop(next_layer_gamma=l4.gamma,next_layer_weights=l4.weights)\n            l2.backprop(next_layer_gamma=l3.gamma,next_layer_weights=l3.weights)\n            l1.backprop(next_layer_gamma=l2.gamma,next_layer_weights=l2.weights)\n            \n            error+=np.mean(l4.error**2)\n            \n            if seen_points%batch_size==0:\n                l4.update_weights()\n                l3.update_weights()\n                l2.update_weights()\n                l1.update_weights()\n                \n                #print(\"Epochs: \",ep+1,\"\/\",epochs,\" - Batches: \", i+1,\"\/\",x.shape[0])\n            \n            seen_points+=1\n            \n        error=error\/x.shape[0]\/batch_size\n        \n        error_list.append(error)\n        \n        print(\"Epochs: \",ep+1,\"\/\",epochs,\" - Error: \", error)\n        \n    return error_list","618405e4":"history=fit(train_x\/train_x.max(),train_y)","914d776c":"print(\"Error\")\nplt.plot(history)\nplt.show()","6a633888":"def prediction(x):\n    o1=l1.feed_forward(x)\n    o2=l2.feed_forward(o1)\n    o3=l3.feed_forward(o2)\n    o4=l4.feed_forward(o3)\n    return o4\n\ndef evaluate(x,y):\n    true=0\n    for i in range(x.shape[0]):\n        if np.argmax(prediction(x[i]))==np.argmax(y[i]):\n            true+=1\n    return true,x.shape[0]","ae46c467":"true,total=evaluate(evalu_x\/evalu_x.max(),evalu_y)\nprint(true\/total)","deeeda05":"show_id=300\n\nplt.imshow(train_x[show_id].reshape(28,28))\nprint(\"Prediction= \",np.argmax(prediction(train_x[show_id].reshape(1,-1)\/train_x.max())))\nprint(\"Real= \",np.argmax(train_y[show_id]))","367a0275":"submission=pd.DataFrame()\n\npreds=[]\nfor i in range(test_data.shape[0]):\n    preds.append(np.argmax(prediction(test_data[i]\/test_data.max())))\n\nsubmission[\"ImageId\"]=np.arange(len(preds))+1\nsubmission[\"Label\"]=preds\n\nsubmission","da54b1e0":"submission.to_csv(\"submission.csv\",index=False)","1617f491":"Firstly, I import required libraries for this notebook. Just **Numpy** for algebra, **Matplotlib** for visualization and **Pandas** for submit predictions as csv.","7b4668d7":"Let's look at the shape of **train**, **test** and **sample_submission** data.","74186e5e":"Last step, save this **DataFrame** as csv and submit it.","48f021d1":"Now start to make this.","b17f8fc3":"Now we define a fit method for run the methods above.","78be2e73":"We should divide our training data into evalu and training.","7483011f":"Finally we can start to make our network.","433ecd7b":"Thanks for reading. I will be glad if you point out my mistakes.","2d1e4cff":"Now, we should write a method that evaluate our network.","fe827a99":"And, we gotta delete nan rows from both **sample** and **test** data.","aaa643b1":"**train_x** values are our input values. What we want to do is design a neural network that maps these input values to **train_y** values.","e60c6c6c":"In **train_data** the first column is categories of digits. We must seperate this column to another variable. Except for the **nan** values above.\nIn fact this nan values is column names but **np.genfromtext** method can't read this headers.","90f190fc":"Our model is small, easy to calculate and enough for this task.\nLet' s look our model' s layers:\n\n1. InputLayer - 784 units - 0 parameter\n2. DenseLayer - 392 units - 307.328 parameters - Sigmoid activation\n3. DenseLayer - 98 units - 38.419 parameters - Sigmoid activation\n4. DenseLayer - 49 units - 4802 parameters - Sigmoid activation\n4. DenseLayer - 10 units - 490 parameters - Sigmoid activation\n\nOur models total parameter(weight) count is **347.224**. This model may seem a little small compared to others but it will be sufficient.\n\nWe use **Adam** algorithm as optimizer.\n\nOur loss function for this model is **Mean Squared Error**","1f22ee86":"Now, look at the shapes of new variable","fd87bac0":"Let' s define our **hyperparameters**. ","305e82e4":"# Model","cefc8dca":"Ok, we have a **train_y** variable that will tell us which output neuron to activate.\nWe must encode this arrays, integer to ten-sized arrays according to the integer value.\n\n**Example**\n* 5 >> 0,0,0,0,0,1,0,0,0,0\n* 8 >> 0,0,0,0,0,0,0,1,0,0","1eb37077":"Now, we must read our data to examine.","0d0ecaa1":"Now, we can make predictions on test value to submit.","1d29c125":"## Preparation","d18d32e6":"# Digit Recognizer With Numpy","aeae8c29":"Let's see our data as image. This will help us to recognize the data.","00dd4472":"Firstly, we should define some methods.","60890525":"Now, we can look out our real problem, recognize this digits.\nWe can design a Fully Connected Network for this job. Actuaally we never use **FCN**' s to recognize image data.\nBut this data is very small and we can calculate all weights easily.\nIf we have a dataset with big images(Like 256x256) we should use **CNN**(Convolutional Neural Network)' s.","fc1ceca7":"Error down, no problem showing.","cbe216e7":"To show image, we must reshape our data to 28x28","81008855":"Let' s look history of our network.","70dd86bb":"Our networks true\/total ratio is around **0.9** on unseen data.","b819c65f":"Ok, now we can build our model.","7372717b":"In this notebook I will show you how to make a fully connected digit recognizer. We use Adam algorithm for optimize network, mean squared error as a loss function."}}