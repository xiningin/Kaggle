{"cell_type":{"952fe890":"code","9d500d6c":"code","1926e17d":"code","07adc703":"code","5f92cc8d":"code","59fe2c72":"code","d699c7c0":"code","81ad1694":"code","85121929":"code","41a5e1c8":"code","39a4a609":"code","de429741":"code","818dfa85":"code","8b52713a":"code","0840ab3c":"code","fc3fce2d":"code","b832919e":"code","40d2665f":"code","1443f2da":"code","5b7d312a":"code","2e6b948c":"code","d5bbc0a2":"code","ff0320ee":"code","576e7b09":"code","a1ae83ca":"code","f9c0b3d6":"code","f9791a63":"code","83489d42":"code","95b94a33":"code","13d41016":"code","7b89e0ca":"code","0a979ac4":"markdown","0038bf41":"markdown","200a1a2a":"markdown","b08170b7":"markdown","201b9709":"markdown","21c95de5":"markdown","73793512":"markdown","2d70a285":"markdown","87a2430d":"markdown","4eaca405":"markdown","4883695c":"markdown","03716075":"markdown","8c5031f8":"markdown","e02532c0":"markdown","36a8c058":"markdown","de14c707":"markdown","f551def9":"markdown","602a6d70":"markdown","b8972682":"markdown","d6e992b2":"markdown","9f67e167":"markdown","3060867e":"markdown","b5056880":"markdown","39afc922":"markdown","000b86b3":"markdown","286486dd":"markdown","de178c36":"markdown","4abd0aeb":"markdown","fa107af8":"markdown","5337deea":"markdown","621802d6":"markdown","818134ad":"markdown","52350b0c":"markdown","57741b6c":"markdown","ff5a393b":"markdown","ecec2ef9":"markdown","235ed23d":"markdown","b25c0d5f":"markdown","8254bf13":"markdown","a83102f0":"markdown"},"source":{"952fe890":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sys\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom collections import Counter","9d500d6c":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}","1926e17d":"%config Completer.use_jedi = False\n\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\n\nnp.set_printoptions(threshold=sys.maxsize)\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n%matplotlib inline","07adc703":"df = pd.read_csv('..\/input\/portuguese-bank-marketing-data-set\/bank-full.csv',sep=';')\ndf.head()","5f92cc8d":"df.shape","59fe2c72":"#  Find missing values of each feature in the data set.\ndf.info()","d699c7c0":"df.describe().astype(np.int64)","81ad1694":"%matplotlib inline\ndf[['age','duration','campaign','previous']].hist(bins=30, figsize=(20,15))\nplt.savefig(\"attribute_histogram_plots\")\nplt.show()","85121929":"# Visualize feature correlations\nfig, ax = plt.subplots(figsize=(10,10))  \nsns.heatmap(df._get_numeric_data().astype(float).corr(),\n            square=True, cmap='RdBu_r', linewidths=.5,\n            annot=True, fmt='.2f').figure.tight_layout()\nplt.show()","41a5e1c8":"category_features = df.select_dtypes(include=['object', 'bool']).columns.values\n\nfor col in category_features:\n    print(col, \"(\", len(df[col].unique()) , \"values):\\n\", np.sort(df[col].unique()))","39a4a609":"for col in category_features:\n    print(f\"\\033[1m\\033[94m{col} \\n{20 * '-'}\\033[0m\")    \n    print(df[col].value_counts(), \"\\n\")\n    \nprint(df.nunique(axis=1))","de429741":"for col in category_features:\n    plt.figure(figsize=(10,5))    \n    sns.barplot(df[col].value_counts().values, df[col].value_counts().index, data=df)    \n    plt.title(col)    \n    plt.tight_layout()","818dfa85":"# Pie chart\nlabels = [\"Not \\nsubscribed\", \"Subscribed\"]\nexplode = (0, 0.1)  # only \"explode\" the second slice (i.e. 'Subscribed')\n\n# depicting the visualization \nfig = plt.figure() \nax = fig.add_axes([0,0,1,1]) \n\nax.pie(df['y'].value_counts(), \n       labels = labels,\n       explode = explode,\n       autopct ='%1.2f%%',\n       frame = True,\n       textprops = dict(color =\"black\", size=12)) \n\nax.axis('equal') \nplt.title('Subcription to the term deposit\\n% of Total Clients',\n     loc='left',\n     color = 'black', \n     fontsize = '18')\n\nplt.show()","8b52713a":"# We will groupby then count\ndf.groupby(['campaign'])['y'].count().reset_index().sort_values(by='y', ascending=False).iloc[:5]","0840ab3c":"table = pd.crosstab(df.job, df.y)\ntable.columns = ['Not subscribed', 'Subscribed']\ntable.plot(kind='bar')\n\nplt.grid(True)\n\nplt.title('Purchase Frequency for Job Title')\nplt.xlabel('Job')\nplt.ylabel('Frequency of Purchase')","fc3fce2d":"table = pd.crosstab(df.job, df.y)\ntable = round(table.div(table.sum(axis=1), axis=0).mul(100), 2)\ntable.columns=['notsubcribed', 'subcribed']\ntable.sort_values(by=['subcribed'], ascending=False).loc[:, 'subcribed']","b832919e":"table = pd.crosstab(df.marital,df.y)\ntable = table.div(table.sum(1).astype(float), axis=0)\ntable.columns = ['Not subscribed', 'Subscribed']\n# Ordering stacked bars and plot the chart\ntable[['Subscribed', 'Not subscribed']].plot(kind='bar', stacked=True)\nplt.title('Frequency of Marital Status vs Purchase')\nplt.xlabel('Marital Status')\nplt.ylabel('Proportion of Customers')","40d2665f":"df = df.drop(['duration'], axis=1)","1443f2da":"# load X and y\nX = df.drop(columns=['y'])\ny = df['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","5b7d312a":"numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.values\nnumeric_features = numeric_features[numeric_features != 'y']\n\ncategory_features = X_train.select_dtypes(include=['object', 'bool']).columns.values\n\nprint(numeric_features)\nprint(category_features)","2e6b948c":"def dummify(ohe, x, columns):\n    transformed_array = ohe.transform(x)\n\n    # list of category columns\n    enc = ohe.named_transformers_['cat'].named_steps['onehot']\n    feature_lst = enc.get_feature_names(category_features.tolist())   \n    \n    cat_colnames = np.concatenate([feature_lst]).tolist()\n    all_colnames = numeric_features.tolist() + cat_colnames \n    \n    # convert numpy array to dataframe\n    df = pd.DataFrame(transformed_array, index = x.index, columns = all_colnames)\n    \n    return transformed_array, df","d5bbc0a2":"# impute missing numerical values with a median value, then scale the values\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\n# impute missing categorical values using the 'missing' and one hot encode the categories\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Each transformer is a three-element tuple that defines \n#                                 the name of the transformer, \n#                                 the transform to apply, \n#                                 and the column features to apply it to\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, category_features)])\n\nohe = preprocessor.fit(X_train)\n\nX_train_t = ohe.transform(X_train)\nX_test_t = ohe.transform(X_test)","ff0320ee":"# transform training and test set and then convert it to dataframe\nX_train_t_array, X_train_t = dummify(ohe, X_train, category_features)\nX_test_t_array, X_test_t = dummify(ohe, X_test, category_features)\n\nX_train_t.head()","576e7b09":"X_train_columns = X_train_t.columns\nprint(X_train_columns)","a1ae83ca":"from imblearn.over_sampling import SMOTE\n\n# summarize class distribution\ncounter = Counter(y_train)\nprint(counter)\n\n# transform the dataset\noversample = SMOTE()\nX_train_smote, y_train = oversample.fit_resample(X_train_t, y_train)\n\n# summarize the new class distribution\ncounter = Counter(y_train)\nprint(counter)","f9c0b3d6":"from sklearn.svm import SVC\n\nfinal_X_train = pd.DataFrame(data=X_train_smote,columns=X_train_columns )\nfinal_y_train = pd.DataFrame(data=y_train,columns=['y'])\n\nrfe_model = RFE(LogisticRegression(solver='lbfgs', max_iter=1000), 25)\nrfe_model = rfe_model.fit(final_X_train, final_y_train)\n\n# feature selection\nprint(rfe_model.support_)\nprint(rfe_model.ranking_)","f9791a63":"selected_columns = X_train_columns[rfe_model.support_]\nprint(selected_columns.tolist())","83489d42":"X_train_final = final_X_train[selected_columns.tolist()]\ny_train_final = final_y_train['y']\nX_test_final = X_test_t[selected_columns.tolist()]\ny_test_final = y_test\n\nX_test_final.head()","95b94a33":"### Logistic Regression Model Fitting","13d41016":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train_final, y_train_final)","7b89e0ca":"y_pred = logreg.predict(X_test_final)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test_final, y_test_final)))","0a979ac4":"In this project, I will use OneHotEncoding for converting ordinal and categorical variables to numerical values. ","0038bf41":"SMOTE is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.","200a1a2a":"# Logistic Regression\n\nThis is my first project on Kaggle. Since I am new to this domain, I am sure that I am making a lot of mistakes. I am open to any constructive criticism.","b08170b7":"## I. About the dataset\n \nThe dataset comes from the UCI Machine Learning repository, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed. \n\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Bank+Marketing\n\nThe dataset is consisted of 45,211 customer data on direct marketing campaigns (phone calls) of a Portuguese banking institution, with variables below: \n+ Client: age, job, marital, education, default status, housing, and loan\n+ Campaign: last contact type, last contact month of year, last contact day of the week, and last contact duration\n+ Others: number of contacts performed in current campaign, number of days that passed by after the client was last contacted, number of contacts performed before this campaign, outcome of previous campaign, and whether a client has subscribed a term deposit.\n\nThe classification goal is to predict whether the client will subscribe (1\/0) to a term deposit (variable y).\n                \n#### 1. Title: Bank Marketing\n\n#### 2. Sources\n\nThe dataset is public available for research. The details are described in [Moro et al., 2011]. \n\n[Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. \n\nIn P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimar\u00e3es, Portugal, October, 2011. EUROSIS.\n\nAvailable at: [pdf] http:\/\/hdl.handle.net\/1822\/14838\n\n              [bib] http:\/\/www3.dsi.uminho.pt\/pcortez\/bib\/2011-esm-1.txt\n              \n\n#### 3. Number of Instances: 45211 for bank.csv\n\n#### 4. Number of Attributes: 17 output attributes.\n\n#### 5. Attribute information:\n\nInput variables\n\n### Bank client data\n\n1 - age (numeric)\n\n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unnon')\n\n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n\n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n\n5 - default: has credit in default? (categorical: 'no','yes','unknown')\n\n6.  balance\n\n7 - housing: TEMPhas housing loan? (categorical: 'no','yes','Unknown')\n\n8 - loan: TEMPhas personal loan? (categorical: 'no','yes','unknow')\n\n### Related wif the last contact of the current campaign\n\n9 - contact: contact communication type (categorical: 'cellular','telephone')\n\n10 - day: last contact day of teh week (categorical: 'mon','tue','wed','thu','fri')\n\n11 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n\n12 - duration: last contact duration, in seconds (numeric). Important note: dis attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not non before a call is performed. Also, after the end of the call y is obviously non. Thus, dis input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n### Other attributes\n\n13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n14 - pdays: number of days dat passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n\n15 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n16 - poutcome: outcome of teh previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n\n### Output variable (desired target):\n\n17 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n\n#### 6. Missing Attribute Values: None","201b9709":"### Category Data Distribution","21c95de5":"<h1>Table of contents<\/h1>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <ol>\n        <li><a href=\"#about_dataset\">About the dataset<\/a><\/li>\n        <li><a href=\"#business_problem\">Business Problem<\/a><\/li>\n        <li><a href=\"#business_problem\">Data Exploration<\/a><\/li>\n        <li><a href=\"#visualization_analysis\">Data Visualization and Analysis<\/a><\/li>\n        <li><a href=\"#classification\">Classification<\/a><\/li>\n    <\/ol>\n<\/div>\n<hr>","73793512":"The target customers are admins, blue-collars and techinicians but the frequency of students and retired people subscribed to the term deposit are pretty high (28.68% for students and 22.79% for retired people).","2d70a285":"## V. Classification","87a2430d":"### 1. Missing values\nLuckily, our dataset does not contain missing data. Hence, we can skip this step.","4eaca405":"Let's load required libraries","4883695c":"Now, we will print out the campaigns that the largest number of customers participate in","03716075":"As mentioned above, our data is imbalanced. We can see that in our dataset, the positive samples (minority class) are much less than negative samples (majority class). The positive samples (the people who subscribed to the term deposit) were only 11.27% from the total samples. Therefore, accuracy is no longer a good measure of performance because if we simply predict all examples to the negative class, we achieve 88,73% accuracy. As a result, we need to apply methods to overcome class imbalance problem. In this section, we use SMOTE method to balance our dataset.","8c5031f8":"RFE is a popular feature selection algorithm. It is easy to configure and pretty effective at selecting features in a training dataset. There are two important configuration options when using RFE: \n\n    1. The number of features to select.\n    \n    2. The choice of algorithm used to help choose features.","e02532c0":"### 5. Random Feature Elimination \u2013 RFE","36a8c058":"## IV. Data Visualization and Analysis","de14c707":"### Role of marital status in subscription behaviour","f551def9":"###  6. Logistic Regression Model Fitting","602a6d70":"### 4. Oversampling using SMOTE","b8972682":"11.27% customers subscribed to the term deposit. Our classes are imbalanced where positive values (subscribed) are only 11.27%. In the next section, we will balance the classes.\n","d6e992b2":"### What is the target audience?\n### Which customers were more likely to subscribe to the term deposit?","9f67e167":"Most of our features are category type; hence, this heatmap does not help much. We can see that duration is a good indicator, however, this value has only known when the call is done.","3060867e":"## III. Data Exploration","b5056880":"###  Subscription to the term deposit","39afc922":" We define a SMOTE instance with default parameters that will balance the minority class and then fit and apply it in one step to create a transformed version of our dataset.","000b86b3":"We start with the exploratory analysis of the categorical features by using seaborn package to plot histogram charts.","286486dd":"### To get a feel for the type of data we are dealing with, we visualize distributions of numerical features with histograms","de178c36":"#### There is no significant impact of marital status on subscription behaviour of customers.","4abd0aeb":"## II. Business Problem\n\nThere has been a revenue decline for the Portuguese bank and they would like to know what actions to take. After investigation, they found out that the root cause is that their clients are not depositing as frequently as before. Knowing that term deposits allow banks to hold onto a deposit for a specific amount of time, so banks can invest in higher gain financial products to make a profit. In addition, banks also hold better chance to persuade term deposit clients into buying other products such as funds or insurance to further increase their revenues. As a result, the Portuguese bank would like to identify existing clients that have higher chance to subscribe for a term deposit and focus marketing effort on such clients.\n\nTo resolve the proble, we suggest a classification approach to predict which clients are more likely to subscribe for term deposits.","fa107af8":"I am not sure why we don't have data for the month of January and February.","5337deea":"Here are steps we follow to preprocess our data:\n1. Dealing with missing values\n2. Splitting of data (80 : 20 split)\n3. Handling Categorical Variable\n4. Oversampling using SMOTE\n5. Random Feature Elimination \u2013 RFE\n6. Logistic Regression Model Fitting","621802d6":"Although \"duration\" feature highly affects the output target, this value is not known before a call is performed. Hence; this feature should been discarded from the list of features to predict.","818134ad":"Input values:    \n* Dataframe: X_train_t, y_train, X_test_t, y_test\n    \n* Array: X_train_t_array, X_test_t_array","52350b0c":"First, we classify features into two groups: numerical and categorical features:","57741b6c":"\nOur observations:\n1. Job: The audiences of these campaigns target mostly administrators, blue-collars, and technicians.\n2. Marital status: Most of them are married; married clients are twice as single people.\n3. Education: Most clients have university education level while illiterate people are very less.\n4. default\/credit: Most people have no default stay on their credit file.\n5. housing: Most people have no housing loan.\n6. loan: Most people have no personal loan.\n7. contact: Common means of communication are cellular.\n8. month - May is the busy month and December is the least busy month (because of the holidays season).\n9. day of week: Thursday is the most busy day while Friday is the least busy day of the week.","ff5a393b":"### Print unique values for each column","ecec2ef9":"### Top 5 of highly successful campaigns","235ed23d":"### 2. Splitting of data (80 : 20 split)","b25c0d5f":"### 3. Handling Categorical Variable","8254bf13":"References:\n\n1. <a href='https:\/\/medium.com\/@ashim.maity8\/predict-if-the-client-will-subscribe-a-term-deposit-or-not-using-machine-learning-c6e4024c7028'>Predict if the client will subscribe a term deposit or not, using \u201cMachine learning\u201d<\/a>\n\n2. <a href='https:\/\/github.com\/maityashim\/Machine-Learning-Project-on-Bank-Marketing-Data-Set\/blob\/master2\/Bank_Marketing.ipynb'>Machine Learning Project on Bank Marketing Data Set<\/a>\n\n3. <a href='https:\/\/towardsdatascience.com\/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8'>Building A Logistic Regression in Python, Step by Step<\/a>\n\n4. Data Cleaning, Feature Selection, and Data Transforms in Python - Jason Brownlee.\n\n5. https:\/\/www.roelpeters.be\/solve-shape-mismatch-if-categories-is-an-array-it-has-to-be-of-shape-onehotencoder\/\n\n6. https:\/\/machinelearningmastery.com\/columntransformer-for-numerical-and-categorical-data\/","a83102f0":"Here we split the data into training and test set so that we can fit and evaluate a learning model. We will use the train_test_split() function from scikit-learn and use 80 percent of the data for training and 20 percent for testing."}}