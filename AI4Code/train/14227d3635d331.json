{"cell_type":{"a9324af4":"code","f4161475":"code","bc963469":"code","df097444":"code","5b3b8b20":"code","b47706bd":"code","8e8d2359":"code","7cbad8be":"code","22b01f7c":"code","c77fddab":"code","a371b0c7":"code","356196dd":"code","b844551b":"code","3563b018":"code","52152549":"code","062c4965":"code","c85ede40":"code","937c76fa":"code","f4655690":"code","eb797b79":"code","3719ef44":"code","0961bb3e":"code","fef12a1f":"code","eb67f017":"code","0c1e323a":"code","85605525":"code","23ccfd27":"code","d3ec2183":"markdown","fa2ff0ab":"markdown","be098ad7":"markdown","41d239ef":"markdown","bc420104":"markdown","4ba04a29":"markdown","26890b1e":"markdown","ceaf9592":"markdown","80a9a25b":"markdown","a3f75e1d":"markdown","056d0d47":"markdown","65f111b9":"markdown"},"source":{"a9324af4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport math\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom collections import Counter\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom scipy import stats\nimport scipy.stats as stats\nimport pymc3 as pm\nimport arviz as az\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import cross_val_score\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB","f4161475":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","bc963469":"test_df.info()","df097444":"new_train_df = train_df.drop(columns=['Survived'], axis=1)\ntotal_df = pd.concat([new_train_df,test_df], sort=False, ignore_index=True)\ntotal_df.isnull().sum()","5b3b8b20":"total_df.head()","b47706bd":"total_df['Title'] = total_df.Name.str.extract('([A-Za-z]+)\\.',expand=False)\npd.crosstab(total_df['Title'],total_df['Sex'])","8e8d2359":"list1 = ['Master','Mr']\nlist2 = ['Miss','Mrs','Ms','Mlle']\n\ndef AgeHist1(list1,list2,dataset):\n    fig = plt.figure(figsize=(10,4))\n    title1 = []\n    title2 = []\n    \n    ax1 = fig.add_subplot(121)\n    for i in np.arange(len(list1)):\n        ax1 = sns.distplot(dataset['Age'].loc[dataset['Title']==list1[i]].dropna(),kde=False)\n        ax1.set_ylabel('Counts')\n        ax1.set_xlabel('Age')\n        title1.append(list1[i])\n    ax1.legend(labels=title1,loc='upper right',fontsize='small')\n    ax2 = fig.add_subplot(122)\n    for i in np.arange(len(list2)):\n        ax2 = sns.distplot(dataset['Age'].loc[dataset['Title']==list2[i]].dropna(),kde=False)\n        ax2.set_ylabel('Counts')\n        ax2.set_xlabel('Age')\n        title2.append(list2[i])\n    ax2.legend(labels=title2,loc='upper right',fontsize='small')\n        \nAgeHist1(list1,list2,total_df)","7cbad8be":"total_df['Title'] = total_df.Name.str.extract('([A-Za-z]+)\\.',expand=False)\ntotal_df['Title'] = total_df['Title'].replace(['Rev','Dr','Sir','Major','Countess'],'Special')\ntotal_df['Title'] = total_df['Title'].replace(['Mlle','Miss','Lady'],'Miss')\ntotal_df['Title'] = total_df['Title'].replace(['Mme','Ms','Mrs'],'Mrs')\ntotal_df['Title'] = total_df['Title'].replace(['Col','Don','Dona','Jonkheer','Capt'],'The others')\n\npd.crosstab(total_df['Title'],total_df['Sex'])","22b01f7c":"pd.crosstab(total_df['Title'],total_df['Age'].isna())","c77fddab":"list = ['Master','Miss','Mr','Mrs']\n\ndef AgeHist2(list,dataset):\n    fig = plt.figure(figsize=(10,6))\n    \n    for i in np.arange(len(list)):\n        \n        plt.subplot(math.ceil(len(list)\/2),2,i+1)\n        ax = sns.distplot(dataset['Age'].loc[dataset['Title']==list[i]].dropna(),kde=False)\n        median = dataset['Age'].loc[dataset['Title']==list[i]].dropna().median()\n        mean = dataset['Age'].loc[dataset['Title']==list[i]].dropna().mean()\n        ax.axvline(mean, color='r', linestyle='--')\n        ax.axvline(median, color='g', linestyle='--')\n        ax.set_title('{}'.format(list[i]))\n        plt.legend(labels=['mean','median'],loc='upper right',fontsize='small')\n        plt.subplots_adjust(wspace=0.5, hspace=0.8)\n        \nAgeHist2(list,total_df)","a371b0c7":"def AgeBayesPredictor(title,dataset):\n    \n    missing = dataset['Age'].loc[dataset['Title']== title].isnull().sum()\n    \n    def AgeExtractor(title,dataset):\n    \n        Age = dataset['Age'].loc[dataset['Title']==title]\n        Age = Age.dropna()\n        \n        return Age\n    \n    Age = AgeExtractor(title,dataset)\n    \n\n    with pm.Model() as model:\n        \n        upper = max(Age)-(1\/4)*(max(Age)-min(Age))\n        lower = min(Age)+(1\/4)*(max(Age)-min(Age))\n    \n        #Set the prior\n        mu = pm.Uniform('mu',upper = upper ,lower= lower)\n        sigma = pm.HalfNormal('sigma',sd=10)\n        \n    \n        #Liklihood\n        observed = pm.Gamma('obs', mu=mu,sigma=sigma,observed=Age)\n        \n        \n    with model:\n        \n        start = pm.find_MAP()\n        \n        #Trace\n        trace = pm.sample(8000, start=start)\n    \n    #Sampling\n    sampling = pm.sample_ppc(trace[1000:], model=model,samples=missing)\n    sampling=[random.choice(sampling['obs'][i]) for i in np.arange(start=0, stop=missing)]\n    return sampling\n\ndef imputeAge(title,dataset):\n    \n    for i in title:\n        \n        imputing_Age = AgeBayesPredictor(i,dataset)\n        idx = dataset['Age'].loc[dataset['Title']==i].isnull()\n        missing = dataset['Age'].loc[dataset['Title']==i][idx]\n        \n        for j in np.arange(len(missing)):\n            missing.iloc[j] = imputing_Age[j]\n\n        dataset.update(missing)\n    return dataset\n\ntotal_df = imputeAge(['Master','Miss','Mr','Mrs'],total_df)","356196dd":"list = ['Master','Miss','Mr','Mrs']\nAgeHist2(list,total_df)","b844551b":"train_Cabin_df = train_df.dropna(subset=[\"Cabin\"])\n#Cabin grouped by Initial Alphabats\ndef CategorizeCabin(data):\n    \n    for i in ['A','B','C','D','E','F','G','T']:\n        Index = data[\"Cabin\"].str.find(i)==0\n        \n        data[\"Cabin\"][Index] = i\n    \n    return data\n\ntrain_Cabin_df=CategorizeCabin(train_Cabin_df)\ntotal_df=CategorizeCabin(total_df)\ntrain_Cabin_df[\"Cabin\"].unique()\n\n#Update train data\ntrain_df.update(train_Cabin_df)","3563b018":"fig = plt.plot()\nsns.countplot(data=train_Cabin_df, x = \"Pclass\",hue =\"Cabin\")\nplt.show()","52152549":"fig = plt.figure\nsns.countplot(data=train_Cabin_df, x =\"Cabin\", hue=\"Survived\")\nplt.show()\nsns.barplot( x =\"Cabin\", y=\"Pclass\",data=train_Cabin_df)\nplt.show()\nsns.countplot(data=train_Cabin_df, x =\"Pclass\", hue=\"Survived\")\nplt.show()\n#sns.countplot(data=train_df, x =\"Pclass\", hue=\"Survived\")\n#plt.show()","062c4965":"corr = total_df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)]= True\n\ncmap = sns.diverging_palette(220,10,as_cmap = True)\n\n\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","c85ede40":"#Fill missing NaN\ntotal_df['Fare'] = total_df['Fare'].fillna(total_df['Fare'][total_df['Pclass']==3].mean())\ntotal_df['Age'] = total_df['Age'].fillna(total_df['Age'][total_df['Title']=='Mr'].mean())\ntotal_df['Embarked'] = total_df['Embarked'].fillna('S')","937c76fa":"total_df.isnull().sum()","f4655690":"bins = np.linspace(min(total_df['Age'])-1,max(total_df['Age'])+1,num=6)\nlabels = ['Kid','Young Adult','Adult','Older Adult','Senior']\ntotal_df['AgeGroup'] = pd.cut(total_df['Age'], bins=bins, labels=labels, right=False)","eb797b79":"pd.crosstab(total_df['AgeGroup'],total_df['Title'])","3719ef44":"features_drop = ['PassengerId','Name', 'Ticket', 'Parch','Cabin','Age','Fare']\nfeatures = total_df.drop(features_drop, axis=1)\nfeatures.head()","0961bb3e":"#one-hot encoding\nfeatures = pd.get_dummies(features)\n#separate train and label\ntrain_label = train_df['Survived']\ntrain_data = features.head(len(train_df))\ntest_data = features.tail(len(test_df))\ntrain_data.head()","fef12a1f":"train_data, train_label = shuffle(train_data, train_label, random_state = 5)","eb67f017":"def train_and_test(model):\n    model.fit(train_data, train_label)\n    prediction = model.predict(test_data)\n    accuracy = round(model.score(train_data, train_label) * 100, 2)\n    print(\"Accuracy : \", accuracy, \"%\")\n    return prediction","0c1e323a":"# Logistic Regression\nlog_pred = train_and_test(LogisticRegression())\n# SVM\nsvm_pred = train_and_test(SVC())\n#kNN\nknn_pred_4 = train_and_test(KNeighborsClassifier(n_neighbors = 4))\n# Random Forest\nrf_pred = train_and_test(RandomForestClassifier(n_estimators=50))\n# Navie Bayes\nnb_pred = train_and_test(GaussianNB())","85605525":"#Create a  DataFrame with the passengers ids and our prediction regarding whether they survived or not\nsubmission2 = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':rf_pred.astype(int)})\n\n#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'Titanic Predictions2.csv'\n\nsubmission2.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","23ccfd27":"cverror = []\ntrerror = []\nfor i in np.arange(5, 105, 5):\n    clf = RandomForestClassifier(n_estimators=i)\n    clf.fit(train_data, train_label)\n    error1 = cross_val_score(clf,train_data,train_label, cv=5).mean()\n    error2 = clf.score(train_data, train_label)\n    cverror.append(1-error1)\n    trerror.append(1-error2)\ncverror = pd.DataFrame(cverror)\ncverror.columns = [\"cv-error\"]\ncverror[\"train-error\"] = trerror\nax1=sns.lineplot(data=cverror)\nax1.set_title(\"5-fold cross validation\")","d3ec2183":"I grouped the titles by 6 categories. Particularly, ('Mlle','Miss','Lady') = Mr(Unmarried) and ('Mme','Ms','Mrs') = Mrs(Married) together. I know Ms is for woman who don't like to mention whether married or not, but grouped it with Mrs category anyways. :)","fa2ff0ab":"I will fill out trivial missing values below","be098ad7":"After dropping passengers with missing cabins, I grouped the cabins by initial alphabats(For example, A12 = A, D22 = D and so on), and then I draw a conter plot to see the cabins assgined for each classes, and There is significant data imbalnce between Pclass 1 and Plcass2,3. I roughly assume that Pclass1 tend to be A, B, C and D Cabin and Pclass2 or 3 are tend to be E, F and G Cabin. As long as keeping in mind that I do not have enough cases from plcass2 and 3, this could be used as potential indicator to infer missing cabins for each classes.","41d239ef":"This is a distribution table after imputing age for each titles, and distribution does not change much as I expect.","bc420104":"Since I catched that title is in relation to the age,; For example, Master is title given to boys and young men, and Miss is given to unmarried woman so that they tend to be younger;so I grouped the passengers by their title and see distributions repectively.\n\nAnd you can check that there are meaningful differences between the titles.","4ba04a29":"Passengers who use B, C, D and E are more likely to be survived, and these are mostly used by pclass1 passengers as we checked. This means that passengers(Pclass1) who use these cabins are tend to be survived than passengers who use F,G cabins which are used by Pclass3 mostly.\nThere is also an exceptional cabin ,that is, E that all classes use. Survival rate is significantly high when they use E. There might be some information about this, but I will look into further.\nI think imputing missing Cabin is the one of important steps to achieve high score on this competion after my experince to try to fill out it. :)","26890b1e":"As you should see, There are postive correlation between Parch-Fare, Age-Fare, SibSp-Fare, and especially SibSp-Parch. This seems to be interpreted as passengers who have many Parch tend to get on a ship with their SibSp possibly. and negative correlation between Pclass-Fare, Pclass-Age and so on...\nThses result seems natural to me. For example, Passengers from pclass1 pay more than thoes from Pclass3 and 2.\nSince SibSp and Parch are highly correlated each other, I can use one of them and also for Pclass and Fare.","ceaf9592":"I dropped some features which are not necessary for train and prediction.","80a9a25b":"I tried to apply bayesian prediction by inferring posterior distribution from the dataset. First, I assume that the liklihood follows Gamma distribution because age is non-negative. It is always matter to choose proper prior because it affects posterior distribution. I have seen that there is no completely correct answer about this. :P Here I choose $\\mu$ drawn from uniform distribution and $\\sigma$ drawn from Halfnormal distribution with $10$ standard deviation.\n\n$$\\mu \\sim Uniform(\\min{Age}+\\frac{\\max{(Age|Title)}-\\min{(Age|Title)}}{4}, \\max{(Age|Title)}-\\frac{\\max{(Age|Title)}-\\min{(Age|Title)}}{4})$$<br>\n$$\\sigma \\sim Halfnormal(\\sigma = 10)$$<br>\nLikehood is $P((Age|Title)|\\mu,\\sigma)$\nYou might know that Gamma distribution has two parameters known as $\\alpha$ and $\\beta$.<br>\nThis can be calculated from $\\mu$ and $\\sigma$, and Gamma distribution from pymc3 allows to use these two parameters as the parameters for Gamma distribution, so I will use it.\n$$likelihood : Gamma(mu = \\mu, sigma = \\sigma, Observation = (Age|Title))$$\n\nSo the final goal is to find the optimal parameters $\\mu$ and $\\sigma$, and then we can draw a posterior gamma distribution from these parameters and sampling from it!\n\nSince I catched that title is in relation to the age,; For example, Master is title given to boys and young men, and Miss is given to unmarried woman so that they are younger; I grouped the passengers by their title and inferred thier age distributions respectively.","a3f75e1d":"Random Forest shows the highest score, so I will choose it for my prediction finally.","056d0d47":"This is a indicator table where the number of True means there are missing values with that amount.","65f111b9":"To check if it is overfitted, I did 5-fold cross validation.\nWe see that gaps between validation errors and train errors do not diverge. This means that the model is not overfitted inside of the train set at least. WE HAVE TO BE CAREFUL THAT IT IS NOT ALWAYS GOOD TO HAVE NEARLY PERFECT SCORE ON THE TRAIN SET BECAUSE IT MIGHT BE OVERFITTED TO THE TRAIN SET , SO IT HARMS TO PREDICT THE TEST SET."}}