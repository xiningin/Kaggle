{"cell_type":{"c200f9ba":"code","52563d81":"code","d11b2383":"code","4c3d0014":"code","0a596894":"code","c59c6dbe":"code","c4a3b2ae":"code","7f766052":"code","5609543f":"code","a8455309":"code","d0f3eeea":"code","644785b5":"code","3efd7312":"code","3c463d75":"code","2da4cf03":"code","e8be7ab9":"code","ba059fed":"code","cd749b44":"code","de3d73c1":"code","62e1d615":"code","672657d7":"code","45a6944e":"code","cf1aee6d":"code","71af0c5c":"code","3771e484":"code","cb13188a":"code","0557b656":"code","3cabed51":"code","e5d2bac4":"code","75cb8092":"code","a19a7da5":"code","c8b99f6a":"code","be90d9b2":"code","4087983d":"code","b356447f":"code","41f80d90":"code","0766b835":"code","567acb3a":"code","74436ef3":"code","808933b5":"code","a8599838":"code","8938b290":"code","5afc0249":"markdown","f71a10cd":"markdown","d231e231":"markdown","d75e386b":"markdown","72ea81bf":"markdown","ed7ba0f7":"markdown","87e27d13":"markdown","f69c1498":"markdown"},"source":{"c200f9ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np\nimport pandas as pd\n\n\n#Modeling Algorithms\nfrom sklearn import linear_model, model_selection, ensemble, preprocessing\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nfrom sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler, LabelEncoder\n\n# Evaluation Metrics\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, r2_score, mean_absolute_error\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport missingno as msno\nfrom yellowbrick.regressor import RegressionScoreVisualizer, PredictionError, ResidualsPlot\n\n#Configure Visualizations\n%matplotlib inline\nmpl.style.use('ggplot')\nplt.style.use('fivethirtyeight')\nsns.set(context='notebook',palette='dark',style='whitegrid',color_codes=True)\nparams={\n    'axes.labelsize':'large',\n    'xtick.labelsize':'large',\n    'legend.fontsize':20,\n    'figure.dpi':150,\n    'figure.figsize':[25,7]\n}\n\nplt.rcParams.update(params)\n\n#Centre all plots\nfrom IPython.core.display import HTML\nHTML('''\n    <style>\n        .output.png{\n            display:table-cell;\n            text-align: centre;\n            vertical-align:middle;\n        }\n    <\/style>\n''')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","52563d81":"train_df = pd.read_csv('..\/input\/train.csv',index_col=0)","d11b2383":"train_df.head()","4c3d0014":"train_df.info()","0a596894":"train_df.describe()","c59c6dbe":"train_df.isnull().sum()","c4a3b2ae":"_ = msno.bar(train_df)","7f766052":"new_train_df = train_df.drop(['Alley','PoolQC','Fence','MiscFeature','FireplaceQu'],axis=1,inplace=False)\nnew_train_df.info()","5609543f":"fig ,ax = plt.subplots(figsize=(20,10))\n_ = sns.boxplot(data=new_train_df,ax=ax)","a8455309":"corr = new_train_df.corr()\ncorr","d0f3eeea":"fig, ax = plt.subplots(figsize=(20,10))\n_= sns.heatmap(corr.iloc[:,1:10],annot=True,ax=ax)","644785b5":"fig, ax = plt.subplots(figsize=(20,10))\n_= sns.heatmap(corr.iloc[:,11:20],annot=True,ax=ax)","3efd7312":"fig, ax = plt.subplots(figsize=(20,10))\n_= sns.heatmap(corr.iloc[:,21:30],annot=True,ax=ax)","3c463d75":"fig, ax = plt.subplots(figsize=(20,10))\n_= sns.heatmap(corr.iloc[:,30:36],annot=True,ax=ax)","2da4cf03":"X_train = train_df.drop(['SalePrice','Alley','PoolQC','Fence','MiscFeature','FireplaceQu'],axis=1)\ny_train = train_df['SalePrice']\n\ntest_df = pd.read_csv('..\/input\/test.csv',index_col=0)\nX_test  = test_df.drop(['Alley','PoolQC','Fence','MiscFeature','FireplaceQu'],axis=1,inplace=False)\n","e8be7ab9":"def getItemsNotInTest(train,test):\n    cat_vars = getColumns(train,False)\n    trainCatValues = set()\n    testCatValues  = set()\n    for c in cat_vars:\n        trainKeys= train[c].value_counts().to_dict().keys()\n        [trainCatValues.add(v) for v in trainKeys ]\n\n        testKeys = test[c].value_counts().to_dict().keys()\n        [testCatValues.add(v) for v in testKeys]\n    \n    finalSet = set()\n    for item in trainCatValues:\n        if item not in testCatValues:\n            finalSet.add(item)\n    return finalSet","ba059fed":"#Simple function to show descriptive stats on the categorical variables\n\ndef describe_categorical(X):\n    '''\n     Just like describe but returns the results for categorical variables only\n    '''\n    from IPython.display import display, HTML\n    display(HTML(X[X.columns[X.dtypes == \"object\"]].describe().to_html()))","cd749b44":"describe_categorical(X_train)","de3d73c1":"\ndef getColumns(X,isNumeric=True):\n    ''' \n        Return the Numeric or Categorical columns list\n    '''\n    if(isNumeric):\n        return list(X_train.dtypes[X_train.dtypes != np.dtype('O')].index)\n    else:\n        return list(X_train.dtypes[X_train.dtypes == np.dtype('O')].index)\n","62e1d615":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# A class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","672657d7":"# Inspired from stackoverflow.com\/questions\/25239958\nclass MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)","45a6944e":"class CustomOneHotEncoder(BaseEstimator, TransformerMixin):\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X,y=None):\n        for variable in X.columns:\n            #Fill missing data with the word \"Missing\"\n            X[variable].fillna(\"Missing\",inplace=True)\n            #Create  array of dummies\n            dummies = pd.get_dummies(X[variable],prefix=variable)\n            #Update X to include dummies and drop the main variables\n            X= pd.concat([X,dummies],axis=1)\n            X.drop([variable],axis=1,inplace=True)\n        return X\n","cf1aee6d":"class ConvertToPandasDF(BaseEstimator,TransformerMixin):\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X,y=None):\n        return pd.DataFrame(X)\n    ","71af0c5c":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('select_numeric',DataFrameSelector(getColumns(X_train,True))),\n        ('imputer', Imputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n        ('convertTo_DF',ConvertToPandasDF())\n    ])\n\nnum_pipeline.fit_transform(X_train).info()","3771e484":"cat_pipeline = Pipeline([\n        ('select_cat',DataFrameSelector(getColumns(X_train,False))),\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\",CustomOneHotEncoder())\n    ])\n\ncat_pipeline.fit_transform(X_train)","cb13188a":"from sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", cat_pipeline)\n    ])\n\nX_train_transformed = pd.DataFrame(preprocess_pipeline.fit_transform(X_train))","0557b656":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","3cabed51":"lr_model = LinearRegression(n_jobs=-1)\nlr_model.fit(X_train_transformed,y_train)\n\nlin_scores = cross_val_score(estimator=lr_model,\n                            X=X_train_transformed,\n                            y=y_train,\n                            cv=5,\n                            scoring=\"neg_mean_squared_error\",\n                            verbose=2)\n\n\n\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n\n","e5d2bac4":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(X_train_transformed, y_train)\n\ntree_scores = cross_val_score(estimator=lr_model,\n                            X=X_train_transformed,\n                            y=y_train,\n                            cv=5,\n                            scoring=\"neg_mean_squared_error\",\n                            verbose=2)\n\n\n\ntree_rmse = np.sqrt(-tree_scores)\ndisplay_scores(tree_rmse)","75cb8092":"num_pipeline_tree = Pipeline([\n        ('select_numeric',DataFrameSelector(getColumns(X_train,True))),\n        ('imputer', Imputer(strategy=\"median\")),\n        ('convertTo_DF',ConvertToPandasDF())\n    ])\ncat_pipeline_tree = Pipeline([\n        ('select_cat',DataFrameSelector(getColumns(X_train,False))),\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\",CustomOneHotEncoder())\n    ])\n\npreprocess_pipeline_tree = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline_tree),\n        (\"cat_pipeline\", cat_pipeline_tree)\n    ])\n\nX_train_tree = pd.DataFrame(preprocess_pipeline_tree.fit_transform(X_train))\n\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(X_train_tree, y_train)\n\ntree_scores = cross_val_score(estimator=lr_model,\n                            X=X_train_tree,\n                            y=y_train,\n                            cv=5,\n                            scoring=\"neg_mean_squared_error\",\n                            verbose=2)\n\n\n\ntree_rmse = np.sqrt(-tree_scores)\ndisplay_scores(tree_rmse)","a19a7da5":"forest_reg = RandomForestRegressor(random_state=42)\nforest_reg.fit(X_train_transformed, y_train)\n\nforest_scores = cross_val_score(forest_reg, X_train_transformed, y_train,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","c8b99f6a":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [5,10, 30,50,70,74], 'max_features': [2, 4, 6, 8,12,15,18]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [5,10, 30,50,70], 'max_features': [2, 4, 6, 8,12,15,18]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(X_train_transformed, y_train)","be90d9b2":"grid_search.best_params_","4087983d":"grid_search.best_estimator_","b356447f":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","41f80d90":"pd.DataFrame(grid_search.cv_results_)","0766b835":"\n\n#Simple version that show all of the variables\nfeature_importances = pd.Series(grid_search.best_estimator_.feature_importances_[1:10],index=X_train.columns[1:10])\nfeature_importances.sort_values(ascending=True,inplace=True)\nfeature_importances[1:10].plot(kind=\"barh\",figsize=(7,6));","567acb3a":"X_test.info()\n#X_train.info()","74436ef3":"num_pipeline_test = Pipeline([\n        ('select_numeric',DataFrameSelector(getColumns(X_test,True))),\n        ('imputer', Imputer(strategy=\"median\")),\n        ('convertTo_DF',ConvertToPandasDF())\n    ])\ncat_pipeline_test = Pipeline([\n        ('select_cat',DataFrameSelector(getColumns(X_test,False))),\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\",CustomOneHotEncoder())\n    ])\n\npreprocess_pipeline_test = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline_test),\n        (\"cat_pipeline\", cat_pipeline_test)\n    ])\n\nX_test_transformed = pd.DataFrame(preprocess_pipeline_test.fit_transform(X_test))\n\ngrid_search.best_estimator_.predict(X_test_transformed)","808933b5":"pd.DataFrame(pd.concat([pd.Series(X_train.columns),pd.Series(X_test.columns)],axis=1))","a8599838":"for c in getColumns(X_train,False):\n    print(X_train[c].value_counts())","8938b290":"getItemsNotInTest(X_train,X_test)","5afc0249":"<b> I am interested in looking into the correlation coefficient of each variable with SalePrice just to get an insight into how it can affect the SalePrice.","f71a10cd":"# Examine the correlations","d231e231":"# Feature Scaling","d75e386b":"# Modeling","72ea81bf":"# Load Data","ed7ba0f7":"\n **<ul>\n    <li> A lot of missing values for the following variables:\n        <ul>\n            <li>Alley\n             <li>PoolQC\n              <li>Fence\n               <li>MiscFeaure.\n            <\/ul>\n                It will make sense to drop these from the regression models as these variables cannot be imputed and most of the values are missing.\n    <li> Almost half of the values are missing for Fireplaces**\n        <\/ul>\n     ","87e27d13":"# Examine NaN values","f69c1498":"<b>Following variables have a low correlation with SalePrice\n\n  <ul>\n        <li>3SsnPorch<\/li>\n        <li>ScreenPorch\n        <li>PoolArea\n        <li>MiscVal\n        <li>MoSold\n        <li>YearSold\n        <li>KitchenAbvGr\n        <li>EnclosedPorch\n        <li>LowQualFinSF\n        <li>BsmtHalfBath\n        <li>BsmtFinSF2\n        <li>OverallCond\n    <\/ul>\n    "}}