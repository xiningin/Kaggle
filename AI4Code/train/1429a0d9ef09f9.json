{"cell_type":{"b3276ed3":"code","03c5945d":"code","ff9cc132":"code","37e9d960":"code","8d54162d":"code","94a6c7ed":"code","2af5cdb6":"code","9d1c91b5":"code","fec9b350":"code","3d057a3d":"code","6ce9d997":"code","56b3cc57":"code","db083018":"code","1dddb6fb":"code","0aff1b46":"code","751c97fe":"code","8fd9139d":"code","9ee51975":"code","43662d20":"code","33c62d4d":"code","b2be44d8":"code","617f15ff":"code","120c2b85":"code","b92ec61c":"code","97f42074":"code","4590a6e5":"code","86e4d8fc":"code","61647ed3":"code","a13bbd6a":"code","d756182d":"code","36547e4e":"code","a428775a":"code","14e0868a":"code","b23da891":"code","548a9e00":"code","cc6ae37f":"code","7848571e":"code","1a751f79":"code","f0fa152d":"code","45082c84":"code","1e699058":"code","12a57b2b":"code","91bf6d95":"code","cdb556ff":"code","1bf55566":"code","4fbdbdcd":"code","baa9804f":"code","eed5c750":"code","62d26353":"code","808cd6b3":"code","d6b4dd3e":"code","34553c55":"code","1bf73203":"code","6b5bb717":"code","06b86d63":"code","261d76aa":"code","ac30bf07":"code","47117956":"code","8db3dfc7":"code","9334b977":"code","32f5cfa4":"code","af59b228":"code","58baec23":"code","8c4620c8":"code","38b67675":"code","0dac2ab7":"code","7a9dd0d5":"code","c3d29f14":"code","08f01460":"code","d06db4d2":"code","dc35161d":"code","96f69e27":"code","ba9e9380":"code","b4a55e5f":"code","6311c186":"code","a63fc845":"code","ebc0ac81":"code","b179013d":"code","2ec88de4":"code","08289154":"code","3cb7cd70":"code","d6c89d09":"code","248a4962":"code","85049603":"code","f82869f9":"code","3a764dfe":"code","f9bcd13a":"code","c70fd469":"code","af4aad57":"code","46f0460d":"code","5dd152a3":"code","bf85c1eb":"code","1c6ab419":"code","fff24b30":"code","6d49d862":"code","a3d3b0f4":"code","9590fc55":"code","dc73a445":"code","52104cf3":"code","be55fddb":"code","c129f9ee":"code","a0183a9b":"code","38f65b41":"markdown","3f7e60a8":"markdown","d8f7ad6a":"markdown","1037fcaa":"markdown","419f5659":"markdown","819e8f4c":"markdown","b1759e0f":"markdown","5908a24c":"markdown","c109f02a":"markdown","e6f14807":"markdown","c674a891":"markdown","7aa0cf5d":"markdown","8eedf7d0":"markdown","c8c12805":"markdown","98e12e53":"markdown","c72719eb":"markdown","b4157d5d":"markdown","8daa26dc":"markdown","b98fe1b6":"markdown","25e5339c":"markdown","156b6d68":"markdown","9ed84fed":"markdown","64122bf5":"markdown","c06fc72e":"markdown","16a2df8c":"markdown","19d57e50":"markdown","853de84c":"markdown","8ea0054d":"markdown","98579fbc":"markdown","67abafc0":"markdown","256a85d8":"markdown","4c601580":"markdown","d06265d2":"markdown","98b99a99":"markdown","a73fc3d2":"markdown","7934953d":"markdown","bc8ee0e6":"markdown","2e304565":"markdown","f17277e5":"markdown","81a5af68":"markdown","66de5a82":"markdown","d40b3f53":"markdown","91a43983":"markdown","91c7a2e2":"markdown","e72e2a18":"markdown","abe18dee":"markdown","6b9d8994":"markdown","8341bfc4":"markdown","f7ea4b71":"markdown","470e7d56":"markdown","8a16d376":"markdown","390725ea":"markdown","3024c137":"markdown","1bae391b":"markdown","93f6c40f":"markdown","e7e5e3e0":"markdown","48c17f54":"markdown","04be2673":"markdown","4264e218":"markdown","109bb4dd":"markdown","b18b2f38":"markdown","2e33576d":"markdown","8266c9f4":"markdown","e0f7a4c2":"markdown","4a5428f7":"markdown","22e4dcf6":"markdown","dbf63f69":"markdown","9c1fc84c":"markdown","95a6a684":"markdown","5457372b":"markdown","3cd6b530":"markdown","273f806a":"markdown","00ac4bcb":"markdown","499215c3":"markdown","d248273a":"markdown","40eedfca":"markdown"},"source":{"b3276ed3":"import pandas as pd\ndf=pd.read_csv('..\/input\/titanic\/train.csv')\ndf1=pd.read_csv('..\/input\/titanic\/test.csv')\n\ndf","03c5945d":"df1","ff9cc132":"df.isnull().sum()","37e9d960":"df1.isnull().sum()","8d54162d":"for n in df['Name']:\n  if n[-5:] == 'James':\n    print(n)","94a6c7ed":"for n in df1['Name']:\n  if n[-5:] == 'James':\n    print(n)","2af5cdb6":"first_name=pd.DataFrame()\nlast_name=pd.DataFrame()\n\nfor a in df['Name']:\n  b,c = a.split(',')\n  first_name=first_name.append([b])\n  last_name=last_name.append([c])","9d1c91b5":"first_name","fec9b350":"last_name","3d057a3d":"first_name1=pd.DataFrame()\nlast_name1=pd.DataFrame()\n\nfor a in df1['Name']:\n  b,c = a.split(',')\n  first_name1=first_name1.append([b])\n  last_name1=last_name1.append([c])","6ce9d997":"first_name1","56b3cc57":"last_name1","db083018":"for aa in first_name1[0].unique():\n  if (aa in first_name[0].unique()) == True:\n    print(aa)","1dddb6fb":"for aa in first_name1[0].unique():\n  if (aa in first_name[0].unique()) == False:\n    print(aa)","0aff1b46":"for aa in last_name1[0].unique():\n  if (aa in last_name[0].unique()) == True:\n    print(aa)","751c97fe":"for aa in last_name1[0].unique():\n  if (aa in last_name[0].unique()) == False:\n    print(aa)","8fd9139d":"last=pd.DataFrame()\nname=pd.DataFrame()\n\nlast1=pd.DataFrame()\nname1=pd.DataFrame()\n\nfor i in last_name[0]:\n  l,n = i.split('.',maxsplit=1)\n  last=last.append([l])\n  name=name.append([n])\n\nfor i in last_name1[0]:\n  l,n = i.split('.',maxsplit=1)\n  last1=last1.append([l])\n  name1=name1.append([n])","9ee51975":"for aa in last1[0].unique():\n  if (aa in last[0].unique()) == True:\n    print(aa)","43662d20":"for aa in last1[0].unique():\n  if (aa in last[0].unique()) == False:\n    print(aa)","33c62d4d":"last[0].unique()","b2be44d8":"last1[0].unique()","617f15ff":"last=last.reset_index().drop(['index'],axis=1)\nlast.columns=['reference']\n\nlast1=last1.reset_index().drop(['index'],axis=1)\nlast1.columns=['reference']","120c2b85":"df['Name'] = last['reference']\ndf","b92ec61c":"df1['Name'] = last1['reference']\ndf1","97f42074":"train_y=df['Survived']\ndf=df.drop(['Survived'],axis=1)\ndf","4590a6e5":"train_test=pd.concat([df,df1],axis=0)\ntrain_test","86e4d8fc":"train_test.isnull().sum()","61647ed3":"train_test['Ticket'].unique()","a13bbd6a":"ticket_number=pd.DataFrame()\nticket_name = pd.DataFrame()\nfor i in train_test['Ticket']:\n  a = i.split(' ',maxsplit=-1)\n  if len(a) == 1:\n    ticket_number=ticket_number.append([a[-1]])\n    ticket_name=ticket_name.append([0])\n  else:\n    ticket_number=ticket_number.append([a[-1]])\n    ticket_name=ticket_name.append([1])","d756182d":"ticket_number","36547e4e":"ticket_number[0]=ticket_number.replace('LINE', 0)\nticket_number","a428775a":"ticket_name","14e0868a":"ticket_number=ticket_number.reset_index().drop(['index'],axis=1)\nticket_name=ticket_name.reset_index().drop(['index'],axis=1)","b23da891":"ticket_number","548a9e00":"ticket_name","cc6ae37f":"train_test","7848571e":"train_test=train_test.reset_index().drop(['index'],axis=1)","1a751f79":"train_test['Ticket']=ticket_name\ntrain_test['Ticket_number']=ticket_number\ntrain_test","f0fa152d":"import seaborn as sns\nsns.countplot(last['reference'])","45082c84":"sns.countplot(last1['reference'])","1e699058":"train_test['Name'].value_counts()","12a57b2b":"train_test['Name']=train_test['Name'].replace(' Mr', 1)\ntrain_test['Name']=train_test['Name'].replace(' Mrs', 2)\ntrain_test['Name']=train_test['Name'].replace(' Miss', 3)\ntrain_test['Name']=train_test['Name'].replace(' Master', 4)\ntrain_test['Name']=train_test['Name'].replace(' Ms', 0)\ntrain_test['Name']=train_test['Name'].replace(' Col', 0)\ntrain_test['Name']=train_test['Name'].replace(' Rev', 0)\ntrain_test['Name']=train_test['Name'].replace(' Dr', 0)\ntrain_test['Name']=train_test['Name'].replace(' Dona', 0)\ntrain_test['Name']=train_test['Name'].replace(' Mlle', 0)\ntrain_test['Name']=train_test['Name'].replace(' Major', 0)\ntrain_test['Name']=train_test['Name'].replace(' Lady', 0)\ntrain_test['Name']=train_test['Name'].replace(' Don', 0)\ntrain_test['Name']=train_test['Name'].replace(' Mme', 0)\ntrain_test['Name']=train_test['Name'].replace(' the Countess', 0)\ntrain_test['Name']=train_test['Name'].replace(' Capt', 0)\ntrain_test['Name']=train_test['Name'].replace(' Sir', 0)\ntrain_test['Name']=train_test['Name'].replace(' Jonkheer', 0)","91bf6d95":"sns.countplot(train_test['Name'])","cdb556ff":"train_test","1bf55566":"sns.countplot(train_test['Sex'])","4fbdbdcd":"train_test['Sex']=train_test['Sex'].replace('male', 0)\ntrain_test['Sex']=train_test['Sex'].replace('female', 1)\ntrain_test","baa9804f":"train_test['Cabin'].value_counts()","eed5c750":"train_test[train_test['Cabin'] == train_test['Cabin'].value_counts().index[0]]","62d26353":"train_test[train_test['Cabin'] == train_test['Cabin'].value_counts().index[1]]","808cd6b3":"train_test[train_test['Cabin'] == train_test['Cabin'].value_counts().index[2]]","d6b4dd3e":"train_test=train_test.reset_index().drop(['index'],axis=1)\ntrain_test","34553c55":"import numpy as np\ntrain_test[train_test['Embarked'].isnull() == True]","1bf73203":"train_test['Embarked'].unique()","6b5bb717":"train_test['Embarked']=train_test['Embarked'].replace('S',0)\ntrain_test['Embarked']=train_test['Embarked'].replace('C',1)\ntrain_test['Embarked']=train_test['Embarked'].replace('Q',2)\ntrain_test","06b86d63":"imputation_x=train_test[['Pclass','Name','Sex','SibSp','Parch','Ticket','Embarked','Ticket_number']]\nimputation_x","261d76aa":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscale_imputation_x=pd.DataFrame(scaler.fit_transform(imputation_x.drop(['Embarked'],axis=1)))\nscale_imputation_x.columns=imputation_x.drop(['Embarked'],axis=1).columns\nscale_imputation_x","ac30bf07":"k=3\n\nfrom sklearn.cluster import KMeans\n\nkmeans=KMeans(n_clusters=k,algorithm='full')\nkmean_result=kmeans.fit_transform(scale_imputation_x)\nkmean_result","47117956":"idx_61=np.argmin(kmean_result[61])\nidx_829=np.argmin(kmean_result[829])","8db3dfc7":"print(idx_61)\nprint(idx_829)","9334b977":"pd.DataFrame(np.argmin(kmean_result,axis=1))[pd.DataFrame(np.argmin(kmean_result,axis=1))[0] == 0].index","32f5cfa4":"sns.countplot(train_test['Embarked'].iloc[pd.DataFrame(np.argmin(kmean_result,axis=1))[pd.DataFrame(np.argmin(kmean_result,axis=1))[0] == 0].index])","af59b228":"train_test['Embarked']=train_test['Embarked'].fillna(0)\ntrain_test.isnull().sum()","58baec23":"train_test['Age']","8c4620c8":"from sklearn.linear_model import LinearRegression\n\nage_x = train_test[['Pclass','Name','Sex','SibSp','Parch','Ticket','Embarked','Ticket_number']]\n\nage_y = pd.DataFrame(train_test['Age'])\n\nage_train_y = age_y[age_y['Age'].isnull()==False]\nage_test_y = age_y[age_y['Age'].isnull()==True]\nage_test_x = age_x.iloc[age_test_y.index]\nage_train_x = age_x.iloc[age_train_y.index]\nage_model=LinearRegression()\nage_model.fit(age_train_x, age_train_y)","38b67675":"from sklearn.metrics import r2_score\nr2_score(age_train_y, age_model.predict(age_train_x))","0dac2ab7":"train_test","7a9dd0d5":"sns.distplot(train_test['Age'])","c3d29f14":"print(train_test['Age'].min())\nprint(train_test['Age'].max())","08f01460":"age=pd.DataFrame()\n\nfor i in train_test['Age']:\n  i = str(i)[0]\n  age=age.append([i])\nage","d06db4d2":"age=age.reset_index().drop(['index'],axis=1)\nage.columns=['Age']\nage","dc35161d":"age=age.replace('n',np.NaN)\nage_x = train_test[['Pclass','Name','Sex','SibSp','Parch','Ticket','Embarked','Ticket_number']]\nscaler=StandardScaler()\nage_xx=pd.DataFrame(scaler.fit_transform(age_x))\nage_xx.columns=age_x.columns\n\n\n\nage_y = pd.DataFrame(age['Age'])\n\nage_train_y = age_y[age_y['Age'].isnull()==False]\nage_test_y = age_y[age_y['Age'].isnull()==True]\nage_test_x = age_xx.iloc[age_test_y.index]\nage_train_x = age_xx.iloc[age_train_y.index]","96f69e27":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nmodel = LogisticRegression()\nmodel.fit(age_train_x, age_train_y)\naccuracy_score(age_train_y, model.predict(age_train_x))","ba9e9380":"train_test['Age']=age['Age']\ntrain_test['Age']=train_test['Age'].fillna(-1)\ntrain_test","b4a55e5f":"sns.distplot(train_test['Fare'])","6311c186":"train_test[train_test['Fare'].isnull()==True]","a63fc845":"train_test[train_test['Ticket_number'] == '3701']","ebc0ac81":"fare_imputation=train_test[(train_test['Pclass'] == 3) & (train_test['Embarked'] == 0)]['Fare'].mean()\nfare_imputation","b179013d":"train_test['Fare']=train_test['Fare'].fillna(fare_imputation)","2ec88de4":"train_test['Ticket_number']","08289154":"train_test[train_test['Ticket_number'] == '21171']","3cb7cd70":"train_test[train_test['Ticket_number'] == '17599']","d6c89d09":"train_test[train_test['Ticket_number'] == '3101282']","248a4962":"train_test[train_test['Ticket_number'] == '113803']","85049603":"train_test[train_test['Cabin'].isnull()== True]['Ticket_number']","f82869f9":"for i in pd.DataFrame(train_test.groupby(by=['Ticket','Ticket_number']))[1]:\n  print(i)","3a764dfe":"train_test","f9bcd13a":"for i in train_test['Cabin']:\n  i = str(i).split(' ')\n  print(i)","c70fd469":"cabin=pd.DataFrame()\ntrain_test['Cabin']=train_test['Cabin'].fillna(-1)\nfor i in train_test['Cabin']:\n  if i != -1:\n    i = str(i).split(' ')\n    cabin=cabin.append([i[0][0]])\n  else:\n    cabin=cabin.append([i])\ncabin","af4aad57":"cabin.value_counts()","46f0460d":"cabin=cabin.replace('C', 0)\ncabin=cabin.replace('B', 1)\ncabin=cabin.replace('D', 2)\ncabin=cabin.replace('E', 3)\ncabin=cabin.replace('A', 4)\ncabin=cabin.replace('F', 5)\ncabin=cabin.replace('G', 6)\ncabin=cabin.replace('T', 7)\ncabin","5dd152a3":"cabin.value_counts()","bf85c1eb":"cabin=cabin.reset_index().drop(['index'],axis=1)\ncabin.columns=['Cabin']\ncabin","1c6ab419":"train_test['Cabin'] = cabin['Cabin']\ntrain_test","fff24b30":"train_test.isnull().sum()","6d49d862":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\n\nscale_data = pd.DataFrame(scaler.fit_transform(train_test.drop(['PassengerId'],axis=1)))\nscale_data.columns=[train_test.drop(['PassengerId'],axis=1).columns]\ntrain_data=scale_data.iloc[:891]\ntest_data=scale_data.iloc[891:]","a3d3b0f4":"train_data","9590fc55":"test_data","dc73a445":"from sklearn.model_selection import train_test_split\n\ntrain_x, val_x, train_y ,val_y = train_test_split(train_data, train_y, test_size=0.4,random_state=2021)\n\ntrain_x=train_x.reset_index().drop(['index'],axis=1)\nval_x=val_x.reset_index().drop(['index'],axis=1)\ntest_data=test_data.reset_index().drop(['index'],axis=1)\n\ntrain_y=pd.DataFrame(train_y).reset_index().drop(['index'],axis=1)\nval_y=pd.DataFrame(val_y).reset_index().drop(['index'],axis=1)","52104cf3":"train_x","be55fddb":"val_x","c129f9ee":"# Load tensorflow for DL.\nimport tensorflow as tf\n\n\n# Set your model with Sequential\nmodel=tf.keras.models.Sequential(\n    # Flatten is used to input your train data.\n    [tf.keras.layers.Flatten(input_shape = [train_x.shape[1]]),\n     # Stack your hidden layers with some neurons.\n     # If you don't know what you should use the activation function, ReLU function will be helpful.\n     # Practice your deep learning by changing some neurons in each layer and try to improve your score iterately.\n     tf.keras.layers.Dense(500,activation='selu'),\n     tf.keras.layers.Dense(500, activation='relu'),\n     tf.keras.layers.Dense(500, activation='relu'),\n     tf.keras.layers.Dense(500, activation='relu'),\n     tf.keras.layers.Dense(500, activation='relu'),\n     tf.keras.layers.Dense(500, activation='relu'),\n     # Dropout is used to regularize and prevent the model from overfitting.\n     tf.keras.layers.Dropout(0.2),\n     tf.keras.layers.Dense(1500, activation='relu'),\n     tf.keras.layers.Dropout(0.15),\n     # This analysis is for classification. So, you have to use the number of neuron as 1 in output layer.\n     # In this case, the activation function in output layer will be more powerful to use sigmoid than relu.\n     tf.keras.layers.Dense(1,activation='sigmoid')]\n)\n\n\n# # Set your learning schedule for get global optimum point.\n# lr = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate=2e-3,\n#     decay_steps=10000,\n#     decay_rate=0.95\n# )\n\n# Set your optimizer as Adagrad or SGD or extra...\nopt = tf.keras.optimizers.Adam(learning_rate = 0.002)\n\n# Compile your model with prepared optimizer, loss function for classification and metrics for monitoring.\nmodel.compile(optimizer = opt, loss='binary_crossentropy', metrics = ['accuracy'])\n\n# Fit your train dataset and validation dataset.\n# You can practice this model by changing some hyper parameters such as epochs, batch_size, and so on into another values.\n# EarlyStopping is used for preventing the training from overfitting.\n# patience is related to your epochs and save your best model by setting restore_best_weights.\nmodel.fit(train_x, train_y, batch_size = 5, epochs=200, verbose=2, validation_split=0.2, workers=3, validation_batch_size = 5, \n          callbacks = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss', patience=200 * 0.1, verbose=2, restore_best_weights=True))\n\n# validate your model performance\n# It will be represented loss value and accuracy value, repectively.\nmodel.evaluate(val_x, val_y)","a0183a9b":"# Load the submission form.\nsubmission=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n\n# We will input our prediction into this data frame.\nprediction=pd.DataFrame()\n\n# DL will show the probability about classification. So, we can classify the survival class or no along with the probability (your prediction)\nfor i in model.predict(test_data):\n    # If the probability is greater than 0.5, this observation is regared as survived.\n    # In this case, the decision boundary is 0.5.\n  if i >=0.5:\n    i=1\n    prediction=prediction.append([i])\n    # If it is not that, this observation is regared as dead.\n  else:\n    i=0\n    prediction=prediction.append([i])\nprediction=prediction.reset_index().drop(['index'],axis=1)\nprediction.columns=['Survived']\n\n\n# Finish your submission.\nsubmission['Survived'] = prediction['Survived']\n\n# Submit your result.\nsubmission.to_csv('submission.csv',index=False)","38f65b41":"## Train dataset contains some non values in Age, Cabin, and Embarked columns.","3f7e60a8":"## By grouping the ticket, ticket number values, we try to find the clues for null values for Cabin.","d8f7ad6a":"## Test dataset contains the non values in Age, Cabin, and Fare, not Embarked.","1037fcaa":"## Now, you are ready to build your model to predict the survival.","419f5659":"## Split train and validation from train dataset.","819e8f4c":"## Next, we've got data pre-preocessing in Ticket column","b1759e0f":"Let's count the values of Cabin.","5908a24c":"## We will estimate the null values of Embarked by cluster analysis.","c109f02a":"## Now, It is end of pre-processing.","e6f14807":"## Same kinds of ticket mean same fare, cabin, and embarked.\n","c674a891":"# Load pandas to handle the dataset","7aa0cf5d":"## Min-Max scaling the values before clustering.","8eedf7d0":"## But it doesn't work well. because age is discrete value.","c8c12805":"## PassengerId 892 is the start point to test dataset.\n","98e12e53":"## Anyway, insert our name data into train, test dataset respectively.","c72719eb":"In this case, null value is alone. So, we will replace the null value of Fare to the average of Pclass and Ebarked.","b4157d5d":"## In train dataset, there are many references compared with test dataset.","8daa26dc":"## So, we will imputate the null values by deviding the age values into group age.\n\nAnd we try to estimate the null values using with logistic regression ( with softmax).","b98fe1b6":"Fare distribution","25e5339c":"## Make a dataset for imputation.","156b6d68":"## Cabin has many information and plenty of clues.\n\nIf cabin is same each other, the passengers might be a family or couple.\n\nSo, we could find the clues about the null value.","9ed84fed":"## Use the fillna method to imputate the null value.","64122bf5":"## It is better to use their references.\n\nIn the name data, there are many last names so it will be hard to find the clue.\n\nHowever, the references such as Mr., Miss., and so on are easy to find.\n\nSo, we are going to capture and convert these references into numeric.","c06fc72e":"## There are only a few values except for 'Mr', 'Miss',' Mrs', and 'Master'.\n\nSo, we will convert these four values (Mr, Miss, Mrs, and Master) into numeric and the others will be clustered in one group.","16a2df8c":"## We will handle the data with Min-Max scaling and devide the dataset into train, validation, test dataet.\n\nAnd, we will rid of PassengerID because it is unnecessary.","19d57e50":"## Fare imputation","853de84c":"## The case of Cabin value.\n\n> A passenger has a couple of seats\n\n> Unknown seat type\n\n> Unknown seat number\n\nHence, if we don't know the value, we will replace as -1, otherwise, we will take only the initial of seat as below:","8ea0054d":"## The optimal cluster is 3.","98579fbc":"## Pre-processing Name values","67abafc0":"## Find the clue from same ticket number.","256a85d8":"## Cabin pre-processing.\n\nCabin has high probability that is related to Ticket.","4c601580":"## Find whether a reference in test data is in train or not.\n\nMr, Mrs, Miss, Master, Ms, Col, Rev, and Dr are in train and test.","d06265d2":"## Both two null values are 0 group.\n\nThen, search what values are. in 0 group.","98b99a99":"# Prepare your submission","a73fc3d2":"## Embarked is split with 3 groups (S, C, and O) so, we determine the number of clusters is 3.","7934953d":"## Total null value?","bc8ee0e6":"## See the value count","2e304565":"## Logistic regression doesn't work well, either.\n\nSo, we won't imputate the null values and just keep going.\n\nThat is, non value is just non value and this is a part of data.\n\nThus, we will convert null values into -1 which is not shown in Age.\n\n-1 means this value is non value.\n","f17277e5":"## We try to imputate the null value of Age with linear regression.","81a5af68":"## Dona is in test but, not in train.","66de5a82":"## Find the relationships between train dataset names and test's one.\n\nWhethere the name (first and last) in test dataset is in train dataset or not?\n\n> first_name1 - first name from test dataset\n\n> first_name = first name from train dataset\n\n> last_name1 = last name from test dataset\n\n> last_name = last name from train dataset","d40b3f53":"## Cabin pre-processing","91a43983":"## Fare pre-processing","91c7a2e2":"## Name data pre-processing\n\nWe will convert name value such as 'Mr','Miss', ... into 1,2,3... as in:**\uad75\uc740 \ud14d\uc2a4\ud2b8**","e72e2a18":"## From now on, we will concatenate train without target value and test dataset. \nBecause data pre-processing will be applied to both train and test at the same time.","abe18dee":"Sex count plot","6b9d8994":"## We will covert male into 0 and female into 1","8341bfc4":"## Find the clue and relationship from Cabin.","f7ea4b71":"## Apply to test dataset with same method","470e7d56":"## Age pre-processing","8a16d376":"## We'll devide the ticket value into ticket number and ticket name.\n\nIf ticket name has 1 value, the ticket was combined with stirng and numbering.\n\nOtherwise, if ticket name is 0 value, the ticket was only numeric ticket.","390725ea":"## However, many cases show that we couldn't find any clues about null values of Cabin. Accordingly, we will replace the null values to -1.\n\n-1 means that the value is null value of Cabin.\n","3024c137":"## Take the initial character or -1.","1bae391b":"# Deep learning for survival classification.","93f6c40f":"## First, split the names into fist name, last name.","e7e5e3e0":"## Find the cluster of null index.","48c17f54":"## Converting the Embarked values into numeric.","04be2673":"## Estimate the null values of Age with logistic regression","4264e218":"## check the test dataset","109bb4dd":"## Identify the distribution from count plot\n\n> train dataset","b18b2f38":"## Concatenating the pre-processed ticket data into train_test dataset.","2e33576d":"## Train, test dataset reindexing before concatenating pre-processed ticket data into train_test dataset.","8266c9f4":"## The min, max values","e0f7a4c2":"## Most values of this group represent the 0 values, that is, S.\n\nSo, we convet non value into 0.","4a5428f7":"## Age distribution","22e4dcf6":"## Ticket_number and ticket_name reindexing","dbf63f69":"## Insert the pre-processed cabin data into train_test dataset.","9c1fc84c":"## LINE, the value in ticket column, must be coverted another value like 0 which represents only LINE as 0.\n\nSo, we can write the code as in:","95a6a684":"## Sex pre-processing","5457372b":"## Spliting train and test.","3cd6b530":"# Read me\n\n> If you only want to know how you can deal with null values (name, embarked, and etc..), see https:\/\/www.kaggle.com\/pythonash\/making-completed-dataset.\n\nThis notebook describes how you can handle raw dataset which consists of some null values.\n\nAnd you can experience deep neural network's performance in small dataset.\n\nAs you may know, when you deal with small dataset, ML is more proper than DL due to lack of data.\n\nHowever, you can achieve up approximately 0.77% accruacy with my notebook.\n\nThis notebook is for begginer who wants to practice deep learning for classification.\n\nSo, EDA is not prepared because there are many notebooks for EDA.\n\nI expect that you can learn about dealing with hadling null data, imputation, and DL softly.\n\nLet's start","273f806a":"## Countplot of Name","00ac4bcb":"> test dataset","499215c3":"## Convert the values into 0, 1, 2....","d248273a":"## Find the rule of dataset, what is the clue in name?\n\n> I think there are many James.....\n\nHow can I handle this column to make tidy dataset?","40eedfca":"## check the train dataset"}}