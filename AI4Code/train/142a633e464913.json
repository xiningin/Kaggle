{"cell_type":{"90b4a297":"code","36a413e5":"code","bc6f3456":"code","d23e753e":"code","b17c139d":"code","1dfaf5dd":"code","dca6db4b":"code","c6b8dc92":"code","924a1cee":"code","a0a1d734":"code","2e2edffb":"code","ce159af2":"code","6f2385fb":"code","094921fd":"code","da5a5492":"code","323273f8":"code","d383c6ad":"code","a52b839c":"code","888aaf77":"code","1bc8a74e":"code","4772fd0d":"code","5310df53":"code","159c0683":"code","e27bd68b":"code","71b237b5":"code","cab9b813":"code","44e9612b":"code","141a842e":"code","a11a2bce":"code","a319751f":"code","ef821025":"code","b3da0556":"markdown","2925e9f9":"markdown","07521f99":"markdown","029f7b51":"markdown","2bb63ef3":"markdown","f5c99bd9":"markdown","57a8b5a3":"markdown","df1e0413":"markdown","cb18072a":"markdown","99851b31":"markdown"},"source":{"90b4a297":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nimport nltk\nimport spacy\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()","36a413e5":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","bc6f3456":"train_df.info()","d23e753e":"train_df.head(3)","b17c139d":"print(train_df.shape)\nprint(test_df.shape)","1dfaf5dd":"count_df = train_df.target.value_counts()\nax = sns.barplot(count_df.index, count_df)\nax.set(xlabel='target', ylabel='count')\nplt.show()","dca6db4b":"train_df['char'] = train_df['text'].str.len()\ntrain_df['words'] = train_df['text'].str.split().map(lambda x: len(x))\ntrain_df.head(3)","c6b8dc92":"fig=plt.figure(figsize=(16,6))\n\nax1=plt.subplot(1, 2, 1)\nax2=plt.subplot(1, 2, 2)\n\nax1.set_title('disaster tweet')\nax2.set_title('Not disaster tweet')\n\nsns.distplot(train_df[train_df['target']==1]['char'], color='red', ax=ax1)\nsns.distplot(train_df[train_df['target']==0]['char'], color='green', ax=ax2)","924a1cee":"fig=plt.figure(figsize=(16,6))\n\nax1=plt.subplot(1, 2, 1)\nax2=plt.subplot(1, 2, 2)\n\nax1.set_title('disaster tweet')\nax2.set_title('Not disaster tweet')\n\nsns.distplot(train_df[train_df['target']==1]['words'], color='red', ax=ax1)\nsns.distplot(train_df[train_df['target']==0]['words'], color='green', ax=ax2)","a0a1d734":"fig=plt.figure(figsize=(16,6))\n\nax1=plt.subplot(1, 2, 1)\nax2=plt.subplot(1, 2, 2)\n\nsns.distplot(train_df[train_df['target']==1]['char'], color='red', ax=ax1)\nsns.distplot(train_df[train_df['target']==0]['char'], color='green', ax=ax1)\n\nsns.distplot(train_df[train_df['target']==1]['words'], color='red', ax=ax2)\nsns.distplot(train_df[train_df['target']==0]['words'], color='green', ax=ax2)","2e2edffb":"print('There are {} tweets in total and {} tweets with keyword'.format(train_df['keyword'].shape[0], train_df['keyword'].notna().sum()))","ce159af2":"train_df['text'] = train_df['text'].str.replace(\"http\\S+\", \" \")\ntest_df['text'] = test_df['text'].str.replace(\"http\\S+\", \" \")","6f2385fb":"train_df = train_df[['text', 'target']]\ntweets = train_df.groupby('text').mean().reset_index()\nprint('There are {} tweets with different label in duplicates.'.format(tweets[(tweets['target']!=1) & (tweets['target']!=0)].shape[0]))\ndf_diff = tweets[(tweets['target']!=1) & (tweets['target']!=0)].reset_index(drop=True)\ntweets = tweets[(tweets['target']==1) | (tweets['target']==0)].reset_index()","094921fd":"# Show the mislabeled samples\nfor i in range(64):\n    print(train_df[train_df['text']==df_diff.loc[i]['text']])","da5a5492":"# Load the large model to get the vectors\nnlp = spacy.load('en_core_web_lg')","323273f8":"# We just want the vectors so we can turn off other models in the pipeline\nwith nlp.disable_pipes():\n    vectors = np.array([nlp(tweet.text).vector for idx, tweet in tweets.iterrows()])\n    \nvectors.shape","d383c6ad":"from sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(vectors, tweets.target, test_size=0.2, \n                                                      random_state=52, stratify = tweets.target)\n\n\n# Create the LinearSVC model\nmodel = LinearSVC(random_state=1, dual=False)\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Uncomment and run to see model accuracy\nprint(f'Model test accuracy: {model.score(X_valid, y_valid)*100:.3f}%')","a52b839c":"# 2nd models\nfrom sklearn.linear_model import LogisticRegression\n\nsecond_model = LogisticRegression(solver='saga')\nsecond_model.fit(X_train, y_train)\nprint(f'Model test accuracy: {second_model.score(X_valid, y_valid)*100:.3f}%')","888aaf77":"# 3rd model\n# from sklearn.linear_model import Ridge\n\nthird_model = linear_model.RidgeClassifier()\nthird_model.fit(X_train, y_train)\n\nprint(f'Model test accuracy: {third_model.score(X_valid, y_valid)*100:.3f}%')","1bc8a74e":"# create dataset for lightgbm\nimport lightgbm as lgb\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n# LightGBM parameters\nparams = {\n    'task' : 'train',\n    'boosting_type' : 'gbdt',\n    'objective' : 'binary',\n    'metric' : {'binary_logloss'},\n    'num_leaves' : 51,\n    'learning_rate' : 0.01,\n    'max_bin': 397,\n    'feature_fraction' : 0.9,\n    'bagging_fraction' : 0.8,\n    'bagging_freq': 5,\n    'verbose' : 0\n}\n\n# train\ngbm = lgb.train(params, lgb_train,\n                valid_sets=[lgb_train, lgb_eval],\n                verbose_eval=10,\n                num_boost_round=1000,\n                early_stopping_rounds=10)\n\ny_pred = gbm.predict(X_valid, num_iteration=gbm.best_iteration)\n\nprint(gbm.best_score['valid_1']['binary_logloss'])\n\nfourth = pd.DataFrame([np.round(y_pred,0), y_valid], index=['y_pred', 'y_valid']).T\naccuracy = fourth[fourth['y_pred']==fourth['y_valid']].shape[0]\/fourth.shape[0]\nprint(f'Model test accuracy: {accuracy*100:.3f}%')","4772fd0d":"from sklearn.metrics import f1_score\n\nprint('SVM f1 score {}'.format(f1_score(y_valid, np.round(model.predict(X_valid), 0).astype(int))))\nprint('Logistic Regression f1 score {}'.format(f1_score(y_valid, np.round(second_model.predict(X_valid), 0).astype(int))))\nprint('Ridge Classifier f1 score {}'.format(f1_score(y_valid, np.round(third_model.predict(X_valid), 0).astype(int))))\nprint('Light GBM f1 score {}'.format(f1_score(y_valid, np.round(gbm.predict(X_valid, num_iteration=gbm.best_iteration), 0).astype(int))))","5310df53":"# apply same word vectors building to test data\nwith nlp.disable_pipes():\n    vectors_pred = np.array([nlp(tweet.text).vector for idx, tweet in test_df.iterrows()])\n    \nvectors_pred.shape","159c0683":"from sklearn.model_selection import KFold\n\nX_train = pd.DataFrame(vectors)\ny_train = tweets['target']\nX_test = pd.DataFrame(vectors_pred)\n\ny_preds = []\nmodels = []\noof_train = np.zeros((len(X_train),))\ncv = KFold(n_splits=5, shuffle=True, random_state=100892)\n\nparams = {\n        'task' : 'train',\n        'boosting_type' : 'gbdt',\n        'objective' : 'binary',\n        'metric' : {'binary_logloss'},\n        'num_leaves' : 51,\n        'max_bin' : 397,\n        'learning_rate' : 0.01,\n        'feature_fraction' : 0.9,\n        'bagging_fraction' : 0.8,\n        'bagging_freq': 5,\n        'verbose' : 0\n}\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train)):\n    X_tr = X_train.iloc[train_index]\n    X_val = X_train.iloc[valid_index]\n    y_tr = y_train.iloc[train_index]\n    y_val = y_train.iloc[valid_index]\n    \n    lgb_train = lgb.Dataset(X_tr, y_tr)\n    \n    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n    \n    model = lgb.train(params, lgb_train,\n                     valid_sets=[lgb_train, lgb_eval],\n                     verbose_eval=10,\n                     num_boost_round=1000,\n                     early_stopping_rounds=10)\n    \n    oof_train[valid_index] = model.predict(X_val, num_iteration=model.best_iteration)\n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    \n    y_preds.append(y_pred)\n    models.append(model)\n\nscores = [\n    m.best_score['valid_1']['binary_logloss'] for m in models\n]\nscore = sum(scores) \/ len(scores)","e27bd68b":"print('===CV scores===')\nprint(scores)\nprint(score)","71b237b5":"pred = pd.DataFrame(np.round(y_preds, 0)).astype('int8').T","cab9b813":"np.round(pred.mean(axis=1),0).astype(int)","44e9612b":"# import optuna\n# from sklearn.metrics import log_loss\n\n# def objective(trial):\n#     params = {\n#         'objective': 'regression',\n#         'metric' : {'binary_logloss'},\n#         'num_leaves': trial.suggest_int('num_leaves', 32, 128),\n#         'learning_rate': 0.01,\n#         'max_bin': trial.suggest_int('max_bin', 255, 500)\n#     }\n    \n#     lgb_train = lgb.Dataset(X_train, y_train)\n    \n#     lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n    \n#     model = lgb.train(params, lgb_train,\n#                      valid_sets=[lgb_train, lgb_eval],\n#                      verbose_eval=10,\n#                      num_boost_round=1000,\n#                      early_stopping_rounds=10)\n    \n#     y_pred_valid = model.predict(X_test,\n#                                 num_iteration=model.best_iteration)\n#     score = log_loss(y_test, y_pred_valid)\n#     return score\n\n# study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n# study.optimize(objective, n_trials=40)\n# study.best_params","141a842e":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission","a11a2bce":"sample_submission[\"target\"] = np.round(pred.mean(axis=1),0).astype(int)\nsample_submission[\"target\"] = sample_submission[\"target\"].astype('int8')","a319751f":"sample_submission.head()","ef821025":"sample_submission.to_csv(\"submission.csv\", index=False)","b3da0556":"### Our model","2925e9f9":"# Mislabeled\n\nThere are 64 unique tweets that have the different label in duplicates. Those mislabeled tweets would lower our model accuracy. Let's remove those tweets and then build a model using identically labeled samples in training dataset.","07521f99":"## Real or Not? NLP with Disaster Tweets\n\n\n\nThis notebook is from **[Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started)**. In this notebook, I show basic EDA, word2vector and light gbm to do a better prediction. Any comments are welcome and appreciated.","029f7b51":"# Hyperparameter tuning LGBM\n\nSince it takes time to run the auto hyperparameter tuning below, I changed the to comments. The result reflected above.","2bb63ef3":"# Read dataset and EDA","f5c99bd9":"# TODO\n\n* Data cleaning: Relabel mislabeled samples and Cleaning puctuation\n* Bulding better word vectors\n* Use keyword column to predict\n* Neural network model","57a8b5a3":"# Building vectors and predicting models\n\nThe tutorial used `CountVectorizer` to count the words in each tweet and turn them into data our machine learning model can process.\n\nHere I use word2vector to build vectors. A good tutorial is here **[Word Vectors](https:\/\/www.kaggle.com\/matleonard\/word-vectors)**.","df1e0413":"Light gbm has the best predicting power for this dataset.","cb18072a":"# Remove URL\n\nI remove URLs from tweets. I assume URLs would not help us make a better prediction.","99851b31":"# Cross Validation"}}