{"cell_type":{"1e189759":"code","698ceba7":"code","30e0d223":"code","2312303c":"code","69a71c30":"code","d7205c64":"code","d2e12280":"code","7a8e43aa":"code","bcc8412e":"code","b3c93192":"code","10b3aeb3":"code","3168dfa9":"code","789b64d4":"code","f273a90e":"code","7e331941":"code","21277f85":"code","8f481148":"code","ac5b8710":"code","ba023a66":"code","95318b0e":"code","b29d71c5":"code","acdc896d":"code","f579ad8d":"code","b10f0e22":"code","62597341":"code","45b90b24":"code","1dbe4506":"code","ac6259ac":"code","db3e38cb":"code","27db0181":"code","e6c701ac":"code","0eea3686":"code","59e81254":"code","35fd0149":"code","d59a3b7a":"code","6b8ee3f0":"code","77283be8":"code","fb4f33d3":"code","1c049d05":"code","9a03f781":"code","9ea7bbff":"code","a2875979":"code","b7ac2fae":"code","a60db92c":"code","6c241173":"code","0e2a3dca":"code","e5e1368b":"code","682e1eae":"code","3eb11291":"code","2cc7014a":"code","c13c4607":"code","b6be635d":"code","1ed49fe6":"code","ed925a2b":"code","76da35f2":"code","9f23ff39":"code","5f9f44ee":"code","788045af":"code","adb6aa15":"code","f5beaf31":"code","6260103e":"code","5279bcb9":"code","b6489978":"code","b0cb1a76":"code","0b9671c3":"code","a3a91481":"code","cdb42d15":"code","98fb1253":"code","fafb8cc5":"code","90176c52":"code","f0b9980c":"code","d947195e":"code","9bcb3ed3":"code","62313240":"code","bc20b39f":"code","2ee5af1e":"code","7a47fb64":"code","ddaa51a8":"code","a6c2a1ce":"code","9cb2a9b8":"code","28d17e64":"code","f95085b9":"code","04a578fc":"code","72c51d5e":"code","c5c6160d":"code","ecb334ab":"code","8692c5e9":"code","4549986d":"code","b86ad3b6":"code","cca7f7f5":"code","6d620555":"code","d2022266":"code","ed5165cc":"code","611a23df":"code","ff38d22e":"code","7c4d03cb":"code","22f541b6":"code","46e97b18":"code","9711467e":"code","20f05dce":"code","23c43965":"markdown","ad206b19":"markdown","a7da95e0":"markdown","88319f61":"markdown","aac5eda6":"markdown","6ed12464":"markdown","b7c3d7a6":"markdown","9b82e827":"markdown","6048a260":"markdown","f4338f6c":"markdown","d9a67511":"markdown","f0073cc0":"markdown"},"source":{"1e189759":"# lets import the required libraries and packages\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_score, recall_score\nimport warnings\nwarnings.filterwarnings('ignore')","698ceba7":"# lets import the dataset\ntelecom = pd.read_csv(\"..\/input\/telecom\/telecom_churn_data.csv\")\ntelecom.head()","30e0d223":"# lets check the dimensions of the dataset\ntelecom.shape","2312303c":"# High-value customers : Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of \n# the average recharge amount in the first two months (the good phase)\n\n# lets dervie features to extract high value customers\n# lets find out total amount spent by customers on data recharge,we have two colums available to find this out \n# first column is av_rech_amt_data_x (x represents month here, would be either 6 or 7 or 8)\n# second column is total_rech_data_x (x represnts month here, would be either 6 or 7 or 8)\n# lets introduce a new column total_rech_data_amt_x which can be calculated as av_rech_amt_data_x * total_rech_data_x\n\ntelecom['total_rech_data_amt_6'] = telecom['av_rech_amt_data_6'] * telecom['total_rech_data_6']\ntelecom['total_rech_data_amt_7'] = telecom['av_rech_amt_data_7'] * telecom['total_rech_data_7']\ntelecom['total_rech_data_amt_8'] = telecom['av_rech_amt_data_8'] * telecom['total_rech_data_8']\ntelecom['total_rech_data_amt_9'] = telecom['av_rech_amt_data_9'] * telecom['total_rech_data_9']\n\n# now we dont need columns av_rech_amt_data_x,total_rech_data_x (x = 6\/7\/8) , lets drop them\ntelecom.drop(['total_rech_data_6','total_rech_data_7','total_rech_data_8','total_rech_data_9',\n'av_rech_amt_data_6','av_rech_amt_data_7','av_rech_amt_data_8','av_rech_amt_data_9'],axis = 1,inplace = True)\n\n# lets find out the average recharge done in the first two months(june & july) - the good phase\n# total amount spend would be the sum of total data recharge done & total call\/sms recharges\ntelecom_av_rech_6n7 = (telecom['total_rech_amt_6'].fillna(0) \n+ telecom['total_rech_amt_7'].fillna(0) \n+ telecom['total_rech_data_amt_6'].fillna(0) \n+ telecom['total_rech_data_amt_7'].fillna(0))\/2\n\n# take 70 percentile of the calculated average amount\npercentile_70_6n7 = np.percentile(telecom_av_rech_6n7, 70.0)\nprint(\"70 percentile is : \", percentile_70_6n7)\n\n# fitler the given data set based on 70th percentile\ntelecom_hv_cust = telecom[telecom_av_rech_6n7 >= percentile_70_6n7]\n\nprint(\"Dimensions of the filtered dataset:\",telecom_hv_cust.shape)","69a71c30":"# lets introduce a new column \"churn\", values would be either 1 (churn) or 0 (non-churn)\n# we will calculate churn\/non-churn based on the usage as mentioned in the problem statement\ntelecom_hv_cust['churn'] = np.where(telecom_hv_cust[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].sum(axis=1) == 0, 1,0)\ntelecom_hv_cust.head()","d7205c64":"# lets find out churn\/non churn percentage\ntelecom_hv_cust['churn'].value_counts()\/len(telecom_hv_cust)*100\n\n#observation : 91% of the customers do not churn, this might be a case of class imbalance, we will treat it later","d2e12280":"# lets check the columns with no variance in their values and drop such columns\nfor i in telecom_hv_cust.columns:\n    if telecom_hv_cust[i].nunique() == 1:\n        print(\"\\nColumn\",i,\"has no variance and contains only\", telecom_hv_cust[i].nunique(),\"unique value\")\n        print(\"Dropping the column\",i)\n        telecom_hv_cust.drop(i,axis=1,inplace = True)\n\nprint(\"\\nDimension of the updated dataset:\",telecom_hv_cust.shape)","7a8e43aa":"# lets check the null values present in the dataset\n(telecom_hv_cust.isnull().sum() * 100 \/ len(telecom_hv_cust)).sort_values(ascending = False)","bcc8412e":"# Drop Columns with > 30% of missing values except 9th Month's columns\ncols = telecom_hv_cust.columns\ntelecom_null_perc = telecom_hv_cust.isnull().sum() * 100 \/ len(telecom_hv_cust)\ntelecom_null_df = pd.DataFrame({'col_name': cols,\n                                 'perc_null': telecom_null_perc})\n\ndrop_cols = telecom_null_df.loc[(telecom_null_df[\"col_name\"].str.contains('_9')==False) & (telecom_null_df[\"perc_null\"] > 30.0)][\"col_name\"]\nprint(\"list of columns dropped:\",drop_cols)\n\n# lets drop these columns\ntelecom_hv_cust.drop(drop_cols, axis=1,inplace = True)\ntelecom_hv_cust.shape","b3c93192":"# lets check for columns that can be changed to integers, floats or date types\nobject_col_data = telecom_hv_cust.select_dtypes(include=['object'])\nprint(object_col_data.iloc[0])\n\n# observation : all the columns below can be converted to date type","10b3aeb3":"# convert to datetime\nfor col in object_col_data.columns:\n    telecom_hv_cust[col] = pd.to_datetime(telecom_hv_cust[col])\n\ntelecom_hv_cust.shape","3168dfa9":"# lets check the correlation amongst the features, drop the highly correlated ones\ncor = telecom_hv_cust.corr()\ncor.loc[:,:] = np.tril(cor, k=-1)\ncor = cor.stack()\ncor[(cor > 0.60) | (cor < -0.60)].sort_values()","789b64d4":"# we will drop the columns with high correlation (+\/- 60%)\ndrop_col_list = ['loc_og_t2m_mou_6','std_og_t2t_mou_6','std_og_t2t_mou_7','std_og_t2t_mou_8','std_og_t2t_mou_9','std_og_t2m_mou_6',\n                'std_og_t2m_mou_7','std_og_t2m_mou_8','std_og_t2m_mou_9','total_og_mou_6','total_og_mou_7','total_og_mou_8',\n                'loc_ic_t2t_mou_6','loc_ic_t2t_mou_7','loc_ic_t2t_mou_8','loc_ic_t2t_mou_9','loc_ic_t2m_mou_6','loc_ic_t2m_mou_7','loc_ic_t2m_mou_8','loc_ic_t2m_mou_9',\n                'std_ic_t2m_mou_6','std_ic_t2m_mou_7','std_ic_t2m_mou_8','std_ic_t2m_mou_9','total_ic_mou_6','total_ic_mou_7','total_ic_mou_8',\n                'total_rech_amt_6','total_rech_amt_7','total_rech_amt_8','total_rech_amt_9','arpu_2g_9','count_rech_2g_9','count_rech_3g_9','vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8',\n                'loc_og_t2t_mou_6','loc_og_t2t_mou_7','loc_og_t2t_mou_8','loc_og_t2t_mou_9','loc_og_t2f_mou_6','loc_og_t2f_mou_7','loc_og_t2f_mou_8','loc_og_t2f_mou_9',\n                'loc_og_t2m_mou_6','loc_og_t2m_mou_7','loc_og_t2m_mou_8','loc_og_t2m_mou_9','loc_ic_t2f_mou_6','loc_ic_t2f_mou_7','loc_ic_t2f_mou_8','loc_ic_t2f_mou_9',\n                'date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8']\n                 \ntelecom_hv_cust.drop(drop_col_list, axis=1, inplace=True)\ntelecom_hv_cust.shape","f273a90e":"# Now we will delete 9th month columns because we would predict churn\/non-churn later based on data from the 1st 3 months\ncols_to_drop = [col for col in telecom_hv_cust.columns if '_9' in col]\nprint(cols_to_drop)\n\ntelecom_hv_cust.drop(cols_to_drop, axis=1, inplace=True)\n\ntelecom_hv_cust.shape","7e331941":"# lets check the dataset again\n(telecom_hv_cust.isnull().sum() * 100 \/ len(telecom_hv_cust)).sort_values(ascending = False)\n\n# Obervation : we are left with few columns with around 4% of null values","21277f85":"# drop rows with null values\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['onnet_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['onnet_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['onnet_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['offnet_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['offnet_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['offnet_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['roam_ic_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['roam_ic_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['roam_ic_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['roam_og_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['roam_og_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['roam_og_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['loc_og_t2c_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['loc_og_t2c_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['loc_og_t2c_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['loc_og_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['loc_og_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['loc_og_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_og_t2f_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_og_t2f_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_og_t2f_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_og_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_og_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_og_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['isd_og_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['isd_og_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['isd_og_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['spl_og_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['spl_og_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['spl_og_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['og_others_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['og_others_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['og_others_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['loc_ic_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['loc_ic_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['loc_ic_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_ic_t2t_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_ic_t2t_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_ic_t2t_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_ic_t2f_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_ic_t2f_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_ic_t2f_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_ic_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_ic_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['std_ic_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['spl_ic_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['spl_ic_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['spl_ic_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['isd_ic_mou_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['isd_ic_mou_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['isd_ic_mou_8'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['ic_others_6'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['ic_others_7'])]\ntelecom_hv_cust = telecom_hv_cust[~np.isnan(telecom_hv_cust['ic_others_8'])]","8f481148":"# lets check the dataset again\n(telecom_hv_cust.isnull().sum() * 100 \/ len(telecom_hv_cust)).sort_values(ascending = False)","ac5b8710":"# lets write a function to plot historgram for some sample columns\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","ba023a66":"# function to plot correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = \"Telecom Churn\"\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","95318b0e":"# function to plot scatter plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","b29d71c5":"# call the function to plot the graphs\nplotPerColumnDistribution(telecom_hv_cust, 10, 5)","acdc896d":"plotCorrelationMatrix(telecom_hv_cust, 53)","f579ad8d":"plotScatterMatrix(telecom_hv_cust, 20, 10)","b10f0e22":"# create a new colulmn, which would be average  of 6th & 7th months\n# lets first create list of columns belonging to 6th and 7th months\ncol_list = telecom_hv_cust.filter(regex='_6|_7').columns.str[:-2]\ncol_list.unique()\n\nprint (telecom_hv_cust.shape)\n\n# lets take the average now\nfor idx, col in enumerate(col_list.unique()):\n    avg_col_name = \"avg_\"+col+\"_av67\" # lets create the column name dynamically\n    col_6 = col+\"_6\"\n    col_7 = col+\"_7\"\n    telecom_hv_cust[avg_col_name] = (telecom_hv_cust[col_6]  + telecom_hv_cust[col_7])\/ 2\n\n","62597341":"# we dont need columns from which we have derived new features, we will drop those columns\nprint (\"dimension of the updated dataset after creating dervied features:\",telecom_hv_cust.shape)\ncol_to_drop = telecom_hv_cust.filter(regex='_6|_7').columns\ntelecom_hv_cust.drop(col_to_drop, axis=1, inplace=True)\n\nprint(\"dimension of the dataset after dropping un-necessary columns:\",telecom_hv_cust.shape)","45b90b24":"# lets now conevrt AON in months\ntelecom_hv_cust['aon_mon'] = telecom_hv_cust['aon']\/30\ntelecom_hv_cust.drop('aon', axis=1, inplace=True)\ntelecom_hv_cust['aon_mon'].head()","1dbe4506":"# lets again draw the plots with the updated dataset\nplotPerColumnDistribution(telecom_hv_cust, 10, 5)","ac6259ac":"plotCorrelationMatrix(telecom_hv_cust, 53)","db3e38cb":"plotScatterMatrix(telecom_hv_cust, 20, 10)","27db0181":"ax = sns.distplot(telecom_hv_cust['aon_mon'], hist=True, kde=False, \n             bins=int(180\/5), color = 'purple', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 10})\nax.set_ylabel('No of Customers')\nax.set_xlabel('Tenure in months')\nax.set_title('Tenure Graph')\n# below graph simply shows the tenure of the customers","e6c701ac":"tn_range = [0, 6, 12, 24, 60, 61]\ntn_label = [ '0-6 Months', '6-12 Months', '1-2 Yrs', '2-5 Yrs', '5 Yrs and above']\ntelecom_hv_cust['tenure_range'] = pd.cut(telecom_hv_cust['aon_mon'], tn_range, labels=tn_label)\ntelecom_hv_cust['tenure_range'].head()","0eea3686":"# lets check correlation of churn with other columns\nplt.figure(figsize=(20,10))\ntelecom_hv_cust.corr()['churn'].sort_values(ascending = False).plot(kind='bar')\n\n# observations : \n# 1. Avg Outgoing Calls & calls on romaning for 6 & 7th months are positively correlated with churn. \n# 2. Avg Revenue, No. Of Recharge for 8th month has negative correlation with churn.","59e81254":"# lets now draw a scatter plot between total recharge and avg revenue for the 8th month\ntelecom_hv_cust[['total_rech_num_8', 'arpu_8']].plot.scatter(x = 'total_rech_num_8',\n                                                              y='arpu_8')","35fd0149":"# plot between tenure and revenue\ntelecom_hv_cust[['aon_mon', 'avg_arpu_av67']].plot.scatter(x = 'aon_mon',\n                                                              y='avg_arpu_av67')","d59a3b7a":"sns.boxplot(x = telecom_hv_cust.churn, y = telecom_hv_cust.aon_mon)\n\n# from the below plot , its clear tenured customers do no churn and they keep availing telecom services","6b8ee3f0":"# churn Vs Base Cost\nax = sns.kdeplot(telecom_hv_cust.avg_max_rech_amt_av67[(telecom_hv_cust[\"churn\"] == 0)],\n                color=\"Red\", shade = True)\nax = sns.kdeplot(telecom_hv_cust.avg_max_rech_amt_av67[(telecom_hv_cust[\"churn\"] == 1)],\n                ax =ax, color=\"Green\", shade= True)\nax.legend([\"No-Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('Volume based cost')\nax.set_title('Churn Vs Base Cost')","77283be8":"# churn vs max rechare amount\nax = sns.kdeplot(telecom_hv_cust.max_rech_amt_8[(telecom_hv_cust[\"churn\"] == 0)],\n                color=\"Red\", shade = True)\nax = sns.kdeplot(telecom_hv_cust.max_rech_amt_8[(telecom_hv_cust[\"churn\"] == 1)],\n                ax =ax, color=\"Blue\", shade= True)\nax.legend([\"No-Churn\",\"Churn\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('Volume based cost')\nax.set_title('Distribution of Max Recharge Amount by churn')","fb4f33d3":"# we will create a new dataset for model building\ndf = telecom_hv_cust[:].copy()\n\n# lets drop tenure range because it is highly correlated with AON MONTH column\ndf.drop('tenure_range', axis=1, inplace=True)\ndf.drop('mobile_number', axis=1, inplace=True)\ndf.head()","1c049d05":"# lets create X & y dataset for model building, X will obviously not have \"churn\" and y will only have \"churn\"\nX = df.drop(['churn'], axis=1)\ny = df['churn']\n\ndf.drop('churn', axis=1, inplace=True)\n","9a03f781":"# apply scaling on the dataset\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = preprocessing.StandardScaler().fit(X)\nX = scaler.transform(X)","9ea7bbff":"# split the dateset into train and test datasets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)\nprint(\"Dimension of X_train:\", X_train.shape)\nprint(\"Dimension of X_test:\", X_test.shape)","a2875979":"# As discussed earlier, given dataset is skewed, lets balance the dataset\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(kind = \"regular\")\nX_tr,y_tr = sm.fit_sample(X_train,y_train)","b7ac2fae":"print(\"Dimension of X_tr Shape:\", X_tr.shape)\nprint(\"Dimension of y_tr Shape:\", y_tr.shape)\n\nprint(\"Imbalance in Training dataset:\",(y_tr != 0).sum()\/(y_tr == 0).sum())","a60db92c":"# Model Building\n# SVM (lets start with linear SVM)\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n\nlr = LogisticRegression()\n\nlr.svm = SVC(kernel='linear') \nlr.svm.fit(X_train,y_train)\npreds = lr.svm.predict(X_test)\nmetrics.accuracy_score(y_test, preds)\n\n# linear SVM gave us accuracy of 94% on test data","6c241173":"# we will now using RFE for feature reduction\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nfrom sklearn.feature_selection import RFE\n\n# lets RFE select 15 most imp features for us\nrfe = RFE(lr, 15)   \nrfe = rfe.fit(X_tr, y_tr)","0e2a3dca":"rfe_features = list(df.columns[rfe.support_])\nprint(\"15 most important features selected by RFE \", rfe_features)","e5e1368b":"X_rfe = pd.DataFrame(data=X_tr).iloc[:, rfe.support_]\ny_rfe = y_tr","682e1eae":"# lets create a Logisctic Regression model on the seleted columns by RFE\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=1)\nlr.fit(X_rfe, y_rfe)","3eb11291":"X_test_rfe = pd.DataFrame(data=X_test).iloc[:, rfe.support_]\n\ny_pred = lr.predict(X_test_rfe)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)\nprint('Accuracy on the test dataset:',lr.score(X_test_rfe, y_test))","2cc7014a":"# lets check classification report on the test dataset\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","c13c4607":"# PCA\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)\n\n# apply SMOTE to correct class imbalance\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(kind = \"regular\")\nX_tr,y_tr = sm.fit_sample(X_train,y_train)\nprint(X_tr.shape)\nprint(y_tr.shape)\n","b6be635d":"# import PCA\nfrom sklearn.decomposition import PCA\npca = PCA(random_state=100)\n\n# apply PCA on train data\npca.fit(X_tr)","1ed49fe6":"X_tr_pca = pca.fit_transform(X_tr)\nprint(X_tr_pca.shape)\n\nX_test_pca = pca.transform(X_test)\nprint(X_test_pca.shape)","ed925a2b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlr_pca = LogisticRegression(C=1e9)\nlr_pca.fit(X_tr_pca, y_tr)\n\n# make the predictions\ny_pred = lr_pca.predict(X_test_pca)\n\n# convert prediction array into a dataframe\ny_pred_df = pd.DataFrame(y_pred)","76da35f2":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Printing confusion matrix\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy of the logistic regression model with PCA: \",accuracy_score(y_test,y_pred))","9f23ff39":"col = list(df.columns)\ndf_pca = pd.DataFrame({'PC-1':pca.components_[0],'PC-2':pca.components_[1], 'PC-3':pca.components_[2],'Feature':col})\ndf_pca.head(10)","5f9f44ee":"# scree plot to check the variance explained by different PCAs\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('no of principal components')\nplt.ylabel('explained variance - cumulative')\nplt.show()","788045af":"np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n# 33 columns explains 90% of the variance, lets apply PCA with 33 components","adb6aa15":"# PCA with 33 components\npca_33 = PCA(n_components=33)\n\ndf_tr_pca_33 = pca_33.fit_transform(X_tr)\nprint(df_tr_pca_33.shape)\n\ndf_test_pca_33 = pca_33.transform(X_test)\nprint(df_test_pca_33.shape)","f5beaf31":"# Let's run the model using the selected variables\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlr_pca1 = LogisticRegression(C=1e9)\nlr_pca1.fit(df_tr_pca_33, y_tr)\n\n# Predicted probabilities\ny_pred33 = lr_pca1.predict(df_test_pca_33)\n\n# Converting y_pred to a dataframe which is an array\ndf_y_pred = pd.DataFrame(y_pred33)\n\nprint(\"Accuracy with 33 PCAs: \",accuracy_score(y_test,y_pred33))","6260103e":"print(confusion_matrix(y_test,y_pred33))","5279bcb9":"# lets create a decision tree now\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)\n\n# apply SMOTE to tackle class imbalance\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(kind = \"regular\")\nX_tr,y_tr = sm.fit_sample(X_train,y_train)\nprint(X_tr.shape)\nprint(y_tr.shape)","b6489978":"# feature selection using lasso\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n \nsvc = LinearSVC(C=0.001, penalty=\"l1\", dual=False).fit(X_tr, y_tr)\nsvc_model = SelectFromModel(svc, prefit=True)\nX_lasso = svc_model.transform(X_tr)\nposition = svc_model.get_support(indices=True)\n\nprint(X_lasso.shape)\nprint(position)","b0cb1a76":"# feature vector for decision tree\nlasso_features = list(df.columns[position])\nprint(\"Lasso Features: \", lasso_features)","0b9671c3":"# import decision tree libraries\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\n# lets create a decision tree with the default hyper parameters except max depth to make the tree readable\ndt1 = DecisionTreeClassifier(max_depth=5)\ndt1.fit(X_lasso, y_tr)","a3a91481":"# lets see the classification reort of the model built\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Model predictions\nX_test = pd.DataFrame(data=X_test).iloc[:, position]\ny_pred1 = dt1.predict(X_test)\n\n# classification report\nprint(classification_report(y_test, y_pred1))","cdb42d15":"# confusion matrix\nprint(confusion_matrix(y_test,y_pred1))\n# accuracy of the decision tree\nprint('Decision Tree - Accuracy :',accuracy_score(y_test,y_pred1))","98fb1253":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(1, 40)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n                               \n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                   return_train_score=True)\ntree.fit(X_lasso, y_tr)","fafb8cc5":"# grid search results\nscore = tree.cv_results_\npd.DataFrame(score).head()","90176c52":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(score[\"param_max_depth\"], \n         score[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(score[\"param_max_depth\"], \n         score[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n\n# max_depth =10 seems to be the optimal one","f0b9980c":"# lets find optimal value of minimum sample leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                   return_train_score=True)\ntree.fit(X_lasso, y_tr)","d947195e":"# grid search results\nscore = tree.cv_results_\npd.DataFrame(score).head()","9bcb3ed3":"# plotting accuracies with min_sample_leaf\nplt.figure()\nplt.plot(score[\"param_min_samples_leaf\"], \n         score[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(score[\"param_min_samples_leaf\"], \n         score[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_sample_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n# min_sample_leaf =25 seems to be the optimal one","62313240":"# lets fine tune min sample split now\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                   return_train_score=True)\ntree.fit(X_lasso, y_tr)","bc20b39f":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","2ee5af1e":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n# min_samples_leaf=50 seems to be optimal","7a47fb64":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(25, 175, 50),\n    'min_samples_split': range(50, 150, 50),\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nn_folds = 5\n\n# Instantiate the grid search model\ndtree = DecisionTreeClassifier()\ngrid_search = GridSearchCV(estimator = dtree, param_grid = param_grid, \n                          cv = n_folds, verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_lasso, y_tr)","ddaa51a8":"# cv results\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results\n# printing the optimal accuracy score and hyperparameters\nprint(\"Best Accuracy\", grid_search.best_score_)\n","a6c2a1ce":"print(grid_search.best_estimator_)","9cb2a9b8":"# model with optimal hyperparameters\nclf_gini = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=5, \n                                  min_samples_leaf=25,\n                                  min_samples_split=50)\nclf_gini.fit(X_lasso, y_tr)","28d17e64":"# accuracy score\nprint ('Accuracy Score for Decision Tree Final Model :',clf_gini.score(X_test,y_test))","f95085b9":"# Conclusion from the above Decision Tree model\n# 1. 85% accuracy on the test dataset\n# 2. lots of false positives in the confusion matrix","04a578fc":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_lasso, y_tr)\n\n# Make predictions\nprediction_test = model_rf.predict(X_test)\nprint ('Randon Forest Accuracy with Default Hyperparameter',metrics.accuracy_score(y_test, prediction_test))","72c51d5e":"print(classification_report(y_test,prediction_test))","c5c6160d":"# Printing confusion matrix\nprint(confusion_matrix(y_test, prediction_test))","ecb334ab":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(2, 20, 5)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                  return_train_score=True)\nrf.fit(X_lasso, y_tr)","8692c5e9":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","4549986d":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","b86ad3b6":"##Tuning n_estimators\n## GridSearchCV to find optimal n_estimators\n#from sklearn.model_selection import KFold\n## specify number of folds for k-fold CV\nn_folds = 5\n#\n## parameters to build the model on\nparameters = {'n_estimators': range(100, 1500, 400)}\n#\n## instantiate the model (note we are specifying a max_depth)\nrf = RandomForestClassifier(max_depth=4)\n#\n#\n## fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                  return_train_score=True)\nrf.fit(X_lasso, y_tr)","cca7f7f5":"## scores of GridSearch CV\nscores = rf.cv_results_\n#\n## plotting accuracies with n_estimators\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","6d620555":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(50, 400, 10)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                  return_train_score=True)\nrf.fit(X_lasso, y_tr)","d2022266":"# scores of GridSearch CV\nscores = rf.cv_results_\n\n# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","ed5165cc":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(100, 500, 25)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                   return_train_score=True)\nrf.fit(X_lasso, y_tr)","611a23df":"# scores of GridSearch CV\nscores = rf.cv_results_\n\n# plotting accuracies with min_samples_split\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","ff38d22e":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 300, 100),\n    'min_samples_split': range(200, 500, 100),\n    'n_estimators': [500,700], \n    'max_features': [10,20,25]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)","7c4d03cb":"# fit the grid search with the data\ngrid_search.fit(X_lasso, y_tr)\n# optimal accuracy score and hyperparameters\nprint('Accuracy is',grid_search.best_score_,'using',grid_search.best_params_)","22f541b6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n\nmodel_rf = RandomForestClassifier(bootstrap=True,\n                                  max_depth=10,\n                                  min_samples_leaf=100, \n                                  min_samples_split=200,\n                                  n_estimators=1000 ,\n                                  oob_score = True, n_jobs = -1,\n                                  random_state =50,\n                                  max_features = 15,\n                                  max_leaf_nodes = 30)\nmodel_rf.fit(X_train, y_train)\n\n# Make predictions\nprediction_test = model_rf.predict(X_test)","46e97b18":"# evaluation metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,prediction_test))\nprint(confusion_matrix(y_test,prediction_test))","9711467e":"# accuracy score\nprint ('Accuracy Score for Random Forest Final Model :',metrics.accuracy_score(y_test, prediction_test))","20f05dce":"# list of important features\nX = df\nfeatures = X.columns.values\nX = pd.DataFrame(scaler.transform(X))\nX.columns = features\n\nimportances = model_rf.feature_importances_\nweights = pd.Series(importances,\n                 index=X.columns.values)\nweights.sort_values()[-10:].plot(kind = 'barh')","23c43965":"Conclusions from the above model:\n\nModel has 80% Accuracy\n33 features can explain 90% variance in the dataset\nmost imp features: arpu_8,onnet_mou_8,offnet_mou_8,roam_ic_mou_8,roam_og_mou_8","ad206b19":"# PCA","a7da95e0":"# Telecom Churn - Case Stud","88319f61":"# Churn Vs other important features","aac5eda6":"# Overall Conclusions\n\n1. Std Outgoing Calls and Revenue Per Customer are strong indicators of Churn.\n2. Local Incoming and Outgoing Calls for 8th Month and avg revenue in 8th Month are the most important columns to predict churn.\n3. cutomers with tenure less than 4 yrs are more likely to churn.\n4. Max Recharge Amount is a strong feature to predict churn.\n6. Random Forest produced the best prediction results followed by SVM.\n","6ed12464":"# Filter High Value Customers","b7c3d7a6":"Conclusion from the above model:\n\nModel Accuracy is approx 79%\nConfusion matix shows high false positive rate, which is not good, lets try PCA now.","9b82e827":"# Conclusions from Random Forest\n\n1. Local Incoming for Month 8, Average Revenue Per Customer for Month 8 and Max Recharge Amount for Month 8 are the most important predictor variables to predict churn.","6048a260":"# Lets fine tune hyperparameters","f4338f6c":"# Random Forest","d9a67511":"# Derive some new feautres from the existing columns","f0073cc0":"# Decision Tree"}}