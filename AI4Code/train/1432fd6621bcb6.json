{"cell_type":{"91e49fd0":"code","38359347":"code","5856580f":"code","827bedf2":"code","ad0ea50d":"code","1fb35549":"code","b83db7f3":"code","cf3e645a":"code","2d0bca68":"code","de7213f8":"code","e275fbda":"code","837ad40d":"code","37c53410":"code","4965db31":"markdown","bd07b48c":"markdown","193176c7":"markdown","73bbe8ec":"markdown","adfb23ad":"markdown","0b04c34b":"markdown","378b238f":"markdown","b900e2a3":"markdown"},"source":{"91e49fd0":"import os\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm, trange\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertPreTrainedModel, BertModel\n\nfrom transformers import AutoConfig, AutoTokenizer","38359347":"MODEL_OUT_DIR = '\/kaggle\/working\/models\/my_model'\nTRAIN_FILE_PATH = '..\/input\/sentiment-analysis-fine-grained5-classes\/FineGrainedForSentiment\/train.tsv'\nVALID_FILE_PATH = '..\/input\/sentiment-analysis-fine-grained5-classes\/FineGrainedForSentiment\/test.tsv'\n## Model Configurations\nMAX_LEN_TRAIN = 30\nMAX_LEN_VALID = 30\nBATCH_SIZE = 32\nLR = 2e-5\nNUM_EPOCHS = 15\nNUM_THREADS = 1  ## Number of threads for collecting dataset\nMODEL_NAME = 'bert-base-uncased'\n\n\nif not os.path.isdir(MODEL_OUT_DIR):\n    os.makedirs(MODEL_OUT_DIR)\n","5856580f":"class SSTDataset(Dataset):\n\n    def __init__(self, filename, maxlen, tokenizer): \n        #Store the contents of the file in a pandas dataframe\n        self.df = pd.read_csv(filename, delimiter = '\\t')\n        #Initialize the tokenizer for the desired transformer model\n        self.tokenizer = tokenizer\n        #Maximum length of the tokens list to keep all the sequences of fixed size\n        self.maxlen = maxlen\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):    \n        #Select the sentence and label at the specified index in the data frame\n        sentence = self.df.loc[index, 'sentence']\n        label = self.df.loc[index, 'label']\n        #Preprocess the text to be suitable for the transformer\n        tokens = self.tokenizer.tokenize(sentence) \n        tokens = ['[CLS]'] + tokens + ['[SEP]'] \n        if len(tokens) < self.maxlen:\n            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] \n        else:\n            tokens = tokens[:self.maxlen-1] + ['[SEP]'] \n        #Obtain the indices of the tokens in the BERT Vocabulary\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens) \n        input_ids = torch.tensor(input_ids) \n        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n        attention_mask = (input_ids != 0).long()\n        \n        label = torch.tensor(label, dtype=torch.long)\n        \n        return input_ids, attention_mask, label","827bedf2":"class BertForSentimentClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        #The classification layer that takes the [CLS] representation and outputs the logit\n        self.cls_layer = nn.Linear(config.hidden_size, 5)\n\n    def forward(self, input_ids, attention_mask):\n        #Feed the input to Bert model to obtain contextualized representations\n        reps, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        #Obtain the representations of [CLS] heads\n        cls_reps = reps[:, 0]\n        # cls_reps = self.dropout(cls_reps)\n        logits = self.cls_layer(cls_reps)\n        return logits\n","ad0ea50d":"\ndef train(model, criterion, optimizer, train_loader, val_loader, epochs):\n    best_acc = 0\n    for epoch in trange(epochs, desc=\"Epoch\"):\n        model.train()\n        train_acc = 0\n        for i, (input_ids, attention_mask, labels) in enumerate(iterable=train_loader):\n#         for i, (input_ids, attention_mask, labels) in enumerate(tqdm(iterable=train_loader, desc=\"Training\")):\n            optimizer.zero_grad()  \n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n#             print(logits)\n#             loss = criterion(input=logits.squeeze(-1), target=labels.float())\n            loss = criterion(logits, labels)\n            loss.backward()\n            optimizer.step()\n            train_acc += get_accuracy_from_logits(logits, labels)\n        print(f\"Training accuracy is {train_acc\/len(train_loader)}\")\n        val_acc, val_loss = evaluate(model=model, criterion=criterion, dataloader=val_loader, device=device)\n        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(epoch, val_acc, val_loss))\n        if val_acc > best_acc:\n            print(\"Best validation accuracy improved from {} to {}, saving model...\".format(best_acc, val_acc))\n            best_acc = val_acc\n            model.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n            config.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n            tokenizer.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n","1fb35549":"\ndef evaluate(model, criterion, dataloader, device):\n    model.eval()\n    mean_acc, mean_loss, count = 0, 0, 0\n    with torch.no_grad():\n#         for input_ids, attention_mask, labels in tqdm(dataloader, desc=\"Evaluating\"):\n        for input_ids, attention_mask, labels in (dataloader):\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            logits = model(input_ids, attention_mask)\n            mean_loss += criterion(logits.squeeze(-1), labels).item()\n            mean_acc += get_accuracy_from_logits(logits, labels)\n            count += 1\n    return mean_acc \/ count, mean_loss \/ count","b83db7f3":"def get_accuracy_from_logits(logits, labels):\n    probs = F.softmax(logits, dim=1)\n    output = torch.argmax(probs, dim=1)\n    #Convert probabilities to predictions, 1 being positive and 0 being negative\n    #Check which predictions are the same as the ground truth and calculate the accuracy\n    acc = (output == labels).float().mean()\n    return acc","cf3e645a":"## Configuration loaded from AutoConfig \nconfig = AutoConfig.from_pretrained(MODEL_NAME)\n## Tokenizer loaded from AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n## Creating the model from the desired transformer model\nmodel = BertForSentimentClassification.from_pretrained(MODEL_NAME, config=config)\n## GPU or CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n## Putting model to device\nmodel = model.to(device)\n## Takes as the input the logits of the positive class and computes the binary cross-entropy \n# criterion = nn.BCEWithLogitsLoss()\ncriterion = nn.CrossEntropyLoss()\n## Optimizer\noptimizer = optim.Adam(params=model.parameters(), lr=LR)\n","2d0bca68":"## Training Dataset\ntrain_set = SSTDataset(filename=TRAIN_FILE_PATH, maxlen=MAX_LEN_TRAIN, tokenizer=tokenizer)\nvalid_set = SSTDataset(filename=VALID_FILE_PATH, maxlen=MAX_LEN_VALID, tokenizer=tokenizer)","de7213f8":"## Data Loaders\ntrain_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\nvalid_loader = DataLoader(dataset=valid_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\nprint(len(train_loader))","e275fbda":"train(model=model, \n      criterion=criterion,\n      optimizer=optimizer, \n      train_loader=train_loader,\n      val_loader=valid_loader,\n      epochs = NUM_EPOCHS\n      )","837ad40d":"def classify_sentiment(sentence):\n    with torch.no_grad():\n        tokens = tokenizer.tokenize(sentence)\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n        input_ids = torch.tensor(input_ids).to(device)\n        input_ids = input_ids.unsqueeze(0)\n        attention_mask = (input_ids != 0).long()\n        attention_mask = attention_mask.to(device)\n        logit = model(input_ids=input_ids, attention_mask=attention_mask)\n        prob = F.softmax(logit, dim=1)\n        output = torch.argmax(prob)\n        print(logit)\n        print(prob)\n        print(output)\n        prob = prob[0][output]\n        if output == 0:\n            print('Extreme Negative {}'.format(int(prob*100)))\n        elif output == 1:\n            print('Negative {}'.format(int(prob*100)))\n        elif output == 2:\n            print('Neutral {}'.format(int(prob*100)))\n        elif output == 3:\n            print('Positive {}'.format(int(prob*100)))\n        else:\n            print('Extreme positve {}'.format(int(100-prob*100)))","37c53410":"sentence = \"Hope you enjoyed this notebook. Don't forget to give me an upvote\"\nclassify_sentiment(sentence)","4965db31":"## Creating Dataset","bd07b48c":"## Building Model","193176c7":"## Create Analysis","73bbe8ec":"Here we are using AutoConfig and AutoTokenizer <> Give the name of the model we want to use and the config and tokenizer for the model will be loaded accordingly. [Check this out](https:\/\/huggingface.co\/transformers\/model_doc\/auto.html)","adfb23ad":"## Train Function","0b04c34b":"## Imports and Libraries","378b238f":"## Configurations","b900e2a3":"\n## Evaluate Function"}}