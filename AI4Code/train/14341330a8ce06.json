{"cell_type":{"85318256":"code","27d22542":"code","c1e1cab3":"code","b432755c":"code","87c51ea9":"code","5f80081b":"code","a5e82f6a":"code","c591ed6b":"code","67a107fc":"code","7fc72bd2":"code","5d28ca9d":"code","e3cb5dc3":"code","42a542bd":"code","2bca6d4d":"code","750741de":"code","9f62571e":"code","1122b622":"code","13083d25":"code","e9b123b8":"code","491c6838":"code","3c0d09ac":"code","368a0265":"code","917c849f":"code","d136d575":"code","7f545d5c":"code","43e22937":"code","a54689cc":"code","4e6a2c07":"code","4959972e":"code","1491afa7":"code","8fa5bf8f":"code","47a07993":"code","9b0d5502":"code","1b6c10e5":"code","1a3791e6":"code","35a6f927":"code","2a9b44c3":"code","d9b7fc73":"code","61742910":"code","cfd18150":"code","c623749f":"code","939d6dcd":"code","c0b16c58":"code","099c072b":"code","18edab7d":"code","8344f73b":"code","e0969f49":"code","9203e3a7":"code","2e097e20":"code","f4f2c178":"code","5df59a9b":"code","b2bc3376":"code","0b03b5ae":"code","364bd32e":"code","011badad":"code","401893d1":"code","78e8b9f7":"code","8328ce66":"code","87ee6b41":"code","5f92a4d7":"code","52540117":"code","ec764f67":"code","b13e06e9":"code","d0e15a57":"code","10c4b17a":"code","08d6de2d":"code","ec4874f4":"code","f71db767":"code","cfd3eda9":"code","c2530248":"code","8aa323c9":"code","f7fc2f8a":"code","cfd4355f":"code","914b499c":"code","665fa309":"markdown","0f04416e":"markdown","564924cd":"markdown","4d670439":"markdown","94fb776f":"markdown","17652514":"markdown","ef74acb1":"markdown","5f63f3ee":"markdown","326259f3":"markdown","87fc9787":"markdown","227665f9":"markdown","8a6b4d31":"markdown","6ff09fe9":"markdown","0b8a003f":"markdown","ced4b3d0":"markdown","0729de9c":"markdown","637988be":"markdown","5ea2572f":"markdown","ed1ade7e":"markdown","20edeb6d":"markdown","bec225b4":"markdown","5a1064ca":"markdown","3fe824c1":"markdown","8bcfc62d":"markdown","39abf1c2":"markdown","7b5f609f":"markdown","7875a229":"markdown","15cdfc04":"markdown","830bf738":"markdown","1978f115":"markdown","0d80d7b4":"markdown","944e0258":"markdown","b81a9dd0":"markdown"},"source":{"85318256":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","27d22542":"data_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nId = data_test['Id']","c1e1cab3":"data_train.info()","b432755c":"for col in list(data_test.columns):\n    if data_train[col].dtype=='object':\n        if data_train[col].nunique() != data_test[col].nunique():\n            print(col,\":\", data_train[col].nunique(), \"uniques in train\", data_test[col].nunique(), \"uniques in test\")\n#due to different number of unique values we have to work with both datasets together","87c51ea9":"q97 = data_train.SalePrice.quantile(0.99)\nq003 = data_train.SalePrice.quantile(0.003)\n\nprint('Removed outliers, price values in: [{:.2f}; {:.2f}]'.format(0, q97)) #best was 0.99\ndata_train = data_train[(data_train.SalePrice < q97)].reset_index()","5f80081b":"Target = data_train.SalePrice\ndata_train=data_train.drop('SalePrice',axis=1)\n#saving target variable","a5e82f6a":"# Join the train and the test set\ndata = pd.concat([data_train, data_test], keys = ['train', 'test'])\ndata = data.drop(['index', 'Id'],axis = 1)\nfeatures = list(data.columns)\nprint('detected {:d} features'.format(len(features)))","c591ed6b":"Check_years = data.columns[data.columns.str.contains(pat = 'Year|Yr')] \ndata[Check_years.values].max().sort_values(ascending = False)","67a107fc":"Replace_year = data.loc[(data['GarageYrBlt'] > 2050), 'GarageYrBlt'].index.tolist()\ndata.loc[Replace_year, 'GarageYrBlt'] = data['GarageYrBlt'].mode()","7fc72bd2":"Na_perc = []\nfor feature in features:\n    percentage = len(data[feature][data[feature].isna() == True]) \/ len(data[feature])\n    Na_perc.append(percentage)\nNas = pd.DataFrame({'feature' : features, 'Na_perc': Na_perc}).sort_values(by = 'Na_perc', ascending = False)\nNas = Nas[Nas.Na_perc > 0.01]\n \nplt.figure(figsize=(7,7))\nplt.title('NaN percentage')\nplt.xlabel(\"Top features\")\nsns.barplot(x = Nas.Na_perc, y = Nas.feature, orient = 'h')\nplt.show()","5d28ca9d":"num_features=[]\ncat_features=[]\nord_features=[]\nfor feature in features:\n    if str(data[feature].dtype) == 'object':\n        data[feature] = data[feature].fillna('None')\n        #data_test[feature] = data_test[feature].fillna('None')\n        cat_features.append(feature)\n        pass\n    \n    if  str(data[feature].dtype)=='float64':\n        if data[feature].nunique() < 50:\n            ord_features.append(feature)\n            #data[feature] = data[feature].fillna(data[feature].mean())\n        else:\n            #data[feature] = data[feature].fillna(int(data[feature].mean()))\n            num_features.append(feature)\n        pass\n    \n    if str(data[feature].dtype) == 'int64':\n        #data_test[feature] =data_test[feature]\n        if data[feature].nunique() < 50:\n            ord_features.append(feature)\n            #data[feature] = data[feature].fillna(data[feature].mean())\n        else:\n            num_features.append(feature)\n            #data[feature] = data[feature].fillna(int(data[feature].mean()))\n        \n        pass\n    ","e3cb5dc3":"from sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import RobustScaler\nnum_ord_data = data[num_features + ord_features].copy()\nscaler = RobustScaler()\nnum_ord_data = scaler.fit_transform(num_ord_data)\nimputer = KNNImputer(n_neighbors = 10, weights = 'uniform')\nnum_ord_data = imputer.fit_transform(num_ord_data)\nnum_ord_data_transformed = pd.DataFrame(num_ord_data, columns = num_features + ord_features)\n#num_ord_data = scaler.inverse_transform(num_ord_data_transformed)\ntrans_back = scaler.inverse_transform(num_ord_data_transformed)\nnum_ord_data_transformed = pd.DataFrame(trans_back, columns = num_features + ord_features)","42a542bd":"data = data.reset_index()\nfor col in num_features + ord_features:\n    data[col] = num_ord_data_transformed[col]","2bca6d4d":"print(\"Numeric features:\", len(num_features))\nprint(\"Ordinal features:\", len(ord_features))\nprint(\"Categorical features:\", len(cat_features))\nif len(num_features) + len(ord_features) + len(cat_features) != len(features):\n    print(\"missing features\")\nelse:\n    print('no missing,', len(num_features) + len(ord_features) + len(cat_features), \"features\")","750741de":"max_list_cat = []\nmax_list_ord = []\nidx_max_cat = []\nidx_max_ord = []\n\nfor feature in cat_features:\n    cat_vals = data[feature].value_counts(normalize=True)\n    max_list_cat.append(cat_vals.max())\n    idx_max_cat.append(cat_vals.idxmax())\n    \nmax_zero_percentage_num = []\nfor feature in num_features:\n    percentage = data[feature][data[feature] == 0].count() \/ len(data[feature])\n    max_zero_percentage_num.append(percentage)\n\nmax_zero_percentage_ord = []\nfor feature in ord_features:\n    ord_vals = data[feature].value_counts(normalize=True)\n    max_list_ord.append(ord_vals.max())\n    idx_max_ord.append(ord_vals.idxmax()) ","9f62571e":"zeroes_num = pd.DataFrame({'feature' : num_features, 'ratio' : max_zero_percentage_num}).sort_values(by = 'ratio', ascending=False)\nvals_ord = pd.DataFrame({'feature' : ord_features, 'ratio' : max_list_ord, 'top_value' : idx_max_ord}).sort_values(by = 'ratio', ascending = False)\nsingl_el = pd.DataFrame({'feature' : cat_features, 'ratio' : max_list_cat, 'top_value' : idx_max_cat}).sort_values(by = 'ratio', ascending = False)\n\nfig = plt.figure(figsize=(15,10))\n\nplt.subplot(1, 3, 1)\nplt.title('Max zero ratio in numeric data')\nplt.xlabel(\"Top features\")\nsns.barplot(x = zeroes_num.ratio, y = zeroes_num.feature, orient = 'h')\n\nplt.subplot(1, 3, 2)\nplt.title('Max zero ratio in numeric data')\nplt.xlabel(\"Top features\")\nsns.barplot(x = vals_ord.ratio, y = vals_ord.feature, orient = 'h')\n\nplt.subplot(1, 3, 3)\nplt.title('Max single element ratio')\nplt.xlabel(\"Top features\")\nsns.barplot(x = singl_el.ratio, y = singl_el.feature, orient = 'h')\nplt.subplots_adjust( bottom=None, right=None, top=None, wspace=1, hspace=None)\nplt.show()\n ","1122b622":"data['TotalPorch'] = (data['ScreenPorch'] + data['EnclosedPorch'] + \n                         data['3SsnPorch'] + data['ScreenPorch'])\n\ndata['Rooms_kitchens'] = (data['TotRmsAbvGrd'] + data['BsmtFullBath'] + \n                             data['BsmtHalfBath'] + data['FullBath'] + \n                             data['HalfBath'])\n\ndata['Sqr_feet_per_room'] = ((data['1stFlrSF'] + \n                                 data['2ndFlrSF']) \/ data['TotRmsAbvGrd'])\ndata['lotfr_lotarea'] = (data['LotFrontage'] + data['LotArea']) \/ 2\n\ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n\ndata['cond_qual'] = (data['OverallCond'] * data['OverallQual'])\ndata['HouseAge'] = data['YrSold'] - data['YearBuilt'] + 1\n#data['TotalPorch'].fillna(data['TotalPorch'].mean())\n#data['Rooms_kitchens'].fillna(data['Rooms_kitchens'].mode())\n\n\nnum_features.append('TotalSF')\nnum_features.append('TotalPorch')\nnum_features.append('lotfr_lotarea')\nnum_features.append('HouseAge')\nord_features.append('Rooms_kitchens')\nord_features.append('cond_qual')\n","13083d25":"imb_num_features = zeroes_num.feature.head(3)\nimb_ord_features = vals_ord[['feature', 'top_value']].head(5)\nimb_cat_features = singl_el.feature[singl_el.top_value == 'None'].head(16)","e9b123b8":"for feature in imb_num_features:\n    data[feature][data[feature] != 0] = 0\n    data[feature][data[feature] == 0] = 1\n    num_features.remove(feature)\n    cat_features.append(feature)\nfor feature in imb_ord_features.feature:\n    data[feature][data[feature] != 0] = 0\n    data[feature][data[feature] == 0] = 1\n    ord_features.remove(feature)\n    cat_features.append(feature)\nfor feature in imb_cat_features:\n    data[feature][data[feature] != 'None'] = 'Present'","491c6838":"print(\"Numeric features:\", len(num_features))\nprint(\"Ordinal features:\", len(ord_features))\nprint(\"Categorical features:\", len(cat_features))\nif len(num_features) + len(ord_features) + len(cat_features) != len(features):\n    print(\"missing features\")\nelse:\n    print('no missing,', len(num_features) + len(ord_features) + len(cat_features), \"features\")","3c0d09ac":"for feature in cat_features:\n    ratio = data[feature][data[feature] == 'None'].count() \/ len(data[feature])\n    if ratio > 0.75: #best was 0.75\n        cat_features.remove(feature)\n        print('removed', feature)\nfor feature in ord_features:\n    ratio = data[feature][data[feature] == 0].count() \/ len(data[feature])\n    if ratio > 0.75: #best was 0.75\n        ord_features.remove(feature)\n        print('removed', feature)","368a0265":"for col in num_features:\n    data[col] = np.log(data[col] + 1)\nfor col in ord_features:\n    data[col] = np.log(data[col] + 1)","917c849f":"data_train = data[data.level_0 == 'train'].drop(['level_0', 'level_1'], axis = 1)\ndata_test = data[data.level_0 == 'test'].drop(['level_0', 'level_1'], axis = 1)","d136d575":"Target.hist(bins = 20)\nplt.show()\nTarget = np.log(Target + 1)\nTarget.hist(bins = 20)\nplt.show()","7f545d5c":"from matplotlib import cm\n\na = 8  # number of rows\nb = 3  # number of columns\nc = 1  # initialize plot counter\n\nfig = plt.figure(figsize=(21,35))\n\nfor feature in num_features:\n    plt.subplot(a, b, c)\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.scatter(data_train[feature], Target ,s = 2, label = feature, c=cm.cool(Target \/ Target.max()\/2))\n    c = c + 1  ","43e22937":"a = 5  # number of rows\nb = 3  # number of columns\nc = 1  # initialize plot counter\n\nfig = plt.figure(figsize=(20,25))\n\nfor feature in ord_features:\n    plt.subplot(a, b, c)\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.scatter(data_train[feature], Target ,s = 2, label = feature, c=cm.cool(Target \/ Target.max()\/2))\n    c = c + 1  ","a54689cc":"data_train['SalePrice'] = Target","4e6a2c07":"fig, ax = plt.subplots(figsize=(11,9)) \ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nmask  = np.triu(np.ones_like(data_train[num_features].corr(), dtype=bool))\nsns.heatmap(data_train[num_features].corr(),ax=ax,\n            cmap = cmap, square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n            mask=mask)","4959972e":"num_features.remove('GarageYrBlt')\nnum_features.remove('1stFlrSF')\nord_features.remove('MoSold')\nord_features.remove('YrSold')","1491afa7":"num_list = []\nord_list = []\nfor feature in num_features:\n    num_list.append(data_train[feature].corr(data_train.SalePrice))\n\nnumcorrs = pd.DataFrame({'num_feature' : num_features, 'corr': num_list}).sort_values(by = 'corr', ascending = False)\nfor feature in ord_features:\n    #print(feature,eda_data[feature].corr(eda_data.SalePrice))\n    ord_list.append(data_train[feature].corr(data_train.SalePrice))\n    #if abs(eda_data[feature].corr(eda_data.SalePrice)) < 0.1:\n    #    ord_features.remove(feature)\nordcorrs = pd.DataFrame({'ord_feature' : ord_features, 'corr': ord_list}).sort_values(by = 'corr', ascending = False)\nfig = plt.figure(figsize=(10,7))\nplt.subplot(1,2,1)\nplt.title('Correlation between numerical features and SalePrice')\nsns.barplot(x = numcorrs['corr'], y = numcorrs['num_feature'], orient = 'h')\nplt.subplot(1,2,2)\nplt.title('Correlation between ordinal features and SalePrice')\nsns.barplot(x = ordcorrs['corr'], y = ordcorrs['ord_feature'], orient = 'h')\nplt.subplots_adjust( bottom=None, right=None, top=None, wspace=1, hspace=None)\n    # compute correlation between sale price and other numeric features","8fa5bf8f":"from sklearn.linear_model import Lasso\ny_train = data_train['SalePrice']\ndata_train.drop('SalePrice', axis = 1)\ndata_train_dum = pd.get_dummies(data_train)\nlasso = Lasso()\nlasso.fit(data_train_dum, y_train)\nouts = ((lasso.predict(data_train_dum) - y_train)**2).to_frame().sort_values(by = 'SalePrice', ascending = False)\nbad_ind = outs[outs.SalePrice > 0.92].index.to_list() #best was 0.92\ndata_train = data_train.drop(bad_ind, axis = 0)\nTarget = Target.drop(bad_ind, axis = 0)","47a07993":"data = pd.concat([data_train, data_test], keys = ['train', 'test']).reset_index()\ndata = data.drop('level_1', axis = 1)","9b0d5502":"for col in num_features + ord_features:\n    data[col] = (data[col] - data[col].min()) \/ (data[col].max() - data[col].min())\n#for col in num_features + ord_features:\n#    data[col] = (data[col] - data[col].mean()) \/ data[col].std()","1b6c10e5":"final_features=num_features + cat_features + ord_features\nprint(\"Amount of features:\",len(final_features))\nfinal_data = data[final_features]","1a3791e6":"from itertools import combinations\n\nhigh_corr_num_features = ['GrLivArea', 'YearBuilt', 'YearRemodAdd', 'HouseAge']\nhigh_corr_ord_features = ['OverallQual', 'Rooms_kitchens', 'GarageCars', 'FullBath', 'Fireplaces', 'TotRmsAbvGrd']\n\nfor c_1, c_2 in combinations(final_data[high_corr_num_features], 2):\n    final_data['{0}x{1}'.format(c_1, c_2)] = final_data[c_1] * final_data[c_2]\n    final_data['{0}+{1}'.format(c_1, c_2)] = (final_data[c_1] + final_data[c_2]) \/ 2 \nfor c_1, c_2 in combinations(final_data[high_corr_ord_features], 2):\n    final_data['{0}x{1}'.format(c_1, c_2)] = final_data[c_1] * final_data[c_2]\n    #final_data['{0}+{1}'.format(c_1, c_2)] = (final_data[c_1] + final_data[c_2])\/2\n\n","35a6f927":"final_data_dummies=pd.get_dummies(final_data, drop_first = False)\nfinal_data_dummies['level']=data.level_0\nfinal_data_dummies_train=final_data_dummies[final_data_dummies.level=='train']\nfinal_data_dummies_test=final_data_dummies[final_data_dummies.level=='test']","2a9b44c3":"X = final_data_dummies_train.drop('level',axis=1)\ny = Target\nX_test = final_data_dummies_test.drop('level', axis = 1)","d9b7fc73":"def evaluate(base_model, tuned_model, X_train, y_train, test_features, test_labels, fit_params=None):\n    base_model.fit(X_train, y_train)\n    predictions = base_model.predict(test_features)\n    print('Model Performance')\n    print('MSE = {:0.4f}'.format(mean_squared_error(predictions, test_labels, squared = False)))\n    \n    base_mse = mean_squared_error(predictions, test_labels)\n    if 'XGBRegressor' in str(tuned_model):\n        tuned_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (test_features, test_labels)], **fit_params)\n    elif 'LGBMRegressor' in str(tuned_model):\n        tuned_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (test_features, test_labels)], **fit_params)\n    else:\n        tuned_model.fit(X_train, y_train, **fit_params)\n    predictions = tuned_model.predict(test_features)\n    print('Model Performance')\n    print('MSE = {:0.4f}'.format(mean_squared_error(predictions, test_labels, squared = False)))\n    tuned_mse = mean_squared_error(predictions, test_labels)\n    print('Improvement of {:0.2f}%.'.format( 100 * (base_mse - tuned_mse) \/ base_mse))\n    ","61742910":"from hyperopt import hp, fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n\nclass HPOpt(object):\n\n    def __init__(self, x_train, x_test, y_train, y_test):\n        self.x_train = x_train\n        self.x_test  = x_test\n        self.y_train = y_train\n        self.y_test  = y_test\n\n    def process(self, fn_name, space, trials, algo, max_evals):\n        fn = getattr(self, fn_name)\n        try:\n            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n        except Exception as e:\n            return {'status': STATUS_FAIL,\n                    'exception': str(e)}\n        return result, trials\n\n    def xgb_reg(self, para):\n        reg = xgb.XGBRegressor(**para['reg_params'])\n        return self.train_reg(reg, para)\n    \n    def rf_reg(self, para):\n        reg = RandomForestRegressor(**para['reg_params'])\n        return self.train_reg(reg, para)\n    \n    def lasso_reg(self, para):\n        reg = linear_model.Lasso(**para['reg_params'])\n        return self.train_reg(reg, para)\n    \n    def elnet_reg(self, para):\n        reg = linear_model.ElasticNet(**para['reg_params'])\n        return self.train_reg(reg, para)\n\n    def lgb_reg(self, para):\n        reg = lgb.LGBMRegressor(**para['reg_params'])\n        return self.train_reg(reg, para)\n    \n    def svr_reg(self, para):\n        reg = SVR(**para['reg_params'])\n        return self.train_reg(reg, para)\n\n\n    def train_reg(self, reg, para):\n        if 'RandomForestRegressor'  in str(reg):\n            reg.fit(self.x_train, self.y_train)\n        elif 'Lasso' in str(reg):\n            reg.fit(self.x_train, self.y_train)\n        elif 'ElasticNet' in str(reg):\n            reg.fit(self.x_train, self.y_train)\n        elif 'SVR' in str(reg):\n            reg.fit(self.x_train, self.y_train)\n        else:   \n            reg.fit(self.x_train, self.y_train,\n                eval_set=[(self.x_train, self.y_train), (self.x_test, self.y_test)],\n                **para['fit_params'])\n        pred = reg.predict(self.x_test)\n        loss = para['loss_func'](self.y_test, pred)\n        return {'loss': loss, 'status': STATUS_OK}","cfd18150":"from sklearn.model_selection import train_test_split\nX_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size = 0.3, random_state = 42)","c623749f":"from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\n\nlasso = linear_model.Lasso()\n\nalpha = {'alpha': [x \/ 25000 for x in range(1, 50, 1)],\n         'tol': [0.0000001], \n          'max_iter': [3000]}\n\n\nlasso.fit(X_train, y_train)","939d6dcd":"max_iter = range(100, 5000, 100)\n\nlasso_reg_params = {'alpha':              hp.uniform('alpha', 0.00000001, 0.5),\n                 'tol':                hp.uniform('tol', 0.0000001, 0.5),\n                 'max_iter':           hp.choice('max_iter', max_iter),\n                }\nlasso_fit_params = {\n                    'sample_weight' : None\n}\nobj = HPOpt(X_train, X_holdout, y_train, y_holdout)\n\n\n#rf_fit_params = {\n#    'sample_weight': None,\n#}\nlasso_para = dict()\nlasso_para['reg_params'] = lasso_reg_params\nlasso_para['fit_params'] = lasso_fit_params\nlasso_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))\n\nlasso_opt_para = obj.process(fn_name='lasso_reg', space=lasso_para, trials=Trials(), algo=tpe.suggest, max_evals=500)","c0b16c58":"#best 0.0929 <- 0.1389","099c072b":"lasso_opt_para\n#{'alpha': 0.0003423653832332181, 'max_iter': 14, 'tol': 0.021055934172052676}","18edab7d":"lasso_reg_opt_params = {'alpha':               lasso_opt_para[0]['alpha'],\n                        'tol':                 lasso_opt_para[0]['tol'],\n                        'max_iter':            max_iter[lasso_opt_para[0]['max_iter']],\n                        }\nlasso_opt = linear_model.Lasso(**lasso_reg_opt_params)","8344f73b":"evaluate(lasso, lasso_opt, X_train, y_train, X_holdout, y_holdout, lasso_fit_params)","e0969f49":"import xgboost as xgb\nxg_reg = xgb.XGBRegressor(learning_rate = 0.1,\n                          max_depth = 4, \n                          objective=\"reg:squarederror\", \n                          n_estimators=700)\nxg_reg.fit(X_train,y_train)","9203e3a7":"obj = HPOpt(X_train, X_holdout, y_train, y_holdout)\nbooster = ['gbtree']\nlearning_rate = np.arange(0.01, 0.7, 0.05)\nmax_depth = range(5, 20, 1)\nmin_child_weight = range(1, 20, 1)\nn_estimators = range(100, 5000, 100)\n\nxgb_reg_params = {\n    'booster':          hp.choice('booster',          booster),\n    'learning_rate':    hp.choice('learning_rate',    learning_rate),\n    'max_depth':        hp.choice('max_depth',        max_depth),\n    'min_child_weight': hp.choice('min_child_weight', min_child_weight),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.0001, 1),\n    'subsample':        hp.uniform('subsample', 0.0001, 1),\n    'n_estimators':     hp.choice('n_estimators',     n_estimators)\n}\nxgb_fit_params = {\n    'eval_metric': 'rmse',\n    'early_stopping_rounds': 10,\n    'verbose': False\n}\nxgb_para = dict()\nxgb_para['reg_params'] = xgb_reg_params\nxgb_para['fit_params'] = xgb_fit_params\nxgb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))\n\nxgb_opt_para = obj.process(fn_name='xgb_reg', space=xgb_para, trials=Trials(), algo = tpe.suggest, max_evals=120)","2e097e20":"#best 0.1000 <- 0.1422","f4f2c178":"xgb_reg_opt_params = {\n    'booster':          'gbtree',\n    'learning_rate':    learning_rate[xgb_opt_para[0]['learning_rate']],\n    'max_depth':        max_depth[xgb_opt_para[0]['max_depth']],\n    'min_child_weight': min_child_weight[xgb_opt_para[0]['min_child_weight']],\n    'colsample_bytree': xgb_opt_para[0]['colsample_bytree'],\n    'subsample':        xgb_opt_para[0]['subsample'],\n    'n_estimators':     n_estimators[xgb_opt_para[0]['n_estimators']]\n}\nxgb_reg_opt = xgb.XGBRegressor(**xgb_reg_opt_params)\nxgb_reg_opt_params","5df59a9b":"evaluate(xg_reg, xgb_reg_opt, X_train, y_train, X_holdout, y_holdout, xgb_fit_params)","b2bc3376":"from sklearn.svm import SVR\n\nsvr_reg = SVR()","0b03b5ae":"C = list(range(1, 100, 1))\nepsilon = [x \/ 2000 for x in range(1, 50, 1)]\ngamma = [x \/ 10000 for x in range(1, 50, 1)]\nsvr_reg_params = {\n              'C' : hp.choice('C', C),\n              'epsilon' : hp.uniform('epsilon', 0, 0.05),\n              'gamma' :   hp.uniform('gamma', 0, 0.05)}\n\nsvr_fit_params = {\n                    'sample_weight' : None\n}\nobj = HPOpt(X_train, X_holdout, y_train, y_holdout)\n\n\n\nsvr_para = dict()\nsvr_para['reg_params'] = svr_reg_params\nsvr_para['fit_params'] = svr_fit_params\nsvr_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))\n\nsvr_opt_para = obj.process(fn_name='svr_reg', space=svr_para, trials=Trials(), algo=tpe.suggest, max_evals=200)\n","364bd32e":"#best 0.092 <- 0.1163","011badad":"svr_opt_para","401893d1":"svr_reg_opt_params = {  'kernel':        'rbf',\n                        'C':              C[svr_opt_para[0]['C']],\n                        'epsilon':         svr_opt_para[0]['epsilon'],\n                        'gamma':         svr_opt_para[0]['gamma'],\n                        }\n\nsvr_reg_opt = SVR(**svr_reg_opt_params)","78e8b9f7":"evaluate(svr_reg, svr_reg_opt, X_train, y_train, X_holdout, y_holdout, svr_fit_params)","8328ce66":"import lightgbm as lgb\nlgb_reg = lgb.LGBMRegressor()\nlgb_reg.fit(X_train, y_train)","87ee6b41":"obj = HPOpt(X_train, X_holdout, y_train, y_holdout)\nreg_lambda = [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2]\nlearning_rate = np.arange(0.01, 0.5, 0.05)\nreg_alpha = np.arange(0.01, 2, 0.05)\nmin_child_samples = range(1, 100, 1)\nnum_leaves = range(2, 200, 1)\nsubsample_freq = range(1, 200, 1)\nmax_depth = range(5, 20, 1)\nmin_child_weight = range(1, 20, 1)\nn_estimators = range(100, 5000, 100)\nmax_bin = range(2, 700, 1)\n\nLGBM_params = {'reg_lambda':                  hp.choice('reg_lambda', reg_lambda),\n                      'reg_alpha':            hp.choice('reg_alpha', reg_alpha),\n                      'min_child_samples':    hp.choice('min_child_samples', min_child_samples),\n                      'subsample':            hp.uniform('subsample', 0.01, 1), # bagging_fraction\n                      'subsample_freq':       hp.choice('subsample_freq', subsample_freq), # bagging_freq\n                      'num_leaves':           hp.choice('num_leaves', num_leaves),\n                      'max_depth':            hp.choice('max_depth', max_depth),\n                      'max_bin':              hp.choice('max_bin', max_bin),\n                      'learning_rate':        hp.choice('learning_rate', learning_rate),\n                      'colsample_bytree':     hp.uniform('colsample_bytree', 0.01, 1)} # feature_fraction \nlgb_fit_params = {\n    'eval_metric': 'rmse',\n    'early_stopping_rounds': 10,\n    'verbose': False\n}\nlgb_para = dict()\nlgb_para['reg_params'] = LGBM_params\nlgb_para['fit_params'] = lgb_fit_params\nlgb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))\n\nlgb_opt_params = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=300)","5f92a4d7":"#best 0.1048 <- 0.1191","52540117":"lgb_reg_params = {\n    'reg_lambda':            reg_lambda[lgb_opt_params[0]['reg_lambda']],\n    'reg_alpha':             reg_alpha[lgb_opt_params[0]['reg_alpha']],\n    'min_child_samples':     min_child_samples[lgb_opt_params[0]['min_child_samples']],\n    'subsample':             lgb_opt_params[0]['subsample'], \n    'subsample_freq':        subsample_freq[lgb_opt_params[0]['subsample_freq']], # \n    'num_leaves':            num_leaves[lgb_opt_params[0]['num_leaves']],\n    'max_depth':             max_depth[lgb_opt_params[0]['max_depth']],\n    'max_bin':               max_bin[lgb_opt_params[0]['max_bin']],\n    'learning_rate':         learning_rate[lgb_opt_params[0]['learning_rate']],\n    'colsample_bytree':      lgb_opt_params[0]['colsample_bytree'] #  \n}\n\nlgb_opt = lgb.LGBMRegressor(**lgb_reg_params)\nlgb_opt.fit(X_train, y_train,\n                eval_set=[(X_train, y_train), (X_holdout, y_holdout)],\n                **lgb_para['fit_params'])\nprint(np.sqrt(mean_squared_error(lgb_opt.predict(X_holdout), y_holdout)))\n","ec764f67":"evaluate(lgb_reg, lgb_opt, X_train, y_train, X_holdout, y_holdout, lgb_fit_params)","b13e06e9":"from sklearn.linear_model import ElasticNet\nbase_elnet = linear_model.ElasticNet()\nmax_iter = range(100, 5000, 100)\n\nelnet_reg_params = {'alpha':              hp.uniform('alpha', 0.00000001, 0.5),\n                    'tol':                hp.uniform('tol', 0.0000001, 0.5),\n                    'max_iter':           hp.choice('max_iter', max_iter),\n                    'l1_ratio':           hp.uniform('l1_ratio', 0.0000001, 0.5),\n                }\nelnet_fit_params = {\n                    'sample_weight' : None\n}\nobj = HPOpt(X_train, X_holdout, y_train, y_holdout)\n\n\n#rf_fit_params = {\n#    'sample_weight': None,\n#}\nelnet_para = dict()\nelnet_para['reg_params'] = elnet_reg_params\nelnet_para['fit_params'] = elnet_fit_params\nelnet_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))\n\nelnet_opt_para = obj.process(fn_name='elnet_reg', space=elnet_para, trials=Trials(), algo=tpe.suggest, max_evals=500)\n","d0e15a57":"#best 0.0932 <- 0.1163","10c4b17a":"elnet_opt_para","08d6de2d":"elnet_reg_opt_params = {'alpha':               elnet_opt_para[0]['alpha'],\n                        'tol':                 elnet_opt_para[0]['tol'],\n                        'max_iter':            max_iter[elnet_opt_para[0]['max_iter']],\n                        'l1_ratio':            elnet_opt_para[0]['l1_ratio']\n                        }\nelnet_opt = linear_model.ElasticNet(**elnet_reg_opt_params)","ec4874f4":"evaluate(base_elnet, elnet_opt, X_train, y_train, X_holdout, y_holdout, elnet_fit_params)","f71db767":"print(np.sqrt(mean_squared_error((lasso_opt.predict(X_holdout) \n                                  + xgb_reg_opt.predict(X_holdout) \n                                  + lgb_opt.predict(X_holdout) \n                                  + elnet_opt.predict(X_holdout)\n                                  + svr_reg_opt.predict(X_holdout)) \/ 5, y_holdout)))","cfd3eda9":"#best 0.0899 <- 0.11","c2530248":"import torch\np = torch.tensor([lasso_opt.predict(X_holdout),\n                  xgb_reg_opt.predict(X_holdout),\n                  lgb_opt.predict(X_holdout),\n                  elnet_opt.predict(X_holdout),\n                  svr_reg_opt.predict(X_holdout)])\ny = torch.tensor(np.array(y_holdout))\nw = torch.tensor([[1.],[1.], [1.], [1.], [1.]], requires_grad = True)\ndef f(p, y, w):\n    return torch.sqrt(torch.sum((y - torch.sum(w*p, dim = 0)\/w.sum())**2)\/len(y))\n\noptimizer = torch.optim.Adam([w], lr = 1)\nfor i in range(5000):\n    fun = f(p, y, w)\n    fun.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint('best score', f(p, y, w))\nwf = np.array([[w.data[0]], [w.data[1]], [w.data[2]], [w.data[3]], [w.data[4]]])","8aa323c9":"#best 0.0892 <- 0.1127","f7fc2f8a":"print(wf)","cfd4355f":"predictions = np.array([lasso_opt.predict(X_holdout),\n                        xgb_reg_opt.predict(X_holdout),\n                        lgb_opt.predict(X_holdout),\n                        elnet_opt.predict(X_holdout),\n                        svr_reg_opt.predict(X_holdout)])\nplt.figure(figsize = (10, 9))\nplt.plot([10.5, 11, 12, 13],[10.5, 11, 12, 13])\nplt.xlabel('ensemble pred')\nplt.ylabel('true values')\nplt.scatter(np.sum((wf*predictions) \/ wf.sum(), axis = 0), y_holdout, c = 'red', alpha=0.5)\nplt.show()","914b499c":"pr = np.array([lasso_opt.predict(X_test),\n               xgb_reg_opt.predict(X_test),\n               lgb_opt.predict(X_test),\n               elnet_opt.predict(X_test),\n               svr_reg_opt.predict(X_test)])\n\n\n\npred = np.sum((wf*pr)\/wf.sum(), axis = 0)\n\npreds = pd.DataFrame({'Id' : Id, 'SalePrice': np.exp(pred)})\npreds.to_csv('sumb.csv', index = False)\n","665fa309":"# **Filling NaN's**","0f04416e":"### ElasticNet","564924cd":"## Removing high collecalted variables","4d670439":"# EDA","94fb776f":"We can see that some features have string correlation with the final price","17652514":"# Averaging the predictions","ef74acb1":"The graph below shows how our features are correlated with the price","5f63f3ee":"I decided to use log of the data to prevent big values","326259f3":"# Final data","87fc9787":"There are some useful functions to measure the quality of the models","227665f9":"### XGB","8a6b4d31":"# Adiing new features","6ff09fe9":"# Visualizing NaNs","0b8a003f":"# Removing the outliers","ced4b3d0":"### Lasso","0729de9c":"I decided to combine the predictions of my models in order to prevent overfitting and to use advantages of all models. The idea is to make a weightened mean prediction and optimize it using Adam or SGD optimizers","637988be":"Removing houses with very big price","5ea2572f":"I used the min max scaling to move the data into [0, 1]","ed1ade7e":"#### Hyperopt tuner class ","20edeb6d":"Here we can see the difference between true and predicted values on a holdout set","bec225b4":"I used get_dummies method to create dummy vars for categorical columns","5a1064ca":"# Training Models and Tuning them with Hyperopt","3fe824c1":"Checking year features","8bcfc62d":"### Support Vector Machine","39abf1c2":"We can see that some columns contain NaN's","7b5f609f":"Log of the price seems to be more normal","7875a229":"# **Importing data**","15cdfc04":"# Dealing with imbalanced features","830bf738":"#### There are some new features got from combinations","1978f115":"I will use Lasso model to remove the outliers by simply deleting rows with the largest residuals","0d80d7b4":"### LGBM","944e0258":"Getting some new variables","b81a9dd0":"Due to different number of uniques in train and test set i will join train and test set to preprocess"}}