{"cell_type":{"ce76a8e5":"code","e462c2bc":"code","dbde9300":"code","c4ddc37b":"code","c03923fc":"code","337e5aa4":"code","1b3f9deb":"code","cd59e8ed":"code","8a7c5bb9":"code","363f769d":"code","22eeb3c6":"code","0ecc6b9d":"code","9661e3c5":"code","d8a5e477":"code","20b4f418":"code","73f9f0f9":"code","60e7ea30":"code","88cde3a3":"code","dfd7519e":"code","1e57242c":"code","98643b5a":"code","0ba9e999":"code","56c12765":"code","163f0d88":"code","2a65983e":"code","b9fdc0c8":"code","319fac04":"code","bb634fe7":"code","08073695":"code","08785bde":"code","0b3f3485":"code","ee79db31":"code","6262bb42":"code","1ae1262a":"code","94e3f96b":"code","a8d94eb6":"code","717cdc00":"code","69fbd4a2":"code","994abbdf":"code","499d3a47":"code","58af295d":"code","a401c1a4":"code","4b004d6b":"code","808adfc1":"code","222b3b02":"code","03bf9922":"code","249c6228":"code","fcfcf32d":"code","84d1154d":"code","8469aee2":"code","5d505fbe":"code","addb975e":"code","a69aed02":"code","c4a26c73":"code","f275dd46":"code","8acacdf4":"code","8166fa0c":"code","d3b6050c":"code","c19dd497":"code","991a0868":"code","492b6f85":"code","e612a8c3":"code","3c8fc12c":"code","59a7d5cf":"code","7951532d":"code","4c958d50":"code","a8386f2c":"code","6dac3ccc":"code","9de350f1":"code","bbddb030":"code","33de0369":"code","9e458e71":"code","ee177eba":"code","5b909307":"markdown","579076a1":"markdown","c304e2f6":"markdown","c9a8ab0f":"markdown","3fdc7cd0":"markdown","e0d6edaf":"markdown","7516fd71":"markdown","ec699d38":"markdown","cfcb9cce":"markdown","8f8dae5b":"markdown","d369935b":"markdown","1de595c7":"markdown","aeaca701":"markdown","1522f645":"markdown","32034a18":"markdown","76d1172a":"markdown","d910dd7f":"markdown","b8d3592d":"markdown","69e36203":"markdown","7700583d":"markdown","df8630bb":"markdown","a50facfa":"markdown","b6c8bca3":"markdown","9cfbd798":"markdown"},"source":{"ce76a8e5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport eli5 # lib to debug ML Models\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Configs\npd.options.display.float_format = '{:,.4f}'.format\nsns.set(style=\"whitegrid\")\nplt.style.use('seaborn')\nseed = 42\nnp.random.seed(seed)","e462c2bc":"file_path = '\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv'\ndf = pd.read_csv(file_path)\nprint(\"DataSet = {:,d} rows and {} columns\".format(df.shape[0], df.shape[1]))\n\nprint(\"\\nAll Columns:\\n=>\", df.columns.tolist())\n\nquantitative = [f for f in df.columns if df.dtypes[f] != 'object']\nqualitative = [f for f in df.columns if df.dtypes[f] == 'object']\n\nprint(\"\\nStrings Columns:\\n=>\", qualitative,\n      \"\\n\\nNumerics Columns:\\n=>\", quantitative)\n\ndf.head()","dbde9300":"file_path = '\/kaggle\/input\/health-insurance-cross-sell-prediction\/sample_submission.csv'\ndf_sample_submission = pd.read_csv(file_path)\nprint(\"DataSet = {:,d} rows and {} columns\".format(df_sample_submission.shape[0], df_sample_submission.shape[1]))\ndf_sample_submission.head(2)","c4ddc37b":"df.describe().T","c03923fc":"# statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew, boxcox_normmax #for some statistics\nfrom scipy.special import boxcox1p\n\ndef test_normal_distribution(serie, series_name='series', thershold=0.4):\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 6), sharex=False)\n    f.suptitle('{} is a Normal Distribution?'.format(series_name), fontsize=18)\n    ax1.set_title(\"Histogram to \" + series_name)\n    ax2.set_title(\"Q-Q-Plot to \"+ series_name)\n    # calculate normal distrib. to series\n    mu, sigma = norm.fit(serie)\n    print('Normal dist. (mu= {:,.2f} and sigma= {:,.2f} )'.format(mu, sigma))\n    # skewness and kurtoise\n    skewness = serie.skew()\n    kurtoise = serie.kurt()\n    print(\"Skewness: {:,.2f} | Kurtosis: {:,.2f}\".format(skewness, kurtoise))\n    # evaluate skeness\n    pre_text = '\\t=> '\n    if(skewness < 0):\n        text = pre_text + 'negatively skewed or left-skewed'\n    else:\n        text =  pre_text + 'positively skewed or right-skewed\\n'\n        text += pre_text + 'in case of positive skewness, log transformations usually works well.\\n'\n        text += pre_text + 'np.log(), np.log1(), boxcox1p()'\n    if(skewness < -1 or skewness > 1):\n        print(\"Evaluate skewness: highly skewed\")\n        print(text)\n    if( (skewness <= -0.5 and skewness > -1) or (skewness >= 0.5 and skewness < 1)):\n        print(\"Evaluate skewness: moderately skewed\")\n        print(text)\n    if(skewness >= -0.5 and skewness <= 0.5):\n        print('Evaluate skewness: approximately symmetric')\n    print('evaluate kurtoise')\n    if(kurtoise > 3 + thershold):\n        print(pre_text + 'Leptokurtic: anormal: Peak is higher')\n    elif(kurtoise < 3 - thershold):\n        print(pre_text + 'Platykurtic: anormal: The peak is lower')\n    else:\n        print(pre_text + 'Mesokurtic: normal: the peack is normal')\n    sns.distplot(serie , fit=norm, ax=ax1)\n    ax1.legend(['Normal dist. ($\\mu=$ {:,.2f} and $\\sigma=$ {:,.2f} )'.format(mu, sigma)],\n            loc='best')\n    ax1.set_ylabel('Frequency')\n    stats.probplot(serie, plot=ax2)\n    plt.show()","337e5aa4":"from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, f1_score\n\nthis_labels = ['Response 0','Response 1']\n\ndef class_report(y_real, y_my_preds, name=\"\", labels=this_labels):\n    if(name != ''):\n        print(name,\"\\n\")\n    print(confusion_matrix(y_real, y_my_preds), '\\n')\n    print(classification_report(y_real, y_my_preds, target_names=labels))","1b3f9deb":"def check_balanced_train_test_binary(x_train, y_train, x_test, y_test, original_size, labels):\n    train_unique_label, train_counts_label = np.unique(y_train, return_counts=True)\n    test_unique_label, test_counts_label = np.unique(y_test, return_counts=True)\n\n    prop_train = train_counts_label\/ len(y_train)\n    prop_test = test_counts_label\/ len(y_test)\n\n    print(\"Original Size:\", '{:,d}'.format(original_size))\n    print(\"\\nTrain: must be 80% of dataset:\\n\", \n          \"the train dataset has {:,d} rows\".format(len(x_train)),\n          'this is ({:.2%}) of original dataset'.format(len(x_train)\/original_size),\n                \"\\n => Classe 0 ({}):\".format(labels[0]), train_counts_label[0], '({:.2%})'.format(prop_train[0]), \n                \"\\n => Classe 1 ({}):\".format(labels[1]), train_counts_label[1], '({:.2%})'.format(prop_train[1]),\n          \"\\n\\nTest: must be 20% of dataset:\\n\",\n          \"the test dataset has {:,d} rows\".format(len(x_test)),\n          'this is ({:.2%}) of original dataset'.format(len(x_test)\/original_size),\n                  \"\\n => Classe 0 ({}):\".format(labels[0]), test_counts_label[0], '({:.2%})'.format(prop_test[0]),\n                  \"\\n => Classe 1 ({}):\".format(labels[1]),test_counts_label[1], '({:.2%})'.format(prop_test[1])\n         )","cd59e8ed":"def eda_categ_feat_desc_plot(series_categorical, title = \"\", fix_labels=False):\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (12,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    if(fix_labels):\n        val_concat = val_concat.sort_values(series_name).reset_index()\n    \n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], '{:,d}'.format(int(row['quantity'])), color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","8a7c5bb9":"def plot_top_bottom_rank_correlation(my_df, column_target, top_rank=5, title=''):\n    corr_matrix = my_df.corr()\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 6), sharex=False)\n    if(title):\n        f.suptitle(title)\n\n    ax1.set_title('Top {} Positive Corr to {}'.format(top_rank, column_target))\n    ax2.set_title('Top {} Negative Corr to {}'.format(top_rank, column_target))\n    \n    cols_top = corr_matrix.nlargest(top_rank+1, column_target)[column_target].index\n    cm = np.corrcoef(my_df[cols_top].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 8}, yticklabels=cols_top.values,\n                     xticklabels=cols_top.values, mask=mask, ax=ax1)\n    \n    cols_bot = corr_matrix.nsmallest(top_rank, column_target)[column_target].index\n    cols_bot  = cols_bot.insert(0, column_target)\n    print(cols_bot)\n    cm = np.corrcoef(my_df[cols_bot].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 10}, yticklabels=cols_bot.values,\n                     xticklabels=cols_bot.values, mask=mask, ax=ax2)\n    \n    plt.show()","363f769d":"def plot_top_rank_correlation(my_df, column_target):\n    corr_matrix = my_df.corr()\n    top_rank = len(corr_matrix)\n    f, ax1 = plt.subplots(ncols=1, figsize=(18, 6), sharex=False)\n\n    ax1.set_title('Top Correlations to {}'.format(top_rank, column_target))\n    \n    cols_top = corr_matrix.nlargest(len(corr_matrix), column_target)[column_target].index\n    cm = np.corrcoef(my_df[cols_top].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 10}, yticklabels=cols_top.values,\n                     xticklabels=cols_top.values, mask=mask, ax=ax1)\n    \n    plt.show()","22eeb3c6":"import time\n\ndef time_spent(time0):\n    t = time.time() - time0\n    t_int = int(t) \/\/ 60\n    t_min = t % 60\n    if(t_int != 0):\n        return '{} min {:.3f} s'.format(t_int, t_min)\n    else:\n        return '{:.3f} s'.format(t_min)","0ecc6b9d":"def eda_numerical_feat(series, title=\"\", with_label=True, number_format=\"\", show_describe=False, size_labels=10):\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 5), sharex=False)\n    if(show_describe):\n        print(series.describe())\n    if(title != \"\"):\n        f.suptitle(title, fontsize=18)\n    sns.distplot(series, ax=ax1)\n    sns.boxplot(series, ax=ax2)\n    if(with_label):\n        describe = series.describe()\n        labels = { 'min': describe.loc['min'], 'max': describe.loc['max'], \n              'Q1': describe.loc['25%'], 'Q2': describe.loc['50%'],\n              'Q3': describe.loc['75%']}\n        if(number_format != \"\"):\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + number_format.format(v), ha='center', va='center', fontweight='bold',\n                         size=size_labels, color='white', bbox=dict(facecolor='#445A64'))\n        else:\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + str(v), ha='center', va='center', fontweight='bold',\n                     size=size_labels, color='white', bbox=dict(facecolor='#445A64'))\n    plt.show()","9661e3c5":"def series_remove_outiliers(series):\n    q25, q75 = series.quantile(0.25), series.quantile(0.75)\n    iqr = q75 - q25\n    print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n    cut_off = iqr * 1.5\n    lower, upper = q25 - cut_off, q75 + cut_off\n    # identify outliers\n    print('Cut Off: below than', lower, 'and above than', upper)\n    outliers = series[ (series > upper) | (series < lower)]\n    print('Identified outliers: {:,d}'.format(len(outliers)), 'that are',\n          '{:.2%}'.format(len(outliers)\/len(series)), 'of total data')\n    # remove outliers\n    outliers_removed = [x for x in series if x >= lower and x <= upper]\n    print('Non-outlier observations: {:,d}'.format(len(outliers_removed)))\n    series_no_outiliers = series[ (series <= upper) & (series >= lower) ]\n    return series_no_outiliers","d8a5e477":"def plot_top_bottom_rank_correlation(my_df, column_target, top_rank=5, title=''):\n    corr_matrix = my_df.corr()\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 6), sharex=False)\n    if(title):\n        f.suptitle(title)\n\n    ax1.set_title('Top {} Positive Corr to {}'.format(top_rank, column_target))\n    ax2.set_title('Top {} Negative Corr to {}'.format(top_rank, column_target))\n    \n    cols_top = corr_matrix.nlargest(top_rank+1, column_target)[column_target].index\n    cm = np.corrcoef(my_df[cols_top].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 11}, yticklabels=cols_top.values,\n                     xticklabels=cols_top.values, mask=mask, ax=ax1)\n    \n    cols_bot = corr_matrix.nsmallest(top_rank, column_target)[column_target].index\n    cols_bot  = cols_bot.insert(0, column_target)\n    cm = np.corrcoef(my_df[cols_bot].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 10}, yticklabels=cols_bot.values,\n                     xticklabels=cols_bot.values, mask=mask, ax=ax2)\n    \n    plt.show()","20b4f418":"df.duplicated().sum()","73f9f0f9":"sns.heatmap(df.isnull(), cbar=False, yticklabels=False)","60e7ea30":"eda_categ_feat_desc_plot(df['Response'], ' \"Response\" Distribution')","88cde3a3":"eda_categ_feat_desc_plot(df['Gender'], title='gender distribution')","dfd7519e":"eda_categ_feat_desc_plot(df['Vehicle_Damage'], title='Vehicle_Damage distribution')","1e57242c":"eda_categ_feat_desc_plot(df['Vehicle_Age'], title='Vehicle_Age distribution')","98643b5a":"eda_categ_feat_desc_plot(df['Driving_License'], title='Driving_License distribution', fix_labels=True)","0ba9e999":"eda_numerical_feat(df['Age'], title='age distribution', number_format='{:.0f}')","56c12765":"eda_numerical_feat(df['Annual_Premium'], title='Annual_Premium distribution WITH OUTILIERS', number_format='{:,.2f}')","163f0d88":"eda_numerical_feat(series_remove_outiliers(df['Annual_Premium']), title='Annual_Premium distribution NO OUTILIERS', number_format='{:,.2f}')","2a65983e":"afilter = df[\"Policy_Sales_Channel\"].value_counts().nlargest(30).index.tolist()\n\nfig, ax = plt.subplots(figsize=(18,6))\nax = sns.countplot(x=df[ df['Policy_Sales_Channel'].isin(afilter)][\"Policy_Sales_Channel\"] )\nplt.title(\"Top 30 Policy_Sales_Channel count\", fontsize=24)\nplt.xlabel('Region_Code', fontsize=18)\nplt.ylabel('Count', fontsize=18)\nplt.xticks(rotation=45, fontsize=12, ha='right')\nplt.show()","b9fdc0c8":"fig, ax = plt.subplots(figsize=(18,6))\nax = sns.countplot(x=\"Region_Code\", data=df)\nplt.title(\"Number of records per year\", fontsize=24)\nplt.xlabel('Region_Code', fontsize=18)\nplt.ylabel('Count', fontsize=18)\nplt.xticks(rotation=45, fontsize=12, ha='right')\nplt.show()","319fac04":"eda_categ_feat_desc_plot(df['Previously_Insured'], title='Previously_Insured distribution')","bb634fe7":"eda_numerical_feat(df['Vintage'], title='Vintage distribution', number_format='{:.0f}')","08073695":"df1 = df.groupby(['Gender', 'Response']).count().reset_index().rename({\"Firstname\": \"Quantity\"}, axis=1)\ndf1 = df1.drop( df1.columns.tolist()[3::], axis=1)\ndf1 = df1.rename({df1.columns[2]: 'Quantity'},axis=1)\n\nf, (ax3, ax1, ax2) = plt.subplots(ncols=3, figsize=(20, 5), sharex=False)\nf.suptitle('Distribution of Response by Gender', fontsize=18)\n\nsns.countplot(x=\"Gender\", data=df, hue='Response', ax=ax3)\nax3.set_title('CountPlot')\n\nalist = df1['Quantity'].tolist()\n\ndf1.query('Gender == \"Male\"').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[2]),\n                                                 'Response 1 = {:,d}'.format(alist[3])],\n                                       title=\"Male Responses (Total = {:,d})\".format(alist[2] + alist[3]),\n                                       ax=ax1, labeldistance=None)\n\ndf1.query('Gender == \"Female\"').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[0]),\n                                                 'Response 1 = {:,d}'.format(alist[1])],\n                                       title=\"Female Responses (Total = {:,d})\".format(alist[0] + alist[1]),\n                                       ax=ax2, labeldistance=None)\n\nplt.show()","08785bde":"df1 = df.groupby(['Vehicle_Damage', 'Response']).count().reset_index().rename({\"Firstname\": \"Quantity\"}, axis=1)\ndf1 = df1.drop( df1.columns.tolist()[3::], axis=1)\ndf1 = df1.rename({df1.columns[2]: 'Quantity'},axis=1)\n\nf, (ax3, ax1, ax2) = plt.subplots(ncols=3, figsize=(20, 5), sharex=False)\nf.suptitle('Distribution of Response by Vehicle_Damage', fontsize=18)\n\nsns.countplot(x=\"Vehicle_Damage\", data=df, hue='Response', ax=ax3)\nax3.set_title('CountPlot')\n\nalist = df1['Quantity'].tolist()\n\ndf1.query('Vehicle_Damage == \"Yes\"').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[2]),\n                                                 'Response 1 = {:,d}'.format(alist[3])],\n                                       title=\"Vehicle_Damage Yes (Total = {:,d})\".format(alist[2] + alist[3]),\n                                       ax=ax1, labeldistance=None)\n\ndf1.query('Vehicle_Damage == \"No\"').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[0]),\n                                                 'Response 1 = {:,d}'.format(alist[1])],\n                                       title=\"Vehicle_Damage No (Total = {:,d})\".format(alist[0] + alist[1]),\n                                       ax=ax2, labeldistance=None)\n\nplt.show()","0b3f3485":"df1 = df.groupby(['Vehicle_Age', 'Response']).count().reset_index().rename({\"Firstname\": \"Quantity\"}, axis=1)\ndf1 = df1.drop( df1.columns.tolist()[3::], axis=1)\ndf1 = df1.rename({df1.columns[2]: 'Quantity'},axis=1)\n\nf, (ax3, ax1, ax2, ax4) = plt.subplots(ncols=4, figsize=(20, 5), sharex=False)\nf.suptitle('Distribution of Response by Vehicle_Age', fontsize=18)\n\nsns.countplot(x=\"Vehicle_Age\", data=df, hue='Response', ax=ax3)\nax3.set_title('CountPlot')\n\nalist = df1['Quantity'].tolist()\n\ndf1.query('Vehicle_Age == \"1-2 Year\"').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[2]),\n                                                 'Response 1 = {:,d}'.format(alist[3])],\n                                       title=\"Vehicle_Age 1-2 Year (Total = {:,d})\".format(alist[2] + alist[3]),\n                                       ax=ax1, labeldistance=None)\n\ndf1.query('Vehicle_Age == \"< 1 Year\"').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[0]),\n                                                 'Response 1 = {:,d}'.format(alist[1])],\n                                       title=\"Vehicle_Age < 1 Year (Total = {:,d})\".format(alist[0] + alist[1]),\n                                       ax=ax2, labeldistance=None)\n\ndf1.query('Vehicle_Age == \"> 2 Years\"').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[4]),\n                                                 'Response 1 = {:,d}'.format(alist[5])],\n                                       title=\"Vehicle_Age > 2 Years (Total = {:,d})\".format(alist[4] + alist[5]),\n                                       ax=ax4, labeldistance=None)\n\nplt.show()","ee79db31":"df1 = df.groupby(['Driving_License', 'Response']).count().reset_index().rename({\"Firstname\": \"Quantity\"}, axis=1)\ndf1 = df1.drop( df1.columns.tolist()[3::], axis=1)\ndf1 = df1.rename({df1.columns[2]: 'Quantity'},axis=1)\n\nf, (ax3, ax1, ax2) = plt.subplots(ncols=3, figsize=(20, 5), sharex=False)\nf.suptitle('Distribution of Response by Driving_License', fontsize=18)\n\nsns.countplot(x=\"Driving_License\", data=df, hue='Response', ax=ax3)\nax3.set_title('CountPlot')\n\nalist = df1['Quantity'].tolist()\n\ndf1.query('Driving_License == 0').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[2]),\n                                                 'Response 1 = {:,d}'.format(alist[3])],\n                                       title=\"Driving_License 0 (Total = {:,d})\".format(alist[2] + alist[3]),\n                                       ax=ax1, labeldistance=None)\n\ndf1.query('Driving_License == 1').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[0]),\n                                                 'Response 1 = {:,d}'.format(alist[1])],\n                                       title=\"Driving_License 1 (Total = {:,d})\".format(alist[0] + alist[1]),\n                                       ax=ax2, labeldistance=None)\n\nplt.show()","6262bb42":"df1 = df.groupby(['Previously_Insured', 'Response']).count().reset_index().rename({\"Firstname\": \"Quantity\"}, axis=1)\ndf1 = df1.drop( df1.columns.tolist()[3::], axis=1)\ndf1 = df1.rename({df1.columns[2]: 'Quantity'},axis=1)\n\nf, (ax3, ax1, ax2) = plt.subplots(ncols=3, figsize=(20, 5), sharex=False)\nf.suptitle('Distribution of Response by Previously_Insured', fontsize=18)\n\nsns.countplot(x=\"Previously_Insured\", data=df, hue='Response', ax=ax3)\nax3.set_title('CountPlot')\n\nalist = df1['Quantity'].tolist()\n\ndf1.query('Previously_Insured == 0').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[2]),\n                                                 'Response 1 = {:,d}'.format(alist[3])],\n                                       title=\"Previously_Insured 0 (Total = {:,d})\".format(alist[2] + alist[3]),\n                                       ax=ax1, labeldistance=None)\n\ndf1.query('Previously_Insured == 1').plot.pie(y='Quantity', figsize=(15, 5), autopct='%1.2f%%', \n                                       labels = ['Response 0 = {:,d}'.format(alist[0]),\n                                                 'Response 1 = {:,d}'.format(alist[1])],\n                                       title=\"Previously_Insured 1 (Total = {:,d})\".format(alist[0] + alist[1]),\n                                       ax=ax2, labeldistance=None)\n\nplt.show()","1ae1262a":"fig, ax = plt.subplots(figsize=(18,6))\nax = sns.countplot(x=\"Region_Code\", hue='Response', data=df)\nplt.title(\"Count Responses by Region_code\", fontsize=24)\nplt.xlabel('Region_Code', fontsize=18)\nplt.ylabel('Count', fontsize=18)\nplt.xticks(rotation=45, fontsize=12, ha='right')\nplt.show()","94e3f96b":"# Percentage of categorical feauture Region_Code by Response\n\nregions_code = df['Region_Code'].unique()\nregions_code\n\ngb = df.groupby(['Region_Code','Response']).count().reset_index()\n\nsum_by_region_code = df['Region_Code'].value_counts().reset_index().rename({'index': 'Region_Codes', 'Region_Code': 'Count'}, axis=1)\nsum_by_region_code\n\nadict = {}\nfor index, row in sum_by_region_code.iterrows():\n    index = row['Region_Codes']\n    count_0 = int(gb.query('Response == 0 & Region_Code == ' + str(index))['Gender'])\n    count_1 = int(gb.query('Response == 1 & Region_Code == ' + str(index))['Gender'])\n    adict[index] = [count_0, count_0\/row['Count'], count_1, count_1\/row['Count'], ]\n\nadict\napandas = pd.DataFrame(adict.values(), columns=['count_0', '%0', 'count_1', '%1'], index=adict.keys()).reset_index()\n\nfinal_df = sum_by_region_code.merge(apandas,left_on='Region_Codes', right_on='index').drop(['index'], axis=1).sort_values(by='%1', ascending=False)\npd.concat([final_df.head(), final_df.tail()]) # Top 5 fist and bottom sorted by '%1'","a8d94eb6":"afilter = df[\"Policy_Sales_Channel\"].value_counts().nlargest(5).index.tolist()\ndf_filted = df[ df['Policy_Sales_Channel'].isin(afilter) ] \n\nfig, ax = plt.subplots(figsize=(18,6))\nax = sns.countplot(x='Policy_Sales_Channel', hue='Response', data=df_filted)\nplt.title(\"Top 5 Policy_Sales_Channel count\", fontsize=24)\nplt.xlabel('Region_Code', fontsize=18)\nplt.ylabel('Count', fontsize=18)\nplt.xticks(rotation=45, fontsize=12, ha='right')\nplt.show()","717cdc00":"removes_fists = df[\"Policy_Sales_Channel\"].value_counts().nlargest(5).index.tolist()\nafilter = df[\"Policy_Sales_Channel\"].value_counts().nlargest(10).drop(removes_fists).index.tolist()\n\ndf_filted = df[ df['Policy_Sales_Channel'].isin(afilter) ] \n\nfig, ax = plt.subplots(figsize=(18,6))\nax = sns.countplot(x='Policy_Sales_Channel', hue='Response', data=df_filted)\nplt.title(\"Top 6-10 Policy_Sales_Channel count\", fontsize=24)\nplt.xlabel('Region_Code', fontsize=18)\nplt.ylabel('Count', fontsize=18)\nplt.xticks(rotation=45, fontsize=12, ha='right')\nplt.show()","69fbd4a2":"removes_fists = df[\"Policy_Sales_Channel\"].value_counts().nlargest(10).index.tolist()\nafilter = df[\"Policy_Sales_Channel\"].value_counts().nlargest(30).drop(removes_fists).index.tolist()\n\ndf_filted = df[ df['Policy_Sales_Channel'].isin(afilter) ] \n\nfig, ax = plt.subplots(figsize=(18,6))\nax = sns.countplot(x='Policy_Sales_Channel', hue='Response', data=df_filted)\nplt.title(\"Top 10-30 Policy_Sales_Channel count\", fontsize=24)\nplt.xlabel('Region_Code', fontsize=18)\nplt.ylabel('Count', fontsize=18)\nplt.xticks(rotation=45, fontsize=12, ha='right')\nplt.show()","994abbdf":"gb = df.groupby(['Policy_Sales_Channel','Response']).count().reset_index()\n\nsum_by_region_code = df['Policy_Sales_Channel'].value_counts().reset_index().rename(\n    {'index': 'Policy_Sales_Channels', 'Policy_Sales_Channel': 'Count'}, axis=1)\n\nadict = {}\nfor _, row in sum_by_region_code.iterrows():\n    index = row['Policy_Sales_Channels']\n    \n    query0 = gb.query('Response == 0 & Policy_Sales_Channel == ' + str(index))['Gender']\n    count_0 = 0 if len(query0) == 0 else int(query0)\n    query1 = gb.query('Response == 1 & Policy_Sales_Channel == ' + str(index))['Gender']\n    count_1 = 0 if len(query1) == 0 else int(query1)\n\n    adict[index] = [count_0, count_0\/row['Count'], count_1, count_1\/row['Count'], ]\n\napandas = pd.DataFrame(data=adict.values(), index=adict.keys(),\n                      columns=['count_0', '%0', 'count_1', '%1'] ).reset_index()\n\nfinal_df = sum_by_region_code.merge(apandas, left_on='Policy_Sales_Channels', right_on='index').drop(\n    ['index'], axis=1).sort_values(by='%1', ascending=False)\npd.concat([final_df.head(), final_df.tail()]) # Top 5 fist and bottom sorted by '%1'","499d3a47":"fig, (ax1, ax2) = plt.subplots(figsize = (16,5), ncols=2, sharex=False, sharey=False)\n\nfont_size = 14\nfig.suptitle('age by reponse', fontsize=18)\n\nsns.boxplot(x=\"Response\", y=\"Age\", data=df, ax=ax1)\nsns.distplot(df[(df.Response == 0)][\"Age\"],color='c',ax=ax2, label='Response 0')\nsns.distplot(df[(df.Response == 1)]['Age'],color='b',ax=ax2, label='Response 1')\n\nax1.set_title('charges by smoke or not', fontsize=font_size)\nax2.set_title('Distribution of charges for smokers or  not', fontsize=font_size)\nplt.show()","58af295d":"fig, (ax1, ax2) = plt.subplots(figsize = (16,5), ncols=2, sharex=False, sharey=False)\n\nfont_size = 14\nfig.suptitle('vintage by reponse', fontsize=18)\n\nsns.boxplot(x=\"Response\", y=\"Vintage\", data=df, ax=ax1)\nsns.distplot(df[(df.Response == 0)][\"Vintage\"],color='c',ax=ax2, label='Response 0')\nsns.distplot(df[(df.Response == 1)]['Vintage'],color='b',ax=ax2, label='Response 1')\n\nax1.set_title('charges by smoke or not', fontsize=font_size)\nax2.set_title('Distribution of charges for smokers or  not', fontsize=font_size)\nplt.show()","a401c1a4":"df2 = df.copy()\ndf2['Annual_Premium'] = series_remove_outiliers(df['Annual_Premium'])\n\nfig, (ax1, ax2) = plt.subplots(figsize = (16,5), ncols=2, sharex=False, sharey=False)\n\nfont_size = 14\nfig.suptitle('Annual_Premium by reponse', fontsize=18)\n\nsns.boxplot(x=\"Response\", y=\"Annual_Premium\", data=df2, ax=ax1)\nsns.distplot(df2[(df2.Response == 0)][\"Annual_Premium\"],color='c',ax=ax2, label='Response 0')\nsns.distplot(df2[(df2.Response == 1)]['Annual_Premium'],color='b',ax=ax2, label='Response 1')\n\nax1.set_title('charges by smoke or not', fontsize=font_size)\nax2.set_title('Distribution of charges for smokers or  not', fontsize=font_size)\nplt.show()","4b004d6b":"df1 = df.copy()\n\ngender = {'Male': 0, 'Female': 1}\ndriving_license = {0: 0, 1: 1}\npreviously_insured = {0: 1, 1: 0}\nvehicle_age = {'> 2 Years': 3, '1-2 Year': 2, '< 1 Year': 1}\nvehicle_damage = {'Yes': 1, 'No': 0}\n\ndf1['Gender'] = df1['Gender'].replace(gender)\ndf1['Driving_License'] = df1['Driving_License'].replace(driving_license)\ndf1['Previously_Insured'] = df1['Previously_Insured'].replace(previously_insured)\ndf1['Vehicle_Age'] = df1['Vehicle_Age'].replace(vehicle_age)\ndf1['Vehicle_Damage'] = df1['Vehicle_Damage'].replace(vehicle_damage)\ndf1 = df1.drop(['id'],axis=1)\n\ndf2, df3 = df1.copy(), df1.copy()","808adfc1":"df1['Policy_Sales_Channel'] = df1['Policy_Sales_Channel'].astype('int32').astype('object')\ndf1 = pd.concat([df1, pd.get_dummies(df['Policy_Sales_Channel'], prefix='PSC')], axis=1)\ndf1.iloc[:, 10:25].head(3) # 15 columns of Policy_Sales_Channel as OneHotEncoding","222b3b02":"plot_top_bottom_rank_correlation(df1, 'Response', top_rank=12, title='Corr to PolicySalesChanell values')","03bf9922":"df2['Region_Code'] = df2['Region_Code'].astype('int32').astype('object')\ndf2 = pd.concat([df2, pd.get_dummies(df['Region_Code'], prefix='RC')], axis=1)\ndf2.iloc[:, 10:25].head(3) # 15 columns of RegionCode as OneHotEncoding","249c6228":"plot_top_bottom_rank_correlation(df2, 'Response', top_rank=10, title='Corr to Region_Code values')","fcfcf32d":"plot_top_rank_correlation(df3, 'Response')","84d1154d":"test_normal_distribution(df['Annual_Premium'], 'Annual_Premium')","8469aee2":"# df['Annual_Premium'] = boxcox1p(df['Annual_Premium'], boxcox_normmax(df['Annual_Premium'] + 1))\ndf['Annual_Premium'] = df['Annual_Premium'].apply(np.log)\ntest_normal_distribution(df['Annual_Premium'], 'Annual_Premium')","5d505fbe":"gender = {'Male': 0, 'Female': 1}\ndriving_license = {0: 0, 1: 1}\npreviously_insured = {0: 1, 1: 0}\nvehicle_age = {'> 2 Years': 3, '1-2 Year': 2, '< 1 Year': 1}\nvehicle_damage = {'Yes': 1, 'No': 0}\n\ndf['Gender'] = df['Gender'].replace(gender)\ndf['Driving_License'] = df['Driving_License'].replace(driving_license)\ndf['Previously_Insured'] = df['Previously_Insured'].replace(previously_insured)\ndf['Vehicle_Age'] = df['Vehicle_Age'].replace(vehicle_age)\ndf['Vehicle_Damage'] = df['Vehicle_Damage'].replace(vehicle_damage)\n\ndf['Policy_Sales_Channel'] = df['Policy_Sales_Channel'].apply(lambda x: np.int(x))\ndf['Region_Code'] = df['Region_Code'].apply(lambda x: np.int(x))\n\ndf = df.drop(['id'],axis=1)\ndf.head()","addb975e":"# OnetHotEndonding to Bigs Coor of 'Policy_Sales_Channel' and 'Region_Code'","a69aed02":"# OneHotEnconding to 'Policy_Sales_Channel' and 'Region_Code'\n\n# df['Policy_Sales_Channel'] = df['Policy_Sales_Channel'].astype('int32').astype('object')\n# df['Region_Code'] = df['Region_Code'].astype('int32').astype('object')\n# df = pd.concat([df, \n#                 pd.get_dummies(df['Region_Code'], prefix='RC'),\n#                 pd.get_dummies(df['Policy_Sales_Channel'], prefix='PSC')], axis=1)\n# df # df with PSC and RC dummies","c4a26c73":"from sklearn.model_selection import train_test_split\n\ncol_1 = ['Gender', 'Age', 'Driving_License', 'Region_Code', 'Previously_Insured',\n         'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n\ncat_col = ['Gender','Driving_License', 'Region_Code', 'Previously_Insured',\n           'Vehicle_Age', 'Vehicle_Damage','Policy_Sales_Channel']\n\nX = df[col_1]\n# X = df.drop(['Response'], axis=1)\n\ny = df['Response']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y.values, test_size=0.20, random_state=42)\n\ncheck_balanced_train_test_binary(x_train, y_train, x_test, y_test, len(df), ['Response 0', 'Response 1'])","f275dd46":"from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, SVMSMOTE, BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\nfrom imblearn.combine import SMOTEENN, SMOTETomek # over and under sampling\nfrom imblearn.metrics import classification_report_imbalanced\n\nimb_models = {\n    'ADASYN': ADASYN(random_state=42),\n    'SMOTE': SMOTE(random_state=42),\n    'SMOTEENN': SMOTEENN(\"minority\", random_state=42),\n    'SMOTETomek': SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=42),\n    'RandomUnderSampler': RandomUnderSampler(random_state=42)\n}\n\nimb_strategy = \"None\"\n\nif(imb_strategy != \"None\"):\n    before = x_train.shape[0]\n    imb_tranformer = imb_models[imb_strategy]\n    x_train, y_train = imb_tranformer.fit_sample(x_train, y_train)\n    print(\"train dataset before: {:,d}\\nimbalanced_strategy: {}\".format(before, imb_strategy),\n          \"\\ntrain dataset after: {:,d}\\ngenerate: {:,d}\".format(x_train.shape[0], x_train.shape[0] - before))\nelse:\n    print(\"Dont correct unbalanced dataset\")","8acacdf4":"# Classifier Libraries\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# Ensemble Classifiers\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\n# Others Linear Classifiers\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\nfrom sklearn.linear_model import Perceptron, PassiveAggressiveClassifier\n\n# xboost\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# scores\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n\n# neural net of sklearn\nfrom sklearn.neural_network import MLPClassifier\n\n# others\nimport time\nimport operator","8166fa0c":"all_classifiers = {\n    \"NaiveBayes\": GaussianNB(),\n#     \"Ridge\": RidgeClassifier(),\n#     \"Perceptron\": Perceptron(),\n#     \"PassiveAggr\": PassiveAggressiveClassifier(),\n    \"XGBoost\": XGBClassifier(),\n    \"LightGB\": LGBMClassifier(boosting_type='gbdt',n_estimators=500,depth=10,learning_rate=0.04,objective='binary',metric='auc',\n                 colsample_bytree=0.5,reg_lambda=2,reg_alpha=2,random_state=294,n_jobs=-1),\n    # \"SVM\": SVC(),\n    \"LogisiticR\": LogisticRegression(),\n#     \"KNearest\": KNeighborsClassifier(),\n#     \"DecisionTree\": DecisionTreeClassifier(),\n    \"AdaBoost\": AdaBoostClassifier(), # All 100 features: 48min\n    # \"SGDC\": SGDClassifier(),\n    \"GBoost\": GradientBoostingClassifier(),\n#     \"Bagging\": BaggingClassifier(),\n    \"RandomForest\": RandomForestClassifier(),\n    \"ExtraTree\": ExtraTreesClassifier()\n}","d3b6050c":"metrics = { 'cv_roc': {}, 'acc_test': {}, 'f1_test': {}, 'roc_auc': {} }\nm = list(metrics.keys())\ntime_start = time.time()\nprint('CrossValidation, Fitting and Testing')\n\n# Cross Validation, Fit and Test\nfor name, model in all_classifiers.items():\n    print('{:15}'.format(name), end='')\n    t0 = time.time()\n    # Cross Validation\n    training_score = cross_val_score(model, x_train, y_train, scoring=\"roc_auc\", cv=4)\n    # Fitting\n    all_classifiers[name] = model.fit(x_train, y_train) \n    # Testing\n    y_pred = all_classifiers[name].predict(x_test)\n    t1 = time.time()\n    # Save metrics\n    metrics[m[0]][name] = training_score.mean()\n    metrics[m[1]][name] = accuracy_score(y_test, y_pred)\n    metrics[m[2]][name] = f1_score(y_test, y_pred, average=\"macro\") \n    metrics[m[3]][name] = roc_auc_score(y_test, all_classifiers[name].predict_proba(x_test)[:, 1])\n    # Show metrics\n    print('| {}: {:6,.4f} | {}: {:6,.4f} | {}: {:6.4f} | {}: {:6.4f} | took: {:>15} |'.format(\n        m[0], metrics[m[0]][name], m[1], metrics[m[1]][name],\n        m[2], metrics[m[2]][name], m[3], metrics[m[3]][name], time_spent(t0) ))\n        \nprint(\"\\nDone in {}\".format(time_spent(time_start)))","c19dd497":"print(\"Best cv acc  :\", max( metrics[m[0]].items(), key=operator.itemgetter(1) ))\nprint(\"Best acc test:\", max( metrics[m[1]].items(), key=operator.itemgetter(1) ))\nprint(\"Best f1 test :\", max( metrics[m[2]].items(), key=operator.itemgetter(1) ))\nprint(\"Best roc_auc :\", max( metrics[m[3]].items(), key=operator.itemgetter(1) ))\n\ndf_metrics = pd.DataFrame(data = [list(metrics[m[0]].values()),\n                                  list(metrics[m[1]].values()),\n                                  list(metrics[m[2]].values()),\n                                  list(metrics[m[3]].values())],\n                          index = ['cv_acc', 'acc_test', 'f1_test', 'roc_auc'],\n                          columns = metrics[m[0]].keys() ).T.sort_values(by=m[3], ascending=False)\ndf_metrics","991a0868":"from catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\nname = 'CatBoost'\ncatb = CatBoostClassifier()\nt0 = time.time()\n# Fitting\n# catb = catb.fit(x_train, y_train, eval_set=(x_test, y_test), plot=False, early_stopping_rounds=30,verbose=0)\ncatb = catb.fit(x_train, y_train, cat_features=cat_col, eval_set=(x_test, y_test), plot=False, early_stopping_rounds=30,verbose=0) \n# Testing\ny_pred = catb.predict(x_test)\nt1 = time.time()\n# Save metrics\nmetrics[m[0]][name] = 0.0\nmetrics[m[1]][name] = accuracy_score(y_test, y_pred)\nmetrics[m[2]][name] = f1_score(y_test, y_pred, average=\"macro\") \nmetrics[m[3]][name] = roc_auc_score(y_test, catb.predict_proba(x_test)[:, 1]) \n# Show metrics\nprint('{:15} | {}: {:6,.4f} | {}: {:6.4f} | {}: {:6.4f} | took: {:>15} |'.format(\n    name, m[1], metrics[m[1]][name],\n    m[2], metrics[m[2]][name], m[3], metrics[m[3]][name], time_spent(t0) ))","492b6f85":"feat_importances = pd.Series(catb.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","e612a8c3":"eli5.explain_weights(catb)","3c8fc12c":"# https:\/\/eli5.readthedocs.io\/en\/latest\/\n\nfrom eli5.sklearn import PermutationImportance\n\n# Check for Permutation Importance of Features\nperm = PermutationImportance(all_classifiers['LightGB'], random_state=42).fit(x_test, y_test)\neli5.show_weights(perm, feature_names=X.columns.tolist())","59a7d5cf":"from sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom mlens.ensemble import SuperLearner\n \n# create a list of base-models\ndef get_models():\n    models = list()\n    models.append(LogisticRegression(solver='liblinear'))\n    models.append(DecisionTreeClassifier())\n    #     models.append(SVC(gamma='scale', probability=True))\n    models.append(GaussianNB())\n    #     models.append(KNeighborsClassifier())\n    models.append(XGBClassifier())\n    models.append(AdaBoostClassifier())\n    #     models.append(BaggingClassifier(n_estimators=10))\n    models.append(RandomForestClassifier())\n    models.append(LGBMClassifier(boosting_type='gbdt',n_estimators=500,depth=10,learning_rate=0.04,objective='binary',metric='auc',\n                 colsample_bytree=0.5,reg_lambda=2,reg_alpha=2,random_state=42))\n    return models\n \n# create the super learner\ndef get_super_learner(X):\n    ensemble = SuperLearner(scorer=accuracy_score, folds=5, shuffle=True, sample_size=len(X))\n    # add base models\n    models = get_models()\n    ensemble.add(models)\n    # add the meta model\n    ensemble.add_meta(LogisticRegression(solver='lbfgs'))\n    return ensemble","7951532d":"t0 = time.time()\n\n# create the super learner\nensemble = get_super_learner(x_train.values)\n\n# fit the super learner\nensemble.fit(x_train.values, y_train)\n\n# summarize base learners\nprint(ensemble.data)\n\n# make predictions on hold out set\ny_pred = ensemble.predict(x_test.values)\n\nprint(\"took \", time_spent(t0))\nclass_report(y_test, y_pred, name=\"SuperLeaner\")\n\ny_probs = ensemble.predict_proba(x_test.values)\n\nroc_auc_score(y_test, y_probs)","4c958d50":"file_path = '\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv'\ndf_test = pd.read_csv(file_path)\ndf_test.head(2)","a8386f2c":"df_test['Policy_Sales_Channel'] = df_test['Policy_Sales_Channel'].astype('int32').astype('object')\ndf_test['Region_Code'] = df_test['Region_Code'].astype('int32').astype('object')\n# df_test = pd.concat([df_test, \n#                 pd.get_dummies(df_test['Region_Code'], prefix='RC'),\n#                 pd.get_dummies(df_test['Policy_Sales_Channel'], prefix='PSC')], axis=1)\ndf_test.head()","6dac3ccc":"file_path = '\/kaggle\/input\/health-insurance-cross-sell-prediction\/sample_submission.csv'\ndf_sample_submission = pd.read_csv(file_path)\ndf_sample_submission.head(2)","9de350f1":"## WITH ALL FEATURES: PRE-PROCESSING\n# extend_columns = df.columns.tolist()[11:]\n# rc = extend_columns[:53]\n# psc = extend_columns[53:]\n\n# for c in extend_columns:\n#     df_test[c] = 0\n    \n# for c in rc:\n#     num = int(c[3:])\n#     df_test[c] = df_test['Region_Code'].apply(lambda x: 1 if x == num else 0)\n    \n# for c in psc:\n#     num = int(c[4:])\n#     df_test[c] = df_test['Policy_Sales_Channel'].apply(lambda x: 1 if x == num else 0)","bbddb030":"gender = {'Male': 0, 'Female': 1}\ndriving_license = {0: 0, 1: 1}\npreviously_insured = {0: 1, 1: 0}\nvehicle_age = {'> 2 Years': 3, '1-2 Year': 2, '< 1 Year': 1}\nvehicle_damage = {'Yes': 1, 'No': 0}\n\ndf_test['Gender'] = df_test['Gender'].replace(gender)\ndf_test['Driving_License'] = df_test['Driving_License'].replace(driving_license)\ndf_test['Previously_Insured'] = df_test['Previously_Insured'].replace(previously_insured)\ndf_test['Vehicle_Age'] = df_test['Vehicle_Age'].replace(vehicle_age)\ndf_test['Vehicle_Damage'] = df_test['Vehicle_Damage'].replace(vehicle_damage)\n\ndf_test['Policy_Sales_Channel'] = df_test['Policy_Sales_Channel'].apply(lambda x: np.int(x))\ndf_test['Region_Code'] = df_test['Region_Code'].apply(lambda x: np.int(x))\n\n# df_test['Annual_Premium'] = boxcox1p(df_test['Annual_Premium'], boxcox_normmax(df_test['Annual_Premium'] + 1))\ndf_test['Annual_Premium'] = df_test['Annual_Premium'].apply(np.log)\n\ndf_test.head()","33de0369":"X_sub = df_test.drop(['id'],axis=1)\nX_sub.head(2)","9e458e71":"all_classifiers.keys()","ee177eba":"# all_classifiers['LightGB'] | catb\nsub_pred = all_classifiers['XGBoost'].predict_proba(X_sub)[:, 1]\ndf_sample_submission['Response'] = sub_pred\ndf_sample_submission.to_csv(\"XGBoost_simples_aproach.csv\", index = False) ","5b909307":"## Handle Unbalanced DataSet <a id='index13'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","579076a1":"## Data Cleaning <a id='index03'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","c304e2f6":"My cat: 0.855876398703992 (2)\nMy lgbm: 0.855358671701313 (3)","c9a8ab0f":"## Split Train and Test <a id='index12'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","3fdc7cd0":"## Snippets <a id='index02'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","e0d6edaf":"### CatBoost <a id='index16'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","7516fd71":"## Submission <a id='index18'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","ec699d38":"## Develop Model <a id='index14'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n### CV, Fitting and Testing <a id='index15'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","cfcb9cce":"### Others Pre Procesing","8f8dae5b":"### Feature by Target: Response <a id='index06'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","d369935b":"## Problem Description\n\n### Goal\n\n**Classification:** Building a model to predict whether a customer would be interested in Vehicle Insurance\n\nMetrics: ROC_AUC\n\n\n### DataSet Links\n\n+ Kaggle: https:\/\/www.kaggle.com\/anmolkumar\/health-insurance-cross-sell-prediction\n+ Kaggle: https:\/\/www.kaggle.com\/jassican\/janatahack-crosssell-prediction \n+ Analytic Vidhya Hackaton: https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-cross-sell-prediction\/\n\n### DataSet Description\n\n| Variable           | Definition                                                                                                                      | Values                                               |\n|--------------------|---------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|\n| id                 | Unique ID for the customer                                                                                                      | int                                                  |\n| Gender             | Gender of the customer                                                                                                          | string :: ['Male','Female']                          |\n| Age                | Age of the customer                                                                                                             | int :: [20, 85]                                      |\n| Driving_License    | 0 : Customer does not have DL;<br>1 : Customer already has DL                                                                   | int(categorical) :: [0,1]                            |\n| Region_Code        | Unique code for the region of the customer                                                                                      | float(categorical) :: [0,52]                         |\n| Previously_Insured | 1 : Customer already has Vehicle Insurance;<br>0 : Customer doesn't have Vehicle Insurance                                      | int(categorical) :: [0,1]                            |\n| Vehicle_Age        | Age of the Vehicle                                                                                                              | string :: [ '1-2 Year'; <br>'< 1 Year'; '> 2 Years'] |\n| Vehicle_Damage     | 1 : Customer got his\/her vehicle damaged in the past;<br>0 : Customer didn't get his\/her vehicle damaged in the past.           | int(categorical) :: [0,1]                            |\n| Annual_Premium     | The amount customer needs to pay as premium in the year                                                                         | float :: [2,630 ; 513,00 ]                           |\n| PolicySalesChannel | Anonymized Code for the channel of outreaching to the customer ie.<br> Different Agents, Over Mail, Over Phone, In Person, etc. | float(categorical) :: [1.0, 163.0]                   |\n| Vintage            | Number of Days, Customer has been associated with the company                                                                   | float :: [10.0, 299.0]                               |\n| Response           | 1 : Customer is interested;<br>0 : Customer is not interested                                                                   | int(categorical) :: [0,1]                            |","1de595c7":"## Table Of Contents (TOC) <a id=\"top\"><\/a>\n\n+ [Import Libs and DataSet](#index01)\n+ [Snippets](#index02)\n+ [Data Cleaning](#index03)\n+ [EDA](#index04)\n  - [Each feature individually](#index05)\n  - [Feature by Target: Response](#index06)\n+ [Corr](#index07)\n  - [Corr to PolicySalesChannel as Categorical Feature](#index08)\n  - [Corr to RegionCode as Categorical Feature](#index09)\n  - [All Corr](#index10)\n+ [Pre-Processing](#index11)\n+ [Split Train and Test](#index12)\n+ [Handle Unbalanced DataSet](#index13)\n+ [Develop Model](#index14)\n  - [CV, Fitting and Testing](#index15)\n  - [CatBoost](#index16)\n  - [Super Leaner](#index17)\n+ [Submission](#index18)\n+ [Conclusion](#index19)\n\n## Import Libs and DataSet <a id='index01'><\/a>","aeaca701":"No completely duplicate Rows and no missing data","1522f645":"## Correlations <a id='index07'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n### Corr to PolicySalesChannel as Categorical Feature <a id='index08'><\/a> ","32034a18":"### Corr to RegionCode as Categorical Feature <a id='index09'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","76d1172a":"<h1 align=\"center\"> Vehicle Insurance: EDA and Classify <\/h1>\n\n<img src=\"https:\/\/cdn.policyx.com\/images\/benefits-12-9-19.jpg\" width=\"50%\" \/>\n\nCreated: 2020-09-16\n\nLast updated: 2020-09-16\n\nKaggle Kernel made by \ud83d\ude80 <a href=\"https:\/\/www.kaggle.com\/rafanthx13\"> Rafael Morais de Assis<\/a>","d910dd7f":"````\nALL FEATURES NO DELETE OLDS\n\nCrossValidation, Fitting and Testing :: Done in 107 min 55.379 s \nNaiveBayes     | cv_roc: 0.8116 | acc_test: 0.6690 | f1_test: 0.5880 | roc_auc: 0.8133 | took:        40.241 s |\nXGBoost        | cv_roc: 0.8551 | acc_test: 0.8751 | f1_test: 0.4829 | roc_auc: 0.8604 | took:  8 min 29.597 s | Kaggle: 0.8563787188344484\nLightGB        | cv_roc: 0.8578 | acc_test: 0.8754 | f1_test: 0.4799 | roc_auc: 0.8619 | took:  1 min 10.827 s | Kaggle: 0.857961964456837\nLogisiticR     | cv_roc: 0.8394 | acc_test: 0.8734 | f1_test: 0.4760 | roc_auc: 0.8384 | took:   1 min 9.852 s |\nAdaBoost       | cv_roc: 0.8521 | acc_test: 0.8750 | f1_test: 0.4680 | roc_auc: 0.8553 | took: 48 min 14.027 s | Kaggle: 0.8520245528346614\nGBoost         | cv_roc: 0.8551 | acc_test: 0.8750 | f1_test: 0.4668 | roc_auc: 0.8590 | took: 25 min 34.422 s | Kaggle: 0.8555352711324061\nRandomForest   | cv_roc: 0.8288 | acc_test: 0.8624 | f1_test: 0.5687 | roc_auc: 0.8345 | took:  8 min 57.028 s |\nExtraTree      | cv_roc: 0.8174 | acc_test: 0.8554 | f1_test: 0.5784 | roc_auc: 0.8222 | took: 13 min 39.380 s |\nCatBoost no_cat (no delete old) | acc_test: 0.8755 | f1_test: 0.4823 | roc_auc: 0.8621 | took:        40.531 s | Kaggle: 0.858025063578067\n````\n\n````\nSIMPLES APROACH :: Done in 10 min 49.542 s\n\nCrossValidation, Fitting and Testing\nNaiveBayes     | cv_roc: 0.8224 | acc_test: 0.6418 | f1_test: 0.5744 | roc_auc: 0.8256 | took:         0.691 s |\nXGBoost        | cv_roc: 0.8547 | acc_test: 0.8748 | f1_test: 0.4910 | roc_auc: 0.8596 | took:        55.237 s | 0.854955240293525\nLightGB        | cv_roc: 0.8576 | acc_test: 0.8752 | f1_test: 0.4791 | roc_auc: 0.8614 | took:        33.404 s | 0.8572426398003856.\nLogisiticR     | cv_roc: 0.8267 | acc_test: 0.8738 | f1_test: 0.4722 | roc_auc: 0.8316 | took:        13.953 s |\nAdaBoost       | cv_roc: 0.8511 | acc_test: 0.8751 | f1_test: 0.4667 | roc_auc: 0.8545 | took:        45.264 s |\nGBoost         | cv_roc: 0.8550 | acc_test: 0.8751 | f1_test: 0.4667 | roc_auc: 0.8592 | took:  2 min 48.569 s |\nRandomForest   | cv_roc: 0.8327 | acc_test: 0.8659 | f1_test: 0.5564 | roc_auc: 0.8371 | took:  3 min 16.980 s |\nExtraTree      | cv_roc: 0.8235 | acc_test: 0.8598 | f1_test: 0.5669 | roc_auc: 0.8267 | took:  2 min 15.439 s |\n\nCatBoost (no cat features)      | acc_test: 0.8752 | f1_test: 0.4837 | roc_auc: 0.8614 | took:        22.913 s |\nCatBoost (cat features)         | acc_test: 0.8757 | f1_test: 0.4842 | roc_auc: 0.8620 | took:        43.988 s | 0.8579703291511471\n\n\n````","b8d3592d":"## EDA <a id='index04'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n### Each feature individually <a id='index05'><\/a> ","69e36203":"### Super Leaner <a id='index17'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","7700583d":"Bigs Corr: PSC_151, PSC_26, PSC_124, PSC_160","df8630bb":"## Conclusion <a id='index19'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\nThe best model is LightGB\n\nSome References:\n\n+ https:\/\/www.kaggle.com\/jassican\/stater-notebook-cross-sell-prediction\n+ https:\/\/www.kaggle.com\/jassican\/janatahack-cross-sell-prediction\n\nstill in progres... last update 2020-09-18","a50facfa":"Big Corr to Region Codes : RC_28\n\n### All Corr <a id='index10'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n","b6c8bca3":"As most important in all features: \\['Previously_Insured', 'Vehicle_Damage', 'Age', 'Annual_Premium', 'PSC_152', 'Vehicle_Age', 'Policy_Sales_Channel', 'Vintage', 'Region_Code', 'PSC_160', 'RC_41', 'RC_8', 'RC_30', 'Gender', 'PSC_26', 'RC_28', 'RC_6', 'RC_11', 'RC_25', 'RC_29'\\]","9cfbd798":"## Pre Processing <a id='index11'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>"}}