{"cell_type":{"020cafa3":"code","66570c1e":"code","ab4cb7e9":"code","0726a290":"code","9393abca":"code","1e56e301":"code","757c04cf":"code","7160ccc4":"code","5ae63d48":"code","a873797f":"code","2ba0c693":"code","2c38f8ca":"code","3cc8beb4":"code","8d8455bb":"code","bb70adbf":"code","449bea63":"code","f8376da4":"code","f33f835d":"code","3b2b8464":"code","d8d88ca3":"code","4fc909fe":"code","e14cae68":"code","7834ac78":"code","0b8f850c":"code","c2469f9a":"code","390b38a8":"code","98cbb7c0":"code","4d89aa50":"code","281afe38":"code","ead42818":"code","db8ec440":"code","770fe7d6":"code","4ba84643":"code","66d98a39":"code","c252e660":"code","f6a15298":"code","9f8a9e89":"code","43afa214":"code","e8606426":"code","6f7214a6":"code","fdac0c6e":"code","fdb76f79":"code","cc8bf92d":"code","cf47b1f9":"code","decbbf91":"code","5097bf9c":"code","cac75192":"code","e1e10e34":"code","446032c8":"code","01d27cbc":"code","14171aa4":"code","d3eb37ba":"code","8e62884d":"markdown","430d33e8":"markdown","bd35f199":"markdown","e387b8ab":"markdown","884c8a43":"markdown","c2642426":"markdown","b1431e5b":"markdown","1a918374":"markdown","f47d7097":"markdown","76d42a0e":"markdown","6339ffe0":"markdown","23d5f8b3":"markdown"},"source":{"020cafa3":"#importing libraries pandas and numpy\n\nimport pandas as pd\nimport numpy as np","66570c1e":"#Read the dataset,make a copy,view top 5 rows of the copy.\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf=pd.read_csv('\/kaggle\/input\/telecom-customer\/Telecom_customer churn.csv')\ndf_new=df.copy()\ndf_new.head()","ab4cb7e9":"# Get the rows, columns and the feature list\nprint(\"Rows:\", df_new.shape[0])\nprint(\"Columns:\", df_new.shape[1])\nfeatures=df.columns.to_list()\nprint(features)","0726a290":"#Get the list of all columns and their number of unique values. \nunique_values=df.nunique().sort_values(ascending=True)\nfor key,value in unique_values.items():\n    if unique_values[key]<50:\n        print(key,value)","9393abca":"#Find the list of all categorical columns\ncat_cols   = df_new.nunique()[df_new.nunique() < 7].keys().tolist()\ncat_cols   = [x for x in cat_cols + ['crclscod','ethnic','area','dwllsize'] ]\ncat_cols","1e56e301":"#Find null value percentages in categorical variables. create a new data frame for df_cat\ndf_cat=df_new[cat_cols]\nnull_value_counts=(df_cat.isnull().sum()\/1000).sort_values(ascending=False)\nnull_value_counts","757c04cf":"# unique value for each cat column\n\nfor col in df_cat.columns:\n    print(col, df_cat[col].unique())","7160ccc4":"#Relace nan with UNKW in hnd_webcap\ndf.hnd_webcap=df.hnd_webcap.replace(np.nan,'UNKW')\ndf_cat.hnd_webcap=df_cat.hnd_webcap.replace(np.nan,'UNKW')\ndf_cat.head()","5ae63d48":"#Imporg libraries for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","a873797f":"# definea function where you can pass feature as parameter to the function automatically to plot the graph for each categorical variable\ndef plot_columnwise(column):\n    x,y = column, 'churn'\n    df=df_cat[df_cat[column].notnull()]\n    ax=sns.countplot(x=column,data=df,hue='churn')\n    ax.set_title('{} Vs Churn'.format(column))\n    ax.set_ylabel('Percentage of %{}'.format(column))\n    ax.set_xlabel('{}'.format(column))\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,height +300,'{:1.2f}%'.format((height\/len(df)*100)),\n            ha=\"center\")\n    plt.show()","2ba0c693":"#Vertical bar charts for features with categories < 5\ncat_cols_bar   = df_new.nunique()[df_new.nunique() < 5].keys().tolist()\ncat_cols_bar=[x for x in cat_cols_bar if x not in ['churn']]\nfor col in cat_cols_bar:\n    plot_columnwise(col)","2c38f8ca":"#Funciton for horizontal barchart when categorical features are too many to fit on vertical bar chart\ndef plot_columnwise_h(column):\n    plt.figure(figsize=(10,8))\n    x,y = column, 'churn'\n    df=df_cat[df_cat[column].notnull()]\n    ax=sns.countplot(y=column,data=df,hue='churn',orient=\"h\")\n    ax.set_title('{} Vs Churn'.format(column))\n    ax.set_ylabel('Percentage of %{}'.format(column))\n    ax.set_xlabel('{}'.format(column))\n    total=len(df)\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()\/total)\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()\/2\n        ax.annotate(percentage, (x, y))\n    plt.show()","3cc8beb4":"#Horizontal barchart except for crclscod\ncat_cols_horizontal  = df_new.nunique()[df_new.nunique() < 4].keys().tolist()\ncat_cols_horizontal=[x for x in cat_cols_horizontal if x not in ['churn','crclscod']]\n\nfor col in cat_cols_horizontal:\n    plot_columnwise_h(col)","8d8455bb":"import plotly.graph_objects as go\nfrom plotly.offline import iplot,init_notebook_mode\ninit_notebook_mode\nimport plotly.figure_factory as ff","bb70adbf":"#Plot churn value\n\nvalues=df_cat.churn.value_counts().to_list()               \nlabels = ['Yes','No']\nlayout={'title':\"Churn counts\",'legend_title_text':'Churn','width':500,'height':400}\ntrace=go.Pie(labels=labels, values=values, hole=.3)\ndata=[trace]\nfig = go.Figure(data=data,layout=layout)\niplot(fig)","449bea63":"#horizontal barchart for crclscod\ndef plot_columnwise1(column):\n    plt.figure(figsize=(10,50))\n    x,y = column, 'churn'\n    df=df_cat[df_cat[column].notnull()]\n    ax=sns.countplot(y=column,data=df,hue='churn',orient=\"h\")\n    ax.set_title('{} Vs Churn'.format(column))\n    ax.set_ylabel('Percentage of %{}'.format(column))\n    ax.set_xlabel('{}'.format(column))\n    total=len(df)\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()\/total)\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()\/2\n        ax.annotate(percentage, (x, y))\n        \n            \n    plt.show()\n\nplot_columnwise1('crclscod')","f8376da4":"#Remove the insignificant percentages and replace with other to have lesser bins\/categories\ncrclscod_unique=df_cat.crclscod.unique().tolist()\ncrclscod_retain=['A','EA','C','B','CA','AA','U','E','E4','DA','D4','ZA','Z4','A2']\nfor key,value in df['crclscod'].items():\n    if value not in crclscod_retain:\n         df['crclscod'][key]='OTHER'","f33f835d":"#Final categorical features\ncat_features=['asl_flag','crclscod','refurb_new','hnd_webcap','area','ethnic','marital']\ncat_features","3b2b8464":"# get the number of numerical columns\nnum_cols= [x for x in features if x not in (cat_cols+['Customer_ID'])]\nnum_cols.append('churn')\n\nprint(len(num_cols))","d8d88ca3":"#create data set for churn and no churn\n\nchurn=df[df['churn']==1]\nno_churn=df[df['churn']==0]","4fc909fe":"#Function to plot KDE plot \ndef kdeplot(feature):\n    plt.figure(figsize=(9, 4))\n    plt.title(\"KDE for {}\".format(feature))\n    ax0 = sns.kdeplot(no_churn[feature].dropna(), color= 'navy', label= 'Churn: No')\n    ax1 = sns.kdeplot(churn[feature].dropna(), color= 'red', label= 'Churn: Yes')\nkdeplot('income')\nkdeplot('lor')","e14cae68":"# remove the >25% null values\nnum_cols= [x for x in num_cols if x not in ['income','lor']]","7834ac78":"#Get null values. Create new data set df_2\ndf_2=df[cat_features+num_cols+['Customer_ID']]\nprint((df_2.isnull().sum()\/1000).sort_values(ascending=False).head(20))","0b8f850c":"# add some interesting ratios and then start numerical analysis\n#Ratios with revenues. Created 3 excel tabs with each bucket - revenue mou and qty and looked at relevant ratios\ndf_2['chngavg_rev_3moavg']=(df_2['avg3rev']-df_2['avgrev'])*100\/df_2['avgrev']\ndf_2['chngavg_rev_6moavg']=(df_2['avg6rev']-df_2['avgrev'])*100\/df_2['avgrev']\ndf_2['rev_adj_total_ratio']=df_2['adjrev']\/df_2['totrev']\n\n#Ratios with MOUS\n\ndf_2['chngavg_mou_3moavg']=(df_2['avg3mou']-df_2['avgmou'])*100\/df_2['avgmou']\ndf_2['chngavg_mou_6moavg']=(df_2['avg6mou']-df_2['avgmou'])*100\/df_2['avgmou']\n\n#ratios with no of calls\n\ndf_2['chngavg_qty_3moavg']=(df_2['avg3rev']-df_2['avgrev'])*100\/df_2['avgrev']\ndf_2['chngavg_qty_6moavg']=(df_2['avg6rev']-df_2['avgrev'])*100\/df_2['avgrev']\ndf_2['qty_adj_total_ratio']=df_2['adjqty']\/df_2['totcalls']","c2469f9a":"#Start with numerical analysis\n\nnumerical_cols=[x for x in df_2.columns if x not in cat_features + ['Customer_ID']]\ncorr=df_2[numerical_cols].corr()\nplt.figure(figsize=(20, 20))\nax = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, \n                 linewidths=.2, cmap=\"YlGnBu\")","390b38a8":"#Arrange top correlaton by descending order and print the first 30. write it to a file for manual analysis\ncorrelation=corr.abs().unstack().sort_values(ascending=False)\ncorrelation=correlation[correlation!=1]\nprint(correlation[0:30])\ncorrelation.to_csv('correlation.csv')","98cbb7c0":"#Remove correlation>0.9\n\n\n#Manually deciding which ones to remove. Automation removes but we have to make sure which ones we are removing to have more consistency\n#keeping totals and remocing adj, or break overs like voie and data\nto_remove=['adjqty','adjmou','adjrev','attempt_Mean','comp_vce_Mean','vceovr_Mean','comp_dat_Mean',\n          'ccrndmou_Mean','inonemin_Mean','avg3qty','avg3mou','opk_dat_Mean','avg3rev','totmou','plcd_vce_Mean',\n           'ovrmou_Mean','mou_opkd_Mean','peak_vce_Mean','peak_dat_Mean','avg6mou','avg6qty']\n\nnumerical_cols=[x for x in numerical_cols if x not in to_remove]\n#removing 11 columns and running correlation again\ncorr=df_2[numerical_cols].corr()\n\n\n\nplt.figure(figsize=(20, 20))\nax = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, \n                 linewidths=.2, cmap=\"YlGnBu\")","4d89aa50":"len(numerical_cols)","281afe38":"\n#Arrange top correlaton by descending order and print the first 30. write it to a file for manual analysis\ncorrelation1=corr.abs().unstack().sort_values(ascending=False)\ncorrelation1=correlation1[correlation1!=1]\nprint(correlation1[0:30])\ncorrelation.to_csv('correlation1.csv')","ead42818":"#create  churn and no churn datasets for df_2\nchurn=df_2[df_2['churn']==1]\nno_churn=df_2[df_2['churn']==0]","db8ec440":"#Plot for each and every numerical feature vs churn\nfig, axes = plt.subplots(ncols=5, nrows=12, figsize=(25,60))\n\nfor feature, ax in zip(numerical_cols, axes.flat):\n    sns.kdeplot( no_churn[feature].dropna(), color= 'navy', label= 'Churn: No',ax=ax,bw=0.1)\n    sns.kdeplot(churn[feature].dropna(), color= 'red', label= 'Churn: Yes',ax=ax,bw=0.1)\n    ax.set_title(\"KDE for {}\".format(feature))\nplt.show()","770fe7d6":"# get the rows and columns\ndf_2.shape","4ba84643":"#dropping numerical columns which have same kde distribution for churn vs non churn\n\nto_drop_cols=['mou_Mean','unan_vce_Mean','mou_rvce_Mean','avgmou','avgqty','avg6rev','models','rev_adj_total_ratio',\n              'qty_adj_total_ratio','totrev','opk_vce_Mean','complete_vce_ratio']\n\nnumerical_cols=[x for x in numerical_cols if x not in to_drop_cols]\nlen(numerical_cols)","66d98a39":"#Get % of null value rows\nnull_value_counts=(df_2.isnull().sum()\/1000).sort_values(ascending=False)\n\nprint(null_value_counts)\nnull_values=list(null_value_counts[null_value_counts>0].keys())","c252e660":"#Drop the null values and validate\ndf_2=df_2.dropna(axis=0)\n\nnull_value_counts=(df_2.isnull().sum()\/1000).sort_values(ascending=False)\nnull_value_counts","f6a15298":"#Merge all features that are selected\nall_cols=numerical_cols+ cat_features+['Customer_ID']\nall_cols=list(dict.fromkeys(all_cols))\nlen(all_cols)\nprint(all_cols)","9f8a9e89":"#Copy only the interesting folders to a new data frame\ndf_3=df_2[all_cols]\nprint(df_3.shape)","43afa214":"#Validate that there are no nulls in the data frame\nnull_value_counts=(df_3.isnull().sum()\/1000).sort_values(ascending=False)\nnull_values=list(null_value_counts[null_value_counts>0].keys())\nnull_values","e8606426":"#Reser the index as we removed some columns. Make a copy into df_4\ndf_3=df_3.reset_index()\ndf_4=df_3.copy()","6f7214a6":"# Label encoding for binary features\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n\nle=LabelEncoder()\ndf_4['asl_flag']=le.fit_transform(df_4['asl_flag'])\ndf_4['refurb_new']=le.fit_transform(df_4['refurb_new'])\n\n\n#Hot encoding for remaining categorical features\ndf_4= pd.get_dummies(data = df_4, drop_first=True,columns =['crclscod','hnd_webcap','area','ethnic','marital'])","fdac0c6e":"#Scaling Numerical columns\nnumerical_cols=[x for x in numerical_cols if x not in ['churn','Customer_ID']]\nstd = StandardScaler()\nscaled = std.fit_transform(df_4[numerical_cols])\nscaled = pd.DataFrame(scaled,columns=numerical_cols)","fdb76f79":"#Drop numerical columns as we have created a scaled copy in scaled dataframe\ndf_4= df_4.drop(columns = numerical_cols,axis = 1)","cc8bf92d":"#Merge scaled copy and df_4 into a new dataframe. We will use this dataframe for our training, testing and model implmentation\ndf_5 = df_4.merge(scaled,left_index=True,right_index=True,how = \"left\")\ndf_5=df_5.drop('Customer_ID',axis=1)\ndf_5.shape","cf47b1f9":"# Split data set into training set and test set\nfrom sklearn.model_selection import train_test_split\ntrain,test = train_test_split(df_5,test_size = .25 ,random_state = 0)\nX_train=train.drop(['churn'],axis=1)\nX_test=test.drop(['churn'],axis=1)\n\ny_train=train['churn']\ny_test=test['churn']","decbbf91":"#Import the libraries for all the models needed\nfrom sklearn.linear_model import LogisticRegression,LogisticRegressionCV,SGDClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score,f1_score,precision_score,recall_score,roc_auc_score\nfrom sklearn.metrics import classification_report,roc_curve\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier","5097bf9c":"#create an empty dataframe to store metrics from each model\nmetrics=pd.DataFrame(columns=['accuracy','roc_auc_score','f1_score','precision_score','recall_score'])","cac75192":"#Baseline model with graphs\ndef classification_model(classifier,X_train,y_train,X_test,y_test,name):\n    global metrics\n    # Fitting the training set to model and predicting on the test set\n    \n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    y_prob = classifier.predict_proba(X_test)\n    \n    # Create cpnfusion matrix and store other metrics in variables. Write the metrics to data frame\n    \n    cm = confusion_matrix(y_test, y_pred)\n    accuracy=accuracy_score(y_test, y_pred)\n    roc_auc= roc_auc_score(y_test,y_pred)\n    f1score=f1_score(y_test,y_pred)\n    precision=precision_score(y_test,y_pred)\n    recall=recall_score(y_test,y_pred)\n    \n    #Writing to metrics data frame\n    metrics.loc[name]=[accuracy,roc_auc,f1score,precision,recall]\n\n    # calculate roc curves\n    \n    fpr, tpr, _ = roc_curve(y_test, y_prob[:,1], pos_label=1)\n\n    \n    #Print the Classification report\n    print(name,' Metrics')\n    print('Confusion Matrix: \\n' , cm)\n    print('Classificaiton Report:\\n',classification_report(y_test,y_pred))\n    \n    \n    #Plot ROC AUC and Confusion Matrix\n    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(15,5))\n    sns.heatmap(cm,annot=True,xticklabels=['Churn No','Churn Yes'], ax=axes[0],\n                    yticklabels=['Churn No','Churn Yes'],cmap='viridis',fmt='g'\n                )\n    axes[0].set_title('{} Confusion Matrix'.format(name))\n    \n    sns.scatterplot(fpr,tpr,color='r',label = \"AUC = \" + str(np.around(roc_auc_score(y_test,y_prob[:,1]),3)),ax=axes[1])\n    sns.lineplot(x=[0,1], y=[0,1],ax=axes[1])\n    axes[1].lines[0].set_linestyle(\"--\")\n    axes[1].set_title('{} ROC Curve'.format(name))\n    axes[1].set_xlabel('False Positive Rate')\n    axes[1].set_ylabel('True Positive Rate')\n    \n    #Get the top features\n    \n    if name in ['Logistic Regression','SVC Linear','Decision Tree', 'Random Forest'] :\n        if name in ['Logistic Regression','SVC Linear']:\n            coeff_df=[]\n            coeff_df=pd.DataFrame(classifier.coef_.ravel())\n        elif name in ['Decision Tree', 'Random Forest'] :\n            coeff_df=[]\n            coeff_df  = pd.DataFrame(classifier.feature_importances_)\n        coeff_df.columns=['coefficients']\n        df_cols=pd.DataFrame((X_train.columns))\n        df_cols.columns=['features']\n\n        coeff_df=pd.merge(coeff_df,df_cols,left_index=True,right_index=True,how='left')\n        coeff_df=pd.DataFrame.sort_values(coeff_df,axis=0,by='coefficients')\n        print('Feature importance - top 6 positive coefficients' )\n        print(coeff_df.tail(6))\n        print('Feature importance top 6 negative coefficients')\n        print(coeff_df.head(6))\n    \n    \n\n\n","e1e10e34":"#SVC models take a long time to run on huge data sets. so ignoring this for now\n'''\nclassifiers = {\n        'SVC Linear': SVC(kernel = 'linear', random_state = 0),\n        'SVC rbf': SVC(kernel = 'rbf', random_state = 0)\n\n\n}\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    classification_model(classifier,X_train,y_train,X_test,y_test,name,)\n    '''","446032c8":"#All Classifiers Except SVC because it takes a lot of time to run on a 100,000 dataset\n\nclassifiers = {\n        'Logistic Regression': LogisticRegression(random_state=0,solver='liblinear'),\n    \n\n    \n        'KNN Classifier': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n                           weights='uniform'),\n\n        'Decision Tree': DecisionTreeClassifier(criterion = 'entropy', random_state = 0),\n    \n        'Random Forest': RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0),\n        'XGBoost': XGBClassifier()\n}\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    classification_model(classifier,X_train,y_train,X_test,y_test,name,)","01d27cbc":"# Artificial Neural Network model\n\n# Initializing the ANN\nann = tf.keras.models.Sequential()\n\nfrom keras.layers.core import Dropout\n# Adding the input layer and the first hidden layer\nann.add(tf.keras.layers.Dense(units=1024, activation='relu'))\nann.add(tf.keras.layers.Dropout(0.2))\n\n# Adding the second hidden layer\nann.add(tf.keras.layers.Dense(units=512, activation='relu'))\nann.add(tf.keras.layers.Dropout(0.2))\n\n# Adding the second hidden layer\nann.add(tf.keras.layers.Dense(units=256, activation='relu'))\nann.add(tf.keras.layers.Dropout(0.2))\n\n# Adding the second hidden layer\nann.add(tf.keras.layers.Dense(units=128, activation='relu'))\nann.add(tf.keras.layers.Dropout(0.2))\n\n# Adding the output layer\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n\n\n# Part 3 - Training the ANN\n\n# Compiling the ANN\nann.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Training the ANN on the Training set\nfit_keras=ann.fit(X_train, y_train, batch_size = 32, epochs = 20,validation_data=(X_test, y_test))\n\nann.summary()\n# Part 4 - Making the predictions and evaluating the model\n# Predicting the Test set results\ny_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)\n# Predicting the result of a single observation\n\naccuracy = ann.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Score: {:.4f}\".format(accuracy[0]))\nprint(\"Testing Accuracy: {:.4f}\".format(accuracy[1]))\n\n\naccuracy = ann.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Score: {:.4f}\".format(accuracy[0]))\nprint(\"Training Accuracy: {:.4f}\".format(accuracy[1]))","14171aa4":"# Applying PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_train1=X_train\nX_test1=X_test\nX_train1 = pca.fit_transform(X_train1)\nX_test1 = pca.transform(X_test1)\n\n# Training the Logistic Regression model on the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassification_model(classifier,X_train1,y_train,X_test1,y_test,'PCA')","d3eb37ba":"metrics\n","8e62884d":"\nFor many features, Churn No% is slightly higher than Yes % for the  different categories of the feature. This is as expected because overall No is slightly higher than Yes. For most features the difference between No and Yes is less than 2%. The difference is acceptable if the overall count for the category is higher but not acceptable when the overall count is lesser.\n\n\nExample: In the area vs churn plot North west mountain area No Vs yes is 1.9% vs 2.5%. This is a considerable difference and ppl from that area have a higher chance of churn than other areas.\nBelow are the features which might impact Churn.\n\n1. ASL_FLAG:  When flag is N, Churn is higher than no churn and when flag is Y difference is >2%. So this feature is included\n2.  dual_band: when flag is N, slightly higher chance that customer will churn\n3. refurb_new: ppl with refurbished handsets are likely to churn than ppl with no refurbished hs\n4. hnd_webcap : >2% difference for some categories.\n5. martial: Looks like unmarried ppl churn is slightly greater\n6. ethnic: some ethnicities like H,O,D churn more than the rest\n7. Area: some areas churn more than the rest\n8. crclscod - too many categories (53 in total). we can leave 5 categories and change the rest to other.\n\n\nHHStatin,numbcars,pwnrent,dwlltype,adjult,infobase have >20% of null values and with the remaining data in the plot do not have too significant impact on churn. So these are removed\ncredited though <2% has slight impact as ppl with no cc are slighty churning less. This is a  may be to include but less than .5% difference . So excluding this for now","430d33e8":"# Numerical varaibles analysis","bd35f199":"Income,Activesubs, models, phones,hnd_price,income,lor (though they have bands) are all numerical\ncrclscod is categorical though it has 54 unique values. Ethnic ,Area,dwelling size and area are categorical as well.\n\nRegression and SVM are algebraic so will leave numerical as is. We can drop churn as well as it is dependent.","e387b8ab":"# Explore Categorical Data","884c8a43":"# Data pre processing for Numerical columns","c2642426":"## Data Overview","b1431e5b":"# Correlation Matrix","1a918374":"The above two features can be removed as they have close >25% null values and also the distribution is not very different for churn and no churn","f47d7097":"# Machine Learning models","76d42a0e":"# Telecom Churn","6339ffe0":"# Plot bar chart for categorical variables wrt Churn","23d5f8b3":"# KDE plot for each numerical feature wrt to Churn"}}