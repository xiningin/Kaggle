{"cell_type":{"c1153daa":"code","bd150732":"code","f06e2931":"code","86c06b82":"code","6e471d56":"code","09afaaf3":"code","dc6a1e62":"code","8bc2102e":"code","aaee0b17":"code","4801ceac":"code","7c41f8e7":"code","181b2cc3":"code","d33bdc3a":"code","85e84500":"markdown","c7c69254":"markdown","abafcf3b":"markdown","4da480ef":"markdown","e1c179c7":"markdown","0eae8cb8":"markdown","ce96ac8d":"markdown","a04d524b":"markdown","b2ba1883":"markdown","ecfc1b50":"markdown","310a2c64":"markdown","a33d0d81":"markdown","b33034b1":"markdown","6b6ac771":"markdown","9854e189":"markdown","7e53e031":"markdown"},"source":{"c1153daa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sqlite3\nimport regex as re\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd150732":"#create dataframe from csv\ndf = pd.read_csv('\/kaggle\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv',encoding='ISO-8859-1')\ndf[\"Category\"].replace({'ham': '0','spam': '1'}, inplace=True)\ndf.head()","f06e2931":"df['Category'] = df['Category'].astype(int)","86c06b82":"df.info()","6e471d56":"print(\"spam count: \" +str(len(df.loc[df.Category==1])))\nprint(\"not spam count: \" +str(len(df.loc[df.Category==0])))\nprint(df.shape)\n\ndf = df.drop_duplicates()\ndf = df.reset_index(inplace = False)[['Category','Message']]\nprint(df.shape)","09afaaf3":"clean_desc = []\nfor w in range(len(df.Message)):\n    desc = df['Message'][w].lower()\n    \n    #remove punctuation\n    desc = re.sub('[^a-zA-Z]', ' ', desc)\n    \n    #remove tags\n    desc=re.sub(\"&lt;\/?.*?&gt;\",\" &lt;&gt; \",desc)\n    \n    #remove digits and special chars\n    desc=re.sub(\"(\\\\d|\\\\W)+\",\" \",desc)\n    \n    clean_desc.append(desc)\n#assign the cleaned descriptions to the data frame\ndf['Message'] = clean_desc\n        \ndf.head(3)","dc6a1e62":"stop_words = ['is','you','your','and', 'the', 'to', 'from', 'or', 'I', 'for', 'do', 'get', 'not', 'here', 'in', 'im', 'have', 'on', 're', 'new', 'subject']","8bc2102e":"wordcloud = WordCloud(width = 800, height = 800, background_color = 'black', stopwords = stop_words, max_words = 1000, min_font_size = 20).generate(str(df['Message']))\n#plot the word cloud\nfig = plt.figure(figsize = (8,8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","aaee0b17":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble \nfrom sklearn.metrics import classification_report, accuracy_score","4801ceac":"#list of sentences\ntext = df.Message\n#instantiate the class\ncv = CountVectorizer()\n#tokenize and build vocab\ncv.fit(text)\nprint(cv.vocabulary_)\n#transform the text\nvector = cv.transform(text)\nprint(vector.toarray())","7c41f8e7":"text_vec = CountVectorizer().fit_transform(df['Message'])\nX_train, X_test, y_train, y_test = train_test_split(text_vec, df['Category'], test_size = 0.90, random_state = 75, shuffle = True)","181b2cc3":"classifier = ensemble.GradientBoostingClassifier(\n    n_estimators = 500, #how many decision trees to build\n    learning_rate = 2.0, #learning rate\n    max_depth = 500\n)","d33bdc3a":"classifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\nprint(classification_report(y_test, predictions))","85e84500":"**The Classifier**","c7c69254":"**Ham is Called 'not spam' so I changed it to '0' and spam is marked as '1' **","abafcf3b":"**Import Dependencies**","4da480ef":"# Exploratory Analysis","e1c179c7":"Bag of Words Method","0eae8cb8":"**I am using the GradientBoostingClassifier() model from the Scikit-Learn Ensemble collection.**","ce96ac8d":"Finally, we fit the data, call predict and generate the classification report. Using classification_report(), it is easy to build a text report showing the main classification metrics.","a04d524b":"**This is a binary classification problem since an email can either be spam (1) or not spam (0). I want to build a machine learneing model that can identify whether or not an email is spam. I am going to use the Python library Scikit-Learn to explore tokenization, vectorization, and statistical classification algorithms.**","b2ba1883":"Import the Scikit-Learn functionality we need to transform and model the data. I will use CountVectorizer, train_test_split, ensemble models, and a couple metrics.","ecfc1b50":"**Generate Predictions**","310a2c64":"**The CountVectorizer is counting the tokens and allowing me to construct the sparse matrix containing the transformed words to numbers.**","a33d0d81":"**our model achieved 95% accuracy**","b33034b1":"Before anything, it is best to do a quick analysis of the data to eliminate duplicate rows and establish some baseline counts. I use pandas drop_duplicates to drop the duplicate rows.","6b6ac771":"# Spam Detection","9854e189":"#  Word Cloud","7e53e031":"**Word clouds are a useful way to visualize text data because they make understanding word frequencies easier. Words that appear more frequently within the email text appear larger in the cloud. Word Clouds make it easy to identify \u201ckey words.\u201d**"}}