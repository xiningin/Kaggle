{"cell_type":{"d21e0860":"code","21fd6e1d":"code","c5a8f1df":"code","fe0e3333":"code","09e29a10":"code","f2afdc30":"code","02e1fa7a":"code","9a999dbc":"code","aee57376":"code","43f20ba0":"code","f0e414f9":"code","5d5e7c5f":"code","5db5c6b4":"code","ef055de0":"code","86fce197":"code","e930eae0":"code","181b2c7d":"code","1616a63d":"code","c1d7f739":"code","c8827850":"code","154932fa":"markdown","79f75986":"markdown","7b235646":"markdown","2a5bc5ff":"markdown","6707cfe7":"markdown","8bbd7d91":"markdown","19ae487d":"markdown","91ac3e2f":"markdown","73d83187":"markdown","793f1b1e":"markdown","bef529d6":"markdown","38642c91":"markdown","9e41c16f":"markdown","e49190f2":"markdown","14423d1f":"markdown","2a3d15ab":"markdown","75e6bffb":"markdown","8e2b651b":"markdown","8d6cb026":"markdown","463493bb":"markdown","94fcd232":"markdown","f815a7fc":"markdown","c4eb31d3":"markdown"},"source":{"d21e0860":"import random\nimport pandas as pd\nimport numpy as np\nfrom wordcloud import WordCloud\nimport ast\nimport collections\nimport heapq\nimport re\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport string\nimport matplotlib.pyplot as plt\nnltk.download('stopwords')\nimport cufflinks as cf\nimport plotly.express as px\nimport plotly.graph_objects as go\n%matplotlib inline\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()","21fd6e1d":"posts_data = pd.read_csv('..\/input\/parler-data\/parler_postsData.csv')\nuser_data = pd.read_csv('..\/input\/parler-data\/parler_userData.csv')\nposts_data.head()","c5a8f1df":"user_data.head()","fe0e3333":"posts_data.shape, user_data.shape","09e29a10":"user_data['Verified\/Human'] = ''\nuser_data['Mentions'] = 0\nuser_data.loc[(user_data['Verified'] == True) & (user_data['Human'] == True), 'Verified\/Human'] = 'Verified and Human'\nuser_data.loc[(user_data['Verified'] == False) & (user_data['Human'] == False), 'Verified\/Human'] = 'Neither Verified nor Human'\nuser_data.loc[(user_data['Verified'] == True) & (user_data['Human'] == False), 'Verified\/Human'] = 'Verified but Not Human'\nuser_data.loc[(user_data['Verified'] == False) & (user_data['Human'] == True), 'Verified\/Human'] = 'Not Verified but Human'\nmentions = list(posts_data['At'])\nmentions_dict = collections.defaultdict(lambda: 0)\nfor d in mentions:\n    for user in ast.literal_eval(d).values():\n        mentions_dict[user] += 1\nfor user, ment in mentions_dict.items():\n    user_data.loc[user_data['Id'] == user, 'Mentions'] = ment\npalette = {'NA': 'black',\n           'Verified and Human': 'rgb(124,255,200)', \n           'Neither Verified nor Human': 'rgb(255,124,179)', \n           'Verified but Not Human': 'rgb(124,179,255)', \n           'Not Verified but Human': 'rgb(124,255,200)'}\nposts_data['CreatedAt'] = pd.to_datetime(posts_data.CreatedAt, format='%Y%m%d%H%M%S').dt.floor('h')\nuser_data['Joined'] = pd.to_datetime(user_data.Joined, format='%Y%m%d%H%M%S').dt.floor('h')\nuser_data.head()","f2afdc30":"def percent(x):\n    return (x\/total_posts)*100\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n      u\"\\U0001F600-\\U0001F64F\"\n      u\"\\U0001F300-\\U0001F5FF\"  \n      u\"\\U0001F680-\\U0001F6FF\" \n      u\"\\U0001F1E0-\\U0001F1FF\"  \n      u\"\\U0001F1F2-\\U0001F1F4\"  \n      u\"\\U0001F1E6-\\U0001F1FF\" \n      u\"\\U0001F600-\\U0001F64F\"\n      u\"\\U00002702-\\U000027B0\"\n      u\"\\U000024C2-\\U0001F251\"\n      u\"\\U0001f926-\\U0001f937\"\n      u\"\\U0001F1F2\"\n      u\"\\U0001F1F4\"\n      u\"\\U0001F620\"\n      u\"\\u200d\"\n      u\"\\u2640-\\u2642\"\n      \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text\n\ndef remove_links(text):\n    rem_url=re.sub('http\\S+', '',text)\n    rem_email=re.sub('\\S*[a-zA-Z0-9._\\-]+@[a-zA-Z0-9._\\-]+.[a-zA-Z0-9_\\-]+s*','',rem_url)\n    return rem_email\n\ndef remove_art_connector(text):\n    articles = [\"GET\",\"THEM\",\"WHAT\",\"OUT\",\"FROM\",\"HAVE\",\"HERE\",\"WE\",\"ALL\",\"THERE\",\"TO\",\"ALSO\",\"AND\",\"AS\",\"BUT\",\"YET\",\"YOU\",\"THE\",\"WAS\",\"FOR\",\"ARE\",\"THEY\",\"THIS\",\"THAT\",\"WERE\",\"WITH\",\"YOUR\",\"JUST\",\"WILL\",\"NOT\"]\n    for a in articles:\n        text = text.replace(a, '')\n    return text\n  \ndef remove_stopwords(text):\n    words = stopwords.words('english')\n    for w in words:\n        text = text.replace(w, '')\n    return text\n    \ndef remove_punc(text):\n    return text.strip(string.punctuation)\n\ndef remove_mentions(text):\n    return re.sub('[@]\\w+\\s*','',text)\n\ndef remove_hashtags(text):\n    return re.sub('[#]\\w+\\s*','',text)\n\ndef remove_nline(text):\n    return re.sub(r'\\\\N','',text)\n\ndef filter_text(text, post_lens):\n    filtered_text = []\n    for post in text:\n        if type(post)==str:\n            post = remove_punc(remove_links(remove_emoji(remove_nline(remove_hashtags(remove_mentions(remove_art_connector(remove_stopwords(post.upper()))))))))\n            post_lens.append(len(re.findall(r'\\w+', post)))\n            filtered_text.append(post)\n    return ''.join(filtered_text)\n\ndef gen_word_cloud(text, c):\n    if c == 'rainbow':\n        cm = 'rainbow'\n        bg = 'navy'\n    elif c == 'navy':\n        cm = 'Set2'\n        bg = 'navy'\n        \n    wordcloud = WordCloud(width = 2550, height = 1600, random_state=42, background_color=bg, colormap=cm, collocations=False, min_word_length=3).generate(text)\n    plt.figure(figsize=(10, 10), dpi=300)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n    return wordcloud.process_text(text)\n\ndef plot_top_10_users_by(by, data):\n    fig = px.bar(data, y='Username', x=by, color='Verified\/Human', text=by,\n                 title=f'Top 10 users by {by}', labels={'Name':'Usernames'})\n    fig.update_traces(texttemplate='%{text:.2s}', textposition='outside', cliponaxis=False)\n    fig.update_layout(yaxis_categoryorder = 'total ascending')\n    return fig\n\ndef get_top10_hashtags(posts, post_lens):\n    occ_length = collections.defaultdict(lambda: [0, 0])\n    i = 0\n    for post in posts:\n        if type(post) == str:\n            hashtags = set(re.findall(r'#(\\w+)', post))\n            length = post_lens[i]\n            i += 1\n            if hashtags:\n                for tag in hashtags:\n                    occ_length[tag][0] += 1\n                    occ_length[tag][1] += length\n                    \n    by_length = [None]*len(occ_length)\n    by_occ = [None]*len(occ_length)\n    i = 0\n    for k, v in occ_length.items():\n        by_length[i] = [k, v[1]\/v[0]]\n        by_occ[i] = [k, v[0]]\n        i += 1\n    return heapq.nlargest(10, by_length, key=lambda x: x[1]), heapq.nlargest(10, by_occ, key=lambda x: x[1])","02e1fa7a":"user_posts = pd.merge(user_data, posts_data, left_on='Id',right_on='Creator', how='left').groupby(by='Id_x')","9a999dbc":"# Processing top 10\nuser_postCount = user_posts.size().sort_values(ascending=False)\ntotal_posts = user_postCount.sum()\n\ntop10_users = ['Total']\ntop10_count = [percent(user_postCount[:10].sum())]\nverified_human = ['NA']\nfor index, value in user_postCount[:10].items():\n    user = user_data.loc[user_data['Id'] == index]\n    top10_users.append(user.Username.item())\n    top10_count.append(percent(value))\n    verified_human.append(user['Verified\/Human'].item())\n\n# Plotting\nfig = px.bar(x=top10_users, y=top10_count, color=verified_human, text=top10_count,\n             title='Top 10 users for generating most Content',\n             labels={'y':'Percentage of Total Posts', 'x': 'Usernames', 'color': 'Verified\/Human'})\nfig.update_traces(texttemplate='%{text:.2f}%', textposition='outside', cliponaxis=False)\nfig.update_layout(xaxis_tickangle=-45)\nfig.update_layout(xaxis_categoryorder = 'total descending')","aee57376":"top10_byUpvotes = user_data.sort_values('Score', ascending=False)[:10]\ntop10_byInteractions = user_data.sort_values('Interactions', ascending=False)[:10]\ntop10_byMentions = user_data.sort_values('Mentions', ascending=False)[:10]","43f20ba0":"plot_top_10_users_by('Score', top10_byUpvotes)","f0e414f9":"plot_top_10_users_by('Interactions', top10_byInteractions)","5d5e7c5f":"plot_top_10_users_by('Mentions', top10_byMentions)","5db5c6b4":"text = top10_byInteractions.Bio.tolist()\ntext = filter_text(text, [])\nwordCount = gen_word_cloud(text, 'navy')","ef055de0":"%%time\ntext = posts_data.Body.tolist()\npost_lens = []\ntext = filter_text(text, post_lens)\nwordCount = gen_word_cloud(text, 'rainbow')","86fce197":"top10_trending_words = sorted(wordCount, key=wordCount.get, reverse=True)[:10]\ntop10_trending_words_count = [wordCount[word] for word in top10_trending_words]\nfig = px.bar(x=top10_trending_words, y=top10_trending_words_count, color=top10_trending_words,\n             text=top10_trending_words_count, title='Top 10 Trending Words',\n             labels={'x':'Words', 'y': 'Occurence', 'color': 'Words'})\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside', cliponaxis=False)\nfig.update_layout(xaxis_tickangle=-45)","e930eae0":"posts_list = posts_data.Body.tolist()\nhashtags_by_length, hashtags_by_occ = get_top10_hashtags(posts_list, post_lens)","181b2c7d":"tags = ['#'+h[0] for h in hashtags_by_occ]\ncount = [h[1] for h in hashtags_by_occ]\nfig = px.bar(x=tags, y=count, color=tags, text=count, title='Top 10 HashTags by Occurence',\n             labels={'x':'Hashtags', 'y': 'Occurence', 'color': 'Hashtags'})\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside', cliponaxis=False)\n\nfig.update_layout(xaxis_tickangle=-45)","1616a63d":"tags = ['#'+h[0] for h in hashtags_by_length]\ncount = [h[1] for h in hashtags_by_length]\nfig = px.bar(x=tags, y=count, color=tags, text=count, title='Top 10 HashTags by Average Post Length',\n             labels={'x':'Hashtags', 'y': 'Average post Length', 'color': 'Hashtags'})\nfig.update_traces(textposition='outside', cliponaxis=False)\n\nfig.update_layout(xaxis_tickangle=-45)","c1d7f739":"posts_byDate = posts_data.groupby('CreatedAt').size().to_frame()\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=posts_byDate.index, y=posts_byDate[0], mode='lines+markers', name='Posts Creation', \n                         line={'color': 'firebrick'}))\nfig.update_layout(\n    title='Content Generation TimeSeries Graph',\n    xaxis= dict(\n        title='Date and time', showline=True, showgrid=True, showticklabels=True, linecolor='rgb(204,204,204)', linewidth=2, ticks='outside',\n        tickfont=dict(family='Arial', size=12, color='rgb(82,82,82)')\n    ), \n    yaxis= dict(\n        title='Number of Posts Created', showline=True, showgrid=True, showticklabels=True, linecolor='rgb(204,204,204)', linewidth=2, ticks='outside',\n        tickfont=dict(family='Arial', size=12, color='rgb(82,82,82)')\n    ), \n)","c8827850":"users_byDate = user_data.groupby('Joined').size().to_frame()\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=users_byDate.index, y=users_byDate[0], mode='lines+markers', name='User Account Creating'))\nfig.update_layout(\n    title='User Account Creation TimeSeries Graph',\n    xaxis= dict(\n        title='Date and time', showline=True, showgrid=True, showticklabels=True, linecolor='rgb(204,204,204)', linewidth=2, ticks='outside',\n        tickfont=dict(family='Arial', size=12, color='rgb(82,82,82)')\n    ), \n    yaxis= dict(\n        title='Number of Users Joined', showline=True, showgrid=True, showticklabels=True, linecolor='rgb(204,204,204)', linewidth=2, ticks='outside',\n        tickfont=dict(family='Arial', size=12, color='rgb(82,82,82)')\n    ), \n)","154932fa":"## User Account Creation on Parler","79f75986":"**My Inferences:**\n* The number of posts per day were very few till around pre November, from where posts per day started getting hot as election was approaching.\n* From start of January there is an explosion in number of posts per day, which can be correlated to increase in bot activity and posting and spreading of propaganda in large scale also when Trump and his supporters were spreading election fraud related posts as we see their popularity in the HashTags too.\n* A majority of posts were created during the time when trump supporters where spreading misinformation about election fraud.\n* The peak in posts is during the capitol storming event (Jan 6th). \u2028","7b235646":"## Mentions","2a5bc5ff":"## By Occurence","6707cfe7":"**My Inferences:**\n* Majority of top 10 users by upvotes are verified which implies that they are famous or known. The human part basically says whether they are a personality or if not then the user mostly represents an organization (which makes sense when we found the word official a lot in their bios) and they are expected to have a huge fan base.\n* From the top10 interaction as well as we can see that most of the verified users are most active.\n* Top10 mentions also we can see that verified accounts are mostly at the top. This might mean that most of the users like interacting and mentioning with verified accounts mostly by tagging them in their posts. By looking at the word cloud we can say that Parler is the most popular word as it is platform which is used, then official,engineer, founder, investor tells us that the people here are largely from this domain, many people are married as dad is also a prominent word.\n* Words like \u201cFree\", \u201cInvestor\u201d, \u201cFounder\u201d, \u201cOrg\u201d, \u201cCEO\" show that the top users are more often into business and investment.\n* The username \"Marklevinshow\u201d is on top 10 in all the 3 categories. According to online sources Mark Levin is a Trump supporter i.e a right winger and him being popular on this platform shows that majority of users on this platform are conservatives. Same can be said about several other popular usernames (SeanHanity, DineshDSouza etc.) which show up in the above graphs.\u2028","8bbd7d91":"# Wordcloud From Bio of Top 10 Most Content Generating Users","19ae487d":"# Helper Functions","91ac3e2f":"## Interactions","73d83187":"# Top 10 Usernames Responsible For Generating Most Content","793f1b1e":"## Content Generation of Parler","bef529d6":"# Time Series Plots","38642c91":"# Top HashTags","9e41c16f":"**My Inferences:**\n* There are spikes in user account creation around Jan 2019, July 2019, July 2020 and Jan 2021. As the election was approaching the spikes got bigger.\n* When comparing to the previous graph on content generation, we do not see any spikes in 2019, but only until late 2020, i.e the accounts which were created before late 2020 did not involve in US election related posts but some other events.\n* The Dec 2018 spike could be due to a tweet from the famous activist Candace Owens.\n* The July 2019 spike could be due to migration of large number of Saudi prince's supporters from Twitter to Parler after censorship issues.\n* The peak in Nov 2020 was before election day, as the trump supporters migrated from twitter to parler for fear of being accused of spreading misinformation about the election (flagged tweets).","e49190f2":"# Preprocessing","14423d1f":"# Parler Data Analysis","2a3d15ab":"## By Average Post Length","75e6bffb":"# Top 10 Usernames With Most","8e2b651b":"**My Inferences:**\n* Total content generated by top 10 users cumulatively is 15.36% i.e just the 0.044% of users are producing 15.35% of posts on Parler which is a lot. From this we infer that only the top few users are generating majority of content.\u2028\n* 4 out of top 10 most content generating users are neither verified nor humans, which are most probably bots. This is concerning since they are probably being used to spread large amount of one sided views and propaganda and they have become popular too.\n* The rest 6 are humans but not famous personalities (not verified) i.e probably not holding some very important social positions in the society as a famous personality would hold.\n* The top user is patriot4us by a pretty large margin and as the name suggests and the top posts on the platform (and further analysis about the alignment of majority of people in this platform) that its posts are most often about elections and right winged.","8d6cb026":"* All of these hashtags have appeared only in 1 post whose length has been longer than average and hence ended up being top hashtags by length of post. Barely any of them are elections related.\n* Pretty much all the election related posts had a very short length (average was around 13-15 words) and did not describe the situation in depth and was merely used to spread the word around and be a part of it without involving much thought to it.","463493bb":"## Upvotes","94fcd232":"#stopthesteal, #voterfraud, #electionfraud and all other similar hashtags are the most\noccurring hashtags which shows that the platform has been used mostly to spread the Republican\u2019s and Trump's propaganda about the elections being fraud and to let trump know their support.\u2028","f815a7fc":"* Most common words are related to the US election, Trump, and about the election being fraud.\n* Most of the words are in favor of the republicans as we saw earlier too that the users are mainly conservatives.","c4eb31d3":"# Major Words in Posts"}}