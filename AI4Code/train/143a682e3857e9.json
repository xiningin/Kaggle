{"cell_type":{"f2fa6eb1":"code","db6359ac":"code","33c39369":"code","d5485328":"code","2f9343da":"code","5db27741":"code","5a9fac55":"code","f59eea9a":"code","e8d5cd97":"code","850784d6":"code","faddc773":"code","530c55f2":"code","b3b738fe":"code","8e3b2570":"code","0edf0bbd":"code","fda01ede":"code","9ca0dbdf":"code","f143b087":"code","fc4bb970":"code","d77b91a1":"code","d36af9aa":"code","9bd0101a":"code","7b2be67e":"code","c2f09ec2":"code","406f06e2":"markdown","9105f44d":"markdown","5950c46e":"markdown","51a1f9b0":"markdown","fcb9b8a3":"markdown","1723f1bd":"markdown","699d38b8":"markdown","69b1fdce":"markdown","cd75357d":"markdown","ee9a9651":"markdown","8bd1bfb9":"markdown","668ad7cd":"markdown","9500aac2":"markdown","8f07aa32":"markdown","bb527d40":"markdown","4d7f759e":"markdown"},"source":{"f2fa6eb1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas import DataFrame\n%matplotlib inline","db6359ac":"train_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")","33c39369":"train_data.head()","d5485328":"train_data.shape","2f9343da":"model_train_data_unscaled = train_data.drop(['label'], axis=1)\nmodel_train_label = train_data['label']","5db27741":"plt.imshow(np.array(model_train_data_unscaled.loc[10]).reshape(28, 28), cmap='Greys')\nprint(model_train_label[10])","5a9fac55":"from sklearn.preprocessing import MinMaxScaler\n\nstd_scaler = MinMaxScaler()\nmodel_train_data = std_scaler.fit_transform(model_train_data_unscaled.astype(np.float64))","f59eea9a":"from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve\n\ndef model_def(model, model_name, m_train_data, m_train_label):\n    model.fit(m_train_data, m_train_label)\n    s = \"predict_\"\n    p = s + model_name\n    p = model.predict(m_train_data)\n    cm = confusion_matrix(m_train_label, p)\n    precision = np.diag(cm)\/np.sum(cm, axis=0)\n    recall    = np.diag(cm)\/np.sum(cm, axis=1)\n    F1 = 2 * np.mean(precision) * np.mean(recall)\/(np.mean(precision) + np.mean(recall))\n    cv_score = cross_val_score(model, m_train_data, m_train_label, cv=3, scoring='accuracy')\n    print(\"Precision Is      :\", np.mean(precision))\n    print(\"Recall Is         :\", np.mean(recall))\n    print(\"F1 Score IS       :\", F1)\n    print(\"Mean CV Score     :\", cv_score.mean())\n    print(\"Std Dev CV Score  :\", cv_score.std())","e8d5cd97":"from sklearn.linear_model import LogisticRegression\n\nsoftmax = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial', C=0.05)\nmodel_def(softmax, \"softmax\", model_train_data, model_train_label)","850784d6":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 200)\nmodel_train_data2D = pca.fit_transform(model_train_data)\nprint(\"Explained Variance Ratio:\", np.sum(pca.explained_variance_ratio_))","faddc773":"from sklearn.svm import SVC\n\npoly6 = SVC(C=2, kernel='poly', degree=3, gamma='auto', random_state=42)\nmodel_def(poly6, \"poly6\", model_train_data2D, model_train_label)","530c55f2":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier","b3b738fe":"def build_classifier():\n    classifier = Sequential([Dense(128, kernel_initializer='random_uniform', activation='relu', input_shape=(200,)),\n                             Dropout(rate=0.2),\n                             Dense(128, kernel_initializer='random_uniform', activation='relu'),\n                             Dropout(rate=0.2),\n                             Dense(10, kernel_initializer='random_uniform', activation='softmax')])\n    classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return classifier\n\nANN_classifier = KerasClassifier(build_fn=build_classifier, batch_size=100, epochs=20)\nANN_classifier.fit(model_train_data2D, model_train_label)","8e3b2570":"cv_score = cross_val_score(ANN_classifier, model_train_data2D, model_train_label, cv=5, scoring='accuracy')\nprint(\"Mean CV Score Is:\", cv_score.mean())","0edf0bbd":"print(\"Mean CV Score Is:\", cv_score.mean())","fda01ede":"test_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\nmodel_test_data = std_scaler.transform(test_data.astype(np.float64))\nmodel_test_data2D = pca.transform(model_test_data)","9ca0dbdf":"predict_test_poly6 = poly6.predict(model_test_data2D)","f143b087":"predict_test_softmax = softmax.predict(model_test_data)\npredict_test_ANN = ANN_classifier.predict(model_test_data2D)\n\nId = DataFrame(np.arange(1,28001))\nId.columns = ['ImageId']\n\nprediction = DataFrame(predict_test_poly6)\nprediction.columns = ['Label']\n\nresult = pd.concat([Id, prediction], axis=1)\nresult.to_csv(\"Submission_Poly.csv\", index=False)\n\nId = DataFrame(np.arange(1,28001))\nId.columns = ['ImageId']\n\nprediction = DataFrame(predict_test_softmax)\nprediction.columns = ['Label']\n\nresult = pd.concat([Id, prediction], axis=1)\nresult.to_csv(\"Submission_Softmax.csv\", index=False)\n\nId = DataFrame(np.arange(1,28001))\nId.columns = ['ImageId']\n\nprediction = DataFrame(predict_test_ANN)\nprediction.columns = ['Label']\n\nresult = pd.concat([Id, prediction], axis=1)\nresult.to_csv(\"Submission_ANN.csv\", index=False)","fc4bb970":"from keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense, Dropout","d77b91a1":"X_train = np.array(model_train_data_unscaled).reshape(42000, 28, 28, 1)\ny_train = model_train_label\nX_test = np.array(test_data).reshape(28000, 28, 28, 1)","d36af9aa":"CNN_model = Sequential([Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), input_shape=(28,28,1),\n                            padding='valid', activation='relu'),\n                         MaxPooling2D(pool_size=(2, 2)),\n                         Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n                            padding='valid', activation='relu'),\n                         MaxPooling2D(pool_size=(2, 2)),\n                         Flatten(),\n                         Dense(128, kernel_initializer='random_uniform', activation='relu'),\n                         Dropout(rate=0.2),\n                         Dense(128, kernel_initializer='random_uniform', activation='relu'),\n                         Dropout(rate=0.2),\n                         Dense(10, kernel_initializer='random_uniform', activation='softmax')])\n\nCNN_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nCNN_model.fit(X_train, y_train, batch_size=100, epochs=20)","9bd0101a":"y_test = CNN_model.predict(X_test)\ny_test.shape","7b2be67e":"predict_test_CNN = np.argmax(y_test, axis=1)\nplt.imshow(np.array(test_data.loc[0]).reshape(28, 28), cmap='Greys')\nprint(predict_test_CNN[0])","c2f09ec2":"Id = DataFrame(np.arange(1,28001))\nId.columns = ['ImageId']\n\nprediction = DataFrame(predict_test_CNN)\nprediction.columns = ['Label']\n\nresult = pd.concat([Id, prediction], axis=1)\nresult.to_csv(\"Submission_CNN.csv\", index=False)","406f06e2":"#### Explore train data.","9105f44d":"#### There is no missing value and there is very little scope of feature engineering; so we will only do scaling of pixel values to bring them within 0 and 1.","5950c46e":"#### Separate out predictor variables i.e pixel values and label.","51a1f9b0":"#### So you can see if we use only 200 columns out of 784 columns we are able to explain 96.61% variability of the data.\n\n#### We will use this updated dataset on Polynomial Kernel Classification.","fcb9b8a3":"#### Load the train data.","1723f1bd":"#### Let's start applying different algorithm on the train dataset.\n\n#### Softmax Regression.","699d38b8":"#### Now we will apply Convolutional Neural Network(CNN) techniques on the original data.","69b1fdce":"#### Import the basic libraries.","cd75357d":"#### So we can see a significant improvement in accuracy using an ANN to classify.\n\n#### Now we will use test data to predict.","ee9a9651":"#### We can see a slight improvement in model performance.\n\n#### Now we will apply ANN to solve the challenge. Since we want to use the standard function to display model metrics like CV score we need to build an wrapper.","8bd1bfb9":"#### Now we will apply PCA and which is one of the main dimensionality reduction technique. PCA stands for Principal Component Analysis and here the columns across which the data has maximum variability are retained; so we can drop few columns without losing significant amount of insights.","668ad7cd":"#### Let's visualize one row of the train dataset.","9500aac2":"#### Handwritten Digit Recognization is one of the oldest problem of Machine Learning and is used to baseline performance of new algorithm.\n\n#### I will share my way of addressing this challenge starting from applying Classification algorithms, PCA, Neural Network and ultimately CNN.\n\n#### There is ample scope to improve this Notebook but I am sure the small footsteps outlined here will be helpful to solidify our learning.","8f07aa32":"#### We will create a standard function so that we can have similar metrics displayed for different algorithms.","bb527d40":"#### Reshaping the data to the format Convolution layer expects.\n\n#### Input shape - 4D tensor with shape: (batch_size, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch_size, rows, cols, channels) if data_format='channels_last'.","4d7f759e":"#### The 'label' is the target column and it says whether the values of pixel0 to pixel784 make it to any digit from 0 to 9.\n\n#### Each pixel has value in the range of 0 to 255."}}