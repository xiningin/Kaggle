{"cell_type":{"524edc6e":"code","2b837d55":"code","18cfaeb8":"code","d34066b4":"code","5906c5d2":"code","d2554a5b":"code","63537ca2":"code","eb93ad2d":"code","18e8edc6":"code","c3aa1937":"code","6ee0bcd2":"code","05cb7df5":"code","b96dfb5e":"code","78cd819a":"code","19a10354":"code","7b91c431":"code","0f1c2b8f":"code","0d233a40":"code","ce95151a":"code","54652f71":"code","b9b8aa44":"code","8b383d4e":"code","e0172091":"code","a67af602":"code","f810f72e":"code","020c5c23":"code","c9003550":"code","52aa3d99":"code","c238455f":"code","dfac545c":"markdown","b81f2fcf":"markdown","f9fcf683":"markdown","e93f21db":"markdown","f3e4ecc9":"markdown","a64c6a6d":"markdown","bd41de6b":"markdown","578b7fd4":"markdown","e7d1566b":"markdown","a5cd13e7":"markdown","43fb94f1":"markdown","4b8e3770":"markdown","4fdf4648":"markdown","7cdf66ca":"markdown","c1b854b2":"markdown","42156954":"markdown","5b2b2a6a":"markdown","d7ed3a68":"markdown","86a4c0a7":"markdown","0f7ff102":"markdown","1979e543":"markdown","407991c8":"markdown","d9ac0e74":"markdown"},"source":{"524edc6e":"import pandas as pd\nimport numpy as  np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.callbacks import ReduceLROnPlateau","2b837d55":"train=pd.read_csv('..\/input\/Kannada-MNIST\/train.csv')\ntest=pd.read_csv('..\/input\/Kannada-MNIST\/test.csv')\nsample_sub=pd.read_csv('..\/input\/Kannada-MNIST\/sample_submission.csv')","18cfaeb8":"print('The Train  dataset has {} rows and {} columns'.format(train.shape[0],train.shape[1]))\nprint('The Test  dataset has {} rows and {} columns'.format(test.shape[0],test.shape[1]))\n","d34066b4":"train.head(3)","5906c5d2":"test.head(3)\ntest=test.drop('id',axis=1)\n","d2554a5b":"y=train.label.value_counts()\nsns.barplot(y.index,y)","63537ca2":"X_train=train.drop('label',axis=1)\nY_train=train.label","eb93ad2d":"X_train=X_train\/255\ntest=test\/255","18e8edc6":"X_train=X_train.values.reshape(-1,28,28,1)\ntest=test.values.reshape(-1,28,28,1)","c3aa1937":"print('The shape of train set now is',X_train.shape)\nprint('The shape of test set now is',test.shape)\n","6ee0bcd2":"X_train,X_test,y_train,y_test=train_test_split(X_train,Y_train,random_state=42,test_size=0.15)","05cb7df5":"plt.imshow(X_train[0][:,:,0])","b96dfb5e":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)\n","78cd819a":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization(momentum=.15))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization(momentum=0.15))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(BatchNormalization(momentum=.15))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1))","19a10354":"model.summary()","7b91c431":"optimizer=Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)","0f1c2b8f":"import keras.backend as K\ndef Acc(y_true, y_pred, from_logits=False, label_smoothing=0):\n    y_pred=K.round(y_pred)\n    return K.mean(K.cast(K.equal(y_true,y_pred),y_pred.dtype))\n\nmodel.compile(optimizer=optimizer,loss='mae',metrics=['accuracy',Acc])","0d233a40":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","ce95151a":"epochs=40 \nbatch_size=64","54652f71":"# Fit the model\nhistory = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_test,y_test),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks=[learning_rate_reduction])","b9b8aa44":"fig,ax=plt.subplots(2,1)\nfig.set\nx=range(1,1+epochs)\nax[0].plot(x,history.history['loss'],color='red')\nax[0].plot(x,history.history['val_loss'],color='blue')\n\nax[1].plot(x,history.history['accuracy'],color='red')\nax[1].plot(x,history.history['val_accuracy'],color='blue')\nax[0].legend(['trainng loss','validation loss'])\nax[1].legend(['trainng acc','validation acc'])\nplt.xlabel('Number of epochs')\nplt.ylabel('accuracy')\n","8b383d4e":"y_pre_test=model.predict(X_test)\ny_pre_test=np.round(y_pre_test)\ny_pre_test[y_pre_test>9]=9\ny_pre_test[y_pre_test<0]=0\ny_pre_test.shape","e0172091":"print(\"Correct predictions\",np.sum((y_pre_test.flat==y_test).astype(int)))\nprint(\"Incorrect predictions\",np.sum((y_pre_test.flat!=y_test).astype(int)))","a67af602":"test=pd.read_csv('..\/input\/Kannada-MNIST\/test.csv')","f810f72e":"test_id=test.id\ntest=test.drop('id',axis=1)\ntest=test\/255\ntest=test.values.reshape(-1,28,28,1)\n","020c5c23":"test.shape","c9003550":"y_pre=model.predict(test)     ##making prediction\ny_pre=np.round(y_pre).astype(int) ##changing the prediction intro labels\ny_pre[y_pre>9]=9\ny_pre[y_pre<0]=0","52aa3d99":"sample_sub['label']=y_pre.flat\nsample_sub.to_csv('submission.csv',index=False)\n","c238455f":"sample_sub.head()","dfac545c":"Forked from this nice kernel https:\/\/www.kaggle.com\/shahules\/indian-way-to-learn-cnn","b81f2fcf":"Things to try:\n1. Different Thresholds\n2. Other loss fuctions","f9fcf683":"All Set,We have our data reshape into 60000 examples of height 28 and width 28 and 1 channel.","e93f21db":"We will make our prediction using our CNN model.","f3e4ecc9":"### More data !","a64c6a6d":"Before jumping to all complex stuff about Convolutions and all,we will simply understand our data.We will learn and gain basic understanding about this data.","bd41de6b":"### Learning rate reduction","578b7fd4":"### Reshape","e7d1566b":"### Loading data","a5cd13e7":"## Modelling <a id='4' ><\/a>","43fb94f1":"In order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n\nFor example, the number is not centered The scale is not the same (some who write with big\/small numbers) The image is rotated...\n\nApproaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more.\n\nBy applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model.\n\n","4b8e3770":"It's Nine in Kannada\n","4fdf4648":"Now we will split out training data into train and validation data.15percent of the training data will be used for validation purpose.","7cdf66ca":"### Normalize Pixel Values\n\nFor most image data, the pixel values are integers with values between 0 and 255.\n\nNeural networks process inputs using small weight values, and inputs with large integer values can disrupt or slow down the learning process. As such it is good practice to normalize the pixel values so that each pixel value has a value between 0 and 1.\n\nIt is valid for images to have pixel values in the range 0-1 and images can be viewed normally.\n\nThis can be achieved by dividing all pixels values by the largest pixel value; that is 255. This is performed across all channels, regardless of the actual range of pixel values that are present in the image.","c1b854b2":"Now we can see that all of the classes has equal distribution.There are 6000 examples of each numbers in kannada in the the training dataset.Cool !","42156954":"## Evaluating our approach <a id='6'><\/a>","5b2b2a6a":"## Making a Submission <a id='7'><\/a>\n","d7ed3a68":"For the data augmentation, i choosed to :\n\n   - Randomly rotate some training images by 10 degrees\n   - Randomly Zoom by 10% some training images\n   - Randomly shift images horizontally by 10% of the width\n   - Randomly shift images vertically by 10% of the height\n\nI did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9.","86a4c0a7":"### Splitting train and test","0f7ff102":"We have plotted the performance of our model.We can see the number of epochs in the X axis and change in model performance in Y axis.","1979e543":"### Fitting our model <a id='5'><\/a>","407991c8":"### Checking Target class distribution..\n","d9ac0e74":"## Data preparation <a id='2'><\/a>"}}