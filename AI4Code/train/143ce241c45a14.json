{"cell_type":{"362ddd52":"code","c8ef9aaf":"code","7a06c744":"code","fea2c099":"code","5d09ca1c":"code","7019de91":"code","acd8d7d8":"code","8bb2e99e":"code","cfe1f804":"code","d6a7542f":"code","52b37175":"code","abe2c97f":"code","4ef8237a":"code","7a1cbaad":"code","96b8ce62":"code","214472a1":"code","03c0649e":"code","dacac8c5":"code","c7bb26a5":"code","0e542cb0":"code","c7c1c463":"code","e4c6572e":"code","5da02723":"code","aa2c7c4d":"code","b5694c9b":"code","c4c0a859":"code","19122294":"code","14864719":"code","0e586eeb":"code","ef168efb":"code","29d58176":"code","531d3fcb":"code","96a51cae":"code","779aeefa":"code","2264ccf1":"code","57898b70":"code","b6c27cbc":"code","dbfa5507":"code","9d13b7f3":"code","3b5bdf7e":"code","c04d444f":"code","c24c5d08":"code","32248090":"code","7ae8ddb6":"code","103488c7":"code","6a27a761":"code","b93d942d":"code","a6ddb131":"code","a455c226":"code","95745ffe":"code","4f849d65":"code","51fd3eba":"code","efb6c89f":"code","bc4a9a37":"code","8c1df8ef":"code","2bb35da7":"code","9817dcfb":"code","d103e5c3":"code","9d0f8b86":"code","3aafe2dd":"code","0cca0f06":"code","7f8d500f":"code","d82d1143":"code","a2b4067c":"code","6045af44":"code","21b8dc72":"code","667dd4fe":"code","8320fc2c":"code","f72a86cf":"code","47d231d1":"code","21113144":"code","ab82f671":"code","28fba3be":"code","830f5ce1":"code","5470cb97":"code","7605123a":"code","572edb3e":"code","dc1eb9ef":"markdown","b4562f42":"markdown","6611c5c4":"markdown","23ea3f6d":"markdown","18ec0046":"markdown","47c4d085":"markdown","1a48ff93":"markdown","cd06898b":"markdown","b64c4b33":"markdown","d53b4bde":"markdown","2125f36d":"markdown","07b58288":"markdown","10e5c426":"markdown","4f93e70b":"markdown","ae0b7acb":"markdown","4d1954a1":"markdown","7f863f5a":"markdown","8dfaaec4":"markdown","68d4178d":"markdown","bd01891d":"markdown","13515b2e":"markdown","787030b3":"markdown","ec7294a8":"markdown","9990b802":"markdown","aa7b9b77":"markdown","fdb0edc0":"markdown","c16d38ac":"markdown","fe34621e":"markdown","122c7b31":"markdown","1bed2446":"markdown","cf671b12":"markdown","0d2d458e":"markdown","2655249a":"markdown","989d2ecf":"markdown","decb035f":"markdown","17bcae50":"markdown","142d28cb":"markdown","3af16bdf":"markdown","42bd99d6":"markdown","134aca39":"markdown","64e2f9c7":"markdown","8f945140":"markdown","b85f7a1c":"markdown","37d582e6":"markdown","7034c1ca":"markdown","6b3488a6":"markdown","15a30f0a":"markdown","3054ae19":"markdown","e859f171":"markdown","403356fb":"markdown","76832efa":"markdown","f456dd3f":"markdown","59b410a6":"markdown"},"source":{"362ddd52":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.special import boxcox1p\n\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, RidgeCV, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport xgboost as xgb\n\nsns.set(style='whitegrid')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntrain_original = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_original = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","c8ef9aaf":"train.info()","7a06c744":"train = train.drop('Id', axis=1)\ntest = test.drop('Id', axis=1)","fea2c099":"train.head()","5d09ca1c":"test.head()","7019de91":"train['SalePrice'].describe()","acd8d7d8":"sns.distplot(train['SalePrice'])","8bb2e99e":"print(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","cfe1f804":"train_quantitative = train[[d for d in train.columns if train.dtypes[d] != 'object']].copy()\ntest_quantitative = test[[d for d in test.columns if test.dtypes[d] != 'object']].copy()","d6a7542f":"train_qualitative = train[[d for d in train.columns if train.dtypes[d] == 'object']].copy()\ntest_qualitative = test[[d for d in test.columns if test.dtypes[d] == 'object']].copy()","52b37175":"train_quantitative.describe()","abe2c97f":"train_qualitative.describe()","4ef8237a":"train_qualitative.columns","7a1cbaad":"missing_train = train_qualitative.isnull().sum().sort_values(ascending=False)\npercentage_train = (train_qualitative.isnull().sum()\/train_qualitative.isnull().count()).sort_values(ascending=False)\ntrain_info = pd.concat([missing_train,percentage_train],keys=['Missing','Percentage'],axis=1)\ntrain_info.head(25)","96b8ce62":"fig = plt.figure(figsize=(10,5))\ntrain_plot = sns.barplot(x=missing_train.index[0:20],y=missing_train[0:20])\ntrain_plot.set_xticklabels(train_plot.get_xticklabels(),rotation=90)\nplt.title('Number of missing values in categorical data(train)')","214472a1":"missing_test = test_qualitative.isnull().sum().sort_values(ascending=False)\npercentage_test = (test_qualitative.isnull().sum()\/test_qualitative.isnull().count()).sort_values(ascending=False)\ntest_info = pd.concat([missing_test,percentage_test],keys=['Missing','Percentage'],axis=1)\ntest_info.head(25)","03c0649e":"fig = plt.figure(figsize=(10,5))\ntest_plot = sns.barplot(x=missing_test.index[0:20],y=missing_test[0:20])\ntest_plot.set_xticklabels(test_plot.get_xticklabels(),rotation=90)\nplt.title('Number of missing values in categorical data(test)')","dacac8c5":"for column in train_qualitative.columns:\n    train_qualitative[column] = train_qualitative[column].fillna(\"None\")\nfor column in test_qualitative.columns:\n    test_qualitative[column] = test_qualitative[column].fillna(\"None\")","c7bb26a5":"train_qualitative['Electrical']=train_qualitative['Electrical'].fillna(method='pad')\ntest_qualitative['SaleType']=test_qualitative['SaleType'].fillna(method='pad')\ntest_qualitative['KitchenQual']=test_qualitative['KitchenQual'].fillna(method='pad')\ntest_qualitative['Exterior1st']=test_qualitative['Exterior1st'].fillna(method='pad')\ntest_qualitative['Exterior2nd']=test_qualitative['Exterior2nd'].fillna(method='pad')\ntest_qualitative['Functional']=test_qualitative['Functional'].fillna(method='pad')\ntest_qualitative['Utilities']=test_qualitative['Utilities'].fillna(method='pad')\ntest_qualitative['MSZoning']=test_qualitative['MSZoning'].fillna(method='pad')","0e542cb0":"train_qualitative.isnull().sum().sum()","c7c1c463":"test_qualitative.isnull().sum().sum()","e4c6572e":"train_qualitative.shape","5da02723":"test_qualitative.shape","aa2c7c4d":"top = 10\ncorr = train_quantitative.corr()\ntop10 = corr.nlargest(top,'SalePrice')['SalePrice'].index\ncorr_top10 = train_quantitative[top10].corr()\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(corr_top10, square=True, ax=ax, annot=True, fmt='.2f', annot_kws={'size':12})\nplt.title('Top correlated quantitative features of dataset')\nplt.show()","b5694c9b":"corr = train_quantitative.corr()['SalePrice'].sort_values(ascending=False)\nprint(corr)","c4c0a859":"fig,ax = plt.subplots(2,2,figsize=(15,15))\nsns.scatterplot(data=train_quantitative, x='SalePrice', y='GarageArea', ax=ax[0][0])\nsns.scatterplot(data=train_quantitative, x='SalePrice', y='GarageCars', ax=ax[0][1])\nsns.scatterplot(data=train_quantitative, x='SalePrice', y='TotRmsAbvGrd', ax=ax[1][0])\nsns.scatterplot(data=train_quantitative, x='SalePrice', y='GrLivArea', ax=ax[1][1])\n\nplt.show()","19122294":"corr = train_quantitative.corr()['SalePrice'].sort_values(ascending=False)\nprint(corr)","14864719":"train_quantitative = train_quantitative.drop(['GarageCars','TotRmsAbvGrd'], axis=1)\ntest_quantitative = test_quantitative.drop(['GarageCars','TotRmsAbvGrd'], axis=1)","0e586eeb":"fig,ax = plt.subplots(17,2,figsize=(15,60))\n\nfor i in range(len(train_quantitative.columns)-1):\n    #-1 in iterator to avoid regplot between \"SalePrice\" and \"SalePrice\"\n    r=i\/\/2\n    c=i%2\n    sns.scatterplot(data=train_quantitative, x=train_quantitative.columns[i], y='SalePrice', hue='SalePrice', palette='rocket', ax=ax[r][c])\n    \nplt.tight_layout()\nplt.show()","ef168efb":"missing_train_num = train_quantitative.isnull().sum().sort_values(ascending=False)\npercentage_train_num = (train_quantitative.isnull().sum()\/train_quantitative.isnull().count()).sort_values(ascending=False)\ntrain_info = pd.concat([missing_train_num,percentage_train_num],keys=['Missing','Percentage'],axis=1)\ntrain_info.head(10)","29d58176":"fig = plt.figure(figsize=(10,5))\ntest_plot = sns.barplot(x=missing_train_num.index[0:5],y=missing_train_num[0:5])\ntest_plot.set_xticklabels(test_plot.get_xticklabels(),rotation=90)\nplt.title('Number of missing values in numerical data(test)')","531d3fcb":"missing_test_num = test_quantitative.isnull().sum().sort_values(ascending=False)\npercentage_test_num = (test_quantitative.isnull().sum()\/test_quantitative.isnull().count()).sort_values(ascending=False)\ntrain_info = pd.concat([missing_test_num,percentage_test_num],keys=['Missing','Percentage'],axis=1)\ntrain_info.head(10)","96a51cae":"fig = plt.figure(figsize=(10,5))\ntest_plot = sns.barplot(x=missing_test_num.index[0:5],y=missing_test_num[0:5])\ntest_plot.set_xticklabels(test_plot.get_xticklabels(),rotation=90)\nplt.title('Number of missing values in numerical data(test)')","779aeefa":"train_quantitative['LotFrontage'] = train_quantitative.groupby(train_qualitative['Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ntest_quantitative['LotFrontage'] = test_quantitative.groupby(test_qualitative['Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))","2264ccf1":"train_quantitative['GarageYrBlt']=train_quantitative['GarageYrBlt'].fillna(train_quantitative['GarageYrBlt'].median())\ntest_quantitative['GarageYrBlt']=test_quantitative['GarageYrBlt'].fillna(test_quantitative['GarageYrBlt'].median())\n\nfor column in train_quantitative.columns:\n    train_quantitative[column] = train_quantitative[column].fillna(0)\nfor column in test_quantitative.columns:\n    test_quantitative[column] = test_quantitative[column].fillna(0)","57898b70":"train_quantitative.isnull().sum().sum()","b6c27cbc":"test_quantitative.isnull().sum().sum()","dbfa5507":"train_quantitative['TotalSF'] = train_quantitative['TotalBsmtSF']+train_quantitative['1stFlrSF']+train_quantitative['2ndFlrSF']\ntrain_quantitative = train_quantitative.drop(columns={'1stFlrSF', '2ndFlrSF','TotalBsmtSF'})\ntrain_quantitative['YrBltAndRemod']=train_quantitative['YearBuilt']+train_quantitative['YearRemodAdd']\ntrain_quantitative = train_quantitative.drop(columns={'YearBuilt', 'YearRemodAdd'})\ntrain_quantitative['Bsmt'] = train_quantitative['BsmtFinSF1']+ train_quantitative['BsmtFinSF2']\ntrain_quantitative = train_quantitative.drop(columns={'BsmtFinSF1','BsmtFinSF2'})\ntrain_quantitative['TotalBathroom'] = (train_quantitative['FullBath'] + (0.5 * train_quantitative['HalfBath']) +\n                               train_quantitative['BsmtFullBath'] + (0.5 * train_quantitative['BsmtHalfBath']))\ntrain_quantitative = train_quantitative.drop(columns={'FullBath','HalfBath','BsmtFullBath','BsmtHalfBath'})\n\n\ntest_quantitative['TotalSF'] = test_quantitative['TotalBsmtSF']+test_quantitative['1stFlrSF']+test_quantitative['2ndFlrSF']\ntest_quantitative = test_quantitative.drop(columns={'1stFlrSF', '2ndFlrSF','TotalBsmtSF'})\ntest_quantitative['YrBltAndRemod']=test_quantitative['YearBuilt']+test_quantitative['YearRemodAdd']\ntest_quantitative = test_quantitative.drop(columns={'YearBuilt', 'YearRemodAdd'})\ntest_quantitative['Bsmt'] = test_quantitative['BsmtFinSF1']+ test_quantitative['BsmtFinSF2']\ntest_quantitative = test_quantitative.drop(columns={'BsmtFinSF1','BsmtFinSF2'})\ntest_quantitative['TotalBathroom'] = (test_quantitative['FullBath'] + (0.5 * test_quantitative['HalfBath']) +\n                               test_quantitative['BsmtFullBath'] + (0.5 * test_quantitative['BsmtHalfBath']))\ntest_quantitative = test_quantitative.drop(columns={'FullBath','HalfBath','BsmtFullBath','BsmtHalfBath'})","9d13b7f3":"fig,ax = plt.subplots(14,2,figsize=(15,60))\n\nfor i in range(len(train_quantitative.columns)):\n    r=i\/\/2\n    c=i%2\n    sns.scatterplot(data=train_quantitative, x=train_quantitative.columns[i], y='SalePrice', hue='SalePrice', palette='viridis', ax=ax[r][c])\n    \nplt.tight_layout()\nplt.show()","3b5bdf7e":"numerical_to_categorical = ['TotalBathroom','Fireplaces','MSSubClass','OverallCond','BedroomAbvGr','LowQualFinSF','KitchenAbvGr','MoSold','YrSold','PoolArea','MiscVal','LotArea','3SsnPorch','ScreenPorch']","c04d444f":"numerical_categorical_train=train_quantitative[numerical_to_categorical]\ntrain_quantitative.drop(columns=numerical_to_categorical,inplace=True)\ntrain_quantitative","c24c5d08":"numerical_categorical_test = test_quantitative[numerical_to_categorical]\ntest_quantitative.drop(columns=numerical_to_categorical, inplace=True)\ntest_quantitative","32248090":"corr = train_quantitative.corr()['SalePrice'].sort_values(ascending=False)\nprint(corr)","7ae8ddb6":"train_qualitative = pd.concat([train_qualitative, numerical_categorical_train], axis=1)\ntest_qualitative = pd.concat([test_qualitative, numerical_categorical_test], axis=1)","103488c7":"qualitative = pd.concat((train_qualitative, test_qualitative), sort=False).reset_index(drop=True)\nqualitative = pd.get_dummies(qualitative)","6a27a761":"train_qualitative_final = qualitative[:train_qualitative.shape[0]]\ntest_qualitative_final = qualitative[train_qualitative.shape[0]:]","b93d942d":"train_qualitative_final.shape","a6ddb131":"test_qualitative_final.shape","a455c226":"# train_quantitative = train_quantitative.drop(train_quantitative[(train_quantitative['GrLivArea']>4000) & (train_quantitative['SalePrice']<300000)].index)\n# train_quantitative = train_quantitative.drop(train_quantitative[(train_quantitative['GarageArea']>1200) & (train_quantitative['SalePrice']<500000)].index)\n# train_quantitative = train_quantitative.drop(train_quantitative[(train_quantitative['Bsmt']>3000) & (train_quantitative['SalePrice']<700000)].index)","95745ffe":"y_pred = np.log1p(train['SalePrice'])\ny_train = np.log1p(train_quantitative['SalePrice'])","4f849d65":"train_quantitative.drop('SalePrice',axis=1, inplace=True)","51fd3eba":"sns.distplot(y_train)","efb6c89f":"print('Train quantitative skewness')\nskewed_features_train = []\nfor column in train_quantitative:\n    skew = abs(train_quantitative[column].skew())\n    print('{:15}'.format(column), \n          'Skewness: {:05.2f}'.format(skew))\n    if skew > 0.5:\n        skewed_features_train.append(column)","bc4a9a37":"skewed_features_train","8c1df8ef":"lam = 0.15\nfor feat in skewed_features_train:\n    train_quantitative[feat] = boxcox1p(train_quantitative[feat], lam)","2bb35da7":"print('Test quantitative skewness')\nskewed_features_test = []\nfor column in test_quantitative:\n    skew = abs(test_quantitative[column].skew())\n    print('{:15}'.format(column), \n          'Skewness: {:05.2f}'.format(skew))\n    if skew > 0.75:\n        skewed_features_test.append(column)","9817dcfb":"skewed_features_test","d103e5c3":"lam = 0.15\nfor feat in skewed_features_test:\n    test_quantitative[feat] = boxcox1p(test_quantitative[feat], lam)","9d0f8b86":"scaling = StandardScaler()\ntrain_quantitative_final = pd.DataFrame(scaling.fit_transform(train_quantitative),columns=train_quantitative.columns)\ntest_quantitative_final = pd.DataFrame(scaling.fit_transform(test_quantitative),columns=test_quantitative.columns)","3aafe2dd":"train_final=train_quantitative_final.merge(train_qualitative_final,left_index=True,right_index=True).reset_index(drop=True)\ntrain_final.head()","0cca0f06":"test_qualitative_final = test_qualitative_final.reset_index(drop=True)\ntest_final=test_quantitative_final.merge(test_qualitative_final,left_index=True,right_index=True).reset_index(drop=True)\ntest_final.head()","7f8d500f":"train_final.shape","d82d1143":"test_final.shape","a2b4067c":"X_train, X_test, Y_train, Y_test = train_test_split(train_final, y_train, test_size = .3, random_state=0)","6045af44":"def rmse(actual,predicted):\n    return(str(np.sqrt(mean_squared_error(actual, predicted))))","21b8dc72":"lin_reg = LinearRegression()\nlin_reg.fit(X_train, Y_train)\n\ny_pred_train = lin_reg.predict(X_train)\ny_pred_test = lin_reg.predict(X_test)\n\nprint('RMSE train = ' + rmse(Y_train,y_pred_train))\nprint('RMSE test = ' + rmse(Y_test,y_pred_test)) \nprint()","667dd4fe":"lasso_reg =Lasso()\nparameters= {'alpha': [0.0005,0.001,0.1,1,5,10,20]}\n\nlasso_reg=GridSearchCV(lasso_reg, param_grid=parameters)\nlasso_reg.fit(X_train,Y_train)\nalpha = lasso_reg.best_params_\nlasso_score = lasso_reg.best_score_\nprint(\"The best alpha value found is:\",alpha['alpha'],'with score:',lasso_score)\n\nlasso_reg_alpha = Lasso(alpha=alpha['alpha'])\nlasso_reg_alpha.fit(train_final,y_train)\ny_pred_train=lasso_reg_alpha.predict(X_train)\ny_pred_test=lasso_reg_alpha.predict(X_test)\n\nprint('RMSE train = ' + rmse(Y_train,y_pred_train))\nprint('RMSE test = ' + rmse(Y_test,y_pred_test))","8320fc2c":"ridge=Ridge()\nparameters= {'alpha': [0.0005,0.001,0.1,0.2,0.4,0.5,0.7,0.8,1]}\n\nridge_reg=GridSearchCV(ridge, param_grid=parameters)\nridge_reg.fit(X_train,Y_train)\nalpha = ridge_reg.best_params_\nridge_score = ridge_reg.best_score_\nprint(\"The best alpha value found is:\",alpha['alpha'],'with score:',ridge_score)\n\nridge_reg_alpha=Ridge(alpha=alpha['alpha'])\nridge_reg_alpha.fit(train_final,y_train)\ny_pred_train=ridge_reg_alpha.predict(X_train)\ny_pred_test=ridge_reg_alpha.predict(X_test)\n\nprint('RMSE train = ' + rmse(Y_train,y_pred_train))\nprint('RMSE test = ' + rmse(Y_test,y_pred_test))","f72a86cf":"rf_reg = RandomForestRegressor()\nparameters = {\"max_depth\":[5, 8, 15, 25, 30], \"n_estimators\":[25,50,100,200]}\n\nrf_reg_param = GridSearchCV(rf_reg, parameters, cv = 10, n_jobs =10)\nrf_reg_param.fit(X_train, Y_train)\nrf_reg_best=rf_reg_param.best_estimator_\ny_pred_train = rf_reg_best.predict(X_train)\ny_pred_test = rf_reg_best.predict(X_test)\n\nprint('RMSE train = ' + rmse(Y_train,y_pred_train))\nprint('RMSE test = ' + rmse(Y_test,y_pred_test))","47d231d1":"import xgboost as xgb \n\nxgb_reg = xgb.XGBRegressor(n_estimators=1000)\nxgb_reg.fit(X_train, Y_train, early_stopping_rounds=5, \n             eval_set=[(X_test, Y_test)], verbose=False)","21113144":"xgb_reg_param = xgb.XGBRegressor(learning_rate=0.05,\n                      n_estimators=1000,\n                      max_depth=3)\n\nxgb_reg_param.fit(train_final, y_train)\nxgb_train_pred = xgb_reg_param.predict(X_train)\nxgb_test_pred = xgb_reg_param.predict(X_test)\n\n\nprint('RMSE train = ' + rmse(Y_train,xgb_train_pred))\nprint('RMSE test = ' + rmse(Y_test,xgb_test_pred))","ab82f671":"def blended_regression(X):\n    return ((0.3 * ridge_reg_alpha.predict(X)) + (0.7 * xgb_reg_param.predict(X)))","28fba3be":"y_pred_train = blended_regression(X_train)\ny_pred_test = blended_regression(X_test)\nprint('RMSE train = ' + rmse(Y_train,y_pred_train))\nprint('RMSE test = ' + rmse(Y_test,y_pred_test))","830f5ce1":"y_test=blended_regression(test_final)","5470cb97":"final_y_test=np.expm1(y_test)","7605123a":"sample=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission=pd.DataFrame({\"Id\":sample['Id'],\n                         \"SalePrice\":final_y_test})\nsubmission.to_csv('submission.csv',index=False)","572edb3e":"final_y_test","dc1eb9ef":"As we can see from the charts in our data, there are still numerical featuers which in fact are categorical ones. I will move them to categorical data and drop them from numerical data. After that i will concat them with qualitative datasets and then i will use pd.getdummies to obtain final qualitative datasets.","b4562f42":"Finally, any missing data in train data?","6611c5c4":"We need to deal with integer, float and categorical(object) data in this dataset.\nLet's start cleaning up the data by removing the unnecseary 'ID' column. ","23ea3f6d":"# Feature engineering","18ec0046":"As we can see, among the numerical data, there are some data that are essentially categorical. Many of them show very weak correlations with our target - \"SalePrice\". I will move some of them to qualitative data but some of them will be used to feature engineering. ","47c4d085":"For all features with skewness above 0.5, a boxcox1p transformation will be applied.","1a48ff93":" # Missing values","cd06898b":"As we can see, we are missing quite a lot of data. A simple and effective solution will be to swap NaN values for None.","b64c4b33":"Which columns are qualitative data?","d53b4bde":"# Main objectives\n\n1. Performing exploratory data analysis on both qualitative and quantitative data.\n2. Ensuring better data shape and quality by numerous transformation which will help to improve performance of regression models.\n3. Checking and tuning different models to get the best results on target variable. ","2125f36d":"In several notebooks I read that you should drop outliers, however in my case model achieves a lower score with outliers. If you want to remove outliers, you can find code to do this below. ","07b58288":"# Exploratory data analysis on categorical data","10e5c426":"Merging quantitative and qualitative data.","4f93e70b":"How much categorical data do we lack? Let's find out. ","ae0b7acb":"For the rest of the missing data we will use the fillna method with the substituted value 0. The exception will be 'GarageYrBlt' which we will replace with median value.","4d1954a1":"# Conclusions and possible future development of the model ","7f863f5a":"Or maybe something in test data?","8dfaaec4":"# My first Kaggle notebook\n\nHello Kagglers,\n\nBelow is my first ever kaggle notebook that I used to work with the famous dataset about residental homes in Ames, Iowa. I started working on this notebook to better understand the problems of advanced regression. During my work, I managed to practically learn about regression models and put together my first Kaggle submission. After several attempts, thanks to blended regression, I was able to reach the top 26% with the result of 0.12371 in public leaderboard.\n\nI realize that this is a very popular problem among beginners and you can find many great kernels associated with it, but despite this I thought I would share my work to get valuable feedbacks from the community on how I can  improve my score.\n\nHere I've tried to make everything as understandable as possible for me and to get the best possible result. I think that the presented way of working on the dataset will be helpful for beginners like me. Before I started, I read many interesting notebooks which provided me a lot of valuable informations. I would especially like to thank the authors of:\n\n1. [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by **Pedro Marcelino**\n2. [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) by **Alexandru Papiu**\n3. [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by **Serigne**\n\nIf you find my notebook useful, please upvote or comment if you have found any possible improvement. I will be very grateful for every feedback. That will keep me motivated to regular update of this notebook.\n\nThank You in advance!","68d4178d":"Looking at the heatmap we can see that \"OverallQual\", \"GrLivArea\" and \"TotalBsmtSF\" are the most correlated features with \"SalePrice\". \n\nOf course, we can't forget about \"GarageCars\" and \"GarageArea\" features, however they are closely related to each other. As the \"GarageArea\" increases, the number of \"GarageCars\" automatically increases.\n\n\"GarageCars\" and \"GarageArea\" are not the only one features where we can see the multicollinearity. A similar situation occurs between \"TotRmsAbvGrd\" and \"GrLIveArea\".\n\nThe graphs below show the collinearity of data. Based on them I choose the following features for further analysis:\n\n- GarageArea\n- GrLivArea\n\nOf those features I dropped the ones that has smaller correlation coeffiecient to \"SalePrice\".","bd01891d":"However, for some of the columns where not much data were missing I used the 'pad' method, which propagate last valid observation forward to next.","13515b2e":"Let's check how our newly created featurs talks with SalePrice.","787030b3":"Or maybe something in test data?","ec7294a8":" # Train test split","9990b802":"Another quick preview of datasets shape.","aa7b9b77":"Categorical data are finished. Let's get back to te numerical data. ","fdb0edc0":"By carefully reading the description of dataset we can apply some feature engineering. ","c16d38ac":"Finally, any missing data in train data?","fe34621e":"And the same story for test quantitative features.","122c7b31":"No 0 values, which could destroy my model. That is a good news. Now take a look at histogram.","1bed2446":"# Ridge Regression (L2 regularization)","cf671b12":"# XGBoost Regressor","0d2d458e":" # Submission\n","2655249a":"The dataset has been pre-arranged, but what about the other features? By making a quick insight into the rest of the data we will be able to see data without strong linear correlation with \"SalePrice\" and by this we also can find some numerical-categorical data.\nTo take whole dataset and see what's going on with the data the seaborn library will be very helpful again. ","989d2ecf":"Surprisingly, it was not that bad. We will see how it goes with the quantitative data.","decb035f":"Quick look at shape of our dataset.","17bcae50":" # Simple Linear Regression without regularization","142d28cb":"Qucik peview of skew and kurt.","3af16bdf":"Thanks to the division we obtained 37 quantitative data and 43 qualitative data, we can now start exploratory data analysis. I will start exploratory data analysis with qualitative data. ","42bd99d6":" # Skewed features\n \n As I mentioned before, distribution of our target - 'SalePrice' is far away from gaussian distribution. Thus we need to perform relevant transformation to obtain distribution closer to normal. To do this I've used log(1+x) transformation.","134aca39":"In order to test blended regression I decided to build a simple model and check how it performs by trial and error method. For this purpose I chose the L2 regularization model and XGBoost regressor becouse they achieve best RMSE scores in single run. ","64e2f9c7":"Based on the chart, we can see that SalePrice has: \n\n - Left(positive) skewness\n - Visible peak\n - Distribution which is far from normal\n\nLater, log transformation could be helpful. Why log transformation? Log transformation is used to transform skewed data into data which distribution is closer to normal. This allows for better performance of regression models. \nThe next sensible step for further exploratory data analysis is the division of data into qualitative and quantitative. This division will allow for a better understanding of the dataset. ","8f945140":"# Random Forest Regressor","b85f7a1c":"# Scaling","37d582e6":" # Exploratory data analysis on quantitative data","7034c1ca":"Becouse property areas are usually similar to other houses in its neighborhood, we can supplement the missing values with the median LotFrontage of the area.","6b3488a6":"Let's check what really matters in quantiative data.\nSns's heatmap will be a great place to start.","15a30f0a":"Among the tested models, the XGBRegressor was the best performing model. Therefore, I used it to perform final submission. \n\nIn case of simple linear regression and random forest regressor rmse of the test sets significantly differs from the rmse of the training sets which may indicate about overfitting. It is also intresting to note that the model with outliers performs better than model without. Any feedback related to this problem will be very helpful.\n\nLooking through other notebooks I noticed that the best performing models are based on blended regressions. My goal for the future will be to create a model which, being a blended model, will allow to obtain a better score.","3054ae19":"The main target of the analysis is \"SalePrice\", let's start our analysis with a brief overview.","e859f171":"# RMSE evaluation","403356fb":" # Outliers\n \n- GrLivArea > 4000\n- GarageArea > 1200\n- TotalBsmtSF > 3000","76832efa":"# Lasso Regression (L1 regularization)","f456dd3f":"Now as we can see our target - 'SalePrice' is normally distributed. But what about other skewed features? Let's take a deeper look into them.","59b410a6":"# Blended approach"}}