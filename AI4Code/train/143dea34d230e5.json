{"cell_type":{"f82b918d":"code","9546fade":"code","3ca1fb1e":"code","35496bbc":"code","a3bd907d":"code","32c5d719":"code","f5da0e7d":"code","3cc07184":"code","bc172369":"code","02cf5dd3":"code","a08d06ce":"code","3f7ae161":"code","80f183ee":"code","ad19835a":"code","c0cffd1c":"code","06de6ad7":"code","6f8e4100":"code","6df499fc":"code","42dd275c":"code","139ebff7":"code","b942fad5":"code","5b3fa8e1":"code","2612c827":"markdown","c1467780":"markdown","8dc52a71":"markdown","ab006d14":"markdown","1adf13b5":"markdown","3e78c2fb":"markdown","ec2ef50a":"markdown","70059d87":"markdown","9a67630b":"markdown"},"source":{"f82b918d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\".\/\"))\n\n# Any results you write to the current directory are saved as output.","9546fade":"from keras.models import Sequential\nfrom keras.layers import *\nfrom keras.callbacks import *","3ca1fb1e":"import tensorflow as tf","35496bbc":"# Loading packages\nimport pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis\n%matplotlib inline","a3bd907d":"#seed \uac12 \uc124\uc815\nseed = 0\nnp.random.seed(seed)\ntf.set_random_seed(seed)","32c5d719":"df_test = pd.read_csv('..\/input\/test.csv')\ndf_train = pd.read_csv('..\/input\/train.csv')","f5da0e7d":"df_train = df_train.loc[df_train['id']!=8912]\ndf_train = df_train.loc[df_train['id']!=2302]\ndf_train = df_train.loc[df_train['id']!=4123]\ndf_train = df_train.loc[df_train['id']!=7173]\ndf_train = df_train.loc[df_train['id']!=2775]","3cc07184":"columns = ['bedrooms','bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n           'sqft_basement', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'zipcode']","bc172369":"skew_columns = ['bedrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement']\n\nfor c in skew_columns:\n    df_train[c] = np.log1p(df_train[c])\n    df_test[c] = np.log1p(df_test[c])","02cf5dd3":"df_test = df_test[columns]\ndf_test = pd.get_dummies(df_test, columns=['floors'], prefix='floors')\ndf_test = pd.get_dummies(df_test, columns=['view'], prefix='view')\ndf_test = pd.get_dummies(df_test, columns=['zipcode'], prefix='zipcode')","a08d06ce":"train_columns = ['price']\ntrain_columns += columns\ndf_train = df_train[train_columns]\ndf_train = pd.get_dummies(df_train, columns=['floors'], prefix='floors')\ndf_train = pd.get_dummies(df_train, columns=['view'], prefix='view')\ndf_train = pd.get_dummies(df_train, columns=['zipcode'], prefix='zipcode')","3f7ae161":"train_data = df_train.values[:,1:]\ntrain_targets = df_train.values[:,0]\ntest_data = df_test.values[:, :]","80f183ee":"#early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100) # \uac1c\uc120\uc774 \uc5c6\ub294 \uacbd\uc6b0 \uc911\ub2e8\n#model_check_point = ModelCheckpoint(filepath='.\/my_model.h5', monitor='val_loss', save_best_only=True) # \ubaa8\ub378\uc758 \ucd5c\uc801\uc0c1\ud0dc\ub9cc \uc800\uc7a5\n#reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10) # \uac1c\uc120\uc774 \uc5c6\uc744 \uacbd\uc6b0 \ud559\uc2b5\ub960 \ub0ae\ucda4(10\ubc30)\n\ndef build_model():\n    model = Sequential()\n    model.add(Dense(16, input_shape = (train_data.shape[1], ), kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001), activation='relu'))\n    model.add(Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001), activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    model.compile(loss='mse', optimizer='rmsprop', metrics=['mae'])\n    \n    return model","ad19835a":"k = 5\nnum_val_samples = len(train_data) \/\/ k\n\nnum_epochs = 1000\nall_mae_histories = []\nfor i in range(k):    \n    print('\ucc98\ub9ac\uc911\uc778 \ud3f4\ub4dc #', i)\n    \n    # \uac80\uc99d \ub370\uc774\ud130 \uc900\ube44: k\ubc88\uc9f8 \ubd84\ud560\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n\n    # \ud6c8\ub828 \ub370\uc774\ud130 \uc900\ube44: \ub2e4\ub978 \ubd84\ud560 \uc804\uccb4\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n\n    # \ucf00\ub77c\uc2a4 \ubaa8\ub378 \uad6c\uc131(\ucef4\ud30c\uc77c \ud3ec\ud568)\n    model = build_model()\n    # \ubaa8\ub378 \ud6c8\ub828(verbose=0 \uc774\ubbc0\ub85c \ud6c8\ub828 \uacfc\uc815\uc774 \ucd9c\ub825\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4)\n    history = model.fit(partial_train_data, partial_train_targets,\n                        validation_data=(val_data, val_targets),\n                        epochs=num_epochs, batch_size=32, verbose=0)\n    mae_history = history.history['val_mean_absolute_error']\n    all_mae_histories.append(mae_history)\n\nprint('\ud6c8\ub828 \ub05d')","c0cffd1c":"average_mae_history = [\n    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]","06de6ad7":"plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","6f8e4100":"def smooth_curve(points, factor=0.9):\n  smoothed_points = []\n  for point in points:\n    if smoothed_points:\n      previous = smoothed_points[-1]\n      smoothed_points.append(previous * factor + point * (1 - factor))\n    else:\n      smoothed_points.append(point)\n  return smoothed_points\n\nsmooth_mae_history = smooth_curve(average_mae_history[10:])\n\nplt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","6df499fc":"# \uc0c8\ub86d\uac8c \ucef4\ud30c\uc778\ub41c \ubaa8\ub378\uc744 \uc5bb\uc2b5\ub2c8\ub2e4\nmodel = build_model()\n# \uc804\uccb4 \ub370\uc774\ud130\ub85c \ud6c8\ub828\uc2dc\ud0b5\ub2c8\ub2e4\nmodel.fit(train_data, train_targets,\n          epochs=800, batch_size=32, verbose=0)\n#test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n#print(test_mse_score, test_mae_score)","42dd275c":"Y_prediction = model.predict(test_data).flatten()","139ebff7":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['price'] = Y_prediction","b942fad5":"submission.to_csv('.\/my_first_submission.csv', index=False)","5b3fa8e1":"submission","2612c827":"## \uc81c\ucd9c\ud30c\uc77c \uc0dd\uc131","c1467780":"## \ub370\uc774\ud130 \uc815\uaddc\ud654","8dc52a71":"## \ud6c8\ub828\uacb0\uacfc \uc2dc\uac01\ud654","ab006d14":"## \uc81c\ucd9c \ud3ec\uba67 \uc77d\uae30","1adf13b5":"## \ub370\uc774\ud130 \uc77d\uae30","3e78c2fb":"## K\uacb9 \uac80\uc99d \ud6c8\ub828","ec2ef50a":"## \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc608\uce21","70059d87":"## \ub300\uc0c1 features \uc120\ubcc4","9a67630b":"## \uc774\uc0c1\uce58 \ub370\uc774\ud130 \uc81c\uac70"}}