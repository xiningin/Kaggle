{"cell_type":{"4c5bb845":"code","af44bdb5":"code","fa4f22fe":"code","b4278661":"code","89c9ad89":"code","d7a820e0":"code","d1b9199c":"code","84574402":"code","d59babd2":"code","be876b19":"code","458e0088":"code","ac4cd2ab":"code","90d53dd6":"code","19f0b31b":"code","66685c85":"code","d4732e00":"code","27984ed0":"code","735693c6":"code","192de511":"code","a98f0b42":"code","e0c02c5f":"code","491404fd":"code","e491548c":"code","4e7aa7b9":"code","3e70b911":"code","1e5678b6":"code","33ac32e1":"code","f6c0ba3d":"code","dc57ae45":"code","ca3a9fae":"code","745a2077":"code","33f445b5":"code","99bbb80e":"code","60c77835":"code","e10b18ff":"code","8155cb4f":"code","1a258a64":"code","57b91bc5":"code","77d6c2f3":"code","e340ab1c":"code","51ead9c6":"code","e0cde7b1":"code","edb4c4b3":"code","888683c6":"code","e4eca3b3":"code","f323bf04":"code","bc4298e9":"code","fccaf24b":"code","9cd84b7d":"code","e920aeb9":"code","fc8a661f":"code","4cd0369d":"code","8bf40650":"code","41cbd211":"code","19990386":"code","f948f069":"code","61cb9345":"code","e2ac0790":"code","4024515d":"code","61a2cb82":"markdown","d025b5a8":"markdown","4d5cd2a9":"markdown","49b4f8d1":"markdown","273cef2d":"markdown","03172a95":"markdown","dde71fa4":"markdown","8cf0e9c9":"markdown","6847f896":"markdown","e61694a7":"markdown","0cf1537b":"markdown","1201718c":"markdown","7f2a9be8":"markdown","32930ea2":"markdown","1a5506d4":"markdown","b8547519":"markdown"},"source":{"4c5bb845":"import numpy as np\nimport pandas as pd\nimport io\nimport seaborn as sns\nimport matplotlib.pyplot as plt","af44bdb5":"df_train= pd.read_csv('..\/input\/themathcompany-carpriceprediction-dataset\/test.csv')\ndf_test= pd.read_csv('..\/input\/themathcompany-carpriceprediction-dataset\/test.csv')","fa4f22fe":"df_train.head()","b4278661":"df_train.shape","89c9ad89":"df_train.info()","d7a820e0":"df_test.info()","d1b9199c":"df_train.drop(columns='ID',inplace=True)\ndf_test.drop(columns=['ID','Price'],inplace=True)","84574402":"#checking for any non-numeric data in the column\nx=np.array(list(filter((lambda x: x.isdigit()==False),df_train['Levy'])))\nnp.unique(x)","d59babd2":"#replacing '-' with 0 as - seems to be no levy present\ndf_train.loc[(df_train.Levy == '-'),'Levy']=0\ndf_test.loc[(df_test.Levy == '-'),'Levy']=0\ndf_train.head()","be876b19":"#converting the column to numeric\ndf_train['Levy']=pd.to_numeric(df_train['Levy'])\ndf_test['Levy']=pd.to_numeric(df_test['Levy'])","458e0088":"df_train['Mileage'][0].split()","ac4cd2ab":"#Removing the work km from the data to make the column numeric and renaming the column so that it conveys the same message \ny_tr=np.array(list(map((lambda x: x.split()),df_train['Mileage'])))\ny_te=np.array(list(map((lambda x: x.split()),df_test['Mileage'])))","90d53dd6":"df_train['Mileage']=y_tr\ndf_test['Mileage']=y_te\ndf_train['Mileage']= pd.to_numeric(df_train['Mileage'])\ndf_test['Mileage']= pd.to_numeric(df_test['Mileage'])","19f0b31b":"df_train.head()","66685c85":"z=np.array(list(filter((lambda x: x.isdigit()==False),df_train['Engine volume'])))\nnp.unique(z)","d4732e00":"#removing turbo and space from the data and converting to numeric type\ndf_train['Engine volume']=list(map((lambda x: x.replace('Turbo','').replace(' ','')),df_train['Engine volume']))\ndf_test['Engine volume']=list(map((lambda x: x.replace('Turbo','').replace(' ','')),df_test['Engine volume']))","27984ed0":"df_train['Engine volume']= pd.to_numeric(df_train['Engine volume'])\ndf_test['Engine volume']= pd.to_numeric(df_test['Engine volume'])","735693c6":"#cleaning the manufacturer and model columns\n(df_train[df_train['Manufacturer']=='\u10e1\u10ee\u10d5\u10d0'])= (df_train[df_train['Manufacturer']=='\u10e1\u10ee\u10d5\u10d0']).replace('\u10e1\u10ee\u10d5\u10d0','kamaz')\n(df_test[df_test['Manufacturer']=='\u10e1\u10ee\u10d5\u10d0'])= (df_test[df_test['Manufacturer']=='\u10e1\u10ee\u10d5\u10d0']).replace('\u10e1\u10ee\u10d5\u10d0','kamaz')\ndf_test[df_test['Model']=='kamaz\/ \u10d9\u10d0\u10db\u10d0\u10d6\u10d8']= (df_test[df_test['Model']=='kamaz\/ \u10d9\u10d0\u10db\u10d0\u10d6\u10d8']).replace('kamaz\/ \u10d9\u10d0\u10db\u10d0\u10d6\u10d8','kamaz')","192de511":"#removing rows with null values in Levy column\ndf_train.drop(labels=(df_train[df_train['Levy'].isnull()].index),axis=0,inplace=True)","a98f0b42":"df_train['Doors'].unique()","e0c02c5f":"#replacing the data of door feature with meaninful values\ndf_train['Doors']=list(map((lambda x: x.replace('04-May','four').replace('02-Mar','two').replace('>5','more than 5')),df_train['Doors']))\ndf_test['Doors']=list(map((lambda x: x.replace('04-May','four').replace('02-Mar','two').replace('>5','more than 5')),df_test['Doors']))","491404fd":"df_train['Wheel'].unique()","e491548c":"df_train['Color'].unique()","4e7aa7b9":"#creating feature age that exhibits the age of the car\ndf_train['car_age']= 2021-df_train['Prod. year']\ndf_test['car_age']= 2021-df_test['Prod. year']","3e70b911":"df_train['car_age']= pd.to_numeric(df_train['car_age'])\ndf_test['car_age']= pd.to_numeric(df_test['car_age'])","1e5678b6":"df_train['Category_cars'] = list(map((lambda x: ('Post_war') if (x<1945) else ('Boomers-I') if (x>=1946 and x <=1954) else ('Boomers-II') if (x>= 1955 and x<= 1964) else ('Gen-X') if  (x>=1965 and x<=1980) else ('Milennials') if (x >= 1981 and x <= 1996) else ('Gen-Z') if (x>=1997 and x<=2012) else ('Gen-Alpha')),df_train['Prod. year']))\ndf_train['Category_cars']= df_train['Category_cars'].astype('category')","33ac32e1":"df_test['Category_cars'] = list(map((lambda x: ('Post_war') if (x<1945) else ('Boomers-I') if (x>=1946 and x <=1954) else ('Boomers-II') if (x>= 1955 and x<= 1964) else ('Gen-X') if  (x>=1965 and x<=1980) else ('Milennials') if (x >= 1981 and x <= 1996) else ('Gen-Z') if (x>=1997 and x<=2012) else ('Gen-Alpha')),df_test['Prod. year']))\ndf_test['Category_cars']= df_test['Category_cars'].astype('category')","f6c0ba3d":"def outlier(df,col):\n    m= df[col].mean()\n    m1= df[col].min()\n    m2=df[col].max()\n    l= len(df)\n    q1= df[col].quantile(0.25)\n    q3=df[col].quantile(0.75)\n    IQR= q3-q1\n    lower= q1-(IQR*1.5)\n    upper= q3+(IQR*1.5)\n    n= len(df.loc[np.where((df[col] > upper) | (df[col] < lower))])\n    perc= (n\/l)*100\n    print(f'{col}\\n percentage= {perc}\\n number={n}\\n mean= {m}\\n min={m1}\\n max={m2}\\n Q1={q1}\\n Q3={q3}')","dc57ae45":"for i in df_train.select_dtypes(exclude=['category','object']).columns:\n  outlier(df_train,i)","ca3a9fae":"for i in df_test.select_dtypes(exclude=['category','object']).columns:\n  outlier(df_test,i)","745a2077":"df_train['Engine volume'].plot(kind='box')","33f445b5":"df_train[df_train['Engine volume']>15]","99bbb80e":"#on reasearching on the the abov observation of engine volume outliers it was found that the two rows should be dropped\ndf_train.drop(labels=(df_train[(df_train['Engine volume']>15)].index),axis=0,inplace=True) ","60c77835":"df_test['Engine volume'].plot(kind='box')","e10b18ff":"df_test[df_test['Engine volume']>8]","8155cb4f":"#based on online research the above was found to be incorrect combination of details thus dropping it\ndf_test.drop(labels=(df_test[(df_test['Engine volume'] > 8)].index),axis=0,inplace=True)","1a258a64":"#dropping the outlier in the price column as it is clearly a noise\ndf_train.drop(labels=(df_train[(df_train['Price']==26307500)].index),axis=0,inplace=True)","57b91bc5":"#lets check the 99th percentile and 1 percentile to see if we can cap the outliers\nfor i in df_train.select_dtypes(exclude=['category','object']).columns:\n  print(f'{i}:')\n  print(f'99th percentile: {np.percentile(df_train[i],99)}')\n  print(f'1 percentile: {np.percentile(df_train[i],1)}')\n  print('xxx'.center(100,'-'))","77d6c2f3":"df_train['Levy'].plot(kind='box')","e340ab1c":"df_train['Mileage'].plot(kind='box',ylim=(0,1000000))","51ead9c6":"#capping the  mileage at 99th percentile \ndf_train.loc[df_train.Mileage >= round((np.percentile(df_train['Mileage'],99)),2), 'Mileage'] = round((np.percentile(df_train['Mileage'],99)),2)\ndf_test.loc[df_test.Mileage >= round((np.percentile(df_test['Mileage'],99)),2), 'Mileage'] = round((np.percentile(df_test['Mileage'],99)),2)","e0cde7b1":"#removing the rows with car_age above 70 as on trying to reasearch a bit it was found that these can be removed \ndf_train.drop(labels=(df_train[df_train['car_age']>70].index),axis=0,inplace=True)\ndf_test.drop(labels=(df_test[df_test['car_age']>70].index),axis=0,inplace=True)","edb4c4b3":"df_train.loc[df_train.Cylinders >= round((np.percentile(df_train['Cylinders'],99)),2), 'Cylinders'] = round((np.percentile(df_train['Cylinders'],99)),2)\ndf_test.loc[df_test.Cylinders >= round((np.percentile(df_test['Cylinders'],99)),2), 'Cylinders'] = round((np.percentile(df_test['Cylinders'],99)),2)","888683c6":"#visualization of different characteristics of the car with its age\nsns.set(font_scale=1.2)\nplt.figure(figsize=(45,35))\n\nfor i, column in enumerate(['Category', 'Leather interior', 'Fuel type','Gear box type', 'Drive wheels', 'Doors', 'Wheel', 'Color','Category_cars'], 1):\n    plt.subplot(3,3, i)\n    g = sns.boxplot(x=f\"{column}\", y='car_age', data=df_train)\n    g.set_xticklabels(g.get_xticklabels())\n    plt.ylabel('Price')\n    plt.xlabel(f'{column}')","e4eca3b3":"#visualization of different characteristics of the car with its age\nplt.figure(figsize=(45,35))\n\nfor i, column in enumerate(['Category', 'Leather interior', 'Fuel type','Gear box type', 'Drive wheels', 'Doors', 'Wheel', 'Color','Category_cars'], 1):\n    plt.subplot(3,3, i)\n    g = sns.boxplot(x=f\"{column}\", y='car_age', data=df_train)\n    g.set_xticklabels(g.get_xticklabels())\n    plt.ylabel('car_age')\n    plt.xlabel(f'{column}')","f323bf04":"df_train[df_train['Levy'].isnull()]","bc4298e9":"df_train[df_train['car_age']>70].sort_values(by='Mileage',ascending=False,)","fccaf24b":"(df_train.var()).plot(kind='bar',ylim= (0,2))","9cd84b7d":"#analysis of effect of the independent categorical features on the the price of the car\nplt.figure(figsize=(45,35))\n\nfor i, column in enumerate(['Category', 'Leather interior', 'Fuel type','Gear box type', 'Drive wheels', 'Doors', 'Wheel', 'Color','Category_cars'], 1):\n    plt.subplot(3,3, i)\n    g = sns.barplot(x=f\"{column}\", y='Price', data=df_train)\n    g.set_xticklabels(g.get_xticklabels())\n    plt.ylabel('Price')\n    plt.xlabel(f'{column}')","e920aeb9":"plt.figure(figsize=(20,10))\nsns.heatmap(df_train.corr(),annot=True,linecolor='white',linewidths=0.5)","fc8a661f":"#Split of train and test into independent(X) and target varible(Y)\nX_train= df_train.drop(columns='Price')\nY_train= df_train[['Price']]\nX_test= df_test.copy()","4cd0369d":"#number of categories in each categorical feature\nfor i in X_train.select_dtypes(include=[ 'object','category']).columns:\n  print(f'{i}= {len(X_train[i].unique())}')","8bf40650":"#Label enccoding for categorical features with more than 5 features and for less than 5 dummy variables is created\nfrom sklearn.preprocessing import LabelEncoder\ndef encode(x):\n  dum=[]\n  for i in x.select_dtypes(include=[ 'object','category']).columns:\n    if len(x[i].unique())>=5:\n      x[i]= LabelEncoder().fit_transform(x[i])\n      x[i]= x[i].astype('category')\n    else:\n      dum.append(i)\n  x=(pd.get_dummies(x,columns=dum))\n  return(x)","41cbd211":"#endcoding the test and train dependent variables using the custom function\nX_train= encode(X_train)\nX_test= encode(X_test)","19990386":"#standardization of the train and test independent features\nfrom sklearn.preprocessing import StandardScaler","f948f069":"X_train[['Levy','Engine volume','Mileage','Airbags','Cylinders','car_age']]= StandardScaler().fit_transform(X_train[['Levy','Engine volume','Mileage','Airbags','Cylinders','car_age']])\nX_test[['Levy','Engine volume','Mileage','Airbags','Cylinders','car_age']]= StandardScaler().fit_transform(X_test[['Levy','Engine volume','Mileage','Airbags','Cylinders','car_age']])","61cb9345":"from sklearn.ensemble import RandomForestRegressor","e2ac0790":"model = RandomForestRegressor(random_state=1, max_depth=10)\nmodel.fit(X_train,Y_train)\nfeatures = X_train.columns\nimportances = model.feature_importances_\nindices = np.argsort(importances)[-9:]  # top 10 features\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='r', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","4024515d":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import minmax_scale\n\nmodel = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.005)\nmodel.fit(X_train, Y_train)\n\ny_pred = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\nprint(np.sqrt(mean_squared_log_error(Y_train, y_pred_train)))","61a2cb82":"# 2. Data Analysis","d025b5a8":"# 3. Modeling","4d5cd2a9":"# 1. Understanding the dataset","49b4f8d1":"# 2. Data Cleaning and Feature Engineering","273cef2d":"Observations:\n\nOn Further Analysing the dataset based on the mileage and other features, and noticing the impact of the changes of features of a car on price the outliers some how are making sense. Thus, i would not make any further changes interms of outliers for now.","03172a95":"# 4. Variance Check","dde71fa4":"The engine volumn feature is exhibiting a very low variance thus, will explore it further","8cf0e9c9":"### 3.3 Hyperparameter Tunning","6847f896":"# 3. Outliers","e61694a7":"#Outlier Capping","0cf1537b":"Observation:\n\nThe above visualization is clearly exhibiting a huge number of outliers for cars above approx. 50 years so we will divide deeper and check the data further","1201718c":"### 3.1 Splitting, Standardization and Encoding","7f2a9be8":"### 3.2 Feature Importance Visualization using RandomForrestRegressor","32930ea2":"### **Categorizing cars as per the production year:**\n\nGen Alpha: <2012\n\nGen Z :\t1997 \u2013 2012\n\nMillennials :\t1981 \u2013 1996\t\n\nGen X :\t1965 \u2013 1980\t\n\nBoomers-II :\t1955 \u2013 1964\t\n\nBoomers-I :\t1946 \u2013 1954\t\n\nPost War :1928 \u2013 1945","1a5506d4":"2.1 Multivariate Analysis","b8547519":"####Since we have less number of features we wont perform dimentionality reduction"}}