{"cell_type":{"be661ae0":"code","74e5646a":"code","b474df2b":"code","41342ed3":"code","f52dc84f":"code","fc054245":"code","4cd62a13":"code","5a072193":"code","f990391b":"code","51138845":"code","ec91c8d4":"code","568d209e":"code","53dfcaee":"code","db70bfa4":"code","6f03f2e5":"code","5c5317c7":"code","3ffcd03c":"code","ca486be6":"code","d3ac46d9":"code","f263fdcb":"code","7747366f":"code","bd6dd475":"code","cef51543":"code","320106ad":"code","58b5c75e":"code","a6a9bca5":"code","761b772c":"code","911b8a22":"code","28940d37":"code","a199fa49":"code","4d22a42e":"markdown","9221eac3":"markdown","12e7d5de":"markdown","b65836cb":"markdown","df2d17f1":"markdown","9582b005":"markdown","21c770d2":"markdown","b77890ad":"markdown","66cf8912":"markdown","732fb984":"markdown","74ffddd0":"markdown","a0c8d952":"markdown","f327ea2d":"markdown","7edeee75":"markdown","c2e07f13":"markdown","4db6e440":"markdown","5088971a":"markdown","0d58c40b":"markdown","fc94273f":"markdown","42306484":"markdown","02867592":"markdown","2a5a2054":"markdown","3e341f96":"markdown"},"source":{"be661ae0":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport warnings\nwarnings.filterwarnings('ignore')","74e5646a":"train = pd.read_csv('..\/input\/forest-cover-type-prediction\/train.csv')\n\n# display train data\ntrain.head()","b474df2b":"# drop ID column\ntrain = train.iloc[:,1:]\ntrain.head()","41342ed3":"train.describe()","f52dc84f":"def outlier_function(df, col_name):\n    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n    IQR = third_quartile - first_quartile\n    \n    upper_limit = third_quartile+(3*IQR)\n    lower_limit = first_quartile-(3*IQR)\n    outlier_count = 0\n    \n    for value in df[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count += 1\n    return lower_limit, upper_limit, outlier_count","fc054245":"for column in train.columns:\n    if outlier_function(train, column)[2] > 0:\n        print(\"There are {} outliers in {}\".format(outlier_function(train, column)[2], column))","4cd62a13":"# remove outliers from Fire Points with highest range of outliers\ntrain = train[(train['Horizontal_Distance_To_Fire_Points'] > \n               outlier_function(train, 'Horizontal_Distance_To_Fire_Points')[0]) &\n            (train['Horizontal_Distance_To_Fire_Points'] <\n            outlier_function(train, 'Horizontal_Distance_To_Fire_Points')[1])]","5a072193":"train.describe()","f990391b":"train['Euclidian_Distance_To_Hydrology'] = (train['Horizontal_Distance_To_Hydrology']**2 + train['Vertical_Distance_To_Hydrology']**2)**0.5\ntrain['Mean_Elevation_Vertical_Distance_Hydrology'] = (train['Elevation'] + train['Vertical_Distance_To_Hydrology'])\/2\ntrain['Mean_Distance_Hydrology_Firepoints'] = (train['Horizontal_Distance_To_Hydrology'] + train['Horizontal_Distance_To_Fire_Points'])\/2\ntrain['Mean_Distance_Hydrology_Roadways'] = (train['Horizontal_Distance_To_Hydrology'] + train['Horizontal_Distance_To_Roadways'])\/2\ntrain['Mean_Distance_Firepoints_Roadways'] = (train['Horizontal_Distance_To_Fire_Points'] + train['Horizontal_Distance_To_Roadways'])\/2\n\ntrain","51138845":"train.dtypes","ec91c8d4":"# create cat, num and y\n# create categorical features\nX_cat = train.iloc[:,10:54].values\n\n# numerical features\n# X_num_ori = train.iloc[:,0:10].values\n# X_num_new = train.iloc[:,56:60].values\nX_num = train.iloc[:, np.r_[0:10, 55:60]].values\n\n# create y\ny = train.iloc[:,-54].values","568d209e":"# scale\/standardize numerical columns\nscaler = StandardScaler() # scaler object\nscaler.fit(X_num) # fit training data\nX_num = scaler.transform(X_num) # scale num columns\n\n# shape\nprint(f'Categorical Shape: {X_cat.shape}')\nprint(f'Numerical Shape: {X_num.shape}')\nprint(f'Label Shape: {y.shape}')","53dfcaee":"# combine num and cat\nX = np.hstack((X_num, X_cat))\nprint(X.shape)","db70bfa4":"from sklearn.decomposition import PCA\n\npca = PCA().fit(X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('PCA Number of Compoenents for Cumulative Variance')","6f03f2e5":"from sklearn.ensemble import ExtraTreesClassifier\n\netc_model = ExtraTreesClassifier(random_state = 53) # pass the model\nX = train.iloc[:,:-54] # feed features to var X\ny = train['Cover_Type'] # feed target variable to y\n\netc_model.fit(X,y) # train the ETC model\n\n# extract feature importances\netc_feature_importances = pd.DataFrame(etc_model.feature_importances_, index=X.columns,\n                                      columns=['ETC']).sort_values('ETC', ascending=False)\n\netc_model = None # remove trace of this ETC model\netc_feature_importances.head(10)","5c5317c7":"from sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier(random_state = 53) # pass the model\nrfc_model.fit(X,y) # train the model\n\n# extract feature importances\nrfc_feature_importances = pd.DataFrame(rfc_model.feature_importances_, index=X.columns, \n                                       columns=['RFC']).sort_values('RFC', ascending=False)\n\nrfc_model = None # remove trace of this RFC model\nrfc_feature_importances.head(10)","3ffcd03c":"from sklearn.ensemble import AdaBoostClassifier\n\nadb_model = AdaBoostClassifier(random_state = 53) # pass the model\nadb_model.fit(X,y) # train the model\n\n# extract feature importances\nadb_feature_importances = pd.DataFrame(adb_model.feature_importances_, index=X.columns,\n                                      columns=['ADB']).sort_values('ADB', ascending=False)\n\nadb_model = None # remove trace of this ADB model\nadb_feature_importances.head(10)","ca486be6":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc_model = GradientBoostingClassifier(random_state = 53) # pass the model\ngbc_model.fit(X,y) # train the model\n\n# extract feature importances\ngbc_feature_importances = pd.DataFrame(gbc_model.feature_importances_, index=X.columns,\n                                      columns=['GBC']).sort_values('GBC', ascending=False)\n\ngbc_model = None # remove trace of GBC model\ngbc_feature_importances.head(10)","d3ac46d9":"sample = train[[\n    'Elevation','Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Hydrology',\n    'Vertical_Distance_To_Hydrology','Aspect','Slope','Euclidian_Distance_To_Hydrology',\n    'Mean_Elevation_Vertical_Distance_Hydrology','Mean_Distance_Hydrology_Firepoints',\n    'Mean_Distance_Hydrology_Roadways','Mean_Distance_Firepoints_Roadways','Cover_Type'\n]]","f263fdcb":"from sklearn.preprocessing import MinMaxScaler\n\n# pass range to the function and then save it\nscaler = MinMaxScaler(feature_range = (0,1))\n\nX = sample.iloc[:,:-1] # feed sample features to X\ny = sample['Cover_Type'] # feed target variable to y\n\nX_scaled = scaler.fit_transform(X) # apply feature scaling to all features","7747366f":"X_scaled","bd6dd475":"from sklearn.model_selection import cross_val_score\nimport time\n\n# function\ndef model_evaluation(clf):\n    clf = clf # pass classifier to variable\n    \n    t_start = time.time() # record time\n    clf = clf.fit(X_scaled, y) # classifier learning model\n    t_end = time.time() # record time\n    \n    c_start = time.time() # record time\n    accuracy = cross_val_score(clf, X_scaled, y, cv=10, scoring='accuracy')\n    f1_score = cross_val_score(clf, X_scaled, y, cv=10, scoring='f1_macro')\n    c_end = time.time() # record time\n    \n    # calculate mean of all 10 obs' accuracy and f1 as percent\n    acc_mean = np.round(accuracy.mean() * 100, 2)\n    f1_mean = np.round(f1_score.mean() * 100, 2)\n    \n    t_time = np.round((t_end - t_start) \/ 60, 3) # time for training\n    c_time = np.round((c_end - c_start) \/ 60, 3) # time for evaluating scores\n    \n    clf = None # remove traces of classifier\n    \n    print(f'The accuracy score of this classifier is: {acc_mean}%.')\n    print(f'The f1 score of this classifier is: {f1_mean}%.')\n    print(f'This classifier took {t_time} minutes to train and {c_time} minutes to evaluate CV and metric scores.')","cef51543":"from sklearn.naive_bayes import MultinomialNB\n\nmodel_evaluation(MultinomialNB())","320106ad":"from sklearn.neighbors import KNeighborsClassifier\nmodel_evaluation(KNeighborsClassifier(n_jobs=-1))","58b5c75e":"from sklearn.ensemble import RandomForestClassifier\nmodel_evaluation(RandomForestClassifier(n_jobs=-1, random_state=53))","a6a9bca5":"from sklearn.linear_model import SGDClassifier\nmodel_evaluation(SGDClassifier(n_jobs=-1, random_state=53))","761b772c":"from sklearn.ensemble import ExtraTreesClassifier\nmodel_evaluation(ExtraTreesClassifier(n_jobs=-1, random_state=53))","911b8a22":"from sklearn.linear_model import LogisticRegression\nmodel_evaluation(LogisticRegression(n_jobs=-1, random_state=53, solver='saga', max_iter = 500))","28940d37":"from sklearn.model_selection import RandomizedSearchCV\n\n# number of trees in the forest algorithm\nn_estimators = [50, 100, 300, 500, 1000]\n\n# minimum number of samples required to split an internal node\nmin_samples_split = [2, 3, 5, 7, 9]\n\n# minimum number of samples required to be at a leaf node\nmin_samples_leaf = [1, 2, 4, 6, 8]\n\n# number of features to consider when looking for the best split\nmax_features = ['auto','sqrt','log2',None]\n\n# define the grid of hyperparameters to search\nhyperparameter_grid = {'n_estimators': n_estimators,\n                       'min_samples_leaf': min_samples_leaf,\n                       'min_samples_split': min_samples_split,\n                       'max_features': max_features}\n\n# create model\nbest_model = ExtraTreesClassifier(random_state=42)\n\n# create randomized search object\nrandom_cv = RandomizedSearchCV(estimator=best_model, param_distributions=hyperparameter_grid, cv=10,\n                               n_iter=20, scoring='accuracy', n_jobs=-1, verbose=1, return_train_score=True, random_state=0)\n\n# fit on all training data using random search object\nrandom_cv.fit(X_scaled, y)\nrandom_cv.best_estimator_","a199fa49":"from sklearn.metrics import accuracy_score, f1_score\n\nclf = ExtraTreesClassifier(n_estimators=1000, random_state=42, max_features='log2') # best classifier\nclf = clf.fit(X, y) # train model\npredict = clf.predict(X) # predict unseen data\naccuracy = accuracy_score(y, predict) # calculate accuracy\nf1_score = f1_score(y, predict, average='macro') # calculate f1 score\n\naccuracy = np.round(accuracy * 100, 3)\nf1_score = np.round(f1_score * 100, 3)\n\nclf = None # clean traces\n\nprint(f'The accuracy score of our final model ETC on our testing set is {accuracy}%.')\nprint(f'The f1 score of our final model ETC on our testing set is {f1_score}%.')","4d22a42e":"Note: As `Wilderness_Area` and `Soil_Type` are one-hot encoded, we can focus on the following:\n\n\n- There are 53 outliers in Horizontal_Distance_To_Hydrology\n- There are 49 outliers in Vertical_Distance_To_Hydrology\n- There are 3 outliers in Horizontal_Distance_To_Roadways\n- There are 7 outliers in Hillshade_9am\n- There are 20 outliers in Hillshade_Noon\n- There are 132 outliers in Horizontal_Distance_To_Fire_Points","9221eac3":"# EDA","12e7d5de":"## 2. Random Forest Classifier","b65836cb":"# Dimensionality Reduction","df2d17f1":"## AdaBoost Classifier","9582b005":"# PCA","21c770d2":"## Benchmark Model: `MultinomialNB Classifier`\nWe will not see how the performance of `MultinomialNB Classifier` on given training data. This performs quite quickly, but has poor **precision** and **recall**.","b77890ad":"## Extra-Trees Classifier","66cf8912":"There are horizontal and vertical distance to hydrology features, which blinks for adding the euclidian distance of the two.","732fb984":"# Feature Engineering & Selection","74ffddd0":"# Model Evaluation","a0c8d952":"## 1. K-Nearest Neighbors","f327ea2d":"# Feature Scaling","7edeee75":"# Features Selection\n\nWe will add the features we found important, plus the new features we engineered.","c2e07f13":"## Gradient Boosting Classifier","4db6e440":"## Random Forest Classifier","5088971a":"# Train Final Model","0d58c40b":"# Preprocessing","fc94273f":"## 4. Extra Trees Classifier","42306484":"# Hyperparameter Tuning","02867592":"## 5. Logisitic Regression","2a5a2054":"# Models","3e341f96":"## 3. Stochastic Gradient Descent Classifier"}}