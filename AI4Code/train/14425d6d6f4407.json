{"cell_type":{"6a5813c1":"code","d6b98917":"code","c56a3e4f":"code","e7f721f6":"code","83ba6628":"code","00b3fa19":"code","57cf8b7b":"code","94d7829e":"code","acfcae75":"code","27842133":"code","f58f7566":"code","1aef81e8":"code","80bdbb4c":"code","74de2c7c":"code","fd4b2f61":"code","2a89c672":"code","b2ea3181":"code","ed21d637":"code","c928883d":"code","faff2d99":"code","afc182a8":"code","2095d7b4":"code","2e2bd3bc":"code","8bb4639c":"code","acf45330":"code","e3a07b95":"markdown","984fe7a6":"markdown","1529d9b7":"markdown","5483bae2":"markdown","b0290b42":"markdown","d55d90f9":"markdown","5ade3492":"markdown","86d24147":"markdown","4ea6bad6":"markdown","de5dbea6":"markdown","add78b2e":"markdown","e5f57358":"markdown","fc92c09f":"markdown","0d4a8273":"markdown","fd19a93d":"markdown"},"source":{"6a5813c1":"import os\nimport pandas as pd\nimport numpy as np","d6b98917":"# measure running time\n%timeit pd.Series(np.random.randint(10, 20, 10000))","c56a3e4f":"%%timeit # for multiple lines use %%\nfor i in range(5):\n    pd.Series(np.random.randint(10, 20, 10000))","e7f721f6":"# sort by cumtime\n%prun -s cumulative np.random.randint(10, 20, 100000)","83ba6628":"!pip install line_profiler","00b3fa19":"import os\n\ntrain_dir = '..\/input\/titanic\/train.csv'\n\ndef count_surv1(df):\n    return df[df['Survived']==1]['Survived'].count()\n\ndef count_surv2(df):\n    total = 0\n    for i,row in df.iterrows():\n        total += row['Survived']\n    return total\n\ndef test(data_path):\n    \n    df = pd.read_csv(train_dir)\n    surv1 = count_surv1(df)\n    surv2 = count_surv2(df)\n\n%load_ext line_profiler\n%lprun -f test test(train_dir)","57cf8b7b":"s = pd.Series(range(10000))","94d7829e":"%%timeit # sum without vectorization\ntotal = 0\nfor val in s:\n    total+=val","acfcae75":"# python built in sum\n%timeit sum(s)","27842133":"# sum pandas version\n%timeit s.sum()","f58f7566":"# sum numpy version\n%timeit s.values.sum()","1aef81e8":"df = pd.DataFrame(pd.date_range(start='1\/1\/2000', end='1\/08\/2018'))\n%timeit df.nunique()","80bdbb4c":"df[0] = df[0].apply(str)\n%timeit df.nunique()","74de2c7c":"df = pd.DataFrame({'a':np.random.rand(10000), 'b':np.random.rand(10000)})\n\n%time x = df.apply(np.max, axis=1)","fd4b2f61":"%time x = df.apply(np.max, axis=1, raw=True)","2a89c672":"df = pd.DataFrame({'a':np.random.choice(list('abcd')), 'b':np.random.rand(10_000_000)})\n%timeit df[(df['a'] == 'c') & (df['b']> 0.3)]","b2ea3181":"%timeit df.query('a == \"c\" & b > 0.3')","ed21d637":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\n\nx = reduce_mem_usage(df)","c928883d":"s = pd.Series(np.random.randint(-3, 10, 1_000_000))\n\ndef relu(n):\n    return 0 if n<0 else n\n\n%timeit s.apply(relu)","faff2d99":"from numba import vectorize, int64\n\n# define return_type(parameter_type)\n@vectorize([int64(int64)])\ndef vect_relu(n):\n    if n<0:\n        return 0\n    else:\n        return n\n\n%timeit vect_relu(s)","afc182a8":"# check if both functions give same result\nnp.allclose(s.apply(relu), vect_relu(s))","2095d7b4":"from pandas_profiling import ProfileReport\ntrain_dir = '..\/input\/titanic\/train.csv'\n\ndf = pd.read_csv(train_dir)\nprofile = ProfileReport(df, title = 'Train data profile')\nprofile.to_notebook_iframe()","2e2bd3bc":"!pip install pywedge \nimport pywedge as pw","8bb4639c":"dash = pw.Pywedge_Charts(df, c=None, y='Survived')\ndashboard = dash.make_charts()","acf45330":"data_dir = '..\/input\/titanic'\n\ndf_train = pd.read_csv(f'{data_dir}\/train.csv')\ndf_test = pd.read_csv(f'{data_dir}\/test.csv')\n\nblm = pw.baseline_model(df_train, df_test, y='Survived', c='PassengerId')\nblm.classification_summary()","e3a07b95":"<a id =topic3> <\/a>\n# Profiling\n\n* **ncallls** - number of calls of each function\n* **tottime** - the total time spent in the given function\n* **percall** - the total call divided by the number of calls\n* **cumtime** - the total time spent in this and all subfunctions (from invocation till exit).\n* **percall** - time per call for the cumulative time","984fe7a6":"<a id =topic4> <\/a>\n# Line-by-line profiling","1529d9b7":"<a id =topic12> <\/a>\n# Pywedge\n\nPywedge is an open-source python library which is a complete package that helps you in Visualizing the data, Pre-process the data and also create some baseline models ","5483bae2":"the output of the command above\n\n```\nTimer unit: 1e-06 s\n\nTotal time: 0.321552 s\nFile: <ipython-input-6-a28c00033674>\nFunction: test at line 14\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    14                                           def test(data_path):\n    15                                               \n    16         1      22239.0  22239.0      6.9      df = pd.read_csv(train_dir)\n    17         1      31185.0  31185.0      9.7      surv1 = count_surv1(df)\n    18         1     268128.0 268128.0     83.4      surv2 = count_surv2(df)\n\n```","b0290b42":"<a id =topic9> <\/a>\n# Reduce memory usage of data frame","d55d90f9":"<a id =topic2> <\/a>\n# Benchmarking\n\nA benchmarking is the act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of a function or part of the program.\n\n**We can measure a running time of a programm using**:\n* magic command `%timeit`\n* using package `time`\n* using `pytest-benchmark` package\n\n**In order to find what part of the function is the slowest**, we can use profiler for the standard library `cProfile`:\n* use magic command `%prun`\n\n**For the memory profiling we can use `memory_profiler`.**\n**For the line-by-line profiling we can use `line_profiler`","5ade3492":"If you run this command, the output will be:\n```\n13 function calls in 0.003 seconds\nOrdered by: cumulative time\n\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    1    0.000    0.000    0.003    0.003 {built-in method builtins.exec}\n    1    0.000    0.000    0.003    0.003 <string>:1(<module>)\n    1    0.003    0.003    0.003    0.003 {method 'randint' of 'numpy.random.mtrand.RandomState' objects}\n    1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(prod)\n    1    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n    1    0.000    0.000    0.000    0.000 fromnumeric.py:2881(prod)\n    1    0.000    0.000    0.000    0.000 fromnumeric.py:70(_wrapreduction)\n    1    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n    1    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n    1    0.000    0.000    0.000    0.000 fromnumeric.py:71(<dictcomp>)\n    1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n    1    0.000    0.000    0.000    0.000 fromnumeric.py:2876(_prod_dispatcher)\n    1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n```","86d24147":"# Content\n\n* [Intro](#topic1)\n* [Benchmarking](#topic2)\n* [Profiling](#topic3)\n* [Line-by-line profiling](#topic4)\n* [Vectorization](#topic5)\n* [Limitation of object dtype](#topic6)\n* [Boost pandas apply function](#topic7)\n* [Filter data frame with query method](#topic8)\n* [Reduce memory usage of data frame](#topic9)\n* [Numba](#topic10)\n* [Pandas profiling](#topic11)\n* [Pywedge](#topic12)\n","4ea6bad6":"<a id =topic7> <\/a>\n## Boost pandas apply function\n\nWith parameter `raw=True`, it'll parse the raw as numpy array","de5dbea6":"<a id =topic1> <\/a>\n# Intro\n\nThis notebook follows the course \"Faster Pandas\" by Miki Tebeka https:\/\/www.linkedin.com\/learning\/faster-pandas\/pandas-and-performance\n\n### Course details\n> *Data scientists often favor pandas, because it lets them work efficiently with larger amounts of data\u2014a useful quality as data sets become bigger and bigger. In this course, instructor Miki Tebeka shows you how to improve your pandas\u2019 code\u2019s speed and efficiency. First, Miki explains why performance matters and how you can measure it with Python profilers. Then, the course teaches you how to use vectorization to manipulate data. The course also walks through some common mistakes and how to address them.*\n\n> *Python and pandas have many high-performance built-in functions, and Miki covers how to use them. Pandas can use a lot of memory, so Miki offers good tips on how to save memory. The course demonstrates how to serialize data with SQL and HDF5. Then Miki goes over how to speed up your code with Numba and Cython. Alternative DataFrames can also speed up your code, and Miki steps through some options. Plus, explore a few extra resources that you can check out.*","add78b2e":"<a id =topic8> <\/a>\n# Filter data frame with query method\n\nUse query method for large data frames and complicated queries","e5f57358":"<a id =topic5> <\/a>\n# Vectorization\n\n**Vectorizaton or array programming is the ability to work on set of variables in parallel. Pandas is capable to use this ability to get huge performance benchmark.**\n\nEvery time you write a for loop, you should check if there is a vectorized option of that function","fc92c09f":"<a id =topic11> <\/a>\n# Pandas profiling\n\nEDA with a few lines of code","0d4a8273":"<a id =topic10> <\/a>\n# Numba\n\nNumba is a jit (just-in-time) compiler. Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. Numba-compiled numerical algorithms in Python can approach the speeds of C or FORTRAN.","fd19a93d":"<a id =topic6> <\/a>\n## Limitation of object dtype\n\nSometimes changing to the right pandas object can speed up the running time. Below is the example of comparison between datatime and str types."}}