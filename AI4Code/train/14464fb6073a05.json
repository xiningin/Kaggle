{"cell_type":{"d6a07917":"code","9cdc19c0":"code","ea9d06a8":"code","bedc6e72":"code","6c0ee88b":"code","5a675cbf":"code","031aec62":"code","f850a5ee":"code","61ff539f":"code","7d70fb0d":"code","b2a2063c":"code","444ccad4":"code","45c6e2fc":"code","8201d560":"code","b0d538c7":"code","7ff53556":"code","911c8d02":"code","5f0b788f":"code","cfbdbe60":"code","fc5ba116":"code","2f14e4ee":"code","69577685":"code","6d5c63b7":"code","120a0c96":"code","30def6c4":"code","f784b1d8":"code","d0d50dbe":"code","779a0fb1":"code","c76590f7":"code","a986ff39":"code","1b39b48a":"code","f7ee929f":"code","574e682f":"code","8d766d4c":"code","1bed2ed0":"code","20a9639b":"code","fb058f41":"code","28b73afc":"code","51a16717":"code","bef9ad08":"code","49370f43":"code","9adb3a30":"code","21469b04":"code","c472b7bd":"code","cd5464c6":"code","f3e753ad":"code","c4f70706":"code","bae68b69":"code","30f69427":"code","12fabbf2":"code","8a295b35":"code","def08976":"code","37274ff8":"code","7eac50fd":"code","e92be758":"code","8162eb59":"code","ab95e680":"code","4cc08792":"code","c44d9b41":"code","d6f2204c":"code","81ade319":"code","d836d97f":"code","68a6ac9f":"code","a46eafcd":"code","14bdbc40":"code","f94c2c12":"code","f12e2fef":"code","18e7a28c":"code","06fbdad6":"code","e35a166e":"code","f0046d13":"markdown","57de547a":"markdown","d3e65c1a":"markdown","3180ec8f":"markdown","f467f9ec":"markdown","326d534c":"markdown","c2d2a5c7":"markdown","a67f543c":"markdown","0ac4dcb9":"markdown","85aeba27":"markdown","7f2fe52a":"markdown","b85556c9":"markdown","36b4949c":"markdown","9b618d7e":"markdown","95931a75":"markdown","083e19f3":"markdown","084fbd39":"markdown","1189b3c9":"markdown","32693f2c":"markdown","b266f6e5":"markdown","6354c0ed":"markdown","882131cb":"markdown","768f40a1":"markdown","08c6404a":"markdown","834c732d":"markdown","29544f3e":"markdown","2cf4fa1e":"markdown"},"source":{"d6a07917":"import  torch\n\n## create a tensor\nyour_first_tensor = torch.tensor([3,3])\n## for random tensor\nyour_first_tensor = torch.rand(3,3)","9cdc19c0":"## finding tensor size\ntensor_size = your_first_tensor.size()","ea9d06a8":"# printing the element\nprint(your_first_tensor)\nprint(tensor_size)","bedc6e72":"tensor_of_ones = torch.ones(3,3)\n## create a identity tensor\nidentity_tensor = torch.eye(3)","6c0ee88b":"print(tensor_of_ones)\nprint(identity_tensor)","5a675cbf":"### matrix multiplication\nmatrices_multiplied = torch.matmul(tensor_of_ones,identity_tensor)","031aec62":"print(matrices_multiplied)","f850a5ee":"#### elementwise multiplication\nelement_multiplication = tensor_of_ones * identity_tensor","61ff539f":"print(element_multiplication)","7d70fb0d":"## forward propargation\n## implement a forward propagation \n## we apply this computational graph","b2a2063c":"import torch\na = torch.Tensor([2])\nb = torch.Tensor([-4])\nc = torch.Tensor([-2])\nd = torch.Tensor([-4])\ne = a+b\nf = c+d\ng = e*f\nprint(e,f,g)","444ccad4":"## second computation graph","45c6e2fc":"## assigning three random matrix with\n## shape (1000,1000)\nimport torch\nx = torch.rand(1000,1000)\ny = torch.rand(1000,1000)\nz = torch.rand(1000,1000)","8201d560":"## multiply (matrix multiplication)\nq = torch.matmul(x,y)","b0d538c7":"## now elementwise multiplication\nf = z*q","7ff53556":"## find the mean\nmean_f = torch.mean(f)","911c8d02":"print(mean_f)","5f0b788f":"### back propagation algorithm","cfbdbe60":"## we use the word gradient in terms of derivative\n## the derivatives \/ gradient in calculated by the reverse mode of auto\n## differentiation call backpropagation\n## lets calculate the derivaties of this computational graph\n","fc5ba116":"import torch\n\n## we initializa the tensor x y and z\n## remember you need to give a . to make it float\n## Only Tensors of floating point dtype can require gradients\nx = torch.tensor(-3.,requires_grad=True)\ny = torch.tensor(5.,requires_grad=True)\nz = torch.tensor(-2.,requires_grad=True)\n\n# doing the operation\nq = x+y\nf = q*z\n\n## finally we compute the derivatices \n## using the backward() method\n\nf.backward()\n\nprint(\"Gradient of z is: \"+str(z.grad))\nprint(\"Gradient of y is: \"+str(y.grad))\nprint(\"Gradient of x is: \"+str(x.grad))\n","2f14e4ee":"## practice derivatives","69577685":"import torch\n\n## we initializa the tensor x y and z\n## remember you need to give a . to make it float\n## Only Tensors of floating point dtype can require gradients\nx = torch.tensor(4.,requires_grad=True)\ny = torch.tensor(-3.,requires_grad=True)\nz = torch.tensor(5.,requires_grad=True)\n\n# doing the operation\nq = x+y\nf = q*z\n\n## finally we compute the derivatices \n## using the backward() method\n\nf.backward()\n\nprint(\"Gradient of z is: \"+str(z.grad))\nprint(\"Gradient of y is: \"+str(y.grad))\nprint(\"Gradient of x is: \"+str(x.grad))\n","6d5c63b7":"## another practice\n# Initialize x, y and z to values 4, -3 and 5\nx = torch.tensor(4.,requires_grad=True)\ny = torch.tensor(-3.,requires_grad=True)\nz = torch.tensor(5.,requires_grad=True)\n\n# Set q to sum of x and y, set f to product of q with z\nq = x+y\nf = q*z\n\n# Compute the derivatives\nf.backward()\n\n# Print the gradients\nprint(\"Gradient of x is: \" + str(x.grad))\nprint(\"Gradient of y is: \" + str(y.grad))\nprint(\"Gradient of z is: \" + str(z.grad))","120a0c96":"### find this graphs gradient ","30def6c4":"import torch\nx = torch.rand(1000,1000,requires_grad=True)\ny = torch.rand(1000,1000,requires_grad=True)\nz = torch.rand(1000,1000,requires_grad=True)\n## multiply (matrix multiplication)\nq = torch.matmul(x,y)\n## now elementwise multiplication\nf = z*q\n## find the mean\nmean_f = torch.mean(f)\nmean_f.backward()","f784b1d8":"# weight_1 = torch.rand(784,200)\n# weight_2 = torch.rand(200, 10)\n\n# # Multiply input_layer with weight_1\n# hidden_1 = torch.matmul(input_layer, weight_1)\n\n# # Multiply hidden_1 with weight_2\n# output_layer = torch.matmul(hidden_1,weight_2)\n# print(output_layer)","d0d50dbe":"import torch\ninput_layer = torch.rand(10)\nw1 = torch.rand(10,20)\nw2 = torch.rand(20,20)\nw3 = torch.rand(20,4)\n\nhidden_layer_1 = torch.matmul(input_layer,w1)\nhidden_layer_2 = torch.matmul(hidden_layer_1,w2)\noutput_layer = torch.matmul(hidden_layer_2,w3)\nprint(output_layer)\n","779a0fb1":"### same neural network object oriented way\nimport torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n  def __init__(self):\n    ## calling the base constructor\n    super(Net,self).__init__()\n    ## instantiate all two linear layers\n    ## input neurron and output neuron\n    self.fc1 = nn.Linear(784,200)\n    ## output neuron of the second will be the input of the first\n    self.fc2 = nn.Linear(200,10)\n\n  def forward(self,x):\n    x = self.fc1(x)\n    x = self.fc2(x)\n    return x\n    ","c76590f7":"net = Net()","a986ff39":"net","1b39b48a":"## simple implementation of relu\nimport torch.nn as nn\nrelu = nn.ReLU()\ntensor_1 = torch.tensor([2.,-4.])\nprint(relu(tensor_1))","f7ee929f":"# # Calculate the first and second hidden layer\n# hidden_1 = torch.matmul(input_layer, weight_1)\n# hidden_2 = torch.matmul(hidden_1, weight_2)\n\n# # Calculate the output\n# print(torch.matmul(hidden_2, weight_3))\n\n# # Calculate weight_composed_1 and weight\n# weight_composed_1 = torch.matmul(weight_1, weight_2)\n# weight = torch.matmul(weight_composed_1, weight_3)\n\n# # Multiply input_layer with weight\n# print(torch.matmul(input_layer,weight))","574e682f":"# # Apply non-linearity on hidden_1 and hidden_2\n# hidden_1_activated = relu(torch.matmul(input_layer, weight_1))\n# hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\n# print(torch.matmul(hidden_2_activated, weight_3))\n\n# # Apply non-linearity in the product of first two weights. \n# weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n\n# # Multiply `weight_composed_1_activated` with `weight_3\n# weight = torch.matmul(weight_composed_1_activated, weight_3)\n# # Multiply input_layer with weight\n# print(torch.matmul(input_layer, weight))\n","8d766d4c":"input_layer = torch.tensor([[ 0.0401, -0.9005,  0.0397, -0.0876]])\n# Instantiate ReLU activation function as relu\nrelu = nn.ReLU()\n\n# Initialize weight_1 and weight_2 with random numbers\nweight_1 = torch.rand(4, 6)\nweight_2 = torch.rand(6, 2)\n\n# Multiply input_layer with weight_1\nhidden_1 = torch.matmul(input_layer, weight_1)\n\n# Apply ReLU activation function over hidden_1 and multiply with weight_2\nhidden_1_activated = relu(hidden_1)\nprint(torch.matmul(hidden_1_activated, weight_2))","1bed2ed0":"## an example of the cross Cross Entropy Loss ","20a9639b":"\nlogits = torch.tensor([[3.2,5.1,-1.7]])\nground_truth = torch.tensor([0])\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(logits,ground_truth)\nprint(loss)","fb058f41":"logits = torch.tensor([[-1.2,.12,4.8]])\nground_truth = torch.tensor([2])\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(logits,ground_truth)\nprint(loss)","28b73afc":"# Import torch and torch.nn\nimport torch \nimport torch.nn as nn\n\n# Initialize logits and ground truth\nlogits = torch.rand(1,1000)\nground_truth = torch.tensor([111])\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(logits,ground_truth)\nprint(loss)","51a16717":"import torch\nimport torchvision\nimport torch.utils.data\nimport torchvision.transforms as transforms","bef9ad08":"transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((.1307),(.3081))])","49370f43":"## download data\ntrainset = torchvision.datasets.MNIST(root=\".\/data\",train=True,download=True,transform=transform)","9adb3a30":"## download data\ntestset = torchvision.datasets.MNIST(root=\".\/data\",train=False,download=True,transform=transform)","21469b04":"## now we make data loader so that we can feed it with batchs\ntrainloader = torch.utils.data.DataLoader(trainset,batch_size=32,shuffle=True,num_workers=0)\ntestloader = torch.utils.data.DataLoader(testset,batch_size=32,shuffle=False,num_workers=0)","c472b7bd":"print(trainloader.dataset.train_data.shape)\nprint(trainloader.dataset.test_data.shape)","cd5464c6":"# Compute the size of the minibatch for training set and testing set\ntrainset_batchsize = trainloader.batch_size\ntestset_batchsize = testloader.batch_size\n\n# Print sizes of the minibatch\nprint(trainset_batchsize,testset_batchsize)","f3e753ad":"hight = 28\nwidth = 28\nchanel = 1 ## grayscale image for rgb it will be 3\nimage_shape = 28*28*1\nhidden_neuron = 500\noutput_neuron = 10","c4f70706":"import torch \nimport torch.nn\nimport torch.nn.functional as F\nimport torch.optim as optim","bae68b69":"### make the class\nclass Net(nn.Module):\n  def __init__(self):\n    super(Net,self).__init__()\n    self.fc1 = nn.Linear(image_shape,hidden_neuron)\n    self.fc2 = nn.Linear(hidden_neuron,output_neuron)\n  def forward(self,x):\n    x = F.relu(self.fc1(x))\n    ## in the final layer there is no relu\n    x = self.fc2(x)\n    return x","30f69427":"net = Net()","12fabbf2":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(),lr=3e-4)","8a295b35":"e = []  ## store the apochs\nl =[] ## store the loss\n\nfor epoch in range(30):\n  ## for 30 iteration\n  ## we loop through the data\n  for data in trainloader:\n    inputs,labels = data  ## list unpacking\n    ## flat the array inti 1D with reshape\n    inputs = inputs.view(-1,28*28*1)\n\n    ## zero gradient so it wont get ata from preious iteration\n    optimizer.zero_grad()\n    ## make a forward pass\n    outputs = net(inputs)\n    loss = criterion(outputs,labels)\n    loss.backward() ## back propagation\n    optimizer.step() ## replace the weight\n  print (\"EPOCHS: {} LOSS {} \".format(epoch,loss))\n  e.append(epoch)\n  l.append(loss)\n","def08976":"import matplotlib.pyplot as plt\nplt.plot(e, l, 'g', label='Training loss')\n#plt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training  loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","37274ff8":"import numpy as np\n### making prediction\ncorrect_prediction = 0\npredictions = []\ntotal_case = 0\nnet.eval() ### before prediction you need to set the evaluation mode\nfor data in testloader:\n  ### list unpacking\n  inputs,labels = data\n  inputs = inputs.view(-1,28*28*1)\n  outputs = net(inputs)\n  ## show the out put of the each batch\n  predicted = torch.max(outputs.data,1)[1]\n  ## append in a prediction list\n  predictions.append(outputs)\n  total_case+=labels.size(0)  ## how you how many labels are there or how many test case\n  correct_prediction += (predicted == labels).sum().item()\nprint(\"the testing set accuracy of the network :  {} %\".format(100 * correct_prediction\/total_case))","7eac50fd":"import torch\nimport torch.nn.functional as F\n## make ramdom 28x28 image for classification\nimage_quantity = 10\nchanel = 1  ## grayscale image\nheight = 28\nweight = 28\n\nimage = torch.rand(image_quantity,chanel,height,weight)","e92be758":"conv_filters = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, stride=1, padding=1)\n\n# Convolve the image with the filters \noutput_feature = conv_filters(image)\nprint(output_feature.shape)","8162eb59":"## example of adding max pooling and avg pooling in image\nimport torch.nn\nim = torch.Tensor([[[[3,1,3,5],[6,0,7,9],[3,2,1,4],[0,2,4,3]]]])","ab95e680":"max_pooling = torch.nn.MaxPool2d(2)\noutput_feature = max_pooling(im)\nprint(output_feature)","4cc08792":"avg_pooling = torch.nn.AvgPool2d(2)\noutput_feature = avg_pooling(im)\nprint(output_feature)","c44d9b41":"class ConvolutionalNet(nn.Module):\n\n  def __init__(self):\n    super(ConvolutionalNet,self).__init__()\n    ## remember the in_channels na dout channels are random\n    ## this properties will be initialized when the class is initialized\n    self.conv1 = nn.Conv2d(in_channels=1,out_channels=5,kernel_size=3,padding=1)\n    self.conv2 = nn.Conv2d(in_channels=5,out_channels=10,kernel_size=3,padding=1)\n    self.relu = nn.ReLU()\n    # instantiate the mx pooling layer\n    self.pool = nn.MaxPool2d(2,2)\n    ## so the final size will be 28\/2*2 (2 by 2 pooling) = 7\n    self.fc = nn.Linear(7*7*10,10)\n  \n  ## apply the forward layer\n  def forward(self,x):\n    x = self.relu(self.conv1(x))\n    x = self.pool(x)\n    x = self.relu(self.conv2(x))\n    x = self.pool(x)\n    x = x.view(-1,7*7*10)\n    return self.fc(x)\n","d6f2204c":"net = ConvolutionalNet()","81ade319":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(),lr=3e-4)","d836d97f":"e = []  ## store the apochs\nl =[] ## store the loss\n\nfor epoch in range(30):\n  ## for 30 iteration\n  ## we loop through the data\n  for data in trainloader:\n    inputs,labels = data  ## list unpacking\n    ## you dont need to flat it we are not deefing it to the FC layer directly\n\n    ## zero gradient so it wont get ata from preious iteration\n    optimizer.zero_grad()\n    ## make a forward pass\n    outputs = net(inputs)\n    loss = criterion(outputs,labels)\n    loss.backward() ## back propagation\n    optimizer.step() ## replace the weight\n  print (\"EPOCHS: {} LOSS {} \".format(epoch,loss))\n  e.append(epoch)\n  l.append(loss)\n","68a6ac9f":"import matplotlib.pyplot as plt\nplt.plot(e, l, 'g', label='Training loss')\n#plt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training  loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","a46eafcd":"import numpy as np\n### making prediction\ncorrect_prediction = 0\npredictions = []\ntotal_case = 0\nnet.eval() ### before prediction you need to set the evaluation mode\nfor data in testloader:\n  ### list unpacking\n  inputs,labels = data\n  outputs = net(inputs)\n  ## show the out put of the each batch\n  predicted = torch.max(outputs.data,1)[1]\n  ## append in a prediction list\n  predictions.append(outputs)\n  total_case+=labels.size(0)  ## how you how many labels are there or how many test case\n  correct_prediction += (predicted == labels).sum().item()\nprint(\"the testing set accuracy of the network :  {} %\".format(100 * correct_prediction\/total_case))","14bdbc40":"## smae network in encapsulated way\n\nclass Net(nn.Module):\n  def __init__(self):\n    super(Net,self).__init__()\n\n    self.features = nn.Sequential(\n        nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1),\n        nn.MaxPool2d(2,2),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n        nn.MaxPool2d(2,2),\n        nn.ReLU(inplace=True)\n\n    )\n    self.classifier = nn.Sequential(\n        nn.Linear(7*7*40,1024),\n        nn.ReLU(inplace=True),\n        nn.Linear(1024,2048),  ## input neyron and output neuron\n        nn.ReLU(inplace=True),\n        nn.Linear(2048,10)\n    )\n\n  def forward(self,x):\n    x = self.features(x)\n    x = x.view(-1,7,7*40)\n    x = self.classifier(x)\n","f94c2c12":"# Shuffle the indices\nindices = np.arange(60000)\nnp.random.shuffle(indices)\n\n# Build the train loader\ntrain_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('mnist', download=True, train=True,\n                     transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n                     batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:55000]))\n\n# Build the validation loader\nval_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('mnist', download=True, train=True,\n                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n                   batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[55000:60000]))","f12e2fef":"# Instantiate the network\nmodel = Net()\n\n# Instantiate the cross-entropy loss\ncriterion = nn.CrossEntropyLoss()\n\n# Instantiate the Adam optimizer\noptimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.001)","18e7a28c":"class Net(nn.Module):\n    def __init__(self):\n        \n        # Define all the parameters of the net\n        self.classifier = nn.Sequential(\n            nn.Linear(28*28, 200),\n            nn.ReLU(inplace=True),\n            nn.Dropout(.5),\n            nn.Linear(200,500),\n            nn.ReLU(inplace=True),\n            nn.Linear(500,10))\n    def forward(self,x):\n      return self.classifier(x)","06fbdad6":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        # Implement the sequential module for feature extraction\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),\n            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(10),\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),\n            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(20))\n        \n        # Implement the fully connected layer for classification\n        self.fc = nn.Linear(in_features=7*7*20, out_features=10)","e35a166e":"# Import the module\nimport torchvision\n\n# Download resnet18\nmodel = torchvision.models.resnet18(pretrained=True)\n\n# Freeze all the layers bar the last one\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Change the number of output units\nmodel.fc = nn.Linear(512, 7)\n## then apply the network in your image","f0046d13":"## **Activation Function**\nactivation function are non linear functions which ae inserted in to the each neural network that make the neural network non linear and allow then to deal with the non linear dataset. the most popular activation function is **rectified linear or Relu**","57de547a":"###**what is derivatives ??** \nderivatives are one of the most important comcepts in calculas. in laymens term derivatives means the rate of change in a function.is the function is rapidly changing then the value of derivatives is high. and if the function is not changing then the derivatives is close to zero.**they could also be treated as the deepness of the function** if you see the picture if the line is dep then the derivatives is high and point B is very low derivatives![alt text](https:\/\/raw.githubusercontent.com\/tanviredu\/DATACAMP_PYTORCH\/master\/graph3.PNG)","d3e65c1a":"# **NEURAL NETWORK TRAINING PROCESS**\n*   initialize neural network with random weight \n*   Do a forward pass\n*   Calculate Loss Function\n*   Calculate The Gradients\n*   Change the weight based on the Gradients\n*   for regression the loss mse\n*   for classification softmax crossentropy","3180ec8f":"## **In batch norm and droupout netwoek works differently** \nso use\n\n\n1.   **model.train()** for training\n2.   **model.eval()** for testing\n\n","f467f9ec":"![alt text](https:\/\/raw.githubusercontent.com\/tanviredu\/DATACAMP_PYTORCH\/master\/graph5.PNG)","326d534c":"**derivative rule **![alt text](https:\/\/raw.githubusercontent.com\/tanviredu\/DATACAMP_PYTORCH\/master\/grapf4.PNG)","c2d2a5c7":"# **Convolutional Nural Network**\nthe convolution process is preety much straight forward. when an image is given and there will be filter smaller than the image and the filter is actually the weight . and you get good feature by multiplying the image with the filter . the filter will splide through the image and by multiplugin with the image it will create a something called activation map also called the feature map a convolustional network is a collection of feature map. the goal of the CNN is learn for this activation map and detext the feature","a67f543c":"## **building a Complete Neural network** ","0ac4dcb9":"![alt text](https:\/\/raw.githubusercontent.com\/tanviredu\/DATACAMP_PYTORCH\/master\/graph6.PNG)","85aeba27":"![alt text](https:\/\/raw.githubusercontent.com\/tanviredu\/DATACAMP_PYTORCH\/master\/graph2.PNG)","7f2fe52a":"## **How To Import Data set and prepare it**","b85556c9":"## **MAX POOLING AND AVG POOLING**\nConvolutional layer is the main building block of the CNN. there is another layer is CNN is called Pooling layer.pooling layer job is purely the feature selection.and its sole job is to extract the dominent feature of the image\nand it also lower the resolution of the image and make the computation easy.\n\n**two types of popular poolign layer**\n1.   Max pooling\n2.   AVG Pooling\n\n\n","36b4949c":"## If you find this notebook useful please upvote.","9b618d7e":"## **Introducing validation set using sub sampleing the test data for checking overfitting**","95931a75":"![alt text](https:\/\/raw.githubusercontent.com\/tanviredu\/DATACAMP_PYTORCH\/master\/graph2.PNG)","083e19f3":"# **Sequential Module** \nBecause we are using object oriented programming we can actually make this code more efficient and with less code we can do the same job we encapsulate all the layer that does the same job","084fbd39":"## **Train The MNIST MODEL** ","1189b3c9":"# Fully Connected Neural net![alt text](https:\/\/raw.githubusercontent.com\/tanviredu\/DATACAMP_PYTORCH\/master\/graph7.PNG)","32693f2c":"![Computational Graph](https:\/\/raw.githubusercontent.com\/tanviredu\/DATACAMP_PYTORCH\/master\/graph.PNG)","b266f6e5":"## **l2 Regularazation**\nto prevent over fitting we can use L2 regularaization . regularaizar penalize the large weight whaich affects the training in  a bad way .using regularazer in pytorch is very easy just add the weight_decay parameter in in the optimizer\n","6354c0ed":"![alt text](https:\/\/raw.githubusercontent.com\/tanviredu\/DATACAMP_PYTORCH\/master\/graph8.PNG)","882131cb":"## **Droput** \nadding droupout laer is another way of minimizing the the overfitting it drps the connection between two subsequent layer","768f40a1":"## **Transfer Learning**\nthe deeper you go into a network the feature become abstract that gives a chance to use CNN transfer laerning","08c6404a":"Remember that torch.max() takes two arguments: -output.data - the tensor which contains the data.\n\n**Either 1 to do argmax or 0 to do max**","834c732d":"why after applying relu the -4 become zero because relu activation function formula is\n**ReLu(x) = max(0,x)** if the value greater then 0 then return the value other wise return 0","29544f3e":"## **Batch normalzation and early stopping**\nbatch normalizatio is used in any network and early stppoing  checks the accuracy of the validation checks and stop the network if the accuracy start to decrease","2cf4fa1e":"![alt text](https:\/\/raw.githubusercontent.com\/tanviredu\/DATACAMP_PYTORCH\/master\/graph9.PNG)"}}