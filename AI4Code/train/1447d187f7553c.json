{"cell_type":{"8e619229":"code","a41f1889":"code","778412a5":"code","b465e599":"code","79ebd446":"code","9fff58b7":"code","c35f21b1":"code","d5cc811c":"code","c2d6dece":"code","d30cdd12":"code","5554c1ac":"code","52a829d2":"code","d4cafe33":"code","f5b7d5be":"code","df19150c":"markdown","36804be6":"markdown","7cac9eca":"markdown","77e92be6":"markdown","321e73aa":"markdown","c5c79ec5":"markdown","8288df9d":"markdown","f78a68cf":"markdown","224ba8e2":"markdown","636dc323":"markdown","6812ab72":"markdown","d5f25c18":"markdown","2838b056":"markdown","00ecbc2f":"markdown"},"source":{"8e619229":"import gc\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\nrandom.seed(42)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\ndef reload():\n    gc.collect()\n    df = pd.read_csv('..\/input\/train_V2.csv')\n    invalid_match_ids = df[df['winPlacePerc'].isna()]['matchId'].values\n    df = df[-df['matchId'].isin(invalid_match_ids)]\n    return df","a41f1889":"df = reload()\n\ncols_to_drop = ['Id', 'groupId', 'matchId', 'matchType']\ncols_to_fit = [col for col in df.columns if col not in cols_to_drop]\ncorr = df[cols_to_fit].corr()\n\nplt.figure(figsize=(9,7))\nsns.heatmap(\n    corr,\n    xticklabels=corr.columns.values,\n    yticklabels=corr.columns.values,\n    linecolor='white',\n    linewidths=0.1,\n    cmap=\"RdBu\"\n)\nplt.show()\n\nagg = df.groupby(['groupId']).size().to_frame('players_in_team')\ndf = df.merge(agg, how='left', on=['groupId'])\ndf['headshotKills_over_kills'] = df['headshotKills'] \/ df['kills']\ndf['headshotKills_over_kills'].fillna(0, inplace=True)\ndf['killPlace_over_maxPlace'] = df['killPlace'] \/ df['maxPlace']\ndf['killPlace_over_maxPlace'].fillna(0, inplace=True)\ndf['killPlace_over_maxPlace'].replace(np.inf, 0, inplace=True)\ncorr = df[['killPlace', 'walkDistance', 'players_in_team', 'headshotKills_over_kills', 'killPlace_over_maxPlace', 'winPlacePerc']].corr()\n\nplt.figure(figsize=(8,6))\nsns.heatmap(\n    corr,\n    xticklabels=corr.columns.values,\n    yticklabels=corr.columns.values,\n    annot=True,\n    linecolor='white',\n    linewidths=0.1,\n    cmap=\"RdBu\"\n)\nplt.show()","778412a5":"def train_test_split(df, test_size=0.1):\n    match_ids = df['matchId'].unique().tolist()\n    train_size = int(len(match_ids) * (1 - test_size))\n    train_match_ids = random.sample(match_ids, train_size)\n\n    train = df[df['matchId'].isin(train_match_ids)]\n    test = df[-df['matchId'].isin(train_match_ids)]\n    \n    return train, test","b465e599":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\ndef run_experiment(preprocess):\n    df = reload()\n    df.drop(columns=['matchType'], inplace=True)\n    \n    df = preprocess(df)\n\n    target = 'winPlacePerc'\n    cols_to_drop = ['Id', 'groupId', 'matchId', target]\n    cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n    train, val = train_test_split(df, 0.1)\n    \n    model = LinearRegression()\n    model.fit(train[cols_to_fit], train[target])\n    \n    y_true = val[target]\n    y_pred = model.predict(val[cols_to_fit])\n    return mean_absolute_error(y_true, y_pred)\n\ndef run_experiments(preprocesses):\n    results = []\n    for preprocess in preprocesses:\n        start = time.time()\n        score = run_experiment(preprocess)\n        execution_time = time.time() - start\n        results.append({\n            'name': preprocess.__name__,\n            'score': score,\n            'execution time': f'{round(execution_time, 2)}s'\n        })\n        gc.collect()\n        \n    return pd.DataFrame(results, columns=['name', 'score', 'execution time']).sort_values(by='score')","79ebd446":"def original(df):\n    return df\n\ndef items(df):\n    df['items'] = df['heals'] + df['boosts']\n    return df\n\ndef players_in_team(df):\n    agg = df.groupby(['groupId']).size().to_frame('players_in_team')\n    return df.merge(agg, how='left', on=['groupId'])\n\ndef total_distance(df):\n    df['total_distance'] = df['rideDistance'] + df['swimDistance'] + df['walkDistance']\n    return df\n\ndef headshotKills_over_kills(df):\n    df['headshotKills_over_kills'] = df['headshotKills'] \/ df['kills']\n    df['headshotKills_over_kills'].fillna(0, inplace=True)\n    return df\n\ndef killPlace_over_maxPlace(df):\n    df['killPlace_over_maxPlace'] = df['killPlace'] \/ df['maxPlace']\n    df['killPlace_over_maxPlace'].fillna(0, inplace=True)\n    df['killPlace_over_maxPlace'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef walkDistance_over_heals(df):\n    df['walkDistance_over_heals'] = df['walkDistance'] \/ df['heals']\n    df['walkDistance_over_heals'].fillna(0, inplace=True)\n    df['walkDistance_over_heals'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef walkDistance_over_kills(df):\n    df['walkDistance_over_kills'] = df['walkDistance'] \/ df['kills']\n    df['walkDistance_over_kills'].fillna(0, inplace=True)\n    df['walkDistance_over_kills'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef teamwork(df):\n    df['teamwork'] = df['assists'] + df['revives']\n    return df","9fff58b7":"run_experiments([\n    original,\n    items,\n    players_in_team,\n    total_distance,\n    headshotKills_over_kills,\n    killPlace_over_maxPlace,\n    walkDistance_over_heals,\n    walkDistance_over_kills,\n    teamwork\n])","c35f21b1":"def min_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId','groupId'])[features].min()\n    return df.merge(agg, suffixes=['', '_min'], how='left', on=['matchId', 'groupId'])\n\ndef max_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId', 'groupId'])[features].max()\n    return df.merge(agg, suffixes=['', '_max'], how='left', on=['matchId', 'groupId'])\n\ndef sum_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId', 'groupId'])[features].sum()\n    return df.merge(agg, suffixes=['', '_sum'], how='left', on=['matchId', 'groupId'])\n\ndef median_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId', 'groupId'])[features].median()\n    return df.merge(agg, suffixes=['', '_median'], how='left', on=['matchId', 'groupId'])\n\ndef mean_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n    return df.merge(agg, suffixes=['', '_mean'], how='left', on=['matchId', 'groupId'])\n\ndef rank_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n    agg = agg.groupby('matchId')[features].rank(pct=True)\n    return df.merge(agg, suffixes=['', '_mean_rank'], how='left', on=['matchId', 'groupId'])","d5cc811c":"run_experiments([\n    original,\n    min_by_team,\n    max_by_team,\n    sum_by_team,\n    median_by_team,\n    mean_by_team,\n    rank_by_team\n])","c2d6dece":"df = reload()\n\n# Option 1: Give it as category\n# df['matchType'] = df['matchType'].astype('category')\n# Option 2: pd.dummies\n# df = pd.concat([df, pd.get_dummies(df['matchType'])], axis=1)\n# Option 3: Drop it for now (Not the best solution)\ndf.drop(columns=['matchType'], inplace=True)\n\ntarget = 'winPlacePerc'\ncols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\ncols_to_fit = [col for col in df.columns if col not in cols_to_drop]\ntrain, val = train_test_split(df, 0.1)\n\nfrom lightgbm import LGBMRegressor\nparams = {\n    'n_estimators': 100,\n    'learning_rate': 0.3, \n    'num_leaves': 20,\n    'objective': 'regression_l2', \n    'metric': 'mae',\n    'verbose': -1,\n}\n\nmodel = LGBMRegressor(**params)\nmodel.fit(\n    train[cols_to_fit], train[target],\n    eval_set=[(val[cols_to_fit], val[target])],\n    eval_metric='mae',\n    verbose=20,\n)\n\nfeature_importance = pd.DataFrame(sorted(zip(model.feature_importances_, cols_to_fit)), columns=['Value','Feature'])\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","d30cdd12":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=42).fit(val[cols_to_fit], val[target])\neli5.show_weights(perm, feature_names=list(cols_to_fit))","5554c1ac":"import shap\nshap.initjs()","52a829d2":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(val[cols_to_fit])","d4cafe33":"shap.summary_plot(shap_values, val[cols_to_fit], plot_type='bar')","f5b7d5be":"shap.summary_plot(shap_values, val[cols_to_fit], feature_names=cols_to_fit)","df19150c":"SHAP also can visualize how the score changes when the feature value is low\/high on each data.","36804be6":"We can see the score and the execution time.\n\nNext, let's see aggregated features.","7cac9eca":"`run_experiments` method takes preprocess functions and returns DataFrame.","77e92be6":"## 5. SHAP values\n\nSHAP proposed a new fair way to measure contribution which is justified in game theory.\n\n[A Unified Approach to Interpreting Model\nPredictions](https:\/\/arxiv.org\/pdf\/1705.07874.pdf) (2017)\n\n> Understanding why a model makes a certain prediction can be as crucial as the prediction\u2019s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability.\n\nSimple models are easy to interpret. They built a simple model which works well only on a local point (We don't need to predict on the all points). Then, use the simple model to interpret how it's trained.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/marcotcr\/lime\/master\/doc\/images\/lime.png\" width=\"420\">\n\nThis method was proposed in this paper: [\u201cWhy Should I Trust You?\u201d\nExplaining the Predictions of Any Classifier](https:\/\/arxiv.org\/pdf\/1602.04938.pdf) (2016, known as LIME)\n\nThe main contribution of SHAP is that they introduced the concept of Shapley Value to measure the contribution.\n\nShapley Value is a solution concept in cooperative game theory, proposed in 1953 by [Lloyd Shapley](https:\/\/en.wikipedia.org\/wiki\/Lloyd_Shapley).\n\n- Question: What is a \"fair\" way for a colition to divide its payoff?\n  - Depends on the definition of \"fairness\"\n- Approach: Identify axioms that express properties of a fair payoff division\n  - Symmetry: Interchangeable agents should receive the same payments\n  - Dummy Players: Dummy players should receive nothing\n  - Additivity: $$(v_1 + v_2)(S) = v_1(S) + v_2(S)$$\n\nThe author of SHAP found that we can apply this concept to machine learning. In machine learning, *player* is *feature*, and *contribution* is *score*.\n\nFor example, we have 3 features L, M, and N. The shapley values are calculated like below.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*DLL5sCQKeVXboAYIvdgwUw.png\" width=\"640\">\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*uGjQRe9U0ebC5HxYXAzg3A.png\" width=\"420\">\n\nIt's the average of combinations of features. Intuitively, it sounds fair.\n\nThe implementation is available on GitHub: [slundberg\/shap: A unified approach to explain the output of any machine learning model](https:\/\/github.com\/slundberg\/shap)","321e73aa":"SHAP values represent the fair score of features depending on their contribution towards the total score in the set of features.","c5c79ec5":"Checking correlation is the fastest way to estimate the impact but it doesn't capture the actual contribution of the score. I will show an example in another Kernel.","8288df9d":"# Effective feature engineering\n\nSay, you have a model which takes 1 hour to train. Do you always need to wait for 1 hour to see the impact of a newly added feature?\n\nI talked with experienced Kaggers about feature engineering.\nThey said that you get more accurate result when you check on the actual complex model you carefully built. But, it can be another risk if longer training time reduces the number of your trials.\n\n**Traid-off: Accurate result <=> The number of trials**\n\nHow they work on a competition is like below.\n\n1. EDA\n2. Build a simple model\n3. Try various features on a simple model\n4. Build a complex model\n5. Train with promising features\n\nThey go back and forth between steps during a competition. If you successfully setup an effective environment for experiments at the beginning of the competition, it puts you at an advantage.\n\nI created a Kernel dedicated for feature engineering for this competition. I'd like to share what I've done so far.\n\n1. Correlation\n2. Score gain on a simple model\n3. Feature importances of Tree models\n4. Permutation importance\n5. SHAP values\n6. Score gain on a complex model\n\nThe upper things are faster but less accurate and lower things are more accurate but slower. I'm trying from the top of the list when I come up with a new idea.\n\n## 1. Correlation\n\nThis is the simplest way to see the relation between features. In here, if the value on the target is close to 0, it means that the feature may be irrelevant to the target.","f78a68cf":"You can see how important rank features are in this competition.\n\n## 3. Feature importances of Tree models\n\nTree models can output feature importances.","224ba8e2":"The figure shows the importance of each feature. ELI5 shuffles the target feature instead of removing it to make it useless so that we don't need to re-train the model again. That's why it's represented like `\u00b1 0.0033` (standard deviation).\n\nRemoving a feature and see the difference... this is what I was doing above, but more reliable! However, there is room to discuss how to define\/measure contribution.","636dc323":"## 4. Permutation importance\n\nThe basic idea is that observing how much the score decreases when a feature is not available; the method is known as \u201cpermutation importance\u201d or \u201cMean Decrease Accuracy (MDA)\u201d.","6812ab72":"There are several options for measuring importance like \"split\" (How many times the feature is used to split), \"gain\" (The average training loss reduction gained when using a feature for splitting).\n\nHowever, sometimes it doesn't represent the actual contribution.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*UEQiHKTnjHJ-swIjcAkRnA.png\" width=\"640\">\n\n> To our dismay we see that the feature importance orderings are very different for each of the three options provided by XGBoost! For the cover method it seems like the capital gain feature is most predictive of income, while for the gain method the relationship status feature dominates all the others. This should make us very uncomfortable about relying on these measures for reporting feature importance without knowing which method is best.\n\n<div align=\"center\">\n    <a href=\"https:\/\/towardsdatascience.com\/interpretable-machine-learning-with-xgboost-9ec80d148d27\">Interpretable Machine Learning with XGBoost<\/a>\n<\/div>\n\nThis is the background of Interpretable machine learning which is a field receiving a lot of attention recently. You can find papers and libraries here: [lopusz\/awesome-interpretable-machine-learning](https:\/\/github.com\/lopusz\/awesome-interpretable-machine-learning)","d5f25c18":"Let's see player-level generated features.","2838b056":"## 6. Score gain on a complex model\n\nChecking on the real model. After selecting important features, I start looking at the actual impact on a complex model.\n\n---\n\nSo, what should we do? It depends on the requirement and there is a trade-off but I'd recommend to build an environment to try new ideas quickly and fail quickly! You'd earn new ideas through the process.\n\nThank you for reading this Kernel. I'm still new to this field. I'd like to hear how you work on feature engineering.","00ecbc2f":"## 2. Score gain on a simple model\n\nI used LinearRegression during feature engineering since it's simple and fast. It'd be enough if you just want to see the impact of the new feature you added.\n\nAside from the main topic of this Kernel, it's better to split dataset by match since **we predict results by group in match**."}}