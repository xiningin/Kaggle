{"cell_type":{"6a8b418e":"code","7f520690":"code","ee677961":"code","64d9927d":"code","1256ad0e":"code","90575689":"code","1ae7bfae":"code","604a328e":"code","38659496":"code","3c8bc346":"code","015b4f01":"code","89702bf5":"code","78c8b77e":"code","e80a9519":"code","5e759726":"code","08e1cd36":"code","288355e5":"code","e0a539a8":"code","91b618be":"code","ef63cb53":"code","6f196e2f":"code","d229ad8d":"code","8bc30fe2":"code","4cd877d7":"code","5a74eb42":"code","c0252abb":"code","6fe44fb7":"code","263e1652":"code","b9eb8fd9":"code","fd449a75":"code","29a7df8c":"code","ce84a27e":"code","279dca90":"code","0e773a37":"code","4e171229":"code","8e5619aa":"code","f0e5aafb":"code","7f23a5b9":"code","94cf9489":"code","e4668c95":"code","db1c8948":"code","03a9e538":"code","d555afe7":"code","312a8966":"code","f3393ce5":"code","e896ba35":"code","10754226":"code","40e2402e":"code","ffcc0795":"code","2d942963":"code","bbaf537f":"code","cc5c99fe":"code","ed78d420":"code","dad3f41b":"code","793f813a":"code","9481c39b":"code","a6af6f1d":"code","6d31213f":"code","887ee2b5":"code","cec6ac2c":"code","1eca9349":"code","4550ad4f":"code","8b2e29c6":"code","0960ec9f":"code","60b7f3ce":"code","9079780d":"code","0bff55ce":"code","0487e421":"code","867e5f86":"code","654d8752":"code","6f9984cc":"code","96901411":"code","0b50311c":"code","258613f5":"code","921e8278":"code","5d63f2ec":"code","f88700f4":"code","07b13639":"code","38d96972":"code","03810dbf":"code","b18ff846":"code","ffba5824":"code","8678ad71":"code","5dcccd35":"code","d8c33988":"code","23449457":"code","0dcd10ae":"code","2fad6b64":"code","84d9e999":"code","aeb71edb":"code","2b9df856":"code","69f318dd":"code","6a970508":"code","d484988f":"code","376b4cb9":"code","1123b204":"code","54e65a51":"code","4541bbfb":"code","535031fd":"code","64d0654f":"code","bd28101b":"code","8cb8cd0e":"code","aec79f23":"code","ca46f80d":"code","f6a2c52c":"code","c776a2ce":"code","cbe0f846":"code","69a9f00f":"code","df9df6b1":"code","bb7cb10b":"code","d8ad2b03":"code","2b88b28d":"code","a8f29390":"code","ea969879":"code","6a886582":"code","4b4e3559":"code","7862320f":"code","3e567017":"code","ff67279c":"code","e12cfe4e":"code","5ced178f":"code","0db3eadb":"code","c2d6f896":"code","7c4312f7":"code","d2e8e40b":"code","de1fdff6":"code","7ef6f9b1":"code","4668d7a0":"code","1bad3fb3":"code","24a3caea":"code","be8a308e":"code","382576e2":"code","b6123d0a":"code","40fc355f":"code","52f3e718":"code","761331e2":"code","df839e22":"code","d4a09976":"code","6a3c4611":"code","35a8a0e1":"code","db53187f":"code","28558d70":"code","e63bf911":"code","8f52de97":"code","1f71f4d4":"code","0b3613dd":"code","2efd7ee3":"code","3ec805e2":"code","5ac943d7":"code","7348bc9b":"code","623846dd":"code","10bf9ded":"code","505fe9b7":"code","bad878db":"code","ca232003":"code","06f15c04":"code","6f2da462":"code","23c0259a":"code","09e59101":"code","36ea52d1":"code","73283a39":"code","538d2b4f":"code","bf02cbdb":"code","36078a6f":"code","4100c1f5":"code","d48269d9":"code","59998344":"code","d05bd18a":"code","fa737ec7":"code","9fffd7fb":"code","2ac70c6f":"code","f809f3b9":"markdown","b88a7d47":"markdown","f9e6bdd5":"markdown","5c4013ce":"markdown","f1e535de":"markdown","4d8bcca9":"markdown","fbf3bd47":"markdown","54b92e37":"markdown","9b62ca34":"markdown","7267e473":"markdown","7537f920":"markdown","a357edfc":"markdown","d0e692f6":"markdown","c8311121":"markdown","ae9fc07d":"markdown","89eb98f8":"markdown","c2751c1c":"markdown","c4a3af1a":"markdown","e6287e3e":"markdown","fb4c1281":"markdown","c09cf923":"markdown","d86302de":"markdown","8579bd77":"markdown","0ee8afc9":"markdown","5a3e72d2":"markdown","c7802ca9":"markdown","05a8cf79":"markdown","a952e058":"markdown","ef8a9084":"markdown","15e11c5d":"markdown","c712da01":"markdown","01ca6679":"markdown","5ba57712":"markdown","a0068e92":"markdown","772769ca":"markdown","d4f0fcb5":"markdown","2a6f7ead":"markdown","f1e60dfa":"markdown","16750886":"markdown","5e85d30f":"markdown","10ca7bcd":"markdown","5d6fdb17":"markdown","1d8e21d1":"markdown","f8a5f997":"markdown","2bbfcaea":"markdown","4f48a894":"markdown","606edfa8":"markdown","3f17b9fa":"markdown","f4a291a8":"markdown","8e042473":"markdown","1229d3ec":"markdown","e34d60ad":"markdown","6ac4b0b4":"markdown","185e287d":"markdown","01f0dd52":"markdown","a1e8c8b0":"markdown","6990049e":"markdown","9eeff3b3":"markdown","f4be63c0":"markdown","22b2e084":"markdown","a0e5fd34":"markdown"},"source":{"6a8b418e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7f520690":"import numpy as np\nimport pandas as pd","ee677961":"loan_data_backup = pd.read_csv('\/kaggle\/input\/loan-data-capstone-2021\/loan_data_2007_2014.csv')","64d9927d":"loan_data = loan_data_backup.copy()","1256ad0e":"loan_data","90575689":"pd.options.display.max_columns = None\n#pd.options.display.max_rows = None","1ae7bfae":"loan_data.head()","604a328e":"loan_data.tail()","38659496":"loan_data.columns","3c8bc346":"loan_data.info()","015b4f01":"loan_data.describe().T","89702bf5":"loan_data['emp_length'].unique()","78c8b77e":"loan_data['emp_length_int'] = loan_data['emp_length'].str.replace('\\+ years', '')\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('< 1 year', str(0))\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('n\/a',  str(0))\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' years', '')\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' year', '')","e80a9519":"type(loan_data['emp_length_int'][0])","5e759726":"loan_data['emp_length_int'] = pd.to_numeric(loan_data['emp_length_int'])","08e1cd36":"type(loan_data['emp_length_int'][0])","288355e5":"loan_data['earliest_cr_line']","e0a539a8":"loan_data['earliest_cr_line_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = '%b-%y')","91b618be":"type(loan_data['earliest_cr_line_date'][0])","ef63cb53":"pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date']","6f196e2f":"loan_data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date']) \/ np.timedelta64(1, 'M')))","d229ad8d":"loan_data['mths_since_earliest_cr_line'].describe()","8bc30fe2":"loan_data.loc[: , ['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']][loan_data['mths_since_earliest_cr_line'] < 0]","4cd877d7":"loan_data['mths_since_earliest_cr_line'][loan_data['mths_since_earliest_cr_line'] < 0] = loan_data['mths_since_earliest_cr_line'].max()","5a74eb42":"min(loan_data['mths_since_earliest_cr_line'])","c0252abb":"loan_data['term']","6fe44fb7":"loan_data['term'].describe()","263e1652":"loan_data['term_int'] = loan_data['term'].str.replace(' months', '')","b9eb8fd9":"loan_data['term_int']","fd449a75":"type(loan_data['term_int'][25])","29a7df8c":"loan_data['term_int'] = pd.to_numeric(loan_data['term'].str.replace(' months', ''))\n\nloan_data['term_int']","ce84a27e":"type(loan_data['term_int'][0])","279dca90":"loan_data['issue_d']","0e773a37":"loan_data['issue_d_date'] = pd.to_datetime(loan_data['issue_d'], format = '%b-%y')\nloan_data['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['issue_d_date']) \/ np.timedelta64(1, 'M')))","4e171229":"loan_data['mths_since_issue_d'].describe()","8e5619aa":"loan_data.info()","f0e5aafb":"pd.get_dummies(loan_data['grade'])","7f23a5b9":"pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':')\n# Create dummy variables from a variable.","94cf9489":"loan_data_dummies = [pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['sub_grade'], prefix = 'sub_grade', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['home_ownership'], prefix = 'home_ownership', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['verification_status'], prefix = 'verification_status', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['loan_status'], prefix = 'loan_status', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['purpose'], prefix = 'purpose', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['addr_state'], prefix = 'addr_state', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['initial_list_status'], prefix = 'initial_list_status', prefix_sep = ':')]\n","e4668c95":"loan_data_dummies = pd.concat(loan_data_dummies, axis = 1)\n# concatenate the dummy variables and this turns them into a dataframe.","db1c8948":"type(loan_data_dummies)\n# Returns the type of the variable.","03a9e538":"loan_data = pd.concat([loan_data, loan_data_dummies], axis = 1)\n# concatenate the dataframe with original data with the dataframe with dummy variables, along the columns. ","d555afe7":"loan_data.columns\n# Display all column names.","312a8966":"# Sets the pandas dataframe options to display all columns\/ rows.\nloan_data.isnull().sum()","f3393ce5":"loan_data['total_rev_hi_lim'].fillna(loan_data['funded_amnt'], inplace=True)","e896ba35":"loan_data['total_rev_hi_lim'].isnull().sum()","10754226":"loan_data['annual_inc'].fillna(loan_data['annual_inc'].mean(), inplace=True)\n#Fill the missing values with the mean value of the non-missing values.","40e2402e":"loan_data['mths_since_earliest_cr_line'].fillna(0, inplace=True)\nloan_data['acc_now_delinq'].fillna(0, inplace=True)\nloan_data['total_acc'].fillna(0, inplace=True)\nloan_data['pub_rec'].fillna(0, inplace=True)\nloan_data['open_acc'].fillna(0, inplace=True)\nloan_data['inq_last_6mths'].fillna(0, inplace=True)\nloan_data['delinq_2yrs'].fillna(0, inplace=True)\nloan_data['emp_length_int'].fillna(0, inplace=True)\n# We fill the missing values with zeroes.","ffcc0795":"loan_data['loan_status'].unique()\n# checking unique values of a column.","2d942963":"loan_data['loan_status'].value_counts() \/ loan_data['loan_status'].count()","bbaf537f":"# Good\/ Bad Definition\nloan_data['good_bad'] = np.where(loan_data['loan_status'].isin(['Charged Off', 'Default',\n                                                       'Does not meet the credit policy. Status:Charged Off',\n                                                       'Late (31-120 days)']), 0, 1)\n","cc5c99fe":"loan_data['good_bad'].head()","ed78d420":"from sklearn.model_selection import train_test_split","dad3f41b":"loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'], test_size = 0.2, random_state = 42)","793f813a":"loan_data_inputs_train.shape","9481c39b":"loan_data_targets_train.shape","a6af6f1d":"loan_data_inputs_test.shape","6d31213f":"loan_data_targets_test.shape","887ee2b5":"#Storing the input and target files in a separate dataframe\n#df_inputs_prepr = loan_data_inputs_train\n#df_targets_prepr = loan_data_targets_train","cec6ac2c":"df_inputs_prepr = loan_data_inputs_test\ndf_targets_prepr = loan_data_targets_test","1eca9349":"df_inputs_prepr.shape","4550ad4f":"df_targets_prepr.shape","8b2e29c6":"df_inputs_prepr['grade'].unique()\n# Displays unique values of a column.","0960ec9f":"df1 = pd.concat([df_inputs_prepr['grade'], df_targets_prepr], axis = 1)\n# Concatenate two dataframes along the columns.\ndf1.head()","60b7f3ce":"df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count()","9079780d":"df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()\n# Here we calculate the mean of the values in the column with index 1 for each value of the column with index 0.","0bff55ce":"df1 = pd.concat([df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count(),\n                df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()], axis = 1)\n# Concatenate two dataframes along the columns.","0487e421":"df1","867e5f86":"df1 = df1.iloc[:, [0, 1, 3]]\n# Select only columns with specific indexes.\ndf1","654d8752":"df1.columns = [df1.columns.values[0], 'n_obs', 'prop_good']\n# Change the names of the columns of a dataframe.\ndf1","6f9984cc":"df1['prop_n_obs'] = df1['n_obs'] \/ df1['n_obs'].sum()\n# We divide the values of one column by he values of another column and save the result in a new variable.\ndf1","96901411":"df1['n_good'] = df1['prop_good'] * df1['n_obs']\n# We multiply the values of one column by he values of another column and save the result in a new variable.\ndf1['n_bad'] = (1 - df1['prop_good']) * df1['n_obs']\ndf1","0b50311c":"df1['prop_n_good'] = df1['n_good'] \/ df1['n_good'].sum()\ndf1['prop_n_bad'] = df1['n_bad'] \/ df1['n_bad'].sum()\ndf1","258613f5":"df1['WoE'] = np.log(df1['prop_n_good'] \/ df1['prop_n_bad'])\n# We take the natural logarithm of a variable and save the result in a nex variable.\ndf1","921e8278":"df1 = df1.sort_values(['WoE'])\n# we Sort the dataframe by the values of a given column.\ndf1 = df1.reset_index(drop = True)\n# We reset the index of a dataframe and overwrite it.\ndf1","5d63f2ec":"df1['diff_prop_good'] = df1['prop_good'].diff().abs()\ndf1['diff_WoE'] = df1['WoE'].diff().abs()\ndf1","f88700f4":"# Sum all values of in the given columns.\n\ndf1['IV'] = (df1['prop_n_good'] - df1['prop_n_bad']) * df1['WoE']\ndf1['IV'] = df1['IV'].sum()\ndf1","07b13639":"# WoE function for discrete unordered variables\ndef woe_discrete(df, discrete_variabe_name, good_bad_variable_df):\n    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n    df = df.iloc[:, [0, 1, 3]]\n    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n    df['prop_n_obs'] = df['n_obs'] \/ df['n_obs'].sum()\n    df['n_good'] = df['prop_good'] * df['n_obs']\n    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n    df['prop_n_good'] = df['n_good'] \/ df['n_good'].sum()\n    df['prop_n_bad'] = df['n_bad'] \/ df['n_bad'].sum()\n    df['WoE'] = np.log(df['prop_n_good'] \/ df['prop_n_bad'])\n    df = df.sort_values(['WoE'])\n    df = df.reset_index(drop = True)\n    df['diff_prop_good'] = df['prop_good'].diff().abs()\n    df['diff_WoE'] = df['WoE'].diff().abs()\n    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n    df['IV'] = df['IV'].sum()\n    return df","38d96972":"# 'grade'\ndf_temp = woe_discrete(df_inputs_prepr, 'grade', df_targets_prepr)\n# We execute the function we defined with the necessary arguments: a dataframe, a string, and a dataframe.\n# We store the result in a dataframe.\ndf_temp","03810dbf":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# Imports the libraries we need.\nsns.set()\n# We set the default style of the graphs to the seaborn style. ","b18ff846":"# The function displays a graph.\ndef plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):\n    x = np.array(df_WoE.iloc[:, 0].apply(str))\n    # Turns the values of the column with index 0 to strings, makes an array from these strings, and passes it to variable x.\n    y = df_WoE['WoE']\n    # Selects a column with label 'WoE' and passes it to variable y.\n    plt.figure(figsize=(18, 6))\n    # Sets the graph size to width 18 x height 6.\n    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')\n    # Plots the datapoints with coordiantes variable x on the x-axis and variable y on the y-axis.\n    # Sets the marker for each datapoint to a circle, the style line between the points to dashed, and the color to black.\n    plt.xlabel(df_WoE.columns[0])\n    # Names the x-axis with the name of the column with index 0.\n    plt.ylabel('Weight of Evidence')\n    # Names the y-axis 'Weight of Evidence'.\n    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))\n    # Names the grapth 'Weight of Evidence by ' the name of the column with index 0.\n    plt.xticks(rotation = rotation_of_x_axis_labels)\n    # Rotates the labels of the x-axis a predefined number of degrees.","ffba5824":"plot_by_woe(df_temp)\n# We execute the function we defined with the necessary arguments: a dataframe.\n# We omit the number argument, which means the function will use its default value, 0.","8678ad71":"# 'home_ownership'\ndf_temp = woe_discrete(df_inputs_prepr, 'home_ownership', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","5dcccd35":"plot_by_woe(df_temp)\n# We plot the weight of evidence values.","d8c33988":"\ndf_inputs_prepr['home_ownership:RENT_OTHER_NONE_ANY'] = sum([df_inputs_prepr['home_ownership:RENT'], df_inputs_prepr['home_ownership:OTHER'],\n                                                      df_inputs_prepr['home_ownership:NONE'],df_inputs_prepr['home_ownership:ANY']])\n# 'RENT_OTHER_NONE_ANY' will be the reference category.","23449457":"# 'addr_state'\ndf_inputs_prepr['addr_state'].unique()","0dcd10ae":"df_temp = woe_discrete(df_inputs_prepr, 'addr_state', df_targets_prepr)\n# Calculating weight of evidence.\ndf_temp","2fad6b64":"plot_by_woe(df_temp)\n# We plot the weight of evidence values.","84d9e999":"if ['addr_state:ND'] in df_inputs_prepr.columns.values:\n    pass\nelse:\n    df_inputs_prepr['addr_state:ND'] = 0","aeb71edb":"plot_by_woe(df_temp.iloc[2: -2, : ])\n# plotting the weight of evidence values.","2b9df856":"plot_by_woe(df_temp.iloc[6: -6, : ])","69f318dd":"df_inputs_prepr['addr_state:ND_NE_IA_NV_FL_HI_AL'] = sum([df_inputs_prepr['addr_state:ND'], df_inputs_prepr['addr_state:NE'],\n                                              df_inputs_prepr['addr_state:IA'], df_inputs_prepr['addr_state:NV'],\n                                              df_inputs_prepr['addr_state:FL'], df_inputs_prepr['addr_state:HI'],\n                                                          df_inputs_prepr['addr_state:AL']])\n\ndf_inputs_prepr['addr_state:NM_VA'] = sum([df_inputs_prepr['addr_state:NM'], df_inputs_prepr['addr_state:VA']])\n\ndf_inputs_prepr['addr_state:OK_TN_MO_LA_MD_NC'] = sum([df_inputs_prepr['addr_state:OK'], df_inputs_prepr['addr_state:TN'],\n                                              df_inputs_prepr['addr_state:MO'], df_inputs_prepr['addr_state:LA'],\n                                              df_inputs_prepr['addr_state:MD'], df_inputs_prepr['addr_state:NC']])\n\ndf_inputs_prepr['addr_state:UT_KY_AZ_NJ'] = sum([df_inputs_prepr['addr_state:UT'], df_inputs_prepr['addr_state:KY'],\n                                              df_inputs_prepr['addr_state:AZ'], df_inputs_prepr['addr_state:NJ']])\n\ndf_inputs_prepr['addr_state:AR_MI_PA_OH_MN'] = sum([df_inputs_prepr['addr_state:AR'], df_inputs_prepr['addr_state:MI'],\n                                              df_inputs_prepr['addr_state:PA'], df_inputs_prepr['addr_state:OH'],\n                                              df_inputs_prepr['addr_state:MN']])\n\ndf_inputs_prepr['addr_state:RI_MA_DE_SD_IN'] = sum([df_inputs_prepr['addr_state:RI'], df_inputs_prepr['addr_state:MA'],\n                                              df_inputs_prepr['addr_state:DE'], df_inputs_prepr['addr_state:SD'],\n                                              df_inputs_prepr['addr_state:IN']])\n\ndf_inputs_prepr['addr_state:GA_WA_OR'] = sum([df_inputs_prepr['addr_state:GA'], df_inputs_prepr['addr_state:WA'],\n                                              df_inputs_prepr['addr_state:OR']])\n\ndf_inputs_prepr['addr_state:WI_MT'] = sum([df_inputs_prepr['addr_state:WI'], df_inputs_prepr['addr_state:MT']])\n\ndf_inputs_prepr['addr_state:IL_CT'] = sum([df_inputs_prepr['addr_state:IL'], df_inputs_prepr['addr_state:CT']])\n\ndf_inputs_prepr['addr_state:KS_SC_CO_VT_AK_MS'] = sum([df_inputs_prepr['addr_state:KS'], df_inputs_prepr['addr_state:SC'],\n                                              df_inputs_prepr['addr_state:CO'], df_inputs_prepr['addr_state:VT'],\n                                              df_inputs_prepr['addr_state:AK'], df_inputs_prepr['addr_state:MS']])\n\ndf_inputs_prepr['addr_state:WV_NH_WY_DC_ME_ID'] = sum([df_inputs_prepr['addr_state:WV'], df_inputs_prepr['addr_state:NH'],\n                                              df_inputs_prepr['addr_state:WY'], df_inputs_prepr['addr_state:DC'],\n                                              df_inputs_prepr['addr_state:ME'], df_inputs_prepr['addr_state:ID']])","6a970508":"# 'verification_status'\ndf_temp = woe_discrete(df_inputs_prepr, 'verification_status', df_targets_prepr)\n# calculating weight of evidence.\ndf_temp","d484988f":"plot_by_woe(df_temp)","376b4cb9":"# 'purpose'\ndf_temp = woe_discrete(df_inputs_prepr, 'purpose', df_targets_prepr)\ndf_temp","1123b204":"plot_by_woe(df_temp)","54e65a51":"df_inputs_prepr['purpose:educ__sm_b__wedd__ren_en__mov__house'] = sum([df_inputs_prepr['purpose:educational'], df_inputs_prepr['purpose:small_business'],\n                                                                 df_inputs_prepr['purpose:wedding'], df_inputs_prepr['purpose:renewable_energy'],\n                                                                 df_inputs_prepr['purpose:moving'], df_inputs_prepr['purpose:house']])\ndf_inputs_prepr['purpose:oth__med__vacation'] = sum([df_inputs_prepr['purpose:other'], df_inputs_prepr['purpose:medical'],\n                                             df_inputs_prepr['purpose:vacation']])\ndf_inputs_prepr['purpose:major_purch__car__home_impr'] = sum([df_inputs_prepr['purpose:major_purchase'], df_inputs_prepr['purpose:car'],\n                                                        df_inputs_prepr['purpose:home_improvement']])","4541bbfb":"# 'initial_list_status'\ndf_temp = woe_discrete(df_inputs_prepr, 'initial_list_status', df_targets_prepr)\ndf_temp","535031fd":"plot_by_woe(df_temp)","64d0654f":"# WoE function for ordered discrete and continuous variables\ndef woe_ordered_continuous(df, discrete_variabe_name, good_bad_variable_df):\n    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n    df = df.iloc[:, [0, 1, 3]]\n    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n    df['prop_n_obs'] = df['n_obs'] \/ df['n_obs'].sum()\n    df['n_good'] = df['prop_good'] * df['n_obs']\n    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n    df['prop_n_good'] = df['n_good'] \/ df['n_good'].sum()\n    df['prop_n_bad'] = df['n_bad'] \/ df['n_bad'].sum()\n    df['WoE'] = np.log(df['prop_n_good'] \/ df['prop_n_bad'])\n    #df = df.sort_values(['WoE'])\n    #df = df.reset_index(drop = True)\n    df['diff_prop_good'] = df['prop_good'].diff().abs()\n    df['diff_WoE'] = df['WoE'].diff().abs()\n    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n    df['IV'] = df['IV'].sum()\n    return df\n# Here we define a function similar to the one above, ...\n# ... with one slight difference: we order the results by the values of a different column.\n# The function takes 3 arguments: a dataframe, a string, and a dataframe. The function returns a dataframe as a result.","bd28101b":"# term\ndf_inputs_prepr['term_int'].unique()\n# There are only two unique values, 36 and 60.","8cb8cd0e":"df_temp = woe_ordered_continuous(df_inputs_prepr, 'term_int', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","aec79f23":"plot_by_woe(df_temp)\n# We plot the weight of evidence values.","ca46f80d":"# Leave as is.\n# '60' will be the reference category.\ndf_inputs_prepr['term:36'] = np.where((df_inputs_prepr['term_int'] == 36), 1, 0)\ndf_inputs_prepr['term:60'] = np.where((df_inputs_prepr['term_int'] == 60), 1, 0)","f6a2c52c":"# emp_length_int\ndf_inputs_prepr['emp_length_int'].unique()\n# Has only 11 levels: from 0 to 10. Hence, we turn it into a factor with 11 levels.","c776a2ce":"df_temp = woe_ordered_continuous(df_inputs_prepr, 'emp_length_int', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","cbe0f846":"plot_by_woe(df_temp)\n# We plot the weight of evidence values.","69a9f00f":"# We create the following categories: '0', '1', '2 - 4', '5 - 6', '7 - 9', '10'\n# '0' will be the reference category\ndf_inputs_prepr['emp_length:0'] = np.where(df_inputs_prepr['emp_length_int'].isin([0]), 1, 0)\ndf_inputs_prepr['emp_length:1'] = np.where(df_inputs_prepr['emp_length_int'].isin([1]), 1, 0)\ndf_inputs_prepr['emp_length:2-4'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(2, 5)), 1, 0)\ndf_inputs_prepr['emp_length:5-6'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(5, 7)), 1, 0)\ndf_inputs_prepr['emp_length:7-9'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(7, 10)), 1, 0)\ndf_inputs_prepr['emp_length:10'] = np.where(df_inputs_prepr['emp_length_int'].isin([10]), 1, 0)","df9df6b1":"df_inputs_prepr['mths_since_issue_d'].unique()","bb7cb10b":"df_inputs_prepr['mths_since_issue_d_factor'] = pd.cut(df_inputs_prepr['mths_since_issue_d'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.","d8ad2b03":"df_inputs_prepr['mths_since_issue_d_factor']","2b88b28d":"# mths_since_issue_d\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_issue_d_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp.head()","a8f29390":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values, rotating the labels 90 degrees.","ea969879":"df_inputs_prepr['mths_since_issue_d:<38'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(38)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:38-39'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(38, 40)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:40-41'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(40, 42)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:42-48'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(42, 49)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:49-52'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(49, 53)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:53-64'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(53, 65)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:65-84'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(65, 85)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:>84'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(85, int(df_inputs_prepr['mths_since_issue_d'].max()))), 1, 0)","6a886582":"# int_rate\ndf_inputs_prepr['int_rate_factor'] = pd.cut(df_inputs_prepr['int_rate'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.","4b4e3559":"df_temp = woe_ordered_continuous(df_inputs_prepr, 'int_rate_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","7862320f":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","3e567017":"df_inputs_prepr['int_rate:<9.548'] = np.where((df_inputs_prepr['int_rate'] <= 9.548), 1, 0)\ndf_inputs_prepr['int_rate:9.548-12.025'] = np.where((df_inputs_prepr['int_rate'] > 9.548) & (df_inputs_prepr['int_rate'] <= 12.025), 1, 0)\ndf_inputs_prepr['int_rate:12.025-15.74'] = np.where((df_inputs_prepr['int_rate'] > 12.025) & (df_inputs_prepr['int_rate'] <= 15.74), 1, 0)\ndf_inputs_prepr['int_rate:15.74-20.281'] = np.where((df_inputs_prepr['int_rate'] > 15.74) & (df_inputs_prepr['int_rate'] <= 20.281), 1, 0)\ndf_inputs_prepr['int_rate:>20.281'] = np.where((df_inputs_prepr['int_rate'] > 20.281), 1, 0)","ff67279c":"# funded_amnt\ndf_inputs_prepr['funded_amnt_factor'] = pd.cut(df_inputs_prepr['funded_amnt'], 50)\n# Here we do fine-classing again\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'funded_amnt_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","e12cfe4e":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","5ced178f":"# mths_since_earliest_cr_line\ndf_inputs_prepr['mths_since_earliest_cr_line_factor'] = pd.cut(df_inputs_prepr['mths_since_earliest_cr_line'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_earliest_cr_line_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","0db3eadb":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","c2d6f896":"df_inputs_prepr['mths_since_earliest_cr_line:<140'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:141-164'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140, 165)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:165-247'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(165, 248)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:248-270'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(248, 271)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:271-352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(271, 353)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:>352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(353, int(df_inputs_prepr['mths_since_earliest_cr_line'].max()))), 1, 0)","7c4312f7":"# delinq_2yrs\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'delinq_2yrs', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","d2e8e40b":"plot_by_woe(df_temp)","de1fdff6":"# Categories: 0, 1-3, >=4\ndf_inputs_prepr['delinq_2yrs:0'] = np.where((df_inputs_prepr['delinq_2yrs'] == 0), 1, 0)\ndf_inputs_prepr['delinq_2yrs:1-3'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 1) & (df_inputs_prepr['delinq_2yrs'] <= 3), 1, 0)\ndf_inputs_prepr['delinq_2yrs:>=4'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 9), 1, 0)","7ef6f9b1":"# inq_last_6mths\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'inq_last_6mths', df_targets_prepr)\ndf_temp.head()","4668d7a0":"plot_by_woe(df_temp)","1bad3fb3":"# Categories: 0, 1 - 2, 3 - 6, > 6\ndf_inputs_prepr['inq_last_6mths:0'] = np.where((df_inputs_prepr['inq_last_6mths'] == 0), 1, 0)\ndf_inputs_prepr['inq_last_6mths:1-2'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 1) & (df_inputs_prepr['inq_last_6mths'] <= 2), 1, 0)\ndf_inputs_prepr['inq_last_6mths:3-6'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 3) & (df_inputs_prepr['inq_last_6mths'] <= 6), 1, 0)\ndf_inputs_prepr['inq_last_6mths:>6'] = np.where((df_inputs_prepr['inq_last_6mths'] > 6), 1, 0)","24a3caea":"# open_acc\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'open_acc', df_targets_prepr)\ndf_temp.head()","be8a308e":"plot_by_woe(df_temp, 90)","382576e2":"plot_by_woe(df_temp.iloc[ : 40, :], 90)","b6123d0a":"# Categories: '0', '1-3', '4-12', '13-17', '18-22', '23-25', '26-30', '>30'\ndf_inputs_prepr['open_acc:0'] = np.where((df_inputs_prepr['open_acc'] == 0), 1, 0)\ndf_inputs_prepr['open_acc:1-3'] = np.where((df_inputs_prepr['open_acc'] >= 1) & (df_inputs_prepr['open_acc'] <= 3), 1, 0)\ndf_inputs_prepr['open_acc:4-12'] = np.where((df_inputs_prepr['open_acc'] >= 4) & (df_inputs_prepr['open_acc'] <= 12), 1, 0)\ndf_inputs_prepr['open_acc:13-17'] = np.where((df_inputs_prepr['open_acc'] >= 13) & (df_inputs_prepr['open_acc'] <= 17), 1, 0)\ndf_inputs_prepr['open_acc:18-22'] = np.where((df_inputs_prepr['open_acc'] >= 18) & (df_inputs_prepr['open_acc'] <= 22), 1, 0)\ndf_inputs_prepr['open_acc:23-25'] = np.where((df_inputs_prepr['open_acc'] >= 23) & (df_inputs_prepr['open_acc'] <= 25), 1, 0)\ndf_inputs_prepr['open_acc:26-30'] = np.where((df_inputs_prepr['open_acc'] >= 26) & (df_inputs_prepr['open_acc'] <= 30), 1, 0)\ndf_inputs_prepr['open_acc:>=31'] = np.where((df_inputs_prepr['open_acc'] >= 31), 1, 0)","40fc355f":"# pub_rec\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'pub_rec', df_targets_prepr)\ndf_temp","52f3e718":"plot_by_woe(df_temp, 90)","761331e2":"# Categories '0-2', '3-4', '>=5'\ndf_inputs_prepr['pub_rec:0-2'] = np.where((df_inputs_prepr['pub_rec'] >= 0) & (df_inputs_prepr['pub_rec'] <= 2), 1, 0)\ndf_inputs_prepr['pub_rec:3-4'] = np.where((df_inputs_prepr['pub_rec'] >= 3) & (df_inputs_prepr['pub_rec'] <= 4), 1, 0)\ndf_inputs_prepr['pub_rec:>=5'] = np.where((df_inputs_prepr['pub_rec'] >= 5), 1, 0)","df839e22":"# total_acc\ndf_inputs_prepr['total_acc_factor'] = pd.cut(df_inputs_prepr['total_acc'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'total_acc_factor', df_targets_prepr)\ndf_temp","d4a09976":"plot_by_woe(df_temp, 90)","6a3c4611":"# Categories: '<=27', '28-51', '>51'\ndf_inputs_prepr['total_acc:<=27'] = np.where((df_inputs_prepr['total_acc'] <= 27), 1, 0)\ndf_inputs_prepr['total_acc:28-51'] = np.where((df_inputs_prepr['total_acc'] >= 28) & (df_inputs_prepr['total_acc'] <= 51), 1, 0)\ndf_inputs_prepr['total_acc:>=52'] = np.where((df_inputs_prepr['total_acc'] >= 52), 1, 0)","35a8a0e1":"# acc_now_delinq\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'acc_now_delinq', df_targets_prepr)\ndf_temp","db53187f":"plot_by_woe(df_temp)","28558d70":"# Categories: '0', '>=1'\ndf_inputs_prepr['acc_now_delinq:0'] = np.where((df_inputs_prepr['acc_now_delinq'] == 0), 1, 0)\ndf_inputs_prepr['acc_now_delinq:>=1'] = np.where((df_inputs_prepr['acc_now_delinq'] >= 1), 1, 0)","e63bf911":"# total_rev_hi_lim\ndf_inputs_prepr['total_rev_hi_lim_factor'] = pd.cut(df_inputs_prepr['total_rev_hi_lim'], 2000)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 2000 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'total_rev_hi_lim_factor', df_targets_prepr)\ndf_temp","8f52de97":"plot_by_woe(df_temp.iloc[: 50, : ], 90)","1f71f4d4":"# Categories\n# '<=5K', '5K-10K', '10K-20K', '20K-30K', '30K-40K', '40K-55K', '55K-95K', '>95K'\ndf_inputs_prepr['total_rev_hi_lim:<=5K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] <= 5000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:5K-10K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 5000) & (df_inputs_prepr['total_rev_hi_lim'] <= 10000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:10K-20K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 10000) & (df_inputs_prepr['total_rev_hi_lim'] <= 20000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:20K-30K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 20000) & (df_inputs_prepr['total_rev_hi_lim'] <= 30000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:30K-40K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 30000) & (df_inputs_prepr['total_rev_hi_lim'] <= 40000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:40K-55K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 40000) & (df_inputs_prepr['total_rev_hi_lim'] <= 55000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:55K-95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 55000) & (df_inputs_prepr['total_rev_hi_lim'] <= 95000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:>95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 95000), 1, 0)","0b3613dd":"# installment\ndf_inputs_prepr['installment_factor'] = pd.cut(df_inputs_prepr['installment'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'installment_factor', df_targets_prepr)\ndf_temp","2efd7ee3":"plot_by_woe(df_temp, 90)","3ec805e2":"# annual_inc\ndf_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)\ndf_temp","5ac943d7":"df_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 100)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 100 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)\ndf_temp","7348bc9b":"# Initial examination shows that there are too few individuals with large income and too many with small income.\n# Hence, we are going to have one category for more than 150K, and we are going to apply our approach to determine\n# the categories of everyone with 140k or less.\ndf_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['annual_inc'] <= 140000, : ]\n#loan_data_temp = loan_data_temp.reset_index(drop = True)\n#df_inputs_prepr_temp","623846dd":"df_inputs_prepr_temp[\"annual_inc_factor\"] = pd.cut(df_inputs_prepr_temp['annual_inc'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'annual_inc_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n# We calculate weight of evidence.\ndf_temp","10bf9ded":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","505fe9b7":"# WoE is monotonically decreasing with income, so we split income in 10 equal categories, each with width of 15k.\ndf_inputs_prepr['annual_inc:<20K'] = np.where((df_inputs_prepr['annual_inc'] <= 20000), 1, 0)\ndf_inputs_prepr['annual_inc:20K-30K'] = np.where((df_inputs_prepr['annual_inc'] > 20000) & (df_inputs_prepr['annual_inc'] <= 30000), 1, 0)\ndf_inputs_prepr['annual_inc:30K-40K'] = np.where((df_inputs_prepr['annual_inc'] > 30000) & (df_inputs_prepr['annual_inc'] <= 40000), 1, 0)\ndf_inputs_prepr['annual_inc:40K-50K'] = np.where((df_inputs_prepr['annual_inc'] > 40000) & (df_inputs_prepr['annual_inc'] <= 50000), 1, 0)\ndf_inputs_prepr['annual_inc:50K-60K'] = np.where((df_inputs_prepr['annual_inc'] > 50000) & (df_inputs_prepr['annual_inc'] <= 60000), 1, 0)\ndf_inputs_prepr['annual_inc:60K-70K'] = np.where((df_inputs_prepr['annual_inc'] > 60000) & (df_inputs_prepr['annual_inc'] <= 70000), 1, 0)\ndf_inputs_prepr['annual_inc:70K-80K'] = np.where((df_inputs_prepr['annual_inc'] > 70000) & (df_inputs_prepr['annual_inc'] <= 80000), 1, 0)\ndf_inputs_prepr['annual_inc:80K-90K'] = np.where((df_inputs_prepr['annual_inc'] > 80000) & (df_inputs_prepr['annual_inc'] <= 90000), 1, 0)\ndf_inputs_prepr['annual_inc:90K-100K'] = np.where((df_inputs_prepr['annual_inc'] > 90000) & (df_inputs_prepr['annual_inc'] <= 100000), 1, 0)\ndf_inputs_prepr['annual_inc:100K-120K'] = np.where((df_inputs_prepr['annual_inc'] > 100000) & (df_inputs_prepr['annual_inc'] <= 120000), 1, 0)\ndf_inputs_prepr['annual_inc:120K-140K'] = np.where((df_inputs_prepr['annual_inc'] > 120000) & (df_inputs_prepr['annual_inc'] <= 140000), 1, 0)\ndf_inputs_prepr['annual_inc:>140K'] = np.where((df_inputs_prepr['annual_inc'] > 140000), 1, 0)","bad878db":"# mths_since_last_delinq\n# We have to create one category for missing values and do fine and coarse classing for the rest.\ndf_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_delinq'])]\ndf_inputs_prepr_temp['mths_since_last_delinq_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_delinq'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_delinq_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n# We calculate weight of evidence.\ndf_temp","ca232003":"plot_by_woe(df_temp, 90)","06f15c04":"# Categories: Missing, 0-3, 4-30, 31-56, >=57\ndf_inputs_prepr['mths_since_last_delinq:Missing'] = np.where((df_inputs_prepr['mths_since_last_delinq'].isnull()), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:0-3'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 0) & (df_inputs_prepr['mths_since_last_delinq'] <= 3), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:4-30'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 4) & (df_inputs_prepr['mths_since_last_delinq'] <= 30), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:31-56'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 31) & (df_inputs_prepr['mths_since_last_delinq'] <= 56), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:>=57'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 57), 1, 0)","6f2da462":"# dti\ndf_inputs_prepr['dti_factor'] = pd.cut(df_inputs_prepr['dti'], 100)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 100 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'dti_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","23c0259a":"plot_by_woe(df_temp, 90)","09e59101":"df_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['dti'] <= 35, : ]","36ea52d1":"df_inputs_prepr_temp['dti_factor'] = pd.cut(df_inputs_prepr_temp['dti'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'dti_factor', df_targets_prepr[df_inputs_prepr_temp.index])\ndf_temp","73283a39":"plot_by_woe(df_temp, 90)","538d2b4f":"# Categories:\ndf_inputs_prepr['dti:<=1.4'] = np.where((df_inputs_prepr['dti'] <= 1.4), 1, 0)\ndf_inputs_prepr['dti:1.4-3.5'] = np.where((df_inputs_prepr['dti'] > 1.4) & (df_inputs_prepr['dti'] <= 3.5), 1, 0)\ndf_inputs_prepr['dti:3.5-7.7'] = np.where((df_inputs_prepr['dti'] > 3.5) & (df_inputs_prepr['dti'] <= 7.7), 1, 0)\ndf_inputs_prepr['dti:7.7-10.5'] = np.where((df_inputs_prepr['dti'] > 7.7) & (df_inputs_prepr['dti'] <= 10.5), 1, 0)\ndf_inputs_prepr['dti:10.5-16.1'] = np.where((df_inputs_prepr['dti'] > 10.5) & (df_inputs_prepr['dti'] <= 16.1), 1, 0)\ndf_inputs_prepr['dti:16.1-20.3'] = np.where((df_inputs_prepr['dti'] > 16.1) & (df_inputs_prepr['dti'] <= 20.3), 1, 0)\ndf_inputs_prepr['dti:20.3-21.7'] = np.where((df_inputs_prepr['dti'] > 20.3) & (df_inputs_prepr['dti'] <= 21.7), 1, 0)\ndf_inputs_prepr['dti:21.7-22.4'] = np.where((df_inputs_prepr['dti'] > 21.7) & (df_inputs_prepr['dti'] <= 22.4), 1, 0)\ndf_inputs_prepr['dti:22.4-35'] = np.where((df_inputs_prepr['dti'] > 22.4) & (df_inputs_prepr['dti'] <= 35), 1, 0)\ndf_inputs_prepr['dti:>35'] = np.where((df_inputs_prepr['dti'] > 35), 1, 0)","bf02cbdb":"# mths_since_last_record\n# We have to create one category for missing values and do fine and coarse classing for the rest.\ndf_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_record'])]\n#sum(loan_data_temp['mths_since_last_record'].isnull())\ndf_inputs_prepr_temp['mths_since_last_record_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_record'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_record_factor', df_targets_prepr[df_inputs_prepr_temp.index])\ndf_temp","36078a6f":"plot_by_woe(df_temp, 90)","4100c1f5":"# Categories: 'Missing', '0-2', '3-20', '21-31', '32-80', '81-86', '>86'\ndf_inputs_prepr['mths_since_last_record:Missing'] = np.where((df_inputs_prepr['mths_since_last_record'].isnull()), 1, 0)\ndf_inputs_prepr['mths_since_last_record:0-2'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 0) & (df_inputs_prepr['mths_since_last_record'] <= 2), 1, 0)\ndf_inputs_prepr['mths_since_last_record:3-20'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 3) & (df_inputs_prepr['mths_since_last_record'] <= 20), 1, 0)\ndf_inputs_prepr['mths_since_last_record:21-31'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 21) & (df_inputs_prepr['mths_since_last_record'] <= 31), 1, 0)\ndf_inputs_prepr['mths_since_last_record:32-80'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 32) & (df_inputs_prepr['mths_since_last_record'] <= 80), 1, 0)\ndf_inputs_prepr['mths_since_last_record:81-86'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 81) & (df_inputs_prepr['mths_since_last_record'] <= 86), 1, 0)\ndf_inputs_prepr['mths_since_last_record:>86'] = np.where((df_inputs_prepr['mths_since_last_record'] > 86), 1, 0)","d48269d9":"#####\n#loan_data_inputs_train = df_inputs_prepr\n#####\nloan_data_inputs_test = df_inputs_prepr","59998344":"loan_data_inputs_train.shape","d05bd18a":"loan_data_targets_train.shape","fa737ec7":"loan_data_inputs_test.shape","9fffd7fb":"loan_data_targets_test.shape","2ac70c6f":"loan_data_inputs_train.to_csv('loan_data_inputs_train.csv')\nloan_data_targets_train.to_csv('loan_data_targets_train.csv')\nloan_data_inputs_test.to_csv('loan_data_inputs_test.csv')\nloan_data_targets_test.to_csv('loan_data_targets_test.csv')","f809f3b9":"### Data Preparation","b88a7d47":"We store the preprocessed \u2018employment length\u2019 variable in a new variable called \u2018employment length int\u2019,\nWe assign the new \u2018employment length int\u2019 to be equal to the \u2018employment length\u2019 variable with the string \u2018+ years\u2019 replaced with nothing. Next, we replace the whole string \u2018less than 1 year\u2019 with the string \u20180\u2019.\nThen, we replace the \u2018n\/a\u2019 string with the string \u20180\u2019. Then, we replace the string \u2018space years\u2019 with nothing.\nFinally, we replace the string \u2018space year\u2019 with nothing.","f9e6bdd5":"Now We create dummy variables from all 8 original independent variables, and save them into a list.\nHere we are using a particular naming convention for all variables: original variable name, colon, category name.","5c4013ce":"Similarly to income, initial examination shows that most values are lower than 200.\nHence, we are going to have one category for more than 35, and we are going to apply our approach to determine\nthe categories of everyone with 150k or less.","f1e535de":"Here we combine all of the operations above in a function.\nThe function takes 3 arguments: a dataframe, a string, and a dataframe. The function returns a dataframe as a result.","4d8bcca9":"'< 9.548', '9.548 - 12.025', '12.025 - 15.74', '15.74 - 20.281', '> 20.281'","fbf3bd47":"Now We create a new variable that has the value of '0' if a condition is met, and the value of '1' if it is not met.","54b92e37":"### Preprocessing few discrete variables","9b62ca34":"## Data preparation","7267e473":"## General Preprocessing","7537f920":"### Preprocessing Discrete Variables: Automating Calculaions","a357edfc":"The Capstone project mandates the current date as December 2017. Now we will calculate the difference between two dates in months, turn it to numeric datatype and round it and save the result in a new variable.","d0e692f6":"Here We replace a string with another string, in this case, with an empty strng (i.e. with nothing).","c8311121":"## Explore Data","ae9fc07d":"### Preprocessing Discrete Variables: Creating Dummy Variables, Part 1","89eb98f8":"Transforming the values to numeric.","c2751c1c":"Now we will do the data preparation on our train and test dataset. Since the logistic regerssion needs to have identical datasets for test and train we will need to do the data preparation for both. But the steps will remaini the same. So we will prepare the train dataset first and then we will use the same steps on the test datasets.","c4a3af1a":"In the cell below We now set the rows that had negative differences to the maximum value.","e6287e3e":"Extracting the date and the time from a string variable that is in a given format.","fb4c1281":"### Splitting Data","c09cf923":"Now we Group the data according to a criterion contained in one column. Do not turn the names of the values of the criterion as indexes and Aagregate the data in another column, using a selected function.\nIn this specific case, we group by the column with index 0 and we aggregate the values of the column with index 1.\nMore specifically, we count them.\nIn other words, we count the values in the column with index 1 for each value of the column with index 0.","d86302de":"# PD model","8579bd77":"### Check for missing values and clean","0ee8afc9":"In the cell above, We take three columns from the dataframe. Then, we display them only for the rows where a variable has negative value.\nThere are 2303 strange negative values.","5a3e72d2":"## Import Libraries","c7802ca9":"In the cell above, we can seesome descriptive statisics for the values of a column. we can notice that Dates from 1969 and before are not being converted well, i.e., they have become 2069 and similar, and negative differences are being calculated.","05a8cf79":"### Preprocessing Discrete Variables: Creating Dummy Variables, Part 2","a952e058":"## Import Data\nThe dataset contains all available data for more than 800,000 consumer loans issued from 2007 to 2015 by Lending Club: a large US peer-to-peer lending company. There are several different versions of this dataset. I have used a version available on kaggle.com You can find it here: https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data\/version\/1\nI divided the data into two periods because we assume that some data are available at the moment when i will be needed to build Expected Loss models, and some data comes from applications after. Later, I will investigate whether the applications, after I built the Probability of Default (PD) model have similar characteristics with the applications I used to build the PD model.","ef8a9084":"### Preprocessing Continuous Variables: Creating Dummy Variables, Part 1","15e11c5d":"### Preprocessing Discrete Variables: Visualizing Results","c712da01":"Now We combine 'educational', 'small_business', 'wedding', 'renewable_energy', 'moving', 'house' in one category: 'educ__sm_b__wedd__ren_en__mov__house'.\n'other', 'medical', 'vacation' in one category: 'oth__med__vacation'\n'major_purchase', 'car', 'home_improvement' in one category: 'major_purch__car__home_impr'\nWe leave 'debt_consolidtion' in a separate category.\n'credit_card' in a separate category 'educ__sm_b__wedd__ren_en__mov__house' will be the reference category.","01ca6679":"Setting the the display of all the columns in the dataframe","5ba57712":"Now We split two dataframes with inputs and targets, each into a train and test dataframe, and store them in variables.\nWe have set the size of the test dataset to be 20%.\nRespectively, the size of the train dataset becomes 80%.\nWe also set a specific random state.\nThis would allow us to perform the exact same split multimple times.\nThis means, to assign the exact same observations to the train and test datasets.","a0068e92":"Below we define a function that takes 2 arguments: a dataframe and a number.\nThe number parameter has a default value of 0.\nThis means that if we call the function and omit the number parameter, it will be executed with it having a value of 0.","772769ca":"### Preprocessing Continuous Variables: Automating Calculations and Visualizing Results","d4f0fcb5":"## Storing the relevant files","2a6f7ead":"# Term Variable","f1e60dfa":"Displaying unique values of a column.","16750886":"### Preprocessing Continuous Variables: Creating Dummy Variables, Part 2","5e85d30f":"In the cell below We replace a string from a variable with another string, in this case, with an empty strng (i.e. with nothing). We turn the result to numeric datatype and save it in another variable.","10ca7bcd":"Lets Check the Data frame with their heads and tails","5d6fdb17":"# Data Preparation","1d8e21d1":"Now that we have preprocessed the train dataset, we will now, move ahead with preprocessing the test dataset. So we will hash out the cells above and remove the hash from the cells below. As well as in the end, we will hashout the loan_data_inputs_test and store the result in the loan_data_targets_test","f8a5f997":"There are many categories with very few observations and many categories with very different \"good\" %.\nTherefore, we create a new discrete variable where we combine some of the categories.\n'OTHERS' and 'NONE' are riskiest but are very few. 'RENT' is the next riskiest.\n'ANY' are least risky but are too few. Conceptually, they belong to the same category. Also, their inclusion would not change anything.\nWe combine them in one category, 'RENT_OTHER_NONE_ANY'.\nWe end up with 3 categories: 'RENT_OTHER_NONE_ANY', 'OWN', 'MORTGAGE'.","2bbfcaea":"Creating dummy variables from a variable.","4f48a894":"In the cell below we Extract the date and the time from a string variable that is in a given format. Then We calculate the difference between two dates in months, turn it to numeric datatype and round it and save the result in a new variable.\n","606edfa8":"'Total revolving high credit\/ credit limit', so it makes sense that the missing values are equal to funded_amnt. We fill the missing values with the values of another variable.","3f17b9fa":"finding the data type and then Calculating the difference between two dates and times.","f4a291a8":"Checking the datatype of a single element of a column.","8e042473":"Now We create the following categories: < 38, 38 - 39, 40 - 41, 42 - 48, 49 - 52, 53 - 64, 65 - 84, > 84.","1229d3ec":"### Preprocessing few continuous variables","e34d60ad":"### Preprocessing Continuous Variables: Creating Dummy Variables, Part 3","6ac4b0b4":"Displays all column names.","185e287d":"We create the following categories: < 140, 141 - 164, 165 - 247, 248 - 270, 271 - 352, > 352","01f0dd52":"We take the difference between two subsequent values of a column. Then, we take the absolute value of the result.","a1e8c8b0":"### Data Preparation: Continuous Variables, Part 1 and 2: Homework","6990049e":"Checking the datatype of a single element of a column.","9eeff3b3":"#### We are splitting the data first and then preprocessing the dataset. This is to simulate the real life datasets where we often have separate datasets for test and train.","f4be63c0":"We create the following categories:\n'ND' 'NE' 'IA' NV' 'FL' 'HI' 'AL'\n'NM' 'VA'\n'NY'\n'OK' 'TN' 'MO' 'LA' 'MD' 'NC'\n'CA'\n'UT' 'KY' 'AZ' 'NJ'\n'AR' 'MI' 'PA' 'OH' 'MN'\n'RI' 'MA' 'DE' 'SD' 'IN'\n'GA' 'WA' 'OR'\n'WI' 'MT'\n'TX'\n'IL' 'CT'\n'KS' 'SC' 'CO' 'VT' 'AK' 'MS'\n'WV' 'NH' 'WY' 'DC' 'ME' 'ID'\n'IA_NV_HI_ID_AL_FL' will be the reference category.","22b2e084":"### Dependent Variable. Good\/ Bad (Default) Definition. Default and Non-default Accounts.","a0e5fd34":"We are going to preprocess the following discrete variables: grade, sub_grade, home_ownership, verification_status, loan_status, purpose, addr_state, initial_list_status. Most likely, we are not going to use sub_grade, as it overlaps with grade."}}