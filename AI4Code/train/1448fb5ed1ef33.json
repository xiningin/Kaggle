{"cell_type":{"f9c7a59e":"code","4baca8de":"code","ada26823":"code","e7df6a1b":"code","1618da04":"code","80e9a84a":"code","f095cf09":"code","a824fad1":"code","4e7c7566":"code","a4d154f6":"code","f9499a71":"code","2476f649":"code","521142ee":"code","95b089dc":"code","20cf6a2e":"code","56d6e80d":"code","1d88326d":"code","f994c359":"code","6e3d9662":"code","ee3dc348":"code","ffb8987f":"code","cb355ccb":"code","f87ac07e":"code","c35c3434":"code","5d085e34":"code","80f34f24":"code","f9aeb222":"code","6fe0cc44":"code","be391650":"code","1d37d0a4":"code","20fb7c3c":"markdown","7b06a43e":"markdown","b248f5e4":"markdown","6e1849a7":"markdown","0659ea19":"markdown","7c1bffd8":"markdown","0634acda":"markdown","7e8923dc":"markdown","4f61b8a8":"markdown","80d39a07":"markdown","00996361":"markdown","b0336d4a":"markdown","b93122c5":"markdown","05e9a4ee":"markdown","de501bd9":"markdown","3c7829ff":"markdown","07a43980":"markdown","e35cc5b3":"markdown","1cc877b2":"markdown","6d806dee":"markdown","1c41b306":"markdown","89dc50ac":"markdown","9e2dc7d5":"markdown","de0dd1cb":"markdown","d4c0e084":"markdown","1875c667":"markdown","41828939":"markdown","2cd68ba5":"markdown","df3bf630":"markdown","25e3a260":"markdown","dd1213d8":"markdown","71609a2e":"markdown","9a9bafb8":"markdown","603d0f34":"markdown"},"source":{"f9c7a59e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport zipfile\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report \nimport tensorflow as tf\nfrom keras.preprocessing.image import load_img, ImageDataGenerator\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, MaxPool2D, Flatten, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Input\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.applications.vgg16 import VGG16, preprocess_input\n\nbatch_size = 128","4baca8de":"seed = 666\ntf.random.set_seed(seed)\nnp.random.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)                      \nrandom.seed(666)","ada26823":"os.listdir(\"..\/input\/dogs-vs-cats\/\")","e7df6a1b":"TRAIN_PATH = \"..\/input\/dogs-vs-cats\/train.zip\"\nTEST_PATH = \"..\/input\/dogs-vs-cats\/test1.zip\"\n\nFILES = \"\/kaggle\/files\/unzipped\/\"\n\nwith zipfile.ZipFile(TRAIN_PATH, 'r') as zipp:\n    zipp.extractall(FILES)\n    \nwith zipfile.ZipFile(TEST_PATH, 'r') as zipp:\n    zipp.extractall(FILES)","1618da04":"train_df = pd.DataFrame({\"file\": os.listdir(\"\/kaggle\/files\/unzipped\/train\")})\ntrain_df[\"label\"] = train_df[\"file\"].apply(lambda x: x.split(\".\")[0])\n\ntrain_df","80e9a84a":"test_df = pd.DataFrame({\"file\": os.listdir(\"\/kaggle\/files\/unzipped\/test1\")})\n\ntest_df.head()","f095cf09":"fig, ax = plt.subplots(figsize = (6, 6), facecolor = \"#e5e5e5\")\nax.set_facecolor(\"#e5e5e5\")\n\nsns.countplot(x = \"label\", data = train_df, ax = ax)\n\nax.set_title(\"Distribution of Class Labels\")\nsns.despine()\nplt.show()","a824fad1":"fig = plt.figure(1, figsize = (8, 8))\nfig.suptitle(\"Training Set Images (Sample)\")\n\nfor i in range(25):\n\n    plt.subplot(5, 5, i + 1)\n    image = load_img(FILES + \"train\/\" + train_df[\"file\"][i])\n    plt.imshow(image)\n    plt.axis(\"off\")\n    \nplt.tight_layout()\nplt.show()","4e7c7566":"fig = plt.figure(1, figsize = (8, 8))\nfig.suptitle(\"Sample Dog images from Training Set\")\n\nfor i in range(25):\n    \n    plt.subplot(5, 5, i + 1)\n    image = load_img(FILES + \"train\/\" + train_df.query(\"label == 'dog'\").file.values[i])\n    plt.imshow(image)\n    plt.axis(\"off\")\n    \nplt.tight_layout()\nplt.show()","a4d154f6":"fig = plt.figure(1, figsize = (8, 8))\nfig.suptitle(\"Sample Cat images from Training Set\")\n\nfor i in range(25):\n    \n    plt.subplot(5, 5, i + 1)\n    image = load_img(FILES + \"train\/\" + train_df.query(\"label == 'cat'\").file.values[i])\n    plt.imshow(image)\n    plt.axis(\"off\")\n    \nplt.tight_layout()\nplt.show()","f9499a71":"def VGG_16(input_shape = (224, 224, 3), n_classes = 1000):\n    \n    model = Sequential(\n        [\n            Conv2D(filters = 64, kernel_size = (3, 3), padding = \"same\", activation = \"relu\", input_shape = input_shape),\n            Conv2D(filters = 64, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            MaxPool2D(pool_size = (2, 2), strides = (2, 2)),\n            \n            Conv2D(filters = 128, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            Conv2D(filters = 128, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            MaxPool2D(pool_size = (2, 2), strides = (2, 2)),\n            \n            Conv2D(filters = 256, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            Conv2D(filters = 256, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            Conv2D(filters = 256, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            MaxPool2D(pool_size = (2, 2), strides = (2, 2)),\n            \n            Conv2D(filters = 512, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            Conv2D(filters = 512, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            Conv2D(filters = 512, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            MaxPool2D(pool_size = (2, 2), strides = (2, 2)),\n            \n            Conv2D(filters = 512, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            Conv2D(filters = 512, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            Conv2D(filters = 512, kernel_size = (3, 3), padding = \"same\", activation = \"relu\"),\n            MaxPool2D(pool_size = (2, 2), strides = (2, 2)),\n            \n            Flatten(),\n            Dense(units = 4096, activation = \"relu\"),\n            Dense(units = 4096, activation = \"relu\"),\n            Dense(units = n_classes, activation = \"softmax\")\n        ]\n    )\n    \n    return model","2476f649":"train_data, val_data = train_test_split(train_df, \n                                        test_size = 0.2, \n                                        stratify = train_df[\"label\"], \n                                        random_state = 666)","521142ee":"datagen = ImageDataGenerator(\n    rotation_range = 30, \n    width_shift_range = 0.1,\n    height_shift_range = 0.1, \n    brightness_range = (0.5, 1), \n    zoom_range = 0.2,\n    horizontal_flip = True, \n    rescale = 1.\/255,\n)\n\nsample_df = train_data.sample(1)\n\nsample_generator = datagen.flow_from_dataframe(\n    dataframe = sample_df,\n    directory = FILES + \"train\/\",\n    x_col = \"file\",\n    y_col = \"label\",\n    class_mode = \"categorical\",\n    target_size = (224, 224),\n    seed = 666\n)\n\nplt.figure(figsize = (14, 8))\n\nfor i in range(50):\n    \n    plt.subplot(5, 10, i + 1)\n    \n    for X, y in sample_generator:\n\n        plt.imshow(X[0])\n        plt.axis(\"off\")\n        break\n        \nplt.tight_layout()\nplt.show()","95b089dc":"train_datagen = ImageDataGenerator(\n    rotation_range = 15, \n#     width_shift_range = 0.1,\n#     height_shift_range = 0.1, \n#     brightness_range = (0.5, 1), \n#     zoom_range = 0.1,\n    horizontal_flip = True,\n    preprocessing_function = preprocess_input\n)\n\nval_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)","20cf6a2e":"train_generator = train_datagen.flow_from_dataframe(\n    dataframe = train_data,\n    directory = FILES + \"train\/\",\n    x_col = \"file\",\n    y_col = \"label\",\n    class_mode = \"categorical\",\n    target_size = (224, 224),\n    batch_size = batch_size,\n    seed = 666,\n)\n\nval_generator = val_datagen.flow_from_dataframe(\n    dataframe = val_data,\n    directory = FILES + \"train\/\",\n    x_col = \"file\",\n    y_col = \"label\",\n    class_mode = \"categorical\",\n    target_size = (224, 224),\n    batch_size = batch_size,\n    seed = 666,\n    shuffle = False\n)","56d6e80d":"base_model = VGG16(\n    weights = \"imagenet\", \n    input_shape = (224, 224, 3),\n    include_top = False\n)\n\n\nfor layers in base_model.layers:\n    layers.trainable = False\n\n\ndef vgg16_pretrained():\n    \n    model = Sequential(\n        [\n            base_model,\n            GlobalAveragePooling2D(),\n            Dense(100, activation = \"relu\"),\n            Dropout(0.4),\n            Dense(64, activation = \"relu\"),\n            Dense(2, activation = \"softmax\")\n        ]\n    )\n    \n    return model\n\ntf.keras.backend.clear_session()","1d88326d":"model = vgg16_pretrained()\n\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel.summary()","f994c359":"reduce_lr = ReduceLROnPlateau(\n    monitor = \"val_accuracy\", \n    patience = 2,\n    verbose = 1, \n    factor = 0.5, \n    min_lr = 0.000000001\n)\n\nearly_stopping = EarlyStopping(\n    monitor = \"val_accuracy\",\n    patience = 5,\n    verbose = 1,\n    mode = \"max\",\n)\n\ncheckpoint = ModelCheckpoint(\n    monitor = \"val_accuracy\",\n    filepath = \"catdog_vgg16_.{epoch:02d}-{val_accuracy:.6f}.hdf5\",\n    verbose = 1,\n    save_best_only = True, \n    save_weights_only = True\n)","6e3d9662":"history = model.fit(\n    train_generator,\n    epochs = 10, \n    validation_data = val_generator,\n    validation_steps = val_data.shape[0] \/\/ batch_size,\n    steps_per_epoch = train_data.shape[0] \/\/ batch_size,\n    callbacks = [reduce_lr, early_stopping, checkpoint]\n)","ee3dc348":"tf.keras.backend.clear_session()\n\nmodel = vgg16_pretrained()\n\nmodel.load_weights(\".\/catdog_vgg16_.10-0.983774.hdf5\")","ffb8987f":"fig, axes = plt.subplots(1, 2, figsize = (12, 4))\n\nsns.lineplot(x = range(len(history.history[\"loss\"])), y = history.history[\"loss\"], ax = axes[0], label = \"Training Loss\")\nsns.lineplot(x = range(len(history.history[\"loss\"])), y = history.history[\"val_loss\"], ax = axes[0], label = \"Validation Loss\")\n\nsns.lineplot(x = range(len(history.history[\"accuracy\"])), y = history.history[\"accuracy\"], ax = axes[1], label = \"Training Accuracy\")\nsns.lineplot(x = range(len(history.history[\"accuracy\"])), y = history.history[\"val_accuracy\"], ax = axes[1], label = \"Validation Accuracy\")\naxes[0].set_title(\"Loss\"); axes[1].set_title(\"Accuracy\")\n\nsns.despine()\nplt.show()","cb355ccb":"val_pred = model.predict(val_generator, steps = np.ceil(val_data.shape[0] \/ batch_size))\nval_data.loc[:, \"val_pred\"] = np.argmax(val_pred, axis = 1)\n\nlabels = dict((v, k) for k, v in val_generator.class_indices.items())\n\nval_data.loc[:, \"val_pred\"] = val_data.loc[:, \"val_pred\"].map(labels)","f87ac07e":"labels","c35c3434":"fig, ax = plt.subplots(figsize = (9, 6))\n\ncm = confusion_matrix(val_data[\"label\"], val_data[\"val_pred\"])\n\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"cat\", \"dog\"])\ndisp.plot(cmap = plt.cm.Blues, ax = ax)\n\nax.set_title(\"Validation Set\")\nplt.show()","5d085e34":"print(classification_report(val_data[\"label\"], val_data[\"val_pred\"]))","80f34f24":"val_errors = val_data[(val_data.label) != (val_data.val_pred)].reset_index(drop = True)\nval_errors","f9aeb222":"fig = plt.figure(1, figsize = (24, 20))\n\nfor i in range(81):\n    \n    plt.subplot(9, 9, i + 1)\n    image = load_img(\"\/kaggle\/files\/unzipped\/train\/\" + val_errors.file[i])\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.title(f\"True Value: {val_errors['label'][i]} \\nPrediction: {val_errors['val_pred'][i]}\")    \n    \nplt.tight_layout()\nplt.show()","6fe0cc44":"test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe = test_df,\n    directory = FILES + \"test1\/\",\n    x_col = \"file\",\n    y_col = None,\n    class_mode = None,\n    target_size = (224, 224),\n    batch_size = batch_size,\n    seed = 666,\n    shuffle = False\n)","be391650":"test_preds = model.predict(test_generator, steps = np.ceil(test_df.shape[0] \/ batch_size))\n\ntest_df[\"test_preds\"] = np.argmax(test_preds, axis = 1)\nlabels = dict((v,k) for k,v in train_generator.class_indices.items())\n\ntest_df['test_preds'] = test_df['test_preds'].map(labels)","1d37d0a4":"sample_test = test_df.sample(64).reset_index(drop = True)\n\nfig = plt.figure(1, figsize = (24, 20))\nfig.suptitle(\"Sample Predictions\")\n\nfor i in range(len(sample_test)):\n    \n    plt.subplot(8, 8, i + 1)\n    image = load_img(\"\/kaggle\/files\/unzipped\/test1\/\" + sample_test.file[i])\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.title(f\"Predicted as {sample_test['test_preds'][i]}\")\n    \nplt.tight_layout()\nplt.show()","20fb7c3c":"[take me to the top](#section-top)","7b06a43e":"<a id=\"section-four\"><\/a>\n\n# 4) Data Preparation & Augmentation","b248f5e4":"I just set simple options for generation. More options increase training time and we probably wait more until model converge.","6e1849a7":"https:\/\/towardsdatascience.com\/illustrated-10-cnn-architectures-95d78ace614d#c5a6\n\nhttps:\/\/keras.io\/guides\/transfer_learning\/\n\nhttps:\/\/paperswithcode.com\/sota\/image-classification-on-imagenet\n\nhttps:\/\/viso.ai\/deep-learning\/vgg-very-deep-convolutional-networks\/\n\nhttps:\/\/medium.com\/mini-distill\/effect-of-batch-size-on-training-dynamics-21c14f7a716e\n\nhttps:\/\/ai.stackexchange.com\/a\/4413\n\nhttps:\/\/www.kaggle.com\/rajmehra03\/a-comprehensive-guide-to-transfer-learning\n\nhttps:\/\/www.kaggle.com\/dansbecker\/transfer-learning\n\n\nGood notebooks about this dataset\n\nhttps:\/\/www.kaggle.com\/uysimty\/keras-cnn-dog-or-cat-classification\n\nhttps:\/\/www.kaggle.com\/bhuvanchennoju\/hey-siri-is-it-a-or-class-f1-0-992\n\n\nYou can also look at\n\nhttps:\/\/www.kaggle.com\/mustafacicek\/mnist-cnn-data-augmentation","0659ea19":"<a id=\"section-six\"><\/a>\n\n# 6) Interpreting Results and Error Analysis","7c1bffd8":"<a id=\"section-five1\"><\/a>\n## Build Model","0634acda":"[take me to the top](#section-top)","7e8923dc":"<a id=\"section-three\"><\/a>\n\n# 3) VGG-16","4f61b8a8":"[take me to the top](#section-top)","80d39a07":"<a id=\"section-read\"><\/a>\n\n# Readings, Resources","00996361":"<a id=\"section-one\"><\/a>\n\n# 1) Unzip Datasets","b0336d4a":"<a id=\"section-two\"><\/a>\n\n# 2) Sample Images","b93122c5":"<a id=\"section-six3\"><\/a>\n## Error Analysis","05e9a4ee":"<a id=\"section-intro\"><\/a>\n\n# Introduction\n\nIn this notebook, my main goal is;\n\n- Implementing VGG-16 model from scratch with using Keras\n\n- Transfer Learning with pre-trained VGG-16 model","de501bd9":"<a id=\"section-conc\"><\/a>\n# Conclusion","3c7829ff":"[take me to the top](#section-top)","07a43980":"<a id=\"section-five2\"><\/a>\n\n## Callbacks","e35cc5b3":"VGG-16 has nearly 138M parameters. It is more than EfficientNet, ResNext, etc.\n\nIt also has 75% accuracy on ImageNet data, that is a poor result wrt other architectures.","1cc877b2":"<a id=\"section-three1\"><\/a>\n\n## Paper \n\n> In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n> \n\nhttps:\/\/arxiv.org\/abs\/1409.1556","6d806dee":"There are lots of architectures about image classification and you can easily reach their trained parameters. You can use that parameters for transfer learning and fine tuning. You don't need to know an algorithm's architecture, but having knowledge about architectures and reading its papers probably gives you an advantage.\n\n\nAlso, I saw lots of notebook about this topic with a few upvotes and much more forks. Please upvote notebooks if you find it useful.","1c41b306":"<a id=\"section-three3\"><\/a>\n\n## Keras Implementation","89dc50ac":"[take me to the top](#section-top)","9e2dc7d5":"[take me to the top](#section-top)","de0dd1cb":"<a id=\"section-six1\"><\/a>\n\n## Learning Curve ","d4c0e084":"<a id=\"section-three2\"><\/a>\n\n## Architecture\n\n![](https:\/\/miro.medium.com\/max\/2000\/1*_vGloND6yyxFeFH5UyCDVg.png)\n\n> *https:\/\/towardsdatascience.com\/illustrated-10-cnn-architectures-95d78ace614d","1875c667":"<a id=\"section-four1\"><\/a>\n\n## Sample Augmentation","41828939":"<a id=\"section-five\"><\/a>\n\n# 5) Pre-Trained VGG-16 Model & Transfer Learning","2cd68ba5":"<a id=\"section-top\"><\/a>\n# Table of Contents\n* [Introduction](#section-intro)\n* [1) Unzip Datasets](#section-one)\n* [2) Sample Images](#section-two)\n\n\n* [3) VGG-16](#section-three)\n    * [Paper](#section-three1)\n    * [Architecture](#section-three2)\n    * [Keras Implementation](#section-three3)\n    \n\n* [4) Data Preparation & Augmentation](#section-four)\n    * [Sample Augmentation](#section-four1)\n    \n\n* [5) Pre-Trained VGG-16 Model & Transfer Learning](#section-five)\n    * [Build Model](#section-five1)\n    * [Callbacks](#section-five2)\n    * [Fit](#section-five3)\n    \n\n* [6) Interpreting Results and Error Analysis](#section-six)\n    * [Learning Curve](#section-six1)\n    * [Confusion Matrix & Classification Report](#section-six2)\n    * [Error Analysis](#section-six3)\n\n\n* [7) Predict Test Set](#section-seven)\n\n\n* [Conclusion](#section-conc)\n* [Readings, Resources](#section-read)","df3bf630":"[take me to the top](#section-top)","25e3a260":"[take me to the top](#section-top)","dd1213d8":"<a id=\"section-five3\"><\/a>\n\n## Fit","71609a2e":"<a id=\"section-six2\"><\/a>\n## Confusion Matrix & Classification Report","9a9bafb8":"I didn't use excessive data augmentation or I didn't set large epochs for concerning time. In this notebook, score is not main goal.","603d0f34":"<a id=\"section-seven\"><\/a>\n# 7) Predict Test Set"}}