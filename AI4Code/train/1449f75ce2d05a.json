{"cell_type":{"274780b3":"code","ec2e3625":"code","308f6712":"code","18faf2f8":"code","07bc99cd":"code","e9e1ef1b":"code","3d217145":"code","7b8b388e":"code","f887c5e4":"code","baffecfb":"code","c5259253":"code","517c628d":"code","c7294b6b":"code","e57be9d7":"code","25f0de28":"code","576eac05":"code","b684be59":"code","8af801f8":"code","983b76fc":"code","2d19de3c":"code","2710a3b9":"code","479bb53b":"code","562a4d4d":"code","26acc887":"code","d7a90d34":"code","c55645d2":"code","bbcebd43":"code","fc7f5a67":"code","91396482":"code","90e28697":"code","4df3e5c5":"code","92fc751b":"markdown","ea75bf4d":"markdown","7efebeab":"markdown","6e8a716e":"markdown","1fe4f85c":"markdown","f0274582":"markdown","a9be0606":"markdown","849b6368":"markdown","e8e345cf":"markdown","ad4fc0c3":"markdown","335fa70a":"markdown","b7866df6":"markdown","7fd2dec6":"markdown","98d62626":"markdown","ffd8608e":"markdown","ae52d771":"markdown","7065dcb1":"markdown","d7e9341d":"markdown","492ef953":"markdown","211fe21f":"markdown"},"source":{"274780b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nplt.style.available\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec2e3625":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_PassengerId = test_df[\"PassengerId\"]\n\n#used for split the data\ntrain_df_len = len(train_df) \n#merge train and test data\ndf = pd.concat([train_df,test_df],axis = 0).reset_index(drop = True)\n\nprint(train_df.shape,test_df.shape,test_PassengerId.shape, df.shape)","308f6712":"df.head()","18faf2f8":"df.info()","07bc99cd":"from pandas_profiling import ProfileReport\n# Generate the Profiling Report\nprofile = ProfileReport(\n    df, title=\"Titanic Dataset\", html={\"style\": {\"full_width\": True}}, sort=\"None\"\n)\n\n# The Notebook Widgets Interface\nprofile.to_widgets()\n# Or use the HTML report in an iframe\n#profile","e9e1ef1b":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","3d217145":"# drop outliers\ndf = df.drop(detect_outliers(df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]),axis = 0).reset_index(drop = True)\ndf.shape","7b8b388e":"#df.columns[train_df.isnull().any()]\ndf.isnull().sum()","f887c5e4":"## missing age values were filled in with the median value\ndf[\"Age\"]=df[\"Age\"].fillna(df[\"Age\"].median())\n# #for another missing values\n# df.fillna(axis=0,method=\"ffill\", inplace=True)","baffecfb":"df[df[\"Fare\"].isnull()]","c5259253":"df[\"Fare\"] = df[\"Fare\"].fillna(np.mean(df[df[\"Embarked\"] == \"S\"][\"Fare\"]))","517c628d":"df[\"Embarked\"] = df[\"Embarked\"].fillna(method = 'ffill')","c7294b6b":"df.isnull().sum()","e57be9d7":"df=df.drop(['PassengerId', 'Name','Ticket', 'Cabin'], axis=1)\ndf.shape","25f0de28":"df[\"Sex\"] = df[\"Sex\"].astype(\"category\")\ndf = pd.get_dummies(df, columns=[\"Sex\"],drop_first=True)\n\ndf[\"Embarked\"] = df[\"Embarked\"].astype(\"category\")\ndf = pd.get_dummies(df, columns=[\"Embarked\"])\ndf.info()","576eac05":"import statsmodels.api as sm\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error,r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\n\nimport xgboost\nfrom xgboost import XGBRFClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","b684be59":"df.shape","8af801f8":"train = df[:len(train_df)]\ntest =  df[len(train_df):]\n\n#this is for test predict\ntest.drop(labels = [\"Survived\"],axis = 1, inplace = True)\n\nprint(\"train: \", len(train),\"\\t test :\",len(test))","983b76fc":"X_train = train.drop(labels = \"Survived\", axis = 1)\ny_train = train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.33, random_state = 42)\nprint(\"X_train\",len(X_train))\nprint(\"X_test\",len(X_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))\n","2d19de3c":"from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test= scaler.transform(X_test)\npd.DataFrame(X_train)","2710a3b9":"y_train = y_train.astype('int')\ny_test = y_test.astype('int')\npd.DataFrame(y_test).head(3)","479bb53b":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier(),\n             GradientBoostingClassifier(random_state = random_state),\n             MLPClassifier(random_state = random_state),\n             XGBRFClassifier(random_state = random_state),\n             LGBMClassifier(random_state = random_state),\n             CatBoostClassifier(random_state = random_state)]","562a4d4d":"dt_params = {\"min_samples_split\" : range(1,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_params = {\"kernel\" : [\"linear\",\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,5,10,30,70,100,200,500]}\n\nrf_params = {\"max_features\" : [1,3,5,7,10],\n              \"n_estimators\" : [100, 250,500,1000], \n              \"min_samples_split\": [2,5,10,20,50],\n                 \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"criterion\":[\"gini\"]}\n\nlogreg_params = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_params = {\"n_neighbors\": np.linspace(1,50,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\ngbc_params = { \"learning_rate\":[0.01,0.001,0.1,0.05],\n                  \"max_depth\": [2,3,5,8],\n                  \"n_estimators\": [100,300,500,1000]}\n\nmlp_params = {\"alpha\":[1,2,3,0.1,0.01,0.03,0.005,0.002,0.0001],\n                  \"hidden_layer_sizes\": [(10,20),(5,5),(100,100,100),(100,100),(3,5)],\n              \"solver\":[\"lbfgs\",\"adam\"]}\n\nxgb_params = { \"learning_rate\":[0.01,0.1,0.001], \n              \"max_depth\": [3,5,7],        \n              \"n_estimators\": [100,500,100],\n              \"subsample\": [0.6,0.8,1]}  \n\nlgbm_params = {\"learning_rate\" : [0.01, 0.1, 0.001],\n               \"n_estimators\" : [200,500,100],\n               \"max_depth\" : [1,2,35,8]}\n\ncatb_params = {\"learning_rate\" : [0.01, 0.03,0.1],\n               \"iterations\" : [1000,200,500],  \n               \"depth\" : [4,5,8]}","26acc887":"classifier_params = [dt_params,\n                   svc_params,\n                   rf_params,\n                   logreg_params,\n                   knn_params,\n                   gbc_params,\n                   mlp_params,\n                   xgb_params,\n                   lgbm_params,\n                   catb_params]","d7a90d34":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_params[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 2)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(\"\\n\",classifier[i])\n    print(cv_result[i],\"\\n\")","c55645d2":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \n                           \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n                                        \"LogisticRegression\",\"KNeighborsClassifier\",\n                                        \"GradientBoostingClassifier\",\"MLPClassifier\",\n                                        \"XGBRFClassifier\",\"LGBMClassifier\",\"CatBoostClassifier\"]})\n\nresults = cv_results.sort_values(['Cross Validation Means']).reset_index(drop=True)\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\");\n\nplt.figure(figsize=(12,6))\nplt.grid(b=True, which='both', color='#999999', linestyle='-', alpha=0.2)\nplt.minorticks_on()","bbcebd43":"cv_results ","fc7f5a67":"# we have t use for using the VotingClassifier ing : we only adding : probability=True\nbest_estimators[1] = SVC(C=200, gamma=0.01, probability=True,random_state=42)\nbest_estimators[1]","91396482":"votingC = VotingClassifier(estimators = [(\"svc\",best_estimators[1]),\n                                         (\"rfc\",best_estimators[2]),\n                                        (\"gbc\",best_estimators[5]),\n                                         (\"mlp\",best_estimators[6]),\n                                        (\"lgb\",best_estimators[8]),\n                                        (\"catb\",best_estimators[9])], \n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(X_train, y_train)\nprint(accuracy_score(votingC.predict(X_test),y_test))\n\n","90e28697":"# before that we have to convert test data to standard scaler\ntest_scaler = scaler.transform(test.values)\ntest = pd.DataFrame(test_scaler, index=test.index, columns=test.columns)","4df3e5c5":"test_survived = pd.Series(votingC.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","92fc751b":"\n### Cleaning Unused Columns\n\nWe can not use some columns for training and predicting models. Because the features does'nt have any information about the dependent variable.\n","ea75bf4d":"<a id = \"4\"><\/a><br>\n# Outlier Detection","7efebeab":"<a id = \"10\"><\/a><br>\n### Fill Missing Value\n","6e8a716e":"<a id = \"7\"><\/a><br>\n# Modeling","1fe4f85c":"# Introduction\nThe sinking of Titanic is one of the most notorious shipwrecks in the history. In 1912, during her voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.\n\n<font color = 'blue'>\nContent: \n\n1. [Load and Check Data](#1)\n1. [Variable Description](#2)\n1. [Basic Data Analysis with \"pandas_profiling\"](#3)    \n1. [Outlier Detection](#4)\n1. [Missing Value](#5)\n1. [Feature Engineering](#6)\n1. [Modeling](#7)\n    * [Train - Test Split](#8)\n    * [Hyperparameter Tuning -- Grid Search -- Cross Validation](#9) \n    * [Ensemble Modeling](#10)\n    * [Prediction and Submission](#11)","f0274582":"#### Filling missing Embarked feature with use forward fill method to fill out the missing values.","a9be0606":"<a id = \"6\"><\/a><br>\n# Feature Engineering\n* Cleaning Unused Features\n* Convert categorical variable into dummy variables(Features)","849b6368":"#### Filling missing Age feature values with the median Age variables","e8e345cf":"<a id = \"10\"><\/a><br>\n## Ensemble Modeling","ad4fc0c3":"<a id = \"11\"><\/a><br>\n## Prediction and Submission","335fa70a":"### Convert categorical variable into dummy variables","b7866df6":"<b>New train and test data sets<\/b>","7fd2dec6":"<a id = \"8\"><\/a><br>\n## Train - Test Split","98d62626":"<a id = \"1\"><\/a><br>\n# Load and Check Data","ffd8608e":"<a id = \"3\"><\/a><br>\n# Basic Data Analysis with \"pandas_profiling\"","ae52d771":"#### Filling missing Fare feature values with the equal Embarked class mean","7065dcb1":"<a id = \"2\"><\/a><br>\n# Variable Description\n1. PassengerId: unique id number to each passenger\n1. Survived: passenger survive(1) or died(0)\n1. Pclass: passenger class\n1. Name: name\n1. Sex: gender of passenger \n1. Age: age of passenger \n1. SibSp: number of siblings\/spouses\n1. Parch: number of parents\/children \n1. Ticket: ticket number \n1. Fare: amount of money spent on ticket\n1. Cabin: cabin category\n1. Embarked: port where passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)\n","d7e9341d":"<a id = \"5\"><\/a><br>\n# Missing Value\n* Find Missing Value\n* Fill Missing Value","492ef953":"<a id = \"9\"><\/a><br>\n### Find Missing Value","211fe21f":"<a id = \"9\"><\/a><br>\n## Hyperparameter and Model Tuning -- Grid Search -- Cross Validation\nWe will compare 10 machine learning classifier and evaluate mean accuracy of each of them by stratified cross validation.\n\n* Decision Tree\n* SVM\n* Random Forest\n* KNN\n* Logistic Regression\n* GradientBoostingClassifier\n* MLPClassifier\n* XGBRFClassifier\n* LGBMClassifier\n* CatBoostClassifier"}}