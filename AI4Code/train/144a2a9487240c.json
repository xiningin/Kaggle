{"cell_type":{"873ef403":"code","3efb9d2e":"code","76d79422":"code","d3ca3c9e":"code","78badf62":"code","d4b99d05":"code","127708be":"code","030b684b":"code","7388ac2b":"code","e1a83160":"code","d86529f0":"code","852d1e41":"code","ea232118":"code","f3aa517a":"code","1fa492cb":"code","e94cd3bf":"code","0c4fb559":"code","45524d73":"code","069c1329":"code","2a51a2e3":"code","95a0aea2":"code","4afd4b6f":"code","3a8b7fc3":"code","2bb5ee4c":"code","55044b1a":"markdown","354921a5":"markdown","140bf71c":"markdown","c0105d2b":"markdown","a7d22fc6":"markdown","af7567ff":"markdown","ab2d7c30":"markdown","51812733":"markdown","cb8f7386":"markdown","982c5a25":"markdown","d5d4ca48":"markdown","adb20c01":"markdown","0c1dedad":"markdown","ed4919e8":"markdown","508b0fd7":"markdown","2663d4dd":"markdown","0d2348ff":"markdown","a027eb47":"markdown","59c7f292":"markdown","fd0a4e0d":"markdown","1755e1b2":"markdown","3938a38a":"markdown","9274b8e4":"markdown","48ff8a2c":"markdown","1e936cc0":"markdown","193b9528":"markdown"},"source":{"873ef403":"import os\nfrom random import shuffle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport numba\nimport warnings\nsns.set()\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=RuntimeWarning)\n\n@numba.jit\ndef get_stats(arr):\n    \"\"\"Memory efficient stats (min, max and mean). \"\"\"\n    size  = len(arr)\n    min_value = max_value = arr[0]\n    mean_value = 0\n    for i in numba.prange(size):\n        if arr[i] < min_value:\n            min_value = arr[i]\n        if arr[i] > max_value:\n            max_value = arr[i]\n        mean_value += arr[i]\n    return min_value, max_value, mean_value\/size\n\n@numba.jit\ndef get_diff(arr, threshold):\n    \"\"\"Find uniques ttf differences between rows. \"\"\"\n    diff_list = []\n    size  = len(arr)\n    uniques = 0\n    for i in numba.prange(size - 1):\n        diff = abs(arr[i+1] - arr[i])\n        if uniques == 0:\n            diff_list.append(diff)\n            uniques += 1\n        else:\n            for j in numba.prange(uniques):\n                if abs(diff - diff_list[j]) < threshold or abs(diff - diff_list[j]) > 1:\n                    break\n            else:\n                diff_list.append(diff)\n                uniques += 1\n    return diff_list","3efb9d2e":"print(os.listdir(\"..\/input\/\"))","76d79422":"test_folder_files = os.listdir(\"..\/input\/test\")\nprint(test_folder_files[:10])  # print first 10\nprint(\"\\nNumber of files in the test folder\", len(test_folder_files))","d3ca3c9e":"sample_sub = pd.read_csv('..\/input\/sample_submission.csv')\nprint(\"Submission shape\", sample_sub.shape)\nsample_sub.head()","78badf62":"train = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\nprint(\"train shape\", train.shape)\npd.set_option(\"display.precision\", 15)  # show more decimals\ntrain.head()","d4b99d05":"pd.set_option(\"display.precision\", 8)\ntrain.acoustic_data.describe()","127708be":"train_sample = train.sample(frac=0.01)\nplt.figure(figsize=(10,5))\nplt.title(\"Acoustic data distribution\")\nax = sns.distplot(train_sample.acoustic_data, label='Train (1% sample)')","030b684b":"train_sample = train.sample(frac=0.01)\nplt.figure(figsize=(10,5))\nplt.title(\"Acoustic data distribution\")\ntmp = train_sample.acoustic_data[train_sample.acoustic_data.between(-25, 25)]\nax = sns.distplot(tmp, label='Train (1% sample)', kde=False, fit=stats.norm)","7388ac2b":"tmin, tmax, tmean = get_stats(train.time_to_failure.values)\nprint(\"min value: {:.6f}, max value: {:.2f}, mean: {:.4f}\".format(tmin, tmax, tmean))","e1a83160":"plt.figure(figsize=(10,5))\nplt.title(\"Time to failure distribution\")\nax = sns.kdeplot(train_sample.time_to_failure, label='Train (1% sample)')","d86529f0":"def single_timeseries(final_idx, init_idx=0, step=1, title=\"\",\n                      color1='orange', color2='blue'):\n    idx = [i for i in range(init_idx, final_idx, step)]\n    fig, ax1 = plt.subplots(figsize=(10, 5))\n    fig.suptitle(title, fontsize=14)\n    \n    ax2 = ax1.twinx()\n    ax1.set_xlabel('index')\n    ax1.set_ylabel('Acoustic data')\n    ax2.set_ylabel('Time to failure')\n    p1 = sns.lineplot(data=train.iloc[idx].acoustic_data.values, ax=ax1, color=color1)\n    p2 = sns.lineplot(data=train.iloc[idx].time_to_failure.values, ax=ax2, color=color2)\n\n\ndef double_timeseries(final_idx1, final_idx2, init_idx1=0, init_idx2=0, step=1, title=\"\"):\n    idx1 = [i for i in range(init_idx1, final_idx1, step)]\n    idx2 = [i for i in range(init_idx2, final_idx2, step)]\n    \n    fig, (ax1a, ax2a) = plt.subplots(1,2, figsize=(12,5))\n    fig.subplots_adjust(wspace=0.4)\n    ax1b = ax1a.twinx()\n    ax2b = ax2a.twinx()\n    \n    ax1a.set_xlabel('index')\n    ax1a.set_ylabel('Acoustic data')\n    ax2a.set_ylabel('Time to failure')\n    p1 = sns.lineplot(data=train.iloc[idx1].acoustic_data.values, ax=ax1a, color='orange')\n    p2 = sns.lineplot(data=train.iloc[idx1].time_to_failure.values, ax=ax1b, color='blue')\n    \n    p3 = sns.lineplot(data=train.iloc[idx2].acoustic_data.values, ax=ax2a, color='orange')\n    p4 = sns.lineplot(data=train.iloc[idx2].time_to_failure.values, ax=ax2b, color='blue')\n    \nsingle_timeseries(1000, title=\"First thousand rows\")","852d1e41":"single_timeseries(10000, title=\"Ten thousand rows\")","ea232118":"single_timeseries(10000000, step=10, title=\"Ten million rows\")","f3aa517a":"single_timeseries(629145000, step=1000, title=\"All training data\")","1fa492cb":"single_timeseries(629145000, step=1000, title=\"All training data\", color2='white')","e94cd3bf":"peaks = train[train.acoustic_data.abs() > 500]\npeaks.time_to_failure.describe()","0c4fb559":"plt.figure(figsize=(10,5))\nplt.title(\"Cumulative distribution - time to failure with high signal\")\nax = sns.distplot(peaks.time_to_failure, hist_kws=dict(cumulative=True), kde_kws=dict(cumulative=True))","45524d73":"unique_diff = get_diff(train.time_to_failure.values, threshold=1e-10)\nplt.figure(figsize=(10,5))\nplt.title(\"Time to failure - unique differences\")\nax = sns.scatterplot(x=range(len(unique_diff)), y=unique_diff, color='red')","069c1329":"pd.set_option(\"display.precision\", 4)\ntest1 = pd.read_csv('..\/input\/test\/seg_37669c.csv', dtype='int16')\nprint(test1.describe())\nplt.figure(figsize=(10,5))\nplt.title(\"Acoustic data distribution\")\nax = sns.distplot(test1.acoustic_data, label='seg_37669c', kde=False)","2a51a2e3":"fig, axis = plt.subplots(5, 2, figsize=(12,14))\nshuffle(test_folder_files)\nxrow = xcol = 0\nfor f in test_folder_files[:10]:\n    tmp = pd.read_csv('..\/input\/test\/{}'.format(f), dtype='int16')\n    ax = sns.distplot(tmp.acoustic_data, label=f.replace('.csv',''), ax=axis[xrow][xcol], kde=False)\n    if xcol == 0:\n        xcol += 1\n    else:\n        xcol = 0\n        xrow += 1","95a0aea2":"fig, axis = plt.subplots(5, 2, figsize=(12,14))\nxrow = xcol = 0\nfor f in test_folder_files[:10]:\n    tmp = pd.read_csv('..\/input\/test\/{}'.format(f), dtype='int16')\n    ax = sns.lineplot(data=tmp.acoustic_data.values,\n                      label=f.replace('.csv',''),\n                      ax=axis[xrow][xcol],\n                      color='orange')\n    if xcol == 0:\n        xcol += 1\n    else:\n        xcol = 0\n        xrow += 1","4afd4b6f":"rolling_mean = []\nrolling_std = []\nlast_time = []\ninit_idx = 0\nfor _ in range(4194):  # 629M \/ 150k = 4194\n    x = train.iloc[init_idx:init_idx + 150000]\n    last_time.append(x.time_to_failure.values[-1])\n    rolling_mean.append(x.acoustic_data.abs().mean())\n    rolling_std.append(x.acoustic_data.abs().std())\n    init_idx += 150000\n    \nrolling_mean = np.array(rolling_mean)\nlast_time = np.array(last_time)\n\n# plot rolling mean\nfig, ax1 = plt.subplots(figsize=(10, 5))\nfig.suptitle('Mean for chunks with 150k samples of training data', fontsize=14)\n\nax2 = ax1.twinx()\nax1.set_xlabel('index')\nax1.set_ylabel('Acoustic data')\nax2.set_ylabel('Time to failure')\np1 = sns.lineplot(data=rolling_mean, ax=ax1, color='orange')\np2 = sns.lineplot(data=last_time, ax=ax2, color='gray')","3a8b7fc3":"# plot rolling mean\nfig, ax1 = plt.subplots(figsize=(10, 5))\nfig.suptitle('Mean (< 8) for chunks of 150k samples', fontsize=14)\n\nax2 = ax1.twinx()\nax1.set_xlabel('index')\nax1.set_ylabel('Acoustic data')\nax2.set_ylabel('Time to failure')\np1 = sns.lineplot(data=rolling_mean[rolling_mean < 8], ax=ax1, color='orange')\np2 = sns.lineplot(data=last_time, ax=ax2, color='gray')","2bb5ee4c":"frame = pd.DataFrame({'rolling_std': rolling_std, 'time': np.around(last_time, 1)})\ns = frame.groupby('time').rolling_std.mean()\ns = s[s < 20]  # remove one outlier\nplt.figure(figsize=(10, 5))\nplt.title(\"Std for chunks with 150k samples of training data\")\nplt.xlabel(\"Time to failure\")\nplt.ylabel(\"Acoustic data\")\nax = sns.lineplot(x=s.index, y=s.values)","55044b1a":"**More than 80% of high acoustic values are around 0.31 seconds before an earthquake!**","354921a5":"<h3>2.1 Acoustic Data<\/h3>\n\nOur single feature are integers in the range [-5515, 5444] with mean 4.52","140bf71c":"This is not a perfect rule though: there are some peaks far from earthquakes (e.g. between the 14\u00ba and 15\u00ba). Another interesting thing to check is the time between these high levels of seismic signal and the earthquakes. I'm considering any acoustic data with **absolute value greater than 500 as a high level:**","c0105d2b":"According to [this discussion](https:\/\/www.kaggle.com\/c\/LANL-Earthquake-Prediction\/discussion\/77526):\n\n> The data is recorded in bins of 4096 samples. Within those bins seismic data is recorded at 4MHz, but there is a 12 microseconds (1e-6) gap between each bin, an artifact of the recording device.\n\nTherefore, the \"jumps\" have 12 microseconds and occur every 4096 measurements. The next plot has ten million rows, but we are plotting only every 10 data points:","a7d22fc6":"<h3>4.2 Standard deviation<\/h3>\n\nIn the next chart, the standard deviation for each chunk was grouped by the time to failure, which in turn was rounded to the first decimal place.","af7567ff":"<h2>3. Test data<\/h2>\n\nEach file is a segment with 150,000 values for acoustic data (single column). The following distribution is for the first test file:","ab2d7c30":"The standard deviation is also higher for chunks that are closer to an earthquake (in general).","51812733":"<h2>4. Statistics for chunks<\/h2>\n\n<h3>4.1 Rolling mean<\/h3>\nMost kernels are grouping the training data every 150,000 rows and calculating statistics about that chunk. The next plot shows the **mean absolute value for every 150,000 data points** and the** time to failure for the last point within each chunk**.","cb8f7386":"There is one file in the test folder for each prediction (seg_id) in sample_submission:","982c5a25":"The test folder has many csv files:","d5d4ca48":"<h2>1. Overview<\/h2>\n\nAs stated in the description, the data for this competition comes from a experimental set-up used to study earthquake physics. Our goal is to predict the time remaining before the next laboratory earthquake. The only feature we have is the seismic signal (acoustic data), which is recorded using a piezoceramic sensor and corresponds to the voltage upon deformation (in integers).\n\n* Training data: single, continuous segment of experimental data.\n\n* Test data: consists of a folder containing many small segments. The data within each test file is continuous, but the test files do not represent a continuous segment of the experiment.\n\n* Both the training and the testing set come from the same experiment.\n\n* There is no overlap between the training and testing sets.\n\nThere are a lot of files in this competition, so let's start with the folders structure:","adb20c01":"The plot below is using a 1% random sample (~6M rows):","0c1dedad":"There are 16 earthquakes in the training data. The shortest time to failure is 1.5 seconds for the first earthquake and 6s to the 7\u00ba, while the longest is around 16 seconds.\n\nIn the plot below I just changed the time_to_failure color to white. Now it's possible to see a peak in acoustic data before each earthquake:","ed4919e8":"<h2>2. Training data<\/h2>\n\nOne huge csv file has all the training data, which is a single continuous experiment. There are only two columns in this file:\n* Acoustic data (int16): the seismic signal\n* Time to failure (float64): the time until the next laboratory earthquake  (in seconds)\n* **No missing values for both columns**","508b0fd7":"<h3>2.2 Time to Failure<\/h3>\n\nThe target variable is given in seconds with a max value of 16 seconds and minimun value close to zero (1e-5).","2663d4dd":"There are outliers in both directions; let's try to plot the same distribution with the range -25 to 25. The black line is the closest normal distribution (gaussian) possible.","0d2348ff":"<h3>2.4 Time between measurements<\/h3>\n\nLet's have a closer look at the TTF difference between each measurament. I'm using a numba function to find each **unique** difference with a given threshold and ploting the values:","a027eb47":"We can see that the mean absolute value is increasing as we get closer to an earthquake.","59c7f292":"I'm not sure if the different values are due to numeric precision, but we can see two main ranges; the first around 0.001 and the other very close to zero. The former should be the 12 microseconds between each bin, but it's actually much higher (from 1.2e-5 to 1e-3).  ","fd0a4e0d":"The time to failure (TTF) is linearly decreasing at a very slow rate. However, when looking at ten thousand rows it's possible to see some \"jumps\":","1755e1b2":"Now the distribution for 10 files choosen at random:","3938a38a":"Distribution for the random sample:","9274b8e4":"Timeseries (index as x-axis) for the same ten files:","48ff8a2c":"<h3>2.3 Timeseries<\/h3>\n\nLet's see how both variables change over time. The next plot has the first thousand rows of data; the orange line is the acoustic data and the blue one is the time to failure:","1e936cc0":"We can't see the jumps anymore; it's as if we have stretched the line and the steps disappeared. There is only one huge change in TTF, which is exactly when an earthquake occured and the time goes from almost zero to eleven seconds.\n\nFinally, let's plot all the data (629.145 million rows):","193b9528":"Removing high values (mean value less than 8):"}}