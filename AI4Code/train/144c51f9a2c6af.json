{"cell_type":{"1d18223f":"code","576dcc8b":"code","0719111f":"code","fb289d2e":"code","64a4f7ab":"code","ddc34c79":"code","60fda8d7":"code","d07f49d4":"code","47dbba74":"code","3dabc950":"code","4b0de66c":"code","31eb2675":"code","32fd808f":"code","951372d5":"code","aeb3e6d9":"code","a835136c":"code","a1552f70":"code","56d22981":"code","c7fd4f97":"code","26ca016c":"code","8895b237":"code","06b09342":"code","6e2f8961":"code","6165d48c":"code","12cf0703":"code","f5d2ef70":"markdown","360d1a9b":"markdown","a46c126f":"markdown","91f6b1cb":"markdown","2d147c77":"markdown","6da4f28d":"markdown","70987ce4":"markdown","c7aca315":"markdown","7aba2892":"markdown","4ee7a624":"markdown","b7551790":"markdown","7ce26c4b":"markdown","76f43692":"markdown","4bd67df4":"markdown","305e8ea7":"markdown","1bc7ec40":"markdown","ff4d85b5":"markdown","a36cce2c":"markdown","067d06a6":"markdown","953900be":"markdown","fd528efd":"markdown","2a0634b5":"markdown","7aab7476":"markdown","931fcb12":"markdown","b64cd168":"markdown","82491101":"markdown","14267604":"markdown","553b8fbc":"markdown","0b479034":"markdown","4ca6e537":"markdown","21744b45":"markdown","b5e4dc16":"markdown","40fa03f9":"markdown","55318eaa":"markdown","0b2586c0":"markdown","59b1f32a":"markdown","f1f650fd":"markdown","ab5e0356":"markdown","db3ce2bb":"markdown","62b351e9":"markdown"},"source":{"1d18223f":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search\n\nimport pickle","576dcc8b":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","0719111f":"VERBOSE=1","fb289d2e":"gb_clf = GradientBoostingClassifier(verbose=VERBOSE,\n                                    random_state=RANDOM_STATE)","64a4f7ab":"parameters = {\n    'loss': ['deviance']\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","ddc34c79":"parameters = {\n    'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.2, 0.25, 0.30, 0.35, 0.4, 0.45, 0.5]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","60fda8d7":"parameters = {\n    'n_estimators': [100, 200, 500, 1000, 2000]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","d07f49d4":"parameters = {\n    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","47dbba74":"parameters = {\n    'criterion': ['friedman_mse', 'mse']\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","3dabc950":"parameters = {\n    'min_samples_split': [2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","4b0de66c":"parameters = {\n    'min_samples_leaf': [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","31eb2675":"parameters = {\n    'min_weight_fraction_leaf': [x \/ 10 for x in range(0, 6)]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","32fd808f":"parameters = {\n    'max_depth': [1, 2, 5, 8, 13, 21, 34, 53, 54, 55, 89, None]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","951372d5":"parameters = {\n    'min_impurity_decrease': [x \/ 100 for x in range(0, 11)]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","aeb3e6d9":"parameters = {\n    'init': ['zero', None]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\ntable_grid_search(clf, all_ranks=True)","a835136c":"parameters = {\n    'max_features': ['auto', 'sqrt', 'log2', 2, 5, 8, 13, 21, 34, None]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","a1552f70":"parameters = {\n    'max_leaf_nodes': [2, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, None]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","56d22981":"parameters = {\n    'warm_start': [True, False]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","c7fd4f97":"parameters = {\n    'presort': [True, False, 'auto']\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","26ca016c":"parameters = {\n    'n_iter_no_change': [1],\n    'validation_fraction': [x \/ 10 for x in range(1, 10)]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","8895b237":"parameters = {\n    'n_iter_no_change': [1, 2, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","06b09342":"parameters = {\n    'tol': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","6e2f8961":"gb_clf.subsample = 0.8\ngb_clf.min_samples_split = 5\ngb_clf.min_samples_leaf = 5\ngb_clf.max_depth = 13\ngb_clf.min_impurity_decrease = 0.03\ngb_clf.max_features = 34\nparameters = {\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'n_estimators': [500, 2000],\n#     'subsample': [0.8, 0.9, 1.0],\n    'criterion': ['friedman_mse', 'mse'],\n#     'min_samples_split': [4, 5, 6],\n#     'min_samples_leaf': [4, 5, 6],\n#     'max_depth': [12, 13, 14],\n    'min_impurity_decrease': [0, 0.03],\n#     'max_features': [21, 34, None]\n}\nclf = GridSearchCV(gb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","6165d48c":"with open('clf.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","12cf0703":"clf.best_estimator_","f5d2ef70":"## Export grid search results","360d1a9b":"This parameter doesn't have influence on the score.","a46c126f":"# n_estimators\n##### : int (default=100)\n\nThe number of boosting stages to perform. Gradient boosting\nis fairly robust to over-fitting so a large number usually\nresults in better performance.","91f6b1cb":"# max_leaf_nodes\n##### : int or None, optional (default=None)\n\nGrow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.","2d147c77":"This parameter doesn't have influence on the score.","6da4f28d":"The default value has the greatest score.","70987ce4":"# learning_rate\n##### : float, optional (default=0.1)\n\nlearning rate shrinks the contribution of each tree by `learning_rate`.\nThere is a trade-off between learning_rate and n_estimators.","c7aca315":"# min_samples_split\n##### : int, float, optional (default=2)\n\nThe minimum number of samples required to split an internal node:\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n  `ceil(min_samples_split * n_samples)` are the minimum\n  number of samples for each split.","7aba2892":"It's a chaotic relation between subsample and score.","4ee7a624":"# min_samples_leaf\n##### : int, float, optional (default=1)\n\nThe minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n  `ceil(min_samples_leaf * n_samples)` are the minimum\n  number of samples for each node.","b7551790":"# loss\n##### : {'deviance', 'exponential'}, optional (default='deviance')\n\nloss function to be optimized. 'deviance' refers to\ndeviance (= logistic regression) for classification\nwith probabilistic outputs. For loss 'exponential' gradient\nboosting recovers the AdaBoost algorithm.\n","7ce26c4b":"# min_impurity_decrease\n##### : float, optional (default=0.)\n\nA node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\n    N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity\n                        - N_t_L \/ N_t * left_impurity)\n                        \nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.","76f43692":"The more estimators, the more the score is. Increasing the fit time proportionally as the number of estimators.","4bd67df4":"As the data is dense, presorting gets a better score.\nDefault value `auto` do presort.","305e8ea7":"# n_iter_no_change\n##### : int, default None\n\n``n_iter_no_change`` is used to decide if early stopping will be used\nto terminate training when validation score is not improving. By\ndefault it is set to None to disable early stopping. If set to a\nnumber, it will set aside ``validation_fraction`` size of the training\ndata as validation and terminate training when validation score is not\nimproving in all of the previous ``n_iter_no_change`` numbers of\niterations. The split is stratified.","1bc7ec40":"`mse` criterion is a slightly better than `friedman_mse` criterior.\n`mae` criterion needs a huge amount of time (500 times more than the other two criteria).\nThat is 3 hours for only one candidate.\nIt has been removed because of time limit of the notebook.","ff4d85b5":"# tol\n##### : float, optional, default 1e-4\n\nTolerance for the early stopping. When the loss is not improving\nby at least tol for ``n_iter_no_change`` iterations (if set to a\nnumber), the training stops.","a36cce2c":"The value of 34 has the greatest score, even more than None value.","067d06a6":"# max_features\n##### : int, float, string or None, optional (default=None)\n\nThe number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n  `int(max_features * n_features)` features are considered at each\n  split.\n- If \"auto\", then `max_features=sqrt(n_features)`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nChoosing `max_features < n_features` leads to a reduction of variance\nand an increase in bias.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features.","953900be":"The default value has the greatest score.\nOther values have a slightly less score,\nexcept 2 and 5 that are much less.","fd528efd":"# warm_start\n##### : bool, default: False\n\nWhen set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just erase the\nprevious solution.","2a0634b5":"# validation_fraction\n##### : float, optional, default 0.1\n\nThe proportion of training data to set aside as validation set for\nearly stopping. Must be between 0 and 1.\nOnly used if ``n_iter_no_change`` is set to an integer.","7aab7476":"This parameter doesn't have influence on the score.","931fcb12":"Exponential value is not working. It gives the following message:\n\n`ValueError: ExponentialLoss requires 2 classes; got 7 class(es)`","b64cd168":"None value has a greatest score.","82491101":"# init\n##### : estimator or 'zero', optional (default=None)\n\nAn estimator object that is used to compute the initial predictions.\n``init`` has to provide `fit` and `predict_proba`. If 'zero', the\ninitial raw predictions are set to zero. By default, a\n``DummyEstimator`` predicting the classes priors is used.","14267604":"# Exhaustive search","553b8fbc":"# Search over parameters","0b479034":"The greater `min_samples_split` is, the lesser the score is.\nThe greatest score value is for `min_samples_split` 5.","4ca6e537":"# criterion\n##### : string, optional (default=\"friedman_mse\")\n\nThe function to measure the quality of a split. Supported criteria\nare \"friedman_mse\" for the mean squared error with improvement\nscore by Friedman, \"mse\" for mean squared error, and \"mae\" for\nthe mean absolute error. The default value of \"friedman_mse\" is\ngenerally the best as it can provide a better approximation in\nsome cases.\n","21744b45":"# Prepare data","b5e4dc16":"# min_weight_fraction_leaf\n##### : float, optional (default=0.)\n\nThe minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.","40fa03f9":"From 21 `max_depth` estabilizes.\nIt takes the greatest score with the value 13.\nAlso is the one with the greatest fit time.","55318eaa":"# subsample\n#### : float, optional (default=1.0)\n\nThe fraction of samples to be used for fitting the individual base\nlearners. If smaller than 1.0 this results in Stochastic Gradient\nBoosting. `subsample` interacts with the parameter `n_estimators`.\nChoosing `subsample < 1.0` leads to a reduction of variance\nand an increase in bias.\n","0b2586c0":"# presort\n##### : bool or 'auto', optional (default='auto')\n\nWhether to presort the data to speed up the finding of best splits in\nfitting. Auto mode by default will use presorting on dense data and\ndefault to normal sorting on sparse data. Setting presort to true on\nsparse data will raise an error.","59b1f32a":"# Introduction\n\nThe aim of this notebook is to optimize the Extra-trees model.\n\nFirst, all [Gradient Boosting for classification](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization).","f1f650fd":"The default value 0.1 and less don't give a good score.\nIt increases for greater values,\nreaching the maximum at 0.3\nand then decreasing.","ab5e0356":"# max_depth\n##### : integer, optional (default=3)\n\nmaximum depth of the individual regression estimators. The maximum\ndepth limits the number of nodes in the tree. Tune this parameter\nfor best performance; the best value depends on the interaction\nof the input variables.","db3ce2bb":"The greater `min_samples_leaf` is, the lesser the score is.\nThe greatest score value is for `min_samples_leaf` 5.","62b351e9":"It presents a chaotic behaviour."}}