{"cell_type":{"f8ca6b49":"code","d7e450f5":"code","cc4157e0":"code","e20c3de0":"code","71eece96":"code","adbb6957":"code","7b962c9a":"code","0836d436":"code","94379e77":"code","ed34ca2a":"code","a0827eae":"code","859ca56b":"code","272e257e":"code","ee6275ce":"code","86977dff":"code","50c9909b":"code","74a61a8e":"code","dfca030f":"code","347f5b9b":"code","18335a9d":"code","5b849433":"code","faff5a97":"code","783b58cd":"markdown","7bbd64be":"markdown"},"source":{"f8ca6b49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, subdirs, filenames in os.walk('\/kaggle\/input\/urbansound8k'):\n    print(dirname)\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7e450f5":"os.listdir(\"\/kaggle\/input\/urbansound8k\")","cc4157e0":"!pip install torchsummary","e20c3de0":"import torch\nimport torchaudio\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchsummary import summary\nfrom tqdm import tqdm","71eece96":"df=pd.read_csv('\/kaggle\/input\/urbansound8k\/UrbanSound8K.csv')","adbb6957":"df","7b962c9a":"class UrbanSoundDataset(Dataset):\n    def __init__(self,annotations_file,audio_dir,transformation,target_sr,num_samples,device):\n        self.annotations=pd.read_csv(annotations_file)\n        self.audio_dir=audio_dir\n        self.device=device\n        self.transformation=transformation.to(self.device)\n        self.target_sr=target_sr\n        self.num_samples=num_samples\n        \n        \n        \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self,index):\n        audio_sample_path=self._get_audio_path(index)\n        label=self._get_audio_label(index)\n        signal,sr=torchaudio.load(audio_sample_path)\n        signal=signal.to(self.device)\n        signal=self._resample(signal,sr)\n        signal=self._to_mono(signal)\n        \n        if signal.shape[1]>self.num_samples:\n            signal=self._trim(signal)\n            \n        elif signal.shape[1]<self.num_samples:\n            signal=self._pad(signal)\n        \n        signal=self.transformation(signal)\n        return signal,label\n    \n    def _get_audio_path(self,index):\n        fold=f\"fold{self.annotations.iloc[index,5]}\"\n        path=os.path.join(self.audio_dir,fold,self.annotations.iloc[index,0])\n        return path\n    \n    def _get_audio_label(self,index):\n        return self.annotations.iloc[index,-2]\n    \n    def _resample(self,signal,sr):\n        if sr != self.target_sr:\n            resampler=torchaudio.transforms.Resample(sr,self.target_sr) #(original_sr, target_sr)\n            signal=resampler(signal)\n        return signal\n    \n    def _to_mono(self,signal):\n        if signal.shape[0]>1:# if signal not mono\n            signal=torch.mean(signal,dim=0,keepdim=True)\n        return signal\n    \n    def _trim(self,signal):\n        return signal[:,:self.num_samples]\n    \n    def _pad(self,signal):\n        length_signal=signal.shape[1]\n        num_missing_samples=self.num_samples-length_signal\n        last_dim_padding=(0,num_missing_samples)\n        return F.pad(signal,last_dim_padding)\n        \n        ","0836d436":"SAMPLE_RATE=22050\nNUM_SAMPLES=55125","94379e77":"device='cuda' if torch.cuda.is_available() else 'cpu'\ndevice","ed34ca2a":"ANNOTATIONS_PATH=r'\/kaggle\/input\/urbansound8k\/UrbanSound8K.csv'\nAUDIO_DIR=r'\/kaggle\/input\/urbansound8k'\n\n\nmel_spec=torchaudio.transforms.MelSpectrogram(\n                    sample_rate=SAMPLE_RATE,\n                    n_fft=1024,\n                    hop_length=512,\n                    n_mels=33)","a0827eae":"usd=UrbanSoundDataset(ANNOTATIONS_PATH,AUDIO_DIR,mel_spec,SAMPLE_RATE,NUM_SAMPLES,device)\nprint(f\"{len(usd)} number of samples in the dataset\")\nsignal,label = usd[1]","859ca56b":"class ConvNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1=self._create_conv_block(in_channels=1,out_channels=32,kernel_size=3,pool_kernel_size=2,stride=1,padding=2)\n        self.conv2=self._create_conv_block(in_channels=32,out_channels=64,kernel_size=2,pool_kernel_size=2,stride=1,padding=2)\n        self.conv3=self._create_conv_block(in_channels=64,out_channels=128,kernel_size=3,pool_kernel_size=2,stride=1,padding=2)\n        self.conv4=self._create_conv_block(in_channels=128,out_channels=256,kernel_size=2,pool_kernel_size=2,stride=1,padding=3)\n\n        self.flatten=nn.Flatten()\n        self.fc1=nn.Linear(256*5*10,512)\n        self.dropout=nn.Dropout(p=0.5)\n        self.fc2=nn.Linear(512,128)\n        self.fc3=nn.Linear(128,10)\n        self.softmax=nn.Softmax(dim=1)\n        \n    def _create_conv_block(self,in_channels,out_channels,kernel_size,pool_kernel_size,stride,padding):\n        return nn.Sequential(\n                            nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=kernel_size,stride=stride,padding=padding),\n                            nn.BatchNorm2d(num_features=out_channels),\n                            nn.ReLU(),\n                            nn.MaxPool2d(kernel_size=pool_kernel_size)\n                            )\n    \n    def forward(self,x):\n        x=self.conv1(x)\n        x=self.conv2(x)\n        x=self.conv3(x)\n        x=self.conv4(x)\n        x=self.flatten(x)\n        x=self.fc1(x)\n        x=self.dropout(x)\n        x=self.fc2(x)\n        x=self.fc3(x)\n        preds=self.softmax(x)\n        \n        return preds","272e257e":"model=ConvNet().to(device)\nsummary(model,(1,33,108))\n","ee6275ce":"def create_data_loader(train_data, batch_size):\n    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n    return train_dataloader","86977dff":"def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n    for input, target in tqdm(data_loader):\n        input, target = input.to(device), target.to(device)\n\n        # calculate loss\n        prediction = model(input)\n        loss = loss_fn(prediction, target)\n\n        # backpropagate error and update weights\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n    print(f\"loss: {loss.item()}\")","50c9909b":"def train(model, data_loader, loss_fn, optimiser, device, epochs):\n    for i in range(epochs):\n        print(f\"Epoch {i+1}\")\n        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n        print(\"---------------------------\")\n    print(\"Finished training\")","74a61a8e":"BATCH_SIZE=64\nEPOCHS=5\nLR=0.0003","dfca030f":"train_dataloader = create_data_loader(usd, BATCH_SIZE)","347f5b9b":"print(model)","18335a9d":"loss_fn = nn.CrossEntropyLoss()\noptimiser = torch.optim.Adam(model.parameters(),lr=LR)","5b849433":"train(model, train_dataloader, loss_fn, optimiser, device, EPOCHS)","faff5a97":"torch.save(model.state_dict(), \"cnn_model.pth\")\nprint(\"Trained feed forward net saved at cnn_model.pth\")","783b58cd":"## for i in range(10):\n    signal,label=usd[i]\n    print(signal.shape)\n    \n#durations need to be constant","7bbd64be":"### To find flattening dimension\n\ndef __init():<br>\n    x=torch.randn(33,108).view(-1,1,33,108) <br>\n    self._convs(x)<br>\n    <br>\ndef _convs(self,x):<br>\n        x=self.conv1(x)<br>\n        x=self.conv2(x)<br>\n        x=self.conv3(x)<br>\n        x=self.conv4(x)<br>\n        print(x.shape)<br>\n        x=self.flatten(x)<br>\n        print(x.shape)<br>\n        \n #### else use AdaptiveMaxPool2d\n       "}}