{"cell_type":{"df3e8be3":"code","bf9c0bb3":"code","4bfd19f1":"code","6dd1aac3":"code","477c8553":"code","5dfc4050":"code","32a5a45c":"code","bf0b48b4":"code","5015e9c3":"code","2cabcc63":"code","649a4d95":"code","95816134":"code","987845b8":"code","16718237":"code","4d6967f6":"code","98e930fe":"code","12a7e031":"code","e1d41b9f":"code","578f3123":"code","02237931":"code","dcccbe6d":"code","09aeca7f":"code","c2e69e93":"code","6393f611":"code","b8e3c6ff":"code","5f6663da":"code","97b4b864":"code","d11b1d2c":"code","a11f072f":"code","508e8a04":"code","71275968":"code","8a76c417":"code","2495bae5":"code","e4fedff4":"code","4d8d91a8":"code","3ae92db0":"code","0f244f92":"code","0381a2b1":"code","736ae570":"code","457bf25e":"code","eec846bd":"code","44732119":"markdown","a01f4b03":"markdown","b345d2e9":"markdown","34ff50dd":"markdown","709696f3":"markdown","5b7a83b6":"markdown","d81d53b3":"markdown","f5122d6a":"markdown","f6197afa":"markdown","ac97977a":"markdown","21327d95":"markdown"},"source":{"df3e8be3":"import numpy as np\nimport pandas as pd\n\n#Reading the test and the train data\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\nprint('train shape : ', train.shape)\nprint('test shape : ', test.shape)\n\n","bf9c0bb3":"train.head()","4bfd19f1":"test.head()","6dd1aac3":"train = train.set_index('row_id')\ntest = test.set_index('row_id')","477c8553":"#Initial EDA \n\ntrain.head(10)","5dfc4050":"train.info()","32a5a45c":"#Check for null values\n\ntrain.isnull().sum()","bf0b48b4":"test.isnull().sum()","5015e9c3":"#Looking at basic stats\n\n#For numeric cols\n\ntrain.describe().T  #The transpose is used for better view.","2cabcc63":"test.describe().T","649a4d95":"#For categorical columns\n\ntrain.describe(include=['O'])","95816134":"test.describe(include=['O'])","987845b8":"#Lets look at the frequency distribution of the key categorical variables country, store, product\n\nprint(train['country'].value_counts())\nprint('\\n')\nprint(train['store']. value_counts())\nprint('\\n')\nprint(train['product'].value_counts())\n\n","16718237":"train.groupby(['country','product']).size()","4d6967f6":"#Looking at the dates feature to understand what period the data belongs to. \n\ndef min_max_dt(df, name='train'):\n    #df['date'] = pd.to_datetime(df['date'])\n    min_date = df['date'].min()\n    max_date = df['date'].max()\n    print(f'For the {name} data : Min_date - {min_date} \/ Max_date - {max_date}')\n    return None\n          \n          \nmin_max_dt(train, 'train')\nmin_max_dt(test, 'test')\n","98e930fe":"\ndef smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) \/ (y_true + np.abs(y_pred)) * 200\n\n#print(smape_loss(tf.constant([1, 2]), tf.constant([3, 4]))) # should print [100, 66.6667]","12a7e031":"#Credit to https:\/\/www.kaggle.com\/jaredfeng\/tps-jan22-inprog-v5\n\nholiday_path = '..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv'\n\ndef GetHoliday(holiday_path, df):\n    \"\"\"\n    Get a boolean feature of whether the current row is a holiday sale\n    \"\"\"\n    \n    holiday = pd.read_csv(holiday_path)\n    fin_holiday = holiday.loc[holiday.Country == 'Finland']\n    swe_holiday = holiday.loc[holiday.Country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.Country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.Date).astype(float)\n    df['swe holiday'] = df.date.isin(swe_holiday.Date).astype(float)\n    df['nor holiday'] = df.date.isin(nor_holiday.Date).astype(float)\n    \n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    return df\n\ntrain = GetHoliday(holiday_path, train)\ntest = GetHoliday(holiday_path, test)\n","e1d41b9f":"# Credit to https:\/\/www.kaggle.com\/ranjeetshrivastav\/tps-jan-21-base-xgb\n# and https:\/\/www.kaggle.com\/bernhardklinger\/tps-jan-2022\/notebook\n\ndef feature_eng(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df['week']= df['date'].dt.week\n    #df['year'] = 'Y' + df['date'].dt.year.astype(str)\n    df['quarter'] = 'Q' + df['date'].dt.quarter.astype(str)\n    df['day'] = df['date'].dt.day\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df.loc[(df.date.dt.is_leap_year) & (df.dayofyear >= 60),'dayofyear'] -= 1\n    df['weekend'] = df['date'].dt.weekday >=5\n    df['weekday'] = 'WD' + df['date'].dt.weekday.astype(str)\n    df.drop(columns=['date'],inplace=True)  \n\nfeature_eng(train)\nfeature_eng(test)","578f3123":"train.columns","02237931":"train.head()","dcccbe6d":"test.head()","09aeca7f":"train.info()","c2e69e93":"train['day'].value_counts()","6393f611":"X_train = train.drop(['num_sold'],axis=1)\ny_train = train['num_sold']\nX_test = test   \n","b8e3c6ff":"X_train.columns","5f6663da":"X_train_ohe = pd.get_dummies(X_train)\nX_test_ohe = pd.get_dummies(X_test)\nfinal_train, final_test = X_train_ohe.align(X_test_ohe,join='left', axis=1)\n\n#Please refer to this excellent notebook from Dans Becker to understand how to align categorical columns\n\n","97b4b864":"final_train.head()","d11b1d2c":"final_test.head()","a11f072f":"#Standardising the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(final_train)\nX_test_scaled = scaler.transform(final_test)\n\n","508e8a04":"#Creating a custom scoring for cross_validation\n\nfrom sklearn.metrics import fbeta_score, make_scorer\n\n# Credit to https:\/\/www.kaggle.com\/c\/web-traffic-time-series-forecasting\/discussion\/36414\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\nsmape_score = make_scorer(SMAPE, greater_is_better=False)\n","71275968":"from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.model_selection import cross_val_score\n\n\ntt_xgb = TransformedTargetRegressor(regressor=XGBRegressor(),\n                                   func=np.log, inverse_func=np.exp)\n\nscores_xgb = cross_val_score(tt_xgb, X_train_scaled, y_train, scoring=smape_score, cv=5)\n\n\ntt_lgb = TransformedTargetRegressor(regressor=LGBMRegressor(),\n                                   func=np.log, inverse_func=np.exp)\n\nscores_lgb = cross_val_score(tt_lgb, X_train_scaled, y_train, scoring=smape_score, cv=5)\n\ntt_gbr = TransformedTargetRegressor(GradientBoostingRegressor(),\n                                   func=np.log, inverse_func=np.exp)\n\nscores_gbr = cross_val_score(tt_gbr, X_train_scaled, y_train, scoring=smape_score, cv=5)\n\n\ntt_ridge = TransformedTargetRegressor(regressor=Ridge(),\n                                   func=np.log, inverse_func=np.exp)\n\nscores_ridge = cross_val_score(tt_ridge, X_train_scaled, y_train, scoring = smape_score, cv = 5)\n\n\ntt_lasso = TransformedTargetRegressor(regressor=Lasso(),\n                                   func=np.log, inverse_func=np.exp)\n\nscores_lasso = cross_val_score(tt_lasso, X_train_scaled, y_train, scoring = smape_score, cv = 5)\n\n\ntt_elasticnet = TransformedTargetRegressor(regressor=ElasticNet(),\n                                   func=np.log, inverse_func=np.exp)\n\nscores_elasticnet = cross_val_score(tt_elasticnet, X_train_scaled, y_train, scoring = smape_score, cv = 5)\n\ntt_svr = TransformedTargetRegressor(regressor = SVR(C=20, epsilon = 0.008, gamma = 0.0003))\n\nscores_svr = cross_val_score(tt_svr, X_train_scaled, y_train, scoring = smape_score, cv = 5)\n\n","8a76c417":"print(scores_xgb)\nprint(scores_lgb)\nprint(scores_gbr)\nprint(scores_ridge)\nprint(scores_lasso)\nprint(scores_elasticnet)\nprint(scores_svr)","2495bae5":"\nstack_gen = StackingCVRegressor(regressors=(tt_xgb, tt_lgb, tt_gbr),\n                                meta_regressor=tt_xgb,\n                                use_features_in_secondary=True)\nscores_stack = cross_val_score(stack_gen, X_train_scaled, y_train, scoring = smape_score, cv = 5)\n","e4fedff4":"print(scores_stack)","4d8d91a8":"#Lets fit all the non linear models on the full training data\n\ntt_xgb.fit(X_train_scaled, y_train)\ntt_lgb.fit(X_train_scaled, y_train)\ntt_gbr.fit(X_train_scaled, y_train)","3ae92db0":"#Lets fit the stacked model on the training data\nstack_gen.fit(X_train_scaled, y_train)\n\n","0f244f92":"def blend_models_predict(X_test, m1, m2, m3, m4):\n    return ((m1 * tt_xgb.predict(X_test)) + \\\n            (m2 * tt_lgb.predict(X_test)) + \\\n            (m3 * tt_gbr.predict(X_test)) + \\\n            (m4 * stack_gen.predict(X_test))\n            )","0381a2b1":"\ntest_y_pred = blend_models_predict(X_test_scaled, 0.1, 0.3, 0.3, 0.3)\n\n","736ae570":"test_y_pred","457bf25e":"#submission\n\nassert(len(test.index)==len(test_y_pred))\n\nsubmission_df = pd.DataFrame(list(zip(test.index, test_y_pred)), columns=['row_id', 'num_sold'])\n\nsubmission_df.to_csv('submission.csv', index=False)","eec846bd":"submission_df","44732119":"Since we want to use log for the dependent variable, we can use scikitlearn's TransformedRegressor.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.TransformedTargetRegressor.html\n\n","a01f4b03":"The train data is from Jan 2015 to Dec 2018, while the test data is from Jan 2019 to Dec 2019","b345d2e9":"Step 1 - Converting categorical variables into numeric & Step 2 - Align the train and test columns\n","34ff50dd":"The metric that will be used to evaluate the competition:","709696f3":"4. Predict a blended model and submit predictions","5b7a83b6":"The dataset does not have any null values","d81d53b3":"Thus we can see in the training dataset we have 3 unique countries, 2 unique stores (KaggleMart or KaggleRama) and 3 unique products","f5122d6a":"Steps to take\n\n1. Convert the categorical variables into numeric\n2. Align the train and test columns\n3. Run the non linear models\n4. Stack the non-linear model\n5. Predict and submit predictions","f6197afa":"Clearly the linear models dont do well. Lets stack the non linear models.","ac97977a":"One interesting aspect is that the product and the country distributions are the same (8766), does that mean a particular country produces just one of those products? Lets check it out.","21327d95":"3. Run the non linear models (also test scores with linear models)"}}