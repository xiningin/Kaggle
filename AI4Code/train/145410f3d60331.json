{"cell_type":{"286f159d":"code","3f688667":"code","683002c9":"code","d32762cb":"code","ff316516":"code","16bcff18":"code","8124a51a":"code","6768b894":"code","d9498afd":"code","eb05f456":"code","3ae9c44f":"code","c07cb29b":"code","33e0da4d":"code","8c5fe2ab":"code","6ef75f6e":"code","830cfc5d":"code","d3cafb3e":"code","f1fb0b0a":"code","6ebb8603":"code","7f50bb04":"code","7e815e7e":"code","82536581":"code","00157083":"code","2ea7cfd3":"code","f3342df5":"code","6efa065c":"code","e0bb257e":"code","6e01cdbe":"code","9d5f115c":"markdown","abaae418":"markdown","6976049a":"markdown","a9aafbff":"markdown"},"source":{"286f159d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f688667":"from sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns","683002c9":"application_train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\napplication_test  = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\napplication_sub   = pd.read_csv('..\/input\/home-credit-default-risk\/sample_submission.csv')","d32762cb":"#Using a for loop in Python to figure out the number of missing values in each column\nmissing_data_application_train = application_train.isnull()\nmost_missing = pd.DataFrame(columns=['Column','Percentage'])\n\nfor column in missing_data_application_train.columns.values.tolist(): \n    #print(column)\n    #print(missing_data_application_train[column].value_counts())\n    #print(\"Percentage of missing values in column:\",np.sum(missing_data_application_train[column])\/missing_data_application_train[column].count()*100)\n    #print(\"\")\n    \n    if (np.sum(missing_data_application_train[column])\/missing_data_application_train[column].count()*100) > 0:\n        most_missing = most_missing.append({'Column': column,'Percentage':np.sum(missing_data_application_train[column])\/missing_data_application_train[column].count()*100}, ignore_index=True)\n    else:\n        continue","ff316516":"most_missing.sort_values(by='Percentage',ascending=False).round(1)[most_missing['Percentage'] > 20]","16bcff18":"#Delete non important columns that we won't take into account in our model\nfor row in most_missing.sort_values(by='Percentage',ascending=False).round(1)[most_missing['Percentage'] > 20].iterrows():\n    name_column = list(row)\n    name_column = [str(i).split('\\n',1)[0] for i in name_column]\n    name_column = name_column[1].split()[1]\n    application_train.drop([name_column],axis=1,inplace=True)\n    application_test.drop( [name_column],axis=1,inplace=True)\n\n#TRAIN: Convert categorical variable into dummy\/indicator variables with get_dummies and concat them to the df\nfor index, dtype in application_train.dtypes.iteritems():\n    if dtype == object:\n        normalized_column = pd.get_dummies(application_train[index],drop_first=True)\n        application_train = pd.concat([application_train,normalized_column],axis=1)\n        #Drop the old columns with categorical variables no longer useful\n        application_train.drop([index],axis=1,inplace=True)\n\n#TEST: Convert categorical variable into dummy\/indicator variables with get_dummies and concat them to the df\nfor index, dtype in application_test.dtypes.iteritems():\n    if dtype == object:\n        normalized_column = pd.get_dummies(application_test[index],drop_first=True)\n        application_test = pd.concat([application_test,normalized_column],axis=1)\n        #Drop the old columns with categorical variables no longer useful\n        application_test.drop([index],axis=1,inplace=True)","8124a51a":"application_train['TARGET']","6768b894":"def getDuplicatesWithCount(listOfElems):\n    ''' Get frequency count of duplicate elements in the given list '''\n    dictOfElems = dict()\n    # Iterate over each element in list\n    for elem in listOfElems:\n        # If element exists in dict then increment its value else add it in dict\n        if elem in dictOfElems:\n            dictOfElems[elem] += 1\n        else:\n            dictOfElems[elem] = 1    \n \n    # Filter key-value pairs in dictionary. Keep pairs whose value is greater than 1 i.e. only duplicate elements from list.\n    dictOfElems = { key:value for key, value in dictOfElems.items() if value > 1}\n    # Returns a dict of duplicate elements and thier frequency count\n    return dictOfElems","d9498afd":"dictOfElems = getDuplicatesWithCount(application_train.columns)     \nfor key, value in dictOfElems.items():\n        print(key , ' :: ', value)","eb05f456":"application_train.drop(['Maternity leave','Unknown','XNA','Y',], axis=1,inplace=True)\napplication_test.drop(['Y','XNA'], axis=1,inplace=True)","3ae9c44f":"len(set(application_test))","c07cb29b":"missing = []\nfor column in application_train.columns:\n    if column not in application_test.columns:\n         missing.append(column)\nmissing","33e0da4d":"#Clean data from NA values, fill them with the mean()\napplication_train.fillna(application_train.mean(),inplace=True)\napplication_test.fillna(application_test.mean(),inplace=True)\n\n#Put aside TARGET column in app_train\napplication_train_target = pd.DataFrame({'TARGET':application_train['TARGET']})\n\n#Put the columns with titles aside before asigning the scalars\ntrain_columns = application_train.loc[:, application_train.columns != 'TARGET'].columns\ntest_columns  = application_test.columns\n\n#Multiply each train and test dataframes by a scalar\nScaler1 = StandardScaler()\nScaler2 = StandardScaler()\napplication_train = pd.DataFrame(Scaler1.fit_transform(application_train.loc[:, application_train.columns != 'TARGET']))\napplication_test  = pd.DataFrame(Scaler2.fit_transform(application_test))\n\n#Reput the write column titles\napplication_train.columns = train_columns\napplication_test.columns  = test_columns\n\n#Reassign TARGET in app_train\napplication_train['TARGET'] = application_train_target\n\napplication_train\n#Save features and target names in separate variables\nfeatures = application_train.iloc[:,2:].columns.tolist()\ntarget   = application_train.loc[:, 'TARGET'].name\n\n#Create n dimensional arrays with features ('X_train') and the targets for each ('Y_train')\nX_train = application_train.iloc[:,2:].values\ny_train = application_train.loc[:,'TARGET'].values\n\n#Create n dimensional arrays with features ('X_test')\nX_test = application_test.iloc[:,1:].values\n","8c5fe2ab":"application_train","6ef75f6e":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader","830cfc5d":"## train data\nclass trainData(Dataset):\n    \n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n\n\ntrain_data = trainData(torch.FloatTensor(X_train), \n                       torch.FloatTensor(y_train))\n## test data    \nclass testData(Dataset):\n    \n    def __init__(self, X_data):\n        self.X_data = X_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n    \n\ntest_data = testData(torch.FloatTensor(X_test))","d3cafb3e":"batch_size = 200\nn_epochs = 300\nbatch_no = len(X_train) \/\/ batch_size\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=1)","f1fb0b0a":"len(features)","6ebb8603":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.layer1 = nn.Linear(148, 296)\n        self.layer2 = nn.Linear(296, 148)\n        self.layer3 = nn.Linear(148, 74)\n        self.layer_out = nn.Linear(74, 1)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(296)\n        self.batchnorm2 = nn.BatchNorm1d(148)\n        self.batchnorm3 = nn.BatchNorm1d(74)\n        \n        \n    def forward(self, inputs):\n        #print('Shape of inputs:',inputs.shape)\n        x = self.relu(self.layer1(inputs))\n        #print('Shape of x after relu.layer1', x.shape)\n        x = self.batchnorm1(x)\n        #print('Shape of x after batchnorm1', x.shape)\n        x = self.relu(self.layer2(x))\n        #print('Shape of x after relu.layer2', x.shape)\n        x = self.batchnorm2(x)\n        #print('Shape of x after batchnorm2', x.shape)\n        x = self.relu(self.layer3(x))\n        #print('Shape of x after relu.layer3', x.shape)\n        x = self.batchnorm3(x)\n        x = self.dropout(x)\n        #print('Shape of x after dropout', x.shape)\n        x = self.layer_out(x)\n        return x\n\n","7f50bb04":"ngpu = 1\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\nprint(device)\n\nmodel = Net()\nmodel.to(device)\nprint(model)","7e815e7e":"criterion = nn.BCEWithLogitsLoss()","82536581":"optimizer = torch.optim.Adam(model.parameters(), lr=0.1)","00157083":"def binary_acc(y_pred, y_test):\n    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n\n    correct_results_sum = (y_pred_tag == y_test).sum().float()\n    acc = correct_results_sum\/y_test.shape[0]\n    acc = torch.round(acc * 100)\n    \n    return acc","2ea7cfd3":"model.train()\nfor e in range(1, n_epochs+1):\n    epoch_loss = 0\n    epoch_acc = 0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        \n        output = model(X_batch)\n        loss = criterion(output, y_batch.unsqueeze(1))\n        acc = binary_acc(output, y_batch.unsqueeze(1))\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        #print(loss.item())\n        \n    print(f'Epoch {e+0:03}: | Loss: {epoch_loss\/len(train_loader):.5f} | Acc: {epoch_acc\/len(train_loader):.3f}')\n    if epoch_acc < epoch_acc += acc.item() ","f3342df5":"y_pred_list = []\nmodel.eval()\nwith torch.no_grad():\n    for X_batch in test_loader:\n        X_batch = X_batch.to(device)\n        y_test_pred = model(X_batch)\n        y_test_pred = torch.sigmoid(y_test_pred)\n        y_pred_tag = torch.round(y_test_pred)\n        y_pred_list.append(y_pred_tag.cpu().numpy())\n\ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]","6efa065c":"perc = submission.loc[submission['TARGET'] == 1].count()\/submission.loc[submission['TARGET'] == 0].count()*100\nperc","e0bb257e":"sum(y_pred_list)\nsns.countplot(y_pred_list)","6e01cdbe":"submission = pd.DataFrame({'SK_ID_CURR': application_sub['SK_ID_CURR'], 'TARGET': y_pred_list})\nsubmission.to_csv('submission.csv', index=False)","9d5f115c":"We import everything we need from Pytorch","abaae418":"We define our data loaders :","6976049a":"Let's examine how much features we get after the normalization of the data, to align this number with the inputs of the NN:","a9aafbff":"In all this configuration of the NN, we also want to check the accuracy of the model, so let's build a function for it:"}}