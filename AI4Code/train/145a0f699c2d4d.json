{"cell_type":{"d82d1585":"code","1c0ed255":"code","1898db21":"code","cb7c1a70":"code","51fa10b8":"code","b48bef92":"code","c1872a79":"code","d4c71915":"code","097f5bf4":"code","2365dc93":"code","5199dd7f":"code","e7d5062b":"markdown"},"source":{"d82d1585":"!pip install pytorch-lightning\n!pip install torch-scatter -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+cu102.html\n!pip install torch-sparse -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+cu102.html\n!pip install torch-cluster -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+cu102.html\n!pip install torch-spline-conv -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+cu102.html\n!pip install torch-geometric\n!pip install captum\n!pip install torch_summary","1c0ed255":"import pytorch_lightning as pl\nimport torchvision.transforms\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom torch.nn import Linear, ReLU\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nfrom torch_geometric.utils import normalized_cut\nfrom torch_geometric.nn import SplineConv\nfrom torch_geometric.nn import (SplineConv, graclus, max_pool, max_pool_x, global_max_pool,\n                                global_mean_pool, GCNConv)\nimport torch.nn.functional as F\nfrom torch_geometric.data import Batch,Data\nfrom torch_geometric.datasets import MNISTSuperpixels\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DataLoader\n\ndef normalized_cut_2d(edge_index, pos):\n    row, col = edge_index\n    edge_attr = torch.norm(pos[row] - pos[col], p=2, dim=1)\n    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0))\n\n\nclass MNISTSplineNet(pl.LightningModule):\n    def __init__(self,in_features,out_features):\n        super(MNISTSplineNet, self).__init__()\n        self.conv1 = SplineConv(in_features, 32, dim=2, kernel_size=5)\n        self.conv2 = SplineConv(32, 64, dim=2, kernel_size=5)\n        self.relu1 = ReLU()\n        self.relu2 = ReLU()\n        self.relu3 = ReLU()\n        \n        \n        self.fc1 = torch.nn.Linear(64, 128)\n        self.fc2 = torch.nn.Linear(128, out_features)\n        \n    def forward(self,data):\n        print(data)\n        data.x = self.relu1(self.conv1(data.x, data.edge_index, data.edge_attr))\n        print(data)\n        weight = normalized_cut_2d(data.edge_index, data.pos)\n        cluster = graclus(data.edge_index, weight, data.x.size(0))\n#         data.edge_attr = None\n        print(data)\n        data = max_pool(cluster, data, transform=self.transform)\n        data.x = F.elu(self.conv2(data.x, data.edge_index, data.edge_attr))\n        print(data)\n        weight = normalized_cut_2d(data.edge_index, data.pos)\n        cluster = graclus(data.edge_index, weight, data.x.size(0))\n        x, batch = max_pool_x(cluster, data.x, data.batch)\n        print(data)\n        x = global_mean_pool(x, batch)\n        x = F.elu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        return F.log_softmax(self.fc2(x), dim=1)\n    \n    def prepare_data(self):\n        self.transform = T.Cartesian(cat=False)\n\n        MNISTSuperpixels.url = ('https:\/\/graphics.cs.tu-dortmund.de\/fileadmin\/ls7-www\/misc\/cvpr\/'\n            'mnist_superpixels.tar.gz')\n\n        train_dataset = MNISTSuperpixels('mnist',True, transform=self.transform)\n        test_dataset = MNISTSuperpixels('mnist',False, transform=self.transform)\n        train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n        self.train_dataset = train_set\n        self.val_dataset = val_set\n        self.test_dataset = test_dataset\n        \n            \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset,batch_size=32, shuffle=True, num_workers=16)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_dataset,batch_size=32, shuffle=False, num_workers=16)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset,batch_size=32, shuffle=False, num_workers=16)\n    \n    def configure_optimizers(self):\n        optimizer = Adam(self.parameters(), lr=0.01)\n        return optimizer\n    \n    def training_step(self,batch,batch_idx):\n        return F.nll_loss(self.forward(batch), batch.y)\n    \n    def validation_step(self, batch,batch_idx):\n        y_hat = self.forward(batch)\n        loss = F.nll_loss(y_hat,batch.y)\n        pred = y_hat.max(1)[1]\n        return {'val_loss':loss}\n#         this.correct += pred.eq(data.y).sum().item()\n#         return {'val_loss':loss, 'correct':correct}\n    \n    def validation_epoch_end(self,outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        tensorboard_logs = {'val_loss':avg_loss}\n        return {'avg_val_loss' : avg_loss, 'log':tensorboard_logs}\n        \n\n\nmodel = MNISTSplineNet(in_features=1, out_features=10)\nmodel.prepare_data()","1898db21":"train_dataset  = model.train_dataset\n\nprint(\"Number of graphs: %i\" % (len(train_dataset)))\n\ngraph = train_dataset[0]\nprint(\"Number of nodes: %i\" % (graph.num_nodes))\nprint(\"Number of features in node: %i\" % (graph.num_node_features))\n\nprint(\"There are 60000 homogenous graphs, each node respresents digit and has features represented as extracted features.\")\nprint()\n\nprint(\"Number of edge feature: %i\" %(graph.num_edge_features))\nprint(\"Edges number of edges: %i\" %(graph.edge_index.size()[1]))\nprint(\"Example edge feature (pos): \" + str(graph.edge_index[:,1000])) \n\n\nprint(\"Edges have 2 features that are the indexes of nodes which are interconnected (from_node_idx,to_node_idx).\")\nprint()\n\nclasses = [data.y.item() for data in train_dataset]\nprint(\"Example y classes: \" + str(classes[0:10]))\nprint(\"Minimal class index: \" + str(min(classes)))\nprint(\"Maximal class index: \" + str(max(classes)))\noutput_classes = max(classes)-min(classes)+1\nprint(\"Number of output classes: \" + str(output_classes))\nprint(\"Output variable represents 10 distinct product categories.\")","cb7c1a70":"do_train = False\n\n\nif do_train:\n#     trainer = pl.Trainer(max_epochs=8)\n    trainer = pl.Trainer(gpus=-1, max_epochs=8)\n    trainer.fit(model)\nelse:\n    #Load weights\n    pass","51fa10b8":"#Node Feature explainations\n\nimport torch_geometric\nfrom captum.attr import Saliency,IntegratedGradients,GradientShap, FeatureAblation, DeepLift\n\ndef attributation_for_feature_forward(x, data):\n    data.x=x\n    return model(data)\n\n\ndata_sample = next(iter(DataLoader(model.train_dataset,batch_size=1, shuffle=True, num_workers=8)))\n\nig = Saliency(attributation_for_feature_forward)\nattr = ig.attribute(data_sample.x, additional_forward_args=((data_sample,)), target=data_sample.y)\n\nprint(\"Node features size: \" + str(list(data_sample.x.size())))\nprint(\"Attributes size: \" + str(list(attr.size())))\nprint()\nprint(\"Minimal value: \" + str(torch.min(attr).item()))\nprint(\"Maximal value: \" + str(torch.max(attr).item()))\nprint()\n# print(\"Top 5 feature indexes: \" + str(torch.topk(torch.mean(attr,axis=0),5)))\n# print()\n# print(\"Top 5 node indexes: \" + str(torch.topk(torch.mean(attr,axis=1),5)))\n# print()","b48bef92":"import torch_geometric\nfrom torch.nn import Module\n\n# https:\/\/github.com\/rusty1s\/pytorch_geometric\/issues\/1730\n\nclass ExplainNet(Module):\n    def __init__(self,model, data):\n        super(ExplainNet, self).__init__()\n        self.model = model\n        self.data = data\n\n    def forward(self,x, edge_index):\n        self.model.forward(self.data)\n        \nexplainer = torch_geometric.nn.GNNExplainer(ExplainNet(model, data_sample))\n\nnode_idx = 1\n# node_feat_mask, edge_mask = explainer.explain_node(node_idx,data_sample.x, data_sample.edge_index)\nexplainer.explain_graph(data=data_sample)\n\nmaximal_feature_importance = torch.max(node_feat_mask).item()\nmaximal_edge_importance = torch.max(edge_mask).item()\n\nprint(\"Maximal feature importance: %d \" % (maximal_feature_importance))\nprint(\"Maximal edge importance: %d \" % (maximal_edge_importance,))\nprint()","c1872a79":"#Node Feature explainations\n\nimport torch_geometric\nfrom captum.attr import Saliency,IntegratedGradients,GradientShap, FeatureAblation, DeepLift\n\ndef attributation_for_feature_forward(x, data):\n    data.x=x\n    return model(data)\n\n\ndata_sample = next(iter(DataLoader(model.train_dataset,batch_size=1, shuffle=True, num_workers=8)))\n\n\nig = IntegratedGradients(attributation_for_feature_forward)\nattr = ig.attribute(data_sample.x, additional_forward_args=((data_sample,)), target=data_sample.y, baselines=torch.zeros_like(data_sample.x), internal_batch_size=1)\n\nprint(\"Node features size: \" + str(list(data_sample.x.size())))\nprint(\"Attributes size: \" + str(list(attr.size())))\nprint()\nprint(\"Minimal value: \" + str(torch.min(attr).item()))\nprint(\"Maximal value: \" + str(torch.max(attr).item()))\n# print()\n# print(\"Top 5 feature indexes: \" + str(torch.topk(torch.mean(attr,axis=0),5)))\n# print()\n# print(\"Top 5 node indexes: \" + str(torch.topk(torch.mean(attr,axis=1),5)))\n# print()","d4c71915":"# #Node Feature explainations\n\n# import torch_geometric\n# from captum.attr import Saliency,IntegratedGradients,GradientShap, FeatureAblation, DeepLift, GuidedBackprop\n\n# def attributation_for_feature_forward(x, data):\n# #     print(\"--\")\n# #     print(data)\n# #     print(data.x.size())\n# #     print(\"x\")\n# #     print(x.size())\n#     data.x=x\n#     return model(data)\n\n\n# data_sample = next(iter(DataLoader(model.train_dataset,batch_size=1, shuffle=True, num_workers=8)))\n\n\n# ig = GuidedBackprop(model.relu1)\n# attr = ig.attribute(data_sample.x, additional_forward_args=((data_sample,)), target=data_sample.y, baselines=torch.zeros_like(data_sample.x))\n\n# print(\"Node features size: \" + str(list(data_sample.x.size())))\n# print(\"Attributes size: \" + str(list(attr.size())))\n# print()\n# print(\"Minimal value: \" + str(torch.min(attr).item()))\n# print(\"Maximal value: \" + str(torch.max(attr).item()))\n# # print()\n# # print(\"Top 5 feature indexes: \" + str(torch.topk(torch.mean(attr,axis=0),5)))\n# # print()\n# # print(\"Top 5 node indexes: \" + str(torch.topk(torch.mean(attr,axis=1),5)))\n# # print()","097f5bf4":"#Node Feature explainations\n\nimport torch_geometric\nfrom captum.attr import Saliency,IntegratedGradients,GradientShap, FeatureAblation, DeepLift, GuidedBackprop\n\ndef attributation_for_feature_forward(x, data):\n    data.x=x\n    return model(data)\n\n\ndata_sample = next(iter(DataLoader(model.train_dataset,batch_size=1, shuffle=True, num_workers=8)))\nprint(data_sample)\nprint(len(data_sample.to_data_list()))\ndata = data_sample.to_data_list()[0]\ndata.x = data_sample.x\nprint(data_sample.x.size())\ndata = torch_geometric.data.Data(x=data_sample.x,edge_index=data_sample.edge_index,y=data_sample.y, pos=data_sample.pos, edge_attr=data_sample.edge_attr)\ndata.batch = None\nprint(data)\nig = GradientShap(attributation_for_feature_forward)\nattr = ig.attribute(data.x, additional_forward_args=((data,)), target=data.y, baselines=torch.zeros_like(data.x))\n\nprint(\"Node features size: \" + str(list(data_sample.x.size())))\nprint(\"Attributes size: \" + str(list(attr.size())))\nprint()\nprint(\"Minimal value: \" + str(torch.min(attr).item()))\nprint(\"Maximal value: \" + str(torch.max(attr).item()))\n# print()\n# print(\"Top 5 feature indexes: \" + str(torch.topk(torch.mean(attr,axis=0),5)))\n# print()\n# print(\"Top 5 node indexes: \" + str(torch.topk(torch.mean(attr,axis=1),5)))\n# print()","2365dc93":"#Node Feature explainations\n\nimport torch_geometric\nfrom captum.attr import Saliency,IntegratedGradients,GradientShap, FeatureAblation, DeepLift\n\ndef attributation_for_feature_forward(x, data):\n    return model(data)\n\n\ndata_sample = next(iter(DataLoader(model.train_dataset,batch_size=1, shuffle=True, num_workers=8)))\n\n\nfeature_mask = torch.zeros_like(data_sample.x)\n\nig = FeatureAblation(attributation_for_feature_forward)\nattr = ig.attribute(data_sample.x, additional_forward_args=((data_sample,)), target=data_sample.y, baselines=torch.zeros_like(data_sample.x))\n\nprint(\"Node features size: \" + str(list(data_sample.x.size())))\nprint(\"Attributes size: \" + str(list(attr.size())))\nprint()\nprint(\"Minimal value: \" + str(torch.min(attr).item()))\nprint(\"Maximal value: \" + str(torch.max(attr).item()))\n# print()\n# print(\"Top 5 feature indexes: \" + str(torch.topk(torch.mean(attr,axis=0),5)))\n# print()\n# print(\"Top 5 node indexes: \" + str(torch.topk(torch.mean(attr,axis=1),5)))\n# print()","5199dd7f":"if do_train:\n    torch.save(model.state_dict(), \"model.pth\")","e7d5062b":"<h1>MNISTSuperpixels Interpretation<\/h1>"}}