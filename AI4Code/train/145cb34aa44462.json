{"cell_type":{"4fe4e6ea":"code","1323bf66":"code","48c58d2f":"code","a18539a6":"code","4c345a0d":"code","e5063ade":"code","69ac4792":"code","44fcf711":"code","e809328a":"code","86773c42":"code","0948a644":"code","5e19f14c":"code","12478e17":"code","673c33e4":"code","d3b8320a":"code","0134d66c":"code","9b9e1ee6":"code","3cdead67":"code","13bd100b":"code","f1e383be":"code","7e1138f9":"code","4a770d40":"code","212874c0":"code","cb11e8c6":"code","f4593704":"code","1949ed0c":"code","0f95ef8f":"code","f3f60cde":"code","9ed52bb0":"code","4c31c4ac":"code","14631c3a":"code","ee1c702e":"code","834ce4fd":"code","85820b35":"code","d9b6e93f":"code","63b95021":"code","8d1291c7":"code","dd6b9b33":"code","fdb96d89":"code","17f5ce60":"code","b7d33e65":"code","814f7bae":"code","372167df":"code","a4257e6a":"code","57cc3af2":"code","3b96b780":"code","0947ce45":"code","34843838":"code","529b5c66":"code","b956baa9":"code","3e5b9d07":"code","13eab787":"code","6211752c":"code","2063166c":"code","fa695713":"markdown","0352b0e6":"markdown","fc1faa15":"markdown","ba812c8e":"markdown","6145a4d9":"markdown","5eca5275":"markdown","16f05b83":"markdown","3816bed1":"markdown","d441585b":"markdown","45fdb1ec":"markdown","c2524537":"markdown","fb1d6670":"markdown","beacbd24":"markdown","add8b8ff":"markdown","54c07069":"markdown","1dc8171b":"markdown","cbc3d689":"markdown","34bf9a2d":"markdown","1c90f815":"markdown","c3f40f83":"markdown","0d55e729":"markdown","e75c1b6d":"markdown","a411b2e0":"markdown","de6379f0":"markdown"},"source":{"4fe4e6ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1323bf66":"import sys\n\n# Libraries for Data-Visualization:\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling # library for automatic EDA\nimport plotly.graph_objs as go # interactive plotting library\nimport plotly.express as px # interactive plotting library\n!pip install swiftviz\nimport swiftviz as sv\n\nfrom IPython.display import display # display from IPython.display\nfrom itertools import cycle # function used for cycling over values\n#%pip install ppscore # installing ppscore, library used to check non-linear relationships between our variables\n#import ppscore as pps # importing ppscore\n\n# just to check the module version\nimport sklearn\n#import autoviz\n\n# Libraries for selecting the best model:\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_predict, cross_val_score\n\n# Libraries containing different Classifier Algorithms:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\n# Libraries for manipulating the data and finding model-accuracy: \nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn import metrics # metrics contains the given beow functions:-\n                            # roc_curve, roc_auc_score, precision_recall_curve, \n                            # f1_score, recall_score, precision_score, confusion_matrix","48c58d2f":"for module in [np, mpl, pd, sns, sklearn, pandas_profiling]:\n    print(f'{module}:-----{module.__version__}')","a18539a6":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_df_results = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","4c345a0d":"train_df.head()","e5063ade":"test_df.head()","69ac4792":"test_df_results.head()","44fcf711":"cust_palette = sv.CustomPalette()\ncust_palette.display_palette(sns.color_palette())","e809328a":"report = pandas_profiling.ProfileReport(train_df, minimal=True)","86773c42":"# Let's look at the generated report\nreport","0948a644":"def unique_val_print(dataset):\n    for col in dataset.columns:\n        print(f'{col}: {dataset[col].unique()}\\nThe no.of unique entries in {col} are: {len(dataset[col].unique())}\\n')","5e19f14c":"def continuous_unique_val(dataset, col):\n    fig, axs = plt.subplots(nrows=2, figsize=(10, 10));\n    sns.distplot(dataset[col], ax=axs[0])\n    sns.boxplot(dataset[col], ax=axs[1]);","12478e17":"def outlier_finder(dataset, cols):\n    quantiles_limit = dict()\n    for col in cols:\n        quantiles_limit[col]=dict()\n        IQR_15 = 1.5*(np.quantile(dataset[col], 0.75) - np.quantile(dataset[col], 0.25))\n        quantiles_limit[col]['left'] = np.quantile(dataset[col], 0.25) - IQR_15\n        quantiles_limit[col]['right'] = np.quantile(dataset[col], 0.75) + IQR_15\n    for col_name, values in quantiles_limit.items():\n        if values['left'] <= dataset[col_name].min():\n            values['left'] = None\n        if values['right'] >= dataset[col_name].max():\n            values['right'] = None\n    return quantiles_limit","673c33e4":"for dataset in [train_df, test_df, test_df_results]:\n    print(f'{dataset.shape}\\n{\"-\"*10}')","d3b8320a":"for dataset in [train_df, test_df, test_df_results]:\n    print(f'{dataset.info()}\\n{\"-\"*40}\\n')    # The 'none' at the end of every dataframe.info() is because print() does not return anything.","0134d66c":"for dataset in [train_df, test_df]:\n    dataset.drop('Cabin', axis=1, inplace=True)","9b9e1ee6":"print(train_df.columns,'\\n', test_df.columns)","3cdead67":"unique_val_print(train_df[['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']])","13bd100b":"cust_palette.set_default_custom_palette('rocket')\ncontinuous_unique_val(train_df, 'Age')","f1e383be":"cust_palette.set_default_custom_palette('Blues_r')\ncontinuous_unique_val(train_df, 'Fare')","7e1138f9":"temp_train_df = train_df.copy()","4a770d40":"temp_train_df['Age'].fillna(temp_train_df.Age.median(), inplace=True)\ntemp_train_df['Embarked'].fillna(temp_train_df.Embarked.mode()[0], inplace=True)  # mode() returns a pandas series\ntemp_train_df[['Age', 'Embarked']].info()","212874c0":"unique_val_print(temp_train_df[['Age', 'Embarked']])","cb11e8c6":"cust_palette.set_default_custom_palette('Oranges_r')\ncontinuous_unique_val(temp_train_df, 'Age')","f4593704":"train_df_outliers = outlier_finder(temp_train_df, ['Age', 'Fare'])\ntrain_df_outliers","1949ed0c":"unique_val_print(test_df[['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']])","0f95ef8f":"cust_palette.set_default_custom_palette('winter_r')\ncontinuous_unique_val(test_df, 'Age')","f3f60cde":"cust_palette.set_default_custom_palette('RdYlBu')\ncontinuous_unique_val(test_df, 'Fare')","9ed52bb0":"test_df[['Age', 'Fare']] = test_df[['Age', 'Fare']].apply(lambda x: x.fillna(x.median()))\ntest_df.info()","4c31c4ac":"cust_palette.set_default_custom_palette('plasma')\ncontinuous_unique_val(test_df, 'Age')","14631c3a":"test_df_outliers = outlier_finder(test_df, ['Age', 'Fare'])\ntest_df_outliers","ee1c702e":"temp_train_df.columns","834ce4fd":"encoder = sv.Encoder()\nencoded_df, key_dict = encoder.column_encoder(temp_train_df, ['Sex', 'Embarked'])\nencoded_df","85820b35":"key_dict","d9b6e93f":"cat_vars = np.array(['Survived', 'Pclass', 'Sex', 'Embarked'])\n\nsurvived_pie_maker = sv.PieCompositionPlots(encoded_df, 'Survived')\nsex_pie_maker = sv.PieCompositionPlots(encoded_df, 'Sex')\npclass_pie_maker = sv.PieCompositionPlots(encoded_df, 'Pclass')","63b95021":"survived_labels = [['Not Survived - 1', 'Not Survived - 2', 'Not Survived - 3', 'Survived - 1', 'Survived - 2', 'Survived - 3'], \n                   ['Not Survived - Male', 'Not Survived - Female', 'Survived - Male', 'Survived - Female'], \n                   ['Not Survived - S', 'Not Survived - C', 'Not Survived - Q', 'Survived - S', 'Survived - C', 'Survived - Q']]\n\nexplode_survive = [[0, 0, 0.1, 0, 0, 0], [0.1, 0, 0, 0], [0.1, 0, 0, 0, 0, 0]]\nsurvived_pie_maker.pie_plotly(list(cat_vars[cat_vars != 'Survived']), survived_labels, cols=3, width=1000, height=500, explode_list=explode_survive, verbo=0, hole=0.4)        # to get the                                                                                                                                                                                       # calculation steps,                                                                                                                                                                               # set \"verbo=1\"","8d1291c7":"sex_labels = [['Male - 1st Class', 'Male - 2nd Class', 'Male - 3rd Class', 'Female - 1st Class', 'Female - 2nd Class', 'Female - 3rd Class'], \n              ['Male - S', 'Male - C', 'Male - Q', 'Female - S', 'Female - C', 'Female - S']]\n\nexplode_sex = [[0, 0, 0.1, 0, 0, 0], [0.1, 0, 0, 0, 0, 0]]\nsex_pie_maker.pie_plotly(['Pclass', 'Embarked'], sex_labels, width=1000, height=500, explode_list=explode_sex, verbo=0, hole=0.4, color_theme=\"seaborn\")","dd6b9b33":"temp_train_df[temp_train_df.Sex == 'Female'].Embarked == 'Q'","fdb96d89":"pclass_labels = [['1st class - S', '1st class - C', '1st class - Q', '2nd class - S', '2nd class - C', '2rd class - Q', '3rd class - S', '3rd class - C', '3rd class - Q']]\n\nexplode_pclass = [[0, 0, 0.1, 0, 0, 0, 0.1, 0, 0]]\npclass_pie_maker.pie_plotly(['Embarked'], pclass_labels, width=1000, height=500, explode_list=explode_pclass, verbo=0, hole=0.4, color_theme='ggplot2')","17f5ce60":"cust_palette.set_default_custom_palette('tab10')\nfig, axs = plt.subplots(figsize=(24, 15), nrows=3, ncols=2)\n\nplotter = sv.Plotter()\nplot_axis = plotter.complete_axis_generator(['Age', 'Fare'], ['Pclass', 'Sex', 'Embarked'], rows=3, cols=2)\n\nfor hue, y, row, col in plot_axis:\n    row = np.int64(row)\n    col = np.int64(col)\n    sns.swarmplot('Survived', y, data=temp_train_df, hue=hue, ax=axs[row, col])","b7d33e65":"fig = px.imshow(temp_train_df.corr())\nfig.show()","814f7bae":"train_df.columns","372167df":"X_train = train_df.drop(columns='Survived')\ny_train = train_df['Survived']\nX_test = test_df\ny_test = test_df_results['Survived']","a4257e6a":"num_features = ['Age', 'Fare']\ncat_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']","57cc3af2":"num_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('std_scalar', StandardScaler()),\n])\n\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder()),\n])\n\nfeat_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_features),\n    (\"cat\", cat_pipeline, ['Sex', 'Embarked']),\n])","3b96b780":"class FeatureDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, drop_features=None):\n        self.drop_features = drop_features\n        \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        if self.drop_features:\n            return X.drop(columns=self.drop_features)\n        else:\n            return X","0947ce45":"full_pipeline = Pipeline([\n    (\"dropper\", FeatureDropper()),\n    (\"feature_handler\", feat_pipeline),\n    (\"model\", AdaBoostClassifier()),\n])","34843838":"rnd_state = 42\n\nparams_grid = [\n    {\n        \"dropper__drop_features\": [None, [\"PassengerId\", \"Name\", \"Ticket\"]],\n        \"model\": [AdaBoostClassifier()],\n        \"model__base_estimator\": [DecisionTreeClassifier(),\n                                  LogisticRegression(multi_class=\"multinomial\",\n                                                     solver=\"lbfgs\", C=10)],# Implementation of Softmax Regression\n        \"model__n_estimators\": [50, 100, 1000],\n        \"model__random_state\": [rnd_state],\n    },\n    {\n        \"dropper__drop_features\": [None, [\"PassengerId\", \"Name\", \"Ticket\"]],\n        \"model\": [RandomForestClassifier()],\n        \"model__n_estimators\": [100, 1000],\n        \"model__max_depth\": [3, 4, 5, 6, 10, 15, 20],\n        \"model__random_state\": [rnd_state],\n    },\n    {\n        \"dropper__drop_features\": [None, [\"PassengerId\", \"Name\", \"Ticket\"]],\n        \"model\": [GradientBoostingClassifier()],\n        \"model__n_estimators\": [100, 1000],\n        \"model__learning_rate\": [0.001, 0.05, 0.1, 0.5],\n        \"model__random_state\": [rnd_state],\n    },\n    {\n        \"dropper__drop_features\": [None, [\"PassengerId\", \"Name\", \"Ticket\"]],\n        \"model\": [SVC()],\n        \"model__C\": [0.1, 0.5, 1, 1.5, 2],\n        \"model__kernel\": [\"rbf\", \"poly\", \"sigmoid\"],\n        \"model__random_state\": [rnd_state],\n    },\n    {\n        \"dropper__drop_features\": [None, [\"PassengerId\", \"Name\", \"Ticket\"]],\n        \"model\": [XGBClassifier()],\n        \"model__n_estimators\": [100, 500, 1000],\n        \"model__learning_rate\": [0.005, 0.01, 0.05, 0.1],\n        \"model__max_depth\": [3, 4, 5, 6, 10],\n        \"model__gamma\": [0, 1, 5],\n        \"model__random_state\": [rnd_state],\n    },\n]\n\nori_grid = GridSearchCV(full_pipeline, params_grid, cv=5, verbose=2, scoring=\"accuracy\")","529b5c66":"ori_grid.fit(X_train, y_train)","b956baa9":"original_best_model = ori_grid.best_estimator_\noriginal_best_model","3e5b9d07":"y_pred = original_best_model.predict(X_test)\ny_pred","13eab787":"metrics.accuracy_score(y_test, y_pred)","6211752c":"print(metrics.classification_report(y_test, y_pred))\nmetrics.plot_roc_curve(original_best_model, X_test, y_test);","2063166c":"my_submission = pd.DataFrame({'PassengerId': X_test.PassengerId, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","fa695713":"## Survival Composition:","0352b0e6":"# Visualization:","fc1faa15":"### Everything is as needed.","ba812c8e":"#### Now, let's look at columns categorized as continuous - Age, Fare","6145a4d9":"### Taking care of missing values:\n\ntest_df has missing values in columns - 'Age' & 'Fare'","5eca5275":"Displaying the current Color Palette:","16f05b83":"The above given key dictionary is going to be useful to create labels because of how the below functions work, also it is better to know which values got replaced by what. ","3816bed1":"No Females boarded titanic from Queenstown(Q)","d441585b":"# Helper Functions","45fdb1ec":"# Model Selection:","c2524537":"### Insight Summary from composition analysis:\n\n* Pclass = 3 was given least priority, and Pclass = 1 was given maximum priority.\n* Females were given more priority as compared to Males by a large margin. \n* The difference in % of Male and Female passengers is highest for Pclass=1.\n* No Females embarked from Queenstown(Q).\n* Also, majority of the people who embarked from Southampton(S) are male. ","fb1d6670":"# Loading Datasets","beacbd24":"## Test Data:","add8b8ff":"## Sex Composition:","54c07069":"###  *Data is like people \u2013 interrogate it hard enough and it will tell you whatever you want to hear.*","1dc8171b":"### Taking care of Missing values:","cbc3d689":"# Setting up the Environment","34bf9a2d":"## Numerical Variables:","1c90f815":"#### 'Age' and 'Embarked' are the two columns having missing values\nFor 'Age' :\n- We can use any from Median or Mean since we can see there are not too many outlier present but, just to stay clear from effect from any outlier effect\n\nFor 'Embarked' :\n- Embarked contains information from where the passenger got onboard, so we can assume that the port from where the maximum no.of passengers were onboarded is the port which can be used for missing values. Hence Mode, since it resonates more with the feature. (Note, that for categorical attributes, mean and median cannot be found) ","c3f40f83":"### Let's look at the composition of different categorical variables w.r.t each other:\n\n<br>\nBut, before that let's label encode our dataframe:","0d55e729":"**train_df:**\n- Age: 177 null\n- Embarked: 2 null\n\n**test_df:**\n- Age: 86 null\n- Fare: 1 null\n\n*I have not mentioned 'Cabin' since we can already see that the null values exceed non-null. Hence, column will be dropped from both dataframes*","e75c1b6d":"## Pclass Composition:","a411b2e0":"# Cleaning Datasets","de6379f0":"## Train Data:"}}