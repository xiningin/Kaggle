{"cell_type":{"5623c9f6":"code","ff92e957":"code","cfc0785a":"code","336f419d":"code","fab55ccb":"code","7e49854a":"code","ced103cb":"code","45860615":"code","b78c0bb0":"code","633986d7":"code","3e47dc3c":"code","8fe715ec":"code","b8860364":"code","6e208bc7":"code","2799450b":"code","1ac26278":"code","8600ffd1":"code","9ffef85f":"code","7667bc74":"code","eb2805ed":"code","26d49494":"code","5d6b2725":"code","a6671ee3":"code","32cd6fcf":"code","233282ff":"code","f9ba7d92":"code","df9c5fd8":"code","ef099704":"code","6b6a955d":"code","c4da101e":"code","65c2f587":"code","a6d36bda":"code","940fb749":"code","e3bda70a":"code","bf26f5b1":"code","452f6684":"code","743c1eac":"code","138cdddb":"code","090b9c0a":"code","fe840b48":"code","dd6c8472":"code","3fc6060c":"code","6a35a574":"code","a26ddeb9":"code","2f069a54":"code","15e4c461":"code","274d063f":"code","b19c5fb9":"code","4a099d7b":"code","2b3ff2cd":"code","59ea6a48":"code","e3a7048a":"code","e172b5fa":"code","bb184c48":"code","91e8b858":"code","5665d13b":"code","b28ff5b4":"code","c295b169":"code","92d47f03":"code","05e375fa":"code","5ce7bb98":"code","c02a73f2":"code","0bb02a6f":"code","fe8ba554":"code","ef42f7a9":"code","903c54cd":"code","55f031d8":"code","a3f015c2":"code","089f0e7e":"code","ca18f11e":"code","1830ea2f":"code","47f53015":"code","a59c8b5a":"code","ee1959f2":"markdown","7324e514":"markdown","adc06a2d":"markdown","13626d14":"markdown","61ea25d8":"markdown","59805a17":"markdown","adc6c1f9":"markdown","4ff9d5fc":"markdown","845d5dfb":"markdown","54838b81":"markdown","b6cf33bb":"markdown","902cd59c":"markdown","f9a9a729":"markdown","cef89bcf":"markdown","50c75e85":"markdown","59d96d57":"markdown","5df62dde":"markdown","95243f01":"markdown","161e3a06":"markdown","415a8b25":"markdown","8cffb9ac":"markdown","98643da0":"markdown","a8790b1d":"markdown","95f56088":"markdown","3257b750":"markdown","28114668":"markdown"},"source":{"5623c9f6":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nfrom sklearn.feature_selection import mutual_info_classif\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier","ff92e957":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.head()","cfc0785a":"train.shape","336f419d":"train.drop_duplicates().shape # default : inplace=False. ","fab55ccb":"test.shape","7e49854a":"train.info()","ced103cb":"from collections import Counter\nCounter(train.dtypes.values)","45860615":"data = []\nfor f in train.columns:\n    # Defining the role\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n        \n    # Defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f =='id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n        \n    # Defining the data type\n    dtype = train[f].dtype\n    \n    # Creating a Dcit that contains all the metadata for the variable\n    f_dict = {\n        'varname':f,\n        'role':role,\n        'level':level,\n        'keep':keep,\n        'dtype':dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname','role','level','keep','dtype'])\nmeta.set_index('varname',inplace=True)    ","b78c0bb0":"meta","633986d7":"meta[(meta.level=='nominal')&(meta.keep)].index","3e47dc3c":"pd.DataFrame({'count':meta.groupby(['role','level'])['role'].size()}).reset_index()","8fe715ec":"v = meta[(meta.level == 'interval')&(meta.keep)].index\ntrain[v].describe()","b8860364":"train['target'].value_counts().plot.pie(autopct='%1.1f%%')\nplt.title('Distribution of target variable')\nplt.show()","6e208bc7":"# \uba4b\uc788\uac8c \ud558\ub824\uba74... (\uc4f8\ub370\uc5c6\uc5b4\ubcf4\uc784\u314e)\ndata = [go.Bar(x=train['target'].value_counts().index.values,y=train['target'].value_counts().values, text='Distribution of target variable')]\nlayout = go.Layout(title='Target variable distribution')\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig,filename='basic-bar')","2799450b":"desired_apriori = 0.10\n\n# Get the indices per target value\nidx_0 = train[train.target==0].index\nidx_1 = train[train.target==1].index\n\n# Get original number of records per target value\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*nb_1)\/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0 : {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nfrom sklearn.utils import shuffle\n\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = train.loc[idx_list].reset_index(drop=True)","1ac26278":"train['target'].value_counts().plot.pie(autopct='%1.1f%%')\nplt.title('Distribution of target variable')\nplt.show()","8600ffd1":"train.isnull().any().any()","9ffef85f":"train_check_null = train\ntrain_check_null = train_check_null.replace(-1,np.NaN)\n\nimport missingno as msno\nmsno.matrix(train_check_null.iloc[:,4:35],figsize=(16,9),color=(0.3,0.1,0.2))","7667bc74":"\nmissing_col = []\nfor c in train_check_null.columns:\n    if train_check_null[c].isnull().sum() > 0:\n        missing_col.append(c)\n        print('col : {:<15}, Nan records : {:>6}, Nan ratio : {:.3f}'.format(c, train_check_null[c].isnull().sum(), 100*(train_check_null[c].isnull().sum()\/train_check_null[c].shape[0])))","eb2805ed":"meta.loc[missing_col,'level']","26d49494":"# dropping the variables\ntrain.drop(['ps_car_03_cat','ps_car_05_cat'],inplace=True, axis=1)\n# updating the meta\nmeta.loc[['ps_car_03_cat','ps_car_05_cat'],'keep']=False","5d6b2725":"f,ax = plt.subplots(1,2,figsize=(12,6))\n\nsns.distplot(train['ps_reg_03'],ax=ax[0])\nsns.distplot(train['ps_car_14'],ax=ax[1])","a6671ee3":"# Imputing with the mean or mode\nmean_imp = Imputer(missing_values=-1,strategy='mean',axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()","32cd6fcf":"tmp = ['ps_car_01_cat','ps_car_02_cat',\n       'ps_car_07_cat','ps_car_09_cat']\n\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()\ntrain['ps_ind_02_cat'] = mode_imp.fit_transform(train[['ps_ind_02_cat']]).ravel()\ntrain['ps_ind_04_cat'] = mode_imp.fit_transform(train[['ps_ind_04_cat']]).ravel()\ntrain['ps_ind_05_cat'] = mode_imp.fit_transform(train[['ps_ind_05_cat']]).ravel()\n\nfor c in tmp:\n    train[c] = mode_imp.fit_transform(train[[c]]).ravel()\n\n\n\n# Serires.ravel(order='C') \n# Return the flattened underlying data as an ndarray. \n# so, the ps_reg_03 column case : (216940, 1) --> (216940,)","233282ff":"v = meta[(meta.level=='nominal')&(meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0] # == nuique() \n    print('col:{:<10}   distinct values count:{}'.format(f,dist_values))","f9ba7d92":"# Script by https:\/\/www.kaggle.com\/ogrellier\n# Code: https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/225952\/7441\/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean #agg = aggregate(func_or_funcs)\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","df9c5fd8":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)","ef099704":"train_encoded.head()","6b6a955d":"train['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","c4da101e":"# if you use one-hot encoding\n# xx = train['ps_ind_02_cat'].values.reshape(-1,1)\n# xx = ohe.fit_transform(xx).toarray()\n# tt = pd.concat([train.drop('ps_ind_02_cat',axis=1,inplace=True), pd.DataFrame(xx)],axis=1)\n\n# I'll create dummy variables\nfor col in v:\n    train = pd.concat([train.drop(col,axis=1),pd.get_dummies(train[col], prefix='dum_'+col)],axis=1)","65c2f587":"train[ 'dum_ps_car_11_cat_104']","a6d36bda":"X_train = train.drop(['id','target'],axis=1)\ny_train = train.target\n\nrf = RandomForestClassifier(n_jobs=-1)\nrf.fit(X_train, y_train)","940fb749":"rf.score(X_train, y_train)","e3bda70a":"# create metadata table\n\ndata = []\nfor f in test.columns:\n    # Defining the role\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n        \n    # Defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f =='id':\n        level = 'nominal'\n    elif test[f].dtype == float:\n        level = 'interval'\n    elif test[f].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n        \n    # Defining the data type\n    dtype = test[f].dtype\n    \n    # Creating a Dcit that contains all the meta_test_testdata for the variable\n    f_dict = {\n        'varname':f,\n        'role':role,\n        'level':level,\n        'keep':keep,\n        'dtype':dtype\n    }\n    data.append(f_dict)\n    \nmeta_test = pd.DataFrame(data, columns=['varname','role','level','keep','dtype'])\nmeta_test.set_index('varname',inplace=True)    ","bf26f5b1":"# Imputing or drop on missing columns\ntest_check_null = test\ntest_check_null = test_check_null.replace(-1,np.NaN)\n\nmissing_col_test = []\nfor c in test_check_null.columns:\n    if test_check_null[c].isnull().sum() > 0:\n        missing_col_test.append(c)\n        print('col : {:<15}, Nan records : {:>6}, Nan ratio : {:.3f}'.format(c, test_check_null[c].isnull().sum(), 100*(test_check_null[c].isnull().sum()\/test_check_null[c].shape[0])))","452f6684":"missing_col == missing_col_test","743c1eac":"# dropping the variables\ntest.drop(['ps_car_03_cat','ps_car_05_cat'],inplace=True, axis=1)\n# updating the meta\nmeta_test.loc[['ps_car_03_cat','ps_car_05_cat'],'keep']=False\n\n# Imputing values - mean\ntest['ps_reg_03'] = mean_imp.fit_transform(test[['ps_reg_03']]).ravel()\ntest['ps_car_14'] = mean_imp.fit_transform(test[['ps_car_14']]).ravel()\n\n# Imputing values - mode\ntmp = ['ps_car_01_cat','ps_car_02_cat','ps_car_11','ps_ind_02_cat',\n       'ps_car_07_cat','ps_car_09_cat','ps_ind_04_cat','ps_ind_05_cat']\n\nfor c in tmp:\n    test[c] = mode_imp.fit_transform(test[[c]]).ravel()","138cdddb":"# create dummy variables\nv = meta_test[(meta_test.level=='nominal')&(meta_test.keep)].index\n\nfor f in v:\n    dist_values = test[f].value_counts().shape[0] # == nuique() \n    print('col:{:<10}   distinct values count:{}'.format(f,dist_values))","090b9c0a":"# I'll create dummy variables\nfor col in v:\n    test = pd.concat([test.drop(col,axis=1),pd.get_dummies(test[col], prefix='dum_'+col)],axis=1)","fe840b48":"test.head(2)","dd6c8472":"X_test = test.drop('id',axis=1)\n\npredicted = rf.predict_proba(X_test)","3fc6060c":"def gini(actual, pred, cmpcol = 0, sortcol = 1):  \n    assert( len(actual) == len(pred) )  \n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)  \n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]  \n    totalLosses = all[:,0].sum()  \n    giniSum = all[:,0].cumsum().sum() \/ totalLosses  \n    giniSum -= (len(actual) + 1) \/ 2.  \n    return giniSum \/ len(actual)  \n\n    def gini_normalized(a, p):  \n        return gini(a, p) \/ gini(a, a)  \n    \n    def test_gini():\n        def fequ(a,b):  \n            return abs( a -b) < 1e-6  \n        def T(a, p, g, n):  \n            assert( fequ(gini(a,p), g) )  \n            assert( fequ(gini_normalized(a,p), n) )  \n        T([1, 2, 3], [10, 20, 30], 0.111111, 1)  \n        T([1, 2, 3], [30, 20, 10], -0.111111, -1)  \n        T([1, 2, 3], [0, 0, 0], -0.111111, -1)  \n        T([3, 2, 1], [0, 0, 0], 0.111111, 1)  \n        T([1, 2, 4, 3], [0, 0, 0, 0], -0.1, -0.8)  \n        T([2, 1, 4, 3], [0, 0, 2, 1], 0.125, 1)  \n        T([0, 20, 40, 0, 10], [40, 40, 10, 5, 5], 0, 0)  \n        T([40, 0, 20, 0, 10], [1000000, 40, 40, 5, 5], 0.171428,0.6)  \n        T([40, 20, 10, 0, 0], [40, 20, 10, 0, 0], 0.285714, 1)  \n        T([1, 1, 0, 1], [0.86, 0.26, 0.52, 0.32], -0.041666, -0.333333)","6a35a574":"# gini(y_train,predicted)\n# print(len(y_train),len(predicted))","a26ddeb9":"pd.DataFrame(predicted).head()","2f069a54":"submission = pd.DataFrame({'id':test['id'], 'target':pd.DataFrame(predicted)[1]})\nsubmission.head(2)","15e4c461":"submission.set_index('id',inplace=True)\nsubmission.head(2)","274d063f":"submission.to_csv('submission.csv',index=False)","b19c5fb9":"pd.read_csv('.\/submission.csv').head()","4a099d7b":"# from sklearn.metrics import accuracy_score\n\n# predicted = rf.predict(X_test)\n# accuracy = accuracy_score(y_test, predicted)\n\n# print(f'Out-of-bag score estimate: {rf.oob_score_:.3}')\n# print(f'Mean accuracy score: {accuracy:.3}')","2b3ff2cd":"v = meta[(meta.level=='nominal')&(meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig,ax = plt.subplots(figsize=(20,10))\n    # Calculate the percentage of taget=1\n    cat_perc = train[[f,'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target',ascending=False,inplace=True)\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","59ea6a48":"def corr_heatmap(v):\n    correlations = train[v].corr()\n    \n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220,10,as_cmap=True)\n    \n    fig,ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations,cmap=cmap,vmax=1.0,center=0,fmt='.2f',square=True,\n               linewidths=.5, annot=True, cbar_kws={'shrink':.75})\n    plt.show();\n    \nv = meta[(meta.level=='interval')&(meta.keep)].index\ncorr_heatmap(v)","e3a7048a":"s = train.sample(frac=0.1)","e172b5fa":"sns.lmplot(x='ps_reg_02',y='ps_reg_03',data=s,hue='target',palette='Set1',scatter_kws={'alpha':0.3})\nplt.show()","bb184c48":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","91e8b858":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","5665d13b":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","b28ff5b4":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","c295b169":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","92d47f03":"train_float = train.select_dtypes(include=['float64'])\ntrain_int = train.select_dtypes(include=['int64'])","05e375fa":"train_int.columns","5ce7bb98":"colormap = plt.cm.YlGnBu\nplt.figure(figsize=(16,12))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(train_float.corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","c02a73f2":"data = [\n    go.Heatmap(\n        z= train_int.corr().values,\n        x=train_int.columns.values,\n        y=train_int.columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n        text = True ,\n        opacity = 1.0 )\n]\n\nlayout = go.Layout(\n    title='Pearson Correlation of Integer-type features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')","0bb02a6f":"\n# mutual_info_classif(X,y) X:Feature matrix, y:Target vector\n# Estimate mutual information for a discrete target variable.\n# 0 : \ub450 \ubcc0\uc218\uac00 \ub3c5\ub9bd\uc801, higher value : \ub192\uc740 \uc885\uc18d\uc131\n# \ub9ac\ud134 : \uac01 feature\uc640 target\ubcc0\uc218 \uac04 \uc0c1\ud638\uc815\ubcf4\n# relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances\nmf = mutual_info_classif(train_float.values, train.target.values, n_neighbors=3, random_state=1)\nprint(mf)\n# low dependency !","fe8ba554":"mutual_info_classif(train_int.values, train.target.values, n_neighbors=3, random_state=1)\n","ef42f7a9":"bin_col = [col for col in train.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col]==0).sum())\n    one_list.append((train[col]==1).sum())","903c54cd":"trace1 = go.Bar(\n    x=bin_col,\n    y=zero_list ,\n    name='Zero count'\n)\ntrace2 = go.Bar(\n    x=bin_col,\n    y=one_list,\n    name='One count'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack',\n    title='Count of 1 and 0 in binary variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='stacked-bar')","55f031d8":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4, max_features=0.2,\n                           n_jobs=-1, random_state=0)\n# n_estimator : \ud2b8\ub9ac \uac2f\uc218\n# min_samples_leaf : \uac01 \ub178\ub4dc\uac00 \uac00\uc9c0\ub294 \ucd5c\uc18c \uc0d8\ud50c \uc218\n# max_features : \uac12\uc774 0.2 float\uc774\uae30\ub54c\ubb38\uc5d0 int(max_features * n_features). split\ud560\ub54c \uace0\ub824\ud558\ub294 feature \uc218\n# n_jobs : number or jobs. -1\uc774\uba74 \uc804\ubd80. default 1\nrf.fit(train.drop(['id','target'],axis=1), train.target)\nfeatures = train.drop(['id','target'],axis=1).columns.values\nprint('----Training Done----')","a3f015c2":"# scatter plot\ntrace = go.Scatter(\n    y = rf.feature_importances_,\n    x = features,\n    mode = 'markers',\n    marker = dict(\n        sizemode='diameter',\n        sizeref =1,\n        size=13,\n        color=rf.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = features\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n","089f0e7e":"x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features), \n                                                            reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Random Forest Feature importance',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of Feature importances',\n     width = 900, height = 2000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n#         domain=[0, 0.85],\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","ca18f11e":"from sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image,ImageDraw, ImageFont\nimport re\n\ndecision_tree = tree.DecisionTreeClassifier(max_depth=3)\ndecision_tree.fit(train.drop(['id','target'],axis=1), train.target)\n\n# Export ourt trained model as a .dot file\nwith open('tree1.dot','w') as f:\n    f = tree.export_graphviz(decision_tree,out_file=f,max_depth=4,impurity=False,\n                             feature_names=train.drop(['id','target'],axis=1).columns.values,\n                            class_names=['No','Yes'], rounded=True, filled=True)\n    \n# Conver .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n\n# Annotating chart with PIL\nimg = Image.open('tree1.png')\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage('sample-out.png')","1830ea2f":"\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(n_estimators=100, max_depth=3, \n                               min_samples_leaf=4, max_features=0.2, random_state=0)\ngb.fit(train.drop(['id', 'target'],axis=1), train.target)\nfeatures = train.drop(['id', 'target'],axis=1).columns.values\nprint(\"----- Training Done -----\")\n\n# n_estimator : The number of boosting stages to perform.\n# Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.","47f53015":"# Scatter plot \ntrace = go.Scatter(\n    y = gb.feature_importances_,\n    x = features,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        #size= rf.feature_importances_,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = gb.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = features\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Machine Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","a59c8b5a":"x, y = (list(x) for x in zip(*sorted(zip(gb.feature_importances_, features), \n                                                            reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Gradient Boosting Classifer Feature importance',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of Feature importances',\n     width = 900, height = 2000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","ee1959f2":"## Handling imbalanced classes\nundersampling records with target = 0","7324e514":"## Exploratory Data Visualization","adc06a2d":"For the ordinal variables we do not see many correlations. We could, on the other hand, look at how the distributions are when grouping by the target value.\n\n\n## Feature engineering\nCreating dummy variables  \nThe values of the categorical variables do not represent any order or magnitude. For instance, category 2 is not twice the value of category 1. Therefore we can create dummy variables to deal with that. We drop the first dummy variable as this information can be derived from the other dummy variables generated for the categories of the original variable.","13626d14":"## Creating interaction variables","61ea25d8":"## Target encoding for categorical features\n\n\nBert Carremans do below job(target encoding) but i won't. I will transform values using dummy variables.","59805a17":"## Correlation plots\n\uc5f4\uc744 float\uacfc int\ub85c \ub098\ub220\uc11c \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uac70\ub4e0. \uc65c\uc774\ub807\uac8c \ud558\ub098 \uad81\uae08\ud588\ub294\ub370  \ntrain_int \ub97c \ubcf4\uba74 \ub300\ubd80\ubd84\uc774 _cat \uc774\ub098 _bin_ \uc778 \ubc94\uc8fc\ud615. boolean \ud0c0\uc785\uc774\uc57c.  \n\uadf8\ub798\uc11c correlation against continous values \ub97c \ubc14\ub85c \uacc4\uc0b0\ud560 \uc218 \uc5c6\uae30\ub54c\ubb38\uc5d0 \ub530\ub85c \ub098\ub234\ub300!","adc6c1f9":"1. null \uccb4\ud06c --> 3\uac1c \uc5f4\uc5d0\uc11c missing\uc774 \ub9ce\uc74c (ps_reg_03, ps_car_03_cat, ps_car_05_cat)\n2. \uc885\uc18d\ubcc0\uc218 \ubc38\ub7f0\uc2a4 \uccb4\ud06c --> imbalanced (0 : 1 = 96% : 4%)\n3. \ub3c5\ub9bd\ubcc0\uc218\ub4e4 \uac04\uc758 \uc0c1\uad00\uad00\uacc4 \uccb4\ud06c \n    --> float\uc5f4 : \ub300\ubd80\ubd84 0. \n        \uac15\ud55c \uc0c1\uad00\uad00\uacc4 : (ps_reg_01, ps_reg_03)  \n                        (ps_reg_02, ps_reg_03)  \n                        (ps_car_12, ps_car_13)\n                        (ps_car_13, ps_car_15)\n    --> int\uc5f4 : \ub300\ubd80\ubd84 0\n        \uc74c\uc218\uc778 \uc0c1\uad00\uad00\uacc4\ub3c4 \uc788\uc74c : ps_ind_06_bin, ps_ind_07_bin, ps_ind_08_bin, ps_ind_09_bin\n        \n4. \ub3c5\ub9bd\ubcc0\uc218\uc640 \uc885\uc18d\ubcc0\uc218 \uac04 \uc885\uc18d\uc131 \uccb4\ud06c\n5. \uc774\uc9c4\ubcc0\uc218 \ubd84\ud3ec \uccb4\ud06c\n6. \ubc94\uc8fc\ud615 \ubcc0\uc218 \uc911\uc694\ub3c4 \uccb4\ud06c\n    - RandomForest\n    - Decision Tree\n    - Gradient Boosting\n","4ff9d5fc":"## Target variable inspection\nimbalanced","845d5dfb":"## Data Quality Check\nChecking missing values","54838b81":"**Metadata**","b6cf33bb":"## Feature importance via Random Forest\n\n\nRandomForest : \n- quick\n- does not require much parameter turning in obtaining useful feature importances\n- pretty robust to target imbalances.","902cd59c":"## Feature importance via DecisionTree\n","f9a9a729":" For classification problems, we can conveniently call Sklearn's mutual_info_classif method which measures the **dependency between two random variables** and ranges from zero (where the random variables are independent of each other) to higher values (indicate some dependency). ","cef89bcf":"### check duplicate rows\npandas.DataFrame.drop_duplicates : Return DataFrame with duplicate rows removed.","50c75e85":"Allright, so now what? How can we decide which of the correlated variables to keep? We could perform Principal Component Analysis (PCA) on the variables to reduce the dimensions. In the AllState Claims Severity Competition I made this kernel to do that. But as the number of correlated variables is rather low, we will let the model do the heavy-lifting.  \n\n\n## Checking the correlations between ordinal variables","59d96d57":"## Interval variables\nChecking the correlations between interval variables.","5df62dde":"No duplicate rows, so that's fine","95243f01":"There are a strong correlations between the variables:  \nps_reg_02 and ps_reg_03 (0.7)  \nps_car_12 and ps_car13 (0.67)  \nps_car_12 and ps_car14 (0.58)  \nps_car_13 and ps_car15 (0.67)  ","161e3a06":"ps_car_03_cat , ps_car_05_cat : \uacb0\uce21\uce58\uac00 \ub108\ubb34 \ub9ce\uc74c --> \uc81c\uac70  \nps_reg_03,ps_car_14 : interval  --> \ubd84\ud3ec \ubcf4\uace0 \uacb0\uc815  \nps_car_11 : ordinal  --> \ucd5c\ube48\uac12  \n\ub098\uba38\uc9c0 : nominal --> \ucd5c\ube48\uac12","415a8b25":"Example to extract all nomial variables that are not dropped","8cffb9ac":"### Descriptive statistics\n\nTo keep things clear, we'll do this per data type  \n**Interval variables**","98643da0":"## Checking the cardinality of the categorical variables","a8790b1d":"Luckly, missing value occurs in the same columns.","95f56088":"## RandomForest \n\nUsing RandomForest without changing the default.","3257b750":"so you can find ..  \nex) ps_reg_03 has missing value (cause it has value -1)","28114668":"## Feature importance via Gradient Boosting model\n\nGradient Boosting proceeds in a forward stage-wise fashion,  \nwhere at each stage regression tress are fitted on the gradient of the loss function  \n(which defaults to the deviance in Sklearn implementation).\n\nfoward stage-wise : \ubbf8\ubd84\uac00\ub2a5\ud55c \uc190\uc2e4\ud568\uc218\uc758 \ucd5c\uc801\ud654\ub97c \uac00\ub2a5\ucf00\ud568"}}