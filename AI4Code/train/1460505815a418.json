{"cell_type":{"f10bd01b":"code","cea3434d":"code","5bd3d8fa":"code","491c7cbe":"code","928cf7b3":"code","7db63f17":"code","ba3ef755":"code","1ce464d1":"code","bb35e68e":"code","ea9352ac":"code","9175a3a4":"code","cce93441":"code","6215e2f8":"code","3336a41b":"code","33bf8b55":"code","afad42d6":"code","585a260f":"code","dd340442":"code","7cce8bf0":"code","ea46c3f1":"code","c224e704":"code","e2b95690":"code","1b48f3ca":"code","afdce64b":"code","1388d1e3":"code","b87c6db7":"code","7e354189":"code","675a5e33":"code","b3f5751e":"code","ae1b48ef":"code","bf948818":"code","d5075a99":"code","bfb97f3f":"code","d98285a9":"code","886da408":"code","d686784c":"code","6388554e":"code","632707ff":"code","bd6e6579":"code","508cd3e4":"code","e346cb10":"code","3366aca5":"code","58f52355":"code","c063e0c8":"code","3a7dccb8":"code","53360c43":"code","3869d85f":"code","a32d70db":"code","fadc2563":"code","67bd5a04":"code","5c8a7c80":"code","5b2e0675":"code","919a3e7d":"code","404dfe05":"code","77045a98":"code","e591c0af":"code","efffa23e":"code","91185b38":"code","60592808":"code","935e3e3b":"code","c3044d0d":"code","17875191":"code","5373b4e9":"code","a1d2a79a":"code","e8d032f5":"code","b289efa3":"code","45b06f5d":"code","ddda60b8":"code","afc3b707":"code","62bd1657":"code","bd1bf95c":"code","856d64fd":"code","5706ce25":"code","87c662c7":"code","776aad96":"code","b108cea0":"code","be07c4c1":"code","70b87432":"code","0cdd085d":"code","724c0c50":"code","e26a4117":"code","7fc55530":"code","364f4628":"code","87a4e0c7":"code","de68888f":"code","fbd2ff2f":"code","36425f2c":"code","e8c4d448":"code","65d9444b":"code","cce86353":"code","3ccf46ee":"code","d32355f3":"code","2955bf2d":"code","33f28063":"code","e81bf9f7":"code","20a1a132":"code","854c9e02":"code","5a9900a0":"code","7bfe515e":"code","0cf180b4":"code","5a635241":"code","506362de":"code","43e1428c":"code","d4f15db4":"code","9d8ad389":"code","1d9f0db8":"code","194d197b":"code","07865240":"code","d8169374":"code","c7068aee":"code","3b8ba43d":"code","89e39a09":"code","2862e4d6":"code","109dba2c":"code","97bc68c2":"markdown","460e4ca1":"markdown","ad93592a":"markdown","30a197d4":"markdown","cebb2c44":"markdown","514a2060":"markdown","d5b19aad":"markdown","e41b21b7":"markdown","d6cd13c3":"markdown","bea7449c":"markdown","6f91af16":"markdown","d5af6f25":"markdown","8df57e84":"markdown","67ebc599":"markdown","c9c896f7":"markdown","e0d4ef62":"markdown","b2bc4c4c":"markdown","5f691ac5":"markdown","6858336e":"markdown","a20baa65":"markdown","03704038":"markdown","e5e3742b":"markdown","5159cda6":"markdown","191fdc59":"markdown","e6521f0e":"markdown","b8791d0a":"markdown","7455fd46":"markdown","43f5d82a":"markdown"},"source":{"f10bd01b":"# Import the Relevent Packages\nimport pandas as pd\nimport os\nimport re\nimport math\nimport statistics as sts\nimport datetime as dt\nimport time\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport spacy\nspacy_model = spacy.load('en_core_web_sm')\n\nimport random as rd\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud","cea3434d":"tweets = pd.read_csv('..\/input\/donald-trump-tweets-dataset\/tweets.csv')\ntweets.head(5)","5bd3d8fa":"# 1) An example of a standard tweet.\ntweets.loc[54],tweets.loc[(54,'text')]","491c7cbe":"# 2) An example of a retweet.\ntweets.loc[942],tweets.loc[(942,'text')]","928cf7b3":"# 3) An example of a qoute tweet.\ntweets.loc[23186], tweets.loc[(23186,'text')]","7db63f17":"# A Funtion to extract info from quote tweets.\n\nisQuoteTweet = []\nquotedFrom = []\nquoteText = []\ntrumpResp = []\n\npattern1 = re.compile(r'\"\"\"(@.+):(.+\"\")(.*)')\npattern2 = re.compile(r'\"\"\"(@.+) (.+\")')\n\nfor i in range(len(tweets)):\n    matches1 = pattern1.finditer(tweets.loc[(i,'text')])\n    initCheck = len(isQuoteTweet)\n    for match1 in matches1:\n        isQuoteTweet.append('t1')\n        quotedFrom.append(match1.group(1))\n        quoteText.append(match1.group(2))\n        trumpResp.append(match1.group(3))\n    if initCheck == len(isQuoteTweet):\n        matches2 = pattern2.finditer(tweets.loc[(i,'text')])\n        for match2 in matches2:\n            isQuoteTweet.append('t2')\n            quotedFrom.append(match2.group(1))\n            quoteText.append(match2.group(2))\n            trumpResp.append('n\/a')\n    if initCheck == len(isQuoteTweet):\n        isQuoteTweet.append('f')\n        quotedFrom.append('n\/a')\n        quoteText.append('n\/a')\n        trumpResp.append('n\/a')\n\ntweets['isQuoteTweet'] = isQuoteTweet\ntweets['quotedFrom'] = quotedFrom\ntweets['quoteText'] = quoteText\ntweets['trumpResp'] = trumpResp","ba3ef755":"# Extract Hashtags and mentions from tweets\ntweets['mentions'] = tweets['text'].str.findall(r'(?:(?<=\\s)|(?<=^))@.*?(?=\\s|$)')\ntweets['hashtags'] = tweets['text'].str.findall(r'(?:(?<=\\s)|(?<=^))#.*?(?=\\s|$)')","1ce464d1":"# A simple application of the datetime module.\nyear = []\nmonth = []\nday = []\nhour = []\ndate = []\nfor i in range(len(tweets)):\n    datestamp = dt.datetime.strptime(tweets.loc[(i,'date')], '%Y-%m-%d %H:%M:%S')\n    year.append(datestamp.date().year)\n    month.append(datestamp.date().month)\n    day.append(datestamp.date().day)\n    hour.append(datestamp.time().hour)\n    date.append(datestamp.date())","bb35e68e":"# Append the date information to our dataframe\ntweets['year'] = year\ntweets['month'] = month\ntweets['day'] = day\ntweets['hour'] = hour\ntweets['date'] = date","ea9352ac":"# Cleaing features from the tweets\nprocessed_features = []\nfor sentence in tweets['text']:\n    # Remove all the http: urls\n    processed_feature = re.sub('(https?:\/\/\\S+)', '', str(sentence))\n    \n    # Remove all the special characters\n    processed_feature = re.sub(r'\\W', ' ', processed_feature)\n \n    #Converting to Lowercase\n    processed_feature = processed_feature.lower()\n    \n    processed_features.append(processed_feature)","9175a3a4":"# Create a function to get the subjectivity\ndef getSubjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\n# Create a function to get the polarity\ndef getPolarity(text):\n    return  TextBlob(text).sentiment.polarity\n\n\n# Create two new columns 'Subjectivity' & 'Polarity'\ntweets['subjectivity'] = pd.Series(processed_features).apply(getSubjectivity)\ntweets['polarity'] = pd.Series(processed_features).apply(getPolarity)","cce93441":"def getAnalysis(score):\n    if score < 0:\n        return 'negative'\n    elif score == 0:\n        return 'neutral'\n    else:\n        return 'positive'\ntweets['analysis'] = tweets['polarity'].apply(getAnalysis)","6215e2f8":"# An example of our Sentiment Analysis.\ntweets[['text','subjectivity','polarity','analysis']].loc[944]","3336a41b":"# Removing stopwords\nstop_words = ['rt','s','t','amp','u','m','w','p','c',' ','  ','   ']\nfor stopword in stop_words:\n    lexeme = spacy_model.vocab[stopword]\n    lexeme.is_stop = True","33bf8b55":"#Lemmanize\n#Note that this block can take ~10 min to run.\nstart = time.time()\ncleanTweet = []\nfor tweet in processed_features:\n    tweet = spacy_model(tweet)\n    tokenTweet = []\n    for token in tweet:\n        if not token.is_punct and not token.is_stop and not token.like_num and token.lemma_ != '-PRON-':\n                tokenTweet.append(token.lemma_)\n    cleanTweet.append(tokenTweet)\nprint('Time to Run:',(time.time() - start)\/60, 'minutes')","afad42d6":"cleanTweet[5]","585a260f":"# Create our Clean tweet  column.\ntweets['cleanText']=cleanTweet","dd340442":"# Creates a list of all tokens (words) used, accross all tweets.\nwordList = []\nfor tweet in tweets['cleanText']:\n    for word in tweet:\n        wordList.append(word)\n\nwordSeries = pd.Series(wordList)","7cce8bf0":"# Number of Tokens\nlen(wordSeries.value_counts())","ea46c3f1":"# Percentage of the dataset the top 500 terms cover.\nwordSeries.value_counts()[1:500].sum()\/wordSeries.value_counts()[1:].sum()","c224e704":"wordSeries.value_counts()[1:20]","e2b95690":"# Sorted Category of tokens. \nselfReference = ['realdonaltrump','president','trump','donald','trump2016','maga','makeamericagreatagain','potus','teamtrump','donaldtrump']\nusa = ['contry','america','state','american','united','states','national','usa']\ngovernment = ['whitehouse','senate','senator','congress','office','govenor','washington','administration',\n             'government','federal','congressman','impeachment']\ndemocrates = ['obama','barackobama','democrats','joe','biden','hillary','clinton','dem','democrat',\n             'obamacare','schiff','nancy','pelosi','comey','bernie']\nrepublicans = ['republican','gop','mittromney','bush','cruz','gopchairwomen']\nelection = ['election','poll','campaign','party','debate','bill','voter','elect','vote','endorsement','primary','candidate','ballot']\npositive = ['great','good','big','win','new','love','true','strong','support','amazing','hope',\n           'happy','agree','friend','wow','totally','wonderful','beautiful','success','important','fantastic',\n           'incredible','tremendous','smart','winner','champion','protect']\nnegative = ['bad','fight','fail','lie','wrong','crooked','kill','terrible','corrupt','sad','disaster','loser','hoax',\n           'hate','lose','sleepy','phoney','rig','fake','attack','destroy','radical']\nnews = ['news','medium','rating','foxandfriend','foxnew','foxnews','fox','seanhannity','cnn','nbc']\nlaw = ['law','justice','drug','fraud','police','crime','security','illegal','enforcement','criminal','military','fbi','investigation',\n      'court','mueller','war','power']\nborder = ['border','build','wall','immigration']\neconomic = ['people','job','work','buisness','money','stock','tariff','cost','oil','tax','economy','economic','unemployment','dollar']\nstates = ['california','florida','carolina','texas','pennsylvania','iowa']\ncountries = ['china','mexico','iran','korea']\n\nbucket = ['twitter','supporter','isis','thank','run','world','right','report','history','leader','honor',\n          'live','family','forward','fact','believe','open','course','order','donaldjtrumpjr','help',\n          'record','give','presedential','political',\n         'stand','cut','golf','join','problem','women','complete','appretice','celebapprentice','apprencticebc','healthcare'\n         'case','city','fire','spend','hit','speech','nation',\n         'miss','tough','meeting','witch','hunt','white','crowd','release','massive','proud','rally','collusion',\n         'leadership','general','rate','buy',\n         'respect','question','price','vet','ammendment','hotel','ivankatrump','politician','truth','coronavirus',\n         'energy','policy','market','small','large','focus','entrepreneur','god','worker','early','raise','safe',\n         'million','tweet','celebrity','check','little','deliver','deal']","1b48f3ca":"# Records the index of tweets which contain 'term'.\n# Keep in mind, term should be the 'lemmanized' version of the word (ie. rig vs. rigged) as per text cleaning.\ndef searchTerm(term):\n    catcher = []\n    for i in range(len(tweets)):\n        if term in tweets.loc[(i,'cleanText')]:\n             catcher.append(tweets.loc[(i,'text')])\n    return pd.Series(catcher)","afdce64b":"# Example of search term\nsample = searchTerm('rig')\nsample[3]","1388d1e3":"# Creates a new column, with values 0,1 based on if given tweet contains one of terms in category bucket.\ndef createCategory(bucket, name):\n    start = time.time()\n    category = []\n    for i in range(len(tweets)):\n        j = 0\n        for word in bucket:\n            if j == 0 and word in tweets.loc[(i,'cleanText')]:\n                j = 1\n        if j == 0:\n            category.append(0)\n        else:\n            category.append(1)\n    tweets[name] = category ","b87c6db7":"# This code block should only take a few minutes.\nstart = time.time()\n\ncreateCategory(selfReference, 'selfReference')\ncreateCategory(usa,'usa')\ncreateCategory(government,'government')\ncreateCategory(democrates,'democrates')\ncreateCategory(republicans,'republicans')\ncreateCategory(election,'election')\ncreateCategory(positive,'positive')\ncreateCategory(negative,'negative')\ncreateCategory(news,'news')\ncreateCategory(law,'law')\ncreateCategory(border,'border')\ncreateCategory(economic,'economic')\ncreateCategory(states,'states')\ncreateCategory(countries,'countries')\n\nprint('Total Time:',time.time() - start)","7e354189":"rd.seed(0)\nx = tweets[(tweets['selfReference'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","675a5e33":"hit = [0,1,1,1,1,1,1,0,1,1,1,1,1,0,0,1,1,1,1,1]\np_st = sum(hit)\/len(hit)\np_st","b3f5751e":"rd.seed(0)\nx = tweets[(tweets['selfReference'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","ae1b48ef":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_sr = sum(hit)\/len(hit)\np_sr","bf948818":"rd.seed(0)\nx = tweets[(tweets['selfReference'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","d5075a99":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_sq = sum(hit)\/len(hit)\np_sq","bfb97f3f":"rd.seed(0)\nx = tweets[(tweets['usa'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","d98285a9":"hit = [1,0,1,1,1,1,1,1,1,1,0,0,1,0,0,1,1,1,1,1]\np_ut = sum(hit)\/len(hit)\np_ut","886da408":"rd.seed(0)\nx = tweets[(tweets['usa'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","d686784c":"hit = [1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1]\np_ur = sum(hit)\/len(hit)\np_ur","6388554e":"rd.seed(0)\nx = tweets[(tweets['usa'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","632707ff":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_uq = sum(hit)\/len(hit)\np_uq","bd6e6579":"rd.seed(0)\nx = tweets[(tweets['government'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","508cd3e4":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1]\np_gt = sum(hit)\/len(hit)\np_gt","e346cb10":"rd.seed(0)\nx = tweets[(tweets['government'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","3366aca5":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1]\np_gr = sum(hit)\/len(hit)\np_gr","58f52355":"rd.seed(0)\nx = tweets[(tweets['government'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","c063e0c8":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_gq = sum(hit)\/len(hit)\np_gq","3a7dccb8":"rd.seed(0)\nx = tweets[(tweets['democrates'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","53360c43":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_dt = sum(hit)\/len(hit)\np_dt","3869d85f":"rd.seed(0)\nx = tweets[(tweets['democrates'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","a32d70db":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_dr = sum(hit)\/len(hit)\np_dr","fadc2563":"rd.seed(0)\nx = tweets[(tweets['democrates'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","67bd5a04":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_dq = sum(hit)\/len(hit)\np_dq","5c8a7c80":"rd.seed(0)\nx = tweets[(tweets['republicans'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","5b2e0675":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_rt = sum(hit)\/len(hit)\np_rt","919a3e7d":"rd.seed(0)\nx = tweets[(tweets['republicans'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","404dfe05":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_rr = sum(hit)\/len(hit)\np_rr","77045a98":"rd.seed(0)\nx = tweets[(tweets['republicans'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","e591c0af":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_rq = sum(hit)\/len(hit)\np_rq","efffa23e":"rd.seed(0)\nx = tweets[(tweets['election'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","91185b38":"hit = [1,1,1,0,1,1,0,1,1,1,0,0,1,0,1,1,1,1,1,0]\np_et = sum(hit)\/len(hit)\np_et","60592808":"rd.seed(0)\nx = tweets[(tweets['election'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","935e3e3b":"hit = [1,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1]\np_er = sum(hit)\/len(hit)\np_er","c3044d0d":"rd.seed(0)\nx = tweets[(tweets['election'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","17875191":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_eq = sum(hit)\/len(hit)\np_eq","5373b4e9":"rd.seed(0)\nx = tweets[(tweets['positive'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","a1d2a79a":"hit = [0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,0]\np_pt = sum(hit)\/len(hit)\np_pt","e8d032f5":"rd.seed(0)\nx = tweets[(tweets['positive'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","b289efa3":"hit = [1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,0,1,1]\np_pr = sum(hit)\/len(hit)\np_pr","45b06f5d":"rd.seed(0)\nx = tweets[(tweets['positive'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","ddda60b8":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_pq = sum(hit)\/len(hit)\np_pq","afc3b707":"rd.seed(0)\nx = tweets[(tweets['negative'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","62bd1657":"hit = [1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,1,1,1,1,1]\np_nt = sum(hit)\/len(hit)\np_nt","bd1bf95c":"rd.seed(0)\nx = tweets[(tweets['negative'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","856d64fd":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_nr = sum(hit)\/len(hit)\np_nr","5706ce25":"rd.seed(0)\nx = tweets[(tweets['negative'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","87c662c7":"hit = [0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1]\np_nq = sum(hit)\/len(hit)\np_nq","776aad96":"rd.seed(0)\nx = tweets[(tweets['news'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","b108cea0":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1]\np_ft = sum(hit)\/len(hit)\np_ft","be07c4c1":"rd.seed(0)\nx = tweets[(tweets['news'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","70b87432":"hit = [1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1]\np_fr = sum(hit)\/len(hit)\np_fr","0cdd085d":"rd.seed(0)\nx = tweets[(tweets['news'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","724c0c50":"hit = [1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_fq = sum(hit)\/len(hit)\np_fq","e26a4117":"rd.seed(0)\nx = tweets[(tweets['law'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","7fc55530":"hit = [1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_lt = sum(hit)\/len(hit)\np_lt","364f4628":"rd.seed(0)\nx = tweets[(tweets['law'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","87a4e0c7":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_lr = sum(hit)\/len(hit)\np_lr","de68888f":"rd.seed(0)\nx = tweets[(tweets['law'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","fbd2ff2f":"hit = [1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,0,1,1,0]\np_lq = sum(hit)\/len(hit)\np_lq","36425f2c":"rd.seed(0)\nx = tweets[(tweets['border'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","e8c4d448":"hit = [1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,0,1,1]\np_bt = sum(hit)\/len(hit)\np_bt","65d9444b":"rd.seed(0)\nx = tweets[(tweets['border'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","cce86353":"hit = [1,0,1,1,0,1,0,1,0,1,1,1,0,1,1,1,0,1,1,1]\np_br = sum(hit)\/len(hit)\np_br","3ccf46ee":"rd.seed(0)\nx = tweets[(tweets['border'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","d32355f3":"hit = [0,1,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_bq = sum(hit)\/len(hit)\np_bq","2955bf2d":"rd.seed(0)\nx = tweets[(tweets['economic'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","33f28063":"hit = [1,1,0,0,1,0,1,1,0,1,1,0,0,1,0,0,1,0,1,1]\np_ct = sum(hit)\/len(hit)\np_ct","e81bf9f7":"rd.seed(0)\nx = tweets[(tweets['economic'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","20a1a132":"hit = [1,0,1,1,1,0,1,0,1,1,1,0,1,1,1,1,1,1,1,1]\np_cr = sum(hit)\/len(hit)\np_cr","854c9e02":"rd.seed(0)\nx = tweets[(tweets['economic'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","5a9900a0":"hit = [1,0,1,1,0,0,0,1,0,0,0,0,1,0,0,1,0,1,0,0]\np_cq = sum(hit)\/len(hit)\np_cq","7bfe515e":"rd.seed(0)\nx = tweets[(tweets['states'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","0cf180b4":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_stt = sum(hit)\/len(hit)\np_stt","5a635241":"rd.seed(0)\nx = tweets[(tweets['states'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","506362de":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_str = sum(hit)\/len(hit)\np_str","43e1428c":"rd.seed(0)\nx = tweets[(tweets['states'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","d4f15db4":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_stq = sum(hit)\/len(hit)\np_stq","9d8ad389":"rd.seed(0)\nx = tweets[(tweets['countries'] == 1) & (tweets['isRetweet'] == 'f') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","1d9f0db8":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_cnt = sum(hit)\/len(hit)\np_cnt","194d197b":"rd.seed(0)\nx = tweets[(tweets['countries'] == 1) & (tweets['isRetweet'] == 't') & (tweets['isQuoteTweet'] == 'f')]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","07865240":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_cnr = sum(hit)\/len(hit)\np_cnr","d8169374":"rd.seed(0)\nx = tweets[(tweets['countries'] == 1) & (tweets['isRetweet'] == 'f') & ((tweets['isQuoteTweet'] == 't1') ^ (tweets['isQuoteTweet'] == 't2'))]\n    \nfor identity in rd.sample(x.index.tolist(), k = 20):\n    print(identity,x.loc[(identity,'text')])","c7068aee":"hit = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\np_cnq = sum(hit)\/len(hit)\np_cnq","3b8ba43d":"table = {\n    'category': ['selfReference','selfReference','selfReference', 'usa', 'usa', 'usa', \n                 'government', 'government', 'government','democrates', 'democrates', 'democrates',\n                 'republicans', 'republicans', 'republicans', 'election', 'election', 'election',\n                'positive', 'positive', 'positive', 'negative', 'negative', 'negative', \n                 'fakeNews', 'fakeNews', 'fakeNews', 'law', 'law', 'law', \n                 'border', 'border', 'border', 'economic', 'economic', 'economic',\n                'states','states','states','countries','countries','countries'],\n    'tweetType': ['trump','retweet','quote','trump','retweet','quote','trump','retweet','quote',\n                 'trump','retweet','quote','trump','retweet','quote','trump','retweet','quote',\n                 'trump','retweet','quote', 'trump','retweet','quote', 'trump','retweet','quote',\n                 'trump','retweet','quote', 'trump','retweet','quote', 'trump','retweet','quote',\n                 'trump','retweet','quote', 'trump','retweet','quote'],\n    'p_hat':[p_st, p_sr, p_sq, p_ut, p_ur, p_uq, p_gt, p_gr, p_gq, p_dt, p_dr, p_dq, p_rt, p_rr, p_rq,\n            p_et, p_er, p_eq, p_pt, p_pr, p_pq, p_nt, p_nr, p_nq, p_ft, p_fr, p_fq, p_lt, p_lr, p_lq,\n            p_bt, p_br, p_bq, p_ct, p_cr, p_cq, p_stt, p_str, p_stq, p_cnt, p_cnr, p_cnq]\n}\ncoherenceTable = pd.DataFrame(table)","89e39a09":"# Over all coherence\nfig_dims = (15, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\ntotalCoherence = coherenceTable.groupby('category').mean().reset_index()\n#totalCoherence\nsns.barplot(data = totalCoherence, x = 'category', y = 'p_hat')\ntotalCoherence","2862e4d6":"# Coherence by category\nfig_dims = (15, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.barplot(data = coherenceTable, x = 'category', y = 'p_hat', hue = 'tweetType')","109dba2c":"# Export\n\ntweets.to_csv('trumpTweetsClean.csv')","97bc68c2":"#### A Quick Disscussion on Topic Coherence.\nNow that we have defined the topics by commonly occuring key words, we want to examine if we have defined our topics apporpriatly.  \nOne way of doinging this, is random sampling our categories and veiwing if the tweets fit that category manualy.  \n  \nOf course, manual labour like this can be time intensive and hard work, so we hope that there is a way to intellegently plan out the work to be done ahead. And in fact, as statisticians will quickly realize, our topic coherence validation process creates a simple binomial distribution for each topic. This gives us a way to intellegently select a sample size for each topic before digging in.","460e4ca1":"### Section 2.3: Extracting Datetime information\nNext, we want to take the existing date strings and convert them into a more python friendly format, by the way of the datetime package.","ad93592a":"#### Negative Category Coherence:\nA tweet should be included in this category if the overall sentiment of the tweet is negative.  \nPositive Example: (31084: I wonder how much money dumb @BuzzFeed and even dumber Ben Smith loooose each year? They have zero credibility - totally irrelevant and sad!)  \nNegative Example: (40751: I highly recommend the just out book - THE FIELD OF FIGHT - by General Michael Flynn. How to defeat radical Islam.)","30a197d4":"# Trump's Twitter History: Data Cleaning\nThose reading this will already be familiar with the political career of Donald Trump, and will no doubt understand the level of influence his twitter account, as well as his television interviews and meetings with the press had in communicating his political message. Often, his tweets were headline news in publications around the globe, and were not uncommon to captured the attention of the world. His twitter account was suspended as of January 8th, 2021, and has not been renstated as of March 19th, 2021.  \nHis complete twitter history has been archived thanks to https:\/\/www.thetrumparchive.com\/, which I have used as my source of data for the following notebooks.\n  \nWhy take the time to preform an overview analysis of Trump's twitter career? Well, most influencial political leaders of the past are nearly completly lost to history. How many speechs of Roman emperors do you know? How many of there speeches even remain?  \nThus this dataset is, in an way, a historical artifact. We, as data scientists\/data analysts, have a chance observe and study this political artifact before it is lost to the memory of human kind. In addition, analysing this dataset allows us to imagine how political leaders of the future will use their rising technological powers to rule the populos of the future.  \n  \nHaving said that, this series of notebooks will serve more so as an investigation of modern natural language processing and data analisys techniques. Both experimental and fundemental, to see what insights we can extract from a twitter account alone.   \n\nIf the reader has any questions or comments, big or small, feel free to leave them on this thread and I will do my best to answer them in a timely manner. The notebook is ment to be readable on its own, but do let me know if anything is unclear.\n  \nThe most interesting of questions, that off the so called 'ratio', cannot be discussed here as per number of responses were not recorded in the original data set, and may now be lost to time forever. If there are any enterprising readers out there who may know how to get there hands on such data, you may hold the answers to the most interesting questions of all. ","cebb2c44":"#### Democratic Category Coherence\nFor the democratic category, we are just looking for the tweet to mention the democratic party, or one of its members.  \nPostive Example: (28382: If only the illegals were Tea Party members then Obama would get them out of the country immediately.)  ","514a2060":"#### Republican Category Coherence\nFor the Republican category, we are looking for tweets to mention the Republican party, or one of its members.  \nPostive Example: (36508: The Republican Party will become \u201cThe Party of Healthcare!\u201d)  ","d5b19aad":"### Section 2.5: Topic Extraction\nThe primary technique we will use to extract the topics from tweets is as follows.  \n1) Sort through the top 500 most used lemminized terms used in the dataset.  \n2) Categorize these terms into 1 or 0 groups based on human judgement.  \n3) Examine a statistically significant number of tweets from each custom made category (and subcategory), to see if the categories make sence.  \n  \nThe issues with this approach: Keywords can be used in different contexts. For example, 'Presedent' is used both with 'Trump', and with 'Obama' in this dataset. To address this issue, I will look at LSA models of each section, in my Analysis Notebook.","e41b21b7":"#### Positive Category Coherence.\nA tweet is considered positive if it has some positive sentiment.  \nPositive Example: (52371: As bad as @CNN is, Comcast MSNBC is worse. Their ratings are also way down because they have lost all credibility. I believe their stories about me are not 93% negative, but actually 100% negative. They are incapable of saying anything positive, despite all of the great things...)  \nNegative Example: (16861: #CelebrityApprentice Who will win? http:\/\/t.co\/1IjFi52y Find out tonight- live Season Finale at 9PM ET on NBC.)","d6cd13c3":"#### Boarder Category Coherence:\nA tweet is in this topic if it referes to the border wall at all.  \n\nPostive Example: (6192: The Wall is funded &amp, being built! https:\/\/t.co\/84BOxKr2Eo)  \n\nNegative Example: (5601: Sleepy Joe Biden was in charge of the H1N1 Swine Flu epidemic which killed thousands of people. The response was one of the worst on record. Our response is one of the best, with fast action of border closings &amp; a 78% Approval Rating, the highest on record. His was lowest!)","bea7449c":"### Section 2.1 Extracting Quote Tweets.\n### Types of Tweets\nThe first thing to mention when examining the tweets is that there appear to be three types of tweets.  \n1) Regular Trump Tweets.  \n2) Tweets that have been retweeted by Trump. These tweets are disinquesed in two ways, they all begin with RT in the text, and isRetweet is valued at 't'.   \n3) Quote tweets; similar to a retweet, but not flagged by isRetweet. These tweets can be detected by the \"\"\"@user that begins the text string. To deal with this we will both extract the username, the quoted message, and the additional message added by trump. ","6f91af16":"Now we will create a function that will categorized tweets, based on the appearence of any of the terms in a given category.","d5af6f25":"### Section 2.4: Sentiment Analysis:  \nThis Sentement Analysis technique is based on work done by kaggle user ahmedterry, his notebook can be found here:  \nhttps:\/\/www.kaggle.com\/ahmedterry\/trump-tweets-eda-nlp-sentiments-analysis  \n  \nThere are few steps involved in this sentiment analysis:  \n1) Clean the text, lemminize.  \n2) Extract 'Subjectivity' and 'Polerization' Scores.  \n3) Simplfy the subjectivity into 'Positive', 'Neutral', and 'Negative' buckets.  \n4) And finnaly we seperate our cleaned sentences into tokens that we can use for searching, as will be see in the create category section.","8df57e84":"### Trump Twitter Series\nI am splitting the work I am doing on the Trump Twitter dataset in three notebooks.  \n1) Data Cleaning.  \n2) Analysis of the Data.  \n3) Evaluation of off the Self NLP Machine Learning Techniques.  \n\n## Table of Content\nSection 1: Data Retreval and Package Download  \nSection 2.1: Extracting Quote Tweets  \nSecion 2.2: Extracting Hashtags and Mentions  \nSection 2.3: Extracting Datetime Information  \nSection 2.4: Sentiment Analysis  \nSection 2.5: Category Extraction  \nSection 2.6: Category Coherence Analysis","67ebc599":"To create the topics, I have looked through the top 500 appearing search terms of Trumps vocab and attempted to sort them into categories by usage. I will analize the effectiveness of this approach later in this notebook, and in the next.","c9c896f7":"#### Economic Category Coherence\nA tweet is in this category if the tweet refers to the economy in some way.  \nPositive Example: (3183: He is out of real solutions--@BarackObama's job bill is nothing more than a tax increase.)  \nNegative Example: (30171: China is closing a massive oil deal w\/ Russia, taking advantage of the Ukraine conflict http:\/\/t.co\/tItkQ0PmZH Smart, unlike our leaders.)","e0d4ef62":"#### Government Category Coherence\nHere what we are looking for is for the tweet to refer to the US government in some way.  \n\nA Positive Example is: (44878: Democrats purposely misstated Medicaid under new Senate bill - actually goes up. https:\/\/t.co\/necCt4K6UH)  \n\nA Negative Example is: (17362: \"\"\"Imagine how much stronger economic shape we would be in if we made the Iraqi government agree to a cost-sharing (cont) http:\/\/t.co\/Zf2pEO80\")","b2bc4c4c":"### States Category Coherence\nTopic includes mention of any state that is in the United States.\"","5f691ac5":"An example of our cleaned text, notice its a list of lemmanized terms. We will use these terms later as the basis of our tweet search.","6858336e":"Our next goal is to seperate out the information encoded in quote tweets.  \nThis section is a bit of the nitty-gritty found in real world data problems, because the formating used is inconsistent through out the dataset.  \n  \nHere, pattern1 identifies tweet['text'] of the form: \"\"\"@user: User_Message\"\" Trump_message\".  \npattern2 identifies tweets with the text of the form: \"\"\"@user User_messsage\".  \n  \nThe function extracts and records each of the above sections, as well as recordes an indicator as to whether the tweet is of type pattern1 (t1), pattern2 (t2), or not quote-tweet ('n\/a').","a20baa65":"### Section 2.2: Hashtags and Mentions  \nA quick an easy way to extract all hastags and mentions from a tweet, using python's Regular Expressions Package (re).  \nCode copied from: https:\/\/stackoverflow.com\/questions\/45874879\/extract-hashtags-from-columns-of-a-pandas-dataframe","03704038":"Further analysis of the quote tweets can be found in the next notebook, Trump Tweets Analysis.","e5e3742b":"After sorting through the top 500 terms, I came to define the categories by the following definitions. If you are interested in seeing more detail about the definitions of any category, see Section 2.6: Topic Coherence for a more detailed look.  \n1) 'selfReference': Any term that may refer to Donald Trump, or his campaign, in the third person. (note 'I' was removed as a stopword.  \n2) 'usa': Terms that refer to the country 'United States of America'.  \n3) 'government': Terms that refer to the government of the United States or its major governmental bodies.  \n4) 'democrates': Terms that refer to either the democratic party, or one of its promenent members.  \n5) 'republicans': Terms that refer to either the republican party, or one of its promenent members.  \n6) 'election': Terms that refer to elections, or electoral processess.  \n7) 'positive': Terms that generaly imply a positive meaning, or a positive adjective.  \n8) 'negative': Terms that generaly imply a negative meaning, or a negative adjective.  \n9) 'news': Terms that talk about news, and news networks.  \n10) 'law': Terms that refer to law and order, or the American Judicial System.  \n11) 'border': Terms that reference the US border wall, or imigration.  \n12) 'economic': Terms that reference the economy.  \n13) 'states': Terms that reference and State in the United States.  \n14) 'countries': Names of other countries used, not including the United States.  \n15) 'bucket': Bucket is the category where I have stored interesting terms, but could not categorized. An enterprising reader may wish to contiune my work were I could not.****","5159cda6":"### Countries Category Coherence\nTopic includes any mention of a country that is not the United States.","191fdc59":"#### Election Category Coherence\nFor a tweet to be included in this category, it must reference an election in some way.  \nPositive Example: (25771: This \u2018deal\u2019 @RNC voted for has $41 in tax increases for every $1 in spending cuts.  It is pathetic.  Obama is laughing at them.)  \nNegative Example: (20764: On Bill O'Reilly in 5 minutes!)","e6521f0e":"#### News Category Coherence:\nA tweet is in this category if it makes mention of news.  \nPositive Example: (31398: Watching Gates on @seanhannity - looks like he got hit by a truck! Why didn't Obama get him, and others,to sign a confidentiality agreement?)  \nNegative Example: (13350: Be sure to watch The Apprentice tonight, 10 p.m. on NBC--it's an episode you won't forget!)","b8791d0a":"There is an interesting note here by the stop words used in the trump tweet dataset. There are many occurances of single letters like, 's','t','u', etc.  \nI am not really sure what is the reason for this, but we can address the common occuring ones up front here. ","7455fd46":"#### Self-Reference Category Coherence\nA tweet qualifies as 'SeflReference' if the tweet in some way referes to Donald Trump.  \nPositive Example (42162: I just got off the phone with the great people of Guam! Thank you for your support! #VoteTrump today! #Trump2016).  \nNegative Example (40775: Our next Vice President of the United States of America, Gov. @Mike_Pence!#GOPinCLE #GOPConvention#AmericaFirst https:\/\/t.co\/TZT3XcKp1c).  \n\nNotice that in our negative example, the tweet was selected for using the keyword 'Presedent' but in context, is refering to Obama and not trump.","43f5d82a":"#### Law and Order Category Coherence:\nA tweet should be included in this category if the tweet has to do with law, justice, or the American Judicial system.  \nPositive Example: (45674: I have made my decision on who I will nominate for The United States Supreme Court. It will be announced live on Tuesday at 8:00 P.M. (W.H.))  \nNegative Example: (36761: Entrepreneurs: Resolve to be bigger than your problems. Who's the boss? Don't negate your own power.)  "}}