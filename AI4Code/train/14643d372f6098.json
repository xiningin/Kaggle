{"cell_type":{"b9bff672":"code","7d71685f":"code","93954ba4":"code","f85f6dcf":"code","633c0d25":"code","1f6b8b5f":"code","cf488121":"code","47855d95":"code","178eadec":"code","0d6b0d0d":"code","62d962fc":"code","a97c7606":"code","8df7a460":"code","ee580f51":"code","100e907d":"code","d979ae53":"code","80aa8697":"code","62ee5ef6":"code","d6dfd81b":"code","ca3ecd34":"code","c674c7d4":"code","8961bbf3":"code","cca1e7c4":"code","800dd270":"code","09e7bd91":"code","9e0c2ab8":"code","781cc415":"code","d62807cc":"code","b5c1b8e4":"code","6546fb20":"code","ec22cd0d":"code","1c448513":"code","08935670":"code","db598ba0":"code","775cc82f":"code","4f581472":"code","98845d90":"code","7f214763":"code","7a3dadfc":"code","94148cac":"code","aca6b85b":"code","edc01249":"markdown","a8674e1f":"markdown","d6c57fd1":"markdown","0de7835d":"markdown","71b64c78":"markdown","15210172":"markdown","9d0eec37":"markdown","1166aa5e":"markdown","b8440896":"markdown","776c0f16":"markdown","0ef8ba8c":"markdown","a1ae3081":"markdown","c56cf3df":"markdown","d1ab9d74":"markdown","8b5dea56":"markdown","a28f9979":"markdown","d9a3097e":"markdown","d0c49c24":"markdown","7a764daa":"markdown","b899ad2d":"markdown","1b8f6bb3":"markdown","08e03863":"markdown","ceb1f5b5":"markdown","7caa4fbc":"markdown","57e5a945":"markdown","9ef5ae8b":"markdown","b1a919cf":"markdown","b99d56eb":"markdown","42eb7bf1":"markdown"},"source":{"b9bff672":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7d71685f":"# training set\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head()","93954ba4":"# test set\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest.head()","f85f6dcf":"print('Shape of Training Set : {}'.format(train.shape))\nprint('Number of training data points : {}\\n'.format(len(train)))\nprint('Shape of Test Set : {}'.format(test.shape))\nprint('Number of test data points : {}\\n'.format(len(test)))\nprint('Columns : {}'.format(train.columns))","633c0d25":"plt.figure(figsize=(17, 17))\ng = sns.heatmap(train.drop('Id', axis=1).corr(), annot=True, cmap='coolwarm',  square=True, fmt='.1f')","1f6b8b5f":"fig, ax = plt.subplots(14, 3, figsize=(25,80))\nax = ax.flatten()\n\nfor i,j in zip(train.select_dtypes(include=['object']).columns, ax):\n    srtd = train.groupby(i)['SalePrice'].median().sort_values(ascending=False)\n    sns.boxplot(x=i,\n                y='SalePrice',\n                data=train,\n                order=srtd.index,\n                ax=j\n               )\n    j.tick_params(labelrotation=45)\n    plt.tight_layout()","cf488121":"fig, ax = plt.subplots(12, 3, figsize=(25,80))\nax = ax.flatten()\n\nfor i,j in zip(train.select_dtypes(include=['number']).columns, ax):\n    \n    sns.regplot(x=i,\n                y='SalePrice',\n                data=train,\n                ax=j,\n                ci=None,\n                line_kws={'color': 'black'},\n                scatter_kws={'alpha':0.4}\n               )\n    j.tick_params(labelrotation=45)\n    plt.tight_layout()","47855d95":"y = train['SalePrice']\ntrain.drop('SalePrice', axis=1, inplace=True)","178eadec":"# combining train and test for feature enginerring\ndata = pd.concat([train, test])\ndata.drop('Id', axis=1, inplace=True)\ndata.shape\ndata.info()","0d6b0d0d":"nan_categorical = []\nnan_num = []\nmissing = {}\ntot = len(data)\n\nfor i in data.columns:\n    var = data[i].isnull().sum()\n    if (var!=0):\n        missing.update({i:var\/tot*100})\n        if (data[i].dtype=='object'):\n            nan_categorical.append(i)\n        else:\n            nan_num.append(i)","62d962fc":"missing = pd.DataFrame.from_dict(missing, orient='index', columns=['Percentage'])\nmissing.sort_values(by='Percentage', ascending=False, inplace=True)\nmissing = missing.T\n\nplt.figure(figsize=(20,4))\ng = sns.barplot(data=missing, palette='Reds_r')\nplt.ylabel('Percentage')\nplt.xticks(rotation=90)\n\ndisplay(missing.style.background_gradient(cmap='Reds', axis=1))","a97c7606":"print(\"Catorical columns with nan values \\n\",nan_categorical)\nprint(\"\\n Int or float columns in data \\n\",nan_num)\nprint(\"\\n total nan columns:\",len(nan_categorical)+len(nan_num) )","8df7a460":"# columns to be filled by none in NA\nnone_columns = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', 'GarageType', \n                'GarageFinish', 'GarageQual', 'GarageCond',  'PoolQC',  'Fence', 'MiscFeature', 'MasVnrType']\n\n# int columns to be filled by 0\nzero_columns = [ 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', \n                'GarageYrBlt', 'GarageCars', 'GarageArea']\n\n# list of columns with Nan values to be replaced by mode \nmode_columns = ['Utilities', 'Exterior1st', 'Exterior2nd', 'Electrical', 'KitchenQual', 'Functional', 'SaleType']\n\nfor i in none_columns:\n    data[i].fillna('None', inplace=True)\n\nfor i in zero_columns:\n    data[i].fillna(0, inplace=True)\n\nfor i in mode_columns:\n    data[i].fillna(data[i].mode()[0], inplace=True)\n    \n# Filling MSZoning according to MSSubClass.\ndata['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n# Filling LotFrontage according to Neighborhood.\ndata['LotFrontage'] = data.groupby(['Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))","ee580f51":"# Features which numerical on data but should be treated as category.\n\ndata['MSSubClass'] = data['MSSubClass'].astype(object)\n\ndata['YrSold'] = data['YrSold'].astype(object)\n\ndata['MoSold'] = data['MoSold'].astype(object)","100e907d":"print('Number of missing values: {data.isnull().sum().sum()}')","d979ae53":"#Excellent, Good, Typical, Fair, Poor, None: Convert to 0-5 scale\ncols_ExGd = ['ExterQual','ExterCond','BsmtQual','BsmtCond',\n             'HeatingQC','KitchenQual','FireplaceQu','GarageQual',\n             'GarageCond','PoolQC']\n\ndict_ExGd = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0}\n\nfor col in cols_ExGd:\n    data[col].replace(dict_ExGd, inplace=True)\n\n#display(data[cols_ExGd].head(5))\ndict_nigh = {'NridgHt': 8, 'NoRidge' : 8, 'StoneBr' : 8, \n             'Timber' : 7, 'Somerst' : 7, 'Veenker' : 7, 'Crawfor' : 7, \n             'ClearCr' : 7, 'CollgCr' : 7, 'Blmngtn' : 7,\n             'Gilbert' : 6, \n             'SawyerW' : 5, \n             'Mitchel' : 4, \n             'NPkVill' : 3, 'NAmes': 3, 'NWAmes' : 3, 'SWISU' : 3, 'Blueste' : 3, \n             'Sawyer' : 2, 'BrkSide' : 2, 'Edwards' : 2, 'OldTown' : 2,\n             'BrDale' : 1, 'IDOTRR' : 1, 'MeadowV' : 1,\n             }\ndata['Neighborhood'].replace(dict_nigh, inplace=True)\n\n# Remaining columns\ndata['BsmtExposure'].replace({'Gd':4,'Av':3,'Mn':2,'No':1,'None':0}, inplace=True)\n\ndata['CentralAir'].replace({'Y':1,'N':0}, inplace=True)\n\ndata['Functional'].replace({'Typ':7,'Min1':6,'Min2':5,'Mod':4,'Maj1':3,'Maj2':2,'Sev':1,'Sal':0}, inplace=True)\n\ndata['GarageFinish'].replace({'Fin':3,'RFn':2,'Unf':1,'None':0}, inplace=True)\n\ndata['LotShape'].replace({'Reg':3,'IR1':2,'IR2':1,'IR3':0}, inplace=True)\n\ndata['Utilities'].replace({'AllPub':3,'NoSewr':2,'NoSeWa':1,'ELO':0}, inplace=True)\n\ndata['LandSlope'].replace({'Gtl':2,'Mod':1,'Sev':0}, inplace=True)\n\nbsm_dict = {'None': 0,\n            'Unf': 1,\n            'LwQ': 2,\n            'Rec': 3,\n            'BLQ': 4,\n            'ALQ': 5,\n            'GLQ': 6\n           }\ndata['BsmtFinType1'].replace(bsm_dict, inplace=True)\ndata['BsmtFinType2'].replace(bsm_dict, inplace=True)","80aa8697":"data['HasAlley'] = data['Alley'].apply(lambda x: 1 if x!='None' else 0)\n\ndata['HasFence'] = data['Fence'].apply(lambda x: 1 if x!='None' else 0)\n\ndata['HasBsmt'] = data['BsmtQual'].apply(lambda x: 1 if x>0 else 0)\n\ndata['HasGarage'] = data['GarageType'].apply(lambda x: 1 if x!='None' else 0)\n\ndata['HasFirePlace'] = data['FireplaceQu'].apply(lambda x: 1 if x>0 else 0)\n\ndata['HasPool'] = data['PoolArea'].apply(lambda x: 1 if x>0 else 0)\n\ndata['Has2ndFloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x>0 else 0)\n\n# Merging quality and conditions.\ndata['TotalExtQual'] = (data['ExterQual'] + data['ExterCond'])\n\ndata['TotalBsmQual'] = (data['BsmtQual'] + data['BsmtCond'] + data['BsmtFinType1'] + data['BsmtFinType2'])\n\ndata['TotalGrgQual'] = (data['GarageQual'] + data['GarageCond'])\n\ndata['TotalQual'] = (data['OverallQual'] + data['TotalExtQual'] + data['TotalBsmQual'] + data['TotalGrgQual']+ \n                     data['KitchenQual'] + data['HeatingQC']\n                    )\n# Creating new data by using new quality indicators.\ndata['QualGr'] = data['TotalQual'] * data['GrLivArea']\n\ndata['QualBsm'] = data['TotalBsmQual'] * (data['BsmtFinSF1'] + data['BsmtFinSF2'])\n\ndata['QualExt'] = data['TotalExtQual'] * data['MasVnrArea']\n\ndata['QualGrg'] = data['TotalGrgQual'] * data['GarageArea']\n\ndata['QualSFNg'] = data['QualGr'] * data['Neighborhood']\n\n# creating freatures\ndata['TotalSF'] = (data['BsmtFinSF1'] + data['BsmtFinSF2'] + data['1stFlrSF'] + data['2ndFlrSF'])\n\ndata['TotalBathrooms'] = (data['FullBath'] + (0.5 * data['HalfBath']) +\n                          data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath'])\n                         )\ndata['TotalPorchSF'] = (data['OpenPorchSF'] + data['3SsnPorch'] + data['EnclosedPorch'] +\n                        data['ScreenPorch'] + data['WoodDeckSF']\n                       )\ndata['YearBlRm'] = (data['YearBuilt'] + data['YearRemodAdd'])","62ee5ef6":"new_features = ['TotalExtQual', 'TotalBsmQual', 'TotalBsmQual', 'TotalGrgQual', 'TotalQual', 'QualGr', \n                'QualBsm', 'QualExt','QualGrg', 'QualSFNg', 'TotalSF', 'TotalBathrooms', 'TotalPorchSF', 'YearBlRm'\n               ]\nfig, ax = plt.subplots(3, 4, figsize=(16,12))\nax = ax.flatten()\nfeatures = data.join(y)\nfor i,j in zip(new_features, ax):\n    \n    sns.regplot(x=i,\n                y='SalePrice',\n                data=features,\n                ax=j,\n                ci=None,\n                line_kws={'color': 'black'},\n                scatter_kws={'alpha':0.4}\n               )\n    j.tick_params(labelrotation=45)\n    plt.tight_layout()","d6dfd81b":"plt.figure(figsize=(15,5))\n\nplt.subplot(1, 2, 1)\nsns.distplot(data.skew(), axlabel ='Skewness')\n\nplt.subplot(1, 2, 2)\nsns.distplot(data.kurt(), axlabel ='Kurtosis')\n\nplt.show()","ca3ecd34":"from scipy.stats import skew\n\nnumeric_feats = data.dtypes[data.dtypes != \"object\"].index\n\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nskewed_feats","c674c7d4":"sns.distplot(data['1stFlrSF']);","8961bbf3":"def fixing_skewness(df):\n    \"\"\"\n    This function takes in a dataframe and return fixed skewed dataframe\n    \"\"\"\n    ## Import necessary modules \n    from scipy.stats import skew\n    from scipy.special import boxcox1p\n    from scipy.stats import boxcox_normmax\n    \n    ## Getting all the data that are not of \"object\" type. \n    numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n\n    # Check the skew of all numerical features\n    skewed_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n    high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n    skewed_features = high_skew.index\n\n    for feat in skewed_features:\n        df[feat] = boxcox1p(df[feat], boxcox_normmax(df[feat] + 1))\n\nfixing_skewness(data)","cca1e7c4":"sns.distplot(data['1stFlrSF']);","800dd270":"# creating dummies\ncols = []\nfor i in data.columns:\n    if data[i].dtype==object:\n        cols.append(i)\n\ndata = pd.get_dummies(data=data, columns=cols)\ndata.shape","09e7bd91":"# *********************Splitting data into train and test set**************************\nX_train = data.iloc[:len(train), :]\nX_test = data.iloc[len(train):, :]","9e0c2ab8":"# I'm using log scale for scaling target values\ny_scaled = np.log(y)\n\nfrom sklearn.preprocessing import RobustScaler\nrobust_scaler = RobustScaler()\n\nX_train_scaled = robust_scaler.fit_transform(X_train)\nX_test_scaled = robust_scaler.transform(X_test)","781cc415":"from sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 0.001)\n\nlasso.fit(X_train_scaled, y_scaled)\n\ny_pred_lasso = lasso.predict(X_test_scaled)\n\nlasso_coeff = pd.DataFrame({'Feature Importance':lasso.coef_}, index=data.columns)\nlasso_coeff.sort_values('Feature Importance', ascending=False)","d62807cc":"g = lasso_coeff[lasso_coeff['Feature Importance'] != 0].sort_values('Feature Importance').plot(kind='barh',figsize=(20,20))","b5c1b8e4":"from sklearn.model_selection import GridSearchCV\n\n# defining grid search function\ndef hyperparameter_tuning(model, parameters, X_train=X_train_scaled, y_train=y_scaled, jobs=-1):\n    grid_search = GridSearchCV(estimator=model,\n                               param_grid=parameters,\n                               cv=5,\n                               scoring='neg_mean_squared_error',\n                           #    n_jobs=jobs,\n                           #    verbose=2\n                              )\n    grid_search.fit(X_train, y_train)\n    print(\"Best Score: {:.5f}\".format(np.sqrt(-grid_search.best_score_)))\n    print(\"Best Parameters:\", grid_search.best_params_)\n    best_model = grid_search.best_estimator_\n    return grid_search, best_model","6546fb20":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(learning_rate =0.0139,\n                   n_estimators =4500,\n                   max_depth =4,\n                   min_child_weight =0,\n                   subsample =0.7968,\n                   colsample_bytree =0.4064,\n                   nthread =-1,\n                   scale_pos_weight =2,\n                   seed=42,\n                  )","ec22cd0d":"from sklearn.ensemble import RandomForestRegressor\n\nrf_param = {'n_estimators': [150, 300],\n            'min_samples_split': [2, 5, 8]\n           }\ngs_rf, rf = hyperparameter_tuning(RandomForestRegressor(), rf_param)","1c448513":"from sklearn.svm import SVR\n\nsvm_param = {'C':[15, 10, 1.5, 1, 0.1, 0.01],\n             'epsilon':[0.0001, 0.001, 0.1, 0.2],\n             'gamma': [0.0001, 0.001, 0.005, 0.1, 1]\n            }\ngs_svr, svr = hyperparameter_tuning(SVR(), svm_param)","08935670":"from sklearn.linear_model import Ridge\n\nridge_param = {'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0], \n               'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n              }\ngr_ridge, ridge = hyperparameter_tuning(Ridge(), ridge_param)","db598ba0":"from lightgbm import LGBMRegressor\n'''\nlgb_param = {'objective': ['regression'],\n             'learning_rate': [0.00721],\n             'n_estimators': [4000, 7000],\n         #    'boosting_type': ['gbdt', 'dart'],\n             'num_leaves': [50, 100, 200],\n             'reg_alpha': [0, 1.2, 1.3],\n             'reg_lambda': [0, 1.2, 1.3]\n            }\ngs_lgb, lgb = hyperparameter_tuning(LGBMRegressor(), lgb_param)            \n'''\nlgb = LGBMRegressor(objective='regression',\n                    n_estimators=5000,\n                    num_leaves=5,\n                    learning_rate=0.00721,\n                    max_bin=163,\n                    bagging_fraction=0.35711,\n                    n_jobs=-1,\n                    bagging_seed=42,\n                    feature_fraction_seed=42,\n                    bagging_freq=7,\n                    feature_fraction=0.1294,\n                    min_data_in_leaf=8\n                   )","775cc82f":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr_param = {'loss': ['ls', 'huber', 'quantile'],\n             'n_estimators': [400, 700],\n             'min_samples_leaf': [17, 15],\n             'max_features': ['sqrt'],\n             'max_depth': [4]\n            }\ngs_gbr, gbr = hyperparameter_tuning(GradientBoostingRegressor(), gbr_param)","4f581472":"from catboost import CatBoostRegressor\n'''cat_param = {'learning_rate': [0.03, 0.1],\n             'depth': [4, 6, 10],\n             'l2_leaf_reg': [1, 3, 5, 7, 9]\n            }\nmodel = CatBoostRegressor()\ngs_cbr, cbr = hyperparameter_tuning(model, cat_param)'''\n\ncbr = CatBoostRegressor(iterations=3500,\n                        learning_rate=0.03,\n                        od_type='Iter',\n                        od_wait=1500,\n                        depth=6,\n                        random_strength=1,\n                        l2_leaf_reg=10,\n                        sampling_frequency='PerTree',\n                        verbose=0\n                       )\ncbr.fit(X_train_scaled, y_scaled)","98845d90":"from sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.metrics import r2_score\n\n#A function for testing multiple estimators\ndef model_check_cv(X, y, estimators, labels):\n   \n    model_table = pd.DataFrame()\n    row_index = 0\n    for est, label in zip(estimators, labels):\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = label\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=5,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True\n                              #      n_jobs=-1\n                                   )\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results['train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results['test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'], inplace=True)\n    return model_table\n\n# Function for r2scores of all models\ndef r2_table(X, y, estimators, labels):\n    table = pd.DataFrame()\n    i = 0\n    for est, label in zip(estimators, labels):\n        table.loc[i, 'Model Name'] = label\n        table.loc[i, 'R2 Score'] = r2_score(y, np.exp(est.predict(X)))\n        i += 1\n    return table.sort_values(by=['R2 Score'], ascending=False)","7f214763":"estimators = [xgb, rf, svr, ridge, lgb, gbr, cbr]\nlabels = ['XGBoostRegressor', 'Random Forest Regressor', 'Support Vector Regressor', \n          'Ridge Regression', 'LGBM Regressor', 'GradientBoostingRegressor', 'CatBoostRegressor']\n\nmodels = model_check_cv(X_train_scaled, y_scaled, estimators, labels)\ndisplay(models.style.background_gradient(cmap='Reds'))","7a3dadfc":"# fitting all models on whole training data\nxgb.fit(X_train_scaled, y_scaled)\nrf.fit(X_train_scaled, y_scaled)\nridge.fit(X_train_scaled, y_scaled)\nlgb.fit(X_train_scaled, y_scaled)\ngbr.fit(X_train_scaled, y_scaled)\ncbr.fit(X_train_scaled, y_scaled)\nsvr.fit(X_train_scaled, y_scaled)\n\n# these are r2 scores on whole training set\narray = r2_table(X_train_scaled, y, estimators, labels)\ndisplay(array.style.background_gradient(cmap='Reds'))","94148cac":"# function for predicting \ndef price_predict(X):\n    return (\n            (0.2 * xgb.predict(X))+\n            (0.2 * cbr.predict(X))+\n            (0.15 * rf.predict(X))+\n            (0.1 * ridge.predict(X))+\n            (0.12 * lgb.predict(X))+\n            (0.15 * gbr.predict(X))+\n            (0.08 * svr.predict(X))\n           )","aca6b85b":"pred = np.exp(price_predict(X_test_scaled))\npred = pd.Series(pred, name='SalePrice')\nresults = pd.concat((test['Id'], pred), axis=1)\nresults.to_csv(\"mysubmission.csv\", index=False)\nresults.head()","edc01249":"finally we are having 256 features, hope it give good results :)","a8674e1f":"## 4.3 Random Forest Regressor","d6c57fd1":"\n# 4. Model\nThese are the models we are goona use\n* XG Boost Regressor\n* Random Forest Regressor\n* LGBM Regressor\n* Support Vector Machine\n* Ridge Regressor\n* Gradient Boosting Regressor\n* Cat Boost Regressor\n\n### 4.1 First lets define a function for hyperparmeter tuning","0de7835d":"## 5. Final Prediction","71b64c78":"## 3.2 Encoding Categorical Data\nThe values gives to Categorical variables are based obervations made form BoxPlot","15210172":"> Currently i am writing this notebook my rank is `125` on 26\/10\/20\n### Upvote will be much appreciated and keep me motivated :)\n# House Prices: Advanced Regression Techniques using Ensemble Learning\n\n### Contents\n\n* 1.Importing Dataset\n* 2.Data Analysis\n    * 2.1 Corr Plot\n    * 2.1 Analyzing Categoricla Data\n    * 2.3 Analyzing Numerical Data\n* 3.Feature Engineering\n    * 3.1 Missing Data\n    * 3.2 Encoding Categorical Data\n    * 3.3 Create New Features\n    * 3.4 Analizing New Features\n    * 3.5 Skewness and Kurtosis\n    * 3.6 Scaling Data\n    * 3.7 Feature Importance using Lasoo cofficient\n* 4.Model\n    * 4.1 First lets define a function for hyperparmeter tuning\n    * 4.2 XG Boost Regressor\n    * 4.3 Random Forest Regressor\n    * 4.4 LGBM Regressor\n    * 4.5 Support Vector Machine\n    * 4.6 Ridge Regressor\n    * 4.7 Gradient Boosting Regressor\n    * 4.8 Cat Boost Regressor\n* 5.Final Prediction","9d0eec37":"**Some Observations:**\n* OverallQual; It's clearly visible that sale price of the house increases with overall quality. This confirms the correlation in first table we did at the beginning. (Pearson corr was 0.8)\n\n* OverallCondition; Looks like overall condition is left skewed where most of the houses are around 5\/10 condition. But it doesn't effect the price like quality indicator...\n\n* YearBuilt; Again new buildings are generally expensive than the old ones.\n\n* Basement; General table shows bigger basements are increasing the price but I see some outliers there...\n\n* GrLivArea; This feature is pretty linear but we can spot two outliers effecting this trend. There are some huge area houses with pretty cheap prices, there might be some reason behind it but we better drop them.\n\n### Lets Seperate Sale Prices from the data and make a set of target values","1166aa5e":"I have seperated columns having Categorical and Numerical Data with nan values it helps me look in the **Data Description** file provided. \ntake a look in `data_description.txt` file for better understanding for filling data\n\n**This is how we gonna fix most of the missing data:**\n\n* First we fill the NaN's in the columns where they mean 'None' so we gonna replace them with that,\n* Then fill numerical columns where missing values indicating there is no parent feature to measure, so we replace them with 0's.\n* Even with these there are some actual missing data, by checking general trends of these features we can fill them with most frequent value(with mode).\n* MSZoning and Lot Frontage part is little bit tricky I choose to fill them with most common type of the related MSSubClass and Neighborhood respective type. It's not perfect but at least we decrease randomness a little bit.\n","b8440896":"based on Root Mean Squared Error values and R2scores I will assign weights to the models\n\n**CatBoost and XG Boosting** outperforms all other models while **SVM has lowest score** but still combareable","776c0f16":"## 3.7 Feature Importance using Lasoo cofficient","0ef8ba8c":"QualSFNg, GrLivArea, OverallQual are some of the important features","a1ae3081":"# 3.3 Let's Create Some New Intresting Features","c56cf3df":"# 1. Importing Dataset","d1ab9d74":"there is a lot of missing data in the dataset specially `PoolQC` with 99% of missing data and MiscFeautres, Alley, Fence etc. closing","8b5dea56":"## 3.5 Skewness and Kurtosis\n\n**Skewness**\n\n* is the degree of distortion from the symmetrical bell curve or the normal curve.\n* So, a symmetrical distribution will have a skewness of \"0\".\n* There are two types of Skewness: **Positive and Negative.**\n* **Positive Skewness**(similar to our target variable distribution) means the tail on the right side of the distribution is longer and fatter.\n* In **positive Skewness** the mean and median will be greater than the mode similar to this dataset. Which means more houses were sold by less than the average price.\n* **Negative Skewness** means the tail on the left side of the distribution is longer and fatter.\n* In **negative Skewness** the mean and median will be less than the mode.\n* Skewness differentiates in extreme values in one versus the other tail.\n\n**Kurtosis** According to Wikipedia,\n\nIn probability theory and statistics, Kurtosis is the measure of the \"tailedness\" of the probability. distribution of a real-valued random variable. So, In other words, it is the measure of the extreme values(outliers) present in the distribution.\n\n* There are three types of Kurtosis: **Mesokurtic, Leptokurtic, and Platykurtic.**\n* Mesokurtic is similar to the normal curve with the standard value of 3. This means that the extreme values of this distribution are similar to that of a normal distribution.\n* Leptokurtic Example of leptokurtic distributions are the T-distributions with small degrees of freedom.\n* Platykurtic: Platykurtic describes a particular statistical distribution with thinner tails than a normal distribution. Because this distribution has thin tails, it has fewer outliers (e.g., extreme values three or more standard deviations from the mean) than do mesokurtic and leptokurtic distributions.","a28f9979":"## 4.6 LGBM Regressor","d9a3097e":"### Double check for nan values","d0c49c24":"## 3.6 Scaling Data Train, Test Set and Target Values","7a764daa":"## 4.8 Cat Boost Regressor","b899ad2d":"## 3.4 Analizing New Features","1b8f6bb3":"## 4.4 Support Vector Machine","08e03863":"**Observations:**\n\n* MSZoning\n    * Residental high and low seems similar meanwhile commercial is the cheap.\n    * LandContour, Hillside houses seems a little bit higher expensive than the rest \n    * meanwhile banked houses are the lowest.\n\n\n* Neighborhood\n    * Northridge Heights and Northridge are top expensive places for houses.\n    * Timberland, Somerset, Veenker, Crawford, Clear Creek, College Creek and Bloomington Heights seems above average.\n    * Sawyer West has wide range for prices related to similar priced regions.\n    * Old Town and Edwards has some outlier prices but they generally below average.\n    * Briardale, Iowa DOT, Rail Road, Meadow Village are the cheapest places for houses it seems...\n   \n\n* Conditions\n    * Meanwhile having wide range of values being close to North-South Railroad seems having positive effect on the price.\n    * Being near or adjacent to positive off-site feature (park, greenbelt, etc.) increases the price.\n    * These values are pretty similar but we can get some useful information from them.\n    \n    \n* MasVnrType Having stone masonry veneer seems better priced than having brick.\n\n* Quality Features; There are many categorical quality values that affects the pricing on some degree, we're going to quantify them so we can create new features based on them. So we don't dive deep on them in this part.\n\n* CentralAir Having central air system has decent positive effect on sale prices.\n\n\n* GarageType\n    * Built-In (Garage part of house - typically has room above garage) garage typed houses are the most expensive ones.\n    * Attached garage types following the built-in ones.\n    * Car ports are the lowest\n* Misc Sale type has some kind of effect on the prices but we won't get into details here. Btw... It seems having tennis court is really adding price to your house, who would have known\n\n## 2.3 Analyzing Numerical Data","ceb1f5b5":"**Observations:**\nthe feature with blue color are less important and of Red are more important\n* Overall condition of the house seems less important on the pricing, i thought it would have weight\n* There's strong relation between overall quality of the houses and their sale prices.\n* Grade living area seems strong indicator for sale price.\n* Garage features, number of baths and rooms, how old the building is etc. are important too.\n* There are some obvious relations we gonna pass like total square feet affecting how many rooms there are or how many cars can fit into a garage vs. garage area etc.\n\n## 2.2 Let's Look at Categorical Data","7caa4fbc":"# 2. Let's Analyze\n## 2.1 Corr Plot","57e5a945":"# 3. Feature Engineering\n## 3.1 Missing Data","9ef5ae8b":"## 4.2 XGB Regressor","b1a919cf":"## 4.7 Gradient Boosting Regressor","b99d56eb":"* Your feedback in comments is much appreciated, Comment if you have any doubts or for inprovement\n* Please **UPVOTE** if you LIKE this notebook, it will keep me motivated","42eb7bf1":"## 4.5 Ridge Regressor"}}