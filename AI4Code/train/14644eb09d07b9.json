{"cell_type":{"be1690c4":"code","7725f8fd":"code","987997a8":"code","e5b59300":"code","c96f1f86":"code","7f9407ec":"code","a0ce0312":"code","252e96b0":"code","9c1dc965":"code","483c234b":"code","b233e3b1":"code","23eb42da":"code","19891741":"code","f6df5646":"code","18711540":"code","8732ffb1":"code","4ca5e6da":"code","bb0b1b87":"code","e46a6885":"code","0d556aaf":"code","19b5a350":"code","5c361b4c":"code","c232472b":"code","fbde7549":"code","a9b1d524":"code","bc10a070":"code","aaa9097d":"code","bba1051c":"code","373e2b57":"code","81196697":"code","cb42f57b":"code","ab64638b":"code","cac3d863":"code","f1514463":"markdown","3f785edb":"markdown","18cf87f0":"markdown","bcf81b6c":"markdown","1e4b1d41":"markdown","aff07e08":"markdown","2cea4632":"markdown","e33466b4":"markdown","729ae085":"markdown","cf0ddaae":"markdown","6aa87108":"markdown","418f4536":"markdown","a0ed4bff":"markdown","12ca9dea":"markdown","6f42f4ca":"markdown","bf381b8c":"markdown","a03cd9f0":"markdown","ea22804d":"markdown","75ad17d6":"markdown","d2635f4e":"markdown","078114cd":"markdown","12ca670e":"markdown","7fb54b02":"markdown","324ee013":"markdown","d0e8b446":"markdown","045c9e20":"markdown","8fb10cd5":"markdown","a4dd57a5":"markdown","263d68fb":"markdown","ba4932bd":"markdown","1c1232b4":"markdown","9fe094bf":"markdown","b43bd81f":"markdown","3b9282fe":"markdown","ba933879":"markdown","dcdcbb4b":"markdown","f3b81877":"markdown","59a17360":"markdown","70632738":"markdown","4ab8ec65":"markdown","c1ad5180":"markdown","9593b46a":"markdown","b2b42538":"markdown","f4a7b3a5":"markdown","e4a61147":"markdown","19a92c1f":"markdown","f7d8fe64":"markdown","668e7932":"markdown","ae5c8e9c":"markdown","3eb565c4":"markdown","4c0db60f":"markdown","5941a30c":"markdown","6bf3cc19":"markdown","798425d9":"markdown","5237602b":"markdown","b961a58a":"markdown","3fddc910":"markdown","acae1b99":"markdown","207fe482":"markdown","a1afd4f5":"markdown","a9a9a0b3":"markdown","1f02b723":"markdown","ea898be8":"markdown","fb055283":"markdown","3a7c7a29":"markdown","e186fa7a":"markdown","54a3bd78":"markdown","3b56d830":"markdown","4f84af21":"markdown","e5ead762":"markdown"},"source":{"be1690c4":"! pip install mglearn","7725f8fd":"import mglearn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","987997a8":"mglearn.plots.plot_linear_regression_wave()","e5b59300":"from sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.make_wave(n_samples=60)","c96f1f86":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=42)","7f9407ec":"lr = LinearRegression()\nlr.fit(x_train, y_train)","a0ce0312":"print('lr.coef_: %s' %lr.coef_)\nprint('lr.intercept_: %s' %lr.intercept_)","252e96b0":"print('training set score: %f' %lr.score(x_train, y_train))\nprint('test set score: %f' %lr.score(x_test, y_test))","9c1dc965":"X, y = mglearn.datasets.load_extended_boston()\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)","483c234b":"lr = LinearRegression()\nlr.fit(x_train, y_train)","b233e3b1":"print('training set score: %s' %lr.score(x_train, y_train))\nprint('test set score: %s' %lr.score(x_test, y_test))","23eb42da":"from sklearn.linear_model import Ridge\nX, y = mglearn.datasets.load_extended_boston()\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)","19891741":"ridge = Ridge()\nridge.fit(x_train, y_train)","f6df5646":"print('training set score: %s' %ridge.score(x_train, y_train))\nprint('test set score: %s' %ridge.score(x_test, y_test))","18711540":"ridge10 = Ridge(alpha=10)\nridge10.fit(x_train, y_train)\nprint('training set score: %s' %ridge10.score(x_train, y_train))\nprint('test set score: %s' %ridge10.score(x_test, y_test))","8732ffb1":"ridge01 = Ridge(alpha=0.1)\nridge01.fit(x_train, y_train)\nprint('training set score: %s' %ridge01.score(x_train, y_train))\nprint('test set score: %s' %ridge01.score(x_test, y_test))","4ca5e6da":"plt.title(\"ridge_coefficients\")\nplt.plot(ridge.coef_, 'o', label=\"Ridge alpha=1\")\nplt.plot(ridge10.coef_, 'o', label=\"Ridge alpha=10\")\nplt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\nplt.plot(lr.coef_, 'o', label=\"LinearRegression\")\nplt.ylim(-25, 25)\nplt.legend()","bb0b1b87":"from sklearn.linear_model import Lasso\nX, y = mglearn.datasets.load_extended_boston()\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)","e46a6885":"lasso = Lasso()\nlasso.fit(x_train, y_train)","0d556aaf":"print('training set score: %f' %lasso.score(x_train, y_train))\nprint('test set score: %f' %lasso.score(x_test, y_test))\nprint('number of features used: %d' %np.sum(lasso.coef_ !=0))","19b5a350":"lasso001 = Lasso(alpha=0.01)\nlasso001.fit(x_train, y_train)\n\nprint('training set score: %f' %lasso001.score(x_train, y_train))\nprint('test set score: %f' %lasso001.score(x_test, y_test))\nprint('number of features used: %d' %np.sum(lasso001.coef_ !=0))","5c361b4c":"lasso00001 = Lasso(alpha=0.0001)\nlasso00001.fit(x_train, y_train)\n\nprint('training set score: %f' %lasso00001.score(x_train, y_train))\nprint('test set score: %f' %lasso00001.score(x_test, y_test))\nprint('number of features used: %d' %np.sum(lasso00001.coef_ !=0))","c232472b":"plt.plot(lasso.coef_, 'o', label=\"Lasso alpha=1\")\nplt.plot(lasso001.coef_, 'o', label=\"Lasso alpha=0.01\")\nplt.plot(lasso00001.coef_, 'o', label=\"Lasso alpha=0.0001\")\nplt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\nplt.ylim(-25, 25)\nplt.legend()","fbde7549":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\n\nX, y =  mglearn.datasets.make_forge()","a9b1d524":"fig, axes = plt.subplots(1,2, figsize=(15,4))\nplt.suptitle(\"linear_classifiers\")\n\nfor model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n    clf = model.fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, ax=ax, alpha=0.7)\n    ax.scatter(X[:,0], X[:,1], c=y, s=60, cmap=mglearn.cm2)\n    ax.set_title(\"%s \"%clf.__class__.__name__)","bc10a070":"mglearn.plots.plot_linear_svc_regularization()","aaa9097d":"from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\n\nx_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, \n                                                    stratify=cancer.target, random_state=42)\nlogisticregression = LogisticRegression().fit(x_train, y_train)\nprint(\"training set score: %f\" % logisticregression.score(x_train, y_train))\nprint(\"test set score: %f\" % logisticregression.score(x_test, y_test))","bba1051c":"logisticregression100 = LogisticRegression(C=100).fit(x_train, y_train)\nprint(\"training set score: %f\" % logisticregression100.score(x_train, y_train))\nprint(\"test set score: %f\" % logisticregression100.score(x_test, y_test))","373e2b57":"logisticregression001 = LogisticRegression(C=0.01).fit(x_train, y_train)\nprint(\"training set score: %f\" % logisticregression001.score(x_train, y_train))\nprint(\"test set score: %f\" % logisticregression001.score(x_test, y_test))","81196697":"plt.plot(logisticregression.coef_.T, 'o', label=\"C=1\")\nplt.plot(logisticregression100.coef_.T, 'o', label=\"C=100\")\nplt.plot(logisticregression001.coef_.T, 'o', label=\"C=0.001\")\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nplt.ylim(-5, 5)\nplt.legend()","cb42f57b":"from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nplt.scatter(X[:,0], X[:,1], c=y, s=60, cmap=mglearn.cm3)","ab64638b":"linear_svm = LinearSVC().fit(X, y)\nprint(\"linear_svm.coef_.shape: \", linear_svm.coef_.shape)\nprint(\"linear_svm.intercept_.shape: \", linear_svm.intercept_.shape)","cac3d863":"plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap=mglearn.cm3)\nline = np.linspace(-15, 15)\nfor coef, intercept in zip(linear_svm.coef_, linear_svm.intercept_):\n    plt.plot(line, -(line * coef[0] + intercept) \/ coef[1])\nplt.ylim(-10, 15)\nplt.xlim(-10, 8)","f1514463":"As you can see, the Lasso does quite badly, both on the training and the test set, This indicates that we are underfitting.<br>\nwe find that it only used 4 of the 105 features.","3f785edb":"In the one-vs-rest approach, a binary model is learned for each class, which tries to separate this class from all the other classes, resulting in as many binary models as there are classes.","18cf87f0":"It is the simplest and most classic linear method for regression.","bcf81b6c":"* The main parameter of linear models is the regularization parameter called <b>alpha<\/b> in regression models and <b>C<\/b> in LinearSVC and LogisticRegression\n* Large alpha or small C mean simple models. In particular for regression models, tunning this parameter is quite important.\n* The other decision u have to make is whether you want to use L1 regularization or L2 regularization.\n* If u assume that only few of your of your are actually important you should use L1, otherwise, you should default to L2.\n* As L1 will use only a few features, it is easier to explain which features are important to the model, and what the effect to these features is.\n* Linear models are very fast to train and also fast to predict.\n* They scale to very large datasets and work well with sparse data.\n* Relatively easy to understand how prediction is made,using formula(1) for regreesion and formula(2) for classification.\n* Linear model perform well when the number of features is large compared to the number of samples. However other models might yeild better generalization performance.","1e4b1d41":"Let's take a look at how $LinearRegression$ performs on most complex dataset.<br>\nThis dataset has $506$ samples and $105$ derived features.","aff07e08":"* The red line shows the decision boundary for the binary classifier for red class and so on.\n* Therefore any point in this area will be classified as red by final classifier (formula(3) of red classifier is greater than zero, while it is less than zero for other two classes.","2cea4632":"we used the default parameter alpha=1.0<br>\nIncreasing alpha forces coefficient to move towards zero, which decreases training set performance, but might help generalization.<br>\nA less complex model means worse performance on the training set, but better generalization.","e33466b4":"## Linear Regression aka Ordinary Least Squares","729ae085":"An alternative to Ridge for regularizing linear regression is the $Lasso$.<br>\nThe Lasso also restrict the coefficient to be close to zero similar to Ridge regression but in slightly different way called <b>l1 regularization<\/b>","cf0ddaae":"<br>","6aa87108":"# Linear models for Regression","418f4536":"Let\u2019s visualize the lines given by the three binary classifiers:","a0ed4bff":"As expected, when moving more to the left in Figure model_complexity from an already underfit model, both training and test set accuracy decrease relative to the default parameters.","12ca9dea":"<b>using boston housing dataset -<\/b>","6f42f4ca":"A lower apha allowed us to fit more complex model, which worked better on training and test set data.","bf381b8c":"Now we train a $LinearSVC$ classifier on the dataset.","a03cd9f0":"* Linear models are a class of models that are widely used in practice, and have been studied extensively in the last few decades, with roots going back over a hundred years.\n* Linear models are models that make a prediction that using a linear function of the input features","ea22804d":"<b>using make_wave dataset from mglearn -<\/b>","75ad17d6":"<h1><center>LINEAR MODELS<\/center><\/h1>","d2635f4e":"## Ridge Regression","078114cd":"For very small values of alpha, coefficients are barely restricted at all, and we end up with a model tha resembles LinearRegression.","12ca670e":"Algorithms differ in following ways:<br>\n* How they measure,how well a particular combination of coefficients and the intercepts fits the training data.\n* If and what kind of regularization they use.","7fb54b02":"Let's try decreasing alpha:","324ee013":"* It finds the parameters $w$ and $b$ that minimize the $mean$ $squared$ $error$ between prediction and true regression targets $y$ on the training set.\n* The mean squared error is the sum of squared difference between the predictions and true values.\n* Linear Regression has no parameters, which is benifit, but it is also has no way to control model complexity.","d0e8b446":"The consequense of of l1 regularization is that when using the Lasso, some coefficients be exactly zero.<br>\nThis means some features are entirely ignored by the model.<br>\nHaving some coefficient be exactly zero often makes a model easier to interpret, and can reveal the important features of the model.","045c9e20":"* for alpha=1 (shown by blue dots) we not only see that most of the coefficients are zero, but the remaining coefficients anr also small in magnitude.\n* Decresing alpha to 0.01 (shown by orange dots) which causes most features be exactly zero.\n* Using alpha=0.0001, we get model that is quite unrelgularized, with most coefficients non zero and of large magitude.","8fb10cd5":"<br>","a4dd57a5":"## Strengths, weaknesses and parameters","263d68fb":"\\begin{align*}\n&\\hat{y} = w[0] x[0] + w[1] x[1] + \\dotsc + w[p] x[p] + b &\\text{(1) linear regression}\n\\end{align*}","ba4932bd":"* For C=100 and C=1, the coefficient is negative, while for C=0.001, the coefficeint is positive with a magnitude that even larger as for C=1.\n* Interpreting the model like this one might think the coefficient tell us which class a feature is associated with.","1c1232b4":"The slope parameters $w$, also called weights or coefficients are stored in the coef_  attribute.<br>\nThe offset or intercept $b$  is stored in the intercept_  attribite.","9fe094bf":"The formula looks similar to the linear regression, but instead of just returning the weighted sum of the features, we threshold the the predicted value at zero.<br>\nIf function was smaller than zero we predict the class -1, if it is larger than zero we predict the class +1.","b43bd81f":"An $R^2$ of arround 0.66 is not very good, but we can see that the score on training and test set are very close to each other.<br>\nThis means we are likely underfitting not overfitting.","3b9282fe":"Lasso has the regularization parameter alpha that controls how strongly coefficients are pushed towards zero.<br>\nAbove we used the default of alpha=1.0","ba933879":"For linear models of classification, $decision$ $boundary$ is a linear function of the input.<br>\nIn other words the linear classifier is a classifier that separates two classes using a line, a plane or a hyperplane.","dcdcbb4b":"Both model come up with similar decision boundaries.<br>\nBy befault both models apply **l2 regularization**, in the same way that Ridge does for regression.","f3b81877":"To make a prediction, all binary classifiers are run on a test point. The classifier that has the highest score on its single class \u201cwins\u201d and this class label is returned as prediction.","59a17360":"## LogisticRegression and LinearSVC","70632738":"<br>","4ab8ec65":"* $x[0]$ to $x[p]$ : features\n* $p$ : number of features\n* $w$ and $b$ are parameters of the model that are learned\n* $\\hat{y}$ is the prediction the model makes","c1ad5180":"Here is an illustration using $LinearSVC()$:","9593b46a":"On the left hand side, we have a very small C corresponding to a lot of regularization.<br>\nMost of the blue points are at the top, and most of the red points are at the bottom.<br>\nThe strongly regularized model chooses a relatively horizontal line, misclassifying\ntwo points.","b2b42538":"# Linear Models for Multiclass Classification","f4a7b3a5":"In the center plot, C is slightly higher, and the model focuses more on the two misclassified samples, tilting the decision boundary.","e4a61147":"<br>","19a92c1f":"\\begin{align*}\n& w[0] x[0] + w[1] x[1] + \\dotsc + w[p] * x[p] + b & \\text{ (3) classification confi\u2010\ndence}\n\\end{align*}","f7d8fe64":"* In practice, Ridge regression is usually the first choice between these two models.\n* However if you have large amount of features and expect only a few of them to be important, Lasso might be better choice.","668e7932":"* for alpha=10 (as shown by orange dots), the coeficients are mostly between around -3 and 3\n* the coeficients for model with alpha=1 (shown by blue dots) are somewhat larger\n* the red dots have larger magitude corresponding to linear regression without any regularization (would be alph=0)","ae5c8e9c":"For $LogisticRegression$ and $LinearSVC$ the trade off parameter that determines the strength of the regularization is called $C$, and higher values of **C** corresponds to less regularization.<br>\nIn other words, when using a high value of the parameter $C$ , LogisticRegression and LinearSVC try to fit the training set as best as possible, while with low values of the parameter $C$ , the model put more emphasis on finding a coefficient vector $w$ that is close to zero.","3eb565c4":"Using C=100 results in higher training set accuracy, and also a slightly increased test set accuracy, confirming our intuition that a more complex model should perform better.","4c0db60f":"# Linear Models for Classification","5941a30c":"The two most common linear classification algorithms are logistic regression and linear support vector machines.<br>\nLogisticRegression is a classification algorithm, and should not be confused with LinearRegression.","6bf3cc19":"Many Linear classification models are binary models, and extebd naturally to the multi-class case.<br>\nA common technique to extend a binary classification algorithm to multi-class classificatin algorithm is the $one-vs-rest$ approach.<br>","798425d9":"Linear models are extensively used for classification. Lets look at the <b>binary classification<\/b> first.","5237602b":"<b>Regularization<\/b> : Explicitly restrciting the model to avoid overfitting<br>\nThe particular kind used by Ridge regression is known as $l2$ $regularization$","b961a58a":"We see that the shape of the coef_ is (3, 2) , meaning that each row of coef_ contains the coefficient vector for one of the three classes. Each row has two entries, corresponding to the two features in the dataset.<br>\nThe intercept_ is now a one-dimensional array, storing the intercepts for each class.","3fddc910":"on the right hand side, a very high value of C in the model tilts the decision boundary a lot, now correctly classifying all red points.<br>\nOne of the blue points is still misclassified, as it is not possible to\ncorrectly classify all points in this dataset using a straight line.<br>\nThe model illustrated on the right hand side tries hard to correctly classify all points, but might not capture the overall layout of the classes well.<br>\nIn other words, this model is likely **overfitting**.","acae1b99":"The default value of C=1 provides quite good performance, with 95% accuracy on both the training and the test set. As training and test set performance are very close, it is likely that we are **underfitting**.","207fe482":"Having one binary classifier per class results in having one vector of coefficients $w$\nand one intercept $b$ for each class. The class for which the result of formula<br>","a1afd4f5":"Using low values of C will cause the algorithms try to adjust to the \u201cmajority\u201d of data points, while using a higher value of C stresses the importance that each individual data point be classified correctly.","a9a9a0b3":"<br>","1f02b723":"We can also investigate what happens if we use an even more regularized model than the default of C=1 , by setting C=0.01 .","ea898be8":"In Ridge regression,the coefficients w are chosen not only so that they predict well on the training data, but there is an additional constraint. We also want the magnitude of coefficients to be as small as possible; in other words, all entries of w should be close to 0 .","fb055283":"\\begin{align*}\n&\\hat{y} = w[0] x[0] + w[1] x[1] + \\dotsc + w[p] * x[p] + b > 0 &\\text{ (2) linear\nbinary classification}\n\\end{align*}","3a7c7a29":"<br>","e186fa7a":"<br>","54a3bd78":"If we set alpha too low, we again remove the effect of regularization and end up with a result to LinearRegression.","3b56d830":"When comparing training set and test set score, we find that we predict very accurately on training data , but the $R^2$ on the test set is much worse.<br>\nThis is a clear sign of $overfitting$, and therefore we should try to find a model that allows us to control complexity.","4f84af21":"Let analyze $LogisticRegression$ in more details:","e5ead762":"## Lasso"}}