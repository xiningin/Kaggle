{"cell_type":{"07c3bcd0":"code","a8c1aa21":"code","3c377a2c":"code","a3fcc3c1":"code","7bdf01f6":"code","6591f16d":"code","2de7daa2":"code","3c6fffdc":"code","8900ddac":"code","043fe240":"code","359c102c":"code","37927667":"code","7a703a50":"code","0a78a11a":"code","e812f99d":"code","5678d8ae":"markdown","0e8521eb":"markdown","99325bba":"markdown","1ef9407b":"markdown","c95c7198":"markdown","770e7dc7":"markdown","e8b236c6":"markdown","a7821309":"markdown","141ded39":"markdown","f60bb67b":"markdown","13cc0b1f":"markdown"},"source":{"07c3bcd0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a8c1aa21":"df = pd.read_csv('..\/input\/train.csv')\ndf.head()","3c377a2c":"df['imagepath'] = df[\"Image\"].map(lambda x:'..\/input\/train\/'+x)","a3fcc3c1":"labels=df.Id #using the ids of whale as labels to our model","7bdf01f6":"from sklearn import preprocessing\nfrom keras.utils import np_utils\n\nlabels1= preprocessing.LabelEncoder()\nlabels1.fit(labels)\nencodedlabels = labels1.transform(labels) #integer encoding\nclearalllabels = np_utils.to_categorical(encodedlabels) #one hot encoding\n\n","6591f16d":"df.head()","2de7daa2":"import cv2\ndef imageProcessing(imagepath,name): \n    img=cv2.imread(imagepath)\n    height,width,channels=img.shape\n    if channels !=1:\n        img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n    img=img.astype(np.float)\n    img = cv2.resize(img,(70,70))\n    img = img-img.min()\/\/img.max()-img.min()#step 3\n    img=img*255#step 4\n    return img\nt=[]\n    \nfor i in range(0,9850):\n    t.append(imageProcessing(df.imagepath[i],df.Image[i]))","3c6fffdc":"t = np.asarray(t)\nt=t.reshape(-1,70,70,1)\nt.shape","8900ddac":"#Same for test images\nfrom glob import glob\npath_to_images = '..\/input\/test\/*.jpg'\nimages=glob(path_to_images)\ntest=[]\nfor i in images:\n    img = cv2.imread(i)\n    height,width,channels=img.shape\n    if channels !=1:\n        img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n    img=img.astype(np.float)\n    img = cv2.resize(img,(70,70))\n    img = img-img.min()\/\/img.max()-img.min()\n    img=img*255\n\n    test.append(cv2.resize(img,(70,70)))\n    # Get image label (folder name)\n","043fe240":"t.shape\ntest=np.asarray(test)\ntest.shape\n","359c102c":"from keras.preprocessing.image import ImageDataGenerator\nimage_gen = ImageDataGenerator(\n    #featurewise_center=True,\n    #featurewise_std_normalization=True,\n    rescale=1.\/255,\n    rotation_range=15,\n    width_shift_range=.15,\n    height_shift_range=.15,\n    horizontal_flip=True)\n#training the image preprocessing\nimage_gen.fit(t) # fit t to the imageGenerator to let the magic happen\n","37927667":"t.shape\nclearalllabels.shape","7a703a50":"test.shape","0a78a11a":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten,ZeroPadding2D\nfrom keras.layers import Conv2D, MaxPooling2D,BatchNormalization,Activation\nfrom keras import optimizers\n\nmodel = Sequential()\nmodel.add(ZeroPadding2D((1,1),input_shape=(70,70,1)))\nmodel.add(Conv2D(64,(3,3),activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(64,(3,3),activation='relu'))\nmodel.add(MaxPooling2D((2,2),strides=(2,2)))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(128,(3,3),activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(128,(3,3),activation='relu'))\nmodel.add(MaxPooling2D((2,2),strides=(2,2)))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(256,(3,3),activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(256,(3,3),activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(256,(3,3),activation='relu'))\nmodel.add(MaxPooling2D((2,2),strides=(2,2)))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512,(3,3),activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512,(3,3),activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512,(3,3),activation='relu'))\nmodel.add(MaxPooling2D((2,2),strides=(2,2)))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512,(3,3),activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512,(3,3),activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512,(3,3),activation='relu'))\nmodel.add(MaxPooling2D((2,2),strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(4096,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4096,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4251,activation='softmax'))\n\noptimizer=optimizers.SGD()\n\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\nmodel.summary()\n","e812f99d":"#model.fit_generator(image_gen.flow(t,clearalllabels, batch_size=128),steps_per_epoch=  9850,epochs=10,verbose=1)\n","5678d8ae":"> What is Categorical Data?\n> Categorical data are variables that contain label values rather than numeric values.\n\nThe number of possible values is often limited to a fixed set.\n\nCategorical variables are often called nominal.\n\nSome examples include:\n\nA \u201cpet\u201d variable with the values: \u201cdog\u201d and \u201ccat\u201c.\nA \u201ccolor\u201d variable with the values: \u201cred\u201c, \u201cgreen\u201d and \u201cblue\u201c.\nA \u201cplace\u201d variable with the values: \u201cfirst\u201d, \u201csecond\u201d and \u201cthird\u201c.\nEach value represents a different category.\n\nSome categories may have a natural relationship to each other, such as a natural ordering.\n\nThe \u201cplace\u201d variable above does have a natural ordering of values. This type of categorical variable is called an ordinal variable.\n\nWhat is the Problem with Categorical Data?\nSome algorithms can work with categorical data directly.\n\nFor example, a decision tree can be learned directly from categorical data with no data transform required (this depends on the specific implementation).\n\nMany machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric.\n\nIn general, this is mostly a constraint of the efficient implementation of machine learning algorithms rather than hard limitations on the algorithms themselves.\n\nThis means that categorical data must be converted to a numerical form. If the categorical variable is an output variable, you may also want to convert predictions by the model back into a categorical form in order to present them or use them in some application.\n\nHow to Convert Categorical Data to Numerical Data?\nThis involves two steps:\n\nInteger Encoding\nOne-Hot Encoding\n**1. Integer Encoding\n**As a first step, each unique category value is assigned an integer value.\n\nFor example, \u201cred\u201d is 1, \u201cgreen\u201d is 2, and \u201cblue\u201d is 3.\n\nThis is called a label encoding or an integer encoding and is easily reversible.\n\nFor some variables, this may be enough.\n\nThe integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship.\n\nFor example, ordinal variables like the \u201cplace\u201d example above would be a good example where a label encoding would be sufficient.\n\n**2. One-Hot Encoding\n**For categorical variables where no such ordinal relationship exists, the integer encoding is not enough.\n\nIn fact, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n\nIn this case, a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n\nIn the \u201ccolor\u201d variable example, there are 3 categories and therefore 3 binary variables are needed. A \u201c1\u201d value is placed in the binary variable for the color and \u201c0\u201d values for the other colors.\n\nFor example:\n------------------------------\n\nred,\tgreen,\tblue\n1,\t\t0,\t\t0\n0,\t\t1,\t\t0\n0,\t\t0,\t\t1\n1\n2\n3\n4\nred,\tgreen,\tblue\n1,\t\t0,\t\t0\n0,\t\t1,\t\t0\n0,\t\t0,\t\t1","0e8521eb":"Adding a column to our data frame named by imagepath and specify the exact image path in order to read the images through it as seen\nif you are not familiar with lambda fn here http:\/\/book.pythontips.com\/en\/latest\/lambdas.html","99325bba":"Downsides : time of course it took hours to run \n**thoughts**\nfor data augmentation i could have used flow_from_directory but it would have taken time to mkdirs with all the labels inside the training directory ,see https:\/\/keras.io\/preprocessing\/image\/ \n-------------------------------------------------------------------------\nfor model selection i could have used a pretrained model but it failed because of memory restrictions on kernels, see https:\/\/keras.io\/applications\/#resnet50\n--------------------------------------------------------------------------\ni couldn't use train_test_split and aside a validation data to check the performance of the model on it but because of our imbalanced data i couldn't as for some classes there is only 1 sample for \n**next step to predict on test data and submit to kaggle**\n\nI hope i benefited you by walking you through my journey which i found amazing and full of knowledge , if you found this kernel useful please upvote , i will be doing some kernels on every idea i learn and find amazing\n*************thanks******************","1ef9407b":"Put t in the form of an array to be able to reshape it so we can feed it to the ImageGenerator that we will be discussing later","c95c7198":"**Scientists want an automatic system to recognize whale species when monitoring their activities with a surveillance system. Thus, in this competition, numerous pictures of whales\u2019 tales are given to identify whales species.\n\nIn the train set, there are 9850 images with 4251 classes, so the data set is highly class imbalance. For example, up to 2220 classes only have one samples in each class. Thus, data augmentation and different class weights in loss function are applied to mitigate the effect of class imbalance.**\nIn addition, all of images in train set are larger than common CNN input size, 224 x 224. For instance, the size of 11% of data are 1050 x 600. Therefore, Auto Encoder is applied to encode image from larger size to common CNN input size. Nevertheless, even though the performance of Auto Encoder is not bad, the improvement in all process is not obvious because of series class imbalance.\n\nIn the test set, there are 15610 images. In this competition, the method can predict 5 labels. The submission is evaluated with the Mean Average Precision.","770e7dc7":"* **Data Augmentation**\nOverfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would\nnever overfit. Data augmentation takes the approach of generating more training data\nfrom existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time,\nyour model will never see the exact same picture twice. This helps expose the model\nto more aspects of the data and generalize better.\nIn Keras, this can be done by configuring a number of random transformations to\nbe performed on the images read by the ImageDataGenerator instance. Let\u2019s get\nstarted with an this example\n--------------------------------------------------------\n\n*These are just a few of the options available (for more, see the Keras documentation).\n*Let\u2019s quickly go over this code:\n\uf0a1 rotation_range is a value in degrees (0\u2013180), a range within which to randomly\nrotate pictures.\n\uf0a1 width_shift and height_shift are ranges (as a fraction of total width or\nheight) within which to randomly translate pictures vertically or horizontally.\n\uf0a1 shear_range is for randomly applying shearing transformations.\n\uf0a1 zoom_range is for randomly zooming inside pictures.\n\uf0a1 horizontal_flip is for randomly flipping half the images horizontally\u2014relevant\nwhen there are no assumptions of horizontal asymmetry (for example,\nreal-world pictures).\n\uf0a1 fill_mode is the strategy used for filling in newly created pixels, which can\nappear after a rotation or a width\/height shift.\n","e8b236c6":"Fitting the training data with it's labels , batch size is the number of instance computed at once before the weights are updated , steps per epochs=9850 because it's the data length so we have to pass over all the data for each epoch to see every possible combination , by 10th epoch it reaches like 94% accuracy on training data ","a7821309":"Used methods:\n* Convolutional Neural Network\n* Data Augmentation\n","141ded39":"**Here fun begins ,\n**\nwe are creating a function with 2 arguments (imagepath and the image name) and this fn will read the image , check if the channels is more than 1 i.e the pictured is colored for the value 3 and convert it back to gray scale for the sake of memory , put it as float in order to normalize it in step 3,4 \n**Normalization\n**\nthe most commonly used normalization \u2014 making the data zero mean and unit variance along each feature. That is, given the data matrix X, where rows represent training instances and columns represent features, you compute the normalized matrix Xnorm with element (i,j) given by\n\nXnorm,(i,j)=X(i,j)\u2212mean(Xj)std(Xj)\n\nwhere Xj is the jth column of matrix X.\n\nThere are several advantages of doing that, many of which are interrelated:\n\nMakes training less sensitive to the scale of features: Consider a regression problem where you\u2019re given features of an apartment and are required to predict the price of the apartment. Let\u2019s say there are 2 features \u2014 no. of bedrooms and the area of the apartment. Now, the no. of bedrooms will be in the range 1\u20134 typically, while the area will be in the range 100\u2013200m2. If you\u2019re modelling the task as linear regression, you want to solve for coefficients w1 and w2 corresponding to no. of bedrooms and area. Now, because of the scale of the features, a small change in w2 will change the prediction by a lot compared to the same change in w1, to the point that setting w2 correctly might dominate the optimization process.\nRegularization behaves differently for different scaling: Suppose you have an \u21132 regularization on the problem above. It is easy to see that \u21132 regularization pushes larger weights towards zero more strongly than smaller weights. So consider that you obtain some optimal values of w1 and w2 using your given unnormalized data matrix X. Now instead of using m2 as the unit of area, if I change the data to represent area in ft2, the corresponding column of X will get multiplied by a factor of ~10. Therefore, you would expect the corresponding optimal coefficient w2 to go down by a factor of 10 to maintain the value of y. But, as stated before, the \u21132 regularization now has a smaller effect because of the smaller value of the coefficient. So you will end up getting a larger value of w2 than you would have expected. This does not make sense \u2014 you did not change the information content of the data, and therefore, your optimal coefficients should not have changed.\nConsistency for comparing results across models: As covered in point 2, scaling of features affects performance. So, if there are scientists developing new methods, and compare previous state-of-the-art methods with their new methods, which uses more carefully chosen scaling, then the results will not be reliable.\nMakes optimization well-conditioned: Most machine learning optimizations are solved using gradient descent, or a variant thereof. And the speed of convergence depends on the scaling of features (or more precisely, the eigenvalues of XTX). Normalization makes the problem better conditioned, improving the convergence rate of gradient descent. I give an intuition of this using a simple example below.\nConsider the simplest case where A is a 2 x 2 diagonal matrix, say A=diag([a1,a2]). Then, the contours of the objective function \u2225Ax\u2212b\u22252 will be axis-aligned ellipses as shown in the figure below:\n\n\nSuppose you start at the point marked in red. Observe that to reach the optimal point, you need to take a very large step in the horizontal direction but a small step in the vertical direction. The descent direction is given by the green arrow. If you go along this direction, then you will move larger distance in the vertical direction and smaller distance in the horizontal direction, which is the opposite of what you want to do!\n\nIf you take a small step along the gradient, covering the large horizontal distance to the optimal is going to take a large number of steps. If you take a large step along the gradient, you will overshoot the optimal in the vertical direction.\n\nThis behavior is due to the shape of the contours. The more circular the contours are, the faster you will converge to the optimal. The elongation of the ellipses is given by the ratio of the largest and the smallest eigenvalues of the matrix A. In general, the convergence of an optimization problem is measured by its condition number, which in this case is the ratio of the two extreme eigenvalues.\n\n(Prasoon Goyal's answer to Why is the Speed Of Convergence of gradient descent depends on the maximal and minimal eigenvalues of A in solving AX=b through least squares.)\n\nFinally, I should mention that normalization does not always help, as far as performance is concerned. Here's a simple example : consider a problem with only one feature with variance 1. Now suppose I add a dummy feature with variance 0.01. If you regularize your model correctly, the solution will not change much because of this dummy dimension. But if you now normalize it to have unit variance, it might hurt the performance.\n----------------------------------------------------------------------------------------------\nthen creating a list and loop through a  loop with the length of training images saving these images to the list we have created","f60bb67b":"> create a data frame named by df and store the image names and it's ID which we will be using it as our label later","13cc0b1f":"This is the model we used for the classification process and it's inspired from VGG16 architecture , for more info i recommend reading deep learning with keras-packt by Antonio Gulli and for any info about the model i recommend visiting https:\/\/keras.io\/"}}