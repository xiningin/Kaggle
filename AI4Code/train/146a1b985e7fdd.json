{"cell_type":{"e616719b":"code","049fd7e3":"code","9f7a03f0":"code","2a931cff":"code","e0185917":"code","f8cbd7ba":"code","c17f13d7":"code","df9fc45c":"code","82fda1e7":"code","7d32d9b3":"code","74551828":"code","eba25ff2":"code","85ab3636":"code","6ca06e83":"code","6c1c673a":"code","f44cca48":"code","c87073b1":"code","ccb8fe83":"code","1c72d858":"code","dae409e7":"code","28f96f6b":"code","3eb0593a":"code","86815b47":"code","32d5a01e":"code","127185a5":"code","12f179d3":"code","8049c23f":"code","a1368c1f":"code","9818694d":"code","17e2b677":"code","5d204017":"markdown","e9652d93":"markdown","a4861328":"markdown","be58562e":"markdown","abeed6b8":"markdown","6f0d62ed":"markdown","e79e805f":"markdown","da89146c":"markdown","3d225303":"markdown","b7ab4b85":"markdown","bc7532d4":"markdown","f2dddee9":"markdown","3c4f0284":"markdown","d9308c6f":"markdown","1d0e533e":"markdown","5af05c66":"markdown","42f33be0":"markdown","8f2f7039":"markdown","38b34093":"markdown","c584f116":"markdown","e80ae116":"markdown","7848039e":"markdown","14a67450":"markdown","2c246990":"markdown","61926c54":"markdown","fe6c374e":"markdown","75bf718b":"markdown","7badb8d7":"markdown","e6bb113a":"markdown","4d374c00":"markdown","0ca69bbb":"markdown"},"source":{"e616719b":"import pandas as pd              #for data manipulation\nimport numpy as np               #for maths\nimport matplotlib.pyplot as plt  #for visualization\n%matplotlib inline","049fd7e3":"#path to data\npath = r'..\/input\/train.csv'\n\n#read data\ntrain_data = pd.read_csv(path)","9f7a03f0":"#lets see the data\ntrain_data.head(3)       ","2a931cff":"shape = train_data.shape  #get shape\nm = shape[0]              #number of images \nnum_pix = shape[1]-1      #number of pixels, First Col is Label of Image so '-1'\nlabels = np.unique(train_data['label'])\n\nprint(\"Shape of Dataset {}\".format(shape))\nprint(\"Number of Images {}\".format(m))\nprint(\"Number of pixels {}\".format(num_pix))\nprint(\"Unique Labels    {}\".format(labels))","e0185917":"#Labels\ntrain_labels = np.array(train_data['label']).reshape(train_data.shape[0],1)\nprint(\"Shape of Labels   = {}\".format(train_labels.shape))\n\n#Features\ntrain_dataset = np.array(train_data.iloc[:,1:785]).reshape(train_data.shape[0],784)\nprint(\"Shape of Features = {}\".format(train_dataset.shape))","f8cbd7ba":"images = np.array(train_dataset[50:55,:]).reshape(5,28,28,1)   #convert it into 28*28*1\nlabs = np.array(train_labels[50:55]).reshape(5,1)\n\nfig = plt.figure()\nprint(\"Labels : \",end= ' ')\nfor i in range(5):\n    fig.add_subplot(1,5,i+1)\n    plt.imshow(images[i,:,:,0],cmap='gray')\n    print(\"{}  \".format(labs[i]),end=\" \")\n    plt.axis(\"off\")\n    ","c17f13d7":"#Normalizing the dataset\ntrain_dataset = (train_dataset-127)\/255","df9fc45c":"#Breaking the dataset into test dataset, valid dataset and train dataset.\n\ntest_dataset = np.array(train_dataset[36000:42000]).reshape(-1,num_pix)\ntest_labels = np.array(train_labels[36000:42000]).reshape(-1,1)\n\nvalid_dataset = np.array(train_dataset[30000:36000]).reshape(-1,num_pix)\nvalid_labels = np.array(train_labels[30000:36000]).reshape(-1,1)\n\ntrain_dataset = np.array(train_dataset[0:30000]).reshape(-1,num_pix)\ntrain_labels = np.array(train_labels[0:30000]).reshape(-1,1)\n\nprint(\"Train dataset = {}\".format(train_dataset.shape,train_labels.shape))\nprint(\"Valid dataset = {}\".format(valid_dataset.shape,valid_labels.shape))\nprint(\"Test dataset  = {}\".format(test_dataset.shape,test_labels.shape))","82fda1e7":"neural_train_labels = np.zeros((train_labels.shape[0],10))\nneural_valid_labels = np.zeros((valid_labels.shape[0],10))\nneural_test_labels = np.zeros((test_labels.shape[0],10))\n\nfor i,value in enumerate(train_labels):\n    neural_train_labels[i,value] = 1\n\nfor i,value in enumerate(valid_labels):\n    neural_valid_labels[i,value] = 1\n\nfor i,value in enumerate(test_labels):\n    neural_test_labels[i,value] = 1\n\nprint(\"Train labels = {}\".format(neural_train_labels.shape))\nprint(\"Valid labels = {}\".format(neural_valid_labels.shape))\nprint(\"Test labels  = {}\".format(neural_test_labels.shape))","7d32d9b3":"image_size = 28\nnum_labels = 10\nnum_channels = 1 # grayscale\n\ndef reformat(dataset):\n    dataset = dataset.reshape(-1,image_size,image_size,num_channels).astype(np.float32)\n    return dataset\n\ntrain_dataset = reformat(train_dataset)\nvalid_dataset = reformat(valid_dataset)\ntest_dataset = reformat(test_dataset)\n\nprint('Training set   = {}'.format(train_dataset.shape, neural_train_labels.shape))\nprint('Validation set = {}'.format(valid_dataset.shape, neural_valid_labels.shape))\nprint('Test set       = {}'.format(test_dataset.shape, neural_test_labels.shape))","74551828":"#pixels\nimage_size = 28\n\n#greyscale\nnum_channels = 1\n\n#patch size\npatch_size = 3\n\n#depth\ndepth = 4\n\n#hidden layers\nhidden1 = 256\n\n#hyperparameters\nlearning_rate = 0.1\n\n#regularization\nbeta = 0\n\n#target_labels\nnum_classes = 10","eba25ff2":"#sigmoid\ndef sigmoid(X):\n    return 1\/(1+np.exp(-1*X))\n\n#softmax\ndef softmax(X):\n    exp_X = np.exp(X)\n    sum_exp_X = np.sum(exp_X,1).reshape(-1,1)  #col-wise sum\n    exp_X = exp_X\/sum_exp_X\n    return exp_X","85ab3636":" def initialize_parameters():\n    #initialize weights values with 0 mean and 0.5 standard deviation.\n    mean = 0\n    std = 0.5\n    \n    #conv layer weights\n    conv_layer1_weights = np.random.normal(mean,std,(patch_size,patch_size,num_channels,depth))\n    conv_layer1_biases = np.zeros([1,depth])\n    conv_layer2_weights = np.random.normal(mean,std,(patch_size,patch_size,depth,depth*4))\n    conv_layer2_biases = np.zeros([1,depth*4])\n    \n    #fully-connected weights\n    full_layer1_weights = np.random.normal(mean,std,(((image_size\/\/4-1) * (image_size\/\/4-1) * depth * 4),hidden1))\n    full_layer1_biases = np.zeros([hidden1])\n    full_layer2_weights = np.random.normal(mean,std,(hidden1,num_classes))\n    full_layer2_biases = np.zeros([num_classes])\n    \n    parameters = dict()\n    parameters['cw1'] = conv_layer1_weights\n    parameters['cb1'] = conv_layer1_biases\n    parameters['cw2'] = conv_layer2_weights\n    parameters['cb2'] = conv_layer2_biases\n    parameters['fw1'] = full_layer1_weights\n    parameters['fb1'] = full_layer1_biases\n    parameters['fw2'] = full_layer2_weights\n    parameters['fb2'] = full_layer2_biases\n    \n    return parameters","6ca06e83":"#Convolution operation i.e multiplying Image with the weights.\n#stride hardcoded = 2\n#padding  = 0\ndef conv_multiply(image,weights):\n    hsize = (image.shape[0]-weights.shape[0])\/\/2 + 1\n    vsize = (image.shape[1]-weights.shape[1])\/\/2 + 1\n    logits = np.zeros([hsize,vsize,weights.shape[3]])\n    for d in range(weights.shape[3]):\n        row = 0\n        for rpos in range(0,(image.shape[0]-patch_size+1),2):\n            col=0\n            for cpos in range(0,(image.shape[1]-patch_size+1),2):\n                logits[row,col,d] = np.sum(np.multiply(image[rpos:rpos+patch_size, cpos:cpos+patch_size, :],weights[:,:,:,d]))\n                col += 1\n            row+=1\n    return logits","6c1c673a":"#FORWARD PROPAGATION\ndef forward_propagation(dataset,parameters):\n    #convolution layers activations\n    m = dataset.shape[0]\n    \n    #get the parameters\n    cw1 = parameters['cw1']\n    cb1 = parameters['cb1']\n    cw2 = parameters['cw2']\n    cb2 = parameters['cb2']\n    \n    fw1 = parameters['fw1']\n    fb1 = parameters['fb1']\n    fw2 = parameters['fw2']\n    fb2 = parameters['fb2']\n    \n    #to store the intermediate activations for backward propagation\n    cache = dict()\n    \n    conv_activation1 = list()\n    conv_activation2 = list()\n    \n    #image by image convolutional forward propagation\n    for i in range(m):\n        image = dataset[i]\n        logits = conv_multiply(image,cw1) + cb1\n        ca1 = sigmoid(logits)\n        ca2 = sigmoid(conv_multiply(ca1,cw2) + cb2).reshape((image_size \/\/ 4 -1) * (image_size \/\/ 4 -1) * depth * 4)\n        \n        conv_activation1.append(ca1)\n        conv_activation2.append(ca2)\n        \n    #convert into numpy array\n    conv_activation1 = np.array(conv_activation1).reshape(m,image_size \/\/ 2 -1, image_size \/\/ 2 -1, depth)\n    conv_activation2 = np.array(conv_activation2).reshape(m,image_size \/\/ 4 -1, image_size \/\/ 4 -1, depth * 4)\n        \n    #expand the conv_activation2 into (m,num_features) \n    #num_features = (image_size \/\/ 4 * image_size \/\/ 4 * depth * 4)\n    temp_activation = np.array(conv_activation2).reshape(m,(image_size \/\/ 4 -1) * (image_size \/\/ 4-1) * depth * 4)\n    \n    #fully connected layers activations\n    full_activation1 = np.matmul(temp_activation,fw1) + fb1\n    full_activation1 = sigmoid(full_activation1)\n    full_activation2 = np.matmul(full_activation1,fw2) + fb2\n    output = softmax(full_activation2)\n    \n    cache['ca1'] = conv_activation1\n    cache['ca2'] = conv_activation2\n    cache['fa1'] = full_activation1\n    cache['output'] = output\n    return cache,output","f44cca48":"#calculate conv deltas or errors only for one example\ndef conv_delta(next_error,weights):\n    delta = np.zeros([next_error.shape[0]*2+1,next_error.shape[1]*2+1,next_error.shape[2]\/\/4])\n    for d in range(weights.shape[3]):\n        row = 0\n        for rpos in range(0,delta.shape[0]-patch_size+1,2):\n            col=0\n            for cpos in range(0,delta.shape[2]-patch_size+1,2):\n                delta[rpos:rpos+patch_size,cpos:cpos+patch_size,:] += weights[:,:,:,d]*next_error[row,col,d]\n                col+=1\n            row +=1\n    return delta","c87073b1":"#conv partial derivatives only for single example\ndef conv_derivatives(delta,activation):\n    partial_derivatives = np.zeros([patch_size,patch_size,activation.shape[2],delta.shape[2]])\n    for d2 in range(0,partial_derivatives.shape[3]):\n        row=0\n        for rpos in range(0,activation.shape[0]-patch_size+1,2):\n            col = 0\n            for cpos in range(0,activation.shape[1]-patch_size+1,2):\n                partial_derivatives[:,:,:,d2] += np.multiply(activation[rpos:rpos+patch_size, cpos:cpos+patch_size, :],delta[row,col,d2])\n                col += 1\n            row += 1\n    return partial_derivatives","ccb8fe83":"def backward_propagation(dataset,labels,cache,parameters):\n    #get activations\n    output = cache['output']\n    fa1 = cache['fa1']\n    ca2 = cache['ca2']\n    ca1 = cache['ca1']\n    \n    temp_act = np.array(ca2).reshape(-1,(image_size \/\/ 4-1) * (image_size \/\/ 4 -1)* depth * 4)\n    \n    #get parameters\n    cw1 = parameters['cw1']\n    cw2 = parameters['cw2']\n    fw1 = parameters['fw1']\n    fw2 = parameters['fw2']\n    \n    \n    #cal errors fully connected\n    error_fa2 = output - labels\n    error_fa1 = np.matmul(error_fa2,fw2.T)\n    error_fa1 = np.multiply(error_fa1,fa1)\n    error_fa1 = np.multiply(error_fa1,(1-fa1))\n    error_temp = np.matmul(error_fa1,fw1.T)\n    error_temp = np.multiply(error_temp,temp_act)\n    error_temp = np.multiply(error_temp,(1-temp_act))\n    \n    m = dataset.shape[0]\n    \n    #cal errors conv layers\n    error_ca2 = np.array(error_temp).reshape(-1,image_size\/\/4-1,image_size\/\/4-1,depth*4)\n    error_ca1 = np.zeros(ca1.shape)\n    ## Image by Image error\n    for i in range(m):\n        error = conv_delta(error_ca2[i],cw2)\n        error = np.multiply(error,ca1[i])\n        error = np.multiply(error,(1-ca1[i]))\n        error_ca1 += error\n    \n    \n    #calculate partial derivatives\n    #fully connected layers\n    fd2 = (np.matmul(fa1.T,error_fa2) + beta*fw2)\/m\n    fd1 = (np.matmul(temp_act.T,error_fa1) + beta*fw1)\/m\n    \n    #conv layers\n    cd2 = np.zeros(cw2.shape)\n    cd1 = np.zeros(cw1.shape)\n    \n    ##Image by Image derivatives\n    for i in range(m):\n        cd2 = cd2 + conv_derivatives(error_ca2[i],ca1[i])\n        cd1 = cd1 + conv_derivatives(error_ca1[i],dataset[i])\n    cd2 = (cd2 + beta*cw2)\/m\n    cd1 = (cd1 + beta*cw1)\/m\n    \n    \n    #store the derivatives in dict\n    derivatives = dict()\n    \n    derivatives['cd1'] = cd1\n    derivatives['cd2'] = cd2\n    derivatives['fd1'] = fd1\n    derivatives['fd2'] = fd2\n    \n    return derivatives","1c72d858":"def update_parameters(derivatives,parameters):\n    #get parameters\n    cw1 = parameters['cw1']\n    cw2 = parameters['cw2']\n    fw1 = parameters['fw1']\n    fw2 = parameters['fw2']\n    \n    #get derivatives\n    cd1 = derivatives['cd1']\n    cd2 = derivatives['cd2']\n    fd1 = derivatives['fd1']\n    fd2 = derivatives['fd2']\n    \n    #update\n    cw1 = cw1 - learning_rate*cd1\n    cw2 = cw2 - learning_rate*cd2\n    fw1 = fw1 - learning_rate*fd1\n    fw2 = fw2 - learning_rate*fd2\n    \n    #update the dict\n    parameters['cw1'] = cw1\n    parameters['cw2'] = cw2\n    parameters['fw1'] = fw1\n    parameters['fw2'] = fw2\n    \n    return parameters","dae409e7":"def cal_loss_accuracy(true_labels,predictions,parameters):\n    #get parameters\n    cw1 = parameters['cw1']\n    cw2 = parameters['cw2']\n    fw1 = parameters['fw1']\n    fw2 = parameters['fw2']\n    \n    m = len(true_labels)\n    \n    #cal loss\n    loss = -1*(np.sum(np.multiply(np.log(predictions),true_labels),1) + np.sum(np.multiply(np.log(1-predictions),1-true_labels),1))\n    loss = np.sum(loss)\n    loss = loss + beta*(np.sum(cw1**2) + np.sum(cw2**2) + np.sum(fw1**2) + np.sum(fw2**2))\n    loss = loss\/m\n    \n    #cal accuracy\n    accuracy = np.sum(np.argmax(true_labels,1)==np.argmax(predictions,1))\/m\n    \n    return loss,accuracy","28f96f6b":"#train function\ndef train(train_dataset,train_labels,batch_size=20,iters=101,stride=2):\n    \n    #initialize the parameters\n    parameters = initialize_parameters()\n    \n    cw1 = parameters['cw1']\n    cb1 = parameters['cb1']\n    cw2 = parameters['cw2']\n    cb2 = parameters['cb2']\n    \n    fw1 = parameters['fw1']\n    fb1 = parameters['fb1']\n    fw2 = parameters['fw2']\n    fb2 = parameters['fb2']\n    \n    J = []  #store the loss o every batch\n    A = []  #store the accuracy of every batch\n    \n    \n    #training process.\n    for step in range(iters):\n        #get the batch data.\n        start = (step*batch_size)%(train_dataset.shape[0])\n        end = start + batch_size\n        \n        batch_dataset = train_dataset[start:end,:,:,:]\n        batch_labels = train_labels[start:end,:]\n        \n        #forward propagation\n        cache,output = forward_propagation(batch_dataset,parameters)\n        \n        #cal_loss and accuracy\n        loss,accuracy = cal_loss_accuracy(batch_labels,output,parameters)\n        \n        #calculate the derivatives\n        derivatives = backward_propagation(batch_dataset,batch_labels,cache,parameters)\n        \n        #update the parameters\n        parameters = update_parameters(derivatives,parameters)\n        \n        #append the loss and accuracy of every batch\n        J.append(loss)\n        A.append(accuracy)\n        \n        #print loss and accuracy of the batch dataset.\n        if(step%100==0):\n            print('Step : %d'%step)\n            print('Loss : %f'%loss)\n            print('Accuracy : %f%%'%(round(accuracy*100,2)))\n            \n    return J,A,parameters","3eb0593a":"#TRAINING\nJ,A,parameters = train(train_dataset,neural_train_labels,iters=1501)","86815b47":"#for training set\n#_,train_pred = forward_propagation(train_dataset,parameters)\n#_,train_accuracy = cal_loss_accuracy(neural_train_labels,train_pred,parameters)\n\n#for valid set\n_,valid_pred = forward_propagation(valid_dataset,parameters)\n_,valid_accuracy = cal_loss_accuracy(neural_valid_labels,valid_pred,parameters)\n\n#for test set\n_,test_pred = forward_propagation(test_dataset,parameters)\n_,test_accuracy = cal_loss_accuracy(neural_test_labels,test_pred,parameters)","32d5a01e":"#print\n#print('Accuracy of Train Set = {}'.format(round(train_accuracy*100,2)))\nprint('Accuracy of Valid Set = {}'.format(round(valid_accuracy*100,2)))\nprint('Accuracy of Test  Set = {}'.format(round(test_accuracy*100,2)))","127185a5":"avg_loss = list()\navg_acc = list()\ni = 0\nwhile(i<len(J)):\n    avg_loss.append(np.mean(J[i:i+30]))\n    avg_acc.append(np.mean(A[i:i+30]))\n    i += 30\n\nplt.plot(list(range(len(avg_loss))),avg_loss)\nplt.xlabel(\"x\")\nplt.ylabel(\"Loss (Avg of 30 batches)\")\nplt.title(\"Loss Graph\")\nplt.show()\n\nplt.plot(list(range(len(avg_acc))),avg_acc)\nplt.xlabel(\"x\")\nplt.ylabel(\"Accuracy (Avg of 30 batches)\")\nplt.title(\"Accuracy Graph\")\nplt.show()    ","12f179d3":"index = 10\ntest_image = test_dataset[index].reshape(1,28,28,1)\nplt.imshow(test_image[0,:,:,0],cmap='gray')\nprint(\"Image\")\nplt.show()\nprint(\"True_Label = {}\".format(np.argmax(neural_test_labels[index])))\nprint(\"Pred_Label = {}\".format(np.argmax(test_pred[index])))","8049c23f":"cache,pred = forward_propagation(test_image,parameters)","a1368c1f":"conv_layer1 = cache['ca1']\nconv_layer2 = cache['ca2']","9818694d":"print(\"Conv layer1 Activations\")\nfor i in range(conv_layer1.shape[3]):\n    plt.imshow(conv_layer1[0,:,:,i],cmap='gray')\n    plt.show()\n\nfig2 = plt.figure()\nprint(\"Conv layer2 Activations\")\nfor i in range(0,(conv_layer2.shape[3]-9)):\n    fig2.add_subplot(1,7,i+1)\n    plt.imshow(conv_layer2[0,:,:,i],cmap='gray')\n    ","17e2b677":"images = test_dataset[0:5]\nimage_labels = neural_test_labels[0:5]\n\nfor i in range(len(images)):\n    plt.imshow(images[i,:,:,0],cmap='gray')\n    plt.show()\n    print(\"Predicted Label = {}\".format(np.argmax(image_labels[i])))","5d204017":"### Import the Required Libraries","e9652d93":"* **Red   Cell : 5x1 + 4x2 + 3x1 + 6x3 + 2x1 + 1x0 + 5x2 + 7x3 + 3x1 = 70**\n* **Green Cell : 4x1 + 3x2 + 1x1 + 2x3 + 1x1 + 4x0 + 7x2 + 3x3 + 2x1 = 43**\n* **Blue  Cell : 6x1 + 2x2 + 1x1 + 5x3 + 7x1 + 3x0 + 1x2 + 4x3 + 2x1 = 49**\n* **Purp  Cell : 2x1 + 1x2 + 4x1 + 7x3 + 3x1 + 2x0 + 4x2 + 2x3 + 3x1 = 49**","a4861328":"### Loss and Accuracy\n* Loss = -sum((Y x log(predictions) + (1-Y) x (log(1-predictions))))\/m + (beta x (sum(cw1^2) + sum(cw2^2) + sum(fw1^2) + sum(fw2^2)))\n* Accuray = sum(Y==Predictions)\/m <br>\n\nm = number of examples","be58562e":"### Train Function\n\n1. Initialize the parameters\n2. Forward Propagation\n3. Calculate Loss and Accuracy\n4. Backward Propagation\n5. Update the Parameters \n\nBatch Size = 20 <br>\nRepeat the steps 2-5 for every Batch","abeed6b8":"#### Calculate Error terms for Conv Layers","6f0d62ed":"![]( \"https:\/\/github.com\/navjindervirdee\/neural-networks\/blob\/master\/Convolutional%20Neural%20Network\/Convolution.PNG?raw=true\" )\n","e79e805f":"### Convolution Operation\n![](https:\/\/github.com\/navjindervirdee\/neural-networks\/blob\/master\/Convolutional%20Neural%20Network\/Convolution.PNG?raw=true>)","da89146c":"### Hyperparameters","3d225303":"###  Let's Train\n\n* 1501 Iterations is equivalent to ony one pass of entire training set (20*1500==30000).i.e an image is seen only once.\n* Takes 15-20 mins for 1501 iterations on CPU","b7ab4b85":"### Lets see some images","bc7532d4":"#### Full Backward Propagation","f2dddee9":"** The above accuracy and loss are of random batch data of size = 20, and not of entire training set **","3c4f0284":"### Normalize the dataset and Break the dataset into Test, Valid, Train dataset\n\n**Normalize :**\nimage_data = (image_data-127)\/255\n\n**Data Distribution**\n1. Train Dataset - 70%\n2. Valid Dataset - 15%\n3. Test  Dataset - 15%\n\n","d9308c6f":"In the function below Hardcoded : <br>\nStride = 2 <br>\nPadding = 0","1d0e533e":"** So we have achieved an accuracy of 87% for all three datasets. Pretty Good!**","5af05c66":"### Some Predictions made by the MODEL  ","42f33be0":"# Implementing Convolutional Neural Network from Scratch.\n**Data-Set Digit Recognizer from Kaggle = https:\/\/www.kaggle.com\/c\/digit-recognizer\/data ** <br>\n**Digits 0-9**","8f2f7039":"### Forward Propagation\n![](https:\/\/github.com\/navjindervirdee\/neural-networks\/blob\/master\/Convolutional%20Neural%20Network\/Forward.JPG?raw=true)\n\n* Function returns the intermediate activations in **cache dictionary** and final output in output","38b34093":"### Update Parameters","c584f116":"### Let's  visualize what the intermediate conv layers are learning!","e80ae116":"### Reformatting the dataset\n**Example : pixels = 784 , convert them into [28,28,1]**","7848039e":"### Let's Plot some graphs\n\n*  Plotted average loss of 30 batches and average accuracy of 30 batches.","14a67450":"# THE END","2c246990":"### Initialize Weights\n* ** conv_layer1_weights = [3,3,1,4] **<br>\n* ** conv_layer2_weights = [3,3,4,16]**<br>\n* ** full_layer1_weights = [504,256] **<br>\n* ** full_layer2_weights = [256,10]  **<br>\n\nStore these weights in parameters dictionary!","61926c54":"### Get the Training Data","fe6c374e":"### Separating Features and Labels from the dataset.","75bf718b":"#### Calculate the derivatives for Conv layer Weights","7badb8d7":"### Backward Propagation\n* Apply the Chain Rule and calculate the derivatives.\n* Returns the derivatives in ** derivatives dictionary **","e6bb113a":"### Let's Calculate the accuracy of entire Training Set, Valid Set and Test Set\n* Will take around 25-30 mins on CPU!","4d374c00":"### One Hot Encoding\n\n**Example : If label = 4 : [0,0,0,0,1,0,0,0,0,0]**","0ca69bbb":"### Activation Functions\n**In-Between Layers** <br>\n* **Sigmoid = 1 \/ (1+exp(-X))** \n\n**For Final Layer**\n* **Softmax = exp(X) \/ (sum(exp(X),1))**"}}