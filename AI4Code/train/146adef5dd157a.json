{"cell_type":{"ad902af6":"code","87098bcd":"code","b12aff98":"code","f7217101":"code","1898dd6a":"code","5aadabac":"code","285957f6":"code","6c58fdb1":"code","0473c67a":"code","14431719":"code","8d3e496d":"code","551db20a":"code","c1b5f0b0":"code","f2f9c926":"code","2cc71ef0":"code","e3529624":"code","10a79fff":"code","f41e5138":"code","ec90a2e5":"code","b41f979b":"code","54a617e3":"code","d5988fb3":"code","26dd077d":"code","60027766":"code","2aa98718":"code","80ba1f33":"code","166ad838":"code","b66b5d1f":"code","26ce0f66":"code","4bbff414":"code","ff32febc":"code","6d965123":"code","df5c8e50":"code","478ff814":"code","c8465bbe":"code","0c98e444":"code","c62cfcbc":"code","a663c45a":"code","719e7389":"code","5a75190e":"code","68989594":"code","8c9081b2":"code","74cc8eb9":"code","2cc108a4":"code","2294c0ea":"code","8902ab6d":"code","925cf889":"code","e3d395cd":"code","f2487d93":"code","4de57b2c":"code","6b538449":"code","62b0a48a":"code","4e3c1779":"code","cef5fc7b":"code","87c1ae32":"code","48756fec":"code","c0bcdf20":"code","d3a6be58":"code","11c42910":"code","a35bb843":"markdown","7795d611":"markdown","449339ae":"markdown","29dc976f":"markdown","951f4200":"markdown","c669d47e":"markdown","2616769c":"markdown","af639581":"markdown","bc63bcfe":"markdown","897680c8":"markdown","4cfb61e2":"markdown"},"source":{"ad902af6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","87098bcd":"#import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport random","b12aff98":"df = pd.read_csv('\/kaggle\/input\/gpu-runtime\/sgemm_product.csv')\ndf.shape","f7217101":"#creating Runtime, target variable by taking average of Run1, Run2, Run3, Run4\ndf['Runtime']=df[['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)']].mean(axis=1)","1898dd6a":"#viewing data\ndf.head()","5aadabac":"#drop other Run time variables\ndf1=df.drop(columns =['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)'], axis = 1)\ndf1.info()","285957f6":"#checking descriptive stats\ndf1.describe().T","6c58fdb1":"#checking for NULL values\ndf1.isnull().sum() #no NULL values","0473c67a":"#checking for outliers\nplt.figure(figsize=(10,6))\nsns.boxplot(df1['Runtime']);","14431719":"#removing outliers\nQ1=df1['Runtime'].quantile(0.25)\nQ2=df1['Runtime'].quantile(0.75)\nIQR = Q2 - Q1\nLL=Q1-1.5*IQR\nUL=Q2+1.5*IQR\ndf2 = df1[(df1.Runtime>LL) & (df1.Runtime<UL)]\ndf2.describe().T","8d3e496d":"plt.figure(figsize=(10,6))\nsns.boxplot(df2['Runtime']);","551db20a":"#checking variable distribution\nfor index in range(10):\n   df2.iloc[:,index] = (df2.iloc[:,index]-df2.iloc[:,index].mean()) \/ df2.iloc[:,index].std();\ndf2.hist(figsize= (14,16));","c1b5f0b0":"#plotting the distribution of Runtime\nsns.distplot(df2['Runtime'])","f2f9c926":"df2['target']=np.log(df2.Runtime)\nsns.distplot(df2['target'])","2cc71ef0":"plt.figure(figsize=(14,14))\nsns.set(font_scale=1)\nsns.heatmap(df2.corr(),cmap='GnBu_r',annot=True, square = True ,linewidths=.5);\nplt.title('Variable Correlation')","e3529624":"#creating an intercept varible during martix dot product\ndf2.insert(0,'intercept',1)\ndf2","10a79fff":"#define cost function\ndef linear_costfunc(dfile,targetvar,coefmat):\n  loss=np.dot(dfile,coefmat.T)-targetvar\n  cost=np.sum(np.power(loss,2)\/(2*len(dfile)))\n  return cost","f41e5138":"#define gradient decent considering fixed iterations\ndef linear_gdesc(dfile,targetvar,coefmat,alpha,iterations,threshold):\n  cost_ls=[linear_costfunc(dfile,targetvar,coefmat)]\n  gddf=pd.DataFrame(coefmat)\n  for i in range(1,iterations):\n    loss=np.dot(dfile,coefmat.T)-targetvar\n    dep=np.dot(loss.T,dfile)\n    coefmat=coefmat-dep*alpha\/len(dfile)   \n    gddf=gddf.append(pd.DataFrame(coefmat),ignore_index=True)\n    cost_ls+=[linear_costfunc(dfile,targetvar,coefmat)]\n    if (abs(cost_ls[i]-cost_ls[i-1]))<=threshold:\n      break\n  gddf['cost']=cost_ls\n  #print(gddf)\n  print(\"Iterations needed to converge: \", i+1)\n  min_cost=gddf[gddf.cost==min(gddf.cost)]\n  print('Cost at convergance: ', cost_ls[i])\n  min_cost=min_cost.drop(columns='cost',axis=1)\n  #print(min_cost)\n  return min_cost","ec90a2e5":"#predicting target variable\ndef predict(cost_mat,xtest):\n  predic_target=xtest.dot(cost_mat.T)\n  return predic_target","b41f979b":"#RMSE\ndef linear_rmse(ypredict,ytest):\n  sum_sq=np.sum((ytest-ypredict)**2)\n  mse=sum_sq\/len(ytest)\n  rmse=(mse)**(1\/2)\n  return rmse","54a617e3":"#Linear Regression fucntion\ndef LinearReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold):\n  if len(alpha)>1:\n    coef_ls=[0]*len(alpha)\n    ypredict=[0]*len(alpha)\n    rmse=[0]*len(alpha)\n    for i, a in enumerate(alpha, start=0):\n      coef_ls[i]=linear_gdesc(x1_train,y1_train,coefmat,a,iterations,threshold)\n      ypredict[i]=predict(coef_ls[i],x1_test)\n      rmse[i]=linear_rmse(ypredict[i],y1_test)\n      print(\"For learning rate=\", a, \" RMSE is: \", rmse[i])\n      print(\"Coeffients: \",coef_ls[i])\n    plt.plot(alpha,rmse)\n    plt.xlabel('Learning Rate')\n    plt.ylabel('RMSE')\n    plt.show()\n  elif len(threshold)>1:\n    coef_ls=[0]*len(threshold)\n    ypredict=[0]*len(threshold)\n    rmse=[0]*len(threshold)\n    for i, t in enumerate(threshold, start=0):\n      coef_ls[i]=linear_gdesc(x1_train,y1_train,coefmat,alpha,iterations,t)\n      ypredict[i]=predict(coef_ls[i],x1_test)\n      rmse[i]=linear_rmse(ypredict[i],y1_test)\n      print(\"For threshold=\", t, \" RMSE is: \", rmse[i])\n      print(\"Coeffients: \",coef_ls[i])\n    plt.plot(threshold,rmse)\n    plt.xlabel('Threshold')\n    plt.ylabel('RMSE')\n    plt.show()\n  else:\n    coef_ls=[0]\n    ypredict=[0]\n    rmse=[0]\n    for i in range(1):\n      coef_ls[i]=linear_gdesc(x1_train,y1_train,coefmat,alpha,iterations,threshold)\n      ypredict[i]=predict(coef_ls[i],x1_test)\n      rmse[i]=linear_rmse(ypredict[i],y1_test)\n      print(\"For threshold=\", threshold,\" and learning rate: \",alpha, \" RMSE is: \", rmse[i])\n      print(\"Coeffients: \",coef_ls[i])\n    return rmse[i]","d5988fb3":"#test and train data\niterations=1000\ndf_target=df2[['target']].values\ndf_features=df2.drop(columns=['target','Runtime'],axis=1).values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)","26dd077d":"#Part 1 for minimum rmse with train and test data\nthreshold=[0.000001]\nalpha=[0.09,0.095,0.1,0.2,0.3]\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nLinearReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","60027766":"threshold=[0.000001]\nalpha=[0.001,0.01,0.1,0.2,0.5]\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nLinearReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","2aa98718":"#Part 2 for minimum rmse within training data\nthreshold=[0.000001]\nalpha=[0.7,0.75,0.8,0.85]\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nLinearReg(x1_train,x1_train,y1_train,y1_train,alpha,iterations,coefmat,threshold)","80ba1f33":"threshold=[0.000001]\nalpha=[0.001,0.01,0.1,0.8,0.9]\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nLinearReg(x1_train,x1_train,y1_train,y1_train,alpha,iterations,coefmat,threshold)","166ad838":"#Part 1 for minimum rmse with train and test data\nthreshold=[0.00000001,0.0000001,0.0000002,0.0000003,0.0000004,0.0000005,0.0000006,0.0000007,0.0000008,0.0000009,0.000001,0.000002,0.000003,0.000004,0.000005]\nalpha=[0.2]\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nLinearReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","b66b5d1f":"#Error as a function of number of gradient descent iterations for test and train\nthreshold=[0.000001]\nalpha=[0.2]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\niterations=1000\nitera=[]\nrmse=[]\ncost_ls=[linear_costfunc(x1_train,y1_train,coefmat)]\ngddf=pd.DataFrame(coefmat)\nfor i in range(1,iterations):\n  loss=np.dot(x1_train,coefmat.T)-y1_train\n  dep=np.dot(loss.T,x1_train)\n  coefmat=coefmat-dep*alpha\/len(x1_train)   \n  ypredict=predict(coefmat,x1_test)\n  rmse+=[linear_rmse(ypredict,y1_test)]\n  itera+=[i]\n  cost_ls+=[linear_costfunc(x1_train,y1_train,coefmat)]\n  if (abs(cost_ls[i]-cost_ls[i-1]))<=threshold:\n    break\n  print(\"For iteration: \",i,\" RMSE is: \", linear_rmse(ypredict,y1_test))\n#print(\"Coeffients: \",min_cost)\nplt.plot(itera,rmse)\nplt.xlabel('Iteration')\nplt.ylabel('RMSE')\nplt.show()","26ce0f66":"#Part 2 for minimum rmse within training data\n#threshold=[0.000000000000001,0.00000000000001,0.0000000000001]\nthreshold=[0.0000000000000001,0.000000000000001,0.000000000000002,0.000000000000003,0.000000000000004,0.000000000000005,0.000000000000006,0.000000000000007,0.000000000000008,0.000000000000009,0.00000000000001]\nalpha=[0.8]\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nLinearReg(x1_train,x1_train,y1_train,y1_train,alpha,iterations,coefmat,threshold)","4bbff414":"#Error as a function of number of gradient descent iterations within train\nthreshold=[0.000001]\nalpha=[0.8]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\niterations=1000\nitera=[]\nrmse=[]\ncost_ls=[linear_costfunc(x1_train,y1_train,coefmat)]\ngddf=pd.DataFrame(coefmat)\nfor i in range(1,iterations):\n  loss=np.dot(x1_train,coefmat.T)-y1_train\n  dep=np.dot(loss.T,x1_train)\n  coefmat=coefmat-dep*alpha\/len(x1_train)   \n  ypredict=predict(coefmat,x1_train)\n  rmse+=[linear_rmse(ypredict,y1_train)]\n  itera+=[i]\n  cost_ls+=[linear_costfunc(x1_train,y1_train,coefmat)]\n  if (abs(cost_ls[i]-cost_ls[i-1]))<=threshold:\n    break\n  print(\"For iteration: \",i,\" RMSE is: \", linear_rmse(ypredict,y1_train))\n#print(\"Coeffients: \",min_cost)\nplt.plot(itera,rmse)\nplt.xlabel('Iteration')\nplt.ylabel('RMSE')\nplt.show()","ff32febc":"iterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\ndf_target=df2[['target']].values\nfeatures=df2.drop(columns=['target','Runtime','intercept'])\ndf_feat=features.sample(axis = 1,random_state=0,n=8) \ndf_feat.insert(0,'intercept',1)\ndf_features=df_feat.values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)","6d965123":"#Random 8 features for test and train\nthreshold=[0.000001]\nalpha=[0.2]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\niterations=1000\nprint('For features: ', df_feat.columns)\nLinearReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","df5c8e50":"#Random 8 features within train\nthreshold=[0.000001]\nalpha=[0.8]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\niterations=1000\nprint('For features: ', df_feat.columns)\nLinearReg(x1_train,x1_train,y1_train,y1_train,alpha,iterations,coefmat,threshold)","478ff814":"#Fixed 8 features for test and train\niterations=1000\ndf_target=df2[['target']].values\nfeatures=df2.drop(columns=['target','Runtime','intercept'])\nls=['MWG', 'NWG', 'KWG', 'MDIMC', 'NDIMC', 'STRM', 'SA', 'SB']\ndf_feat=features[ls] \ndf_feat.insert(0,'intercept',1)\ndf_features=df_feat.values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)\nthreshold=[0.000001]\nalpha=[0.2]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nprint('For features: ', df_feat.columns)\nLinearReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","c8465bbe":"#Random 8 features for test and train loop to validate\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\ndf_target=df2[['target']].values\nfeatures=df2.drop(columns=['target','Runtime','intercept'])\nitera=[]\nlsrmse=[]\nfor i in range(50):\n  print('For seed=', i)\n  df_feat=features.sample(axis = 1,random_state=i,n=8) \n  df_feat.insert(0,'intercept',1)\n  df_features=df_feat.values\n  x1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)\n  threshold=[0.000001]\n  alpha=[0.2]\n  coefmat=np.zeros((1,len(x1_train[0])),dtype=int)\n  iterations=1000\n  print('For features: ', df_feat.columns)\n  r=LinearReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)\n  itera+=[i]\n  lsrmse+=[r]\nplt.plot(itera,lsrmse)","0c98e444":"#Random 8 features within train\niterations=1000\ndf_target=df2[['target']].values\nfeatures=df2.drop(columns=['target','Runtime','intercept'])\nls=['MWG', 'NWG', 'KWG', 'MDIMC', 'NDIMC', 'STRM', 'SA', 'SB']\ndf_feat=features[ls] \ndf_feat.insert(0,'intercept',1)\ndf_features=df_feat.values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)\nthreshold=[0.000001]\nalpha=[0.8]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nprint('For features: ', df_feat.columns)\nLinearReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","c62cfcbc":"#Random 8 features within train loop to validate\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\ndf_target=df2[['target']].values\nfeatures=df2.drop(columns=['target','Runtime','intercept'])\nitera=[]\nlsrmse=[]\nfor i in range(50):\n  print('For seed=', i)\n  df_feat=features.sample(axis = 1,random_state=i,n=8) \n  df_feat.insert(0,'intercept',1)\n  df_features=df_feat.values\n  x1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)\n  threshold=[0.000001]\n  alpha=[0.2]\n  coefmat=np.zeros((1,len(x1_train[0])),dtype=int)\n  iterations=1000\n  print('For features: ', df_feat.columns)\n  r=LinearReg(x1_train,x1_train,y1_train,y1_train,alpha,iterations,coefmat,threshold)\n  itera+=[i]\n  lsrmse+=[r]\nplt.plot(itera,lsrmse)","a663c45a":"iterations=1000\nmean = df2['target'].mean()\ndf2.loc[df2['target'] <= mean, 'target'] = 0\ndf2.loc[df2['target'] > mean, 'target'] = 1\ndf_target=df2[['target']].values\ndf_features=df2.drop(columns=['target','Runtime'],axis=1).values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)","719e7389":"def sigmoid(dfile,coefmat):\n  y_hat=np.dot(dfile,coefmat.T)\n  z=1\/(1+np.exp(-y_hat))\n  return z","5a75190e":"#define cost function\ndef logit_costfunc(dfile,targetvar,coefmat):\n  fterm=np.sum(np.dot(targetvar.T,np.log(sigmoid(dfile,coefmat))))\n  sterm=np.sum(np.dot((1-targetvar).T,np.log(1-sigmoid(dfile,coefmat))))\n  cost=-(fterm+sterm)\/len(dfile)\n  return cost","68989594":"#define gradient decent considering fixed iterations\ndef logit_gdesc(dfile,targetvar,coefmat,alpha,iterations,threshold):\n  cost_ls=[logit_costfunc(dfile,targetvar,coefmat)]\n  gddf=pd.DataFrame(coefmat)\n  for i in range(1,iterations):\n    loss=sigmoid(dfile,coefmat)-targetvar\n    dep=np.dot(loss.T,dfile)\n    coefmat=coefmat-dep*alpha\/len(dfile)   \n    #print('matrix: ',coefmat)\n    gddf=gddf.append(pd.DataFrame(coefmat),ignore_index=True)\n    cost_ls+=[logit_costfunc(dfile,targetvar,coefmat)]\n    #print('iteration=',i,' cost_ls:',cost_ls)\n    if (abs(cost_ls[i]-cost_ls[i-1]))<=threshold:\n      break\n  gddf['cost']=cost_ls\n  #print(gddf)\n  print(\"Iterations needed to converge: \", i+1)\n  min_cost=gddf[gddf.cost==min(gddf.cost)]\n  print('Cost at convergance: ', cost_ls[i])\n  min_cost=min_cost.drop(columns='cost')\n  return min_cost","8c9081b2":"def log_predict(cost_mat,xtest):\n  predic_target=xtest.dot(cost_mat.T)\n  target= np.where(predic_target >= 0.5 , 1, 0)\n  return target","74cc8eb9":"def accuracy(ypredict,ytest):\n  df = pd.DataFrame({'actual': ytest.flatten(), 'predicted': ypredict.flatten()})\n  correct= df.loc[df['actual'] == df['predicted']]\n  rate=len(correct)\/len(ytest)   \n  return rate","2cc108a4":"def LogisticReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold):\n  if len(alpha)>1:\n    coef_ls=[0]*len(alpha)\n    ypredict=[0]*len(alpha)\n    accu=[0]*len(alpha)\n    for i, a in enumerate(alpha, start=0):\n      coef_ls[i]=logit_gdesc(x1_train,y1_train,coefmat,a,iterations,threshold)\n      ypredict[i]=log_predict(coef_ls[i],x1_test)\n      accu[i]=accuracy(ypredict[i],y1_test)\n      print(\"For learning rate=\", a, \" Accuracy is: \", accu[i])\n      print(\"Coeffients: \",coef_ls[i])\n    plt.plot(alpha,accu)\n    plt.xlabel('Learning Rate')\n    plt.ylabel('Accuracy')\n    plt.show()\n  elif len(threshold)>1:\n    coef_ls=[0]*len(threshold)\n    ypredict=[0]*len(threshold)\n    accu=[0]*len(threshold)\n    for i, t in enumerate(threshold, start=0):\n      #print('t',t)\n      coef_ls[i]=logit_gdesc(x1_train,y1_train,coefmat,alpha,iterations,t)\n      ypredict[i]=log_predict(coef_ls[i],x1_test)\n      #print('y_hat:',ypredict)\n      accu[i]=accuracy(ypredict[i],y1_test)\n      print(\"For threshold=\", t, \" Accuracy is: \", accu[i])\n      print(\"Coeffients: \",coef_ls[i])\n    plt.plot(threshold,accu)\n    plt.xlabel('Threshold')\n    plt.ylabel('Accuracy')\n    plt.show()\n  else:\n    coef_ls=[0]\n    ypredict=[0]\n    accu=[0]\n    for i in range(1):\n      coef_ls[i]=logit_gdesc(x1_train,y1_train,coefmat,alpha,iterations,threshold)\n      ypredict[i]=log_predict(coef_ls[i],x1_test)\n      accu[i]=accuracy(ypredict[i],y1_test)\n      print(\"For threshold=\", threshold,\" and learning rate: \",alpha, \" Accuracy is: \", accu[i])\n      print(\"Coeffients: \",coef_ls[i])\n    return accu[i]","2294c0ea":"#Part 1 for minimum rmse with train and test data\nthreshold=[0.000001]\nalpha=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3]\nLogisticReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","8902ab6d":"threshold=[0.000001]\nalpha=[0.0001,0.001,0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3]\nLogisticReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","925cf889":"#Part 2 for minimum rmse within training data\nthreshold=[0.000001]\nalpha=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5]\nLogisticReg(x1_train,x1_train,y1_train,y1_train,alpha,iterations,coefmat,threshold)","e3d395cd":"threshold=[0.000001]\nalpha=[0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5]\nLogisticReg(x1_train,x1_train,y1_train,y1_train,alpha,iterations,coefmat,threshold)","f2487d93":"#Part 1 for minimum rmse with train and test data\nthreshold=[0.0000005,0.000001,0.000002,0.000003]\nalpha=[0.4]\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nLogisticReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","4de57b2c":"#Error as a function of number of gradient descent iterations for test and train\nthreshold=[0.000001]\nalpha=[0.6]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\niterations=1000\nitera=[]\naccu=[]\ncost_ls=[logit_costfunc(x1_train,y1_train,coefmat)]\ngddf=pd.DataFrame(coefmat)\nfor i in range(1,iterations):\n  loss=sigmoid(x1_train,coefmat)-y1_train\n  dep=np.dot(loss.T,x1_train)\n  coefmat=coefmat-dep*alpha\/len(x1_train)   \n  ypredict=log_predict(coefmat,x1_test)\n  accu+=[accuracy(ypredict,y1_test)]\n  itera+=[i]\n  cost_ls+=[logit_costfunc(x1_train,y1_train,coefmat)]\n  if (abs(cost_ls[i]-cost_ls[i-1]))<=threshold:\n    break\n  print(\"For iteration: \",i,\" accuracy is: \", accuracy(ypredict,y1_test))\n#print(\"Coeffients: \",min_cost)\nplt.plot(itera,accu)\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy')\nplt.show()","6b538449":"#Part 2 for minimum rmse within training data\nthreshold=[0.0000001,0.0000005,0.000001,0.000002,0.000003,0.000004,0.000005]\nalpha=[1]\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nLogisticReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","62b0a48a":"#Error as a function of number of gradient descent iterations within train\nthreshold=[0.000001]\nalpha=[1]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\niterations=1000\nitera=[]\naccu=[]\ncost_ls=[logit_costfunc(x1_train,y1_train,coefmat)]\ngddf=pd.DataFrame(coefmat)\nfor i in range(1,iterations):\n  loss=sigmoid(x1_train,coefmat)-y1_train\n  dep=np.dot(loss.T,x1_train)\n  coefmat=coefmat-dep*alpha\/len(x1_train)   \n  ypredict=log_predict(coefmat,x1_train)\n  accu+=[accuracy(ypredict,y1_train)]\n  itera+=[i]\n  cost_ls+=[logit_costfunc(x1_train,y1_train,coefmat)]\n  if (abs(cost_ls[i]-cost_ls[i-1]))<=threshold:\n    break\n  print(\"For iteration: \",i,\" accuracy is: \", accuracy(ypredict,y1_train))\n#print(\"Coeffients: \",min_cost)\nplt.plot(itera,accu)\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy')\nplt.show()","4e3c1779":"iterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nmean = df2['target'].mean()\ndf2.loc[df2['target'] <= mean, 'target'] = 0\ndf2.loc[df2['target'] > mean, 'target'] = 1\ndf_target=df2[['target']].values\nfeatures=df2.drop(columns=['target','Runtime','intercept'])\ndf_feat=features.sample(axis = 1,random_state=0,n=8) \ndf_feat.insert(0,'intercept',1)\ndf_features=df_feat.values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)","cef5fc7b":"#Random 8 features for test and train\nthreshold=[0.000001]\nalpha=[0.6]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\niterations=1000\nprint('For features: ', df_feat.columns)\nLogisticReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","87c1ae32":"#Random 8 features within train\nthreshold=[0.000001]\nalpha=[1]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\niterations=1000\nprint('For features: ', df_feat.columns)\nLogisticReg(x1_train,x1_train,y1_train,y1_train,alpha,iterations,coefmat,threshold)","48756fec":"iterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nmean = df2['target'].mean()\ndf2.loc[df2['target'] <= mean, 'target'] = 0\ndf2.loc[df2['target'] > mean, 'target'] = 1\ndf_target=df2[['target']].values\nfeatures=df2.drop(columns=['target','Runtime','intercept'])\nls=['MWG', 'NWG', 'KWG', 'MDIMC', 'NDIMC', 'STRM', 'SA', 'SB']\ndf_feat=features[ls] \ndf_feat.insert(0,'intercept',1)\ndf_features=df_feat.values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)\nthreshold=[0.000001]\nalpha=[0.6]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nprint('For features: ', df_feat.columns)\nLogisticReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","c0bcdf20":"#Random 8 features for test and train for loop to validate\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nmean = df2['target'].mean()\ndf2.loc[df2['target'] <= mean, 'target'] = 0\ndf2.loc[df2['target'] > mean, 'target'] = 1\ndf_target=df2[['target']].values\nfeatures=df2.drop(columns=['target','Runtime','intercept'])\nitera=[]\nlsaccu=[]\nfor i in range(50):\n  print('For seed=', i)\n  df_feat=features.sample(axis = 1,random_state=i,n=8) \n  df_feat.insert(0,'intercept',1)\n  df_features=df_feat.values\n  x1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)\n  threshold=[0.000001]\n  alpha=[0.6]\n  coefmat=np.zeros((1,len(x1_train[0])),dtype=int)\n  iterations=1000\n  print('For features: ', df_feat.columns)\n  a=LogisticReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)\n  itera+=[i]\n  lsaccu+=[a]\nplt.plot(itera,lsaccu)","d3a6be58":"iterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nmean = df2['target'].mean()\ndf2.loc[df2['target'] <= mean, 'target'] = 0\ndf2.loc[df2['target'] > mean, 'target'] = 1\ndf_target=df2[['target']].values\nfeatures=df2.drop(columns=['target','Runtime','intercept'])\nls=['MWG', 'NWG', 'KWG', 'MDIMC', 'NDIMC', 'STRM', 'SA', 'SB']\ndf_feat=features[ls] \ndf_feat.insert(0,'intercept',1)\ndf_features=df_feat.values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)\nthreshold=[0.000001]\nalpha=[1]\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nprint('For features: ', df_feat.columns)\nLogisticReg(x1_train,x1_test,y1_train,y1_test,alpha,iterations,coefmat,threshold)","11c42910":"#Random 8 features within train to validate\niterations=1000\ncoefmat=np.zeros((1,len(x1_train[0])),dtype=int)\nmean = df2['target'].mean()\ndf2.loc[df2['target'] <= mean, 'target'] = 0\ndf2.loc[df2['target'] > mean, 'target'] = 1\ndf_target=df2[['target']].values\nfeatures=df2.drop(columns=['target','Runtime','intercept'])\nitera=[]\nlsaccu=[]\nfor i in range(50):\n  print('For seed=', i)\n  df_feat=features.sample(axis = 1,random_state=i,n=8) \n  df_feat.insert(0,'intercept',1)\n  df_features=df_feat.values\n  x1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.2, random_state = 0)\n  threshold=[0.000001]\n  alpha=[1]\n  coefmat=np.zeros((1,len(x1_train[0])),dtype=int)\n  iterations=1000\n  print('For features: ', df_feat.columns)\n  a=LogisticReg(x1_train,x1_train,y1_train,y1_train,alpha,iterations,coefmat,threshold)\n  itera+=[i]\n  lsaccu+=[a]\nplt.plot(itera,lsaccu)","a35bb843":"Experiment 4 - Choosing 8 best features","7795d611":"Experiment 1 - varying learning rate, fixed iterations","449339ae":"Experiment 3 - choosing 8 random features","29dc976f":"Experiment 2 - varying threshold with best alpha","951f4200":"Experiment 1 - varying learning rate, fixed iterations","c669d47e":"**Data Preprocessing**","2616769c":"**Logistic Regression**","af639581":"Experiment 3 - choosing 8 random features","bc63bcfe":"Experiment 4 - Choosing 8 best features","897680c8":"Experiment 2 - varying threshold with best alpha","4cfb61e2":"**Linear Regression**"}}