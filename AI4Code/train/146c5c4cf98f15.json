{"cell_type":{"fe0beb4d":"code","a9147a02":"code","674daabf":"code","8adefcb2":"code","b1e31dcd":"code","89e5b854":"code","b7efb9e8":"code","db0ad665":"code","5b60db57":"code","3e6b292c":"markdown","dc0ec933":"markdown","5d7ea186":"markdown","4ea77262":"markdown","0623929e":"markdown","4d4eb713":"markdown","2a55f196":"markdown","6af1c272":"markdown"},"source":{"fe0beb4d":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # silence info and warnings from tensorflow\nfrom tqdm import tqdm # for progress bar\nimport tensorflow as tf \nfrom tensorflow import keras # will use keras Sequential model\nimport numpy as np\nimport pandas as pd\nimport sys\nimport json\nfrom joblib import Parallel,delayed\nimport glob","a9147a02":"# given data directory and sequence ID, returns a list of tensors containing dssp, HMM and PSI matrices\ndef parse_seqID(data_dir,seqID):\n    seqID = str(seqID)\n    hmm_path = data_dir + seqID + '.hmm'\n    pssm_path = data_dir + seqID + '.pssm'\n    dssp_path = data_dir + seqID + '.dssp'\n    assert os.path.exists(pssm_path)\n    hmm = np.loadtxt(hmm_path,delimiter=' ',dtype=np.float32)\n    pssm = np.loadtxt(pssm_path,delimiter=' ',dtype=np.float32)\n    dssp = parse_dssp(dssp_path)\n    return [tf.convert_to_tensor(x,dtype=tf.float32) for x in [dssp,hmm,pssm]]\n\ndef parse_dssp(dssp_path):\n    # one hot encoding e.g.,\n    # '-H---HHHH--EE--' is converted into [[0,0,1],[0,1,0],[0,0,1],...]\n    def encode_dssp(s):\n        res = np.empty(shape=(len(s),3),dtype=np.byte)\n        res[:] = np.nan\n        for i in range(0,len(s)):\n            if s[i] == 'H':\n                res[i] = np.array([0,1,0])\n            else:\n                if s[i] == 'E':\n                    res[i] = np.array([1,0,0])\n                else:\n                    assert s[i]\n                    res[i] = np.array([0,0,1])\n        assert not np.isnan(np.sum(res))\n        return res\n    with open(dssp_path,'r') as f:\n        dssp = f.read().strip()\n    return encode_dssp(dssp)\n\n# loads data from a a splits file\ndef load_train_data(data_dir, splits_dir):\n    \n    with open(splits_dir,'r') as f:\n        splits = json.load(f)\n    \n    def get_split_data(split):\n        return {seqID:parse_seqID(data_dir,seqID) for seqID in split}\n    \n    return [get_split_data(split) for split in tqdm(splits)]","674daabf":"def window(data):  \n    return Parallel(n_jobs=-1,verbose=2)(delayed(window_split)(data[i],8) for i in range(len(data)))\n\ndef window_split(data,flank):\n        data_out = {}\n        for key in data.keys():\n            whmm = _window(data[key][1],flank)\n            wpssm = _window(data[key][2],flank)\n            data_out[key] = [data[key][0], whmm, wpssm]\n        return data_out\n\ndef _window(x, flank):\n            seqlen,ncol = x.get_shape()\n            padded_x = tf.pad(x, ((flank,flank),(0,0)),'constant')\n            ta = tf.TensorArray(dtype=tf.dtypes.float32, size=seqlen)\n            for i in range(seqlen):\n                window_x = padded_x[i:i+2*flank+1]\n                vector_x = tf.reshape(window_x,(-1,))\n                ta.write(i, vector_x)\n            return ta.stack()\n    \ndef get_x(data_dict):\n    for _,(_,hmm,pssm) in data_dict.items():\n        data = tf.concat([hmm,pssm],axis=1)\n        yield data\n\ndef get_y(data_dict):\n    for _,(dssp,_,_) in data_dict.items():\n        yield dssp","8adefcb2":"def run_cross_val(data_dir,splits_dir,**params):\n    \n    clf_models = []\n    clf_history = []\n    \n    print('Loading training data from source files into memory')\n    data = load_train_data(data_dir,splits_dir)\n    print('Preprocessing training data with sliding window')\n    data = window(data)\n    \n    def split_data(val_idx):\n        d_train={}\n        for idx, d_split in enumerate(data):\n            if idx!=val_idx:\n                d_train.update(d_split)\n        return d_train, data[val_idx]\n    \n    for k in range(7):\n\n        print(f'Commencing cross validation fold {k+1}')\n\n        clf = keras.Sequential([\n            keras.layers.Dense(units=100,input_shape=[748],activation='sigmoid',kernel_initializer=keras.initializers.RandomUniform()),\n            keras.layers.Dense(units=3,activation='softmax',kernel_initializer=keras.initializers.RandomUniform()),\n        ])\n\n        clf.compile(loss = params['loss'], optimizer = params['optimizer'], metrics=['accuracy'])\n\n        d_train,d_valid = split_data(k)\n\n        trg_x = tf.concat(list(get_x(d_train)),axis=0)\n        trg_y = tf.concat(list(get_y(d_train)),axis=0)\n        val_x = tf.concat(list(get_x(d_valid)),axis=0)\n        val_y = tf.concat(list(get_y(d_valid)),axis=0)    \n\n        fit_params = {\n            'x' : trg_x,\n            'y' : trg_y,\n            'validation_data':(val_x,val_y),\n            'epochs':params['epochs'],\n            'batch_size':params['batch_size'],\n            'verbose' : 2,\n        }\n\n        history = clf.fit(**fit_params)\n        history = history.history # dictionary\n        clf_models.append(clf)\n        clf_history.append(pd.DataFrame(history))\n\n        print(f'Saving training results for fold {k+1}')\n\n        clf.save(f'model_{k+1}')\n        with open(f'model_{k+1}.json','w') as f:\n            json.dump(history,f)\n\n    return clf_models,clf_history","b1e31dcd":"data_dir = '..\/input\/dcpb1500\/train\/'\nsplits_dir = '..\/input\/dcpb1500\/shuffle.json'\n\nparams = dict(\n    batch_size = 1024,\n    loss = keras.losses.CategoricalCrossentropy(),\n    optimizer = keras.optimizers.SGD(learning_rate=1e-2),\n    epochs = 30\n)\n\n# using a FFNN with 100 hidden nodes\nmodels,history=run_cross_val(data_dir,splits_dir,**params)","89e5b854":"# visualize categorical cross-entropy loss of training set and validation set, for all 7 folds of cross-validation\n\nloss = pd.concat([df['loss'] for df in history], axis = 1)\nloss.columns = [f\"loss_{k}\" for k in range(1,8)]\n\nval_loss = pd.concat([df['val_loss'] for df in history], axis = 1)\nval_loss.columns = [f\"val_loss_{k}\" for k in range(1,8)]\n\nlosses = pd.concat([loss, val_loss], axis=1)\nax = losses.plot()\nlgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","b7efb9e8":"# visualize accuracy of the training set and validation set, for all 7 folds of cross validation\n\nacc = pd.concat([df['accuracy'] for df in history], axis = 1)\nacc.columns = [f\"accuracy_{k}\" for k in range(1,8)]\n\nval_acc = pd.concat([df['val_accuracy'] for df in history], axis = 1)\nval_acc.columns = [f\"val_accuracy_{k}\" for k in range(1,8)]\n\naccs = pd.concat([acc, val_acc], axis=1)\nax = accs.plot()\nlgd = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))","db0ad665":"test_dir = '..\/input\/dcpb1500\/blind\/'\nseq_ids_pattern = test_dir + '*.pssm'\n\n\ntest_files = glob.glob(seq_ids_pattern)\ntest_seqids = [f.split('\/')[-1].replace('.pssm','') for f in test_files]\n\ntest_data = [(seqid, parse_seqID(test_dir, seqid)) for seqid in tqdm(test_seqids)]","5b60db57":"preds = []\nfor seqtuple in tqdm(test_data):\n    # seqtuple[0] => seqID\n    # seqtuple[1] => (target, predictors)\n    seqid = seqtuple[0]\n    target = seqtuple[1][0]\n    features = _window(seqtuple[1][1], 8)\n    assert 0\n    \n    pred = [model.predict(x, y) for model in models for x,y in zip(features, target)]\n    \n    preds.append({seqid:pred})","3e6b292c":"# Neural net training ","dc0ec933":"# Prediction on blind test set!\ud83d\udd2e","5d7ea186":"# Cross validation routine","4ea77262":"# Plot loss and accuracy \ud83c\udfaf","0623929e":"# Preprocessing functions","4d4eb713":"# Parsing functions","2a55f196":"# Load packages","6af1c272":"# Greetings biologists and non-biologists!\n\nFirst of all, apologies for using a \ud83e\uddec in the title of a notebook about protein (secondary) structure prediction \ud83d\ude30. I couldn't find a  protein emoji. Anyway, we are working with sequences.\n\nThis notebook gives a glimpse of what was the state-of-art back in the 1990s when joint prediction (JPred) of protein secondary structure was conceived. This protein secondary structure prediction webserver is accessible at https:\/\/www.compbio.dundee.ac.uk\/jpred\/. Credits to the Barton Group, University of Dundee.\n\n## Key topics covered in this notebook:\n1. Converting multiple alignments data into training records \ud83d\udcdc (more tedious than you think!)\n2. Converting secondary structure data into labels - one-hot encoding \ud83d\udcbb\n3. Build a simple feed-forward neural network \ud83c\udfb0\n4. Train the network with 7-fold cross validation \ud83e\udd16\n5. Accuracy analysis on test set! \ud83d\udcc8\n\nWe will also learn to use the `joblib` library for parallelizing the file I\/O.\n \n## References\nMost of the code from this notebook is taken from: https:\/\/github.com\/JiaGengChang\/JPred4_retraining"}}