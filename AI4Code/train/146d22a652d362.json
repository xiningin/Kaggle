{"cell_type":{"e5361ab6":"code","e201b4d8":"code","824323a9":"code","322f066f":"code","2b7fab54":"code","04fa8472":"code","6600bd14":"code","6bf41f27":"code","19eeafdc":"code","0f13af2c":"code","061f71f4":"code","38b9a3ce":"code","c325716a":"code","8277cef3":"code","4dd5c85e":"code","cc3a398f":"code","eb623a93":"code","16ca1c14":"code","e366d598":"code","97dd310d":"code","1013a210":"code","4d8957d1":"code","795275ec":"code","4d260183":"code","93bb24a1":"code","752ae842":"code","23277f45":"code","6b97a857":"code","d85539b7":"code","c55838e1":"code","530aed27":"code","e51776af":"code","8f055349":"code","1b189197":"code","fd9adf6a":"code","fb816d72":"code","b87aabbd":"code","cac28fae":"code","986b5bc6":"markdown","9621f6f9":"markdown","5689e5c5":"markdown","2d8d13c3":"markdown","981ed65a":"markdown","00452994":"markdown","bb7ee057":"markdown","37df01a6":"markdown","66c98bf1":"markdown","e670ac5b":"markdown","fbeb7310":"markdown","9c9e7edb":"markdown","9929fe55":"markdown"},"source":{"e5361ab6":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e201b4d8":"Train = pd.read_csv(\"..\/input\/fake-news-data\/kaggle_fake_train.csv\")\n","824323a9":"# here we are printing first five lines of our train dataset\nTrain.head()","322f066f":"# here we are Getting the Independent Features\nX=Train.drop('label',axis=1)","2b7fab54":"# printing head of our independent features\nX.head()","04fa8472":"# here we are printing shape of our dataset\nTrain.shape\n","6600bd14":"# here we are checking if there is null value or not\nTrain.isnull().sum()","6bf41f27":"# here we are droping NaN values from our dataset\nTrain=Train.dropna()","19eeafdc":"# here we are checking again if there is any NaN value or not\nTrain.isnull().sum()","0f13af2c":"Train.head(10)","061f71f4":"import seaborn as sns\nsns.catplot('label', data=Train, kind='count')\n","38b9a3ce":"import plotly.graph_objs as go\nimport plotly.figure_factory as ff","c325716a":"import plotly","8277cef3":"labels = ['0',' 1']\nvalues = [\n      len(Train[(Train[\"label\"] == 0)]), \n      len(Train[(Train[\"label\"] == 1)]), \n]\ncolors = ['#FEBFBB', '#E13966']\n\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='label+percent', textinfo='value', \n               textfont=dict(size=20),\n               marker=dict(colors=colors, \n                           line=dict(color='#000000', width=2)))\n\nplotly.offline.iplot([trace], filename='styled_pie_chart')","4dd5c85e":"# here we are copying our dataset .\nTrain=Train.copy()","cc3a398f":"# here we are reseting our index\nTrain.reset_index(inplace=True)","eb623a93":"# here we are printing our first 10 line of dataset for checking indexing\nTrain.head(10)","16ca1c14":"x=Train['title']\n# here we are making independent features\ny=Train['label']\ny.shape","e366d598":"# here we are importing nltk,stopwords and porterstemmer we are using stemming on the text \n# we have and stopwords will help in removing the stopwords in the text\n\n#re is regular expressions used for identifying only words in the text and ignoring anything else\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","97dd310d":"ps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(Train)):\n    review = re.sub('[^a-zA-Z]', ' ', Train['title'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","1013a210":"corpus[30]","4d8957d1":"# here we are setting vocabulary size\nvoc_size=5000","795275ec":"# here we are performing one hot representation\nfrom tensorflow.keras.preprocessing.text import one_hot\none_hot_rep=[one_hot(words,voc_size)for words in corpus] ","4d260183":"# here we are printing length of first line\nlen(one_hot_rep[0])","93bb24a1":"# here we are printing length of 70 line\nlen(one_hot_rep[70])","752ae842":"# here we are importing library for doind padding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n# here we are specifying a sentence length so that every sentence in the corpus will be of same length\n\nsentence_length=25\n\n# here we are using padding for creating equal length sentences\n\n\nembedded_docs=pad_sequences(one_hot_rep,padding='pre',maxlen=sentence_length)\nprint(embedded_docs)","23277f45":"z =np.array(embedded_docs)\ny =np.array(y)","6b97a857":"# here we are printing shape \nz.shape,y.shape","d85539b7":"# here we are splitting the data for training and testing the model\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(z, y, test_size=0.10, random_state=42)","c55838e1":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\ndef print_score(clf, x_train, y_train, x_test, y_test, train=True):\n    if train:\n        pred = clf.predict(x_train)\n        print(\"Train Result:\\n===========================================\")\n        print(f\"accuracy score: {accuracy_score(y_train, pred):.4f}\\n\")\n        print(f\"Classification Report: \\n \\tPrecision: {precision_score(y_train, pred)}\\n\\tRecall Score: {recall_score(y_train, pred)}\\n\\tF1 score: {f1_score(y_train, pred)}\\n\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, clf.predict(x_train))}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(x_test)\n        print(\"Test Result:\\n===========================================\")        \n        print(f\"accuracy score: {accuracy_score(y_test, pred)}\\n\")\n        print(f\"Classification Report: \\n \\tPrecision: {precision_score(y_test, pred)}\\n\\tRecall Score: {recall_score(y_test, pred)}\\n\\tF1 score: {f1_score(y_test, pred)}\\n\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","530aed27":"from sklearn.ensemble import BaggingClassifier\nbagging_clf = BaggingClassifier(n_estimators=10, random_state=42)\nbagging_clf.fit(x_train, y_train)","e51776af":"print_score(bagging_clf, x_train, y_train, x_test, y_test, train=True)\nprint_score(bagging_clf, x_train, y_train, x_test, y_test, train=False)","8f055349":"from sklearn.ensemble import RandomForestClassifier\n\nrand_forest = RandomForestClassifier(random_state=42, n_estimators=1000)\nrand_forest.fit(x_train, y_train)","1b189197":"print_score(rand_forest, x_train, y_train, x_test, y_test, train=True)\nprint_score(rand_forest, x_train, y_train, x_test, y_test, train=False)","fd9adf6a":"from sklearn.ensemble import AdaBoostClassifier\n\nada_boost_clf = AdaBoostClassifier(n_estimators=30)\nada_boost_clf.fit(x_train, y_train)","fb816d72":"print_score(ada_boost_clf, x_train, y_train, x_test, y_test, train=True)\nprint_score(ada_boost_clf, x_train, y_train, x_test, y_test, train=False)","b87aabbd":"from sklearn.ensemble import GradientBoostingClassifier\n\ngrad_boost_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngrad_boost_clf.fit(x_train, y_train)","cac28fae":"print_score(grad_boost_clf, x_train, y_train, x_test, y_test, train=True)\nprint_score(grad_boost_clf, x_train, y_train, x_test, y_test, train=False)","986b5bc6":"as above we are observing that after removing NaN values index number 6 and number 8 is missing because they are NaN value so they are get removed so for managing index we have to do index reseting","9621f6f9":"# Reading dataset","5689e5c5":"# Random Forest Classifier","2d8d13c3":"# Stochastic Gradient Boosting","981ed65a":"# Building Models","00452994":"# Bagging Classifier","bb7ee057":"1. train.csv: A full training dataset with the following attributes:\n2. id: unique id for a news article\n3. title: the title of a news article\n4. author: author of the news article\n5. text: the text of the article; could be incomplete\n6. label: a label that marks the article as potentially unreliable.\nWhere 1: unreliable and 0: reliable.","37df01a6":"# Spiliting and Training","66c98bf1":"# Importing important libraries","e670ac5b":"# Contents\n","fbeb7310":"# Dataset","9c9e7edb":"# Data Pre-Processing","9929fe55":"# Boosting Algorithms\n\nAdaBoost\n\nStochastic Gradient Boosting"}}