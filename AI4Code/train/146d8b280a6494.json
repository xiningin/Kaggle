{"cell_type":{"a47671dd":"code","90c31035":"code","97b291d0":"code","10fa221c":"code","edcc5640":"code","9b58b16c":"code","830e1827":"code","4cc01e5d":"code","7f3b7176":"code","691e340c":"code","29c0049d":"code","fd2f9b1b":"code","3e812e0e":"code","df6cba92":"code","31c4e344":"code","ab1e1878":"code","483f412f":"code","715682f6":"code","d627c6db":"code","1810881f":"code","a9a9b3e7":"code","065d5767":"code","3522a096":"code","3717ad9d":"code","03ac6e82":"code","bb0b23ca":"code","0e6052a9":"code","cf0af945":"code","988117fa":"code","a4179196":"code","9e042641":"code","3960b13d":"code","1b26daa1":"code","79f39a3b":"code","c50c13cc":"code","b3f164dc":"code","fb809874":"code","1c1ff083":"code","f8d90605":"code","0b2e75f0":"code","6f38b05d":"code","bc7b47a3":"code","4a0513c0":"code","ef326410":"markdown","5af28bec":"markdown","f0ece19a":"markdown"},"source":{"a47671dd":"import numpy as np\nimport os\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n# importing required libraries\nimport seaborn as sns\n\n# importing matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline","90c31035":"import pandas as pd\n#sample = pd.read_csv(\"..\/input\/sample.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/train.csv\")","97b291d0":"#create a copy of the dataframe\ndata = pd.DataFrame.copy(train)","10fa221c":"data.head()","edcc5640":"data.isnull().sum()","9b58b16c":"data.head(1)\nprint(data.dtypes)\nprint(data.shape)\nprint(data.describe())","830e1827":"# Summary of Object Variables\ndata.describe(include=[np.object])","4cc01e5d":"# Summary of Numeric Variables\ndata.describe(percentiles=[.05,.25,.5,.75,.95]).round(1)","7f3b7176":"# For tag appeared in title\ndata[\"tag appered in title\"] = data[\"tag appered in title\"].fillna(data[\"tag appered in title\"].mode()[0])","691e340c":"# Summary of Boolean Variables\ndata.select_dtypes(include=[np.bool]).apply(pd.Series.value_counts,dropna=False)","29c0049d":"data = data.replace([''], np.nan)","fd2f9b1b":"# For Tag_count\ndata[\"Tag_count\"] = data[\"Tag_count\"].fillna(data[\"Tag_count\"].mode()[0])\ndata['Tag_count'] = data['Tag_count'].astype(int)\n\n# For Trend_tag_count\ndata[\"Trend_tag_count\"] = data[\"Trend_tag_count\"].fillna(data[\"Trend_tag_count\"].mode()[0])\ndata['Trend_tag_count'] = data['Trend_tag_count'].astype(str)\ndata['Trend_tag_count'] = data['Trend_tag_count'].apply(lambda x: x.replace('>',data['Trend_tag_count'].mode()[0]))\ndata['Trend_tag_count'] = data['Trend_tag_count'].astype(int)\n\n# For subscriber\ndata[\"subscriber\"] = data[\"subscriber\"].fillna(data[\"subscriber\"].mode()[0])\ndata['subscriber'] = data['subscriber'].astype(int)\n\n# For category_id\ndata[\"category_id\"] = data[\"category_id\"].fillna(data[\"category_id\"].mode()[0])\ndata['category_id'] = data['category_id'].astype(str)\ndata['category_id'] = data['category_id'].apply(lambda x: x.replace('\u00e2\u20ac\u015324',data['category_id'].mode()[0]))\ndata['category_id'] = data['category_id'].apply(lambda x: x.replace('\u201c24','24'))\ndata['category_id'] = data['category_id'].astype(int)\n\n# For comment_count \ndata['comment_count'] = data['comment_count'].apply(pd.to_numeric, errors='coerce')\ndata['comment_count'] = data['comment_count'].fillna(data['comment_count'].median())\ndata['comment_count'] = data['comment_count'].astype(int)\n\n\n\n# For views\ndata['views'] = data['views'].astype(str)\ndata['views'] = data['views'].apply(lambda x: x.replace('#VALUE!',data['views'].mode()[0]))\ndata['views'] = data['views'].astype(int)","3e812e0e":"from sklearn.preprocessing import LabelEncoder\nencode = LabelEncoder()\n\ndata[\"comment_disabled\"] = encode.fit_transform(data[\"comment_disabled\"])\ndata[\"like dislike disabled\"] = encode.fit_transform(data[\"like dislike disabled\"])\ndata[\"tag appered in title\"] = encode.fit_transform(data[\"tag appered in title\"])","df6cba92":"# overwriting data after changing format \ndata[\"publish_date\"]= pd.to_datetime(data[\"publish_date\"]) \ndata[\"trending_date\"]= pd.to_datetime(data[\"trending_date\"])\n","31c4e344":"# Transforming publish_date to datetime\npublish_time = pd.to_datetime(data.publish_date, format='%Y-%m-%dT%H:%M:%S.%fZ')","ab1e1878":"# Create New Variable Counting Days to Achieving Trending Status\ndata['days_to_trending'] = (data.trending_date - data.publish_date).dt.days\ndata.days_to_trending.describe(percentiles=[.05,.25,.5,.75,.95])","483f412f":"#data[\"days_to_trending\"].head(5)","715682f6":"# Transform trending_date to datetime date format\ndata['trending_date'] = pd.to_datetime(data['trending_date'], format='%y-%m-%d').dt.date\ndata.trending_date.value_counts().sort_index(inplace=True)\ndata.head()","d627c6db":"data[\"days_to_trending\"] = data[\"days_to_trending\"].fillna(data[\"days_to_trending\"].mode()[0])\ndata['days_to_trending'] = data['days_to_trending'].astype(int)","1810881f":"# Dataset is sorted by trending_date\npd.Index(data.publish_date).is_monotonic","a9a9b3e7":"data.columns","065d5767":"#get correlations of each features in dataset\n#corrmat = df.corr()\n#top_corr_features = corrmat.index\n#plt.figure(figsize=(20,20))\n#plot heat map\n#g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","3522a096":"df = data.drop(['Video_id', 'publish_date', 'trending_date','channel_title', 'title', 'tags', 'description'], axis = 1)","3717ad9d":"df.head()","03ac6e82":"\"\"\"features = features.drop(['Video_id', 'publish_date', 'trending_date','channel_title', 'title', 'tags', 'description'], axis = 1)\nfeatureshot = pd.get_dummies(features)\nencoded = list(featureshot.columns)\n\nprint(encoded)\"\"\"\nfeatures = df.drop('views', axis = 1)\ntarget = df['views']","bb0b23ca":"\n\n#One Hot encode features with textual values\n#featureshot = pd.get_dummies(features)\n#encoded = list(featureshot.columns)\n\n#print(encoded)","0e6052a9":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nnumerical = ['category_id','subscriber', 'Trend_day_count', 'Tag_count', 'Trend_tag_count', 'comment_count',\n             'likes','dislike','days_to_trending']\nfeatures[numerical] = scaler.fit_transform(features[numerical])\n# Show an example of a record with scaling applied\ndisplay(features.head(n = 5))\n","cf0af945":"\"\"\"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nnumerical = ['subscriber', 'Trend_day_count', 'Tag_count', 'Trend_tag_count', 'comment_count',\n             'likes','dislike', 'days_to_trending']\nfeatures[numerical] = scaler.fit_transform(features[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(features.head(n = 5))\"\"\"","988117fa":"features.head(3)","a4179196":"#Check if we have correlating features which we could drop from the data\n\ncorr = features.corr()\n# plot the heatmap\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\n\nwith sns.axes_style(\"white\"):\n    sns.heatmap(corr, mask=mask, annot=True, annot_kws={\"size\": 7}, cmap='RdBu', fmt='+.2f', cbar=False)","9e042641":"#from sklearn.model_selection import train_test_split\n\n#X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.25, random_state = 100)\n\n\n#print(\"Training set has {} samples.\".format(X_train.shape[0]))\n#print(\"Testing set has {} samples.\".format(X_test.shape[0]))","3960b13d":"X = features\ny = target","1b26daa1":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nfrom sklearn.preprocessing import PolynomialFeatures","79f39a3b":"num_instances = len(X)\n\nmodels = []\n#models.append(('Ridge Reg', Ridge()))\n#models.append(('Linear Reg', LinearRegression()))\nmodels.append(('Lasso Reg', Lasso()))\n#models.append(('Poly Reg',PolynomialFeatures(degree = 11)))\n#models.append(('Bag Reg', BaggingRegressor()))\nmodels.append(('RandomForest Reg', RandomForestRegressor()))\nmodels.append(('XGBoost Reg', xgb.XGBRegressor()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('Decesion Tree reg', DecisionTreeRegressor()))\nmodels.append(('SVM', SVR()))\n\n# Evaluations\nresults = []\nnames = []\nscoring = []\n\nfor name, model in models:\n    # Fit the model\n    model.fit(X, y)\n    \n    predictions = model.predict(X)\n    \n    # Evaluate the model\n    score = explained_variance_score(y, predictions)\n    mae = mean_absolute_error(y, predictions)\n    r2 = r2_score(y, predictions)\n    mse = mean_squared_error(y, predictions)\n    # print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    results.append(mae)\n    names.append(name)\n    \n    msg = \"%s: %f (%f) (%f) (%f)\" % (name, score, mae, r2, mse)\n    print(msg)","c50c13cc":"from sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nrf = RandomForestRegressor(n_estimators=500,criterion=\"mse\",min_samples_split=8)\nxjb = xgb.XGBRegressor(base_estimator=rf, learning_rate=500)\nrf.fit(X, y)\nrf.score(X, y)\ny_pred_rf = rf.predict(X)\nRMSE_rf =np.sqrt(mean_squared_error(y,y_pred_rf))\nRMSE_rf","b3f164dc":"y_pred_rf","fb809874":"df = pd.DataFrame( y_pred_rf ) ","1c1ff083":"df","f8d90605":"y.head(10)","0b2e75f0":"data.head()","6f38b05d":"predictions","bc7b47a3":"# intialise data of lists. \n\n  \n# Create DataFrame \ndf = pd.DataFrame( predictions ) \ndf  \n# Print the output","4a0513c0":"df.to_csv('mycsvfile4.csv',index=False)","ef326410":"* ### Handling the missing values","5af28bec":"* ## Creating copy of dataset to work(Not to modify original)","f0ece19a":"* #### Days to Trending"}}