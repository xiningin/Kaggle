{"cell_type":{"db8eb3b7":"code","00aa6311":"code","44812cab":"code","77d6d718":"code","29dcc887":"code","ec7fbcb7":"code","b88e3aba":"code","e589639e":"code","3f202976":"code","4f820b97":"code","4637426e":"code","27d21498":"code","2249c3dd":"code","339957fc":"code","5a0296dd":"code","914b0742":"code","af44d767":"code","6dcaf802":"code","04f01a51":"code","c2911fa0":"code","b09d407a":"code","5e682969":"code","28b66fac":"code","b991d3d1":"code","ffd5e896":"code","68a0462d":"code","444514d3":"code","e3d6b387":"markdown","ecf99df6":"markdown","d0c64a6d":"markdown","1e07c7c4":"markdown","ac908cba":"markdown"},"source":{"db8eb3b7":"# Import libararies\n\nimport re\nimport pandas as pd # CSV file I\/O (pd.read_csv)\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport sklearn\nimport nltk\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score ,confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings('ignore')","00aa6311":"news = pd.read_json('..\/input\/News_Category_Dataset.json', lines=True)\n#remove_columns_list = ['authors', 'date', 'link', 'short_description', 'headline']\nnews['information'] = news[['headline', 'short_description']].apply(lambda x: ' '.join(x), axis=1)","44812cab":"# Dataset dimension(row, columns)\nnews.shape","77d6d718":"# To display entire text\npd.set_option('display.max_colwidth', -1)","29dcc887":"news.head(1)\n#news[['information', 'category']].head(5)","ec7fbcb7":"#news[news['authors'] == ''].groupby(by='category').size()\n#news[(news['authors'] == '') & (news['short_description'] == '' )].index\n# Drop those rows which has authors and short_description column as empty.\nnews.drop(news[(news['authors'] == '') & (news['short_description'] == '' )].index, inplace=True)","b88e3aba":"news.groupby(by='category').size()","e589639e":"fig, ax = plt.subplots(1, 1, figsize=(35,7))\nsns.countplot(x = 'category', data = news)","3f202976":"fig, ax = plt.subplots(1, 1, figsize=(15,15))\nnews['category'].value_counts().plot.pie( autopct = '%1.1f%%')","4f820b97":"#count the number of author in the dataset\n#news.authors.value_counts()\ntotal_authors = news.authors.nunique()\nnews_counts = news.shape[0]\nprint('Total Number of authors : ', total_authors)\nprint('avg articles written by per author: ' + str(news_counts\/\/total_authors))\nprint('Total news counts : ' + str(news_counts))","4637426e":"authors_news_counts = news.authors.value_counts()\nsum_contribution = 0\nauthor_count = 0\nfor author_contribution in authors_news_counts:\n    author_count += 1\n    if author_contribution < 80:\n        break\n    sum_contribution += author_contribution\nprint('{} of news is contributed by {} authors i.e  {} % of news is contributed by {} % of authors'.\n      format(sum_contribution, author_count, format((sum_contribution*100\/news_counts), '.2f'), format((author_count*100\/total_authors), '.2f')))","27d21498":"news.authors.value_counts()[0:10]","2249c3dd":"author_name = 'Lee Moran'\n#author_name = 'Ed Mazza'\nparticular_author_news = news[news['authors'] == author_name]\ndf = particular_author_news.groupby(by='category')['information'].count()\ndf","339957fc":"fig, ax = plt.subplots(1, 1, figsize=(20,20))\ndf.plot.pie( autopct = '%1.1f%%')","5a0296dd":"# Split the data into train and test.\nX_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(news[['information', 'authors']], news['category'], test_size=0.33)","914b0742":"# Convert pandas series into numpy array\nX_train = np.array(X_train);\nX_test = np.array(X_test);\nY_train = np.array(Y_train);\nY_test = np.array(Y_test);\ncleanHeadlines_train = [] #To append processed headlines\ncleanHeadlines_test = [] #To append processed headlines\nnumber_reviews_train = len(X_train) #Calculating the number of reviews\nnumber_reviews_test = len(X_test) #Calculating the number of reviews","af44d767":"from nltk.stem import PorterStemmer, WordNetLemmatizer\nlemmetizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\ndef get_words(headlines_list):\n    headlines = headlines_list[0]   \n    author_names = [x for x in headlines_list[1].lower().replace('and',',').replace(' ', '').split(',') if x != '']\n    headlines_only_letters = re.sub('[^a-zA-Z]', ' ', headlines)\n    words = nltk.word_tokenize(headlines_only_letters.lower())\n    stops = set(stopwords.words('english'))\n    meaningful_words = [lemmetizer.lemmatize(w) for w in words if w not in stops]\n    return ' '.join(meaningful_words + author_names)","6dcaf802":"for i in range(0,number_reviews_train):\n    cleanHeadline = get_words(X_train[i]) #Processing the data and getting words with no special characters, numbers or html tags\n    cleanHeadlines_train.append( cleanHeadline )","04f01a51":"for i in range(0,number_reviews_test):\n    cleanHeadline = get_words(X_test[i]) #Processing the data and getting words with no special characters, numbers or html tags\n    cleanHeadlines_test.append( cleanHeadline )","c2911fa0":"vectorize = sklearn.feature_extraction.text.TfidfVectorizer(analyzer = \"word\", max_features=30000)\ntfidwords_train = vectorize.fit_transform(cleanHeadlines_train)\nX_train = tfidwords_train.toarray()\n\ntfidwords_test = vectorize.transform(cleanHeadlines_test)\nX_test = tfidwords_test.toarray()","b09d407a":"from sklearn.svm import LinearSVC\n\nmodel = LinearSVC()\nmodel.fit(X_train,Y_train)\nY_predict = model.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_predict)*100\nprint(format(accuracy, '.2f'))","5e682969":"logistic_Regression = LogisticRegression()\nlogistic_Regression.fit(X_train,Y_train)\nY_predict = logistic_Regression.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_predict)*100\nprint(format(accuracy, '.2f'))","28b66fac":"from sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB(alpha=0.1)\nmodel.fit(X_train,Y_train)\nY_predict = model.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_predict)*100\nprint(format(accuracy, '.2f'))","b991d3d1":"# from sklearn.ensemble import BaggingClassifier\n# model = BaggingClassifier(random_state=0, n_estimators=10)\n# model.fit(X_train, Y_train)\n# prediction = model.predict(X_test)\n# print('Accuracy of bagged KNN is :',accuracy_score(prediction, Y_test))","ffd5e896":"# from sklearn.tree import DecisionTreeClassifier\n\n# model = DecisionTreeClassifier()\n# model.fit(X_train, Y_train)\n# prediction_decision_tree = model.predict(X_test)\n# print('The accuracy of Decision Tree is', accuracy_score(prediction_decision_tree, Y_test))","68a0462d":"# from sklearn.neighbors import KNeighborsClassifier\n\n# model = KNeighborsClassifier()\n# model.fit(X_train, Y_train)\n# prediction_knn = model.predict(X_test)\n# print('The accuracy of the KNN is', metrics.accuracy_score(prediction_knn, Y_test))\n","444514d3":"# from sklearn.svm import SVC\n# model = SVC(kernel='rbf',C=1,gamma=0.1)\n# model.fit(X_train, Y_train)\n# predict_rsvm = model.predict(X_test)\n# print('Predict accuracy is ',accuracy_score(predict_rsvm,Y_test))","e3d6b387":"# Some analysis on author to see if \"authors\" has any relationship with category","ecf99df6":"\n# Some analysis on \"category\" of news","d0c64a6d":"### We can observe that ~1% of authors are contributing to ~61% of the news. \n### If all authors are writing only of few categories of news then we can consider the author feature as well for modeling.","1e07c7c4":"### We can observe that even though authors are  writing for almost all category but majority of their contribution","ac908cba":"### We can see that almost 38% of the news is of category POLITICS and ENTERTAINMENT. I think we shoud focus more on these 2 category for time being."}}