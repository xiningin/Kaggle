{"cell_type":{"cc20e1ac":"code","a7f4ad84":"code","7fd52949":"code","9e6292f3":"code","605c0deb":"code","3ca7bae6":"code","a6ba56ba":"code","9dcac2e4":"code","b365922d":"code","35f1b08b":"code","78f2b50a":"code","b42e967f":"code","a4c21031":"code","e83f884c":"code","e819643a":"code","193f5f1c":"code","8339ae8e":"code","6f961cf9":"code","d3c79344":"code","3b3e89ec":"code","1cb0776b":"code","7e8dd6ce":"code","d2e2f5e7":"code","fec59411":"code","9f52beb0":"code","cfc0eb79":"code","21100f82":"markdown","4438c208":"markdown"},"source":{"cc20e1ac":"import warnings\nwarnings.simplefilter(action='ignore', \n                      category=FutureWarning)      # suppress warnings\nimport numpy as np                                 # linear algebra\nimport pandas as pd                                # data analysis\nimport matplotlib.pyplot as plt                    # visualization - plt means we don't have to use the full pyplot every time we use it.\n%matplotlib inline\nimport seaborn as sns                              # visualization\nimport scipy.stats as scipystats                   # statistics  \nimport statsmodels.formula.api as smf              # statistics\nfrom statsmodels.api import add_constant           # statistics\nfrom sklearn.feature_selection import SelectKBest  # feature selection\nfrom sklearn.feature_selection import f_regression # feature selection\n\npd.set_option('display.float_format', lambda x: '%.1f' % x) # format decimals\nsns.set(font_scale=1.5) # increse font size for seaborn charts\n\nprint(\"Setup Complete\")","a7f4ad84":"file_path = \"..\/input\/world-happiness\/2019.csv\"\ndata = pd.read_csv(file_path)\ndata.head()","7fd52949":"#Check NA\ndata.isnull().sum()","9e6292f3":"sns.distplot(a=data['Score'], label=\"Score\", kde=False)\nsns.distplot(a=data['Social support'], label=\"Social support\", kde=False)\nsns.distplot(a=data['Generosity'], label=\"Generosity\", kde=False)","605c0deb":"sns.lmplot(x=\"Score\", y=\"Social support\", data=data, alpha, scatter_kws={'alpha':0.15})\nsns.set_style(\"darkgrid\")\n","3ca7bae6":"sns.regplot(x=, lowess=True, data=df, scatter_kws={'alpha':0.15}, line_kws={'color': 'red'})","a6ba56ba":"\nplt.plot(data['Social support'], data['Perceptions of corruption'], color='#c3fdff', marker='.', linestyle='--')\nplt.title('Perceptions of corruption vs generosity')\nplt.xlabel('Corruption')\nplt.ylabel('Generosity')\nax = plt.axes()\nax.set_facecolor('#2f3952') #Change background colour\nplt.style.use('fivethirtyeight')\n\nplt.legend()  #Automatically the label from the top will be put into the legend.\n\nplt.style.available\n#plt.savefig('plot.png') #- save it to current directory","9dcac2e4":"fig, axes = plt.subplots(2, 3, figsize=(8, 5))\nplt.rcParams['figure.dpi'] = 200\nplt.tight_layout()\n\n","b365922d":"s = (data.dtypes == 'object') #TAKE OUT COUNTRY OR REGION\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","35f1b08b":"#Label Encoding of Categorical Variables\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    data[col] = label_encoder.fit_transform(data[col])","78f2b50a":"data.corr()['Score'].sort_values()","b42e967f":"#First select the top 10 based on overall rank\ntop_10 = data.loc[data['Overall rank'] <= 10]\ntop_10\n","a4c21031":"#To improve\nsns.heatmap(top_10, linewidths=0.1,cbar=True, annot=True, square=True, fmt='.1f')","e83f884c":"#2D Kernel Density Plot to investigate social security and total score\nsns.jointplot(x=data['Score'], y=data['Social support'], kind=\"kde\")\n","e819643a":"data.groupby('Generosity').head()","193f5f1c":"#Using Random Forest\nfrom sklearn.model_selection import train_test_split\ny=data.Score\nhappiness_features = ['Generosity', 'Social support', 'Perceptions of corruption']\nX = data[happiness_features]\nX.describe\n","8339ae8e":"#Define Model\nfrom sklearn.tree import DecisionTreeRegressor\nbasic_model=DecisionTreeRegressor(random_state=1)\nbasic_model.fit(X, y)","6f961cf9":"#Predictions\nprint(\"Making predictions for happiness scores:\")\nprint(X.head())\nprint(\"The predictions are\")\nprint(basic_model.predict(X.head()))","d3c79344":"#Basic Validation using in-sample score\nfrom sklearn.metrics import mean_absolute_error\n\npredicted_happiness_scores = basic_model.predict(X)\nmean_absolute_error(y, predicted_happiness_score)","3b3e89ec":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n\nbasic_model = DecisionTreeRegressor()\n\nbasic_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nbasic_val_predictions = basic_model.predict(val_X)\nbasic_score = mean_absolute_error(val_y, basic_val_predictions)\nprint(basic_score)","1cb0776b":"# Random Forest Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(train_X, train_y)\nhappiness_preds = rf_model.predict(val_X)\nrf_score = mean_absolute_error(val_y, happiness_preds)\nprint(rf_score)","7e8dd6ce":"#Calculate max leaf nodes to optimise model\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","d2e2f5e7":"for max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","fec59411":"#Using Pipelines before using Cross-Validation and XGBoosting to optimise predictions\n#First we bundle preprocessing and modelling\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n#NUMERICAL DATA\n#numerical_transformer = SimpleImputer(strategy='constant')\n#CATEGORICAL DATA\n#categorical_transformer = Pipeline(steps=[\n#    ('imputer', SimpleImputer(strategy='most_frequent')),\n#    ('one hot', OneHotEncoder())\n#])\n\n#PREPROCESSING\n#preprocessor = ColumnTransformer(\n#transformers=[\n#    ('num', numerical_transformer, numerical_cols)\n#    ('cat', categorical_transformer, categorical_cols)\n#])","9f52beb0":"#Define Model\n#from sklearn.ensemble import RansomForestRegressor\n#model = RandomForestRegressor(n=estimators=100, random_state=0)","cfc0eb79":"#Bundle preprocessing and modelling into pipeline\n\nfrom sklearn.metrics import mean_absolute_error\n\n#Preprocessing and model bundled\n#my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n #                            ('model', model)\n  #                           ])\n\n#fit model to training data\n#my_pipeline.fit(X_train, y_train)\n\n#Get Preds\n#preds = my_pipeline.predict(X_valid)\n\n#Evaluate\n#score = mean_absolute_error(y_valid, preds)\n#print(score)","21100f82":"MORE VISUALISATION\n","4438c208":"GDP seems to be the most highly correlated factor.\nLet's visualise on a heatmap"}}