{"cell_type":{"8a34ca88":"code","5c4bf6a2":"code","54130779":"code","a2ace51e":"code","9201cbe6":"code","38a5327e":"code","33b2bdf6":"code","4b0e9ac0":"code","1fdc43be":"code","d5809c9b":"code","adf23761":"code","cebbbb7c":"code","137ce325":"code","ecdd9b41":"code","40f21c5c":"code","10fec79f":"code","b2eb1fb6":"code","59c74bba":"code","0882afc8":"code","872fe302":"code","cfe1796c":"code","d906034f":"code","c331aa2e":"code","b5a74959":"code","221f6de7":"code","3f5e1163":"code","afe05436":"code","0271ec0b":"code","b5a49ff7":"code","9e864bfb":"code","093a82a0":"code","696240f5":"code","632af4ed":"code","f41d53a3":"code","984b69ea":"code","ee619879":"code","0057ae84":"markdown","4cabf65a":"markdown","fe451784":"markdown","9a0b14d9":"markdown","ba1f2292":"markdown","5bc2f69c":"markdown","3c40a8d0":"markdown","a2b97833":"markdown","966d4058":"markdown","2ccf4c41":"markdown","f6e04347":"markdown","0440e83c":"markdown","809a2917":"markdown","8f03a4fd":"markdown","596990cf":"markdown","44437732":"markdown","d560ee1e":"markdown","442d9ffa":"markdown","add36c5b":"markdown","9c00c34d":"markdown","281b48bf":"markdown","dab281cd":"markdown","1f99aea8":"markdown","642e6199":"markdown","7f955eb3":"markdown","ae94bb38":"markdown","45f7e481":"markdown","19520a7d":"markdown","5576ba31":"markdown","308f2926":"markdown","a8063ffc":"markdown","9f1a8182":"markdown","efb7aa18":"markdown"},"source":{"8a34ca88":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","5c4bf6a2":"data=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","54130779":"data.head()","a2ace51e":"data.describe()","9201cbe6":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score\nX = data.iloc[:, 0:30]\nY = data.iloc[:, 30]\nX_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100)\n\ndtree= DecisionTreeClassifier(criterion='gini',max_depth=10,random_state=100,min_samples_leaf=10)\ndtree.fit(X_train, y_train)\nprediction=dtree.predict(X_test)\nacc = accuracy_score(y_test,prediction)*100\nprint(\"Decision Tree accuracy:\",acc)\n\n","38a5327e":"import seaborn as sns\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nscore=round(accuracy_score(y_test,prediction),3)\nprint(classification_report(y_test,prediction))\ncm= confusion_matrix\ncm1=cm(y_test,prediction)\nsns.heatmap(cm1, annot=True,fmt=\".1f\",linewidths=3,square=True, cmap='PuBu',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title('accuracy score: {0}'.format(score),size=12)\nplt.show()","33b2bdf6":"cohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)","4b0e9ac0":"import matplotlib.pyplot as plt\n\ndata['Class'].value_counts().plot(kind='pie',colors=['#cd1076', '#008080'],autopct='%1.1f%%',figsize=(9,9))\nplt.show\nvarValue = data.Class.value_counts()\nprint(varValue)\n\n","1fdc43be":"from sklearn.utils import resample\ndf_majority = data.loc[data.Class == 0].copy()\ndf_minority = data.loc[data.Class == 1].copy()\ndf_minority_upsampled = resample(df_minority,\n                             replace=True,  # sample with replacement\n                             n_samples=284315,  # to match majority class\n                             random_state=123) \ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])","d5809c9b":"import matplotlib.pyplot as plt\n\ndf_upsampled['Class'].value_counts().plot(kind='pie',colors=['#cd1076', '#008080'],autopct='%1.1f%%',figsize=(9,9))\nplt.show\nvarValue = df_upsampled.Class.value_counts()\nprint(varValue)\n","adf23761":"colors = {0:'#cd1076', 1:'#008080'}\nfig, ax = plt.subplots()\ngrouped = df_upsampled.groupby('Class')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter'\n               ,x='V20', y='V24', label=key\n               ,color=colors[key])\nplt.show()","cebbbb7c":"print(df_upsampled.isnull().sum())","137ce325":"from sklearn.ensemble import IsolationForest\nfrom collections import Counter\nrs=np.random.RandomState(0)\nclf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \nclf.fit(df_upsampled)\ny_pred_train = clf.predict(df_upsampled)\nsay\u0131 = Counter(y_pred_train)\nprint(say\u0131)\n\nif_scores = clf.decision_function(df_upsampled)\nif_anomalies=clf.predict(df_upsampled)\nif_anomalies=pd.Series(if_anomalies).replace([-1,1],[1,0])\nif_anomalies=df_upsampled[if_anomalies==1];\nplt.figure(figsize=(12,8))\nplt.hist(if_scores)\nplt.title('Histogram of Avg Anomaly Scores: Lower => More Anomalous')\n","ecdd9b41":"Q1 = df_upsampled.quantile(0.25)\nQ3 = df_upsampled.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","40f21c5c":"data = df_upsampled[~((df_upsampled < (Q1 - 1.5 * IQR)) |(df_upsampled > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint(data.shape)","10fec79f":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndata[\"Class\"] = data.Class\nX = data.drop(\"Class\",1)\ny = data[\"Class\"]\ndata.head()\nplt.figure(figsize=(25,25))\ncor = data.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()\n\ncor_target = abs(cor[\"Class\"]) #absolute value\n#High Correlations\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features","b2eb1fb6":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nX = data.iloc[:, 0:30]\ny = data.iloc[:, 30]\nfeature_names = data.columns\n\nclf = LassoCV().fit(X, y)\nimportance = np.abs(clf.coef_)\nprint(importance)\n\nclf2= RidgeCV().fit(X,y)\nimportance2= np.abs(clf2.coef_)\nprint(importance2)\n\nidx_third = importance.argsort()[-3]\nthreshold = importance[idx_third]+0.01\nidx_features = (-importance).argsort()[:20]\nname_features = np.array(feature_names)[idx_features]\nprint('Selected features: {}'.format(name_features))\n\n\nsfm = SelectFromModel(clf, threshold=threshold)\nsfm.fit(X, y)\nX_transform = sfm.transform(X)\nn_features = sfm.transform(X).shape[1]\n","59c74bba":"data[['Time','V14','V4','V12','V17','V27','V26' ,'V25','V24','V23',\n 'V22','V21','V20','V19','V18','V3','V5','V15','V28','Amount','Class']]","0882afc8":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import f1_score\n                  \nX = data.iloc[:, 0:30]\nY = data.iloc[:, 30]\nX_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2, random_state = 100)\n\naccuracies ={}\nkappaScores= {}\nf1scores={}\n","872fe302":"#Manual Tuning\n\nfrom xgboost import XGBClassifier\naccuracy = []\nfor n in range(1,11):\n    xgb =XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.78,\n                           colsample_bytree=1, max_depth=n)\n    xgb.fit(X_train,y_train)\n    prediction = xgb.predict(X_test)\n    accuracy.append(accuracy_score(y_test, prediction))\nprint(accuracy)    \nplt.plot(range(1,11), accuracy)\nplt.xlabel('Max_depth')\nplt.ylabel('Accuracy')\nplt.show()    \n    ","cfe1796c":"# RandomizeSearchCV\n\nfrom sklearn.model_selection import RandomizedSearchCV\nxgb_params = {\n    'learning_rate' : [0.08, 0.06, 0.04, 0.09],      \n    'max_depth': range(1,11),\n    'n_estimators': [100, 200, 300,500,1000]}\nxgb =XGBClassifier()\nxgb_randomcv_model=RandomizedSearchCV(estimator=xgb, param_distributions=xgb_params, n_iter=2, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\nprint(xgb_randomcv_model.best_params_)\nprint('xgb_randomcv_model accuracy = {}'.format(xgb_randomcv_model.best_score_))","d906034f":"from xgboost import XGBClassifier\nxgb =XGBClassifier(n_estimators=200, learning_rate=0.08, max_depth=6)\nxgb.fit(X_train,y_train)\nprediction = xgb.predict(X_test)\nacc = accuracy_score(y_test, prediction)*100\nprint(\"Xgboost Classifier accuracy:\",acc)\naccuracies['Xgboost Classifier']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1 Score: \",f1)\nf1scores['Xgboost Classifier']=f1\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Xgboost Classifier']=cohen_kappa\n\n","c331aa2e":"score=round(accuracy_score(y_test,prediction),3)\ncm= confusion_matrix\ncm1=cm(y_test,prediction)\nsns.heatmap(cm1, annot=True,fmt=\".1f\",linewidths=3,square=True, cmap='PuBu',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title('XGBOOST Accuracy Score: {0}'.format(score),size=12)\nplt.show()","b5a74959":"# RandomizeSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nrf_params = {'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': range(1,11),\n    'min_samples_split': range(1,15,5),\n    'n_estimators': [100, 200, 300,500,1000]}\nrf=RandomForestClassifier()\nrf_randomcv_model=RandomizedSearchCV(estimator=rf, param_distributions=rf_params, n_iter=2, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)\nprint(rf_randomcv_model.best_params_)\nprint('rf_randomcv_model accuracy score = {}'.format(rf_randomcv_model.best_score_))\n\n","221f6de7":"clf = RandomForestClassifier(n_estimators=200,min_samples_split=11,min_samples_leaf=7, max_features=2,max_depth= 80)\nclf.fit(X_train,y_train)\n\nprediction = clf.predict(X_test)\nacc = accuracy_score(y_test,prediction)*100\nprint(\"Random Forest Accuracy:\",acc)\naccuracies['Random Forest']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['Random Forest']=f1\n\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Random Forest']=cohen_kappa","3f5e1163":"score=round(accuracy_score(y_test,prediction),3)\ncm= confusion_matrix\ncm1=cm(y_test,prediction)\nsns.heatmap(cm1, annot=True,fmt=\".1f\",linewidths=3,square=True, cmap='PuBu',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title('Random Forest accuracy_score: {0}'.format(score),size=12)\nplt.show()","afe05436":"from sklearn.tree import DecisionTreeClassifier\ndtree= DecisionTreeClassifier(criterion='gini',max_depth=10,random_state=100,min_samples_leaf=10)\ndtree.fit(X_train, y_train)\nprediction=dtree.predict(X_test)\n\nprint(cross_val_score(dtree,X,Y,cv=5))\nscores = cross_val_score(dtree, X, Y,scoring='accuracy', cv=10)\nprint(scores.mean())","0271ec0b":"acc = accuracy_score(y_test,prediction)*100\nprint(\"Decision Tree Accuracy:\",acc)\naccuracies['Decision Tree']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['Decision Tree']=f1\n\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Decision Tree']=cohen_kappa","b5a49ff7":"score=round(accuracy_score(y_test,prediction),3)\ncm= confusion_matrix\ncm1=cm(y_test,prediction)\nsns.heatmap(cm1, annot=True,fmt=\".1f\",linewidths=3,square=True, cmap='PuBu',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title('Decision Tree Accuracy Score: {0}'.format(score),size=12)\nplt.show()","9e864bfb":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(random_state=101,multi_class='ovr',solver='liblinear',class_weight='balanced',C=0.2)\nlr.fit(X_train,y_train)\nprediction = lr.predict(X_test)\nscores = cross_val_score(lr, X, Y, cv=10)\nprint(scores.mean())\n\n","093a82a0":"acc = accuracy_score(y_test,prediction)*100\nprint(\"Logistic Regression Accuracy:\",acc)\naccuracies['Logistic Regression']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['Logistic Regression']=f1\n\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Logistic Regression']=cohen_kappa","696240f5":"score=round(accuracy_score(y_test,prediction),3)\ncm= confusion_matrix\ncm1=cm(y_test,prediction)\nsns.heatmap(cm1, annot=True,fmt=\".1f\",linewidths=3,square=True, cmap='PuBu',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title('Logistic Regression Accuracy Score: {0}'.format(score),size=12)\nplt.show()","632af4ed":"feature_importance = abs(lr.coef_[0])\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfeatfig = plt.figure(figsize=(15,15))\nfeatax = featfig.add_subplot(1, 1, 1)\nfeatax.barh(pos, feature_importance[sorted_idx], align='center',color='#00e5ee')\nfeatax.set_yticks(pos)\nfeatax.set_yticklabels(np.array(X.columns)[sorted_idx], fontsize=8)\nfeatax.set_xlabel('Relative Feature Importance')\n\nplt.tight_layout()\nplt.show()","f41d53a3":"colors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,101,5))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"\\n\\n Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","984b69ea":"colors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,101,5))\nplt.ylabel(\"Kappa Score %\")\nplt.xlabel(\"\\n\\n Algorithms\")\nsns.barplot(x=list(kappaScores.keys()), y=list(kappaScores.values()), palette=colors)\nplt.show()","ee619879":"colors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,101,5))\nplt.ylabel(\"F1 Score %\")\nplt.xlabel(\"\\n\\n Algorithms\")\nsns.barplot(x=list(f1scores.keys()), y=list(f1scores.values()), palette=colors)\nplt.show()","0057ae84":"# Isolation Forest: 56755 outliers","4cabf65a":"<a id = \"7\"><\/a><br>\n# Interquartile Range (IQR) to Detect Outliers","fe451784":"<a id = \"16\"><\/a><br>\n# Logistic Regression","9a0b14d9":"<a id = \"2\"><\/a><br>\n# I want to do a simple algorithm work without any action.","ba1f2292":"# In critical studies, 76% is not an acceptable score. 24% of our data is wrong. If we consider this as a laboratory research, it means we have a serious quality problem.","5bc2f69c":"<a id = \"18\"><\/a><br>\n# Comparison of Kappa Scores","3c40a8d0":"<a id = \"14\"><\/a><br>\n# Random Forest","a2b97833":"![qcon-rio-machine-learning-for-everyone-51-638.jpg](attachment:qcon-rio-machine-learning-for-everyone-51-638.jpg)","966d4058":"<a id = \"10\"><\/a><br>\n## Pearson Correlation Coefficient","2ccf4c41":"<a id = \"6\"><\/a><br>\n## Isolation Forest","f6e04347":"<a id = \"4\"><\/a><br>\n# Missing Value","0440e83c":"<a id = \"11\"><\/a><br>\n## LassoCV and RidgeCV\n\n### Train your data with the model you want to apply, and you can choose top x according to the materiality coefficient of that model result.","809a2917":"# And Cohen Kappa Score:\nCohen's kappa, (7), symbolized by the lowercase Greek letter, is a powerful statistic useful for testing reliability. Similar to the correlation coefficients, between -1 and +1; where 0 represents the availability that can be expected from random chance, and 1 represents the perfect match between raters.\n* 0 indicates no information agreement\n* 0.01-0.20 Slight agreement\n* 0.21-0.40 Fair agreement\n* 0.41-0.60 Moderate agreement\n* 0.61-0.80 Substantial agreement\n* 0.81-1.00 Almost perfect agreement","8f03a4fd":"### Of the 284,807 rows, only 492 were categorized as fraud.\n### This is an imbalanced dataset\n## Looking at these percentages, the model is prone to overfitting of non-fraud (class = 0). Therefore, we should not use the accuracy score metric on imbalanced datasets.\n### Two methods can be used to avoid this situation; Up sampling and Down sampling methods:\n\u25cfUpsampling Method: The imbalance between the data group is eliminated by increasing the data in the group with less number.\n\n\u25cf Downsampling Method: The imbalance between the data group is eliminated by reducing the data in the more numerous groups.\n\n### I used the upsampling method in this study\n### let's balance this data set.","596990cf":"# Great, but let's look at Confusion Matrix.","44437732":"\n## Let's look at the current version of our data set.","d560ee1e":"<a id = \"19\"><\/a><br>\n# Comparison of F1 Scores","442d9ffa":" #                Credit Card Fraud\n # Please don't forget to write down your positive or negative comments.","add36c5b":"<a id = \"17\"><\/a><br>\n# Comparison of accuracies","9c00c34d":"![7c586a90f77af83e8dad698135101d96cd041bd6fd4ae6f1703a3f67e5d2655e._V_SX1080_.jpg](attachment:7c586a90f77af83e8dad698135101d96cd041bd6fd4ae6f1703a3f67e5d2655e._V_SX1080_.jpg)","281b48bf":"<a id = \"13\"><\/a><br>\n# XGBOOST Classifier\n* Manual Tuning\n* RandomizedSearchCV\n","dab281cd":"# When we look at Precision, Recall and F1-scores, it is seen that the model output is insufficient.","1f99aea8":"# No missing value.","642e6199":"<a id = \"15\"><\/a><br>\n# Decision Tree","7f955eb3":"<a id = \"5\"><\/a><br>\n# Outlier Detection\n* Isolation Forest\n* Interquartile Range (IQR)\n ","ae94bb38":"<a id = \"9\"><\/a><br>\n# Feature Selection\n* Pearson Correlation Coefficient\n* LassoCV and RidgeCV","45f7e481":"# Since the process of finding the best parameters takes a long time, I did not do this on Decision Tree and Logistic Regression.","19520a7d":"<a id = \"3\"><\/a><br>\n# Sampling\n## Why do we need sampling ?","5576ba31":"<a id = \"12\"><\/a><br>\n# Algorithms\n1. XGBOOST Classifier\n1. Random Forest\n1. Decision Tree\n1. Logistic Regression","308f2926":"## Let's look at the graph of the coefficients showing the effect of the variables on the target variable.","a8063ffc":"<a id = \"1\"><\/a><br>\n# Load and Check Data","9f1a8182":"<font color = '#cdcd00'>\nContent:\n    \n1. [Load and check data](#1)\n1. [Suitable Metric](#2)\n1. [Sampling](#3)    \n1. [Missing Value](#4)     \n1. [Outlier Detection](#5)\n    *           [Isolation Forest](#6)\n    *           [Interquartile Range(IQR)](#7)\n1. [Feature Selection](#9) \n    *           [Pearson Correlation Coefficient](#10)\n    *           [LassoCV and RidgeCV](#11)\n1. [Algorithm Works](#12) \n    *           [Xgboost Classifier](#13)\n    *           [Random Forest](#14)\n    *           [Decision Tree](#15) \n    *           [Logistic Regression](#16)\n1. [Comparison of algorithms](#17) \n2. [Comparison of Kappa Scores](#18)\n3. [Comparison of F1 Scores](#19) \n    ","efb7aa18":"![biti%C5%9F.jpg](attachment:biti%C5%9F.jpg)"}}