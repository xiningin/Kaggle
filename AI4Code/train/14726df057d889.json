{"cell_type":{"338587d4":"code","86952afb":"code","75d6f8a2":"code","5f2c4fef":"code","1d64840a":"code","2772d733":"code","0bd02414":"code","a7838ccf":"code","7aacc529":"code","7c850682":"code","58f53381":"code","9700712e":"code","dcbac917":"code","59a36aac":"code","e3114d5b":"code","d73f4745":"code","fb5afd94":"code","a1c78800":"code","b9a57b1f":"code","fc4f6fb5":"code","6cecbe65":"code","6df56550":"code","32785341":"code","db83234e":"code","a1ed7869":"code","fb751c6a":"code","aa5e0d63":"code","71b7a61f":"code","fae49f5f":"code","0ea39131":"code","641c0343":"code","1ceaeb5f":"code","3a87b287":"code","a5d88e57":"code","13b3b8f8":"code","d02b8205":"code","7c70bbbb":"code","441b252c":"code","ce824e1d":"code","2c362692":"code","4bc6e585":"code","7871ca4c":"code","99fa1a4e":"code","6a96cb32":"code","af8b86f7":"code","162a80ed":"code","cffc6f3a":"code","474aa14c":"code","3b07f39a":"code","f1973141":"code","46304ff1":"code","5bc1e810":"code","971d115e":"code","557511e8":"code","6079d656":"code","8646d54e":"code","508082ce":"code","14b35f44":"code","8b48e1ca":"code","87a8a78f":"code","c6fb900c":"code","2d8f6371":"code","f32e7e3a":"code","7a8afd99":"code","fa2476a3":"code","3d217456":"code","8a0ae44b":"code","37699d5f":"code","e4bc3e90":"code","8006cc87":"code","310a7b90":"code","ff309413":"code","5a25c434":"code","9d961442":"code","a0bb947f":"code","c62c3079":"code","e2743bf9":"code","6a7fec13":"code","29711c09":"code","ec2342ef":"code","fa26f6b1":"code","1523ffa5":"code","d40370dd":"markdown","6991c81b":"markdown","d3dd0064":"markdown","56889e7e":"markdown","b8ea20c8":"markdown","9a516f31":"markdown","440fbdf7":"markdown","00af21e2":"markdown","d0071aee":"markdown","de7d0ecb":"markdown","e027fc1a":"markdown","d36f9c14":"markdown","4a18796b":"markdown","181c0a0f":"markdown","29a285b5":"markdown","93c2c253":"markdown","6a29a801":"markdown","e1fbc0db":"markdown","641526eb":"markdown","4e8e2de5":"markdown","6ad6873b":"markdown","ee9d73db":"markdown","a8644df6":"markdown","2f808411":"markdown","84b4bbdd":"markdown","5601ce40":"markdown","5e44055b":"markdown","717d4833":"markdown"},"source":{"338587d4":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","86952afb":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","75d6f8a2":"df=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","5f2c4fef":"df.info()","1d64840a":"df.describe().transpose()","2772d733":"df.head()","0bd02414":"df=df.drop(['PassengerId', 'Name','Ticket','Cabin'],axis=1)","a7838ccf":"df.head()","7aacc529":"sns.distplot(df.Age)","7c850682":"sns.distplot(df.Fare)","58f53381":"sns.countplot(df.Embarked)","9700712e":"sns.countplot(df.Sex)","dcbac917":"sns.countplot(df.SibSp)","59a36aac":"sns.countplot(df.Parch)","e3114d5b":"df.isnull().sum()","d73f4745":"df[df.Embarked.isnull()]","fb5afd94":"df[df.Age.isnull()]","a1c78800":"df['Fare_Buck']=pd.cut(df.Fare,[-1,25,50,75,100,1000],labels=['Low','Medium','High','VHigh','ExHigh'])","b9a57b1f":"sns.countplot(df.Fare_Buck)","fc4f6fb5":"df[df['Survived']==1].groupby(['Pclass','Sex','Fare_Buck'])['Embarked'].value_counts()","6cecbe65":"df.loc[df.Embarked.isnull(),'Embarked']='S'","6df56550":"df[df.Embarked.isnull()]","32785341":"res=df.groupby(['Pclass','Sex','Fare_Buck'])['Age'].median().rename('Median').reset_index()\nres","db83234e":"def age_calc(x):\n    return res.loc[ (res.Pclass==x.Pclass)  & (res.Sex==x.Sex) & (res.Fare_Buck==x.Fare_Buck),'Median'].values[0]","a1ed7869":"df.loc[df.Age.isnull(),'Age']=df[df.Age.isnull()].apply(age_calc,axis=1)","fb751c6a":"df.isnull().sum()","aa5e0d63":"df[df.Age.isnull()]","71b7a61f":"df[(df.Pclass==3) & (df.Sex=='female')].median()","fae49f5f":"df.loc[df.Age.isnull(),'Age']=22.0  # imputing with median value","0ea39131":"sum(df.Survived)\/len(df) *100","641c0343":"# Creating Dummy Variable for Pclass\nPclass=pd.get_dummies(df.Pclass, prefix='Class', drop_first=True)\n\n# Creating Dummy Variable for Embarked\nEmbarked=pd.get_dummies(df.Embarked, prefix='Embarked', drop_first=True)\n\n#Concatenate the Dataset\ndf=pd.concat([df,Pclass,Embarked],axis=1)","1ceaeb5f":"df.head()","3a87b287":"# Drop categorical column from which dummies variable created\ndf.drop(['Pclass','Embarked'],axis=1,inplace=True)","a5d88e57":"df.info()","13b3b8f8":"# Creating Dummy Variable for Embarked\nSex=pd.get_dummies(df.Sex,  drop_first=True)\n\n#Concatenate the Dataset\ndf=pd.concat([df,Sex],axis=1)","d02b8205":"# Drop the remaining categorical column and the Fare bucked column created \ndf.drop(['Sex','Fare_Buck'],axis=1,inplace=True)","7c70bbbb":"df.info()","441b252c":"df.describe()","ce824e1d":"#Putting feature variables into X\nX=df.drop('Survived',axis=1)\n\n#Fetch Target Variable\ny=df['Survived']","2c362692":"# Split the data\n\nX_train, X_test, y_train, y_test= train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)","4bc6e585":"scaler=StandardScaler()\n\nX_train[['Age','SibSp','Parch','Fare']]=scaler.fit_transform(X_train[['Age','SibSp','Parch','Fare']])","7871ca4c":"X_train.head()","99fa1a4e":"#before starting with  the Model building, checking the correlation\nplt.figure(figsize=(15,8))\nsns.heatmap(df.corr(),annot=True,cmap='YlGnBu')\nplt.show()","6a96cb32":"#Importing necessary libraries\nimport statsmodels.api as sm\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","af8b86f7":"col=X.columns\ncol","162a80ed":"X_train_1=sm.add_constant(X_train[col])","cffc6f3a":"ml1= sm.GLM(y_train,X_train_1,family=sm.families.Binomial()).fit()\nml1.summary()","474aa14c":"vif=pd.DataFrame()\nX=X_train_1\nvif['Features']=X.columns\nvif['VIF']=[round(variance_inflation_factor(X.values,i),2) for i in range(X.shape[1])]\nvif.sort_values('VIF', ascending=False,inplace=True)\nvif","3b07f39a":"col=col.drop('Embarked_Q',1)","f1973141":"X_train2=sm.add_constant(X_train[col])\nml2=sm.GLM(y_train, X_train2, family=sm.families.Binomial()).fit()\nml2.summary()","46304ff1":"vif=pd.DataFrame()\nX=X_train2\nvif['Features']=X.columns\nvif['VIF']=[round(variance_inflation_factor(X.values,i),2) for i in range(X.shape[1])]\nvif.sort_values('VIF', ascending=False,inplace=True)\nvif","5bc1e810":"col=col.drop('Fare',1)\nX_train3=sm.add_constant(X_train[col])\nml3=sm.GLM(y_train, X_train3, family=sm.families.Binomial()).fit()\nml3.summary()","971d115e":"vif=pd.DataFrame()\nX=X_train3\nvif['Features']=X.columns\nvif['VIF']=[round(variance_inflation_factor(X.values,i),2) for i in range(X.shape[1])]\nvif.sort_values('VIF', ascending=False,inplace=True)\nvif","557511e8":"col=col.drop('Parch',1)\nX_train4=sm.add_constant(X_train[col])\nml4=sm.GLM(y_train, X_train4, family=sm.families.Binomial()).fit()\nml4.summary()","6079d656":"vif=pd.DataFrame()\nX=X_train4\nvif['Features']=X.columns\nvif['VIF']=[round(variance_inflation_factor(X.values,i),2) for i in range(X.shape[1])]\nvif.sort_values('VIF', ascending=False,inplace=True)\nvif","8646d54e":"col=col.drop('Embarked_S',1)\nX_train5=sm.add_constant(X_train[col])\nml5=sm.GLM(y_train, X_train5, family=sm.families.Binomial()).fit()\nml5.summary()","508082ce":"vif=pd.DataFrame()\nX=X_train4\nvif['Features']=X.columns\nvif['VIF']=[round(variance_inflation_factor(X.values,i),2) for i in range(X.shape[1])]\nvif.sort_values('VIF', ascending=False,inplace=True)\nvif","14b35f44":"y_train_pred=ml5.predict(X_train5)\n\ny_train_predict=pd.DataFrame({'Actual':y_train.values,'Prob':y_train_pred })","8b48e1ca":"fpr, tpr, threshold=metrics.roc_curve(y_train,y_train_pred,drop_intermediate=False)\nauc=metrics.roc_auc_score(y_train,y_train_pred)\nplt.plot(fpr,tpr,label='%0.2f AUC Score'%auc)\nplt.plot([1,0],[1,0],'k--')\nplt.legend(loc=\"lower right\")\nplt.show()","87a8a78f":"num=[float(i)\/10 for i in range(10)]\n\nfor i in num:\n    y_train_predict[i]=y_train_predict.Prob.apply(lambda x: 1 if x>=i else 0)\ny_train_predict","c6fb900c":"cut_off_mat=pd.DataFrame(columns=['Prob','Accuracy', 'Precision','Recall', 'F1'])\n\nfor i in num:\n    cnf=metrics.confusion_matrix(y_train_predict['Actual'],y_train_predict[i])\n    tot=sum(sum(cnf))\n    acc=(cnf[0,0]+cnf[1,1])\/tot\n    rec=cnf[1,1]\/(cnf[1,0]+cnf[1,1])\n    pre=cnf[1,1]\/(cnf[0,1]+cnf[1,1])\n    f1=2*pre*rec\/(pre+rec)\n    cut_off_mat.loc[i]=[i,acc,pre,rec,f1]\ncut_off_mat","2d8f6371":"cut_off_mat.index","f32e7e3a":"cut_off_mat.plot(x='Prob', y=['Accuracy','Precision','Recall', 'F1'])\nplt.show()","7a8afd99":"y_train_predict['Final_Pred']=y_train_predict['Prob'].apply(lambda x: 1 if x>0.6 else 0)\n\nconfusion=metrics.confusion_matrix(y_train_predict['Actual'],y_train_predict['Final_Pred'])\nTN=confusion[0,0]\nFP=confusion[0,1]\nFN=confusion[1,0]\nTP=confusion[1,1]","fa2476a3":"#Sesitivity\nprint('Sensitivity: ',confusion[1,1]\/(confusion[1,1]+confusion[1,0]))\n\n#Specificity\nprint('Specificity: ',confusion[0,0]\/(confusion[0,0]+confusion[0,1]))\n\n#Accuracy\nprint('Accuracy: ', (confusion[0,0]+confusion[1,1]) \/ sum(sum(confusion)))\n\n#Recall\nr=confusion[1,1]\/(confusion[1,1]+confusion[1,0])\nprint('Recall: ',r)\n\n#Precision\np=confusion[1,1]\/(confusion[1,1]+confusion[0,1])\nprint('Precision: ',p)\n\n#F1-Score\nprint('F1-Score: ',(2*p*r\/(p+r)))\n","3d217456":"X_test[['Age','SibSp','Parch','Fare']]=scaler.transform(X_test[['Age','SibSp','Parch','Fare']])","8a0ae44b":"X_test=X_test[col]\nX_test_sm=sm.add_constant(X_test)\n\ny_test_prob = ml5.predict(X_test_sm)","37699d5f":"y_test_pred = pd.DataFrame({\"Test_Acutal\":y_test.values, \"Test_Prob\":y_test_prob.values})\ny_test_pred['Pred_Val']=y_test_pred.Test_Prob.apply(lambda x: 1 if x>0.6 else 0)","e4bc3e90":"confusion2=metrics.confusion_matrix(y_test_pred['Test_Acutal'],y_test_pred['Pred_Val'])\nTN=confusion2[0,0]\nFP=confusion2[0,1]\nFN=confusion2[1,0]\nTP=confusion2[1,1]","8006cc87":"#Sesitivity\nprint('Sensitivity: ',confusion2[1,1]\/(confusion2[1,1]+confusion2[1,0]))\n\n#Specificity\nprint('Specificity: ',confusion2[0,0]\/(confusion2[0,0]+confusion2[0,1]))\n\n#Accuracy\nprint('Accuracy: ', (confusion2[0,0]+confusion2[1,1]) \/ sum(sum(confusion2)))\n\n#Recall\nr=confusion2[1,1]\/(confusion2[1,1]+confusion2[1,0])\nprint('Recall: ',r)\n\n#Precision\np=confusion2[1,1]\/(confusion2[1,1]+confusion2[0,1])\nprint('Precision: ',p)\n\n#F1-Score\nprint('F1-Score: ',(2*p*r\/(p+r)))\n","310a7b90":"ml5.summary()","ff309413":"test.info()","5a25c434":"test['Fare_Buck']=pd.cut(test.Fare,[-1,25,50,75,100,1000],labels=['Low','Medium','High','VHigh','ExHigh'])\n\nres=test.groupby(['Pclass','Sex','Fare_Buck'])['Age'].median().rename('Median').reset_index()\n\n\ndef age_calc(x):\n    return res.loc[ (res.Pclass==x.Pclass)  & (res.Sex==x.Sex) & (res.Fare_Buck==x.Fare_Buck),'Median'].values[0]\n\ntest.loc[test.Age.isnull(),'Age']=test[test.Age.isnull()].apply(age_calc,axis=1)\n\n","9d961442":"test.isnull().sum()","a0bb947f":"test[test.Age.isnull()]","c62c3079":"test.loc[test.Age.isnull(),'Age']=test.loc[(test.Pclass==3) & (test.Sex=='female'),'Age'].median()","e2743bf9":"# Creating Dummy Variable for Pclass\nPclass=pd.get_dummies(test.Pclass, prefix='Class', drop_first=True)\n\n# Creating Dummy Variable for Sex\nSex=pd.get_dummies(test.Sex, drop_first=True)\n\n#Concatenate the Dataset\ntest=pd.concat([test,Sex,Pclass],axis=1)\n\n\n# Drop the remaining categorical column and the Fare bucked column created \ntest.drop(['Sex','Fare_Buck','Pclass','Name','Cabin','Embarked','Ticket',],axis=1,inplace=True)","6a7fec13":"test.head()","29711c09":"#Putting feature variables into X\nX_test=test.drop('PassengerId',axis=1)\n\n#Fetch Target Variable\ny=pd.DataFrame({'PassengerId':test['PassengerId']})            ","ec2342ef":"X_test[['Age','SibSp','Parch','Fare']]=scaler.transform(X_test[['Age','SibSp','Parch','Fare']])\n\nX_test=X_test[col]\nX_test_sm=sm.add_constant(X_test)\n\ny_test_prob = ml5.predict(X_test_sm)","fa26f6b1":"y['Survived']=y_test_prob.apply(lambda x: 1 if x>0.6 else 0)","1523ffa5":"y.to_csv('my_submission.csv', index=False)","d40370dd":"### Step 6b: Data Prep","6991c81b":"**Dropping repeated features**","d3dd0064":"## Step 5: Model Prediction","56889e7e":"**Find Optimal Cut-Off probability**","b8ea20c8":"#### Model 1","9a516f31":"Dropping Fare as its p-value is high\n\n#### Model 3","440fbdf7":"Drop Parch Freature as p-value is still high\n\n#### Model 4","00af21e2":"## Step 2: Data Preparation","d0071aee":"## Step 3: Model Building","de7d0ecb":"This model looks fine as the p-value and VIF are within range","e027fc1a":"**Checking Distribution**","d36f9c14":"There is no such relation for Embarkment. Thus imputing the mode which is `S` for the Embarked columns for the missing value","4a18796b":"## Step 4: Model Evaluation","181c0a0f":"From above we can see that:\n- Name \/ Ticket\/ Cabin can be dropped\n- Age has some missing values\n- Embarked has some missing values\n- For Train Dataset PassengerID can be dropped","29a285b5":"### Step 6a: Data Clean","93c2c253":"Imputing values for Age","6a29a801":"Creating Bucket for Fare. The variable would be used for imputing the missing values for Age and Embarkment","e1fbc0db":"Scores above looks good on the Test Data. As see from above only 5 variables are used we would drop the remaining columns and would need to create the dummy varable for Class and Sex. Also we would need to impute missing values for Age.\n\nFor imputation of Age variable we would also need to create a temporary Fare bucket.\n\n\n## Step 6: Data Submission on Final Data","641526eb":"Check for Data imbalance","4e8e2de5":"# Titanic Disaster\n\n\n## Step1: Load Dataset and inspecting Dataframe","6ad6873b":"Drop Embarked_S due to its high p-value\n\n#### Model 5","ee9d73db":"### Step 6c: Final Predictions","a8644df6":"3 records are still null. Need to impute these values","2f808411":"Dropping Embarked_Q as the p-value is very high\n\n#### Model 2","84b4bbdd":"Data is ready for further processing\n\n**Train-Test Split**","5601ce40":"Optimal cut-off is 0.4. But as per the competetion we would need to target the accuracy. So we would be choosing probability as 0.6","5e44055b":"Parch and Fare are not dropped now. These variables are used by the scaler. So would be dropped post scaling","717d4833":"**Data Scaling**"}}