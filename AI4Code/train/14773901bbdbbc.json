{"cell_type":{"21d938a8":"code","63262316":"code","0a778f9c":"code","d025b1eb":"code","05863dc1":"code","3e549d7a":"code","fbfe919f":"code","7d9f2d3d":"code","cf11b481":"code","3c6bc8ae":"code","4d13c6ed":"code","453beab8":"code","6277e515":"code","f0154aac":"code","8573d5f4":"code","6fe45ce5":"code","be2a0b66":"code","8f2660b2":"code","9cda2c33":"code","0774b949":"code","9b80f6ce":"code","7efa6adf":"code","caf89dfa":"code","14f1fae2":"code","3646973d":"markdown","da0cd897":"markdown","e7c66885":"markdown","a06a908e":"markdown","19868675":"markdown","4c44775f":"markdown","9b88f24f":"markdown","4143e6b8":"markdown","00fd3683":"markdown","d676607f":"markdown","4e1d536b":"markdown","fff22b23":"markdown","7531a460":"markdown","11f3af90":"markdown","8097c254":"markdown","1cee8040":"markdown","e1784963":"markdown","5d99ccc2":"markdown","a85ee9a0":"markdown","9cf214f9":"markdown","0c10e777":"markdown","09bef3e1":"markdown","2ee0cc2e":"markdown","155b227f":"markdown","b81f7842":"markdown","668e57c6":"markdown","9956525f":"markdown","eda9f9ee":"markdown","5efdb07a":"markdown","80068e1c":"markdown","5b49e331":"markdown","fe15bb2d":"markdown","66958d97":"markdown","3a8406f1":"markdown","05f7f333":"markdown","cafed0bd":"markdown","86ffe725":"markdown","7cf7f03b":"markdown","39c2c8f6":"markdown","a91ea704":"markdown","bb1c421d":"markdown","fe00e94e":"markdown","3b576364":"markdown","b1867e76":"markdown","24c3d45e":"markdown","d1d1019d":"markdown","e0a422d2":"markdown","ea728c66":"markdown","ff3ff418":"markdown","8559b73b":"markdown","dc871a5d":"markdown","1c47add9":"markdown","dad63998":"markdown","75bfbe08":"markdown","922d150b":"markdown","1403cb11":"markdown","0692037d":"markdown","aa92245a":"markdown","fc99be0d":"markdown","fa2630f7":"markdown","4f93f166":"markdown","ce3e0063":"markdown","b5bd5a36":"markdown","5d59b202":"markdown","a712bba9":"markdown","c398f4d7":"markdown","3146df05":"markdown","b7e07cda":"markdown","9d3cb2a6":"markdown","37c8a497":"markdown","c89549be":"markdown","3907145b":"markdown","157f32bb":"markdown","f16eef04":"markdown","e248e7ce":"markdown","8b02a254":"markdown","b87b8d5b":"markdown","3d486d72":"markdown","de002460":"markdown","aa72e38d":"markdown","026e673b":"markdown","2ef89e61":"markdown","4eaa8b29":"markdown","49fb6cae":"markdown","afc5e8d1":"markdown","9bfb32bb":"markdown","e18d3a91":"markdown","a82bcc6e":"markdown","a32621e1":"markdown"},"source":{"21d938a8":"import pandas as pd\nimport numpy as np\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","63262316":"from IPython.display import Image # Introduce the display module for convenience\nimport os","0a778f9c":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods1.png\")","d025b1eb":"Image(\"\/kaggle\/input\/week9dataset\/Quality_Based_Reinforcement_Learning_Methods2.jpeg\")","05863dc1":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods3.png\")","3e549d7a":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods4.png\")","fbfe919f":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods5.png\")","7d9f2d3d":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods6.png\")","cf11b481":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods7.png\")","3c6bc8ae":"import time\n\ndef init_env():\n    start=(0, 0)\n    terminal=(3, 2)\n    hole=(2, 1)\n    env = np.array([['_ '] * 4] * 4) # Build a 4*4 environment \n    env[terminal] = '$ ' # Big ham\n    env[hole] = '# ' # Trap\n    env[start] = 'L '# Lion\n    interaction = ''\n    for i in env:\n        interaction += ''.join(i) + '\\n'\n    print(interaction)","4d13c6ed":"init_env()","453beab8":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods8.png\")","6277e515":"def init_q_table():\n    actions = np.array(['up', 'down', 'left', 'right'])\n    q_table = pd.DataFrame(np.zeros((16, len(actions))), columns=actions) \n    return q_table","f0154aac":"init_q_table()","8573d5f4":"def act_choose(state, q_table, epsilon):\n    \"\"\"\n    Parameters:\n    state\n    q_table\n    epsilon -- The probability\n    Returns:\n    action -- Next action\n    \"\"\"\n    state_act = q_table.iloc[state, :]\n    actions = np.array(['up', 'down', 'left', 'right'])\n    if (np.random.uniform() > epsilon or state_act.all() == 0):\n        action = np.random.choice(actions)\n    else:\n        action = state_act.idxmax()\n    return action","6fe45ce5":"def env_feedback(state, action, hole, terminal):\n    \"\"\"\n    Parameters:\n    state\n    action\n    hole -- Where the trap is\n    terminal\n    Returns:\n    next_state\n    reward\n    end -- The end signal\n    \"\"\"\n    reward = 0.\n    end = 0\n    a, b = state\n    if action == 'up':\n        a -= 1\n        if a < 0:\n            a = 0\n        next_state = (a, b)\n    elif action == 'down':\n        a += 1\n        if a >= 4:\n            a = 3\n        next_state = (a, b)\n    elif action == 'left':\n        b -= 1\n        if b < 0:\n            b = 0\n        next_state = (a, b)\n    elif action == 'right':\n        b += 1\n        if b >= 4:\n            b = 3\n        next_state = (a, b)\n    if next_state == terminal:\n        reward = 10.\n        end = 2\n    elif next_state == hole:\n        reward = -10.\n        end = 1\n    else:\n        reward = -1.\n    return next_state, reward, end","be2a0b66":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods9.png\")","8f2660b2":"def update_q_table(q_table, state, action, next_state, terminal, gamma, alpha, reward):\n    \"\"\"\n    Parameters:\n    q_table\n    state\n    action\n    next_state\n    terminal\n    gamma -- The discount factor\n    alpha -- The learning rate\n    reward\n    Returns:\n    q_table -- Updated Q-Table\n    \"\"\"\n    x, y = state\n    next_x, next_y = next_state\n    q_original = q_table.loc[x * 4 + y, action]\n    if next_state != terminal:\n        q_predict = reward + gamma * q_table.iloc[next_x * 4 + next_y].max()\n    else:\n        q_predict = reward\n    q_table.loc[x * 4 + y, action] = (1-alpha) * q_original+alpha*q_predict\n    return q_table","9cda2c33":"def show_state(end, state, episode, step, q_table):\n    \"\"\"\n    Parameters:\n    end -- End signal\n    state \n    episode -- The number of iterations\n    step -- The step of iteration\n    q_table-- Q-Table\n    \"\"\"\n    terminal = (3, 2)\n    hole = (2, 1)\n    env = np.array([['_ '] * 4] * 4)\n    env[terminal] = '$ '\n    env[hole] = '# '\n    env[state] = 'L '\n    interaction = ''\n    for i in env:\n        interaction += ''.join(i) + '\\n'\n    if state == terminal:\n        message = 'EPISODE: {}, STEP: {}'.format(episode, step)\n        interaction += message\n        display.clear_output(wait=True)  # Clear\n        print(interaction)\n        print(\"\\n\"+\"q_table:\")\n        print(q_table)\n        time.sleep(3)  # Wait 3 seconds\n    else:\n        display.clear_output(wait=True)\n        print(interaction)\n        print(q_table)\n        time.sleep(0.3)  # Control the time taken for each step","0774b949":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods12.png\")","9b80f6ce":"def q_learning(max_episodes, alpha, gamma, epsilon):\n    \"\"\"\n    Parameters:\n    max_episodes -- The maximum number of iterations\n    alpha -- Learning rate\n    gamma -- Discount factor\n    epsilon -- Probability\n    Returns:\n    q_table -- Updated Q-Table\n    \"\"\"\n    q_table = init_q_table()\n    terminal = (3, 2)\n    hole = (2, 1)\n    episodes = 0\n    while(episodes <= max_episodes):\n        step = 0\n        state = (0, 0)\n        end = 0\n        show_state(end, state, episodes, step, q_table)\n        while(end == 0):\n            x, y = state\n            act = act_choose(x * 4 + y, q_table, epsilon)  # Choose action\n            next_state, reward, end = env_feedback(\n                state, act, hole, terminal)  # Reward from the environment\n            q_table = update_q_table(\n                q_table, state, act, next_state, terminal, gamma, alpha, reward)  # q-table update\n            state = next_state\n            step += 1\n            show_state(end, state, episodes, step, q_table)\n        if end == 2:\n            episodes += 1","7efa6adf":"from IPython import display\nq_learning(max_episodes=10, alpha=0.8, gamma=0.9, epsilon=0.9)","caf89dfa":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods12.png\")","14f1fae2":"Image(\"\/kaggle\/input\/week9dataset\/Quality-based Reinforcement Learning Methods13.png\")","3646973d":"Sarsa is essentially similar to Q-Learning. The biggest difference is the step of updating Q-Table. Let's first look at Sarsa from the pseudocode perspective:","da0cd897":"Regarding the difference between Sarsa and Q-Learning algorithms, no matter whether from pseudocode or algorithm principle, we can't understand it intuitively. Let's take a picture to illustrate it:","e7c66885":"As shown in the following figure (where the data is a random example), the columns in the Q-Table are optional Action and each row of data consists of a State corresponding to the execution of different actions. The data stored is the maximum expected reward value (Q value):","a06a908e":"In the quality-based reinforcement learning algorithm, there is another algorithm similar to Q-Learning, called Sarsa. Let's take a quick look at the difference between Sarsa and Q-Learning.","19868675":"## Q-Learning","4c44775f":"- Q-Learning<br>\n- Update process of Q-Table<br>\n- Discount factor<br>\n- Sarsa<br>\n- Off-Policy and On-Policy algorithms<br>\n- Common terminology","9b88f24f":"### Implementation and Visualization of Q-Learning","4143e6b8":"In the reinforcement learning method, the quality-based reinforcement learning method is a very important branch. This chapter will introduce two representative methods: Q-Learning and Sarsa. Combined with the algorithm procedure, Q-Learning will be implemented in Python.","00fd3683":"## Quality-Based Reinforcement Learning Methods","d676607f":"\nAction reward<br>\n","4e1d536b":"### Create the Environment","fff22b23":"As shown in the above figure, based on the Markov decision process (MDP), the model-based and model-free reinforcement learning methods have been evolved. The entire course of this study focuses on the part of time difference learning, especially the three most basic methods:  Q-learning, Sarsa and Policy Gradient.","7531a460":"In fact, the previous classification is not rigorous, and the more rigorous reinforcement learning algorithms are classified as follows:","11f3af90":"This chapter introduces the quality-based reinforcement learning algorithm in detail. Since the content is very abstract, we suggest repeated learning and training. The knowledge points are:<br>\n<br>\n- Q-Learning<br>\n- Update process of Q-Table<br>\n- Discount factor<br>\n- Sarsa<br>\n- Off-Policy and On-Policy algorithms<br>\n- Common terminology","8097c254":"The above figure presents a typical reinforcement learning process, five elements of which we have explained in detail in the introductory course. This time, we have to re-examine the composition of the **Agent**.","1cee8040":"<div style=\"color: #999;font-size: 12px;font-style: italic;\">Congratulations! You've completed the Quality-based Reinforcement Learning Methods Lab.<\/div>","e1784963":"Expand $(1)$:","5d99ccc2":"**1. Initialize Q-Table:** Build tables of the same aspect based on environment and action type.<br>\n<br>\n**2. Action Choice:** Action selection based on the value of Q-Table.<br>\n<br>\n**3. Action Reward:** Get action feedback from the environment after taking action.<br>\n<br>\n**4. Q-Table Update:** Update Q-Table based on feedback and future estimated rewards.<br>\n<br>\nThen repeat steps 2 through 4 until the expected result is reached or the preset number of iterations ends, and Q-Table is output to perform the Q-Learning learning process.","a85ee9a0":"Here we will not implement the Sarsa algorithm. You may try to implement the Sarsa algorithm update process in the next challenge.","9cf214f9":"Based on the algorithmic principle of Q-Learning, let's use Python to restore the game scene of the lion looking for big ham, and use the Q-Learning algorithm to help the little lion find the big ham as soon as possible.","0c10e777":"As mentioned earlier, the entire process of reinforcement learning is done in the environment. Therefore, we need to build an environment that can be used for algorithm testing, which is also the difference in reinforcement learning.","09bef3e1":"\nQ-Table update function<br>\n","2ee0cc2e":"Next the function that implements the action choice:","155b227f":"### Introduction","b81f7842":"In this experiment, in order to present more intuitively and also for the support of the online environment, we simplified the lion game to a text version. In the word game shown below, the little lion is represented by `L`, the trap is represented by `#`, the big ham is represented by `$` and the ordinary lawn is represented by `_`:","668e57c6":"Some of the basic elements of reinforcement learning have been introduced above. Next we are ready to let the Agent start learning. In this experiment, we have used the Q-Learning algorithm. The core of the Q-Learning algorithm is the Q-Table update process.<br>\n<br>\nQ-Learning adopts the idea of [Bellman Equation](https:\/\/en.wikipedia.org\/wiki\/Bellman_equation) to update. Due to the complex derivation, no detail is made here. Interested students can view it through the link. Below we will explain the update of Q-Table.","9956525f":"To answer the above question, we have to go back and explain the reinforcement learning process diagram:","eda9f9ee":"- **Policy:** The agent's behavior function, reflecting the actions the Agent can take. Here the state is an input, with its next action decision as an output.<br>\n- **Value function:** Determining the value of each state or action, similar to evaluating the expected reward after taking some action.<br>\n- **Model:** Model is used for Agent to perceive Environment changes. Notice that this is not the real environment, but the environment in the eyes of the Agent.","5efdb07a":"\nVisualization of the state<br>\n","80068e1c":"On running the program, it is very straightforward to see that, as the iteration increases, the lion walks more and more correctly, and `step` (= number of steps taken) used will be fewer and fewer until the shortest path is found. However, after finding the shortest path, there may still be a situation of `step` increase, which is the result of exploration performed randomly by introducing $\\varepsilon $$-greedy$.","5b49e331":"Sarsa is more \"pragmatic\". It is an On-Policy algorithm that learns through its own experience.<br>\n<br>\nWhen updating the Q-Table, Sarsa uses the actual action to update, that is, after $S_{t+1}$, the $Q(S_{t}, a)$ is updated. In fact, the performance of the little lion is very cautious. When choosing a path, he pays more attention to the existence of traps.","fe15bb2d":"### Initialize the Q-Table","66958d97":"**1. Initialize Q-Table: **Build tables of the same aspect based on environment and action type. (same as Q-Learning)<br>\n<br>\n**2. Action Choice: **Select the next action based on the value of Q-Table. (same as Q-Learning)<br>\n<br>\n**3. Action Reward: ** Get action feedback from the environment after taking action. (same as Q-Learning)<br>\n<br>\n**4. Q-Table Update: **Update Q-Table based on feedback and actual rewards for the next status. (different from Q-Learning)<br>\n<br>\nRepeat steps 2 through 4 until the expected result is reached or the preset number of iterations is over. Output Q-Table and finish Sarsa.","3a8406f1":"In the first lesson of this week's content, we present the classification table for the reinforcement learning algorithm. This table classifies common algorithms from three aspects: value iteration, strategy iteration and model iteration. **So, how are these three aspects determined?**","05f7f333":"Where $\\gamma$ is a number `0` to `1`. It is easy to see that the later rewards have less impact on the current estimate by power calculation.","cafed0bd":"$$\nQ(s_{1})=r_{2}+\\gamma \\cdot r_{3}+\\gamma^{2} \\cdot r_{4}+\\gamma^{3} \\cdot r_{5}+\\gamma^{4} \\cdot r_{6}+\\cdots \\tag2\n$$","86ffe725":"Below, the complete implementation of the Q-Learning algorithm is given:","7cf7f03b":"As you can see in the diagram, the environment of the text maze game has been created.","39c2c8f6":"Next write the feedback function of the Agent:","a91ea704":"---","bb1c421d":"### Principle of Q-Learning","fe00e94e":"Similar to the example where the little lion looks for the big ham, the principle of the Q-Learning algorithm is as follows:","3b576364":"### Principle of Sarsa","b1867e76":"### Update the Q-Table","24c3d45e":"## Preface","d1d1019d":"Since it is a recursive iterative process, we expand the recursive formula to gain a deeper understanding of the discount factor. For the convenience of writing here, we abbreviate the formula. Assuming that $Q(s_{1})$ is the reward value in the initial state, then the update formula for $Q(s_{1})$ is:","e0a422d2":"The pseudocode is explained as follows:","ea728c66":"Assume that the lion walks to the upper right corner of the grid, defining the state at this time as $s_{t}$, the direction moving to is $a_{t}$ and the reward value of the action taken at the current state is represented by $Q(s_{t}, a_{t})$ in Q-Table. After defining the basic variables, the formula for updating the Q value is given directly:","ff3ff418":"Now the Q-Learning algorithm is finally implemented which can help the little lion find the big ham. Next we specify the parameters and execute a function to observe how the lion uses the Q-Learning algorithm to find the shortest path to the big ham placement point:","8559b73b":"$$\nQ(s_{1})=r_{2}+\\gamma \\cdot Q(s_{2})=r_{2}+\\gamma \\cdot [r_{3}+\\gamma \\cdot Q(s_{3})]=r_{2}+\\gamma \\cdot [r_{3}+\\gamma \\cdot [r_{4}+\\gamma \\cdot Q(s_{4})]] =\\cdots \\tag1\n$$","dc871a5d":"### Reward","1c47add9":"### Environment","dad63998":"After completing the challenge of this experiment, by showing the comparison, it is found that Q-Learning is easier than Sarsa to find a suitable path, but, in the process of finding a suitable path, Q-Learning fails more than Sarsa does.","75bfbe08":"---","922d150b":"## Sarsa","1403cb11":"First of all, let's think about what we will do in real life when we encounter an unfamiliar environment.<br>\n<br>\nThe most common way is to make marks: On the passing lawn, write down the four directions (_up_, _down_, _left_ and _right_) and explore the possible benefits. At the beginning, we may move randomly in the lawn and mark it. When a mark hits a trap, we will change the mark and give ourselves a reminder. As the marker increases, we select the next action based on the marker (Q value) and then update the Q value on the lawn based on the result of the action.<br>\n<br>\nIn this way, through repeated self-learning, the whole lawn will be marked. In fact, the process of updating the mark (Q value) is a process of being familiar with the environment, and the mark can quickly find the goal in future attempts.<br>\n<br>\nWhat about applying real-life thinking to our supposed game scenarios? In fact, the Q in which the mark is made is the value (Quality), and the repeated self-learning based on the Q value is called **Quality-based reinforcement learning**.","0692037d":"### Concept of Q-Table","aa92245a":"Suppose there is such a game scene: The little lion, is playing happily on the grassland. It knows that someone will place its favorite big ham in a fixed position every time. It wants to find the big ham quickly, but there are many traps between it and the place. Once the trap is reached, the game is over and the little lion is back to the beginning. So, how can the little lion find the big ham as soon as possible?<br>\n<br>\nThe most stupid way: The little lion attempts to remember the locations of traps, and then, in case of avoiding the trap, it searches each safe place until it finds the big ham. Obviously, it is not easy to find the shortest path by random. So, is there any better way to solve this problem?","fc99be0d":"**References:**<br>\n<br>\n- [Q-learning - Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Q-learning)<br>\n- [Sarsa - Wikipedia](https:\/\/en.wikipedia.org\/wiki\/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action)","fa2630f7":"From the pseudocode, you can summarize Sarsa's procedure into the following steps:","4f93f166":"### Scenarios","ce3e0063":"**So, what Q-Table looks like**","b5bd5a36":"We want to test the Q-Learning algorithm in the maze. In the local environment, you can use Python-supported _Tkinter_, _PyQt_ and _wxPython_ to write a GUI program. The previous process is still complicated.","5d59b202":"- Python 3.6<br>\n- NumPy 1.14.2<br>\n- Matplotlib 2.2.2<br>\n- Pandas 0.22.0<br>\n- `scikit-learn` 0.19.1","a712bba9":"After defining the visualization function, Q-Learning is implemented. Here is the pseudocode for the Q-Learning algorithm:","c398f4d7":"Agent lion will get reward from the environment after every action is executed and then will immediately enter the respectively next State.<br>\n<br>\nHere, in our environment, the reward value at the end (where the big ham is) is `10` and the reward value at the trap is `-10`. At the same time, in order to help the model converge quickly, that is, let the little lion find the shortest path as soon as possible, we set the reward value for each step to `-1`. This is also a small trick that will cause the Agent to be punished when going further or getting around in an area.","3146df05":"### Choice of Action","b7e07cda":"\nInitialize Q-Table<br>\n","9d3cb2a6":"In quality-based reinforcement learning, the most basic algorithms are **Q-Learning** and **Sarsa**, and Q-Learning is a more widely used algorithm in practice.","37c8a497":"In order to facilitate the process of the lion's step, we define another function below. This function is convenient for displaying the state of the lion's every step:","c89549be":"Of the above three components, the Agent does not need to be equipped with them all the time. Based on these three components, we have obtained three classification aspects in the table.","3907145b":"Next we implement the Q-Table update function:","157f32bb":"## Summary","f16eef04":"### Key Points","e248e7ce":"After getting the Q-Table, how to proceed to the next action strategy based on Q-Table?<br>\n<br>\nAccording to common sense, it is definitely the choice of the best decision (the action with the highest Q value) which maximizes long-term rewards. But there will be some problems, that is: At beginning, because the value in Q-Table is random, it has no meaning for learning, and it is easy to make mistakes. After learning for a while, due to the fixed value in the table, the routes are fixed, which cannot effectively explore the environment and we easily fall into local optimal problems.<br>\n<br>\nSo, how should action be selected?<br>\n<br>\nThe $\\varepsilon $$-greedy$ algorithm provides a better solution. When selecting an action, a probability value is introduced and the optimal strategy is selected under a certain probability. A random selection is performed so that the best decision may be selected while doing the Environment exploration.","8b02a254":"In the above case, let us combine the five elements of the game scene and the reinforcement learning: The lion is the **Agent**, the lawn is the **Environment**, the movement is **Action**, the state when moving to each lawn is **State** and the feedback of moving to the corresponding lawn is **Reward**.<br>\n<br>\nCombine the marked Q values on all the lawns into a single table, the **Q-Table**. In fact, Q-Table is the core of the quality-based reinforcement learning algorithm because, in the quality-based reinforcement learning, the Q-Table can determine the choice of action and the environmental feedback (Reward) obtained after the action has been performed, thus to update the Q-Table.","b87b8d5b":"## Brief Introduction to Quality-Based Methods","3d486d72":"<div style=\"color: #999;font-size: 12px;font-style: italic;\">*Note: The update formula of Q-Table in pseudocode is the same as the update formula of Q-Table mentioned above. The actual meaning is the same.<\/div>","de002460":"```<br>\nInitialize Q arbitrarily \/\/ Randomly initialize Q value\nRepeat (for each episode): \/\/ Every game is called an episode\n    Initialize S \/\/ The state of the lion's initial position\n    Repeat (for each step of episode):\n        According to the current Q and position S, use the \u03b5-greedy strategy to get action A.\n        Did action A, the little lion arrived at the new location S' and received a reward R\n        Q(S,A) \u2190 (1-\u03b1)*Q(S,A) + \u03b1*[R + \u03b3*maxQ(S',a)] \/\/ Update S in Q\n        S \u2190 S'\n    until S is terminal \/\/ The little lion finds the big ham or falls into the trap and starts again.\n```","aa72e38d":"The Q-Learning algorithm is actually an Off-Policy algorithm that can be learned by estimation or other predictions.<br>\n<br>\nWhen updating the Q-Table, Q-Learning uses a \"greedy\" algorithm that updates based on the estimated future reward value. In fact, the performance of the little lion is very adventurous. When choosing the path, finding the big ham will be a priority, no matter how dangerous the route is. As long as it is closer to the big ham, it will move forward.","026e673b":"You will find that Sarsa has only one different step. That is, in Q-Learning we update Q-Table based on feedback and future estimated rewards while in Sarsa we update the Q-Table based on feedback and actual rewards at the next state.","2ef89e61":"### Difference between Sarsa and Q-Learning","4eaa8b29":"<h1 style=\"color:brown\">Reinforcement Learning<\/h1>","49fb6cae":"According to the actual environment, draw a Q-Table of $4*4$ and divide the space in the four directions of `up`, `down`, `left` and `right` in each grid, which is actually $4*4*4$. For a more intuitive view of Q-Table, we define Q-Table in the form of a two-dimensional table $16*4$. The column name is _Action_ and each _State_ is represented by a row:","afc5e8d1":"It can be seen from the above equation that the update process of the Q value is actually a recursive iterative process.<br>\n<br>\nIn simple terms, the update of the Q value is mainly composed of two parts: the current reward value and the learning reward value, which are connected by the learning rate $\\alpha$. When the value of $\\alpha$ is higher, lesser the current value is retained, the more important it is to learn, and _vice versa_.<br>\n<br>\nThe current reward value is the original value of the Q-Table, and the learning reward value is also composed of two parts: $r_{t+1}$ (reward after the move) and an estimate of the future optimal value. The discount factor is quoted here, as a weight, which is used to reduce the reward for the later estimated state.","9bfb32bb":"Next let's use Python to write the environment. You don't need to master this part of the code since the course focuses on understanding the reinforcement learning algorithm:","e18d3a91":"---","a82bcc6e":"At the time of initialization, we set each Q value of the Q-Table to `0`. The DataFrame in Pandas is used directly to output the Q-Table of the initialization state:","a32621e1":"In fact, the Agent usually contains the following three components:"}}