{"cell_type":{"d0548f0d":"code","765a39f5":"code","6ef2e3cd":"code","ef3899bb":"code","8afaa963":"code","6d230204":"code","038577f9":"code","8a0db382":"code","c924848e":"code","9bc8de91":"code","b59bf579":"code","f30ef29e":"code","fab3c07c":"code","cee413a0":"code","d77002e5":"code","3ea4167c":"code","df3aa657":"code","bcb6d007":"code","4e09b60c":"code","175ce38d":"code","915a8cea":"code","e9c816dc":"code","fb3d9728":"code","8a4596f2":"code","8060eea2":"code","a8d03008":"code","9d59f40e":"code","b8e2b504":"code","2d0789c2":"code","ff9aad56":"code","ad41c048":"code","bdb4ca8b":"code","d08a7733":"code","4bed1ee4":"code","5af3d799":"code","b053aa34":"code","7d6a44de":"code","c2a67aa3":"code","fc346ed1":"code","5d05dd11":"code","e2e2a394":"code","f73da05b":"code","81090d22":"code","5fdf4949":"code","fb1a229e":"code","5c10937a":"code","8e096b30":"code","c2e29c5f":"code","289280a9":"code","3d84a78e":"code","c7a884a5":"code","d91f7338":"code","6c7c3849":"code","4929d28a":"code","1703bea3":"code","b780931e":"code","689a8261":"code","24f5fb52":"code","73690cfb":"code","963ad359":"code","aee008c2":"code","2b30564e":"code","c3ef1679":"code","000bc46d":"code","6fbd0093":"code","04b281d8":"code","172f07ff":"code","713669b5":"code","d7d95d9e":"code","0a7ce05a":"code","0ac6dab7":"code","dd132d7f":"code","dfe4f73f":"code","01c04987":"code","a1662d8c":"code","cd0bcedc":"code","07dba8db":"code","1cb330fa":"code","7a9045de":"code","051c8bb6":"code","689738a2":"code","32570450":"code","35c36422":"code","41853dfc":"code","ba4973fd":"code","f089c023":"code","d6513d63":"code","30d2e4a1":"code","1b63be8b":"code","ba88a50d":"code","2cf42b04":"markdown","9e450665":"markdown","3206dc71":"markdown","608e7314":"markdown","46f0e545":"markdown","508a7fd7":"markdown","649f526d":"markdown","fc239f48":"markdown","330033b3":"markdown","884c790f":"markdown","7d846db4":"markdown","4f37115d":"markdown"},"source":{"d0548f0d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as st","765a39f5":"df=pd.read_csv('..\/input\/house-price-data\/house_data.csv')","6ef2e3cd":"df","ef3899bb":"df.info()","8afaa963":"df['date'] = df['date'].str.split('T').str[0]\ndf.head()","6d230204":"df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors = 'coerce')","038577f9":"df.head()","8a0db382":"df.info()","c924848e":"df.isnull().sum()","9bc8de91":"(df[df.columns]==0).any()","b59bf579":"df.loc[df.bedrooms==0,['id','bathrooms','sqft_living','price']]","f30ef29e":"df.shape[0] - df.id.nunique()","fab3c07c":"plt.figure(figsize=(10,8))\nsns.boxplot(y='price',data=df)","cee413a0":"pd.DataFrame(df.groupby('bedrooms')['price'].mean())","d77002e5":"df.loc[df.bedrooms==33]","3ea4167c":"sns.relplot(x='bedrooms',y='price',data=df,kind='scatter')","df3aa657":"sns.relplot(x='bedrooms',y='price',data=df,kind='scatter')","bcb6d007":"df.shape","4e09b60c":"df = df[df.bedrooms < 12]","175ce38d":"df.shape","915a8cea":"df.head()","e9c816dc":"df.shape","fb3d9728":"df.bathrooms.value_counts()","8a4596f2":"sns.relplot(x='bathrooms',y='price',data=df,kind='scatter')","8060eea2":"df.bathrooms=df['bathrooms'].transform(lambda x:x.replace(0,x.median()))","a8d03008":"sns.relplot(x='bathrooms',y='price',data=df,kind='scatter')","9d59f40e":"df.head()","b8e2b504":"(df[df.columns]==0).any()","2d0789c2":"pd.set_option('display.max_columns',21)\ndf.head()","ff9aad56":"df['Vintage']=2020-df['yr_built']","ad41c048":"df.head()","bdb4ca8b":"df=df.drop_duplicates(keep ='last',subset ='id')","d08a7733":"df.shape","4bed1ee4":"df.drop(['id','date','yr_built','yr_renovated','zipcode','lat','long'],axis=1,inplace=True)","5af3d799":"df.shape","b053aa34":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nscaled=pd.DataFrame(sc.fit_transform(df),columns=df.columns)","7d6a44de":"## Raw linear regression model\nX = df.drop('price', axis=1)\ny= df['price']\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\n\nprint(f'Coefficients: {lin_reg.coef_}')\nprint(f'Intercept: {lin_reg.intercept_}')\nprint(f'R^2 score: {lin_reg.score(X, y)}')","c2a67aa3":"from sklearn.model_selection import train_test_split\nX_train, X_test , y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","fc346ed1":"lin_reg = LinearRegression()\nmodel = lin_reg.fit(X_train,y_train)\nprint(f'R^2 score for train: {lin_reg.score(X_train, y_train)}')\nprint(f'R^2 score for test: {lin_reg.score(X_test, y_test)}')","5d05dd11":"import warnings \nwarnings.filterwarnings('ignore')\nimport statsmodels.api as sm\n\nX_constant = sm.add_constant(X)\nlin_reg = sm.OLS(y,X_constant).fit()\nlin_reg.summary()","e2e2a394":"df1=df.copy()","f73da05b":"df1.skew()","81090d22":"### As the dataset is positively skewed,we have to reduce the powers by using log or sqrt transformations.","5fdf4949":"df1=df1.transform(lambda x:x**0.5)","fb1a229e":"X =df1.drop('price',axis=1)\ny =df1.price\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","5c10937a":"df2=df.copy()","8e096b30":"df2=df2.transform(lambda x:np.log1p(x))","c2e29c5f":"X =df2.drop('price',axis=1)\ny =df2.price\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","289280a9":"#Adding constant column of ones, mandatory for sm.OLS model\nX_1 = sm.add_constant(X)\n#Fitting sm.OLS model\nmodel = sm.OLS(y,X_1).fit()\nmodel.pvalues","3d84a78e":"cols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","c7a884a5":"df.shape","d91f7338":"import statsmodels.api as sm\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso","6c7c3849":"model = LinearRegression()","4929d28a":"#Initializing RFE model\nrfe = RFE(model,5)","1703bea3":"#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)\nprint(rfe.support_)\nprint(rfe.ranking_)","b780931e":"X.columns","689a8261":"#no of features\nnof_list=np.arange(1,15)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train)\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))","24f5fb52":"cols = list(X.columns)\nmodel = LinearRegression()\n#Initializing RFE model\nrfe = RFE(model, 5)             \n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)              \ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)","73690cfb":"df.head()","963ad359":"reg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)","aee008c2":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","2b30564e":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","c3ef1679":"## Building of simple OLS model.\nX = df1.drop('price',1)\ny = df1.price\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","000bc46d":"### calculating the vif values as multicollinearity exists (as stated by warning 2)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n[variance_inflation_factor(X.values, j) for j in range(1, X.shape[1])]","6fbd0093":"# removing collinear variables\n# function definition\n\ndef calculate_vif(x):\n    thresh = 5.0\n    output = pd.DataFrame()\n    k = x.shape[1]\n    vif = [variance_inflation_factor(x.values, j) for j in range(x.shape[1])]\n    for i in range(1,k):\n        print(\"Iteration no.\")\n        print(i)\n        print(vif)\n        a = np.argmax(vif)\n        print(\"Max VIF is for variable no.:\")\n        print(a)\n        if vif[a] <= thresh :\n            break\n        if i == 1 :          \n            output = x.drop(x.columns[a], axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n        elif i > 1 :\n            output = output.drop(output.columns[a],axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n    return(output)","04b281d8":"## passing X to the function so that the multicollinearity gets removed.\ntrain_out = calculate_vif(X)","172f07ff":"## includes only the relevant features\ntrain_out.head()","713669b5":"lr = LinearRegression()\nlr.fit(X_train, y_train)","d7d95d9e":"# higher the alpha value, more restriction on the coefficients; \n# low alpha > more generalization, coefficients are barely\nrr = Ridge(alpha=0.01) \n# restricted and in this case linear and ridge regression resembles\nrr.fit(X_train, y_train)","0a7ce05a":"rr100 = Ridge(alpha=100) #  comparison with alpha value\nrr100.fit(X_train, y_train)","0ac6dab7":"train_score=lr.score(X_train, y_train)\ntest_score=lr.score(X_test, y_test)","dd132d7f":"Ridge_train_score = rr.score(X_train,y_train)\nRidge_test_score = rr.score(X_test, y_test)","dfe4f73f":"Ridge_train_score100 = rr100.score(X_train,y_train)\nRidge_test_score100 = rr100.score(X_test, y_test)","01c04987":"print(\"linear regression train score:\", train_score)\nprint(\"linear regression test score:\", test_score)\nprint(\"ridge regression train score low alpha:\", Ridge_train_score)\nprint(\"ridge regression test score low alpha:\", Ridge_test_score)\nprint(\"ridge regression train score high alpha:\", Ridge_train_score100)\nprint(\"ridge regression test score high alpha:\", Ridge_test_score100)","a1662d8c":"from sklearn.linear_model import Lasso","cd0bcedc":"lasso = Lasso()\nlasso.fit(X_train,y_train)\ntrain_score=lasso.score(X_train,y_train)\ntest_score=lasso.score(X_test,y_test)\ncoeff_used = np.sum(lasso.coef_!=0)","07dba8db":"print(\"training score:\"), train_score \nprint(\"test score: \"), test_score\nprint(\"number of features used: \"), coeff_used","1cb330fa":"lasso001 = Lasso(alpha=0.01, max_iter=10e5)\nlasso001.fit(X_train,y_train)","7a9045de":"train_score001=lasso001.score(X_train,y_train)\ntest_score001=lasso001.score(X_test,y_test)\ncoeff_used001 = np.sum(lasso001.coef_!=0)","051c8bb6":"print(\"training score for alpha=0.01:\"), train_score001 \nprint(\"test score for alpha =0.01: \"), test_score001\nprint(\"number of features used: for alpha =0.01:\"), coeff_used001","689738a2":"lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\nlasso00001.fit(X_train,y_train)","32570450":"train_score00001=lasso00001.score(X_train,y_train)\ntest_score00001=lasso00001.score(X_test,y_test)\ncoeff_used00001 = np.sum(lasso00001.coef_!=0)","35c36422":"print(\"training score for alpha=0.0001:\"), train_score00001 \nprint(\"test score for alpha =0.0001: \"), test_score00001\nprint(\"number of features used: for alpha =0.0001:\"), coeff_used00001","41853dfc":"lr = LinearRegression()\nlr.fit(X_train,y_train)\nlr_train_score=lr.score(X_train,y_train)\nlr_test_score=lr.score(X_test,y_test)","ba4973fd":"print(\"LR training score:\"), lr_train_score \nprint(\"LR test score: \"), lr_test_score","f089c023":"# Let's perform a cross-validation to find the best combination of alpha and l1_ratio\nfrom sklearn.linear_model import ElasticNetCV, ElasticNet\nfrom sklearn.metrics import r2_score\n\ncv_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, .995, 1], eps=0.001, n_alphas=100, fit_intercept=True, \n                        normalize=True, precompute='auto', max_iter=2000, tol=0.0001, cv=5, \n                        copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=None, selection='cyclic')","d6513d63":"cv_model.fit(X_train, y_train)","30d2e4a1":"print('Optimal alpha: %.8f'%cv_model.alpha_)\nprint('Optimal l1_ratio: %.3f'%cv_model.l1_ratio_)\nprint('Number of iterations %d'%cv_model.n_iter_)","1b63be8b":"# train model with best parameters from CV\nmodel = ElasticNet(l1_ratio=cv_model.l1_ratio_, alpha = cv_model.alpha_, max_iter=cv_model.n_iter_, fit_intercept=True, normalize = True)\nmodel.fit(X_train, y_train)","ba88a50d":"print(r2_score(y_test, model.predict(X_test))) # test data performance","2cf42b04":"Build a regression model to predict the price of house based on the the given features in the dataset.","9e450665":"### Elastic Net","3206dc71":"### Backward Elimination","608e7314":"### RFE","46f0e545":"### VIF:","508a7fd7":"### Importing necessary libraries:","649f526d":"## Regularization\n### Ridge","fc239f48":"### Problem Statement:","330033b3":"### Lasso:","884c790f":"## Feature Selection","7d846db4":"### Lasso Feature selection","4f37115d":"### Checking skewness of dataset:"}}