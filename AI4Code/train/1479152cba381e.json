{"cell_type":{"4f33d71a":"code","a9138ebe":"code","1446f950":"code","fe80f69d":"code","df975c3d":"code","830961f1":"code","d161bdd1":"code","78c99d4d":"code","2eff2eef":"code","67c309f6":"code","75dffa96":"code","a96674eb":"code","d05bc980":"code","0b624c24":"code","b0ffc21e":"code","c2838266":"code","e2d82293":"code","26dd5e2e":"code","cba25c0d":"code","4be5de1b":"code","60af47ab":"code","4067b2cd":"code","ccb06527":"markdown","36f51126":"markdown","e9f424d3":"markdown","0659915f":"markdown","967a5881":"markdown","aea920f5":"markdown","cd105ca7":"markdown","5cb2ae64":"markdown","fb4f07cb":"markdown","5ded072d":"markdown"},"source":{"4f33d71a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport os\nimport time\nimport shutil\nimport torch.nn as nn\nfrom skimage import io\nimport torchvision\nimport cv2\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nfrom albumentations.pytorch import ToTensor\nfrom torchvision import utils\nfrom albumentations import (HorizontalFlip, ShiftScaleRotate, VerticalFlip, Normalize,Flip,\n                            Compose, GaussNoise)\n\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","a9138ebe":"csv_path = '\/kaggle\/input\/global-wheat-detection\/train.csv'\ntrain_dir = '\/kaggle\/input\/global-wheat-detection\/train'","1446f950":"df = pd.read_csv(csv_path)\ndf.head()","fe80f69d":"print(f'Total number of train images is {len(os.listdir(train_dir))}')\nprint(f'shape of dataframe is {df.shape}')\nprint(f'Number of images in dataframe is {len(np.unique(df[\"image_id\"]))}')\nprint(f'Number of train images with no bounding boxes {len(os.listdir(train_dir)) - len(np.unique(df[\"image_id\"]))}')","df975c3d":"# this function will take the dataframe and vertically stack the image ids \n# with no bounding boxes\ndef process_bbox(df):\n    df['bbox'] = df['bbox'].apply(lambda x: eval(x))\n    df['x'] = df['bbox'].apply(lambda x: x[0])\n    df['y'] = df['bbox'].apply(lambda x: x[1])\n    df['w'] = df['bbox'].apply(lambda x: x[2])\n    df['h'] = df['bbox'].apply(lambda x: x[3])\n    df['x'] = df['x'].astype(np.float)\n    df['y'] = df['y'].astype(np.float)\n    df['w'] = df['w'].astype(np.float)\n    df['h'] = df['h'].astype(np.float)\n\n    df.drop(columns=['bbox'],inplace=True)\n#     df.reset_index(drop=True)\n    return df","830961f1":"df_new = process_bbox(df)\nprint(f'shape of dataframe after prerpocessing {df_new.shape}')\ndf_new.tail()","d161bdd1":"\nimage_ids = df_new['image_id'].unique()\ntrain_ids = image_ids[0:int(0.8*len(image_ids))]\nval_ids = image_ids[int(0.8*len(image_ids)):]\nprint(f'Total images {len(image_ids)}')\nprint(f'No of train images {len(train_ids)}')\nprint(f'No of validation images {len(val_ids)}')","78c99d4d":"train_df = df_new[df_new['image_id'].isin(train_ids)]\nval_df = df_new[df_new['image_id'].isin(val_ids)]","2eff2eef":"def get_transforms(phase):\n            list_transforms = []\n            if phase == 'train':\n                list_transforms.extend([\n                       Flip(p=0.5)\n                         ])\n            list_transforms.extend(\n                    [\n            ToTensor(),\n                    ])\n            list_trfms = Compose(list_transforms,\n                                 bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n            return list_trfms","67c309f6":"class Wheatset(Dataset):\n    def __init__(self,data_frame,image_dir,phase='train'):\n        super().__init__()\n        self.df = data_frame\n        self.image_dir = image_dir\n        self.images = data_frame['image_id'].unique()\n        self.transforms = get_transforms(phase)\n        \n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self,idx):\n        image = self.images[idx] + '.jpg'\n#         image_arr = io.imread(os.path.join(self.image_dir,image))\n        \n        image_arr = cv2.imread(os.path.join(self.image_dir,image), cv2.IMREAD_COLOR)\n        image_arr = cv2.cvtColor(image_arr, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image_arr \/= 255.0\n        image_id = str(image.split('.')[0])\n        point = self.df[self.df['image_id'] == image_id]\n        boxes = point[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        # there is only one class\n        labels = torch.ones((point.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((point.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor(idx)\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        if self.transforms:\n            sample = {\n                'image': image_arr,\n                'bboxes': target['boxes'],\n                'labels': target['labels']\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n        target['boxes'] = torch.stack(tuple(map(torch.tensor, \n                                                zip(*sample['bboxes'])))).permute(1, 0)\n        \n        return image, target, image_id\n            ","75dffa96":"train_data = Wheatset(train_df,train_dir,phase='train')\nval_data = Wheatset(val_df,train_dir,phase='validation')\n\nprint(f'Length of train data {len(train_data)}')\nprint(f'Length of validation data {len(val_data)}')","a96674eb":"# batching\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_data_loader = DataLoader(\n    train_data,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    val_data,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)","d05bc980":"def image_convert(image):\n    image = image.clone().cpu().numpy()\n    image = image.transpose((1,2,0))\n    image = (image * 255).astype(np.uint8)\n    return image\n\ndef plot_img(data,idx):\n    out = data.__getitem__(idx)\n    image = image_convert(out[0])\n    image = np.ascontiguousarray(image)\n    bb = out[1]['boxes'].numpy()\n    for i in bb:\n        cv2.rectangle(image, (i[0],i[1]), (i[2],i[3]), (0,255,0), thickness=2)\n    plt.figure(figsize=(10,10))\n    plt.imshow(image)","0b624c24":"plot_img(train_data,101)","b0ffc21e":"plot_img(train_data,22)","c2838266":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","e2d82293":"num_classes = 2  # 1 class (wheat) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","26dd5e2e":"images, targets, ids = next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]","cba25c0d":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# optimizer = torch.optim.Adam(params, lr=0.001)\n# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","4be5de1b":"# helper functions to save best model\n\ndef save_ckp(state, is_best, checkpoint_path, best_model_path):\n    \"\"\"\n    state: checkpoint we want to save\n    is_best: is this the best checkpoint; min validation loss\n    checkpoint_path: path to save checkpoint\n    best_model_path: path to save best model\n    \"\"\"\n    # save checkpoint data to the path given, checkpoint_path\n    torch.save(state, checkpoint_path)\n    # if it is a best model, min validation loss\n    if is_best:\n        # copy that checkpoint file to best path given, best_model_path\n        shutil.copyfile(checkpoint_path, best_model_path)\n        \ndef load_ckp(checkpoint_fpath, model, optimizer):\n    \"\"\"\n    checkpoint_path: path to save checkpoint\n    model: model that we want to load checkpoint parameters into       \n    optimizer: optimizer we defined in previous training\n    \"\"\"\n    # load check point\n    checkpoint = torch.load(checkpoint_fpath)\n    # initialize state_dict from checkpoint to model\n    model.load_state_dict(checkpoint['state_dict'])\n    # initialize optimizer from checkpoint to optimizer\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    # initialize valid_loss_min from checkpoint to valid_loss_min\n    valid_loss_min = checkpoint['valid_loss_min']\n    # return model, optimizer, epoch value, min validation loss \n    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()","60af47ab":"num_epochs = 5\ntrain_loss_min = 0.9\ntotal_train_loss = []\n\n\ncheckpoint_path = '\/kaggle\/working\/chkpoint_'\nbest_model_path = '\/kaggle\/working\/bestmodel_may12.pt'\n\nfor epoch in range(num_epochs):\n    print(f'Epoch :{epoch + 1}')\n    start_time = time.time()\n    train_loss = []\n    model.train()\n    for images, targets, image_ids in train_data_loader:\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        train_loss.append(losses.item())        \n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n    #train_loss\/len(train_data_loader.dataset)\n    epoch_train_loss = np.mean(train_loss)\n    total_train_loss.append(epoch_train_loss)\n    print(f'Epoch train loss is {epoch_train_loss}')\n    \n#     if lr_scheduler is not None:\n#         lr_scheduler.step()\n    \n    # create checkpoint variable and add important data\n    checkpoint = {\n            'epoch': epoch + 1,\n            'train_loss_min': epoch_train_loss,\n            'state_dict': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }\n    \n    # save checkpoint\n    save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n    ## TODO: save the model if validation loss has decreased\n    if epoch_train_loss <= train_loss_min:\n            print('Train loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(train_loss_min,epoch_train_loss))\n            # save checkpoint as best model\n            save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n            train_loss_min = epoch_train_loss\n    \n    time_elapsed = time.time() - start_time\n    print('{:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))","4067b2cd":"plt.title('Train Loss')\nplt.plot(total_train_loss)\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.show()","ccb06527":"### Training","36f51126":"### Helper functions for image convertion and visualization**","e9f424d3":"### Helper function for augmentation","0659915f":"### Basic EDA","967a5881":"Now we will load the training data and validation data","aea920f5":"* This is a starter kernel using pytorch.\n* We used Faster Rcnn for bounding box detection of wheat heads.\n* Incoperated data augmentation for better results","cd105ca7":"### Splitting to train and validation data","5cb2ae64":"### Dataset Loader function","fb4f07cb":"Inorder to achieve better results, some usefull approaches can be:\n* Introducing more augmentations\n* Hyper parameter tuning\n* Use different backbone\n\n\nYou can refer my [inference kernel](https:\/\/www.kaggle.com\/arunmohan003\/inferance-kernel-fasterrcnn) and [psudo labelling kernel](https:\/\/www.kaggle.com\/arunmohan003\/faster-rcnn-with-psudo-labeling-pytorch) for further steps.\n\n###  **If you find this kernel useful please support with an upvote!**\n","5ded072d":"## Loading the model"}}