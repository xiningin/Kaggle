{"cell_type":{"938ce4f9":"code","ec18e926":"code","4592e4d7":"code","45c234cd":"code","14196b07":"code","cc61fdb6":"code","45bff632":"code","e913b566":"code","3ab9d1e6":"code","d9e7aa46":"code","4fa095cd":"code","ee756af8":"code","1976a97a":"code","a4c5ad24":"code","5b899ec1":"code","6efbae13":"code","d0cdf33a":"code","392e3810":"code","14eb192e":"code","5f28a265":"code","770e24ca":"code","4eccac4e":"code","b22abc73":"code","72454391":"code","9fd4c6ba":"code","fba75d21":"code","cf969e1a":"code","1e609142":"code","39bc1224":"code","0a274ede":"code","bfe9f065":"code","04497ce4":"code","2d4a8e5d":"code","a62d5428":"code","890b32fe":"code","0ebfe2b4":"code","b0d3d4bb":"code","df04df3f":"code","0baa57ec":"code","06fbe237":"code","b3b8ee17":"code","49859991":"code","048b3fee":"code","d7e82be7":"code","bad8c80a":"code","6af961f3":"code","d73e2af3":"code","13afe0d3":"code","a16d5b37":"code","b361a2c2":"code","229d5442":"code","fc371ed9":"code","9ca3163e":"code","c5a22d69":"code","20fb1a36":"code","3306dc61":"code","002c384f":"code","a7005a56":"code","9035e97a":"code","632dc4db":"code","9bb3c9f3":"code","80be1efd":"code","01f933a5":"code","fe0953cf":"code","c1eb2372":"code","b58bf133":"code","28480b55":"code","ec57d717":"code","b7f0d381":"code","92f37971":"code","0d16fb60":"code","02548c56":"code","eec565c8":"markdown","19e74a27":"markdown","53b12b41":"markdown","893a210e":"markdown","a8aac135":"markdown","a2d4c2fc":"markdown","df850219":"markdown","d4d90772":"markdown","254b46e2":"markdown","6bc6b21f":"markdown","d1ce157e":"markdown","906f2dcc":"markdown","d11f5540":"markdown","9554d7b9":"markdown","1e442695":"markdown","6819c9a0":"markdown","e9c8ba4f":"markdown","6881db99":"markdown","0f35ae9c":"markdown","daa1387f":"markdown","5361ad01":"markdown","a86f95e5":"markdown","c8649a3a":"markdown","c9ccdd6c":"markdown","6da864fd":"markdown","64dc5b4b":"markdown","1089326a":"markdown","29132acd":"markdown","c289df3d":"markdown","c161fea9":"markdown","29f16fb6":"markdown","c2e9927b":"markdown","8835db63":"markdown","6bde8ee9":"markdown","5c71b238":"markdown","5aee2328":"markdown","f912565b":"markdown","3806f175":"markdown","12588964":"markdown","bc621463":"markdown","6831cca8":"markdown","8e5a5b44":"markdown","15da720a":"markdown","d31e57f6":"markdown"},"source":{"938ce4f9":"import tensorflow\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n#from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.vgg16 import VGG16\n\nimport os\n#from tensorflow.python.keras.applications.nasnet import preprocess_input\nfrom tqdm import tqdm","ec18e926":"#model_prefix = 'MobileNetV2'\nmodel_prefix = 'VGG16'\n\nBATCH_SIZE = 32\n\nSIZE = 224 # MobileNetV2, ResNet50V2, MobileNet, RegNetX002, MobileNetV3Large, MobileNetV3Small, EfficientNetB7, DenseNet121, VGG16\n#SIZE = 331 # NASNetLarge\n#SIZE = 299 # InceptionResNetV2, InceptionV3, Xception\n\n# For mean-std measurements -> KFOLD = True\n# For model final testing -> KFOLD = False\nKFOLD = False\n\nEPOCHS = 30\nEPOCHS_KFOLD = 30\nEPOCHS_TEST = 1000","4592e4d7":"models_dir = '.\/MODELS\/'\ndir_root = '..\/input\/lyme-clean-and-dirty\/Lyme_ver03\/'\ndir_original = dir_root + 'data\/'\ndir_augmented = dir_root + 'augmented_dataset\/'\ntest_df = pd.read_csv(dir_root + 'test_data.csv')\ntrain_df = pd.read_csv(dir_root + 'training_data.csv')\naugmented_df = pd.read_csv(dir_root + 'data_augmented.csv')","45c234cd":"\"\"\"\nmodels_dir = '\/data1\/LYME\/MODELS\/'\ndir_root = '\/data1\/LYME\/ver_03\/'\ndir_original = dir_root + 'data\/'\ndir_augmented = dir_root + 'augmented_dataset\/'\ntest_df = pd.read_csv(dir_root + 'test_data.csv')\ntrain_df = pd.read_csv(dir_root + 'training_data.csv')\naugmented_df = pd.read_csv(dir_root + 'data_augmented.csv')\n\"\"\"","14196b07":"%%time\n\ndata = {}\n\ntarget_size = (SIZE, SIZE)\n\n\nfor i in range(len(test_df['image'])):\n    image_name = dir_original + test_df['image'][i]\n    image=tf.keras.preprocessing.image.load_img(image_name, color_mode='rgb', target_size = target_size)\n    image=np.array(image)\n    data.update({test_df['image'][i]: image})\n    \n\nfor i in range(len(train_df['image'])):\n    image_name = dir_original + train_df['image'][i]\n    image=tf.keras.preprocessing.image.load_img(image_name, color_mode='rgb', target_size = target_size)\n    image=np.array(image)\n    data.update({train_df['image'][i]: image})\n\n\nfor i in range(len(augmented_df['image'])):\n    image_name = dir_augmented + augmented_df['image'][i]\n    image=tf.keras.preprocessing.image.load_img(image_name, color_mode='rgb', target_size = target_size)\n    image=np.array(image)\n    data.update({augmented_df['image'][i]: image})\n\n\n","cc61fdb6":"print('train_df size: ',len(train_df))\nprint('test_df size: ',len(test_df))\n#data_df = pd.concat([test_df, train_df])\ndata_df = train_df\n\ndata_df.reset_index(drop=True, inplace=True)\ndata_df = data_df.sample(frac=1, random_state=123)\ndata_df.reset_index(drop=True, inplace=True)\n\nprint('data_original_df size: ',len(data_df))","45bff632":"def show_model_results(acc_per_fold, loss_per_fold, auc_per_fold):\n    # == Provide average scores ==\n    print('------------------------------------------------------------------------')\n    print('Score per fold')\n    for i in range(0, len(acc_per_fold)):\n      print('------------------------------------------------------------------------')\n      print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]} - AUC: {auc_per_fold[i]}')\n    print('------------------------------------------------------------------------')\n    print('Average scores for all folds:')\n    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n    print(f'> AUC: {np.mean(auc_per_fold)} (+- {np.std(auc_per_fold)})')\n    print(f'> Loss: {np.mean(loss_per_fold)} (+- {np.std(loss_per_fold)})')","e913b566":"def plot_mean_std(label, line_color, fill_color, epoch_list, mean_list, std_list, ax):\n  \n  ax.plot(\n    epoch_list,\n    mean_list,\n    color=line_color,\n    label=label, \n    lw=2,\n    alpha=0.8,\n  )\n\n  upper = (mean_list + std_list)\n  lower = (mean_list - std_list)\n\n  ax.fill_between(\n    epoch_list,\n    lower,\n    upper,\n    color=fill_color,\n    alpha=0.5,\n    label=r\"$\\pm$ 1 std. dev.\",\n  )\n\n\ndef plot_train_val_mean_std(metric_label, legend_location, metric_ylim, history_train_list_means, history_train_list_stds, history_val_list_means, history_val_list_stds, ax):\n\n\n  epochs_list = range(EPOCHS)\n\n  plot_mean_std('train', 'r', \"lightcoral\", epochs_list, history_train_list_means, history_train_list_stds, ax)\n  plot_mean_std('val', 'b', \"lightsteelblue\", epochs_list, history_val_list_means, history_val_list_stds, ax)\n\n  ax.set(\n    xlim = [0, EPOCHS-1],\n    ylim = metric_ylim,\n    title = metric_label + \" - History\",\n  )\n\n  ax.legend(loc=legend_location, ncol=2)\n\n  model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name    \n  figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n  os.makedirs(figures_dir, exist_ok=True)    \n  fig_filename = model_name + '_' + metric_label + '_history_' + add_title + '.png'\n  fig_file_path = os.path.join(figures_dir, fig_filename)\n  plt.savefig(fig_file_path, dpi=300)\n\n  plt.show()  \n","3ab9d1e6":"class Base_Model(tf.keras.Model):\n\n  def __init__(self, target_size):\n    super().__init__()\n    self.target_size = target_size\n    self.base_model = VGG16(\n    #self.base_model = MobileNetV2(\n        include_top=False,\n        pooling='max', \n        weights=WEIGHTS, \n        input_shape = self.target_size)\n    \n    print(target_size)\n\n    # make the weights and biases of the base model non-trainable\n    # by \"freezing\" each layer of the BASE network\n    for layer in self.layers:\n        print(layer.name)\n        layer.trainable = TRAINABLE    \n    \n    self.flat_layer = tf.keras.layers.Flatten()\n    self.dense1_layer = tf.keras.layers.Dense(512, activation='relu')\n    self.dense2_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n\n  def call(self, inputs, training=False):\n    x = self.base_model(inputs)\n    x = self.flat_layer(x)\n    x = self.dense1_layer(x)\n\n\n    return self.dense2_layer(x)\n    ","d9e7aa46":"class Complex_Model():\n\n  def __init__(self, result_data, result_label, test_data, test_label, \n               save_dir, save_best_model_path, epoch_num, \n               target_size = (SIZE, SIZE, 3), num_folds = 5):\n    \n    self.X = result_data\n    self.Y = result_label\n    self.x = test_data\n    self.y = test_label\n\n    self.target_size = target_size\n    self.save_dir = save_dir\n    self.save_best_model_path = save_best_model_path\n    self.epoch_num = epoch_num\n    self.num_folds = num_folds\n\n    self.acc_per_fold = []\n    self.loss_per_fold = []\n    self.auc_per_fold = []\n\n    self.history_acc_list = []\n    self.history_loss_list = []\n    self.history_auc_list = []\n    self.history_val_acc_list = []\n    self.history_val_loss_list = []\n    self.history_val_auc_list = []\n\n\n\n  def run(self):\n\n    kfold = StratifiedKFold(n_splits = self.num_folds)\n  \n\n    from tqdm import tqdm\n    for train, test in tqdm(kfold.split(self.X, self.Y)):\n\n      model = Base_Model(self.target_size)\n      #model = create_model()\n\n      #model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001), \n      model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), \n                  loss = 'binary_crossentropy', \n                  metrics = [tf.keras.metrics.AUC(name='auc'),'acc'])\n\n\n      if DA == True:  \n        train_data_aug = ImageDataGenerator(\n          rescale=1.\/255,\n          featurewise_center=True, \n          samplewise_center=True, featurewise_std_normalization=True, samplewise_std_normalization=True,\n          zca_whitening=True, zca_epsilon=1e-06, rotation_range=90, \n          width_shift_range=1.0, height_shift_range=1.0, brightness_range=[0.5, 1.5], \n          shear_range=90.0, zoom_range=1.0, channel_shift_range=0.0, \n          fill_mode='nearest', horizontal_flip=False, vertical_flip=False\n        ).flow(self.X[train], self.Y[train])\n      else:\n        train_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.X[train], self.Y[train])\n    \n      test_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.X[test], self.Y[test])\n\n      checkpoint_AUC = ModelCheckpoint(save_dir, monitor='val_auc', verbose=2, save_best_only=True, \n                                      mode='max', save_weights_only=True)\n      \n      history = model.fit(\n        train_data_aug,\n        batch_size=BATCH_SIZE,\n        epochs=self.epoch_num,\n        verbose=2,\n        callbacks = [checkpoint_AUC],\n        validation_data=test_data_aug)\n        \n      model.summary()        \n      \n      model.load_weights(save_dir)\n      \n      scores = model.evaluate(test_data_aug, verbose=0)\n\n      self.loss_per_fold.append(scores[0])\n      self.auc_per_fold.append(scores[1])\n      self.acc_per_fold.append(scores[2])\n\n      \n      self.history_acc_list.append(history.history['acc'])\n      self.history_loss_list.append(history.history['loss'])\n      self.history_auc_list.append(history.history['auc'])\n      self.history_val_acc_list.append(history.history['val_acc'])\n      self.history_val_loss_list.append(history.history['val_loss'])\n      self.history_val_auc_list.append(history.history['val_auc'])\n\n\n  def run_full(self):\n    model = Base_Model(self.target_size)\n    #model = create_model()\n    \n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001), \n                  loss = 'binary_crossentropy', \n                  metrics = [tf.keras.metrics.AUC(name='auc'),'acc'])\n\n    if DA == True:  \n        train_data_aug = ImageDataGenerator(\n          rescale=1.\/255,\n          featurewise_center=True, \n          samplewise_center=True, featurewise_std_normalization=True, samplewise_std_normalization=True,\n          zca_whitening=True, zca_epsilon=1e-06, rotation_range=90, \n          width_shift_range=1.0, height_shift_range=1.0, brightness_range=[0.5, 1.5], \n          shear_range=90.0, zoom_range=1.0, channel_shift_range=0.0, \n          fill_mode='nearest', horizontal_flip=False, vertical_flip=False\n        ).flow(self.X, self.Y)\n    else:\n        train_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.X, self.Y)\n    \n    test_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.x, self.y)\n\n    checkpoint_AUC = ModelCheckpoint(save_dir, monitor='val_auc', verbose=2, save_best_only=True, \n                                      mode='max', save_weights_only=True)\n      \n    history = model.fit(\n        train_data_aug,\n        batch_size=BATCH_SIZE,\n        epochs=self.epoch_num,\n        verbose=2,\n        callbacks = [checkpoint_AUC],\n        validation_data=test_data_aug)\n        \n    model.summary()        \n      \n    model.load_weights(save_dir)\n      \n    scores = model.evaluate(test_data_aug, verbose=0)\n\n    print('**** Test BEST model ****')\n    Accuracy_string = 'Acc{acc_best:.3f}'.format(acc_best=scores[2])\n    print('Accuracy = {acc_best:.4f}'.format(acc_best=scores[2]))\n    AUC_string = 'AUC{AUC_best:.3f}'.format(AUC_best=scores[1])\n    print('AUC= {AUC_best:.4f}'.format(AUC_best=scores[1]))\n    Loss_string = 'Loss{Loss_best:.3f}'.format(Loss_best=scores[0])\n    print('Loss = {Loss_best:.4f}'.format(Loss_best=scores[0]))\n    \n    best_model_metrics_filename = self.save_best_model_path + '_' + Loss_string + '_' + Accuracy_string + '_' + AUC_string  + '_best.h5'\n    model.save_weights(best_model_metrics_filename) \n    \n    return model, best_model_metrics_filename, self.x, self.y\n    \n    \n  def show_results(self):\n    show_model_results(self.acc_per_fold, self.loss_per_fold, self.auc_per_fold)\n\n\n  def history_stat_metrics(self, history_list):\n    history_list_means = np.mean(np.asarray(history_list),axis=0)\n    history_list_stds = np.std(np.asarray(history_list),axis=0)\n    return(history_list_means, history_list_stds)\n\n\n  def plot_accuracy(self):\n\n    train_acc_list_means, train_acc_list_stds = self.history_stat_metrics(self.history_acc_list)\n    val_acc_list_means, val_acc_list_stds = self.history_stat_metrics(self.history_val_acc_list)\n\n    fig, ax = plt.subplots()\n\n    plot_train_val_mean_std('Accuracy', \"lower center\", [0.01, 1.05], train_acc_list_means, \n                        train_acc_list_stds, val_acc_list_means, val_acc_list_stds, ax)\n    \n   \n  def plot_auc(self):\n\n    train_auc_list_means, train_auc_list_stds = self.history_stat_metrics(self.history_auc_list)\n    val_auc_list_means, val_auc_list_stds = self.history_stat_metrics(self.history_val_auc_list)\n\n    fig, ax = plt.subplots()\n\n    plot_train_val_mean_std('AUC', \"lower center\", [0.01, 1.05], train_auc_list_means, train_auc_list_stds, \n                        val_auc_list_means, val_auc_list_stds, ax)\n\n\n\n  def plot_loss(self):\n\n    train_loss_list_means, train_loss_list_stds = self.history_stat_metrics(self.history_loss_list)\n    val_loss_list_means, val_loss_list_stds = self.history_stat_metrics(self.history_val_loss_list)\n\n    fig, ax = plt.subplots()\n\n    plot_train_val_mean_std('Loss', \"upper center\", [0.01, 6], train_loss_list_means, train_loss_list_stds, \n                        val_loss_list_means, val_loss_list_stds, ax) \n\n\n  def folds_mean_max_auc(self):\n    return np.mean(self.acc_per_fold), np.std(self.acc_per_fold), np.mean(self.auc_per_fold), np.std(self.auc_per_fold), np.mean(self.loss_per_fold), np.std(self.loss_per_fold)  ","4fa095cd":"def prepare_data_growing(data_df, noisy_part, limit):\n    result_label = []\n    result_data = []\n\n    dirty_pos_lyme_count = 0\n    dirty_no_lyme_count = 0\n\n    clean_pos_lyme_count = 0\n    clean_no_lyme_count = 0\n\n    clean_count = 0\n    dirty_count = 0\n    \n    dirty_limit = limit * noisy_part\n    print('dirty_limit: ', dirty_limit)\n    clean_limit = limit - dirty_limit\n    print('clean_limit: ', clean_limit)\n    \n    num_images = len(data_df['image'])\n    print('**************************')\n    print('Subset (images): ', num_images)\n    for i in range(num_images):\n        if (data_df['clean'][i] == 1) and (data_df['lyme_positive'][i] == 1) and (clean_pos_lyme_count < clean_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            clean_pos_lyme_count += 1\n            clean_count += 1\n\n        if (data_df['clean'][i] == 1) and (data_df['lyme_positive'][i] == 0) and (clean_no_lyme_count < clean_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            clean_no_lyme_count += 1\n            clean_count += 1\n       \n        if (data_df['clean'][i] == 0) and (data_df['lyme_positive'][i] == 1) and (dirty_pos_lyme_count < dirty_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            dirty_pos_lyme_count += 1\n            dirty_count += 1\n\n        if (data_df['clean'][i] == 0) and (data_df['lyme_positive'][i] == 0) and (dirty_no_lyme_count < dirty_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            dirty_no_lyme_count += 1       \n            dirty_count += 1       \n\n    print('dirty_pos_lyme_count:', dirty_pos_lyme_count)            \n    print('dirty_no_lyme_count:', dirty_no_lyme_count)\n    print('dirty_count:', dirty_count)\n\n    print('clean_pos_lyme_count:', clean_pos_lyme_count)            \n    print('clean_no_lyme_count:', clean_no_lyme_count)    \n    print('clean_count:', clean_count)\n  \n    result_data = np.asarray(result_data).astype(np.float32)\n    result_label = np.asarray(result_label).astype(np.float32)\n\n    print('**************************')\n    print('Final state of subset ...')\n    print('pos_lyme_count: ', dirty_pos_lyme_count + clean_pos_lyme_count)\n    print('no_lyme_count: ', dirty_no_lyme_count + clean_no_lyme_count)\n    print('Noise ratio: ', dirty_count\/(dirty_count + clean_count))\n    print('Growing subset (data) size: ',len(result_data))\n    print('Growing subset (label) size: ',len(result_label))\n    print('**************************')\n\n    return result_data, result_label","ee756af8":"def plot_noise_vs(x, y, x_label, y_label, plot_title):\n  plt.plot(x, y)\n  #plt.xticks(x)\n  plt.xlabel(x_label)\n  plt.ylabel(y_label)\n  plt.title(plot_title)\n\n  model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name    \n  figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n  os.makedirs(figures_dir, exist_ok=True)    \n  fig_filename = model_name + '_' + plot_title + '.png'\n  fig_file_path = os.path.join(figures_dir, fig_filename)\n  plt.savefig(fig_file_path, dpi=300)\n\n  plt.show()","1976a97a":"def plot_noise_mean_std(metric_label, legend_location, metric_ylim, x_list, list_means, list_stds, ax):\n\n    plot_mean_std('Mean', 'b', \"lightsteelblue\", x_list, list_means, list_stds, ax)\n\n    ax.set(\n        xlim = [0, len(x_list)+5],\n        ylim = metric_ylim,\n        title = metric_label + \" vs \" + x_label,\n        xlabel = x_label,\n        ylabel = metric_label\n    )\n\n    ax.legend(loc=legend_location, ncol=2)\n\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name    \n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)    \n    fig_filename = model_name + '_' + metric_label + \"_vs_\" + x_label_filename + '_FILL.png'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, dpi=300)\n\n    plt.show()  ","a4c5ad24":"def run_kfold():  \n    #save_dir = '.\/MODELS\/best'\n    #save_dir = os.path.dirname(checkpoint_path)\n    #os.makedirs(save_dir, exist_ok=True)\n    \n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    save_model_filename = models_dir + model_prefix + '\/' + model_name\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    \n    #num_list = [15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n    num_list = list(range(5,46,1))\n\n    metrics_list = []\n    for n in tqdm(num_list):\n        result_data, result_label = prepare_data_growing(train_df, NOISE_PART, n)\n        add_title = 'noise' + str(NOISE_PART) + '_num' + str(n)\n        model = Complex_Model(result_data=result_data, result_label=result_label, \n                              test_data=[], test_label=[],\n                              save_dir=save_dir, save_best_model_path=save_model_filename, epoch_num = EPOCHS)\n    \n        model.run()\n        model.show_results()\n        model.plot_accuracy()\n        model.plot_loss()\n        model.plot_auc()\n    \n        metrics = model.folds_mean_max_auc()\n        metrics_list.append(metrics)   \n\n    acc_mean_list = []\n    acc_std_list = []\n    auc_mean_list = []\n    auc_std_list = []\n    loss_mean_list = []\n    loss_std_list = []\n    for n in range(len(num_list)):\n        acc_mean_list.append(metrics_list[n][0])\n        acc_std_list.append(metrics_list[n][1])\n        auc_mean_list.append(metrics_list[n][2])\n        auc_std_list.append(metrics_list[n][3])\n        loss_mean_list.append(metrics_list[n][4])\n        loss_std_list.append(metrics_list[n][5])    \n        \n    # Summary\n    summary = np.column_stack((np.array(num_list),acc_mean_list,acc_std_list,auc_mean_list,auc_std_list,loss_mean_list,loss_std_list))\n    summary_df = pd.DataFrame(summary)\n    summary_df.columns = ['num', 'acc_mean', 'acc_std', 'auc_mean', 'auc_std', 'loss_mean', 'loss_std']\n    summary_df.head()\n    summary_filename = model_name + '_summary.txt'\n    summary_file_path = os.path.join(figures_dir, summary_filename)\n    summary_df.to_csv(summary_file_path, index=False)\n        \n    plot_noise_vs(num_list, acc_mean_list, x_label, 'Accuracy (mean)', 'Accuracy_mean_vs_' + x_label_filename)\n    plot_noise_vs(num_list, acc_std_list, x_label, 'Accuracy (std)', 'Accuracy_std_vs_' + x_label_filename)\n    plot_noise_vs(num_list, auc_mean_list, x_label, 'AUC (mean)', 'AUC_mean_vs_' + x_label_filename)\n    plot_noise_vs(num_list, auc_std_list, x_label, 'AUC (std)', 'AUC_std_vs_' + x_label_filename)\n    plot_noise_vs(num_list, loss_mean_list, x_label, 'Loss (mean)', 'Loss_mean_vs_' + x_label_filename)\n    plot_noise_vs(num_list, loss_std_list, x_label, 'Loss (std)', 'Loss_std_vs_' + x_label_filename)\n    \n    fig, ax = plt.subplots()\n    plot_noise_mean_std('Accuracy', \"lower center\", [0.3, 0.9], num_list, np.array(acc_mean_list), np.array(acc_std_list), ax)   \n    fig, ax = plt.subplots()\n    plot_noise_mean_std('AUC', \"upper center\", [0.5, 1.1], num_list, np.array(auc_mean_list), np.array(auc_std_list), ax)    \n    fig, ax = plt.subplots()\n    plot_noise_mean_std('Loss', \"upper center\", [-0.2, 3.6], num_list, np.array(loss_mean_list), np.array(loss_std_list), ax)    ","5b899ec1":"def run_final(test_data, test_label):  \n    \n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    save_model_filename = models_dir + model_prefix + '\/' + model_name\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    \n    print('**************************')\n    print('Training subset ...')\n    train_data, train_label = prepare_data_growing(train_df, NOISE_PART, 45)\n    print('**************************')    \n    print('Testing subset ...')\n    print('For all testing procedures use the same: test_data, test_label')    \n    #test_data, test_label = prepare_data_growing(test_df, NOISE_PART, 20)\n    \n    add_title = 'noise' + str(NOISE_PART) + '_num' + str(45)\n    model = Complex_Model(result_data=train_data, result_label=train_label, \n                          test_data=test_data, test_label=test_label,\n                          save_dir=save_dir, save_best_model_path=save_model_filename, \n                          epoch_num = EPOCHS)\n    \n    model, best_model_metrics_filename, test_data, test_label = model.run_full()\n        \n    return model, best_model_metrics_filename","6efbae13":"import itertools\ndef plot_confusion_matrix(cm, \n                          #cm_std, # for k-fold only\n                          classes,\n                          normalize=False,\n                          title='Means',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm_sum = cm.sum(axis=1)[:, np.newaxis]\n        cm = cm.astype('float') \/ cm_sum\n#        cm_std = cm_std.astype('float') \/ cm_sum # for k-fold only\n        print(\"Confusion matrix, NORMALIZED\")\n    else:\n        print('Confusion matrix, WITHOUT normalization')\n\n    print('Mean values:')\n    print(cm)\n\n#    print('Standard deviation values:') # for k-fold only\n#    print(cm_std)\n\n    plt.title(title)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, fontsize = 8)\n    plt.yticks(tick_marks, classes, fontsize = 8)\n\n    fmt = '.2f' if normalize else '.0f' # 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 #+ \"\\n\" + r'$\\pm$' + format(cm_std[i, j], fmt), # for k-fold only\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 7)\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)    \n    \n    plt.title('')     \n    fig_filename = title + '_ConfusionMatrix.eps'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)\n    \n    plt.title(title)\n    fig_filename = title + '_ConfusionMatrix.png'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)","d0cdf33a":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\ndef test_model(model, best_model_metrics_filename, test_data, test_label):\n#def test_model(model, best_model_metrics_filename, test_data_aug, test_label):\n    \n    model.load_weights(best_model_metrics_filename)\n    \n    test_data_aug = ImageDataGenerator(rescale=1.\/255).flow(test_data, test_label)\n    scores = model.evaluate(test_data_aug, verbose=1)\n    \n    print('**** EXTENDED TEST *****')\n    print(' Scores for TEST subset:')\n    Loss_string = 'Loss{Loss_best:.3f}'.format(Loss_best=scores[0])\n    print('Loss = {Loss_best:.4f}'.format(Loss_best=scores[0]))\n    AUC_string = 'AUC{AUC_best:.3f}'.format(AUC_best=scores[1])\n    print('AUC= {AUC_best:.4f}'.format(AUC_best=scores[1]))\n    Accuracy_string = 'Acc{acc_best:.3f}'.format(acc_best=scores[2])\n    print('Accuracy = {acc_best:.4f}'.format(acc_best=scores[2]))    \n\n    # CM\n    print('**** CONFUSION MATRIX *****')\n    #x_test, y_test = test_data, test_label\n\n    #score = model.evaluate(x_test, y_test)\n    #print('score:', score)\n    #predicted_probs = model.predict(x_test)\n    #print('predicted_probs:', predicted_probs)\n    \n    #samples = test_data_aug.samples\n    #nb_samples = len(samples)\n    #predicted_probs = model.predict_generator(test_data_aug, steps = np.ceil(nb_samples\/32))\n    predicted_probs = model.predict(test_data\/255)\n    #print('predicted_probs:',predicted_probs)\n\n    df = pd.DataFrame(predicted_probs)\n    predicted_probs = df[0].values.tolist()\n    predicted = [round(x) for x in predicted_probs]\n    #predicted = predicted_probs #.argmax(axis=-1)\n    #print('predicted:', predicted)\n\n    expected = test_label\n    #expected = y_test\n    #expected = y_test.argmax(axis=-1)\n    #print('expected:', expected)\n\n    # Print test of confusion matrix\n    conf_matrix = confusion_matrix(expected, predicted)\n    print(conf_matrix)      \n    \n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)\n    title = model_name\n    print('title: ', title)\n\n    BINARY = True\n\n    #class_names = ['AZU','ONU', 'IZU', 'MYSLITE']\n    #class_names = subfolder_names\n    class_names = ['Lyme','NO']\n\n    # Compute confusion matrix\n    #cnf_matrix = confusion_matrix(expected, predicted)\n    np.set_printoptions(precision=4)\n\n    print('**************************')\n    # Plot non-normalized confusion matrix\n    if BINARY == True:\n        fig, ax = plt.subplots(dpi=300, figsize=(6, 3)) \n    else: \n        fig, ax = plt.subplots(dpi=300, figsize=(15, 15))\n    \n    plot_confusion_matrix(conf_matrix, \n                      #confusion_matrix_stds, # for k-fold only\n                      classes=class_names, normalize=False,\n                      #title = MODEL_NAME + '_' + TRAIN_TITLE + '_' + DA_TYPE + '_' + SAVED_MODEL + '_NOT_normalized')\n                      title = title + '_NOT_normalized')\n\n    print('**************************')\n    # Plot normalized confusion matrix\n    if BINARY == True:\n        fig, ax = plt.subplots(dpi=300, figsize=(6, 3)) \n    else: \n        fig, ax = plt.subplots(dpi=300, figsize=(15, 15))\n    plot_confusion_matrix(conf_matrix, \n                      #confusion_matrix_stds, # for k-fold only\n                      classes=class_names, normalize=True,\n                      #title = MODEL_NAME + '_' + TRAIN_TITLE + '_' + DA_TYPE + '_' + SAVED_MODEL + '_normalized')\n                      title = title + '_normalized')\n\n    plt.show()   \n    \n    TP = conf_matrix[0][0]\n    #print(TP)\n    FN = conf_matrix[0][1]\n    FP = conf_matrix[1][0]\n    TN = conf_matrix[1][1]    \n    \n    # ROC\n    from sklearn.metrics import roc_curve, auc\n\n    #fpr = dict()\n    #tpr = dict()\n    #roc_auc = dict()\n\n    fpr, tpr, _ = roc_curve(expected, predicted_probs)\n    roc_auc = auc(fpr, tpr)\n    #roc_auc_1 = roc_auc_score(expected, predicted_probs)\n\n    fig, ax = plt.subplots(figsize=(5, 5)) \n\n    ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Random, 0.5\", alpha=0.8)\n    ax.plot(fpr, tpr,\n         #label = \"AUC, %0.4f; , %0.4f\" % (roc_auc,roc_auc_1),\n         label = \"AUC, %0.4f\" % (roc_auc),\n         color='navy', linestyle='-', linewidth=4)\n\n    ax.legend(loc=\"lower right\", title = 'AUC')\n\n    plt.title('')     \n    fig_filename = title + '_ROC.eps'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)\n    \n    plt.title(title)\n    fig_filename = title + '_ROC.png'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)\n\n    plt.show()   \n    \n    # Save TEST results\n    \n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)\n\n    test_CM_AUC_filename = model_name + '_CM_AUC.csv'\n    print('test_CM_AUC_filename: ', test_CM_AUC_filename)\n    test_CM_AUC_file_path = os.path.join(figures_dir, test_CM_AUC_filename)\n\n    test_ROC_filename = model_name + '_ROC.csv'\n    print('test_results_filename: ', test_ROC_filename)\n    test_ROC_file_path = os.path.join(figures_dir, test_ROC_filename)\n\n    roc_df = pd.DataFrame(fpr.T, columns=['fpr'])\n    roc_df['tpr'] = pd.DataFrame(tpr)\n    roc_df.head()\n    roc_df.to_csv(test_ROC_file_path)\n\n    cm_auc_df = pd.DataFrame(columns=['Loss', 'AUC', 'Accuracy', 'ROC_AUC', 'TP', 'FN', 'FP', 'TN'], \n                         data=[[scores[0], scores[1], scores[2], roc_auc, TP, FN, FP, TN]])\n    cm_auc_df.head()\n    cm_auc_df.to_csv(test_CM_AUC_file_path)    ","392e3810":"def model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name):\n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)\n    return model_name, figures_dir","14eb192e":"# For all testing procedures: use the same test_data, test_label\nprint('Testing subset ...')\nNOISE_PART = 0.5\ntest_data, test_label = prepare_data_growing(test_df, NOISE_PART, 20)\ntest_data_aug = ImageDataGenerator(rescale=1.\/255).flow(test_data, test_label)","5f28a265":"# Train from ImageNet-trainable Weights\nWEIGHTS = 'imagenet'\nWEIGHTS_name = 'imagenet'\n# Train from scratch\n#WEIGHTS = None\n#WEIGHTS_name = 'Scratch'\n\nadd_title = ''\nx_label = 'Number of images'\nx_label_filename = 'Num'    \n\n#save_dir = ''\nsave_dir = models_dir + model_prefix + '\/'\nmodel_name = ''\nsave_model_filename = ''\nfigures_dir = ''","770e24ca":"# Clean dataset \nNOISE_PART = 0.0\ndata_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","4eccac4e":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\n#TRAINABLE = True\n#TL_name = 'NOTL'\n# Transfer Learning (TL) -> False\nTRAINABLE = False\nTL_name = 'TL'","b22abc73":"%%time\n# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","72454391":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","9fd4c6ba":"%%time\n\n# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","fba75d21":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","cf969e1a":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","1e609142":"%%time\n# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","39bc1224":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","0a274ede":"%%time\n# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","bfe9f065":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","04497ce4":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \nNOISE_PART = 1.0\ndata_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","2d4a8e5d":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\n#TRAINABLE = True\n#TL_name = 'NOTL'\n# Transfer Learning (TL) -> False\nTRAINABLE = False\nTL_name = 'TL'","a62d5428":"%%time\n# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","890b32fe":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","0ebfe2b4":"%%time\n# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","b0d3d4bb":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","df04df3f":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","0baa57ec":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","06fbe237":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","b3b8ee17":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","49859991":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","048b3fee":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \nNOISE_PART = 0.5\ndata_quality = 'DIRTY_CLEAN'","d7e82be7":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\n#TRAINABLE = True\n#TL_name = 'NOTL'\n# Transfer Learning (TL) -> False\nTRAINABLE = False\nTL_name = 'TL'","bad8c80a":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","6af961f3":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","d73e2af3":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","13afe0d3":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","a16d5b37":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","b361a2c2":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","229d5442":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","fc371ed9":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","9ca3163e":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","c5a22d69":"# Train from ImageNet-trainable Weights\n#WEIGHTS = 'imagenet'\n#WEIGHTS_name = 'imagenet'\n# Train from scratch\nWEIGHTS = None\nWEIGHTS_name = 'random'","20fb1a36":"# Clean dataset \nNOISE_PART = 0.0\ndata_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","3306dc61":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","002c384f":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","a7005a56":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","9035e97a":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","632dc4db":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","9bb3c9f3":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \nNOISE_PART = 1.0\ndata_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","80be1efd":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","01f933a5":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","fe0953cf":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","c1eb2372":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","b58bf133":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","28480b55":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \nNOISE_PART = 0.5\ndata_quality = 'DIRTY_CLEAN'","ec57d717":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","b7f0d381":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","92f37971":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","0d16fb60":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","02548c56":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","eec565c8":"#### Regime = CLEAN + NOTL + DA","19e74a27":"# Plot functions","53b12b41":"## Regime = DIRTY","893a210e":"## Load from Local Storage","a8aac135":"#### Regime = CLEAN + NOTL + DA","a2d4c2fc":"#### Regime = DIRTY + CLEAN + NOTL + NODA","df850219":"# Regimes","d4d90772":"### Regime = CLEAN + NOTL","254b46e2":"# Different number of images","6bc6b21f":"#### Regime = DIRTY + NOTL + DA","d1ce157e":"#### Regime = DIRTY + TL + NODA","906f2dcc":"## Regime = DIRTY + CLEAN","d11f5540":"#### Regime = CLEAN + NOTL + NODA","9554d7b9":"#### Regime = DIRTY + NOTL + DA","1e442695":"### Regime = DIRTY + CLEAN + NOTL","6819c9a0":"#### Regime = CLEAN + TL + DA","e9c8ba4f":"### Regime = DIRTY + CLEAN + TL","6881db99":"### Regime = CLEAN + NOTL","0f35ae9c":"## Regime = DIRTY + CLEAN","daa1387f":"#### Regime = DIRTY + NOTL + NODA","5361ad01":"# From random weights","a86f95e5":"#### TEST -> model","c8649a3a":"#### Regime = DIRTY + CLEAN + NOTL + DA","c9ccdd6c":"#### Regime = DIRTY + CLEAN + NOTL + DA","6da864fd":"# Prepare data","64dc5b4b":"## Load from Kaggle","1089326a":"#### Regime = DIRTY + NOTL + NODA","29132acd":"#### Regime = DIRTY + CLEAN + NOTL + NODA","c289df3d":"### Regime = DIRTY + NOTL","c161fea9":"#### Regime = DIRTY + TL + DA","29f16fb6":"# Model_class","c2e9927b":"#### Regime = CLEAN + TL + NODA","8835db63":"## Regime = CLEAN","6bde8ee9":"# Regime = CLEAN","5c71b238":"### Regime = DIRTY + TL","5aee2328":"## Data PreProcessing ","f912565b":"### Regime = DIRTY + CLEAN + NOTL","3806f175":"### Regime = DIRTY + NOTL","12588964":"## Regime = DIRTY","bc621463":"### Regime = CLEAN + TL","6831cca8":"## Basic Parameters","8e5a5b44":"#### Regime = DIRTY + CLEAN + TL + DA","15da720a":"#### Regime = CLEAN + NOTL + NODA","d31e57f6":"#### Regime = DIRTY + CLEAN + TL + NODA"}}