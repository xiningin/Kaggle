{"cell_type":{"b8a4f8c9":"code","fbd6e39e":"code","4f151fcd":"code","0637dbb0":"code","6454f4da":"code","55a43416":"code","308143b9":"code","121513f1":"code","6bf2975b":"code","5d4631d3":"code","24e09f8d":"code","e0732d56":"code","e54575a4":"code","ed6de83b":"code","ea214363":"code","b7607665":"code","30303c3f":"markdown","061c64e7":"markdown","bf6ac1bf":"markdown","dd7ee1b7":"markdown","8bd82c8a":"markdown","e8c2e4f5":"markdown","da997658":"markdown","abbd37ac":"markdown","5a17234f":"markdown","1a0774f1":"markdown","04330ced":"markdown","5f0c6f04":"markdown","0f2e0a14":"markdown","22f0375b":"markdown","b823f2cf":"markdown"},"source":{"b8a4f8c9":"import random\nimport librosa\nimport scipy\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nimport cv2\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\n\n%matplotlib inline","fbd6e39e":"file_path = '..\/input\/birdsong-recognition\/train_audio\/aldfly\/XC134874.mp3'","4f151fcd":"wav, sr = librosa.load(file_path, sr=None)\nprint(wav.shape, wav.max(), wav.min())\nipd.Audio(file_path)","0637dbb0":"def show_spectrogram(wav):\n    plotter = librosa.stft(wav, n_fft=480, hop_length=160,win_length=480, window='hamming')\n    spect, phase = librosa.magphase(plotter)\n    return spect","6454f4da":"log_spect = np.log(show_spectrogram(wav))\nprint('spectrogram shape:', log_spect.shape)\nplt.imshow(log_spect, aspect='auto', origin='lower',)\nplt.title('spectrogram of origin audio')\nplt.show()","55a43416":"start_ = int(np.random.uniform(-18000,18000))\nprint('time shift: ',start_)\nif start_ >= 0:\n    wav_time_shift = np.r_[wav[start_:], np.random.uniform(-0.01,0.01, start_)]\nelse:\n    wav_time_shift = np.r_[np.random.uniform(-0.01,0.01, -start_), wav[:start_]]\nipd.Audio(wav_time_shift, rate=sr)\n","308143b9":"EPS = 1e-8\nlog_spect = np.log(show_spectrogram(wav_time_shift)+EPS)\nprint('spectrogram shape:', log_spect.shape)\nplt.imshow(log_spect, aspect='auto', origin='lower',)\nplt.title('spectrogram of time shifted audio')\nplt.show()","121513f1":"speed_rate = np.random.uniform(0.7,1.3)\nwav_speed_tune = cv2.resize(wav, (1, int(len(wav) * speed_rate))).squeeze()\nprint('speed rate: %.3f' % speed_rate, '(lower is faster)')\nif len(wav_speed_tune) < 1223424:\n    pad_len = 1223424 - len(wav_speed_tune)\n    wav_speed_tune = np.r_[np.random.uniform(-0.001,0.001,int(pad_len\/2)),wav_speed_tune,np.random.uniform(-0.001,0.001,int(np.ceil(pad_len\/2)))]\nelse: \n    cut_len = len(wav_speed_tune) - 1223424\n    wav_speed_tune = wav_speed_tune[int(cut_len\/2):int(cut_len\/2)+1223424]\nprint('wav length: ', wav_speed_tune.shape[0])\nipd.Audio(wav_speed_tune, rate=sr)","6bf2975b":"log_spect = np.log(show_spectrogram(wav_speed_tune)+EPS)\nprint('spectrogram shape:', log_spect.shape)\nplt.imshow(log_spect, aspect='auto', origin='lower',)\nplt.title('spectrogram of speed tuned audio')\nplt.show()","5d4631d3":"audio,sr = torchaudio.load(file_path)\nsample=(audio,sr)\n","24e09f8d":"\ndef tfm_spectro(ad, sr=16000, to_db_scale=False, n_fft=1024, \n                ws=None, hop=None, f_min=0.0, f_max=-80, pad=0, n_mels=128):\n    # We must reshape signal for torchaudio to generate the spectrogram.\n    mel = transforms.MelSpectrogram(sample_rate=ad[1], n_mels=n_mels, n_fft=n_fft, hop_length=hop, \n                                    f_min=f_min, f_max=f_max, pad=pad,)(ad[0].reshape(1, -1))\n    mel = mel.permute(0,2,1) # swap dimension, mostly to look sane to a human.\n    if to_db_scale: mel = transforms.AmplitudeToDB(stype='magnitude', top_db=f_max)(mel)\n    return mel\n\nspectro = tfm_spectro(sample, ws=512, hop=256, n_mels=128, to_db_scale=True, f_max=8000, f_min=-80)","e0732d56":"#displaying\ndef tensor_to_img(spectrogram): \n    plt.imshow(spectrogram[0],aspect='auto', origin='lower')\n    plt.show();\n    display(spectrogram.shape)\ntensor_to_img(spectro)","e54575a4":"def freq_mask(spec, F=250, num_masks=1):\n    test = spec.clone()\n    num_mel_channels = test.shape[1]\n    for i in range(0, num_masks):        \n        freq = random.randrange(0, F)\n        zero = random.randrange(0, num_mel_channels - freq)\n        # avoids randrange error if values are equal and range is empty\n        if (zero == zero + freq): return test\n        mask_end = random.randrange(zero, zero + freq) \n        test[0][zero:mask_end] = test.mean()\n    return test","ed6de83b":"def test_freq_mask():\n    print('Original')\n    tensor_to_img(spectro)\n    print('5 masks')\n    tensor_to_img(freq_mask(spectro, num_masks=5))\ntest_freq_mask()","ea214363":"def time_mask(spec, time=40, num_masks=1):\n    test = spec.clone()\n    length = test.shape[2]\n    for i in range(0, num_masks):\n        t = random.randrange(0, time)\n        zero = random.randrange(0, length - t)\n        if (zero == zero + t): return cloned\n        mask_end = random.randrange(zero, zero + t)\n        test[0][:,zero:mask_end] = test.mean()\n    return test","b7607665":"def test_time_mask():\n    print('One Mask')\n    tensor_to_img(time_mask(spectro))\n    print('Two Mask')\n    tensor_to_img(time_mask(spectro, num_masks=2))\ntest_time_mask()","30303c3f":"loading Audio file via Librosa","061c64e7":"# Speed Rate","bf6ac1bf":"I have many changes left to do this was my first attempt and was inspired by SpecAugment. Please leave a like and any recommendation in comments.","dd7ee1b7":"# Time Shifting  \n> Basic Time shifted according ratio taken by you","8bd82c8a":"**Showing spectogram via log **","e8c2e4f5":"Output","da997658":"# Loading Audio on pytorch","abbd37ac":"Output","5a17234f":"Converting audio into Melspectrogram","1a0774f1":"Output","04330ced":"# Frequency Masking\nApply masking to a spectrogram in the frequency domain.","5f0c6f04":"# Time Masking\nApplying masking in time domain","0f2e0a14":"Output","22f0375b":"# Audio Augmentation \nThis notebook is using Google specaugment techniques for audio augmentation, which was used in speech recognition and some primary audio augmentation, just for experimenting purpose. It is interesting as it is the first time dealing with Audio in deep learning, decided to do some audio augmentation. We have taken a single mp3, but this can be done to the whole dataset.\nAugmentation done :\n* Time Shift\n* Speed Rate  Manipulation\n* Frequency Masking\n* Time Masking","b823f2cf":"Importing Libraries"}}