{"cell_type":{"925bc7b6":"code","40a1ee8d":"code","7632c1fc":"code","ad2b24c2":"code","0fcaffc3":"code","f2aed7d4":"code","e5ed225c":"code","1b9f07a0":"markdown","e2825be2":"markdown","d584fe18":"markdown","34bddd66":"markdown","6b637bdf":"markdown","a7c51da5":"markdown","e400c79d":"markdown","ee9782de":"markdown"},"source":{"925bc7b6":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport seaborn as sns\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import learning_curve\n","40a1ee8d":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.info()","7632c1fc":"train['Title'] = train.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\ntest['Title'] = test.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n# normalize the titles\nnormalized_titles = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"\n}\n\ntrain.Title = train.Title.map(normalized_titles)\ntest.Title = train.Title.map(normalized_titles)\n\ntrain.Title.replace(['Mr', 'Miss', 'Mrs', 'Master', 'Officer', 'Royalty'], [1, 2, 3, 4, 5, 6], inplace=True)\ntest.Title.replace(['Mr', 'Miss', 'Mrs', 'Master', 'Officer', 'Royalty'], [1, 2, 3, 4, 5, 6], inplace=True)","ad2b24c2":"# Drop the name feature\ntrain = train.drop('Name', 1)\ntest = test.drop('Name', 1)\n# Encoding the features\ntrain.Embarked.replace(['S', 'C', 'Q'], [1, 2, 3], inplace=True)\ntrain.Sex.replace(['male', 'female'], [1, 2], inplace=True)\ntest.Embarked.replace(['S', 'C', 'Q'], [1, 2, 3], inplace=True)\ntest.Sex.replace(['male', 'female'], [1, 2], inplace=True)\n\n# Temporary drop of some columns\ntrain = train.drop('Ticket', 1)\ntrain = train.drop('Cabin', 1)\ntest = test.drop('Ticket', 1)\ntest = test.drop('Cabin', 1)\n\n#Fill NaN values of Age with the mean Age\ntrain['Age'] = train['Age'].fillna(train['Age'].sum()\/len(train))\ntest['Age'] = test['Age'].fillna(test['Age'].sum()\/len(test))\n#Drop other raw where there is a NaN value\ntrain= train.dropna(how='any',axis=0)  \ntest = test.fillna(0)\n\ntrain.info()","0fcaffc3":"train","f2aed7d4":"Data_train = train.values\nData_test = test.values\n\n# m = number of input samples\nm_train = len(Data_train)\nm_test = len(Data_test)\n# prediction for training\nYtrain = Data_train[:m_train,1]\n# features for training\nXtrain = Data_train[:m_train,2:]\n# features for testing\nXtest = Data_test[:m_test,1:]\n\nparameters = {'C': [1, 10, 50, 100,200,300, 1000],'gamma':[0.0001,0.001,0.01,0.1,1.]}\n#run SVM with rbf kernel\nrbf_SVM = SVC(kernel='rbf')\n# ADD CODE: DO THE SAME AS ABOVE FOR RBF KERNEL\nclf = GridSearchCV(rbf_SVM,parameters,cv=5)\nclf.fit(Xtrain,Ytrain)\n\nprint ('\\n RESULTS FOR rbf KERNEL \\n')\n\nprint(\"Best parameters set found:\")\nbest_param = clf.best_params_\nvalue_best_param_rbf_gammma = best_param['gamma']\nvalue_best_param_rbf_c = best_param['C']\nestim_best = clf.best_estimator_\nprint(\"Best Estimator: \", estim_best)\n\n#get training and test error for the best SVM model from CV\nbest_SVM = SVC(C = value_best_param_rbf_c, gamma = value_best_param_rbf_gammma, kernel='rbf')\n\nbest_SVM.fit(Xtrain,Ytrain)\n\ntraining_error = 1. - best_SVM.score(Xtrain,Ytrain)\nprint(\"Training error: \", training_error)\n\nYtest_predicted = best_SVM.predict(Xtest)","e5ed225c":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes= np.linspace(.1, 1.0, 10)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\nplot_learning_curve(estim_best,\"SVC learning curves\",Xtrain,Ytrain,cv=5)\nplt.show()","1b9f07a0":"So after some data tuning and adjustments this is the training set:","e2825be2":"Load the training set and the test set, print some info about the training set","d584fe18":"First i decided to adjust some features of my dataset. The problem with this dataset is that it is not plentiful and doesn't have many features so what we can try to do is clean my data and trying to add some features in order to get a better estimate. \nFirst as i saw in another kernel, i decided to obtain the social title of people as probably those who had a higher title socially took precedence during the rescue","34bddd66":"Hello everyone!!, this is my first kaggle and therefore any advice is welcome to learn and become betterThese are the libraries that i used in order to do my stuff.","6b637bdf":"Then, i decided obviusly to remove the name feature since is not useful in order to do a good prediction. I coded the Embarked and the Sex features and i decided to temporary drop the Ticket and the Cabin since are very complex to code \nas there is a lot of missing data. Then i decided to fill the NaN values of the Age with the mean value as it is usually a strategy used. I removed the last two NaN values in the Embarked column","a7c51da5":"With this model i obtained an accurancy of 0.8279% in the training set, but, now i have to understand how this model perform with the test set, and check that not overfitted the training set. So i decided to plot the learning curves with a 5-fold cross validation since as i said, the dataset is not plentiful.","e400c79d":"So here we are, let's do some predictions! I decided to use SVM since it is a powerful classification tool. First i started with a simple SVM with a linear kernel  in order to see how this simple model predict well our data. Then to obtain a better accurancy i decided to use the Radial Basis Function Kernel, an amazing type of kernel that can bring our data in an infinite number of dimensions in order to find the best separating hyperplane. I tryed different parameters and i used GridSearch to find the best parameters that gave me the best score","ee9782de":"As we can see, increasing the number of samples the difference between the two errors is reduced and consequently we can assume not to be overfitting. I looked at other kernels of kaggle that used other features but did not get better results, so I can think that it is not a problem of the number of features, because the features extracted by the other users were linear combinations of the already existing features. I do not know how to improve performance and get a better score in the ranking of the challenge? any suggestions?"}}