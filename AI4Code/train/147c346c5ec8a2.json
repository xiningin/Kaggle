{"cell_type":{"2bb78294":"code","4a73b887":"code","f2da0b53":"code","638e6bb1":"code","0f9a4634":"code","4406d703":"code","dd807f59":"code","97ade669":"code","54f70e2e":"code","9f14aeb7":"code","de145724":"code","08d2c692":"code","09666a2f":"code","c58f5470":"code","1055d5fa":"code","4d31a177":"code","16beb715":"code","2687ad0c":"code","973dc85e":"code","49d126d4":"code","daec6269":"code","c3d30f9d":"code","1c3b0308":"code","9867f97a":"code","d7af360e":"code","20b96774":"code","05118a6e":"code","a89aae8c":"code","609271f0":"code","21911014":"code","549fd21d":"markdown","89712ec9":"markdown","acd83627":"markdown","24abd8a5":"markdown","16d9cb41":"markdown","ac50b619":"markdown","083352d3":"markdown","04577edd":"markdown","c23217f8":"markdown","daa7c1a5":"markdown","edefa48e":"markdown","2305134f":"markdown","75a574ce":"markdown","f176ea6e":"markdown","eab92010":"markdown","68bda7df":"markdown","0bbe4fe6":"markdown","de555fef":"markdown","1ebf7794":"markdown","9bb51d5f":"markdown","416f716a":"markdown","b3bfbfc3":"markdown","4a029412":"markdown","a4e0cc31":"markdown","77574d30":"markdown","bf4e9c91":"markdown","61c311f3":"markdown"},"source":{"2bb78294":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4a73b887":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')","f2da0b53":"train.info()","638e6bb1":"test.info()","0f9a4634":"train.head()","4406d703":"test.head()","dd807f59":"train.describe()","97ade669":"train.groupby('Survived').mean()","54f70e2e":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot(x='Pclass',data=train)","9f14aeb7":"sns.countplot(x='Pclass',hue='Survived',data=train)","de145724":"sns.countplot(x='Sex',hue='Survived',data=train)","08d2c692":"sns.countplot(x='Survived',hue='Sex',data=train)","09666a2f":"g=plt.figure()\ntrain[train.Survived==0].Age.hist(color='r',alpha=0.5)\ntrain[train.Survived==1].Age.hist(color='g',alpha=0.5)\ng.legend(labels='01',loc=0)","c58f5470":"sns.distplot(train[train.Survived==0].Age.dropna(), hist=True)\nsns.distplot(train[train.Survived==1].Age.dropna(), hist=True)","1055d5fa":"g=train.groupby(['SibSp','Survived'])\ndf=pd.DataFrame(g.count()['PassengerId'])\nprint(df)\nsns.countplot(x='SibSp',hue='Survived',data=train)","4d31a177":"g=train.groupby(['Parch','Survived'])\ndf=pd.DataFrame(g.count()['PassengerId'])\nprint(df)\nsns.countplot(x='Parch',hue='Survived',data=train)","16beb715":"plt.figure(figsize=(15,10))\nsns.distplot(train[train.Survived==0].Fare.dropna(), hist=True)\nsns.distplot(train[train.Survived==1].Fare.dropna(), hist=True)","2687ad0c":"sns.countplot(x='Embarked',hue='Survived',data=train)","973dc85e":"plt.figure(figsize=(10,10))\nsns.heatmap(train.drop('PassengerId',axis=1).corr(),square=True,annot=True,cmap='YlGnBu')\nplt.title('Correlation between features')","49d126d4":"#Selecting Features.\nselected_features=['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']\nX_train=train[selected_features]#training Features\nX_test=test[selected_features]#testing Features\ny_train=train['Survived']#labels","daec6269":"X_train['Embarked'].fillna('S',inplace=True)\nX_train['Age'].fillna(X_train['Age'].median(),inplace=True)\nX_test['Age'].fillna(X_test['Age'].median(),inplace=True)\nX_test['Fare'].fillna(X_test['Fare'].mean(),inplace=True)\n","c3d30f9d":"from sklearn.feature_extraction import DictVectorizer\ndict_vec=DictVectorizer(sparse=False)\nX_train=dict_vec.fit_transform(X_train.to_dict(orient='record'))\ndict_vec.feature_names_\n","1c3b0308":"X_test=dict_vec.transform(X_test.to_dict(orient='record'))","9867f97a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\ndtc=DecisionTreeClassifier(criterion='entropy')\nrfc=RandomForestClassifier()\ngbc=GradientBoostingClassifier()\nxgbc=XGBClassifier()","d7af360e":"from sklearn.model_selection import cross_val_score\nprint('DecisionTree:',cross_val_score(dtc,X_train,y_train,cv=5).mean())\nprint('RandomForest:',cross_val_score(rfc,X_train,y_train,cv=5).mean())\nprint('GradientBoosting:',cross_val_score(gbc,X_train,y_train,cv=5).mean())\nprint('XGBClassifier:',cross_val_score(xgbc,X_train,y_train,cv=5).mean())\n","20b96774":"gbc.fit(X_train,y_train)","05118a6e":"gbc_y_predict=gbc.predict(X_test)\nprint(gbc_y_predict)","a89aae8c":"gbc_submission=pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':gbc_y_predict})\ngbc_submission.to_csv('gbc_submission_b.csv',index=False)","609271f0":"'''\nimport tensorflow as tf\nfrom tensorflow import keras\nX_train=np.asarray(X_train).reshape(891,10)\nX_test=np.asarray(X_test).reshape(418,10)\ny_train=np.asarray(y_train)\nmodel = keras.Sequential([\n    keras.layers.Dense(20,activation=tf.nn.relu,input_shape=(10,)),    \n    keras.layers.Dense(30, activation=tf.nn.selu),\n    keras.layers.Dense(2, activation=tf.nn.softmax)\n])\n\nmodel.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.005),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, epochs=4000)\npredictions = model.predict(X_test)\n\ny_predictions=[]\nfor i in range(0,test.shape[0]):\n    y_predictions.append(np.argmax(predictions[i]))\n\ny_predictions_df=np.asarray(y_predictions)\ny_predictions_df_submission=pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':y_predictions_df})\ny_predictions_df_submission.to_csv('C:\/Users\/h206765\/Desktop\/datasets\/titanic\/tf_submission_nn.csv',index=False)\n\n'''","21911014":"'''\nX_train=train.drop(['PassengerId','Ticket','Survived'],axis=1)\nX_test=test.drop(['PassengerId','Ticket'],axis=1)\ny_train=train['Survived']\n\nX_train['Embarked'].fillna('S',inplace=True)\nX_train.loc[X_train.Cabin.notna(),'Cabin']='Yes'\nX_train['Cabin'].fillna('NO',inplace=True)\nX_test['Fare'].fillna(X_test['Fare'].mean(),inplace=True)\nX_test.loc[X_train.Age.notna(),'Cabin']='Yes'\nX_test['Cabin'].fillna('NO',inplace=True)\n\nXtitle=[]\nfor n in list(X_train['Name']):\n    Xtitle.append(n.split(',')[1].split('.')[0])\nXTitle=pd.Series(Xtitle)\nttitle=[]\nfor tn in list(X_test['Name']):\n    ttitle.append(tn.split(',')[1].split('.')[0])\ntTitle=pd.Series(ttitle)\n\nX_train['Name']=XTitle\nX_test['Name']=tTitle\ndef agepredict1(axtrain,aytrain,aytest):\n    dict_vec0=DictVectorizer(sparse=False)\n    axtrain_s=dict_vec0.fit_transform(axtrain.to_dict(orient='record'))\n    aytest_s=dict_vec0.transform(aytest.to_dict(orient='record'))   \n    dtc=tre.ExtraTreeRegressor()\n    dtc.fit(axtrain_s,aytrain)\n    age_pred=dtc.predict(aytest_s)\n    return age_pred\n    \ndef agepredict(axtrain,aytrain,aytest):\n    dict_vec0=DictVectorizer(sparse=False)\n    axtrain_s=dict_vec0.fit_transform(axtrain.to_dict(orient='record'))\n    aytest_s=dict_vec0.transform(aytest.to_dict(orient='record'))\n    ss_X=StandardScaler()\n    axtrain_s=ss_X.fit_transform(axtrain_s)\n    aytest_s=ss_X.transform(aytest_s)\n    lr=SGDRegressor()\n    lr.fit(axtrain_s,aytrain)\n    age_pred=lr.predict(aytest_s)\n    return age_pred\n\navr=X_train['Age'].median()\nxtr1=X_train[X_train.Age.notna()].drop('Age',axis=1)\nxtr2=X_test[X_test.Age.notna()].drop('Age',axis=1)\nxtr=xtr1.append(xtr2)\n\nytr1=X_train[X_train.Age.notna()].Age\nytr2=X_test[X_test.Age.notna()].Age\nytr=ytr1.append(ytr2)\nxte=X_train[X_train.Age.isna()].drop('Age',axis=1)\nX_train.loc[X_train.Age.isna(),'Age']=agepredict(xtr,ytr,xte)\nX_test.loc[X_test.Age.isna(),'Age']=agepredict(xtr,ytr,X_test[X_test.Age.isna()].drop('Age',axis=1))\n\n'''","549fd21d":"Till now, we have all null values filled, but it haven't finished data preparation for modelling yet. As you can see from the train.head(), there are several features that are not numbers.So we need to vectorized them.","89712ec9":"Since Cabin column are missing too much data and I didn't see much infomation from the Name column.I will ignore Cabin and Name in the first time predictions in the following preanalysis.","acd83627":"Training the Model","24abd8a5":"It is very important to understand the data! Without this step, you will not be able to select an appropriate group of features and prepare input data for models to predict results.","16d9cb41":"It seems like if you didn't have siblings, you may be helpless and if you have 1 or 2 siblings, you got nearly 50% chance of survival.But your survival rate will decrease when you have more than 3 siblings.","ac50b619":"**Importing the Data**","083352d3":"Survival rate is 0.38","04577edd":"Filling NaN values\n1. fill all empty 'Embarked' with \u2018S\u2019,the highest frequency value.\n2. fill all empty 'Age' with 'Age' median value rather than mean age.\n3. fill 'Fare'with mean Fare.","c23217f8":"Although it is nonsensical to judge if someone survived","daa7c1a5":"or you may also want to dig more infomations from the features to help improve your score.For example,you may find that some infomations in names,such as Mr,Mrs., can help better predict Ages than just fill it with median or mean.","edefa48e":"**Prediction of Survival of Titanic Passengers: GradientBosostClassifier**\n**Contents**\n1. Importing the Data\n2. Exploring the Data\n3. Preparing Data for Modeling\n4. Modelling the Data & Validating the models\n5. Predictions and making a Submission","2305134f":"Checking Missing Data\n* train dataset:Age,Cabin,Embarked\n* test dataset: Age,Cabin,Fare","75a574ce":"GradientBoostingClassfier won the validations!Theoretically,I will better choose GradientBoostingClassfier as my first model to predict the test data.","f176ea6e":"Based on the analysis aboved, we will drop features that have a lot of null values and those have little relation to survivals, such as Cabin,Name\uff0cPassengerId\uff0cTicket.","eab92010":"**Preparing Data for Modeling**","68bda7df":"let's take a look at the data","0bbe4fe6":"As shown in the figure, people who with better Pclass got much more chance to survive in the tragedy. Your live is decided by your wealth. Now let's see how sex affect the survivals.\n","de555fef":"**Exploring Data**","1ebf7794":"Yeah, of course, ladies first!!! female had more chance to survive the disaster.","9bb51d5f":"**Prediction and making a submission**","416f716a":"Hah,seems everything is ready for modelling now. But what models are we going to choose? DecisionTreeClassifier?RandomForestClassifier? Well, I would like to choose several models and validate them to see which one can reach a higher accuracy.","b3bfbfc3":"**After words**\nyou may want to try other models like simple deep learning or bagging, but they didn't work better than GBClassifier in my works. A simple deep learning model codes are shown as follows:","4a029412":"obviously, the more one paid for his\/her ticket,the more likely he\/she survived disaster","a4e0cc31":"Using 5 fold cross validation to evalue each models.","77574d30":"Those who had children were more likely to survived.","bf4e9c91":"looks like children were more likely to survive.","61c311f3":"> "}}