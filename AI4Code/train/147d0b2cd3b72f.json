{"cell_type":{"fd56803d":"code","b183e01f":"code","a84f795f":"code","e382cf52":"code","5ab6fc01":"code","398a7f05":"code","ec075e69":"code","29920c9b":"code","96031b2a":"markdown","3c4a0a1c":"markdown","a234aa90":"markdown","1b294fc6":"markdown","212ff57d":"markdown","f5065bfe":"markdown","0418a374":"markdown","2a774b88":"markdown","488ff90f":"markdown","6eee12c2":"markdown","64d0ddc3":"markdown"},"source":{"fd56803d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport os\n\n%matplotlib inline\ntrain = pd.read_csv('..\/input\/train.csv', index_col='ID')\ntest = pd.read_csv('..\/input\/test.csv', index_col='ID')\n\ntrain.head()","b183e01f":"print('\\trows\\tcolumns')\nprint('Train:\\t{:>6,}\\t{:>6,}'.format(*train.shape))\nprint('Test:\\t{:>6,}\\t{:>6,}'.format(*test.shape))","a84f795f":"target = train.pop('target')\n\nsns.set()\nmn_train = train.mean()\nstd_train = train.std()\n\nmn_test = test.mean()\nstd_test = test.std()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nax = axes[0]\nax = sns.distplot(mn_train, kde=False, norm_hist=False, ax=ax)\nsns.distplot(mn_test, kde=False, norm_hist=False, ax=ax)\nax.get_xaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nax.get_yaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nax.set_title('Distribution of the mean value of train\/test features.')\nax.set_xlabel(r'Mean value ($\\mu$)')\nax.set_ylabel('Number of features')\nax.legend(['train', 'test'])\n\nax = axes[1]\nsns.distplot(std_train, kde=False, norm_hist=False, ax=ax)\nsns.distplot(std_test, kde=False, norm_hist=False, ax=ax)\nax.get_yaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nax.set_title('Distribution of std value of train\/test features.')\nax.set_xlabel(r'Standard Deviation ($\\sigma^2$)')\nax.set_ylabel('Number of features')\nax.legend(['Train', 'Test']);","e382cf52":"cr = (train.max() == 0) & (train.min() == 0)\ntrain_all_zero_feature_count = cr.index[cr].shape[0]\ncr2 = (test.max() == 0) & (test.max() == 0)\ntest_all_zero_feature_count = cr2.index[cr2].shape[0]\nprint('Number of training features with all 0 values:\\t{}'.format(train_all_zero_feature_count))\nprint('Number of test features with all 0 values:\\t{}'.format(test_all_zero_feature_count))","5ab6fc01":"train_all_zero_features = cr.index[cr]\ntrain.drop(columns=train_all_zero_features, inplace=True)\n\ncount_of_binary_features = (train.max() == 1).sum()\nprint('Number of binary features: {}'.format(count_of_binary_features))","398a7f05":"less_than_1000_count = (train.max() < 1000).sum()\nprint('Number of train features with max value < 1,000: {}'.format(less_than_1000_count))","ec075e69":"plt.figure(figsize=(13, 5))\nax = sns.distplot(train.max(), kde=False, norm_hist=False, bins=1000)\nax = sns.distplot(test.max(), kde=False, norm_hist=False, bins=1000)\nplt.xlim(left=-20000000, right=1e9)\nax.set_xlabel('Max feature value (axis clipped @ 1e9)')\nax.set_ylabel('Number of features')\nax.set_title('Distribution of max value of features')\nax.legend(['Train', 'Test']);","29920c9b":"plt.figure(figsize=(13, 5))\nax = sns.distplot(target, kde=False, norm_hist=False, bins=200)\nax.get_yaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nax.set_xlabel('Transaction value')\nax.set_ylabel('Number of transactions')\nax.set_title('Distribution of target transaction values');","96031b2a":"In fact, this is what ~ 2% of the columns look like.  The tranining (and test) data set has a massive 4,990 numeric features + ID and target columns.  The `ID` column is the bank customer's identification value.\n\nThe test data is about 10 times the size of the training data.  Here is their shape:","3c4a0a1c":"The test set data features have a higher low mean value (left plot), and higher low variance (right plot).","a234aa90":"### 256 All Zero Features\nIn our training data we have 256 features with all zeros values.  None of the test data features have all zero values, though.\n\nSince the test data by definition does not include the target feature, we cannot train our model on these features.  However, if we can infer the meaning of some of these features, we may be able to put them to good use.\n\nWe will drop these 256 features from our training data set and will not include them in our analysis moving forward.\n\nLet's see if we have any binary features in our data.","1b294fc6":"# Introduction\nThe dataset for this competition has masked feature names.  Even though in practice this information is usually available to in-house modellers, hiding it this way gives this problem a numerology flavor.  Nevertheless, we will make reasonable assumptions as to what the features and their order means.\n\nThis is Santander's third Kaggle competition.  One of their two earlier competitions had a dataset [with meaningful column names][3].  However, the features had different types and ranges compared to what we have in this competition.  So it may not help us decipher our features.\n\nIn this analysis, I will go beyond exploring the data and will try to understand what the features represent.\n\n[1]: https:\/\/www.santander.com\/csgs\/Satellite\/CFWCSancomQP01\/es_ES\/Corporativo.html?leng=en_GB\n[2]: http:\/\/banksdaily.com\/topbanks\/Europe\/market-cap-2017.html\n[3]: https:\/\/www.kaggle.com\/c\/santander-product-recommendation\/data","212ff57d":"Here is the distribution of the maximum value for the remaining features.","f5065bfe":"# Warning\nThe features we are working with are most likely temporal data.  However, we don't konw for sure the temporal order of the features.  We should think of this problem as a time series prediction.  We have past values in the series and we're trying to predict the next value.  Therfore, trying to glean insight from finding correlation between the target feature and the independent variables will not be useful at best, and may be misleading.\n\nAlso, we don't know the categories of the features.  In financial institutions, it's common to have such very wide tables where some columns represent monthly credit card transactions over the past five years, and other represent daily checking account transactions over the past 3 years, and so on.  Grouping the features by their mean value or the frequency of transactions may help in categorizing them.","0418a374":"# Target Feature\nWe know from the competition description that the target feature is a transaction value.  Therefore, it's distribution should not be much different from the rest of the features, asuming they represent bank transactions as well.  The max feature transaction showed earlier may make us question that assumption though.\n\nHere is what the target feature distribution looks like.","2a774b88":"# 4,990 Features\nThe dataset is made of one traning (`train.csv`) and one test (`test.csv`) files.  Here is what the training data looks like:","488ff90f":"### No Binary or Percentage Features\nNo binary features in our data.  Also, there is no percentage or ratio feature (continuous feature rangin from 0 to 1 ) in our data.\n\nThere are no remaining features with a maximum value less than 1,000.  This is probably a sign that these are commercial banking transactions with high values.  Also, the currency used is of high denomination.\n","6eee12c2":"# Breakdown the Features by Type\nBased on my own experience working in a financial institution and manipulating similar data sets, most of these columns most likely represent the sum of customers daily transactions.  Every day (or week or month), a new column is added with the total value of transactions for the customer in that time period.\n\nTo know more about the features, let's see their mean value distribution.","64d0ddc3":"The distribution is drastically different between the training and the test data sets.  The train and test data does not seem to be drawn at random.  Using the training data alone in feature engineering will negatively affect the model performance.\n\nOne thing to note about our features is their temporal nature, and some of them might be cyclic (monthly or weekly deposits) since they represent banking transactions.  However, unfortunately we don't know their order or even the time period they represent."}}