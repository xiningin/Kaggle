{"cell_type":{"284294b1":"code","a9ad5421":"code","671e03df":"code","85721731":"code","374bba7d":"code","449e68a0":"code","2294a577":"code","e8426360":"code","7564e6f3":"code","05b008e5":"code","882e8a58":"code","17ac8734":"code","86e38793":"code","87eed7ea":"code","5faa70a5":"code","f262adae":"code","950bd1c2":"code","e2c94abc":"code","5dce5586":"code","93d826e7":"code","b48286d2":"code","6eae5fc1":"code","de9e3489":"code","ca5c073a":"code","eaec0cf2":"code","abfbb9e9":"code","0d7dd737":"code","a6c55085":"code","0b52483c":"code","8b2a2205":"code","8fb08834":"code","80ad1f81":"code","27be94aa":"code","3d2decdf":"code","0f7b7a28":"code","68e4d81b":"code","88c6cff2":"code","3b09a3c0":"code","4de1f6e5":"code","65827e51":"code","7c26da3f":"code","7c6db100":"code","00128cb3":"code","e4d64963":"code","55fe2fde":"code","d38c8b01":"code","d744cb7a":"code","75f7ed5a":"code","7cd7fbec":"code","e478da1f":"code","4832277a":"code","06570392":"markdown","adad175e":"markdown","b6638972":"markdown","7c0f2525":"markdown","693d7112":"markdown","dedcdf73":"markdown","8c4dbbba":"markdown"},"source":{"284294b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a9ad5421":"import json\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Third party packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\nfrom plotly.offline import iplot\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nsns.set(context= \"notebook\", color_codes=True)\nplt.style.use('bmh')\n\npyo.init_notebook_mode()\n\n%matplotlib inline","671e03df":"data_train = pd.read_csv(\"\/kaggle\/input\/private1\/app_train.csv\")\ndata_test = pd.read_csv(\"\/kaggle\/input\/private1\/app_test.csv\")\ndata_description = pd.read_csv(\"\/kaggle\/input\/private1\/columns_description.csv\")\ndata_installment = pd.read_csv(\"\/kaggle\/input\/private1\/installment_payment.csv\")\ndata_previous = pd.read_csv(\"\/kaggle\/input\/private1\/prev_app.csv\")\n\n\ndata_train.head(10)","85721731":"data_train.info()","374bba7d":"data_description.info()","449e68a0":"data_installment.info()","2294a577":"data_train.isnull().sum()","e8426360":"data_test.isnull().sum()","7564e6f3":"sns.pairplot(data_train);","05b008e5":"df_summary = data_train.describe()\ndef draw_axvlines(plt, col):\n    mean = df_summary.loc[\"mean\", col]\n    q1 = df_summary.loc[\"25%\", col]\n    q2 = df_summary.loc[\"50%\", col]\n    q3 = df_summary.loc[\"75%\", col]\n    plt.axvline(mean, color = \"g\");              #mean\n    plt.axvline(q1, color = \"b\");               \n    plt.axvline(q2, color = \"navy\");             \n    plt.axvline(q3, color = \"purple\");           \n    plt.legend({\"Mean\": mean, \"25%\" : q1, \"50%\" : q2, \"75%\" : q3});\n\nfig, axes = plt.subplots(3, 2, figsize = (20,12));\nfig.suptitle('Distribution .');\n\n\n# distribution of NUM_CHILDREN\nsns.boxplot(data_train[\"NUM_CHILDREN\"], ax = axes[0][0], color = \"mediumslateblue\");\naxes[0][0].set(xlabel = 'Distribution of NUM_CHILDREN');\n\npp = sns.distplot(data_train[\"NUM_CHILDREN\"], ax = axes[0][1], bins = 10, color = \"mediumslateblue\");\naxes[0][1].set(xlabel = 'Distribution of NUM_CHILDREN');\ndraw_axvlines(pp, \"NUM_CHILDREN\");\n\n\n# distribution of APPROVED_CREDIT\nsns.boxplot(data_train[\"APPROVED_CREDIT\"], ax = axes[1][0], color = \"mediumslateblue\");\naxes[1][0].set(xlabel = 'Distribution of APPROVED_CREDIT');\n\npp = sns.distplot(data_train[\"APPROVED_CREDIT\"], ax = axes[1][1], bins = 10, color = \"mediumslateblue\");\naxes[1][1].set(xlabel = 'Distribution of APPROVED_CREDIT');\ndraw_axvlines(pp, \"APPROVED_CREDIT\")\n\n\n# distribution of Income\nsns.boxplot(data_train[\"INCOME\"], ax = axes[2][0], color = \"mediumslateblue\");\naxes[2][0].set(xlabel = 'Distribution of INCOME\t');\n\npp = sns.distplot(data_train[\"INCOME\"], ax = axes[2][1], color = \"mediumslateblue\");\naxes[2][1].set(xlabel = 'Distribution of INCOME\t');\ndraw_axvlines(pp, \"INCOME\")","882e8a58":"def groupby_get_cc_count(tdf, col):\n    tdf = tdf.groupby([col, \"CONTRACT_TYPE\"])[\"CONTRACT_TYPE\"].count().reset_index(level = 0)\n    tdf.columns = [col, \"count\"]\n    tdf = tdf.reset_index()\n    return tdf\n\nfig, axes = plt.subplots(1,1, figsize=(20, 10))\naa = data_train[\"NUM_CHILDREN\"].value_counts().reset_index()\naa[\"index\"] = aa[\"index\"].astype(str)\naa[\"family_perc\"] = aa[\"NUM_CHILDREN\"].apply(lambda x : round((x\/5000)*100, 2))\n\nplt.subplot(1, 2, 2)\naa = groupby_get_cc_count(data_train[[\"NUM_CHILDREN\", \"CONTRACT_TYPE\"]], \"NUM_CHILDREN\")\nsns.barplot(aa[\"NUM_CHILDREN\"], aa[\"count\"], hue = aa[\"CONTRACT_TYPE\"], palette = [\"skyblue\", \"darkgreen\"]);\nplt.show();\n\ndf=data_train\n","17ac8734":"df['DAYS_AGE'] = df['DAYS_AGE']\/-365\n\nfig, axes = plt.subplots(3, 2, figsize = (25,15))\n\n# Distribution of FAMILY_STATUS\naa = groupby_get_cc_count(df[[\"FAMILY_STATUS\", \"CONTRACT_TYPE\"]], \"FAMILY_STATUS\")\n# d_dict = {1 : \"Married\", 2 : \"Single \/ not married\", 3 : \"Civil marriage\"}\n# aa[\"STATUS\"] = aa[\"FAMILY_STATUS\"].apply(lambda x : d_dict[x])\nax = sns.barplot(aa[\"FAMILY_STATUS\"], aa[\"count\"], hue = aa[\"CONTRACT_TYPE\"], ax = axes[0][0], palette= [\"skyblue\", \"darkgreen\"]);\nax.set(xlabel = 'FAMILY_STATUS', ylabel = 'Count of customers');\n\n# Distribution of Age\ndf['age_bin'] = pd.cut(df['DAYS_AGE'], bins = [0, 30, 40, 50, 60, 100], labels = ['0-30', '31-40', '41-50', '51-60', '60+'])\naa = groupby_get_cc_count(df[[\"age_bin\", \"CONTRACT_TYPE\"]], \"age_bin\")\nax = sns.barplot(aa[\"age_bin\"], aa[\"count\"], hue = aa[\"CONTRACT_TYPE\"], ax = axes[0][1], palette= [\"skyblue\", \"darkgreen\"]);\nax.set(xlabel = 'Age bins', ylabel = 'Count of customers');\n\n# Distribution of GENDER\naa = groupby_get_cc_count(df[[\"GENDER\", \"CONTRACT_TYPE\"]], \"GENDER\")\n# d_dict = {1 : \"Married\", 2 : \"Single \/ not married\", 3 : \"Civil marriage\"}\n# aa[\"STATUS\"] = aa[\"FAMILY_STATUS\"].apply(lambda x : d_dict[x])\nax = sns.barplot(aa[\"GENDER\"], aa[\"count\"], hue = aa[\"CONTRACT_TYPE\"], ax = axes[1][0], palette= [\"skyblue\", \"darkgreen\"]);\nax.set(xlabel = 'GENDER', ylabel = 'Count of customers');\n\n# Distribution of INCOME_TYPE\naa = groupby_get_cc_count(df[[\"INCOME_TYPE\", \"CONTRACT_TYPE\"]], \"INCOME_TYPE\")\n# d_dict = {1 : \"Married\", 2 : \"Single \/ not married\", 3 : \"Civil marriage\"}\n# aa[\"STATUS\"] = aa[\"FAMILY_STATUS\"].apply(lambda x : d_dict[x])\nax = sns.barplot(aa[\"INCOME_TYPE\"], aa[\"count\"], hue = aa[\"CONTRACT_TYPE\"], ax = axes[1][1], palette= [\"darkgreen\",\"skyblue\"]);\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", size=10);\n\n# Distribution of HOUSING_TYPE\t\naa = groupby_get_cc_count(df[[\"HOUSING_TYPE\", \"CONTRACT_TYPE\"]], \"HOUSING_TYPE\")\n# d_dict = {1 : \"Married\", 2 : \"Single \/ not married\", 3 : \"Civil marriage\"}\n# aa[\"STATUS\"] = aa[\"FAMILY_STATUS\"].apply(lambda x : d_dict[x])\nax = sns.barplot(aa[\"HOUSING_TYPE\"], aa[\"count\"], hue = aa[\"CONTRACT_TYPE\"], ax = axes[2][0], palette= [\"skyblue\", \"darkgreen\"]);\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", size=20)\n\n\n# Distribution of HWEEKDAYS_APPLY\t\naa = groupby_get_cc_count(df[[\"WEEKDAYS_APPLY\", \"CONTRACT_TYPE\"]], \"WEEKDAYS_APPLY\")\n# d_dict = {1 : \"MONDAY\", 2 : \"TUESDAY\", 3 : \"WEDNESDAY\", 4 : \"THURSDAY\", 5  : \"FRIDAY\", 6 : \"SATURDAY\", 7 : \"SUNDAY\"}\n# aa[\"WEEKDAYS_APPLY\"] = aa[\"WEEKDAYS_APPLY\"].apply(lambda x : d_dict[x])\nax = sns.barplot(aa[\"WEEKDAYS_APPLY\"], aa[\"count\"], hue = aa[\"CONTRACT_TYPE\"], ax = axes[2][1], palette= [\"skyblue\", \"darkgreen\"]);\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", size=20)\n\n\n\n","86e38793":"fig, axes = plt.subplots(figsize=(25,15))\n# Distribution of ORGANIZATION_TYPE\t\naa = groupby_get_cc_count(df[[\"ORGANIZATION_TYPE\", \"CONTRACT_TYPE\"]], \"ORGANIZATION_TYPE\")\n\nax = sns.barplot(aa[\"ORGANIZATION_TYPE\"], aa[\"count\"], hue = aa[\"CONTRACT_TYPE\"], palette= [\"skyblue\", \"darkgreen\"]);\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", size=10)\n","87eed7ea":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","5faa70a5":"missing_values = missing_values_table(data_train)\nmissing_values1 = missing_values_table(data_test)\nmissing_values.head(20)","f262adae":"df = pd.concat([data_train,data_test])","950bd1c2":"df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","e2c94abc":"from sklearn.preprocessing import LabelEncoder\nimport os\n\n\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in data_train:\n    if data_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(data_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(data_train[col])\n            # Transform both training and testing data\n            data_train[col] = le.transform(data_train[col])\n            data_test[col] = le.transform(data_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","5dce5586":"data_train = pd.get_dummies(data_train)\ndata_test = pd.get_dummies(data_test)\n\nprint('Training Features shape: ', data_train.shape)\nprint('Testing Features shape: ', data_test.shape)","93d826e7":"train_labels = data_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\ndata_train, data_test = data_train.align(data_test, join = 'inner', axis = 1)\n\n# Add the target back in\ndata_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', data_train.shape)\nprint('Testing Features shape: ', data_test.shape)","b48286d2":"anom = data_train[data_train['DAYS_WORK'] == 365243]\nnon_anom = data_train[data_train['DAYS_WORK'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","6eae5fc1":"data_train['DAYS_EMPLOYED_ANOM'] = data_train[\"DAYS_WORK\"] == 365243\n\n# Replace the anomalous values with nan\ndata_train['DAYS_WORK'].replace({365243: np.nan}, inplace = True)\n\ndata_train['DAYS_WORK'].plot.hist(title = 'Days Employment Histogram');\nplt.style.use('ggplot')\nplt.xlabel('Days Employment');","de9e3489":"data_test['DAYS_EMPLOYED_ANOM'] = data_test[\"DAYS_WORK\"] == 365243\ndata_test[\"DAYS_WORK\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (data_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(data_test)))","ca5c073a":"# Find correlations with the target and sort\ncorrelations = data_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","eaec0cf2":"# Find the correlation of the positive days since birth and target\ndata_train['DAYS_AGE'] = abs(data_train['DAYS_AGE'])\ndata_train['DAYS_AGE'].corr(data_train['TARGET'])","abfbb9e9":"plt.style.use('ggplot')\n\n# Plot the distribution of ages in years\nplt.hist(data_train['DAYS_AGE'] \/ 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","0d7dd737":"plt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(data_train.loc[data_train['TARGET'] == 0, 'DAYS_AGE'] \/ 365, label = 'target = 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(data_train.loc[data_train['TARGET'] == 1, 'DAYS_AGE'] \/ 365, label = 'target = 1')\n\n# Labeling of plot\nplt.style.use('ggplot')\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","a6c55085":"poly_features = data_train[['EXT_SCORE_1', 'EXT_SCORE_2', 'EXT_SCORE_3', 'DAYS_AGE', 'TARGET']]\npoly_features_test = data_test[['EXT_SCORE_1', 'EXT_SCORE_2', 'EXT_SCORE_3', 'DAYS_AGE']]\n\n# imputer for handling missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","0b52483c":"poly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","8b2a2205":"poly_transformer.get_feature_names(input_features = ['EXT_SCORE_1', 'EXT_SCORE_2', 'EXT_SCORE_3', 'DAYS_AGE'])[:15]","8fb08834":"poly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SCORE_1', 'EXT_SCORE_2', \n                                                                           'EXT_SCORE_3', 'DAYS_AGE']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","80ad1f81":"poly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SCORE_1', 'EXT_SCORE_2', \n                                                                                'EXT_SCORE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['LN_ID'] = data_train['LN_ID']\ndata_train_poly = data_train.merge(poly_features, on = 'LN_ID', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['LN_ID'] = data_test['LN_ID']\ndata_test_poly = data_test.merge(poly_features_test, on = 'LN_ID', how = 'left')\n\n# Align the dataframes\ndata_train_poly, data_test_poly = data_train_poly.align(data_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', data_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', data_test_poly.shape)","27be94aa":"data_train_domain = data_train.copy()\ndata_test_domain = data_test.copy()\n\ndata_train_domain['CREDIT_INCOME_PERCENT'] = data_train_domain['APPROVED_CREDIT'] \/ data_train_domain['INCOME']\ndata_train_domain['ANNUITY_INCOME_PERCENT'] = data_train_domain['ANNUITY'] \/ data_train_domain['INCOME']\ndata_train_domain['CREDIT_TERM'] = data_train_domain['ANNUITY'] \/ data_train_domain['APPROVED_CREDIT']\ndata_train_domain['DAYS_EMPLOYED_PERCENT'] = data_train_domain['DAYS_WORK'] \/ data_train_domain['DAYS_AGE']","3d2decdf":"data_test_domain['CREDIT_INCOME_PERCENT'] = data_test_domain['APPROVED_CREDIT'] \/ data_test_domain['INCOME']\ndata_test_domain['ANNUITY_INCOME_PERCENT'] = data_test_domain['ANNUITY'] \/ data_test_domain['INCOME']\ndata_test_domain['CREDIT_TERM'] = data_test_domain['ANNUITY'] \/ data_test_domain['APPROVED_CREDIT']\ndata_test_domain['DAYS_EMPLOYED_PERCENT'] = data_test_domain['DAYS_WORK'] \/ data_test_domain['DAYS_AGE']","0f7b7a28":"# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(data_train_domain.loc[data_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(data_train_domain.loc[data_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \n","68e4d81b":"data_train_domain.head()","88c6cff2":"data_test.drop(columns='Unnamed: 0',inplace=True)","3b09a3c0":"from sklearn.preprocessing import MinMaxScaler\n# Drop the target from the training data\n\nif 'TARGET' in data_train:\n    train = data_train.drop(columns = ['TARGET'])\nelse:\n    train = data_train.copy()\n    \n#Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = data_test.copy()\n\n# Median imputation of missing values\nimputer = SimpleImputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(data_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","4de1f6e5":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","65827e51":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","7c26da3f":"log_reg_pred","7c6db100":"# Submission dataframe\nsubmit = data_test[['LN_ID']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","00128cb3":"log_reg.score(train, train_labels)","e4d64963":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","55fe2fde":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","d38c8b01":"random_forest.score(train, train_labels)","d744cb7a":"\ndomain_features_names = list(data_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = SimpleImputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(data_train_domain)\ndomain_features_test = imputer.transform(data_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","75f7ed5a":"random_forest_domain.score(domain_features, train_labels)","7cd7fbec":"def plot_feature_importances(df):\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","e478da1f":"feature_importances_sorted = plot_feature_importances(feature_importances)","4832277a":"feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)\nfeature_importances_domain_sorted.drop([0], inplace=True)\nplot_feature_importances(feature_importances_domain_sorted)","06570392":"RF","adad175e":"Missing Values and Other things","b6638972":"**Interpretation**","7c0f2525":"**Thats it.**","693d7112":"Client Age Dist","dedcdf73":"Terlihat top 5 best feature pada kasus ini adalah\n1. EXT_SCORE_2\n2. EXT_SCORE_3\n3. DAYS_AGE\n4. DAYS_ID_CHANGE\n5. EXT_SCORE_1","8c4dbbba":"**Feature Eng.**"}}