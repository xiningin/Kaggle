{"cell_type":{"ebb0236d":"code","5ca7fa71":"code","aaba50f3":"code","46b5f26d":"code","05e36b3a":"code","5738f4f1":"code","da9fb68e":"code","0f1ad665":"code","2b3eb89f":"code","1e9e19be":"code","4e2bcdda":"code","def1ed90":"code","f964d034":"code","ee0bc44a":"code","bf59eeff":"code","33924d4c":"code","7f791b38":"code","19d87005":"code","e075759f":"code","a0c2ecd8":"code","cb69f644":"code","d906de6e":"code","b1eac2af":"markdown","dc5b6f0c":"markdown","4058ec40":"markdown","b8c34ade":"markdown","ff5cf1ae":"markdown","2dfbf8ea":"markdown"},"source":{"ebb0236d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\"","5ca7fa71":"#input the dataset\n\n#csv file formats are most common and convinient . \n#No surprise python has a separate library for that!!\n\ndata=pd.read_csv(\"..\/input\/deep-learning-az-ann\/Churn_Modelling.csv\")\n\n#..\/input\/path is a way how the path for raw data files are found.\n\n# **General Trivia**\n#One can use the special characters double-dot (..) to move one directory up \n#instead of placing 'kaggle' everytime while taking the input. \n\ndata.head()\n# Generally done to display get first look of the dataset in first five rows by default.","aaba50f3":"#Returns the dimension of your dataframe- (rows , columns). \n#Gives one an idea of number of features \n# and rows that will be trained further in the model.\n\ndata.shape","46b5f26d":"#Returns all statiscal details for each feature present in the data.\n#Common idea - One can see the data with no blinders on \n#and study how the values range in each feature.\n\ndata.describe()","05e36b3a":"# Returns the data type of each column .\n# Commmon idea - One gets to decide which columns are categorical\n# or numeric(int\/float) in their type \n# and how can one commence data pre-processing journey.\n\ndata.dtypes","5738f4f1":"#Returns null values present in the dataset\n#Common idea- One gets to decide which columns need missing value treatments.\n\ndata.isnull().sum()","da9fb68e":"data.head()","0f1ad665":"#Returns \"what it says\" - counts of a value in a particular column.\n#bins - is a very underrated parameter but it can be used to get some really good\n#grouping of data values\n\ndata.CreditScore.value_counts(bins=3)","2b3eb89f":"# To get number of unique values in Geography column\n\ndata.Geography.nunique()","1e9e19be":"# Creation of dummy variables for Gender and Geography \n# Idea behind this - 'Geography' if label encoded in 0, 1 ,2 will have no meaning , thus the model wont't be able to understand the importance of this column . \n#However label encoding 'Gender' could have made sense in a way and can also be done alternatively.\n\ngender_cat = pd.get_dummies(data['Gender'] , drop_first=True)\ngeo_cat = pd.get_dummies(data['Geography'] , drop_first=True)\n\ndata=pd.concat([data , gender_cat ,geo_cat] , axis=1)","4e2bcdda":"# Practice of checking the update done by above cell\ndata.head()","def1ed90":"#Dropping \"of no use\" columns to eliminate redundancy.\n\ndata.drop(columns=['Gender', 'Geography' , 'RowNumber', 'CustomerId' , 'Surname'] , axis = 1 , inplace = True )","f964d034":"data.head()","ee0bc44a":"#Data that needs to trained goes in X and respective labels into y (here 'Exited').\n\nfrom sklearn.model_selection import train_test_split\ny = data['Exited']\nX = data.drop(['Exited'] , axis = 1)\n\n#Splitting the data into training and testing by specifying it in the test_size ,using other parameters i.e. random_state and shuffle depends on you solely.\n\nX_train, X_test, y_train, y_test = train_test_split(X , y , test_size=0.3 , random_state=0 , shuffle=False)","bf59eeff":"# Scaling is a crucial step to get apt results when your data value range across columns differ in large scale.\n\n# StandardScaler and MinMaxScaler are more common when dealing with continuous numerical data.\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\n#Below columns get scaled for train and test respectively.\nX_train[['Age',\"Tenure\" , 'Balance' , 'CreditScore' , \"EstimatedSalary\" , 'NumOfProducts']] = ss.fit_transform(X_train[['Age',\"Tenure\" , 'Balance' , 'CreditScore' , \"EstimatedSalary\" , 'NumOfProducts']])\nX_test[['Age',\"Tenure\" , 'Balance' , 'CreditScore' , \"EstimatedSalary\" , 'NumOfProducts']] = ss.fit_transform(X_test[['Age',\"Tenure\" , 'Balance' , 'CreditScore' , \"EstimatedSalary\" , 'NumOfProducts']])","33924d4c":"#Importing the libraries we need to build a neural network\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers , Sequential \nfrom keras.layers import Dense","7f791b38":"\nmodel=Sequential([\n    layers.Dense(8, activation = 'relu' , input_shape = [11]),\n    layers.Dense(8 , activation = 'relu' ),\n    layers.Dense(1 , activation = 'sigmoid')\n])\n      ","19d87005":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","e075759f":"model.fit(X_train, y_train, batch_size = 25, epochs = 100,verbose = 0)\n","a0c2ecd8":"# Predicting on train data\ny_pred = model.predict(X_train)\nscore, acc = model.evaluate(X_train, y_train,batch_size=10)\nprint('Train score:', score)\nprint('Train accuracy:', acc*100)\n","cb69f644":"# Predicting on test data\ny_pred = model.predict(X_test)\nscore, acc = model.evaluate(X_test, y_test,batch_size=10)\nprint('Test score:', score)\nprint('Test accuracy:', acc*100)\n","d906de6e":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import mean_absolute_error\n\n\ny_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)*1\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy = (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\nprint(\"Confusion Matrix Accuracy: \"+ str(accuracy*100)+\"%\")\n\n#F1 score\nrecall=(cm[0][0])\/(cm[0][0]+cm[0][1])\nprecision=(cm[0][0])\/(cm[0][0]+cm[1][0])\nF1=(2*recall*precision)\/(precision+recall)\nprint(\"F1 Score:\"+str(F1))\n\n#MAE\nmae=mean_absolute_error(y_test, y_pred)\nprint(\"MAE:\"+str(mae))\n","b1eac2af":"**NOTE** - *The most confusing thing here is that the shape of the input to the model is defined as an argument on the first hidden layer. This means that the line of code that adds the first Dense layer is doing 2 things, defining the input or visible layer and the first hidden layer.*","dc5b6f0c":"The main idea from here onwards is to convert numeric values into categorical ones since we are trying to classify customers who churned or not.\n\nMethods that we will try in this dataset -\n\n1. Binning\n2. One Hot Encoding","4058ec40":"Once the model gets defined it can now be compiled. \n\n* **optimizer** - adam ( stochasticc gradient descent algorithm ) , because it automatically tunes itself and give good results. \n\n* **loss** - binary crossentropy ( to evaluate the error in current state of the model which will be estimated repeatedly )\n\n* **metrics** = reports the classification accuracy","b8c34ade":"**We can piece it all together by adding each layer:**\n\n* The model expects sample of data with 11 features mentioned in the input_dim = 11 argument\n* The first and the second hidden layer comprises of 8 nodes and uses the relu activation function.\n* The output layer has 1 node and uses sigmoid activation function.\n\nChoosing number of nuerons for each hidden layer is intutive\nUsing too many neurons in the hidden layers may result in overfitting.\n\n**Few rules of thumb that one can consider for determining acceptable number of nuerons to use in the hidden layer -**\n\n* No. of hidden nuerons should be between the size of input layer and size of the output layer.\n* No. of hidden layer neurons should be 2\/3 the size of the input layer , plus the size of the output layer.\n* No. of hidden nuerons should be less than twice the size of the input layer\n\nThese three rules can give you a good start and it can eventually come down to intutive reasoning of trial and error in selecting the no. of neurons.\n","ff5cf1ae":"* **Standardization** -   *we center the feature columns at mean 0 with standard deviation 1 so that the feature columns take the form of a normal distribution, which makes it easier to learn the weights.*","2dfbf8ea":"Once the model gets compiled its ready to be trained.\n\n**Epoch** can be thought of as a nested for-loop  that iterates over each batch of samples, where one batch has the specified \u201cbatch size\u201d number of samples.\n\n**Batch** is analogous to a for-loop iterating over one or more samples and making predictions.\n\nThese configurations can be chosen by trial and error.\n"}}