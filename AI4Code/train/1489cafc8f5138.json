{"cell_type":{"6d510e94":"code","ab7f1757":"code","160872f6":"code","8fa6e7df":"code","994e2b1b":"code","a2131d08":"code","fe6aeb2a":"code","0fbc8cde":"code","8883f85a":"code","1ec4046f":"code","3dcc4776":"code","6315dbb3":"code","39b3ea79":"code","e6672e1e":"code","b328b603":"code","dda87a35":"code","4d64dc72":"code","575c3d2c":"code","fbc8ff08":"code","b44a932d":"code","81f23712":"code","19236c1d":"code","3ff05f21":"code","d6275e89":"code","a4f3ac44":"code","8f59f5fa":"code","ffcb3b16":"code","70c684f3":"code","9472bd48":"code","b7f66f23":"code","185f4ae5":"code","b6a07125":"code","bf94c57d":"code","fe67dd13":"code","26b9a6ee":"code","9ac87607":"code","4d005190":"code","682ea0ce":"code","95c1a079":"code","7518a527":"code","16539f7d":"code","09c3bf65":"code","4460660a":"code","8798129a":"code","d2f70473":"code","c7fde893":"code","3c2e2fe0":"code","33cebe87":"code","51ed8451":"code","e148617a":"code","f1861707":"code","37ab75db":"code","93cd9bda":"code","831bd84c":"code","72615b91":"code","707f5e5c":"markdown","055cffda":"markdown","dd4eb456":"markdown","2e386c8b":"markdown","52e3e0d8":"markdown","90e5429c":"markdown","539a3f1b":"markdown","12b127ae":"markdown","be172cb5":"markdown","b8ecfb11":"markdown","75e4a958":"markdown","0b20266f":"markdown","c8130538":"markdown","6473e406":"markdown","eeed4004":"markdown","f9fcb0b7":"markdown","c704e4c4":"markdown","0c446111":"markdown","0a9f0eee":"markdown","5e76302d":"markdown","d0afba78":"markdown","740f4c00":"markdown","b78b9b01":"markdown","e9244c7b":"markdown","f17a2133":"markdown","6490d222":"markdown","a5e84ef4":"markdown","ea7a74fc":"markdown","c48c0c37":"markdown","f73e04ba":"markdown","30841954":"markdown","4a233712":"markdown","719e7ad3":"markdown","1e5a57bd":"markdown","a8058ab4":"markdown","dc860c22":"markdown","795beb74":"markdown","399cdee2":"markdown","caf8926b":"markdown","3609b9bc":"markdown","533f4a40":"markdown"},"source":{"6d510e94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab7f1757":"import os\nfrom os.path import join\nimport copy\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport missingno as msno\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\nfrom sklearn import tree\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, auc\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\n","160872f6":"#Use the first set of data to predict the occurence of diabetes in second dataset\ndata1=pd.read_csv('..\/input\/korean-genome-and-epidemiology-study-koges\/follow_01_data.csv')\ndata2=pd.read_csv('..\/input\/korean-genome-and-epidemiology-study-koges\/follow_02_data.csv')","8fa6e7df":"#\ub79c\ub364\ud558\uac8c \uc0d8\ud50c \uc11e\uae30\ndf1=data1.sample(frac=1)\n#\ubd84\uc11d\ud560 \ud544\uc694 \uc5c6\ub294 \uceec\ub7fc \ub9ac\uc2a4\ud2b8 (\uc0ad\uc81c) -T01_HBA1C\ub294\ub098\uc911\uc5d0 \ud480\uc5b4\ub3c4\ub428\nmissing_column=['T00_DATA_CLASS','T01_EDATE','T01_SMAG','T01_HTNAG','T01_DM','T01_DMAG','T01_LIPAG','T01_FMFHTAG','T01_FMMHTAG','T01_FMFDMAG','T01_FMMDMAG','T01_MNSAG','T01_FPREGAG','T01_FLABAG','T01_PMAG_C','T01_TAKAM','T01_RICEAM','T01_WINEAM','T01_HLIQAM','T01_TAKFQ','T01_RICEFQ','T01_WINEFQ','T01_HLIQFQ']\n\n#\ud574\ub2f9\uc0ac\ud56d\uc5c6\uc74c (77777)\uc744 0\uc73c\ub85c \ubcc0\uacbd\ud560 \uceec\ub7fc \ub9ac\uc2a4\ud2b8 (\ubcc0\uacbd)\nreplace_column=['T01_DRDU','T01_SOJUAM','T01_BEERAM','T01_SMDU','T01_SMAM','T01_PREG','T01_CHILD','T01_PMYN_C']\n\n#\uce74\ud14c\uace0\ub9ac\uc640 \ub418\uc5b4\uc788\uc5b4\uc11c get_dummies\ub85c \ubcc0\uacbd\ud560 \ub9ac\uc2a4\ud2b8\ncategorical=['T01_MARRY','T01_DRINK','T01_SMOKE']\n\n#\uce74\ud14c\uace0\ub9ac\ud615\ud0dc\uc5d0\uc11c 1,2\ub85c \ub418\uc5b4\uc788\ub294\uac83\uc744 0,1\ub85c \ubcc0\uacbd\uc2dc\ud0a8\ub2e4.\ncategorical_1_2=['T00_SEX','T01_PSM','T01_EXER','T01_HTN','T01_LIP','T01_FMFHT','T01_FMMHT','T01_FMFDM','T01_FMMDM','T01_PREG','T01_CHILD','T01_PMYN_C']\n\n#replace_column\uc5d0 \uc788\ub294 \uceec\ub7fc \ubcc0\uacbd\uc2dc\ud0a4\uae30\nfor col in replace_column:\n\tdf1[col].replace(77777.0,0,inplace=True)\n\n#categorical_1_2\uc5d0 \uc788\ub294\uac83\uc744 0,1\ub85c \ubcc0\uacbd\uc2dc\ud0a4\uae30\ndf1[categorical_1_2]=df1[categorical_1_2].replace({1.0:0,1:0,2.0:1,2:1})\n\n\n#T2DM\/T2DM\uacfc\uac70\ub825\uc788\ub294 \uc0d8\ud50c \uc0ad\uc81c \uceec\ub7fc \uc81c\uac70\ndf2=df1.drop(df1[(df1.T01_DM==2)|(df1.T01_GLU0>=126)].index)\n\n#missing column\uc5d0 \uc788\ub294 \uceec\ub7fc\ub4e4 \uc0ad\uc81c\ud558\uae30\ndf2.drop(missing_column, axis=1, inplace=True)\n\n#NA\ub85c \ubcc0\uacbd\ud558\uae30\ndf3=df2.replace(dict.fromkeys([66666.0,99999.0],None))\n\ndf3.head()","994e2b1b":"p=df3.hist(figsize=(20,20))","a2131d08":"col_mean=['T01_HEIGHT','T01_WEIGHT','T01_WAIST','T01_HIP','T01_PULSE','T01_SBP','T01_DBP','T01_HBA1C','T01_GLU0','T01_TCHL','T01_HDL','T01_HBA1C']\nfor header in df3.keys():\n    if df3[header].isna().sum()>0:\n        if header in col_mean:\n            df3[header].fillna(df3[header].mean(), inplace = True)\n        else:\n            df3[header].fillna(df3[header].median(), inplace = True)\ndf3.head()","fe6aeb2a":"d2f1=data2.replace(dict.fromkeys([66666.0,77777.0,99999.0],None))\nd2f1.drop(d2f1[(d2f1.T02_DM==None)|(d2f1.T02_GLU0==None)].index, inplace=True)\nd2f1['T2DM']=(d2f1.T02_DM==2)|(d2f1.T02_GLU0>=126)\nT2DM_set=d2f1[['T00_ID','T2DM']].replace(False,0).replace(True,1)\nT2DM_set.head()","0fbc8cde":"df4=df3.merge(T2DM_set, on=\"T00_ID\")\nlabel=df4['T2DM']\nlabel.head()","8883f85a":"df4_1=df4.copy(deep=True)\nscale_column = ['T01_HEIGHT','T01_WEIGHT','T01_WAIST','T01_HIP','T01_PULSE','T01_SBP','T01_DBP','T01_CREATININE','T01_HBA1C','T01_GLU0','T01_AST','T01_ALT','T01_TCHL','T01_HDL','T01_TG','T01_INS0']\nfeatures = df4_1[scale_column]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\ndf4_1[scale_column] = features\ndf4_1.head()","1ec4046f":"p=df4_1.T2DM.value_counts().plot(kind=\"bar\")","3dcc4776":"colormap = plt.cm.viridis\nplt.figure(figsize=(10,10))\nsns.heatmap(df4_1.corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, annot=False)","6315dbb3":"df5=pd.get_dummies(data=df4_1,columns=categorical)\ndf5.drop(columns=['T00_ID','T2DM'], inplace=True)\ndf5.head()","39b3ea79":"X_train,X_test,y_train,y_test = train_test_split(df5,label,test_size=1\/3,random_state=42, stratify=label)\nsmote=SMOTE(k_neighbors=5)\nsmoted_X_train,smoted_y_train=smote.fit_resample(X_train,y_train)\nsmoted_X_test,smoted_y_test=smote.fit_resample(X_test,y_test)","e6672e1e":"smoted_X_train","b328b603":"types=['rbf','linear']\nfor i in types:\n    model=svm.SVC(kernel=i)\n    model.fit(smoted_X_train,smoted_y_train)\n    prediction=model.predict(smoted_X_test)\n    print('Accuracy for SVM kernel=',i,'is',accuracy_score(prediction,smoted_y_test))","dda87a35":"model = LogisticRegression()\nmodel.fit(smoted_X_train,smoted_y_train)\nprediction=model.predict(smoted_X_test)\nprint('The accuracy of the Logistic Regression is',accuracy_score(prediction,smoted_y_test))","4d64dc72":"model=DecisionTreeClassifier()\nmodel.fit(smoted_X_train,smoted_y_train)\nprediction=model.predict(smoted_X_test)\nprint('The accuracy of the Decision Tree is',accuracy_score(prediction,smoted_y_test))","575c3d2c":"test_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n  \n    knn.fit(smoted_X_train,smoted_y_train)\n    \n    train_scores.append(knn.score(smoted_X_train,smoted_y_train))\n    test_scores.append(knn.score(smoted_X_test,smoted_y_test))\n\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","fbc8ff08":"max_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","b44a932d":"knn_max_score=list(map(lambda x: x+1, test_scores_ind))[0]","81f23712":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')","19236c1d":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(knn_max_score)\n\nknn.fit(smoted_X_train,smoted_y_train)\nknn.score(smoted_X_test,smoted_y_test)","3ff05f21":"abc=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=knn_max_score),DecisionTreeClassifier()]\nfor i in models:\n    model = i\n    model.fit(smoted_X_train,smoted_y_train)\n    prediction=model.predict(smoted_X_test)\n    abc.append([roc_auc_score(smoted_y_test,prediction),accuracy_score(prediction,smoted_y_test)])\nmodels_dataframe=pd.DataFrame(abc,index=classifiers)   \nmodels_dataframe.columns=['AUC','Accuracy']\nmodels_dataframe","d6275e89":"smote=SMOTE(k_neighbors=5)\nsmoted_X,smoted_y=smote.fit_resample(df5,label)","a4f3ac44":"sns.heatmap(smoted_X[smoted_X.columns].corr(),annot=False,xticklabels=True,yticklabels=True,cmap='RdYlGn')\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","8f59f5fa":"df6=smoted_X.copy(deep=True)\ndef correlation(dataset, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (abs(corr_matrix.iloc[i, j]) >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n                if colname in dataset.columns:\n                    print('Delete: '+colname+\" \"+str(corr_matrix.iloc[i, j]))\n                    del dataset[colname] # deleting the column from the dataset\n    print(dataset)\n                    \ncorrelation(df6,0.8)","ffcb3b16":"from sklearn.ensemble import RandomForestClassifier \nmodel= RandomForestClassifier(n_estimators=100,random_state=0)\nX=df6[df6.columns]\nY=smoted_y\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)","70c684f3":"df5_label=df5.copy(deep=True)\ndf5_label['T2DM']=label\nsns.lmplot('T01_HBA1C','T01_GLU0', data=df5_label, fit_reg=False, scatter_kws={\"s\":50},markers=[\"o\",\"x\"],hue=\"T2DM\")\nplt.title('HBA1C and GLU0 in 2d plane')","9472bd48":"# smoted_X_simple=smoted_X.loc[:,['T01_HBA1C','T01_GLU0']]\n# smoted_X_simple=smoted_X.loc[:,['T01_HBA1C','T01_GLU0']]\n\n# smoted_X_train_simple=smoted_X_train.loc[:,['T01_HBA1C','T01_GLU0']]\n# smoted_X_test_simple=smoted_X_test.loc[:,['T01_HBA1C','T01_GLU0']]","b7f66f23":"# def svc_param_selection(kernel,X,y,nfolds):\n#     svm_parameters=[\n#         {'kernel':[kernel],\n#         'gamma':[0.00001,0.0001,0.001,0.01,0.1,1],\n#         'C':[0.01,0.1,1,10,100,1000]\n#         }\n#     ]\n#     clf=GridSearchCV(svm.SVC(),svm_parameters,cv=nfolds)\n#     clf.fit(X,y)\n#     return(clf.best_params_)\n\n# rbf_clf=svc_param_selection('rbf',smoted_X_train_simple,smoted_y_train.values.ravel(),10)\n# linear_clf=svc_param_selection('linear',smoted_X_train_simple,smoted_y_train.values.ravel(),10)\n# print(rbf_clf, linear_clf)","185f4ae5":"# def dt_param_selection(X,y):\n#     dt_parm={'criterion': ['gini', 'entropy'],'max_depth': [2,4,6,8,10,12]}\n    \n#     clf_GS=GridSearchCV(DecisionTreeClassifier(),dt_parm)\n#     clf_GS.fit(X,y)\n#     return(clf_GS.best_params_)\n    \n\n# best_dt_param=dt_param_selection(smoted_X_train_simple,smoted_y_train.values.ravel())\n# best_dt_param","b6a07125":"# abc=[]\n# classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree']\n# models=[svm.SVC(kernel='linear',C=linear_clf.get('C'),gamma=linear_clf.get('gamma')),svm.SVC(kernel='rbf',C=rbf_clf.get('C'),gamma=rbf_clf.get('gamma')),LogisticRegression(),KNeighborsClassifier(n_neighbors=4),DecisionTreeClassifier(criterion = best_dt_param.get('criterion'), max_depth = best_dt_param.get('max_depth'))]\n# for i in models:\n#     model = i\n#     model.fit(smoted_X_train_simple,smoted_y_train)\n#     prediction=model.predict(smoted_X_test_simple)\n#     abc.append(roc_auc_score(smoted_y_test,prediction))\n# new_models_dataframe=pd.DataFrame(abc,index=classifiers)   \n# new_models_dataframe.columns=['New AUC']    ","bf94c57d":"smoted_y.value_counts()","fe67dd13":"xyz=[]\nroc=[]\nclassifiers=['Linear Svm','Radial Svm','LR','KNN','Decision Tree']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(),DecisionTreeClassifier()]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,smoted_X_simple[['T01_HBA1C','T01_GLU0']],smoted_y, scoring = \"roc_auc\")\n    xyz.append(cv_result.mean())\n    roc.append(cv_result)\nnew_models_dataframe2=pd.DataFrame(xyz,index=classifiers)   \nnew_models_dataframe2.columns=['CV Mean']    \nnew_models_dataframe2","26b9a6ee":"box=pd.DataFrame(roc,index=classifiers)\nsns.boxplot(data=box.T)\nplt.show()","9ac87607":"# linear_svc=svm.SVC(kernel='linear')\n# radial_svm=svm.SVC(kernel='rbf')\n# lr=LogisticRegression()\n# knn=KNeighborsClassifier()\n# decision_tree=DecisionTreeClassifier()","4d005190":"# ensemble_lin_dt=VotingClassifier(estimators=[('Linear_svm', linear_svc), ('Decision Tree', decision_tree)], \n#                        voting='hard', weights=[2,1]).fit(smoted_X_train_simple,smoted_y_train)\n# print('The accuracy for Linear SVM and Decision tree is:',ensemble_lin_dt.score(smoted_X_test_simple,smoted_y_test))","682ea0ce":"# ensemble_lr_dt=VotingClassifier(estimators=[('Logistic regression', lr), ('Decision Tree', decision_tree)], \n#                        voting='hard', weights=[2,1]).fit(smoted_X_train_simple,smoted_y_train)\n# print('The accuracy for Logistic Regression and Decision Tree is:',ensemble_lr_dt.score(smoted_X_test_simple,smoted_y_test))","95c1a079":"# ensemble_knn_dt=VotingClassifier(estimators=[('KNN', knn), ('Decision Tree', decision_tree)], \n#                        voting='hard', weights=[1,2]).fit(smoted_X_train_simple,smoted_y_train)\n# print('The accuracy for Radial SVM and Logistic Regression is:',ensemble_knn_dt.score(smoted_X_test_simple,smoted_y_test))","7518a527":"# ensemble_rad_lr_lin=VotingClassifier(estimators=[('Radial_svm', radial_svm), ('Logistic Regression', lr),('Linear_svm',linear_svc)], \n#                        voting='soft', weights=[2,1,3]).fit(smoted_X_train_simple,smoted_y_train)\n# print('The ensembled model with all the 3 classifiers is:',ensemble_rad_lr_lin.score(smoted_X_test_simple,smoted_y_test))","16539f7d":"def dt_param_selection(X,y):\n    dt_parm={'criterion': ['gini', 'entropy'],'max_depth': [2,4,6,8,10,12]}\n    \n    clf_GS=GridSearchCV(DecisionTreeClassifier(),dt_parm)\n    clf_GS.fit(X,y)\n    return(clf_GS.best_params_)\n    \n\nbest_dt_param=dt_param_selection(smoted_X_train_simple,smoted_y_train.values.ravel())\nbest_dt_param","09c3bf65":"# def knn_param_selection(X,y):\n#     k_range = list(range(1,31))\n#     weight_options = [\"uniform\", \"distance\"]\n#     param_grid = dict(n_neighbors = k_range, weights = weight_options)\n#     knn = KNeighborsClassifier()\n#     grid = GridSearchCV(knn, param_grid, cv = 10, scoring = 'accuracy')\n#     grid.fit(X,y)\n#     return(grid.best_params_)\n\n# best_knn_param=knn_param_selection(smoted_X_train_simple,smoted_y_train.values.ravel())\n# best_knn_param","4460660a":"# new_knn=KNeighborsClassifier(n_neighbors=best_knn_param.get('n_neighbors'), weights=best_knn_param.get('weights'))\n# new_decision_tree=DecisionTreeClassifier(criterion=best_dt_param.get('criterion'), max_depth=best_dt_param.get('max_depth'))","8798129a":"abc=[]\nclassifiers=['Decision Tree']\nmodels=[DecisionTreeClassifier(criterion = best_dt_param.get('criterion'), max_depth = best_dt_param.get('max_depth'))]\nfor i in models:\n    model = i\n    model.fit(smoted_X_train_simple,smoted_y_train)\n    prediction=model.predict(smoted_X_test_simple)\n    abc.append(roc_auc_score(smoted_y_test,prediction))\nnew_models_dataframe=pd.DataFrame(abc,index=classifiers)   \nnew_models_dataframe.columns=['New AUC']    ","d2f70473":"new_models_dataframe=new_models_dataframe.merge(models_dataframe,left_index=True,right_index=True,how='left')\nnew_models_dataframe['Increase']=new_models_dataframe['New AUC']-new_models_dataframe['AUC']\nnew_models_dataframe","c7fde893":"clf = DecisionTreeClassifier(criterion = best_dt_param.get('criterion'), max_depth = best_dt_param.get('max_depth')).fit(smoted_X_simple[['T01_HBA1C','T01_GLU0']],smoted_y.ravel())\n","3c2e2fe0":"\nplot_decision_regions(smoted_X_simple.values, smoted_y.values.astype(np.integer), clf=clf, legend=2)\n\n# Adding axes annotations\nplt.xlabel('T01_HBA1C')\nplt.ylabel('T01_GLU0')\nplt.title('Decision Tree with Diabetes Data')\nplt.show()","33cebe87":"clf = DecisionTreeClassifier(criterion = best_dt_param.get('criterion'), max_depth = best_dt_param.get('max_depth')).fit(smoted_X_train_simple[['T01_HBA1C','T01_GLU0']],smoted_y_train.ravel())","51ed8451":"smoted_y_pred = clf.predict(smoted_X_test_simple)\ncf_matrix=confusion_matrix(smoted_y_test,smoted_y_pred)\npd.crosstab(smoted_y_test, smoted_y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","e148617a":"p = sns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","f1861707":"print(classification_report(smoted_y_test,smoted_y_pred))","37ab75db":"smoted_y_pred_proba = clf.predict_proba(smoted_X_test_simple)[:,1]\nfpr, tpr, thresholds = roc_curve(smoted_y_test, smoted_y_pred_proba)","93cd9bda":"plt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Decision Tree ROC curve')\nplt.show()","831bd84c":"roc_auc_score(smoted_y_test,smoted_y_pred_proba)\n","72615b91":"clf = DecisionTreeClassifier(criterion = best_dt_param.get('criterion'), max_depth = best_dt_param.get('max_depth')).fit(smoted_X_simple[['T01_HBA1C','T01_GLU0']],smoted_y.ravel())\ndot_data = tree.export_graphviz(clf, out_file=None)\ngraph = graphviz.Source(dot_data)\ngraph.render(\"T2DM\")\ndot_data = tree.export_graphviz(clf, out_file=None, feature_names=['T01_HBA1C','T01_GLU0'],class_names=['0.0','1.0'],filled=True, rounded=True, special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph","707f5e5c":"heatmap\uc73c\ub85c \ubcc0\uc218\uac04 \uc5f0\uad00\uc131 \uccb4\ud06c","055cffda":"Ensembling KNN with Decision Tree increase the performance of data prediction.","dd4eb456":"**SVM**","2e386c8b":"# Feature Extraction","52e3e0d8":"Random Forest Classifier","90e5429c":"# Recalculate using only important features","539a3f1b":"# Model Performace Analysis","12b127ae":"\uacb0\uacfc \ub3c4\ud45c\ud654\uc2dc\ud0a4\uae30","be172cb5":"All 3 classifiers combined","b8ecfb11":"**Logistic Regression**","75e4a958":"Confusion metrix \uc0dd\uc131\ud574\uc11c \uc815\ud655\ub3c4 \uacc4\uc0b0\ud558\uae30","0b20266f":"Standard scaling \uc218\ud589\ud558\uae30","c8130538":"\uc815\ud655\ub3c4 \uc0b0\ucd9c\ud558\uae30","6473e406":"Linear SVM with Decision Tree","eeed4004":"# Scrubbing\/Cleaning","f9fcb0b7":"NA\ub85c \ub418\uc5b4\uc788\uc73c\uba74 mean, median\uc73c\ub85c \ucc44\uc6b0\uae30","c704e4c4":"# Cross Validation","0c446111":"**Hyperparameter**","0a9f0eee":"Healthy vs T2DM \ube44\uad50 \ud788\uc2a4\ud1a0\uadf8\ub7a8","5e76302d":"ROC-AUC\uacc4\uc0b0\ud558\uae30","d0afba78":"categorical column\uc740 get_dummies\ub85c \ubcc0\uacbd","740f4c00":"**Visaulize data distribution on histogram**","b78b9b01":"# Convert categorical to value (get_dummies)","e9244c7b":"Linear SVM and decision tree","f17a2133":"**K-Nearest Neighbours**","6490d222":"**Observations:**\n1) The important features are: GLU0 and HBA1C","a5e84ef4":"Remove column with high correlation","ea7a74fc":"Answer_set \uceec\ub7fc \uc0dd\uc131\ud558\uace0, label \ubcc0\uc218 \uc0dd\uc131\ud558\uae30","c48c0c37":"# Decision Tree (2 columns) evaluation","f73e04ba":"**train\uacfc test \uc138\ud2b8\ub85c \ub098\ub204\uace0 SMOTE oversampling**","30841954":"T2DM\uc73c\ub85c \uad6c\ubd84\ub41c \ub370\uc774\ud130 \uc218\uac00 \uc801\uc73c\ubbc0\ub85c \uacfc\ub300\uc801\ud569\uc744 \ud53c\ud558\uae30\uc704\ud574 feature selection\uc744 \uc9c4\ud589","4a233712":"The above boxplot shows that Decision Tree model perform the best while Radial SVM performs the worst.","719e7ad3":"Decision Tree hyperparameter","1e5a57bd":"# Exploring \/ Visualizing","a8058ab4":"# Ensembling","dc860c22":"**Decision Tree**","795beb74":"SVC hyperparameter","399cdee2":"**In a Nutshell**","caf8926b":"KNN with Decision Tree","3609b9bc":"# Hyperparameter","533f4a40":"1\ucc28 \ucd94\uc801 \ub370\uc774\ud130\uc14b\uc5d0\uc11c T2DM \uacb0\uacfc label\ub9cc\ub4e4\uae30"}}