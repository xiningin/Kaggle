{"cell_type":{"ecc0747a":"code","055f4afb":"code","984753e1":"code","023a548e":"code","b008f282":"code","7453ac45":"code","cc6217e9":"code","cc29a2dc":"code","cdd44fa8":"code","705db5b5":"code","a0d9230a":"code","80071880":"code","b2207f37":"code","66a1b0ba":"code","9590a2d0":"code","a7439d35":"code","c890c9d8":"code","055829a3":"code","7eb2dcc2":"code","a90c5fd9":"code","ed9fb6cc":"code","d2644bf7":"code","ff82ce8c":"code","48d587c1":"code","ad6b92ac":"code","c5d4ca40":"code","80635b0c":"code","13e275ed":"code","3fc05dce":"code","784092d5":"code","88340b75":"code","e2d3820d":"code","a7bcd952":"code","8b810dea":"code","5fe10d7d":"code","92f6e4c7":"code","d5b85755":"code","154208f3":"code","2c5c8da6":"code","97d1e7a7":"code","60627a3c":"code","94d9bf35":"code","5f2c18ac":"code","cca16aa0":"code","9fba8624":"code","558f7904":"code","78f60d4d":"code","14fbaf1a":"code","9e5c5e84":"code","14331e35":"code","a2202853":"code","1625bb97":"code","77052e64":"markdown","c57c3c5b":"markdown","bf322ec8":"markdown","130d50c3":"markdown","db3b201e":"markdown","4b59a0ff":"markdown","a18733da":"markdown","4cf0ae7a":"markdown","5d8e358c":"markdown","b121805e":"markdown","abe5b87d":"markdown","c4b9f451":"markdown","1b7e5e92":"markdown","6e120d74":"markdown","d02019ab":"markdown"},"source":{"ecc0747a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nsns.set_style('darkgrid')\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","055f4afb":"train_data= pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntrain_data.head()","984753e1":"train_data.tail()","023a548e":"train_data.info()","b008f282":"train_data.describe()","7453ac45":"train_data.isna().sum()","cc6217e9":"train_data.drop(['enrollee_id', 'city'], axis=1, inplace=True)\ntrain_data","cc29a2dc":"num_cols= ['city_development_index' ,'training_hours']\ncat_cols= train_data.drop(['city_development_index' ,'training_hours', 'target'], axis=1).columns","cdd44fa8":"cat_cols","705db5b5":"import pandas_profiling\ntrain_data.profile_report()","a0d9230a":"train_data['company_size']= train_data['company_size'].replace('10\/49', '10-49')","80071880":"exp_counts= train_data['experience'].value_counts()\nexp_counts","b2207f37":"mask= train_data['experience'].isin(exp_counts[exp_counts<600].index)\ntrain_data['experience'][mask]= 'other'\ntrain_data['experience'].value_counts()","66a1b0ba":"train_data.head()","9590a2d0":"from sklearn.preprocessing import LabelEncoder\n\nle= LabelEncoder()\n\nfor col in cat_cols:\n    train_data[col]= le.fit_transform(train_data[col])\n","a7439d35":"train_data","c890c9d8":"train_data.head()","055829a3":"plt.figure(figsize=(10,8))\nsns.heatmap(train_data.corr(), annot=True)","7eb2dcc2":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression\n\nlr= LinearRegression()\n\nimputer = IterativeImputer(random_state=42, estimator=lr, max_iter=10, n_nearest_features=2, imputation_order = 'roman')\ntrain_data_final = imputer.fit_transform(train_data.drop('target', axis=1))\n\ntrain_data_final = pd.DataFrame(train_data_final, columns = train_data.drop('target', axis=1).columns)\n\n","a90c5fd9":"train_data_final.isna().sum()","ed9fb6cc":"train_data_final.head()","d2644bf7":"train_data['target']= train_data['target'].map(lambda x: 1 if x==1.0 else 0)","ff82ce8c":"plt.figure(figsize=(10,8))\nsns.heatmap(train_data_final.corr(), annot=True)","48d587c1":"for col in cat_cols:\n    plt.figure()\n    sns.countplot(train_data_final[col])\n        \n        \nplt.show()","ad6b92ac":"for col in num_cols:\n    plt.figure()\n    sns.histplot(train_data_final[col])\n    \nplt.show()","c5d4ca40":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import normalize\nfrom sklearn.model_selection import train_test_split\n\nX= train_data_final\ny= train_data['target']\nX_train, X_test,y_train,y_test= train_test_split(X,y, test_size=0.2, stratify=y,random_state=42)\n\n","80635b0c":"X_train","13e275ed":"y_train","3fc05dce":"ss= StandardScaler()\nX_train[num_cols]= ss.fit_transform(X_train[num_cols])\nX_test[num_cols]= ss.transform(X_test[num_cols])","784092d5":"X_train","88340b75":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV","e2d3820d":"key= ['LogisticRegression', 'DecisionTreeRegressor', 'DecisionTreeClassifier',  'RandomForestClassifier', 'KNeighborsClassifier', 'GaussianNB', 'XGBClassifier', 'SVC']\n\nvalue= [LogisticRegression(), DecisionTreeRegressor() , DecisionTreeClassifier() ,  RandomForestClassifier() ,  KNeighborsClassifier(), GaussianNB() , XGBClassifier(), SVC()]\nmodels= dict(zip(key, value))","a7bcd952":"models","8b810dea":"scores=[]\nfor key,value in models.items():\n    score= -1*cross_val_score(value, X,y, cv=5, scoring='neg_mean_absolute_error')\n    scores.append(score)\n    print(key, score.mean())","5fe10d7d":"accuracy_scores=[]\nfor key,value in models.items():\n    value.fit(X_train,y_train)\n    y_pred= value.predict(X_test)\n    accuracy= value.score(X_test,y_test)\n    \n    accuracy_scores.append(accuracy)\n    print(key, accuracy)","92f6e4c7":"rfc= RandomForestClassifier(random_state=42)\nrfc.fit(X_train,y_train)\ny_pred= rfc.predict(X_test)\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n","d5b85755":"xgb= XGBClassifier(random_state=42)\nxgb.fit(X_train,y_train)\ny_pred= xgb.predict(X_test)\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","154208f3":"params= {'objective':['binary:logistic'],\n              'max_depth': [3,4,5,6],\n              'min_child_weight': [1,5,10,12],\n              'subsample': [0.6,0.8,1.0],\n              'colsample_bytree': [0.6,0.8,1.0], 'gamma': [0.5,1,1.5,2]}\n\nxgb= XGBClassifier(n_estimators=600)","2c5c8da6":"grid= RandomizedSearchCV(xgb, cv=5, verbose=3, param_distributions=params, n_iter=5)\n\ngrid.fit(X,y)","97d1e7a7":"grid.best_score_","60627a3c":"grid.best_estimator_","94d9bf35":"grid.best_estimator_.fit(X_train,y_train)\ny_pred= grid.best_estimator_.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","5f2c18ac":"test_data= pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\ntest_data","cca16aa0":"test_data.drop(['enrollee_id', 'city'], axis=1, inplace=True)\ntest_data","9fba8624":"mask= test_data['experience'].isin(exp_counts[exp_counts<600].index)\ntest_data['experience'][mask]= 'other'\ntest_data","558f7904":"for col in cat_cols:\n    test_data[col]= le.fit_transform(test_data[col])\ntest_data","78f60d4d":"test_data_final = imputer.transform(test_data)\n\ntest_data_final = pd.DataFrame(test_data_final, columns = test_data.columns)","14fbaf1a":"test_data_final[num_cols]= ss.transform(test_data_final[num_cols])","9e5c5e84":"test_data_final","14331e35":"test_data_final.info()","a2202853":"predictions= grid.best_estimator_.predict(test_data_final)","1625bb97":"my_array = predictions\nprint(my_array)\nnp. set_printoptions(threshold=np. inf)\nprint(my_array)\n\n","77052e64":"**XGBClassifier and RandomForestClassifier are the best models**","c57c3c5b":"# Exploratory Data Analysis","bf322ec8":"**Dividing the columns into categorical and numerical for ease in future**","130d50c3":"# Training the Data","db3b201e":"# Preprocessing the Data","4b59a0ff":"# Dealing with Missing Values","a18733da":"**Many values for experience so grouping all values under 600 to one category in order to prevent too many variables while encoding**","4cf0ae7a":"# Test Data","5d8e358c":"# Final Predictions on Test Dataset","b121805e":"# Final Checking the data after preprocessing","abe5b87d":"**XGBClassifier is the best model for this data**","c4b9f451":"# Reading the Data","1b7e5e92":"# Encoding Categorical Variables","6e120d74":"**The numerical columns need to be standardized**","d02019ab":"# Tuning the Hyperparameters"}}