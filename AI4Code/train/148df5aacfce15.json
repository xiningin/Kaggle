{"cell_type":{"0bfab3dc":"code","18a51107":"code","7660544f":"code","e211e271":"code","75655c35":"code","dc82570d":"code","72a16714":"code","6033b262":"code","1cc35147":"code","54f11b10":"code","43e56a02":"code","d7d8a2f4":"code","7757377d":"code","271abec7":"code","69813d51":"code","ceed887f":"code","42b86149":"code","f88c7a68":"code","100eef35":"code","9212b7d0":"code","36215dc1":"code","a5bb2836":"code","2ee7509f":"code","68a6ecc9":"code","bc22e0af":"code","2bfb09c5":"code","b83e553e":"code","1c6062e4":"code","955a92b1":"code","943d6257":"code","1c8990ec":"code","8399b57d":"code","9d779879":"code","419666cb":"code","321b8195":"code","b64b5034":"code","4a9338b2":"code","fc7113c0":"code","07df9157":"code","01db5473":"code","9024c9b1":"code","2bebc2ce":"code","16e2ec48":"code","56824f6a":"code","f554b2ba":"code","49b7e28f":"code","98623e82":"code","6562d8c5":"code","4c7bf82a":"code","56cd975c":"code","7edfd766":"code","efae9682":"code","7bb9537d":"code","3c90f1ef":"code","4bc74d45":"code","0da957f0":"code","57f3714e":"code","b280db7c":"code","b55f1b93":"code","7bc3959e":"code","61e9ccac":"code","87bda47d":"code","1679825e":"code","cceef95b":"code","d60c1dc2":"code","72f04acc":"code","b1ca8199":"code","7bd9e757":"code","009517c8":"code","f125c8e8":"code","e1216bdf":"code","cf630778":"code","0165b6c4":"markdown","e339ad2d":"markdown","1ba68507":"markdown","ef5372d9":"markdown","4a67b0e8":"markdown","4935e1eb":"markdown","396e9a55":"markdown","5c935b0d":"markdown","0790801c":"markdown","1640c457":"markdown","1204a183":"markdown","b10ecb01":"markdown","61041a20":"markdown","a31278cc":"markdown","048d1bc3":"markdown","f70f9479":"markdown","43252954":"markdown","603c0c73":"markdown","9b0670a7":"markdown","e397363c":"markdown","97a7d181":"markdown","7d13cecf":"markdown","be714434":"markdown","5e4415b7":"markdown","36be9937":"markdown","57e60146":"markdown","b64e50b5":"markdown","5317936d":"markdown","5400bcca":"markdown","dc3897b9":"markdown","0419d649":"markdown","cb692d61":"markdown","7a75c1e4":"markdown","aad52fb6":"markdown","87149028":"markdown","737426b1":"markdown","c1d4a29e":"markdown","2106df10":"markdown","f079d58b":"markdown","66d403dc":"markdown","0b8e8b86":"markdown","d2b05044":"markdown","85ebff3b":"markdown","6461e2b0":"markdown","3f91236d":"markdown","0b293ebe":"markdown","ac439e71":"markdown","8abc113c":"markdown","49ce4f33":"markdown","47ec50b6":"markdown","446bef05":"markdown","edb45615":"markdown","1ed80c35":"markdown","649c06b3":"markdown","1e4e33db":"markdown","6ddf08c4":"markdown","0a12b2c9":"markdown","6d53c6ce":"markdown","5fd85159":"markdown","652e826f":"markdown","7ab16bc8":"markdown","58a81ddd":"markdown","a64c9ab4":"markdown","0c97d229":"markdown","7017c565":"markdown","a50bd855":"markdown","e007cca9":"markdown","1029192e":"markdown","17c76397":"markdown","61fe6f1f":"markdown","65634409":"markdown","72b4d310":"markdown","cf18a8bd":"markdown","a8433bb4":"markdown","a0607f7f":"markdown","67b86030":"markdown","d509f2ee":"markdown","ab85f67c":"markdown","44d0ba57":"markdown","563e7fb1":"markdown","2b0da06d":"markdown","ee017ead":"markdown","84202153":"markdown","0227e650":"markdown","53656e05":"markdown","f9100f60":"markdown","f869a1b4":"markdown"},"source":{"0bfab3dc":"from IPython.display import HTML\nHTML('<center><iframe width=\"640\" height=\"360\" src=\"https:\/\/player.vimeo.com\/video\/389096888\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe><\/center>')","18a51107":"import os\nfrom os import listdir\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport numpy as np\n\n#color\nfrom colorama import Fore, Back, Style\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","7660544f":"# List files available\nlist(os.listdir(\"..\/input\/lyft-motion-prediction-autonomous-vehicles\"))","e211e271":"single_mode_sample_submission = pd.read_csv('..\/input\/lyft-motion-prediction-autonomous-vehicles\/multi_mode_sample_submission.csv')\nmulti_mode_sample_submission = pd.read_csv('..\/input\/lyft-motion-prediction-autonomous-vehicles\/single_mode_sample_submission.csv')","75655c35":"print(Fore.YELLOW + 'Sample submission for single mode shape: ',Style.RESET_ALL,single_mode_sample_submission.shape)\nsingle_mode_sample_submission.head(5)","dc82570d":"print(Fore.BLUE + 'Sample submission for multi mode shape: ',Style.RESET_ALL,multi_mode_sample_submission.shape)\nmulti_mode_sample_submission.head(5)","72a16714":"# Null values and Data types\nprint(Fore.YELLOW + 'Single Mode Sample Submission !!',Style.RESET_ALL)\nprint(single_mode_sample_submission.info())\nprint('-------------')\nprint(Fore.BLUE + 'Multi Mode Sample Submission !!',Style.RESET_ALL)\nprint(multi_mode_sample_submission.info())","6033b262":"!pip install --upgrade pip > \/dev\/null \n!pip uninstall typing -y > \/dev\/null \n!pip install --ignore-installed --target=\/kaggle\/working l5kit > \/dev\/null ","1cc35147":"from l5kit.rasterization import build_rasterizer","54f11b10":"from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR","43e56a02":"from l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\n\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable","d7d8a2f4":"cfg = {\n    #'format_version': 4,   \n    'raster_params': {\n        'raster_size': [224, 224],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map\/aerial_map.png',\n        'semantic_map_key': 'semantic_map\/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    \n    'val_data_loader': {\n        'key': 'scenes\/train.zarr',\n        'batch_size': 12,\n        'shuffle': True,\n        'num_workers': 16\n    },\n}","7757377d":"print(f'current raster_param:\\n')\nfor k,v in cfg[\"raster_params\"].items():\n    print(Fore.YELLOW + f\"{k}\",Style.RESET_ALL + f\":{v}\")","271abec7":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"","69813d51":"dm = LocalDataManager()\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","ceed887f":"import zarr\ntrain_zarr = zarr.open(\"..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/train.zarr\")\n\nprint(type(train_zarr))","42b86149":"train_zarr.info","f88c7a68":"fields = [\n    \"Num Scenes\",\n    \"Num Frames\",\n    \"Num Agents\",\n    \"Total Time (hr)\",\n    \"Avg Frames per Scene\",\n    \"Avg Agents per Frame\",\n    \"Avg Scene Time (sec)\",\n    \"Avg Frame frequency\",\n]","100eef35":"if len(zarr_dataset.frames) > 1:\n    times = zarr_dataset.frames[1:50][\"timestamp\"] - zarr_dataset.frames[0:49][\"timestamp\"]\n    frequency = np.mean(1 \/ (times \/ 1e9))  # from nano to sec\nelse:\n    print(f\"warning, not enough frames({len(zarr_dataset.frames)}) to read the frequency, 10 will be set\")\n    frequency = 10","9212b7d0":"values = [\n    len(zarr_dataset.scenes),\n    len(zarr_dataset.frames),\n    len(zarr_dataset.agents),\n    len(zarr_dataset.frames) \/ max(frequency, 1) \/ 3600,\n    len(zarr_dataset.frames) \/ max(len(zarr_dataset.scenes), 1),\n    len(zarr_dataset.agents) \/ max(len(zarr_dataset.frames), 1),\n    len(zarr_dataset.frames) \/ max(len(zarr_dataset.scenes), 1) \/ frequency,\n    frequency,\n]","36215dc1":"table = PrettyTable(field_names=[fields[0]])\ntable.add_row([values[0]])","a5bb2836":"print(Fore.YELLOW + str(table) + Style.RESET_ALL)","2ee7509f":"print(Fore.YELLOW + table.get_string(fields=[\"Num Scenes\"]) + Style.RESET_ALL)","68a6ecc9":"table = PrettyTable(field_names=[fields[1]])\ntable.add_row([values[1]])\nprint(Fore.BLUE + str(table) + Style.RESET_ALL)","bc22e0af":"table = PrettyTable(field_names=[fields[2]])\ntable.add_row([values[2]])\nprint(Fore.YELLOW + str(table) + Style.RESET_ALL)","2bfb09c5":"table = PrettyTable(field_names=[fields[3]])\ntable.float_format = \".2\"\ntable.add_row([values[3]])\nprint(Fore.BLUE + str(table) + Style.RESET_ALL)","b83e553e":"table = PrettyTable(field_names=[fields[4]])\ntable.float_format = \".2\"\ntable.add_row([values[4]])\nprint(Fore.YELLOW + str(table) + Style.RESET_ALL)","1c6062e4":"table = PrettyTable(field_names=[fields[5]])\ntable.float_format = \".2\"\ntable.add_row([values[5]])\nprint(Fore.BLUE + str(table) + Style.RESET_ALL)","955a92b1":"table = PrettyTable(field_names=[fields[6]])\ntable.float_format = \".2\"\ntable.add_row([values[6]])\nprint(Fore.YELLOW + str(table) + Style.RESET_ALL)","943d6257":"agents = pd.read_csv('..\/input\/lyft-motion-prediction-autonomous-vehicles-as-csv\/agents_0_10019001_10019001.csv')\nagents","1c8990ec":"cont_feats = [\"centroid_x\", \"centroid_y\", \"extent_x\", \"extent_y\", \"extent_z\", \"yaw\"]\nfig = px.imshow(agents[cont_feats].corr(),\n                labels=dict(x=\"Correlation of features\", y=\"\", color=\"Correlation\"),\n                x=[\"centroid_x\", \"centroid_y\", \"extent_x\", \"extent_y\", \"extent_z\", \"yaw\"],\n                y=[\"centroid_x\", \"centroid_y\", \"extent_x\", \"extent_y\", \"extent_z\", \"yaw\"]\n               )\nplt.figure(figsize=(16,12));\nfig.update_xaxes(side=\"top\")\nfig.show()","8399b57d":"fig = plt.figure(figsize=(16, 12));\nsns.distplot(agents['centroid_x'], color='steelblue');\nsns.distplot(agents['centroid_y'], color='red');\nplt.title(\"Distributions of Centroid X and Y\");","9d779879":"fig = plt.figure(figsize=(16, 12));\nsns.distplot(agents['extent_x'], color='steelblue');\nsns.distplot(agents['extent_y'], color='red');\n\nplt.title(\"Distributions of Extents X and Y\");","419666cb":"fig = plt.figure(figsize=(16, 12));\nsns.distplot(agents['extent_z'], color='blue');\n\nplt.title(\"Distributions of Extents z\");","321b8195":"fig = plt.figure(figsize=(16, 12));\nsns.distplot(agents['yaw'], color='blue');\n\nplt.title(\"Distributions of Extents z\");","b64b5034":"frms = pd.read_csv(\"..\/input\/lyft-motion-prediction-autonomous-vehicles-as-csv\/frames_0_124167_124167.csv\")\nfrms.head()","4a9338b2":"import seaborn as sns\ncolormap = plt.cm.magma\ncont_feats = [\"ego_rotation_xx\", \"ego_rotation_xy\", \"ego_rotation_xz\", \"ego_rotation_yx\", \"ego_rotation_yy\", \"ego_rotation_yz\", \"ego_rotation_zx\", \"ego_rotation_zy\", \"ego_rotation_zz\"]\nplt.figure(figsize=(16,12));\nplt.title('Pearson correlation of features', y=1.05, size=15);\nsns.heatmap(frms[cont_feats].corr(),linewidths=0.1,vmax=1.0, square=True, \n            cmap=colormap, linecolor='white', annot=True)","fc7113c0":"cont_feats = [\"ego_rotation_xx\", \"ego_rotation_xy\", \"ego_rotation_xz\", \"ego_rotation_yx\", \"ego_rotation_yy\", \"ego_rotation_yz\", \"ego_rotation_zx\", \"ego_rotation_zy\", \"ego_rotation_zz\"]\nfig = px.imshow(frms[cont_feats].corr(),\n                labels=dict(x=\"Correlation of features\", y=\"\", color=\"Correlation\"),\n                x=[\"ego_rotation_xx\", \"ego_rotation_xy\", \"ego_rotation_xz\", \"ego_rotation_yx\", \"ego_rotation_yy\", \"ego_rotation_yz\", \"ego_rotation_zx\", \"ego_rotation_zy\", \"ego_rotation_zz\"],\n                y=[\"ego_rotation_xx\", \"ego_rotation_xy\", \"ego_rotation_xz\", \"ego_rotation_yx\", \"ego_rotation_yy\", \"ego_rotation_yz\", \"ego_rotation_zx\", \"ego_rotation_zy\", \"ego_rotation_zz\"]\n               )\nplt.figure(figsize=(16,12));\nfig.update_xaxes(side=\"top\")\nfig.show()","07df9157":"import numpy as np\nzero_count_list, one_count_list = [], []\ncols_list = [\"label_probabilities_PERCEPTION_LABEL_UNKNOWN\",\"label_probabilities_PERCEPTION_LABEL_CAR\",\"label_probabilities_PERCEPTION_LABEL_CYCLIST\",\"label_probabilities_PERCEPTION_LABEL_PEDESTRIAN\"]\nfor col in cols_list:\n    zero_count_list.append((agents[col]==0).sum())\n    one_count_list.append((agents[col]==1).sum())\n\nN = len(cols_list)\nind = np.arange(N)\nwidth = 0.35\n\nplt.figure(figsize=(6,10))\np1 = plt.barh(ind, zero_count_list, width, color='red')\np2 = plt.barh(ind, one_count_list, width, left=zero_count_list, color=\"blue\")\nplt.yticks(ind, cols_list)\nplt.legend((p1[0], p2[0]), ('Zero count', 'One Count'))\nplt.show()","01db5473":"#plotly\n!pip install chart_studio\nimport plotly.express as px\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')","9024c9b1":"cfg = {\n    'raster_params': {\n        'raster_size': [224, 224],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map\/aerial_map.png',\n        'semantic_map_key': 'semantic_map\/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    \n    'val_data_loader': {\n        'key': 'scenes\/sample.zarr',\n        'batch_size': 12,\n        'shuffle': True,\n        'num_workers': 16\n    },\n}","2bebc2ce":"dm = LocalDataManager()\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","16e2ec48":"agents = zarr_dataset.agents\nagents_df = pd.DataFrame(agents)\nagents_df.columns = [\"data\"]; features = ['centroid', 'extent', 'yaw', 'velocity', 'track_id', 'label_probabilities']\n\nfor i, feature in enumerate(features):\n    agents_df[feature] = agents_df['data'].apply(lambda x: x[i])\nagents_df.drop(columns=[\"data\"],inplace=True)\nprint(f\"agents dataset: {agents_df.shape}\")\nagents_df.head()","56824f6a":"agents_df['cx'] = agents_df['centroid'].apply(lambda x: x[0])\nagents_df['cy'] = agents_df['centroid'].apply(lambda x: x[1])\n\nfig, ax = plt.subplots(1,1,figsize=(8,8))\nplt.scatter(agents_df['cx'], agents_df['cy'], marker='+')\nplt.xlabel('x', fontsize=11); plt.ylabel('y', fontsize=11)\nplt.title(\"Centroids distribution\")\nplt.show()","f554b2ba":"agents_df['ex'] = agents_df['extent'].apply(lambda x: x[0])\nagents_df['ey'] = agents_df['extent'].apply(lambda x: x[1])\nagents_df['ez'] = agents_df['extent'].apply(lambda x: x[2])\n\nsns.set_style('whitegrid')\n\nfig, ax = plt.subplots(1,3,figsize=(16,5))\nplt.subplot(1,3,1)\nplt.scatter(agents_df['ex'], agents_df['ey'], marker='+')\nplt.xlabel('ex', fontsize=11); plt.ylabel('ey', fontsize=11)\nplt.title(\"Extent: ex-ey\")\nplt.subplot(1,3,2)\nplt.scatter(agents_df['ey'], agents_df['ez'], marker='+', color=\"red\")\nplt.xlabel('ey', fontsize=11); plt.ylabel('ez', fontsize=11)\nplt.title(\"Extent: ey-ez\")\nplt.subplot(1,3,3)\nplt.scatter(agents_df['ez'], agents_df['ex'], marker='+', color=\"green\")\nplt.xlabel('ez', fontsize=11); plt.ylabel('ex', fontsize=11)\nplt.title(\"Extent: ez-ex\")\nplt.show();","49b7e28f":"agents_df['vx'] = agents_df['velocity'].apply(lambda x: x[0])\nagents_df['vy'] = agents_df['velocity'].apply(lambda x: x[1])\n\nfig, ax = plt.subplots(1,1,figsize=(8,8))\nplt.title(\"Velocity distribution\")\nplt.scatter(agents_df['vx'], agents_df['vy'], marker='.', color=\"red\")\nplt.xlabel('vx', fontsize=11); plt.ylabel('vy', fontsize=11)\nplt.show();","98623e82":"scenes = zarr_dataset.scenes\nscenes_df = pd.DataFrame(scenes)\nscenes_df.columns = [\"data\"]; features = ['frame_index_interval', 'host', 'start_time', 'end_time']\nfor i, feature in enumerate(features):\n    scenes_df[feature] = scenes_df['data'].apply(lambda x: x[i])\nscenes_df.drop(columns=[\"data\"],inplace=True)\nprint(f\"scenes dataset: {scenes_df.shape}\")\nscenes_df.head()","6562d8c5":"f, ax = plt.subplots(1,1, figsize=(6,4))\nsns.countplot(scenes_df.host)\nplt.xlabel('Host')\nplt.ylabel(f'Count')\nplt.title(\"Scenes host count distribution\")\nplt.show()","4c7bf82a":"scenes_df['frame_index_start'] = scenes_df['frame_index_interval'].apply(lambda x: x[0])\nscenes_df['frame_index_end'] = scenes_df['frame_index_interval'].apply(lambda x: x[1])\nscenes_df.head()","56cd975c":"frames = zarr_dataset.frames\ncoords = np.zeros((len(frames), 2))\nfor idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\n    frame = zarr_dataset.frames[idx_data]\n    coords[idx_coord] = frame[\"ego_translation\"][:2]","7edfd766":"fig = go.Figure(data=go.Scatter(x=coords[:, 0], y=coords[:, 1], mode='lines'))\nfig.show()","efae9682":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=coords[:, 0], y=coords[:, 1], mode='lines', name='lines'))\nfig.add_trace(go.Scatter(x=coords[:, 0], y=coords[:, 1], mode='markers', name='lines+markers'))\nfig.show()","7bb9537d":"frames_df = pd.DataFrame(zarr_dataset.frames)\nframes_df.columns = [\"data\"]; features = ['timestamp', 'agent_index_interval', 'traffic_light_faces_index_interval', \n                                          'ego_translation','ego_rotation']\nfor i, feature in enumerate(features):\n    frames_df[feature] = frames_df['data'].apply(lambda x: x[i])\nframes_df.drop(columns=[\"data\"],inplace=True)\nprint(f\"frames dataset: {frames_df.shape}\")\nframes_df.head()","3c90f1ef":"frames_df['dx'] = frames_df['ego_translation'].apply(lambda x: x[0])\nframes_df['dy'] = frames_df['ego_translation'].apply(lambda x: x[1])\nframes_df['dz'] = frames_df['ego_translation'].apply(lambda x: x[2])","4bc74d45":"sns.set_style('whitegrid')\nplt.figure()\n\nfig, ax = plt.subplots(1,3,figsize=(16,5))\n\nplt.subplot(1,3,1)\nplt.scatter(frames_df['dx'], frames_df['dy'], marker='+')\nplt.xlabel('dx', fontsize=11); plt.ylabel('dy', fontsize=11)\nplt.title(\"Translations: dx-dy\")\nplt.subplot(1,3,2)\nplt.scatter(frames_df['dy'], frames_df['dz'], marker='+', color=\"red\")\nplt.xlabel('dy', fontsize=11); plt.ylabel('dz', fontsize=11)\nplt.title(\"Translations: dy-dz\")\nplt.subplot(1,3,3)\nplt.scatter(frames_df['dz'], frames_df['dx'], marker='+', color=\"green\")\nplt.xlabel('dz', fontsize=11); plt.ylabel('dx', fontsize=11)\nplt.title(\"Translations: dz-dx\")\n\nfig.suptitle(\"Ego translations in 2D planes of the 3 components (dx,dy,dz)\")\nplt.show();","0da957f0":"fig, ax = plt.subplots(3,3,figsize=(12,12))\ncolors = ['magenta', 'orange', 'darkblue', 'black', 'cyan', 'darkgreen', 'red', 'blue', 'green']\nfor i in range(0,3):\n    for j in range(0,3):\n        df = frames_df['ego_rotation'].apply(lambda x: x[i][j])\n        plt.subplot(3,3,i * 3 + j + 1)\n        sns.distplot(df, hist=False, color = colors[ i * 3 + j  ])\n        plt.xlabel(f'r[ {i + 1} ][ {j + 1} ]')\nfig.suptitle(\"Ego rotation angles distribution\")\nplt.show()","57f3714e":"agents = zarr_dataset.agents\nprobabilities = agents[\"label_probabilities\"]\nlabels_indexes = np.argmax(probabilities, axis=1)\ncounts = []\nfor idx_label, label in enumerate(PERCEPTION_LABELS):\n    counts.append(np.sum(labels_indexes == idx_label))\n    \ntable = PrettyTable(field_names=[\"label\", \"counts\"])\nfor count, label in zip(counts, PERCEPTION_LABELS):\n    table.add_row([label, count])\nprint(table)","b280db7c":"cfg = {\n    'model_params': {\n        'model_architecture': 'resnet50',\n        'history_num_frames': 10,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n    'raster_params': {\n        'raster_size': [224, 224],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map\/aerial_map.png',\n        'semantic_map_key': 'semantic_map\/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },    \n    'val_data_loader': {\n        'key': 'scenes\/sample.zarr',\n        'batch_size': 12,\n        'shuffle': True,\n        'num_workers': 16\n    },\n}","b55f1b93":"rast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)","7bc3959e":"from l5kit.geometry import transform_points\n\nfrom l5kit.visualization import (draw_trajectory,       # draws 2D trajectories from coordinates and yaws offset on an image\n                                 TARGET_POINTS_COLOR)\n\ndata = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nfig = px.imshow(im[::-1])\nfig.show()","61e9ccac":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\n\n# EgoDataset object\ndataset = EgoDataset(cfg, zarr_dataset, rast)\ndata = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nfig = px.imshow(im[::-1], title='Satellite View: Ground Truth Trajectory of Autonomous Vehicle')\nfig.show()","87bda47d":"dataset = AgentDataset(cfg, zarr_dataset, rast)\ndata = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nfig = px.imshow(im[::-1])\nfig.update_layout(\n    title={\n        'text': \"Agent\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","1679825e":"from IPython.display import display, clear_output\nfrom IPython.display import HTML\n\nimport PIL\nfrom matplotlib import animation","cceef95b":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfig = plt.figure(figsize = (10,10))\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    im = plt.imshow(PIL.Image.fromarray(im[::-1]), animated=True)\n    plt.axis(\"off\")\n    images.append([im])\nani = animation.ArtistAnimation(fig, images, interval=100, blit=False, repeat_delay=1000)","d60c1dc2":"HTML(ani.to_jshtml())","72f04acc":"cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfig = plt.figure(figsize = (10,10))\n\nfor idx in indexes:\n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    im = plt.imshow(PIL.Image.fromarray(im[::-1]), animated=True)\n    plt.axis(\"off\")\n    images.append([im])\nani = animation.ArtistAnimation(fig, images, interval=100, blit=False, repeat_delay=1000)","b1ca8199":"HTML(ani.to_jshtml())","7bd9e757":"# satellite view\ncfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfig = plt.figure(figsize = (10,10))\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    im = plt.imshow(PIL.Image.fromarray(im[::-1]), animated=True)\n    plt.axis(\"off\")\n    images.append([im])\nani = animation.ArtistAnimation(fig, images, interval=100, blit=False, repeat_delay=1000)","009517c8":"HTML(ani.to_jshtml())","f125c8e8":"from l5kit.data.map_api import MapAPI\nfrom l5kit.rasterization.rasterizer_builder import _load_metadata\n\nsemantic_map_filepath = dm.require(cfg[\"raster_params\"][\"semantic_map_key\"])\ndataset_meta = _load_metadata(cfg[\"raster_params\"][\"dataset_meta_key\"], dm)\nworld_to_ecef = np.array(dataset_meta[\"world_to_ecef\"], dtype=np.float64)\n\nmap_api = MapAPI(semantic_map_filepath, world_to_ecef)\nMAP_LAYERS = [\"junction\", \"node\", \"segment\", \"lane\"]\n\n\ndef element_of_type(elem, layer_name):\n    return elem.element.HasField(layer_name)\n\n\ndef get_elements_from_layer(map_api, layer_name):\n    return [elem for elem in map_api.elements if element_of_type(elem, layer_name)]\n\n\nclass MapRenderer:\n    \n    def __init__(self, map_api):\n        self._color_map = dict(drivable_area='#a6cee3',\n                               road_segment='#1f78b4',\n                               road_block='#b2df8a',\n                               lane='#474747')\n        self._map_api = map_api\n    \n    def render_layer(self, layer_name):\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_axes([0, 0, 1, 1])\n        \n    def render_lanes(self):\n        all_lanes = get_elements_from_layer(self._map_api, \"lane\")\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_axes([0, 0, 1, 1])\n        for lane in all_lanes:\n            self.render_lane(ax, lane)\n        return fig, ax\n        \n    def render_lane(self, ax, lane):\n        coords = self._map_api.get_lane_coords(MapAPI.id_as_str(lane.id))\n        self.render_boundary(ax, coords[\"xyz_left\"])\n        self.render_boundary(ax, coords[\"xyz_right\"])\n        \n    def render_boundary(self, ax, boundary):\n        xs = boundary[:, 0]\n        ys = boundary[:, 1] \n        ax.plot(xs, ys, color=self._color_map[\"lane\"], label=\"lane\")\n        \n        \nrenderer = MapRenderer(map_api)\nfig, ax = renderer.render_lanes()","e1216bdf":"def visualize_rgb_image(dataset, index, title=\"\", ax=None):\n    \"\"\"Visualizes Rasterizer's RGB image\"\"\"\n    data = dataset[index]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n\n    if ax is None:\n        fig, ax = plt.subplots()\n    if title:\n        ax.set_title(title)\n    ax.imshow(im[::-1])\n# Prepare all rasterizer and EgoDataset for each rasterizer\nrasterizer_dict = {}\ndataset_dict = {}","cf630778":"rasterizer_type_list = [\"py_satellite\", \"satellite_debug\", \"py_semantic\", \"semantic_debug\", \"box_debug\", \"stub_debug\"]\n\nfor i, key in enumerate(rasterizer_type_list):\n    # print(\"key\", key)\n    cfg[\"raster_params\"][\"map_type\"] = key\n    rasterizer_dict[key] = build_rasterizer(cfg, dm)\n    dataset_dict[key] = EgoDataset(cfg, zarr_dataset, rasterizer_dict[key])\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\nfor i, key in enumerate([\"stub_debug\", \"satellite_debug\", \"semantic_debug\", \"box_debug\", \"py_satellite\", \"py_semantic\"]):\n    visualize_rgb_image(dataset_dict[key], index=0, title=f\"{key}: {type(rasterizer_dict[key]).__name__}\", ax=axes[i])\nfig.show()","0165b6c4":"* The rotation coordinates with y and z : uncorrelated","e339ad2d":"We just change `scenes\/train.zarr` to `scenes\/sample.zarr` in cfg and re-load dataset.","1ba68507":"I found [Lyft Dataset for csv](http:\/\/https:\/\/www.kaggle.com\/kneroma\/lyft-motion-prediction-autonomous-vehicles-as-csv). So, Let's try EDA using this.","ef5372d9":"### Centroid distribution","4a67b0e8":"## Inside the dataset","4935e1eb":"## Visualising the Autonomous Vehicle (AV)\n","396e9a55":"### binary features","5c935b0d":"`9`: ego rotation columns corresponding to each","0790801c":"# 2. <a id='importing'>Importing the necessary libraries\ud83d\udcd7<\/a> ","1640c457":"## Agents","1204a183":"for more detailed understanding, we can visualize the satellite view.","b10ecb01":"### Rasterization","61041a20":"`float64`: The type of columns except for 2\n<p><\/p>\n    \n`int64` : 2 columns - timestamp, track_id","a31278cc":"Using below codes, we can `label_probabilities` for agents.\n\n- If you use `train.zaar`, your kernel will be restarted because OOM.","048d1bc3":"`.zarr` files support most of the traditional `numpy array` operations. In the following cell we iterate over the frames to get a scatter plot of the AV locations:","f70f9479":"### yaw","43252954":"Reading the data is also complex - please refer to Lyft's [L5Kit](http:\/\/https:\/\/github.com\/lyft\/l5kit) module and sample notebooks to properly load the data and use it for training. Further Kaggle-specific sample notebooks will follow shortly if you need more.\n\n","603c0c73":"I will explain the visualization utility of L5Kit. The core packages for visualisation are:\n- `Rasterization`\n- `visualization`","9b0670a7":"The fields in the agents dataset are the following:\n\n* `centroid` - the agent position (in plane - two dimmensions)\n* `extent` - the agent dimmensions (three dimmensions, let's called length, width, height)\n* `yaw` - the agent oscilation\/twist about the vertical plane\n* `velocity` - the speed of the agent - in euclidian space\n* `track_id` - index of track associated to the agent\n* `label_probabilities` - gives the probability for the agent to belong to one of 17 different agent type; we will explore these labels in a moment","e397363c":"It is different between `centriod_x` and `centroid_y` in distbution plot.","97a7d181":"`79.25`sec : Avg Scene Time (sec)","7d13cecf":"# Visualizing Agent","be714434":"## Visualization","5e4415b7":"This include familiar features like:\n\n- `x, y, and z` coords\n- `yaw`\n- `probabilites` of other extraneous factors.","36be9937":"- Please check [L5Kit github page](http:\/\/https:\/\/github.com\/lyft\/l5kit).","57e60146":"### Zarr Dataset - Avg Scene Time (sec)","b64e50b5":"### Extent distribution","5317936d":"- `ChunkedDataset`\n\n\nA dataset that lives on disk in compressed chunks, it has easy to use data loading and writing interfaces that involves making numpy-like slices.","5400bcca":"## Metadata Exploration","dc3897b9":"`16265` : `scenes`- driving episodes acquired from a given vehicle","0419d649":"## Install Kaggle_L5Kit","cb692d61":"# General View of the Street\n\n* https:\/\/www.kaggle.com\/t3nyks\/lyft-working-with-map-api","7a75c1e4":"* `centroid_x` and `centroid_y` : negative correlations\n* `extent_z` and `extent_x` : correlations","aad52fb6":"In this animation, you can see the Autonomous Vehicle is moving on path.\n\n<\/p>\nThere is intersection of roads.\n\n- `green box` : Our AV agent and the arrow on top of it represent its motion\n- `blue boxes` : Agents such as cars, cyclists, predestrians\n","87149028":"### centroid_x and centroid_y","737426b1":"### extent_x, extent_y and extent_z","c1d4a29e":"`248.36` : Avg Frames per Scene","2106df10":"For plot `train.zarr`, It takes a lot of time. So, I'll use `sample.zarr` in this section.","f079d58b":"We need `model_params` for Visualising the Autonomous Vehicle.","66d403dc":"- https:\/\/github.com\/lyft\/l5kit\n\n- https:\/\/github.com\/lyft\/l5kit\/blob\/master\/examples\/visualisation\/visualise_data.ipynb","0b8e8b86":"## Load the data","d2b05044":"# 4. <a id='basic'>Basic Data Exploration \ud83d\udd75\u200d<\/a>","85ebff3b":"### Zarr Dataset - Avg Frames per Scene","6461e2b0":"### Zarr Dataset - Avg Frame frequency","3f91236d":"###  1.2 What is Lyft Motion Prediction for Autonomous Vehicles Competition?\n- In this competition, you\u2019ll predict the motion of traffic agents. It is important to predict the movement of traffic agents around the AV such as cars, cyclists, and pedestrians. The challenge is to use machine learning techniques to make prediction for how cars, cyclists,and pedestrians move in the AV's environment.\n<img src=\"https:\/\/self-driving.lyft.com\/wp-content\/uploads\/2020\/06\/diagram-prediction-1.jpg\" style=\"width:80%\"\/>\n\n- You'll predict the motion of the objects in a given scene. For test, you will have 99 frames of objects moving around will be asked to predict their location in the next 50.\n\n<div style=\"clear:both;display:table\">\n\n<img src=\"https:\/\/self-driving.lyft.com\/wp-content\/uploads\/2020\/06\/motion_dataset_lrg_redux.gif\" style=\"width:45%;float:left\"\/>\n<img src=\"https:\/\/self-driving.lyft.com\/wp-content\/uploads\/2020\/06\/motion_dataset_2-1.png\" style=\"width:45%;float:left\"\/>\n<\/div>\n    \n    \n###  1.3 What we need to do? Observation\nThe goal of this competition is to predict the trajectories of other traffic participants. You can predict up to 3 trajectories for each agent in the test set. Every agent is identified by its `track_id` and its `timestamp`. Each trajectory holds 50 2D `(X,Y)` predictions. \n\n- The leaderboard of this competition is calculated with approximately 50% of the test data. The final results will be based on the other 50%, so the final standings may be different.\n    \n###  1.4 Metric: Negative Log-likelihood of the Ground Truth Data\n![](https:\/\/latex.codecogs.com\/gif.latex?%5Cbg_white%20%5Clarge%20L%20%3D%20-%20%5Clog%20p%28x_%7B1%2C%20%5Cldots%2C%20T%7D%2C%20y_%7B1%2C%20%5Cldots%2C%20T%7D%7Cc%5E%7B1%2C%20%5Cldots%2C%20K%7D%2C%20%5Cbar%7Bx%7D_%7B1%2C%20%5Cldots%2C%20T%7D%5E%7B1%2C%20%5Cldots%2C%20K%7D%2C%20%5Cbar%7By%7D_%7B1%2C%20%5Cldots%2C%20T%7D%5E%7B1%2C%20%5Cldots%2C%20K%7D%29)\n![](https:\/\/latex.codecogs.com\/gif.latex?%5Cbg_white%20%5Clarge%20%3D%20-%20%5Clog%20%5Csum_k%20e%5E%7B%5Clog%28c%5Ek%29%20-%5Cfrac%7B1%7D%7B2%7D%20%5Csum_t%20%28%5Cbar%7Bx%7D_t%5Ek%20-%20x_t%29%5E2%20+%20%28%5Cbar%7By%7D_t%5Ek%20-%20y_t%29%5E2%7D)\n- Image Credits: https:\/\/github.com\/lyft\/l5kit\/blob\/master\/competition.md\n    \nThe evaluation metric of this competition is the negative log-likelihood of the ground truth data given the multi-modal predictions. Because the format is a CSV file, all 3 trajectories fields must have a value, even if your prediction is single-modal. However, each one of the three trajectory has its own confidence, and you can set it 0 to completely ignore one or more trajectories during evaluation. The 3 confidences must sum to 1.\n\n\nRead more about it on the [Evaluation Page](https:\/\/www.kaggle.com\/c\/lyft-motion-prediction-autonomous-vehicles\/overview\/evaluation) and [Metrics Page in L5Kit repository](http:\/\/https:\/\/github.com\/lyft\/l5kit\/blob\/master\/competition.md).\n\nIf you feel this was something new and fresh, and it added some value to you, please consider <font color='orange'>upvoting<\/font>, it motivates to keep writing good kernels. \ud83d\ude04","0b293ebe":"### Zarr Dataset - Total Time (hr)","ac439e71":"## Visualize Individual Scene: Semantic","8abc113c":"## If this kernel is useful, <font color='orange'>please upvote<\/font>!\n- See you next time and I will update it soon!","49ce4f33":"### Zarr Dataset - agents","47ec50b6":"`4039527` : `frames` - snapshots in time of the pose of the vehicle","446bef05":"## Scenes","edb45615":"First, I check zarr format.","1ed80c35":"## L5Kit Cores","649c06b3":"And then I'll show you more information for zzaz.","1e4e33db":"## <font size='5' color='blue'>Contents<\/font> \n\n\n* [Basic Exploratory Data Analysis](#1)  \n    * [Getting started - Importing libraries]()\n    * [Reading the sample_submission]()\n    \n \n* [Data Exploration](#2)   \n     * [Introduction of L5Kit Data]()\n     * [How to see L5Kit for Data]()\n         * [Load the data]()\n         * [Inside the dataset]()\n         * [Metadata exploration]()\n\n \n* [Visualising ](#3)    \n     * [Visualising Autonomous Vehicles]()\n     * [Visualising Satellite View]()\n     * [Visualising Agent]()\n     * [Visualising Individual Scene]()\n ","6ddf08c4":"Note also that this competition requires that submissions be made from kernels, and that internet must be turned off in your submission kernels. For your convenience, Lyft's l5kit module is provided via a utility script called [kaggle_l5kit](http:\/\/https:\/\/www.kaggle.com\/philculliton\/kaggle-l5kit). Just attach it to your kernel, and the latest version of l5kit and all dependencies will be available.","0a12b2c9":"# Visualizing Satellite View","6d53c6ce":"## Visualize Individual Scene: Satellite##","5fd85159":"## Import Packages","652e826f":"I'll import some other packages in L5Kit including above 2 packages.","7ab16bc8":"- `Arrays`: scenes, frames, agents, traffic_light_faces\n- `Groups`: agents_mask","58a81ddd":"We need to config - `cfg` for visualization. \n<\/p>\nYou can see this:\n\nhttps:\/\/github.com\/lyft\/l5kit\/blob\/master\/examples\/visualisation\/visualisation_config.yaml","a64c9ab4":"<img src='https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Lyft-Kaggle\/Motion%20Prediction\/BP9I1484%20(1).jpg'>\n\n<p>\n<h1><center>Lyft Motion Prediction for Autonomous Vehicles - EDA\u26d0<\/center><h1>\n    \n# 1. <a id='Introduction'>Introduction \ud83c\udccf <\/a>\n    \n### 1.1 What is Autonomous Vehicle?\n* [Autonomous Vehicle also known as self-driving car is a vehicle that is capable of sensing its environment and moving safely with little or no human input](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Self-driving_car). Self-driving cars combine a variety of sensors to perceive their surroundings, such as radar, lidar, sonar, GPS, odometry and inertial measurement units. Advanced control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant signage.\n* In this competition, you\u2019ll predict the motion of traffic agents. It is important to predict the movement of traffic agents around the AV such as cars, cyclists, and pedestrians. The challenge is to use machine learning techniques to make prediction for how cars, cyclists,and pedestrians move in the AV's environment.\n\n* You'll predict the motion of the objects in a given scene. For test, you will have 99 frames of objects moving around will be asked to predict their location in the next 50.\n    \nI have linked below an informative video.\n","0c97d229":"It contains utilities to draw additional information (e.g. trajectories) onto RGB images. These utilities are commonly used after a to_rgb call to add other information to the final visualisation.\n- `draw_trajectory`: this function draws 2D trajectories from coordinates and yaws offset on an image","7017c565":"It contains classes for getting visual data as multi-channel tensors and turning them into interpretable RGB images. Every class has at least a rasterize method to get the tensor and a to_rgb method to convert it into an image. A few examples are:\n\n- `BoxRasterizer`: this object renders agents (e.g. vehicles or pedestrians) as oriented 2D boxes\n- `SatelliteRasterizer`: this object renders an oriented crop from a satellite map","a50bd855":"### Velocity","e007cca9":"`320124624` : `agents` - a generic entity captured by the vehicle's sensors. Note that only 4 of the 17 possible agent label_probabilities are present in this dataset.","1029192e":"they seems right-skewed distribution.","17c76397":"* `Green box` : AV agent\n* `Blue boxes` : Entities which we are captured by the sensors\n\n<\/p>\n\nWe want to predict the motion of these entities so that our AV can more effectively predict its path.\n","61fe6f1f":"### ego_rotatations","65634409":"### Zarr Dataset - scenes","72b4d310":"# 6. <a id='basic'>How to see L5Kit for Data \ud83d\ude46\u200d\u2642<\/a> ","cf18a8bd":"Original kaggle l5kit code is below.","a8433bb4":"## General Info of Sample Submission","a0607f7f":"And I import some other packages in L5Kit.","67b86030":"You can use `get_string` for printing `Num Scenes` columns.","d509f2ee":"`24.83` : Avg Frame frequency","ab85f67c":"# 3. <a id='reading'>Reading the submission.csv \ud83d\udcda<\/a>","44d0ba57":"- `raster_size`: the image plane size\n- `pixel_size`: how many meters correspond to a pixel\n- `ego_center`: our raster is centered around an agent, we can move the agent in the image plane with this param\n- `map_type`: the rasterizer to be employed. We currently support a satellite-based and a semantic-based one. We will look at the differences further down in this script\n- `filter_agents_threshold` : e.g. 0.0 include every obstacle, 0.5 show those obstacles with >0.5 probability of being one of the classes we care about (cars, bikes, peds, etc.), >=1.0 filter all other agents.\n- `Others - map_key` : the keys are relative to the dataset environment variable.","563e7fb1":"`112.19`hr : Total Time","2b0da06d":"## Centroid to plot trajectory","ee017ead":"### Zarr Dataset - frames","84202153":"* https:\/\/www.kaggle.com\/gpreda\/lyft-first-data-exploration\/data","0227e650":"- `Green` : Autonomous Vehicle(AV) agent\n- `Blue ` : Entities (cars, bicycles and pedestrians)","53656e05":"# 6. <a id='basic'>Visualization \ud83c\udfca\u200d\u2640<\/a> ","f9100f60":"# 5. <a id='basic'>Introduction of L5Kit for Data \ud83c\udfc4<\/a> ","f869a1b4":"You can see that `Num Scenes` is just `100`."}}