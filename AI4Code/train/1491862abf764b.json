{"cell_type":{"2d3abbc7":"code","f4ca3235":"code","c23d750b":"code","81338568":"code","ec12f6ce":"code","207e19aa":"code","01e1efc4":"code","47a42abf":"code","6fd5b2d3":"code","80292c4b":"code","5c5e5de9":"code","d8fa1cc3":"code","3c046837":"code","b5f97977":"code","e9443e29":"code","3a091d1e":"code","988ab74d":"code","03604583":"code","b884fd5d":"code","850a4559":"code","da59ff80":"code","54533136":"code","8fc469aa":"code","3bebbf83":"code","3c4c226d":"code","4424f562":"code","df2b6115":"code","93323792":"code","589f3bfa":"code","1b1dc4e3":"code","1c6cd17e":"code","76773024":"code","ff4fa59e":"code","9d5da072":"code","c1899699":"code","2c7754fa":"code","72fc5808":"code","c8abe37f":"code","36137833":"code","4724e73c":"code","a3ffc209":"code","6e93bc2f":"code","f047b869":"code","c9c549f1":"code","c11e4280":"markdown","9aa087eb":"markdown","d3199576":"markdown","c16d4276":"markdown","8ea3d6fd":"markdown","4fae88eb":"markdown","5e9f7473":"markdown","b8066857":"markdown","ee1e1459":"markdown","ca14733c":"markdown","00fe5a49":"markdown","fe9c800e":"markdown","9f09bbab":"markdown","7233d5c5":"markdown","262c5583":"markdown"},"source":{"2d3abbc7":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport pickle","f4ca3235":"train_df = pd.read_csv('..\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv')\ntrain_df.head()","c23d750b":"train_df.info()","81338568":"## How Likely Married or single people are loan defaulters\nsns.countplot(x = train_df['Married\/Single'] , hue = train_df['Risk_Flag']);","ec12f6ce":"# Checking the people distribution with their working experience\nsns.histplot(train_df['Experience'])","207e19aa":"# Checking, people with how much expreince are usually loan defaulters\nsns.countplot(train_df['Experience'] , hue = train_df['Risk_Flag']);","01e1efc4":"# How Likely house owners, house renters and withour house people does not pay loan\nsns.countplot(train_df['House_Ownership'] , hue = train_df['Risk_Flag']);","47a42abf":"##How many Loan Defaulters are their in the dataset\ntrain_df[\"Risk_Flag\"].value_counts().plot.bar(figsize=(6,6))","6fd5b2d3":"train_df.Profession.unique()","80292c4b":"# Droping the Id, CITY, STATE and Profession columns from the dataset and copying the new dataset\ndf_copy = train_df.drop(['Id', 'CITY', 'STATE', 'Profession'], axis=1).copy()\ndf_copy.head()","5c5e5de9":"df_copy.corr()","d8fa1cc3":"df_copy.describe()","3c046837":"## This will convert all the string values into category value\n# def convert_string_to_category():\nfor label,content in df_copy.items():\n    if pd.api.types.is_string_dtype(content):\n        df_copy[label] = content.astype(\"category\").cat.as_ordered()","b5f97977":"df_copy.info()","e9443e29":"# Turn Categorical variables into numbers\nfor labels, content in df_copy.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        df_copy[labels] = pd.Categorical(content).codes + 1","3a091d1e":"df_copy['Married\/Single'].value_counts()","988ab74d":"df_copy.House_Ownership.value_counts()","03604583":"df_copy.describe()","b884fd5d":"# Increase the size of the heatmap.\nplt.figure(figsize=(16, 6))\n# Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\nheatmap = sns.heatmap(df_copy.corr(), vmin=-1, vmax=1, annot=True)\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","850a4559":"X_train , X_test , y_train , y_test = train_test_split(df_copy.drop(\"Risk_Flag\", axis=1) , df_copy[\"Risk_Flag\"] , train_size = 0.8, random_state=42)","da59ff80":"X_train.shape, y_train.shape","54533136":"%%time\nlogistic_model = LogisticRegression()\nlogistic_model.fit(X_train,y_train)","8fc469aa":"def evaluation_metrics(y_test,X_test, model):\n    pred = model.predict(X_test)\n    acc_score = accuracy_score(y_test, pred)\n    roc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n    score = f1_score(y_test,pred)\n    cm = confusion_matrix(y_test, pred)\n    \n    \n    print(\"Accuracy Score of the classification model is:\", acc_score)\n    print(\"ROC_AUC Score of the classification model is:\", roc_score)\n    print(\"F1 Score of the classification model is:\", score)\n    plt.figure(figsize=(9,9))\n    sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n    plt.ylabel('Actual label');\n    plt.xlabel('Predicted label');\n    all_sample_title = 'Accuracy Score: {0}'.format(acc_score)\n    plt.title(all_sample_title, size = 15);\n    ","3bebbf83":"evaluation_metrics(y_test, X_test, logistic_model)","3c4c226d":"%%time\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)","4424f562":"evaluation_metrics(y_test, X_test, rfc)","df2b6115":"%%time\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)","93323792":"evaluation_metrics(y_test, X_test, dtc)","589f3bfa":"%%time\nbgc = BaggingClassifier()\nbgc.fit(X_train, y_train)","1b1dc4e3":"evaluation_metrics(y_test, X_test, bgc)","1c6cd17e":"vc = VotingClassifier(estimators=[('bagging classifier', bgc), ('random forest', rfc), ('decision tree classifier', dtc)],voting='soft')\nvc.fit(X_train, y_train)","76773024":"evaluation_metrics(y_test, X_test, vc)","ff4fa59e":"rfc.feature_importances_","9d5da072":"def plot_features(columns, importances, n=8):\n    df = pd.DataFrame({\"features\": columns,\n                          \"features_importances\": importances}).sort_values(\"features_importances\", ascending=False).reset_index(drop=True)\n\n    #Plot the DataFRame\\n\",\n    fig, ax = plt.subplots()\n    ax.barh(df[\"features\"][:n], df[\"features_importances\"][:n])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Feature Importance\")\n    ax.invert_yaxis()","c1899699":"plot_features(X_train.columns,rfc.feature_importances_)","2c7754fa":"filename = 'Random_Forest_Classifier.sav'\npickle.dump(rfc, open(filename, 'wb'))","72fc5808":"# load the model from disk\nloaded_model = pickle.load(open(filename, 'rb'))\nresult = loaded_model.score(X_test, y_test)\nprint(result)","c8abe37f":"# Import the test data\ntest_df = pd.read_csv(\"..\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv\")\n\ntest_df.head()","36137833":"def preprocess_data(df):\n    \"\"\"\n    Perform data preprocessing for model prediction\n    \"\"\"\n    \n    df = df.drop([\"ID\",\"Profession\", \"CITY\", \"STATE\"], axis=1)\n    \n    ##Fill the categorical datab missing data and turned categories into numbers\n    for labels, content in df.items():\n        if not pd.api.types.is_numeric_dtype(content):\n            df[labels] = pd.Categorical(content).codes + 1\n            \n        \n    return df","4724e73c":"processed_test_df = preprocess_data(test_df)\nprocessed_test_df.head()","a3ffc209":"test_preds = rfc.predict(processed_test_df)","6e93bc2f":"df_preds = pd.DataFrame()\ndf_preds[\"id\"] = test_df[\"ID\"]\ndf_preds[\"risk_flag\"] = test_preds\n\ndf_preds","f047b869":"df_preds.risk_flag.value_counts()","c9c549f1":"#Export predictions data\ndf_preds.to_csv(\"Submission.csv\", index=False)","c11e4280":"### Using Random Forest Classifier","9aa087eb":"From the correlation matrix, we can see that there is not much effect of the features on the risk_flag","d3199576":"# Data Processing","c16d4276":"## Splitting the dataset into train and test set","8ea3d6fd":"# Make Predictions on Test Dataset\nI am going to use Random Forest Classifier as it gives the best f1 and accuracy score","4fae88eb":"## Checking Features Importance for this task","5e9f7473":"## 2. Data\nThere are 2 main datasets:\n\n* Training Data.csv is the training set, which contains data of previous bank customers and defaulters.*\n* Test Data.csv is the test set, which is going to used for testing our model prediction accuracy*\n","b8066857":"### Using Bagging Classifier","ee1e1459":"### Using Logistic Regression","ca14733c":"## 1. Problem defination\n\n* Classify the Potential Bank Loan Defaulters","00fe5a49":"# Load Prediction based on Customer Behavious","fe9c800e":"## Saving Machine Learning Model","9f09bbab":"### Using Decision Tree Classifier","7233d5c5":"# Fitting the Classification models on the train dataset ","262c5583":"### Using Voting Classifier"}}