{"cell_type":{"4bfe3ee9":"code","cd5c654e":"code","68655b3c":"code","655abdb7":"code","ab028af0":"code","30d2e799":"code","c0344aa0":"code","19f9a4d7":"code","f86e536f":"code","a42a577c":"code","373f6bbc":"code","65d76e51":"code","948994bf":"code","f03f9d26":"code","d30aa6a4":"code","2527fb6e":"code","dc2133c2":"code","a7f4a86b":"code","e702e3b5":"code","227bee0e":"code","f01f2454":"code","3e564251":"code","c4551904":"code","8ad7fa43":"code","8d84ca6c":"code","1ac10781":"code","d425a355":"code","aaf4f864":"code","cb86a17a":"code","22d6f715":"code","91bf77f5":"code","bc66a5a1":"code","0ecaaac8":"code","eb115e71":"code","83892906":"code","65fe01ec":"code","7b6ce99c":"code","523f98d5":"code","453fa31e":"code","637d011b":"code","9684073f":"code","1a394ddc":"code","26b10445":"code","698937d0":"code","afc3d7e3":"code","edfe2dd9":"code","3ef7fd0e":"code","f4989e84":"code","c8f9d5ae":"code","30ae8dc1":"code","e5750da3":"code","d6edf19b":"code","0e922aac":"code","9f6207fb":"code","b187a2dc":"code","e5e05d13":"code","08bf2023":"code","e39bff9f":"code","a62f41a8":"code","83d08bde":"code","05f812c8":"code","a0396c01":"code","59765cc3":"code","c73109a6":"code","021eeff2":"markdown","fa844ed8":"markdown","f1f6b54d":"markdown","028fd650":"markdown","a9c05b28":"markdown","847c6779":"markdown","61354b9d":"markdown","af9228a5":"markdown","899518ce":"markdown","b5685118":"markdown","1d5e2e6f":"markdown","a6028ddc":"markdown","0ef9aed5":"markdown","6aae6a60":"markdown","f1b4a0fc":"markdown","38f08782":"markdown","8fb52edf":"markdown","30ba7ecf":"markdown","124774f5":"markdown","10d8e715":"markdown"},"source":{"4bfe3ee9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Plotly Libraris\nimport plotly.express as px\nimport plotly.graph_objects as go\n#import plotly.figure_factory as ff\n#from plotly.colors import n_colors\n\nfrom plotly.subplots import make_subplots\n# Minmax scaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n#itertools\nimport itertools\n\n#dataframe display settings\npd.set_option('display.max_columns', 5000000)\npd.set_option('display.max_rows', 50000000)\n\n#to suppress un-necessary warnings\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n\nimport xgboost as xgb\n\n#Importing SKlearn models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, r2_score\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression  \nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\n\n#Package to flatten python lists\nfrom pandas.core.common import flatten\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd5c654e":"confirmed_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_global.csv')\ndeaths_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_global.csv')\nrecoveries_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_recovered_global.csv')\nlatest_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_daily_reports\/08-22-2020.csv')\nus_medical_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_daily_reports_us\/08-22-2020.csv')","68655b3c":"deaths_df.columns","655abdb7":"# Processing the death,cases,active by countries\nconfirmed_group_df = confirmed_df.groupby(by='Country\/Region',as_index=False).sum()\ndeaths_group_df = deaths_df.groupby(by='Country\/Region',as_index=False).sum()\nrecoveries_group_df = recoveries_df.groupby(by='Country\/Region',as_index=False).sum()\n\nactive_group_df = pd.DataFrame(columns=[confirmed_group_df.columns])\nactive_group_df = deaths_group_df.copy()\nfor i in range(confirmed_group_df.shape[0]):\n    for j in range(3, confirmed_group_df.shape[1]):\n        active_group_df.iloc[i,j] = confirmed_group_df.iloc[i,j]-(recoveries_group_df.iloc[i,j]+deaths_group_df.iloc[i,j])","ab028af0":"deaths_group_df_sorted_df_30 = deaths_group_df.sort_values(by=deaths_group_df.columns[-2], ascending=False).iloc[0:30]\ndel deaths_group_df_sorted_df_30['Lat']\ndel deaths_group_df_sorted_df_30['Long']\ndel confirmed_group_df['Lat']\ndel confirmed_group_df['Long']\ndel active_group_df['Lat']\ndel active_group_df['Long']\ndeaths_group_df_sorted_df_30 = deaths_group_df_sorted_df_30.sort_values(by='Country\/Region')","30d2e799":"# filtering out to only the top 30 countries with death\ncountries = list(deaths_group_df_sorted_df_30['Country\/Region'])\n\nsorted_confirmed_df = confirmed_group_df.loc[confirmed_group_df['Country\/Region'].isin(countries)]\nsorted_active_df = active_group_df.loc[confirmed_group_df['Country\/Region'].isin(countries)]\n\nnew_df = pd.DataFrame(columns=['Country','Continent','Date','Deaths on Date','Confirmed Cases on date','Active Cases On Date','Cumulative complete Vaccination to date','Average Age'])\nn_df_columns = deaths_group_df_sorted_df_30.columns\n\n# checking for indexing\nassert list(sorted_active_df['Country\/Region']) == list(sorted_confirmed_df['Country\/Region']) == list(deaths_group_df_sorted_df_30['Country\/Region'])","c0344aa0":"for dates_i in range(1,deaths_group_df_sorted_df_30.shape[1]):\n    for country_i in range(deaths_group_df_sorted_df_30.shape[0]):\n        new_row = {'Date':n_df_columns[dates_i], 'Country' : deaths_group_df_sorted_df_30.iloc[country_i,0], 'Deaths on Date' : deaths_group_df_sorted_df_30.iloc[country_i,dates_i]\n                  ,'Confirmed Cases on date' : sorted_confirmed_df.iloc[country_i,dates_i],\n                   'Active Cases on date' : sorted_active_df.iloc[country_i,dates_i]\n                  }\n        new_df = new_df.append(new_row,ignore_index=True)","19f9a4d7":"cc_map = {'US': 'North America',\n 'Brazil': 'South America',\n 'India': 'Asia',\n 'Mexico': 'North America',\n 'Russia': 'Europe',\n 'Peru': 'South America',\n 'Indonesia': 'Asia',\n 'United Kingdom': 'Europe',\n 'Italy': 'Europe',\n 'Colombia': 'South America',\n 'Iran': 'Asia',\n 'France': 'Europe',\n 'Argentina': 'South America',\n 'Germany': 'Europe',\n 'South Africa': 'Africa',\n 'Spain': 'Europe',\n 'Poland': 'Europe',\n 'Turkey': 'Asia',\n 'Ukraine': 'Europe',\n 'Romania': 'Europe',\n 'Philippines': 'Asia',\n 'Chile': 'South America',\n 'Ecuador': 'South America',\n 'Czechia': 'Europe',\n 'Hungary': 'Europe',\n 'Canada': 'North America',\n 'Pakistan': 'Asia',\n 'Malaysia': 'Asia',\n 'Bangladesh': 'Asia',\n 'Belgium': 'Europe'}\n\n# applying mapping of continents\nnew_df['Continent'] = new_df['Country'].map(cc_map)","f86e536f":"new_df.head()","a42a577c":"#Encoding for continents and countries\n# using lable_encoder\n# from sklearn.preprocessing import LabelEncoder\n\n# labelencoder = LabelEncoder()\n# label_encoded_df = new_df.iloc[:,:]\n# label_encoded_df['Continent'] = labelencoder.fit_transform(label_encoded_df['Continent'])\n# out = label_encoded_df.iloc[:,:5].reset_index()\n# del out['index']\n\n\n# One Hot encoding for continentsand \none_hot_cont = pd.get_dummies(new_df['Continent'], prefix=\"Continent_\")\none_hot_country = pd.get_dummies(new_df['Country'], prefix=\"Country_\")\nnew_df = new_df.join(one_hot_cont)\nnew_df = new_df.join(one_hot_country)\n\n","373f6bbc":"new_df.head()","65d76e51":"del new_df['Active Cases On Date']\ndel new_df['Cumulative complete Vaccination to date']\ndel new_df['Average Age']\n","948994bf":"new_df.to_csv(\"continent_updateV3.csv\",index=False)","f03f9d26":"import json\nimport pandas as pd\ngdp_dict = json.load(open('..\/input\/task1countriesd1\/gdp (2).json'))\ncont_df = new_df[:]\n","d30aa6a4":"cont_df.head()","2527fb6e":"countries = list(cont_df['Country'])\ngdp_country_map = {}\nfor i in gdp_dict:\n    gdp = i['2021 GDP']\n    country = i[\"Country\"]\n    try:\n        gdp_country_map[country] = float(gdp)\n    except:\n        continue\n","dc2133c2":"cont_df['GDP Per Capita'] = cont_df['Country'].map(gdp_country_map)","a7f4a86b":"cont_df.tail()","e702e3b5":"cont_df = cont_df.dropna()","227bee0e":"assert len(set(cont_df['Country'])) == 20","f01f2454":"cont_df.to_csv('gdp_updateV3.csv')","3e564251":"df = cont_df[:]","c4551904":"# sort by date to find the difference between countries and days\ndf['Date'] = df['Date'].apply(pd.to_datetime)\ndf.sort_values(by=['Date'], ascending=True, inplace=True)\n\n\n# Calculate the cases per day by taking the difference between day 1 and day 2\ndf['Cases per day'] = df.groupby(['Country'])['Confirmed Cases on date'].diff()\n\n# calculate deaths per day by taking the difference between day 1 and day 2\ndf['Deaths per day'] = df.groupby(['Country'])['Deaths on Date'].diff()\n\ndf = df.fillna(0)\n","8ad7fa43":"df.head()","8d84ca6c":"# changing the column for easier processing \ndf['Deaths on Date'], df['Deaths per day'] = df.pop('Deaths on Date'), df.pop('Deaths per day')","1ac10781":"df.loc[df['Country'] == 'US'].tail()","d425a355":"t1 = pd.to_datetime('2020-01-22')\ndf['Date'] = df['Date'].apply(lambda x: (x - t1).days)\ndf = df.rename(columns={'Date': 'Days from start'})\n","aaf4f864":"len(set(df['Country'])) == 20","cb86a17a":"# df.to_csv(\"V3TimeDelta.csv\",index=False)\nimport pandas as pd\ndf = pd.read_csv(\"..\/input\/task1countriesd1\/V3TimeDelta.csv\")\ndf.tail()","22d6f715":"columns = ['Country','Continent__Africa','Continent__Asia','Continent__Europe','Continent__North America','Continent__South America','Days from start','Active Cases on date','GDP Per Capita','Cases per day','Deaths per day','Deaths on Date']\ndf2 = df.loc[:,columns]\ncolumns2 = ['Continent__Africa','Continent__Asia','Continent__Europe','Continent__North America','Continent__South America','Days from start','Active Cases on date','GDP Per Capita','Cases per day','Deaths per day','Deaths on Date']\n\ndf3 = df.loc[:,columns2]","91bf77f5":"# change to type int\ncolumns = ['GDP Per Capita','Active Cases on date']\n# df['GDP Per Capita'].apply(lambda val : int(val))","bc66a5a1":"df2.reset_index().to_csv(\"V4NewVariable.csv\",index=False)","0ecaaac8":"import pandas as pd\nimport numpy as np","eb115e71":"def standard_scaling(df):\n    dfout = df.apply(lambda x: (x - np.mean(x)) \/ np.std(x))\n    return dfout\n\ndef min_max_scaling(df):\n    return df.apply(lambda x: (x - x.min()) \/ (x.max() - x.min()))\n\ndef normalization(df):\n    return df.apply(lambda x: (x - np.mean(x)) \/ (np.max(x) - np.min(x)))\n\ndf3 = standard_scaling(df3)","83892906":"# Seperate features and target variable\nX = df.iloc[:, 2:6]\ny = df.loc[:,'Deaths per day']\n\n\ndef standard_scaling(df):\n    return df.apply(lambda x: (x - np.mean(x)) \/ np.std(x))\n\ndef min_max_scaling(df):\n    return df.apply(lambda x: (x - x.min()) \/ (x.max() - x.min()))\n\ndef normalization(df):\n    return df.apply(lambda x: (x - np.mean(x)) \/ (np.max(x) - np.min(x)))\n\n","65fe01ec":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.45)","7b6ce99c":"def train_test_split(df_feature, df_target, random_state=None, test_size=0.5):\n    np.random.seed(random_state)\n    N = df_feature.shape[0]\n    print(N)\n    sample = int(test_size*N)\n    train_idx = np.random.choice(N, sample,replace=False)\n    print(len(train_idx))\n    \n    df_feature_train = df_feature.iloc[train_idx]\n    df_target_train = df_target.iloc[train_idx]\n\n    test_idx = [idx for idx in range(N) if idx not in train_idx]\n    print(len(test_idx))\n    \n    df_feature_test = df_feature.iloc[test_idx]\n    df_target_test = df_target.iloc[test_idx]\n\n    return df_feature_train, df_feature_test, df_target_train, df_target_test\n\n# Dividing the data into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.45)","523f98d5":"# # Cost Function\n\n# def cost_function(X, y, w, b):\n#     \"\"\"\n#     Parameters:\n#     X: features\n#     y: target values\n#     w: weights\n#     b: bias\n    \n#     Returns:\n#     cost: cost with current weights and bias\n#     \"\"\"\n#     cost = np.sum((((X.dot(w) + b) - y) ** 2) \/ (2*len(y)))\n#     return cost\n","453fa31e":"\n# gradient descent function to perform the feed-forward propagation, backpropagation, calculate the gradient and update our weights.\ndef gradient_descent_function(X, y, w, b, alpha=0.01, epochs=1000):\n    \"\"\"\n    Parameters:\n    X: features\n    y: target values\n    w: initial weights\n    b: initial bias\n    alpha: learning rate\n    epochs: number of iterations\n    \n    Returns:\n    costs: cost per epoch\n    w: finalised weights\n    b: finalised bias\n    \"\"\"\n    m = len(y)\n    costs = [0] * epochs\n    \n    for epoch in range(epochs):\n        # Calculate the value -- Forward Propagation\n        z = X.dot(w) + b\n        \n        # Calculate the losses\n        loss = z - y\n        \n        # Calculate gradient descent\n        weight_gradient = X.T.dot(loss) \/ m\n        bias_gradient = np.sum(loss) \/ m\n        \n        # Update weights and bias\n        w = w - alpha*weight_gradient\n        b = b - alpha*bias_gradient\n        \n        # Store current lost\n        cost = cost_function(X, y, w, b)\n        costs[epoch] = cost\n        \n    return w, b, costs\n","637d011b":"def CostFunction(x,y,w,b):\n    cost = np.sum((((x.dot(w) + b) - y) ** 2) \/ (2*len(y)))\n    return cost\n\ndef GradientDescent(x, y, w, b, learning_rate, epochs):\n    cost_list = [0] * epochs\n   \n    for epoch in range(epochs):\n        z = x.dot(w) + b\n        loss = z - y\n        \n        weight_gradient = x.T.dot(loss) \/ len(y)\n        bias_gradient = np.sum(loss) \/ len(y)\n        \n        w = w - learning_rate*weight_gradient\n        b = b - learning_rate*bias_gradient\n  \n        cost = CostFunction(x, y, w, b)\n        cost_list[epoch] = cost\n        \n        if (epoch%(epochs\/10)==0):\n            print(\"Cost is:\",cost)\n        \n    return w, b, cost_list","9684073f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\ndf = pd.read_csv(\"..\/input\/task1countriesd1\/V3TimeDelta.csv\")\ndf.tail()\n# sns.scatterplot(y='resale_price', x='floor_area_sqm', data=df_tampines)","1a394ddc":"set(df['Country'])","26b10445":"sns.scatterplot(y='Deaths per day', x='Days from start', data=df)","698937d0":"y.shape","afc3d7e3":"df.loc[:,'Deaths per day'].to_frame()","edfe2dd9":"# Using Scikit Learn\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Seperate features and target variable\nX = df.iloc[:, 2:6]\ny = df.loc[:,'Deaths per day'].to_frame()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.45)\nlr = LinearRegression()\n# Train the model using the training sets\nlr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = lr.predict(X_test)\n\n# The coefficients\nprint(\"Coefficients: \\n\", lr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_test, diabetes_y_pred))\n\n# Plot outputs\nplt.scatter(X_test[:,1], y_test, color=\"black\")\nplt.plot(X_test[:,1], y_pred, color=\"blue\", linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","3ef7fd0e":"w, b, c= GradientDescent(X_train, y_train, np.zeros(X_train.shape[1]), 0, 0.002,epochs=100)\nplt.plot(c)\n","f4989e84":"def predict(X, w, b):\n    return X.dot(w) + b\n\ndef r2score(y_pred, y):\n    rss = np.sum((y_pred - y) ** 2)\n    tss = np.sum((y-y.mean()) ** 2)\n    \n    r2 = 1 - (rss \/ tss)\n    return r2\n\ndef mse(Y_pred,Y_true):\n    return np.square(np.subtract(Y_true,Y_pred)).mean()\n\n\ny_pred = predict(X_test, w, b)\n\nmse(y_pred, y_test)","c8f9d5ae":"df_test = Y_trueY_pred","30ae8dc1":"# Initial random weights\nw = np.random.randn(X_train.shape[1])\n# Initial bias\nb = 0\n\n# call the gradient descent function to get the finalised weights and bias (model training)\nweights, bias, costs = gradient_descent_function(X_train, y_train, w, b, epochs=200)","e5750da3":"# Plotting the cost function per epoch\nimport matplotlib.pyplot as plt\nplt.plot(costs)\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.title('Cost per epoch')\nplt.show()","d6edf19b":"def r2score(y_pred, y):\n    \"\"\"\n    Parameters:\n    y_pred: predicted values\n    y: actual values\n    \n    Returns:\n    r2: r2 score\n    \"\"\"\n    rss = np.sum((y_pred - y) ** 2)\n    tss = np.sum((y-y.mean()) ** 2)\n    \n    r2 = 1 - (rss \/ tss)\n    return r2\n\ndef predict(X, w, b):\n    \"\"\"\n    Parameters:\n    X: features\n    w: weights\n    b: bias\n    \n    Returns:\n    y_pred: predicted values\n    \"\"\"\n    y_pred = X.dot(w) + b\n    return y_pred","0e922aac":"y_pred = predict(X_test, w, bias)\nr2 = r2score(y_pred, y_test)\nprint(r2)\n","9f6207fb":"base_stats = pd.DataFrame(columns=['Dates','Confirmed','Deaths','Recovered','Active'])\nbase_stats['Dates'] = confirmed_df.columns[4:]\n\nbase_stats['Confirmed'] = base_stats['Dates'].apply(lambda x: confirmed_df[x].sum())\nbase_stats['Deaths'] = base_stats['Dates'].apply(lambda x: deaths_df[x].sum())\nbase_stats['Recovered'] = base_stats['Dates'].apply(lambda x: recoveries_df[x].sum())\nbase_stats.reset_index(drop=False, inplace=True)\nbase_stats['Active'] = base_stats['index'].apply(lambda x: (base_stats['Confirmed'][x]-(base_stats['Deaths'][x]+base_stats['Recovered'][x])))\nbase_stats.head()","b187a2dc":"\n\nbase_stats_fig = go.Figure() # create canvas\n\nfor column in base_stats.columns.to_list()[2:6]:\n    color_dict = {\n      \"Deaths\": \"#073b4c\"\n        }\n    base_stats_fig.add_trace(\n        go.Scatter(\n            x = base_stats['Dates'],\n            y = base_stats[column],\n            name = column,\n            line = dict(color=color_dict[column]),\n            hovertemplate ='<br><b>Date<\/b>: %{x}'+'<br><i>Count<\/i>:'+'%{y}',\n        )\n    )\n    \nfor column in base_stats.columns.to_list()[2:6]:\n    color_dict = {\n      \"Deaths\": \"#0C6583\"\n        }\n    base_stats_fig.add_trace(\n        go.Scatter(\n            x = base_stats['Dates'],\n            y = base_stats['index'].apply(lambda x: (base_stats[column][x-7:x].sum())\/7 if x>7 else (base_stats[column][0:x].sum())\/7),\n            name = column+\" 7-day Moving Avg.\",\n            line = dict(dash=\"dash\", color=color_dict[column]), showlegend=False,\n            hovertemplate = '<br><b>Date<\/b>: %{x}'+'<br><i>7-day moving avg.<\/i>: %{y}'\n        )\n    )\n    \nbase_stats_fig.update_layout(\n    updatemenus=[\n        dict(\n        buttons=list(\n            [\n             dict(label = 'Deaths',\n                  method = 'update',\n                  args = [{'visible': [False, True, False, False, False, True, False, False]},\n                          {'title': 'Deaths',\n                           'showlegend':True}]),\n            ]),\n             type = \"dropdown\",\n             direction=\"down\",\n#             pad={\"r\": 10, \"t\": 40},\n             showactive=True,\n             x=0,\n             xanchor=\"left\",\n             y=1.25,\n             yanchor=\"top\"\n        ),\n        dict(\n        buttons=list(\n            [dict(label = 'Linear Scale',\n                  method = 'relayout',\n                  args = [{'yaxis': {'type': 'linear'}},\n                          {'title': 'All Cases',\n                           'showlegend':True}]),\n             dict(label = 'Log Scale',\n                  method = 'relayout',\n                  args = [{'yaxis': {'type': 'log'}},\n                          {'title': 'Confirmed',\n                           'showlegend':True}]),\n            ]),\n             type = \"dropdown\",\n             direction=\"down\",\n#             pad={\"r\": 10, \"t\": 10},\n             showactive=True,\n             x=0,\n             xanchor=\"left\",\n             y=1.36,\n             yanchor=\"top\"\n        )\n    ])\n\n\nbase_stats_fig.update_xaxes(showticklabels=False)\nbase_stats_fig.update_layout(\n    #height=600, width=600, \n    title_text=\"Basic Statistics for Covid19\", title_x=0.5, title_font_size=20,\n                            legend=dict(orientation='h',yanchor='top',y=1.15,xanchor='right',x=1), paper_bgcolor=\"mintcream\",\n                            xaxis_title=\"Date\", yaxis_title=\"# of Cases\")\nbase_stats_fig.show()","e5e05d13":"country_data = go.Figure()\ncountry_data.add_trace(go.Table(\n    header=dict(values=['Country','Confirmed','Active','Recovered','Deaths','Daily Increase','Mortality Rate'],\n                fill = dict(color='#A5B3F3'),\n                line_color='darkslategray',\n                align = ['left'] * 5),\n    cells=dict(values=[confirmed_group_df.sort_values(by=confirmed_group_df.columns[-1], ascending=False)['Country\/Region'], \n                      confirmed_group_df.sort_values(by=confirmed_group_df.columns[-1], ascending=False)['Country\/Region'].apply(lambda x: confirmed_group_df[confirmed_group_df['Country\/Region']==x][confirmed_group_df.columns[4:]].values.tolist()[0][-1]),\n                      confirmed_group_df.sort_values(by=confirmed_group_df.columns[-1], ascending=False)['Country\/Region'].apply(lambda x: active_group_df[active_group_df['Country\/Region']==x][active_group_df.columns[4:]].values.tolist()[0][-1]),\n                      confirmed_group_df.sort_values(by=confirmed_group_df.columns[-1], ascending=False)['Country\/Region'].apply(lambda x: recoveries_group_df[recoveries_group_df['Country\/Region']==x][recoveries_group_df.columns[4:]].values.tolist()[0][-1]),\n                      confirmed_group_df.sort_values(by=confirmed_group_df.columns[-1], ascending=False)['Country\/Region'].apply(lambda x: deaths_group_df[deaths_group_df['Country\/Region']==x][deaths_group_df.columns[4:]].values.tolist()[0][-1]),\n                      confirmed_group_df.sort_values(by=confirmed_group_df.columns[-1], ascending=False)['Country\/Region'].apply(lambda x: confirmed_group_df[confirmed_group_df['Country\/Region']==x][confirmed_group_df.columns[4:]].values.tolist()[0][-1]-confirmed_group_df[confirmed_group_df['Country\/Region']==x][confirmed_group_df.columns[4:]].values.tolist()[0][-2]),\n                      confirmed_group_df.sort_values(by=confirmed_group_df.columns[-1], ascending=False)['Country\/Region'].apply(lambda x: (deaths_group_df[deaths_group_df['Country\/Region']==x][deaths_group_df.columns[4:]].values.tolist()[0][-1]\/confirmed_group_df[confirmed_group_df['Country\/Region']==x][confirmed_group_df.columns[4:]].values.tolist()[0][-1])*100).round(decimals=3)\n                      ],\n               fill = dict(color='#F0FCFD'),\n               line_color='darkslategray',\n               align = ['left'] * 5)))\n\ncountry_data.update_layout(\n    #height=600, width=1100, \n    title_text=\"Country wise stats\",\n                                     title_x=0.5, title_font_size=20,\n                                     paper_bgcolor=\"mintcream\")\ncountry_data.show()\n","08bf2023":"treemap_fig = go.Figure()\ndf_dict={\n  \"Confirmed\": [confirmed_group_df,True],\n  \"Active\": [active_group_df,False],\n  \"Recovered\": [recoveries_group_df,False],\n  \"Deaths\": [deaths_group_df,False],\n  \"Daily_inc\": [None,False]\n}\nfor column in ['Confirmed','Active','Recovered','Deaths']:\n\n    treemap_fig.add_trace(go.Treemap(labels = confirmed_group_df['Country\/Region'], name=\"Treemap\",\n                                     parents = ['']*confirmed_group_df.shape[0],\n                                     values = df_dict[column][0][confirmed_group_df.columns[-1]],\n                                     branchvalues=\"total\",\n                                     textinfo = \"percent root+label+value+text\", outsidetextfont = {\"size\": 30, \"color\": \"darkblue\"},\n                                     marker = {\"line\": {\"width\": 2}}, pathbar = {\"visible\": False}, visible = df_dict[column][1], \n                                     hovertemplate='<b>%{label} <\/b> <br> Count: %{value}<br>'\n                                     )) \n    \ntreemap_fig.add_trace(go.Treemap(labels = confirmed_group_df.sort_values(by=confirmed_group_df.columns[-1], ascending=False)['Country\/Region'], name=\"Treemap\",\n                                 parents = ['']*confirmed_group_df.shape[0],\n                                 values = confirmed_group_df.sort_values(by=confirmed_group_df.columns[-1], ascending=False)['Country\/Region'].apply(lambda x: confirmed_group_df[confirmed_group_df['Country\/Region']==x][confirmed_group_df.columns[4:]].values.tolist()[0][-1]-confirmed_group_df[confirmed_group_df['Country\/Region']==x][confirmed_group_df.columns[4:]].values.tolist()[0][-2]),\n                                 branchvalues=\"total\",\n                                 textinfo = \"percent root+label+value+text\", outsidetextfont = {\"size\": 30, \"color\": \"darkblue\"},\n                                 marker = {\"line\": {\"width\": 2}}, pathbar = {\"visible\": False}, visible = df_dict['Daily_inc'][1], \n                                 hovertemplate='<b>%{label} <\/b> <br> Count: %{value}<br>'\n                                 )) \n\ntreemap_fig.update_layout(\n    updatemenus=[\n        dict(\n        buttons=list([\n             dict(label = 'Confirmed',\n                  method = 'update',\n                  args = [{'visible': [True, False, False, False, False]},\n                          {'title': 'Confirmed',\n                           'showlegend':True}]),\n             dict(label = 'Active',\n                  method = 'update',\n                  args = [{'visible': [False, True, False, False, False]},\n                          {'title': 'Active',\n                           'showlegend':True}]),\n             dict(label = 'Recovered',\n                  method = 'update',\n                  args = [{'visible': [False, False, True, False, False]},\n                          {'title': 'Recovered',\n                           'showlegend':True}]),\n             dict(label = 'Deaths',\n                  method = 'update',\n                  args = [{'visible': [False, False, False, True, False]},\n                          {'title': 'Deaths',\n                           'showlegend':True}]),\n            dict(label = 'Daily Increase',\n                  method = 'update',\n                  args = [{'visible': [False, False, False, False, True]},\n                          {'title': 'Daily Increase',\n                           'showlegend':True}]),\n            ]),\n             type = \"buttons\",\n             direction=\"down\",\n#             pad={\"r\": 10, \"t\": 40},\n             showactive=True,\n             x=1.01,\n             xanchor=\"left\",\n             y=0.8,\n             yanchor=\"top\"\n        )\n    ])\n\ntreemap_fig.update_layout(\n    #height=600, width=1100, \n    title_text=\"Treemap of Countries <br> The Treemap shows the number of Cases in Different coutries <br> and their percent of total cases worldwide\",\n                          title_x=0.5, title_font_size=15,\n                          legend=dict(orientation='h',yanchor='top',y=1.12,xanchor='right',x=1), paper_bgcolor=\"mintcream\")\ntreemap_fig.show()","e39bff9f":"# Linear Regression using Scikit Learn\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression","a62f41a8":"death_pred_df = deaths_group_df.copy()\ndeath_pred_df[\"Cumulative Deaths\"] = deaths_group_df.loc[:,'1\/22\/20':].sum(axis= 1)\nfrom datetime import datetime\nstart_date = datetime.strptime('1\/22\/20','%m\/%d\/%y')\nend_date = datetime.strptime(deaths_df.columns[-1],'%m\/%d\/%y')\ndelta_t = end_date - start_date\n\ndeath_pred_df[\"Time Period (days)\"] = delta_t.days\n\ndeath_pred_df_sorted_30 = death_pred_df.sort_values(by=death_pred_df.columns[-2], ascending=False).iloc[0:30].reset_index(drop=True)","83d08bde":"death_pred_df_sorted_30.head()","05f812c8":"usa_death_pred = death_pred_df_sorted_30.iloc[0]\n# Convert days to days from start\nusa_death_pred['days_from_start'] = (df.index - df.index[0]).days; df\n","a0396c01":"usa_death_pred = death_pred_df_sorted_30.iloc[0]\nusa_death_pred.to_csv('usa_p.csv')","59765cc3":"usa_death_pred","c73109a6":"y","021eeff2":"### Using random Weights for initialization of weights","fa844ed8":"### Update with GDP\n","f1f6b54d":"### Changing date time to time delta","028fd650":"## Update with daily data","a9c05b28":"### Splitting Data","847c6779":"# Preprocessing","61354b9d":"## Country Wise Stats","af9228a5":"### Processing for modelling\n\nStandard Scaler : Normal Distribution. Scales data to unit variance\n\nMinMaxScaler(feature_range = (0, 1)) will transform each value in the column proportionally within the range [0,1].Use StandardScaler if you know the data distribution is normal.","899518ce":"# Modelling with Linear Regression","b5685118":"### Preprocessing Continents","1d5e2e6f":"# Linear Regression","a6028ddc":"# Using Scikit Learn to verify","0ef9aed5":"Sample with USA","6aae6a60":"# Importing Data\n\n* All datasets are sourced from [COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University](https:\/\/github.com\/CSSEGISandData\/COVID-19) which is updated daily by them.\n* Click [here](https:\/\/coronavirus.jhu.edu\/map.html) to visit dashboard created by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University.","f1b4a0fc":"# Data Viz\n","38f08782":"## Linear Regression Models\n\n\nSimple Linear Regression\n\n`y = b0 + b1 * x`\n\nwhere b0 and b1 are the coefficients we must estimate from the training data.\n```\nB1 = sum((x(i) - mean(x)) * (y(i) - mean(y))) \/ sum( (x(i) - mean(x))^2 )\nB0 = mean(y) - B1 * mean(x)\n```","8fb52edf":"Calculating Mean and Variance\n\nMean: \n\n`mean(x) = sum(x) \/ count(x)`\n\nVariance:\n\n`variance = sum( (x - mean(x))^2 )`","30ba7ecf":"## Deaths over time (Macro)","124774f5":"Deaths as the only parameter","10d8e715":"### Without using Scikit learn"}}