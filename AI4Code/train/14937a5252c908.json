{"cell_type":{"4e3d2b51":"code","8d8cf1d0":"code","3631d40f":"code","77c0271c":"code","4e7ac2ee":"code","12fd426a":"code","06509cd1":"code","2a3d03df":"code","405a2515":"code","bebc8ddf":"code","dc1572bb":"code","20a7b67c":"code","20e6d1f2":"code","ac09f2d1":"code","e60b0e0f":"code","4cba3ea1":"code","192ece17":"code","319052f0":"markdown","39b38b5e":"markdown","eef8869d":"markdown","b1f2916e":"markdown","66f3f7e2":"markdown","df706a39":"markdown","f341da1c":"markdown","f2fa19c8":"markdown","f1d56032":"markdown","ed84c03d":"markdown","b365e25c":"markdown","bf7c3c29":"markdown","6308a3cb":"markdown","018f9d9a":"markdown","d05a72a6":"markdown","fa0515c2":"markdown","d1fb92d7":"markdown","5cb137fd":"markdown"},"source":{"4e3d2b51":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8d8cf1d0":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","3631d40f":"train.head()","77c0271c":"test.head()","4e7ac2ee":"train.columns","12fd426a":"train.info()","06509cd1":"plt.figure(figsize=(20,20))\nsns.heatmap(train.isnull(),yticklabels=False, cbar=False, cmap='viridis')","2a3d03df":"parameter_na=[i for i in train.columns if train[i].isnull().sum()>1]\n\nfor i in parameter_na:\n    print(i, np.round(train[i].isnull().mean(),2),' % missing values')","405a2515":"numerical_features = [feature for feature in train.columns if train[feature].dtypes != 'O']\n\nprint('Number of numerical datatype parameters:',len(numerical_features))","bebc8ddf":"train[numerical_features].head()","dc1572bb":"year_feature = [f for f in numerical_features if 'Yr' in f or 'Year' in f]\nyear_feature","20a7b67c":"train.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('Year the House was Sold')\nplt.ylabel('Median House Price')\nplt.title(\"House Price vs Year House was Sold\")","20e6d1f2":"# Let's check out other type sof variables like Numerical variables.\n# As, all know it is further subdivided into two types: 1) Continous, and 2) Discrete variables\n\ndiscrete_feature = [p for p in numerical_features if len(train[p].unique())<25 and p not in year_feature+['Id']]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","ac09f2d1":"# Names of columns in train datsets with discrete variables\nprint(discrete_feature)","e60b0e0f":"# Let's check out continuous varables:\ncont_feature=[f for f in numerical_features if f not in discrete_feature+year_feature+['Id']]\nprint(\"Continuous feature paramters are Count {}\".format(len(cont_feature)))","4cba3ea1":"print(cont_feature)","192ece17":"corrmat = train.corr()\ntop=corrmat.index\nplt.figure(figsize=(30,30))\ngraph = sns.heatmap(train[top].corr(),annot=True,cmap='RdYlGn')","319052f0":"Test and Train dataset visual analysis shows that the target variable or parameter is 'Saleprice'! Thus, one is going to predict the **Selling price** of house under sale.","39b38b5e":"train.info() code is highly useful to capture missing values, datatype of each and every columns.  ","eef8869d":".head() function helps to print top 5 rows of the datasets ","b1f2916e":"# Feature Engineering","66f3f7e2":"Inferences: PoolQC, Fence,MiscFeature,Alley, and FireplaceQu are few columns or features that are predominantly composed with missing values!","df706a39":"# **Do, UPVOTE this kernel if you LIKE!** ","f341da1c":"Do visualize the dataset to explore insights from it!","f2fa19c8":"Let's check out the dependent variable[SalePrice] and other independent variables!","f1d56032":"NOTE:\n* The **Prices** of houses decreases as years increase i.e olden day houses costed more than 2010. The graph clearly states that 2006 and 2007 were the year of highest house sales and sellers would have had happy times, unlike owners sellers today!  \n* Also, the curve slants down linearly from the later half of 2007 and still decreasing!! Alert for Business officials, and Investors. Careful!!","ed84c03d":"WOW! Huge and colorful right? But don'worry we are looking into the last row only as this shows how independent features correlate to our target feature 'SalesPrice'.\nNOTE: Don't eliminate columns as it shows negative correlation. Negative corrleation is nothing but inverse correlation that indicates that the two variables move in the opposite direction. It simply means that if feature X increases then feature Y decreases. If there is a good negative correlation ranging b\/w -0.9 to -1.0, you are lucky. ","b365e25c":"There are lot of temporal variables in this dataset. Temporal variables are those columns that comprises years\/data\/time related data. Let's check if these temporal variables contribute to our target variable or the parameter we are predict (SalePrice). If there is no good realtionship b\/w these two paramters let's eliminate them and get rid from the 'CURSE OF DIMENSIONALITY'! ","bf7c3c29":"# Importing the datasets into memory","6308a3cb":"Accept the Challlenge! Lol!!\nLet's clean the data!! However one should not drop columns based on presence of missing values! Here come into play statistics in DS projects. But we have SimpleImputer as a weapon to combat with missing values!!","018f9d9a":"# COMPLETE LIFE-CYCLE OF A DATA SCIENCE PROJECT\n\n**1. Exploratory Data Analysis\n**2. Feature Engg and Selection \n**3. Best Model Selection using Sklearn, AUTOML TPOT, Tensorflow, and Keras\n**4. Hyper-paramter tuning and modelling ","d05a72a6":"# Importing Necessary Libraries and Packages","fa0515c2":"Check out my Second stage in Complete Data Science project: Feature engg and selection stage","d1fb92d7":"Simple Imputer does not work when the dataset has categorical values i.e non-numerical values. eg..Male-Female in gender column that needs to be converted to 0s and 1s.  So, officially moving into Feature Engg step of Data Science!","5cb137fd":"Again, we can eliminate only Id column as it is unnecessary in this application!But the temporal variable spaly an important role as visualized in the above graph!"}}