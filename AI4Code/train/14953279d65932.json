{"cell_type":{"35fa81cf":"code","d1a51687":"code","a866cc83":"code","c5326feb":"code","26770367":"code","a9c57286":"code","8d7b74bb":"code","50dadf12":"code","a009a958":"code","6dc47038":"code","0f4c2362":"code","ccf7537a":"code","a5db49d1":"code","a1302014":"code","0f72bbb0":"code","785c0be9":"code","dea53f01":"code","fe75ae1d":"code","0098e794":"code","7117726a":"code","0bdde0b5":"code","27d0f757":"code","673ee983":"code","5aafcda5":"code","82b6f83b":"code","7b342405":"code","54ded4f7":"code","2bb44d99":"code","c6688769":"code","b0ced348":"code","a4cb29b5":"code","4dec5602":"code","493d6e10":"code","6a12097d":"code","81464efe":"code","ef6eedc4":"code","4ea7424b":"code","1c366fd6":"code","40828ae0":"code","27c0b366":"code","7faed7e9":"code","8a3078a1":"code","d3561b0e":"code","3c881b4b":"code","d7c13582":"code","bc30ab72":"code","a8940d36":"code","af900f45":"code","bfa2f515":"code","9f5b6c66":"code","fc35810c":"code","a50889ba":"code","869b7164":"markdown","8a1de43e":"markdown","f6d2b334":"markdown","4bb092ed":"markdown","c6dd5741":"markdown","c82691ae":"markdown","817cca05":"markdown","0108912d":"markdown","32f25e73":"markdown","44de1f3c":"markdown","dbb18143":"markdown","1f08e015":"markdown","1cd29342":"markdown","f02f2d89":"markdown","6fd55321":"markdown","37db2c7c":"markdown","9de29be2":"markdown","569e7aac":"markdown","ef1908f7":"markdown","94f8e06f":"markdown","d7499913":"markdown","ee04b660":"markdown","006e2591":"markdown","16c39813":"markdown","fc481884":"markdown","f73a338b":"markdown","7fc008e4":"markdown","c8740f10":"markdown","4de14e91":"markdown","cc5b980f":"markdown","5cd5aa2d":"markdown"},"source":{"35fa81cf":"import pandas as pd\nimport matplotlib as mpl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom xgboost import XGBClassifier, plot_importance\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve\nimport numpy as np","d1a51687":"dataset = pd.read_excel('dataset.xlsx', index_col=0)","a866cc83":"dataset['Urine - pH'].replace('N\u00e3o Realizado', np.nan, inplace=True)","c5326feb":"dataset['Urine - pH'] = dataset['Urine - pH'].astype('float64')","26770367":"dataset.replace('not_done', np.nan, inplace=True)","a9c57286":"dataset['Urine - Leukocytes'].replace('<1000', '999', inplace=True)\ndataset['Urine - Leukocytes'] = dataset['Urine - Leukocytes'].astype('float64')","8d7b74bb":"dataset.replace('not_detected', 0, inplace=True)\ndataset.replace('detected', 0, inplace=True)\ndataset.replace('negative', 0, inplace=True)\ndataset.replace('positive', 1, inplace=True)\ndataset.replace('absent', 0, inplace=True)\ndataset.replace('present', 1, inplace=True)","50dadf12":"df_temp = dataset[['Urine - Aspect', 'Urine - Urobilinogen', 'Urine - Crystals', 'Urine - Color']].astype(\"str\").apply(LabelEncoder().fit_transform)\ndataset[['Urine - Aspect', 'Urine - Urobilinogen', 'Urine - Crystals', 'Urine - Color']] = df_temp.where(~dataset[['Urine - Aspect', 'Urine - Urobilinogen', 'Urine - Crystals', 'Urine - Color']].isna(), dataset[['Urine - Aspect', 'Urine - Urobilinogen', 'Urine - Crystals', 'Urine - Color']])","a009a958":"dataset['Urine - Aspect'] = dataset['Urine - Aspect'].astype(\"float64\")\ndataset['Urine - Urobilinogen'] = dataset['Urine - Urobilinogen'].astype(\"float64\")\ndataset['Urine - Crystals'] = dataset['Urine - Crystals'].astype(\"float64\")\ndataset['Urine - Color'] = dataset['Urine - Color'].astype(\"float64\")","6dc47038":"dataset.drop(columns=['Patient addmited to regular ward (1=yes, 0=no)',\n                      'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                      'Patient addmited to intensive care unit (1=yes, 0=no)'], inplace=True)","0f4c2362":"def plot_missing_data(missing_data, title):\n    f, ax = plt.subplots(figsize=(15, 6))\n    plt.xticks(rotation='90')\n    sns.barplot(x=missing_data.index, y=missing_data['Percent'])\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title(title, fontsize=15)","ccf7537a":"def get_missing_data(dataset):\n    total = dataset.isnull().sum().sort_values(ascending=False)\n    percent = (dataset.isnull().sum()\/dataset.isnull().count()).sort_values(ascending=False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","a5db49d1":"missing_data = get_missing_data(dataset)\nplot_missing_data(missing_data, 'Percent missing data by feature')\nmissing_data.head(10)","a1302014":"dataset_positive = dataset[dataset['SARS-Cov-2 exam result'] == 1]\nmissing_data_positive = get_missing_data(dataset_positive)\n\nplot_missing_data(missing_data_positive, 'Percent positive missing data by feature')\nmissing_data_positive.head(10)","0f72bbb0":"dataset_negative = dataset[dataset['SARS-Cov-2 exam result'] == 0]\nmissing_data_negative = get_missing_data(dataset_negative)\n\nplot_missing_data(missing_data_negative, 'Percent negative missing data by feature')\nmissing_data_negative.head(10)","785c0be9":"corrmat = abs(dataset.corr())","dea53f01":"# Correlation with output variable\ncor_target = corrmat[\"SARS-Cov-2 exam result\"]\n# Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.15].index.tolist()","fe75ae1d":"f, ax = plt.subplots(figsize=(16, 8))\nsns.heatmap(abs(dataset[relevant_features].corr().iloc[0:1, :]), yticklabels=[relevant_features[0]], xticklabels=relevant_features, vmin = 0.0, square=True, annot=True, vmax=1.0, cmap='RdPu')","0098e794":"nof_positive_cases = len(dataset_positive.index)\nnof_negative_cases = len(dataset_negative.index)","7117726a":"fig1, ax1 = plt.subplots()\nax1.pie([nof_positive_cases, nof_negative_cases], labels=['Positive cases', 'Negative cases'], autopct='%1.1f%%', startangle=90, colors=['#c0ffd5', '#ffc0cb'])\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.","0bdde0b5":"columns_to_exclude = missing_data_positive.index[missing_data_positive['Percent'] > 0.998].tolist()\ndataset.drop(columns=columns_to_exclude, inplace=True)\nprint(columns_to_exclude)","27d0f757":"# Redefine dataset positive and negative\ndataset_negative = dataset[dataset['SARS-Cov-2 exam result'] == 0]\ndataset_positive = dataset[dataset['SARS-Cov-2 exam result'] == 1]","673ee983":"dataset_negative = dataset_negative.dropna(axis=0, thresh=20)","5aafcda5":"X = pd.concat([dataset_negative, dataset_positive])","82b6f83b":"nof_positive_cases = len(dataset_positive.index)\nnof_negative_cases = len(dataset_negative.index)","7b342405":"fig1, ax1 = plt.subplots()\nax1.pie([nof_positive_cases, nof_negative_cases], labels=['Positive cases', 'Negative cases'], autopct='%1.1f%%', startangle=90, colors=['#c0ffd5', '#ffc0cb'])\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.","54ded4f7":"corrmat = abs(X.corr())","2bb44d99":"# Correlation with output variable\ncor_target = corrmat[\"SARS-Cov-2 exam result\"]\n# Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.15].index.tolist()","c6688769":"f, ax = plt.subplots(figsize=(8, 4))\nsns.heatmap(abs(X[relevant_features].corr().iloc[0:1, :]), yticklabels=[relevant_features[0]], xticklabels=relevant_features, vmin = 0.0, square=True, annot=True, vmax=1.0, cmap='RdPu')","b0ced348":"X_with_relevant_features = X[relevant_features]\ny_with_relevant_features = X_with_relevant_features['SARS-Cov-2 exam result']\nX_with_relevant_features.drop(columns=['SARS-Cov-2 exam result'], inplace=True)","a4cb29b5":"y = X['SARS-Cov-2 exam result']\nX.drop(columns=['SARS-Cov-2 exam result'], inplace=True)","4dec5602":"def print_scores(y_test, y_pred):\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    roc = roc_auc_score(y_test, y_pred)\n    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n    print(\"Precision: %.2f%% \" % (precision * 100))\n    print(\"Recall: %.2f%% \" % (recall * 100))\n    print(\"AUC: %.2f%% \" % (roc * 100))","493d6e10":"def plot_confusion_matrix(y_test, y_pred):\n    confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n    sns.heatmap(confusion_matrix, annot=True, cmap='RdPu')","6a12097d":"X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_with_relevant_features, y_with_relevant_features, test_size=0.2, random_state=42)","81464efe":"print(\"Number of samples in train set: %d\" % y_train_rf.shape)\nprint(\"Number of positive samples in train set: %d\" % (y_train_rf == 1).sum(axis=0))\nprint(\"Number of negative samples in train set: %d\" % (y_train_rf == 0).sum(axis=0))\nprint()\nprint(\"Number of samples in test set: %d\" % y_test_rf.shape)\nprint(\"Number of positive samples in test set: %d\" % (y_test_rf == 1).sum(axis=0))\nprint(\"Number of negative samples in test set: %d\" % (y_test_rf == 0).sum(axis=0))","ef6eedc4":"imp = SimpleImputer(strategy='median')\nimp = imp.fit(X_with_relevant_features)","4ea7424b":"rfc = RandomForestClassifier()\n\n# Define parameters and grid search\nn_estimators = [100, 300, 500, 800, 1000]\nmax_depth = [5, 8, 15, 25, 30]\ngrid = dict(n_estimators=n_estimators, max_depth=max_depth)\ngrid_search = GridSearchCV(estimator=rfc, param_grid=grid, n_jobs=-1, cv=10, scoring='recall', error_score=0)\ngrid_result = grid_search.fit(imp.transform(X_train_rf), y_train_rf)\nprint(\"Best recall: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","1c366fd6":"rfc.n_estimators = grid_result.best_params_['n_estimators']\nrfc.max_depth = grid_result.best_params_['max_depth']\n                                   \nmodel_rfc = rfc.fit(imp.transform(X_train_rf), y_train_rf)","40828ae0":"y_pred_rf = model_rfc.predict(imp.transform(X_test_rf))\nprint_scores(y_test_rf, y_pred_rf)","27c0b366":"plot_confusion_matrix(y_test_rf, y_pred_rf)","7faed7e9":"X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X, y, test_size=0.3, random_state=42)","8a3078a1":"X_test_xgb, X_validation_xgb, y_test_xgb, y_validation_xgb = train_test_split(X_test_xgb, y_test_xgb, test_size=0.5, random_state=42)","d3561b0e":"print(\"Number of samples in train set: %d\" % y_train_xgb.shape)\nprint(\"Number of positive samples in train set: %d\" % (y_train_xgb == 1).sum(axis=0))\nprint(\"Number of negative samples in train set: %d\" % (y_train_xgb == 0).sum(axis=0))\nprint()\nprint(\"Number of samples in validation set: %d\" % y_validation_xgb.shape)\nprint(\"Number of positive samples in validation set: %d\" % (y_validation_xgb == 1).sum(axis=0))\nprint(\"Number of negative samples in validation set: %d\" % (y_validation_xgb == 0).sum(axis=0))\nprint()\nprint(\"Number of samples in test set: %d\" % y_test_rf.shape)\nprint(\"Number of positive samples in test set: %d\" % (y_test_xgb == 1).sum(axis=0))\nprint(\"Number of negative samples in test set: %d\" % (y_test_xgb == 0).sum(axis=0))","3c881b4b":"model_xgb = XGBClassifier()\n\n# Define parameters and grid search\nn_estimators = [100, 300, 500, 700]\nsubsample = [0.5, 0.7, 1.0]\nmax_depth = [6, 7, 9]\ngrid = dict(n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\ngrid_search = GridSearchCV(estimator=model_xgb, param_grid=grid, n_jobs=-1, cv=10, scoring='roc_auc', error_score=0)\ngrid_result = grid_search.fit(X_train_xgb, y_train_xgb)\nprint(\"Best AUC: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","d7c13582":"model_xgb.n_estimators = grid_result.best_params_['n_estimators']\nmodel_xgb.subsample = grid_result.best_params_['subsample']\nmodel_xgb.max_depth = grid_result.best_params_['max_depth']","bc30ab72":"model_xgb.fit(X_train_xgb, y_train_xgb, eval_metric='auc', eval_set=[(X_train_xgb, y_train_xgb), (X_validation_xgb, y_validation_xgb)], verbose=False)","a8940d36":"val_predictions_xgb = model_xgb.predict(X_validation_xgb)\nprint_scores(y_validation_xgb, val_predictions_xgb)","af900f45":"predictions_xgb = model_xgb.predict(X_test_xgb)\nprint_scores(y_test_xgb, predictions_xgb)","bfa2f515":"plot_confusion_matrix(y_test_xgb, predictions_xgb)","9f5b6c66":"feature_importances = model_xgb.get_booster().get_fscore()\nfeature_importances_df = pd.DataFrame({'Feature Score': list(feature_importances.values()), 'Features': list(feature_importances.keys())})\nfeature_importances_df.sort_values(by='Feature Score', ascending=False, inplace=True)\nfeature_importances_df = feature_importances_df.head(15)","fc35810c":"f, ax = plt.subplots(figsize=(7, 7))\nplt.title('Top 15 Feature Importances', fontsize=14)\nsns.barplot(x=feature_importances_df['Feature Score'], y=feature_importances_df['Features'])","a50889ba":"f, ax = plt.subplots(figsize=(8, 8))\nplt.plot([0, 1], [0, 1], '--', color='silver')\nplt.title('ROC Curve', fontsize=14)\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nfpr, tpr, thresholds = roc_curve(y_test_xgb, model_xgb.predict_proba(X_test_xgb)[:,1]) \nsns.lineplot(x=fpr, y=tpr, color=sns.color_palette(\"husl\", 8)[-2], linewidth=2, label=\"AUC = 95.41%\")","869b7164":"### XGBoost Classifier","8a1de43e":"**Authors: **[Isabela Telles](https:\/\/github.com\/isabelatelles\/) and [Thamiris Coelho](https:\/\/github.com\/thamycoelho)","f6d2b334":"Then, we trained the XGBoost Classifier ","4bb092ed":"### Negative and Positive cases","c6dd5741":"### Metrics Choice","c82691ae":"## Dealing with missing values and imbalanced data","817cca05":"The second consisted in using all of the features to train our models.","0108912d":"Below, we can see the percentage of negative and positive cases after under-sampling.","32f25e73":"And our results evaluated for test set, along with the confusion matrix, feature importances and ROC curve.","44de1f3c":"Finally, our results evaluated for validation set.","dbb18143":"As can be seen from the pie chart above, the dataset is highly imbalanced. We tried to train some models with the imbalanced data, but the results achieved were around 6% of recall. Then, in order to balance the dataset we chose to keep only the rows with at least 20 non-NaN values.","1f08e015":"## Correlation Vector","1cd29342":"### Replace column values that should be empty for NaN","f02f2d89":"First we split the dataset into train, validation and test set in a proportion of 70:15:15.","6fd55321":"### Correlation Vector","37db2c7c":"We tried to use two approaches, the first consisted in using only the revelant features to train our models.","9de29be2":"## Data Preparation","569e7aac":"### Replace True and False values for 0 and 1","ef1908f7":"The chosen metrics to train our models were:\n\n- Recall, because we wanted to measure the proportion of actual positive cases of COVID-19 identified correctly;\n- AUC, because we wanted to measure the probability that the model ranks a random positive case of COVID-19 more highly than a random negative COVID-19 case.\n\nWe believe that the goal is to avoid as many false negatives as possible, because a false negative would mean a patient wrongfully diagnosed could infect others. We're also going to share scores as accuracy and precision for informational reasons.","94f8e06f":"### Drop unnecessary columns","d7499913":"### Define features and target","ee04b660":"## Training Binary Classifier of COVID-19","006e2591":"First we split the dataset into train and test set, in a proportion of 80:20.","16c39813":"Then, we imput the median values into the NaN values.","fc481884":"### Random Forest Classifier","f73a338b":"## Negative and Positive Cases","7fc008e4":"Finally, we trained the random forest classifier using 10-fold cross validation, in order to avoid overfitting, and grid search, in order to tune hyperparameters.","c8740f10":"First of all, we chose to drop the features with more than 99,8% of NaN values in positive cases, since we don't believe they'll help to predict COVID-19.","4de14e91":"### Replace Leukocytes' values '<1000' for 999","cc5b980f":"### Label Encoding","5cd5aa2d":"## Analysis of missing data"}}