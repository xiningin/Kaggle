{"cell_type":{"c080cb78":"code","52d3a51b":"code","b1e4c188":"code","63efab34":"code","f50b4940":"code","0dd4a0de":"code","1ae39fd7":"code","99768f84":"code","83c6117e":"code","dc4f3962":"code","92c7d007":"code","8fc5762d":"code","7d4251df":"code","44515557":"code","e5ddb810":"code","3aa3e3b0":"code","8ea06d25":"code","9215fce6":"code","920bcfc1":"code","6ac35c6e":"code","a15c57ad":"code","e7013d60":"code","835f7af4":"code","1fec1a21":"code","89f0dd9d":"markdown","6e19c2fa":"markdown","397badb4":"markdown","08c07210":"markdown","71a5e192":"markdown","9996b17f":"markdown","f72ced65":"markdown","e9a455f3":"markdown","ce05adba":"markdown","9cb1f206":"markdown","6b06138b":"markdown","9d6c6a96":"markdown","ed3e8880":"markdown","22e838ba":"markdown","3863733c":"markdown","ce9d338f":"markdown","f7e3f207":"markdown","13847028":"markdown","1b7fa7bc":"markdown","2092abf7":"markdown","afb32407":"markdown","6f980fdc":"markdown","1992d32d":"markdown","25e0dc1d":"markdown","1624ffca":"markdown","dd0a3935":"markdown","a5d36648":"markdown","4930d145":"markdown"},"source":{"c080cb78":"!pip install pycaret","52d3a51b":"# Importing the libraries\nimport numpy as np\nimport pandas as pd\n\nfrom pycaret.classification import *\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(font = 'Serif', style = 'white', rc = {'axes.facecolor':'#f1f1f1', 'figure.facecolor':'#f1f1f1'})\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","b1e4c188":"# Reading the data\ndf = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\ndf.head()","63efab34":"# checking the data types\ndf.info()","f50b4940":"# 'Potability' is a categorical feature, so changing it's data types from int to category\ndf['Potability'] = df['Potability'].astype('category')","0dd4a0de":"# Looking for the missing values\ndf.isnull().sum()*100\/len(df)","1ae39fd7":"numeric_col = df.select_dtypes(float).columns.to_list()","99768f84":"fig = plt.figure(figsize = (10,8))\naxis = sns.heatmap(df[numeric_col].isnull(), cbar=False)\naxis.set_title('Missing Values', size = 16, weight = 'bold')\naxis.set_xticklabels(numeric_col, rotation = 30)\naxis.set_xlabel('Numeric Features', size = 12, weight = 'bold');","83c6117e":"fig, (axis1,axis2) = plt.subplots(1, 2, figsize = (10,6))\nsns.kdeplot(x = 'Sulfate', hue = 'Potability', fill = True, data = df, hue_order=[1,0], ax = axis1)\nsns.violinplot(x = 'Potability', y = 'Sulfate', data = df, ax = axis2)\n\naxis1.set_title('Kde plot', size = 12, weight = 'bold')\naxis2.set_title('Violin plot', size = 12, weight = 'bold')\nfig.suptitle('Sulfate values distribution', size = 16, weight = 'bold');","dc4f3962":"fig, ax = plt.subplots(9, 2, figsize=(12,20), constrained_layout = True)\n\nfor i, col in enumerate(numeric_col):\n    sns.boxplot(x = 'Potability', y = col, data = df, ax = ax[i][0])\n    \n    sns.kdeplot(x = col, hue = 'Potability', fill = True, multiple = 'stack',\n                alpha = 0.6, linewidth = 1.5, data = df, ax = ax[i][1])\n    ax[i][0].set_xlabel(None)\n    ax[i][0].set_ylabel(col, size = 14, weight = 'bold')\n    ax[i][1].set_xlabel(None)\n    ax[i][1].set_ylabel(None)\n    \nfig.suptitle('Features Analysis', size = 16, weight = 'bold');","92c7d007":"fig = plt.figure()\nsns.pairplot(df, hue = 'Potability')","8fc5762d":"# Correlation between numeric variables\nfig=plt.figure(figsize=(10,7))\naxis=sns.heatmap(df[numeric_col].corr(), annot=True, linewidths=3, square=True, cmap='Blues', fmt=\".0%\")\n\naxis.set_title('Correlation between the features', fontsize=16, weight='bold', y=1.05);\naxis.set_xticklabels(numeric_col, fontsize=12)\naxis.set_yticklabels(numeric_col, fontsize=12, rotation=0);","7d4251df":"colors = ['#06344d', '#00b2ff']\n\nfig = plt.figure(figsize = (10, 6))\nax = sns.countplot(x = 'Potability', data = df)\n\nfor i in ax.patches:\n    ax.text(x = i.get_x() + i.get_width()\/2, y = i.get_height()\/7, \n            s = f\"{np.round(i.get_height()\/len(df)*100, 0)}%\", \n            ha = 'center', size = 50, weight = 'bold', rotation = 90, color = 'white')\n\nplt.title(\"Water Potability Count\", size = 20, weight = 'bold')\n\nplt.annotate(text = \"Non Potable Water\", xytext = (0.6, 1900), xy = (0.1, 1500),\n             arrowprops = dict(arrowstyle = \"->\", color = 'black', connectionstyle = \"angle3, angleA = 0, angleB = 90\"), \n             color = 'red', weight = 'bold', size = 14)\nplt.annotate(text = \"Potable Water\", xytext = (0.6, 1400), xy = (1.3, 1000), \n             arrowprops = dict(arrowstyle = \"->\", color = 'black', connectionstyle = \"angle3, angleA = 0, angleB = 90\"), \n             color = 'green', weight = 'bold', size = 14)\n\nplt.xlabel(None)\nplt.ylabel('Number of Samples', weight = 'bold');","44515557":"# Using 'setup' from pycaret.classification for preprocessing the data\nclf = setup(df, target = 'Potability',\n            remove_outliers = True, outliers_threshold = 0.05, # Removing outliers with threshold of 5 percentile\n            numeric_imputation = 'mean', # Imputing missing values with mean\n            normalize = True, # Normalizing the features, so that Gradient Descent will converge fast\n            normalize_method = 'zscore', # Mean => 0 and std. deviation => 1\n            train_size = 0.8,\n            fold = 10, # Number of K-folds\n            use_gpu = True)","e5ddb810":"best_model = compare_models()","3aa3e3b0":"# Creating the model\ncatboost = create_model('catboost')","8ea06d25":"# Results for Test set\nresult = predict_model(catboost)","9215fce6":"plot_model(catboost)","920bcfc1":"plot_model(catboost, plot = 'confusion_matrix')","6ac35c6e":"plot_model(catboost, plot = 'boundary')","a15c57ad":"# This may take a while\nplot_model(catboost, plot = 'learning')","e7013d60":"plot_model(catboost, 'feature')","835f7af4":"interpret_model(catboost)","1fec1a21":"interpret_model(catboost, plot = 'reason', observation = 20) # Checking for sample no. 20","89f0dd9d":"> `ph` and `Trihalomethanes` have less than 20% missing values, so they can be imputed. I generally practice this 20% rule, but it also depends on the data and business demand. Coming to the `Sulfate` feature, as it has more than 20% missing values, let's do some univariate analysis on it and if we find that it is important, then we will impute it too or else drop the whole column.\n\n> The imputation part will happen by setting `numeric_imputation` to True, while data setup. First let's do some univariate analysis on `Sulfate` column.","6e19c2fa":"<h2><center>Analysing the model \ud83e\uddd0<\/center><\/h2>\n\n> Analyzing performance of trained machine learning model is an integral step in any machine learning workflow. Analyzing model performance in PyCaret is as simple as writing plot_model. The function takes trained model object and type of plot as string within plot_model function.\n\n> The available plots are Area Under the Curve, Discrimination Threshold, Precision Recall Curve, Confusion Matrix, etc. for more info, click [here](https:\/\/pycaret.org\/plot-model\/).","397badb4":"> From the Kdeplot and Violin plot, the distribution of `Sulfate` is little different when the `Potability` is 0 and when it is 1. So, I think `Sulphate` has some influence on the `Potability`, so we will keep it and impute it too.","08c07210":"> This plot is based on the SHAP values. This plot is made of all the dots in the train data. It demonstrates the following information:\n\n> **Feature importance**: Variables are ranked in descending order.\n\n> **Impact**: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n\n> **Original value**: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n\n>**Correlation**: A high level of the `ph` has a high and positive impact on the potability of water. The \u201chigh\u201d comes from the red color, and the \u201cpositive\u201d impact is shown on the X-axis. Similarly, we can say that, `Hardness` is negatively correlated with the potability of water.\n\n> \ud83d\udcd8 If you want to learn about them in detail, you can read this [blog](https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d).","71a5e192":"<h1><center> Selecting the ML model <\/center><\/h1>\n\n> There are many models available for classification e.g. Logistic Regression, SVC, Decision Tree, Random Forest, and list goes on...so which model to choose? \n\n> <center><img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTIM5JO2fkc8oZ_eh2xRZGlSWTLvqaqtJ-UjpM1-0slaP37ZYHMsVmskjPj3mEo4hxvr-k&usqp=CAU\"><\/center>\n\n> One way is that, choosing the model by comparing them on certain parameters and then selecting the best, so let's compare them first and then decide which is better.\n\n> If you are worried to see a page long code, for comparing the models, then don't worry! `compare_models` trains varies models like Logistic Regression, Decision Tree, SVM, Random Forest, XGBoost, etc. and compares them based on varies parameters like Accuracy, AUC ROC score, Recall, Precision, etc. So, it becomes easy for us to choose from them.","9996b17f":"<h1><center>Let's dive into the Data \ud83c\udfca<\/center><\/h1>","f72ced65":"<h3><center>Checking for Class Imbalance<\/center><\/h3>","e9a455f3":"> The values are missing from the random indexes, so it is hard to identify what is causing them. Let's see how the values are distributed for `Sulfate`.","ce05adba":"> It does'nt seem to be any linear relationship between the features as the plots are kind of circle. We can say that their is no multicollinearity but to be 100% sure, let's find out the correlation between them.","9cb1f206":">When you run the cell, you have to press *enter* for proceeding further, otherwise you can type *quit* for not proceeding further. Once you preceed further, you will see all the parameters and their corresponding values. If you are not happy with this, you can change the values and run the setup again. For this example, the parameters look fine so I will proceed further.\n\n>\ud83d\udccc If you want to see the train and test data, you can use `get_config('X_train')` and `get_config('X_test')`","6b06138b":"![image](https:\/\/i2.wp.com\/amazingsoak.com\/wp-content\/uploads\/2019\/08\/mineral-water-in-qatar.png?fit=1020%2C544&ssl=1)\n\n<h1><center> Introduction <\/center><\/h1>\n\n> Human body contains 60% to 65% of water. We can't even survive without water for few days. But we have a huge source of water. Water covers majority portion of the Earth.\n\n> Though \ud83d\udca7 water is the most abandant resource on the Earth, still only about 2% water can be used for drinking. Now, the 2% also contains suspended solids, sulfates, organic carbon, etc. and if their portions increases more than specified value, this water can't be used for drinking. So the most abandant resource resource on the Earth, which covers 70% of our planet and we can't use it!\n\n> The \ud83c\udfaf aim of this exercise is to find out which parameter influences the potability of water and to differntiate potable and non potable water using a ML model.\n\n> This notebook as the title suggests covers the EDA and ML for the dataset. I am using [PyCharet library](https:\/\/pycaret.org\/guide\/), which a user friendly auto ML library and can save your ton of time.","9d6c6a96":"<div> <h3><center style=\"background-color:#00b2ff; color:white;\">\ud83e\uddac Decision Boundary \ud83e\uddac<\/center><\/h3><\/div>","ed3e8880":"## Data Preprocessing ","22e838ba":"#### Run this `!pip install pycaret` before proceeding","3863733c":"<h3><center>Univariate Analysis on Sulfate column<\/center><\/h3>","ce9d338f":"<div> <h3><center style=\"background-color:#00b2ff; color:white;\">AUC ROC Curve<\/center><\/h3><\/div>","f7e3f207":"> The maximum correlation is 17% (negetive) between `Sulfate` and `Solids`, it means only 17% variance in the `Solids` can be explained by `Sulfate` and vice versa.\n\n> It seems that there is no multicollinearity, as for it to be present, the correlation should be higher than 80-85% (positive or negetive). If it is present, we can set `remove_multicollinearity` to True, during data setup.","13847028":"<div> <h3><center style=\"background-color:#00b2ff; color:white;\">Learning Curve \ud83d\udcc8<\/center><\/h3><\/div>","1b7fa7bc":"> There certainly are many data points which are above 75 percentile and below 25 pecentile. I will remove the data points which above 95 percentile and below 5 percentile. You can choose any other threshold values also. The removal of the outliers will be done during data setup using `remove_outliers`.\n\n> Now, let's see how the features influence each other.","2092abf7":"<h3><center>Hyperparameter tuning<\/center><\/h3>\n\n> Hyperparameter tuning is a time consuming task, as there are lots of hyperparameter associated with a algorithm and setting them up to the right value to get best results can take significant amount of \u23f3 time \u231b.\n\n> With PyCaret, tuning hyperparameters of a ML model in any module is as simple as writing `tune_model`. It tunes the hyperparameter of the model passed as an estimator using Random grid search with pre-defined grids that are fully customizable.\n\n> I am not tuning the Catboost model, as when I run it previously it took like 30 mins, and the AUC score didn't improve much. This is only the case with heavy models like catboost model, for others light weight models the hyperparameter tuning takes lesser time. So, if you want to improve the AUC score on the account of time, you can run the below cell. \n\n`tune_model(catboost, optimize = 'AUC')`\n","afb32407":"<div> <h3><center style=\"background-color:#00b2ff; color:white;\">\ud83d\ude35 Confusion Matrix<\/center><\/h3><\/div>","6f980fdc":"<h3><center>Feature Importance \u2755<\/center><\/h3><\/div>","1992d32d":"<h3><center>Local Interpretation<\/center><\/h3><\/div>\n\n> If you want to know, for a perticular sample why the model predicted a perticular output, then you can use this.","25e0dc1d":"### That's it for this notebook. If you like it, don't forget to upvote!","1624ffca":"> **The base value** (-0.582) : the value that would be predicted if we did not know any features for the current output\n\n> **Red\/blue**: Features that push the prediction higher (to the right) are shown in red, and those pushing the prediction lower are in blue.\n\n> The prediction is **0** for the sample 20, you can check this using `df.loc[20,'Potability']`","dd0a3935":"<h2><center> Evaluation Criterion \ud83e\uddea <\/center><\/h2>\n\n> AUC (Area Under Curve) - ROC (Receiver Operator Characteristic) curve\n\n><center><img src=\"https:\/\/miro.medium.com\/max\/722\/1*pk05QGzoWhCgRiiFbz-oKQ.png\"><\/center>\n\n> AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability.\n>It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the AUC, the better the model is at distinguishing between potable water with the non potable water, which is imp to us and so I choose AUC as evaluation metrics.\n\n> The **Catboost** has highest AUC, so we will select that for model creation.\n","a5d36648":"> The count of both the classes are comparable (in 1000's), so there is no class imbalance. If it is there, you can set `fix_imbalance` to True (by defalt it is False) and also can choose the method to remove it using `fix_imbalance_method` (by defalt it is SMOTE)\n\n> Now, everything is set, let's setup the data.","4930d145":"<h2><center>Interpreting Model \ud83d\ude2e<\/center><\/h2>\n\n> \ud83d\udccc Interpreting Model for binary classification, only tree based models can only be used like Desicion Tree, Random Forest, XGBoost, CatBoost, lightgbm, etc. So, if you have created some other model, you have to create one of the tree based model to interprete the model."}}