{"cell_type":{"6b5cf8da":"code","e2f4e233":"code","3b20cc76":"code","b44d875e":"code","5aa5d35f":"code","5a62c30b":"code","2886eecf":"code","ac5f23a3":"code","cb71893d":"code","8f2cf428":"code","8285304f":"code","b2b10b9c":"code","9586fe99":"code","4af9f425":"code","ac885fd6":"code","7642b9be":"code","e63dab47":"code","e4eeade9":"code","fc78e823":"code","1f4b7990":"code","ff1d7db4":"code","dff87ad6":"code","cdf682ed":"code","050a427d":"code","9d0599cf":"code","76effa43":"code","8d8614b2":"code","c2a396d7":"code","04e90c68":"code","1360c322":"code","958f5653":"code","131936cf":"code","01f7bd87":"code","71c25dac":"code","42feb2be":"code","6e1d2434":"code","e273bc69":"code","9c5bca3a":"code","482a7d2a":"code","67e69075":"code","f7108987":"code","d28fdb93":"code","d4d4c6c5":"code","a866fb91":"code","20e1d34b":"code","9801c113":"code","9f97aba1":"code","459ce69b":"code","bdda2b11":"code","f6b095c7":"code","b2858fb6":"code","19ab4bea":"code","facd97b6":"code","84de99d7":"code","be2b492f":"code","32339cbe":"code","6fa96929":"code","40ce9566":"markdown","e75fc0a1":"markdown","7dc54e46":"markdown","0ba0dac8":"markdown","105ff1c1":"markdown","5b093756":"markdown","391d89c8":"markdown","11111b0e":"markdown","59faff7c":"markdown","dc1b8a62":"markdown","d472c2aa":"markdown","28c0e236":"markdown","08fd88fb":"markdown","914f3523":"markdown","36db6866":"markdown","230661af":"markdown","bb388748":"markdown","730a33d6":"markdown","ed222787":"markdown","a4474a00":"markdown","4d16465e":"markdown","a78db342":"markdown","0b82c2da":"markdown","80e8ed65":"markdown","b56198ff":"markdown","c9534028":"markdown","1d2a2560":"markdown","f3cec7ce":"markdown","67780c59":"markdown"},"source":{"6b5cf8da":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","e2f4e233":"from keras.datasets import cifar10","3b20cc76":"(X_train, y_train), (X_test, y_test) = cifar10.load_data()\nprint(\"train set: X=%s , Y =%s\" % (X_train.shape, y_train.shape))\nprint(\"test set: X=%s , Y =%s\" % (X_test.shape, y_test.shape))","b44d875e":"feature_name = {0:'airplane',\n                1:'automobile',\n                2:'bird',\n                3:'cat',\n                4:'deer',\n                5:'dog',\n                6:'frog',\n                7:'horse',\n                8:'ship',\n                9:'truck'\n               }","5aa5d35f":"print(y_train[0])\nprint(X_train[0])","5a62c30b":"plt.figure(figsize=(10,5))\nfor i in range (5):\n  plt.subplot(1,5,i+1)\n  plt.imshow(X_train[i])\n  plt.title(feature_name[y_train[i][0]])","2886eecf":"from tensorflow import keras\nfrom tensorflow.keras import layers","ac5f23a3":"resize = tf.keras.Sequential([\n  layers.experimental.preprocessing.Resizing(16, 16),\n  layers.experimental.preprocessing.Rescaling(1.\/255),\n])","cb71893d":"resized_img = resize(X_train[1])","8f2cf428":"plt.subplot(121)\nplt.imshow(X_train[1])\nplt.subplot(122)\nplt.imshow(resized_img)","8285304f":"gray_img = tf.image.rgb_to_grayscale(X_train[1])","b2b10b9c":"plt.subplot(121)\nplt.imshow(X_train[1])\nplt.subplot(122)\nplt.imshow(gray_img,cmap='gray')","9586fe99":"data_augmentation = tf.keras.Sequential([\n  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n  layers.experimental.preprocessing.RandomRotation(0.2),\n])","4af9f425":"fliped_img = tf.expand_dims(X_train[1], 0)","ac885fd6":"plt.figure(figsize=(10, 10))\nfor i in range(9):\n  augmented_image = data_augmentation(fliped_img)\n  ax = plt.subplot(3, 3, i + 1)\n  plt.imshow(augmented_image[0])\n  plt.axis(\"off\")","7642b9be":"data_crop = tf.keras.Sequential([\n  layers.Cropping2D((4,4))\n])","e63dab47":"croped_img = data_crop(tf.expand_dims(X_train[1], 0))[0]","e4eeade9":"plt.subplot(121)\nplt.imshow(X_train[1])\nplt.subplot(122)\nplt.imshow(croped_img)","fc78e823":"data_translation = tf.keras.Sequential([\n  layers.experimental.preprocessing.RandomTranslation(.4,.4)\n])","1f4b7990":"translated_img = data_translation(tf.expand_dims(X_train[1], 0))[0]","ff1d7db4":"plt.subplot(121)\nplt.imshow(X_train[1])\nplt.subplot(122)\nplt.imshow(translated_img)","dff87ad6":"X_train = X_train\/255\nX_test = X_test\/255","cdf682ed":"X_train[0]","050a427d":"X_test[0]","9d0599cf":"from tensorflow.keras.utils import to_categorical\ny_train_cat = to_categorical(y_train)\ny_test_cat = to_categorical(y_test)","76effa43":"X_train_batches = []\ny_train_cat_batches = []\nbatch_size = X_train.shape[0]\/5\nfor i in range(5): \n  X_train_batches.append(X_train[int((batch_size)*i): int((batch_size)*(i+1))])\n  y_train_cat_batches.append(y_train_cat[int((batch_size)*i): int((batch_size)*(i+1))])","8d8614b2":"X_train_batches[0].shape","c2a396d7":"y_train_cat_batches[0].shape","04e90c68":"from tensorflow import keras\nfrom tensorflow.keras import layers","1360c322":"initial_learning_rate = 0.01\ndef lr_exp_decay(epoch, lr):\n    k = 0.1\n    return initial_learning_rate * np.exp(-k*epoch)","958f5653":"callbacks=[keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=1)]","131936cf":"ann = keras.models.Sequential()\n# ann.add(keras.layers.experimental.preprocessing.Resizing(16,16))\n# ann.add(layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"))\n# ann.add(layers.experimental.preprocessing.RandomRotation(0.2))\n# ann.add(layers.Cropping2D((2,2)))\nann.add(keras.layers.Flatten(input_shape=[32,32,3]))\nann.add(layers.Dense(4000,activation='relu'))\nann.add(layers.Dense(1000,activation='relu'))\nann.add(layers.Dense(4000,activation='relu'))\nann.add(layers.Dense(10,activation=\"softmax\"))","01f7bd87":"ann.compile(optimizer=keras.optimizers.Adagrad(),\n          loss=keras.losses.CategoricalCrossentropy(),\n          metrics=['accuracy'])","71c25dac":"model_history = ann.fit(X_train[:,:], y_train_cat, epochs=18, batch_size=32, validation_split=.2, callbacks=callbacks)","42feb2be":"model_history.params","6e1d2434":"ann.summary()","e273bc69":"import pydot\nkeras.utils.plot_model(ann)","9c5bca3a":"pd.DataFrame(model_history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,2.5)\nplt.show()","482a7d2a":"ann.evaluate(X_test, y_test_cat)","67e69075":"y_proba = ann.predict(X_test)\ny_proba.round(2)","f7108987":"y_pred = np.argmax(ann.predict(X_test), axis=-1)\ny_pred","d28fdb93":"y_pred_name = np.array(list(feature_name.values()))[y_pred]","d4d4c6c5":"y_pred_name","a866fb91":"plt.figure(figsize=(10,10))\nfor i in range(5):\n    plt.subplot(5,1,i+1)\n    plt.imshow(X_test[i])\n    plt.title('True label: {}, Prediction: {}'.format(feature_name[y_test_cat[i].argmax()], y_pred_name[i]))\nplt.tight_layout()   ","20e1d34b":"from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay","9801c113":"y_test_numbers = np.array([y.argmax() for y in y_test_cat])","9f97aba1":"cm = confusion_matrix(y_test_numbers, y_pred)","459ce69b":"cm_display = ConfusionMatrixDisplay(cm,display_labels=list(feature_name.values()))\nfig, ax = plt.subplots(figsize=(10,10))\ncm_display.plot(ax=ax)","bdda2b11":"print(classification_report(y_test_numbers, y_pred, target_names = list(feature_name.values())))","f6b095c7":"# !pip install Lime","b2858fb6":"from lime import lime_image","19ab4bea":"explainer = lime_image.LimeImageExplainer()","facd97b6":"explanation = explainer.explain_instance(X_test[0].astype('double'), ann.predict, top_labels=5, hide_color=0, num_samples=2000)","84de99d7":"from skimage.segmentation import mark_boundaries","be2b492f":"temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=1, hide_rest=True)\nplt.imshow(mark_boundaries(temp \/ 2 + 0.5, mask))","32339cbe":"temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=1, hide_rest=False)\nplt.imshow(mark_boundaries(temp \/ 2 + 0.5, mask))","6fa96929":"#Select the same class explained on the figures above.\nind =  explanation.top_labels[0]\n\n#Map each explanation weight to the corresponding superpixel\ndict_heatmap = dict(explanation.local_exp[ind])\nheatmap = np.vectorize(dict_heatmap.get)(explanation.segments) \n\n#Plot. The visualization makes more sense if a symmetrical colorbar is used.\nplt.imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())\nplt.colorbar()","40ce9566":"### Rotation","e75fc0a1":"## __Model Evaluation__","7dc54e46":"## __Data Preparation to Feed the Model__","0ba0dac8":"### Explanation of the network architecture:","105ff1c1":"## __Visualizing our data__","5b093756":"we are going to make 5 batches(optional) to lessen the training time of the model then we'll get result out of the full training set","391d89c8":"### Translation","11111b0e":"## __Model Evaluation__","59faff7c":"### Confusion Matrix","dc1b8a62":"## __Importing Cifar-10 Dataset__\nThe dataset is comprised of 60,000 32\u00d732 pixel color photographs of objects from 10 classes, such as frogs, birds, cats, ships, etc. With 6000 images per class, there are 50000 training images and 10000 test images. \nThe class labels and their standard associated integer values are listed below:\n\n0. airplane\n1. automobile\n2. bird\n3. cat\n4. deer\n5. dog\n6. frog\n7. horse\n8. ship\n9. truck\n\nThese are very small images, much smaller than a typical photograph, and the dataset was intended for computer vision research.","d472c2aa":"__I'll use it for just parameter tuning so in the training section, used all the data to feed the model.__","28c0e236":"Resizing an image is about reshaping it, but with losing some information or adding new rows and columns to an image, we can say it is something like filters or convolutions but with another approach.\n\nWe'll have in our model as a layer for reshaping","08fd88fb":"### Data Normalization","914f3523":"## __Model Interpertation__","36db6866":"## __Model Creation__","230661af":"## __Importing basic libraries__","bb388748":"### Resizing and Resacling","730a33d6":"Now we need to setup the learning process using compile method","ed222787":"## Conclusion","a4474a00":"## __Model Summary__","4d16465e":"### Crop","a78db342":"### Classification Report","0b82c2da":"## __Data Preprocessing and Augmentation__","80e8ed65":"There are two types of implementing neural networks in keras\n1. Sequntial API ( for simple ANNs )\n2. Functional API ( for complex architectures )\n\nAt first we are going to implement sequential API","b56198ff":"In this notebook, I tried to implement the best neural network architecture without using convolution.\nIn further versions or notebooks, I'll try to explain the model interpretability, the reason behind every choice of parameters, and data preprocessing.","c9534028":"The following arcitecture improvised out of a massive grid search of neurons, layers, hyperparameters as following\n( Using GridSearchCv, KerasClassifier ) :\n* optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adam']\n* learn_rate = [0.001, 0.005, 0.01, 0.05, 0.1]\n* batch_size = [0, 10, 32, 64, 128, 256]\n* epochs = [10, 20, 50, 100]\n\n( plus data preprocessing approches mentioned in the previous section)\n\nFrom the list above, the best parameters listed as [optmizer='Adagrad', **learn_rate=***, batch_size=32, epochs=20]\n\n*for better result, I'll try to make it adaptive with the time and the number of epochs and create and exponential decay pass it to callbacks\n\n__should hva mention that use early stopping for the GridSearch processing__","1d2a2560":"to prevent the model from biase we are going to make the labels categorical","f3cec7ce":"### RGB to Grayscale","67780c59":"We have some main kind of preprocessing and augmentation of image datasets:\n1. Resizing\n2. RGB_to_Grayscale conversion and vise versa\n3. Rotation\n4. translation\n5. Crop\n6. Guassian Blur\n\nand so on...\n\n<img src=\"https:\/\/blog.roboflow.com\/content\/images\/size\/w1600\/2020\/02\/preprocess-1.png\" alt=\"Drawing\" width=400\/>"}}