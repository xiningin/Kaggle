{"cell_type":{"72dee261":"code","4f6a6eb7":"code","f3467846":"code","eb0e153a":"code","34bfcbad":"code","99f13dd7":"code","fea1b457":"code","dbdfdfbb":"code","d8bcb2d5":"code","48392afb":"markdown","ba29695d":"markdown","79a3d041":"markdown","8acf2e99":"markdown","e5ec3bab":"markdown","7811ce2f":"markdown","39f4c57e":"markdown"},"source":{"72dee261":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","4f6a6eb7":"# Read data in\ndf = pd.read_csv('..\/input\/cuneiform-language-identification\/train.csv')\nprint('Shape:', df.shape)\ndf.sample(10).head(10)","f3467846":"# Helper functions\n## Preprocess cuneiform data\ndef preprocess(input):\n    '''\n    Preprocess lines in cuneiform script. Performs two steps: (1) Adds beggining- and end-of-sentence tokens \n    (B and E, respectively) (2) Adds a space between characters. Parameters:\n    :input: Pandas Series, each line being a document in cuneiform alphabet with no separation between characters.\n    '''\n    # Add beggining- and end-of-sentence caracter to the end - just a B and an E\n    input_mod = 'B' + input + 'E'\n\n    # Split line by character\n    for i in range(len(input)):\n        input_mod[i] = \" \".join(input_mod[i])\n\n    return input_mod\n\n## Count ngrams and put them in a dictionary\n## Elaborated from https:\/\/stackoverflow.com\/questions\/13423919\/computing-n-grams-using-python\ndef ngrams(input, n):\n    '''\n    Create a dictionary with each ngram and the number of occurrences\n    :input: string of characters separated by a white space (' ')\n    :n: int. Length of ngram\n    '''\n    input = input.split(' ')\n    output = {}\n   \n    for i in range(len(input)-n+1):\n        g = ' '.join(input[i:i+n])\n        output.setdefault(g, 0)\n        output[g] += 1\n       \n    for key in list(output):\n        if key[0] == 'E': # Drops 'E B' bigram\n            del output[key]\n\n    return output\n\n## Get a list of ngrams - similar to ngrams function, but this one doesn't return a dictionary with frequencies\ndef ngram_list(input, n):\n    '''\n    Return a list of ngrams within sentence. Params:\n    :input: String. Sequence of cuneiform characters\n    :n: Int. length of ngram. \n    '''\n    input = input.split(' ')\n    _ngrams = []\n    for i in range(len(input)-n+1):\n        g = ' '.join(input[i:i+n])\n        _ngrams.append(g)\n    return _ngrams\n\n## Calculate log probability of a sequence\ndef logprob(input, lang, probs):\n    '''\n    Calculate the probability of a sequence of ngrams. Params:\n    :input: list of ngrams\n    :lang: summerian language out of ['NEA', 'LTB', 'MPB', 'OLB', 'NEB', 'SUX', 'STB']\n    '''\n    _logprob = 0\n    for _ngram in input:                                                                                            # Iterate over ngrams within list of ngrams for that sequence\n        if _ngram in list(probs[lang]['Bigram']):                                                 \n            _logprob = _logprob + np.log(probs[lang]['Probability'][probs[lang]['Bigram'] == _ngram].values)      # If there is a probability for that ngram, we add it to our conditional probability\n        else:\n            _logprob = _logprob + np.log(0.00001)                                                                   # Update by very low probability (almost zero)\n\n    return float(_logprob)","eb0e153a":"# Define language model class\nclass CuneiPy:\n\n    def __init__(self):\n        self.languages = []\n        self.train_data = pd.DataFrame()\n        self._dfs = {}\n        self._freqs = {}\n        self._probs = {}\n        self.n = None\n        self.predictions = pd.DataFrame()\n\n    def fit(self, _df, input, target, n):\n        '''\n        Fits the model; i.e. calculates probability matrices with ngrams of length n for each language.\n        Parameters:\n        :_df: Pandas dataframe with raining data.\n        :input: String. Name of column containing text in cuneiform alphabet. \n        :target: String. Name of column containing the target values of our model. Values should be strings\n            such as \"SUX\" for Sumerian, \"LTB\" for Late Babylonian, etc.\n        :n: Integer. Length of n-grams.\n        '''\n        self.languages = list(set(_df[target]))\n        self.train_data = _df\n        self.n = n\n\n        # Preprocess training data\n        self.train_data['cuneiform_mod'] = preprocess(_df[input])\n\n        #\u00a0Split dataframes by language\n        for lang in self.languages:\n            self._dfs[lang] = self.train_data[self.train_data[target] == lang].reset_index(drop = True)\n\n        # Put frequencies in dictionaries by language\n        for lang in self.languages:\n            self._freqs[lang] = ngrams(\" \".join(self._dfs[lang]['cuneiform_mod']), n)\n\n        # Put probabilities in a dictionary of dataframes - These are our probability matrices\n        for lang in self.languages:\n            self._probs[lang] = pd.DataFrame(list(self._freqs[lang].items()),columns = ['Bigram','Frequency']).sort_values(by = 'Frequency', ascending=False)\n            self._probs[lang]['Probability'] = self._probs[lang]['Frequency'] \/ sum(self._probs[lang]['Frequency'])\n\n    def predict(self, input):\n        '''\n        Predict language of a given sequence in cuneiform language. Parameters:\n        :input: Pandas series containing text in cuneiform alphabet. Each observation will get assigned a \n            predicted language. \n        '''\n        self.predictions['cuneiform'] = input\n\n        # Preprocess text, just like in training\n        self.predictions['cuneiform_mod'] = preprocess(self.predictions['cuneiform']) \n\n        # Get all n-grams from the text\n        self.predictions['ngrams'] = [ngram_list(self.predictions['cuneiform_mod'][i], self.n) for i in range(len(self.predictions))]\n\n        # Return a column with the probability of the sequence belonging to each language\n        for lang in self.languages:\n            self.predictions[lang] = self.predictions.apply(lambda x: logprob(x['ngrams'], lang, self._probs), axis = 1)\n\n        # Predict label - Name of the column for which log probability is maximised\n        self.predictions['lang_pred'] = self.predictions[self.languages].idxmax(axis = 1)\n\n        return self.predictions['lang_pred']\n","34bfcbad":"# Split data into training and testing sets\ndf_test = df.sample(n = 1000, random_state = 10)\ndf_train = df.drop(df_test.index)\n\ndf_test = df_test.reset_index(drop = True)\ndf_train = df_train.reset_index(drop = True)","99f13dd7":"# Initialise model\ncp = CuneiPy()\n\n# Fit\ncp.fit(_df = df_train, \n       input = 'cuneiform',\n       target = 'lang',\n       n = 2)","fea1b457":"# Predict on test set\ndf_test['lang_pred'] = cp.predict(df_test['cuneiform']).values","dbdfdfbb":"# Classification report\nprint(classification_report(df_test['lang'], df_test['lang_pred']))","d8bcb2d5":"# Plot confussion matrix\ncm = confusion_matrix(df_test['lang'], df_test['lang_pred'])\nfig, ax = plt.subplots(figsize=(8, 8))\nax.matshow(cm, cmap=plt.cm.Reds, alpha=0.5)\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='large')\n\nclassNames = ['LTB','MPB', 'NEA', 'NEB', 'OLB', 'STB', 'SUX']\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, fontsize = 10)\nplt.yticks(tick_marks, classNames, fontsize = 10)\nplt.xlabel('Predictions', fontsize=12)\nplt.ylabel('Actuals', fontsize=12)\nplt.title('Classification of cuneiform texts by language',fontsize = 18)\n\nplt.show()","48392afb":"## Parting thoughts\nThanks for reading! This is my first Kaggle notebook, so if you have any feedback, it's more than welcome!\n\nIf you found the notebook interesting and you want to reach out to me, feel free to contact me on [Linkedin](https:\/\/www.linkedin.com\/in\/alvaro-corrales-cano\/) or via email. \n\nYou can read more about my approach in [this blog](https:\/\/towardsdatascience.com\/assyrian-or-babylonian-language-identification-in-cuneiform-texts-4f15a14a5d70) that I published in Towards Data Science.","ba29695d":"## Results","79a3d041":"## Construct the model","8acf2e99":"Not too bad! We get a weighted F1 score of 0.81. Not surprisingly, the languages wiht less support in the training sample, such as MBP (Middle Babylonian Peripheral) or Neo-Babylonian (NEB) get the lowest scores, but overall the model seems to perform relatively well for its level of complexity. ","e5ec3bab":"# A Language Model for Cuneiform Texts \n\n## Introduction\nIn this notebook I show how to construct a simple language model to identify the language or dialect in which the cuneiform texts are written. Generally speaking, a [language model](https:\/\/en.wikipedia.org\/wiki\/Language_model) is a probability distribution over sequences of words. Within this broad definition, we find different types of models. Some of the most common use the concept of _n-gram_, i.e. sequences of n consecutive characters in which we split the text. N-gram models are common in NLP literature. Their applications include speech recognition, machine translation, or handwriting recognition. Not surprisingly, there are some well-known packages in Python, such as [NLTK](http:\/\/www.nltk.org\/), that make constructing an N-gram model becomes a relatively easy task. Here, I use an N-gram model to predict the probability that a sequence of cuneiform n-grams belongs to a particular Ancient Mesopotamian language or dialect.\n\nNow, as you may have already guessed, I wouldn't be writing this post if I had followed that easy path for this challenge. On the contrary, this notebook is about how to construct an N-gram model from scratch. In fact, the only libraries I use are `numpy` and `pandas`. There are two reasons why I decided to do it this way: first of all, standard NLP libraries are designed to work with Latin characters, so adapting them to cuneiform text is a bit of a pain. Second, it's fun! I am a firm believer that the best way of testing your knowledge on a subject is to go to the basics and see if you can code it with as few specialised libraries as possible. That is what I do in this notebook.\n\nThe N-gram model implementation that I have come up with is inspired in the type of N-gram models that use Markov chains and the Markov assumption. Simply put, this assumption states that the probability of an n-gram in position $t$ depends only on the previous $k$ n-grams (where $k$ is usually 1). Hence, the probability of a sequence can be computed by taking the product of conditional probabilities of its n-grams. \n\nMy code does not _exactly_ do that, but something simpler. What the code actually does is to compute the probability of bigrams ($n$ = 2) for a training set of each of the languages, and estimates the probability that an observed sequence belong to each of the languages. The language with the highest probability is assigned as a prediction. \n\nObviously, more complex methods can be developed. However, I have found that this very simple model already performs really well on a test set, with a weighted F-1 score of ~0.8 (depending on how train-test split). \n\nHope you find it interesting!","7811ce2f":"## Implementation\nThe cells below show the helper functions and class that make up the implementation of the simple model explained in the introduction. The class `CuneiPy` follows the classic Scikit-Learn logic of `train()` and `predict()`.\n\nThis implementation has its limitations, of course. Perhaps the most important is that the prediction part, so if your test set is large, hit the button and go grab a coffee!","39f4c57e":"## Libraries and data"}}