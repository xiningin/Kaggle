{"cell_type":{"4cf35108":"code","81baf959":"code","2a35d059":"code","818ce1cf":"code","dc4a3294":"code","d32de43e":"code","4dcfc714":"code","b6dd64ff":"code","f48bfa1a":"code","f18f1935":"code","dbf5319b":"code","a2325e56":"code","3b355488":"code","3414d3ee":"code","b728b0b7":"code","43320b1e":"code","fa4100ca":"code","bff36532":"code","348c1ddc":"code","8f038e5c":"code","daf86c4f":"code","ea7301e3":"code","96c644bc":"code","029ca887":"code","ccba7f3f":"code","f983d751":"code","a8e0f688":"code","430e545c":"code","fa6979fb":"code","16497bcc":"code","7eba65b4":"code","40739c95":"code","8d7011a3":"code","f30b8dd6":"code","35d4664e":"code","9caa86e4":"code","8906b9db":"code","d3fc7407":"code","615073a5":"code","d9310955":"code","1007db7c":"code","dc1667ca":"code","58728384":"code","25a5f5d5":"code","3ceeb623":"code","beb83ad8":"code","af8e70da":"code","526f2336":"code","ec638820":"code","d96e9d2b":"code","0bfe0b9a":"code","b9392c5e":"markdown","114a4cef":"markdown","0b65a2ae":"markdown","b7e19ca0":"markdown","9984f58a":"markdown","f6f71307":"markdown","666ece0c":"markdown","78eaac8c":"markdown","63d5de3a":"markdown","6e54fa22":"markdown","6d7a1528":"markdown","bcbcbffc":"markdown","6e10e19b":"markdown","f9cd7bef":"markdown","8c07a5ef":"markdown","f91bc54f":"markdown","92944776":"markdown","baab863f":"markdown","290e51be":"markdown","4513670d":"markdown","bbbf125b":"markdown","c093147c":"markdown","64bdfd54":"markdown","480cdc2a":"markdown","bb9254d3":"markdown","66a277ca":"markdown","bbbd716a":"markdown","87dbeb1c":"markdown","51e06ef2":"markdown","bfc5646a":"markdown","4622201f":"markdown","3565f456":"markdown"},"source":{"4cf35108":"!pip install visualkeras  ","81baf959":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport visualkeras\n\nfrom sklearn.model_selection import train_test_split","2a35d059":"Train=pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nTest=pd.read_csv('..\/input\/digit-recognizer\/test.csv')","818ce1cf":"Train.head()","dc4a3294":"Train.shape, Test.shape","d32de43e":"Y_train=Train['label']\n\nX_train=Train.drop(['label'],axis=1)\nsns.countplot(Y_train)","4dcfc714":"X_train.shape","b6dd64ff":"X_train.isna().sum().sum()","f48bfa1a":"Test.isna().sum().sum()","f18f1935":"X_train=X_train\/255\nTest=Test\/255","dbf5319b":"X_train = X_train.values.reshape(-1,28,28,1)\nTest = Test.values.reshape(-1,28,28,1)","a2325e56":"X_train.shape, Y_train.shape","3b355488":"Test.shape","3414d3ee":"X_train[0].shape","b728b0b7":"print('Image class is: ', Y_train[34]),\nplt.imshow(np.squeeze(X_train[34], axis=-1))","43320b1e":"from keras.utils.np_utils import to_categorical\nY_train = to_categorical(Y_train, num_classes = 10)","fa4100ca":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=42)","bff36532":"X_train.shape, Y_train.shape","348c1ddc":"X_val.shape, Y_val.shape","8f038e5c":"import tensorflow as tf","daf86c4f":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta\nfrom keras.preprocessing.image import ImageDataGenerator","ea7301e3":"datagen = ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=10,\n    zoom_range = 0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=False,\n    vertical_flip=False)  ","96c644bc":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('val_accuracy')>0.995):\n      print(\"\\nReached 99.5% accuracy so cancelling training!\")\n      self.model.stop_training = True","029ca887":"model=Sequential()\nmodel.add(Conv2D(64,(3,3),strides=1,padding='Same',activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],1)))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,(3,3), strides=1,padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,(3,3), strides=1,padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation = \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation = \"softmax\"))\n\noptimizer = Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","ccba7f3f":"datagen.fit(X_train)\ncallbacks = myCallback()\nhistory = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                              validation_data=(X_val,Y_val), epochs=5, verbose=1,\n                              callbacks=[callbacks])","f983d751":"pd.DataFrame(history.history)","a8e0f688":"optimizer2=SGD(learning_rate=0.01)\nmodel.compile(optimizer = optimizer2 , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","430e545c":"history2 = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                              validation_data=(X_val,Y_val), epochs=5, verbose=1,\n                              callbacks=[callbacks])","fa6979fb":"pd.DataFrame(history2.history)","16497bcc":"optimizer3=RMSprop(learning_rate=0.001,rho=0.9,momentum=0.0,epsilon=1e-07)\nmodel.compile(optimizer = optimizer3, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","7eba65b4":"history3 = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                              validation_data=(X_val,Y_val), epochs=5, verbose=1,\n                              callbacks=[callbacks])","40739c95":"pd.DataFrame(history3.history)","8d7011a3":"optimizer4=Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07)\nmodel.compile(optimizer = optimizer4, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","f30b8dd6":"history4 = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                              validation_data=(X_val,Y_val), epochs=5, verbose=1,\n                              callbacks=[callbacks])","35d4664e":"pd.DataFrame(history4.history)","9caa86e4":"fig, axs = plt.subplots(2, 2, figsize=(20,17))\n\naxs[0, 0].plot(history.history[\"loss\"],c = \"purple\")\naxs[0, 0].plot(history2.history[\"loss\"],c = \"orange\")\naxs[0, 0].plot(history3.history[\"loss\"],c = \"green\")\naxs[0, 0].plot(history4.history[\"loss\"],c = \"blue\")\naxs[0, 0].set_ylim([-0.1,1.1])\naxs[0, 0].legend([\"Adam\", \"SGD\", \"RMSprop\", \"Adadelta\"])\naxs[0, 0].set_title('Optimizers Train-Loss')\n\naxs[0, 1].plot(history.history[\"val_loss\"],c = \"purple\")\naxs[0, 1].plot(history2.history[\"val_loss\"],c = \"orange\")\naxs[0, 1].plot(history3.history[\"val_loss\"],c = \"green\")\naxs[0, 1].plot(history4.history[\"val_loss\"],c = \"blue\")\naxs[0, 1].set_ylim([-0.1,1.1])\naxs[0, 1].legend([\"Adam\", \"SGD\", \"RMSprop\", \"Adadelta\"])\naxs[0, 1].set_title('Optimizers Val-Loss')\n\naxs[1, 0].plot(history.history[\"accuracy\"],c = \"purple\")\naxs[1, 0].plot(history2.history[\"accuracy\"],c = \"orange\")\naxs[1, 0].plot(history3.history[\"accuracy\"],c = \"green\")\naxs[1, 0].plot(history4.history[\"accuracy\"],c = \"blue\")\naxs[1, 0].set_ylim([0.6,1.1])\naxs[1, 0].legend([\"Adam\", \"SGD\", \"RMSprop\", \"Adadelta\"])\naxs[1, 0].set_title('Optimizers Train-Accuracy')\n\naxs[1, 1].plot(history.history[\"val_accuracy\"],c = \"purple\")\naxs[1, 1].plot(history2.history[\"val_accuracy\"],c = \"orange\")\naxs[1, 1].plot(history3.history[\"val_accuracy\"],c = \"green\")\naxs[1, 1].plot(history4.history[\"val_accuracy\"],c = \"blue\")\naxs[1, 1].set_ylim([0.6,1.1])\naxs[1, 1].legend([\"Adam\", \"SGD\", \"RMSprop\", \"Adadelta\"])\naxs[1, 1].set_title('Optimizers Val-Accuracy')","8906b9db":"model.summary()","d3fc7407":"visualkeras.layered_view(model)","615073a5":"import matplotlib.pyplot as plt\n\ndef metrics_plot(history):\n  acc = history.history['accuracy']\n  val_acc = history.history['val_accuracy']\n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  epochs = range(len(acc))\n\n  plt.plot(epochs, acc, 'r', label='Training accuracy')\n  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n  plt.title('Training and validation accuracy')\n  plt.legend()\n  plt.figure()\n\n  plt.plot(epochs, loss, 'r', label='Training Loss')\n  plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n  plt.title('Training and validation loss')\n  plt.legend()\n\n  plt.show()","d9310955":"print('Train accuracy: ',history2.history['accuracy'][-1]),\nprint('Val accuracy: ',history2.history['val_accuracy'][-1]),\nprint('Train loss: ',history2.history['loss'][-1]),\nprint('Val loss: ',history2.history['val_loss'][-1])","1007db7c":"metrics_plot(history2)","dc1667ca":"prediction_val=model.predict(X_val),\nprint(prediction_val[0][:5])","58728384":"class_pred_val = [np.argmax(i) for i in prediction_val[0]]\nprint(class_pred_val[:5])","25a5f5d5":"val_labels =[np.argmax(i) for i in Y_val]\nprint(val_labels[:5])","3ceeb623":"from sklearn.metrics import confusion_matrix\n\nf,ax = plt.subplots(figsize=(15, 15))\nconfusion_mtx = confusion_matrix(val_labels, class_pred_val)\nsns.set(font_scale=1.4)\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\",ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix Validation set\")\nplt.show()","beb83ad8":"from sklearn.metrics import classification_report\n\nreport = classification_report(val_labels, class_pred_val)\n\nprint(report)","af8e70da":"results=model.predict(Test)","526f2336":"results[0:5]","ec638820":"pred_test = [np.argmax(i) for i in results]\nprint(pred_test[:5])","d96e9d2b":"submission=pd.DataFrame(pred_test, index=pd.Series(range(1,28001), name='ImageId'),columns=['Label'])\nsubmission.to_csv(\"mnist_prediction_submission.csv\")","0bfe0b9a":"submission.head(10)","b9392c5e":"The following code will train our model with Adam optimizer, then will be printed as a dataframe its four metrics (Accuracy, Loss, Val accuracy, Val loss) in order to be compared easily with other performances:","114a4cef":"Nice!, our prediction corresponds to integer numbers from 0 to 9, as we want to compare with actual label we have to convert the label too:","0b65a2ae":"Just to confirm that both datasets have changed their shape properly:","b7e19ca0":"Unfortunately, this function to print the layers of our model does not details what means each color, but looking at the summary of the network you can understand it perfectly!","9984f58a":"Also, the classification report is useful to know for each class the recall, precision and F1-score, and obviously the accuracy:","f6f71307":"Time now to train-test split our data, test set will take 10% of total instances:","666ece0c":"Now, it's time to reshape the images, where each row in the csv file is an image flattened, so will be converted to an squared image of dimentions 28x28. However, that's not all, as we know images must be in 4-dimentions such as: \n\n(N\u00b0 images, height, width, color channels):","78eaac8c":"This step will consider the evaluation of one robust model with four optimizers and the best one will be chosen.\n\nLet's import some libraries which will be used next:","63d5de3a":"Read the training and testing dataset files:","6e54fa22":"Defining the dataset to be used in the modeling and its label:","6d7a1528":"## Adadelta:","bcbcbffc":"## RMSprop:","6e10e19b":"Our label can be used as is or one-hot encoded, I will do this last using to_categorical function, take into account that by doing this our model must use as loss function \"categorical_crossentropy\":","f9cd7bef":"## Adam:\n\nTime to build our network, will consider the following layers: 3 2D-convolution, 3 MaxPooling2D, 3 BatchNormalization, 1 Flatten, 2 Dense, 1 Dropout, with next characteristics:","8c07a5ef":"Above we can see how our four model have an outstanding superlative performance, just under 99.5% of validation accuracy. After seeing this plot chosing the best model does not makes much difference, but Adadelta is chosen to continue since now.","f91bc54f":"Also, let's see if there is any null value in both datasets:","92944776":"The following code will print the confution matrix of our prediction with actual label as a heat map:","baab863f":"The following is to create a dataframe with the results and saving as csv file:","290e51be":"The following function will plot the metrics of the best model chosen:","4513670d":"Let's see a random image from the training dataset. In order to plot it using matplotlib we have to firstly reshape to a two-dimentional instance i.e. (28x28), for this task np.squeeze will be useful:","bbbf125b":"Let's define a CallBack which stops the training when the validation accuracy has reached 99.5%, the evaluation will be done at the end of the epoch:","c093147c":"Change the one-hot encoding of our label to classes predicted:","64bdfd54":"# Modeling","480cdc2a":"# Prediction of out-of-bag instances:\n\nLet's predict the classes for the testing dataset using our model:","bb9254d3":"ImageDataGenerator function will be used to increase the number of images by changing a bit of their characteristics in order to make our model even more robust to changes in typical images.","66a277ca":"As we are dealing with images we have to apply Min-Max Scaling on both datasets:","bbbd716a":"# Data Exploration:\n\nThe shape of testing should be 28000 instances and 784 columns (pixels) and for training should be 42000 instances by 785 columns (one more because the first one is the label):","87dbeb1c":"## Stochastic Gradient Descent:","51e06ef2":"## Comparing performance of the four models:","bfc5646a":"As we can see above the prediction for each instance is one-hot encoded so we need to get the index with the highest probability which in simple words corresponds to the class predicted:","4622201f":"Let's predict the classes of the validation dataset which contains 4200 instances and look at the first 5 predictions:","3565f456":"Above we can see that the classes in our label are quite balanced, thus there will not  be any problem in classification."}}