{"cell_type":{"25d89aea":"code","cc80d39c":"code","7e5da010":"code","02819fa0":"code","d0320875":"code","d405aa7d":"code","887015e5":"code","00d017e4":"code","480bbe27":"code","3f1c6e09":"code","e5deb511":"code","6516bdae":"code","f3ff4974":"code","b171714a":"code","dbfcff32":"code","b0624d28":"code","e897822f":"code","a83f3013":"code","d31e86f9":"code","bcfac021":"code","f00624e9":"code","725edb81":"code","9d1b5471":"code","74c53179":"code","47baedac":"code","abda8cc6":"code","d9c1c88f":"code","756ff526":"code","ebeef78e":"code","fe2aa7d2":"code","4bf2ab34":"code","2aa2da0d":"code","99b7581f":"code","d6755c67":"code","838ba37c":"code","cd9e7c2d":"code","8937efe4":"code","c8d8fd8f":"code","a7bddcc7":"code","1c234592":"code","68f85572":"code","c35bd3d5":"code","fc6b977a":"code","dc353d57":"code","dceb175a":"code","c22276dc":"code","92447485":"code","0bf9c6b7":"code","56acea21":"code","f409311b":"code","c326ab89":"code","cbf70de1":"code","3b4bbae8":"code","0aa11f9e":"code","6d315b26":"code","0fb9da59":"code","ea0cc4f2":"code","0fb98476":"code","62c88a9c":"code","e7f2c50b":"code","c03f1ee0":"code","b8e9d472":"code","0f7269f3":"code","29b8d852":"code","03b96a43":"code","3e0c15ac":"code","9ef69788":"code","c9963cf9":"code","ce2b68dd":"code","f4a06c5d":"code","794dd098":"code","31412c41":"code","c8d12e86":"code","dda4558a":"code","46027828":"code","9ed10cf6":"code","1460a519":"code","9e66cc0c":"code","8c1db9e1":"code","b09ef0d0":"code","3ec1c858":"code","fb455a17":"code","26f0904e":"code","d489197b":"code","ce8bc962":"code","43514fc5":"code","b01dd639":"code","d7f61e0a":"code","1ec6e236":"code","0945aca2":"code","cf7e9cc9":"code","8be450b8":"code","a36f0b2b":"code","72ced541":"code","ef65788e":"code","2b46d043":"code","1d568924":"code","af9200a2":"code","181830d5":"code","aa50a915":"code","a4d72fd1":"code","1f415e38":"code","04c15dc6":"code","56acde6a":"code","a2bfffdc":"code","a65441bd":"code","c4a921eb":"code","2e5258d6":"code","60eb6c46":"code","c4e10806":"code","fc3145ff":"code","5e4d2cae":"code","881b5557":"code","77e8e3be":"code","d81975dd":"code","b3a89729":"code","8668bb2b":"code","63a1f9fe":"code","f9230f89":"code","124d690d":"code","9da9d7f8":"code","7766a415":"code","610dc311":"code","6bfb2ddd":"code","c57f4056":"code","24bbd946":"code","bcbcebe1":"code","1b2589a5":"code","071d0736":"code","1f752697":"code","cd85bac3":"code","962a8ea3":"code","11bad31d":"code","e73f34d9":"code","ec847e79":"code","b1a251a4":"code","775e9ab6":"code","686bd0d3":"code","477cb65a":"code","5b83bb5f":"code","4ba785f6":"code","8682b3d6":"code","ec734918":"code","201133ec":"code","0e4a357b":"code","11243444":"code","8d377c67":"code","3fab0e2a":"code","42f5080f":"code","3414f2a3":"code","fdb084ea":"code","c97f31a1":"code","441a08c2":"code","fdf15e87":"markdown","b1b271bb":"markdown","16227e21":"markdown","193a3343":"markdown","2372d3ef":"markdown","09be4616":"markdown","97da9436":"markdown","ad4e3f42":"markdown","b5d00fdf":"markdown","1e72f232":"markdown","27848838":"markdown","2174eb7b":"markdown","89ca78a3":"markdown","e6f88189":"markdown","afb30b92":"markdown","7f1e58fa":"markdown","ce2ee5a1":"markdown","f93c63f2":"markdown","ac47115b":"markdown","7ff22dcb":"markdown","924bf859":"markdown","168ab4a8":"markdown","29484073":"markdown","8ec22055":"markdown","b8716a66":"markdown","3d9e7385":"markdown","60eeda59":"markdown","7a143fc2":"markdown","5f481a34":"markdown","d6a577f5":"markdown","526e3bf9":"markdown","1fcaff6d":"markdown","274bbcde":"markdown","05f46915":"markdown","6c462ce2":"markdown","1c4610b0":"markdown","947a74cf":"markdown"},"source":{"25d89aea":"#Importing Packages\/Libraries\n#from google.colab import files\nfrom sklearn import svm\nfrom sklearn import metrics\nimport missingno as msno\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport pandas_profiling\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import KNNImputer\nimport matplotlib.pylab as pylab\nimport warnings\nimport pandas_profiling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectPercentile\n%matplotlib inline\nwarnings.filterwarnings('ignore')","cc80d39c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7e5da010":"#For setting the charts size globally\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (15, 5),\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large',\n         'legend.title_fontsize':'x-large'}\npylab.rcParams.update(params)","02819fa0":"Train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nTest=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","d0320875":"print(Train.shape)\nprint(Test.shape)","d405aa7d":"Train.head()","887015e5":"Train.columns","00d017e4":"Train.info()","480bbe27":"# Number of of null values in each column\ncount=round(Train.isnull().sum(),2)\npercent=round((Train.isnull().sum()\/Train.shape[0])*100,2)\ndata=pd.concat([count,percent],axis=1)\ndata.reset_index(inplace=True)\ndata.rename(columns={0: 'Missing Values Count',1: 'Missing Values %'},inplace=True)\ndata[data['Missing Values Count']!=0]","3f1c6e09":"# No duplicates.\nTrain.duplicated().sum()","e5deb511":"#How many unique values are present in each column\nfeatures=Train.columns\nprint(\"Number of unique values are as below:\\n\")\nfor i in features:\n  uniqueValues=Train[i].nunique()\n  uniqueValues_per=round(Train[i].nunique()\/Train.shape[0],2)\n  print(i,uniqueValues)","6516bdae":"#dropping variables obtained from abov observations as they are not useful\ndropColList=['Alley', 'PoolQC', 'Fence', 'MiscFeature','Id']\nTrain.drop(dropColList,axis=1,inplace=True)\nTest.drop(dropColList,axis=1,inplace=True)","f3ff4974":"print(Train.shape)\nprint(Test.shape)","b171714a":"# Mean and Median are almost same\nTrain.describe(include = 'all').T","dbfcff32":"#separating Numerical and categorical variables\nNumericData=Train.select_dtypes(include=['float64','int64'])\nrows,col=(NumericData.shape)\nprint(\"Number of Numeric columns are:\",col)\nprint(NumericData.columns)","b0624d28":"#separating Numerical and categorical variables\nCategoricData=Train.select_dtypes(include=['object','category'])\nrows,col=(CategoricData.shape)\nprint(\"Number of Categorical columns are:\",col)\nprint(CategoricData.columns)","e897822f":"CategoricData.head()","a83f3013":"#melting the dataframe to bring the data into single column\nConvertedCatDataMelt=CategoricData.melt()\n\n#Univariate Analysis for CountPlot for categorical variables\nCatFacetGrid = sns.FacetGrid(ConvertedCatDataMelt, col='variable',sharex=False, dropna=True, sharey=False, size=4,col_wrap=4)\nCatFacetGrid.set_xticklabels(rotation=90)\ncountPlot=CatFacetGrid.map(sns.countplot,'value')\nplt.show()","d31e86f9":"#melting the dataframe to bring the data into single column\nCategoricalConData=Train[['OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','KitchenQual','TotRmsAbvGrd','GarageCars',\n'Fireplaces','GarageType','MiscVal','MoSold','YrSold']].copy()\n\nCategoricalConDataMelt=CategoricalConData.melt()\n\n#Univariate Analysis for CountPlot for categorical variables\nCatFacetGrid = sns.FacetGrid(CategoricalConDataMelt, col='variable',sharex=False, dropna=True, sharey=False, size=4,col_wrap=4)\nCatFacetGrid.set_xticklabels(rotation=90)\ncountPlot=CatFacetGrid.map(sns.countplot,'value')\nplt.show()\n","bcfac021":"#histograms for numerical variables\nNumericData.drop(['OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','GarageCars',\n'Fireplaces','MiscVal','MoSold','YrSold'],axis=1,inplace=True)\nNumericDataMelt=NumericData.melt()\n\nCatFacetGrid = sns.FacetGrid(NumericDataMelt, col='variable',sharex=False, dropna=True, sharey=False, size=4,col_wrap=4)\n#CatFacetGrid.set_xticklabels(rotation=90)\ncountPlot=CatFacetGrid.map(sns.histplot,'value')\nplt.show()","f00624e9":"NumericData.shape","725edb81":"#distplot and boxplots for numerical variables\nfig, axes = plt.subplots(nrows=24,ncols=2,  figsize=(20, 90))\nfig.subplots_adjust(hspace = .8, wspace=.3)\ni = 0\nfor col in NumericData.columns:\n    if NumericData[col].dtype == 'int64' or NumericData[col].dtype == 'float64':\n        sns.distplot(NumericData[col], ax=axes[i][0]).set_title(\"Hisotogram of \" + col)\n        sns.boxplot(NumericData[col], ax=axes[i][1]).set_title(\"Boxplot of \" + col)\n        i = i + 1","9d1b5471":"CategoricalConData.columns","74c53179":"size = (20, 50)\nCategoricalConData['SalePrice']=Train['SalePrice']\n\nfig, axs = plt.subplots(ncols=2,nrows=8,figsize=size)\nfig.subplots_adjust(hspace = .2, wspace=.2)\n\nsns.boxplot(x=CategoricalConData['OverallQual'],y=CategoricalConData['SalePrice'],ax=axs[0][0], ).set_title(\"Boxplot of OverallQual\")\nsns.boxplot(x=CategoricalConData['OverallCond'],y=CategoricalConData['SalePrice'],ax=axs[0][1] ).set_title(\"Boxplot of OverallCond\")\n\nsns.boxplot(x=CategoricalConData['BsmtFullBath'],y=CategoricalConData['SalePrice'],ax=axs[1][0], ).set_title(\"Boxplot of BsmtFullBath\")\nsns.boxplot(x=CategoricalConData['BsmtHalfBath'],y=CategoricalConData['SalePrice'],ax=axs[1][1] ).set_title(\"Boxplot of BsmtHalfBath\")\n\nsns.boxplot(x=CategoricalConData['FullBath'],y=CategoricalConData['SalePrice'],ax=axs[2][0], ).set_title(\"Boxplot of FullBath\")\nsns.boxplot(x=CategoricalConData['HalfBath'],y=CategoricalConData['SalePrice'],ax=axs[2][1] ).set_title(\"Boxplot of HalfBath\")\n\nsns.boxplot(x=CategoricalConData['BedroomAbvGr'],y=CategoricalConData['SalePrice'],ax=axs[3][0], ).set_title(\"Boxplot of BedroomAbvGr\")\nsns.boxplot(x=CategoricalConData['KitchenAbvGr'],y=CategoricalConData['SalePrice'],ax=axs[3][1] ).set_title(\"Boxplot of KitchenAbvGr\")\n\nsns.boxplot(x=CategoricalConData['KitchenQual'],y=CategoricalConData['SalePrice'],ax=axs[4][0], ).set_title(\"Boxplot of KitchenQual\")\nsns.boxplot(x=CategoricalConData['TotRmsAbvGrd'],y=CategoricalConData['SalePrice'],ax=axs[4][1] ).set_title(\"Boxplot of TotRmsAbvGrd\")\n\nsns.boxplot(x=CategoricalConData['GarageCars'],y=CategoricalConData['SalePrice'],ax=axs[5][0], ).set_title(\"Boxplot of GarageCars\")\nsns.boxplot(x=CategoricalConData['Fireplaces'],y=CategoricalConData['SalePrice'],ax=axs[5][1] ).set_title(\"Boxplot of Fireplaces\")\n\nsns.boxplot(x=CategoricalConData['GarageType'],y=CategoricalConData['SalePrice'],ax=axs[6][0], ).set_title(\"Boxplot of GarageType\")\nsns.boxplot(x=CategoricalConData['MiscVal'],y=CategoricalConData['SalePrice'],ax=axs[6][1] ).set_title(\"Boxplot of MiscVal\")\n\nsns.boxplot(x=CategoricalConData['MoSold'],y=CategoricalConData['SalePrice'],ax=axs[7][0], ).set_title(\"Boxplot of MoSold\")\nsns.boxplot(x=CategoricalConData['YrSold'],y=CategoricalConData['SalePrice'],ax=axs[7][1] ).set_title(\"Boxplot of YrSold\")","47baedac":"CategoricData['SalePrice']=Train['SalePrice'].copy()","abda8cc6":"size = (20, 60)\n\nfig, axs = plt.subplots(ncols=2,nrows=8,figsize=size)\nfig.subplots_adjust(hspace = .4, wspace=.2)\n\nsns.boxplot(x=CategoricData['MSZoning'],y=CategoricData['SalePrice'],ax=axs[0][0], ).set_title(\"Boxplot of MSZoning\")\nsns.boxplot(x=CategoricData['Street'],y=CategoricData['SalePrice'],ax=axs[0][1] ).set_title(\"Boxplot of OverallCond\")\n\nsns.boxplot(x=CategoricData['LotShape'],y=CategoricData['SalePrice'],ax=axs[1][0], ).set_title(\"Boxplot of LotShape\")\nsns.boxplot(x=CategoricData['LandContour'],y=CategoricData['SalePrice'],ax=axs[1][1] ).set_title(\"Boxplot of LandContour\")\n\nsns.boxplot(x=CategoricData['Utilities'],y=CategoricData['SalePrice'],ax=axs[2][0], ).set_title(\"Boxplot of Utilities\")\nsns.boxplot(x=CategoricData['LotConfig'],y=CategoricData['SalePrice'],ax=axs[2][1] ).set_title(\"Boxplot of LotConfig\")\n\nsns.boxplot(x=CategoricData['LandSlope'],y=CategoricData['SalePrice'],ax=axs[3][0], ).set_title(\"Boxplot of LandSlope\")\nax=sns.boxplot(x=CategoricData['Neighborhood'],y=CategoricData['SalePrice'],ax=axs[3][1] )\nax.set_title(\"Boxplot of Neighborhood\")\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n\nsns.boxplot(x=CategoricData['Condition1'],y=CategoricData['SalePrice'],ax=axs[4][0], ).set_title(\"Boxplot of Condition1\")\nsns.boxplot(x=CategoricData['Condition2'],y=CategoricData['SalePrice'],ax=axs[4][1] ).set_title(\"Boxplot of Condition2\")\n\nsns.boxplot(x=CategoricData['BldgType'],y=CategoricData['SalePrice'],ax=axs[5][0], ).set_title(\"Boxplot of BldgType\")\nsns.boxplot(x=CategoricData['HouseStyle'],y=CategoricData['SalePrice'],ax=axs[5][1] ).set_title(\"Boxplot of HouseStyle\")\n\nsns.boxplot(x=CategoricData['RoofStyle'],y=CategoricData['SalePrice'],ax=axs[6][0], ).set_title(\"Boxplot of RoofStyle\")\nsns.boxplot(x=CategoricData['RoofMatl'],y=CategoricData['SalePrice'],ax=axs[6][1] ).set_title(\"Boxplot of RoofMatl\")\n\nax=sns.boxplot(x=CategoricData['Exterior1st'],y=CategoricData['SalePrice'],ax=axs[7][0], )\nax.set_title(\"Boxplot of Exterior1st\")\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nax=sns.boxplot(x=CategoricData['Exterior2nd'],y=CategoricData['SalePrice'],ax=axs[7][1] )\nax.set_title(\"Boxplot of Exterior2nd\")\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)","d9c1c88f":"#Bivariate Analysis for actual categorical Variables with Attrition\nNumericData['SalePrice']=Train['SalePrice']\nsns.pairplot(NumericData)","756ff526":"plt.figure(figsize=(15,15))\nsns.heatmap(round(NumericData.corr(),2),annot=True,mask=None,cmap='GnBu')\nplt.show()","ebeef78e":"# Number of of null values in each column\ncount=round(Train.isnull().sum(),2)\npercent=round((Train.isnull().sum()\/Train.shape[0])*100,2)\ndata=pd.concat([count,percent],axis=1)\ndata.reset_index(inplace=True)\ndata.rename(columns={0: 'Missing Values Count',1: 'Missing Values %'},inplace=True)\nmissingData=data[data['Missing Values Count']!=0]\nmissingData","fe2aa7d2":"missingDf=Train[['LotFrontage', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond']].copy()\nmissingDf","4bf2ab34":"msno.heatmap(Train[missingDf.columns])","2aa2da0d":"# filling a null values of 'GarageType','GarageYrBlt','GarageFinish','GarageQual','GarageCond' using fillna()  andupdating as 'No Garage'\nTrain[\"GarageType\"].fillna(\"No Garage\", inplace = True)  \nTrain[\"GarageYrBlt\"].fillna(\"No Garage\", inplace = True)  \nTrain[\"GarageFinish\"].fillna(\"No Garage\", inplace = True)  \nTrain[\"GarageQual\"].fillna(\"No Garage\", inplace = True)  \nTrain[\"GarageCond\"].fillna(\"No Garage\", inplace = True)  ","99b7581f":"# filling a null values of 'GarageType','GarageYrBlt','GarageFinish','GarageQual','GarageCond' using fillna()  andupdating as 'No Garage'\nTest[\"GarageType\"].fillna(\"No Garage\", inplace = True)  \nTest[\"GarageYrBlt\"].fillna(\"No Garage\", inplace = True)  \nTest[\"GarageFinish\"].fillna(\"No Garage\", inplace = True)  \nTest[\"GarageQual\"].fillna(\"No Garage\", inplace = True)  \nTest[\"GarageCond\"].fillna(\"No Garage\", inplace = True)  ","d6755c67":"# filling a null values of 'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2' using fillna()  and updating as 'No Basement'\nTrain[\"BsmtQual\"].fillna(\"No Basement\", inplace = True)  \nTrain[\"BsmtCond\"].fillna(\"No Basement\", inplace = True)  \nTrain[\"BsmtExposure\"].fillna(\"No Basement\", inplace = True)  \nTrain[\"BsmtFinType1\"].fillna(\"No Basement\", inplace = True)  \nTrain[\"BsmtFinType2\"].fillna(\"No Basement\", inplace = True)","838ba37c":"# filling a null values of 'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2' using fillna()  and updating as 'No Basement'\nTest[\"BsmtQual\"].fillna(\"No Basement\", inplace = True)  \nTest[\"BsmtCond\"].fillna(\"No Basement\", inplace = True)  \nTest[\"BsmtExposure\"].fillna(\"No Basement\", inplace = True)  \nTest[\"BsmtFinType1\"].fillna(\"No Basement\", inplace = True)  \nTest[\"BsmtFinType2\"].fillna(\"No Basement\", inplace = True)","cd9e7c2d":"# filling a null values of 'MasVnrType','MasVnrArea' using fillna()  and updating as 'No None'\nTrain[\"MasVnrType\"].fillna(\"None\", inplace = True)  \nTrain[\"MasVnrArea\"].fillna(\"None\", inplace = True)  ","8937efe4":"# filling a null values of 'MasVnrType','MasVnrArea' using fillna()  and updating as 'No None'\nTest[\"MasVnrType\"].fillna(\"None\", inplace = True)  \nTest[\"MasVnrArea\"].fillna(\"None\", inplace = True)  ","c8d8fd8f":"# filling a null values of 'Electrical' with mode\nTrain['Electrical'].fillna(Train['Electrical'].mode()[0], inplace=True)","a7bddcc7":"# filling a null values of 'Electrical' with mode\nTest['Electrical'].fillna(Train['Electrical'].mode()[0], inplace=True)","1c234592":"# filling a null values of 'FireplaceQu' using fill na and updating as 'No Fireplace'\nTrain[\"FireplaceQu\"].fillna(\"No Fireplace\", inplace = True)  ","68f85572":"# filling a null values of 'FireplaceQu' using fill na and updating as 'No Fireplace'\nTest[\"FireplaceQu\"].fillna(\"No Fireplace\", inplace = True)  ","c35bd3d5":"Train['LotFrontage'].describe()","fc6b977a":"# filling a null values of 'LotFrontage' with mean\nTrain['LotFrontage'].fillna(Train['LotFrontage'].mean(), inplace=True)","dc353d57":"# filling a null values of 'LotFrontage' with mean\nTest['LotFrontage'].fillna(Test['LotFrontage'].mean(), inplace=True)","dceb175a":"# # filling a null values of 'LotFrontage' with mean\n# Train['LotFrontage'].fillna(Train['LotFrontage'].mean(), inplace=True)","c22276dc":"# Number of of null values in each column after imputation\ncount=round(Train.isnull().sum(),2)\npercent=round((Train.isnull().sum()\/Train.shape[0])*100,2)\ndata=pd.concat([count,percent],axis=1)\ndata.reset_index(inplace=True)\ndata.rename(columns={0: 'Missing Values Count',1: 'Missing Values %'},inplace=True)\nmissingData=data[data['Missing Values Count']!=0]\nmissingData","92447485":"missing=Test[['MSZoning','Utilities','Exterior1st','Exterior2nd','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','KitchenQual','Functional','GarageArea','SaleType']].copy()\nmissing","0bf9c6b7":"missing.info()","56acea21":"# filling a null values of 'LotFrontage' with mean\nTest['TotalBsmtSF'].fillna(Test['TotalBsmtSF'].mean(), inplace=True)\n# filling a null values of 'LotFrontage' with mean\nTest['GarageArea'].fillna(Test['GarageArea'].mean(), inplace=True)","f409311b":"Test = Test.apply(lambda x:x.fillna(x.value_counts().index[0]))","c326ab89":"# Number of of null values in each column after imputation\ncount=round(Test.isnull().sum(),2)\npercent=round((Test.isnull().sum()\/Test.shape[0])*100,2)\ndata=pd.concat([count,percent],axis=1)\ndata.reset_index(inplace=True)\ndata.rename(columns={0: 'Missing Values Count',1: 'Missing Values %'},inplace=True)\nmissingData=data[data['Missing Values Count']!=0]\nmissingData","cbf70de1":"CategoricData=Train.select_dtypes(include=['object','category'])\nrows,col=(CategoricData.shape)\nprint(\"Number of Categorical columns are:\",col)\nprint(CategoricData.columns)","3b4bbae8":"# #Displaying the number of occurences for each categorical variable\n# CatCol = CategoricData.select_dtypes(include = \"object\").columns\n# print(CatCol)\n# print(\"\\n\")\n# for col in CatCol:\n#     print(CategoricData[col].value_counts())\n#     print(\"\\n\")","0aa11f9e":"# #taking databackup into temp for plotting a correlation plot\n# temp=CategoricData.copy()\n\n# #selecting only categorical variables for Label encoding\n# CatCol = temp.select_dtypes(include = \"object\").columns\n# print(CatCol)\n\n# #instantiating LabelEncoder() object\n# le = LabelEncoder()\n\n# #Label encoding the categorical columns by converting them into string type\n# for feat in CatCol:\n#     temp[feat] = le.fit_transform(temp[feat].astype(str))\n\n# #plottting correlational plot checking Correlations and dependencies for imputation\n# plt.figure(figsize=(25,15))\n# sns.heatmap(round(temp.corr(method='kendall'),2),annot=True,mask=None,cmap='GnBu')\n# plt.show()","6d315b26":"# #plottting correlational plot checking Correlations and dependencies for imputation\n# plt.figure(figsize=(25,15))\n# sns.heatmap(round(NumericData.corr(method='pearson'),2),annot=True,mask=None,cmap='GnBu')\n# plt.show()","0fb9da59":"# NumericData=Train.select_dtypes(include=['float64','int64'])\n# # NumericData.drop(['OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','GarageCars',\n# # 'Fireplaces','MiscVal','MoSold','YrSold'],axis=1,inplace=True)\n# NumericData.isnull().sum()\n\n# NumericData.drop(['SalePrice','BsmtUnfSF','2ndFlrSF','YrSold','YearRemodAdd'],axis=1,inplace=True)\n# #checking for presence onf multi-collieanrity\n# vif=pd.DataFrame()\n# vif['Features']=NumericData.columns\n# vif['VIF']=[variance_inflation_factor(NumericData.values,i) for i in range(NumericData.shape[1])]\n# vif['VIF']=round(vif['VIF'],2)\n# vif=vif.sort_values(by=\"VIF\",ascending=False)\n# vif","ea0cc4f2":"Train.drop(['1stFlrSF','2ndFlrSF','LowQualFinSF','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotRmsAbvGrd','GarageCars'],axis=1,inplace=True)\nTest.drop(['1stFlrSF','2ndFlrSF','LowQualFinSF','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotRmsAbvGrd','GarageCars'],axis=1,inplace=True)","0fb98476":"#taking databackup \nTrain_bkup=Train.copy()\ntemp=CategoricData.copy()","62c88a9c":"Train_bkup.drop(['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond',\n       'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n       'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n       'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n       'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType',\n       'SaleCondition'],axis=1,inplace=True)","e7f2c50b":"#taking databackup \nTest_bkup=Test.copy()","c03f1ee0":"Test_bkup.drop(['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond',\n       'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n       'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n       'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n       'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType',\n       'SaleCondition'],axis=1,inplace=True)","b8e9d472":"CategoricDataT=Test.select_dtypes(include=['object','category'])\nrows,col=(CategoricDataT.shape)\nprint(\"Number of Categorical columns are:\",col)\nprint(CategoricDataT.columns)","0f7269f3":"tempT=CategoricDataT.copy()","29b8d852":"oheDataT=pd.get_dummies(tempT,drop_first=True)","03b96a43":"oheData=pd.get_dummies(temp,drop_first=True)","3e0c15ac":"oheData.columns","9ef69788":"Train_bkup.columns","c9963cf9":"#concatenating with categoricalEncoded DF\noheData=pd.concat([oheData,Train_bkup],axis=1)","ce2b68dd":"#concatenating with categoricalEncoded DF\noheDataT=pd.concat([oheDataT,Test_bkup],axis=1)","f4a06c5d":"#one hot encoded dataframe shape\noheData.shape","794dd098":"oheDataT.shape","31412c41":"#taking backup of oheDataDF\noheDataDF=oheData.copy()","c8d12e86":"# import sys\n# !{sys.executable} -m pip install -U pandas-profiling[notebook]\n# !jupyter nbextension enable --py widgetsnbextension","dda4558a":"# #Panda Profiling gives almost all the data analysis required for the EDA\n# pandas_profiling.ProfileReport(CategoricData)","46027828":"CategoricData.columns","9ed10cf6":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","1460a519":"corr_features = correlation(NumericData, 0.8)\nlen(set(corr_features))","9e66cc0c":"corr_features","8c1db9e1":"NumericData=Train.select_dtypes(include=['float64','int64'])","b09ef0d0":"plt.figure(figsize=(25,15))\nsns.heatmap(round(NumericData.corr(method='pearson'),2),annot=True,mask=None,cmap='GnBu')\nplt.show()","3ec1c858":"#checking for presence onf multi-collieanrity\nvif=pd.DataFrame()\nvif['Features']=NumericData.columns\nvif['VIF']=[variance_inflation_factor(NumericData.values,i) for i in range(NumericData.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif","fb455a17":"#plotting after droppping\nNumericData=Train.select_dtypes(include=['float64','int64'])\nplt.figure(figsize=(25,15))\nsns.heatmap(round(NumericData.corr(method='pearson'),2),annot=True,mask=None,cmap='GnBu')\nplt.show()","26f0904e":"oheDataDF.shape","d489197b":"oheDataDF.columns","ce8bc962":"X=oheDataDF.drop(labels=['SalePrice'], axis=1)\ny=oheDataDF['SalePrice']\n\nfrom sklearn.model_selection import train_test_split\n# separate dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    oheDataDF.drop(labels=['SalePrice'], axis=1),\n    oheDataDF['SalePrice'],\n    test_size=0.3,\n    random_state=55)","43514fc5":"var_thres=VarianceThreshold(threshold=0.05)\nvar_thres.fit(X_train)","b01dd639":"### Finding non constant features\nsum(var_thres.get_support())","d7f61e0a":"# Lets Find non-constant features \nlen(X_train.columns[var_thres.get_support()])","1ec6e236":"constant_columns = [column for column in X_train.columns\n                    if column not in X_train.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","0945aca2":"X_train.drop(constant_columns,axis=1,inplace=True)\nX_test.drop(constant_columns,axis=1,inplace=True)","cf7e9cc9":"print(X_test.shape)\nprint(y_test.shape)","8be450b8":"print(X_train.shape)\nprint(X_test.shape)","a36f0b2b":"oheDataDF.shape","72ced541":"#  #independent columns\n# X = oheDataDF.iloc[:,0:665] \n#  #target column i.e price range\n# y = oheDataDF.iloc[:,-1]   ","ef65788e":"X=X_train.copy()\ny=y_train.copy()","2b46d043":"#apply SelectKBest class to extract top 10 best features-above 99 features obtained\nbestfeatures = SelectKBest(score_func=f_regression, k=10)\nfit = bestfeatures.fit(X,y)","1d568924":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)","af9200a2":"#concat two dataframes \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\n#naming the dataframe columns\nfeatureScores.columns = ['Specs','Score']  \n#print 10 best features\nprint(featureScores.nlargest(15,'Score'))  ","181830d5":"#  #independent columns\n# X = oheDataDF.iloc[:,0:665] \n#  #target column i.e price range\n# y = oheDataDF.iloc[:,-1]   ","aa50a915":"from sklearn.ensemble import ExtraTreesRegressor\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesRegressor()\nmodel.fit(X,y)","a4d72fd1":"# #use inbuilt class feature_importances of tree based classifiers\n# print(model.feature_importances_) ","1f415e38":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')\nplt.show()","04c15dc6":"feat_importances.nlargest(15)","56acde6a":"# determine the mutual information\nmutual_info = mutual_info_regression(X_train, y_train)\nmutual_info","a2bfffdc":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","a65441bd":"mutual_info.sort_values(ascending=False).plot.bar(figsize=(15,5))","c4a921eb":"## Selecting the top 20 percentile\nselected_top_columns = SelectPercentile(mutual_info_regression, percentile=15)\nselected_top_columns.fit(X_train, y_train)\n","2e5258d6":"X_train.columns[selected_top_columns.get_support()]","60eb6c46":"rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=15)\nmodel = DecisionTreeRegressor()","c4e10806":"rfe = RFE(model, 15)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)","fc3145ff":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","5e4d2cae":"col = X_train.columns[rfe.support_]","881b5557":"X_train.columns[~rfe.support_]","77e8e3be":"col","d81975dd":"# Sequential Forward Selection(sfs)\nsfs = SFS(LinearRegression(),\n          k_features=15,\n          forward=True,\n          floating=False,\n          scoring = 'r2',\n          cv = 0)","b3a89729":"#fitting and predicting for top 15 features\nsfs.fit(X, y)\nsfs.k_feature_names_","8668bb2b":"from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\nfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()","63a1f9fe":"sbs = SFS(LinearRegression(),\n         k_features=15,\n         forward=False,\n         floating=False,\n         cv=0)\n","f9230f89":"sbs.fit(X.values, y.values)","124d690d":"sbs.k_feature_names_","9da9d7f8":"from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\nfig1 = plot_sfs(sbs.get_metric_dict(), kind='std_dev')\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()","7766a415":"sffs = SFS(LinearRegression(),\n         k_features=(3,15),\n         forward=True,\n         floating=True,\n         cv=0)","610dc311":"sffs.fit(X.values, y.values)\nsffs.k_feature_names_","6bfb2ddd":"from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\nfig1 = plot_sfs(sffs.get_metric_dict(), kind='std_dev')\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()","c57f4056":"# Function to check performance metrics\ndef check_performance(model):\n  print(\"Train_R_squared =\", model.best_estimator_.score(X_train, y_train)) \n  print(\"Test_R_squared =\", model.best_estimator_.score(X_test, y_test))  \n  print(\"RMSE_train =\", np.sqrt(mean_squared_error(y_train, model.best_estimator_.predict(X_train))))\n  print(\"RMSE_test = \", np.sqrt(mean_squared_error(y_test, model.best_estimator_.predict(X_test))))\n  print(\"MAPE_train =\", mean_absolute_percentage_error(y_train, model.best_estimator_.predict(X_train)))\n  print(\"MAPE_test = \", mean_absolute_percentage_error(y_test, model.best_estimator_.predict(X_test)))","24bbd946":"# Function to calculate MAPE\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred =  np.array(y_true), np.array(pd.DataFrame(y_pred))\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","bcbcebe1":"# # Building Decision Tree Regressor\n# DTR = DecisionTreeRegressor(random_state=123)\n# DTR.fit(X_train,y_train)","1b2589a5":"# print(X_train.shape)\n# print(y_train.shape)","071d0736":"# #Using the model to predict on test data\n# y_pred_dt = decisionTree.predict(X_test)","1f752697":"# %%time\n# param_grid = {\n#               'max_depth' : [4,6,8,10,12],\n#               'min_samples_leaf' : [20,30,40,50], # 1-3% of length of dataset\n#               'min_samples_split' : [40,60,80]\n#              }\n\n# DTR = DecisionTreeRegressor(random_state=123)\n\n# DT_random = RandomizedSearchCV(estimator = DTR, param_distributions = param_grid, cv = 3)\n\n# DT_random.fit(X_train,y_train)","cd85bac3":"# # Best parameters\n# DT_random.best_params_","962a8ea3":"# check_performance(DT_random)","11bad31d":"DT_Train=X_train[['ExterQual_Gd',\n'ExterQual_TA',\n'KitchenQual_TA',\n'LotArea',\n'MSSubClass',\n'LotFrontage',\n'OverallQual',\n'YearBuilt',\n'YearRemodAdd',\n'TotalBsmtSF',\n'GrLivArea',\n'FullBath',\n'Fireplaces',\n'GarageArea',\n'OpenPorchSF'\n ]].copy()\n\n","e73f34d9":" DT_Test=X_test[['ExterQual_Gd',\n'ExterQual_TA',\n'KitchenQual_TA',\n'LotArea',\n'MSSubClass',\n'LotFrontage',\n'OverallQual',\n'YearBuilt',\n'YearRemodAdd',\n'TotalBsmtSF',\n'GrLivArea',\n'FullBath',\n'Fireplaces',\n'GarageArea',\n'OpenPorchSF'\n ]].copy()","ec847e79":"y_train_log=np.log(y_train)","b1a251a4":"y_test_log=np.log(y_test)","775e9ab6":"# Building Decision Tree Regressor\nDTR = DecisionTreeRegressor(random_state=123)\nDTR.fit(DT_Train,y_train_log)","686bd0d3":"print(DT_Train.shape)\nprint(y_train_log.shape)","477cb65a":"print(DT_Test.shape)\nprint(y_test_log.shape)","5b83bb5f":"#Using the model to predict on test data\ny_pred_dt = DTR.predict(DT_Test)","4ba785f6":"%%time\nparam_grid = {\n              'max_depth' : [4,6,8,10,12],\n              'min_samples_leaf' : [20,30,40,50], # 1-3% of length of dataset\n              'min_samples_split' : [40,60,80]\n             }\n\n#DTR = DecisionTreeRegressor(random_state=123)\n\nDT_random = RandomizedSearchCV(estimator = DTR, param_distributions = param_grid, cv = 3)\n\nDT_random.fit(DT_Train,y_train_log)","8682b3d6":"# Best parameters\nDT_random.best_params_","ec734918":"print(\"Train_R_squared =\", DT_random.best_estimator_.score(DT_Train, y_train_log)) \nprint(\"Test_R_squared =\", DT_random.best_estimator_.score(DT_Test, y_test_log))  \nprint(\"RMSE_train =\", np.sqrt(mean_squared_error(y_train_log, DT_random.best_estimator_.predict(DT_Train))))\nprint(\"RMSE_test = \", np.sqrt(mean_squared_error(y_test_log, DT_random.best_estimator_.predict(DT_Test))))\nprint(\"MAPE_train =\", mean_absolute_percentage_error(y_train_log, DT_random.best_estimator_.predict(DT_Train)))\nprint(\"MAPE_test = \", mean_absolute_percentage_error(y_test_log, DT_random.best_estimator_.predict(DT_Test)))","201133ec":"# Building Random Forest Regressor\nRFR = RandomForestRegressor(random_state=123)\nRFR.fit(DT_Train,y_train_log)","0e4a357b":"%%time\nparam_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)],\n              'max_features' : ['auto', 'sqrt'],\n              'max_depth' : np.linspace(1, 20, 20, endpoint=True),\n              'min_samples_leaf' : [10,20,30,50,70,90], # 1-3% of length of dataset\n              'min_samples_split' : [30,40,50,60,70,80,90], # approx 3 times the min_samples_leaf\n             }\n\nRFR = RandomForestRegressor(random_state=123)\n\nRF_random = RandomizedSearchCV(estimator = RFR, param_distributions = param_grid, cv = 3)\n\nRF_random.fit(DT_Train,y_train_log)","11243444":"# Best parameters\nRF_random.best_params_","8d377c67":"print(\"Train_R_squared =\", DT_random.best_estimator_.score(DT_Train, y_train_log)) \nprint(\"Test_R_squared =\", DT_random.best_estimator_.score(DT_Test, y_test_log))  \nprint(\"RMSE_train =\", np.sqrt(mean_squared_error(y_train_log, DT_random.best_estimator_.predict(DT_Train))))\nprint(\"RMSE_test = \", np.sqrt(mean_squared_error(y_test_log, DT_random.best_estimator_.predict(DT_Test))))\nprint(\"MAPE_train =\", mean_absolute_percentage_error(y_train_log, DT_random.best_estimator_.predict(DT_Train)))\nprint(\"MAPE_test = \", mean_absolute_percentage_error(y_test_log, DT_random.best_estimator_.predict(DT_Test)))","3fab0e2a":"DT_Test1=oheDataT[['ExterQual_Gd',\n'ExterQual_TA',\n'KitchenQual_TA',\n'LotArea',\n'MSSubClass',\n'LotFrontage',\n'OverallQual',\n'YearBuilt',\n'YearRemodAdd',\n'TotalBsmtSF',\n'GrLivArea',\n'FullBath',\n'Fireplaces',\n'GarageArea',\n'OpenPorchSF'\n ]].copy()","42f5080f":"DT_Test1.isnull().sum()","3414f2a3":"#Using the model to predict on test data\ny_pred_dt1 = RF_random.predict(DT_Test)","fdb084ea":"y_pred_dt1","c97f31a1":"dataset = pd.DataFrame({'Column1': y_pred_dt})\ndataset","441a08c2":"#download result from colab\ndataset.to_csv('result.csv',index=False)\nfiles.download(\"result.csv\")","fdf15e87":"### One hot encoding for categorical data\n\n","b1b271bb":"Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.","16227e21":"## Random Forest","193a3343":"# Method 7: RFE (Recursive Feature Elimination)","2372d3ef":"## Missing values imputation","09be4616":"## Decision Tree","97da9436":"## Categorical vs Target variable","ad4e3f42":"### Separating Numerical and categorical features for plotting the data","b5d00fdf":"Alley, PoolQC, Fence, MiscFeature columns can be dropped as they have more than 80% of nulls in respective columns","1e72f232":"# Method 8: Forward feature Selection","27848838":"# **House Prices - Advanced Regression Techniques**\nPredict sales prices and practice feature engineering, RFs, and gradient boosting\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n\nMethod 1: Correlation Heatmap\n\nMethod 2: VIF\n\nMethod 3: Variance Threshold\n\nMethod 4: SelectKBest\n\nMethod 5: ExtraTreesRegressor\n\nMethod 6: Mutual Information Gain\n\nMethod 7: Recursive Feature Elimination\n\nMethod 8: Forward Feature selection\n\nMethod 9: Backward elimination \n\nMethod 10: Bidirectional\/ Stepwise feature elimination\n","2174eb7b":"### Dropping variables obtained from above observations","89ca78a3":"# Bivariate Analysis","e6f88189":"#Testing on kaggle test data","afb30b92":"# Method 3: VarianceThreshold","7f1e58fa":"# Method:1 Correlation heatmap","ce2ee5a1":"1stFlrSF+ 2ndFlrSF + LowQualFinSF= GrLivArea\t\nBsmtFinSF1\t+ BsmtFinSF2 + BsmtUnfSF = TotalBsmtSF\n\n6 variables can be dropped","f93c63f2":"### Checking Missing values percentage","ac47115b":"## Univariate Analysis plot for Numerical data","7ff22dcb":"# Method 10: Bidirectional\/Step-wise feature Elimination","924bf859":"### Checking for duplicate records","168ab4a8":"# Method 6: Mutual Information gain","29484073":"# Method 4 :SelectKBest","8ec22055":"## Univariate Analysis Plots for categorical data","b8716a66":"### Checking for unique values count in each column","3d9e7385":"## Continous vs Target variable","60eeda59":"# Univariate Analysis","7a143fc2":"From correlation heatmap below we understand variables are having high correlation\n\n1stFlrSF', 'GarageArea', 'GarageYrBlt', 'TotRmsAbvGrd'}","5f481a34":"we are left with 97 variables and this can be used for further reduction","d6a577f5":"# Method 9: Backward elimination","526e3bf9":"# Method 5: ExtraTreesRegressor","1fcaff6d":"# Method:2 VIF values","274bbcde":"# Feature Selection","05f46915":"#Model Building","6c462ce2":"'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','Bedroom','Kitchen','KitchenQual','TotRmsAbvGrd',\n'Fireplaces','GarageType','MiscVal','MoSold','YrSold' variables are in int64\/float64 type, but they can be treated as categorical. Below are the bar plots for the same","1c4610b0":"https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python\/","947a74cf":"https:\/\/stats.stackexchange.com\/questions\/204141\/difference-between-selecting-features-based-on-f-regression-and-based-on-r2\/207396#207396?newreg=bc481cdb1ae54e85acad7fc29d220346\n\nhttps:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/\n\nfor classification score fucntion woould be score_func=chi2"}}