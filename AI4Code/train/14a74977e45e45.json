{"cell_type":{"77e4b28a":"code","b7032700":"code","82b17fa4":"code","da6aff41":"code","30396e9b":"code","9762985a":"code","e11ef02e":"code","574d4611":"code","d3176ba5":"code","2c069e30":"code","1f10e0d4":"code","48f7523a":"code","0433d83b":"code","0ed25f59":"code","3c8a0d72":"code","0b246f8e":"code","ccee2624":"code","228b0d93":"code","d68fc214":"code","234a6ec9":"code","b4b02bbf":"code","335388f8":"code","427ad3a8":"code","f6a6f04d":"code","6f81e964":"code","faeb266d":"code","d3deeb63":"markdown","0bfeed83":"markdown","215d0aa0":"markdown","b75183df":"markdown","d071bd13":"markdown","a947822b":"markdown","d81ed5c7":"markdown","f133b752":"markdown","9b3d6fb7":"markdown","d66effd4":"markdown","214fe3ea":"markdown","a757b087":"markdown","40fef65e":"markdown","06034e9d":"markdown","7a98ef42":"markdown","006944bc":"markdown","51b9dc9a":"markdown","079ba312":"markdown","3d2365f8":"markdown","d3941b25":"markdown","7dec2a40":"markdown","b155cdaf":"markdown","3f82898a":"markdown","c71ca30f":"markdown","99fcb6b7":"markdown"},"source":{"77e4b28a":"import numpy as np \nimport pandas as pd\n\n# graphics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# plotly\nimport plotly as py\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly.express as px\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n# modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","b7032700":"games_source = pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')","82b17fa4":"print('data dimension:', games_source.shape)\nprint('\\n')\nprint('column names:', list(games_source.columns))\nprint('\\n')\nprint('missing values:', games_source.isnull().any().sum())\nprint('\\n')\ngames_source.head(10)","da6aff41":"# make a copy of data, set gameId to index\ngames = games_source.copy()\ngames.drop(columns='gameId', inplace=True)\ngames.iloc[0,:]","30396e9b":"# heatmap\ncorr = games.corr()\n\nplt.figure(figsize=(18,18))\nsns.heatmap(corr, square=True)\nplt.show()","9762985a":"games.drop(columns=['blueTotalGold', 'blueTotalExperience', 'blueTotalMinionsKilled', 'blueCSPerMin', 'blueGoldPerMin', 'blueAvgLevel'], inplace=True)\ngames.drop(columns=['redFirstBlood', 'redGoldDiff', 'redExperienceDiff', 'redKills', 'redDeaths', 'redAssists', 'redTotalGold', \n                    'redTotalExperience', 'redTotalMinionsKilled', 'redGoldPerMin', 'redAvgLevel', 'redCSPerMin'], inplace=True)","e11ef02e":"# heatmap\ncorr = games.corr()\n\nplt.figure(figsize=(18,18))\nsns.heatmap(corr, annot=True, square=True)\nplt.show()","574d4611":"# count blueWins\n\nplt.figure(figsize=(6,6))\nsns.countplot(data=games, x='blueWins')\nplt.title('Distribution of Blue Side win\/lose', size=19)\nplt.show()","d3176ba5":"# train, test split\nX = games.iloc[:,1:]\ny = games['blueWins']\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n\nprint('x_train:', x_train.shape)\nprint('x_test:', x_test.shape)\nprint('y_train:', y_train.shape)\nprint('y_test:', y_test.shape)\nprint('\\n')\nprint('mean gold diff:', x_train['blueGoldDiff'].mean())\nprint('mean exp diff:', x_train['blueExperienceDiff'].mean())","2c069e30":"# count blueWins\ny_train_count = pd.DataFrame({'blueWins':y_train})\ny_test_count = pd.DataFrame({'blueWins':y_test})\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(12,6))\n\nsns.countplot(data=y_train_count, x='blueWins', ax=ax1)\nax1.set_title('Training set')\nax1.grid(False)\n\nsns.countplot(data=y_test_count, x='blueWins',ax=ax2)\nax2.set_title('test set')\nax2.grid(False)\n\nplt.suptitle('Distribution of Blue Side win\/lose', size=20)\nplt.show()","1f10e0d4":"model = RandomForestClassifier(random_state=10)\nmodel.fit(x_train, y_train)\nmodel.score(x_test, y_test)","48f7523a":"important_features = pd.DataFrame({'feature':x_train.columns, 'importance_score':model.feature_importances_})\nimportant_features.sort_values(by='importance_score', ascending=False, inplace=True)\n\nplt.figure(figsize=(24,6))\np = sns.barplot(data=important_features, x='feature', y='importance_score')\n\nplt.xticks(rotation=65)\nplt.show()","0433d83b":"# # set range of parameters for tuning\n# n_estimators = [int(x) for x in np.linspace(start=50, stop=2000, num=79)]\n# criterion = ['gini', 'entropy']\n# max_features = ['auto','sqrt','log2',None]\n# max_depth = [int(x) for x in np.linspace(start=5, stop=25, num=20)]\n# max_depth.append(None)\n# min_samples_split = [2,5,8]\n# min_samples_leaf = [1, 2, 4]\n\n# parameters = {'n_estimators' : n_estimators,\n#                  'criterion' : criterion,\n#                  'max_features' : max_features,\n#                  'max_depth' : max_depth,\n#                  'min_samples_split' : min_samples_split,\n#                  'min_samples_leaf' : min_samples_split}\n\n\n# rdm_for = RandomForestClassifier()\n# rdm_CV = RandomizedSearchCV(rdm_for, parameters, cv=5, n_iter = 10, verbose=1)\n# rdm_CV.fit(X,y)\n\n# rdm_CV.best_params_\n# # {'n_estimators': 950,\n# #  'min_samples_split': 8,\n# #  'min_samples_leaf': 1,\n# #  'max_features': 'auto',\n# #  'max_depth': 7,\n# #  'criterion': 'gini'}","0ed25f59":"model = RandomForestClassifier(n_estimators=950, \n                               min_samples_split=8, \n                               min_samples_leaf=1, \n                               max_features='auto',\n                              max_depth=7,\n                              criterion='gini',\n                              random_state=10)\nmodel.fit(x_train, y_train)\nmodel.score(x_test, y_test)\n","3c8a0d72":"# logistice regression\nclf = LogisticRegression()\nclf.fit(x_train, y_train)\nclf.score(x_test, y_test)","0b246f8e":"# set range of parameters for tuning\nC = np.arange(0.00001, 5, 0.1)\npenalty = ['l1', 'l2', 'elasticnet', 'none']\n\n\nparameters = {'C' : C,'penalty' : penalty}\n\n\nclf = LogisticRegression()\nclf_CV = RandomizedSearchCV(clf, parameters, cv=5, n_iter = 10, verbose=1)\nclf_CV.fit(X,y)\n\n# clf_CV.best_params_\n# {'penalty': 'l2', 'C': 4.50001}","ccee2624":"# logistice regression after fine tuning\nclf = LogisticRegression(penalty='l2', C=4.5)\nclf.fit(x_train, y_train)\nclf.score(x_test, y_test)","228b0d93":"# make a copy of data, set gameId to index\ngames = games_source.copy()\ngames.drop(columns='gameId', inplace=True)\n\nX = games.iloc[:,1:]\ny = games['blueWins']\n\n\n# Standardize\nX_scaled = StandardScaler().fit_transform(X)\n\n# PCA\npca = PCA(n_components='mle', svd_solver='full')\npca.fit(X_scaled)\n","d68fc214":"# var ratio\nvariance_ratio = pd.DataFrame({'Component':range(0,25), 'Percentage_explained_variance':pca.explained_variance_ratio_})\nvariance_ratio\n\nplt.figure(figsize=(24,6))\nsns.barplot(data=variance_ratio, x='Component', y='Percentage_explained_variance')\nplt.title('Percentage of Variance Explained', size=25)\nplt.show()\n\n# cumulative sum\ncumsum_ratio = pd.DataFrame({'Component':range(0,25), 'Cumulative_sum':pca.explained_variance_ratio_.cumsum()})\n\nplt.figure(figsize=(24,6))\nsns.set_style(\"whitegrid\")\nsns.lineplot(data=cumsum_ratio, x='Component', y='Cumulative_sum')\nplt.title('Cumulative Sum of Variance Explained', size=25)\nplt.xticks(np.arange(0,26, step=1))\nplt.show()","234a6ec9":"aa = pd.DataFrame({'feature':X.columns, 'eigenvalue': abs(pca.components_[0])})\naa.sort_values(by='eigenvalue', ascending=False).head(10)","b4b02bbf":"aa = pd.DataFrame({'feature':X.columns, 'eigenvalue': abs(pca.components_[1])})\naa.sort_values(by='eigenvalue', ascending=False).head(10)","335388f8":"aa = pd.DataFrame({'feature':X.columns, 'eigenvalue': abs(pca.components_[2])})\naa.sort_values(by='eigenvalue', ascending=False).head(10)","427ad3a8":"aa = pd.DataFrame({'feature':X.columns, 'eigenvalue': abs(pca.components_[3])})\naa.sort_values(by='eigenvalue', ascending=False).head(10)","f6a6f04d":"new_x = pca.transform(X_scaled)\nnew_x = pd.DataFrame(new_x)\nnew_x","6f81e964":"# train, test split\nX = new_x.copy()\ny = games['blueWins']\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n\nprint('x_train:', x_train.shape)\nprint('x_test:', x_test.shape)\nprint('y_train:', y_train.shape)\nprint('y_test:', y_test.shape)","faeb266d":"model = RandomForestClassifier()\nmodel.fit(x_train, y_train)\nmodel.score(x_test, y_test)\n","d3deeb63":"# Conclusion\n\n**Gold Difference** and **Experience Difference** are the two greatest factors in the game, any actions that enlarge the gap of gold and experience will help teams towards victory.\n\nRegarding prediction accuracy, is it impossible to be close to 100% because the data is only for first 10 minutes and an average game can last between 25 -35 minutes. Also comebacks do happen from time to time and numerical advantage on certain stats does not guarantee victory.","0bfeed83":"# Deciding Factors in League of Legends\n\n### This notebooks tries to find out what are the key reasons a team wins\/loses, and at the same time build a model predicting game results only using data from the first 10 mins. The data consists of SOLO QUEUE games at high ELO (DIAMOND I to MASTER)\n\n### The algorithms used are:\n* Random Forest\n* Logistic Regression\n* Principal Compnent Analysis (PCA)\n\n### Summary of findings:\n\nThe two most important statistics on winning\/losing are:\n* Gold difference\n* Experience difference","215d0aa0":"# Data cleaning\n## Remove redundant variables\n\nClearly, some of the variables are not needed\n\n* some variables have correlation = -1, e.g. `blueKills` = `redDeaths` * -1\n* some variables can be calculated using other variables, e.g. `blueGoldDiff` = `blueTotalGold` - `redTotalGold`\n","b75183df":"### Not much of improvement","d071bd13":"### Fit the model using results from random search","a947822b":"## Logistic Regression - Hyperparameter Tuning","d81ed5c7":"# Random Forest\n\nFirst fit a basic Random Forest model","f133b752":"# Logistic Regression","9b3d6fb7":"![image.png](attachment:image.png)","d66effd4":"## Fitting Random Forest using the PCA transformed X","214fe3ea":"### Component 2\n\n* Kills \/ Deaths \/ Assists","a757b087":"## Count number of wins on each side, data look quite balanced","40fef65e":"## Fine Tuning Hyperparameters using `RandomizedSearchCV`\n\n### This takes a long time to run, uncomment the code if you want to run it yourself","06034e9d":"## Make sure the classes are balanced","7a98ef42":"## Important features in Random Forest\n\nIt seems like `blueGoldDiff` and `blueExperienceDiff` are the most significant factors (this is intuitively correct if you play the game)","006944bc":"## Examine important features in first few components\n\n### Take a look at highest eigenvalues in the first few components, identify important features","51b9dc9a":"# Basic info","079ba312":"## Explained variance by each component\n\n* The first component is fairly dominant compare to others, explaining over 30% of total variance\n* The first 15 components explains over 90% of total variance","3d2365f8":"## Remove variables that can be calculated by others\n\n* `blueTotalGold`, `blueTotalExperience`, `blueTotalMinionsKilled`, `blueCSPerMin`, `blueGoldPerMin`, `blueAvgLevel`\n* `redFirstBlood`, `redGoldDiff`, `redExperienceDiff` `redKills`, `redDeaths`, `redAssists`, `redTotalGold`, `redTotalExperience`, `redTotalMinionsKilled`, `redCSPerMin`, `redGoldPerMin`, `redAvgLevel`\n\n### Some of the reasons are:\n\nVariables such as `CS per min`, `minions kill` are all reflected in `gold differeneces`\n\n`redFirstBlood` and `blueFirstBlood` are complete opposite so only need to keep one of them\n","d3941b25":"### Component 4\n\n* Again, experience of players (equivalent to average level)","7dec2a40":"### Component 3\n\n* Elite monsters, includes dragons and Heralds\n* Interestingly the eigenvalue is not that big for Heralds, this might be due to early tower damage does not have hugh impact unless the team can snowball really well with the vision advantage. Or the Herald could be traded with Dragon kill from the other team, but Dragons are more valuable.","b155cdaf":"## Principal Component Analysis (PCA)\n\n### Standardize the data, then perform PCA","3f82898a":"## Split data into train and test ","c71ca30f":"## Load Libraries","99fcb6b7":"### Component 1\n\n* Gold difference \n* Experience difference"}}