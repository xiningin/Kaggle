{"cell_type":{"0aeed720":"code","17b8e3aa":"code","ca69df2c":"code","fbba6b40":"code","7b477489":"code","005c4672":"code","2736ef6b":"code","db222d58":"code","001a1cb8":"code","e6954b22":"code","94155bcc":"code","1cea347a":"code","cc7ba3e9":"code","2c21f75c":"code","47006cfe":"code","1a9db45e":"code","6f24d39e":"code","ee877f2d":"code","6e645989":"code","32750fed":"markdown","af55e818":"markdown","12701e56":"markdown","ae572f61":"markdown","7e8f7bf3":"markdown"},"source":{"0aeed720":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom matplotlib import colors \nfrom matplotlib.ticker import PercentFormatter \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\n\nprint(os.listdir(\"..\/input\"))\n\nabnormal = pd.read_csv(\"..\/input\/ptbdb_abnormal.csv\", header = None) \nnormal = pd.read_csv(\"..\/input\/ptbdb_normal.csv\", header = None)\n\nabnormal = abnormal.drop([187], axis=1)\nnormal = normal.drop([187], axis=1)\n\nabnormal.head","17b8e3aa":"flatten_ab_y = (abnormal.values)\nflatten_ab_y  = flatten_ab_y[:,5:70].flatten()\n\nprint(flatten_ab_y.shape)\n\nab_x=np.arange(0,65)\nab_x = np.tile(ab_x, abnormal.shape[0])\n\nplt.hist2d(ab_x, flatten_ab_y, bins = (65,100), cmap = plt.cm.jet) \n\nplt.show()","ca69df2c":"plt.plot((abnormal.values)[0][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[50][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[117][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[1111][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[100][5:70])\nplt.show()","fbba6b40":"flatten_norm_y = normal.values\nflatten_norm_y  = flatten_norm_y[:,5:70].flatten()\n\nnorm_x=np.arange(0,65)\nnorm_x = np.tile(norm_x, normal.shape[0])\n\nplt.hist2d(norm_x,flatten_norm_y, bins=(65,100), cmap=plt.cm.jet)\nplt.show()","7b477489":"plt.plot((normal.values)[0][5:70])\nplt.show()\n\nplt.plot((normal.values)[50][5:70])\nplt.show()\n\nplt.plot((normal.values)[117][5:70])\nplt.show()\n\nplt.plot((normal.values)[1111][5:70])\nplt.show()\n\nplt.plot((normal.values)[100][5:70])\nplt.show()","005c4672":"y_abnormal = np.ones((abnormal.shape[0]))\ny_abnormal = pd.DataFrame(y_abnormal)\n\ny_normal = np.zeros((normal.shape[0]))\ny_normal = pd.DataFrame(y_normal)\n\nX = pd.concat([abnormal, normal], sort=True)\ny = pd.concat([y_abnormal, y_normal] ,sort=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","2736ef6b":"print(abnormal.dtypes, normal.dtypes)","db222d58":"abnormal.shape","001a1cb8":"normal.shape","e6954b22":"np.any(X_train.isna().sum())","94155bcc":"np.any(X_test.isna().sum())","1cea347a":"# Create sequential model \nmodel = tf.keras.models.Sequential()\n\n#Dense layer as first layer with 10 neurons, input share (187,) and and leaky Relu activation\nmodel.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape=(187,)))\n\n#Dense layer as second layer with 10 neurons and leaky Relu activation\nmodel.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n\n#Dense layer as third layer with 10 neurons and leaky Relu activation\nmodel.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n\n#Dense layer as fourth layer with 10 neurons and leaky Relu activation\nmodel.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n\n#Softmax as last layer with two outputs\nmodel.add(tf.keras.layers.Dense(2, activation='softmax'))","cc7ba3e9":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","2c21f75c":"model.summary()","47006cfe":"ann_model_history = model.fit(X_train, y_train, epochs=300, batch_size = 10, validation_data = (X_test, y_test))","1a9db45e":"from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout\n\n\n#Reshape train and test data to (n_samples, 187, 1), where each sample is of size (187, 1)\nX_train = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test = np.array(X_test).reshape(X_test.shape[0], X_test.shape[1], 1)\n\nprint(\"Train shape: \", X_train.shape)\nprint(\"Test shape: \", X_test.shape)\n\n# Create sequential model \nclf = tf.keras.models.Sequential()\n\n#First CNN layer  with 32 filters, conv window 5, relu activation and same padding\nclf.add(Conv1D(filters=32, kernel_size=(5,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape = (X_train.shape[1],1)))\n\n#Second CNN layer  with 64 filters, conv window 5, relu activation and same padding\nclf.add(Conv1D(filters=64, kernel_size=(5,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n\n#Third CNN layer with 64 filters, conv window 5, relu activation and same padding\nclf.add(Conv1D(filters=128, kernel_size=(5,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n\n#Fourth CNN layer with Max pooling\nclf.add(MaxPool1D(pool_size=(5,), strides=2, padding='same'))\nclf.add(Dropout(0.5))\n\n#Flatten the output\nclf.add(Flatten())\n\n#Add a dense layer with 512 neurons\nclf.add(Dense(units = 512, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n\n#Add a dense layer with 1024 neurons\nclf.add(Dense(units = 1024, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n\n#Softmax as last layer with two outputs\nclf.add(Dense(units = 2, activation='softmax'))","6f24d39e":"clf.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])","ee877f2d":"clf.summary()","6e645989":"history = clf.fit(X_train, y_train, epochs = 10)\ny_pred = clf.predict(X_test)","32750fed":"**From the above, you can see that PTB data marked as abnormal donot have any fixed pattern in the data**","af55e818":"**From the above, you can see that PTB data marked as normal has a fixed bell shape in the data and it peaks between the features 20-30 **","12701e56":"**Check any of the features have a null**","ae572f61":"**From the above histogram color map for PTB data marked as normal, you can infer that the graph of ECG features follow a standard bell shape and they peak in between the features 20-30**","7e8f7bf3":"**From the above histogram color map for PTB data marked as abnormal, you can infer that most of the ECG features is widely distributed in the range of 0 - 0.4 and there is no fixed pattern to this data**"}}