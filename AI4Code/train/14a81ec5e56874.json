{"cell_type":{"ba2b7bb1":"code","3f71a54e":"code","7af404df":"code","4f733627":"code","dcd05674":"code","efde6caf":"code","c1506d0a":"code","5caf4da1":"code","f983bdcf":"code","bccd1e68":"code","de513c3a":"code","541ac457":"code","86741e46":"code","5321a45e":"code","4b81f230":"code","cd89f55f":"code","63e86557":"code","cad4b909":"code","aba71972":"code","2e8f39bb":"code","847ed4c8":"code","83102b2f":"code","0794fc11":"code","0280ae29":"code","063d9734":"code","df3086af":"code","a01254a4":"code","60c83c43":"code","667e7633":"code","4ef40080":"code","e41a7189":"code","12f7bba4":"code","653751a4":"code","683341f8":"code","b7d44644":"code","60c1c9b3":"code","4818cfed":"code","c7f3e9cc":"code","9073af33":"code","d7429aa7":"code","58627b97":"code","dbb00092":"code","ba33de28":"code","490eefe2":"code","e4208e9e":"code","cb1325d0":"code","e8517d46":"code","00d77ba7":"code","3966b34f":"code","871b0b18":"code","2aa41bd2":"code","6a9cec89":"code","8601697b":"code","5a06d9b3":"code","9de42f8e":"code","638688b5":"code","555b1a31":"code","b84f2efc":"code","91cd82f1":"code","548c39a0":"code","c86bbe8d":"code","ff1e9248":"code","509c0326":"code","b711aba9":"code","d3b3feb3":"code","86c0314c":"code","ec5f3a8c":"code","f0b37e1c":"code","9c1f0e90":"code","027e8417":"code","aade4855":"code","a13eb328":"code","a8858439":"code","cc8b0403":"code","180ddcd4":"code","644f19c2":"code","144f6935":"code","8a5952d0":"code","dea81fb8":"code","e135b865":"code","90331c85":"code","84b6abc4":"code","79b37e03":"code","8728d22a":"code","799c7702":"code","54696f80":"code","819d282a":"code","f0f7595d":"code","588014db":"code","2babf49e":"code","f4505b66":"code","d6c192ed":"code","e4d7753b":"code","2ddc6023":"code","4afd0bcf":"code","bec33d92":"code","ed02314b":"code","712bd37c":"code","fcc14743":"code","31e83269":"code","14a7a521":"code","0ef19679":"code","0c26fced":"code","c6279fc4":"code","fa72e634":"code","7e2e548c":"code","65f02d62":"code","973df451":"code","f433be95":"code","e804c34b":"code","5ac5ab41":"code","789f2212":"code","935e739e":"code","6d767a43":"code","4d7bb842":"code","39b5b1c7":"code","a79a9401":"code","e89e8761":"code","7996a571":"code","d580b0fe":"code","159e8064":"code","47fa8d1a":"code","e40de66c":"code","134c8bf6":"code","107e2ad1":"code","b57178a4":"code","a451bd2d":"code","16b51114":"code","427ab79d":"code","06087a77":"code","dfe8aed0":"code","b5903f5b":"code","9a2ea4aa":"code","e6dceed9":"code","43314d02":"code","d29487b5":"code","59733cb9":"code","508c46dc":"code","e9b79482":"code","a7b042dd":"code","7223ecb0":"code","6024c1b2":"code","ff1c885d":"code","39d3fe42":"code","6bf26f33":"code","26a3e6c4":"code","3ccc62df":"code","18eb538c":"code","2e97883d":"code","a348613c":"code","126601f9":"code","d26b80a0":"code","bc4c25ec":"code","54016c75":"code","267411dc":"markdown","46e31341":"markdown","5cf592e4":"markdown","1e00346e":"markdown","aea4364e":"markdown","d995d1a4":"markdown","4863eee2":"markdown","53874257":"markdown","7b3cd144":"markdown","4aa9fa38":"markdown","cf6cc994":"markdown","f847cd84":"markdown","ccf1674a":"markdown","ba51709c":"markdown","44e7c6f4":"markdown","dc883a68":"markdown","8a32a046":"markdown","a7ee51c5":"markdown","b335bc7b":"markdown","eec06d9c":"markdown","9474eec7":"markdown","f8be36e0":"markdown","157033b1":"markdown","aadf8841":"markdown","3ac92522":"markdown","d660ebe8":"markdown","88994b99":"markdown","63bf4102":"markdown","362f9662":"markdown","52c72652":"markdown","990cb24e":"markdown","935d3047":"markdown","112392d4":"markdown","d677c16c":"markdown","d2909f3e":"markdown","fc8a9ac1":"markdown","8df78bb1":"markdown","4c6704b6":"markdown","11f77913":"markdown","ebb43461":"markdown","8b5570c5":"markdown","bb0718e8":"markdown","4741fbec":"markdown","e1c8cfcd":"markdown","4843ddc0":"markdown","297b5bb9":"markdown","fde7c46c":"markdown","f7397d31":"markdown","7bee26d3":"markdown","3ebe2a70":"markdown","7f36d31c":"markdown","2c2d302c":"markdown","67da6969":"markdown","0f03a24b":"markdown","10b02e80":"markdown","e297bb1f":"markdown","03851570":"markdown","80e21e9d":"markdown","08395063":"markdown","9044b6aa":"markdown","d1bd3084":"markdown","e386db03":"markdown","ee8747cf":"markdown","c81160f3":"markdown","82865b9e":"markdown","b92d8c5e":"markdown","e8660d38":"markdown","6450f01b":"markdown","1645ed50":"markdown","dc700539":"markdown","15035d82":"markdown","6c46275b":"markdown","e5bbbe31":"markdown","33e49068":"markdown","dfa011d2":"markdown","f62632d5":"markdown","87aaf72b":"markdown","f0687edb":"markdown","1f85b489":"markdown","53713e4a":"markdown","3530a72a":"markdown","7bc48159":"markdown","00366127":"markdown","0c32a288":"markdown","288353c8":"markdown","33bc35c2":"markdown","621efc18":"markdown","9385b19b":"markdown","10bf369f":"markdown","5b4cb964":"markdown","b39fa2a4":"markdown","2fd3611f":"markdown","61add118":"markdown"},"source":{"ba2b7bb1":"# Author: Caleb Woy\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nfrom scipy.stats import kurtosis, skew # checking distributions\nimport scipy.stats as stat # plotting, mostly\nimport scipy.spatial.distance as sp # Computing distances in kNN\nimport matplotlib.pyplot as pl # plotting\nimport seaborn as sb # plotting\nimport sklearn as sk # Regression modelling\nimport os # Reading data\nimport sklearn.model_selection # train and test splitting\nimport matplotlib.pylab as plt # plotting hyperparamter cost curves\nimport time # timing custom knn model\nfrom sklearn.model_selection import RandomizedSearchCV # tuning hyperparams for complex models\nfrom sklearn.metrics.scorer import make_scorer # defining custom model evaluation function\nfrom sklearn.ensemble import GradientBoostingClassifier as gb # modelling\nfrom sklearn.neighbors import KNeighborsClassifier # modelling\nfrom sklearn.model_selection import GridSearchCV # tuning hyperparams for simple models\nfrom sklearn.ensemble import RandomForestClassifier as rf # modelling\nfrom sklearn.svm import SVC as sv # modelling","3f71a54e":"# So we can see some interesting output without truncation\npd.options.display.max_rows = 1000\n\npath_to_data = \"\/kaggle\/input\/\"\n\n# Loading the training and test data sets into pandas\ntrain_original = pd.read_csv(path_to_data + \"\/adult.data\", names=['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n                                                                  'marital-status', 'occupation', 'relationship', 'race', 'sex',\n                                                                  'cap-gain', 'cap-loss', 'hrsperwk', 'native', 'label'])\ntest_original = pd.read_csv(path_to_data + \"\/adult.test\", skiprows=1, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n                                                                             'marital-status', 'occupation', 'relationship', 'race', 'sex',\n                                                                             'cap-gain', 'cap-loss', 'hrsperwk', 'native', 'label'])\n\n# Combining the training and test sets\nframes = [train_original, test_original]\ndata_original = pd.concat(frames)\n\n# print the head\ndata_original.head()","7af404df":"feature_name = 'label'\n\n# Checking counts per designation\ndata_original.groupby(feature_name).count()","4f733627":"# the output here is erroneously grouped into 4 rows, I need to remove the period from every label in the test set to get an accurate count.\ndata_original[[feature_name]] = data_original[[feature_name]].replace([\" <=50K.\", \" >50K.\"], [\" <=50K\", \" >50K\"])\ndata_original.groupby(feature_name).count()","dcd05674":"# better, now we can view a summary\ndata_original[[feature_name]].describe()","efde6caf":"# So, there are 48842 values in the label column. There are 2 factor levels for the column. The most common label is '<= 50K' and it occurs 37155 times, \n# roughly 3\/4 of the individuals.\n# Now we'll check for missing values.\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","c1506d0a":"# No missing values for our label, that's good. I'll move onto the next feature.","5caf4da1":"feature_name = 'age'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","f983bdcf":"#checking for missing values\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","bccd1e68":"# No missing values on age, let's check skewness and kurtosis\nprint(f'Skewness: {skew(data_original.age)}')\nprint(f'Kurtosis: {kurtosis(data_original.age)}')","de513c3a":"# The sample distribution of ages appears to be slightly right skewed with very slight negative kurtosis. This may need transformed for future modelling.\n# Let's visualize this one to confirm the skewness.\nx = data_original.age\npl.hist(x, bins=80)\npl.xlabel('Age')\npl.ylabel('Frequency')","541ac457":"# The values at the end of the right tail are definitely outliers however they're meaningful in our analysis (the elderly are important too). There don't appear to be any obvious\n# errors caused by typos (like 500 or 0) \n\n# I'll create a new feature by taking the log \n# I'll create a new feature by centering with the z score\n# I'll create a new feature by taking the log and centering with the z score\n\nlogage = np.log(data_original['age'])\ndata_original['log_age'] = logage\n\nmean = np.mean(data_original['age'])\nstdev = np.std(data_original['age'])\ndata_original['age_ZCentered'] = (data_original['age'] - mean) \/ stdev\n\nmean = np.mean(logage)\nstdev = np.std(logage)\ndata_original['log_age_ZCentered'] = (logage - mean) \/ stdev\n\nx = data_original['log_age_ZCentered']\npl.hist(x, bins=80)\npl.xlabel('log_age_ZCentered')\npl.ylabel('Frequency')\n\n# checking for success\ndata_original.head()\n\n#all good","86741e46":"feature_name = 'workclass'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","5321a45e":"# Roughly 3\/4 of our individuals appear to be working in the private sector. Describe returned that there are 9 factor levels in this feature when we know there are actually \n# only 8. so there must be missing values in this feature. Let's check.\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')\n","4b81f230":"# There are 2799 ? values currently. None of them are Null or NaN values, so that's good. We have a few options here. The first is to impute the mode level (Private). \n# The second is to check if there are any other features here that might explain variation in with workclass, then if so, predict the missing workclass values. The third is \n# leave the ? value in as a placeholder unkown value and predict based on the effect of the level as we would any other feature.\n\n# I'll make some boxplots to see if there's any explainable variation.\n\nsb.boxplot( x=data_original[\"log_age_ZCentered\"], y=data_original[\"workclass\"] )\nsb.boxplot( x=data_original[\"fnlwgt\"], y=data_original[\"workclass\"] )\nsb.boxplot( x=data_original[\"education-num\"], y=data_original[\"workclass\"] )\nsb.boxplot( x=data_original[\"cap-gain\"], y=data_original[\"workclass\"] )\nsb.boxplot( x=data_original[\"cap-loss\"], y=data_original[\"workclass\"] )\nsb.boxplot( x=data_original[\"hrsperwk\"], y=data_original[\"workclass\"] )","cd89f55f":"# None of these give off the appearance of explainatory variation that I'm looking to test with ANOVA so I'll impute the mode (Private) for the missing values. This can\n# always be undone later during the modelling fase should we like to check how well we can predict with an unkown value effect.\n\n# Checking the original counts at each factor level\ndata_original.groupby(feature_name).count()\n\n# Making the replacement and recalculating the values\ndata_original[[feature_name]] = data_original[[feature_name]].replace([\" ?\"], [\" Private\"])\ndata_original.groupby(feature_name).count()\n\n# All good.","63e86557":"feature_name = 'fnlwgt'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","cad4b909":"# These are large numbers, any predictive model we apply on this data set would befit from some regularization here in the future. The max is exponetially larger than the mean.\n# High values in fnlwgt will need investigated.\n\n# Let's check for missing values.\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","aba71972":"# None, the data set authors created this feature so that should have been expected. Thanks authors!\n\n# Checking skewness and kurtosis.\nprint(f'Skewness: {skew(data_original[feature_name])}')\nprint(f'Kurtosis: {kurtosis(data_original[feature_name])}')","2e8f39bb":"# The fnlwgt column has some strong right skew and high positive kurtosis. It should look like a big spike on the left side of the distribution.\n\n# Let's visualize to confirm.\nx = data_original[feature_name]\npl.hist(x, bins=100)\npl.xlabel('fnlwgt')\npl.ylabel('Frequency')\n","847ed4c8":"# Yup. This feature would benefit from a log transformation. \n\n# Creating new features. I'll take the log transform then standardize that using the z-score.\nlogfnlwgt = np.log(data_original['fnlwgt'])\ndata_original['log_fnlwgt'] = logfnlwgt\n\nmean = np.mean(data_original['fnlwgt'])\nstdev = np.std(data_original['fnlwgt'])\ndata_original['fnl_wgt_ZCentered'] = (data_original['fnlwgt'] - mean) \/ stdev\n\nmean = np.mean(logfnlwgt)\nstdev = np.std(logfnlwgt)\ndata_original['log_fnl_wgt_ZCentered'] = (logfnlwgt - mean) \/ stdev\n\nx = data_original['log_fnl_wgt_ZCentered']\npl.hist(x, bins=100)\npl.xlabel('log_fnl_wgt_ZCentered')\npl.ylabel('Frequency')\n\n# checking for success\ndata_original.head()\n\n# all good","83102b2f":"#Let's view the largest values of the distribution. \ndata_original.nlargest(10, ['log_fnl_wgt_ZCentered']) ","0794fc11":"# Regarding the outliers at the tail of fnlwgt, none of these appear to be abnormal. We can't know forsure \n# without knowing how fnlwgt was calulated, yet the consistent increasing of the feature values up to the max appears systematic and not erroneous. I won't do anything\n# special about them.","0280ae29":"feature_name = 'education'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","063d9734":"# Seeing the correct number of unique factor levels here so likely no missing values. HS-grad is the most common level\n# Let's confirm:\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","df3086af":"# Yup. Looking good here.","a01254a4":"feature_name = 'education-num'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","60c83c43":"# Mean value of ~10. Max and min are prestty evenly spread.\n# Checking for missing values:\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","667e7633":"# Let's look at the skewness and kurtosis:\n\nprint(f'Skewness: {skew(data_original[feature_name])}')\nprint(f'Kurtosis: {kurtosis(data_original[feature_name])}')","4ef40080":"# Slight positive kurtosis, slight left skew. Let's visualize:\n\nx = data_original[feature_name]\npl.hist(x, bins=16)\npl.xlabel('education-num')\npl.ylabel('Frequency')","e41a7189":"# This distribution appears bimodal. Likely due to the effect of college. This might make the categorical feature (education) more useful to us.\n\n# I'll scale this by transforming it with the Z-score.\n\nmean = np.mean(data_original[feature_name])\nstdev = np.std(data_original[feature_name])\neducation_num_ZCentered = (data_original[feature_name] - mean) \/ stdev\n\n# Visualizing:\nx = education_num_ZCentered\npl.hist(x, bins=16)\npl.xlabel('education_num_ZCentered')\npl.ylabel('Frequency')","12f7bba4":"# Now to replace the original feature with the transformed version.\n\ndata_original['education_num_ZCentered'] = education_num_ZCentered\n\n# Checking:\ndata_original.head()","653751a4":"feature_name = 'marital-status'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","683341f8":"# There are 7 unique factor levels present in our distribution. So, likely no missing values. We can confirm.\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","b7d44644":"feature_name = 'occupation'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","60c1c9b3":"# There are only supposed to be 14 factor levels so they're definitely some missing values here.\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","4818cfed":"# 2809 missing values. I'll impute the mode value (Prof-specialty)\n\ndata_original.groupby(feature_name).count()\n\n# Making the replacement and recalculating the values\ndata_original[[feature_name]] = data_original[[feature_name]].replace([\" ?\"], [\" Prof-specialty\"])\ndata_original.groupby(feature_name).count()\n\n# Worked fine.","c7f3e9cc":"feature_name = 'relationship'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","9073af33":"# Seeing 6 unique factor levels as expected. Most common level is Husband.\n# Confirming no missing values\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","d7429aa7":"# Good.","58627b97":"feature_name = 'race'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","dbb00092":"# Seeing 5 unique factor levels as expected. Most common level is White.\n# Confirming no missing values\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","ba33de28":"# Good.","490eefe2":"feature_name = 'sex'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","e4208e9e":"# Seeing 2 unique factor levels as expected. Most common level is Male.\n# Confirming no missing values\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","cb1325d0":"# Good.","e8517d46":"feature_name = 'cap-gain'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","00d77ba7":"# The summary here tells us the mean gain is about a thousand dollars. The distribution appears to be dramatically right skewed and is likely mostly (0) values.\n# Let's comfirm by checking skew and kurtosis.\n\nprint(f'Skewness: {skew(data_original[feature_name])}')\nprint(f'Kurtosis: {kurtosis(data_original[feature_name])}')","3966b34f":"# Yeah . . . we'll be transforming this one. First, Let's check for missing values.\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","871b0b18":"# None. That's helpful. Time to visualize this one.\n\nx = data_original[feature_name]\npl.hist(x, bins=100)\npl.xlabel('cap_gain')\npl.ylabel('Frequency')","2aa41bd2":"# Before I transform this, I want to investigate the outlier here that's near 100K.\n\ndata_original.nlargest(10, [feature_name]) \n\n# I've actually viewed the top 60 largest values here but I set the code back to outputting the top 10 to keep this report cleaner.\n# The trend shown in the top 10 is the same as the top 60, all are labeled > 50k. ","6a9cec89":"# I think I want to create a new feature here. A simple binary feature specifying whether the individual made > 50K in capital gains alone.\n\ndata_original['cap-gains50k'] = data_original.apply(lambda x: True if x[feature_name] > 50000 else False, axis=1).astype('category')\n\n# Checking that it worked:\ndata_original.nlargest(10, [feature_name]) ","8601697b":"# That should be a really significant factor in whatever model we might use to predict our label.\n# Now I'll transform the original cap-gain feature. Taking a log of 0 will produce NaNs so I'll transform the feature to log(cap-gains + 1) and then I'll scale It with the z-score.\n\nlog_cap_gain = np.log(data_original[feature_name] + 1)\ndata_original['log_cap_gain'] = log_cap_gain\n\n# Visualizing:\nx = log_cap_gain\npl.hist(x, bins=100)\npl.xlabel('log_cap_gain')\npl.ylabel('Frequency')","5a06d9b3":"# Now scaling by Z-score:\n\nmean = np.mean(data_original[feature_name])\nstdev = np.std(data_original[feature_name])\ndata_original['cap_gain_ZCentered'] = (data_original[feature_name] - mean) \/ stdev\n\nmean = np.mean(log_cap_gain)\nstdev = np.std(log_cap_gain)\ndata_original['log_cap_gain_ZCentered'] = (log_cap_gain - mean) \/ stdev\n\n# Visualizing:\nx = data_original['log_cap_gain_ZCentered']\npl.hist(x, bins=100)\npl.xlabel('log_cap_gain_ZCentered')\npl.ylabel('Frequency')","9de42f8e":"# Checking:\ndata_original.head()","638688b5":"# Good.","555b1a31":"feature_name = 'cap-loss'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","b84f2efc":"# This distribution appears that it'll be similar to cap-gain. However, the max loss is far less than 50K so I don't think I'll be making a new feature representing this one.\n# Let's check for missing values:\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","91cd82f1":"# Good. Now skewness and kurtosis:\n\nprint(f'Skewness: {skew(data_original[feature_name])}')\nprint(f'Kurtosis: {kurtosis(data_original[feature_name])}')","548c39a0":"# Yeah pretty bad. Time to visualize:\n\nx = data_original[feature_name]\npl.hist(x, bins=100)\npl.xlabel('log_cap_gain')\npl.ylabel('Frequency')","c86bbe8d":"# I know the max value, it's definitly an outlier. Let's invetigate for others.\n\ndata_original.nlargest(10, [feature_name]) ","ff1e9248":"# These all look proper. I'll apply the same transformation to this as I did on cap-gain to keep it consistently scaled with the rest of our features.\n\nlog_cap_loss = np.log(data_original[feature_name] + 1)\ndata_original['log_cap_loss'] = log_cap_loss\n\nmean = np.mean(data_original[feature_name])\nstdev = np.std(data_original[feature_name])\ncap_loss_ZCentered = (data_original[feature_name] - mean) \/ stdev\n\nmean = np.mean(log_cap_loss)\nstdev = np.std(log_cap_loss)\nlog_cap_loss_ZCentered = (log_cap_loss - mean) \/ stdev\n\n# Visualizing:\nx = log_cap_loss_ZCentered\npl.hist(x, bins=100)\npl.xlabel('log_cap_loss_ZCentered')\npl.ylabel('Frequency')","509c0326":"# Now to replace the original feature with the transformed version.\n\ndata_original['cap_loss_ZCentered'] = cap_loss_ZCentered\ndata_original['log_cap_loss_ZCentered'] = log_cap_loss_ZCentered\n\n# Checking:\ndata_original.head()","b711aba9":"# Good.","d3b3feb3":"feature_name = 'hrsperwk'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","86c0314c":"# Mean is about 40 hours, as expected. The first and third quartile are pretty tight to the mean so we'll likely see high kurtosis here. Probably some minor right skew too.\n# I'll check skewness and kurtosis, as well as for missing values:\n\nprint(f'Skewness: {skew(data_original[feature_name])}')\nprint(f'Kurtosis: {kurtosis(data_original[feature_name])}')\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","ec5f3a8c":"# Yup. Glad there aren't missing values.\n# Let's visualize:\n\nx = data_original[feature_name]\npl.hist(x, bins=100)\npl.xlabel('hrsperwk')\npl.ylabel('Frequency')","f0b37e1c":"# I'll just Z-center this one to scale it properly. The skewness here isn't that extreme.\n\nmean = np.mean(data_original[feature_name])\nstdev = np.std(data_original[feature_name])\nhrs_per_wk_ZCentered = (data_original[feature_name] - mean) \/ stdev\n\n# Now to add the transformed version.\n\ndata_original['hrs_per_wk_ZCentered'] = hrs_per_wk_ZCentered\n\n# Checking:\ndata_original.head()","9c1f0e90":"feature_name = 'native'\n\n# viewing a summary\ndata_original[[feature_name]].describe()","027e8417":"# Our summary tells us there are 42 unique factor levels here. However, there are only 41 listed in the description, so we have missing values. Most common value is United-states\n# Confirming:\n\nboolseries = data_original.apply(lambda x: True if x[feature_name] == ' ?' else False, axis=1)\nprint(f'Number of missing values (?): {len(boolseries[boolseries == True].index)}')\nprint(f'Number of null values: {data_original[[feature_name]].isnull().sum()}')","aade4855":"# I'll impute the mode (United-States) for the missing values.\n\ndata_original.groupby(feature_name).count()\n\n# Making the replacement and recalculating the values\ndata_original[[feature_name]] = data_original[[feature_name]].replace([\" ?\"], [\" United-States\"])\ndata_original.groupby(feature_name).count()\n\n# Worked fine.","a13eb328":"# Checking correlation between all numeric features.\n\ncorrelation_matrix = data_original.corr().round(2)\npl.figure(figsize=(10,8))\nsb.heatmap(data=correlation_matrix, annot=True, center=0.0, cmap='coolwarm')","a8858439":"# Our numeric features have very weak correlation to eachother, despite the strong correlations between features and their transformed features that I created. \n# This is good for us if we're looking to predict our label because it means we won't have to worry about the preoblem of multicollinearity among them. ","cc8b0403":"# Each graph will be created with the same code, I'll just switch the names of the variables hue_lab and x_lab\nhue_lab = 'workclass'\nx_lab = 'education'\n\n# Grouping by the hue_group, then counting by the x_lab group\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\n\n# Creating the percentage vector to measure the frequency of each type\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\n\n# Creating and plotting the new dataframe \ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","180ddcd4":"hue_lab = 'workclass'\nx_lab = 'occupation'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","644f19c2":"hue_lab = 'cap-gains50k'\nx_lab = 'workclass'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","144f6935":"hue_lab = 'cap-gains50k'\nx_lab = 'education'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","8a5952d0":"hue_lab = 'marital-status'\nx_lab = 'occupation'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","dea81fb8":"hue_lab = 'relationship'\nx_lab = 'marital-status'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","e135b865":"hue_lab = 'sex'\nx_lab = 'marital-status'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","90331c85":"hue_lab = 'cap-gains50k'\nx_lab = 'marital-status'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","84b6abc4":"hue_lab = 'relationship'\nx_lab = 'occupation'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","79b37e03":"hue_lab = 'sex'\nx_lab = 'occupation'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","8728d22a":"hue_lab = 'cap-gains50k'\nx_lab = 'occupation'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","799c7702":"hue_lab = 'race'\nx_lab = 'relationship'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","54696f80":"hue_lab = 'sex'\nx_lab = 'relationship'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","819d282a":"hue_lab = 'cap-gains50k'\nx_lab = 'relationship'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","f0f7595d":"hue_lab = 'cap-gains50k'\nx_lab = 'sex'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","588014db":"sb.boxplot( x=data_original[\"log_age_ZCentered\"], y=data_original[\"workclass\"] )","2babf49e":"sb.boxplot( x=data_original[\"log_age_ZCentered\"], y=data_original[\"education\"] )","f4505b66":"sb.boxplot( x=data_original[\"log_age_ZCentered\"], y=data_original[\"marital-status\"] )","d6c192ed":"sb.boxplot( x=data_original[\"log_age_ZCentered\"], y=data_original[\"cap-gains50k\"] )","e4d7753b":"sb.boxplot( x=data_original[\"log_fnl_wgt_ZCentered\"], y=data_original[\"race\"] )","2ddc6023":"sb.boxplot( x=data_original[\"education_num_ZCentered\"], y=data_original[\"workclass\"] )","4afd0bcf":"sb.boxplot( x=data_original[\"education_num_ZCentered\"], y=data_original[\"occupation\"] )","bec33d92":"sb.boxplot( x=data_original[\"education_num_ZCentered\"], y=data_original[\"race\"] )","ed02314b":"sb.boxplot( x=data_original[\"education_num_ZCentered\"], y=data_original[\"cap-gains50k\"] )","712bd37c":"sb.boxplot( x=data_original[\"hrs_per_wk_ZCentered\"], y=data_original[\"workclass\"] )","fcc14743":"sb.boxplot( x=data_original[\"hrs_per_wk_ZCentered\"], y=data_original[\"education\"] )","31e83269":"sb.boxplot( x=data_original[\"hrs_per_wk_ZCentered\"], y=data_original[\"marital-status\"] )","14a7a521":"sb.boxplot( x=data_original[\"hrs_per_wk_ZCentered\"], y=data_original[\"occupation\"] )","0ef19679":"sb.boxplot( x=data_original[\"hrs_per_wk_ZCentered\"], y=data_original[\"relationship\"] )","0c26fced":"sb.boxplot( x=data_original[\"hrs_per_wk_ZCentered\"], y=data_original[\"sex\"] )","c6279fc4":"sb.boxplot( x=data_original[\"hrs_per_wk_ZCentered\"], y=data_original[\"cap-gains50k\"] )","fa72e634":"sb.boxplot( x=data_original[\"log_age_ZCentered\"], y=data_original[\"label\"] )","7e2e548c":"sb.boxplot( x=data_original[\"education_num_ZCentered\"], y=data_original[\"label\"] )","65f02d62":"sb.boxplot( x=data_original[\"log_fnl_wgt_ZCentered\"], y=data_original[\"label\"] )","973df451":"sb.boxplot( x=data_original[\"log_cap_gain_ZCentered\"], y=data_original[\"label\"] )","f433be95":"sb.boxplot( x=data_original[\"log_cap_loss_ZCentered\"], y=data_original[\"label\"] )","e804c34b":"sb.boxplot( x=data_original[\"hrs_per_wk_ZCentered\"], y=data_original[\"label\"] )","5ac5ab41":"hue_lab = 'label'\nx_lab = 'workclass'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","789f2212":"hue_lab = 'label'\nx_lab = 'education'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","935e739e":"hue_lab = 'label'\nx_lab = 'marital-status'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","6d767a43":"hue_lab = 'label'\nx_lab = 'occupation'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","4d7bb842":"hue_lab = 'label'\nx_lab = 'relationship'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","39b5b1c7":"hue_lab = 'label'\nx_lab = 'race'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","a79a9401":"hue_lab = 'label'\nx_lab = 'sex'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","e89e8761":"hue_lab = 'cap-gains50k'\nx_lab = 'label'\n\nhue_group = data_original.groupby([hue_lab], sort=False)\ncounts = hue_group[x_lab].value_counts(normalize=True, sort=False)\ndata = [\n    {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n    (hue, x), percentage in dict(counts).items()\n]\ndf = pd.DataFrame(data)\np = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\np.set_xticklabels(rotation=90)\np.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')","7996a571":"# Finally, dropping any duplicate rows\nfirst_len = len(data_original)\ndata_original.drop_duplicates()\nprint(f'Dropped {first_len - len(data_original)} records.')\n\n# Final data set\ndata_original.head()","d580b0fe":"# splitting off the test set\nprint(len(train_original), end = '\\t')\nprint(len(test_original))\n\nholdout = data_original.tail(16281)\ndata_original = data_original.head(32561)","159e8064":"\"\"\"\nDefinition of custom scoring function that utilizes our scoring metric.\n\ny: array, actual test labels\ny_pred: array, predicted labels\n\"\"\"\ndef score(y, y_pred):\n    score = 0\n    for x1, x2 in zip(y, y_pred):\n        # increase score by 1 for every true positive\n        if x2 == 1 and x1 == x2:\n            score += 1\n        elif x2 == 1 and x1 != x2:\n            # decrease score by 1 for every false positive\n            score -= 1\n    return score","47fa8d1a":"\"\"\"\nUtilizing GridSearchCV's parallel processing to speed up the process of finding the optimal k value.\nMaximum number of available processors will be used.\n\ndata: pd dataframe, the features and labels\ncat_columns: array, names of categorical columns to create dummy encodings for\ndistance: str, metric to be used in distance calculation\n\n\"\"\"\ndef fit_sklearn_knn_hyperparams(data, cat_columns, score_function, distance = 'euclidean'):\n\n    # initialize scorer function, compatible with GridSearchCV\n    my_scorer = make_scorer(score_function, greater_is_better=True)\n\n    data_knn_full = data\n    # create dummy encodings of categorical features\n    data_knn_full = pd.get_dummies(data_knn_full, columns=cat_columns)\n\n    # define the values of k to test\n    k = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90]\n    grid_param = {'n_neighbors': k}\n\n    # initialize model with given distance metric\n    model = KNeighborsClassifier(metric = distance)\n    # initialize grid search with custom scoring function, using default number of folds (5)\n    KNN_random = GridSearchCV(estimator = model, \n                                 param_grid = grid_param,\n                                 scoring = my_scorer,\n                                 verbose = 2,\n                                 cv = 5,\n                                 n_jobs = -1)\n    # begin tuning\n    KNN_random.fit(data_knn_full.drop(columns=['label']), data_knn_full['label'].replace([\" <=50K\", \" >50K\"], [0, 1]))\n    # print results\n    print(KNN_random.best_params_)\n    print(f'score: {KNN_random.best_score_}')","e40de66c":"\"\"\"\nCompute and print the cost matrix for a single k value. Utilizes sklearn kNN.\n\ntrain: pd dataframe, training data\ntest: pd dataframe, testing data\ncat_columns: array, names of categorical columns to create dummy encodings for\ndistance: str, metric to be used in distance calculation\nk: int, the number of neighbors to tally a vote with\n\n\"\"\"\ndef test_sklearn_knn(train, test, cat_columns, distance='euclidean', k = 1):\n    # define number of expiriments to perform and initialize confusion matrix\n    result_avgs = [0, 0, 0, 0]\n\n    # create dummy encodings of categorical features\n    train = pd.get_dummies(train, columns=cat_columns)\n    test = pd.get_dummies(test, columns=cat_columns)\n\n    # seperate features from labels, replace labels with 0s and 1s\n    xtrain = train.drop(columns=['label'])\n    xtest = test.drop(columns=['label'])\n    ytrain = train['label'].replace([\" <=50K\", \" >50K\"], [0, 1])\n    ytest = test['label'].replace([\" <=50K\", \" >50K\"], [0, 1])\n\n    # predict result matrix\n    knn = KNeighborsClassifier(n_neighbors = k, metric = distance)\n    knn.fit(xtrain, ytrain)\n    result = knn.predict(xtest)\n\n    # create simple list of test labels\n    y = ytest.values.tolist()\n\n    # iterate over the results for fixed k value and increment counts for each metric of the confusion matrix\n    count_true_pos, count_false_pos, count_true_neg, count_false_neg = 0, 0, 0, 0\n    for j in range(len(result)):\n        if y[j] == 1:\n            if result[j] == y[j]:\n                count_true_pos += 1\n            else:\n                count_false_neg += 1\n        else:\n            if result[j] == y[j]:\n                count_true_neg += 1\n            else:\n                count_false_pos += 1\n                \n    # bin the counts\n    result_avgs[0] += count_true_pos\n    result_avgs[1] += count_false_pos\n    result_avgs[2] += count_true_neg\n    result_avgs[3] += count_false_neg\n\n    # print confusion matrix\n    print()     \n    print(f'CONFUSION MATRIX FOR K = {k}: ')\n    print(f'*********************************')\n    print(f'| TP = {round(result_avgs[0], 2)} \\t| FP = {round(result_avgs[1], 2)} \\t|')\n    print(f'*********************************')\n    print(f'| FN = {round(result_avgs[3], 2)} \\t| TN = {round(result_avgs[2], 2)} \\t|')\n    print(f'*********************************')\n    print(f'SCORE ON SAMPLE: {result_avgs[0] - result_avgs[1]}')\n    print(f'ACCURACY: {(result_avgs[0] + result_avgs[2])\/(result_avgs[0] + result_avgs[1] + result_avgs[3] + result_avgs[2])}', end='\\t')\n    print(f'ALPHA: {(result_avgs[1])\/(result_avgs[0] + result_avgs[1] + result_avgs[3] + result_avgs[2])}', end='\\t')\n    print(f'BETA: {(result_avgs[3])\/(result_avgs[0] + result_avgs[1] + result_avgs[3] + result_avgs[2])}')\n    print()","134c8bf6":"#Fitting model with raw data\nfit_sklearn_knn_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age', 'hrsperwk']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], score)","107e2ad1":"#Fitting model with log transformed data\nfit_sklearn_knn_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age', 'hrsperwk']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], score)","b57178a4":"#Fitting model with z-score centered data\nfit_sklearn_knn_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age_ZCentered', 'hrs_per_wk_ZCentered']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], score)","a451bd2d":"#Fitting model with log and z-score centered data\nfit_sklearn_knn_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age_ZCentered', 'hrs_per_wk_ZCentered']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], score)","16b51114":"# I'll test both the z-score only model and the log + z-score model because they're so close in cross validation.","427ab79d":"#Testing model with z-score centered data, k = 40\ntest_sklearn_knn(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age_ZCentered', 'hrs_per_wk_ZCentered']], \n                 holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age_ZCentered', 'hrs_per_wk_ZCentered']],\n                 ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], distance = 'euclidean', k = 40)","06087a77":"#Testing model with log and z-score centered data, k = 50\ntest_sklearn_knn(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age_ZCentered', 'hrs_per_wk_ZCentered']], \n                 holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age_ZCentered', 'hrs_per_wk_ZCentered']],\n                 ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], distance = 'euclidean', k = 50)","dfe8aed0":"#Fitting model with just categorical features\nfit_sklearn_knn_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], score, 'jaccard')","b5903f5b":"#Testing model with just categorical features, k = 70, distance = 'jaccard'\ntest_sklearn_knn(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass']], \n                 holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass']],\n                 ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 'jaccard', k = 70)","9a2ea4aa":"\"\"\"\nUtilizing RandomizedSearchCV's parallel processing to speed up the process of finding the optimal values of n_estimators, min_samples_split, and min_samples_leaf.\nFitting a Random Forest.\nMaximum number of available processors will be used.\n\ndata: pd dataframe, the features and labels\ncat_columns: array, names of categorical columns to create dummy encodings for\nrandom_seed_adder: int, value to be used in calculating random seed of train-test split\n\n\"\"\"\ndef fit_sklearn_rf_hyperparams(data, cat_columns, random_seed_adder, score_function):\n    # initialize scorer function, compatible with RandomizedSearchCV\n    my_scorer = make_scorer(score_function, greater_is_better=True)\n\n    data_rf_full = data\n    # create dummy encodings of categorical features\n    data_rf_full = pd.get_dummies(data_rf_full, columns=cat_columns)\n\n    # define the values of n_estimators, min_samples_split, min_samples_leaf to test\n    # n_estimators effects the bias of the model\n    # min_samples_split and min_samples_leaf mainly effect model variance\n    n_estimators = [100, 200, 500]\n    min_samples_split = [2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n    min_samples_leaf = [1, 2, 5, 10, 15, 20, 25, 30]\n    grid_param = {'n_estimators': n_estimators,\n                  'min_samples_split': min_samples_split,\n                  'min_samples_leaf': min_samples_leaf}\n    \n    # initialize model\n    model = rf(random_state=1)\n    # initialize randomized search with custom scoring function, using default number of folds (5)\n    RFC_random = RandomizedSearchCV(estimator = model, \n                                 param_distributions = grid_param,\n                                 n_iter = 100,\n                                 scoring = my_scorer,\n                                 verbose=2,\n                                 cv = 5,\n                                 random_state = random_seed_adder,\n                                 n_jobs = -1)\n    # begin tuning\n    RFC_random.fit(data_rf_full.drop(columns=['label']), data_rf_full['label'].replace([\" <=50K\", \" >50K\"], [0, 1]))\n    # print results\n    print(RFC_random.best_params_)\n    print(f'score: {RFC_random.best_score_}')","e6dceed9":"\"\"\"\nCompute and print the cost matrix for a single hyperparameter setting. Utilizes sklearn Random Forest.\n\ntrain: pd dataframe, training data\ntest: pd dataframe, testing data\ncat_columns: array, names of categorical columns to create dummy encodings for\nn_trees: int, number of trees to generate\nm_min: int, min number of samples required to split a node\nm_leave: int, min number of samples required to be at each leaf \n\n\"\"\"\ndef test_sklearn_rf(train, test, cat_columns, n_trees, m_min, m_leave):\n    # define number of expiriments to perform and initialize confusion matrix\n    result_avgs = [0, 0, 0, 0]\n\n    # create dummy encodings of categorical features\n    train = pd.get_dummies(train, columns=cat_columns)\n    test = pd.get_dummies(test, columns=cat_columns)\n\n    # seperate features from labels, replace labels with 0s and 1s\n    xtrain = train.drop(columns=['label'])\n    xtest = test.drop(columns=['label'])\n    ytrain = train['label'].replace([\" <=50K\", \" >50K\"], [0, 1])\n    ytest = test['label'].replace([\" <=50K\", \" >50K\"], [0, 1])\n\n    # initialize model and predict result matrix\n    rf_model = rf(n_estimators = n_trees, min_samples_split = m_min, min_samples_leaf = m_leave)\n    rf_model.fit(xtrain, ytrain)\n    result = rf_model.predict(xtest)\n\n    # create simple list of test labels\n    y = ytest.values.tolist()\n\n    # iterate over the results for fixed k value and increment counts for each metric of the confusion matrix\n    count_true_pos, count_false_pos, count_true_neg, count_false_neg = 0, 0, 0, 0\n    for j in range(len(result)):\n        if y[j] == 1:\n            if result[j] == y[j]:\n                count_true_pos += 1\n            else:\n                count_false_neg += 1\n        else:\n            if result[j] == y[j]:\n                count_true_neg += 1\n            else:\n                count_false_pos += 1\n                \n    # bin the counts\n    result_avgs[0] += count_true_pos\n    result_avgs[1] += count_false_pos\n    result_avgs[2] += count_true_neg\n    result_avgs[3] += count_false_neg\n\n    # print confusion matrix\n    print()     \n    print(f'CONFUSION MATRIX FOR n_estimators = {n_trees}, min_samples_split = {m_min}, min_samples_leaf = {m_leave}: ')\n    print(f'*********************************')\n    print(f'| TP = {round(result_avgs[0], 2)} \\t| FP = {round(result_avgs[1], 2)} \\t|')\n    print(f'*********************************')\n    print(f'| FN = {round(result_avgs[3], 2)} \\t| TN = {round(result_avgs[2], 2)} \\t|')\n    print(f'*********************************')\n    print(f'SCORE ON SAMPLE: {result_avgs[0] - result_avgs[1]}')\n    print(f'ACCURACY: {(result_avgs[0] + result_avgs[2])\/(result_avgs[0] + result_avgs[1] + result_avgs[3] + result_avgs[2])}', end='\\t')\n    print(f'ALPHA: {(result_avgs[1])\/(result_avgs[0] + result_avgs[1] + result_avgs[3] + result_avgs[2])}', end='\\t')\n    print(f'BETA: {(result_avgs[3])\/(result_avgs[0] + result_avgs[1] + result_avgs[3] + result_avgs[2])}')\n    print()","43314d02":"#Fitting model with raw data\nfit_sklearn_rf_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age', 'hrsperwk']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 654, score)","d29487b5":"#Fitting model with log transformed data\nfit_sklearn_rf_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age', 'hrsperwk']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 96846, score)","59733cb9":"#Fitting model with z-score centered data\nfit_sklearn_rf_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age_ZCentered', 'hrs_per_wk_ZCentered']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 496874, score)","508c46dc":"#Fitting model with log and z-score centered data\nfit_sklearn_rf_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age_ZCentered', 'hrs_per_wk_ZCentered']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 564, score)","e9b79482":"# Testing all because they're very close","a7b042dd":"#Testing model with raw data, n_estimators = 200, min_samples_split = 35, min_samples_leaf = 2\ntest_sklearn_rf(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age', 'hrsperwk']], \n                holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age', 'hrsperwk']],\n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 200, 35, 2)","7223ecb0":"#Testing model with log transformed data, n_estimators = 200, min_samples_split = 35, min_samples_leaf = 2\ntest_sklearn_rf(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age', 'hrsperwk']], \n                holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age', 'hrsperwk']],\n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 200, 35, 2)","6024c1b2":"#Testing model with z-score centered data, n_estimators = 500, min_samples_split = 35, min_samples_leaf = 2\ntest_sklearn_rf(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age_ZCentered', 'hrs_per_wk_ZCentered']], \n                holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age_ZCentered', 'hrs_per_wk_ZCentered']],\n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 500, 35, 2)","ff1c885d":"#Testing model with log and z-score centered data, n_estimators = 500, min_samples_split = 35, min_samples_leaf = 2\ntest_sklearn_rf(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age_ZCentered', 'hrs_per_wk_ZCentered']], \n                holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age_ZCentered', 'hrs_per_wk_ZCentered']],\n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 500, 35, 2)","39d3fe42":"\"\"\"\nUtilizing RandomizedSearchCV's parallel processing to speed up the process of finding the optimal values of n_estimators, min_samples_split, and min_samples_leaf.\nFitting a Gradient Boosting Classifier.\nMaximum number of available processors will be used.\n\ndata: pd dataframe, the features and labels\ncat_columns: array, names of categorical columns to create dummy encodings for\nrandom_seed_adder: int, value to be used in calculating random seed of train-test split\n\n\"\"\"\ndef fit_sklearn_GB_hyperparams(data, cat_columns, random_seed_adder, score_function):\n    # initialize scorer function, compatible with RandomizedSearchCV\n    my_scorer = make_scorer(score_function, greater_is_better=True)\n\n    # define the values of n_estimators, learning_rate, min_samples_split, min_samples_leaf to test\n    # n_estimators effects the bias of the model\n    # min_samples_split and min_samples_leaf mainly effect model variance\n    n_estimators = [100, 250, 500, 750, 1000, 1250]\n    learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3]\n    min_samples_split = [2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n    min_samples_leaf = [1, 2, 5, 10, 15, 20, 25, 30]\n    grid_param = {'n_estimators': n_estimators,\n                  'learning_rate': learning_rate,\n                  'min_samples_split': min_samples_split,\n                  'min_samples_leaf': min_samples_leaf}\n    \n    data_gb_full = data\n    # create dummy encodings of categorical features\n    data_gb_full = pd.get_dummies(data_gb_full, columns=cat_columns)\n\n    # initialize model\n    model = gb(random_state=1)\n    # initialize randomized search with custom scoring function, using default number of folds (5)\n    GB_random = RandomizedSearchCV(estimator = model, \n                                 param_distributions = grid_param,\n                                 scoring = my_scorer,\n                                 verbose=5,\n                                 cv = 5,\n                                 random_state = random_seed_adder,\n                                 n_jobs = -1)\n    # begin tuning\n    GB_random.fit(data_gb_full.drop(columns=['label']), data_gb_full['label'].replace([\" <=50K\", \" >50K\"], [0, 1]))\n    # print result\n    print(GB_random.best_params_)\n    print(f'score: {GB_random.best_score_}')","6bf26f33":"\"\"\"\nCompute and print the cost matrix for a single hyperparameter setting. Utilizes sklearn Gradient Boosting Classifier.\n\ntrain: pd dataframe, training data\ntest: pd dataframe, testing data\ncat_columns: array, names of categorical columns to create dummy encodings for\nn_trees: int, number of trees to generate\nlr: float, the learning rate\nm_min: int, min number of samples required to split a node\nm_leave: int, min number of samples required to be at each leaf \n\n\"\"\"\ndef test_sklearn_gb(train, test, cat_columns, n_trees, lr, m_min, m_leave):\n    # define number of expiriments to perform and initialize confusion matrix\n    result_avgs = [0, 0, 0, 0]\n\n    # create dummy encodings of categorical features\n    train = pd.get_dummies(train, columns=cat_columns)\n    test = pd.get_dummies(test, columns=cat_columns)\n\n    # seperate features from labels, replace labels with 0s and 1s\n    xtrain = train.drop(columns=['label'])\n    xtest = test.drop(columns=['label'])\n    ytrain = train['label'].replace([\" <=50K\", \" >50K\"], [0, 1])\n    ytest = test['label'].replace([\" <=50K\", \" >50K\"], [0, 1])\n\n    # initialize model and predict result matrix\n    gb_model = gb(n_estimators = n_trees, learning_rate = lr, min_samples_split = m_min, min_samples_leaf = m_leave)\n    gb_model.fit(xtrain, ytrain)\n    result = gb_model.predict(xtest)\n\n    # create simple list of test labels\n    y = ytest.values.tolist()\n\n    # iterate over the results for fixed k value and increment counts for each metric of the confusion matrix\n    count_true_pos, count_false_pos, count_true_neg, count_false_neg = 0, 0, 0, 0\n    for j in range(len(result)):\n        if y[j] == 1:\n            if result[j] == y[j]:\n                count_true_pos += 1\n            else:\n                count_false_neg += 1\n        else:\n            if result[j] == y[j]:\n                count_true_neg += 1\n            else:\n                count_false_pos += 1\n                \n    # bin the counts\n    result_avgs[0] += count_true_pos\n    result_avgs[1] += count_false_pos\n    result_avgs[2] += count_true_neg\n    result_avgs[3] += count_false_neg\n\n    # print confusion matrix\n    print()     \n    print(f'CONFUSION MATRIX FOR n_estimators = {n_trees}, learning_rate = {lr}, min_samples_split = {m_min}, min_samples_leaf = {m_leave}: ')\n    print(f'*********************************')\n    print(f'| TP = {round(result_avgs[0], 2)} \\t| FP = {round(result_avgs[1], 2)} \\t|')\n    print(f'*********************************')\n    print(f'| FN = {round(result_avgs[3], 2)} \\t| TN = {round(result_avgs[2], 2)} \\t|')\n    print(f'*********************************')\n    print(f'SCORE ON SAMPLE: {result_avgs[0] - result_avgs[1]}')\n    print(f'ACCURACY: {(result_avgs[0] + result_avgs[2])\/(result_avgs[0] + result_avgs[1] + result_avgs[3] + result_avgs[2])}', end='\\t')\n    print(f'ALPHA: {(result_avgs[1])\/(result_avgs[0] + result_avgs[1] + result_avgs[3] + result_avgs[2])}', end='\\t')\n    print(f'BETA: {(result_avgs[3])\/(result_avgs[0] + result_avgs[1] + result_avgs[3] + result_avgs[2])}')\n    print()","26a3e6c4":"#Fitting model with raw data\nfit_sklearn_GB_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age', 'hrsperwk']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 5416, score)","3ccc62df":"#Fitting model with log transformed data\nfit_sklearn_GB_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age', 'hrsperwk']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 6748, score)","18eb538c":"#Fitting model with z-score centered data\nfit_sklearn_GB_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age_ZCentered', 'hrs_per_wk_ZCentered']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 86784, score)","2e97883d":"#Fitting model with log and z-score centered data\nfit_sklearn_GB_hyperparams(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age_ZCentered', 'hrs_per_wk_ZCentered']], \n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 6874384, score)","a348613c":"# Testing all because they're very close","126601f9":"#Testing model with raw data, n_estimators = 750, learning_rate = 0.05, min_samples_split = 50, min_samples_leaf = 20\ntest_sklearn_gb(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age', 'hrsperwk']],\n                holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age', 'hrsperwk']],\n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 750, 0.05, 50, 20)","d26b80a0":"#Testing model with log transformed data, n_estimators = 500, learning_rate = 0.1, min_samples_split = 2, min_samples_leaf = 25\ntest_sklearn_gb(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age', 'hrsperwk']],\n                holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age', 'hrsperwk']],\n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 500, 0.1, 2, 25)","bc4c25ec":"#Testing model with z-score centered data, n_estimators = 1250, learning_rate = 0.05, min_samples_split = 40, min_samples_leaf = 3\ntest_sklearn_gb(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age_ZCentered', 'hrs_per_wk_ZCentered']],\n                holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'age_ZCentered', 'hrs_per_wk_ZCentered']],\n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 1250, 0.05, 40, 2)","54016c75":"#Testing model with log and z-score centered data, n_estimators = 500, learning_rate = 0.1, min_samples_split = 55, min_samples_leaf = 20\ntest_sklearn_gb(data_original[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age_ZCentered', 'hrs_per_wk_ZCentered']],\n                holdout[['label', 'cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass', 'log_age_ZCentered', 'hrs_per_wk_ZCentered']],\n                           ['cap-gains50k', 'sex', 'relationship', 'occupation', 'education', 'workclass'], 750, 0.05, 20, 30)","267411dc":"CONFUSION MATRIX FOR K = 50: \n*********************************\n| TP = 2201 \t| FP = 1059 \t|\n*********************************\n| FN = 1645 \t| TN = 11376 \t|\n*********************************\nSCORE ON SAMPLE: 1142\n\nACCURACY: 0.8339168355752103\n\nALPHA: 0.06504514464713469\n\nBETA: 0.10103801977765493","46e31341":"Features to look at here: Continuous[log_age_ZCentered, log_fnl_wgt_ZCentered, education_num_ZCentered, log_cap_gain_ZCentered, log_cap_loss_ZCentered, hrs_per_wk_ZCentered] ~ Categorical[workclass, education, marital-status, occupation, relationship, race, sex, cap-gains50k]\n\nI'll be skipping interactions with the feature (native) because of the high volume of factor levels. Plots will just be confusing with too many levels.\n\nThere will be 48 total pairs here to represent with plots so, again, I'll also omit any plots that appear to exhibit little interaction for the sake of brevity.","5cf592e4":"All individuals making capital gains greater than 50k end the year with the label >50k. (This is why I made this feature) Roughly 20% of individuals making less than 50k in capital gains still make >50k, so this won't be the only feature we need to predict the label.","1e00346e":"There's some interaction here. The percentage of individuals making > 50k is higher in all factor levels except for 'Private' and 'never-worked'. Could be useful. I would have expected private workers to have the advantage here. I don't know how this works . . .","aea4364e":"# ***Ranking of modelling results based on best score achieved***","d995d1a4":"#######################################################################################################################################################\n\n### **race**    \/Categorical. Factor levels include: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n             \nMeaning: The race of the individual.","4863eee2":"Hours worked per week also avaries noticable based on the marital-status of an individual. While all of these factor levels show a similar mean, the first and third quartiles are where we see the effect. Widowed individuals work less than the mean of all groups, likely due to being older on average and perhaps retired. Never married individuals also work less than the shared mean. (perhaps this contributes to why they were never married. Married individuals work slightly more hours than divorced inidividuals for unkown reasons.","53874257":"## 1. Gradient Boosting Classifier best performance for raw data: \n### label ~ cap-gains50k + sex + relationship + occupation + education + workclass + age + hrsperwk","7b3cd144":"#######################################################################################################################################################\n\n### **workclass**    \/Categorical. Factor levels include: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n                  \nMeaning: This feature explains the general category of the economy the individual works within.","4aa9fa38":"# Fitting and testing sklearn ~ Random Forest\nI'm choosing to test a random forest here because of the lack of assumptions for the model and it's general robustness to the imbalanced class problem. I think it will perform better than kNN.","cf6cc994":"This interaction exhibits something a lot of people might expect. Married individuals have a significantly hgiher percentage of cap-gains50k being True than False. Most likely due to two people being able to contribute more capital than one. Also, individuals never-married have significantly lower percentage of cap-gains50k being True than False. For the same reason as previous.","f847cd84":"#######################################################################################################################################################\n\n### **native**    \/Categorical. Factor Levels include: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n\nMeaning: The country the individual was born in.","ccf1674a":"## Comments on GBC performance:\n\nBest performance is for raw data:","ba51709c":"Here we see younger than average individuals mostly make up the never-worked factor level.","44e7c6f4":"#######################################################################################################################################################\n\n### **fnlwgt**    \/Continuous. \n\nMeaning: The feature \"fnlwgt\" was added by the dataset authors as a controlled estimate of certain socio-economic effects that take individual state population distributions into account.","dc883a68":"#######################################################################################################################################################\n\n# ***Defining model evaluation measures***","8a32a046":"# ***Business Understanding***\n\nRecorded originally by the US census bureau for the purpose of determining the correct number of House representatives per state via the 1990 census survey. Extracted by from the 1994 census database by Barry Becker under the following conditions: \"((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0)).\" Each record represents a single American Citizen that is > 16 years of age, has a difference between their Total income and Adjustments to income of > 100, has a fnlwgt attribute > 1, and works more than 0 hours per week.\n\nThe data set has been curated as a sample of the working population in the United States, for the purpose of predicting whether an individual makes > 50K per year. \n\nThe feature \"fnlwgt\" was added by the dataset authors as a controlled estimate of certain socio-economic effects that take individual state population distributions into account. The controls accounted for are:\n\n    | 1.  A single cell estimate of the population 16+ for each state.\n    | 2.  Controls for Hispanic Origin by age and sex.\n    | 3.  Controls by Race, age and sex.\n    \nOriginal data converted, by the data set authors, as follows:\n\n    | 1. Discretized agrossincome into two ranges with threshold 50,000.\n    | 2. Convert U.S. to US to avoid periods.\n    | 3. Convert Unknown to \"?\"\n    | 4. Run MLC++ GenCVFiles to generate data,test.\n    \nHere, numerical data (gross income) was simplified into binary categories of > 50K or <= 50K. MLC++ GenCVFiles is used to randomly split the data into training and tesing set for the purpose of ML application.\n\nI have edited the files adult.data and adult.test to include column names.","a7ee51c5":"# ***__Data Understanding & Data Processing__***","b335bc7b":"The distributions of the two final weight groups here are very similar. It's likely I wouldn't use this in a predictive model given that thte label was our target. ","eec06d9c":"Disregard the disparities here in the 'Husband' and 'wife' relationship designations, these are dependent on sex. The most interesting effect here is that women appear more likely to be an 'only child' or 'unmarried'.","9474eec7":"#######################################################################################################################################################\n\n### **occupation**    \/Categorical. Factor-levels include: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces\n                       \nMeaning: The field in which the individual works.","f8be36e0":"#######################################################################################################################################################\n\n### **sex**    \/Categorical. Factor levels include: Female, Male. \n\nMeaning: The sex of the individual.","157033b1":"## 2. Random Forest for z-score of log transformed data: \n### label ~ cap-gains50k + sex + relationship + occupation + education + workclass + log_age_ZCentered + hrs_per_wk_ZCentered","aadf8841":"Why does it appear that men are more likely to have capital gains greater than 50k than women? Are men more aggressive with investments on average? Is it due to men being more likely to be married? Other socio-economic factors?","3ac92522":"Here we can get a bit of a clue into how racial descriminators effect the value of the final weight feature. American-Indian-Exkimos have a slightly lower average than the other racial groups, but a much lower first quartile than the other racial groups. I still don't know how the authors calculated this statistic so I don't know why this is.","d660ebe8":"Age also effects marital-status. We see younger individuals making up the majority of 'never-married' individuals. We see older individuals making up the mojority of the 'Widowed' factor level.","88994b99":"The average age on individuals making less than 50k in capital gains is lower than the average age of those making more than 50k in capital gains. The variance of the less than 50k sample is also much wider than the other sample. This makes sense because older individuals have more access to capital and can achieve higher dollar value gains with the same percentage increase in portfolio value.","63bf4102":"## Now testing jaccard distance metric","362f9662":"Features to look at here [workclass, education, marital-status, occupation, relationship, race, sex, cap-gains50k]\n\nPairs of categorical variables don't have a pearson correlation coefficient.\n\nTo explore whether one categorical group effects the distribution of another, I'll be making interaction plots. The plots will consist of lines marking the change in\nfrequency from one categorical factor level to another. There will be a set of lines per factor level of the second categorical varaible.\nGenerally when the lines between the factor levels controlling the hue are approximatly paralell this means that the second variable is not really effecting the distribution \nof the first. Otherwise, there may be an interesting effect. In such cases I'll try to explain the interaction or raise questions regarding it.\n\nI'll be skipping interactions with the feature (native) because of the high volume of factor levels. This type of plot will just be confusing with too many levels.\n\nI'll also omit any plots that appear to exhibit little interaction for the sake of brevity.","52c72652":"This graph is somewhat interesting. Occupations on the right side of the x-axis appear very paralell, where occupations on the left side of the x-axis appear much more mixed. I don't really have any idea why this is occurring.","990cb24e":"#######################################################################################################################################################\n\n### **hrsperwk**    \/Continuous. \n\nMeaning: The number of hours an individual works per week.","935d3047":"It appears older individuals making > 50k are on average older than the average individual in the sample. Just as we've seen with the cap-gains50k distribution. This is likely due to older individuals being in more senior positions that pay more.","112392d4":"This interaction shows a lot of the same variation that we saw in the occupation ~ marital-status interation. I'm beleiving more and more that marital-status and relationship are a bit redundant.","d677c16c":"Here we see how some occupations simply require more time than others, especially farming for example. We all know the common trope of the farmer waking up at the crack of dawn to manage the fields. Is this what the data is showing us?","d2909f3e":"I've kept all of the features of the original data set in order to preserve the information available when we begin modelling the data.\n\nFor each numeric feature I created a new transformed feature using either the log, the z-score, and\/or the s-zcore and log. Some models fail (especially those using gradient descent) if some features have much larger numbers than others. The features: 'age', 'fnlwgt', 'cap_gain', and 'cap_loss' were all transformed using the log function before they were scaled by the Z-score.\n\nI created one new feature, 'cap-gain50k' which is a binary categorical feature. Its value is True if the individual made more than 50k in capital gains and False otherwise.\n\nNone of the outliers that I've observed appeared erroneous so I've left them as is. \n\nMissing values within a categorical feature were replaced with the mode value. Missing values within a numeric feature were replaced with the mean value.","fc8a9ac1":"The distribution of education years completed appears to change noticably when factored by race. All of the education distributions have similar a mean (likely around graduating from HS) but the thrid quartiles for whites and asians are higher than the remaining races. The other category has a lower first quartile than the remaining distributions. There are outliers in all categories. This effecty is likely due to some socio-economic factors within these communities.","8df78bb1":"I believe these measures will be best because we'll be able to emphasize amount of correct classifications, false positives, and false negatives.\n\nBased on the EDA I conducted above, I believe the best model to use for predicting our label will be based on the following variables:\n\ncap-gains50k, sex, relationship, occupation, education, workclass, age, and hrsperwk\n\nThese variables have stronger interaction with the label than the remaining variables. I won't be training any models witht he independent features assumption so I'm not terribly worried about the effect of multipcollinearity, but, you may notice I'm not using both the education and education-num features. I'll test a variation of the model for each kind of numeric transformation I created in the pre-processing phase to see which data works best. ","4c6704b6":"Again, we see greater financial succes with married individuals. Joint income is powerful. We also see a higher frequency of <= 50k individuals under the 'Never-married' factor level.","11f77913":"CONFUSION MATRIX FOR K = 50: \n*********************************\n| TP = 2201 \t| FP = 1059 \t|\n*********************************\n| FN = 1645 \t| TN = 11376 \t|\n*********************************\nSCORE ON SAMPLE: 1142\nACCURACY: 0.8339168355752103\tALPHA: 0.06504514464713469\tBETA: 0.10103801977765493","ebb43461":"Here's something interesting, some variation in frequency occuring in the rates of 'cap-gains50k' based on economic sector. Seeing greater frequency of 'cap-gains50k' for individuals that are self employed. This could result from these individuals making less consistent pay than the other factor levels of workclass, thus taking more risk in equities. Or it could also be the result of these individuals making more money than others, due to having a greater share of profit. Higher principle risked on average provides higher return in markets.","8b5570c5":"This interaction isn't all that interesting. I only left it in because of one small detail. The levels of relationship 'Husband' and 'wife' are almost perfectly matched to the factor level 'Married-civ-spouse' in marital-status. These levels essentially describe the same thing and we may want to exclude one the two features from our final model.","bb0718e8":"The average, first & third quartiles, and tails of the two capital gains groups here are all basically 0. This is why I made a categorical feature off of this numeric feature. The distribution of the > 50k outlier appears more concentrated higher than the <= 50k outliers. ","4741fbec":"The most interesting variation between these 'relationship' groups is that husband work on average more hours per week than wives. Is this due to the effect of child rearing? The 'own-child' factor level shows lower hours worked per week than the everage of the rest of the groups. Is this because most of these individuals are younger?","e1c8cfcd":"#######################################################################################################################################################\n\n### **Education**    \/Categorical. Factor levels include: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n                  \nMeaning: How much schooling the individual has completed.","4843ddc0":"Here we see a bit of variation between the hours worked per week within different economic sectors. Self employed individuals work more than average and 'without-pay' + 'Never-worked' individuals work much less than average. The remaining groups have similar distributions with many outliers. ","297b5bb9":"#######################################################################################################################################################\n\n### **education-num**    \/continuous. \n\nMeaning: The number of years of education completed by and individual.","fde7c46c":"College educated individuals and Prof-school individuals appear to work more hours than the average when compared to all other groups. Is this because these groups are on average tackling more complex problems? Is this because these groups are in positions requiring more responsibility, and thus, more time?","f7397d31":"#######################################################################################################################################################\n\n### **age**    \/Continuous. \n\nMeaning: the integer value age of the individual","7bee26d3":"This graph is almost identical to the relationship 'sex ~ cap-gains50k'. We, again, see men being more likely to make more than 50k than women. Likely, attributable to the effect of child rearing on average.","3ebe2a70":"#######################################################################################################################################################\n\n### **cap-gain**    \/Continuous. \n\nMeaning: Dollars gained by the individual's investments during the year.","7f36d31c":"Those who work more hours per week appear more likely to achieve grater than 50k of capital gains. Is this because longer hours worked achieves higher pay and more access to capital?","2c2d302c":"There's a lot of variation here. In advanced degrees such as prof-school, bachelors, masters, and doctorate individuals have a higher frequency of cap-gains50k being True than False. All other education levels are the opposite. This could be because these degree holders have a hgiher access to capital than the others, due to having better job oppotunities. ","67da6969":"# Fitting and testing sklearn ~ Gradient Boosting Classifier\nI chose to test a Gradient Boosting Classifier because they're generally even better than random forests for unbalanced class problems. ","0f03a24b":"CONFUSION MATRIX FOR n_estimators = 750, learning_rate = 0.05, min_samples_split = 50, min_samples_leaf = 20: \n*********************************\n| TP = 2226 \t| FP = 922 \t|\n*********************************\n| FN = 1620 \t| TN = 11513 \t|\n*********************************\nSCORE ON SAMPLE: 1304\n\nACCURACY: 0.8438670843314293\n\nALPHA: 0.05663042810638167\t\n\nBETA: 0.09950248756218906","10b02e80":"Individuals making > 50k have higher than average years of education completed. The mean of the <= 50k group appears almost and entire standard deviation lower and the variances of the two groups appear significantly different.","e297bb1f":"#######################################################################################################################################################\n\n### **relationship**    \/Categorical. Factor Values: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n                     \nMeaning: The type of relationship the individual is in.","03851570":"Very similar distributions in capital loss as with capital gain. We see higher magnitude gain\/loss in the >50k column because of greater dollar value gain on the same percent increase of a likely larger thasn average pool of capital. ","80e21e9d":"#######################################################################################################################################################\n\n### **Investigating explainable variation between non-label categorical features**","08395063":"Here we see that 'husband' and 'wife' individuals are more likely to have capital gains greater than 50k. Likely, a repeat of the effect we saw earlier that I attributed to married individuals having reater access to capital on average.","9044b6aa":"Years of education completed has a noticable effect on the variation of the 'occupation' feature. Some jobs have higher barriers to entry than others.","d1bd3084":"# Fitting and testing sklearn ~ knn\n","e386db03":"This interactions displays the idea that men and women often tend to work in different fields. Especially so in the 'adm-clerical' and 'craft-repair' factor levels. The court is still out on whether this is due to socially constructed entry barriers or that men and women tend to prefer focusing on different thing on average.","ee8747cf":"#######################################################################################################################################################\n\n### **capital-loss**    \/Continuous. \n\nMeaning: Dollars lost by the individual's investments during the year.","c81160f3":"The > 50k group here shows a higher average number of hours worked per week than the <= 50k group. Both distributions have pretty tight quartiles and have many outliers. ","82865b9e":"Here we see that more educated individuals are much more likely than less educated individuals to achieve greater than 50k in capital gains. Probably due to more educated individuals on average being more specialized and paid more.","b92d8c5e":"Age effects the degree of education an individual has completed. Somewhat a given, but worth noting.","e8660d38":"Here, our data seems to imply that American men are much more likely to be married than american women. Also, women have a higher percentage of diverce than men. Perhaps the two are connected? Perhaps not? Women are also more liekly to have never been married which may also contribute.","6450f01b":"CONFUSION MATRIX FOR n_estimators = 750, learning_rate = 0.05, min_samples_split = 50, min_samples_leaf = 20: \n*********************************\n| TP = 2226 \t| FP = 922 \t|\n*********************************\n| FN = 1620 \t| TN = 11513 \t|\n*********************************\nSCORE ON SAMPLE: 1304\nACCURACY: 0.8438670843314293\tALPHA: 0.05663042810638167\tBETA: 0.09950248756218906","1645ed50":"Occupation levels provide a lot of variation in frequency regarding our label. The two best off occupation levels are 'Prof-specialty' and 'Exec-managerial'. Likely due to higher than average pay.","dc700539":"CONFUSION MATRIX FOR n_estimators = 500, min_samples_split = 35, min_samples_leaf = 2: \n*********************************\n| TP = 2186 \t| FP = 927 \t|\n*********************************\n| FN = 1660 \t| TN = 11508 \t|\n*********************************\nSCORE ON SAMPLE: 1259\n\nACCURACY: 0.8411031263435906\n\nALPHA: 0.056937534549474846\n\nBETA: 0.10195933910693446","15035d82":"#######################################################################################################################################################\n\n### **marital-status**    \/Categorical. Factor-levels include: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse\n                       \nMeaning: Whether the individual is married or divorced, etc. Interesting because houses with two income sources will be different than one.","6c46275b":"#######################################################################################################################################################\n\n# ***Summary of findings after EDA***","e5bbbe31":"## 3. sklearn KNN (Euclidean) for z-score of log transformed data:\n### label ~ cap-gains50k + sex + relationship + occupation + education + workclass + log_age_ZCentered + hrs_per_wk_ZCentered","33e49068":"#######################################################################################################################################################\n\n### **Investigating explainable variation between non-label categorical features and numerical features**","dfa011d2":"Individuals that're Husband or wives, again, appear better off. Liekly another redundant effec that we've seen under the 'marital-status ~ label' interaction.","f62632d5":"CONFUSION MATRIX FOR K = 70: \n*********************************\n| TP = 2040 \t| FP = 971 \t|\n*********************************\n| FN = 1806 \t| TN = 11464 \t|\n*********************************\nSCORE ON SAMPLE: 1069\n\nACCURACY: 0.82943308150605\n\nALPHA: 0.0596400712486948\n\nBETA: 0.1109268472452552","87aaf72b":"Mostly less educated individuals appear to make up the 'never-worked' factor level. As we have already attributed younger individuals to the never worked category and to the lower education category, we're likely seeing the same effect here.","f0687edb":"## Comments on Random Forest model performance :\n\nBest performance is for z-score of log transformed data: ","1f85b489":"Something interesting is definitly happening here. Race appears to effect whether an individual is a husband or Unmarried. The 'wife' designation doesn't show this effect much. All of the wife observations are very close to eachother racially. Yet, 'husband' and 'unmarried' have wider variation and an almost preserved ordering, suggesting a lack of 'black' 'husband' individuals may influence a gain in 'black' 'unmarried' individuals.","53713e4a":"Again, we're seeing that men on average work more hours than women. Again, is this because of the effect of child rearing on the average?","3530a72a":"The interaction between education and workclass shows a higher percentage of 'HS-grad' individuals in the occupation 'Without-pay' than all the other 'HS-grad' individuals.\nThere also appear to be more individuals than average who have completed HS or less that are 'Without-pay' or 'Never-worked' than other education groups.","7bc48159":"#######################################################################################################################################################\n\n### **label**:     \/Categorical. Factor levels include: > 50K, <= 50K\n\nMeaning: whether the indiviual makes more or less than 50K per year","00366127":"#######################################################################################################################################################\n\n### **Checking for numeric feature correlation**","0c32a288":"This interaction doesn't appear to be especially interesting for all races except black individuals, whom unfortunatly have a higher frequency of making less than 50k. The remaining racial groups are all very similar.","288353c8":"Our label distributions are pretty unbalanced. So, accuracy alone won't be a good measure of how our model is performing.\nI think it'll best best to utilize a Score function, Confusion matrix, accuracy, and our type 1 and 2 error rates.\nS.T.:","33bc35c2":"## 4. sklearn KNN (Jaccard) only categorical model features: \n### label ~ cap-gains50k + sex + relationship + occupation + education + workclass","621efc18":"## Best kNN Euclidiean distance metric model:\n\nBest performance is for z-score of log transformed data:","9385b19b":"CONFUSION MATRIX: \n*****************************************\n| TP = TP_count   \t| FP = FP_count \t|\n*****************************************\n| FN = FN_count   \t| TN = TN_count \t|\n*****************************************\nSCORE ON SAMPLE: TP_count - FP_count \nACCURACY: (TP_count + TN_count) \/ all_count\t\nALPHA: (FP_count) \/ all_count\t\nBETA: (FN_count) \/ all_count","10bf369f":"There is a lot of variation here within these groups. This may be explained by some occupations being available at different frequencies within different economic sectors. One of which, that is rather extreme, is the armed forces only being a part of the federal government.","5b4cb964":"CONFUSION MATRIX FOR n_estimators = 500, min_samples_split = 35, min_samples_leaf = 2: \n*********************************\n| TP = 2186 \t| FP = 927 \t|\n*********************************\n| FN = 1660 \t| TN = 11508 \t|\n*********************************\nSCORE ON SAMPLE: 1259\nACCURACY: 0.8411031263435906\tALPHA: 0.056937534549474846\tBETA: 0.10195933910693446","b39fa2a4":"Here we see that occupation likely has a lot of influence over whether a person makes capital gains greater than 50k in a year. Occupations with the largest disparity between cap-gains50k being True or False are 'Exec-managerial' and 'Prof-specialty'.","2fd3611f":"#######################################################################################################################################################\n\n### **Investigating explainable variation between the label and predictor features**\n\nHere, we'll be investigating the relationships between the label and [log_age_ZCentered, log_fnl_wgt_ZCentered, education_num_ZCentered, log_cap_gain_ZCentered, log_cap_loss_ZCentered, hrs_per_wk_ZCentered, workclass, education, marital-status, occupation, relationship, race, sex, cap-gains50k]\n\nI'll be using a combination of the same plots from the last two sections.\n\nI won't omit any plot from this section because there are only 14 of them.","61add118":"Again, similar to the interaction between education ~ cap-gains50k, we see advanced degrees such as 'Prof-school', 'Bachelors', 'Masters', & 'Doctorate' having the advantage here. Pretty significant difference in the proportions at these levels."}}