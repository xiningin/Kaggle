{"cell_type":{"689cd495":"code","58f0a748":"code","43c62604":"code","cef6aea6":"code","50db427e":"code","c2e736b2":"code","f36d84c9":"code","6ce3cba7":"code","ee632aec":"code","0c915b5e":"code","0af6070b":"code","3827a2d8":"code","ebb599b8":"code","e5a7e62d":"code","63afa0b6":"code","d4954011":"code","9e41b9ef":"code","1f8ae674":"markdown","e22c4095":"markdown","e2316264":"markdown","e10caf62":"markdown","9758104d":"markdown","20772e77":"markdown","f27b52ce":"markdown","25b64e04":"markdown","335c06b5":"markdown","1bd081ae":"markdown","3dd0e54c":"markdown","5e9e5b9c":"markdown","405426b3":"markdown","bc767669":"markdown"},"source":{"689cd495":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","58f0a748":"fb = pd.read_csv('..\/input\/fb_sentiment.csv')","43c62604":"fb.head()","cef6aea6":"# lower-casing the coloumn names\nfb.columns = map(str.lower, fb.columns)","50db427e":"# checkin the shape of the DF\nfb.shape","c2e736b2":"#lowercasing the text and removing symbols though RegEx\nimport re\nfb['fbpost'] = fb['fbpost'].apply(lambda x: x.lower())\nfb['fbpost'] = fb['fbpost'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))","f36d84c9":"fb = fb[fb.label != \"O\"]","6ce3cba7":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(fb['fbpost'].values)\nX = tokenizer.texts_to_sequences(fb['fbpost'].values)\nX = pad_sequences(X)","ee632aec":"fb.label.value_counts()","0c915b5e":"Y = pd.get_dummies(fb['label']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","0af6070b":"embed_dim = 200\nlstm_out = 200\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","3827a2d8":"# Here we train the model\nbatch_size = 32\nhist = model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)","ebb599b8":"#Plotting a histogram over the 7 epocs and plotting the accuracy and loss\nhistory = pd.DataFrame(hist.history)\nplt.figure(figsize=(7,7));\nplt.plot(history[\"loss\"]);\nplt.plot(history[\"acc\"]);\nplt.title(\"Loss and accuracy of model\");\nplt.show();","e5a7e62d":"#Testing the model, and retrieveing score and accuracy:\nscore,acc = model.evaluate(X_test,Y_test)\nprint(\"score: %.2f\" % (score))\nprint(\"accuracy: %.2f\" % (acc))\n","63afa0b6":"#now we validate for the models accuracy in predicting either a positive, or a negative score:\nvalidation_size = 1500\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nx_test = X_test[:-validation_size]\ny_test = Y_test[:-validation_size]","d4954011":"pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    result = model.predict(X_validate[x].reshape(1,x_test.shape[1]),verbose = 2)[0]\n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n    if np.argmax(Y_validate[x]) == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\nprint(\"positive_acc\", pos_correct\/pos_cnt*100, \"%\")\nprint(\"negative_acc\", neg_correct\/neg_cnt*100, \"%\")","9e41b9ef":"#now testing  on a random sample from the Facebook comments on Kindle's page:\ncmnt = ['your customer service is the absolute worst i now have a mess of books on my kindle']\n#vectorizing the comment\ncmnt = tokenizer.texts_to_sequences(cmnt)\ncmnt = pad_sequences(cmnt, maxlen=203, dtype='int32', value=0)\nprint(cmnt)\nsentiment = model.predict(cmnt,batch_size=2,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","1f8ae674":"# Facebook comments Sentiment analysis","e22c4095":"## Exploring Data","e2316264":"The data that we have chosen are Facebook-comments (`FBPost`) , with sentiments (`Label`) rated *positive*, *negative* or *other*. There are rows in the dataset, with an uneven distribution of sentiments.\n\nIn this section, some minor cleaning will take place.","e10caf62":"## The Neural Network","9758104d":"Here the lables are checked after the removel of the \"other\" sentimented comments. Also some preperation to the algorithm, as preparing the test-, and training sets are done.","20772e77":"## Data preperation","f27b52ce":"### Preparing the Facebook comments","25b64e04":"What went wrong? \n\nOur sample size for the training-, and test set was in fact quite small (under 1000 comments), and the proportion of negative to positive comments was skewed about 1 to 9. So the algorithm was not really optimized on the basis of our data. This is the reason for the algorithm choosing the wrong sentiment in the sample-test above.","335c06b5":"In this section, the algorithm is prepared with following features:\n\n- The model is Sequential\n- The model type is an LSTM model","1bd081ae":"The aim of this notebook is to train and test a Neural Network to detect, if a Facebook comment is either positive or negative in nature, based on a sample of Facebook comments with attatched sentiment ratings.","3dd0e54c":"### Preparing the labels","5e9e5b9c":"Now to tokenize the actual Facebook comments:","405426b3":"## Loading packages","bc767669":"With this Neural network, we want to predict, wether a comment is Positive (`P`) or Negative (`N`), so the comments with the sentiment labeled Other (`O`) is of no use to us, so it's removed from the dataset."}}