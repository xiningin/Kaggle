{"cell_type":{"efa6e252":"code","c2ab6536":"code","7daa25b9":"code","d86630a4":"code","0afe7b30":"code","dd6f5174":"code","23626f45":"code","0520b0be":"code","37274248":"code","6313ee20":"code","e0f86c58":"code","3d68e68f":"code","46bc4fba":"code","bab65387":"code","5501e558":"code","97dc4912":"code","ebd72a17":"code","436645b5":"code","d8aa18f7":"code","6574b959":"code","4e98b44b":"code","8877f69d":"code","48574455":"code","1ca29d3d":"markdown","2f057d7b":"markdown","09651ab5":"markdown","918b9d23":"markdown","87c984de":"markdown","60eade08":"markdown","4495efd0":"markdown","fd521789":"markdown","eb01aa6a":"markdown","0bd2fe34":"markdown","f96956c2":"markdown","404ef702":"markdown","e8b8dfb2":"markdown","4b7d13fe":"markdown","b63d4adb":"markdown","b352b8e7":"markdown","87f58278":"markdown","87d88137":"markdown","473c11f1":"markdown","e90dd7d8":"markdown","18821a9d":"markdown","65da4092":"markdown"},"source":{"efa6e252":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n    print('and then re-execute this cell.')\nelse:\n    print(gpu_info)","c2ab6536":"class CFG:\n    \n    n_splits = 5 \n    \n    fold_id = 0 # Fold to train\n\n    image_size = 512 \n    seed = 42\n    init_lr = 5e-4\n    batch_size = 64\n    valid_batch_size = 64\n    n_epochs = 15\n    num_workers = 8\n\n    use_amp = True  \n    early_stop = 5\n    \n    AGC = False # Adaptive Gradient Clipping\n    optimizer = 'Ranger' #Ranger, SAM, Adam\n\n    model_name = 'eca_nfnet_l0'\n    train_dir = '..\/input\/trainfolds\/train_folds.csv'\n    data_dir = '..\/input\/ranzor-clip-resized-data-512-256\/trainXray_512'\n    \n    target_cols = ['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', \n           'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal',\n           'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present']\n    \nmodel_dir = f'weights\/'\n! mkdir $model_dir","7daa25b9":"import sys\nsys.path.append('..\/input\/nfnets\/pytorch-image-models-master')\nimport timm\nfrom timm.utils.agc import adaptive_clip_grad\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\nimport math\n\nimport os\nimport time\nimport cv2\nimport PIL.Image\nimport random\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import CosineAnnealingLR \nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport gc\nfrom sklearn.metrics import roc_auc_score\n\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nplt.style.use('ggplot')","d86630a4":"def seed_everything(seed):\n    \n    \"\"\"Seeding everything for consistent experiments...\"\"\"\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(42)","0afe7b30":"# Loading metadata\n\ntrain_df = pd.read_csv(CFG.train_dir)","dd6f5174":"# Counting target values.\n\ntarg_cts=train_df.iloc[:,1:-2].sum(axis=0)\nfig = plt.figure(figsize=(12,6))\nsns.barplot(y=targ_cts.sort_values(ascending=False).index, x=targ_cts.sort_values(ascending=False).values, palette='inferno')\nplt.show()","23626f45":"# Labels per sample.\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.iloc[:,1:-2].columns.values[1:]\nplt.title('Total Target Score Counts', weight='bold')\nsns.countplot(train_df[features].sum(axis=1), palette='inferno')\nplt.xlabel('Total Number of Targets per Sample')\nplt.legend()\nplt.show()\n\n","0520b0be":"# counting patient id's\n\nplt.figure(figsize=(20,5))\npat_counts=train_df.PatientID.value_counts().reset_index().iloc[:25,:]\npat_counts.columns=['PatientID','Count']\nsns.barplot(x='PatientID', y='Count',data=pat_counts, palette='inferno')\nplt.xticks(rotation=60)\nplt.show()","37274248":"# Checking fold target distributions\n\nfold_dist=train_df.groupby('fold').sum().reset_index()\nfig, axs = plt.subplots(1, 11, figsize=(40,4))\nfor i,j in enumerate(fold_dist.columns[1:].tolist()):\n\n   axs[i].bar(x=fold_dist.fold, height=fold_dist.loc[:,j])\n   axs[i].set_title(j)\n\nplt.show()","6313ee20":"class NFNetModel(nn.Module):\n    \n    \"Main nfnet model class. Where we load pretrained imagenet weights for it and customizing head layer for our case in competition.\"\n\n    def __init__(self, num_classes=len(CFG.target_cols), model_name=CFG.model_name, pretrained=False):\n        super(NFNetModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        #if pretrained:\n        #    pretrained_path = f'..\/input\/nfnet-pretrained-weights\/{model_name}.pth'\n        #    self.model.load_state_dict(torch.load(pretrained_path))\n            \n        self.model.head.fc = nn.Linear(self.model.head.fc.in_features, num_classes)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","e0f86c58":"# applying some augmentations for regularizing effect\n\ntransforms_train = A.Compose([\n   A.RandomResizedCrop(CFG.image_size, CFG.image_size, scale=(0.95, 1), p=1), \n   A.HorizontalFlip(p=0.5),\n   #A.ShiftScaleRotate(rotate_limit=(-8, 8),p=0.5),\n   A.HueSaturationValue(hue_shift_limit=(-10, 10), sat_shift_limit=(-10, 10), val_shift_limit=(-10, 10), p=0.5),\n   A.RandomBrightnessContrast(always_apply=False, brightness_limit=(-0.05, 0.05), contrast_limit=(-0.05, 0.05), brightness_by_max=True, p=0.7),\n   A.CLAHE(clip_limit=(1,4), p=0.25),\n\n  A.Cutout(max_h_size=int(CFG.image_size * 0.05), max_w_size=int(CFG.image_size * 0.05), num_holes=12, p=0.5),\n  A.Normalize(\n         mean=[0.485, 0.456, 0.406],\n         std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n         ToTensorV2(p=1.0)\n])\n\ntransforms_valid = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(\n         mean=[0.485, 0.456, 0.406],\n         std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n    ToTensorV2(p=1.0)\n])","3d68e68f":"class TrainDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.file_names = df['StudyInstanceUID'].values\n        self.labels = df[CFG.target_cols].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.file_names[idx]\n        file_path = f'{CFG.data_dir}\/{file_name}.jpg'\n        image = cv2.imread(file_path)        \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        label = torch.tensor(self.labels[idx]).float()\n        return image, label","46bc4fba":"# loading the images with augmentations\n\ntrain_dataset = TrainDataset(train_df, transform=transforms_train)\n\nfig, axs = plt.subplots(1, 5, figsize=(40,12))\n\nfor i in range(5):\n    image, label = train_dataset[i]\n    axs[i].imshow(image[0])\n    axs[i].title.set_text(f'Target Labels: {label}')\n\nplt.show() ","bab65387":"def train_func(train_loader):\n    \n    \"\"\" Main training function: Takes loaded images to predict labels, computes losses between predicted and training labels, clip gradients, return updated losses. \"\"\"\n    \n    model.train()\n    bar = tqdm(train_loader)\n    if CFG.use_amp:\n        scaler = torch.cuda.amp.GradScaler()\n    losses = []\n    scores = []\n    for batch_idx, (images, targets) in enumerate(bar):\n\n        images, targets = images.to(device), targets.to(device)\n        \n        if CFG.use_amp:\n            if CFG.optimizer =='SAM':\n                with torch.cuda.amp.autocast():\n                    preds_first = model(images)\n                    loss = trn_criterion(preds_first, targets)\n\n                scaler.scale(loss).backward()   \n                if CFG.AGC:\n                    adaptive_clip_grad(model.parameters(), clip_factor=0.01, eps=1e-3, norm_type=2.0)\n                optimizer.first_step(zero_grad=True)\n\n                with torch.cuda.amp.autocast():\n                    preds_second = model(images)\n                    loss_second = trn_criterion(preds_second, targets)\n\n                scaler.scale(loss_second).backward()\n                if CFG.AGC:\n                    adaptive_clip_grad(model.parameters(), clip_factor=0.01, eps=1e-3, norm_type=2.0)\n                optimizer.second_step(zero_grad=True)\n                if not CFG.AGC:\n                    scaler.update()\n            else:\n                with torch.cuda.amp.autocast():\n                    preds = model(images)\n                    loss = trn_criterion(preds, targets)\n                    scaler.scale(loss).backward()\n                    if CFG.AGC:\n                        adaptive_clip_grad(model.parameters(), clip_factor=0.01, eps=1e-3, norm_type=2.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n                \n            \n            \n\n            \n        else:\n            output = model(images)\n            loss = trn_criterion(output, targets)\n            loss.backward()\n            if CFG.AGC:\n                adaptive_clip_grad(model.parameters(), clip_factor=0.01, eps=1e-3, norm_type=2.0)\n            optimizer.step()\n            optimizer.zero_grad()\n\n\n        losses.append(loss.item())\n        \n\n        bar.set_description(f'Mean Loss: {np.mean(losses):.5f}')\n\n    loss_train = np.mean(losses)\n    return loss_train\n\n\ndef valid_func(valid_loader):\n    \n    \"\"\" Main validation function: Takes loaded images to predict labels, computes losses between predicted and valid labels, clip gradients, return updated losses. \"\"\"\n    \n    \n    model.eval()\n    bar = tqdm(valid_loader)\n\n    PROB = []\n    TARGETS = []\n    losses = []\n    PREDS = []\n    \n    with torch.no_grad():\n        for batch_idx, (images, targets) in enumerate(bar):\n\n            images, targets = images.to(device), targets.to(device)\n            output = model(images)\n            PREDS += [output.sigmoid()]\n            TARGETS += [targets.detach().cpu()]\n            loss = val_criterion(output, targets)\n            losses.append(loss.item())\n            bar.set_description(f'Loss: {loss.item():.5f}')\n            \n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n    roc_auc = macro_multilabel_auc(TARGETS, PREDS)\n    loss_valid = np.mean(losses)\n    return loss_valid, roc_auc","5501e558":"def macro_multilabel_auc(label, pred):\n    \n    \"\"\"A function recevies couple columns as inputs, predicts AUC score column-wise and returns mean scores.\"\"\"\n    \n    aucs = []\n    for i in range(len(CFG.target_cols)):\n        aucs.append(roc_auc_score(label[:, i], pred[:, i]))\n    print(np.round(aucs, 4))\n    return np.mean(aucs)","97dc4912":"class SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n        defaults = dict(rho=rho, **kwargs)\n        super(SAM, self).__init__(params, defaults)\n\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group[\"rho\"] \/ (grad_norm + 1e-12)\n\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                e_w = p.grad * scale.to(p)\n                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n                self.state[p][\"e_w\"] = e_w\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n\n        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n\n        self.first_step(zero_grad=True)\n        closure()\n        self.second_step()\n\n    def _grad_norm(self):\n        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n        norm = torch.norm(\n                    torch.stack([\n                        p.grad.norm(p=2).to(shared_device)\n                        for group in self.param_groups for p in group[\"params\"]\n                        if p.grad is not None\n                    ]),\n                    p=2\n               )\n        return norm","ebd72a17":"def centralized_gradient(x, use_gc=True, gc_conv_only=False):\n    '''credit - https:\/\/github.com\/Yonghongwei\/Gradient-Centralization '''\n    if use_gc:\n        if gc_conv_only:\n            if len(list(x.size())) > 3:\n                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n        else:\n            if len(list(x.size())) > 1:\n                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n    return x\n\n\nclass Ranger(Optimizer):\n\n    def __init__(self, params, lr=1e-3,                       # lr\n                 alpha=0.5, k=5, N_sma_threshhold=5,           # Ranger options\n                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n                 use_gc=True, gc_conv_only=False, gc_loc=True\n                 ):\n\n        # parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        if not lr > 0:\n            raise ValueError(f'Invalid Learning Rate: {lr}')\n        if not eps > 0:\n            raise ValueError(f'Invalid eps: {eps}')\n\n        # parameter comments:\n        # beta1 (momentum) of .95 seems to work better than .90...\n        # N_sma_threshold of 5 seems better in testing than 4.\n        # In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n\n        # prep defaults and init torch.optim base\n        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n        # adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        # look ahead params\n\n        self.alpha = alpha\n        self.k = k\n\n        # radam buffer for state\n        self.radam_buffer = [[None, None, None] for ind in range(10)]\n\n        # gc on or off\n        self.gc_loc = gc_loc\n        self.use_gc = use_gc\n        self.gc_conv_only = gc_conv_only\n        # level of gradient centralization\n        #self.gc_gradient_threshold = 3 if gc_conv_only else 1\n\n        print(\n            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\")\n        if (self.use_gc and self.gc_conv_only == False):\n            print(f\"GC applied to both conv and fc layers\")\n        elif (self.use_gc and self.gc_conv_only == True):\n            print(f\"GC applied to conv layers only\")\n\n    def __setstate__(self, state):\n        print(\"set state called\")\n        super(Ranger, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n        # note - below is commented out b\/c I have other work that passes back the loss as a float, and thus not a callable closure.\n        # Uncomment if you need to use the actual closure...\n\n        # if closure is not None:\n        #loss = closure()\n\n        # Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        'Ranger optimizer does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]  # get state dict for this param\n\n                if len(state) == 0:  # if first time to run...init dictionary with our desired entries\n                    # if self.first_run_check==0:\n                    # self.first_run_check=1\n                    #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n\n                    # look ahead weight storage now in state dict\n                    state['slow_buffer'] = torch.empty_like(p.data)\n                    state['slow_buffer'].copy_(p.data)\n\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n                        p_data_fp32)\n\n                # begin computations\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                # GC operation for Conv layers and FC layers\n                # if grad.dim() > self.gc_gradient_threshold:\n                #    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n                if self.gc_loc:\n                    grad = centralized_gradient(grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n\n                state['step'] += 1\n\n                # compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                # compute mean moving avg\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                buffered = self.radam_buffer[int(state['step'] % 10)]\n\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * \\\n                        state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (\n                            N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    else:\n                        step_size = 1.0 \/ (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                # if group['weight_decay'] != 0:\n                #    p_data_fp32.add_(-group['weight_decay']\n                #                     * group['lr'], p_data_fp32)\n\n                # apply lr\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    G_grad = exp_avg \/ denom\n                else:\n                    G_grad = exp_avg\n\n                if group['weight_decay'] != 0:\n                    G_grad.add_(p_data_fp32, alpha=group['weight_decay'])\n                # GC operation\n                if self.gc_loc == False:\n                    G_grad = centralized_gradient(G_grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n\n                p_data_fp32.add_(G_grad, alpha=-step_size * group['lr'])\n                p.data.copy_(p_data_fp32)\n\n                # integrated look ahead...\n                # we do it at the param level instead of group level\n                if state['step'] % group['k'] == 0:\n                    # get access to slow param tensor\n                    slow_p = state['slow_buffer']\n                    # (fast weights - slow weights) * alpha\n                    slow_p.add_(p.data - slow_p, alpha=self.alpha)\n                    # copy interpolated weights to RAdam param tensor\n                    p.data.copy_(slow_p)\n\n        return loss","436645b5":"class Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https:\/\/github.com\/tyunist\/memory_efficient_mish_swish\/blob\/master\/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1.\/h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1.\/v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)","d8aa18f7":"def replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n\n    return model","6574b959":"# loading the model\n\nmodel = NFNetModel(pretrained=True)\nif CFG.optimizer == 'Ranger':\n    model = replace_activations(model, nn.ReLU, Mish())\nmodel = model.to(device)","4e98b44b":"# setting criterions, optimizers, folds to train etc.\n\nval_criterion = nn.BCEWithLogitsLoss()\ntrn_criterion = nn.BCEWithLogitsLoss()\n\n# for sam optimizer you can change the base optimizer to get better results\n\nif CFG.optimizer == 'SAM':\n    base_optimizer = torch.optim.Adam\n    optimizer = SAM(model.parameters(), base_optimizer, lr=CFG.init_lr)\nif CFG.optimizer == 'Ranger':\n    optimizer = Ranger(model.parameters(), CFG.init_lr)\nif CFG.optimizer == 'Adam':\n    optimizer = torch.optim.Adam(model.parameters(),lr=CFG.init_lr)\n    \n\n    \n# here you can experiment with other schedulers too, they have decent impact on this competition\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, CFG.n_epochs, eta_min=1e-7)\n\n\ntrain_df_this = train_df[train_df['fold'] != CFG.fold_id]\ndf_valid_this = train_df[train_df['fold'] == CFG.fold_id]\n\ndataset_train = TrainDataset(train_df_this, transform=transforms_train)\ndataset_valid = TrainDataset(df_valid_this, transform=transforms_valid)\n\ntrain_loader = torch.utils.data.DataLoader(dataset_train, batch_size=CFG.batch_size, shuffle=True,  num_workers=CFG.num_workers, pin_memory=True)\nvalid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=CFG.valid_batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)","8877f69d":"# visualizing the learning rate\n\nlrs = []\n\nmodelt = torch.nn.Linear(2, 1)\n\noptimizert = torch.optim.Adam(modelt.parameters(),lr=CFG.init_lr)\n\nschedulert = torch.optim.lr_scheduler.CosineAnnealingLR(optimizert, CFG.n_epochs, eta_min=1e-7)\n\nfor i in range(CFG.n_epochs):\n    optimizert.step()\n    lrs.append(optimizert.param_groups[0][\"lr\"])\n    schedulert.step()\nplt.title('Learning Rate over Epochs')\nplt.plot(lrs)\nplt.show()","48574455":"# single fold training\n\nlog = {}\nroc_auc_max = 0.\nloss_min = 99999\nnot_improving = 0\n\n\nfor epoch in range(1, CFG.n_epochs+1):\n    \n    \n    loss_train = train_func(train_loader)\n    loss_valid, roc_auc = valid_func(valid_loader)\n\n    log['loss_train'] = log.get('loss_train', []) + [loss_train]\n    log['loss_valid'] = log.get('loss_valid', []) + [loss_valid]\n    log['lr'] = log.get('lr', []) + [optimizer.param_groups[0][\"lr\"]]\n    log['roc_auc'] = log.get('roc_auc', []) + [roc_auc]\n\n    content = time.ctime() + ' ' + f'Fold: {CFG.fold_id}, Epoch: {epoch}\/{CFG.n_epochs}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, loss_train: {loss_train:.5f}, loss_valid: {loss_valid:.5f}, roc_auc: {roc_auc:.6f}.'\n    print(content)\n    not_improving += 1\n    \n    scheduler.step()\n    \n    if roc_auc > roc_auc_max:\n        print(f'roc_auc_max ({roc_auc_max:.6f} --> {roc_auc:.6f}). Saving model ...')\n        torch.save(model.state_dict(), f'{model_dir}{CFG.model_name}_fold{CFG.fold_id}_best_AUC.pth')\n        roc_auc_max = roc_auc\n        not_improving = 0\n\n    if loss_valid < loss_min:\n        loss_min = loss_valid\n        torch.save(model.state_dict(), f'{model_dir}{CFG.model_name}_fold{CFG.fold_id}_best_loss.pth')\n        \n    if not_improving == CFG.early_stop:\n        print('Early Stopping...')\n        break\n        \n\n\ntorch.save(model.state_dict(), f'{model_dir}{CFG.model_name}_fold{CFG.fold_id}_final.pth')","1ca29d3d":"\nAfter stratifying our samples by patient and their labels we can say that we have healthy target distribution between folds now...","2f057d7b":"# Loading Libraries","09651ab5":"# Training Single Fold","918b9d23":"### Learning Rates\n\nOn official paper of NFNet's authors used gradient warming for 5 epochs and then cosine decay to 0 next epochs. In my personal experiments it didn't give better results so we only use cosine annealing here...","87c984de":"# High-Performance Large-Scale Image Recognition Without Normalization\n\n![](https:\/\/pbs.twimg.com\/media\/EuB11mwXEAAaG_T.png:large)\n\nGoogle's DeepMind recently published their latest CNN network called Normalization Free Networks (NFNets). Paper claims state of the art imagenet results and faster speed on their tests. So in this notebook I'm going to give it a try with some changes on the problem this competition gives us, hope you find it useful!\n\nBefore I start I wanted to thank sin for his great notebook [here](https:\/\/www.kaggle.com\/underwearfitting\/single-fold-training-of-resnet200d-lb0-965). I took it as a starting point for this notebook; go check it out. Anyways let's get started!\n\n## Main Points of NFNets:\n*From the paper itself:*\n\n* Authors propose **Adaptive Gradient Clipping (AGC)**, which clips gradients based on the unit-wise ratio of gradient norms to parameter norms, and we demonstrate that AGC allows them to train Normalizer-Free Networks with larger batch sizes and stronger data augmentations.\n\n* The authors have designed a family of **Normalizer-Free ResNets, called NFNets**, which set new state-of-the-art validation accuracies on ImageNet for a range of training latencies(See the Image above). The NFNet-F1 model achieves similar accuracy to EfficientNet-B7 while being 8.7\u00d7 faster to train, and largest model sets a new overall state of the art without extra data of 86.5% top-1 accuracy.\n\n* The authors have show that NFNets achieve substantially higher validation accuracies than batch-normalized networks when fine-tuning on ImageNet after pre-training on a large private dataset of 300 million labelled images. The best model achieves 89.2% top-1 after fine-tuning\n\n## Some Notes About Notebook:\n\n* I've changed some settings to make this work in Kaggle's GPU hours. So this is kinda **light version**.\n* This is only **single fold** training because of same reasons above.\n* You can play with config to your liking, to get better results.\n* If you want to switch to SAM optimizer **expect 50%+ longer training times**, since it does forward-backward twice.\n* Again for SAM optimizer, don't forget to reduce lr for better results.\n* AGC is optional but if you getting nan losses you might want to enable it.\n* You can train for other folds as well and get the weights for inference notebooks. CV\/LB balance was solid in my trials...\n\n### If you have any further questions leave a comment and **if you liked this work please give an upvote**, thanks!","60eade08":"## Train Function\n\nHere we set our main training function. Key points are we implement AGC the Adaptive Gradient Clipping where authors used on the official paper. Then we do two forward-backward functions because SAM needs two forward-backward passes to estime the \"sharpness-aware\" gradient. It slows our training but that's the optimizer where authors used on their paper.","4495efd0":"## Ranger Optimizer\n\nRanger is a synergistic optimizer combining RAdam (Rectified Adam) and LookAhead. I choose this one over SAM on this notebook for timing purposes.","fd521789":"# Metadata & Exploratory Data Analysis\n\nHere we try to find some insights from data metadata given to us for getting better scores and undserstanding problem better.","eb01aa6a":"We observe that most of the samples having only one label per examination. But we also see decent part of the samples having several labels at the same time, making our problem a **Multi-Label Classification**...\n\nI'm not an expert on the subject but this might be caused by several catheters connected to patient at the same time...","0bd2fe34":"### Here we set our train and validation criterions (They both same here for now but adds flexibility for future uses). Our baseline optimizer, SAM optimizer, schedulers etc...","f96956c2":"# Utils","404ef702":"# Summary\n\nOn this notebook I wanted to try latest state of the art(*) CNN models: Normalization Free Networks (NFNets) with some other promising additions. I hope you enjoyed reading it as much as I did while creating this content.\n\nAgain if you enjoyed this work please leave an upvote, thanks!","e8b8dfb2":"### We replace our default activation functions to Mish since it was recommended on Ranger implementation.","4b7d13fe":"## Target Labels\n\nLet's check our target distribution to see if there's a common pattern. We observe that labels are not that balanced, while CVC - Normal beign dominant class, ETT - Abnormal sits at the bottom beign rarest one.","b63d4adb":"# Checking Device","b352b8e7":"# NFNet Pretrained Model\n\nHere we define our CNN model using pretrained ImageNet weights.","87f58278":"## Sharpness-Aware Minimization for Efficiently Improving Generalization\n\nSAM Optimizer implementations is taken from [here.](https:\/\/github.com\/davda54\/sam) \n\n> SAM simultaneously minimizes loss value and loss sharpness. In particular, it seeks parameters that lie in neighborhoods having uniformly low loss. SAM improves model generalization and yields SoTA performance for several datasets. Additionally, it provides robustness to label noise on par with that provided by SoTA procedures that specifically target learning with noisy labels.\n\n\n![](https:\/\/raw.githubusercontent.com\/davda54\/sam\/main\/img\/loss_landscape.png)\n*ResNet loss landscape at the end of training with and without SAM. Sharpness-aware updates lead to a significantly wider minimum, which then leads to better generalization properties.*\n\n\nYou can find about it more here on the official paper:\nhttps:\/\/arxiv.org\/abs\/2010.01412","87d88137":"## Patient ID's:\n\nAfter inspecting patient id's closely we notice there are some patients with high number of chest x-ray samples in our training set. This might lead data leakage in our cross validation if we don't stratify them accordingly.\n","473c11f1":"## Train Loader\n\nThis is the class for loading our images and indexes them together with metadata for the training.","e90dd7d8":"# Configuration\n\nHere's our command center, we can try some different settings to get better results, hope you find some and share back to community!","18821a9d":"## Checking the Images","65da4092":"# Augmentations\n\nOn paper authors showed that using heavy augmentations leading better results. Here we do some mid level augmentations just in case..."}}