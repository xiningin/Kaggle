{"cell_type":{"26e40645":"code","2c52559a":"code","5b2a71b9":"markdown","1c85c4a2":"markdown"},"source":{"26e40645":"import sys\nsys.path.append('..\/input\/flight-revenue-simulator\/')\nfrom flight_revenue_simulator import simulate_revenue, score_me\nprint('ok')","2c52559a":"# This is my machine learning version of this challenge\n# Try to do RL from scratch similar to Krapathy Pong from Pixels\n\nimport sys\nimport numpy as np\nimport pickle as pickle\n\nsys.path.append('..\/input\/flight-revenue-simulator\/')\nfrom flight_revenue_simulator import simulate_revenue, score_me\n\nD         = 3     # Dimensions of input\nH         = 92    # Number of hidden nodes\nNORMAL    = 200.0 # Normalization Variable\nRB        = 175.0 # Baseline reward ## Was 100 --- what is the right number????\n\nMAX_PRICE  = 200\nf_training = False\nresume     = True\n\n# Build or load the model\nif resume:\n    model = pickle.load(open('..\/input\/50runs\/save.p', 'rb'))\nelse:\n    model = {}\n    model['W1'] = np.random.randn(H,D) \/ np.sqrt(D) # Xavier Initialization\n    model['W2'] = np.random.randn(H) \/ np.sqrt(H)\n\ngrad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # Gradient update buffers to add\nrmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rms prop memory\n\ndef sigmoid(x):\n    return 1.0 \/ (1.0 + np.exp(-x))\n\ndef policy_fwd(x):\n    h = np.dot(model['W1'], x) # hidden layer\n    h[h<0] = 0 # Relu -- consider changing the non-linearity\n    logp = np.dot(model['W2'], h)\n    p = sigmoid(logp) # output non-linearity --> probability\n    return p, h\n\ndef policy_backward():\n    # uses all global variables \n    global eph, epdlogp, epx, model\n    \n    dW2 = np.dot(eph.T, epdlogp).ravel() # backprop through output layer -- does this ignore the sigmoid?\n    dh = np.outer(epdlogp, model['W2'])\n    dh[eph<=0] = 0\n    dW1 = np.dot(dh.T, epx)\n    return {'W1':dW1, 'W2':dW2}\n\ndef pricing_function(days_left, tickets_left, demand_level):\n    # use the training flag to know if we are training or now\n    # if we are training, sample a space near the desired output; otherwise, get model prediction\n    global f_training, xs, hs, days_to_sell,dlogps\n    # Generate x with proper normalization to keep matrices happy\n    x = [days_left\/NORMAL, tickets_left\/NORMAL, demand_level\/NORMAL]\n    if(f_training):\n        p, h = policy_fwd(x)\n        xs.append(x) # store input states\n        hs.append(h) # store hidden states\n        days_to_sell += 1  # This is really counting the number of states\n        y = np.random.normal(p,0.05) # generate a random number near desired output of the model\n                                     # This is designed to help explore the space\n        # The following is from Karpathy code, it may or may not make sense for this application since this\n        # not a classifier.  \n        # Quick exploration of y-p function\n        #     y is a scaled version of actual price set\n        #     p is the target price that the model is suggesting\n        # y-p approaches zero if the sampeled number is the target of the network (no updates will happen)\n        # y-p > 0 if actual price is greater thant he model target price\n        # y-p < 0 if the model target price is greater than the actual price\n        #\n        # TODO: Come back later and make sure the signs make sense, then possibly re-evaluate if this is\n        #       the right function to use        \n        dlogps.append(y-p)\n        price = y * MAX_PRICE\n    else:\n        p, _ = policy_fwd(x)\n        price = p * MAX_PRICE\n    return price\n\ndef model_test(n):\n    # n - how many times to run before averaging\n    \n    # store current value of the training flag\n    global f_training\n    f_t = f_training\n    f_training = False\n    \n    # Variables to store total tickets sold and dollars made\n    t_total = 0\n    s_total = 0\n    \n    # iterate through forward prop\n    for i in range(n):\n        d = np.random.randint(1,high=NORMAL) # Generate a random number of days\n        t = np.random.randint(1,high=NORMAL) # Generate a random number of tickets remaining    \n        s = simulate_revenue(days_left=d, tickets_left=t, pricing_function=pricing_function, verbose=False)\n        \n        s_total += s\n        t_total += t\n        \n    average_ticket_price = (1.0*s_total)\/(1.0*t_total)\n    \n    # reset the training flag\n    f_training = f_t\n    return average_ticket_price\n\ndef model_train():\n    global f_training\n    f_t = f_training\n    f_training = True\n    \n    # TODO: Consider taking samples from randome distributions biased towards 0\n    \n    d = np.random.randint(1,high=NORMAL) # Generate a random number of days\n    t = np.random.randint(1,high=NORMAL) # Generate a random number of tickets remaining\n        \n    # Get a total revenue for random input vector x\n    a = simulate_revenue(days_left=d, tickets_left=t, pricing_function=pricing_function, verbose=False)\n        \n    # Intial try at reward fuction\n    # Calculate the average ticket price based on total revenue\n    # This is the value we will try to maximize\n    \n    # Initially, we will assign global reward per 'simulate_revenue' and apply it evenly to every day\n    # TODO: Consider outputing actual daily reward and using that in the pricing funciton if this doesn't work\n    r = (a\/t-RB)\/RB\n    \n    f_training = f_t\n    return r\n\n\n## Start of code\nbatches = 50\nbatch_size = 10\ngames_per_batch = 5000\n#learning_rate_max = 0.01\nlearning_rate = 0.005\ndecay_rate = 0.99\nepisode_number = 0\nbatch_counter = 50\nmodel_check_iterations = 200\n\nprint('Setting a baseline...')\nscore_me(pricing_function)\n\nfor b in range(batches*batch_size):\n\n    xs,hs,rs,dlogps = [],[],[],[]\n    \n    # At the beginning of each set of experiments, get a baseline of model\n    average_ticket_price = model_test(model_check_iterations)\n    print('MODEL TEST: --> Average Ticket Price over %d iterations is: $%0.2f' % (model_check_iterations, average_ticket_price))\n\n    for g in range(games_per_batch):\n        days_to_sell = 0 # need a counter to count the number of states output by the model\n        r = model_train()\n        d = days_to_sell\n\n        for j in range(d):\n            rs.append(r)\n\n    episode_number += 1\n    \n    # Stack the states for easy math\n    epr = np.vstack(rs)\n    eph = np.vstack(hs)\n    epx = np.vstack(xs)\n    epdlogp = np.vstack(dlogps)\n\n    epdlogp *= epr  # Multiply dlogp * reward function (PG Magic???)\n    \n    grad = policy_backward()\n    for k in model: grad_buffer[k] += grad[k]\n    \n    #lrate = learning_rate_min+np.exp(-batch_counter\/25.0)*learning_rate_max #exponential decay of learning rate\n    \n    # Training math from Karpathy directly\n    if(episode_number % batch_size == 0):\n        for k,v in model.items():\n            g = grad_buffer[k]\n            rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1- decay_rate) * g**2\n            model[k] += learning_rate * g \/ (np.sqrt(rmsprop_cache[k])+1e-5) # gradient ascent\n            grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n            \n        batch_counter += 1\n        print('Batch %d complete: Updating Model with a learning rate of %f' % (batch_counter, learning_rate))\n        score_me(pricing_function)\n        pickle.dump(model, open(('save.p.%d' % batch_counter), 'wb')) # \n\n#score_me(pricing_function)\n\npickle.dump(model, open('save.p', 'wb'))","5b2a71b9":"# Intro\n\nData scientists tend to focus on **prediction** because that's where conventional machine learning excels. But real world decision-making involves both prediction and **optimization**.  After predicting what will happen, you decide what to do about it.\n\nOptimization gets less attention than it deserves. So this micro-challenge will test your optimization skills as you write a function to improve how airlines set prices.\n\n![Imgur](https:\/\/i.imgur.com\/AKrbLMR.jpg)\n\n\n# The Problem\n\nYou recently started Aviato.com, a startup that helps airlines set ticket prices. \n\nAviato's success will depend on a function called `pricing_function`.  This notebook already includes a very simple version of `pricing_function`.  You will modify `pricing_function` to maximize the total revenue collected for all flights in our simulated environment.\n\nFor each flight, `pricing_function` will be run once per (simulated) day to set that day's ticket price. The seats you don't sell today will be available to sell tomorrow, unless the flight leaves that day.\n\nYour `pricing_function` is run for one flight at a time, and it takes following inputs:\n- **Number of days until the flight**\n- **Number of seats they have left to sell**\n- **A variable called `demand_level` that determines how many tickets you can sell at any given price. **\n\nThe quantity you sell at any price is:\n> quantity_sold = demand_level - price\n\nTicket quantities are capped at the number of seats available.\n\nYour function will output the ticket price.\n\nYou learn the `demand_level` for each day at the time you need to make predictions for that day. For all days in the future, you only know `demand_level` will be drawn from the uniform distribution between 100 and 200.  So, for any day in the future, it is equally likely to be each value between 100 and 200.\n\nIn case this is still unclear, some relevant implementation code is shown below.\n\n# The Simulator\nWe will run your pricing function in a simulator to test how well it performs on a range of flight situations.  **Run the following code cell to set up your simulation environment:**","1c85c4a2":"In case you want to check your understanding of the simulator logic, here is a simplified version of some of the key logic (leaving out the code that prints your progress). If you feel you understand the description above, you can skip reading this code.\n\n```\ndef _tickets_sold(p, demand_level, max_qty):\n        quantity_demanded = floor(max(0, p - demand_level))\n        return min(quantity_demanded, max_qty)\n\ndef simulate_revenue(days_left, tickets_left, pricing_function, rev_to_date=0, demand_level_min=100, demand_level_max=200):\n    if (days_left == 0) or (tickets_left == 0):\n        return rev_to_date\n    else:\n        demand_level = uniform(demand_level_min, demand_level_max)\n        p = pricing_function(days_left, tickets_left, demand_level)\n        q = _tickets_sold(demand_level, p, tickets_left)\n        return _total_revenue(days_left = days_left-1, \n                              tickets_left = tickets_left-q, \n                              pricing_function = pricing_function, \n                              rev_to_date = rev_to_date + p * q,\n                              demand_level_min = demand_level_min,\n                              demand_level_max = demand_level_max\n                             )\n```\n\n# Your Code\n\nHere is starter code for the pricing function.  If you use this function, you will sell 10 tickets each day (until you run out of tickets)."}}