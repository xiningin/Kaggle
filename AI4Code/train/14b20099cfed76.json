{"cell_type":{"22775513":"code","f54988b4":"code","2679ade4":"code","a4b2abfd":"code","56cb27eb":"code","0a2357d6":"code","41cbff34":"code","e9928c2d":"code","9833b043":"markdown","7e8a07f8":"markdown","39afa06b":"markdown","6d96c4de":"markdown","cc5b1814":"markdown","94e4b587":"markdown","4108910c":"markdown"},"source":{"22775513":"#push all the imports here\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\n\n","f54988b4":"#analyze the data\ndf = pd.read_csv(\"..\/input\/covid19-in-india\/IndividualDetails.csv\")\n\n#plenty of null values are there\ndf.isnull()\n\n#dropping some unimportant columns\ndf = df.drop(columns=[\"government_id\", \"detected_city\", \"detected_state\", \"nationality\", \"status_change_date\", \"notes\", \"diagnosed_date\", \"detected_district\"], axis=1)\n\ndf = df.drop(df.index[924:])\n\n#So drop pretty much all the NaN rows\ndf = df.dropna()\n\n\n#>>>>>>>>>>>>>>>>>this set is unnecessary because the data is having huge number of NaN\n# #convert the string dates into datetime values\n# df[\"diagnosed_date\"] = pd.to_datetime(df[\"diagnosed_date\"],format='%d\/%m\/%Y')\n# df[\"status_change_date\"] = pd.to_datetime(df[\"status_change_date\"],format='%d\/%m\/%Y')\n\n\n# survived_for = df[\"status_change_date\"] - df[\"diagnosed_date\"]\n\n# #add the survived for time\n# df[\"survived_for\"] = df.apply(lambda row: row[\"status_change_date\"] - row[\"diagnosed_date\"], axis=1)\n\n#>>>>>>>>>>>>>>>>>>>>>>>unused data processing\n\n#Recovered or Hospitalized is 0, deceased is 1\ndf['deceased'] = df.apply(lambda row: 0 if row.current_status == 'Recovered' or 'Hospitalized' else 1, axis = 1)\n\n\n#Male is 0, Female 1\ndf['gender_binary'] = df.apply(lambda row: 0 if row.gender == 'M' else 1, axis = 1)\n\ndf['age'] = df.apply(lambda row: pd.to_numeric(row.age), axis=1)\n\ndf.count()\n\ndf.head()","2679ade4":"#Split data\nmodel_data = df[[\"age\", \"gender_binary\"]]\nmodel_target = df[[\"deceased\"]]\n\nx_train,x_test, y_train, y_test = train_test_split(model_data, model_target, test_size=0.33)\n\nscaler = StandardScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\n\nN, D = x_train.shape\n\nplt.plot(y_train) #this is why this model will fail","a4b2abfd":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Input(shape=(D,)))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer = 'adam',\n             loss = 'binary_crossentropy',\n             metrics=['accuracy'])\n","56cb27eb":"history = model.fit(x_train, y_train, validation_data=(x_train, y_train), epochs = 500)\n","0a2357d6":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend()","41cbff34":"new_data = [[10,1],\n            [50,1],\n            [80,1]]\n\nmodel.predict(new_data)","e9928c2d":"# save the model\nmodel.save('covid19_mortality_india_ANN_failed_model.h5')","9833b043":"**So basically this model is of no use. Either I stripped off too much data or it's fully biased. Anyway. Will try some other time.**","7e8a07f8":"Plot some learning curves and stuff","39afa06b":"Make the Model","6d96c4de":"This is a elemental notebook where a classification algo is being applied on the covid19-in-india dataset to find out human mortality chances.","cc5b1814":"Try predicting\n","94e4b587":"Do some regular splitting and normalizing","4108910c":"Training"}}