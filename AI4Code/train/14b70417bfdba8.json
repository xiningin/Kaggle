{"cell_type":{"3b97672a":"code","9bbe02be":"code","1daa981d":"code","f81ec6ac":"code","3a70f7b8":"code","67f8a3ef":"code","b131f011":"code","581536c2":"code","f4fb2602":"code","63417ddf":"code","9fd02b2d":"code","cad243a6":"code","ca98ab1f":"code","481f731d":"markdown","1c9b8e9f":"markdown","267e1477":"markdown","f374a215":"markdown","bfcef995":"markdown","b8b13147":"markdown","828e2ff3":"markdown","7b4e9aa7":"markdown","cc29a9fc":"markdown","6891b2d7":"markdown","5f22ea36":"markdown","92c2c9e0":"markdown","5991e62c":"markdown","20ddc254":"markdown","d856a8f8":"markdown","2106f8c7":"markdown","62075ea2":"markdown","c1c00f4a":"markdown","06d1abab":"markdown"},"source":{"3b97672a":"# General libs\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import fmin\nfrom functools import partial\n# XGB lib\nimport xgboost as xgb\n# sklearn\nfrom sklearn import ensemble\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn import neighbors\nfrom sklearn import gaussian_process\nfrom sklearn import tree\nfrom sklearn import neural_network\nfrom sklearn import model_selection\nfrom sklearn import metrics\n# Additional libs\nimport warnings\nwarnings.filterwarnings('ignore')","9bbe02be":"# Read the data\nall_data = pd.read_csv('..\/input\/fraud-detection-bank-dataset-20k-records-binary\/fraud_detection_bank_dataset.csv', index_col=0)\nprint(f'The number of features columns is {len(all_data.columns) - 1}')\nprint()\nall_data.head(3)","1daa981d":"plt.title('Target variable distribution')\nsns.countplot(all_data['targets'])\nplt.show();\nprint('Normal transactions are dominated -> it is expected')","f81ec6ac":"# reset index, and replace with current data\nall_data.reset_index(drop=True, inplace=True)\n\n# split on Train \/ Valid part 10%\nTrain, Valid = model_selection.train_test_split(all_data, test_size=0.1, random_state=11)\n\n# split on Train, Test DO TO 0.5 not\nX_train, X_test, y_train, y_test = model_selection.train_test_split(Train.drop('targets', axis=1), Train.targets.values, test_size=0.5, stratify=Train.targets.values, random_state=11)\n\nprint(Train.shape, Valid.shape)\nassert X_train.shape[0] + X_test.shape[0] + Valid.shape[0] == all_data.shape[0]","3a70f7b8":"# List of the models to train \nmodel_list = [linear_model.LogisticRegression(solver='liblinear', random_state=11),\n              neighbors.KNeighborsClassifier(),\n              linear_model.SGDClassifier(loss='log', random_state=11),\n              tree.DecisionTreeClassifier(random_state=11),\n              ensemble.GradientBoostingClassifier(random_state=11),\n              ensemble.RandomForestClassifier(random_state=11),\n              neural_network.MLPClassifier(random_state=11),\n              xgb.XGBClassifier(use_label_encoder=False, verbosity=0, random_state=11)\n#               svm.SVC(probability=True),  # too slow, but you can try! \n#               gaussian_process.GaussianProcessClassifier(), # too slow, but you can try!              \n             ]","67f8a3ef":"class OptimizeRocAucScore:\n    \n    def __init__(self):\n        self.coef_ = 0 # set the coeff 0 at the beggining\n        \n    def calculate_negative_roc_auc(self, coef, X, y):\n        # predict the values by multiply all X by coeff \n        predictions = np.sum(X * coef, axis=1)\n        # calculate auc score for each prediction\n        auc_score = metrics.roc_auc_score(y, predictions)\n        # return negative roc_auc to use fmin (i.g. finding max of the function)\n        return -1.0 * auc_score\n    \n    def fit(self, X, y):\n        # use partial function to add new arguments, more here https:\/\/docs.python.org\/3\/library\/functools.html#functools.partial\n        loss_partial = partial(self.calculate_negative_roc_auc, X=X, y=y)\n        # set how values  are distributed (could be normal dist -> initial_coef = np.random.normal(np.ones(X.shape[1])))\n        initial_coef = np.random.dirichlet(np.ones(X.shape[1]), size=1)\n        # use scipy fmin to minimize the loss function (roc_auc)\n        self.coef_ = fmin(loss_partial, initial_coef, disp=True)\n        \n    def predict(self, X):\n        # predict in the same way \n        return np.sum(X * self.coef_, axis=1)","b131f011":"# let have a function \ndef funct_test(x):\n    return x ** 2 + x * 6 - 10\n\n# set the interval of x\ninterval = range(-12, 8)\n\n# find value there `funct_test` has local minimun. Second argument of fmin for our initial guess (could be wrong)\nfmin_result = fmin(funct_test, -1)\nprint()\nplt.rcParams[\"figure.figsize\"] = (6,6)\nplt.title(f'This fucntion has local minima when `x` is {fmin_result[0]} and `y` equals to {funct_test(fmin_result[0])}')\nplt.plot([i for i in interval], [funct_test(i) for i in interval])\nplt.plot(fmin_result[0], funct_test(fmin_result[0]), marker='o', markersize=5, color=\"red\")\nplt.show();","581536c2":"number_ = 15\n\nplt.rcParams[\"figure.figsize\"] = (6,6)\nplt.title('Different distributions over randome variable ')\nplt.plot(np.random.dirichlet(np.ones(number_)), linewidth=3, label='dirichlet')\nplt.plot(np.random.normal(np.ones(number_)),label='normal')\nplt.plot(np.random.exponential(np.ones(number_)),label='exponential')\nplt.plot(np.random.gamma(np.ones(number_)),label='gamma')\nplt.plot(np.random.poisson(np.ones(number_)),label='poisson')\nplt.plot(np.random.uniform(np.ones(number_)),label='uniform')\nplt.legend()\nplt.show();","f4fb2602":"def calculate_models_predictions(X_train, X_test, y_train, y_test, model_list, print_=False):\n    ''' Calculate predicitons (avg probabilities) for each model and return a staked array '''\n    avg_pred_ = np.zeros(len(X_test))\n    all_predictions = []\n    # iterate each model, collect result and divide by the number of models (we have a simple average)\n    for index, model in enumerate(model_list):\n        pred = model.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n        # add predictions of each model to as 1d array to list.\n        all_predictions.append(pred)\n        # calculate roc_auc score for each prediction \n        auc = metrics.roc_auc_score(y_test, pred)\n        # sum up prediction to calculate later average\n        avg_pred_ += pred\n        if print_:\n            print(f' model # {index} is {str(model_list[index])} and has roc_auc {auc}')\n            print()    \n    # average all predictions (we have n models, i.g. all_predictions \/ n)\n    avg_pred = avg_pred_ \/ len(model_list)\n    # add prediction as as stacked array -> each column is model results, each raw is features\n    all_predictions_staked = np.column_stack((all_predictions))\n    # tests\n    assert all_predictions_staked.shape == (len(X_test), len(model_list))\n    assert len(avg_pred) == len(y_test)\n    if print_:\n        print()\n        print(f' The average roc_auc of all models is {metrics.roc_auc_score(y_test, avg_pred)}')\n        \n    return all_predictions_staked\n\nall_predictions_staked_train = calculate_models_predictions(X_train, X_test, y_train, y_test, model_list, print_=True)","63417ddf":"# Get the predcitions for test part\nall_predictions_staked_test = calculate_models_predictions(X_test, X_train, y_test, y_train, model_list, print_=False)","9fd02b2d":"def find_optimized_weights(all_predictions_staked_test, all_predictions_staked_train, y_train, y_test):\n    opt = OptimizeRocAucScore()\n    # fit one fold of average predictions with targets (They have same size)\n    opt.fit(all_predictions_staked_test, y_train)\n    # predict coeff for each model over another hold\n    predict_ = opt.predict(all_predictions_staked_train)\n    print(f' predictions is {predict_} with shape {len(predict_)}')\n    auc = metrics.roc_auc_score(y_test, predict_)\n    print()\n    print(f'ROC_AUC is {auc} and OPT coeffs is {opt.coef_} to apply to model weights')\n    # return weights\n    return opt.coef_\n\noptimized_weights_train = find_optimized_weights(all_predictions_staked_test, all_predictions_staked_train, y_train, y_test)","cad243a6":"# get the weights on the second part\noptimized_weights_test = find_optimized_weights(all_predictions_staked_train, all_predictions_staked_test, y_test, y_train)","ca98ab1f":"def calculate_models_predictions_with_weights(Train, Valid, model_list, optimized_weights_train):\n    ''' Calculate predicitons with optimized weights for each model\n    Train all models on the Train set and calculate roc_auc on Valid Set\n    '''\n    weighted_pred_ = np.zeros(len(Valid))\n    pred_ = np.zeros(len(Valid))\n    # iterate over each model, apply weights\n    for index, model in enumerate(model_list):\n        pred = model.fit(Train.drop('targets', axis=1), Train.targets.values).predict_proba(Valid.drop('targets', axis=1))[:, 1]\n        weighted_pred_ += pred * optimized_weights_train[index]\n        pred_ += pred \n        \n    avg_pred = pred_ \/ len(model_list)\n    \n    print(f' The weighted probabilities over all models has roc_auc {metrics.roc_auc_score(Valid.targets.values, weighted_pred_)} on the Valid part')\n    print()\n    print(f' The Average probabilities over all models has roc_auc {metrics.roc_auc_score(Valid.targets.values, avg_pred)} on the Valid part')\n\ncalculate_models_predictions_with_weights(Train, Valid, model_list, optimized_weights_train)","481f731d":"Comments and suggestions are appreciated! about me https:\/\/www.linkedin.com\/in\/volodymyrgavrish\/","1c9b8e9f":"## Class for optimizing ROC_AUC scope\n\n#### We create class that take random coeff (in size of array) multiply by X values and calculate loss (in this case roc_auc)\n#### With **fmin** function, we will try to find such `x` where `y` is minimal  ## Class for optimizing ROC_AUC scope","267e1477":"## Short excursus about **fmin** function from scipy.optimize\n#### https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.optimize.fmin.html\n\n#### Fmin -> minimize a function using the downhill simplex algorithm.","f374a215":"* ### Train all the models on 2 train parts (0.5 split)\n* ### Predict on the second part\n* ### Calculate the **roc_auc** with average predictions (over all models)","bfcef995":"## Let see can we beat them if we find **weights** for each model predictions (not simply average)!\n\n* ### Optimize weight of the predictions of each model with fmin function\n* ### Calculate the roc_auc with optimized weights","b8b13147":"## Let split data on Valid \/ Train parts and when split Train part on training and testing parts","828e2ff3":"### We use distribution to initialize the coefficients as we start a search of best weights in OptimizeRocAucScoreOptimizeRocAucScore class\n\n` initial_coef = np.random.dirichlet(np.ones(X.shape[1]), size=1)initial_coef = np.random.dirichlet(np.ones(X.shape[1]), size=1) `\n\n#### It is obvious to see that **dirichlet** dist has less variance than others (even normal), but still not flat as uniform. \n#### It might depend on the task (domain knowledge) which distribution to choose. Here it is dirichletdirichlet\n\nmore about https:\/\/en.wikipedia.org\/wiki\/Dirichlet_distribution","7b4e9aa7":"## Let see how generally train \/ test validation schema looks like \n### For example, let us split data on the 4 parts","cc29a9fc":"* ## We get the predictions in the same way for second part, just changing X_test with X_train","6891b2d7":"## Learn best weights of each model","5f22ea36":"## We have the best ROC_AUC (0.97+) with `optimized_weights_train` weights \n\n### Next step we take apply these weights to the model predictions (not as average, as we did before) and test it on Valid part (as we have separated above)\n\n### By this we can really find out performance on the data that has not been used in any train\/test processes.","92c2c9e0":"### Average roc_auc is slightly less than XGB and RandomForest. (note: number can be slightly differently if you run on your cluster)\n\n### Generally, I guess that having average is better than single classifier with the highest score, since each model can grasp something from the data differently and at the end, would be more robust and descriptive to the new data.","5991e62c":"### Short excursus to the distribution -> let us draw several of them ","20ddc254":"### In this little research, we did the following\n\n- ### Trained a number of models on the training data\n- ### Found the optimal weights for each model\n- ### Stack all the results of each model taking into account the weights of each model\n- ### Counted the metric for the **Valid** part\n\n## Our result shows that the simple mean may not be optimal in the process of combining different models and it is necessary to conduct a separate study to find weights for each model for the stacked classifier.\n\n","d856a8f8":"![Validation_schema_and_Ensemble_models.jpg](attachment:369983d0-ca68-421c-bc03-6a9dab3d4de5.jpg)","2106f8c7":"## Weighted model predictions give at least +1% to the metrics on the data that has not been seen by any model! ","62075ea2":"## Target variable distribution is skewed to normal (not fraud) transaction class","c1c00f4a":"## List of models to use (could be more!)","06d1abab":"# Models staking with linear weights optimization over all model  \n\n#### inspired by book [approaching-almost-any-machine-learning-problem](https:\/\/github.com\/abhishekkrthakur\/approachingalmost) and author [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek)"}}