{"cell_type":{"ff99731d":"code","26782e0c":"code","b336c2d2":"code","e9c93c41":"code","06861106":"code","430b348b":"code","550e52a3":"code","af3b196d":"code","4c0861f0":"code","825c1545":"code","a45726f5":"markdown","2fa67d86":"markdown","8177bb5b":"markdown","f932a787":"markdown","7cef5c3a":"markdown","aebc3ee5":"markdown","aaaa1c9b":"markdown","6b1b44bf":"markdown","b9454272":"markdown"},"source":{"ff99731d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","26782e0c":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.head()","b336c2d2":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest.head()","e9c93c41":"sex_gb = train.groupby('Sex').agg({'Survived':['count','sum']}).reset_index()\nsex_gb.columns = ['sex', 'total', 'survived']\nsex_gb['survival_rate'] = sex_gb.survived*100 \/ sex_gb.total\nplt.figure(figsize=(15,8))\nax1 = plt.subplot(1,2,1)\n# Survival Rate by gender\nbars1 = plt.bar(sex_gb.sex, sex_gb.survival_rate)\nplt.title('Survival Rate by Gender', fontweight='bold')\nplt.ylabel('Survival Rate', fontweight='bold')\nplt.xlabel('Gender', fontweight='bold')\nplt.xticks(rotation=0)\nplt.ylim(0,80)\nplt.legend(loc='best')\n\nfor p in bars1:\n    height = p.get_height()\n    ax1.annotate(f'{height:.1f}',\n                xy=((p.get_x() + p.get_width() \/ 2.), height),\n                xytext=(0,3),  # 3 points vertical offset\n                textcoords=\"offset points\",\n                ha='center', va='bottom')\n# Survival Rate by class\ntrain['class_string'] = 'Third'\ntrain.loc[train.Pclass == 1, 'class_string'] = 'First'\ntrain.loc[train.Pclass == 2, 'class_string'] = 'Second'\nclass_gb = train.groupby('class_string').agg({'Survived':['count','sum']}).reset_index()\nclass_gb.columns = ['class_string', 'total', 'survived']\nclass_gb['survival_rate'] = class_gb.survived*100 \/ class_gb.total\nax2 = plt.subplot(1,2,2)\nbars2 = plt.bar(class_gb.class_string, class_gb.survival_rate)\nplt.title('Survival Rate by Class', fontweight='bold')\nplt.ylabel('Survival Rate', fontweight='bold')\nplt.xlabel('Class', fontweight='bold')\nplt.xticks(rotation=0)\nplt.ylim(0,80)\nplt.legend(loc='best')\n\nfor p in bars2:\n    height = p.get_height()\n    ax2.annotate(f'{height:.1f}',\n                xy=((p.get_x() + p.get_width() \/ 2.), height),\n                xytext=(0,3),  # 3 points vertical offset\n                textcoords=\"offset points\",\n                ha='center', va='bottom')\n\n    \nplt.show()","06861106":"bins = [x*5 for x in range(int(80\/5))]\n\nplt.figure(figsize=(20,12))\n# All passengers\nplt.subplot(2,3,1)\nsurvived_ages = train.loc[train.Survived==1, 'Age']\nperished_ages = train.loc[train.Survived==0, 'Age']\nplt.hist(survived_ages, bins, label='Survived', density=True, alpha=0.5)\nplt.hist(perished_ages, bins, label='Perished', density=True, alpha=0.5)\nplt.legend(loc='best')\nplt.title('Historgram of Ages by Survival Flag', fontweight='bold')\nplt.xlabel('Age', fontweight='bold')\nplt.ylabel('density')\n# Men\nplt.subplot(2,3,2)\nsurvived_ages = train.loc[(train.Survived==1) & (train.Sex=='male'), 'Age']\nperished_ages = train.loc[(train.Survived==0) & (train.Sex=='male'), 'Age']\nplt.hist(survived_ages, bins, label='Survived', density=True, alpha=0.5)\nplt.hist(perished_ages, bins, label='Perished', density=True, alpha=0.5)\nplt.legend(loc='best')\nplt.title('Historgram of Ages by Survival Flag\\n(male passengers only)', fontweight='bold')\nplt.xlabel('Age', fontweight='bold')\nplt.ylabel('density')\n#Women\nplt.subplot(2,3,3)\nsurvived_ages = train.loc[(train.Survived==1) & (train.Sex=='female'), 'Age']\nperished_ages = train.loc[(train.Survived==0) & (train.Sex=='female'), 'Age']\nplt.hist(survived_ages, bins, label='Survived', density=True, alpha=0.5)\nplt.hist(perished_ages, bins, label='Perished', density=True, alpha=0.5)\nplt.legend(loc='best')\nplt.title('Historgram of Ages by Survival Flag\\n(female passengers only)', fontweight='bold')\nplt.xlabel('Age', fontweight='bold')\nplt.ylabel('density')\n# First Class\nplt.subplot(2,3,4)\nsurvived_ages = train.loc[(train.Survived==1) & (train.Pclass==1), 'Age']\nperished_ages = train.loc[(train.Survived==0) & (train.Pclass==1), 'Age']\nplt.hist(survived_ages, bins, label='Survived', density=True, alpha=0.5)\nplt.hist(perished_ages, bins, label='Perished', density=True, alpha=0.5)\nplt.legend(loc='best')\nplt.title('Historgram of Ages by Survival Flag\\n(first class passengers only)', fontweight='bold')\nplt.xlabel('Age', fontweight='bold')\nplt.ylabel('density')\n# Second Class\nplt.subplot(2,3,5)\nsurvived_ages = train.loc[(train.Survived==1) & (train.Pclass==2), 'Age']\nperished_ages = train.loc[(train.Survived==0) & (train.Pclass==2), 'Age']\nplt.hist(survived_ages, bins, label='Survived', density=True, alpha=0.5)\nplt.hist(perished_ages, bins, label='Perished', density=True, alpha=0.5)\nplt.legend(loc='best')\nplt.title('Historgram of Ages by Survival Flag\\n(second class passengers only)', fontweight='bold')\nplt.xlabel('Age', fontweight='bold')\nplt.ylabel('density')\n#Thrid Class\nplt.subplot(2,3,6)\nsurvived_ages = train.loc[(train.Survived==1) & (train.Pclass==3), 'Age']\nperished_ages = train.loc[(train.Survived==0) & (train.Pclass==3), 'Age']\nplt.hist(survived_ages, bins, label='Survived', density=True, alpha=0.5)\nplt.hist(perished_ages, bins, label='Perished', density=True, alpha=0.5)\nplt.legend(loc='best')\nplt.title('Historgram of Ages by Survival Flag\\n(third class passengers only)', fontweight='bold')\nplt.xlabel('Age', fontweight='bold')\nplt.ylabel('density')\n\nplt.show()","430b348b":"clean_train = train[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Survived']].copy()\n# One-hot encode gender\nfor g in ['male', 'female']:\n    clean_train[g] = 0\n    clean_train.loc[clean_train.Sex==g, g]=1\nclean_train = clean_train.drop(columns='Sex')\n# One-hot encode class\nfor c,s in zip(range(1,4), ['first_class', 'second_class', 'third_class']):\n    clean_train[s] = 0\n    clean_train.loc[clean_train.Pclass==c, s]=1\nclean_train = clean_train.drop(columns='Pclass')   \n\n\n# Impute null age values\ntemp_train = clean_train.loc[~clean_train.Age.isna()].copy()\ntemp_test = clean_train.loc[clean_train.Age.isna()].copy().drop(columns='Age')\n\nX_train = temp_train.drop(columns='Age')\ny_train = temp_train.Age\nX_test = temp_test.copy()\n\nlr1 = LinearRegression()\nlr1.fit(X_train, y_train)\ntemp_test['Age'] = lr1.predict(X_test)\n\nno_nulls_train = pd.concat([temp_train, temp_test])\nno_nulls_train.sample(10)","550e52a3":"# # Create the parameter grid based on the results of random search \n# param_grid = {\n#     'max_depth': [10, 20, 30],\n#     #'max_features': [2, 3],\n#     'min_samples_leaf': [5, 10, 15],\n#     #'min_samples_split': [5, 8, 12],\n#     'n_estimators': [300,400]\n# }\n# # Create a based model\n# rf = RandomForestClassifier()\n# X_train = no_nulls_train.drop(columns='Survived')\n# y_train = no_nulls_train.Survived\n# # Get model scoring\n\n# clf = GridSearchCV(\n#     rf, param_grid, cv=5, scoring='roc_auc', verbose=1\n# )\n# clf.fit(X_train, y_train)\n\n# print(\"Best parameters set found on development set:\")\n# print()\n# print(clf.best_params_)\n# print(\"Best Score:\")\n# print()\n# print(clf.best_score_)","af3b196d":"clean_test = test[['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch']].copy()\n# One-hot encode gender\nfor g in ['male', 'female']:\n    clean_test[g] = 0\n    clean_test.loc[clean_test.Sex==g, g]=1\nclean_test = clean_test.drop(columns='Sex')\n# One-hot encode class\nfor c,s in zip(range(1,4), ['first_class', 'second_class', 'third_class']):\n    clean_test[s] = 0\n    clean_test.loc[clean_test.Pclass==c, s]=1\nclean_test = clean_test.drop(columns='Pclass')   \n\n\nX_test = clean_test.drop(columns=['PassengerId', 'Age']).copy()\nX_train = temp_train.drop(columns=['Age', 'Survived'])\ny_train = temp_train.Age\nlr2 = LinearRegression()\nlr2.fit(X_train, y_train)\nno_nulls_test = clean_test.copy()\nno_nulls_test['Pred_Age'] = lr2.predict(X_test)\n\nno_nulls_test['Age'] = clean_test.Age.combine_first(no_nulls_test['Pred_Age']).drop(columns='Pred_Age')\nno_nulls_test = no_nulls_test.drop(columns='Pred_Age')\nno_nulls_test","4c0861f0":"rf = RandomForestClassifier(max_depth=20,min_samples_leaf=5, n_estimators=400)\nrf.fit(no_nulls_train.drop(columns='Survived'), no_nulls_train.Survived)\nno_nulls_test['Survived']=rf.predict(no_nulls_test.drop(columns='PassengerId'))\nno_nulls_test.sample(10)","825c1545":"output = no_nulls_test[['PassengerId', 'Survived']]\noutput.to_csv('output.csv', index=False)\nprint(\"Your submission was successfully saved!\")","a45726f5":"### Grid search cross validation\n\n* Looking for the best hyper parameters to train our random forest with. ","2fa67d86":"***\n# Land Ho! Building our classifier. \n\n### Feature engineering and data classification\n\n* One hot encode categorical data\n* Impute null values in age column","8177bb5b":"# A study of survival on board the Titanic\n***\n### Import usefule libraries","f932a787":"***\n## Iceberg Ahead! Making Predicitons.","7cef5c3a":"### Feature Engineering Test Data","aebc3ee5":"## A list of our sailors","aaaa1c9b":"## Data visualization\n***\n### Survival rate by gender and class","6b1b44bf":"### Formating output","b9454272":"### Histograms of age by survial"}}