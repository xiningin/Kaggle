{"cell_type":{"a941a460":"code","0ddeb8d7":"code","a7391980":"code","0e8d0dff":"code","42dbb3ac":"code","687b721e":"code","a4b03519":"code","f08b1364":"code","216cbbd9":"code","4b571e14":"code","ee0c54a1":"code","a0ba9a52":"code","f7fc711a":"code","3ddd5795":"code","e7edc699":"code","dec94adf":"code","7d300b70":"code","4384de47":"code","404d8199":"code","f8bd0fcc":"code","4e84f5bd":"code","5554808d":"code","08e73e46":"code","3b53fe50":"code","11f5afec":"code","2809b3f4":"code","fdc1782d":"code","6ec4cc0f":"code","4f0a74a2":"code","028e82b7":"code","9b4cdfee":"code","981faae4":"code","eac7a2bf":"code","eb4715ef":"code","48f9612e":"code","7cd84b80":"code","8b8408d7":"code","9aeae6e6":"code","f58efeda":"code","80846bfe":"code","abdaa28c":"code","36047aba":"code","56002315":"code","4e5dab3d":"markdown","f26326f9":"markdown","75c750e6":"markdown","45361d0b":"markdown","30102cf6":"markdown","93e9675a":"markdown","1909ce26":"markdown","9a4e2300":"markdown","36a9cbdd":"markdown","936d2907":"markdown","8213ea14":"markdown","78f15538":"markdown","1a879100":"markdown","d0a1879a":"markdown","7e8f7664":"markdown","3249d9e1":"markdown","cea9cc26":"markdown"},"source":{"a941a460":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n# better display of review text in dataframes\npd.set_option('display.max_colwidth', None) \n\n# Seaborn options\nsns.set(style=\"whitegrid\", font_scale=1.4)\n\n# Auto reload modules\n%load_ext autoreload\n%autoreload 2","0ddeb8d7":"import tensorflow as tf\nassert tf.__version__ >= \"2.0\"","a7391980":"import tensorflow \nprint(tensorflow.__version__)","0e8d0dff":"df = pd.read_csv('..\/input\/products-csv\/train_data.csv')\ntrain, test = train_test_split(df, test_size=0.2,random_state=42,shuffle=True, stratify=None)\ndf_train, df_val = train_test_split(train, test_size=0.1,random_state=42,shuffle=True, stratify=None)\n# Reviews need to be tokenized\ntrain_reviews = df_train['pname'].astype(str)\nval_reviews = df_val['pname'].astype(str)\ntest_reviews = test['pname'].astype(str)\n\ntrain_labels = df_train['category_id'].astype(int)\nval_labels = df_val['category_id'].astype(int)\ntest_labels = test['category_id'].astype(int)\n","42dbb3ac":"from transformers import CamembertTokenizer\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n#camembert-base\nmodel_name = \"camembert-base\"\n#CamembertTokenizer\ntokenizer = CamembertTokenizer.from_pretrained(model_name)","687b721e":"some_review = train_reviews[0]\nsome_review","a4b03519":"tokenizer.tokenize(some_review)[:15]","f08b1364":"tokenizer.encode(some_review)[:15]","216cbbd9":"tokenizer.decode(tokenizer.encode(some_review))","4b571e14":"reviews_len = [len(tokenizer.encode(review, max_length=20))\n                          for review in train_reviews]\n\"\"\"print(\"Average length: {:.1f}\".format(np.mean(reviews_len)))\nprint(\"Max length: {}\".format(max(reviews_len)))\n\nplt.figure(figsize=(10,5))\nax = sns.distplot(reviews_len, bins=150, kde=False, hist_kws=dict(alpha=0.8))\nax.set(xlabel='Number of tokens')\n\n# Finalize the plot\nsns.despine(bottom=True)\nplt.tight_layout(h_pad=2)\n\n# Saving plot\nfig = ax.get_figure()\nfig.savefig('img\/bert\/number_of_tokens.png', dpi=200)\"\"\"","ee0c54a1":"tokenizer.vocab_size","a0ba9a52":"MAX_SEQ_LEN = 20 # in terms of generated tokens (not words)\n\nshort_reviews = sum(np.array(reviews_len) <= MAX_SEQ_LEN)\nlong_reviews = sum(np.array(reviews_len) > MAX_SEQ_LEN)\n\nprint(\"{} reviews with LEN > {} ({:.2f} % of total data)\".format(\n    long_reviews,\n    MAX_SEQ_LEN,\n    100 * long_reviews \/ len(reviews_len)\n))","f7fc711a":"import numpy as np\n\ndef encode_reviews(tokenizer, reviews, max_length):\n    token_ids = np.zeros(shape=(len(reviews), max_length),\n                         dtype=np.int32)\n    for i, review in enumerate(reviews):\n        encoded = tokenizer.encode(review, max_length=max_length)\n        token_ids[i, 0:len(encoded)] = encoded\n    attention_mask = (token_ids != 0).astype(np.int32)\n    return {\"input_ids\": token_ids, \"attention_mask\": attention_mask}","3ddd5795":"encoded_train = encode_reviews(tokenizer, train_reviews, MAX_SEQ_LEN)\nencoded_valid = encode_reviews(tokenizer, val_reviews, MAX_SEQ_LEN)\nencoded_test = encode_reviews(tokenizer, test_reviews, MAX_SEQ_LEN)","e7edc699":"y_train = np.array(train_labels)\ny_val = np.array(val_labels)\ny_test = np.array(test_labels)","dec94adf":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CamembertPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, tokenizer, max_seq_length):\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def fit(self, X=None):\n        pass\n    \n    def transform(self, X, y):\n        # 1. Tokenize\n        X_encoded = encode_reviews(self.tokenizer, X, self.max_seq_length)\n        # 2. Labels\n        y_array = np.array(y)\n        return X_encoded, y_array     \n    \n    def fit_transform(self, X, y):        \n        return self.transform(X, y)","7d300b70":"from transformers import TFCamembertForSequenceClassification\n\nmodel = TFCamembertForSequenceClassification.from_pretrained(\"jplu\/tf-camembert-base\")\n#, epsilon=1e-08\nopt = tf.keras.optimizers.SGD(learning_rate=5e-3)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)    \n\nmodel.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])","4384de47":"initial_weights = model.get_weights()\nmodel.summary()","404d8199":"# TODO (in fact, done in the Accuracy vs Training Data part)\nhistory = model.fit(\n    encoded_train, y_train, epochs=10, batch_size=256, \n    validation_data=(encoded_valid, y_val), verbose=1\n)","f8bd0fcc":"from sklearn.base import BaseEstimator\n\nclass EarlyStoppingModel(BaseEstimator):\n    def __init__(self, transformers_model, max_epoches, batch_size, validation_data):\n        self.model = transformers_model\n        self.max_epoches = max_epoches\n        self.batch_size = batch_size\n        self.validation_data = validation_data\n        \n    def fit(self, X, y):\n        # Defines early stopper\n        early_stopper = tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss', mode='auto', patience=2, # only 1 !\n            verbose=1, restore_best_weights=True\n        )        \n\n        # Train model on data subset\n        self.model.fit(\n            X, y,\n            validation_data=self.validation_data,\n            epochs=self.max_epoches, \n            batch_size=self.batch_size,\n            callbacks=[early_stopper], \n            verbose=1\n        )        \n        return self\n    \n    def predict(self, X):        \n        scores = self.model.predict(X)\n        y_pred = np.argmax(scores, axis=1)\n        return y_pred","4e84f5bd":"from sklearn import metrics \n\ndef accuracy_vs_training_data(camembert_model, initial_weights, \n                              preprocessor, sizes,\n                              train_reviews, train_labels,\n                              val_reviews, val_labels,\n                              test_reviews, test_labels):\n    test_accuracies = []\n    for size in sizes:        \n        # Preprocess data\n        X_train, y_train = preprocessor.fit_transform(\n            train_reviews[:size], train_labels[:size]\n        )\n        X_val, y_val = preprocessor.transform(val_reviews, val_labels)\n        X_test, y_test = preprocessor.transform(test_reviews, test_labels)\n        \n        # Reset weights to initial value\n        camembert_model.set_weights(initial_weights)\n        best_model = EarlyStoppingModel(\n            camembert_model, max_epoches=20, batch_size=4,\n            validation_data=(X_val, y_val)\n        )\n        \n        # Train model\n        best_model.fit(X_train, y_train)\n        \n        # Evaluate on test set\n        y_pred = best_model.predict(X_test)\n        test_acc = metrics.accuracy_score(y_test, y_pred)\n        test_accuracies.append(test_acc)\n        print(\"Test acc: \" + str(test_acc))\n        \n    return test_accuracies    ","5554808d":"sizes = [int(p) for p in np.geomspace(500, 160000, 5)]\npreprocessor = CamembertPreprocessor(tokenizer, MAX_SEQ_LEN)\n\ntest_accuracies = accuracy_vs_training_data(\n    model, initial_weights, \n    preprocessor, sizes,\n    train_reviews, train_labels,\n    val_reviews, val_labels,\n    test_reviews, test_labels\n)","08e73e46":"# Saving last model (full dataset)\nmodel.save_weights('data\/bert\/camembert_weights.hdf5')","3b53fe50":"import pickle\n\nOUTPUT_PATH = 'data\/bert\/camembert_accuracies.pickle'\n\noutput_dict = {\n    \"sizes\": sizes,\n    \"test_accuracies\": test_accuracies\n}\n\nwith open(OUTPUT_PATH, 'wb') as writer:\n    pickle.dump(output_dict, writer)","11f5afec":"from sklearn import metrics\n\nmodel.load_weights('data\/camembert_weights.hdf5')\nscores = model.predict(encoded_valid)\ny_pred = np.argmax(scores, axis=1)\n    \nprint(\"Val Accuracy: {:.2f}\".format(100 * metrics.accuracy_score(y_val, y_pred)))\nprint(\"Val F1-Score: {:.2f}\".format(100 * metrics.f1_score(y_val, y_pred)))","2809b3f4":"from utils import print_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\nconf_mx = confusion_matrix(y_val, y_pred)\n\nfig = print_confusion_matrix(\n    conf_mx, \n    class_names.values(), \n    figsize=(7,5)\n)\n\n# Finalize the plot\nsns.despine(bottom=True)\nplt.tight_layout(h_pad=2)\n\n# Saving plot\nfig.savefig('img\/bert\/val_confusion_mx.png', dpi=200)","fdc1782d":"## False positive \/ negative","6ec4cc0f":"false_pos = val_reviews[(y_val == 0) & (y_pred == 1)]\nfalse_neg = val_reviews[(y_val == 1) & (y_pred == 0)]","4f0a74a2":"pd.DataFrame(false_pos[:5])","028e82b7":"pd.DataFrame(false_neg[:5])","9b4cdfee":"model.load_weights('data\/bert\/camembert_weights.hdf5')\n\nscores = model.predict(encoded_test)\ny_pred = np.argmax(scores, axis=1)","981faae4":"from sklearn import metrics\n    \nprint(\"Test Accuracy: {:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred)))\nprint(\"Test F1-Score: {:.2f}\".format(100 * metrics.f1_score(y_test, y_pred)))\nprint()\n\nreport = metrics.classification_report(\n    y_test, y_pred, \n    target_names=class_names.values()\n)\nprint(report)","eac7a2bf":"from sklearn.metrics import confusion_matrix\nconf_mx = confusion_matrix(y_test, y_pred)\n\nfig = print_confusion_matrix(\n    conf_mx, \n    class_names.values(), \n    figsize=(7,5)\n)\n\n# Finalize the plot\nsns.despine(bottom=True)\nplt.tight_layout(h_pad=2)\n\n# Saving plot\nfig.savefig('img\/bert\/test_confusion_mx.png', dpi=200)","eb4715ef":"model.load_weights('data\/bert\/camembert_weights.hdf5')","48f9612e":"import time\n\ninference_times = []\n\nfor i in range(1000):\n    x = {\n    'input_ids': np.array([encoded_test['input_ids'][i], ]),\n    'attention_mask':  np.array([encoded_test['attention_mask'][i], ]),\n    }\n    start_time = time.time()\n    y_pred = model.predict(x)\n    stop_time = time.time()\n    \n    inference_times.append(stop_time - start_time)    ","7cd84b80":"OUTPUT_PATH = 'data\/bert\/camembert_times.pickle'\n\nwith open(OUTPUT_PATH, 'wb') as writer:\n    pickle.dump(inference_times, writer)","8b8408d7":"import os\nfrom utils_acl import get_data\n\nACL_FOLDER = 'data\/cls-acl10-unprocessed\/fr'\nBOOKS_FOLDER = os.path.join(ACL_FOLDER, 'books')\nDVD_FOLDER = os.path.join(ACL_FOLDER, 'dvd')\nMUSIC_FOLDER = os.path.join(ACL_FOLDER, 'music')\n\n_, _, test_reviews_b, test_labels_b = get_data(BOOKS_FOLDER)\n_, _, test_reviews_d, test_labels_d  = get_data(DVD_FOLDER)\n_, _, test_reviews_m, test_labels_m  = get_data(MUSIC_FOLDER)","9aeae6e6":"from sklearn import metrics \n\ndef evaluate(model, X, y):\n    scores = model.predict(X)\n    y_pred = np.argmax(scores, axis=1)\n    print(\"Accuracy: {:.2f}\".format(100 * metrics.accuracy_score(y, y_pred)))\n    print(\"F1-Score: {:.2f}\".format(100 * metrics.f1_score(y, y_pred)))","f58efeda":"preprocessor = CamembertPreprocessor(tokenizer, MAX_SEQ_LEN)","80846bfe":"model.load_weights('data\/bert\/camembert_weights.hdf5')","abdaa28c":"X_books, y_books = preprocessor.transform(test_reviews_b, test_labels_b)\nevaluate(model, X_books, y_books)","36047aba":"X_dvd, y_dvd = preprocessor.transform(test_reviews_d, test_labels_d)\nevaluate(model, X_dvd, y_dvd)","56002315":"X_music, y_music = preprocessor.transform(test_reviews_m, test_labels_m)\nevaluate(model, X_music, y_music)","4e5dab3d":"# Training","f26326f9":"## Preprocess dataset","75c750e6":"# Model","45361d0b":"# Generalizability","30102cf6":"### Tokenize","93e9675a":"# Prepare data","1909ce26":"### Preprocessing pipeline","9a4e2300":"# Error Analysis","36a9cbdd":"## Load dataset","936d2907":"# Testing best model","8213ea14":"### Labels","78f15538":"## Music","1a879100":"# Accuracy vs Training Data","d0a1879a":"## Confusion Matrix","7e8f7664":"# Inference time","3249d9e1":"## Books","cea9cc26":"## DVD"}}