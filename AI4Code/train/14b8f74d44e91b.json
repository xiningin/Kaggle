{"cell_type":{"c33d6155":"code","7cf66d8d":"code","2f55e1f1":"code","8bad2c78":"code","c9c7be10":"code","2f17d336":"code","8c4a65d5":"code","ec08f4e0":"code","4a1e2a73":"code","6f387a05":"code","282108dc":"code","f72371dc":"code","ea1becbe":"code","8709c8ca":"code","0b510ae9":"code","a1b128a9":"code","086e250d":"code","219f9581":"code","e5e7ca13":"code","c42fbe5f":"code","063a8cb3":"code","d9e3f79a":"code","ac242900":"code","f6354499":"code","02083093":"code","6d086cfe":"code","4882dbcb":"code","9c37ec6c":"code","fd9e7386":"code","64de7248":"code","b56330e7":"markdown","caf8dd89":"markdown","afac1a4d":"markdown","b420b2b7":"markdown","738bb0eb":"markdown","0a85ea48":"markdown","5f6f466e":"markdown"},"source":{"c33d6155":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nSEED = 42\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","7cf66d8d":"train = pd.read_csv(\"\/kaggle\/input\/credit-card-customers\/BankChurners.csv\", low_memory=True)\ntrain.drop(columns=[col for col in train.columns if col.startswith(\"Naive_\")] , inplace=True) # Let's drop the Naive_Bayes... column\ntarget = \"Attrition_Flag\" # Target to train on\nids = [\"CLIENTNUM\"] # IDs to drop, or use only to identify data (else will overfit)\ntrain.drop(columns=ids, inplace=True)\n\ntrain.head()","2f55e1f1":"if \"Set\" not in train.columns:\n    print(\"Building tailored column\")\n    train_valid_index, test_index = next(\n        StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=SEED).split(\n            range(train[target].shape[0]), train[target].values\n        )\n    )\n    train_index, valid_index = next(\n        StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=SEED).split(\n            train_valid_index, train[target].values[train_valid_index]\n        )\n    )\n    train[\"Set\"] = \"train\"\n    train[\"Set\"][valid_index] = \"valid\"\n    train[\"Set\"][test_index] = \"test\"","8bad2c78":"train_indices = train[train.Set == \"train\"].index\nvalid_indices = train[train.Set == \"valid\"].index\ntest_indices = train[train.Set == \"test\"].index","c9c7be10":"y = train[target].astype(\"category\").cat.codes\ntrain.drop(columns=[target, \"Set\"], inplace=True)","2f17d336":"# Identify categorical columns + label encode","8c4a65d5":"cat_idxs = []","ec08f4e0":"for i, col in enumerate(train.columns):\n    if train[col].dtype == \"object\":\n        train[col] = train[col].astype(\"category\").cat.codes\n        cat_idxs.append(i)","4a1e2a73":"# No Nan, no need for imputing","6f387a05":"clf = LGBMClassifier(num_leaves=7, importance_type=\"gain\", n_estimators=20000, random_state=SEED)\nclf.fit(\n    train.values[train_indices],\n    y[train_indices],\n    eval_set=[(train.values[valid_indices], y[valid_indices])],\n    early_stopping_rounds=20,\n    categorical_feature=cat_idxs,\n    verbose=10\n)","282108dc":"roc_auc_score(\n    y_true=y[valid_indices],\n    y_score=clf.predict(train.values[valid_indices]).reshape(-1),\n)","f72371dc":"roc_auc_score(\n    y_true=y[test_indices],\n    y_score=clf.predict(train.values[test_indices]).reshape(-1),\n)","ea1becbe":"importances = clf.feature_importances_ \/ clf.feature_importances_.sum()","8709c8ca":"def explain_plot(importances, columns):\n    selection = np.argsort(np.absolute(importances))\n    performance = importances[selection]\n    y_pos = np.arange(performance.shape[0])\n\n    plt.barh(y_pos, performance, align=\"center\", alpha=0.5)\n    plt.yticks(y_pos, columns[selection])\n    plt.title(\"Feature importance\")\n\n    plt.show()","0b510ae9":"explain_plot(importances, train.columns)","a1b128a9":"indexes = np.argsort(-np.absolute(importances))\nindexes","086e250d":"to_keep = []\nvariance = 0\ni = 0\nwhile variance < 0.99:\n    variance += importances[indexes[i]]\n    to_keep.append(indexes[i])\n    i+=1\nto_keep","219f9581":"len(train.columns) - len(to_keep)","e5e7ca13":"new_cat_idxs = [i for i, idx in enumerate(to_keep) if idx in cat_idxs]\nnew_cat_idxs","c42fbe5f":"clf_selected = LGBMClassifier(num_leaves=7, importance_type=\"gain\", n_estimators=20000, random_state=SEED)\nclf_selected.fit(\n    train.values[train_indices][:, to_keep],\n    y[train_indices],\n    eval_set=[(train.values[valid_indices][:, to_keep], y[valid_indices])],\n    early_stopping_rounds=20,\n    categorical_feature=new_cat_idxs,\n    verbose=10\n)","063a8cb3":"model_auc = roc_auc_score(\n    y_true=y[valid_indices],\n    y_score=clf_selected.predict(train.values[valid_indices][:, to_keep]).reshape(-1),\n)\nmodel_auc","d9e3f79a":"model_auc = roc_auc_score(\n    y_true=y[test_indices],\n    y_score=clf_selected.predict(train.values[test_indices][:, to_keep]).reshape(-1),\n)\nmodel_auc","ac242900":"importances = clf_selected.feature_importances_ \/ clf_selected.feature_importances_.sum()","f6354499":"explain_plot(importances, train.columns[to_keep])","02083093":"from sklearn.tree import DecisionTreeClassifier","6d086cfe":"tree = DecisionTreeClassifier(random_state=42, max_depth=5)\ntree.fit(train.values[train_indices], y.values[train_indices])","4882dbcb":"model_auc = roc_auc_score(\n    y_true=y[valid_indices],\n    y_score=tree.predict(train.values[valid_indices]).reshape(-1),\n)\nmodel_auc","9c37ec6c":"model_auc = roc_auc_score(\n    y_true=y[test_indices],\n    y_score=tree.predict(train.values[test_indices]).reshape(-1),\n)\nmodel_auc","fd9e7386":"from sklearn.tree import plot_tree\n\nplot_tree(tree, feature_names=train.columns.tolist()) #, max_depth=5)\nplt.show()","64de7248":"n_nodes = tree.tree_.node_count\nchildren_left = tree.tree_.children_left\nchildren_right = tree.tree_.children_right\nfeature = tree.tree_.feature\nthreshold = tree.tree_.threshold\n\nnode_depth = np.zeros(shape=n_nodes, dtype=np.int64)\nis_leaves = np.zeros(shape=n_nodes, dtype=bool)\nstack = [(0, 0)]  # start with the root node id (0) and its depth (0)\nwhile len(stack) > 0:\n    # `pop` ensures each node is only visited once\n    node_id, depth = stack.pop()\n    node_depth[node_id] = depth\n\n    # If the left and right child of a node is not the same we have a split\n    # node\n    is_split_node = children_left[node_id] != children_right[node_id]\n    # If a split node, append left and right children and depth to `stack`\n    # so we can loop through them\n    if is_split_node:\n        stack.append((children_left[node_id], depth + 1))\n        stack.append((children_right[node_id], depth + 1))\n    else:\n        is_leaves[node_id] = True\n\nprint(\"The binary tree structure has {n} nodes and has \"\n      \"the following tree structure:\\n\".format(n=n_nodes))\nfor i in range(n_nodes):\n    if is_leaves[i]:\n        print(\"{space}node={node} is a leaf node.\".format(\n            space=node_depth[i] * \"\\t\", node=i))\n    else:\n        print(\"{space}node={node} is a split node: \"\n              \"go to node {left} if X[:, {feature}] <= {threshold} \"\n              \"else to node {right}.\".format(\n                  space=node_depth[i] * \"\\t\",\n                  node=i,\n                  left=children_left[i],\n                  feature=feature[i],\n                  threshold=threshold[i],\n                  right=children_right[i]))","b56330e7":"Results seems too good, looks like this is not real data.\nDecision already has good results","caf8dd89":"# Build tailored split","afac1a4d":"# Feature selection (keep 99% of variance)","b420b2b7":"Valid for early stopping\nTest should be untouched to evaluate performance (and compare with valid)","738bb0eb":"# Trying decision tree","0a85ea48":"# Read data & remove cols","5f6f466e":"# Simple LGBM + performance"}}