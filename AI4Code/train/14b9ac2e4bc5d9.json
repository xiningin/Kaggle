{"cell_type":{"bbdfbba6":"code","569d1bfa":"code","83762aa9":"code","d9c68399":"code","0a1c438c":"code","4cc06c07":"code","8811203d":"code","11ff7024":"code","39617845":"code","e3db0fa6":"code","dab8c4f0":"code","f6110828":"code","b50a7944":"code","e57affbf":"code","a9601a69":"code","460b0ab1":"markdown","d0476a34":"markdown","309fb297":"markdown","4bdd2600":"markdown","4f2d90a6":"markdown","a1b311bb":"markdown","fd95bda9":"markdown","60182678":"markdown"},"source":{"bbdfbba6":"!pip install -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","569d1bfa":"from PIL import Image\nImage.open('..\/input\/pennfudanped\/PNGImages\/FudanPed00001.png')","83762aa9":"import os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n\n        mask = np.array(mask)\n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n#         target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","d9c68399":"dataset = PennFudanDataset('..\/input\/pennfudanped\/')\ndataset[0]","0a1c438c":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n      \ndef get_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","4cc06c07":"# Download TorchVision repo to use some files from\n# references\/detection\n!git clone https:\/\/github.com\/pytorch\/vision.git\n!git checkout v0.3.0\n\n!cp .\/vision\/references\/detection\/utils.py .\/\n!cp .\/vision\/references\/detection\/transforms.py .\/\n!cp .\/vision\/references\/detection\/coco_eval.py .\/\n!cp .\/vision\/references\/detection\/engine.py .\/\n!cp .\/vision\/references\/detection\/coco_utils.py .\/","8811203d":"from engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","11ff7024":"# use our dataset and defined transformations\ndataset = PennFudanDataset('..\/input\/pennfudanped', get_transform(train=True))\ndataset_test = PennFudanDataset('..\/input\/pennfudanped', get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])","39617845":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","e3db0fa6":"for imgs, targets in data_loader:\n    print(len(imgs), imgs[0].shape)\n    print(len(targets), targets[0].keys())\n    break","dab8c4f0":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","f6110828":"# let's train it\nnum_epochs = 8\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)","b50a7944":"# pick one image from the test set\nimg, _ = dataset_test[2]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])","e57affbf":"prediction","a9601a69":"from PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\n \nimg_new = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\ndraw = ImageDraw.Draw(img_new)\n\nbbs = prediction[0]['boxes']\n\nfor bb in bbs:\n    draw.rectangle(bb.cpu().numpy(), outline=(255, 255, 255), width=3)\nimg_new","460b0ab1":"\u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 `PIL.Image` \u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0439 \u043f\u043e\u043b\u044f.","d0476a34":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0438 \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 bounding box.\n\u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u0432 \u043e\u0431\u044b\u0447\u043d\u043e\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0431\u044b\u043b\u0430 \u043e\u0442\u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0430 \u0434\u043e 0-1 \u0438 \u0438\u043c\u0435\u043b\u0430 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 `[C, H, W]`.","309fb297":"## \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\n\n\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c Fast R-CNN - \u044d\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0430\u0439 bounding box'\u044b \u0438 \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u043f\u043e\u0442\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043d\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438.\n\n\n\u0411\u0443\u0434\u0435\u043c \u0434\u043e\u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043d\u0430\u0448 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e\u0447\u0435\u043d\u044c \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0439.","4bdd2600":"## \u0421\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0414\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n\n[torchvision reference scripts for training object detection, instance segmentation and person keypoint detection](https:\/\/github.com\/pytorch\/vision\/tree\/v0.3.0\/references\/detection)\n\n\u041a\u0430\u0441\u0442\u043e\u043c\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043e\u043b\u0436\u0435\u043d \u043d\u0430\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043e\u0442 `torch.utils.data.Dataset` \u0438 \u0438\u043c\u043f\u043b\u0435\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u043e\u0434\u044b `__len__` \u0438 `__getitem__`.\n\n\u041c\u0435\u0442\u043e\u0434 `__getitem__` \u0434\u043e\u043b\u0436\u0435\u043d \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c:\n\n* \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435: PIL Image \u0440\u0430\u0437\u043c\u0435\u0440\u0430 (H, W)\n* \u0442\u0430\u0440\u0433\u0435\u0442: \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0439 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u043f\u043e\u043b\u044f\n    * `boxes` (`FloatTensor[N, 4]`): \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b bounding box'a `[x0, y0, x1, y1]`\n    * `labels` (`Int64Tensor[N]`): \u043b\u0435\u0439\u0431\u043b\u044b \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e bounding box'a\n    * `image_id` (`Int64Tensor[1]`): \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\n    * `area` (`Tensor[N]`): \u043f\u043b\u043e\u0449\u0430\u0434\u044c bounding box'a. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430, \u0447\u0442\u043e\u0431\u044b \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043c\u0435\u0436\u0434\u0443 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u043c\u0438, \u0441\u0440\u0435\u0434\u043d\u0438\u043c\u0438 \u0438 \u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438 \u0431\u043e\u043a\u0441\u0430\u043c\u0438\n    * `iscrowd` (`UInt8Tensor[N]`): \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0441 `iscrowd=True` \u0431\u0443\u0434\u0443\u0442 \u0438\u0433\u043d\u043e\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043f\u0440\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\n    * (optionally) `masks` (`UInt8Tensor[N, H, W]`): \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0441\u043a\u0430\n    * (optionally) `keypoints` (`FloatTensor[N, K, 3]`): \u0434\u043b\u044f \u0432\u0441\u0435\u0445 `N` \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 `K` \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0445 \u0442\u043e\u0447\u0435\u043a `[x, y, visibility]`. `visibility=0` \u0437\u043d\u0430\u0447\u0438\u0442, \u0447\u0442\u043e \u043a\u043b\u044e\u0447\u0435\u0432\u0430\u044f \u0442\u043e\u0447\u043a\u0430 \u043d\u0435 \u0432\u0438\u0434\u043d\u0430.\n","4f2d90a6":"# TorchVision 0.3 Object Detection finetuning tutorial\n\n\u0421\u0435\u0433\u043e\u0434\u043d\u044f \u0431\u0443\u0434\u0435\u043c \u0434\u043e\u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c fastrcnn \u043d\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 [*Penn-Fudan Database for Pedestrian Detection and Segmentation*](https:\/\/www.cis.upenn.edu\/~jshi\/ped_html\/). \u0412 \u043d\u0435\u043c 170 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0441 345 \u043f\u0435\u0448\u0435\u0445\u043e\u0434\u0430\u043c\u0438, \u044d\u0442\u043e\u0442 \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442, \u043a\u0430\u043a \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0438 \u043d\u0430 \u043a\u0430\u0441\u0442\u043e\u043c\u043d\u043e\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435.\n\n\u041d\u0443\u0436\u043d\u043e \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c pycocotools. \u042d\u0442\u0430 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0430 \u0434\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0435\u0442\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430.","a1b311bb":"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043b\u043e\u0432\u0430\u0440\u0435\u0439. \u041a\u0430\u0436\u0434\u044b\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442 \u0441\u043f\u0438\u0441\u043a\u0430 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044e, \u043d\u043e \u0442\u0430\u043a \u043a\u0430\u043a \u0443 \u043d\u0430\u0441 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0434\u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430, \u0442\u043e \u043e\u0431\u044a\u0435\u043a\u0442 \u0432 \u0441\u043f\u0438\u0441\u043a\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0434\u0438\u043d.\n\u0421\u043b\u043e\u0432\u0430\u0440\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 `boxes`, `labels` \u0438 `scores`.","fd95bda9":"\u041d\u0430\u043f\u0438\u0448\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0438 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445.\n","60182678":"## \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n\n\u0412 `references\/detection\/,` \u0435\u0441\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0444\u0443\u043d\u043a\u0446\u0438\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0442 \u0443\u043f\u0440\u043e\u0441\u0442\u0438\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0438.\n\n\n\u0421\u043a\u043e\u043f\u0438\u0440\u0443\u0435\u043c \u044d\u0442\u0438 \u0444\u0430\u0439\u043b\u044b \u0438 \u0438\u0445 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u0431\u044b\u043b\u0438 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b \u0432 \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435."}}