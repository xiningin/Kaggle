{"cell_type":{"9c3e069c":"code","6a248f8f":"code","b9f28514":"code","71a99dbc":"code","6e987dfe":"code","3486c19f":"code","f7ee4ffa":"code","aa1737de":"code","3a14c0ab":"code","e740ffd2":"code","b3941856":"code","1b35b985":"code","a87f09c6":"code","1142fbb5":"code","f87c3b1a":"code","4bacbef3":"code","1e732993":"code","0a5864bc":"code","246b2cf0":"code","3c42b9a4":"code","47219858":"code","bb4c6213":"code","f3a944d1":"code","2c6a8ddd":"code","1a162833":"code","ca9d9f49":"code","773e2b0f":"code","ddf12830":"markdown","ecd907c1":"markdown","5242df36":"markdown","43bbb34e":"markdown","b991dae8":"markdown","95151a6a":"markdown","46ef5cbc":"markdown","920a8ff6":"markdown","46c22c24":"markdown","d922872c":"markdown","5c1b4918":"markdown","6e54837c":"markdown","84d64436":"markdown","d897df4a":"markdown","0f5a6d59":"markdown","f653535d":"markdown","035afcc5":"markdown","2ff922d2":"markdown","e50417d0":"markdown","30863317":"markdown","f4f88d76":"markdown","a595ebce":"markdown","ffd46439":"markdown","2cebba2e":"markdown","224f00c2":"markdown","8ce846b5":"markdown"},"source":{"9c3e069c":"import pandas as pd\n\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\nlen(df_train[df_train.Survived == 0])\/len(df_train)","6a248f8f":"df_train.info()","b9f28514":"df_train.describe()","71a99dbc":"df_train.head()","6e987dfe":"sm = pd.plotting.scatter_matrix(df_train, figsize=(15,8))","3486c19f":"df_train[df_train.SibSp == df_train.SibSp.max()]","f7ee4ffa":"fare_hist = df_train.Fare.hist(bins=20)","aa1737de":"df_train[df_train.Fare == df_train.Fare.max()]","3a14c0ab":"df_train.groupby('Pclass').Survived.mean()","e740ffd2":"age_hist = df_train.Age.hist(bins=20)","b3941856":"df_train.Sex.value_counts()","1b35b985":"df_train.groupby('Sex').Survived.mean()","a87f09c6":"pd.crosstab(index=df_train['Sex'], columns=df_train['Pclass'], values=df_train.Survived, aggfunc='mean')","1142fbb5":"df_train = pd.get_dummies(df_train, columns = [\"Sex\"])\ndf_train.shape","f87c3b1a":"df_train['Pclass Sex_female'] = df_train.Pclass * df_train.Sex_female","4bacbef3":"df_train['Title'] = [i[i.find(',') + 1: i.find('.')].strip() for i in df_train.Name]\nage_bytitle_box = df_train.boxplot(column=['Age'], by=['Title'], figsize=(15,8))","1e732993":"df_train['Age'] = df_train['Age'].fillna(df_train.groupby('Title')['Age'].transform('mean'))\ndf_train.Age.isna().sum()","0a5864bc":"from sklearn import preprocessing\n\ndf_train.Age = preprocessing.scale(df_train.Age)\nage_hist = df_train.Age.hist(bins=20)","246b2cf0":"pd.concat([df_train.groupby('Title').Survived.mean().sort_values(ascending=True),\n           df_train.groupby('Title').Title.count()], \n          axis=1)","3c42b9a4":"df_train['IsHonor'] = 0\nhonorable_titles = [\"Capt\", \"Don\", \"Jonkheer\", \"Rev\"]\ndf_train.loc[df_train['Title'].isin(honorable_titles), \"IsHonor\"] = 1\ndf_train[df_train.IsHonor == 1].IsHonor.sum()","47219858":"df_train.loc[df_train.Cabin.isna(), 'Cabin'] = 0\ndf_train.loc[df_train.Cabin != 0, 'Cabin'] = 1\n        \ndf_train['Embarked'] = df_train['Embarked'].fillna('S')\ndf_train = pd.get_dummies(df_train, columns = [\"Embarked\"])\n\ndf_train.info()","bb4c6213":"df_train['FamilySize'] = df_train.SibSp + df_train.Parch","f3a944d1":"child_age_margin = 11\nmother_age_margin = 20\n\ndef is_row_mother(row):\n    if (row.Sex_female == 1 and row.Age > mother_age_margin):\n        return True\n    return False\n\ndf_train['LastName'] = [i[0:i.find(',')] for i in df_train.Name]\ndf_train['MotherChildRelation'] = 0\nfor index, row in df_train.iterrows():\n    if (row.Age < child_age_margin):\n        for index2, row2 in df_train.iterrows():\n            if (row.LastName == row2.LastName):\n                if (is_row_mother(row2)):\n                    if (row2.Survived == 1):\n                        df_train.loc[index, 'MotherChildRelation'] = 1\n                    else:\n                        df_train.loc[index, 'MotherChildRelation'] = -1\n                    if (row.Survived == 1):\n                        df_train.loc[index2, 'MotherChildRelation'] = 1\n                    else:\n                        df_train.loc[index2, 'MotherChildRelation'] = -1","2c6a8ddd":"df_train[\"MotherChildRelation\"].value_counts()","1a162833":"df_train[\"Ticket\"].value_counts()","ca9d9f49":"df_train[df_train[\"Ticket\"] == \"CA. 2343\"].Survived","773e2b0f":"df_train['TicketProbability'] = 0\nfor index, row in df_train.iterrows():\n    ticket = row.Ticket\n    survived_array = []\n    for index2, row2 in df_train.iterrows():\n        if (index != index2 and row.Ticket == row2.Ticket):\n            survived_array.append(row2.Survived)\n    if (len(survived_array) > 0):\n        df_train.loc[index, 'TicketProbability'] = sum(survived_array) \/ len(survived_array)\n    else:\n        df_train.loc[index, 'TicketProbability'] = df_train.Survived.mean()","ddf12830":"#### Name. Age.\nAt first there seems to be no reasonable information coming from the _Name_ feature. However, we have already discovered that the title of a person might be a good estimator for missing _Age_ values.\n\nWe try to extract the title from the _Name_ column with a custom class","ecd907c1":"#### SibSp \/ Parch\n\nTo reduce dimensions we will combine both _SibSp_ and _Parch_ to a new discrete feature calling it _FamilySize_","5242df36":"Of course we have many titles that only contain few people within this group, but let's make an assumption here: _\"In the good old days, there had been more people with honorable character\"_. Looking at 'Rev' or 'Capt' (always sinking with the ship... also when he is actually just a passenger) I would say we have reason to say that these people are somehow \"d'accord\" with dying. We should give them a boost in our model (however it's more like a negative boost, meaning their probability of dying increases). ","43bbb34e":"## Ticket \"Survival\" Probability\n\nAnalogous to the mother \/ child relationship we also think of a certain \"shared destiny\" among people traveling with the same ticket number, as we have already seen that all people with ticket number \"PC 17755\" have survived. Let's inspect the most occuring ticket number.","b991dae8":"Of course this could also just be pure luck to discover 6 dead reverends (e.g. let's take the male deathrate of ~81% to the power of 6 and you end up with a chance ~28%), but I think we could assume that this has an influence, since our assumption is that these people are at peace with themselves.\n\nAt this point we could discuss whether it makes sense to stick with the feature _Sex_ or replace it with something more powerful, including the information _Sex_.\n\nAs we know _Sex_ helps us to divide our dataset into male and females, with males survived roughly 18% and females 72%.\n\n#### Cabin \/ Embarked\n\nAs we have seen in the beginning, there are missing some values for features _Cabin_ and _Embarked_. We will will quickly clean this up and replace each _Cabin_ value with a 1 (and a 0 when value is missing). Furthermore, we will fill up missing _Embarked_ values with the most occuring value, which is \"S\" (Southampton) and directly encode this feature.","95151a6a":"## Data Preprocessing (that will boost you model to the TOP 5%) -> THE INTERESTING STUFF (of this Notebook)\n\n![alt-text](https:\/\/santhalus.de\/img\/kaggle\/titanic\/as2y2.jpg)\n\nSince now we have only perpared our data in a sense like \"Get rid of missing values\", \"Make some easy combinations\" etc. but the next two data preparation steps will really make the difference between a good and an even better model (so this here is the interesting stuff from here on).\n\nUnfortunately, these preparation steps will end up in two different type of training data sets: one for just training and evaluating our model(s) (let's call this our _training data set_ ) and another one for preparing our test dataset in the end (let's call this the _preparation data set_ ). It's important to notice that we can't use the _preparation data set_ for training our models since data points from this data set contain information about their own label (meaning e.g. data point 37 \"Mamee, Mr. Hanna\" has has a feature that includes information about the result of herself, namely that she survived).\n\n## The Mother \/ Child Relationship\n\nAs we mentioned earlier we assume that there might be a special relation between a child and his\/her mother. First let's make an assumption that such a relation is present when we have a young child that is of age 10 or less and that you could be a mother when you are at least 20 years old (in the beginning of the 20th century it was common that women tend to be younger when becoming a mother compared nowadays).\n\nWhat we will do, is to create a new feature that catches the result (survived\/death) of the related child or mother. For that we will search for people with \n- same last name\n- age criteria (child < 11ys, female > 20ys)\n\nOf course we will make errors here, e.g. we could als catch a brother \/ sister relationship, but we will be fine with this, keeping in mind that there also might be a strong sibling relationship.\n\nWe will have three different values for this new feature:\n\n- No mother \/ child relationsip present (0)\n- mother or child died (-1)\n- mother or child survived (1)","46ef5cbc":"Like _Fare_ the feature _Age_ is a continous variable and has a \"very\" nice distribution at first. However we note that very young people are little bit \"overdistributed\" (40 person less than 5 years old) what makes _Age_ look a little bit unnaturally. **We will reshape our age attribute to have a smoother look, as well as refactor it to a similiar scale like _Fare_.**\n\nNow we take a look at the attribute _Sex_.","920a8ff6":"So at a first glance it seems correct that for higher (upper) classes we observe more survived people. Also note that the difference in survival rate between class 1 and 2 as well as class 2 und 3 seems to be nearly identical. If we later think about **preparing our dataset and encoding this feature _Pclass_ we may keep this datatype since it already contains a \"meaningful\" order.**\n\nNow let's take a closer look at _Age_ feature:","46c22c24":"#### Pclass.\nThis feature is already ordinal scaled. We have seen that the assumptions for odinal scaled feature seem to be fulfilled. We just stick with this feature as it is.\n\nHowever, from our exploration we mentioned that it might be useful to have an interaction term between _Sex_ and _Pclass_. So let's quickly add this feature.","d922872c":"# Result \n\nSo finally we came up with some very nice data cleaning and also with two new features _MotherChildRelationship_ and _TicketProbability_. Of course, this is just the beginning of the competition and you also have to check if these new features really have a strong predictive power.\n\nHowever, I hope you like it and maybe share some thoughts about it.","5c1b4918":"We can see that the title groups have different mean ages, with some of them being clearly separated and others being more close together. Now let's replace all NaNs with the mean age for each title.","6e54837c":"Therefore, we obtain 891 datapoints (rows) on 10 features and 1 label (columns). Already note from the info() method that we also have missing values for some features as well as \"non readable\" features like _Ticket, Cabin_ etc. which will need some preparation.\n\nSince we now know our data size and our types of data let's explore the data set and plot same visualizations.\n\n# Data Exploration.\n\nAt first we will have a quick (and dirty) look on a scatter plot of all columns. Also note that for continous variables we will always have a little look at correlation among feautures.","84d64436":"We observe that among all female passengers ~74% survived and among all males ~19% survived. Thinking a little bit longer about this attribute and remembering that we reformulatet our problem to \"Who did reach a life boat?\" I would say that there might be a special group of female passengers, namely \"real ladies\", that are easily able to get to one of the life boats (because of their social status) and get aboard (because women are preferred to get access to the boats).\n\n![alt text](https:\/\/santhalus.de\/img\/kaggle\/titanic\/p01l2w9t.jpg)\n\nStatistically talking I would say there might be an special influence of _Pclass_ on _Sex_ given that we have a female passenger. To check this let's calculate a crosstab and observe the average survival probability in these classes.","d897df4a":"This data transformation is called _Mean Encoding_. Note that we did not include the result of a data point in it's newly created feature (index != index2). However, for creating the test data set we will include the result in each data point (which will be gathered in the _preparation data set_ ), since (like in the mother \/ child relationship) we will lookup the values for the test data set in the _prepation data set_.","0f5a6d59":"Let's examine this new feature _Title_ a little bit more (going back a little bit more to data exploration again). We take a look at the mean survival rate among _Title_ and also display the number of persons with this title.","f653535d":"So there are no NaNs left in the _Age_ feature. However, we can see a little drawback among the 'Miss' title since the age of all Misses nearly covers the whole age range.","035afcc5":"# Data Preparation.\n\nFor data preparation we will create a feature matrix _X_ that will be constructed based on the values from data table _df_train_.\n\n#### Sex.\nFor encoding _Sex_ we will use Scikit-Learns OneHotEncoder to derive **two** dummy columns for male and female (however we will probably make only use of one of them).","2ff922d2":"The people with the highest _Fare_ values also seem to be related since they all have the same _Ticket_ (like the big family from above). Also note that passenger 680 has somehow booked three cabins, what sounds like an error first. However, looking at the deckplan we discover that some suites have multiple _Cabin_ numbers (like the one of Mr. Thomas Cardeza).\n\n![alt text](https:\/\/santhalus.de\/img\/kaggle\/titanic\/B51B53B55.JPG)\n\nAs we mentioned earlier, _Cabin_ has some missing values, as we discover there is no Cabin number for Miss. Anna Ward. We want to make a little remark here, that \"sharing a ticket\" might be quoted as \"sharing your destiny\" because you do not want to leave your friends or family members behind (like the saying \"one for all, all for one\"). **We could later transform this into a new feature accounting for the other people having the same ticket \/ cabin.**\n\nAlso note that _Fare_ ranges from 0 to ~512. **Applying some neural networks or k-nearest neighbours later may require transformation of this feature _Fare_ onto a smaller scale.**\n\nLet's have a look at the attribute _Pclass_. As we have already seen _Pclass_ only takes three values representing first to third class. As someone might guess, people from first class could be preferred to get on a life boat let's quickly check this assumption.","e50417d0":"# Kaggle's Titanic Competition - My Way to TOP5%.\n\nThis is just my first attempt on the titanic learning competition (and I am pretty sure there are some copyright issues from all the memes I use ... however, memes make everything better)\n\nThe objective of this notebook is just to *prepare the data set* in order to create a basis for a very good model for predictions about the survival (encoded 0 = dead, 1 = survived) of passengers on board of the titanic. The result will be a training data set with cleaned up columns and new features that have a very strong predictive power.\n\nAs a short remark: I actually ran all the described steps of this competition in my custom python module. This module can be viewed under https:\/\/github.com\/Kaggle-Train\/Titanic. The repository also contains the necessary steps for creating the test data set as well as performing the model training\/evaluation\/reporting etc. steps. I decided to not include this in this notebook since it would really blow up the length and especially the fun to read the notebook.\n\nSo the main benefit of this notebook is (what at least I think) is:\n\n<p style=\"text-align: center;\"><b>\"How to think of data preparation for real world machine learning competitions.\"<\/b><\/p>","30863317":"The outlier in _SibSp_ direction seems to be one big family. Note that this was a fairly unlucky family since all of them died. We keep in mind that this datapoint (resp. _SibSp_ ) might have a high influence, indicating that people with a high _SibSp_ value are likely to die. We can also identify that there are no values for age for this family present. \nAt this point we should start thinking about how we want to handle this problem when we prepare our dataset. \n\nAn easy way would be to drop all datapoints with _Age_ = NaN. However this would mean that we lose roughly 20% of our datapoints (see 714 rows with _Age_ not Nan from above). Another simple approach would be to use a metric like mean or median for missing values; however, for this particular family \"Sage\" we would all estimate the same age. This doesn't sound reasonable since we know that Master. Thomas Henry Sage is probably a baby and all the Mr. and Mrs. are older. Looking at the whole dataset one can see that the title is always included in _Name_. **Therefore, we should somehow calculate missing values for _Age_ by a new feature called _Title_.**\n\nLet's look at the Fare outlier:","f4f88d76":"We can see, that nearly all of the \"real ladies\" (~97% of 1st and ~92% of 2nd class) have survived. We can think about putting this information into new categorcial variable(s), however I would prefer to account for real ladies as an **interaction between _Sex_ and _Pclass_** in order to prevent overfitting because of too many new variables.\n\nNow, let's think a little bit more about our reformulated problem. In my opinion you only have two options **how** to go aboard of a life boat:\n\n- walking (with your own legs)\n- being carried (by someone else)\n\nSo the question we gonne ask is _\"Who is carrying you aboard of one of these life boats?\"_. If I think about my family and image I had been one of these baby passengers, it would have been probably my mother. I also think that my mother would have searched me until she a) found me or b) ... well I think she would have searched even the ship was already under water. In both cases I would say that I have shared my destiny with my mother's destiny. So let's examine the relationship between a mother and her child aboard of the Titanic.\n\nIn order to do so, we will collect all rows in our data set that have _Age < Age_Child_Treshold_ and examine whether there is another female passenger with same lastname and _Age > Age_Mother_Treshold_ (with tresholds being defined afterwards). However, we will perform this step later in our data preparation part, since we need to fix the missing age values.","a595ebce":"# Prepare Yourselve.\nBefore we start jumping into the data we need to prepare ourselves for the task.\n#### Reformulate the Problem.\nSometimes it is a good idea trying to ask a different question solving the same problem. Answering this new question makes you think about other features that could have an influence and translates the problem into something less abstract.\nAt this point (since the water seemed to be super cold) I think instead of asking **\"Who survived?\"** we will ask ourselves **\"Who did reach a life boat?\"** - knowing that there had been only a handful of people being rescued by two life boats with some of them dying because of hypothermia.\n\n![alt text](https:\/\/santhalus.de\/img\/kaggle\/titanic\/freezing_jack.jpg)\n\n\n#### Define a baseline estimator.\nHaving tedious discussions with my father I learned one important thing when it comes to talking about the results of statistical models (of any kind). Before you start exploring data, prepare everthing, build your model (and all the other checkboxes on the classical road to a statistical model) you should define a benchmark (call it a baseline estimator) you want to beat in order to know \"if it was really worth all the trouble\" in the end. \n\nThe most simple approach here is to look at the fraction of deaths in the training data set. For that we load the train dataset and calculate the fraction of deaths among all passengers in the train dataset.","ffd46439":"At this point, note that we have to prepare our test data set in a different manner. We have to lookup the values from our test set in our (expanded) training set. We cannot mix train and test set since we do not have any labels in our test set. Now let's take a look at our data points with _MotherChildRelation_","2cebba2e":"So if someone classifies all passengers as 0 (dead) he will get an accuracy of **~ 62%** (on the training set). So let's say this is our benchmark which we will be compared with.\n\n![alt text](https:\/\/santhalus.de\/img\/kaggle\/titanic\/Movie_i_see_dead_people-769472.jpg)\n\nBut let's get it on and get our hands on the data.\n\n# Data Retrieving.\nSince we already stored our train dataset in df_train we could have a quick look at the available features and labels now.","224f00c2":"Well this looks like a very unlucky ticket. All of the passengers with this ticket number died. Now, let's create a feature that measures the mean survival quote of each ticket.","8ce846b5":"We discover that we have some outliers for _Fare_ (continous) and _SibSp_ (discrete) with high values."}}