{"cell_type":{"5ecc9e4d":"code","d922150e":"code","b04e49b0":"code","abbe6664":"code","22ae2605":"code","0b06d1d3":"code","669abd11":"code","67eac752":"code","5e43f3e7":"code","e04a60ae":"code","ee40412e":"code","800c43a8":"code","4e71d29d":"code","717735d3":"code","d5cb1dea":"markdown","5e3a02e0":"markdown","eaa2a284":"markdown","7ee21b00":"markdown","324b1838":"markdown","f2f72ecf":"markdown","b980a6ff":"markdown","f36cd460":"markdown","07488482":"markdown","689c311a":"markdown","26ac8f6c":"markdown","333f20c2":"markdown"},"source":{"5ecc9e4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d922150e":"#This is requiered to run Tensroflow HUB\n!pip install -q tensorflow-text\n","b04e49b0":"import numpy as np \nimport pandas as pd \nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport time\nimport numpy as np\nimport tensorflow_hub as hub\nimport tensorflow_text","abbe6664":"use = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\")","22ae2605":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","0b06d1d3":"# Function to embeed the tweets with the Universal Sentence Encoder\ndef embedd(dataset):\n    data = []\n    for i in tqdm(dataset):\n      embeddings = use(i)\n      embeddings = tf.reshape(embeddings, [-1]).numpy()\n      data.append(embeddings)\n    return data","669abd11":"#First we need to reshape our data, the version 4th of the USE worked like use(\"sentence to embeed\") but in the 5th version you need to pass a list as \n# a parameter like: use([\"sentence to embeed\"]), for that reason we are reshaping our data\nx_training = np.reshape(train_df.text.values,(len(train_df.text.values),1))\nx_topredict = np.reshape(test_df.text.values,(len(test_df.text.values),1))\n\nprint(\"Embedding training data...\")\nx_training = embedd(x_training)\nprint(\"Embedding data for prediction...\")\nx_topredict = embedd(x_topredict)","67eac752":"x_training = np.array(x_training)\ny_train = train_df.target.values\nx_topredict = np.array(x_topredict)","5e43f3e7":"X_train, X_test, y_train, y_test = train_test_split(x_training, y_train,test_size = 0.25, random_state=7)","e04a60ae":"from sklearn import feature_extraction, linear_model, model_selection, preprocessing\nclf = linear_model.RidgeClassifier()\nclf.fit(X_train, y_train)","ee40412e":"from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\ny_pred = clf.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"The accuracy of the model in the tested data is: \",accuracy_score(y_test, y_pred))\nprint(\"The f1 score of the model in the tested data is: \",f1_score(y_test, y_pred))","800c43a8":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier()\nsvm_clf = SVC()\n\nlog_clf.fit(X_train, y_train)\nrnd_clf.fit(X_train, y_train)\nsvm_clf.fit(X_train, y_train)","4e71d29d":"\"\"\"###Logstic regression###\"\"\"\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score\ny_pred = log_clf.predict(X_test)\ny_pred = (y_pred > 0.5)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"The accuracy of the model Logistic regression in the tested data is: \",accuracy_score(y_test, y_pred))\nprint(\"The f1 score of the model Logistic regression in the tested data is: \",f1_score(y_test, y_pred))\n\n\n\"\"\"###Random Forest###\"\"\"\ny_pred = rnd_clf.predict(X_test)\ny_pred = (y_pred > 0.5)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"The accuracy of the model Random Forest in the tested data is: \",accuracy_score(y_test, y_pred))\nprint(\"The f1 score of the model Random Forest in the tested data is: \",f1_score(y_test, y_pred))\n\n\"\"\"###SVC kernel###\"\"\"\ny_pred = svm_clf.predict(X_test)\ny_pred = (y_pred > 0.5)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"The accuracy of the model SVC in the tested data is: \",accuracy_score(y_test, y_pred))\nprint(\"The f1 score of the model SVC in the tested data is: \",f1_score(y_test, y_pred))","717735d3":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\ny_pred = svm_clf.predict(x_topredict)\ny_pred = (y_pred > 0.5).astype(int)\nsample_submission[\"target\"] = y_pred\nsample_submission.to_csv(\"submission_SVC_universal_sentence_encoder.csv\", index=False)","d5cb1dea":"# Base libraries ","5e3a02e0":"## Lets try first with the RidgeRegression algorith","eaa2a284":"# Process","7ee21b00":"#### As we can see the SVC model was the one that performed better, so lets submitt that one to see our score","324b1838":"### Splitting the data","f2f72ecf":"#### As we can see we get an accuracy of 0.8156 and f1 of 0.77, this is a really good score for a simple linear model and without cleaning the data","b980a6ff":"#### Create a confusion matrix to test the data","f36cd460":"### Get the data and preprare de function for the USE","07488482":"**We will use the raw data, without preprocessing (without cleaning). For this project, I have noticed that cleaning the data does not greatly improve model accuracy, like 1% or less, and in this case we will use simple machine learning models. I did the experiment and after spending hours cleaning the data and doing a super complex deep learning model, I was able to improve the score by only 2%, then to be able to get 80% accuracy without cleaning and with simple models, I think it's a good achievement**\n\nThere is a good notebook that perfoms an incredible cleaning procees and also explain the visualization of the data\nhttps:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert","689c311a":"### We will be using the universal sentence encoder large (USE), the 5th version, so lets initiate the connection","26ac8f6c":"# Now lets try with some ML algorithms from the Scikit Learn librarie","333f20c2":"# This is a simple notebook showing how to use the universal sentence encoder with some basic algorithms like SVC, Ridge Regression, Gaussian Kernel and some more. "}}