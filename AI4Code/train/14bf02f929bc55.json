{"cell_type":{"00e7bc71":"code","096fe314":"code","899e915a":"code","7599c51f":"code","eceed6ba":"code","0970637d":"code","20a26592":"code","99a1b227":"code","77cecf5d":"code","504d76de":"code","3f920c8d":"code","2173d9e3":"code","c792ad23":"code","0ac4d7ff":"code","4f697c58":"code","8574cafe":"code","64fcffcb":"code","17e047fd":"code","b653664a":"code","388e32c4":"code","ec0f9d93":"code","5c2641cb":"code","b629a68c":"code","4e6bfc5a":"code","aeddfa18":"markdown","473f8dc5":"markdown","b78bc226":"markdown","279c8cbc":"markdown"},"source":{"00e7bc71":"#let's imprt the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\nimport glob","096fe314":"# Get all the files saved into a list and then iterate over them like below to extract relevant information\n# hold this information in a dataframe and then move forward from there. ","899e915a":"#Creating an empty dataframe with only column names to fill it with files content\ndf = pd.DataFrame(columns=['Doc_ID', 'Title', 'Text', 'Source'])\n\n","7599c51f":"df","eceed6ba":"#Grabbing the files from the repositories using glob library\n\njson_filenames = glob.glob(f'\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/**\/**\/*.json', recursive=True)","0970637d":"#Taking a look at the first 10 filenames path ","20a26592":"json_filenames[:10]","99a1b227":"# Now we just iterate over the files and populate the data frame. ","77cecf5d":"def get_df(json_filenames, df):\n\n    for file_name in json_filenames:\n\n        row = {\"Doc_ID\": None, \"Title\": None, \"Text\": None, \"Source\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n            \n            #getting the column values for this specific document\n            row['Doc_ID'] = data['paper_id']\n            row['Title'] = data['metadata']['title']            \n            body_list = []\n            for _ in range(len(data['body_text'])):\n                try:\n                    body_list.append(data['body_text'][_]['text'])\n                except:\n                    pass\n\n            body = \" \".join(body_list)\n            row['Text'] = body\n            \n            # Now just add to the dataframe. \n            row['Source'] = file_name.split(\"\/\")[5]\n            \n            df = df.append(row, ignore_index=True)\n    \n    return df","504d76de":"corona_dataframe = get_df(json_filenames, df)","3f920c8d":"corona_dataframe.shape","2173d9e3":"corona_dataframe.head()","c792ad23":"corona_dataframe.tail()","0ac4d7ff":"output = corona_dataframe.to_csv('kaggle_CORD-19_csv_format.csv')","4f697c58":"# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English","8574cafe":"nlp = spacy.load('en_core_web_lg')","64fcffcb":"#Here's we'll visualize the extraction of entities from some text in the dataframe generated previously","17e047fd":"#NER extraction using Spacy library\ndoc = nlp(corona_dataframe[\"Text\"][10])\nspacy.displacy.render(doc, style='ent',jupyter=True)","b653664a":"#Loading the necessary libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Load the regular expression library\nimport re\nfrom wordcloud import WordCloud\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# Load the LDA model from sk-learn\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA","388e32c4":"#Let's first clean our text datas with some basic operations \n#It may be improved more and more \n\n#Remove punctuation\ncorona_dataframe['Text'] = corona_dataframe['Text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n#Convert to lowercase\ncorona_dataframe['Text'] = corona_dataframe['Text'].map(lambda x: x.lower())\n#Print out the first rows of papers\ncorona_dataframe['Text'].head()","ec0f9d93":"#Let's have an idea about what reveal the titles of the papers\n\n\n#Join the different processed titles together.\nlong_string = ','.join(list(corona_dataframe['Title'].values))\n#Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n#Generate a word cloud\nwordcloud.generate(long_string)\n#Visualize the word cloud\nwordcloud.to_image()","5c2641cb":"#Let's take a look at the distribution of the most significant words of the text corpus\n\n#Helper function\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n    \n    plt.figure(2, figsize=(15, 15\/1.6180))\n    plt.subplot(title='10 most common words')\n    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n    sns.barplot(x_pos, counts, palette='husl')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.show()\n#Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n#Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(corona_dataframe['Text'])\n#Visualise the 10 most common words\nplot_10_most_common_words(count_data, count_vectorizer)","b629a68c":"#LDA model training and results visualization\n#To keep things simple, we will only tweak the number of topic parameters.\n#The first 5 topics mention the most meaningful terms related in the text of all papers \n \n#Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n#Tweak the two parameters below\nnumber_topics = 5\nnumber_words = 10\n#Create and fit the LDA model imported from sklearn library\nlda = LDA(n_components=number_topics, n_jobs=1)\nlda.fit(count_data)\n#Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)","4e6bfc5a":"#I hope you enjoy it ","aeddfa18":"**NER extraction from Text**","473f8dc5":"This is a starting kernel to convert the documents into a meaningful dataframe and visualize texts and the most meaningful words from it to serve for the further analysis\n\n\n\n\n\n\n","b78bc226":"**Generating a dataframe from the documents (json files)**","279c8cbc":"**LDA Topic modeling**"}}