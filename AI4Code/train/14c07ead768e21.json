{"cell_type":{"fb3654f4":"code","b425fcd3":"code","1e259318":"code","a392d6dc":"code","0dc15e83":"code","2c9ac4ab":"code","6a2e1c84":"code","90f87a3e":"code","fe6a6ff1":"code","068572c8":"code","ef1ed349":"code","324da071":"code","b65a8c0f":"code","c0f2b654":"code","1e18ff5b":"code","473f9616":"code","7453de9d":"code","42c5ee96":"code","bc4653dd":"code","af5ef297":"code","31373b6a":"code","5684cb84":"code","e937e81a":"code","378b41b7":"code","dbd1f85c":"code","06856899":"code","cbf5c118":"code","daf27d97":"code","f17a2100":"code","3b7722ed":"code","328b68a7":"code","31a24bfc":"code","a2f7f21c":"code","0a2cfb91":"code","d09b1a2b":"code","0435e344":"code","aeca86c2":"code","7b8d25a1":"code","7c212dea":"code","04cca42e":"code","f6c8b27a":"code","32295dc8":"markdown","54be1074":"markdown","d2c10504":"markdown","74522246":"markdown","c9c97f01":"markdown","a4b186fe":"markdown","7393fc49":"markdown","2b217b62":"markdown","6aa666fb":"markdown","760f831f":"markdown","533cdf76":"markdown","c1bb96e6":"markdown","c7140834":"markdown","3e362e70":"markdown","9fcb8c31":"markdown","da9d8a9a":"markdown","ffacdf40":"markdown","6b48e1e4":"markdown","cc1d5861":"markdown","cb93c79e":"markdown","b5c8368a":"markdown","1bf5a817":"markdown","8a940786":"markdown","443d39ef":"markdown","29e712cc":"markdown","98fd0508":"markdown","828af70c":"markdown","8067ad26":"markdown","a419279e":"markdown","cd07b0ff":"markdown","6d833603":"markdown","c205393f":"markdown","e1fa0729":"markdown","79efc038":"markdown","bf8a08aa":"markdown","1e4a0b45":"markdown","f04ae19e":"markdown"},"source":{"fb3654f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom  xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import resample\nfrom scipy import stats\nfrom sklearn import metrics\nfrom catboost import CatBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b425fcd3":"allHyperTest = pd.read_csv(\"..\/input\/hypothyroid-multi-dataset\/allhyperTestEDIT.CSV\")\nallHyperTrain = pd.read_csv(\"..\/input\/hypothyroid-multi-dataset\/allhyperTrainEDIT.CSV\")\nallHypoTest = pd.read_csv(\"..\/input\/hypothyroid-multi-dataset\/allhypoTEST.csv\")\nallHypoTrain = pd.read_csv(\"..\/input\/hypothyroid-multi-dataset\/allhypoDATA.CSV\")\n\ndisplay(allHypoTest.head(10))\ndisplay(allHypoTrain.dtypes)","1e259318":"def handleDuplicated(df):\n    if df[\"ID\"].duplicated().sum() == 0 :\n        print(\"There aren't duplicates\")\n    elif (df[\"ID\"].duplicated().sum()) < len(df) \/ 100:\n        df[\"ID\"].drop_duplicates(keep=\"first\", inplace=True)\n        print(\"duplicates were less than the 1% of all the data, they have been dropped\")\n    else:\n        index_duplicated = df[\"ID\"].duplicated().index\n        print(\"duplicates are more than the 1% of all the data, they have been preserved\")\n        print(index_duplicated)\n\nhandleDuplicated(allHyperTest)\nhandleDuplicated(allHyperTrain)\nhandleDuplicated(allHypoTest)\nhandleDuplicated(allHypoTrain)","a392d6dc":"del allHyperTest[\"ID\"]\ndel allHyperTrain[\"ID\"]\ndel allHypoTest[\"ID\"]\ndel allHypoTrain[\"ID\"]","0dc15e83":"def notCorrect_TargetFilter(df,correct_Target,target):\n    df = df[df.Target.isin(correct_Target)]\n    df.replace(correct_Target,target,inplace = True)\n    return df\n    \nallHyperTest = notCorrect_TargetFilter(allHyperTest,[\"hyperthyroid\",\"T3_toxic\",\"goitre\",\"secondary_toxic\"],\"hyperthyroid\")\nallHyperTrain = notCorrect_TargetFilter(allHyperTrain,[\"hyperthyroid\",\"T3_toxic\",\"goitre\",\"secondary_toxic\"],\"hyperthyroid\")\nallHypoTest = notCorrect_TargetFilter(allHypoTest,[\"hypothyroid\", \"primary_hypothyroid\", \"compensated_hypothyroid\", \"secondary_hypothyroid\"],\"hypothyroid\")\nallHypoTrain = notCorrect_TargetFilter(allHypoTrain,[\"hypothyroid\", \"primary_hypothyroid\", \"compensated_hypothyroid\", \"secondary_hypothyroid\"],\"hypothyroid\")","2c9ac4ab":"allDataset = pd.concat([allHyperTest,allHyperTrain,allHypoTest,allHypoTrain], ignore_index = True)\ndisplay(allDataset.shape)","6a2e1c84":"thyroid0387 = pd.read_csv(\"..\/input\/hypothyroid-multi-dataset\/thyroid0387EDIT.CSV\")\ndisplay(thyroid0387.head(10))\ndisplay(thyroid0387.dtypes)","90f87a3e":"handleDuplicated(thyroid0387)","fe6a6ff1":"del thyroid0387[\"ID\"]","068572c8":"thyroid0387['sex'] = thyroid0387['sex'].map({'F': 1, 'M': 0})\n\nthyroid0387.replace(['A','B','C','D'],\"hyperthyroid\",inplace = True)\nthyroid0387.replace(['E','F','G','H'],\"hypothyroid\",inplace = True)\n\nfor value in set(thyroid0387['Target']):\n    if(value != 'hypothyroid' and value != 'hyperthyroid'):\n        thyroid0387.replace(value,'negative',inplace=True)","ef1ed349":"hypothyroid = pd.read_csv(\"..\/input\/hypothyroid-multi-dataset\/hypothyroid.csv\")\ndisplay(hypothyroid.shape)\ndisplay(hypothyroid.head(10))\ndisplay(hypothyroid.dtypes)","324da071":"hypothyroid = hypothyroid.rename(columns={hypothyroid.columns[0]:\"Target\",hypothyroid.columns[1]:\"age\",hypothyroid.columns[2]:\"sex\" })\nhypothyroid = hypothyroid[hypothyroid.Target.isin(['hypothyroid'])]","b65a8c0f":"sick_euthyroid = pd.read_csv(\"..\/input\/hypothyroid-multi-dataset\/sick-euthyroid.CSV\")\ndisplay(sick_euthyroid.shape)\ndisplay(sick_euthyroid.head(10))\ndisplay(sick_euthyroid.dtypes)","c0f2b654":"sick_euthyroid = sick_euthyroid[sick_euthyroid.Target.isin(['negative'])]\ndisplay(sick_euthyroid.shape)","1e18ff5b":"ann_train = pd.read_csv(\"..\/input\/hypothyroid-multi-dataset\/ann-train.CSV\")\nann_test = pd.read_csv(\"..\/input\/hypothyroid-multi-dataset\/ann-test.CSV\")\ndisplay(ann_test.head(10))\ndisplay(ann_test.dtypes)","473f9616":"target1 = pd.Series(ann_test[ann_test.columns[-1]].values)\ndisplay(target1.value_counts())\ntarget2 = pd.Series(ann_train[ann_train.columns[-1]].values)\ndisplay(target2.value_counts())","7453de9d":"print(\"Sex thyroid0387 1=F,0=M:\")\nsex_series1 = pd.Series(thyroid0387[thyroid0387.columns[1]].values)\ndisplay(sex_series1.value_counts())\nprint(\"Sick-euthyroid:\")\nsex_series2 = pd.Series(sick_euthyroid[sick_euthyroid.columns[2]].values)\ndisplay(sex_series2.value_counts())","42c5ee96":"sex1 = pd.Series(ann_test[ann_test.columns[1]].values)\ndisplay(sex1.value_counts())\nsex2 = pd.Series(ann_train[ann_train.columns[1]].values)\ndisplay(sex2.value_counts())","bc4653dd":"for column in ann_train.columns:\n    listOfValues=set(ann_train[column])\n    print(column,\": \",listOfValues)","af5ef297":"ann = pd.concat([ann_train,ann_test], ignore_index = True)\nann['sex'] = ann['sex'].map({0:'F',1:'M'})\nann['Target'] = ann['Target'].map({3:'negative',2:'hypothyroid',1:'hyperthyroid'})\n\ncontinuos_attributes = ['age','TSH','T3','TT4','T4U','FTI']\nfor attribute in continuos_attributes:\n    ann[attribute] = ann[attribute] * 100\n\ndef fillNewAttributes(row,attribute):\n    if row[attribute] > 0:\n        return 'y'\n    else:\n        return 'n'\n\nann['TSH_measured'] = ann.apply(lambda row: fillNewAttributes(row,'TSH'), axis=1)\nann['T3_measured'] = ann.apply(lambda row: fillNewAttributes(row,'T3'), axis=1)\nann['TT4_measured'] = ann.apply(lambda row: fillNewAttributes(row,'TT4'), axis=1)\nann['T4U_measured'] = ann.apply(lambda row: fillNewAttributes(row,'T4U'), axis=1)\nann['FTI_measured'] = ann.apply(lambda row: fillNewAttributes(row,'FTI'), axis=1)\ndisplay(ann.dtypes)","31373b6a":"data = pd.concat([allDataset,thyroid0387,hypothyroid,sick_euthyroid,ann], ignore_index = True)\ndisplay(data.shape)\ndisplay(data.dtypes)","5684cb84":"for column in data.columns:\n    listOfValues=set(data[column])\n    print(column,\": \",listOfValues)","e937e81a":"data=data.replace({\"?\":np.NAN})\ndata.isna().sum()","378b41b7":"del data['TBG']\ndel data['referral_source']\ndel data['TBG_measured']\ndel data['sex']","dbd1f85c":"data.dropna(axis = 0, thresh = 20, inplace = True)\ndata.isna().sum()","06856899":"data = data.replace({\"t\":1,\"f\":0, \"y\":1, \"n\":0, \"hypothyroid\":1, \"negative\":0,\"hyperthyroid\":2, \"F\":1, \"M\":0})\ndisplay(data.dtypes)","cbf5c118":"cols = data.columns[data.dtypes.eq('object')]\ndata[cols] = data[cols].apply(pd.to_numeric, errors='coerce')\ndisplay(data.dtypes)","daf27d97":"corr_values = abs(data[data.columns[0:]].corr()['Target'][:])\ncorr_values = corr_values.drop('Target')\ncorr_values = corr_values[corr_values > 0.04]\ndisplay(corr_values)","f17a2100":"def holdout(dataframe):\n  x = dataframe[corr_values.index]\n  y = dataframe['Target']\n  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42) \n  return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = holdout(data)","3b7722ed":"classifiers = {\n    \"XGBClassifier\" : XGBClassifier(learning_rate=0.01),\n    \"CatBoostClassifier\" : CatBoostClassifier(max_depth=4,verbose=0),\n}","328b68a7":"def classification(classifiers, X_train, X_test, y_train, y_test):\n    # Creo un dataframe per visualizzare i risultati calcolati\n  res = pd.DataFrame(columns=[\"Classifier\", \n                                \"Accuracy\", \n                                \"Precision\", \n                                \"Recall\", \n                                \"FScore\"])\n  for name, clf in classifiers.items():\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            pr, rc, fs, sup = metrics.precision_recall_fscore_support(y_test, y_pred, average='macro')\n            res = res.append({\"Classifier\": name,\"Accuracy\": round(metrics.accuracy_score(y_test, y_pred), 4),\n                              \"Precision\": round(pr, 4), \"Recall\":round(rc, 4), \"FScore\":round(fs, 4)}, ignore_index=True)\n            print(\"Confusion matrix for: \", name)\n            display(confusion_matrix(y_test, y_pred))\n  res.set_index(\"FScore\", inplace=True)\n  res.sort_values(by=\"FScore\", ascending=False, inplace=True)   \n  return res\n\ndisplay(classification(classifiers, X_train, X_test, y_train, y_test))","31a24bfc":"display(data.shape)\ndata.Target.value_counts()","a2f7f21c":"data1 = data.interpolate(method = 'spline', order = 3)\ndisplay(data1.isna().sum())","0a2cfb91":"classifiers1 = {\n    \"XGBClassifier\" : XGBClassifier(learning_rate=0.01),\n    \"CatBoostClassifier\" : CatBoostClassifier(max_depth=4,verbose=0),\n    \"Nearest Neighbors\" : KNeighborsClassifier(4),\n    \"Decision Tree\" : DecisionTreeClassifier(class_weight = 'balanced'),\n    \"Random Forest\": RandomForestClassifier(class_weight = 'balanced',random_state = 1),\n    \"ExtraTrees\": ExtraTreesClassifier(class_weight = 'balanced',random_state = 1),\n    \"MLPClassifier\": MLPClassifier(hidden_layer_sizes=(256,128,64,32),activation=\"relu\",random_state=1)\n}","d09b1a2b":"corr_values = abs(data1[data1.columns[0:]].corr()['Target'][:])\ncorr_values = corr_values.drop('Target')\ncorr_values = corr_values[corr_values > 0.04]\ndisplay(corr_values)\n\nX_train1, X_test1, y_train1, y_test1 = holdout(data1)\n\ndisplay(classification(classifiers1,X_train1, X_test1, y_train1, y_test1))","0435e344":"def fdiscretizer(attribute,dataframe):\n    enc = LabelEncoder()\n    dataframe[attribute] = pd.qcut(dataframe[attribute], 20, duplicates='drop')\n    dataframe[attribute] = enc.fit_transform(dataframe[attribute])\n    dataframe = dataframe.convert_dtypes(convert_integer=True)\n\ndata2 = data1.copy()\nfdiscretizer('age',data2)\nfdiscretizer('TSH',data2)\nfdiscretizer('T3',data2)\nfdiscretizer('TT4',data2)\nfdiscretizer('T4U',data2)\nfdiscretizer('FTI',data2)","aeca86c2":"corr_values = abs(data2[data2.columns[0:]].corr()['Target'][:])\ncorr_values = corr_values.drop('Target')\ncorr_values = corr_values[corr_values > 0.04]\ndisplay(corr_values)\n\nX_train2, X_test2, y_train2, y_test2 = holdout(data2)\n\ndisplay(classification(classifiers1,X_train2, X_test2, y_train2, y_test2))","7b8d25a1":"data3 = ((data1-data1.min())\/(data1.max()-data1.min()))*20\n\ncorr_values = abs(data3[data3.columns[0:]].corr()['Target'][:])\ncorr_values = corr_values.drop('Target')\ncorr_values = corr_values[corr_values > 0.04]\ndisplay(corr_values)\n\nX_train3, X_test3, y_train3, y_test3 = holdout(data3)\n\ndisplay(classification(classifiers1,X_train3, X_test3, y_train3, y_test3))","7c212dea":"smote = SMOTE('not majority',random_state = 1)\nX_train_sm, y_train_sm = smote.fit_sample(X_train3,y_train3)\nX_test_sm, y_test_sm = smote.fit_sample(X_test3,y_test3)\ndisplay(X_train3.shape)\ndisplay(X_train_sm.shape)\ndisplay(classification(classifiers1,X_train_sm, X_test_sm, y_train_sm, y_test_sm))","04cca42e":"df_negative = data3[data3.Target==0]\ndf_hyperthyroid = data3[data3.Target==20]\ndf_hypothyroid = data3[data3.Target==10]\n\ndf_negative_downsampled = resample(df_negative,replace=False,n_samples=450,random_state=123)\ndf_hypothyroid_downsampled = resample(df_hypothyroid,replace=False,n_samples=450,random_state=123)\n\ndf_downsampled = pd.concat([df_negative_downsampled,df_hypothyroid_downsampled,df_hyperthyroid])\ndf_downsampled.Target.value_counts()","f6c8b27a":"X_train4, X_test4, y_train4, y_test4 = holdout(df_downsampled)\ndisplay(classification(classifiers1,X_train4, X_test4, y_train4, y_test4))","32295dc8":"### The 'Unnamed' attribute indicate the class of the istance, so I have to rename it. Then I will filter the 'hypothyroid' class istances. For this dataset I don't have 'I131_treatment', 'hypopituitary', 'psych' and 'referral_source' attributes.","54be1074":"### Now I will merge the four datasets:","d2c10504":"### The 'TBG', 'referral_source' and 'TBG_measured' attributes have too many nan values, I have to drop them. Let's try to drop the 'sex' attribute too:","74522246":"### Now it's time to train the classifiers and discuss the results:","c9c97f01":"### ID is an identificator, so I have to check if there are any istances with the same value for this attribute:","a4b186fe":"### I don't have 'measured' attributes, the 'TBG' and the 'referral_source' attributes. I should create the 'measured' attributes basing on the other columns.","7393fc49":"### That's all for the 'all' series. Let's go on with thyroid0387:","2b217b62":"### For this dataset I don't have 'I131_treatment', 'hypopituitary', 'psych' and 'referral_source' attributes.","6aa666fb":"# Part 1: Data Integration\n### I have to integrate these six different datasets. I will start from the 'all' series because they have the same scheme. ","760f831f":"### Sometimes '?' has been used  instead of 'nan', so before counting how many nans are present, I need to do a substitoution:","533cdf76":"### Another thing that I have to do is to divide the dataset into two sets: the training set and the testing set.","c1bb96e6":"### Let's continue with the 'hypothyroid' dataset:","c7140834":"### For 'sick-euthyroid' I have to filter all the 'negative' istances:","3e362e70":"# Part 3: training of the classifiers\n### Before the training starts, I have to find the attributes most related to the target:","9fcb8c31":"### This dataset has different interesting classes: A,B,C,D,E,F,G,H. All the others should be considered as 'negative'. I have to be careful because 'F' and 'M' are used in the 'sex' attribute too, so before any sostitution, I have to handle this problem:","da9d8a9a":"### So, there are more female than male patients in these datasets. Looking at the \"ann\" series I got:","ffacdf40":"### Looking at the distribuition of the values for the 'Target' attribute, we can understand that:\n* 3 is referring to the 'negative' class\n* 2 is referring to the 'hypothyroid' class\n* 1 is referring to the 'hyperthyroid' class\n\n### I should analyze the distribuition of the sex attribute in the other datasets to understand how I should treat it in the 'ann' series:","6b48e1e4":"### Now 'T3' is one of the most related attributes, but there aren't major changes in the results. FScore got slightly worse.  Maybe it's possible to apply a normalization istead of a discretization:","cc1d5861":"### As we can see, 'data' is rather unbalanced, so the accurancy isn't a very good metric. I have a serious problem with the third class. I should try some alternatives transformation and see how the results change.","cb93c79e":"# Introduction\n### In this notebook I want to study thyroid problems and I want to train and prepare some classifiers that could recognize any kind of thyroid problem in a patient.<br>To do this, I will use six datasets I got from -> https:\/\/archive.ics.uci.edu\/ml\/datasets\/Thyroid+Disease:\n* allhyperTestEDIT and allhyperTrainEDIT present classes'hyperthyroid','T3 toxic','goitre','secondary toxic' and 'negative'\n* allhypoDATA and allhypoTEST present classes 'hypothyroid','primary hypothyroid','compensated hypothyroid','secondary hypothyroid' and 'negative'\n* hypothyroid present classes 'hypothyroid' and 'negative'\n* sick-euthyroid present classes 'sick-euthyroid' and 'negative'\n* thyroid0387 present classes hyperthyroid conditions (A, B, C, D), hypothyroid conditions (E, F, G, H), binding protein (I, J), general health (K), replacement therapy (L, M, N), discordant results (R) \n* ann-test and ann-train present classes normal (not hypothyroid), hyperfunction and subnormal functioning\n\n### <br> You can find the full documentation on the link above. I want to build a dataset, merging these six above, which present only three classes: hypothyroid, hyperthyroid and negative. Once this work is done, I will go on with the data pre processing and then I will train and test the classifiers.","b5c8368a":"# Part 4: Alternative transformations\n### First of all, I will try to fill the nan values with the spline interpolation:","1bf5a817":"### Now I will define the classifiers that I'm going to use. I need some classifiers that are friendly with nan values:","8a940786":"### So, we got a huge improvement on all the metrics' score, except for Accuracy. But now that the data is balanced, it is a valid metric too. Now it's time to try an under-sampling method: ","443d39ef":"### So, we can see that the results haven't changed that much. The most related attributes are the same and the FScore improved slightly. Let's check if there are any differences after a discretization:","29e712cc":"### Now I have a new dataset, so I have to repeat all the steps previous to the evalutation of the results. Also, now that there aren't nan values, it's possible to use more classifiers:","98fd0508":"### Now 'T3' has gone again and we can see a tiny improvement from the last time. To obtain better results, I should try to balance the dataset by over-sampling (by adding more samples from under-represented classes) or by under-sampling (by removing samples from over-represented classes). Let's start with an over-sampling method:","828af70c":"### We have the ID attribute here too, so:","8067ad26":" ### Now it's possible to drop the ID attribute because it's useless for the classification:","a419279e":"### From these four datasets I will obtain all the istances presentig a class that's different from 'negative':","cd07b0ff":"# Part 2: Data pre processing\n### I will start the data pre processing observing the set of possible values for each attribute:","6d833603":"### Now it's time to work on the \"ann\" series:","c205393f":"### I can have maximum nine nan values in a row, so I will drop all the rows wtih more than five nan values because they present very few data and aren't good enough for the classification:","e1fa0729":"### Using the same attributes, with the under-sampling of the dataset we got very good results, even if there weren't that many rows.","79efc038":"# Part 5: Final Discussion\n### The final dataset that I got is very umbalanced, that's true, but it's normal because only a small percentage of the world population suffers of thyroid disease. Nevertheless, thanks to a good pre-elaboration of the data, I got some very accurate classifiers, that have a good FScore too. I could handle the nan values beacause the results didn't get much worse, and even with the normalization and the discretization they didn't change that much. After having balanced the normalized dataset, we got the best results of the notebook, this means that the work that had been done before was pretty good. In the future, it would be interesting to continue these studies hoping to use more data.","bf8a08aa":"### Now I can merge all the datasets in one:","1e4a0b45":"### I can assume that '0' refers to female patients and '1' refers to male patients. Another important things to do is to multply for 100 all the continuos and numerical attributes and to add the 'measured' attributes.","f04ae19e":"### For the classification is important that the dataset only has numerical attributes, so I have to encode the categorical values into numerical values:"}}