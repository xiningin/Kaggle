{"cell_type":{"3dd02809":"code","17337cce":"code","05187598":"code","40f7560f":"code","7fa76759":"code","415315e5":"code","2110d7ac":"code","472fa775":"code","23fdac05":"code","36bbe3f7":"code","2eefed7b":"code","26eb2ede":"code","862a9803":"code","a45132da":"code","28bf7aa1":"code","7e18e5ca":"code","34ed860e":"code","869de021":"code","0a556a46":"code","e0392658":"code","7f2416a1":"code","6c019a1b":"code","eb36e116":"code","cde1af46":"code","b0a551f2":"code","9dfdf88a":"code","816eb462":"code","f7289c9b":"code","e16f05a6":"code","5646e6fa":"code","6c66e028":"code","06d8760a":"code","7ffb87fa":"code","43d2d0ef":"code","bf52baa6":"code","621261a3":"code","1c0de621":"code","0035ac23":"code","fcb7b66f":"code","ad523ea1":"code","863c2579":"code","3d080dd6":"code","359a9b53":"code","dcca7a58":"code","22f0c1d8":"code","c7ea68bf":"code","d7e4684c":"code","bbb18a0d":"code","376861f6":"code","1fba1f91":"code","2d67436f":"code","722c475a":"code","d40c08b7":"code","52e26813":"code","2a0d2946":"code","946490ac":"code","fd1f38df":"code","1b892f47":"code","fa9da4b5":"code","6987c9bd":"code","d5a4388d":"code","408edd7f":"code","af0a08a4":"code","d3b77a96":"code","cc6c7edb":"code","e2ebb895":"code","9483b9c3":"code","9369d836":"markdown","46c6aee3":"markdown","87506f98":"markdown","bccc01f8":"markdown","88dff48c":"markdown","1ca9aae6":"markdown","d10831a1":"markdown","1b58b151":"markdown","809ce926":"markdown","4b6e0be6":"markdown","ba566928":"markdown","c6eb821b":"markdown","fd207cbc":"markdown","9fd49983":"markdown"},"source":{"3dd02809":"# general libraries\nimport os,gc\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# sklearn - metric, train test split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\ninput_path = '..\/input\/analytics-vidya-ltfs-finhack-3\/'","17337cce":"#Data Dictionary\ndata_dict = pd.read_csv(input_path+'ltfs3_demographics_dictionary.csv')\ntrain_dict = data_dict.T.iloc[:2,:]\nbureau_dict = data_dict.T.iloc[2:,:]","05187598":"train_dict.T","40f7560f":"bureau_dict.T","7fa76759":"train_bureau = pd.read_csv(input_path+\"ltfs3_train_bureau.csv\")\npd.set_option('display.max_colwidth',None)\ntrain_bureau.head()","415315e5":"test_bureau = pd.read_csv(input_path+\"ltfs3_test_bureau.csv\")\npd.set_option('display.max_colwidth',None)\ntest_bureau.head()","2110d7ac":"train_bureau = train_bureau.drop_duplicates()\ntest_bureau  = test_bureau.drop_duplicates()","472fa775":"# add extra column for 'train' and 'test'\ntrain_bureau['source'] = 'train'\ntest_bureau['source']  = 'test'","23fdac05":"train_bureau.columns","36bbe3f7":"for col in train_bureau.columns:\n    if train_bureau[col].dtype=='object':\n        print(train_bureau[col].value_counts())\n        print('')","2eefed7b":"bureau = pd.concat([train_bureau, test_bureau])","26eb2ede":"del train_bureau,test_bureau\ngc.collect()","862a9803":"# ACCT-TYPE\nbureau['ACCT-TYPE'] = bureau['ACCT-TYPE'].replace(bureau['ACCT-TYPE'].value_counts()[bureau['ACCT-TYPE'].value_counts()\/bureau.shape[0]*100<3].index, 'Rest')\nbureau['ACCT-TYPE'].value_counts()","a45132da":"# CONTRIBUTOR-TYPE - # Replace to merge similar values\nbureau['CONTRIBUTOR-TYPE'] = bureau['CONTRIBUTOR-TYPE'].replace(['HFC', 'MFI', 'FRB','SFB', 'ARC', 'OFI' ,'CCC'], 'OTHERS')\nbureau['CONTRIBUTOR-TYPE'].value_counts()","28bf7aa1":"# Replacing with similar values\n\nbureau['ACCOUNT-STATUS'] = bureau['ACCOUNT-STATUS'].replace(['SUIT FILED (WILFUL DEFAULT)','Written Off', 'Suit Filed', 'Restructured', 'Settled',\n                                                             'WILFUL DEFAULT', 'Cancelled', 'Sold\/Purchased'], 'Written Off')\nbureau['ACCOUNT-STATUS'].value_counts()","7e18e5ca":"# drop columns with greater than 20% null values\nbureau.drop(bureau.isnull().sum()[bureau.isnull().sum()\/bureau.shape[0]*100>20].index, axis=1, inplace=True)","34ed860e":"bureau['DISBURSED-DT']  = pd.to_datetime(bureau['DISBURSED-DT'])\nbureau['DATE-REPORTED'] = pd.to_datetime(bureau['DATE-REPORTED'])\n\nbureau['DATE-REPORTED'].max(), bureau['DISBURSED-DT'].max()","869de021":"bureau['DATE-REPORTED'] = bureau['DATE-REPORTED'].fillna(bureau['DATE-REPORTED'].max())\nbureau['DISBURSED-DT']  = bureau['DISBURSED-DT'].fillna(bureau['DATE-REPORTED'])","0a556a46":"bureau.drop('MATCH-TYPE', axis=1, inplace=True)","e0392658":"bureau = bureau.drop(['REPORTED DATE - HIST', 'DPD - HIST', 'CUR BAL - HIST','AMT OVERDUE - HIST', 'AMT PAID - HIST'], axis=1) # may be useful later","7f2416a1":"# split the data by whitespace\n\nbureau['CURRENT-BAL']               = bureau['CURRENT-BAL'].apply(lambda x: str(x).replace(',', ''))\nbureau['DISBURSED-AMT\/HIGH CREDIT'] = bureau['DISBURSED-AMT\/HIGH CREDIT'].apply(lambda x: str(x).replace(',', ''))\nbureau['WRITE-OFF-AMT']             = bureau['WRITE-OFF-AMT'].apply(lambda x: str(x).replace(',', ''))","6c019a1b":"bureau.info()","eb36e116":"# convert to float type these columns\nbureau.columns[-4:-1]","cde1af46":"for col in bureau.columns[-4:-1]:\n    print(col)\n    bureau[col] = bureau[col].replace('nan', 0)\n    bureau[col] = bureau[col].apply(lambda x: float(x))","b0a551f2":"# no nill values \nbureau.isnull().sum()","9dfdf88a":"bureau = pd.get_dummies(data=bureau, columns=['ACCT-TYPE', 'OWNERSHIP-IND', 'CONTRIBUTOR-TYPE','ACCOUNT-STATUS'])\nbureau.head()","816eb462":"bureau_data = bureau.groupby(['ID', 'source']).sum().reset_index()\nbureau_data.to_csv('bureau_data.csv', index=False)","f7289c9b":"gc.collect()","e16f05a6":"train = pd.read_csv(input_path+'ltfs3_train.csv')\npd.set_option('display.max_colwidth',None)\ntrain.head()","5646e6fa":"test = pd.read_csv(input_path+'ltfs3_test.csv')\npd.set_option('display.max_colwidth',None)\ntest.head()","6c66e028":"gc.collect()","06d8760a":"# drop unwanted columns\ntrain.drop(columns=[\"Area\",\"City\",\"BranchID\",\"ZiPCODE\",\"ManufacturerID\",'SupplierID', 'AssetID'],inplace=True)\n# drop unwanted columns\ntest.drop(columns=[\"Area\",\"City\",\"BranchID\",\"ZiPCODE\",\"ManufacturerID\",'SupplierID', 'AssetID'],inplace=True)","7ffb87fa":"# value counts for all columns\nfor col in train.columns:\n    if train[col].dtype=='object':\n        print(train[col].value_counts())\n        print('')","43d2d0ef":"train['State'] = train['State'].replace(train['State'].value_counts()[train['State'].value_counts()\/train.shape[0]*100<3].index, 'Others')\ntrain['State'].value_counts()\n\ntest['State'] = test['State'].replace(test['State'].value_counts()[test['State'].value_counts()\/test.shape[0]*100<3].index, 'Others')\ntest['State'].value_counts()","bf52baa6":"train['PaymentMode'] = train['PaymentMode'].replace(train['PaymentMode'].value_counts()[train['PaymentMode'].value_counts()\/train.shape[0]*100<3].index, 'Others')\ntest['PaymentMode'] = test['PaymentMode'].replace(test['PaymentMode'].value_counts()[test['PaymentMode'].value_counts()\/test.shape[0]*100<3].index, 'Others')\ntrain['PaymentMode'].value_counts()","621261a3":"train['DisbursalDate'] = pd.to_datetime(train['DisbursalDate'])\ntrain['MaturityDAte']  = pd.to_datetime(train['MaturityDAte'])\ntrain['AuthDate']      = pd.to_datetime(train['AuthDate'])\n# test\ntest['DisbursalDate']  = pd.to_datetime(test['DisbursalDate'])\ntest['MaturityDAte']   = pd.to_datetime(test['MaturityDAte'])\ntest['AuthDate']       = pd.to_datetime(test['AuthDate'])","1c0de621":"train['MonthlyIncome']  = train['MonthlyIncome'].fillna(0)\ntrain['MaturityDAte']   = train['MaturityDAte'].fillna(train['MaturityDAte'].mode()[0])\n#test\ntest['MonthlyIncome']   = test['MonthlyIncome'].fillna(0)\ntest['MaturityDAte']    = test['MaturityDAte'].fillna(test['MaturityDAte'].mode()[0])","0035ac23":"train['AGE'] = train['AGE'].fillna(train['AGE'].mode()[0])\ntest['AGE']  = test['AGE'].fillna(test['AGE'].mode()[0])","fcb7b66f":"train.drop(['DisbursalDate',\"SEX\",'MaturityDAte', 'AuthDate'], axis=1, inplace=True)\ntest.drop(['DisbursalDate', \"SEX\",'MaturityDAte', 'AuthDate'], axis=1, inplace=True)","ad523ea1":"print(train.isnull().sum())","863c2579":"train['YearsOfService']   = 60 - train['AGE']\ntest['YearsOfService']   = 60 - test['AGE']\n\ntrain.drop('AGE', 1, inplace=True)\ntest.drop('AGE', 1, inplace=True)","3d080dd6":"train.to_csv('cleaned_train.csv', index=False)\ntest.to_csv('cleaned_test.csv', index=False)","359a9b53":"gc.collect()","dcca7a58":"train = pd.read_csv('cleaned_train.csv')\ntest = pd.read_csv('cleaned_test.csv')","22f0c1d8":"from sklearn import preprocessing\n\ncategories = train.pop(\"Top-up Month\")\nle = preprocessing.LabelEncoder()\nle.fit(categories)\ny = le.transform(categories)\n\nprint(\"Number of Targets:\",len(y))\nprint(\"Number of Samples:\", len(train))","c7ea68bf":"train = train.drop(['ID'], 1)\ntest  = test.drop(['ID'], 1)","d7e4684c":"from sklearn.model_selection import train_test_split\n\nX = train\n\nX_train,X_val,y_train,y_val = train_test_split(X, y,test_size=0.2, random_state=42)\n\nprint(X_train.shape,y_train.shape)","bbb18a0d":"from sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier\nfrom matplotlib import pyplot\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n!pip install pycaret\nfrom pycaret.classification import *","376861f6":"gc.collect()","1fba1f91":"train = pd.read_csv('cleaned_train.csv')\ntrain = train.drop(['ID'], 1)","2d67436f":"clf = pycaret.classification.setup(data =train,target ='Top-up Month', session_id=14)","722c475a":"models()","d40c08b7":"best_model = compare_models(exclude = ['knn','lr','nb','ridge','qda','lda','et','ada','rbfsvm','svm','dt','gbc','mlp','gpc','lightgbm'])","52e26813":"gc.collect()","2a0d2946":"# stacking models\n# create individual models for stacking\n#xgboost = create_model('xgboost')\n#rf = create_model('rf')\ncatboost = create_model('catboost')\n#model = blend_models(['xgboost','rf','catboost'])","946490ac":"# plotting a model\nplot_model(catboost,plot = 'confusion_matrix')","fd1f38df":"# plotting AUC Curve\nplot_model(catboost, plot = 'auc')","1b892f47":"# Plotting precision - recall Curve\nplot_model(catboost, plot = 'pr')","fa9da4b5":"# feature importance plot\nplot_model(catboost, plot='feature')","6987c9bd":"predict_model(catboost)","d5a4388d":"final_model = finalize_model(catboost)\npredict_model(final_model)","408edd7f":"gc.collect()","af0a08a4":"save_model(final_model,'ltfs3_catboost_model')","d3b77a96":"best_params = final_model.get_all_params()\nprint(best_params)","cc6c7edb":"gc.collect()","e2ebb895":"test = pd.read_csv('cleaned_test.csv')\nID =test.ID\ntest = test.drop(['ID'], 1)\npreds = predict_model(final_model,data=test).Label\nsubmission = pd.DataFrame({\"ID\":ID,\"Top-up Month\":preds})","9483b9c3":"submission.to_csv('submission_ltfs3.csv',index=False)","9369d836":"## Split the data","46c6aee3":"### Concatnating train and test bureau data to save extra steps to process","87506f98":"### Predict on test\/hold-out data","bccc01f8":"### Train the data with PyCaret Classification Module","88dff48c":"## Checking NA values and replace with 0 or max","1ca9aae6":"## Preprocessing of the train and test data","d10831a1":"### Saving the Model","1b58b151":"### Data Processed!","809ce926":"## Preprocessing of the Bureau Data","4b6e0be6":"### Finalize the model","ba566928":"### train data\/test data","c6eb821b":"# L&T Financial Services Top-up loan \ud83d\udcb5 Up-sell prediction\n\n\n- A top-up loan is a facility of availing further funds on an existing loan\n\n- LTFS provides it\u2019s loan services to its customers and is interested in selling more of its Top-up loan services to its existing customers \n\n**Business Objective: Predict when to pitch a Top-up during the original loan tenure**\n\n\n### Data \n\n1. Customer\u2019s Demographics: The demography table along with the target variable & demographic information contains variables related to Frequency of the loan, Tenure of the loan, Disbursal Amount for a loan & LTV.\n\n2. Bureau data:  Bureau data contains the behavioural and transactional attributes of the customers like current balance, Loan Amount, Overdue etc. for various tradelines of a given customer\n\nI am a building a model given the Top-up loan bucket of 128655 customers along with demographic and bureau data to predict the right bucket\/period for 14745 customers in the test data.\n\n###  Evaluation\nThe evaluation metric is macro_f1_score.\n\n**Problem Statement: It's multilabel classification.**\n\nLet's explote different classification algorithms along with exploratory analysis of the data!!!","fd207cbc":"## Label the target class","9fd49983":"### Merging similar columns into single column for ease"}}