{"cell_type":{"6558d6a2":"code","17cd3bee":"code","1bf401c7":"code","d4ce35b6":"code","c73f0010":"code","4e46f1ea":"code","966218b8":"code","e4fa4848":"code","d0436533":"code","27f24a25":"code","fbcafdbb":"code","b2f04800":"code","0e976e62":"code","32fce755":"code","566eaff7":"code","a30508b6":"code","e1d2b3c9":"code","121c2a2f":"code","2200cc2d":"code","0af2170f":"code","0b0cde30":"code","fa90d74d":"code","f81defe7":"code","6ea77760":"code","b41ac4e3":"code","664635c3":"code","e4597fb5":"code","636c64a8":"code","796c1a0a":"code","b52b3ea8":"code","64d8e13f":"code","8975af36":"code","11fcdae8":"code","7412215b":"code","7f784182":"code","0b334b09":"markdown","b510382b":"markdown","c1d8e5c5":"markdown","1e0399b4":"markdown","ea5d5140":"markdown","e6ab28a6":"markdown","e98499ad":"markdown","5ade140b":"markdown"},"source":{"6558d6a2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","17cd3bee":"pip install openpyxl","1bf401c7":"df = pd.read_excel('..\/input\/cancer-patients-data\/cancer patient data sets.xlsx')","d4ce35b6":"df.head()","c73f0010":"df.columns","4e46f1ea":"df.shape","966218b8":"sns.set(rc = {'figure.figsize': (10,7)})\nlabels = list(df.Level.unique())\nsizes = list(df.Level.value_counts())\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct = '%1.2f%%', startangle = 65)","e4fa4848":"df.drop(['Patient Id'], axis=1, inplace=True)\ndf.head()","d0436533":"sns.heatmap(df.corr(), annot=False)","27f24a25":"#sns.pairplot(df, hue='Level')","fbcafdbb":"df.loc[df.Level == 'Low', 'Level'] = 0\ndf.loc[df.Level == 'Medium','Level'] = 1\ndf.loc[df.Level == 'High', 'Level'] = 2\n#dic = {'Low': 1, 'Medium': 2, 'High':3}\n#df['Level'].map(dic)\n#df['Level'].map(dic).fillna(df['Level'])\n\ndf['Level'] = df['Level'].astype(int)\ndf = df.astype(int)\n        \ndf.head()","b2f04800":"data_train = df.sample(frac = 0.8, random_state=1)\ndata_test = df.drop(data_train.index)","0e976e62":"X_train = data_train.drop(['Level'], axis=1)\nY_train = data_train['Level']\nX_test = data_test.drop(['Level'], axis=1)\nY_test = data_test['Level']","32fce755":"from sklearn.preprocessing import MinMaxScaler","566eaff7":"scaler = MinMaxScaler(feature_range=(0,1))\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","a30508b6":"from sklearn.linear_model import LogisticRegression","e1d2b3c9":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, Y_train) #Fitting\nY_lr = lr.predict(X_test)","121c2a2f":"from sklearn.metrics import accuracy_score, classification_report\nscore_LR = accuracy_score(Y_test, Y_lr)\nscore_LR","2200cc2d":"from sklearn.metrics import confusion_matrix\nresultat = confusion_matrix(Y_test, Y_lr)\nsns.heatmap(resultat, annot=True)\nplt.title('Confusion Matrix')\nplt.xlabel('Y_test')\nplt.ylabel('Y_lr')","0af2170f":"from sklearn import tree\nad = tree.DecisionTreeClassifier()\nad.fit(X_train,Y_train)\nY_ad = ad.predict(X_test)\nscore_AD = accuracy_score(Y_test, Y_ad)\nscore_AD","0b0cde30":"resultat_1 = confusion_matrix(Y_test, Y_ad)\nsns.heatmap(resultat_1, annot=True)\nplt.title('Confusion Matrix')\nplt.xlabel('Y_test')\nplt.ylabel('Y_ad')","fa90d74d":"ad1 = tree.DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 20)\nad1.fit(X_train,Y_train)\nY_ad1 = ad1.predict(X_test)\nscore_AD1 = accuracy_score(Y_test, Y_ad1)\nscore_AD1","f81defe7":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, Y_train)\nY_rf = rf.predict(X_test)\nscore_RF = accuracy_score(Y_test, Y_rf)\nscore_RF","6ea77760":"resultat_2 = confusion_matrix(Y_test, Y_rf)\nsns.heatmap(resultat_2, annot=True)\nplt.title('Confusion Matrix')\nplt.xlabel('Y_test')\nplt.ylabel('Y_rf')","b41ac4e3":"print(type(X_train))","664635c3":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","e4597fb5":"model = Sequential()\n#model.add(Dense(8, activation='relu'))\n#model.add(Dense(1, activation=\"sigmoid\"))\nmodel.add(Dense(10, activation = 'relu'))\nmodel.add(Dense(5, activation = 'relu'))\nmodel.add(Dense(3, activation = 'sigmoid')) #softmax seulement \u00e0 la derni\u00e8re couche","636c64a8":"#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #avec activation softmax","796c1a0a":"train = model.fit(X_train , Y_train , validation_data=(X_test,Y_test), epochs=50, verbose=1)","b52b3ea8":"y_ann = model.predict_classes(X_test).flatten()\ny_ann","64d8e13f":"accuracy_score(Y_test, y_ann)","8975af36":"from sklearn import neighbors\nn_neighbors = 5\nknc = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\nknc.fit(X_train, Y_train)\nY_knc = knc.predict(X_test)\nscore_KNC = accuracy_score(Y_test, Y_knc)\nscore_KNC","11fcdae8":"resultat_3 = confusion_matrix(Y_test, Y_knc)\nsns.heatmap(resultat_3, annot=True)\nplt.title('Confusion Matrix')\nplt.xlabel('Y_test')\nplt.ylabel('Y_knc')","7412215b":"print(classification_report(Y_test, Y_knc))","7f784182":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 15\n\n# import some data to play with\niris = datasets.load_iris()\n\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\ncmap_bold = ['darkorange', 'c', 'darkblue']\n\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y],\n                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n    plt.xlabel(iris.feature_names[0])\n    plt.ylabel(iris.feature_names[1])\n\nplt.show()","0b334b09":"# Pr\u00e9dicton","b510382b":"# **KNN**","c1d8e5c5":"On peut modifier certains param\u00e8tres : Le param\u00e8tre max_depth est un seuil sur la profondeur maximale de l\u2019arbre. Le param\u00e8tre min_samples_leaf donne le nombre minimal d\u2019\u00e9chantillons dans un noeud feuille.","1e0399b4":"## Random Forests","ea5d5140":"Pour construire un arbre de d\u00e9cision \u00e0 partir d'un ensemble d'apprentissage, on va choisir une variable qui s\u00e9pare l'ensemble en deux parties les plus distinctes en fonction d'un crit\u00e8re. Sur les iris par exemple, on peut utiliser la largeur du p\u00e9tale pour s\u00e9parer l'esp\u00e8ce Setosa des autres.\n\nL'indice GINI mesure avec quelle fr\u00e9quence un \u00e9l\u00e9ment al\u00e9atoire de l'ensemble serait mal class\u00e9 si son \u00e9tiquette \u00e9tait s\u00e9lectionn\u00e9e al\u00e9atoirement depuis la distribution des \u00e9tiquettes dans le sous-ensemble.","e6ab28a6":"## Arbres de d\u00e9cision","e98499ad":"## R\u00e9seaux","5ade140b":"## R\u00e9gression Logistique"}}