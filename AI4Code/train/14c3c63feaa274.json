{"cell_type":{"0893cb4c":"code","b187ffd2":"code","ccbc9997":"code","ec666efa":"code","c0b0f503":"code","08a6d034":"code","cc5e0d4c":"code","3cb59358":"code","63a00225":"code","ec14fcc9":"code","66dd90ff":"code","a4f9e802":"code","b75d74c1":"code","0aac0f93":"code","41146811":"code","92c26a26":"code","e6fbac3d":"markdown","da16d59f":"markdown","920f8ca6":"markdown","5c0566d7":"markdown","f5a16b3d":"markdown","3f32ec6c":"markdown","cfcbaf14":"markdown","f7c98340":"markdown","0ac10fed":"markdown","f6766e41":"markdown","81ccf08d":"markdown","92949ab5":"markdown","33536fd3":"markdown","57a52225":"markdown","d3710bd4":"markdown"},"source":{"0893cb4c":"#Imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport warnings\n\n#Suppressing all warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","b187ffd2":"df = pd.read_csv('..\/input\/drug-classification\/drug200.csv')","ccbc9997":"df.head()","ec666efa":"fig = px.pie(df,names='Drug', title='Drug Distribution',width=600, height=400)\nfig.show()","c0b0f503":"df['Drug'].replace('DrugY', 'drugY', inplace=True)","08a6d034":"#Copying df to avoid manipulating original data\ndf2 = df.copy()\n\ndf2['Sex'].replace({'M', 'F'},{1, 0}, inplace=True)\ndf2['BP'].replace({'HIGH', 'LOW', 'NORMAL'},{1, -1, 0}, inplace=True)\ndf2['Cholesterol'].replace({'HIGH', 'NORMAL'},{1, 0}, inplace=True)","cc5e0d4c":"x = df2.drop(['Drug'], axis=1)\ny = df2['Drug']\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.2)\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix","3cb59358":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(max_iter=10000)\nlr.fit(x_train,y_train)\np1=lr.predict(x_test)\ns1=accuracy_score(y_test,p1)\nprint(\"Linear Regression Success Rate :\", \"{:.2f}%\".format(100*s1))\nplot_confusion_matrix(lr, x_test, y_test)\nplt.show()","63a00225":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\np2=gbc.predict(x_test)\ns2=accuracy_score(y_test,p2)\nprint(\"Gradient Booster Classifier Success Rate :\", \"{:.2f}%\".format(100*s2))\nplot_confusion_matrix(gbc, x_test, y_test)\nplt.show()","ec14fcc9":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\np3=rfc.predict(x_test)\ns3=accuracy_score(y_test,p3)\nprint(\"Random Forest Classifier Success Rate :\", \"{:.2f}%\".format(100*s3))\nplot_confusion_matrix(rfc, x_test, y_test)\nplt.show()","66dd90ff":"from sklearn.svm import SVC\nsvm=SVC()\nsvm.fit(x_train,y_train)\np4=svm.predict(x_test)\ns4=accuracy_score(y_test,p4)\nprint(\"Support Vector Machine Success Rate :\", \"{:.2f}%\".format(100*s4))\nplot_confusion_matrix(svm, x_test, y_test)\nplt.show()","a4f9e802":"from sklearn.neighbors import KNeighborsClassifier\nscorelist=[]\nfor i in range(1,21):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    p5=knn.predict(x_test)\n    s5=accuracy_score(y_test,p5)\n    scorelist.append(round(100*s5, 2))\nprint(\"K Nearest Neighbors Top 5 Success Rates:\")\nprint(sorted(scorelist)[:-6:-1])\nplot_confusion_matrix(knn, x_test, y_test)\nplt.show()","b75d74c1":"from xgboost import XGBClassifier\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\n\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\nxgb = XGBClassifier(learning_rate=0.01, n_estimators=1000, objective='binary:logistic')\n\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 0)\n\ngrid = GridSearchCV(estimator=xgb, param_grid=params, n_jobs=4, \n                    cv=skf.split(x_train,y_train), verbose=0 )\n\ngrid.fit(x_train,y_train,early_stopping_rounds=20,eval_set=[(x_test, y_test)])\np2x = grid.best_estimator_.predict(x_test)\ns2x=accuracy_score(y_test,p2x)\nplot_confusion_matrix(grid.best_estimator_, x_test, y_test)\nplt.show()","0aac0f93":"print(\"Extra Gradient Booster Classifier Success Rate :\", \"{:.2f}%\".format(100*s2x))","41146811":"target = df['Drug']\nx = df.drop('Drug', axis=1)\n\npred = []\nfor index, row in x.iterrows():\n    if row['Na_to_K'] > 15:\n        pred.append('drugY')\n    elif row['BP']=='HIGH' and row['Age'] <= 50:\n        pred.append('drugA')\n    elif row['BP']=='HIGH' and row['Age'] >50:\n        pred.append('drugB')\n    elif row['BP']=='LOW' and row['Cholesterol']=='HIGH':\n        pred.append('drugC')\n    else:\n        pred.append('drugX')","92c26a26":"print(accuracy_score(target, pred)*100,'%', sep='')","e6fbac3d":"### Random Forest Classifier","da16d59f":"### Linear Regression","920f8ca6":"## The data size is small enough to classify 'Drug' without using models","5c0566d7":"### Support Vector Machine","f5a16b3d":"Replacing Categorical Values","3f32ec6c":"# Using Models to classify 'Drug'","cfcbaf14":"## Reviewing Data","f7c98340":"### The above logic was derived by pure observation of raw data by filtering it in Excel.","0ac10fed":"### Gradient Booster Classifier","f6766e41":"Hope this notebooks helped someone. Thoughts appreciated.","81ccf08d":"### K Nearest Neighbor","92949ab5":"Splitting data into test and train sets","33536fd3":"# Importing Modules and Data","57a52225":"Replacing 'DrugY' with 'drugY' to preserve consistency","d3710bd4":"### Extra Gradient Booster Classifier"}}