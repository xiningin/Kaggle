{"cell_type":{"0e0dcbf0":"code","0c18ecc5":"code","062de28e":"code","6c8dc7c2":"code","f55e70e8":"code","1db1410b":"code","162a581d":"code","bd582b86":"code","ea8eb30b":"code","e63f7627":"code","5ecd8871":"code","4ddf69b2":"code","b9dcf0b4":"code","f4ebd081":"code","e9d66782":"code","3bab776f":"code","2ac26cf2":"code","c774fafe":"code","745998f1":"code","2eb91e79":"code","17d8da9a":"code","a32d2a2d":"code","23c31748":"code","fc4ea6ef":"code","925b8fb2":"code","2e67d39b":"code","6f9d584c":"markdown","582de616":"markdown","5907b682":"markdown","1598368e":"markdown","e820c4ca":"markdown","b8d39396":"markdown","563d6b7b":"markdown","994e390c":"markdown","bc819927":"markdown","c54b8ca8":"markdown","378eae08":"markdown","3ea6af37":"markdown"},"source":{"0e0dcbf0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0c18ecc5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom yellowbrick.classifier import ROCAUC\nfrom yellowbrick.features import Rank1D, Rank2D\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, learning_curve, cross_validate, train_test_split, KFold\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","062de28e":"pip install git+https:\/\/github.com\/AutoViML\/Auto_ViML.git","6c8dc7c2":"data=pd.read_csv('\/kaggle\/input\/fetal-health-classification\/fetal_health.csv')\nprint(data.shape)\ndata.head()","f55e70e8":"sns.countplot(data.fetal_health)","1db1410b":"cols=data.columns\nprint(cols)","162a581d":"data.isnull().any()","bd582b86":"features = ['baseline value', 'accelerations', 'fetal_movement',\n       'uterine_contractions', 'light_decelerations', 'severe_decelerations',\n       'prolongued_decelerations', 'abnormal_short_term_variability',\n       'mean_value_of_short_term_variability',\n       'percentage_of_time_with_abnormal_long_term_variability',\n       'mean_value_of_long_term_variability', 'histogram_width',\n       'histogram_min', 'histogram_max', 'histogram_number_of_peaks',\n       'histogram_number_of_zeroes', 'histogram_mode', 'histogram_mean',\n       'histogram_median', 'histogram_variance', 'histogram_tendency']\n\nX = pd.DataFrame(data=data, columns=features)\ny = pd.DataFrame(data=data, columns=['fetal_health'])\ny = y.astype(int)\n# to make labels start from 0 to n_classes, otherwise I couldn't manage to run xgb with labels starting from 1 to n_classes \ud83d\ude1f\ud83d\ude1f\ud83d\ude1f\ny = y-1 \nX.head()\n","ea8eb30b":"all_features = features+['fetal_health']\nnew_data = data[all_features]\nnew_data.shape","e63f7627":"# 30% test and 70% train data as mentioned by dataset Author\n# in the task https:\/\/www.kaggle.com\/andrewmvd\/fetal-health-classification\/tasks?taskId=2410\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.30, stratify=y)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape,","5ecd8871":"train, test = train_test_split(new_data,random_state=42, test_size=0.30, stratify=y)\n\ntrain.shape, test.shape","4ddf69b2":"### Let's import Auto_ViML\nfrom autoviml.Auto_ViML import Auto_ViML","b9dcf0b4":"### Auto_ViML with all settings false except for IMbalanced Flag set to True ### \n### Using CatBoost Model #####\ntarget = 'fetal_health'\nm, feats, trainm, testm = Auto_ViML(train, target, test,\n                            sample_submission='',\n                            scoring_parameter='balanced_accuracy', KMeans_Featurizer=False,\n                            hyper_param='RS',feature_reduction=True,\n                             Boosting_Flag=True, Binning_Flag=False,\n                            Add_Poly=0, Stacking_Flag=True,Imbalanced_Flag=True,\n                            verbose=1)","f4ebd081":"#### These are the 11 features selected as important by BorutaSHAP ############\nboruta = ['histogram_min', 'histogram_mean', 'percentage_of_time_with_abnormal_long_term_variability', 'mean_value_of_short_term_variability', 'uterine_contractions', 'histogram_variance', 'histogram_mode', 'prolongued_decelerations', 'abnormal_short_term_variability', 'mean_value_of_long_term_variability', 'accelerations']\nlen(boruta)","e9d66782":"### These are the 14 features selected as important by Auto_ViML ###########\nprint(len(feats))\nfeats","3bab776f":"def left_subtract(l1,l2):\n    lst = []\n    for i in l1:\n        if i not in l2:\n            lst.append(i)\n    return lst\nleft_subtract(feats, boruta)","2ac26cf2":"left_subtract(boruta, feats)","c774fafe":"dictio = {1.0: 0, 2.0: 1, 3.0: 2}\nreverse_dictio = dict(zip(dictio.values(),dictio.keys()))\nreverse_dictio","745998f1":"y_true = test[target]\ny_pred = pd.Series(m.predict(testm[feats]).ravel()).map(reverse_dictio).values\ny_true.shape, y_pred.shape","2eb91e79":"from autoviml.Auto_ViML import print_regression_metrics, print_classification_metrics","17d8da9a":"print_classification_metrics(y_true, y_pred,False)","a32d2a2d":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report","23c31748":"cm = confusion_matrix(y_true, y_pred)\ncm","fc4ea6ef":"def plot_confusion_matrix(cm, classes, normalized=True, cmap='bone'):\n    plt.figure(figsize=[7, 6])\n    norm_cm = cm\n    if normalized:\n        norm_cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=classes, yticklabels=classes, cmap=cmap)\nplot_confusion_matrix(cm, ['Normal', 'Suspect', 'Pathological'])","925b8fb2":"print(classification_report(y_true, y_pred))","2e67d39b":"f1_score(y_true, y_pred, average=None)","6f9d584c":"# **Train Test Split: First let's fix train and test","582de616":"# This notebook is based on the following excellent notebook by Landfall:\nhttps:\/\/www.kaggle.com\/landfallmotto\/fetal-health-data-profile-boruta-model-stacking","5907b682":"# Feature Selection: Comparing BorutaSHAP with Auto_ViML","1598368e":"# **Target Variable**\n\nTarget variable distribution looks imbalanced. \nSome options are over\/under sampling data or weighting classes.","e820c4ca":"## This Notebook is primarily designed to compare BorutaSHAP's results against Auto_ViML's results","b8d39396":"# **Confusion Matrix**","563d6b7b":"# **Classification Report**","994e390c":"# **Features in Dataset**","bc819927":"# **Fetal Health Dataset Notebook**\n\n\nI tried to demonstrate multiple methods in a single notebook:<p>\n- XGBoost multiclass mode\n- XGBoost built-in feature importance\n- XGBoost feature importance with Shap\n- Feature Selection with BorutaShap\n- Multiple model running and selection\n- Model Stacking with sklearn\n- Data profiling and Visualization\n- Scaling data\n- Finding best parameter with GridSearchCV\n   \n    \n    ","c54b8ca8":"# **F1 Score**","378eae08":"# **Model**","3ea6af37":"# **Missing Values**\n\nLooks like we don't have any missing values."}}