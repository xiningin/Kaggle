{"cell_type":{"e3b55f4f":"code","7e6cd0c9":"code","93e24436":"code","6259744e":"code","0e556d97":"code","963b4055":"code","b4e39adf":"code","b57ed290":"code","679b63c3":"code","79eed0a8":"code","100b1232":"code","820b4cc3":"code","c50a2516":"code","b4a76c33":"markdown","5cb7c77d":"markdown","3942b673":"markdown","cc321a59":"markdown","58f3b006":"markdown","5bd841a2":"markdown","f713ef39":"markdown"},"source":{"e3b55f4f":"# Import bibliotek\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom IPython.core.display import display, HTML\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import PolynomialFeatures\nsns.set_style('darkgrid')\nfrom scipy import stats","7e6cd0c9":"# Import danych\nfrom sklearn.datasets import load_boston\nboston_dataset = load_boston()\ndataset = pd.DataFrame(boston_dataset.data, columns = boston_dataset.feature_names)","93e24436":"# Podgl\u0105d danych\ndataset.head()","6259744e":"# Dodanie kolumny \"MEDV\" kt\u00f3r\u0105 staramy si\u0119 przewidzie\u0107\ndataset['MEDV'] = boston_dataset.target","0e556d97":"# Przegl\u0105d danych\ndataset.describe()","963b4055":"# Sprawdzenie brak\u00f3w w danych\ndataset.isnull().sum()","b4e39adf":"# Podzia\u0142 danych na treningowe i testowe\nX = dataset.iloc[:, 0:13].values\ny = dataset.iloc[:, 13].values.reshape(-1,1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 25)","b57ed290":"# Korelacja\ncorr = dataset.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Blues')","679b63c3":"sns.pairplot(dataset)\nplt.show()","79eed0a8":"# Regresja Liniowa\nregressor_linear = LinearRegression()\nregressor_linear.fit(X_train, y_train)\n\n# Walidacja Krzy\u017cowa\ncv_linear = cross_val_score(estimator = regressor_linear, X = X_train, y = y_train, cv = 10)\n# R2 dla danych treningowych\ny_pred_linear_train = regressor_linear.predict(X_train)\nr2_score_linear_train = r2_score(y_train, y_pred_linear_train)\n# R2 dla danych testowych\ny_pred_linear_test = regressor_linear.predict(X_test)\nr2_score_linear_test = r2_score(y_test, y_pred_linear_test)\n# Pierwiastek b\u0142\u0119du \u015bredniokwadratowego\nrmse_linear = (np.sqrt(mean_squared_error(y_test, y_pred_linear_test)))\n# \u015aredni b\u0142\u0105d bezwzgl\u0119dny\nmae_linear= mae(y_test, y_pred_linear_test)\n\n# Wykres\nslope, intercept, r, p, std_err = stats.linregress(np.concatenate(y_test), np.concatenate(y_pred_linear_test))\n\ndef myfunc(x):\n  return slope * x + intercept\n\nmymodel = list(map(myfunc, y_test))\n\nplt.scatter(y_test,y_pred_linear_test)\nplt.plot(y_test, mymodel, color='red')\nplt.title(\"Wykres regresji liniowej\", fontsize=16)\nplt.xlabel('W\u0142a\u015bciwe wielko\u015bci')\nplt.ylabel('Predyktowane wielko\u015bci')\nplt.show()\n\nprint(\"CV: \", cv_linear.mean())\nprint('R2_score (train): ', r2_score_linear_train)\nprint('R2_score (test): ', r2_score_linear_test)\nprint(\"RMSE: \", rmse_linear)\nprint(\"MAE: \", mae_linear)","100b1232":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curves(model, X, y, ylim=None):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n        train_errors, test_errors = [], []\n        if ylim is not None:\n            plt.ylim(*ylim)\n\ntrain_errors = list()\ntest_errors = list()\n\nfor m in range(1, len(X_train)):\n        regressor_linear.fit(X_train[:m], y_train[:m])\n        y_train_predict = regressor_linear.predict(X_train[:m])\n        y_test_predict = regressor_linear.predict(X_test)\n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n        test_errors.append(mean_squared_error(y_test, y_test_predict))\n\nplt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\nplt.plot(np.sqrt(test_errors), \"b-\", linewidth=3, label=\"test\")\nplt.title(\"Krzywa uczenia regresji liniowej\", fontsize=16)\nplt.xlabel(\"Rozmiar zestawu ucz\u0105cego\")\nplt.ylabel(\"B\u0142\u0105d RMSE\")\nplt.legend(loc=\"best\")\nplot_learning_curves(regressor_linear,X,y, ylim=(0, 5.5))\n# Krzywa uczenia pokazana na ograniczonym odcinku w celu lepszej widoczno\u015bci ","820b4cc3":"# Regresja wielomianowa\npoly_reg = PolynomialFeatures(degree = 2, include_bias=False)\nX_poly = poly_reg.fit_transform(X_train)\npoly_reg.fit(X_poly, y_train)\nregressor_poly2 = LinearRegression()\nregressor_poly2.fit(X_poly, y_train)\n\nY_pred = regressor_poly2.predict(X_poly)\n\n# Walidacja Krzy\u017cowa\ncv_poly2 = cross_val_score(estimator = regressor_poly2, X = X_train, y = y_train, cv = 10)\n# R2 dla danych treningowych\ny_pred_poly2_train = regressor_poly2.predict(poly_reg.fit_transform(X_train))\nr2_score_poly2_train = r2_score(y_train, y_pred_poly2_train)\n# R2 dla danych testowych\ny_pred_poly2_test = regressor_poly2.predict(poly_reg.fit_transform(X_test))\nr2_score_poly2_test = r2_score(y_test, y_pred_poly2_test)\n# Pierwiastek b\u0142\u0119du \u015bredniokwadratowego\nrmse_poly2 = (np.sqrt(mean_squared_error(y_test, y_pred_poly2_test)))\n# \u015aredni b\u0142\u0105d bezwzgl\u0119dny\nmae_poly2= mae(y_test, y_pred_poly2_test)\n\n# Wykres\nmymodel = np.poly1d(np.polyfit(np.concatenate(y_test), np.concatenate(y_pred_poly2_test), 4))\n\nmyline = np.linspace(0, 50, 100)\n\nplt.scatter(y_test, y_pred_poly2_test)\nplt.xlim(0, 51)\nplt.ylim(0, 51)\nplt.plot(myline, mymodel(myline), color='red')\nplt.title(\"Regresja wielomianowa\", fontsize=16)\nplt.xlabel('W\u0142a\u015bciwe wielko\u015bci')\nplt.ylabel('Predyktowane wielko\u015bci')\nplt.show()\n\nprint('CV: ', cv_poly2.mean())\nprint('R2_score (train): ', r2_score_poly2_train)\nprint('R2_score (test): ', r2_score_poly2_test)\nprint(\"RMSE: \", rmse_poly2)\nprint(\"MAE: \", mae_poly2)\n","c50a2516":"from sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import learning_curve\n\npolynomial_regression = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=2, include_bias = False)),\n    (\"lin_reg\",LinearRegression()),\n    ])\npolynomial_regression.fit(X_train, y_train)\n\ntrain_errors1 = list()\ntest_errors1 = list()\n\nfor m in range(1, len(X_train)):\n        polynomial_regression.fit(X_train[:m], y_train[:m])\n        y_train_predict = polynomial_regression.predict(X_train[:m])\n        y_test_predict = polynomial_regression.predict(X_test)\n        train_errors1.append(mean_squared_error(y_train[:m], y_train_predict))\n        test_errors1.append(mean_squared_error(y_test, y_test_predict))\n\nplt.plot(np.sqrt(train_errors1), \"r-+\", linewidth=2, label=\"train\")\nplt.plot(np.sqrt(test_errors1), \"b-\", linewidth=3, label=\"test\")\nplt.title(\"Krzywa uczenia regresji wielomianowej\", fontsize=16)\nplt.xlabel(\"Rozmiar zestawu ucz\u0105cego\")\nplt.ylabel(\"B\u0142\u0105d RMSE\")\nplt.legend(loc=\"best\")\n\nplot_learning_curves(polynomial_regression, X, y,ylim=(0.5, 6.5))\n# Krzywa uczenia pokazana na ograniczonym odcinku w celu lepszej widoczno\u015bci ","b4a76c33":"Columns:\n\nCRIM: Per capita crime rate by town\n\nZN: Proportion of residential land zoned for lots over 25,000 sq. ft\n\nINDUS: Proportion of non-retail business acres per town\n\nCHAS : Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n\nNOX: Nitric oxide concentration (parts per 10 million)\n\nRM: Average number of rooms per dwelling\n\nAGE: Proportion of owner-occupied units built prior to 1940\n\nDIS: Weighted distances to five Boston employment centers\n\nRAD: Index of accessibility to radial highways\n\nPTRATIO: Pupil-teacher ratio by town\n\nB: 1000(Bk \u2014 0.63)\u00b2, where Bk is the proportion of [people of African American descent] by town\n\nLSTAT: Percentage of lower status of the population\n\nMEDV: Median value of owner-occupied homes in $1000s","5cb7c77d":"### Przygotowanie danych","3942b673":"### Regresja Liniowa","cc321a59":"### Wizualizacja danych","58f3b006":"### Regresja wielomianowa - II stopnia","5bd841a2":"\ud835\udc45^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n\nAdjusted \ud835\udc45^2 :The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n\nMAE : It is the mean of the absolute value of the errors. It measures the difference between two continuous variables, here actual and predicted values of y. \n\nMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. \n\nRMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. ","f713ef39":"### Import"}}