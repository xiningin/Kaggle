{"cell_type":{"03eef3aa":"code","de8ccc3a":"code","47fd9a91":"code","d89a017b":"code","91a5e9d3":"markdown","3d8d36d6":"markdown","ebd3c71a":"markdown"},"source":{"03eef3aa":"import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import svm\n# from sklearn import neighbors\nfrom sklearn.metrics import f1_score\nimport seaborn as sns\nimport collections\nimport matplotlib.pyplot as plt\n\ntraining_data = os.path.join(os.getcwd(), \"tamu-datathon\/equip_failures_training_neg_one.csv\")\ntest_data = os.path.join(os.getcwd(), \"tamu-datathon\/equip_failures_test_set.csv\")\ntraining_data_cleaned = os.path.join(os.getcwd(), 'tamu-datathon\/equip_failures_train_clean_drop_79.csv')\ntest_data_cleaned = os.path.join(os.getcwd(), 'tamu-datathon\/equip_failures_test_clean_drop_79.csv')\n\n\ndef clean_data(df_clean):\n    df_train = pd.read_csv(settings.training_data)\n    df_test = pd.read_csv(settings.test_data)\n    d1, d2 = clean_data(df_train, df_test)\n    d1.to_csv(training_data_cleaned, index = False)\n    d2.to_csv(test_data_cleaned, index = False)\n\n# Clean Data\n# clean_data(training_data)","de8ccc3a":"import pickle\nfrom sklearn.decomposition import NMF\n\ndf_training = pd.read_csv(training_data_cleaned)\ndf_drop = df_training.drop(columns=['id','target'])\ndf_drop = df_drop + 1\n\nnp_drop = df_drop.to_numpy()\nmodel = NMF(n_components=2, init='random', random_state=0)\nW = model.fit_transform(np_drop)\n\nwith open('new_matrix.pickle', 'wb') as handle:\n    pickle.dump(W, handle)","47fd9a91":"def gen_output(predictions):\n    columns = ['id', 'target']\n\n    df = pd.read_csv(settings.test_data_cleaned)\n    X_test = df.iloc[:, 1:]  # First one columns are id\n    model = alg.fit(X, Y)\n    predictions = model.predict(X_test)\n    id = list(range(16002))\n    id = id[1::]\n    csv = pd.DataFrame()\n    csv['id'] = id\n    csv['target'] = predictions\n    csv.to_csv('sample_drop79.csv', index = False)\n\nprint(\"Executing...\")\n\ndf = pd.read_csv(training_data_cleaned)\nX = df.iloc[:, 2:]  # First two columns are id and target\nY = np.array(df.iloc[:, 1])\n\nalg = RandomForestClassifier(n_estimators=250)\n\ncv = StratifiedKFold(n_splits=10)\n\nfscores = []\nfor i, (train, test) in enumerate(cv.split(X, Y)):\n    model = alg.fit(X.iloc[train], Y[train])\n    Y_pred = model.predict(X.iloc[test])\n    fscore = f1_score(Y[test], Y_pred, average='weighted', labels=np.unique(Y[test]))\n    fscores.append(fscore)\n    print('Fold', i, ':', fscore)\n\nprint('Average F-measure:', sum(fscores) \/ len(fscores))\n","d89a017b":"#Plot Feature Importance\nimportances = list(model.feature_importances_)\ncolumn_headers = list(df.columns.values)\ndicy = dict(zip(importances, column_headers))\ndicysort = collections.OrderedDict(sorted(dicy.items()))\nsns.set(style='whitegrid')\nax = sns.barplot(x=[dicysort[i] for i in dicysort.keys()], y=dicysort.keys(), data=dict(dicysort))\nplt.show()","91a5e9d3":"# Conoco Phillips Dataset Challenge - Team J.I.I.S\n\n\n\n\nThe initial portion of our code is import statements and our data cleaning function. We experimented with a variety of ways to process the data, mainly due to the issue of large amounts of 'na's being found within. One of the methods implemented was replacing all entries of 'na' with -1 , we also tried dropping all columns above a certain 'na' threshold. In the end we found that a threshold of 79% resulted in the highest score. This threshold was decided based on sensors 41, 42, and 43 being sequential while also having a similar amount of 'na' percentage, around 80%.\n\nWe attempted a few methods of normalization within the data, such as standard and robust normalization. They were implemented using the standard scikit library functions StandardScaler and RobustScaler. However, we found that normalizing the data resulted in lower accuracy.","3d8d36d6":"We tested machine learning algorithms such as kNN, SVM, Random Forest and AdaBoost. For the majority of experiments, Random Forest gave the best performance regardless of the cleaning or normalization method implemented.\n\nWe did attempt to do some hyperparameter tuning on the algorithms we ran. In some instances (e.g. number of trees comprising Random Forest) we tried manual hyperparameter tuning. We also attempted to run a randomized search of the hyperparameters; however we were unable to get results in time.","ebd3c71a":"Another technique we attempted was non-negative matrix factorization. We converted all 'na's to -1, then shifted the entire dataset up by 1, resulting in 'na' being 0. We then converted it to an embedding space using scikit learns MF algorithm. The model was then stored as a pickle object and fed into our Random Forest algorithm. Ultimately we didn't use this method as the results were less accurate than the other data cleaning methods we tried."}}