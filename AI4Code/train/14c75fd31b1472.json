{"cell_type":{"b97c75d4":"code","60b2fdcf":"code","578c47e2":"code","1960a221":"code","33f2bfe8":"code","50807b0c":"code","f9a926ad":"code","984c7d2e":"code","f7c79bdf":"code","32fe65ed":"code","b0ba8259":"code","38f8bc2f":"code","cfab7651":"code","327ea60a":"code","c0286eca":"code","bdbc6aa3":"code","c636b3b7":"code","d905bfd4":"code","5acf80d5":"code","a8bff33c":"markdown","32f97d44":"markdown"},"source":{"b97c75d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","60b2fdcf":"# DATA IMPORT\ndataframe=pd.read_csv(\"..\/input\/Admission_Predict.csv\")\n\ndataframe.info()","578c47e2":"dataframe.head()","1960a221":"# DROP UNNECESSARY COLUMNS\ndataframe=dataframe.drop([\"Serial No.\",\"Research\"],axis=1)\ndataframe.tail()\n","33f2bfe8":"\ny1=dataframe[\"Chance of Admit \"].values.reshape(400,1)\n\nx1=(dataframe- np.min(dataframe))\/(np.max(dataframe)-np.min(dataframe)).values\n\nprint(\"x1 shape: \" , x1.shape)\nprint(\"y1 shape: \" , y1.shape)\n","50807b0c":"#normalization\nx1=(dataframe- np.min(dataframe))\/(np.max(dataframe)-np.min(dataframe)).values","f9a926ad":"x1.tail()","984c7d2e":"# TRAIN\/TEST SPLIT SKLEARN\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.15, random_state=42)\n\n","f7c79bdf":"#SHAPE TRAIN TEST \nx_test = x_test.T\nx_train = x_train.T\ny_test = y_test.T\ny_train = y_train.T\nprint(\"X train: \",x_train.shape)\nprint(\"X test: \",x_test.shape)\nprint(\"Y train: \",y_train.shape)\nprint(\"Y test: \",y_test.shape)","32fe65ed":"\n# calculation of z with sigmoid fucntion between L2 layer to Output layer activation function\n# z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","b0ba8259":"# we have 1 hidden layer \"L2\" and input layer \"L1\" \n#it means we have two sets weights and biases to initialize to;\n#intialize parameters and layer sizes\ndef initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters","38f8bc2f":"# when doing forward propagation  we use tanh activation function to between L1 and L2\n# after that at the output layer we use sigmoid \n\n\ndef forward_propagation_NN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) + parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","cfab7651":"# Cost function for Compute cost \ndef compute_cost_NN(A2, y1, parameters):\n    logprobs = np.multiply(np.log(A2),y1)\n    cost = -np.sum(logprobs)\/y1.shape[1]\n    return cost\n","327ea60a":"# Backward Propagation\n# Gradient descent algorithm , this function helps update weights and biases with chained derivative calculus.\ndef backward_propagation_NN(parameters, cache, x1, y1):\n\n    dZ2 = cache[\"A2\"]-y1\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/x1.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/x1.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,x1.T)\/x1.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/x1.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","c0286eca":"# update parameters\n# learning rate helps us how fast update parameters after backward propagation\ndef update_parameters_NN(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","bdbc6aa3":"# we have output layer and this network gives an output, we use output as a prediction for comparison between  real data and output\ndef predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.6,  prediction is Chance of Admit True  ,\n    # if z is smaller than 0.6, prediction is Chance of Admit False,\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","c636b3b7":"# 2 - Layer neural network model\ndef two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n         # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n         # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n          # we plot cost every 100th calculation.\n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\n\n# Hiperparemeters are (learning_rate=0.01, number of iteration = 3000 and logistic treshold is 0.85 to Chance to Admission) \nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=3000)","d905bfd4":"#rehape for keras library\n\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","5acf80d5":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","a8bff33c":"# 2 LAYER NEURAL NETWORK COMPUTATION GRAPH\n![resim.png](attachment:resim.png)","32f97d44":"# This is my 2 Layer Neural Network study for better understand  how it works Neural Networks\n* First data preprocessing and train and test split\n"}}