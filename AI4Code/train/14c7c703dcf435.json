{"cell_type":{"e0ac2e81":"code","71823064":"code","34b3b82d":"code","6119df9a":"code","f15851e4":"code","eb79777c":"code","27dcadec":"code","35d32209":"code","cae28ca3":"code","0a9b3b99":"code","00ef89d0":"code","7fa3825f":"code","a10c10d1":"code","4c425c05":"code","63b44f27":"code","73add856":"code","830d64d2":"code","a1c68b3f":"code","0220d082":"code","ec6f4af3":"code","7d28eb74":"code","afb0925e":"code","a25969e4":"code","77fc4689":"code","7fc9af11":"code","9b363832":"code","7df16b70":"markdown","61947217":"markdown","85048101":"markdown","d1bc6793":"markdown","cfe634cc":"markdown","1846be50":"markdown"},"source":{"e0ac2e81":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71823064":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport tqdm\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error\n\nimport xgboost as xgb\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\npd.options.display.max_rows = 100","34b3b82d":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","6119df9a":"y_test = sample_submission.SalePrice","f15851e4":"train.info()","eb79777c":"print(train.shape)\nprint(test.shape)","27dcadec":"plt.figure(figsize=(15,10))\nsns.distplot(train.SalePrice)\nplt.show()","35d32209":"# Defer target in advance\ntrain_target = train.SalePrice","cae28ca3":"# Remove unnecessary columns in train and test\ntrain = train.drop(['Id', 'SalePrice'], axis=1)\ntest = test.drop(['Id'],axis=1)","0a9b3b99":"# Link them together for general processing\ndata = pd.concat([train, test])","00ef89d0":"cat_feat = list(data.dtypes[data.dtypes == object].index)","7fa3825f":"# encode the missing values with a string, the fact of the missing value can also carry information\ndata[cat_feat] = data[cat_feat].fillna('nan')\n# filter out continuous features\nnum_feat = [f for f in data if f not in (cat_feat + ['Id', 'SalePrice'])]","a10c10d1":"cat_nunique = data[cat_feat].nunique()\nprint(cat_nunique)\ncat_feat = list(cat_nunique[cat_nunique < 30].index)","4c425c05":"dummy_data = pd.get_dummies(data[cat_feat], columns=cat_feat)\n\ndummy_cols = list(set(dummy_data))\n\ndummy_data = dummy_data[dummy_cols]\n\n\ndata = pd.concat([data[num_feat].fillna(-999),\n                     dummy_data], axis=1)\n","63b44f27":"data.head()","73add856":"train = data.iloc[:train.shape[0], :]\ntest = data.iloc[train.shape[0]:, :]","830d64d2":"X_train = train\ny_train = train_target\nX_test = test\ny_test = sample_submission.SalePrice","a1c68b3f":"params = {'n_estimators': 130,\n          'learning_rate': 0.01,\n          'max_depth': 3,\n          'min_child_weight': 1,\n          'subsample': 1,\n          'colsample_bytree': 1,\n          'objective': 'reg:linear',\n          'n_jobs': 4}\nclf_xgb = xgb.XGBRegressor(**params)\n\nclf_xgb.fit(X_train, y_train, eval_metric='mae', eval_set=[(X_train, y_train), (X_test, y_test)])","0220d082":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials","ec6f4af3":"# function to be MINIMIZED\ndef score(params):\n    params['max_depth'] = int(params['max_depth'])\n    params['n_jobs'] = -1\n    print(\"Training with params : \", params)\n    clf = xgb.XGBRegressor(**params)\n    clf.fit(X_train, y_train)\n    y_pred_xgb_test = clf.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred_xgb_test)\n    result = {'loss': mae, 'status': STATUS_OK}\n    print('TEST ROC AUC: {0:.4f}'.format(mae))\n    return result\n\n\nspace = {\n            'max_depth' : hp.choice('max_depth', range(1, 15)),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'eta': hp.quniform('eta', 0.025, 0.5, 0.10),\n            'max_depth':  hp.choice('max_depth', np.arange(1, 14, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 6, 0.5),\n            'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n            'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n            'eval_metric': 'mae',\n            'objective': 'reg:linear',\n            # Increase this number if you have more cores. Otherwise, remove it and it will default\n            # to the maxium number.\n            'nthread': 4,\n            'booster': 'gbtree',\n            'tree_method': 'exact',\n            'silent': 1\n        }\n\ntrials = Trials()\n\nbest = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=20)","7d28eb74":"best","afb0925e":"trials.best_trial","a25969e4":"clf_xgb = xgb.XGBRegressor(**best)\n\nclf_xgb.fit(X_train, y_train, eval_metric='mae', eval_set=[(X_train, y_train), (X_test, y_test)])","77fc4689":"predict = clf_xgb.predict(X_test)","7fc9af11":"submission = pd.DataFrame({\n        \"Id\": sample_submission[\"Id\"],\n        \"SalePrice\": predict\n    })\nsubmission.to_csv('submission.csv', index=False)","9b363832":"submission","7df16b70":"# Train the model","61947217":"# Split the sample","85048101":"# Visualization SalePrice","d1bc6793":"** Create signs for \"wooden\" models **\n\n1. Replace the gaps with the special value -999 so that the trees can distinguish them\n3. Create dummy variables for categories","cfe634cc":"# Hyperopt","1846be50":"# Data Preprocessing "}}