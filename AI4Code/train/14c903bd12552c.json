{"cell_type":{"e53d6722":"code","81d1521f":"code","955fd895":"code","d3aa72d8":"code","424a82b5":"code","4711d41c":"code","c0f4b501":"code","0f881df6":"code","cb711463":"code","230c7b90":"code","9d02ddf0":"code","6235c8b7":"code","5208ce6a":"code","34f9b05d":"code","c3809077":"code","538197d5":"code","4e7abd4e":"code","f7643917":"code","6b28cbdc":"code","e86ee38e":"code","f7f25bf2":"code","c7e4006f":"code","a4a0b24d":"code","854d208c":"code","95e8c3a9":"code","df37511a":"code","47f54555":"code","d9791d1a":"code","2b0dffb5":"code","d60781e0":"code","261ca955":"code","1a5b1e83":"code","4923227e":"code","cbd87e7d":"code","98c1b36c":"code","df8fe9ed":"code","8174077f":"code","82011819":"code","e98947a4":"code","2b5b4a7f":"code","dd4f8b8b":"code","63390188":"code","bf7b5769":"code","6f728466":"code","d3340705":"code","d0dfb1c0":"code","147ba5b3":"code","2d2acd96":"code","df2a7336":"code","46899bcd":"code","dc7bd2ff":"code","f4f48d93":"code","f071dde4":"code","1a1c2875":"code","59344b2c":"code","f986ad01":"code","f9cfd4be":"markdown","0ae62fb7":"markdown","b8c6b3f1":"markdown","d7ca967e":"markdown","a7d43717":"markdown","3f833314":"markdown","64dca112":"markdown","00e69791":"markdown","41206797":"markdown","7f1f9ce0":"markdown","aa70358e":"markdown","78a1644a":"markdown","a9e4c5bc":"markdown","51f1d830":"markdown","9f3142b1":"markdown","730319c3":"markdown","f47a53ab":"markdown","530416df":"markdown","8300c44d":"markdown","0d011c2d":"markdown","a8dae210":"markdown","f85b4414":"markdown","20ac3c43":"markdown","27e282dc":"markdown","92de435f":"markdown","d7cf69a9":"markdown","0db7ad47":"markdown","9040079b":"markdown","04d98d6d":"markdown","b3ebf056":"markdown","61b0ee00":"markdown","f538968c":"markdown","22e7d6c1":"markdown","5914d5d6":"markdown","7ccb7c60":"markdown","eac87598":"markdown","b80ec179":"markdown","bed4a14d":"markdown","d6c56377":"markdown","d826ba0b":"markdown","c788221f":"markdown","fb042143":"markdown","142c9544":"markdown","52a4e1d9":"markdown","b23c795b":"markdown","07a6ca57":"markdown","092bd569":"markdown","84ee9aab":"markdown","473c3a80":"markdown","8078a7c8":"markdown","f7208839":"markdown","019d549a":"markdown","92b6859d":"markdown","e372b760":"markdown","a21b84df":"markdown","b7340209":"markdown","c36d10cb":"markdown","647b790b":"markdown","ede66c5d":"markdown","2c923414":"markdown","002f11aa":"markdown","552d6e32":"markdown","9a1ddea0":"markdown","3dc56037":"markdown","004e3946":"markdown","6512009c":"markdown","e325cbe1":"markdown","fc39e11a":"markdown","482eca72":"markdown","dc72826f":"markdown","bb80ee08":"markdown","b18962bd":"markdown"},"source":{"e53d6722":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')\n$.getScript('https:\/\/ipython-contrib.github.io\/jupyter_contrib_nbextensions\/blob\/master\/src\/jupyter_contrib_nbextensions\/nbextensions\/toc2\/main.js')","81d1521f":"# Import environment tools\nimport re\nimport itertools\nimport warnings\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport keras\n\n# Import plotly tools\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n# Import keras tools\nfrom keras import regularizers\nfrom keras.callbacks import History \nfrom keras.layers import Dense, Input, Dropout\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.utils.np_utils import to_categorical\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# Import other tools\nfrom __future__ import print_function\nfrom pandas import read_excel\nfrom IPython.display import Image\nfrom collections import Counter\nfrom itertools import cycle\nfrom scipy import stats, integrate, interp\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","955fd895":"# Import relevant machine learning models\n\nimport sklearn\n# Gradient Boosters\nimport xgboost as xgb # Accuracy\nimport lightgbm as lgb # Speed\n\nfrom sklearn import decomposition, preprocessing, svm\n# Dimensionality Reduction\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n# Ensemble\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n# Guassian\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\n# Regression\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n# Bayesian\nfrom sklearn.naive_bayes import GaussianNB\n# Instance Based\nfrom sklearn.neighbors import KNeighborsClassifier\n# Nueral Network\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import relevant machine learning analyis tools\nfrom sklearn import metrics\n#from sklearn.cross_validation import KFold, train_test_split\n# Imputation\nfrom sklearn.impute import SimpleImputer \nfrom sklearn.metrics import mean_absolute_error,roc_curve,accuracy_score,auc,roc_auc_score,confusion_matrix,precision_score,recall_score,f1_score, classification_report\nfrom sklearn.metrics.cluster import fowlkes_mallows_score\nfrom sklearn.model_selection import BaseCrossValidator, GridSearchCV, train_test_split,cross_val_score,cross_validate,cross_val_predict, KFold, StratifiedKFold, learning_curve\nfrom sklearn.pipeline import Pipeline\n# Standardization\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n\n\n# Initial tool settings\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nplt.style.use('fivethirtyeight')\nsns.set(style='white', context='notebook', palette='deep')\npy.init_notebook_mode(connected=True)\n\nhistory = History()\nrandom_state = 43\n\nnames = [\"k-Nearest Neighbors\",         \n         \"Support Vector Machine\",\n         \"Linear SVM\",\n         \"RBF SVM\",\n         \"Gaussian Process\",\n         \"Decision Tree\",\n         \"Extra Trees\",\n         \"Random Forest\",\n         \"Extra Forest\",\n         \"AdaBoost\",\n         \"Gaussian Naive Bayes\",\n         \"LDA\",\n         \"QDA\",\n         \"Logistic Regression\",\n         \"SGD Classifier\",\n         \"Multilayer Perceptron\",\n         \"Voting Classifier\"\n        ]\n\nalgorithms = [ KNeighborsClassifier(n_neighbors=3),\n               SVC(random_state=random_state),\n               SVC(kernel=\"linear\",random_state=random_state),\n               SVC(kernel=\"rbf\",random_state=random_state),\n               GaussianProcessClassifier(),\n               DecisionTreeClassifier(random_state=random_state),\n               ExtraTreesClassifier(random_state=random_state),\n               RandomForestClassifier(random_state=random_state),\n               GradientBoostingClassifier(random_state=random_state),\n               AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),n_estimators=10,learning_rate=0.1,random_state=random_state),\n               GaussianNB(),\n               LinearDiscriminantAnalysis(),\n               QuadraticDiscriminantAnalysis(),\n               LogisticRegression(random_state=random_state),\n               SGDClassifier(),               \n               MLPClassifier(hidden_layer_sizes=(100,),momentum=0.9,solver='sgd',random_state=random_state),\n               VotingClassifier(estimators=[('log', LogisticRegression()), ('SVM',SVC(C=1000)), ('MLP', MLPClassifier(hidden_layer_sizes=(100,)))], voting='hard')\n              ]\n#algorithms.append(SVC(random_state=random_state))\n\nclassifiers = {  \"k-Nearest Neighbors\" : KNeighborsClassifier(n_neighbors=3),\n                 \"Support Vector Machine\" :  SVC(random_state=random_state),\n                 \"Linear SVM\" :  SVC(kernel=\"linear\",random_state=random_state),\n                 \"RBF SVM\" :  SVC(kernel=\"rbf\",random_state=random_state),\n                 \"Gaussian Process\" : GaussianProcessClassifier(),\n                 \"Decision Tree\" : DecisionTreeClassifier(random_state=random_state),\n                 \"Extra Trees\" : ExtraTreesClassifier(random_state=random_state),\n                 \"Random Forest\" : RandomForestClassifier(random_state=random_state),\n                 \"Extra Forest\" : GradientBoostingClassifier(random_state=random_state),\n                 \"AdaBoost\" : AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),n_estimators=10,random_state=random_state,learning_rate=0.1),\n                 \"Gaussian Naive Bayes\" : GaussianNB(),\n                 \"LDA\" : LinearDiscriminantAnalysis(),\n                 \"QDA\" :  QuadraticDiscriminantAnalysis(),\n                 \"Logistic Regression\" : LogisticRegression(random_state=random_state),\n                 \"SGD Classifier\" : SGDClassifier(),\n                 \"Multilayer Perceptron\" :  MLPClassifier(hidden_layer_sizes=(100,),momentum=0.9,solver='sgd',random_state=random_state),\n                 \"Voting Classifier\" : VotingClassifier(estimators=[('log', LogisticRegression()), ('SVM',SVC(C=1000)), ('MLP', MLPClassifier(hidden_layer_sizes=(100,)))], voting='hard')\n              }","d3aa72d8":"# Input data files are available in the \"..\/input\/\" directory\ndataset = pd.read_csv(\"..\/input\/Heart_Disease_Data.csv\", na_values=\"?\", low_memory = False)\n\n#dataset=pd.read_csv(\"..\/input\/Heart_Disease_Data.csv\", delimiter=',', header = None, )\n#dataset = pd.read_csv('http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/heart-disease\/cleve.mod', skiprows=20, header=None, sep='\\s+', names=features, index_col=False, na_values=\"?\")\ndataset[\"pred_attribute\"].replace(inplace=True, value=[1, 1, 1, 1], to_replace=[1, 2, 3, 4])\n\nnp_dataset = np.asarray(dataset)  \n#np_dataset = open(\"..\/input\/Heart_Disease_Data.csv\")  \n#np_dataset.readline()  # Skip the feature names row\n#np_data = np.loadtxt(np_dataset, delimiter = ',')  \n#mx_dataset = dataset.as_matrix()\n\n# 13 dataset features\nfeature13 = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slop','ca','thal']","424a82b5":"dataset.head()","4711d41c":"dataset.tail()","c0f4b501":"columns=dataset.columns[:13]\nplt.subplots(figsize=(20,15))\nlength=len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    dataset[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","0f881df6":"dataset_copy=dataset[dataset['pred_attribute']==1]\ncolumns=dataset.columns[:13]\nplt.subplots(figsize=(20,15))\nlength=len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    dataset_copy[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","cb711463":"features_continuous=[\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"pred_attribute\"]\n#features_continuous=[\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]","230c7b90":"sns.pairplot(data=dataset[features_continuous],hue='pred_attribute',diag_kind='kde')\n#plt.gcf().set_size_inches(20,15)\nplt.show()","9d02ddf0":"sns.boxplot(data=dataset[features_continuous])\n#plt.gcf().set_size_inches(20,15)\nplt.show()","6235c8b7":"dataset[features_continuous].describe()","5208ce6a":"sns.countplot(x='pred_attribute',data=dataset)\nplt.show()","34f9b05d":"sns.countplot(x='sex',data=dataset)\nplt.show()","c3809077":"sns.countplot(x='fbs',data=dataset)\nplt.show()","538197d5":"sns.countplot(x='slop',data=dataset)\nplt.show()","4e7abd4e":"sns.countplot(x='cp',data=dataset)\nplt.show()","f7643917":"sns.countplot(x='exang',data=dataset)\nplt.show()","6b28cbdc":"sns.countplot(x='ca',data=dataset)\nplt.show()","e86ee38e":"sns.countplot(x='thal',data=dataset)\nplt.show()","f7f25bf2":"sns.countplot(x='restecg',data=dataset)\nplt.show()","c7e4006f":"dataset.isnull().sum()","a4a0b24d":"#dataset = dataset.convert_objects(convert_numeric=True)\n\n# Load data\nX = dataset.iloc[:, :-1].values  \ny = dataset.iloc[:, -1].values # = dataset.iloc[:, 13].values\n\n#dataset.dropna(inplace=True, axis=0, how=\"any\")\n#X=dataset.loc[:, \"age\":\"thal\" ]\n#y=dataset[\"pred_attribute\"]\n#np_X = np_dataset[:, :-1]  \n#np_y = np_dataset[:, -1]  \n\nmy_imputer = SimpleImputer()\nmy_imputer = my_imputer.fit(X[:,0:13])   \nX[:, 0:13] = my_imputer.transform(X[:, 0:13])","854d208c":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n#features_all = dataset[dataset.columns[:13]]\n#features_standard = scaler.fit_transform(dataset[dataset.columns[:13]]) # Gaussian Standardisation\n#X = pd.DataFrame(features_standard,columns=[feature13])\n#X['pred_attribute'] = dataset['pred_attribute']\n#outcome = X['pred_attribute']","95e8c3a9":"# evaluate the model by splitting into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n\nfreqs = pd.DataFrame({\"Training dataset\": y_train.sum(),\n                      \"Test dataset\":y_test.sum(),\n                      \"Total\": y.sum()},\n                     index=[\"Healthy\", \"Sick\"])\nfreqs[[\"Training dataset\", \"Test dataset\", \"Total\"]]","df37511a":"def err_score(X_train, X_test, y_train, y_test):\n    model =  RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n    \n    # predict class labels for the train set\n    pred_train = model.predict(X_train)\n    # predict class labels for the test set\n    pred_test = model.predict(X_test)\n    # check the mean absolute error on test set\n    print(\"Mean Absolute Error from imputation: \", mean_absolute_error(y_test, pred_test))\n    \n#X = my_imputer.fit_transform(X)\n#X_train = my_imputer.fit_transform(X_train)\n#X_test = my_imputer.transform(X_test)\nerr_score(X_train, X_test, y_train, y_test)","47f54555":"# instantiate a logistic regression model, and fit with X and y (with training data in X,y)\nmodel = LogisticRegression(random_state = random_state)\nmodel.fit(X_train, y_train)\n\n# check the accuracy on the training set\nprint(\"Accuracy on training set: \", model.score(X_train, y_train))\n# check the accuracy on the test set\nprint(\"Accuracy on test set: \", model.score(X_test, y_test))","d9791d1a":"pred_train = model.predict(X_train)\n#confusion_matrix(y_train,pred_train)\npd.crosstab(y_train, pred_train, rownames=['Predicted'], colnames=['Reality'], margins=True)","2b0dffb5":"pred_test = model.predict(X_test)\n#confusion_matrix(y_test,pred_test)\npd.crosstab(y_test, pred_test, rownames=['Predicted'], colnames=['Reality'], margins=True)","d60781e0":"data = dataset[dataset.columns[:13]]\noutcome = dataset['pred_attribute']\n\n#train,test=train_test_split(dataset,test_size=0.25,random_state=0,stratify=dataset['pred_attribute'])# stratify the outcome\n#train_X=train[train.columns[:13]]\n#test_X=test[test.columns[:13]]\n#train_Y=train['pred_attribute']\n#test_Y=test['pred_attribute']\n\ndata_copy=[]\nfor model in algorithms:\n    model.fit(X_train,y_train)\n    pred_test = model.predict(X_test)\n    data_copy.append(metrics.accuracy_score(pred_test, y_test))\n    \nmodels_df = pd.DataFrame(data_copy, index=names)   \nmodels_df.columns=['Accuracy']\nmodels_df","261ca955":"sns.heatmap(dataset[dataset.columns[:14]].corr(),annot=True,cmap='RdYlGn')\nfig=plt.gcf()\nfig.set_size_inches(25,20)\n#plt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nplt.show()","1a5b1e83":"model = ExtraTreesClassifier(n_estimators=100,random_state=random_state)\nmodel.fit(X, y)\npd.Series(model.feature_importances_, index=data.columns).sort_values(ascending=False)","4923227e":"model = RandomForestClassifier(n_estimators=100,random_state=random_state) # , max_features=(13 ** 0.5))\nmodel.fit(X, y)\npd.Series(model.feature_importances_, index=data.columns).sort_values(ascending=False)","cbd87e7d":"# Dataset of selected features\ndataset_select = dataset[['ca','thal','thalach','cp','oldpeak','exang','pred_attribute']]\n\nX = dataset_select.iloc[:, :-1].values  \ny = dataset_select.iloc[:, -1].values\n#X = np_dataset[:, :-1]  \n#y = np_dataset[:, -1]  ","98c1b36c":"my_imputer = my_imputer.fit(X[:,0:6])   \nX[:, 0:6] = my_imputer.transform(X[:, 0:6])","df8fe9ed":"X = scaler.fit_transform(X)","8174077f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)","82011819":"err_score(X_train, X_test, y_train, y_test)","e98947a4":"data_copy=[]\nfor model in algorithms:\n    model.fit(X_train,y_train)\n    pred_test = model.predict(X_test)\n    data_copy.append(metrics.accuracy_score(pred_test, y_test))\n    \nmodels_df2 = pd.DataFrame(data_copy, index=names)  \nmodels_df2.columns = ['New Accuracy']  \n\nmodels_df2 = models_df2.merge(models_df, left_index=True, right_index=True, how='left')\nmodels_df2['Increase'] = models_df2['New Accuracy'] - models_df2['Accuracy']\nmodels_df2","2b5b4a7f":"#Testing and comparing various classifications for: Accuracy | ROC | Area under Curve | Gmean | Precision | Recall \n\nkfold = KFold(n_splits=10, random_state=random_state) # k=10, split the data into 10 equal parts\nCV_results = []\n# iterate over classifiers\nAcc= {}\nAcc_Train = {}\nAcc_Test = {}\nStd_Train={}\nStd_Test={}\nPredictions = {}\nROC = {}\nAUC = {}\nGmean = {}\nPrecision = {}\nRecall = {}\nF1_score = {}\nConfusion_Matrix = {}\nmats = pd.DataFrame(Confusion_Matrix)\n\nfor clf in classifiers:\n    #Acc[clf] = cross_validate(classifiers[clf],X=X,y=y,cv=10,n_jobs=-1,scoring='accuracy',return_train_score=True)\n    Acc[clf] = cross_validate(classifiers[clf],X_train,y_train,cv=kfold,n_jobs=-1,scoring='accuracy',return_train_score=True)\n    Acc_Train[clf] =  Acc[clf]['train_score'].mean()\n    Acc_Test[clf] = Acc[clf]['test_score'].mean()\n    Std_Train[clf] = Acc[clf]['train_score'].std()\n    Std_Test[clf] = Acc[clf]['test_score'].std()\n    CV_results.append(Acc[clf])\n    \n    classifiers[clf].fit(scaler.transform(X_train),y_train)\n    pred =  classifiers[clf].predict(scaler.transform(X_test))\n    ROC[clf] = roc_auc_score(y_test,pred)\n    AUC[clf] = auc(y_test,pred,reorder=True)\n    Gmean[clf] = fowlkes_mallows_score(y_test,pred)\n    Precision[clf] = precision_score(y_test,pred)\n    Recall[clf] = recall_score(y_test,pred)    \n    F1_score[clf] = f1_score(y_test,pred)\n    Confusion_Matrix[clf] = confusion_matrix(y_test,pred)\n\nAccuracy_train = pd.DataFrame([Acc_Train[vals]*100 for vals in Acc_Train],columns=['Accuracy Train'],index=[vals for vals in Acc_Train])\nStd_train = pd.DataFrame([Std_Train[vals]*100 for vals in Std_Train],columns=['S. Deviation Train'],index=[vals for vals in Std_Train])\nAccuracy_pred = pd.DataFrame([Acc_Test[vals]*100 for vals in Acc_Test],columns=['Accuracy Test'],index=[vals for vals in Acc_Test])\nStd_test = pd.DataFrame([Std_Test[vals]*100 for vals in Std_Test],columns=['S. Deviation Test'], index=[vals for vals in Std_Test])\nROC_Area = pd.DataFrame([ROC[vals] for vals in ROC],columns=['ROC(area)'],index=[vals for vals in ROC])\nAUC_Area = pd.DataFrame([AUC[vals] for vals in AUC],columns=['AUC(area)'],index=[vals for vals in AUC])\nGm = pd.DataFrame([Gmean[vals] for vals in Gmean],columns=['Gmean'],index=[vals for vals in Gmean])\nPrec = pd.DataFrame([Precision[vals] for vals in Precision],columns=['Precision'],index=[vals for vals in Precision])\nRec = pd.DataFrame([Recall[vals] for vals in Recall],columns=['Recall'],index=[vals for vals in Recall])\nF1 =  pd.DataFrame([F1_score[vals] for vals in F1_score],columns=['F1_score'],index=[vals for vals in F1_score])\n\ntable = pd.concat([Accuracy_train,Std_train,Accuracy_pred,Std_test,ROC_Area,AUC_Area,Gm,Prec,Rec,F1], axis=1)\ntable.loc['MEAN VALUE'] = table.mean()\ntable","dd4f8b8b":"#box=pd.DataFrame(Acc_Test,index=[names])\n#sns.boxplot(Accuracy_pred.T)\n#plt.show()","63390188":"k_range=list(range(1,50))\ndata_copy=pd.Series()\nk_scores = []\n#x=[0,1,2,3,4]\nfor i in k_range:\n    model=KNeighborsClassifier(n_neighbors=i) \n    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n    k_scores.append(scores.mean())\n    model.fit(X_train,y_train)\n    pred_test=model.predict(X_test)\n    data_copy=data_copy.append(pd.Series(metrics.accuracy_score(pred_test,y_test)))\n\n#plt.plot(k_range, k_scores)\nplt.plot(k_range, data_copy)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')    \n#plt.xticks(X)\nplt.show()\nprint('Accuracies for different values of n are:\\n', data_copy.values)","bf7b5769":"svc = SVC(probability=True, random_state=random_state)\n# Set the parameters by cross-validation\nsvc_pg = [{'kernel': ['rbf'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4],'C': [1, 10, 100, 1000]},\n          {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    svc.fit(X_train, y_train)\n    y_eval = svc.predict(X_test)\n    acc = sum(y_eval == y_test) \/ float(len(y_test))\n    print(\"Accuracy of SVC: %.2f%%\" % (100*acc))\n    \n    print(\"Tuning hyper-parameters for %s\\n\" % score)\n    svc_gscv = GridSearchCV(svc, svc_pg, cv=kfold, scoring='%s_macro' % score)\n    svc_gscv.fit(X_train, y_train)\n    print(\"Best parameters set found on training set:\")\n    print(svc_gscv.best_params_,\"\\n\")\n    print(\"Grid scores on training set:\")\n    means = svc_gscv.cv_results_['mean_test_score']\n    stds = svc_gscv.cv_results_['std_test_score']\n    \n    for mean, std, params in zip(means, stds, svc_gscv.cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\n    print(\"Detailed classification report:\")\n    print(\"The model is trained on the full training set.\")\n    print(\"The scores are computed on the full test set.\")\n    y_true, y_pred = y_test, svc_gscv.predict(X_test)\n    print(classification_report(y_true, y_pred))\n    \n    svc_est = svc_gscv.best_estimator_\n    svc_score = svc_gscv.best_score_\n    print(\"Best estimator for parameter C: %f\\n\" % (svc_est.C))\n    print(\"Best score: %0.2f%%\\n\" % (100*svc_score))\n\n    #print(clf)","6f728466":"gnb = GaussianNB()\n#gnb_gscv = GridSearchCV(gnb,param_grid = ,cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n#gnb.fit(X_train, y_train)\n\n#gnb_est = gnb.best_estimator_\n#gnb_score = gnb.best_score_\n#print(\"Best estimator:\", gnb_est, \"\\nBest Score:\", gnb_score)","d3340705":"mlp = MLPClassifier(momentum=0.15,solver='sgd',learning_rate_init=1.0, early_stopping=True, shuffle=True,random_state=random_state)\n\nmlp_pg={\n'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n'hidden_layer_sizes': [(10,10),(100,),(100,10)],\n#'hidden_layer_sizes': [x for x in itertools.product((10,20,30,40,50,100),repeat=3)],\n#'tol': [1e-2, 1e-3, 1e-4],\n#'epsilon': [1e-3, 1e-7, 1e-8],\n'alpha': [1e-2, 1e-3, 1e-4],\n#'activation': [\"logistic\", \"relu\", \"Tanh\"]\n}\n\nmlp_gscv = GridSearchCV(mlp,param_grid=mlp_pg, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\nmlp_gscv.fit(X_train, y_train)\n\nmlp_est = mlp_gscv.best_estimator_\nmlp_score = mlp_gscv.best_score_\nprint(\"Best estimator:\", mlp_est, \"\\nBest Score:\", mlp_score)","d0dfb1c0":"lr = LogisticRegression(\n    C=0.1,\n    penalty='l2',\n    dual=True,\n    tol=0.0001, \n    fit_intercept=True,\n    intercept_scaling=1.0, \n    class_weight=None,\n    random_state=random_state)\n\nlr_pg = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\nlr_gscv = GridSearchCV(lr,param_grid=lr_pg, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\nlr_gscv.fit(X_train, y_train)\n\nlr_est = lr_gscv.best_estimator_\nlr_score = lr_gscv.best_score_\nprint(\"Best estimator:\", lr_est, \"\\nBest Score:\", lr_score)","147ba5b3":"xgbc = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n     n_estimators= 2000,\n     max_depth= 4,\n     min_child_weight= 2,\n     gamma=0.9,                        \n     subsample=0.8,\n     colsample_bytree=0.8,\n     objective= 'binary:logistic',\n     nthread= -1,\n     scale_pos_weight=1)\n\nxgbc.fit(X_train, y_train)\n\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_train, y_train, test_size=0.3, random_state=random_state)\ntrain2 = xgb.DMatrix(X_train2, y_train2)\ntest2 = xgb.DMatrix(X_test2, y_test2)\nwatchlist = [(test2, 'test')]\nxgb_pg = {\n    'max_depth': 3,\n    'booster': 'gbtree',\n    'objective': 'reg:linear',#'binary:logistic'\n    'subsample': 0.8,\n    'colsample_bytree': 0.85,\n    'eta': 0.05,\n    'max_depth': 7,\n    'seed': 43,\n    'silent': 0,\n    'eval_metric': 'rmse' #root mean square estimate\n}\n\nbst = xgb.train(xgb_pg, train2, 500, watchlist, early_stopping_rounds=50)\n#bst.save_model(\"model_new\")\n#bst = xgb.Booster(xgb_pg)\n#bst.load_model(\"model_new\")\n\n# make prediction\n#pred = bst.predict(xgb.DMatrix(X_test))\nprediction = xgbc.predict(X_test)\npred = bst.predict(test2,ntree_limit=bst.best_ntree_limit)\nxgb.to_graphviz(bst, num_trees=2)","2d2acd96":"class Ensemble(object):\n    def __init__(self, n_folds, stacker, base_models):\n        self.n_folds = n_folds\n        self.stacker = stacker\n        self.base_models = base_models\n\n    def fit_predict(self, X, y, T):\n        X = np.array(X)\n        y = np.array(y)\n        T = np.array(T)\n\n        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n\n        for i, clf in enumerate(self.base_models):\n            S_test_i = np.zeros((T.shape[0], len(folds)))\n\n            for j, (train_idx, test_idx) in enumerate(folds):\n                X_train = X[train_idx]\n                y_train = y[train_idx]\n                X_holdout = X[test_idx]\n                # y_holdout = y[test_idx]\n                clf.fit(X_train, y_train)\n                y_pred = clf.predict(X_holdout)[:]\n                S_train[test_idx, i] = y_pred\n                S_test_i[:, j] = clf.predict(T)[:]\n\n            S_test[:, i] = S_test_i.mean(1)\n\n        self.stacker.fit(S_train, y)\n        y_pred = self.stacker.predict(S_test)[:]\n        return y_pred","df2a7336":"gnb_mlp=VotingClassifier(estimators=[('Guassian Naive Bayes', gnb),('Multilayer Perceptron',mlp)], voting='soft', weights=[2,1]).fit(X_train,y_train)\nprint('The accuracy for Guassian Naive Bayes and Multilayer Perceptron:',gnb_mlp.score(X_test,y_test))","46899bcd":"gnb_lr=VotingClassifier(estimators=[('Guassian Naive Bayes', gnb),('Logistic Regression', lr)], voting='soft', weights=[2,1]).fit(X_train,y_train)\nprint('The accuracy for Guassian Naive Bayes and Logistic Regression:',gnb_lr.score(X_test,y_test))","dc7bd2ff":"mlp_lr=VotingClassifier(estimators=[('Multilayer Perceptron',mlp), ('Logistic Regression', lr)], voting='soft', weights=[2,1]).fit(X_train,y_train)\nprint('The accuracy for Multilayer Perceptron and Logistic Regression:',mlp_lr.score(X_test,y_test))","f4f48d93":"gnb_mlp_lr=VotingClassifier(estimators=[('Guassian Naive Bayes', gnb),('Multilayer Perceptron',mlp), ('Logistic Regression', lr)], voting='soft', weights=[3,2,1]).fit(X_train,y_train)\nprint('The ensembled model with Guassian Naive Bayes, Multilayer Perceptron, and Logistic Regression:',gnb_mlp_lr.score(X_test,y_test))","f071dde4":"# Generate a simple plot of the test and training learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)): \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt\n\n#gnb_plot = plot_learning_curve(gnb_est,\"GNB learning curves\",X_train,Y_train,cv=kfold)\nmlp_plot = plot_learning_curve(mlp_est,\"MLP learning curves\",X_train,y_train,cv=kfold)\nlr_plot = plot_learning_curve(lr_est,\"LR learning curves\",X_train,y_train,cv=kfold)\nsvc_plot = plot_learning_curve(svc_est,\"SVC learning curves\",X_train,y_train,cv=kfold)\n#xgb_plot = plot_learning_curve(xgb_gscv.best_estimator_,\"XBG learning curves\",X_train,Y_train,cv=kfold)","1a1c2875":"#test_survived_gnb = pd.Series(gnb_est.predict(test), name=\"GNBC\")\ntest_Survived_mlp = pd.Series(mlp_est.predict(X_test), name=\"MLP\")\ntest_Survived_lr = pd.Series(lr_est.predict(X_test), name=\"LR\")\ntest_Survived_svc = pd.Series(svc_est.predict(X_test), name=\"SVC\")\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_mlp,test_Survived_lr,test_Survived_svc],axis=1)\ng= sns.heatmap(ensemble_results.corr(),annot=True)","59344b2c":"finalVC = VotingClassifier(estimators=[('Guassian Naive Bayes', gnb),('Multilayer Perceptron',mlp), ('Logistic Regression', lr),('SVM',svc)], voting='soft', weights=[4,3,2,1], n_jobs = -1).fit(X_train,y_train)\nprint('The ensembled model with all classfiers: ',finalVC.score(X_test,y_test))\n\n# Generate Submission File \ntest_healthy = pd.Series(finalVC.predict(X_test), name=\"Healthy\")\n#results = pd.concat([X_test['index'],test_healthy],axis=1)\n#results.to_csv(\"Results_series_NSR.csv\", index=False)\n#submission1 = pd.DataFrame({'Healthy Probabilty': pred })\n#submission1.to_csv(\"Results_df1_NSR.csv\", index=True)\n#print(submission1)\n\n","f986ad01":"submission = pd.DataFrame({'Healthy': prediction })\nsubmission.to_csv(\"Results_df_NSR.csv\", index=True)\nprint(submission)","f9cfd4be":"### Tuning XGBoost\n These XGBoost parameters are generally considered to have real impacts on its performance:\n\n1. eta: Step size used in updating weights in each boosting step to prevent overfitting. Lower eta means slower training but better convergence.\n1. num_round: Total number of iterations.\n1. subsample: The ratio of training data used in each iteration. This is to combat overfitting.\n1. colsample_bytree: The ratio of features used in each iteration. This is like max_features in RandomForestClassifier.\n1. gamma : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n1. max_depth: The maximum depth of each tree. Unlike random forest, gradient boosting would eventually overfit if we do not limit its depth. If set to too high a number might run the risk of overfitting.\n1. early_stopping_rounds: If we don\u2019t see an increase of validation score for a given number of iterations, the algorithm will stop early. This is to combat overfitting, too.\n\nUsual tuning steps:\n\n1. Reserve a portion of training set as the validation set.\n1. Set eta to a relatively high value (e.g. 0.05 ~ 0.1), num_round to 300 ~ 500.\n1. Use grid search to find the best combination of other parameters.\n1. Gradually lower eta until we reach the optimum.\n1. Use the validation set as watch_list to re-train the model with the best parameters. \n1. Observe how score changes on validation set in each iteration. Find the optimal value for early_stopping_rounds.","0ae62fb7":"#### Thalium heart test: \"thal\"  \nValue 3 = normal  \nValue 6 = fixed defect  \nValue 7 = reversable defect   ","b8c6b3f1":"## Data and Environment Setup","d7ca967e":"# Data Exploration\n\nData exploration will be accomplished using exploratory data analysis (EDA). EDA should provide some insights for subsequent processing and modeling. This analysis will include visualization of the data and statistical examinations.","a7d43717":"# Ensemble Engineering\n\nCreating ensemble models from base models is a common way to boost accuracy. Ensemble methods are techniques that create multiple models and then combine them to produce improved results. Ensemble methods usually produces more accurate solutions than a single model would. The models used to create such ensemble models are called \u2018**base models**\u2019. Ensemble Learning refers to the technique of combining different models. It reduces both bias and variance of the final model , thus increasing the score and reducing the risk of overfitting. \n\nIn theory, for the ensemble to perform well, two factors matter:  \n1. Base models should be as unrelated as possibly. This is why we tend to include non-tree-based models in the ensemble even though they don\u2019t perform as well. The math says that the greater the diversity, and less bias in the final ensemble.  \n1. Performance of base models shouldn\u2019t differ to much.\n    1. Trade-off: In practice we may end up with highly related models of comparable performances. Yet we ensemble them anyway because it usually increase the overall performance.","3f833314":"# **Using machine learning to understand, predict, and prevent cardiovascular disease**\n    Nathan S. Robinson\n    Computing Science Department\n    Sam Houston State University\n    nsr004@shsu.edu\n    August 5th, 2018","64dca112":"# Feature Engineering\n\n1. A lot of features can affect the accuracy of the algorithm.  \n1. Less features mean faster training\n1. Some features are linearly related to others. This might put a strain on the model.\n1. By picking up the most important features, we can use interactions between them as new features. Sometimes this gives surprising improvement.\n1. Feature Selection\n    1. means to select only the important features in-order to improve the accuracy of the algorithm.  \n1. It reduces training time and reduces overfitting    \n1. We can choose important features in 3 ways:  \n    1. Correlation matrix: selecting only the uncorrelated features.  \n    1. Extra Trees Classifier\n    1. RandomForestClassifier: gives the importance of the features","00e69791":"## Review\n\nA quick outline of what was done in this notebook:\n\n1. Introduction: Premise for this work and import literally hundreds of machine learning algorithms, analysis tools, and evironment tools\n1. Data Exploration: Discuss the heart data to be used and inspect it visually\n1. Data Preprocessing: Clean up the messy parts of the dataset and prepare it to be used by the ML models\n1. Feature Engineering: Compare importance of features and make a new dataset with only the most impotant ones\n1. Model Engineering: Compare the accuracy of models and tune only the most accurate ones\n1. Ensemble Generation: Combine the now tuned models into one model in hopes of boosting the accuracy of the predictions\n1. Conclusion: Where we are now: discussing the results\n\nAll of these steps contributibute to a more useful, accurate model, with a final accuracy of 89.01%.","41206797":"### Tuning Guassian Naive Bayes  \nGNB has no hyperparameters for the Cross Validation Grid Search to test. Instead GNB benefits largely from the data preprocessing and feature selection that was performed earlier","7f1f9ce0":"## Stacking\n\nIt\u2019s much like cross validation. Take 5-fold stacking as an example.  \n1. First we split the training data into 5 folds.  \n1. Next we will do 5 iterations. In each iteration, train every base model on 4 folds and predict on the hold-out fold. You have to keep the predictions on the testing data as well. This way, in each iteration every base model will make predictions on 1 fold of the training data and all of the testing data.  \n1. After 5 iterations we will obtain a matrix of shape #(samples in training data) X #(base models). This matrix is then fed to the stacker (it\u2019s just another model) in the second level.  \n1. After the stacker is fitted, use the predictions on testing data by base models (each base model is trained 5 times, therefore we have to take an average to obtain a matrix of the same shape) as the input for the stacker and obtain our final predictions.","aa70358e":"#### Box plot","78a1644a":"## Accuracy of models with all features\nNow without feauture engineering and  lets check how models perform on test and train data","a9e4c5bc":"# Model Engineering","51f1d830":"### Review continuous features\nPair plot, box plot, basic statistics","9f3142b1":"## Imputation","730319c3":"### Heatmap of Ensemable ","f47a53ab":"#### Pair plot\n   \n   1. The diagonal shows the distribution of the the dataset with the kernel density plots.  \n   1. The scatter-plots shows the relation between each and every attribute or features taken pairwise.  \n   1. Looking at the scatter-plots, we can say that no two attributes are able to clearly seperate the two outcome-class instances.  ","530416df":"### Extra Trees Classification","8300c44d":"#### Resting electrocardiographic results: \"restecg\"   \nValue 0 = normal   \nValue 1 = having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)   \nValue 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria  ","0d011c2d":"#### Fasting blood sugar: \"fbs\"\nValue 0 = fbs <= 120 mg\/dl  \nValue 1 = fbs > 120 mg\/dl","a8dae210":"#### Only heart disease partipants","f85b4414":"## Motivation\nThe American Heart Association Statistics 2016 Report indicates that heart disease is the leading cause of death for both men and women, responsible for 1 in every 4 deaths. Even modest improvements in prognostic models of heart events and complications could save hundreds of lives and help to significantly reduce the cost of health care services, medications, and lost productivity.\n","20ac3c43":"#### Slope of the peak exercise ST segment: \"slop\"    \nValue 1 = upsloping   \nValue 2 = flat   \nValue 3 = downsloping  ","27e282dc":"### k-Fold Cross Validation\n\nCross validation is an essential step in model training. It tells us whether our model is at high risk of overfitting. In many competitions, public LB scores are not very reliable. Often when we improve the model and get a better local CV score, the LB score becomes worse. It is widely believed that we should trust our CV scores under such situation. Ideally we would want CV scores obtained by different approaches to improve in sync with each other and with the LB score, but this is not always possible.\n\nUsually 5-fold CV is good enough. If we use more folds, the CV score would become more reliable, but the training takes longer to finish as well. However, we shouldn\u2019t use too many folds if our training data is limited. Otherwise we would have too few samples in each fold to guarantee statistical significance.\n\nMany times the data is imbalanced, i.e there may be a high number of class1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset. \n\n1. The k-Fold Cross Validation works by first dividing the dataset into k-subsets.\n1. Let's say we divide the dataset into (k=10) parts. We reserve 1 part for testing and train the algorithm over the other 9 parts.\n1. We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm.\n1. An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model.","92de435f":"#### Number of major vessels colored by flourosopy : \"ca\"\nValue 0 = 0 major blood vessels colored\nValue 1 = 1 major blood vessels colored\nValue 2 = 2 major blood vessels colored\nValue 3 = 3 major blood vessels colored","d7cf69a9":"### Pearson Correlation Heatmap","0db7ad47":"**Predict class labels for the test set**  \n0 = Healthy  \n1 = Sick","9040079b":"# Conclusion  \n\nWhen given a raw set of data to work with there are many ways in which to refine it, analyze it, and make use of it. That is the ultimate goal accomplished by this notebook. I have demonstrated nearly all the key steps to take a dataset and turn it into a tool to potentially make the world a better place.","04d98d6d":"#### Exercise induced angina (chest pain): \"exang\"\nValue 0 = no    \nValue 1 = yes  ","b3ebf056":"#### Sex: \"sex\"\nValue 0 = female  \nValue 1 = male  ","61b0ee00":"## Error from imputation","f538968c":"## Methods \n\nThere are hundreds of machine learning models, but often only a few yeild high accuracy results for a given dataset. In an effort to refine the search for a useful and accurate method with this dataset, the results of serveral algorithms will be compared. The front-runnners will be analyzed and used to develop a unique, higher-accuracy method. The end goal is to produce an approved machine learning application in healthcare; to be approved it had to pass tests to show it can produce results at least as accurately as humans are currently able to. Recently, such ML models were also used to detect with cardiologist-level accuracy 14 types of arrhythmias (sometime life-threatening heart beats) form ECG-electrocardiogram signals generated by wearable monitors. ","22e7d6c1":"## Standardization","5914d5d6":" ### Import and store ML models\n 17 different models from 8 different categories of machine learning algorithms","7ccb7c60":"### Import data processing and analysis tools\nThis Python 3 environment comes with many helpful analytics libraries installed  \nIt is defined by the [kaggle\/python docker image](https:\/\/github.com\/kaggle\/docker-python)","eac87598":"### Candidates for selection\nGuassian Naive Bayes(GNB), Multilayer Perceptron, and Logistic Regression are the top three performers. We will use these and a couple other base models to create a stacked ensemble. Almost all of the decision-based models over fit on the train.","b80ec179":"## Accuracy of models with selected features","bed4a14d":"**Selected Features Preprocessing:**  \nData must be stardardized, stratified, and imputated again before the ML models can be compared.  \nUsing the overlapping features from the two tests above, the new set of features will only include: _cp, thalach, oldpeak, ca, thal, exang_","d6c56377":"### Tuning Logistic Regression","d826ba0b":"### Random Forest Classification","c788221f":"# Data Preprocessing\n\nRaw data files must often but review and then cleaned to be effectively used for analysis and machine learning purposes. This includes:\n\n1. Dealing with missing data.\n1. Dealing with outliers.\n1. Encoding categorical variables if necessary.\n1. Dealing with noise.\n    1. For example: floats derived from raw figures may be truncated. The loss of precision during floating-point arithemics can bring noise into the data: two seemingly different values might be the same before conversion. Noise can harm model performance.","fb042143":"## Objectives\n\n1. Understand the problem: look at each variable and do a philosophical analysis about their meaning and importance for this problem.  \n1. Univariable study: focus on the dependent variable ('SalePrice') and try to know a little bit more about it.  \n1. Multivariate study: understand how the dependent variable and independent variables relate.  \n1. Basic cleaning: clean the dataset and handle the missing data, outliers and categorical variables  \n1. Test assumptions: check if our data meets the assumptions required by most multivariate techniques.  \n\nEstablish the relative performance of deep learning models, such as deep belief networks and convolutional neural networks, and ensembles with respect to classical machine learning algorithms (including logistic regression) using cases studies built from well-known heart disease data sets such as the Cleveland set available from the UCI repository  \n\nResearch questions of interest are:  \nFor what would be the threshold of sample size in heart disease studies where the more complex but potentially more effective deep learning models would be recommended?  \nWould ensembles of machine learning models be able to provide more robust predictions as it has been the case in other knowledge domains?  \nDoes the ACC\/AHA list of eight risk factors should be updated with other genetic or lifestyle factors?  \n\nThe deep learning models will be implemented in Tensorflow (originally from Google, now open source) and healthcare.ai, an open source that facilitate the development of machine learning in healthcare, with the prevision that can handle so called big data by using the Hadoop\/Spark platform.   ","142c9544":"## Review Data Source\n\nThe authors of the databases have requested:  \n  ...that any publications resulting from the use of the data include the names of the principal investigator responsible for the data collection at each institution.  They would be:  \n\n   1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n   2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n   3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n   4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:  Robert Detrano, M.D., Ph.D.\n\n#### Past Usage\n\n1. Detrano,~R., Janosi,~A., Steinbrunn,~W., Pfisterer,~M., Schmid,~J., Sandhu,~S., Guppy,~K., Lee,~S., \\& Froelicher,~V. (1989). \n    1. {\\it International application of a new probability algorithm for the diagnosis of coronary artery disease.} {\\it American Journal of Cardiology}, {\\it 64},304-310.\n    1. International Probability Analysis\n    1. Address: Robert Detrano, M.D. Cardiology 111-C V.A. Medical Center 5901 E. 7th Street Long Beach, CA 90028\n    1. Results in percent accuracy: (for 0.5 probability threshold) Data Name: CDF CADENZA\n    1. Hungarian 77 74 Long beach 79 77 Swiss 81 81\n    1. Approximately a 77% correct classification accuracy with a logistic-regression-derived discriminant function\n1. David W. Aha & Dennis Kibler \n    1. Instance-based prediction of heart-disease presence with the Cleveland database \n    1. NTgrowth: 77.0% accuracy \n    1. C4: 74.8% accuracy\n1. John Gennari \n    1. Gennari, J.~H., Langley, P, \\& Fisher, D. (1989). Models of incremental concept formation. {\\it Artificial Intelligence, 40}, 11-61. \n    1. Results: The CLASSIT conceptual clustering system achieved a 78.9% accuracy on the Cleveland database.\n\n#### Relevant Information\n\n1. This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.\n1. In particular, the Cleveland Clinic Foundation (cleveland.data) database is the only one that has been used by ML researchers to this date. \n1. The \"pred_attribute\" field refers to the presence of heart disease in the patient. \n1. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\n\n#### Complete attribute documentation\n\n1. age: age in years\n2. sex: (1 = male; 0 = female) \n3. cp: chest pain type\n    1. Value 1: typical angina\n    1. Value 2: atypical angina\n    1. Value 3: non-anginal pain\n    1. Value 4: asymptomatic  \n4. trestbps: resting blood pressure (in mm Hg on admission to the hospital) \n5. chol: serum cholestoral in mg\/dl  \n6. fbs: (fasting blood sugar > 120 mg\/dl)\n    1. 1 = true\n    1. 0 = false  \n7. restecg: (resting electrocardiographic results)\n    1. Value 0: normal    \n    1. Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)    \n    1. Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n8. thalach: maximum heart rate achieved  \n9. exang: exercise induced angina\n    1. 1 = yes\n    1. 0 = no \n10. oldpeak: ST depression induced by exercise relative to rest \n11. slope: the slope of the peak exercise ST segment     \n    1. Value 1: upsloping\n    1. Value 2: flat    \n    1. Value 3: downsloping  \n12. ca: number of major vessels (0-3) colored by flourosopy  \n13. thal: thalium heart scan    \n    1. 3 = normal (no cold spots)\n    1. 6 = fixed defect (cold spots during rest and exercise)   \n    1. 7 = reversible defect (when cold spots only appear during exercise)\n14. pred_attribute: (the predicted attribute) diagnosis of heart disease (angiographic disease status)    \n    1. Value 0: < 50% diameter narrowing     \n    1. Value 1: > 50% diameter narrowing (in any major vessel: attributes 59 through 68 are vessels)","52a4e1d9":"### Import Dataset","b23c795b":"## Stratification","07a6ca57":"## Error from imputation\nGet mean absolute error score from imputation with extra columns showing what was imputed","092bd569":"## Confusion matrices of training and test sets","84ee9aab":"### Tuning Linear and Radial SVC\nNormally the best set of parameters are found by a process called grid search. Grid search iterates through all the possible combinations to find the best set of parameters.","473c3a80":"## Ensemble models","8078a7c8":"## Standardization\nThere can be a lot of deviation in the given dataset. High variance has to be standardised. Standardization, or normalization, is a useful technique to transform attributes with a Gaussian distribution,  differing means, and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1.","f7208839":"### Review categorical features  \nHistograms, basic statistics","019d549a":"### Example of Tuning\nk-NN Accuracies for different values of 'n' neaghbors","92b6859d":"### Voting Classifier of all tuned models for submission","e372b760":"## Visualization\n\n1. Inspect the distribution of the target variable (an imbalanced distribution of target variable might harm the performance of some models) \n1. Use box plot and scatter plot to inspect their distributions and check for outliers\n1. Plot the data with points colored according to their classification tasks, this helps with feature engineering\n1. Make pairwise distribution plots and examine their correlations\n\n**Statistical Tests**  \nWe can perform some statistical tests to confirm our hypotheses. Sometimes we can get enough intuition from visualization, but quantitative results are always good to have. Note that we will always encounter non-i.i.d. data in real world. So we have to be careful about which test to use and how we interpret the findings.\n\nIn many competitions public LB scores are not very consistent with local CV scores due to noise or non-i.i.d. distribution. You can use test results to roughly set a threshold for determining whether an increase of score is due to genuine improvment or randomness.","a21b84df":"#### Chest pain type: \"cp\"  \nValue 1 = typical angina   \nValue 2 = atypical angina   \nValue 3 = non-anginal pain   \nValue 4 = asymptomatic   ","b7340209":"## Model Selection\nSelecting accurate models, tuning them, and training them is essential to robust results. Just as some features were selected based on their importance or correlation, selecting a subgroup of base models will inform a better-performing ensemble model.","c36d10cb":"### Review all features","647b790b":"#### Diagnosis of heart disease (angiographic disease status): \"pred_attribute\"  \nValue 0 = diameter narrowing <= 50%  (Healthy)  \nValue 1 =  diameter narrowing > 50% (Sick)  ","ede66c5d":"# Introduction","2c923414":"## Parameter Tuning\nSelecting best possible parameters for  our top three: Guassian Naive Bayes(GNB), Multilayer Perceptron(MP), and Logistic Regression(LR).\nImprove a model\u2019s performance by tuning its parameters. A model usually have many parameters, but only a few of them are significant to its performance. For example, the most important parameters for a random forest is the number of trees in the forest and the maximum number of features used in developing each tree. We need to understand how models work and what impact each parameter has to the model\u2019s performance, whether it's accuracy, robustness, or speed.","002f11aa":"## Imputation\n**Check for invalid tuples in dataset**\nAny NaN or invalid entries will have to be addressed via imputation or feature removal","552d6e32":"### Tuning Multilayer Perceptron","9a1ddea0":"The Pearson Correlation plot indicated that there are no strongly correlated features.  \nThis is good from a point of view of feeding these features into the learning model because this means that there isn't much redundant or superfluous data in our training set.  \nNo features can be selected from this plot.  ","3dc56037":"#### All participants","004e3946":"### Review heart disease dataset samples","6512009c":"## Voting Ensemble\nVoting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n\nA Weighted Voting Classifier will be used to stack the top three base models: Guassian Naive Bayes, Multilayer Perceptron, and Logistic Regression. Classifiers will be assigned weight according to their accuracies. So the classifier with single accuracy will be assigned the highest weight and so on. ","e325cbe1":"## Stratification\nWhen we split the dataset into train and test datasets, the split is completely random. Thus the instances of each class label or outcome in the train or test datasets is random. Thus we may have many instances of class 1 in training data and less instances of class 2 in the training data. So during classification, we may have accurate predictions for class1 but not for class2. Thus we stratify the data, so that we have proportionate data for all the classes in both the training and testing data.\n\n**Define training and test samples**  \nThe Cleveland data set available from the UCI repository has 303 samples; the training and test data sets were randomly selected with 30% of the original data set corresponding to the test data set.  The relative proportions of the classes of interest (disease\/no disease) in both sets were checked to be similar.","fc39e11a":"** Predict class labels for the training set**  \n0 = Healthy  \n1 = Sick","482eca72":"## Accuracy of training and test sets","dc72826f":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>","bb80ee08":"## Feature Selection","b18962bd":"#### Statistics"}}