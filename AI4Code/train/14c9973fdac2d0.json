{"cell_type":{"863868c2":"code","650b4caf":"code","4fc65796":"code","6716c276":"code","8cc05986":"code","343d3bfc":"code","de3015e6":"code","9c626a07":"code","efcafb1c":"code","eae53d63":"code","dfb2152b":"code","b46ede98":"code","cfe91bba":"code","5d883f54":"code","3e6aa115":"code","f3a84d7a":"code","cb554196":"code","8e97b16a":"code","25b56543":"code","550ed76e":"code","b392c731":"code","ab91cdf6":"code","9f1556a2":"code","d783f18b":"code","108f8156":"code","a9a7052d":"code","d8442121":"code","df1b1472":"code","344bce33":"code","dfce901e":"code","7742f7dd":"code","0fef2890":"code","95282a35":"code","de264411":"code","56d0dc59":"code","49068561":"code","4c2670d4":"code","348da7fb":"code","48b356e2":"code","55a901aa":"code","0b9816b1":"code","59208cea":"code","8a5f0142":"code","c1abe633":"code","4fc25b54":"code","12a74a68":"code","a8970dae":"code","48867ff4":"code","3d866076":"code","94f64464":"code","b43205ac":"code","d511ca86":"code","cb721fe1":"code","f3dbd236":"code","690847f2":"code","09fdb4f1":"code","e3098cd8":"code","d04fd60c":"code","46fa6ac3":"code","04e5b2ff":"code","7f7d8de6":"markdown","dd7dafad":"markdown","455c43d0":"markdown","cf7f5b40":"markdown","af8b2073":"markdown","353af465":"markdown","3694c67c":"markdown","41ae5533":"markdown","c4bbe7a7":"markdown","4b01888b":"markdown","c7063497":"markdown","06aab7ce":"markdown","283dc7a3":"markdown","c471af12":"markdown","21ae0a77":"markdown","7d89b883":"markdown","1300571d":"markdown","2716090e":"markdown","93ec9dac":"markdown","8457beca":"markdown","4d6105d4":"markdown","45e91636":"markdown","67a0ebad":"markdown","bafad38a":"markdown","1e9f1f35":"markdown","0acbc956":"markdown","1e5e6cf5":"markdown","e35d0c9a":"markdown","e5dc25bd":"markdown","dec553b4":"markdown","5b0fcd18":"markdown","2f5bb118":"markdown","4baf9195":"markdown","3ce787d1":"markdown","8acf2665":"markdown","02930412":"markdown","527b6835":"markdown","e8c66eac":"markdown"},"source":{"863868c2":"#Import the required Libraries.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","650b4caf":"#Read the data in pandas\ninp0= pd.read_csv(\"..\/input\/dress-sales-eda\/Dress.csv\")\ninp1= pd.read_csv(\"..\/input\/dress-sales-eda\/DressSales.csv\")","4fc65796":"inp0.head()","6716c276":"inp1.head()","8cc05986":"# Print the information about the attributes of inp0 and inp1.\ninp0.Price.value_counts()","343d3bfc":"inp0.Season.value_counts()","de3015e6":"inp0.info()","9c626a07":"# Column fixing, correcting size abbreviation. count the percentage of each size category in \"Size\" column.\ninp0.Size = inp0.Size.apply(lambda x: \"Medium\" if x == 'M' else x)\ninp0.Size = inp0.Size.apply(lambda x: \"Large\" if x == 'L' else x)\ninp0.Size = inp0.Size.apply(lambda x: \"Extra large\" if x == 'XL' else x)\ninp0.Size = inp0.Size.apply(lambda x: \"Free\" if x == 'free' else x)\ninp0.Size = inp0.Size.apply(lambda x: \"Small\" if x == 'S' else x)\ninp0.Size = inp0.Size.apply(lambda x: \"Small\" if x == 'small' else x)","efcafb1c":"inp0.Size.value_counts(normalize=True)","eae53d63":"# Print the value counts of each category in \"Size\" column.\ninp0.Size.value_counts()","dfb2152b":"# Print the null count of each variables of inp0 and inp1.\ninp0.isnull().sum()","b46ede98":"inp1.isnull().sum()","cfe91bba":"# Print the data types information of inp1 i.e. \"Dress Sales\" data.\ninp1.info()\n","5d883f54":"# Do the required changes in the \"Dress Sales\" data set to make it ready for conversion into float type.\ninp1.loc[inp1['09-12-2013'] == 'Removed',\"09-12-2013\"] = np.NaN\ninp1.loc[inp1['14-09-2013'] == 'removed',\"14-09-2013\"] = np.NaN\ninp1.loc[inp1['16-09-2013'] == 'removed',\"16-09-2013\"] = np.NaN\ninp1.loc[inp1['18-09-2013'] == 'removed',\"18-09-2013\"] = np.NaN\ninp1.loc[inp1['20-09-2013'] == 'removed',\"20-09-2013\"] = np.NaN\ninp1.loc[inp1['22-09-2013'] == 'Orders',\"20-09-2013\"] = np.NaN","3e6aa115":"# Convert the object type columns in \"Dress Sales\" into float type of data type.\ninp1['09-12-2013'] = inp1['09-12-2013'].apply(pd.to_numeric, downcast='float', errors='coerce')\ninp1['14-09-2013'] = inp1['14-09-2013'].apply(pd.to_numeric, downcast='float', errors='coerce')\ninp1['16-09-2013'] = inp1['16-09-2013'].apply(pd.to_numeric, downcast='float', errors='coerce')\ninp1['18-09-2013'] = inp1['18-09-2013'].apply(pd.to_numeric, downcast='float', errors='coerce')\ninp1['20-09-2013'] = inp1['20-09-2013'].apply(pd.to_numeric, downcast='float', errors='coerce')\ninp1['22-09-2013'] = inp1['22-09-2013'].apply(pd.to_numeric, downcast='float', errors='coerce')\n","f3a84d7a":"inp1.info()","cb554196":"# Print the null percetange of each column of inp1.\n(inp1.isnull().sum()\/479)*100","8e97b16a":"inp1.shape","25b56543":"# Drop the columns in \"Dress Sales\" which have more than 40% of missing values.\n\ninp1.drop(columns = ['26-09-2013','30-09-2013','10-02-2013','10-04-2013','10-08-2013','10-10-2013'], inplace = True)","550ed76e":"inp1.shape","b392c731":"# Create the four seasons columns in inp1, according to the above criteria.\ninp1['Summer']= inp1[\"29-08-2013\"].add(inp1[\"31-08-2013\"].add(inp1[\"09-06-2013\"].add(inp1[\"09-08-2013\"].add(inp1[\"10-06-2013\"]))))\ninp1['Autumn']= inp1[\"09-10-2013\"].add(inp1[\"14-09-2013\"].add(inp1[\"16-09-2013\"].add(inp1[\"18-09-2013\"].add(inp1[\"20-09-2013\"].add(inp1[\"22-09-2013\"].add(inp1[\"24-09-2013\"].add(inp1[\"28-09-2013\"])))))))\ninp1['Winter']= inp1[\"09-02-2013\"].add(inp1[\"09-12-2013\"].add(inp1[\"10-12-2013\"]))\ninp1['Spring']= inp1[[\"09-04-2013\"]]","ab91cdf6":"# calculate the sum of sales in each seasons in inp1 i.e. \"Dress Sales\".\nprint(inp1['Summer'].sum())\nprint(inp1['Autumn'].sum())\nprint(inp1['Winter'].sum())\nprint(inp1['Spring'].sum())","9f1556a2":"# Merge inp0 with inp1 into inp0. this is also called left merge.\ninp0 = pd.merge(left=inp0,right=inp1, how='left', left_on='Dress_ID', right_on='Dress_ID')\ninp0.head()","d783f18b":"# Now Drop the Date columns from inp0 as it is already combined into four seasons.\ninp0.drop(inp0.loc[:,'29-08-2013':'10-12-2013'].columns, axis= 1, inplace= True)\ninp0.isnull().sum()","108f8156":"# Print the null count of each columns in inp0 dataframe i.e. combined data frame of inp0 and inp1 without date columns.\n","a9a7052d":"\ninp0[inp0.Autumn.isnull()]","d8442121":"# Deal with the missing values of Type-1 columns: Price, Season, NeckLine, SleeveLength, Winter and Autumn.\n\ninp0 = inp0[~inp0.Price.isnull()]\n\ninp0 = inp0[~inp0.Season.isnull()]\n\ninp0 = inp0[~inp0.NeckLine.isnull()]\n\ninp0 = inp0[~inp0.SleeveLength.isnull()]\n\ninp0 = inp0[~inp0.Winter.isnull()]\n\ninp0 = inp0[~inp0.Autumn.isnull()]","df1b1472":"# Deal with the missing values for Type-2 columns: Material, FabricType, Decoration and Pattern Type.\ninp0.Material= inp0.Material.replace(np.nan, \"Missing\")\n\ninp0.FabricType= inp0.FabricType.replace(np.nan, \"Missing\")\n\ninp0.Decoration= inp0.Decoration.replace(np.nan, \"Missing\")\n\ninp0['Pattern Type']= inp0['Pattern Type'].replace(np.nan, \"Missing\")","344bce33":"#correcting the spellings.\ninp0.Season= inp0.Season.replace('Automn', \"Autumn\")\n\ninp0.Season= inp0.Season.replace('spring', \"Spring\")\n\ninp0.Season= inp0.Season.replace('winter', \"Winter\")","dfce901e":"#correcting the Spellings.\ninp0.SleeveLength= inp0.SleeveLength.replace(['cap-sleeves', 'capsleeves'], \"cap sleeves\")\n\ninp0.SleeveLength= inp0.SleeveLength.replace('full', \"full sleeves\")\n\ninp0.SleeveLength= inp0.SleeveLength.replace(['half','halfsleeve'], \"half sleeves\")\n\ninp0.SleeveLength= inp0.SleeveLength.replace(['sleevless', 'sleeevless', 'sleeveless', 'sleveless'], \"sleeve less\")\n\ninp0.SleeveLength= inp0.SleeveLength.replace(['threequarter','threequater', 'thressqatar'], \"three quater\")\n\ninp0.SleeveLength= inp0.SleeveLength.replace(['turndowncollor','urndowncollor'], \"turn down collar\")\n","7742f7dd":"# Group \"Style\" categories into \"Others\" which have less than 50000 sales across all the seasons.\nother_group = inp0.groupby(['Style'])[[\"Summer\",\"Autumn\", \"Winter\",\"Spring\"]].sum()\nother_group","0fef2890":"other_group[\"Total\"] = other_group[\"Autumn\"].add(other_group[\"Winter\"].add(other_group[\"Spring\"].add(other_group[\"Summer\"])))\nother_group","95282a35":"other_group[\"Total\"].sum()","de264411":"others = other_group[other_group[\"Total\"]<50000]\n","56d0dc59":"others","49068561":"others[\"Total\"].sum()","4c2670d4":"# Calculate the percentage of each categories in the \"Style\" variable.\ncute_sum = 44\/479 * 100\nprint(cute_sum)\nothers_sum = 10\/479 *100\nprint(others_sum)","348da7fb":"print(inp0.Style.value_counts(normalize=True))\ninp0.Style.value_counts(normalize=True).plot.barh()\nplt.show()","48b356e2":"# Describe the numerical variale: \"Autumn\".\ninp0.Autumn.describe()","55a901aa":"55532.000000 - 3126.000000","0b9816b1":"# plot the boxplot of \"Autumn\" column.\ninp0.Autumn.plot.box()","59208cea":"# Find the maximum and 99th percentile of Winter season.\ninp0.Winter.quantile([0.99,1.0])","8a5f0142":"20388.0 - 8985.3","c1abe633":"# Find the maximum and 99th percentile of Summer season.\ninp0.Summer.quantile([0.99,1.0])","4fc25b54":"35577.0 - 13974.0","12a74a68":"# Find the maximum and 99th percentile of Spring season.\ninp0.Spring.quantile([0.99,1.0])","a8970dae":"7374.0 - 2657.2","48867ff4":"# Find the maximum and 99th percentile of Autumn season.\ninp0.Autumn.quantile([0.99,1.0])\n","3d866076":"55532.0 - 24134.0","94f64464":"# Find the Mean of Ratings for each Price category.\ninp0.groupby(\"Price\")['Rating'].mean()","b43205ac":"# Find the median of Ratings for each Style category.\ninp0.groupby(\"Style\")['Rating'].median()","d511ca86":"# Summer sale vs Recommendation.\ninp0[inp0.Recommendation == 1].Summer.mean()","cb721fe1":"# Spring sale vs Recommendation.\ninp0[inp0.Recommendation == 1].Spring.mean()","f3dbd236":"# Autumn sale vs Recommendation.\ninp0[inp0.Recommendation == 1].Autumn.mean()","690847f2":"# Winter sale vs Recommendation.\ninp0[inp0.Recommendation == 1].Winter.mean()","09fdb4f1":"# Size vs Recommendation.\ninp0.groupby(['Size'])['Recommendation'].mean()","e3098cd8":"# plot the heat map of Style, price and Recommendation.\nres = pd.pivot_table(data=inp0, index=\"Style\", columns=\"Price\", values=\"Recommendation\")\nsns.heatmap(res, cmap=\"RdYlGn\", annot=True, center=0.427)\nplt.show()","d04fd60c":"# plot the heat map of Style, price and Recommendation.\nres = pd.pivot_table(data=inp0, index=\"Style\", columns=\"Price\", values=\"Recommendation\")\nsns.heatmap(res, cmap=\"RdYlGn\", annot=True, center=0.427)\nplt.show()","46fa6ac3":"inp0.Recommendation.mean()","04e5b2ff":"inp0[inp0.Price == \"Average\"].Recommendation.mean()","7f7d8de6":"### Numerical variable Univariate analysis:","dd7dafad":"Print the null count of inp0 to get the idea about the missing values in data set.","455c43d0":"Which of the following 'Style' type has the highest recommendation in the 'Average' price segment?\n- Work\n- Vintage\n- Casual\n- Party\n","cf7f5b40":"### Caregorical Ordered Univariate Analysis","af8b2073":"### Fixing the Rows and Columns ","353af465":"## Univariate Analysis","3694c67c":"You should categorise the dates into seasons in \u201cDress Sales\u201d data to simplify the analysis according to the following criteria:\n- June, July and August: Summer.\n- September, October and November: Autumn.\n- December, January and February: WInter.\n- March, April and May: Spring.\n\n\n","41ae5533":"## Exploratory Data Analysis\n\n*Hello fellow Kagglers,* \nThis notebook was a part of my graded questions in one of the courses that I was a part of.\nAlong with the code, the notebook also contains a question stub, which tries to help us understand what we are really looking for, in the data and a greater sense of the context on Dress Sales\n\nThe data contains the following columns\/metrics to describe the Dress Sales -\n- DressID\n- Style \n- Price\n- Rating\n- Size\n- Season\n- Neckline\n- Sleeve Length \n- Material\n- Fabric Type\n- Decoration\n- Pattern Type\n- Recommendation\n\nThe dataset is private, but is same as Dress Sales dataset available on Kaggle. Explore the data in your own ways and create unique observations on the dataset. Happy Kaggling!","c4bbe7a7":"What is the median of the rating of \u201cvintage\u201d category in Style column?\n- 4.6\n- 4.7\n- 4.55\n- 0.00\n","4b01888b":"What is the percentage of \u201ccute\u201d and \u201cOthers\u201d category in \u201cStyle\u201d column in \u201cAttribute DataSet\u201d respectively?\n- 46%, 5%\n- 9%, 2.1%\n- 2.1%, 5%\n- 13.8%, 9%\n","c7063497":"You can see that there are two types of variables one with a large number of missing values and another is very less number of missing values. These two columns can be categorized as:\n\nType-1: Missing values are very less (around 2 or 3 missing values): Price, Season, NeckLine, SleeveLength, Winter and Autumn. \n\nType-2: Missing values are large in numbers (more than 15%): Material, FabricType, Decoration and Pattern Type.\n\n","06aab7ce":"Which of the following categories in \u2018Style\u2019 column can be grouped into \u2018Others\u2019 category? and perform the grouping operation in the notebook for further analysis.\n- Flare, fashion\n- Novelty, bohemian\n- OL, fashion, work\n- Novelty, fashion, Flare\n","283dc7a3":"### Impute\/Remove Missing values","c471af12":"### Standardise value ","21ae0a77":"### Multivariate analysis ","7d89b883":"### Data Reading & Data Types ","1300571d":"Which of the following season has the highest average value of sale for \u201cRecommendation\u201d value equals to 1.\n- Summer\n- Spring\n- Autumn\n- Winter\n","2716090e":"## Bivariate Analysis","93ec9dac":"There is a column named \u2018Style\u2019 in \u2018Attribute Dataset\u2019 which consists of the different style categories of the women apparels. Certain categories whose total sale is less than 50000 across all the seasons is considered under one single category as \u2018Others\u2019.\n","8457beca":"## Data Cleaning ","4d6105d4":"You are given another dataset named \u201cDress Sales\u201d. Now if you observe the datatypes of the columns using \u2018inp1.info()\u2019 command, you can identify that there are certain columns defined as object data type though they primarily consist of numeric data.\n\nNow if you try and convert these object data type columns into numeric data type(float), you will come across an error message. Try to correct this error.\n\n\n\n\n\n","45e91636":"Which of the following pair of \u201cStyle\u201d and \u201cPrice\u201d category has the highest average of positive recommendations?\n- Price: medium and style: vintage\n- Price: medium and style: cute\n- Price: very high and style: party\n- Price: low and style: sexy\n","67a0ebad":"Which of the following column do you think are of no use in \u201cAttribute DataSet\u201d.\n- Dress_ID\n- Price\n- Size and material\n- NeckLine\n- None of the above\n","bafad38a":"### Categorical Unordered Univariate Analysis\n ","1e9f1f35":"There might be some string values in the dataset with type objects, as those strings arent useful we might convert the dataset into float type replacing these strings values with NaN\n\nUsing function -\n\ninp1[cols] = inp1[cols].apply(pd.to_numeric, downcast='float', errors='coerce')\n\nPlease find the reference of this function at https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.to_numeric.html","0acbc956":"Which of the following size categories has the highest positive recommendations?\n- Medium and extra large\n- Extra large and small\n- Free and small\n- Free and medium\n","1e5e6cf5":"What is the approximate difference between the maximum value and 75th percentile in \u201cAutumn\u201d column.\n- Approx 54000\n- Approx 55000\n- Approx 52000\n- Approx 50000\n\n","e35d0c9a":"Which of the following is an unordered variable in \u201cAttribute DataSet\u201d.\n- Style\n- Price\n- Season\n- Size\n","e5dc25bd":"There is another column in \u201cAttribute DataSet\u201d named as \u201cRecommendation\u201d, choose the correct statement about its data type and variable type.\n- Integer type and categorical\n- Object type and categorical\n- Integer type and continuous numerical\n- Object type only.\n","dec553b4":"You have \u201cAttribute DataSet\u201d which contains a column named \u201cPrice\u201d. Choose the correct statement from the following about its data type and variable type.\n- Integer type and numerical variable\n- Object type and categorical ordinal variable\n- Object type and categorical nominal variable\n- Float type and categorical variable.\n","5b0fcd18":"Which of the following \u201cPrice\u201d category has the lowest average value of rating?\n- very-high\n- Medium\n- Low\n- High\n","2f5bb118":"### Numerical- Categorical analysis","4baf9195":"When you see the null counts in \u201cDress Sales\u201d dataset after performing all the operations that have been mentioned in jupyter notebook, you will find that there are some columns in \u201cDress Sales\u201d data where there are more than 40% of missing values. Based on your understanding of dealing with missing values do the following steps.","3ce787d1":"In the given dataset, there are certain discrepancies with the categorical names such as irregular spellings. Choose the correct option of columns with irregular categories and update them.\n \n- Season, NeckLine\n- Price, Material\n- fabricType, Decoration\n- Season, SleeveLength\n","8acf2665":"Now let's merge inp1 with inp0 with left join manner, so that the information of inp0 should remain intact.","02930412":"As you can see, there is a column in \u201cAttribute Dataset\u201d named as \u2018Size\u2019. This column contains the values in abbreviation format. Write a code in Python to convert the followings:\n\n- M into  \u201cMedium\u201d\n- L into  \u201cLarge\u201d\n- XL into \u201cExtra large\u201d\n- free into \u201cFree\u201d\n- S, s & small into \u201cSmall\u201d.\n\nNow once you are done with changes in the dataset, what is the value of the lowest percentage, the highest percentage and the percentage of Small size categories in the column named \u201cSize\u201d?\n","527b6835":"### Categorical categorical bivariate analysis\n","e8c66eac":"Which of the following season has the highest difference between the maximum value and 99th quantile of sales?\n- Winter\n- Summer\n- Spring\n- Autumn\n"}}