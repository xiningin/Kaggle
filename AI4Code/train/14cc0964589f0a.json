{"cell_type":{"3ee75144":"code","0b674e8f":"code","d873b47f":"code","0791dc53":"code","f0014273":"code","c79f8220":"code","f19254ca":"code","173fe006":"code","eb96848b":"code","cf7c4808":"code","5bcfc303":"code","282a6101":"code","60beb84b":"code","237563e3":"code","b7be14ba":"code","479598b7":"code","4080244e":"code","fd203820":"code","855a3845":"code","f98eafca":"code","7d423338":"code","10aec0c8":"code","308c1fc4":"code","b34fab89":"code","81646dca":"code","b3e991de":"code","b7308a94":"code","0f826e8b":"code","3b2d5207":"code","90e3bcea":"code","68acd70c":"code","534bea85":"code","4ace3a3d":"code","a8bd4dd1":"code","4b0fba35":"code","315b5bfa":"code","8b392b00":"code","60892cf5":"code","211e1aea":"code","e371867e":"markdown","9ad96298":"markdown","a179b396":"markdown","4e28664e":"markdown","263699cf":"markdown","74321e64":"markdown","837d007b":"markdown","c140bf8a":"markdown","008838a8":"markdown","4b2ef52a":"markdown","020fed26":"markdown","b7bb4e74":"markdown","834332e2":"markdown","ae32eee5":"markdown","b1a7069c":"markdown","1c064460":"markdown","50688084":"markdown","c189d3d2":"markdown"},"source":{"3ee75144":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import cm\nimport itertools\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0b674e8f":"heart = pd.read_csv('..\/input\/heart.csv')","d873b47f":"heart.head()","0791dc53":"heart.describe()","f0014273":"heart.info()","c79f8220":"heart['age'].value_counts()","f19254ca":"plt.figure(figsize=(15,7), dpi = 100)\nsns.countplot(heart['age'], color='red')","173fe006":"print('Num of Male records : ', heart[heart['sex'] == 1]['sex'].count())\nprint('Num of Male records with target = 0 : ', heart[(heart['sex'] == 1) & (heart['target'] == 0)]['sex'].count())\nprint('Num of Male records with target = 1 : ', heart[(heart['sex'] == 1) & (heart['target'] == 1)]['sex'].count())\nprint('Num of Female records : ', heart[heart['sex'] == 0]['sex'].count())\nprint('Num of Female records with target = 0 : ', heart[(heart['sex'] == 0) & (heart['target'] == 0)]['sex'].count())\nprint('Num of Female records with target = 1 : ', heart[(heart['sex'] == 0) & (heart['target'] == 1)]['sex'].count())","eb96848b":"plt.figure(figsize=(15,7), dpi = 100)\nx = list()\ny_male = list()\ny_female = list()\nfor age in list(heart['age'].unique()):\n    x.append(age)\n    y_female.append(heart[(heart['sex'] == 0) & (heart['age'] == age)]['age'].count())\n    y_male.append(heart[(heart['sex'] == 1) & (heart['age'] == age)]['age'].count())\nplt.bar(x,y_female, color = 'red', alpha = 0.3, label = 'Female Count is 96')\nplt.bar(x,y_male, color = 'blue', alpha = 0.2, label = 'Male Count is 207')\nplt.xticks(x)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.legend()","cf7c4808":"plt.figure(figsize = (10,5), dpi = 100)\nsns.swarmplot(x='sex',y='age',data=heart,hue='target',palette='Set2')","5bcfc303":"plt.figure(figsize = (10,5), dpi = 100)\nsns.swarmplot(x='target',y='age',data=heart,palette='Set2',hue='sex')","282a6101":"x = list()\ny = list()\nprint('Age    Percentage of people with heart disease')\nfor age in sorted(list(heart['age'].unique())):\n    percentage = heart[(heart['age'] == age) & (heart['target'] == 1)]['target'].count()*100\/(heart[heart['age'] == age]['target'].count())\n    x.append(age)\n    y.append(percentage)\n    print(age,\"    \", percentage)\nplt.figure(figsize = (15,5), dpi = 100)\nplt.bar(x,y,color = 'blue', alpha = 0.4)\nplt.xticks(x)\nplt.xlabel('Age')\nplt.ylabel('% of people with heart disease')\n","60beb84b":"fig, axes = plt.subplots(2,1,figsize = (15,10), dpi = 100)\nfor s in list(heart['sex'].unique()):\n    x = list()\n    y = list()\n    if s == 0:\n        str1 = 'Female'\n    elif s == 1:\n        str1 = \"Male\"\n    print('Age    Percentage of {} people with heart disease'.format(str1))\n    for age in sorted(list(heart['age'].unique())):\n        percentage = heart[(heart['age'] == age) & (heart['sex'] == s) & (heart['target'] == 1)]['target'].count()*100\/(heart[(heart['age'] == age) & (heart['sex'] == s)]['target'].count())\n        x.append(age)\n        y.append(percentage)\n        print(age,\"    \", percentage)\n    axes[s].bar(x,y,color = 'green', alpha = 0.5)\n    #axes[s].set_xticklabels(x)\n    axes[s].set_xlabel('Age')\n    axes[s].set_ylabel('% of {} people with heart disease'.format(str1))\n    print(\"\\n\")","237563e3":"plt.figure(figsize=(10,7), dpi = 100)\nsns.heatmap(heart.drop(axis=1,columns='target').corr(), annot=True)","b7be14ba":"sns.pairplot(heart, hue='target')","479598b7":"heart.head()","4080244e":"# Lets look @ chest pain column\nplt.figure(figsize = (10,5), dpi = 100)\nsns.swarmplot(x='cp',y='age',data=heart,palette='Set2',hue='target')","fd203820":"print('Chest pain level     % chance of having a heart disease')\nfor cp in sorted(list(heart['cp'].unique())):\n    print(cp, '                     ',heart[(heart['cp'] == cp) & (heart['target'] == 1)]['target'].count()*100\/(heart[heart['cp'] == cp]['target'].count()))","855a3845":"# Lets look @ number of major vessels (ca) pain column\nplt.figure(figsize = (10,5), dpi = 100)\nsns.swarmplot(x='ca',y='age',data=heart,palette='Set2',hue='target')","f98eafca":"print('number of major vessels (ca) pain     % chance of having a heart disease')\nfor ca in sorted(list(heart['ca'].unique())):\n    print(ca, '                                     ',heart[(heart['ca'] == ca) & (heart['target'] == 1)]['target'].count()*100\/(heart[heart['ca'] == ca]['target'].count()))","7d423338":"# Lets look @ exercise induced angina column\nplt.figure(figsize = (10,5), dpi = 100)\nsns.swarmplot(x='exang',y='age',data=heart,palette='Set2',hue='target')","10aec0c8":"print('exercise induced angina      % chance of having a heart disease')\nfor ex in sorted(list(heart['exang'].unique())):\n    print(ex, '                            ',heart[(heart['exang'] == ex) & (heart['target'] == 1)]['target'].count()*100\/(heart[heart['exang'] == ex]['target'].count()))","308c1fc4":"# Lets look @ number of major vessels (ca) pain column\nplt.figure(figsize = (10,5), dpi = 100)\nsns.swarmplot(x='thal',y='age',data=heart,palette='Set2',hue='target')","b34fab89":"#-------------Machine learning-----------------\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix,mean_absolute_error,mean_squared_error \n#-- Models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV","81646dca":"#Scaling the dataframe\nscaler = MinMaxScaler()\nscaler.fit(heart.drop(['target'], axis = 1))\nheart_scaler =scaler.transform(heart.drop(['target'], axis = 1))","b3e991de":"#Train-test-split\nX = heart_scaler\ny = heart['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 101)","b7308a94":"#DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test,dtree_pred))\nprint(classification_report(y_test,dtree_pred))","0f826e8b":"from IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nfrom sklearn.tree import export_graphviz\nimport pydot \n\nfeatures = list(heart.columns[:-1])\nfeatures\ndot_data = StringIO()  \nexport_graphviz(dtree, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n\ngraph = pydot.graph_from_dot_data(dot_data.getvalue())  \nImage(graph[0].create_png())  ","3b2d5207":"#RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test,rfc_pred))\nprint(classification_report(y_test,rfc_pred))","90e3bcea":"#LogisticRegression\nlog = LogisticRegression()\nlog.fit(X_train,y_train)\nlog_pred = log.predict(X_test)\nprint(confusion_matrix(y_test,log_pred))\nprint(classification_report(y_test,log_pred))","68acd70c":"#KMeans\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X_test)\nkmeans.cluster_centers_\nkmeans.labels_","534bea85":"#KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)\nprint(confusion_matrix(y_test,knn_pred))\nprint(classification_report(y_test,knn_pred))\n\nerror_rate = []\n# Will take some time\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","4ace3a3d":"#Retrain with chosen K value\nknn = KNeighborsClassifier(n_neighbors=24)\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)\nprint(confusion_matrix(y_test,knn_pred))\nprint(classification_report(y_test,knn_pred))","a8bd4dd1":"#SVC\nsvc = SVC()\nsvc.fit(X_train,y_train)\nsvc_pred = svc.predict(X_test)\nprint(confusion_matrix(y_test,svc_pred))\nprint(classification_report(y_test,svc_pred))","4b0fba35":"#Grid Search\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\nprint(grid.best_estimator_)\ngrid_pred = grid.predict(X_test)\nprint(confusion_matrix(y_test,grid_pred))\nprint(classification_report(y_test,grid_pred))","315b5bfa":"#Comparison of Regression models\nfig,axes = plt.subplots(1,3,figsize=(17,5))\nx = ['DTree','RFC','LogReg','KnnCl','SVC','Grid']\n\n#Mean Absolute Error\naxes[0].set_title(\"Mean Absolute Error\")\ny_mae = np.array([mean_absolute_error(y_test,dtree_pred),mean_absolute_error(y_test,rfc_pred),mean_absolute_error(y_test,log_pred),mean_absolute_error(y_test,knn_pred),mean_absolute_error(y_test,svc_pred),mean_absolute_error(y_test,grid_pred)])\naxes[0].bar(x,y_mae)\n\n#Mean Squared Error\naxes[1].set_title(\"Mean Squared Error\")\ny_mse = np.array([mean_squared_error(y_test,dtree_pred),mean_squared_error(y_test,rfc_pred),mean_squared_error(y_test,log_pred),mean_squared_error(y_test,knn_pred),mean_squared_error(y_test,svc_pred),mean_squared_error(y_test,grid_pred)])\naxes[1].bar(x,y_mse)\n\n#Root Mean Squared Error\naxes[2].set_title(\"Root Mean Squared Error\")\ny_rmse = np.array([np.sqrt(mean_squared_error(y_test,dtree_pred)),np.sqrt(mean_squared_error(y_test,rfc_pred)),np.sqrt(mean_squared_error(y_test,log_pred)),np.sqrt(mean_squared_error(y_test,knn_pred)),np.sqrt(mean_squared_error(y_test,svc_pred)),np.sqrt(mean_squared_error(y_test,grid_pred))])\naxes[2].bar(x,y_rmse)","8b392b00":"fig,axes = plt.subplots(2,3,figsize=(17,10))\nmodel = ['DTree','RFC','LogReg','KnnCl','SVC','Grid']\ni = 0\nfor pred in model:\n    if pred == 'DTree':\n        cm = confusion_matrix(y_test,dtree_pred)\n    elif pred == 'RFC':\n        cm = confusion_matrix(y_test,rfc_pred)\n    elif pred == 'LogReg':\n        cm = confusion_matrix(y_test,log_pred)\n    elif pred == 'KnnCl':\n        cm = confusion_matrix(y_test,knn_pred)\n    elif pred == 'SVC':\n        cm = confusion_matrix(y_test,svc_pred)\n    elif pred == 'Grid':\n        cm = confusion_matrix(y_test,grid_pred)\n    c_mtx = axes.flatten()[i].matshow(cm)\n    axes.flatten()[i].set_title(pred)\n    axes.flatten()[i].set_ylabel('True Target')\n    axes.flatten()[i].set_xlabel('Predicted Target')\n    thresh = cm.max() \/ 2\n    normalize = 0\n    print(range(cm.shape[0]), range(cm.shape[1]))\n    for k, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        axes.flatten()[i].text(j, k, \"{:,}\".format(cm[k, j]),\n                               horizontalalignment=\"center\",\n                               size = 30,\n                               color=\"black\" if cm[k, j] > thresh else \"red\")\n    i = i+1\n#ColorBar\nfig.subplots_adjust(right=0.8)\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(c_mtx, cax=cbar_ax)","60892cf5":"def plot_classification_report(cr,axes, title='Classification report ', with_avg_total=False, cmap=plt.cm.Blues):\n\n    lines = cr.split('\\n')\n    classes = []\n    plotMat = []\n\n    for line in lines[2 : 4]:\n        #print(line)\n        t = line.split()\n        #print(t[0])\n        classes.append(t[0])\n        v = [float(x) for x in t[1: len(t) - 1]]\n        #print(v)\n        plotMat.append(v)\n    \n    axes.imshow(plotMat, interpolation='nearest', cmap=cmap)\n    axes.set_title(title)\n    #axes.colorbar()\n    x_tick_marks = np.arange(3)\n    #print(x_tick_marks)\n    y_tick_marks = np.arange(len(classes))\n    axes.set_xticklabels(['','precision','', 'recall','', 'f1-score',''])\n    axes.set_yticklabels(['','',0,'','','',1,'',''])\n    #axes.set_yticklabels(classes)\n    #axes.tight_layout()\n    axes.set_ylabel('Classes')\n    axes.set_xlabel('Measures')\n    for k, j in itertools.product(range(0,2), range(0,3)):\n        axes.text(j, k, \"{:,}\".format(plotMat[k][j]),\n                               horizontalalignment=\"center\",\n                               size = 20,\n                               color=\"red\")","211e1aea":"fig,axes = plt.subplots(2,3,figsize=(17,10))\nmodel = ['DTree','RFC','LogReg','KnnCl','SVC','Grid']\ni = 0\nfor pred in model:\n    if pred == 'DTree':\n        plot_classification_report(classification_report(y_test,dtree_pred),axes = axes.flatten()[i],title = pred)\n        \n    elif pred == 'RFC':\n        plot_classification_report(classification_report(y_test,rfc_pred),axes = axes.flatten()[i],title = pred)\n        \n    elif pred == 'LogReg':\n        plot_classification_report(classification_report(y_test,log_pred),axes = axes.flatten()[i],title = pred)\n        \n    elif pred == 'KnnCl':\n        plot_classification_report(classification_report(y_test,knn_pred),axes = axes.flatten()[i],title = pred)\n        \n    elif pred == 'SVC':\n        plot_classification_report(classification_report(y_test,svc_pred),axes = axes.flatten()[i],title = pred)\n        \n    elif pred == 'Grid':\n        plot_classification_report(classification_report(y_test,grid_pred),axes = axes.flatten()[i],title = pred)\n        \n    i = i+1","e371867e":"### Lets do the same splitup based on the sex","9ad96298":"Lets compare age with the target column","a179b396":"### Lets denote sex = 0 as female, sex = 1 as male","4e28664e":"### Takeaway: Majority of People with chest pain > 1 seem to have a heart disease. Chest pain column seems to be fairly correlated with target column.","263699cf":"Now, lets see what % of the records is male and female and cross it with the target column","74321e64":"## Takeaway : K value of 7, 24 seem to have the least error rate. Lets retrain our model with k = 24","837d007b":"## Confusion Matrices for different models","c140bf8a":"### Lets plot the % of chance of getting heart disease by age","008838a8":"# Takeaway: Grid Search modeling seems to have the best results.","4b2ef52a":"## Next Steps: Feature Engineering. Lets see if by removing certain columns, our models perform better!!!","020fed26":"## Takeaway:","b7bb4e74":"## Lets look at Age","834332e2":"# DATA ANALYSIS","ae32eee5":"# MODELING ","b1a7069c":"### Takeaway: Based on the dataset, there are few outliers (Like age 29, 34 - total count of these records is < 3, which can be removed)","1c064460":"Attribute Information: \n> 1. age \n> 2. sex \n> 3. chest pain type (4 values) \n> 4. resting blood pressure \n> 5. serum cholestoral in mg\/dl \n> 6. fasting blood sugar > 120 mg\/dl\n> 7. resting electrocardiographic results (values 0,1,2)\n> 8. maximum heart rate achieved \n> 9. exercise induced angina \n> 10. oldpeak = ST depression induced by exercise relative to rest \n> 11. the slope of the peak exercise ST segment \n> 12. number of major vessels (0-3) colored by flourosopy \n> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","50688084":"## Takeaway: Pretty much all columns are poorly correlated with one another. The best pair being slope and thalach","c189d3d2":"### Takeaway: Majority of People who exercise regularly are bound to have less heart disease"}}