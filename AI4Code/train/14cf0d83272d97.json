{"cell_type":{"19bbcd10":"code","28f18c58":"code","c2ce9ca5":"code","f052dfce":"code","5add0d04":"code","a1cd63c4":"code","a4bb2a82":"code","104f9a8c":"code","901cc50e":"code","74618e4e":"code","537b4787":"code","da624d95":"code","1557425c":"code","ed54c5f5":"code","884a48ce":"code","d95840b5":"code","4f548631":"code","5905f280":"code","99d1c4cb":"code","c0795245":"code","1010155e":"code","f1d046af":"code","f48428f7":"markdown"},"source":{"19bbcd10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly.express as px \nfrom warnings import filterwarnings as filt \n\nfilt('ignore')\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12, 8)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    if len(filenames) > 0:\n        print(dirname)\n#     for filename in filenames:\n        \n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28f18c58":"base_path = '\/kaggle\/input\/basicshapes\/shapes\/'\ndef get_filenames(shape):\n    path = os.path.join(base_path, shape)\n    return [os.path.join(path, img) for img in os.listdir(path)]","c2ce9ca5":"c = pd.DataFrame({\n    'img_path' : get_filenames('circles'),\n    'ishape'    : 'circle'\n})\n\ns = pd.DataFrame({\n    'img_path' : get_filenames('squares'),\n    'ishape'    : 'square'\n})\n\nt = pd.DataFrame({\n    'img_path' : get_filenames('triangles'),\n    'ishape'    : 'triangle'\n})\n\ndf = pd.concat([c, s, t]).reset_index(drop = True)\ndf.head()","f052dfce":"shape_count = df.ishape.value_counts()\npx.bar(x = shape_count.index, y = shape_count, color = shape_count.index, title = 'shape counts')","5add0d04":"def plot_img(df, num_img = 3):\n    df = df.sample(frac = 1)\n    f3 = df.head(3)\n    for row in f3.iterrows():\n        img = row[1]['img_path']\n        cls = row[1]['ishape']\n        plt.figure(row[0])\n        img = plt.imread(img)\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title(f'{cls}  -  {img.shape}')","a1cd63c4":"plot_img(df)","a4bb2a82":"import tensorflow as tf \nimport tensorflow.keras as keras \nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.utils import plot_model, to_categorical\nfrom tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Flatten\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow_addons as tfa\n\n\nloss = keras.losses\nmetrics = keras.metrics\n\nclass Network:\n    def __init__(self, layers = [], activations = [], dropout = [], batchnorm = [], compile_params = {}, flatten = False):\n        self.model = None\n        self.losses = None \n        self.metrics = None\n        self.layers = layers \n        self.activations = activations \n        self.compile_params = compile_params \n        self.dropout = dropout if len(dropout) == (len(activations) - 1) else [None for _ in range(len(activations) - 1)]\n        self.batchnorm = batchnorm if len(batchnorm) == (len(activations) - 1) else [None for _ in range(len(activations) - 1)]\n        self.flatten = flatten\n        self.initialize_model()\n    \n    def initialize_model(self):\n        \n        self.model = Sequential()\n        \n        # input layer         \n        self.model.add(Input(shape = self.layers[0], name = 'Input_Layer'))\n        \n        if self.flatten:\n            self.model.add(Flatten())\n            \n        # hidden layers         \n        for idx in range(len(self.layers) - 2):\n            units = self.layers[idx + 1]\n            activation = self.activations[idx]\n            dp = self.dropout[idx]\n            bn = self.batchnorm[idx]\n            self.model.add(Dense(units, activation = activation, name = f'Hidden_Layer_{idx + 1}'))\n            if bn:\n                self.model.add(BatchNormalization())\n            if dp:\n                self.model.add(Dropout(dp, name = f'Dropout_{idx + 1}_{dp}'))\n                \n        # output layer\n        self.model.add(Dense(self.layers[-1], activation = self.activations[-1], name = 'Output_Layer'))\n                \n        self.model.compile(**self.compile_params)\n        return self.model\n    \n    def fit(self, fit_params):\n        history = self.model.fit(**fit_params)\n        return self.store_history(history)\n    \n    def store_history(self, history):\n        his = pd.DataFrame(history.history)\n        l = [c for c in his.columns if 'loss' in c]\n        m = [c for c in his.columns if 'loss' not in c]\n        self.losses = his[l]\n        self.metrics = his[m]\n        return his\n    \n    def fit_generator(self, fit_params):\n        history = self.model.fit_generator(**fit_params)\n        return self.store_history(history)\n          \n    def predict(self, x, softmax = 0):\n        pred = self.model.predict(x)\n        if softmax == 0:\n            return np.argmax(pred, axis = 1)\n        return pred\n    \n    def predict_generator(self, xgen, softmax = 0):\n        pred = self.model.predict_generator(xgen)\n        if softmax == 0:\n            return np.argmax(pred, axis = 1)\n        return pred\n    \n    def plot_arch(self):\n        if self.model is not None:\n            return plot_model(self.model, show_shapes = True, show_layer_names = True)\n        \n    def plot_loss(self):\n        if self.losses is not None:\n            self.losses.plot(kind = 'line')\n            plt.title('loss comparison')\n            plt.legend(self.losses.columns)\n            \n    def plot_metrics(self):\n        if self.metrics is not None:\n            self.metrics.plot(kind = 'line')\n            plt.title('metrics comparison')\n            plt.legend(self.metrics.columns)\n\n        \ndef report(yt, pred, inverse_to_cat = True):\n    if inverse_to_cat:\n        yt = np.argmax(yt, axis = 1)\n    print(classification_report(yt, pred))\n    sns.heatmap(confusion_matrix(yt, pred), fmt = '.1f', annot = True)\n    plt.title('confusion matrix')\n    \ndef hardmax(y):\n    return np.argmax(y, axis = 1)\n\ndef sample(x, y, frac, return_val = False):\n    x, xt, y, yt = train_test_split(x, y, test_size = frac, stratify = y)\n    if return_val:\n        return x, xt, y, yt\n    return x, y\n\ndef plot(history, kind):\n    his = pd.DataFrame(history.history)\n    l = [c for c in his.columns if 'loss' in c]\n    m = [c for c in his.columns if 'loss' not in c]\n    losses = his[l]\n    metrics = his[m]\n    if losses is not None and kind == 'loss':\n        losses.plot(kind = 'line')\n        plt.title('loss comparison')\n        plt.legend(losses.columns)\n    if metrics is not None and kind == 'metrics':\n        metrics.plot(kind = 'line')\n        plt.title('metrics comparison')\n        plt.legend(metrics.columns)","104f9a8c":"cls_encoder = {\n    'circle'   : 0,\n    'square'   : 1,\n    'triangle' : 2\n}\ncls_inverse_encoder = {\n    0 : 'circle',\n    1 : 'square',\n    2 : 'triangle',\n}\n\ndf['ishape'] = df.ishape.replace(cls_encoder)\n\ndf.head()","901cc50e":"df.ishape.unique()","74618e4e":"x = df.drop(['ishape'], axis = 1)\ny = df.ishape\n\nx_train, x_dev, y_train, y_dev = sample(x, y, 0.3, True)\n\ny_train.shape, y_dev.shape","537b4787":"def join_dfs(x, y, cname, ohe = 0):\n    if ohe > 0 :\n        y = hardmax(y)\n    y = pd.Series(y, index = x.index, name = cname)\n    return pd.concat([x, y], axis = 1)\n\ndef create_data_generators(df, batch_size = 16, datagen_args = {\n     'rotation_range'     : 20,\n     'width_shift_range'  : 0.2,\n     'height_shift_range' : 0.2,\n     'rescale'            : 1.\/255,\n     'vertical_flip'      : True,\n     'horizontal_flip'    : True\n    }, shuffle = True):\n    \n    datagen = ImageDataGenerator(**datagen_args)\n    datagenerator = datagen.flow_from_dataframe(\n                                            df,\n                                            base_path,\n                                            x_col = 'img_path',\n                                            y_col = 'ishape',\n                                            target_size = (28,28),\n                                            class_mode='categorical',\n                                            batch_size = batch_size, \n                                            shuffle= shuffle\n                                            )\n    return datagenerator","da624d95":"train_df = join_dfs(x_train, y_train, 'ishape', 0)\ntrain_df['ishape'] = train_df.ishape.replace(cls_inverse_encoder)\n\ndev_df = join_dfs(x_dev, y_dev, 'ishape', 0)\ndev_df['ishape'] = dev_df.ishape.replace(cls_inverse_encoder)\n\ndatagen_args = {\n     'rescale'            : 1.\/255,\n     'zoom_range'         : 0.1, \n     'vertical_flip'      : True,\n     'horizontal_flip'    : True\n    }\nx_train_gen = create_data_generators(train_df, datagen_args = datagen_args)\nx_dev_gen = create_data_generators(dev_df, datagen_args = {'rescale': 1.\/255}, shuffle = False)","1557425c":"class myCb(tf.keras.callbacks.Callback):\n    def __init__(self, times, thresh):\n        self.times = times\n        self.thresh = thresh\n        self.reached = 0\n        \n    def on_epoch_end(self, epoch, logs={}):\n        if logs.get(\"val_accuracy\") >= self.thresh:\n            self.reached += 1\n            print(f'Reached >= 95% {self.reached} \/ {self.times}')\n            if self.reached >= self.times:\n                print(\"Reached 95% accuracy so cancelling training!\")\n                self.model.stop_training = True","ed54c5f5":"layers = [(28, 28, 3), 128, 64, 3]\nactivations = ['relu', 'relu', 'softmax']\ndropout = []\nbatchNorm = []\n\ncompile_params = {\n    'loss'      : 'categorical_crossentropy',\n    'optimizer' : keras.optimizers.RMSprop(learning_rate = 0.01),\n    'metrics'   : ['accuracy']\n}\n\nmodel1 = Network(layers, activations, dropout = dropout, batchnorm = batchNorm, flatten = True, compile_params = compile_params) \nmodel1.plot_arch()","884a48ce":"callbacks = myCb(1, 0.95)\nfit_params = {\n    'generator' : x_train_gen,\n    'validation_data' : x_dev_gen,\n    'epochs' : 10,\n    'callbacks': [callbacks]\n}\n\nhis = model1.fit_generator(fit_params)\nmodel1.plot_loss()","d95840b5":"model1.plot_metrics()","4f548631":"from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, ReLU, BatchNormalization\ndef Cnn():\n    ## input layer\n    ip = Input(shape = (28, 28, 3))\n    \n    ## convolution layer 1     \n    c1 = Conv2D(filters = 32, kernel_size = (3,3), strides = 1, padding = 'same')(ip)\n    a1 = ReLU()(c1)\n#     bn = BatchNormalization()(a1)\n    mp1 = MaxPool2D(pool_size = (2,2), strides = (2,2))(a1)\n    \n    ## convolution layer 2     \n    c2 = Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'same')(mp1)\n    a2 = ReLU()(c2)\n#     bn = BatchNormalization()(a2)\n    mp2 = MaxPool2D(pool_size = (2,2), strides = (2,2))(a2)\n    \n    ## convolution layer 3\n    c3 = Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'same')(mp2)\n    a3 = ReLU()(c3)\n#     bn = BatchNormalization()(a2)\n    mp3 = MaxPool2D(pool_size = (2,2), strides = (2,2))(a3)\n    \n    ## flatten\n    F = Flatten()(mp3)\n    \n    ## dropout\n    dp = Dropout(0.25)(F)\n    \n    ## hidden dense layer 3\n    h3 = Dense(256, activation = 'relu')(dp)\n    \n    ## dropout\n    dp = Dropout(0.2)(h3)\n    \n    ## output dense layer 4\n    op = Dense(3, activation = 'softmax')(dp)\n    \n    model = keras.Model(inputs = ip, outputs = op)\n    return  model","5905f280":"model2 = Cnn()\nmodel2.summary()","99d1c4cb":"model2.compile(\n    loss = 'categorical_crossentropy',\n    optimizer = keras.optimizers.RMSprop(learning_rate = 0.0005), \n    metrics = ['accuracy']\n)","c0795245":"callbacks = myCb(3, 0.95)\nfit_params = {\n    'generator' : x_train_gen,\n    'validation_data' : x_dev_gen,\n    'epochs' : 100,\n    'callbacks': [callbacks]\n}\nhis = model2.fit_generator(**fit_params)\nplot(his, 'loss')","1010155e":"plot(his, 'metrics')","f1d046af":"pred = hardmax(model2.predict_generator(x_dev_gen))\nyt = x_dev_gen.labels\nreport(yt, pred, False)","f48428f7":"i tried many different optimizers, different learning rate and always get the same validation accurary, this shows that this vanilla model is not so good to classify the shapes with just small amount of dataset"}}