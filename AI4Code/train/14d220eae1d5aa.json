{"cell_type":{"c363c606":"code","efd85620":"code","f23d9c25":"code","e9bfa515":"code","b578f341":"code","42b29aac":"code","c98ee926":"code","9a0a4ad6":"code","28b7c2c8":"code","aae888a8":"code","07f93a01":"code","d757c888":"code","1a9f2fe9":"code","3fa1bb02":"code","ef87f045":"code","74705db6":"code","d72f666d":"code","236d8400":"code","0af3b9a2":"code","a09fd547":"code","90535500":"code","18eab711":"code","94f8b2a6":"code","c3ee09fa":"markdown"},"source":{"c363c606":"%%writefile unet3d.py\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n# Ref https:\/\/github.com\/Thvnvtos\/Lung_Segmentation\n\n# __                            __\n#  1|__   ________________   __|1\n#     2|__  ____________  __|2\n#        3|__  ______  __|3\n#           4|__ __ __|4\n\nclass ConvUnit(nn.Module):\n    \"\"\"\n        Convolution Unit: (Conv3D -> BatchNorm -> ReLu) * 2\n    \"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size = 3, padding = 1),\n            nn.BatchNorm3d(out_channels),\n            nn.ReLU(inplace=True), # inplace=True means it changes the input directly, input is lost\n\n            nn.Conv3d(out_channels, out_channels, kernel_size = 3, padding = 1),\n            nn.BatchNorm3d(out_channels),\n            nn.ReLU(inplace=True)\n          )\n\n    def forward(self,x):\n        return self.double_conv(x)\n\nclass EncoderUnit(nn.Module):\n    \"\"\"\n    An Encoder Unit with the ConvUnit and MaxPool\n    \"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.MaxPool3d(2),\n            ConvUnit(in_channels, out_channels)\n        )\n    def forward(self, x):\n        return self.encoder(x)\n\nclass DecoderUnit(nn.Module):\n    \"\"\"\n    ConvUnit and upsample with Upsample or convTranspose\n    \"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.up = nn.ConvTranspose3d(in_channels \/\/ 2, in_channels \/\/ 2, kernel_size=2, stride=2)\n        self.conv = ConvUnit(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n\n        diffZ = x2.size()[2] - x1.size()[2]\n        diffY = x2.size()[3] - x1.size()[3]\n        diffX = x2.size()[4] - x1.size()[4]\n        x1 = F.pad(x1, [diffX \/\/ 2, diffX - diffX \/\/ 2, diffY \/\/ 2, diffY - diffY \/\/ 2, diffZ \/\/ 2, diffZ - diffZ \/\/ 2])\n\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size = 1)\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass UNet3d(nn.Module):\n    def __init__(self, in_channels, n_classes, s_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.n_classes = n_classes\n        self.s_channels = s_channels\n\n        self.conv = ConvUnit(in_channels, s_channels)\n        self.enc1 = EncoderUnit(s_channels, 2 * s_channels)\n        self.enc2 = EncoderUnit(2 * s_channels, 4 * s_channels)\n        self.enc3 = EncoderUnit(4 * s_channels, 8 * s_channels)\n        self.enc4 = EncoderUnit(8 * s_channels, 8 * s_channels)\n\n        self.dec1 = DecoderUnit(16 * s_channels, 4 * s_channels)\n        self.dec2 = DecoderUnit(8 * s_channels, 2 * s_channels)\n        self.dec3 = DecoderUnit(4 * s_channels, s_channels)\n        self.dec4 = DecoderUnit(2 * s_channels, s_channels)\n        self.out = OutConv(s_channels, n_classes)\n\n    def forward(self, x):\n        x1 = self.conv(x)\n        x2 = self.enc1(x1)\n        x3 = self.enc2(x2)\n        x4 = self.enc3(x3)\n        x5 = self.enc4(x4)\n\n        mask = self.dec1(x5, x4)\n        mask = self.dec2(mask, x3)\n        mask = self.dec3(mask, x2)\n        mask = self.dec4(mask, x1)\n        mask = self.out(mask)\n        return mask, x5","efd85620":"import os\nimport cv2\nimport torch\nimport warnings\nimport ipywidgets\nimport numpy as np \nimport pandas as pd\nfrom torch import nn\nimport nibabel as nib\nfrom glob import glob\nfrom unet3d import UNet3d\nfrom IPython import display\nimport matplotlib.pyplot as plt\nfrom skimage.util import montage\nfrom tqdm.autonotebook import tqdm\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom sklearn.model_selection import StratifiedShuffleSplit","f23d9c25":"class Visulizer:\n    def montage_nd(self, image):\n        if len(image.shape)>3:\n            return montage(np.stack([self.montage_nd(img) for img in image],0))\n        elif len(image.shape)==3:\n            return montage(image)\n        else:\n            warn('Input less than 3d image, returning original', RuntimeWarning)\n            return image\n\n    def visualize(self, image, mask):\n        fig, axs = plt.subplots(1, 2, figsize = (20, 15 * 2))\n        axs[0].imshow(self.montage_nd(image[..., 0]), cmap = 'bone')\n        axs[1].imshow(self.montage_nd(mask[..., 0]), cmap = 'bone')\n        plt.show()","e9bfa515":"viz = Visulizer()","b578f341":"class TrainDataset(Dataset):\n    def __init__(self, BASE_PATH, num_slices = 64):\n        self.num_slices = num_slices\n        self.images = self.read_data(glob(os.path.join(BASE_PATH, \"3d_images\", \"IMG_*\")))\n        self.masks = self.read_data(glob(os.path.join(BASE_PATH, \"3d_images\",\"MASK_*\")), False)\n        assert len(self.images) == len(self.masks)\n\n    def read_data(self, paths, rescale = True, DS_FACT = 8):\n        data = np.concatenate([nib.load(path).get_fdata()[:, ::DS_FACT, ::DS_FACT] for path in sorted(paths)], 0)\n        if rescale: data = (data - data.min())\/(data.max()-data.min()) * 255\n        return np.expand_dims(data, -1).astype('float32') \/ 255\n\n    def __len__(self):\n        return len(self.images)-self.num_slices\n\n    def __getitem__(self, idx):\n        image = self.images[idx: idx + self.num_slices]\n        mask = self.masks[idx: idx + self.num_slices]\n        return image, mask","42b29aac":"train_dataset = TrainDataset(os.path.join('\/','kaggle','input', 'finding-lungs-in-ct-data'))","c98ee926":"idx = np.random.choice(len(train_dataset))\nimage, mask = train_dataset[idx]\nviz.visualize(image, mask)","9a0a4ad6":"class AugDataLoader(DataLoader):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.aug_data_gen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.15,\n            height_shift_range=0.15,\n            shear_range=0.1,\n            zoom_range=0.25,\n            fill_mode='nearest',\n            horizontal_flip=True,\n            vertical_flip=False\n        )\n\n    def aug_data(self, x, y):\n        xy = torch.cat([x, y], dim = 1).squeeze(dim = -1).permute(0, 2, 3, 1)\n        img_gen = self.aug_data_gen.flow(xy, shuffle=True, seed=42, batch_size = len(x))\n        # unblock\n        xy_scat = torch.tensor(next(img_gen)).permute(0, 3, 1, 2).unsqueeze(dim = 1)\n        return xy_scat[:, :, :xy_scat.shape[2]\/\/2], xy_scat[:, :, xy_scat.shape[2]\/\/2:]\n\n    def __iter__(self):\n        for data in super().__iter__():\n            data = self.aug_data(*data)\n            yield data","28b7c2c8":"training = 0\nnum_epoch = 10\nbatch_size = 16\nnum_folds = 1\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nwarnings.filterwarnings(\"ignore\")","aae888a8":"checkpoint_path = os.path.join('\/','kaggle','input', 'lung-segmentation-pytorch-unet3d', 'checkpoint.pth')\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location = device)\n    torch.save(checkpoint, \"checkpoint.pth\")\nelse:\n    checkpoint = {}","07f93a01":"train_loaders = {}\nvalid_loaders = {}\ntrain_folds = checkpoint.get('train_folds',{})\nvalid_folds = checkpoint.get('valid_folds',{})\nsss = StratifiedShuffleSplit(n_splits = num_folds, test_size = 0.2, random_state = 42)\nsplitter = sss.split(train_dataset.images[:len(train_dataset)], train_dataset.masks[:len(train_dataset)].sum(1).sum(1).sum(1).astype('int')%64)\nfor fold, (train_indices, valid_indices) in enumerate(splitter):\n    train_folds[fold] = train_folds.get(fold, train_indices)\n    valid_folds[fold] = valid_folds.get(fold, valid_indices)\n    # Creating PT data samplers and loaders\n    train_sampler = SubsetRandomSampler(train_folds[fold])\n    valid_sampler = SubsetRandomSampler(valid_folds[fold])\n    train_loaders[fold] = AugDataLoader(train_dataset, batch_size = batch_size, sampler = train_sampler)\n    valid_loaders[fold] = AugDataLoader(train_dataset, batch_size = batch_size, sampler = valid_sampler)","d757c888":"fold = np.random.choice(list(train_loaders))\nimages, masks = next(iter(train_loaders[fold]))\nviz.visualize(images.permute(0, 2, 3, 4, 1), masks.permute(0, 2, 3, 4, 1))","1a9f2fe9":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=0, logits=True, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduce = reduce\n        self.loss_fn = (nn.BCEWithLogitsLoss if logits else nn.BCELoss)(reduction = 'none')\n\n    def forward(self, pred, target):\n        BCE_loss = self.loss_fn(pred, target)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n        return F_loss.mean() if self.reduce else F_loss","3fa1bb02":"class DiceScore(nn.Module):\n    def __init__(self, smooth = 1e-6):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, pred, target):\n        pred = torch.sigmoid(pred)\n        batch_size = target.size(0)\n        pred = pred.view(batch_size,-1)\n        target = target.view(batch_size,-1)\n        intersection = (pred * target)\n        score = (2. * intersection.sum(1) + self.smooth) \/ (pred.sum(1) + target.sum(1) + self.smooth)\n        return score.mean()","ef87f045":"models = {}\noptimizers = {}\nschedulers = {}\n\nmetric = DiceScore()\ncriterion = FocalLoss()\n\nfor fold in tqdm(range(num_folds)):\n    models[fold] = UNet3d(in_channels = 1, n_classes = 1, s_channels = 32).to(device)\n    models[fold].load_state_dict(checkpoint.get(\"models\", {}).get(fold, models[fold].state_dict()))\n    # Prepare optimizer and schedule (linear warmup and decay)    \n    params = [p for n, p in models[fold].named_parameters() if p.requires_grad]\n    optimizers[fold] = torch.optim.Adam(params, lr=1e-3)\n    optimizers[fold].load_state_dict(checkpoint.get(\"optimizers\", {}).get(fold, optimizers[fold].state_dict()))\n    schedulers[fold] = torch.optim.lr_scheduler.StepLR(optimizers[fold], step_size=90, gamma=0.1, last_epoch=-1)\n    schedulers[fold].load_state_dict(checkpoint.get(\"schedulers\", {}).get(fold, schedulers[fold].state_dict()))","74705db6":"def valid(fold):\n    total_loss = 0\n    total_score = 0\n    models[fold].eval()\n    loader = tqdm(valid_loaders[fold], desc = f\"Validating fold {fold+1}\")\n    for idx, (images, targets) in enumerate(loader, start=1):\n        images = images.to(device)\n        targets = targets.to(device)\n        # Execute\n        with torch.no_grad():\n            outputs, _ = models[fold](images)\n        loss = criterion(outputs, targets).item()\n        score = metric(outputs, targets).item()\n        loss += 1 - score\n        total_loss += loss\n        total_score += score\n        # print statistics\n        loader.set_postfix_str(f\"Score: {score:.4f} | Loss: {loss:.4f}\")\n        loader.update()\n        # Clear variable\n        del images; targets; del outputs; del loss; del score\n    loader.write(f\"Validated fold {fold+1} | Score: {total_score\/idx:.4f} | Loss: {total_loss\/idx:.4f}\")\n    return total_score\/idx, total_loss\/idx","d72f666d":"def train(fold):\n    total_loss = 0\n    total_score = 0\n    models[fold].train()\n    loader = tqdm(train_loaders[fold], desc = f\"Training fold {fold+1}\")\n    for idx, (images, targets) in enumerate(loader, start=1):\n        images = images.to(device)\n        targets = targets.to(device)\n        # Execute\n        outputs, _ = models[fold](images)\n        loss = criterion(outputs, targets)\n        score = metric(outputs, targets)\n        loss += 1 - score\n        total_loss += loss.item()\n        total_score += score.item()\n        # Optimize + Backward\n        optimizers[fold].zero_grad()\n        loss.backward()\n        optimizers[fold].step()\n        # print statistics\n        loader.set_postfix_str(f\"Score: {score:.4f} | Loss: {loss:.4f}\")\n        loader.update()\n        # Clear variable\n        del images; targets; del outputs; del loss; del score\n    loader.write(f\"Trained fold {fold+1} | Score: {total_score\/idx:.4f} | Loss: {total_loss\/idx:.4f}\")\n    valid_score, valid_loss = valid(fold)\n    schedulers[fold].step()\n    return total_score\/idx, total_loss\/idx, valid_score, valid_loss","236d8400":"train_data = checkpoint.get('train_data', {})\nvalid_data = checkpoint.get('valid_data', {})\nepoch_data = checkpoint.get('epoch_data', [])\ndel checkpoint\nfig, axs = plt.subplots(num_folds, 2, figsize=(10*2, 5*num_folds))\nfor fold in range(num_folds):\n    if fold in valid_data and fold in train_data:\n        # Visualize\n        ax = axs[fold] if num_folds > 1 else axs\n        ax[0].clear(); ax[1].clear()\n        ax[0].plot(epoch_data, train_data[fold][:, 0], label = f\"Train fold {fold+1} score {train_data[fold][-1, 0]:.4f}\")\n        ax[0].plot(epoch_data, valid_data[fold][:, 0], label = f\"Valid fold {fold+1} score {valid_data[fold][-1, 0]:.4f}\")\n        ax[1].plot(epoch_data, train_data[fold][:, 1], label = f\"Train fold {fold+1} loss {train_data[fold][-1, 1]:.4f}\")\n        ax[1].plot(epoch_data, valid_data[fold][:, 1], label = f\"Valid fold {fold+1} loss {valid_data[fold][-1, 1]:.4f}\")\n        ax[0].legend(); ax[1].legend()\nplt.show()","0af3b9a2":"if training:\n    loader = tqdm(range(len(epoch_data), len(epoch_data) + num_epoch), desc = \"Epoch\")\n    board = ipywidgets.Output()\n    display.display(board)\n    graph = display.display(None, display_id = True)\n    for i in loader:\n        with board:\n            epoch_data.append(i+1)\n            # Make grid\n            fig, axs = plt.subplots(num_folds, 2, figsize=(10*2, 5*num_folds))\n            # Close figure\n            plt.close(fig)\n            for fold in range(num_folds):\n                train_score, train_loss, valid_score, valid_loss = train(fold)\n                train_data[fold] = np.append(train_data.get(fold, np.empty((0, 2))), [[train_score, train_loss]], axis = 0)\n                valid_data[fold] = np.append(valid_data.get(fold, np.empty((0, 2))), [[valid_score, valid_loss]], axis = 0)\n                # Visualize\n                ax = axs[fold] if num_folds > 1 else axs\n                ax[0].clear(); ax[1].clear()\n                ax[0].plot(epoch_data, train_data[fold][:, 0], label = f\"Train fold {fold+1} score {train_data[fold][-1, 0]:.4f}\")\n                ax[0].plot(epoch_data, valid_data[fold][:, 0], label = f\"Valid fold {fold+1} score {valid_data[fold][-1, 0]:.4f}\")\n                ax[1].plot(epoch_data, train_data[fold][:, 1], label = f\"Train fold {fold+1} loss {train_data[fold][-1, 1]:.4f}\")\n                ax[1].plot(epoch_data, valid_data[fold][:, 1], label = f\"Valid fold {fold+1} loss {valid_data[fold][-1, 1]:.4f}\")\n                ax[0].legend(); ax[1].legend()\n                graph.update(fig)\n            # Clear all progress bar with in board widget\n            display.clear_output()\n            graph = display.display(fig, display_id = True)\n        # Save model\n        params = {\n            'models': dict([(fold, models[fold].state_dict()) for fold in models]),\n            'optimizers': dict([(fold, optimizers[fold].state_dict()) for fold in optimizers]),\n            'schedulers': dict([(fold, schedulers[fold].state_dict()) for fold in schedulers]),\n            'train_folds': train_folds,\n            'valid_folds': valid_folds,\n            'train_data': train_data,\n            'valid_data': valid_data,\n            'epoch_data': epoch_data\n        }\n        torch.save(params, \"checkpoint.pth\")\n    loader.write(\"Done!\")","a09fd547":"idx = np.random.randint(len(train_dataset))\nimage, mask = train_dataset[idx]\nimg = torch.tensor(image, dtype = torch.float32).permute(3, 0, 1, 2).to(device)\npred = models[0](img[None].float())[0].detach().cpu()\npred = pred.squeeze(0).permute(1, 2, 3, 0)","90535500":"viz.visualize(torch.sigmoid(pred), image)","18eab711":"viz.visualize(torch.sigmoid(pred), mask)","94f8b2a6":"metric(torch.tensor(pred[None]), torch.tensor(mask[None])).item()","c3ee09fa":"## [UNet3d](https:\/\/github.com\/JielongZ\/3D-UNet-PyTorch-Implementation)"}}