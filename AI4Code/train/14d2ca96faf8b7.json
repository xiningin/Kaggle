{"cell_type":{"30500946":"code","11a14e03":"code","d5792af1":"code","daccd4e8":"code","a54f7924":"code","a2075b2e":"code","ca319674":"code","75eafc80":"code","58b92205":"code","17b60458":"code","1ea2819f":"code","5c0b19c2":"code","509bf7fd":"code","fbb672df":"code","c60ea35d":"code","68f348fd":"code","24c67d74":"code","bdfe9063":"code","6b71c6c7":"code","c5afc27f":"code","581cbf9a":"code","0a14926c":"code","bf5c0cb7":"code","5643e7b0":"code","19934ce7":"code","dbe2ce2e":"code","4c577a15":"code","fc9ba29a":"code","b74e1657":"code","b0235ca4":"code","0deffa2b":"code","bef3056c":"code","d026a4a0":"code","cbdd8f76":"code","e6762915":"code","c019d2c6":"code","ab300f54":"code","9ca5f0d5":"code","6e10b152":"code","065d76b8":"code","2e8a76c9":"code","9cb9bb60":"markdown","7768c42b":"markdown","58699fc5":"markdown","152e99f1":"markdown","d039cb8d":"markdown","3e1dc09d":"markdown","027e4844":"markdown","24858ef2":"markdown","5a80795b":"markdown","2ad31d9f":"markdown","7bd13106":"markdown"},"source":{"30500946":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport warnings\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n","11a14e03":"data = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')","d5792af1":"data.shape","daccd4e8":"data.head()","a54f7924":"label = data.label\ndata=data.drop('label',axis=1)\nprint(\"Data Shape: \",data.shape)\nprint(\"Label Shape: \",label.shape)","a2075b2e":"data.columns","ca319674":"for x in range(0,8):\n    train_0=data[label==x]\n    data_new=[]\n    for idx in train_0.index:\n        val=train_0.loc[idx].values.reshape(28,28)\n        data_new.append(val)\n    plt.figure(figsize=(25,25))   \n    for x in range(1,8):\n        ax1=plt.subplot(1, 20, x)\n        ax1.imshow(data_new[x],cmap='gray')","75eafc80":"y = pd.value_counts(data.values.ravel()).sort_index()\nN = len(y)\nx = range(N)\nwidth =0.9\nplt.figure(figsize=[20,8])\nax1=plt.subplot(1, 2,1)\nax1.bar(x, y, width, color=\"blue\")\nplt.title('Pixel Value Frequency For Gray Scale Images')\nplt.xlabel('Pixel Value (0-255)')\nplt.ylabel('Frequency')\n#ax1.imshow(data_new[x],cmap='gray')\nax2=plt.subplot(1, 2,2)\nax2.bar(x, y, width, color=\"blue\")\nplt.title('Pixel Value Frequency (Log Scale)')\nplt.yscale('log')\nplt.xlabel('Pixel Value (0-255)')\nplt.ylabel('Frequency')","58b92205":"data_test = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')","17b60458":"data_test.shape","1ea2819f":"data.shape","5c0b19c2":"label2 = data_test.label\ndata_test = data_test.drop('label',axis=1)","509bf7fd":"data_test.shape","fbb672df":"final_df=pd.concat([data, data_test],axis=0)\nfinal_label = pd.concat([label, label2],axis=0)","c60ea35d":"final_df.shape","68f348fd":"label2.shape","24c67d74":"train, test,train_labels, test_labels = train_test_split(final_df, final_label, train_size=0.8, random_state=42)\nprint(\"Train Data Shape: \",train.shape)\nprint(\"Train Label Shape: \",train_labels.shape)\nprint(\"Test Data Shape: \",test.shape)\nprint(\"Test Label Shape: \",test_labels.shape)","bdfe9063":"from sklearn import svm","6b71c6c7":"i=5000;\nscore=[]\nfittime=[]\nscoretime=[]\nclf = svm.SVC(random_state=42)","c5afc27f":"import time\nimport os","581cbf9a":"start_time = time.time()\nclf.fit(train[:i], train_labels[:i].values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nstart_time = time.time()\nscore=clf.score(test,test_labels)\nprint(\"Accuracy for grayscale: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase1=[score,fittime,scoretime]","0a14926c":"start_time = time.time()\nclf.fit(train_b[:i], train_labels[:i].values.ravel())\nfititme = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nscore=clf.score(test_b,test_labels)\nstart_time = time.time()\nclf.fit(train_b[:i], train_labels[:i].values.ravel())\nprint(\"Accuracy for binary: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase2=[score,fittime,scoretime]","bf5c0cb7":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA as sklearnPCA","5643e7b0":"#standardized data\nsc = StandardScaler().fit(train)\nX_std_train = sc.transform(train)\nX_std_test = sc.transform(test)\n\n#If n_components is not set then all components are stored \nsklearn_pca = sklearnPCA().fit(X_std_train)\ntrain_pca = sklearn_pca.transform(X_std_train)\ntest_pca = sklearn_pca.transform(X_std_test)\n\n#Percentage of variance explained by each of the selected components.\n#If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\nvar_per = sklearn_pca.explained_variance_ratio_\ncum_var_per = sklearn_pca.explained_variance_ratio_.cumsum()\n\nplt.figure(figsize=(30,10))\nind = np.arange(len(var_per)) \nplt.bar(ind,var_per)\nplt.xlabel('n_components')\nplt.ylabel('Variance')","19934ce7":"n_comp=len(cum_var_per[cum_var_per <= 0.90])\nprint(\"Keeping 90% Info with \",n_comp,\" components\")\nsklearn_pca = sklearnPCA(n_components=n_comp)\ntrain_pca = sklearn_pca.fit_transform(X_std_train)\ntest_pca = sklearn_pca.transform(X_std_test)\nprint(\"Shape before PCA for Train: \",X_std_train.shape)\nprint(\"Shape after PCA for Train: \",train_pca.shape)\nprint(\"Shape before PCA for Test: \",X_std_test.shape)\nprint(\"Shape after PCA for Test: \",test_pca.shape)","dbe2ce2e":"start_time = time.time()\nclf.fit(train_pca[:i], train_labels[:i].values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nstart_time = time.time()\nscore=clf.score(test_pca,test_labels)\nprint(\"Accuracy for grayscale: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score model: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase3=[score,fittime,scoretime]","4c577a15":"#standardized data\nsc = StandardScaler().fit(train_b)\nX_std_train = sc.transform(train_b)\nX_std_test = sc.transform(test_b)\n\n#If n_components is not set then all components are stored \nsklearn_pca = sklearnPCA().fit(X_std_train)\n#train_pca_b = sklearn_pca.transform(X_std_train)\n#test_pca_b = sklearn_pca.transform(X_std_test)\n\n#Percentage of variance explained by each of the selected components.\n#If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\nvar_per = sklearn_pca.explained_variance_ratio_\ncum_var_per = sklearn_pca.explained_variance_ratio_.cumsum()\n\nplt.figure(figsize=(30,10))\nind = np.arange(len(var_per)) \nplt.bar(ind,var_per)\nplt.xlabel('n_components')\nplt.ylabel('Variance')","fc9ba29a":"n_comp=len(cum_var_per[cum_var_per <= 0.90])\nprint(\"Keeping 90% Info with \",n_comp,\" components\")\nsklearn_pca = sklearnPCA(n_components=n_comp)\ntrain_pca_b = sklearn_pca.fit_transform(X_std_train)\ntest_pca_b = sklearn_pca.transform(X_std_test)\nprint(\"Shape before PCA for Train: \",X_std_train.shape)\nprint(\"Shape after PCA for Train: \",train_pca_b.shape)\nprint(\"Shape before PCA for Test: \",X_std_test.shape)\nprint(\"Shape after PCA for Test: \",test_pca_b.shape)","b74e1657":"start_time = time.time()\nclf.fit(train_pca_b[:i], train_labels[:i].values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nstart_time = time.time()\nscore=clf.score(test_pca_b,test_labels)\nprint(\"Accuracy for grayscale: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score model: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase4=[score,fittime,scoretime]","b0235ca4":"head =[\"Accuracy\",\"FittingTime\",\"ScoringTime\"]\nprint(\"\\t\\t case1 \\t\\t\\t case2 \\t\\t\\t case3 \\t\\t\\t case4\")\nfor h, c1, c2, c3, c4 in zip(head, case1, case2, case3, case4):\n    print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(h, c1, c2, c3, c4))","0deffa2b":"from tqdm import tqdm\n\nfit_time=[]\nscore=[]\nscore_time=[]\nfor j in tqdm(range(1000,31000,5000)):\n    start_time = time.time()\n    clf.fit(train_pca_b[:j], train_labels[:j].values.ravel())\n    fit_time.append(time.time() - start_time)\n    start_time = time.time()\n    score.append(clf.score(test_pca_b,test_labels))\n    score_time.append(time.time() - start_time)","bef3056c":"x=list(range(1000,31000,5000))\nplt.figure(figsize=[20,5]);\nax1=plt.subplot(1, 2,1)\nax1.plot(x,score,'-o');\nplt.xlabel('Number of Training Samples')\nplt.ylabel('Accuray')\nax2=plt.subplot(1, 2,2)\nax2.plot(x,score_time,'-o');\nax2.plot(x,fit_time,'-o');\nplt.xlabel('Number of Training Samples')\nplt.ylabel('Time to Compute Score\/Fit (sec)')\nplt.legend(['score_time','fitting_time'])","d026a4a0":"clf.get_params","cbdd8f76":"from sklearn.model_selection import GridSearchCV","e6762915":"parameters = {'gamma': [1, 0.1, 0.01, 0.001],\n             'C': [1000, 100, 10, 1]} \n\np = GridSearchCV(clf , param_grid=parameters, cv=3)","c019d2c6":"X=train_pca_b[:i]\ny=train_labels[:i].values.ravel()\nstart_time = time.time()\np.fit(X,y)\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))","ab300f54":"print(\"Scores for all Parameter Combination: \\n\",p.cv_results_['mean_test_score'])\nprint(\"\\nOptimal C and Gamma Combination: \",p.best_params_)\nprint(\"\\nMaximum Accuracy acheieved on LeftOut Data: \",p.best_score_)","9ca5f0d5":"C=p.best_params_['C']\ngamma=p.best_params_['gamma']\nclf=svm.SVC(C=C,gamma=gamma, random_state=42)","6e10b152":"start_time = time.time()\nclf.fit(train_pca_b[:i], train_labels[:i].values.ravel())\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint(\"Accuracy for binary: \",clf.score(test_pca_b,test_labels))","065d76b8":"start_time = time.time()\nclf.fit(train_pca_b, train_labels.values.ravel())\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint(\"Accuracy for binary: \",clf.score(test_pca_b,test_labels))","2e8a76c9":"import pickle\nfilename = 'finalized_model.pkl'\npickle.dump(clf, open(filename, 'wb'))","9cb9bb60":"Having a look at pixel values frequency (0 to 255)\nTo get a better idea, lets convert the plot to Log Scale.\nBased on leading 0s and 255s we can try converting it to binary in the later steps to simplify the problem.","7768c42b":"Keeping 90% of information by choosing components falling within 0.90 cumulative.","58699fc5":"### Training Data Size Vs Accuracy , Fitting & Score Times \n\nUnderstanding how training data size affects accuracy,","152e99f1":"### Classification using SVM\nUsing SVM Classifier from sklearn library.\nWe have 33600 training samples, fitting them is going to take alot of time. To keep it simple for now, lets select 5000 out of them.\nYou can change the value i to use the desired number of samples.","d039cb8d":"### GrayScale + Dimensionality Reduction - PCA\nIt is a linear transformation technique used to identify strong patterns in data by finding out variable correlation. It maps the data to a lower dimensional subspace in a way that data variance is maximized while retaining most of the information.\n\nTo understand how PCA works, this tutorial may help - Principal Component Analysis Explained\n\nWe are using sklearnPCA library here to perform PCA Dimensionality Reduction.\n\nHere, Data is standardized and PCA is performed on data with all the components. Then variance is plotted for all components to decide which components to remove.","3e1dc09d":"### Gray Scale Images\nFitting train data and finding a score for test data to check model performance","027e4844":"Lets find the score using reduced dimensions keeping the same amount of samples, to compare accuracy.","24858ef2":"### Binary + Dimensionality Reduction - PCA","5a80795b":"### Binary Images\n\nTo simply the problem, converting images to black and white from gray scale by replacing all values > 0 to 1.\nAnd Converting 1D array to 2D 28x28 array using [**reshape**](https:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/reference\/generated\/numpy.reshape.html) , to plot and view **binary** images.","2ad31d9f":"### Parameter Selection for SVM using GridSearchCV\nOut of parameters below, we will be playing with Gamma and C, where\n\nGamma is the parameter of a Gaussian Kernel (to handle non-linear classification)\nand C the parameter for the soft margin cost function, also known as cost of misclassification. A large C gives you low bias and high variance and vice versa.","7bd13106":"to verify, lets pass the optimal parameters to Classifier and check the score."}}