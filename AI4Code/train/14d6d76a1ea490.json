{"cell_type":{"5cea9f55":"code","29624102":"code","34cd33ca":"code","cb4d897d":"code","480fb0f7":"code","a829d050":"code","1c480e1f":"code","2138febe":"code","8cbac7bf":"code","364e44d2":"code","c14356c5":"code","979b4958":"code","990f78ac":"code","fd21128a":"code","9d32baef":"code","acffa705":"code","90788156":"code","7050b55d":"code","4e466e78":"code","f2acb46e":"code","c0b7dc68":"code","17620750":"code","cc22677e":"code","b1e33053":"code","57d1d57c":"code","b879d4bd":"code","7c26f2a4":"code","356661c6":"code","1a89c86b":"code","8d1ee4cc":"code","5a293507":"code","282b64c9":"code","e62755f8":"markdown","0db1a761":"markdown","9e954dcc":"markdown","c1b5b35b":"markdown","6e129c09":"markdown"},"source":{"5cea9f55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n!pip install autoviml\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\npd.set_option('display.max_colwidth', -1)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29624102":"from autoviml.Auto_NLP import Auto_NLP","34cd33ca":"destination_folder = '.\/'","cb4d897d":"import re\nimport string\nclass TextProcessing:\n    def __init__(self,text):\n        self.text = text\n    \n    def remove_punctuation(self,text):\n        return \"\".join([i for i in text if i not in string.punctuation])\n    def remove_url(self,text):\n        url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n        text = url.sub(r'',text)\n        url = re.compile(r'http?:\/\/\\S+|www\\.\\S+')\n        return url.sub(r'',text)\n    def remove_emoji(self,text):\n        emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', text)\n    def remove_html(self,text):\n        html=re.compile(r'<.*?>')\n        return html.sub(r'',text)\n    def clean(self):\n#         self.text = self.text.apply(lambda x: re.sub(\"s+\",\" \", x) )\n#         text = self.text.apply(lambda x: self.remove_punctuation(x)  )\n        self.text = self.text.apply(lambda x: x.lower()  )\n#         self.text = self.text.apply(lambda x: self.remove_url(x)  )\n#         self.text = self.text.apply(lambda x: self.remove_emoji(x)  )\n#         self.text = self.text.apply(lambda x: self.remove_html(x)  )\n        return self.text\n        ","480fb0f7":"# Libraries\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n# Preliminaries\n\nfrom torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n\n# Models\n\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Training\n\nimport torch.optim as optim\n\n# Evaluation\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n","a829d050":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","1c480e1f":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\").fillna('')\ntrain['text_keyword'] = train[[\"keyword\",\"text\"]].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\nttp = TextProcessing(train['text_keyword'])\ntrain['text_keyword'] = ttp.clean()","2138febe":"train.head()","8cbac7bf":"train.keyword.value_counts()","364e44d2":"train.isna().sum() \/train.shape[0]","c14356c5":"# train[train.target==1].groupby(['keyword'])['keyword'].count().plot.bar()\ntrain[train.target==1]['keyword'].value_counts()","979b4958":"train[train.target==0]['keyword'].value_counts()","990f78ac":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(4, 4), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","fd21128a":"common_words = get_top_n_bigram(train[train.target == 1]['text'], 20)\ndf1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\ndf1.groupby('text').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams for disaster text')","9d32baef":"common_words = get_top_n_bigram(train[train.target == 0]['text'], 20)\ndf1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\ndf1.groupby('text').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigramsv for non disaster text')","acffa705":"test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\").fillna('')\ntest['text_keyword'] = test[[\"keyword\",\"text\"]].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\ntetp = TextProcessing(test['text_keyword'])\ntest['text_keyword'] = tetp.clean()\n#Dummy target Variable\ntest['target'] = 0\ntest[['text_keyword','target']].to_csv('test_bert.csv',index=False)","90788156":"train_bert = pd.DataFrame()\nvalid_bert = pd.DataFrame()\ndef create_dataset(X,Y,df):\n    df['text_keyword'] = X\n    df['target'] = Y    ","7050b55d":"X_train, X_valid, y_train, y_valid = train_test_split( train['text_keyword'],train['target'], test_size=0.33, random_state=42, stratify=train['target'])\ncreate_dataset(X_train,y_train,train_bert)\ncreate_dataset(X_valid,y_valid,valid_bert)\ntrain_bert.to_csv(\"train_bert.csv\",index=False)\nvalid_bert.to_csv(\"valid_bert.csv\",index=False)","4e466e78":"train_x, test_x, predictor, predicted= Auto_NLP('text_keyword', train_bert, test,'target',score_type=\"balanced_accuracy\",\n                                            top_num_features=200,modeltype=\"Classification\",verbose=2,build_model=True)","f2acb46e":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Model parameter\nMAX_SEQ_LEN = 128\nPAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\nUNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n\n# Fields\n\nlabel_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\ntext_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\ntrain_fields = [ ('text_keyword', text_field),('target', label_field)]\ntest_fields = [ ('text_keyword', text_field)]\n\n# TabularDataset\n\ntrain, valid,test = TabularDataset.splits(path='.\/', train='train_bert.csv', validation='valid_bert.csv',\n                                          test='test_bert.csv', format='CSV', fields=train_fields, skip_header=True)\n\n\n# Iterators\n\ntrain_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text_keyword),\n                            device=device, train=True, sort=True, sort_within_batch=True)\nvalid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text_keyword),\n                            device=device, train=True, sort=True, sort_within_batch=True)\ntest_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)\n","c0b7dc68":"class BERT(nn.Module):\n\n    def __init__(self):\n        super(BERT, self).__init__()\n\n        options_name = \"bert-base-uncased\"\n        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n\n    def forward(self, text, label):\n        loss, text_fea = self.encoder(text, labels=label)[:2]\n\n        return loss, text_fea","17620750":"# Save and Load Functions\n\ndef save_checkpoint(save_path, model, valid_loss):\n\n    if save_path == None:\n        return\n    \n    state_dict = {'model_state_dict': model.state_dict(),\n                  'valid_loss': valid_loss}\n    \n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')\n\ndef load_checkpoint(load_path, model):\n    \n    if load_path==None:\n        return\n    \n    state_dict = torch.load(load_path, map_location=device)\n    print(f'Model loaded from <== {load_path}')\n    \n    model.load_state_dict(state_dict['model_state_dict'])\n    return state_dict['valid_loss']\n\n\ndef save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n\n    if save_path == None:\n        return\n    \n    state_dict = {'train_loss_list': train_loss_list,\n                  'valid_loss_list': valid_loss_list,\n                  'global_steps_list': global_steps_list}\n    \n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')\n\n\ndef load_metrics(load_path):\n\n    if load_path==None:\n        return\n    \n    state_dict = torch.load(load_path, map_location=device)\n    print(f'Model loaded from <== {load_path}')\n    \n    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']","cc22677e":"# Training Function\n\ndef train(model,\n          optimizer,\n          criterion = nn.BCELoss(),\n          train_loader = train_iter,\n          valid_loader = valid_iter,\n          num_epochs = 5,\n          eval_every = len(train_iter) \/\/ 2,\n          file_path = destination_folder,\n          best_valid_loss = float(\"Inf\")):\n    \n    # initialize running values\n    running_loss = 0.0\n    valid_running_loss = 0.0\n    global_step = 0\n    train_loss_list = []\n    valid_loss_list = []\n    global_steps_list = []\n\n    # training loop\n    model.train()\n    for epoch in range(num_epochs):\n        for (text_keyword, target), _ in train_loader:\n            target = target.type(torch.LongTensor)           \n            target = target.to(device)\n            text_keyword = text_keyword.type(torch.LongTensor)  \n            text_keyword = text_keyword.to(device)\n            output = model(text_keyword, target)\n            loss, _ = output\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # update running values\n            running_loss += loss.item()\n            global_step += 1\n\n            # evaluation step\n            if global_step % eval_every == 0:\n                model.eval()\n                with torch.no_grad():                    \n\n                    # validation loop\n                    for (text_keyword,target), _ in valid_loader:\n                        target = target.type(torch.LongTensor)           \n                        target = target.to(device)\n                        text_keyword = text_keyword.type(torch.LongTensor)  \n                        text_keyword = text_keyword.to(device)\n                        output = model(text_keyword, target)\n                        loss, _ = output\n                        \n                        valid_running_loss += loss.item()\n\n                # evaluation\n                average_train_loss = running_loss \/ eval_every\n                average_valid_loss = valid_running_loss \/ len(valid_loader)\n                train_loss_list.append(average_train_loss)\n                valid_loss_list.append(average_valid_loss)\n                global_steps_list.append(global_step)\n\n                # resetting running values\n                running_loss = 0.0                \n                valid_running_loss = 0.0\n                model.train()\n\n                # print progress\n                print('Epoch [{}\/{}], Step [{}\/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n                              average_train_loss, average_valid_loss))\n                \n                # checkpoint\n                if best_valid_loss > average_valid_loss:\n                    best_valid_loss = average_valid_loss\n                    save_checkpoint(file_path + '\/' + 'model.pt', model, best_valid_loss)\n                    save_metrics(file_path + '\/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n    \n    save_metrics(file_path + '\/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n    print('Finished Training!')","b1e33053":"model = BERT().to(device)\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\n\ntrain(model=model, optimizer=optimizer)","57d1d57c":"train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '\/metrics.pt')\nplt.plot(global_steps_list, train_loss_list, label='Train')\nplt.plot(global_steps_list, valid_loss_list, label='Valid')\nplt.xlabel('Global Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show() ","b879d4bd":"best_model = BERT().to(device)\n\nload_checkpoint(destination_folder + '\/model.pt', best_model)","7c26f2a4":"y_eval = []\ny_true_eval = []\ndef evaluate(model, test_loader):  \n\n    model.eval()\n    with torch.no_grad():\n        for (text_keyword,target), _ in test_loader:\n\n                target = target.type(torch.LongTensor)           \n                target = target.to(device)\n                text_keyword = text_keyword.type(torch.LongTensor)  \n                text_keyword = text_keyword.to(device)\n                output = model(text_keyword, target)\n\n                _, output = output\n                y_eval.extend(torch.argmax(output, 1).tolist())\n                y_true_eval.extend(target.tolist())\n    \n    print('Classification Report:')\n    print(classification_report(y_true_eval, y_eval, labels=[0,1], digits=4))\n    \n    cm = confusion_matrix(y_true_eval, y_eval, labels=[0,1])\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n\n    ax.set_title('Confusion Matrix')\n\n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('True Labels')\n\n    ax.xaxis.set_ticklabels(['Not Disaster', 'REAL'])\n    ax.yaxis.set_ticklabels(['Not Disaster', 'REAL'])","356661c6":"evaluate(best_model, valid_iter)","1a89c86b":"valid_bert['y_eval'] = y_eval\n\nvalid_bert[valid_bert.target ==1]","8d1ee4cc":"y_pred = []\ndef predict(model, test_loader):    \n\n    model.eval()\n    with torch.no_grad():\n        for (text_keyword,target), _ in test_loader:\n                target = target.type(torch.LongTensor)           \n                target = target.to(device)\n                text_keyword = text_keyword.type(torch.LongTensor)  \n                text_keyword = text_keyword.to(device)\n                output = model(text_keyword, target)\n\n                _, output = output\n                y_pred.extend(torch.argmax(output, 1).tolist())\n               ","5a293507":"predict(best_model, test_iter)\ntest_final = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest_final['target'] = y_pred\ntest_final[['id','target']].to_csv('submission.csv',index=False)","282b64c9":"# test_final['autoNlpPrediction'] = predicted\n# test_final[(test_final.target==1) & (test_final.autoNlpPrediction==0)]\ntest_final['target'] = predicted\ntest_final[['id','target']].to_csv('submission1.csv',index=False)","e62755f8":"Training","0db1a761":"Evaluate","9e954dcc":"**Model**","c1b5b35b":"**Selecting the best model after training step**","6e129c09":"**Building a benchmark model using AutoNLP**"}}