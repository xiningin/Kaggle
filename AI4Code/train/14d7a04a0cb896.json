{"cell_type":{"e9016acf":"code","ae654921":"code","01e2c37d":"code","c30f0675":"code","1323b6f1":"code","1f2038ec":"code","bc98dfed":"code","d7343f8a":"code","1cd3dc5e":"code","9ac2d9e8":"code","ace0493c":"code","5ca3f0f0":"code","b0406dc2":"code","a1c90092":"code","62418d14":"markdown","3d9a59cb":"markdown","8d41a0e7":"markdown","c183dc55":"markdown","05f45398":"markdown","89039419":"markdown","6bb0d827":"markdown","d0e48718":"markdown","476a52a6":"markdown","42a1defd":"markdown","5cdde381":"markdown","baa71c2d":"markdown","201752fb":"markdown","f18e7f30":"markdown","09c13826":"markdown","53fd702d":"markdown","74fa4c45":"markdown","5ebdc562":"markdown","9f2ca840":"markdown"},"source":{"e9016acf":"import pandas as pd\nimport matplotlib.pyplot as plt        #reading libraries\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport warnings  \n","ae654921":"warnings.filterwarnings('ignore')\npd.set_option('display.max_columns',None)                             #reading dataset\ndf = pd.read_csv(\"\/kaggle\/input\/real-estate-dataset\/data.csv\")           ","01e2c37d":"print(df.head())\nprint(df.shape)\nprint(df.describe())\nprint(df.isnull().sum())\ndf[df.isnull().any(axis=1)]","c30f0675":"df2 = df.dropna() #removing rows with na\ndf2.shape","1323b6f1":"print(df2.columns[0:13])\nx_names = df2.columns[0:13]           # ploting all x variables with y \ny_name = df2.columns[-1]\ndef pllot(x,y):\n  plt.scatter(df2[x],df2[y])\n  plt.xlabel(x)\n  plt.ylabel(y)\n  plt.title(\"Scatter plot of \"+x+\" and \"+y)\n  plt.show()\nfor i in x_names:\n  pllot(i, y_name)","1f2038ec":"def outliers(x):                       # removing outliers\n  l_b = x.mean()-3*x.std() \n  u_b = x.mean()+3*x.std()\n  x_u = x.index[x>u_b]\n  x_l = x.index[x<l_b]\n  x[x_u] = max(x.drop(x_u, axis=0))\n  x[x_l] = min(x.drop(x_l, axis=0))\n  return x\nfor i in x_names:\n  if i != 'CHAS':\n    df2[i] = outliers(df2[i])","bc98dfed":"cor_matrix = df2.corr().abs().round(2)\nsns.set(rc={'figure.figsize':(12,6)})\nsns.heatmap(data=cor_matrix , annot=True)\ncor_matrix","d7343f8a":"x_names = x_names.drop(\"RAD\")\nx_scaled = StandardScaler().fit_transform(df2[x_names])\nfeatures = x_scaled.T\ncov_matrix = np.cov(features)\nvalues, vectors = np.linalg.eig(cov_matrix)\nexplained_variances = []\ncum_variances = []\nfor i in range(len(values)):\n    explained_variances.append(values[i] \/ np.sum(values))\n    cum_variances.append(sum(explained_variances))\n \nprint(explained_variances)\nprint(cum_variances)\nplt.plot(explained_variances, label = \"explained variance\")\nplt.plot(cum_variances, label = \"cumulative explained variance\")\nplt.legend(loc = \"right\")","1cd3dc5e":"x_train,x_test,y_train,y_test= train_test_split(df2[x_names],df2[y_name],test_size=0.3,random_state=1)\nx_train_scaled = StandardScaler().fit_transform(x_train)\nx_test_scaled = StandardScaler().fit_transform(x_test)","9ac2d9e8":"model = LinearRegression()\nmodel.fit(x_train,y_train)\n#print(model.intercept_)\n#print(model.coef_)\nx_train2 = sm.add_constant(x_train)\nest = sm.OLS(y_train, x_train2)\nest2 = est.fit()\nprint(est2.summary())\ny_pred_train1 = model.predict(x_train)\ny_pred_test1 = model.predict(x_test)\nprint(\"MSE train\", round(mean_squared_error(y_train,y_pred_train1),2))\nprint(\"MAE train\", round(mean_absolute_error(y_train,y_pred_train1),2))\nprint(\"RMSE train\", round(np.sqrt(mean_squared_error(y_train,y_pred_train1)),2))\nprint(\"MSE test\", round(mean_squared_error(y_test,y_pred_test1),2))\nprint(\"MAE test\", round(mean_absolute_error(y_test,y_pred_test1),2))\nprint(\"RMSE test\", round(np.sqrt(mean_squared_error(y_test,y_pred_test1)),2))\nscore = cross_val_score(model,x_train,y_train, scoring =\"r2\" ,cv=RepeatedKFold(n_splits=10, n_repeats=3, random_state=1),n_jobs=-1)\nprint(\"Average value of r2 score for cross validation was equal to {}.\".format(round(score.mean(),4)))\nplt.scatter(y_pred_test1, y_test)\nplt.plot(y_test, y_test, color = \"red\")\nplt.title(\"Plot of real values vs predicted\")\nplt.xlabel('Predictions')\nplt.ylabel('Real values')","ace0493c":"model2 = SVR()\nkernel = [\"linear\",\"sigmoid\",\"rbf\",\"poly\"]\ntolerance = [1e-3, 1e-4, 1e-5, 1e-6]\nC = [1, 1.5, 2, 2.5, 3, 4, 5]\ngrid = dict(kernel=[\"linear\"], tol=tolerance, C=C)\ncvFold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\ngridSearch = GridSearchCV(estimator=model2, param_grid=grid, n_jobs=-1,\n\tcv=cvFold, scoring=\"neg_mean_squared_error\")\nsearchResults = gridSearch.fit(x_train_scaled, y_train)\nbestModel = searchResults.best_estimator_","5ca3f0f0":"y_pred_train2 = bestModel.predict(x_train_scaled)\ny_pred_test2 = bestModel.predict(x_test_scaled)\nprint(\"MSE train\", round(mean_squared_error(y_train,y_pred_train2),2))\nprint(\"MAE train\", round(mean_absolute_error(y_train,y_pred_train2),2))\nprint(\"RMSE train\", round(np.sqrt(mean_squared_error(y_train,y_pred_train2)),2))\nprint(\"MSE test\", round(mean_squared_error(y_test,y_pred_test2),2))\nprint(\"MAE test\", round(mean_absolute_error(y_test,y_pred_test2),2))\nprint(\"RMSE test\", round(np.sqrt(mean_squared_error(y_test,y_pred_test2)),2))\nprint(bestModel)\nplt.scatter(y_pred_test2, y_test)\nplt.plot(y_pred_test2, y_pred_test2, color = \"red\")\nplt.title(\"Plot of real values vs predicted\")\nplt.xlabel('Predictions')\nplt.ylabel('Real values')\nplt.show()\npd.Series(abs(bestModel.coef_[0]), index=x_names).nlargest(12).plot(kind='barh')\nplt.title(\"Plot of variable improtance for SVR\")\n","b0406dc2":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RepeatedKFold\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nGBR=GradientBoostingRegressor()\nsearch_grid={'n_estimators':[25,50,100,200],'learning_rate':[0.15,0.1,0.05,0.01]}\nsearch=GridSearchCV(estimator=GBR,param_grid=search_grid,scoring='neg_mean_squared_error',n_jobs=1,cv=cv)\nsearch_fit = search.fit(x_train_scaled, y_train)\nbest_model_gbr = search_fit.best_estimator_","a1c90092":"y_pred_train3 = bestModel.predict(x_train_scaled)\nprint(\"MSE train\", round(mean_squared_error(y_train,y_pred_train3),2))\nprint(\"MAE train\", round(mean_absolute_error(y_train,y_pred_train3),2))\nprint(\"RMSE train\", round(np.sqrt(mean_squared_error(y_train,y_pred_train3)),2))\ny_pred_test3 = best_model_gbr.predict(x_test_scaled)\nprint(\"MSE test\", round(mean_squared_error(y_test,y_pred_test3),2))\nprint(\"MAE test\", round(mean_absolute_error(y_test,y_pred_test3),2))\nprint(\"RMSE test\", round(np.sqrt(mean_squared_error(y_test,y_pred_test3)),2))\nprint(best_model_gbr)\n\nplt.scatter(y_pred_test3, y_test)\nplt.plot(y_pred_test3, y_pred_test3, color = \"red\")\nplt.title(\"Plot of real values vs predicted\")\nplt.xlabel('Predictions')\nplt.ylabel('Real values')\nplt.show()\npd.Series(abs(best_model_gbr.feature_importances_), index=x_names).nlargest(12).plot(kind='barh')\nplt.title(\"Plot of variable improtance for GBR\")","62418d14":"## Gradient boosting regressor\n\nLast used method will be GBR for standardized dataset. ","3d9a59cb":"For GBR cross validation was used to determine two parameters: number of estimators and learning rate, rest of the parameters was left as default. Similarly to previous models cross validation was made with 10 splits and 3 repeats and MSE for scoring, based on that the best set of parameters was choosen to fit model. ","8d41a0e7":"## First look at the data\n","c183dc55":"For SVR it is neccessary to specify some parameters like kernel, tolerance and cost. Due to that tuning of these parameters was performed, based on the cross validation with 10 splits and 3 repeats and scoring based on MSE the best combination of parametrs was choosen and used to fit model.","05f45398":"## Summary\n\nThe best method based on choosen metrics was GBR, second was linear regression and last one SVR. Errors achieved for all methods show that predictions weren't very precise, but at least models gave information which factors have higher impact on prices of homes. 3 most improtant features were:\n- RM - average number of rooms per dwelling\n- LSTAT - % lower status of the population\n- DIS - weighted distances to five Boston employment centres\n\nOn the other hand 3 features that had the lowest impact were:\n- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n- INDUS - proportion of non-retail business acres per town\n- CHAS - variable that informs if tract bounds river\n\n\n","89039419":"Metrics results based on GBR model achieved the least error for test set, and are much lower than for training set. For GBR model most important variable was % of lower status population and number of rooms per dwelling. Other variables were far less important unlike in previous methods.","6bb0d827":"Before making any models dataset has to be split into training and test in proportion 70:30. Beside unscaled dataset there will be also dataset with standardized values.","d0e48718":"## Introduction\n\nGoal of this notebook is to predict variable MEDV, which is corresponding to median value of owner-occupied homes in thousands of dollars. Methods used are:\n- linear regression\n- Support vector regression\n- Gradient boosting regressor","476a52a6":"## Linear Regression\n\nFirst model will be a classical linear regression. ","42a1defd":"PCA analysis informs how much variance of Y variable is explained in the space with reduced number of dimensions. For example in 1 dimension variance is explained in about 47% for 2 dimensions in 59% and so on. If there was a point where grow of explained variance rapidly dropped and cumulative variance was high (80-90%) it would be reasonable to consider transforming data, but it's not the case in this dataset so for the further analysis no changes were made. ","5cdde381":"Plots above show relation between endogenous variable MEDV and all other variables. Firstly it is possible to see that almost all variables are biased and many of them have some clear outliers. Variables ZN, CHAS, RAD, TAX seems to be categorical as they have only several specified values, but from the dataset description only CHAS is clearly categorical as it gives information if home bounds tract with Charles river.  Variables RM and LSTAT show the best correlation based on the plots. RM corresponds to numebr of rooms per dwelling and shows positive relation with price of home, which is not suprising as bigger house have more rooms and cost more, LSTAT on the other hand show negative correlation which is also not suprising as it corresponds to % of lower status of the population. ","baa71c2d":"Errors achieved by this model were a bit higher for test dataset, and higher than in case of linear regression. The most important feature for SVR model is number of rooms per dwelling, and next are distance from the employment centers and % of lower status population, so it looks very similar as in the linear regression case. ","201752fb":"In the next step principial component analysis was implented to check possibility of reducing number of variables, for this purpose all exogenous variables were standardized ","f18e7f30":"## Support vector regression\n\nNext method used will be SVR, because this method is based on distances between observations training will be implemented on standardized dataset. ","09c13826":"Firstly it is good to look which variables are most significant and how they impact the created model. Looking at the t statistic and probability that coefficient is equal to 0 three most significant varaibles are:\n - RM - average number of rooms per dwelling\n - DIS - weighted distances to five Boston employment centres\n - LSTAT - % lower status of the population\n They can be interpreted as:\n - Each additional room increase the price of home by around 6053 dollars ceteris paribus\n - Increase of weighted distances to Boston employment centers by 1 unit decrease the price of the house by around 1867 dollars ceteris paribus\n - Increase of lower status population by 1% decrease the price of the home by around 284 dollars ceteris paribus\n \n Cross validation was applied for 10 splits and 3 repeats, and scoring method was r2. Average value of r2 was equal to 64,28% which is around 4% lower than r2 achieved for whole training set (68,8%). Metrics choosen to measure goodness of predictions were MSE, MAE and RMSE. Difference between training and test sets are very small, MAE even shows smaller error for test set.","53fd702d":"Table and plot above show strength of correlation in absolute values. As it was possible to see on the previous plots LSTAT and RM have the strongest correlation with MEDV equal to 0,66 and 0,68. Other variables aren't that strongly correlated but for many of them it is about 0,4-0,45 which is not that low. The strongest relation for all x variables exists for RAD and TAX, and is equal over 90% which is strong enough to call it collinearity. Due to that as TAX variable is higher correlated with Y variable it will remain for the future analysis and RAD will be removed","74fa4c45":"## Preparation of data\n\nBefore constructing prediction models it is neccessary to prepare data for this purpose. Firstly problem of the outliers will be solved. Observations that extend more than 3 times standard devation of the varaible in any direction will be replaced with value of the closest observation, using function showed below.","5ebdc562":"In the next step correlation between all variables was calculated, to check for potential problems with collinearity, and see which exogenous variables have strongest relation with MEDV. ","9f2ca840":"Dataset has 14 variables and 511 observations. 5 rows contain a missing values for variable rm (average number of rooms per dwelling) because these observations aren't very numerous removing them shouldn't have impact on future models, so I decided skip this rows in later parts. Summary of variables show that, values have different scales and for some of them difference is quite big.  "}}