{"cell_type":{"6df23f40":"code","3f249670":"code","121cb710":"code","470328f2":"code","76ae5618":"code","c904a0f7":"code","b2e4ceb1":"code","15776ec0":"code","330fd5f3":"code","ab8eb289":"code","0b825c64":"code","625f60d3":"code","88bb66c6":"code","a882a748":"markdown","c40a2ade":"markdown","325edaaa":"markdown"},"source":{"6df23f40":"!pip install kaggle-environments --upgrade -q","3f249670":"%%writefile submission.py\n\nimport json\nimport numpy as np\nimport pandas as pd\n\nclass MultiArmedBandit:\n\n    def __init__(self, no_reward_step=0.95, retry_winrate=False):\n        self.no_reward_step = no_reward_step\n        self.retry_winrate  = retry_winrate\n\n        self.bandit_state   = None\n        self.total_reward   = 0\n        self.last_step      = 0\n        \n    def __call__(self, obs, conf):\n        return self.agent(obs, conf)\n        \n        \n    # observation   {'remainingOverageTime': 60, 'agentIndex': 1, 'reward': 0, 'step': 0, 'lastActions': []}\n    # configuration {'episodeSteps': 2000, 'actTimeout': 0.25, 'runTimeout': 1200, 'banditCount': 100, 'decayRate': 0.97, 'sampleResolution': 100}\n    def agent(self, obs, conf):\n        # print('observation',   obs)\n        # print('configuration', conf)\n        # print('self.bandit_state', self.bandit_state)\n        # global history, history_bandit\n        # global bandit_state,total_reward,last_step\n\n        # updating bandit_state using the result of the previous step\n        last_reward       = obs.reward - self.total_reward\n        self.total_reward = obs.reward\n        \n        if obs.step == 0:\n            # initial bandit state\n            self.bandit_state = [[1,1] for i in range(conf.banditCount)]\n        else:       \n            if last_reward > 0:\n                self.bandit_state[ obs.lastActions[obs.agentIndex] ][0] += last_reward\n            else:\n                self.bandit_state[ obs.lastActions[obs.agentIndex] ][1] += self.no_reward_step\n\n            self.bandit_state[ obs.lastActions[0] ][0] = (\n                (self.bandit_state[ obs.lastActions[0] ][0] - 1) * conf.decayRate + 1\n            )\n            self.bandit_state[ obs.lastActions[1] ][0] = (\n                (self.bandit_state[ obs.lastActions[1] ][0] - 1) * conf.decayRate + 1\n            )\n\n            \n        # Repeat last action if we got a reward\n        if self.retry_winrate and last_reward == 1:\n            best_agent = self.last_step\n\n        # generate random number from Beta distribution for each agent and select the most lucky one            \n        else:            \n            best_proba = -1\n            best_agent = self.last_step  # None\n            for k in range(conf.banditCount):\n                proba = np.random.beta( self.bandit_state[k][0], self.bandit_state[k][1] )\n                if proba > best_proba:\n                    best_proba = proba\n                    best_agent = k\n\n        self.last_step = best_agent\n        return best_agent\n    \n    \nMultiArmedBandit_instance = MultiArmedBandit()\ndef MultiArmedBandit_agent(obs, conf):\n    return MultiArmedBandit_instance(obs, conf)","121cb710":"%%writefile random_agent.py\n\nimport random\nclass RandomAgent():\n    def __call__(observation, configuration):\n        return random.randrange(configuration.banditCount)\n    \ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","470328f2":"%run submission.py\n%run random_agent.py","76ae5618":"from kaggle_environments import make, evaluate\n\nprint([\"submission.py\", \"random_agent.py\"], evaluate(\"mab\", [\"submission.py\", \"random_agent.py\"]) )\nprint([\"submission.py\", \"submission.py  \"], evaluate(\"mab\", [\"submission.py\", \"submission.py\"]) )\nprint([\"MultiArmedBandit(0.4)\", \"MultiArmedBandit(0.75)\"], evaluate(\"mab\", [MultiArmedBandit(no_reward_step=0.4), MultiArmedBandit(no_reward_step=0.75)]) )","c904a0f7":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nenv.reset()\nenv.run([\"submission.py\", \"random_agent.py\"])\nenv.render(mode=\"ipython\", width=500, height=500)","b2e4ceb1":"import glob\nimport re\nimport os\nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nfrom collections import defaultdict\nfrom joblib import Parallel, delayed\nfrom kaggle_environments import evaluate, make, utils","15776ec0":"%%time\nagents = {\n    MultiArmedBandit(no_reward_step=n, retry_winrate=retry_winrate): f'no_reward_step={n:.2f} retry_winrate={retry_winrate}'\n    for n in np.arange(0.75, 1.0, 0.05)\n    for retry_winrate in [ True, False ]\n}\nagents[\"random_agent.py\"] = \"random_agent\" \nagents[\"..\/input\/rock-paper-candy-copy-opponent-move-unless-win\/submission.py\"] = \"copy_opponent_unless_win\" \nagents[\"..\/input\/candy-cane-optimized-ucb\/submission.py\"]                       = \"optimized-ucb\"\n# print(agents)\n\ndef evaluate_mab(i1, i2, agent1, agent2):\n    # print(i1, i2, agent1, agent2)\n    try:\n        result = evaluate(\"mab\", [ agent1, agent2 ])\n        result = np.array(result).flatten()\n    except:\n        result = np.array([0,0])\n    return (i1, i2, result)\n    \nresults = Parallel(-1)( \n    delayed(evaluate_mab)(i1, i2, agent1, agent2) \n    for i1, agent1 in enumerate(agents.keys())\n    for i2, agent2 in enumerate(agents.keys())\n    for n in range(10)\n    if i1 < i2\n)\n# results","330fd5f3":"def winrate_score(score1, score2):\n    try:\n        if score1 == score2: return  0\n        if score1 is None:   return -1\n        if score2 is None:   return  1\n        if score1 >  score2: return  1\n        if score1 <  score2: return -1\n    except: pass\n    return 0\n    \n\nscores_agent = defaultdict(list)\nscores_total = np.zeros(( len(agents), len(agents) ), dtype=np.int)\nscores_diff  = np.zeros(( len(agents), len(agents) ), dtype=np.float)\nwinrates     = np.zeros(( len(agents), len(agents) ), dtype=np.int)\n\nfor (i1, i2, result) in results:\n    scores_total[i1,i2] += (result[0] or 0)\n    scores_total[i2,i1] += (result[1] or 0)\n    scores_diff[i1,i2]  += (result[0] or 0) - (result[1] or 0) \n    scores_diff[i2,i1]  += (result[1] or 0) - (result[0] or 0)\n    winrates[i1,i2]     += winrate_score(result[0], result[1])\n    winrates[i2,i1]     += winrate_score(result[1], result[0])\n    scores_agent[ list(agents.values())[i1] ].append( result[0] )\n    scores_agent[ list(agents.values())[i2] ].append( result[1] )\n    \ndf_scores_total = pd.DataFrame(\n    scores_total, \n    index   = list(agents.values()), \n    columns = list(agents.values()),\n)\ndf_scores_diff = pd.DataFrame(\n    scores_diff, \n    index   = list(agents.values()), \n    columns = list(agents.values()),\n)\ndf_winrates = pd.DataFrame(\n    winrates, \n    index   = list(agents.values()), \n    columns = list(agents.values()),\n)\ndf_scores_agent = pd.DataFrame(scores_agent)\n\n# Sort by mean score\nfor axis in [0,1]:\n    df_scores_total = df_scores_total.reindex( df_scores_agent.mean().sort_values(ascending=False).index, axis=axis)\n    df_scores_diff  = df_scores_diff.reindex(  df_scores_agent.mean().sort_values(ascending=False).index, axis=axis)\n    df_winrates     = df_winrates.reindex(     df_scores_agent.mean().sort_values(ascending=False).index, axis=axis)\ndf_scores_agent = df_scores_agent.reindex( df_scores_agent.mean().sort_values(ascending=False).index, axis=1)\n\n\ndf_scores_agent.T","ab8eb289":"def batch(iterable, n=1):\n    l = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx:min(ndx + n, l)]\n\n        \ndef plot_df_heatmap(df, title, **kwargs):\n    plt.figure(figsize=(df.shape[0], df.shape[1]))\n    plt.title(title)\n    sns.heatmap(\n        df, annot=True, cbar=False, \n        cmap='coolwarm', linewidths=1, \n        linecolor='black', \n        fmt='.0f',\n        **kwargs\n    )\n    plt.tick_params(labeltop=True, labelright=True)\n    plt.xticks(rotation=90, fontsize=max(10,df.shape[0]))\n    plt.yticks(rotation=0,  fontsize=max(10,df.shape[0]))\n    print(title)\n    print(df.mean(axis=1).sort_values(ascending=False))\n    \n    \ndef plot_df_boxplot(df, title, columns=10, boxplot_args={}, stripplot_args={}):\n    df_orig = df\n    n_rows    = math.ceil( len(df.columns) \/ columns )\n    n_columns = math.ceil( len(df.columns) \/ n_rows  )\n    for cols in batch(df.columns, n_columns):\n        df = df_orig[cols]\n        plt.figure(figsize=(n_columns*2, n_rows*6))\n        plt.title(title, loc=\"center\")\n\n        stripplot_args = { \"facecolor\": 'white', **boxplot_args }\n        ax = sns.boxplot(data=df, **boxplot_args)\n        plt.setp(ax.artists, edgecolor='grey', facecolor='w')\n        plt.setp(ax.lines, color='grey')\n\n        stripplot_args = { \"jitter\": 0.33, \"size\": 5, **stripplot_args }\n        ax = sns.stripplot(data=df, **stripplot_args)\n\n        # ax = sns.swarmplot(data=df_scores_agent)\n        plt.xticks(rotation=90, fontsize=15)\n        plt.yticks(rotation=0,  fontsize=15)\n        pass","0b825c64":"plot_df_heatmap(df_scores_total, 'Total Scores')","625f60d3":"plot_df_heatmap(df_scores_diff, 'Relative Scores')","88bb66c6":"plot_df_boxplot(df_scores_agent, \"All Matchmaking Scores\")","a882a748":"# Candy Cane - Multi-Armed Bandit\n\nThis notebook shows how to use multi-armed bandit.\n\nMulti-armed bandit is a widely used RL-algorithm because it is very balanced in terms of exploitation\/exploration.\n\nAlgorithm logic:\n\n- At each step for each bandit generate a random number from B(a+1, b+1). B - beta-distribution, a - decay adjusted total reward from this bandit, b - number of this bandits's historical losses.\n- Select the bandit with the largest generated number and use it to generate the next step\n- Unless we won last round, in which case repeat the last action","c40a2ade":"# Hyperparameters","325edaaa":"# Further Reading\n\nThis notebook is part of a series exploring the Santa2020 Candy Cane competition\n- [Rock Paper Candy - Copy Opponent Move](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-candy-copy-opponent-move)\n- [Rock Paper Candy - Copy Opponent Move Unless Win](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-candy-copy-opponent-move-unless-win)\n- [Candy Cane - Multi-Armed Bandit](https:\/\/www.kaggle.com\/jamesmcguigan\/candy-cane-multi-armed-bandit)\n- [Candy Cane - Optimized UCB](https:\/\/www.kaggle.com\/jamesmcguigan\/candy-cane-optimized-ucb)\n- [Candy Cane - Random Agent](https:\/\/www.kaggle.com\/jamesmcguigan\/candy-cane-random-agent)\n\nI also created an agents comparison notebook to compare the relative strengths of public agents:\n- [Santa 2020 - Agents Comparison](https:\/\/www.kaggle.com\/jamesmcguigan\/santa-2020-agents-comparison\/)"}}