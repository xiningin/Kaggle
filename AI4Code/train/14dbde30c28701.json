{"cell_type":{"9a140cda":"code","cccfd951":"code","f790895e":"code","6de5c434":"code","2f112a7a":"code","db7c049e":"code","01450930":"code","496d3b4f":"code","db634d23":"code","3805e83b":"code","6ce8fffc":"code","95368ab3":"code","d857074e":"code","5a99bf6a":"code","cc5e137c":"code","e28bc359":"code","ded25570":"code","a5ebb2f7":"code","8d7f5810":"code","923faa70":"code","5b67d347":"code","d16f13c6":"code","ab9520a1":"code","fb4aa7bd":"code","1e7210b2":"code","092bce37":"code","75c4fa63":"code","59722742":"code","7d8f2e90":"code","559094a8":"code","f850705e":"code","a55c8c68":"code","8bcca546":"code","082f1caa":"code","84880a19":"code","6743da13":"code","b7ad3308":"code","32c966e8":"code","38f7f84b":"code","11015c3c":"code","27104108":"code","3f5ab483":"code","d174d1f3":"code","4fc04387":"code","9ac8e9c4":"code","fa1a454d":"code","5d1518f5":"code","20139e20":"code","aaa4d5a2":"code","92da4009":"code","dd319bcd":"code","d4e81744":"code","ca78f4f8":"code","9c5775f8":"code","fab22c89":"code","e6cc7fe8":"code","310c8e76":"code","730ab841":"code","b3f808e8":"code","c1eb8fd6":"code","5ace1c83":"code","a337d81e":"code","d5674413":"code","2611ae91":"code","6a8381f2":"code","bcbbf41a":"code","0e04a012":"code","ebcca046":"code","2aff48d0":"code","c6c80a35":"code","17f0c259":"code","f23dbecc":"code","35431d55":"code","63f2b075":"code","a8ea637b":"code","31991599":"code","77cea6a0":"code","1cdeb242":"code","2751b819":"code","7a9b811e":"code","656e634f":"code","4ef6b0e7":"code","5d6c5b18":"code","5918900b":"code","e32ba6fa":"code","384ed806":"code","d7096110":"code","1e548baf":"code","9db5023b":"code","b4b53968":"code","da78481d":"code","018967ce":"code","ea76fa13":"code","89f79406":"code","09d96eb3":"code","058b3a37":"code","58fe6511":"code","26b04c9f":"markdown","342f0f90":"markdown","7e6632a3":"markdown","7d1b0a34":"markdown","5abdc38b":"markdown","31dfe0d7":"markdown","41ed8f2e":"markdown","a42a5dd6":"markdown","e0fd80c5":"markdown","a997d95e":"markdown","c2b64b2d":"markdown","1ed1c1d6":"markdown","55b7b7b4":"markdown","0d3dc4f3":"markdown","431a9bfa":"markdown","960da12a":"markdown","0c43bd1a":"markdown","92aa8616":"markdown","d914c53b":"markdown","bec63269":"markdown","c3b5462c":"markdown","e8649858":"markdown","e246ec08":"markdown","d06d74ba":"markdown","da6ff81c":"markdown","4d8f4c92":"markdown","78b11971":"markdown","3ed64040":"markdown","68a72ff3":"markdown","f7b1e449":"markdown","0b0f5d8b":"markdown","d674c852":"markdown","859d19cc":"markdown"},"source":{"9a140cda":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\nimport seaborn as sns\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")","cccfd951":"diabetes = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf=diabetes","f790895e":"df.head()","6de5c434":"df.tail()","2f112a7a":"df.columns","db7c049e":"df.info()","01450930":"#import missingno as msno\n#msno.bar(df)\n#plt.show","496d3b4f":"def plot_hist(df,feature):\n    plt.hist(df[feature], bins = 50)\n    plt.xlabel(feature)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(feature))\n    plt.show()","db634d23":"numericVar = df.columns[:-1]\nfor n in numericVar:\n    plot_hist(df,n)","3805e83b":"#Define the number of ZERO s and their percentage\ndef ZEROs(df):\n    print(\"               # of ZEROs  \\t Length \\t  Percent \"  )\n    print(\"------------------------------------------------------\"  )\n\n    for i in range(6) :\n        feature=df[df.columns[i]]\n        ZeroSum=(feature==0).sum()\n        Percent=int(round((ZeroSum\/(len(feature)))*100))\n        print(df.columns[i], \"\\t:\", ZeroSum , \"\\t\\t:\", len(feature), \"\\t \\t:\", Percent,\"% \"  )\n","6ce8fffc":"ZEROs(df)","95368ab3":"# Defining X and y\nX = df.iloc[:,:-1]   # Dependants\ny = df.iloc[:, -1]   # Outcome\n","d857074e":"\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=0, strategy='median')\n\nX =imputer.fit_transform(X)    # it transform it as an array\nX = pd.DataFrame(X,columns=df.columns[:-1])     # Needs to be made as DataFrame again\ndf=pd.concat([X,y],axis=1)\ndf2=df  # \u0131t xill be used later in KNN ","5a99bf6a":"ZEROs(df)","cc5e137c":"pd.options.display.float_format = \"{:,.0f}\".format\ndf","e28bc359":"numericVar = df\nfor n in numericVar:\n    plot_hist(df,n)","ded25570":"#pd.options.display.float_format = \"{:,.0f}\".format\nround(df.describe(),1)","a5ebb2f7":"def detect_outliers(df, features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile Q1\n        Q1 = np.percentile(df[c], 25)\n        # 3st quartile Q3\n        Q3 = np.percentile(df[c], 75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indices\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indices\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","8d7f5810":"df.loc[detect_outliers(df, ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])]","923faa70":"# drop outliers\ndf = df.drop(detect_outliers(df, ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']), axis = 0).reset_index(drop= True)","5b67d347":"list1 = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']","d16f13c6":"sns.heatmap(df[list1].corr(), annot = True, cmap='Blues', fmt = \".2f\")\nplt.show()","ab9520a1":"sns.heatmap(df.corr(), annot = True, cmap='Blues', fmt = \".2f\")\nplt.show()","fb4aa7bd":"x=X\nfor i in x:\n    g = sns.distplot(x[i], color = \"b\", label = \"Skewness : %.2f\"%(x[i].skew()))\n    g = g.legend(loc = \"best\")\n    plt.show()","1e7210b2":"for i in X:\n    g = sns.FacetGrid(df, col = \"Outcome\")\n    g.map(sns.distplot, i, bins= 25)        \n    plt.show()","092bce37":"def plotHistogram(values,label,feature,title):\n    sns.set_style(\"whitegrid\")\n    plotOne = sns.FacetGrid(values, hue=label,aspect=2)\n    plotOne.map(sns.distplot,feature,kde=False)\n    plotOne.set(xlim=(0, values[feature].max()))\n    plotOne.add_legend()\n    plotOne.set_axis_labels(feature, 'Proportion')\n    plotOne.fig.suptitle(title)\n    plt.show()\nfor i in X:\n    plotHistogram(df,\"Outcome\",i,' Diagnosis (Blue = Healthy; Orange = Diabetes)')\n\n\n","75c4fa63":"for i in X.columns[:-1]:\n    sns.factorplot(x = \"Age\", y= i, hue= \"Outcome\", data = df)\n    plt.show()","59722742":"Ls=[\"Pregnancies\",'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']\nfor i in Ls:\n    sns.factorplot(x =i, y= \"Glucose\", hue= \"Outcome\", data = df)\n    plt.show()","7d8f2e90":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","559094a8":"print(\"X_train\", X_train.shape)\nprint(\"X_test\", X_test.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"y_test\", y_test.shape)","f850705e":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()","a55c8c68":"X_train = scale.fit_transform(X_train)\nX_test = scale.transform(X_test)","8bcca546":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression()","082f1caa":"log_model.fit(X_train, y_train)\ny_pred = log_model.predict(X_test)\ny_train_pred = log_model.predict(X_train)","84880a19":"print(\"log_model.coef_:\",log_model.coef_,\"\\nlog_model.intercept_ :\",log_model.intercept_)","6743da13":"from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score","b7ad3308":"scr=[precision_score, recall_score, accuracy_score, f1_score]\nfor i in scr:\n    print(i(y_train, y_train_pred))","32c966e8":"for i in scr:\n    print(i(y_test, y_pred))","38f7f84b":"print(\"f1_score(y_train, y_train_pred, average = weighted) \\t :\",f1_score(y_train, y_train_pred, average = \"weighted\"))\nprint(\"f1_score(y_test, y_pred, average = weighted)\\t\\t :\",f1_score(y_test, y_pred, average = \"weighted\"))","11015c3c":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix,plot_roc_curve\nconfusion_matrix(y_test, y_pred)","27104108":"plot_confusion_matrix(log_model, X_test, y_test)","3f5ab483":"plot_confusion_matrix(log_model, X_test, y_test, normalize= \"all\")","d174d1f3":"from sklearn.model_selection import cross_val_score, cross_validate","4fc04387":"model = LogisticRegression()","9ac8e9c4":"scores = cross_val_score(model, X_train, y_train, cv= 10)\nprint(\"Cross- validation mean of accuracy scores\", scores.mean())\nprint(\"\")\nprint(scores)","fa1a454d":"scores = cross_val_score(model, X_test, y_test, cv= 10)\nprint(\"Cross- validation mean of accuracy scores\", scores.mean())\nprint(\"\")\nprint(scores)","5d1518f5":"pd.options.display.float_format = \"{:,.4f}\".format\nmodel = LogisticRegression()\n\nscores = cross_validate(model, X_train, y_train, scoring = [\"accuracy\", \"precision_weighted\", \"recall_weighted\", \n                                                               \"f1_weighted\"], cv = 10)\ndf_scores = pd.DataFrame(scores, index= range(1, 11))\ndf_scores","20139e20":"df_scores.mean()[2:]","aaa4d5a2":"model = LogisticRegression()\n\nscores = cross_validate(model, X_test, y_test, scoring = [\"accuracy\", \"precision_weighted\", \"recall_weighted\", \n                                                               \"f1_weighted\"], cv = 10)\ndf_scores = pd.DataFrame(scores, index= range(1, 11))\ndf_scores","92da4009":"df_scores.mean()[2:]","dd319bcd":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","d4e81744":"print(classification_report(y_train, y_train_pred))","ca78f4f8":"df2.head()  # ZEROs converted to median ","9c5775f8":"df2.Outcome.value_counts()","fab22c89":"colors = [ \"green\",\"purple\"]\nsns.countplot(df2.Outcome, palette= colors)\nplt.show()","e6cc7fe8":"ND= df[df2.Outcome == 0]\nD= df[df2.Outcome == 1]","310c8e76":"plt.scatter(D.Age, D.Glucose, color= \"red\", label= \"Bad\")\nplt.scatter(ND.Age, ND.Glucose, color= \"green\", label = \"Good\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Glucose\")\nplt.legend()\nplt.show()","730ab841":"labels = [\"Healthy\",\"Diabetics\"]\nexplode = [0, 0]\nsizes = df2.Outcome.value_counts().values\n\nplt.figure(figsize= (7, 7))\nplt.pie(sizes, explode= explode, labels= labels, autopct = \"%1.1f%%\",)\nplt.title(\"Percentage of diagnosis diabetics\", color = \"orange\", fontsize = 20)\nplt.show()","b3f808e8":"corr = df2.corr().Outcome\ncorr[np.argsort(corr, axis= 0)[:-1]]","c1eb8fd6":"y = df2.Outcome\nx = df2.drop(columns = \"Outcome\")","5ace1c83":"x.head()","a337d81e":"x.columns","d5674413":"y.head()","2611ae91":"from sklearn.preprocessing import Normalizer\nx = Normalizer().fit_transform(x)\nx=pd.DataFrame(x,columns=df2.columns[:-1])\nx","6a8381f2":"for i in x:\n    g = sns.distplot(x[i], color = \"b\", label = \"Skewness : %.2f\"%(x[i].skew()))\n    g = g.legend(loc = \"best\")\n    plt.show()","bcbbf41a":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 42)","0e04a012":"print(\"x_train: {}\\nx_test: {}\\ny_train: {}\\ny_test: {}\".format(x_train.shape, x_test.shape, y_train.shape, y_test.shape))","ebcca046":"from sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier(n_neighbors= 3)\nknn_model.fit(x_train, y_train)\ny_pred = knn_model.predict(x_test)","2aff48d0":"y_pred","c6c80a35":"#   p <= %50 = 0,  p > %50 = 1 ","17f0c259":"y_pred_proba = knn_model.predict_proba(x_test)\ny_pred_proba","f23dbecc":"pd.DataFrame(y_pred_proba).sample(10)","35431d55":"my_dict = {\"Actual\": y_test, \"Pred\": y_pred, \"Pred_proba\": y_pred_proba[:,1]}\npd.DataFrame.from_dict(my_dict).sample(20)","63f2b075":"from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix","a8ea637b":"confusion_matrix(y_test, y_pred)","31991599":"plot_confusion_matrix(knn_model, x_test, y_test);","77cea6a0":"plot_confusion_matrix(knn_model, x_test, y_test, normalize= \"all\")","1cdeb242":"print(classification_report(y_test, y_pred))","2751b819":"y_train_pred = knn_model.predict(x_train)\nprint(classification_report(y_train, y_train_pred))","7a9b811e":"from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score","656e634f":"test_error_rates = []\nfor k in range(20, 50):\n    knn_model = KNeighborsClassifier(n_neighbors= k)\n    knn_model.fit(x_train, y_train)\n    \n    y_pred_test = knn_model.predict(x_test)\n    \n    test_error = 1 - accuracy_score(y_test, y_pred_test)\n    test_error_rates.append(test_error)\n","4ef6b0e7":"plt.figure(figsize = (15, 8))\nplt.plot(range(20,50), test_error_rates, color = \"blue\", linestyle = \"--\", marker = \"o\",\n         markerfacecolor= \"red\", markersize= 10)\nplt.title(\"Error Rate vs K-Values\")\nplt.xlabel(\"K_values\")\nplt.ylabel(\"Error Rate\")\nplt.hlines(y= 0.34, xmin= 20, xmax= 50,colors = \"r\", linestyles= \"--\")\nplt.hlines(y= 0.32, xmin= 20, xmax= 50,colors = \"r\", linestyles= \"--\")\nplt.show()","5d6c5b18":"# k = 1\nknn = KNeighborsClassifier(n_neighbors= 28)\nknn.fit(x_train, y_train)\npred = knn.predict(x_test)\n\nprint(\"TEST\")\nprint(\"\\n\")\nprint(confusion_matrix(y_test, pred))\nprint(\"\\n\")\nprint(classification_report(y_test, pred))","5918900b":"\nknn = KNeighborsClassifier(n_neighbors= 45)\nknn.fit(x_train, y_train)\npred = knn.predict(x_test)\n\nprint(\"TEST\")\nprint(\"\\n\")\nprint(confusion_matrix(y_test, pred))\nprint(\"\\n\")\nprint(classification_report(y_test, pred))","e32ba6fa":"from sklearn.model_selection import cross_val_score, cross_validate","384ed806":"model = KNeighborsClassifier(n_neighbors= 45)\n\nscores = cross_validate(model, x_train, y_train, scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"], cv = 10)\n","d7096110":"df_scores = pd.DataFrame(scores, index = range(1,11))\ndf_scores","1e548baf":"df_scores.mean()[2:]","9db5023b":"from sklearn.model_selection import GridSearchCV","b4b53968":"knn_grid = KNeighborsClassifier()","da78481d":"k_values = range(1, 50)","018967ce":"param_grid = {\"n_neighbors\": k_values, \"weights\": [\"uniform\", \"distance\"]}","ea76fa13":"knn_grid_model = GridSearchCV(knn_grid, param_grid, cv= 10, scoring = \"accuracy\")","89f79406":"knn_grid_model.fit(x_train, y_train)","09d96eb3":"knn_grid_model.best_estimator_","058b3a37":"\nknn = KNeighborsClassifier(n_neighbors= 45)\n\nknn.fit(x_train, y_train)\npred = knn.predict(x_test)\n\nprint(\"TEST\")\nprint(\"\\n\")\nprint(confusion_matrix(y_test, pred))\nprint(\"\\n\")\nprint(classification_report(y_test, pred))","58fe6511":"knn = KNeighborsClassifier(n_neighbors= 45)\n\nknn.fit(x_train, y_train)\npred = knn.predict(x_train)\n\nprint(\"TRAIN\")\nprint(\"\\n\")\nprint(confusion_matrix(y_train, pred))\nprint(\"\\n\")\nprint(classification_report(y_train, pred))","26b04c9f":"    * Half of the Insuline records are invalid, so high.\n    Lets change all the ZERO s to their features medians","342f0f90":"## Gridseach Method for Choosing Reasonable K Values","7e6632a3":"## Train - Test split","7d1b0a34":"    * When the shapes are more unfamilar, it shows the feature have  more correlation ","5abdc38b":"## Split Dataset","31dfe0d7":"## Outlier Detection","41ed8f2e":"    * We are free from zeros anymore. Lets continue to investigate the features","a42a5dd6":"# KNN","e0fd80c5":"## Scaling and Categorizing Data Features","a997d95e":"## Load and Check Dataset","c2b64b2d":"    * There are many ZEROS in features they correspond = NaN values.","1ed1c1d6":"    * We will try both,  droping and not  ","55b7b7b4":"## Scores by Various K Values","0d3dc4f3":" *0 = not diabetics : ND\n ","431a9bfa":"## Modeling","960da12a":"## Cross Validate For Optimal K Value","0c43bd1a":"    * No meaningful inter correlation ","92aa8616":"CONTEXT\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases of India","d914c53b":"## Feature comparasion with dependant \"Outcome\"","bec63269":"## EDA","c3b5462c":"## Model Performance on Classification Tasks","e8649858":"About this file\nPregnancies     : Number of times pregnant\nGlucose         : Plasma glucose concentration a 2 hours in an oral glucose tolerance test\nBloodPressure   : Diastolic blood pressure (mm Hg)\nSkinThickness   : Triceps skin fold thickness (mm)\nInsulin         : 2-Hour serum insulin (mu U\/ml)\nBMI             : Body mass index (weight in kg\/(height in m)^2)\nDiabetesPedigreeFunction: Diabetes pedigree function\nAge             : Age (years)\nOutcome         : Class variable (0 or 1)\n<p>","e246ec08":"## Outliers and skew check","d06d74ba":"## Explaining Features and Determine Target Value","da6ff81c":"## Correlation in Data","4d8f4c92":"    * First 6 features has invalid values   \n      There are many ZEROS in them,  they correspond = NaN values. Let see the situation in detail","78b11971":"## Logistic Reggression","3ed64040":"## Elbow Method for Choosing Reasonable K Values","68a72ff3":"## Visualization\n\nCorreleations","f7b1e449":"    * There is a high relation between higher glucose level and diabet","0b0f5d8b":"    * The most correlation with depandant comes from Glucose:0.49 ","d674c852":" *1 = diabetics : D\n \n \n ","859d19cc":"Precision quantifies the number of positive class predictions that actually belong to the positive class.\nRecall quantifies the number of positive class predictions made out of all positive examples in the dataset.\nF-Measure provides a single score that balances both the concerns of precision and recall in one number.\n\naccuracy_score = R2_score"}}