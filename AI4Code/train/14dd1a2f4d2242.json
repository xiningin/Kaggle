{"cell_type":{"f9f02837":"code","2b9b14a3":"code","a657eaf3":"code","2dbc5451":"code","088c10c1":"code","6b6dc576":"code","04fb2b77":"code","859a0c63":"code","ef955fb9":"code","4078d41c":"code","ba7a93a8":"code","877708d3":"code","dac43b9a":"code","f35e9900":"code","de059005":"code","cf48fde0":"code","27c6ee69":"code","a30efa6c":"code","8371a861":"code","286153df":"markdown","2d793962":"markdown","9f69d0e2":"markdown","e3e6472f":"markdown","73fd23a0":"markdown","e29bb198":"markdown","08dbed49":"markdown","23493adb":"markdown","ef1466c1":"markdown","901047bf":"markdown","abb24077":"markdown"},"source":{"f9f02837":"from IPython.display import clear_output\n!pip install epitran\n!pip install sklearn_crfsuite\n!pip install pythainlp[attacut,icu,wangchanberta]\n!pip install datasets transformers\n!pip install deepcut\nclear_output()","2b9b14a3":"from datasets import load_dataset\nimport pandas as pd\nimport numpy as np\nimport pythainlp\n\ndataset = load_dataset(\"thai_toxicity_tweet\")\ndf = dataset.set_format(type='pandas', columns=['tweet_text','is_toxic']); clear_output()","a657eaf3":"df = dataset['train'][:]\ndf['tweet_text'] = df['tweet_text'].apply(lambda x: np.nan if x=='TWEET_NOT_FOUND' or len(x)==0 else x)","2dbc5451":"print(df.isnull().sum())\ndf.dropna(inplace=True)\nprint(\"Remaining data :\",df.shape)","088c10c1":"from pythainlp.tokenize import word_tokenize, clause_tokenize\n\ntext = '\"\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e17\u0e18\u0e4c\" \u0e15\u0e34\u0e14\u0e42\u0e1c\u0e1c\u0e39\u0e49\u0e19\u0e33\u0e04\u0e38\u0e01\u0e04\u0e32\u0e21\u0e40\u0e2a\u0e23\u0e35\u0e20\u0e32\u0e1e\u0e2a\u0e37\u0e48\u0e2d \u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e08\u0e31\u0e14\u0e2d\u0e31\u0e19\u0e14\u0e31\u0e1a\u0e02\u0e2d\u0e07 Reporters without borders'\n\nprint(word_tokenize(text, engine=\"deepcut\",keep_whitespace=False))\nprint(word_tokenize(text, engine='icu',keep_whitespace=False))","6b6dc576":"from pythainlp.tokenize import word_tokenize\n\nsequences = []\n\nfor sentence in df['tweet_text']:\n    tokens = word_tokenize(sentence, engine='deepcut', keep_whitespace=False)\n    sequences.append(tokens)","04fb2b77":"import seaborn as sns\nlen_s = np.array([len(e) for e in sequences])\nsns.violinplot(x=len_s);","859a0c63":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\nMAX_VOCAB = 500\nMAX_LEN = 50\n\npadded_data = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post', \n                                                            maxlen=MAX_LEN, dtype=object,value='')","ef955fb9":"padded_data","4078d41c":"tokens_to_input_ids = layers.experimental.preprocessing.StringLookup(max_tokens=MAX_VOCAB)\ntokens_to_input_ids.adapt(padded_data)\ntokens_to_input_ids(padded_data)","ba7a93a8":"print(tokens_to_input_ids.get_vocabulary()[:20])","877708d3":"targets = df['is_toxic']","dac43b9a":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(\n        input_dim=MAX_VOCAB,\n        output_dim=64,\n        mask_zero=True),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])","f35e9900":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=[\"accuracy\"])","de059005":"from sklearn.model_selection import train_test_split\nseq_train, seq_test, target_train, target_test = train_test_split(padded_data,targets, test_size=0.2, stratify=targets)","cf48fde0":"train_input_ids = tokens_to_input_ids(seq_train)\ntest_input_ids = tokens_to_input_ids(seq_test)","27c6ee69":"early_stop = tf.keras.callbacks.EarlyStopping(\n                monitor='val_loss', min_delta=0.01, patience=10, \n                verbose=1,restore_best_weights=False\n            )\nhistory = model.fit(train_input_ids, target_train, epochs=100,\n                    batch_size=32,\n                    validation_split=0.2,\n                    callbacks=[early_stop])","a30efa6c":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(12,6))\n\nax[0].plot(history.history['loss'], label='loss')\nax[0].plot(history.history['val_loss'], label='val_loss')\nax[0].legend()\n\nax[1].plot(history.history['accuracy'], label='accuracy')\nax[1].plot(history.history['val_accuracy'], label='val_accuracy')\nax[1].legend()\n\nplt.show()","8371a861":"from sklearn.metrics import classification_report\npred = model.predict(test_input_ids)\nprint(classification_report(y_true = target_test, y_pred = pred>0.5))","286153df":"## 3.3) Evaluation","2d793962":"## 2.2) padding","9f69d0e2":"# 2. Preprocessing","e3e6472f":"### Check for missing data","73fd23a0":"### tokenizer \nTry different tokenizers available in PyThaiNLP.","e29bb198":"## 2.3) get input ids from tokens","08dbed49":"## 2.1) Tokenization\n---\nUse `pythainlp.tokenize.word_tokenize` tokenizer with `deepcut` function.","23493adb":"# 3. Building model","ef1466c1":"## 3.1) Train test split","901047bf":"## 3.2) Fit model","abb24077":"# 1. Dataset\nWe load dataset from HuggingFace model hub"}}